program start:
num_rounds= 0
node_emb_dim= 32

----------------------------------------new_epoch--------------------------------------

epoch:  0
iteration : 0
train acc:  0.546875
train loss:  0.7134292721748352
train gradient:  0.34866493169361673
iteration : 1
train acc:  0.53125
train loss:  0.7145648002624512
train gradient:  0.43626918368248235
iteration : 2
train acc:  0.484375
train loss:  0.765728235244751
train gradient:  0.6092738628072425
iteration : 3
train acc:  0.4921875
train loss:  0.7579125165939331
train gradient:  0.6289250538292339
iteration : 4
train acc:  0.546875
train loss:  0.6782066822052002
train gradient:  0.35065996956973894
iteration : 5
train acc:  0.5390625
train loss:  0.7069503664970398
train gradient:  0.43230686554337683
iteration : 6
train acc:  0.5625
train loss:  0.7024819850921631
train gradient:  0.5551774099157751
iteration : 7
train acc:  0.5859375
train loss:  0.6634087562561035
train gradient:  0.2905522357528796
iteration : 8
train acc:  0.59375
train loss:  0.6960372924804688
train gradient:  0.35805066148818565
iteration : 9
train acc:  0.5625
train loss:  0.6997689604759216
train gradient:  0.2844966077680419
iteration : 10
train acc:  0.5703125
train loss:  0.6682376861572266
train gradient:  0.3998743284771105
iteration : 11
train acc:  0.6328125
train loss:  0.6409006118774414
train gradient:  0.20192221001071836
iteration : 12
train acc:  0.5625
train loss:  0.6991696357727051
train gradient:  0.22799909668922058
iteration : 13
train acc:  0.5859375
train loss:  0.6836872696876526
train gradient:  0.23131272450632157
iteration : 14
train acc:  0.546875
train loss:  0.6866691708564758
train gradient:  0.20565690164958556
iteration : 15
train acc:  0.6328125
train loss:  0.6379454731941223
train gradient:  0.22020188583414652
iteration : 16
train acc:  0.6953125
train loss:  0.6389607191085815
train gradient:  0.149622040106608
iteration : 17
train acc:  0.5546875
train loss:  0.683112621307373
train gradient:  0.22567088129530144
iteration : 18
train acc:  0.7109375
train loss:  0.6281978487968445
train gradient:  0.19314871409534634
iteration : 19
train acc:  0.609375
train loss:  0.698556661605835
train gradient:  0.2564114197832496
iteration : 20
train acc:  0.5625
train loss:  0.6718462705612183
train gradient:  0.16058550820367315
iteration : 21
train acc:  0.640625
train loss:  0.6511344909667969
train gradient:  0.17585374015225147
iteration : 22
train acc:  0.6484375
train loss:  0.6412428617477417
train gradient:  0.20446282799822346
iteration : 23
train acc:  0.6640625
train loss:  0.649777352809906
train gradient:  0.1470124938805859
iteration : 24
train acc:  0.7421875
train loss:  0.5763304233551025
train gradient:  0.18589760691906637
iteration : 25
train acc:  0.5234375
train loss:  0.672753095626831
train gradient:  0.21277439317792646
iteration : 26
train acc:  0.65625
train loss:  0.6184424161911011
train gradient:  0.19037145290462956
iteration : 27
train acc:  0.6171875
train loss:  0.6313924789428711
train gradient:  0.179331183847799
iteration : 28
train acc:  0.6484375
train loss:  0.6470503807067871
train gradient:  0.19759944015427416
iteration : 29
train acc:  0.625
train loss:  0.6309309005737305
train gradient:  0.2139438756887035
iteration : 30
train acc:  0.6484375
train loss:  0.6556117534637451
train gradient:  0.1811983890531093
iteration : 31
train acc:  0.640625
train loss:  0.6202402114868164
train gradient:  0.18120303419254086
iteration : 32
train acc:  0.6171875
train loss:  0.6537674069404602
train gradient:  0.23089136953132594
iteration : 33
train acc:  0.578125
train loss:  0.6706862449645996
train gradient:  0.19982393653683417
iteration : 34
train acc:  0.6015625
train loss:  0.6747909784317017
train gradient:  0.18045603089604031
iteration : 35
train acc:  0.703125
train loss:  0.5947391390800476
train gradient:  0.2449717207802831
iteration : 36
train acc:  0.7109375
train loss:  0.5794395208358765
train gradient:  0.1415163581394444
iteration : 37
train acc:  0.6328125
train loss:  0.6173985004425049
train gradient:  0.2511689038411642
iteration : 38
train acc:  0.6953125
train loss:  0.6172325611114502
train gradient:  0.21504963599718524
iteration : 39
train acc:  0.59375
train loss:  0.6759381294250488
train gradient:  0.1530247651677708
iteration : 40
train acc:  0.6328125
train loss:  0.6372803449630737
train gradient:  0.17658645319833877
iteration : 41
train acc:  0.59375
train loss:  0.6732145547866821
train gradient:  0.20935309266632346
iteration : 42
train acc:  0.640625
train loss:  0.6211285591125488
train gradient:  0.17807914475870296
iteration : 43
train acc:  0.7265625
train loss:  0.5970511436462402
train gradient:  0.17143942850177946
iteration : 44
train acc:  0.6640625
train loss:  0.6369998455047607
train gradient:  0.15670334307440503
iteration : 45
train acc:  0.6875
train loss:  0.5729814171791077
train gradient:  0.15829189975877006
iteration : 46
train acc:  0.6015625
train loss:  0.659898042678833
train gradient:  0.15589365124504445
iteration : 47
train acc:  0.6484375
train loss:  0.6525185704231262
train gradient:  0.2584945715896268
iteration : 48
train acc:  0.65625
train loss:  0.6086797714233398
train gradient:  0.17977481654257688
iteration : 49
train acc:  0.6796875
train loss:  0.6014226078987122
train gradient:  0.1722855743217392
iteration : 50
train acc:  0.671875
train loss:  0.6183042526245117
train gradient:  0.16061433443273238
iteration : 51
train acc:  0.6171875
train loss:  0.673872172832489
train gradient:  0.19445134166664246
iteration : 52
train acc:  0.640625
train loss:  0.6410859823226929
train gradient:  0.17066585592002637
iteration : 53
train acc:  0.6953125
train loss:  0.6191459894180298
train gradient:  0.24334670263064878
iteration : 54
train acc:  0.5546875
train loss:  0.6857376098632812
train gradient:  0.20361137916392175
iteration : 55
train acc:  0.6640625
train loss:  0.5902682542800903
train gradient:  0.17229702507381767
iteration : 56
train acc:  0.6171875
train loss:  0.6126758456230164
train gradient:  0.18885778861147018
iteration : 57
train acc:  0.6015625
train loss:  0.6441448330879211
train gradient:  0.16802221110938698
iteration : 58
train acc:  0.6640625
train loss:  0.585267186164856
train gradient:  0.1836790671506302
iteration : 59
train acc:  0.6171875
train loss:  0.6459363698959351
train gradient:  0.18803572458255596
iteration : 60
train acc:  0.5859375
train loss:  0.7148311138153076
train gradient:  0.23142982001817636
iteration : 61
train acc:  0.6328125
train loss:  0.6221693754196167
train gradient:  0.21325446808934964
iteration : 62
train acc:  0.640625
train loss:  0.6303434371948242
train gradient:  0.14448558520939253
iteration : 63
train acc:  0.6015625
train loss:  0.6248096227645874
train gradient:  0.17208530918421694
iteration : 64
train acc:  0.6484375
train loss:  0.6624652743339539
train gradient:  0.2027974697802072
iteration : 65
train acc:  0.6484375
train loss:  0.6351252794265747
train gradient:  0.2015895844388259
iteration : 66
train acc:  0.6484375
train loss:  0.641954779624939
train gradient:  0.21315571800709543
iteration : 67
train acc:  0.6953125
train loss:  0.5968663692474365
train gradient:  0.15868556974931453
iteration : 68
train acc:  0.671875
train loss:  0.6228111982345581
train gradient:  0.157738960265293
iteration : 69
train acc:  0.6796875
train loss:  0.6259421110153198
train gradient:  0.17955860431647963
iteration : 70
train acc:  0.6328125
train loss:  0.6726890802383423
train gradient:  0.2265538431139532
iteration : 71
train acc:  0.6484375
train loss:  0.6717187762260437
train gradient:  0.16431690553813144
iteration : 72
train acc:  0.5859375
train loss:  0.6509978771209717
train gradient:  0.2759817300218142
iteration : 73
train acc:  0.6171875
train loss:  0.6294095516204834
train gradient:  0.13987272217801539
iteration : 74
train acc:  0.6875
train loss:  0.5826249122619629
train gradient:  0.14217293162673578
iteration : 75
train acc:  0.6171875
train loss:  0.6548901200294495
train gradient:  0.21417992384473783
iteration : 76
train acc:  0.6171875
train loss:  0.64164799451828
train gradient:  0.14514332305261735
iteration : 77
train acc:  0.640625
train loss:  0.6365845799446106
train gradient:  0.18847912471960682
iteration : 78
train acc:  0.609375
train loss:  0.6422149538993835
train gradient:  0.15932974192535265
iteration : 79
train acc:  0.6171875
train loss:  0.6384502053260803
train gradient:  0.21844030739410625
iteration : 80
train acc:  0.671875
train loss:  0.6022804975509644
train gradient:  0.16962248658300705
iteration : 81
train acc:  0.5546875
train loss:  0.6712528467178345
train gradient:  0.25228819396002045
iteration : 82
train acc:  0.65625
train loss:  0.614290714263916
train gradient:  0.13308913130998326
iteration : 83
train acc:  0.5625
train loss:  0.6561437845230103
train gradient:  0.17189361134937467
iteration : 84
train acc:  0.6484375
train loss:  0.6667733788490295
train gradient:  0.16308647472316162
iteration : 85
train acc:  0.65625
train loss:  0.6164116263389587
train gradient:  0.17966146976786052
iteration : 86
train acc:  0.6015625
train loss:  0.65749591588974
train gradient:  0.16416259356366003
iteration : 87
train acc:  0.6640625
train loss:  0.6058043241500854
train gradient:  0.14422924330103307
iteration : 88
train acc:  0.671875
train loss:  0.6156837940216064
train gradient:  0.17871929927634944
iteration : 89
train acc:  0.6171875
train loss:  0.6428120136260986
train gradient:  0.17171950644678602
iteration : 90
train acc:  0.6796875
train loss:  0.5944403409957886
train gradient:  0.1680387288859152
iteration : 91
train acc:  0.65625
train loss:  0.6035023927688599
train gradient:  0.17902887547422106
iteration : 92
train acc:  0.703125
train loss:  0.6024419069290161
train gradient:  0.27918571740796083
iteration : 93
train acc:  0.671875
train loss:  0.6080389618873596
train gradient:  0.15954469730182208
iteration : 94
train acc:  0.6796875
train loss:  0.6126019954681396
train gradient:  0.24464649637509156
iteration : 95
train acc:  0.6796875
train loss:  0.6081395149230957
train gradient:  0.18683108272475812
iteration : 96
train acc:  0.640625
train loss:  0.6283931136131287
train gradient:  0.11947324003685467
iteration : 97
train acc:  0.5859375
train loss:  0.6695682406425476
train gradient:  0.18357661593127483
iteration : 98
train acc:  0.671875
train loss:  0.5853009223937988
train gradient:  0.19476525203408024
iteration : 99
train acc:  0.625
train loss:  0.647507905960083
train gradient:  0.18829217612285581
iteration : 100
train acc:  0.6328125
train loss:  0.6661465764045715
train gradient:  0.27161011434457577
iteration : 101
train acc:  0.6796875
train loss:  0.601974368095398
train gradient:  0.15214532726363167
iteration : 102
train acc:  0.671875
train loss:  0.6115765571594238
train gradient:  0.21229342963842346
iteration : 103
train acc:  0.6328125
train loss:  0.6188864707946777
train gradient:  0.1579405168073117
iteration : 104
train acc:  0.6484375
train loss:  0.623321533203125
train gradient:  0.1859093892863559
iteration : 105
train acc:  0.6015625
train loss:  0.6321138143539429
train gradient:  0.13286316425088943
iteration : 106
train acc:  0.59375
train loss:  0.6044142246246338
train gradient:  0.1994438649899065
iteration : 107
train acc:  0.671875
train loss:  0.6113405227661133
train gradient:  0.17493009443506213
iteration : 108
train acc:  0.609375
train loss:  0.6324107646942139
train gradient:  0.17628412271854565
iteration : 109
train acc:  0.6171875
train loss:  0.667425274848938
train gradient:  0.1801330548883649
iteration : 110
train acc:  0.609375
train loss:  0.6498168706893921
train gradient:  0.17691598581363896
iteration : 111
train acc:  0.5859375
train loss:  0.6323379278182983
train gradient:  0.13671653063331157
iteration : 112
train acc:  0.59375
train loss:  0.6863987445831299
train gradient:  0.21912747960831191
iteration : 113
train acc:  0.640625
train loss:  0.6035671234130859
train gradient:  0.13826870355656173
iteration : 114
train acc:  0.6484375
train loss:  0.6186562180519104
train gradient:  0.16364270781508727
iteration : 115
train acc:  0.6796875
train loss:  0.6354408264160156
train gradient:  0.17537843910935108
iteration : 116
train acc:  0.6640625
train loss:  0.5993699431419373
train gradient:  0.16876686166721738
iteration : 117
train acc:  0.6015625
train loss:  0.6701357960700989
train gradient:  0.2243906695222721
iteration : 118
train acc:  0.6484375
train loss:  0.5958207845687866
train gradient:  0.1473276001323754
iteration : 119
train acc:  0.6015625
train loss:  0.6257205009460449
train gradient:  0.17844436089662996
iteration : 120
train acc:  0.578125
train loss:  0.6568397879600525
train gradient:  0.1457873754120027
iteration : 121
train acc:  0.6328125
train loss:  0.6473786234855652
train gradient:  0.1728142749391121
iteration : 122
train acc:  0.59375
train loss:  0.6347258687019348
train gradient:  0.30001753743367615
iteration : 123
train acc:  0.65625
train loss:  0.6162129640579224
train gradient:  0.13278247010152
iteration : 124
train acc:  0.671875
train loss:  0.6366039514541626
train gradient:  0.1911176831967954
iteration : 125
train acc:  0.515625
train loss:  0.6918497681617737
train gradient:  0.22091150905865567
iteration : 126
train acc:  0.6484375
train loss:  0.5647145509719849
train gradient:  0.14700143282478434
iteration : 127
train acc:  0.6171875
train loss:  0.6499544382095337
train gradient:  0.15662945370624437
iteration : 128
train acc:  0.7578125
train loss:  0.5476911067962646
train gradient:  0.1742207252778115
iteration : 129
train acc:  0.609375
train loss:  0.6329095959663391
train gradient:  0.17639617537459784
iteration : 130
train acc:  0.6015625
train loss:  0.6673271059989929
train gradient:  0.1899616253636582
iteration : 131
train acc:  0.5859375
train loss:  0.6621947288513184
train gradient:  0.24704382479902431
iteration : 132
train acc:  0.6171875
train loss:  0.6592656373977661
train gradient:  0.2358154838343728
iteration : 133
train acc:  0.65625
train loss:  0.6040871143341064
train gradient:  0.21835841532779282
iteration : 134
train acc:  0.6953125
train loss:  0.598992109298706
train gradient:  0.2936196470244887
iteration : 135
train acc:  0.625
train loss:  0.618216872215271
train gradient:  0.129499874537239
iteration : 136
train acc:  0.6953125
train loss:  0.5878366827964783
train gradient:  0.16006104643431657
iteration : 137
train acc:  0.7109375
train loss:  0.5899688005447388
train gradient:  0.20346699898542175
iteration : 138
train acc:  0.609375
train loss:  0.65081787109375
train gradient:  0.1863721079846503
iteration : 139
train acc:  0.703125
train loss:  0.5949763059616089
train gradient:  0.17187719543149638
iteration : 140
train acc:  0.640625
train loss:  0.6327307820320129
train gradient:  0.16995589646098916
iteration : 141
train acc:  0.640625
train loss:  0.6059285998344421
train gradient:  0.15744774412584417
iteration : 142
train acc:  0.703125
train loss:  0.5914705991744995
train gradient:  0.18339099607048842
iteration : 143
train acc:  0.6484375
train loss:  0.6254919171333313
train gradient:  0.14812834364971555
iteration : 144
train acc:  0.6953125
train loss:  0.6107501983642578
train gradient:  0.18683291660389728
iteration : 145
train acc:  0.6171875
train loss:  0.633887529373169
train gradient:  0.22756665496196588
iteration : 146
train acc:  0.6875
train loss:  0.6115109324455261
train gradient:  0.15596454698705764
iteration : 147
train acc:  0.6640625
train loss:  0.6065730452537537
train gradient:  0.19281557635119823
iteration : 148
train acc:  0.609375
train loss:  0.6338897943496704
train gradient:  0.23636155186342575
iteration : 149
train acc:  0.6640625
train loss:  0.5929908752441406
train gradient:  0.17401920601741622
iteration : 150
train acc:  0.59375
train loss:  0.6661437153816223
train gradient:  0.19773013164115805
iteration : 151
train acc:  0.71875
train loss:  0.5595884919166565
train gradient:  0.15598923734659628
iteration : 152
train acc:  0.6171875
train loss:  0.6179428100585938
train gradient:  0.135292695430197
iteration : 153
train acc:  0.734375
train loss:  0.5595709681510925
train gradient:  0.18990542169930474
iteration : 154
train acc:  0.59375
train loss:  0.6168479323387146
train gradient:  0.13915740870491855
iteration : 155
train acc:  0.6484375
train loss:  0.647735595703125
train gradient:  0.2399938291449908
iteration : 156
train acc:  0.6328125
train loss:  0.6242175698280334
train gradient:  0.21328413148976075
iteration : 157
train acc:  0.671875
train loss:  0.5812534093856812
train gradient:  0.19321556955510888
iteration : 158
train acc:  0.671875
train loss:  0.6132404208183289
train gradient:  0.12967625680381453
iteration : 159
train acc:  0.6484375
train loss:  0.5937963724136353
train gradient:  0.13600350348809137
iteration : 160
train acc:  0.6640625
train loss:  0.5798864364624023
train gradient:  0.1553704371547811
iteration : 161
train acc:  0.6796875
train loss:  0.5952346324920654
train gradient:  0.1518605184348522
iteration : 162
train acc:  0.5859375
train loss:  0.6389644742012024
train gradient:  0.3527218373472278
iteration : 163
train acc:  0.59375
train loss:  0.6650322079658508
train gradient:  0.21917076581567296
iteration : 164
train acc:  0.640625
train loss:  0.6274224519729614
train gradient:  0.18471702301709586
iteration : 165
train acc:  0.6484375
train loss:  0.6124857068061829
train gradient:  0.1901567833430072
iteration : 166
train acc:  0.65625
train loss:  0.6261409521102905
train gradient:  0.1644272804090283
iteration : 167
train acc:  0.6171875
train loss:  0.6302059292793274
train gradient:  0.1888681692243324
iteration : 168
train acc:  0.6328125
train loss:  0.6370843052864075
train gradient:  0.18103435555897124
iteration : 169
train acc:  0.6640625
train loss:  0.5937239527702332
train gradient:  0.15597469003395825
iteration : 170
train acc:  0.6796875
train loss:  0.6046946048736572
train gradient:  0.12385617605177586
iteration : 171
train acc:  0.703125
train loss:  0.5995299816131592
train gradient:  0.18889989118799055
iteration : 172
train acc:  0.6484375
train loss:  0.6046642065048218
train gradient:  0.16400567794982363
iteration : 173
train acc:  0.65625
train loss:  0.586266815662384
train gradient:  0.13892775477927188
iteration : 174
train acc:  0.6484375
train loss:  0.6086505651473999
train gradient:  0.16759203228099712
iteration : 175
train acc:  0.625
train loss:  0.6214979887008667
train gradient:  0.19977518357356533
iteration : 176
train acc:  0.640625
train loss:  0.6162621974945068
train gradient:  0.2326904737995077
iteration : 177
train acc:  0.6640625
train loss:  0.6131550073623657
train gradient:  0.16209336683560605
iteration : 178
train acc:  0.578125
train loss:  0.657336950302124
train gradient:  0.17926747785515107
iteration : 179
train acc:  0.59375
train loss:  0.660453200340271
train gradient:  0.16987574334244468
iteration : 180
train acc:  0.703125
train loss:  0.565654993057251
train gradient:  0.1637737896457318
iteration : 181
train acc:  0.6640625
train loss:  0.5825177431106567
train gradient:  0.17087260742244617
iteration : 182
train acc:  0.640625
train loss:  0.6258689165115356
train gradient:  0.17909549256059293
iteration : 183
train acc:  0.546875
train loss:  0.6542230844497681
train gradient:  0.14541098913891193
iteration : 184
train acc:  0.6328125
train loss:  0.6538490056991577
train gradient:  0.14616612556855957
iteration : 185
train acc:  0.640625
train loss:  0.5895460247993469
train gradient:  0.21316161260508182
iteration : 186
train acc:  0.6796875
train loss:  0.5727213621139526
train gradient:  0.14157273706057727
iteration : 187
train acc:  0.6328125
train loss:  0.6413514614105225
train gradient:  0.2036428369259569
iteration : 188
train acc:  0.6328125
train loss:  0.5959823727607727
train gradient:  0.19357247887316859
iteration : 189
train acc:  0.65625
train loss:  0.6089158058166504
train gradient:  0.1878470944038093
iteration : 190
train acc:  0.6640625
train loss:  0.6013346314430237
train gradient:  0.18797163425999802
iteration : 191
train acc:  0.671875
train loss:  0.5719552636146545
train gradient:  0.14055244522885374
iteration : 192
train acc:  0.6796875
train loss:  0.6052905321121216
train gradient:  0.19506419188610108
iteration : 193
train acc:  0.671875
train loss:  0.5896857380867004
train gradient:  0.15864885577924165
iteration : 194
train acc:  0.625
train loss:  0.6679991483688354
train gradient:  0.24237398001927885
iteration : 195
train acc:  0.6484375
train loss:  0.6518210172653198
train gradient:  0.2171532076007007
iteration : 196
train acc:  0.6484375
train loss:  0.5897481441497803
train gradient:  0.2026462103077739
iteration : 197
train acc:  0.5859375
train loss:  0.6760739684104919
train gradient:  0.21581677929180657
iteration : 198
train acc:  0.6484375
train loss:  0.6270121932029724
train gradient:  0.28428120950802743
iteration : 199
train acc:  0.7265625
train loss:  0.5617762804031372
train gradient:  0.13293784602199138
iteration : 200
train acc:  0.6328125
train loss:  0.624916672706604
train gradient:  0.14684409040175783
iteration : 201
train acc:  0.6328125
train loss:  0.6012463569641113
train gradient:  0.1755874703158527
iteration : 202
train acc:  0.6171875
train loss:  0.6281496286392212
train gradient:  0.28236751220831036
iteration : 203
train acc:  0.6484375
train loss:  0.6177327036857605
train gradient:  0.17998229897148676
iteration : 204
train acc:  0.703125
train loss:  0.5685765743255615
train gradient:  0.14358516421660256
iteration : 205
train acc:  0.6796875
train loss:  0.5995351076126099
train gradient:  0.16932237962742552
iteration : 206
train acc:  0.6875
train loss:  0.5912265777587891
train gradient:  0.2184344144196032
iteration : 207
train acc:  0.71875
train loss:  0.5396565198898315
train gradient:  0.1501952477617336
iteration : 208
train acc:  0.6484375
train loss:  0.591713547706604
train gradient:  0.12564069029768576
iteration : 209
train acc:  0.671875
train loss:  0.6266072988510132
train gradient:  0.14169775029394427
iteration : 210
train acc:  0.734375
train loss:  0.6081972122192383
train gradient:  0.23806140885996696
iteration : 211
train acc:  0.671875
train loss:  0.6434687972068787
train gradient:  0.19273379939106244
iteration : 212
train acc:  0.6875
train loss:  0.5828326344490051
train gradient:  0.1676517778690026
iteration : 213
train acc:  0.6171875
train loss:  0.6005681753158569
train gradient:  0.1627493832142205
iteration : 214
train acc:  0.6875
train loss:  0.5809227228164673
train gradient:  0.25165660546302226
iteration : 215
train acc:  0.640625
train loss:  0.6443926692008972
train gradient:  0.16610984141121612
iteration : 216
train acc:  0.625
train loss:  0.632552981376648
train gradient:  0.21876852796847168
iteration : 217
train acc:  0.671875
train loss:  0.5882265567779541
train gradient:  0.18627524154715452
iteration : 218
train acc:  0.6484375
train loss:  0.612226128578186
train gradient:  0.22755959371316958
iteration : 219
train acc:  0.6640625
train loss:  0.5733295679092407
train gradient:  0.11741043294012767
iteration : 220
train acc:  0.578125
train loss:  0.6367453932762146
train gradient:  0.22011177102666146
iteration : 221
train acc:  0.625
train loss:  0.6518936157226562
train gradient:  0.19394503615073647
iteration : 222
train acc:  0.6796875
train loss:  0.5722184777259827
train gradient:  0.1266651637797198
iteration : 223
train acc:  0.6328125
train loss:  0.6241718530654907
train gradient:  0.16139153174346943
iteration : 224
train acc:  0.703125
train loss:  0.5588015913963318
train gradient:  0.13099849621714457
iteration : 225
train acc:  0.625
train loss:  0.6103702783584595
train gradient:  0.12112559840645429
iteration : 226
train acc:  0.71875
train loss:  0.5673071146011353
train gradient:  0.13722191220997965
iteration : 227
train acc:  0.765625
train loss:  0.5623174905776978
train gradient:  0.13636382978021622
iteration : 228
train acc:  0.640625
train loss:  0.6044779419898987
train gradient:  0.13830450119924215
iteration : 229
train acc:  0.65625
train loss:  0.6330146193504333
train gradient:  0.1899195741542763
iteration : 230
train acc:  0.6015625
train loss:  0.6971727609634399
train gradient:  0.25325102337462346
iteration : 231
train acc:  0.7265625
train loss:  0.5767767429351807
train gradient:  0.14267407589982478
iteration : 232
train acc:  0.6875
train loss:  0.5801560878753662
train gradient:  0.29222075806500813
iteration : 233
train acc:  0.6796875
train loss:  0.5824648141860962
train gradient:  0.15016002456323807
iteration : 234
train acc:  0.7265625
train loss:  0.5680993795394897
train gradient:  0.21987591590153727
iteration : 235
train acc:  0.6796875
train loss:  0.5768375396728516
train gradient:  0.14383566795452812
iteration : 236
train acc:  0.6875
train loss:  0.6075853109359741
train gradient:  0.17715209362059342
iteration : 237
train acc:  0.7421875
train loss:  0.5438328981399536
train gradient:  0.1880242191031523
iteration : 238
train acc:  0.65625
train loss:  0.5852957367897034
train gradient:  0.1329321593597509
iteration : 239
train acc:  0.6171875
train loss:  0.631057620048523
train gradient:  0.19786423450244744
iteration : 240
train acc:  0.7109375
train loss:  0.5849431157112122
train gradient:  0.14516718789776634
iteration : 241
train acc:  0.5546875
train loss:  0.6853326559066772
train gradient:  0.21357373165899635
iteration : 242
train acc:  0.671875
train loss:  0.6327418684959412
train gradient:  0.17037640262801404
iteration : 243
train acc:  0.6328125
train loss:  0.6113017201423645
train gradient:  0.1517930014880236
iteration : 244
train acc:  0.6953125
train loss:  0.5818194150924683
train gradient:  0.22086307512481054
iteration : 245
train acc:  0.703125
train loss:  0.583000659942627
train gradient:  0.15520697092018304
iteration : 246
train acc:  0.6640625
train loss:  0.6123687028884888
train gradient:  0.1685129841585031
iteration : 247
train acc:  0.6328125
train loss:  0.6212940216064453
train gradient:  0.1868520846023896
iteration : 248
train acc:  0.6171875
train loss:  0.6174153089523315
train gradient:  0.14783883696487377
iteration : 249
train acc:  0.5703125
train loss:  0.6647368669509888
train gradient:  0.17773499322117667
iteration : 250
train acc:  0.6796875
train loss:  0.6065951585769653
train gradient:  0.19688274610778783
iteration : 251
train acc:  0.609375
train loss:  0.6425454616546631
train gradient:  0.15974994658805858
iteration : 252
train acc:  0.6640625
train loss:  0.6094319820404053
train gradient:  0.14559248872443808
iteration : 253
train acc:  0.625
train loss:  0.6083719730377197
train gradient:  0.19758497473239336
iteration : 254
train acc:  0.6953125
train loss:  0.5919573903083801
train gradient:  0.1558877825233511
iteration : 255
train acc:  0.6640625
train loss:  0.6358778476715088
train gradient:  0.19315567053450816
iteration : 256
train acc:  0.703125
train loss:  0.5800042152404785
train gradient:  0.14431867314764935
iteration : 257
train acc:  0.6953125
train loss:  0.580175518989563
train gradient:  0.15260326306990368
iteration : 258
train acc:  0.6796875
train loss:  0.6105437278747559
train gradient:  0.16510979718420574
iteration : 259
train acc:  0.703125
train loss:  0.6197006702423096
train gradient:  0.1407361320694236
iteration : 260
train acc:  0.6484375
train loss:  0.6131343841552734
train gradient:  0.2087540479370159
iteration : 261
train acc:  0.625
train loss:  0.6204137206077576
train gradient:  0.20775908739663485
iteration : 262
train acc:  0.6875
train loss:  0.6111551523208618
train gradient:  0.14955924860443495
iteration : 263
train acc:  0.609375
train loss:  0.6104177236557007
train gradient:  0.16729854079866158
iteration : 264
train acc:  0.6953125
train loss:  0.6173642873764038
train gradient:  0.15923329036684075
iteration : 265
train acc:  0.6484375
train loss:  0.5968072414398193
train gradient:  0.20555162117107595
iteration : 266
train acc:  0.671875
train loss:  0.6897107362747192
train gradient:  0.3250276103443712
iteration : 267
train acc:  0.6171875
train loss:  0.632478654384613
train gradient:  0.16957669025755512
iteration : 268
train acc:  0.71875
train loss:  0.5607497692108154
train gradient:  0.16266578307125304
iteration : 269
train acc:  0.65625
train loss:  0.6179371476173401
train gradient:  0.2656681251040803
iteration : 270
train acc:  0.6875
train loss:  0.5893025398254395
train gradient:  0.18376939508427048
iteration : 271
train acc:  0.546875
train loss:  0.672783374786377
train gradient:  0.23213086560883975
iteration : 272
train acc:  0.6328125
train loss:  0.6371328234672546
train gradient:  0.16512938130139676
iteration : 273
train acc:  0.6875
train loss:  0.5830193758010864
train gradient:  0.17488190032187956
iteration : 274
train acc:  0.609375
train loss:  0.6132164001464844
train gradient:  0.13239346997392287
iteration : 275
train acc:  0.671875
train loss:  0.5809180736541748
train gradient:  0.14951121583697757
iteration : 276
train acc:  0.765625
train loss:  0.5355899333953857
train gradient:  0.11752681778520903
iteration : 277
train acc:  0.640625
train loss:  0.6068491339683533
train gradient:  0.1771483717409295
iteration : 278
train acc:  0.6171875
train loss:  0.627752959728241
train gradient:  0.17951294501714155
iteration : 279
train acc:  0.6328125
train loss:  0.6169706583023071
train gradient:  0.20994062332879387
iteration : 280
train acc:  0.5703125
train loss:  0.677641749382019
train gradient:  0.1810517251226215
iteration : 281
train acc:  0.671875
train loss:  0.6087023019790649
train gradient:  0.20444528672431272
iteration : 282
train acc:  0.640625
train loss:  0.5948656797409058
train gradient:  0.18691982461003553
iteration : 283
train acc:  0.640625
train loss:  0.5856181979179382
train gradient:  0.20051972745056035
iteration : 284
train acc:  0.6875
train loss:  0.578445315361023
train gradient:  0.17093002694441423
iteration : 285
train acc:  0.75
train loss:  0.5487918853759766
train gradient:  0.17418650263346994
iteration : 286
train acc:  0.6484375
train loss:  0.6581003069877625
train gradient:  0.19177989364302772
iteration : 287
train acc:  0.6328125
train loss:  0.6167721152305603
train gradient:  0.18698645844949754
iteration : 288
train acc:  0.703125
train loss:  0.5710375308990479
train gradient:  0.17606787748499322
iteration : 289
train acc:  0.640625
train loss:  0.6020544767379761
train gradient:  0.14596082899504337
iteration : 290
train acc:  0.7109375
train loss:  0.5575188398361206
train gradient:  0.14978178055393776
iteration : 291
train acc:  0.6015625
train loss:  0.6246799230575562
train gradient:  0.363257980821445
iteration : 292
train acc:  0.640625
train loss:  0.6051537990570068
train gradient:  0.1380647812360294
iteration : 293
train acc:  0.609375
train loss:  0.6316152215003967
train gradient:  0.265497351876559
iteration : 294
train acc:  0.6875
train loss:  0.5950437784194946
train gradient:  0.1513490807496598
iteration : 295
train acc:  0.6015625
train loss:  0.6213629245758057
train gradient:  0.20184597126277326
iteration : 296
train acc:  0.625
train loss:  0.575844943523407
train gradient:  0.1253440363698357
iteration : 297
train acc:  0.703125
train loss:  0.5615514516830444
train gradient:  0.16393685992900586
iteration : 298
train acc:  0.6484375
train loss:  0.5927010178565979
train gradient:  0.13941870296506093
iteration : 299
train acc:  0.625
train loss:  0.6444693207740784
train gradient:  0.21922496865467694
iteration : 300
train acc:  0.65625
train loss:  0.5747417211532593
train gradient:  0.16801091724051176
iteration : 301
train acc:  0.609375
train loss:  0.6223630309104919
train gradient:  0.1353673651462471
iteration : 302
train acc:  0.7109375
train loss:  0.5527025461196899
train gradient:  0.17123182143413546
iteration : 303
train acc:  0.6484375
train loss:  0.6193636059761047
train gradient:  0.14719405106681918
iteration : 304
train acc:  0.6171875
train loss:  0.6154317855834961
train gradient:  0.13923178339238787
iteration : 305
train acc:  0.734375
train loss:  0.5326545834541321
train gradient:  0.15392257829509742
iteration : 306
train acc:  0.609375
train loss:  0.6210342049598694
train gradient:  0.13900628292836215
iteration : 307
train acc:  0.71875
train loss:  0.5639175176620483
train gradient:  0.14176765261159321
iteration : 308
train acc:  0.6796875
train loss:  0.5862873792648315
train gradient:  0.1391436044011787
iteration : 309
train acc:  0.6640625
train loss:  0.5893391966819763
train gradient:  0.15251035995397247
iteration : 310
train acc:  0.6953125
train loss:  0.6238603591918945
train gradient:  0.1997520278703362
iteration : 311
train acc:  0.5859375
train loss:  0.6400315165519714
train gradient:  0.20519396800304834
iteration : 312
train acc:  0.609375
train loss:  0.6213307976722717
train gradient:  0.14531888247882419
iteration : 313
train acc:  0.6328125
train loss:  0.635673999786377
train gradient:  0.19107393105463283
iteration : 314
train acc:  0.625
train loss:  0.636573076248169
train gradient:  0.21992706440502152
iteration : 315
train acc:  0.7109375
train loss:  0.5480395555496216
train gradient:  0.1523092223675079
iteration : 316
train acc:  0.71875
train loss:  0.5950839519500732
train gradient:  0.14772177453065466
iteration : 317
train acc:  0.6796875
train loss:  0.6175918579101562
train gradient:  0.19347588207186067
iteration : 318
train acc:  0.6796875
train loss:  0.6044787168502808
train gradient:  0.25575170046684376
iteration : 319
train acc:  0.671875
train loss:  0.6153220534324646
train gradient:  0.14390745279867342
iteration : 320
train acc:  0.59375
train loss:  0.6279497742652893
train gradient:  0.19115541541869585
iteration : 321
train acc:  0.640625
train loss:  0.6182013750076294
train gradient:  0.20590947184172914
iteration : 322
train acc:  0.71875
train loss:  0.5462520122528076
train gradient:  0.13252521105547732
iteration : 323
train acc:  0.7109375
train loss:  0.5691884756088257
train gradient:  0.22178035059065465
iteration : 324
train acc:  0.625
train loss:  0.6489402651786804
train gradient:  0.16705582808919445
iteration : 325
train acc:  0.65625
train loss:  0.6106442213058472
train gradient:  0.22961666597343156
iteration : 326
train acc:  0.6875
train loss:  0.6336585283279419
train gradient:  0.13123890863269938
iteration : 327
train acc:  0.6796875
train loss:  0.6211580038070679
train gradient:  0.14957167457197168
iteration : 328
train acc:  0.640625
train loss:  0.6247931718826294
train gradient:  0.14536949342270358
iteration : 329
train acc:  0.765625
train loss:  0.5385024547576904
train gradient:  0.15071282881226755
iteration : 330
train acc:  0.6171875
train loss:  0.6611127257347107
train gradient:  0.19769765828828492
iteration : 331
train acc:  0.640625
train loss:  0.609063982963562
train gradient:  0.14342476212626656
iteration : 332
train acc:  0.625
train loss:  0.661506712436676
train gradient:  0.1537690721870792
iteration : 333
train acc:  0.640625
train loss:  0.595609188079834
train gradient:  0.16883572779722172
iteration : 334
train acc:  0.703125
train loss:  0.5699998140335083
train gradient:  0.18997200685190274
iteration : 335
train acc:  0.671875
train loss:  0.5642238855361938
train gradient:  0.1890751443615159
iteration : 336
train acc:  0.671875
train loss:  0.6037397980690002
train gradient:  0.1487057364316537
iteration : 337
train acc:  0.546875
train loss:  0.6604151725769043
train gradient:  0.20032255622333806
iteration : 338
train acc:  0.671875
train loss:  0.5839818120002747
train gradient:  0.18781012330306568
iteration : 339
train acc:  0.6171875
train loss:  0.6246662139892578
train gradient:  0.16532113471690868
iteration : 340
train acc:  0.6328125
train loss:  0.6090279817581177
train gradient:  0.17951650032103972
iteration : 341
train acc:  0.6640625
train loss:  0.5947308540344238
train gradient:  0.20289671829492323
iteration : 342
train acc:  0.6328125
train loss:  0.6258316040039062
train gradient:  0.15220607990790858
iteration : 343
train acc:  0.671875
train loss:  0.5938476324081421
train gradient:  0.1288326429676991
iteration : 344
train acc:  0.625
train loss:  0.6236686110496521
train gradient:  0.1934240069178833
iteration : 345
train acc:  0.640625
train loss:  0.6445401906967163
train gradient:  0.17591134611615367
iteration : 346
train acc:  0.609375
train loss:  0.624992847442627
train gradient:  0.22416038702211538
iteration : 347
train acc:  0.71875
train loss:  0.5343034267425537
train gradient:  0.13572931344024627
iteration : 348
train acc:  0.65625
train loss:  0.5860627889633179
train gradient:  0.1493684792299156
iteration : 349
train acc:  0.6953125
train loss:  0.5875449180603027
train gradient:  0.15997647896090172
iteration : 350
train acc:  0.578125
train loss:  0.6115500330924988
train gradient:  0.1373221906548153
iteration : 351
train acc:  0.6953125
train loss:  0.5694999694824219
train gradient:  0.14776057484797772
iteration : 352
train acc:  0.6640625
train loss:  0.6030819416046143
train gradient:  0.17213040943034918
iteration : 353
train acc:  0.6328125
train loss:  0.6415717005729675
train gradient:  0.1828899106519048
iteration : 354
train acc:  0.6875
train loss:  0.5845751762390137
train gradient:  0.14818950208530454
iteration : 355
train acc:  0.7578125
train loss:  0.5488736629486084
train gradient:  0.22001817856144196
iteration : 356
train acc:  0.6640625
train loss:  0.6083890795707703
train gradient:  0.1934486216631507
iteration : 357
train acc:  0.6875
train loss:  0.5936136245727539
train gradient:  0.2118766789731701
iteration : 358
train acc:  0.6484375
train loss:  0.6370899677276611
train gradient:  0.36058361215344414
iteration : 359
train acc:  0.6953125
train loss:  0.5856037139892578
train gradient:  0.16693465474817437
iteration : 360
train acc:  0.6171875
train loss:  0.5965929627418518
train gradient:  0.20922119057421878
iteration : 361
train acc:  0.640625
train loss:  0.6061131954193115
train gradient:  0.15816542663745733
iteration : 362
train acc:  0.703125
train loss:  0.5904066562652588
train gradient:  0.1372194724635364
iteration : 363
train acc:  0.6640625
train loss:  0.5980814099311829
train gradient:  0.15262847469457858
iteration : 364
train acc:  0.734375
train loss:  0.5870643854141235
train gradient:  0.19118602978668933
iteration : 365
train acc:  0.6640625
train loss:  0.5853145122528076
train gradient:  0.1355247881001891
iteration : 366
train acc:  0.609375
train loss:  0.6476560831069946
train gradient:  0.1570849923367541
iteration : 367
train acc:  0.59375
train loss:  0.6518936157226562
train gradient:  0.20059985338804107
iteration : 368
train acc:  0.5703125
train loss:  0.6564288139343262
train gradient:  0.18031790144893406
iteration : 369
train acc:  0.671875
train loss:  0.6241475343704224
train gradient:  0.20571217919333085
iteration : 370
train acc:  0.6640625
train loss:  0.5820093154907227
train gradient:  0.15770262078127062
iteration : 371
train acc:  0.640625
train loss:  0.5748538970947266
train gradient:  0.21505840972151025
iteration : 372
train acc:  0.71875
train loss:  0.559851884841919
train gradient:  0.1404634409681083
iteration : 373
train acc:  0.6875
train loss:  0.5933623313903809
train gradient:  0.14372022891041378
iteration : 374
train acc:  0.671875
train loss:  0.5716406106948853
train gradient:  0.1579880846000092
iteration : 375
train acc:  0.6640625
train loss:  0.5901744365692139
train gradient:  0.15349599396565594
iteration : 376
train acc:  0.625
train loss:  0.5981068015098572
train gradient:  0.1427712602144881
iteration : 377
train acc:  0.6875
train loss:  0.583619236946106
train gradient:  0.1261037554869448
iteration : 378
train acc:  0.609375
train loss:  0.600088357925415
train gradient:  0.1512074761512863
iteration : 379
train acc:  0.71875
train loss:  0.5504583716392517
train gradient:  0.16592575148387612
iteration : 380
train acc:  0.65625
train loss:  0.5692222118377686
train gradient:  0.14136037733141044
iteration : 381
train acc:  0.59375
train loss:  0.6173636317253113
train gradient:  0.17235077855541497
iteration : 382
train acc:  0.7109375
train loss:  0.5839114785194397
train gradient:  0.14208098783346154
iteration : 383
train acc:  0.6328125
train loss:  0.604045569896698
train gradient:  0.15787186540812198
iteration : 384
train acc:  0.6484375
train loss:  0.5840175151824951
train gradient:  0.13974461601887095
iteration : 385
train acc:  0.65625
train loss:  0.5798044204711914
train gradient:  0.21532675162519402
iteration : 386
train acc:  0.640625
train loss:  0.633041501045227
train gradient:  0.1700708743595413
iteration : 387
train acc:  0.6484375
train loss:  0.5710663795471191
train gradient:  0.12865484624806678
iteration : 388
train acc:  0.671875
train loss:  0.587568461894989
train gradient:  0.19778486168503673
iteration : 389
train acc:  0.703125
train loss:  0.5440589189529419
train gradient:  0.1439154648134866
iteration : 390
train acc:  0.609375
train loss:  0.5841784477233887
train gradient:  0.13026842148073714
iteration : 391
train acc:  0.6796875
train loss:  0.5623520016670227
train gradient:  0.11567217902250997
iteration : 392
train acc:  0.6875
train loss:  0.5732590556144714
train gradient:  0.15485049089528735
iteration : 393
train acc:  0.7109375
train loss:  0.5468881130218506
train gradient:  0.11944136025355229
iteration : 394
train acc:  0.59375
train loss:  0.6664354801177979
train gradient:  0.29169054410569867
iteration : 395
train acc:  0.671875
train loss:  0.5950589179992676
train gradient:  0.1582448975322798
iteration : 396
train acc:  0.671875
train loss:  0.6195402145385742
train gradient:  0.2066271040956614
iteration : 397
train acc:  0.5859375
train loss:  0.6283383369445801
train gradient:  0.1672362322930237
iteration : 398
train acc:  0.6875
train loss:  0.5565133690834045
train gradient:  0.1459881373089184
iteration : 399
train acc:  0.7578125
train loss:  0.5273287296295166
train gradient:  0.15939743691324332
iteration : 400
train acc:  0.6328125
train loss:  0.6166213750839233
train gradient:  0.15542529952269835
iteration : 401
train acc:  0.671875
train loss:  0.5839875936508179
train gradient:  0.16277573629611522
iteration : 402
train acc:  0.640625
train loss:  0.6092661023139954
train gradient:  0.14175949113868866
iteration : 403
train acc:  0.6875
train loss:  0.6052008867263794
train gradient:  0.1737868363220914
iteration : 404
train acc:  0.640625
train loss:  0.6113002300262451
train gradient:  0.14172194580947145
iteration : 405
train acc:  0.6328125
train loss:  0.6106052398681641
train gradient:  0.15724091694143638
iteration : 406
train acc:  0.6875
train loss:  0.5738835334777832
train gradient:  0.14469500321027084
iteration : 407
train acc:  0.59375
train loss:  0.6583608388900757
train gradient:  0.1802586326942066
iteration : 408
train acc:  0.6640625
train loss:  0.5822126865386963
train gradient:  0.2235403305489805
iteration : 409
train acc:  0.6640625
train loss:  0.5740374326705933
train gradient:  0.1359605509131795
iteration : 410
train acc:  0.6484375
train loss:  0.6365795135498047
train gradient:  0.2508367983096874
iteration : 411
train acc:  0.671875
train loss:  0.5727108120918274
train gradient:  0.15234744365894845
iteration : 412
train acc:  0.6484375
train loss:  0.595633864402771
train gradient:  0.1908932778612349
iteration : 413
train acc:  0.6640625
train loss:  0.6039443016052246
train gradient:  0.14603438526585683
iteration : 414
train acc:  0.578125
train loss:  0.6367044448852539
train gradient:  0.18552084113429784
iteration : 415
train acc:  0.640625
train loss:  0.6107545495033264
train gradient:  0.19808407150928747
iteration : 416
train acc:  0.6796875
train loss:  0.6027044057846069
train gradient:  0.14243084002022696
iteration : 417
train acc:  0.7578125
train loss:  0.5212675333023071
train gradient:  0.20473080971294683
iteration : 418
train acc:  0.6015625
train loss:  0.6330162286758423
train gradient:  0.19199657324127517
iteration : 419
train acc:  0.625
train loss:  0.6833568215370178
train gradient:  0.28178863845909263
iteration : 420
train acc:  0.7109375
train loss:  0.5568084120750427
train gradient:  0.18044943084571724
iteration : 421
train acc:  0.671875
train loss:  0.6171263456344604
train gradient:  0.18385146724645002
iteration : 422
train acc:  0.703125
train loss:  0.6091365814208984
train gradient:  0.16079660178599334
iteration : 423
train acc:  0.609375
train loss:  0.6405702829360962
train gradient:  0.260382554400969
iteration : 424
train acc:  0.59375
train loss:  0.6469869613647461
train gradient:  0.185727866851073
iteration : 425
train acc:  0.65625
train loss:  0.6182636618614197
train gradient:  0.15734229951463974
iteration : 426
train acc:  0.6328125
train loss:  0.5897496938705444
train gradient:  0.14851030952862043
iteration : 427
train acc:  0.5625
train loss:  0.6548270583152771
train gradient:  0.1692267568495058
iteration : 428
train acc:  0.7109375
train loss:  0.5500175952911377
train gradient:  0.14141875181585029
iteration : 429
train acc:  0.6796875
train loss:  0.5613265037536621
train gradient:  0.15265257707104435
iteration : 430
train acc:  0.6484375
train loss:  0.5958912968635559
train gradient:  0.16550800978631028
iteration : 431
train acc:  0.65625
train loss:  0.5885772705078125
train gradient:  0.40543163408256544
iteration : 432
train acc:  0.671875
train loss:  0.5750387907028198
train gradient:  0.20445974323506455
iteration : 433
train acc:  0.609375
train loss:  0.6184190511703491
train gradient:  0.22854109555420854
iteration : 434
train acc:  0.703125
train loss:  0.5770502090454102
train gradient:  0.16133513666545274
iteration : 435
train acc:  0.6796875
train loss:  0.5913888812065125
train gradient:  0.20715205029890704
iteration : 436
train acc:  0.6328125
train loss:  0.6120913624763489
train gradient:  0.1728402227151956
iteration : 437
train acc:  0.6796875
train loss:  0.5806411504745483
train gradient:  0.14821266200527294
iteration : 438
train acc:  0.7265625
train loss:  0.5561139583587646
train gradient:  0.13742645636020578
iteration : 439
train acc:  0.640625
train loss:  0.6320427656173706
train gradient:  0.22411277015636494
iteration : 440
train acc:  0.6640625
train loss:  0.6025816798210144
train gradient:  0.14377435288897744
iteration : 441
train acc:  0.6328125
train loss:  0.6010759472846985
train gradient:  0.16839976765492715
iteration : 442
train acc:  0.703125
train loss:  0.5616441369056702
train gradient:  0.1839521781868147
iteration : 443
train acc:  0.7109375
train loss:  0.5693758726119995
train gradient:  0.1532807186251493
iteration : 444
train acc:  0.703125
train loss:  0.5807552337646484
train gradient:  0.13312589379453266
iteration : 445
train acc:  0.6171875
train loss:  0.656166136264801
train gradient:  0.2233707145116695
iteration : 446
train acc:  0.671875
train loss:  0.5928833484649658
train gradient:  0.1608041176349334
iteration : 447
train acc:  0.671875
train loss:  0.6205666661262512
train gradient:  0.1901225459375876
iteration : 448
train acc:  0.6796875
train loss:  0.5675770044326782
train gradient:  0.1373001012401309
iteration : 449
train acc:  0.7578125
train loss:  0.5150859355926514
train gradient:  0.19469811920778002
iteration : 450
train acc:  0.6796875
train loss:  0.5797356367111206
train gradient:  0.16392698494681945
iteration : 451
train acc:  0.765625
train loss:  0.5336635708808899
train gradient:  0.13662736099514106
iteration : 452
train acc:  0.671875
train loss:  0.6093828678131104
train gradient:  0.1734579749811444
iteration : 453
train acc:  0.703125
train loss:  0.5716540813446045
train gradient:  0.1224947903067471
iteration : 454
train acc:  0.5546875
train loss:  0.6644772887229919
train gradient:  0.19421860318583767
iteration : 455
train acc:  0.6640625
train loss:  0.6442445516586304
train gradient:  0.18511621847233706
iteration : 456
train acc:  0.7109375
train loss:  0.5739028453826904
train gradient:  0.1901459080740685
iteration : 457
train acc:  0.671875
train loss:  0.6180201768875122
train gradient:  0.139965189189282
iteration : 458
train acc:  0.6484375
train loss:  0.63752281665802
train gradient:  0.16030636175563312
iteration : 459
train acc:  0.71875
train loss:  0.5674327611923218
train gradient:  0.1369480508113256
iteration : 460
train acc:  0.71875
train loss:  0.5999903082847595
train gradient:  0.14660976697003952
iteration : 461
train acc:  0.6484375
train loss:  0.5929586887359619
train gradient:  0.12338294714953997
iteration : 462
train acc:  0.6640625
train loss:  0.5973033905029297
train gradient:  0.14145017396649154
iteration : 463
train acc:  0.6640625
train loss:  0.5960664749145508
train gradient:  0.17549846056110696
iteration : 464
train acc:  0.6640625
train loss:  0.6118345260620117
train gradient:  0.11730778162278305
iteration : 465
train acc:  0.78125
train loss:  0.49052995443344116
train gradient:  0.16051905127867483
iteration : 466
train acc:  0.6796875
train loss:  0.5999218821525574
train gradient:  0.1892940796293317
iteration : 467
train acc:  0.6796875
train loss:  0.5920802354812622
train gradient:  0.1830010552446461
iteration : 468
train acc:  0.6796875
train loss:  0.5768256783485413
train gradient:  0.13270782502207723
iteration : 469
train acc:  0.6953125
train loss:  0.5608044862747192
train gradient:  0.19490957289789934
iteration : 470
train acc:  0.6796875
train loss:  0.5781299471855164
train gradient:  0.13481336675133745
iteration : 471
train acc:  0.640625
train loss:  0.6399542689323425
train gradient:  0.18587299298686005
iteration : 472
train acc:  0.6640625
train loss:  0.5860337018966675
train gradient:  0.13564791451729302
iteration : 473
train acc:  0.671875
train loss:  0.5835744142532349
train gradient:  0.1352899734402544
iteration : 474
train acc:  0.625
train loss:  0.5806697607040405
train gradient:  0.12907278304204456
iteration : 475
train acc:  0.734375
train loss:  0.5079617500305176
train gradient:  0.12795806010757255
iteration : 476
train acc:  0.6171875
train loss:  0.6370456218719482
train gradient:  0.21971006947021218
iteration : 477
train acc:  0.6875
train loss:  0.5766999125480652
train gradient:  0.15921455285709835
iteration : 478
train acc:  0.703125
train loss:  0.5920006036758423
train gradient:  0.14011748974484778
iteration : 479
train acc:  0.65625
train loss:  0.629518985748291
train gradient:  0.194491926243763
iteration : 480
train acc:  0.7109375
train loss:  0.5473732352256775
train gradient:  0.14458676358844377
iteration : 481
train acc:  0.671875
train loss:  0.5879634022712708
train gradient:  0.15872149054501944
iteration : 482
train acc:  0.6796875
train loss:  0.5772403478622437
train gradient:  0.14747749427768458
iteration : 483
train acc:  0.671875
train loss:  0.5852209329605103
train gradient:  0.18107732380300595
iteration : 484
train acc:  0.6640625
train loss:  0.5748625993728638
train gradient:  0.12031814207127767
iteration : 485
train acc:  0.6328125
train loss:  0.6278504133224487
train gradient:  0.22636227175486348
iteration : 486
train acc:  0.6796875
train loss:  0.573798418045044
train gradient:  0.162319652314044
iteration : 487
train acc:  0.6640625
train loss:  0.5979028344154358
train gradient:  0.17231488740467904
iteration : 488
train acc:  0.6640625
train loss:  0.6062684059143066
train gradient:  0.22575900580245212
iteration : 489
train acc:  0.6640625
train loss:  0.602581262588501
train gradient:  0.17569524475033768
iteration : 490
train acc:  0.7734375
train loss:  0.512856125831604
train gradient:  0.1529296332200489
iteration : 491
train acc:  0.6328125
train loss:  0.6286367177963257
train gradient:  0.14381523286820924
iteration : 492
train acc:  0.65625
train loss:  0.563728928565979
train gradient:  0.17648250390114867
iteration : 493
train acc:  0.6953125
train loss:  0.5693577527999878
train gradient:  0.1789951847631418
iteration : 494
train acc:  0.6875
train loss:  0.6212947964668274
train gradient:  0.17949314081309548
iteration : 495
train acc:  0.6953125
train loss:  0.5739500522613525
train gradient:  0.1694608587232207
iteration : 496
train acc:  0.6015625
train loss:  0.6121132969856262
train gradient:  0.1788118287745607
iteration : 497
train acc:  0.75
train loss:  0.5341947674751282
train gradient:  0.1498431441650989
iteration : 498
train acc:  0.625
train loss:  0.6410449743270874
train gradient:  0.15737892638587708
iteration : 499
train acc:  0.71875
train loss:  0.553873598575592
train gradient:  0.1329864711863089
iteration : 500
train acc:  0.640625
train loss:  0.6312928199768066
train gradient:  0.17370786576181058
iteration : 501
train acc:  0.7109375
train loss:  0.5826689600944519
train gradient:  0.18170481937257832
iteration : 502
train acc:  0.6953125
train loss:  0.5898966789245605
train gradient:  0.17396388228187729
iteration : 503
train acc:  0.625
train loss:  0.6002641320228577
train gradient:  0.15755768146014704
iteration : 504
train acc:  0.6796875
train loss:  0.5565794706344604
train gradient:  0.1631188726405658
iteration : 505
train acc:  0.6640625
train loss:  0.5971755385398865
train gradient:  0.14514340326206426
iteration : 506
train acc:  0.6328125
train loss:  0.6062743663787842
train gradient:  0.17734142919172194
iteration : 507
train acc:  0.625
train loss:  0.5778324604034424
train gradient:  0.17267789160080022
iteration : 508
train acc:  0.7421875
train loss:  0.4943441152572632
train gradient:  0.13702367460623177
iteration : 509
train acc:  0.625
train loss:  0.6105762124061584
train gradient:  0.169117252546936
iteration : 510
train acc:  0.71875
train loss:  0.5727354288101196
train gradient:  0.13236251089237075
iteration : 511
train acc:  0.59375
train loss:  0.6189398765563965
train gradient:  0.16334495487402276
iteration : 512
train acc:  0.6796875
train loss:  0.5619596838951111
train gradient:  0.11555959169086846
iteration : 513
train acc:  0.6875
train loss:  0.5848304629325867
train gradient:  0.13003442025848716
iteration : 514
train acc:  0.6953125
train loss:  0.5994794368743896
train gradient:  0.22263554242308828
iteration : 515
train acc:  0.6640625
train loss:  0.5813602805137634
train gradient:  0.15506194184414548
iteration : 516
train acc:  0.65625
train loss:  0.5983181595802307
train gradient:  0.19184111959912473
iteration : 517
train acc:  0.578125
train loss:  0.6196535229682922
train gradient:  0.2394197423050955
iteration : 518
train acc:  0.6953125
train loss:  0.5648319721221924
train gradient:  0.20003625187201446
iteration : 519
train acc:  0.6953125
train loss:  0.5731067657470703
train gradient:  0.19308477021652418
iteration : 520
train acc:  0.6171875
train loss:  0.6327715516090393
train gradient:  0.18678658176414412
iteration : 521
train acc:  0.6953125
train loss:  0.5631579160690308
train gradient:  0.15117384667304956
iteration : 522
train acc:  0.71875
train loss:  0.5553563833236694
train gradient:  0.13853103096360364
iteration : 523
train acc:  0.6875
train loss:  0.5797665119171143
train gradient:  0.14965723652598156
iteration : 524
train acc:  0.765625
train loss:  0.5494377613067627
train gradient:  0.17647789807952585
iteration : 525
train acc:  0.7421875
train loss:  0.5322754383087158
train gradient:  0.11489462154290647
iteration : 526
train acc:  0.6484375
train loss:  0.5624672770500183
train gradient:  0.12296354029645903
iteration : 527
train acc:  0.6796875
train loss:  0.5637099742889404
train gradient:  0.1681666240346169
iteration : 528
train acc:  0.65625
train loss:  0.5852485299110413
train gradient:  0.19164039816102468
iteration : 529
train acc:  0.6484375
train loss:  0.6243054866790771
train gradient:  0.17412983311091207
iteration : 530
train acc:  0.6640625
train loss:  0.5890340805053711
train gradient:  0.21028002443449983
iteration : 531
train acc:  0.7265625
train loss:  0.5686299800872803
train gradient:  0.12404323606202887
iteration : 532
train acc:  0.6796875
train loss:  0.5681582093238831
train gradient:  0.2079394821220807
iteration : 533
train acc:  0.703125
train loss:  0.5591304898262024
train gradient:  0.14875677808842402
iteration : 534
train acc:  0.671875
train loss:  0.5932444930076599
train gradient:  0.1370304455227531
iteration : 535
train acc:  0.6953125
train loss:  0.5844312906265259
train gradient:  0.14511405440533984
iteration : 536
train acc:  0.7421875
train loss:  0.5515621900558472
train gradient:  0.20126204592850105
iteration : 537
train acc:  0.578125
train loss:  0.6797654628753662
train gradient:  0.2645212997728623
iteration : 538
train acc:  0.703125
train loss:  0.5648002624511719
train gradient:  0.15098800098257129
iteration : 539
train acc:  0.6953125
train loss:  0.6278637647628784
train gradient:  0.1540793923503639
iteration : 540
train acc:  0.6875
train loss:  0.5819507241249084
train gradient:  0.18420498889193185
iteration : 541
train acc:  0.6953125
train loss:  0.5767253637313843
train gradient:  0.12241607108291765
iteration : 542
train acc:  0.6328125
train loss:  0.6240072250366211
train gradient:  0.220836795208322
iteration : 543
train acc:  0.6953125
train loss:  0.5636807680130005
train gradient:  0.14681368851713378
iteration : 544
train acc:  0.671875
train loss:  0.6059597134590149
train gradient:  0.1545744472564337
iteration : 545
train acc:  0.7265625
train loss:  0.5457959175109863
train gradient:  0.11162519186766558
iteration : 546
train acc:  0.6484375
train loss:  0.5873924493789673
train gradient:  0.1663852637541988
iteration : 547
train acc:  0.6171875
train loss:  0.6003930568695068
train gradient:  0.1599404354461909
iteration : 548
train acc:  0.6484375
train loss:  0.5951335430145264
train gradient:  0.1948834355188993
iteration : 549
train acc:  0.7421875
train loss:  0.521572470664978
train gradient:  0.1236522578999288
iteration : 550
train acc:  0.7109375
train loss:  0.528746485710144
train gradient:  0.13383095358405453
iteration : 551
train acc:  0.71875
train loss:  0.5775818824768066
train gradient:  0.1655951328863856
iteration : 552
train acc:  0.640625
train loss:  0.5938432216644287
train gradient:  0.16634152319518483
iteration : 553
train acc:  0.6171875
train loss:  0.6000171899795532
train gradient:  0.1727618247899233
iteration : 554
train acc:  0.640625
train loss:  0.6318301558494568
train gradient:  0.17047482870470204
iteration : 555
train acc:  0.6640625
train loss:  0.5914114117622375
train gradient:  0.13829097158540626
iteration : 556
train acc:  0.6796875
train loss:  0.5650287866592407
train gradient:  0.2738698062257029
iteration : 557
train acc:  0.6328125
train loss:  0.6237889528274536
train gradient:  0.15288959689176718
iteration : 558
train acc:  0.6796875
train loss:  0.6091271638870239
train gradient:  0.16241637230582195
iteration : 559
train acc:  0.6875
train loss:  0.5634636282920837
train gradient:  0.16165936151822327
iteration : 560
train acc:  0.6953125
train loss:  0.597434401512146
train gradient:  0.15519394808695647
iteration : 561
train acc:  0.7109375
train loss:  0.5695501565933228
train gradient:  0.1486112511042041
iteration : 562
train acc:  0.6796875
train loss:  0.6206634640693665
train gradient:  0.16892008854653462
iteration : 563
train acc:  0.640625
train loss:  0.5888432264328003
train gradient:  0.1662850242456817
iteration : 564
train acc:  0.671875
train loss:  0.6180561780929565
train gradient:  0.18202945414024596
iteration : 565
train acc:  0.578125
train loss:  0.6622787714004517
train gradient:  0.22836632002506257
iteration : 566
train acc:  0.7109375
train loss:  0.5967785120010376
train gradient:  0.1404704486383896
iteration : 567
train acc:  0.7265625
train loss:  0.5719238519668579
train gradient:  0.21084088062424888
iteration : 568
train acc:  0.7578125
train loss:  0.5300285816192627
train gradient:  0.14399611153459346
iteration : 569
train acc:  0.7109375
train loss:  0.5694557428359985
train gradient:  0.12448865652493793
iteration : 570
train acc:  0.6640625
train loss:  0.61266028881073
train gradient:  0.16986005141872323
iteration : 571
train acc:  0.6953125
train loss:  0.5830682516098022
train gradient:  0.1395381974668652
iteration : 572
train acc:  0.6484375
train loss:  0.6313540935516357
train gradient:  0.1984281637246284
iteration : 573
train acc:  0.6953125
train loss:  0.5891469120979309
train gradient:  0.16374829466228147
iteration : 574
train acc:  0.703125
train loss:  0.5600796341896057
train gradient:  0.14675463938552358
iteration : 575
train acc:  0.6484375
train loss:  0.6163172721862793
train gradient:  0.2678699856139909
iteration : 576
train acc:  0.5859375
train loss:  0.666221559047699
train gradient:  0.30480147044475114
iteration : 577
train acc:  0.6328125
train loss:  0.6421822309494019
train gradient:  0.19279396775946472
iteration : 578
train acc:  0.6953125
train loss:  0.5663493871688843
train gradient:  0.1578895719192266
iteration : 579
train acc:  0.6328125
train loss:  0.5990564823150635
train gradient:  0.13731844983594288
iteration : 580
train acc:  0.6640625
train loss:  0.5781509280204773
train gradient:  0.1553548700230276
iteration : 581
train acc:  0.6015625
train loss:  0.6486285924911499
train gradient:  0.17620500550398974
iteration : 582
train acc:  0.625
train loss:  0.6072070598602295
train gradient:  0.19548949955178885
iteration : 583
train acc:  0.6171875
train loss:  0.667411744594574
train gradient:  0.23285626402830095
iteration : 584
train acc:  0.6796875
train loss:  0.5748191475868225
train gradient:  0.15324451899204328
iteration : 585
train acc:  0.703125
train loss:  0.584736704826355
train gradient:  0.14201683984815733
iteration : 586
train acc:  0.6953125
train loss:  0.587179958820343
train gradient:  0.15231739098083147
iteration : 587
train acc:  0.6796875
train loss:  0.5806224346160889
train gradient:  0.14025908074886437
iteration : 588
train acc:  0.7578125
train loss:  0.5141110420227051
train gradient:  0.19577669573474496
iteration : 589
train acc:  0.6875
train loss:  0.5810246467590332
train gradient:  0.16857900434559842
iteration : 590
train acc:  0.6796875
train loss:  0.554124116897583
train gradient:  0.12321516360839119
iteration : 591
train acc:  0.6640625
train loss:  0.5843634009361267
train gradient:  0.186676708636829
iteration : 592
train acc:  0.6328125
train loss:  0.6220952272415161
train gradient:  0.17566790083983813
iteration : 593
train acc:  0.6484375
train loss:  0.6238304376602173
train gradient:  0.14431272326586925
iteration : 594
train acc:  0.734375
train loss:  0.5580641627311707
train gradient:  0.15084572135564867
iteration : 595
train acc:  0.703125
train loss:  0.5824804306030273
train gradient:  0.19776718904817803
iteration : 596
train acc:  0.6328125
train loss:  0.6190356016159058
train gradient:  0.19844588130202867
iteration : 597
train acc:  0.7421875
train loss:  0.5478954315185547
train gradient:  0.16392370623521224
iteration : 598
train acc:  0.6484375
train loss:  0.6046862602233887
train gradient:  0.15218117669413658
iteration : 599
train acc:  0.7265625
train loss:  0.5460742712020874
train gradient:  0.19565962427420241
iteration : 600
train acc:  0.7109375
train loss:  0.5975492596626282
train gradient:  0.22905227244037335
iteration : 601
train acc:  0.6953125
train loss:  0.5702757835388184
train gradient:  0.1633714426177431
iteration : 602
train acc:  0.671875
train loss:  0.6202490925788879
train gradient:  0.14916229637349748
iteration : 603
train acc:  0.625
train loss:  0.631630539894104
train gradient:  0.14457837747500313
iteration : 604
train acc:  0.609375
train loss:  0.6229017376899719
train gradient:  0.18968492949869042
iteration : 605
train acc:  0.625
train loss:  0.6133981347084045
train gradient:  0.19261221164692804
iteration : 606
train acc:  0.640625
train loss:  0.603659451007843
train gradient:  0.23364455199221124
iteration : 607
train acc:  0.6875
train loss:  0.5868183374404907
train gradient:  0.18752559960656953
iteration : 608
train acc:  0.6796875
train loss:  0.5870649218559265
train gradient:  0.174607826344471
iteration : 609
train acc:  0.6953125
train loss:  0.5741541981697083
train gradient:  0.11177349969471283
iteration : 610
train acc:  0.6953125
train loss:  0.5625539422035217
train gradient:  0.17764127849188938
iteration : 611
train acc:  0.640625
train loss:  0.5954486727714539
train gradient:  0.17491757279062387
iteration : 612
train acc:  0.6953125
train loss:  0.6082491874694824
train gradient:  0.15047623208818706
iteration : 613
train acc:  0.6796875
train loss:  0.6157018542289734
train gradient:  0.13168521687474807
iteration : 614
train acc:  0.671875
train loss:  0.5899870991706848
train gradient:  0.16803334043610657
iteration : 615
train acc:  0.7109375
train loss:  0.586605966091156
train gradient:  0.16164965291803776
iteration : 616
train acc:  0.6484375
train loss:  0.6149500012397766
train gradient:  0.17800614801827747
iteration : 617
train acc:  0.6484375
train loss:  0.5952216386795044
train gradient:  0.13982548230847217
iteration : 618
train acc:  0.640625
train loss:  0.5814260244369507
train gradient:  0.26010223937714017
iteration : 619
train acc:  0.6796875
train loss:  0.5838197469711304
train gradient:  0.14571121571884998
iteration : 620
train acc:  0.7109375
train loss:  0.5665536522865295
train gradient:  0.136312933360997
iteration : 621
train acc:  0.640625
train loss:  0.6179472208023071
train gradient:  0.19803027159340042
iteration : 622
train acc:  0.6875
train loss:  0.604301929473877
train gradient:  0.17695199576855858
iteration : 623
train acc:  0.703125
train loss:  0.5812686085700989
train gradient:  0.22325738466565703
iteration : 624
train acc:  0.703125
train loss:  0.5515487194061279
train gradient:  0.14929472445704578
iteration : 625
train acc:  0.625
train loss:  0.6270995140075684
train gradient:  0.17639068649172607
iteration : 626
train acc:  0.6953125
train loss:  0.5781457424163818
train gradient:  0.1669732804581106
iteration : 627
train acc:  0.6796875
train loss:  0.5983483195304871
train gradient:  0.20045353092006543
iteration : 628
train acc:  0.5859375
train loss:  0.6244972348213196
train gradient:  0.15002611496013382
iteration : 629
train acc:  0.6171875
train loss:  0.6328721046447754
train gradient:  0.19167015093007328
iteration : 630
train acc:  0.7578125
train loss:  0.5304582118988037
train gradient:  0.1224793672348132
iteration : 631
train acc:  0.71875
train loss:  0.5683658123016357
train gradient:  0.13487330890728705
iteration : 632
train acc:  0.71875
train loss:  0.5641250610351562
train gradient:  0.17019329483520262
iteration : 633
train acc:  0.7265625
train loss:  0.5797576308250427
train gradient:  0.15406439680832412
iteration : 634
train acc:  0.7421875
train loss:  0.5308221578598022
train gradient:  0.13240806983708597
iteration : 635
train acc:  0.703125
train loss:  0.5827139019966125
train gradient:  0.11580427311382677
iteration : 636
train acc:  0.65625
train loss:  0.5862048864364624
train gradient:  0.187288510556686
iteration : 637
train acc:  0.6484375
train loss:  0.5846951603889465
train gradient:  0.15403245367452817
iteration : 638
train acc:  0.6640625
train loss:  0.5701714158058167
train gradient:  0.1853502734783433
iteration : 639
train acc:  0.6875
train loss:  0.616118848323822
train gradient:  0.2034562789638395
iteration : 640
train acc:  0.609375
train loss:  0.5823285579681396
train gradient:  0.1701995365695158
iteration : 641
train acc:  0.703125
train loss:  0.5610449910163879
train gradient:  0.14652446137421316
iteration : 642
train acc:  0.671875
train loss:  0.6165581345558167
train gradient:  0.1606098265852982
iteration : 643
train acc:  0.7109375
train loss:  0.59450364112854
train gradient:  0.13113059982882946
iteration : 644
train acc:  0.6328125
train loss:  0.6563472151756287
train gradient:  0.23945788825118314
iteration : 645
train acc:  0.65625
train loss:  0.6203694939613342
train gradient:  0.20537516246677928
iteration : 646
train acc:  0.6953125
train loss:  0.5782185196876526
train gradient:  0.14455557284784254
iteration : 647
train acc:  0.6796875
train loss:  0.5868174433708191
train gradient:  0.16930833870004922
iteration : 648
train acc:  0.703125
train loss:  0.5610508918762207
train gradient:  0.17268959872104633
iteration : 649
train acc:  0.7421875
train loss:  0.5579208135604858
train gradient:  0.13419557046153063
iteration : 650
train acc:  0.640625
train loss:  0.6373569965362549
train gradient:  0.19992653650651507
iteration : 651
train acc:  0.671875
train loss:  0.628086507320404
train gradient:  0.1612771747572691
iteration : 652
train acc:  0.6875
train loss:  0.5732091069221497
train gradient:  0.13656337317941408
iteration : 653
train acc:  0.6953125
train loss:  0.609119713306427
train gradient:  0.18538386765852455
iteration : 654
train acc:  0.7109375
train loss:  0.5545672178268433
train gradient:  0.15024161925554919
iteration : 655
train acc:  0.625
train loss:  0.6302109360694885
train gradient:  0.20684440931932674
iteration : 656
train acc:  0.75
train loss:  0.5406177043914795
train gradient:  0.1505455505687196
iteration : 657
train acc:  0.71875
train loss:  0.6134086847305298
train gradient:  0.15444679677955697
iteration : 658
train acc:  0.6484375
train loss:  0.6287758350372314
train gradient:  0.16308568618622724
iteration : 659
train acc:  0.6953125
train loss:  0.5890029668807983
train gradient:  0.11787507801830716
iteration : 660
train acc:  0.7265625
train loss:  0.5611788630485535
train gradient:  0.17537131438080814
iteration : 661
train acc:  0.65625
train loss:  0.5652954578399658
train gradient:  0.12726161565145644
iteration : 662
train acc:  0.6328125
train loss:  0.6110869646072388
train gradient:  0.1361816818802154
iteration : 663
train acc:  0.6640625
train loss:  0.6232845783233643
train gradient:  0.22163792272536098
iteration : 664
train acc:  0.671875
train loss:  0.561371386051178
train gradient:  0.12063609385131921
iteration : 665
train acc:  0.6796875
train loss:  0.5732859373092651
train gradient:  0.1284391960762717
iteration : 666
train acc:  0.6484375
train loss:  0.5759321451187134
train gradient:  0.21298423255561566
iteration : 667
train acc:  0.7421875
train loss:  0.5247457027435303
train gradient:  0.1413706385647705
iteration : 668
train acc:  0.703125
train loss:  0.5563368201255798
train gradient:  0.13729001853265155
iteration : 669
train acc:  0.6796875
train loss:  0.5581586360931396
train gradient:  0.12865678673637765
iteration : 670
train acc:  0.7421875
train loss:  0.5169365406036377
train gradient:  0.21414241050280647
iteration : 671
train acc:  0.671875
train loss:  0.5718455910682678
train gradient:  0.14885933614550062
iteration : 672
train acc:  0.703125
train loss:  0.5620975494384766
train gradient:  0.12088575139348341
iteration : 673
train acc:  0.7109375
train loss:  0.5506167411804199
train gradient:  0.30356590868922984
iteration : 674
train acc:  0.6875
train loss:  0.6392079591751099
train gradient:  0.21873908634641154
iteration : 675
train acc:  0.71875
train loss:  0.5905043482780457
train gradient:  0.2301747947743879
iteration : 676
train acc:  0.609375
train loss:  0.6490986943244934
train gradient:  0.15501202885788795
iteration : 677
train acc:  0.7421875
train loss:  0.5540415644645691
train gradient:  0.16464849675219084
iteration : 678
train acc:  0.6953125
train loss:  0.5726765394210815
train gradient:  0.14930857934872765
iteration : 679
train acc:  0.71875
train loss:  0.5375286936759949
train gradient:  0.3196234467853726
iteration : 680
train acc:  0.625
train loss:  0.629647970199585
train gradient:  0.16712517916652136
iteration : 681
train acc:  0.6328125
train loss:  0.6405400633811951
train gradient:  0.21861118205894592
iteration : 682
train acc:  0.671875
train loss:  0.631014347076416
train gradient:  0.14927752335950184
iteration : 683
train acc:  0.6640625
train loss:  0.6124463677406311
train gradient:  0.1626544770340177
iteration : 684
train acc:  0.625
train loss:  0.5789321660995483
train gradient:  0.15175088511761398
iteration : 685
train acc:  0.703125
train loss:  0.5765549540519714
train gradient:  0.14171404870690163
iteration : 686
train acc:  0.7421875
train loss:  0.5451998710632324
train gradient:  0.13606670144400324
iteration : 687
train acc:  0.671875
train loss:  0.5658520460128784
train gradient:  0.2043139128117763
iteration : 688
train acc:  0.734375
train loss:  0.5457670092582703
train gradient:  0.15025714041107988
iteration : 689
train acc:  0.6640625
train loss:  0.5694746971130371
train gradient:  0.15358508134434617
iteration : 690
train acc:  0.703125
train loss:  0.591593861579895
train gradient:  0.21562919873036263
iteration : 691
train acc:  0.6953125
train loss:  0.5428935289382935
train gradient:  0.11766102968101343
iteration : 692
train acc:  0.6328125
train loss:  0.6141749024391174
train gradient:  0.16616676936334981
iteration : 693
train acc:  0.671875
train loss:  0.5848977565765381
train gradient:  0.15521318513147309
iteration : 694
train acc:  0.703125
train loss:  0.5587211847305298
train gradient:  0.13282762001260165
iteration : 695
train acc:  0.765625
train loss:  0.5131721496582031
train gradient:  0.1473766462517681
iteration : 696
train acc:  0.7109375
train loss:  0.5669004917144775
train gradient:  0.1355797320394164
iteration : 697
train acc:  0.6953125
train loss:  0.5536404848098755
train gradient:  0.18176559853087004
iteration : 698
train acc:  0.671875
train loss:  0.5725533962249756
train gradient:  0.1589291872647126
iteration : 699
train acc:  0.6875
train loss:  0.5767467021942139
train gradient:  0.15488154054150405
iteration : 700
train acc:  0.625
train loss:  0.6031070351600647
train gradient:  0.20083962339034886
iteration : 701
train acc:  0.703125
train loss:  0.570205807685852
train gradient:  0.13996540446494257
iteration : 702
train acc:  0.6796875
train loss:  0.6344355344772339
train gradient:  0.16640163893925308
iteration : 703
train acc:  0.6484375
train loss:  0.6308640241622925
train gradient:  0.18645459362045122
iteration : 704
train acc:  0.6953125
train loss:  0.5731537938117981
train gradient:  0.16837436835666486
iteration : 705
train acc:  0.609375
train loss:  0.6278848648071289
train gradient:  0.17299365721849475
iteration : 706
train acc:  0.65625
train loss:  0.6457861065864563
train gradient:  0.21514590235305725
iteration : 707
train acc:  0.6328125
train loss:  0.6273775100708008
train gradient:  0.15455689476596998
iteration : 708
train acc:  0.671875
train loss:  0.5747735500335693
train gradient:  0.1508014919195768
iteration : 709
train acc:  0.6875
train loss:  0.5705122947692871
train gradient:  0.13391264209054
iteration : 710
train acc:  0.7109375
train loss:  0.5631969571113586
train gradient:  0.11955081300269571
iteration : 711
train acc:  0.65625
train loss:  0.5995557308197021
train gradient:  0.16030233626059376
iteration : 712
train acc:  0.6171875
train loss:  0.6658846139907837
train gradient:  0.2115959501437517
iteration : 713
train acc:  0.7421875
train loss:  0.5330450534820557
train gradient:  0.1940142246562651
iteration : 714
train acc:  0.71875
train loss:  0.5612888336181641
train gradient:  0.14178732800768284
iteration : 715
train acc:  0.6015625
train loss:  0.6389199495315552
train gradient:  0.20367746693736824
iteration : 716
train acc:  0.671875
train loss:  0.5834323763847351
train gradient:  0.15135048474703897
iteration : 717
train acc:  0.7421875
train loss:  0.5273386240005493
train gradient:  0.12719565069590916
iteration : 718
train acc:  0.671875
train loss:  0.5633999109268188
train gradient:  0.21100851845480656
iteration : 719
train acc:  0.7109375
train loss:  0.5992011427879333
train gradient:  0.16789770756737185
iteration : 720
train acc:  0.6640625
train loss:  0.6080586910247803
train gradient:  0.13467052645711014
iteration : 721
train acc:  0.609375
train loss:  0.6083298921585083
train gradient:  0.17296223369322677
iteration : 722
train acc:  0.6640625
train loss:  0.5840240120887756
train gradient:  0.1862942163124673
iteration : 723
train acc:  0.640625
train loss:  0.6383956670761108
train gradient:  0.16179371173297621
iteration : 724
train acc:  0.6953125
train loss:  0.565022349357605
train gradient:  0.16967065867783815
iteration : 725
train acc:  0.65625
train loss:  0.5719879269599915
train gradient:  0.16584529547661053
iteration : 726
train acc:  0.6171875
train loss:  0.6302173733711243
train gradient:  0.15945183646911043
iteration : 727
train acc:  0.75
train loss:  0.5423468351364136
train gradient:  0.13982553993809482
iteration : 728
train acc:  0.7109375
train loss:  0.5316550731658936
train gradient:  0.15676947099148358
iteration : 729
train acc:  0.6640625
train loss:  0.5838320851325989
train gradient:  0.18865637165707014
iteration : 730
train acc:  0.703125
train loss:  0.5868866443634033
train gradient:  0.17117237065307706
iteration : 731
train acc:  0.7578125
train loss:  0.5326809883117676
train gradient:  0.12209916643923138
iteration : 732
train acc:  0.5625
train loss:  0.6682493686676025
train gradient:  0.22728711367624532
iteration : 733
train acc:  0.6796875
train loss:  0.6046417951583862
train gradient:  0.147788888632281
iteration : 734
train acc:  0.7421875
train loss:  0.515712559223175
train gradient:  0.18190042448629382
iteration : 735
train acc:  0.640625
train loss:  0.6134375333786011
train gradient:  0.19754229932368897
iteration : 736
train acc:  0.6953125
train loss:  0.5982751846313477
train gradient:  0.2093727572739564
iteration : 737
train acc:  0.71875
train loss:  0.5682648420333862
train gradient:  0.169736225884266
iteration : 738
train acc:  0.6015625
train loss:  0.6155757904052734
train gradient:  0.21310245831138286
iteration : 739
train acc:  0.65625
train loss:  0.606836199760437
train gradient:  0.14370347257458332
iteration : 740
train acc:  0.671875
train loss:  0.5634313821792603
train gradient:  0.14298748784445248
iteration : 741
train acc:  0.671875
train loss:  0.5644655227661133
train gradient:  0.13071412262570814
iteration : 742
train acc:  0.7109375
train loss:  0.5325302481651306
train gradient:  0.1763690970926574
iteration : 743
train acc:  0.6328125
train loss:  0.6523236036300659
train gradient:  0.21283328345166658
iteration : 744
train acc:  0.65625
train loss:  0.625119149684906
train gradient:  0.1834254236142498
iteration : 745
train acc:  0.7109375
train loss:  0.5808312892913818
train gradient:  0.18853482349170234
iteration : 746
train acc:  0.65625
train loss:  0.5770310163497925
train gradient:  0.12571960229017257
iteration : 747
train acc:  0.6484375
train loss:  0.5984051823616028
train gradient:  0.19210225291070682
iteration : 748
train acc:  0.7109375
train loss:  0.5742298364639282
train gradient:  0.131960033368864
iteration : 749
train acc:  0.6640625
train loss:  0.604506254196167
train gradient:  0.15402112317255853
iteration : 750
train acc:  0.6796875
train loss:  0.5795694589614868
train gradient:  0.13455109484313305
iteration : 751
train acc:  0.7421875
train loss:  0.576109766960144
train gradient:  0.12198255442189011
iteration : 752
train acc:  0.625
train loss:  0.6229051351547241
train gradient:  0.1698566800078591
iteration : 753
train acc:  0.6796875
train loss:  0.5696889162063599
train gradient:  0.1339472761901933
iteration : 754
train acc:  0.6953125
train loss:  0.5707854628562927
train gradient:  0.1395042872059534
iteration : 755
train acc:  0.7890625
train loss:  0.5129774808883667
train gradient:  0.15094065564020206
iteration : 756
train acc:  0.703125
train loss:  0.5539678335189819
train gradient:  0.13885724651267128
iteration : 757
train acc:  0.65625
train loss:  0.6009543538093567
train gradient:  0.16343983385941774
iteration : 758
train acc:  0.671875
train loss:  0.6052528619766235
train gradient:  0.16399653235030273
iteration : 759
train acc:  0.640625
train loss:  0.5911986231803894
train gradient:  0.14444939615201152
iteration : 760
train acc:  0.703125
train loss:  0.5844098329544067
train gradient:  0.18815713181360194
iteration : 761
train acc:  0.7578125
train loss:  0.5417802333831787
train gradient:  0.1604606633813352
iteration : 762
train acc:  0.6796875
train loss:  0.5729395151138306
train gradient:  0.2188205173628214
iteration : 763
train acc:  0.7109375
train loss:  0.5670385956764221
train gradient:  0.19263138163500804
iteration : 764
train acc:  0.671875
train loss:  0.5954306125640869
train gradient:  0.13813035322741835
iteration : 765
train acc:  0.6484375
train loss:  0.595965564250946
train gradient:  0.13899216893000518
iteration : 766
train acc:  0.6875
train loss:  0.5760639905929565
train gradient:  0.13129543182572811
iteration : 767
train acc:  0.71875
train loss:  0.5489112138748169
train gradient:  0.15366961378946115
iteration : 768
train acc:  0.7421875
train loss:  0.5293700695037842
train gradient:  0.165128318980966
iteration : 769
train acc:  0.6640625
train loss:  0.6058211922645569
train gradient:  0.1748881133759821
iteration : 770
train acc:  0.71875
train loss:  0.5563538074493408
train gradient:  0.13882847845015084
iteration : 771
train acc:  0.6484375
train loss:  0.6075170040130615
train gradient:  0.196818550937217
iteration : 772
train acc:  0.6484375
train loss:  0.5989702939987183
train gradient:  0.15129450317306614
iteration : 773
train acc:  0.6875
train loss:  0.560451865196228
train gradient:  0.17491018198968789
iteration : 774
train acc:  0.6796875
train loss:  0.56202232837677
train gradient:  0.18424050463900038
iteration : 775
train acc:  0.6640625
train loss:  0.6111010313034058
train gradient:  0.16436510808843677
iteration : 776
train acc:  0.7109375
train loss:  0.5215838551521301
train gradient:  0.1474287837373257
iteration : 777
train acc:  0.6328125
train loss:  0.6107826828956604
train gradient:  0.15960943351779813
iteration : 778
train acc:  0.6953125
train loss:  0.57320237159729
train gradient:  0.10210141859134679
iteration : 779
train acc:  0.6640625
train loss:  0.56830233335495
train gradient:  0.17262512484799558
iteration : 780
train acc:  0.7265625
train loss:  0.5568375587463379
train gradient:  0.11296492244325158
iteration : 781
train acc:  0.703125
train loss:  0.5992602705955505
train gradient:  0.21402840192513103
iteration : 782
train acc:  0.6796875
train loss:  0.5708997845649719
train gradient:  0.1498942573651272
iteration : 783
train acc:  0.703125
train loss:  0.5631051063537598
train gradient:  0.15257992040605545
iteration : 784
train acc:  0.6640625
train loss:  0.5734485387802124
train gradient:  0.17328137551714184
iteration : 785
train acc:  0.734375
train loss:  0.546476423740387
train gradient:  0.13818213374332444
iteration : 786
train acc:  0.7421875
train loss:  0.529571533203125
train gradient:  0.19831823546703875
iteration : 787
train acc:  0.609375
train loss:  0.5973221063613892
train gradient:  0.19382258174660572
iteration : 788
train acc:  0.6953125
train loss:  0.5886544585227966
train gradient:  0.16136239731950214
iteration : 789
train acc:  0.6484375
train loss:  0.5573680400848389
train gradient:  0.17756100476350348
iteration : 790
train acc:  0.7421875
train loss:  0.517663836479187
train gradient:  0.11271022950159962
iteration : 791
train acc:  0.671875
train loss:  0.5844953656196594
train gradient:  0.13248363338217695
iteration : 792
train acc:  0.6640625
train loss:  0.6113827228546143
train gradient:  0.1999397744204185
iteration : 793
train acc:  0.609375
train loss:  0.6598286032676697
train gradient:  0.17957645446618806
iteration : 794
train acc:  0.65625
train loss:  0.6071418523788452
train gradient:  0.20293409911566695
iteration : 795
train acc:  0.734375
train loss:  0.5224646329879761
train gradient:  0.1269372768641256
iteration : 796
train acc:  0.7109375
train loss:  0.5574393272399902
train gradient:  0.15827818133127092
iteration : 797
train acc:  0.6640625
train loss:  0.5772316455841064
train gradient:  0.228178427138135
iteration : 798
train acc:  0.734375
train loss:  0.5217946767807007
train gradient:  0.15214185007031977
iteration : 799
train acc:  0.6171875
train loss:  0.5887752771377563
train gradient:  0.1627927067046594
iteration : 800
train acc:  0.6796875
train loss:  0.5945917367935181
train gradient:  0.14559070989353173
iteration : 801
train acc:  0.7109375
train loss:  0.5249965190887451
train gradient:  0.16249753032056105
iteration : 802
train acc:  0.625
train loss:  0.6389225125312805
train gradient:  0.21758581262092344
iteration : 803
train acc:  0.6796875
train loss:  0.5786776542663574
train gradient:  0.1346410892783983
iteration : 804
train acc:  0.71875
train loss:  0.5164904594421387
train gradient:  0.1366784122335931
iteration : 805
train acc:  0.71875
train loss:  0.5734046101570129
train gradient:  0.15888112197641754
iteration : 806
train acc:  0.671875
train loss:  0.5826022624969482
train gradient:  0.1856550594045855
iteration : 807
train acc:  0.734375
train loss:  0.5590214729309082
train gradient:  0.15573885194140896
iteration : 808
train acc:  0.734375
train loss:  0.5606718063354492
train gradient:  0.19473296722072178
iteration : 809
train acc:  0.734375
train loss:  0.5590274333953857
train gradient:  0.2582025618636561
iteration : 810
train acc:  0.671875
train loss:  0.5849658250808716
train gradient:  0.24739161945339588
iteration : 811
train acc:  0.671875
train loss:  0.5921674966812134
train gradient:  0.13206669948921937
iteration : 812
train acc:  0.6953125
train loss:  0.5718455910682678
train gradient:  0.20951847236445614
iteration : 813
train acc:  0.6953125
train loss:  0.5557379722595215
train gradient:  0.14470943178546453
iteration : 814
train acc:  0.6953125
train loss:  0.6011298894882202
train gradient:  0.16812787736750828
iteration : 815
train acc:  0.6328125
train loss:  0.6190342903137207
train gradient:  0.1634168251807877
iteration : 816
train acc:  0.6328125
train loss:  0.6386231184005737
train gradient:  0.16857858251106764
iteration : 817
train acc:  0.7109375
train loss:  0.5719393491744995
train gradient:  0.17127226297347742
iteration : 818
train acc:  0.6796875
train loss:  0.5694233179092407
train gradient:  0.2108004446682875
iteration : 819
train acc:  0.671875
train loss:  0.5866691470146179
train gradient:  0.14244375533051368
iteration : 820
train acc:  0.640625
train loss:  0.5838613510131836
train gradient:  0.19563503111102942
iteration : 821
train acc:  0.625
train loss:  0.6347415447235107
train gradient:  0.24376030079840094
iteration : 822
train acc:  0.6875
train loss:  0.6084181070327759
train gradient:  0.20789023705903942
iteration : 823
train acc:  0.6640625
train loss:  0.590367317199707
train gradient:  0.1869416021413171
iteration : 824
train acc:  0.6484375
train loss:  0.589478075504303
train gradient:  0.19273349687962074
iteration : 825
train acc:  0.703125
train loss:  0.5649417042732239
train gradient:  0.13346049002604016
iteration : 826
train acc:  0.71875
train loss:  0.5533473491668701
train gradient:  0.14142287610948082
iteration : 827
train acc:  0.625
train loss:  0.6111330986022949
train gradient:  0.19472860962530758
iteration : 828
train acc:  0.7109375
train loss:  0.5851729512214661
train gradient:  0.14834976737574726
iteration : 829
train acc:  0.71875
train loss:  0.5560696721076965
train gradient:  0.16809285192929468
iteration : 830
train acc:  0.6796875
train loss:  0.5393888354301453
train gradient:  0.16923614402979753
iteration : 831
train acc:  0.7421875
train loss:  0.5464956760406494
train gradient:  0.17309157181619983
iteration : 832
train acc:  0.7109375
train loss:  0.5775010585784912
train gradient:  0.1508344735810018
iteration : 833
train acc:  0.6875
train loss:  0.6205507516860962
train gradient:  0.2331371626981871
iteration : 834
train acc:  0.7109375
train loss:  0.5402805805206299
train gradient:  0.14484004642246018
iteration : 835
train acc:  0.6796875
train loss:  0.5737013220787048
train gradient:  0.17139817959442688
iteration : 836
train acc:  0.71875
train loss:  0.6294918656349182
train gradient:  0.18033935319509223
iteration : 837
train acc:  0.7578125
train loss:  0.5142268538475037
train gradient:  0.1285272195768164
iteration : 838
train acc:  0.6796875
train loss:  0.6012359857559204
train gradient:  0.17834428743677372
iteration : 839
train acc:  0.5859375
train loss:  0.6924512386322021
train gradient:  0.2245574494781144
iteration : 840
train acc:  0.6640625
train loss:  0.6026716232299805
train gradient:  0.19049670317545087
iteration : 841
train acc:  0.7265625
train loss:  0.5417520403862
train gradient:  0.16178857356591247
iteration : 842
train acc:  0.6640625
train loss:  0.5580020546913147
train gradient:  0.13721389650387117
iteration : 843
train acc:  0.75
train loss:  0.5176174640655518
train gradient:  0.16717486673347776
iteration : 844
train acc:  0.703125
train loss:  0.5677272081375122
train gradient:  0.17514562413185492
iteration : 845
train acc:  0.6328125
train loss:  0.652102530002594
train gradient:  0.19954229982674518
iteration : 846
train acc:  0.6796875
train loss:  0.5757354497909546
train gradient:  0.1250178398681965
iteration : 847
train acc:  0.703125
train loss:  0.5350419282913208
train gradient:  0.12978476310251008
iteration : 848
train acc:  0.703125
train loss:  0.5147009491920471
train gradient:  0.13994896776337462
iteration : 849
train acc:  0.703125
train loss:  0.5512375235557556
train gradient:  0.15576655695812663
iteration : 850
train acc:  0.7421875
train loss:  0.5021958351135254
train gradient:  0.22586894022359805
iteration : 851
train acc:  0.65625
train loss:  0.5729700922966003
train gradient:  0.15494770561120969
iteration : 852
train acc:  0.6875
train loss:  0.5833512544631958
train gradient:  0.16378908891187127
iteration : 853
train acc:  0.671875
train loss:  0.6033022403717041
train gradient:  0.15359922483904104
iteration : 854
train acc:  0.765625
train loss:  0.5607103109359741
train gradient:  0.23906925486968883
iteration : 855
train acc:  0.640625
train loss:  0.6036242246627808
train gradient:  0.1813506350313013
iteration : 856
train acc:  0.6796875
train loss:  0.555095911026001
train gradient:  0.15763104945098258
iteration : 857
train acc:  0.6796875
train loss:  0.558409571647644
train gradient:  0.15180318755171654
iteration : 858
train acc:  0.7109375
train loss:  0.5659522414207458
train gradient:  0.17758457952917728
iteration : 859
train acc:  0.78125
train loss:  0.47561731934547424
train gradient:  0.16545351029114105
iteration : 860
train acc:  0.6796875
train loss:  0.5662600994110107
train gradient:  0.13834467234423467
iteration : 861
train acc:  0.7265625
train loss:  0.5652329921722412
train gradient:  0.13568292347981373
iteration : 862
train acc:  0.71875
train loss:  0.5415741801261902
train gradient:  0.16713300794878774
iteration : 863
train acc:  0.6484375
train loss:  0.6286196708679199
train gradient:  0.17017492171088164
iteration : 864
train acc:  0.6640625
train loss:  0.6076757311820984
train gradient:  0.23063842299248477
iteration : 865
train acc:  0.71875
train loss:  0.5199490785598755
train gradient:  0.1153566348983085
iteration : 866
train acc:  0.6875
train loss:  0.5735586881637573
train gradient:  0.12523233851500243
iteration : 867
train acc:  0.7109375
train loss:  0.5585653185844421
train gradient:  0.1770334424159134
iteration : 868
train acc:  0.6875
train loss:  0.5801459550857544
train gradient:  0.18185815450070242
iteration : 869
train acc:  0.65625
train loss:  0.5516973733901978
train gradient:  0.12666849669540278
iteration : 870
train acc:  0.71875
train loss:  0.5448548793792725
train gradient:  0.12371971825055929
iteration : 871
train acc:  0.6484375
train loss:  0.6318187117576599
train gradient:  0.19304433739127194
iteration : 872
train acc:  0.71875
train loss:  0.5327223539352417
train gradient:  0.16558015000082066
iteration : 873
train acc:  0.6484375
train loss:  0.5899258852005005
train gradient:  0.18396205547278993
iteration : 874
train acc:  0.6328125
train loss:  0.625320553779602
train gradient:  0.19411874981981642
iteration : 875
train acc:  0.671875
train loss:  0.6143505573272705
train gradient:  0.21063842774572433
iteration : 876
train acc:  0.6484375
train loss:  0.6250964403152466
train gradient:  0.205627499641221
iteration : 877
train acc:  0.6875
train loss:  0.5627413988113403
train gradient:  0.1637009617041785
iteration : 878
train acc:  0.71875
train loss:  0.5634403824806213
train gradient:  0.13043957402945022
iteration : 879
train acc:  0.6875
train loss:  0.5699453353881836
train gradient:  0.1631401766379615
iteration : 880
train acc:  0.640625
train loss:  0.6047645211219788
train gradient:  0.2585892951736618
iteration : 881
train acc:  0.609375
train loss:  0.6043218374252319
train gradient:  0.1765527000520298
iteration : 882
train acc:  0.6640625
train loss:  0.5863264799118042
train gradient:  0.14465184525255756
iteration : 883
train acc:  0.671875
train loss:  0.563176155090332
train gradient:  0.13056827682403782
iteration : 884
train acc:  0.6640625
train loss:  0.6124080419540405
train gradient:  0.24193034739624314
iteration : 885
train acc:  0.65625
train loss:  0.5575969815254211
train gradient:  0.16894840046388498
iteration : 886
train acc:  0.6875
train loss:  0.6069869995117188
train gradient:  0.16370020142989167
iteration : 887
train acc:  0.6015625
train loss:  0.6691421270370483
train gradient:  0.20023281787860803
iteration : 888
train acc:  0.7265625
train loss:  0.5127601623535156
train gradient:  0.1293097697106833
iteration : 889
train acc:  0.6796875
train loss:  0.5982643365859985
train gradient:  0.19691917377799553
iteration : 890
train acc:  0.6953125
train loss:  0.5869700908660889
train gradient:  0.19081254250194807
iteration : 891
train acc:  0.6015625
train loss:  0.6304866075515747
train gradient:  0.17899262398418983
iteration : 892
train acc:  0.6875
train loss:  0.527911365032196
train gradient:  0.12648818430500425
iteration : 893
train acc:  0.7265625
train loss:  0.5540453195571899
train gradient:  0.1663135388461887
iteration : 894
train acc:  0.671875
train loss:  0.6171882152557373
train gradient:  0.238821403636127
iteration : 895
train acc:  0.75
train loss:  0.5438620448112488
train gradient:  0.15642444089614838
iteration : 896
train acc:  0.671875
train loss:  0.5627583265304565
train gradient:  0.12951842211018075
iteration : 897
train acc:  0.734375
train loss:  0.539671003818512
train gradient:  0.1021035855376347
iteration : 898
train acc:  0.7734375
train loss:  0.5141434669494629
train gradient:  0.13856729892449698
iteration : 899
train acc:  0.6640625
train loss:  0.5814764499664307
train gradient:  0.15310442586650377
iteration : 900
train acc:  0.6875
train loss:  0.576715350151062
train gradient:  0.14391853093370588
iteration : 901
train acc:  0.6875
train loss:  0.5685450434684753
train gradient:  0.15954249598740217
iteration : 902
train acc:  0.625
train loss:  0.6274915933609009
train gradient:  0.171855736461445
iteration : 903
train acc:  0.6328125
train loss:  0.6216250658035278
train gradient:  0.16757822222211294
iteration : 904
train acc:  0.671875
train loss:  0.6008206605911255
train gradient:  0.13321146635649816
iteration : 905
train acc:  0.6328125
train loss:  0.5906811952590942
train gradient:  0.21987989113783146
iteration : 906
train acc:  0.75
train loss:  0.5358878374099731
train gradient:  0.1552296715313729
iteration : 907
train acc:  0.6484375
train loss:  0.5934398174285889
train gradient:  0.13298483333566063
iteration : 908
train acc:  0.6796875
train loss:  0.6151241064071655
train gradient:  0.1611051650845338
iteration : 909
train acc:  0.7109375
train loss:  0.5568261742591858
train gradient:  0.14335643780383564
iteration : 910
train acc:  0.6484375
train loss:  0.6151905059814453
train gradient:  0.23357090047108842
iteration : 911
train acc:  0.7265625
train loss:  0.5618448257446289
train gradient:  0.13479174052981363
iteration : 912
train acc:  0.6953125
train loss:  0.596795916557312
train gradient:  0.17158088717736508
iteration : 913
train acc:  0.703125
train loss:  0.5337625741958618
train gradient:  0.12660057469586639
iteration : 914
train acc:  0.6796875
train loss:  0.6019296050071716
train gradient:  0.1435772941901427
iteration : 915
train acc:  0.6640625
train loss:  0.5976048111915588
train gradient:  0.1702981658648664
iteration : 916
train acc:  0.6875
train loss:  0.5739879012107849
train gradient:  0.1617244399433282
iteration : 917
train acc:  0.7109375
train loss:  0.5405426025390625
train gradient:  0.12280988939608237
iteration : 918
train acc:  0.625
train loss:  0.6285951733589172
train gradient:  0.2336689882478972
iteration : 919
train acc:  0.734375
train loss:  0.5322201251983643
train gradient:  0.17514152190960702
iteration : 920
train acc:  0.6875
train loss:  0.617790937423706
train gradient:  0.18798977194301297
iteration : 921
train acc:  0.734375
train loss:  0.5708109140396118
train gradient:  0.18889570060929933
iteration : 922
train acc:  0.671875
train loss:  0.5510727763175964
train gradient:  0.1786189566304128
iteration : 923
train acc:  0.6640625
train loss:  0.595842719078064
train gradient:  0.1991869661594967
iteration : 924
train acc:  0.7109375
train loss:  0.5456885099411011
train gradient:  0.35994242533006066
iteration : 925
train acc:  0.6640625
train loss:  0.6143937706947327
train gradient:  0.17574223510310702
iteration : 926
train acc:  0.7421875
train loss:  0.5302140116691589
train gradient:  0.13199725617701924
iteration : 927
train acc:  0.6328125
train loss:  0.5760321617126465
train gradient:  0.141902484154935
iteration : 928
train acc:  0.7421875
train loss:  0.5463384389877319
train gradient:  0.14834708770219027
iteration : 929
train acc:  0.75
train loss:  0.5195870399475098
train gradient:  0.21471992083412217
iteration : 930
train acc:  0.6328125
train loss:  0.5952273011207581
train gradient:  0.18876992763670214
iteration : 931
train acc:  0.65625
train loss:  0.5858674049377441
train gradient:  0.1881551999353549
iteration : 932
train acc:  0.703125
train loss:  0.555717408657074
train gradient:  0.15812533485033986
iteration : 933
train acc:  0.7265625
train loss:  0.568859875202179
train gradient:  0.1430771754451075
iteration : 934
train acc:  0.71875
train loss:  0.5512712001800537
train gradient:  0.1530191014901926
iteration : 935
train acc:  0.6875
train loss:  0.5960296988487244
train gradient:  0.22835971410669906
iteration : 936
train acc:  0.6796875
train loss:  0.5713052153587341
train gradient:  0.17623163907631098
iteration : 937
train acc:  0.7109375
train loss:  0.5486797094345093
train gradient:  0.1860902509542553
iteration : 938
train acc:  0.640625
train loss:  0.6165562272071838
train gradient:  0.2953108071543503
iteration : 939
train acc:  0.6796875
train loss:  0.5909632444381714
train gradient:  0.17099337545931093
iteration : 940
train acc:  0.65625
train loss:  0.570823073387146
train gradient:  0.18246807071734672
iteration : 941
train acc:  0.734375
train loss:  0.5404565334320068
train gradient:  0.16800684572742644
iteration : 942
train acc:  0.6875
train loss:  0.5804747343063354
train gradient:  0.2345257756681704
iteration : 943
train acc:  0.5859375
train loss:  0.6092210412025452
train gradient:  0.21486597638932636
iteration : 944
train acc:  0.6796875
train loss:  0.6327917575836182
train gradient:  0.1877582368444936
iteration : 945
train acc:  0.65625
train loss:  0.6172871589660645
train gradient:  0.18222684953344465
iteration : 946
train acc:  0.6796875
train loss:  0.6046371459960938
train gradient:  0.14688030220553736
iteration : 947
train acc:  0.6796875
train loss:  0.5932866930961609
train gradient:  0.13879456656881461
iteration : 948
train acc:  0.671875
train loss:  0.5774770975112915
train gradient:  0.19528763134609511
iteration : 949
train acc:  0.6171875
train loss:  0.6518287658691406
train gradient:  0.2895817343746649
iteration : 950
train acc:  0.7578125
train loss:  0.5177285671234131
train gradient:  0.15199698798413172
iteration : 951
train acc:  0.640625
train loss:  0.619690477848053
train gradient:  0.15191734845568272
iteration : 952
train acc:  0.65625
train loss:  0.6177263259887695
train gradient:  0.22201651391447882
iteration : 953
train acc:  0.6484375
train loss:  0.6511791348457336
train gradient:  0.1839271466920638
iteration : 954
train acc:  0.71875
train loss:  0.5667160153388977
train gradient:  0.2310027294114082
iteration : 955
train acc:  0.640625
train loss:  0.6015735864639282
train gradient:  0.17201617669460176
iteration : 956
train acc:  0.6875
train loss:  0.6395789384841919
train gradient:  0.2101758327414322
iteration : 957
train acc:  0.6328125
train loss:  0.6227060556411743
train gradient:  0.1484442345029114
iteration : 958
train acc:  0.671875
train loss:  0.5777925252914429
train gradient:  0.1312160222255062
iteration : 959
train acc:  0.7421875
train loss:  0.5413284301757812
train gradient:  0.161148996878495
iteration : 960
train acc:  0.6953125
train loss:  0.5534400939941406
train gradient:  0.13355514391853834
iteration : 961
train acc:  0.6640625
train loss:  0.5707298517227173
train gradient:  0.15809327008501048
iteration : 962
train acc:  0.734375
train loss:  0.5223424434661865
train gradient:  0.18085728671173748
iteration : 963
train acc:  0.7109375
train loss:  0.565764307975769
train gradient:  0.14788437211203165
iteration : 964
train acc:  0.7421875
train loss:  0.5619421005249023
train gradient:  0.2001749019370208
iteration : 965
train acc:  0.6875
train loss:  0.5726357698440552
train gradient:  0.2594113931711166
iteration : 966
train acc:  0.7109375
train loss:  0.5331342220306396
train gradient:  0.16099546061278205
iteration : 967
train acc:  0.6875
train loss:  0.5683101415634155
train gradient:  0.13149675861879734
iteration : 968
train acc:  0.671875
train loss:  0.6109037399291992
train gradient:  0.2022653933620774
iteration : 969
train acc:  0.671875
train loss:  0.5697361826896667
train gradient:  0.16819777193700697
iteration : 970
train acc:  0.71875
train loss:  0.5389713048934937
train gradient:  0.1444836467108414
iteration : 971
train acc:  0.6640625
train loss:  0.5985698699951172
train gradient:  0.15971819500698206
iteration : 972
train acc:  0.703125
train loss:  0.5399906635284424
train gradient:  0.1325620871403273
iteration : 973
train acc:  0.7109375
train loss:  0.5482758283615112
train gradient:  0.16143363519984055
iteration : 974
train acc:  0.6484375
train loss:  0.6065458059310913
train gradient:  0.19807876134252494
iteration : 975
train acc:  0.703125
train loss:  0.5480851531028748
train gradient:  0.12642302395741342
iteration : 976
train acc:  0.75
train loss:  0.5530111789703369
train gradient:  0.13263203067944485
iteration : 977
train acc:  0.6953125
train loss:  0.5440913438796997
train gradient:  0.11932380982733543
iteration : 978
train acc:  0.6640625
train loss:  0.550161600112915
train gradient:  0.14197246688452386
iteration : 979
train acc:  0.7109375
train loss:  0.5453415513038635
train gradient:  0.13254301223361492
iteration : 980
train acc:  0.734375
train loss:  0.5141348242759705
train gradient:  0.13605107829233493
iteration : 981
train acc:  0.671875
train loss:  0.5783304572105408
train gradient:  0.21358457891070792
iteration : 982
train acc:  0.78125
train loss:  0.5127189755439758
train gradient:  0.12950773452032857
iteration : 983
train acc:  0.6875
train loss:  0.5817531943321228
train gradient:  0.1301745583135811
iteration : 984
train acc:  0.7421875
train loss:  0.5551528930664062
train gradient:  0.26612558925465896
iteration : 985
train acc:  0.71875
train loss:  0.5767004489898682
train gradient:  0.1459561152591104
iteration : 986
train acc:  0.7578125
train loss:  0.5270169973373413
train gradient:  0.13543463141753354
iteration : 987
train acc:  0.71875
train loss:  0.5347509384155273
train gradient:  0.18648350683958037
iteration : 988
train acc:  0.625
train loss:  0.6120495796203613
train gradient:  0.12966372349705693
iteration : 989
train acc:  0.6640625
train loss:  0.6208288669586182
train gradient:  0.18022449892148132
iteration : 990
train acc:  0.65625
train loss:  0.6161338090896606
train gradient:  0.20610452005598878
iteration : 991
train acc:  0.6640625
train loss:  0.5598614811897278
train gradient:  0.1978266537654862
iteration : 992
train acc:  0.640625
train loss:  0.6457161903381348
train gradient:  0.20813375311929036
iteration : 993
train acc:  0.703125
train loss:  0.5928219556808472
train gradient:  0.1356876143322167
iteration : 994
train acc:  0.6875
train loss:  0.5330432057380676
train gradient:  0.16163433113158834
iteration : 995
train acc:  0.7109375
train loss:  0.5793482065200806
train gradient:  0.24182220718901015
iteration : 996
train acc:  0.7109375
train loss:  0.5813862681388855
train gradient:  0.19738192931422924
iteration : 997
train acc:  0.7421875
train loss:  0.5488706827163696
train gradient:  0.18607914685025795
iteration : 998
train acc:  0.7265625
train loss:  0.5315110683441162
train gradient:  0.13275043490071786
iteration : 999
train acc:  0.6328125
train loss:  0.6255578994750977
train gradient:  0.13887563000782496
iteration : 1000
train acc:  0.703125
train loss:  0.5900105237960815
train gradient:  0.163741802439614
iteration : 1001
train acc:  0.75
train loss:  0.5110228657722473
train gradient:  0.10969419785471023
iteration : 1002
train acc:  0.6875
train loss:  0.5672906637191772
train gradient:  0.1807388299041817
iteration : 1003
train acc:  0.671875
train loss:  0.5976303815841675
train gradient:  0.1361196858679834
iteration : 1004
train acc:  0.625
train loss:  0.6558408737182617
train gradient:  0.20321313351870812
iteration : 1005
train acc:  0.6484375
train loss:  0.5932421684265137
train gradient:  0.15233291945151603
iteration : 1006
train acc:  0.6875
train loss:  0.5372046232223511
train gradient:  0.11238415022939742
iteration : 1007
train acc:  0.6796875
train loss:  0.5862935781478882
train gradient:  0.15286612704335498
iteration : 1008
train acc:  0.671875
train loss:  0.561677873134613
train gradient:  0.1313566823613232
iteration : 1009
train acc:  0.6015625
train loss:  0.6599866151809692
train gradient:  0.19243143695093134
iteration : 1010
train acc:  0.6484375
train loss:  0.6056089401245117
train gradient:  0.1801972145111419
iteration : 1011
train acc:  0.6875
train loss:  0.6102440357208252
train gradient:  0.1722656840512446
iteration : 1012
train acc:  0.765625
train loss:  0.5059286952018738
train gradient:  0.15188541297792535
iteration : 1013
train acc:  0.71875
train loss:  0.5364429950714111
train gradient:  0.1774568257955152
iteration : 1014
train acc:  0.65625
train loss:  0.5801888704299927
train gradient:  0.3701684510343056
iteration : 1015
train acc:  0.71875
train loss:  0.544600784778595
train gradient:  0.13421314805885173
iteration : 1016
train acc:  0.7109375
train loss:  0.589341402053833
train gradient:  0.17546170102752462
iteration : 1017
train acc:  0.71875
train loss:  0.5757604837417603
train gradient:  0.16177888529342135
iteration : 1018
train acc:  0.7421875
train loss:  0.5021510720252991
train gradient:  0.13379866620547307
iteration : 1019
train acc:  0.6953125
train loss:  0.580546498298645
train gradient:  0.18612585803442544
iteration : 1020
train acc:  0.71875
train loss:  0.5393284559249878
train gradient:  0.1469602732105108
iteration : 1021
train acc:  0.71875
train loss:  0.539530873298645
train gradient:  0.28443344133103365
iteration : 1022
train acc:  0.78125
train loss:  0.5135443210601807
train gradient:  0.10695648487654805
iteration : 1023
train acc:  0.7109375
train loss:  0.5451684594154358
train gradient:  0.1454552765303349
iteration : 1024
train acc:  0.6875
train loss:  0.5649216175079346
train gradient:  0.1753589965714617
iteration : 1025
train acc:  0.671875
train loss:  0.5985046625137329
train gradient:  0.1796254888459229
iteration : 1026
train acc:  0.734375
train loss:  0.5348214507102966
train gradient:  0.14838508811351997
iteration : 1027
train acc:  0.6796875
train loss:  0.5699838399887085
train gradient:  0.16372534750955725
iteration : 1028
train acc:  0.640625
train loss:  0.5968347191810608
train gradient:  0.17747175664451148
iteration : 1029
train acc:  0.78125
train loss:  0.5044575333595276
train gradient:  0.17380914517518423
iteration : 1030
train acc:  0.6484375
train loss:  0.6064010858535767
train gradient:  0.18180742927058335
iteration : 1031
train acc:  0.6875
train loss:  0.5926949381828308
train gradient:  0.16927218191607551
iteration : 1032
train acc:  0.6953125
train loss:  0.5587081909179688
train gradient:  0.13267003802412675
iteration : 1033
train acc:  0.7421875
train loss:  0.5159438848495483
train gradient:  0.1333249973790297
iteration : 1034
train acc:  0.703125
train loss:  0.5720984935760498
train gradient:  0.16172825801744284
iteration : 1035
train acc:  0.6484375
train loss:  0.602776825428009
train gradient:  0.1497748542243326
iteration : 1036
train acc:  0.6796875
train loss:  0.5819306969642639
train gradient:  0.1301764408528478
iteration : 1037
train acc:  0.7109375
train loss:  0.5579200983047485
train gradient:  0.21158881020851605
iteration : 1038
train acc:  0.65625
train loss:  0.6041736602783203
train gradient:  0.21995786760101485
iteration : 1039
train acc:  0.7109375
train loss:  0.5592646598815918
train gradient:  0.141910081838859
iteration : 1040
train acc:  0.6796875
train loss:  0.5722537636756897
train gradient:  0.1569517680227005
iteration : 1041
train acc:  0.6640625
train loss:  0.5679977536201477
train gradient:  0.16354799777484924
iteration : 1042
train acc:  0.71875
train loss:  0.5167127251625061
train gradient:  0.13202105706957334
iteration : 1043
train acc:  0.7421875
train loss:  0.5315313935279846
train gradient:  0.135456129152769
iteration : 1044
train acc:  0.75
train loss:  0.49912139773368835
train gradient:  0.1340955037159144
iteration : 1045
train acc:  0.640625
train loss:  0.6040505766868591
train gradient:  0.15304354202511639
iteration : 1046
train acc:  0.6796875
train loss:  0.5682271718978882
train gradient:  0.22929959912762546
iteration : 1047
train acc:  0.6875
train loss:  0.5561782121658325
train gradient:  0.169176516547458
iteration : 1048
train acc:  0.7109375
train loss:  0.5667102336883545
train gradient:  0.139569396685282
iteration : 1049
train acc:  0.8125
train loss:  0.496404230594635
train gradient:  0.21968946820357227
iteration : 1050
train acc:  0.6953125
train loss:  0.5620502233505249
train gradient:  0.17189915050337
iteration : 1051
train acc:  0.703125
train loss:  0.5658828616142273
train gradient:  0.15903887960865687
iteration : 1052
train acc:  0.7265625
train loss:  0.5621768236160278
train gradient:  0.2045713243007617
iteration : 1053
train acc:  0.6796875
train loss:  0.5773397088050842
train gradient:  0.16471587618765787
iteration : 1054
train acc:  0.671875
train loss:  0.565596342086792
train gradient:  0.2149672343947372
iteration : 1055
train acc:  0.6953125
train loss:  0.5948946475982666
train gradient:  0.14562727408867657
iteration : 1056
train acc:  0.734375
train loss:  0.5596365928649902
train gradient:  0.14570625643150867
iteration : 1057
train acc:  0.671875
train loss:  0.6537253260612488
train gradient:  0.2516444308744598
iteration : 1058
train acc:  0.65625
train loss:  0.5632652044296265
train gradient:  0.19542628347448834
iteration : 1059
train acc:  0.734375
train loss:  0.5646253824234009
train gradient:  0.1956250212618887
iteration : 1060
train acc:  0.65625
train loss:  0.5838571786880493
train gradient:  0.17388464151646943
iteration : 1061
train acc:  0.7109375
train loss:  0.5254620313644409
train gradient:  0.13266279225656058
iteration : 1062
train acc:  0.6328125
train loss:  0.6160581111907959
train gradient:  0.1693032109611643
iteration : 1063
train acc:  0.7109375
train loss:  0.5257799625396729
train gradient:  0.22203448444960788
iteration : 1064
train acc:  0.7578125
train loss:  0.5054552555084229
train gradient:  0.15012343352129948
iteration : 1065
train acc:  0.65625
train loss:  0.5912739634513855
train gradient:  0.18827647680631193
iteration : 1066
train acc:  0.71875
train loss:  0.5498974323272705
train gradient:  0.15900034402128163
iteration : 1067
train acc:  0.6953125
train loss:  0.5890693664550781
train gradient:  0.1806011687130909
iteration : 1068
train acc:  0.6640625
train loss:  0.5507910251617432
train gradient:  0.12204400391930217
iteration : 1069
train acc:  0.6484375
train loss:  0.6510834693908691
train gradient:  0.22994910292723447
iteration : 1070
train acc:  0.6796875
train loss:  0.624765932559967
train gradient:  0.1772038402112569
iteration : 1071
train acc:  0.78125
train loss:  0.5134544372558594
train gradient:  0.14897697187651326
iteration : 1072
train acc:  0.7109375
train loss:  0.5524522662162781
train gradient:  0.17545643990355653
iteration : 1073
train acc:  0.7734375
train loss:  0.5049431324005127
train gradient:  0.11205740886948792
iteration : 1074
train acc:  0.6953125
train loss:  0.559673547744751
train gradient:  0.1401559444241049
iteration : 1075
train acc:  0.703125
train loss:  0.5592474937438965
train gradient:  0.17257770282671464
iteration : 1076
train acc:  0.640625
train loss:  0.6002877354621887
train gradient:  0.19265515370904263
iteration : 1077
train acc:  0.7421875
train loss:  0.5689020156860352
train gradient:  0.2388736887291688
iteration : 1078
train acc:  0.6875
train loss:  0.5723955631256104
train gradient:  0.20544009200360286
iteration : 1079
train acc:  0.6953125
train loss:  0.6144095659255981
train gradient:  0.23650526931795057
iteration : 1080
train acc:  0.7265625
train loss:  0.508417010307312
train gradient:  0.1734436278821601
iteration : 1081
train acc:  0.6875
train loss:  0.5885112285614014
train gradient:  0.20163468437839452
iteration : 1082
train acc:  0.671875
train loss:  0.5833642482757568
train gradient:  0.14663057942201702
iteration : 1083
train acc:  0.6875
train loss:  0.5833005905151367
train gradient:  0.1575290650092135
iteration : 1084
train acc:  0.6875
train loss:  0.5739794969558716
train gradient:  0.1705921799546503
iteration : 1085
train acc:  0.703125
train loss:  0.5844123959541321
train gradient:  0.21012323913263745
iteration : 1086
train acc:  0.6953125
train loss:  0.5478429794311523
train gradient:  0.12816921605944226
iteration : 1087
train acc:  0.7578125
train loss:  0.5100105404853821
train gradient:  0.17435172523288878
iteration : 1088
train acc:  0.7109375
train loss:  0.5898545384407043
train gradient:  0.1461193512916488
iteration : 1089
train acc:  0.6640625
train loss:  0.5916796922683716
train gradient:  0.14734465111574713
iteration : 1090
train acc:  0.734375
train loss:  0.5470072031021118
train gradient:  0.19199455062728638
iteration : 1091
train acc:  0.609375
train loss:  0.6011526584625244
train gradient:  0.1609005013130338
iteration : 1092
train acc:  0.6796875
train loss:  0.5557323694229126
train gradient:  0.16849952964777376
iteration : 1093
train acc:  0.65625
train loss:  0.5877588987350464
train gradient:  0.17426840211807765
iteration : 1094
train acc:  0.6953125
train loss:  0.5751032829284668
train gradient:  0.14503725871940898
iteration : 1095
train acc:  0.703125
train loss:  0.5718538165092468
train gradient:  0.1877009331281337
iteration : 1096
train acc:  0.65625
train loss:  0.5832751393318176
train gradient:  0.1496093096601272
iteration : 1097
train acc:  0.7265625
train loss:  0.540569007396698
train gradient:  0.1811974848196872
iteration : 1098
train acc:  0.6875
train loss:  0.5631035566329956
train gradient:  0.1406018599252914
iteration : 1099
train acc:  0.578125
train loss:  0.6751595735549927
train gradient:  0.20488592152726312
iteration : 1100
train acc:  0.7578125
train loss:  0.5417433977127075
train gradient:  0.20490405659575206
iteration : 1101
train acc:  0.703125
train loss:  0.5538615584373474
train gradient:  0.17785038055313168
iteration : 1102
train acc:  0.6875
train loss:  0.5611472725868225
train gradient:  0.1301009032448873
iteration : 1103
train acc:  0.6484375
train loss:  0.5905871987342834
train gradient:  0.22049755619158815
iteration : 1104
train acc:  0.6796875
train loss:  0.6017604470252991
train gradient:  0.17914498148536318
iteration : 1105
train acc:  0.7265625
train loss:  0.5371404886245728
train gradient:  0.12939072873522003
iteration : 1106
train acc:  0.765625
train loss:  0.5112689733505249
train gradient:  0.2231385686275255
iteration : 1107
train acc:  0.6796875
train loss:  0.5947567820549011
train gradient:  0.13976075842813598
iteration : 1108
train acc:  0.6640625
train loss:  0.5652372241020203
train gradient:  0.1763808662239183
iteration : 1109
train acc:  0.7421875
train loss:  0.5101591348648071
train gradient:  0.10756049635659544
iteration : 1110
train acc:  0.625
train loss:  0.6016769409179688
train gradient:  0.22375823181720178
iteration : 1111
train acc:  0.796875
train loss:  0.47397279739379883
train gradient:  0.17742741118219718
iteration : 1112
train acc:  0.6796875
train loss:  0.5843102931976318
train gradient:  0.16034069745340782
iteration : 1113
train acc:  0.6484375
train loss:  0.6021080613136292
train gradient:  0.16957701053264368
iteration : 1114
train acc:  0.625
train loss:  0.5969142913818359
train gradient:  0.17519615653072224
iteration : 1115
train acc:  0.703125
train loss:  0.6013362407684326
train gradient:  0.20524666143061998
iteration : 1116
train acc:  0.6796875
train loss:  0.56800776720047
train gradient:  0.16293459548785955
iteration : 1117
train acc:  0.734375
train loss:  0.5522192716598511
train gradient:  0.17819275943267873
iteration : 1118
train acc:  0.671875
train loss:  0.5284092426300049
train gradient:  0.15887384285006834
iteration : 1119
train acc:  0.6953125
train loss:  0.5803804397583008
train gradient:  0.11477278015880085
iteration : 1120
train acc:  0.7265625
train loss:  0.5156639814376831
train gradient:  0.20396841073101404
iteration : 1121
train acc:  0.765625
train loss:  0.490151584148407
train gradient:  0.18021561533912017
iteration : 1122
train acc:  0.75
train loss:  0.5442690253257751
train gradient:  0.15557816642529704
iteration : 1123
train acc:  0.6953125
train loss:  0.5342239737510681
train gradient:  0.1322535086727299
iteration : 1124
train acc:  0.7578125
train loss:  0.4998297095298767
train gradient:  0.1594236818401122
iteration : 1125
train acc:  0.7109375
train loss:  0.5341998338699341
train gradient:  0.17559924488586282
iteration : 1126
train acc:  0.6640625
train loss:  0.6156721115112305
train gradient:  0.2077010903637479
iteration : 1127
train acc:  0.640625
train loss:  0.6178137063980103
train gradient:  0.23856300668228544
iteration : 1128
train acc:  0.6796875
train loss:  0.5598600506782532
train gradient:  0.15881600673423116
iteration : 1129
train acc:  0.703125
train loss:  0.5795016288757324
train gradient:  0.20572247365158497
iteration : 1130
train acc:  0.6875
train loss:  0.5844972133636475
train gradient:  0.18760580698218876
iteration : 1131
train acc:  0.71875
train loss:  0.5133461952209473
train gradient:  0.1580015330590291
iteration : 1132
train acc:  0.6953125
train loss:  0.5960540175437927
train gradient:  0.19295967027713867
iteration : 1133
train acc:  0.65625
train loss:  0.6178896427154541
train gradient:  0.16956734124972067
iteration : 1134
train acc:  0.6171875
train loss:  0.6414617896080017
train gradient:  0.21595568420529299
iteration : 1135
train acc:  0.7265625
train loss:  0.5347515940666199
train gradient:  0.1965211069658066
iteration : 1136
train acc:  0.6484375
train loss:  0.6336476802825928
train gradient:  0.2507306550987398
iteration : 1137
train acc:  0.6328125
train loss:  0.6067792177200317
train gradient:  0.21137433526737925
iteration : 1138
train acc:  0.6640625
train loss:  0.5903561115264893
train gradient:  0.21233614377234405
iteration : 1139
train acc:  0.734375
train loss:  0.5214625597000122
train gradient:  0.15788728410713987
iteration : 1140
train acc:  0.65625
train loss:  0.5735318660736084
train gradient:  0.16653508510897788
iteration : 1141
train acc:  0.7109375
train loss:  0.5967415571212769
train gradient:  0.1727526343295115
iteration : 1142
train acc:  0.734375
train loss:  0.5533244013786316
train gradient:  0.21400061461012598
iteration : 1143
train acc:  0.6796875
train loss:  0.5879387855529785
train gradient:  0.20203759660198684
iteration : 1144
train acc:  0.6953125
train loss:  0.5454553365707397
train gradient:  0.20183697365368036
iteration : 1145
train acc:  0.65625
train loss:  0.5786561965942383
train gradient:  0.25576278752560255
iteration : 1146
train acc:  0.640625
train loss:  0.5884010195732117
train gradient:  0.24987978188870608
iteration : 1147
train acc:  0.7109375
train loss:  0.5585219860076904
train gradient:  0.21345671422475634
iteration : 1148
train acc:  0.6953125
train loss:  0.5594537258148193
train gradient:  0.17884415924820518
iteration : 1149
train acc:  0.6640625
train loss:  0.5610857009887695
train gradient:  0.29157626893536587
iteration : 1150
train acc:  0.6640625
train loss:  0.6166141629219055
train gradient:  0.2868912866930544
iteration : 1151
train acc:  0.7421875
train loss:  0.5329993367195129
train gradient:  0.15679798147765933
iteration : 1152
train acc:  0.7421875
train loss:  0.5326464176177979
train gradient:  0.16559188870168046
iteration : 1153
train acc:  0.6953125
train loss:  0.5599311590194702
train gradient:  0.1439995333159538
iteration : 1154
train acc:  0.75
train loss:  0.5164598822593689
train gradient:  0.1931629857270275
iteration : 1155
train acc:  0.7109375
train loss:  0.5578396916389465
train gradient:  0.1425482240746574
iteration : 1156
train acc:  0.6484375
train loss:  0.5969588756561279
train gradient:  0.16490378823787805
iteration : 1157
train acc:  0.6796875
train loss:  0.5632363557815552
train gradient:  0.16142284556973296
iteration : 1158
train acc:  0.7109375
train loss:  0.5472327470779419
train gradient:  0.16686783985543974
iteration : 1159
train acc:  0.6875
train loss:  0.5951277017593384
train gradient:  0.22487826337271613
iteration : 1160
train acc:  0.6953125
train loss:  0.5502179861068726
train gradient:  0.12899865169893437
iteration : 1161
train acc:  0.703125
train loss:  0.5974570512771606
train gradient:  0.2504684050346035
iteration : 1162
train acc:  0.6953125
train loss:  0.5612236261367798
train gradient:  0.1890407455581682
iteration : 1163
train acc:  0.6796875
train loss:  0.5750114917755127
train gradient:  0.19744297508014974
iteration : 1164
train acc:  0.578125
train loss:  0.6219416856765747
train gradient:  0.15083638883188122
iteration : 1165
train acc:  0.7109375
train loss:  0.5462545156478882
train gradient:  0.21309141725946668
iteration : 1166
train acc:  0.7265625
train loss:  0.5537376403808594
train gradient:  0.1477818400961501
iteration : 1167
train acc:  0.6484375
train loss:  0.5986455678939819
train gradient:  0.17453600841177147
iteration : 1168
train acc:  0.71875
train loss:  0.5445355176925659
train gradient:  0.19353373315883543
iteration : 1169
train acc:  0.625
train loss:  0.6449441313743591
train gradient:  0.21976307948412943
iteration : 1170
train acc:  0.7109375
train loss:  0.5360500812530518
train gradient:  0.1786642314613962
iteration : 1171
train acc:  0.6015625
train loss:  0.5895284414291382
train gradient:  0.17216918670259235
iteration : 1172
train acc:  0.7109375
train loss:  0.5423237681388855
train gradient:  0.14242826445946444
iteration : 1173
train acc:  0.7265625
train loss:  0.5127037763595581
train gradient:  0.16478015426886236
iteration : 1174
train acc:  0.7421875
train loss:  0.5307962894439697
train gradient:  0.19806579066111296
iteration : 1175
train acc:  0.7109375
train loss:  0.5733451247215271
train gradient:  0.17638413685478788
iteration : 1176
train acc:  0.640625
train loss:  0.6340133547782898
train gradient:  0.2570854529905886
iteration : 1177
train acc:  0.6953125
train loss:  0.5662536025047302
train gradient:  0.2180870669901226
iteration : 1178
train acc:  0.7734375
train loss:  0.511165976524353
train gradient:  0.15974027025229473
iteration : 1179
train acc:  0.71875
train loss:  0.5567346811294556
train gradient:  0.16497048074391749
iteration : 1180
train acc:  0.671875
train loss:  0.5943034887313843
train gradient:  0.2267570206592746
iteration : 1181
train acc:  0.671875
train loss:  0.6117653846740723
train gradient:  0.17789934707953614
iteration : 1182
train acc:  0.71875
train loss:  0.5235140323638916
train gradient:  0.12770646979121464
iteration : 1183
train acc:  0.7109375
train loss:  0.5335439443588257
train gradient:  0.13378759454788858
iteration : 1184
train acc:  0.6484375
train loss:  0.6493113040924072
train gradient:  0.18099040823726956
iteration : 1185
train acc:  0.671875
train loss:  0.5711915493011475
train gradient:  0.23556733167173333
iteration : 1186
train acc:  0.75
train loss:  0.5120749473571777
train gradient:  0.1494944725450516
iteration : 1187
train acc:  0.71875
train loss:  0.5122326612472534
train gradient:  0.15199304581001805
iteration : 1188
train acc:  0.6328125
train loss:  0.5601524710655212
train gradient:  0.14589712378814254
iteration : 1189
train acc:  0.6953125
train loss:  0.6017248630523682
train gradient:  0.17548916675643308
iteration : 1190
train acc:  0.6484375
train loss:  0.6133742332458496
train gradient:  0.1927709607404339
iteration : 1191
train acc:  0.6328125
train loss:  0.640129566192627
train gradient:  0.2691377309363203
iteration : 1192
train acc:  0.703125
train loss:  0.5707790851593018
train gradient:  0.1584550668605685
iteration : 1193
train acc:  0.7109375
train loss:  0.5469182133674622
train gradient:  0.14250823002472263
iteration : 1194
train acc:  0.7109375
train loss:  0.5326089262962341
train gradient:  0.16852292836466187
iteration : 1195
train acc:  0.65625
train loss:  0.6257715225219727
train gradient:  0.21398085718688464
iteration : 1196
train acc:  0.6328125
train loss:  0.6571369171142578
train gradient:  0.21995349854787652
iteration : 1197
train acc:  0.65625
train loss:  0.5807744860649109
train gradient:  0.16713024107945257
iteration : 1198
train acc:  0.7265625
train loss:  0.6253763437271118
train gradient:  0.23823809399342316
iteration : 1199
train acc:  0.703125
train loss:  0.5291237235069275
train gradient:  0.14873567192100356
iteration : 1200
train acc:  0.671875
train loss:  0.614990234375
train gradient:  0.20819890821003956
iteration : 1201
train acc:  0.765625
train loss:  0.5331562161445618
train gradient:  0.15859980060126716
iteration : 1202
train acc:  0.6875
train loss:  0.5317895412445068
train gradient:  0.1697149560781
iteration : 1203
train acc:  0.6484375
train loss:  0.5706402659416199
train gradient:  0.19642189216330147
iteration : 1204
train acc:  0.6640625
train loss:  0.5314667820930481
train gradient:  0.131310261625932
iteration : 1205
train acc:  0.671875
train loss:  0.572439968585968
train gradient:  0.14156666072433918
iteration : 1206
train acc:  0.75
train loss:  0.5083180069923401
train gradient:  0.14548030892514424
iteration : 1207
train acc:  0.6640625
train loss:  0.5785878300666809
train gradient:  0.2054366058334007
iteration : 1208
train acc:  0.7109375
train loss:  0.5403435230255127
train gradient:  0.1792510039111186
iteration : 1209
train acc:  0.6875
train loss:  0.5816642045974731
train gradient:  0.19782149086771
iteration : 1210
train acc:  0.671875
train loss:  0.5741280913352966
train gradient:  0.20144291903297035
iteration : 1211
train acc:  0.65625
train loss:  0.6037989258766174
train gradient:  0.19161919529662036
iteration : 1212
train acc:  0.625
train loss:  0.646034836769104
train gradient:  0.2085886737167361
iteration : 1213
train acc:  0.7421875
train loss:  0.5254340171813965
train gradient:  0.16855652775946683
iteration : 1214
train acc:  0.7578125
train loss:  0.5183617472648621
train gradient:  0.15323462230707804
iteration : 1215
train acc:  0.71875
train loss:  0.5484539866447449
train gradient:  0.15623164166444914
iteration : 1216
train acc:  0.7109375
train loss:  0.5405147671699524
train gradient:  0.15479252272902788
iteration : 1217
train acc:  0.6875
train loss:  0.5823381543159485
train gradient:  0.16812293054047672
iteration : 1218
train acc:  0.6171875
train loss:  0.6283789277076721
train gradient:  0.1927727585316732
iteration : 1219
train acc:  0.625
train loss:  0.6053906083106995
train gradient:  0.16176145633917227
iteration : 1220
train acc:  0.7109375
train loss:  0.5498898029327393
train gradient:  0.16503808658492033
iteration : 1221
train acc:  0.7421875
train loss:  0.5258586406707764
train gradient:  0.1766715349234358
iteration : 1222
train acc:  0.6328125
train loss:  0.6274031400680542
train gradient:  0.2466620125664109
iteration : 1223
train acc:  0.6328125
train loss:  0.594567596912384
train gradient:  0.19624425640876436
iteration : 1224
train acc:  0.671875
train loss:  0.589012861251831
train gradient:  0.1956143089028914
iteration : 1225
train acc:  0.6953125
train loss:  0.5725576877593994
train gradient:  0.15417635786279393
iteration : 1226
train acc:  0.6875
train loss:  0.57563316822052
train gradient:  0.15910596439903932
iteration : 1227
train acc:  0.71875
train loss:  0.5380133390426636
train gradient:  0.19770058389187967
iteration : 1228
train acc:  0.6640625
train loss:  0.5635186433792114
train gradient:  0.1764950599814054
iteration : 1229
train acc:  0.734375
train loss:  0.5095364451408386
train gradient:  0.11229144849332152
iteration : 1230
train acc:  0.7265625
train loss:  0.5709829926490784
train gradient:  0.13681234504558665
iteration : 1231
train acc:  0.6484375
train loss:  0.6113903522491455
train gradient:  0.1518467818609875
iteration : 1232
train acc:  0.6640625
train loss:  0.5885394215583801
train gradient:  0.1401021929526537
iteration : 1233
train acc:  0.609375
train loss:  0.6303325891494751
train gradient:  0.20151784663878283
iteration : 1234
train acc:  0.7265625
train loss:  0.5603525042533875
train gradient:  0.18886399703077905
iteration : 1235
train acc:  0.734375
train loss:  0.5428457260131836
train gradient:  0.15068835468627492
iteration : 1236
train acc:  0.7734375
train loss:  0.5068694353103638
train gradient:  0.13350216031703444
iteration : 1237
train acc:  0.7734375
train loss:  0.5022286176681519
train gradient:  0.16100305135562382
iteration : 1238
train acc:  0.640625
train loss:  0.5620816946029663
train gradient:  0.1495376931404721
iteration : 1239
train acc:  0.6171875
train loss:  0.6440253853797913
train gradient:  0.26651993321669915
iteration : 1240
train acc:  0.640625
train loss:  0.6441943645477295
train gradient:  0.1929284295642159
iteration : 1241
train acc:  0.71875
train loss:  0.5808380842208862
train gradient:  0.16148106526813072
iteration : 1242
train acc:  0.6953125
train loss:  0.6004757881164551
train gradient:  0.20268798746136443
iteration : 1243
train acc:  0.7109375
train loss:  0.5484480857849121
train gradient:  0.13856891905617055
iteration : 1244
train acc:  0.7265625
train loss:  0.5157185792922974
train gradient:  0.16049830578746804
iteration : 1245
train acc:  0.671875
train loss:  0.5883200764656067
train gradient:  0.2109483360407669
iteration : 1246
train acc:  0.6796875
train loss:  0.5808827877044678
train gradient:  0.1532205097574702
iteration : 1247
train acc:  0.6484375
train loss:  0.6077580451965332
train gradient:  0.18347048287217582
iteration : 1248
train acc:  0.71875
train loss:  0.5305546522140503
train gradient:  0.17196100908114315
iteration : 1249
train acc:  0.734375
train loss:  0.5492210984230042
train gradient:  0.19456112970328834
iteration : 1250
train acc:  0.6796875
train loss:  0.5565876364707947
train gradient:  0.1382532006224665
iteration : 1251
train acc:  0.765625
train loss:  0.5202903747558594
train gradient:  0.10328751026824737
iteration : 1252
train acc:  0.6953125
train loss:  0.5494309067726135
train gradient:  0.14475703257035744
iteration : 1253
train acc:  0.6796875
train loss:  0.536903977394104
train gradient:  0.12794813705019364
iteration : 1254
train acc:  0.6875
train loss:  0.614924967288971
train gradient:  0.3699473845699103
iteration : 1255
train acc:  0.75
train loss:  0.5487195253372192
train gradient:  0.1416544889495421
iteration : 1256
train acc:  0.625
train loss:  0.5970962047576904
train gradient:  0.18750536602922138
iteration : 1257
train acc:  0.7109375
train loss:  0.5518182516098022
train gradient:  0.1626088716554835
iteration : 1258
train acc:  0.7421875
train loss:  0.563262939453125
train gradient:  0.21540643104878177
iteration : 1259
train acc:  0.6328125
train loss:  0.5840461254119873
train gradient:  0.1589200846341258
iteration : 1260
train acc:  0.765625
train loss:  0.4773251414299011
train gradient:  0.24868223188792477
iteration : 1261
train acc:  0.7109375
train loss:  0.5525099039077759
train gradient:  0.1983609127175269
iteration : 1262
train acc:  0.6484375
train loss:  0.6300392150878906
train gradient:  0.1671549441326573
iteration : 1263
train acc:  0.71875
train loss:  0.5370444059371948
train gradient:  0.1919395148820105
iteration : 1264
train acc:  0.75
train loss:  0.528247594833374
train gradient:  0.1492750881911092
iteration : 1265
train acc:  0.765625
train loss:  0.5156174898147583
train gradient:  0.1877868499354462
iteration : 1266
train acc:  0.6796875
train loss:  0.5811386108398438
train gradient:  0.18186393463481193
iteration : 1267
train acc:  0.7109375
train loss:  0.5641942620277405
train gradient:  0.14957900348248862
iteration : 1268
train acc:  0.71875
train loss:  0.5725135803222656
train gradient:  0.1644020181994322
iteration : 1269
train acc:  0.6796875
train loss:  0.5374115705490112
train gradient:  0.18867070091453203
iteration : 1270
train acc:  0.7109375
train loss:  0.6172337532043457
train gradient:  0.18289649982651524
iteration : 1271
train acc:  0.703125
train loss:  0.5415917634963989
train gradient:  0.19321569655871168
iteration : 1272
train acc:  0.65625
train loss:  0.6145522594451904
train gradient:  0.20420900478057902
iteration : 1273
train acc:  0.7109375
train loss:  0.5668851137161255
train gradient:  0.19930018545448802
iteration : 1274
train acc:  0.6875
train loss:  0.5283472537994385
train gradient:  0.12352382031605554
iteration : 1275
train acc:  0.6875
train loss:  0.5566160678863525
train gradient:  0.16657304815160245
iteration : 1276
train acc:  0.671875
train loss:  0.6075866222381592
train gradient:  0.16805850362989044
iteration : 1277
train acc:  0.6484375
train loss:  0.6171686053276062
train gradient:  0.20510649199868936
iteration : 1278
train acc:  0.6875
train loss:  0.5950444340705872
train gradient:  0.185016792645202
iteration : 1279
train acc:  0.6953125
train loss:  0.5273324251174927
train gradient:  0.17905758727670737
iteration : 1280
train acc:  0.7734375
train loss:  0.4874253273010254
train gradient:  0.20319209244158604
iteration : 1281
train acc:  0.71875
train loss:  0.5827136039733887
train gradient:  0.14764719588747466
iteration : 1282
train acc:  0.7109375
train loss:  0.5405542850494385
train gradient:  0.15107071406610223
iteration : 1283
train acc:  0.6328125
train loss:  0.6268632411956787
train gradient:  0.2120617392764756
iteration : 1284
train acc:  0.6796875
train loss:  0.5472557544708252
train gradient:  0.13429362828446573
iteration : 1285
train acc:  0.6796875
train loss:  0.5364381074905396
train gradient:  0.15153622737447903
iteration : 1286
train acc:  0.6640625
train loss:  0.60334312915802
train gradient:  0.18841742801983674
iteration : 1287
train acc:  0.7109375
train loss:  0.5444063544273376
train gradient:  0.16456443968628817
iteration : 1288
train acc:  0.78125
train loss:  0.4930432140827179
train gradient:  0.13983081028711494
iteration : 1289
train acc:  0.734375
train loss:  0.4934660792350769
train gradient:  0.2043021156544716
iteration : 1290
train acc:  0.734375
train loss:  0.5094473361968994
train gradient:  0.14863884681862577
iteration : 1291
train acc:  0.6875
train loss:  0.5673859119415283
train gradient:  0.14873783842408378
iteration : 1292
train acc:  0.71875
train loss:  0.5710194110870361
train gradient:  0.19103637125631462
iteration : 1293
train acc:  0.734375
train loss:  0.5213795304298401
train gradient:  0.18802133781551783
iteration : 1294
train acc:  0.71875
train loss:  0.5532751083374023
train gradient:  0.14169627138242133
iteration : 1295
train acc:  0.6953125
train loss:  0.5476759672164917
train gradient:  0.18722367705060966
iteration : 1296
train acc:  0.7265625
train loss:  0.5428608655929565
train gradient:  0.13125556034012423
iteration : 1297
train acc:  0.6796875
train loss:  0.5719752907752991
train gradient:  0.18102843668544982
iteration : 1298
train acc:  0.71875
train loss:  0.5482898950576782
train gradient:  0.19143861818458258
iteration : 1299
train acc:  0.671875
train loss:  0.5426338911056519
train gradient:  0.18561920417824915
iteration : 1300
train acc:  0.65625
train loss:  0.5845286846160889
train gradient:  0.2849854357065306
iteration : 1301
train acc:  0.78125
train loss:  0.4991829991340637
train gradient:  0.16136111321917196
iteration : 1302
train acc:  0.703125
train loss:  0.5825777053833008
train gradient:  0.19009092664277238
iteration : 1303
train acc:  0.6796875
train loss:  0.542140007019043
train gradient:  0.15474858197272962
iteration : 1304
train acc:  0.6796875
train loss:  0.5610134601593018
train gradient:  0.17506415760491253
iteration : 1305
train acc:  0.6796875
train loss:  0.5716943740844727
train gradient:  0.17417632882516518
iteration : 1306
train acc:  0.7578125
train loss:  0.534686267375946
train gradient:  0.17854899142450165
iteration : 1307
train acc:  0.71875
train loss:  0.5231693983078003
train gradient:  0.2141075730343503
iteration : 1308
train acc:  0.6953125
train loss:  0.5309330821037292
train gradient:  0.16571531205085158
iteration : 1309
train acc:  0.640625
train loss:  0.6328877210617065
train gradient:  0.18277006173068588
iteration : 1310
train acc:  0.6875
train loss:  0.5653148293495178
train gradient:  0.17399756837466746
iteration : 1311
train acc:  0.640625
train loss:  0.5844632387161255
train gradient:  0.17938662955162
iteration : 1312
train acc:  0.78125
train loss:  0.4942896366119385
train gradient:  0.11784903138908798
iteration : 1313
train acc:  0.7265625
train loss:  0.5251035690307617
train gradient:  0.17792488584932378
iteration : 1314
train acc:  0.7109375
train loss:  0.5679032206535339
train gradient:  0.22949370601681995
iteration : 1315
train acc:  0.7265625
train loss:  0.5049710869789124
train gradient:  0.13853528708803903
iteration : 1316
train acc:  0.6796875
train loss:  0.5723880529403687
train gradient:  0.1677262784265452
iteration : 1317
train acc:  0.640625
train loss:  0.5998518466949463
train gradient:  0.15177105692980306
iteration : 1318
train acc:  0.7265625
train loss:  0.5392909049987793
train gradient:  0.15019543078350994
iteration : 1319
train acc:  0.6640625
train loss:  0.5530487298965454
train gradient:  0.2810185774602358
iteration : 1320
train acc:  0.734375
train loss:  0.5591778755187988
train gradient:  0.19484742698156973
iteration : 1321
train acc:  0.75
train loss:  0.5152289867401123
train gradient:  0.12665529224292968
iteration : 1322
train acc:  0.640625
train loss:  0.595750629901886
train gradient:  0.2934696215466686
iteration : 1323
train acc:  0.71875
train loss:  0.6143730878829956
train gradient:  0.2161775206320026
iteration : 1324
train acc:  0.6484375
train loss:  0.5818819403648376
train gradient:  0.18030189282809148
iteration : 1325
train acc:  0.6484375
train loss:  0.6103086471557617
train gradient:  0.22065682856463759
iteration : 1326
train acc:  0.6328125
train loss:  0.6401543021202087
train gradient:  0.22155142498028932
iteration : 1327
train acc:  0.671875
train loss:  0.6130632162094116
train gradient:  0.20811588915026824
iteration : 1328
train acc:  0.6484375
train loss:  0.5978094339370728
train gradient:  0.21209315470642148
iteration : 1329
train acc:  0.6875
train loss:  0.549124002456665
train gradient:  0.14789320328297745
iteration : 1330
train acc:  0.6953125
train loss:  0.5304871201515198
train gradient:  0.15053441326839706
iteration : 1331
train acc:  0.65625
train loss:  0.6134623885154724
train gradient:  0.18559905324802992
iteration : 1332
train acc:  0.7109375
train loss:  0.5485752820968628
train gradient:  0.16553279276233795
iteration : 1333
train acc:  0.6484375
train loss:  0.5782979726791382
train gradient:  0.1889801100215331
iteration : 1334
train acc:  0.7421875
train loss:  0.5344754457473755
train gradient:  0.16585147351911966
iteration : 1335
train acc:  0.7421875
train loss:  0.5247102975845337
train gradient:  0.21610970961724735
iteration : 1336
train acc:  0.7578125
train loss:  0.5206472277641296
train gradient:  0.22286427642626938
iteration : 1337
train acc:  0.7265625
train loss:  0.5425993204116821
train gradient:  0.1867042077091457
iteration : 1338
train acc:  0.734375
train loss:  0.5288729667663574
train gradient:  0.12524357064177635
iteration : 1339
train acc:  0.6796875
train loss:  0.5514222979545593
train gradient:  0.140809598746289
iteration : 1340
train acc:  0.7734375
train loss:  0.5078858137130737
train gradient:  0.15355485669119406
iteration : 1341
train acc:  0.765625
train loss:  0.4905673563480377
train gradient:  0.1592176392477925
iteration : 1342
train acc:  0.6796875
train loss:  0.5685877799987793
train gradient:  0.18973345977510248
iteration : 1343
train acc:  0.703125
train loss:  0.5620298981666565
train gradient:  0.19473709025530528
iteration : 1344
train acc:  0.6171875
train loss:  0.6568233966827393
train gradient:  0.37782251930164207
iteration : 1345
train acc:  0.7265625
train loss:  0.5469226837158203
train gradient:  0.14300584851978665
iteration : 1346
train acc:  0.6875
train loss:  0.5848169326782227
train gradient:  0.1615450187958705
iteration : 1347
train acc:  0.6796875
train loss:  0.5717940330505371
train gradient:  0.16785246468887616
iteration : 1348
train acc:  0.765625
train loss:  0.5520158410072327
train gradient:  0.15917488608509678
iteration : 1349
train acc:  0.671875
train loss:  0.559673011302948
train gradient:  0.182756210667613
iteration : 1350
train acc:  0.6875
train loss:  0.589732825756073
train gradient:  0.18065793692960844
iteration : 1351
train acc:  0.7109375
train loss:  0.5706379413604736
train gradient:  0.15920986200733822
iteration : 1352
train acc:  0.703125
train loss:  0.573065459728241
train gradient:  0.24154110769450632
iteration : 1353
train acc:  0.734375
train loss:  0.5643659234046936
train gradient:  0.137872481082116
iteration : 1354
train acc:  0.6484375
train loss:  0.5975087881088257
train gradient:  0.16559455413886792
iteration : 1355
train acc:  0.7265625
train loss:  0.5224629640579224
train gradient:  0.15043281066483613
iteration : 1356
train acc:  0.671875
train loss:  0.5939688086509705
train gradient:  0.2461627856077854
iteration : 1357
train acc:  0.7421875
train loss:  0.49432119727134705
train gradient:  0.1310125766316105
iteration : 1358
train acc:  0.703125
train loss:  0.5431634187698364
train gradient:  0.18882095960002077
iteration : 1359
train acc:  0.7109375
train loss:  0.5188711881637573
train gradient:  0.11985398058389768
iteration : 1360
train acc:  0.6953125
train loss:  0.5618555545806885
train gradient:  0.1712806336341769
iteration : 1361
train acc:  0.765625
train loss:  0.5348962545394897
train gradient:  0.16025782323980453
iteration : 1362
train acc:  0.6875
train loss:  0.5928137302398682
train gradient:  0.22381162122682585
iteration : 1363
train acc:  0.6953125
train loss:  0.5702755451202393
train gradient:  0.1945516852206064
iteration : 1364
train acc:  0.7109375
train loss:  0.5544753670692444
train gradient:  0.2566895125139015
iteration : 1365
train acc:  0.6484375
train loss:  0.656204104423523
train gradient:  0.3887325854566287
iteration : 1366
train acc:  0.6796875
train loss:  0.5649702548980713
train gradient:  0.19528926381610057
iteration : 1367
train acc:  0.7421875
train loss:  0.53461754322052
train gradient:  0.22453495778303123
iteration : 1368
train acc:  0.671875
train loss:  0.5868595242500305
train gradient:  0.18174225413339573
iteration : 1369
train acc:  0.7421875
train loss:  0.5598040819168091
train gradient:  0.1835586029155118
iteration : 1370
train acc:  0.6171875
train loss:  0.5960270166397095
train gradient:  0.19027313269986146
iteration : 1371
train acc:  0.6953125
train loss:  0.5925907492637634
train gradient:  0.17562903217055315
iteration : 1372
train acc:  0.6796875
train loss:  0.5843017101287842
train gradient:  0.18403151654495697
iteration : 1373
train acc:  0.8203125
train loss:  0.47351640462875366
train gradient:  0.14021141576362692
iteration : 1374
train acc:  0.7109375
train loss:  0.5674749612808228
train gradient:  0.20087078148967896
iteration : 1375
train acc:  0.6953125
train loss:  0.5433259606361389
train gradient:  0.14251880892656812
iteration : 1376
train acc:  0.671875
train loss:  0.570996880531311
train gradient:  0.18193870983344956
iteration : 1377
train acc:  0.703125
train loss:  0.5715218782424927
train gradient:  0.14248918294807209
iteration : 1378
train acc:  0.7421875
train loss:  0.5508322715759277
train gradient:  0.24333597386761296
iteration : 1379
train acc:  0.7265625
train loss:  0.5395734310150146
train gradient:  0.1804093417231596
iteration : 1380
train acc:  0.71875
train loss:  0.540605902671814
train gradient:  0.19190411073141245
iteration : 1381
train acc:  0.7578125
train loss:  0.5045734643936157
train gradient:  0.1362694481747989
iteration : 1382
train acc:  0.7109375
train loss:  0.5426387190818787
train gradient:  0.14096102508112177
iteration : 1383
train acc:  0.640625
train loss:  0.6336101293563843
train gradient:  0.24991047637029618
iteration : 1384
train acc:  0.7109375
train loss:  0.5628902316093445
train gradient:  0.23259647488218266
iteration : 1385
train acc:  0.7265625
train loss:  0.5290226936340332
train gradient:  0.15105982813005736
iteration : 1386
train acc:  0.65625
train loss:  0.5554518699645996
train gradient:  0.162635140274946
iteration : 1387
train acc:  0.703125
train loss:  0.5653064250946045
train gradient:  0.13989560194311368
iteration : 1388
train acc:  0.71875
train loss:  0.5490291714668274
train gradient:  0.13507208157303707
iteration : 1389
train acc:  0.6875
train loss:  0.5839405059814453
train gradient:  0.1906629872141512
iteration : 1390
train acc:  0.6484375
train loss:  0.5629383325576782
train gradient:  0.15122146843925352
iteration : 1391
train acc:  0.6953125
train loss:  0.5591444373130798
train gradient:  0.2221636191825026
iteration : 1392
train acc:  0.6875
train loss:  0.5705848932266235
train gradient:  0.15513799129056693
iteration : 1393
train acc:  0.7265625
train loss:  0.5550740957260132
train gradient:  0.22967364712999566
iteration : 1394
train acc:  0.75
train loss:  0.5157989859580994
train gradient:  0.14052446444687322
iteration : 1395
train acc:  0.7265625
train loss:  0.5742925405502319
train gradient:  0.16498842652624168
iteration : 1396
train acc:  0.6328125
train loss:  0.6302475333213806
train gradient:  0.1946203196266324
iteration : 1397
train acc:  0.7109375
train loss:  0.545937716960907
train gradient:  0.19000037421385976
iteration : 1398
train acc:  0.6875
train loss:  0.5687188506126404
train gradient:  0.18864134121814105
iteration : 1399
train acc:  0.7265625
train loss:  0.521308183670044
train gradient:  0.16139636074930497
iteration : 1400
train acc:  0.6875
train loss:  0.5955289602279663
train gradient:  0.17867595926136828
iteration : 1401
train acc:  0.7109375
train loss:  0.5713786482810974
train gradient:  0.20240124286779726
iteration : 1402
train acc:  0.6328125
train loss:  0.6475335955619812
train gradient:  0.17290132211815556
iteration : 1403
train acc:  0.625
train loss:  0.6284078359603882
train gradient:  0.16911463715922712
iteration : 1404
train acc:  0.671875
train loss:  0.5746238231658936
train gradient:  0.20019781552724153
iteration : 1405
train acc:  0.671875
train loss:  0.5330200791358948
train gradient:  0.14674047888205385
iteration : 1406
train acc:  0.6875
train loss:  0.5466156005859375
train gradient:  0.1291674075226975
iteration : 1407
train acc:  0.7421875
train loss:  0.5324304699897766
train gradient:  0.17426842968098682
iteration : 1408
train acc:  0.7578125
train loss:  0.5301975011825562
train gradient:  0.19878973431347532
iteration : 1409
train acc:  0.7265625
train loss:  0.5420305728912354
train gradient:  0.20382088119325004
iteration : 1410
train acc:  0.7109375
train loss:  0.4980815649032593
train gradient:  0.13106468835595436
iteration : 1411
train acc:  0.6796875
train loss:  0.5750777125358582
train gradient:  0.21458171170941148
iteration : 1412
train acc:  0.6328125
train loss:  0.5961884260177612
train gradient:  0.16659710672075145
iteration : 1413
train acc:  0.6875
train loss:  0.5705679655075073
train gradient:  0.1629912913793306
iteration : 1414
train acc:  0.7421875
train loss:  0.5392531156539917
train gradient:  0.15435862718297177
iteration : 1415
train acc:  0.6796875
train loss:  0.5827208161354065
train gradient:  0.1614829792807002
iteration : 1416
train acc:  0.6875
train loss:  0.5455420017242432
train gradient:  0.16491672862801232
iteration : 1417
train acc:  0.6953125
train loss:  0.5450179576873779
train gradient:  0.13058297820027218
iteration : 1418
train acc:  0.6875
train loss:  0.5913830399513245
train gradient:  0.17077378666705553
iteration : 1419
train acc:  0.7109375
train loss:  0.5434412956237793
train gradient:  0.21002469991331027
iteration : 1420
train acc:  0.7109375
train loss:  0.531312108039856
train gradient:  0.18747275839715383
iteration : 1421
train acc:  0.703125
train loss:  0.5933418273925781
train gradient:  0.19472537465878773
iteration : 1422
train acc:  0.7109375
train loss:  0.563746452331543
train gradient:  0.17819511551386125
iteration : 1423
train acc:  0.6640625
train loss:  0.5896434783935547
train gradient:  0.24897362130323192
iteration : 1424
train acc:  0.7421875
train loss:  0.5454933643341064
train gradient:  0.13656772023992825
iteration : 1425
train acc:  0.6796875
train loss:  0.5437862277030945
train gradient:  0.13247785602370846
iteration : 1426
train acc:  0.6640625
train loss:  0.6098495721817017
train gradient:  0.17606398535225826
iteration : 1427
train acc:  0.6953125
train loss:  0.531293511390686
train gradient:  0.11843545522601205
iteration : 1428
train acc:  0.65625
train loss:  0.6271716952323914
train gradient:  0.18632417357011907
iteration : 1429
train acc:  0.6875
train loss:  0.5568846464157104
train gradient:  0.12760456965816946
iteration : 1430
train acc:  0.6875
train loss:  0.5799843072891235
train gradient:  0.17102304032780835
iteration : 1431
train acc:  0.6484375
train loss:  0.6071346998214722
train gradient:  0.28429148356245826
iteration : 1432
train acc:  0.7265625
train loss:  0.5154610276222229
train gradient:  0.1673826492324742
iteration : 1433
train acc:  0.671875
train loss:  0.5580970644950867
train gradient:  0.17646843065410922
iteration : 1434
train acc:  0.71875
train loss:  0.5288851261138916
train gradient:  0.13421868152551003
iteration : 1435
train acc:  0.7265625
train loss:  0.5234586000442505
train gradient:  0.16570895948137626
iteration : 1436
train acc:  0.6796875
train loss:  0.5659399032592773
train gradient:  0.16685985628763797
iteration : 1437
train acc:  0.7265625
train loss:  0.5340597629547119
train gradient:  0.1360560411343465
iteration : 1438
train acc:  0.6875
train loss:  0.5829254388809204
train gradient:  0.1792548170728668
iteration : 1439
train acc:  0.75
train loss:  0.5508646965026855
train gradient:  0.20974432117475517
iteration : 1440
train acc:  0.6484375
train loss:  0.5993449687957764
train gradient:  0.2195480027749623
iteration : 1441
train acc:  0.6796875
train loss:  0.5812662243843079
train gradient:  0.1557629066518442
iteration : 1442
train acc:  0.640625
train loss:  0.608925998210907
train gradient:  0.17890373432562073
iteration : 1443
train acc:  0.6953125
train loss:  0.5757328867912292
train gradient:  0.17784718219977458
iteration : 1444
train acc:  0.640625
train loss:  0.6092085838317871
train gradient:  0.18925053385878254
iteration : 1445
train acc:  0.71875
train loss:  0.6552457213401794
train gradient:  0.2401079294164848
iteration : 1446
train acc:  0.703125
train loss:  0.5490348935127258
train gradient:  0.17021091945557631
iteration : 1447
train acc:  0.6953125
train loss:  0.5630959272384644
train gradient:  0.15172484623409516
iteration : 1448
train acc:  0.7265625
train loss:  0.526856541633606
train gradient:  0.2000553007087346
iteration : 1449
train acc:  0.7578125
train loss:  0.5704272985458374
train gradient:  0.13332180031099689
iteration : 1450
train acc:  0.71875
train loss:  0.5941880941390991
train gradient:  0.1725404387034884
iteration : 1451
train acc:  0.6953125
train loss:  0.5511569976806641
train gradient:  0.14144686832664838
iteration : 1452
train acc:  0.71875
train loss:  0.5327180027961731
train gradient:  0.13184727209103803
iteration : 1453
train acc:  0.640625
train loss:  0.5640984773635864
train gradient:  0.1919887629106362
iteration : 1454
train acc:  0.671875
train loss:  0.5834164023399353
train gradient:  0.1881387480835169
iteration : 1455
train acc:  0.6640625
train loss:  0.5797181129455566
train gradient:  0.21128609806818718
iteration : 1456
train acc:  0.734375
train loss:  0.5251395106315613
train gradient:  0.13203183521239814
iteration : 1457
train acc:  0.65625
train loss:  0.6067644357681274
train gradient:  0.18032597105898082
iteration : 1458
train acc:  0.671875
train loss:  0.5479365587234497
train gradient:  0.2169897371154138
iteration : 1459
train acc:  0.6953125
train loss:  0.5446208715438843
train gradient:  0.17197122720541191
iteration : 1460
train acc:  0.7109375
train loss:  0.5311048626899719
train gradient:  0.14031453446399347
iteration : 1461
train acc:  0.7421875
train loss:  0.5036674737930298
train gradient:  0.14953134639644314
iteration : 1462
train acc:  0.65625
train loss:  0.5704199075698853
train gradient:  0.15745212118272556
iteration : 1463
train acc:  0.671875
train loss:  0.558017373085022
train gradient:  0.1662274048591053
iteration : 1464
train acc:  0.6953125
train loss:  0.5806715488433838
train gradient:  0.19299689172164242
iteration : 1465
train acc:  0.7109375
train loss:  0.5413439273834229
train gradient:  0.14880260730330053
iteration : 1466
train acc:  0.78125
train loss:  0.5378397703170776
train gradient:  0.1227074477390416
iteration : 1467
train acc:  0.6796875
train loss:  0.6095542907714844
train gradient:  0.18996214841701753
iteration : 1468
train acc:  0.734375
train loss:  0.5363572239875793
train gradient:  0.17370464182073153
iteration : 1469
train acc:  0.734375
train loss:  0.5197869539260864
train gradient:  0.12685954301555102
iteration : 1470
train acc:  0.78125
train loss:  0.4941796362400055
train gradient:  0.14473525517812683
iteration : 1471
train acc:  0.7109375
train loss:  0.5696085691452026
train gradient:  0.17139083961055626
iteration : 1472
train acc:  0.75
train loss:  0.5440717339515686
train gradient:  0.12684405463077206
iteration : 1473
train acc:  0.6484375
train loss:  0.5804858803749084
train gradient:  0.15505100240148428
iteration : 1474
train acc:  0.6796875
train loss:  0.5601330399513245
train gradient:  0.16761731453537304
iteration : 1475
train acc:  0.703125
train loss:  0.49712073802948
train gradient:  0.1330574180824518
iteration : 1476
train acc:  0.6484375
train loss:  0.5860458612442017
train gradient:  0.1437603851253256
iteration : 1477
train acc:  0.7109375
train loss:  0.54972904920578
train gradient:  0.1555520470579551
iteration : 1478
train acc:  0.7734375
train loss:  0.4982042610645294
train gradient:  0.12718990361250757
iteration : 1479
train acc:  0.6328125
train loss:  0.6065438985824585
train gradient:  0.15266086166653156
iteration : 1480
train acc:  0.7421875
train loss:  0.5144374370574951
train gradient:  0.1560989810469837
iteration : 1481
train acc:  0.7421875
train loss:  0.5058404207229614
train gradient:  0.14624735647747714
iteration : 1482
train acc:  0.6796875
train loss:  0.5664229393005371
train gradient:  0.14055371898292113
iteration : 1483
train acc:  0.71875
train loss:  0.5560799241065979
train gradient:  0.17661806131274188
iteration : 1484
train acc:  0.703125
train loss:  0.5287498235702515
train gradient:  0.1285165925960311
iteration : 1485
train acc:  0.7578125
train loss:  0.5302300453186035
train gradient:  0.1820338989982544
iteration : 1486
train acc:  0.734375
train loss:  0.4797969460487366
train gradient:  0.17030505096642262
iteration : 1487
train acc:  0.65625
train loss:  0.5870538949966431
train gradient:  0.19930014991388922
iteration : 1488
train acc:  0.703125
train loss:  0.5826662182807922
train gradient:  0.22044016197245364
iteration : 1489
train acc:  0.703125
train loss:  0.5251168012619019
train gradient:  0.11380512244636999
iteration : 1490
train acc:  0.703125
train loss:  0.5726543068885803
train gradient:  0.1747869046948864
iteration : 1491
train acc:  0.6796875
train loss:  0.5883316993713379
train gradient:  0.17276636672567292
iteration : 1492
train acc:  0.65625
train loss:  0.602273166179657
train gradient:  0.16843352388992816
iteration : 1493
train acc:  0.7421875
train loss:  0.5018962621688843
train gradient:  0.22445370360760153
iteration : 1494
train acc:  0.6640625
train loss:  0.6027217507362366
train gradient:  0.1623115740107628
iteration : 1495
train acc:  0.6875
train loss:  0.5770924091339111
train gradient:  0.16293558113381892
iteration : 1496
train acc:  0.71875
train loss:  0.5549489855766296
train gradient:  0.1642755614800514
iteration : 1497
train acc:  0.734375
train loss:  0.5593531131744385
train gradient:  0.17219828871780218
iteration : 1498
train acc:  0.765625
train loss:  0.5033934116363525
train gradient:  0.14140961028361906
iteration : 1499
train acc:  0.703125
train loss:  0.5656303763389587
train gradient:  0.16947278613685
iteration : 1500
train acc:  0.75
train loss:  0.5297272205352783
train gradient:  0.16383946964747043
iteration : 1501
train acc:  0.65625
train loss:  0.590979278087616
train gradient:  0.20548935522313921
iteration : 1502
train acc:  0.7421875
train loss:  0.5555840134620667
train gradient:  0.18963669093094634
iteration : 1503
train acc:  0.65625
train loss:  0.6076233983039856
train gradient:  0.16215477114314744
iteration : 1504
train acc:  0.640625
train loss:  0.6291161775588989
train gradient:  0.2944445976413465
iteration : 1505
train acc:  0.6875
train loss:  0.6053013205528259
train gradient:  0.17551116966930647
iteration : 1506
train acc:  0.7265625
train loss:  0.5409794449806213
train gradient:  0.20046623646511103
iteration : 1507
train acc:  0.7578125
train loss:  0.5163865089416504
train gradient:  0.12502512184608772
iteration : 1508
train acc:  0.6953125
train loss:  0.5854083895683289
train gradient:  0.20091009073878846
iteration : 1509
train acc:  0.6484375
train loss:  0.5950573086738586
train gradient:  0.1824661853925872
iteration : 1510
train acc:  0.6328125
train loss:  0.6199515461921692
train gradient:  0.20274033519783202
iteration : 1511
train acc:  0.6796875
train loss:  0.610443651676178
train gradient:  0.15930930067710977
iteration : 1512
train acc:  0.671875
train loss:  0.6084500551223755
train gradient:  0.21765233172293325
iteration : 1513
train acc:  0.75
train loss:  0.5463066101074219
train gradient:  0.15057796119902894
iteration : 1514
train acc:  0.71875
train loss:  0.5525391697883606
train gradient:  0.12536252013630755
iteration : 1515
train acc:  0.6171875
train loss:  0.583759605884552
train gradient:  0.16790707443176703
iteration : 1516
train acc:  0.6796875
train loss:  0.611772358417511
train gradient:  0.19971335273705787
iteration : 1517
train acc:  0.75
train loss:  0.5227193236351013
train gradient:  0.1633471685610069
iteration : 1518
train acc:  0.7109375
train loss:  0.5069688558578491
train gradient:  0.13719028416929435
iteration : 1519
train acc:  0.734375
train loss:  0.5126984119415283
train gradient:  0.14214083509636805
iteration : 1520
train acc:  0.6875
train loss:  0.5636008977890015
train gradient:  0.13320798769858683
iteration : 1521
train acc:  0.6640625
train loss:  0.605586051940918
train gradient:  0.19270938295487797
iteration : 1522
train acc:  0.6796875
train loss:  0.5512841939926147
train gradient:  0.13260613253038717
iteration : 1523
train acc:  0.734375
train loss:  0.5522118210792542
train gradient:  0.13443390168342315
iteration : 1524
train acc:  0.7109375
train loss:  0.5356297492980957
train gradient:  0.15972778574980545
iteration : 1525
train acc:  0.75
train loss:  0.5408903360366821
train gradient:  0.1485295148221022
iteration : 1526
train acc:  0.6953125
train loss:  0.5431768894195557
train gradient:  0.12875576066996447
iteration : 1527
train acc:  0.7109375
train loss:  0.525176465511322
train gradient:  0.13701629667481166
iteration : 1528
train acc:  0.703125
train loss:  0.5173322558403015
train gradient:  0.14506574709970765
iteration : 1529
train acc:  0.640625
train loss:  0.6152178049087524
train gradient:  0.19559544086926375
iteration : 1530
train acc:  0.7421875
train loss:  0.5461305379867554
train gradient:  0.16622936459209536
iteration : 1531
train acc:  0.734375
train loss:  0.5659719705581665
train gradient:  0.17633758326021676
iteration : 1532
train acc:  0.75
train loss:  0.5165228843688965
train gradient:  0.15562938570527318
iteration : 1533
train acc:  0.7734375
train loss:  0.5050172805786133
train gradient:  0.1516476465954787
iteration : 1534
train acc:  0.609375
train loss:  0.6059167385101318
train gradient:  0.1763017553495902
iteration : 1535
train acc:  0.7265625
train loss:  0.5517566204071045
train gradient:  0.1391394996781924
iteration : 1536
train acc:  0.7265625
train loss:  0.5394607782363892
train gradient:  0.14237983118529973
iteration : 1537
train acc:  0.71875
train loss:  0.537865936756134
train gradient:  0.1844458418834643
iteration : 1538
train acc:  0.75
train loss:  0.5148606300354004
train gradient:  0.14155265382328366
iteration : 1539
train acc:  0.75
train loss:  0.5343292951583862
train gradient:  0.168111731712103
iteration : 1540
train acc:  0.6796875
train loss:  0.5535604953765869
train gradient:  0.20495446966049363
iteration : 1541
train acc:  0.6875
train loss:  0.5836727023124695
train gradient:  0.22216624554383327
iteration : 1542
train acc:  0.7109375
train loss:  0.6100831031799316
train gradient:  0.23387671768281817
iteration : 1543
train acc:  0.609375
train loss:  0.6461558938026428
train gradient:  0.2683711829117426
iteration : 1544
train acc:  0.7578125
train loss:  0.5070456266403198
train gradient:  0.14098933596466262
iteration : 1545
train acc:  0.71875
train loss:  0.5625711679458618
train gradient:  0.16599357356608183
iteration : 1546
train acc:  0.7421875
train loss:  0.48967456817626953
train gradient:  0.1281554761721522
iteration : 1547
train acc:  0.7109375
train loss:  0.5420968532562256
train gradient:  0.16166029802474913
iteration : 1548
train acc:  0.7421875
train loss:  0.4953587055206299
train gradient:  0.20199257297043538
iteration : 1549
train acc:  0.65625
train loss:  0.5834609270095825
train gradient:  0.15155563010507003
iteration : 1550
train acc:  0.65625
train loss:  0.6119678616523743
train gradient:  0.22030729938738225
iteration : 1551
train acc:  0.734375
train loss:  0.5204031467437744
train gradient:  0.142977498979932
iteration : 1552
train acc:  0.6640625
train loss:  0.6130824089050293
train gradient:  0.2626642353035163
iteration : 1553
train acc:  0.703125
train loss:  0.5655843615531921
train gradient:  0.1761693499875168
iteration : 1554
train acc:  0.6328125
train loss:  0.6287308931350708
train gradient:  0.2122437971462064
iteration : 1555
train acc:  0.7421875
train loss:  0.51874840259552
train gradient:  0.16344599043253905
iteration : 1556
train acc:  0.7109375
train loss:  0.5425194501876831
train gradient:  0.1628607751493602
iteration : 1557
train acc:  0.625
train loss:  0.6235228776931763
train gradient:  0.17956644585066253
iteration : 1558
train acc:  0.671875
train loss:  0.5972185134887695
train gradient:  0.21381761676775285
iteration : 1559
train acc:  0.734375
train loss:  0.5844457745552063
train gradient:  0.2459481171882798
iteration : 1560
train acc:  0.6796875
train loss:  0.5974298715591431
train gradient:  0.22675114194479984
iteration : 1561
train acc:  0.6640625
train loss:  0.6066595315933228
train gradient:  0.23230208718731626
iteration : 1562
train acc:  0.765625
train loss:  0.5089846849441528
train gradient:  0.12464042196131686
iteration : 1563
train acc:  0.6796875
train loss:  0.5682550072669983
train gradient:  0.18965067995497897
iteration : 1564
train acc:  0.609375
train loss:  0.650830864906311
train gradient:  0.23876763870413065
iteration : 1565
train acc:  0.6953125
train loss:  0.5552911758422852
train gradient:  0.24985127538412805
iteration : 1566
train acc:  0.703125
train loss:  0.5752131938934326
train gradient:  0.17141405152772954
iteration : 1567
train acc:  0.7421875
train loss:  0.5043359994888306
train gradient:  0.1177760631361281
iteration : 1568
train acc:  0.7265625
train loss:  0.5110911130905151
train gradient:  0.15069535140822204
iteration : 1569
train acc:  0.65625
train loss:  0.6135454177856445
train gradient:  0.2079226778776359
iteration : 1570
train acc:  0.6796875
train loss:  0.5457091331481934
train gradient:  0.18554638001553017
iteration : 1571
train acc:  0.546875
train loss:  0.6956502199172974
train gradient:  0.3127348792707352
iteration : 1572
train acc:  0.765625
train loss:  0.5199201107025146
train gradient:  0.19727107914386854
iteration : 1573
train acc:  0.75
train loss:  0.5324766039848328
train gradient:  0.16791656528897714
iteration : 1574
train acc:  0.6796875
train loss:  0.5628145337104797
train gradient:  0.17517828887247677
iteration : 1575
train acc:  0.65625
train loss:  0.5834582448005676
train gradient:  0.1822316769538856
iteration : 1576
train acc:  0.6953125
train loss:  0.5137141346931458
train gradient:  0.175656453399478
iteration : 1577
train acc:  0.65625
train loss:  0.5741481781005859
train gradient:  0.2832476127688984
iteration : 1578
train acc:  0.6796875
train loss:  0.5459367036819458
train gradient:  0.14407329415140607
iteration : 1579
train acc:  0.6484375
train loss:  0.608279824256897
train gradient:  0.21524923180265162
iteration : 1580
train acc:  0.71875
train loss:  0.5375614166259766
train gradient:  0.14434238404052507
iteration : 1581
train acc:  0.65625
train loss:  0.5833838582038879
train gradient:  0.16835095409571454
iteration : 1582
train acc:  0.7578125
train loss:  0.5382398366928101
train gradient:  0.1469110071748368
iteration : 1583
train acc:  0.6953125
train loss:  0.540481686592102
train gradient:  0.1649927456573146
iteration : 1584
train acc:  0.703125
train loss:  0.5807921886444092
train gradient:  0.1627574706318094
iteration : 1585
train acc:  0.65625
train loss:  0.5922533869743347
train gradient:  0.18174904617272386
iteration : 1586
train acc:  0.71875
train loss:  0.5675517320632935
train gradient:  0.13664654490862888
iteration : 1587
train acc:  0.71875
train loss:  0.5389865636825562
train gradient:  0.15377503150963898
iteration : 1588
train acc:  0.6640625
train loss:  0.6157898902893066
train gradient:  0.21671669781017683
iteration : 1589
train acc:  0.7265625
train loss:  0.5500290989875793
train gradient:  0.15420566229421362
iteration : 1590
train acc:  0.6953125
train loss:  0.5831371545791626
train gradient:  0.1333637749104492
iteration : 1591
train acc:  0.578125
train loss:  0.64960777759552
train gradient:  0.2047097448630141
iteration : 1592
train acc:  0.703125
train loss:  0.5856319665908813
train gradient:  0.1774352497752485
iteration : 1593
train acc:  0.765625
train loss:  0.48938119411468506
train gradient:  0.18016243756558845
iteration : 1594
train acc:  0.6328125
train loss:  0.6066578030586243
train gradient:  0.20665545552957815
iteration : 1595
train acc:  0.6484375
train loss:  0.5950279235839844
train gradient:  0.15536662842483837
iteration : 1596
train acc:  0.671875
train loss:  0.5717991590499878
train gradient:  0.13687837663417
iteration : 1597
train acc:  0.7265625
train loss:  0.5227696895599365
train gradient:  0.2707999149615608
iteration : 1598
train acc:  0.71875
train loss:  0.5074262619018555
train gradient:  0.15661531860878064
iteration : 1599
train acc:  0.6796875
train loss:  0.5463346838951111
train gradient:  0.16482743666929217
iteration : 1600
train acc:  0.6796875
train loss:  0.573823094367981
train gradient:  0.15350083180978685
iteration : 1601
train acc:  0.6796875
train loss:  0.5594794154167175
train gradient:  0.1729497077682055
iteration : 1602
train acc:  0.6640625
train loss:  0.5792778730392456
train gradient:  0.14626964988694718
iteration : 1603
train acc:  0.6875
train loss:  0.5606083869934082
train gradient:  0.1366174405810383
iteration : 1604
train acc:  0.734375
train loss:  0.5545912384986877
train gradient:  0.13451033404589624
iteration : 1605
train acc:  0.6875
train loss:  0.5581775903701782
train gradient:  0.1717396543611972
iteration : 1606
train acc:  0.6953125
train loss:  0.5899547934532166
train gradient:  0.14616126298006404
iteration : 1607
train acc:  0.7265625
train loss:  0.5293904542922974
train gradient:  0.15462626657877324
iteration : 1608
train acc:  0.71875
train loss:  0.5444072484970093
train gradient:  0.111879903122625
iteration : 1609
train acc:  0.625
train loss:  0.6565640568733215
train gradient:  0.2751625363182635
iteration : 1610
train acc:  0.640625
train loss:  0.6293837428092957
train gradient:  0.16508912505456735
iteration : 1611
train acc:  0.71875
train loss:  0.5533514022827148
train gradient:  0.1348474913185106
iteration : 1612
train acc:  0.6796875
train loss:  0.5653911232948303
train gradient:  0.15867972086377669
iteration : 1613
train acc:  0.6484375
train loss:  0.5775616765022278
train gradient:  0.18269183690560703
iteration : 1614
train acc:  0.7265625
train loss:  0.5385831594467163
train gradient:  0.1768334739629508
iteration : 1615
train acc:  0.703125
train loss:  0.5355815291404724
train gradient:  0.13859952333017178
iteration : 1616
train acc:  0.7421875
train loss:  0.5374641418457031
train gradient:  0.17418304280124203
iteration : 1617
train acc:  0.703125
train loss:  0.537566602230072
train gradient:  0.13515706629367563
iteration : 1618
train acc:  0.71875
train loss:  0.5538836717605591
train gradient:  0.14996785193398815
iteration : 1619
train acc:  0.71875
train loss:  0.5409637689590454
train gradient:  0.16141535321871453
iteration : 1620
train acc:  0.7421875
train loss:  0.5297342538833618
train gradient:  0.14464805564365957
iteration : 1621
train acc:  0.7890625
train loss:  0.5237787961959839
train gradient:  0.13423453495125562
iteration : 1622
train acc:  0.6953125
train loss:  0.5487394332885742
train gradient:  0.1337765584952651
iteration : 1623
train acc:  0.734375
train loss:  0.49876320362091064
train gradient:  0.17559823672483865
iteration : 1624
train acc:  0.6953125
train loss:  0.5825748443603516
train gradient:  0.17289933663858262
iteration : 1625
train acc:  0.7890625
train loss:  0.4659445285797119
train gradient:  0.13217943848226515
iteration : 1626
train acc:  0.625
train loss:  0.591251790523529
train gradient:  0.16593869407050077
iteration : 1627
train acc:  0.7109375
train loss:  0.5702924728393555
train gradient:  0.18731228642607256
iteration : 1628
train acc:  0.6640625
train loss:  0.6190087795257568
train gradient:  0.1844670121307384
iteration : 1629
train acc:  0.71875
train loss:  0.5880623459815979
train gradient:  0.22415092052420027
iteration : 1630
train acc:  0.703125
train loss:  0.5264369249343872
train gradient:  0.17616156485429546
iteration : 1631
train acc:  0.7265625
train loss:  0.5584548711776733
train gradient:  0.16927959534096237
iteration : 1632
train acc:  0.765625
train loss:  0.5005506277084351
train gradient:  0.1469060836334605
iteration : 1633
train acc:  0.703125
train loss:  0.581557035446167
train gradient:  0.15697989532664966
iteration : 1634
train acc:  0.75
train loss:  0.5022200345993042
train gradient:  0.14549818108487328
iteration : 1635
train acc:  0.7109375
train loss:  0.5103874206542969
train gradient:  0.12064467878538342
iteration : 1636
train acc:  0.71875
train loss:  0.5720503330230713
train gradient:  0.16806553502463867
iteration : 1637
train acc:  0.671875
train loss:  0.567594051361084
train gradient:  0.19251597732340886
iteration : 1638
train acc:  0.765625
train loss:  0.5289533734321594
train gradient:  0.14861169511386196
iteration : 1639
train acc:  0.7265625
train loss:  0.5518613457679749
train gradient:  0.155299212069519
iteration : 1640
train acc:  0.703125
train loss:  0.5681460499763489
train gradient:  0.17691205599370666
iteration : 1641
train acc:  0.671875
train loss:  0.5421696901321411
train gradient:  0.11948343005090417
iteration : 1642
train acc:  0.796875
train loss:  0.4755594730377197
train gradient:  0.17699075930505237
iteration : 1643
train acc:  0.6796875
train loss:  0.5574446320533752
train gradient:  0.1359881287266289
iteration : 1644
train acc:  0.71875
train loss:  0.592903733253479
train gradient:  0.16502560264305272
iteration : 1645
train acc:  0.734375
train loss:  0.5151216387748718
train gradient:  0.1744034091617732
iteration : 1646
train acc:  0.6640625
train loss:  0.5507915019989014
train gradient:  0.12342496749161751
iteration : 1647
train acc:  0.6796875
train loss:  0.5235041975975037
train gradient:  0.17112075723224157
iteration : 1648
train acc:  0.734375
train loss:  0.5376715660095215
train gradient:  0.168939918824405
iteration : 1649
train acc:  0.734375
train loss:  0.5085347890853882
train gradient:  0.11243380011110074
iteration : 1650
train acc:  0.703125
train loss:  0.5710525512695312
train gradient:  0.18689750442337313
iteration : 1651
train acc:  0.703125
train loss:  0.5398948192596436
train gradient:  0.15819234967087326
iteration : 1652
train acc:  0.7109375
train loss:  0.5512017607688904
train gradient:  0.19791802719038296
iteration : 1653
train acc:  0.703125
train loss:  0.5653313398361206
train gradient:  0.16261741591246653
iteration : 1654
train acc:  0.6484375
train loss:  0.5713223814964294
train gradient:  0.1617672048386727
iteration : 1655
train acc:  0.71875
train loss:  0.515949010848999
train gradient:  0.12870860535666745
iteration : 1656
train acc:  0.703125
train loss:  0.5408952236175537
train gradient:  0.15079455795969782
iteration : 1657
train acc:  0.7109375
train loss:  0.5583219528198242
train gradient:  0.17772715754049714
iteration : 1658
train acc:  0.71875
train loss:  0.5595493316650391
train gradient:  0.27506930003809077
iteration : 1659
train acc:  0.7421875
train loss:  0.5293271541595459
train gradient:  0.14346404577525226
iteration : 1660
train acc:  0.6875
train loss:  0.5502065420150757
train gradient:  0.14543550479385364
iteration : 1661
train acc:  0.6328125
train loss:  0.6011728644371033
train gradient:  0.253923328606533
iteration : 1662
train acc:  0.7109375
train loss:  0.5610626339912415
train gradient:  0.197484624159047
iteration : 1663
train acc:  0.671875
train loss:  0.5924910306930542
train gradient:  0.3719473599565278
iteration : 1664
train acc:  0.6484375
train loss:  0.5896316766738892
train gradient:  0.17841168001562568
iteration : 1665
train acc:  0.6953125
train loss:  0.5236729383468628
train gradient:  0.11106724792447936
iteration : 1666
train acc:  0.6484375
train loss:  0.6075709462165833
train gradient:  0.2663446665124718
iteration : 1667
train acc:  0.65625
train loss:  0.5926841497421265
train gradient:  0.19632502362745563
iteration : 1668
train acc:  0.8046875
train loss:  0.46518540382385254
train gradient:  0.14793326986319286
iteration : 1669
train acc:  0.65625
train loss:  0.5475655794143677
train gradient:  0.14704762423628231
iteration : 1670
train acc:  0.6640625
train loss:  0.5811388492584229
train gradient:  0.20402743737446744
iteration : 1671
train acc:  0.7109375
train loss:  0.5106250047683716
train gradient:  0.1194242142820053
iteration : 1672
train acc:  0.6875
train loss:  0.5933769941329956
train gradient:  0.2454707375708045
iteration : 1673
train acc:  0.671875
train loss:  0.5640018582344055
train gradient:  0.2329318053882478
iteration : 1674
train acc:  0.625
train loss:  0.6611056327819824
train gradient:  0.2541848998805233
iteration : 1675
train acc:  0.7265625
train loss:  0.5218915343284607
train gradient:  0.1662861350241518
iteration : 1676
train acc:  0.7265625
train loss:  0.51979660987854
train gradient:  0.17820683679842242
iteration : 1677
train acc:  0.71875
train loss:  0.5532540082931519
train gradient:  0.13703880274514774
iteration : 1678
train acc:  0.6640625
train loss:  0.6096823215484619
train gradient:  0.2002600359663523
iteration : 1679
train acc:  0.6953125
train loss:  0.5395056009292603
train gradient:  0.17507232228864927
iteration : 1680
train acc:  0.640625
train loss:  0.6041431427001953
train gradient:  0.22775755823523036
iteration : 1681
train acc:  0.7109375
train loss:  0.5335289835929871
train gradient:  0.12099638945994416
iteration : 1682
train acc:  0.703125
train loss:  0.5784292221069336
train gradient:  0.21928281555261664
iteration : 1683
train acc:  0.7109375
train loss:  0.5670282244682312
train gradient:  0.18205956237208798
iteration : 1684
train acc:  0.6953125
train loss:  0.553756594657898
train gradient:  0.16717281493017197
iteration : 1685
train acc:  0.7734375
train loss:  0.5153892040252686
train gradient:  0.13636723666012684
iteration : 1686
train acc:  0.6484375
train loss:  0.5832668542861938
train gradient:  0.16374330441327323
iteration : 1687
train acc:  0.703125
train loss:  0.5274022817611694
train gradient:  0.1709670603201655
iteration : 1688
train acc:  0.734375
train loss:  0.51127028465271
train gradient:  0.16436704557821236
iteration : 1689
train acc:  0.7421875
train loss:  0.5388085842132568
train gradient:  0.1515676160173865
iteration : 1690
train acc:  0.7265625
train loss:  0.569098174571991
train gradient:  0.21678300000543121
iteration : 1691
train acc:  0.734375
train loss:  0.5135098695755005
train gradient:  0.17837343599832572
iteration : 1692
train acc:  0.7109375
train loss:  0.5652865171432495
train gradient:  0.17519731729607357
iteration : 1693
train acc:  0.6796875
train loss:  0.6116883754730225
train gradient:  0.21299690256335158
iteration : 1694
train acc:  0.6484375
train loss:  0.5630860924720764
train gradient:  0.1551525353124189
iteration : 1695
train acc:  0.7109375
train loss:  0.549687385559082
train gradient:  0.14921097393176896
iteration : 1696
train acc:  0.6796875
train loss:  0.5356762409210205
train gradient:  0.14284252644543566
iteration : 1697
train acc:  0.703125
train loss:  0.548412561416626
train gradient:  0.14896516813367125
iteration : 1698
train acc:  0.71875
train loss:  0.5481042861938477
train gradient:  0.1444014236263457
iteration : 1699
train acc:  0.765625
train loss:  0.5189482569694519
train gradient:  0.14619002327631933
iteration : 1700
train acc:  0.765625
train loss:  0.5276824235916138
train gradient:  0.14799146361278756
iteration : 1701
train acc:  0.71875
train loss:  0.5527432560920715
train gradient:  0.14537785843427578
iteration : 1702
train acc:  0.6953125
train loss:  0.571155309677124
train gradient:  0.1722194208713962
iteration : 1703
train acc:  0.6796875
train loss:  0.5770865678787231
train gradient:  0.19498314617637785
iteration : 1704
train acc:  0.734375
train loss:  0.5410109162330627
train gradient:  0.13889121307927305
iteration : 1705
train acc:  0.7265625
train loss:  0.5253605842590332
train gradient:  0.1845291017088957
iteration : 1706
train acc:  0.7421875
train loss:  0.5554007291793823
train gradient:  0.13654831837922626
iteration : 1707
train acc:  0.7890625
train loss:  0.5083035230636597
train gradient:  0.1441535212929944
iteration : 1708
train acc:  0.6640625
train loss:  0.5693797469139099
train gradient:  0.21819389600723316
iteration : 1709
train acc:  0.7421875
train loss:  0.48314863443374634
train gradient:  0.12962262790859072
iteration : 1710
train acc:  0.7578125
train loss:  0.5771138072013855
train gradient:  0.3094878720616988
iteration : 1711
train acc:  0.765625
train loss:  0.5135270953178406
train gradient:  0.16357682792737333
iteration : 1712
train acc:  0.7265625
train loss:  0.569217324256897
train gradient:  0.1619639743627515
iteration : 1713
train acc:  0.7421875
train loss:  0.48982420563697815
train gradient:  0.22813066899160192
iteration : 1714
train acc:  0.7421875
train loss:  0.5652327537536621
train gradient:  0.2140593248270019
iteration : 1715
train acc:  0.75
train loss:  0.48001083731651306
train gradient:  0.1500061190318202
iteration : 1716
train acc:  0.75
train loss:  0.537773847579956
train gradient:  0.18821482946775864
iteration : 1717
train acc:  0.671875
train loss:  0.5413081645965576
train gradient:  0.14456481249687347
iteration : 1718
train acc:  0.65625
train loss:  0.5612983107566833
train gradient:  0.15210104157992665
iteration : 1719
train acc:  0.6796875
train loss:  0.57464599609375
train gradient:  0.14875224560354233
iteration : 1720
train acc:  0.6875
train loss:  0.5602578520774841
train gradient:  0.13484076521186217
iteration : 1721
train acc:  0.71875
train loss:  0.5515367984771729
train gradient:  0.2197665668534196
iteration : 1722
train acc:  0.7265625
train loss:  0.5287454128265381
train gradient:  0.18596588613266618
iteration : 1723
train acc:  0.734375
train loss:  0.5376639366149902
train gradient:  0.15872443063557398
iteration : 1724
train acc:  0.6015625
train loss:  0.5897411704063416
train gradient:  0.20539309955337431
iteration : 1725
train acc:  0.7421875
train loss:  0.5357608199119568
train gradient:  0.25187446027822313
iteration : 1726
train acc:  0.6328125
train loss:  0.5785155296325684
train gradient:  0.15879783042940238
iteration : 1727
train acc:  0.703125
train loss:  0.5288392901420593
train gradient:  0.17115905812151416
iteration : 1728
train acc:  0.6640625
train loss:  0.5894611477851868
train gradient:  0.2756789906439067
iteration : 1729
train acc:  0.703125
train loss:  0.547201931476593
train gradient:  0.17488512656046706
iteration : 1730
train acc:  0.6796875
train loss:  0.5608667135238647
train gradient:  0.17477125010928302
iteration : 1731
train acc:  0.71875
train loss:  0.5292215347290039
train gradient:  0.18613024242134885
iteration : 1732
train acc:  0.6640625
train loss:  0.6071109771728516
train gradient:  0.1740856020155746
iteration : 1733
train acc:  0.6796875
train loss:  0.5806865096092224
train gradient:  0.16963898967670388
iteration : 1734
train acc:  0.71875
train loss:  0.5713750123977661
train gradient:  0.13362558828126603
iteration : 1735
train acc:  0.7890625
train loss:  0.46373364329338074
train gradient:  0.18368868153372678
iteration : 1736
train acc:  0.7421875
train loss:  0.5302928686141968
train gradient:  0.15693566131614722
iteration : 1737
train acc:  0.6796875
train loss:  0.6240395307540894
train gradient:  0.15650896854337298
iteration : 1738
train acc:  0.6875
train loss:  0.5590910315513611
train gradient:  0.17243882652207748
iteration : 1739
train acc:  0.6796875
train loss:  0.57143235206604
train gradient:  0.2479784381678651
iteration : 1740
train acc:  0.7109375
train loss:  0.5445429682731628
train gradient:  0.22926947484650112
iteration : 1741
train acc:  0.6875
train loss:  0.517888069152832
train gradient:  0.16969976205042475
iteration : 1742
train acc:  0.6328125
train loss:  0.5666194558143616
train gradient:  0.14300766332187417
iteration : 1743
train acc:  0.6796875
train loss:  0.5708431005477905
train gradient:  0.16474450259802642
iteration : 1744
train acc:  0.671875
train loss:  0.6025331616401672
train gradient:  0.2115847153279356
iteration : 1745
train acc:  0.71875
train loss:  0.5073254108428955
train gradient:  0.1388296961117294
iteration : 1746
train acc:  0.6953125
train loss:  0.5312764048576355
train gradient:  0.15232820072091463
iteration : 1747
train acc:  0.71875
train loss:  0.5258502960205078
train gradient:  0.17991135330447738
iteration : 1748
train acc:  0.6953125
train loss:  0.6039911508560181
train gradient:  0.28671724155377126
iteration : 1749
train acc:  0.7421875
train loss:  0.49141883850097656
train gradient:  0.18489880130269015
iteration : 1750
train acc:  0.6484375
train loss:  0.603084921836853
train gradient:  0.19328113518773204
iteration : 1751
train acc:  0.65625
train loss:  0.5584689974784851
train gradient:  0.1876992106665658
iteration : 1752
train acc:  0.6796875
train loss:  0.5658806562423706
train gradient:  0.1990824743330057
iteration : 1753
train acc:  0.671875
train loss:  0.5798232555389404
train gradient:  0.2187809362982437
iteration : 1754
train acc:  0.6328125
train loss:  0.6469289064407349
train gradient:  0.2810046750068792
iteration : 1755
train acc:  0.71875
train loss:  0.5129366517066956
train gradient:  0.14580653482057637
iteration : 1756
train acc:  0.6875
train loss:  0.5507811307907104
train gradient:  0.18463194973319874
iteration : 1757
train acc:  0.75
train loss:  0.5120142698287964
train gradient:  0.16102922265299413
iteration : 1758
train acc:  0.671875
train loss:  0.6194443702697754
train gradient:  0.2472265942021377
iteration : 1759
train acc:  0.703125
train loss:  0.541243314743042
train gradient:  0.1408200877070368
iteration : 1760
train acc:  0.703125
train loss:  0.5435147285461426
train gradient:  0.16335233061207488
iteration : 1761
train acc:  0.671875
train loss:  0.5942865610122681
train gradient:  0.18035937996264992
iteration : 1762
train acc:  0.71875
train loss:  0.5214153528213501
train gradient:  0.14517249735075932
iteration : 1763
train acc:  0.7265625
train loss:  0.5636638402938843
train gradient:  0.1767703774046756
iteration : 1764
train acc:  0.796875
train loss:  0.49876099824905396
train gradient:  0.14443419828905477
iteration : 1765
train acc:  0.7578125
train loss:  0.4880102872848511
train gradient:  0.14373540378998118
iteration : 1766
train acc:  0.703125
train loss:  0.5723292827606201
train gradient:  0.15932656251741584
iteration : 1767
train acc:  0.75
train loss:  0.485138863325119
train gradient:  0.14853006192545776
iteration : 1768
train acc:  0.703125
train loss:  0.5495628118515015
train gradient:  0.1626066099759202
iteration : 1769
train acc:  0.703125
train loss:  0.5251121520996094
train gradient:  0.14626733932525626
iteration : 1770
train acc:  0.625
train loss:  0.5575491786003113
train gradient:  0.15194896779992323
iteration : 1771
train acc:  0.6640625
train loss:  0.6286135911941528
train gradient:  0.1890005453083773
iteration : 1772
train acc:  0.6796875
train loss:  0.5846474170684814
train gradient:  0.19253014337139038
iteration : 1773
train acc:  0.703125
train loss:  0.5357526540756226
train gradient:  0.1347646274450869
iteration : 1774
train acc:  0.6171875
train loss:  0.6206852197647095
train gradient:  0.20548221208175338
iteration : 1775
train acc:  0.609375
train loss:  0.6365433931350708
train gradient:  0.22345080533422612
iteration : 1776
train acc:  0.703125
train loss:  0.5192577838897705
train gradient:  0.1365578835166722
iteration : 1777
train acc:  0.75
train loss:  0.5562596917152405
train gradient:  0.3338443677921423
iteration : 1778
train acc:  0.6953125
train loss:  0.5389395356178284
train gradient:  0.15816804529168785
iteration : 1779
train acc:  0.734375
train loss:  0.5403640270233154
train gradient:  0.15311773295075165
iteration : 1780
train acc:  0.7109375
train loss:  0.5520321130752563
train gradient:  0.14093307382520137
iteration : 1781
train acc:  0.6484375
train loss:  0.6085063219070435
train gradient:  0.19647621693094303
iteration : 1782
train acc:  0.703125
train loss:  0.5408506989479065
train gradient:  0.14654977442564526
iteration : 1783
train acc:  0.6640625
train loss:  0.617400050163269
train gradient:  0.16154339968107406
iteration : 1784
train acc:  0.6171875
train loss:  0.6333990097045898
train gradient:  0.22134533304036824
iteration : 1785
train acc:  0.6796875
train loss:  0.5689373016357422
train gradient:  0.1919932156863033
iteration : 1786
train acc:  0.7421875
train loss:  0.5695799589157104
train gradient:  0.23376625313230615
iteration : 1787
train acc:  0.71875
train loss:  0.5325096845626831
train gradient:  0.13208403942264293
iteration : 1788
train acc:  0.6796875
train loss:  0.5801824331283569
train gradient:  0.18529515553890705
iteration : 1789
train acc:  0.7421875
train loss:  0.5462239980697632
train gradient:  0.17841719113777899
iteration : 1790
train acc:  0.6484375
train loss:  0.5736823081970215
train gradient:  0.20016414585652026
iteration : 1791
train acc:  0.6796875
train loss:  0.5637532472610474
train gradient:  0.17267120502999794
iteration : 1792
train acc:  0.7265625
train loss:  0.5183554887771606
train gradient:  0.14549354597100533
iteration : 1793
train acc:  0.6796875
train loss:  0.607364296913147
train gradient:  0.2264151528628648
iteration : 1794
train acc:  0.734375
train loss:  0.5979956388473511
train gradient:  0.3209752590028311
iteration : 1795
train acc:  0.6953125
train loss:  0.5471838116645813
train gradient:  0.11199265214157514
iteration : 1796
train acc:  0.6875
train loss:  0.5252100229263306
train gradient:  0.14102821028708545
iteration : 1797
train acc:  0.6875
train loss:  0.534915030002594
train gradient:  0.15229289342189262
iteration : 1798
train acc:  0.7734375
train loss:  0.5453611612319946
train gradient:  0.21278908125398063
iteration : 1799
train acc:  0.625
train loss:  0.6083212494850159
train gradient:  0.17503040607696002
iteration : 1800
train acc:  0.765625
train loss:  0.485851526260376
train gradient:  0.13299163041481354
iteration : 1801
train acc:  0.7578125
train loss:  0.5174592733383179
train gradient:  0.1423305096128421
iteration : 1802
train acc:  0.6953125
train loss:  0.563227117061615
train gradient:  0.18741345164139386
iteration : 1803
train acc:  0.6640625
train loss:  0.6428105235099792
train gradient:  0.4116681163666345
iteration : 1804
train acc:  0.703125
train loss:  0.5732635259628296
train gradient:  0.19985707150592963
iteration : 1805
train acc:  0.7265625
train loss:  0.5183049440383911
train gradient:  0.1446027181586178
iteration : 1806
train acc:  0.6953125
train loss:  0.5716015100479126
train gradient:  0.1424801952699886
iteration : 1807
train acc:  0.7578125
train loss:  0.5169340968132019
train gradient:  0.13827462968688892
iteration : 1808
train acc:  0.703125
train loss:  0.5626504421234131
train gradient:  0.1587725850932431
iteration : 1809
train acc:  0.7734375
train loss:  0.5094239115715027
train gradient:  0.14953180712276315
iteration : 1810
train acc:  0.65625
train loss:  0.5739650726318359
train gradient:  0.14943869795196923
iteration : 1811
train acc:  0.8125
train loss:  0.45192140340805054
train gradient:  0.14394431336972963
iteration : 1812
train acc:  0.765625
train loss:  0.5023619532585144
train gradient:  0.1480554014506849
iteration : 1813
train acc:  0.703125
train loss:  0.5837255716323853
train gradient:  0.21577722853014814
iteration : 1814
train acc:  0.7265625
train loss:  0.5121698975563049
train gradient:  0.16009841182541834
iteration : 1815
train acc:  0.671875
train loss:  0.6053093075752258
train gradient:  0.18688530803106337
iteration : 1816
train acc:  0.765625
train loss:  0.49505558609962463
train gradient:  0.13613204245687116
iteration : 1817
train acc:  0.6953125
train loss:  0.5663235783576965
train gradient:  0.18233279322725204
iteration : 1818
train acc:  0.59375
train loss:  0.6347906589508057
train gradient:  0.345169691992696
iteration : 1819
train acc:  0.6953125
train loss:  0.5384247303009033
train gradient:  0.15943462398286534
iteration : 1820
train acc:  0.6796875
train loss:  0.5386823415756226
train gradient:  0.13965628675972447
iteration : 1821
train acc:  0.7734375
train loss:  0.4969823956489563
train gradient:  0.18410849927410328
iteration : 1822
train acc:  0.703125
train loss:  0.5107027292251587
train gradient:  0.1445092577875129
iteration : 1823
train acc:  0.78125
train loss:  0.49914538860321045
train gradient:  0.17064445117348326
iteration : 1824
train acc:  0.671875
train loss:  0.6467649340629578
train gradient:  0.19705308933349217
iteration : 1825
train acc:  0.6875
train loss:  0.5901104211807251
train gradient:  0.17380793878858197
iteration : 1826
train acc:  0.71875
train loss:  0.5556035041809082
train gradient:  0.2367154641948369
iteration : 1827
train acc:  0.71875
train loss:  0.5128064155578613
train gradient:  0.1433720101908273
iteration : 1828
train acc:  0.734375
train loss:  0.514889657497406
train gradient:  0.15013740276185814
iteration : 1829
train acc:  0.734375
train loss:  0.5133567452430725
train gradient:  0.17163676681492565
iteration : 1830
train acc:  0.671875
train loss:  0.5742951035499573
train gradient:  0.17836561493580821
iteration : 1831
train acc:  0.625
train loss:  0.6011233329772949
train gradient:  0.21498159308225007
iteration : 1832
train acc:  0.71875
train loss:  0.5328949689865112
train gradient:  0.1345212554964167
iteration : 1833
train acc:  0.703125
train loss:  0.5389085412025452
train gradient:  0.2649441091301437
iteration : 1834
train acc:  0.7734375
train loss:  0.5002732872962952
train gradient:  0.2024899673835952
iteration : 1835
train acc:  0.78125
train loss:  0.48872143030166626
train gradient:  0.19943929558499476
iteration : 1836
train acc:  0.71875
train loss:  0.5319586992263794
train gradient:  0.14767332185833465
iteration : 1837
train acc:  0.7421875
train loss:  0.5014774203300476
train gradient:  0.14817001254396578
iteration : 1838
train acc:  0.7265625
train loss:  0.5618966817855835
train gradient:  0.2382393303176253
iteration : 1839
train acc:  0.671875
train loss:  0.571415364742279
train gradient:  0.18019117818871855
iteration : 1840
train acc:  0.6953125
train loss:  0.5432003140449524
train gradient:  0.19635129584598568
iteration : 1841
train acc:  0.6953125
train loss:  0.5291231870651245
train gradient:  0.13502685037419077
iteration : 1842
train acc:  0.75
train loss:  0.5165047645568848
train gradient:  0.1457949978869932
iteration : 1843
train acc:  0.6796875
train loss:  0.5654542446136475
train gradient:  0.1796070360723456
iteration : 1844
train acc:  0.6875
train loss:  0.6030983924865723
train gradient:  0.2580359733221398
iteration : 1845
train acc:  0.7109375
train loss:  0.5516312122344971
train gradient:  0.14938170483706817
iteration : 1846
train acc:  0.703125
train loss:  0.5046875476837158
train gradient:  0.20343030925899708
iteration : 1847
train acc:  0.6484375
train loss:  0.5870485901832581
train gradient:  0.15571251165582403
iteration : 1848
train acc:  0.6484375
train loss:  0.602020263671875
train gradient:  0.1824767327288851
iteration : 1849
train acc:  0.6953125
train loss:  0.5763949155807495
train gradient:  0.14671499789583187
iteration : 1850
train acc:  0.65625
train loss:  0.6345551013946533
train gradient:  0.19341371938438912
iteration : 1851
train acc:  0.7109375
train loss:  0.5797716379165649
train gradient:  0.15358490958908974
iteration : 1852
train acc:  0.6875
train loss:  0.5725628137588501
train gradient:  0.234225083489077
iteration : 1853
train acc:  0.6328125
train loss:  0.6161552667617798
train gradient:  0.24903650064590271
iteration : 1854
train acc:  0.6875
train loss:  0.5692484378814697
train gradient:  0.20642547192611754
iteration : 1855
train acc:  0.7421875
train loss:  0.5111145973205566
train gradient:  0.14364334758039912
iteration : 1856
train acc:  0.6796875
train loss:  0.5488125085830688
train gradient:  0.1987274214992586
iteration : 1857
train acc:  0.765625
train loss:  0.530208945274353
train gradient:  0.19676808679963367
iteration : 1858
train acc:  0.6484375
train loss:  0.6356802582740784
train gradient:  0.23699491892441982
iteration : 1859
train acc:  0.6640625
train loss:  0.5805768966674805
train gradient:  0.20800829140045843
iteration : 1860
train acc:  0.7421875
train loss:  0.5250365138053894
train gradient:  0.21172180338820107
iteration : 1861
train acc:  0.625
train loss:  0.5735026001930237
train gradient:  0.19413324848709412
iteration : 1862
train acc:  0.75
train loss:  0.5180792212486267
train gradient:  0.16104296326837758
iteration : 1863
train acc:  0.6953125
train loss:  0.5403302907943726
train gradient:  0.1414751491043223
iteration : 1864
train acc:  0.7421875
train loss:  0.5138760805130005
train gradient:  0.13839733652867592
iteration : 1865
train acc:  0.6953125
train loss:  0.5608901977539062
train gradient:  0.14934356008824823
iteration : 1866
train acc:  0.7109375
train loss:  0.5429633855819702
train gradient:  0.1402078327169628
iteration : 1867
train acc:  0.703125
train loss:  0.5428597331047058
train gradient:  0.2410973074730664
iteration : 1868
train acc:  0.7265625
train loss:  0.5211777687072754
train gradient:  0.16273920805242298
iteration : 1869
train acc:  0.734375
train loss:  0.526608943939209
train gradient:  0.14154529277423394
iteration : 1870
train acc:  0.7265625
train loss:  0.5458977222442627
train gradient:  0.16786337894044256
iteration : 1871
train acc:  0.671875
train loss:  0.5864412784576416
train gradient:  0.17496471101737382
iteration : 1872
train acc:  0.7109375
train loss:  0.5524949431419373
train gradient:  0.17597740522569033
iteration : 1873
train acc:  0.734375
train loss:  0.5131760835647583
train gradient:  0.20541235030715577
iteration : 1874
train acc:  0.703125
train loss:  0.5526307821273804
train gradient:  0.1766547088824579
iteration : 1875
train acc:  0.625
train loss:  0.607641875743866
train gradient:  0.24131632738683084
iteration : 1876
train acc:  0.734375
train loss:  0.5785053968429565
train gradient:  0.17127862380894462
iteration : 1877
train acc:  0.6328125
train loss:  0.5991746783256531
train gradient:  0.17659656632285256
iteration : 1878
train acc:  0.6640625
train loss:  0.5373245477676392
train gradient:  0.2551566527096857
iteration : 1879
train acc:  0.6484375
train loss:  0.595374584197998
train gradient:  0.18557426086929085
iteration : 1880
train acc:  0.7265625
train loss:  0.5365906357765198
train gradient:  0.1721337285781221
iteration : 1881
train acc:  0.6875
train loss:  0.600108802318573
train gradient:  0.15187216958941518
iteration : 1882
train acc:  0.7109375
train loss:  0.5520324110984802
train gradient:  0.13259760464891096
iteration : 1883
train acc:  0.7890625
train loss:  0.4930141568183899
train gradient:  0.15141752433313377
iteration : 1884
train acc:  0.75
train loss:  0.514205813407898
train gradient:  0.16266873220058697
iteration : 1885
train acc:  0.765625
train loss:  0.5421240329742432
train gradient:  0.19193823120452405
iteration : 1886
train acc:  0.65625
train loss:  0.5474505424499512
train gradient:  0.18821883703747577
iteration : 1887
train acc:  0.7578125
train loss:  0.5182963609695435
train gradient:  0.12719234279176672
iteration : 1888
train acc:  0.703125
train loss:  0.5735020637512207
train gradient:  0.2801880300532399
iteration : 1889
train acc:  0.671875
train loss:  0.5647244453430176
train gradient:  0.1579621235272753
iteration : 1890
train acc:  0.7578125
train loss:  0.476798951625824
train gradient:  0.17690457162327244
iteration : 1891
train acc:  0.6796875
train loss:  0.5642566680908203
train gradient:  0.1457024039241828
iteration : 1892
train acc:  0.7421875
train loss:  0.5145624876022339
train gradient:  0.17036317654359912
iteration : 1893
train acc:  0.671875
train loss:  0.5854054689407349
train gradient:  0.1771286018929687
iteration : 1894
train acc:  0.6953125
train loss:  0.5482605695724487
train gradient:  0.161346728535884
iteration : 1895
train acc:  0.6875
train loss:  0.6039944887161255
train gradient:  0.1946774537576605
iteration : 1896
train acc:  0.7109375
train loss:  0.56548011302948
train gradient:  0.18116318564401607
iteration : 1897
train acc:  0.7421875
train loss:  0.47289079427719116
train gradient:  0.16898872103681156
iteration : 1898
train acc:  0.71875
train loss:  0.5738956332206726
train gradient:  0.1584974229697504
iteration : 1899
train acc:  0.71875
train loss:  0.5461134910583496
train gradient:  0.12790117210469104
iteration : 1900
train acc:  0.65625
train loss:  0.6127262711524963
train gradient:  0.20291985704977497
iteration : 1901
train acc:  0.6640625
train loss:  0.6201568841934204
train gradient:  0.2165956761764175
iteration : 1902
train acc:  0.671875
train loss:  0.5828481912612915
train gradient:  0.23242247907604868
iteration : 1903
train acc:  0.6796875
train loss:  0.5806020498275757
train gradient:  0.2724946265741985
iteration : 1904
train acc:  0.78125
train loss:  0.4758712649345398
train gradient:  0.13507839870390714
iteration : 1905
train acc:  0.71875
train loss:  0.5479966402053833
train gradient:  0.13223398407123974
iteration : 1906
train acc:  0.75
train loss:  0.5311621427536011
train gradient:  0.15873589189758947
iteration : 1907
train acc:  0.796875
train loss:  0.5259764790534973
train gradient:  0.14747543565023258
iteration : 1908
train acc:  0.640625
train loss:  0.5913816690444946
train gradient:  0.17065622836123256
iteration : 1909
train acc:  0.71875
train loss:  0.5448201894760132
train gradient:  0.13528246238386235
iteration : 1910
train acc:  0.734375
train loss:  0.5250751972198486
train gradient:  0.16992583828902708
iteration : 1911
train acc:  0.7109375
train loss:  0.527917742729187
train gradient:  0.1213763562651981
iteration : 1912
train acc:  0.7265625
train loss:  0.5385316610336304
train gradient:  0.1867696763077783
iteration : 1913
train acc:  0.6953125
train loss:  0.5998523235321045
train gradient:  0.21969612725881368
iteration : 1914
train acc:  0.71875
train loss:  0.48878294229507446
train gradient:  0.13902793190394286
iteration : 1915
train acc:  0.7421875
train loss:  0.5162456035614014
train gradient:  0.31308521623911384
iteration : 1916
train acc:  0.6484375
train loss:  0.5901563167572021
train gradient:  0.17898644252974366
iteration : 1917
train acc:  0.71875
train loss:  0.5626233816146851
train gradient:  0.13967026113363584
iteration : 1918
train acc:  0.6875
train loss:  0.5888469219207764
train gradient:  0.27737353004367515
iteration : 1919
train acc:  0.6875
train loss:  0.5926482677459717
train gradient:  0.17787654367164601
iteration : 1920
train acc:  0.671875
train loss:  0.5723223090171814
train gradient:  0.15046263789863962
iteration : 1921
train acc:  0.6953125
train loss:  0.5400576591491699
train gradient:  0.1411612193116011
iteration : 1922
train acc:  0.6875
train loss:  0.5613119602203369
train gradient:  0.15093867823170443
iteration : 1923
train acc:  0.7109375
train loss:  0.5502974390983582
train gradient:  0.16574934559302718
iteration : 1924
train acc:  0.7421875
train loss:  0.5004305839538574
train gradient:  0.13243956078862373
iteration : 1925
train acc:  0.609375
train loss:  0.6201266646385193
train gradient:  0.15614941109839708
iteration : 1926
train acc:  0.6875
train loss:  0.5450462102890015
train gradient:  0.22495933218387817
iteration : 1927
train acc:  0.7421875
train loss:  0.4879463315010071
train gradient:  0.1617020832599626
iteration : 1928
train acc:  0.7265625
train loss:  0.5513432621955872
train gradient:  0.24014081112180127
iteration : 1929
train acc:  0.7578125
train loss:  0.4961506128311157
train gradient:  0.15166036168523855
iteration : 1930
train acc:  0.71875
train loss:  0.5464669466018677
train gradient:  0.18189015537451647
iteration : 1931
train acc:  0.71875
train loss:  0.5251907110214233
train gradient:  0.15950819635253705
iteration : 1932
train acc:  0.7109375
train loss:  0.541143000125885
train gradient:  0.16129682930162118
iteration : 1933
train acc:  0.671875
train loss:  0.6076749563217163
train gradient:  0.17892729371242405
iteration : 1934
train acc:  0.7890625
train loss:  0.4728086590766907
train gradient:  0.15344659636384955
iteration : 1935
train acc:  0.7578125
train loss:  0.5043365955352783
train gradient:  0.13422058788067748
iteration : 1936
train acc:  0.6796875
train loss:  0.5714356899261475
train gradient:  0.1655740729721178
iteration : 1937
train acc:  0.71875
train loss:  0.4781055450439453
train gradient:  0.1470634077057843
iteration : 1938
train acc:  0.734375
train loss:  0.5076358318328857
train gradient:  0.14328496882767003
iteration : 1939
train acc:  0.7578125
train loss:  0.4861343801021576
train gradient:  0.148429441173046
iteration : 1940
train acc:  0.734375
train loss:  0.5291991233825684
train gradient:  0.15594834008104652
iteration : 1941
train acc:  0.6875
train loss:  0.5511335134506226
train gradient:  0.1466750850505006
iteration : 1942
train acc:  0.640625
train loss:  0.6417853832244873
train gradient:  0.19377547334146644
iteration : 1943
train acc:  0.7265625
train loss:  0.49781519174575806
train gradient:  0.13487956436658582
iteration : 1944
train acc:  0.6796875
train loss:  0.5821903347969055
train gradient:  0.17376682258266227
iteration : 1945
train acc:  0.6484375
train loss:  0.6123005151748657
train gradient:  0.19906798687347788
iteration : 1946
train acc:  0.671875
train loss:  0.581700325012207
train gradient:  0.2626825533712612
iteration : 1947
train acc:  0.71875
train loss:  0.551854133605957
train gradient:  0.16161354277467738
iteration : 1948
train acc:  0.703125
train loss:  0.521883487701416
train gradient:  0.12672477282123928
iteration : 1949
train acc:  0.6953125
train loss:  0.5599676370620728
train gradient:  0.15235076122863098
iteration : 1950
train acc:  0.6796875
train loss:  0.5790930390357971
train gradient:  0.19786536032089708
iteration : 1951
train acc:  0.75
train loss:  0.5723869800567627
train gradient:  0.17798816181024651
iteration : 1952
train acc:  0.6796875
train loss:  0.5365313291549683
train gradient:  0.2611036069572823
iteration : 1953
train acc:  0.671875
train loss:  0.5934611558914185
train gradient:  0.1675390079272615
iteration : 1954
train acc:  0.7265625
train loss:  0.585803747177124
train gradient:  0.26491928731875386
iteration : 1955
train acc:  0.7109375
train loss:  0.5002459287643433
train gradient:  0.12326149106494662
iteration : 1956
train acc:  0.7265625
train loss:  0.5239603519439697
train gradient:  0.12221386631439901
iteration : 1957
train acc:  0.71875
train loss:  0.5596052408218384
train gradient:  0.1992743296374343
iteration : 1958
train acc:  0.734375
train loss:  0.5108839869499207
train gradient:  0.14328599582510113
iteration : 1959
train acc:  0.7421875
train loss:  0.5474507808685303
train gradient:  0.14056732007206357
iteration : 1960
train acc:  0.7421875
train loss:  0.5527750253677368
train gradient:  0.17850350528833375
iteration : 1961
train acc:  0.703125
train loss:  0.493670254945755
train gradient:  0.15738191356143122
iteration : 1962
train acc:  0.7421875
train loss:  0.5037592649459839
train gradient:  0.11324080465806698
iteration : 1963
train acc:  0.640625
train loss:  0.6031745076179504
train gradient:  0.17888477249960655
iteration : 1964
train acc:  0.6796875
train loss:  0.5954430103302002
train gradient:  0.16079247877637837
iteration : 1965
train acc:  0.6796875
train loss:  0.5523118376731873
train gradient:  0.1537493881714116
iteration : 1966
train acc:  0.6953125
train loss:  0.5687466859817505
train gradient:  0.20664540051680663
iteration : 1967
train acc:  0.7109375
train loss:  0.5779417157173157
train gradient:  0.17663017279985993
iteration : 1968
train acc:  0.671875
train loss:  0.5477182865142822
train gradient:  0.15492924686228066
iteration : 1969
train acc:  0.7421875
train loss:  0.525388777256012
train gradient:  0.13395209926816706
iteration : 1970
train acc:  0.671875
train loss:  0.5846868753433228
train gradient:  0.16853532728483375
iteration : 1971
train acc:  0.6953125
train loss:  0.5689335465431213
train gradient:  0.1984692600794603
iteration : 1972
train acc:  0.640625
train loss:  0.593017578125
train gradient:  0.18968252284500575
iteration : 1973
train acc:  0.7578125
train loss:  0.556594967842102
train gradient:  0.16510122780064296
iteration : 1974
train acc:  0.734375
train loss:  0.5159078240394592
train gradient:  0.2264636323424278
iteration : 1975
train acc:  0.703125
train loss:  0.5449408292770386
train gradient:  0.14442449528585521
iteration : 1976
train acc:  0.7578125
train loss:  0.5494773983955383
train gradient:  0.16997718123914912
iteration : 1977
train acc:  0.71875
train loss:  0.5876603722572327
train gradient:  0.16017517941999132
iteration : 1978
train acc:  0.7421875
train loss:  0.494182288646698
train gradient:  0.25709150418184074
iteration : 1979
train acc:  0.6796875
train loss:  0.5156001448631287
train gradient:  0.18251205998687606
iteration : 1980
train acc:  0.671875
train loss:  0.5648144483566284
train gradient:  0.19105023023007048
iteration : 1981
train acc:  0.6875
train loss:  0.5922110080718994
train gradient:  0.16031923612722948
iteration : 1982
train acc:  0.703125
train loss:  0.5925523638725281
train gradient:  0.23475043599873996
iteration : 1983
train acc:  0.703125
train loss:  0.5153783559799194
train gradient:  0.13964983693796235
iteration : 1984
train acc:  0.7109375
train loss:  0.5110699534416199
train gradient:  0.1361770982319519
iteration : 1985
train acc:  0.671875
train loss:  0.6268936395645142
train gradient:  0.17716581793453623
iteration : 1986
train acc:  0.75
train loss:  0.5363240242004395
train gradient:  0.13823078652214027
iteration : 1987
train acc:  0.75
train loss:  0.5726335048675537
train gradient:  0.1770012796544742
iteration : 1988
train acc:  0.6875
train loss:  0.5558075904846191
train gradient:  0.18763268435383554
iteration : 1989
train acc:  0.7109375
train loss:  0.5314225554466248
train gradient:  0.16732416691114024
iteration : 1990
train acc:  0.703125
train loss:  0.5605323314666748
train gradient:  0.1344794703765544
iteration : 1991
train acc:  0.7109375
train loss:  0.5561761856079102
train gradient:  0.14522408132512205
iteration : 1992
train acc:  0.6171875
train loss:  0.614313006401062
train gradient:  0.2118955356226661
iteration : 1993
train acc:  0.7421875
train loss:  0.5711479783058167
train gradient:  0.19045678461043963
iteration : 1994
train acc:  0.7265625
train loss:  0.5251017808914185
train gradient:  0.16733526309802332
iteration : 1995
train acc:  0.6484375
train loss:  0.6273207068443298
train gradient:  0.18324203809498815
iteration : 1996
train acc:  0.75
train loss:  0.56679368019104
train gradient:  0.20021955030411304
iteration : 1997
train acc:  0.6953125
train loss:  0.5262792110443115
train gradient:  0.17237615394626488
iteration : 1998
train acc:  0.7265625
train loss:  0.5432925224304199
train gradient:  0.18467807102592176
iteration : 1999
train acc:  0.7265625
train loss:  0.5457087755203247
train gradient:  0.15743766786743363
iteration : 2000
train acc:  0.71875
train loss:  0.5318317413330078
train gradient:  0.1602316441346716
iteration : 2001
train acc:  0.7265625
train loss:  0.5187709331512451
train gradient:  0.18339551266060686
iteration : 2002
train acc:  0.765625
train loss:  0.49001216888427734
train gradient:  0.14709713468609903
iteration : 2003
train acc:  0.671875
train loss:  0.5480550527572632
train gradient:  0.16405815004702662
iteration : 2004
train acc:  0.7109375
train loss:  0.5374332666397095
train gradient:  0.13452599754058978
iteration : 2005
train acc:  0.5703125
train loss:  0.6027326583862305
train gradient:  0.1571394793415576
iteration : 2006
train acc:  0.6796875
train loss:  0.5462207198143005
train gradient:  0.14517601623922582
iteration : 2007
train acc:  0.6875
train loss:  0.5627561807632446
train gradient:  0.1728534420536913
iteration : 2008
train acc:  0.7109375
train loss:  0.5629338026046753
train gradient:  0.2139768638450082
iteration : 2009
train acc:  0.7421875
train loss:  0.5469499826431274
train gradient:  0.1743377587159911
iteration : 2010
train acc:  0.7265625
train loss:  0.5378563404083252
train gradient:  0.13820046841633363
iteration : 2011
train acc:  0.6328125
train loss:  0.6097922325134277
train gradient:  0.22368711789490026
iteration : 2012
train acc:  0.6484375
train loss:  0.5831413269042969
train gradient:  0.17992990269606568
iteration : 2013
train acc:  0.71875
train loss:  0.5749990344047546
train gradient:  0.23412492890126402
iteration : 2014
train acc:  0.71875
train loss:  0.5179575681686401
train gradient:  0.2302471699662022
iteration : 2015
train acc:  0.7421875
train loss:  0.5182303190231323
train gradient:  0.18900632664611067
iteration : 2016
train acc:  0.6484375
train loss:  0.5456163883209229
train gradient:  0.1474137062295629
iteration : 2017
train acc:  0.7109375
train loss:  0.5344840884208679
train gradient:  0.15400717242596013
iteration : 2018
train acc:  0.703125
train loss:  0.5522531867027283
train gradient:  0.1407992760581483
iteration : 2019
train acc:  0.75
train loss:  0.5024256110191345
train gradient:  0.14608704004833017
iteration : 2020
train acc:  0.671875
train loss:  0.5670410990715027
train gradient:  0.15308786834856847
iteration : 2021
train acc:  0.6796875
train loss:  0.5469013452529907
train gradient:  0.1759425236427305
iteration : 2022
train acc:  0.6875
train loss:  0.535381019115448
train gradient:  0.15483572574531165
iteration : 2023
train acc:  0.7109375
train loss:  0.5414756536483765
train gradient:  0.15349633494753312
iteration : 2024
train acc:  0.6875
train loss:  0.5278580188751221
train gradient:  0.11654539117735868
iteration : 2025
train acc:  0.7265625
train loss:  0.5474267601966858
train gradient:  0.23683125650165462
iteration : 2026
train acc:  0.65625
train loss:  0.6205702424049377
train gradient:  0.18506821239834537
iteration : 2027
train acc:  0.6875
train loss:  0.5809004306793213
train gradient:  0.17685864370737395
iteration : 2028
train acc:  0.7421875
train loss:  0.5465630292892456
train gradient:  0.157775506418939
iteration : 2029
train acc:  0.6875
train loss:  0.6021018028259277
train gradient:  0.16667732024294635
iteration : 2030
train acc:  0.7578125
train loss:  0.5094360709190369
train gradient:  0.15402874834563318
iteration : 2031
train acc:  0.71875
train loss:  0.5229074954986572
train gradient:  0.18524489263026733
iteration : 2032
train acc:  0.734375
train loss:  0.5397522449493408
train gradient:  0.166473985902089
iteration : 2033
train acc:  0.7734375
train loss:  0.49544695019721985
train gradient:  0.16425632636337212
iteration : 2034
train acc:  0.6796875
train loss:  0.6178533434867859
train gradient:  0.2136580227303043
iteration : 2035
train acc:  0.671875
train loss:  0.544333279132843
train gradient:  0.14696689613802466
iteration : 2036
train acc:  0.7421875
train loss:  0.4569266140460968
train gradient:  0.13408983507637506
iteration : 2037
train acc:  0.703125
train loss:  0.5437321662902832
train gradient:  0.16717579094027624
iteration : 2038
train acc:  0.6953125
train loss:  0.5790389776229858
train gradient:  0.19657886928757085
iteration : 2039
train acc:  0.703125
train loss:  0.5700274705886841
train gradient:  0.1254152809835617
iteration : 2040
train acc:  0.7421875
train loss:  0.5098048448562622
train gradient:  0.11768245669085645
iteration : 2041
train acc:  0.671875
train loss:  0.5742942094802856
train gradient:  0.21832461473914558
iteration : 2042
train acc:  0.6796875
train loss:  0.5584836602210999
train gradient:  0.20259478378425022
iteration : 2043
train acc:  0.8046875
train loss:  0.5048456192016602
train gradient:  0.11755588933733421
iteration : 2044
train acc:  0.6953125
train loss:  0.5393555164337158
train gradient:  0.1427597983067933
iteration : 2045
train acc:  0.71875
train loss:  0.5278119444847107
train gradient:  0.15941677202707624
iteration : 2046
train acc:  0.7265625
train loss:  0.5615125894546509
train gradient:  0.15353223286535517
iteration : 2047
train acc:  0.6953125
train loss:  0.5611004829406738
train gradient:  0.15993636075798995
iteration : 2048
train acc:  0.6484375
train loss:  0.6265079379081726
train gradient:  0.22597123337612107
iteration : 2049
train acc:  0.7265625
train loss:  0.557941198348999
train gradient:  0.1488441826885366
iteration : 2050
train acc:  0.703125
train loss:  0.565299391746521
train gradient:  0.18933049598920776
iteration : 2051
train acc:  0.7109375
train loss:  0.5492831468582153
train gradient:  0.1640741980541343
iteration : 2052
train acc:  0.703125
train loss:  0.577445387840271
train gradient:  0.2224136923958987
iteration : 2053
train acc:  0.6015625
train loss:  0.5933798551559448
train gradient:  0.162131658606292
iteration : 2054
train acc:  0.7265625
train loss:  0.5174139738082886
train gradient:  0.15824402414714156
iteration : 2055
train acc:  0.7421875
train loss:  0.517558217048645
train gradient:  0.17901848351830202
iteration : 2056
train acc:  0.6640625
train loss:  0.5394380688667297
train gradient:  0.16310615219359906
iteration : 2057
train acc:  0.6796875
train loss:  0.5691695213317871
train gradient:  0.214943025128263
iteration : 2058
train acc:  0.6875
train loss:  0.5552562475204468
train gradient:  0.13694054247580398
iteration : 2059
train acc:  0.703125
train loss:  0.5808272361755371
train gradient:  0.16973329894963454
iteration : 2060
train acc:  0.6640625
train loss:  0.5822793841362
train gradient:  0.1640393931296148
iteration : 2061
train acc:  0.6953125
train loss:  0.5536320209503174
train gradient:  0.14298103058886924
iteration : 2062
train acc:  0.7734375
train loss:  0.47487473487854004
train gradient:  0.1389692647442898
iteration : 2063
train acc:  0.7578125
train loss:  0.5354517698287964
train gradient:  0.17053508268859302
iteration : 2064
train acc:  0.7109375
train loss:  0.5801303386688232
train gradient:  0.17262573327175545
iteration : 2065
train acc:  0.6875
train loss:  0.5343576073646545
train gradient:  0.12753152831272524
iteration : 2066
train acc:  0.671875
train loss:  0.5779772996902466
train gradient:  0.16544646511315414
iteration : 2067
train acc:  0.71875
train loss:  0.5132409334182739
train gradient:  0.1571788079910787
iteration : 2068
train acc:  0.7109375
train loss:  0.5484619736671448
train gradient:  0.13284553515730535
iteration : 2069
train acc:  0.71875
train loss:  0.5563176870346069
train gradient:  0.17198744306419633
iteration : 2070
train acc:  0.7734375
train loss:  0.5534640550613403
train gradient:  0.17952874663827573
iteration : 2071
train acc:  0.7578125
train loss:  0.5516999363899231
train gradient:  0.17358087604194183
iteration : 2072
train acc:  0.765625
train loss:  0.4881354570388794
train gradient:  0.12879301357841788
iteration : 2073
train acc:  0.671875
train loss:  0.5472450852394104
train gradient:  0.1705720800807698
iteration : 2074
train acc:  0.640625
train loss:  0.6091773509979248
train gradient:  0.2811186962932644
iteration : 2075
train acc:  0.71875
train loss:  0.5061284899711609
train gradient:  0.1452214552130966
iteration : 2076
train acc:  0.734375
train loss:  0.47797077894210815
train gradient:  0.16194947513267155
iteration : 2077
train acc:  0.6875
train loss:  0.573607325553894
train gradient:  0.23273762027450606
iteration : 2078
train acc:  0.75
train loss:  0.4931309223175049
train gradient:  0.13605735593473284
iteration : 2079
train acc:  0.765625
train loss:  0.4761350154876709
train gradient:  0.15018406913011687
iteration : 2080
train acc:  0.7578125
train loss:  0.5251721143722534
train gradient:  0.17447121639761498
iteration : 2081
train acc:  0.65625
train loss:  0.618361234664917
train gradient:  0.20270486572060104
iteration : 2082
train acc:  0.7421875
train loss:  0.4942305088043213
train gradient:  0.16101187664223454
iteration : 2083
train acc:  0.7421875
train loss:  0.5339106917381287
train gradient:  0.14955449102906174
iteration : 2084
train acc:  0.765625
train loss:  0.5096442699432373
train gradient:  0.12611603073720937
iteration : 2085
train acc:  0.625
train loss:  0.6214829683303833
train gradient:  0.27111881498541823
iteration : 2086
train acc:  0.671875
train loss:  0.5758278965950012
train gradient:  0.18656309808558305
iteration : 2087
train acc:  0.7109375
train loss:  0.556770920753479
train gradient:  0.20588466345442502
iteration : 2088
train acc:  0.7109375
train loss:  0.5565404891967773
train gradient:  0.18838888678397805
iteration : 2089
train acc:  0.734375
train loss:  0.5707321166992188
train gradient:  0.17678253332299626
iteration : 2090
train acc:  0.71875
train loss:  0.5831269025802612
train gradient:  0.1567102697947509
iteration : 2091
train acc:  0.640625
train loss:  0.6283458471298218
train gradient:  0.22085267971352834
iteration : 2092
train acc:  0.6796875
train loss:  0.5543646812438965
train gradient:  0.20373453103824368
iteration : 2093
train acc:  0.7109375
train loss:  0.5473830103874207
train gradient:  0.18256950161645102
iteration : 2094
train acc:  0.734375
train loss:  0.5350505113601685
train gradient:  0.15911953901204554
iteration : 2095
train acc:  0.65625
train loss:  0.5579754710197449
train gradient:  0.1440485962437132
iteration : 2096
train acc:  0.6953125
train loss:  0.5733815431594849
train gradient:  0.19161186816847325
iteration : 2097
train acc:  0.6796875
train loss:  0.5541626214981079
train gradient:  0.16533581513817536
iteration : 2098
train acc:  0.6875
train loss:  0.5547060966491699
train gradient:  0.15699597067797783
iteration : 2099
train acc:  0.734375
train loss:  0.5280096530914307
train gradient:  0.12024649726600217
iteration : 2100
train acc:  0.7109375
train loss:  0.5382856726646423
train gradient:  0.2087853089476947
iteration : 2101
train acc:  0.671875
train loss:  0.578101634979248
train gradient:  0.2479165957143908
iteration : 2102
train acc:  0.7109375
train loss:  0.5510848760604858
train gradient:  0.15380891939905977
iteration : 2103
train acc:  0.7421875
train loss:  0.47499656677246094
train gradient:  0.2069110179061398
iteration : 2104
train acc:  0.796875
train loss:  0.503463864326477
train gradient:  0.13930316577716811
iteration : 2105
train acc:  0.7109375
train loss:  0.5747175812721252
train gradient:  0.21076554615003118
iteration : 2106
train acc:  0.7265625
train loss:  0.543769121170044
train gradient:  0.13520397946096913
iteration : 2107
train acc:  0.7109375
train loss:  0.5491358041763306
train gradient:  0.132617482203142
iteration : 2108
train acc:  0.6796875
train loss:  0.5752558708190918
train gradient:  0.2060349384974262
iteration : 2109
train acc:  0.75
train loss:  0.5328019261360168
train gradient:  0.1596360840424758
iteration : 2110
train acc:  0.65625
train loss:  0.5821951627731323
train gradient:  0.174154427991585
iteration : 2111
train acc:  0.6953125
train loss:  0.52977454662323
train gradient:  0.16978412247277147
iteration : 2112
train acc:  0.734375
train loss:  0.526276707649231
train gradient:  0.15382565609121995
iteration : 2113
train acc:  0.75
train loss:  0.5046963691711426
train gradient:  0.14586662995897842
iteration : 2114
train acc:  0.6953125
train loss:  0.5652332305908203
train gradient:  0.18727616485077325
iteration : 2115
train acc:  0.7265625
train loss:  0.5027040243148804
train gradient:  0.13877298622973167
iteration : 2116
train acc:  0.6953125
train loss:  0.542629599571228
train gradient:  0.14538296503700993
iteration : 2117
train acc:  0.7265625
train loss:  0.5287294387817383
train gradient:  0.1443509394322878
iteration : 2118
train acc:  0.6796875
train loss:  0.5930110216140747
train gradient:  0.18262380572480427
iteration : 2119
train acc:  0.6484375
train loss:  0.6015743017196655
train gradient:  0.171665358532326
iteration : 2120
train acc:  0.6875
train loss:  0.6090399026870728
train gradient:  0.16058345421711606
iteration : 2121
train acc:  0.7578125
train loss:  0.47964316606521606
train gradient:  0.11235254139110559
iteration : 2122
train acc:  0.65625
train loss:  0.5759438872337341
train gradient:  0.1537034005580199
iteration : 2123
train acc:  0.6796875
train loss:  0.5760706663131714
train gradient:  0.1518869689224464
iteration : 2124
train acc:  0.7109375
train loss:  0.535758376121521
train gradient:  0.14388447525185202
iteration : 2125
train acc:  0.7265625
train loss:  0.5073605179786682
train gradient:  0.1241633892359339
iteration : 2126
train acc:  0.6796875
train loss:  0.531288743019104
train gradient:  0.160895684129922
iteration : 2127
train acc:  0.7109375
train loss:  0.5372583270072937
train gradient:  0.2232730034701481
iteration : 2128
train acc:  0.71875
train loss:  0.5397648811340332
train gradient:  0.1282143318985105
iteration : 2129
train acc:  0.734375
train loss:  0.5390421152114868
train gradient:  0.12628290622946925
iteration : 2130
train acc:  0.734375
train loss:  0.5362399816513062
train gradient:  0.1725550836640582
iteration : 2131
train acc:  0.7578125
train loss:  0.4974216818809509
train gradient:  0.1238307697318744
iteration : 2132
train acc:  0.7265625
train loss:  0.5367412567138672
train gradient:  0.1736590237395578
iteration : 2133
train acc:  0.6953125
train loss:  0.5208020210266113
train gradient:  0.14783508559494044
iteration : 2134
train acc:  0.6796875
train loss:  0.5574889779090881
train gradient:  0.1499957640916008
iteration : 2135
train acc:  0.796875
train loss:  0.49373573064804077
train gradient:  0.14146813895852087
iteration : 2136
train acc:  0.7421875
train loss:  0.5258519649505615
train gradient:  0.15514731308621124
iteration : 2137
train acc:  0.65625
train loss:  0.6508058309555054
train gradient:  0.3651883362111232
iteration : 2138
train acc:  0.7734375
train loss:  0.50218665599823
train gradient:  0.11550844197801906
iteration : 2139
train acc:  0.671875
train loss:  0.5550236701965332
train gradient:  0.16560370433016097
iteration : 2140
train acc:  0.65625
train loss:  0.5803564190864563
train gradient:  0.18537786154707703
iteration : 2141
train acc:  0.765625
train loss:  0.51784747838974
train gradient:  0.13360014319787328
iteration : 2142
train acc:  0.6796875
train loss:  0.5767419338226318
train gradient:  0.1812337408671566
iteration : 2143
train acc:  0.6796875
train loss:  0.5977550745010376
train gradient:  0.19842232342762867
iteration : 2144
train acc:  0.6796875
train loss:  0.5656536817550659
train gradient:  0.1498463877519874
iteration : 2145
train acc:  0.7109375
train loss:  0.5828815698623657
train gradient:  0.1775432158391762
iteration : 2146
train acc:  0.7265625
train loss:  0.5107958316802979
train gradient:  0.17838079880813135
iteration : 2147
train acc:  0.734375
train loss:  0.514633059501648
train gradient:  0.13980627035668525
iteration : 2148
train acc:  0.7109375
train loss:  0.5302844047546387
train gradient:  0.12109842586128906
iteration : 2149
train acc:  0.734375
train loss:  0.5397223234176636
train gradient:  0.13005634181421738
iteration : 2150
train acc:  0.6875
train loss:  0.5162678360939026
train gradient:  0.13103864943997273
iteration : 2151
train acc:  0.75
train loss:  0.5095928907394409
train gradient:  0.1635700175457288
iteration : 2152
train acc:  0.6484375
train loss:  0.5823330879211426
train gradient:  0.16165650053462383
iteration : 2153
train acc:  0.765625
train loss:  0.48997193574905396
train gradient:  0.12272925801974648
iteration : 2154
train acc:  0.6953125
train loss:  0.5856993794441223
train gradient:  0.21201162276038465
iteration : 2155
train acc:  0.6875
train loss:  0.5936146378517151
train gradient:  0.18265167026201257
iteration : 2156
train acc:  0.6484375
train loss:  0.6173092126846313
train gradient:  0.24204538220675748
iteration : 2157
train acc:  0.6640625
train loss:  0.5809711217880249
train gradient:  0.1870433276462522
iteration : 2158
train acc:  0.71875
train loss:  0.5774062275886536
train gradient:  0.2176761133663478
iteration : 2159
train acc:  0.734375
train loss:  0.5396854877471924
train gradient:  0.13467532703949558
iteration : 2160
train acc:  0.7421875
train loss:  0.5166979432106018
train gradient:  0.16894017048813004
iteration : 2161
train acc:  0.7265625
train loss:  0.5524357557296753
train gradient:  0.15126116988021687
iteration : 2162
train acc:  0.7109375
train loss:  0.5326801538467407
train gradient:  0.15893436011622783
iteration : 2163
train acc:  0.703125
train loss:  0.5369422435760498
train gradient:  0.1487815086420288
iteration : 2164
train acc:  0.71875
train loss:  0.5744458436965942
train gradient:  0.1445321778555374
iteration : 2165
train acc:  0.703125
train loss:  0.5380722880363464
train gradient:  0.19435629153714795
iteration : 2166
train acc:  0.671875
train loss:  0.5523402690887451
train gradient:  0.17155426590169645
iteration : 2167
train acc:  0.71875
train loss:  0.5348093509674072
train gradient:  0.14863065173837167
iteration : 2168
train acc:  0.71875
train loss:  0.5910353064537048
train gradient:  0.21954825488408836
iteration : 2169
train acc:  0.71875
train loss:  0.5382285118103027
train gradient:  0.14139094519542983
iteration : 2170
train acc:  0.734375
train loss:  0.5071173906326294
train gradient:  0.17327829261417488
iteration : 2171
train acc:  0.8046875
train loss:  0.47958874702453613
train gradient:  0.1368731705534551
iteration : 2172
train acc:  0.6875
train loss:  0.5789111852645874
train gradient:  0.15636977926759443
iteration : 2173
train acc:  0.6796875
train loss:  0.5618746876716614
train gradient:  0.28346601275673794
iteration : 2174
train acc:  0.7421875
train loss:  0.5150026082992554
train gradient:  0.15772628846249045
iteration : 2175
train acc:  0.671875
train loss:  0.5446594953536987
train gradient:  0.18165580063857234
iteration : 2176
train acc:  0.671875
train loss:  0.5654728412628174
train gradient:  0.1693240246255987
iteration : 2177
train acc:  0.7578125
train loss:  0.5330350399017334
train gradient:  0.14725787657505274
iteration : 2178
train acc:  0.6875
train loss:  0.5752218961715698
train gradient:  0.15598668907274424
iteration : 2179
train acc:  0.6953125
train loss:  0.5713217854499817
train gradient:  0.1657071178984283
iteration : 2180
train acc:  0.6640625
train loss:  0.53668212890625
train gradient:  0.13364983768152755
iteration : 2181
train acc:  0.6640625
train loss:  0.5688552856445312
train gradient:  0.1396551634439009
iteration : 2182
train acc:  0.75
train loss:  0.507749080657959
train gradient:  0.1188773472332771
iteration : 2183
train acc:  0.65625
train loss:  0.621823251247406
train gradient:  0.1798880581441628
iteration : 2184
train acc:  0.71875
train loss:  0.5549442768096924
train gradient:  0.14692846462874765
iteration : 2185
train acc:  0.703125
train loss:  0.5181223154067993
train gradient:  0.1640662372341381
iteration : 2186
train acc:  0.7265625
train loss:  0.5431902408599854
train gradient:  0.16197567486308606
iteration : 2187
train acc:  0.7578125
train loss:  0.47966158390045166
train gradient:  0.12156470850102734
iteration : 2188
train acc:  0.71875
train loss:  0.5510767102241516
train gradient:  0.15531197068915892
iteration : 2189
train acc:  0.765625
train loss:  0.5322977304458618
train gradient:  0.1350957232328961
iteration : 2190
train acc:  0.71875
train loss:  0.5516970157623291
train gradient:  0.1426520988799556
iteration : 2191
train acc:  0.640625
train loss:  0.5999768972396851
train gradient:  0.20889274825575402
iteration : 2192
train acc:  0.7109375
train loss:  0.5817636251449585
train gradient:  0.12378442690190188
iteration : 2193
train acc:  0.7265625
train loss:  0.4986289143562317
train gradient:  0.14126234310858274
iteration : 2194
train acc:  0.7578125
train loss:  0.5233056545257568
train gradient:  0.13926686800939073
iteration : 2195
train acc:  0.6640625
train loss:  0.5356249809265137
train gradient:  0.15108655182186154
iteration : 2196
train acc:  0.71875
train loss:  0.5245288610458374
train gradient:  0.20175412783377045
iteration : 2197
train acc:  0.75
train loss:  0.48187342286109924
train gradient:  0.12364650619094168
iteration : 2198
train acc:  0.6796875
train loss:  0.5629034638404846
train gradient:  0.13276727907247257
iteration : 2199
train acc:  0.7109375
train loss:  0.5369415283203125
train gradient:  0.1500135996534091
iteration : 2200
train acc:  0.7421875
train loss:  0.5074998736381531
train gradient:  0.16354592122517111
iteration : 2201
train acc:  0.7265625
train loss:  0.5085121989250183
train gradient:  0.19737858226562927
iteration : 2202
train acc:  0.71875
train loss:  0.5595203042030334
train gradient:  0.1730046616947594
iteration : 2203
train acc:  0.671875
train loss:  0.6060402393341064
train gradient:  0.15281074674499484
iteration : 2204
train acc:  0.703125
train loss:  0.5614393353462219
train gradient:  0.167026854402563
iteration : 2205
train acc:  0.703125
train loss:  0.5523313283920288
train gradient:  0.15397237203523978
iteration : 2206
train acc:  0.71875
train loss:  0.5533425807952881
train gradient:  0.11345597075725332
iteration : 2207
train acc:  0.7421875
train loss:  0.524979829788208
train gradient:  0.14828768866530068
iteration : 2208
train acc:  0.6875
train loss:  0.622732937335968
train gradient:  0.19960153200250091
iteration : 2209
train acc:  0.71875
train loss:  0.5023990869522095
train gradient:  0.10486874035855512
iteration : 2210
train acc:  0.65625
train loss:  0.5687943696975708
train gradient:  0.18064275386931547
iteration : 2211
train acc:  0.6953125
train loss:  0.5343618392944336
train gradient:  0.15730286126304074
iteration : 2212
train acc:  0.6796875
train loss:  0.581602931022644
train gradient:  0.18847596692919255
iteration : 2213
train acc:  0.6640625
train loss:  0.5840181112289429
train gradient:  0.2312871225529751
iteration : 2214
train acc:  0.7421875
train loss:  0.5199902057647705
train gradient:  0.15696933513488445
iteration : 2215
train acc:  0.671875
train loss:  0.5496617555618286
train gradient:  0.14041947553905504
iteration : 2216
train acc:  0.7109375
train loss:  0.5469422340393066
train gradient:  0.14988920001502853
iteration : 2217
train acc:  0.65625
train loss:  0.5962637066841125
train gradient:  0.20198502442138697
iteration : 2218
train acc:  0.75
train loss:  0.5139002799987793
train gradient:  0.15371945031930048
iteration : 2219
train acc:  0.6796875
train loss:  0.6292871236801147
train gradient:  0.1874194815721388
iteration : 2220
train acc:  0.734375
train loss:  0.5238651037216187
train gradient:  0.1549016579470564
iteration : 2221
train acc:  0.75
train loss:  0.5387756824493408
train gradient:  0.14223050151514857
iteration : 2222
train acc:  0.65625
train loss:  0.5906438827514648
train gradient:  0.17037871822989004
iteration : 2223
train acc:  0.7265625
train loss:  0.5236285924911499
train gradient:  0.15066923236106558
iteration : 2224
train acc:  0.734375
train loss:  0.48704609274864197
train gradient:  0.19254477286976313
iteration : 2225
train acc:  0.765625
train loss:  0.4715345501899719
train gradient:  0.13838507335188444
iteration : 2226
train acc:  0.6484375
train loss:  0.6058784127235413
train gradient:  0.1644225367405297
iteration : 2227
train acc:  0.7265625
train loss:  0.4947243630886078
train gradient:  0.17520240818638708
iteration : 2228
train acc:  0.7734375
train loss:  0.489303857088089
train gradient:  0.10103415789491509
iteration : 2229
train acc:  0.640625
train loss:  0.5602096319198608
train gradient:  0.15164308612287067
iteration : 2230
train acc:  0.71875
train loss:  0.5529574751853943
train gradient:  0.1670887728224567
iteration : 2231
train acc:  0.7421875
train loss:  0.52979975938797
train gradient:  0.1962101618211307
iteration : 2232
train acc:  0.703125
train loss:  0.5429696440696716
train gradient:  0.16281223546529291
iteration : 2233
train acc:  0.6875
train loss:  0.5921130776405334
train gradient:  0.20381366756790936
iteration : 2234
train acc:  0.734375
train loss:  0.5342031121253967
train gradient:  0.18269707076553932
iteration : 2235
train acc:  0.6875
train loss:  0.5693646669387817
train gradient:  0.17740735924032552
iteration : 2236
train acc:  0.75
train loss:  0.5403204560279846
train gradient:  0.17616091722475163
iteration : 2237
train acc:  0.6171875
train loss:  0.5788818597793579
train gradient:  0.14756700737680828
iteration : 2238
train acc:  0.765625
train loss:  0.49646905064582825
train gradient:  0.1537992921336302
iteration : 2239
train acc:  0.7109375
train loss:  0.5434936285018921
train gradient:  0.13699614980799718
iteration : 2240
train acc:  0.6640625
train loss:  0.5816288590431213
train gradient:  0.16767037768704093
iteration : 2241
train acc:  0.75
train loss:  0.5188931822776794
train gradient:  0.17541479823155604
iteration : 2242
train acc:  0.6328125
train loss:  0.5824692249298096
train gradient:  0.17869653992402545
iteration : 2243
train acc:  0.703125
train loss:  0.5904655456542969
train gradient:  0.21670454991478388
iteration : 2244
train acc:  0.7421875
train loss:  0.4768868684768677
train gradient:  0.10689025495502048
iteration : 2245
train acc:  0.7421875
train loss:  0.5244549512863159
train gradient:  0.13253335499966423
iteration : 2246
train acc:  0.6875
train loss:  0.5879615545272827
train gradient:  0.23824212820066074
iteration : 2247
train acc:  0.75
train loss:  0.4984908401966095
train gradient:  0.13186301152466354
iteration : 2248
train acc:  0.6171875
train loss:  0.6793893575668335
train gradient:  0.19275282391760898
iteration : 2249
train acc:  0.7265625
train loss:  0.5165001749992371
train gradient:  0.1838608505226832
iteration : 2250
train acc:  0.75
train loss:  0.4973617196083069
train gradient:  0.12477160007277664
iteration : 2251
train acc:  0.7109375
train loss:  0.5538830757141113
train gradient:  0.17642150634563802
iteration : 2252
train acc:  0.6953125
train loss:  0.5344260931015015
train gradient:  0.14132143338584652
iteration : 2253
train acc:  0.671875
train loss:  0.6280088424682617
train gradient:  0.19412828067037705
iteration : 2254
train acc:  0.7578125
train loss:  0.4860190451145172
train gradient:  0.12977021009979448
iteration : 2255
train acc:  0.7265625
train loss:  0.5071104764938354
train gradient:  0.15245708063376673
iteration : 2256
train acc:  0.7421875
train loss:  0.5161278247833252
train gradient:  0.13420510792791882
iteration : 2257
train acc:  0.6953125
train loss:  0.5280431509017944
train gradient:  0.18662206179331936
iteration : 2258
train acc:  0.71875
train loss:  0.5332241058349609
train gradient:  0.17190624543753133
iteration : 2259
train acc:  0.765625
train loss:  0.4726269841194153
train gradient:  0.14389762292694128
iteration : 2260
train acc:  0.6953125
train loss:  0.5067025423049927
train gradient:  0.1488821900653078
iteration : 2261
train acc:  0.6640625
train loss:  0.6021325588226318
train gradient:  0.27088006029767864
iteration : 2262
train acc:  0.6640625
train loss:  0.6014581918716431
train gradient:  0.20348213806188734
iteration : 2263
train acc:  0.6015625
train loss:  0.6441821455955505
train gradient:  0.18483146722706112
iteration : 2264
train acc:  0.6953125
train loss:  0.5622861385345459
train gradient:  0.16926218027080836
iteration : 2265
train acc:  0.6875
train loss:  0.5395867824554443
train gradient:  0.17498419124946474
iteration : 2266
train acc:  0.6640625
train loss:  0.6017494797706604
train gradient:  0.29284711413199016
iteration : 2267
train acc:  0.6796875
train loss:  0.591942548751831
train gradient:  0.20370951317364272
iteration : 2268
train acc:  0.6875
train loss:  0.49733591079711914
train gradient:  0.26196622912981504
iteration : 2269
train acc:  0.734375
train loss:  0.5607748031616211
train gradient:  0.17829597907218586
iteration : 2270
train acc:  0.7109375
train loss:  0.5695863366127014
train gradient:  0.16026556487485127
iteration : 2271
train acc:  0.7890625
train loss:  0.4896515905857086
train gradient:  0.14345819997158132
iteration : 2272
train acc:  0.734375
train loss:  0.5172291398048401
train gradient:  0.11699215125991556
iteration : 2273
train acc:  0.7265625
train loss:  0.5262197256088257
train gradient:  0.15372906418223437
iteration : 2274
train acc:  0.703125
train loss:  0.5522706508636475
train gradient:  0.18530155275520319
iteration : 2275
train acc:  0.65625
train loss:  0.5377237796783447
train gradient:  0.1877573354480244
iteration : 2276
train acc:  0.6328125
train loss:  0.6524794101715088
train gradient:  0.1885318732307345
iteration : 2277
train acc:  0.71875
train loss:  0.5530382394790649
train gradient:  0.14617245500928483
iteration : 2278
train acc:  0.7265625
train loss:  0.5349234342575073
train gradient:  0.14271081915089456
iteration : 2279
train acc:  0.671875
train loss:  0.5594656467437744
train gradient:  0.1727243279923044
iteration : 2280
train acc:  0.7265625
train loss:  0.5091509819030762
train gradient:  0.14719432110020025
iteration : 2281
train acc:  0.6796875
train loss:  0.5308759808540344
train gradient:  0.1310073632621532
iteration : 2282
train acc:  0.703125
train loss:  0.56559157371521
train gradient:  0.16099659269776084
iteration : 2283
train acc:  0.6640625
train loss:  0.5712074041366577
train gradient:  0.15625900324613617
iteration : 2284
train acc:  0.6875
train loss:  0.5702775716781616
train gradient:  0.18863826018692878
iteration : 2285
train acc:  0.71875
train loss:  0.5562641024589539
train gradient:  0.14436377096825143
iteration : 2286
train acc:  0.71875
train loss:  0.53770911693573
train gradient:  0.1388981844042157
iteration : 2287
train acc:  0.7109375
train loss:  0.5283610820770264
train gradient:  0.13142112589013144
iteration : 2288
train acc:  0.6953125
train loss:  0.566199779510498
train gradient:  0.2176012253297233
iteration : 2289
train acc:  0.7265625
train loss:  0.5280724167823792
train gradient:  0.17173390908417163
iteration : 2290
train acc:  0.6328125
train loss:  0.6014456152915955
train gradient:  0.19852867789226195
iteration : 2291
train acc:  0.7421875
train loss:  0.5054442882537842
train gradient:  0.1351396931432283
iteration : 2292
train acc:  0.703125
train loss:  0.506584882736206
train gradient:  0.16493074668501145
iteration : 2293
train acc:  0.7109375
train loss:  0.5868244171142578
train gradient:  0.19138593737634896
iteration : 2294
train acc:  0.703125
train loss:  0.5344763994216919
train gradient:  0.14032403579534036
iteration : 2295
train acc:  0.78125
train loss:  0.4683108329772949
train gradient:  0.14125591606913362
iteration : 2296
train acc:  0.7265625
train loss:  0.5234181880950928
train gradient:  0.14776205897056455
iteration : 2297
train acc:  0.7109375
train loss:  0.57117760181427
train gradient:  0.18070196079731457
iteration : 2298
train acc:  0.703125
train loss:  0.5529277324676514
train gradient:  0.14710358947745356
iteration : 2299
train acc:  0.65625
train loss:  0.5682586431503296
train gradient:  0.23020078883820522
iteration : 2300
train acc:  0.6875
train loss:  0.5359328985214233
train gradient:  0.13081532242291305
iteration : 2301
train acc:  0.6953125
train loss:  0.5797008872032166
train gradient:  0.18124728097625536
iteration : 2302
train acc:  0.8046875
train loss:  0.4475908875465393
train gradient:  0.18085037497448586
iteration : 2303
train acc:  0.6640625
train loss:  0.6094428896903992
train gradient:  0.1650809207787425
iteration : 2304
train acc:  0.6953125
train loss:  0.5205367803573608
train gradient:  0.17055225269523466
iteration : 2305
train acc:  0.65625
train loss:  0.6190183162689209
train gradient:  0.18315438300063197
iteration : 2306
train acc:  0.65625
train loss:  0.5925571322441101
train gradient:  0.19800021279255495
iteration : 2307
train acc:  0.7734375
train loss:  0.47048139572143555
train gradient:  0.1253715134017423
iteration : 2308
train acc:  0.71875
train loss:  0.5287859439849854
train gradient:  0.1683827748160563
iteration : 2309
train acc:  0.7734375
train loss:  0.5131321549415588
train gradient:  0.20344122752074612
iteration : 2310
train acc:  0.7421875
train loss:  0.5144718289375305
train gradient:  0.1487474387815657
iteration : 2311
train acc:  0.640625
train loss:  0.5932981371879578
train gradient:  0.21572416676954237
iteration : 2312
train acc:  0.7109375
train loss:  0.5641006231307983
train gradient:  0.16448535626467775
iteration : 2313
train acc:  0.6484375
train loss:  0.5858942270278931
train gradient:  0.1640265700287429
iteration : 2314
train acc:  0.671875
train loss:  0.5671472549438477
train gradient:  0.16212791003328592
iteration : 2315
train acc:  0.6953125
train loss:  0.5385346412658691
train gradient:  0.18963692114589636
iteration : 2316
train acc:  0.6640625
train loss:  0.6103471517562866
train gradient:  0.3060618719461965
iteration : 2317
train acc:  0.71875
train loss:  0.5367622375488281
train gradient:  0.16132761111108335
iteration : 2318
train acc:  0.734375
train loss:  0.48842763900756836
train gradient:  0.13805466324229282
iteration : 2319
train acc:  0.7265625
train loss:  0.5350824594497681
train gradient:  0.171962491025292
iteration : 2320
train acc:  0.671875
train loss:  0.6113987565040588
train gradient:  0.18181635337081298
iteration : 2321
train acc:  0.734375
train loss:  0.537811279296875
train gradient:  0.1517013409533381
iteration : 2322
train acc:  0.609375
train loss:  0.6182702779769897
train gradient:  0.2024791368345944
iteration : 2323
train acc:  0.6484375
train loss:  0.5406892895698547
train gradient:  0.13213966985941472
iteration : 2324
train acc:  0.671875
train loss:  0.5810024738311768
train gradient:  0.2844525198735826
iteration : 2325
train acc:  0.6953125
train loss:  0.580950140953064
train gradient:  0.2144350251602445
iteration : 2326
train acc:  0.6640625
train loss:  0.5717257857322693
train gradient:  0.18878861091619997
iteration : 2327
train acc:  0.65625
train loss:  0.596680760383606
train gradient:  0.22181793537623637
iteration : 2328
train acc:  0.6171875
train loss:  0.5842186212539673
train gradient:  0.15507290925586725
iteration : 2329
train acc:  0.65625
train loss:  0.610984206199646
train gradient:  0.20725635658853658
iteration : 2330
train acc:  0.6640625
train loss:  0.570716142654419
train gradient:  0.24650824546013217
iteration : 2331
train acc:  0.78125
train loss:  0.5081080198287964
train gradient:  0.17685585046830593
iteration : 2332
train acc:  0.7109375
train loss:  0.5348104238510132
train gradient:  0.16030111241629966
iteration : 2333
train acc:  0.78125
train loss:  0.48611414432525635
train gradient:  0.1415856944621263
iteration : 2334
train acc:  0.71875
train loss:  0.5425523519515991
train gradient:  0.13071932074996348
iteration : 2335
train acc:  0.703125
train loss:  0.5273113250732422
train gradient:  0.14269157376125235
iteration : 2336
train acc:  0.671875
train loss:  0.5542413592338562
train gradient:  0.1542731143111811
iteration : 2337
train acc:  0.65625
train loss:  0.6000624299049377
train gradient:  0.206462522666498
iteration : 2338
train acc:  0.6796875
train loss:  0.5556247234344482
train gradient:  0.2542584321515503
iteration : 2339
train acc:  0.6328125
train loss:  0.5880182385444641
train gradient:  0.1664188814355556
iteration : 2340
train acc:  0.6484375
train loss:  0.6072442531585693
train gradient:  0.17049204512480207
iteration : 2341
train acc:  0.71875
train loss:  0.5216231346130371
train gradient:  0.165957616588372
iteration : 2342
train acc:  0.7109375
train loss:  0.497416615486145
train gradient:  0.10968101057421965
iteration : 2343
train acc:  0.7578125
train loss:  0.4912596344947815
train gradient:  0.1612045395251947
iteration : 2344
train acc:  0.75
train loss:  0.5318340063095093
train gradient:  0.17266203268114647
iteration : 2345
train acc:  0.71875
train loss:  0.5572895407676697
train gradient:  0.13741690290184322
iteration : 2346
train acc:  0.703125
train loss:  0.5126256942749023
train gradient:  0.1482883170780177
iteration : 2347
train acc:  0.7734375
train loss:  0.5037848949432373
train gradient:  0.17870658043260873
iteration : 2348
train acc:  0.703125
train loss:  0.612372875213623
train gradient:  0.23334028128056483
iteration : 2349
train acc:  0.6875
train loss:  0.5688987970352173
train gradient:  0.20667425411894094
iteration : 2350
train acc:  0.7109375
train loss:  0.5618247389793396
train gradient:  0.16285190282389284
iteration : 2351
train acc:  0.765625
train loss:  0.49305620789527893
train gradient:  0.11412901163876132
iteration : 2352
train acc:  0.734375
train loss:  0.5751007795333862
train gradient:  0.1478425396772316
iteration : 2353
train acc:  0.65625
train loss:  0.5663888454437256
train gradient:  0.17812802252994508
iteration : 2354
train acc:  0.6640625
train loss:  0.6082131862640381
train gradient:  0.16395292298890538
iteration : 2355
train acc:  0.75
train loss:  0.5283570289611816
train gradient:  0.15113249743249807
iteration : 2356
train acc:  0.7109375
train loss:  0.5532742142677307
train gradient:  0.1883259258591386
iteration : 2357
train acc:  0.625
train loss:  0.6132929921150208
train gradient:  0.2001579056995136
iteration : 2358
train acc:  0.6484375
train loss:  0.6091012358665466
train gradient:  0.19280342714611942
iteration : 2359
train acc:  0.7109375
train loss:  0.543471097946167
train gradient:  0.1552579297413405
iteration : 2360
train acc:  0.703125
train loss:  0.5141721963882446
train gradient:  0.13828764054163842
iteration : 2361
train acc:  0.6640625
train loss:  0.5740478038787842
train gradient:  0.21616447150021129
iteration : 2362
train acc:  0.671875
train loss:  0.5669257640838623
train gradient:  0.19856051819508663
iteration : 2363
train acc:  0.71875
train loss:  0.5292579531669617
train gradient:  0.15354181739606
iteration : 2364
train acc:  0.7265625
train loss:  0.5269657969474792
train gradient:  0.1557300067042508
iteration : 2365
train acc:  0.703125
train loss:  0.5626033544540405
train gradient:  0.18097498515234406
iteration : 2366
train acc:  0.734375
train loss:  0.5234207510948181
train gradient:  0.14829246038893856
iteration : 2367
train acc:  0.7265625
train loss:  0.5390936136245728
train gradient:  0.24126120094863823
iteration : 2368
train acc:  0.671875
train loss:  0.6243902444839478
train gradient:  0.3126904757968167
iteration : 2369
train acc:  0.6875
train loss:  0.5675246715545654
train gradient:  0.1602884219675827
iteration : 2370
train acc:  0.6640625
train loss:  0.6092712879180908
train gradient:  0.20816802389667505
iteration : 2371
train acc:  0.6953125
train loss:  0.6006554365158081
train gradient:  0.2408410163498917
iteration : 2372
train acc:  0.734375
train loss:  0.5035502314567566
train gradient:  0.15816744264617974
iteration : 2373
train acc:  0.7109375
train loss:  0.5470623970031738
train gradient:  0.15461910250617877
iteration : 2374
train acc:  0.6953125
train loss:  0.5694413185119629
train gradient:  0.17650742683833764
iteration : 2375
train acc:  0.6015625
train loss:  0.633110761642456
train gradient:  0.18954700669322636
iteration : 2376
train acc:  0.7421875
train loss:  0.5491113066673279
train gradient:  0.20004585761182098
iteration : 2377
train acc:  0.7421875
train loss:  0.5579380989074707
train gradient:  0.1640812822623618
iteration : 2378
train acc:  0.734375
train loss:  0.49017900228500366
train gradient:  0.26948052206828804
iteration : 2379
train acc:  0.6953125
train loss:  0.6148049831390381
train gradient:  0.16591015645138202
iteration : 2380
train acc:  0.6796875
train loss:  0.5640624165534973
train gradient:  0.19024454261527854
iteration : 2381
train acc:  0.75
train loss:  0.5051746964454651
train gradient:  0.12123226480200733
iteration : 2382
train acc:  0.6875
train loss:  0.5537121295928955
train gradient:  0.15181642014691316
iteration : 2383
train acc:  0.7109375
train loss:  0.5249828696250916
train gradient:  0.1277655311554764
iteration : 2384
train acc:  0.8125
train loss:  0.42692404985427856
train gradient:  0.13929770975474032
iteration : 2385
train acc:  0.71875
train loss:  0.5499832630157471
train gradient:  0.20600055196448502
iteration : 2386
train acc:  0.734375
train loss:  0.5144199132919312
train gradient:  0.12890715208364711
iteration : 2387
train acc:  0.703125
train loss:  0.5314126014709473
train gradient:  0.1547400378909413
iteration : 2388
train acc:  0.671875
train loss:  0.5546199083328247
train gradient:  0.1337860419795815
iteration : 2389
train acc:  0.6640625
train loss:  0.5907813310623169
train gradient:  0.1782811364619881
iteration : 2390
train acc:  0.625
train loss:  0.6124180555343628
train gradient:  0.22042876218783397
iteration : 2391
train acc:  0.703125
train loss:  0.5494215488433838
train gradient:  0.1848305663651698
iteration : 2392
train acc:  0.75
train loss:  0.5291028618812561
train gradient:  0.20323480470636007
iteration : 2393
train acc:  0.765625
train loss:  0.4960983097553253
train gradient:  0.12333040269051296
iteration : 2394
train acc:  0.765625
train loss:  0.5067307949066162
train gradient:  0.20242665170644858
iteration : 2395
train acc:  0.671875
train loss:  0.6125649213790894
train gradient:  0.242529088026877
iteration : 2396
train acc:  0.6484375
train loss:  0.5954074859619141
train gradient:  0.2724570511500673
iteration : 2397
train acc:  0.65625
train loss:  0.5863507986068726
train gradient:  0.19604880267063446
iteration : 2398
train acc:  0.8046875
train loss:  0.4583544433116913
train gradient:  0.11841396449386775
iteration : 2399
train acc:  0.6875
train loss:  0.5592204332351685
train gradient:  0.15834721950279834
iteration : 2400
train acc:  0.6796875
train loss:  0.5769028663635254
train gradient:  0.18365742707638905
iteration : 2401
train acc:  0.7109375
train loss:  0.5470201969146729
train gradient:  0.14179638200364614
iteration : 2402
train acc:  0.6953125
train loss:  0.5522545576095581
train gradient:  0.13596604649125998
iteration : 2403
train acc:  0.78125
train loss:  0.46171820163726807
train gradient:  0.11878063955587638
iteration : 2404
train acc:  0.7265625
train loss:  0.5326812267303467
train gradient:  0.16834878796834118
iteration : 2405
train acc:  0.703125
train loss:  0.5259149074554443
train gradient:  0.13943980461937874
iteration : 2406
train acc:  0.7265625
train loss:  0.5638638734817505
train gradient:  0.13725996651609726
iteration : 2407
train acc:  0.6796875
train loss:  0.5128824710845947
train gradient:  0.11264551622162504
iteration : 2408
train acc:  0.7578125
train loss:  0.5109475255012512
train gradient:  0.1345068697907043
iteration : 2409
train acc:  0.6640625
train loss:  0.6012610793113708
train gradient:  0.20088988578386324
iteration : 2410
train acc:  0.671875
train loss:  0.609912633895874
train gradient:  0.1766850129230991
iteration : 2411
train acc:  0.7421875
train loss:  0.5152775645256042
train gradient:  0.14496664914096072
iteration : 2412
train acc:  0.6796875
train loss:  0.559659481048584
train gradient:  0.16969573397195897
iteration : 2413
train acc:  0.6640625
train loss:  0.5631033182144165
train gradient:  0.20851152967845427
iteration : 2414
train acc:  0.75
train loss:  0.48394569754600525
train gradient:  0.12244773475454007
iteration : 2415
train acc:  0.6953125
train loss:  0.547876238822937
train gradient:  0.17012566379576258
iteration : 2416
train acc:  0.75
train loss:  0.514186441898346
train gradient:  0.14097493952150955
iteration : 2417
train acc:  0.7265625
train loss:  0.5505199432373047
train gradient:  0.22441798585096873
iteration : 2418
train acc:  0.75
train loss:  0.5237578749656677
train gradient:  0.16668990909303372
iteration : 2419
train acc:  0.7265625
train loss:  0.5286493301391602
train gradient:  0.1237195484953752
iteration : 2420
train acc:  0.7421875
train loss:  0.5028543472290039
train gradient:  0.15251096804710784
iteration : 2421
train acc:  0.6875
train loss:  0.5439754724502563
train gradient:  0.18513331894233032
iteration : 2422
train acc:  0.65625
train loss:  0.5812418460845947
train gradient:  0.2246919277594972
iteration : 2423
train acc:  0.6953125
train loss:  0.5900927186012268
train gradient:  0.17373274303384995
iteration : 2424
train acc:  0.6796875
train loss:  0.5457437038421631
train gradient:  0.17456696947404265
iteration : 2425
train acc:  0.6875
train loss:  0.5495067834854126
train gradient:  0.1464527606767022
iteration : 2426
train acc:  0.7421875
train loss:  0.484315425157547
train gradient:  0.20353291581622435
iteration : 2427
train acc:  0.7265625
train loss:  0.5199836492538452
train gradient:  0.15728729735443053
iteration : 2428
train acc:  0.7109375
train loss:  0.5622299909591675
train gradient:  0.1623280723838095
iteration : 2429
train acc:  0.7265625
train loss:  0.5120458006858826
train gradient:  0.13268891620535578
iteration : 2430
train acc:  0.6640625
train loss:  0.5694440603256226
train gradient:  0.20607415821789402
iteration : 2431
train acc:  0.6875
train loss:  0.6000748872756958
train gradient:  0.22514333005570258
iteration : 2432
train acc:  0.6953125
train loss:  0.5526120662689209
train gradient:  0.18397878650585897
iteration : 2433
train acc:  0.703125
train loss:  0.5514687299728394
train gradient:  0.16112034109809686
iteration : 2434
train acc:  0.7109375
train loss:  0.5844470262527466
train gradient:  0.18172434293207734
iteration : 2435
train acc:  0.6953125
train loss:  0.5757107138633728
train gradient:  0.21184338469149483
iteration : 2436
train acc:  0.734375
train loss:  0.5504990816116333
train gradient:  0.14966507066053267
iteration : 2437
train acc:  0.671875
train loss:  0.5984877347946167
train gradient:  0.1887086386577852
iteration : 2438
train acc:  0.640625
train loss:  0.6120885014533997
train gradient:  0.23418175132460645
iteration : 2439
train acc:  0.671875
train loss:  0.5291941165924072
train gradient:  0.1590745322769277
iteration : 2440
train acc:  0.703125
train loss:  0.5372140407562256
train gradient:  0.13669189021463418
iteration : 2441
train acc:  0.6875
train loss:  0.5245434045791626
train gradient:  0.1421352189935235
iteration : 2442
train acc:  0.640625
train loss:  0.6103801727294922
train gradient:  0.18286670525736867
iteration : 2443
train acc:  0.75
train loss:  0.5319404602050781
train gradient:  0.14071952035942842
iteration : 2444
train acc:  0.7265625
train loss:  0.5346726179122925
train gradient:  0.15583337914534726
iteration : 2445
train acc:  0.7109375
train loss:  0.5131386518478394
train gradient:  0.18132205453769473
iteration : 2446
train acc:  0.6796875
train loss:  0.5611315965652466
train gradient:  0.14958041165459995
iteration : 2447
train acc:  0.703125
train loss:  0.5592457056045532
train gradient:  0.1567276053881799
iteration : 2448
train acc:  0.65625
train loss:  0.5737287998199463
train gradient:  0.17139873832798525
iteration : 2449
train acc:  0.671875
train loss:  0.5235850811004639
train gradient:  0.1486506339260624
iteration : 2450
train acc:  0.7265625
train loss:  0.5176774263381958
train gradient:  0.13937370842298052
iteration : 2451
train acc:  0.734375
train loss:  0.5227795243263245
train gradient:  0.14936349495407747
iteration : 2452
train acc:  0.6796875
train loss:  0.5103067755699158
train gradient:  0.1408023976630028
iteration : 2453
train acc:  0.6484375
train loss:  0.6344348192214966
train gradient:  0.2643987913751693
iteration : 2454
train acc:  0.65625
train loss:  0.5742005109786987
train gradient:  0.17396207227005872
iteration : 2455
train acc:  0.6796875
train loss:  0.5182709693908691
train gradient:  0.13476946823990174
iteration : 2456
train acc:  0.734375
train loss:  0.47504621744155884
train gradient:  0.1405544348107166
iteration : 2457
train acc:  0.7109375
train loss:  0.5306389331817627
train gradient:  0.13011494649040503
iteration : 2458
train acc:  0.7734375
train loss:  0.5144249200820923
train gradient:  0.11212766563772678
iteration : 2459
train acc:  0.6796875
train loss:  0.5614767074584961
train gradient:  0.23840709782035568
iteration : 2460
train acc:  0.703125
train loss:  0.5300501585006714
train gradient:  0.1423944176022125
iteration : 2461
train acc:  0.6796875
train loss:  0.5509243011474609
train gradient:  0.13653492525928101
iteration : 2462
train acc:  0.6875
train loss:  0.5540583729743958
train gradient:  0.14119531295162224
iteration : 2463
train acc:  0.7265625
train loss:  0.5255470871925354
train gradient:  0.2089105369193249
iteration : 2464
train acc:  0.78125
train loss:  0.4714900553226471
train gradient:  0.14947080679075442
iteration : 2465
train acc:  0.6875
train loss:  0.5290541648864746
train gradient:  0.14107721892220004
iteration : 2466
train acc:  0.6796875
train loss:  0.5840182900428772
train gradient:  0.20700821039289374
iteration : 2467
train acc:  0.6953125
train loss:  0.6109449863433838
train gradient:  0.20278058394665804
iteration : 2468
train acc:  0.7265625
train loss:  0.5272657871246338
train gradient:  0.15146437753669073
iteration : 2469
train acc:  0.71875
train loss:  0.5540583729743958
train gradient:  0.22288856841651034
iteration : 2470
train acc:  0.6796875
train loss:  0.5530130863189697
train gradient:  0.1687314091780842
iteration : 2471
train acc:  0.6796875
train loss:  0.6000235080718994
train gradient:  0.2447940313265336
iteration : 2472
train acc:  0.703125
train loss:  0.5243093371391296
train gradient:  0.12211252717156848
iteration : 2473
train acc:  0.78125
train loss:  0.47583484649658203
train gradient:  0.12854499400519528
iteration : 2474
train acc:  0.734375
train loss:  0.49364030361175537
train gradient:  0.1082704714335192
iteration : 2475
train acc:  0.765625
train loss:  0.47949302196502686
train gradient:  0.11517341936462326
iteration : 2476
train acc:  0.7109375
train loss:  0.5547885298728943
train gradient:  0.16916514227332746
iteration : 2477
train acc:  0.703125
train loss:  0.5764225721359253
train gradient:  0.20516082589169649
iteration : 2478
train acc:  0.7265625
train loss:  0.5233027935028076
train gradient:  0.13209248621068695
iteration : 2479
train acc:  0.71875
train loss:  0.5289634466171265
train gradient:  0.16771723677601597
iteration : 2480
train acc:  0.734375
train loss:  0.5486623048782349
train gradient:  0.1876514304412678
iteration : 2481
train acc:  0.7578125
train loss:  0.5217871069908142
train gradient:  0.1464966405820241
iteration : 2482
train acc:  0.7890625
train loss:  0.49174654483795166
train gradient:  0.14911265966132758
iteration : 2483
train acc:  0.6796875
train loss:  0.5835897922515869
train gradient:  0.18021208796283789
iteration : 2484
train acc:  0.7265625
train loss:  0.5269654989242554
train gradient:  0.18036543889529405
iteration : 2485
train acc:  0.7734375
train loss:  0.5134569406509399
train gradient:  0.12332894736728015
iteration : 2486
train acc:  0.65625
train loss:  0.5821444392204285
train gradient:  0.1598911094338702
iteration : 2487
train acc:  0.71875
train loss:  0.4912254810333252
train gradient:  0.13054765707943683
iteration : 2488
train acc:  0.7265625
train loss:  0.5392842292785645
train gradient:  0.15683643505129652
iteration : 2489
train acc:  0.75
train loss:  0.5061919689178467
train gradient:  0.20635693107719771
iteration : 2490
train acc:  0.7265625
train loss:  0.5580237507820129
train gradient:  0.16671393756803543
iteration : 2491
train acc:  0.71875
train loss:  0.49132800102233887
train gradient:  0.1297915899721448
iteration : 2492
train acc:  0.7265625
train loss:  0.5314587950706482
train gradient:  0.14441258481134905
iteration : 2493
train acc:  0.734375
train loss:  0.5203095078468323
train gradient:  0.1288249488859678
iteration : 2494
train acc:  0.71875
train loss:  0.5210132002830505
train gradient:  0.13831486834822945
iteration : 2495
train acc:  0.71875
train loss:  0.5298560857772827
train gradient:  0.12504736556673068
iteration : 2496
train acc:  0.703125
train loss:  0.5204542279243469
train gradient:  0.14430292487069463
iteration : 2497
train acc:  0.703125
train loss:  0.5260214805603027
train gradient:  0.19273766540446902
iteration : 2498
train acc:  0.703125
train loss:  0.5468878746032715
train gradient:  0.1611911521836876
iteration : 2499
train acc:  0.7578125
train loss:  0.4990506172180176
train gradient:  0.15383898911074695
iteration : 2500
train acc:  0.6953125
train loss:  0.5585517883300781
train gradient:  0.2082777086864815
iteration : 2501
train acc:  0.7109375
train loss:  0.5412881970405579
train gradient:  0.1404140884757816
iteration : 2502
train acc:  0.7421875
train loss:  0.5184364318847656
train gradient:  0.1174206427066267
iteration : 2503
train acc:  0.6640625
train loss:  0.5861485004425049
train gradient:  0.16671447647535206
iteration : 2504
train acc:  0.703125
train loss:  0.5526853203773499
train gradient:  0.160787302118386
iteration : 2505
train acc:  0.703125
train loss:  0.5776684284210205
train gradient:  0.21480362343858517
iteration : 2506
train acc:  0.7265625
train loss:  0.5133248567581177
train gradient:  0.15227023703432718
iteration : 2507
train acc:  0.78125
train loss:  0.4880296587944031
train gradient:  0.16345600338921668
iteration : 2508
train acc:  0.6953125
train loss:  0.6314178705215454
train gradient:  0.28513120856186147
iteration : 2509
train acc:  0.6953125
train loss:  0.571442723274231
train gradient:  0.19244981006418482
iteration : 2510
train acc:  0.78125
train loss:  0.48021793365478516
train gradient:  0.13854116440452896
iteration : 2511
train acc:  0.6953125
train loss:  0.5430926084518433
train gradient:  0.163307900772749
iteration : 2512
train acc:  0.671875
train loss:  0.5657212734222412
train gradient:  0.18460211503114862
iteration : 2513
train acc:  0.6484375
train loss:  0.5858399271965027
train gradient:  0.12313145546928493
iteration : 2514
train acc:  0.703125
train loss:  0.5566319227218628
train gradient:  0.12801739542140048
iteration : 2515
train acc:  0.7734375
train loss:  0.4691617488861084
train gradient:  0.13600747268934812
iteration : 2516
train acc:  0.6796875
train loss:  0.544593334197998
train gradient:  0.1659328130710915
iteration : 2517
train acc:  0.65625
train loss:  0.6318162083625793
train gradient:  0.18827882759794193
iteration : 2518
train acc:  0.7265625
train loss:  0.5419559478759766
train gradient:  0.21599186635007553
iteration : 2519
train acc:  0.71875
train loss:  0.537668764591217
train gradient:  0.17188088346003746
iteration : 2520
train acc:  0.7109375
train loss:  0.5851539373397827
train gradient:  0.16110477146007662
iteration : 2521
train acc:  0.7421875
train loss:  0.48749011754989624
train gradient:  0.1351265504269762
iteration : 2522
train acc:  0.703125
train loss:  0.5599393844604492
train gradient:  0.1587565575991271
iteration : 2523
train acc:  0.78125
train loss:  0.45558494329452515
train gradient:  0.1390073454650046
iteration : 2524
train acc:  0.7265625
train loss:  0.5653734803199768
train gradient:  0.16363235037108972
iteration : 2525
train acc:  0.703125
train loss:  0.5954543352127075
train gradient:  0.15781515593480375
iteration : 2526
train acc:  0.7109375
train loss:  0.5694197416305542
train gradient:  0.19859368321629672
iteration : 2527
train acc:  0.7265625
train loss:  0.5412222146987915
train gradient:  0.1781759202027538
iteration : 2528
train acc:  0.6875
train loss:  0.5860612392425537
train gradient:  0.18308583228158107
iteration : 2529
train acc:  0.6796875
train loss:  0.5731527209281921
train gradient:  0.18390585156729009
iteration : 2530
train acc:  0.7578125
train loss:  0.4981851279735565
train gradient:  0.1326488632538077
iteration : 2531
train acc:  0.7265625
train loss:  0.5103917121887207
train gradient:  0.14626973032715979
iteration : 2532
train acc:  0.671875
train loss:  0.56292724609375
train gradient:  0.20843632309855625
iteration : 2533
train acc:  0.75
train loss:  0.5354254245758057
train gradient:  0.1562165677071245
iteration : 2534
train acc:  0.7578125
train loss:  0.542496919631958
train gradient:  0.1530214748983188
iteration : 2535
train acc:  0.6953125
train loss:  0.5301070213317871
train gradient:  0.13219360050853549
iteration : 2536
train acc:  0.7265625
train loss:  0.5222436785697937
train gradient:  0.14311221747417252
iteration : 2537
train acc:  0.6953125
train loss:  0.5650568008422852
train gradient:  0.19493248229383658
iteration : 2538
train acc:  0.7421875
train loss:  0.5115728378295898
train gradient:  0.12983628791549962
iteration : 2539
train acc:  0.7421875
train loss:  0.5311331748962402
train gradient:  0.12936799432626425
iteration : 2540
train acc:  0.7421875
train loss:  0.579073429107666
train gradient:  0.17005954129200362
iteration : 2541
train acc:  0.71875
train loss:  0.5296158790588379
train gradient:  0.15886847482677297
iteration : 2542
train acc:  0.71875
train loss:  0.5306620597839355
train gradient:  0.17083848148734926
iteration : 2543
train acc:  0.6953125
train loss:  0.5839039087295532
train gradient:  0.16212751200844502
iteration : 2544
train acc:  0.71875
train loss:  0.5230197310447693
train gradient:  0.15668763555767534
iteration : 2545
train acc:  0.734375
train loss:  0.5049196481704712
train gradient:  0.14195174294033963
iteration : 2546
train acc:  0.671875
train loss:  0.5110464096069336
train gradient:  0.1451958241362205
iteration : 2547
train acc:  0.6015625
train loss:  0.6507012844085693
train gradient:  0.21015568916354735
iteration : 2548
train acc:  0.65625
train loss:  0.5932670831680298
train gradient:  0.21108549221646072
iteration : 2549
train acc:  0.7421875
train loss:  0.5063132643699646
train gradient:  0.14736500420160653
iteration : 2550
train acc:  0.640625
train loss:  0.5707430243492126
train gradient:  0.17580772937082528
iteration : 2551
train acc:  0.7265625
train loss:  0.49337226152420044
train gradient:  0.1504771609985875
iteration : 2552
train acc:  0.7421875
train loss:  0.5251137018203735
train gradient:  0.1815626586271068
iteration : 2553
train acc:  0.65625
train loss:  0.6505836248397827
train gradient:  0.21366278480800494
iteration : 2554
train acc:  0.671875
train loss:  0.5660427212715149
train gradient:  0.13879888751042466
iteration : 2555
train acc:  0.7265625
train loss:  0.523709774017334
train gradient:  0.1640582244058498
iteration : 2556
train acc:  0.7421875
train loss:  0.5146932005882263
train gradient:  0.1707436400364903
iteration : 2557
train acc:  0.6875
train loss:  0.5263293981552124
train gradient:  0.16739002611582382
iteration : 2558
train acc:  0.734375
train loss:  0.5379652976989746
train gradient:  0.17362403360962553
iteration : 2559
train acc:  0.6953125
train loss:  0.5952844619750977
train gradient:  0.15568042565229
iteration : 2560
train acc:  0.7109375
train loss:  0.564366340637207
train gradient:  0.14772250895735586
iteration : 2561
train acc:  0.71875
train loss:  0.5578292608261108
train gradient:  0.18679100831284529
iteration : 2562
train acc:  0.7734375
train loss:  0.48835289478302
train gradient:  0.16760550615055156
iteration : 2563
train acc:  0.6015625
train loss:  0.6224570274353027
train gradient:  0.18627224281960197
iteration : 2564
train acc:  0.734375
train loss:  0.533168613910675
train gradient:  0.2549364763586299
iteration : 2565
train acc:  0.703125
train loss:  0.5609315633773804
train gradient:  0.22368838383022405
iteration : 2566
train acc:  0.703125
train loss:  0.5300198793411255
train gradient:  0.11947252684896224
iteration : 2567
train acc:  0.75
train loss:  0.492736279964447
train gradient:  0.14719401121400408
iteration : 2568
train acc:  0.7109375
train loss:  0.5715892314910889
train gradient:  0.14420397636790228
iteration : 2569
train acc:  0.703125
train loss:  0.5193629264831543
train gradient:  0.14033365151293994
iteration : 2570
train acc:  0.734375
train loss:  0.5359330177307129
train gradient:  0.1738033998046137
iteration : 2571
train acc:  0.7421875
train loss:  0.5436351299285889
train gradient:  0.1437650191642289
iteration : 2572
train acc:  0.71875
train loss:  0.4856661558151245
train gradient:  0.1412500274807013
iteration : 2573
train acc:  0.671875
train loss:  0.5792443752288818
train gradient:  0.1612524673254469
iteration : 2574
train acc:  0.7109375
train loss:  0.5192571878433228
train gradient:  0.1889988638334119
iteration : 2575
train acc:  0.6875
train loss:  0.5361866354942322
train gradient:  0.1848274988978591
iteration : 2576
train acc:  0.6796875
train loss:  0.571114718914032
train gradient:  0.1849630370094287
iteration : 2577
train acc:  0.734375
train loss:  0.5352379083633423
train gradient:  0.20784590857014862
iteration : 2578
train acc:  0.6796875
train loss:  0.5656560659408569
train gradient:  0.2908191529344241
iteration : 2579
train acc:  0.7734375
train loss:  0.5137876272201538
train gradient:  0.1711966309603815
iteration : 2580
train acc:  0.7421875
train loss:  0.5281652212142944
train gradient:  0.17382034503646748
iteration : 2581
train acc:  0.671875
train loss:  0.5589950680732727
train gradient:  0.16716755450792664
iteration : 2582
train acc:  0.828125
train loss:  0.4396250247955322
train gradient:  0.12764882929552263
iteration : 2583
train acc:  0.6875
train loss:  0.5340099334716797
train gradient:  0.17795877289739637
iteration : 2584
train acc:  0.6640625
train loss:  0.5840215086936951
train gradient:  0.15289363046358637
iteration : 2585
train acc:  0.7109375
train loss:  0.5690900683403015
train gradient:  0.2662147465084421
iteration : 2586
train acc:  0.71875
train loss:  0.5776486396789551
train gradient:  0.1930502932338406
iteration : 2587
train acc:  0.65625
train loss:  0.5897189378738403
train gradient:  0.16440068756412202
iteration : 2588
train acc:  0.734375
train loss:  0.5140581130981445
train gradient:  0.15849282403089707
iteration : 2589
train acc:  0.65625
train loss:  0.6062494516372681
train gradient:  0.22065804831289826
iteration : 2590
train acc:  0.7578125
train loss:  0.4822654724121094
train gradient:  0.1481522141334241
iteration : 2591
train acc:  0.765625
train loss:  0.47680026292800903
train gradient:  0.13718070239789631
iteration : 2592
train acc:  0.6484375
train loss:  0.5706660151481628
train gradient:  0.29395151906684125
iteration : 2593
train acc:  0.7265625
train loss:  0.5172151923179626
train gradient:  0.14784910034805482
iteration : 2594
train acc:  0.703125
train loss:  0.5671898126602173
train gradient:  0.1790024958125121
iteration : 2595
train acc:  0.7109375
train loss:  0.5343915224075317
train gradient:  0.15581839232783298
iteration : 2596
train acc:  0.6875
train loss:  0.5757535696029663
train gradient:  0.1587436559160992
iteration : 2597
train acc:  0.703125
train loss:  0.5328031778335571
train gradient:  0.1444169583763335
iteration : 2598
train acc:  0.671875
train loss:  0.5710419416427612
train gradient:  0.21425382297300022
iteration : 2599
train acc:  0.7265625
train loss:  0.5374441742897034
train gradient:  0.14927794438829822
iteration : 2600
train acc:  0.7421875
train loss:  0.5181765556335449
train gradient:  0.15421875397414983
iteration : 2601
train acc:  0.7421875
train loss:  0.539081871509552
train gradient:  0.23371551995319212
iteration : 2602
train acc:  0.7734375
train loss:  0.48175176978111267
train gradient:  0.13725978348037193
iteration : 2603
train acc:  0.7265625
train loss:  0.5373507738113403
train gradient:  0.159706405136662
iteration : 2604
train acc:  0.703125
train loss:  0.5406217575073242
train gradient:  0.14054699090933515
iteration : 2605
train acc:  0.6875
train loss:  0.6008758544921875
train gradient:  0.2370810199735706
iteration : 2606
train acc:  0.703125
train loss:  0.5296652317047119
train gradient:  0.16685220192147043
iteration : 2607
train acc:  0.7421875
train loss:  0.5612537264823914
train gradient:  0.18633319724646596
iteration : 2608
train acc:  0.75
train loss:  0.5064420700073242
train gradient:  0.15646700758836882
iteration : 2609
train acc:  0.6640625
train loss:  0.5693769454956055
train gradient:  0.17610107453761645
iteration : 2610
train acc:  0.734375
train loss:  0.500920295715332
train gradient:  0.1572242492374059
iteration : 2611
train acc:  0.7890625
train loss:  0.468900591135025
train gradient:  0.13157424045144678
iteration : 2612
train acc:  0.65625
train loss:  0.6770754456520081
train gradient:  0.27707463844106395
iteration : 2613
train acc:  0.7578125
train loss:  0.5288964509963989
train gradient:  0.14680646806164466
iteration : 2614
train acc:  0.6875
train loss:  0.5920261144638062
train gradient:  0.16672299142911473
iteration : 2615
train acc:  0.703125
train loss:  0.5293318033218384
train gradient:  0.17616729416952398
iteration : 2616
train acc:  0.734375
train loss:  0.5122460126876831
train gradient:  0.15556840988772466
iteration : 2617
train acc:  0.6953125
train loss:  0.5867424011230469
train gradient:  0.19756726241904343
iteration : 2618
train acc:  0.7421875
train loss:  0.4821224808692932
train gradient:  0.1299983839547677
iteration : 2619
train acc:  0.6640625
train loss:  0.5709258317947388
train gradient:  0.17301944488027016
iteration : 2620
train acc:  0.734375
train loss:  0.4768730401992798
train gradient:  0.11456945203246045
iteration : 2621
train acc:  0.7265625
train loss:  0.5254657864570618
train gradient:  0.18032462308527103
iteration : 2622
train acc:  0.7265625
train loss:  0.5190901160240173
train gradient:  0.16059496801557757
iteration : 2623
train acc:  0.734375
train loss:  0.5084894299507141
train gradient:  0.1658061548050898
iteration : 2624
train acc:  0.6875
train loss:  0.5472279787063599
train gradient:  0.15495537177455732
iteration : 2625
train acc:  0.7734375
train loss:  0.47607916593551636
train gradient:  0.13647642690175302
iteration : 2626
train acc:  0.78125
train loss:  0.511981725692749
train gradient:  0.14958740684662294
iteration : 2627
train acc:  0.703125
train loss:  0.5577256083488464
train gradient:  0.16371491249181067
iteration : 2628
train acc:  0.71875
train loss:  0.494584858417511
train gradient:  0.13324182125253414
iteration : 2629
train acc:  0.609375
train loss:  0.6509596109390259
train gradient:  0.2008979497805788
iteration : 2630
train acc:  0.7421875
train loss:  0.517780065536499
train gradient:  0.1362125750985304
iteration : 2631
train acc:  0.703125
train loss:  0.6078011989593506
train gradient:  0.18170755826731988
iteration : 2632
train acc:  0.6796875
train loss:  0.571031928062439
train gradient:  0.21397629557091713
iteration : 2633
train acc:  0.7421875
train loss:  0.5164471864700317
train gradient:  0.14034250231373885
iteration : 2634
train acc:  0.7265625
train loss:  0.5201097130775452
train gradient:  0.18328955865262464
iteration : 2635
train acc:  0.75
train loss:  0.509190559387207
train gradient:  0.183886448896017
iteration : 2636
train acc:  0.734375
train loss:  0.5417356491088867
train gradient:  0.16457319533826822
iteration : 2637
train acc:  0.75
train loss:  0.5323137044906616
train gradient:  0.2037069025271455
iteration : 2638
train acc:  0.7421875
train loss:  0.5286881923675537
train gradient:  0.16043516244035771
iteration : 2639
train acc:  0.71875
train loss:  0.5839565992355347
train gradient:  0.20136277101359434
iteration : 2640
train acc:  0.671875
train loss:  0.5829403400421143
train gradient:  0.21357618240484333
iteration : 2641
train acc:  0.78125
train loss:  0.466096431016922
train gradient:  0.1438840724822438
iteration : 2642
train acc:  0.7265625
train loss:  0.5285191535949707
train gradient:  0.12761664702746456
iteration : 2643
train acc:  0.75
train loss:  0.5100206136703491
train gradient:  0.159980407221327
iteration : 2644
train acc:  0.6640625
train loss:  0.5448176264762878
train gradient:  0.2226416886278732
iteration : 2645
train acc:  0.6640625
train loss:  0.6122621297836304
train gradient:  0.22489199066665222
iteration : 2646
train acc:  0.7421875
train loss:  0.5253697633743286
train gradient:  0.17212863718528676
iteration : 2647
train acc:  0.65625
train loss:  0.6115001440048218
train gradient:  0.2089122326891366
iteration : 2648
train acc:  0.65625
train loss:  0.5516339540481567
train gradient:  0.15187889898384777
iteration : 2649
train acc:  0.6875
train loss:  0.5537787675857544
train gradient:  0.14985228926370184
iteration : 2650
train acc:  0.6484375
train loss:  0.6429132223129272
train gradient:  0.18506240487231218
iteration : 2651
train acc:  0.765625
train loss:  0.5117105841636658
train gradient:  0.12270874275461216
iteration : 2652
train acc:  0.84375
train loss:  0.4521498680114746
train gradient:  0.1438099052806634
iteration : 2653
train acc:  0.6640625
train loss:  0.6071437001228333
train gradient:  0.21933907893549687
iteration : 2654
train acc:  0.765625
train loss:  0.47957050800323486
train gradient:  0.14113340023961024
iteration : 2655
train acc:  0.71875
train loss:  0.5271428227424622
train gradient:  0.1708845477188152
iteration : 2656
train acc:  0.703125
train loss:  0.5700676441192627
train gradient:  0.214638701122967
iteration : 2657
train acc:  0.6328125
train loss:  0.5846282839775085
train gradient:  0.1483798088705247
iteration : 2658
train acc:  0.7109375
train loss:  0.5552530288696289
train gradient:  0.24240454144522966
iteration : 2659
train acc:  0.734375
train loss:  0.5209356546401978
train gradient:  0.16272436868618084
iteration : 2660
train acc:  0.6875
train loss:  0.5260955095291138
train gradient:  0.12289779931466835
iteration : 2661
train acc:  0.6796875
train loss:  0.5907058715820312
train gradient:  0.204574823350185
iteration : 2662
train acc:  0.7265625
train loss:  0.496094286441803
train gradient:  0.16825107016456162
iteration : 2663
train acc:  0.703125
train loss:  0.5594668984413147
train gradient:  0.19265085765440423
iteration : 2664
train acc:  0.625
train loss:  0.5990172624588013
train gradient:  0.21177309756470053
iteration : 2665
train acc:  0.671875
train loss:  0.6026873588562012
train gradient:  0.24980366479254573
iteration : 2666
train acc:  0.8046875
train loss:  0.42731204628944397
train gradient:  0.12254770213186617
iteration : 2667
train acc:  0.6953125
train loss:  0.5707437992095947
train gradient:  0.20883371322222938
iteration : 2668
train acc:  0.6796875
train loss:  0.5387154817581177
train gradient:  0.17506223159883874
iteration : 2669
train acc:  0.765625
train loss:  0.5300052165985107
train gradient:  0.16397475218326404
iteration : 2670
train acc:  0.7578125
train loss:  0.498389333486557
train gradient:  0.14176286235915492
iteration : 2671
train acc:  0.703125
train loss:  0.5737937688827515
train gradient:  0.22746328319715847
iteration : 2672
train acc:  0.703125
train loss:  0.5957090258598328
train gradient:  0.19178193927575424
iteration : 2673
train acc:  0.6953125
train loss:  0.559730589389801
train gradient:  0.16138829194879442
iteration : 2674
train acc:  0.78125
train loss:  0.45732346177101135
train gradient:  0.10277237743655496
iteration : 2675
train acc:  0.75
train loss:  0.5155004262924194
train gradient:  0.16821344400637897
iteration : 2676
train acc:  0.6953125
train loss:  0.5605098009109497
train gradient:  0.17667874798630884
iteration : 2677
train acc:  0.7578125
train loss:  0.48237133026123047
train gradient:  0.15070410934663067
iteration : 2678
train acc:  0.7265625
train loss:  0.5178191661834717
train gradient:  0.1756086582088725
iteration : 2679
train acc:  0.734375
train loss:  0.5293492078781128
train gradient:  0.16212891143859598
iteration : 2680
train acc:  0.734375
train loss:  0.524520754814148
train gradient:  0.15866125731224867
iteration : 2681
train acc:  0.6953125
train loss:  0.5209752321243286
train gradient:  0.18505188214934398
iteration : 2682
train acc:  0.7421875
train loss:  0.4814783036708832
train gradient:  0.14276882001602872
iteration : 2683
train acc:  0.734375
train loss:  0.5058296918869019
train gradient:  0.15371328679125051
iteration : 2684
train acc:  0.7734375
train loss:  0.4726998805999756
train gradient:  0.13738639071261638
iteration : 2685
train acc:  0.7421875
train loss:  0.5061401724815369
train gradient:  0.16829178445726328
iteration : 2686
train acc:  0.71875
train loss:  0.5687578916549683
train gradient:  0.16865791267620683
iteration : 2687
train acc:  0.6796875
train loss:  0.5321130156517029
train gradient:  0.20094967444190065
iteration : 2688
train acc:  0.703125
train loss:  0.5682152509689331
train gradient:  0.17040941479233618
iteration : 2689
train acc:  0.609375
train loss:  0.6153422594070435
train gradient:  0.24979615782170378
iteration : 2690
train acc:  0.6875
train loss:  0.5499331951141357
train gradient:  0.16983992847597604
iteration : 2691
train acc:  0.703125
train loss:  0.5283298492431641
train gradient:  0.18214311913895148
iteration : 2692
train acc:  0.703125
train loss:  0.5398866534233093
train gradient:  0.18760028241937837
iteration : 2693
train acc:  0.78125
train loss:  0.43692126870155334
train gradient:  0.15154983708286812
iteration : 2694
train acc:  0.7265625
train loss:  0.5860376954078674
train gradient:  0.1862060710675601
iteration : 2695
train acc:  0.6953125
train loss:  0.5553717613220215
train gradient:  0.1697712908426835
iteration : 2696
train acc:  0.7265625
train loss:  0.54839026927948
train gradient:  0.15165277366842245
iteration : 2697
train acc:  0.7421875
train loss:  0.5015323162078857
train gradient:  0.22123515983621403
iteration : 2698
train acc:  0.78125
train loss:  0.48018878698349
train gradient:  0.14569044009575272
iteration : 2699
train acc:  0.671875
train loss:  0.5687546133995056
train gradient:  0.24227023265390712
iteration : 2700
train acc:  0.7265625
train loss:  0.525263786315918
train gradient:  0.17103255476033818
iteration : 2701
train acc:  0.703125
train loss:  0.564699649810791
train gradient:  0.19683875752890642
iteration : 2702
train acc:  0.7265625
train loss:  0.5116037130355835
train gradient:  0.1359277023886697
iteration : 2703
train acc:  0.6796875
train loss:  0.5826468467712402
train gradient:  0.1852582016075047
iteration : 2704
train acc:  0.7109375
train loss:  0.5294805765151978
train gradient:  0.13166446559401174
iteration : 2705
train acc:  0.7578125
train loss:  0.49554744362831116
train gradient:  0.1812471612614291
iteration : 2706
train acc:  0.671875
train loss:  0.5482917428016663
train gradient:  0.16408173133845738
iteration : 2707
train acc:  0.78125
train loss:  0.5130882263183594
train gradient:  0.10601669038919817
iteration : 2708
train acc:  0.6484375
train loss:  0.5241098403930664
train gradient:  0.14924184570966437
iteration : 2709
train acc:  0.71875
train loss:  0.5543792247772217
train gradient:  0.15892744919543397
iteration : 2710
train acc:  0.734375
train loss:  0.5663586854934692
train gradient:  0.18042288240127025
iteration : 2711
train acc:  0.6640625
train loss:  0.5863802433013916
train gradient:  0.1490918553084868
iteration : 2712
train acc:  0.75
train loss:  0.5404374599456787
train gradient:  0.19715160978791935
iteration : 2713
train acc:  0.6953125
train loss:  0.5861151218414307
train gradient:  0.21817794901648163
iteration : 2714
train acc:  0.7421875
train loss:  0.5019522905349731
train gradient:  0.1262243580190572
iteration : 2715
train acc:  0.65625
train loss:  0.6271194815635681
train gradient:  0.24759630665025617
iteration : 2716
train acc:  0.703125
train loss:  0.5801746845245361
train gradient:  0.18013972217177854
iteration : 2717
train acc:  0.6328125
train loss:  0.6487115621566772
train gradient:  0.18547197059510306
iteration : 2718
train acc:  0.703125
train loss:  0.5741587281227112
train gradient:  0.1828410381078136
iteration : 2719
train acc:  0.6484375
train loss:  0.6264211535453796
train gradient:  0.21011443543841563
iteration : 2720
train acc:  0.7734375
train loss:  0.45790398120880127
train gradient:  0.10432251304086851
iteration : 2721
train acc:  0.7578125
train loss:  0.5106483697891235
train gradient:  0.14982862950233972
iteration : 2722
train acc:  0.734375
train loss:  0.5337834358215332
train gradient:  0.182616706520483
iteration : 2723
train acc:  0.796875
train loss:  0.44363823533058167
train gradient:  0.11405731701402774
iteration : 2724
train acc:  0.796875
train loss:  0.45933812856674194
train gradient:  0.1450889021883729
iteration : 2725
train acc:  0.6484375
train loss:  0.6255618333816528
train gradient:  0.25068843337726343
iteration : 2726
train acc:  0.734375
train loss:  0.5152996778488159
train gradient:  0.12810651636896725
iteration : 2727
train acc:  0.734375
train loss:  0.5103813409805298
train gradient:  0.19606267664073868
iteration : 2728
train acc:  0.6640625
train loss:  0.6013442277908325
train gradient:  0.24304588164107338
iteration : 2729
train acc:  0.7265625
train loss:  0.5296987295150757
train gradient:  0.17701670637772587
iteration : 2730
train acc:  0.734375
train loss:  0.5046553611755371
train gradient:  0.1915546428614718
iteration : 2731
train acc:  0.6796875
train loss:  0.5809355974197388
train gradient:  0.20160655165993868
iteration : 2732
train acc:  0.6875
train loss:  0.5873029232025146
train gradient:  0.21857486837182966
iteration : 2733
train acc:  0.6875
train loss:  0.5316621661186218
train gradient:  0.1632694197337633
iteration : 2734
train acc:  0.7265625
train loss:  0.5385938286781311
train gradient:  0.1306115413581198
iteration : 2735
train acc:  0.6953125
train loss:  0.5543387532234192
train gradient:  0.15246668755749682
iteration : 2736
train acc:  0.703125
train loss:  0.6049417853355408
train gradient:  0.16848267969295228
iteration : 2737
train acc:  0.7265625
train loss:  0.5316985845565796
train gradient:  0.14808929026250411
iteration : 2738
train acc:  0.65625
train loss:  0.5680254101753235
train gradient:  0.1756787606144241
iteration : 2739
train acc:  0.765625
train loss:  0.47276759147644043
train gradient:  0.1366399940120075
iteration : 2740
train acc:  0.7578125
train loss:  0.48411107063293457
train gradient:  0.13143071667769968
iteration : 2741
train acc:  0.7578125
train loss:  0.5142470598220825
train gradient:  0.1387577986436053
iteration : 2742
train acc:  0.7265625
train loss:  0.5062982439994812
train gradient:  0.1366425677579626
iteration : 2743
train acc:  0.7578125
train loss:  0.5013396739959717
train gradient:  0.11377767555390947
iteration : 2744
train acc:  0.6953125
train loss:  0.5536613464355469
train gradient:  0.16629804006081422
iteration : 2745
train acc:  0.734375
train loss:  0.5045104622840881
train gradient:  0.12774907867267046
iteration : 2746
train acc:  0.71875
train loss:  0.5180357098579407
train gradient:  0.15133250601115883
iteration : 2747
train acc:  0.6796875
train loss:  0.5686378479003906
train gradient:  0.16264949925279842
iteration : 2748
train acc:  0.75
train loss:  0.5080952644348145
train gradient:  0.1724082390803456
iteration : 2749
train acc:  0.78125
train loss:  0.4784975051879883
train gradient:  0.14428979029601086
iteration : 2750
train acc:  0.734375
train loss:  0.49150514602661133
train gradient:  0.13199972105921431
iteration : 2751
train acc:  0.7109375
train loss:  0.5743545293807983
train gradient:  0.21315229723995682
iteration : 2752
train acc:  0.7421875
train loss:  0.47052258253097534
train gradient:  0.10962870805961701
iteration : 2753
train acc:  0.7109375
train loss:  0.5535495281219482
train gradient:  0.22406008800498572
iteration : 2754
train acc:  0.6953125
train loss:  0.5299471616744995
train gradient:  0.1993354449797317
iteration : 2755
train acc:  0.6953125
train loss:  0.5701166391372681
train gradient:  0.1339048030231167
iteration : 2756
train acc:  0.71875
train loss:  0.5286239385604858
train gradient:  0.16218586130699275
iteration : 2757
train acc:  0.703125
train loss:  0.5516402125358582
train gradient:  0.15210589391643758
iteration : 2758
train acc:  0.703125
train loss:  0.5479599833488464
train gradient:  0.15847236669059628
iteration : 2759
train acc:  0.71875
train loss:  0.5532048940658569
train gradient:  0.178138384879416
iteration : 2760
train acc:  0.7109375
train loss:  0.5681645274162292
train gradient:  0.15315105936310994
iteration : 2761
train acc:  0.6953125
train loss:  0.5555839538574219
train gradient:  0.18688502755663644
iteration : 2762
train acc:  0.7578125
train loss:  0.5131071209907532
train gradient:  0.23901921440610602
iteration : 2763
train acc:  0.5859375
train loss:  0.6657665967941284
train gradient:  0.22608409793239928
iteration : 2764
train acc:  0.734375
train loss:  0.5267991423606873
train gradient:  0.16921332744868028
iteration : 2765
train acc:  0.671875
train loss:  0.5586065053939819
train gradient:  0.20748245731007292
iteration : 2766
train acc:  0.734375
train loss:  0.5479403734207153
train gradient:  0.1553844558573924
iteration : 2767
train acc:  0.828125
train loss:  0.4525372087955475
train gradient:  0.17314855047981148
iteration : 2768
train acc:  0.7109375
train loss:  0.5143926739692688
train gradient:  0.171541012427981
iteration : 2769
train acc:  0.7265625
train loss:  0.5400034189224243
train gradient:  0.1643221781936443
iteration : 2770
train acc:  0.71875
train loss:  0.5683872699737549
train gradient:  0.18875765172899894
iteration : 2771
train acc:  0.7421875
train loss:  0.545455276966095
train gradient:  0.15510323559705275
iteration : 2772
train acc:  0.765625
train loss:  0.5114138126373291
train gradient:  0.15943210387442136
iteration : 2773
train acc:  0.71875
train loss:  0.5517755150794983
train gradient:  0.19419055240059946
iteration : 2774
train acc:  0.6796875
train loss:  0.5607364773750305
train gradient:  0.17191877157095084
iteration : 2775
train acc:  0.703125
train loss:  0.5416433811187744
train gradient:  0.14537701783399953
iteration : 2776
train acc:  0.8046875
train loss:  0.45833492279052734
train gradient:  0.19006204513388023
iteration : 2777
train acc:  0.765625
train loss:  0.4743073284626007
train gradient:  0.14402920705103417
iteration : 2778
train acc:  0.7421875
train loss:  0.49619191884994507
train gradient:  0.1318924565950806
iteration : 2779
train acc:  0.734375
train loss:  0.5495119094848633
train gradient:  0.1733593715209855
iteration : 2780
train acc:  0.7109375
train loss:  0.525566577911377
train gradient:  0.26618011536367386
iteration : 2781
train acc:  0.7265625
train loss:  0.550663948059082
train gradient:  0.18133716817927514
iteration : 2782
train acc:  0.6640625
train loss:  0.6043360829353333
train gradient:  0.16242050338617028
iteration : 2783
train acc:  0.71875
train loss:  0.5155239105224609
train gradient:  0.16167614076742376
iteration : 2784
train acc:  0.6328125
train loss:  0.602613091468811
train gradient:  0.23640993280899045
iteration : 2785
train acc:  0.6640625
train loss:  0.6287020444869995
train gradient:  0.24883520076504673
iteration : 2786
train acc:  0.7421875
train loss:  0.5304520726203918
train gradient:  0.16591834895912244
iteration : 2787
train acc:  0.734375
train loss:  0.5543990731239319
train gradient:  0.16773117177111194
iteration : 2788
train acc:  0.7421875
train loss:  0.508420467376709
train gradient:  0.1346814114966149
iteration : 2789
train acc:  0.6640625
train loss:  0.5692359209060669
train gradient:  0.17068793643000424
iteration : 2790
train acc:  0.6796875
train loss:  0.583387017250061
train gradient:  0.23659831488296218
iteration : 2791
train acc:  0.734375
train loss:  0.5347094535827637
train gradient:  0.1576116455209522
iteration : 2792
train acc:  0.71875
train loss:  0.5436376333236694
train gradient:  0.2093697987770607
iteration : 2793
train acc:  0.6171875
train loss:  0.6187161207199097
train gradient:  0.2722786430789454
iteration : 2794
train acc:  0.71875
train loss:  0.546003520488739
train gradient:  0.16668793556727834
iteration : 2795
train acc:  0.703125
train loss:  0.5671916007995605
train gradient:  0.15673473490120315
iteration : 2796
train acc:  0.7265625
train loss:  0.48512542247772217
train gradient:  0.12049537982866404
iteration : 2797
train acc:  0.71875
train loss:  0.5837793946266174
train gradient:  0.16419290647565724
iteration : 2798
train acc:  0.75
train loss:  0.4890698790550232
train gradient:  0.12163378475566056
iteration : 2799
train acc:  0.78125
train loss:  0.507601261138916
train gradient:  0.13506418877360146
iteration : 2800
train acc:  0.71875
train loss:  0.49923521280288696
train gradient:  0.142574711505984
iteration : 2801
train acc:  0.703125
train loss:  0.5497514009475708
train gradient:  0.17589941630505088
iteration : 2802
train acc:  0.6640625
train loss:  0.5863332748413086
train gradient:  0.20334895661678348
iteration : 2803
train acc:  0.7109375
train loss:  0.5058339834213257
train gradient:  0.17395273402665157
iteration : 2804
train acc:  0.734375
train loss:  0.5324357748031616
train gradient:  0.1338700997213828
iteration : 2805
train acc:  0.671875
train loss:  0.6057955622673035
train gradient:  0.19553246792816995
iteration : 2806
train acc:  0.7421875
train loss:  0.4935741126537323
train gradient:  0.1312085170341117
iteration : 2807
train acc:  0.7265625
train loss:  0.4904527962207794
train gradient:  0.12443134750950047
iteration : 2808
train acc:  0.734375
train loss:  0.5351651906967163
train gradient:  0.1512693204889728
iteration : 2809
train acc:  0.703125
train loss:  0.5437511205673218
train gradient:  0.18407543765941115
iteration : 2810
train acc:  0.7265625
train loss:  0.5459703803062439
train gradient:  0.1811851945670596
iteration : 2811
train acc:  0.71875
train loss:  0.5872668027877808
train gradient:  0.20495243321509865
iteration : 2812
train acc:  0.71875
train loss:  0.5310133695602417
train gradient:  0.16117032311542429
iteration : 2813
train acc:  0.7265625
train loss:  0.5434765815734863
train gradient:  0.1574759118524483
iteration : 2814
train acc:  0.65625
train loss:  0.6650636196136475
train gradient:  0.2645165903765803
iteration : 2815
train acc:  0.734375
train loss:  0.48210904002189636
train gradient:  0.1466209477978327
iteration : 2816
train acc:  0.65625
train loss:  0.623386025428772
train gradient:  0.21847861549572561
iteration : 2817
train acc:  0.7734375
train loss:  0.5123348832130432
train gradient:  0.2256651161640736
iteration : 2818
train acc:  0.6875
train loss:  0.5621455907821655
train gradient:  0.2046164438504906
iteration : 2819
train acc:  0.734375
train loss:  0.5119037628173828
train gradient:  0.1820339334466072
iteration : 2820
train acc:  0.75
train loss:  0.5137935280799866
train gradient:  0.14985336028462654
iteration : 2821
train acc:  0.7265625
train loss:  0.524971067905426
train gradient:  0.1587230257262781
iteration : 2822
train acc:  0.7890625
train loss:  0.48388800024986267
train gradient:  0.21277002538125872
iteration : 2823
train acc:  0.71875
train loss:  0.5674525499343872
train gradient:  0.171560394932318
iteration : 2824
train acc:  0.7421875
train loss:  0.5497028827667236
train gradient:  0.14461418726078695
iteration : 2825
train acc:  0.8125
train loss:  0.49813345074653625
train gradient:  0.16128220160596812
iteration : 2826
train acc:  0.7265625
train loss:  0.5201047658920288
train gradient:  0.15950544037251868
iteration : 2827
train acc:  0.7109375
train loss:  0.48583030700683594
train gradient:  0.12217385329080394
iteration : 2828
train acc:  0.7109375
train loss:  0.5393487215042114
train gradient:  0.17889312438885968
iteration : 2829
train acc:  0.7578125
train loss:  0.503822922706604
train gradient:  0.1377980571318601
iteration : 2830
train acc:  0.7578125
train loss:  0.5187810063362122
train gradient:  0.22656570633533707
iteration : 2831
train acc:  0.7578125
train loss:  0.47694510221481323
train gradient:  0.12188078896068862
iteration : 2832
train acc:  0.7265625
train loss:  0.5537809133529663
train gradient:  0.16145019857031478
iteration : 2833
train acc:  0.734375
train loss:  0.48332321643829346
train gradient:  0.14240342136153605
iteration : 2834
train acc:  0.671875
train loss:  0.6005403995513916
train gradient:  0.26482455841573105
iteration : 2835
train acc:  0.6953125
train loss:  0.580645740032196
train gradient:  0.14588680370624846
iteration : 2836
train acc:  0.7578125
train loss:  0.4688073396682739
train gradient:  0.12209442074247415
iteration : 2837
train acc:  0.640625
train loss:  0.6063337326049805
train gradient:  0.21306853511046772
iteration : 2838
train acc:  0.7890625
train loss:  0.4728407859802246
train gradient:  0.13259834250090885
iteration : 2839
train acc:  0.734375
train loss:  0.5469221472740173
train gradient:  0.28888849694939706
iteration : 2840
train acc:  0.7265625
train loss:  0.5345252752304077
train gradient:  0.1589209050203222
iteration : 2841
train acc:  0.6640625
train loss:  0.6217496395111084
train gradient:  0.2267649912453581
iteration : 2842
train acc:  0.671875
train loss:  0.5445418357849121
train gradient:  0.16011339514340045
iteration : 2843
train acc:  0.7421875
train loss:  0.5607349276542664
train gradient:  0.21567261311515495
iteration : 2844
train acc:  0.6953125
train loss:  0.5475964546203613
train gradient:  0.17321068991364189
iteration : 2845
train acc:  0.6640625
train loss:  0.6226623058319092
train gradient:  0.19550594925828685
iteration : 2846
train acc:  0.7421875
train loss:  0.5245457887649536
train gradient:  0.1460226050759992
iteration : 2847
train acc:  0.734375
train loss:  0.5400035977363586
train gradient:  0.16242375149827462
iteration : 2848
train acc:  0.734375
train loss:  0.5152441263198853
train gradient:  0.1526129068462766
iteration : 2849
train acc:  0.7890625
train loss:  0.4648064970970154
train gradient:  0.1482401203608517
iteration : 2850
train acc:  0.734375
train loss:  0.5296801328659058
train gradient:  0.21185927549983485
iteration : 2851
train acc:  0.765625
train loss:  0.49409574270248413
train gradient:  0.14204637358701738
iteration : 2852
train acc:  0.7265625
train loss:  0.5139798521995544
train gradient:  0.15023456205917818
iteration : 2853
train acc:  0.7265625
train loss:  0.5483737587928772
train gradient:  0.15466119415297785
iteration : 2854
train acc:  0.7734375
train loss:  0.5021299123764038
train gradient:  0.15095615480231497
iteration : 2855
train acc:  0.671875
train loss:  0.5904483795166016
train gradient:  0.19608893721848059
iteration : 2856
train acc:  0.7109375
train loss:  0.5769898891448975
train gradient:  0.2626924563949965
iteration : 2857
train acc:  0.6796875
train loss:  0.573746383190155
train gradient:  0.21195463794002511
iteration : 2858
train acc:  0.78125
train loss:  0.48503145575523376
train gradient:  0.13126532137727076
iteration : 2859
train acc:  0.703125
train loss:  0.5236678719520569
train gradient:  0.1531370303065392
iteration : 2860
train acc:  0.765625
train loss:  0.5125688910484314
train gradient:  0.17716643940828503
iteration : 2861
train acc:  0.6875
train loss:  0.5402771234512329
train gradient:  0.14030987212241258
iteration : 2862
train acc:  0.78125
train loss:  0.48066169023513794
train gradient:  0.12694412322434245
iteration : 2863
train acc:  0.7578125
train loss:  0.482876181602478
train gradient:  0.1307396354872003
iteration : 2864
train acc:  0.703125
train loss:  0.5344455242156982
train gradient:  0.15013750939799958
iteration : 2865
train acc:  0.7109375
train loss:  0.5502173900604248
train gradient:  0.14360669662475495
iteration : 2866
train acc:  0.7265625
train loss:  0.5333898067474365
train gradient:  0.13916476106283876
iteration : 2867
train acc:  0.71875
train loss:  0.5292856693267822
train gradient:  0.16638951700123386
iteration : 2868
train acc:  0.671875
train loss:  0.582415759563446
train gradient:  0.1765854223648322
iteration : 2869
train acc:  0.7109375
train loss:  0.5268853902816772
train gradient:  0.16078881956978286
iteration : 2870
train acc:  0.65625
train loss:  0.6002507209777832
train gradient:  0.19003514129478555
iteration : 2871
train acc:  0.734375
train loss:  0.5240944027900696
train gradient:  0.20880700701965324
iteration : 2872
train acc:  0.6796875
train loss:  0.5504240989685059
train gradient:  0.18181796508602216
iteration : 2873
train acc:  0.703125
train loss:  0.5197869539260864
train gradient:  0.14879350729748242
iteration : 2874
train acc:  0.703125
train loss:  0.5253861546516418
train gradient:  0.1689594077455947
iteration : 2875
train acc:  0.7890625
train loss:  0.4877721667289734
train gradient:  0.11527057126601904
iteration : 2876
train acc:  0.75
train loss:  0.49601614475250244
train gradient:  0.15201934492388317
iteration : 2877
train acc:  0.796875
train loss:  0.43170952796936035
train gradient:  0.12002065576848278
iteration : 2878
train acc:  0.671875
train loss:  0.5721941590309143
train gradient:  0.17478705307025966
iteration : 2879
train acc:  0.7578125
train loss:  0.5161370038986206
train gradient:  0.21354173263472354
iteration : 2880
train acc:  0.7578125
train loss:  0.48184502124786377
train gradient:  0.12704509020682014
iteration : 2881
train acc:  0.7578125
train loss:  0.5171583294868469
train gradient:  0.17133472940618322
iteration : 2882
train acc:  0.734375
train loss:  0.5233431458473206
train gradient:  0.13988087668088972
iteration : 2883
train acc:  0.671875
train loss:  0.597475528717041
train gradient:  0.19041108085683184
iteration : 2884
train acc:  0.625
train loss:  0.6380702257156372
train gradient:  0.17058080453877184
iteration : 2885
train acc:  0.78125
train loss:  0.4731825292110443
train gradient:  0.10379342584602577
iteration : 2886
train acc:  0.6796875
train loss:  0.5806121826171875
train gradient:  0.16352582532697912
iteration : 2887
train acc:  0.765625
train loss:  0.5301949977874756
train gradient:  0.1718069707002711
iteration : 2888
train acc:  0.765625
train loss:  0.4845968782901764
train gradient:  0.12527180679692998
iteration : 2889
train acc:  0.6796875
train loss:  0.5449048280715942
train gradient:  0.15788706061884955
iteration : 2890
train acc:  0.6796875
train loss:  0.5756582021713257
train gradient:  0.20033353442877216
iteration : 2891
train acc:  0.765625
train loss:  0.47634124755859375
train gradient:  0.1818826517984427
iteration : 2892
train acc:  0.734375
train loss:  0.5475269556045532
train gradient:  0.15006848208294815
iteration : 2893
train acc:  0.71875
train loss:  0.5573194026947021
train gradient:  0.15665283467749197
iteration : 2894
train acc:  0.7109375
train loss:  0.5652173757553101
train gradient:  0.19808831040095834
iteration : 2895
train acc:  0.765625
train loss:  0.5235016345977783
train gradient:  0.16714545112202706
iteration : 2896
train acc:  0.734375
train loss:  0.5443870425224304
train gradient:  0.1989013481224559
iteration : 2897
train acc:  0.75
train loss:  0.49973610043525696
train gradient:  0.14978672165668822
iteration : 2898
train acc:  0.7265625
train loss:  0.5132833123207092
train gradient:  0.1492334675370489
iteration : 2899
train acc:  0.734375
train loss:  0.5049456357955933
train gradient:  0.16563225229907647
iteration : 2900
train acc:  0.703125
train loss:  0.5105332136154175
train gradient:  0.14481784742926626
iteration : 2901
train acc:  0.8203125
train loss:  0.421977162361145
train gradient:  0.1367911814506147
iteration : 2902
train acc:  0.6953125
train loss:  0.5307325720787048
train gradient:  0.1826149222342745
iteration : 2903
train acc:  0.7421875
train loss:  0.49374353885650635
train gradient:  0.14141340217027515
iteration : 2904
train acc:  0.71875
train loss:  0.5111120343208313
train gradient:  0.1513801658075039
iteration : 2905
train acc:  0.703125
train loss:  0.5784900188446045
train gradient:  0.26216382697847807
iteration : 2906
train acc:  0.6796875
train loss:  0.6616657376289368
train gradient:  0.20472225271268538
iteration : 2907
train acc:  0.7109375
train loss:  0.5340753793716431
train gradient:  0.15428625150221448
iteration : 2908
train acc:  0.6796875
train loss:  0.5233758687973022
train gradient:  0.15474142578442943
iteration : 2909
train acc:  0.7421875
train loss:  0.5082690715789795
train gradient:  0.19973976965226542
iteration : 2910
train acc:  0.7265625
train loss:  0.5133039951324463
train gradient:  0.1740528948875562
iteration : 2911
train acc:  0.765625
train loss:  0.4869043231010437
train gradient:  0.1250528041497475
iteration : 2912
train acc:  0.640625
train loss:  0.5485832691192627
train gradient:  0.21475933254984772
iteration : 2913
train acc:  0.7421875
train loss:  0.48570793867111206
train gradient:  0.1481635171083817
iteration : 2914
train acc:  0.71875
train loss:  0.531531572341919
train gradient:  0.1569654972478015
iteration : 2915
train acc:  0.671875
train loss:  0.5691747665405273
train gradient:  0.15519339135843022
iteration : 2916
train acc:  0.7109375
train loss:  0.5663790702819824
train gradient:  0.1633050386911602
iteration : 2917
train acc:  0.6953125
train loss:  0.5512661933898926
train gradient:  0.17323217399207735
iteration : 2918
train acc:  0.734375
train loss:  0.4784129858016968
train gradient:  0.15711738825647942
iteration : 2919
train acc:  0.7265625
train loss:  0.5649288892745972
train gradient:  0.19425576646858206
iteration : 2920
train acc:  0.6953125
train loss:  0.5569421052932739
train gradient:  0.18040304503203164
iteration : 2921
train acc:  0.7421875
train loss:  0.4831361472606659
train gradient:  0.13216060662151047
iteration : 2922
train acc:  0.75
train loss:  0.5288165807723999
train gradient:  0.1475049924670993
iteration : 2923
train acc:  0.734375
train loss:  0.512782871723175
train gradient:  0.1447405034336656
iteration : 2924
train acc:  0.75
train loss:  0.5319907665252686
train gradient:  0.17716995586936168
iteration : 2925
train acc:  0.71875
train loss:  0.5392675995826721
train gradient:  0.16626889189831223
iteration : 2926
train acc:  0.7421875
train loss:  0.505131721496582
train gradient:  0.16806509577280043
iteration : 2927
train acc:  0.6953125
train loss:  0.5786836743354797
train gradient:  0.18029209190687517
iteration : 2928
train acc:  0.71875
train loss:  0.5114196538925171
train gradient:  0.15165399121394818
iteration : 2929
train acc:  0.703125
train loss:  0.5282700061798096
train gradient:  0.1766328257056468
iteration : 2930
train acc:  0.7265625
train loss:  0.5299966931343079
train gradient:  0.2128012622872424
iteration : 2931
train acc:  0.6953125
train loss:  0.5501837730407715
train gradient:  0.225152932070527
iteration : 2932
train acc:  0.765625
train loss:  0.4581142067909241
train gradient:  0.14325391577576602
iteration : 2933
train acc:  0.7734375
train loss:  0.47780248522758484
train gradient:  0.1274298163562751
iteration : 2934
train acc:  0.71875
train loss:  0.5351575613021851
train gradient:  0.13353202561920535
iteration : 2935
train acc:  0.7421875
train loss:  0.513635516166687
train gradient:  0.1244964447043544
iteration : 2936
train acc:  0.734375
train loss:  0.5209101438522339
train gradient:  0.20535991501111317
iteration : 2937
train acc:  0.6328125
train loss:  0.6101207733154297
train gradient:  0.17762751458013198
iteration : 2938
train acc:  0.78125
train loss:  0.454181432723999
train gradient:  0.15141500449581258
iteration : 2939
train acc:  0.6640625
train loss:  0.5545929074287415
train gradient:  0.22352020525456665
iteration : 2940
train acc:  0.6484375
train loss:  0.6120145320892334
train gradient:  0.20133673010771022
iteration : 2941
train acc:  0.6953125
train loss:  0.5506189465522766
train gradient:  0.1525138759597891
iteration : 2942
train acc:  0.703125
train loss:  0.5197350382804871
train gradient:  0.14172719366547654
iteration : 2943
train acc:  0.765625
train loss:  0.48963356018066406
train gradient:  0.14968434774528705
iteration : 2944
train acc:  0.703125
train loss:  0.5351911783218384
train gradient:  0.16130657366853368
iteration : 2945
train acc:  0.703125
train loss:  0.5754686594009399
train gradient:  0.15500252659485939
iteration : 2946
train acc:  0.7265625
train loss:  0.5563814640045166
train gradient:  0.18210009355424706
iteration : 2947
train acc:  0.75
train loss:  0.4950651228427887
train gradient:  0.14682996537213405
iteration : 2948
train acc:  0.7109375
train loss:  0.5135045051574707
train gradient:  0.134032180900803
iteration : 2949
train acc:  0.65625
train loss:  0.5667505264282227
train gradient:  0.1604291932131164
iteration : 2950
train acc:  0.703125
train loss:  0.5590754747390747
train gradient:  0.15666498847536356
iteration : 2951
train acc:  0.6640625
train loss:  0.5594006180763245
train gradient:  0.22110746640597306
iteration : 2952
train acc:  0.734375
train loss:  0.5476091504096985
train gradient:  0.16867638044640276
iteration : 2953
train acc:  0.7421875
train loss:  0.5325547456741333
train gradient:  0.15080155660676164
iteration : 2954
train acc:  0.7734375
train loss:  0.4840286374092102
train gradient:  0.13447559024238603
iteration : 2955
train acc:  0.734375
train loss:  0.5870434045791626
train gradient:  0.18449243746951816
iteration : 2956
train acc:  0.7421875
train loss:  0.5026463270187378
train gradient:  0.16768934431599225
iteration : 2957
train acc:  0.765625
train loss:  0.4869025945663452
train gradient:  0.1413277253802298
iteration : 2958
train acc:  0.6640625
train loss:  0.6058902144432068
train gradient:  0.2195298505559615
iteration : 2959
train acc:  0.6328125
train loss:  0.5953586101531982
train gradient:  0.2418783474427874
iteration : 2960
train acc:  0.765625
train loss:  0.5236625671386719
train gradient:  0.17809295360910493
iteration : 2961
train acc:  0.671875
train loss:  0.6137072443962097
train gradient:  0.2540678172163504
iteration : 2962
train acc:  0.71875
train loss:  0.5372397303581238
train gradient:  0.15383122677029815
iteration : 2963
train acc:  0.7421875
train loss:  0.4773350656032562
train gradient:  0.16246122446325284
iteration : 2964
train acc:  0.7421875
train loss:  0.5209363698959351
train gradient:  0.14825769959909588
iteration : 2965
train acc:  0.6328125
train loss:  0.5626908540725708
train gradient:  0.14822799536313042
iteration : 2966
train acc:  0.6171875
train loss:  0.5703199505805969
train gradient:  0.17256462062455874
iteration : 2967
train acc:  0.75
train loss:  0.5336909294128418
train gradient:  0.21314494023424588
iteration : 2968
train acc:  0.6953125
train loss:  0.5384061932563782
train gradient:  0.17127293722155168
iteration : 2969
train acc:  0.75
train loss:  0.520195722579956
train gradient:  0.21650902489056384
iteration : 2970
train acc:  0.7734375
train loss:  0.4923080503940582
train gradient:  0.23631078880031942
iteration : 2971
train acc:  0.7578125
train loss:  0.49234098196029663
train gradient:  0.17353152828122903
iteration : 2972
train acc:  0.6953125
train loss:  0.5271669626235962
train gradient:  0.1455607232860332
iteration : 2973
train acc:  0.6484375
train loss:  0.6238164901733398
train gradient:  0.2217286805917846
iteration : 2974
train acc:  0.7109375
train loss:  0.5511491298675537
train gradient:  0.1596119169295205
iteration : 2975
train acc:  0.7421875
train loss:  0.540276288986206
train gradient:  0.16176391507046145
iteration : 2976
train acc:  0.75
train loss:  0.5019134283065796
train gradient:  0.13582835574056093
iteration : 2977
train acc:  0.7265625
train loss:  0.5212854146957397
train gradient:  0.18432381148010565
iteration : 2978
train acc:  0.6875
train loss:  0.6101775765419006
train gradient:  0.24381727697991734
iteration : 2979
train acc:  0.7578125
train loss:  0.49176713824272156
train gradient:  0.11922879751820083
iteration : 2980
train acc:  0.6953125
train loss:  0.5413033962249756
train gradient:  0.1726151571708589
iteration : 2981
train acc:  0.7578125
train loss:  0.5051888823509216
train gradient:  0.1725317270013294
iteration : 2982
train acc:  0.640625
train loss:  0.6125080585479736
train gradient:  0.19012569548171815
iteration : 2983
train acc:  0.7578125
train loss:  0.4751966595649719
train gradient:  0.12406357441192031
iteration : 2984
train acc:  0.640625
train loss:  0.6037337779998779
train gradient:  0.19740617942382432
iteration : 2985
train acc:  0.71875
train loss:  0.535875141620636
train gradient:  0.15814772813511885
iteration : 2986
train acc:  0.7109375
train loss:  0.5216222405433655
train gradient:  0.14266269457837888
iteration : 2987
train acc:  0.765625
train loss:  0.4582252502441406
train gradient:  0.1401529374205483
iteration : 2988
train acc:  0.71875
train loss:  0.5031399726867676
train gradient:  0.14561229849003288
iteration : 2989
train acc:  0.71875
train loss:  0.5448751449584961
train gradient:  0.16945852609950762
iteration : 2990
train acc:  0.6875
train loss:  0.5404587984085083
train gradient:  0.17839531255333277
iteration : 2991
train acc:  0.65625
train loss:  0.5696336030960083
train gradient:  0.18568622321208733
iteration : 2992
train acc:  0.75
train loss:  0.5200701951980591
train gradient:  0.15163961374108542
iteration : 2993
train acc:  0.75
train loss:  0.4828255772590637
train gradient:  0.1513131276498722
iteration : 2994
train acc:  0.75
train loss:  0.4664463400840759
train gradient:  0.13733888633242747
iteration : 2995
train acc:  0.71875
train loss:  0.553826093673706
train gradient:  0.16453756186013202
iteration : 2996
train acc:  0.6796875
train loss:  0.5620690584182739
train gradient:  0.17800045495512326
iteration : 2997
train acc:  0.78125
train loss:  0.4659993052482605
train gradient:  0.17230815871990632
iteration : 2998
train acc:  0.75
train loss:  0.5247868299484253
train gradient:  0.1667347199894944
iteration : 2999
train acc:  0.6171875
train loss:  0.5594085454940796
train gradient:  0.26406365196063847
iteration : 3000
train acc:  0.65625
train loss:  0.604273796081543
train gradient:  0.20994632659083087
iteration : 3001
train acc:  0.6484375
train loss:  0.6269973516464233
train gradient:  0.15593253749031802
iteration : 3002
train acc:  0.75
train loss:  0.5426310300827026
train gradient:  0.1876788329513096
iteration : 3003
train acc:  0.6328125
train loss:  0.6015942096710205
train gradient:  0.1859713313039244
iteration : 3004
train acc:  0.703125
train loss:  0.591945469379425
train gradient:  0.21659614668685212
iteration : 3005
train acc:  0.6875
train loss:  0.5547775030136108
train gradient:  0.1630916844511896
iteration : 3006
train acc:  0.71875
train loss:  0.5345179438591003
train gradient:  0.24227432803438742
iteration : 3007
train acc:  0.6953125
train loss:  0.5362405776977539
train gradient:  0.17110070814657757
iteration : 3008
train acc:  0.75
train loss:  0.5075335502624512
train gradient:  0.2746407973494493
iteration : 3009
train acc:  0.6953125
train loss:  0.5534852743148804
train gradient:  0.16234137126486645
iteration : 3010
train acc:  0.7265625
train loss:  0.5042955875396729
train gradient:  0.19148052376800073
iteration : 3011
train acc:  0.703125
train loss:  0.5291906595230103
train gradient:  0.16574623804356214
iteration : 3012
train acc:  0.65625
train loss:  0.5746471881866455
train gradient:  0.1480087104486439
iteration : 3013
train acc:  0.7421875
train loss:  0.5127968192100525
train gradient:  0.15587732885422173
iteration : 3014
train acc:  0.734375
train loss:  0.5299060344696045
train gradient:  0.18410104707002045
iteration : 3015
train acc:  0.71875
train loss:  0.5073369741439819
train gradient:  0.19284357862388735
iteration : 3016
train acc:  0.6875
train loss:  0.6200843453407288
train gradient:  0.22696516000365313
iteration : 3017
train acc:  0.7265625
train loss:  0.5206875801086426
train gradient:  0.15505849270188876
iteration : 3018
train acc:  0.75
train loss:  0.5072535872459412
train gradient:  0.11602319215898235
iteration : 3019
train acc:  0.71875
train loss:  0.5657486915588379
train gradient:  0.15981333157233857
iteration : 3020
train acc:  0.6953125
train loss:  0.5267674326896667
train gradient:  0.2134600506797288
iteration : 3021
train acc:  0.7734375
train loss:  0.48907193541526794
train gradient:  0.15252592747094015
iteration : 3022
train acc:  0.71875
train loss:  0.5019067525863647
train gradient:  0.18188912664348045
iteration : 3023
train acc:  0.7109375
train loss:  0.5620043277740479
train gradient:  0.16549774715577764
iteration : 3024
train acc:  0.6953125
train loss:  0.5348470211029053
train gradient:  0.17762325017864664
iteration : 3025
train acc:  0.75
train loss:  0.4712238609790802
train gradient:  0.13053520568141674
iteration : 3026
train acc:  0.703125
train loss:  0.5583752393722534
train gradient:  0.24436640481575939
iteration : 3027
train acc:  0.703125
train loss:  0.5454835891723633
train gradient:  0.14713078304743915
iteration : 3028
train acc:  0.6484375
train loss:  0.5767303705215454
train gradient:  0.19485094636316228
iteration : 3029
train acc:  0.7265625
train loss:  0.5568249225616455
train gradient:  0.19345759948323288
iteration : 3030
train acc:  0.7578125
train loss:  0.4907500743865967
train gradient:  0.11497423751991952
iteration : 3031
train acc:  0.765625
train loss:  0.4941292703151703
train gradient:  0.14234282557963077
iteration : 3032
train acc:  0.6875
train loss:  0.5286695957183838
train gradient:  0.1174160035208942
iteration : 3033
train acc:  0.7265625
train loss:  0.5828871726989746
train gradient:  0.18623674646231697
iteration : 3034
train acc:  0.7578125
train loss:  0.494689404964447
train gradient:  0.11232899638207473
iteration : 3035
train acc:  0.7734375
train loss:  0.5050070881843567
train gradient:  0.15755674682044946
iteration : 3036
train acc:  0.703125
train loss:  0.5395642518997192
train gradient:  0.1530808320671831
iteration : 3037
train acc:  0.734375
train loss:  0.4904734790325165
train gradient:  0.17727288780644834
iteration : 3038
train acc:  0.703125
train loss:  0.5434156060218811
train gradient:  0.15611266004864768
iteration : 3039
train acc:  0.7890625
train loss:  0.4803515076637268
train gradient:  0.15219139233501078
iteration : 3040
train acc:  0.71875
train loss:  0.5501161813735962
train gradient:  0.15925934915662646
iteration : 3041
train acc:  0.703125
train loss:  0.5357630848884583
train gradient:  0.1767921889297715
iteration : 3042
train acc:  0.7265625
train loss:  0.5336722731590271
train gradient:  0.1973226661623672
iteration : 3043
train acc:  0.7265625
train loss:  0.5147606134414673
train gradient:  0.13967619716482046
iteration : 3044
train acc:  0.7421875
train loss:  0.5081076622009277
train gradient:  0.15743783935764818
iteration : 3045
train acc:  0.703125
train loss:  0.540856122970581
train gradient:  0.13051056923535403
iteration : 3046
train acc:  0.6484375
train loss:  0.5976592302322388
train gradient:  0.19961244291967495
iteration : 3047
train acc:  0.75
train loss:  0.5286785364151001
train gradient:  0.15635370038140234
iteration : 3048
train acc:  0.71875
train loss:  0.5504248738288879
train gradient:  0.19774157681362425
iteration : 3049
train acc:  0.7734375
train loss:  0.45066922903060913
train gradient:  0.16820584717309706
iteration : 3050
train acc:  0.6640625
train loss:  0.563750684261322
train gradient:  0.1666667335196938
iteration : 3051
train acc:  0.703125
train loss:  0.5540671348571777
train gradient:  0.18362276112407494
iteration : 3052
train acc:  0.7109375
train loss:  0.5176524519920349
train gradient:  0.17124636381768488
iteration : 3053
train acc:  0.6328125
train loss:  0.6663320064544678
train gradient:  0.3419174825794785
iteration : 3054
train acc:  0.734375
train loss:  0.5419117212295532
train gradient:  0.2915710359387892
iteration : 3055
train acc:  0.671875
train loss:  0.6021206974983215
train gradient:  0.2400254798076819
iteration : 3056
train acc:  0.765625
train loss:  0.4751129150390625
train gradient:  0.1264120529269644
iteration : 3057
train acc:  0.6796875
train loss:  0.6029843688011169
train gradient:  0.20562051165916717
iteration : 3058
train acc:  0.7109375
train loss:  0.5441866517066956
train gradient:  0.18201679282882763
iteration : 3059
train acc:  0.765625
train loss:  0.5435059070587158
train gradient:  0.16396400625696791
iteration : 3060
train acc:  0.6640625
train loss:  0.5751346349716187
train gradient:  0.19876233638002966
iteration : 3061
train acc:  0.7578125
train loss:  0.5066104531288147
train gradient:  0.10971489414134743
iteration : 3062
train acc:  0.7578125
train loss:  0.46794426441192627
train gradient:  0.159221576662684
iteration : 3063
train acc:  0.6796875
train loss:  0.5909324884414673
train gradient:  0.17042702377534252
iteration : 3064
train acc:  0.7265625
train loss:  0.5217319130897522
train gradient:  0.17242127160465792
iteration : 3065
train acc:  0.7265625
train loss:  0.5578315258026123
train gradient:  0.17028023850217144
iteration : 3066
train acc:  0.734375
train loss:  0.4849957823753357
train gradient:  0.14213123896126606
iteration : 3067
train acc:  0.7421875
train loss:  0.49862271547317505
train gradient:  0.1251130926713302
iteration : 3068
train acc:  0.625
train loss:  0.6213036179542542
train gradient:  0.1937999852960356
iteration : 3069
train acc:  0.6875
train loss:  0.5525549054145813
train gradient:  0.16939299353099785
iteration : 3070
train acc:  0.78125
train loss:  0.4675052762031555
train gradient:  0.1306624783906284
iteration : 3071
train acc:  0.7578125
train loss:  0.5502107739448547
train gradient:  0.21566738538219138
iteration : 3072
train acc:  0.7421875
train loss:  0.5151952505111694
train gradient:  0.17377540127708796
iteration : 3073
train acc:  0.765625
train loss:  0.5049515962600708
train gradient:  0.16249578455707497
iteration : 3074
train acc:  0.71875
train loss:  0.5586932897567749
train gradient:  0.15995908414183588
iteration : 3075
train acc:  0.6953125
train loss:  0.5534164309501648
train gradient:  0.13429302034692742
iteration : 3076
train acc:  0.71875
train loss:  0.5727087259292603
train gradient:  0.15470594798658366
iteration : 3077
train acc:  0.6875
train loss:  0.5879515409469604
train gradient:  0.21855484033342204
iteration : 3078
train acc:  0.7421875
train loss:  0.5013453960418701
train gradient:  0.13105948000219533
iteration : 3079
train acc:  0.703125
train loss:  0.5641698837280273
train gradient:  0.20569354374993692
iteration : 3080
train acc:  0.703125
train loss:  0.5565476417541504
train gradient:  0.21306083155017405
iteration : 3081
train acc:  0.7265625
train loss:  0.5073410272598267
train gradient:  0.19619089688177488
iteration : 3082
train acc:  0.6953125
train loss:  0.552273154258728
train gradient:  0.1721474978837868
iteration : 3083
train acc:  0.7734375
train loss:  0.5067688226699829
train gradient:  0.15617514026361803
iteration : 3084
train acc:  0.6796875
train loss:  0.5114643573760986
train gradient:  0.1598267525424515
iteration : 3085
train acc:  0.734375
train loss:  0.5242139101028442
train gradient:  0.2212812576432836
iteration : 3086
train acc:  0.65625
train loss:  0.5935375690460205
train gradient:  0.21709655280800721
iteration : 3087
train acc:  0.6953125
train loss:  0.5392109751701355
train gradient:  0.1637938171780312
iteration : 3088
train acc:  0.71875
train loss:  0.5653445720672607
train gradient:  0.19658915490737544
iteration : 3089
train acc:  0.75
train loss:  0.5429027080535889
train gradient:  0.14296298153714915
iteration : 3090
train acc:  0.6875
train loss:  0.5341172218322754
train gradient:  0.1650166180086886
iteration : 3091
train acc:  0.71875
train loss:  0.4836554527282715
train gradient:  0.1398140378800932
iteration : 3092
train acc:  0.75
train loss:  0.5369434356689453
train gradient:  0.1556015610219727
iteration : 3093
train acc:  0.703125
train loss:  0.5377721190452576
train gradient:  0.15101473165974882
iteration : 3094
train acc:  0.71875
train loss:  0.5248812437057495
train gradient:  0.2349570348429591
iteration : 3095
train acc:  0.6328125
train loss:  0.5866912007331848
train gradient:  0.17777175593184952
iteration : 3096
train acc:  0.7578125
train loss:  0.46323075890541077
train gradient:  0.1157533276672231
iteration : 3097
train acc:  0.7265625
train loss:  0.5151985883712769
train gradient:  0.12644766550857414
iteration : 3098
train acc:  0.6875
train loss:  0.6013182401657104
train gradient:  0.19219120905811143
iteration : 3099
train acc:  0.71875
train loss:  0.5394617319107056
train gradient:  0.165510518353566
iteration : 3100
train acc:  0.765625
train loss:  0.46251413226127625
train gradient:  0.13584011186694356
iteration : 3101
train acc:  0.71875
train loss:  0.5680935382843018
train gradient:  0.17516614086483848
iteration : 3102
train acc:  0.734375
train loss:  0.5345238447189331
train gradient:  0.1580849280515013
iteration : 3103
train acc:  0.75
train loss:  0.47109436988830566
train gradient:  0.1769155159816585
iteration : 3104
train acc:  0.6953125
train loss:  0.5693379640579224
train gradient:  0.20252979265143134
iteration : 3105
train acc:  0.71875
train loss:  0.5383520126342773
train gradient:  0.1752887611407649
iteration : 3106
train acc:  0.6953125
train loss:  0.5577133893966675
train gradient:  0.19442575361004655
iteration : 3107
train acc:  0.703125
train loss:  0.5536903142929077
train gradient:  0.17597205383857953
iteration : 3108
train acc:  0.765625
train loss:  0.4832042455673218
train gradient:  0.14099261267089816
iteration : 3109
train acc:  0.7265625
train loss:  0.5476136207580566
train gradient:  0.17182037841375003
iteration : 3110
train acc:  0.7734375
train loss:  0.49407100677490234
train gradient:  0.16911273134653343
iteration : 3111
train acc:  0.75
train loss:  0.5488598346710205
train gradient:  0.21395284335494946
iteration : 3112
train acc:  0.6875
train loss:  0.5843292474746704
train gradient:  0.2451319529655766
iteration : 3113
train acc:  0.703125
train loss:  0.5113979578018188
train gradient:  0.18136002969946557
iteration : 3114
train acc:  0.6875
train loss:  0.5353630781173706
train gradient:  0.16750611573683577
iteration : 3115
train acc:  0.734375
train loss:  0.5513914823532104
train gradient:  0.1617832000350833
iteration : 3116
train acc:  0.7421875
train loss:  0.5003806352615356
train gradient:  0.14743863325621082
iteration : 3117
train acc:  0.6953125
train loss:  0.536773681640625
train gradient:  0.16929182901494655
iteration : 3118
train acc:  0.78125
train loss:  0.4626767933368683
train gradient:  0.1208338193381042
iteration : 3119
train acc:  0.71875
train loss:  0.5538119673728943
train gradient:  0.15733714882952532
iteration : 3120
train acc:  0.7421875
train loss:  0.5374940633773804
train gradient:  0.17939786743183425
iteration : 3121
train acc:  0.765625
train loss:  0.4597708582878113
train gradient:  0.11764027144114755
iteration : 3122
train acc:  0.703125
train loss:  0.5240417718887329
train gradient:  0.163084644559166
iteration : 3123
train acc:  0.75
train loss:  0.5604598522186279
train gradient:  0.22897732812473412
iteration : 3124
train acc:  0.6171875
train loss:  0.6419775485992432
train gradient:  0.21586276836504226
iteration : 3125
train acc:  0.765625
train loss:  0.4771193861961365
train gradient:  0.15336213218912936
iteration : 3126
train acc:  0.6875
train loss:  0.551975429058075
train gradient:  0.15059887476522166
iteration : 3127
train acc:  0.671875
train loss:  0.5615384578704834
train gradient:  0.16417326616531053
iteration : 3128
train acc:  0.6640625
train loss:  0.6053789854049683
train gradient:  0.22560493806458315
iteration : 3129
train acc:  0.75
train loss:  0.4918287396430969
train gradient:  0.1696610075356733
iteration : 3130
train acc:  0.78125
train loss:  0.5119432210922241
train gradient:  0.17239920832417066
iteration : 3131
train acc:  0.7109375
train loss:  0.5401554107666016
train gradient:  0.1534255503204576
iteration : 3132
train acc:  0.7421875
train loss:  0.5520906448364258
train gradient:  0.17838625194596644
iteration : 3133
train acc:  0.7421875
train loss:  0.5243839621543884
train gradient:  0.2061684047231533
iteration : 3134
train acc:  0.7109375
train loss:  0.5033136606216431
train gradient:  0.13427184018485788
iteration : 3135
train acc:  0.6875
train loss:  0.5549771785736084
train gradient:  0.18131745261121762
iteration : 3136
train acc:  0.65625
train loss:  0.5954898595809937
train gradient:  0.22721880554547655
iteration : 3137
train acc:  0.7109375
train loss:  0.49728021025657654
train gradient:  0.14821039242517978
iteration : 3138
train acc:  0.6015625
train loss:  0.6370680928230286
train gradient:  0.26000231774038995
iteration : 3139
train acc:  0.703125
train loss:  0.5843280553817749
train gradient:  0.22003760933534783
iteration : 3140
train acc:  0.703125
train loss:  0.5402493476867676
train gradient:  0.2119545814236926
iteration : 3141
train acc:  0.6796875
train loss:  0.5834927558898926
train gradient:  0.22302263209200518
iteration : 3142
train acc:  0.7734375
train loss:  0.49871885776519775
train gradient:  0.1588668581166059
iteration : 3143
train acc:  0.703125
train loss:  0.5621908903121948
train gradient:  0.18974004492137986
iteration : 3144
train acc:  0.6796875
train loss:  0.5747685432434082
train gradient:  0.20710966919873502
iteration : 3145
train acc:  0.6796875
train loss:  0.5728040933609009
train gradient:  0.20098958860408295
iteration : 3146
train acc:  0.6796875
train loss:  0.538463294506073
train gradient:  0.1592621642551371
iteration : 3147
train acc:  0.7734375
train loss:  0.5033847093582153
train gradient:  0.12223809271043981
iteration : 3148
train acc:  0.6875
train loss:  0.592843770980835
train gradient:  0.1547810491321866
iteration : 3149
train acc:  0.7421875
train loss:  0.4701334834098816
train gradient:  0.13674782123174573
iteration : 3150
train acc:  0.734375
train loss:  0.5228656530380249
train gradient:  0.15612834306727358
iteration : 3151
train acc:  0.7578125
train loss:  0.493946373462677
train gradient:  0.1770652202022228
iteration : 3152
train acc:  0.59375
train loss:  0.640069305896759
train gradient:  0.26091075418460813
iteration : 3153
train acc:  0.703125
train loss:  0.5673773884773254
train gradient:  0.1559169932037495
iteration : 3154
train acc:  0.6796875
train loss:  0.6039190292358398
train gradient:  0.253748508520126
iteration : 3155
train acc:  0.765625
train loss:  0.49111902713775635
train gradient:  0.14869958059770993
iteration : 3156
train acc:  0.6953125
train loss:  0.558243989944458
train gradient:  0.1743182346935786
iteration : 3157
train acc:  0.609375
train loss:  0.6195577383041382
train gradient:  0.3130456248555274
iteration : 3158
train acc:  0.7265625
train loss:  0.5407224297523499
train gradient:  0.14538334019176966
iteration : 3159
train acc:  0.6953125
train loss:  0.5254201889038086
train gradient:  0.1537806626208285
iteration : 3160
train acc:  0.734375
train loss:  0.5254063606262207
train gradient:  0.14973555042901127
iteration : 3161
train acc:  0.71875
train loss:  0.567516565322876
train gradient:  0.1680205521398121
iteration : 3162
train acc:  0.78125
train loss:  0.5087084770202637
train gradient:  0.17339175482749605
iteration : 3163
train acc:  0.6875
train loss:  0.5567605495452881
train gradient:  0.15676011692537495
iteration : 3164
train acc:  0.7421875
train loss:  0.5128182172775269
train gradient:  0.14787049944218653
iteration : 3165
train acc:  0.6640625
train loss:  0.5774185061454773
train gradient:  0.16964115517970937
iteration : 3166
train acc:  0.7578125
train loss:  0.503877580165863
train gradient:  0.16824086035545505
iteration : 3167
train acc:  0.703125
train loss:  0.5696482062339783
train gradient:  0.17712337677020973
iteration : 3168
train acc:  0.734375
train loss:  0.5122252702713013
train gradient:  0.17548401857632406
iteration : 3169
train acc:  0.625
train loss:  0.6191911697387695
train gradient:  0.18612345028242888
iteration : 3170
train acc:  0.6796875
train loss:  0.5401386022567749
train gradient:  0.18574947845168893
iteration : 3171
train acc:  0.65625
train loss:  0.5760458111763
train gradient:  0.142387366864821
iteration : 3172
train acc:  0.734375
train loss:  0.527901828289032
train gradient:  0.16954521029014955
iteration : 3173
train acc:  0.6796875
train loss:  0.5297839641571045
train gradient:  0.1848998225861559
iteration : 3174
train acc:  0.7109375
train loss:  0.5460202097892761
train gradient:  0.16411623184582574
iteration : 3175
train acc:  0.703125
train loss:  0.592605471611023
train gradient:  0.2314999680009237
iteration : 3176
train acc:  0.734375
train loss:  0.5257609486579895
train gradient:  0.13457708753506378
iteration : 3177
train acc:  0.75
train loss:  0.4956410527229309
train gradient:  0.15423231111583702
iteration : 3178
train acc:  0.6171875
train loss:  0.592360258102417
train gradient:  0.23445488135145537
iteration : 3179
train acc:  0.7265625
train loss:  0.5030490159988403
train gradient:  0.16367167865326315
iteration : 3180
train acc:  0.7578125
train loss:  0.5104395151138306
train gradient:  0.1399129739428122
iteration : 3181
train acc:  0.765625
train loss:  0.4898552894592285
train gradient:  0.14560757546449077
iteration : 3182
train acc:  0.6953125
train loss:  0.5925723910331726
train gradient:  0.14164862043459336
iteration : 3183
train acc:  0.796875
train loss:  0.4546108841896057
train gradient:  0.12143533305635826
iteration : 3184
train acc:  0.7578125
train loss:  0.5416396260261536
train gradient:  0.1685710605325052
iteration : 3185
train acc:  0.7265625
train loss:  0.48975521326065063
train gradient:  0.10658709338246418
iteration : 3186
train acc:  0.7265625
train loss:  0.5580675601959229
train gradient:  0.1995113479658266
iteration : 3187
train acc:  0.7109375
train loss:  0.5067540407180786
train gradient:  0.13988786226928648
iteration : 3188
train acc:  0.7578125
train loss:  0.46350833773612976
train gradient:  0.127922430214056
iteration : 3189
train acc:  0.6875
train loss:  0.5690900683403015
train gradient:  0.1793616206284737
iteration : 3190
train acc:  0.7265625
train loss:  0.5643473863601685
train gradient:  0.19304843993383883
iteration : 3191
train acc:  0.7265625
train loss:  0.49520808458328247
train gradient:  0.13130708568889693
iteration : 3192
train acc:  0.7421875
train loss:  0.5182681083679199
train gradient:  0.1678366607332878
iteration : 3193
train acc:  0.71875
train loss:  0.5107607245445251
train gradient:  0.17072933537217294
iteration : 3194
train acc:  0.6875
train loss:  0.5634485483169556
train gradient:  0.17092641461191713
iteration : 3195
train acc:  0.6796875
train loss:  0.5435324311256409
train gradient:  0.16946113022617093
iteration : 3196
train acc:  0.78125
train loss:  0.4644840359687805
train gradient:  0.1712096098518604
iteration : 3197
train acc:  0.703125
train loss:  0.5418806672096252
train gradient:  0.2017840108954272
iteration : 3198
train acc:  0.7265625
train loss:  0.5083571672439575
train gradient:  0.18731398468355842
iteration : 3199
train acc:  0.7578125
train loss:  0.4618871808052063
train gradient:  0.1262713064216807
iteration : 3200
train acc:  0.640625
train loss:  0.5788544416427612
train gradient:  0.1771307398753387
iteration : 3201
train acc:  0.71875
train loss:  0.49826306104660034
train gradient:  0.11077378141751705
iteration : 3202
train acc:  0.7265625
train loss:  0.5301437973976135
train gradient:  0.14920928103598102
iteration : 3203
train acc:  0.71875
train loss:  0.5092667937278748
train gradient:  0.1450635941680719
iteration : 3204
train acc:  0.625
train loss:  0.6055469512939453
train gradient:  0.22711568285468225
iteration : 3205
train acc:  0.7109375
train loss:  0.5048882365226746
train gradient:  0.1689610267167424
iteration : 3206
train acc:  0.671875
train loss:  0.5772311091423035
train gradient:  0.1863023593944537
iteration : 3207
train acc:  0.75
train loss:  0.4655359387397766
train gradient:  0.13429974858999344
iteration : 3208
train acc:  0.7578125
train loss:  0.5220280885696411
train gradient:  0.12171686606336204
iteration : 3209
train acc:  0.71875
train loss:  0.4978441894054413
train gradient:  0.1567173817500091
iteration : 3210
train acc:  0.71875
train loss:  0.5564134120941162
train gradient:  0.1666818514254974
iteration : 3211
train acc:  0.71875
train loss:  0.5487287044525146
train gradient:  0.15991993084337025
iteration : 3212
train acc:  0.796875
train loss:  0.5002249479293823
train gradient:  0.21163907667224108
iteration : 3213
train acc:  0.7265625
train loss:  0.5563921928405762
train gradient:  0.1686168792651009
iteration : 3214
train acc:  0.7578125
train loss:  0.5408320426940918
train gradient:  0.20102627163910203
iteration : 3215
train acc:  0.703125
train loss:  0.5524212121963501
train gradient:  0.19493975877902897
iteration : 3216
train acc:  0.6640625
train loss:  0.5706668496131897
train gradient:  0.2332136863394106
iteration : 3217
train acc:  0.7734375
train loss:  0.48457586765289307
train gradient:  0.13362094593201945
iteration : 3218
train acc:  0.75
train loss:  0.5185163617134094
train gradient:  0.1674311096407469
iteration : 3219
train acc:  0.8125
train loss:  0.4583093523979187
train gradient:  0.15799598101789633
iteration : 3220
train acc:  0.6875
train loss:  0.5442402362823486
train gradient:  0.18982413913244228
iteration : 3221
train acc:  0.7109375
train loss:  0.5139257311820984
train gradient:  0.1901105291609711
iteration : 3222
train acc:  0.65625
train loss:  0.5914599299430847
train gradient:  0.18622433691296264
iteration : 3223
train acc:  0.765625
train loss:  0.4604226350784302
train gradient:  0.18703889401923354
iteration : 3224
train acc:  0.7265625
train loss:  0.5272829532623291
train gradient:  0.14636152643815148
iteration : 3225
train acc:  0.703125
train loss:  0.5318626761436462
train gradient:  0.1521958851126633
iteration : 3226
train acc:  0.71875
train loss:  0.5410106182098389
train gradient:  0.17622786830879728
iteration : 3227
train acc:  0.75
train loss:  0.508308470249176
train gradient:  0.13138748696638036
iteration : 3228
train acc:  0.6796875
train loss:  0.5396847128868103
train gradient:  0.1898994068724425
iteration : 3229
train acc:  0.6640625
train loss:  0.6477663516998291
train gradient:  0.21330255181652458
iteration : 3230
train acc:  0.6796875
train loss:  0.5877063274383545
train gradient:  0.20618739753773518
iteration : 3231
train acc:  0.7734375
train loss:  0.46059712767601013
train gradient:  0.11799754169963654
iteration : 3232
train acc:  0.75
train loss:  0.5269651412963867
train gradient:  0.16109050822935642
iteration : 3233
train acc:  0.734375
train loss:  0.5232428312301636
train gradient:  0.18123413719509676
iteration : 3234
train acc:  0.765625
train loss:  0.4931170344352722
train gradient:  0.1042833132885742
iteration : 3235
train acc:  0.703125
train loss:  0.5202916860580444
train gradient:  0.15453794173992952
iteration : 3236
train acc:  0.671875
train loss:  0.5858602523803711
train gradient:  0.1539842576125641
iteration : 3237
train acc:  0.65625
train loss:  0.6140013933181763
train gradient:  0.16454373836126848
iteration : 3238
train acc:  0.734375
train loss:  0.5449304580688477
train gradient:  0.13908458763030931
iteration : 3239
train acc:  0.6796875
train loss:  0.5435978770256042
train gradient:  0.15648398886893466
iteration : 3240
train acc:  0.6875
train loss:  0.5479782819747925
train gradient:  0.1663064882200091
iteration : 3241
train acc:  0.703125
train loss:  0.5146419405937195
train gradient:  0.15974346315623233
iteration : 3242
train acc:  0.7421875
train loss:  0.5133678317070007
train gradient:  0.1790706450165683
iteration : 3243
train acc:  0.6875
train loss:  0.5183672904968262
train gradient:  0.20182445424643258
iteration : 3244
train acc:  0.671875
train loss:  0.5330324769020081
train gradient:  0.17380265568927875
iteration : 3245
train acc:  0.765625
train loss:  0.5031884908676147
train gradient:  0.1947796007645155
iteration : 3246
train acc:  0.7421875
train loss:  0.49332213401794434
train gradient:  0.15946398073717938
iteration : 3247
train acc:  0.71875
train loss:  0.5443628430366516
train gradient:  0.1291197346430028
iteration : 3248
train acc:  0.6796875
train loss:  0.5593187808990479
train gradient:  0.1811606309352551
iteration : 3249
train acc:  0.7265625
train loss:  0.5330377817153931
train gradient:  0.15501686810653398
iteration : 3250
train acc:  0.765625
train loss:  0.4857512414455414
train gradient:  0.11908155440982854
iteration : 3251
train acc:  0.828125
train loss:  0.4427434504032135
train gradient:  0.10268831825307127
iteration : 3252
train acc:  0.7578125
train loss:  0.5072311758995056
train gradient:  0.13231599330111038
iteration : 3253
train acc:  0.671875
train loss:  0.5432335138320923
train gradient:  0.1721143672953007
iteration : 3254
train acc:  0.6796875
train loss:  0.5564846992492676
train gradient:  0.17462803317260578
iteration : 3255
train acc:  0.6875
train loss:  0.5599274635314941
train gradient:  0.17435112510586814
iteration : 3256
train acc:  0.6171875
train loss:  0.6042990684509277
train gradient:  0.19971769368196668
iteration : 3257
train acc:  0.734375
train loss:  0.5343106985092163
train gradient:  0.18070762048806502
iteration : 3258
train acc:  0.703125
train loss:  0.5648897886276245
train gradient:  0.2042306457775081
iteration : 3259
train acc:  0.7578125
train loss:  0.47147899866104126
train gradient:  0.12188931047545704
iteration : 3260
train acc:  0.6328125
train loss:  0.6241545677185059
train gradient:  0.22511345213770223
iteration : 3261
train acc:  0.7421875
train loss:  0.5469109416007996
train gradient:  0.13716564101112383
iteration : 3262
train acc:  0.7578125
train loss:  0.5079925060272217
train gradient:  0.15281445173320968
iteration : 3263
train acc:  0.6953125
train loss:  0.5333362221717834
train gradient:  0.14109808980940136
iteration : 3264
train acc:  0.8046875
train loss:  0.46151602268218994
train gradient:  0.1394512525935092
iteration : 3265
train acc:  0.7421875
train loss:  0.5620381832122803
train gradient:  0.15045397349353573
iteration : 3266
train acc:  0.7421875
train loss:  0.5034365057945251
train gradient:  0.16232781460577655
iteration : 3267
train acc:  0.734375
train loss:  0.5221329927444458
train gradient:  0.1664806462057164
iteration : 3268
train acc:  0.734375
train loss:  0.6138888001441956
train gradient:  0.2994155330543134
iteration : 3269
train acc:  0.7890625
train loss:  0.4661903977394104
train gradient:  0.15326617423239047
iteration : 3270
train acc:  0.7578125
train loss:  0.4994570016860962
train gradient:  0.2382193367983604
iteration : 3271
train acc:  0.6953125
train loss:  0.5188947916030884
train gradient:  0.1570011159699955
iteration : 3272
train acc:  0.640625
train loss:  0.5833985805511475
train gradient:  0.194437974634045
iteration : 3273
train acc:  0.7109375
train loss:  0.5332922339439392
train gradient:  0.144916514264475
iteration : 3274
train acc:  0.65625
train loss:  0.5690511465072632
train gradient:  0.26111040428536897
iteration : 3275
train acc:  0.7421875
train loss:  0.5047873258590698
train gradient:  0.12309856197437807
iteration : 3276
train acc:  0.6796875
train loss:  0.604273796081543
train gradient:  0.17736925003393078
iteration : 3277
train acc:  0.703125
train loss:  0.5282683968544006
train gradient:  0.13819830753288442
iteration : 3278
train acc:  0.75
train loss:  0.5322072505950928
train gradient:  0.15451611466399878
iteration : 3279
train acc:  0.7109375
train loss:  0.4972659945487976
train gradient:  0.13862960561806054
iteration : 3280
train acc:  0.7421875
train loss:  0.5165385007858276
train gradient:  0.16402415578653706
iteration : 3281
train acc:  0.7109375
train loss:  0.546863317489624
train gradient:  0.15366787054875491
iteration : 3282
train acc:  0.7265625
train loss:  0.5158479809761047
train gradient:  0.13574429833417295
iteration : 3283
train acc:  0.6171875
train loss:  0.5847808122634888
train gradient:  0.17785247400663234
iteration : 3284
train acc:  0.6953125
train loss:  0.5427907705307007
train gradient:  0.24544294507062586
iteration : 3285
train acc:  0.7265625
train loss:  0.5001948475837708
train gradient:  0.14534348148189874
iteration : 3286
train acc:  0.75
train loss:  0.5425103902816772
train gradient:  0.15206544757896823
iteration : 3287
train acc:  0.7578125
train loss:  0.539193868637085
train gradient:  0.13517597381821983
iteration : 3288
train acc:  0.7734375
train loss:  0.5036643743515015
train gradient:  0.12578132585562163
iteration : 3289
train acc:  0.7109375
train loss:  0.529687225818634
train gradient:  0.19885310513389673
iteration : 3290
train acc:  0.640625
train loss:  0.6001362800598145
train gradient:  0.21980975269856112
iteration : 3291
train acc:  0.6875
train loss:  0.5406540632247925
train gradient:  0.14671085229315603
iteration : 3292
train acc:  0.7265625
train loss:  0.5235305428504944
train gradient:  0.13266547765898276
iteration : 3293
train acc:  0.6875
train loss:  0.5878509283065796
train gradient:  0.17248550199759263
iteration : 3294
train acc:  0.671875
train loss:  0.5952733755111694
train gradient:  0.2179274093394712
iteration : 3295
train acc:  0.7578125
train loss:  0.5137008428573608
train gradient:  0.15223965362345748
iteration : 3296
train acc:  0.703125
train loss:  0.5695332884788513
train gradient:  0.15082391412637394
iteration : 3297
train acc:  0.7109375
train loss:  0.5200060606002808
train gradient:  0.166682714765372
iteration : 3298
train acc:  0.734375
train loss:  0.517853856086731
train gradient:  0.13012162616487494
iteration : 3299
train acc:  0.765625
train loss:  0.5105310678482056
train gradient:  0.1597733639087709
iteration : 3300
train acc:  0.7734375
train loss:  0.46666407585144043
train gradient:  0.18283470809616859
iteration : 3301
train acc:  0.7421875
train loss:  0.5170080065727234
train gradient:  0.14878929645424255
iteration : 3302
train acc:  0.6796875
train loss:  0.5601242780685425
train gradient:  0.22293679751045192
iteration : 3303
train acc:  0.6640625
train loss:  0.5693566203117371
train gradient:  0.15915725097235628
iteration : 3304
train acc:  0.65625
train loss:  0.5709269046783447
train gradient:  0.19380862288178038
iteration : 3305
train acc:  0.734375
train loss:  0.47833168506622314
train gradient:  0.1328726898890754
iteration : 3306
train acc:  0.6875
train loss:  0.5601048469543457
train gradient:  0.20305574221258216
iteration : 3307
train acc:  0.7421875
train loss:  0.543080747127533
train gradient:  0.18261225212113308
iteration : 3308
train acc:  0.703125
train loss:  0.5408095121383667
train gradient:  0.15286961805209556
iteration : 3309
train acc:  0.65625
train loss:  0.5621094703674316
train gradient:  0.17652244778507123
iteration : 3310
train acc:  0.6796875
train loss:  0.5182898044586182
train gradient:  0.1630959298344723
iteration : 3311
train acc:  0.6796875
train loss:  0.5466399788856506
train gradient:  0.20811729479265345
iteration : 3312
train acc:  0.734375
train loss:  0.5017035007476807
train gradient:  0.14200621488055357
iteration : 3313
train acc:  0.734375
train loss:  0.52484130859375
train gradient:  0.20271856619078132
iteration : 3314
train acc:  0.765625
train loss:  0.5018510222434998
train gradient:  0.14423811514146276
iteration : 3315
train acc:  0.65625
train loss:  0.555821418762207
train gradient:  0.17918022150720903
iteration : 3316
train acc:  0.65625
train loss:  0.6124195456504822
train gradient:  0.272346100018766
iteration : 3317
train acc:  0.703125
train loss:  0.569464385509491
train gradient:  0.1474014430279177
iteration : 3318
train acc:  0.765625
train loss:  0.48520129919052124
train gradient:  0.13953871947708843
iteration : 3319
train acc:  0.7421875
train loss:  0.4978843629360199
train gradient:  0.15174894277627865
iteration : 3320
train acc:  0.671875
train loss:  0.5845339298248291
train gradient:  0.19243947335449121
iteration : 3321
train acc:  0.75
train loss:  0.5052911639213562
train gradient:  0.13767700415914502
iteration : 3322
train acc:  0.71875
train loss:  0.5154688358306885
train gradient:  0.15870346214986947
iteration : 3323
train acc:  0.734375
train loss:  0.51207035779953
train gradient:  0.16535988161753595
iteration : 3324
train acc:  0.7265625
train loss:  0.5258117318153381
train gradient:  0.16730636507479713
iteration : 3325
train acc:  0.7890625
train loss:  0.48027992248535156
train gradient:  0.18029036299625933
iteration : 3326
train acc:  0.7109375
train loss:  0.5598456859588623
train gradient:  0.22542789539914584
iteration : 3327
train acc:  0.6796875
train loss:  0.5735507607460022
train gradient:  0.15364042029820107
iteration : 3328
train acc:  0.7578125
train loss:  0.4800584316253662
train gradient:  0.132222472860214
iteration : 3329
train acc:  0.734375
train loss:  0.5201314687728882
train gradient:  0.1414005443836202
iteration : 3330
train acc:  0.8046875
train loss:  0.4693346619606018
train gradient:  0.13249709392738784
iteration : 3331
train acc:  0.7265625
train loss:  0.504461407661438
train gradient:  0.12531685427951636
iteration : 3332
train acc:  0.7890625
train loss:  0.4891287684440613
train gradient:  0.163659024223801
iteration : 3333
train acc:  0.6953125
train loss:  0.5357077121734619
train gradient:  0.1418465283100306
iteration : 3334
train acc:  0.71875
train loss:  0.5281836986541748
train gradient:  0.12880384379756954
iteration : 3335
train acc:  0.6875
train loss:  0.5506554245948792
train gradient:  0.21255953753412937
iteration : 3336
train acc:  0.6796875
train loss:  0.5511817932128906
train gradient:  0.1854164971823258
iteration : 3337
train acc:  0.8046875
train loss:  0.4787099361419678
train gradient:  0.12016924316987179
iteration : 3338
train acc:  0.75
train loss:  0.5088575482368469
train gradient:  0.15532854934928259
iteration : 3339
train acc:  0.6640625
train loss:  0.5736773610115051
train gradient:  0.21828682700180696
iteration : 3340
train acc:  0.71875
train loss:  0.5045389533042908
train gradient:  0.14958726624530905
iteration : 3341
train acc:  0.765625
train loss:  0.49604374170303345
train gradient:  0.17035122442918899
iteration : 3342
train acc:  0.734375
train loss:  0.5238447785377502
train gradient:  0.1506098687422882
iteration : 3343
train acc:  0.75
train loss:  0.4488030672073364
train gradient:  0.1342554002371301
iteration : 3344
train acc:  0.6953125
train loss:  0.4890148639678955
train gradient:  0.11933230572769447
iteration : 3345
train acc:  0.6953125
train loss:  0.5613363981246948
train gradient:  0.21097116854590084
iteration : 3346
train acc:  0.71875
train loss:  0.5518348217010498
train gradient:  0.2118692179623709
iteration : 3347
train acc:  0.7109375
train loss:  0.5370892286300659
train gradient:  0.17630424795685268
iteration : 3348
train acc:  0.65625
train loss:  0.5762551426887512
train gradient:  0.1883555101838723
iteration : 3349
train acc:  0.6953125
train loss:  0.5415424108505249
train gradient:  0.1591378934440702
iteration : 3350
train acc:  0.640625
train loss:  0.585284948348999
train gradient:  0.15907020554853096
iteration : 3351
train acc:  0.703125
train loss:  0.556841254234314
train gradient:  0.1819177171545619
iteration : 3352
train acc:  0.75
train loss:  0.5000909566879272
train gradient:  0.19038245686837238
iteration : 3353
train acc:  0.71875
train loss:  0.5368072390556335
train gradient:  0.15630441863491779
iteration : 3354
train acc:  0.640625
train loss:  0.5431107878684998
train gradient:  0.17439274089856904
iteration : 3355
train acc:  0.734375
train loss:  0.5359699130058289
train gradient:  0.15505262404560152
iteration : 3356
train acc:  0.7109375
train loss:  0.5568625330924988
train gradient:  0.15412313079008405
iteration : 3357
train acc:  0.765625
train loss:  0.47933125495910645
train gradient:  0.12899687525226716
iteration : 3358
train acc:  0.7421875
train loss:  0.48626160621643066
train gradient:  0.13501576374203444
iteration : 3359
train acc:  0.734375
train loss:  0.5182422399520874
train gradient:  0.17628373861994784
iteration : 3360
train acc:  0.6484375
train loss:  0.6112432479858398
train gradient:  0.16754302138963928
iteration : 3361
train acc:  0.7578125
train loss:  0.49632325768470764
train gradient:  0.15480326044600923
iteration : 3362
train acc:  0.75
train loss:  0.496744304895401
train gradient:  0.14310877942422442
iteration : 3363
train acc:  0.7421875
train loss:  0.5248110294342041
train gradient:  0.17542957734949535
iteration : 3364
train acc:  0.7109375
train loss:  0.5167648792266846
train gradient:  0.18092985872266024
iteration : 3365
train acc:  0.7421875
train loss:  0.5014559030532837
train gradient:  0.15357535068812486
iteration : 3366
train acc:  0.703125
train loss:  0.5198696255683899
train gradient:  0.1570624246443738
iteration : 3367
train acc:  0.65625
train loss:  0.5937767028808594
train gradient:  0.21064330175916973
iteration : 3368
train acc:  0.671875
train loss:  0.5835084915161133
train gradient:  0.22006697983285645
iteration : 3369
train acc:  0.6640625
train loss:  0.5580736398696899
train gradient:  0.18402888595946743
iteration : 3370
train acc:  0.703125
train loss:  0.5358420610427856
train gradient:  0.1591797061234681
iteration : 3371
train acc:  0.6796875
train loss:  0.5714350342750549
train gradient:  0.15594010662984842
iteration : 3372
train acc:  0.7109375
train loss:  0.4680311977863312
train gradient:  0.11648710170329822
iteration : 3373
train acc:  0.6640625
train loss:  0.537919819355011
train gradient:  0.15431032144290102
iteration : 3374
train acc:  0.6875
train loss:  0.5062308311462402
train gradient:  0.190575637683361
iteration : 3375
train acc:  0.65625
train loss:  0.6081557273864746
train gradient:  0.1905927109898723
iteration : 3376
train acc:  0.6875
train loss:  0.6092546582221985
train gradient:  0.255111535549695
iteration : 3377
train acc:  0.765625
train loss:  0.4928436875343323
train gradient:  0.16933999724849674
iteration : 3378
train acc:  0.6796875
train loss:  0.5596127510070801
train gradient:  0.17751679873448645
iteration : 3379
train acc:  0.703125
train loss:  0.5417392253875732
train gradient:  0.16374164566260202
iteration : 3380
train acc:  0.6875
train loss:  0.5772219896316528
train gradient:  0.20230950625647517
iteration : 3381
train acc:  0.6640625
train loss:  0.589180588722229
train gradient:  0.23821853479177002
iteration : 3382
train acc:  0.7109375
train loss:  0.5244253873825073
train gradient:  0.18995436940202148
iteration : 3383
train acc:  0.7734375
train loss:  0.4906955659389496
train gradient:  0.15527569255697524
iteration : 3384
train acc:  0.734375
train loss:  0.510872483253479
train gradient:  0.18180943198208602
iteration : 3385
train acc:  0.7421875
train loss:  0.5142495036125183
train gradient:  0.15812951074679177
iteration : 3386
train acc:  0.71875
train loss:  0.5893444418907166
train gradient:  0.17026723890480142
iteration : 3387
train acc:  0.6875
train loss:  0.5476773977279663
train gradient:  0.17051302685947567
iteration : 3388
train acc:  0.65625
train loss:  0.5992062091827393
train gradient:  0.20850091425274242
iteration : 3389
train acc:  0.6875
train loss:  0.5256731510162354
train gradient:  0.14308799124603985
iteration : 3390
train acc:  0.65625
train loss:  0.5904040932655334
train gradient:  0.20472826430798974
iteration : 3391
train acc:  0.75
train loss:  0.511096715927124
train gradient:  0.13018110959132762
iteration : 3392
train acc:  0.7578125
train loss:  0.4871443510055542
train gradient:  0.167923368084415
iteration : 3393
train acc:  0.7578125
train loss:  0.5626299381256104
train gradient:  0.1832475180483108
iteration : 3394
train acc:  0.71875
train loss:  0.5390713214874268
train gradient:  0.15278034750114056
iteration : 3395
train acc:  0.765625
train loss:  0.5283327102661133
train gradient:  0.16375172545286684
iteration : 3396
train acc:  0.7578125
train loss:  0.4891766607761383
train gradient:  0.14038388342645713
iteration : 3397
train acc:  0.703125
train loss:  0.5573127269744873
train gradient:  0.18313799180904022
iteration : 3398
train acc:  0.6953125
train loss:  0.594062089920044
train gradient:  0.1597943756415764
iteration : 3399
train acc:  0.7578125
train loss:  0.46227091550827026
train gradient:  0.13160245299908024
iteration : 3400
train acc:  0.703125
train loss:  0.5499602556228638
train gradient:  0.2062484079750259
iteration : 3401
train acc:  0.7578125
train loss:  0.47643640637397766
train gradient:  0.15561801708284145
iteration : 3402
train acc:  0.7734375
train loss:  0.4994869828224182
train gradient:  0.15470173356470107
iteration : 3403
train acc:  0.765625
train loss:  0.5139392614364624
train gradient:  0.1563075975580585
iteration : 3404
train acc:  0.734375
train loss:  0.4926135540008545
train gradient:  0.11952065286456301
iteration : 3405
train acc:  0.6875
train loss:  0.5316128134727478
train gradient:  0.15501892226010042
iteration : 3406
train acc:  0.6875
train loss:  0.5807633996009827
train gradient:  0.17204770864005986
iteration : 3407
train acc:  0.7890625
train loss:  0.5285950899124146
train gradient:  0.13856492049188618
iteration : 3408
train acc:  0.734375
train loss:  0.5144599080085754
train gradient:  0.1344224234062611
iteration : 3409
train acc:  0.6875
train loss:  0.5355491638183594
train gradient:  0.14596892698047537
iteration : 3410
train acc:  0.7109375
train loss:  0.509947657585144
train gradient:  0.13553767415767226
iteration : 3411
train acc:  0.734375
train loss:  0.49372321367263794
train gradient:  0.1434228742808485
iteration : 3412
train acc:  0.6953125
train loss:  0.5351841449737549
train gradient:  0.1956832883737466
iteration : 3413
train acc:  0.7109375
train loss:  0.5651259422302246
train gradient:  0.17588968850132644
iteration : 3414
train acc:  0.7265625
train loss:  0.5386086702346802
train gradient:  0.1446059481115236
iteration : 3415
train acc:  0.7421875
train loss:  0.5211052894592285
train gradient:  0.24523025670655296
iteration : 3416
train acc:  0.7109375
train loss:  0.5312601327896118
train gradient:  0.13141486711754463
iteration : 3417
train acc:  0.7421875
train loss:  0.5157490372657776
train gradient:  0.21582842299519442
iteration : 3418
train acc:  0.7421875
train loss:  0.5219278931617737
train gradient:  0.1557968642486961
iteration : 3419
train acc:  0.7421875
train loss:  0.5351694822311401
train gradient:  0.1601077009071662
iteration : 3420
train acc:  0.734375
train loss:  0.4988257586956024
train gradient:  0.13091640924910636
iteration : 3421
train acc:  0.7265625
train loss:  0.5627491474151611
train gradient:  0.14843611577708826
iteration : 3422
train acc:  0.6640625
train loss:  0.5664461255073547
train gradient:  0.18988365515029187
iteration : 3423
train acc:  0.734375
train loss:  0.4807073771953583
train gradient:  0.18525914452371284
iteration : 3424
train acc:  0.75
train loss:  0.5393747091293335
train gradient:  0.22325252080858465
iteration : 3425
train acc:  0.703125
train loss:  0.5563098192214966
train gradient:  0.1470457540603502
iteration : 3426
train acc:  0.671875
train loss:  0.5570527911186218
train gradient:  0.16299518509230673
iteration : 3427
train acc:  0.671875
train loss:  0.54356849193573
train gradient:  0.1807822084941022
iteration : 3428
train acc:  0.6953125
train loss:  0.5443751811981201
train gradient:  0.1837084007543987
iteration : 3429
train acc:  0.6796875
train loss:  0.5416632890701294
train gradient:  0.18762065640582756
iteration : 3430
train acc:  0.6953125
train loss:  0.526020348072052
train gradient:  0.12587026213699803
iteration : 3431
train acc:  0.7109375
train loss:  0.5278356671333313
train gradient:  0.16181037120314246
iteration : 3432
train acc:  0.7109375
train loss:  0.5403966903686523
train gradient:  0.1924963493686017
iteration : 3433
train acc:  0.640625
train loss:  0.5948168039321899
train gradient:  0.22770719938769557
iteration : 3434
train acc:  0.7578125
train loss:  0.494634211063385
train gradient:  0.13866489602043566
iteration : 3435
train acc:  0.7109375
train loss:  0.5526022911071777
train gradient:  0.1354174306482248
iteration : 3436
train acc:  0.7109375
train loss:  0.5540244579315186
train gradient:  0.17239882011006163
iteration : 3437
train acc:  0.75
train loss:  0.4469054341316223
train gradient:  0.13734925635326378
iteration : 3438
train acc:  0.7890625
train loss:  0.476037859916687
train gradient:  0.17063464098979636
iteration : 3439
train acc:  0.640625
train loss:  0.5492644309997559
train gradient:  0.16137462188069568
iteration : 3440
train acc:  0.7578125
train loss:  0.5045530796051025
train gradient:  0.15169487859613956
iteration : 3441
train acc:  0.6875
train loss:  0.5626205801963806
train gradient:  0.14568679191415634
iteration : 3442
train acc:  0.65625
train loss:  0.5618979334831238
train gradient:  0.15104085229332626
iteration : 3443
train acc:  0.6171875
train loss:  0.6316335797309875
train gradient:  0.24415031949023366
iteration : 3444
train acc:  0.6875
train loss:  0.5453839302062988
train gradient:  0.14621489829861195
iteration : 3445
train acc:  0.6640625
train loss:  0.5430196523666382
train gradient:  0.1926166226822234
iteration : 3446
train acc:  0.75
train loss:  0.49840036034584045
train gradient:  0.1629707615327733
iteration : 3447
train acc:  0.7890625
train loss:  0.5034019947052002
train gradient:  0.136219088115135
iteration : 3448
train acc:  0.7421875
train loss:  0.5475624799728394
train gradient:  0.15077995212724893
iteration : 3449
train acc:  0.6328125
train loss:  0.5982515811920166
train gradient:  0.19684321877997313
iteration : 3450
train acc:  0.7265625
train loss:  0.559202253818512
train gradient:  0.1982605076518591
iteration : 3451
train acc:  0.7578125
train loss:  0.5577747821807861
train gradient:  0.16878058661387674
iteration : 3452
train acc:  0.7265625
train loss:  0.5616468787193298
train gradient:  0.18571130650343926
iteration : 3453
train acc:  0.6640625
train loss:  0.5889707803726196
train gradient:  0.16460819228163806
iteration : 3454
train acc:  0.7890625
train loss:  0.45390474796295166
train gradient:  0.14179357157938824
iteration : 3455
train acc:  0.7421875
train loss:  0.4733984172344208
train gradient:  0.1387147380390863
iteration : 3456
train acc:  0.6796875
train loss:  0.5761423110961914
train gradient:  0.19117612684089058
iteration : 3457
train acc:  0.75
train loss:  0.5469361543655396
train gradient:  0.1841865352909975
iteration : 3458
train acc:  0.71875
train loss:  0.5216989517211914
train gradient:  0.16143108232361475
iteration : 3459
train acc:  0.6796875
train loss:  0.5897495746612549
train gradient:  0.16382733320068546
iteration : 3460
train acc:  0.734375
train loss:  0.5445834398269653
train gradient:  0.16211675970838957
iteration : 3461
train acc:  0.703125
train loss:  0.5171920657157898
train gradient:  0.17733931190125463
iteration : 3462
train acc:  0.734375
train loss:  0.5526584982872009
train gradient:  0.15980925526243264
iteration : 3463
train acc:  0.6875
train loss:  0.5357733368873596
train gradient:  0.19444805636599433
iteration : 3464
train acc:  0.734375
train loss:  0.4992575943470001
train gradient:  0.1896532271870828
iteration : 3465
train acc:  0.7421875
train loss:  0.4863970875740051
train gradient:  0.1418114030317826
iteration : 3466
train acc:  0.78125
train loss:  0.484737753868103
train gradient:  0.1354088219810527
iteration : 3467
train acc:  0.7109375
train loss:  0.5425237417221069
train gradient:  0.15652612839574404
iteration : 3468
train acc:  0.78125
train loss:  0.46039798855781555
train gradient:  0.13279216054649357
iteration : 3469
train acc:  0.75
train loss:  0.5145630836486816
train gradient:  0.15184875377485485
iteration : 3470
train acc:  0.71875
train loss:  0.4885651469230652
train gradient:  0.11931629220114476
iteration : 3471
train acc:  0.7421875
train loss:  0.5230790376663208
train gradient:  0.16564561660742716
iteration : 3472
train acc:  0.6875
train loss:  0.6010279059410095
train gradient:  0.18747282045510888
iteration : 3473
train acc:  0.734375
train loss:  0.524499773979187
train gradient:  0.19092193578871525
iteration : 3474
train acc:  0.6640625
train loss:  0.5633739233016968
train gradient:  0.21274893400067213
iteration : 3475
train acc:  0.765625
train loss:  0.48883065581321716
train gradient:  0.1157103983175981
iteration : 3476
train acc:  0.71875
train loss:  0.5887646675109863
train gradient:  0.2820776522756565
iteration : 3477
train acc:  0.7265625
train loss:  0.5151243209838867
train gradient:  0.15747260264677893
iteration : 3478
train acc:  0.71875
train loss:  0.5069308876991272
train gradient:  0.1275269123776565
iteration : 3479
train acc:  0.71875
train loss:  0.5493609309196472
train gradient:  0.12686478348912655
iteration : 3480
train acc:  0.7265625
train loss:  0.5285894274711609
train gradient:  0.13432226638798578
iteration : 3481
train acc:  0.703125
train loss:  0.5201743841171265
train gradient:  0.17337616413525536
iteration : 3482
train acc:  0.6796875
train loss:  0.5908674597740173
train gradient:  0.206344109270882
iteration : 3483
train acc:  0.75
train loss:  0.5108797550201416
train gradient:  0.15390780116599115
iteration : 3484
train acc:  0.7578125
train loss:  0.5125086307525635
train gradient:  0.1289952334345975
iteration : 3485
train acc:  0.6953125
train loss:  0.5413671135902405
train gradient:  0.20040705276770765
iteration : 3486
train acc:  0.671875
train loss:  0.5647372603416443
train gradient:  0.17019008664788707
iteration : 3487
train acc:  0.6953125
train loss:  0.5220547318458557
train gradient:  0.13844976982153445
iteration : 3488
train acc:  0.703125
train loss:  0.5392130613327026
train gradient:  0.1801654700342799
iteration : 3489
train acc:  0.6640625
train loss:  0.6364555358886719
train gradient:  0.2084884169428703
iteration : 3490
train acc:  0.6640625
train loss:  0.5891232490539551
train gradient:  0.19443857636567263
iteration : 3491
train acc:  0.7265625
train loss:  0.5488554239273071
train gradient:  0.17150386095464742
iteration : 3492
train acc:  0.7421875
train loss:  0.5376228094100952
train gradient:  0.20292476271368198
iteration : 3493
train acc:  0.734375
train loss:  0.5384572148323059
train gradient:  0.1533268276885619
iteration : 3494
train acc:  0.671875
train loss:  0.5807355642318726
train gradient:  0.15018165553561705
iteration : 3495
train acc:  0.7109375
train loss:  0.5333049297332764
train gradient:  0.1775004234322572
iteration : 3496
train acc:  0.6953125
train loss:  0.5569482445716858
train gradient:  0.16261346943588634
iteration : 3497
train acc:  0.75
train loss:  0.4651191234588623
train gradient:  0.12119531846623494
iteration : 3498
train acc:  0.6484375
train loss:  0.5588884949684143
train gradient:  0.1343708406914636
iteration : 3499
train acc:  0.734375
train loss:  0.5146076083183289
train gradient:  0.13567907774183835
iteration : 3500
train acc:  0.7109375
train loss:  0.5936408042907715
train gradient:  0.1576213314043196
iteration : 3501
train acc:  0.703125
train loss:  0.5491581559181213
train gradient:  0.19055032432450364
iteration : 3502
train acc:  0.6875
train loss:  0.593422532081604
train gradient:  0.3548258822336876
iteration : 3503
train acc:  0.7578125
train loss:  0.5019147992134094
train gradient:  0.15318513382247467
iteration : 3504
train acc:  0.71875
train loss:  0.5619341135025024
train gradient:  0.1700956653019354
iteration : 3505
train acc:  0.7578125
train loss:  0.47921323776245117
train gradient:  0.15349608513347412
iteration : 3506
train acc:  0.6875
train loss:  0.5396144986152649
train gradient:  0.13183624189944193
iteration : 3507
train acc:  0.6953125
train loss:  0.57274329662323
train gradient:  0.3236532735034675
iteration : 3508
train acc:  0.8125
train loss:  0.47576987743377686
train gradient:  0.12075491494649103
iteration : 3509
train acc:  0.6953125
train loss:  0.5325595140457153
train gradient:  0.14726337033004194
iteration : 3510
train acc:  0.7890625
train loss:  0.49788331985473633
train gradient:  0.17356725681486157
iteration : 3511
train acc:  0.703125
train loss:  0.5687073469161987
train gradient:  0.1596024547714028
iteration : 3512
train acc:  0.734375
train loss:  0.5219301581382751
train gradient:  0.14322096134777396
iteration : 3513
train acc:  0.7734375
train loss:  0.45251327753067017
train gradient:  0.107993510607316
iteration : 3514
train acc:  0.625
train loss:  0.6001929044723511
train gradient:  0.22626476085896022
iteration : 3515
train acc:  0.7109375
train loss:  0.5600127577781677
train gradient:  0.15400318818181896
iteration : 3516
train acc:  0.703125
train loss:  0.5160953998565674
train gradient:  0.1577057298995915
iteration : 3517
train acc:  0.7578125
train loss:  0.5049095153808594
train gradient:  0.15730755320535322
iteration : 3518
train acc:  0.7109375
train loss:  0.5505213141441345
train gradient:  0.13967471418139293
iteration : 3519
train acc:  0.7578125
train loss:  0.4780149757862091
train gradient:  0.14102009454141617
iteration : 3520
train acc:  0.6796875
train loss:  0.590808093547821
train gradient:  0.19961290290519174
iteration : 3521
train acc:  0.7734375
train loss:  0.47883111238479614
train gradient:  0.11784729653318761
iteration : 3522
train acc:  0.703125
train loss:  0.540184497833252
train gradient:  0.16080266937705967
iteration : 3523
train acc:  0.671875
train loss:  0.5791633129119873
train gradient:  0.17630190090228448
iteration : 3524
train acc:  0.734375
train loss:  0.5457496643066406
train gradient:  0.2366440018403307
iteration : 3525
train acc:  0.765625
train loss:  0.48839372396469116
train gradient:  0.1308671789697753
iteration : 3526
train acc:  0.7734375
train loss:  0.511681318283081
train gradient:  0.13409530128619374
iteration : 3527
train acc:  0.6875
train loss:  0.5469963550567627
train gradient:  0.15515845402956813
iteration : 3528
train acc:  0.625
train loss:  0.6274813413619995
train gradient:  0.18633844753673645
iteration : 3529
train acc:  0.75
train loss:  0.4872610569000244
train gradient:  0.15048406376596507
iteration : 3530
train acc:  0.7265625
train loss:  0.500386118888855
train gradient:  0.14881440461532824
iteration : 3531
train acc:  0.7109375
train loss:  0.5952528119087219
train gradient:  0.23434904810857476
iteration : 3532
train acc:  0.7421875
train loss:  0.5281704068183899
train gradient:  0.1561135778432074
iteration : 3533
train acc:  0.7109375
train loss:  0.5607689619064331
train gradient:  0.15668642474956712
iteration : 3534
train acc:  0.6640625
train loss:  0.5901552438735962
train gradient:  0.16284989169241113
iteration : 3535
train acc:  0.6015625
train loss:  0.6294070482254028
train gradient:  0.22551957794821148
iteration : 3536
train acc:  0.6875
train loss:  0.5647859573364258
train gradient:  0.27793926710633876
iteration : 3537
train acc:  0.75
train loss:  0.506415843963623
train gradient:  0.10658131631521844
iteration : 3538
train acc:  0.7265625
train loss:  0.500762939453125
train gradient:  0.16253649939087228
iteration : 3539
train acc:  0.6875
train loss:  0.5743253827095032
train gradient:  0.1909762761334563
iteration : 3540
train acc:  0.6796875
train loss:  0.5319029688835144
train gradient:  0.14540497041147932
iteration : 3541
train acc:  0.7265625
train loss:  0.5264853239059448
train gradient:  0.15932865509220123
iteration : 3542
train acc:  0.765625
train loss:  0.4968574345111847
train gradient:  0.14999341450015458
iteration : 3543
train acc:  0.75
train loss:  0.5142114758491516
train gradient:  0.24068461825895393
iteration : 3544
train acc:  0.765625
train loss:  0.4612213969230652
train gradient:  0.14374544098654207
iteration : 3545
train acc:  0.7578125
train loss:  0.4526354670524597
train gradient:  0.16298499676608272
iteration : 3546
train acc:  0.6953125
train loss:  0.491001158952713
train gradient:  0.10963836811604386
iteration : 3547
train acc:  0.8046875
train loss:  0.4747886657714844
train gradient:  0.11342304951241838
iteration : 3548
train acc:  0.7265625
train loss:  0.49702632427215576
train gradient:  0.18675219824515796
iteration : 3549
train acc:  0.6953125
train loss:  0.5758036375045776
train gradient:  0.1638017415862586
iteration : 3550
train acc:  0.71875
train loss:  0.5389245748519897
train gradient:  0.15268756218032165
iteration : 3551
train acc:  0.671875
train loss:  0.5639608502388
train gradient:  0.218671835470221
iteration : 3552
train acc:  0.765625
train loss:  0.4771718382835388
train gradient:  0.14064611059551552
iteration : 3553
train acc:  0.6640625
train loss:  0.575355589389801
train gradient:  0.16346956728650158
iteration : 3554
train acc:  0.7734375
train loss:  0.49242082238197327
train gradient:  0.14474555082511678
iteration : 3555
train acc:  0.7578125
train loss:  0.5103311538696289
train gradient:  0.15347759099348465
iteration : 3556
train acc:  0.7109375
train loss:  0.5229963064193726
train gradient:  0.13657447719617594
iteration : 3557
train acc:  0.7734375
train loss:  0.44718727469444275
train gradient:  0.11070706599502388
iteration : 3558
train acc:  0.6875
train loss:  0.5405430197715759
train gradient:  0.13787170709971175
iteration : 3559
train acc:  0.75
train loss:  0.5038152933120728
train gradient:  0.1331062663794353
iteration : 3560
train acc:  0.71875
train loss:  0.48071327805519104
train gradient:  0.11851498923913882
iteration : 3561
train acc:  0.8046875
train loss:  0.4955621063709259
train gradient:  0.1266559889493366
iteration : 3562
train acc:  0.6953125
train loss:  0.5183956623077393
train gradient:  0.18113416858285802
iteration : 3563
train acc:  0.75
train loss:  0.5417742133140564
train gradient:  0.1439599082660209
iteration : 3564
train acc:  0.703125
train loss:  0.5523045659065247
train gradient:  0.18848146536265506
iteration : 3565
train acc:  0.7734375
train loss:  0.5130623579025269
train gradient:  0.15137967082893272
iteration : 3566
train acc:  0.671875
train loss:  0.5977160930633545
train gradient:  0.16048588889395482
iteration : 3567
train acc:  0.7109375
train loss:  0.525117814540863
train gradient:  0.14646454754022376
iteration : 3568
train acc:  0.703125
train loss:  0.565861165523529
train gradient:  0.17823597966416416
iteration : 3569
train acc:  0.765625
train loss:  0.46454858779907227
train gradient:  0.11042985137792814
iteration : 3570
train acc:  0.7890625
train loss:  0.5398682951927185
train gradient:  0.16862118250566094
iteration : 3571
train acc:  0.7265625
train loss:  0.5234616994857788
train gradient:  0.1311120665476695
iteration : 3572
train acc:  0.703125
train loss:  0.5124254822731018
train gradient:  0.13122168125807446
iteration : 3573
train acc:  0.7421875
train loss:  0.4825176000595093
train gradient:  0.15212934568923292
iteration : 3574
train acc:  0.7578125
train loss:  0.4736844301223755
train gradient:  0.11170670297210632
iteration : 3575
train acc:  0.75
train loss:  0.5344067811965942
train gradient:  0.16186682659883542
iteration : 3576
train acc:  0.6640625
train loss:  0.5677402019500732
train gradient:  0.15538359925546746
iteration : 3577
train acc:  0.6875
train loss:  0.5298979878425598
train gradient:  0.15070878158530213
iteration : 3578
train acc:  0.703125
train loss:  0.5831559896469116
train gradient:  0.17554993067085037
iteration : 3579
train acc:  0.6640625
train loss:  0.6281000971794128
train gradient:  0.165311675523796
iteration : 3580
train acc:  0.75
train loss:  0.4786660671234131
train gradient:  0.1549504791938665
iteration : 3581
train acc:  0.671875
train loss:  0.5693942308425903
train gradient:  0.18638777518197835
iteration : 3582
train acc:  0.75
train loss:  0.5069293975830078
train gradient:  0.19054610020983315
iteration : 3583
train acc:  0.71875
train loss:  0.497170627117157
train gradient:  0.23749916183344472
iteration : 3584
train acc:  0.75
train loss:  0.5128575563430786
train gradient:  0.18597854152535004
iteration : 3585
train acc:  0.71875
train loss:  0.4838038682937622
train gradient:  0.13684135127253266
iteration : 3586
train acc:  0.7265625
train loss:  0.5269606113433838
train gradient:  0.14591613508909979
iteration : 3587
train acc:  0.6953125
train loss:  0.5305353999137878
train gradient:  0.23043057864073022
iteration : 3588
train acc:  0.671875
train loss:  0.5675772428512573
train gradient:  0.16255497461290017
iteration : 3589
train acc:  0.765625
train loss:  0.49233826994895935
train gradient:  0.14018683900886258
iteration : 3590
train acc:  0.7421875
train loss:  0.45945239067077637
train gradient:  0.14290238931919697
iteration : 3591
train acc:  0.7890625
train loss:  0.4594074785709381
train gradient:  0.11500245754258226
iteration : 3592
train acc:  0.7109375
train loss:  0.5071477293968201
train gradient:  0.14797372102386097
iteration : 3593
train acc:  0.7421875
train loss:  0.46709972620010376
train gradient:  0.19652238931736424
iteration : 3594
train acc:  0.7109375
train loss:  0.5214613676071167
train gradient:  0.13098575080546548
iteration : 3595
train acc:  0.7109375
train loss:  0.5382015705108643
train gradient:  0.13686467920863274
iteration : 3596
train acc:  0.671875
train loss:  0.5451802015304565
train gradient:  0.17265400772170728
iteration : 3597
train acc:  0.7578125
train loss:  0.49352961778640747
train gradient:  0.16731230598714147
iteration : 3598
train acc:  0.6640625
train loss:  0.5656212568283081
train gradient:  0.19264757165191965
iteration : 3599
train acc:  0.7265625
train loss:  0.5470597743988037
train gradient:  0.1570389178334344
iteration : 3600
train acc:  0.7421875
train loss:  0.5844999551773071
train gradient:  0.15071768105281586
iteration : 3601
train acc:  0.671875
train loss:  0.5875267386436462
train gradient:  0.22227435402337098
iteration : 3602
train acc:  0.8046875
train loss:  0.41938239336013794
train gradient:  0.12157880272816259
iteration : 3603
train acc:  0.75
train loss:  0.47259634733200073
train gradient:  0.12766002681321342
iteration : 3604
train acc:  0.65625
train loss:  0.6602518558502197
train gradient:  0.27996994455368673
iteration : 3605
train acc:  0.7734375
train loss:  0.46941280364990234
train gradient:  0.16388533707212555
iteration : 3606
train acc:  0.7421875
train loss:  0.5114321708679199
train gradient:  0.17350247089528298
iteration : 3607
train acc:  0.7421875
train loss:  0.4941936135292053
train gradient:  0.16532324894940054
iteration : 3608
train acc:  0.703125
train loss:  0.5671235918998718
train gradient:  0.19732623770546603
iteration : 3609
train acc:  0.6953125
train loss:  0.5519427061080933
train gradient:  0.2069067045993382
iteration : 3610
train acc:  0.7109375
train loss:  0.5668990612030029
train gradient:  0.15960848543566175
iteration : 3611
train acc:  0.71875
train loss:  0.5586749315261841
train gradient:  0.1934293638635753
iteration : 3612
train acc:  0.6640625
train loss:  0.5455283522605896
train gradient:  0.1762409910397411
iteration : 3613
train acc:  0.7109375
train loss:  0.5368250608444214
train gradient:  0.17271598666444907
iteration : 3614
train acc:  0.7109375
train loss:  0.5181975960731506
train gradient:  0.23691006861321967
iteration : 3615
train acc:  0.78125
train loss:  0.49689781665802
train gradient:  0.17910629336529604
iteration : 3616
train acc:  0.6953125
train loss:  0.5625913143157959
train gradient:  0.21973647184085954
iteration : 3617
train acc:  0.796875
train loss:  0.4582993686199188
train gradient:  0.13598691496720405
iteration : 3618
train acc:  0.7109375
train loss:  0.5003169178962708
train gradient:  0.1505980210356608
iteration : 3619
train acc:  0.71875
train loss:  0.5413099527359009
train gradient:  0.1571732980788097
iteration : 3620
train acc:  0.7109375
train loss:  0.5372812747955322
train gradient:  0.19697824535766806
iteration : 3621
train acc:  0.6953125
train loss:  0.5427759885787964
train gradient:  0.19561648091297063
iteration : 3622
train acc:  0.7265625
train loss:  0.4835180342197418
train gradient:  0.1918526582242342
iteration : 3623
train acc:  0.7734375
train loss:  0.48920002579689026
train gradient:  0.16840883326140738
iteration : 3624
train acc:  0.78125
train loss:  0.5081009864807129
train gradient:  0.18845920586474652
iteration : 3625
train acc:  0.7734375
train loss:  0.482283353805542
train gradient:  0.1702503521607766
iteration : 3626
train acc:  0.7109375
train loss:  0.5324035882949829
train gradient:  0.19277261129168471
iteration : 3627
train acc:  0.671875
train loss:  0.5293956995010376
train gradient:  0.1410980021330836
iteration : 3628
train acc:  0.671875
train loss:  0.5493669509887695
train gradient:  0.18814234320915343
iteration : 3629
train acc:  0.7421875
train loss:  0.4699428081512451
train gradient:  0.15283597771871543
iteration : 3630
train acc:  0.7265625
train loss:  0.5119583010673523
train gradient:  0.12469645529646954
iteration : 3631
train acc:  0.734375
train loss:  0.5362100601196289
train gradient:  0.17237465483308187
iteration : 3632
train acc:  0.6875
train loss:  0.5566283464431763
train gradient:  0.16698078020929485
iteration : 3633
train acc:  0.78125
train loss:  0.46884602308273315
train gradient:  0.15390606734292456
iteration : 3634
train acc:  0.7421875
train loss:  0.47986990213394165
train gradient:  0.14045208309947532
iteration : 3635
train acc:  0.71875
train loss:  0.5090259909629822
train gradient:  0.1424057461124046
iteration : 3636
train acc:  0.71875
train loss:  0.5339982509613037
train gradient:  0.15221376359611669
iteration : 3637
train acc:  0.6953125
train loss:  0.5658532381057739
train gradient:  0.20043651964199666
iteration : 3638
train acc:  0.7421875
train loss:  0.5347487330436707
train gradient:  0.20376050778228855
iteration : 3639
train acc:  0.7734375
train loss:  0.4618356227874756
train gradient:  0.16086113591111711
iteration : 3640
train acc:  0.75
train loss:  0.5266807079315186
train gradient:  0.14582406278039717
iteration : 3641
train acc:  0.7109375
train loss:  0.5667403340339661
train gradient:  0.16424175770959734
iteration : 3642
train acc:  0.7890625
train loss:  0.5034922361373901
train gradient:  0.15390801462143439
iteration : 3643
train acc:  0.7734375
train loss:  0.4431849718093872
train gradient:  0.14267132727529436
iteration : 3644
train acc:  0.5859375
train loss:  0.6442201733589172
train gradient:  0.19539428373558465
iteration : 3645
train acc:  0.7578125
train loss:  0.4706842303276062
train gradient:  0.14559355998986168
iteration : 3646
train acc:  0.6953125
train loss:  0.560300350189209
train gradient:  0.16622010791175001
iteration : 3647
train acc:  0.640625
train loss:  0.6152430772781372
train gradient:  0.23542302939934867
iteration : 3648
train acc:  0.765625
train loss:  0.47150903940200806
train gradient:  0.10812924033614368
iteration : 3649
train acc:  0.7578125
train loss:  0.4802510142326355
train gradient:  0.18721025186653237
iteration : 3650
train acc:  0.6875
train loss:  0.5450572967529297
train gradient:  0.13521161566404033
iteration : 3651
train acc:  0.6640625
train loss:  0.5869276523590088
train gradient:  0.20155840571410147
iteration : 3652
train acc:  0.7265625
train loss:  0.5001767873764038
train gradient:  0.1512579089432705
iteration : 3653
train acc:  0.6328125
train loss:  0.609641432762146
train gradient:  0.2461416675524049
iteration : 3654
train acc:  0.75
train loss:  0.4942503869533539
train gradient:  0.16419348268636752
iteration : 3655
train acc:  0.7265625
train loss:  0.5349841117858887
train gradient:  0.17016778191344828
iteration : 3656
train acc:  0.6875
train loss:  0.5953752994537354
train gradient:  0.2832432723193014
iteration : 3657
train acc:  0.671875
train loss:  0.6309671401977539
train gradient:  0.20432948249324467
iteration : 3658
train acc:  0.71875
train loss:  0.5214399099349976
train gradient:  0.16417580080895394
iteration : 3659
train acc:  0.71875
train loss:  0.5769671201705933
train gradient:  0.2103740124098409
iteration : 3660
train acc:  0.734375
train loss:  0.4929502308368683
train gradient:  0.15029391219073646
iteration : 3661
train acc:  0.765625
train loss:  0.4701378345489502
train gradient:  0.1171238229107621
iteration : 3662
train acc:  0.765625
train loss:  0.46843278408050537
train gradient:  0.17869508546507618
iteration : 3663
train acc:  0.734375
train loss:  0.5177062153816223
train gradient:  0.13146860798280907
iteration : 3664
train acc:  0.75
train loss:  0.5080690383911133
train gradient:  0.14049314581129613
iteration : 3665
train acc:  0.7109375
train loss:  0.5294693112373352
train gradient:  0.14900642659331165
iteration : 3666
train acc:  0.7578125
train loss:  0.5182426571846008
train gradient:  0.15393991856123876
iteration : 3667
train acc:  0.7734375
train loss:  0.4890379011631012
train gradient:  0.14276932700615338
iteration : 3668
train acc:  0.6171875
train loss:  0.5602881908416748
train gradient:  0.1446132244699338
iteration : 3669
train acc:  0.671875
train loss:  0.5817330479621887
train gradient:  0.18085019327521817
iteration : 3670
train acc:  0.7421875
train loss:  0.4731140434741974
train gradient:  0.1454758760870381
iteration : 3671
train acc:  0.75
train loss:  0.48815131187438965
train gradient:  0.12296121402455092
iteration : 3672
train acc:  0.765625
train loss:  0.5430804491043091
train gradient:  0.19835629127752416
iteration : 3673
train acc:  0.734375
train loss:  0.5422024726867676
train gradient:  0.17962613001794198
iteration : 3674
train acc:  0.703125
train loss:  0.49585527181625366
train gradient:  0.14508305029090762
iteration : 3675
train acc:  0.6953125
train loss:  0.5973128080368042
train gradient:  0.17948012827832163
iteration : 3676
train acc:  0.6796875
train loss:  0.5276092290878296
train gradient:  0.14289225864993413
iteration : 3677
train acc:  0.734375
train loss:  0.5012155175209045
train gradient:  0.1946020101622665
iteration : 3678
train acc:  0.6875
train loss:  0.5295451879501343
train gradient:  0.17972115075321793
iteration : 3679
train acc:  0.7421875
train loss:  0.5388606786727905
train gradient:  0.16662759653550008
iteration : 3680
train acc:  0.7265625
train loss:  0.5165258646011353
train gradient:  0.14634046131501202
iteration : 3681
train acc:  0.625
train loss:  0.6302699446678162
train gradient:  0.28354195148740624
iteration : 3682
train acc:  0.671875
train loss:  0.5822435617446899
train gradient:  0.164168028746565
iteration : 3683
train acc:  0.75
train loss:  0.4721285104751587
train gradient:  0.12962881362766412
iteration : 3684
train acc:  0.7890625
train loss:  0.4257127642631531
train gradient:  0.1137081076067633
iteration : 3685
train acc:  0.6796875
train loss:  0.6450715065002441
train gradient:  0.2951499049894058
iteration : 3686
train acc:  0.7421875
train loss:  0.5369009971618652
train gradient:  0.15675869289547145
iteration : 3687
train acc:  0.7265625
train loss:  0.5348343849182129
train gradient:  0.1542316822298091
iteration : 3688
train acc:  0.734375
train loss:  0.48273080587387085
train gradient:  0.15514774350538427
iteration : 3689
train acc:  0.6875
train loss:  0.5891783833503723
train gradient:  0.16889114419642914
iteration : 3690
train acc:  0.796875
train loss:  0.4947603940963745
train gradient:  0.16812972926766628
iteration : 3691
train acc:  0.734375
train loss:  0.5133388042449951
train gradient:  0.1532860251116675
iteration : 3692
train acc:  0.6796875
train loss:  0.6544960141181946
train gradient:  0.2902784300244051
iteration : 3693
train acc:  0.6484375
train loss:  0.5656340718269348
train gradient:  0.17943790495384168
iteration : 3694
train acc:  0.7421875
train loss:  0.5101724863052368
train gradient:  0.19507301448432496
iteration : 3695
train acc:  0.7109375
train loss:  0.5386185050010681
train gradient:  0.14151363001966688
iteration : 3696
train acc:  0.7578125
train loss:  0.5166863799095154
train gradient:  0.13598419903332326
iteration : 3697
train acc:  0.75
train loss:  0.47459232807159424
train gradient:  0.12797295959284344
iteration : 3698
train acc:  0.6875
train loss:  0.5677887201309204
train gradient:  0.19178341760470366
iteration : 3699
train acc:  0.7265625
train loss:  0.5141966342926025
train gradient:  0.15420740673760497
iteration : 3700
train acc:  0.734375
train loss:  0.5655018091201782
train gradient:  0.17180882229647015
iteration : 3701
train acc:  0.7734375
train loss:  0.5097044706344604
train gradient:  0.13743407416390802
iteration : 3702
train acc:  0.734375
train loss:  0.5311130285263062
train gradient:  0.13301711318759268
iteration : 3703
train acc:  0.65625
train loss:  0.5578757524490356
train gradient:  0.17652485183333375
iteration : 3704
train acc:  0.671875
train loss:  0.5654855966567993
train gradient:  0.14505314287583182
iteration : 3705
train acc:  0.71875
train loss:  0.5072673559188843
train gradient:  0.152071540402023
iteration : 3706
train acc:  0.703125
train loss:  0.6045702695846558
train gradient:  0.2202356103581321
iteration : 3707
train acc:  0.765625
train loss:  0.48231634497642517
train gradient:  0.19697476617510662
iteration : 3708
train acc:  0.765625
train loss:  0.4869626760482788
train gradient:  0.1633436870615424
iteration : 3709
train acc:  0.6875
train loss:  0.571648359298706
train gradient:  0.1855075270805245
iteration : 3710
train acc:  0.8046875
train loss:  0.45653462409973145
train gradient:  0.1645384316561252
iteration : 3711
train acc:  0.6953125
train loss:  0.5145572423934937
train gradient:  0.14940510674536323
iteration : 3712
train acc:  0.7265625
train loss:  0.5124328136444092
train gradient:  0.1353295394791657
iteration : 3713
train acc:  0.7265625
train loss:  0.5172445774078369
train gradient:  0.1583505213094311
iteration : 3714
train acc:  0.75
train loss:  0.5047272443771362
train gradient:  0.18171021985450592
iteration : 3715
train acc:  0.671875
train loss:  0.637287974357605
train gradient:  0.20743469502059458
iteration : 3716
train acc:  0.7265625
train loss:  0.496061235666275
train gradient:  0.12750628835074035
iteration : 3717
train acc:  0.7109375
train loss:  0.5662302374839783
train gradient:  0.23417122437505544
iteration : 3718
train acc:  0.7734375
train loss:  0.5277272462844849
train gradient:  0.16559472306637038
iteration : 3719
train acc:  0.7265625
train loss:  0.5250751972198486
train gradient:  0.1686659373693684
iteration : 3720
train acc:  0.7421875
train loss:  0.5383047461509705
train gradient:  0.20203218314109495
iteration : 3721
train acc:  0.7421875
train loss:  0.521058976650238
train gradient:  0.20546094117150332
iteration : 3722
train acc:  0.734375
train loss:  0.5025857090950012
train gradient:  0.14928219577521662
iteration : 3723
train acc:  0.734375
train loss:  0.5171064138412476
train gradient:  0.14757438464565853
iteration : 3724
train acc:  0.703125
train loss:  0.5816360712051392
train gradient:  0.20980906911176025
iteration : 3725
train acc:  0.7578125
train loss:  0.5128710269927979
train gradient:  0.13801880877432893
iteration : 3726
train acc:  0.71875
train loss:  0.5796819925308228
train gradient:  0.19913223127584795
iteration : 3727
train acc:  0.65625
train loss:  0.5955284237861633
train gradient:  0.20960903938369352
iteration : 3728
train acc:  0.71875
train loss:  0.5765147805213928
train gradient:  0.1601691485507986
iteration : 3729
train acc:  0.734375
train loss:  0.5307279825210571
train gradient:  0.12182074222746618
iteration : 3730
train acc:  0.703125
train loss:  0.5773335695266724
train gradient:  0.2257230754936873
iteration : 3731
train acc:  0.75
train loss:  0.551703929901123
train gradient:  0.2003854416992584
iteration : 3732
train acc:  0.765625
train loss:  0.4759475290775299
train gradient:  0.16669765599476355
iteration : 3733
train acc:  0.703125
train loss:  0.5121290683746338
train gradient:  0.15848019763892224
iteration : 3734
train acc:  0.7265625
train loss:  0.5493751764297485
train gradient:  0.18499426761944465
iteration : 3735
train acc:  0.78125
train loss:  0.5091020464897156
train gradient:  0.1374450277741083
iteration : 3736
train acc:  0.6953125
train loss:  0.6043022871017456
train gradient:  0.19617000235101253
iteration : 3737
train acc:  0.671875
train loss:  0.5794110298156738
train gradient:  0.1682674711731874
iteration : 3738
train acc:  0.7578125
train loss:  0.4980268180370331
train gradient:  0.12643009749583695
iteration : 3739
train acc:  0.7421875
train loss:  0.5021853446960449
train gradient:  0.13947316956937172
iteration : 3740
train acc:  0.7109375
train loss:  0.521433413028717
train gradient:  0.19983928275465107
iteration : 3741
train acc:  0.78125
train loss:  0.46056538820266724
train gradient:  0.13908343873948706
iteration : 3742
train acc:  0.765625
train loss:  0.5026358366012573
train gradient:  0.19971915658335374
iteration : 3743
train acc:  0.71875
train loss:  0.48815077543258667
train gradient:  0.180593306934972
iteration : 3744
train acc:  0.7265625
train loss:  0.48824214935302734
train gradient:  0.15091073593106286
iteration : 3745
train acc:  0.7109375
train loss:  0.5172981023788452
train gradient:  0.13711695508590108
iteration : 3746
train acc:  0.7578125
train loss:  0.4558964669704437
train gradient:  0.11601413356582888
iteration : 3747
train acc:  0.7109375
train loss:  0.5801768898963928
train gradient:  0.16265989618914034
iteration : 3748
train acc:  0.765625
train loss:  0.44475486874580383
train gradient:  0.11878827499232233
iteration : 3749
train acc:  0.7578125
train loss:  0.5604418516159058
train gradient:  0.1607604023256451
iteration : 3750
train acc:  0.6796875
train loss:  0.5476913452148438
train gradient:  0.15030657069308234
iteration : 3751
train acc:  0.6953125
train loss:  0.5522739291191101
train gradient:  0.2367479279114763
iteration : 3752
train acc:  0.7578125
train loss:  0.5518850088119507
train gradient:  0.172614812321626
iteration : 3753
train acc:  0.7109375
train loss:  0.514433741569519
train gradient:  0.17170234156792435
iteration : 3754
train acc:  0.671875
train loss:  0.6097955703735352
train gradient:  0.18847312281589784
iteration : 3755
train acc:  0.7421875
train loss:  0.516961395740509
train gradient:  0.15608737901704356
iteration : 3756
train acc:  0.6796875
train loss:  0.5760610103607178
train gradient:  0.15265298555876877
iteration : 3757
train acc:  0.65625
train loss:  0.6175857782363892
train gradient:  0.20632327259134953
iteration : 3758
train acc:  0.6796875
train loss:  0.6083539128303528
train gradient:  0.20535718340202636
iteration : 3759
train acc:  0.6796875
train loss:  0.5668201446533203
train gradient:  0.2504008542185483
iteration : 3760
train acc:  0.71875
train loss:  0.538926362991333
train gradient:  0.20309550729010717
iteration : 3761
train acc:  0.7109375
train loss:  0.51505047082901
train gradient:  0.1572026933890282
iteration : 3762
train acc:  0.78125
train loss:  0.4695095419883728
train gradient:  0.14581260207077426
iteration : 3763
train acc:  0.7265625
train loss:  0.5137039422988892
train gradient:  0.14142827363214094
iteration : 3764
train acc:  0.7421875
train loss:  0.5278182029724121
train gradient:  0.13424073918100643
iteration : 3765
train acc:  0.78125
train loss:  0.4953482151031494
train gradient:  0.13345549517259317
iteration : 3766
train acc:  0.671875
train loss:  0.6122814416885376
train gradient:  0.2443014258058923
iteration : 3767
train acc:  0.6875
train loss:  0.5509567260742188
train gradient:  0.148322321118671
iteration : 3768
train acc:  0.7578125
train loss:  0.4849390983581543
train gradient:  0.12772646767338253
iteration : 3769
train acc:  0.7265625
train loss:  0.5537106990814209
train gradient:  0.1776656884795617
iteration : 3770
train acc:  0.7578125
train loss:  0.5016612410545349
train gradient:  0.13206620693324708
iteration : 3771
train acc:  0.75
train loss:  0.5069264769554138
train gradient:  0.2093639140807615
iteration : 3772
train acc:  0.7265625
train loss:  0.5190954208374023
train gradient:  0.12749737080237883
iteration : 3773
train acc:  0.703125
train loss:  0.4946017861366272
train gradient:  0.18235693504829875
iteration : 3774
train acc:  0.7734375
train loss:  0.49394258856773376
train gradient:  0.1732012353503482
iteration : 3775
train acc:  0.71875
train loss:  0.47474581003189087
train gradient:  0.11484931428117615
iteration : 3776
train acc:  0.71875
train loss:  0.5329184532165527
train gradient:  0.15133663269047626
iteration : 3777
train acc:  0.703125
train loss:  0.5487596988677979
train gradient:  0.16613383207379628
iteration : 3778
train acc:  0.71875
train loss:  0.5040881037712097
train gradient:  0.16939454741971202
iteration : 3779
train acc:  0.75
train loss:  0.496582955121994
train gradient:  0.12764946797424992
iteration : 3780
train acc:  0.7265625
train loss:  0.49575111269950867
train gradient:  0.1664893699817724
iteration : 3781
train acc:  0.6875
train loss:  0.5965917706489563
train gradient:  0.19031865381417806
iteration : 3782
train acc:  0.671875
train loss:  0.5882523655891418
train gradient:  0.216105910763837
iteration : 3783
train acc:  0.7578125
train loss:  0.5110993385314941
train gradient:  0.15841206598045254
iteration : 3784
train acc:  0.71875
train loss:  0.5590434670448303
train gradient:  0.21138740260837108
iteration : 3785
train acc:  0.6875
train loss:  0.5151733756065369
train gradient:  0.13829383104896206
iteration : 3786
train acc:  0.703125
train loss:  0.5181862115859985
train gradient:  0.1655744724305655
iteration : 3787
train acc:  0.7109375
train loss:  0.5282167196273804
train gradient:  0.13871894860765027
iteration : 3788
train acc:  0.78125
train loss:  0.5136042237281799
train gradient:  0.12569987596450588
iteration : 3789
train acc:  0.7265625
train loss:  0.5154110193252563
train gradient:  0.20839647108549658
iteration : 3790
train acc:  0.71875
train loss:  0.5336304903030396
train gradient:  0.20901119709616264
iteration : 3791
train acc:  0.7421875
train loss:  0.5404560565948486
train gradient:  0.15653327608347767
iteration : 3792
train acc:  0.65625
train loss:  0.5619372725486755
train gradient:  0.17708915258712604
iteration : 3793
train acc:  0.78125
train loss:  0.5174909234046936
train gradient:  0.19491708711030215
iteration : 3794
train acc:  0.7109375
train loss:  0.520725667476654
train gradient:  0.15681110070856874
iteration : 3795
train acc:  0.765625
train loss:  0.4895027279853821
train gradient:  0.1564054631000539
iteration : 3796
train acc:  0.734375
train loss:  0.5415031909942627
train gradient:  0.18267160120848663
iteration : 3797
train acc:  0.671875
train loss:  0.5725894570350647
train gradient:  0.18722132074736747
iteration : 3798
train acc:  0.7265625
train loss:  0.5242522954940796
train gradient:  0.25546416098513236
iteration : 3799
train acc:  0.7578125
train loss:  0.5513840913772583
train gradient:  0.15959209150342363
iteration : 3800
train acc:  0.6796875
train loss:  0.5692205429077148
train gradient:  0.19517393182879034
iteration : 3801
train acc:  0.75
train loss:  0.5132555961608887
train gradient:  0.13678760098697804
iteration : 3802
train acc:  0.765625
train loss:  0.4325934648513794
train gradient:  0.11443890179470517
iteration : 3803
train acc:  0.734375
train loss:  0.5573250651359558
train gradient:  0.17010572302604365
iteration : 3804
train acc:  0.7890625
train loss:  0.4762455224990845
train gradient:  0.1368985304693388
iteration : 3805
train acc:  0.6875
train loss:  0.5410078763961792
train gradient:  0.1560119031514729
iteration : 3806
train acc:  0.7578125
train loss:  0.4919772446155548
train gradient:  0.11520531405209726
iteration : 3807
train acc:  0.7421875
train loss:  0.5239535570144653
train gradient:  0.17780335862421381
iteration : 3808
train acc:  0.6796875
train loss:  0.5587705969810486
train gradient:  0.17475044187293892
iteration : 3809
train acc:  0.734375
train loss:  0.5370677709579468
train gradient:  0.1814758892755351
iteration : 3810
train acc:  0.6875
train loss:  0.5532877445220947
train gradient:  0.21921674667254437
iteration : 3811
train acc:  0.8046875
train loss:  0.42812058329582214
train gradient:  0.1479537901406976
iteration : 3812
train acc:  0.6953125
train loss:  0.6226071715354919
train gradient:  0.23517087940004225
iteration : 3813
train acc:  0.7890625
train loss:  0.48537853360176086
train gradient:  0.12409145251476586
iteration : 3814
train acc:  0.7109375
train loss:  0.5621134042739868
train gradient:  0.17030507759555508
iteration : 3815
train acc:  0.6796875
train loss:  0.5937178730964661
train gradient:  0.22813781298872643
iteration : 3816
train acc:  0.75
train loss:  0.5475412011146545
train gradient:  0.215165234729787
iteration : 3817
train acc:  0.7421875
train loss:  0.5015081167221069
train gradient:  0.14748200232424763
iteration : 3818
train acc:  0.7578125
train loss:  0.5031012296676636
train gradient:  0.1472153344838577
iteration : 3819
train acc:  0.765625
train loss:  0.49089083075523376
train gradient:  0.13271312755471162
iteration : 3820
train acc:  0.6796875
train loss:  0.5358447432518005
train gradient:  0.16349440320360337
iteration : 3821
train acc:  0.7109375
train loss:  0.5207740664482117
train gradient:  0.15127427328578336
iteration : 3822
train acc:  0.6875
train loss:  0.5269293785095215
train gradient:  0.15582777064874714
iteration : 3823
train acc:  0.703125
train loss:  0.5345569849014282
train gradient:  0.1769010988290287
iteration : 3824
train acc:  0.7421875
train loss:  0.4833002984523773
train gradient:  0.10153438926302416
iteration : 3825
train acc:  0.6875
train loss:  0.5987237691879272
train gradient:  0.167047568455427
iteration : 3826
train acc:  0.7421875
train loss:  0.5160260200500488
train gradient:  0.1498899852644892
iteration : 3827
train acc:  0.6796875
train loss:  0.5395793318748474
train gradient:  0.15972291987299303
iteration : 3828
train acc:  0.671875
train loss:  0.5289949178695679
train gradient:  0.1495117715494218
iteration : 3829
train acc:  0.7265625
train loss:  0.5045632123947144
train gradient:  0.13037281073884316
iteration : 3830
train acc:  0.703125
train loss:  0.5586327314376831
train gradient:  0.1546763247184565
iteration : 3831
train acc:  0.7265625
train loss:  0.5087896585464478
train gradient:  0.14217254875094942
iteration : 3832
train acc:  0.7734375
train loss:  0.530640184879303
train gradient:  0.1557924194909821
iteration : 3833
train acc:  0.6953125
train loss:  0.5773918032646179
train gradient:  0.19579389524666146
iteration : 3834
train acc:  0.734375
train loss:  0.5248706340789795
train gradient:  0.13573063128199955
iteration : 3835
train acc:  0.7578125
train loss:  0.5459858179092407
train gradient:  0.18588052107366132
iteration : 3836
train acc:  0.71875
train loss:  0.5409483909606934
train gradient:  0.18570780918414306
iteration : 3837
train acc:  0.75
train loss:  0.5505478382110596
train gradient:  0.15469904076591223
iteration : 3838
train acc:  0.703125
train loss:  0.526168704032898
train gradient:  0.13045451515841128
iteration : 3839
train acc:  0.6640625
train loss:  0.5956323146820068
train gradient:  0.22267176996719212
iteration : 3840
train acc:  0.7109375
train loss:  0.5242593288421631
train gradient:  0.1784688335410266
iteration : 3841
train acc:  0.703125
train loss:  0.5675023794174194
train gradient:  0.15590710112739253
iteration : 3842
train acc:  0.640625
train loss:  0.5569002628326416
train gradient:  0.1996040402009018
iteration : 3843
train acc:  0.7265625
train loss:  0.5310285091400146
train gradient:  0.19357531164142927
iteration : 3844
train acc:  0.7109375
train loss:  0.5434165000915527
train gradient:  0.17251378239631582
iteration : 3845
train acc:  0.734375
train loss:  0.5404753684997559
train gradient:  0.15873806492738418
iteration : 3846
train acc:  0.71875
train loss:  0.48246434330940247
train gradient:  0.11717851795830535
iteration : 3847
train acc:  0.6796875
train loss:  0.61415696144104
train gradient:  0.1859174833970384
iteration : 3848
train acc:  0.7109375
train loss:  0.5402495861053467
train gradient:  0.13934746103297524
iteration : 3849
train acc:  0.7109375
train loss:  0.5000635385513306
train gradient:  0.13363880057852978
iteration : 3850
train acc:  0.75
train loss:  0.4634679853916168
train gradient:  0.12510802541249585
iteration : 3851
train acc:  0.6875
train loss:  0.5768418908119202
train gradient:  0.1995201858890996
iteration : 3852
train acc:  0.703125
train loss:  0.5609745979309082
train gradient:  0.177058829799364
iteration : 3853
train acc:  0.734375
train loss:  0.5188889503479004
train gradient:  0.13997528999697795
iteration : 3854
train acc:  0.7578125
train loss:  0.4920177459716797
train gradient:  0.15393888709250944
iteration : 3855
train acc:  0.703125
train loss:  0.533927857875824
train gradient:  0.16871752979650212
iteration : 3856
train acc:  0.7890625
train loss:  0.4839633107185364
train gradient:  0.1697897063211068
iteration : 3857
train acc:  0.7109375
train loss:  0.5329543352127075
train gradient:  0.13529537673337855
iteration : 3858
train acc:  0.75
train loss:  0.5197281241416931
train gradient:  0.1453218883978834
iteration : 3859
train acc:  0.75
train loss:  0.4940926432609558
train gradient:  0.14990227562820502
iteration : 3860
train acc:  0.671875
train loss:  0.5397289395332336
train gradient:  0.1546861673173865
iteration : 3861
train acc:  0.7578125
train loss:  0.444841593503952
train gradient:  0.11840744478049142
iteration : 3862
train acc:  0.6953125
train loss:  0.5120266675949097
train gradient:  0.12011238558106317
iteration : 3863
train acc:  0.75
train loss:  0.4607692062854767
train gradient:  0.15865580225689754
iteration : 3864
train acc:  0.734375
train loss:  0.5633249282836914
train gradient:  0.15293314181222672
iteration : 3865
train acc:  0.7265625
train loss:  0.49889349937438965
train gradient:  0.12731130030298338
iteration : 3866
train acc:  0.734375
train loss:  0.5967224836349487
train gradient:  0.26420131267830455
iteration : 3867
train acc:  0.71875
train loss:  0.534885585308075
train gradient:  0.149701860330267
iteration : 3868
train acc:  0.71875
train loss:  0.5114141702651978
train gradient:  0.13676785112048612
iteration : 3869
train acc:  0.7109375
train loss:  0.5708733797073364
train gradient:  0.20189427184638578
iteration : 3870
train acc:  0.6875
train loss:  0.5552316904067993
train gradient:  0.17222351547265347
iteration : 3871
train acc:  0.6953125
train loss:  0.5304278135299683
train gradient:  0.14895583990456998
iteration : 3872
train acc:  0.65625
train loss:  0.6175163984298706
train gradient:  0.2511669695217481
iteration : 3873
train acc:  0.7734375
train loss:  0.47501659393310547
train gradient:  0.1410268728215126
iteration : 3874
train acc:  0.796875
train loss:  0.46544766426086426
train gradient:  0.14505285710653287
iteration : 3875
train acc:  0.6953125
train loss:  0.5369861721992493
train gradient:  0.1964547337737499
iteration : 3876
train acc:  0.796875
train loss:  0.4765435457229614
train gradient:  0.160135240005727
iteration : 3877
train acc:  0.7578125
train loss:  0.510033130645752
train gradient:  0.17811951791647002
iteration : 3878
train acc:  0.71875
train loss:  0.5135815143585205
train gradient:  0.13889132539205354
iteration : 3879
train acc:  0.765625
train loss:  0.48934024572372437
train gradient:  0.17139960403889726
iteration : 3880
train acc:  0.7421875
train loss:  0.55368971824646
train gradient:  0.12384518350843553
iteration : 3881
train acc:  0.6953125
train loss:  0.5472670793533325
train gradient:  0.1721466852182772
iteration : 3882
train acc:  0.765625
train loss:  0.5005572438240051
train gradient:  0.12118657275514133
iteration : 3883
train acc:  0.734375
train loss:  0.48931390047073364
train gradient:  0.1394326987455078
iteration : 3884
train acc:  0.765625
train loss:  0.4634013772010803
train gradient:  0.10282385072552319
iteration : 3885
train acc:  0.7109375
train loss:  0.5488975048065186
train gradient:  0.1457049934467367
iteration : 3886
train acc:  0.640625
train loss:  0.5966334342956543
train gradient:  0.22956202788127156
iteration : 3887
train acc:  0.734375
train loss:  0.5614867210388184
train gradient:  0.19909534926034217
iteration : 3888
train acc:  0.703125
train loss:  0.5476603507995605
train gradient:  0.16269043480375922
iteration : 3889
train acc:  0.7578125
train loss:  0.5148754715919495
train gradient:  0.12666881180498196
iteration : 3890
train acc:  0.7421875
train loss:  0.49743711948394775
train gradient:  0.1540558584388571
iteration : 3891
train acc:  0.71875
train loss:  0.5225129127502441
train gradient:  0.13042936330994964
iteration : 3892
train acc:  0.6484375
train loss:  0.6284189820289612
train gradient:  0.21534091138518635
iteration : 3893
train acc:  0.6875
train loss:  0.5383447408676147
train gradient:  0.2304070963261282
iteration : 3894
train acc:  0.734375
train loss:  0.48944091796875
train gradient:  0.16403994750153467
iteration : 3895
train acc:  0.6875
train loss:  0.547562837600708
train gradient:  0.14741667732725822
iteration : 3896
train acc:  0.765625
train loss:  0.4864310324192047
train gradient:  0.13485670075992642
iteration : 3897
train acc:  0.8046875
train loss:  0.4694660007953644
train gradient:  0.13821955127001267
iteration : 3898
train acc:  0.65625
train loss:  0.5791722536087036
train gradient:  0.18460433017058828
iteration : 3899
train acc:  0.6875
train loss:  0.5875318050384521
train gradient:  0.18131195169082814
iteration : 3900
train acc:  0.7265625
train loss:  0.5485150814056396
train gradient:  0.15235317996665257
iteration : 3901
train acc:  0.7265625
train loss:  0.49377572536468506
train gradient:  0.15491675798234888
iteration : 3902
train acc:  0.78125
train loss:  0.45969557762145996
train gradient:  0.19622170265024602
iteration : 3903
train acc:  0.71875
train loss:  0.502773642539978
train gradient:  0.20691366753027757
iteration : 3904
train acc:  0.7421875
train loss:  0.5331951379776001
train gradient:  0.14201651344644609
iteration : 3905
train acc:  0.7109375
train loss:  0.5425549149513245
train gradient:  0.1612573326023879
iteration : 3906
train acc:  0.7109375
train loss:  0.5370767116546631
train gradient:  0.15008103661769315
iteration : 3907
train acc:  0.7734375
train loss:  0.492112934589386
train gradient:  0.1925784003773704
iteration : 3908
train acc:  0.765625
train loss:  0.510274350643158
train gradient:  0.1485456026001713
iteration : 3909
train acc:  0.8203125
train loss:  0.5000216960906982
train gradient:  0.1391279851305311
iteration : 3910
train acc:  0.7109375
train loss:  0.5055264830589294
train gradient:  0.1651686837100789
iteration : 3911
train acc:  0.703125
train loss:  0.5073498487472534
train gradient:  0.19133023875735428
iteration : 3912
train acc:  0.6953125
train loss:  0.5811620950698853
train gradient:  0.2244416146033239
iteration : 3913
train acc:  0.703125
train loss:  0.5442094206809998
train gradient:  0.12096550268111107
iteration : 3914
train acc:  0.7578125
train loss:  0.44952812790870667
train gradient:  0.12789605712628543
iteration : 3915
train acc:  0.7734375
train loss:  0.4889482259750366
train gradient:  0.1488493680028538
iteration : 3916
train acc:  0.7109375
train loss:  0.4941471517086029
train gradient:  0.1368672447937065
iteration : 3917
train acc:  0.8125
train loss:  0.4583691954612732
train gradient:  0.10677085014650016
iteration : 3918
train acc:  0.7734375
train loss:  0.4938414692878723
train gradient:  0.13550976421376354
iteration : 3919
train acc:  0.6953125
train loss:  0.5404772758483887
train gradient:  0.1313316879802589
iteration : 3920
train acc:  0.7109375
train loss:  0.5668433308601379
train gradient:  0.17676503138337438
iteration : 3921
train acc:  0.7421875
train loss:  0.4844074845314026
train gradient:  0.17731875152640839
iteration : 3922
train acc:  0.75
train loss:  0.5147428512573242
train gradient:  0.13645387309955448
iteration : 3923
train acc:  0.75
train loss:  0.5047289133071899
train gradient:  0.2317358541670166
iteration : 3924
train acc:  0.75
train loss:  0.5200583338737488
train gradient:  0.19431685743768087
iteration : 3925
train acc:  0.7421875
train loss:  0.5287880897521973
train gradient:  0.15787451212114462
iteration : 3926
train acc:  0.703125
train loss:  0.49498921632766724
train gradient:  0.13726714133561074
iteration : 3927
train acc:  0.65625
train loss:  0.5349361896514893
train gradient:  0.15783105906199818
iteration : 3928
train acc:  0.734375
train loss:  0.48045670986175537
train gradient:  0.14683169241377875
iteration : 3929
train acc:  0.8203125
train loss:  0.4149925410747528
train gradient:  0.1250159313527533
iteration : 3930
train acc:  0.6796875
train loss:  0.5596640110015869
train gradient:  0.18191870394513965
iteration : 3931
train acc:  0.7421875
train loss:  0.5495689511299133
train gradient:  0.18182507006358822
iteration : 3932
train acc:  0.7421875
train loss:  0.5251073241233826
train gradient:  0.14273951923159617
iteration : 3933
train acc:  0.703125
train loss:  0.5150763988494873
train gradient:  0.16669472927196333
iteration : 3934
train acc:  0.7578125
train loss:  0.47187715768814087
train gradient:  0.12231357204853797
iteration : 3935
train acc:  0.7109375
train loss:  0.5016721487045288
train gradient:  0.12937096917938667
iteration : 3936
train acc:  0.7109375
train loss:  0.514920711517334
train gradient:  0.15035888801437935
iteration : 3937
train acc:  0.765625
train loss:  0.4998844861984253
train gradient:  0.14742551595558215
iteration : 3938
train acc:  0.71875
train loss:  0.4997974932193756
train gradient:  0.1395985332994779
iteration : 3939
train acc:  0.7578125
train loss:  0.5447149276733398
train gradient:  0.2402415899039338
iteration : 3940
train acc:  0.7109375
train loss:  0.515861988067627
train gradient:  0.13616007179461148
iteration : 3941
train acc:  0.7578125
train loss:  0.5019068717956543
train gradient:  0.11450554292025716
iteration : 3942
train acc:  0.6953125
train loss:  0.5201977491378784
train gradient:  0.1554302614068014
iteration : 3943
train acc:  0.765625
train loss:  0.499320924282074
train gradient:  0.17498303132018891
iteration : 3944
train acc:  0.703125
train loss:  0.5087284445762634
train gradient:  0.14236605196099456
iteration : 3945
train acc:  0.671875
train loss:  0.5648907423019409
train gradient:  0.18019123257498665
iteration : 3946
train acc:  0.6953125
train loss:  0.5542512536048889
train gradient:  0.16833190459159342
iteration : 3947
train acc:  0.7578125
train loss:  0.508886456489563
train gradient:  0.16368276840117854
iteration : 3948
train acc:  0.7421875
train loss:  0.5431501865386963
train gradient:  0.22584255694495095
iteration : 3949
train acc:  0.7890625
train loss:  0.49293452501296997
train gradient:  0.14583986588691109
iteration : 3950
train acc:  0.6796875
train loss:  0.5482802391052246
train gradient:  0.17615478232891818
iteration : 3951
train acc:  0.6953125
train loss:  0.5132817029953003
train gradient:  0.14904429894330284
iteration : 3952
train acc:  0.7265625
train loss:  0.5008658170700073
train gradient:  0.14808420083990728
iteration : 3953
train acc:  0.71875
train loss:  0.5357242822647095
train gradient:  0.16018098904225364
iteration : 3954
train acc:  0.75
train loss:  0.49327579140663147
train gradient:  0.1510814033997675
iteration : 3955
train acc:  0.78125
train loss:  0.5057405233383179
train gradient:  0.13301525589796248
iteration : 3956
train acc:  0.7734375
train loss:  0.47511857748031616
train gradient:  0.17369658344396
iteration : 3957
train acc:  0.7109375
train loss:  0.511831521987915
train gradient:  0.2039250750903615
iteration : 3958
train acc:  0.734375
train loss:  0.48190322518348694
train gradient:  0.1669624402186632
iteration : 3959
train acc:  0.734375
train loss:  0.5155574679374695
train gradient:  0.15795227892778313
iteration : 3960
train acc:  0.765625
train loss:  0.4823010563850403
train gradient:  0.14167360842200488
iteration : 3961
train acc:  0.6875
train loss:  0.5583605766296387
train gradient:  0.1812283514089485
iteration : 3962
train acc:  0.78125
train loss:  0.5021198987960815
train gradient:  0.16485031092374047
iteration : 3963
train acc:  0.6796875
train loss:  0.5618728399276733
train gradient:  0.15823073622535266
iteration : 3964
train acc:  0.671875
train loss:  0.5310161709785461
train gradient:  0.21666268071938496
iteration : 3965
train acc:  0.8203125
train loss:  0.4297904372215271
train gradient:  0.13053989753435102
iteration : 3966
train acc:  0.7421875
train loss:  0.475181519985199
train gradient:  0.12413279839340939
iteration : 3967
train acc:  0.7578125
train loss:  0.5245327353477478
train gradient:  0.14127038833404382
iteration : 3968
train acc:  0.71875
train loss:  0.5454804301261902
train gradient:  0.1897072303042731
iteration : 3969
train acc:  0.734375
train loss:  0.5104923844337463
train gradient:  0.1602864245411911
iteration : 3970
train acc:  0.734375
train loss:  0.5290055274963379
train gradient:  0.1765534795173605
iteration : 3971
train acc:  0.7421875
train loss:  0.4922182261943817
train gradient:  0.18646325153679416
iteration : 3972
train acc:  0.7734375
train loss:  0.48050397634506226
train gradient:  0.13957166362770027
iteration : 3973
train acc:  0.6875
train loss:  0.5810210704803467
train gradient:  0.23093562485758784
iteration : 3974
train acc:  0.6875
train loss:  0.5678627490997314
train gradient:  0.17776302626945836
iteration : 3975
train acc:  0.78125
train loss:  0.46589118242263794
train gradient:  0.12114380280824773
iteration : 3976
train acc:  0.6484375
train loss:  0.5548170804977417
train gradient:  0.1810151246810385
iteration : 3977
train acc:  0.7265625
train loss:  0.4873712360858917
train gradient:  0.13174517346623815
iteration : 3978
train acc:  0.6953125
train loss:  0.5690864324569702
train gradient:  0.15841840055562012
iteration : 3979
train acc:  0.6953125
train loss:  0.5610823035240173
train gradient:  0.18274096808666054
iteration : 3980
train acc:  0.7421875
train loss:  0.4947109818458557
train gradient:  0.14123514594192155
iteration : 3981
train acc:  0.7421875
train loss:  0.5279675722122192
train gradient:  0.15184079290381136
iteration : 3982
train acc:  0.703125
train loss:  0.5239765644073486
train gradient:  0.15826315069911007
iteration : 3983
train acc:  0.7421875
train loss:  0.4980325996875763
train gradient:  0.11721723821180424
iteration : 3984
train acc:  0.7109375
train loss:  0.5467475652694702
train gradient:  0.22897134963971155
iteration : 3985
train acc:  0.6796875
train loss:  0.5249573588371277
train gradient:  0.15447663374248366
iteration : 3986
train acc:  0.6796875
train loss:  0.56791090965271
train gradient:  0.1749925930940688
iteration : 3987
train acc:  0.703125
train loss:  0.5139185190200806
train gradient:  0.15096411595369075
iteration : 3988
train acc:  0.7109375
train loss:  0.527741551399231
train gradient:  0.13767939988150985
iteration : 3989
train acc:  0.6875
train loss:  0.5605440139770508
train gradient:  0.23547702652053826
iteration : 3990
train acc:  0.6875
train loss:  0.5744268894195557
train gradient:  0.1797475019528751
iteration : 3991
train acc:  0.7265625
train loss:  0.5616692900657654
train gradient:  0.23905067541086644
iteration : 3992
train acc:  0.6875
train loss:  0.5379223823547363
train gradient:  0.16927124113098996
iteration : 3993
train acc:  0.7421875
train loss:  0.517582893371582
train gradient:  0.15189374549816656
iteration : 3994
train acc:  0.765625
train loss:  0.47221675515174866
train gradient:  0.11594135276173231
iteration : 3995
train acc:  0.6796875
train loss:  0.5562000274658203
train gradient:  0.1662539559468956
iteration : 3996
train acc:  0.71875
train loss:  0.5381603240966797
train gradient:  0.154918282146842
iteration : 3997
train acc:  0.7421875
train loss:  0.5547126531600952
train gradient:  0.1699205917433749
iteration : 3998
train acc:  0.6875
train loss:  0.592296838760376
train gradient:  0.18723661462128116
iteration : 3999
train acc:  0.6875
train loss:  0.5936930179595947
train gradient:  0.2492609080398121
iteration : 4000
train acc:  0.71875
train loss:  0.5083739161491394
train gradient:  0.15189460371566937
iteration : 4001
train acc:  0.6875
train loss:  0.6016109585762024
train gradient:  0.199596661265655
iteration : 4002
train acc:  0.7578125
train loss:  0.5175052881240845
train gradient:  0.1710192576204801
iteration : 4003
train acc:  0.7734375
train loss:  0.4667527675628662
train gradient:  0.10400087232453793
iteration : 4004
train acc:  0.703125
train loss:  0.5194640159606934
train gradient:  0.20021278216203303
iteration : 4005
train acc:  0.6953125
train loss:  0.576290488243103
train gradient:  0.33598031055180444
iteration : 4006
train acc:  0.671875
train loss:  0.5138353109359741
train gradient:  0.18684794184132636
iteration : 4007
train acc:  0.65625
train loss:  0.5852911472320557
train gradient:  0.18231149930364618
iteration : 4008
train acc:  0.71875
train loss:  0.512756884098053
train gradient:  0.16051406417512368
iteration : 4009
train acc:  0.65625
train loss:  0.5835599303245544
train gradient:  0.22939862360458252
iteration : 4010
train acc:  0.75
train loss:  0.468360960483551
train gradient:  0.139609236343126
iteration : 4011
train acc:  0.6875
train loss:  0.5671427845954895
train gradient:  0.24267645306407326
iteration : 4012
train acc:  0.703125
train loss:  0.5957949161529541
train gradient:  0.2403663130055016
iteration : 4013
train acc:  0.703125
train loss:  0.5230058431625366
train gradient:  0.14856939472780256
iteration : 4014
train acc:  0.78125
train loss:  0.42521318793296814
train gradient:  0.11713595537908562
iteration : 4015
train acc:  0.703125
train loss:  0.5373234748840332
train gradient:  0.12953597246688534
iteration : 4016
train acc:  0.6953125
train loss:  0.5518946647644043
train gradient:  0.1536084252479516
iteration : 4017
train acc:  0.6640625
train loss:  0.5913692712783813
train gradient:  0.1888734728716138
iteration : 4018
train acc:  0.75
train loss:  0.47076615691185
train gradient:  0.10239716587069116
iteration : 4019
train acc:  0.671875
train loss:  0.5937907695770264
train gradient:  0.18957693039681517
iteration : 4020
train acc:  0.7109375
train loss:  0.47177940607070923
train gradient:  0.15261603583528804
iteration : 4021
train acc:  0.75
train loss:  0.494331032037735
train gradient:  0.12731936560705348
iteration : 4022
train acc:  0.6796875
train loss:  0.5125100612640381
train gradient:  0.14344993146259066
iteration : 4023
train acc:  0.7421875
train loss:  0.5068650245666504
train gradient:  0.14781185397216956
iteration : 4024
train acc:  0.7578125
train loss:  0.5223563313484192
train gradient:  0.14820783672235077
iteration : 4025
train acc:  0.6640625
train loss:  0.5372620820999146
train gradient:  0.1577968612628357
iteration : 4026
train acc:  0.703125
train loss:  0.5035618543624878
train gradient:  0.13192931730269813
iteration : 4027
train acc:  0.671875
train loss:  0.6037111282348633
train gradient:  0.1946273270708327
iteration : 4028
train acc:  0.796875
train loss:  0.48898082971572876
train gradient:  0.1304582408102945
iteration : 4029
train acc:  0.75
train loss:  0.4876900911331177
train gradient:  0.15704558921795492
iteration : 4030
train acc:  0.796875
train loss:  0.4899557828903198
train gradient:  0.12035923468446456
iteration : 4031
train acc:  0.640625
train loss:  0.5317785143852234
train gradient:  0.15905306751792675
iteration : 4032
train acc:  0.7890625
train loss:  0.4713197946548462
train gradient:  0.1403772462672287
iteration : 4033
train acc:  0.6953125
train loss:  0.536940336227417
train gradient:  0.18163240923307447
iteration : 4034
train acc:  0.75
train loss:  0.4734854996204376
train gradient:  0.11399740046987528
iteration : 4035
train acc:  0.7109375
train loss:  0.5070650577545166
train gradient:  0.12769520567216985
iteration : 4036
train acc:  0.7421875
train loss:  0.5074811577796936
train gradient:  0.11547263518747955
iteration : 4037
train acc:  0.7578125
train loss:  0.4672221541404724
train gradient:  0.1807411611931949
iteration : 4038
train acc:  0.7265625
train loss:  0.5232754945755005
train gradient:  0.1624396198470926
iteration : 4039
train acc:  0.703125
train loss:  0.5338870286941528
train gradient:  0.17269267251982817
iteration : 4040
train acc:  0.734375
train loss:  0.5224422812461853
train gradient:  0.1593813617066473
iteration : 4041
train acc:  0.640625
train loss:  0.602104127407074
train gradient:  0.2034195058204258
iteration : 4042
train acc:  0.6953125
train loss:  0.5847120881080627
train gradient:  0.2135949806506367
iteration : 4043
train acc:  0.7265625
train loss:  0.4861640930175781
train gradient:  0.13702469806854478
iteration : 4044
train acc:  0.7890625
train loss:  0.4757702350616455
train gradient:  0.15671872066495604
iteration : 4045
train acc:  0.703125
train loss:  0.5492916107177734
train gradient:  0.16175357742056762
iteration : 4046
train acc:  0.6875
train loss:  0.572868824005127
train gradient:  0.1829830507135515
iteration : 4047
train acc:  0.734375
train loss:  0.49809205532073975
train gradient:  0.1506254592465271
iteration : 4048
train acc:  0.7734375
train loss:  0.46138545870780945
train gradient:  0.11725841830849078
iteration : 4049
train acc:  0.7265625
train loss:  0.5331196784973145
train gradient:  0.15024964009954556
iteration : 4050
train acc:  0.7578125
train loss:  0.48110759258270264
train gradient:  0.14386518171065382
iteration : 4051
train acc:  0.71875
train loss:  0.49584895372390747
train gradient:  0.14056815699241343
iteration : 4052
train acc:  0.734375
train loss:  0.49895307421684265
train gradient:  0.12778772138237
iteration : 4053
train acc:  0.6796875
train loss:  0.5493283867835999
train gradient:  0.16287562212970347
iteration : 4054
train acc:  0.7265625
train loss:  0.5389623045921326
train gradient:  0.13629055589408404
iteration : 4055
train acc:  0.6796875
train loss:  0.5648790597915649
train gradient:  0.19593478665047004
iteration : 4056
train acc:  0.734375
train loss:  0.5002342462539673
train gradient:  0.11899822223310398
iteration : 4057
train acc:  0.7265625
train loss:  0.5369684100151062
train gradient:  0.1474605506416155
iteration : 4058
train acc:  0.7265625
train loss:  0.49967023730278015
train gradient:  0.1815191734127211
iteration : 4059
train acc:  0.7421875
train loss:  0.46881014108657837
train gradient:  0.17495107226856893
iteration : 4060
train acc:  0.6484375
train loss:  0.5663121938705444
train gradient:  0.17441618668487946
iteration : 4061
train acc:  0.6953125
train loss:  0.5328543186187744
train gradient:  0.18273430561637616
iteration : 4062
train acc:  0.6640625
train loss:  0.5251514315605164
train gradient:  0.1482008461024104
iteration : 4063
train acc:  0.75
train loss:  0.46325433254241943
train gradient:  0.1286438208074282
iteration : 4064
train acc:  0.7109375
train loss:  0.5252881050109863
train gradient:  0.16221297720567085
iteration : 4065
train acc:  0.6796875
train loss:  0.5092548131942749
train gradient:  0.11213957387787317
iteration : 4066
train acc:  0.703125
train loss:  0.5449701547622681
train gradient:  0.13343345595198303
iteration : 4067
train acc:  0.7109375
train loss:  0.5074449777603149
train gradient:  0.14073909714012398
iteration : 4068
train acc:  0.7265625
train loss:  0.4845713675022125
train gradient:  0.12565264007862958
iteration : 4069
train acc:  0.6953125
train loss:  0.5222197771072388
train gradient:  0.14741978183547408
iteration : 4070
train acc:  0.7265625
train loss:  0.5127366185188293
train gradient:  0.16598307026821474
iteration : 4071
train acc:  0.7890625
train loss:  0.45041438937187195
train gradient:  0.12905156316022204
iteration : 4072
train acc:  0.7109375
train loss:  0.5720134973526001
train gradient:  0.16191580086320614
iteration : 4073
train acc:  0.734375
train loss:  0.5525563955307007
train gradient:  0.1285410690821906
iteration : 4074
train acc:  0.8046875
train loss:  0.462691992521286
train gradient:  0.11807209944171794
iteration : 4075
train acc:  0.7578125
train loss:  0.5383200645446777
train gradient:  0.1617002789661853
iteration : 4076
train acc:  0.7109375
train loss:  0.49243682622909546
train gradient:  0.11629537474467531
iteration : 4077
train acc:  0.703125
train loss:  0.5234243869781494
train gradient:  0.1375871166476742
iteration : 4078
train acc:  0.78125
train loss:  0.5140695571899414
train gradient:  0.16670541266963818
iteration : 4079
train acc:  0.6875
train loss:  0.5708409547805786
train gradient:  0.21337741343458566
iteration : 4080
train acc:  0.7265625
train loss:  0.5160835385322571
train gradient:  0.21359049787798004
iteration : 4081
train acc:  0.6796875
train loss:  0.5889003276824951
train gradient:  0.23447449142023447
iteration : 4082
train acc:  0.78125
train loss:  0.48352235555648804
train gradient:  0.13235336984386437
iteration : 4083
train acc:  0.796875
train loss:  0.4750143885612488
train gradient:  0.18756193523708226
iteration : 4084
train acc:  0.7109375
train loss:  0.5452148914337158
train gradient:  0.1851671095376984
iteration : 4085
train acc:  0.765625
train loss:  0.5281294584274292
train gradient:  0.16265021588809267
iteration : 4086
train acc:  0.6796875
train loss:  0.5424348711967468
train gradient:  0.13307341236565445
iteration : 4087
train acc:  0.6875
train loss:  0.588166356086731
train gradient:  0.15920520319805437
iteration : 4088
train acc:  0.78125
train loss:  0.45073050260543823
train gradient:  0.10529807326570838
iteration : 4089
train acc:  0.8203125
train loss:  0.4571218192577362
train gradient:  0.15118363480161626
iteration : 4090
train acc:  0.765625
train loss:  0.505158007144928
train gradient:  0.12450492295269486
iteration : 4091
train acc:  0.7734375
train loss:  0.45903536677360535
train gradient:  0.1321413231502882
iteration : 4092
train acc:  0.703125
train loss:  0.5157576203346252
train gradient:  0.1276570417002264
iteration : 4093
train acc:  0.6953125
train loss:  0.5257738828659058
train gradient:  0.15121585493286926
iteration : 4094
train acc:  0.734375
train loss:  0.4941723346710205
train gradient:  0.10814391930029524
iteration : 4095
train acc:  0.6953125
train loss:  0.4907605051994324
train gradient:  0.1322170048421319
iteration : 4096
train acc:  0.71875
train loss:  0.4843476712703705
train gradient:  0.14973772580348155
iteration : 4097
train acc:  0.78125
train loss:  0.468683123588562
train gradient:  0.1348270410491907
iteration : 4098
train acc:  0.7421875
train loss:  0.507408618927002
train gradient:  0.11608663474552608
iteration : 4099
train acc:  0.6875
train loss:  0.6239063143730164
train gradient:  0.18780022143452837
iteration : 4100
train acc:  0.7109375
train loss:  0.5424423217773438
train gradient:  0.1382036904075453
iteration : 4101
train acc:  0.7109375
train loss:  0.5258939266204834
train gradient:  0.19263128976377342
iteration : 4102
train acc:  0.6796875
train loss:  0.5728106498718262
train gradient:  0.18297608126749648
iteration : 4103
train acc:  0.734375
train loss:  0.514418363571167
train gradient:  0.13720846636234232
iteration : 4104
train acc:  0.734375
train loss:  0.552344560623169
train gradient:  0.17975494353159
iteration : 4105
train acc:  0.703125
train loss:  0.5232599973678589
train gradient:  0.16796220641595672
iteration : 4106
train acc:  0.75
train loss:  0.5151855945587158
train gradient:  0.14901824463325966
iteration : 4107
train acc:  0.7734375
train loss:  0.426372230052948
train gradient:  0.11013887198044191
iteration : 4108
train acc:  0.7109375
train loss:  0.49523231387138367
train gradient:  0.1346194561920843
iteration : 4109
train acc:  0.7265625
train loss:  0.5142214894294739
train gradient:  0.16218423119876996
iteration : 4110
train acc:  0.7421875
train loss:  0.4929182231426239
train gradient:  0.1377578209352438
iteration : 4111
train acc:  0.7421875
train loss:  0.574167013168335
train gradient:  0.22284884641063063
iteration : 4112
train acc:  0.71875
train loss:  0.4969252943992615
train gradient:  0.157058465516333
iteration : 4113
train acc:  0.78125
train loss:  0.47750934958457947
train gradient:  0.13623312855753988
iteration : 4114
train acc:  0.65625
train loss:  0.5971544981002808
train gradient:  0.21738920695013622
iteration : 4115
train acc:  0.796875
train loss:  0.5148496627807617
train gradient:  0.2076370500703837
iteration : 4116
train acc:  0.6796875
train loss:  0.5668120384216309
train gradient:  0.20618348331916403
iteration : 4117
train acc:  0.6875
train loss:  0.549035370349884
train gradient:  0.16711500594574735
iteration : 4118
train acc:  0.703125
train loss:  0.5394903421401978
train gradient:  0.16762589629441912
iteration : 4119
train acc:  0.75
train loss:  0.49464428424835205
train gradient:  0.1599841467483741
iteration : 4120
train acc:  0.703125
train loss:  0.5131100416183472
train gradient:  0.17806809012244212
iteration : 4121
train acc:  0.7109375
train loss:  0.5235636830329895
train gradient:  0.16051166562468766
iteration : 4122
train acc:  0.734375
train loss:  0.5379088521003723
train gradient:  0.1445527699048933
iteration : 4123
train acc:  0.7421875
train loss:  0.5035300850868225
train gradient:  0.17629934871444244
iteration : 4124
train acc:  0.7421875
train loss:  0.5161800980567932
train gradient:  0.14658561943029053
iteration : 4125
train acc:  0.7265625
train loss:  0.4732328951358795
train gradient:  0.13387103454172478
iteration : 4126
train acc:  0.7109375
train loss:  0.537392258644104
train gradient:  0.17831506615231044
iteration : 4127
train acc:  0.71875
train loss:  0.5574885606765747
train gradient:  0.15671793549724827
iteration : 4128
train acc:  0.7421875
train loss:  0.5773959159851074
train gradient:  0.20419608281143187
iteration : 4129
train acc:  0.765625
train loss:  0.4546639919281006
train gradient:  0.12992827084577702
iteration : 4130
train acc:  0.6796875
train loss:  0.5751702785491943
train gradient:  0.2101039818489514
iteration : 4131
train acc:  0.6953125
train loss:  0.5382589101791382
train gradient:  0.15793845371789228
iteration : 4132
train acc:  0.6640625
train loss:  0.5537841320037842
train gradient:  0.17976921752382943
iteration : 4133
train acc:  0.6953125
train loss:  0.5633372068405151
train gradient:  0.16635454619172252
iteration : 4134
train acc:  0.6875
train loss:  0.5941240787506104
train gradient:  0.18107166574441608
iteration : 4135
train acc:  0.7421875
train loss:  0.49888962507247925
train gradient:  0.15468284061493703
iteration : 4136
train acc:  0.703125
train loss:  0.5367379188537598
train gradient:  0.13421828282107134
iteration : 4137
train acc:  0.7265625
train loss:  0.48106998205184937
train gradient:  0.13014388943450578
iteration : 4138
train acc:  0.703125
train loss:  0.5423300266265869
train gradient:  0.12634214784519868
iteration : 4139
train acc:  0.78125
train loss:  0.4649695158004761
train gradient:  0.11935265743599484
iteration : 4140
train acc:  0.7109375
train loss:  0.5289528369903564
train gradient:  0.20365543826841487
iteration : 4141
train acc:  0.65625
train loss:  0.5936175584793091
train gradient:  0.1780421540723959
iteration : 4142
train acc:  0.734375
train loss:  0.47834479808807373
train gradient:  0.1606745893063116
iteration : 4143
train acc:  0.765625
train loss:  0.4710248112678528
train gradient:  0.12925101652972054
iteration : 4144
train acc:  0.7109375
train loss:  0.5605367422103882
train gradient:  0.1529823904881184
iteration : 4145
train acc:  0.671875
train loss:  0.5843018293380737
train gradient:  0.1649495066677782
iteration : 4146
train acc:  0.703125
train loss:  0.5693436861038208
train gradient:  0.18051300221322547
iteration : 4147
train acc:  0.703125
train loss:  0.5254788398742676
train gradient:  0.14635991760804373
iteration : 4148
train acc:  0.765625
train loss:  0.5088167190551758
train gradient:  0.178781732943101
iteration : 4149
train acc:  0.6875
train loss:  0.5930113792419434
train gradient:  0.25118671987116303
iteration : 4150
train acc:  0.703125
train loss:  0.5518257021903992
train gradient:  0.22861053961688013
iteration : 4151
train acc:  0.7734375
train loss:  0.5102356672286987
train gradient:  0.12242616101159921
iteration : 4152
train acc:  0.6640625
train loss:  0.6024782657623291
train gradient:  0.19265778830090585
iteration : 4153
train acc:  0.7265625
train loss:  0.5211048722267151
train gradient:  0.15958426170596263
iteration : 4154
train acc:  0.7265625
train loss:  0.5177532434463501
train gradient:  0.13980889082232023
iteration : 4155
train acc:  0.671875
train loss:  0.5517668128013611
train gradient:  0.17973049524520315
iteration : 4156
train acc:  0.671875
train loss:  0.5693542957305908
train gradient:  0.17147323180152985
iteration : 4157
train acc:  0.703125
train loss:  0.5750213861465454
train gradient:  0.16823107200970344
iteration : 4158
train acc:  0.734375
train loss:  0.47895365953445435
train gradient:  0.1433164793100299
iteration : 4159
train acc:  0.703125
train loss:  0.5005494952201843
train gradient:  0.1392665002894721
iteration : 4160
train acc:  0.734375
train loss:  0.510537326335907
train gradient:  0.16509421904001378
iteration : 4161
train acc:  0.7421875
train loss:  0.4913068413734436
train gradient:  0.1604809815738847
iteration : 4162
train acc:  0.6953125
train loss:  0.5657234787940979
train gradient:  0.18966609061867606
iteration : 4163
train acc:  0.7421875
train loss:  0.49898993968963623
train gradient:  0.2306492345750033
iteration : 4164
train acc:  0.7109375
train loss:  0.5520272254943848
train gradient:  0.1741252021876811
iteration : 4165
train acc:  0.703125
train loss:  0.5382335186004639
train gradient:  0.1778206683448481
iteration : 4166
train acc:  0.75
train loss:  0.5139418840408325
train gradient:  0.19238696235773906
iteration : 4167
train acc:  0.71875
train loss:  0.49808284640312195
train gradient:  0.14405237405164462
iteration : 4168
train acc:  0.7578125
train loss:  0.5464016795158386
train gradient:  0.12186697769694912
iteration : 4169
train acc:  0.6875
train loss:  0.5579336881637573
train gradient:  0.17162625898815986
iteration : 4170
train acc:  0.734375
train loss:  0.5425616502761841
train gradient:  0.15963421203597944
iteration : 4171
train acc:  0.6640625
train loss:  0.5807164907455444
train gradient:  0.18566527975943317
iteration : 4172
train acc:  0.7421875
train loss:  0.4826726019382477
train gradient:  0.1319062666388173
iteration : 4173
train acc:  0.6953125
train loss:  0.5540379881858826
train gradient:  0.16025033781929393
iteration : 4174
train acc:  0.6796875
train loss:  0.5835791826248169
train gradient:  0.20345035193059824
iteration : 4175
train acc:  0.703125
train loss:  0.5253664255142212
train gradient:  0.13644731210931127
iteration : 4176
train acc:  0.7265625
train loss:  0.49011847376823425
train gradient:  0.1860584294698014
iteration : 4177
train acc:  0.75
train loss:  0.5036317110061646
train gradient:  0.13514901720647077
iteration : 4178
train acc:  0.7265625
train loss:  0.498270183801651
train gradient:  0.13635730544363295
iteration : 4179
train acc:  0.71875
train loss:  0.5585563778877258
train gradient:  0.16252193115590613
iteration : 4180
train acc:  0.71875
train loss:  0.4903314411640167
train gradient:  0.17160930837750532
iteration : 4181
train acc:  0.6640625
train loss:  0.5244359970092773
train gradient:  0.1451820821425175
iteration : 4182
train acc:  0.796875
train loss:  0.45127737522125244
train gradient:  0.13129572646689325
iteration : 4183
train acc:  0.78125
train loss:  0.4610510468482971
train gradient:  0.12439500566581078
iteration : 4184
train acc:  0.703125
train loss:  0.5428297519683838
train gradient:  0.13456845372642057
iteration : 4185
train acc:  0.71875
train loss:  0.5212540626525879
train gradient:  0.14752594195130747
iteration : 4186
train acc:  0.78125
train loss:  0.4558917284011841
train gradient:  0.1430647833471722
iteration : 4187
train acc:  0.7109375
train loss:  0.5047751069068909
train gradient:  0.14882326251539252
iteration : 4188
train acc:  0.7421875
train loss:  0.5317583084106445
train gradient:  0.18299535210036322
iteration : 4189
train acc:  0.6953125
train loss:  0.5596624612808228
train gradient:  0.15835837347946863
iteration : 4190
train acc:  0.6640625
train loss:  0.6222347617149353
train gradient:  0.20703873574807438
iteration : 4191
train acc:  0.734375
train loss:  0.465480238199234
train gradient:  0.1158874371754235
iteration : 4192
train acc:  0.7890625
train loss:  0.4543110132217407
train gradient:  0.11830836009375462
iteration : 4193
train acc:  0.6796875
train loss:  0.5381031036376953
train gradient:  0.15197453123753282
iteration : 4194
train acc:  0.6796875
train loss:  0.536543607711792
train gradient:  0.18590927196585932
iteration : 4195
train acc:  0.7734375
train loss:  0.4776259660720825
train gradient:  0.12802771170448762
iteration : 4196
train acc:  0.7578125
train loss:  0.518526554107666
train gradient:  0.12637785518215938
iteration : 4197
train acc:  0.6953125
train loss:  0.5282224416732788
train gradient:  0.17383141208358815
iteration : 4198
train acc:  0.7421875
train loss:  0.4819488823413849
train gradient:  0.13356997925482478
iteration : 4199
train acc:  0.75
train loss:  0.4811519384384155
train gradient:  0.1274537698871797
iteration : 4200
train acc:  0.734375
train loss:  0.5316223502159119
train gradient:  0.1722946259186515
iteration : 4201
train acc:  0.75
train loss:  0.4648545980453491
train gradient:  0.13572298092589652
iteration : 4202
train acc:  0.6953125
train loss:  0.5288987755775452
train gradient:  0.25046774189049326
iteration : 4203
train acc:  0.703125
train loss:  0.5177042484283447
train gradient:  0.18970072197893179
iteration : 4204
train acc:  0.6953125
train loss:  0.5935019254684448
train gradient:  0.165570770099447
iteration : 4205
train acc:  0.71875
train loss:  0.5064620971679688
train gradient:  0.15874086538028148
iteration : 4206
train acc:  0.75
train loss:  0.5185191035270691
train gradient:  0.1852008045288539
iteration : 4207
train acc:  0.7109375
train loss:  0.5336335301399231
train gradient:  0.2147252773109726
iteration : 4208
train acc:  0.7578125
train loss:  0.5386841297149658
train gradient:  0.17361516040735678
iteration : 4209
train acc:  0.765625
train loss:  0.5132448077201843
train gradient:  0.1195170889189612
iteration : 4210
train acc:  0.6484375
train loss:  0.6090494394302368
train gradient:  0.29689094244153075
iteration : 4211
train acc:  0.75
train loss:  0.4941651225090027
train gradient:  0.12910102352372443
iteration : 4212
train acc:  0.7109375
train loss:  0.5076120495796204
train gradient:  0.1510093127876967
iteration : 4213
train acc:  0.7265625
train loss:  0.517566442489624
train gradient:  0.13284410346958667
iteration : 4214
train acc:  0.71875
train loss:  0.5341812372207642
train gradient:  0.14800657052962302
iteration : 4215
train acc:  0.7890625
train loss:  0.44906580448150635
train gradient:  0.11369189538051945
iteration : 4216
train acc:  0.7109375
train loss:  0.505174458026886
train gradient:  0.2605385046672313
iteration : 4217
train acc:  0.7578125
train loss:  0.47568535804748535
train gradient:  0.11185846559201193
iteration : 4218
train acc:  0.75
train loss:  0.5236076712608337
train gradient:  0.17979488497826707
iteration : 4219
train acc:  0.75
train loss:  0.48337194323539734
train gradient:  0.18161046997900243
iteration : 4220
train acc:  0.78125
train loss:  0.43004417419433594
train gradient:  0.0977871162468653
iteration : 4221
train acc:  0.7265625
train loss:  0.49082130193710327
train gradient:  0.15789982023354737
iteration : 4222
train acc:  0.7265625
train loss:  0.5286110639572144
train gradient:  0.1450685617182207
iteration : 4223
train acc:  0.6953125
train loss:  0.5564097762107849
train gradient:  0.17469801383132635
iteration : 4224
train acc:  0.7578125
train loss:  0.48833930492401123
train gradient:  0.18684708791375848
iteration : 4225
train acc:  0.7109375
train loss:  0.5305671095848083
train gradient:  0.18142558793569838
iteration : 4226
train acc:  0.7421875
train loss:  0.5132466554641724
train gradient:  0.17380185861049685
iteration : 4227
train acc:  0.7421875
train loss:  0.5399231314659119
train gradient:  0.16759291022223782
iteration : 4228
train acc:  0.7109375
train loss:  0.49349772930145264
train gradient:  0.1488103373870346
iteration : 4229
train acc:  0.734375
train loss:  0.532823383808136
train gradient:  0.20559644604564437
iteration : 4230
train acc:  0.71875
train loss:  0.5250013470649719
train gradient:  0.14071612987547014
iteration : 4231
train acc:  0.6640625
train loss:  0.5523848533630371
train gradient:  0.16068865661678283
iteration : 4232
train acc:  0.734375
train loss:  0.5445935726165771
train gradient:  0.2529345040485989
iteration : 4233
train acc:  0.7578125
train loss:  0.4996625483036041
train gradient:  0.14551662457659342
iteration : 4234
train acc:  0.75
train loss:  0.5153613686561584
train gradient:  0.13301439736847304
iteration : 4235
train acc:  0.7109375
train loss:  0.5554372072219849
train gradient:  0.22238055254279882
iteration : 4236
train acc:  0.75
train loss:  0.49260979890823364
train gradient:  0.11642101273408444
iteration : 4237
train acc:  0.7578125
train loss:  0.516433835029602
train gradient:  0.1397915656406415
iteration : 4238
train acc:  0.7734375
train loss:  0.47742345929145813
train gradient:  0.13503490235930504
iteration : 4239
train acc:  0.7421875
train loss:  0.5261136293411255
train gradient:  0.22624646987257685
iteration : 4240
train acc:  0.6640625
train loss:  0.6013275384902954
train gradient:  0.29819849124597975
iteration : 4241
train acc:  0.6796875
train loss:  0.5304610729217529
train gradient:  0.14194656347581663
iteration : 4242
train acc:  0.6484375
train loss:  0.6551738977432251
train gradient:  0.23098303476363252
iteration : 4243
train acc:  0.6953125
train loss:  0.5205246210098267
train gradient:  0.16529642768473474
iteration : 4244
train acc:  0.7265625
train loss:  0.5181142687797546
train gradient:  0.15138197222603378
iteration : 4245
train acc:  0.734375
train loss:  0.480566143989563
train gradient:  0.15079249069013578
iteration : 4246
train acc:  0.6953125
train loss:  0.5401877164840698
train gradient:  0.1382940232661184
iteration : 4247
train acc:  0.7890625
train loss:  0.46354392170906067
train gradient:  0.11793659509782499
iteration : 4248
train acc:  0.6875
train loss:  0.5432420969009399
train gradient:  0.135559466801436
iteration : 4249
train acc:  0.6796875
train loss:  0.5453130006790161
train gradient:  0.1808700757130126
iteration : 4250
train acc:  0.6953125
train loss:  0.5614044666290283
train gradient:  0.15706324154875462
iteration : 4251
train acc:  0.7734375
train loss:  0.4809485971927643
train gradient:  0.17777083864522508
iteration : 4252
train acc:  0.734375
train loss:  0.5366708636283875
train gradient:  0.25024943506626096
iteration : 4253
train acc:  0.7265625
train loss:  0.539405345916748
train gradient:  0.13249552630374145
iteration : 4254
train acc:  0.7421875
train loss:  0.5111755132675171
train gradient:  0.1387899192234936
iteration : 4255
train acc:  0.734375
train loss:  0.4938794672489166
train gradient:  0.1596510432657517
iteration : 4256
train acc:  0.7109375
train loss:  0.5499891638755798
train gradient:  0.15849642882485138
iteration : 4257
train acc:  0.703125
train loss:  0.557174801826477
train gradient:  0.15278574495566494
iteration : 4258
train acc:  0.71875
train loss:  0.5177477598190308
train gradient:  0.15479390533044587
iteration : 4259
train acc:  0.6640625
train loss:  0.5834457278251648
train gradient:  0.16928391850013508
iteration : 4260
train acc:  0.8203125
train loss:  0.43241918087005615
train gradient:  0.15697745256689105
iteration : 4261
train acc:  0.6640625
train loss:  0.5800684094429016
train gradient:  0.19202093453793756
iteration : 4262
train acc:  0.7421875
train loss:  0.5446254014968872
train gradient:  0.17843069901151426
iteration : 4263
train acc:  0.7265625
train loss:  0.5139689445495605
train gradient:  0.1261800473664599
iteration : 4264
train acc:  0.703125
train loss:  0.5125926733016968
train gradient:  0.16686128981118398
iteration : 4265
train acc:  0.8203125
train loss:  0.41356343030929565
train gradient:  0.12444347947932963
iteration : 4266
train acc:  0.7578125
train loss:  0.508159875869751
train gradient:  0.14285214287965775
iteration : 4267
train acc:  0.6953125
train loss:  0.5510761737823486
train gradient:  0.2134320885472479
iteration : 4268
train acc:  0.7421875
train loss:  0.5162403583526611
train gradient:  0.1359764795167864
iteration : 4269
train acc:  0.703125
train loss:  0.5416330695152283
train gradient:  0.15590946119016336
iteration : 4270
train acc:  0.7265625
train loss:  0.5820785760879517
train gradient:  0.22152343877600011
iteration : 4271
train acc:  0.6796875
train loss:  0.5666295886039734
train gradient:  0.15181454344094725
iteration : 4272
train acc:  0.6953125
train loss:  0.5580013990402222
train gradient:  0.18453844180539208
iteration : 4273
train acc:  0.7109375
train loss:  0.5381937026977539
train gradient:  0.17732507318969037
iteration : 4274
train acc:  0.7109375
train loss:  0.5265671014785767
train gradient:  0.16126115251771028
iteration : 4275
train acc:  0.7578125
train loss:  0.4498000741004944
train gradient:  0.1016118205952354
iteration : 4276
train acc:  0.7578125
train loss:  0.4812849760055542
train gradient:  0.15596214565032768
iteration : 4277
train acc:  0.6796875
train loss:  0.552888035774231
train gradient:  0.19001311774141733
iteration : 4278
train acc:  0.734375
train loss:  0.5040969848632812
train gradient:  0.1740706038959129
iteration : 4279
train acc:  0.7265625
train loss:  0.5032566785812378
train gradient:  0.15995039880947812
iteration : 4280
train acc:  0.734375
train loss:  0.5067228078842163
train gradient:  0.12631663276528415
iteration : 4281
train acc:  0.7578125
train loss:  0.5170484781265259
train gradient:  0.15070572245211603
iteration : 4282
train acc:  0.65625
train loss:  0.5383362174034119
train gradient:  0.13155457507178347
iteration : 4283
train acc:  0.703125
train loss:  0.5448464155197144
train gradient:  0.14666292349998747
iteration : 4284
train acc:  0.71875
train loss:  0.5023928284645081
train gradient:  0.1329373697152322
iteration : 4285
train acc:  0.7109375
train loss:  0.5359537601470947
train gradient:  0.1277915948725198
iteration : 4286
train acc:  0.703125
train loss:  0.5705955624580383
train gradient:  0.21249184936596338
iteration : 4287
train acc:  0.734375
train loss:  0.5246702432632446
train gradient:  0.19486432314939445
iteration : 4288
train acc:  0.7265625
train loss:  0.5187318325042725
train gradient:  0.163832050850528
iteration : 4289
train acc:  0.7265625
train loss:  0.5470156669616699
train gradient:  0.1595058794331513
iteration : 4290
train acc:  0.6953125
train loss:  0.533422589302063
train gradient:  0.17279294564512393
iteration : 4291
train acc:  0.6484375
train loss:  0.5946625471115112
train gradient:  0.19922165620603122
iteration : 4292
train acc:  0.7734375
train loss:  0.4857444763183594
train gradient:  0.1515462039641732
iteration : 4293
train acc:  0.6953125
train loss:  0.5068866014480591
train gradient:  0.14409281781533229
iteration : 4294
train acc:  0.6640625
train loss:  0.6259860396385193
train gradient:  0.23135371407180175
iteration : 4295
train acc:  0.7421875
train loss:  0.5467171669006348
train gradient:  0.16146421206056494
iteration : 4296
train acc:  0.6640625
train loss:  0.5579994320869446
train gradient:  0.15175404443764404
iteration : 4297
train acc:  0.7265625
train loss:  0.5564546585083008
train gradient:  0.18136961133356924
iteration : 4298
train acc:  0.7265625
train loss:  0.4904074966907501
train gradient:  0.11423089898486366
iteration : 4299
train acc:  0.75
train loss:  0.4755437970161438
train gradient:  0.1290516612511483
iteration : 4300
train acc:  0.6875
train loss:  0.5636634826660156
train gradient:  0.17415489431380834
iteration : 4301
train acc:  0.7578125
train loss:  0.4761143624782562
train gradient:  0.11559605126060023
iteration : 4302
train acc:  0.7734375
train loss:  0.5097626447677612
train gradient:  0.18777685324228965
iteration : 4303
train acc:  0.703125
train loss:  0.535647988319397
train gradient:  0.1586015647802111
iteration : 4304
train acc:  0.7734375
train loss:  0.46217429637908936
train gradient:  0.11341427155482267
iteration : 4305
train acc:  0.7109375
train loss:  0.542722761631012
train gradient:  0.18841203291738606
iteration : 4306
train acc:  0.7109375
train loss:  0.5384990572929382
train gradient:  0.16856101960232217
iteration : 4307
train acc:  0.7265625
train loss:  0.4918544292449951
train gradient:  0.1411030043037267
iteration : 4308
train acc:  0.6796875
train loss:  0.5509011745452881
train gradient:  0.16742952341764494
iteration : 4309
train acc:  0.796875
train loss:  0.4863610863685608
train gradient:  0.1497050495971599
iteration : 4310
train acc:  0.6875
train loss:  0.5598418712615967
train gradient:  0.18209923005863404
iteration : 4311
train acc:  0.7578125
train loss:  0.49986201524734497
train gradient:  0.16845520264251046
iteration : 4312
train acc:  0.6875
train loss:  0.5787975192070007
train gradient:  0.1613354385576927
iteration : 4313
train acc:  0.78125
train loss:  0.4515056014060974
train gradient:  0.1735979917264907
iteration : 4314
train acc:  0.734375
train loss:  0.4968893527984619
train gradient:  0.14522532308418706
iteration : 4315
train acc:  0.7578125
train loss:  0.5161353349685669
train gradient:  0.16563321412788656
iteration : 4316
train acc:  0.703125
train loss:  0.4901624917984009
train gradient:  0.1262392705506991
iteration : 4317
train acc:  0.7109375
train loss:  0.5226390361785889
train gradient:  0.15384493775892066
iteration : 4318
train acc:  0.75
train loss:  0.5186440944671631
train gradient:  0.12806715840376265
iteration : 4319
train acc:  0.7265625
train loss:  0.5217983722686768
train gradient:  0.2008333223422203
iteration : 4320
train acc:  0.703125
train loss:  0.5775167942047119
train gradient:  0.14691552960624896
iteration : 4321
train acc:  0.7421875
train loss:  0.529441237449646
train gradient:  0.16343818986020847
iteration : 4322
train acc:  0.6953125
train loss:  0.5416593551635742
train gradient:  0.14971381631086778
iteration : 4323
train acc:  0.75
train loss:  0.5245877504348755
train gradient:  0.19081854995729036
iteration : 4324
train acc:  0.7421875
train loss:  0.5045943260192871
train gradient:  0.11720662715687064
iteration : 4325
train acc:  0.734375
train loss:  0.5467754602432251
train gradient:  0.2091891578904464
iteration : 4326
train acc:  0.703125
train loss:  0.5296419262886047
train gradient:  0.17035574282645513
iteration : 4327
train acc:  0.7109375
train loss:  0.5376294255256653
train gradient:  0.17657435224235246
iteration : 4328
train acc:  0.78125
train loss:  0.4509645104408264
train gradient:  0.11594003486013425
iteration : 4329
train acc:  0.765625
train loss:  0.47162100672721863
train gradient:  0.15918224900985517
iteration : 4330
train acc:  0.765625
train loss:  0.5159721374511719
train gradient:  0.16632546585213406
iteration : 4331
train acc:  0.765625
train loss:  0.4864526391029358
train gradient:  0.1639146053413259
iteration : 4332
train acc:  0.7109375
train loss:  0.499161958694458
train gradient:  0.15037877991187631
iteration : 4333
train acc:  0.65625
train loss:  0.568454384803772
train gradient:  0.2009893312295763
iteration : 4334
train acc:  0.671875
train loss:  0.5330475568771362
train gradient:  0.13353252104569985
iteration : 4335
train acc:  0.7734375
train loss:  0.47138723731040955
train gradient:  0.1460155207868019
iteration : 4336
train acc:  0.78125
train loss:  0.4911476969718933
train gradient:  0.1483417567199167
iteration : 4337
train acc:  0.7578125
train loss:  0.49433112144470215
train gradient:  0.12520907858600677
iteration : 4338
train acc:  0.734375
train loss:  0.5214922428131104
train gradient:  0.14432558289805958
iteration : 4339
train acc:  0.796875
train loss:  0.4581725001335144
train gradient:  0.13505621620757774
iteration : 4340
train acc:  0.734375
train loss:  0.5423181056976318
train gradient:  0.1311458807991257
iteration : 4341
train acc:  0.7109375
train loss:  0.550506055355072
train gradient:  0.22252922045581766
iteration : 4342
train acc:  0.7421875
train loss:  0.519079327583313
train gradient:  0.1841451084729826
iteration : 4343
train acc:  0.671875
train loss:  0.5477168560028076
train gradient:  0.19254864247049014
iteration : 4344
train acc:  0.609375
train loss:  0.5894829630851746
train gradient:  0.22418357505419417
iteration : 4345
train acc:  0.65625
train loss:  0.6409543752670288
train gradient:  0.23293220719257057
iteration : 4346
train acc:  0.671875
train loss:  0.5755720138549805
train gradient:  0.283749603496756
iteration : 4347
train acc:  0.734375
train loss:  0.4905255436897278
train gradient:  0.15577057243438386
iteration : 4348
train acc:  0.7578125
train loss:  0.5112456679344177
train gradient:  0.13141191708261094
iteration : 4349
train acc:  0.7265625
train loss:  0.5245336294174194
train gradient:  0.148422084335864
iteration : 4350
train acc:  0.734375
train loss:  0.49924999475479126
train gradient:  0.13084630873537545
iteration : 4351
train acc:  0.7421875
train loss:  0.5039591789245605
train gradient:  0.15683716137164427
iteration : 4352
train acc:  0.6953125
train loss:  0.5409383773803711
train gradient:  0.14380314463292293
iteration : 4353
train acc:  0.6875
train loss:  0.530723512172699
train gradient:  0.12742700007709748
iteration : 4354
train acc:  0.765625
train loss:  0.519287109375
train gradient:  0.19432069070804886
iteration : 4355
train acc:  0.75
train loss:  0.554101288318634
train gradient:  0.16613168609533668
iteration : 4356
train acc:  0.7265625
train loss:  0.5594102144241333
train gradient:  0.16183067127232914
iteration : 4357
train acc:  0.7734375
train loss:  0.47607535123825073
train gradient:  0.11612650558785227
iteration : 4358
train acc:  0.734375
train loss:  0.5124105215072632
train gradient:  0.1211866024899697
iteration : 4359
train acc:  0.6953125
train loss:  0.5617406368255615
train gradient:  0.1347233883839998
iteration : 4360
train acc:  0.71875
train loss:  0.5493748188018799
train gradient:  0.17457570982555376
iteration : 4361
train acc:  0.6796875
train loss:  0.6031590700149536
train gradient:  0.22375952754055226
iteration : 4362
train acc:  0.703125
train loss:  0.5597115755081177
train gradient:  0.1777845182240742
iteration : 4363
train acc:  0.7578125
train loss:  0.5083738565444946
train gradient:  0.17254311760168756
iteration : 4364
train acc:  0.6796875
train loss:  0.5541982054710388
train gradient:  0.2262208806080503
iteration : 4365
train acc:  0.78125
train loss:  0.43662920594215393
train gradient:  0.13049297665131668
iteration : 4366
train acc:  0.7421875
train loss:  0.47948580980300903
train gradient:  0.1376125377958018
iteration : 4367
train acc:  0.703125
train loss:  0.5716648697853088
train gradient:  0.1752911780585532
iteration : 4368
train acc:  0.6640625
train loss:  0.5507476329803467
train gradient:  0.16200242706282175
iteration : 4369
train acc:  0.7421875
train loss:  0.5152140259742737
train gradient:  0.1199496459168745
iteration : 4370
train acc:  0.78125
train loss:  0.4932800531387329
train gradient:  0.14220050755995844
iteration : 4371
train acc:  0.703125
train loss:  0.547818660736084
train gradient:  0.19521115958954985
iteration : 4372
train acc:  0.8125
train loss:  0.4928905963897705
train gradient:  0.15166359967270066
iteration : 4373
train acc:  0.6875
train loss:  0.5394024848937988
train gradient:  0.16797230873240862
iteration : 4374
train acc:  0.6953125
train loss:  0.5992263555526733
train gradient:  0.2861648487662184
iteration : 4375
train acc:  0.6796875
train loss:  0.597730278968811
train gradient:  0.17577120600430934
iteration : 4376
train acc:  0.765625
train loss:  0.5347499847412109
train gradient:  0.20539907646894967
iteration : 4377
train acc:  0.703125
train loss:  0.536760687828064
train gradient:  0.20684908014051068
iteration : 4378
train acc:  0.703125
train loss:  0.5076244473457336
train gradient:  0.12209286703621477
iteration : 4379
train acc:  0.734375
train loss:  0.5152125954627991
train gradient:  0.14702493972413627
iteration : 4380
train acc:  0.7734375
train loss:  0.4538600444793701
train gradient:  0.1033034822429914
iteration : 4381
train acc:  0.75
train loss:  0.5037201046943665
train gradient:  0.14760336739220772
iteration : 4382
train acc:  0.7265625
train loss:  0.5391084551811218
train gradient:  0.17250696576644975
iteration : 4383
train acc:  0.765625
train loss:  0.4819234013557434
train gradient:  0.1903773553360008
iteration : 4384
train acc:  0.6796875
train loss:  0.559481143951416
train gradient:  0.17619019794276447
iteration : 4385
train acc:  0.6640625
train loss:  0.5695447325706482
train gradient:  0.1812912449977656
iteration : 4386
train acc:  0.6796875
train loss:  0.5357078313827515
train gradient:  0.1749114789846813
iteration : 4387
train acc:  0.7421875
train loss:  0.5129944086074829
train gradient:  0.13941327982880225
iteration : 4388
train acc:  0.7421875
train loss:  0.5139530897140503
train gradient:  0.1818414670399075
iteration : 4389
train acc:  0.7578125
train loss:  0.4968095123767853
train gradient:  0.13979908003343286
iteration : 4390
train acc:  0.703125
train loss:  0.5637640953063965
train gradient:  0.15179658181078517
iteration : 4391
train acc:  0.6953125
train loss:  0.5115677714347839
train gradient:  0.16370693878610373
iteration : 4392
train acc:  0.734375
train loss:  0.4795285761356354
train gradient:  0.15398459221518004
iteration : 4393
train acc:  0.78125
train loss:  0.4477449059486389
train gradient:  0.1204886907861349
iteration : 4394
train acc:  0.6875
train loss:  0.5718661546707153
train gradient:  0.17556323361299153
iteration : 4395
train acc:  0.703125
train loss:  0.5478544235229492
train gradient:  0.16168779505868172
iteration : 4396
train acc:  0.75
train loss:  0.5047847032546997
train gradient:  0.14982288543202155
iteration : 4397
train acc:  0.734375
train loss:  0.5045555830001831
train gradient:  0.18182977477005324
iteration : 4398
train acc:  0.7578125
train loss:  0.5126965045928955
train gradient:  0.15552106789160325
iteration : 4399
train acc:  0.6640625
train loss:  0.5639625787734985
train gradient:  0.17815586464540062
iteration : 4400
train acc:  0.703125
train loss:  0.5447177886962891
train gradient:  0.20728236660625948
iteration : 4401
train acc:  0.7578125
train loss:  0.48523104190826416
train gradient:  0.12078684701110724
iteration : 4402
train acc:  0.765625
train loss:  0.4770982265472412
train gradient:  0.12033724218892215
iteration : 4403
train acc:  0.765625
train loss:  0.4889877438545227
train gradient:  0.14963502757096778
iteration : 4404
train acc:  0.703125
train loss:  0.5378468632698059
train gradient:  0.1766639853105653
iteration : 4405
train acc:  0.75
train loss:  0.4795376658439636
train gradient:  0.14528892144771077
iteration : 4406
train acc:  0.7578125
train loss:  0.4959946870803833
train gradient:  0.1438217724370892
iteration : 4407
train acc:  0.7109375
train loss:  0.551957368850708
train gradient:  0.15595836855581097
iteration : 4408
train acc:  0.7578125
train loss:  0.5086094737052917
train gradient:  0.12052576085252092
iteration : 4409
train acc:  0.71875
train loss:  0.5125565528869629
train gradient:  0.14240543999855737
iteration : 4410
train acc:  0.71875
train loss:  0.523173987865448
train gradient:  0.17307047106070675
iteration : 4411
train acc:  0.75
train loss:  0.5045728087425232
train gradient:  0.12019495309790992
iteration : 4412
train acc:  0.7578125
train loss:  0.4698667526245117
train gradient:  0.11806120756297334
iteration : 4413
train acc:  0.7734375
train loss:  0.48745647072792053
train gradient:  0.13678514881598458
iteration : 4414
train acc:  0.6796875
train loss:  0.5935045480728149
train gradient:  0.17995334163146398
iteration : 4415
train acc:  0.734375
train loss:  0.5216347575187683
train gradient:  0.1658377173994648
iteration : 4416
train acc:  0.75
train loss:  0.5370197296142578
train gradient:  0.13338779724822505
iteration : 4417
train acc:  0.7265625
train loss:  0.516604483127594
train gradient:  0.13951321627293994
iteration : 4418
train acc:  0.765625
train loss:  0.4695345461368561
train gradient:  0.16420042344723185
iteration : 4419
train acc:  0.65625
train loss:  0.5591475963592529
train gradient:  0.1861833084781408
iteration : 4420
train acc:  0.734375
train loss:  0.5475345849990845
train gradient:  0.18457709949551754
iteration : 4421
train acc:  0.7890625
train loss:  0.4899104833602905
train gradient:  0.13537281286938224
iteration : 4422
train acc:  0.7421875
train loss:  0.4790630340576172
train gradient:  0.1411605995835332
iteration : 4423
train acc:  0.7265625
train loss:  0.4941983222961426
train gradient:  0.1144067459179293
iteration : 4424
train acc:  0.703125
train loss:  0.5666395425796509
train gradient:  0.1569881694429193
iteration : 4425
train acc:  0.734375
train loss:  0.5110100507736206
train gradient:  0.12319459105155893
iteration : 4426
train acc:  0.7109375
train loss:  0.5304034948348999
train gradient:  0.17660553562320358
iteration : 4427
train acc:  0.7578125
train loss:  0.505484402179718
train gradient:  0.14448101969882143
iteration : 4428
train acc:  0.6875
train loss:  0.5824739933013916
train gradient:  0.20778941889186991
iteration : 4429
train acc:  0.75
train loss:  0.4997827410697937
train gradient:  0.17260241814148225
iteration : 4430
train acc:  0.75
train loss:  0.4819299578666687
train gradient:  0.1149758791486965
iteration : 4431
train acc:  0.765625
train loss:  0.47595784068107605
train gradient:  0.1407694992410504
iteration : 4432
train acc:  0.7109375
train loss:  0.5200397372245789
train gradient:  0.20113133930771854
iteration : 4433
train acc:  0.6328125
train loss:  0.5966875553131104
train gradient:  0.18957182348210666
iteration : 4434
train acc:  0.734375
train loss:  0.5212513208389282
train gradient:  0.15893478272787412
iteration : 4435
train acc:  0.7109375
train loss:  0.4899108111858368
train gradient:  0.13834818712295033
iteration : 4436
train acc:  0.7734375
train loss:  0.4642834961414337
train gradient:  0.18025529459145842
iteration : 4437
train acc:  0.71875
train loss:  0.5144997239112854
train gradient:  0.11238424638487837
iteration : 4438
train acc:  0.765625
train loss:  0.4850023090839386
train gradient:  0.11193163767118597
iteration : 4439
train acc:  0.703125
train loss:  0.5293378829956055
train gradient:  0.1418415500057799
iteration : 4440
train acc:  0.75
train loss:  0.4875527620315552
train gradient:  0.13321157062567002
iteration : 4441
train acc:  0.765625
train loss:  0.48103266954421997
train gradient:  0.16315964397904514
iteration : 4442
train acc:  0.6640625
train loss:  0.5469177961349487
train gradient:  0.17504561979349686
iteration : 4443
train acc:  0.7109375
train loss:  0.5356322526931763
train gradient:  0.1695129250584529
iteration : 4444
train acc:  0.7421875
train loss:  0.48956868052482605
train gradient:  0.14564071879551588
iteration : 4445
train acc:  0.7578125
train loss:  0.5138839483261108
train gradient:  0.15345618700417796
iteration : 4446
train acc:  0.7578125
train loss:  0.5042973756790161
train gradient:  0.1405340407455598
iteration : 4447
train acc:  0.71875
train loss:  0.5335565209388733
train gradient:  0.13217183211911282
iteration : 4448
train acc:  0.7265625
train loss:  0.46925994753837585
train gradient:  0.15228813873431543
iteration : 4449
train acc:  0.7421875
train loss:  0.49600422382354736
train gradient:  0.17227487935955557
iteration : 4450
train acc:  0.703125
train loss:  0.5138490200042725
train gradient:  0.18155913023465567
iteration : 4451
train acc:  0.6875
train loss:  0.5509408116340637
train gradient:  0.1864409666823134
iteration : 4452
train acc:  0.6953125
train loss:  0.5101695656776428
train gradient:  0.15529506826812878
iteration : 4453
train acc:  0.7578125
train loss:  0.5323173999786377
train gradient:  0.164314960904742
iteration : 4454
train acc:  0.703125
train loss:  0.5205795764923096
train gradient:  0.17667290796839713
iteration : 4455
train acc:  0.734375
train loss:  0.4717232584953308
train gradient:  0.1335920359004824
iteration : 4456
train acc:  0.6953125
train loss:  0.5140438675880432
train gradient:  0.15227529267198409
iteration : 4457
train acc:  0.703125
train loss:  0.5802722573280334
train gradient:  0.1919510229325776
iteration : 4458
train acc:  0.6640625
train loss:  0.5794556736946106
train gradient:  0.248007625055527
iteration : 4459
train acc:  0.7421875
train loss:  0.5226527452468872
train gradient:  0.16956519696249475
iteration : 4460
train acc:  0.7734375
train loss:  0.5214566588401794
train gradient:  0.13994510707202598
iteration : 4461
train acc:  0.7734375
train loss:  0.4977513253688812
train gradient:  0.12364080028993582
iteration : 4462
train acc:  0.7265625
train loss:  0.5244008302688599
train gradient:  0.17051547677551632
iteration : 4463
train acc:  0.65625
train loss:  0.5769438743591309
train gradient:  0.16222247555094185
iteration : 4464
train acc:  0.734375
train loss:  0.49546027183532715
train gradient:  0.1348050763343181
iteration : 4465
train acc:  0.6875
train loss:  0.5599082112312317
train gradient:  0.1805230859845373
iteration : 4466
train acc:  0.796875
train loss:  0.44115883111953735
train gradient:  0.1326806576721966
iteration : 4467
train acc:  0.7890625
train loss:  0.4684101939201355
train gradient:  0.1645902489527084
iteration : 4468
train acc:  0.734375
train loss:  0.5606733560562134
train gradient:  0.17521612176204548
iteration : 4469
train acc:  0.6796875
train loss:  0.5470696687698364
train gradient:  0.15120594769823573
iteration : 4470
train acc:  0.734375
train loss:  0.48143088817596436
train gradient:  0.1246653359921688
iteration : 4471
train acc:  0.703125
train loss:  0.5225982069969177
train gradient:  0.1302248515063777
iteration : 4472
train acc:  0.6796875
train loss:  0.567974328994751
train gradient:  0.18005937053245807
iteration : 4473
train acc:  0.703125
train loss:  0.47948646545410156
train gradient:  0.13962229536811505
iteration : 4474
train acc:  0.71875
train loss:  0.522232711315155
train gradient:  0.2685751543575789
iteration : 4475
train acc:  0.7578125
train loss:  0.4829421639442444
train gradient:  0.14043502792934784
iteration : 4476
train acc:  0.7890625
train loss:  0.46360093355178833
train gradient:  0.10949321226791078
iteration : 4477
train acc:  0.7578125
train loss:  0.567132830619812
train gradient:  0.17253906300032917
iteration : 4478
train acc:  0.7265625
train loss:  0.5012083053588867
train gradient:  0.12669363421743143
iteration : 4479
train acc:  0.75
train loss:  0.44348418712615967
train gradient:  0.12066865656554811
iteration : 4480
train acc:  0.7265625
train loss:  0.5520957708358765
train gradient:  0.17981813086908166
iteration : 4481
train acc:  0.6953125
train loss:  0.579529881477356
train gradient:  0.1990398469483884
iteration : 4482
train acc:  0.7578125
train loss:  0.515191912651062
train gradient:  0.16584502250309602
iteration : 4483
train acc:  0.7265625
train loss:  0.5833457112312317
train gradient:  0.21813130901240352
iteration : 4484
train acc:  0.6796875
train loss:  0.5650577545166016
train gradient:  0.16714970884891667
iteration : 4485
train acc:  0.7578125
train loss:  0.47479134798049927
train gradient:  0.16325151247864222
iteration : 4486
train acc:  0.6875
train loss:  0.5499676465988159
train gradient:  0.1787905604528015
iteration : 4487
train acc:  0.8046875
train loss:  0.43743669986724854
train gradient:  0.13121191033003582
iteration : 4488
train acc:  0.7734375
train loss:  0.4624481201171875
train gradient:  0.12445088080740166
iteration : 4489
train acc:  0.640625
train loss:  0.6131786704063416
train gradient:  0.17296073263492892
iteration : 4490
train acc:  0.734375
train loss:  0.5112683773040771
train gradient:  0.1790085676112963
iteration : 4491
train acc:  0.734375
train loss:  0.553986668586731
train gradient:  0.18161474801939198
iteration : 4492
train acc:  0.734375
train loss:  0.4968933165073395
train gradient:  0.19552842015355526
iteration : 4493
train acc:  0.7421875
train loss:  0.502441942691803
train gradient:  0.13406235772218844
iteration : 4494
train acc:  0.703125
train loss:  0.5593458414077759
train gradient:  0.20679959712716645
iteration : 4495
train acc:  0.78125
train loss:  0.4485551118850708
train gradient:  0.13934491281879846
iteration : 4496
train acc:  0.7421875
train loss:  0.5386837124824524
train gradient:  0.17827762094019686
iteration : 4497
train acc:  0.6953125
train loss:  0.5788272619247437
train gradient:  0.21214078816121817
iteration : 4498
train acc:  0.734375
train loss:  0.5119478106498718
train gradient:  0.1904327409613259
iteration : 4499
train acc:  0.765625
train loss:  0.4855247139930725
train gradient:  0.12914483682097588
iteration : 4500
train acc:  0.7265625
train loss:  0.48272624611854553
train gradient:  0.11010288032787498
iteration : 4501
train acc:  0.734375
train loss:  0.47572648525238037
train gradient:  0.12617928929600827
iteration : 4502
train acc:  0.75
train loss:  0.5128811597824097
train gradient:  0.1581332626070434
iteration : 4503
train acc:  0.8125
train loss:  0.4523226022720337
train gradient:  0.10688353646765351
iteration : 4504
train acc:  0.734375
train loss:  0.5245428085327148
train gradient:  0.18240233031306607
iteration : 4505
train acc:  0.7890625
train loss:  0.5183645486831665
train gradient:  0.1641135396265159
iteration : 4506
train acc:  0.75
train loss:  0.49639517068862915
train gradient:  0.13467194483367298
iteration : 4507
train acc:  0.7578125
train loss:  0.5352709293365479
train gradient:  0.18109367136479645
iteration : 4508
train acc:  0.71875
train loss:  0.5174409747123718
train gradient:  0.1279419772429427
iteration : 4509
train acc:  0.7421875
train loss:  0.4927295446395874
train gradient:  0.15043746481204495
iteration : 4510
train acc:  0.7109375
train loss:  0.5340437293052673
train gradient:  0.16450078595432838
iteration : 4511
train acc:  0.6796875
train loss:  0.5429951548576355
train gradient:  0.15935370800275
iteration : 4512
train acc:  0.8125
train loss:  0.47349071502685547
train gradient:  0.20231746535308343
iteration : 4513
train acc:  0.71875
train loss:  0.5753521919250488
train gradient:  0.15547908700525476
iteration : 4514
train acc:  0.75
train loss:  0.4978472888469696
train gradient:  0.13014943326386227
iteration : 4515
train acc:  0.7421875
train loss:  0.48615121841430664
train gradient:  0.1656613847670242
iteration : 4516
train acc:  0.6796875
train loss:  0.5747231245040894
train gradient:  0.157100736304717
iteration : 4517
train acc:  0.703125
train loss:  0.5197947025299072
train gradient:  0.14254961489561324
iteration : 4518
train acc:  0.6640625
train loss:  0.5575729012489319
train gradient:  0.16894395810302998
iteration : 4519
train acc:  0.71875
train loss:  0.5336828231811523
train gradient:  0.15183290766559226
iteration : 4520
train acc:  0.65625
train loss:  0.558355987071991
train gradient:  0.24998885591644146
iteration : 4521
train acc:  0.6796875
train loss:  0.5436999797821045
train gradient:  0.1600460739638796
iteration : 4522
train acc:  0.75
train loss:  0.48250341415405273
train gradient:  0.15774403973261394
iteration : 4523
train acc:  0.7265625
train loss:  0.536912202835083
train gradient:  0.1990372744150526
iteration : 4524
train acc:  0.7265625
train loss:  0.494486927986145
train gradient:  0.11779902973406382
iteration : 4525
train acc:  0.7421875
train loss:  0.5323265194892883
train gradient:  0.20491978333464678
iteration : 4526
train acc:  0.71875
train loss:  0.48452141880989075
train gradient:  0.17893163933638523
iteration : 4527
train acc:  0.8046875
train loss:  0.46902593970298767
train gradient:  0.13247671138398842
iteration : 4528
train acc:  0.734375
train loss:  0.5160621404647827
train gradient:  0.15791141466048908
iteration : 4529
train acc:  0.671875
train loss:  0.5640785694122314
train gradient:  0.21911911641247545
iteration : 4530
train acc:  0.765625
train loss:  0.5336829423904419
train gradient:  0.1835208596466575
iteration : 4531
train acc:  0.7734375
train loss:  0.4847182035446167
train gradient:  0.14985508204681972
iteration : 4532
train acc:  0.7578125
train loss:  0.48550188541412354
train gradient:  0.13789048209156218
iteration : 4533
train acc:  0.734375
train loss:  0.5230655670166016
train gradient:  0.1554486886877115
iteration : 4534
train acc:  0.765625
train loss:  0.4621753692626953
train gradient:  0.13584993735306933
iteration : 4535
train acc:  0.7109375
train loss:  0.5536036491394043
train gradient:  0.2162572726137876
iteration : 4536
train acc:  0.75
train loss:  0.5102781057357788
train gradient:  0.1879739434790073
iteration : 4537
train acc:  0.6875
train loss:  0.5703594088554382
train gradient:  0.1658378396179151
iteration : 4538
train acc:  0.7421875
train loss:  0.488799124956131
train gradient:  0.21536197114698397
iteration : 4539
train acc:  0.7421875
train loss:  0.5522412061691284
train gradient:  0.22468667181953994
iteration : 4540
train acc:  0.7890625
train loss:  0.4989674687385559
train gradient:  0.1306900858877838
iteration : 4541
train acc:  0.765625
train loss:  0.47208020091056824
train gradient:  0.1460359009427754
iteration : 4542
train acc:  0.671875
train loss:  0.5390982031822205
train gradient:  0.15442167087605452
iteration : 4543
train acc:  0.765625
train loss:  0.47099190950393677
train gradient:  0.12165382078869495
iteration : 4544
train acc:  0.78125
train loss:  0.4419071674346924
train gradient:  0.1185841105747456
iteration : 4545
train acc:  0.734375
train loss:  0.4978010952472687
train gradient:  0.14377856709751857
iteration : 4546
train acc:  0.75
train loss:  0.5443334579467773
train gradient:  0.1678050182073133
iteration : 4547
train acc:  0.765625
train loss:  0.45490917563438416
train gradient:  0.1478960149529588
iteration : 4548
train acc:  0.8046875
train loss:  0.4065384268760681
train gradient:  0.11233618180642083
iteration : 4549
train acc:  0.7578125
train loss:  0.5194168090820312
train gradient:  0.13780177990051623
iteration : 4550
train acc:  0.6796875
train loss:  0.5371868014335632
train gradient:  0.17058053680887042
iteration : 4551
train acc:  0.65625
train loss:  0.6076537370681763
train gradient:  0.16208870062979705
iteration : 4552
train acc:  0.8125
train loss:  0.4228067398071289
train gradient:  0.1487369909095328
iteration : 4553
train acc:  0.765625
train loss:  0.513160228729248
train gradient:  0.18103353634032537
iteration : 4554
train acc:  0.7265625
train loss:  0.5302214622497559
train gradient:  0.18234110361851508
iteration : 4555
train acc:  0.78125
train loss:  0.47167983651161194
train gradient:  0.1324963036369443
iteration : 4556
train acc:  0.7421875
train loss:  0.5107964277267456
train gradient:  0.16394082017131129
iteration : 4557
train acc:  0.625
train loss:  0.5874111652374268
train gradient:  0.1987356707668115
iteration : 4558
train acc:  0.71875
train loss:  0.5444971919059753
train gradient:  0.18304510263781862
iteration : 4559
train acc:  0.6875
train loss:  0.5559200048446655
train gradient:  0.21021691364721257
iteration : 4560
train acc:  0.765625
train loss:  0.46676111221313477
train gradient:  0.13176959575008507
iteration : 4561
train acc:  0.78125
train loss:  0.46196213364601135
train gradient:  0.13139343120051505
iteration : 4562
train acc:  0.7109375
train loss:  0.5426363945007324
train gradient:  0.19690274557706078
iteration : 4563
train acc:  0.765625
train loss:  0.4618573486804962
train gradient:  0.131600682198523
iteration : 4564
train acc:  0.7421875
train loss:  0.5163192749023438
train gradient:  0.17274074674900464
iteration : 4565
train acc:  0.734375
train loss:  0.4957129657268524
train gradient:  0.15725144593685575
iteration : 4566
train acc:  0.7734375
train loss:  0.4943814277648926
train gradient:  0.12567121646147314
iteration : 4567
train acc:  0.7109375
train loss:  0.4941752254962921
train gradient:  0.13297895374086527
iteration : 4568
train acc:  0.71875
train loss:  0.5430294871330261
train gradient:  0.14414015041835332
iteration : 4569
train acc:  0.71875
train loss:  0.5595026612281799
train gradient:  0.21346082036957215
iteration : 4570
train acc:  0.7421875
train loss:  0.5329927802085876
train gradient:  0.2942107845586447
iteration : 4571
train acc:  0.765625
train loss:  0.49592238664627075
train gradient:  0.14967207226567775
iteration : 4572
train acc:  0.7421875
train loss:  0.48602771759033203
train gradient:  0.12802655459398787
iteration : 4573
train acc:  0.734375
train loss:  0.5126017332077026
train gradient:  0.14456009119983582
iteration : 4574
train acc:  0.7890625
train loss:  0.44976550340652466
train gradient:  0.14221511926909622
iteration : 4575
train acc:  0.734375
train loss:  0.5268456339836121
train gradient:  0.1484263215737754
iteration : 4576
train acc:  0.75
train loss:  0.5283005237579346
train gradient:  0.18008474789808698
iteration : 4577
train acc:  0.765625
train loss:  0.5203704833984375
train gradient:  0.16313549585271825
iteration : 4578
train acc:  0.765625
train loss:  0.5146698951721191
train gradient:  0.15488643670362606
iteration : 4579
train acc:  0.71875
train loss:  0.5413867831230164
train gradient:  0.18408168503653177
iteration : 4580
train acc:  0.71875
train loss:  0.5211122035980225
train gradient:  0.19387924738846196
iteration : 4581
train acc:  0.6875
train loss:  0.6060681343078613
train gradient:  0.2276660621481904
iteration : 4582
train acc:  0.6875
train loss:  0.4871501922607422
train gradient:  0.14156110929875607
iteration : 4583
train acc:  0.7734375
train loss:  0.4565790891647339
train gradient:  0.1003088843094697
iteration : 4584
train acc:  0.71875
train loss:  0.575325608253479
train gradient:  0.1554202556515793
iteration : 4585
train acc:  0.6796875
train loss:  0.5610384345054626
train gradient:  0.1970672440736302
iteration : 4586
train acc:  0.734375
train loss:  0.5211635231971741
train gradient:  0.16358325668272566
iteration : 4587
train acc:  0.71875
train loss:  0.493419349193573
train gradient:  0.18376335414019188
iteration : 4588
train acc:  0.734375
train loss:  0.5271261930465698
train gradient:  0.15785767242650295
iteration : 4589
train acc:  0.6953125
train loss:  0.5596284866333008
train gradient:  0.17633532216698897
iteration : 4590
train acc:  0.6875
train loss:  0.5951725244522095
train gradient:  0.1595592841013272
iteration : 4591
train acc:  0.6796875
train loss:  0.5348795056343079
train gradient:  0.17865544503348774
iteration : 4592
train acc:  0.7421875
train loss:  0.5514348745346069
train gradient:  0.21881635205663302
iteration : 4593
train acc:  0.75
train loss:  0.5283083915710449
train gradient:  0.1746341191662138
iteration : 4594
train acc:  0.71875
train loss:  0.5702013969421387
train gradient:  0.14859669272310588
iteration : 4595
train acc:  0.75
train loss:  0.5166866779327393
train gradient:  0.13895508668937673
iteration : 4596
train acc:  0.6640625
train loss:  0.5801107287406921
train gradient:  0.2010822785553914
iteration : 4597
train acc:  0.6953125
train loss:  0.4991722106933594
train gradient:  0.11771179808822535
iteration : 4598
train acc:  0.75
train loss:  0.49636808037757874
train gradient:  0.15024650253994198
iteration : 4599
train acc:  0.7265625
train loss:  0.5292422771453857
train gradient:  0.14027602599886413
iteration : 4600
train acc:  0.7109375
train loss:  0.5244481563568115
train gradient:  0.15248257564129608
iteration : 4601
train acc:  0.6875
train loss:  0.560035228729248
train gradient:  0.1783763910613187
iteration : 4602
train acc:  0.6953125
train loss:  0.5509361028671265
train gradient:  0.1389701510621178
iteration : 4603
train acc:  0.7578125
train loss:  0.5203812122344971
train gradient:  0.19289264553934177
iteration : 4604
train acc:  0.6640625
train loss:  0.5649893879890442
train gradient:  0.18518812078237423
iteration : 4605
train acc:  0.7109375
train loss:  0.5183379054069519
train gradient:  0.15558574084380142
iteration : 4606
train acc:  0.8203125
train loss:  0.4358648657798767
train gradient:  0.12180578722030953
iteration : 4607
train acc:  0.6875
train loss:  0.555395245552063
train gradient:  0.15956258091495276
iteration : 4608
train acc:  0.703125
train loss:  0.5340182781219482
train gradient:  0.15911305183513974
iteration : 4609
train acc:  0.7265625
train loss:  0.47395482659339905
train gradient:  0.14223533093602597
iteration : 4610
train acc:  0.7265625
train loss:  0.5202274322509766
train gradient:  0.16202763195767025
iteration : 4611
train acc:  0.7109375
train loss:  0.5178059339523315
train gradient:  0.1730360567517373
iteration : 4612
train acc:  0.671875
train loss:  0.5763073563575745
train gradient:  0.20821330800717613
iteration : 4613
train acc:  0.7734375
train loss:  0.4594871401786804
train gradient:  0.17697573906151542
iteration : 4614
train acc:  0.6875
train loss:  0.52657151222229
train gradient:  0.15074409873712893
iteration : 4615
train acc:  0.703125
train loss:  0.535124659538269
train gradient:  0.17503496925060225
iteration : 4616
train acc:  0.6796875
train loss:  0.6014971733093262
train gradient:  0.22574360745168176
iteration : 4617
train acc:  0.6796875
train loss:  0.5197294354438782
train gradient:  0.1282642316919737
iteration : 4618
train acc:  0.75
train loss:  0.5088543891906738
train gradient:  0.14971661324170776
iteration : 4619
train acc:  0.7265625
train loss:  0.5123719573020935
train gradient:  0.17410550344860887
iteration : 4620
train acc:  0.7578125
train loss:  0.5031188726425171
train gradient:  0.1486548145077255
iteration : 4621
train acc:  0.7109375
train loss:  0.5033448934555054
train gradient:  0.15589724139169409
iteration : 4622
train acc:  0.65625
train loss:  0.5488858222961426
train gradient:  0.14145801212516862
iteration : 4623
train acc:  0.71875
train loss:  0.5387571454048157
train gradient:  0.1528357765438147
iteration : 4624
train acc:  0.7265625
train loss:  0.48365890979766846
train gradient:  0.12308635619281397
iteration : 4625
train acc:  0.78125
train loss:  0.5349326133728027
train gradient:  0.20178360352115443
iteration : 4626
train acc:  0.6953125
train loss:  0.5801185369491577
train gradient:  0.17373600025449637
iteration : 4627
train acc:  0.734375
train loss:  0.5542585849761963
train gradient:  0.15608504193836226
iteration : 4628
train acc:  0.7890625
train loss:  0.4874093532562256
train gradient:  0.14645493560670114
iteration : 4629
train acc:  0.8203125
train loss:  0.4656802713871002
train gradient:  0.13651126406619135
iteration : 4630
train acc:  0.640625
train loss:  0.6055200099945068
train gradient:  0.16097541320487535
iteration : 4631
train acc:  0.6953125
train loss:  0.5370793342590332
train gradient:  0.1725239978270452
iteration : 4632
train acc:  0.6796875
train loss:  0.5283012986183167
train gradient:  0.1451756462434879
iteration : 4633
train acc:  0.7578125
train loss:  0.5105166435241699
train gradient:  0.16389723140195098
iteration : 4634
train acc:  0.703125
train loss:  0.504869282245636
train gradient:  0.1798915368473799
iteration : 4635
train acc:  0.7109375
train loss:  0.49220502376556396
train gradient:  0.1314857002719071
iteration : 4636
train acc:  0.6875
train loss:  0.5866979360580444
train gradient:  0.20715913249702023
iteration : 4637
train acc:  0.6953125
train loss:  0.5272607803344727
train gradient:  0.1972868838691587
iteration : 4638
train acc:  0.71875
train loss:  0.5385143756866455
train gradient:  0.14487711600944148
iteration : 4639
train acc:  0.734375
train loss:  0.5178540945053101
train gradient:  0.1689561758987299
iteration : 4640
train acc:  0.671875
train loss:  0.5637446641921997
train gradient:  0.19613857738609008
iteration : 4641
train acc:  0.734375
train loss:  0.524019718170166
train gradient:  0.15492490500796305
iteration : 4642
train acc:  0.796875
train loss:  0.45245933532714844
train gradient:  0.13715730645401447
iteration : 4643
train acc:  0.7265625
train loss:  0.5237104892730713
train gradient:  0.14364019225352997
iteration : 4644
train acc:  0.7578125
train loss:  0.4984018802642822
train gradient:  0.1465197630283954
iteration : 4645
train acc:  0.78125
train loss:  0.47031962871551514
train gradient:  0.14371228539653663
iteration : 4646
train acc:  0.71875
train loss:  0.48754096031188965
train gradient:  0.13761984740696404
iteration : 4647
train acc:  0.6171875
train loss:  0.6062161922454834
train gradient:  0.19114514481741438
iteration : 4648
train acc:  0.7578125
train loss:  0.48638224601745605
train gradient:  0.16225179116191896
iteration : 4649
train acc:  0.6796875
train loss:  0.5400258302688599
train gradient:  0.16174093093234995
iteration : 4650
train acc:  0.7421875
train loss:  0.5481509566307068
train gradient:  0.14376116983267745
iteration : 4651
train acc:  0.75
train loss:  0.4972613453865051
train gradient:  0.15661683806567553
iteration : 4652
train acc:  0.7890625
train loss:  0.40857672691345215
train gradient:  0.13161296997916055
iteration : 4653
train acc:  0.7265625
train loss:  0.513577938079834
train gradient:  0.21134172688391417
iteration : 4654
train acc:  0.734375
train loss:  0.5087082982063293
train gradient:  0.16878496617877137
iteration : 4655
train acc:  0.7734375
train loss:  0.4947913587093353
train gradient:  0.12816754396108282
iteration : 4656
train acc:  0.703125
train loss:  0.5094743967056274
train gradient:  0.14411276534833853
iteration : 4657
train acc:  0.7265625
train loss:  0.560636579990387
train gradient:  0.1731271221775676
iteration : 4658
train acc:  0.734375
train loss:  0.5689008235931396
train gradient:  0.23759417085335216
iteration : 4659
train acc:  0.703125
train loss:  0.5348395109176636
train gradient:  0.22258222280768092
iteration : 4660
train acc:  0.765625
train loss:  0.4503766596317291
train gradient:  0.11783803573826479
iteration : 4661
train acc:  0.6953125
train loss:  0.540077269077301
train gradient:  0.1757320684126769
iteration : 4662
train acc:  0.6484375
train loss:  0.6126354336738586
train gradient:  0.2659578899658993
iteration : 4663
train acc:  0.78125
train loss:  0.47765034437179565
train gradient:  0.1485861551149883
iteration : 4664
train acc:  0.7109375
train loss:  0.5327571034431458
train gradient:  0.19672188320722866
iteration : 4665
train acc:  0.71875
train loss:  0.5347944498062134
train gradient:  0.17190754193986452
iteration : 4666
train acc:  0.75
train loss:  0.5298792123794556
train gradient:  0.16551317496103496
iteration : 4667
train acc:  0.625
train loss:  0.6177542805671692
train gradient:  0.2268697183952001
iteration : 4668
train acc:  0.765625
train loss:  0.5169379115104675
train gradient:  0.17836775102353528
iteration : 4669
train acc:  0.734375
train loss:  0.4915480315685272
train gradient:  0.13870317477720503
iteration : 4670
train acc:  0.65625
train loss:  0.5846149325370789
train gradient:  0.20182391213868608
iteration : 4671
train acc:  0.7890625
train loss:  0.48641467094421387
train gradient:  0.13579928469191438
iteration : 4672
train acc:  0.734375
train loss:  0.486966997385025
train gradient:  0.13464454243446694
iteration : 4673
train acc:  0.6640625
train loss:  0.6022581458091736
train gradient:  0.18417794979240376
iteration : 4674
train acc:  0.75
train loss:  0.5000202059745789
train gradient:  0.1523093922174515
iteration : 4675
train acc:  0.703125
train loss:  0.5734306573867798
train gradient:  0.19793858844008183
iteration : 4676
train acc:  0.71875
train loss:  0.5389198064804077
train gradient:  0.18789853593209774
iteration : 4677
train acc:  0.6796875
train loss:  0.5947681665420532
train gradient:  0.17889231457835533
iteration : 4678
train acc:  0.7578125
train loss:  0.4818658232688904
train gradient:  0.17783846187952204
iteration : 4679
train acc:  0.7734375
train loss:  0.49677416682243347
train gradient:  0.15817584908132631
iteration : 4680
train acc:  0.71875
train loss:  0.5279841423034668
train gradient:  0.12632589764897328
iteration : 4681
train acc:  0.78125
train loss:  0.4719904661178589
train gradient:  0.15508909322368503
iteration : 4682
train acc:  0.734375
train loss:  0.5403555631637573
train gradient:  0.16269434076751055
iteration : 4683
train acc:  0.78125
train loss:  0.47888267040252686
train gradient:  0.15119068651505588
iteration : 4684
train acc:  0.6484375
train loss:  0.5375386476516724
train gradient:  0.20159922497305907
iteration : 4685
train acc:  0.6796875
train loss:  0.5632122755050659
train gradient:  0.2263541859739429
iteration : 4686
train acc:  0.7734375
train loss:  0.48344743251800537
train gradient:  0.1318152484718275
iteration : 4687
train acc:  0.71875
train loss:  0.5275009870529175
train gradient:  0.17171770791234367
iteration : 4688
train acc:  0.7578125
train loss:  0.507509171962738
train gradient:  0.14832724172122697
iteration : 4689
train acc:  0.6875
train loss:  0.5461021661758423
train gradient:  0.15460049873424198
iteration : 4690
train acc:  0.71875
train loss:  0.49736833572387695
train gradient:  0.1806447321382503
iteration : 4691
train acc:  0.734375
train loss:  0.5325930714607239
train gradient:  0.1704690075863005
iteration : 4692
train acc:  0.7109375
train loss:  0.5513173937797546
train gradient:  0.1846702964726351
iteration : 4693
train acc:  0.6953125
train loss:  0.5947853326797485
train gradient:  0.15297365194794466
iteration : 4694
train acc:  0.703125
train loss:  0.5327408909797668
train gradient:  0.22360889513845952
iteration : 4695
train acc:  0.7109375
train loss:  0.5265841484069824
train gradient:  0.13745736399253533
iteration : 4696
train acc:  0.7421875
train loss:  0.4846228361129761
train gradient:  0.15266520309370046
iteration : 4697
train acc:  0.7265625
train loss:  0.5572361350059509
train gradient:  0.15663368684940462
iteration : 4698
train acc:  0.7578125
train loss:  0.43385109305381775
train gradient:  0.11457969643328496
iteration : 4699
train acc:  0.71875
train loss:  0.5301059484481812
train gradient:  0.17766213628210936
iteration : 4700
train acc:  0.703125
train loss:  0.5408759713172913
train gradient:  0.15563645281299143
iteration : 4701
train acc:  0.703125
train loss:  0.529330849647522
train gradient:  0.1708657773924157
iteration : 4702
train acc:  0.78125
train loss:  0.47571080923080444
train gradient:  0.13330013247432917
iteration : 4703
train acc:  0.765625
train loss:  0.4939808249473572
train gradient:  0.14326330769580792
iteration : 4704
train acc:  0.7265625
train loss:  0.5242821574211121
train gradient:  0.1679613212147419
iteration : 4705
train acc:  0.71875
train loss:  0.5680844783782959
train gradient:  0.1802694573230057
iteration : 4706
train acc:  0.7421875
train loss:  0.5030354857444763
train gradient:  0.12522876009263603
iteration : 4707
train acc:  0.703125
train loss:  0.5661084651947021
train gradient:  0.1555098160594689
iteration : 4708
train acc:  0.671875
train loss:  0.5898735523223877
train gradient:  0.18869560620395542
iteration : 4709
train acc:  0.7109375
train loss:  0.517661452293396
train gradient:  0.13809016546817754
iteration : 4710
train acc:  0.765625
train loss:  0.5254656672477722
train gradient:  0.22677887239199107
iteration : 4711
train acc:  0.65625
train loss:  0.6094903945922852
train gradient:  0.20378271707786533
iteration : 4712
train acc:  0.734375
train loss:  0.4840458035469055
train gradient:  0.1389817935726907
iteration : 4713
train acc:  0.75
train loss:  0.48018547892570496
train gradient:  0.12042208509849532
iteration : 4714
train acc:  0.7109375
train loss:  0.5315984487533569
train gradient:  0.1548723297995372
iteration : 4715
train acc:  0.734375
train loss:  0.48947280645370483
train gradient:  0.11782130870076664
iteration : 4716
train acc:  0.65625
train loss:  0.5998761653900146
train gradient:  0.18916675170165603
iteration : 4717
train acc:  0.6953125
train loss:  0.5036876201629639
train gradient:  0.12168049344912005
iteration : 4718
train acc:  0.8046875
train loss:  0.42868077754974365
train gradient:  0.10042749091581156
iteration : 4719
train acc:  0.78125
train loss:  0.4654651880264282
train gradient:  0.1377425954267669
iteration : 4720
train acc:  0.7578125
train loss:  0.4836197793483734
train gradient:  0.1629929624640056
iteration : 4721
train acc:  0.703125
train loss:  0.5658798217773438
train gradient:  0.22737731874648265
iteration : 4722
train acc:  0.71875
train loss:  0.5341561436653137
train gradient:  0.15332293547675066
iteration : 4723
train acc:  0.7265625
train loss:  0.5217417478561401
train gradient:  0.16402641756843153
iteration : 4724
train acc:  0.7421875
train loss:  0.4822400212287903
train gradient:  0.1196657194147818
iteration : 4725
train acc:  0.7578125
train loss:  0.5415340065956116
train gradient:  0.12316242851086119
iteration : 4726
train acc:  0.71875
train loss:  0.5259502530097961
train gradient:  0.21108385619188696
iteration : 4727
train acc:  0.7421875
train loss:  0.48329904675483704
train gradient:  0.12983379008295204
iteration : 4728
train acc:  0.7421875
train loss:  0.49317753314971924
train gradient:  0.18340649331712533
iteration : 4729
train acc:  0.734375
train loss:  0.48146915435791016
train gradient:  0.11755181314425978
iteration : 4730
train acc:  0.7578125
train loss:  0.4778541922569275
train gradient:  0.13858505307281122
iteration : 4731
train acc:  0.6875
train loss:  0.6135355234146118
train gradient:  0.24071882177014936
iteration : 4732
train acc:  0.75
train loss:  0.4889376163482666
train gradient:  0.10972606104274381
iteration : 4733
train acc:  0.703125
train loss:  0.5222782492637634
train gradient:  0.1252898096640341
iteration : 4734
train acc:  0.7109375
train loss:  0.5338705778121948
train gradient:  0.15283247672352956
iteration : 4735
train acc:  0.7109375
train loss:  0.5409294366836548
train gradient:  0.1689425139763331
iteration : 4736
train acc:  0.7109375
train loss:  0.5226553678512573
train gradient:  0.15265172465652255
iteration : 4737
train acc:  0.75
train loss:  0.5135014057159424
train gradient:  0.1496665231628861
iteration : 4738
train acc:  0.6953125
train loss:  0.5531671643257141
train gradient:  0.1299084977911372
iteration : 4739
train acc:  0.734375
train loss:  0.5357752442359924
train gradient:  0.19821199352577373
iteration : 4740
train acc:  0.7578125
train loss:  0.452968955039978
train gradient:  0.11582925153748662
iteration : 4741
train acc:  0.6953125
train loss:  0.5603716373443604
train gradient:  0.15897105364084152
iteration : 4742
train acc:  0.703125
train loss:  0.5715073347091675
train gradient:  0.16753345779154946
iteration : 4743
train acc:  0.7421875
train loss:  0.5223544836044312
train gradient:  0.14162813006628994
iteration : 4744
train acc:  0.7109375
train loss:  0.5568293929100037
train gradient:  0.18903700452686142
iteration : 4745
train acc:  0.6953125
train loss:  0.553415834903717
train gradient:  0.19148326799864962
iteration : 4746
train acc:  0.8046875
train loss:  0.48085010051727295
train gradient:  0.13624641183606917
iteration : 4747
train acc:  0.671875
train loss:  0.568402886390686
train gradient:  0.15048990336961143
iteration : 4748
train acc:  0.734375
train loss:  0.49268245697021484
train gradient:  0.13467247819620703
iteration : 4749
train acc:  0.703125
train loss:  0.48144298791885376
train gradient:  0.13785299190212197
iteration : 4750
train acc:  0.7734375
train loss:  0.44412970542907715
train gradient:  0.1251584843821584
iteration : 4751
train acc:  0.7265625
train loss:  0.5799739360809326
train gradient:  0.1620969900129267
iteration : 4752
train acc:  0.7421875
train loss:  0.4683407247066498
train gradient:  0.14900808206848087
iteration : 4753
train acc:  0.734375
train loss:  0.5256439447402954
train gradient:  0.21431109632276168
iteration : 4754
train acc:  0.6796875
train loss:  0.5444799661636353
train gradient:  0.20690381238499178
iteration : 4755
train acc:  0.7265625
train loss:  0.4723702669143677
train gradient:  0.1341481962891455
iteration : 4756
train acc:  0.7421875
train loss:  0.4971773028373718
train gradient:  0.20267017441893367
iteration : 4757
train acc:  0.75
train loss:  0.4966512620449066
train gradient:  0.14622986770981525
iteration : 4758
train acc:  0.7109375
train loss:  0.4997735321521759
train gradient:  0.15319857088566174
iteration : 4759
train acc:  0.765625
train loss:  0.48245203495025635
train gradient:  0.11114663047109365
iteration : 4760
train acc:  0.796875
train loss:  0.4579048454761505
train gradient:  0.11893434218629667
iteration : 4761
train acc:  0.765625
train loss:  0.505725085735321
train gradient:  0.1407611801200186
iteration : 4762
train acc:  0.7265625
train loss:  0.5846503376960754
train gradient:  0.2078694595414794
iteration : 4763
train acc:  0.7421875
train loss:  0.5173771381378174
train gradient:  0.1491551593959251
iteration : 4764
train acc:  0.7578125
train loss:  0.45351770520210266
train gradient:  0.11364217633988351
iteration : 4765
train acc:  0.765625
train loss:  0.5082739591598511
train gradient:  0.15668947677273762
iteration : 4766
train acc:  0.75
train loss:  0.547705888748169
train gradient:  0.19540801022960655
iteration : 4767
train acc:  0.7265625
train loss:  0.5251802206039429
train gradient:  0.1671250839493726
iteration : 4768
train acc:  0.703125
train loss:  0.5150482058525085
train gradient:  0.15392359533910488
iteration : 4769
train acc:  0.8125
train loss:  0.4359626770019531
train gradient:  0.13565632381564635
iteration : 4770
train acc:  0.6875
train loss:  0.5687980055809021
train gradient:  0.15453634030351326
iteration : 4771
train acc:  0.6875
train loss:  0.5700117349624634
train gradient:  0.16648772346155682
iteration : 4772
train acc:  0.6171875
train loss:  0.659548282623291
train gradient:  0.24290815072466349
iteration : 4773
train acc:  0.734375
train loss:  0.49309056997299194
train gradient:  0.14746800715765837
iteration : 4774
train acc:  0.734375
train loss:  0.4786017835140228
train gradient:  0.12827574733089564
iteration : 4775
train acc:  0.7734375
train loss:  0.5032656192779541
train gradient:  0.20368500788339022
iteration : 4776
train acc:  0.7109375
train loss:  0.5134835243225098
train gradient:  0.15904695102345667
iteration : 4777
train acc:  0.75
train loss:  0.4626930058002472
train gradient:  0.1431083234138547
iteration : 4778
train acc:  0.65625
train loss:  0.5845890045166016
train gradient:  0.22910728271464592
iteration : 4779
train acc:  0.7109375
train loss:  0.5389111042022705
train gradient:  0.14586395615446912
iteration : 4780
train acc:  0.6875
train loss:  0.5043638348579407
train gradient:  0.12486365427136768
iteration : 4781
train acc:  0.7421875
train loss:  0.5170620083808899
train gradient:  0.16676805439193718
iteration : 4782
train acc:  0.7734375
train loss:  0.4843101501464844
train gradient:  0.1599321131344395
iteration : 4783
train acc:  0.7265625
train loss:  0.4884289503097534
train gradient:  0.1067327202783729
iteration : 4784
train acc:  0.7265625
train loss:  0.5133388638496399
train gradient:  0.16885200440671
iteration : 4785
train acc:  0.703125
train loss:  0.5365426540374756
train gradient:  0.15035370469921286
iteration : 4786
train acc:  0.7578125
train loss:  0.5270732641220093
train gradient:  0.14837231822691557
iteration : 4787
train acc:  0.765625
train loss:  0.4625438153743744
train gradient:  0.11981922361847933
iteration : 4788
train acc:  0.6875
train loss:  0.5890003442764282
train gradient:  0.1566402954853774
iteration : 4789
train acc:  0.75
train loss:  0.5223106741905212
train gradient:  0.13578817426216197
iteration : 4790
train acc:  0.7109375
train loss:  0.54463791847229
train gradient:  0.17103673005387673
iteration : 4791
train acc:  0.6875
train loss:  0.5159619450569153
train gradient:  0.17888383714414796
iteration : 4792
train acc:  0.6953125
train loss:  0.5703358054161072
train gradient:  0.1757291471938561
iteration : 4793
train acc:  0.734375
train loss:  0.5076286792755127
train gradient:  0.15205140878273654
iteration : 4794
train acc:  0.734375
train loss:  0.4829588532447815
train gradient:  0.12411611872973544
iteration : 4795
train acc:  0.7265625
train loss:  0.5869346857070923
train gradient:  0.1883293336069666
iteration : 4796
train acc:  0.6640625
train loss:  0.5783917903900146
train gradient:  0.22049849930365192
iteration : 4797
train acc:  0.7421875
train loss:  0.5389774441719055
train gradient:  0.17514347314731166
iteration : 4798
train acc:  0.78125
train loss:  0.4347580075263977
train gradient:  0.1516850386191222
iteration : 4799
train acc:  0.8046875
train loss:  0.4438832402229309
train gradient:  0.1689612263161464
iteration : 4800
train acc:  0.6953125
train loss:  0.5967281460762024
train gradient:  0.2312903973522213
iteration : 4801
train acc:  0.703125
train loss:  0.5120819807052612
train gradient:  0.15412897671936981
iteration : 4802
train acc:  0.8046875
train loss:  0.46022385358810425
train gradient:  0.118243290489358
iteration : 4803
train acc:  0.734375
train loss:  0.5346108675003052
train gradient:  0.14702210099800167
iteration : 4804
train acc:  0.6953125
train loss:  0.55592942237854
train gradient:  0.14134412321103496
iteration : 4805
train acc:  0.7109375
train loss:  0.5871315598487854
train gradient:  0.21628014570849324
iteration : 4806
train acc:  0.765625
train loss:  0.4986342489719391
train gradient:  0.17098268835563538
iteration : 4807
train acc:  0.78125
train loss:  0.45430129766464233
train gradient:  0.13789113277107706
iteration : 4808
train acc:  0.7109375
train loss:  0.5179136991500854
train gradient:  0.1440334919765633
iteration : 4809
train acc:  0.71875
train loss:  0.5361059904098511
train gradient:  0.19899047087319788
iteration : 4810
train acc:  0.7109375
train loss:  0.48500996828079224
train gradient:  0.11925034745638399
iteration : 4811
train acc:  0.7265625
train loss:  0.5446102619171143
train gradient:  0.1610366348215811
iteration : 4812
train acc:  0.75
train loss:  0.5075953602790833
train gradient:  0.11495978755962352
iteration : 4813
train acc:  0.8046875
train loss:  0.46863871812820435
train gradient:  0.14350253367114035
iteration : 4814
train acc:  0.703125
train loss:  0.5466646552085876
train gradient:  0.1592745683499311
iteration : 4815
train acc:  0.78125
train loss:  0.4936050772666931
train gradient:  0.1581683845607924
iteration : 4816
train acc:  0.75
train loss:  0.5025210976600647
train gradient:  0.13297737153376993
iteration : 4817
train acc:  0.703125
train loss:  0.5438930988311768
train gradient:  0.16398204196993393
iteration : 4818
train acc:  0.734375
train loss:  0.49424368143081665
train gradient:  0.15542538226791477
iteration : 4819
train acc:  0.6875
train loss:  0.5447878837585449
train gradient:  0.1301808536142962
iteration : 4820
train acc:  0.75
train loss:  0.49607598781585693
train gradient:  0.15494189353923093
iteration : 4821
train acc:  0.71875
train loss:  0.5449470281600952
train gradient:  0.1943158156538925
iteration : 4822
train acc:  0.7421875
train loss:  0.5636459589004517
train gradient:  0.16558545447800632
iteration : 4823
train acc:  0.7734375
train loss:  0.46781110763549805
train gradient:  0.11047189486789306
iteration : 4824
train acc:  0.6875
train loss:  0.5594877004623413
train gradient:  0.1874901346602584
iteration : 4825
train acc:  0.75
train loss:  0.5081131458282471
train gradient:  0.16967435030571684
iteration : 4826
train acc:  0.7109375
train loss:  0.5133301019668579
train gradient:  0.12857766528979764
iteration : 4827
train acc:  0.8046875
train loss:  0.4814615547657013
train gradient:  0.1675423208941776
iteration : 4828
train acc:  0.734375
train loss:  0.509436309337616
train gradient:  0.15167585646745968
iteration : 4829
train acc:  0.78125
train loss:  0.4992310404777527
train gradient:  0.14261665837053467
iteration : 4830
train acc:  0.6171875
train loss:  0.6281951665878296
train gradient:  0.19858205930546524
iteration : 4831
train acc:  0.71875
train loss:  0.5492420792579651
train gradient:  0.1546429864032197
iteration : 4832
train acc:  0.78125
train loss:  0.45625215768814087
train gradient:  0.1264834418934427
iteration : 4833
train acc:  0.6953125
train loss:  0.5228791236877441
train gradient:  0.15360164144253258
iteration : 4834
train acc:  0.6640625
train loss:  0.5943865776062012
train gradient:  0.26438809899635085
iteration : 4835
train acc:  0.75
train loss:  0.5002661943435669
train gradient:  0.17115096054034545
iteration : 4836
train acc:  0.75
train loss:  0.49249765276908875
train gradient:  0.14389594080855062
iteration : 4837
train acc:  0.71875
train loss:  0.5150102972984314
train gradient:  0.1444609659932068
iteration : 4838
train acc:  0.7265625
train loss:  0.5264525413513184
train gradient:  0.17086007048988053
iteration : 4839
train acc:  0.734375
train loss:  0.5194332599639893
train gradient:  0.14379322633507136
iteration : 4840
train acc:  0.671875
train loss:  0.5232768058776855
train gradient:  0.14950292312599853
iteration : 4841
train acc:  0.6640625
train loss:  0.5328720808029175
train gradient:  0.15501596194392736
iteration : 4842
train acc:  0.765625
train loss:  0.45010679960250854
train gradient:  0.11351180480302883
iteration : 4843
train acc:  0.7578125
train loss:  0.5431491136550903
train gradient:  0.22038575761906096
iteration : 4844
train acc:  0.78125
train loss:  0.43821442127227783
train gradient:  0.13080371090496556
iteration : 4845
train acc:  0.6953125
train loss:  0.5721949338912964
train gradient:  0.13388151767853312
iteration : 4846
train acc:  0.7265625
train loss:  0.5129551291465759
train gradient:  0.14527348842142218
iteration : 4847
train acc:  0.7578125
train loss:  0.46345239877700806
train gradient:  0.15725575532552882
iteration : 4848
train acc:  0.7265625
train loss:  0.5486890077590942
train gradient:  0.13928361977229725
iteration : 4849
train acc:  0.75
train loss:  0.5071761608123779
train gradient:  0.15639495215658905
iteration : 4850
train acc:  0.7265625
train loss:  0.5115050077438354
train gradient:  0.13467004223512888
iteration : 4851
train acc:  0.7734375
train loss:  0.49965059757232666
train gradient:  0.15152004684116302
iteration : 4852
train acc:  0.6953125
train loss:  0.5573904514312744
train gradient:  0.16757731565132572
iteration : 4853
train acc:  0.6640625
train loss:  0.5242332220077515
train gradient:  0.13378296837022152
iteration : 4854
train acc:  0.796875
train loss:  0.46713975071907043
train gradient:  0.125567851762316
iteration : 4855
train acc:  0.7421875
train loss:  0.5040497183799744
train gradient:  0.14116069242316473
iteration : 4856
train acc:  0.671875
train loss:  0.6415150761604309
train gradient:  0.2541750078369128
iteration : 4857
train acc:  0.6484375
train loss:  0.6053012609481812
train gradient:  0.2004281549969429
iteration : 4858
train acc:  0.671875
train loss:  0.5375046730041504
train gradient:  0.15240502099760306
iteration : 4859
train acc:  0.7890625
train loss:  0.5123201012611389
train gradient:  0.139449566870381
iteration : 4860
train acc:  0.7265625
train loss:  0.48192721605300903
train gradient:  0.1430238297327851
iteration : 4861
train acc:  0.7265625
train loss:  0.5322875380516052
train gradient:  0.1903486172071323
iteration : 4862
train acc:  0.6796875
train loss:  0.5358690023422241
train gradient:  0.1780430923009596
iteration : 4863
train acc:  0.7109375
train loss:  0.5059906244277954
train gradient:  0.16442276224256439
iteration : 4864
train acc:  0.7578125
train loss:  0.45268702507019043
train gradient:  0.13759003626798208
iteration : 4865
train acc:  0.765625
train loss:  0.49611327052116394
train gradient:  0.12127923690107963
iteration : 4866
train acc:  0.71875
train loss:  0.5229562520980835
train gradient:  0.17169730914971182
iteration : 4867
train acc:  0.703125
train loss:  0.5273641347885132
train gradient:  0.21014011071580652
iteration : 4868
train acc:  0.7890625
train loss:  0.45830434560775757
train gradient:  0.1257013388065088
iteration : 4869
train acc:  0.7734375
train loss:  0.4352750778198242
train gradient:  0.10918537839105061
iteration : 4870
train acc:  0.71875
train loss:  0.5011146664619446
train gradient:  0.1309555545421966
iteration : 4871
train acc:  0.78125
train loss:  0.4683235287666321
train gradient:  0.1226625665745548
iteration : 4872
train acc:  0.7265625
train loss:  0.49529194831848145
train gradient:  0.13553048346345647
iteration : 4873
train acc:  0.6796875
train loss:  0.4979821741580963
train gradient:  0.1340513327858751
iteration : 4874
train acc:  0.625
train loss:  0.5894299745559692
train gradient:  0.20636509724397817
iteration : 4875
train acc:  0.75
train loss:  0.4892982542514801
train gradient:  0.13555475656367674
iteration : 4876
train acc:  0.6640625
train loss:  0.5167306065559387
train gradient:  0.16881599291925053
iteration : 4877
train acc:  0.75
train loss:  0.5044962167739868
train gradient:  0.12863801364634178
iteration : 4878
train acc:  0.734375
train loss:  0.5412662029266357
train gradient:  0.1717366807874413
iteration : 4879
train acc:  0.765625
train loss:  0.451590895652771
train gradient:  0.11203092904238202
iteration : 4880
train acc:  0.7109375
train loss:  0.5165259838104248
train gradient:  0.18594634435168916
iteration : 4881
train acc:  0.78125
train loss:  0.46764305233955383
train gradient:  0.1121880892695104
iteration : 4882
train acc:  0.671875
train loss:  0.6186761856079102
train gradient:  0.2827599309335506
iteration : 4883
train acc:  0.71875
train loss:  0.5612249970436096
train gradient:  0.15861320909561888
iteration : 4884
train acc:  0.7421875
train loss:  0.5013793110847473
train gradient:  0.14578436652924925
iteration : 4885
train acc:  0.734375
train loss:  0.4903339743614197
train gradient:  0.12097946317872477
iteration : 4886
train acc:  0.75
train loss:  0.5085676312446594
train gradient:  0.14909695939995604
iteration : 4887
train acc:  0.734375
train loss:  0.49470028281211853
train gradient:  0.11363636427546699
iteration : 4888
train acc:  0.734375
train loss:  0.5149422883987427
train gradient:  0.17378509086171168
iteration : 4889
train acc:  0.7421875
train loss:  0.4864358901977539
train gradient:  0.15457572611932374
iteration : 4890
train acc:  0.6875
train loss:  0.5610592365264893
train gradient:  0.19651902029394844
iteration : 4891
train acc:  0.7109375
train loss:  0.5255069136619568
train gradient:  0.15568577707978354
iteration : 4892
train acc:  0.7578125
train loss:  0.5279203653335571
train gradient:  0.14195748979924494
iteration : 4893
train acc:  0.7734375
train loss:  0.48031163215637207
train gradient:  0.165298698677864
iteration : 4894
train acc:  0.6953125
train loss:  0.5455615520477295
train gradient:  0.1545015157070409
iteration : 4895
train acc:  0.7578125
train loss:  0.5154733657836914
train gradient:  0.13340255866230552
iteration : 4896
train acc:  0.7890625
train loss:  0.444352388381958
train gradient:  0.11861507817194507
iteration : 4897
train acc:  0.7578125
train loss:  0.4896209239959717
train gradient:  0.15472538447212292
iteration : 4898
train acc:  0.78125
train loss:  0.45358145236968994
train gradient:  0.11020588229586707
iteration : 4899
train acc:  0.75
train loss:  0.5040995478630066
train gradient:  0.12547733383163948
iteration : 4900
train acc:  0.75
train loss:  0.483551561832428
train gradient:  0.12697776504936142
iteration : 4901
train acc:  0.7109375
train loss:  0.5133401155471802
train gradient:  0.16880635104954778
iteration : 4902
train acc:  0.7578125
train loss:  0.5224764347076416
train gradient:  0.1388995302538909
iteration : 4903
train acc:  0.734375
train loss:  0.5695903301239014
train gradient:  0.1600831563740197
iteration : 4904
train acc:  0.6640625
train loss:  0.544434130191803
train gradient:  0.1676138053295851
iteration : 4905
train acc:  0.6875
train loss:  0.5268542170524597
train gradient:  0.15651379584161995
iteration : 4906
train acc:  0.671875
train loss:  0.5642203092575073
train gradient:  0.15867787059921284
iteration : 4907
train acc:  0.7265625
train loss:  0.5269655585289001
train gradient:  0.16900983130085642
iteration : 4908
train acc:  0.71875
train loss:  0.5022332072257996
train gradient:  0.13812497745908736
iteration : 4909
train acc:  0.734375
train loss:  0.546560525894165
train gradient:  0.17417108722846908
iteration : 4910
train acc:  0.7890625
train loss:  0.42545580863952637
train gradient:  0.10517425032987908
iteration : 4911
train acc:  0.703125
train loss:  0.5472334623336792
train gradient:  0.18324243908137333
iteration : 4912
train acc:  0.71875
train loss:  0.5324969291687012
train gradient:  0.16451765677102653
iteration : 4913
train acc:  0.7734375
train loss:  0.48097819089889526
train gradient:  0.10949615727247604
iteration : 4914
train acc:  0.7109375
train loss:  0.5069961547851562
train gradient:  0.14204557197700138
iteration : 4915
train acc:  0.734375
train loss:  0.5452667474746704
train gradient:  0.1639787502711207
iteration : 4916
train acc:  0.7109375
train loss:  0.5422471165657043
train gradient:  0.1739450039699419
iteration : 4917
train acc:  0.75
train loss:  0.5062880516052246
train gradient:  0.1738657482732317
iteration : 4918
train acc:  0.765625
train loss:  0.5447127819061279
train gradient:  0.16550893516342985
iteration : 4919
train acc:  0.75
train loss:  0.5424181818962097
train gradient:  0.1760599774221287
iteration : 4920
train acc:  0.7421875
train loss:  0.5458596348762512
train gradient:  0.1732250595163347
iteration : 4921
train acc:  0.8125
train loss:  0.47234851121902466
train gradient:  0.13443831268488377
iteration : 4922
train acc:  0.8046875
train loss:  0.4855477213859558
train gradient:  0.12175762424278264
iteration : 4923
train acc:  0.765625
train loss:  0.486091673374176
train gradient:  0.10841823837728568
iteration : 4924
train acc:  0.734375
train loss:  0.5228271484375
train gradient:  0.1434304894211264
iteration : 4925
train acc:  0.7421875
train loss:  0.44900697469711304
train gradient:  0.13844694238567506
iteration : 4926
train acc:  0.8046875
train loss:  0.4408767819404602
train gradient:  0.11281167893355883
iteration : 4927
train acc:  0.734375
train loss:  0.4974520206451416
train gradient:  0.13479549033161298
iteration : 4928
train acc:  0.6953125
train loss:  0.5735869407653809
train gradient:  0.19308650268106242
iteration : 4929
train acc:  0.6953125
train loss:  0.5725458860397339
train gradient:  0.1661297097098074
iteration : 4930
train acc:  0.8203125
train loss:  0.4361788332462311
train gradient:  0.1367940771005982
iteration : 4931
train acc:  0.7265625
train loss:  0.5566511750221252
train gradient:  0.17329116085362378
iteration : 4932
train acc:  0.765625
train loss:  0.4740425646305084
train gradient:  0.12965079897005682
iteration : 4933
train acc:  0.703125
train loss:  0.5422823429107666
train gradient:  0.11692029926182677
iteration : 4934
train acc:  0.6640625
train loss:  0.5766264200210571
train gradient:  0.23861334417602986
iteration : 4935
train acc:  0.796875
train loss:  0.4383949339389801
train gradient:  0.11616437090155138
iteration : 4936
train acc:  0.734375
train loss:  0.5183376669883728
train gradient:  0.15171101201596512
iteration : 4937
train acc:  0.7109375
train loss:  0.5398396253585815
train gradient:  0.1652463860044172
iteration : 4938
train acc:  0.7265625
train loss:  0.5855300426483154
train gradient:  0.3063103147441718
iteration : 4939
train acc:  0.7578125
train loss:  0.5043526291847229
train gradient:  0.1295086168136767
iteration : 4940
train acc:  0.703125
train loss:  0.5595729351043701
train gradient:  0.17415888259175524
iteration : 4941
train acc:  0.7265625
train loss:  0.5082471370697021
train gradient:  0.16864983246954598
iteration : 4942
train acc:  0.7578125
train loss:  0.5052266716957092
train gradient:  0.13327676700758587
iteration : 4943
train acc:  0.6640625
train loss:  0.613725483417511
train gradient:  0.2602322146814597
iteration : 4944
train acc:  0.6875
train loss:  0.5473198294639587
train gradient:  0.20419545289910995
iteration : 4945
train acc:  0.734375
train loss:  0.5079357624053955
train gradient:  0.13058441463036605
iteration : 4946
train acc:  0.7578125
train loss:  0.5150259137153625
train gradient:  0.13512745883910635
iteration : 4947
train acc:  0.6796875
train loss:  0.6137435436248779
train gradient:  0.17227411408589238
iteration : 4948
train acc:  0.765625
train loss:  0.49430549144744873
train gradient:  0.15256363556000124
iteration : 4949
train acc:  0.6953125
train loss:  0.5950495600700378
train gradient:  0.22404646460569533
iteration : 4950
train acc:  0.7734375
train loss:  0.44281500577926636
train gradient:  0.11080233747413032
iteration : 4951
train acc:  0.7421875
train loss:  0.5473505258560181
train gradient:  0.15857432453064804
iteration : 4952
train acc:  0.7421875
train loss:  0.5141815543174744
train gradient:  0.11690923756720667
iteration : 4953
train acc:  0.78125
train loss:  0.4878605008125305
train gradient:  0.12538447128227242
iteration : 4954
train acc:  0.78125
train loss:  0.5057998895645142
train gradient:  0.14560072459946982
iteration : 4955
train acc:  0.6953125
train loss:  0.49236372113227844
train gradient:  0.17012156939443768
iteration : 4956
train acc:  0.75
train loss:  0.4927012324333191
train gradient:  0.13735418226123286
iteration : 4957
train acc:  0.734375
train loss:  0.49263766407966614
train gradient:  0.1558316119054494
iteration : 4958
train acc:  0.7109375
train loss:  0.5237421989440918
train gradient:  0.12660095998824344
iteration : 4959
train acc:  0.75
train loss:  0.473199725151062
train gradient:  0.11626062877843925
iteration : 4960
train acc:  0.78125
train loss:  0.46788451075553894
train gradient:  0.12531233030744587
iteration : 4961
train acc:  0.7578125
train loss:  0.4791204035282135
train gradient:  0.14466366369872324
iteration : 4962
train acc:  0.78125
train loss:  0.43017640709877014
train gradient:  0.11725875218553643
iteration : 4963
train acc:  0.796875
train loss:  0.44748997688293457
train gradient:  0.14580059623740493
iteration : 4964
train acc:  0.7578125
train loss:  0.5251358151435852
train gradient:  0.16960925363424306
iteration : 4965
train acc:  0.734375
train loss:  0.5502386093139648
train gradient:  0.1862942348592931
iteration : 4966
train acc:  0.6796875
train loss:  0.5771965384483337
train gradient:  0.2343396252030761
iteration : 4967
train acc:  0.7578125
train loss:  0.4794654846191406
train gradient:  0.12312776157446532
iteration : 4968
train acc:  0.765625
train loss:  0.47861239314079285
train gradient:  0.15660424432055287
iteration : 4969
train acc:  0.75
train loss:  0.5201476812362671
train gradient:  0.1290566498783879
iteration : 4970
train acc:  0.7421875
train loss:  0.5236790776252747
train gradient:  0.14797164138376417
iteration : 4971
train acc:  0.8125
train loss:  0.44746148586273193
train gradient:  0.1279960765637426
iteration : 4972
train acc:  0.75
train loss:  0.48006269335746765
train gradient:  0.16852019206651125
iteration : 4973
train acc:  0.6640625
train loss:  0.5900968909263611
train gradient:  0.23570086361989226
iteration : 4974
train acc:  0.6640625
train loss:  0.5901639461517334
train gradient:  0.23063886624690638
iteration : 4975
train acc:  0.734375
train loss:  0.5179960131645203
train gradient:  0.17954974495371187
iteration : 4976
train acc:  0.671875
train loss:  0.5927952527999878
train gradient:  0.19240818799105042
iteration : 4977
train acc:  0.6953125
train loss:  0.5392462611198425
train gradient:  0.1674392067774373
iteration : 4978
train acc:  0.703125
train loss:  0.5764174461364746
train gradient:  0.18316997573695348
iteration : 4979
train acc:  0.7734375
train loss:  0.4766415059566498
train gradient:  0.1517200569813258
iteration : 4980
train acc:  0.71875
train loss:  0.5371431708335876
train gradient:  0.1891118511007619
iteration : 4981
train acc:  0.6953125
train loss:  0.5395500659942627
train gradient:  0.14339099296903962
iteration : 4982
train acc:  0.7421875
train loss:  0.47993794083595276
train gradient:  0.198218841309855
iteration : 4983
train acc:  0.7578125
train loss:  0.4843437671661377
train gradient:  0.11788197450194939
iteration : 4984
train acc:  0.765625
train loss:  0.5021429061889648
train gradient:  0.1906478979385512
iteration : 4985
train acc:  0.75
train loss:  0.4828646779060364
train gradient:  0.14103980968455582
iteration : 4986
train acc:  0.75
train loss:  0.47271034121513367
train gradient:  0.13588022908686712
iteration : 4987
train acc:  0.703125
train loss:  0.527891993522644
train gradient:  0.147813295431403
iteration : 4988
train acc:  0.703125
train loss:  0.5107005834579468
train gradient:  0.16294068381403026
iteration : 4989
train acc:  0.7265625
train loss:  0.5188011527061462
train gradient:  0.16809389636280536
iteration : 4990
train acc:  0.6953125
train loss:  0.5600188374519348
train gradient:  0.1901042532617183
iteration : 4991
train acc:  0.7265625
train loss:  0.46021080017089844
train gradient:  0.13916271862324242
iteration : 4992
train acc:  0.703125
train loss:  0.5199786424636841
train gradient:  0.20423516209286335
iteration : 4993
train acc:  0.7578125
train loss:  0.49280017614364624
train gradient:  0.13710084800867942
iteration : 4994
train acc:  0.734375
train loss:  0.5416982173919678
train gradient:  0.17678700805716663
iteration : 4995
train acc:  0.6484375
train loss:  0.5679706931114197
train gradient:  0.1616196765742537
iteration : 4996
train acc:  0.671875
train loss:  0.578913688659668
train gradient:  0.21471793175656279
iteration : 4997
train acc:  0.796875
train loss:  0.4571389853954315
train gradient:  0.14035482202484806
iteration : 4998
train acc:  0.6953125
train loss:  0.5635383129119873
train gradient:  0.15514887596757476
iteration : 4999
train acc:  0.734375
train loss:  0.5214419960975647
train gradient:  0.12171402902716301
iteration : 5000
train acc:  0.6953125
train loss:  0.5578759908676147
train gradient:  0.15190338934016567
iteration : 5001
train acc:  0.7734375
train loss:  0.4980859160423279
train gradient:  0.14966579600486246
iteration : 5002
train acc:  0.734375
train loss:  0.5639702081680298
train gradient:  0.18333197167478238
iteration : 5003
train acc:  0.7109375
train loss:  0.5454188585281372
train gradient:  0.1338666391018876
iteration : 5004
train acc:  0.6875
train loss:  0.5882296562194824
train gradient:  0.19896483183216884
iteration : 5005
train acc:  0.7421875
train loss:  0.5610360503196716
train gradient:  0.2118188012496825
iteration : 5006
train acc:  0.734375
train loss:  0.5349162220954895
train gradient:  0.1793265870436927
iteration : 5007
train acc:  0.7109375
train loss:  0.5807609558105469
train gradient:  0.1707659010896187
iteration : 5008
train acc:  0.734375
train loss:  0.4796638488769531
train gradient:  0.13662249715486854
iteration : 5009
train acc:  0.7578125
train loss:  0.4921291470527649
train gradient:  0.1546360603756543
iteration : 5010
train acc:  0.765625
train loss:  0.5017119646072388
train gradient:  0.15857607304356092
iteration : 5011
train acc:  0.71875
train loss:  0.5648803114891052
train gradient:  0.1984459796763014
iteration : 5012
train acc:  0.6796875
train loss:  0.5926399827003479
train gradient:  0.151493293400086
iteration : 5013
train acc:  0.7421875
train loss:  0.5186424255371094
train gradient:  0.19404166296034336
iteration : 5014
train acc:  0.6640625
train loss:  0.5740714073181152
train gradient:  0.19234500126370085
iteration : 5015
train acc:  0.71875
train loss:  0.5177174210548401
train gradient:  0.16301569353558776
iteration : 5016
train acc:  0.6875
train loss:  0.5522801280021667
train gradient:  0.2058412297416657
iteration : 5017
train acc:  0.7265625
train loss:  0.5619321465492249
train gradient:  0.16830317440044132
iteration : 5018
train acc:  0.6953125
train loss:  0.5411494970321655
train gradient:  0.1314906313718142
iteration : 5019
train acc:  0.75
train loss:  0.45278027653694153
train gradient:  0.11035324170499275
iteration : 5020
train acc:  0.8203125
train loss:  0.43911874294281006
train gradient:  0.12640797302803944
iteration : 5021
train acc:  0.734375
train loss:  0.5096172094345093
train gradient:  0.12623783195312646
iteration : 5022
train acc:  0.6796875
train loss:  0.5361478924751282
train gradient:  0.17681914424143838
iteration : 5023
train acc:  0.734375
train loss:  0.525374174118042
train gradient:  0.13757189600363703
iteration : 5024
train acc:  0.7109375
train loss:  0.521853506565094
train gradient:  0.1734375296924261
iteration : 5025
train acc:  0.78125
train loss:  0.46231165528297424
train gradient:  0.1372261847642945
iteration : 5026
train acc:  0.7265625
train loss:  0.5678572654724121
train gradient:  0.1622187579528646
iteration : 5027
train acc:  0.703125
train loss:  0.5126659870147705
train gradient:  0.15418150956047633
iteration : 5028
train acc:  0.671875
train loss:  0.5520232915878296
train gradient:  0.1656362476753711
iteration : 5029
train acc:  0.7734375
train loss:  0.45513662695884705
train gradient:  0.09532973551027636
iteration : 5030
train acc:  0.7578125
train loss:  0.46202003955841064
train gradient:  0.10879761916197664
iteration : 5031
train acc:  0.765625
train loss:  0.5369336605072021
train gradient:  0.17728053500504437
iteration : 5032
train acc:  0.703125
train loss:  0.534723699092865
train gradient:  0.17227250431737065
iteration : 5033
train acc:  0.7265625
train loss:  0.5531138181686401
train gradient:  0.16065533539802243
iteration : 5034
train acc:  0.78125
train loss:  0.47634410858154297
train gradient:  0.1292873460227818
iteration : 5035
train acc:  0.765625
train loss:  0.4848017692565918
train gradient:  0.1492163301047768
iteration : 5036
train acc:  0.6640625
train loss:  0.5639066696166992
train gradient:  0.20417577438662904
iteration : 5037
train acc:  0.7109375
train loss:  0.5196451544761658
train gradient:  0.15400127126609053
iteration : 5038
train acc:  0.7109375
train loss:  0.5388562083244324
train gradient:  0.15650791779444417
iteration : 5039
train acc:  0.734375
train loss:  0.5145246386528015
train gradient:  0.14453227022865367
iteration : 5040
train acc:  0.78125
train loss:  0.48418909311294556
train gradient:  0.12910525507707565
iteration : 5041
train acc:  0.65625
train loss:  0.5596019625663757
train gradient:  0.15246560593676603
iteration : 5042
train acc:  0.734375
train loss:  0.47809669375419617
train gradient:  0.1289326131790447
iteration : 5043
train acc:  0.7578125
train loss:  0.4905092716217041
train gradient:  0.11723259507570703
iteration : 5044
train acc:  0.7890625
train loss:  0.46821320056915283
train gradient:  0.15631721753416378
iteration : 5045
train acc:  0.7734375
train loss:  0.47626787424087524
train gradient:  0.16580637168898063
iteration : 5046
train acc:  0.7109375
train loss:  0.5357926487922668
train gradient:  0.15919838094517952
iteration : 5047
train acc:  0.7578125
train loss:  0.5128704309463501
train gradient:  0.18263594865712873
iteration : 5048
train acc:  0.7578125
train loss:  0.4631090760231018
train gradient:  0.12784879510554745
iteration : 5049
train acc:  0.671875
train loss:  0.5104601383209229
train gradient:  0.11326290386633608
iteration : 5050
train acc:  0.7265625
train loss:  0.6238189935684204
train gradient:  0.24705831020421742
iteration : 5051
train acc:  0.6796875
train loss:  0.5306645631790161
train gradient:  0.14495519205306712
iteration : 5052
train acc:  0.7265625
train loss:  0.5027517676353455
train gradient:  0.12571093510105147
iteration : 5053
train acc:  0.6875
train loss:  0.5221534967422485
train gradient:  0.13790549876557562
iteration : 5054
train acc:  0.765625
train loss:  0.5082565546035767
train gradient:  0.16283992612080972
iteration : 5055
train acc:  0.6953125
train loss:  0.5613173842430115
train gradient:  0.18307961199554607
iteration : 5056
train acc:  0.703125
train loss:  0.5126039981842041
train gradient:  0.11716108242598934
iteration : 5057
train acc:  0.734375
train loss:  0.5093770027160645
train gradient:  0.16642716865914386
iteration : 5058
train acc:  0.703125
train loss:  0.5352351665496826
train gradient:  0.14869057149866696
iteration : 5059
train acc:  0.7109375
train loss:  0.5345736145973206
train gradient:  0.15055325204918912
iteration : 5060
train acc:  0.703125
train loss:  0.5408799648284912
train gradient:  0.19945096499328546
iteration : 5061
train acc:  0.7421875
train loss:  0.4837194085121155
train gradient:  0.1263698212723449
iteration : 5062
train acc:  0.7109375
train loss:  0.5036999583244324
train gradient:  0.13177119130433085
iteration : 5063
train acc:  0.78125
train loss:  0.4658280313014984
train gradient:  0.1345882313201418
iteration : 5064
train acc:  0.7265625
train loss:  0.5268796682357788
train gradient:  0.12065489217324503
iteration : 5065
train acc:  0.734375
train loss:  0.5158513188362122
train gradient:  0.16666036375779636
iteration : 5066
train acc:  0.796875
train loss:  0.4145275354385376
train gradient:  0.10596407495899393
iteration : 5067
train acc:  0.71875
train loss:  0.5351643562316895
train gradient:  0.16510555853623138
iteration : 5068
train acc:  0.734375
train loss:  0.5188915133476257
train gradient:  0.15607920554156296
iteration : 5069
train acc:  0.7265625
train loss:  0.5190792083740234
train gradient:  0.141320647318745
iteration : 5070
train acc:  0.7265625
train loss:  0.4920565187931061
train gradient:  0.1631713534194774
iteration : 5071
train acc:  0.734375
train loss:  0.5368530750274658
train gradient:  0.23532260331498053
iteration : 5072
train acc:  0.734375
train loss:  0.5335768461227417
train gradient:  0.15091712736967983
iteration : 5073
train acc:  0.7421875
train loss:  0.5110154151916504
train gradient:  0.18181382152540532
iteration : 5074
train acc:  0.7421875
train loss:  0.49418866634368896
train gradient:  0.16794288507968402
iteration : 5075
train acc:  0.75
train loss:  0.5050657987594604
train gradient:  0.21100897984408767
iteration : 5076
train acc:  0.765625
train loss:  0.4833393394947052
train gradient:  0.1499516091731694
iteration : 5077
train acc:  0.703125
train loss:  0.5254616737365723
train gradient:  0.12242560143830176
iteration : 5078
train acc:  0.703125
train loss:  0.5481126308441162
train gradient:  0.1942413240886512
iteration : 5079
train acc:  0.640625
train loss:  0.6359615325927734
train gradient:  0.2182650753829311
iteration : 5080
train acc:  0.734375
train loss:  0.4956604242324829
train gradient:  0.1484949798646459
iteration : 5081
train acc:  0.6953125
train loss:  0.5487100481987
train gradient:  0.1657562383899451
iteration : 5082
train acc:  0.7734375
train loss:  0.5041834115982056
train gradient:  0.11978651483935797
iteration : 5083
train acc:  0.7265625
train loss:  0.5064951777458191
train gradient:  0.20634246294691536
iteration : 5084
train acc:  0.7578125
train loss:  0.4627116322517395
train gradient:  0.14422433816788127
iteration : 5085
train acc:  0.65625
train loss:  0.6114236116409302
train gradient:  0.18715241962065082
iteration : 5086
train acc:  0.765625
train loss:  0.4927294850349426
train gradient:  0.12607839108674376
iteration : 5087
train acc:  0.7109375
train loss:  0.49938279390335083
train gradient:  0.13723979206842002
iteration : 5088
train acc:  0.7265625
train loss:  0.5311670303344727
train gradient:  0.18973789164492355
iteration : 5089
train acc:  0.7578125
train loss:  0.4876507520675659
train gradient:  0.12241943130105605
iteration : 5090
train acc:  0.7109375
train loss:  0.5163009166717529
train gradient:  0.10903258271054884
iteration : 5091
train acc:  0.7734375
train loss:  0.5106745958328247
train gradient:  0.140300074952602
iteration : 5092
train acc:  0.6484375
train loss:  0.5744302868843079
train gradient:  0.17015178114269586
iteration : 5093
train acc:  0.734375
train loss:  0.5345721244812012
train gradient:  0.14791658677766167
iteration : 5094
train acc:  0.75
train loss:  0.5245851278305054
train gradient:  0.18436860103051944
iteration : 5095
train acc:  0.734375
train loss:  0.5125200748443604
train gradient:  0.13331729898177852
iteration : 5096
train acc:  0.7421875
train loss:  0.5372321605682373
train gradient:  0.1568828013929407
iteration : 5097
train acc:  0.7265625
train loss:  0.49040237069129944
train gradient:  0.14378768745692255
iteration : 5098
train acc:  0.6953125
train loss:  0.5167679786682129
train gradient:  0.15252908557080821
iteration : 5099
train acc:  0.6640625
train loss:  0.5719062089920044
train gradient:  0.17407809755856513
iteration : 5100
train acc:  0.7109375
train loss:  0.5330822467803955
train gradient:  0.1767241675716258
iteration : 5101
train acc:  0.78125
train loss:  0.5008037090301514
train gradient:  0.12041098630726663
iteration : 5102
train acc:  0.75
train loss:  0.48485803604125977
train gradient:  0.1413425522797382
iteration : 5103
train acc:  0.7421875
train loss:  0.5187365412712097
train gradient:  0.16180947229040554
iteration : 5104
train acc:  0.7421875
train loss:  0.5536099672317505
train gradient:  0.1781462763615703
iteration : 5105
train acc:  0.7578125
train loss:  0.4854782819747925
train gradient:  0.12030219111588358
iteration : 5106
train acc:  0.75
train loss:  0.5420548915863037
train gradient:  0.14287241941542056
iteration : 5107
train acc:  0.7890625
train loss:  0.48903441429138184
train gradient:  0.14017831486329235
iteration : 5108
train acc:  0.65625
train loss:  0.6261324882507324
train gradient:  0.20785238783905602
iteration : 5109
train acc:  0.78125
train loss:  0.46759435534477234
train gradient:  0.1396393870117409
iteration : 5110
train acc:  0.765625
train loss:  0.45246732234954834
train gradient:  0.10310241318241764
iteration : 5111
train acc:  0.734375
train loss:  0.5125118494033813
train gradient:  0.13313054156166532
iteration : 5112
train acc:  0.6875
train loss:  0.5452420711517334
train gradient:  0.1485335179428635
iteration : 5113
train acc:  0.671875
train loss:  0.5638630390167236
train gradient:  0.20947630903211595
iteration : 5114
train acc:  0.671875
train loss:  0.562455415725708
train gradient:  0.2023933914647657
iteration : 5115
train acc:  0.75
train loss:  0.4982485771179199
train gradient:  0.1427482736917267
iteration : 5116
train acc:  0.7109375
train loss:  0.5137590169906616
train gradient:  0.12612604041319148
iteration : 5117
train acc:  0.734375
train loss:  0.49438488483428955
train gradient:  0.11338835741594634
iteration : 5118
train acc:  0.6640625
train loss:  0.5712448358535767
train gradient:  0.19480650064033211
iteration : 5119
train acc:  0.734375
train loss:  0.5402057766914368
train gradient:  0.15051813373043038
iteration : 5120
train acc:  0.7265625
train loss:  0.5003546476364136
train gradient:  0.20659782481525368
iteration : 5121
train acc:  0.7421875
train loss:  0.49138230085372925
train gradient:  0.13754921239448678
iteration : 5122
train acc:  0.765625
train loss:  0.5293049216270447
train gradient:  0.14852222606434493
iteration : 5123
train acc:  0.75
train loss:  0.5250319242477417
train gradient:  0.14745111542263725
iteration : 5124
train acc:  0.75
train loss:  0.4820331931114197
train gradient:  0.13791108000467853
iteration : 5125
train acc:  0.65625
train loss:  0.5391997694969177
train gradient:  0.13461315323784162
iteration : 5126
train acc:  0.625
train loss:  0.5759906768798828
train gradient:  0.19009355460699218
iteration : 5127
train acc:  0.6953125
train loss:  0.5358129739761353
train gradient:  0.19510925576452665
iteration : 5128
train acc:  0.65625
train loss:  0.5733342170715332
train gradient:  0.20829840667375377
iteration : 5129
train acc:  0.71875
train loss:  0.5387846231460571
train gradient:  0.20114498924753105
iteration : 5130
train acc:  0.7109375
train loss:  0.5160670876502991
train gradient:  0.18036341019026975
iteration : 5131
train acc:  0.7265625
train loss:  0.512387216091156
train gradient:  0.13550997286650387
iteration : 5132
train acc:  0.71875
train loss:  0.5342988967895508
train gradient:  0.1985001210966919
iteration : 5133
train acc:  0.7265625
train loss:  0.48361119627952576
train gradient:  0.1487486000568541
iteration : 5134
train acc:  0.7578125
train loss:  0.468811571598053
train gradient:  0.1429259390947628
iteration : 5135
train acc:  0.78125
train loss:  0.4882188141345978
train gradient:  0.10114223677453762
iteration : 5136
train acc:  0.828125
train loss:  0.43429097533226013
train gradient:  0.1168263715255121
iteration : 5137
train acc:  0.6953125
train loss:  0.5697896480560303
train gradient:  0.16104491559460096
iteration : 5138
train acc:  0.796875
train loss:  0.453580379486084
train gradient:  0.11202470834613053
iteration : 5139
train acc:  0.6796875
train loss:  0.511643648147583
train gradient:  0.19106515841507632
iteration : 5140
train acc:  0.734375
train loss:  0.5448763966560364
train gradient:  0.1451033964457219
iteration : 5141
train acc:  0.7578125
train loss:  0.5037698745727539
train gradient:  0.14559818508675887
iteration : 5142
train acc:  0.7265625
train loss:  0.4941573441028595
train gradient:  0.12168387764878222
iteration : 5143
train acc:  0.7265625
train loss:  0.5136337876319885
train gradient:  0.14711969613090278
iteration : 5144
train acc:  0.7421875
train loss:  0.5310943126678467
train gradient:  0.15924606042696693
iteration : 5145
train acc:  0.734375
train loss:  0.506671667098999
train gradient:  0.15929304162077906
iteration : 5146
train acc:  0.7578125
train loss:  0.4705456495285034
train gradient:  0.15246512004675378
iteration : 5147
train acc:  0.75
train loss:  0.47011029720306396
train gradient:  0.14913327604027937
iteration : 5148
train acc:  0.6640625
train loss:  0.5827575325965881
train gradient:  0.22168306515968283
iteration : 5149
train acc:  0.7109375
train loss:  0.5671269297599792
train gradient:  0.22567238260805123
iteration : 5150
train acc:  0.796875
train loss:  0.4359425902366638
train gradient:  0.15332495985863157
iteration : 5151
train acc:  0.6875
train loss:  0.5823308229446411
train gradient:  0.19239967811077724
iteration : 5152
train acc:  0.7578125
train loss:  0.46845144033432007
train gradient:  0.12798791113245225
iteration : 5153
train acc:  0.734375
train loss:  0.5279785394668579
train gradient:  0.16101027842754756
iteration : 5154
train acc:  0.71875
train loss:  0.5540673732757568
train gradient:  0.15178456994673217
iteration : 5155
train acc:  0.71875
train loss:  0.5053374767303467
train gradient:  0.16215691458500753
iteration : 5156
train acc:  0.734375
train loss:  0.48626038432121277
train gradient:  0.16387687080650382
iteration : 5157
train acc:  0.7109375
train loss:  0.5513006448745728
train gradient:  0.1738496247176683
iteration : 5158
train acc:  0.7734375
train loss:  0.4904766082763672
train gradient:  0.17505464961277822
iteration : 5159
train acc:  0.7421875
train loss:  0.4776532053947449
train gradient:  0.14000902875687993
iteration : 5160
train acc:  0.7265625
train loss:  0.5114895701408386
train gradient:  0.14882985516160468
iteration : 5161
train acc:  0.65625
train loss:  0.5938241481781006
train gradient:  0.18711358617684729
iteration : 5162
train acc:  0.7421875
train loss:  0.4803089201450348
train gradient:  0.17358351623049356
iteration : 5163
train acc:  0.7265625
train loss:  0.5985598564147949
train gradient:  0.2999798754945541
iteration : 5164
train acc:  0.7265625
train loss:  0.5337613821029663
train gradient:  0.16804093313579732
iteration : 5165
train acc:  0.7734375
train loss:  0.4891323447227478
train gradient:  0.16488844589657953
iteration : 5166
train acc:  0.7421875
train loss:  0.49046963453292847
train gradient:  0.14927342472225846
iteration : 5167
train acc:  0.765625
train loss:  0.489694744348526
train gradient:  0.1452006041300693
iteration : 5168
train acc:  0.765625
train loss:  0.4374176859855652
train gradient:  0.10339545255899849
iteration : 5169
train acc:  0.7734375
train loss:  0.49388328194618225
train gradient:  0.12857309569896802
iteration : 5170
train acc:  0.8203125
train loss:  0.44011828303337097
train gradient:  0.11979921429010201
iteration : 5171
train acc:  0.734375
train loss:  0.5202056169509888
train gradient:  0.1528212951593126
iteration : 5172
train acc:  0.796875
train loss:  0.4606764018535614
train gradient:  0.18144861313228108
iteration : 5173
train acc:  0.7578125
train loss:  0.4923133850097656
train gradient:  0.15629488634359492
iteration : 5174
train acc:  0.7109375
train loss:  0.5137497186660767
train gradient:  0.15838850049054315
iteration : 5175
train acc:  0.6796875
train loss:  0.5375098586082458
train gradient:  0.13776498900706044
iteration : 5176
train acc:  0.75
train loss:  0.4795428514480591
train gradient:  0.14977159408242186
iteration : 5177
train acc:  0.7734375
train loss:  0.46097037196159363
train gradient:  0.14702540578118461
iteration : 5178
train acc:  0.6875
train loss:  0.5799837112426758
train gradient:  0.20833015250038595
iteration : 5179
train acc:  0.71875
train loss:  0.5248037576675415
train gradient:  0.23987596981814352
iteration : 5180
train acc:  0.703125
train loss:  0.5277314186096191
train gradient:  0.159949913715314
iteration : 5181
train acc:  0.78125
train loss:  0.4947161078453064
train gradient:  0.17287491277471811
iteration : 5182
train acc:  0.7734375
train loss:  0.46265602111816406
train gradient:  0.1737947772661313
iteration : 5183
train acc:  0.765625
train loss:  0.5016416907310486
train gradient:  0.14677333362698933
iteration : 5184
train acc:  0.75
train loss:  0.4915817379951477
train gradient:  0.149624253291302
iteration : 5185
train acc:  0.6953125
train loss:  0.5779175162315369
train gradient:  0.1796825714192473
iteration : 5186
train acc:  0.7109375
train loss:  0.5247759819030762
train gradient:  0.15901678865357702
iteration : 5187
train acc:  0.765625
train loss:  0.5207188129425049
train gradient:  0.12396214938906726
iteration : 5188
train acc:  0.703125
train loss:  0.5613677501678467
train gradient:  0.1591098148492312
iteration : 5189
train acc:  0.78125
train loss:  0.46534809470176697
train gradient:  0.17206383278048445
iteration : 5190
train acc:  0.71875
train loss:  0.4716048836708069
train gradient:  0.1306866231193538
iteration : 5191
train acc:  0.7421875
train loss:  0.48410743474960327
train gradient:  0.1218652282772495
iteration : 5192
train acc:  0.65625
train loss:  0.5411638021469116
train gradient:  0.16451991994617904
iteration : 5193
train acc:  0.75
train loss:  0.4480138123035431
train gradient:  0.10774385433869571
iteration : 5194
train acc:  0.75
train loss:  0.48946088552474976
train gradient:  0.14692847228374636
iteration : 5195
train acc:  0.7578125
train loss:  0.4857702851295471
train gradient:  0.13594325308837218
iteration : 5196
train acc:  0.6484375
train loss:  0.6324924230575562
train gradient:  0.17444051422029888
iteration : 5197
train acc:  0.703125
train loss:  0.5038290023803711
train gradient:  0.24019867800550987
iteration : 5198
train acc:  0.7578125
train loss:  0.49994269013404846
train gradient:  0.13213920684200334
iteration : 5199
train acc:  0.7421875
train loss:  0.5527681112289429
train gradient:  0.19991568255507264
iteration : 5200
train acc:  0.71875
train loss:  0.5399980545043945
train gradient:  0.16245724605010597
iteration : 5201
train acc:  0.765625
train loss:  0.40713536739349365
train gradient:  0.09580519669391177
iteration : 5202
train acc:  0.7578125
train loss:  0.46244674921035767
train gradient:  0.11896885463100669
iteration : 5203
train acc:  0.6875
train loss:  0.5477451682090759
train gradient:  0.20098592022854186
iteration : 5204
train acc:  0.703125
train loss:  0.5195308923721313
train gradient:  0.1734901485961076
iteration : 5205
train acc:  0.703125
train loss:  0.5508744716644287
train gradient:  0.18783626767488504
iteration : 5206
train acc:  0.75
train loss:  0.49476325511932373
train gradient:  0.19089538080253665
iteration : 5207
train acc:  0.6875
train loss:  0.5466224551200867
train gradient:  0.17475488786962112
iteration : 5208
train acc:  0.6796875
train loss:  0.5297722816467285
train gradient:  0.1476010796169296
iteration : 5209
train acc:  0.78125
train loss:  0.4821465015411377
train gradient:  0.13232571467179982
iteration : 5210
train acc:  0.6640625
train loss:  0.598453938961029
train gradient:  0.2576530771134246
iteration : 5211
train acc:  0.7265625
train loss:  0.5378304719924927
train gradient:  0.1437538573369577
iteration : 5212
train acc:  0.734375
train loss:  0.49372705817222595
train gradient:  0.17438687665742714
iteration : 5213
train acc:  0.75
train loss:  0.48714855313301086
train gradient:  0.1635618224403922
iteration : 5214
train acc:  0.7265625
train loss:  0.5135204195976257
train gradient:  0.14789001798112578
iteration : 5215
train acc:  0.7109375
train loss:  0.5288376808166504
train gradient:  0.1396936303854509
iteration : 5216
train acc:  0.8046875
train loss:  0.4791656732559204
train gradient:  0.15170809715944963
iteration : 5217
train acc:  0.65625
train loss:  0.5816230773925781
train gradient:  0.15525681336344876
iteration : 5218
train acc:  0.640625
train loss:  0.5465602874755859
train gradient:  0.19714602021582772
iteration : 5219
train acc:  0.7421875
train loss:  0.518492579460144
train gradient:  0.1788267331060477
iteration : 5220
train acc:  0.734375
train loss:  0.5356848239898682
train gradient:  0.15296701902562548
iteration : 5221
train acc:  0.7578125
train loss:  0.5526748895645142
train gradient:  0.18868630493939767
iteration : 5222
train acc:  0.7421875
train loss:  0.510940670967102
train gradient:  0.1670054933985332
iteration : 5223
train acc:  0.796875
train loss:  0.4783760607242584
train gradient:  0.14857552714538194
iteration : 5224
train acc:  0.71875
train loss:  0.5209640264511108
train gradient:  0.1676907399134276
iteration : 5225
train acc:  0.71875
train loss:  0.4810161590576172
train gradient:  0.14246489188254413
iteration : 5226
train acc:  0.7578125
train loss:  0.4884515106678009
train gradient:  0.1527294361925696
iteration : 5227
train acc:  0.765625
train loss:  0.5127341747283936
train gradient:  0.15262597950167966
iteration : 5228
train acc:  0.75
train loss:  0.4842243492603302
train gradient:  0.13470097718853766
iteration : 5229
train acc:  0.7265625
train loss:  0.5207260847091675
train gradient:  0.1444627823570394
iteration : 5230
train acc:  0.6953125
train loss:  0.5551527142524719
train gradient:  0.16327975335681466
iteration : 5231
train acc:  0.7734375
train loss:  0.40937450528144836
train gradient:  0.12071887402277454
iteration : 5232
train acc:  0.6640625
train loss:  0.6004445552825928
train gradient:  0.17256481786288244
iteration : 5233
train acc:  0.7578125
train loss:  0.476009339094162
train gradient:  0.1257351927486945
iteration : 5234
train acc:  0.7578125
train loss:  0.47800642251968384
train gradient:  0.13563337465682362
iteration : 5235
train acc:  0.765625
train loss:  0.48845770955085754
train gradient:  0.1330161363043879
iteration : 5236
train acc:  0.7890625
train loss:  0.4424924850463867
train gradient:  0.12266708730931215
iteration : 5237
train acc:  0.75
train loss:  0.4574049711227417
train gradient:  0.13736619591201793
iteration : 5238
train acc:  0.703125
train loss:  0.5560067892074585
train gradient:  0.17169970459367648
iteration : 5239
train acc:  0.6484375
train loss:  0.5807451009750366
train gradient:  0.21231359792040694
iteration : 5240
train acc:  0.734375
train loss:  0.5079027414321899
train gradient:  0.18945014415801903
iteration : 5241
train acc:  0.7265625
train loss:  0.4981887936592102
train gradient:  0.14478949850617417
iteration : 5242
train acc:  0.7109375
train loss:  0.5271502733230591
train gradient:  0.15202969898657537
iteration : 5243
train acc:  0.71875
train loss:  0.5574290752410889
train gradient:  0.19261109859981423
iteration : 5244
train acc:  0.71875
train loss:  0.5296435356140137
train gradient:  0.16573618067996254
iteration : 5245
train acc:  0.703125
train loss:  0.5138694047927856
train gradient:  0.1675024793863383
iteration : 5246
train acc:  0.78125
train loss:  0.4679861068725586
train gradient:  0.10354200473669921
iteration : 5247
train acc:  0.7265625
train loss:  0.5036386251449585
train gradient:  0.1360813888926331
iteration : 5248
train acc:  0.75
train loss:  0.5035609006881714
train gradient:  0.1377746318027547
iteration : 5249
train acc:  0.7734375
train loss:  0.5027294754981995
train gradient:  0.12330941235116835
iteration : 5250
train acc:  0.71875
train loss:  0.504695475101471
train gradient:  0.1424176567995113
iteration : 5251
train acc:  0.6015625
train loss:  0.6378030180931091
train gradient:  0.24696155123705305
iteration : 5252
train acc:  0.7734375
train loss:  0.46642953157424927
train gradient:  0.15215626640257052
iteration : 5253
train acc:  0.75
train loss:  0.5039519667625427
train gradient:  0.16568404567097278
iteration : 5254
train acc:  0.78125
train loss:  0.4749245047569275
train gradient:  0.13303055805099143
iteration : 5255
train acc:  0.796875
train loss:  0.4418967366218567
train gradient:  0.1375702467676561
iteration : 5256
train acc:  0.671875
train loss:  0.5628684759140015
train gradient:  0.1650854091625885
iteration : 5257
train acc:  0.7421875
train loss:  0.563961923122406
train gradient:  0.1723811714738987
iteration : 5258
train acc:  0.75
train loss:  0.5255740880966187
train gradient:  0.14533904237941142
iteration : 5259
train acc:  0.765625
train loss:  0.462158739566803
train gradient:  0.1669095666181504
iteration : 5260
train acc:  0.7734375
train loss:  0.47346189618110657
train gradient:  0.1379121478112173
iteration : 5261
train acc:  0.8203125
train loss:  0.44224873185157776
train gradient:  0.1419881778528404
iteration : 5262
train acc:  0.765625
train loss:  0.4590107202529907
train gradient:  0.15113898698373934
iteration : 5263
train acc:  0.7109375
train loss:  0.5179226398468018
train gradient:  0.1419963779341048
iteration : 5264
train acc:  0.6953125
train loss:  0.5409916043281555
train gradient:  0.2337335481441418
iteration : 5265
train acc:  0.703125
train loss:  0.5388915538787842
train gradient:  0.17441463112641195
iteration : 5266
train acc:  0.7109375
train loss:  0.5176530480384827
train gradient:  0.1618944081843544
iteration : 5267
train acc:  0.75
train loss:  0.4856230914592743
train gradient:  0.13416899552694195
iteration : 5268
train acc:  0.75
train loss:  0.4853355586528778
train gradient:  0.13139664105318571
iteration : 5269
train acc:  0.671875
train loss:  0.5640403032302856
train gradient:  0.2253344822232442
iteration : 5270
train acc:  0.8125
train loss:  0.4278867840766907
train gradient:  0.12358443769642997
iteration : 5271
train acc:  0.765625
train loss:  0.5162150859832764
train gradient:  0.13810745088430923
iteration : 5272
train acc:  0.765625
train loss:  0.4554758667945862
train gradient:  0.145982976975989
iteration : 5273
train acc:  0.6953125
train loss:  0.52140212059021
train gradient:  0.1301373115784441
iteration : 5274
train acc:  0.734375
train loss:  0.49706512689590454
train gradient:  0.1452112505081211
iteration : 5275
train acc:  0.7265625
train loss:  0.5177280902862549
train gradient:  0.1470715296358952
iteration : 5276
train acc:  0.765625
train loss:  0.48306742310523987
train gradient:  0.1450979777294999
iteration : 5277
train acc:  0.7109375
train loss:  0.5095274448394775
train gradient:  0.15571108280984905
iteration : 5278
train acc:  0.7421875
train loss:  0.48384180665016174
train gradient:  0.13394283234963827
iteration : 5279
train acc:  0.78125
train loss:  0.4589254856109619
train gradient:  0.14802869628905815
iteration : 5280
train acc:  0.6484375
train loss:  0.5642776489257812
train gradient:  0.20641247355904008
iteration : 5281
train acc:  0.78125
train loss:  0.443499356508255
train gradient:  0.1388002602170535
iteration : 5282
train acc:  0.75
train loss:  0.47366246581077576
train gradient:  0.16744460780734033
iteration : 5283
train acc:  0.765625
train loss:  0.48448312282562256
train gradient:  0.1336185010985472
iteration : 5284
train acc:  0.765625
train loss:  0.5109103918075562
train gradient:  0.18671843686607298
iteration : 5285
train acc:  0.7265625
train loss:  0.49122709035873413
train gradient:  0.12912212390180555
iteration : 5286
train acc:  0.7734375
train loss:  0.44897109270095825
train gradient:  0.11704846835113179
iteration : 5287
train acc:  0.765625
train loss:  0.494836688041687
train gradient:  0.16747425630329688
iteration : 5288
train acc:  0.7109375
train loss:  0.5031994581222534
train gradient:  0.14731011871604588
iteration : 5289
train acc:  0.7421875
train loss:  0.45923084020614624
train gradient:  0.1326728346731009
iteration : 5290
train acc:  0.7109375
train loss:  0.5356191396713257
train gradient:  0.18988468912698214
iteration : 5291
train acc:  0.765625
train loss:  0.48956525325775146
train gradient:  0.15384670627317454
iteration : 5292
train acc:  0.6953125
train loss:  0.5621125102043152
train gradient:  0.17002087224308526
iteration : 5293
train acc:  0.7890625
train loss:  0.44694364070892334
train gradient:  0.10995272040256772
iteration : 5294
train acc:  0.703125
train loss:  0.5682341456413269
train gradient:  0.22145951058070673
iteration : 5295
train acc:  0.765625
train loss:  0.5075146555900574
train gradient:  0.18498540427929833
iteration : 5296
train acc:  0.765625
train loss:  0.4780937731266022
train gradient:  0.14776242114750926
iteration : 5297
train acc:  0.765625
train loss:  0.5117799639701843
train gradient:  0.13684289129190866
iteration : 5298
train acc:  0.703125
train loss:  0.549559473991394
train gradient:  0.20195138686093692
iteration : 5299
train acc:  0.71875
train loss:  0.48351171612739563
train gradient:  0.12678351207452304
iteration : 5300
train acc:  0.75
train loss:  0.4853242039680481
train gradient:  0.12421299188573623
iteration : 5301
train acc:  0.7265625
train loss:  0.5007093548774719
train gradient:  0.21241995863101804
iteration : 5302
train acc:  0.765625
train loss:  0.495033323764801
train gradient:  0.17894900791157387
iteration : 5303
train acc:  0.734375
train loss:  0.5372053384780884
train gradient:  0.1502534242902632
iteration : 5304
train acc:  0.6953125
train loss:  0.5318606495857239
train gradient:  0.22625956116170776
iteration : 5305
train acc:  0.7109375
train loss:  0.5863155126571655
train gradient:  0.23050416011991806
iteration : 5306
train acc:  0.7734375
train loss:  0.4759502112865448
train gradient:  0.15202979473301154
iteration : 5307
train acc:  0.734375
train loss:  0.5163487195968628
train gradient:  0.1660776757020678
iteration : 5308
train acc:  0.6640625
train loss:  0.5296117663383484
train gradient:  0.1947704217334125
iteration : 5309
train acc:  0.7421875
train loss:  0.5391527414321899
train gradient:  0.15990069221126635
iteration : 5310
train acc:  0.703125
train loss:  0.5487688779830933
train gradient:  0.20792002431581436
iteration : 5311
train acc:  0.7421875
train loss:  0.5177144408226013
train gradient:  0.1588159917124288
iteration : 5312
train acc:  0.765625
train loss:  0.47408217191696167
train gradient:  0.1756258236997596
iteration : 5313
train acc:  0.7578125
train loss:  0.4720861315727234
train gradient:  0.1312188678031148
iteration : 5314
train acc:  0.75
train loss:  0.5162062644958496
train gradient:  0.16414389496747167
iteration : 5315
train acc:  0.75
train loss:  0.5373927354812622
train gradient:  0.15111257148981994
iteration : 5316
train acc:  0.796875
train loss:  0.44045761227607727
train gradient:  0.12846684263732383
iteration : 5317
train acc:  0.7578125
train loss:  0.4944773316383362
train gradient:  0.1392594960442714
iteration : 5318
train acc:  0.7265625
train loss:  0.5909336805343628
train gradient:  0.20403128156709927
iteration : 5319
train acc:  0.75
train loss:  0.47249269485473633
train gradient:  0.1220331627385243
iteration : 5320
train acc:  0.7421875
train loss:  0.47631096839904785
train gradient:  0.14936773014132304
iteration : 5321
train acc:  0.71875
train loss:  0.539435863494873
train gradient:  0.19153213978680778
iteration : 5322
train acc:  0.8046875
train loss:  0.44649630784988403
train gradient:  0.1471249075736085
iteration : 5323
train acc:  0.75
train loss:  0.47898492217063904
train gradient:  0.13728381010931343
iteration : 5324
train acc:  0.7890625
train loss:  0.4607640504837036
train gradient:  0.12076507790959125
iteration : 5325
train acc:  0.7578125
train loss:  0.5133339762687683
train gradient:  0.2105182762061052
iteration : 5326
train acc:  0.6875
train loss:  0.5434356927871704
train gradient:  0.17097118310558174
iteration : 5327
train acc:  0.6953125
train loss:  0.5820355415344238
train gradient:  0.2016863223759623
iteration : 5328
train acc:  0.765625
train loss:  0.4988164007663727
train gradient:  0.189167943360528
iteration : 5329
train acc:  0.7109375
train loss:  0.5335355401039124
train gradient:  0.18648931757351217
iteration : 5330
train acc:  0.6953125
train loss:  0.5832049250602722
train gradient:  0.20403932009364636
iteration : 5331
train acc:  0.703125
train loss:  0.5155349969863892
train gradient:  0.1685339257187327
iteration : 5332
train acc:  0.765625
train loss:  0.4505888521671295
train gradient:  0.1401672386607565
iteration : 5333
train acc:  0.6953125
train loss:  0.6077404022216797
train gradient:  0.19913746903694504
iteration : 5334
train acc:  0.7265625
train loss:  0.5563192367553711
train gradient:  0.16620330135075867
iteration : 5335
train acc:  0.6015625
train loss:  0.5943896174430847
train gradient:  0.1680415613178297
iteration : 5336
train acc:  0.734375
train loss:  0.49844658374786377
train gradient:  0.152250865060218
iteration : 5337
train acc:  0.796875
train loss:  0.4775508642196655
train gradient:  0.13145709385119497
iteration : 5338
train acc:  0.796875
train loss:  0.4942774176597595
train gradient:  0.1465247322483687
iteration : 5339
train acc:  0.703125
train loss:  0.532091498374939
train gradient:  0.18327907985049158
iteration : 5340
train acc:  0.6953125
train loss:  0.5321935415267944
train gradient:  0.17876391230931354
iteration : 5341
train acc:  0.7578125
train loss:  0.5042908191680908
train gradient:  0.13293116622032772
iteration : 5342
train acc:  0.734375
train loss:  0.4967362880706787
train gradient:  0.19235410437581452
iteration : 5343
train acc:  0.71875
train loss:  0.5513495206832886
train gradient:  0.13629298846454013
iteration : 5344
train acc:  0.7578125
train loss:  0.49629196524620056
train gradient:  0.1524532771439467
iteration : 5345
train acc:  0.7421875
train loss:  0.4802148938179016
train gradient:  0.1437038952758798
iteration : 5346
train acc:  0.6953125
train loss:  0.5255268812179565
train gradient:  0.1914187127578461
iteration : 5347
train acc:  0.7421875
train loss:  0.5163552761077881
train gradient:  0.15222374578586031
iteration : 5348
train acc:  0.7265625
train loss:  0.4906524121761322
train gradient:  0.12661205271181686
iteration : 5349
train acc:  0.7734375
train loss:  0.4651997983455658
train gradient:  0.1231568344348789
iteration : 5350
train acc:  0.7109375
train loss:  0.5263956189155579
train gradient:  0.1808715478296472
iteration : 5351
train acc:  0.78125
train loss:  0.49026262760162354
train gradient:  0.16770077516251158
iteration : 5352
train acc:  0.6640625
train loss:  0.5296411514282227
train gradient:  0.14544744753773772
iteration : 5353
train acc:  0.6796875
train loss:  0.5300372242927551
train gradient:  0.20034709549162233
iteration : 5354
train acc:  0.7265625
train loss:  0.5293576121330261
train gradient:  0.17770675628037366
iteration : 5355
train acc:  0.7578125
train loss:  0.5045807957649231
train gradient:  0.13671272147852914
iteration : 5356
train acc:  0.6796875
train loss:  0.5891282558441162
train gradient:  0.23997322564863222
iteration : 5357
train acc:  0.671875
train loss:  0.5193241238594055
train gradient:  0.16292282998115032
iteration : 5358
train acc:  0.7265625
train loss:  0.5076661109924316
train gradient:  0.1264881989684805
iteration : 5359
train acc:  0.6953125
train loss:  0.5104497671127319
train gradient:  0.15067335119495906
iteration : 5360
train acc:  0.6875
train loss:  0.5619834065437317
train gradient:  0.16774712543366244
iteration : 5361
train acc:  0.6796875
train loss:  0.5951555967330933
train gradient:  0.20145518288391856
iteration : 5362
train acc:  0.734375
train loss:  0.5104244947433472
train gradient:  0.1799408407093948
iteration : 5363
train acc:  0.7109375
train loss:  0.543285071849823
train gradient:  0.1624340461501972
iteration : 5364
train acc:  0.6796875
train loss:  0.5387899875640869
train gradient:  0.18911261907719262
iteration : 5365
train acc:  0.7578125
train loss:  0.4936756491661072
train gradient:  0.1435973062934099
iteration : 5366
train acc:  0.6953125
train loss:  0.548883318901062
train gradient:  0.15099681515698635
iteration : 5367
train acc:  0.765625
train loss:  0.5551831126213074
train gradient:  0.18511832616878907
iteration : 5368
train acc:  0.6640625
train loss:  0.5498850345611572
train gradient:  0.1357650390502621
iteration : 5369
train acc:  0.75
train loss:  0.45988941192626953
train gradient:  0.1460953784098717
iteration : 5370
train acc:  0.6953125
train loss:  0.5170341730117798
train gradient:  0.18114933422003499
iteration : 5371
train acc:  0.765625
train loss:  0.4772823452949524
train gradient:  0.10759826367214942
iteration : 5372
train acc:  0.78125
train loss:  0.45403990149497986
train gradient:  0.13365705390740118
iteration : 5373
train acc:  0.78125
train loss:  0.43027564883232117
train gradient:  0.12000154195141864
iteration : 5374
train acc:  0.7578125
train loss:  0.5021212100982666
train gradient:  0.15065616532656617
iteration : 5375
train acc:  0.7265625
train loss:  0.5032109022140503
train gradient:  0.20428825002223733
iteration : 5376
train acc:  0.7578125
train loss:  0.5188942551612854
train gradient:  0.12654944321856426
iteration : 5377
train acc:  0.6875
train loss:  0.5289549231529236
train gradient:  0.1765356360013204
iteration : 5378
train acc:  0.7109375
train loss:  0.5311800241470337
train gradient:  0.14365266068764654
iteration : 5379
train acc:  0.7265625
train loss:  0.5081348419189453
train gradient:  0.15880190672578365
iteration : 5380
train acc:  0.765625
train loss:  0.5406654477119446
train gradient:  0.19459157698679472
iteration : 5381
train acc:  0.6796875
train loss:  0.5032844543457031
train gradient:  0.14852152006816932
iteration : 5382
train acc:  0.71875
train loss:  0.6028199791908264
train gradient:  0.16930562764284987
iteration : 5383
train acc:  0.7265625
train loss:  0.5317903757095337
train gradient:  0.14947801004269612
iteration : 5384
train acc:  0.7578125
train loss:  0.4749758839607239
train gradient:  0.13050584796187803
iteration : 5385
train acc:  0.734375
train loss:  0.509306013584137
train gradient:  0.25965511770371474
iteration : 5386
train acc:  0.71875
train loss:  0.4639511704444885
train gradient:  0.1305723179853308
iteration : 5387
train acc:  0.78125
train loss:  0.41671228408813477
train gradient:  0.101082891083815
iteration : 5388
train acc:  0.671875
train loss:  0.5655623078346252
train gradient:  0.18618163941608484
iteration : 5389
train acc:  0.7109375
train loss:  0.49509456753730774
train gradient:  0.1467376601552962
iteration : 5390
train acc:  0.734375
train loss:  0.4852563440799713
train gradient:  0.16091830505407823
iteration : 5391
train acc:  0.734375
train loss:  0.47899824380874634
train gradient:  0.14733803493009445
iteration : 5392
train acc:  0.703125
train loss:  0.5317127108573914
train gradient:  0.14814962199437123
iteration : 5393
train acc:  0.7421875
train loss:  0.5228105783462524
train gradient:  0.1731913559579164
iteration : 5394
train acc:  0.7734375
train loss:  0.42453715205192566
train gradient:  0.10868328881539294
iteration : 5395
train acc:  0.828125
train loss:  0.4138087034225464
train gradient:  0.10990189183453826
iteration : 5396
train acc:  0.6875
train loss:  0.5606948733329773
train gradient:  0.21014707833610247
iteration : 5397
train acc:  0.65625
train loss:  0.5501827001571655
train gradient:  0.13718074405814115
iteration : 5398
train acc:  0.734375
train loss:  0.46697139739990234
train gradient:  0.15922545255211204
iteration : 5399
train acc:  0.7421875
train loss:  0.49130287766456604
train gradient:  0.1593253606079843
iteration : 5400
train acc:  0.7265625
train loss:  0.5118733644485474
train gradient:  0.14385423352634374
iteration : 5401
train acc:  0.7578125
train loss:  0.4716995656490326
train gradient:  0.1255226310473031
iteration : 5402
train acc:  0.7734375
train loss:  0.47649383544921875
train gradient:  0.14856886608700187
iteration : 5403
train acc:  0.7265625
train loss:  0.5401105284690857
train gradient:  0.18118660483829752
iteration : 5404
train acc:  0.71875
train loss:  0.5696732997894287
train gradient:  0.15002087042282436
iteration : 5405
train acc:  0.7109375
train loss:  0.5479778051376343
train gradient:  0.2013514356301917
iteration : 5406
train acc:  0.703125
train loss:  0.5602754950523376
train gradient:  0.18418845569816042
iteration : 5407
train acc:  0.78125
train loss:  0.47886621952056885
train gradient:  0.11196000494886715
iteration : 5408
train acc:  0.71875
train loss:  0.4804452955722809
train gradient:  0.11607206992514384
iteration : 5409
train acc:  0.7109375
train loss:  0.5754504203796387
train gradient:  0.30573670573093176
iteration : 5410
train acc:  0.703125
train loss:  0.4931012988090515
train gradient:  0.1571771221198122
iteration : 5411
train acc:  0.6953125
train loss:  0.5407662391662598
train gradient:  0.20679218890269302
iteration : 5412
train acc:  0.7578125
train loss:  0.480049729347229
train gradient:  0.11919244819700929
iteration : 5413
train acc:  0.765625
train loss:  0.504550576210022
train gradient:  0.1871365531393722
iteration : 5414
train acc:  0.734375
train loss:  0.5352902412414551
train gradient:  0.16293439031603715
iteration : 5415
train acc:  0.765625
train loss:  0.4656946063041687
train gradient:  0.15909531112611675
iteration : 5416
train acc:  0.703125
train loss:  0.5523408651351929
train gradient:  0.19556610628275947
iteration : 5417
train acc:  0.734375
train loss:  0.5475426912307739
train gradient:  0.18952736980144447
iteration : 5418
train acc:  0.7578125
train loss:  0.44900834560394287
train gradient:  0.17231787936325563
iteration : 5419
train acc:  0.703125
train loss:  0.5087283849716187
train gradient:  0.1549558876584724
iteration : 5420
train acc:  0.7578125
train loss:  0.4871996343135834
train gradient:  0.1386104525739232
iteration : 5421
train acc:  0.71875
train loss:  0.5218157768249512
train gradient:  0.14234518264971568
iteration : 5422
train acc:  0.6953125
train loss:  0.5081429481506348
train gradient:  0.1400306381911188
iteration : 5423
train acc:  0.7421875
train loss:  0.497730016708374
train gradient:  0.1509163459897119
iteration : 5424
train acc:  0.7734375
train loss:  0.46805915236473083
train gradient:  0.14004113697885712
iteration : 5425
train acc:  0.671875
train loss:  0.5545837879180908
train gradient:  0.18338797540281876
iteration : 5426
train acc:  0.7109375
train loss:  0.5409004092216492
train gradient:  0.18085153667735732
iteration : 5427
train acc:  0.7421875
train loss:  0.47279804944992065
train gradient:  0.16583867283944606
iteration : 5428
train acc:  0.7109375
train loss:  0.5490322113037109
train gradient:  0.15391929174489022
iteration : 5429
train acc:  0.6875
train loss:  0.5926359295845032
train gradient:  0.32623838054564147
iteration : 5430
train acc:  0.6796875
train loss:  0.52496337890625
train gradient:  0.17497135345315046
iteration : 5431
train acc:  0.7578125
train loss:  0.5331154465675354
train gradient:  0.18489300514621393
iteration : 5432
train acc:  0.75
train loss:  0.4790140390396118
train gradient:  0.141367232929119
iteration : 5433
train acc:  0.703125
train loss:  0.5526136755943298
train gradient:  0.19277477635735624
iteration : 5434
train acc:  0.71875
train loss:  0.5562276840209961
train gradient:  0.21263441937675348
iteration : 5435
train acc:  0.7265625
train loss:  0.5230374336242676
train gradient:  0.16990723542181208
iteration : 5436
train acc:  0.734375
train loss:  0.5195730924606323
train gradient:  0.15703698388808202
iteration : 5437
train acc:  0.734375
train loss:  0.5090344548225403
train gradient:  0.13467328140463442
iteration : 5438
train acc:  0.6640625
train loss:  0.5463922023773193
train gradient:  0.15635005359890491
iteration : 5439
train acc:  0.796875
train loss:  0.4489991068840027
train gradient:  0.13632666175446007
iteration : 5440
train acc:  0.7265625
train loss:  0.5243372917175293
train gradient:  0.1479495910497588
iteration : 5441
train acc:  0.71875
train loss:  0.5499393939971924
train gradient:  0.12256762082027906
iteration : 5442
train acc:  0.7265625
train loss:  0.5179523229598999
train gradient:  0.15008463838417763
iteration : 5443
train acc:  0.7421875
train loss:  0.5084975957870483
train gradient:  0.1230622684314502
iteration : 5444
train acc:  0.6953125
train loss:  0.5755534172058105
train gradient:  0.18468235863963944
iteration : 5445
train acc:  0.703125
train loss:  0.4982443153858185
train gradient:  0.1509506554142749
iteration : 5446
train acc:  0.6875
train loss:  0.5356159210205078
train gradient:  0.19184390419174518
iteration : 5447
train acc:  0.6640625
train loss:  0.5641167759895325
train gradient:  0.14029854479188497
iteration : 5448
train acc:  0.7109375
train loss:  0.5860143899917603
train gradient:  0.17011660703341075
iteration : 5449
train acc:  0.6796875
train loss:  0.5634838342666626
train gradient:  0.16812378370552034
iteration : 5450
train acc:  0.7109375
train loss:  0.5429146885871887
train gradient:  0.1324583777178229
iteration : 5451
train acc:  0.7734375
train loss:  0.4762832522392273
train gradient:  0.1603063976036304
iteration : 5452
train acc:  0.75
train loss:  0.4634851813316345
train gradient:  0.14435039603477523
iteration : 5453
train acc:  0.7109375
train loss:  0.5246598720550537
train gradient:  0.204647417857723
iteration : 5454
train acc:  0.7421875
train loss:  0.4849357604980469
train gradient:  0.16703967980034584
iteration : 5455
train acc:  0.7421875
train loss:  0.5228836536407471
train gradient:  0.16580172003395965
iteration : 5456
train acc:  0.6953125
train loss:  0.5585973262786865
train gradient:  0.17556881413904468
iteration : 5457
train acc:  0.7421875
train loss:  0.4913734197616577
train gradient:  0.15407313038157072
iteration : 5458
train acc:  0.75
train loss:  0.544562578201294
train gradient:  0.21031944145889594
iteration : 5459
train acc:  0.6640625
train loss:  0.5931410789489746
train gradient:  0.3452417262784624
iteration : 5460
train acc:  0.71875
train loss:  0.47489726543426514
train gradient:  0.11338694173317801
iteration : 5461
train acc:  0.6796875
train loss:  0.5993838310241699
train gradient:  0.21302190364411738
iteration : 5462
train acc:  0.703125
train loss:  0.5887241959571838
train gradient:  0.23263870872106698
iteration : 5463
train acc:  0.75
train loss:  0.4890555143356323
train gradient:  0.1409738745248398
iteration : 5464
train acc:  0.7265625
train loss:  0.5189993381500244
train gradient:  0.16463664489661062
iteration : 5465
train acc:  0.734375
train loss:  0.5169748067855835
train gradient:  0.16205001418281745
iteration : 5466
train acc:  0.75
train loss:  0.5264277458190918
train gradient:  0.13026136760024798
iteration : 5467
train acc:  0.75
train loss:  0.4954189360141754
train gradient:  0.13495661869928094
iteration : 5468
train acc:  0.6796875
train loss:  0.5955994129180908
train gradient:  0.1696044376503078
iteration : 5469
train acc:  0.7109375
train loss:  0.5476467609405518
train gradient:  0.23537730315961913
iteration : 5470
train acc:  0.734375
train loss:  0.48552027344703674
train gradient:  0.12385830895050594
iteration : 5471
train acc:  0.765625
train loss:  0.47849777340888977
train gradient:  0.14464363195964766
iteration : 5472
train acc:  0.703125
train loss:  0.5150338411331177
train gradient:  0.13841357023156617
iteration : 5473
train acc:  0.7265625
train loss:  0.50966477394104
train gradient:  0.14990185051123378
iteration : 5474
train acc:  0.6484375
train loss:  0.5819511413574219
train gradient:  0.19082119059491431
iteration : 5475
train acc:  0.671875
train loss:  0.561935544013977
train gradient:  0.16355652708706278
iteration : 5476
train acc:  0.7421875
train loss:  0.5052492618560791
train gradient:  0.1612138765672646
iteration : 5477
train acc:  0.703125
train loss:  0.5671164989471436
train gradient:  0.18550480660826285
iteration : 5478
train acc:  0.7265625
train loss:  0.5374268293380737
train gradient:  0.17831099186603533
iteration : 5479
train acc:  0.765625
train loss:  0.48662757873535156
train gradient:  0.1324353872741123
iteration : 5480
train acc:  0.78125
train loss:  0.4459495544433594
train gradient:  0.13594111777263676
iteration : 5481
train acc:  0.796875
train loss:  0.4841775596141815
train gradient:  0.15560486844780222
iteration : 5482
train acc:  0.75
train loss:  0.5158505439758301
train gradient:  0.12170756964754198
iteration : 5483
train acc:  0.765625
train loss:  0.4514807462692261
train gradient:  0.11722395073088361
iteration : 5484
train acc:  0.7109375
train loss:  0.5415347814559937
train gradient:  0.16065608461575986
iteration : 5485
train acc:  0.625
train loss:  0.6187575459480286
train gradient:  0.23343146456040542
iteration : 5486
train acc:  0.796875
train loss:  0.46755582094192505
train gradient:  0.11493668006317231
iteration : 5487
train acc:  0.734375
train loss:  0.5127184391021729
train gradient:  0.14263299809942423
iteration : 5488
train acc:  0.7421875
train loss:  0.4959230422973633
train gradient:  0.13779011992439014
iteration : 5489
train acc:  0.71875
train loss:  0.5432541966438293
train gradient:  0.19349689356122396
iteration : 5490
train acc:  0.7734375
train loss:  0.4731658697128296
train gradient:  0.1500622538486806
iteration : 5491
train acc:  0.734375
train loss:  0.5388117432594299
train gradient:  0.1449968377738457
iteration : 5492
train acc:  0.8046875
train loss:  0.4155840277671814
train gradient:  0.12083610674642685
iteration : 5493
train acc:  0.71875
train loss:  0.5329959392547607
train gradient:  0.2091838469944425
iteration : 5494
train acc:  0.75
train loss:  0.5184282064437866
train gradient:  0.16356167165916446
iteration : 5495
train acc:  0.7109375
train loss:  0.5092599987983704
train gradient:  0.17250363670832708
iteration : 5496
train acc:  0.65625
train loss:  0.5475014448165894
train gradient:  0.14723642340885784
iteration : 5497
train acc:  0.765625
train loss:  0.4956498146057129
train gradient:  0.14460866675636724
iteration : 5498
train acc:  0.71875
train loss:  0.5608053803443909
train gradient:  0.17833352152325535
iteration : 5499
train acc:  0.75
train loss:  0.5082416534423828
train gradient:  0.15538680819555623
iteration : 5500
train acc:  0.8046875
train loss:  0.4465920329093933
train gradient:  0.12654233822833683
iteration : 5501
train acc:  0.703125
train loss:  0.5246485471725464
train gradient:  0.18106624023721707
iteration : 5502
train acc:  0.7421875
train loss:  0.5079334378242493
train gradient:  0.12950761692329893
iteration : 5503
train acc:  0.7578125
train loss:  0.5168638229370117
train gradient:  0.16045960409653776
iteration : 5504
train acc:  0.8125
train loss:  0.4304983615875244
train gradient:  0.14393424588074716
iteration : 5505
train acc:  0.7265625
train loss:  0.5133704543113708
train gradient:  0.14413755395822742
iteration : 5506
train acc:  0.671875
train loss:  0.506264865398407
train gradient:  0.12778587193069954
iteration : 5507
train acc:  0.75
train loss:  0.48300570249557495
train gradient:  0.16711763856222905
iteration : 5508
train acc:  0.7421875
train loss:  0.53815758228302
train gradient:  0.18152162649224876
iteration : 5509
train acc:  0.7734375
train loss:  0.5048730373382568
train gradient:  0.15301339459987517
iteration : 5510
train acc:  0.7109375
train loss:  0.5158742666244507
train gradient:  0.13746936678888105
iteration : 5511
train acc:  0.7578125
train loss:  0.4761147201061249
train gradient:  0.17175568232355692
iteration : 5512
train acc:  0.765625
train loss:  0.47917675971984863
train gradient:  0.11948022536103078
iteration : 5513
train acc:  0.7578125
train loss:  0.4446967840194702
train gradient:  0.10727295194883293
iteration : 5514
train acc:  0.6953125
train loss:  0.5313294529914856
train gradient:  0.1520803107118949
iteration : 5515
train acc:  0.7421875
train loss:  0.490772545337677
train gradient:  0.14861153180203485
iteration : 5516
train acc:  0.671875
train loss:  0.5123906135559082
train gradient:  0.173320371967068
iteration : 5517
train acc:  0.7109375
train loss:  0.5576111078262329
train gradient:  0.19041622170311032
iteration : 5518
train acc:  0.703125
train loss:  0.5303791761398315
train gradient:  0.1494173010320884
iteration : 5519
train acc:  0.7421875
train loss:  0.5191751718521118
train gradient:  0.13352587806147842
iteration : 5520
train acc:  0.7421875
train loss:  0.4833299517631531
train gradient:  0.10870547953155475
iteration : 5521
train acc:  0.6484375
train loss:  0.6103952527046204
train gradient:  0.20555926729014223
iteration : 5522
train acc:  0.7421875
train loss:  0.5365908145904541
train gradient:  0.14841242563183432
iteration : 5523
train acc:  0.7578125
train loss:  0.49973639845848083
train gradient:  0.12846409541399684
iteration : 5524
train acc:  0.7109375
train loss:  0.4980926215648651
train gradient:  0.16428319941342617
iteration : 5525
train acc:  0.6796875
train loss:  0.5248219966888428
train gradient:  0.14086355787420785
iteration : 5526
train acc:  0.734375
train loss:  0.5141757726669312
train gradient:  0.12077407083128826
iteration : 5527
train acc:  0.6875
train loss:  0.5374370813369751
train gradient:  0.1636798506585453
iteration : 5528
train acc:  0.7421875
train loss:  0.5598928928375244
train gradient:  0.16128382461593152
iteration : 5529
train acc:  0.7578125
train loss:  0.4797201454639435
train gradient:  0.15329249732175124
iteration : 5530
train acc:  0.71875
train loss:  0.48401325941085815
train gradient:  0.12346570396866362
iteration : 5531
train acc:  0.7578125
train loss:  0.4890609085559845
train gradient:  0.11452604115766202
iteration : 5532
train acc:  0.765625
train loss:  0.45245668292045593
train gradient:  0.11382025597655472
iteration : 5533
train acc:  0.7265625
train loss:  0.490342378616333
train gradient:  0.14384313379236896
iteration : 5534
train acc:  0.7421875
train loss:  0.46878671646118164
train gradient:  0.1416982992269395
iteration : 5535
train acc:  0.765625
train loss:  0.43854767084121704
train gradient:  0.13021431110484333
iteration : 5536
train acc:  0.71875
train loss:  0.5293493866920471
train gradient:  0.12562670829983025
iteration : 5537
train acc:  0.6875
train loss:  0.5552105903625488
train gradient:  0.18299714089786062
iteration : 5538
train acc:  0.703125
train loss:  0.5129274725914001
train gradient:  0.18701049831887773
iteration : 5539
train acc:  0.7109375
train loss:  0.48544448614120483
train gradient:  0.1849379685096108
iteration : 5540
train acc:  0.6953125
train loss:  0.4912160038948059
train gradient:  0.13839509613145756
iteration : 5541
train acc:  0.765625
train loss:  0.5398241877555847
train gradient:  0.2034496414163518
iteration : 5542
train acc:  0.75
train loss:  0.47512394189834595
train gradient:  0.12619041216396737
iteration : 5543
train acc:  0.7109375
train loss:  0.560387372970581
train gradient:  0.16913017329017482
iteration : 5544
train acc:  0.765625
train loss:  0.5018887519836426
train gradient:  0.1445955051215974
iteration : 5545
train acc:  0.765625
train loss:  0.4529024362564087
train gradient:  0.12003654628007757
iteration : 5546
train acc:  0.6953125
train loss:  0.5788562297821045
train gradient:  0.17602809906382416
iteration : 5547
train acc:  0.7265625
train loss:  0.5006461143493652
train gradient:  0.1262903246094753
iteration : 5548
train acc:  0.7734375
train loss:  0.48166623711586
train gradient:  0.1903564696394187
iteration : 5549
train acc:  0.671875
train loss:  0.550971508026123
train gradient:  0.15718539935428366
iteration : 5550
train acc:  0.75
train loss:  0.5118997097015381
train gradient:  0.1568669905697086
iteration : 5551
train acc:  0.765625
train loss:  0.5146397352218628
train gradient:  0.13223019118837664
iteration : 5552
train acc:  0.65625
train loss:  0.5916441082954407
train gradient:  0.18567029124625062
iteration : 5553
train acc:  0.78125
train loss:  0.4437568783760071
train gradient:  0.13024098034769438
iteration : 5554
train acc:  0.734375
train loss:  0.5331375598907471
train gradient:  0.14380481438994014
iteration : 5555
train acc:  0.703125
train loss:  0.5197011232376099
train gradient:  0.16141037777285328
iteration : 5556
train acc:  0.71875
train loss:  0.5532796382904053
train gradient:  0.13899295972464004
iteration : 5557
train acc:  0.7734375
train loss:  0.4476047158241272
train gradient:  0.1147054395838405
iteration : 5558
train acc:  0.734375
train loss:  0.5567511916160583
train gradient:  0.15527876428268628
iteration : 5559
train acc:  0.7578125
train loss:  0.4725309908390045
train gradient:  0.15439511035700978
iteration : 5560
train acc:  0.7109375
train loss:  0.5633838176727295
train gradient:  0.17805491283574826
iteration : 5561
train acc:  0.7734375
train loss:  0.4866369664669037
train gradient:  0.12044914001579914
iteration : 5562
train acc:  0.7734375
train loss:  0.4652005434036255
train gradient:  0.12769935047392958
iteration : 5563
train acc:  0.703125
train loss:  0.5398948788642883
train gradient:  0.20723539733997076
iteration : 5564
train acc:  0.7421875
train loss:  0.5388686656951904
train gradient:  0.1592886294631689
iteration : 5565
train acc:  0.765625
train loss:  0.5068473815917969
train gradient:  0.1487826224911517
iteration : 5566
train acc:  0.7265625
train loss:  0.5703548192977905
train gradient:  0.1731256111086084
iteration : 5567
train acc:  0.7265625
train loss:  0.627224326133728
train gradient:  0.25057837800455984
iteration : 5568
train acc:  0.7265625
train loss:  0.514583170413971
train gradient:  0.1581143303723757
iteration : 5569
train acc:  0.7265625
train loss:  0.5469839572906494
train gradient:  0.17634779723574656
iteration : 5570
train acc:  0.78125
train loss:  0.5024032592773438
train gradient:  0.14708927998800964
iteration : 5571
train acc:  0.7265625
train loss:  0.4935153126716614
train gradient:  0.15297888389988637
iteration : 5572
train acc:  0.7265625
train loss:  0.45484885573387146
train gradient:  0.12364472139165679
iteration : 5573
train acc:  0.75
train loss:  0.4816718101501465
train gradient:  0.1131219790320508
iteration : 5574
train acc:  0.8125
train loss:  0.44636070728302
train gradient:  0.12650419145052302
iteration : 5575
train acc:  0.7109375
train loss:  0.516460657119751
train gradient:  0.16592413480415583
iteration : 5576
train acc:  0.6953125
train loss:  0.5695035457611084
train gradient:  0.21364079780713513
iteration : 5577
train acc:  0.7421875
train loss:  0.4895794689655304
train gradient:  0.13907188252774014
iteration : 5578
train acc:  0.734375
train loss:  0.539216935634613
train gradient:  0.15408168260553895
iteration : 5579
train acc:  0.71875
train loss:  0.5323881506919861
train gradient:  0.14753309995051958
iteration : 5580
train acc:  0.6796875
train loss:  0.5783296823501587
train gradient:  0.14029191042956604
iteration : 5581
train acc:  0.734375
train loss:  0.5162984132766724
train gradient:  0.15856786720202065
iteration : 5582
train acc:  0.7265625
train loss:  0.4934804439544678
train gradient:  0.16171191587489994
iteration : 5583
train acc:  0.7578125
train loss:  0.5120184421539307
train gradient:  0.14874619620224758
iteration : 5584
train acc:  0.7734375
train loss:  0.5463054776191711
train gradient:  0.16812094525749743
iteration : 5585
train acc:  0.734375
train loss:  0.5202568769454956
train gradient:  0.16651585714448328
iteration : 5586
train acc:  0.734375
train loss:  0.5358500480651855
train gradient:  0.17728022475846325
iteration : 5587
train acc:  0.6953125
train loss:  0.5483143329620361
train gradient:  0.1579996120255114
iteration : 5588
train acc:  0.6875
train loss:  0.5544902682304382
train gradient:  0.14656617245748477
iteration : 5589
train acc:  0.6484375
train loss:  0.5575826168060303
train gradient:  0.16527706122580244
iteration : 5590
train acc:  0.765625
train loss:  0.49710917472839355
train gradient:  0.14135870552539498
iteration : 5591
train acc:  0.703125
train loss:  0.5007942914962769
train gradient:  0.17415154456055099
iteration : 5592
train acc:  0.7578125
train loss:  0.47583335638046265
train gradient:  0.1498743683423352
iteration : 5593
train acc:  0.7109375
train loss:  0.5183965563774109
train gradient:  0.1614445845477358
iteration : 5594
train acc:  0.6875
train loss:  0.5423725843429565
train gradient:  0.16602121556982538
iteration : 5595
train acc:  0.7421875
train loss:  0.5130437016487122
train gradient:  0.14698246379041513
iteration : 5596
train acc:  0.6796875
train loss:  0.5791336894035339
train gradient:  0.1787048689468927
iteration : 5597
train acc:  0.7421875
train loss:  0.5402109026908875
train gradient:  0.16838050509121413
iteration : 5598
train acc:  0.7734375
train loss:  0.4384751319885254
train gradient:  0.11069972545408477
iteration : 5599
train acc:  0.703125
train loss:  0.4927544891834259
train gradient:  0.12076016042049195
iteration : 5600
train acc:  0.7109375
train loss:  0.4840393364429474
train gradient:  0.18308898970530146
iteration : 5601
train acc:  0.71875
train loss:  0.5226925015449524
train gradient:  0.14681553925658664
iteration : 5602
train acc:  0.6953125
train loss:  0.5460631847381592
train gradient:  0.17177978087293283
iteration : 5603
train acc:  0.7109375
train loss:  0.5581017732620239
train gradient:  0.16153516000424709
iteration : 5604
train acc:  0.734375
train loss:  0.5644032955169678
train gradient:  0.16590069601372703
iteration : 5605
train acc:  0.75
train loss:  0.49570852518081665
train gradient:  0.11545797342534837
iteration : 5606
train acc:  0.734375
train loss:  0.5168200731277466
train gradient:  0.1647322782670948
iteration : 5607
train acc:  0.78125
train loss:  0.4450065493583679
train gradient:  0.10719836864756346
iteration : 5608
train acc:  0.734375
train loss:  0.5383270978927612
train gradient:  0.18209777591829235
iteration : 5609
train acc:  0.7578125
train loss:  0.4661253094673157
train gradient:  0.12016373651575292
iteration : 5610
train acc:  0.703125
train loss:  0.5110936164855957
train gradient:  0.1396327299941821
iteration : 5611
train acc:  0.7421875
train loss:  0.510459303855896
train gradient:  0.13502093741269744
iteration : 5612
train acc:  0.8125
train loss:  0.4353117048740387
train gradient:  0.13494224041479186
iteration : 5613
train acc:  0.796875
train loss:  0.44224613904953003
train gradient:  0.11341502140887125
iteration : 5614
train acc:  0.75
train loss:  0.5154417753219604
train gradient:  0.16026842490688892
iteration : 5615
train acc:  0.7265625
train loss:  0.47434455156326294
train gradient:  0.13065851621058552
iteration : 5616
train acc:  0.734375
train loss:  0.498629629611969
train gradient:  0.15883339007253516
iteration : 5617
train acc:  0.765625
train loss:  0.4724159836769104
train gradient:  0.13921617676437026
iteration : 5618
train acc:  0.7109375
train loss:  0.5255336165428162
train gradient:  0.1379325585743558
iteration : 5619
train acc:  0.7265625
train loss:  0.5464316606521606
train gradient:  0.20311319180278364
iteration : 5620
train acc:  0.765625
train loss:  0.4717598259449005
train gradient:  0.13268774057030175
iteration : 5621
train acc:  0.6796875
train loss:  0.5641759037971497
train gradient:  0.19166599983592153
iteration : 5622
train acc:  0.7734375
train loss:  0.4735720455646515
train gradient:  0.13975172349341008
iteration : 5623
train acc:  0.7734375
train loss:  0.45471182465553284
train gradient:  0.12576791536811682
iteration : 5624
train acc:  0.7578125
train loss:  0.46931278705596924
train gradient:  0.12342394301195261
iteration : 5625
train acc:  0.7109375
train loss:  0.5449190139770508
train gradient:  0.19821204187907177
iteration : 5626
train acc:  0.7265625
train loss:  0.5622115135192871
train gradient:  0.1834416682780971
iteration : 5627
train acc:  0.765625
train loss:  0.5395742058753967
train gradient:  0.23511336162061888
iteration : 5628
train acc:  0.6875
train loss:  0.5383243560791016
train gradient:  0.15476941779434109
iteration : 5629
train acc:  0.734375
train loss:  0.5034350156784058
train gradient:  0.1486778315058816
iteration : 5630
train acc:  0.8046875
train loss:  0.4420730173587799
train gradient:  0.13690999774634297
iteration : 5631
train acc:  0.7421875
train loss:  0.49488013982772827
train gradient:  0.15640074668578402
iteration : 5632
train acc:  0.75
train loss:  0.5153228044509888
train gradient:  0.1445807047149784
iteration : 5633
train acc:  0.7421875
train loss:  0.4949478209018707
train gradient:  0.1448010859959009
iteration : 5634
train acc:  0.7734375
train loss:  0.4433988332748413
train gradient:  0.11309726593744696
iteration : 5635
train acc:  0.734375
train loss:  0.5289920568466187
train gradient:  0.14571838952349347
iteration : 5636
train acc:  0.8125
train loss:  0.42483457922935486
train gradient:  0.10940179697974477
iteration : 5637
train acc:  0.65625
train loss:  0.5792542099952698
train gradient:  0.16684762194805858
iteration : 5638
train acc:  0.7109375
train loss:  0.5392378568649292
train gradient:  0.20959884612324503
iteration : 5639
train acc:  0.71875
train loss:  0.47299110889434814
train gradient:  0.14182989390146444
iteration : 5640
train acc:  0.7421875
train loss:  0.4958842396736145
train gradient:  0.14830273787776838
iteration : 5641
train acc:  0.7578125
train loss:  0.4714146852493286
train gradient:  0.12200649368759385
iteration : 5642
train acc:  0.78125
train loss:  0.47727730870246887
train gradient:  0.16431779613249647
iteration : 5643
train acc:  0.703125
train loss:  0.5551171898841858
train gradient:  0.17786533982428587
iteration : 5644
train acc:  0.71875
train loss:  0.47880426049232483
train gradient:  0.13142322313894844
iteration : 5645
train acc:  0.6875
train loss:  0.5881620049476624
train gradient:  0.3030628643324039
iteration : 5646
train acc:  0.7109375
train loss:  0.4898935854434967
train gradient:  0.15799767086986288
iteration : 5647
train acc:  0.6484375
train loss:  0.6300771832466125
train gradient:  0.270071253602368
iteration : 5648
train acc:  0.8359375
train loss:  0.42917367815971375
train gradient:  0.1372781169918914
iteration : 5649
train acc:  0.765625
train loss:  0.48036491870880127
train gradient:  0.1062197492294238
iteration : 5650
train acc:  0.7421875
train loss:  0.4940929412841797
train gradient:  0.14051114078555524
iteration : 5651
train acc:  0.6875
train loss:  0.5471017360687256
train gradient:  0.2007581627325163
iteration : 5652
train acc:  0.765625
train loss:  0.5022168159484863
train gradient:  0.12825152415437707
iteration : 5653
train acc:  0.7265625
train loss:  0.5390176773071289
train gradient:  0.161995180672023
iteration : 5654
train acc:  0.6953125
train loss:  0.5649691224098206
train gradient:  0.14516615507765449
iteration : 5655
train acc:  0.7109375
train loss:  0.5473398566246033
train gradient:  0.1885216920982269
iteration : 5656
train acc:  0.7890625
train loss:  0.40662944316864014
train gradient:  0.11504955088119177
iteration : 5657
train acc:  0.7734375
train loss:  0.47717994451522827
train gradient:  0.12843719250238683
iteration : 5658
train acc:  0.6640625
train loss:  0.5798994898796082
train gradient:  0.1592247722709821
iteration : 5659
train acc:  0.7578125
train loss:  0.5228753089904785
train gradient:  0.17658563635279806
iteration : 5660
train acc:  0.7421875
train loss:  0.49572038650512695
train gradient:  0.16162273370784308
iteration : 5661
train acc:  0.734375
train loss:  0.5170589089393616
train gradient:  0.18765075834196215
iteration : 5662
train acc:  0.71875
train loss:  0.5601785182952881
train gradient:  0.16578961230250414
iteration : 5663
train acc:  0.7578125
train loss:  0.5313589572906494
train gradient:  0.15559431876548716
iteration : 5664
train acc:  0.75
train loss:  0.49429166316986084
train gradient:  0.144528854895966
iteration : 5665
train acc:  0.7265625
train loss:  0.4976557493209839
train gradient:  0.16083260210648548
iteration : 5666
train acc:  0.8046875
train loss:  0.4734582006931305
train gradient:  0.15956428461527497
iteration : 5667
train acc:  0.7421875
train loss:  0.5280643105506897
train gradient:  0.15031653818573104
iteration : 5668
train acc:  0.6796875
train loss:  0.5646357536315918
train gradient:  0.20308595489453285
iteration : 5669
train acc:  0.71875
train loss:  0.5731865167617798
train gradient:  0.16730148807515932
iteration : 5670
train acc:  0.671875
train loss:  0.6376060247421265
train gradient:  0.2640456599495375
iteration : 5671
train acc:  0.6796875
train loss:  0.5579330325126648
train gradient:  0.21196875698247147
iteration : 5672
train acc:  0.7109375
train loss:  0.5351711511611938
train gradient:  0.1537790823793786
iteration : 5673
train acc:  0.7578125
train loss:  0.4754372537136078
train gradient:  0.10471494073451222
iteration : 5674
train acc:  0.734375
train loss:  0.4743318557739258
train gradient:  0.14502219322999366
iteration : 5675
train acc:  0.7734375
train loss:  0.4747667908668518
train gradient:  0.14516132491601652
iteration : 5676
train acc:  0.6640625
train loss:  0.5330925583839417
train gradient:  0.19765770206313577
iteration : 5677
train acc:  0.6875
train loss:  0.5699016451835632
train gradient:  0.1717634822660772
iteration : 5678
train acc:  0.65625
train loss:  0.6402003765106201
train gradient:  0.1708646151955361
iteration : 5679
train acc:  0.7734375
train loss:  0.46803319454193115
train gradient:  0.1588332636996747
iteration : 5680
train acc:  0.71875
train loss:  0.5652849674224854
train gradient:  0.17035492305418298
iteration : 5681
train acc:  0.6796875
train loss:  0.5729050040245056
train gradient:  0.1607978440488685
iteration : 5682
train acc:  0.7265625
train loss:  0.5030504465103149
train gradient:  0.18291174031418062
iteration : 5683
train acc:  0.6953125
train loss:  0.5202623605728149
train gradient:  0.1202343715780465
iteration : 5684
train acc:  0.734375
train loss:  0.513043224811554
train gradient:  0.12556260048388238
iteration : 5685
train acc:  0.6875
train loss:  0.5802851915359497
train gradient:  0.2231991560255851
iteration : 5686
train acc:  0.7109375
train loss:  0.5372043251991272
train gradient:  0.1921773172335131
iteration : 5687
train acc:  0.7734375
train loss:  0.4469795823097229
train gradient:  0.12850219188771983
iteration : 5688
train acc:  0.703125
train loss:  0.5339198112487793
train gradient:  0.15581508034051347
iteration : 5689
train acc:  0.7265625
train loss:  0.5365042686462402
train gradient:  0.13443975758386045
iteration : 5690
train acc:  0.6953125
train loss:  0.5118679404258728
train gradient:  0.14680699335623942
iteration : 5691
train acc:  0.703125
train loss:  0.5503109693527222
train gradient:  0.1383711873281222
iteration : 5692
train acc:  0.7265625
train loss:  0.5229711532592773
train gradient:  0.10925800194107646
iteration : 5693
train acc:  0.75
train loss:  0.5319586992263794
train gradient:  0.16570807752488112
iteration : 5694
train acc:  0.78125
train loss:  0.4500541687011719
train gradient:  0.13724608312503955
iteration : 5695
train acc:  0.796875
train loss:  0.4925997853279114
train gradient:  0.1169935312713663
iteration : 5696
train acc:  0.7578125
train loss:  0.4643218517303467
train gradient:  0.09812151062874497
iteration : 5697
train acc:  0.765625
train loss:  0.4969148635864258
train gradient:  0.12482633103229324
iteration : 5698
train acc:  0.734375
train loss:  0.49972057342529297
train gradient:  0.1361502767801047
iteration : 5699
train acc:  0.7265625
train loss:  0.4918051064014435
train gradient:  0.13666310980186247
iteration : 5700
train acc:  0.796875
train loss:  0.46684932708740234
train gradient:  0.12308786886749724
iteration : 5701
train acc:  0.8359375
train loss:  0.4578413665294647
train gradient:  0.1263596909218719
iteration : 5702
train acc:  0.7890625
train loss:  0.4615156650543213
train gradient:  0.12189933962448964
iteration : 5703
train acc:  0.7421875
train loss:  0.5190483927726746
train gradient:  0.1369363712361233
iteration : 5704
train acc:  0.765625
train loss:  0.4798072278499603
train gradient:  0.18410195877688712
iteration : 5705
train acc:  0.796875
train loss:  0.45232731103897095
train gradient:  0.15318161977284372
iteration : 5706
train acc:  0.7890625
train loss:  0.45047199726104736
train gradient:  0.1289720313420616
iteration : 5707
train acc:  0.7578125
train loss:  0.4880672097206116
train gradient:  0.1056366427267253
iteration : 5708
train acc:  0.75
train loss:  0.5229392051696777
train gradient:  0.1640174849036364
iteration : 5709
train acc:  0.6796875
train loss:  0.530107855796814
train gradient:  0.16130088941121096
iteration : 5710
train acc:  0.7578125
train loss:  0.49808576703071594
train gradient:  0.12436022387554017
iteration : 5711
train acc:  0.6484375
train loss:  0.5984092950820923
train gradient:  0.2981160576170042
iteration : 5712
train acc:  0.765625
train loss:  0.5071259140968323
train gradient:  0.15695755909350906
iteration : 5713
train acc:  0.7734375
train loss:  0.49171918630599976
train gradient:  0.16751720312385876
iteration : 5714
train acc:  0.7265625
train loss:  0.45907506346702576
train gradient:  0.12298171673708426
iteration : 5715
train acc:  0.6640625
train loss:  0.5558878183364868
train gradient:  0.13204770586417902
iteration : 5716
train acc:  0.7109375
train loss:  0.5513094067573547
train gradient:  0.16863657563518825
iteration : 5717
train acc:  0.75
train loss:  0.5289108753204346
train gradient:  0.15828116281985105
iteration : 5718
train acc:  0.625
train loss:  0.5993080735206604
train gradient:  0.17034479363690458
iteration : 5719
train acc:  0.6484375
train loss:  0.5855153799057007
train gradient:  0.22913187695546677
iteration : 5720
train acc:  0.703125
train loss:  0.5394726395606995
train gradient:  0.16701504009660256
iteration : 5721
train acc:  0.71875
train loss:  0.47973620891571045
train gradient:  0.12113381505863297
iteration : 5722
train acc:  0.78125
train loss:  0.46350765228271484
train gradient:  0.10746318886808738
iteration : 5723
train acc:  0.7421875
train loss:  0.5256194472312927
train gradient:  0.17817495876410427
iteration : 5724
train acc:  0.6484375
train loss:  0.5827077627182007
train gradient:  0.19330028138658367
iteration : 5725
train acc:  0.703125
train loss:  0.5049087405204773
train gradient:  0.148483790855858
iteration : 5726
train acc:  0.71875
train loss:  0.5240625143051147
train gradient:  0.12384789978070182
iteration : 5727
train acc:  0.765625
train loss:  0.4764084815979004
train gradient:  0.12352542981814899
iteration : 5728
train acc:  0.8359375
train loss:  0.4221591055393219
train gradient:  0.13490553527425525
iteration : 5729
train acc:  0.7578125
train loss:  0.5029466152191162
train gradient:  0.1440919365096088
iteration : 5730
train acc:  0.71875
train loss:  0.4759054183959961
train gradient:  0.1355568673728177
iteration : 5731
train acc:  0.7734375
train loss:  0.498007208108902
train gradient:  0.1415802877965388
iteration : 5732
train acc:  0.6484375
train loss:  0.5868114233016968
train gradient:  0.18055373666254998
iteration : 5733
train acc:  0.71875
train loss:  0.5088045001029968
train gradient:  0.17771524155749602
iteration : 5734
train acc:  0.75
train loss:  0.46432310342788696
train gradient:  0.13132714718613736
iteration : 5735
train acc:  0.7734375
train loss:  0.48255378007888794
train gradient:  0.1310939672242326
iteration : 5736
train acc:  0.7578125
train loss:  0.5678128004074097
train gradient:  0.21330433160257845
iteration : 5737
train acc:  0.78125
train loss:  0.4466986060142517
train gradient:  0.12220830912870755
iteration : 5738
train acc:  0.765625
train loss:  0.4881860911846161
train gradient:  0.18501515855325695
iteration : 5739
train acc:  0.6796875
train loss:  0.5752077698707581
train gradient:  0.18503671962111556
iteration : 5740
train acc:  0.6640625
train loss:  0.6419948935508728
train gradient:  0.21449562324162477
iteration : 5741
train acc:  0.6875
train loss:  0.5599559545516968
train gradient:  0.19053717866619976
iteration : 5742
train acc:  0.6640625
train loss:  0.5844900012016296
train gradient:  0.17453492452251074
iteration : 5743
train acc:  0.7578125
train loss:  0.47768962383270264
train gradient:  0.1349759588750412
iteration : 5744
train acc:  0.734375
train loss:  0.4903833866119385
train gradient:  0.16237557317848883
iteration : 5745
train acc:  0.734375
train loss:  0.5163401365280151
train gradient:  0.156327509195081
iteration : 5746
train acc:  0.6953125
train loss:  0.5205065011978149
train gradient:  0.1372101779375325
iteration : 5747
train acc:  0.765625
train loss:  0.4572739601135254
train gradient:  0.12084811901003763
iteration : 5748
train acc:  0.8203125
train loss:  0.4630317986011505
train gradient:  0.12414671075121816
iteration : 5749
train acc:  0.734375
train loss:  0.49937885999679565
train gradient:  0.17878309090703287
iteration : 5750
train acc:  0.7578125
train loss:  0.4788682460784912
train gradient:  0.1105710277303565
iteration : 5751
train acc:  0.71875
train loss:  0.4786824882030487
train gradient:  0.11266924225063076
iteration : 5752
train acc:  0.8125
train loss:  0.45237094163894653
train gradient:  0.12913149565994023
iteration : 5753
train acc:  0.6953125
train loss:  0.555723249912262
train gradient:  0.1590044039592131
iteration : 5754
train acc:  0.7421875
train loss:  0.5560312271118164
train gradient:  0.1588428754492815
iteration : 5755
train acc:  0.828125
train loss:  0.3953790068626404
train gradient:  0.11849994728211838
iteration : 5756
train acc:  0.7734375
train loss:  0.5325267314910889
train gradient:  0.1379104571638394
iteration : 5757
train acc:  0.6953125
train loss:  0.5243580937385559
train gradient:  0.15090979227298987
iteration : 5758
train acc:  0.6953125
train loss:  0.5492843389511108
train gradient:  0.15777909461400097
iteration : 5759
train acc:  0.71875
train loss:  0.49614018201828003
train gradient:  0.12377263526290005
iteration : 5760
train acc:  0.7734375
train loss:  0.4231511354446411
train gradient:  0.11965337710891193
iteration : 5761
train acc:  0.7578125
train loss:  0.4907535910606384
train gradient:  0.12604933310379918
iteration : 5762
train acc:  0.6796875
train loss:  0.608161449432373
train gradient:  0.20275593098424693
iteration : 5763
train acc:  0.71875
train loss:  0.5491610169410706
train gradient:  0.13243895334741748
iteration : 5764
train acc:  0.8203125
train loss:  0.4237193167209625
train gradient:  0.11262386163260324
iteration : 5765
train acc:  0.7109375
train loss:  0.5732940435409546
train gradient:  0.20273508292786124
iteration : 5766
train acc:  0.78125
train loss:  0.46575555205345154
train gradient:  0.11378836156625058
iteration : 5767
train acc:  0.6875
train loss:  0.5098979473114014
train gradient:  0.2568330161272612
iteration : 5768
train acc:  0.7734375
train loss:  0.4645189046859741
train gradient:  0.1352456306625347
iteration : 5769
train acc:  0.7265625
train loss:  0.49488580226898193
train gradient:  0.12767063189496858
iteration : 5770
train acc:  0.671875
train loss:  0.5782923698425293
train gradient:  0.16418645417169028
iteration : 5771
train acc:  0.7421875
train loss:  0.4958540201187134
train gradient:  0.09927920387185153
iteration : 5772
train acc:  0.6875
train loss:  0.5594787001609802
train gradient:  0.18059042341011883
iteration : 5773
train acc:  0.703125
train loss:  0.5368399620056152
train gradient:  0.14036679735055352
iteration : 5774
train acc:  0.7109375
train loss:  0.5525550842285156
train gradient:  0.1486564273414526
iteration : 5775
train acc:  0.71875
train loss:  0.5149725079536438
train gradient:  0.12540080513411167
iteration : 5776
train acc:  0.71875
train loss:  0.5536324977874756
train gradient:  0.1628560806892536
iteration : 5777
train acc:  0.8046875
train loss:  0.4769674837589264
train gradient:  0.14179789023314204
iteration : 5778
train acc:  0.796875
train loss:  0.46556925773620605
train gradient:  0.13077054058819865
iteration : 5779
train acc:  0.65625
train loss:  0.5438133478164673
train gradient:  0.1291671419699829
iteration : 5780
train acc:  0.734375
train loss:  0.5420199632644653
train gradient:  0.1425453904419145
iteration : 5781
train acc:  0.703125
train loss:  0.5096405744552612
train gradient:  0.12144664158909649
iteration : 5782
train acc:  0.765625
train loss:  0.5200252532958984
train gradient:  0.19781673636273484
iteration : 5783
train acc:  0.703125
train loss:  0.5735553503036499
train gradient:  0.15826997943727728
iteration : 5784
train acc:  0.671875
train loss:  0.5609073638916016
train gradient:  0.1700125605801887
iteration : 5785
train acc:  0.7890625
train loss:  0.4780675172805786
train gradient:  0.12650046026261563
iteration : 5786
train acc:  0.6953125
train loss:  0.5188446044921875
train gradient:  0.14072700832886162
iteration : 5787
train acc:  0.7265625
train loss:  0.5105393528938293
train gradient:  0.12136203160788656
iteration : 5788
train acc:  0.71875
train loss:  0.5636056661605835
train gradient:  0.16550517517380353
iteration : 5789
train acc:  0.78125
train loss:  0.47296786308288574
train gradient:  0.1424040453261769
iteration : 5790
train acc:  0.71875
train loss:  0.54132479429245
train gradient:  0.17902737003231134
iteration : 5791
train acc:  0.7265625
train loss:  0.5575761795043945
train gradient:  0.15329240474323347
iteration : 5792
train acc:  0.765625
train loss:  0.46497929096221924
train gradient:  0.12282535812857093
iteration : 5793
train acc:  0.7109375
train loss:  0.5238473415374756
train gradient:  0.1745639062516559
iteration : 5794
train acc:  0.765625
train loss:  0.454018771648407
train gradient:  0.11645959615269648
iteration : 5795
train acc:  0.671875
train loss:  0.5291662216186523
train gradient:  0.20390715080188215
iteration : 5796
train acc:  0.8125
train loss:  0.46450987458229065
train gradient:  0.122450829014149
iteration : 5797
train acc:  0.7421875
train loss:  0.5200601816177368
train gradient:  0.14226965913991196
iteration : 5798
train acc:  0.765625
train loss:  0.4879690706729889
train gradient:  0.1425918317105118
iteration : 5799
train acc:  0.734375
train loss:  0.48078328371047974
train gradient:  0.1237172979712687
iteration : 5800
train acc:  0.75
train loss:  0.4924764037132263
train gradient:  0.13292406876981872
iteration : 5801
train acc:  0.6875
train loss:  0.573755145072937
train gradient:  0.15493831510980485
iteration : 5802
train acc:  0.734375
train loss:  0.5396255254745483
train gradient:  0.18553011093551225
iteration : 5803
train acc:  0.7109375
train loss:  0.5161172747612
train gradient:  0.1347561068487132
iteration : 5804
train acc:  0.8046875
train loss:  0.4280981421470642
train gradient:  0.11233561809868625
iteration : 5805
train acc:  0.71875
train loss:  0.5083105564117432
train gradient:  0.184841478590742
iteration : 5806
train acc:  0.6875
train loss:  0.5462606549263
train gradient:  0.13986252558068038
iteration : 5807
train acc:  0.6640625
train loss:  0.5485052466392517
train gradient:  0.15257481428217304
iteration : 5808
train acc:  0.703125
train loss:  0.5231648087501526
train gradient:  0.151735895113031
iteration : 5809
train acc:  0.734375
train loss:  0.4583187997341156
train gradient:  0.13344868210210387
iteration : 5810
train acc:  0.71875
train loss:  0.5130518674850464
train gradient:  0.13171598973290977
iteration : 5811
train acc:  0.8046875
train loss:  0.45380455255508423
train gradient:  0.13881448084683745
iteration : 5812
train acc:  0.6640625
train loss:  0.5645713806152344
train gradient:  0.19342213028806007
iteration : 5813
train acc:  0.75
train loss:  0.5067206025123596
train gradient:  0.11544912320778179
iteration : 5814
train acc:  0.6796875
train loss:  0.5433233976364136
train gradient:  0.2346343597523051
iteration : 5815
train acc:  0.765625
train loss:  0.5067189931869507
train gradient:  0.15919672299377655
iteration : 5816
train acc:  0.7734375
train loss:  0.4823877811431885
train gradient:  0.13995171567429765
iteration : 5817
train acc:  0.7109375
train loss:  0.4803451895713806
train gradient:  0.1239072684289912
iteration : 5818
train acc:  0.734375
train loss:  0.4904481768608093
train gradient:  0.12287388699385636
iteration : 5819
train acc:  0.7578125
train loss:  0.4853783845901489
train gradient:  0.12772069384119203
iteration : 5820
train acc:  0.7578125
train loss:  0.45138928294181824
train gradient:  0.09798231603046158
iteration : 5821
train acc:  0.75
train loss:  0.5177124738693237
train gradient:  0.14162258587094834
iteration : 5822
train acc:  0.7734375
train loss:  0.4760169982910156
train gradient:  0.14026832167552855
iteration : 5823
train acc:  0.765625
train loss:  0.43317896127700806
train gradient:  0.1094342464370121
iteration : 5824
train acc:  0.8125
train loss:  0.4761969745159149
train gradient:  0.10803467928145721
iteration : 5825
train acc:  0.7578125
train loss:  0.47403115034103394
train gradient:  0.13749426554526312
iteration : 5826
train acc:  0.7578125
train loss:  0.48453372716903687
train gradient:  0.15800610913925356
iteration : 5827
train acc:  0.7109375
train loss:  0.5182036757469177
train gradient:  0.1744163236260386
iteration : 5828
train acc:  0.71875
train loss:  0.5614739656448364
train gradient:  0.23748900497951075
iteration : 5829
train acc:  0.671875
train loss:  0.5665043592453003
train gradient:  0.23099114224985356
iteration : 5830
train acc:  0.734375
train loss:  0.49953922629356384
train gradient:  0.1369564533791578
iteration : 5831
train acc:  0.7109375
train loss:  0.543653130531311
train gradient:  0.18208505267535993
iteration : 5832
train acc:  0.7421875
train loss:  0.496809184551239
train gradient:  0.137822217343384
iteration : 5833
train acc:  0.6796875
train loss:  0.6091020107269287
train gradient:  0.1987451557842021
iteration : 5834
train acc:  0.7734375
train loss:  0.47418296337127686
train gradient:  0.10414335650171334
iteration : 5835
train acc:  0.6875
train loss:  0.5588142275810242
train gradient:  0.16638118053026027
iteration : 5836
train acc:  0.65625
train loss:  0.5932625532150269
train gradient:  0.20140472670592177
iteration : 5837
train acc:  0.734375
train loss:  0.4961792230606079
train gradient:  0.13329489748453338
iteration : 5838
train acc:  0.765625
train loss:  0.4740210473537445
train gradient:  0.09864247040313567
iteration : 5839
train acc:  0.7578125
train loss:  0.5292587876319885
train gradient:  0.13855117678979945
iteration : 5840
train acc:  0.75
train loss:  0.5041186809539795
train gradient:  0.16534642857102166
iteration : 5841
train acc:  0.71875
train loss:  0.5175145268440247
train gradient:  0.14931981607922298
iteration : 5842
train acc:  0.7265625
train loss:  0.5313740968704224
train gradient:  0.14243690347542698
iteration : 5843
train acc:  0.7421875
train loss:  0.5095921754837036
train gradient:  0.12140513080701319
iteration : 5844
train acc:  0.78125
train loss:  0.466535747051239
train gradient:  0.13972597083609445
iteration : 5845
train acc:  0.75
train loss:  0.5167710781097412
train gradient:  0.15243637371889526
iteration : 5846
train acc:  0.71875
train loss:  0.4952563941478729
train gradient:  0.12396817047724766
iteration : 5847
train acc:  0.765625
train loss:  0.4994947910308838
train gradient:  0.22180814801531445
iteration : 5848
train acc:  0.765625
train loss:  0.47726041078567505
train gradient:  0.11713914099905064
iteration : 5849
train acc:  0.7890625
train loss:  0.4320557713508606
train gradient:  0.10503303834624897
iteration : 5850
train acc:  0.75
train loss:  0.5069422125816345
train gradient:  0.13535731013318353
iteration : 5851
train acc:  0.6640625
train loss:  0.5973379611968994
train gradient:  0.253183450205906
iteration : 5852
train acc:  0.75
train loss:  0.49271732568740845
train gradient:  0.13008693840289592
iteration : 5853
train acc:  0.7890625
train loss:  0.4428936839103699
train gradient:  0.10947040449264403
iteration : 5854
train acc:  0.71875
train loss:  0.48565995693206787
train gradient:  0.11021187769736927
iteration : 5855
train acc:  0.7265625
train loss:  0.5306477546691895
train gradient:  0.17934611943677342
iteration : 5856
train acc:  0.6953125
train loss:  0.5244094729423523
train gradient:  0.15271172009375808
iteration : 5857
train acc:  0.7578125
train loss:  0.45581644773483276
train gradient:  0.1142115479843199
iteration : 5858
train acc:  0.6953125
train loss:  0.5456863641738892
train gradient:  0.19612169755426045
iteration : 5859
train acc:  0.7109375
train loss:  0.5089542865753174
train gradient:  0.15361585059917965
iteration : 5860
train acc:  0.796875
train loss:  0.44996142387390137
train gradient:  0.12126160787238575
iteration : 5861
train acc:  0.7265625
train loss:  0.48668545484542847
train gradient:  0.12168686907202107
iteration : 5862
train acc:  0.6640625
train loss:  0.585056722164154
train gradient:  0.16252053913536757
iteration : 5863
train acc:  0.7734375
train loss:  0.46424615383148193
train gradient:  0.11753680603508866
iteration : 5864
train acc:  0.703125
train loss:  0.5323266983032227
train gradient:  0.15016856302156426
iteration : 5865
train acc:  0.703125
train loss:  0.521582841873169
train gradient:  0.18101767898659882
iteration : 5866
train acc:  0.6796875
train loss:  0.5348443984985352
train gradient:  0.16117421011772542
iteration : 5867
train acc:  0.7734375
train loss:  0.47928586602211
train gradient:  0.12890268724427406
iteration : 5868
train acc:  0.765625
train loss:  0.47005853056907654
train gradient:  0.1311972959913234
iteration : 5869
train acc:  0.703125
train loss:  0.5112586617469788
train gradient:  0.15737140087638218
iteration : 5870
train acc:  0.75
train loss:  0.485022634267807
train gradient:  0.16925330380621567
iteration : 5871
train acc:  0.6796875
train loss:  0.5642133951187134
train gradient:  0.23714573827749064
iteration : 5872
train acc:  0.7421875
train loss:  0.4762461185455322
train gradient:  0.16355397024936608
iteration : 5873
train acc:  0.7265625
train loss:  0.5060421228408813
train gradient:  0.12104905780390594
iteration : 5874
train acc:  0.7421875
train loss:  0.5098979473114014
train gradient:  0.13471630330855983
iteration : 5875
train acc:  0.734375
train loss:  0.45223796367645264
train gradient:  0.11040793945335423
iteration : 5876
train acc:  0.8125
train loss:  0.45448213815689087
train gradient:  0.10383251930504983
iteration : 5877
train acc:  0.7421875
train loss:  0.5071249008178711
train gradient:  0.1585142535360877
iteration : 5878
train acc:  0.6875
train loss:  0.543407142162323
train gradient:  0.152271813580019
iteration : 5879
train acc:  0.6953125
train loss:  0.5406595468521118
train gradient:  0.15702326196900007
iteration : 5880
train acc:  0.7109375
train loss:  0.5339698791503906
train gradient:  0.19731659961138195
iteration : 5881
train acc:  0.765625
train loss:  0.4728056788444519
train gradient:  0.13332944578437222
iteration : 5882
train acc:  0.7578125
train loss:  0.48140352964401245
train gradient:  0.157435807626417
iteration : 5883
train acc:  0.7109375
train loss:  0.526424765586853
train gradient:  0.1559504546044648
iteration : 5884
train acc:  0.75
train loss:  0.48239684104919434
train gradient:  0.10380234122438807
iteration : 5885
train acc:  0.75
train loss:  0.49966055154800415
train gradient:  0.13388966037000025
iteration : 5886
train acc:  0.6953125
train loss:  0.5158828496932983
train gradient:  0.17434945458666418
iteration : 5887
train acc:  0.7265625
train loss:  0.4882640242576599
train gradient:  0.14874611007361496
iteration : 5888
train acc:  0.7578125
train loss:  0.47863417863845825
train gradient:  0.14219678955797282
iteration : 5889
train acc:  0.734375
train loss:  0.4997490644454956
train gradient:  0.128400144657624
iteration : 5890
train acc:  0.6796875
train loss:  0.5562410354614258
train gradient:  0.18588353162981974
iteration : 5891
train acc:  0.6953125
train loss:  0.5186874270439148
train gradient:  0.14021519594259513
iteration : 5892
train acc:  0.7578125
train loss:  0.4671178162097931
train gradient:  0.1332018634108089
iteration : 5893
train acc:  0.6953125
train loss:  0.5476365089416504
train gradient:  0.19341466078207803
iteration : 5894
train acc:  0.7421875
train loss:  0.5424601435661316
train gradient:  0.16236945196440167
iteration : 5895
train acc:  0.7578125
train loss:  0.47343555092811584
train gradient:  0.13912367782864837
iteration : 5896
train acc:  0.734375
train loss:  0.49737662076950073
train gradient:  0.12566308012199567
iteration : 5897
train acc:  0.765625
train loss:  0.4556984305381775
train gradient:  0.1134200613271422
iteration : 5898
train acc:  0.65625
train loss:  0.5702321529388428
train gradient:  0.18389055652625574
iteration : 5899
train acc:  0.7109375
train loss:  0.4949077069759369
train gradient:  0.13997933493504627
iteration : 5900
train acc:  0.703125
train loss:  0.5074093341827393
train gradient:  0.11975467629920826
iteration : 5901
train acc:  0.6875
train loss:  0.5783323645591736
train gradient:  0.16400551382292258
iteration : 5902
train acc:  0.7734375
train loss:  0.47175824642181396
train gradient:  0.15180534557540457
iteration : 5903
train acc:  0.7890625
train loss:  0.46734946966171265
train gradient:  0.12098982554030688
iteration : 5904
train acc:  0.7421875
train loss:  0.47767066955566406
train gradient:  0.14537632040980963
iteration : 5905
train acc:  0.703125
train loss:  0.5042631030082703
train gradient:  0.15522209583087027
iteration : 5906
train acc:  0.7109375
train loss:  0.5480103492736816
train gradient:  0.16748424204087342
iteration : 5907
train acc:  0.7578125
train loss:  0.5328782200813293
train gradient:  0.2209869610237263
iteration : 5908
train acc:  0.7421875
train loss:  0.5056793689727783
train gradient:  0.18077499042887826
iteration : 5909
train acc:  0.7890625
train loss:  0.469554603099823
train gradient:  0.14414040899096356
iteration : 5910
train acc:  0.7578125
train loss:  0.4965299069881439
train gradient:  0.15300878691917746
iteration : 5911
train acc:  0.75
train loss:  0.574588418006897
train gradient:  0.2030073894192649
iteration : 5912
train acc:  0.8125
train loss:  0.4906187057495117
train gradient:  0.1482613629492841
iteration : 5913
train acc:  0.75
train loss:  0.48204004764556885
train gradient:  0.14461091441747725
iteration : 5914
train acc:  0.8046875
train loss:  0.4564707279205322
train gradient:  0.1274438237851207
iteration : 5915
train acc:  0.71875
train loss:  0.5119361281394958
train gradient:  0.1572069133651755
iteration : 5916
train acc:  0.7109375
train loss:  0.5448909997940063
train gradient:  0.17728944750857678
iteration : 5917
train acc:  0.734375
train loss:  0.5395893454551697
train gradient:  0.1511393581594297
iteration : 5918
train acc:  0.7734375
train loss:  0.44095221161842346
train gradient:  0.15317867478543357
iteration : 5919
train acc:  0.78125
train loss:  0.4753595292568207
train gradient:  0.12723566849752227
iteration : 5920
train acc:  0.7421875
train loss:  0.562069296836853
train gradient:  0.17948266588749318
iteration : 5921
train acc:  0.71875
train loss:  0.4933170676231384
train gradient:  0.15385757040345405
iteration : 5922
train acc:  0.640625
train loss:  0.6541702747344971
train gradient:  0.2015414826995005
iteration : 5923
train acc:  0.7734375
train loss:  0.4854887127876282
train gradient:  0.13061436190929843
iteration : 5924
train acc:  0.671875
train loss:  0.5665676593780518
train gradient:  0.18763239632700135
iteration : 5925
train acc:  0.703125
train loss:  0.5535882711410522
train gradient:  0.16820475062665471
iteration : 5926
train acc:  0.7578125
train loss:  0.5143616199493408
train gradient:  0.15079191003258693
iteration : 5927
train acc:  0.71875
train loss:  0.5153229236602783
train gradient:  0.17622307100316142
iteration : 5928
train acc:  0.703125
train loss:  0.5657421350479126
train gradient:  0.16076758556738208
iteration : 5929
train acc:  0.6953125
train loss:  0.5523589849472046
train gradient:  0.13090707331886292
iteration : 5930
train acc:  0.7421875
train loss:  0.515344500541687
train gradient:  0.16141701575600398
iteration : 5931
train acc:  0.75
train loss:  0.5010862350463867
train gradient:  0.12578455553577772
iteration : 5932
train acc:  0.75
train loss:  0.5192367434501648
train gradient:  0.15429005241709212
iteration : 5933
train acc:  0.7421875
train loss:  0.518290638923645
train gradient:  0.1293505516379011
iteration : 5934
train acc:  0.7421875
train loss:  0.4934260845184326
train gradient:  0.12955277676024296
iteration : 5935
train acc:  0.7578125
train loss:  0.5020356178283691
train gradient:  0.17730939663382286
iteration : 5936
train acc:  0.7421875
train loss:  0.516161322593689
train gradient:  0.14882703477379608
iteration : 5937
train acc:  0.7265625
train loss:  0.4835159480571747
train gradient:  0.12339118706090635
iteration : 5938
train acc:  0.7421875
train loss:  0.513954758644104
train gradient:  0.134997734140655
iteration : 5939
train acc:  0.78125
train loss:  0.45688772201538086
train gradient:  0.12419304116956748
iteration : 5940
train acc:  0.7421875
train loss:  0.4911269545555115
train gradient:  0.15872109554238095
iteration : 5941
train acc:  0.7421875
train loss:  0.5051066875457764
train gradient:  0.13902272357739295
iteration : 5942
train acc:  0.7578125
train loss:  0.45000728964805603
train gradient:  0.10300754968380223
iteration : 5943
train acc:  0.6875
train loss:  0.5398610830307007
train gradient:  0.1428166861824694
iteration : 5944
train acc:  0.671875
train loss:  0.5649721622467041
train gradient:  0.1707531694512773
iteration : 5945
train acc:  0.703125
train loss:  0.5012137293815613
train gradient:  0.10677474851626956
iteration : 5946
train acc:  0.71875
train loss:  0.5215994119644165
train gradient:  0.18318255983565684
iteration : 5947
train acc:  0.71875
train loss:  0.49118292331695557
train gradient:  0.12671625927863758
iteration : 5948
train acc:  0.671875
train loss:  0.5469347834587097
train gradient:  0.20346790036055118
iteration : 5949
train acc:  0.78125
train loss:  0.4532237648963928
train gradient:  0.10548603158117531
iteration : 5950
train acc:  0.78125
train loss:  0.48342034220695496
train gradient:  0.14984654906627354
iteration : 5951
train acc:  0.6875
train loss:  0.5472398400306702
train gradient:  0.13062015688416975
iteration : 5952
train acc:  0.6875
train loss:  0.5349873304367065
train gradient:  0.14421923273960421
iteration : 5953
train acc:  0.703125
train loss:  0.5352973341941833
train gradient:  0.1519162992146642
iteration : 5954
train acc:  0.6640625
train loss:  0.5783544778823853
train gradient:  0.1597261387699802
iteration : 5955
train acc:  0.7109375
train loss:  0.519600510597229
train gradient:  0.14721174920708585
iteration : 5956
train acc:  0.7421875
train loss:  0.5101123452186584
train gradient:  0.17319716985352823
iteration : 5957
train acc:  0.7578125
train loss:  0.494714617729187
train gradient:  0.1722308465926165
iteration : 5958
train acc:  0.71875
train loss:  0.4754008650779724
train gradient:  0.12189903717132751
iteration : 5959
train acc:  0.6328125
train loss:  0.5802844762802124
train gradient:  0.19340942029338803
iteration : 5960
train acc:  0.7734375
train loss:  0.5037620663642883
train gradient:  0.13554725070749646
iteration : 5961
train acc:  0.71875
train loss:  0.4987224042415619
train gradient:  0.17360328915834922
iteration : 5962
train acc:  0.71875
train loss:  0.5336198210716248
train gradient:  0.1720817685378335
iteration : 5963
train acc:  0.671875
train loss:  0.563672661781311
train gradient:  0.162883845412773
iteration : 5964
train acc:  0.7421875
train loss:  0.5226847529411316
train gradient:  0.15452904763164665
iteration : 5965
train acc:  0.75
train loss:  0.5114216208457947
train gradient:  0.11087444664611215
iteration : 5966
train acc:  0.8046875
train loss:  0.4170548617839813
train gradient:  0.10974522360681939
iteration : 5967
train acc:  0.734375
train loss:  0.492867648601532
train gradient:  0.1371726517098737
iteration : 5968
train acc:  0.8046875
train loss:  0.4486769437789917
train gradient:  0.12734927794260206
iteration : 5969
train acc:  0.796875
train loss:  0.4394415318965912
train gradient:  0.132953531517424
iteration : 5970
train acc:  0.734375
train loss:  0.49260178208351135
train gradient:  0.10705268069221842
iteration : 5971
train acc:  0.734375
train loss:  0.5102394819259644
train gradient:  0.1434405881969943
iteration : 5972
train acc:  0.7109375
train loss:  0.5282349586486816
train gradient:  0.13167289978336802
iteration : 5973
train acc:  0.734375
train loss:  0.48224347829818726
train gradient:  0.1166508805514981
iteration : 5974
train acc:  0.71875
train loss:  0.5036320686340332
train gradient:  0.16264603664692062
iteration : 5975
train acc:  0.703125
train loss:  0.573663592338562
train gradient:  0.15265694706908234
iteration : 5976
train acc:  0.671875
train loss:  0.5989101529121399
train gradient:  0.2098865751311248
iteration : 5977
train acc:  0.7421875
train loss:  0.5047348737716675
train gradient:  0.15971646258396532
iteration : 5978
train acc:  0.7109375
train loss:  0.5338370203971863
train gradient:  0.18401491448135895
iteration : 5979
train acc:  0.7578125
train loss:  0.46939027309417725
train gradient:  0.11083962226625124
iteration : 5980
train acc:  0.6875
train loss:  0.5116193294525146
train gradient:  0.13923533202351054
iteration : 5981
train acc:  0.75
train loss:  0.514978289604187
train gradient:  0.1428609730223292
iteration : 5982
train acc:  0.7421875
train loss:  0.5243284106254578
train gradient:  0.17317631333538785
iteration : 5983
train acc:  0.7109375
train loss:  0.5930378437042236
train gradient:  0.20258908621432473
iteration : 5984
train acc:  0.7421875
train loss:  0.4969901442527771
train gradient:  0.11942391074205484
iteration : 5985
train acc:  0.7578125
train loss:  0.458160400390625
train gradient:  0.10967196143975927
iteration : 5986
train acc:  0.7890625
train loss:  0.5352717638015747
train gradient:  0.14418202310677566
iteration : 5987
train acc:  0.71875
train loss:  0.5146443247795105
train gradient:  0.12865761915892088
iteration : 5988
train acc:  0.703125
train loss:  0.5449988842010498
train gradient:  0.1736133866618229
iteration : 5989
train acc:  0.71875
train loss:  0.5348039269447327
train gradient:  0.15072566436179827
iteration : 5990
train acc:  0.7109375
train loss:  0.521422803401947
train gradient:  0.15385522031330034
iteration : 5991
train acc:  0.6796875
train loss:  0.5889679789543152
train gradient:  0.17653884708946554
iteration : 5992
train acc:  0.734375
train loss:  0.4944779872894287
train gradient:  0.15322591131828553
iteration : 5993
train acc:  0.703125
train loss:  0.568833589553833
train gradient:  0.15169175878518898
iteration : 5994
train acc:  0.734375
train loss:  0.5340765714645386
train gradient:  0.16278111975048776
iteration : 5995
train acc:  0.71875
train loss:  0.5148836374282837
train gradient:  0.11442723672440823
iteration : 5996
train acc:  0.7265625
train loss:  0.5240438580513
train gradient:  0.1288898093375403
iteration : 5997
train acc:  0.7421875
train loss:  0.4547796845436096
train gradient:  0.1038069561155596
iteration : 5998
train acc:  0.7578125
train loss:  0.4940975606441498
train gradient:  0.16068453484655615
iteration : 5999
train acc:  0.734375
train loss:  0.5454990863800049
train gradient:  0.21231440833201204
iteration : 6000
train acc:  0.7890625
train loss:  0.4579153060913086
train gradient:  0.14088143744227516
iteration : 6001
train acc:  0.78125
train loss:  0.47264426946640015
train gradient:  0.12251181616802109
iteration : 6002
train acc:  0.7578125
train loss:  0.5447428822517395
train gradient:  0.15730758518712717
iteration : 6003
train acc:  0.7890625
train loss:  0.47828173637390137
train gradient:  0.13470387894805536
iteration : 6004
train acc:  0.7890625
train loss:  0.45898786187171936
train gradient:  0.13174478513642557
iteration : 6005
train acc:  0.765625
train loss:  0.4792477786540985
train gradient:  0.14492596648861955
iteration : 6006
train acc:  0.7578125
train loss:  0.48456496000289917
train gradient:  0.14589649026998142
iteration : 6007
train acc:  0.7265625
train loss:  0.5050239562988281
train gradient:  0.11657184656359508
iteration : 6008
train acc:  0.7421875
train loss:  0.4880824089050293
train gradient:  0.12791309621447536
iteration : 6009
train acc:  0.71875
train loss:  0.5207324028015137
train gradient:  0.17043237667821015
iteration : 6010
train acc:  0.75
train loss:  0.5039652585983276
train gradient:  0.17348073784948326
iteration : 6011
train acc:  0.7421875
train loss:  0.5127180814743042
train gradient:  0.16090216328815127
iteration : 6012
train acc:  0.6171875
train loss:  0.6298754215240479
train gradient:  0.22682444114230255
iteration : 6013
train acc:  0.7734375
train loss:  0.4956333339214325
train gradient:  0.1568192620756162
iteration : 6014
train acc:  0.78125
train loss:  0.4555749297142029
train gradient:  0.1342388789680085
iteration : 6015
train acc:  0.671875
train loss:  0.541370153427124
train gradient:  0.18858856192730827
iteration : 6016
train acc:  0.6875
train loss:  0.5694244503974915
train gradient:  0.2227040865990889
iteration : 6017
train acc:  0.8046875
train loss:  0.4420803189277649
train gradient:  0.09806515352420091
iteration : 6018
train acc:  0.7578125
train loss:  0.5127431154251099
train gradient:  0.1543795665090415
iteration : 6019
train acc:  0.71875
train loss:  0.5179414749145508
train gradient:  0.14533455654907818
iteration : 6020
train acc:  0.734375
train loss:  0.4888525605201721
train gradient:  0.14036940317733063
iteration : 6021
train acc:  0.765625
train loss:  0.49951744079589844
train gradient:  0.12452502095265003
iteration : 6022
train acc:  0.7578125
train loss:  0.4831119775772095
train gradient:  0.1330713211506901
iteration : 6023
train acc:  0.71875
train loss:  0.5916606187820435
train gradient:  0.21919637537546394
iteration : 6024
train acc:  0.7421875
train loss:  0.4662489891052246
train gradient:  0.13911361957634683
iteration : 6025
train acc:  0.7265625
train loss:  0.600440502166748
train gradient:  0.16810209522465108
iteration : 6026
train acc:  0.75
train loss:  0.5069384574890137
train gradient:  0.19431533422025554
iteration : 6027
train acc:  0.7578125
train loss:  0.5527389645576477
train gradient:  0.1769085761612696
iteration : 6028
train acc:  0.703125
train loss:  0.6159445643424988
train gradient:  0.359388231849484
iteration : 6029
train acc:  0.7890625
train loss:  0.47117942571640015
train gradient:  0.16298263001990293
iteration : 6030
train acc:  0.734375
train loss:  0.5466665029525757
train gradient:  0.1643865957347675
iteration : 6031
train acc:  0.7578125
train loss:  0.4835236668586731
train gradient:  0.13237836710854178
iteration : 6032
train acc:  0.8046875
train loss:  0.4433439075946808
train gradient:  0.1367209238737917
iteration : 6033
train acc:  0.7421875
train loss:  0.5081903338432312
train gradient:  0.21438522566212537
iteration : 6034
train acc:  0.640625
train loss:  0.6130672693252563
train gradient:  0.24195081993274867
iteration : 6035
train acc:  0.7265625
train loss:  0.5231019854545593
train gradient:  0.17372271207710585
iteration : 6036
train acc:  0.7421875
train loss:  0.5160801410675049
train gradient:  0.1273083821643321
iteration : 6037
train acc:  0.7734375
train loss:  0.5050756931304932
train gradient:  0.12694242314509802
iteration : 6038
train acc:  0.765625
train loss:  0.4787065088748932
train gradient:  0.1341697883715346
iteration : 6039
train acc:  0.6953125
train loss:  0.6055245399475098
train gradient:  0.24976284671629384
iteration : 6040
train acc:  0.7109375
train loss:  0.5324790477752686
train gradient:  0.14093566641512112
iteration : 6041
train acc:  0.75
train loss:  0.48271268606185913
train gradient:  0.10987221250491433
iteration : 6042
train acc:  0.8359375
train loss:  0.4353644847869873
train gradient:  0.1126204346715168
iteration : 6043
train acc:  0.734375
train loss:  0.4897921085357666
train gradient:  0.16052076230761894
iteration : 6044
train acc:  0.71875
train loss:  0.49443963170051575
train gradient:  0.13894225621467282
iteration : 6045
train acc:  0.71875
train loss:  0.4879703223705292
train gradient:  0.15945435407259628
iteration : 6046
train acc:  0.78125
train loss:  0.45067840814590454
train gradient:  0.13867056747024714
iteration : 6047
train acc:  0.7421875
train loss:  0.4944930672645569
train gradient:  0.14204326594215905
iteration : 6048
train acc:  0.765625
train loss:  0.5028892159461975
train gradient:  0.13252498834461027
iteration : 6049
train acc:  0.7265625
train loss:  0.5553569793701172
train gradient:  0.1528741106319091
iteration : 6050
train acc:  0.75
train loss:  0.47427308559417725
train gradient:  0.16091823489533508
iteration : 6051
train acc:  0.7578125
train loss:  0.46934351325035095
train gradient:  0.140130320826571
iteration : 6052
train acc:  0.6953125
train loss:  0.49844664335250854
train gradient:  0.1324201650601576
iteration : 6053
train acc:  0.734375
train loss:  0.5175883769989014
train gradient:  0.13208218929619847
iteration : 6054
train acc:  0.71875
train loss:  0.4770635962486267
train gradient:  0.1203792894858015
iteration : 6055
train acc:  0.75
train loss:  0.5265200138092041
train gradient:  0.1414204126792113
iteration : 6056
train acc:  0.6796875
train loss:  0.5622186064720154
train gradient:  0.20344042526495834
iteration : 6057
train acc:  0.6640625
train loss:  0.5727508068084717
train gradient:  0.18167196501299698
iteration : 6058
train acc:  0.734375
train loss:  0.4806005358695984
train gradient:  0.12573042365674214
iteration : 6059
train acc:  0.7578125
train loss:  0.449248731136322
train gradient:  0.11977362841505845
iteration : 6060
train acc:  0.703125
train loss:  0.5401841998100281
train gradient:  0.2192163656339557
iteration : 6061
train acc:  0.6796875
train loss:  0.5875958204269409
train gradient:  0.1985266473169267
iteration : 6062
train acc:  0.796875
train loss:  0.46806642413139343
train gradient:  0.11704591276414611
iteration : 6063
train acc:  0.765625
train loss:  0.4949241876602173
train gradient:  0.14007198185864644
iteration : 6064
train acc:  0.734375
train loss:  0.5743579864501953
train gradient:  0.16829910986027213
iteration : 6065
train acc:  0.828125
train loss:  0.43734726309776306
train gradient:  0.11370650571881757
iteration : 6066
train acc:  0.71875
train loss:  0.53076171875
train gradient:  0.1336762362499149
iteration : 6067
train acc:  0.7890625
train loss:  0.46261006593704224
train gradient:  0.1378195776935567
iteration : 6068
train acc:  0.7578125
train loss:  0.47332727909088135
train gradient:  0.12051824809823954
iteration : 6069
train acc:  0.78125
train loss:  0.4594784677028656
train gradient:  0.12147673695672319
iteration : 6070
train acc:  0.7421875
train loss:  0.5107083320617676
train gradient:  0.165779895155924
iteration : 6071
train acc:  0.75
train loss:  0.4493657350540161
train gradient:  0.14807713865720318
iteration : 6072
train acc:  0.734375
train loss:  0.4986354112625122
train gradient:  0.1832462551368116
iteration : 6073
train acc:  0.734375
train loss:  0.49759694933891296
train gradient:  0.17193507803132324
iteration : 6074
train acc:  0.7578125
train loss:  0.4613422751426697
train gradient:  0.1579931633390927
iteration : 6075
train acc:  0.828125
train loss:  0.41436195373535156
train gradient:  0.12584020290755166
iteration : 6076
train acc:  0.7265625
train loss:  0.4989762306213379
train gradient:  0.1060852157386032
iteration : 6077
train acc:  0.7421875
train loss:  0.5267090797424316
train gradient:  0.12721406682348374
iteration : 6078
train acc:  0.8359375
train loss:  0.4203217029571533
train gradient:  0.11705397119987232
iteration : 6079
train acc:  0.7578125
train loss:  0.43983790278434753
train gradient:  0.10280391644691002
iteration : 6080
train acc:  0.734375
train loss:  0.5212224125862122
train gradient:  0.1569223276124147
iteration : 6081
train acc:  0.8046875
train loss:  0.4505486786365509
train gradient:  0.15464211443665205
iteration : 6082
train acc:  0.765625
train loss:  0.525393009185791
train gradient:  0.19242361194598387
iteration : 6083
train acc:  0.7578125
train loss:  0.48813486099243164
train gradient:  0.13987553067626926
iteration : 6084
train acc:  0.734375
train loss:  0.4894309639930725
train gradient:  0.15754287945400297
iteration : 6085
train acc:  0.7265625
train loss:  0.523635983467102
train gradient:  0.15839977400629407
iteration : 6086
train acc:  0.7265625
train loss:  0.4910600185394287
train gradient:  0.12797321882791687
iteration : 6087
train acc:  0.7109375
train loss:  0.5071906447410583
train gradient:  0.13659232519718328
iteration : 6088
train acc:  0.7421875
train loss:  0.5118156671524048
train gradient:  0.14923302736648564
iteration : 6089
train acc:  0.7421875
train loss:  0.48503828048706055
train gradient:  0.15913508374883995
iteration : 6090
train acc:  0.703125
train loss:  0.5231006145477295
train gradient:  0.13929112668046073
iteration : 6091
train acc:  0.6640625
train loss:  0.5808073878288269
train gradient:  0.15431540104355396
iteration : 6092
train acc:  0.71875
train loss:  0.49930539727211
train gradient:  0.16540752306327827
iteration : 6093
train acc:  0.6953125
train loss:  0.5736110210418701
train gradient:  0.18346364272629206
iteration : 6094
train acc:  0.8046875
train loss:  0.4564755856990814
train gradient:  0.20843820967438303
iteration : 6095
train acc:  0.765625
train loss:  0.47673481702804565
train gradient:  0.12358326137200999
iteration : 6096
train acc:  0.734375
train loss:  0.4711627960205078
train gradient:  0.10271087975592734
iteration : 6097
train acc:  0.7421875
train loss:  0.5248100757598877
train gradient:  0.144732325363095
iteration : 6098
train acc:  0.65625
train loss:  0.5677402019500732
train gradient:  0.2064844807911051
iteration : 6099
train acc:  0.7109375
train loss:  0.5111539363861084
train gradient:  0.16937199649741957
iteration : 6100
train acc:  0.734375
train loss:  0.5193301439285278
train gradient:  0.17518304788636285
iteration : 6101
train acc:  0.71875
train loss:  0.5058006048202515
train gradient:  0.12398002990088716
iteration : 6102
train acc:  0.7109375
train loss:  0.4905613660812378
train gradient:  0.12943503614001214
iteration : 6103
train acc:  0.7890625
train loss:  0.45286130905151367
train gradient:  0.12691265235324703
iteration : 6104
train acc:  0.71875
train loss:  0.5339394807815552
train gradient:  0.14769674838646685
iteration : 6105
train acc:  0.7109375
train loss:  0.5609883666038513
train gradient:  0.1776140771994452
iteration : 6106
train acc:  0.7578125
train loss:  0.4754287898540497
train gradient:  0.12248881840461474
iteration : 6107
train acc:  0.6484375
train loss:  0.5833060145378113
train gradient:  0.21235844838660128
iteration : 6108
train acc:  0.7421875
train loss:  0.4795399606227875
train gradient:  0.12151640696863496
iteration : 6109
train acc:  0.703125
train loss:  0.5346822738647461
train gradient:  0.17062543136107594
iteration : 6110
train acc:  0.7421875
train loss:  0.5287939310073853
train gradient:  0.1527772489928469
iteration : 6111
train acc:  0.765625
train loss:  0.5214083194732666
train gradient:  0.17859912255364582
iteration : 6112
train acc:  0.6796875
train loss:  0.5189924836158752
train gradient:  0.11738591215154154
iteration : 6113
train acc:  0.7265625
train loss:  0.5136932730674744
train gradient:  0.1464138883457115
iteration : 6114
train acc:  0.6484375
train loss:  0.5714204907417297
train gradient:  0.19924803413927833
iteration : 6115
train acc:  0.65625
train loss:  0.5752459764480591
train gradient:  0.1587508592851894
iteration : 6116
train acc:  0.6953125
train loss:  0.5508860349655151
train gradient:  0.16788698499411459
iteration : 6117
train acc:  0.7421875
train loss:  0.5155608654022217
train gradient:  0.1452855771280742
iteration : 6118
train acc:  0.734375
train loss:  0.49742794036865234
train gradient:  0.1359111626247674
iteration : 6119
train acc:  0.7578125
train loss:  0.4785391092300415
train gradient:  0.12979562774606657
iteration : 6120
train acc:  0.7578125
train loss:  0.5231465101242065
train gradient:  0.13607247473994685
iteration : 6121
train acc:  0.7265625
train loss:  0.51311856508255
train gradient:  0.16176815956209417
iteration : 6122
train acc:  0.6875
train loss:  0.5567764043807983
train gradient:  0.16713456536065843
iteration : 6123
train acc:  0.75
train loss:  0.48591989278793335
train gradient:  0.14530971720977554
iteration : 6124
train acc:  0.8046875
train loss:  0.49053287506103516
train gradient:  0.12285838251057125
iteration : 6125
train acc:  0.71875
train loss:  0.5241372585296631
train gradient:  0.14731330914550086
iteration : 6126
train acc:  0.7265625
train loss:  0.5883312225341797
train gradient:  0.18681503614331343
iteration : 6127
train acc:  0.734375
train loss:  0.47620564699172974
train gradient:  0.13132124009634694
iteration : 6128
train acc:  0.75
train loss:  0.4798462390899658
train gradient:  0.13403349387998192
iteration : 6129
train acc:  0.71875
train loss:  0.5248545408248901
train gradient:  0.129629124135914
iteration : 6130
train acc:  0.796875
train loss:  0.4871135354042053
train gradient:  0.11355705627534016
iteration : 6131
train acc:  0.65625
train loss:  0.536857008934021
train gradient:  0.1775277492432986
iteration : 6132
train acc:  0.734375
train loss:  0.5366382598876953
train gradient:  0.18704812916053704
iteration : 6133
train acc:  0.7578125
train loss:  0.5230358839035034
train gradient:  0.16954207000758492
iteration : 6134
train acc:  0.71875
train loss:  0.5307092070579529
train gradient:  0.1431709166377668
iteration : 6135
train acc:  0.765625
train loss:  0.5190550684928894
train gradient:  0.13090824359616582
iteration : 6136
train acc:  0.75
train loss:  0.5314016342163086
train gradient:  0.14279844923366694
iteration : 6137
train acc:  0.8046875
train loss:  0.48275288939476013
train gradient:  0.1400598179943658
iteration : 6138
train acc:  0.7265625
train loss:  0.5318478345870972
train gradient:  0.14999352840523872
iteration : 6139
train acc:  0.78125
train loss:  0.5317386984825134
train gradient:  0.14429417530606148
iteration : 6140
train acc:  0.8203125
train loss:  0.42092183232307434
train gradient:  0.0976810223601555
iteration : 6141
train acc:  0.7265625
train loss:  0.5149970650672913
train gradient:  0.13159265001755274
iteration : 6142
train acc:  0.7265625
train loss:  0.4859240651130676
train gradient:  0.13527921579981494
iteration : 6143
train acc:  0.828125
train loss:  0.5005912780761719
train gradient:  0.14105843432903803
iteration : 6144
train acc:  0.7734375
train loss:  0.4797251224517822
train gradient:  0.15405872565526352
iteration : 6145
train acc:  0.6953125
train loss:  0.518444836139679
train gradient:  0.13568484848985624
iteration : 6146
train acc:  0.7421875
train loss:  0.4946681261062622
train gradient:  0.1598902654707409
iteration : 6147
train acc:  0.765625
train loss:  0.4928508698940277
train gradient:  0.13312999870892508
iteration : 6148
train acc:  0.6796875
train loss:  0.5220243334770203
train gradient:  0.17871118984526443
iteration : 6149
train acc:  0.765625
train loss:  0.4578125774860382
train gradient:  0.12645598030023417
iteration : 6150
train acc:  0.7421875
train loss:  0.4710950255393982
train gradient:  0.16223921647459544
iteration : 6151
train acc:  0.7265625
train loss:  0.5356301665306091
train gradient:  0.19732150078726451
iteration : 6152
train acc:  0.7109375
train loss:  0.5756222605705261
train gradient:  0.16646631159355935
iteration : 6153
train acc:  0.765625
train loss:  0.46451354026794434
train gradient:  0.11161354092428712
iteration : 6154
train acc:  0.75
train loss:  0.4710152745246887
train gradient:  0.14072279719442016
iteration : 6155
train acc:  0.8046875
train loss:  0.4574272036552429
train gradient:  0.13856085954419883
iteration : 6156
train acc:  0.7421875
train loss:  0.5045756101608276
train gradient:  0.18851039758133453
iteration : 6157
train acc:  0.71875
train loss:  0.5817258954048157
train gradient:  0.221256090550573
iteration : 6158
train acc:  0.796875
train loss:  0.4697832763195038
train gradient:  0.14182804040136907
iteration : 6159
train acc:  0.7109375
train loss:  0.5543224811553955
train gradient:  0.1693145184359564
iteration : 6160
train acc:  0.765625
train loss:  0.5160812735557556
train gradient:  0.1713590183776842
iteration : 6161
train acc:  0.734375
train loss:  0.5071300268173218
train gradient:  0.13256445743663808
iteration : 6162
train acc:  0.734375
train loss:  0.4888068437576294
train gradient:  0.12734326528335826
iteration : 6163
train acc:  0.7421875
train loss:  0.5175158977508545
train gradient:  0.18476576894205565
iteration : 6164
train acc:  0.7265625
train loss:  0.5010571479797363
train gradient:  0.14128540723341917
iteration : 6165
train acc:  0.6484375
train loss:  0.582658052444458
train gradient:  0.22536862961627327
iteration : 6166
train acc:  0.734375
train loss:  0.49055883288383484
train gradient:  0.15054521765684192
iteration : 6167
train acc:  0.7265625
train loss:  0.5142515301704407
train gradient:  0.15440332463453674
iteration : 6168
train acc:  0.703125
train loss:  0.5765966176986694
train gradient:  0.15434816123959727
iteration : 6169
train acc:  0.71875
train loss:  0.4973118305206299
train gradient:  0.12577520515307597
iteration : 6170
train acc:  0.7265625
train loss:  0.5096142292022705
train gradient:  0.19289423786921872
iteration : 6171
train acc:  0.7265625
train loss:  0.48825138807296753
train gradient:  0.1348628622494284
iteration : 6172
train acc:  0.765625
train loss:  0.4791775941848755
train gradient:  0.1300279295104288
iteration : 6173
train acc:  0.7734375
train loss:  0.4466095566749573
train gradient:  0.12364407608814813
iteration : 6174
train acc:  0.7265625
train loss:  0.5757601261138916
train gradient:  0.19775125312188874
iteration : 6175
train acc:  0.6640625
train loss:  0.5371112823486328
train gradient:  0.1913917083410036
iteration : 6176
train acc:  0.734375
train loss:  0.47320714592933655
train gradient:  0.15981724231187244
iteration : 6177
train acc:  0.7578125
train loss:  0.4838888645172119
train gradient:  0.11957341753355433
iteration : 6178
train acc:  0.6875
train loss:  0.5084578990936279
train gradient:  0.14761148159995505
iteration : 6179
train acc:  0.6875
train loss:  0.5065640211105347
train gradient:  0.15727001997429202
iteration : 6180
train acc:  0.7265625
train loss:  0.5027578473091125
train gradient:  0.12614137932014388
iteration : 6181
train acc:  0.7421875
train loss:  0.5115834474563599
train gradient:  0.14514726516509102
iteration : 6182
train acc:  0.765625
train loss:  0.47803813219070435
train gradient:  0.1634421021404925
iteration : 6183
train acc:  0.765625
train loss:  0.4850993752479553
train gradient:  0.1908132325838462
iteration : 6184
train acc:  0.7421875
train loss:  0.501169741153717
train gradient:  0.14516159445980165
iteration : 6185
train acc:  0.71875
train loss:  0.5273897647857666
train gradient:  0.18021463434686136
iteration : 6186
train acc:  0.765625
train loss:  0.4555102288722992
train gradient:  0.16601741081716964
iteration : 6187
train acc:  0.7890625
train loss:  0.47711774706840515
train gradient:  0.15735351334437644
iteration : 6188
train acc:  0.734375
train loss:  0.5199936032295227
train gradient:  0.15466524534500065
iteration : 6189
train acc:  0.671875
train loss:  0.5628595352172852
train gradient:  0.16342302466496722
iteration : 6190
train acc:  0.78125
train loss:  0.4413564205169678
train gradient:  0.12024231844352645
iteration : 6191
train acc:  0.71875
train loss:  0.4651712477207184
train gradient:  0.10743237766622162
iteration : 6192
train acc:  0.7265625
train loss:  0.4921768307685852
train gradient:  0.1346147734089865
iteration : 6193
train acc:  0.78125
train loss:  0.4846523702144623
train gradient:  0.12145967747592949
iteration : 6194
train acc:  0.8125
train loss:  0.48933160305023193
train gradient:  0.16201747174554224
iteration : 6195
train acc:  0.75
train loss:  0.5017048120498657
train gradient:  0.131823926336407
iteration : 6196
train acc:  0.796875
train loss:  0.4627242684364319
train gradient:  0.15156313079994865
iteration : 6197
train acc:  0.6796875
train loss:  0.546342134475708
train gradient:  0.1587209425836169
iteration : 6198
train acc:  0.71875
train loss:  0.5107824802398682
train gradient:  0.15252589252042267
iteration : 6199
train acc:  0.7265625
train loss:  0.5252583026885986
train gradient:  0.18208145245340612
iteration : 6200
train acc:  0.671875
train loss:  0.5723809003829956
train gradient:  0.19158661802584825
iteration : 6201
train acc:  0.6796875
train loss:  0.5042293071746826
train gradient:  0.1812732101882754
iteration : 6202
train acc:  0.8203125
train loss:  0.45917290449142456
train gradient:  0.13321361737779447
iteration : 6203
train acc:  0.7890625
train loss:  0.4690643548965454
train gradient:  0.11759356467489121
iteration : 6204
train acc:  0.796875
train loss:  0.4585176110267639
train gradient:  0.1333976278693884
iteration : 6205
train acc:  0.7265625
train loss:  0.4995020031929016
train gradient:  0.1272872970617311
iteration : 6206
train acc:  0.7578125
train loss:  0.4752182960510254
train gradient:  0.15451329320456592
iteration : 6207
train acc:  0.8515625
train loss:  0.3861543834209442
train gradient:  0.0922787446283656
iteration : 6208
train acc:  0.796875
train loss:  0.46458446979522705
train gradient:  0.11809847753328204
iteration : 6209
train acc:  0.71875
train loss:  0.5339367389678955
train gradient:  0.1489078832490222
iteration : 6210
train acc:  0.75
train loss:  0.4475244879722595
train gradient:  0.12761178573701876
iteration : 6211
train acc:  0.7578125
train loss:  0.5265877842903137
train gradient:  0.2030511304264565
iteration : 6212
train acc:  0.7421875
train loss:  0.5405329465866089
train gradient:  0.17388986325768654
iteration : 6213
train acc:  0.7265625
train loss:  0.5074397921562195
train gradient:  0.16139224011560924
iteration : 6214
train acc:  0.734375
train loss:  0.5561406016349792
train gradient:  0.22762812732189308
iteration : 6215
train acc:  0.6953125
train loss:  0.5403541326522827
train gradient:  0.20137532125667673
iteration : 6216
train acc:  0.671875
train loss:  0.6128846406936646
train gradient:  0.19347608656937043
iteration : 6217
train acc:  0.796875
train loss:  0.4491071105003357
train gradient:  0.1421933512420619
iteration : 6218
train acc:  0.7578125
train loss:  0.5139375925064087
train gradient:  0.1629764951225353
iteration : 6219
train acc:  0.7890625
train loss:  0.4839840531349182
train gradient:  0.137410821926969
iteration : 6220
train acc:  0.6484375
train loss:  0.6463201642036438
train gradient:  0.2664332806822444
iteration : 6221
train acc:  0.7265625
train loss:  0.5042547583580017
train gradient:  0.1385257696387147
iteration : 6222
train acc:  0.7890625
train loss:  0.4716053605079651
train gradient:  0.13408681405793577
iteration : 6223
train acc:  0.7578125
train loss:  0.4677063822746277
train gradient:  0.14210487531549865
iteration : 6224
train acc:  0.734375
train loss:  0.5081321001052856
train gradient:  0.14385947086329276
iteration : 6225
train acc:  0.7578125
train loss:  0.4670754373073578
train gradient:  0.11277394823186421
iteration : 6226
train acc:  0.6875
train loss:  0.5624760389328003
train gradient:  0.21049076940983685
iteration : 6227
train acc:  0.796875
train loss:  0.46565404534339905
train gradient:  0.1296526382652548
iteration : 6228
train acc:  0.734375
train loss:  0.463265597820282
train gradient:  0.11077549251047084
iteration : 6229
train acc:  0.703125
train loss:  0.5181134939193726
train gradient:  0.1764851518022823
iteration : 6230
train acc:  0.671875
train loss:  0.6047126650810242
train gradient:  0.17341692273641324
iteration : 6231
train acc:  0.71875
train loss:  0.5495219230651855
train gradient:  0.16093315647655598
iteration : 6232
train acc:  0.6953125
train loss:  0.5163835883140564
train gradient:  0.1370730564442723
iteration : 6233
train acc:  0.703125
train loss:  0.5499358177185059
train gradient:  0.15277221758403597
iteration : 6234
train acc:  0.7421875
train loss:  0.493833988904953
train gradient:  0.15653083844320764
iteration : 6235
train acc:  0.734375
train loss:  0.4559783637523651
train gradient:  0.13921794467593357
iteration : 6236
train acc:  0.7578125
train loss:  0.4958963990211487
train gradient:  0.15286599509686039
iteration : 6237
train acc:  0.6796875
train loss:  0.5625993609428406
train gradient:  0.16440771453547204
iteration : 6238
train acc:  0.7265625
train loss:  0.5110411047935486
train gradient:  0.17062335763562975
iteration : 6239
train acc:  0.6953125
train loss:  0.5611370205879211
train gradient:  0.15266360748801347
iteration : 6240
train acc:  0.734375
train loss:  0.5426770448684692
train gradient:  0.16436959576004812
iteration : 6241
train acc:  0.7421875
train loss:  0.4497631788253784
train gradient:  0.11584871785919446
iteration : 6242
train acc:  0.78125
train loss:  0.460662841796875
train gradient:  0.11286196324183667
iteration : 6243
train acc:  0.7578125
train loss:  0.49148017168045044
train gradient:  0.16103322305737036
iteration : 6244
train acc:  0.6875
train loss:  0.5585616230964661
train gradient:  0.16007786400447077
iteration : 6245
train acc:  0.71875
train loss:  0.5223286747932434
train gradient:  0.17003058352115888
iteration : 6246
train acc:  0.7734375
train loss:  0.4413079619407654
train gradient:  0.1192254215633156
iteration : 6247
train acc:  0.734375
train loss:  0.5411713719367981
train gradient:  0.1759054803845839
iteration : 6248
train acc:  0.71875
train loss:  0.5253379344940186
train gradient:  0.1708401427782304
iteration : 6249
train acc:  0.703125
train loss:  0.546906590461731
train gradient:  0.20751599229661855
iteration : 6250
train acc:  0.78125
train loss:  0.4699461758136749
train gradient:  0.11488187915682632
iteration : 6251
train acc:  0.6953125
train loss:  0.5940444469451904
train gradient:  0.20552692610407708
iteration : 6252
train acc:  0.75
train loss:  0.4751114249229431
train gradient:  0.1164090446001706
iteration : 6253
train acc:  0.703125
train loss:  0.5479760766029358
train gradient:  0.14478684107680118
iteration : 6254
train acc:  0.78125
train loss:  0.4714481234550476
train gradient:  0.15473816786167638
iteration : 6255
train acc:  0.7578125
train loss:  0.4779699444770813
train gradient:  0.12481624196412014
iteration : 6256
train acc:  0.7265625
train loss:  0.5077616572380066
train gradient:  0.1418498306325559
iteration : 6257
train acc:  0.71875
train loss:  0.5044043660163879
train gradient:  0.1423343161191532
iteration : 6258
train acc:  0.75
train loss:  0.4690193235874176
train gradient:  0.11693033600482615
iteration : 6259
train acc:  0.7890625
train loss:  0.4691969156265259
train gradient:  0.13252625619585934
iteration : 6260
train acc:  0.7890625
train loss:  0.4648330807685852
train gradient:  0.1191831869639217
iteration : 6261
train acc:  0.765625
train loss:  0.4887595772743225
train gradient:  0.12437947812437213
iteration : 6262
train acc:  0.75
train loss:  0.5171422958374023
train gradient:  0.1251511895113148
iteration : 6263
train acc:  0.703125
train loss:  0.5482063293457031
train gradient:  0.1854807371627447
iteration : 6264
train acc:  0.7734375
train loss:  0.5037232041358948
train gradient:  0.1507220405498765
iteration : 6265
train acc:  0.71875
train loss:  0.5128828883171082
train gradient:  0.18454485939536536
iteration : 6266
train acc:  0.75
train loss:  0.5272876024246216
train gradient:  0.11974080811666457
iteration : 6267
train acc:  0.703125
train loss:  0.5518323183059692
train gradient:  0.14229201290827148
iteration : 6268
train acc:  0.671875
train loss:  0.5426643490791321
train gradient:  0.21095647038470877
iteration : 6269
train acc:  0.6953125
train loss:  0.5157920122146606
train gradient:  0.1835466831396671
iteration : 6270
train acc:  0.734375
train loss:  0.5358404517173767
train gradient:  0.15198964842918145
iteration : 6271
train acc:  0.765625
train loss:  0.46069151163101196
train gradient:  0.12617077463201196
iteration : 6272
train acc:  0.703125
train loss:  0.5177892446517944
train gradient:  0.1594353342322798
iteration : 6273
train acc:  0.734375
train loss:  0.5397621989250183
train gradient:  0.163519722178259
iteration : 6274
train acc:  0.765625
train loss:  0.45524778962135315
train gradient:  0.1017989313222691
iteration : 6275
train acc:  0.71875
train loss:  0.5511181950569153
train gradient:  0.1634385077122661
iteration : 6276
train acc:  0.78125
train loss:  0.4611658751964569
train gradient:  0.11215989205718098
iteration : 6277
train acc:  0.7265625
train loss:  0.5621824860572815
train gradient:  0.15339328627647592
iteration : 6278
train acc:  0.6953125
train loss:  0.5602996349334717
train gradient:  0.2120922218344798
iteration : 6279
train acc:  0.734375
train loss:  0.5150997638702393
train gradient:  0.15040891181481328
iteration : 6280
train acc:  0.7109375
train loss:  0.5324013233184814
train gradient:  0.15346228707083687
iteration : 6281
train acc:  0.7265625
train loss:  0.4942438006401062
train gradient:  0.1411986891268222
iteration : 6282
train acc:  0.8203125
train loss:  0.43539100885391235
train gradient:  0.10848241220090941
iteration : 6283
train acc:  0.75
train loss:  0.44290047883987427
train gradient:  0.14088903653996582
iteration : 6284
train acc:  0.734375
train loss:  0.5005450248718262
train gradient:  0.1340671953839917
iteration : 6285
train acc:  0.75
train loss:  0.5372859239578247
train gradient:  0.14836174534371313
iteration : 6286
train acc:  0.75
train loss:  0.4786692261695862
train gradient:  0.12218022806622997
iteration : 6287
train acc:  0.7890625
train loss:  0.4441184103488922
train gradient:  0.09535888056198133
iteration : 6288
train acc:  0.6875
train loss:  0.5145994424819946
train gradient:  0.15713903733351614
iteration : 6289
train acc:  0.7109375
train loss:  0.5126017332077026
train gradient:  0.16499086156324017
iteration : 6290
train acc:  0.7578125
train loss:  0.4735737144947052
train gradient:  0.1396845542169969
iteration : 6291
train acc:  0.734375
train loss:  0.55539870262146
train gradient:  0.18205442119125448
iteration : 6292
train acc:  0.75
train loss:  0.506834864616394
train gradient:  0.12306231399488134
iteration : 6293
train acc:  0.6796875
train loss:  0.5778457522392273
train gradient:  0.18766116686454132
iteration : 6294
train acc:  0.75
train loss:  0.49568048119544983
train gradient:  0.14248383115178062
iteration : 6295
train acc:  0.71875
train loss:  0.5795390009880066
train gradient:  0.1911260216762225
iteration : 6296
train acc:  0.734375
train loss:  0.5197832584381104
train gradient:  0.1350210525352546
iteration : 6297
train acc:  0.734375
train loss:  0.5006930828094482
train gradient:  0.17126743837555952
iteration : 6298
train acc:  0.703125
train loss:  0.5863981246948242
train gradient:  0.14456632066519015
iteration : 6299
train acc:  0.6796875
train loss:  0.5569379329681396
train gradient:  0.15993211295720122
iteration : 6300
train acc:  0.71875
train loss:  0.5224083662033081
train gradient:  0.1530705360906282
iteration : 6301
train acc:  0.71875
train loss:  0.5503114461898804
train gradient:  0.148204955605831
iteration : 6302
train acc:  0.71875
train loss:  0.5155916810035706
train gradient:  0.13703374653742886
iteration : 6303
train acc:  0.734375
train loss:  0.5305333137512207
train gradient:  0.1579551543375638
iteration : 6304
train acc:  0.7890625
train loss:  0.49223095178604126
train gradient:  0.14144030296503848
iteration : 6305
train acc:  0.7578125
train loss:  0.5135411024093628
train gradient:  0.14354324929063098
iteration : 6306
train acc:  0.71875
train loss:  0.5051645040512085
train gradient:  0.12672695281538798
iteration : 6307
train acc:  0.7109375
train loss:  0.5283142328262329
train gradient:  0.1534010884247125
iteration : 6308
train acc:  0.75
train loss:  0.5084930062294006
train gradient:  0.14211587065346115
iteration : 6309
train acc:  0.65625
train loss:  0.5892094373703003
train gradient:  0.18668641789087137
iteration : 6310
train acc:  0.7265625
train loss:  0.5151575803756714
train gradient:  0.15311197856824194
iteration : 6311
train acc:  0.71875
train loss:  0.5314929485321045
train gradient:  0.16675522745467294
iteration : 6312
train acc:  0.734375
train loss:  0.5479730367660522
train gradient:  0.180906554398021
iteration : 6313
train acc:  0.8046875
train loss:  0.43004685640335083
train gradient:  0.11095204929356454
iteration : 6314
train acc:  0.6953125
train loss:  0.5669320821762085
train gradient:  0.2086397014222745
iteration : 6315
train acc:  0.6953125
train loss:  0.4881601929664612
train gradient:  0.12202120074836258
iteration : 6316
train acc:  0.734375
train loss:  0.5518293380737305
train gradient:  0.16330102259010346
iteration : 6317
train acc:  0.7109375
train loss:  0.5279095768928528
train gradient:  0.20421609147470826
iteration : 6318
train acc:  0.7578125
train loss:  0.49444580078125
train gradient:  0.13835077270029333
iteration : 6319
train acc:  0.75
train loss:  0.5306522846221924
train gradient:  0.15365352493811443
iteration : 6320
train acc:  0.7734375
train loss:  0.46518298983573914
train gradient:  0.13881530146012475
iteration : 6321
train acc:  0.671875
train loss:  0.5600239634513855
train gradient:  0.1704393881566188
iteration : 6322
train acc:  0.8046875
train loss:  0.44473275542259216
train gradient:  0.114512964355154
iteration : 6323
train acc:  0.7109375
train loss:  0.5276021957397461
train gradient:  0.1365622251329114
iteration : 6324
train acc:  0.8046875
train loss:  0.44060054421424866
train gradient:  0.13132538593997956
iteration : 6325
train acc:  0.75
train loss:  0.47244992852211
train gradient:  0.14249881897624256
iteration : 6326
train acc:  0.734375
train loss:  0.4795854389667511
train gradient:  0.14144383370037225
iteration : 6327
train acc:  0.7734375
train loss:  0.4952000379562378
train gradient:  0.1254208386993123
iteration : 6328
train acc:  0.7578125
train loss:  0.4791484475135803
train gradient:  0.1367205729595674
iteration : 6329
train acc:  0.7734375
train loss:  0.47073403000831604
train gradient:  0.1342260474465039
iteration : 6330
train acc:  0.75
train loss:  0.4691547453403473
train gradient:  0.09555545805640558
iteration : 6331
train acc:  0.7734375
train loss:  0.44753003120422363
train gradient:  0.11762537871822804
iteration : 6332
train acc:  0.78125
train loss:  0.4502844214439392
train gradient:  0.11724230320559609
iteration : 6333
train acc:  0.8046875
train loss:  0.4412534832954407
train gradient:  0.10991307065854157
iteration : 6334
train acc:  0.6796875
train loss:  0.5544857978820801
train gradient:  0.16981159029843906
iteration : 6335
train acc:  0.703125
train loss:  0.5576026439666748
train gradient:  0.17845171345144378
iteration : 6336
train acc:  0.75
train loss:  0.49813294410705566
train gradient:  0.17819072722858678
iteration : 6337
train acc:  0.7734375
train loss:  0.4655919671058655
train gradient:  0.12326452020775004
iteration : 6338
train acc:  0.7890625
train loss:  0.48471730947494507
train gradient:  0.14125367828670898
iteration : 6339
train acc:  0.6640625
train loss:  0.5655777454376221
train gradient:  0.1735005712353946
iteration : 6340
train acc:  0.7890625
train loss:  0.45427200198173523
train gradient:  0.11624821183097299
iteration : 6341
train acc:  0.734375
train loss:  0.48169025778770447
train gradient:  0.12818220924825308
iteration : 6342
train acc:  0.7421875
train loss:  0.5116090774536133
train gradient:  0.14642488685038937
iteration : 6343
train acc:  0.7265625
train loss:  0.5118300914764404
train gradient:  0.13704386024257403
iteration : 6344
train acc:  0.765625
train loss:  0.4365532696247101
train gradient:  0.10882581880441403
iteration : 6345
train acc:  0.7265625
train loss:  0.5032549500465393
train gradient:  0.130772564538351
iteration : 6346
train acc:  0.7734375
train loss:  0.48332661390304565
train gradient:  0.12936076839825716
iteration : 6347
train acc:  0.7734375
train loss:  0.5213538408279419
train gradient:  0.14884079472820425
iteration : 6348
train acc:  0.71875
train loss:  0.6160919070243835
train gradient:  0.262847118547077
iteration : 6349
train acc:  0.7890625
train loss:  0.4282195568084717
train gradient:  0.16241320887747315
iteration : 6350
train acc:  0.7421875
train loss:  0.46794161200523376
train gradient:  0.12400199378599601
iteration : 6351
train acc:  0.765625
train loss:  0.45684248208999634
train gradient:  0.11113149132374249
iteration : 6352
train acc:  0.7734375
train loss:  0.4723716378211975
train gradient:  0.15201580158372086
iteration : 6353
train acc:  0.671875
train loss:  0.5508806109428406
train gradient:  0.20293889770921159
iteration : 6354
train acc:  0.71875
train loss:  0.5207132697105408
train gradient:  0.15573786391301286
iteration : 6355
train acc:  0.71875
train loss:  0.5111496448516846
train gradient:  0.13789298147444373
iteration : 6356
train acc:  0.7265625
train loss:  0.5335537791252136
train gradient:  0.1746654789663315
iteration : 6357
train acc:  0.6953125
train loss:  0.5616194009780884
train gradient:  0.17782015938686085
iteration : 6358
train acc:  0.6953125
train loss:  0.6011391878128052
train gradient:  0.18184204871314963
iteration : 6359
train acc:  0.7734375
train loss:  0.45066821575164795
train gradient:  0.1423875992805233
iteration : 6360
train acc:  0.7265625
train loss:  0.49083423614501953
train gradient:  0.21434932439331533
iteration : 6361
train acc:  0.71875
train loss:  0.5259528160095215
train gradient:  0.16055403126467455
iteration : 6362
train acc:  0.734375
train loss:  0.49426963925361633
train gradient:  0.13312715596796992
iteration : 6363
train acc:  0.65625
train loss:  0.5652092099189758
train gradient:  0.15256982457568255
iteration : 6364
train acc:  0.734375
train loss:  0.526852011680603
train gradient:  0.154904651331164
iteration : 6365
train acc:  0.75
train loss:  0.5235891342163086
train gradient:  0.1549322914906885
iteration : 6366
train acc:  0.7109375
train loss:  0.5198225378990173
train gradient:  0.12388626916501218
iteration : 6367
train acc:  0.703125
train loss:  0.5654130578041077
train gradient:  0.1450083100705058
iteration : 6368
train acc:  0.7734375
train loss:  0.4464651942253113
train gradient:  0.11878319961535727
iteration : 6369
train acc:  0.7578125
train loss:  0.5180926322937012
train gradient:  0.12967675287661445
iteration : 6370
train acc:  0.765625
train loss:  0.4881799519062042
train gradient:  0.1403902357010059
iteration : 6371
train acc:  0.8046875
train loss:  0.4320887625217438
train gradient:  0.13117097039240094
iteration : 6372
train acc:  0.7265625
train loss:  0.5240787267684937
train gradient:  0.14168629357410895
iteration : 6373
train acc:  0.65625
train loss:  0.6039124131202698
train gradient:  0.23234332134330327
iteration : 6374
train acc:  0.7578125
train loss:  0.5144774317741394
train gradient:  0.14808468243959097
iteration : 6375
train acc:  0.6953125
train loss:  0.5402354001998901
train gradient:  0.16506117217642946
iteration : 6376
train acc:  0.7109375
train loss:  0.5692464113235474
train gradient:  0.18161423149117178
iteration : 6377
train acc:  0.71875
train loss:  0.48458319902420044
train gradient:  0.1424157790806237
iteration : 6378
train acc:  0.703125
train loss:  0.5831298828125
train gradient:  0.19347325821390624
iteration : 6379
train acc:  0.8203125
train loss:  0.4149079918861389
train gradient:  0.14810037912065968
iteration : 6380
train acc:  0.7109375
train loss:  0.5730568766593933
train gradient:  0.1531519914349072
iteration : 6381
train acc:  0.765625
train loss:  0.47329244017601013
train gradient:  0.12097557968884275
iteration : 6382
train acc:  0.734375
train loss:  0.4977189302444458
train gradient:  0.11131110555067711
iteration : 6383
train acc:  0.78125
train loss:  0.4773765802383423
train gradient:  0.1186979220139731
iteration : 6384
train acc:  0.7734375
train loss:  0.5782138109207153
train gradient:  0.22877910824792028
iteration : 6385
train acc:  0.7578125
train loss:  0.4733757972717285
train gradient:  0.10833580698553566
iteration : 6386
train acc:  0.75
train loss:  0.44091084599494934
train gradient:  0.14129403550020864
iteration : 6387
train acc:  0.7421875
train loss:  0.48228123784065247
train gradient:  0.1402523438904218
iteration : 6388
train acc:  0.734375
train loss:  0.5211942195892334
train gradient:  0.15799511252895299
iteration : 6389
train acc:  0.765625
train loss:  0.45342642068862915
train gradient:  0.12842913438443623
iteration : 6390
train acc:  0.6953125
train loss:  0.5362801551818848
train gradient:  0.1752928412229301
iteration : 6391
train acc:  0.7421875
train loss:  0.49770861864089966
train gradient:  0.1445801842568442
iteration : 6392
train acc:  0.7578125
train loss:  0.4561903178691864
train gradient:  0.10255558254346374
iteration : 6393
train acc:  0.7890625
train loss:  0.43071043491363525
train gradient:  0.12352064672728476
iteration : 6394
train acc:  0.734375
train loss:  0.49611377716064453
train gradient:  0.1946990577874048
iteration : 6395
train acc:  0.7890625
train loss:  0.4492368698120117
train gradient:  0.14594330557811527
iteration : 6396
train acc:  0.7421875
train loss:  0.4773169755935669
train gradient:  0.11310209440839857
iteration : 6397
train acc:  0.703125
train loss:  0.5623092651367188
train gradient:  0.20891687593306563
iteration : 6398
train acc:  0.6875
train loss:  0.5475747585296631
train gradient:  0.14837856847574865
iteration : 6399
train acc:  0.75
train loss:  0.5361872911453247
train gradient:  0.17902756150584564
iteration : 6400
train acc:  0.8046875
train loss:  0.46186500787734985
train gradient:  0.10496962901694434
iteration : 6401
train acc:  0.71875
train loss:  0.5008841753005981
train gradient:  0.13909360837659185
iteration : 6402
train acc:  0.734375
train loss:  0.5166481137275696
train gradient:  0.13458771691800364
iteration : 6403
train acc:  0.640625
train loss:  0.6336298584938049
train gradient:  0.1709922319471007
iteration : 6404
train acc:  0.734375
train loss:  0.5064496994018555
train gradient:  0.14181563344287734
iteration : 6405
train acc:  0.734375
train loss:  0.5041144490242004
train gradient:  0.135956335294864
iteration : 6406
train acc:  0.671875
train loss:  0.5441259145736694
train gradient:  0.15892281074072875
iteration : 6407
train acc:  0.6875
train loss:  0.5281065702438354
train gradient:  0.14949062343826072
iteration : 6408
train acc:  0.703125
train loss:  0.4959641695022583
train gradient:  0.12717000045343466
iteration : 6409
train acc:  0.765625
train loss:  0.4753250777721405
train gradient:  0.15431971784969367
iteration : 6410
train acc:  0.7734375
train loss:  0.5153795480728149
train gradient:  0.16048876989305685
iteration : 6411
train acc:  0.734375
train loss:  0.5217536687850952
train gradient:  0.15152952448969545
iteration : 6412
train acc:  0.7109375
train loss:  0.5135456919670105
train gradient:  0.1427044755437202
iteration : 6413
train acc:  0.765625
train loss:  0.477034330368042
train gradient:  0.12473613897219965
iteration : 6414
train acc:  0.7421875
train loss:  0.5493570566177368
train gradient:  0.17140822729378652
iteration : 6415
train acc:  0.7734375
train loss:  0.4586971402168274
train gradient:  0.1458979110640231
iteration : 6416
train acc:  0.7578125
train loss:  0.45211291313171387
train gradient:  0.10860191914622341
iteration : 6417
train acc:  0.7109375
train loss:  0.5127437710762024
train gradient:  0.1771326769986063
iteration : 6418
train acc:  0.7265625
train loss:  0.5521037578582764
train gradient:  0.17780362658644933
iteration : 6419
train acc:  0.7578125
train loss:  0.4955994784832001
train gradient:  0.1617153973927779
iteration : 6420
train acc:  0.8203125
train loss:  0.4354635179042816
train gradient:  0.11365392327733881
iteration : 6421
train acc:  0.765625
train loss:  0.5149592161178589
train gradient:  0.15237215023273432
iteration : 6422
train acc:  0.8203125
train loss:  0.4677196145057678
train gradient:  0.10581024717719323
iteration : 6423
train acc:  0.75
train loss:  0.4513578414916992
train gradient:  0.13093658837176606
iteration : 6424
train acc:  0.640625
train loss:  0.5625739097595215
train gradient:  0.21829949766043955
iteration : 6425
train acc:  0.765625
train loss:  0.45308369398117065
train gradient:  0.1328860574557961
iteration : 6426
train acc:  0.703125
train loss:  0.5194982290267944
train gradient:  0.14909863491541858
iteration : 6427
train acc:  0.6953125
train loss:  0.5281786918640137
train gradient:  0.16579341016338528
iteration : 6428
train acc:  0.7578125
train loss:  0.5215135812759399
train gradient:  0.15508626262775008
iteration : 6429
train acc:  0.7734375
train loss:  0.4537450075149536
train gradient:  0.1171469418088315
iteration : 6430
train acc:  0.8046875
train loss:  0.4143162965774536
train gradient:  0.14232804085185896
iteration : 6431
train acc:  0.7109375
train loss:  0.5074646472930908
train gradient:  0.13142421395917236
iteration : 6432
train acc:  0.7109375
train loss:  0.5028378963470459
train gradient:  0.1743776500596943
iteration : 6433
train acc:  0.7890625
train loss:  0.45043301582336426
train gradient:  0.13765188006472975
iteration : 6434
train acc:  0.6875
train loss:  0.5430700182914734
train gradient:  0.1318097845698758
iteration : 6435
train acc:  0.78125
train loss:  0.5138083100318909
train gradient:  0.15416588046319024
iteration : 6436
train acc:  0.703125
train loss:  0.5137795805931091
train gradient:  0.19137279759966203
iteration : 6437
train acc:  0.734375
train loss:  0.47187256813049316
train gradient:  0.13332277315633578
iteration : 6438
train acc:  0.765625
train loss:  0.5182422995567322
train gradient:  0.15921590827693727
iteration : 6439
train acc:  0.7265625
train loss:  0.5443416833877563
train gradient:  0.20375860914748595
iteration : 6440
train acc:  0.796875
train loss:  0.47562962770462036
train gradient:  0.13353064268115428
iteration : 6441
train acc:  0.6875
train loss:  0.5388721823692322
train gradient:  0.15285905419097306
iteration : 6442
train acc:  0.7109375
train loss:  0.5264965295791626
train gradient:  0.13601361215443641
iteration : 6443
train acc:  0.75
train loss:  0.49330025911331177
train gradient:  0.1348318238332913
iteration : 6444
train acc:  0.734375
train loss:  0.4846288561820984
train gradient:  0.13495593664844235
iteration : 6445
train acc:  0.734375
train loss:  0.5236026048660278
train gradient:  0.20342688831971834
iteration : 6446
train acc:  0.7734375
train loss:  0.42629945278167725
train gradient:  0.09768615130174389
iteration : 6447
train acc:  0.7265625
train loss:  0.5507888793945312
train gradient:  0.14270719800026005
iteration : 6448
train acc:  0.78125
train loss:  0.46898818016052246
train gradient:  0.1511976877725939
iteration : 6449
train acc:  0.703125
train loss:  0.5407999753952026
train gradient:  0.19092265960724525
iteration : 6450
train acc:  0.7265625
train loss:  0.4818364679813385
train gradient:  0.11554818959669846
iteration : 6451
train acc:  0.7421875
train loss:  0.5376026630401611
train gradient:  0.18373932936299145
iteration : 6452
train acc:  0.796875
train loss:  0.41765034198760986
train gradient:  0.1491789059715697
iteration : 6453
train acc:  0.765625
train loss:  0.46801212430000305
train gradient:  0.13699707505376107
iteration : 6454
train acc:  0.7890625
train loss:  0.4225192666053772
train gradient:  0.154619914770515
iteration : 6455
train acc:  0.734375
train loss:  0.5095411539077759
train gradient:  0.15467333873192135
iteration : 6456
train acc:  0.65625
train loss:  0.5995150208473206
train gradient:  0.25420950827493466
iteration : 6457
train acc:  0.703125
train loss:  0.4907853603363037
train gradient:  0.14422026614932124
iteration : 6458
train acc:  0.703125
train loss:  0.527188777923584
train gradient:  0.18243602333186365
iteration : 6459
train acc:  0.734375
train loss:  0.5025019645690918
train gradient:  0.15881397044701484
iteration : 6460
train acc:  0.6796875
train loss:  0.5049711465835571
train gradient:  0.12960477982329954
iteration : 6461
train acc:  0.796875
train loss:  0.47076544165611267
train gradient:  0.1259854032887039
iteration : 6462
train acc:  0.7734375
train loss:  0.4974697232246399
train gradient:  0.1875063176909757
iteration : 6463
train acc:  0.7734375
train loss:  0.480925053358078
train gradient:  0.12267684600692073
iteration : 6464
train acc:  0.7421875
train loss:  0.48285791277885437
train gradient:  0.11851895719632088
iteration : 6465
train acc:  0.7421875
train loss:  0.5005166530609131
train gradient:  0.19275822916150231
iteration : 6466
train acc:  0.703125
train loss:  0.4986610412597656
train gradient:  0.1364483602819712
iteration : 6467
train acc:  0.7265625
train loss:  0.5105115175247192
train gradient:  0.14879913724521074
iteration : 6468
train acc:  0.7421875
train loss:  0.5063945651054382
train gradient:  0.14881144950983913
iteration : 6469
train acc:  0.6484375
train loss:  0.6009329557418823
train gradient:  0.20891492044535592
iteration : 6470
train acc:  0.7421875
train loss:  0.4826580882072449
train gradient:  0.12906378814799047
iteration : 6471
train acc:  0.734375
train loss:  0.48425284028053284
train gradient:  0.1387976675854754
iteration : 6472
train acc:  0.7109375
train loss:  0.5257019996643066
train gradient:  0.13350939951892288
iteration : 6473
train acc:  0.7109375
train loss:  0.5070654153823853
train gradient:  0.13981223506966278
iteration : 6474
train acc:  0.75
train loss:  0.46691030263900757
train gradient:  0.11693912966910075
iteration : 6475
train acc:  0.765625
train loss:  0.4902305006980896
train gradient:  0.18070268462069133
iteration : 6476
train acc:  0.7578125
train loss:  0.49337899684906006
train gradient:  0.1540079701734893
iteration : 6477
train acc:  0.7421875
train loss:  0.5052847862243652
train gradient:  0.15550105115587107
iteration : 6478
train acc:  0.7421875
train loss:  0.5863012075424194
train gradient:  0.16928904080493043
iteration : 6479
train acc:  0.75
train loss:  0.45391184091567993
train gradient:  0.12401573598665998
iteration : 6480
train acc:  0.7890625
train loss:  0.4684528112411499
train gradient:  0.12753158635147677
iteration : 6481
train acc:  0.7265625
train loss:  0.4913717806339264
train gradient:  0.15894781490697474
iteration : 6482
train acc:  0.6796875
train loss:  0.5808367729187012
train gradient:  0.15031134842617846
iteration : 6483
train acc:  0.6875
train loss:  0.5421576499938965
train gradient:  0.1452455911562344
iteration : 6484
train acc:  0.78125
train loss:  0.4457687437534332
train gradient:  0.10609002015993625
iteration : 6485
train acc:  0.7265625
train loss:  0.5219483971595764
train gradient:  0.19324529646256683
iteration : 6486
train acc:  0.6875
train loss:  0.5270876288414001
train gradient:  0.15007721821332143
iteration : 6487
train acc:  0.7734375
train loss:  0.43234145641326904
train gradient:  0.11313767946734775
iteration : 6488
train acc:  0.8125
train loss:  0.4347180128097534
train gradient:  0.12072575168060415
iteration : 6489
train acc:  0.6796875
train loss:  0.5591728091239929
train gradient:  0.17773736406463636
iteration : 6490
train acc:  0.71875
train loss:  0.5526731014251709
train gradient:  0.1312891752082064
iteration : 6491
train acc:  0.734375
train loss:  0.5115236639976501
train gradient:  0.13600474103826632
iteration : 6492
train acc:  0.7578125
train loss:  0.5107027292251587
train gradient:  0.14963125797495036
iteration : 6493
train acc:  0.6953125
train loss:  0.5674718618392944
train gradient:  0.14045013460412936
iteration : 6494
train acc:  0.75
train loss:  0.5389305353164673
train gradient:  0.16711556262062166
iteration : 6495
train acc:  0.78125
train loss:  0.46088260412216187
train gradient:  0.14564145067395348
iteration : 6496
train acc:  0.7109375
train loss:  0.49833011627197266
train gradient:  0.17107036410976117
iteration : 6497
train acc:  0.734375
train loss:  0.5244555473327637
train gradient:  0.1337107326261126
iteration : 6498
train acc:  0.8046875
train loss:  0.4498196840286255
train gradient:  0.14687660443090872
iteration : 6499
train acc:  0.71875
train loss:  0.5461533069610596
train gradient:  0.17256872126949885
iteration : 6500
train acc:  0.7734375
train loss:  0.4783254861831665
train gradient:  0.13375941819442094
iteration : 6501
train acc:  0.8125
train loss:  0.42950424551963806
train gradient:  0.12444030314436533
iteration : 6502
train acc:  0.71875
train loss:  0.5006131529808044
train gradient:  0.1868579474459633
iteration : 6503
train acc:  0.6953125
train loss:  0.5486873388290405
train gradient:  0.2232642892776338
iteration : 6504
train acc:  0.6953125
train loss:  0.5890367031097412
train gradient:  0.1886881596530363
iteration : 6505
train acc:  0.71875
train loss:  0.5390424728393555
train gradient:  0.1463738168412887
iteration : 6506
train acc:  0.7578125
train loss:  0.44612494111061096
train gradient:  0.13024826451992816
iteration : 6507
train acc:  0.78125
train loss:  0.45929843187332153
train gradient:  0.12613471266894793
iteration : 6508
train acc:  0.734375
train loss:  0.4843662977218628
train gradient:  0.11657461015598294
iteration : 6509
train acc:  0.734375
train loss:  0.48432716727256775
train gradient:  0.1132485781104642
iteration : 6510
train acc:  0.7890625
train loss:  0.46117210388183594
train gradient:  0.1642630112983077
iteration : 6511
train acc:  0.7109375
train loss:  0.5626173615455627
train gradient:  0.16729057354866272
iteration : 6512
train acc:  0.734375
train loss:  0.4971173107624054
train gradient:  0.16253182579328318
iteration : 6513
train acc:  0.7265625
train loss:  0.4713982939720154
train gradient:  0.12958235684781982
iteration : 6514
train acc:  0.6796875
train loss:  0.5335065126419067
train gradient:  0.1685098350945301
iteration : 6515
train acc:  0.78125
train loss:  0.48921144008636475
train gradient:  0.16797331813527608
iteration : 6516
train acc:  0.78125
train loss:  0.4959019124507904
train gradient:  0.15876790806830784
iteration : 6517
train acc:  0.7265625
train loss:  0.5301727056503296
train gradient:  0.18339940992733633
iteration : 6518
train acc:  0.7109375
train loss:  0.5197761654853821
train gradient:  0.1242106721865897
iteration : 6519
train acc:  0.7421875
train loss:  0.44314485788345337
train gradient:  0.12287660598164339
iteration : 6520
train acc:  0.7890625
train loss:  0.4646404981613159
train gradient:  0.12887136051703346
iteration : 6521
train acc:  0.7890625
train loss:  0.4688633680343628
train gradient:  0.11147242863469871
iteration : 6522
train acc:  0.7578125
train loss:  0.5367165207862854
train gradient:  0.19875239529084204
iteration : 6523
train acc:  0.7109375
train loss:  0.5709764361381531
train gradient:  0.1855788885927645
iteration : 6524
train acc:  0.796875
train loss:  0.4491567611694336
train gradient:  0.15208954731843016
iteration : 6525
train acc:  0.765625
train loss:  0.47105756402015686
train gradient:  0.12383746860041775
iteration : 6526
train acc:  0.703125
train loss:  0.5292702913284302
train gradient:  0.1709893486678128
iteration : 6527
train acc:  0.7578125
train loss:  0.5300149321556091
train gradient:  0.1372995886433081
iteration : 6528
train acc:  0.734375
train loss:  0.5956943035125732
train gradient:  0.177629606871928
iteration : 6529
train acc:  0.6953125
train loss:  0.5564807653427124
train gradient:  0.16039837036971347
iteration : 6530
train acc:  0.6953125
train loss:  0.5561835169792175
train gradient:  0.16647716927957057
iteration : 6531
train acc:  0.734375
train loss:  0.5366744995117188
train gradient:  0.16142534469508413
iteration : 6532
train acc:  0.7734375
train loss:  0.44489383697509766
train gradient:  0.11410894621745185
iteration : 6533
train acc:  0.7265625
train loss:  0.5276575684547424
train gradient:  0.1355463635746916
iteration : 6534
train acc:  0.7109375
train loss:  0.5178015232086182
train gradient:  0.13504560235713156
iteration : 6535
train acc:  0.765625
train loss:  0.5178048610687256
train gradient:  0.155304992595519
iteration : 6536
train acc:  0.7421875
train loss:  0.5658600330352783
train gradient:  0.18794063057076105
iteration : 6537
train acc:  0.8046875
train loss:  0.43231403827667236
train gradient:  0.13423810007444212
iteration : 6538
train acc:  0.7265625
train loss:  0.5374392867088318
train gradient:  0.14725847570799389
iteration : 6539
train acc:  0.6953125
train loss:  0.522824764251709
train gradient:  0.1394492876114226
iteration : 6540
train acc:  0.6953125
train loss:  0.5480873584747314
train gradient:  0.14376411058422436
iteration : 6541
train acc:  0.765625
train loss:  0.4821912348270416
train gradient:  0.1141666007267802
iteration : 6542
train acc:  0.75
train loss:  0.4915858507156372
train gradient:  0.13943050350992087
iteration : 6543
train acc:  0.734375
train loss:  0.5168052315711975
train gradient:  0.15420621443068044
iteration : 6544
train acc:  0.7578125
train loss:  0.5218990445137024
train gradient:  0.16479192586180602
iteration : 6545
train acc:  0.796875
train loss:  0.4718693792819977
train gradient:  0.11133456137503694
iteration : 6546
train acc:  0.765625
train loss:  0.4849811792373657
train gradient:  0.1259802385312387
iteration : 6547
train acc:  0.7109375
train loss:  0.5290173292160034
train gradient:  0.16519141234055362
iteration : 6548
train acc:  0.7109375
train loss:  0.49633723497390747
train gradient:  0.12927052405796058
iteration : 6549
train acc:  0.7265625
train loss:  0.5290544629096985
train gradient:  0.14147081985623855
iteration : 6550
train acc:  0.7734375
train loss:  0.5194907188415527
train gradient:  0.20527558909874719
iteration : 6551
train acc:  0.7890625
train loss:  0.46045345067977905
train gradient:  0.1530581112051111
iteration : 6552
train acc:  0.7109375
train loss:  0.542717456817627
train gradient:  0.13410419309550078
iteration : 6553
train acc:  0.8125
train loss:  0.46338802576065063
train gradient:  0.11924751715471174
iteration : 6554
train acc:  0.6796875
train loss:  0.5515566468238831
train gradient:  0.14899496495425488
iteration : 6555
train acc:  0.71875
train loss:  0.5830751657485962
train gradient:  0.19124988454319258
iteration : 6556
train acc:  0.7578125
train loss:  0.4724667966365814
train gradient:  0.115958141127552
iteration : 6557
train acc:  0.703125
train loss:  0.5150009989738464
train gradient:  0.13782592191965728
iteration : 6558
train acc:  0.6640625
train loss:  0.6348538994789124
train gradient:  0.23040681909532373
iteration : 6559
train acc:  0.7578125
train loss:  0.5329581499099731
train gradient:  0.20189495835255836
iteration : 6560
train acc:  0.765625
train loss:  0.520045280456543
train gradient:  0.14541439219649627
iteration : 6561
train acc:  0.6953125
train loss:  0.5441862344741821
train gradient:  0.12501675993479588
iteration : 6562
train acc:  0.75
train loss:  0.5424458980560303
train gradient:  0.15318937625381165
iteration : 6563
train acc:  0.703125
train loss:  0.5171179175376892
train gradient:  0.13691026096411574
iteration : 6564
train acc:  0.7265625
train loss:  0.5366134643554688
train gradient:  0.15707036329169438
iteration : 6565
train acc:  0.765625
train loss:  0.48828989267349243
train gradient:  0.14367238285474523
iteration : 6566
train acc:  0.71875
train loss:  0.5872517228126526
train gradient:  0.2343828798761423
iteration : 6567
train acc:  0.7734375
train loss:  0.4776419997215271
train gradient:  0.14282118293593654
iteration : 6568
train acc:  0.7578125
train loss:  0.4931681752204895
train gradient:  0.1315780561717329
iteration : 6569
train acc:  0.75
train loss:  0.4981662333011627
train gradient:  0.1225622798484488
iteration : 6570
train acc:  0.71875
train loss:  0.5338567495346069
train gradient:  0.16995763330160207
iteration : 6571
train acc:  0.6640625
train loss:  0.6017069816589355
train gradient:  0.15706281770948852
iteration : 6572
train acc:  0.640625
train loss:  0.6266555786132812
train gradient:  0.23343799693300793
iteration : 6573
train acc:  0.75
train loss:  0.4912131428718567
train gradient:  0.12510651006658946
iteration : 6574
train acc:  0.7265625
train loss:  0.48973146080970764
train gradient:  0.1489478528225764
iteration : 6575
train acc:  0.7421875
train loss:  0.5108707547187805
train gradient:  0.17678415846648954
iteration : 6576
train acc:  0.7109375
train loss:  0.5047818422317505
train gradient:  0.18598103619312611
iteration : 6577
train acc:  0.7265625
train loss:  0.5290307402610779
train gradient:  0.14200503410109871
iteration : 6578
train acc:  0.765625
train loss:  0.47228768467903137
train gradient:  0.13177732942928566
iteration : 6579
train acc:  0.734375
train loss:  0.4437238574028015
train gradient:  0.12417654161547872
iteration : 6580
train acc:  0.6875
train loss:  0.513823926448822
train gradient:  0.13070510257140605
iteration : 6581
train acc:  0.7109375
train loss:  0.5446774363517761
train gradient:  0.13335855786967984
iteration : 6582
train acc:  0.7734375
train loss:  0.46792280673980713
train gradient:  0.13646944439110156
iteration : 6583
train acc:  0.7421875
train loss:  0.5075066089630127
train gradient:  0.16570066849013398
iteration : 6584
train acc:  0.7421875
train loss:  0.5062595009803772
train gradient:  0.15141903655704114
iteration : 6585
train acc:  0.7421875
train loss:  0.5284171104431152
train gradient:  0.12935483956327576
iteration : 6586
train acc:  0.6796875
train loss:  0.521764874458313
train gradient:  0.2245734459514221
iteration : 6587
train acc:  0.7421875
train loss:  0.5023505091667175
train gradient:  0.1261385394632129
iteration : 6588
train acc:  0.7109375
train loss:  0.5097139477729797
train gradient:  0.13562878083668853
iteration : 6589
train acc:  0.71875
train loss:  0.5146209001541138
train gradient:  0.1347093575807703
iteration : 6590
train acc:  0.7421875
train loss:  0.5292772650718689
train gradient:  0.14904514073921613
iteration : 6591
train acc:  0.6953125
train loss:  0.49245399236679077
train gradient:  0.14520969543193618
iteration : 6592
train acc:  0.7421875
train loss:  0.48780620098114014
train gradient:  0.155582830660086
iteration : 6593
train acc:  0.7578125
train loss:  0.507580578327179
train gradient:  0.11386362699196732
iteration : 6594
train acc:  0.75
train loss:  0.520356297492981
train gradient:  0.1265311342162676
iteration : 6595
train acc:  0.6953125
train loss:  0.5274198055267334
train gradient:  0.22089146978169416
iteration : 6596
train acc:  0.734375
train loss:  0.48804718255996704
train gradient:  0.12350766425044758
iteration : 6597
train acc:  0.765625
train loss:  0.4748557507991791
train gradient:  0.1261656573996956
iteration : 6598
train acc:  0.7578125
train loss:  0.48440077900886536
train gradient:  0.1390542963960059
iteration : 6599
train acc:  0.6640625
train loss:  0.6113319993019104
train gradient:  0.1543484084497555
iteration : 6600
train acc:  0.703125
train loss:  0.49928203225135803
train gradient:  0.14053699682382917
iteration : 6601
train acc:  0.7890625
train loss:  0.43489134311676025
train gradient:  0.11287622236034447
iteration : 6602
train acc:  0.6875
train loss:  0.5531766414642334
train gradient:  0.12780089838332226
iteration : 6603
train acc:  0.75
train loss:  0.5035011768341064
train gradient:  0.13888845474095346
iteration : 6604
train acc:  0.78125
train loss:  0.47682490944862366
train gradient:  0.191275356208503
iteration : 6605
train acc:  0.75
train loss:  0.5442790985107422
train gradient:  0.16044654557892563
iteration : 6606
train acc:  0.78125
train loss:  0.4593212306499481
train gradient:  0.12108391436281553
iteration : 6607
train acc:  0.6484375
train loss:  0.5666656494140625
train gradient:  0.1808795684766198
iteration : 6608
train acc:  0.7109375
train loss:  0.5155328512191772
train gradient:  0.16199172766342404
iteration : 6609
train acc:  0.7265625
train loss:  0.4572061002254486
train gradient:  0.09776889173157693
iteration : 6610
train acc:  0.7734375
train loss:  0.5000690817832947
train gradient:  0.13816697951276008
iteration : 6611
train acc:  0.7578125
train loss:  0.4809727668762207
train gradient:  0.17052500452538394
iteration : 6612
train acc:  0.75
train loss:  0.5615270733833313
train gradient:  0.15617279867736333
iteration : 6613
train acc:  0.7265625
train loss:  0.46450701355934143
train gradient:  0.1392767303496525
iteration : 6614
train acc:  0.6875
train loss:  0.542472243309021
train gradient:  0.2105540389612207
iteration : 6615
train acc:  0.71875
train loss:  0.5380340814590454
train gradient:  0.21449269734964999
iteration : 6616
train acc:  0.75
train loss:  0.46926894783973694
train gradient:  0.14216565577072499
iteration : 6617
train acc:  0.7109375
train loss:  0.5557956695556641
train gradient:  0.18989145062294904
iteration : 6618
train acc:  0.7890625
train loss:  0.4556378722190857
train gradient:  0.10084476768584004
iteration : 6619
train acc:  0.7890625
train loss:  0.5116212964057922
train gradient:  0.15464281616791858
iteration : 6620
train acc:  0.78125
train loss:  0.46795356273651123
train gradient:  0.10945003811134732
iteration : 6621
train acc:  0.7890625
train loss:  0.4734381437301636
train gradient:  0.1303302619922456
iteration : 6622
train acc:  0.703125
train loss:  0.5635417699813843
train gradient:  0.14733697133730836
iteration : 6623
train acc:  0.703125
train loss:  0.5253195762634277
train gradient:  0.15503241504545529
iteration : 6624
train acc:  0.7890625
train loss:  0.4431971311569214
train gradient:  0.13807024506498516
iteration : 6625
train acc:  0.75
train loss:  0.4567559063434601
train gradient:  0.1140764372055268
iteration : 6626
train acc:  0.6953125
train loss:  0.5580162405967712
train gradient:  0.21220839740820827
iteration : 6627
train acc:  0.765625
train loss:  0.5106199979782104
train gradient:  0.11358657884710674
iteration : 6628
train acc:  0.734375
train loss:  0.4781213700771332
train gradient:  0.11681177828295984
iteration : 6629
train acc:  0.734375
train loss:  0.5203538537025452
train gradient:  0.13601747284913218
iteration : 6630
train acc:  0.7109375
train loss:  0.5299297571182251
train gradient:  0.16152002034729634
iteration : 6631
train acc:  0.65625
train loss:  0.5832875967025757
train gradient:  0.1665838822401929
iteration : 6632
train acc:  0.7265625
train loss:  0.5120795369148254
train gradient:  0.17018791043328163
iteration : 6633
train acc:  0.765625
train loss:  0.4497431218624115
train gradient:  0.10858225412455658
iteration : 6634
train acc:  0.703125
train loss:  0.5275259017944336
train gradient:  0.15512096160954916
iteration : 6635
train acc:  0.75
train loss:  0.5165915489196777
train gradient:  0.19033708971608415
iteration : 6636
train acc:  0.71875
train loss:  0.5524933338165283
train gradient:  0.14350098101177342
iteration : 6637
train acc:  0.7109375
train loss:  0.531066358089447
train gradient:  0.1527948139578606
iteration : 6638
train acc:  0.7890625
train loss:  0.4153938591480255
train gradient:  0.10487701771943092
iteration : 6639
train acc:  0.640625
train loss:  0.5386318564414978
train gradient:  0.15564460367273092
iteration : 6640
train acc:  0.78125
train loss:  0.49803823232650757
train gradient:  0.13786177985901665
iteration : 6641
train acc:  0.75
train loss:  0.47409141063690186
train gradient:  0.119889183392432
iteration : 6642
train acc:  0.71875
train loss:  0.5135154128074646
train gradient:  0.13103501082265218
iteration : 6643
train acc:  0.734375
train loss:  0.4741612672805786
train gradient:  0.15480477578739055
iteration : 6644
train acc:  0.765625
train loss:  0.4638925790786743
train gradient:  0.1623161174904737
iteration : 6645
train acc:  0.734375
train loss:  0.5218267440795898
train gradient:  0.15490734955412905
iteration : 6646
train acc:  0.703125
train loss:  0.4992251992225647
train gradient:  0.14491601997218384
iteration : 6647
train acc:  0.7578125
train loss:  0.46594545245170593
train gradient:  0.10021831477467588
iteration : 6648
train acc:  0.6953125
train loss:  0.5131133794784546
train gradient:  0.13404314814720256
iteration : 6649
train acc:  0.6953125
train loss:  0.5457159280776978
train gradient:  0.15432302706681095
iteration : 6650
train acc:  0.6640625
train loss:  0.576958417892456
train gradient:  0.19305982488211448
iteration : 6651
train acc:  0.796875
train loss:  0.47579050064086914
train gradient:  0.133961745755086
iteration : 6652
train acc:  0.7578125
train loss:  0.5112372636795044
train gradient:  0.14234275179412653
iteration : 6653
train acc:  0.7109375
train loss:  0.49895337224006653
train gradient:  0.1184406702972482
iteration : 6654
train acc:  0.7265625
train loss:  0.4725387394428253
train gradient:  0.13454893085664504
iteration : 6655
train acc:  0.703125
train loss:  0.5368119478225708
train gradient:  0.15168644255316832
iteration : 6656
train acc:  0.65625
train loss:  0.5699953436851501
train gradient:  0.21472772563370918
iteration : 6657
train acc:  0.7421875
train loss:  0.46987366676330566
train gradient:  0.14353418034206306
iteration : 6658
train acc:  0.75
train loss:  0.5060514211654663
train gradient:  0.12936566322271145
iteration : 6659
train acc:  0.7265625
train loss:  0.5064548254013062
train gradient:  0.16808581519788923
iteration : 6660
train acc:  0.7265625
train loss:  0.5234470963478088
train gradient:  0.16663808546430742
iteration : 6661
train acc:  0.7421875
train loss:  0.4986845850944519
train gradient:  0.10764467419042154
iteration : 6662
train acc:  0.7734375
train loss:  0.4395114779472351
train gradient:  0.13847293461198212
iteration : 6663
train acc:  0.7890625
train loss:  0.4476706385612488
train gradient:  0.09724650864204623
iteration : 6664
train acc:  0.7421875
train loss:  0.49836987257003784
train gradient:  0.12862069451683322
iteration : 6665
train acc:  0.7890625
train loss:  0.48692578077316284
train gradient:  0.16092112388440621
iteration : 6666
train acc:  0.6796875
train loss:  0.5554006099700928
train gradient:  0.19660350509376837
iteration : 6667
train acc:  0.7421875
train loss:  0.5307968854904175
train gradient:  0.13418810738691966
iteration : 6668
train acc:  0.71875
train loss:  0.5245005488395691
train gradient:  0.12035346751407285
iteration : 6669
train acc:  0.6953125
train loss:  0.5088150501251221
train gradient:  0.17155685866090348
iteration : 6670
train acc:  0.6796875
train loss:  0.5335514545440674
train gradient:  0.14118073134459555
iteration : 6671
train acc:  0.6875
train loss:  0.56160569190979
train gradient:  0.18272336000417955
iteration : 6672
train acc:  0.703125
train loss:  0.5386319160461426
train gradient:  0.15739007128990065
iteration : 6673
train acc:  0.703125
train loss:  0.5234156847000122
train gradient:  0.13830742571123
iteration : 6674
train acc:  0.7578125
train loss:  0.5044163465499878
train gradient:  0.17013762092623186
iteration : 6675
train acc:  0.7578125
train loss:  0.47904831171035767
train gradient:  0.14381007797295092
iteration : 6676
train acc:  0.7265625
train loss:  0.510942816734314
train gradient:  0.16796796254060123
iteration : 6677
train acc:  0.7421875
train loss:  0.5286590456962585
train gradient:  0.15780728522655169
iteration : 6678
train acc:  0.7109375
train loss:  0.5353412628173828
train gradient:  0.1336988701773192
iteration : 6679
train acc:  0.7265625
train loss:  0.5271030068397522
train gradient:  0.13627567843793031
iteration : 6680
train acc:  0.828125
train loss:  0.44163239002227783
train gradient:  0.1290685975802001
iteration : 6681
train acc:  0.78125
train loss:  0.4543581008911133
train gradient:  0.12543137154361972
iteration : 6682
train acc:  0.75
train loss:  0.48587822914123535
train gradient:  0.1347538946620917
iteration : 6683
train acc:  0.7265625
train loss:  0.4978596568107605
train gradient:  0.13133095464979422
iteration : 6684
train acc:  0.75
train loss:  0.4826141595840454
train gradient:  0.12628419442943178
iteration : 6685
train acc:  0.71875
train loss:  0.5158449411392212
train gradient:  0.1288890904676451
iteration : 6686
train acc:  0.734375
train loss:  0.5310614109039307
train gradient:  0.15878846395818008
iteration : 6687
train acc:  0.7265625
train loss:  0.5291323661804199
train gradient:  0.16327217540476896
iteration : 6688
train acc:  0.765625
train loss:  0.5146135091781616
train gradient:  0.13422242887084102
iteration : 6689
train acc:  0.6953125
train loss:  0.5818451642990112
train gradient:  0.23719723248170937
iteration : 6690
train acc:  0.78125
train loss:  0.48945045471191406
train gradient:  0.14634146317706803
iteration : 6691
train acc:  0.7578125
train loss:  0.4732007384300232
train gradient:  0.10800106643322994
iteration : 6692
train acc:  0.7109375
train loss:  0.5010507702827454
train gradient:  0.1426314783872069
iteration : 6693
train acc:  0.75
train loss:  0.47152984142303467
train gradient:  0.11695505942012052
iteration : 6694
train acc:  0.7265625
train loss:  0.5410658121109009
train gradient:  0.1288800659807559
iteration : 6695
train acc:  0.7265625
train loss:  0.4882971942424774
train gradient:  0.1305127882246988
iteration : 6696
train acc:  0.640625
train loss:  0.6669063568115234
train gradient:  0.19307719597587658
iteration : 6697
train acc:  0.8125
train loss:  0.4726410210132599
train gradient:  0.13529653291934834
iteration : 6698
train acc:  0.78125
train loss:  0.4964900612831116
train gradient:  0.1366606662666453
iteration : 6699
train acc:  0.734375
train loss:  0.5465148687362671
train gradient:  0.15178441279148247
iteration : 6700
train acc:  0.7109375
train loss:  0.5271810293197632
train gradient:  0.162217361927458
iteration : 6701
train acc:  0.71875
train loss:  0.554286003112793
train gradient:  0.1680992530136099
iteration : 6702
train acc:  0.6953125
train loss:  0.5327443480491638
train gradient:  0.12388567421622163
iteration : 6703
train acc:  0.7265625
train loss:  0.5088406801223755
train gradient:  0.1498589849264403
iteration : 6704
train acc:  0.7890625
train loss:  0.4547528028488159
train gradient:  0.11434302369345842
iteration : 6705
train acc:  0.7421875
train loss:  0.4872696101665497
train gradient:  0.12679611924364137
iteration : 6706
train acc:  0.7734375
train loss:  0.5108513832092285
train gradient:  0.15037779900401324
iteration : 6707
train acc:  0.75
train loss:  0.5243289470672607
train gradient:  0.18242715581651903
iteration : 6708
train acc:  0.7265625
train loss:  0.5415477156639099
train gradient:  0.17462000831316543
iteration : 6709
train acc:  0.7421875
train loss:  0.5271680355072021
train gradient:  0.1585850740934068
iteration : 6710
train acc:  0.7421875
train loss:  0.5148202180862427
train gradient:  0.14525507135913102
iteration : 6711
train acc:  0.6953125
train loss:  0.5296721458435059
train gradient:  0.14194612194706724
iteration : 6712
train acc:  0.765625
train loss:  0.48109155893325806
train gradient:  0.12344736759140676
iteration : 6713
train acc:  0.7578125
train loss:  0.47009775042533875
train gradient:  0.13469046671844337
iteration : 6714
train acc:  0.796875
train loss:  0.44821697473526
train gradient:  0.09947930582733244
iteration : 6715
train acc:  0.5859375
train loss:  0.6442946195602417
train gradient:  0.24209515278216992
iteration : 6716
train acc:  0.71875
train loss:  0.49026933312416077
train gradient:  0.12834551805193153
iteration : 6717
train acc:  0.734375
train loss:  0.5353446006774902
train gradient:  0.11904104442749705
iteration : 6718
train acc:  0.7890625
train loss:  0.4328225255012512
train gradient:  0.12985273926477756
iteration : 6719
train acc:  0.78125
train loss:  0.42646777629852295
train gradient:  0.09479188755262366
iteration : 6720
train acc:  0.796875
train loss:  0.4567296504974365
train gradient:  0.12039559245061791
iteration : 6721
train acc:  0.734375
train loss:  0.4911656975746155
train gradient:  0.17388267198080826
iteration : 6722
train acc:  0.703125
train loss:  0.5362560749053955
train gradient:  0.14027136952126223
iteration : 6723
train acc:  0.7734375
train loss:  0.4931388199329376
train gradient:  0.13251947289619886
iteration : 6724
train acc:  0.75
train loss:  0.51535964012146
train gradient:  0.13490322485095746
iteration : 6725
train acc:  0.765625
train loss:  0.47271019220352173
train gradient:  0.11429779958151891
iteration : 6726
train acc:  0.7421875
train loss:  0.46148985624313354
train gradient:  0.1139759568496522
iteration : 6727
train acc:  0.7421875
train loss:  0.5006347894668579
train gradient:  0.13046828388199005
iteration : 6728
train acc:  0.7734375
train loss:  0.4671766459941864
train gradient:  0.12209035143417873
iteration : 6729
train acc:  0.796875
train loss:  0.5070968270301819
train gradient:  0.2002196426189473
iteration : 6730
train acc:  0.703125
train loss:  0.5721332430839539
train gradient:  0.1447801678301998
iteration : 6731
train acc:  0.7421875
train loss:  0.4984414279460907
train gradient:  0.13665842027378905
iteration : 6732
train acc:  0.6953125
train loss:  0.5461747050285339
train gradient:  0.1714858097333263
iteration : 6733
train acc:  0.71875
train loss:  0.5121493339538574
train gradient:  0.1281944958077858
iteration : 6734
train acc:  0.703125
train loss:  0.5445643663406372
train gradient:  0.1713940045658999
iteration : 6735
train acc:  0.71875
train loss:  0.5408339500427246
train gradient:  0.21964030547044644
iteration : 6736
train acc:  0.7890625
train loss:  0.5279362797737122
train gradient:  0.15920778006939476
iteration : 6737
train acc:  0.7421875
train loss:  0.47749757766723633
train gradient:  0.11539714273079074
iteration : 6738
train acc:  0.78125
train loss:  0.47167348861694336
train gradient:  0.10498737905447247
iteration : 6739
train acc:  0.703125
train loss:  0.5246137380599976
train gradient:  0.1656125925954124
iteration : 6740
train acc:  0.7734375
train loss:  0.5086997747421265
train gradient:  0.15606841172221597
iteration : 6741
train acc:  0.71875
train loss:  0.4681236743927002
train gradient:  0.1359513277113964
iteration : 6742
train acc:  0.7734375
train loss:  0.4973548948764801
train gradient:  0.126860195075997
iteration : 6743
train acc:  0.6953125
train loss:  0.5443466901779175
train gradient:  0.18424083222176735
iteration : 6744
train acc:  0.7265625
train loss:  0.5618200302124023
train gradient:  0.1754495253334546
iteration : 6745
train acc:  0.8125
train loss:  0.472925066947937
train gradient:  0.09230328135167484
iteration : 6746
train acc:  0.78125
train loss:  0.5117225646972656
train gradient:  0.1692836042536851
iteration : 6747
train acc:  0.6875
train loss:  0.5225005149841309
train gradient:  0.14788011170053866
iteration : 6748
train acc:  0.703125
train loss:  0.5187952518463135
train gradient:  0.1842675043634616
iteration : 6749
train acc:  0.828125
train loss:  0.41587960720062256
train gradient:  0.10298687287196993
iteration : 6750
train acc:  0.6875
train loss:  0.554113507270813
train gradient:  0.18180627865227994
iteration : 6751
train acc:  0.6796875
train loss:  0.594630777835846
train gradient:  0.1653555871810586
iteration : 6752
train acc:  0.7109375
train loss:  0.5047078132629395
train gradient:  0.133216647567749
iteration : 6753
train acc:  0.703125
train loss:  0.5703240633010864
train gradient:  0.1437903486228963
iteration : 6754
train acc:  0.7421875
train loss:  0.5010499358177185
train gradient:  0.14687320070320237
iteration : 6755
train acc:  0.7421875
train loss:  0.507096529006958
train gradient:  0.1449292474928971
iteration : 6756
train acc:  0.703125
train loss:  0.5501785278320312
train gradient:  0.1905176398459935
iteration : 6757
train acc:  0.6953125
train loss:  0.5423766374588013
train gradient:  0.19555539511814157
iteration : 6758
train acc:  0.703125
train loss:  0.530943751335144
train gradient:  0.1295631855161637
iteration : 6759
train acc:  0.7578125
train loss:  0.47438567876815796
train gradient:  0.12185849935759147
iteration : 6760
train acc:  0.8046875
train loss:  0.4417140483856201
train gradient:  0.11667373832785766
iteration : 6761
train acc:  0.7109375
train loss:  0.52054762840271
train gradient:  0.13719899372656416
iteration : 6762
train acc:  0.734375
train loss:  0.5166621208190918
train gradient:  0.14420916992875932
iteration : 6763
train acc:  0.7421875
train loss:  0.49314194917678833
train gradient:  0.10996295756732202
iteration : 6764
train acc:  0.75
train loss:  0.5001161694526672
train gradient:  0.12060025950219816
iteration : 6765
train acc:  0.75
train loss:  0.5218414664268494
train gradient:  0.12810444454636466
iteration : 6766
train acc:  0.71875
train loss:  0.511657178401947
train gradient:  0.14653797329048562
iteration : 6767
train acc:  0.8046875
train loss:  0.4249380826950073
train gradient:  0.11550338652946808
iteration : 6768
train acc:  0.6953125
train loss:  0.5096774101257324
train gradient:  0.15945833943376608
iteration : 6769
train acc:  0.75
train loss:  0.4552210569381714
train gradient:  0.11244926112510223
iteration : 6770
train acc:  0.7265625
train loss:  0.5239599943161011
train gradient:  0.1218915536468104
iteration : 6771
train acc:  0.703125
train loss:  0.5168960094451904
train gradient:  0.17036106978802745
iteration : 6772
train acc:  0.75
train loss:  0.45773640275001526
train gradient:  0.12304170896994832
iteration : 6773
train acc:  0.734375
train loss:  0.5232136845588684
train gradient:  0.21734872963512253
iteration : 6774
train acc:  0.734375
train loss:  0.5083671808242798
train gradient:  0.14890883075660666
iteration : 6775
train acc:  0.71875
train loss:  0.5112055540084839
train gradient:  0.1603701930820381
iteration : 6776
train acc:  0.6953125
train loss:  0.5460479855537415
train gradient:  0.1556764019350568
iteration : 6777
train acc:  0.78125
train loss:  0.47143733501434326
train gradient:  0.11454830945484892
iteration : 6778
train acc:  0.71875
train loss:  0.563651978969574
train gradient:  0.21033936162279815
iteration : 6779
train acc:  0.7109375
train loss:  0.5869163870811462
train gradient:  0.1727470577914822
iteration : 6780
train acc:  0.6875
train loss:  0.5482161641120911
train gradient:  0.14201522649772275
iteration : 6781
train acc:  0.6640625
train loss:  0.5777091383934021
train gradient:  0.17738479152527797
iteration : 6782
train acc:  0.78125
train loss:  0.5103734731674194
train gradient:  0.10193723187163371
iteration : 6783
train acc:  0.75
train loss:  0.5869839191436768
train gradient:  0.16882389082689808
iteration : 6784
train acc:  0.6875
train loss:  0.5227918028831482
train gradient:  0.1363258250779696
iteration : 6785
train acc:  0.7265625
train loss:  0.5384536981582642
train gradient:  0.15957998810443122
iteration : 6786
train acc:  0.7421875
train loss:  0.5202475190162659
train gradient:  0.14157072004503055
iteration : 6787
train acc:  0.75
train loss:  0.4690534472465515
train gradient:  0.12952800541986886
iteration : 6788
train acc:  0.6796875
train loss:  0.5449255704879761
train gradient:  0.11373988669792386
iteration : 6789
train acc:  0.7578125
train loss:  0.4942399561405182
train gradient:  0.13179523814987856
iteration : 6790
train acc:  0.6796875
train loss:  0.5581930875778198
train gradient:  0.15465775422507133
iteration : 6791
train acc:  0.71875
train loss:  0.49833136796951294
train gradient:  0.1504001028080705
iteration : 6792
train acc:  0.7265625
train loss:  0.5316201448440552
train gradient:  0.13027024604606946
iteration : 6793
train acc:  0.7890625
train loss:  0.47078603506088257
train gradient:  0.1475505639594613
iteration : 6794
train acc:  0.6953125
train loss:  0.5506889820098877
train gradient:  0.14431641596280922
iteration : 6795
train acc:  0.8125
train loss:  0.45326828956604004
train gradient:  0.11082728215096399
iteration : 6796
train acc:  0.7265625
train loss:  0.531392514705658
train gradient:  0.1511826065674259
iteration : 6797
train acc:  0.7734375
train loss:  0.5267425179481506
train gradient:  0.15842490693965183
iteration : 6798
train acc:  0.7109375
train loss:  0.5171666741371155
train gradient:  0.14596336060242976
iteration : 6799
train acc:  0.6640625
train loss:  0.5666244626045227
train gradient:  0.2178879501476943
iteration : 6800
train acc:  0.7109375
train loss:  0.5362906455993652
train gradient:  0.14126857935113118
iteration : 6801
train acc:  0.796875
train loss:  0.42516255378723145
train gradient:  0.10293507138390573
iteration : 6802
train acc:  0.671875
train loss:  0.5253713130950928
train gradient:  0.12088614044658551
iteration : 6803
train acc:  0.75
train loss:  0.494594544172287
train gradient:  0.12941320523564925
iteration : 6804
train acc:  0.71875
train loss:  0.4785500764846802
train gradient:  0.11527674212953778
iteration : 6805
train acc:  0.671875
train loss:  0.5576302409172058
train gradient:  0.18166856398613046
iteration : 6806
train acc:  0.7734375
train loss:  0.43205469846725464
train gradient:  0.11235779188358579
iteration : 6807
train acc:  0.7578125
train loss:  0.4774937033653259
train gradient:  0.11775219296596645
iteration : 6808
train acc:  0.703125
train loss:  0.5214276909828186
train gradient:  0.11042466182329722
iteration : 6809
train acc:  0.7421875
train loss:  0.5299824476242065
train gradient:  0.15915911972045083
iteration : 6810
train acc:  0.6953125
train loss:  0.5187947154045105
train gradient:  0.16428429889509824
iteration : 6811
train acc:  0.8515625
train loss:  0.4009588658809662
train gradient:  0.08895592889911844
iteration : 6812
train acc:  0.703125
train loss:  0.5884215831756592
train gradient:  0.21071569916453564
iteration : 6813
train acc:  0.7734375
train loss:  0.48417359590530396
train gradient:  0.12398663308882951
iteration : 6814
train acc:  0.7265625
train loss:  0.5213590860366821
train gradient:  0.1458425345708147
iteration : 6815
train acc:  0.7578125
train loss:  0.5033290982246399
train gradient:  0.17013456841653096
iteration : 6816
train acc:  0.734375
train loss:  0.48054030537605286
train gradient:  0.14010270180960033
iteration : 6817
train acc:  0.75
train loss:  0.48155367374420166
train gradient:  0.11358684455493159
iteration : 6818
train acc:  0.78125
train loss:  0.501335620880127
train gradient:  0.11838811477752395
iteration : 6819
train acc:  0.640625
train loss:  0.6008093357086182
train gradient:  0.17924281455558674
iteration : 6820
train acc:  0.71875
train loss:  0.5284391045570374
train gradient:  0.1416433198532353
iteration : 6821
train acc:  0.71875
train loss:  0.520409345626831
train gradient:  0.1527275193269897
iteration : 6822
train acc:  0.703125
train loss:  0.5404737591743469
train gradient:  0.147074314081973
iteration : 6823
train acc:  0.671875
train loss:  0.5632340312004089
train gradient:  0.18622072527208816
iteration : 6824
train acc:  0.7421875
train loss:  0.46054255962371826
train gradient:  0.09970120388175299
iteration : 6825
train acc:  0.671875
train loss:  0.5606886148452759
train gradient:  0.16698169654654127
iteration : 6826
train acc:  0.7421875
train loss:  0.497844934463501
train gradient:  0.13319074054282598
iteration : 6827
train acc:  0.7421875
train loss:  0.48184022307395935
train gradient:  0.11551676848744331
iteration : 6828
train acc:  0.7109375
train loss:  0.5366833806037903
train gradient:  0.1592359431444741
iteration : 6829
train acc:  0.7734375
train loss:  0.49865850806236267
train gradient:  0.1328115449144547
iteration : 6830
train acc:  0.7734375
train loss:  0.4539901912212372
train gradient:  0.12506175381829965
iteration : 6831
train acc:  0.734375
train loss:  0.5388510227203369
train gradient:  0.1654106721755304
iteration : 6832
train acc:  0.71875
train loss:  0.511559247970581
train gradient:  0.13043732527181756
iteration : 6833
train acc:  0.7890625
train loss:  0.48288846015930176
train gradient:  0.1613490430450345
iteration : 6834
train acc:  0.6796875
train loss:  0.5138322710990906
train gradient:  0.13484490703082133
iteration : 6835
train acc:  0.6640625
train loss:  0.5724551677703857
train gradient:  0.14235391061403962
iteration : 6836
train acc:  0.7421875
train loss:  0.4901939630508423
train gradient:  0.15268566028796687
iteration : 6837
train acc:  0.703125
train loss:  0.5690593719482422
train gradient:  0.15075339283057862
iteration : 6838
train acc:  0.703125
train loss:  0.48063966631889343
train gradient:  0.1223836025457339
iteration : 6839
train acc:  0.8203125
train loss:  0.40710538625717163
train gradient:  0.10017722096992746
iteration : 6840
train acc:  0.734375
train loss:  0.5087031722068787
train gradient:  0.13530350082659476
iteration : 6841
train acc:  0.75
train loss:  0.5064672231674194
train gradient:  0.12801375467170512
iteration : 6842
train acc:  0.6953125
train loss:  0.5559049248695374
train gradient:  0.17048973388451966
iteration : 6843
train acc:  0.7578125
train loss:  0.4882669448852539
train gradient:  0.13868556795927758
iteration : 6844
train acc:  0.7421875
train loss:  0.4810909032821655
train gradient:  0.1204856431311319
iteration : 6845
train acc:  0.765625
train loss:  0.49547266960144043
train gradient:  0.15315387091835325
iteration : 6846
train acc:  0.7421875
train loss:  0.5039829015731812
train gradient:  0.20915119036177676
iteration : 6847
train acc:  0.7265625
train loss:  0.5290399789810181
train gradient:  0.18396875942289354
iteration : 6848
train acc:  0.7109375
train loss:  0.5372992753982544
train gradient:  0.14668656592952545
iteration : 6849
train acc:  0.6953125
train loss:  0.5443642139434814
train gradient:  0.1143935846861323
iteration : 6850
train acc:  0.78125
train loss:  0.5161216855049133
train gradient:  0.12266625837415351
iteration : 6851
train acc:  0.6328125
train loss:  0.6265953183174133
train gradient:  0.22720775536913923
iteration : 6852
train acc:  0.7109375
train loss:  0.5400592684745789
train gradient:  0.14748015380285123
iteration : 6853
train acc:  0.8515625
train loss:  0.40960395336151123
train gradient:  0.08692030202751222
iteration : 6854
train acc:  0.7890625
train loss:  0.4582618772983551
train gradient:  0.13636451742245187
iteration : 6855
train acc:  0.78125
train loss:  0.4858010709285736
train gradient:  0.13003316450812868
iteration : 6856
train acc:  0.6875
train loss:  0.5448653697967529
train gradient:  0.12721074973742494
iteration : 6857
train acc:  0.796875
train loss:  0.46591901779174805
train gradient:  0.08683409336486106
iteration : 6858
train acc:  0.7109375
train loss:  0.4856591522693634
train gradient:  0.11878385365568508
iteration : 6859
train acc:  0.796875
train loss:  0.44645577669143677
train gradient:  0.13694766255395793
iteration : 6860
train acc:  0.78125
train loss:  0.43555548787117004
train gradient:  0.16400082665750168
iteration : 6861
train acc:  0.7265625
train loss:  0.4990572929382324
train gradient:  0.12818212261437162
iteration : 6862
train acc:  0.7421875
train loss:  0.5276948809623718
train gradient:  0.1415843475484221
iteration : 6863
train acc:  0.8046875
train loss:  0.4299089312553406
train gradient:  0.1061871245443754
iteration : 6864
train acc:  0.71875
train loss:  0.5303990244865417
train gradient:  0.1538682215035495
iteration : 6865
train acc:  0.75
train loss:  0.5238497853279114
train gradient:  0.1529525285297882
iteration : 6866
train acc:  0.7109375
train loss:  0.5512187480926514
train gradient:  0.14510047646161578
iteration : 6867
train acc:  0.7421875
train loss:  0.4907759428024292
train gradient:  0.1354177664243118
iteration : 6868
train acc:  0.7265625
train loss:  0.5186439156532288
train gradient:  0.17691404864176213
iteration : 6869
train acc:  0.796875
train loss:  0.47138506174087524
train gradient:  0.15080227606194363
iteration : 6870
train acc:  0.75
train loss:  0.5016459822654724
train gradient:  0.1489498757447999
iteration : 6871
train acc:  0.6328125
train loss:  0.6017447710037231
train gradient:  0.17188691533976433
iteration : 6872
train acc:  0.71875
train loss:  0.5374129414558411
train gradient:  0.14591478812741782
iteration : 6873
train acc:  0.734375
train loss:  0.509907603263855
train gradient:  0.16529096317987851
iteration : 6874
train acc:  0.796875
train loss:  0.44694414734840393
train gradient:  0.11848168864421742
iteration : 6875
train acc:  0.703125
train loss:  0.4842281937599182
train gradient:  0.12633995979752963
iteration : 6876
train acc:  0.734375
train loss:  0.5209648013114929
train gradient:  0.1254570535876668
iteration : 6877
train acc:  0.7109375
train loss:  0.518025279045105
train gradient:  0.15139588978923793
iteration : 6878
train acc:  0.7734375
train loss:  0.4780329167842865
train gradient:  0.13445907110095068
iteration : 6879
train acc:  0.7109375
train loss:  0.5067998170852661
train gradient:  0.11364092427198759
iteration : 6880
train acc:  0.6875
train loss:  0.5835138559341431
train gradient:  0.13999184393809194
iteration : 6881
train acc:  0.7421875
train loss:  0.5001325011253357
train gradient:  0.15449607845684915
iteration : 6882
train acc:  0.6953125
train loss:  0.5589368343353271
train gradient:  0.14155087998996174
iteration : 6883
train acc:  0.7421875
train loss:  0.4784355163574219
train gradient:  0.12366673751941026
iteration : 6884
train acc:  0.7578125
train loss:  0.44912469387054443
train gradient:  0.1167573569084167
iteration : 6885
train acc:  0.8125
train loss:  0.4609494209289551
train gradient:  0.14475688978789145
iteration : 6886
train acc:  0.7734375
train loss:  0.494617223739624
train gradient:  0.14437098699337572
iteration : 6887
train acc:  0.7421875
train loss:  0.4974435269832611
train gradient:  0.1251392725327512
iteration : 6888
train acc:  0.7421875
train loss:  0.5138362646102905
train gradient:  0.10862663850581712
iteration : 6889
train acc:  0.7578125
train loss:  0.5064195394515991
train gradient:  0.15167931688542072
iteration : 6890
train acc:  0.6953125
train loss:  0.5391205549240112
train gradient:  0.19796760391437124
iteration : 6891
train acc:  0.6640625
train loss:  0.6027040481567383
train gradient:  0.14299932721477704
iteration : 6892
train acc:  0.7109375
train loss:  0.543694019317627
train gradient:  0.1827540747838342
iteration : 6893
train acc:  0.734375
train loss:  0.48124033212661743
train gradient:  0.11696941161070538
iteration : 6894
train acc:  0.7734375
train loss:  0.5077551603317261
train gradient:  0.1324102217426371
iteration : 6895
train acc:  0.6953125
train loss:  0.51199871301651
train gradient:  0.13039280246901752
iteration : 6896
train acc:  0.6953125
train loss:  0.5294166803359985
train gradient:  0.13817750836057144
iteration : 6897
train acc:  0.6484375
train loss:  0.6079766750335693
train gradient:  0.17975952184255067
iteration : 6898
train acc:  0.6796875
train loss:  0.51932692527771
train gradient:  0.1302932093110993
iteration : 6899
train acc:  0.7578125
train loss:  0.503555953502655
train gradient:  0.1468002834140426
iteration : 6900
train acc:  0.734375
train loss:  0.5070489048957825
train gradient:  0.12112898438588082
iteration : 6901
train acc:  0.6875
train loss:  0.5946242213249207
train gradient:  0.19319033549812342
iteration : 6902
train acc:  0.71875
train loss:  0.49189329147338867
train gradient:  0.16035493005974744
iteration : 6903
train acc:  0.7109375
train loss:  0.5773666501045227
train gradient:  0.16593496848189823
iteration : 6904
train acc:  0.75
train loss:  0.532197892665863
train gradient:  0.1252126443333133
iteration : 6905
train acc:  0.7109375
train loss:  0.5763095617294312
train gradient:  0.20533283278929637
iteration : 6906
train acc:  0.6796875
train loss:  0.541506290435791
train gradient:  0.20265371952941036
iteration : 6907
train acc:  0.75
train loss:  0.5005017518997192
train gradient:  0.13385966857923032
iteration : 6908
train acc:  0.75
train loss:  0.5166283845901489
train gradient:  0.18643771068837572
iteration : 6909
train acc:  0.671875
train loss:  0.5548630952835083
train gradient:  0.135085183649359
iteration : 6910
train acc:  0.7109375
train loss:  0.5545136332511902
train gradient:  0.14945745125680276
iteration : 6911
train acc:  0.734375
train loss:  0.5072000026702881
train gradient:  0.12870782852081997
iteration : 6912
train acc:  0.7578125
train loss:  0.48328354954719543
train gradient:  0.1315623587729531
iteration : 6913
train acc:  0.703125
train loss:  0.5187411308288574
train gradient:  0.1717675850916931
iteration : 6914
train acc:  0.671875
train loss:  0.5762690305709839
train gradient:  0.15364408013540592
iteration : 6915
train acc:  0.7265625
train loss:  0.5304915308952332
train gradient:  0.16290403881762885
iteration : 6916
train acc:  0.734375
train loss:  0.5392547845840454
train gradient:  0.14884729895257146
iteration : 6917
train acc:  0.734375
train loss:  0.4817965030670166
train gradient:  0.17958804352490879
iteration : 6918
train acc:  0.71875
train loss:  0.5043485164642334
train gradient:  0.1769453885599484
iteration : 6919
train acc:  0.7421875
train loss:  0.5130098462104797
train gradient:  0.1249680202613067
iteration : 6920
train acc:  0.734375
train loss:  0.5088558197021484
train gradient:  0.1474331859525446
iteration : 6921
train acc:  0.703125
train loss:  0.5442186594009399
train gradient:  0.14217125873643555
iteration : 6922
train acc:  0.6953125
train loss:  0.49697792530059814
train gradient:  0.13376299502305872
iteration : 6923
train acc:  0.6953125
train loss:  0.518200159072876
train gradient:  0.15457183267586785
iteration : 6924
train acc:  0.7265625
train loss:  0.528731107711792
train gradient:  0.13220787684108753
iteration : 6925
train acc:  0.765625
train loss:  0.4757162034511566
train gradient:  0.1444913251639977
iteration : 6926
train acc:  0.671875
train loss:  0.5425360798835754
train gradient:  0.17202680680673244
iteration : 6927
train acc:  0.765625
train loss:  0.4972567856311798
train gradient:  0.16511549497791417
iteration : 6928
train acc:  0.7578125
train loss:  0.556159496307373
train gradient:  0.20225752176252762
iteration : 6929
train acc:  0.6171875
train loss:  0.6111277937889099
train gradient:  0.18838108079769583
iteration : 6930
train acc:  0.7265625
train loss:  0.4848136901855469
train gradient:  0.11476699300200488
iteration : 6931
train acc:  0.6796875
train loss:  0.5260950326919556
train gradient:  0.1965322327596103
iteration : 6932
train acc:  0.796875
train loss:  0.44854599237442017
train gradient:  0.13112316386558054
iteration : 6933
train acc:  0.7421875
train loss:  0.481067955493927
train gradient:  0.12731353404892792
iteration : 6934
train acc:  0.7109375
train loss:  0.5312777757644653
train gradient:  0.16947710974455113
iteration : 6935
train acc:  0.703125
train loss:  0.570281982421875
train gradient:  0.1796128563082316
iteration : 6936
train acc:  0.796875
train loss:  0.46497809886932373
train gradient:  0.10821402950053563
iteration : 6937
train acc:  0.640625
train loss:  0.5923665761947632
train gradient:  0.16004655674828294
iteration : 6938
train acc:  0.7421875
train loss:  0.46931347250938416
train gradient:  0.09651100080943803
iteration : 6939
train acc:  0.71875
train loss:  0.5022702217102051
train gradient:  0.14555287766821529
iteration : 6940
train acc:  0.75
train loss:  0.49419641494750977
train gradient:  0.11754459426142655
iteration : 6941
train acc:  0.765625
train loss:  0.4472578763961792
train gradient:  0.15417145019713857
iteration : 6942
train acc:  0.75
train loss:  0.5389549136161804
train gradient:  0.14603754995000318
iteration : 6943
train acc:  0.6953125
train loss:  0.4958362579345703
train gradient:  0.11799813506238861
iteration : 6944
train acc:  0.7421875
train loss:  0.5379923582077026
train gradient:  0.17310397288616086
iteration : 6945
train acc:  0.7109375
train loss:  0.5487033128738403
train gradient:  0.17101287111001973
iteration : 6946
train acc:  0.75
train loss:  0.4750646650791168
train gradient:  0.11222002722121022
iteration : 6947
train acc:  0.734375
train loss:  0.5226376056671143
train gradient:  0.14502551025615115
iteration : 6948
train acc:  0.828125
train loss:  0.4267743229866028
train gradient:  0.10799414024632084
iteration : 6949
train acc:  0.7265625
train loss:  0.5093094110488892
train gradient:  0.12950769306911247
iteration : 6950
train acc:  0.7578125
train loss:  0.499374121427536
train gradient:  0.14611951444339202
iteration : 6951
train acc:  0.75
train loss:  0.4915699064731598
train gradient:  0.13362847057018964
iteration : 6952
train acc:  0.7421875
train loss:  0.5212932825088501
train gradient:  0.14143490827114696
iteration : 6953
train acc:  0.7265625
train loss:  0.5595778822898865
train gradient:  0.19088240262294065
iteration : 6954
train acc:  0.7578125
train loss:  0.5250006914138794
train gradient:  0.17062357903388836
iteration : 6955
train acc:  0.7265625
train loss:  0.5264377593994141
train gradient:  0.21210411775612004
iteration : 6956
train acc:  0.75
train loss:  0.49116525053977966
train gradient:  0.1712003513583294
iteration : 6957
train acc:  0.7265625
train loss:  0.45984700322151184
train gradient:  0.11323179767667665
iteration : 6958
train acc:  0.7421875
train loss:  0.4556044936180115
train gradient:  0.13024400100458966
iteration : 6959
train acc:  0.765625
train loss:  0.49818703532218933
train gradient:  0.14062921873947726
iteration : 6960
train acc:  0.734375
train loss:  0.47703826427459717
train gradient:  0.1234171526409222
iteration : 6961
train acc:  0.703125
train loss:  0.5626437067985535
train gradient:  0.1577816214010782
iteration : 6962
train acc:  0.6953125
train loss:  0.5519782900810242
train gradient:  0.16947094305965857
iteration : 6963
train acc:  0.734375
train loss:  0.4890163242816925
train gradient:  0.13567011576808558
iteration : 6964
train acc:  0.7734375
train loss:  0.46112364530563354
train gradient:  0.10529473285007367
iteration : 6965
train acc:  0.6953125
train loss:  0.5467997789382935
train gradient:  0.19401314290689622
iteration : 6966
train acc:  0.7578125
train loss:  0.4758325219154358
train gradient:  0.1363985146078528
iteration : 6967
train acc:  0.7890625
train loss:  0.45197930932044983
train gradient:  0.11177942191685598
iteration : 6968
train acc:  0.734375
train loss:  0.47480010986328125
train gradient:  0.1193222796149167
iteration : 6969
train acc:  0.7109375
train loss:  0.57940673828125
train gradient:  0.19198043620662697
iteration : 6970
train acc:  0.75
train loss:  0.46905022859573364
train gradient:  0.13695461203620438
iteration : 6971
train acc:  0.7265625
train loss:  0.5231350660324097
train gradient:  0.14927728859232636
iteration : 6972
train acc:  0.765625
train loss:  0.4769737422466278
train gradient:  0.1199771597325734
iteration : 6973
train acc:  0.71875
train loss:  0.5342976450920105
train gradient:  0.12947929485628393
iteration : 6974
train acc:  0.71875
train loss:  0.47789737582206726
train gradient:  0.12614324171353888
iteration : 6975
train acc:  0.75
train loss:  0.4609231948852539
train gradient:  0.1096478565344265
iteration : 6976
train acc:  0.75
train loss:  0.503479540348053
train gradient:  0.13691636191147477
iteration : 6977
train acc:  0.7421875
train loss:  0.5130940675735474
train gradient:  0.1637828530155881
iteration : 6978
train acc:  0.7109375
train loss:  0.5757231712341309
train gradient:  0.15897651876503696
iteration : 6979
train acc:  0.765625
train loss:  0.45404112339019775
train gradient:  0.1291931285978843
iteration : 6980
train acc:  0.75
train loss:  0.47104084491729736
train gradient:  0.13698561854167451
iteration : 6981
train acc:  0.6484375
train loss:  0.5604634284973145
train gradient:  0.14890435623541282
iteration : 6982
train acc:  0.7265625
train loss:  0.5139869451522827
train gradient:  0.13534015284656767
iteration : 6983
train acc:  0.734375
train loss:  0.5229012370109558
train gradient:  0.1931553424825848
iteration : 6984
train acc:  0.78125
train loss:  0.4936239421367645
train gradient:  0.12339321387458847
iteration : 6985
train acc:  0.7109375
train loss:  0.49569958448410034
train gradient:  0.1479752012201926
iteration : 6986
train acc:  0.78125
train loss:  0.4701758623123169
train gradient:  0.12973724375694565
iteration : 6987
train acc:  0.6875
train loss:  0.5534085035324097
train gradient:  0.20365049597743218
iteration : 6988
train acc:  0.6875
train loss:  0.5699574947357178
train gradient:  0.20845050219654374
iteration : 6989
train acc:  0.7421875
train loss:  0.5162248611450195
train gradient:  0.16844252207589372
iteration : 6990
train acc:  0.703125
train loss:  0.5186688303947449
train gradient:  0.15486298182076252
iteration : 6991
train acc:  0.703125
train loss:  0.5403240919113159
train gradient:  0.15506754780112436
iteration : 6992
train acc:  0.6796875
train loss:  0.604476809501648
train gradient:  0.18147472285006627
iteration : 6993
train acc:  0.7265625
train loss:  0.49482619762420654
train gradient:  0.12828689051969183
iteration : 6994
train acc:  0.7421875
train loss:  0.513057291507721
train gradient:  0.1533320956641005
iteration : 6995
train acc:  0.6875
train loss:  0.5513556003570557
train gradient:  0.15214668217766936
iteration : 6996
train acc:  0.7890625
train loss:  0.4647926092147827
train gradient:  0.12001700812551896
iteration : 6997
train acc:  0.6953125
train loss:  0.5255328416824341
train gradient:  0.11981263037847846
iteration : 6998
train acc:  0.7734375
train loss:  0.4597069025039673
train gradient:  0.11518246251513373
iteration : 6999
train acc:  0.796875
train loss:  0.45871323347091675
train gradient:  0.12967033664587407
iteration : 7000
train acc:  0.6640625
train loss:  0.5446262359619141
train gradient:  0.161389947404247
iteration : 7001
train acc:  0.6953125
train loss:  0.5973422527313232
train gradient:  0.2049637170464781
iteration : 7002
train acc:  0.6796875
train loss:  0.5606973171234131
train gradient:  0.19485069848084435
iteration : 7003
train acc:  0.71875
train loss:  0.5164717435836792
train gradient:  0.11441244322291957
iteration : 7004
train acc:  0.765625
train loss:  0.4733300805091858
train gradient:  0.12239344431752015
iteration : 7005
train acc:  0.703125
train loss:  0.5497774481773376
train gradient:  0.151889832415594
iteration : 7006
train acc:  0.765625
train loss:  0.44889387488365173
train gradient:  0.1282187668223091
iteration : 7007
train acc:  0.7265625
train loss:  0.5409955382347107
train gradient:  0.17030239119739737
iteration : 7008
train acc:  0.75
train loss:  0.4593246281147003
train gradient:  0.1076357306047238
iteration : 7009
train acc:  0.6875
train loss:  0.5648791790008545
train gradient:  0.1863726400887729
iteration : 7010
train acc:  0.78125
train loss:  0.4688635468482971
train gradient:  0.14197898903422757
iteration : 7011
train acc:  0.75
train loss:  0.45378535985946655
train gradient:  0.10144595160303373
iteration : 7012
train acc:  0.7734375
train loss:  0.45205092430114746
train gradient:  0.10624363662178465
iteration : 7013
train acc:  0.765625
train loss:  0.4349774718284607
train gradient:  0.11885176254543826
iteration : 7014
train acc:  0.7734375
train loss:  0.45911723375320435
train gradient:  0.11740776672724287
iteration : 7015
train acc:  0.65625
train loss:  0.5623931288719177
train gradient:  0.15744922024722913
iteration : 7016
train acc:  0.765625
train loss:  0.48131972551345825
train gradient:  0.11747326480813422
iteration : 7017
train acc:  0.765625
train loss:  0.5493334531784058
train gradient:  0.1574719286623919
iteration : 7018
train acc:  0.7421875
train loss:  0.4856279492378235
train gradient:  0.17714662647896473
iteration : 7019
train acc:  0.6875
train loss:  0.5613526105880737
train gradient:  0.148086539242193
iteration : 7020
train acc:  0.671875
train loss:  0.5833824872970581
train gradient:  0.2013472383064358
iteration : 7021
train acc:  0.734375
train loss:  0.5489844083786011
train gradient:  0.14765707481775275
iteration : 7022
train acc:  0.75
train loss:  0.48368215560913086
train gradient:  0.0988896215992686
iteration : 7023
train acc:  0.7109375
train loss:  0.5418113470077515
train gradient:  0.13782820614560476
iteration : 7024
train acc:  0.7578125
train loss:  0.5251836180686951
train gradient:  0.16263477048876984
iteration : 7025
train acc:  0.671875
train loss:  0.5530330538749695
train gradient:  0.19171641757121338
iteration : 7026
train acc:  0.734375
train loss:  0.4969523549079895
train gradient:  0.14730827903883842
iteration : 7027
train acc:  0.78125
train loss:  0.43420782685279846
train gradient:  0.12070522552926317
iteration : 7028
train acc:  0.734375
train loss:  0.5013826489448547
train gradient:  0.13836215000571433
iteration : 7029
train acc:  0.734375
train loss:  0.5018675327301025
train gradient:  0.14572479161214463
iteration : 7030
train acc:  0.625
train loss:  0.6469123959541321
train gradient:  0.15860365509437555
iteration : 7031
train acc:  0.7734375
train loss:  0.4791336953639984
train gradient:  0.13164364544481083
iteration : 7032
train acc:  0.8125
train loss:  0.4305976927280426
train gradient:  0.127545859616467
iteration : 7033
train acc:  0.7421875
train loss:  0.5068936347961426
train gradient:  0.11237228598262891
iteration : 7034
train acc:  0.7265625
train loss:  0.5095382928848267
train gradient:  0.11669801551903317
iteration : 7035
train acc:  0.6875
train loss:  0.47959911823272705
train gradient:  0.13773078643279069
iteration : 7036
train acc:  0.734375
train loss:  0.4779532849788666
train gradient:  0.10374511081633746
iteration : 7037
train acc:  0.734375
train loss:  0.5071604251861572
train gradient:  0.15205563621391227
iteration : 7038
train acc:  0.71875
train loss:  0.5062214732170105
train gradient:  0.11565001581555251
iteration : 7039
train acc:  0.6875
train loss:  0.5563459396362305
train gradient:  0.19818422666752114
iteration : 7040
train acc:  0.78125
train loss:  0.5041587948799133
train gradient:  0.13215136959569404
iteration : 7041
train acc:  0.7578125
train loss:  0.49716442823410034
train gradient:  0.1216206676904818
iteration : 7042
train acc:  0.734375
train loss:  0.4729222059249878
train gradient:  0.16251599213194373
iteration : 7043
train acc:  0.7265625
train loss:  0.5067723989486694
train gradient:  0.1279190819751888
iteration : 7044
train acc:  0.65625
train loss:  0.5780116319656372
train gradient:  0.16868393695524553
iteration : 7045
train acc:  0.7734375
train loss:  0.5163098573684692
train gradient:  0.13801038167868057
iteration : 7046
train acc:  0.78125
train loss:  0.4919082522392273
train gradient:  0.1189132487794222
iteration : 7047
train acc:  0.7421875
train loss:  0.48853227496147156
train gradient:  0.10795993980738738
iteration : 7048
train acc:  0.765625
train loss:  0.4818473756313324
train gradient:  0.12540043775904477
iteration : 7049
train acc:  0.8046875
train loss:  0.42516377568244934
train gradient:  0.09723297233224276
iteration : 7050
train acc:  0.6953125
train loss:  0.5228054523468018
train gradient:  0.16380320888659677
iteration : 7051
train acc:  0.703125
train loss:  0.5694566965103149
train gradient:  0.16731991072950486
iteration : 7052
train acc:  0.6640625
train loss:  0.5322341918945312
train gradient:  0.17953514575086282
iteration : 7053
train acc:  0.7734375
train loss:  0.4885829985141754
train gradient:  0.1348910134748688
iteration : 7054
train acc:  0.6484375
train loss:  0.5934927463531494
train gradient:  0.16461469019279745
iteration : 7055
train acc:  0.765625
train loss:  0.511989951133728
train gradient:  0.1317773383984729
iteration : 7056
train acc:  0.71875
train loss:  0.5608368515968323
train gradient:  0.1815687425518054
iteration : 7057
train acc:  0.734375
train loss:  0.493549108505249
train gradient:  0.10860788573037693
iteration : 7058
train acc:  0.6796875
train loss:  0.527629554271698
train gradient:  0.13035782812677751
iteration : 7059
train acc:  0.6953125
train loss:  0.5225350260734558
train gradient:  0.1429811792970378
iteration : 7060
train acc:  0.75
train loss:  0.4850974380970001
train gradient:  0.09851804836161751
iteration : 7061
train acc:  0.78125
train loss:  0.43511608242988586
train gradient:  0.123891610762587
iteration : 7062
train acc:  0.7421875
train loss:  0.48928725719451904
train gradient:  0.1316720184658841
iteration : 7063
train acc:  0.7421875
train loss:  0.4617765545845032
train gradient:  0.1549359104753872
iteration : 7064
train acc:  0.7265625
train loss:  0.4802558422088623
train gradient:  0.145174447231677
iteration : 7065
train acc:  0.7265625
train loss:  0.4990316927433014
train gradient:  0.11822264383816461
iteration : 7066
train acc:  0.7265625
train loss:  0.471640408039093
train gradient:  0.11677890622254763
iteration : 7067
train acc:  0.75
train loss:  0.4809456765651703
train gradient:  0.12339885731539822
iteration : 7068
train acc:  0.671875
train loss:  0.5813161730766296
train gradient:  0.2192953474270491
iteration : 7069
train acc:  0.7578125
train loss:  0.49883362650871277
train gradient:  0.12426468144427864
iteration : 7070
train acc:  0.7265625
train loss:  0.5296890139579773
train gradient:  0.1695165894095398
iteration : 7071
train acc:  0.6796875
train loss:  0.5629171133041382
train gradient:  0.168875856686657
iteration : 7072
train acc:  0.75
train loss:  0.44340404868125916
train gradient:  0.12604326543429156
iteration : 7073
train acc:  0.6953125
train loss:  0.564214825630188
train gradient:  0.19431555725689964
iteration : 7074
train acc:  0.75
train loss:  0.45395225286483765
train gradient:  0.1153995518116624
iteration : 7075
train acc:  0.765625
train loss:  0.5012679696083069
train gradient:  0.12970439962111827
iteration : 7076
train acc:  0.7890625
train loss:  0.4978870451450348
train gradient:  0.14877288879908973
iteration : 7077
train acc:  0.703125
train loss:  0.503257155418396
train gradient:  0.12934522682568383
iteration : 7078
train acc:  0.7109375
train loss:  0.5615322589874268
train gradient:  0.13622962926904028
iteration : 7079
train acc:  0.7265625
train loss:  0.49329817295074463
train gradient:  0.130315836837143
iteration : 7080
train acc:  0.7265625
train loss:  0.49820268154144287
train gradient:  0.16249379889361837
iteration : 7081
train acc:  0.6875
train loss:  0.6064086556434631
train gradient:  0.19033130068702056
iteration : 7082
train acc:  0.8046875
train loss:  0.48376643657684326
train gradient:  0.1082629101339131
iteration : 7083
train acc:  0.7109375
train loss:  0.520964503288269
train gradient:  0.12028426421555338
iteration : 7084
train acc:  0.765625
train loss:  0.4829834997653961
train gradient:  0.1344352759194871
iteration : 7085
train acc:  0.71875
train loss:  0.5308066010475159
train gradient:  0.15357064068796678
iteration : 7086
train acc:  0.734375
train loss:  0.5197664499282837
train gradient:  0.13667564580376612
iteration : 7087
train acc:  0.71875
train loss:  0.5159977674484253
train gradient:  0.15976974475360534
iteration : 7088
train acc:  0.7421875
train loss:  0.460466206073761
train gradient:  0.10489914123799172
iteration : 7089
train acc:  0.6953125
train loss:  0.5216906070709229
train gradient:  0.17277694902843244
iteration : 7090
train acc:  0.7109375
train loss:  0.5322703123092651
train gradient:  0.14383497267665601
iteration : 7091
train acc:  0.75
train loss:  0.4954792261123657
train gradient:  0.11283039677227173
iteration : 7092
train acc:  0.7890625
train loss:  0.4270187020301819
train gradient:  0.10480314583627776
iteration : 7093
train acc:  0.7734375
train loss:  0.46677395701408386
train gradient:  0.1262099605769268
iteration : 7094
train acc:  0.75
train loss:  0.4882020354270935
train gradient:  0.12396511331810138
iteration : 7095
train acc:  0.71875
train loss:  0.5056692361831665
train gradient:  0.1162456730528048
iteration : 7096
train acc:  0.7421875
train loss:  0.46340566873550415
train gradient:  0.09409491584254334
iteration : 7097
train acc:  0.8046875
train loss:  0.4409371018409729
train gradient:  0.16884961957128103
iteration : 7098
train acc:  0.6796875
train loss:  0.5251238942146301
train gradient:  0.16730621424298658
iteration : 7099
train acc:  0.7421875
train loss:  0.519385576248169
train gradient:  0.15564071800761314
iteration : 7100
train acc:  0.78125
train loss:  0.4993634819984436
train gradient:  0.11549398928371081
iteration : 7101
train acc:  0.7265625
train loss:  0.512846052646637
train gradient:  0.1403282319291078
iteration : 7102
train acc:  0.6953125
train loss:  0.5280787944793701
train gradient:  0.1322558897189029
iteration : 7103
train acc:  0.640625
train loss:  0.5594152212142944
train gradient:  0.16837042397637975
iteration : 7104
train acc:  0.6953125
train loss:  0.563888430595398
train gradient:  0.200762687928086
iteration : 7105
train acc:  0.7265625
train loss:  0.5562067031860352
train gradient:  0.13973140929259578
iteration : 7106
train acc:  0.796875
train loss:  0.46716997027397156
train gradient:  0.10709842541045932
iteration : 7107
train acc:  0.7421875
train loss:  0.48887908458709717
train gradient:  0.14312882257172746
iteration : 7108
train acc:  0.7890625
train loss:  0.45828238129615784
train gradient:  0.12897383983898628
iteration : 7109
train acc:  0.7421875
train loss:  0.500318169593811
train gradient:  0.14581843605275224
iteration : 7110
train acc:  0.71875
train loss:  0.5252463817596436
train gradient:  0.15811859551392843
iteration : 7111
train acc:  0.7109375
train loss:  0.5758790969848633
train gradient:  0.16251423705193152
iteration : 7112
train acc:  0.7421875
train loss:  0.4772607684135437
train gradient:  0.15002404123532548
iteration : 7113
train acc:  0.734375
train loss:  0.5216514468193054
train gradient:  0.1333151091872955
iteration : 7114
train acc:  0.734375
train loss:  0.4898410141468048
train gradient:  0.12604133677180107
iteration : 7115
train acc:  0.65625
train loss:  0.5609611868858337
train gradient:  0.14564477508741658
iteration : 7116
train acc:  0.71875
train loss:  0.5310816168785095
train gradient:  0.1489582977242452
iteration : 7117
train acc:  0.765625
train loss:  0.4922550916671753
train gradient:  0.11482480701675855
iteration : 7118
train acc:  0.75
train loss:  0.4932871460914612
train gradient:  0.12002612435910615
iteration : 7119
train acc:  0.7265625
train loss:  0.47862187027931213
train gradient:  0.12361440721506203
iteration : 7120
train acc:  0.7578125
train loss:  0.5196157693862915
train gradient:  0.20741227836081394
iteration : 7121
train acc:  0.7265625
train loss:  0.47971105575561523
train gradient:  0.12378139898088367
iteration : 7122
train acc:  0.75
train loss:  0.4858996272087097
train gradient:  0.12360815314472057
iteration : 7123
train acc:  0.71875
train loss:  0.5647292733192444
train gradient:  0.16142925585376444
iteration : 7124
train acc:  0.7109375
train loss:  0.4970959424972534
train gradient:  0.1257881175070841
iteration : 7125
train acc:  0.75
train loss:  0.493607759475708
train gradient:  0.15124167065844457
iteration : 7126
train acc:  0.7109375
train loss:  0.4866674244403839
train gradient:  0.11143022929205908
iteration : 7127
train acc:  0.796875
train loss:  0.4590725898742676
train gradient:  0.15357996132091278
iteration : 7128
train acc:  0.7421875
train loss:  0.5693095326423645
train gradient:  0.17353321724483564
iteration : 7129
train acc:  0.7578125
train loss:  0.495674729347229
train gradient:  0.1587602836974449
iteration : 7130
train acc:  0.6796875
train loss:  0.5628206729888916
train gradient:  0.1755284414403202
iteration : 7131
train acc:  0.7421875
train loss:  0.5111860036849976
train gradient:  0.16641480673323772
iteration : 7132
train acc:  0.75
train loss:  0.5208013653755188
train gradient:  0.12463316207584056
iteration : 7133
train acc:  0.6953125
train loss:  0.5092471241950989
train gradient:  0.14053891241877053
iteration : 7134
train acc:  0.7265625
train loss:  0.5681201219558716
train gradient:  0.18902201266153656
iteration : 7135
train acc:  0.71875
train loss:  0.5258945226669312
train gradient:  0.15302590825024237
iteration : 7136
train acc:  0.7109375
train loss:  0.5264983773231506
train gradient:  0.13878099081830955
iteration : 7137
train acc:  0.7734375
train loss:  0.5009773373603821
train gradient:  0.1040788853762311
iteration : 7138
train acc:  0.765625
train loss:  0.4450090527534485
train gradient:  0.15442475566662378
iteration : 7139
train acc:  0.7890625
train loss:  0.46835416555404663
train gradient:  0.12792562315197026
iteration : 7140
train acc:  0.7578125
train loss:  0.4459041357040405
train gradient:  0.10707054692498814
iteration : 7141
train acc:  0.7265625
train loss:  0.553601086139679
train gradient:  0.15750870856484048
iteration : 7142
train acc:  0.7578125
train loss:  0.49319928884506226
train gradient:  0.12809714121173077
iteration : 7143
train acc:  0.7421875
train loss:  0.5277443528175354
train gradient:  0.1573396541546393
iteration : 7144
train acc:  0.75
train loss:  0.5433157682418823
train gradient:  0.19517278437266122
iteration : 7145
train acc:  0.796875
train loss:  0.4355635643005371
train gradient:  0.11862220013003937
iteration : 7146
train acc:  0.765625
train loss:  0.4299199879169464
train gradient:  0.09900927113673381
iteration : 7147
train acc:  0.6796875
train loss:  0.5372203588485718
train gradient:  0.1366259993672233
iteration : 7148
train acc:  0.78125
train loss:  0.48986828327178955
train gradient:  0.12318239222202326
iteration : 7149
train acc:  0.7578125
train loss:  0.5080792903900146
train gradient:  0.19582151127452663
iteration : 7150
train acc:  0.609375
train loss:  0.6386262774467468
train gradient:  0.21245689704498733
iteration : 7151
train acc:  0.7890625
train loss:  0.4343258738517761
train gradient:  0.09103840761297127
iteration : 7152
train acc:  0.75
train loss:  0.5075230002403259
train gradient:  0.11890534024704266
iteration : 7153
train acc:  0.765625
train loss:  0.4866105616092682
train gradient:  0.1237446354298466
iteration : 7154
train acc:  0.640625
train loss:  0.6058200597763062
train gradient:  0.18151205460326175
iteration : 7155
train acc:  0.7578125
train loss:  0.4896436333656311
train gradient:  0.13296295831169902
iteration : 7156
train acc:  0.765625
train loss:  0.5199467539787292
train gradient:  0.14549920299605168
iteration : 7157
train acc:  0.6875
train loss:  0.6013498902320862
train gradient:  0.2042793151631508
iteration : 7158
train acc:  0.6953125
train loss:  0.5275471210479736
train gradient:  0.1579759275607318
iteration : 7159
train acc:  0.7890625
train loss:  0.5095622539520264
train gradient:  0.1735464985084665
iteration : 7160
train acc:  0.671875
train loss:  0.5301527976989746
train gradient:  0.16518368610384904
iteration : 7161
train acc:  0.671875
train loss:  0.6296603083610535
train gradient:  0.24414676386339718
iteration : 7162
train acc:  0.7421875
train loss:  0.49023717641830444
train gradient:  0.12837295885807304
iteration : 7163
train acc:  0.7265625
train loss:  0.5143204927444458
train gradient:  0.14633440862125718
iteration : 7164
train acc:  0.734375
train loss:  0.4684257507324219
train gradient:  0.10060925741386874
iteration : 7165
train acc:  0.78125
train loss:  0.5074604749679565
train gradient:  0.13792028733570194
iteration : 7166
train acc:  0.7265625
train loss:  0.5054885149002075
train gradient:  0.1409894672206257
iteration : 7167
train acc:  0.734375
train loss:  0.49911147356033325
train gradient:  0.10978819751369367
iteration : 7168
train acc:  0.71875
train loss:  0.5101978182792664
train gradient:  0.12681860766166123
iteration : 7169
train acc:  0.71875
train loss:  0.46523892879486084
train gradient:  0.11144458474938455
iteration : 7170
train acc:  0.671875
train loss:  0.5790045261383057
train gradient:  0.20064703489156227
iteration : 7171
train acc:  0.7734375
train loss:  0.49761897325515747
train gradient:  0.1502067906309657
iteration : 7172
train acc:  0.7265625
train loss:  0.5112667083740234
train gradient:  0.15446970356739567
iteration : 7173
train acc:  0.7421875
train loss:  0.4860154390335083
train gradient:  0.12690583268656966
iteration : 7174
train acc:  0.734375
train loss:  0.48686355352401733
train gradient:  0.12377528113164289
iteration : 7175
train acc:  0.7421875
train loss:  0.4692348837852478
train gradient:  0.12170826269880002
iteration : 7176
train acc:  0.7578125
train loss:  0.4861508011817932
train gradient:  0.1171993201940313
iteration : 7177
train acc:  0.7109375
train loss:  0.5292087197303772
train gradient:  0.12342842205351427
iteration : 7178
train acc:  0.703125
train loss:  0.5047242641448975
train gradient:  0.1527954113232024
iteration : 7179
train acc:  0.8046875
train loss:  0.4114339351654053
train gradient:  0.11037456837066428
iteration : 7180
train acc:  0.734375
train loss:  0.5142971277236938
train gradient:  0.1888877983956553
iteration : 7181
train acc:  0.703125
train loss:  0.5088489055633545
train gradient:  0.11114202006503857
iteration : 7182
train acc:  0.78125
train loss:  0.47825756669044495
train gradient:  0.11429362883913544
iteration : 7183
train acc:  0.828125
train loss:  0.4124699532985687
train gradient:  0.09672703581420365
iteration : 7184
train acc:  0.7421875
train loss:  0.49323561787605286
train gradient:  0.1425170254284414
iteration : 7185
train acc:  0.703125
train loss:  0.5191615223884583
train gradient:  0.14385390593441558
iteration : 7186
train acc:  0.7109375
train loss:  0.4965355396270752
train gradient:  0.14662693055651474
iteration : 7187
train acc:  0.8203125
train loss:  0.44887351989746094
train gradient:  0.10488383394549154
iteration : 7188
train acc:  0.7890625
train loss:  0.43960461020469666
train gradient:  0.10702832579416177
iteration : 7189
train acc:  0.703125
train loss:  0.5346362590789795
train gradient:  0.17002108950054376
iteration : 7190
train acc:  0.7578125
train loss:  0.5416315793991089
train gradient:  0.15196838843324317
iteration : 7191
train acc:  0.7578125
train loss:  0.4939628839492798
train gradient:  0.12635898575980747
iteration : 7192
train acc:  0.6328125
train loss:  0.6108519434928894
train gradient:  0.1787490585295269
iteration : 7193
train acc:  0.734375
train loss:  0.532128095626831
train gradient:  0.15849994715889432
iteration : 7194
train acc:  0.71875
train loss:  0.5305781364440918
train gradient:  0.13367670358672953
iteration : 7195
train acc:  0.6953125
train loss:  0.5320405960083008
train gradient:  0.1511156935454
iteration : 7196
train acc:  0.7421875
train loss:  0.48510587215423584
train gradient:  0.10633087188823234
iteration : 7197
train acc:  0.7421875
train loss:  0.5330532789230347
train gradient:  0.16838294983672314
iteration : 7198
train acc:  0.765625
train loss:  0.5061917901039124
train gradient:  0.14531816145091395
iteration : 7199
train acc:  0.7421875
train loss:  0.5777318477630615
train gradient:  0.15007169132677822
iteration : 7200
train acc:  0.71875
train loss:  0.5328909754753113
train gradient:  0.13384346609954592
iteration : 7201
train acc:  0.7109375
train loss:  0.5135912895202637
train gradient:  0.16716321545628765
iteration : 7202
train acc:  0.71875
train loss:  0.4802315831184387
train gradient:  0.11689717577877559
iteration : 7203
train acc:  0.7265625
train loss:  0.5290377140045166
train gradient:  0.1283414645906782
iteration : 7204
train acc:  0.765625
train loss:  0.4619509279727936
train gradient:  0.12778226756287753
iteration : 7205
train acc:  0.734375
train loss:  0.5083259344100952
train gradient:  0.13670182355008953
iteration : 7206
train acc:  0.7578125
train loss:  0.443482905626297
train gradient:  0.10852388763120645
iteration : 7207
train acc:  0.6953125
train loss:  0.51811283826828
train gradient:  0.15323813465261582
iteration : 7208
train acc:  0.7421875
train loss:  0.4883226454257965
train gradient:  0.1461508339273707
iteration : 7209
train acc:  0.765625
train loss:  0.49993816018104553
train gradient:  0.14082439658220208
iteration : 7210
train acc:  0.7265625
train loss:  0.5257434248924255
train gradient:  0.130394948366858
iteration : 7211
train acc:  0.6796875
train loss:  0.571179986000061
train gradient:  0.18195143227132538
iteration : 7212
train acc:  0.7265625
train loss:  0.5042295455932617
train gradient:  0.1841253967214591
iteration : 7213
train acc:  0.6640625
train loss:  0.5846730470657349
train gradient:  0.177103019792722
iteration : 7214
train acc:  0.765625
train loss:  0.5116844177246094
train gradient:  0.11308430832665638
iteration : 7215
train acc:  0.71875
train loss:  0.4887275695800781
train gradient:  0.15321898338049855
iteration : 7216
train acc:  0.6875
train loss:  0.5550053119659424
train gradient:  0.14046640974675112
iteration : 7217
train acc:  0.78125
train loss:  0.498080313205719
train gradient:  0.1490242523502561
iteration : 7218
train acc:  0.75
train loss:  0.45924457907676697
train gradient:  0.10337515161920084
iteration : 7219
train acc:  0.7421875
train loss:  0.46682173013687134
train gradient:  0.10533407561345097
iteration : 7220
train acc:  0.8046875
train loss:  0.43527525663375854
train gradient:  0.10266201336286254
iteration : 7221
train acc:  0.78125
train loss:  0.5032402873039246
train gradient:  0.14981107185262346
iteration : 7222
train acc:  0.75
train loss:  0.4952198266983032
train gradient:  0.14899332493755305
iteration : 7223
train acc:  0.7890625
train loss:  0.4364135265350342
train gradient:  0.129567379455309
iteration : 7224
train acc:  0.78125
train loss:  0.4425591230392456
train gradient:  0.09745112652374222
iteration : 7225
train acc:  0.78125
train loss:  0.4873165488243103
train gradient:  0.11678965998800127
iteration : 7226
train acc:  0.7734375
train loss:  0.438577264547348
train gradient:  0.11885323930973866
iteration : 7227
train acc:  0.734375
train loss:  0.467011958360672
train gradient:  0.13517799193966518
iteration : 7228
train acc:  0.7421875
train loss:  0.5054060816764832
train gradient:  0.1200604475285194
iteration : 7229
train acc:  0.7734375
train loss:  0.4762319326400757
train gradient:  0.1377178820485255
iteration : 7230
train acc:  0.703125
train loss:  0.5026034712791443
train gradient:  0.1698092820775493
iteration : 7231
train acc:  0.7421875
train loss:  0.5203056335449219
train gradient:  0.2304284599520503
iteration : 7232
train acc:  0.7890625
train loss:  0.4744639992713928
train gradient:  0.1191036438215898
iteration : 7233
train acc:  0.7890625
train loss:  0.4572409391403198
train gradient:  0.09702174878848832
iteration : 7234
train acc:  0.7265625
train loss:  0.5749763250350952
train gradient:  0.16996912377106274
iteration : 7235
train acc:  0.8125
train loss:  0.4470886290073395
train gradient:  0.10348216108619501
iteration : 7236
train acc:  0.7734375
train loss:  0.4540630877017975
train gradient:  0.11635614986555103
iteration : 7237
train acc:  0.75
train loss:  0.46984195709228516
train gradient:  0.10845348699344634
iteration : 7238
train acc:  0.7109375
train loss:  0.5404084920883179
train gradient:  0.16355174866851602
iteration : 7239
train acc:  0.7578125
train loss:  0.4898563623428345
train gradient:  0.13502696832847183
iteration : 7240
train acc:  0.75
train loss:  0.4952203929424286
train gradient:  0.13948266567806458
iteration : 7241
train acc:  0.765625
train loss:  0.4818245470523834
train gradient:  0.1174111917948097
iteration : 7242
train acc:  0.6875
train loss:  0.5175275802612305
train gradient:  0.1317899835316812
iteration : 7243
train acc:  0.7265625
train loss:  0.537355899810791
train gradient:  0.11811806584797412
iteration : 7244
train acc:  0.71875
train loss:  0.5716174840927124
train gradient:  0.1616045794467505
iteration : 7245
train acc:  0.6328125
train loss:  0.6243350505828857
train gradient:  0.21941124195718878
iteration : 7246
train acc:  0.6875
train loss:  0.5170966386795044
train gradient:  0.13795666846448845
iteration : 7247
train acc:  0.75
train loss:  0.4933096766471863
train gradient:  0.14273868078524948
iteration : 7248
train acc:  0.71875
train loss:  0.5032139420509338
train gradient:  0.12676053419726996
iteration : 7249
train acc:  0.7890625
train loss:  0.4446745812892914
train gradient:  0.12287784254009718
iteration : 7250
train acc:  0.765625
train loss:  0.46857789158821106
train gradient:  0.13262648051844333
iteration : 7251
train acc:  0.765625
train loss:  0.5006283521652222
train gradient:  0.1258230448709586
iteration : 7252
train acc:  0.6640625
train loss:  0.6106483340263367
train gradient:  0.24936883463967682
iteration : 7253
train acc:  0.7265625
train loss:  0.5008373260498047
train gradient:  0.15664582044060796
iteration : 7254
train acc:  0.734375
train loss:  0.5231794118881226
train gradient:  0.16258447318712949
iteration : 7255
train acc:  0.765625
train loss:  0.4702119529247284
train gradient:  0.11474356237577535
iteration : 7256
train acc:  0.6796875
train loss:  0.5671830773353577
train gradient:  0.16864719959026842
iteration : 7257
train acc:  0.765625
train loss:  0.499300092458725
train gradient:  0.1503778951140529
iteration : 7258
train acc:  0.7421875
train loss:  0.49579760432243347
train gradient:  0.09874365754653626
iteration : 7259
train acc:  0.65625
train loss:  0.5728681087493896
train gradient:  0.19678762236937092
iteration : 7260
train acc:  0.7578125
train loss:  0.49941131472587585
train gradient:  0.12175740823414737
iteration : 7261
train acc:  0.6953125
train loss:  0.5299174189567566
train gradient:  0.11811997466004961
iteration : 7262
train acc:  0.75
train loss:  0.5460970997810364
train gradient:  0.1605492518121996
iteration : 7263
train acc:  0.703125
train loss:  0.5174996852874756
train gradient:  0.16288407643228303
iteration : 7264
train acc:  0.765625
train loss:  0.4518960416316986
train gradient:  0.14187058233124883
iteration : 7265
train acc:  0.734375
train loss:  0.5005389451980591
train gradient:  0.1885857868659923
iteration : 7266
train acc:  0.7109375
train loss:  0.4932898283004761
train gradient:  0.14096659602647627
iteration : 7267
train acc:  0.6953125
train loss:  0.5395171046257019
train gradient:  0.1685758162878923
iteration : 7268
train acc:  0.6875
train loss:  0.5435409545898438
train gradient:  0.1490069746802441
iteration : 7269
train acc:  0.6953125
train loss:  0.5463799834251404
train gradient:  0.15982730295066855
iteration : 7270
train acc:  0.7734375
train loss:  0.5012412071228027
train gradient:  0.15250163700168384
iteration : 7271
train acc:  0.75
train loss:  0.4647125005722046
train gradient:  0.11989091470543194
iteration : 7272
train acc:  0.7578125
train loss:  0.475436270236969
train gradient:  0.12664875905791625
iteration : 7273
train acc:  0.703125
train loss:  0.5007321834564209
train gradient:  0.14063425598100526
iteration : 7274
train acc:  0.7890625
train loss:  0.5109273195266724
train gradient:  0.16862243041460545
iteration : 7275
train acc:  0.78125
train loss:  0.5102620720863342
train gradient:  0.11131036842435982
iteration : 7276
train acc:  0.7890625
train loss:  0.5007623434066772
train gradient:  0.1403667239244153
iteration : 7277
train acc:  0.765625
train loss:  0.4887601137161255
train gradient:  0.151597397623621
iteration : 7278
train acc:  0.7421875
train loss:  0.47523200511932373
train gradient:  0.11029232513428071
iteration : 7279
train acc:  0.75
train loss:  0.48636770248413086
train gradient:  0.1329027170948588
iteration : 7280
train acc:  0.8125
train loss:  0.4456940293312073
train gradient:  0.13541982087363347
iteration : 7281
train acc:  0.8046875
train loss:  0.4517606496810913
train gradient:  0.11429108656220612
iteration : 7282
train acc:  0.71875
train loss:  0.5135997533798218
train gradient:  0.14513046052409048
iteration : 7283
train acc:  0.6875
train loss:  0.5592774152755737
train gradient:  0.13902448043766266
iteration : 7284
train acc:  0.7265625
train loss:  0.5198438167572021
train gradient:  0.17506833163354524
iteration : 7285
train acc:  0.734375
train loss:  0.5399514436721802
train gradient:  0.1670266794761412
iteration : 7286
train acc:  0.75
train loss:  0.5179069638252258
train gradient:  0.13599940215758377
iteration : 7287
train acc:  0.71875
train loss:  0.49323904514312744
train gradient:  0.12006780915852368
iteration : 7288
train acc:  0.734375
train loss:  0.4803124666213989
train gradient:  0.13522827654761116
iteration : 7289
train acc:  0.7578125
train loss:  0.4603947401046753
train gradient:  0.11355085018449546
iteration : 7290
train acc:  0.71875
train loss:  0.5226163864135742
train gradient:  0.13525556375030778
iteration : 7291
train acc:  0.7421875
train loss:  0.44772446155548096
train gradient:  0.08782626842413523
iteration : 7292
train acc:  0.7109375
train loss:  0.4959222674369812
train gradient:  0.16543052608338416
iteration : 7293
train acc:  0.734375
train loss:  0.47778743505477905
train gradient:  0.13017240424739865
iteration : 7294
train acc:  0.7890625
train loss:  0.4115637242794037
train gradient:  0.09818859518281443
iteration : 7295
train acc:  0.78125
train loss:  0.4399296045303345
train gradient:  0.1275927379142563
iteration : 7296
train acc:  0.7890625
train loss:  0.4778757095336914
train gradient:  0.14806213059106435
iteration : 7297
train acc:  0.8046875
train loss:  0.4296056032180786
train gradient:  0.1252312256157447
iteration : 7298
train acc:  0.6796875
train loss:  0.6213808059692383
train gradient:  0.202113631713986
iteration : 7299
train acc:  0.7265625
train loss:  0.4722382426261902
train gradient:  0.18254679492976472
iteration : 7300
train acc:  0.6796875
train loss:  0.5480677485466003
train gradient:  0.14590560861636723
iteration : 7301
train acc:  0.7265625
train loss:  0.5136507749557495
train gradient:  0.1286377095030809
iteration : 7302
train acc:  0.703125
train loss:  0.5583342909812927
train gradient:  0.17760294449709846
iteration : 7303
train acc:  0.7109375
train loss:  0.5224907398223877
train gradient:  0.12193463587767127
iteration : 7304
train acc:  0.6796875
train loss:  0.516960084438324
train gradient:  0.1297978018596607
iteration : 7305
train acc:  0.7734375
train loss:  0.496738076210022
train gradient:  0.15456018909565944
iteration : 7306
train acc:  0.703125
train loss:  0.5600938200950623
train gradient:  0.18158607028331558
iteration : 7307
train acc:  0.78125
train loss:  0.44377508759498596
train gradient:  0.1344002050042588
iteration : 7308
train acc:  0.6953125
train loss:  0.5634219646453857
train gradient:  0.21558386100394972
iteration : 7309
train acc:  0.78125
train loss:  0.4829297363758087
train gradient:  0.13506122354638617
iteration : 7310
train acc:  0.7421875
train loss:  0.4806855022907257
train gradient:  0.11589039377166213
iteration : 7311
train acc:  0.703125
train loss:  0.5487269759178162
train gradient:  0.18305696762429924
iteration : 7312
train acc:  0.7265625
train loss:  0.49628663063049316
train gradient:  0.13944846960901736
iteration : 7313
train acc:  0.671875
train loss:  0.5530861020088196
train gradient:  0.12932108820357957
iteration : 7314
train acc:  0.78125
train loss:  0.4666398763656616
train gradient:  0.12058091871288053
iteration : 7315
train acc:  0.671875
train loss:  0.5511164665222168
train gradient:  0.14601880560318425
iteration : 7316
train acc:  0.7734375
train loss:  0.433090478181839
train gradient:  0.09273276026077552
iteration : 7317
train acc:  0.765625
train loss:  0.5225284695625305
train gradient:  0.14006069982922684
iteration : 7318
train acc:  0.734375
train loss:  0.505475640296936
train gradient:  0.1560766052228934
iteration : 7319
train acc:  0.796875
train loss:  0.4748581647872925
train gradient:  0.1664870060463248
iteration : 7320
train acc:  0.8125
train loss:  0.43590429425239563
train gradient:  0.10816149991396891
iteration : 7321
train acc:  0.7421875
train loss:  0.48115837574005127
train gradient:  0.13310651738011642
iteration : 7322
train acc:  0.6953125
train loss:  0.5527449250221252
train gradient:  0.1308165167214434
iteration : 7323
train acc:  0.6953125
train loss:  0.5396203994750977
train gradient:  0.16345208409750997
iteration : 7324
train acc:  0.7578125
train loss:  0.5058751702308655
train gradient:  0.1644069683599187
iteration : 7325
train acc:  0.703125
train loss:  0.5475319623947144
train gradient:  0.1593178072820502
iteration : 7326
train acc:  0.75
train loss:  0.49946725368499756
train gradient:  0.12961146871336293
iteration : 7327
train acc:  0.765625
train loss:  0.48016729950904846
train gradient:  0.10714420317318551
iteration : 7328
train acc:  0.75
train loss:  0.508231520652771
train gradient:  0.172045307372656
iteration : 7329
train acc:  0.7421875
train loss:  0.5176660418510437
train gradient:  0.12724763780130577
iteration : 7330
train acc:  0.7734375
train loss:  0.427031546831131
train gradient:  0.09816751111355056
iteration : 7331
train acc:  0.7734375
train loss:  0.5260297060012817
train gradient:  0.1586378650665733
iteration : 7332
train acc:  0.7578125
train loss:  0.5052067041397095
train gradient:  0.13904199751617768
iteration : 7333
train acc:  0.703125
train loss:  0.5114787220954895
train gradient:  0.11978242270871986
iteration : 7334
train acc:  0.7109375
train loss:  0.5079280138015747
train gradient:  0.21778981268242098
iteration : 7335
train acc:  0.7890625
train loss:  0.4659722149372101
train gradient:  0.12913435308793597
iteration : 7336
train acc:  0.7421875
train loss:  0.46614205837249756
train gradient:  0.1338658518213336
iteration : 7337
train acc:  0.6640625
train loss:  0.5586633682250977
train gradient:  0.1437408039410017
iteration : 7338
train acc:  0.734375
train loss:  0.5395199656486511
train gradient:  0.1418716704697593
iteration : 7339
train acc:  0.7578125
train loss:  0.5277546644210815
train gradient:  0.15585872591439165
iteration : 7340
train acc:  0.703125
train loss:  0.5499720573425293
train gradient:  0.15389047879141848
iteration : 7341
train acc:  0.78125
train loss:  0.4561062753200531
train gradient:  0.12141126788834869
iteration : 7342
train acc:  0.6953125
train loss:  0.5454798340797424
train gradient:  0.15300961626680448
iteration : 7343
train acc:  0.71875
train loss:  0.5823225378990173
train gradient:  0.18357925688028404
iteration : 7344
train acc:  0.71875
train loss:  0.5097966194152832
train gradient:  0.1286104151516946
iteration : 7345
train acc:  0.71875
train loss:  0.49553054571151733
train gradient:  0.11185599941241993
iteration : 7346
train acc:  0.703125
train loss:  0.5185098648071289
train gradient:  0.14609509421008735
iteration : 7347
train acc:  0.7421875
train loss:  0.526299238204956
train gradient:  0.15634588844346514
iteration : 7348
train acc:  0.7578125
train loss:  0.47009119391441345
train gradient:  0.12667172949732053
iteration : 7349
train acc:  0.703125
train loss:  0.534488320350647
train gradient:  0.1499315881161304
iteration : 7350
train acc:  0.765625
train loss:  0.5213422775268555
train gradient:  0.11169585030285747
iteration : 7351
train acc:  0.6796875
train loss:  0.6171330213546753
train gradient:  0.23013131106267248
iteration : 7352
train acc:  0.671875
train loss:  0.5496674180030823
train gradient:  0.15676897561725106
iteration : 7353
train acc:  0.75
train loss:  0.46509504318237305
train gradient:  0.11402088551744208
iteration : 7354
train acc:  0.7265625
train loss:  0.5404887199401855
train gradient:  0.13188671739309965
iteration : 7355
train acc:  0.8046875
train loss:  0.4760236442089081
train gradient:  0.1318885531473919
iteration : 7356
train acc:  0.6875
train loss:  0.5317418575286865
train gradient:  0.15891081133863422
iteration : 7357
train acc:  0.8125
train loss:  0.41020113229751587
train gradient:  0.09950280423582808
iteration : 7358
train acc:  0.65625
train loss:  0.5728341341018677
train gradient:  0.16970020410173697
iteration : 7359
train acc:  0.703125
train loss:  0.5451494455337524
train gradient:  0.15096142937880508
iteration : 7360
train acc:  0.765625
train loss:  0.46958988904953003
train gradient:  0.13549723875732395
iteration : 7361
train acc:  0.71875
train loss:  0.5224975347518921
train gradient:  0.20769243059569403
iteration : 7362
train acc:  0.703125
train loss:  0.5441535711288452
train gradient:  0.1616199781678032
iteration : 7363
train acc:  0.8046875
train loss:  0.4006868004798889
train gradient:  0.10620012826203763
iteration : 7364
train acc:  0.734375
train loss:  0.4935566782951355
train gradient:  0.17534972556667622
iteration : 7365
train acc:  0.703125
train loss:  0.5395052433013916
train gradient:  0.1438184733662069
iteration : 7366
train acc:  0.7421875
train loss:  0.49687695503234863
train gradient:  0.14073202552128247
iteration : 7367
train acc:  0.7265625
train loss:  0.5511360168457031
train gradient:  0.15279916045857905
iteration : 7368
train acc:  0.796875
train loss:  0.5226131081581116
train gradient:  0.16027569091370314
iteration : 7369
train acc:  0.78125
train loss:  0.525628924369812
train gradient:  0.1727941415064351
iteration : 7370
train acc:  0.671875
train loss:  0.5205974578857422
train gradient:  0.1557751695435861
iteration : 7371
train acc:  0.7265625
train loss:  0.531555712223053
train gradient:  0.1281971925491622
iteration : 7372
train acc:  0.7109375
train loss:  0.5370681285858154
train gradient:  0.17718162221257705
iteration : 7373
train acc:  0.796875
train loss:  0.42398351430892944
train gradient:  0.10207085063229535
iteration : 7374
train acc:  0.734375
train loss:  0.5028280019760132
train gradient:  0.14750404881529017
iteration : 7375
train acc:  0.7734375
train loss:  0.5218645930290222
train gradient:  0.12478652217891831
iteration : 7376
train acc:  0.71875
train loss:  0.524457573890686
train gradient:  0.13619125995914738
iteration : 7377
train acc:  0.75
train loss:  0.49319684505462646
train gradient:  0.12525094014426058
iteration : 7378
train acc:  0.7734375
train loss:  0.4108094573020935
train gradient:  0.11042876769224362
iteration : 7379
train acc:  0.7578125
train loss:  0.5036317110061646
train gradient:  0.1782855670579595
iteration : 7380
train acc:  0.703125
train loss:  0.5274478197097778
train gradient:  0.18997638703012207
iteration : 7381
train acc:  0.7421875
train loss:  0.4816737473011017
train gradient:  0.12021515825324947
iteration : 7382
train acc:  0.6953125
train loss:  0.5131791234016418
train gradient:  0.1348817417349027
iteration : 7383
train acc:  0.7109375
train loss:  0.531512975692749
train gradient:  0.13944909928236604
iteration : 7384
train acc:  0.71875
train loss:  0.5535150170326233
train gradient:  0.18176672490780837
iteration : 7385
train acc:  0.7421875
train loss:  0.5321236848831177
train gradient:  0.15476256480527106
iteration : 7386
train acc:  0.75
train loss:  0.5076817274093628
train gradient:  0.11559004417259851
iteration : 7387
train acc:  0.75
train loss:  0.4978034496307373
train gradient:  0.14612493654210504
iteration : 7388
train acc:  0.6171875
train loss:  0.6037047505378723
train gradient:  0.14996565118353566
iteration : 7389
train acc:  0.703125
train loss:  0.4778297543525696
train gradient:  0.12530577496623269
iteration : 7390
train acc:  0.765625
train loss:  0.4767647981643677
train gradient:  0.11633265883625758
iteration : 7391
train acc:  0.7265625
train loss:  0.5643205046653748
train gradient:  0.16864805496842683
iteration : 7392
train acc:  0.7578125
train loss:  0.4994884729385376
train gradient:  0.1598411008823213
iteration : 7393
train acc:  0.6953125
train loss:  0.5519108176231384
train gradient:  0.1597045722289991
iteration : 7394
train acc:  0.7109375
train loss:  0.5646966695785522
train gradient:  0.20691692277024176
iteration : 7395
train acc:  0.7265625
train loss:  0.5536997318267822
train gradient:  0.1810894520481997
iteration : 7396
train acc:  0.703125
train loss:  0.5328971147537231
train gradient:  0.13492107967858047
iteration : 7397
train acc:  0.71875
train loss:  0.5165168046951294
train gradient:  0.16800085478218135
iteration : 7398
train acc:  0.765625
train loss:  0.5077129602432251
train gradient:  0.15112978441849267
iteration : 7399
train acc:  0.796875
train loss:  0.4603370428085327
train gradient:  0.1486272397903156
iteration : 7400
train acc:  0.71875
train loss:  0.5177398324012756
train gradient:  0.12035174456796978
iteration : 7401
train acc:  0.7265625
train loss:  0.5003477931022644
train gradient:  0.1375480836869069
iteration : 7402
train acc:  0.7578125
train loss:  0.47152864933013916
train gradient:  0.09859869145712954
iteration : 7403
train acc:  0.8203125
train loss:  0.41091370582580566
train gradient:  0.10702693173599973
iteration : 7404
train acc:  0.75
train loss:  0.5104491710662842
train gradient:  0.1111647887637875
iteration : 7405
train acc:  0.703125
train loss:  0.5301653742790222
train gradient:  0.1382681525194118
iteration : 7406
train acc:  0.7265625
train loss:  0.5069414377212524
train gradient:  0.15435657788487517
iteration : 7407
train acc:  0.765625
train loss:  0.4665283262729645
train gradient:  0.12647603483450337
iteration : 7408
train acc:  0.8125
train loss:  0.4557190537452698
train gradient:  0.11758468079559573
iteration : 7409
train acc:  0.625
train loss:  0.608802318572998
train gradient:  0.18402622107682223
iteration : 7410
train acc:  0.7421875
train loss:  0.5458181500434875
train gradient:  0.1756689131152799
iteration : 7411
train acc:  0.6796875
train loss:  0.5745934844017029
train gradient:  0.1784456597729246
iteration : 7412
train acc:  0.703125
train loss:  0.503803551197052
train gradient:  0.1529641647495583
iteration : 7413
train acc:  0.7109375
train loss:  0.5259044766426086
train gradient:  0.15489663292583639
iteration : 7414
train acc:  0.71875
train loss:  0.4863426685333252
train gradient:  0.11768174647531973
iteration : 7415
train acc:  0.78125
train loss:  0.45903196930885315
train gradient:  0.1301200305754439
iteration : 7416
train acc:  0.7265625
train loss:  0.5630525350570679
train gradient:  0.16779931525394112
iteration : 7417
train acc:  0.8125
train loss:  0.4894266724586487
train gradient:  0.10872952289973573
iteration : 7418
train acc:  0.7578125
train loss:  0.5030476450920105
train gradient:  0.18226802422246569
iteration : 7419
train acc:  0.7265625
train loss:  0.5284597873687744
train gradient:  0.1417959492572651
iteration : 7420
train acc:  0.75
train loss:  0.47589412331581116
train gradient:  0.11696022363910127
iteration : 7421
train acc:  0.7734375
train loss:  0.45331084728240967
train gradient:  0.112785748044388
iteration : 7422
train acc:  0.6015625
train loss:  0.658624529838562
train gradient:  0.2557640119073088
iteration : 7423
train acc:  0.7578125
train loss:  0.46420609951019287
train gradient:  0.09688926846045551
iteration : 7424
train acc:  0.7109375
train loss:  0.5217992663383484
train gradient:  0.15335867035112088
iteration : 7425
train acc:  0.765625
train loss:  0.47004836797714233
train gradient:  0.10920911835987211
iteration : 7426
train acc:  0.765625
train loss:  0.5369277000427246
train gradient:  0.1654243481286638
iteration : 7427
train acc:  0.6796875
train loss:  0.5618541240692139
train gradient:  0.1720267340742755
iteration : 7428
train acc:  0.671875
train loss:  0.5463277697563171
train gradient:  0.11852074685459707
iteration : 7429
train acc:  0.703125
train loss:  0.5348083972930908
train gradient:  0.1731995497825194
iteration : 7430
train acc:  0.7265625
train loss:  0.4998414218425751
train gradient:  0.1237346074370811
iteration : 7431
train acc:  0.796875
train loss:  0.47095274925231934
train gradient:  0.10954521022890093
iteration : 7432
train acc:  0.7734375
train loss:  0.4720480442047119
train gradient:  0.1231428074730516
iteration : 7433
train acc:  0.6875
train loss:  0.5675849318504333
train gradient:  0.14819094951340714
iteration : 7434
train acc:  0.7265625
train loss:  0.5377204418182373
train gradient:  0.1489126210769946
iteration : 7435
train acc:  0.734375
train loss:  0.4893819987773895
train gradient:  0.13028550071691214
iteration : 7436
train acc:  0.75
train loss:  0.4626994729042053
train gradient:  0.1132889120326304
iteration : 7437
train acc:  0.7734375
train loss:  0.48352450132369995
train gradient:  0.12828183229706197
iteration : 7438
train acc:  0.7265625
train loss:  0.5160512924194336
train gradient:  0.14111276002044817
iteration : 7439
train acc:  0.7265625
train loss:  0.5012858510017395
train gradient:  0.10699935133880188
iteration : 7440
train acc:  0.734375
train loss:  0.5096154808998108
train gradient:  0.1308768418085578
iteration : 7441
train acc:  0.6953125
train loss:  0.5333716869354248
train gradient:  0.13637185959675144
iteration : 7442
train acc:  0.703125
train loss:  0.5583677291870117
train gradient:  0.16405103852128528
iteration : 7443
train acc:  0.8125
train loss:  0.4647849202156067
train gradient:  0.11842555092370168
iteration : 7444
train acc:  0.75
train loss:  0.4783064126968384
train gradient:  0.13397782643982717
iteration : 7445
train acc:  0.765625
train loss:  0.4613600969314575
train gradient:  0.10029009389310065
iteration : 7446
train acc:  0.7265625
train loss:  0.512225866317749
train gradient:  0.161224175041672
iteration : 7447
train acc:  0.7421875
train loss:  0.5129848122596741
train gradient:  0.14274885928339676
iteration : 7448
train acc:  0.78125
train loss:  0.442037969827652
train gradient:  0.10988822082798726
iteration : 7449
train acc:  0.65625
train loss:  0.5190797448158264
train gradient:  0.13973154803171112
iteration : 7450
train acc:  0.78125
train loss:  0.4451538622379303
train gradient:  0.10907023355444945
iteration : 7451
train acc:  0.6953125
train loss:  0.5339653491973877
train gradient:  0.14014582732457004
iteration : 7452
train acc:  0.796875
train loss:  0.45072224736213684
train gradient:  0.10151230196816263
iteration : 7453
train acc:  0.7265625
train loss:  0.4889300465583801
train gradient:  0.13146312676312247
iteration : 7454
train acc:  0.78125
train loss:  0.4745749831199646
train gradient:  0.11347771120696032
iteration : 7455
train acc:  0.734375
train loss:  0.5256640911102295
train gradient:  0.159910142365731
iteration : 7456
train acc:  0.703125
train loss:  0.5514450073242188
train gradient:  0.17687884674402962
iteration : 7457
train acc:  0.6953125
train loss:  0.5705065727233887
train gradient:  0.22420893201545383
iteration : 7458
train acc:  0.703125
train loss:  0.5423091053962708
train gradient:  0.148829660257351
iteration : 7459
train acc:  0.71875
train loss:  0.5532090067863464
train gradient:  0.17869850389645642
iteration : 7460
train acc:  0.7265625
train loss:  0.5323975086212158
train gradient:  0.14078304003771813
iteration : 7461
train acc:  0.6640625
train loss:  0.55729740858078
train gradient:  0.19987854925412296
iteration : 7462
train acc:  0.796875
train loss:  0.42560428380966187
train gradient:  0.11398639250697477
iteration : 7463
train acc:  0.7265625
train loss:  0.4942750632762909
train gradient:  0.13620483959208457
iteration : 7464
train acc:  0.78125
train loss:  0.5769318342208862
train gradient:  0.22562154017270875
iteration : 7465
train acc:  0.75
train loss:  0.5263107419013977
train gradient:  0.1588930837159373
iteration : 7466
train acc:  0.703125
train loss:  0.5365935564041138
train gradient:  0.18547763052890226
iteration : 7467
train acc:  0.6875
train loss:  0.5472863912582397
train gradient:  0.18123393032573626
iteration : 7468
train acc:  0.6875
train loss:  0.6108489036560059
train gradient:  0.19254118396227943
iteration : 7469
train acc:  0.71875
train loss:  0.5118248462677002
train gradient:  0.14638937698902432
iteration : 7470
train acc:  0.7421875
train loss:  0.5291362404823303
train gradient:  0.16942018751076
iteration : 7471
train acc:  0.78125
train loss:  0.44592469930648804
train gradient:  0.1292147812560491
iteration : 7472
train acc:  0.734375
train loss:  0.4979397654533386
train gradient:  0.13018779567764593
iteration : 7473
train acc:  0.71875
train loss:  0.5400744080543518
train gradient:  0.17279175430119315
iteration : 7474
train acc:  0.7578125
train loss:  0.4617442786693573
train gradient:  0.11820885695436116
iteration : 7475
train acc:  0.6796875
train loss:  0.5432597398757935
train gradient:  0.13522064953861337
iteration : 7476
train acc:  0.7109375
train loss:  0.573123574256897
train gradient:  0.24233330141874015
iteration : 7477
train acc:  0.75
train loss:  0.5229241847991943
train gradient:  0.17070325351556237
iteration : 7478
train acc:  0.7890625
train loss:  0.4599485993385315
train gradient:  0.1351344711931839
iteration : 7479
train acc:  0.6484375
train loss:  0.5983823537826538
train gradient:  0.20911692252545588
iteration : 7480
train acc:  0.7265625
train loss:  0.4774388074874878
train gradient:  0.13466482536300778
iteration : 7481
train acc:  0.6953125
train loss:  0.541698157787323
train gradient:  0.1775925086360451
iteration : 7482
train acc:  0.6953125
train loss:  0.5903213024139404
train gradient:  0.22950941442963962
iteration : 7483
train acc:  0.7578125
train loss:  0.501060962677002
train gradient:  0.15029487052813473
iteration : 7484
train acc:  0.734375
train loss:  0.4855785071849823
train gradient:  0.10605562978803999
iteration : 7485
train acc:  0.734375
train loss:  0.4891658425331116
train gradient:  0.11394275445280638
iteration : 7486
train acc:  0.7421875
train loss:  0.4855802059173584
train gradient:  0.11929396849170444
iteration : 7487
train acc:  0.7421875
train loss:  0.5141947269439697
train gradient:  0.13452127667610653
iteration : 7488
train acc:  0.78125
train loss:  0.4643058180809021
train gradient:  0.12831138043968016
iteration : 7489
train acc:  0.703125
train loss:  0.48407208919525146
train gradient:  0.11418432405275586
iteration : 7490
train acc:  0.734375
train loss:  0.4847651422023773
train gradient:  0.1616829584415282
iteration : 7491
train acc:  0.75
train loss:  0.4734090566635132
train gradient:  0.15075165661427845
iteration : 7492
train acc:  0.703125
train loss:  0.5415960550308228
train gradient:  0.17991446285559387
iteration : 7493
train acc:  0.75
train loss:  0.5116866827011108
train gradient:  0.18618132050115432
iteration : 7494
train acc:  0.671875
train loss:  0.5281668305397034
train gradient:  0.14813874476569588
iteration : 7495
train acc:  0.7578125
train loss:  0.5185585021972656
train gradient:  0.11779729472726592
iteration : 7496
train acc:  0.71875
train loss:  0.5711276531219482
train gradient:  0.14660532661386355
iteration : 7497
train acc:  0.71875
train loss:  0.5372017621994019
train gradient:  0.14582803077977757
iteration : 7498
train acc:  0.75
train loss:  0.5716659426689148
train gradient:  0.17842611231363809
iteration : 7499
train acc:  0.75
train loss:  0.48684412240982056
train gradient:  0.12885904172349932
iteration : 7500
train acc:  0.78125
train loss:  0.41666078567504883
train gradient:  0.09294566403207169
iteration : 7501
train acc:  0.7421875
train loss:  0.5210316777229309
train gradient:  0.13880902331992517
iteration : 7502
train acc:  0.71875
train loss:  0.46494317054748535
train gradient:  0.1411269670598149
iteration : 7503
train acc:  0.7734375
train loss:  0.493327796459198
train gradient:  0.17486567472840742
iteration : 7504
train acc:  0.75
train loss:  0.4735814332962036
train gradient:  0.12929410738594105
iteration : 7505
train acc:  0.6640625
train loss:  0.53401780128479
train gradient:  0.1770002116339977
iteration : 7506
train acc:  0.7734375
train loss:  0.4941123425960541
train gradient:  0.12134270818471253
iteration : 7507
train acc:  0.7265625
train loss:  0.4796293079853058
train gradient:  0.13453376718707516
iteration : 7508
train acc:  0.765625
train loss:  0.4623728394508362
train gradient:  0.10856335004589363
iteration : 7509
train acc:  0.75
train loss:  0.4524470865726471
train gradient:  0.12948184570181134
iteration : 7510
train acc:  0.75
train loss:  0.509017825126648
train gradient:  0.13801193764773978
iteration : 7511
train acc:  0.7890625
train loss:  0.4671531617641449
train gradient:  0.10482111031851099
iteration : 7512
train acc:  0.7421875
train loss:  0.5038050413131714
train gradient:  0.12720179924449398
iteration : 7513
train acc:  0.75
train loss:  0.4782431721687317
train gradient:  0.16113183104671466
iteration : 7514
train acc:  0.734375
train loss:  0.46278464794158936
train gradient:  0.13515188224214444
iteration : 7515
train acc:  0.75
train loss:  0.48402127623558044
train gradient:  0.13008113664410967
iteration : 7516
train acc:  0.75
train loss:  0.5103965401649475
train gradient:  0.16463191714252273
iteration : 7517
train acc:  0.65625
train loss:  0.587422251701355
train gradient:  0.20780242263689697
iteration : 7518
train acc:  0.71875
train loss:  0.5074860453605652
train gradient:  0.1771260230422959
iteration : 7519
train acc:  0.71875
train loss:  0.5490869283676147
train gradient:  0.15952587015450853
iteration : 7520
train acc:  0.7421875
train loss:  0.4863559603691101
train gradient:  0.1234857238504993
iteration : 7521
train acc:  0.703125
train loss:  0.5546892285346985
train gradient:  0.1480498813282124
iteration : 7522
train acc:  0.6953125
train loss:  0.5329142212867737
train gradient:  0.1996866138237675
iteration : 7523
train acc:  0.796875
train loss:  0.4631425142288208
train gradient:  0.15492944025021466
iteration : 7524
train acc:  0.7578125
train loss:  0.4902764558792114
train gradient:  0.14229109362130002
iteration : 7525
train acc:  0.71875
train loss:  0.5214102864265442
train gradient:  0.12942693337696165
iteration : 7526
train acc:  0.7421875
train loss:  0.4684278070926666
train gradient:  0.15423197529002303
iteration : 7527
train acc:  0.7421875
train loss:  0.45768484473228455
train gradient:  0.10803926229105958
iteration : 7528
train acc:  0.7421875
train loss:  0.5039165616035461
train gradient:  0.12412205054891175
iteration : 7529
train acc:  0.7421875
train loss:  0.5148046612739563
train gradient:  0.14317484838539052
iteration : 7530
train acc:  0.6796875
train loss:  0.6186632513999939
train gradient:  0.17393755217549728
iteration : 7531
train acc:  0.7890625
train loss:  0.473861426115036
train gradient:  0.13154614098056822
iteration : 7532
train acc:  0.734375
train loss:  0.43170690536499023
train gradient:  0.12515320964095436
iteration : 7533
train acc:  0.734375
train loss:  0.512843668460846
train gradient:  0.12783557533131257
iteration : 7534
train acc:  0.75
train loss:  0.5230227708816528
train gradient:  0.14166408957282264
iteration : 7535
train acc:  0.7109375
train loss:  0.529649019241333
train gradient:  0.21890647489179937
iteration : 7536
train acc:  0.6796875
train loss:  0.5615991353988647
train gradient:  0.16671547943456583
iteration : 7537
train acc:  0.6953125
train loss:  0.49270370602607727
train gradient:  0.1387588216499637
iteration : 7538
train acc:  0.6875
train loss:  0.5139973759651184
train gradient:  0.1017118954032716
iteration : 7539
train acc:  0.75
train loss:  0.5186450481414795
train gradient:  0.13618683906055815
iteration : 7540
train acc:  0.765625
train loss:  0.4706565737724304
train gradient:  0.13457380996773524
iteration : 7541
train acc:  0.7265625
train loss:  0.49496909976005554
train gradient:  0.15256515965058295
iteration : 7542
train acc:  0.703125
train loss:  0.5128169059753418
train gradient:  0.11053899949041975
iteration : 7543
train acc:  0.796875
train loss:  0.47883594036102295
train gradient:  0.16877094377651175
iteration : 7544
train acc:  0.71875
train loss:  0.5238837003707886
train gradient:  0.14709861018114234
iteration : 7545
train acc:  0.71875
train loss:  0.5459091663360596
train gradient:  0.15771211428323645
iteration : 7546
train acc:  0.75
train loss:  0.47943878173828125
train gradient:  0.11860921782518055
iteration : 7547
train acc:  0.671875
train loss:  0.6095870733261108
train gradient:  0.20210153863145605
iteration : 7548
train acc:  0.7109375
train loss:  0.5291939973831177
train gradient:  0.16647822797593448
iteration : 7549
train acc:  0.71875
train loss:  0.4822388291358948
train gradient:  0.1287594472944988
iteration : 7550
train acc:  0.7578125
train loss:  0.47117316722869873
train gradient:  0.1398156402731699
iteration : 7551
train acc:  0.7421875
train loss:  0.5114436149597168
train gradient:  0.1453268288611746
iteration : 7552
train acc:  0.7578125
train loss:  0.5006781816482544
train gradient:  0.12512969129384882
iteration : 7553
train acc:  0.7265625
train loss:  0.5033412575721741
train gradient:  0.1306277082799018
iteration : 7554
train acc:  0.71875
train loss:  0.5424917936325073
train gradient:  0.15625806936562175
iteration : 7555
train acc:  0.6796875
train loss:  0.5839669704437256
train gradient:  0.19470532128299695
iteration : 7556
train acc:  0.75
train loss:  0.5284762382507324
train gradient:  0.17437783932098597
iteration : 7557
train acc:  0.7890625
train loss:  0.3969305753707886
train gradient:  0.09068507021261318
iteration : 7558
train acc:  0.7109375
train loss:  0.5821423530578613
train gradient:  0.2005048291019192
iteration : 7559
train acc:  0.6640625
train loss:  0.6157342195510864
train gradient:  0.17010710069246615
iteration : 7560
train acc:  0.6875
train loss:  0.5638374090194702
train gradient:  0.14135519684748307
iteration : 7561
train acc:  0.7109375
train loss:  0.5116269588470459
train gradient:  0.1396450349228283
iteration : 7562
train acc:  0.671875
train loss:  0.5406135320663452
train gradient:  0.1395574306532963
iteration : 7563
train acc:  0.7734375
train loss:  0.4736103117465973
train gradient:  0.10868258716383335
iteration : 7564
train acc:  0.6953125
train loss:  0.5422854423522949
train gradient:  0.1802773150221843
iteration : 7565
train acc:  0.6953125
train loss:  0.5693596601486206
train gradient:  0.1901302026723405
iteration : 7566
train acc:  0.8046875
train loss:  0.44311460852622986
train gradient:  0.10098239265711507
iteration : 7567
train acc:  0.6796875
train loss:  0.5395649671554565
train gradient:  0.1839741669523161
iteration : 7568
train acc:  0.71875
train loss:  0.5299506783485413
train gradient:  0.15758203004120103
iteration : 7569
train acc:  0.703125
train loss:  0.5387214422225952
train gradient:  0.12208050580212834
iteration : 7570
train acc:  0.6640625
train loss:  0.5524582862854004
train gradient:  0.12473493066418108
iteration : 7571
train acc:  0.7421875
train loss:  0.526172935962677
train gradient:  0.16849672984603875
iteration : 7572
train acc:  0.75
train loss:  0.49643474817276
train gradient:  0.13639323668361109
iteration : 7573
train acc:  0.75
train loss:  0.45196041464805603
train gradient:  0.1297312061631833
iteration : 7574
train acc:  0.71875
train loss:  0.4860001802444458
train gradient:  0.15516191440814864
iteration : 7575
train acc:  0.7421875
train loss:  0.5326585173606873
train gradient:  0.16024884366612366
iteration : 7576
train acc:  0.78125
train loss:  0.48963606357574463
train gradient:  0.12181339163267947
iteration : 7577
train acc:  0.75
train loss:  0.5197343230247498
train gradient:  0.1380582123286549
iteration : 7578
train acc:  0.7890625
train loss:  0.4585829973220825
train gradient:  0.12825588738158045
iteration : 7579
train acc:  0.7578125
train loss:  0.5147708654403687
train gradient:  0.13735178870150722
iteration : 7580
train acc:  0.765625
train loss:  0.4816385507583618
train gradient:  0.12842769336641535
iteration : 7581
train acc:  0.7578125
train loss:  0.5120118856430054
train gradient:  0.16633003963000592
iteration : 7582
train acc:  0.7890625
train loss:  0.4790027141571045
train gradient:  0.11977557610941224
iteration : 7583
train acc:  0.734375
train loss:  0.5530540943145752
train gradient:  0.14991776151788216
iteration : 7584
train acc:  0.6875
train loss:  0.5626514554023743
train gradient:  0.15698196230454686
iteration : 7585
train acc:  0.6796875
train loss:  0.5598386526107788
train gradient:  0.1590155620123686
iteration : 7586
train acc:  0.75
train loss:  0.521257221698761
train gradient:  0.17073369936039273
iteration : 7587
train acc:  0.703125
train loss:  0.5756629109382629
train gradient:  0.17191205656416042
iteration : 7588
train acc:  0.703125
train loss:  0.5345075130462646
train gradient:  0.13466942005278276
iteration : 7589
train acc:  0.7734375
train loss:  0.49673527479171753
train gradient:  0.08354578249532696
iteration : 7590
train acc:  0.796875
train loss:  0.45174640417099
train gradient:  0.12032150002118756
iteration : 7591
train acc:  0.7734375
train loss:  0.48608845472335815
train gradient:  0.12520309574389926
iteration : 7592
train acc:  0.7890625
train loss:  0.4649530053138733
train gradient:  0.10616454298698806
iteration : 7593
train acc:  0.6796875
train loss:  0.5855017304420471
train gradient:  0.21011655569850574
iteration : 7594
train acc:  0.734375
train loss:  0.49178436398506165
train gradient:  0.10411814794425438
iteration : 7595
train acc:  0.765625
train loss:  0.5135694742202759
train gradient:  0.12748053436657472
iteration : 7596
train acc:  0.6953125
train loss:  0.5626087784767151
train gradient:  0.13379163574893105
iteration : 7597
train acc:  0.7578125
train loss:  0.468985915184021
train gradient:  0.1705574129068504
iteration : 7598
train acc:  0.6875
train loss:  0.5343164205551147
train gradient:  0.1371335232071737
iteration : 7599
train acc:  0.7109375
train loss:  0.5351892709732056
train gradient:  0.1538740792480806
iteration : 7600
train acc:  0.734375
train loss:  0.548884391784668
train gradient:  0.1620127324943187
iteration : 7601
train acc:  0.7734375
train loss:  0.4635702669620514
train gradient:  0.09632264514600511
iteration : 7602
train acc:  0.7109375
train loss:  0.5358383059501648
train gradient:  0.19955406329552608
iteration : 7603
train acc:  0.7265625
train loss:  0.5192700624465942
train gradient:  0.13697373311268574
iteration : 7604
train acc:  0.65625
train loss:  0.5955168604850769
train gradient:  0.16815545318867064
iteration : 7605
train acc:  0.734375
train loss:  0.4951937794685364
train gradient:  0.16185114240759202
iteration : 7606
train acc:  0.75
train loss:  0.4945586323738098
train gradient:  0.12047586470304056
iteration : 7607
train acc:  0.6640625
train loss:  0.5456236600875854
train gradient:  0.1401799929334817
iteration : 7608
train acc:  0.71875
train loss:  0.5196292996406555
train gradient:  0.22202992392619253
iteration : 7609
train acc:  0.7265625
train loss:  0.49450159072875977
train gradient:  0.13702258298899544
iteration : 7610
train acc:  0.7890625
train loss:  0.4764820337295532
train gradient:  0.11460459746960538
iteration : 7611
train acc:  0.671875
train loss:  0.5697868466377258
train gradient:  0.14936349717220837
iteration : 7612
train acc:  0.7734375
train loss:  0.47571074962615967
train gradient:  0.10744198272294572
iteration : 7613
train acc:  0.7734375
train loss:  0.45162686705589294
train gradient:  0.11234534005755695
iteration : 7614
train acc:  0.7578125
train loss:  0.49820926785469055
train gradient:  0.1423008284306519
iteration : 7615
train acc:  0.78125
train loss:  0.49966421723365784
train gradient:  0.10999917850963384
iteration : 7616
train acc:  0.6953125
train loss:  0.5759062170982361
train gradient:  0.1589935185602491
iteration : 7617
train acc:  0.625
train loss:  0.7069196701049805
train gradient:  0.2049442143162053
iteration : 7618
train acc:  0.7578125
train loss:  0.4444478154182434
train gradient:  0.0956649986128652
iteration : 7619
train acc:  0.6640625
train loss:  0.5295170545578003
train gradient:  0.12100331405394604
iteration : 7620
train acc:  0.7734375
train loss:  0.44630739092826843
train gradient:  0.09162532552608973
iteration : 7621
train acc:  0.703125
train loss:  0.563048779964447
train gradient:  0.16916423260978622
iteration : 7622
train acc:  0.6796875
train loss:  0.5409380197525024
train gradient:  0.13998299509632228
iteration : 7623
train acc:  0.703125
train loss:  0.5147148370742798
train gradient:  0.1244568588274982
iteration : 7624
train acc:  0.7109375
train loss:  0.493562787771225
train gradient:  0.11159835943629277
iteration : 7625
train acc:  0.734375
train loss:  0.5104043483734131
train gradient:  0.13348355965736536
iteration : 7626
train acc:  0.7578125
train loss:  0.5360046625137329
train gradient:  0.1475111000522466
iteration : 7627
train acc:  0.796875
train loss:  0.4732667803764343
train gradient:  0.1470469890770915
iteration : 7628
train acc:  0.671875
train loss:  0.5670545101165771
train gradient:  0.15980856794997028
iteration : 7629
train acc:  0.7109375
train loss:  0.5341750383377075
train gradient:  0.1650260906358899
iteration : 7630
train acc:  0.703125
train loss:  0.5325826406478882
train gradient:  0.13036318728636032
iteration : 7631
train acc:  0.71875
train loss:  0.4894806444644928
train gradient:  0.1165911225155465
iteration : 7632
train acc:  0.78125
train loss:  0.47914841771125793
train gradient:  0.12244013486703209
iteration : 7633
train acc:  0.828125
train loss:  0.45296213030815125
train gradient:  0.11729332176527657
iteration : 7634
train acc:  0.7109375
train loss:  0.5590850114822388
train gradient:  0.2135199163323468
iteration : 7635
train acc:  0.765625
train loss:  0.49358972907066345
train gradient:  0.1228042327952933
iteration : 7636
train acc:  0.796875
train loss:  0.4326680302619934
train gradient:  0.10822462667521308
iteration : 7637
train acc:  0.7109375
train loss:  0.5395724773406982
train gradient:  0.13341627729945044
iteration : 7638
train acc:  0.7421875
train loss:  0.5152187347412109
train gradient:  0.17232832591733727
iteration : 7639
train acc:  0.78125
train loss:  0.47185635566711426
train gradient:  0.12350896994646185
iteration : 7640
train acc:  0.7265625
train loss:  0.5050110220909119
train gradient:  0.13231836466114055
iteration : 7641
train acc:  0.7890625
train loss:  0.460656076669693
train gradient:  0.13630702607277578
iteration : 7642
train acc:  0.7265625
train loss:  0.5120552778244019
train gradient:  0.13863159865105412
iteration : 7643
train acc:  0.6953125
train loss:  0.5924767851829529
train gradient:  0.19088009599795053
iteration : 7644
train acc:  0.78125
train loss:  0.442302405834198
train gradient:  0.1373010284874398
iteration : 7645
train acc:  0.625
train loss:  0.5491372346878052
train gradient:  0.13693683794603828
iteration : 7646
train acc:  0.7421875
train loss:  0.5113781094551086
train gradient:  0.12787796822967223
iteration : 7647
train acc:  0.765625
train loss:  0.4816417992115021
train gradient:  0.13066798844794036
iteration : 7648
train acc:  0.765625
train loss:  0.486649751663208
train gradient:  0.11022861377952946
iteration : 7649
train acc:  0.7265625
train loss:  0.48179709911346436
train gradient:  0.1083933901601622
iteration : 7650
train acc:  0.765625
train loss:  0.4861625134944916
train gradient:  0.12224878163251425
iteration : 7651
train acc:  0.765625
train loss:  0.4420015215873718
train gradient:  0.09801009049303143
iteration : 7652
train acc:  0.75
train loss:  0.4460326135158539
train gradient:  0.11405268235247822
iteration : 7653
train acc:  0.8125
train loss:  0.46333009004592896
train gradient:  0.0977082854390621
iteration : 7654
train acc:  0.7421875
train loss:  0.4859328269958496
train gradient:  0.13645898503270126
iteration : 7655
train acc:  0.7109375
train loss:  0.566077709197998
train gradient:  0.14037409088984576
iteration : 7656
train acc:  0.7734375
train loss:  0.4762382209300995
train gradient:  0.12387813956280005
iteration : 7657
train acc:  0.7265625
train loss:  0.4985499978065491
train gradient:  0.14323206974284747
iteration : 7658
train acc:  0.7734375
train loss:  0.4760458469390869
train gradient:  0.09974011292412617
iteration : 7659
train acc:  0.6953125
train loss:  0.5509207844734192
train gradient:  0.13948253989160184
iteration : 7660
train acc:  0.7265625
train loss:  0.5338532328605652
train gradient:  0.13142353441221857
iteration : 7661
train acc:  0.7265625
train loss:  0.5553629398345947
train gradient:  0.14777461637255632
iteration : 7662
train acc:  0.765625
train loss:  0.4695725739002228
train gradient:  0.1614345409103199
iteration : 7663
train acc:  0.703125
train loss:  0.6385607123374939
train gradient:  0.3323369463365268
iteration : 7664
train acc:  0.734375
train loss:  0.5036649703979492
train gradient:  0.11000958438271091
iteration : 7665
train acc:  0.7421875
train loss:  0.529303789138794
train gradient:  0.12525579770541792
iteration : 7666
train acc:  0.8125
train loss:  0.42002350091934204
train gradient:  0.10181528665465171
iteration : 7667
train acc:  0.765625
train loss:  0.47736799716949463
train gradient:  0.12894510554325134
iteration : 7668
train acc:  0.78125
train loss:  0.4421657919883728
train gradient:  0.0943648663589268
iteration : 7669
train acc:  0.75
train loss:  0.5085937976837158
train gradient:  0.17109581016415282
iteration : 7670
train acc:  0.609375
train loss:  0.5693747997283936
train gradient:  0.19205661230607296
iteration : 7671
train acc:  0.8046875
train loss:  0.4545248746871948
train gradient:  0.10527243966810226
iteration : 7672
train acc:  0.7265625
train loss:  0.5026335716247559
train gradient:  0.1301511028098461
iteration : 7673
train acc:  0.7578125
train loss:  0.48923489451408386
train gradient:  0.11007101749394567
iteration : 7674
train acc:  0.71875
train loss:  0.5261290073394775
train gradient:  0.11605484662899754
iteration : 7675
train acc:  0.7109375
train loss:  0.4850662648677826
train gradient:  0.12434893996207529
iteration : 7676
train acc:  0.734375
train loss:  0.51105135679245
train gradient:  0.12057656300902875
iteration : 7677
train acc:  0.765625
train loss:  0.43986403942108154
train gradient:  0.11270212956937085
iteration : 7678
train acc:  0.7421875
train loss:  0.5136897563934326
train gradient:  0.13804677426535775
iteration : 7679
train acc:  0.6953125
train loss:  0.5113843679428101
train gradient:  0.1450348731622474
iteration : 7680
train acc:  0.703125
train loss:  0.4845808744430542
train gradient:  0.12722547574194104
iteration : 7681
train acc:  0.7734375
train loss:  0.47430193424224854
train gradient:  0.14968523113426688
iteration : 7682
train acc:  0.6875
train loss:  0.5241844654083252
train gradient:  0.13524097078189348
iteration : 7683
train acc:  0.765625
train loss:  0.48751503229141235
train gradient:  0.1244542025288935
iteration : 7684
train acc:  0.75
train loss:  0.48512518405914307
train gradient:  0.12847077071961827
iteration : 7685
train acc:  0.7265625
train loss:  0.4733952283859253
train gradient:  0.11576633782650497
iteration : 7686
train acc:  0.75
train loss:  0.4836217761039734
train gradient:  0.10806795516142464
iteration : 7687
train acc:  0.7578125
train loss:  0.5014991760253906
train gradient:  0.14675865366734897
iteration : 7688
train acc:  0.7109375
train loss:  0.5607171058654785
train gradient:  0.16553902316820418
iteration : 7689
train acc:  0.796875
train loss:  0.4407709240913391
train gradient:  0.11393458996503392
iteration : 7690
train acc:  0.6796875
train loss:  0.5494559407234192
train gradient:  0.13068089032045427
iteration : 7691
train acc:  0.7734375
train loss:  0.4792259931564331
train gradient:  0.16030943058911004
iteration : 7692
train acc:  0.7578125
train loss:  0.4802510142326355
train gradient:  0.12259986562087553
iteration : 7693
train acc:  0.8203125
train loss:  0.4252643585205078
train gradient:  0.12578380061228622
iteration : 7694
train acc:  0.7109375
train loss:  0.5357517004013062
train gradient:  0.12377983694537624
iteration : 7695
train acc:  0.7890625
train loss:  0.43802180886268616
train gradient:  0.11211875164859453
iteration : 7696
train acc:  0.7890625
train loss:  0.44081825017929077
train gradient:  0.09755936188511362
iteration : 7697
train acc:  0.7734375
train loss:  0.5088627338409424
train gradient:  0.14704394441899205
iteration : 7698
train acc:  0.7109375
train loss:  0.5107361078262329
train gradient:  0.1523151647252058
iteration : 7699
train acc:  0.7109375
train loss:  0.5224111676216125
train gradient:  0.14348056354554772
iteration : 7700
train acc:  0.7734375
train loss:  0.5097917318344116
train gradient:  0.18305258271276031
iteration : 7701
train acc:  0.7109375
train loss:  0.4958701729774475
train gradient:  0.13613652588480377
iteration : 7702
train acc:  0.6875
train loss:  0.5672181844711304
train gradient:  0.15553552188965625
iteration : 7703
train acc:  0.7578125
train loss:  0.47213655710220337
train gradient:  0.17308372623369378
iteration : 7704
train acc:  0.7734375
train loss:  0.429328978061676
train gradient:  0.10561910639149474
iteration : 7705
train acc:  0.734375
train loss:  0.4747956395149231
train gradient:  0.12351560286551427
iteration : 7706
train acc:  0.6953125
train loss:  0.48588770627975464
train gradient:  0.1258283112966288
iteration : 7707
train acc:  0.7421875
train loss:  0.5576101541519165
train gradient:  0.16531393105588404
iteration : 7708
train acc:  0.7109375
train loss:  0.5792827010154724
train gradient:  0.21739884244698587
iteration : 7709
train acc:  0.734375
train loss:  0.48030874133110046
train gradient:  0.1140866462263224
iteration : 7710
train acc:  0.6796875
train loss:  0.5521489381790161
train gradient:  0.23367631079609597
iteration : 7711
train acc:  0.7578125
train loss:  0.49368584156036377
train gradient:  0.1060181437813131
iteration : 7712
train acc:  0.8203125
train loss:  0.39813828468322754
train gradient:  0.10723286318871185
iteration : 7713
train acc:  0.71875
train loss:  0.5227000117301941
train gradient:  0.13009214085599016
iteration : 7714
train acc:  0.7734375
train loss:  0.48094305396080017
train gradient:  0.12500101516890016
iteration : 7715
train acc:  0.734375
train loss:  0.5346848964691162
train gradient:  0.1474082919109252
iteration : 7716
train acc:  0.7578125
train loss:  0.4769231081008911
train gradient:  0.11994491507053527
iteration : 7717
train acc:  0.7265625
train loss:  0.5116233825683594
train gradient:  0.17881947248550123
iteration : 7718
train acc:  0.7890625
train loss:  0.4721505045890808
train gradient:  0.11342241135096395
iteration : 7719
train acc:  0.65625
train loss:  0.5236248970031738
train gradient:  0.1296779516580488
iteration : 7720
train acc:  0.7265625
train loss:  0.5562023520469666
train gradient:  0.20098682939806334
iteration : 7721
train acc:  0.7421875
train loss:  0.4825223982334137
train gradient:  0.11833666378330386
iteration : 7722
train acc:  0.6484375
train loss:  0.5331619381904602
train gradient:  0.14727848629947118
iteration : 7723
train acc:  0.703125
train loss:  0.5491114258766174
train gradient:  0.1715366715985842
iteration : 7724
train acc:  0.7421875
train loss:  0.5624312162399292
train gradient:  0.1564016916740658
iteration : 7725
train acc:  0.7421875
train loss:  0.48598504066467285
train gradient:  0.11512091639452669
iteration : 7726
train acc:  0.65625
train loss:  0.5813355445861816
train gradient:  0.17050413039772272
iteration : 7727
train acc:  0.7421875
train loss:  0.5093912482261658
train gradient:  0.1296596272281792
iteration : 7728
train acc:  0.7734375
train loss:  0.49486061930656433
train gradient:  0.16679032133026261
iteration : 7729
train acc:  0.7109375
train loss:  0.5831562280654907
train gradient:  0.23858335364346456
iteration : 7730
train acc:  0.703125
train loss:  0.5093411207199097
train gradient:  0.13759669288286186
iteration : 7731
train acc:  0.7265625
train loss:  0.4948202967643738
train gradient:  0.17590273390027983
iteration : 7732
train acc:  0.796875
train loss:  0.4512004554271698
train gradient:  0.11228399649236598
iteration : 7733
train acc:  0.7578125
train loss:  0.5028611421585083
train gradient:  0.11263685443169248
iteration : 7734
train acc:  0.7265625
train loss:  0.5488495826721191
train gradient:  0.15239173118304655
iteration : 7735
train acc:  0.6875
train loss:  0.5889323949813843
train gradient:  0.14487589114138338
iteration : 7736
train acc:  0.8046875
train loss:  0.4649551212787628
train gradient:  0.10844011542774666
iteration : 7737
train acc:  0.7421875
train loss:  0.49559247493743896
train gradient:  0.1578612783988786
iteration : 7738
train acc:  0.7578125
train loss:  0.5134909749031067
train gradient:  0.13799275967256397
iteration : 7739
train acc:  0.7734375
train loss:  0.4595775604248047
train gradient:  0.1026149887883137
iteration : 7740
train acc:  0.7265625
train loss:  0.4921988844871521
train gradient:  0.11657904379724628
iteration : 7741
train acc:  0.75
train loss:  0.4934667944908142
train gradient:  0.15891188239029294
iteration : 7742
train acc:  0.6875
train loss:  0.5226809978485107
train gradient:  0.14426289444958945
iteration : 7743
train acc:  0.765625
train loss:  0.5032878518104553
train gradient:  0.14299315122611114
iteration : 7744
train acc:  0.6953125
train loss:  0.548507571220398
train gradient:  0.1629276607031624
iteration : 7745
train acc:  0.6796875
train loss:  0.5237859487533569
train gradient:  0.15311985128277517
iteration : 7746
train acc:  0.8046875
train loss:  0.43727266788482666
train gradient:  0.16301221775178054
iteration : 7747
train acc:  0.703125
train loss:  0.5088684558868408
train gradient:  0.1428091598822689
iteration : 7748
train acc:  0.734375
train loss:  0.5014972686767578
train gradient:  0.15702267213876298
iteration : 7749
train acc:  0.765625
train loss:  0.49872887134552
train gradient:  0.12573489014137892
iteration : 7750
train acc:  0.75
train loss:  0.4987064599990845
train gradient:  0.13956191542446877
iteration : 7751
train acc:  0.7890625
train loss:  0.482648640871048
train gradient:  0.12787644202301568
iteration : 7752
train acc:  0.734375
train loss:  0.5505199432373047
train gradient:  0.16034199573859836
iteration : 7753
train acc:  0.734375
train loss:  0.46870723366737366
train gradient:  0.13067090130976777
iteration : 7754
train acc:  0.75
train loss:  0.5089072585105896
train gradient:  0.13815135884720503
iteration : 7755
train acc:  0.6640625
train loss:  0.5413853526115417
train gradient:  0.16270662375161704
iteration : 7756
train acc:  0.7578125
train loss:  0.5449901819229126
train gradient:  0.1420647429436987
iteration : 7757
train acc:  0.734375
train loss:  0.5435311794281006
train gradient:  0.13565237147075235
iteration : 7758
train acc:  0.7421875
train loss:  0.5175395011901855
train gradient:  0.16378610510617853
iteration : 7759
train acc:  0.7578125
train loss:  0.48577356338500977
train gradient:  0.17920249225469081
iteration : 7760
train acc:  0.703125
train loss:  0.511652946472168
train gradient:  0.14729488333764185
iteration : 7761
train acc:  0.734375
train loss:  0.45515871047973633
train gradient:  0.1012419696178576
iteration : 7762
train acc:  0.796875
train loss:  0.4360803961753845
train gradient:  0.10171337851196137
iteration : 7763
train acc:  0.78125
train loss:  0.4594408869743347
train gradient:  0.11502377792412032
iteration : 7764
train acc:  0.71875
train loss:  0.553772509098053
train gradient:  0.1478343265324532
iteration : 7765
train acc:  0.75
train loss:  0.5040148496627808
train gradient:  0.12694172230129713
iteration : 7766
train acc:  0.78125
train loss:  0.45041707158088684
train gradient:  0.11138306544681412
iteration : 7767
train acc:  0.7421875
train loss:  0.5057147145271301
train gradient:  0.13381947239754635
iteration : 7768
train acc:  0.7421875
train loss:  0.5216594934463501
train gradient:  0.15419517277275718
iteration : 7769
train acc:  0.7578125
train loss:  0.4960116147994995
train gradient:  0.17220449664635357
iteration : 7770
train acc:  0.78125
train loss:  0.4902034401893616
train gradient:  0.12589921014331726
iteration : 7771
train acc:  0.6875
train loss:  0.546987771987915
train gradient:  0.16053156502532806
iteration : 7772
train acc:  0.71875
train loss:  0.516593337059021
train gradient:  0.1543359451679962
iteration : 7773
train acc:  0.78125
train loss:  0.4452119469642639
train gradient:  0.1222444623953568
iteration : 7774
train acc:  0.703125
train loss:  0.5736924409866333
train gradient:  0.16177185515906328
iteration : 7775
train acc:  0.7578125
train loss:  0.47147437930107117
train gradient:  0.14701153434684833
iteration : 7776
train acc:  0.71875
train loss:  0.49998271465301514
train gradient:  0.1195696483140243
iteration : 7777
train acc:  0.7421875
train loss:  0.4752790331840515
train gradient:  0.09480932452491633
iteration : 7778
train acc:  0.765625
train loss:  0.4493029713630676
train gradient:  0.1189716288967866
iteration : 7779
train acc:  0.703125
train loss:  0.5268082618713379
train gradient:  0.15494072118740548
iteration : 7780
train acc:  0.71875
train loss:  0.4805346131324768
train gradient:  0.11389397395907058
iteration : 7781
train acc:  0.703125
train loss:  0.5571229457855225
train gradient:  0.183562321108525
iteration : 7782
train acc:  0.71875
train loss:  0.48642775416374207
train gradient:  0.12775403872679947
iteration : 7783
train acc:  0.7265625
train loss:  0.46204596757888794
train gradient:  0.11863456181938727
iteration : 7784
train acc:  0.6875
train loss:  0.5415118932723999
train gradient:  0.13662039198556658
iteration : 7785
train acc:  0.7421875
train loss:  0.5053014755249023
train gradient:  0.1808166380824482
iteration : 7786
train acc:  0.7109375
train loss:  0.5114569664001465
train gradient:  0.25771145804835827
iteration : 7787
train acc:  0.7734375
train loss:  0.4971693754196167
train gradient:  0.13707187428578715
iteration : 7788
train acc:  0.7734375
train loss:  0.4905247688293457
train gradient:  0.11521605197866694
iteration : 7789
train acc:  0.6484375
train loss:  0.5708821415901184
train gradient:  0.21245401284643783
iteration : 7790
train acc:  0.71875
train loss:  0.5436409711837769
train gradient:  0.19636186744511702
iteration : 7791
train acc:  0.6796875
train loss:  0.5546677112579346
train gradient:  0.23915388913915453
iteration : 7792
train acc:  0.7265625
train loss:  0.5347968339920044
train gradient:  0.14457325794124487
iteration : 7793
train acc:  0.78125
train loss:  0.46736231446266174
train gradient:  0.1245756921320647
iteration : 7794
train acc:  0.703125
train loss:  0.5649921894073486
train gradient:  0.14612527410553344
iteration : 7795
train acc:  0.7109375
train loss:  0.4988085627555847
train gradient:  0.13401861035076965
iteration : 7796
train acc:  0.7734375
train loss:  0.523209273815155
train gradient:  0.12841783968824627
iteration : 7797
train acc:  0.640625
train loss:  0.6177980899810791
train gradient:  0.25651786550100925
iteration : 7798
train acc:  0.734375
train loss:  0.48752933740615845
train gradient:  0.11215870585228498
iteration : 7799
train acc:  0.6953125
train loss:  0.604641854763031
train gradient:  0.26711798988421126
iteration : 7800
train acc:  0.6875
train loss:  0.5633503198623657
train gradient:  0.18826440680755047
iteration : 7801
train acc:  0.75
train loss:  0.4740261733531952
train gradient:  0.09831965224915157
iteration : 7802
train acc:  0.734375
train loss:  0.5102769732475281
train gradient:  0.13461997101553752
iteration : 7803
train acc:  0.75
train loss:  0.49971574544906616
train gradient:  0.122323368144233
iteration : 7804
train acc:  0.71875
train loss:  0.5111477971076965
train gradient:  0.1265969343543303
iteration : 7805
train acc:  0.7890625
train loss:  0.47735562920570374
train gradient:  0.1280583797779352
iteration : 7806
train acc:  0.7109375
train loss:  0.5104023218154907
train gradient:  0.1633409729673087
iteration : 7807
train acc:  0.71875
train loss:  0.49752727150917053
train gradient:  0.13367635002680522
iteration : 7808
train acc:  0.765625
train loss:  0.4859185814857483
train gradient:  0.12253316513800806
iteration : 7809
train acc:  0.8046875
train loss:  0.4423714280128479
train gradient:  0.1024715190426122
iteration : 7810
train acc:  0.71875
train loss:  0.528069257736206
train gradient:  0.1582662373731259
iteration : 7811
train acc:  0.7734375
train loss:  0.44758909940719604
train gradient:  0.14573688335525126
iteration : 7812
train acc:  0.7421875
train loss:  0.4999183118343353
train gradient:  0.12642469849506932
iteration : 7813
train acc:  0.796875
train loss:  0.42437535524368286
train gradient:  0.11029708338473984
iteration : 7814
train acc:  0.6875
train loss:  0.5691457390785217
train gradient:  0.16555698626066293
iteration : 7815
train acc:  0.6875
train loss:  0.5486321449279785
train gradient:  0.15356130466615553
iteration : 7816
train acc:  0.75
train loss:  0.4462161064147949
train gradient:  0.11539569675047945
iteration : 7817
train acc:  0.78125
train loss:  0.48875701427459717
train gradient:  0.13841849858523103
iteration : 7818
train acc:  0.78125
train loss:  0.4651036262512207
train gradient:  0.10812067631596295
iteration : 7819
train acc:  0.7421875
train loss:  0.4871283173561096
train gradient:  0.10923516076489402
iteration : 7820
train acc:  0.7734375
train loss:  0.44829773902893066
train gradient:  0.12970146556916187
iteration : 7821
train acc:  0.7734375
train loss:  0.46227166056632996
train gradient:  0.16239216905117282
iteration : 7822
train acc:  0.78125
train loss:  0.4992462694644928
train gradient:  0.14006095954479444
iteration : 7823
train acc:  0.75
train loss:  0.5074217319488525
train gradient:  0.12641922328661548
iteration : 7824
train acc:  0.71875
train loss:  0.46635839343070984
train gradient:  0.11649084471505208
iteration : 7825
train acc:  0.734375
train loss:  0.5558975338935852
train gradient:  0.16182105042936268
iteration : 7826
train acc:  0.734375
train loss:  0.5189245939254761
train gradient:  0.14073978967508993
iteration : 7827
train acc:  0.78125
train loss:  0.43580162525177
train gradient:  0.12900849622666477
iteration : 7828
train acc:  0.71875
train loss:  0.536680281162262
train gradient:  0.21522999218601918
iteration : 7829
train acc:  0.734375
train loss:  0.481539785861969
train gradient:  0.11010370420999485
iteration : 7830
train acc:  0.8125
train loss:  0.4376961588859558
train gradient:  0.11082843951395638
iteration : 7831
train acc:  0.7734375
train loss:  0.4724753201007843
train gradient:  0.1238986650537246
iteration : 7832
train acc:  0.71875
train loss:  0.5407143831253052
train gradient:  0.16226228358384082
iteration : 7833
train acc:  0.7734375
train loss:  0.479844331741333
train gradient:  0.1365089986839147
iteration : 7834
train acc:  0.7734375
train loss:  0.4980556070804596
train gradient:  0.12883885025358238
iteration : 7835
train acc:  0.71875
train loss:  0.526320219039917
train gradient:  0.13241668863570344
iteration : 7836
train acc:  0.8046875
train loss:  0.4532584547996521
train gradient:  0.11100546111879604
iteration : 7837
train acc:  0.8203125
train loss:  0.4786282777786255
train gradient:  0.14796905627456303
iteration : 7838
train acc:  0.7109375
train loss:  0.5248010754585266
train gradient:  0.12549435487945865
iteration : 7839
train acc:  0.6953125
train loss:  0.5497100353240967
train gradient:  0.16425928327490072
iteration : 7840
train acc:  0.7734375
train loss:  0.4576282501220703
train gradient:  0.12751937227801946
iteration : 7841
train acc:  0.734375
train loss:  0.5257738828659058
train gradient:  0.18696170963761577
iteration : 7842
train acc:  0.796875
train loss:  0.4743642210960388
train gradient:  0.1169431329233111
iteration : 7843
train acc:  0.703125
train loss:  0.5087029933929443
train gradient:  0.16793572579691057
iteration : 7844
train acc:  0.671875
train loss:  0.5800217390060425
train gradient:  0.18995392744537473
iteration : 7845
train acc:  0.6484375
train loss:  0.617445707321167
train gradient:  0.18435836263048738
iteration : 7846
train acc:  0.8125
train loss:  0.448474645614624
train gradient:  0.12161964569870704
iteration : 7847
train acc:  0.71875
train loss:  0.550269603729248
train gradient:  0.16594597639497294
iteration : 7848
train acc:  0.78125
train loss:  0.4562530517578125
train gradient:  0.10756752784469638
iteration : 7849
train acc:  0.7265625
train loss:  0.5064186453819275
train gradient:  0.1216804715386024
iteration : 7850
train acc:  0.6953125
train loss:  0.540015459060669
train gradient:  0.14185595265426776
iteration : 7851
train acc:  0.734375
train loss:  0.5353236794471741
train gradient:  0.18918719090113542
iteration : 7852
train acc:  0.765625
train loss:  0.5158107876777649
train gradient:  0.1356574899725962
iteration : 7853
train acc:  0.7109375
train loss:  0.5418881177902222
train gradient:  0.12679100211432065
iteration : 7854
train acc:  0.7734375
train loss:  0.4681096076965332
train gradient:  0.14447257443219524
iteration : 7855
train acc:  0.78125
train loss:  0.4665648341178894
train gradient:  0.13681691574680746
iteration : 7856
train acc:  0.7578125
train loss:  0.47944533824920654
train gradient:  0.16725147542398927
iteration : 7857
train acc:  0.7265625
train loss:  0.489658385515213
train gradient:  0.11334055567028586
iteration : 7858
train acc:  0.65625
train loss:  0.6009221076965332
train gradient:  0.23186366202590827
iteration : 7859
train acc:  0.78125
train loss:  0.4498928487300873
train gradient:  0.1100803772235885
iteration : 7860
train acc:  0.765625
train loss:  0.47736552357673645
train gradient:  0.10474582295254443
iteration : 7861
train acc:  0.734375
train loss:  0.5221803784370422
train gradient:  0.1599443646655792
iteration : 7862
train acc:  0.7734375
train loss:  0.5139803886413574
train gradient:  0.1628615528709541
iteration : 7863
train acc:  0.7578125
train loss:  0.49664735794067383
train gradient:  0.12277058941597498
iteration : 7864
train acc:  0.6953125
train loss:  0.555050253868103
train gradient:  0.18622996593544766
iteration : 7865
train acc:  0.78125
train loss:  0.4515001177787781
train gradient:  0.1143050395602932
iteration : 7866
train acc:  0.625
train loss:  0.6227763891220093
train gradient:  0.21856468355170566
iteration : 7867
train acc:  0.7421875
train loss:  0.5211020708084106
train gradient:  0.17232719797962953
iteration : 7868
train acc:  0.734375
train loss:  0.46417200565338135
train gradient:  0.12278574646447456
iteration : 7869
train acc:  0.7109375
train loss:  0.5725760459899902
train gradient:  0.16861408091797842
iteration : 7870
train acc:  0.7421875
train loss:  0.4838138818740845
train gradient:  0.15550426659467503
iteration : 7871
train acc:  0.7265625
train loss:  0.5101611018180847
train gradient:  0.13984795301991293
iteration : 7872
train acc:  0.734375
train loss:  0.5105390548706055
train gradient:  0.1462611633037877
iteration : 7873
train acc:  0.7109375
train loss:  0.5621554255485535
train gradient:  0.17194540218919835
iteration : 7874
train acc:  0.796875
train loss:  0.43768787384033203
train gradient:  0.10993509743101071
iteration : 7875
train acc:  0.8046875
train loss:  0.45588064193725586
train gradient:  0.1234665920026431
iteration : 7876
train acc:  0.6953125
train loss:  0.592875599861145
train gradient:  0.16546525674535906
iteration : 7877
train acc:  0.71875
train loss:  0.5486014485359192
train gradient:  0.21173899243927896
iteration : 7878
train acc:  0.75
train loss:  0.44539275765419006
train gradient:  0.11578780262112442
iteration : 7879
train acc:  0.765625
train loss:  0.5106647610664368
train gradient:  0.16435376399097879
iteration : 7880
train acc:  0.71875
train loss:  0.5265172123908997
train gradient:  0.15257323393153716
iteration : 7881
train acc:  0.7421875
train loss:  0.5168697237968445
train gradient:  0.12872722354257843
iteration : 7882
train acc:  0.828125
train loss:  0.45466092228889465
train gradient:  0.14398004037961076
iteration : 7883
train acc:  0.7578125
train loss:  0.5085797905921936
train gradient:  0.17972006583722583
iteration : 7884
train acc:  0.7578125
train loss:  0.4973040819168091
train gradient:  0.1445272924080835
iteration : 7885
train acc:  0.765625
train loss:  0.5141029953956604
train gradient:  0.14177708519881177
iteration : 7886
train acc:  0.7578125
train loss:  0.5307049751281738
train gradient:  0.13339114719929057
iteration : 7887
train acc:  0.65625
train loss:  0.5614796876907349
train gradient:  0.15737008703978472
iteration : 7888
train acc:  0.75
train loss:  0.5206443071365356
train gradient:  0.12802482692223838
iteration : 7889
train acc:  0.796875
train loss:  0.4026854932308197
train gradient:  0.12647079149372656
iteration : 7890
train acc:  0.7265625
train loss:  0.5375075340270996
train gradient:  0.13617564799365955
iteration : 7891
train acc:  0.6796875
train loss:  0.5639904141426086
train gradient:  0.19020741018945395
iteration : 7892
train acc:  0.7265625
train loss:  0.5171043872833252
train gradient:  0.1923974000241525
iteration : 7893
train acc:  0.671875
train loss:  0.564619243144989
train gradient:  0.18816850019954323
iteration : 7894
train acc:  0.765625
train loss:  0.5078111290931702
train gradient:  0.13158908255284052
iteration : 7895
train acc:  0.6640625
train loss:  0.5992528200149536
train gradient:  0.1993928500524725
iteration : 7896
train acc:  0.8359375
train loss:  0.45717382431030273
train gradient:  0.12359673015532792
iteration : 7897
train acc:  0.7421875
train loss:  0.47186362743377686
train gradient:  0.11556830709312386
iteration : 7898
train acc:  0.78125
train loss:  0.46841102838516235
train gradient:  0.12101347655910877
iteration : 7899
train acc:  0.734375
train loss:  0.5163964033126831
train gradient:  0.12935359866849805
iteration : 7900
train acc:  0.765625
train loss:  0.4882069230079651
train gradient:  0.19745178266072477
iteration : 7901
train acc:  0.78125
train loss:  0.4521613121032715
train gradient:  0.129201064898946
iteration : 7902
train acc:  0.7734375
train loss:  0.44216567277908325
train gradient:  0.11573523894756972
iteration : 7903
train acc:  0.75
train loss:  0.4597589671611786
train gradient:  0.11046721304341017
iteration : 7904
train acc:  0.734375
train loss:  0.5125924348831177
train gradient:  0.11508773748476771
iteration : 7905
train acc:  0.7421875
train loss:  0.5291712284088135
train gradient:  0.15950203488798526
iteration : 7906
train acc:  0.71875
train loss:  0.48660340905189514
train gradient:  0.1203495191688026
iteration : 7907
train acc:  0.7265625
train loss:  0.5222733616828918
train gradient:  0.12502726027509686
iteration : 7908
train acc:  0.671875
train loss:  0.5885136127471924
train gradient:  0.1594808807990632
iteration : 7909
train acc:  0.75
train loss:  0.501985490322113
train gradient:  0.12419602042791895
iteration : 7910
train acc:  0.7421875
train loss:  0.46851223707199097
train gradient:  0.12764080694086205
iteration : 7911
train acc:  0.765625
train loss:  0.48030543327331543
train gradient:  0.16046641817277668
iteration : 7912
train acc:  0.7578125
train loss:  0.5329630374908447
train gradient:  0.1565883268318786
iteration : 7913
train acc:  0.75
train loss:  0.510511577129364
train gradient:  0.13526615441766254
iteration : 7914
train acc:  0.734375
train loss:  0.567193865776062
train gradient:  0.19920729162195913
iteration : 7915
train acc:  0.6640625
train loss:  0.5572729110717773
train gradient:  0.1854364849814299
iteration : 7916
train acc:  0.7265625
train loss:  0.5084151029586792
train gradient:  0.12987528304315016
iteration : 7917
train acc:  0.7421875
train loss:  0.5345692038536072
train gradient:  0.13710733895240088
iteration : 7918
train acc:  0.6953125
train loss:  0.5162165760993958
train gradient:  0.1413077453315655
iteration : 7919
train acc:  0.75
train loss:  0.49226003885269165
train gradient:  0.14704644836201886
iteration : 7920
train acc:  0.765625
train loss:  0.459841251373291
train gradient:  0.12461906900634594
iteration : 7921
train acc:  0.7578125
train loss:  0.4632830023765564
train gradient:  0.1194414845774833
iteration : 7922
train acc:  0.78125
train loss:  0.4617586135864258
train gradient:  0.13891389566214568
iteration : 7923
train acc:  0.7421875
train loss:  0.483958899974823
train gradient:  0.11279483406810553
iteration : 7924
train acc:  0.7265625
train loss:  0.5229027271270752
train gradient:  0.1661644911807253
iteration : 7925
train acc:  0.75
train loss:  0.4945183992385864
train gradient:  0.11402791832336083
iteration : 7926
train acc:  0.75
train loss:  0.5188937783241272
train gradient:  0.12567819642491448
iteration : 7927
train acc:  0.7578125
train loss:  0.43681079149246216
train gradient:  0.10680943602634105
iteration : 7928
train acc:  0.7734375
train loss:  0.4590027332305908
train gradient:  0.10716085030922626
iteration : 7929
train acc:  0.7734375
train loss:  0.48281165957450867
train gradient:  0.1373112725675355
iteration : 7930
train acc:  0.75
train loss:  0.5200871825218201
train gradient:  0.12821815686659022
iteration : 7931
train acc:  0.6953125
train loss:  0.49860480427742004
train gradient:  0.13873312786925918
iteration : 7932
train acc:  0.7734375
train loss:  0.4449998438358307
train gradient:  0.16933658927202425
iteration : 7933
train acc:  0.6875
train loss:  0.5856739282608032
train gradient:  0.17774254485570773
iteration : 7934
train acc:  0.7109375
train loss:  0.5102670788764954
train gradient:  0.13617089480354144
iteration : 7935
train acc:  0.765625
train loss:  0.4686651825904846
train gradient:  0.1130545332687204
iteration : 7936
train acc:  0.7265625
train loss:  0.5576353073120117
train gradient:  0.130060254406837
iteration : 7937
train acc:  0.734375
train loss:  0.49821823835372925
train gradient:  0.1427884914918827
iteration : 7938
train acc:  0.6953125
train loss:  0.4976757764816284
train gradient:  0.13064161287633552
iteration : 7939
train acc:  0.7421875
train loss:  0.49630123376846313
train gradient:  0.1050403837110798
iteration : 7940
train acc:  0.7421875
train loss:  0.5431603789329529
train gradient:  0.20942930003623034
iteration : 7941
train acc:  0.765625
train loss:  0.502758800983429
train gradient:  0.1498470651420084
iteration : 7942
train acc:  0.71875
train loss:  0.5199476480484009
train gradient:  0.18018781726567235
iteration : 7943
train acc:  0.796875
train loss:  0.45444780588150024
train gradient:  0.13577492921273165
iteration : 7944
train acc:  0.734375
train loss:  0.5470530986785889
train gradient:  0.12870920944798125
iteration : 7945
train acc:  0.6796875
train loss:  0.5213631391525269
train gradient:  0.12583963819371544
iteration : 7946
train acc:  0.734375
train loss:  0.5223302245140076
train gradient:  0.15287521860934472
iteration : 7947
train acc:  0.7109375
train loss:  0.5181935429573059
train gradient:  0.13344474364391928
iteration : 7948
train acc:  0.7578125
train loss:  0.4861447215080261
train gradient:  0.11265253205014529
iteration : 7949
train acc:  0.6875
train loss:  0.5135521292686462
train gradient:  0.10972090075490672
iteration : 7950
train acc:  0.6953125
train loss:  0.5335297584533691
train gradient:  0.1807913278751204
iteration : 7951
train acc:  0.7265625
train loss:  0.5931239128112793
train gradient:  0.17524049414631165
iteration : 7952
train acc:  0.703125
train loss:  0.5152069926261902
train gradient:  0.14234519360670853
iteration : 7953
train acc:  0.7578125
train loss:  0.4924890697002411
train gradient:  0.1656802446973144
iteration : 7954
train acc:  0.6328125
train loss:  0.5652472972869873
train gradient:  0.15665327040439453
iteration : 7955
train acc:  0.734375
train loss:  0.4896460771560669
train gradient:  0.11386887898152491
iteration : 7956
train acc:  0.75
train loss:  0.5057128667831421
train gradient:  0.12660495756157453
iteration : 7957
train acc:  0.7421875
train loss:  0.4775520861148834
train gradient:  0.1360636074689866
iteration : 7958
train acc:  0.703125
train loss:  0.5010898113250732
train gradient:  0.1346203154587108
iteration : 7959
train acc:  0.734375
train loss:  0.47976991534233093
train gradient:  0.10737803592479668
iteration : 7960
train acc:  0.7265625
train loss:  0.5078696012496948
train gradient:  0.1312836202959335
iteration : 7961
train acc:  0.765625
train loss:  0.47756555676460266
train gradient:  0.1350739046646554
iteration : 7962
train acc:  0.7578125
train loss:  0.4483637511730194
train gradient:  0.12347516622811024
iteration : 7963
train acc:  0.7578125
train loss:  0.5339276790618896
train gradient:  0.13866722006393822
iteration : 7964
train acc:  0.6953125
train loss:  0.5357509851455688
train gradient:  0.17703830828149075
iteration : 7965
train acc:  0.671875
train loss:  0.5435695052146912
train gradient:  0.1597685644609894
iteration : 7966
train acc:  0.6953125
train loss:  0.5163001418113708
train gradient:  0.12926426284282666
iteration : 7967
train acc:  0.671875
train loss:  0.5388157963752747
train gradient:  0.1712993423734085
iteration : 7968
train acc:  0.75
train loss:  0.5020226240158081
train gradient:  0.13275961089471544
iteration : 7969
train acc:  0.7578125
train loss:  0.4990440011024475
train gradient:  0.125415655828021
iteration : 7970
train acc:  0.734375
train loss:  0.4783437252044678
train gradient:  0.13036923374177295
iteration : 7971
train acc:  0.671875
train loss:  0.5919685363769531
train gradient:  0.1851823809757218
iteration : 7972
train acc:  0.7421875
train loss:  0.4617007374763489
train gradient:  0.11239216115426238
iteration : 7973
train acc:  0.734375
train loss:  0.4860667884349823
train gradient:  0.13499586170502392
iteration : 7974
train acc:  0.75
train loss:  0.4490165114402771
train gradient:  0.11664169439408908
iteration : 7975
train acc:  0.71875
train loss:  0.4796469509601593
train gradient:  0.11748037595424671
iteration : 7976
train acc:  0.6953125
train loss:  0.5369990468025208
train gradient:  0.18947374030816325
iteration : 7977
train acc:  0.7890625
train loss:  0.4385552704334259
train gradient:  0.10316276306244966
iteration : 7978
train acc:  0.71875
train loss:  0.5809526443481445
train gradient:  0.17777981953726513
iteration : 7979
train acc:  0.734375
train loss:  0.4645237326622009
train gradient:  0.1062682792489396
iteration : 7980
train acc:  0.734375
train loss:  0.4707939624786377
train gradient:  0.12854553810332642
iteration : 7981
train acc:  0.765625
train loss:  0.4704558551311493
train gradient:  0.11249999518710585
iteration : 7982
train acc:  0.78125
train loss:  0.4505297541618347
train gradient:  0.14925295507387498
iteration : 7983
train acc:  0.671875
train loss:  0.5608458518981934
train gradient:  0.1920409280852432
iteration : 7984
train acc:  0.7890625
train loss:  0.4766960144042969
train gradient:  0.104938183292639
iteration : 7985
train acc:  0.71875
train loss:  0.5300581455230713
train gradient:  0.1506552181973618
iteration : 7986
train acc:  0.7265625
train loss:  0.5139288902282715
train gradient:  0.12928781276213944
iteration : 7987
train acc:  0.8046875
train loss:  0.48640093207359314
train gradient:  0.15834914207016487
iteration : 7988
train acc:  0.7734375
train loss:  0.49303629994392395
train gradient:  0.11082182932186199
iteration : 7989
train acc:  0.7734375
train loss:  0.4605503976345062
train gradient:  0.10706649306822265
iteration : 7990
train acc:  0.7421875
train loss:  0.4825410842895508
train gradient:  0.1636488785124578
iteration : 7991
train acc:  0.765625
train loss:  0.45778167247772217
train gradient:  0.14434846272385726
iteration : 7992
train acc:  0.78125
train loss:  0.423435240983963
train gradient:  0.10316112869961047
iteration : 7993
train acc:  0.6953125
train loss:  0.5440471172332764
train gradient:  0.162952275094718
iteration : 7994
train acc:  0.796875
train loss:  0.46155959367752075
train gradient:  0.10837831209324529
iteration : 7995
train acc:  0.6796875
train loss:  0.5634645819664001
train gradient:  0.1945334297388408
iteration : 7996
train acc:  0.7578125
train loss:  0.4805371165275574
train gradient:  0.11107260083322565
iteration : 7997
train acc:  0.671875
train loss:  0.5614404678344727
train gradient:  0.18906197241191558
iteration : 7998
train acc:  0.734375
train loss:  0.47346508502960205
train gradient:  0.12176965413274037
iteration : 7999
train acc:  0.703125
train loss:  0.5574114918708801
train gradient:  0.16194436593996991
iteration : 8000
train acc:  0.8046875
train loss:  0.46626874804496765
train gradient:  0.12079163017606108
iteration : 8001
train acc:  0.7578125
train loss:  0.5163587927818298
train gradient:  0.19074831566693762
iteration : 8002
train acc:  0.75
train loss:  0.5196412801742554
train gradient:  0.16637562898406005
iteration : 8003
train acc:  0.7734375
train loss:  0.470619797706604
train gradient:  0.09317336131027384
iteration : 8004
train acc:  0.7734375
train loss:  0.47065937519073486
train gradient:  0.14193054168013555
iteration : 8005
train acc:  0.75
train loss:  0.4964284300804138
train gradient:  0.15043345251583556
iteration : 8006
train acc:  0.7578125
train loss:  0.4875832796096802
train gradient:  0.12079688215136317
iteration : 8007
train acc:  0.7578125
train loss:  0.5243746042251587
train gradient:  0.13920171524488476
iteration : 8008
train acc:  0.75
train loss:  0.49734413623809814
train gradient:  0.1306735691282963
iteration : 8009
train acc:  0.75
train loss:  0.4936662018299103
train gradient:  0.12707176616820312
iteration : 8010
train acc:  0.8125
train loss:  0.44982609152793884
train gradient:  0.15627385045488282
iteration : 8011
train acc:  0.734375
train loss:  0.5621317625045776
train gradient:  0.15499981991167114
iteration : 8012
train acc:  0.75
train loss:  0.4543852210044861
train gradient:  0.09447335812372354
iteration : 8013
train acc:  0.734375
train loss:  0.5363047122955322
train gradient:  0.14729380422468785
iteration : 8014
train acc:  0.7109375
train loss:  0.4891626238822937
train gradient:  0.12715077431034927
iteration : 8015
train acc:  0.78125
train loss:  0.45009589195251465
train gradient:  0.1261295006850673
iteration : 8016
train acc:  0.734375
train loss:  0.5080623626708984
train gradient:  0.12934268645445815
iteration : 8017
train acc:  0.7734375
train loss:  0.4988134801387787
train gradient:  0.1408371781956484
iteration : 8018
train acc:  0.7734375
train loss:  0.49683791399002075
train gradient:  0.16283462333850074
iteration : 8019
train acc:  0.7265625
train loss:  0.5334192514419556
train gradient:  0.15897112446221118
iteration : 8020
train acc:  0.734375
train loss:  0.4706137478351593
train gradient:  0.14084297136119617
iteration : 8021
train acc:  0.7265625
train loss:  0.525080680847168
train gradient:  0.14418269272406714
iteration : 8022
train acc:  0.7734375
train loss:  0.45099878311157227
train gradient:  0.1208972166795932
iteration : 8023
train acc:  0.7265625
train loss:  0.5103350877761841
train gradient:  0.14313972397052063
iteration : 8024
train acc:  0.7421875
train loss:  0.5394441485404968
train gradient:  0.14548957352389436
iteration : 8025
train acc:  0.796875
train loss:  0.4897206425666809
train gradient:  0.1372234989997256
iteration : 8026
train acc:  0.6953125
train loss:  0.5967807769775391
train gradient:  0.20299503369175914
iteration : 8027
train acc:  0.71875
train loss:  0.5120234489440918
train gradient:  0.15370674633416972
iteration : 8028
train acc:  0.703125
train loss:  0.5398427248001099
train gradient:  0.17718146054726908
iteration : 8029
train acc:  0.78125
train loss:  0.4559670388698578
train gradient:  0.12251037796570394
iteration : 8030
train acc:  0.71875
train loss:  0.5200023651123047
train gradient:  0.14629762322938383
iteration : 8031
train acc:  0.75
train loss:  0.5263926982879639
train gradient:  0.1573424567361818
iteration : 8032
train acc:  0.734375
train loss:  0.47885164618492126
train gradient:  0.13871613527999566
iteration : 8033
train acc:  0.75
train loss:  0.479745477437973
train gradient:  0.10846631283905314
iteration : 8034
train acc:  0.703125
train loss:  0.5348724126815796
train gradient:  0.1634531402970908
iteration : 8035
train acc:  0.7421875
train loss:  0.5731436014175415
train gradient:  0.20797694708078954
iteration : 8036
train acc:  0.6875
train loss:  0.49287623167037964
train gradient:  0.15330693722638145
iteration : 8037
train acc:  0.765625
train loss:  0.4788253903388977
train gradient:  0.1837735393382195
iteration : 8038
train acc:  0.7890625
train loss:  0.5111843347549438
train gradient:  0.167792217011571
iteration : 8039
train acc:  0.734375
train loss:  0.47496503591537476
train gradient:  0.12798817264250295
iteration : 8040
train acc:  0.765625
train loss:  0.5126327276229858
train gradient:  0.1386191292721996
iteration : 8041
train acc:  0.7578125
train loss:  0.49841153621673584
train gradient:  0.1324514879822904
iteration : 8042
train acc:  0.734375
train loss:  0.49962279200553894
train gradient:  0.13373172982899026
iteration : 8043
train acc:  0.7421875
train loss:  0.4563828110694885
train gradient:  0.11172340311037962
iteration : 8044
train acc:  0.7578125
train loss:  0.5296080708503723
train gradient:  0.13295360417291655
iteration : 8045
train acc:  0.8125
train loss:  0.4830508232116699
train gradient:  0.13608357848707758
iteration : 8046
train acc:  0.84375
train loss:  0.4087573289871216
train gradient:  0.10944357237265982
iteration : 8047
train acc:  0.734375
train loss:  0.5134801268577576
train gradient:  0.1504681072713218
iteration : 8048
train acc:  0.7734375
train loss:  0.47942984104156494
train gradient:  0.10538092303551432
iteration : 8049
train acc:  0.734375
train loss:  0.4843311905860901
train gradient:  0.11919617500012995
iteration : 8050
train acc:  0.7265625
train loss:  0.528344988822937
train gradient:  0.1373027084177711
iteration : 8051
train acc:  0.7109375
train loss:  0.4865989089012146
train gradient:  0.16016909486189712
iteration : 8052
train acc:  0.734375
train loss:  0.524043083190918
train gradient:  0.14636027309691724
iteration : 8053
train acc:  0.765625
train loss:  0.4896974563598633
train gradient:  0.11420555440150579
iteration : 8054
train acc:  0.71875
train loss:  0.4935277998447418
train gradient:  0.1417532709835168
iteration : 8055
train acc:  0.71875
train loss:  0.5828351378440857
train gradient:  0.16368420629202632
iteration : 8056
train acc:  0.65625
train loss:  0.5487576127052307
train gradient:  0.13145753367600588
iteration : 8057
train acc:  0.703125
train loss:  0.5514960289001465
train gradient:  0.14914871176800298
iteration : 8058
train acc:  0.6796875
train loss:  0.5382660627365112
train gradient:  0.1572184953847301
iteration : 8059
train acc:  0.7578125
train loss:  0.4820210933685303
train gradient:  0.12438696531010383
iteration : 8060
train acc:  0.78125
train loss:  0.44720616936683655
train gradient:  0.13374918708446046
iteration : 8061
train acc:  0.7265625
train loss:  0.5085543990135193
train gradient:  0.13865594318723806
iteration : 8062
train acc:  0.71875
train loss:  0.5469452142715454
train gradient:  0.16291036425238292
iteration : 8063
train acc:  0.75
train loss:  0.5360164642333984
train gradient:  0.13979387749229058
iteration : 8064
train acc:  0.7109375
train loss:  0.4820133149623871
train gradient:  0.13449960513967846
iteration : 8065
train acc:  0.7265625
train loss:  0.5061204433441162
train gradient:  0.12600415082190145
iteration : 8066
train acc:  0.6953125
train loss:  0.5201470851898193
train gradient:  0.15128123719312664
iteration : 8067
train acc:  0.734375
train loss:  0.5610765814781189
train gradient:  0.1804005793313357
iteration : 8068
train acc:  0.703125
train loss:  0.5289812088012695
train gradient:  0.15607112119838384
iteration : 8069
train acc:  0.7109375
train loss:  0.5030122399330139
train gradient:  0.1215245159473733
iteration : 8070
train acc:  0.71875
train loss:  0.5404983758926392
train gradient:  0.14192707118699532
iteration : 8071
train acc:  0.7265625
train loss:  0.5049666166305542
train gradient:  0.21106127630953175
iteration : 8072
train acc:  0.8203125
train loss:  0.43417036533355713
train gradient:  0.09264861856407841
iteration : 8073
train acc:  0.7578125
train loss:  0.4479050040245056
train gradient:  0.12753605290433295
iteration : 8074
train acc:  0.6796875
train loss:  0.5428301095962524
train gradient:  0.1686620113813071
iteration : 8075
train acc:  0.7421875
train loss:  0.4813915491104126
train gradient:  0.14829185216539348
iteration : 8076
train acc:  0.8125
train loss:  0.4246080815792084
train gradient:  0.10625560209500796
iteration : 8077
train acc:  0.71875
train loss:  0.5066150426864624
train gradient:  0.15268418584872323
iteration : 8078
train acc:  0.703125
train loss:  0.5329886674880981
train gradient:  0.150305005380708
iteration : 8079
train acc:  0.84375
train loss:  0.42918282747268677
train gradient:  0.12316428060895057
iteration : 8080
train acc:  0.734375
train loss:  0.5188449621200562
train gradient:  0.16407032695298285
iteration : 8081
train acc:  0.765625
train loss:  0.5267282724380493
train gradient:  0.18353961689086026
iteration : 8082
train acc:  0.734375
train loss:  0.5400619506835938
train gradient:  0.1789664380375693
iteration : 8083
train acc:  0.765625
train loss:  0.44720011949539185
train gradient:  0.140789701063035
iteration : 8084
train acc:  0.703125
train loss:  0.5617485046386719
train gradient:  0.20656562927913114
iteration : 8085
train acc:  0.703125
train loss:  0.5594092011451721
train gradient:  0.20170726875907558
iteration : 8086
train acc:  0.7578125
train loss:  0.49603837728500366
train gradient:  0.13917115177492573
iteration : 8087
train acc:  0.7109375
train loss:  0.5175784826278687
train gradient:  0.1330561313641414
iteration : 8088
train acc:  0.734375
train loss:  0.49333468079566956
train gradient:  0.12767009837242574
iteration : 8089
train acc:  0.734375
train loss:  0.4894753396511078
train gradient:  0.12311222348552947
iteration : 8090
train acc:  0.7578125
train loss:  0.5248475670814514
train gradient:  0.13882956227896298
iteration : 8091
train acc:  0.7578125
train loss:  0.4837590754032135
train gradient:  0.12070042634675175
iteration : 8092
train acc:  0.7578125
train loss:  0.542648434638977
train gradient:  0.14775605490233273
iteration : 8093
train acc:  0.6875
train loss:  0.5261867046356201
train gradient:  0.16270452703449512
iteration : 8094
train acc:  0.7421875
train loss:  0.4946955442428589
train gradient:  0.1357685770872898
iteration : 8095
train acc:  0.6796875
train loss:  0.5418283939361572
train gradient:  0.14576024613706018
iteration : 8096
train acc:  0.6953125
train loss:  0.5327523946762085
train gradient:  0.13728324207506323
iteration : 8097
train acc:  0.7734375
train loss:  0.45440801978111267
train gradient:  0.11162198710405188
iteration : 8098
train acc:  0.734375
train loss:  0.5298085808753967
train gradient:  0.1463851600091034
iteration : 8099
train acc:  0.6875
train loss:  0.5435528755187988
train gradient:  0.14456203797107936
iteration : 8100
train acc:  0.765625
train loss:  0.5098944902420044
train gradient:  0.15310009427474386
iteration : 8101
train acc:  0.8203125
train loss:  0.4291611909866333
train gradient:  0.1335060821457685
iteration : 8102
train acc:  0.78125
train loss:  0.44986996054649353
train gradient:  0.12653822875348214
iteration : 8103
train acc:  0.7421875
train loss:  0.5223155617713928
train gradient:  0.14981965138156733
iteration : 8104
train acc:  0.75
train loss:  0.48559412360191345
train gradient:  0.16589037306290763
iteration : 8105
train acc:  0.7421875
train loss:  0.5235186815261841
train gradient:  0.1445510053045882
iteration : 8106
train acc:  0.6953125
train loss:  0.5203089714050293
train gradient:  0.19720819956442898
iteration : 8107
train acc:  0.71875
train loss:  0.5136630535125732
train gradient:  0.14374166544765804
iteration : 8108
train acc:  0.7890625
train loss:  0.43506449460983276
train gradient:  0.10464658286316242
iteration : 8109
train acc:  0.734375
train loss:  0.5002155900001526
train gradient:  0.18241987711865676
iteration : 8110
train acc:  0.7109375
train loss:  0.5238914489746094
train gradient:  0.13926178129680472
iteration : 8111
train acc:  0.7421875
train loss:  0.5024138689041138
train gradient:  0.14643382973148975
iteration : 8112
train acc:  0.71875
train loss:  0.5271828174591064
train gradient:  0.1293089176610952
iteration : 8113
train acc:  0.796875
train loss:  0.45236894488334656
train gradient:  0.14856347885484106
iteration : 8114
train acc:  0.71875
train loss:  0.49822020530700684
train gradient:  0.14763951591910474
iteration : 8115
train acc:  0.703125
train loss:  0.5221846103668213
train gradient:  0.1263376429203102
iteration : 8116
train acc:  0.765625
train loss:  0.44834962487220764
train gradient:  0.09542814747897033
iteration : 8117
train acc:  0.7578125
train loss:  0.4657544493675232
train gradient:  0.12402173924567012
iteration : 8118
train acc:  0.7421875
train loss:  0.5241819024085999
train gradient:  0.1531150607731677
iteration : 8119
train acc:  0.7578125
train loss:  0.456734299659729
train gradient:  0.12841496282694803
iteration : 8120
train acc:  0.734375
train loss:  0.5027370452880859
train gradient:  0.1345652407902898
iteration : 8121
train acc:  0.7578125
train loss:  0.45642703771591187
train gradient:  0.13673951912888932
iteration : 8122
train acc:  0.8359375
train loss:  0.4003699719905853
train gradient:  0.1484086966871226
iteration : 8123
train acc:  0.796875
train loss:  0.4388158619403839
train gradient:  0.0998855798451691
iteration : 8124
train acc:  0.7578125
train loss:  0.48778754472732544
train gradient:  0.12463300581631188
iteration : 8125
train acc:  0.7265625
train loss:  0.5071869492530823
train gradient:  0.13225196090219593
iteration : 8126
train acc:  0.7890625
train loss:  0.4873141646385193
train gradient:  0.1451147267201074
iteration : 8127
train acc:  0.6640625
train loss:  0.5487384796142578
train gradient:  0.13691253989906726
iteration : 8128
train acc:  0.8125
train loss:  0.4749193489551544
train gradient:  0.1336921673170256
iteration : 8129
train acc:  0.75
train loss:  0.47217661142349243
train gradient:  0.14012411087339438
iteration : 8130
train acc:  0.7734375
train loss:  0.4481702744960785
train gradient:  0.1117893264067642
iteration : 8131
train acc:  0.84375
train loss:  0.40101319551467896
train gradient:  0.09919898642918
iteration : 8132
train acc:  0.7734375
train loss:  0.45722848176956177
train gradient:  0.12431461987742333
iteration : 8133
train acc:  0.7109375
train loss:  0.5335310697555542
train gradient:  0.1896557195633371
iteration : 8134
train acc:  0.7734375
train loss:  0.505655825138092
train gradient:  0.19021238682900998
iteration : 8135
train acc:  0.78125
train loss:  0.4725126624107361
train gradient:  0.13897020049789888
iteration : 8136
train acc:  0.7421875
train loss:  0.4888700842857361
train gradient:  0.13920385410834546
iteration : 8137
train acc:  0.734375
train loss:  0.5102227926254272
train gradient:  0.13954663316867227
iteration : 8138
train acc:  0.6796875
train loss:  0.6300503015518188
train gradient:  0.215864726423897
iteration : 8139
train acc:  0.6953125
train loss:  0.583484411239624
train gradient:  0.17122473359982537
iteration : 8140
train acc:  0.7109375
train loss:  0.5403380393981934
train gradient:  0.15213336122599014
iteration : 8141
train acc:  0.703125
train loss:  0.497226744890213
train gradient:  0.12088686074262064
iteration : 8142
train acc:  0.7734375
train loss:  0.49226418137550354
train gradient:  0.12072350551442593
iteration : 8143
train acc:  0.796875
train loss:  0.4547494053840637
train gradient:  0.13761415577825353
iteration : 8144
train acc:  0.765625
train loss:  0.5150641202926636
train gradient:  0.14250980290466714
iteration : 8145
train acc:  0.6796875
train loss:  0.5638116002082825
train gradient:  0.1364569308424644
iteration : 8146
train acc:  0.6875
train loss:  0.5338717699050903
train gradient:  0.14474889941027563
iteration : 8147
train acc:  0.734375
train loss:  0.5071884989738464
train gradient:  0.13046132597001212
iteration : 8148
train acc:  0.734375
train loss:  0.5260344743728638
train gradient:  0.25437945092280145
iteration : 8149
train acc:  0.765625
train loss:  0.46914422512054443
train gradient:  0.13504228733509716
iteration : 8150
train acc:  0.7578125
train loss:  0.5279821753501892
train gradient:  0.17631707261467816
iteration : 8151
train acc:  0.7578125
train loss:  0.452046662569046
train gradient:  0.11018509995978876
iteration : 8152
train acc:  0.7109375
train loss:  0.49660706520080566
train gradient:  0.12409528361765332
iteration : 8153
train acc:  0.8046875
train loss:  0.4365870952606201
train gradient:  0.09091908526821652
iteration : 8154
train acc:  0.734375
train loss:  0.5529798865318298
train gradient:  0.16374591076736125
iteration : 8155
train acc:  0.7109375
train loss:  0.5007637143135071
train gradient:  0.120238110464134
iteration : 8156
train acc:  0.78125
train loss:  0.473783016204834
train gradient:  0.1243559842805196
iteration : 8157
train acc:  0.7578125
train loss:  0.4718138575553894
train gradient:  0.1379050425701363
iteration : 8158
train acc:  0.7421875
train loss:  0.4739444851875305
train gradient:  0.12748876765858136
iteration : 8159
train acc:  0.765625
train loss:  0.4612366259098053
train gradient:  0.10748726350197685
iteration : 8160
train acc:  0.7109375
train loss:  0.506319522857666
train gradient:  0.10936979021703357
iteration : 8161
train acc:  0.75
train loss:  0.47929710149765015
train gradient:  0.11393664413135333
iteration : 8162
train acc:  0.7578125
train loss:  0.4676482677459717
train gradient:  0.14433580074856853
iteration : 8163
train acc:  0.7578125
train loss:  0.5150133371353149
train gradient:  0.16628651054340743
iteration : 8164
train acc:  0.703125
train loss:  0.5157663822174072
train gradient:  0.149762257337527
iteration : 8165
train acc:  0.8359375
train loss:  0.4224102795124054
train gradient:  0.12376463407794612
iteration : 8166
train acc:  0.7109375
train loss:  0.4985380172729492
train gradient:  0.12512453645294708
iteration : 8167
train acc:  0.734375
train loss:  0.5507817268371582
train gradient:  0.18374981749995395
iteration : 8168
train acc:  0.6796875
train loss:  0.5554807186126709
train gradient:  0.16572920510656403
iteration : 8169
train acc:  0.6953125
train loss:  0.5348719954490662
train gradient:  0.17379940132426724
iteration : 8170
train acc:  0.75
train loss:  0.5193564891815186
train gradient:  0.1608332270343127
iteration : 8171
train acc:  0.6640625
train loss:  0.5667092800140381
train gradient:  0.20202082125317178
iteration : 8172
train acc:  0.8046875
train loss:  0.407906711101532
train gradient:  0.1032373995857968
iteration : 8173
train acc:  0.703125
train loss:  0.5281417965888977
train gradient:  0.19257557226408367
iteration : 8174
train acc:  0.75
train loss:  0.4825192093849182
train gradient:  0.15811662181735275
iteration : 8175
train acc:  0.75
train loss:  0.5157566070556641
train gradient:  0.19616236638547985
iteration : 8176
train acc:  0.734375
train loss:  0.47987228631973267
train gradient:  0.19376761551425906
iteration : 8177
train acc:  0.6875
train loss:  0.47001034021377563
train gradient:  0.12731586147440194
iteration : 8178
train acc:  0.75
train loss:  0.5354427099227905
train gradient:  0.19470476929804525
iteration : 8179
train acc:  0.75
train loss:  0.446557879447937
train gradient:  0.11393538408560024
iteration : 8180
train acc:  0.7890625
train loss:  0.498841792345047
train gradient:  0.1482695109586812
iteration : 8181
train acc:  0.765625
train loss:  0.48056361079216003
train gradient:  0.14537441455692443
iteration : 8182
train acc:  0.75
train loss:  0.48475465178489685
train gradient:  0.1680088140307308
iteration : 8183
train acc:  0.7734375
train loss:  0.4764477610588074
train gradient:  0.13045582569937575
iteration : 8184
train acc:  0.78125
train loss:  0.5062720775604248
train gradient:  0.13223900033420005
iteration : 8185
train acc:  0.7421875
train loss:  0.5104208588600159
train gradient:  0.15243720411753722
iteration : 8186
train acc:  0.7265625
train loss:  0.5245985984802246
train gradient:  0.1514015641073974
iteration : 8187
train acc:  0.6875
train loss:  0.5472598671913147
train gradient:  0.14658654517373332
iteration : 8188
train acc:  0.7578125
train loss:  0.5032578110694885
train gradient:  0.14342584047476276
iteration : 8189
train acc:  0.65625
train loss:  0.5758126378059387
train gradient:  0.16344610570241672
iteration : 8190
train acc:  0.8046875
train loss:  0.4493655264377594
train gradient:  0.21268591639170054
iteration : 8191
train acc:  0.75
train loss:  0.5222052335739136
train gradient:  0.1916849507236507
iteration : 8192
train acc:  0.6484375
train loss:  0.6120018362998962
train gradient:  0.1934984077991726
iteration : 8193
train acc:  0.71875
train loss:  0.5471418499946594
train gradient:  0.12590861130420566
iteration : 8194
train acc:  0.828125
train loss:  0.39090174436569214
train gradient:  0.11244123708143607
iteration : 8195
train acc:  0.7734375
train loss:  0.48380106687545776
train gradient:  0.13059222784350422
iteration : 8196
train acc:  0.7109375
train loss:  0.5807390213012695
train gradient:  0.18613537926271725
iteration : 8197
train acc:  0.734375
train loss:  0.49829792976379395
train gradient:  0.13826519526116648
iteration : 8198
train acc:  0.7734375
train loss:  0.45913687348365784
train gradient:  0.12081742914610145
iteration : 8199
train acc:  0.6796875
train loss:  0.5215049386024475
train gradient:  0.15161330816024723
iteration : 8200
train acc:  0.7421875
train loss:  0.4938489496707916
train gradient:  0.14440073481579624
iteration : 8201
train acc:  0.734375
train loss:  0.5286382436752319
train gradient:  0.1876214085503226
iteration : 8202
train acc:  0.703125
train loss:  0.5371221303939819
train gradient:  0.1472570894333304
iteration : 8203
train acc:  0.8125
train loss:  0.4321241080760956
train gradient:  0.12558771543015856
iteration : 8204
train acc:  0.765625
train loss:  0.4990634322166443
train gradient:  0.17443609273744226
iteration : 8205
train acc:  0.8046875
train loss:  0.43760377168655396
train gradient:  0.1349391073825076
iteration : 8206
train acc:  0.75
train loss:  0.5438868999481201
train gradient:  0.16744521485405284
iteration : 8207
train acc:  0.7109375
train loss:  0.5900357961654663
train gradient:  0.18296931901393046
iteration : 8208
train acc:  0.6875
train loss:  0.5604839324951172
train gradient:  0.1910402346005081
iteration : 8209
train acc:  0.7734375
train loss:  0.4984740912914276
train gradient:  0.11185332703936912
iteration : 8210
train acc:  0.6796875
train loss:  0.5775496959686279
train gradient:  0.17018764397097685
iteration : 8211
train acc:  0.796875
train loss:  0.45663464069366455
train gradient:  0.11866693121345118
iteration : 8212
train acc:  0.7734375
train loss:  0.4751259386539459
train gradient:  0.12880312785053427
iteration : 8213
train acc:  0.75
train loss:  0.4825603663921356
train gradient:  0.12890583335276617
iteration : 8214
train acc:  0.7265625
train loss:  0.5285258889198303
train gradient:  0.13877589165406667
iteration : 8215
train acc:  0.703125
train loss:  0.5485154986381531
train gradient:  0.18059553781330367
iteration : 8216
train acc:  0.7578125
train loss:  0.506564199924469
train gradient:  0.14565146951484026
iteration : 8217
train acc:  0.796875
train loss:  0.4804602265357971
train gradient:  0.15517294512121427
iteration : 8218
train acc:  0.71875
train loss:  0.5543862581253052
train gradient:  0.15062131986965022
iteration : 8219
train acc:  0.734375
train loss:  0.5136919617652893
train gradient:  0.14771531389603665
iteration : 8220
train acc:  0.7578125
train loss:  0.5214420557022095
train gradient:  0.1931136362248242
iteration : 8221
train acc:  0.78125
train loss:  0.5297635197639465
train gradient:  0.1775785563427771
iteration : 8222
train acc:  0.71875
train loss:  0.5402004718780518
train gradient:  0.1688778851375425
iteration : 8223
train acc:  0.7109375
train loss:  0.5167206525802612
train gradient:  0.12057459052194491
iteration : 8224
train acc:  0.796875
train loss:  0.42655378580093384
train gradient:  0.1089239234414036
iteration : 8225
train acc:  0.71875
train loss:  0.4882510304450989
train gradient:  0.15487226676286397
iteration : 8226
train acc:  0.8125
train loss:  0.42795249819755554
train gradient:  0.09288176949006151
iteration : 8227
train acc:  0.71875
train loss:  0.5582615733146667
train gradient:  0.14860752812897252
iteration : 8228
train acc:  0.734375
train loss:  0.520401656627655
train gradient:  0.15420092295930826
iteration : 8229
train acc:  0.7109375
train loss:  0.5740195512771606
train gradient:  0.2048602977347821
iteration : 8230
train acc:  0.71875
train loss:  0.46884194016456604
train gradient:  0.10653087058314155
iteration : 8231
train acc:  0.7734375
train loss:  0.5038668513298035
train gradient:  0.15268377756938661
iteration : 8232
train acc:  0.734375
train loss:  0.5056015253067017
train gradient:  0.177807094509761
iteration : 8233
train acc:  0.71875
train loss:  0.5142865180969238
train gradient:  0.1400328893599248
iteration : 8234
train acc:  0.7265625
train loss:  0.48070427775382996
train gradient:  0.09738473959218537
iteration : 8235
train acc:  0.78125
train loss:  0.4393009841442108
train gradient:  0.1080158430873115
iteration : 8236
train acc:  0.734375
train loss:  0.544398844242096
train gradient:  0.18548615507965055
iteration : 8237
train acc:  0.7109375
train loss:  0.48666730523109436
train gradient:  0.10133292341637343
iteration : 8238
train acc:  0.7578125
train loss:  0.4772736728191376
train gradient:  0.10904559861570524
iteration : 8239
train acc:  0.7734375
train loss:  0.44797664880752563
train gradient:  0.10118624389702843
iteration : 8240
train acc:  0.75
train loss:  0.4899024963378906
train gradient:  0.11098792659730287
iteration : 8241
train acc:  0.6875
train loss:  0.5596989989280701
train gradient:  0.17452879760332152
iteration : 8242
train acc:  0.6953125
train loss:  0.5590729713439941
train gradient:  0.1404041251911829
iteration : 8243
train acc:  0.7890625
train loss:  0.4705163836479187
train gradient:  0.10858127015632506
iteration : 8244
train acc:  0.7265625
train loss:  0.5200793743133545
train gradient:  0.13432959745730372
iteration : 8245
train acc:  0.6796875
train loss:  0.5351555943489075
train gradient:  0.15660524274453022
iteration : 8246
train acc:  0.75
train loss:  0.4917169511318207
train gradient:  0.1385619755063597
iteration : 8247
train acc:  0.734375
train loss:  0.49475714564323425
train gradient:  0.15377647521380478
iteration : 8248
train acc:  0.71875
train loss:  0.5333004593849182
train gradient:  0.16873364689288736
iteration : 8249
train acc:  0.8125
train loss:  0.396122545003891
train gradient:  0.1031687449011698
iteration : 8250
train acc:  0.78125
train loss:  0.48066744208335876
train gradient:  0.12327161870839416
iteration : 8251
train acc:  0.75
train loss:  0.47812479734420776
train gradient:  0.14043597983287237
iteration : 8252
train acc:  0.703125
train loss:  0.4711621403694153
train gradient:  0.1190261553818458
iteration : 8253
train acc:  0.765625
train loss:  0.451371431350708
train gradient:  0.11986664297067777
iteration : 8254
train acc:  0.6875
train loss:  0.5299204587936401
train gradient:  0.13957110086786856
iteration : 8255
train acc:  0.78125
train loss:  0.46174952387809753
train gradient:  0.12867325536214957
iteration : 8256
train acc:  0.78125
train loss:  0.49765637516975403
train gradient:  0.1500193736945732
iteration : 8257
train acc:  0.8203125
train loss:  0.43925347924232483
train gradient:  0.10738404343982622
iteration : 8258
train acc:  0.671875
train loss:  0.5878427624702454
train gradient:  0.19709254443657825
iteration : 8259
train acc:  0.765625
train loss:  0.4568118751049042
train gradient:  0.12619585335671332
iteration : 8260
train acc:  0.7265625
train loss:  0.542554497718811
train gradient:  0.16518767205111934
iteration : 8261
train acc:  0.7421875
train loss:  0.4681745767593384
train gradient:  0.13510510395491188
iteration : 8262
train acc:  0.734375
train loss:  0.5030421018600464
train gradient:  0.16493694526919336
iteration : 8263
train acc:  0.7578125
train loss:  0.5129805207252502
train gradient:  0.13420899575117423
iteration : 8264
train acc:  0.7109375
train loss:  0.525226354598999
train gradient:  0.1198568843848258
iteration : 8265
train acc:  0.7265625
train loss:  0.5497280359268188
train gradient:  0.17375876314317856
iteration : 8266
train acc:  0.7421875
train loss:  0.47609788179397583
train gradient:  0.12797552227994868
iteration : 8267
train acc:  0.734375
train loss:  0.4724857211112976
train gradient:  0.11992834898949493
iteration : 8268
train acc:  0.71875
train loss:  0.49514269828796387
train gradient:  0.12980813739822217
iteration : 8269
train acc:  0.7265625
train loss:  0.5253610610961914
train gradient:  0.14429817648776982
iteration : 8270
train acc:  0.7421875
train loss:  0.5066866278648376
train gradient:  0.13937981174941247
iteration : 8271
train acc:  0.78125
train loss:  0.4738839268684387
train gradient:  0.11649590784113883
iteration : 8272
train acc:  0.7421875
train loss:  0.5149240493774414
train gradient:  0.152116492484965
iteration : 8273
train acc:  0.765625
train loss:  0.4479079246520996
train gradient:  0.12444163929340121
iteration : 8274
train acc:  0.765625
train loss:  0.5002573728561401
train gradient:  0.14128975489001838
iteration : 8275
train acc:  0.6953125
train loss:  0.5270662307739258
train gradient:  0.15355216393322732
iteration : 8276
train acc:  0.75
train loss:  0.5154146552085876
train gradient:  0.1586604526187353
iteration : 8277
train acc:  0.7734375
train loss:  0.42619916796684265
train gradient:  0.09571012229600437
iteration : 8278
train acc:  0.7578125
train loss:  0.4880610704421997
train gradient:  0.1470661444590746
iteration : 8279
train acc:  0.703125
train loss:  0.5643743872642517
train gradient:  0.16518132541652011
iteration : 8280
train acc:  0.7578125
train loss:  0.49771416187286377
train gradient:  0.12159855998288542
iteration : 8281
train acc:  0.703125
train loss:  0.5199969410896301
train gradient:  0.15095647554906222
iteration : 8282
train acc:  0.8046875
train loss:  0.472390741109848
train gradient:  0.11710264056515723
iteration : 8283
train acc:  0.7578125
train loss:  0.482221782207489
train gradient:  0.1752635874763399
iteration : 8284
train acc:  0.671875
train loss:  0.5572579503059387
train gradient:  0.1514021574374942
iteration : 8285
train acc:  0.7265625
train loss:  0.5381778478622437
train gradient:  0.15775672076061337
iteration : 8286
train acc:  0.7265625
train loss:  0.5204671025276184
train gradient:  0.15529547836431135
iteration : 8287
train acc:  0.7421875
train loss:  0.4959975481033325
train gradient:  0.13165914558276787
iteration : 8288
train acc:  0.6640625
train loss:  0.6065986156463623
train gradient:  0.21529686014876093
iteration : 8289
train acc:  0.71875
train loss:  0.5138440132141113
train gradient:  0.1873126166519658
iteration : 8290
train acc:  0.75
train loss:  0.5081961154937744
train gradient:  0.12344243770657895
iteration : 8291
train acc:  0.7734375
train loss:  0.49053242802619934
train gradient:  0.1713763018203409
iteration : 8292
train acc:  0.734375
train loss:  0.5338914394378662
train gradient:  0.1529387360200969
iteration : 8293
train acc:  0.765625
train loss:  0.4727569818496704
train gradient:  0.09970954635669409
iteration : 8294
train acc:  0.703125
train loss:  0.5475047826766968
train gradient:  0.1730511116861102
iteration : 8295
train acc:  0.7265625
train loss:  0.5078274607658386
train gradient:  0.1365854329408715
iteration : 8296
train acc:  0.703125
train loss:  0.5035127997398376
train gradient:  0.1566086279280126
iteration : 8297
train acc:  0.7890625
train loss:  0.4486599266529083
train gradient:  0.1084981769182831
iteration : 8298
train acc:  0.6875
train loss:  0.4901544749736786
train gradient:  0.12662745532383962
iteration : 8299
train acc:  0.796875
train loss:  0.47732627391815186
train gradient:  0.11083339942112881
iteration : 8300
train acc:  0.765625
train loss:  0.5527089238166809
train gradient:  0.1996277151004049
iteration : 8301
train acc:  0.7265625
train loss:  0.5371448993682861
train gradient:  0.18002197044523555
iteration : 8302
train acc:  0.7734375
train loss:  0.5063549876213074
train gradient:  0.14504415312340035
iteration : 8303
train acc:  0.8359375
train loss:  0.4177367091178894
train gradient:  0.09496201634374668
iteration : 8304
train acc:  0.796875
train loss:  0.47336602210998535
train gradient:  0.1450076645983272
iteration : 8305
train acc:  0.7890625
train loss:  0.44935643672943115
train gradient:  0.13657477343988872
iteration : 8306
train acc:  0.7890625
train loss:  0.4444100856781006
train gradient:  0.15446141693097082
iteration : 8307
train acc:  0.7265625
train loss:  0.48091965913772583
train gradient:  0.1580278095862111
iteration : 8308
train acc:  0.71875
train loss:  0.5503444671630859
train gradient:  0.15213962633795458
iteration : 8309
train acc:  0.7421875
train loss:  0.5423630475997925
train gradient:  0.15750112790450033
iteration : 8310
train acc:  0.703125
train loss:  0.5603901743888855
train gradient:  0.16544031408971988
iteration : 8311
train acc:  0.6875
train loss:  0.5867577791213989
train gradient:  0.2062192031203793
iteration : 8312
train acc:  0.703125
train loss:  0.5233888030052185
train gradient:  0.1771615035038671
iteration : 8313
train acc:  0.75
train loss:  0.4568156599998474
train gradient:  0.09949890149668643
iteration : 8314
train acc:  0.796875
train loss:  0.4342629015445709
train gradient:  0.1296203232766058
iteration : 8315
train acc:  0.71875
train loss:  0.5523465871810913
train gradient:  0.1574772197437431
iteration : 8316
train acc:  0.75
train loss:  0.5147427320480347
train gradient:  0.17877967933619848
iteration : 8317
train acc:  0.8046875
train loss:  0.440868616104126
train gradient:  0.09049160378816407
iteration : 8318
train acc:  0.75
train loss:  0.4923098683357239
train gradient:  0.1395213765383338
iteration : 8319
train acc:  0.796875
train loss:  0.441857248544693
train gradient:  0.13404466501916337
iteration : 8320
train acc:  0.71875
train loss:  0.5070698261260986
train gradient:  0.11955220984318404
iteration : 8321
train acc:  0.796875
train loss:  0.47094738483428955
train gradient:  0.13569051500611312
iteration : 8322
train acc:  0.71875
train loss:  0.5137110948562622
train gradient:  0.12572364775549336
iteration : 8323
train acc:  0.796875
train loss:  0.4530589282512665
train gradient:  0.11405698159149513
iteration : 8324
train acc:  0.7734375
train loss:  0.4570038914680481
train gradient:  0.11986055749685091
iteration : 8325
train acc:  0.765625
train loss:  0.49227088689804077
train gradient:  0.1321060642620866
iteration : 8326
train acc:  0.6953125
train loss:  0.5735889673233032
train gradient:  0.1609800601981583
iteration : 8327
train acc:  0.6875
train loss:  0.5454109907150269
train gradient:  0.1361510321495819
iteration : 8328
train acc:  0.7890625
train loss:  0.46891480684280396
train gradient:  0.16154720704588543
iteration : 8329
train acc:  0.7421875
train loss:  0.5192556381225586
train gradient:  0.12479879657242317
iteration : 8330
train acc:  0.7890625
train loss:  0.4327441453933716
train gradient:  0.12118034870402662
iteration : 8331
train acc:  0.609375
train loss:  0.5852053761482239
train gradient:  0.20954852330680146
iteration : 8332
train acc:  0.71875
train loss:  0.5327887535095215
train gradient:  0.15278217488230106
iteration : 8333
train acc:  0.75
train loss:  0.5116903781890869
train gradient:  0.1458721747271981
iteration : 8334
train acc:  0.8203125
train loss:  0.4212472438812256
train gradient:  0.10853091673928457
iteration : 8335
train acc:  0.75
train loss:  0.5200790762901306
train gradient:  0.1460989365921738
iteration : 8336
train acc:  0.828125
train loss:  0.4071248769760132
train gradient:  0.11477611050806562
iteration : 8337
train acc:  0.65625
train loss:  0.6178293824195862
train gradient:  0.16570042820638042
iteration : 8338
train acc:  0.65625
train loss:  0.6263601779937744
train gradient:  0.20097302992572808
iteration : 8339
train acc:  0.703125
train loss:  0.48323875665664673
train gradient:  0.11919868096059337
iteration : 8340
train acc:  0.7578125
train loss:  0.5000554919242859
train gradient:  0.13481695167864988
iteration : 8341
train acc:  0.75
train loss:  0.49821245670318604
train gradient:  0.1532047263656412
iteration : 8342
train acc:  0.7734375
train loss:  0.4976465106010437
train gradient:  0.10214833999229625
iteration : 8343
train acc:  0.75
train loss:  0.48182255029678345
train gradient:  0.16528861583609653
iteration : 8344
train acc:  0.796875
train loss:  0.45891228318214417
train gradient:  0.1253729923030964
iteration : 8345
train acc:  0.7265625
train loss:  0.4953843355178833
train gradient:  0.13713462013956715
iteration : 8346
train acc:  0.75
train loss:  0.5202509164810181
train gradient:  0.15773068944533353
iteration : 8347
train acc:  0.765625
train loss:  0.5199027061462402
train gradient:  0.14921068226836512
iteration : 8348
train acc:  0.71875
train loss:  0.49319934844970703
train gradient:  0.10914403732566869
iteration : 8349
train acc:  0.78125
train loss:  0.471291720867157
train gradient:  0.1352448778325236
iteration : 8350
train acc:  0.78125
train loss:  0.4645877182483673
train gradient:  0.09200817675809966
iteration : 8351
train acc:  0.7734375
train loss:  0.532635509967804
train gradient:  0.13312093943827674
iteration : 8352
train acc:  0.78125
train loss:  0.43292123079299927
train gradient:  0.09948087697217368
iteration : 8353
train acc:  0.7109375
train loss:  0.48560357093811035
train gradient:  0.12819829451000303
iteration : 8354
train acc:  0.78125
train loss:  0.4536134600639343
train gradient:  0.10528562993559602
iteration : 8355
train acc:  0.6875
train loss:  0.5951725244522095
train gradient:  0.1922454120652779
iteration : 8356
train acc:  0.8046875
train loss:  0.4532592296600342
train gradient:  0.13197684579302976
iteration : 8357
train acc:  0.7421875
train loss:  0.5552986860275269
train gradient:  0.19395018600037733
iteration : 8358
train acc:  0.703125
train loss:  0.563789963722229
train gradient:  0.16720829754406588
iteration : 8359
train acc:  0.6953125
train loss:  0.5193275213241577
train gradient:  0.15495659384471686
iteration : 8360
train acc:  0.7421875
train loss:  0.4913378059864044
train gradient:  0.10479666223742606
iteration : 8361
train acc:  0.71875
train loss:  0.5626192092895508
train gradient:  0.15593865343337085
iteration : 8362
train acc:  0.734375
train loss:  0.49908313155174255
train gradient:  0.1327576326116532
iteration : 8363
train acc:  0.796875
train loss:  0.444461464881897
train gradient:  0.1330313279795691
iteration : 8364
train acc:  0.6796875
train loss:  0.5440278053283691
train gradient:  0.18447968871278858
iteration : 8365
train acc:  0.734375
train loss:  0.4895019233226776
train gradient:  0.11697707583692832
iteration : 8366
train acc:  0.7109375
train loss:  0.5176318883895874
train gradient:  0.12656337953950406
iteration : 8367
train acc:  0.7109375
train loss:  0.5375192165374756
train gradient:  0.16852669945870674
iteration : 8368
train acc:  0.7734375
train loss:  0.4635738730430603
train gradient:  0.13080904072500416
iteration : 8369
train acc:  0.6796875
train loss:  0.5880736112594604
train gradient:  0.21106719309753696
iteration : 8370
train acc:  0.703125
train loss:  0.5289130210876465
train gradient:  0.15266383659396543
iteration : 8371
train acc:  0.7265625
train loss:  0.5136606693267822
train gradient:  0.1369368398213178
iteration : 8372
train acc:  0.75
train loss:  0.513835608959198
train gradient:  0.16972002273187592
iteration : 8373
train acc:  0.71875
train loss:  0.553044319152832
train gradient:  0.1510404259027695
iteration : 8374
train acc:  0.765625
train loss:  0.4432520866394043
train gradient:  0.10491898655076777
iteration : 8375
train acc:  0.734375
train loss:  0.4910534918308258
train gradient:  0.1279790691967695
iteration : 8376
train acc:  0.734375
train loss:  0.4780537486076355
train gradient:  0.13034656582685702
iteration : 8377
train acc:  0.765625
train loss:  0.4691992998123169
train gradient:  0.13405992125023003
iteration : 8378
train acc:  0.75
train loss:  0.49295327067375183
train gradient:  0.14066588084255513
iteration : 8379
train acc:  0.7578125
train loss:  0.49798423051834106
train gradient:  0.13756030490005206
iteration : 8380
train acc:  0.6875
train loss:  0.5529640913009644
train gradient:  0.17296768628302775
iteration : 8381
train acc:  0.8125
train loss:  0.4067177474498749
train gradient:  0.10840744324864086
iteration : 8382
train acc:  0.71875
train loss:  0.47682517766952515
train gradient:  0.14511117976423543
iteration : 8383
train acc:  0.7890625
train loss:  0.41413557529449463
train gradient:  0.087378478697456
iteration : 8384
train acc:  0.71875
train loss:  0.5495558977127075
train gradient:  0.16325674558786546
iteration : 8385
train acc:  0.671875
train loss:  0.5247700214385986
train gradient:  0.1451065263553849
iteration : 8386
train acc:  0.796875
train loss:  0.44275158643722534
train gradient:  0.12742207169364705
iteration : 8387
train acc:  0.8046875
train loss:  0.486867219209671
train gradient:  0.1422305687270788
iteration : 8388
train acc:  0.703125
train loss:  0.5038453936576843
train gradient:  0.15142012288290174
iteration : 8389
train acc:  0.6875
train loss:  0.5351560115814209
train gradient:  0.16300283544969746
iteration : 8390
train acc:  0.7890625
train loss:  0.43777889013290405
train gradient:  0.1022422635614649
iteration : 8391
train acc:  0.765625
train loss:  0.4609382152557373
train gradient:  0.11953999354418789
iteration : 8392
train acc:  0.7578125
train loss:  0.48300886154174805
train gradient:  0.12064789422900926
iteration : 8393
train acc:  0.7265625
train loss:  0.5096665620803833
train gradient:  0.14043357892444178
iteration : 8394
train acc:  0.7109375
train loss:  0.45636415481567383
train gradient:  0.11601961713571254
iteration : 8395
train acc:  0.7421875
train loss:  0.5064565539360046
train gradient:  0.1288208981644246
iteration : 8396
train acc:  0.734375
train loss:  0.5172395706176758
train gradient:  0.165972464451796
iteration : 8397
train acc:  0.7890625
train loss:  0.46651554107666016
train gradient:  0.1078976534995513
iteration : 8398
train acc:  0.7578125
train loss:  0.42955759167671204
train gradient:  0.09515196455228689
iteration : 8399
train acc:  0.78125
train loss:  0.4450945258140564
train gradient:  0.14149485702730574
iteration : 8400
train acc:  0.6875
train loss:  0.533899188041687
train gradient:  0.17588621525255327
iteration : 8401
train acc:  0.6484375
train loss:  0.5796759128570557
train gradient:  0.1590923574997249
iteration : 8402
train acc:  0.78125
train loss:  0.4627859592437744
train gradient:  0.14581319185244873
iteration : 8403
train acc:  0.75
train loss:  0.5006359219551086
train gradient:  0.11781557415914247
iteration : 8404
train acc:  0.7734375
train loss:  0.45832085609436035
train gradient:  0.14184222444555555
iteration : 8405
train acc:  0.6953125
train loss:  0.5358887314796448
train gradient:  0.1588597497595805
iteration : 8406
train acc:  0.734375
train loss:  0.5299306511878967
train gradient:  0.1722213402257869
iteration : 8407
train acc:  0.7109375
train loss:  0.547798216342926
train gradient:  0.17495482686869632
iteration : 8408
train acc:  0.7265625
train loss:  0.5232990980148315
train gradient:  0.14751013453010473
iteration : 8409
train acc:  0.8515625
train loss:  0.3814173936843872
train gradient:  0.093097666010694
iteration : 8410
train acc:  0.734375
train loss:  0.5239993333816528
train gradient:  0.1419547523124749
iteration : 8411
train acc:  0.734375
train loss:  0.528344452381134
train gradient:  0.1497155668459626
iteration : 8412
train acc:  0.75
train loss:  0.49010521173477173
train gradient:  0.13797980490203754
iteration : 8413
train acc:  0.7265625
train loss:  0.5255820155143738
train gradient:  0.14504288494684553
iteration : 8414
train acc:  0.734375
train loss:  0.49900415539741516
train gradient:  0.13018912512145464
iteration : 8415
train acc:  0.7890625
train loss:  0.4650823473930359
train gradient:  0.14587259026409682
iteration : 8416
train acc:  0.703125
train loss:  0.5014349222183228
train gradient:  0.13749829257339938
iteration : 8417
train acc:  0.796875
train loss:  0.42330271005630493
train gradient:  0.10944481911637693
iteration : 8418
train acc:  0.765625
train loss:  0.5195986032485962
train gradient:  0.1255517739814156
iteration : 8419
train acc:  0.6875
train loss:  0.581006646156311
train gradient:  0.1543490574118141
iteration : 8420
train acc:  0.78125
train loss:  0.4665524959564209
train gradient:  0.12791650658434944
iteration : 8421
train acc:  0.7734375
train loss:  0.4888904094696045
train gradient:  0.10690681083409426
iteration : 8422
train acc:  0.78125
train loss:  0.4524793028831482
train gradient:  0.13563233978680084
iteration : 8423
train acc:  0.78125
train loss:  0.42600104212760925
train gradient:  0.10333339471043035
iteration : 8424
train acc:  0.71875
train loss:  0.4851398468017578
train gradient:  0.13393699352755925
iteration : 8425
train acc:  0.703125
train loss:  0.509408175945282
train gradient:  0.14832159490437102
iteration : 8426
train acc:  0.7578125
train loss:  0.5097180604934692
train gradient:  0.13703045270536318
iteration : 8427
train acc:  0.6640625
train loss:  0.5420373678207397
train gradient:  0.12637423072092224
iteration : 8428
train acc:  0.7734375
train loss:  0.46935081481933594
train gradient:  0.1152363031396565
iteration : 8429
train acc:  0.7421875
train loss:  0.4992247223854065
train gradient:  0.1310907990550679
iteration : 8430
train acc:  0.6953125
train loss:  0.5368862748146057
train gradient:  0.15697500151246024
iteration : 8431
train acc:  0.7890625
train loss:  0.46652358770370483
train gradient:  0.12971906230813574
iteration : 8432
train acc:  0.796875
train loss:  0.4601294994354248
train gradient:  0.11447859158653112
iteration : 8433
train acc:  0.703125
train loss:  0.590802788734436
train gradient:  0.2301570318730165
iteration : 8434
train acc:  0.6484375
train loss:  0.6029551029205322
train gradient:  0.19016965150897244
iteration : 8435
train acc:  0.75
train loss:  0.46206462383270264
train gradient:  0.12480436262775164
iteration : 8436
train acc:  0.6796875
train loss:  0.5939805507659912
train gradient:  0.17908712760041345
iteration : 8437
train acc:  0.7265625
train loss:  0.5728424191474915
train gradient:  0.2247882954657015
iteration : 8438
train acc:  0.7265625
train loss:  0.517757773399353
train gradient:  0.1644616913622613
iteration : 8439
train acc:  0.734375
train loss:  0.4694156050682068
train gradient:  0.13606246357303586
iteration : 8440
train acc:  0.71875
train loss:  0.5100200176239014
train gradient:  0.15625319638801302
iteration : 8441
train acc:  0.765625
train loss:  0.44763296842575073
train gradient:  0.11267490339410594
iteration : 8442
train acc:  0.6953125
train loss:  0.6173192858695984
train gradient:  0.20432210055546135
iteration : 8443
train acc:  0.75
train loss:  0.4897530674934387
train gradient:  0.1287040817200154
iteration : 8444
train acc:  0.7265625
train loss:  0.5018656253814697
train gradient:  0.12577745460360462
iteration : 8445
train acc:  0.671875
train loss:  0.6419708728790283
train gradient:  0.24566391818895789
iteration : 8446
train acc:  0.796875
train loss:  0.45244598388671875
train gradient:  0.11887071430111538
iteration : 8447
train acc:  0.734375
train loss:  0.5283221006393433
train gradient:  0.1576963252616853
iteration : 8448
train acc:  0.734375
train loss:  0.511951744556427
train gradient:  0.13722359713799615
iteration : 8449
train acc:  0.7421875
train loss:  0.49784573912620544
train gradient:  0.1072938843418996
iteration : 8450
train acc:  0.78125
train loss:  0.4807785749435425
train gradient:  0.1448956823089307
iteration : 8451
train acc:  0.8046875
train loss:  0.4475313127040863
train gradient:  0.1209105374636585
iteration : 8452
train acc:  0.6796875
train loss:  0.5735865235328674
train gradient:  0.18769248328093857
iteration : 8453
train acc:  0.7734375
train loss:  0.4772663712501526
train gradient:  0.13866346063784873
iteration : 8454
train acc:  0.7578125
train loss:  0.49947723746299744
train gradient:  0.13718623557751022
iteration : 8455
train acc:  0.703125
train loss:  0.5519100427627563
train gradient:  0.1509095150294013
iteration : 8456
train acc:  0.75
train loss:  0.4717082381248474
train gradient:  0.11739396750484128
iteration : 8457
train acc:  0.7421875
train loss:  0.49986934661865234
train gradient:  0.1643505201464926
iteration : 8458
train acc:  0.7421875
train loss:  0.528530478477478
train gradient:  0.18315190991739588
iteration : 8459
train acc:  0.765625
train loss:  0.46166789531707764
train gradient:  0.13455290214267426
iteration : 8460
train acc:  0.6875
train loss:  0.5552412271499634
train gradient:  0.1638019046443953
iteration : 8461
train acc:  0.7109375
train loss:  0.5407675504684448
train gradient:  0.1541643517787722
iteration : 8462
train acc:  0.71875
train loss:  0.483737051486969
train gradient:  0.12434476401307205
iteration : 8463
train acc:  0.7578125
train loss:  0.48606669902801514
train gradient:  0.11951767657051894
iteration : 8464
train acc:  0.6875
train loss:  0.5461393594741821
train gradient:  0.18200931457278707
iteration : 8465
train acc:  0.8046875
train loss:  0.40501710772514343
train gradient:  0.09608960208965707
iteration : 8466
train acc:  0.765625
train loss:  0.46174323558807373
train gradient:  0.12891030565365846
iteration : 8467
train acc:  0.7734375
train loss:  0.45933717489242554
train gradient:  0.1258694293499788
iteration : 8468
train acc:  0.7109375
train loss:  0.5140782594680786
train gradient:  0.14069857862638346
iteration : 8469
train acc:  0.71875
train loss:  0.5204235315322876
train gradient:  0.15322863833280798
iteration : 8470
train acc:  0.734375
train loss:  0.47571513056755066
train gradient:  0.10691478818877187
iteration : 8471
train acc:  0.828125
train loss:  0.4180164635181427
train gradient:  0.13303502291382752
iteration : 8472
train acc:  0.7109375
train loss:  0.5527858734130859
train gradient:  0.16978061833419356
iteration : 8473
train acc:  0.765625
train loss:  0.4526749849319458
train gradient:  0.12763423937942478
iteration : 8474
train acc:  0.71875
train loss:  0.48263558745384216
train gradient:  0.12201966460078358
iteration : 8475
train acc:  0.7578125
train loss:  0.49844062328338623
train gradient:  0.13821383781519989
iteration : 8476
train acc:  0.75
train loss:  0.5165237784385681
train gradient:  0.13638443761095143
iteration : 8477
train acc:  0.7109375
train loss:  0.5851290225982666
train gradient:  0.1865953288546312
iteration : 8478
train acc:  0.8125
train loss:  0.4550212025642395
train gradient:  0.1064021898181123
iteration : 8479
train acc:  0.78125
train loss:  0.44681018590927124
train gradient:  0.11372089508562248
iteration : 8480
train acc:  0.75
train loss:  0.4717947542667389
train gradient:  0.14498583318641528
iteration : 8481
train acc:  0.8125
train loss:  0.4366617798805237
train gradient:  0.10471419154612531
iteration : 8482
train acc:  0.8046875
train loss:  0.44303441047668457
train gradient:  0.104609193199971
iteration : 8483
train acc:  0.7421875
train loss:  0.48174649477005005
train gradient:  0.14014175105515925
iteration : 8484
train acc:  0.6953125
train loss:  0.508476734161377
train gradient:  0.11526663929635296
iteration : 8485
train acc:  0.78125
train loss:  0.5009217262268066
train gradient:  0.13042396321889416
iteration : 8486
train acc:  0.7265625
train loss:  0.5517505407333374
train gradient:  0.12518884858772214
iteration : 8487
train acc:  0.78125
train loss:  0.4735642075538635
train gradient:  0.12053012587613125
iteration : 8488
train acc:  0.78125
train loss:  0.4811099171638489
train gradient:  0.12239664461769321
iteration : 8489
train acc:  0.734375
train loss:  0.5042177438735962
train gradient:  0.13250300640824147
iteration : 8490
train acc:  0.7421875
train loss:  0.4836047291755676
train gradient:  0.11992686434070568
iteration : 8491
train acc:  0.765625
train loss:  0.512700617313385
train gradient:  0.13778152126598825
iteration : 8492
train acc:  0.7578125
train loss:  0.48141399025917053
train gradient:  0.09845864643794207
iteration : 8493
train acc:  0.8046875
train loss:  0.4368412494659424
train gradient:  0.11515041517366327
iteration : 8494
train acc:  0.7578125
train loss:  0.4945843815803528
train gradient:  0.1356595204632215
iteration : 8495
train acc:  0.7734375
train loss:  0.43877464532852173
train gradient:  0.10301446985657832
iteration : 8496
train acc:  0.71875
train loss:  0.4838976263999939
train gradient:  0.1151087428161503
iteration : 8497
train acc:  0.75
train loss:  0.5212981700897217
train gradient:  0.12143861502895029
iteration : 8498
train acc:  0.765625
train loss:  0.4653317928314209
train gradient:  0.12157158200751088
iteration : 8499
train acc:  0.78125
train loss:  0.4783207178115845
train gradient:  0.11990347616796282
iteration : 8500
train acc:  0.75
train loss:  0.504473865032196
train gradient:  0.1198980025408083
iteration : 8501
train acc:  0.75
train loss:  0.4961147606372833
train gradient:  0.10346854335837333
iteration : 8502
train acc:  0.71875
train loss:  0.5703679323196411
train gradient:  0.17416558794019144
iteration : 8503
train acc:  0.7109375
train loss:  0.4942045509815216
train gradient:  0.13021073299142394
iteration : 8504
train acc:  0.765625
train loss:  0.5143561959266663
train gradient:  0.1568223388262866
iteration : 8505
train acc:  0.6953125
train loss:  0.5279830694198608
train gradient:  0.16536523355789007
iteration : 8506
train acc:  0.75
train loss:  0.5012420415878296
train gradient:  0.1184578426620333
iteration : 8507
train acc:  0.7421875
train loss:  0.48500490188598633
train gradient:  0.11857767312337851
iteration : 8508
train acc:  0.78125
train loss:  0.48753684759140015
train gradient:  0.1091515887819735
iteration : 8509
train acc:  0.7578125
train loss:  0.5480403304100037
train gradient:  0.13994959577047195
iteration : 8510
train acc:  0.7578125
train loss:  0.4510233998298645
train gradient:  0.11386062930862062
iteration : 8511
train acc:  0.7578125
train loss:  0.4950675070285797
train gradient:  0.15883474349966636
iteration : 8512
train acc:  0.71875
train loss:  0.5048881769180298
train gradient:  0.14945394785059518
iteration : 8513
train acc:  0.765625
train loss:  0.47964051365852356
train gradient:  0.17801173586691305
iteration : 8514
train acc:  0.78125
train loss:  0.44936567544937134
train gradient:  0.10601391865502158
iteration : 8515
train acc:  0.8046875
train loss:  0.4532548785209656
train gradient:  0.15079294298607288
iteration : 8516
train acc:  0.75
train loss:  0.46950727701187134
train gradient:  0.13526272090589264
iteration : 8517
train acc:  0.6953125
train loss:  0.5096796751022339
train gradient:  0.14224301996848626
iteration : 8518
train acc:  0.8359375
train loss:  0.4173174500465393
train gradient:  0.08881265006185338
iteration : 8519
train acc:  0.796875
train loss:  0.43006432056427
train gradient:  0.09245531489268434
iteration : 8520
train acc:  0.65625
train loss:  0.5391398072242737
train gradient:  0.12986459448338086
iteration : 8521
train acc:  0.765625
train loss:  0.48548609018325806
train gradient:  0.11579951688875119
iteration : 8522
train acc:  0.7109375
train loss:  0.49258196353912354
train gradient:  0.12466327401436676
iteration : 8523
train acc:  0.6953125
train loss:  0.5763088464736938
train gradient:  0.14861609044305385
iteration : 8524
train acc:  0.8125
train loss:  0.468606173992157
train gradient:  0.11885977634273205
iteration : 8525
train acc:  0.6796875
train loss:  0.5297170877456665
train gradient:  0.1357735393855024
iteration : 8526
train acc:  0.8046875
train loss:  0.4500119686126709
train gradient:  0.12016126020674871
iteration : 8527
train acc:  0.6484375
train loss:  0.5447981357574463
train gradient:  0.18122229697295228
iteration : 8528
train acc:  0.7890625
train loss:  0.430269718170166
train gradient:  0.12595797519534435
iteration : 8529
train acc:  0.75
train loss:  0.4547688663005829
train gradient:  0.09405099089724328
iteration : 8530
train acc:  0.71875
train loss:  0.6008328199386597
train gradient:  0.23388312113666404
iteration : 8531
train acc:  0.828125
train loss:  0.3988393545150757
train gradient:  0.07507231370240657
iteration : 8532
train acc:  0.7109375
train loss:  0.5325816869735718
train gradient:  0.1571619057054488
iteration : 8533
train acc:  0.671875
train loss:  0.5615957975387573
train gradient:  0.14476184222383098
iteration : 8534
train acc:  0.703125
train loss:  0.5344901084899902
train gradient:  0.1622001292110023
iteration : 8535
train acc:  0.7421875
train loss:  0.5267679691314697
train gradient:  0.17603744555731704
iteration : 8536
train acc:  0.671875
train loss:  0.5731492042541504
train gradient:  0.19291230406858473
iteration : 8537
train acc:  0.78125
train loss:  0.48528894782066345
train gradient:  0.1279722220421588
iteration : 8538
train acc:  0.7109375
train loss:  0.5597786903381348
train gradient:  0.1468931405810185
iteration : 8539
train acc:  0.78125
train loss:  0.46745485067367554
train gradient:  0.12452490966832333
iteration : 8540
train acc:  0.7421875
train loss:  0.510300874710083
train gradient:  0.1142802516432512
iteration : 8541
train acc:  0.7421875
train loss:  0.4537145793437958
train gradient:  0.09901041373324174
iteration : 8542
train acc:  0.6953125
train loss:  0.5461125373840332
train gradient:  0.17890620726275214
iteration : 8543
train acc:  0.75
train loss:  0.4877534806728363
train gradient:  0.13955265218412138
iteration : 8544
train acc:  0.7421875
train loss:  0.548340916633606
train gradient:  0.21273941665885043
iteration : 8545
train acc:  0.765625
train loss:  0.4777476191520691
train gradient:  0.14764615703277034
iteration : 8546
train acc:  0.7578125
train loss:  0.5100914239883423
train gradient:  0.14071592977331865
iteration : 8547
train acc:  0.703125
train loss:  0.5180419087409973
train gradient:  0.13255824994328658
iteration : 8548
train acc:  0.734375
train loss:  0.4651024341583252
train gradient:  0.11795094083592586
iteration : 8549
train acc:  0.703125
train loss:  0.4969051778316498
train gradient:  0.12705306030441751
iteration : 8550
train acc:  0.6953125
train loss:  0.561226487159729
train gradient:  0.16269104073246238
iteration : 8551
train acc:  0.78125
train loss:  0.4978765547275543
train gradient:  0.11815847724413074
iteration : 8552
train acc:  0.75
train loss:  0.47624140977859497
train gradient:  0.11638487396224993
iteration : 8553
train acc:  0.765625
train loss:  0.4842188358306885
train gradient:  0.1536325558762464
iteration : 8554
train acc:  0.75
train loss:  0.5401318669319153
train gradient:  0.16166229125360865
iteration : 8555
train acc:  0.734375
train loss:  0.5448395609855652
train gradient:  0.14977601546866987
iteration : 8556
train acc:  0.7265625
train loss:  0.5038620233535767
train gradient:  0.12963310526220284
iteration : 8557
train acc:  0.734375
train loss:  0.46948057413101196
train gradient:  0.13543530356187977
iteration : 8558
train acc:  0.7578125
train loss:  0.46927136182785034
train gradient:  0.12046178970641645
iteration : 8559
train acc:  0.7265625
train loss:  0.4976204037666321
train gradient:  0.14452092178700524
iteration : 8560
train acc:  0.7265625
train loss:  0.49409961700439453
train gradient:  0.1406298854696445
iteration : 8561
train acc:  0.7421875
train loss:  0.5479729175567627
train gradient:  0.17792206623704263
iteration : 8562
train acc:  0.78125
train loss:  0.4590427875518799
train gradient:  0.13113143605331043
iteration : 8563
train acc:  0.7578125
train loss:  0.4780137836933136
train gradient:  0.1207232752423287
iteration : 8564
train acc:  0.765625
train loss:  0.478201687335968
train gradient:  0.10956144755902754
iteration : 8565
train acc:  0.6953125
train loss:  0.5520807504653931
train gradient:  0.14389920195213052
iteration : 8566
train acc:  0.7734375
train loss:  0.42886340618133545
train gradient:  0.11635576649707527
iteration : 8567
train acc:  0.7578125
train loss:  0.4567800760269165
train gradient:  0.13063565676450406
iteration : 8568
train acc:  0.671875
train loss:  0.5802850723266602
train gradient:  0.17796966972262585
iteration : 8569
train acc:  0.65625
train loss:  0.6084560751914978
train gradient:  0.19125983262126298
iteration : 8570
train acc:  0.703125
train loss:  0.5230907797813416
train gradient:  0.16800250067421474
iteration : 8571
train acc:  0.6875
train loss:  0.5654820203781128
train gradient:  0.18236889766769815
iteration : 8572
train acc:  0.8125
train loss:  0.4313429594039917
train gradient:  0.09522626192076868
iteration : 8573
train acc:  0.7578125
train loss:  0.4910740256309509
train gradient:  0.14125051196469884
iteration : 8574
train acc:  0.8125
train loss:  0.4237927496433258
train gradient:  0.10347067009922051
iteration : 8575
train acc:  0.703125
train loss:  0.5247853398323059
train gradient:  0.15607518168240947
iteration : 8576
train acc:  0.78125
train loss:  0.47043853998184204
train gradient:  0.1273921351627537
iteration : 8577
train acc:  0.6875
train loss:  0.5123215317726135
train gradient:  0.20721830081964437
iteration : 8578
train acc:  0.78125
train loss:  0.4242674708366394
train gradient:  0.10413019524560577
iteration : 8579
train acc:  0.7734375
train loss:  0.45571663975715637
train gradient:  0.11961580339869786
iteration : 8580
train acc:  0.796875
train loss:  0.44994547963142395
train gradient:  0.11192607606454807
iteration : 8581
train acc:  0.734375
train loss:  0.5377355813980103
train gradient:  0.13157783333973094
iteration : 8582
train acc:  0.7421875
train loss:  0.48833373188972473
train gradient:  0.1482932739907717
iteration : 8583
train acc:  0.7265625
train loss:  0.5058344006538391
train gradient:  0.11859558519524306
iteration : 8584
train acc:  0.7109375
train loss:  0.5685403347015381
train gradient:  0.17240369750282183
iteration : 8585
train acc:  0.7109375
train loss:  0.5701162219047546
train gradient:  0.17776550897383941
iteration : 8586
train acc:  0.71875
train loss:  0.5572587251663208
train gradient:  0.13981400107582923
iteration : 8587
train acc:  0.7734375
train loss:  0.45561885833740234
train gradient:  0.09883281007301593
iteration : 8588
train acc:  0.7421875
train loss:  0.5411959886550903
train gradient:  0.16157517174013847
iteration : 8589
train acc:  0.6796875
train loss:  0.5838161706924438
train gradient:  0.20714168119934978
iteration : 8590
train acc:  0.7421875
train loss:  0.5008741617202759
train gradient:  0.1269301226672919
iteration : 8591
train acc:  0.71875
train loss:  0.5095144510269165
train gradient:  0.11967942018470476
iteration : 8592
train acc:  0.75
train loss:  0.4603781998157501
train gradient:  0.11474042636006554
iteration : 8593
train acc:  0.765625
train loss:  0.4618791341781616
train gradient:  0.12483537312172477
iteration : 8594
train acc:  0.7734375
train loss:  0.4631544351577759
train gradient:  0.11341815591564629
iteration : 8595
train acc:  0.78125
train loss:  0.43958860635757446
train gradient:  0.11234582154419241
iteration : 8596
train acc:  0.7265625
train loss:  0.49058061838150024
train gradient:  0.12164503899112787
iteration : 8597
train acc:  0.75
train loss:  0.5239320397377014
train gradient:  0.15308683296026818
iteration : 8598
train acc:  0.7578125
train loss:  0.4822050631046295
train gradient:  0.13396883138413312
iteration : 8599
train acc:  0.6875
train loss:  0.5762572288513184
train gradient:  0.16907492879975872
iteration : 8600
train acc:  0.7421875
train loss:  0.5147978663444519
train gradient:  0.12772351935643056
iteration : 8601
train acc:  0.734375
train loss:  0.4857102036476135
train gradient:  0.1657917916721327
iteration : 8602
train acc:  0.6953125
train loss:  0.5357476472854614
train gradient:  0.19132749754569728
iteration : 8603
train acc:  0.7890625
train loss:  0.45654064416885376
train gradient:  0.121709773115775
iteration : 8604
train acc:  0.765625
train loss:  0.4335229694843292
train gradient:  0.1397497878837204
iteration : 8605
train acc:  0.703125
train loss:  0.5680102705955505
train gradient:  0.16137176564776734
iteration : 8606
train acc:  0.7265625
train loss:  0.5191612243652344
train gradient:  0.12947738909954523
iteration : 8607
train acc:  0.7421875
train loss:  0.5143706798553467
train gradient:  0.1463381888048967
iteration : 8608
train acc:  0.7109375
train loss:  0.5329537987709045
train gradient:  0.12728409607818877
iteration : 8609
train acc:  0.7578125
train loss:  0.5144913792610168
train gradient:  0.13152666002491714
iteration : 8610
train acc:  0.78125
train loss:  0.4535181522369385
train gradient:  0.12106480590538218
iteration : 8611
train acc:  0.7890625
train loss:  0.47109371423721313
train gradient:  0.1370882630219648
iteration : 8612
train acc:  0.734375
train loss:  0.48345139622688293
train gradient:  0.1251420490588807
iteration : 8613
train acc:  0.7578125
train loss:  0.4689880907535553
train gradient:  0.15772066926909403
iteration : 8614
train acc:  0.7734375
train loss:  0.46919310092926025
train gradient:  0.13583386606499778
iteration : 8615
train acc:  0.7265625
train loss:  0.5374743938446045
train gradient:  0.1262850543469392
iteration : 8616
train acc:  0.8203125
train loss:  0.4500444531440735
train gradient:  0.1306398688179004
iteration : 8617
train acc:  0.7734375
train loss:  0.5074983239173889
train gradient:  0.14849306955365743
iteration : 8618
train acc:  0.671875
train loss:  0.6072586178779602
train gradient:  0.20135512811490316
iteration : 8619
train acc:  0.671875
train loss:  0.5378848314285278
train gradient:  0.1685099969380774
iteration : 8620
train acc:  0.71875
train loss:  0.553708553314209
train gradient:  0.13117970474886143
iteration : 8621
train acc:  0.765625
train loss:  0.49591153860092163
train gradient:  0.1358225761673904
iteration : 8622
train acc:  0.765625
train loss:  0.5075989961624146
train gradient:  0.15960584416987256
iteration : 8623
train acc:  0.7421875
train loss:  0.4989017844200134
train gradient:  0.107063709756901
iteration : 8624
train acc:  0.6953125
train loss:  0.4949105679988861
train gradient:  0.12549258895192728
iteration : 8625
train acc:  0.734375
train loss:  0.46020615100860596
train gradient:  0.10844908565674757
iteration : 8626
train acc:  0.703125
train loss:  0.5196738243103027
train gradient:  0.14484504671120937
iteration : 8627
train acc:  0.765625
train loss:  0.4621700048446655
train gradient:  0.1352225687822162
iteration : 8628
train acc:  0.7265625
train loss:  0.580349326133728
train gradient:  0.19382707120729525
iteration : 8629
train acc:  0.71875
train loss:  0.5220276713371277
train gradient:  0.14812126667338188
iteration : 8630
train acc:  0.6796875
train loss:  0.6193321943283081
train gradient:  0.22590805839938974
iteration : 8631
train acc:  0.765625
train loss:  0.5075472593307495
train gradient:  0.12007894810323301
iteration : 8632
train acc:  0.796875
train loss:  0.43112778663635254
train gradient:  0.16707550985945696
iteration : 8633
train acc:  0.703125
train loss:  0.540912389755249
train gradient:  0.16030800296245434
iteration : 8634
train acc:  0.71875
train loss:  0.5195873975753784
train gradient:  0.18580091246528016
iteration : 8635
train acc:  0.7734375
train loss:  0.4294910430908203
train gradient:  0.10992781449089245
iteration : 8636
train acc:  0.75
train loss:  0.4696807265281677
train gradient:  0.14312958305586473
iteration : 8637
train acc:  0.6875
train loss:  0.5331396460533142
train gradient:  0.13139706068564083
iteration : 8638
train acc:  0.75
train loss:  0.5101475715637207
train gradient:  0.11790347866873246
iteration : 8639
train acc:  0.7578125
train loss:  0.4410666823387146
train gradient:  0.10716291421161299
iteration : 8640
train acc:  0.71875
train loss:  0.48621290922164917
train gradient:  0.11963058967634145
iteration : 8641
train acc:  0.7421875
train loss:  0.502007007598877
train gradient:  0.1288756034620139
iteration : 8642
train acc:  0.7265625
train loss:  0.5175274610519409
train gradient:  0.13448420026922586
iteration : 8643
train acc:  0.7109375
train loss:  0.5309397578239441
train gradient:  0.1570941541281214
iteration : 8644
train acc:  0.734375
train loss:  0.5448453426361084
train gradient:  0.15764275185764556
iteration : 8645
train acc:  0.7890625
train loss:  0.4587981700897217
train gradient:  0.10532736685251663
iteration : 8646
train acc:  0.734375
train loss:  0.5173357725143433
train gradient:  0.12998020453321124
iteration : 8647
train acc:  0.7265625
train loss:  0.5128383636474609
train gradient:  0.13387384531391733
iteration : 8648
train acc:  0.703125
train loss:  0.5067005157470703
train gradient:  0.12106947391748012
iteration : 8649
train acc:  0.703125
train loss:  0.5377969145774841
train gradient:  0.1705488053415991
iteration : 8650
train acc:  0.765625
train loss:  0.49846941232681274
train gradient:  0.15924815166978357
iteration : 8651
train acc:  0.6875
train loss:  0.5342121720314026
train gradient:  0.20246183769718215
iteration : 8652
train acc:  0.765625
train loss:  0.5034394264221191
train gradient:  0.1341753533795047
iteration : 8653
train acc:  0.7890625
train loss:  0.42492255568504333
train gradient:  0.10374800113904371
iteration : 8654
train acc:  0.7109375
train loss:  0.558661937713623
train gradient:  0.1504213544106727
iteration : 8655
train acc:  0.7734375
train loss:  0.46327173709869385
train gradient:  0.11127835455283197
iteration : 8656
train acc:  0.7265625
train loss:  0.4826546609401703
train gradient:  0.12419223712208412
iteration : 8657
train acc:  0.8359375
train loss:  0.42516186833381653
train gradient:  0.0935492162973293
iteration : 8658
train acc:  0.7734375
train loss:  0.5153290033340454
train gradient:  0.1459105504502216
iteration : 8659
train acc:  0.7421875
train loss:  0.5282604694366455
train gradient:  0.17548767681055466
iteration : 8660
train acc:  0.7421875
train loss:  0.49762940406799316
train gradient:  0.13931789387503823
iteration : 8661
train acc:  0.75
train loss:  0.47481945157051086
train gradient:  0.11782090053230433
iteration : 8662
train acc:  0.703125
train loss:  0.546359658241272
train gradient:  0.1986808256120126
iteration : 8663
train acc:  0.7265625
train loss:  0.5218839645385742
train gradient:  0.17581267913064896
iteration : 8664
train acc:  0.734375
train loss:  0.5049320459365845
train gradient:  0.12833278873695397
iteration : 8665
train acc:  0.703125
train loss:  0.4964381158351898
train gradient:  0.1198992361644023
iteration : 8666
train acc:  0.75
train loss:  0.5048560500144958
train gradient:  0.12860280343095448
iteration : 8667
train acc:  0.78125
train loss:  0.440476655960083
train gradient:  0.14496403897102594
iteration : 8668
train acc:  0.8125
train loss:  0.4401363730430603
train gradient:  0.09995484110086975
iteration : 8669
train acc:  0.6875
train loss:  0.5653451681137085
train gradient:  0.1269705522200626
iteration : 8670
train acc:  0.71875
train loss:  0.5269798040390015
train gradient:  0.1325475543575198
iteration : 8671
train acc:  0.7421875
train loss:  0.5127643942832947
train gradient:  0.14517335834630224
iteration : 8672
train acc:  0.7421875
train loss:  0.48008716106414795
train gradient:  0.1767482534410723
iteration : 8673
train acc:  0.765625
train loss:  0.4732644259929657
train gradient:  0.13603186333176392
iteration : 8674
train acc:  0.75
train loss:  0.4936677813529968
train gradient:  0.15761256551474911
iteration : 8675
train acc:  0.71875
train loss:  0.5608384609222412
train gradient:  0.19971526244112514
iteration : 8676
train acc:  0.7265625
train loss:  0.5148751735687256
train gradient:  0.13253321711396554
iteration : 8677
train acc:  0.7578125
train loss:  0.4941598176956177
train gradient:  0.11849579563200478
iteration : 8678
train acc:  0.75
train loss:  0.5065113306045532
train gradient:  0.12207020773929866
iteration : 8679
train acc:  0.734375
train loss:  0.47660407423973083
train gradient:  0.12142893651406975
iteration : 8680
train acc:  0.765625
train loss:  0.49531567096710205
train gradient:  0.12690087146360385
iteration : 8681
train acc:  0.6953125
train loss:  0.5551592111587524
train gradient:  0.1616073227591746
iteration : 8682
train acc:  0.75
train loss:  0.48794493079185486
train gradient:  0.11840056258454122
iteration : 8683
train acc:  0.765625
train loss:  0.47871190309524536
train gradient:  0.13394675464752087
iteration : 8684
train acc:  0.734375
train loss:  0.5371370315551758
train gradient:  0.15928426175107546
iteration : 8685
train acc:  0.71875
train loss:  0.47903716564178467
train gradient:  0.1369746865103244
iteration : 8686
train acc:  0.734375
train loss:  0.5388028621673584
train gradient:  0.1524811853092905
iteration : 8687
train acc:  0.7734375
train loss:  0.4592382311820984
train gradient:  0.11960675560249169
iteration : 8688
train acc:  0.703125
train loss:  0.5065512657165527
train gradient:  0.13895497320043237
iteration : 8689
train acc:  0.7109375
train loss:  0.5259907245635986
train gradient:  0.16008950305719832
iteration : 8690
train acc:  0.7109375
train loss:  0.5061578154563904
train gradient:  0.17346808545546571
iteration : 8691
train acc:  0.6328125
train loss:  0.6375182867050171
train gradient:  0.21343956154064442
iteration : 8692
train acc:  0.7578125
train loss:  0.41596078872680664
train gradient:  0.09668365023825984
iteration : 8693
train acc:  0.7109375
train loss:  0.5146310329437256
train gradient:  0.15782441958872637
iteration : 8694
train acc:  0.71875
train loss:  0.4760323762893677
train gradient:  0.11967122158855362
iteration : 8695
train acc:  0.7890625
train loss:  0.44905880093574524
train gradient:  0.10774303309885366
iteration : 8696
train acc:  0.71875
train loss:  0.5160872340202332
train gradient:  0.1299849623768883
iteration : 8697
train acc:  0.7890625
train loss:  0.4479740858078003
train gradient:  0.13878103597551628
iteration : 8698
train acc:  0.7734375
train loss:  0.45457756519317627
train gradient:  0.12793487557742397
iteration : 8699
train acc:  0.7578125
train loss:  0.4622266888618469
train gradient:  0.12808268655825747
iteration : 8700
train acc:  0.734375
train loss:  0.5389639139175415
train gradient:  0.13729271801489767
iteration : 8701
train acc:  0.734375
train loss:  0.5266532301902771
train gradient:  0.14629676116991416
iteration : 8702
train acc:  0.703125
train loss:  0.5925240516662598
train gradient:  0.17528234608269583
iteration : 8703
train acc:  0.7109375
train loss:  0.5887727737426758
train gradient:  0.17968608987550905
iteration : 8704
train acc:  0.734375
train loss:  0.5116652250289917
train gradient:  0.1796840900138304
iteration : 8705
train acc:  0.6953125
train loss:  0.5142138004302979
train gradient:  0.12135613019105551
iteration : 8706
train acc:  0.7265625
train loss:  0.49737831950187683
train gradient:  0.14550772202887674
iteration : 8707
train acc:  0.734375
train loss:  0.5337337255477905
train gradient:  0.14205454949552615
iteration : 8708
train acc:  0.75
train loss:  0.5020555257797241
train gradient:  0.12955874334567957
iteration : 8709
train acc:  0.765625
train loss:  0.4866195321083069
train gradient:  0.13913770679164572
iteration : 8710
train acc:  0.796875
train loss:  0.4581051170825958
train gradient:  0.1251707376172948
iteration : 8711
train acc:  0.7421875
train loss:  0.510796844959259
train gradient:  0.12859807966796008
iteration : 8712
train acc:  0.7421875
train loss:  0.5155618190765381
train gradient:  0.12670137569402742
iteration : 8713
train acc:  0.734375
train loss:  0.48915818333625793
train gradient:  0.14114043411747795
iteration : 8714
train acc:  0.734375
train loss:  0.5189478397369385
train gradient:  0.11870252068678801
iteration : 8715
train acc:  0.6953125
train loss:  0.5323206186294556
train gradient:  0.13138032958747037
iteration : 8716
train acc:  0.765625
train loss:  0.47016313672065735
train gradient:  0.11166222986149037
iteration : 8717
train acc:  0.7578125
train loss:  0.44221511483192444
train gradient:  0.10448818448590919
iteration : 8718
train acc:  0.765625
train loss:  0.49675092101097107
train gradient:  0.14816941714710596
iteration : 8719
train acc:  0.671875
train loss:  0.5462333559989929
train gradient:  0.17787050315002334
iteration : 8720
train acc:  0.75
train loss:  0.49579352140426636
train gradient:  0.1395031331850366
iteration : 8721
train acc:  0.6796875
train loss:  0.5703712105751038
train gradient:  0.18404579286647507
iteration : 8722
train acc:  0.765625
train loss:  0.48493027687072754
train gradient:  0.14220281880030067
iteration : 8723
train acc:  0.7109375
train loss:  0.5402024984359741
train gradient:  0.15114443558377655
iteration : 8724
train acc:  0.7265625
train loss:  0.5053359270095825
train gradient:  0.15759208912593348
iteration : 8725
train acc:  0.78125
train loss:  0.46097999811172485
train gradient:  0.14377782730086355
iteration : 8726
train acc:  0.7734375
train loss:  0.522793173789978
train gradient:  0.22974185440677516
iteration : 8727
train acc:  0.7265625
train loss:  0.5211496353149414
train gradient:  0.15724994654626656
iteration : 8728
train acc:  0.734375
train loss:  0.48312899470329285
train gradient:  0.11700035886951488
iteration : 8729
train acc:  0.6953125
train loss:  0.5233595371246338
train gradient:  0.16267822673082655
iteration : 8730
train acc:  0.765625
train loss:  0.4685156047344208
train gradient:  0.15071497130331812
iteration : 8731
train acc:  0.7265625
train loss:  0.4837135076522827
train gradient:  0.15877424689784142
iteration : 8732
train acc:  0.75
train loss:  0.4825374484062195
train gradient:  0.14530885113819347
iteration : 8733
train acc:  0.7421875
train loss:  0.46973174810409546
train gradient:  0.10838777191000692
iteration : 8734
train acc:  0.65625
train loss:  0.6300984621047974
train gradient:  0.16366301770993116
iteration : 8735
train acc:  0.71875
train loss:  0.5379910469055176
train gradient:  0.1543422888144449
iteration : 8736
train acc:  0.7421875
train loss:  0.46766558289527893
train gradient:  0.14842087003566837
iteration : 8737
train acc:  0.7421875
train loss:  0.47184523940086365
train gradient:  0.1038555161528581
iteration : 8738
train acc:  0.7734375
train loss:  0.4500916302204132
train gradient:  0.10541499869890457
iteration : 8739
train acc:  0.796875
train loss:  0.44084498286247253
train gradient:  0.09592072973829263
iteration : 8740
train acc:  0.7421875
train loss:  0.505415678024292
train gradient:  0.14455568689739737
iteration : 8741
train acc:  0.71875
train loss:  0.5000004172325134
train gradient:  0.15632651896523664
iteration : 8742
train acc:  0.75
train loss:  0.5117874145507812
train gradient:  0.12969291593225413
iteration : 8743
train acc:  0.7265625
train loss:  0.493586003780365
train gradient:  0.12242540632406688
iteration : 8744
train acc:  0.7109375
train loss:  0.5343478322029114
train gradient:  0.1374916821260078
iteration : 8745
train acc:  0.6640625
train loss:  0.5680495500564575
train gradient:  0.1500357732871883
iteration : 8746
train acc:  0.7109375
train loss:  0.5457679033279419
train gradient:  0.1454699867313186
iteration : 8747
train acc:  0.75
train loss:  0.4806908667087555
train gradient:  0.15556848860738567
iteration : 8748
train acc:  0.734375
train loss:  0.4947977662086487
train gradient:  0.14617982434096966
iteration : 8749
train acc:  0.734375
train loss:  0.46318143606185913
train gradient:  0.12081664100943507
iteration : 8750
train acc:  0.7421875
train loss:  0.4827386736869812
train gradient:  0.13291545178995376
iteration : 8751
train acc:  0.71875
train loss:  0.550960123538971
train gradient:  0.20295295029935304
iteration : 8752
train acc:  0.703125
train loss:  0.5229853987693787
train gradient:  0.11937238016270232
iteration : 8753
train acc:  0.7265625
train loss:  0.5278995037078857
train gradient:  0.15241648092060522
iteration : 8754
train acc:  0.75
train loss:  0.49570226669311523
train gradient:  0.148107582694153
iteration : 8755
train acc:  0.7109375
train loss:  0.4947751760482788
train gradient:  0.12579857837574906
iteration : 8756
train acc:  0.6953125
train loss:  0.5421332716941833
train gradient:  0.15683520663036588
iteration : 8757
train acc:  0.7734375
train loss:  0.521475613117218
train gradient:  0.13601589459247013
iteration : 8758
train acc:  0.75
train loss:  0.5090115070343018
train gradient:  0.13802852404748467
iteration : 8759
train acc:  0.734375
train loss:  0.5621029734611511
train gradient:  0.13208110854685018
iteration : 8760
train acc:  0.671875
train loss:  0.5846553444862366
train gradient:  0.14009468204752337
iteration : 8761
train acc:  0.7890625
train loss:  0.42634785175323486
train gradient:  0.10153073971532793
iteration : 8762
train acc:  0.8203125
train loss:  0.3942744731903076
train gradient:  0.11354524000710846
iteration : 8763
train acc:  0.7421875
train loss:  0.4988292455673218
train gradient:  0.1693720467467884
iteration : 8764
train acc:  0.7578125
train loss:  0.469767302274704
train gradient:  0.10481650882615838
iteration : 8765
train acc:  0.7265625
train loss:  0.5158818960189819
train gradient:  0.1640282251854997
iteration : 8766
train acc:  0.71875
train loss:  0.5269993543624878
train gradient:  0.12531349074918272
iteration : 8767
train acc:  0.75
train loss:  0.45728811621665955
train gradient:  0.09605252270397453
iteration : 8768
train acc:  0.7578125
train loss:  0.451174795627594
train gradient:  0.11068284190706311
iteration : 8769
train acc:  0.7265625
train loss:  0.5327160358428955
train gradient:  0.19849918380186476
iteration : 8770
train acc:  0.7734375
train loss:  0.495083749294281
train gradient:  0.1285103362314826
iteration : 8771
train acc:  0.796875
train loss:  0.4284115135669708
train gradient:  0.08787835970511482
iteration : 8772
train acc:  0.7578125
train loss:  0.49750474095344543
train gradient:  0.1607533601782541
iteration : 8773
train acc:  0.7265625
train loss:  0.5009373426437378
train gradient:  0.13256674671738283
iteration : 8774
train acc:  0.671875
train loss:  0.6101713180541992
train gradient:  0.2267312014151362
iteration : 8775
train acc:  0.765625
train loss:  0.49298256635665894
train gradient:  0.15172251409788529
iteration : 8776
train acc:  0.8046875
train loss:  0.44921407103538513
train gradient:  0.09947321237155078
iteration : 8777
train acc:  0.71875
train loss:  0.5338833332061768
train gradient:  0.18214282823096795
iteration : 8778
train acc:  0.7578125
train loss:  0.49804794788360596
train gradient:  0.15033668562053804
iteration : 8779
train acc:  0.734375
train loss:  0.49804842472076416
train gradient:  0.12575538007045886
iteration : 8780
train acc:  0.6875
train loss:  0.5184569358825684
train gradient:  0.11687229987095284
iteration : 8781
train acc:  0.8203125
train loss:  0.419264554977417
train gradient:  0.08400966684929133
iteration : 8782
train acc:  0.7421875
train loss:  0.5000913739204407
train gradient:  0.12914654649130308
iteration : 8783
train acc:  0.734375
train loss:  0.5253822803497314
train gradient:  0.15795542701121534
iteration : 8784
train acc:  0.7421875
train loss:  0.4616427719593048
train gradient:  0.10954136173982736
iteration : 8785
train acc:  0.75
train loss:  0.5034304261207581
train gradient:  0.1507046735973389
iteration : 8786
train acc:  0.703125
train loss:  0.5631014108657837
train gradient:  0.1955443967984905
iteration : 8787
train acc:  0.75
train loss:  0.4873286783695221
train gradient:  0.1170144119503446
iteration : 8788
train acc:  0.71875
train loss:  0.574673056602478
train gradient:  0.15442758538205237
iteration : 8789
train acc:  0.6953125
train loss:  0.5306028127670288
train gradient:  0.12913769841304257
iteration : 8790
train acc:  0.703125
train loss:  0.5662563443183899
train gradient:  0.14721983866493438
iteration : 8791
train acc:  0.734375
train loss:  0.5297279357910156
train gradient:  0.13657554413596146
iteration : 8792
train acc:  0.734375
train loss:  0.5285348296165466
train gradient:  0.12663995994569072
iteration : 8793
train acc:  0.765625
train loss:  0.477206826210022
train gradient:  0.11065029723174284
iteration : 8794
train acc:  0.703125
train loss:  0.5159449577331543
train gradient:  0.12783466969010396
iteration : 8795
train acc:  0.75
train loss:  0.5081788301467896
train gradient:  0.1313516783822733
iteration : 8796
train acc:  0.7265625
train loss:  0.5150497555732727
train gradient:  0.1431480937090076
iteration : 8797
train acc:  0.703125
train loss:  0.5747702121734619
train gradient:  0.1713010802439794
iteration : 8798
train acc:  0.75
train loss:  0.4769710302352905
train gradient:  0.15114573878269133
iteration : 8799
train acc:  0.71875
train loss:  0.4963502287864685
train gradient:  0.12437708911156325
iteration : 8800
train acc:  0.671875
train loss:  0.5717025995254517
train gradient:  0.17485630194815316
iteration : 8801
train acc:  0.7890625
train loss:  0.4275074303150177
train gradient:  0.09912012228250042
iteration : 8802
train acc:  0.765625
train loss:  0.4818028211593628
train gradient:  0.14343090073392417
iteration : 8803
train acc:  0.8203125
train loss:  0.4080039858818054
train gradient:  0.11558806161084403
iteration : 8804
train acc:  0.7421875
train loss:  0.4993385970592499
train gradient:  0.12713456161650843
iteration : 8805
train acc:  0.75
train loss:  0.4800395965576172
train gradient:  0.14221514186780831
iteration : 8806
train acc:  0.7578125
train loss:  0.4771815836429596
train gradient:  0.12465119062851433
iteration : 8807
train acc:  0.7421875
train loss:  0.4945063889026642
train gradient:  0.1221559032386448
iteration : 8808
train acc:  0.796875
train loss:  0.4443758428096771
train gradient:  0.10507561747544389
iteration : 8809
train acc:  0.7578125
train loss:  0.4871138334274292
train gradient:  0.15656444993575486
iteration : 8810
train acc:  0.765625
train loss:  0.46551811695098877
train gradient:  0.15041023084291338
iteration : 8811
train acc:  0.6875
train loss:  0.5549482703208923
train gradient:  0.2129982979734601
iteration : 8812
train acc:  0.765625
train loss:  0.4910704493522644
train gradient:  0.12256101552996314
iteration : 8813
train acc:  0.8046875
train loss:  0.4176204800605774
train gradient:  0.1265948861162347
iteration : 8814
train acc:  0.7578125
train loss:  0.4483792185783386
train gradient:  0.12511116660561614
iteration : 8815
train acc:  0.6796875
train loss:  0.5483748912811279
train gradient:  0.16767281416881127
iteration : 8816
train acc:  0.8046875
train loss:  0.4639893174171448
train gradient:  0.10595716388472312
iteration : 8817
train acc:  0.7421875
train loss:  0.48916634917259216
train gradient:  0.11407911786913229
iteration : 8818
train acc:  0.7890625
train loss:  0.4715006351470947
train gradient:  0.11045457572976736
iteration : 8819
train acc:  0.7421875
train loss:  0.507350742816925
train gradient:  0.125004314602694
iteration : 8820
train acc:  0.7890625
train loss:  0.4916388988494873
train gradient:  0.14816104610356695
iteration : 8821
train acc:  0.7109375
train loss:  0.5555633306503296
train gradient:  0.13206735665704764
iteration : 8822
train acc:  0.765625
train loss:  0.481728196144104
train gradient:  0.15225594936962009
iteration : 8823
train acc:  0.765625
train loss:  0.5019983053207397
train gradient:  0.12311267351681851
iteration : 8824
train acc:  0.6796875
train loss:  0.5335313081741333
train gradient:  0.16254972591600703
iteration : 8825
train acc:  0.78125
train loss:  0.46808740496635437
train gradient:  0.13574275888581788
iteration : 8826
train acc:  0.78125
train loss:  0.4231542646884918
train gradient:  0.10456338634562756
iteration : 8827
train acc:  0.6796875
train loss:  0.5469554662704468
train gradient:  0.1308326399995139
iteration : 8828
train acc:  0.71875
train loss:  0.525530993938446
train gradient:  0.1397633069189689
iteration : 8829
train acc:  0.6875
train loss:  0.5789394378662109
train gradient:  0.20578841428343758
iteration : 8830
train acc:  0.75
train loss:  0.4803350865840912
train gradient:  0.12480905535699625
iteration : 8831
train acc:  0.75
train loss:  0.5392336845397949
train gradient:  0.11802780988851819
iteration : 8832
train acc:  0.78125
train loss:  0.45650264620780945
train gradient:  0.10578439447038346
iteration : 8833
train acc:  0.75
train loss:  0.505917489528656
train gradient:  0.1354701515043962
iteration : 8834
train acc:  0.765625
train loss:  0.47469189763069153
train gradient:  0.12570519545882833
iteration : 8835
train acc:  0.765625
train loss:  0.4546443521976471
train gradient:  0.12508635150181577
iteration : 8836
train acc:  0.7734375
train loss:  0.48171502351760864
train gradient:  0.1404168383627215
iteration : 8837
train acc:  0.7578125
train loss:  0.46497276425361633
train gradient:  0.11061316200285978
iteration : 8838
train acc:  0.765625
train loss:  0.4719201922416687
train gradient:  0.14124893851993656
iteration : 8839
train acc:  0.7578125
train loss:  0.4515758752822876
train gradient:  0.11792882154395866
iteration : 8840
train acc:  0.7734375
train loss:  0.4757126569747925
train gradient:  0.12143327889480368
iteration : 8841
train acc:  0.734375
train loss:  0.5252780318260193
train gradient:  0.11863283659230785
iteration : 8842
train acc:  0.7109375
train loss:  0.48487919569015503
train gradient:  0.10800125908126312
iteration : 8843
train acc:  0.734375
train loss:  0.4682353138923645
train gradient:  0.11242855288691293
iteration : 8844
train acc:  0.75
train loss:  0.5389121770858765
train gradient:  0.1666511423059847
iteration : 8845
train acc:  0.7578125
train loss:  0.4968555271625519
train gradient:  0.13090514881619636
iteration : 8846
train acc:  0.7109375
train loss:  0.5208020210266113
train gradient:  0.11911606528813606
iteration : 8847
train acc:  0.75
train loss:  0.5335695743560791
train gradient:  0.13339409957493154
iteration : 8848
train acc:  0.8046875
train loss:  0.4284172058105469
train gradient:  0.1492102388759261
iteration : 8849
train acc:  0.6640625
train loss:  0.5816576480865479
train gradient:  0.16988428055731208
iteration : 8850
train acc:  0.6640625
train loss:  0.5177565813064575
train gradient:  0.13164997280652943
iteration : 8851
train acc:  0.6796875
train loss:  0.5457010269165039
train gradient:  0.14490771866102112
iteration : 8852
train acc:  0.7421875
train loss:  0.4787837266921997
train gradient:  0.12900905127426165
iteration : 8853
train acc:  0.765625
train loss:  0.44730672240257263
train gradient:  0.09911052586779494
iteration : 8854
train acc:  0.734375
train loss:  0.5259445905685425
train gradient:  0.14236268742695568
iteration : 8855
train acc:  0.75
train loss:  0.5169212818145752
train gradient:  0.14064023582171598
iteration : 8856
train acc:  0.7421875
train loss:  0.4876353442668915
train gradient:  0.1257820042885864
iteration : 8857
train acc:  0.8203125
train loss:  0.4759155511856079
train gradient:  0.12676605291813786
iteration : 8858
train acc:  0.703125
train loss:  0.5158695578575134
train gradient:  0.1531168868907166
iteration : 8859
train acc:  0.75
train loss:  0.5263574719429016
train gradient:  0.13882736693050116
iteration : 8860
train acc:  0.7734375
train loss:  0.538456380367279
train gradient:  0.16954676282972964
iteration : 8861
train acc:  0.8125
train loss:  0.4509735107421875
train gradient:  0.09638968457385073
iteration : 8862
train acc:  0.734375
train loss:  0.5554525852203369
train gradient:  0.15438736118586543
iteration : 8863
train acc:  0.734375
train loss:  0.4780784845352173
train gradient:  0.11655757728487953
iteration : 8864
train acc:  0.75
train loss:  0.44222795963287354
train gradient:  0.07971068592080964
iteration : 8865
train acc:  0.71875
train loss:  0.5585012435913086
train gradient:  0.15610719734698747
iteration : 8866
train acc:  0.671875
train loss:  0.5895657539367676
train gradient:  0.15263096960218026
iteration : 8867
train acc:  0.78125
train loss:  0.4521349370479584
train gradient:  0.11704726349560624
iteration : 8868
train acc:  0.7421875
train loss:  0.5420128107070923
train gradient:  0.16447740667484592
iteration : 8869
train acc:  0.7265625
train loss:  0.5206509828567505
train gradient:  0.19113003039771398
iteration : 8870
train acc:  0.6875
train loss:  0.5494982004165649
train gradient:  0.20326089717242182
iteration : 8871
train acc:  0.7734375
train loss:  0.48697763681411743
train gradient:  0.10440614521615192
iteration : 8872
train acc:  0.7578125
train loss:  0.5070928931236267
train gradient:  0.11817772893444159
iteration : 8873
train acc:  0.84375
train loss:  0.4173097014427185
train gradient:  0.11170125532522636
iteration : 8874
train acc:  0.7265625
train loss:  0.5443249940872192
train gradient:  0.15324009258929241
iteration : 8875
train acc:  0.7890625
train loss:  0.38258838653564453
train gradient:  0.07914737475621787
iteration : 8876
train acc:  0.7265625
train loss:  0.5068322420120239
train gradient:  0.15070826343176696
iteration : 8877
train acc:  0.6953125
train loss:  0.512488842010498
train gradient:  0.1365793590076939
iteration : 8878
train acc:  0.75
train loss:  0.5488433837890625
train gradient:  0.15967253480973598
iteration : 8879
train acc:  0.6875
train loss:  0.536456823348999
train gradient:  0.1987902003642696
iteration : 8880
train acc:  0.75
train loss:  0.4442114233970642
train gradient:  0.11476241080078826
iteration : 8881
train acc:  0.6796875
train loss:  0.5099022388458252
train gradient:  0.1378209983251051
iteration : 8882
train acc:  0.7421875
train loss:  0.5206191539764404
train gradient:  0.15150458502848435
iteration : 8883
train acc:  0.734375
train loss:  0.5275172591209412
train gradient:  0.1526391196573389
iteration : 8884
train acc:  0.6875
train loss:  0.5526623725891113
train gradient:  0.16363356125988254
iteration : 8885
train acc:  0.7109375
train loss:  0.536405622959137
train gradient:  0.15326875540648194
iteration : 8886
train acc:  0.7734375
train loss:  0.44459420442581177
train gradient:  0.09694811987763079
iteration : 8887
train acc:  0.7109375
train loss:  0.5398321151733398
train gradient:  0.21161026513207046
iteration : 8888
train acc:  0.6328125
train loss:  0.5480948686599731
train gradient:  0.1636380066764838
iteration : 8889
train acc:  0.796875
train loss:  0.46688777208328247
train gradient:  0.10054069414424732
iteration : 8890
train acc:  0.7265625
train loss:  0.48230668902397156
train gradient:  0.13476137426758952
iteration : 8891
train acc:  0.75
train loss:  0.47742289304733276
train gradient:  0.11840540588075887
iteration : 8892
train acc:  0.6953125
train loss:  0.5666182637214661
train gradient:  0.15303139274769556
iteration : 8893
train acc:  0.765625
train loss:  0.5181398391723633
train gradient:  0.14533532583581532
iteration : 8894
train acc:  0.7578125
train loss:  0.5008158683776855
train gradient:  0.1812319416049636
iteration : 8895
train acc:  0.671875
train loss:  0.5647290945053101
train gradient:  0.15518194736831506
iteration : 8896
train acc:  0.71875
train loss:  0.5185880661010742
train gradient:  0.13482013613449484
iteration : 8897
train acc:  0.765625
train loss:  0.4794524908065796
train gradient:  0.14897045709547663
iteration : 8898
train acc:  0.7734375
train loss:  0.4878453314304352
train gradient:  0.12390491919162358
iteration : 8899
train acc:  0.7421875
train loss:  0.4956149458885193
train gradient:  0.12526262470976132
iteration : 8900
train acc:  0.75
train loss:  0.5195678472518921
train gradient:  0.17323033092967913
iteration : 8901
train acc:  0.65625
train loss:  0.5950123071670532
train gradient:  0.16828750293898115
iteration : 8902
train acc:  0.7734375
train loss:  0.48826658725738525
train gradient:  0.11542996792036674
iteration : 8903
train acc:  0.734375
train loss:  0.4896193742752075
train gradient:  0.16485202426842144
iteration : 8904
train acc:  0.7265625
train loss:  0.5284723043441772
train gradient:  0.17779248997130817
iteration : 8905
train acc:  0.75
train loss:  0.4351794123649597
train gradient:  0.08527999544157228
iteration : 8906
train acc:  0.75
train loss:  0.49526447057724
train gradient:  0.11454037581062486
iteration : 8907
train acc:  0.765625
train loss:  0.5444382429122925
train gradient:  0.1576593718942128
iteration : 8908
train acc:  0.671875
train loss:  0.5764671564102173
train gradient:  0.17845702986559459
iteration : 8909
train acc:  0.7734375
train loss:  0.4755849838256836
train gradient:  0.1403497171358451
iteration : 8910
train acc:  0.78125
train loss:  0.429109662771225
train gradient:  0.09249420659453203
iteration : 8911
train acc:  0.703125
train loss:  0.49334806203842163
train gradient:  0.16980801701036277
iteration : 8912
train acc:  0.765625
train loss:  0.4975952208042145
train gradient:  0.12509125849631791
iteration : 8913
train acc:  0.71875
train loss:  0.5623329877853394
train gradient:  0.15947630528028756
iteration : 8914
train acc:  0.734375
train loss:  0.47251808643341064
train gradient:  0.1606680107645993
iteration : 8915
train acc:  0.7734375
train loss:  0.4602813124656677
train gradient:  0.1215467036015686
iteration : 8916
train acc:  0.71875
train loss:  0.4999394118785858
train gradient:  0.11680477353990533
iteration : 8917
train acc:  0.7578125
train loss:  0.46618905663490295
train gradient:  0.16788877089441637
iteration : 8918
train acc:  0.6875
train loss:  0.5528118014335632
train gradient:  0.149730593049295
iteration : 8919
train acc:  0.75
train loss:  0.44418007135391235
train gradient:  0.12267924236351445
iteration : 8920
train acc:  0.7578125
train loss:  0.5362153053283691
train gradient:  0.1739820063262421
iteration : 8921
train acc:  0.8046875
train loss:  0.4615037441253662
train gradient:  0.15579074048971578
iteration : 8922
train acc:  0.7265625
train loss:  0.4944325089454651
train gradient:  0.1643244461107134
iteration : 8923
train acc:  0.671875
train loss:  0.5459140539169312
train gradient:  0.12071997964296036
iteration : 8924
train acc:  0.6953125
train loss:  0.5596745014190674
train gradient:  0.15843602076346375
iteration : 8925
train acc:  0.6640625
train loss:  0.5892063975334167
train gradient:  0.15845917088292866
iteration : 8926
train acc:  0.6953125
train loss:  0.5423847436904907
train gradient:  0.14239085021775816
iteration : 8927
train acc:  0.8046875
train loss:  0.4400981664657593
train gradient:  0.10766151554605047
iteration : 8928
train acc:  0.8125
train loss:  0.4736114740371704
train gradient:  0.11946405580281821
iteration : 8929
train acc:  0.71875
train loss:  0.4929983615875244
train gradient:  0.12863548920636914
iteration : 8930
train acc:  0.7578125
train loss:  0.4758951663970947
train gradient:  0.10924377768391305
iteration : 8931
train acc:  0.6953125
train loss:  0.5277436971664429
train gradient:  0.14687511717168517
iteration : 8932
train acc:  0.7265625
train loss:  0.48551857471466064
train gradient:  0.1268547440612184
iteration : 8933
train acc:  0.75
train loss:  0.4783165752887726
train gradient:  0.1319867142895369
iteration : 8934
train acc:  0.765625
train loss:  0.4613347351551056
train gradient:  0.12192388962209513
iteration : 8935
train acc:  0.734375
train loss:  0.5094268321990967
train gradient:  0.12843878365365616
iteration : 8936
train acc:  0.71875
train loss:  0.49567732214927673
train gradient:  0.1580046116260962
iteration : 8937
train acc:  0.796875
train loss:  0.4354180693626404
train gradient:  0.10230814164579353
iteration : 8938
train acc:  0.7890625
train loss:  0.4831363260746002
train gradient:  0.13198617518406475
iteration : 8939
train acc:  0.7734375
train loss:  0.49206310510635376
train gradient:  0.09711575652134413
iteration : 8940
train acc:  0.75
train loss:  0.46228909492492676
train gradient:  0.09655368180947912
iteration : 8941
train acc:  0.7421875
train loss:  0.4854411780834198
train gradient:  0.14388459431436623
iteration : 8942
train acc:  0.7734375
train loss:  0.4896913766860962
train gradient:  0.13450871550894417
iteration : 8943
train acc:  0.765625
train loss:  0.49471068382263184
train gradient:  0.14198208411258792
iteration : 8944
train acc:  0.671875
train loss:  0.5955286026000977
train gradient:  0.19418169849082123
iteration : 8945
train acc:  0.7265625
train loss:  0.5031603574752808
train gradient:  0.15664276589777984
iteration : 8946
train acc:  0.8046875
train loss:  0.46886950731277466
train gradient:  0.1321185954026995
iteration : 8947
train acc:  0.7890625
train loss:  0.4079529345035553
train gradient:  0.09832137600545375
iteration : 8948
train acc:  0.7734375
train loss:  0.48736390471458435
train gradient:  0.14008255643352624
iteration : 8949
train acc:  0.78125
train loss:  0.49327513575553894
train gradient:  0.13610728733777316
iteration : 8950
train acc:  0.7109375
train loss:  0.5210448503494263
train gradient:  0.17295352669045422
iteration : 8951
train acc:  0.8359375
train loss:  0.41543760895729065
train gradient:  0.11710435189572865
iteration : 8952
train acc:  0.7109375
train loss:  0.512095034122467
train gradient:  0.13061208898822932
iteration : 8953
train acc:  0.7734375
train loss:  0.47152212262153625
train gradient:  0.09852958521021832
iteration : 8954
train acc:  0.765625
train loss:  0.5314106941223145
train gradient:  0.15166418929597603
iteration : 8955
train acc:  0.7109375
train loss:  0.5094591379165649
train gradient:  0.15061036233396524
iteration : 8956
train acc:  0.75
train loss:  0.5628048181533813
train gradient:  0.16179540889587357
iteration : 8957
train acc:  0.78125
train loss:  0.4808397889137268
train gradient:  0.1467973152985635
iteration : 8958
train acc:  0.7265625
train loss:  0.5317414999008179
train gradient:  0.14028339782210403
iteration : 8959
train acc:  0.6796875
train loss:  0.5588597059249878
train gradient:  0.15823848729051598
iteration : 8960
train acc:  0.7265625
train loss:  0.5081853866577148
train gradient:  0.12465646610329428
iteration : 8961
train acc:  0.78125
train loss:  0.48581206798553467
train gradient:  0.18336654479381448
iteration : 8962
train acc:  0.6875
train loss:  0.5648289918899536
train gradient:  0.1810691170728843
iteration : 8963
train acc:  0.734375
train loss:  0.5019772052764893
train gradient:  0.13753665933593137
iteration : 8964
train acc:  0.765625
train loss:  0.444144070148468
train gradient:  0.13689436258478352
iteration : 8965
train acc:  0.7578125
train loss:  0.45766377449035645
train gradient:  0.11598072154900325
iteration : 8966
train acc:  0.78125
train loss:  0.49178916215896606
train gradient:  0.12204712632722982
iteration : 8967
train acc:  0.71875
train loss:  0.4824022650718689
train gradient:  0.12087175952248398
iteration : 8968
train acc:  0.6953125
train loss:  0.5397382974624634
train gradient:  0.145518102172464
iteration : 8969
train acc:  0.8046875
train loss:  0.4532510042190552
train gradient:  0.1090043899255277
iteration : 8970
train acc:  0.71875
train loss:  0.5582685470581055
train gradient:  0.1823786223565131
iteration : 8971
train acc:  0.7578125
train loss:  0.49758100509643555
train gradient:  0.11868270698424263
iteration : 8972
train acc:  0.703125
train loss:  0.48602163791656494
train gradient:  0.12743885789753118
iteration : 8973
train acc:  0.8046875
train loss:  0.46343451738357544
train gradient:  0.12916381665227566
iteration : 8974
train acc:  0.7734375
train loss:  0.4859868288040161
train gradient:  0.11216538799903362
iteration : 8975
train acc:  0.703125
train loss:  0.5276494026184082
train gradient:  0.15650610850234126
iteration : 8976
train acc:  0.828125
train loss:  0.4343993365764618
train gradient:  0.11567845426170105
iteration : 8977
train acc:  0.65625
train loss:  0.5784331560134888
train gradient:  0.1817261373397383
iteration : 8978
train acc:  0.71875
train loss:  0.49807441234588623
train gradient:  0.1437242019075084
iteration : 8979
train acc:  0.7578125
train loss:  0.5347700715065002
train gradient:  0.14820318963737816
iteration : 8980
train acc:  0.7890625
train loss:  0.4446070194244385
train gradient:  0.13839737166895288
iteration : 8981
train acc:  0.75
train loss:  0.5326392650604248
train gradient:  0.14783736761075067
iteration : 8982
train acc:  0.765625
train loss:  0.48092955350875854
train gradient:  0.10544346021439784
iteration : 8983
train acc:  0.7421875
train loss:  0.48021310567855835
train gradient:  0.11350069425908359
iteration : 8984
train acc:  0.7578125
train loss:  0.4638081192970276
train gradient:  0.12207203156730698
iteration : 8985
train acc:  0.7265625
train loss:  0.5086194276809692
train gradient:  0.1401913458644209
iteration : 8986
train acc:  0.8046875
train loss:  0.41468310356140137
train gradient:  0.12052948230626834
iteration : 8987
train acc:  0.7265625
train loss:  0.5123692750930786
train gradient:  0.1387812381242876
iteration : 8988
train acc:  0.703125
train loss:  0.5563279390335083
train gradient:  0.1506473116582082
iteration : 8989
train acc:  0.7734375
train loss:  0.5115764141082764
train gradient:  0.1935496080163633
iteration : 8990
train acc:  0.78125
train loss:  0.4675169587135315
train gradient:  0.11985946179756816
iteration : 8991
train acc:  0.8046875
train loss:  0.48010995984077454
train gradient:  0.12800390770699543
iteration : 8992
train acc:  0.7734375
train loss:  0.4727398157119751
train gradient:  0.12564467912258726
iteration : 8993
train acc:  0.75
train loss:  0.5264429450035095
train gradient:  0.1867821594302697
iteration : 8994
train acc:  0.78125
train loss:  0.4644784927368164
train gradient:  0.10236565176122454
iteration : 8995
train acc:  0.78125
train loss:  0.4275952875614166
train gradient:  0.11779825295039405
iteration : 8996
train acc:  0.75
train loss:  0.502736508846283
train gradient:  0.1631508489890024
iteration : 8997
train acc:  0.796875
train loss:  0.4471883475780487
train gradient:  0.11711125356462056
iteration : 8998
train acc:  0.71875
train loss:  0.518452525138855
train gradient:  0.1377780471115871
iteration : 8999
train acc:  0.6953125
train loss:  0.5314938426017761
train gradient:  0.12944538736036737
iteration : 9000
train acc:  0.7421875
train loss:  0.4847163259983063
train gradient:  0.12350964072035539
iteration : 9001
train acc:  0.71875
train loss:  0.49454259872436523
train gradient:  0.11967733218660029
iteration : 9002
train acc:  0.71875
train loss:  0.53493732213974
train gradient:  0.1665226792239174
iteration : 9003
train acc:  0.78125
train loss:  0.46673664450645447
train gradient:  0.12234671204333339
iteration : 9004
train acc:  0.7265625
train loss:  0.4962835907936096
train gradient:  0.1577066134812592
iteration : 9005
train acc:  0.71875
train loss:  0.5149416923522949
train gradient:  0.13582943060780872
iteration : 9006
train acc:  0.75
train loss:  0.5191278457641602
train gradient:  0.14326904080584732
iteration : 9007
train acc:  0.7109375
train loss:  0.5417093634605408
train gradient:  0.1366877581389936
iteration : 9008
train acc:  0.6640625
train loss:  0.5809985995292664
train gradient:  0.15870194022528866
iteration : 9009
train acc:  0.734375
train loss:  0.5009692907333374
train gradient:  0.11093408157509818
iteration : 9010
train acc:  0.7421875
train loss:  0.44795891642570496
train gradient:  0.1259682529331112
iteration : 9011
train acc:  0.7109375
train loss:  0.5387729406356812
train gradient:  0.12816312002037994
iteration : 9012
train acc:  0.71875
train loss:  0.5068992376327515
train gradient:  0.13224597147744382
iteration : 9013
train acc:  0.765625
train loss:  0.43042057752609253
train gradient:  0.09970095363405061
iteration : 9014
train acc:  0.7578125
train loss:  0.46591219305992126
train gradient:  0.10961891380271138
iteration : 9015
train acc:  0.734375
train loss:  0.5711610317230225
train gradient:  0.18179781999110808
iteration : 9016
train acc:  0.75
train loss:  0.4868534803390503
train gradient:  0.1199981514271181
iteration : 9017
train acc:  0.6796875
train loss:  0.5154893398284912
train gradient:  0.17614990129090924
iteration : 9018
train acc:  0.75
train loss:  0.4739609658718109
train gradient:  0.12151455794852653
iteration : 9019
train acc:  0.6953125
train loss:  0.5301761031150818
train gradient:  0.12519406617199572
iteration : 9020
train acc:  0.7109375
train loss:  0.4665431082248688
train gradient:  0.11442033762169224
iteration : 9021
train acc:  0.7890625
train loss:  0.4638059735298157
train gradient:  0.18236248038923353
iteration : 9022
train acc:  0.6953125
train loss:  0.5234525203704834
train gradient:  0.1460275609692147
iteration : 9023
train acc:  0.7421875
train loss:  0.5330315828323364
train gradient:  0.13514095669896325
iteration : 9024
train acc:  0.6953125
train loss:  0.5279374718666077
train gradient:  0.18620399490180778
iteration : 9025
train acc:  0.7109375
train loss:  0.5363910794258118
train gradient:  0.1748990260332241
iteration : 9026
train acc:  0.6328125
train loss:  0.5648916363716125
train gradient:  0.18143004908105892
iteration : 9027
train acc:  0.796875
train loss:  0.4470672607421875
train gradient:  0.10036780972417315
iteration : 9028
train acc:  0.7421875
train loss:  0.5233104228973389
train gradient:  0.2018277252310351
iteration : 9029
train acc:  0.8046875
train loss:  0.4332588315010071
train gradient:  0.10516946008334618
iteration : 9030
train acc:  0.6875
train loss:  0.5586984157562256
train gradient:  0.22981456669943473
iteration : 9031
train acc:  0.6796875
train loss:  0.5251226425170898
train gradient:  0.1386679166048875
iteration : 9032
train acc:  0.7421875
train loss:  0.47506213188171387
train gradient:  0.13540642998518806
iteration : 9033
train acc:  0.671875
train loss:  0.5587193965911865
train gradient:  0.19006607295146194
iteration : 9034
train acc:  0.7421875
train loss:  0.48316580057144165
train gradient:  0.1240771640551741
iteration : 9035
train acc:  0.765625
train loss:  0.4513474106788635
train gradient:  0.10623726219140782
iteration : 9036
train acc:  0.7734375
train loss:  0.44726210832595825
train gradient:  0.13018095146486153
iteration : 9037
train acc:  0.7421875
train loss:  0.5221913456916809
train gradient:  0.1578901694456877
iteration : 9038
train acc:  0.765625
train loss:  0.45407646894454956
train gradient:  0.1370978579449701
iteration : 9039
train acc:  0.7578125
train loss:  0.4791707992553711
train gradient:  0.14604370848948758
iteration : 9040
train acc:  0.734375
train loss:  0.5025737285614014
train gradient:  0.10910846594406066
iteration : 9041
train acc:  0.7265625
train loss:  0.440934956073761
train gradient:  0.11238451186065056
iteration : 9042
train acc:  0.7265625
train loss:  0.5298228859901428
train gradient:  0.15746194697578536
iteration : 9043
train acc:  0.7421875
train loss:  0.48930680751800537
train gradient:  0.12364811618871491
iteration : 9044
train acc:  0.6796875
train loss:  0.5540527105331421
train gradient:  0.15865384276786607
iteration : 9045
train acc:  0.703125
train loss:  0.5603667497634888
train gradient:  0.1301765403561187
iteration : 9046
train acc:  0.78125
train loss:  0.48899221420288086
train gradient:  0.1336207965773757
iteration : 9047
train acc:  0.6953125
train loss:  0.5481825470924377
train gradient:  0.19673466140648094
iteration : 9048
train acc:  0.6796875
train loss:  0.5435914993286133
train gradient:  0.19252453652507295
iteration : 9049
train acc:  0.6484375
train loss:  0.5303215980529785
train gradient:  0.13754572095875647
iteration : 9050
train acc:  0.7421875
train loss:  0.470064640045166
train gradient:  0.12657387327242436
iteration : 9051
train acc:  0.7734375
train loss:  0.44518375396728516
train gradient:  0.10855410874241724
iteration : 9052
train acc:  0.7265625
train loss:  0.5139824151992798
train gradient:  0.13742812695445328
iteration : 9053
train acc:  0.734375
train loss:  0.49404484033584595
train gradient:  0.116208643199962
iteration : 9054
train acc:  0.7421875
train loss:  0.46212238073349
train gradient:  0.11388515624054125
iteration : 9055
train acc:  0.71875
train loss:  0.543442964553833
train gradient:  0.15946725547430685
iteration : 9056
train acc:  0.6640625
train loss:  0.5874535441398621
train gradient:  0.1570638026018717
iteration : 9057
train acc:  0.6875
train loss:  0.5599303245544434
train gradient:  0.22358289784740001
iteration : 9058
train acc:  0.7578125
train loss:  0.5048230886459351
train gradient:  0.132010146143134
iteration : 9059
train acc:  0.765625
train loss:  0.46339303255081177
train gradient:  0.10654282652661115
iteration : 9060
train acc:  0.7734375
train loss:  0.4890412986278534
train gradient:  0.16191970927004196
iteration : 9061
train acc:  0.8671875
train loss:  0.42292433977127075
train gradient:  0.13469842333445586
iteration : 9062
train acc:  0.78125
train loss:  0.47188958525657654
train gradient:  0.11010693682082504
iteration : 9063
train acc:  0.765625
train loss:  0.44586753845214844
train gradient:  0.13836067625557916
iteration : 9064
train acc:  0.6953125
train loss:  0.5037572383880615
train gradient:  0.12438884204254753
iteration : 9065
train acc:  0.7421875
train loss:  0.5512814521789551
train gradient:  0.21310706691255377
iteration : 9066
train acc:  0.703125
train loss:  0.5243465900421143
train gradient:  0.17387810509721113
iteration : 9067
train acc:  0.78125
train loss:  0.5006691217422485
train gradient:  0.12608846770005983
iteration : 9068
train acc:  0.75
train loss:  0.48608359694480896
train gradient:  0.14126168063933148
iteration : 9069
train acc:  0.75
train loss:  0.47984129190444946
train gradient:  0.11272252742447428
iteration : 9070
train acc:  0.7421875
train loss:  0.45143255591392517
train gradient:  0.1221530551282646
iteration : 9071
train acc:  0.734375
train loss:  0.4946412444114685
train gradient:  0.13384615612178047
iteration : 9072
train acc:  0.703125
train loss:  0.5359794497489929
train gradient:  0.15314748416163346
iteration : 9073
train acc:  0.75
train loss:  0.5363209843635559
train gradient:  0.18133546985867693
iteration : 9074
train acc:  0.6875
train loss:  0.5261988639831543
train gradient:  0.13834844563051066
iteration : 9075
train acc:  0.765625
train loss:  0.5017374753952026
train gradient:  0.14839581955597164
iteration : 9076
train acc:  0.78125
train loss:  0.5217848420143127
train gradient:  0.15040327994541286
iteration : 9077
train acc:  0.796875
train loss:  0.4559482932090759
train gradient:  0.13581458506581318
iteration : 9078
train acc:  0.765625
train loss:  0.5454918146133423
train gradient:  0.21076502662509206
iteration : 9079
train acc:  0.7265625
train loss:  0.5121408104896545
train gradient:  0.14019508759622745
iteration : 9080
train acc:  0.796875
train loss:  0.46080970764160156
train gradient:  0.11819586133830519
iteration : 9081
train acc:  0.7578125
train loss:  0.4839305877685547
train gradient:  0.15652223326839887
iteration : 9082
train acc:  0.7734375
train loss:  0.45153695344924927
train gradient:  0.10977183572274249
iteration : 9083
train acc:  0.78125
train loss:  0.4266555905342102
train gradient:  0.11804028576033707
iteration : 9084
train acc:  0.703125
train loss:  0.5611928701400757
train gradient:  0.14817431654563085
iteration : 9085
train acc:  0.7109375
train loss:  0.516344428062439
train gradient:  0.1160751919335336
iteration : 9086
train acc:  0.78125
train loss:  0.47887569665908813
train gradient:  0.13084467508495454
iteration : 9087
train acc:  0.734375
train loss:  0.5049468278884888
train gradient:  0.1268425438487677
iteration : 9088
train acc:  0.6875
train loss:  0.5353106260299683
train gradient:  0.1557358521037428
iteration : 9089
train acc:  0.6796875
train loss:  0.5932682752609253
train gradient:  0.15647372136245832
iteration : 9090
train acc:  0.75
train loss:  0.4746452867984772
train gradient:  0.11504258745610034
iteration : 9091
train acc:  0.7890625
train loss:  0.4762845039367676
train gradient:  0.12894913578883324
iteration : 9092
train acc:  0.7734375
train loss:  0.4365188479423523
train gradient:  0.11204033536513613
iteration : 9093
train acc:  0.765625
train loss:  0.46362751722335815
train gradient:  0.14868906767587498
iteration : 9094
train acc:  0.7265625
train loss:  0.5360285043716431
train gradient:  0.12826345166716263
iteration : 9095
train acc:  0.6484375
train loss:  0.6356889009475708
train gradient:  0.1996912739670198
iteration : 9096
train acc:  0.734375
train loss:  0.459331214427948
train gradient:  0.12666117036185598
iteration : 9097
train acc:  0.71875
train loss:  0.5323575735092163
train gradient:  0.15191871489223369
iteration : 9098
train acc:  0.7109375
train loss:  0.501451313495636
train gradient:  0.11194490418338945
iteration : 9099
train acc:  0.765625
train loss:  0.448507696390152
train gradient:  0.10850871259880669
iteration : 9100
train acc:  0.75
train loss:  0.4950423836708069
train gradient:  0.14185358711244683
iteration : 9101
train acc:  0.765625
train loss:  0.4710875451564789
train gradient:  0.12078613458738043
iteration : 9102
train acc:  0.78125
train loss:  0.4344465136528015
train gradient:  0.10260714446040844
iteration : 9103
train acc:  0.71875
train loss:  0.5272401571273804
train gradient:  0.14577223075185147
iteration : 9104
train acc:  0.7890625
train loss:  0.43337514996528625
train gradient:  0.0979030847769229
iteration : 9105
train acc:  0.734375
train loss:  0.5310114622116089
train gradient:  0.1962598737768697
iteration : 9106
train acc:  0.75
train loss:  0.5341179370880127
train gradient:  0.1492173329703852
iteration : 9107
train acc:  0.7421875
train loss:  0.424641489982605
train gradient:  0.11673794762702824
iteration : 9108
train acc:  0.703125
train loss:  0.5268625617027283
train gradient:  0.12636932639961412
iteration : 9109
train acc:  0.71875
train loss:  0.5294948816299438
train gradient:  0.12670662529792412
iteration : 9110
train acc:  0.71875
train loss:  0.49739551544189453
train gradient:  0.12346470903557986
iteration : 9111
train acc:  0.75
train loss:  0.5081691741943359
train gradient:  0.11978718230830616
iteration : 9112
train acc:  0.640625
train loss:  0.620574951171875
train gradient:  0.16153483096497767
iteration : 9113
train acc:  0.734375
train loss:  0.4596395194530487
train gradient:  0.12890873066464636
iteration : 9114
train acc:  0.734375
train loss:  0.45643308758735657
train gradient:  0.11970152950523824
iteration : 9115
train acc:  0.7265625
train loss:  0.5144952535629272
train gradient:  0.15159406194842712
iteration : 9116
train acc:  0.765625
train loss:  0.4469224810600281
train gradient:  0.09130289342992877
iteration : 9117
train acc:  0.71875
train loss:  0.5454691648483276
train gradient:  0.1522805223351263
iteration : 9118
train acc:  0.6875
train loss:  0.507604718208313
train gradient:  0.15830056779284946
iteration : 9119
train acc:  0.7890625
train loss:  0.47830700874328613
train gradient:  0.14969899605601553
iteration : 9120
train acc:  0.703125
train loss:  0.5187841653823853
train gradient:  0.13778815235303396
iteration : 9121
train acc:  0.8359375
train loss:  0.406352698802948
train gradient:  0.10066474503558692
iteration : 9122
train acc:  0.65625
train loss:  0.5554870367050171
train gradient:  0.1797489877652577
iteration : 9123
train acc:  0.625
train loss:  0.5939992666244507
train gradient:  0.16456046408597735
iteration : 9124
train acc:  0.7421875
train loss:  0.4694080054759979
train gradient:  0.12218845406091143
iteration : 9125
train acc:  0.765625
train loss:  0.5465090870857239
train gradient:  0.14707933592025546
iteration : 9126
train acc:  0.7734375
train loss:  0.5137726068496704
train gradient:  0.1308928419212232
iteration : 9127
train acc:  0.734375
train loss:  0.466498464345932
train gradient:  0.12934562776541192
iteration : 9128
train acc:  0.7421875
train loss:  0.4985996186733246
train gradient:  0.13758833653656494
iteration : 9129
train acc:  0.7734375
train loss:  0.40480321645736694
train gradient:  0.10492272728336313
iteration : 9130
train acc:  0.7421875
train loss:  0.5016470551490784
train gradient:  0.1540303352452126
iteration : 9131
train acc:  0.7890625
train loss:  0.4682156443595886
train gradient:  0.1268445826941959
iteration : 9132
train acc:  0.734375
train loss:  0.5460159182548523
train gradient:  0.14948111597930452
iteration : 9133
train acc:  0.671875
train loss:  0.6156013011932373
train gradient:  0.2163403399893169
iteration : 9134
train acc:  0.7421875
train loss:  0.5496138334274292
train gradient:  0.16565292494685843
iteration : 9135
train acc:  0.71875
train loss:  0.44862014055252075
train gradient:  0.10535521033326083
iteration : 9136
train acc:  0.734375
train loss:  0.5071188807487488
train gradient:  0.15403641266503343
iteration : 9137
train acc:  0.7109375
train loss:  0.5366476774215698
train gradient:  0.14476949887491591
iteration : 9138
train acc:  0.71875
train loss:  0.5235387086868286
train gradient:  0.14078024554266627
iteration : 9139
train acc:  0.7734375
train loss:  0.4793437123298645
train gradient:  0.1309563706893635
iteration : 9140
train acc:  0.7265625
train loss:  0.4664280116558075
train gradient:  0.10917482088708731
iteration : 9141
train acc:  0.75
train loss:  0.49812179803848267
train gradient:  0.14479953279751834
iteration : 9142
train acc:  0.765625
train loss:  0.4952906370162964
train gradient:  0.16954772047203331
iteration : 9143
train acc:  0.765625
train loss:  0.4396604895591736
train gradient:  0.09365302178450162
iteration : 9144
train acc:  0.8046875
train loss:  0.46511369943618774
train gradient:  0.10347835225730714
iteration : 9145
train acc:  0.796875
train loss:  0.5247071981430054
train gradient:  0.1410375253786642
iteration : 9146
train acc:  0.765625
train loss:  0.4833320379257202
train gradient:  0.11958768062389699
iteration : 9147
train acc:  0.734375
train loss:  0.5119292140007019
train gradient:  0.11673576244833556
iteration : 9148
train acc:  0.7578125
train loss:  0.5115423202514648
train gradient:  0.1325182041659577
iteration : 9149
train acc:  0.7265625
train loss:  0.5134548544883728
train gradient:  0.1445040132117143
iteration : 9150
train acc:  0.7109375
train loss:  0.5196632742881775
train gradient:  0.1204551559255286
iteration : 9151
train acc:  0.7421875
train loss:  0.5311965942382812
train gradient:  0.1245120933647646
iteration : 9152
train acc:  0.7578125
train loss:  0.4513391852378845
train gradient:  0.10312671531686655
iteration : 9153
train acc:  0.734375
train loss:  0.5361831188201904
train gradient:  0.14300374863310492
iteration : 9154
train acc:  0.78125
train loss:  0.4704282879829407
train gradient:  0.1244362879845308
iteration : 9155
train acc:  0.75
train loss:  0.5359535813331604
train gradient:  0.13092929652521212
iteration : 9156
train acc:  0.7421875
train loss:  0.4979746341705322
train gradient:  0.11778371220445627
iteration : 9157
train acc:  0.7734375
train loss:  0.460648775100708
train gradient:  0.12342371192523467
iteration : 9158
train acc:  0.7578125
train loss:  0.5218666195869446
train gradient:  0.135082058661479
iteration : 9159
train acc:  0.765625
train loss:  0.4758644700050354
train gradient:  0.13253671943145148
iteration : 9160
train acc:  0.7578125
train loss:  0.45824337005615234
train gradient:  0.10009497615089996
iteration : 9161
train acc:  0.734375
train loss:  0.5126461982727051
train gradient:  0.13491573564778983
iteration : 9162
train acc:  0.71875
train loss:  0.5102872252464294
train gradient:  0.1377605477139502
iteration : 9163
train acc:  0.71875
train loss:  0.5105355381965637
train gradient:  0.11977780893795993
iteration : 9164
train acc:  0.765625
train loss:  0.5148332118988037
train gradient:  0.19256765970624107
iteration : 9165
train acc:  0.6328125
train loss:  0.5354743003845215
train gradient:  0.1611069595989728
iteration : 9166
train acc:  0.7421875
train loss:  0.534354567527771
train gradient:  0.18688293547633483
iteration : 9167
train acc:  0.765625
train loss:  0.48000603914260864
train gradient:  0.11781456130550262
iteration : 9168
train acc:  0.7421875
train loss:  0.5246434807777405
train gradient:  0.12316039471471035
iteration : 9169
train acc:  0.71875
train loss:  0.557608425617218
train gradient:  0.15882667277091067
iteration : 9170
train acc:  0.734375
train loss:  0.4909445643424988
train gradient:  0.1065237888813225
iteration : 9171
train acc:  0.8125
train loss:  0.44000694155693054
train gradient:  0.11648246789863505
iteration : 9172
train acc:  0.765625
train loss:  0.4617842733860016
train gradient:  0.10437804069225178
iteration : 9173
train acc:  0.78125
train loss:  0.49674177169799805
train gradient:  0.13778873958619248
iteration : 9174
train acc:  0.7109375
train loss:  0.49461841583251953
train gradient:  0.11003421399876356
iteration : 9175
train acc:  0.734375
train loss:  0.5089689493179321
train gradient:  0.11483215147797862
iteration : 9176
train acc:  0.7265625
train loss:  0.5108400583267212
train gradient:  0.14495921880799778
iteration : 9177
train acc:  0.6328125
train loss:  0.6050804853439331
train gradient:  0.17967510349183785
iteration : 9178
train acc:  0.703125
train loss:  0.5363348722457886
train gradient:  0.15972360051235446
iteration : 9179
train acc:  0.7421875
train loss:  0.4877801537513733
train gradient:  0.11121856381362391
iteration : 9180
train acc:  0.7734375
train loss:  0.477827250957489
train gradient:  0.10586187824699997
iteration : 9181
train acc:  0.7265625
train loss:  0.47549182176589966
train gradient:  0.10463940510193806
iteration : 9182
train acc:  0.71875
train loss:  0.5052422285079956
train gradient:  0.1643658229135288
iteration : 9183
train acc:  0.7734375
train loss:  0.44138267636299133
train gradient:  0.11237634232524353
iteration : 9184
train acc:  0.7109375
train loss:  0.5341808795928955
train gradient:  0.20226233779843777
iteration : 9185
train acc:  0.7578125
train loss:  0.4710504710674286
train gradient:  0.12582062667313232
iteration : 9186
train acc:  0.703125
train loss:  0.4990619421005249
train gradient:  0.12397755272858113
iteration : 9187
train acc:  0.8125
train loss:  0.42126795649528503
train gradient:  0.0885282155348859
iteration : 9188
train acc:  0.75
train loss:  0.4468224048614502
train gradient:  0.11498121161365193
iteration : 9189
train acc:  0.796875
train loss:  0.4254513084888458
train gradient:  0.12959221886086503
iteration : 9190
train acc:  0.7890625
train loss:  0.4898960292339325
train gradient:  0.12411344781644713
iteration : 9191
train acc:  0.765625
train loss:  0.47055789828300476
train gradient:  0.10033510405540501
iteration : 9192
train acc:  0.6796875
train loss:  0.5268762111663818
train gradient:  0.14440177959422798
iteration : 9193
train acc:  0.7421875
train loss:  0.5040707588195801
train gradient:  0.14713019361807023
iteration : 9194
train acc:  0.7109375
train loss:  0.514628529548645
train gradient:  0.12112482259758897
iteration : 9195
train acc:  0.7734375
train loss:  0.46325647830963135
train gradient:  0.12245333699630961
iteration : 9196
train acc:  0.7421875
train loss:  0.5103641748428345
train gradient:  0.14687869782491914
iteration : 9197
train acc:  0.78125
train loss:  0.47209322452545166
train gradient:  0.13702274755575164
iteration : 9198
train acc:  0.6953125
train loss:  0.5837239027023315
train gradient:  0.17551563281629962
iteration : 9199
train acc:  0.6953125
train loss:  0.5790623426437378
train gradient:  0.15247073762539282
iteration : 9200
train acc:  0.6953125
train loss:  0.515920877456665
train gradient:  0.12048153587292385
iteration : 9201
train acc:  0.7265625
train loss:  0.5036655068397522
train gradient:  0.1319239947252676
iteration : 9202
train acc:  0.7265625
train loss:  0.5310250520706177
train gradient:  0.19295820135897526
iteration : 9203
train acc:  0.796875
train loss:  0.42722249031066895
train gradient:  0.09566850128847779
iteration : 9204
train acc:  0.8046875
train loss:  0.47950708866119385
train gradient:  0.12783975644066603
iteration : 9205
train acc:  0.671875
train loss:  0.546808660030365
train gradient:  0.17491580638915816
iteration : 9206
train acc:  0.6640625
train loss:  0.5582022666931152
train gradient:  0.16396826103008155
iteration : 9207
train acc:  0.7265625
train loss:  0.5242512226104736
train gradient:  0.17591894546954548
iteration : 9208
train acc:  0.71875
train loss:  0.576728880405426
train gradient:  0.20381084806293948
iteration : 9209
train acc:  0.71875
train loss:  0.5366391539573669
train gradient:  0.14712239063856292
iteration : 9210
train acc:  0.7265625
train loss:  0.4944852292537689
train gradient:  0.10860948061314389
iteration : 9211
train acc:  0.78125
train loss:  0.4423871338367462
train gradient:  0.10517018466283802
iteration : 9212
train acc:  0.6953125
train loss:  0.5784403085708618
train gradient:  0.13661860502212445
iteration : 9213
train acc:  0.6953125
train loss:  0.47458335757255554
train gradient:  0.1217613388827994
iteration : 9214
train acc:  0.734375
train loss:  0.5203788876533508
train gradient:  0.1265530322603646
iteration : 9215
train acc:  0.7578125
train loss:  0.5140784978866577
train gradient:  0.13099565324743007
iteration : 9216
train acc:  0.7578125
train loss:  0.5172320604324341
train gradient:  0.13886584796051568
iteration : 9217
train acc:  0.7890625
train loss:  0.4399857521057129
train gradient:  0.14070325189977828
iteration : 9218
train acc:  0.7421875
train loss:  0.5025040507316589
train gradient:  0.14259128550308847
iteration : 9219
train acc:  0.65625
train loss:  0.5527635812759399
train gradient:  0.15974201030293192
iteration : 9220
train acc:  0.7265625
train loss:  0.5093486905097961
train gradient:  0.13003416331305845
iteration : 9221
train acc:  0.7578125
train loss:  0.4913346767425537
train gradient:  0.10499937390679634
iteration : 9222
train acc:  0.765625
train loss:  0.44147881865501404
train gradient:  0.10672577669397999
iteration : 9223
train acc:  0.7109375
train loss:  0.47791746258735657
train gradient:  0.12103423914712852
iteration : 9224
train acc:  0.7734375
train loss:  0.44880083203315735
train gradient:  0.09233639752991643
iteration : 9225
train acc:  0.703125
train loss:  0.49033910036087036
train gradient:  0.12861244180951514
iteration : 9226
train acc:  0.6953125
train loss:  0.5516211986541748
train gradient:  0.14071356827804135
iteration : 9227
train acc:  0.75
train loss:  0.4780054986476898
train gradient:  0.14529832948485347
iteration : 9228
train acc:  0.7421875
train loss:  0.5037220120429993
train gradient:  0.12916729158424786
iteration : 9229
train acc:  0.765625
train loss:  0.49364835023880005
train gradient:  0.12054400099865228
iteration : 9230
train acc:  0.6875
train loss:  0.550814151763916
train gradient:  0.17426801484097637
iteration : 9231
train acc:  0.7578125
train loss:  0.49939119815826416
train gradient:  0.13446242168303768
iteration : 9232
train acc:  0.703125
train loss:  0.5234965085983276
train gradient:  0.12110234989762098
iteration : 9233
train acc:  0.6484375
train loss:  0.5859802961349487
train gradient:  0.20657234068599117
iteration : 9234
train acc:  0.71875
train loss:  0.55510014295578
train gradient:  0.16330110645984516
iteration : 9235
train acc:  0.765625
train loss:  0.44059616327285767
train gradient:  0.1164131131016571
iteration : 9236
train acc:  0.609375
train loss:  0.6288814544677734
train gradient:  0.3016507257312756
iteration : 9237
train acc:  0.7109375
train loss:  0.4664752781391144
train gradient:  0.11900444351554941
iteration : 9238
train acc:  0.7734375
train loss:  0.4723016917705536
train gradient:  0.13510875854090418
iteration : 9239
train acc:  0.8125
train loss:  0.49519747495651245
train gradient:  0.14357935870128563
iteration : 9240
train acc:  0.78125
train loss:  0.43076035380363464
train gradient:  0.12956944902123907
iteration : 9241
train acc:  0.734375
train loss:  0.47305411100387573
train gradient:  0.11288159117209245
iteration : 9242
train acc:  0.8046875
train loss:  0.4428308606147766
train gradient:  0.09847676805592277
iteration : 9243
train acc:  0.796875
train loss:  0.5239919424057007
train gradient:  0.16001533082309727
iteration : 9244
train acc:  0.6953125
train loss:  0.5685864686965942
train gradient:  0.16552594875284066
iteration : 9245
train acc:  0.6875
train loss:  0.5430520176887512
train gradient:  0.16754312386044468
iteration : 9246
train acc:  0.6875
train loss:  0.5435567498207092
train gradient:  0.13620384016735257
iteration : 9247
train acc:  0.6328125
train loss:  0.5780364274978638
train gradient:  0.17197746079486126
iteration : 9248
train acc:  0.703125
train loss:  0.4633261561393738
train gradient:  0.10742368043846476
iteration : 9249
train acc:  0.6875
train loss:  0.5279980897903442
train gradient:  0.13037113230823144
iteration : 9250
train acc:  0.7890625
train loss:  0.4536651074886322
train gradient:  0.10103313824929983
iteration : 9251
train acc:  0.734375
train loss:  0.4685625433921814
train gradient:  0.13425083308647678
iteration : 9252
train acc:  0.75
train loss:  0.4695209860801697
train gradient:  0.1279484777835903
iteration : 9253
train acc:  0.7578125
train loss:  0.4658694267272949
train gradient:  0.14711114189642488
iteration : 9254
train acc:  0.75
train loss:  0.42986854910850525
train gradient:  0.10454487130714818
iteration : 9255
train acc:  0.734375
train loss:  0.49158263206481934
train gradient:  0.13854537765305303
iteration : 9256
train acc:  0.6484375
train loss:  0.6002259254455566
train gradient:  0.1919707290094468
iteration : 9257
train acc:  0.75
train loss:  0.5251635313034058
train gradient:  0.1557131965837001
iteration : 9258
train acc:  0.7578125
train loss:  0.46862679719924927
train gradient:  0.15095330676984375
iteration : 9259
train acc:  0.8125
train loss:  0.4427194595336914
train gradient:  0.13173964158958557
iteration : 9260
train acc:  0.7265625
train loss:  0.5018501281738281
train gradient:  0.10990359317451341
iteration : 9261
train acc:  0.75
train loss:  0.5031708478927612
train gradient:  0.1753771454036896
iteration : 9262
train acc:  0.75
train loss:  0.4613950848579407
train gradient:  0.10063539848398771
iteration : 9263
train acc:  0.7890625
train loss:  0.4320898950099945
train gradient:  0.11383500888412622
iteration : 9264
train acc:  0.6875
train loss:  0.5534692406654358
train gradient:  0.15742203261325327
iteration : 9265
train acc:  0.6640625
train loss:  0.5699429512023926
train gradient:  0.15778292405486422
iteration : 9266
train acc:  0.75
train loss:  0.4868474006652832
train gradient:  0.15266801267984179
iteration : 9267
train acc:  0.7265625
train loss:  0.5205811858177185
train gradient:  0.1733420662868671
iteration : 9268
train acc:  0.734375
train loss:  0.5241451859474182
train gradient:  0.1989464366350122
iteration : 9269
train acc:  0.7421875
train loss:  0.49075156450271606
train gradient:  0.1230795250364932
iteration : 9270
train acc:  0.6953125
train loss:  0.5978558659553528
train gradient:  0.19187073101095614
iteration : 9271
train acc:  0.703125
train loss:  0.5464959740638733
train gradient:  0.14864798549855035
iteration : 9272
train acc:  0.796875
train loss:  0.43858563899993896
train gradient:  0.10607454657536954
iteration : 9273
train acc:  0.765625
train loss:  0.4567004144191742
train gradient:  0.13717014758855253
iteration : 9274
train acc:  0.7265625
train loss:  0.5154668092727661
train gradient:  0.17204026036797165
iteration : 9275
train acc:  0.796875
train loss:  0.4578903317451477
train gradient:  0.12953785100708243
iteration : 9276
train acc:  0.7265625
train loss:  0.5208930373191833
train gradient:  0.11175954646184458
iteration : 9277
train acc:  0.78125
train loss:  0.44096434116363525
train gradient:  0.11729637130374314
iteration : 9278
train acc:  0.796875
train loss:  0.4527914822101593
train gradient:  0.1219928005290374
iteration : 9279
train acc:  0.765625
train loss:  0.46924087405204773
train gradient:  0.12809346433961827
iteration : 9280
train acc:  0.7734375
train loss:  0.4632914960384369
train gradient:  0.09299381348581842
iteration : 9281
train acc:  0.75
train loss:  0.4954327940940857
train gradient:  0.17760512407482948
iteration : 9282
train acc:  0.8125
train loss:  0.4518755078315735
train gradient:  0.133933680036811
iteration : 9283
train acc:  0.703125
train loss:  0.49678391218185425
train gradient:  0.13041224943000998
iteration : 9284
train acc:  0.7578125
train loss:  0.491598904132843
train gradient:  0.15358571331022774
iteration : 9285
train acc:  0.8046875
train loss:  0.46433305740356445
train gradient:  0.13960761841063313
iteration : 9286
train acc:  0.7265625
train loss:  0.4985824525356293
train gradient:  0.1622418356626359
iteration : 9287
train acc:  0.703125
train loss:  0.5760294198989868
train gradient:  0.1619550291101327
iteration : 9288
train acc:  0.734375
train loss:  0.5203046798706055
train gradient:  0.13742692941903795
iteration : 9289
train acc:  0.7734375
train loss:  0.46546363830566406
train gradient:  0.10192671952724347
iteration : 9290
train acc:  0.734375
train loss:  0.4754047691822052
train gradient:  0.13493135852198038
iteration : 9291
train acc:  0.765625
train loss:  0.48604345321655273
train gradient:  0.17284273429943664
iteration : 9292
train acc:  0.7578125
train loss:  0.5318191051483154
train gradient:  0.14519388321466065
iteration : 9293
train acc:  0.734375
train loss:  0.5180277824401855
train gradient:  0.18887808320842414
iteration : 9294
train acc:  0.71875
train loss:  0.5621943473815918
train gradient:  0.18665084630778722
iteration : 9295
train acc:  0.75
train loss:  0.4437329173088074
train gradient:  0.1132134810298659
iteration : 9296
train acc:  0.75
train loss:  0.5106977820396423
train gradient:  0.11816107809933685
iteration : 9297
train acc:  0.8046875
train loss:  0.4453962743282318
train gradient:  0.12038114773369746
iteration : 9298
train acc:  0.6875
train loss:  0.5810748338699341
train gradient:  0.1542450281755267
iteration : 9299
train acc:  0.7265625
train loss:  0.5014261603355408
train gradient:  0.17513416241696528
iteration : 9300
train acc:  0.7734375
train loss:  0.5031842589378357
train gradient:  0.1608718091855897
iteration : 9301
train acc:  0.7890625
train loss:  0.44789668917655945
train gradient:  0.09907228513338223
iteration : 9302
train acc:  0.7421875
train loss:  0.5086559653282166
train gradient:  0.1412474123996446
iteration : 9303
train acc:  0.78125
train loss:  0.4602556824684143
train gradient:  0.0958255173741007
iteration : 9304
train acc:  0.7265625
train loss:  0.5137980580329895
train gradient:  0.1325801856282239
iteration : 9305
train acc:  0.703125
train loss:  0.5676743984222412
train gradient:  0.1837372642133755
iteration : 9306
train acc:  0.765625
train loss:  0.45299023389816284
train gradient:  0.13083028713440878
iteration : 9307
train acc:  0.703125
train loss:  0.47656458616256714
train gradient:  0.12534774437297577
iteration : 9308
train acc:  0.75
train loss:  0.5399482250213623
train gradient:  0.15516668872527428
iteration : 9309
train acc:  0.7890625
train loss:  0.5090125203132629
train gradient:  0.16095185227542358
iteration : 9310
train acc:  0.7578125
train loss:  0.4767235517501831
train gradient:  0.12028220625837922
iteration : 9311
train acc:  0.78125
train loss:  0.47380322217941284
train gradient:  0.12942764523186423
iteration : 9312
train acc:  0.7109375
train loss:  0.5138041377067566
train gradient:  0.19238762589663794
iteration : 9313
train acc:  0.75
train loss:  0.49309924244880676
train gradient:  0.15431358463921885
iteration : 9314
train acc:  0.734375
train loss:  0.5097707509994507
train gradient:  0.13206766908734527
iteration : 9315
train acc:  0.7265625
train loss:  0.5362966060638428
train gradient:  0.13491367689414582
iteration : 9316
train acc:  0.7734375
train loss:  0.4627307057380676
train gradient:  0.16623133361871134
iteration : 9317
train acc:  0.78125
train loss:  0.48925453424453735
train gradient:  0.11541246851479367
iteration : 9318
train acc:  0.796875
train loss:  0.43831580877304077
train gradient:  0.11325038384384513
iteration : 9319
train acc:  0.71875
train loss:  0.5494009852409363
train gradient:  0.14968426423119963
iteration : 9320
train acc:  0.734375
train loss:  0.5303033590316772
train gradient:  0.16521999377146612
iteration : 9321
train acc:  0.765625
train loss:  0.4726886749267578
train gradient:  0.09817479907957484
iteration : 9322
train acc:  0.7734375
train loss:  0.4639706611633301
train gradient:  0.12616857165789203
iteration : 9323
train acc:  0.7109375
train loss:  0.5468836426734924
train gradient:  0.14603027534701668
iteration : 9324
train acc:  0.734375
train loss:  0.493877649307251
train gradient:  0.12206755012006709
iteration : 9325
train acc:  0.7109375
train loss:  0.5165935754776001
train gradient:  0.1469316722540438
iteration : 9326
train acc:  0.765625
train loss:  0.4637294411659241
train gradient:  0.1371255896198103
iteration : 9327
train acc:  0.7421875
train loss:  0.510179877281189
train gradient:  0.11551505203088497
iteration : 9328
train acc:  0.765625
train loss:  0.4905705153942108
train gradient:  0.13129133217337677
iteration : 9329
train acc:  0.7578125
train loss:  0.5236170887947083
train gradient:  0.14479018724982412
iteration : 9330
train acc:  0.703125
train loss:  0.5330549478530884
train gradient:  0.1659742860503955
iteration : 9331
train acc:  0.6796875
train loss:  0.4988117814064026
train gradient:  0.15436274313474985
iteration : 9332
train acc:  0.6953125
train loss:  0.5544774532318115
train gradient:  0.16509628588251146
iteration : 9333
train acc:  0.7578125
train loss:  0.4724038243293762
train gradient:  0.11673387471970462
iteration : 9334
train acc:  0.734375
train loss:  0.5130749344825745
train gradient:  0.12369742927645389
iteration : 9335
train acc:  0.765625
train loss:  0.434478223323822
train gradient:  0.12232901666691963
iteration : 9336
train acc:  0.7734375
train loss:  0.4793841242790222
train gradient:  0.12158314822264692
iteration : 9337
train acc:  0.78125
train loss:  0.4894499182701111
train gradient:  0.15182621544327757
iteration : 9338
train acc:  0.796875
train loss:  0.49678149819374084
train gradient:  0.16521766108505942
iteration : 9339
train acc:  0.6796875
train loss:  0.5704304575920105
train gradient:  0.17616250186739663
iteration : 9340
train acc:  0.6875
train loss:  0.5120031237602234
train gradient:  0.1663824894740772
iteration : 9341
train acc:  0.7578125
train loss:  0.49111443758010864
train gradient:  0.1626701408010348
iteration : 9342
train acc:  0.7265625
train loss:  0.47007015347480774
train gradient:  0.1404031588043734
iteration : 9343
train acc:  0.71875
train loss:  0.5862011909484863
train gradient:  0.1762291569480483
iteration : 9344
train acc:  0.7109375
train loss:  0.5473673939704895
train gradient:  0.18919386103415553
iteration : 9345
train acc:  0.75
train loss:  0.509056568145752
train gradient:  0.12917867274143424
iteration : 9346
train acc:  0.7109375
train loss:  0.5430173873901367
train gradient:  0.13675656321214336
iteration : 9347
train acc:  0.78125
train loss:  0.48781469464302063
train gradient:  0.15713692714332012
iteration : 9348
train acc:  0.71875
train loss:  0.5709785223007202
train gradient:  0.16567954838266247
iteration : 9349
train acc:  0.671875
train loss:  0.5727546215057373
train gradient:  0.16153249501278955
iteration : 9350
train acc:  0.71875
train loss:  0.5073184967041016
train gradient:  0.11610809941965478
iteration : 9351
train acc:  0.7265625
train loss:  0.53437739610672
train gradient:  0.16142724893518695
iteration : 9352
train acc:  0.6953125
train loss:  0.5118770599365234
train gradient:  0.1339077222668249
iteration : 9353
train acc:  0.734375
train loss:  0.5492101907730103
train gradient:  0.16908788350332946
iteration : 9354
train acc:  0.765625
train loss:  0.475441575050354
train gradient:  0.12485641965983454
iteration : 9355
train acc:  0.71875
train loss:  0.49352341890335083
train gradient:  0.12627602070059288
iteration : 9356
train acc:  0.7734375
train loss:  0.45692020654678345
train gradient:  0.11794520206895828
iteration : 9357
train acc:  0.7734375
train loss:  0.4194415211677551
train gradient:  0.11143117124091115
iteration : 9358
train acc:  0.7421875
train loss:  0.5072024464607239
train gradient:  0.13114780160004202
iteration : 9359
train acc:  0.7109375
train loss:  0.5120506286621094
train gradient:  0.1205425508691381
iteration : 9360
train acc:  0.78125
train loss:  0.443203330039978
train gradient:  0.0965689079433212
iteration : 9361
train acc:  0.7421875
train loss:  0.5138684511184692
train gradient:  0.11865544120673609
iteration : 9362
train acc:  0.7109375
train loss:  0.5048302412033081
train gradient:  0.12934298526526009
iteration : 9363
train acc:  0.703125
train loss:  0.5402357578277588
train gradient:  0.14362826317258895
iteration : 9364
train acc:  0.7734375
train loss:  0.4580923616886139
train gradient:  0.10742981485761346
iteration : 9365
train acc:  0.6640625
train loss:  0.5691496133804321
train gradient:  0.17573460197246638
iteration : 9366
train acc:  0.671875
train loss:  0.5526347160339355
train gradient:  0.15989289974724394
iteration : 9367
train acc:  0.8125
train loss:  0.4339069724082947
train gradient:  0.11390440537209393
iteration : 9368
train acc:  0.7109375
train loss:  0.5392130017280579
train gradient:  0.1289288673530095
iteration : 9369
train acc:  0.7890625
train loss:  0.43982452154159546
train gradient:  0.11091212348148514
iteration : 9370
train acc:  0.734375
train loss:  0.4750669598579407
train gradient:  0.10531144629793292
iteration : 9371
train acc:  0.7421875
train loss:  0.5013577938079834
train gradient:  0.12429030346653726
iteration : 9372
train acc:  0.7578125
train loss:  0.4296785593032837
train gradient:  0.117387616493557
iteration : 9373
train acc:  0.734375
train loss:  0.49989986419677734
train gradient:  0.13779886029148425
iteration : 9374
train acc:  0.8046875
train loss:  0.4260554909706116
train gradient:  0.09254009033062528
iteration : 9375
train acc:  0.75
train loss:  0.507975697517395
train gradient:  0.1576423669662994
iteration : 9376
train acc:  0.6953125
train loss:  0.5387526750564575
train gradient:  0.15079240212282063
iteration : 9377
train acc:  0.7421875
train loss:  0.4875684678554535
train gradient:  0.11963319303102403
iteration : 9378
train acc:  0.7890625
train loss:  0.4289066791534424
train gradient:  0.141043340589817
iteration : 9379
train acc:  0.828125
train loss:  0.4787805676460266
train gradient:  0.12473221621400139
iteration : 9380
train acc:  0.75
train loss:  0.4551360309123993
train gradient:  0.12378738230478664
iteration : 9381
train acc:  0.7421875
train loss:  0.5192306041717529
train gradient:  0.11805394825877098
iteration : 9382
train acc:  0.7734375
train loss:  0.4604479968547821
train gradient:  0.09719175911343099
iteration : 9383
train acc:  0.7265625
train loss:  0.49692803621292114
train gradient:  0.1013974349590734
iteration : 9384
train acc:  0.7109375
train loss:  0.5202592611312866
train gradient:  0.14856063832205796
iteration : 9385
train acc:  0.8203125
train loss:  0.38754111528396606
train gradient:  0.09182594819973869
iteration : 9386
train acc:  0.7421875
train loss:  0.4974583685398102
train gradient:  0.13527748059034622
iteration : 9387
train acc:  0.78125
train loss:  0.40823620557785034
train gradient:  0.0967463671637024
iteration : 9388
train acc:  0.765625
train loss:  0.44229066371917725
train gradient:  0.10054788450581856
iteration : 9389
train acc:  0.7421875
train loss:  0.4955157935619354
train gradient:  0.12631122618489227
iteration : 9390
train acc:  0.765625
train loss:  0.42608344554901123
train gradient:  0.09576079484925375
iteration : 9391
train acc:  0.765625
train loss:  0.47829946875572205
train gradient:  0.11552076399581618
iteration : 9392
train acc:  0.7109375
train loss:  0.5210556983947754
train gradient:  0.14536796444639868
iteration : 9393
train acc:  0.796875
train loss:  0.4675663113594055
train gradient:  0.15066622867707288
iteration : 9394
train acc:  0.75
train loss:  0.4541547894477844
train gradient:  0.11983935969615836
iteration : 9395
train acc:  0.7265625
train loss:  0.5199715495109558
train gradient:  0.17534286876237154
iteration : 9396
train acc:  0.765625
train loss:  0.49113982915878296
train gradient:  0.1395928208421891
iteration : 9397
train acc:  0.75
train loss:  0.5290367603302002
train gradient:  0.15605256928314704
iteration : 9398
train acc:  0.7265625
train loss:  0.50335294008255
train gradient:  0.1167390163111453
iteration : 9399
train acc:  0.671875
train loss:  0.5507417917251587
train gradient:  0.16341624041833414
iteration : 9400
train acc:  0.7890625
train loss:  0.42462557554244995
train gradient:  0.09528496048822974
iteration : 9401
train acc:  0.7109375
train loss:  0.5025099515914917
train gradient:  0.11840937007980826
iteration : 9402
train acc:  0.6953125
train loss:  0.5916620492935181
train gradient:  0.17594224813419496
iteration : 9403
train acc:  0.71875
train loss:  0.5044224262237549
train gradient:  0.1419004691787027
iteration : 9404
train acc:  0.75
train loss:  0.5120171904563904
train gradient:  0.14525204078820025
iteration : 9405
train acc:  0.734375
train loss:  0.5599373579025269
train gradient:  0.1492646328031162
iteration : 9406
train acc:  0.7421875
train loss:  0.4821516275405884
train gradient:  0.10103225096453829
iteration : 9407
train acc:  0.6484375
train loss:  0.5565989017486572
train gradient:  0.18194039223102182
iteration : 9408
train acc:  0.703125
train loss:  0.5564998388290405
train gradient:  0.1436430850647703
iteration : 9409
train acc:  0.71875
train loss:  0.5210548639297485
train gradient:  0.11159454761420105
iteration : 9410
train acc:  0.703125
train loss:  0.5384561419487
train gradient:  0.17113371250976497
iteration : 9411
train acc:  0.796875
train loss:  0.4416240155696869
train gradient:  0.09616681148606654
iteration : 9412
train acc:  0.7421875
train loss:  0.5135974884033203
train gradient:  0.1270404642622709
iteration : 9413
train acc:  0.75
train loss:  0.5046485662460327
train gradient:  0.12881813564342023
iteration : 9414
train acc:  0.765625
train loss:  0.4865345060825348
train gradient:  0.11597644106949465
iteration : 9415
train acc:  0.75
train loss:  0.48999011516571045
train gradient:  0.11835152393784011
iteration : 9416
train acc:  0.6875
train loss:  0.5181836485862732
train gradient:  0.1444364829512173
iteration : 9417
train acc:  0.75
train loss:  0.49493539333343506
train gradient:  0.12181534454061389
iteration : 9418
train acc:  0.7109375
train loss:  0.5913950204849243
train gradient:  0.18536517550037585
iteration : 9419
train acc:  0.7421875
train loss:  0.5108803510665894
train gradient:  0.1253392642935517
iteration : 9420
train acc:  0.78125
train loss:  0.4462105333805084
train gradient:  0.11439627226007734
iteration : 9421
train acc:  0.6484375
train loss:  0.5857623815536499
train gradient:  0.16355640634617785
iteration : 9422
train acc:  0.734375
train loss:  0.5165163278579712
train gradient:  0.1374566245452063
iteration : 9423
train acc:  0.8046875
train loss:  0.47926557064056396
train gradient:  0.15248936453883316
iteration : 9424
train acc:  0.6796875
train loss:  0.5793874263763428
train gradient:  0.1661744688698354
iteration : 9425
train acc:  0.7265625
train loss:  0.5418145060539246
train gradient:  0.1870536462960697
iteration : 9426
train acc:  0.71875
train loss:  0.5127068758010864
train gradient:  0.12794280592394675
iteration : 9427
train acc:  0.8203125
train loss:  0.4300897717475891
train gradient:  0.0932092873114205
iteration : 9428
train acc:  0.7734375
train loss:  0.4544089436531067
train gradient:  0.104426775949859
iteration : 9429
train acc:  0.765625
train loss:  0.5040477514266968
train gradient:  0.13585940975159555
iteration : 9430
train acc:  0.7734375
train loss:  0.5119619965553284
train gradient:  0.12248584462914226
iteration : 9431
train acc:  0.7265625
train loss:  0.46255066990852356
train gradient:  0.13320382799030975
iteration : 9432
train acc:  0.7890625
train loss:  0.4189552962779999
train gradient:  0.10757173215037182
iteration : 9433
train acc:  0.734375
train loss:  0.49402037262916565
train gradient:  0.12245392943081393
iteration : 9434
train acc:  0.6640625
train loss:  0.5263831615447998
train gradient:  0.14181191460199633
iteration : 9435
train acc:  0.7734375
train loss:  0.46867865324020386
train gradient:  0.1504816708588551
iteration : 9436
train acc:  0.7265625
train loss:  0.5492551922798157
train gradient:  0.1548831897917629
iteration : 9437
train acc:  0.734375
train loss:  0.5184849500656128
train gradient:  0.16544867615919379
iteration : 9438
train acc:  0.7734375
train loss:  0.4866710901260376
train gradient:  0.11530203916672215
iteration : 9439
train acc:  0.734375
train loss:  0.5236226916313171
train gradient:  0.15226568312315952
iteration : 9440
train acc:  0.7265625
train loss:  0.4955519139766693
train gradient:  0.12029137428700397
iteration : 9441
train acc:  0.7421875
train loss:  0.5001029968261719
train gradient:  0.15632902012682595
iteration : 9442
train acc:  0.6640625
train loss:  0.5672084093093872
train gradient:  0.14242353283240372
iteration : 9443
train acc:  0.8125
train loss:  0.43022722005844116
train gradient:  0.11686877000508716
iteration : 9444
train acc:  0.7109375
train loss:  0.5434825420379639
train gradient:  0.14910144557996172
iteration : 9445
train acc:  0.7421875
train loss:  0.4768826365470886
train gradient:  0.09158845243786053
iteration : 9446
train acc:  0.78125
train loss:  0.42668479681015015
train gradient:  0.09745401944552111
iteration : 9447
train acc:  0.796875
train loss:  0.440948486328125
train gradient:  0.11564785349278275
iteration : 9448
train acc:  0.7265625
train loss:  0.45838460326194763
train gradient:  0.11919167421286675
iteration : 9449
train acc:  0.6796875
train loss:  0.4903090000152588
train gradient:  0.12465939313588449
iteration : 9450
train acc:  0.71875
train loss:  0.5410194993019104
train gradient:  0.16858607766994288
iteration : 9451
train acc:  0.7109375
train loss:  0.5117625594139099
train gradient:  0.14855785940239277
iteration : 9452
train acc:  0.7421875
train loss:  0.5136370658874512
train gradient:  0.12490528849120304
iteration : 9453
train acc:  0.75
train loss:  0.4655854105949402
train gradient:  0.10121872611353465
iteration : 9454
train acc:  0.765625
train loss:  0.42902201414108276
train gradient:  0.095843607216305
iteration : 9455
train acc:  0.7578125
train loss:  0.460336297750473
train gradient:  0.1462955390745397
iteration : 9456
train acc:  0.7578125
train loss:  0.4779248833656311
train gradient:  0.12104114141873136
iteration : 9457
train acc:  0.7578125
train loss:  0.48575806617736816
train gradient:  0.13214190176608057
iteration : 9458
train acc:  0.765625
train loss:  0.4047386050224304
train gradient:  0.07930540979825361
iteration : 9459
train acc:  0.8125
train loss:  0.45969271659851074
train gradient:  0.1629793066695256
iteration : 9460
train acc:  0.796875
train loss:  0.43203482031822205
train gradient:  0.10231362069704138
iteration : 9461
train acc:  0.6953125
train loss:  0.5329070091247559
train gradient:  0.15515465510842558
iteration : 9462
train acc:  0.703125
train loss:  0.5121941566467285
train gradient:  0.1626013191086474
iteration : 9463
train acc:  0.7578125
train loss:  0.4687941074371338
train gradient:  0.10331733751309348
iteration : 9464
train acc:  0.75
train loss:  0.49713897705078125
train gradient:  0.17485045838537142
iteration : 9465
train acc:  0.65625
train loss:  0.6374014616012573
train gradient:  0.18100780467889524
iteration : 9466
train acc:  0.71875
train loss:  0.5523019433021545
train gradient:  0.20367305358567728
iteration : 9467
train acc:  0.7421875
train loss:  0.48899123072624207
train gradient:  0.12725058235346276
iteration : 9468
train acc:  0.75
train loss:  0.46618643403053284
train gradient:  0.10652941634790025
iteration : 9469
train acc:  0.7265625
train loss:  0.4904837906360626
train gradient:  0.1576358195924113
iteration : 9470
train acc:  0.7734375
train loss:  0.45222827792167664
train gradient:  0.10654587310723156
iteration : 9471
train acc:  0.765625
train loss:  0.4878968298435211
train gradient:  0.12445369333942506
iteration : 9472
train acc:  0.7890625
train loss:  0.4534744918346405
train gradient:  0.11101179407243951
iteration : 9473
train acc:  0.7421875
train loss:  0.4885537326335907
train gradient:  0.13739970631095993
iteration : 9474
train acc:  0.734375
train loss:  0.4938684403896332
train gradient:  0.11652791172174015
iteration : 9475
train acc:  0.734375
train loss:  0.5084603428840637
train gradient:  0.13520144423551306
iteration : 9476
train acc:  0.7265625
train loss:  0.5314851403236389
train gradient:  0.17030946166671157
iteration : 9477
train acc:  0.71875
train loss:  0.5409783720970154
train gradient:  0.15028122976463612
iteration : 9478
train acc:  0.7421875
train loss:  0.4943608045578003
train gradient:  0.12707377465693354
iteration : 9479
train acc:  0.796875
train loss:  0.4503883123397827
train gradient:  0.12811204099650517
iteration : 9480
train acc:  0.75
train loss:  0.46313977241516113
train gradient:  0.11273858242440042
iteration : 9481
train acc:  0.703125
train loss:  0.6233945488929749
train gradient:  0.21276074630856603
iteration : 9482
train acc:  0.71875
train loss:  0.5343334674835205
train gradient:  0.1619872484917649
iteration : 9483
train acc:  0.7578125
train loss:  0.47778448462486267
train gradient:  0.13041855442703507
iteration : 9484
train acc:  0.78125
train loss:  0.438396155834198
train gradient:  0.10631517666767591
iteration : 9485
train acc:  0.7578125
train loss:  0.4392070770263672
train gradient:  0.12323714509603656
iteration : 9486
train acc:  0.7109375
train loss:  0.5633873343467712
train gradient:  0.1724126669702875
iteration : 9487
train acc:  0.703125
train loss:  0.5414361357688904
train gradient:  0.13524613169580046
iteration : 9488
train acc:  0.7265625
train loss:  0.4966053366661072
train gradient:  0.14150580854435835
iteration : 9489
train acc:  0.7265625
train loss:  0.49527859687805176
train gradient:  0.14463532595389886
iteration : 9490
train acc:  0.7578125
train loss:  0.48959869146347046
train gradient:  0.1465882262728941
iteration : 9491
train acc:  0.7421875
train loss:  0.4838849902153015
train gradient:  0.11853651082308239
iteration : 9492
train acc:  0.6953125
train loss:  0.5160524249076843
train gradient:  0.12342952361959968
iteration : 9493
train acc:  0.71875
train loss:  0.5345021486282349
train gradient:  0.16813106563582597
iteration : 9494
train acc:  0.7734375
train loss:  0.47025805711746216
train gradient:  0.09759338365570608
iteration : 9495
train acc:  0.75
train loss:  0.4964844286441803
train gradient:  0.12655118693937734
iteration : 9496
train acc:  0.7265625
train loss:  0.5510041117668152
train gradient:  0.14856240025186018
iteration : 9497
train acc:  0.7734375
train loss:  0.4414706230163574
train gradient:  0.08610340257984776
iteration : 9498
train acc:  0.765625
train loss:  0.507304310798645
train gradient:  0.16009623412186824
iteration : 9499
train acc:  0.765625
train loss:  0.45399248600006104
train gradient:  0.148855992627214
iteration : 9500
train acc:  0.703125
train loss:  0.5112112760543823
train gradient:  0.12245884534119322
iteration : 9501
train acc:  0.671875
train loss:  0.5692726373672485
train gradient:  0.17352829514020673
iteration : 9502
train acc:  0.7578125
train loss:  0.5168631076812744
train gradient:  0.11783175116338289
iteration : 9503
train acc:  0.703125
train loss:  0.5408185720443726
train gradient:  0.1623266887268722
iteration : 9504
train acc:  0.71875
train loss:  0.5612285733222961
train gradient:  0.14794856273319462
iteration : 9505
train acc:  0.7265625
train loss:  0.5335884094238281
train gradient:  0.16047259534567287
iteration : 9506
train acc:  0.7421875
train loss:  0.4898938536643982
train gradient:  0.11074408614539574
iteration : 9507
train acc:  0.765625
train loss:  0.4686095118522644
train gradient:  0.12581948877071997
iteration : 9508
train acc:  0.703125
train loss:  0.5304971933364868
train gradient:  0.14582915867126978
iteration : 9509
train acc:  0.7421875
train loss:  0.49509337544441223
train gradient:  0.1343756887400468
iteration : 9510
train acc:  0.75
train loss:  0.511610746383667
train gradient:  0.1076308678113912
iteration : 9511
train acc:  0.734375
train loss:  0.530717134475708
train gradient:  0.1341355250374449
iteration : 9512
train acc:  0.7421875
train loss:  0.5058720111846924
train gradient:  0.15399029110459111
iteration : 9513
train acc:  0.7421875
train loss:  0.4905744791030884
train gradient:  0.10741144350503037
iteration : 9514
train acc:  0.7734375
train loss:  0.44524019956588745
train gradient:  0.09576143367507292
iteration : 9515
train acc:  0.71875
train loss:  0.5057095289230347
train gradient:  0.1660971488312864
iteration : 9516
train acc:  0.734375
train loss:  0.45212939381599426
train gradient:  0.13915856553393602
iteration : 9517
train acc:  0.7421875
train loss:  0.47598180174827576
train gradient:  0.1361732590344752
iteration : 9518
train acc:  0.71875
train loss:  0.53910231590271
train gradient:  0.15789083447539587
iteration : 9519
train acc:  0.734375
train loss:  0.5258563756942749
train gradient:  0.272035939882159
iteration : 9520
train acc:  0.7265625
train loss:  0.47750768065452576
train gradient:  0.11592898368169503
iteration : 9521
train acc:  0.78125
train loss:  0.430368572473526
train gradient:  0.104068453924079
iteration : 9522
train acc:  0.6875
train loss:  0.5445564389228821
train gradient:  0.13605159914226006
iteration : 9523
train acc:  0.78125
train loss:  0.4408320188522339
train gradient:  0.11952409609243585
iteration : 9524
train acc:  0.703125
train loss:  0.5315747857093811
train gradient:  0.17730670386094205
iteration : 9525
train acc:  0.6796875
train loss:  0.564670205116272
train gradient:  0.19765181969717766
iteration : 9526
train acc:  0.703125
train loss:  0.5092626810073853
train gradient:  0.1330927644640636
iteration : 9527
train acc:  0.7734375
train loss:  0.5258479118347168
train gradient:  0.16321822068339553
iteration : 9528
train acc:  0.78125
train loss:  0.4562833309173584
train gradient:  0.12648228545026952
iteration : 9529
train acc:  0.7109375
train loss:  0.5222839117050171
train gradient:  0.14298577963308393
iteration : 9530
train acc:  0.75
train loss:  0.44242456555366516
train gradient:  0.11377886768581524
iteration : 9531
train acc:  0.7421875
train loss:  0.4582820534706116
train gradient:  0.13269860681920992
iteration : 9532
train acc:  0.734375
train loss:  0.48410889506340027
train gradient:  0.1143406051237397
iteration : 9533
train acc:  0.6484375
train loss:  0.5618513822555542
train gradient:  0.14301761361846987
iteration : 9534
train acc:  0.734375
train loss:  0.4808587431907654
train gradient:  0.13556706102067068
iteration : 9535
train acc:  0.7890625
train loss:  0.4522547423839569
train gradient:  0.10465288073507471
iteration : 9536
train acc:  0.765625
train loss:  0.48295944929122925
train gradient:  0.14081783164157033
iteration : 9537
train acc:  0.71875
train loss:  0.49371814727783203
train gradient:  0.11373652327270128
iteration : 9538
train acc:  0.7109375
train loss:  0.4950583577156067
train gradient:  0.13602236377056268
iteration : 9539
train acc:  0.75
train loss:  0.474437952041626
train gradient:  0.14456473283222737
iteration : 9540
train acc:  0.75
train loss:  0.4695996642112732
train gradient:  0.1231819268627268
iteration : 9541
train acc:  0.734375
train loss:  0.47405344247817993
train gradient:  0.11968872426427517
iteration : 9542
train acc:  0.8125
train loss:  0.4159766435623169
train gradient:  0.09016706401610776
iteration : 9543
train acc:  0.75
train loss:  0.48946285247802734
train gradient:  0.11692917275194627
iteration : 9544
train acc:  0.71875
train loss:  0.5021325945854187
train gradient:  0.09505558221357122
iteration : 9545
train acc:  0.78125
train loss:  0.46032485365867615
train gradient:  0.1245781531029302
iteration : 9546
train acc:  0.703125
train loss:  0.5435652732849121
train gradient:  0.1459971903929572
iteration : 9547
train acc:  0.640625
train loss:  0.6204372644424438
train gradient:  0.21445234490792037
iteration : 9548
train acc:  0.6875
train loss:  0.5367522239685059
train gradient:  0.12252402601723243
iteration : 9549
train acc:  0.84375
train loss:  0.4152817726135254
train gradient:  0.07845400834879973
iteration : 9550
train acc:  0.7421875
train loss:  0.4992687702178955
train gradient:  0.12985209943249665
iteration : 9551
train acc:  0.765625
train loss:  0.47721371054649353
train gradient:  0.11403970611766619
iteration : 9552
train acc:  0.6640625
train loss:  0.5730990171432495
train gradient:  0.15426133256126678
iteration : 9553
train acc:  0.796875
train loss:  0.5014566779136658
train gradient:  0.13751219718864072
iteration : 9554
train acc:  0.765625
train loss:  0.43705612421035767
train gradient:  0.1366704780193573
iteration : 9555
train acc:  0.625
train loss:  0.6074395179748535
train gradient:  0.19647110198659948
iteration : 9556
train acc:  0.6953125
train loss:  0.5507235527038574
train gradient:  0.1641441410800406
iteration : 9557
train acc:  0.6484375
train loss:  0.578803539276123
train gradient:  0.16878775585415157
iteration : 9558
train acc:  0.7734375
train loss:  0.46046096086502075
train gradient:  0.09043128402636276
iteration : 9559
train acc:  0.6953125
train loss:  0.5550187230110168
train gradient:  0.14933350696625902
iteration : 9560
train acc:  0.7890625
train loss:  0.4793848395347595
train gradient:  0.1282336689405822
iteration : 9561
train acc:  0.734375
train loss:  0.511222243309021
train gradient:  0.11072898874887528
iteration : 9562
train acc:  0.7421875
train loss:  0.4797992706298828
train gradient:  0.14210992216575727
iteration : 9563
train acc:  0.796875
train loss:  0.4515775144100189
train gradient:  0.1470498716304675
iteration : 9564
train acc:  0.7265625
train loss:  0.4953174889087677
train gradient:  0.15543818674560117
iteration : 9565
train acc:  0.71875
train loss:  0.5418381690979004
train gradient:  0.1681736183122466
iteration : 9566
train acc:  0.7421875
train loss:  0.502300500869751
train gradient:  0.1288003670252834
iteration : 9567
train acc:  0.703125
train loss:  0.5128887891769409
train gradient:  0.10408311321259707
iteration : 9568
train acc:  0.71875
train loss:  0.518844723701477
train gradient:  0.144697695489024
iteration : 9569
train acc:  0.734375
train loss:  0.4740316867828369
train gradient:  0.12460491962417433
iteration : 9570
train acc:  0.828125
train loss:  0.44380706548690796
train gradient:  0.10396948235398967
iteration : 9571
train acc:  0.734375
train loss:  0.505113959312439
train gradient:  0.16902215064650558
iteration : 9572
train acc:  0.7421875
train loss:  0.45610666275024414
train gradient:  0.11500658460622924
iteration : 9573
train acc:  0.71875
train loss:  0.5517623424530029
train gradient:  0.1433529401223363
iteration : 9574
train acc:  0.71875
train loss:  0.49772828817367554
train gradient:  0.12977991336339376
iteration : 9575
train acc:  0.7109375
train loss:  0.5357016324996948
train gradient:  0.11487925930859293
iteration : 9576
train acc:  0.7421875
train loss:  0.5047762989997864
train gradient:  0.1351857805055813
iteration : 9577
train acc:  0.75
train loss:  0.5055632591247559
train gradient:  0.11676483470677558
iteration : 9578
train acc:  0.734375
train loss:  0.5475986003875732
train gradient:  0.13737754420722564
iteration : 9579
train acc:  0.765625
train loss:  0.47861015796661377
train gradient:  0.13026618159572156
iteration : 9580
train acc:  0.75
train loss:  0.5044387578964233
train gradient:  0.11746087774958354
iteration : 9581
train acc:  0.7578125
train loss:  0.4754641056060791
train gradient:  0.16882622697912036
iteration : 9582
train acc:  0.7109375
train loss:  0.49064719676971436
train gradient:  0.11333813692988245
iteration : 9583
train acc:  0.75
train loss:  0.45350024104118347
train gradient:  0.10413824166089258
iteration : 9584
train acc:  0.796875
train loss:  0.4526022672653198
train gradient:  0.1133344509139572
iteration : 9585
train acc:  0.7109375
train loss:  0.5215778350830078
train gradient:  0.11881502369757521
iteration : 9586
train acc:  0.6484375
train loss:  0.5614291429519653
train gradient:  0.1852368366503954
iteration : 9587
train acc:  0.671875
train loss:  0.532988429069519
train gradient:  0.15798137104719195
iteration : 9588
train acc:  0.734375
train loss:  0.49418920278549194
train gradient:  0.11524838046201574
iteration : 9589
train acc:  0.765625
train loss:  0.45489630103111267
train gradient:  0.12640328039403792
iteration : 9590
train acc:  0.78125
train loss:  0.47755876183509827
train gradient:  0.11957654250344374
iteration : 9591
train acc:  0.7578125
train loss:  0.4614008665084839
train gradient:  0.0907631530150548
iteration : 9592
train acc:  0.7890625
train loss:  0.44383788108825684
train gradient:  0.12693011558897627
iteration : 9593
train acc:  0.7734375
train loss:  0.46580398082733154
train gradient:  0.09411277156016024
iteration : 9594
train acc:  0.71875
train loss:  0.5153811573982239
train gradient:  0.14684124031273094
iteration : 9595
train acc:  0.7890625
train loss:  0.4423428177833557
train gradient:  0.15376763555973977
iteration : 9596
train acc:  0.6796875
train loss:  0.5281003713607788
train gradient:  0.1545502336604323
iteration : 9597
train acc:  0.7265625
train loss:  0.5023409128189087
train gradient:  0.12288254619103059
iteration : 9598
train acc:  0.7734375
train loss:  0.45877355337142944
train gradient:  0.11022128646479552
iteration : 9599
train acc:  0.6953125
train loss:  0.6210622787475586
train gradient:  0.17292799162977185
iteration : 9600
train acc:  0.7578125
train loss:  0.4869804382324219
train gradient:  0.13553106782220334
iteration : 9601
train acc:  0.796875
train loss:  0.4977045953273773
train gradient:  0.13953198000400122
iteration : 9602
train acc:  0.796875
train loss:  0.45506203174591064
train gradient:  0.09709789562664196
iteration : 9603
train acc:  0.71875
train loss:  0.5160706639289856
train gradient:  0.1409682042345341
iteration : 9604
train acc:  0.796875
train loss:  0.43867820501327515
train gradient:  0.12087590407928754
iteration : 9605
train acc:  0.71875
train loss:  0.554122269153595
train gradient:  0.16770056425361146
iteration : 9606
train acc:  0.7734375
train loss:  0.44420450925827026
train gradient:  0.0936288086778361
iteration : 9607
train acc:  0.796875
train loss:  0.44215601682662964
train gradient:  0.10688010899724701
iteration : 9608
train acc:  0.7265625
train loss:  0.4798930883407593
train gradient:  0.10139112229807945
iteration : 9609
train acc:  0.7578125
train loss:  0.4905085265636444
train gradient:  0.10816524390647726
iteration : 9610
train acc:  0.796875
train loss:  0.42737719416618347
train gradient:  0.11202733260087606
iteration : 9611
train acc:  0.7265625
train loss:  0.4804157614707947
train gradient:  0.13087907139666524
iteration : 9612
train acc:  0.71875
train loss:  0.487866073846817
train gradient:  0.12658174618368576
iteration : 9613
train acc:  0.703125
train loss:  0.5151157379150391
train gradient:  0.1369726742426321
iteration : 9614
train acc:  0.7109375
train loss:  0.521108865737915
train gradient:  0.16831856391355315
iteration : 9615
train acc:  0.8125
train loss:  0.45158129930496216
train gradient:  0.10742170572402748
iteration : 9616
train acc:  0.7421875
train loss:  0.4762524962425232
train gradient:  0.10203714066702638
iteration : 9617
train acc:  0.7734375
train loss:  0.45897287130355835
train gradient:  0.1295787826787631
iteration : 9618
train acc:  0.7578125
train loss:  0.4780949354171753
train gradient:  0.10849638435139601
iteration : 9619
train acc:  0.7421875
train loss:  0.5141019821166992
train gradient:  0.15164245342628652
iteration : 9620
train acc:  0.7109375
train loss:  0.5083507895469666
train gradient:  0.1328308179942665
iteration : 9621
train acc:  0.8203125
train loss:  0.37792161107063293
train gradient:  0.07641528931899337
iteration : 9622
train acc:  0.7265625
train loss:  0.5202299356460571
train gradient:  0.14070944373213165
iteration : 9623
train acc:  0.7578125
train loss:  0.4691755771636963
train gradient:  0.11503674694920388
iteration : 9624
train acc:  0.75
train loss:  0.43675389885902405
train gradient:  0.08787963006926051
iteration : 9625
train acc:  0.7890625
train loss:  0.4636579751968384
train gradient:  0.11686454942902262
iteration : 9626
train acc:  0.7421875
train loss:  0.5092341899871826
train gradient:  0.13989685974912475
iteration : 9627
train acc:  0.796875
train loss:  0.40696683526039124
train gradient:  0.08879868275276974
iteration : 9628
train acc:  0.640625
train loss:  0.5609492063522339
train gradient:  0.15112158914036689
iteration : 9629
train acc:  0.7109375
train loss:  0.5124925374984741
train gradient:  0.14393384942259135
iteration : 9630
train acc:  0.703125
train loss:  0.5564402341842651
train gradient:  0.21046054974429246
iteration : 9631
train acc:  0.765625
train loss:  0.4651990234851837
train gradient:  0.11206883446649275
iteration : 9632
train acc:  0.8125
train loss:  0.42624151706695557
train gradient:  0.10085215176209854
iteration : 9633
train acc:  0.796875
train loss:  0.4414439797401428
train gradient:  0.09204214608634495
iteration : 9634
train acc:  0.6953125
train loss:  0.5682523250579834
train gradient:  0.1531303383212016
iteration : 9635
train acc:  0.71875
train loss:  0.48574697971343994
train gradient:  0.13998996138761155
iteration : 9636
train acc:  0.6796875
train loss:  0.5428985953330994
train gradient:  0.13807180674323827
iteration : 9637
train acc:  0.734375
train loss:  0.49933797121047974
train gradient:  0.11353459297326907
iteration : 9638
train acc:  0.7734375
train loss:  0.4422592520713806
train gradient:  0.12738738241681918
iteration : 9639
train acc:  0.6953125
train loss:  0.5174614191055298
train gradient:  0.15339633974946743
iteration : 9640
train acc:  0.7578125
train loss:  0.4532621502876282
train gradient:  0.11174651441092412
iteration : 9641
train acc:  0.734375
train loss:  0.5147813558578491
train gradient:  0.13601811349481902
iteration : 9642
train acc:  0.765625
train loss:  0.4751773476600647
train gradient:  0.14573006900439062
iteration : 9643
train acc:  0.6796875
train loss:  0.5544803142547607
train gradient:  0.18703471868761656
iteration : 9644
train acc:  0.78125
train loss:  0.5218564867973328
train gradient:  0.15132022414999696
iteration : 9645
train acc:  0.8125
train loss:  0.41401225328445435
train gradient:  0.11240396284131281
iteration : 9646
train acc:  0.71875
train loss:  0.5325404405593872
train gradient:  0.14472813247700464
iteration : 9647
train acc:  0.6953125
train loss:  0.5471988320350647
train gradient:  0.12673466932270044
iteration : 9648
train acc:  0.7578125
train loss:  0.5406540036201477
train gradient:  0.15836459568188105
iteration : 9649
train acc:  0.765625
train loss:  0.5217390656471252
train gradient:  0.14224084666643724
iteration : 9650
train acc:  0.7421875
train loss:  0.4928729832172394
train gradient:  0.14566552588707318
iteration : 9651
train acc:  0.71875
train loss:  0.5131831169128418
train gradient:  0.1563872970054907
iteration : 9652
train acc:  0.7734375
train loss:  0.47340017557144165
train gradient:  0.12004414705246386
iteration : 9653
train acc:  0.6875
train loss:  0.5806920528411865
train gradient:  0.1716539385362512
iteration : 9654
train acc:  0.6953125
train loss:  0.5641999244689941
train gradient:  0.17503974045354337
iteration : 9655
train acc:  0.71875
train loss:  0.5300896763801575
train gradient:  0.14958823455106293
iteration : 9656
train acc:  0.828125
train loss:  0.42917400598526
train gradient:  0.11165269608281185
iteration : 9657
train acc:  0.78125
train loss:  0.45345813035964966
train gradient:  0.08563969112684731
iteration : 9658
train acc:  0.7265625
train loss:  0.5555433630943298
train gradient:  0.13512481371198454
iteration : 9659
train acc:  0.75
train loss:  0.4806295335292816
train gradient:  0.14521375593453023
iteration : 9660
train acc:  0.8125
train loss:  0.46569156646728516
train gradient:  0.10604141279289407
iteration : 9661
train acc:  0.7421875
train loss:  0.5170425772666931
train gradient:  0.11719868658853197
iteration : 9662
train acc:  0.7890625
train loss:  0.4468511939048767
train gradient:  0.0972760296188747
iteration : 9663
train acc:  0.7421875
train loss:  0.462199866771698
train gradient:  0.11815662684298754
iteration : 9664
train acc:  0.7734375
train loss:  0.48324155807495117
train gradient:  0.11925685803218239
iteration : 9665
train acc:  0.796875
train loss:  0.4641824960708618
train gradient:  0.1517613207525607
iteration : 9666
train acc:  0.8359375
train loss:  0.37702658772468567
train gradient:  0.08613199591215435
iteration : 9667
train acc:  0.7265625
train loss:  0.49790745973587036
train gradient:  0.13238428934023008
iteration : 9668
train acc:  0.7890625
train loss:  0.46586111187934875
train gradient:  0.10691752871052364
iteration : 9669
train acc:  0.65625
train loss:  0.5539325475692749
train gradient:  0.18487263346693672
iteration : 9670
train acc:  0.7265625
train loss:  0.4895809292793274
train gradient:  0.13485859454784732
iteration : 9671
train acc:  0.734375
train loss:  0.5088737607002258
train gradient:  0.13663548761826622
iteration : 9672
train acc:  0.7734375
train loss:  0.5125164985656738
train gradient:  0.19493880352573728
iteration : 9673
train acc:  0.671875
train loss:  0.5814133286476135
train gradient:  0.16205785932772063
iteration : 9674
train acc:  0.7890625
train loss:  0.4691660702228546
train gradient:  0.13896319280508496
iteration : 9675
train acc:  0.71875
train loss:  0.5407438278198242
train gradient:  0.14981840331739765
iteration : 9676
train acc:  0.765625
train loss:  0.5231304168701172
train gradient:  0.1675045520043451
iteration : 9677
train acc:  0.6484375
train loss:  0.5984135866165161
train gradient:  0.19145493066439423
iteration : 9678
train acc:  0.71875
train loss:  0.48041462898254395
train gradient:  0.11496634142008727
iteration : 9679
train acc:  0.734375
train loss:  0.46547266840934753
train gradient:  0.1361241926974559
iteration : 9680
train acc:  0.75
train loss:  0.53596031665802
train gradient:  0.15957629445813382
iteration : 9681
train acc:  0.7890625
train loss:  0.537376880645752
train gradient:  0.15286047899437424
iteration : 9682
train acc:  0.78125
train loss:  0.4364086985588074
train gradient:  0.10184004045067782
iteration : 9683
train acc:  0.703125
train loss:  0.5630714893341064
train gradient:  0.17817541270114562
iteration : 9684
train acc:  0.7265625
train loss:  0.5253257751464844
train gradient:  0.13846452481579885
iteration : 9685
train acc:  0.75
train loss:  0.4638594388961792
train gradient:  0.10225151134553453
iteration : 9686
train acc:  0.796875
train loss:  0.43540582060813904
train gradient:  0.119577428425907
iteration : 9687
train acc:  0.6875
train loss:  0.5076995491981506
train gradient:  0.12668244915056914
iteration : 9688
train acc:  0.734375
train loss:  0.5295028686523438
train gradient:  0.1294634902604853
iteration : 9689
train acc:  0.7265625
train loss:  0.48620831966400146
train gradient:  0.11846499910660727
iteration : 9690
train acc:  0.6953125
train loss:  0.49083566665649414
train gradient:  0.13249089288233365
iteration : 9691
train acc:  0.6484375
train loss:  0.6897435188293457
train gradient:  0.22394986617224127
iteration : 9692
train acc:  0.7890625
train loss:  0.455918550491333
train gradient:  0.12725965504390152
iteration : 9693
train acc:  0.7421875
train loss:  0.44467538595199585
train gradient:  0.10731833716658462
iteration : 9694
train acc:  0.7421875
train loss:  0.4882054030895233
train gradient:  0.1135111626461486
iteration : 9695
train acc:  0.7890625
train loss:  0.46680232882499695
train gradient:  0.15054804133468436
iteration : 9696
train acc:  0.796875
train loss:  0.4385356903076172
train gradient:  0.11212951556387943
iteration : 9697
train acc:  0.7578125
train loss:  0.45323285460472107
train gradient:  0.12699015379258807
iteration : 9698
train acc:  0.78125
train loss:  0.4404234290122986
train gradient:  0.10663064922653838
iteration : 9699
train acc:  0.7578125
train loss:  0.5001254677772522
train gradient:  0.1492247325771341
iteration : 9700
train acc:  0.7578125
train loss:  0.4665724039077759
train gradient:  0.12538050066616846
iteration : 9701
train acc:  0.6875
train loss:  0.5674501657485962
train gradient:  0.19130092722333203
iteration : 9702
train acc:  0.6875
train loss:  0.5313097238540649
train gradient:  0.14979318756219584
iteration : 9703
train acc:  0.8046875
train loss:  0.4261842966079712
train gradient:  0.12109518141513838
iteration : 9704
train acc:  0.75
train loss:  0.5236225128173828
train gradient:  0.16088074701150995
iteration : 9705
train acc:  0.765625
train loss:  0.4615378975868225
train gradient:  0.11719399830206711
iteration : 9706
train acc:  0.765625
train loss:  0.49966752529144287
train gradient:  0.15298837197911969
iteration : 9707
train acc:  0.796875
train loss:  0.45977455377578735
train gradient:  0.12180187934434533
iteration : 9708
train acc:  0.828125
train loss:  0.4093707799911499
train gradient:  0.10241055922623417
iteration : 9709
train acc:  0.75
train loss:  0.48475557565689087
train gradient:  0.11288365246897865
iteration : 9710
train acc:  0.703125
train loss:  0.5238704681396484
train gradient:  0.1290861117239392
iteration : 9711
train acc:  0.6875
train loss:  0.5731357336044312
train gradient:  0.15578733352853746
iteration : 9712
train acc:  0.78125
train loss:  0.4334794580936432
train gradient:  0.104041264679576
iteration : 9713
train acc:  0.6953125
train loss:  0.5231954455375671
train gradient:  0.1323924244139475
iteration : 9714
train acc:  0.7109375
train loss:  0.49044036865234375
train gradient:  0.11987180732533716
iteration : 9715
train acc:  0.7578125
train loss:  0.4669276177883148
train gradient:  0.09799084723345737
iteration : 9716
train acc:  0.6953125
train loss:  0.5535261034965515
train gradient:  0.19652298944741692
iteration : 9717
train acc:  0.75
train loss:  0.501317024230957
train gradient:  0.13968742608157858
iteration : 9718
train acc:  0.703125
train loss:  0.5015467405319214
train gradient:  0.14850640800454856
iteration : 9719
train acc:  0.7265625
train loss:  0.4965413510799408
train gradient:  0.1535119359634674
iteration : 9720
train acc:  0.6328125
train loss:  0.6139222383499146
train gradient:  0.16980964119228076
iteration : 9721
train acc:  0.7109375
train loss:  0.557591438293457
train gradient:  0.13573707061948065
iteration : 9722
train acc:  0.765625
train loss:  0.4757821261882782
train gradient:  0.11687823871524539
iteration : 9723
train acc:  0.75
train loss:  0.4521991014480591
train gradient:  0.11627420882631335
iteration : 9724
train acc:  0.7578125
train loss:  0.5048637390136719
train gradient:  0.12732083169716757
iteration : 9725
train acc:  0.734375
train loss:  0.508970320224762
train gradient:  0.16916883763798862
iteration : 9726
train acc:  0.7265625
train loss:  0.5439109206199646
train gradient:  0.1545478251014154
iteration : 9727
train acc:  0.6953125
train loss:  0.5386281609535217
train gradient:  0.13111528258508054
iteration : 9728
train acc:  0.7109375
train loss:  0.5644495487213135
train gradient:  0.13001299311612052
iteration : 9729
train acc:  0.6953125
train loss:  0.5209051370620728
train gradient:  0.15380233055888926
iteration : 9730
train acc:  0.7421875
train loss:  0.5053788423538208
train gradient:  0.14547034518796034
iteration : 9731
train acc:  0.71875
train loss:  0.5271205306053162
train gradient:  0.1509229260102949
iteration : 9732
train acc:  0.7421875
train loss:  0.4889213442802429
train gradient:  0.133604433417492
iteration : 9733
train acc:  0.65625
train loss:  0.5782920122146606
train gradient:  0.1516939379389445
iteration : 9734
train acc:  0.765625
train loss:  0.468680202960968
train gradient:  0.14728795055350974
iteration : 9735
train acc:  0.7421875
train loss:  0.5316517353057861
train gradient:  0.1858484037586528
iteration : 9736
train acc:  0.828125
train loss:  0.4302932322025299
train gradient:  0.09675316712647318
iteration : 9737
train acc:  0.7578125
train loss:  0.46819382905960083
train gradient:  0.1080973982541612
iteration : 9738
train acc:  0.7578125
train loss:  0.48563680052757263
train gradient:  0.11686936704944763
iteration : 9739
train acc:  0.7109375
train loss:  0.523431658744812
train gradient:  0.23286940775021703
iteration : 9740
train acc:  0.7421875
train loss:  0.540412425994873
train gradient:  0.1267910410494919
iteration : 9741
train acc:  0.7421875
train loss:  0.459411084651947
train gradient:  0.12330191576680745
iteration : 9742
train acc:  0.71875
train loss:  0.4911993145942688
train gradient:  0.1297021862968375
iteration : 9743
train acc:  0.75
train loss:  0.4544377326965332
train gradient:  0.09784943753395546
iteration : 9744
train acc:  0.734375
train loss:  0.5027076005935669
train gradient:  0.13509253161910822
iteration : 9745
train acc:  0.734375
train loss:  0.4925976097583771
train gradient:  0.13020910184814877
iteration : 9746
train acc:  0.7578125
train loss:  0.4864746928215027
train gradient:  0.1156413574605923
iteration : 9747
train acc:  0.703125
train loss:  0.57198566198349
train gradient:  0.16767635580900306
iteration : 9748
train acc:  0.7109375
train loss:  0.4901864528656006
train gradient:  0.12237533237992428
iteration : 9749
train acc:  0.7421875
train loss:  0.5307835936546326
train gradient:  0.15841959324491361
iteration : 9750
train acc:  0.6875
train loss:  0.5583896636962891
train gradient:  0.19221695909587833
iteration : 9751
train acc:  0.71875
train loss:  0.4813648760318756
train gradient:  0.1266425615717674
iteration : 9752
train acc:  0.765625
train loss:  0.5118335485458374
train gradient:  0.13967298773371953
iteration : 9753
train acc:  0.6796875
train loss:  0.5044010877609253
train gradient:  0.16870672417949167
iteration : 9754
train acc:  0.7421875
train loss:  0.46365466713905334
train gradient:  0.10927297496104454
iteration : 9755
train acc:  0.734375
train loss:  0.4917718172073364
train gradient:  0.13978619573318285
iteration : 9756
train acc:  0.7109375
train loss:  0.5200262069702148
train gradient:  0.1260607358711112
iteration : 9757
train acc:  0.71875
train loss:  0.5360771417617798
train gradient:  0.15973545529741628
iteration : 9758
train acc:  0.734375
train loss:  0.5033603310585022
train gradient:  0.14120564656657758
iteration : 9759
train acc:  0.6953125
train loss:  0.543146014213562
train gradient:  0.16019126912496445
iteration : 9760
train acc:  0.7578125
train loss:  0.4553641378879547
train gradient:  0.0975937772788995
iteration : 9761
train acc:  0.7265625
train loss:  0.4915618300437927
train gradient:  0.12091328184603377
iteration : 9762
train acc:  0.7734375
train loss:  0.4677520990371704
train gradient:  0.11838169945496926
iteration : 9763
train acc:  0.71875
train loss:  0.490844190120697
train gradient:  0.10981376274842142
iteration : 9764
train acc:  0.765625
train loss:  0.4602453112602234
train gradient:  0.13668197575101337
iteration : 9765
train acc:  0.7890625
train loss:  0.45268428325653076
train gradient:  0.09614045284172991
iteration : 9766
train acc:  0.734375
train loss:  0.46431082487106323
train gradient:  0.12373247501583097
iteration : 9767
train acc:  0.703125
train loss:  0.5163611769676208
train gradient:  0.13240004130368746
iteration : 9768
train acc:  0.765625
train loss:  0.4981018304824829
train gradient:  0.10332537891683213
iteration : 9769
train acc:  0.6953125
train loss:  0.5465226173400879
train gradient:  0.1334189543770367
iteration : 9770
train acc:  0.6875
train loss:  0.5415376424789429
train gradient:  0.1651130432707045
iteration : 9771
train acc:  0.7578125
train loss:  0.5112264752388
train gradient:  0.1604663299953378
iteration : 9772
train acc:  0.7578125
train loss:  0.4722190201282501
train gradient:  0.11107081274495142
iteration : 9773
train acc:  0.7578125
train loss:  0.49148029088974
train gradient:  0.19699520010013075
iteration : 9774
train acc:  0.7734375
train loss:  0.4542098045349121
train gradient:  0.11035217812696409
iteration : 9775
train acc:  0.8515625
train loss:  0.4003746509552002
train gradient:  0.09566391761342952
iteration : 9776
train acc:  0.703125
train loss:  0.5910484790802002
train gradient:  0.18191978132815573
iteration : 9777
train acc:  0.78125
train loss:  0.4607011079788208
train gradient:  0.1150179859720185
iteration : 9778
train acc:  0.75
train loss:  0.4826779365539551
train gradient:  0.11472859781376475
iteration : 9779
train acc:  0.7578125
train loss:  0.45560839772224426
train gradient:  0.11402215791269624
iteration : 9780
train acc:  0.7109375
train loss:  0.5397688746452332
train gradient:  0.1560337664623243
iteration : 9781
train acc:  0.7265625
train loss:  0.4760833978652954
train gradient:  0.1095267526989689
iteration : 9782
train acc:  0.78125
train loss:  0.49548137187957764
train gradient:  0.15522064352580428
iteration : 9783
train acc:  0.7421875
train loss:  0.4899415671825409
train gradient:  0.1623093343329136
iteration : 9784
train acc:  0.765625
train loss:  0.4480014145374298
train gradient:  0.10917923655317642
iteration : 9785
train acc:  0.671875
train loss:  0.48612064123153687
train gradient:  0.12424217758711643
iteration : 9786
train acc:  0.8125
train loss:  0.4209565222263336
train gradient:  0.09560125437287648
iteration : 9787
train acc:  0.75
train loss:  0.4739919602870941
train gradient:  0.1184253718111527
iteration : 9788
train acc:  0.703125
train loss:  0.49314162135124207
train gradient:  0.11425920088503355
iteration : 9789
train acc:  0.78125
train loss:  0.46176934242248535
train gradient:  0.14326559697208757
iteration : 9790
train acc:  0.640625
train loss:  0.5946071147918701
train gradient:  0.17738182114012768
iteration : 9791
train acc:  0.6953125
train loss:  0.5996595621109009
train gradient:  0.15679944730041764
iteration : 9792
train acc:  0.7421875
train loss:  0.4682822823524475
train gradient:  0.10759798780834863
iteration : 9793
train acc:  0.75
train loss:  0.4819468855857849
train gradient:  0.11665359353817015
iteration : 9794
train acc:  0.734375
train loss:  0.5560730695724487
train gradient:  0.1267550620229809
iteration : 9795
train acc:  0.734375
train loss:  0.46675074100494385
train gradient:  0.116924699430345
iteration : 9796
train acc:  0.765625
train loss:  0.46549445390701294
train gradient:  0.12115303660651403
iteration : 9797
train acc:  0.6640625
train loss:  0.5869287848472595
train gradient:  0.18694693844015442
iteration : 9798
train acc:  0.78125
train loss:  0.4518342912197113
train gradient:  0.11837913022683477
iteration : 9799
train acc:  0.828125
train loss:  0.44693389534950256
train gradient:  0.11339328204121105
iteration : 9800
train acc:  0.6640625
train loss:  0.5688213109970093
train gradient:  0.16921827069037887
iteration : 9801
train acc:  0.7890625
train loss:  0.4155883193016052
train gradient:  0.12749903458033535
iteration : 9802
train acc:  0.7265625
train loss:  0.4480017125606537
train gradient:  0.107785664353822
iteration : 9803
train acc:  0.7421875
train loss:  0.4603327512741089
train gradient:  0.11279239148041405
iteration : 9804
train acc:  0.71875
train loss:  0.47459161281585693
train gradient:  0.1304644945272518
iteration : 9805
train acc:  0.7109375
train loss:  0.5266578793525696
train gradient:  0.15199005966562226
iteration : 9806
train acc:  0.78125
train loss:  0.4622287452220917
train gradient:  0.14636342652671744
iteration : 9807
train acc:  0.734375
train loss:  0.529273271560669
train gradient:  0.1325581928832262
iteration : 9808
train acc:  0.7890625
train loss:  0.5003323554992676
train gradient:  0.11460316309414212
iteration : 9809
train acc:  0.75
train loss:  0.47891539335250854
train gradient:  0.12351513881136848
iteration : 9810
train acc:  0.6875
train loss:  0.5437071323394775
train gradient:  0.18676311952380809
iteration : 9811
train acc:  0.75
train loss:  0.45722848176956177
train gradient:  0.11623833985385716
iteration : 9812
train acc:  0.7265625
train loss:  0.5308204889297485
train gradient:  0.11277235480579409
iteration : 9813
train acc:  0.65625
train loss:  0.6641422510147095
train gradient:  0.22731505540073957
iteration : 9814
train acc:  0.703125
train loss:  0.5168256163597107
train gradient:  0.17406140346051113
iteration : 9815
train acc:  0.6953125
train loss:  0.5408518314361572
train gradient:  0.17258795052153153
iteration : 9816
train acc:  0.7578125
train loss:  0.4480245113372803
train gradient:  0.12314827821833417
iteration : 9817
train acc:  0.6953125
train loss:  0.600016713142395
train gradient:  0.15342407849183637
iteration : 9818
train acc:  0.7890625
train loss:  0.4416402578353882
train gradient:  0.10380592579845
iteration : 9819
train acc:  0.703125
train loss:  0.552290678024292
train gradient:  0.15376163362390627
iteration : 9820
train acc:  0.78125
train loss:  0.48153260350227356
train gradient:  0.12772535395419565
iteration : 9821
train acc:  0.71875
train loss:  0.47208094596862793
train gradient:  0.11994877140810103
iteration : 9822
train acc:  0.7734375
train loss:  0.47448623180389404
train gradient:  0.11987219534447999
iteration : 9823
train acc:  0.6796875
train loss:  0.5905560255050659
train gradient:  0.1831233006217107
iteration : 9824
train acc:  0.8046875
train loss:  0.4396946430206299
train gradient:  0.11175845380793156
iteration : 9825
train acc:  0.765625
train loss:  0.47014760971069336
train gradient:  0.10819762146585889
iteration : 9826
train acc:  0.7578125
train loss:  0.41428327560424805
train gradient:  0.09395947881636908
iteration : 9827
train acc:  0.71875
train loss:  0.497333288192749
train gradient:  0.15319110120931437
iteration : 9828
train acc:  0.8046875
train loss:  0.445561945438385
train gradient:  0.09404374346183658
iteration : 9829
train acc:  0.7109375
train loss:  0.531498908996582
train gradient:  0.13490222027719345
iteration : 9830
train acc:  0.7109375
train loss:  0.5663782358169556
train gradient:  0.18616086034813506
iteration : 9831
train acc:  0.7421875
train loss:  0.487690806388855
train gradient:  0.09472903551090536
iteration : 9832
train acc:  0.671875
train loss:  0.5468875765800476
train gradient:  0.15539052935013747
iteration : 9833
train acc:  0.7421875
train loss:  0.49575674533843994
train gradient:  0.13485180331004917
iteration : 9834
train acc:  0.75
train loss:  0.49245524406433105
train gradient:  0.14493043465899857
iteration : 9835
train acc:  0.8046875
train loss:  0.4704952538013458
train gradient:  0.11978902642345404
iteration : 9836
train acc:  0.7109375
train loss:  0.5483555793762207
train gradient:  0.1485815977456141
iteration : 9837
train acc:  0.78125
train loss:  0.4299265742301941
train gradient:  0.12855944045719475
iteration : 9838
train acc:  0.7734375
train loss:  0.4845568537712097
train gradient:  0.1042329191594251
iteration : 9839
train acc:  0.7265625
train loss:  0.5212972164154053
train gradient:  0.17192500678971379
iteration : 9840
train acc:  0.78125
train loss:  0.5145931839942932
train gradient:  0.1438677261148195
iteration : 9841
train acc:  0.7109375
train loss:  0.5284249782562256
train gradient:  0.13653608295605346
iteration : 9842
train acc:  0.734375
train loss:  0.48953306674957275
train gradient:  0.11500979930615794
iteration : 9843
train acc:  0.7734375
train loss:  0.46225863695144653
train gradient:  0.14903209057159397
iteration : 9844
train acc:  0.75
train loss:  0.5100661516189575
train gradient:  0.1599779208587282
iteration : 9845
train acc:  0.703125
train loss:  0.5651497840881348
train gradient:  0.16231214398521437
iteration : 9846
train acc:  0.765625
train loss:  0.4890870451927185
train gradient:  0.1434133339336497
iteration : 9847
train acc:  0.765625
train loss:  0.526750922203064
train gradient:  0.13723219175487697
iteration : 9848
train acc:  0.7578125
train loss:  0.4891491234302521
train gradient:  0.15093374715834723
iteration : 9849
train acc:  0.78125
train loss:  0.42064207792282104
train gradient:  0.10959839550475622
iteration : 9850
train acc:  0.8203125
train loss:  0.4332084655761719
train gradient:  0.11066076310313834
iteration : 9851
train acc:  0.796875
train loss:  0.4418288469314575
train gradient:  0.13689538736203188
iteration : 9852
train acc:  0.765625
train loss:  0.4986562728881836
train gradient:  0.12638520080242832
iteration : 9853
train acc:  0.71875
train loss:  0.48715612292289734
train gradient:  0.11877934559229031
iteration : 9854
train acc:  0.7734375
train loss:  0.436309814453125
train gradient:  0.12298275755105252
iteration : 9855
train acc:  0.6953125
train loss:  0.5676725506782532
train gradient:  0.17998039245551045
iteration : 9856
train acc:  0.7890625
train loss:  0.4971490502357483
train gradient:  0.12536226936085626
iteration : 9857
train acc:  0.75
train loss:  0.4584864377975464
train gradient:  0.10341984378145676
iteration : 9858
train acc:  0.7265625
train loss:  0.49906468391418457
train gradient:  0.15271637458091858
iteration : 9859
train acc:  0.75
train loss:  0.5378363132476807
train gradient:  0.15921558431161664
iteration : 9860
train acc:  0.78125
train loss:  0.454858660697937
train gradient:  0.12681472246248876
iteration : 9861
train acc:  0.7734375
train loss:  0.44223934412002563
train gradient:  0.13002703028910867
iteration : 9862
train acc:  0.8125
train loss:  0.43769946694374084
train gradient:  0.0977097814653036
iteration : 9863
train acc:  0.7890625
train loss:  0.470575749874115
train gradient:  0.11567748349329054
iteration : 9864
train acc:  0.71875
train loss:  0.5546824336051941
train gradient:  0.1782965245431768
iteration : 9865
train acc:  0.703125
train loss:  0.5620181560516357
train gradient:  0.18744747123762995
iteration : 9866
train acc:  0.703125
train loss:  0.49196454882621765
train gradient:  0.13556428306345975
iteration : 9867
train acc:  0.734375
train loss:  0.5069958567619324
train gradient:  0.12647707765379035
iteration : 9868
train acc:  0.6796875
train loss:  0.504449725151062
train gradient:  0.12202311286812619
iteration : 9869
train acc:  0.75
train loss:  0.46068185567855835
train gradient:  0.11259960509163129
iteration : 9870
train acc:  0.7734375
train loss:  0.4816915690898895
train gradient:  0.1694222601099803
iteration : 9871
train acc:  0.65625
train loss:  0.5810269713401794
train gradient:  0.1795992682207446
iteration : 9872
train acc:  0.734375
train loss:  0.5087273120880127
train gradient:  0.13701790683415918
iteration : 9873
train acc:  0.8046875
train loss:  0.4335986375808716
train gradient:  0.10577668246723528
iteration : 9874
train acc:  0.6953125
train loss:  0.541309654712677
train gradient:  0.1594279561154998
iteration : 9875
train acc:  0.78125
train loss:  0.49866023659706116
train gradient:  0.1133438477955285
iteration : 9876
train acc:  0.7265625
train loss:  0.5217177867889404
train gradient:  0.17293924917500164
iteration : 9877
train acc:  0.78125
train loss:  0.4623846113681793
train gradient:  0.13579583270395584
iteration : 9878
train acc:  0.6796875
train loss:  0.5376538634300232
train gradient:  0.1570016418540055
iteration : 9879
train acc:  0.734375
train loss:  0.5407835841178894
train gradient:  0.1378761626660503
iteration : 9880
train acc:  0.8515625
train loss:  0.39716628193855286
train gradient:  0.09418966431502276
iteration : 9881
train acc:  0.765625
train loss:  0.4606885313987732
train gradient:  0.1490493870997851
iteration : 9882
train acc:  0.7109375
train loss:  0.49072790145874023
train gradient:  0.12517362902673485
iteration : 9883
train acc:  0.6953125
train loss:  0.5510122776031494
train gradient:  0.194598009506309
iteration : 9884
train acc:  0.7265625
train loss:  0.5012461543083191
train gradient:  0.16285975910061123
iteration : 9885
train acc:  0.8125
train loss:  0.4741964340209961
train gradient:  0.12693202083600108
iteration : 9886
train acc:  0.7734375
train loss:  0.4953708052635193
train gradient:  0.1273814674815257
iteration : 9887
train acc:  0.7734375
train loss:  0.48373621702194214
train gradient:  0.12267522657770334
iteration : 9888
train acc:  0.7890625
train loss:  0.4482448399066925
train gradient:  0.12278962582541254
iteration : 9889
train acc:  0.765625
train loss:  0.4480817914009094
train gradient:  0.1086214661447565
iteration : 9890
train acc:  0.765625
train loss:  0.5023030638694763
train gradient:  0.12849886774331204
iteration : 9891
train acc:  0.7265625
train loss:  0.5402745008468628
train gradient:  0.1549975840668526
iteration : 9892
train acc:  0.765625
train loss:  0.5180014967918396
train gradient:  0.13741726455153624
iteration : 9893
train acc:  0.78125
train loss:  0.5496454238891602
train gradient:  0.1495977572057421
iteration : 9894
train acc:  0.8203125
train loss:  0.39566224813461304
train gradient:  0.08829401612906025
iteration : 9895
train acc:  0.734375
train loss:  0.4630745053291321
train gradient:  0.12556563577285038
iteration : 9896
train acc:  0.765625
train loss:  0.47126877307891846
train gradient:  0.11672560642359726
iteration : 9897
train acc:  0.71875
train loss:  0.49802669882774353
train gradient:  0.11018411322955013
iteration : 9898
train acc:  0.71875
train loss:  0.5115414261817932
train gradient:  0.10722635942610752
iteration : 9899
train acc:  0.765625
train loss:  0.4388948678970337
train gradient:  0.14001603168511606
iteration : 9900
train acc:  0.7890625
train loss:  0.41489744186401367
train gradient:  0.09581057191676824
iteration : 9901
train acc:  0.734375
train loss:  0.5020476579666138
train gradient:  0.13635124272779883
iteration : 9902
train acc:  0.7734375
train loss:  0.472618043422699
train gradient:  0.12650189101424483
iteration : 9903
train acc:  0.6640625
train loss:  0.6042722463607788
train gradient:  0.19479149511634153
iteration : 9904
train acc:  0.7578125
train loss:  0.4630507826805115
train gradient:  0.10143572599714672
iteration : 9905
train acc:  0.7109375
train loss:  0.5346863269805908
train gradient:  0.1509751185706104
iteration : 9906
train acc:  0.7578125
train loss:  0.4753645360469818
train gradient:  0.1394310710340099
iteration : 9907
train acc:  0.734375
train loss:  0.5304334163665771
train gradient:  0.15703171596805393
iteration : 9908
train acc:  0.7578125
train loss:  0.4686669111251831
train gradient:  0.10983600481123637
iteration : 9909
train acc:  0.7109375
train loss:  0.521958589553833
train gradient:  0.1390881752597256
iteration : 9910
train acc:  0.75
train loss:  0.4729834794998169
train gradient:  0.08885535841790922
iteration : 9911
train acc:  0.7734375
train loss:  0.524864912033081
train gradient:  0.12351168224035917
iteration : 9912
train acc:  0.7109375
train loss:  0.5704341530799866
train gradient:  0.22812749883490274
iteration : 9913
train acc:  0.7109375
train loss:  0.5032554268836975
train gradient:  0.10954073740440377
iteration : 9914
train acc:  0.75
train loss:  0.5318387746810913
train gradient:  0.1496451734695806
iteration : 9915
train acc:  0.7109375
train loss:  0.5141164064407349
train gradient:  0.14828559827107152
iteration : 9916
train acc:  0.7265625
train loss:  0.5203291773796082
train gradient:  0.1702947960078628
iteration : 9917
train acc:  0.7265625
train loss:  0.5220796465873718
train gradient:  0.15941390799182803
iteration : 9918
train acc:  0.765625
train loss:  0.4728330969810486
train gradient:  0.12317593333729643
iteration : 9919
train acc:  0.7265625
train loss:  0.4986700117588043
train gradient:  0.10663997109399945
iteration : 9920
train acc:  0.75
train loss:  0.4650780260562897
train gradient:  0.10480030848850407
iteration : 9921
train acc:  0.78125
train loss:  0.46773239970207214
train gradient:  0.12047298063506959
iteration : 9922
train acc:  0.7578125
train loss:  0.5107123851776123
train gradient:  0.15562882263484573
iteration : 9923
train acc:  0.640625
train loss:  0.5641322135925293
train gradient:  0.16424978572232704
iteration : 9924
train acc:  0.7265625
train loss:  0.5060112476348877
train gradient:  0.14550188503391012
iteration : 9925
train acc:  0.7265625
train loss:  0.5115442872047424
train gradient:  0.1341134133358578
iteration : 9926
train acc:  0.75
train loss:  0.45001304149627686
train gradient:  0.12136313600797877
iteration : 9927
train acc:  0.75
train loss:  0.48203060030937195
train gradient:  0.10806705831265703
iteration : 9928
train acc:  0.7265625
train loss:  0.46986669301986694
train gradient:  0.12206859623709733
iteration : 9929
train acc:  0.734375
train loss:  0.4956151247024536
train gradient:  0.16981340444103582
iteration : 9930
train acc:  0.765625
train loss:  0.48570287227630615
train gradient:  0.140426691613461
iteration : 9931
train acc:  0.7578125
train loss:  0.47481271624565125
train gradient:  0.12314148475199985
iteration : 9932
train acc:  0.7265625
train loss:  0.5323315858840942
train gradient:  0.1287479823058768
iteration : 9933
train acc:  0.828125
train loss:  0.45531412959098816
train gradient:  0.10815896317670019
iteration : 9934
train acc:  0.6875
train loss:  0.5254044532775879
train gradient:  0.16752545464731816
iteration : 9935
train acc:  0.78125
train loss:  0.4839715361595154
train gradient:  0.11479883083499605
iteration : 9936
train acc:  0.6953125
train loss:  0.5274766683578491
train gradient:  0.15314799055084488
iteration : 9937
train acc:  0.7421875
train loss:  0.4979625642299652
train gradient:  0.1289830172380882
iteration : 9938
train acc:  0.7421875
train loss:  0.5421122312545776
train gradient:  0.16554769503881311
iteration : 9939
train acc:  0.734375
train loss:  0.4414524435997009
train gradient:  0.11915152427629286
iteration : 9940
train acc:  0.765625
train loss:  0.5324742794036865
train gradient:  0.16993403144982283
iteration : 9941
train acc:  0.8046875
train loss:  0.4718872308731079
train gradient:  0.10631739333734866
iteration : 9942
train acc:  0.734375
train loss:  0.46025002002716064
train gradient:  0.10730456843373729
iteration : 9943
train acc:  0.7421875
train loss:  0.45859295129776
train gradient:  0.1294696702619979
iteration : 9944
train acc:  0.71875
train loss:  0.5195905566215515
train gradient:  0.15587569917268312
iteration : 9945
train acc:  0.7890625
train loss:  0.4886104464530945
train gradient:  0.13702687825155085
iteration : 9946
train acc:  0.7421875
train loss:  0.4519873559474945
train gradient:  0.1272132836069058
iteration : 9947
train acc:  0.6796875
train loss:  0.5120478868484497
train gradient:  0.12944989874131327
iteration : 9948
train acc:  0.765625
train loss:  0.4832276403903961
train gradient:  0.16545703072687384
iteration : 9949
train acc:  0.734375
train loss:  0.5037107467651367
train gradient:  0.15043697082962493
iteration : 9950
train acc:  0.78125
train loss:  0.43400320410728455
train gradient:  0.09389446525896897
iteration : 9951
train acc:  0.7265625
train loss:  0.5151426792144775
train gradient:  0.18126145907369717
iteration : 9952
train acc:  0.765625
train loss:  0.45215895771980286
train gradient:  0.09014241770335124
iteration : 9953
train acc:  0.765625
train loss:  0.46280384063720703
train gradient:  0.15180461665194722
iteration : 9954
train acc:  0.7109375
train loss:  0.5408077239990234
train gradient:  0.14680317278715155
iteration : 9955
train acc:  0.7890625
train loss:  0.42909175157546997
train gradient:  0.08592266220654934
iteration : 9956
train acc:  0.7890625
train loss:  0.4342597723007202
train gradient:  0.13085021849950307
iteration : 9957
train acc:  0.8125
train loss:  0.4299037754535675
train gradient:  0.12065823068078746
iteration : 9958
train acc:  0.7109375
train loss:  0.5958417654037476
train gradient:  0.29228662373517666
iteration : 9959
train acc:  0.7890625
train loss:  0.412445604801178
train gradient:  0.10104730743528023
iteration : 9960
train acc:  0.7109375
train loss:  0.5178254842758179
train gradient:  0.11250044712141337
iteration : 9961
train acc:  0.7578125
train loss:  0.5080935955047607
train gradient:  0.15182678130405403
iteration : 9962
train acc:  0.7109375
train loss:  0.48178550601005554
train gradient:  0.1413168034514564
iteration : 9963
train acc:  0.6953125
train loss:  0.4771422743797302
train gradient:  0.12326618901529315
iteration : 9964
train acc:  0.7421875
train loss:  0.4918114244937897
train gradient:  0.13139908076690632
iteration : 9965
train acc:  0.7578125
train loss:  0.5344038009643555
train gradient:  0.16217917917368638
iteration : 9966
train acc:  0.8203125
train loss:  0.4473838806152344
train gradient:  0.1028558209984743
iteration : 9967
train acc:  0.7109375
train loss:  0.5061121582984924
train gradient:  0.13100408217573126
iteration : 9968
train acc:  0.765625
train loss:  0.4480746388435364
train gradient:  0.1103223675597526
iteration : 9969
train acc:  0.75
train loss:  0.48761627078056335
train gradient:  0.12865442937912797
iteration : 9970
train acc:  0.734375
train loss:  0.472171813249588
train gradient:  0.10317070167836173
iteration : 9971
train acc:  0.7421875
train loss:  0.48745042085647583
train gradient:  0.14995580844425788
iteration : 9972
train acc:  0.6953125
train loss:  0.5714796185493469
train gradient:  0.16673000448071515
iteration : 9973
train acc:  0.7265625
train loss:  0.49579182267189026
train gradient:  0.13911032210735086
iteration : 9974
train acc:  0.703125
train loss:  0.4982459843158722
train gradient:  0.10268582931493189
iteration : 9975
train acc:  0.734375
train loss:  0.5843584537506104
train gradient:  0.1875074533091079
iteration : 9976
train acc:  0.7109375
train loss:  0.5311006903648376
train gradient:  0.15870115907589916
iteration : 9977
train acc:  0.75
train loss:  0.47761070728302
train gradient:  0.11816876679681558
iteration : 9978
train acc:  0.734375
train loss:  0.5199083089828491
train gradient:  0.16024319492080552
iteration : 9979
train acc:  0.8046875
train loss:  0.43972936272621155
train gradient:  0.10256901089445915
iteration : 9980
train acc:  0.71875
train loss:  0.539745569229126
train gradient:  0.14332503421962972
iteration : 9981
train acc:  0.796875
train loss:  0.4743853807449341
train gradient:  0.12233771002849982
iteration : 9982
train acc:  0.78125
train loss:  0.45175033807754517
train gradient:  0.08626062728425354
iteration : 9983
train acc:  0.7265625
train loss:  0.4877317547798157
train gradient:  0.12361116474487095
iteration : 9984
train acc:  0.796875
train loss:  0.46389031410217285
train gradient:  0.1627917858688051
iteration : 9985
train acc:  0.7578125
train loss:  0.518897533416748
train gradient:  0.14908877622435213
iteration : 9986
train acc:  0.7421875
train loss:  0.5025159120559692
train gradient:  0.1462611764208807
iteration : 9987
train acc:  0.6953125
train loss:  0.5208995342254639
train gradient:  0.1520192855322015
iteration : 9988
train acc:  0.75
train loss:  0.4593127369880676
train gradient:  0.10455214218545818
iteration : 9989
train acc:  0.734375
train loss:  0.545628011226654
train gradient:  0.18138186912686383
iteration : 9990
train acc:  0.78125
train loss:  0.47329962253570557
train gradient:  0.11127041058118423
iteration : 9991
train acc:  0.6875
train loss:  0.5569158792495728
train gradient:  0.1537698886046143
iteration : 9992
train acc:  0.71875
train loss:  0.48783692717552185
train gradient:  0.14811866468858798
iteration : 9993
train acc:  0.75
train loss:  0.49177780747413635
train gradient:  0.13431596528541218
iteration : 9994
train acc:  0.765625
train loss:  0.45955562591552734
train gradient:  0.1124416703959415
iteration : 9995
train acc:  0.75
train loss:  0.4923824667930603
train gradient:  0.1437551298861393
iteration : 9996
train acc:  0.703125
train loss:  0.5090281963348389
train gradient:  0.15771790613610392
iteration : 9997
train acc:  0.7734375
train loss:  0.4449627995491028
train gradient:  0.11262726307762312
iteration : 9998
train acc:  0.6875
train loss:  0.54166579246521
train gradient:  0.1683544239293155
iteration : 9999
train acc:  0.75
train loss:  0.4825195074081421
train gradient:  0.11946252075019696
iteration : 10000
train acc:  0.7734375
train loss:  0.4740431308746338
train gradient:  0.12082285983896325
iteration : 10001
train acc:  0.765625
train loss:  0.4933566451072693
train gradient:  0.12696338046712574
iteration : 10002
train acc:  0.7421875
train loss:  0.48612886667251587
train gradient:  0.11416768272948
iteration : 10003
train acc:  0.765625
train loss:  0.4499226212501526
train gradient:  0.11266722703074812
iteration : 10004
train acc:  0.8046875
train loss:  0.4403728246688843
train gradient:  0.12876365487534036
iteration : 10005
train acc:  0.6953125
train loss:  0.5524012446403503
train gradient:  0.1700701256081124
iteration : 10006
train acc:  0.765625
train loss:  0.46790799498558044
train gradient:  0.12014515737984166
iteration : 10007
train acc:  0.796875
train loss:  0.47153961658477783
train gradient:  0.1291514470560832
iteration : 10008
train acc:  0.765625
train loss:  0.4827335476875305
train gradient:  0.11278120094158478
iteration : 10009
train acc:  0.7578125
train loss:  0.5017518401145935
train gradient:  0.11004158171124537
iteration : 10010
train acc:  0.7109375
train loss:  0.5142849683761597
train gradient:  0.11897343565961002
iteration : 10011
train acc:  0.765625
train loss:  0.4792391359806061
train gradient:  0.12010358206052273
iteration : 10012
train acc:  0.7734375
train loss:  0.44929277896881104
train gradient:  0.10987454052680654
iteration : 10013
train acc:  0.703125
train loss:  0.520493745803833
train gradient:  0.17488062439369
iteration : 10014
train acc:  0.78125
train loss:  0.4468815326690674
train gradient:  0.09065347248098712
iteration : 10015
train acc:  0.8125
train loss:  0.4153316617012024
train gradient:  0.08934094878313607
iteration : 10016
train acc:  0.6875
train loss:  0.5031291246414185
train gradient:  0.14386329276186005
iteration : 10017
train acc:  0.7578125
train loss:  0.4862498939037323
train gradient:  0.18701281841306028
iteration : 10018
train acc:  0.7578125
train loss:  0.4705103635787964
train gradient:  0.12227127170820462
iteration : 10019
train acc:  0.734375
train loss:  0.4731564521789551
train gradient:  0.1283576178787661
iteration : 10020
train acc:  0.7578125
train loss:  0.5576353073120117
train gradient:  0.2615306528523783
iteration : 10021
train acc:  0.734375
train loss:  0.48743540048599243
train gradient:  0.1361182101999155
iteration : 10022
train acc:  0.7421875
train loss:  0.4874761402606964
train gradient:  0.13016070581342382
iteration : 10023
train acc:  0.7421875
train loss:  0.47539910674095154
train gradient:  0.11817861514767655
iteration : 10024
train acc:  0.7421875
train loss:  0.5296941995620728
train gradient:  0.1664487606207239
iteration : 10025
train acc:  0.765625
train loss:  0.4708555340766907
train gradient:  0.12520919563159952
iteration : 10026
train acc:  0.7421875
train loss:  0.526716411113739
train gradient:  0.16114043524147353
iteration : 10027
train acc:  0.7578125
train loss:  0.4949827492237091
train gradient:  0.12279755899861439
iteration : 10028
train acc:  0.7421875
train loss:  0.5066325664520264
train gradient:  0.15854757748310663
iteration : 10029
train acc:  0.78125
train loss:  0.5459933280944824
train gradient:  0.14914499802387968
iteration : 10030
train acc:  0.6953125
train loss:  0.5611079931259155
train gradient:  0.16797286039419143
iteration : 10031
train acc:  0.8046875
train loss:  0.42569541931152344
train gradient:  0.10200001677485945
iteration : 10032
train acc:  0.7109375
train loss:  0.5532686710357666
train gradient:  0.1461630993472979
iteration : 10033
train acc:  0.703125
train loss:  0.571251630783081
train gradient:  0.17367441885134807
iteration : 10034
train acc:  0.734375
train loss:  0.4845240116119385
train gradient:  0.1502057538887626
iteration : 10035
train acc:  0.671875
train loss:  0.5489989519119263
train gradient:  0.17767571065303106
iteration : 10036
train acc:  0.7578125
train loss:  0.476465106010437
train gradient:  0.14565977011004222
iteration : 10037
train acc:  0.78125
train loss:  0.46835142374038696
train gradient:  0.11904423708883385
iteration : 10038
train acc:  0.734375
train loss:  0.5178180932998657
train gradient:  0.12977732458424757
iteration : 10039
train acc:  0.8203125
train loss:  0.4432127773761749
train gradient:  0.13428874000915839
iteration : 10040
train acc:  0.7734375
train loss:  0.4548795521259308
train gradient:  0.11986206637057417
iteration : 10041
train acc:  0.75
train loss:  0.48292702436447144
train gradient:  0.12149925698554184
iteration : 10042
train acc:  0.71875
train loss:  0.5148135423660278
train gradient:  0.16569744195819883
iteration : 10043
train acc:  0.8359375
train loss:  0.41283950209617615
train gradient:  0.12246908718509236
iteration : 10044
train acc:  0.7578125
train loss:  0.4630924463272095
train gradient:  0.10505563707505737
iteration : 10045
train acc:  0.78125
train loss:  0.49774789810180664
train gradient:  0.17223252501825012
iteration : 10046
train acc:  0.75
train loss:  0.48386070132255554
train gradient:  0.15665074382982208
iteration : 10047
train acc:  0.7734375
train loss:  0.4461429715156555
train gradient:  0.11305796334969785
iteration : 10048
train acc:  0.765625
train loss:  0.4433688521385193
train gradient:  0.10912732252243593
iteration : 10049
train acc:  0.6875
train loss:  0.541986882686615
train gradient:  0.16232184714808537
iteration : 10050
train acc:  0.7421875
train loss:  0.4549594521522522
train gradient:  0.1119044295396424
iteration : 10051
train acc:  0.765625
train loss:  0.5269265174865723
train gradient:  0.16492604112534304
iteration : 10052
train acc:  0.7265625
train loss:  0.490817129611969
train gradient:  0.13392702513588198
iteration : 10053
train acc:  0.796875
train loss:  0.4511525630950928
train gradient:  0.10516013253692427
iteration : 10054
train acc:  0.6484375
train loss:  0.5542961359024048
train gradient:  0.16822418390329552
iteration : 10055
train acc:  0.765625
train loss:  0.5187660455703735
train gradient:  0.1246030066116584
iteration : 10056
train acc:  0.7265625
train loss:  0.5463659763336182
train gradient:  0.17404482763692347
iteration : 10057
train acc:  0.7578125
train loss:  0.5284786820411682
train gradient:  0.14744376832367864
iteration : 10058
train acc:  0.7578125
train loss:  0.5247797966003418
train gradient:  0.1883089367176669
iteration : 10059
train acc:  0.7890625
train loss:  0.5121173858642578
train gradient:  0.169050565973309
iteration : 10060
train acc:  0.7265625
train loss:  0.5359954833984375
train gradient:  0.1372664301132998
iteration : 10061
train acc:  0.75
train loss:  0.5106962323188782
train gradient:  0.14520177003071544
iteration : 10062
train acc:  0.765625
train loss:  0.5258632302284241
train gradient:  0.17134242085852275
iteration : 10063
train acc:  0.7421875
train loss:  0.5162324905395508
train gradient:  0.1462497323194129
iteration : 10064
train acc:  0.7421875
train loss:  0.4796738028526306
train gradient:  0.1230484627776522
iteration : 10065
train acc:  0.7890625
train loss:  0.46654826402664185
train gradient:  0.1262275066666516
iteration : 10066
train acc:  0.7109375
train loss:  0.5433781743049622
train gradient:  0.18061976650243186
iteration : 10067
train acc:  0.7578125
train loss:  0.4979957342147827
train gradient:  0.1515882917521495
iteration : 10068
train acc:  0.7578125
train loss:  0.4761471152305603
train gradient:  0.123064768855981
iteration : 10069
train acc:  0.7109375
train loss:  0.4891504645347595
train gradient:  0.11728244726228575
iteration : 10070
train acc:  0.8125
train loss:  0.38681572675704956
train gradient:  0.13993411598430028
iteration : 10071
train acc:  0.8125
train loss:  0.44312721490859985
train gradient:  0.1035782452096707
iteration : 10072
train acc:  0.7421875
train loss:  0.499758780002594
train gradient:  0.15724662352929963
iteration : 10073
train acc:  0.671875
train loss:  0.5490986108779907
train gradient:  0.24257769153697833
iteration : 10074
train acc:  0.6875
train loss:  0.5299781560897827
train gradient:  0.1274409655163591
iteration : 10075
train acc:  0.7734375
train loss:  0.4896671175956726
train gradient:  0.11745684448358612
iteration : 10076
train acc:  0.7421875
train loss:  0.4887630343437195
train gradient:  0.12440875124684016
iteration : 10077
train acc:  0.78125
train loss:  0.4647705852985382
train gradient:  0.10032865196533516
iteration : 10078
train acc:  0.765625
train loss:  0.4921346604824066
train gradient:  0.1238496741115947
iteration : 10079
train acc:  0.75
train loss:  0.47055914998054504
train gradient:  0.14940001558420926
iteration : 10080
train acc:  0.796875
train loss:  0.4791383147239685
train gradient:  0.13119811783027585
iteration : 10081
train acc:  0.765625
train loss:  0.5125814080238342
train gradient:  0.14296216184435934
iteration : 10082
train acc:  0.71875
train loss:  0.46168163418769836
train gradient:  0.1120678576326712
iteration : 10083
train acc:  0.765625
train loss:  0.49072369933128357
train gradient:  0.10381944657034811
iteration : 10084
train acc:  0.7421875
train loss:  0.45573240518569946
train gradient:  0.13992562854154342
iteration : 10085
train acc:  0.7734375
train loss:  0.45241695642471313
train gradient:  0.1305176992851613
iteration : 10086
train acc:  0.71875
train loss:  0.5058210492134094
train gradient:  0.1296937438240347
iteration : 10087
train acc:  0.6796875
train loss:  0.5548907518386841
train gradient:  0.1488484632151833
iteration : 10088
train acc:  0.71875
train loss:  0.5065948963165283
train gradient:  0.13579232709256928
iteration : 10089
train acc:  0.734375
train loss:  0.5152252316474915
train gradient:  0.14438649076450066
iteration : 10090
train acc:  0.6953125
train loss:  0.5315564870834351
train gradient:  0.1621152481137843
iteration : 10091
train acc:  0.75
train loss:  0.49068325757980347
train gradient:  0.1373486631502245
iteration : 10092
train acc:  0.6875
train loss:  0.5943261384963989
train gradient:  0.18671923175708244
iteration : 10093
train acc:  0.75
train loss:  0.46504145860671997
train gradient:  0.14265160225635948
iteration : 10094
train acc:  0.7109375
train loss:  0.5723381042480469
train gradient:  0.21267865917761059
iteration : 10095
train acc:  0.7109375
train loss:  0.5371243953704834
train gradient:  0.15607436487345494
iteration : 10096
train acc:  0.7109375
train loss:  0.565361738204956
train gradient:  0.18014682382837677
iteration : 10097
train acc:  0.796875
train loss:  0.43808287382125854
train gradient:  0.14004224663829865
iteration : 10098
train acc:  0.75
train loss:  0.4506993889808655
train gradient:  0.0974724942969356
iteration : 10099
train acc:  0.734375
train loss:  0.5105337500572205
train gradient:  0.13092348104087162
iteration : 10100
train acc:  0.8046875
train loss:  0.4538349509239197
train gradient:  0.08665106064365022
iteration : 10101
train acc:  0.75
train loss:  0.4855588674545288
train gradient:  0.1221705901714508
iteration : 10102
train acc:  0.71875
train loss:  0.5038511753082275
train gradient:  0.1504327793970867
iteration : 10103
train acc:  0.7578125
train loss:  0.4846011996269226
train gradient:  0.12739885634564369
iteration : 10104
train acc:  0.71875
train loss:  0.5452748537063599
train gradient:  0.16590160913664143
iteration : 10105
train acc:  0.703125
train loss:  0.4931434392929077
train gradient:  0.11937761925819065
iteration : 10106
train acc:  0.7421875
train loss:  0.49917927384376526
train gradient:  0.14736320152433824
iteration : 10107
train acc:  0.6328125
train loss:  0.5597230195999146
train gradient:  0.17709214441190807
iteration : 10108
train acc:  0.75
train loss:  0.4760637879371643
train gradient:  0.13240601696589524
iteration : 10109
train acc:  0.765625
train loss:  0.4762784242630005
train gradient:  0.16727013255654216
iteration : 10110
train acc:  0.734375
train loss:  0.5487135648727417
train gradient:  0.16541456671505642
iteration : 10111
train acc:  0.8203125
train loss:  0.44071152806282043
train gradient:  0.11713005748777487
iteration : 10112
train acc:  0.78125
train loss:  0.4469396770000458
train gradient:  0.1334697401125866
iteration : 10113
train acc:  0.765625
train loss:  0.5241129398345947
train gradient:  0.14693346060513574
iteration : 10114
train acc:  0.6875
train loss:  0.49168866872787476
train gradient:  0.12217736865990157
iteration : 10115
train acc:  0.7734375
train loss:  0.45586371421813965
train gradient:  0.10688225144830572
iteration : 10116
train acc:  0.796875
train loss:  0.4524261951446533
train gradient:  0.11837599644996238
iteration : 10117
train acc:  0.703125
train loss:  0.5754866600036621
train gradient:  0.1908577046597394
iteration : 10118
train acc:  0.75
train loss:  0.5627963542938232
train gradient:  0.16179550219321676
iteration : 10119
train acc:  0.71875
train loss:  0.4931601881980896
train gradient:  0.1167374679993623
iteration : 10120
train acc:  0.75
train loss:  0.5562551617622375
train gradient:  0.18841142000029912
iteration : 10121
train acc:  0.7265625
train loss:  0.5071172714233398
train gradient:  0.1325728092868551
iteration : 10122
train acc:  0.7578125
train loss:  0.4889835715293884
train gradient:  0.13135763102787798
iteration : 10123
train acc:  0.7421875
train loss:  0.4763609766960144
train gradient:  0.10770885536482858
iteration : 10124
train acc:  0.765625
train loss:  0.4132486581802368
train gradient:  0.10532769030144955
iteration : 10125
train acc:  0.8203125
train loss:  0.4371740221977234
train gradient:  0.11145054912892156
iteration : 10126
train acc:  0.6875
train loss:  0.5331827998161316
train gradient:  0.15704949190061795
iteration : 10127
train acc:  0.7421875
train loss:  0.4964849054813385
train gradient:  0.14841691548740285
iteration : 10128
train acc:  0.7578125
train loss:  0.5256572961807251
train gradient:  0.14051771838622132
iteration : 10129
train acc:  0.765625
train loss:  0.4836742877960205
train gradient:  0.10421201247404398
iteration : 10130
train acc:  0.765625
train loss:  0.4523707330226898
train gradient:  0.1385843943350875
iteration : 10131
train acc:  0.703125
train loss:  0.5542790293693542
train gradient:  0.1607258458946741
iteration : 10132
train acc:  0.640625
train loss:  0.6082414388656616
train gradient:  0.17154182727164924
iteration : 10133
train acc:  0.8359375
train loss:  0.38443732261657715
train gradient:  0.08591634993880111
iteration : 10134
train acc:  0.75
train loss:  0.5029475688934326
train gradient:  0.1591501173235944
iteration : 10135
train acc:  0.859375
train loss:  0.4215599596500397
train gradient:  0.10241908614586209
iteration : 10136
train acc:  0.7421875
train loss:  0.4730777144432068
train gradient:  0.1307529471836338
iteration : 10137
train acc:  0.7265625
train loss:  0.5003568530082703
train gradient:  0.15529084020303197
iteration : 10138
train acc:  0.671875
train loss:  0.5720263719558716
train gradient:  0.16160859708697117
iteration : 10139
train acc:  0.765625
train loss:  0.45707839727401733
train gradient:  0.11498993662904743
iteration : 10140
train acc:  0.734375
train loss:  0.5263789296150208
train gradient:  0.14462198178222743
iteration : 10141
train acc:  0.7578125
train loss:  0.4375283122062683
train gradient:  0.11638702385945549
iteration : 10142
train acc:  0.7265625
train loss:  0.5041927099227905
train gradient:  0.15057194288611797
iteration : 10143
train acc:  0.7734375
train loss:  0.4678908586502075
train gradient:  0.10572932949689152
iteration : 10144
train acc:  0.7109375
train loss:  0.5340453386306763
train gradient:  0.20190696742696615
iteration : 10145
train acc:  0.75
train loss:  0.48366332054138184
train gradient:  0.11054041584034999
iteration : 10146
train acc:  0.765625
train loss:  0.4767974019050598
train gradient:  0.12668977541210133
iteration : 10147
train acc:  0.75
train loss:  0.5072527527809143
train gradient:  0.16590991420519163
iteration : 10148
train acc:  0.78125
train loss:  0.42377328872680664
train gradient:  0.13436757153870704
iteration : 10149
train acc:  0.75
train loss:  0.4734204113483429
train gradient:  0.1461038075672308
iteration : 10150
train acc:  0.7734375
train loss:  0.4238995313644409
train gradient:  0.1264716137855212
iteration : 10151
train acc:  0.7109375
train loss:  0.5099717378616333
train gradient:  0.12689426066704768
iteration : 10152
train acc:  0.65625
train loss:  0.606891930103302
train gradient:  0.20565215614097768
iteration : 10153
train acc:  0.75
train loss:  0.4689478874206543
train gradient:  0.16256756048565915
iteration : 10154
train acc:  0.7109375
train loss:  0.49547436833381653
train gradient:  0.17346451480698255
iteration : 10155
train acc:  0.7265625
train loss:  0.5066817402839661
train gradient:  0.12071462689176637
iteration : 10156
train acc:  0.6875
train loss:  0.5526082515716553
train gradient:  0.19592932958975345
iteration : 10157
train acc:  0.6953125
train loss:  0.5167855024337769
train gradient:  0.12769411694062108
iteration : 10158
train acc:  0.75
train loss:  0.5278924107551575
train gradient:  0.13953465220283495
iteration : 10159
train acc:  0.7578125
train loss:  0.5243896842002869
train gradient:  0.1432542839714931
iteration : 10160
train acc:  0.7109375
train loss:  0.5654405355453491
train gradient:  0.13720544900513082
iteration : 10161
train acc:  0.7265625
train loss:  0.4605007469654083
train gradient:  0.12528893373126032
iteration : 10162
train acc:  0.7265625
train loss:  0.5261818766593933
train gradient:  0.14699414976001934
iteration : 10163
train acc:  0.6640625
train loss:  0.5547975897789001
train gradient:  0.1467570597010498
iteration : 10164
train acc:  0.78125
train loss:  0.47082459926605225
train gradient:  0.14227808978665768
iteration : 10165
train acc:  0.7890625
train loss:  0.44079840183258057
train gradient:  0.10015022048432355
iteration : 10166
train acc:  0.765625
train loss:  0.4845876097679138
train gradient:  0.13489370785060456
iteration : 10167
train acc:  0.7109375
train loss:  0.5730835795402527
train gradient:  0.16062218465240546
iteration : 10168
train acc:  0.7734375
train loss:  0.4421342611312866
train gradient:  0.1376249142395318
iteration : 10169
train acc:  0.7890625
train loss:  0.44993746280670166
train gradient:  0.12460190157191832
iteration : 10170
train acc:  0.71875
train loss:  0.549726128578186
train gradient:  0.169590999807289
iteration : 10171
train acc:  0.7734375
train loss:  0.4718790650367737
train gradient:  0.1255711233828241
iteration : 10172
train acc:  0.7890625
train loss:  0.4276576340198517
train gradient:  0.10088180049549732
iteration : 10173
train acc:  0.765625
train loss:  0.47014039754867554
train gradient:  0.15024397928381372
iteration : 10174
train acc:  0.6953125
train loss:  0.5839446783065796
train gradient:  0.15574402064464543
iteration : 10175
train acc:  0.7421875
train loss:  0.4513188600540161
train gradient:  0.12887026661024492
iteration : 10176
train acc:  0.71875
train loss:  0.5372263193130493
train gradient:  0.16352631178787227
iteration : 10177
train acc:  0.7265625
train loss:  0.5070766806602478
train gradient:  0.13335209911921997
iteration : 10178
train acc:  0.6875
train loss:  0.5521356463432312
train gradient:  0.12840643786295663
iteration : 10179
train acc:  0.765625
train loss:  0.48450928926467896
train gradient:  0.13215526186370008
iteration : 10180
train acc:  0.7734375
train loss:  0.46579432487487793
train gradient:  0.14047943861421547
iteration : 10181
train acc:  0.7734375
train loss:  0.49504393339157104
train gradient:  0.15699539826152697
iteration : 10182
train acc:  0.7265625
train loss:  0.5067232847213745
train gradient:  0.13789422458462114
iteration : 10183
train acc:  0.75
train loss:  0.5156535506248474
train gradient:  0.1602555358171168
iteration : 10184
train acc:  0.7109375
train loss:  0.49813151359558105
train gradient:  0.1103282135078646
iteration : 10185
train acc:  0.71875
train loss:  0.528297483921051
train gradient:  0.15704689053271115
iteration : 10186
train acc:  0.78125
train loss:  0.4652729630470276
train gradient:  0.12553526448973235
iteration : 10187
train acc:  0.7109375
train loss:  0.5227200984954834
train gradient:  0.1517245918421835
iteration : 10188
train acc:  0.7421875
train loss:  0.49597400426864624
train gradient:  0.12733498847981656
iteration : 10189
train acc:  0.6953125
train loss:  0.5729000568389893
train gradient:  0.13701211208813915
iteration : 10190
train acc:  0.75
train loss:  0.4766492545604706
train gradient:  0.18562767091907512
iteration : 10191
train acc:  0.7734375
train loss:  0.46841174364089966
train gradient:  0.11140998560980248
iteration : 10192
train acc:  0.765625
train loss:  0.4629601240158081
train gradient:  0.11012912897509813
iteration : 10193
train acc:  0.78125
train loss:  0.42322614789009094
train gradient:  0.10381331345097367
iteration : 10194
train acc:  0.7578125
train loss:  0.5115588903427124
train gradient:  0.1345995458737754
iteration : 10195
train acc:  0.8046875
train loss:  0.46520495414733887
train gradient:  0.11760587776428628
iteration : 10196
train acc:  0.6953125
train loss:  0.5594576597213745
train gradient:  0.1474317345197968
iteration : 10197
train acc:  0.734375
train loss:  0.5186136960983276
train gradient:  0.15589162005476584
iteration : 10198
train acc:  0.7578125
train loss:  0.4521859288215637
train gradient:  0.13936479338722704
iteration : 10199
train acc:  0.6953125
train loss:  0.5289719104766846
train gradient:  0.16484310061617224
iteration : 10200
train acc:  0.75
train loss:  0.48609521985054016
train gradient:  0.13857539521709736
iteration : 10201
train acc:  0.6484375
train loss:  0.5769076347351074
train gradient:  0.2297791048007342
iteration : 10202
train acc:  0.6796875
train loss:  0.5804620981216431
train gradient:  0.20004758468654388
iteration : 10203
train acc:  0.78125
train loss:  0.48472002148628235
train gradient:  0.1196255787315373
iteration : 10204
train acc:  0.7265625
train loss:  0.4785856604576111
train gradient:  0.11019438138438944
iteration : 10205
train acc:  0.71875
train loss:  0.5295183062553406
train gradient:  0.15950081846561848
iteration : 10206
train acc:  0.7265625
train loss:  0.5302102565765381
train gradient:  0.1439294243946057
iteration : 10207
train acc:  0.7890625
train loss:  0.444277822971344
train gradient:  0.10633834091900182
iteration : 10208
train acc:  0.75
train loss:  0.4804818332195282
train gradient:  0.14108293107269204
iteration : 10209
train acc:  0.734375
train loss:  0.48899126052856445
train gradient:  0.14309288674952822
iteration : 10210
train acc:  0.8046875
train loss:  0.4269965887069702
train gradient:  0.12420815985780062
iteration : 10211
train acc:  0.7421875
train loss:  0.5283082723617554
train gradient:  0.14569483710880882
iteration : 10212
train acc:  0.7578125
train loss:  0.4909955561161041
train gradient:  0.14929415590957956
iteration : 10213
train acc:  0.7421875
train loss:  0.46082383394241333
train gradient:  0.16302380330669475
iteration : 10214
train acc:  0.734375
train loss:  0.5179082155227661
train gradient:  0.1602254930077306
iteration : 10215
train acc:  0.7109375
train loss:  0.5426843166351318
train gradient:  0.18904712742089988
iteration : 10216
train acc:  0.7578125
train loss:  0.4954683184623718
train gradient:  0.13168071717593766
iteration : 10217
train acc:  0.6953125
train loss:  0.5358633995056152
train gradient:  0.155161762836239
iteration : 10218
train acc:  0.6953125
train loss:  0.5834171772003174
train gradient:  0.15609031520981398
iteration : 10219
train acc:  0.75
train loss:  0.49283313751220703
train gradient:  0.10197343181569435
iteration : 10220
train acc:  0.765625
train loss:  0.510042667388916
train gradient:  0.18520223430501695
iteration : 10221
train acc:  0.75
train loss:  0.45264190435409546
train gradient:  0.1210294081655056
iteration : 10222
train acc:  0.75
train loss:  0.5096653699874878
train gradient:  0.16259556231338557
iteration : 10223
train acc:  0.75
train loss:  0.4972619414329529
train gradient:  0.15869353092571115
iteration : 10224
train acc:  0.7421875
train loss:  0.45322680473327637
train gradient:  0.11945685174309441
iteration : 10225
train acc:  0.7265625
train loss:  0.45811575651168823
train gradient:  0.11259015352165269
iteration : 10226
train acc:  0.734375
train loss:  0.47034770250320435
train gradient:  0.10988136790093585
iteration : 10227
train acc:  0.6875
train loss:  0.49634093046188354
train gradient:  0.13395409044627268
iteration : 10228
train acc:  0.796875
train loss:  0.424674928188324
train gradient:  0.10792819525089489
iteration : 10229
train acc:  0.6796875
train loss:  0.5193778276443481
train gradient:  0.15765153367861556
iteration : 10230
train acc:  0.703125
train loss:  0.47097721695899963
train gradient:  0.12273807446774725
iteration : 10231
train acc:  0.7421875
train loss:  0.5194745659828186
train gradient:  0.14253276717743363
iteration : 10232
train acc:  0.6796875
train loss:  0.505535364151001
train gradient:  0.15214104648908733
iteration : 10233
train acc:  0.703125
train loss:  0.5292589068412781
train gradient:  0.14152060815087206
iteration : 10234
train acc:  0.671875
train loss:  0.5638569593429565
train gradient:  0.11401420183397182
iteration : 10235
train acc:  0.7265625
train loss:  0.5237464904785156
train gradient:  0.13001704542388448
iteration : 10236
train acc:  0.8046875
train loss:  0.45803165435791016
train gradient:  0.10391617535006677
iteration : 10237
train acc:  0.6953125
train loss:  0.5151984691619873
train gradient:  0.1406201998932397
iteration : 10238
train acc:  0.7109375
train loss:  0.4903263747692108
train gradient:  0.14737879294453451
iteration : 10239
train acc:  0.8046875
train loss:  0.45175492763519287
train gradient:  0.10952373261198754
iteration : 10240
train acc:  0.7578125
train loss:  0.43668460845947266
train gradient:  0.13017834912334836
iteration : 10241
train acc:  0.671875
train loss:  0.551913857460022
train gradient:  0.20852621540761906
iteration : 10242
train acc:  0.7734375
train loss:  0.42845410108566284
train gradient:  0.07509202596684633
iteration : 10243
train acc:  0.6796875
train loss:  0.5426530838012695
train gradient:  0.14028989049765517
iteration : 10244
train acc:  0.71875
train loss:  0.48842161893844604
train gradient:  0.12699129909010767
iteration : 10245
train acc:  0.6796875
train loss:  0.5715306997299194
train gradient:  0.1790606004663372
iteration : 10246
train acc:  0.7421875
train loss:  0.5089169144630432
train gradient:  0.16793625508502535
iteration : 10247
train acc:  0.75
train loss:  0.4947795867919922
train gradient:  0.12341608175740736
iteration : 10248
train acc:  0.7421875
train loss:  0.45159095525741577
train gradient:  0.11989987405522876
iteration : 10249
train acc:  0.65625
train loss:  0.5895443558692932
train gradient:  0.1341067055533952
iteration : 10250
train acc:  0.7265625
train loss:  0.5217150449752808
train gradient:  0.14066652719496614
iteration : 10251
train acc:  0.703125
train loss:  0.5281953811645508
train gradient:  0.14447015303331928
iteration : 10252
train acc:  0.71875
train loss:  0.4932451546192169
train gradient:  0.10847368293153797
iteration : 10253
train acc:  0.6875
train loss:  0.5255202054977417
train gradient:  0.1947922710905166
iteration : 10254
train acc:  0.734375
train loss:  0.4523688852787018
train gradient:  0.12541487774317522
iteration : 10255
train acc:  0.8203125
train loss:  0.41742077469825745
train gradient:  0.09031200319541301
iteration : 10256
train acc:  0.7578125
train loss:  0.5468064546585083
train gradient:  0.19001792640257886
iteration : 10257
train acc:  0.6953125
train loss:  0.5749853253364563
train gradient:  0.21386747068686074
iteration : 10258
train acc:  0.7578125
train loss:  0.46783050894737244
train gradient:  0.10726678049490071
iteration : 10259
train acc:  0.71875
train loss:  0.5045578479766846
train gradient:  0.12372202084708045
iteration : 10260
train acc:  0.7890625
train loss:  0.45112037658691406
train gradient:  0.11800801335183131
iteration : 10261
train acc:  0.765625
train loss:  0.5169035196304321
train gradient:  0.16111641056474663
iteration : 10262
train acc:  0.7109375
train loss:  0.543707013130188
train gradient:  0.1469733266243884
iteration : 10263
train acc:  0.796875
train loss:  0.4339311420917511
train gradient:  0.13640798805405063
iteration : 10264
train acc:  0.7265625
train loss:  0.49485185742378235
train gradient:  0.14446364233408565
iteration : 10265
train acc:  0.7265625
train loss:  0.5335543155670166
train gradient:  0.13683535188973078
iteration : 10266
train acc:  0.7734375
train loss:  0.5259934663772583
train gradient:  0.14753257744961973
iteration : 10267
train acc:  0.6640625
train loss:  0.5293964743614197
train gradient:  0.13697831008750394
iteration : 10268
train acc:  0.734375
train loss:  0.4920055568218231
train gradient:  0.13738533637196582
iteration : 10269
train acc:  0.71875
train loss:  0.5287892818450928
train gradient:  0.15405843967087315
iteration : 10270
train acc:  0.71875
train loss:  0.5670126080513
train gradient:  0.17885316793672712
iteration : 10271
train acc:  0.765625
train loss:  0.4876445531845093
train gradient:  0.11556613534166364
iteration : 10272
train acc:  0.78125
train loss:  0.46438413858413696
train gradient:  0.115856070562694
iteration : 10273
train acc:  0.75
train loss:  0.5265252590179443
train gradient:  0.16871619755296863
iteration : 10274
train acc:  0.71875
train loss:  0.5156315565109253
train gradient:  0.14545589807995157
iteration : 10275
train acc:  0.734375
train loss:  0.4729483127593994
train gradient:  0.10357060485742854
iteration : 10276
train acc:  0.7109375
train loss:  0.4801146388053894
train gradient:  0.11744896003736519
iteration : 10277
train acc:  0.796875
train loss:  0.44777852296829224
train gradient:  0.12138259086265608
iteration : 10278
train acc:  0.7109375
train loss:  0.5042928457260132
train gradient:  0.1342787314091688
iteration : 10279
train acc:  0.78125
train loss:  0.45647192001342773
train gradient:  0.11295175928109727
iteration : 10280
train acc:  0.734375
train loss:  0.5212020874023438
train gradient:  0.1469406864523334
iteration : 10281
train acc:  0.78125
train loss:  0.4248686730861664
train gradient:  0.08715796942370514
iteration : 10282
train acc:  0.75
train loss:  0.4683564305305481
train gradient:  0.10499663659185518
iteration : 10283
train acc:  0.78125
train loss:  0.4736230671405792
train gradient:  0.10853781173394751
iteration : 10284
train acc:  0.7578125
train loss:  0.5007534027099609
train gradient:  0.15279311346656718
iteration : 10285
train acc:  0.7265625
train loss:  0.5180816650390625
train gradient:  0.1535441746124095
iteration : 10286
train acc:  0.734375
train loss:  0.5168909430503845
train gradient:  0.14181077045359453
iteration : 10287
train acc:  0.671875
train loss:  0.5229697227478027
train gradient:  0.13850002083910093
iteration : 10288
train acc:  0.7734375
train loss:  0.45397689938545227
train gradient:  0.11394509190134489
iteration : 10289
train acc:  0.78125
train loss:  0.4527793228626251
train gradient:  0.10068738102372704
iteration : 10290
train acc:  0.7890625
train loss:  0.4768698811531067
train gradient:  0.11604272064941226
iteration : 10291
train acc:  0.6953125
train loss:  0.5447443723678589
train gradient:  0.14279626028531095
iteration : 10292
train acc:  0.6796875
train loss:  0.5305739045143127
train gradient:  0.16311099906871362
iteration : 10293
train acc:  0.7265625
train loss:  0.4860829710960388
train gradient:  0.17949270664461592
iteration : 10294
train acc:  0.7265625
train loss:  0.5069488883018494
train gradient:  0.15909549759026476
iteration : 10295
train acc:  0.7421875
train loss:  0.4661775827407837
train gradient:  0.11987167448483552
iteration : 10296
train acc:  0.75
train loss:  0.4777830243110657
train gradient:  0.1435303716537379
iteration : 10297
train acc:  0.75
train loss:  0.4859645962715149
train gradient:  0.13617505265201013
iteration : 10298
train acc:  0.7109375
train loss:  0.514201283454895
train gradient:  0.13309249268557377
iteration : 10299
train acc:  0.828125
train loss:  0.3899642527103424
train gradient:  0.086724826562182
iteration : 10300
train acc:  0.734375
train loss:  0.49102583527565
train gradient:  0.17651170306849698
iteration : 10301
train acc:  0.765625
train loss:  0.4772258400917053
train gradient:  0.13146727320262097
iteration : 10302
train acc:  0.7578125
train loss:  0.5011831521987915
train gradient:  0.13326100739359184
iteration : 10303
train acc:  0.6640625
train loss:  0.6308470964431763
train gradient:  0.2093543361705663
iteration : 10304
train acc:  0.7109375
train loss:  0.5126609206199646
train gradient:  0.12732298633345812
iteration : 10305
train acc:  0.7265625
train loss:  0.4868049621582031
train gradient:  0.11867181087470943
iteration : 10306
train acc:  0.7578125
train loss:  0.4666007161140442
train gradient:  0.13027661101715127
iteration : 10307
train acc:  0.734375
train loss:  0.5001075863838196
train gradient:  0.15518902162541545
iteration : 10308
train acc:  0.7734375
train loss:  0.46505123376846313
train gradient:  0.13544027585413349
iteration : 10309
train acc:  0.71875
train loss:  0.5168899297714233
train gradient:  0.1267899686408015
iteration : 10310
train acc:  0.6875
train loss:  0.6008883118629456
train gradient:  0.21726758084579656
iteration : 10311
train acc:  0.796875
train loss:  0.4676656126976013
train gradient:  0.13181112779614354
iteration : 10312
train acc:  0.7578125
train loss:  0.46842116117477417
train gradient:  0.11236160509361857
iteration : 10313
train acc:  0.7421875
train loss:  0.46442198753356934
train gradient:  0.11630168436759313
iteration : 10314
train acc:  0.6953125
train loss:  0.530448317527771
train gradient:  0.12217395361274254
iteration : 10315
train acc:  0.7109375
train loss:  0.46658414602279663
train gradient:  0.1398579294023204
iteration : 10316
train acc:  0.765625
train loss:  0.44506940245628357
train gradient:  0.1393257578324864
iteration : 10317
train acc:  0.734375
train loss:  0.5159502029418945
train gradient:  0.11556576497708572
iteration : 10318
train acc:  0.7578125
train loss:  0.4553881287574768
train gradient:  0.14947325038864065
iteration : 10319
train acc:  0.7109375
train loss:  0.5420571565628052
train gradient:  0.1859128096719717
iteration : 10320
train acc:  0.8125
train loss:  0.4437023997306824
train gradient:  0.1000628844478814
iteration : 10321
train acc:  0.8046875
train loss:  0.4154660105705261
train gradient:  0.11875563610879256
iteration : 10322
train acc:  0.734375
train loss:  0.4671572148799896
train gradient:  0.11908631421087397
iteration : 10323
train acc:  0.7734375
train loss:  0.4922487437725067
train gradient:  0.13861398236664269
iteration : 10324
train acc:  0.765625
train loss:  0.5324035286903381
train gradient:  0.1605547098651114
iteration : 10325
train acc:  0.734375
train loss:  0.4952391982078552
train gradient:  0.13509512345879499
iteration : 10326
train acc:  0.7734375
train loss:  0.4957393407821655
train gradient:  0.16424440224605902
iteration : 10327
train acc:  0.6953125
train loss:  0.541709303855896
train gradient:  0.16021050790623204
iteration : 10328
train acc:  0.7578125
train loss:  0.4701332449913025
train gradient:  0.12166747701769381
iteration : 10329
train acc:  0.7421875
train loss:  0.5118663311004639
train gradient:  0.13955239437393185
iteration : 10330
train acc:  0.765625
train loss:  0.4854693114757538
train gradient:  0.11621299296351957
iteration : 10331
train acc:  0.703125
train loss:  0.5064431428909302
train gradient:  0.1781466597515363
iteration : 10332
train acc:  0.71875
train loss:  0.4976109266281128
train gradient:  0.13125461012683876
iteration : 10333
train acc:  0.765625
train loss:  0.4703594148159027
train gradient:  0.1308436437839847
iteration : 10334
train acc:  0.75
train loss:  0.5389212369918823
train gradient:  0.19617477897992425
iteration : 10335
train acc:  0.8203125
train loss:  0.41368627548217773
train gradient:  0.09964896787364314
iteration : 10336
train acc:  0.796875
train loss:  0.43960466980934143
train gradient:  0.11028940988124307
iteration : 10337
train acc:  0.765625
train loss:  0.47186946868896484
train gradient:  0.1321267021133818
iteration : 10338
train acc:  0.7578125
train loss:  0.4830256700515747
train gradient:  0.10357110969062945
iteration : 10339
train acc:  0.6796875
train loss:  0.5569394826889038
train gradient:  0.1715683167962043
iteration : 10340
train acc:  0.796875
train loss:  0.4424525201320648
train gradient:  0.1380196451674528
iteration : 10341
train acc:  0.6875
train loss:  0.4911971688270569
train gradient:  0.13274233413793302
iteration : 10342
train acc:  0.6796875
train loss:  0.5431873798370361
train gradient:  0.14785938047993458
iteration : 10343
train acc:  0.8203125
train loss:  0.4468608498573303
train gradient:  0.10834361127714803
iteration : 10344
train acc:  0.7421875
train loss:  0.4719464182853699
train gradient:  0.11362293413719442
iteration : 10345
train acc:  0.6953125
train loss:  0.5421686172485352
train gradient:  0.18229605518545483
iteration : 10346
train acc:  0.765625
train loss:  0.46172818541526794
train gradient:  0.11996471734249625
iteration : 10347
train acc:  0.8046875
train loss:  0.43100717663764954
train gradient:  0.11417023583482348
iteration : 10348
train acc:  0.75
train loss:  0.48636573553085327
train gradient:  0.15195225277023083
iteration : 10349
train acc:  0.7265625
train loss:  0.5022167563438416
train gradient:  0.1431321595367906
iteration : 10350
train acc:  0.8125
train loss:  0.4234222173690796
train gradient:  0.11441500858953378
iteration : 10351
train acc:  0.78125
train loss:  0.4419606328010559
train gradient:  0.11806372086399865
iteration : 10352
train acc:  0.734375
train loss:  0.4962260127067566
train gradient:  0.15143438926291256
iteration : 10353
train acc:  0.796875
train loss:  0.4491199851036072
train gradient:  0.12863243018796489
iteration : 10354
train acc:  0.75
train loss:  0.4972745478153229
train gradient:  0.13719343632960598
iteration : 10355
train acc:  0.796875
train loss:  0.4340251386165619
train gradient:  0.12173630613552891
iteration : 10356
train acc:  0.7578125
train loss:  0.5108566880226135
train gradient:  0.15283073499062183
iteration : 10357
train acc:  0.7421875
train loss:  0.5616803765296936
train gradient:  0.14395864618333848
iteration : 10358
train acc:  0.71875
train loss:  0.4937047064304352
train gradient:  0.11924357643432962
iteration : 10359
train acc:  0.7421875
train loss:  0.5104750990867615
train gradient:  0.14675754323153628
iteration : 10360
train acc:  0.7890625
train loss:  0.43013301491737366
train gradient:  0.11355140498240675
iteration : 10361
train acc:  0.7421875
train loss:  0.4940904378890991
train gradient:  0.14113863376176547
iteration : 10362
train acc:  0.7421875
train loss:  0.4887372553348541
train gradient:  0.13407520315062949
iteration : 10363
train acc:  0.734375
train loss:  0.5482034087181091
train gradient:  0.15329660671424572
iteration : 10364
train acc:  0.796875
train loss:  0.4660514295101166
train gradient:  0.1177811480575572
iteration : 10365
train acc:  0.7421875
train loss:  0.4910624027252197
train gradient:  0.15680751070548088
iteration : 10366
train acc:  0.7734375
train loss:  0.4605867862701416
train gradient:  0.13476729153168104
iteration : 10367
train acc:  0.703125
train loss:  0.5202671885490417
train gradient:  0.15499264610566132
iteration : 10368
train acc:  0.8359375
train loss:  0.4136187732219696
train gradient:  0.11115945347995747
iteration : 10369
train acc:  0.765625
train loss:  0.4472217857837677
train gradient:  0.10482851146395418
iteration : 10370
train acc:  0.6796875
train loss:  0.5185719728469849
train gradient:  0.1600998101858896
iteration : 10371
train acc:  0.796875
train loss:  0.43517452478408813
train gradient:  0.10472552393964894
iteration : 10372
train acc:  0.765625
train loss:  0.4759116768836975
train gradient:  0.13518158174029182
iteration : 10373
train acc:  0.703125
train loss:  0.5032787322998047
train gradient:  0.1680822652932129
iteration : 10374
train acc:  0.7265625
train loss:  0.5065387487411499
train gradient:  0.17000813982640373
iteration : 10375
train acc:  0.7578125
train loss:  0.46145159006118774
train gradient:  0.11435774281690483
iteration : 10376
train acc:  0.703125
train loss:  0.5509878396987915
train gradient:  0.19729417562082624
iteration : 10377
train acc:  0.8046875
train loss:  0.4197274446487427
train gradient:  0.08648001767367865
iteration : 10378
train acc:  0.75
train loss:  0.49499082565307617
train gradient:  0.1069732360092509
iteration : 10379
train acc:  0.734375
train loss:  0.5368789434432983
train gradient:  0.11994849863913998
iteration : 10380
train acc:  0.75
train loss:  0.5035033226013184
train gradient:  0.13650974545010136
iteration : 10381
train acc:  0.703125
train loss:  0.5851754546165466
train gradient:  0.2405486351699977
iteration : 10382
train acc:  0.71875
train loss:  0.5194846987724304
train gradient:  0.1368718866822638
iteration : 10383
train acc:  0.7578125
train loss:  0.4742558002471924
train gradient:  0.12238348882853975
iteration : 10384
train acc:  0.8046875
train loss:  0.4486593008041382
train gradient:  0.09228831525238655
iteration : 10385
train acc:  0.703125
train loss:  0.5146899223327637
train gradient:  0.1187661222817726
iteration : 10386
train acc:  0.78125
train loss:  0.459256649017334
train gradient:  0.12007142323024553
iteration : 10387
train acc:  0.71875
train loss:  0.5342512130737305
train gradient:  0.13823664621915877
iteration : 10388
train acc:  0.8046875
train loss:  0.47507402300834656
train gradient:  0.10969664886242411
iteration : 10389
train acc:  0.671875
train loss:  0.5490020513534546
train gradient:  0.1777491159448182
iteration : 10390
train acc:  0.75
train loss:  0.47527599334716797
train gradient:  0.12591394524924188
iteration : 10391
train acc:  0.7265625
train loss:  0.5114365220069885
train gradient:  0.16419942702976015
iteration : 10392
train acc:  0.7265625
train loss:  0.4853304624557495
train gradient:  0.13613778489391315
iteration : 10393
train acc:  0.7109375
train loss:  0.510704517364502
train gradient:  0.16796523670328367
iteration : 10394
train acc:  0.734375
train loss:  0.559057354927063
train gradient:  0.14021536510315336
iteration : 10395
train acc:  0.796875
train loss:  0.4591505527496338
train gradient:  0.1485850812418903
iteration : 10396
train acc:  0.75
train loss:  0.4594837725162506
train gradient:  0.13686527764527467
iteration : 10397
train acc:  0.7265625
train loss:  0.5174429416656494
train gradient:  0.21764811737801454
iteration : 10398
train acc:  0.78125
train loss:  0.4664202928543091
train gradient:  0.12176640063332132
iteration : 10399
train acc:  0.71875
train loss:  0.5051630735397339
train gradient:  0.16987119593383498
iteration : 10400
train acc:  0.6640625
train loss:  0.6058317422866821
train gradient:  0.19486828740487241
iteration : 10401
train acc:  0.7890625
train loss:  0.4556615948677063
train gradient:  0.09905584209113953
iteration : 10402
train acc:  0.7734375
train loss:  0.45396238565444946
train gradient:  0.11451588748017756
iteration : 10403
train acc:  0.7578125
train loss:  0.5609163045883179
train gradient:  0.1795501221621521
iteration : 10404
train acc:  0.8203125
train loss:  0.42378270626068115
train gradient:  0.11456160113514413
iteration : 10405
train acc:  0.6796875
train loss:  0.5496320724487305
train gradient:  0.15456027691041843
iteration : 10406
train acc:  0.6484375
train loss:  0.5897042751312256
train gradient:  0.2121096982313332
iteration : 10407
train acc:  0.75
train loss:  0.44447606801986694
train gradient:  0.11131315766252589
iteration : 10408
train acc:  0.7265625
train loss:  0.4962974190711975
train gradient:  0.11910130333462625
iteration : 10409
train acc:  0.734375
train loss:  0.49942177534103394
train gradient:  0.11958501083249609
iteration : 10410
train acc:  0.71875
train loss:  0.4882394075393677
train gradient:  0.1415293556835535
iteration : 10411
train acc:  0.7421875
train loss:  0.48742973804473877
train gradient:  0.1301075247701374
iteration : 10412
train acc:  0.7890625
train loss:  0.46155625581741333
train gradient:  0.1147784708597825
iteration : 10413
train acc:  0.75
train loss:  0.5450075268745422
train gradient:  0.14294387265776098
iteration : 10414
train acc:  0.7734375
train loss:  0.4585765600204468
train gradient:  0.10003925251947292
iteration : 10415
train acc:  0.8046875
train loss:  0.43089890480041504
train gradient:  0.105648891355885
iteration : 10416
train acc:  0.6875
train loss:  0.5285344123840332
train gradient:  0.20000397293309463
iteration : 10417
train acc:  0.8046875
train loss:  0.437070369720459
train gradient:  0.13432659974783104
iteration : 10418
train acc:  0.703125
train loss:  0.5010825991630554
train gradient:  0.13422896501124856
iteration : 10419
train acc:  0.671875
train loss:  0.5289765000343323
train gradient:  0.16551543229390436
iteration : 10420
train acc:  0.75
train loss:  0.4778439402580261
train gradient:  0.12431569890528911
iteration : 10421
train acc:  0.78125
train loss:  0.4891807436943054
train gradient:  0.11713006145035548
iteration : 10422
train acc:  0.7578125
train loss:  0.4940752685070038
train gradient:  0.1584378766267877
iteration : 10423
train acc:  0.71875
train loss:  0.5223735570907593
train gradient:  0.13449909940722865
iteration : 10424
train acc:  0.765625
train loss:  0.42842525243759155
train gradient:  0.115589500055581
iteration : 10425
train acc:  0.7421875
train loss:  0.4751267731189728
train gradient:  0.11930937301957197
iteration : 10426
train acc:  0.734375
train loss:  0.4727938771247864
train gradient:  0.12304094256288174
iteration : 10427
train acc:  0.7734375
train loss:  0.4481216073036194
train gradient:  0.12108389513113624
iteration : 10428
train acc:  0.75
train loss:  0.48810985684394836
train gradient:  0.1134423860070016
iteration : 10429
train acc:  0.8125
train loss:  0.46426039934158325
train gradient:  0.11634106074688542
iteration : 10430
train acc:  0.796875
train loss:  0.4949006736278534
train gradient:  0.15197832648436982
iteration : 10431
train acc:  0.6640625
train loss:  0.5602457523345947
train gradient:  0.18662408003634184
iteration : 10432
train acc:  0.7265625
train loss:  0.48116451501846313
train gradient:  0.11034830594821225
iteration : 10433
train acc:  0.75
train loss:  0.508938193321228
train gradient:  0.1436874452028471
iteration : 10434
train acc:  0.7109375
train loss:  0.5075857639312744
train gradient:  0.1351405458441759
iteration : 10435
train acc:  0.7578125
train loss:  0.45798856019973755
train gradient:  0.09857570273727115
iteration : 10436
train acc:  0.71875
train loss:  0.5593858957290649
train gradient:  0.13730315116337138
iteration : 10437
train acc:  0.7421875
train loss:  0.4919103980064392
train gradient:  0.11871569819526094
iteration : 10438
train acc:  0.75
train loss:  0.4754272699356079
train gradient:  0.16066694620382305
iteration : 10439
train acc:  0.7421875
train loss:  0.5526078343391418
train gradient:  0.16484583632743466
iteration : 10440
train acc:  0.765625
train loss:  0.5058540105819702
train gradient:  0.14110193698231843
iteration : 10441
train acc:  0.75
train loss:  0.49636831879615784
train gradient:  0.13951513316854225
iteration : 10442
train acc:  0.703125
train loss:  0.5348480343818665
train gradient:  0.14788219878681863
iteration : 10443
train acc:  0.765625
train loss:  0.4366137385368347
train gradient:  0.09770621123995292
iteration : 10444
train acc:  0.6796875
train loss:  0.5932794809341431
train gradient:  0.19390091519793978
iteration : 10445
train acc:  0.7265625
train loss:  0.47837069630622864
train gradient:  0.12262384114799316
iteration : 10446
train acc:  0.7421875
train loss:  0.46214139461517334
train gradient:  0.10111223973206065
iteration : 10447
train acc:  0.765625
train loss:  0.5038880109786987
train gradient:  0.1395333258973042
iteration : 10448
train acc:  0.828125
train loss:  0.4351781904697418
train gradient:  0.11194193846584957
iteration : 10449
train acc:  0.6953125
train loss:  0.5559728145599365
train gradient:  0.1699814752112358
iteration : 10450
train acc:  0.7421875
train loss:  0.4751780033111572
train gradient:  0.11538603667291096
iteration : 10451
train acc:  0.7109375
train loss:  0.5168332457542419
train gradient:  0.14289752564722374
iteration : 10452
train acc:  0.6953125
train loss:  0.5735269784927368
train gradient:  0.14096199543923027
iteration : 10453
train acc:  0.7578125
train loss:  0.5072447061538696
train gradient:  0.12494208147307712
iteration : 10454
train acc:  0.765625
train loss:  0.4998355507850647
train gradient:  0.1454769818010853
iteration : 10455
train acc:  0.671875
train loss:  0.5920306444168091
train gradient:  0.1363449071942739
iteration : 10456
train acc:  0.734375
train loss:  0.4438186585903168
train gradient:  0.1251859748966271
iteration : 10457
train acc:  0.7421875
train loss:  0.47456884384155273
train gradient:  0.118884165459699
iteration : 10458
train acc:  0.6640625
train loss:  0.5731889009475708
train gradient:  0.145820477043182
iteration : 10459
train acc:  0.7734375
train loss:  0.441203773021698
train gradient:  0.14590906654364258
iteration : 10460
train acc:  0.78125
train loss:  0.420026034116745
train gradient:  0.11225599231764857
iteration : 10461
train acc:  0.8046875
train loss:  0.448514461517334
train gradient:  0.1340349663682035
iteration : 10462
train acc:  0.7109375
train loss:  0.5017094612121582
train gradient:  0.1503610684045319
iteration : 10463
train acc:  0.734375
train loss:  0.45204856991767883
train gradient:  0.11135647792352905
iteration : 10464
train acc:  0.75
train loss:  0.4686834514141083
train gradient:  0.127896749779789
iteration : 10465
train acc:  0.734375
train loss:  0.5300107598304749
train gradient:  0.13989629472493545
iteration : 10466
train acc:  0.765625
train loss:  0.5028290748596191
train gradient:  0.17392610584953713
iteration : 10467
train acc:  0.7734375
train loss:  0.4215714931488037
train gradient:  0.11627384865759353
iteration : 10468
train acc:  0.7265625
train loss:  0.47542741894721985
train gradient:  0.12234109202497356
iteration : 10469
train acc:  0.6640625
train loss:  0.5339323282241821
train gradient:  0.17967739570959373
iteration : 10470
train acc:  0.6328125
train loss:  0.5689654350280762
train gradient:  0.2099196824762573
iteration : 10471
train acc:  0.765625
train loss:  0.4580751657485962
train gradient:  0.1305716896294203
iteration : 10472
train acc:  0.7734375
train loss:  0.4658321738243103
train gradient:  0.12985734604879814
iteration : 10473
train acc:  0.7265625
train loss:  0.46425896883010864
train gradient:  0.10816134157103727
iteration : 10474
train acc:  0.734375
train loss:  0.5288734436035156
train gradient:  0.1482930753356691
iteration : 10475
train acc:  0.7109375
train loss:  0.5127956867218018
train gradient:  0.12397584916618296
iteration : 10476
train acc:  0.7890625
train loss:  0.39758625626564026
train gradient:  0.10149358134777917
iteration : 10477
train acc:  0.7109375
train loss:  0.48818492889404297
train gradient:  0.14294894974173994
iteration : 10478
train acc:  0.765625
train loss:  0.4907557964324951
train gradient:  0.143811689534373
iteration : 10479
train acc:  0.75
train loss:  0.5624898076057434
train gradient:  0.17543967890619788
iteration : 10480
train acc:  0.7421875
train loss:  0.5451779365539551
train gradient:  0.14622451032445916
iteration : 10481
train acc:  0.7265625
train loss:  0.5186989903450012
train gradient:  0.1350725463715628
iteration : 10482
train acc:  0.71875
train loss:  0.5641667246818542
train gradient:  0.1636428824280291
iteration : 10483
train acc:  0.78125
train loss:  0.5054565668106079
train gradient:  0.12346951478679725
iteration : 10484
train acc:  0.765625
train loss:  0.481059193611145
train gradient:  0.12884403220022025
iteration : 10485
train acc:  0.6953125
train loss:  0.5391714572906494
train gradient:  0.12712282705385164
iteration : 10486
train acc:  0.7421875
train loss:  0.4556271731853485
train gradient:  0.11873541782187051
iteration : 10487
train acc:  0.75
train loss:  0.46650996804237366
train gradient:  0.1008633222493055
iteration : 10488
train acc:  0.7265625
train loss:  0.47465991973876953
train gradient:  0.10107699448242781
iteration : 10489
train acc:  0.703125
train loss:  0.4910309910774231
train gradient:  0.10979817975560509
iteration : 10490
train acc:  0.828125
train loss:  0.4505905508995056
train gradient:  0.12590270502839895
iteration : 10491
train acc:  0.7421875
train loss:  0.45234060287475586
train gradient:  0.1020214326552904
iteration : 10492
train acc:  0.7265625
train loss:  0.49725162982940674
train gradient:  0.1288598635714331
iteration : 10493
train acc:  0.6953125
train loss:  0.5091310739517212
train gradient:  0.130749548726332
iteration : 10494
train acc:  0.75
train loss:  0.503351092338562
train gradient:  0.1435349079703719
iteration : 10495
train acc:  0.7578125
train loss:  0.5129362940788269
train gradient:  0.19472102094399169
iteration : 10496
train acc:  0.734375
train loss:  0.48066866397857666
train gradient:  0.13320962742520143
iteration : 10497
train acc:  0.75
train loss:  0.4854532480239868
train gradient:  0.11341923748007358
iteration : 10498
train acc:  0.734375
train loss:  0.5269954204559326
train gradient:  0.1448473338086505
iteration : 10499
train acc:  0.7421875
train loss:  0.4914824962615967
train gradient:  0.14400915582549567
iteration : 10500
train acc:  0.75
train loss:  0.48116856813430786
train gradient:  0.13123672431499578
iteration : 10501
train acc:  0.7265625
train loss:  0.507107675075531
train gradient:  0.13045064771082407
iteration : 10502
train acc:  0.7421875
train loss:  0.48697155714035034
train gradient:  0.14761855312431138
iteration : 10503
train acc:  0.7734375
train loss:  0.474402517080307
train gradient:  0.12420461767256544
iteration : 10504
train acc:  0.7421875
train loss:  0.4937092065811157
train gradient:  0.126299151514206
iteration : 10505
train acc:  0.7734375
train loss:  0.45120930671691895
train gradient:  0.1272559889343332
iteration : 10506
train acc:  0.7421875
train loss:  0.4896007180213928
train gradient:  0.1760522166173738
iteration : 10507
train acc:  0.7421875
train loss:  0.4776809513568878
train gradient:  0.13666395969100537
iteration : 10508
train acc:  0.734375
train loss:  0.525159478187561
train gradient:  0.15508036968534766
iteration : 10509
train acc:  0.796875
train loss:  0.4369168281555176
train gradient:  0.11356427055316655
iteration : 10510
train acc:  0.7578125
train loss:  0.43061119318008423
train gradient:  0.09713338284403362
iteration : 10511
train acc:  0.71875
train loss:  0.5394692420959473
train gradient:  0.1453674956251877
iteration : 10512
train acc:  0.7734375
train loss:  0.47030267119407654
train gradient:  0.14536058852933947
iteration : 10513
train acc:  0.6875
train loss:  0.5406593680381775
train gradient:  0.13797725157357008
iteration : 10514
train acc:  0.796875
train loss:  0.43916749954223633
train gradient:  0.1294963270299827
iteration : 10515
train acc:  0.6953125
train loss:  0.5642773509025574
train gradient:  0.17504633682701617
iteration : 10516
train acc:  0.71875
train loss:  0.5491746664047241
train gradient:  0.15403389832072584
iteration : 10517
train acc:  0.7109375
train loss:  0.5389401912689209
train gradient:  0.12841479831408173
iteration : 10518
train acc:  0.6953125
train loss:  0.5390545725822449
train gradient:  0.18299644964800463
iteration : 10519
train acc:  0.71875
train loss:  0.4866913855075836
train gradient:  0.12405628027087173
iteration : 10520
train acc:  0.7578125
train loss:  0.5092073678970337
train gradient:  0.1883241578053939
iteration : 10521
train acc:  0.7578125
train loss:  0.48801279067993164
train gradient:  0.11970904171536188
iteration : 10522
train acc:  0.6875
train loss:  0.5200760960578918
train gradient:  0.14731555210006902
iteration : 10523
train acc:  0.7734375
train loss:  0.5333570837974548
train gradient:  0.15904195377601477
iteration : 10524
train acc:  0.7421875
train loss:  0.43358251452445984
train gradient:  0.09574659869093513
iteration : 10525
train acc:  0.7734375
train loss:  0.4433169364929199
train gradient:  0.09101586284853336
iteration : 10526
train acc:  0.7890625
train loss:  0.44647544622421265
train gradient:  0.11147924586226116
iteration : 10527
train acc:  0.6796875
train loss:  0.5706233382225037
train gradient:  0.1711084246741042
iteration : 10528
train acc:  0.71875
train loss:  0.5258831977844238
train gradient:  0.14627671941864817
iteration : 10529
train acc:  0.765625
train loss:  0.4692240357398987
train gradient:  0.10932467848182112
iteration : 10530
train acc:  0.7265625
train loss:  0.5076794624328613
train gradient:  0.15549600323838805
iteration : 10531
train acc:  0.859375
train loss:  0.395896315574646
train gradient:  0.10110886522495854
iteration : 10532
train acc:  0.7578125
train loss:  0.49717310070991516
train gradient:  0.1210308207011831
iteration : 10533
train acc:  0.7734375
train loss:  0.44469982385635376
train gradient:  0.10146257069882346
iteration : 10534
train acc:  0.6796875
train loss:  0.5401550531387329
train gradient:  0.21925305563009873
iteration : 10535
train acc:  0.75
train loss:  0.5022191405296326
train gradient:  0.14417355334496343
iteration : 10536
train acc:  0.7265625
train loss:  0.4850073456764221
train gradient:  0.10365300587885248
iteration : 10537
train acc:  0.71875
train loss:  0.5178511738777161
train gradient:  0.14038450279932083
iteration : 10538
train acc:  0.71875
train loss:  0.5528854131698608
train gradient:  0.1556744967549517
iteration : 10539
train acc:  0.703125
train loss:  0.501023530960083
train gradient:  0.14808807083515457
iteration : 10540
train acc:  0.6796875
train loss:  0.5595388412475586
train gradient:  0.16523782111388474
iteration : 10541
train acc:  0.7109375
train loss:  0.5293447971343994
train gradient:  0.14724746925791266
iteration : 10542
train acc:  0.71875
train loss:  0.4999849796295166
train gradient:  0.1162907401781322
iteration : 10543
train acc:  0.71875
train loss:  0.51430344581604
train gradient:  0.17928222143518618
iteration : 10544
train acc:  0.734375
train loss:  0.5091124773025513
train gradient:  0.16699890244487098
iteration : 10545
train acc:  0.71875
train loss:  0.5150883197784424
train gradient:  0.1538940306666231
iteration : 10546
train acc:  0.78125
train loss:  0.45730873942375183
train gradient:  0.1327301741664044
iteration : 10547
train acc:  0.8125
train loss:  0.4494727849960327
train gradient:  0.10485946156127772
iteration : 10548
train acc:  0.7421875
train loss:  0.5268442630767822
train gradient:  0.16344375996683402
iteration : 10549
train acc:  0.7265625
train loss:  0.5269620418548584
train gradient:  0.1394045289674713
iteration : 10550
train acc:  0.8359375
train loss:  0.40969911217689514
train gradient:  0.0862510646000809
iteration : 10551
train acc:  0.7578125
train loss:  0.47359877824783325
train gradient:  0.1665374415500787
iteration : 10552
train acc:  0.671875
train loss:  0.6252856850624084
train gradient:  0.19330852732804124
iteration : 10553
train acc:  0.6875
train loss:  0.5279040336608887
train gradient:  0.14476917762115576
iteration : 10554
train acc:  0.7109375
train loss:  0.48806318640708923
train gradient:  0.11278662462928725
iteration : 10555
train acc:  0.765625
train loss:  0.5007032155990601
train gradient:  0.13716205018925648
iteration : 10556
train acc:  0.75
train loss:  0.5333288908004761
train gradient:  0.16128035064875657
iteration : 10557
train acc:  0.7734375
train loss:  0.505362868309021
train gradient:  0.16129974405839395
iteration : 10558
train acc:  0.7578125
train loss:  0.46046245098114014
train gradient:  0.13160578199093503
iteration : 10559
train acc:  0.7578125
train loss:  0.4594648778438568
train gradient:  0.1054241900375406
iteration : 10560
train acc:  0.796875
train loss:  0.45386749505996704
train gradient:  0.11729020398048758
iteration : 10561
train acc:  0.7734375
train loss:  0.4397923946380615
train gradient:  0.11504371348905112
iteration : 10562
train acc:  0.7421875
train loss:  0.46402186155319214
train gradient:  0.10860700496435474
iteration : 10563
train acc:  0.765625
train loss:  0.46696358919143677
train gradient:  0.1271773226101282
iteration : 10564
train acc:  0.703125
train loss:  0.4908095896244049
train gradient:  0.1396268299232726
iteration : 10565
train acc:  0.7265625
train loss:  0.5337924361228943
train gradient:  0.12623118702031955
iteration : 10566
train acc:  0.734375
train loss:  0.530409574508667
train gradient:  0.19403184214285363
iteration : 10567
train acc:  0.6953125
train loss:  0.5602920055389404
train gradient:  0.11769830009050988
iteration : 10568
train acc:  0.7578125
train loss:  0.5047705173492432
train gradient:  0.12914332237572493
iteration : 10569
train acc:  0.7890625
train loss:  0.4708658456802368
train gradient:  0.10646402231000673
iteration : 10570
train acc:  0.765625
train loss:  0.4496186375617981
train gradient:  0.10374591228109754
iteration : 10571
train acc:  0.7109375
train loss:  0.5654967427253723
train gradient:  0.16051546626974855
iteration : 10572
train acc:  0.7265625
train loss:  0.469235897064209
train gradient:  0.11389822029334996
iteration : 10573
train acc:  0.6875
train loss:  0.5401474833488464
train gradient:  0.13029447077552092
iteration : 10574
train acc:  0.828125
train loss:  0.43137580156326294
train gradient:  0.13464097577481027
iteration : 10575
train acc:  0.65625
train loss:  0.6020781993865967
train gradient:  0.16676496650662373
iteration : 10576
train acc:  0.734375
train loss:  0.464082270860672
train gradient:  0.12076266662731643
iteration : 10577
train acc:  0.7578125
train loss:  0.4807489514350891
train gradient:  0.13120647027848192
iteration : 10578
train acc:  0.7578125
train loss:  0.443321168422699
train gradient:  0.12139507737680746
iteration : 10579
train acc:  0.7421875
train loss:  0.4935501217842102
train gradient:  0.11080245839999815
iteration : 10580
train acc:  0.75
train loss:  0.5069447755813599
train gradient:  0.1424234037295515
iteration : 10581
train acc:  0.7265625
train loss:  0.5264557003974915
train gradient:  0.15045502925193086
iteration : 10582
train acc:  0.71875
train loss:  0.509020209312439
train gradient:  0.10785294941987233
iteration : 10583
train acc:  0.7421875
train loss:  0.4868910610675812
train gradient:  0.14142314129135422
iteration : 10584
train acc:  0.734375
train loss:  0.4945579767227173
train gradient:  0.13869601663430609
iteration : 10585
train acc:  0.765625
train loss:  0.45074790716171265
train gradient:  0.14482552374912186
iteration : 10586
train acc:  0.71875
train loss:  0.4943512976169586
train gradient:  0.11500211967857125
iteration : 10587
train acc:  0.734375
train loss:  0.4601978063583374
train gradient:  0.09700156364867457
iteration : 10588
train acc:  0.7265625
train loss:  0.5458923578262329
train gradient:  0.13219940402203234
iteration : 10589
train acc:  0.765625
train loss:  0.49654608964920044
train gradient:  0.13690645682536554
iteration : 10590
train acc:  0.8359375
train loss:  0.3835516571998596
train gradient:  0.10173192693788938
iteration : 10591
train acc:  0.7578125
train loss:  0.4627370238304138
train gradient:  0.14726834639805642
iteration : 10592
train acc:  0.71875
train loss:  0.5043774843215942
train gradient:  0.1336311340978636
iteration : 10593
train acc:  0.7734375
train loss:  0.4373050928115845
train gradient:  0.10897253251545712
iteration : 10594
train acc:  0.734375
train loss:  0.5024584531784058
train gradient:  0.1520281953417561
iteration : 10595
train acc:  0.7578125
train loss:  0.496356725692749
train gradient:  0.11550902796658628
iteration : 10596
train acc:  0.7265625
train loss:  0.5587081909179688
train gradient:  0.20250454888513653
iteration : 10597
train acc:  0.7265625
train loss:  0.4972080588340759
train gradient:  0.12494812666146735
iteration : 10598
train acc:  0.7109375
train loss:  0.4901622533798218
train gradient:  0.14166459487436484
iteration : 10599
train acc:  0.7734375
train loss:  0.47321805357933044
train gradient:  0.10348572545899276
iteration : 10600
train acc:  0.734375
train loss:  0.5092120170593262
train gradient:  0.1569732186151256
iteration : 10601
train acc:  0.7109375
train loss:  0.48483026027679443
train gradient:  0.12171900562838041
iteration : 10602
train acc:  0.6953125
train loss:  0.5324947237968445
train gradient:  0.15390769107506697
iteration : 10603
train acc:  0.8046875
train loss:  0.4249945282936096
train gradient:  0.13145530619317777
iteration : 10604
train acc:  0.8046875
train loss:  0.4558994770050049
train gradient:  0.12406105220154255
iteration : 10605
train acc:  0.734375
train loss:  0.5479482412338257
train gradient:  0.1563949442386955
iteration : 10606
train acc:  0.7265625
train loss:  0.5737878084182739
train gradient:  0.16982649227309232
iteration : 10607
train acc:  0.7578125
train loss:  0.4619458317756653
train gradient:  0.1612637742113271
iteration : 10608
train acc:  0.75
train loss:  0.48913854360580444
train gradient:  0.11776895970607709
iteration : 10609
train acc:  0.7421875
train loss:  0.4783951938152313
train gradient:  0.1312334481718654
iteration : 10610
train acc:  0.671875
train loss:  0.5867655277252197
train gradient:  0.2021985401769709
iteration : 10611
train acc:  0.7109375
train loss:  0.523460865020752
train gradient:  0.1240567227471962
iteration : 10612
train acc:  0.734375
train loss:  0.533647358417511
train gradient:  0.16920184951994063
iteration : 10613
train acc:  0.75
train loss:  0.47301995754241943
train gradient:  0.13989463642761224
iteration : 10614
train acc:  0.765625
train loss:  0.5016208291053772
train gradient:  0.1549771589988434
iteration : 10615
train acc:  0.703125
train loss:  0.5046495795249939
train gradient:  0.11614459290778457
iteration : 10616
train acc:  0.703125
train loss:  0.5571165680885315
train gradient:  0.17825500128022176
iteration : 10617
train acc:  0.6796875
train loss:  0.5437447428703308
train gradient:  0.1706890241614436
iteration : 10618
train acc:  0.7890625
train loss:  0.47470933198928833
train gradient:  0.18312279427628614
iteration : 10619
train acc:  0.703125
train loss:  0.5557668209075928
train gradient:  0.15336687224684842
iteration : 10620
train acc:  0.796875
train loss:  0.4805395305156708
train gradient:  0.12207215484888614
iteration : 10621
train acc:  0.71875
train loss:  0.4749094843864441
train gradient:  0.11979335160166683
iteration : 10622
train acc:  0.6953125
train loss:  0.49839502573013306
train gradient:  0.14762778660061635
iteration : 10623
train acc:  0.78125
train loss:  0.48621541261672974
train gradient:  0.1366788053969606
iteration : 10624
train acc:  0.75
train loss:  0.5724098682403564
train gradient:  0.16615654516241526
iteration : 10625
train acc:  0.734375
train loss:  0.49372678995132446
train gradient:  0.12608392053780632
iteration : 10626
train acc:  0.7578125
train loss:  0.4597015976905823
train gradient:  0.1495947587830943
iteration : 10627
train acc:  0.65625
train loss:  0.5972803831100464
train gradient:  0.1651534036045137
iteration : 10628
train acc:  0.6953125
train loss:  0.5418393611907959
train gradient:  0.16806037590389897
iteration : 10629
train acc:  0.7421875
train loss:  0.4730684161186218
train gradient:  0.11232006851512713
iteration : 10630
train acc:  0.7578125
train loss:  0.45157116651535034
train gradient:  0.08834757399897658
iteration : 10631
train acc:  0.734375
train loss:  0.5105195045471191
train gradient:  0.1375959134536779
iteration : 10632
train acc:  0.75
train loss:  0.4740927219390869
train gradient:  0.14509269241604694
iteration : 10633
train acc:  0.78125
train loss:  0.44842156767845154
train gradient:  0.11745706140875511
iteration : 10634
train acc:  0.6953125
train loss:  0.5483490228652954
train gradient:  0.16830479638096407
iteration : 10635
train acc:  0.6640625
train loss:  0.6157476902008057
train gradient:  0.2545138632527529
iteration : 10636
train acc:  0.6953125
train loss:  0.5582687258720398
train gradient:  0.16286721580486063
iteration : 10637
train acc:  0.796875
train loss:  0.4394141435623169
train gradient:  0.11931930870413755
iteration : 10638
train acc:  0.6796875
train loss:  0.5635359287261963
train gradient:  0.16983190900352346
iteration : 10639
train acc:  0.7109375
train loss:  0.4957412779331207
train gradient:  0.1793284005529201
iteration : 10640
train acc:  0.8046875
train loss:  0.4361284077167511
train gradient:  0.12788506805139177
iteration : 10641
train acc:  0.7734375
train loss:  0.4981880784034729
train gradient:  0.157198615713891
iteration : 10642
train acc:  0.78125
train loss:  0.44849130511283875
train gradient:  0.10986316308377926
iteration : 10643
train acc:  0.765625
train loss:  0.4671207368373871
train gradient:  0.09689331021321414
iteration : 10644
train acc:  0.7890625
train loss:  0.4350927472114563
train gradient:  0.10295875824932191
iteration : 10645
train acc:  0.78125
train loss:  0.4886457324028015
train gradient:  0.12864426443582694
iteration : 10646
train acc:  0.734375
train loss:  0.5081990957260132
train gradient:  0.13435277299003381
iteration : 10647
train acc:  0.7109375
train loss:  0.5169686675071716
train gradient:  0.13481362013737014
iteration : 10648
train acc:  0.71875
train loss:  0.5241246223449707
train gradient:  0.18606934689191984
iteration : 10649
train acc:  0.703125
train loss:  0.48533207178115845
train gradient:  0.1168591558554478
iteration : 10650
train acc:  0.8125
train loss:  0.4470007121562958
train gradient:  0.10245264073551866
iteration : 10651
train acc:  0.765625
train loss:  0.4803224205970764
train gradient:  0.15471459160660034
iteration : 10652
train acc:  0.703125
train loss:  0.4958561062812805
train gradient:  0.12920646066216326
iteration : 10653
train acc:  0.7578125
train loss:  0.4900929629802704
train gradient:  0.13371180208981348
iteration : 10654
train acc:  0.703125
train loss:  0.555526614189148
train gradient:  0.1474053702708708
iteration : 10655
train acc:  0.671875
train loss:  0.53838050365448
train gradient:  0.1570744097245832
iteration : 10656
train acc:  0.78125
train loss:  0.42830920219421387
train gradient:  0.1073401544139006
iteration : 10657
train acc:  0.7265625
train loss:  0.5285750031471252
train gradient:  0.13396067305371517
iteration : 10658
train acc:  0.7734375
train loss:  0.4298366606235504
train gradient:  0.0965074783010406
iteration : 10659
train acc:  0.7890625
train loss:  0.47562146186828613
train gradient:  0.12502969846733591
iteration : 10660
train acc:  0.7578125
train loss:  0.46339350938796997
train gradient:  0.10949687420486978
iteration : 10661
train acc:  0.796875
train loss:  0.46157997846603394
train gradient:  0.10718306017930455
iteration : 10662
train acc:  0.7109375
train loss:  0.5104600191116333
train gradient:  0.1560688388271567
iteration : 10663
train acc:  0.7578125
train loss:  0.4791579842567444
train gradient:  0.11236415304601738
iteration : 10664
train acc:  0.7734375
train loss:  0.4569193124771118
train gradient:  0.10270633664680005
iteration : 10665
train acc:  0.7421875
train loss:  0.501652717590332
train gradient:  0.13285594380083565
iteration : 10666
train acc:  0.7421875
train loss:  0.48366570472717285
train gradient:  0.11747605359959346
iteration : 10667
train acc:  0.75
train loss:  0.48971492052078247
train gradient:  0.11730487192549437
iteration : 10668
train acc:  0.734375
train loss:  0.5134375095367432
train gradient:  0.1411385531540707
iteration : 10669
train acc:  0.765625
train loss:  0.494223415851593
train gradient:  0.11527790903278755
iteration : 10670
train acc:  0.765625
train loss:  0.4382322132587433
train gradient:  0.09497352480173427
iteration : 10671
train acc:  0.7265625
train loss:  0.5111640095710754
train gradient:  0.13575147483589392
iteration : 10672
train acc:  0.6875
train loss:  0.52996826171875
train gradient:  0.14452241256157394
iteration : 10673
train acc:  0.7265625
train loss:  0.5391242504119873
train gradient:  0.13168575275817912
iteration : 10674
train acc:  0.8046875
train loss:  0.44507482647895813
train gradient:  0.11231740799388105
iteration : 10675
train acc:  0.7265625
train loss:  0.5054540634155273
train gradient:  0.12606441291766543
iteration : 10676
train acc:  0.7734375
train loss:  0.46291986107826233
train gradient:  0.1398184335177951
iteration : 10677
train acc:  0.8203125
train loss:  0.40495067834854126
train gradient:  0.09112014104067569
iteration : 10678
train acc:  0.765625
train loss:  0.487089067697525
train gradient:  0.1398925859371154
iteration : 10679
train acc:  0.765625
train loss:  0.5229594707489014
train gradient:  0.13323666849315624
iteration : 10680
train acc:  0.7734375
train loss:  0.4670052230358124
train gradient:  0.12453323514160755
iteration : 10681
train acc:  0.7890625
train loss:  0.46436163783073425
train gradient:  0.11171809118307938
iteration : 10682
train acc:  0.7734375
train loss:  0.4912168085575104
train gradient:  0.12283020105426071
iteration : 10683
train acc:  0.71875
train loss:  0.5074304342269897
train gradient:  0.11475887781971804
iteration : 10684
train acc:  0.7265625
train loss:  0.4799799919128418
train gradient:  0.1279943005469045
iteration : 10685
train acc:  0.8125
train loss:  0.43234819173812866
train gradient:  0.07628156016260865
iteration : 10686
train acc:  0.6875
train loss:  0.5983462333679199
train gradient:  0.1858927862768095
iteration : 10687
train acc:  0.71875
train loss:  0.47685617208480835
train gradient:  0.12111212063476881
iteration : 10688
train acc:  0.8203125
train loss:  0.40647101402282715
train gradient:  0.1097149257924661
iteration : 10689
train acc:  0.734375
train loss:  0.4841291904449463
train gradient:  0.14896515700706117
iteration : 10690
train acc:  0.7109375
train loss:  0.5601324439048767
train gradient:  0.16680162722724357
iteration : 10691
train acc:  0.6796875
train loss:  0.5556741952896118
train gradient:  0.13496663557277688
iteration : 10692
train acc:  0.703125
train loss:  0.5780501365661621
train gradient:  0.16771380996190866
iteration : 10693
train acc:  0.78125
train loss:  0.4457392394542694
train gradient:  0.11192114169875922
iteration : 10694
train acc:  0.734375
train loss:  0.4717295169830322
train gradient:  0.11526556327397967
iteration : 10695
train acc:  0.75
train loss:  0.5141359567642212
train gradient:  0.199931614938824
iteration : 10696
train acc:  0.7578125
train loss:  0.5042120218276978
train gradient:  0.11849383148931275
iteration : 10697
train acc:  0.7421875
train loss:  0.5174132585525513
train gradient:  0.14081129270077736
iteration : 10698
train acc:  0.703125
train loss:  0.5101102590560913
train gradient:  0.14598348144478765
iteration : 10699
train acc:  0.671875
train loss:  0.6055654287338257
train gradient:  0.20383021159727324
iteration : 10700
train acc:  0.7734375
train loss:  0.47708743810653687
train gradient:  0.11411885254391393
iteration : 10701
train acc:  0.7734375
train loss:  0.47882384061813354
train gradient:  0.0964327928195665
iteration : 10702
train acc:  0.8125
train loss:  0.41116899251937866
train gradient:  0.07848675743990714
iteration : 10703
train acc:  0.71875
train loss:  0.5089513659477234
train gradient:  0.16786319742308126
iteration : 10704
train acc:  0.7421875
train loss:  0.5257636904716492
train gradient:  0.1575052866504473
iteration : 10705
train acc:  0.6953125
train loss:  0.47306209802627563
train gradient:  0.11510582501563346
iteration : 10706
train acc:  0.734375
train loss:  0.5085824131965637
train gradient:  0.1281512172057112
iteration : 10707
train acc:  0.75
train loss:  0.5187382698059082
train gradient:  0.13932564941770342
iteration : 10708
train acc:  0.6875
train loss:  0.5403891801834106
train gradient:  0.1480390520284327
iteration : 10709
train acc:  0.71875
train loss:  0.5295788049697876
train gradient:  0.1510234057014554
iteration : 10710
train acc:  0.7265625
train loss:  0.49865198135375977
train gradient:  0.10834720090989264
iteration : 10711
train acc:  0.796875
train loss:  0.4325213134288788
train gradient:  0.15341801188530785
iteration : 10712
train acc:  0.796875
train loss:  0.43279552459716797
train gradient:  0.12237746626121239
iteration : 10713
train acc:  0.7109375
train loss:  0.5103554725646973
train gradient:  0.12271885513430385
iteration : 10714
train acc:  0.703125
train loss:  0.5300824642181396
train gradient:  0.15917146369880675
iteration : 10715
train acc:  0.703125
train loss:  0.5081043243408203
train gradient:  0.1736131751740875
iteration : 10716
train acc:  0.71875
train loss:  0.5315773487091064
train gradient:  0.1466080669641505
iteration : 10717
train acc:  0.7109375
train loss:  0.4866448640823364
train gradient:  0.13830503561074337
iteration : 10718
train acc:  0.6953125
train loss:  0.5753463506698608
train gradient:  0.20033341872049146
iteration : 10719
train acc:  0.75
train loss:  0.4831779897212982
train gradient:  0.14857154826806968
iteration : 10720
train acc:  0.7109375
train loss:  0.49850553274154663
train gradient:  0.10511729289585103
iteration : 10721
train acc:  0.7265625
train loss:  0.5280694961547852
train gradient:  0.16876218501478735
iteration : 10722
train acc:  0.734375
train loss:  0.5275465250015259
train gradient:  0.1819203972725458
iteration : 10723
train acc:  0.71875
train loss:  0.4866105020046234
train gradient:  0.11229298880304239
iteration : 10724
train acc:  0.734375
train loss:  0.47546836733818054
train gradient:  0.12486923013131884
iteration : 10725
train acc:  0.7734375
train loss:  0.47563689947128296
train gradient:  0.10994454191294778
iteration : 10726
train acc:  0.7578125
train loss:  0.4781610369682312
train gradient:  0.11814790640073111
iteration : 10727
train acc:  0.703125
train loss:  0.4684320092201233
train gradient:  0.12447523187907159
iteration : 10728
train acc:  0.7421875
train loss:  0.4791738986968994
train gradient:  0.12755973893318348
iteration : 10729
train acc:  0.7265625
train loss:  0.5291247963905334
train gradient:  0.1486150069154638
iteration : 10730
train acc:  0.765625
train loss:  0.5257516503334045
train gradient:  0.16690683265981204
iteration : 10731
train acc:  0.7421875
train loss:  0.4845730662345886
train gradient:  0.12287925067266832
iteration : 10732
train acc:  0.78125
train loss:  0.49941694736480713
train gradient:  0.14175290369218413
iteration : 10733
train acc:  0.6875
train loss:  0.5724223256111145
train gradient:  0.16244806309484727
iteration : 10734
train acc:  0.765625
train loss:  0.45411205291748047
train gradient:  0.12299713728452862
iteration : 10735
train acc:  0.7265625
train loss:  0.4724263548851013
train gradient:  0.1258081138617034
iteration : 10736
train acc:  0.7578125
train loss:  0.4192752242088318
train gradient:  0.08129343752554039
iteration : 10737
train acc:  0.703125
train loss:  0.5092823505401611
train gradient:  0.18297028883864472
iteration : 10738
train acc:  0.734375
train loss:  0.5311970114707947
train gradient:  0.15035715735383426
iteration : 10739
train acc:  0.71875
train loss:  0.518438458442688
train gradient:  0.1485170540974467
iteration : 10740
train acc:  0.7890625
train loss:  0.438318133354187
train gradient:  0.0941692681618444
iteration : 10741
train acc:  0.71875
train loss:  0.5123817920684814
train gradient:  0.12530729756647968
iteration : 10742
train acc:  0.6953125
train loss:  0.5373216867446899
train gradient:  0.16531049579144713
iteration : 10743
train acc:  0.734375
train loss:  0.5439609289169312
train gradient:  0.15158444197110577
iteration : 10744
train acc:  0.7734375
train loss:  0.49371689558029175
train gradient:  0.11693546119484334
iteration : 10745
train acc:  0.7421875
train loss:  0.5289489030838013
train gradient:  0.12601470911923446
iteration : 10746
train acc:  0.75
train loss:  0.5184046030044556
train gradient:  0.14690887744280695
iteration : 10747
train acc:  0.6875
train loss:  0.5584655404090881
train gradient:  0.1487864855881936
iteration : 10748
train acc:  0.71875
train loss:  0.5167500972747803
train gradient:  0.14133362776155287
iteration : 10749
train acc:  0.734375
train loss:  0.4881461560726166
train gradient:  0.11666865417431652
iteration : 10750
train acc:  0.765625
train loss:  0.4825201630592346
train gradient:  0.12334269297504445
iteration : 10751
train acc:  0.6484375
train loss:  0.5656619071960449
train gradient:  0.18073331238694315
iteration : 10752
train acc:  0.734375
train loss:  0.5508785247802734
train gradient:  0.1627699209446371
iteration : 10753
train acc:  0.671875
train loss:  0.5890797972679138
train gradient:  0.17417444860721387
iteration : 10754
train acc:  0.7734375
train loss:  0.4711776375770569
train gradient:  0.11687554762232504
iteration : 10755
train acc:  0.703125
train loss:  0.5314136743545532
train gradient:  0.16223950429588663
iteration : 10756
train acc:  0.7578125
train loss:  0.4749329686164856
train gradient:  0.10269284413732792
iteration : 10757
train acc:  0.8125
train loss:  0.44170933961868286
train gradient:  0.0982413550606662
iteration : 10758
train acc:  0.6796875
train loss:  0.5362181663513184
train gradient:  0.15117295739410722
iteration : 10759
train acc:  0.734375
train loss:  0.48491281270980835
train gradient:  0.14125230841807496
iteration : 10760
train acc:  0.7265625
train loss:  0.5402246117591858
train gradient:  0.12764392176569883
iteration : 10761
train acc:  0.7734375
train loss:  0.4330308437347412
train gradient:  0.10484864297985412
iteration : 10762
train acc:  0.7421875
train loss:  0.5248979330062866
train gradient:  0.14670612868849303
iteration : 10763
train acc:  0.75
train loss:  0.45792484283447266
train gradient:  0.10698446368145376
iteration : 10764
train acc:  0.7265625
train loss:  0.5123226642608643
train gradient:  0.14672768417980497
iteration : 10765
train acc:  0.7890625
train loss:  0.4226956367492676
train gradient:  0.09916889809528893
iteration : 10766
train acc:  0.7109375
train loss:  0.5578211545944214
train gradient:  0.1479095419187016
iteration : 10767
train acc:  0.78125
train loss:  0.47338464856147766
train gradient:  0.11553837565298652
iteration : 10768
train acc:  0.75
train loss:  0.5119212865829468
train gradient:  0.1835334627523368
iteration : 10769
train acc:  0.75
train loss:  0.49156516790390015
train gradient:  0.12036629324572665
iteration : 10770
train acc:  0.765625
train loss:  0.4657140076160431
train gradient:  0.13220309035747618
iteration : 10771
train acc:  0.703125
train loss:  0.5591776967048645
train gradient:  0.18054572000550528
iteration : 10772
train acc:  0.6953125
train loss:  0.5426356196403503
train gradient:  0.15721205476203182
iteration : 10773
train acc:  0.7109375
train loss:  0.5642499923706055
train gradient:  0.18049139893033372
iteration : 10774
train acc:  0.7734375
train loss:  0.4741135835647583
train gradient:  0.13223435193927496
iteration : 10775
train acc:  0.7734375
train loss:  0.48736709356307983
train gradient:  0.16382375673008706
iteration : 10776
train acc:  0.75
train loss:  0.46368807554244995
train gradient:  0.10874153887689311
iteration : 10777
train acc:  0.8359375
train loss:  0.44171229004859924
train gradient:  0.13025810593586695
iteration : 10778
train acc:  0.7578125
train loss:  0.4621727764606476
train gradient:  0.11399358705972845
iteration : 10779
train acc:  0.8046875
train loss:  0.4082362651824951
train gradient:  0.10115460542364019
iteration : 10780
train acc:  0.703125
train loss:  0.5382885932922363
train gradient:  0.16247909240884373
iteration : 10781
train acc:  0.734375
train loss:  0.4618988633155823
train gradient:  0.11120607840976501
iteration : 10782
train acc:  0.7265625
train loss:  0.4982167184352875
train gradient:  0.14664492513852456
iteration : 10783
train acc:  0.7578125
train loss:  0.514757513999939
train gradient:  0.16866975787919036
iteration : 10784
train acc:  0.7421875
train loss:  0.4262987971305847
train gradient:  0.10346285109743694
iteration : 10785
train acc:  0.7265625
train loss:  0.5236551761627197
train gradient:  0.12709766413240392
iteration : 10786
train acc:  0.7578125
train loss:  0.508272647857666
train gradient:  0.1632743337013351
iteration : 10787
train acc:  0.734375
train loss:  0.5305526852607727
train gradient:  0.14278872420055533
iteration : 10788
train acc:  0.75
train loss:  0.47180822491645813
train gradient:  0.11106055296626095
iteration : 10789
train acc:  0.75
train loss:  0.4604489803314209
train gradient:  0.10991649164964572
iteration : 10790
train acc:  0.6953125
train loss:  0.5578337907791138
train gradient:  0.15306454038844491
iteration : 10791
train acc:  0.8046875
train loss:  0.49317777156829834
train gradient:  0.15144285150433562
iteration : 10792
train acc:  0.7421875
train loss:  0.4845740497112274
train gradient:  0.14561339068801823
iteration : 10793
train acc:  0.828125
train loss:  0.4833483099937439
train gradient:  0.10699583667103053
iteration : 10794
train acc:  0.6796875
train loss:  0.5894430875778198
train gradient:  0.17328836361164418
iteration : 10795
train acc:  0.7421875
train loss:  0.46959298849105835
train gradient:  0.11384282580173638
iteration : 10796
train acc:  0.7421875
train loss:  0.47840365767478943
train gradient:  0.1429164576352572
iteration : 10797
train acc:  0.7109375
train loss:  0.49557894468307495
train gradient:  0.15358785054866098
iteration : 10798
train acc:  0.796875
train loss:  0.4829762876033783
train gradient:  0.167227990757309
iteration : 10799
train acc:  0.7421875
train loss:  0.4982469975948334
train gradient:  0.121589198460917
iteration : 10800
train acc:  0.7890625
train loss:  0.4283944368362427
train gradient:  0.10893925186383158
iteration : 10801
train acc:  0.765625
train loss:  0.4788787364959717
train gradient:  0.10432072247337028
iteration : 10802
train acc:  0.7578125
train loss:  0.45685485005378723
train gradient:  0.1289037978576654
iteration : 10803
train acc:  0.6875
train loss:  0.5519753694534302
train gradient:  0.20408648169838264
iteration : 10804
train acc:  0.734375
train loss:  0.5399940013885498
train gradient:  0.13281997641968613
iteration : 10805
train acc:  0.7265625
train loss:  0.48670685291290283
train gradient:  0.12361486101292506
iteration : 10806
train acc:  0.7109375
train loss:  0.5019882917404175
train gradient:  0.17243705169045248
iteration : 10807
train acc:  0.7265625
train loss:  0.5169030427932739
train gradient:  0.12406700974122677
iteration : 10808
train acc:  0.7421875
train loss:  0.5136381387710571
train gradient:  0.14262984215988983
iteration : 10809
train acc:  0.7734375
train loss:  0.4522913992404938
train gradient:  0.0936597192898312
iteration : 10810
train acc:  0.6953125
train loss:  0.5449386239051819
train gradient:  0.14325871162930195
iteration : 10811
train acc:  0.7421875
train loss:  0.4628552794456482
train gradient:  0.10971507111768843
iteration : 10812
train acc:  0.7578125
train loss:  0.47059571743011475
train gradient:  0.11161702570477061
iteration : 10813
train acc:  0.7578125
train loss:  0.48558712005615234
train gradient:  0.11189714532839878
iteration : 10814
train acc:  0.796875
train loss:  0.4744437336921692
train gradient:  0.15287943181898325
iteration : 10815
train acc:  0.7734375
train loss:  0.5020618438720703
train gradient:  0.18188586235948467
iteration : 10816
train acc:  0.7109375
train loss:  0.4911569058895111
train gradient:  0.13242737438377467
iteration : 10817
train acc:  0.71875
train loss:  0.5730710625648499
train gradient:  0.15973270190328342
iteration : 10818
train acc:  0.6875
train loss:  0.6096044778823853
train gradient:  0.197443671918889
iteration : 10819
train acc:  0.75
train loss:  0.4909328818321228
train gradient:  0.1493503116137184
iteration : 10820
train acc:  0.7265625
train loss:  0.5637032985687256
train gradient:  0.18646884298732785
iteration : 10821
train acc:  0.7890625
train loss:  0.44242048263549805
train gradient:  0.10862721607199874
iteration : 10822
train acc:  0.7265625
train loss:  0.4709014296531677
train gradient:  0.11495628462509798
iteration : 10823
train acc:  0.75
train loss:  0.4813108742237091
train gradient:  0.1399270770479241
iteration : 10824
train acc:  0.7734375
train loss:  0.44648489356040955
train gradient:  0.09607800381271134
iteration : 10825
train acc:  0.6875
train loss:  0.5199475884437561
train gradient:  0.14853826283119842
iteration : 10826
train acc:  0.71875
train loss:  0.5055336952209473
train gradient:  0.13574840985542094
iteration : 10827
train acc:  0.671875
train loss:  0.5747275352478027
train gradient:  0.21150288020432056
iteration : 10828
train acc:  0.75
train loss:  0.5044295787811279
train gradient:  0.16813235589443676
iteration : 10829
train acc:  0.7109375
train loss:  0.5151715874671936
train gradient:  0.20016715146254072
iteration : 10830
train acc:  0.7578125
train loss:  0.4655575752258301
train gradient:  0.13749404107392138
iteration : 10831
train acc:  0.7109375
train loss:  0.5611493587493896
train gradient:  0.16018189158104482
iteration : 10832
train acc:  0.7265625
train loss:  0.5171729922294617
train gradient:  0.16231910607516734
iteration : 10833
train acc:  0.8046875
train loss:  0.4144015908241272
train gradient:  0.10650067412820728
iteration : 10834
train acc:  0.7890625
train loss:  0.4713670015335083
train gradient:  0.11890032588419343
iteration : 10835
train acc:  0.796875
train loss:  0.4261484444141388
train gradient:  0.10600363927385881
iteration : 10836
train acc:  0.703125
train loss:  0.5373971462249756
train gradient:  0.15923061411683595
iteration : 10837
train acc:  0.6953125
train loss:  0.5392838716506958
train gradient:  0.16532899306058868
iteration : 10838
train acc:  0.765625
train loss:  0.4646979570388794
train gradient:  0.1372688441450476
iteration : 10839
train acc:  0.734375
train loss:  0.4957156777381897
train gradient:  0.13161243562276864
iteration : 10840
train acc:  0.7578125
train loss:  0.4766683578491211
train gradient:  0.12136335042163372
iteration : 10841
train acc:  0.765625
train loss:  0.500818133354187
train gradient:  0.1595857327045646
iteration : 10842
train acc:  0.6953125
train loss:  0.5014853477478027
train gradient:  0.11305676522662349
iteration : 10843
train acc:  0.7265625
train loss:  0.5448625087738037
train gradient:  0.17689662126255132
iteration : 10844
train acc:  0.7890625
train loss:  0.4640948474407196
train gradient:  0.10197005958878728
iteration : 10845
train acc:  0.71875
train loss:  0.5260813236236572
train gradient:  0.15234259997194666
iteration : 10846
train acc:  0.75
train loss:  0.4956625699996948
train gradient:  0.1591227113778927
iteration : 10847
train acc:  0.734375
train loss:  0.5042939186096191
train gradient:  0.15902664572061853
iteration : 10848
train acc:  0.7421875
train loss:  0.5183473825454712
train gradient:  0.10882851691999358
iteration : 10849
train acc:  0.7734375
train loss:  0.4629613161087036
train gradient:  0.11704171639401993
iteration : 10850
train acc:  0.765625
train loss:  0.4513082504272461
train gradient:  0.10017877277207139
iteration : 10851
train acc:  0.734375
train loss:  0.496825635433197
train gradient:  0.12170373412628874
iteration : 10852
train acc:  0.7890625
train loss:  0.41756802797317505
train gradient:  0.0936320643456029
iteration : 10853
train acc:  0.6796875
train loss:  0.5215632915496826
train gradient:  0.1280142791261997
iteration : 10854
train acc:  0.75
train loss:  0.46342557668685913
train gradient:  0.11862940844307639
iteration : 10855
train acc:  0.671875
train loss:  0.5695862770080566
train gradient:  0.19285545413375044
iteration : 10856
train acc:  0.75
train loss:  0.4812363386154175
train gradient:  0.12727741806991963
iteration : 10857
train acc:  0.703125
train loss:  0.45721685886383057
train gradient:  0.11031143951675319
iteration : 10858
train acc:  0.6484375
train loss:  0.6012259721755981
train gradient:  0.18084796594628955
iteration : 10859
train acc:  0.765625
train loss:  0.46803516149520874
train gradient:  0.10235310357087463
iteration : 10860
train acc:  0.7578125
train loss:  0.4787970185279846
train gradient:  0.1097768211016918
iteration : 10861
train acc:  0.75
train loss:  0.5514330267906189
train gradient:  0.14096653486370084
iteration : 10862
train acc:  0.75
train loss:  0.4567181468009949
train gradient:  0.09213371998422602
iteration : 10863
train acc:  0.7734375
train loss:  0.4446042776107788
train gradient:  0.1142302554089651
iteration : 10864
train acc:  0.765625
train loss:  0.4749704897403717
train gradient:  0.10453130376992341
iteration : 10865
train acc:  0.7421875
train loss:  0.5081995725631714
train gradient:  0.1569826395482949
iteration : 10866
train acc:  0.65625
train loss:  0.5458561182022095
train gradient:  0.16194628043697715
iteration : 10867
train acc:  0.71875
train loss:  0.48618489503860474
train gradient:  0.13114162370178245
iteration : 10868
train acc:  0.75
train loss:  0.47830480337142944
train gradient:  0.10319186140078201
iteration : 10869
train acc:  0.765625
train loss:  0.477338582277298
train gradient:  0.1257919476317089
iteration : 10870
train acc:  0.8515625
train loss:  0.41448986530303955
train gradient:  0.08765992586026133
iteration : 10871
train acc:  0.7265625
train loss:  0.5023831129074097
train gradient:  0.1413967479258182
iteration : 10872
train acc:  0.765625
train loss:  0.5358901619911194
train gradient:  0.15658357427017316
iteration : 10873
train acc:  0.703125
train loss:  0.5230761766433716
train gradient:  0.15634856263724692
iteration : 10874
train acc:  0.7578125
train loss:  0.5562459826469421
train gradient:  0.1679514616316043
iteration : 10875
train acc:  0.7421875
train loss:  0.4952033758163452
train gradient:  0.1348365070951275
iteration : 10876
train acc:  0.734375
train loss:  0.47998183965682983
train gradient:  0.10607333027302444
iteration : 10877
train acc:  0.7578125
train loss:  0.5055949091911316
train gradient:  0.10681767936106544
iteration : 10878
train acc:  0.65625
train loss:  0.5525189638137817
train gradient:  0.1773412083001758
iteration : 10879
train acc:  0.7421875
train loss:  0.5488768815994263
train gradient:  0.14491981434981901
iteration : 10880
train acc:  0.71875
train loss:  0.5230278968811035
train gradient:  0.145472600798617
iteration : 10881
train acc:  0.796875
train loss:  0.43905603885650635
train gradient:  0.11329105795524509
iteration : 10882
train acc:  0.7109375
train loss:  0.5186783671379089
train gradient:  0.1470354840793231
iteration : 10883
train acc:  0.734375
train loss:  0.4730418920516968
train gradient:  0.11265099551617007
iteration : 10884
train acc:  0.7578125
train loss:  0.46377938985824585
train gradient:  0.11175878990625437
iteration : 10885
train acc:  0.765625
train loss:  0.4918195605278015
train gradient:  0.12632033471405424
iteration : 10886
train acc:  0.75
train loss:  0.4888044595718384
train gradient:  0.11850489335665894
iteration : 10887
train acc:  0.7578125
train loss:  0.4944919943809509
train gradient:  0.14661211315380532
iteration : 10888
train acc:  0.7578125
train loss:  0.4713016748428345
train gradient:  0.1459778988855108
iteration : 10889
train acc:  0.7109375
train loss:  0.5452630519866943
train gradient:  0.13771997346166232
iteration : 10890
train acc:  0.7265625
train loss:  0.4922856092453003
train gradient:  0.14448514363070603
iteration : 10891
train acc:  0.7421875
train loss:  0.4714308977127075
train gradient:  0.10799269526958691
iteration : 10892
train acc:  0.75
train loss:  0.4992656409740448
train gradient:  0.1141301355337995
iteration : 10893
train acc:  0.75
train loss:  0.4620457887649536
train gradient:  0.11010873721125676
iteration : 10894
train acc:  0.734375
train loss:  0.4819084405899048
train gradient:  0.10681592812586466
iteration : 10895
train acc:  0.7890625
train loss:  0.43102285265922546
train gradient:  0.12290459799715044
iteration : 10896
train acc:  0.8203125
train loss:  0.3816966414451599
train gradient:  0.11191707242350202
iteration : 10897
train acc:  0.7734375
train loss:  0.4687134623527527
train gradient:  0.19792683841742742
iteration : 10898
train acc:  0.7265625
train loss:  0.46317118406295776
train gradient:  0.12184995156943805
iteration : 10899
train acc:  0.7734375
train loss:  0.520359992980957
train gradient:  0.12316546373317512
iteration : 10900
train acc:  0.734375
train loss:  0.4922131299972534
train gradient:  0.12340378222381501
iteration : 10901
train acc:  0.7578125
train loss:  0.48251470923423767
train gradient:  0.15784737330124013
iteration : 10902
train acc:  0.65625
train loss:  0.6247329711914062
train gradient:  0.2486162321801692
iteration : 10903
train acc:  0.7734375
train loss:  0.530248761177063
train gradient:  0.15461579062023761
iteration : 10904
train acc:  0.7734375
train loss:  0.45052242279052734
train gradient:  0.12079688241503654
iteration : 10905
train acc:  0.703125
train loss:  0.5405769348144531
train gradient:  0.14345575364711866
iteration : 10906
train acc:  0.703125
train loss:  0.5631394982337952
train gradient:  0.16404085750967712
iteration : 10907
train acc:  0.71875
train loss:  0.5638393759727478
train gradient:  0.21616833039854855
iteration : 10908
train acc:  0.7734375
train loss:  0.4577171802520752
train gradient:  0.09735815805335986
iteration : 10909
train acc:  0.765625
train loss:  0.4972039759159088
train gradient:  0.15911755390569238
iteration : 10910
train acc:  0.765625
train loss:  0.49983125925064087
train gradient:  0.14174422850504292
iteration : 10911
train acc:  0.75
train loss:  0.5216814279556274
train gradient:  0.13447216761408784
iteration : 10912
train acc:  0.7734375
train loss:  0.4692176580429077
train gradient:  0.13313416864109814
iteration : 10913
train acc:  0.78125
train loss:  0.476105272769928
train gradient:  0.11023132528919825
iteration : 10914
train acc:  0.78125
train loss:  0.4476390480995178
train gradient:  0.12498075673950362
iteration : 10915
train acc:  0.7109375
train loss:  0.5831664800643921
train gradient:  0.15558151779480212
iteration : 10916
train acc:  0.78125
train loss:  0.45710182189941406
train gradient:  0.11728422301199767
iteration : 10917
train acc:  0.75
train loss:  0.5013800859451294
train gradient:  0.11938628249335276
iteration : 10918
train acc:  0.7109375
train loss:  0.49102258682250977
train gradient:  0.14297735216640745
iteration : 10919
train acc:  0.7421875
train loss:  0.49396634101867676
train gradient:  0.11518543255503759
iteration : 10920
train acc:  0.7109375
train loss:  0.5057269930839539
train gradient:  0.1258564057092455
iteration : 10921
train acc:  0.7265625
train loss:  0.4937222898006439
train gradient:  0.10905661107222434
iteration : 10922
train acc:  0.640625
train loss:  0.6022509336471558
train gradient:  0.22339115650691627
iteration : 10923
train acc:  0.796875
train loss:  0.44800692796707153
train gradient:  0.14390259411505288
iteration : 10924
train acc:  0.7421875
train loss:  0.5381999015808105
train gradient:  0.16060218454897263
iteration : 10925
train acc:  0.8046875
train loss:  0.4475019574165344
train gradient:  0.11673096285924377
iteration : 10926
train acc:  0.75
train loss:  0.46574094891548157
train gradient:  0.09759416945268377
iteration : 10927
train acc:  0.734375
train loss:  0.508650541305542
train gradient:  0.1579866636730845
iteration : 10928
train acc:  0.765625
train loss:  0.4815542995929718
train gradient:  0.13353553744008628
iteration : 10929
train acc:  0.71875
train loss:  0.5692627429962158
train gradient:  0.14903730684279082
iteration : 10930
train acc:  0.7890625
train loss:  0.4068557620048523
train gradient:  0.09090873920334933
iteration : 10931
train acc:  0.796875
train loss:  0.45310884714126587
train gradient:  0.10313427758763558
iteration : 10932
train acc:  0.765625
train loss:  0.42071956396102905
train gradient:  0.10376429365233228
iteration : 10933
train acc:  0.6796875
train loss:  0.5174764394760132
train gradient:  0.13573154428015377
iteration : 10934
train acc:  0.75
train loss:  0.4539072513580322
train gradient:  0.101146352544117
iteration : 10935
train acc:  0.7578125
train loss:  0.48188790678977966
train gradient:  0.12973118547430434
iteration : 10936
train acc:  0.7734375
train loss:  0.47018879652023315
train gradient:  0.13497506667340964
iteration : 10937
train acc:  0.7578125
train loss:  0.4595128297805786
train gradient:  0.12347659835446596
iteration : 10938
train acc:  0.75
train loss:  0.5417757034301758
train gradient:  0.17043282483807376
iteration : 10939
train acc:  0.7109375
train loss:  0.4922000765800476
train gradient:  0.1349078773160684
iteration : 10940
train acc:  0.6875
train loss:  0.5202116370201111
train gradient:  0.17222320551477918
iteration : 10941
train acc:  0.7890625
train loss:  0.43918663263320923
train gradient:  0.09749632834774287
iteration : 10942
train acc:  0.8203125
train loss:  0.39476609230041504
train gradient:  0.09647225705531287
iteration : 10943
train acc:  0.703125
train loss:  0.5546533465385437
train gradient:  0.16536000348352992
iteration : 10944
train acc:  0.7265625
train loss:  0.47839492559432983
train gradient:  0.13163913267259053
iteration : 10945
train acc:  0.75
train loss:  0.49768030643463135
train gradient:  0.1292465357012214
iteration : 10946
train acc:  0.703125
train loss:  0.543138861656189
train gradient:  0.14981449222336507
iteration : 10947
train acc:  0.7109375
train loss:  0.5802310705184937
train gradient:  0.16349884785698313
iteration : 10948
train acc:  0.6953125
train loss:  0.5314274430274963
train gradient:  0.15956582446742823
iteration : 10949
train acc:  0.734375
train loss:  0.5004221200942993
train gradient:  0.11967323461216965
iteration : 10950
train acc:  0.734375
train loss:  0.5424669981002808
train gradient:  0.20381289657117738
iteration : 10951
train acc:  0.6953125
train loss:  0.5430828332901001
train gradient:  0.19258069141146267
iteration : 10952
train acc:  0.6953125
train loss:  0.5676844120025635
train gradient:  0.16946419838535293
iteration : 10953
train acc:  0.7421875
train loss:  0.4479893445968628
train gradient:  0.11880620294704662
iteration : 10954
train acc:  0.8203125
train loss:  0.42606690526008606
train gradient:  0.08466510354687663
iteration : 10955
train acc:  0.7578125
train loss:  0.48852449655532837
train gradient:  0.1268314259886582
iteration : 10956
train acc:  0.734375
train loss:  0.46783682703971863
train gradient:  0.1181365204521999
iteration : 10957
train acc:  0.75
train loss:  0.4883733093738556
train gradient:  0.13176748639656166
iteration : 10958
train acc:  0.7421875
train loss:  0.5227459669113159
train gradient:  0.15046668689182013
iteration : 10959
train acc:  0.7578125
train loss:  0.4482142925262451
train gradient:  0.09954443962494888
iteration : 10960
train acc:  0.6953125
train loss:  0.47552478313446045
train gradient:  0.11978149402872013
iteration : 10961
train acc:  0.7421875
train loss:  0.47116971015930176
train gradient:  0.10468007186028641
iteration : 10962
train acc:  0.75
train loss:  0.48769569396972656
train gradient:  0.12974337841076683
iteration : 10963
train acc:  0.7421875
train loss:  0.5338590145111084
train gradient:  0.1718004745188717
iteration : 10964
train acc:  0.765625
train loss:  0.4285097122192383
train gradient:  0.11336315059035532
iteration : 10965
train acc:  0.78125
train loss:  0.49469155073165894
train gradient:  0.11506808963303856
iteration : 10966
train acc:  0.6875
train loss:  0.5480045080184937
train gradient:  0.16235450798858525
iteration : 10967
train acc:  0.7109375
train loss:  0.498254656791687
train gradient:  0.12926664905666216
iteration : 10968
train acc:  0.6953125
train loss:  0.5274440050125122
train gradient:  0.12450916835006877
iteration : 10969
train acc:  0.7421875
train loss:  0.4830157458782196
train gradient:  0.13207391029903626
iteration : 10970
train acc:  0.734375
train loss:  0.5313938856124878
train gradient:  0.15372871635277058
iteration : 10971
train acc:  0.671875
train loss:  0.505842924118042
train gradient:  0.11115899356530236
iteration : 10972
train acc:  0.734375
train loss:  0.47938084602355957
train gradient:  0.114922005108883
iteration : 10973
train acc:  0.7734375
train loss:  0.4493717849254608
train gradient:  0.12500858712279556
iteration : 10974
train acc:  0.7890625
train loss:  0.4794173836708069
train gradient:  0.1154024881394917
iteration : 10975
train acc:  0.8125
train loss:  0.4180055856704712
train gradient:  0.09458913506658535
iteration : 10976
train acc:  0.7578125
train loss:  0.48195356130599976
train gradient:  0.1307019724936171
iteration : 10977
train acc:  0.6875
train loss:  0.5092522501945496
train gradient:  0.1332552888215695
iteration : 10978
train acc:  0.7421875
train loss:  0.47456780076026917
train gradient:  0.1011488127613402
iteration : 10979
train acc:  0.7890625
train loss:  0.43974485993385315
train gradient:  0.1333019672025354
iteration : 10980
train acc:  0.765625
train loss:  0.4343253970146179
train gradient:  0.08995017942233428
iteration : 10981
train acc:  0.75
train loss:  0.49496570229530334
train gradient:  0.13676068435749375
iteration : 10982
train acc:  0.8125
train loss:  0.4320436120033264
train gradient:  0.0991661201926318
iteration : 10983
train acc:  0.75
train loss:  0.5204786062240601
train gradient:  0.17509125552033494
iteration : 10984
train acc:  0.734375
train loss:  0.4829842448234558
train gradient:  0.13137834208972293
iteration : 10985
train acc:  0.6796875
train loss:  0.5500221252441406
train gradient:  0.17238948514025315
iteration : 10986
train acc:  0.7578125
train loss:  0.45646947622299194
train gradient:  0.1074107882434305
iteration : 10987
train acc:  0.8203125
train loss:  0.4392445385456085
train gradient:  0.10782975306991809
iteration : 10988
train acc:  0.765625
train loss:  0.42312562465667725
train gradient:  0.09001655364328368
iteration : 10989
train acc:  0.78125
train loss:  0.4370647966861725
train gradient:  0.11187326272254942
iteration : 10990
train acc:  0.75
train loss:  0.5116294622421265
train gradient:  0.13189492285309512
iteration : 10991
train acc:  0.6796875
train loss:  0.5469520092010498
train gradient:  0.11847070185155441
iteration : 10992
train acc:  0.765625
train loss:  0.4626377820968628
train gradient:  0.11254764545374009
iteration : 10993
train acc:  0.765625
train loss:  0.4815150499343872
train gradient:  0.11577256484866204
iteration : 10994
train acc:  0.7734375
train loss:  0.47045427560806274
train gradient:  0.12359700100986284
iteration : 10995
train acc:  0.6953125
train loss:  0.5275465846061707
train gradient:  0.13366946235371596
iteration : 10996
train acc:  0.734375
train loss:  0.6359225511550903
train gradient:  0.18324334794772013
iteration : 10997
train acc:  0.6953125
train loss:  0.5631316304206848
train gradient:  0.14926073877781554
iteration : 10998
train acc:  0.8203125
train loss:  0.3962034285068512
train gradient:  0.087247067205536
iteration : 10999
train acc:  0.796875
train loss:  0.4450475871562958
train gradient:  0.11992233869631404
iteration : 11000
train acc:  0.7265625
train loss:  0.4912247061729431
train gradient:  0.153925983468118
iteration : 11001
train acc:  0.71875
train loss:  0.5581521987915039
train gradient:  0.1611789039516256
iteration : 11002
train acc:  0.8359375
train loss:  0.40874990820884705
train gradient:  0.10242887557434739
iteration : 11003
train acc:  0.75
train loss:  0.4754171371459961
train gradient:  0.1239793722171575
iteration : 11004
train acc:  0.7265625
train loss:  0.4870237708091736
train gradient:  0.13269616083277835
iteration : 11005
train acc:  0.703125
train loss:  0.5543230772018433
train gradient:  0.15871011066652274
iteration : 11006
train acc:  0.71875
train loss:  0.5713091492652893
train gradient:  0.18011440816661645
iteration : 11007
train acc:  0.765625
train loss:  0.4669550061225891
train gradient:  0.13635231477569587
iteration : 11008
train acc:  0.75
train loss:  0.49574965238571167
train gradient:  0.16692062097907068
iteration : 11009
train acc:  0.7265625
train loss:  0.4680955111980438
train gradient:  0.12521898755873753
iteration : 11010
train acc:  0.6875
train loss:  0.5264921188354492
train gradient:  0.15924113133255952
iteration : 11011
train acc:  0.734375
train loss:  0.5038330554962158
train gradient:  0.13886113252825283
iteration : 11012
train acc:  0.7890625
train loss:  0.45512139797210693
train gradient:  0.12077811634217706
iteration : 11013
train acc:  0.7734375
train loss:  0.46887969970703125
train gradient:  0.11487377588509155
iteration : 11014
train acc:  0.71875
train loss:  0.5144686102867126
train gradient:  0.14380520628498736
iteration : 11015
train acc:  0.71875
train loss:  0.4959796369075775
train gradient:  0.14065181933136495
iteration : 11016
train acc:  0.734375
train loss:  0.48991265892982483
train gradient:  0.12491758214883042
iteration : 11017
train acc:  0.703125
train loss:  0.4999844431877136
train gradient:  0.14846779447527916
iteration : 11018
train acc:  0.78125
train loss:  0.4508289098739624
train gradient:  0.13000463534618523
iteration : 11019
train acc:  0.7265625
train loss:  0.5077508687973022
train gradient:  0.14034449649992542
iteration : 11020
train acc:  0.7578125
train loss:  0.518157422542572
train gradient:  0.1310105968345795
iteration : 11021
train acc:  0.78125
train loss:  0.48946547508239746
train gradient:  0.11130614427504897
iteration : 11022
train acc:  0.7734375
train loss:  0.44094085693359375
train gradient:  0.12742799867418048
iteration : 11023
train acc:  0.75
train loss:  0.48704785108566284
train gradient:  0.15836373730614317
iteration : 11024
train acc:  0.7109375
train loss:  0.5191740393638611
train gradient:  0.12186847309732121
iteration : 11025
train acc:  0.734375
train loss:  0.4428848326206207
train gradient:  0.09166304358826222
iteration : 11026
train acc:  0.71875
train loss:  0.4790479838848114
train gradient:  0.1297942991635242
iteration : 11027
train acc:  0.78125
train loss:  0.4889955520629883
train gradient:  0.13303067584725758
iteration : 11028
train acc:  0.703125
train loss:  0.5448315739631653
train gradient:  0.13666408166282498
iteration : 11029
train acc:  0.6953125
train loss:  0.5729923248291016
train gradient:  0.16249520454039823
iteration : 11030
train acc:  0.7109375
train loss:  0.4998374581336975
train gradient:  0.14181198377141563
iteration : 11031
train acc:  0.6953125
train loss:  0.49326273798942566
train gradient:  0.10931656471999988
iteration : 11032
train acc:  0.8203125
train loss:  0.43339547514915466
train gradient:  0.11014016409085665
iteration : 11033
train acc:  0.734375
train loss:  0.504755973815918
train gradient:  0.1390261908881218
iteration : 11034
train acc:  0.796875
train loss:  0.4683253765106201
train gradient:  0.11559511345751115
iteration : 11035
train acc:  0.7578125
train loss:  0.5484412908554077
train gradient:  0.1275170822182298
iteration : 11036
train acc:  0.75
train loss:  0.46220913529396057
train gradient:  0.11635148440570604
iteration : 11037
train acc:  0.78125
train loss:  0.4573083519935608
train gradient:  0.08720513875637531
iteration : 11038
train acc:  0.7265625
train loss:  0.5079621076583862
train gradient:  0.12277447832587257
iteration : 11039
train acc:  0.6484375
train loss:  0.5496037006378174
train gradient:  0.14636526593518967
iteration : 11040
train acc:  0.7734375
train loss:  0.46355578303337097
train gradient:  0.13830092019941131
iteration : 11041
train acc:  0.7421875
train loss:  0.4754336178302765
train gradient:  0.1358208961677429
iteration : 11042
train acc:  0.7265625
train loss:  0.5333319902420044
train gradient:  0.13241205608553608
iteration : 11043
train acc:  0.703125
train loss:  0.48755043745040894
train gradient:  0.12297283762175487
iteration : 11044
train acc:  0.6953125
train loss:  0.5216745138168335
train gradient:  0.18143327188164518
iteration : 11045
train acc:  0.7578125
train loss:  0.5225746631622314
train gradient:  0.19183692349221496
iteration : 11046
train acc:  0.6640625
train loss:  0.5286482572555542
train gradient:  0.1817510704222548
iteration : 11047
train acc:  0.765625
train loss:  0.43898531794548035
train gradient:  0.09993423683732326
iteration : 11048
train acc:  0.7265625
train loss:  0.47466766834259033
train gradient:  0.11917049223604267
iteration : 11049
train acc:  0.6640625
train loss:  0.5369240045547485
train gradient:  0.15439356901483353
iteration : 11050
train acc:  0.734375
train loss:  0.5167771577835083
train gradient:  0.14529237384830207
iteration : 11051
train acc:  0.765625
train loss:  0.48981019854545593
train gradient:  0.1428376987564962
iteration : 11052
train acc:  0.7734375
train loss:  0.5048815011978149
train gradient:  0.11328955443089964
iteration : 11053
train acc:  0.7578125
train loss:  0.5244282484054565
train gradient:  0.12167568248418205
iteration : 11054
train acc:  0.6640625
train loss:  0.5642030835151672
train gradient:  0.16073841472917905
iteration : 11055
train acc:  0.75
train loss:  0.44507747888565063
train gradient:  0.09716795719347102
iteration : 11056
train acc:  0.8046875
train loss:  0.43119657039642334
train gradient:  0.11160769048169043
iteration : 11057
train acc:  0.7578125
train loss:  0.5221171975135803
train gradient:  0.14745070299772933
iteration : 11058
train acc:  0.6875
train loss:  0.4840266704559326
train gradient:  0.12302907263220948
iteration : 11059
train acc:  0.703125
train loss:  0.5352904796600342
train gradient:  0.13583066373666958
iteration : 11060
train acc:  0.7890625
train loss:  0.4888020157814026
train gradient:  0.15422091578964556
iteration : 11061
train acc:  0.7421875
train loss:  0.5193339586257935
train gradient:  0.1356694733157863
iteration : 11062
train acc:  0.7734375
train loss:  0.4928620457649231
train gradient:  0.14809267868529935
iteration : 11063
train acc:  0.71875
train loss:  0.538169801235199
train gradient:  0.1526888558374411
iteration : 11064
train acc:  0.7890625
train loss:  0.4484151601791382
train gradient:  0.11058380809498196
iteration : 11065
train acc:  0.765625
train loss:  0.47519421577453613
train gradient:  0.13277892380754874
iteration : 11066
train acc:  0.71875
train loss:  0.4922049641609192
train gradient:  0.1512012767788693
iteration : 11067
train acc:  0.8125
train loss:  0.4111773669719696
train gradient:  0.09982219706769682
iteration : 11068
train acc:  0.6953125
train loss:  0.5538208484649658
train gradient:  0.13441116134519848
iteration : 11069
train acc:  0.7734375
train loss:  0.4397602677345276
train gradient:  0.07863475316739466
iteration : 11070
train acc:  0.703125
train loss:  0.49208003282546997
train gradient:  0.13717789187192084
iteration : 11071
train acc:  0.71875
train loss:  0.5027684569358826
train gradient:  0.13487913440789084
iteration : 11072
train acc:  0.75
train loss:  0.4663908779621124
train gradient:  0.09458387873937596
iteration : 11073
train acc:  0.7734375
train loss:  0.4690210223197937
train gradient:  0.10255428698296093
iteration : 11074
train acc:  0.796875
train loss:  0.452659010887146
train gradient:  0.12095772088941756
iteration : 11075
train acc:  0.703125
train loss:  0.5306921005249023
train gradient:  0.20570530123275874
iteration : 11076
train acc:  0.6328125
train loss:  0.6522318124771118
train gradient:  0.18965754846903038
iteration : 11077
train acc:  0.7265625
train loss:  0.48856121301651
train gradient:  0.11456522277470717
iteration : 11078
train acc:  0.8046875
train loss:  0.4642556607723236
train gradient:  0.11300034270133627
iteration : 11079
train acc:  0.703125
train loss:  0.5014407634735107
train gradient:  0.13114170150343213
iteration : 11080
train acc:  0.765625
train loss:  0.4649716913700104
train gradient:  0.11960123332983622
iteration : 11081
train acc:  0.7109375
train loss:  0.5604368448257446
train gradient:  0.18728344207136943
iteration : 11082
train acc:  0.6875
train loss:  0.5018892288208008
train gradient:  0.1475334751844527
iteration : 11083
train acc:  0.71875
train loss:  0.5222240686416626
train gradient:  0.13543103187607372
iteration : 11084
train acc:  0.7421875
train loss:  0.4616779685020447
train gradient:  0.1219992375190955
iteration : 11085
train acc:  0.7578125
train loss:  0.4803074896335602
train gradient:  0.113323489250173
iteration : 11086
train acc:  0.703125
train loss:  0.5107800960540771
train gradient:  0.1229027674972265
iteration : 11087
train acc:  0.703125
train loss:  0.5067312717437744
train gradient:  0.11659856533569854
iteration : 11088
train acc:  0.7265625
train loss:  0.5301353931427002
train gradient:  0.12837124943031092
iteration : 11089
train acc:  0.765625
train loss:  0.4649670124053955
train gradient:  0.13444192988563994
iteration : 11090
train acc:  0.7421875
train loss:  0.4503730237483978
train gradient:  0.084089268925627
iteration : 11091
train acc:  0.6953125
train loss:  0.5385514497756958
train gradient:  0.1728564277231215
iteration : 11092
train acc:  0.7734375
train loss:  0.47694629430770874
train gradient:  0.18263208261303415
iteration : 11093
train acc:  0.6640625
train loss:  0.513343870639801
train gradient:  0.1137691430629112
iteration : 11094
train acc:  0.734375
train loss:  0.484024316072464
train gradient:  0.11868568089095329
iteration : 11095
train acc:  0.7734375
train loss:  0.47249048948287964
train gradient:  0.11519243207584097
iteration : 11096
train acc:  0.7578125
train loss:  0.5120515823364258
train gradient:  0.18587282631923058
iteration : 11097
train acc:  0.734375
train loss:  0.5086426734924316
train gradient:  0.1537775032615979
iteration : 11098
train acc:  0.6953125
train loss:  0.5242798924446106
train gradient:  0.14540576940407068
iteration : 11099
train acc:  0.7421875
train loss:  0.5026403665542603
train gradient:  0.1344315569759137
iteration : 11100
train acc:  0.7421875
train loss:  0.45501717925071716
train gradient:  0.08990002112869973
iteration : 11101
train acc:  0.6875
train loss:  0.4903472661972046
train gradient:  0.11878677781645347
iteration : 11102
train acc:  0.7578125
train loss:  0.464243084192276
train gradient:  0.12577705458129318
iteration : 11103
train acc:  0.7265625
train loss:  0.5348688364028931
train gradient:  0.12586932755974045
iteration : 11104
train acc:  0.6953125
train loss:  0.545363187789917
train gradient:  0.19902978642405417
iteration : 11105
train acc:  0.78125
train loss:  0.44772976636886597
train gradient:  0.1274140653795109
iteration : 11106
train acc:  0.765625
train loss:  0.4897347092628479
train gradient:  0.12497283275244185
iteration : 11107
train acc:  0.6875
train loss:  0.531879723072052
train gradient:  0.1655931544628163
iteration : 11108
train acc:  0.7109375
train loss:  0.49801260232925415
train gradient:  0.14018409857173125
iteration : 11109
train acc:  0.78125
train loss:  0.49676966667175293
train gradient:  0.13485761454177708
iteration : 11110
train acc:  0.734375
train loss:  0.5420233011245728
train gradient:  0.166989026174029
iteration : 11111
train acc:  0.7109375
train loss:  0.5039360523223877
train gradient:  0.11898949125408928
iteration : 11112
train acc:  0.7421875
train loss:  0.5036038160324097
train gradient:  0.15646852260985739
iteration : 11113
train acc:  0.6953125
train loss:  0.5326383113861084
train gradient:  0.1276015895889008
iteration : 11114
train acc:  0.859375
train loss:  0.4430280029773712
train gradient:  0.13388186437875677
iteration : 11115
train acc:  0.8203125
train loss:  0.4110257029533386
train gradient:  0.07921261557342894
iteration : 11116
train acc:  0.796875
train loss:  0.4035664200782776
train gradient:  0.08711664428256069
iteration : 11117
train acc:  0.6953125
train loss:  0.5900633335113525
train gradient:  0.17597303469997028
iteration : 11118
train acc:  0.734375
train loss:  0.4980834126472473
train gradient:  0.1350444782676113
iteration : 11119
train acc:  0.734375
train loss:  0.5198403596878052
train gradient:  0.16978062587574588
iteration : 11120
train acc:  0.7109375
train loss:  0.5186223983764648
train gradient:  0.13585448574750164
iteration : 11121
train acc:  0.7421875
train loss:  0.5155544281005859
train gradient:  0.15984594950241163
iteration : 11122
train acc:  0.6640625
train loss:  0.5788903832435608
train gradient:  0.18462454938468917
iteration : 11123
train acc:  0.7578125
train loss:  0.46003401279449463
train gradient:  0.09906349004743356
iteration : 11124
train acc:  0.796875
train loss:  0.4814545512199402
train gradient:  0.1479760642518066
iteration : 11125
train acc:  0.734375
train loss:  0.48563069105148315
train gradient:  0.1261559652742038
iteration : 11126
train acc:  0.765625
train loss:  0.5066559314727783
train gradient:  0.12708948038952725
iteration : 11127
train acc:  0.7421875
train loss:  0.4885483384132385
train gradient:  0.12574661499412507
iteration : 11128
train acc:  0.7578125
train loss:  0.4868847131729126
train gradient:  0.11592485264471493
iteration : 11129
train acc:  0.7109375
train loss:  0.5416518449783325
train gradient:  0.1691470271774104
iteration : 11130
train acc:  0.6953125
train loss:  0.5670669078826904
train gradient:  0.14702269322281858
iteration : 11131
train acc:  0.6875
train loss:  0.5343831777572632
train gradient:  0.11802247758135657
iteration : 11132
train acc:  0.671875
train loss:  0.5312789678573608
train gradient:  0.14272753905952484
iteration : 11133
train acc:  0.7578125
train loss:  0.4897799491882324
train gradient:  0.12650565015952642
iteration : 11134
train acc:  0.7734375
train loss:  0.46143412590026855
train gradient:  0.0977219835232551
iteration : 11135
train acc:  0.7890625
train loss:  0.48300936818122864
train gradient:  0.1248030078711369
iteration : 11136
train acc:  0.75
train loss:  0.5022496581077576
train gradient:  0.1513537706855337
iteration : 11137
train acc:  0.7578125
train loss:  0.5468666553497314
train gradient:  0.1448933273373646
iteration : 11138
train acc:  0.765625
train loss:  0.4165520966053009
train gradient:  0.10770622704667274
iteration : 11139
train acc:  0.7265625
train loss:  0.4798980951309204
train gradient:  0.11472249249265315
iteration : 11140
train acc:  0.765625
train loss:  0.47424590587615967
train gradient:  0.10559714639036084
iteration : 11141
train acc:  0.7109375
train loss:  0.5182003974914551
train gradient:  0.15357669277111213
iteration : 11142
train acc:  0.78125
train loss:  0.4789120554924011
train gradient:  0.12393741452147655
iteration : 11143
train acc:  0.734375
train loss:  0.5548648834228516
train gradient:  0.1397744764219886
iteration : 11144
train acc:  0.7421875
train loss:  0.5028521418571472
train gradient:  0.1343747610945793
iteration : 11145
train acc:  0.703125
train loss:  0.5177578926086426
train gradient:  0.11125508901467954
iteration : 11146
train acc:  0.71875
train loss:  0.5312817692756653
train gradient:  0.16955075134257108
iteration : 11147
train acc:  0.75
train loss:  0.47349339723587036
train gradient:  0.13462960844543237
iteration : 11148
train acc:  0.7734375
train loss:  0.4468959867954254
train gradient:  0.10460965955633979
iteration : 11149
train acc:  0.7265625
train loss:  0.49708229303359985
train gradient:  0.10570072024985577
iteration : 11150
train acc:  0.7578125
train loss:  0.4703456163406372
train gradient:  0.13927178986079017
iteration : 11151
train acc:  0.828125
train loss:  0.4197823405265808
train gradient:  0.1134037228337279
iteration : 11152
train acc:  0.734375
train loss:  0.5230208039283752
train gradient:  0.16750747802048857
iteration : 11153
train acc:  0.734375
train loss:  0.4870222806930542
train gradient:  0.12611310847350882
iteration : 11154
train acc:  0.7421875
train loss:  0.4830811619758606
train gradient:  0.1174732670661579
iteration : 11155
train acc:  0.6953125
train loss:  0.511212170124054
train gradient:  0.1458826558172019
iteration : 11156
train acc:  0.8046875
train loss:  0.44948258996009827
train gradient:  0.14499487023319835
iteration : 11157
train acc:  0.75
train loss:  0.4849802255630493
train gradient:  0.1191980680646396
iteration : 11158
train acc:  0.765625
train loss:  0.4786001145839691
train gradient:  0.13078011157736177
iteration : 11159
train acc:  0.734375
train loss:  0.46170079708099365
train gradient:  0.11782959461814957
iteration : 11160
train acc:  0.734375
train loss:  0.5637633800506592
train gradient:  0.1680528305672933
iteration : 11161
train acc:  0.7734375
train loss:  0.5313328504562378
train gradient:  0.1297551574835525
iteration : 11162
train acc:  0.796875
train loss:  0.42510131001472473
train gradient:  0.09344406237136835
iteration : 11163
train acc:  0.7265625
train loss:  0.45901018381118774
train gradient:  0.11691075732542877
iteration : 11164
train acc:  0.7265625
train loss:  0.48833346366882324
train gradient:  0.12155925807643209
iteration : 11165
train acc:  0.75
train loss:  0.5024268627166748
train gradient:  0.14023199989965884
iteration : 11166
train acc:  0.671875
train loss:  0.5468504428863525
train gradient:  0.17897367084664173
iteration : 11167
train acc:  0.75
train loss:  0.4943966865539551
train gradient:  0.16337440759718236
iteration : 11168
train acc:  0.75
train loss:  0.5052464008331299
train gradient:  0.16847065886586576
iteration : 11169
train acc:  0.75
train loss:  0.48769232630729675
train gradient:  0.1112519184812422
iteration : 11170
train acc:  0.7421875
train loss:  0.5121161937713623
train gradient:  0.13875593593074526
iteration : 11171
train acc:  0.671875
train loss:  0.5657986402511597
train gradient:  0.14151251694240344
iteration : 11172
train acc:  0.71875
train loss:  0.482967734336853
train gradient:  0.12264981886503039
iteration : 11173
train acc:  0.84375
train loss:  0.3693213164806366
train gradient:  0.0756179451108283
iteration : 11174
train acc:  0.7578125
train loss:  0.44510355591773987
train gradient:  0.1159970764061274
iteration : 11175
train acc:  0.734375
train loss:  0.507140040397644
train gradient:  0.1335165876023206
iteration : 11176
train acc:  0.7421875
train loss:  0.47100329399108887
train gradient:  0.12847583246800537
iteration : 11177
train acc:  0.7734375
train loss:  0.4207640290260315
train gradient:  0.11712288351492066
iteration : 11178
train acc:  0.796875
train loss:  0.479635089635849
train gradient:  0.12067051920694931
iteration : 11179
train acc:  0.7734375
train loss:  0.46276116371154785
train gradient:  0.1399067553999523
iteration : 11180
train acc:  0.7109375
train loss:  0.554709255695343
train gradient:  0.16054270560310402
iteration : 11181
train acc:  0.7265625
train loss:  0.4707834720611572
train gradient:  0.1359438153533753
iteration : 11182
train acc:  0.6796875
train loss:  0.5977580547332764
train gradient:  0.1733337607329295
iteration : 11183
train acc:  0.6953125
train loss:  0.5678499937057495
train gradient:  0.15803816866189963
iteration : 11184
train acc:  0.7421875
train loss:  0.5169211626052856
train gradient:  0.17173292653364797
iteration : 11185
train acc:  0.75
train loss:  0.5120120048522949
train gradient:  0.15249976764315498
iteration : 11186
train acc:  0.8125
train loss:  0.4264785647392273
train gradient:  0.09912522037569722
iteration : 11187
train acc:  0.765625
train loss:  0.5009051561355591
train gradient:  0.1584158600309259
iteration : 11188
train acc:  0.78125
train loss:  0.4304056763648987
train gradient:  0.0908452111436592
iteration : 11189
train acc:  0.7578125
train loss:  0.5000351667404175
train gradient:  0.14636015074720798
iteration : 11190
train acc:  0.7578125
train loss:  0.4339296221733093
train gradient:  0.09112292434360601
iteration : 11191
train acc:  0.8125
train loss:  0.3894602060317993
train gradient:  0.11712029400644688
iteration : 11192
train acc:  0.71875
train loss:  0.542067289352417
train gradient:  0.1409441732337539
iteration : 11193
train acc:  0.7734375
train loss:  0.4607308506965637
train gradient:  0.10480341124918033
iteration : 11194
train acc:  0.71875
train loss:  0.48229196667671204
train gradient:  0.14098836471935938
iteration : 11195
train acc:  0.765625
train loss:  0.520220160484314
train gradient:  0.13460660406960162
iteration : 11196
train acc:  0.6640625
train loss:  0.5847548246383667
train gradient:  0.18440884227201063
iteration : 11197
train acc:  0.796875
train loss:  0.44906163215637207
train gradient:  0.09176509634999512
iteration : 11198
train acc:  0.7421875
train loss:  0.4919239282608032
train gradient:  0.10595528617975244
iteration : 11199
train acc:  0.734375
train loss:  0.5508345365524292
train gradient:  0.14269194517306605
iteration : 11200
train acc:  0.7109375
train loss:  0.4755288362503052
train gradient:  0.11118012188934462
iteration : 11201
train acc:  0.7578125
train loss:  0.5158258676528931
train gradient:  0.12717744670368586
iteration : 11202
train acc:  0.7421875
train loss:  0.47298264503479004
train gradient:  0.13811277205109665
iteration : 11203
train acc:  0.765625
train loss:  0.4558139443397522
train gradient:  0.10866915839565236
iteration : 11204
train acc:  0.703125
train loss:  0.5337356328964233
train gradient:  0.18963116231813448
iteration : 11205
train acc:  0.7890625
train loss:  0.4597277045249939
train gradient:  0.10482703792221935
iteration : 11206
train acc:  0.7578125
train loss:  0.4995177686214447
train gradient:  0.11118604173344916
iteration : 11207
train acc:  0.75
train loss:  0.4746689796447754
train gradient:  0.11607790402782592
iteration : 11208
train acc:  0.7109375
train loss:  0.5140460729598999
train gradient:  0.1420362574036479
iteration : 11209
train acc:  0.8125
train loss:  0.40804043412208557
train gradient:  0.11525840707523387
iteration : 11210
train acc:  0.7109375
train loss:  0.5668830871582031
train gradient:  0.16941028110727374
iteration : 11211
train acc:  0.8359375
train loss:  0.42184174060821533
train gradient:  0.11441761656594754
iteration : 11212
train acc:  0.7265625
train loss:  0.5479645729064941
train gradient:  0.13399745707188288
iteration : 11213
train acc:  0.75
train loss:  0.5056880116462708
train gradient:  0.1219490898102169
iteration : 11214
train acc:  0.7421875
train loss:  0.4825989902019501
train gradient:  0.11416857131296823
iteration : 11215
train acc:  0.7109375
train loss:  0.5363782644271851
train gradient:  0.1457917441075507
iteration : 11216
train acc:  0.6953125
train loss:  0.5610688924789429
train gradient:  0.1471326469422658
iteration : 11217
train acc:  0.7421875
train loss:  0.4770180583000183
train gradient:  0.1400718070461506
iteration : 11218
train acc:  0.7421875
train loss:  0.46341627836227417
train gradient:  0.12310092107594871
iteration : 11219
train acc:  0.7734375
train loss:  0.5175999402999878
train gradient:  0.15363114293305152
iteration : 11220
train acc:  0.7578125
train loss:  0.4989427328109741
train gradient:  0.10715558462727291
iteration : 11221
train acc:  0.7734375
train loss:  0.4145357012748718
train gradient:  0.0989283101910715
iteration : 11222
train acc:  0.703125
train loss:  0.5474834442138672
train gradient:  0.14294272708830458
iteration : 11223
train acc:  0.7890625
train loss:  0.45828357338905334
train gradient:  0.11409767111910007
iteration : 11224
train acc:  0.765625
train loss:  0.5217561721801758
train gradient:  0.11373147325084686
iteration : 11225
train acc:  0.7265625
train loss:  0.4960828125476837
train gradient:  0.16140254482931476
iteration : 11226
train acc:  0.71875
train loss:  0.49515214562416077
train gradient:  0.11226958459910001
iteration : 11227
train acc:  0.796875
train loss:  0.42856213450431824
train gradient:  0.10838636143056521
iteration : 11228
train acc:  0.7109375
train loss:  0.5041556358337402
train gradient:  0.12340487417557924
iteration : 11229
train acc:  0.7734375
train loss:  0.5078606605529785
train gradient:  0.11884351462322969
iteration : 11230
train acc:  0.734375
train loss:  0.5103852152824402
train gradient:  0.11977337313778158
iteration : 11231
train acc:  0.7265625
train loss:  0.4897916316986084
train gradient:  0.11930655760202664
iteration : 11232
train acc:  0.7109375
train loss:  0.4798395037651062
train gradient:  0.139052108171691
iteration : 11233
train acc:  0.7734375
train loss:  0.4181521534919739
train gradient:  0.10043262955355357
iteration : 11234
train acc:  0.8046875
train loss:  0.4316638112068176
train gradient:  0.09387914693804714
iteration : 11235
train acc:  0.734375
train loss:  0.4870772957801819
train gradient:  0.14041983184685092
iteration : 11236
train acc:  0.7109375
train loss:  0.5077781677246094
train gradient:  0.13985538255793578
iteration : 11237
train acc:  0.8125
train loss:  0.4624744653701782
train gradient:  0.18871668293886967
iteration : 11238
train acc:  0.6875
train loss:  0.5474281311035156
train gradient:  0.1989126041761879
iteration : 11239
train acc:  0.7734375
train loss:  0.47006282210350037
train gradient:  0.1563495546115582
iteration : 11240
train acc:  0.7265625
train loss:  0.5401666164398193
train gradient:  0.15309726990048134
iteration : 11241
train acc:  0.7890625
train loss:  0.472687304019928
train gradient:  0.11143762028079734
iteration : 11242
train acc:  0.7578125
train loss:  0.509136438369751
train gradient:  0.1235123510719962
iteration : 11243
train acc:  0.7421875
train loss:  0.5001139044761658
train gradient:  0.15544218538907423
iteration : 11244
train acc:  0.7265625
train loss:  0.5323253870010376
train gradient:  0.15768792647075283
iteration : 11245
train acc:  0.7578125
train loss:  0.45682621002197266
train gradient:  0.09836920239241377
iteration : 11246
train acc:  0.7265625
train loss:  0.5445828437805176
train gradient:  0.14644031131765192
iteration : 11247
train acc:  0.78125
train loss:  0.47149088978767395
train gradient:  0.11085082701977307
iteration : 11248
train acc:  0.7265625
train loss:  0.4852844178676605
train gradient:  0.12973432971075563
iteration : 11249
train acc:  0.71875
train loss:  0.5281519293785095
train gradient:  0.14978635173536137
iteration : 11250
train acc:  0.71875
train loss:  0.5422765612602234
train gradient:  0.14692151754969884
iteration : 11251
train acc:  0.765625
train loss:  0.44664937257766724
train gradient:  0.11360489447625337
iteration : 11252
train acc:  0.7421875
train loss:  0.49064475297927856
train gradient:  0.12593345732456718
iteration : 11253
train acc:  0.78125
train loss:  0.44544717669487
train gradient:  0.10195591857635916
iteration : 11254
train acc:  0.75
train loss:  0.458751916885376
train gradient:  0.1039239444182535
iteration : 11255
train acc:  0.75
train loss:  0.5272972583770752
train gradient:  0.13346534216721817
iteration : 11256
train acc:  0.671875
train loss:  0.5597991943359375
train gradient:  0.15654453442894095
iteration : 11257
train acc:  0.78125
train loss:  0.4623061418533325
train gradient:  0.09651469587764473
iteration : 11258
train acc:  0.75
train loss:  0.4868384599685669
train gradient:  0.1377363303805057
iteration : 11259
train acc:  0.7890625
train loss:  0.45591628551483154
train gradient:  0.10684480239170573
iteration : 11260
train acc:  0.7421875
train loss:  0.5269302129745483
train gradient:  0.12627592244813768
iteration : 11261
train acc:  0.6953125
train loss:  0.49924078583717346
train gradient:  0.12803039201163846
iteration : 11262
train acc:  0.75
train loss:  0.4809601902961731
train gradient:  0.13287580011378558
iteration : 11263
train acc:  0.765625
train loss:  0.5018265843391418
train gradient:  0.13397044717582463
iteration : 11264
train acc:  0.7578125
train loss:  0.42083585262298584
train gradient:  0.08109812158458152
iteration : 11265
train acc:  0.765625
train loss:  0.4377927780151367
train gradient:  0.12413825472808002
iteration : 11266
train acc:  0.7265625
train loss:  0.5291671752929688
train gradient:  0.12694483557215364
iteration : 11267
train acc:  0.7265625
train loss:  0.45918866991996765
train gradient:  0.11611629183897053
iteration : 11268
train acc:  0.78125
train loss:  0.4432445764541626
train gradient:  0.1087091158716304
iteration : 11269
train acc:  0.796875
train loss:  0.4299716353416443
train gradient:  0.11927838424497028
iteration : 11270
train acc:  0.8046875
train loss:  0.42217177152633667
train gradient:  0.10726717696865418
iteration : 11271
train acc:  0.71875
train loss:  0.5483726263046265
train gradient:  0.21444646009645704
iteration : 11272
train acc:  0.8125
train loss:  0.41156673431396484
train gradient:  0.07758228117606765
iteration : 11273
train acc:  0.7109375
train loss:  0.4944547116756439
train gradient:  0.15867881457053634
iteration : 11274
train acc:  0.7734375
train loss:  0.4701666831970215
train gradient:  0.11001931465170695
iteration : 11275
train acc:  0.7734375
train loss:  0.43629395961761475
train gradient:  0.12065595312074455
iteration : 11276
train acc:  0.7734375
train loss:  0.44384145736694336
train gradient:  0.11499617223916773
iteration : 11277
train acc:  0.7734375
train loss:  0.5226920247077942
train gradient:  0.13479053175800781
iteration : 11278
train acc:  0.78125
train loss:  0.4203397035598755
train gradient:  0.0863620287765877
iteration : 11279
train acc:  0.7890625
train loss:  0.4664353132247925
train gradient:  0.16283526148566616
iteration : 11280
train acc:  0.765625
train loss:  0.4838753938674927
train gradient:  0.13153180432524453
iteration : 11281
train acc:  0.7578125
train loss:  0.5182931423187256
train gradient:  0.1736940937240678
iteration : 11282
train acc:  0.7265625
train loss:  0.48512667417526245
train gradient:  0.12242126121440977
iteration : 11283
train acc:  0.78125
train loss:  0.5083234310150146
train gradient:  0.2788208029552199
iteration : 11284
train acc:  0.7109375
train loss:  0.5131703615188599
train gradient:  0.1624474592903787
iteration : 11285
train acc:  0.71875
train loss:  0.5133872628211975
train gradient:  0.15580537267777078
iteration : 11286
train acc:  0.78125
train loss:  0.4572846293449402
train gradient:  0.13865517199258953
iteration : 11287
train acc:  0.75
train loss:  0.4766838550567627
train gradient:  0.1449465609513595
iteration : 11288
train acc:  0.765625
train loss:  0.44764000177383423
train gradient:  0.1502468757645657
iteration : 11289
train acc:  0.640625
train loss:  0.6221548318862915
train gradient:  0.17132822437455497
iteration : 11290
train acc:  0.6640625
train loss:  0.5964956879615784
train gradient:  0.16299652698042788
iteration : 11291
train acc:  0.8671875
train loss:  0.3801865875720978
train gradient:  0.098670323774399
iteration : 11292
train acc:  0.7109375
train loss:  0.5346192121505737
train gradient:  0.1354251829599355
iteration : 11293
train acc:  0.7109375
train loss:  0.5435584783554077
train gradient:  0.1653237340292965
iteration : 11294
train acc:  0.6953125
train loss:  0.5723906755447388
train gradient:  0.18613726963759483
iteration : 11295
train acc:  0.7578125
train loss:  0.48420724272727966
train gradient:  0.12055704868267568
iteration : 11296
train acc:  0.703125
train loss:  0.5186231136322021
train gradient:  0.14235026006960566
iteration : 11297
train acc:  0.8203125
train loss:  0.44305068254470825
train gradient:  0.10781426175654701
iteration : 11298
train acc:  0.7734375
train loss:  0.474750280380249
train gradient:  0.1488966699096502
iteration : 11299
train acc:  0.7109375
train loss:  0.5512833595275879
train gradient:  0.14003676369506213
iteration : 11300
train acc:  0.734375
train loss:  0.4422031342983246
train gradient:  0.10643975910286266
iteration : 11301
train acc:  0.7421875
train loss:  0.5201351642608643
train gradient:  0.1292057273033589
iteration : 11302
train acc:  0.734375
train loss:  0.5329699516296387
train gradient:  0.1413474268527723
iteration : 11303
train acc:  0.7109375
train loss:  0.5486366748809814
train gradient:  0.14764388323166777
iteration : 11304
train acc:  0.7265625
train loss:  0.5052209496498108
train gradient:  0.130190443368144
iteration : 11305
train acc:  0.7578125
train loss:  0.4925355911254883
train gradient:  0.11151068216451668
iteration : 11306
train acc:  0.71875
train loss:  0.5019147396087646
train gradient:  0.12879965816189903
iteration : 11307
train acc:  0.7421875
train loss:  0.5096065998077393
train gradient:  0.14900965672863947
iteration : 11308
train acc:  0.7890625
train loss:  0.4595714211463928
train gradient:  0.10841298012817624
iteration : 11309
train acc:  0.7109375
train loss:  0.480389267206192
train gradient:  0.10660521130763011
iteration : 11310
train acc:  0.7265625
train loss:  0.4595421850681305
train gradient:  0.11886290634751777
iteration : 11311
train acc:  0.703125
train loss:  0.5417529344558716
train gradient:  0.1998760158015796
iteration : 11312
train acc:  0.75
train loss:  0.4562450647354126
train gradient:  0.12147067037489033
iteration : 11313
train acc:  0.7265625
train loss:  0.5615339279174805
train gradient:  0.17206657157595334
iteration : 11314
train acc:  0.78125
train loss:  0.4752313792705536
train gradient:  0.13268560086955394
iteration : 11315
train acc:  0.7421875
train loss:  0.4988493025302887
train gradient:  0.16903497933334807
iteration : 11316
train acc:  0.78125
train loss:  0.43228790163993835
train gradient:  0.10229425150477162
iteration : 11317
train acc:  0.6640625
train loss:  0.5816384553909302
train gradient:  0.14203763678345319
iteration : 11318
train acc:  0.8046875
train loss:  0.43963849544525146
train gradient:  0.10822831485755333
iteration : 11319
train acc:  0.6015625
train loss:  0.6404131650924683
train gradient:  0.20751453192269648
iteration : 11320
train acc:  0.7265625
train loss:  0.5080464482307434
train gradient:  0.15562657433208094
iteration : 11321
train acc:  0.6875
train loss:  0.5369536280632019
train gradient:  0.18376563000839957
iteration : 11322
train acc:  0.78125
train loss:  0.45415958762168884
train gradient:  0.1110368459529373
iteration : 11323
train acc:  0.7265625
train loss:  0.5347945094108582
train gradient:  0.21836266218041056
iteration : 11324
train acc:  0.7734375
train loss:  0.4986742436885834
train gradient:  0.13758097942412573
iteration : 11325
train acc:  0.8125
train loss:  0.40277373790740967
train gradient:  0.09309049238263775
iteration : 11326
train acc:  0.7734375
train loss:  0.5064430832862854
train gradient:  0.12371294940464138
iteration : 11327
train acc:  0.7578125
train loss:  0.48665934801101685
train gradient:  0.11189219694361771
iteration : 11328
train acc:  0.7265625
train loss:  0.5637025833129883
train gradient:  0.1765211435741761
iteration : 11329
train acc:  0.75
train loss:  0.49091437458992004
train gradient:  0.126427886593603
iteration : 11330
train acc:  0.71875
train loss:  0.5245056748390198
train gradient:  0.13821536293703096
iteration : 11331
train acc:  0.6953125
train loss:  0.5269428491592407
train gradient:  0.12710187041552024
iteration : 11332
train acc:  0.734375
train loss:  0.5076453685760498
train gradient:  0.15795309307762162
iteration : 11333
train acc:  0.8046875
train loss:  0.4536541700363159
train gradient:  0.11465526505551916
iteration : 11334
train acc:  0.734375
train loss:  0.47840577363967896
train gradient:  0.12995480717881075
iteration : 11335
train acc:  0.734375
train loss:  0.5064460039138794
train gradient:  0.13370331589766776
iteration : 11336
train acc:  0.7578125
train loss:  0.5113293528556824
train gradient:  0.1139219099944912
iteration : 11337
train acc:  0.7734375
train loss:  0.4797239601612091
train gradient:  0.11812567454929133
iteration : 11338
train acc:  0.8125
train loss:  0.420747309923172
train gradient:  0.10146166087434864
iteration : 11339
train acc:  0.6953125
train loss:  0.4986143708229065
train gradient:  0.11832348107899372
iteration : 11340
train acc:  0.734375
train loss:  0.44263067841529846
train gradient:  0.10146625667348233
iteration : 11341
train acc:  0.6953125
train loss:  0.5608037114143372
train gradient:  0.18333292950161617
iteration : 11342
train acc:  0.75
train loss:  0.5126503705978394
train gradient:  0.14673681814935102
iteration : 11343
train acc:  0.75
train loss:  0.5135921239852905
train gradient:  0.13961753396874443
iteration : 11344
train acc:  0.7265625
train loss:  0.5146592855453491
train gradient:  0.16399057260336103
iteration : 11345
train acc:  0.7265625
train loss:  0.5105128884315491
train gradient:  0.12407168230867362
iteration : 11346
train acc:  0.7578125
train loss:  0.49423879384994507
train gradient:  0.1989941848431059
iteration : 11347
train acc:  0.796875
train loss:  0.499362587928772
train gradient:  0.11345965774315427
iteration : 11348
train acc:  0.78125
train loss:  0.42744410037994385
train gradient:  0.11769386390723588
iteration : 11349
train acc:  0.78125
train loss:  0.461161732673645
train gradient:  0.1530631370891345
iteration : 11350
train acc:  0.7578125
train loss:  0.4767473042011261
train gradient:  0.14175133455792893
iteration : 11351
train acc:  0.7421875
train loss:  0.47065484523773193
train gradient:  0.1033170049525061
iteration : 11352
train acc:  0.7109375
train loss:  0.5187320709228516
train gradient:  0.14264972407831283
iteration : 11353
train acc:  0.71875
train loss:  0.48624616861343384
train gradient:  0.14128508600209344
iteration : 11354
train acc:  0.71875
train loss:  0.4679275155067444
train gradient:  0.09728038042534821
iteration : 11355
train acc:  0.7421875
train loss:  0.4805094003677368
train gradient:  0.13722489130198712
iteration : 11356
train acc:  0.71875
train loss:  0.4621626138687134
train gradient:  0.11670620791179526
iteration : 11357
train acc:  0.7734375
train loss:  0.4827573895454407
train gradient:  0.1242246174926444
iteration : 11358
train acc:  0.7734375
train loss:  0.47502678632736206
train gradient:  0.14881109807485648
iteration : 11359
train acc:  0.7109375
train loss:  0.6224466562271118
train gradient:  0.17703001773030716
iteration : 11360
train acc:  0.765625
train loss:  0.5028587579727173
train gradient:  0.11501516551637303
iteration : 11361
train acc:  0.7421875
train loss:  0.4932233393192291
train gradient:  0.14351578218212363
iteration : 11362
train acc:  0.6875
train loss:  0.5470200777053833
train gradient:  0.19725589757440973
iteration : 11363
train acc:  0.734375
train loss:  0.5051969289779663
train gradient:  0.16136206407683812
iteration : 11364
train acc:  0.71875
train loss:  0.5368546843528748
train gradient:  0.20229215974946643
iteration : 11365
train acc:  0.78125
train loss:  0.46010148525238037
train gradient:  0.11899153266869288
iteration : 11366
train acc:  0.6953125
train loss:  0.5154582262039185
train gradient:  0.15163216467667567
iteration : 11367
train acc:  0.6796875
train loss:  0.5616713762283325
train gradient:  0.16483950540404285
iteration : 11368
train acc:  0.7890625
train loss:  0.4389004707336426
train gradient:  0.0957318798971055
iteration : 11369
train acc:  0.7734375
train loss:  0.45377403497695923
train gradient:  0.10124097848340252
iteration : 11370
train acc:  0.78125
train loss:  0.45267075300216675
train gradient:  0.10297272465995452
iteration : 11371
train acc:  0.78125
train loss:  0.4559483528137207
train gradient:  0.1023339082305605
iteration : 11372
train acc:  0.7890625
train loss:  0.5043492317199707
train gradient:  0.15271404062550448
iteration : 11373
train acc:  0.78125
train loss:  0.536523699760437
train gradient:  0.12480497664409329
iteration : 11374
train acc:  0.78125
train loss:  0.4402432441711426
train gradient:  0.09986289740888692
iteration : 11375
train acc:  0.7421875
train loss:  0.475652813911438
train gradient:  0.1169256878394205
iteration : 11376
train acc:  0.8046875
train loss:  0.4499623477458954
train gradient:  0.11003216427492891
iteration : 11377
train acc:  0.796875
train loss:  0.5155160427093506
train gradient:  0.17053933078057623
iteration : 11378
train acc:  0.6796875
train loss:  0.555540919303894
train gradient:  0.15953006778274742
iteration : 11379
train acc:  0.765625
train loss:  0.5038114786148071
train gradient:  0.12810988630820308
iteration : 11380
train acc:  0.71875
train loss:  0.4672197699546814
train gradient:  0.13428317315427035
iteration : 11381
train acc:  0.7265625
train loss:  0.4898783564567566
train gradient:  0.10840789562988297
iteration : 11382
train acc:  0.7734375
train loss:  0.4818696677684784
train gradient:  0.14096514712949443
iteration : 11383
train acc:  0.75
train loss:  0.4931480586528778
train gradient:  0.14551338513727535
iteration : 11384
train acc:  0.6953125
train loss:  0.5284090638160706
train gradient:  0.12834697443344836
iteration : 11385
train acc:  0.75
train loss:  0.4819933772087097
train gradient:  0.1526225297494615
iteration : 11386
train acc:  0.6953125
train loss:  0.5306897163391113
train gradient:  0.13167071079864184
iteration : 11387
train acc:  0.7734375
train loss:  0.4810016453266144
train gradient:  0.146724137445846
iteration : 11388
train acc:  0.734375
train loss:  0.5079084634780884
train gradient:  0.12070328456051939
iteration : 11389
train acc:  0.828125
train loss:  0.39811789989471436
train gradient:  0.09779203819965851
iteration : 11390
train acc:  0.734375
train loss:  0.5246021747589111
train gradient:  0.18321868318020001
iteration : 11391
train acc:  0.796875
train loss:  0.477431982755661
train gradient:  0.14665821340606305
iteration : 11392
train acc:  0.78125
train loss:  0.5144767761230469
train gradient:  0.13748290436688482
iteration : 11393
train acc:  0.71875
train loss:  0.5458312630653381
train gradient:  0.20036324260265082
iteration : 11394
train acc:  0.734375
train loss:  0.5739185810089111
train gradient:  0.27616503202797316
iteration : 11395
train acc:  0.734375
train loss:  0.5659167766571045
train gradient:  0.16042871896777627
iteration : 11396
train acc:  0.7421875
train loss:  0.46373385190963745
train gradient:  0.11200285777167951
iteration : 11397
train acc:  0.7421875
train loss:  0.48915714025497437
train gradient:  0.1038636171415592
iteration : 11398
train acc:  0.6953125
train loss:  0.5555219650268555
train gradient:  0.1607211561620383
iteration : 11399
train acc:  0.7421875
train loss:  0.5043110847473145
train gradient:  0.1218717208176607
iteration : 11400
train acc:  0.765625
train loss:  0.48694908618927
train gradient:  0.12983260371503952
iteration : 11401
train acc:  0.7265625
train loss:  0.4680372476577759
train gradient:  0.12677731205098058
iteration : 11402
train acc:  0.71875
train loss:  0.5231136083602905
train gradient:  0.11639787381493522
iteration : 11403
train acc:  0.6484375
train loss:  0.5990874767303467
train gradient:  0.1554989444487751
iteration : 11404
train acc:  0.71875
train loss:  0.5052845478057861
train gradient:  0.0976200323271232
iteration : 11405
train acc:  0.7421875
train loss:  0.445071280002594
train gradient:  0.17086888171421138
iteration : 11406
train acc:  0.7578125
train loss:  0.47217971086502075
train gradient:  0.14478556625449338
iteration : 11407
train acc:  0.7578125
train loss:  0.48175227642059326
train gradient:  0.1082030467623426
iteration : 11408
train acc:  0.65625
train loss:  0.6514981985092163
train gradient:  0.2080821065982596
iteration : 11409
train acc:  0.671875
train loss:  0.580448567867279
train gradient:  0.13277348399912425
iteration : 11410
train acc:  0.7578125
train loss:  0.4927818477153778
train gradient:  0.1260641413452989
iteration : 11411
train acc:  0.7578125
train loss:  0.42966005206108093
train gradient:  0.103255876112271
iteration : 11412
train acc:  0.7421875
train loss:  0.4650691747665405
train gradient:  0.11355436731713146
iteration : 11413
train acc:  0.7734375
train loss:  0.4588136076927185
train gradient:  0.11309630689698508
iteration : 11414
train acc:  0.7578125
train loss:  0.45878344774246216
train gradient:  0.12698316663486436
iteration : 11415
train acc:  0.703125
train loss:  0.5191494226455688
train gradient:  0.14793264198532535
iteration : 11416
train acc:  0.734375
train loss:  0.5117142200469971
train gradient:  0.1624367844730663
iteration : 11417
train acc:  0.734375
train loss:  0.5284226536750793
train gradient:  0.14309585509538875
iteration : 11418
train acc:  0.7109375
train loss:  0.5671315789222717
train gradient:  0.15992028190184554
iteration : 11419
train acc:  0.7265625
train loss:  0.5157647728919983
train gradient:  0.13241147077046664
iteration : 11420
train acc:  0.7421875
train loss:  0.48812684416770935
train gradient:  0.12407121190453016
iteration : 11421
train acc:  0.7421875
train loss:  0.483093798160553
train gradient:  0.12159751511011208
iteration : 11422
train acc:  0.7734375
train loss:  0.47191232442855835
train gradient:  0.177064518617051
iteration : 11423
train acc:  0.6875
train loss:  0.5978744029998779
train gradient:  0.19227819911956834
iteration : 11424
train acc:  0.8046875
train loss:  0.4588266611099243
train gradient:  0.10298764363016095
iteration : 11425
train acc:  0.8046875
train loss:  0.41400042176246643
train gradient:  0.11201504207099298
iteration : 11426
train acc:  0.7734375
train loss:  0.43456804752349854
train gradient:  0.10488810586405409
iteration : 11427
train acc:  0.703125
train loss:  0.48498761653900146
train gradient:  0.16565271931145845
iteration : 11428
train acc:  0.7890625
train loss:  0.4339076578617096
train gradient:  0.1266821575632126
iteration : 11429
train acc:  0.7265625
train loss:  0.4919990301132202
train gradient:  0.09988752568311084
iteration : 11430
train acc:  0.7265625
train loss:  0.5130866765975952
train gradient:  0.14346650249768558
iteration : 11431
train acc:  0.71875
train loss:  0.5674987435340881
train gradient:  0.16001377695219776
iteration : 11432
train acc:  0.78125
train loss:  0.48046019673347473
train gradient:  0.11374471979829939
iteration : 11433
train acc:  0.7578125
train loss:  0.5300360918045044
train gradient:  0.1132599290922535
iteration : 11434
train acc:  0.71875
train loss:  0.5083482265472412
train gradient:  0.13804122296865312
iteration : 11435
train acc:  0.8046875
train loss:  0.43607211112976074
train gradient:  0.07701260184885485
iteration : 11436
train acc:  0.6796875
train loss:  0.5481389164924622
train gradient:  0.15275693949156255
iteration : 11437
train acc:  0.8203125
train loss:  0.4487149119377136
train gradient:  0.10421367060678374
iteration : 11438
train acc:  0.671875
train loss:  0.570087194442749
train gradient:  0.16011803854359152
iteration : 11439
train acc:  0.7734375
train loss:  0.4216366410255432
train gradient:  0.11991784363889771
iteration : 11440
train acc:  0.78125
train loss:  0.5025929808616638
train gradient:  0.13383992718088417
iteration : 11441
train acc:  0.6953125
train loss:  0.5517274141311646
train gradient:  0.1370890141892674
iteration : 11442
train acc:  0.765625
train loss:  0.4840799570083618
train gradient:  0.13634894354738708
iteration : 11443
train acc:  0.671875
train loss:  0.5636307001113892
train gradient:  0.1328346766399376
iteration : 11444
train acc:  0.765625
train loss:  0.49932101368904114
train gradient:  0.11100894964836022
iteration : 11445
train acc:  0.71875
train loss:  0.502244234085083
train gradient:  0.1296082213185613
iteration : 11446
train acc:  0.6640625
train loss:  0.5139739513397217
train gradient:  0.12184707294038186
iteration : 11447
train acc:  0.703125
train loss:  0.4699655771255493
train gradient:  0.10298278389737375
iteration : 11448
train acc:  0.7578125
train loss:  0.49415186047554016
train gradient:  0.1414981823467049
iteration : 11449
train acc:  0.75
train loss:  0.46096229553222656
train gradient:  0.11759594201312831
iteration : 11450
train acc:  0.703125
train loss:  0.516736626625061
train gradient:  0.13120840277463314
iteration : 11451
train acc:  0.7578125
train loss:  0.45892244577407837
train gradient:  0.09180629170083325
iteration : 11452
train acc:  0.734375
train loss:  0.5208943486213684
train gradient:  0.13162506506311325
iteration : 11453
train acc:  0.765625
train loss:  0.43499159812927246
train gradient:  0.09644162314993628
iteration : 11454
train acc:  0.765625
train loss:  0.45441657304763794
train gradient:  0.08860232403092505
iteration : 11455
train acc:  0.7734375
train loss:  0.5152559876441956
train gradient:  0.1270844493650954
iteration : 11456
train acc:  0.6796875
train loss:  0.5527178645133972
train gradient:  0.1595559690645803
iteration : 11457
train acc:  0.7578125
train loss:  0.44489139318466187
train gradient:  0.12953722460259454
iteration : 11458
train acc:  0.6953125
train loss:  0.5344768166542053
train gradient:  0.17228012602332554
iteration : 11459
train acc:  0.7578125
train loss:  0.4652891159057617
train gradient:  0.09708318933667266
iteration : 11460
train acc:  0.6953125
train loss:  0.5646650791168213
train gradient:  0.17210978589463305
iteration : 11461
train acc:  0.7421875
train loss:  0.508224368095398
train gradient:  0.13942323019683933
iteration : 11462
train acc:  0.734375
train loss:  0.4995644688606262
train gradient:  0.1272559422295403
iteration : 11463
train acc:  0.7734375
train loss:  0.4790056347846985
train gradient:  0.1381728798540396
iteration : 11464
train acc:  0.8046875
train loss:  0.4578554630279541
train gradient:  0.11239889612330317
iteration : 11465
train acc:  0.71875
train loss:  0.5140694975852966
train gradient:  0.12938579996771396
iteration : 11466
train acc:  0.7734375
train loss:  0.4412519335746765
train gradient:  0.10408027777551607
iteration : 11467
train acc:  0.7578125
train loss:  0.4886556565761566
train gradient:  0.13185765523658
iteration : 11468
train acc:  0.7578125
train loss:  0.5188443660736084
train gradient:  0.13297486398685393
iteration : 11469
train acc:  0.7734375
train loss:  0.47138845920562744
train gradient:  0.12643553935142637
iteration : 11470
train acc:  0.734375
train loss:  0.5344370007514954
train gradient:  0.15013066293266022
iteration : 11471
train acc:  0.6953125
train loss:  0.5619637966156006
train gradient:  0.1595645347796169
iteration : 11472
train acc:  0.7265625
train loss:  0.4874146580696106
train gradient:  0.1525235732490384
iteration : 11473
train acc:  0.796875
train loss:  0.4424578547477722
train gradient:  0.1177644099877932
iteration : 11474
train acc:  0.765625
train loss:  0.46831992268562317
train gradient:  0.12443008360885092
iteration : 11475
train acc:  0.6875
train loss:  0.5499587059020996
train gradient:  0.14861511912153078
iteration : 11476
train acc:  0.8125
train loss:  0.4190291166305542
train gradient:  0.08169264470884825
iteration : 11477
train acc:  0.71875
train loss:  0.5322647094726562
train gradient:  0.13369381064236435
iteration : 11478
train acc:  0.6640625
train loss:  0.5490995645523071
train gradient:  0.1434171842035698
iteration : 11479
train acc:  0.7734375
train loss:  0.48018670082092285
train gradient:  0.11430063250463063
iteration : 11480
train acc:  0.8203125
train loss:  0.4564402997493744
train gradient:  0.12352538088292701
iteration : 11481
train acc:  0.78125
train loss:  0.46004387736320496
train gradient:  0.10971565211205067
iteration : 11482
train acc:  0.734375
train loss:  0.5399488210678101
train gradient:  0.16959069542037203
iteration : 11483
train acc:  0.8203125
train loss:  0.40933096408843994
train gradient:  0.10296148787360758
iteration : 11484
train acc:  0.703125
train loss:  0.5181230902671814
train gradient:  0.12347130877145475
iteration : 11485
train acc:  0.75
train loss:  0.5342922210693359
train gradient:  0.14484090867533664
iteration : 11486
train acc:  0.765625
train loss:  0.5167263746261597
train gradient:  0.13295380836810483
iteration : 11487
train acc:  0.7265625
train loss:  0.4831641912460327
train gradient:  0.09776856533149973
iteration : 11488
train acc:  0.734375
train loss:  0.48090773820877075
train gradient:  0.12617293336806892
iteration : 11489
train acc:  0.734375
train loss:  0.4871610999107361
train gradient:  0.11519713314116871
iteration : 11490
train acc:  0.734375
train loss:  0.5117207765579224
train gradient:  0.14486588480775503
iteration : 11491
train acc:  0.8125
train loss:  0.42141562700271606
train gradient:  0.09156090715414131
iteration : 11492
train acc:  0.71875
train loss:  0.5846185088157654
train gradient:  0.1791973852057273
iteration : 11493
train acc:  0.703125
train loss:  0.5540277361869812
train gradient:  0.1629112118590625
iteration : 11494
train acc:  0.765625
train loss:  0.4804794490337372
train gradient:  0.14178962559122724
iteration : 11495
train acc:  0.703125
train loss:  0.5291121006011963
train gradient:  0.13640727352196155
iteration : 11496
train acc:  0.734375
train loss:  0.4354080855846405
train gradient:  0.0983650480389029
iteration : 11497
train acc:  0.7734375
train loss:  0.44281819462776184
train gradient:  0.12249291036153072
iteration : 11498
train acc:  0.7578125
train loss:  0.5188007950782776
train gradient:  0.11159142371528023
iteration : 11499
train acc:  0.796875
train loss:  0.436831533908844
train gradient:  0.12335629807611695
iteration : 11500
train acc:  0.703125
train loss:  0.5213743448257446
train gradient:  0.1584447245845257
iteration : 11501
train acc:  0.7265625
train loss:  0.48934680223464966
train gradient:  0.1563437645010416
iteration : 11502
train acc:  0.71875
train loss:  0.4697096347808838
train gradient:  0.1289315412024426
iteration : 11503
train acc:  0.8046875
train loss:  0.4132533073425293
train gradient:  0.11082688292985697
iteration : 11504
train acc:  0.78125
train loss:  0.44804197549819946
train gradient:  0.09190468306482037
iteration : 11505
train acc:  0.7890625
train loss:  0.46214330196380615
train gradient:  0.1074321490411181
iteration : 11506
train acc:  0.71875
train loss:  0.49394652247428894
train gradient:  0.14848803632702132
iteration : 11507
train acc:  0.7421875
train loss:  0.44515854120254517
train gradient:  0.11249940219950011
iteration : 11508
train acc:  0.75
train loss:  0.479910284280777
train gradient:  0.12487648809054248
iteration : 11509
train acc:  0.7890625
train loss:  0.4550454616546631
train gradient:  0.11499445906918244
iteration : 11510
train acc:  0.78125
train loss:  0.49647629261016846
train gradient:  0.13631665836726892
iteration : 11511
train acc:  0.7421875
train loss:  0.45409026741981506
train gradient:  0.09443697634707027
iteration : 11512
train acc:  0.796875
train loss:  0.43606969714164734
train gradient:  0.0986236583387671
iteration : 11513
train acc:  0.7578125
train loss:  0.5174948573112488
train gradient:  0.11551517716327947
iteration : 11514
train acc:  0.796875
train loss:  0.44127386808395386
train gradient:  0.09986176437983868
iteration : 11515
train acc:  0.7578125
train loss:  0.5117681622505188
train gradient:  0.12138440495172315
iteration : 11516
train acc:  0.6875
train loss:  0.55311119556427
train gradient:  0.15496860726579129
iteration : 11517
train acc:  0.7734375
train loss:  0.4319010376930237
train gradient:  0.08617043761896265
iteration : 11518
train acc:  0.7421875
train loss:  0.4886184632778168
train gradient:  0.14240800845642293
iteration : 11519
train acc:  0.6953125
train loss:  0.5214868783950806
train gradient:  0.1291913675726491
iteration : 11520
train acc:  0.671875
train loss:  0.5648846626281738
train gradient:  0.21814921600401466
iteration : 11521
train acc:  0.7265625
train loss:  0.47402769327163696
train gradient:  0.1259050922539026
iteration : 11522
train acc:  0.7734375
train loss:  0.4638673961162567
train gradient:  0.12852823678065994
iteration : 11523
train acc:  0.8046875
train loss:  0.4389931261539459
train gradient:  0.10457705647371268
iteration : 11524
train acc:  0.7734375
train loss:  0.5222271680831909
train gradient:  0.13509955016489616
iteration : 11525
train acc:  0.7421875
train loss:  0.5282162427902222
train gradient:  0.1742248622224002
iteration : 11526
train acc:  0.7578125
train loss:  0.5130609273910522
train gradient:  0.14492129569554119
iteration : 11527
train acc:  0.765625
train loss:  0.43247878551483154
train gradient:  0.09558514138385871
iteration : 11528
train acc:  0.71875
train loss:  0.5072983503341675
train gradient:  0.14948138728620697
iteration : 11529
train acc:  0.78125
train loss:  0.42401957511901855
train gradient:  0.0844161304767727
iteration : 11530
train acc:  0.7890625
train loss:  0.4210045635700226
train gradient:  0.1193218257854401
iteration : 11531
train acc:  0.703125
train loss:  0.5347223281860352
train gradient:  0.16847050541760214
iteration : 11532
train acc:  0.765625
train loss:  0.44732487201690674
train gradient:  0.10435429281718045
iteration : 11533
train acc:  0.7421875
train loss:  0.4850616157054901
train gradient:  0.19135518804319748
iteration : 11534
train acc:  0.7109375
train loss:  0.5466636419296265
train gradient:  0.15434083524477202
iteration : 11535
train acc:  0.703125
train loss:  0.5020148158073425
train gradient:  0.11156720102933924
iteration : 11536
train acc:  0.7265625
train loss:  0.48175495862960815
train gradient:  0.12619349138438507
iteration : 11537
train acc:  0.7578125
train loss:  0.49569088220596313
train gradient:  0.12499171659858141
iteration : 11538
train acc:  0.703125
train loss:  0.5491552948951721
train gradient:  0.17881904540353954
iteration : 11539
train acc:  0.734375
train loss:  0.520709216594696
train gradient:  0.14815081526873122
iteration : 11540
train acc:  0.7109375
train loss:  0.5213600397109985
train gradient:  0.15394412716173633
iteration : 11541
train acc:  0.7421875
train loss:  0.5106795430183411
train gradient:  0.11816179334918055
iteration : 11542
train acc:  0.7421875
train loss:  0.5070143938064575
train gradient:  0.13183050314026465
iteration : 11543
train acc:  0.765625
train loss:  0.5231509804725647
train gradient:  0.1524201158370353
iteration : 11544
train acc:  0.671875
train loss:  0.5397478342056274
train gradient:  0.14375773572733086
iteration : 11545
train acc:  0.7421875
train loss:  0.47064560651779175
train gradient:  0.11633481669691513
iteration : 11546
train acc:  0.6953125
train loss:  0.5479096174240112
train gradient:  0.1662340057525618
iteration : 11547
train acc:  0.8046875
train loss:  0.42955565452575684
train gradient:  0.12387011508912858
iteration : 11548
train acc:  0.6796875
train loss:  0.5548682808876038
train gradient:  0.14427079747166727
iteration : 11549
train acc:  0.7265625
train loss:  0.5433263778686523
train gradient:  0.1404597568856655
iteration : 11550
train acc:  0.6875
train loss:  0.5241824388504028
train gradient:  0.1320772385753567
iteration : 11551
train acc:  0.734375
train loss:  0.4820306599140167
train gradient:  0.1430181738069705
iteration : 11552
train acc:  0.7109375
train loss:  0.49957993626594543
train gradient:  0.13194382810513305
iteration : 11553
train acc:  0.6953125
train loss:  0.5269333720207214
train gradient:  0.13642365521412447
iteration : 11554
train acc:  0.78125
train loss:  0.4528501629829407
train gradient:  0.11997352476686987
iteration : 11555
train acc:  0.7109375
train loss:  0.48260053992271423
train gradient:  0.1019040303047372
iteration : 11556
train acc:  0.78125
train loss:  0.47737860679626465
train gradient:  0.11197643120989498
iteration : 11557
train acc:  0.765625
train loss:  0.517385721206665
train gradient:  0.15248230350817282
iteration : 11558
train acc:  0.71875
train loss:  0.5200340151786804
train gradient:  0.13751751961698722
iteration : 11559
train acc:  0.7109375
train loss:  0.5089821815490723
train gradient:  0.1487535484510797
iteration : 11560
train acc:  0.671875
train loss:  0.5432897210121155
train gradient:  0.15861287264038598
iteration : 11561
train acc:  0.7421875
train loss:  0.455150842666626
train gradient:  0.09367166345584951
iteration : 11562
train acc:  0.7734375
train loss:  0.47621259093284607
train gradient:  0.10388086266618797
iteration : 11563
train acc:  0.7890625
train loss:  0.4769360423088074
train gradient:  0.1414478760433286
iteration : 11564
train acc:  0.734375
train loss:  0.5890973806381226
train gradient:  0.20120012808980242
iteration : 11565
train acc:  0.71875
train loss:  0.5322059392929077
train gradient:  0.1508601846449015
iteration : 11566
train acc:  0.7421875
train loss:  0.535252034664154
train gradient:  0.1302078194086161
iteration : 11567
train acc:  0.734375
train loss:  0.5130397081375122
train gradient:  0.1366379271156727
iteration : 11568
train acc:  0.7421875
train loss:  0.5229436159133911
train gradient:  0.14017074918145367
iteration : 11569
train acc:  0.703125
train loss:  0.5116133689880371
train gradient:  0.12334589442106282
iteration : 11570
train acc:  0.703125
train loss:  0.5466468930244446
train gradient:  0.13119117278114611
iteration : 11571
train acc:  0.8125
train loss:  0.43958353996276855
train gradient:  0.1217190390382834
iteration : 11572
train acc:  0.71875
train loss:  0.5144623517990112
train gradient:  0.1113387124089232
iteration : 11573
train acc:  0.7421875
train loss:  0.4798029065132141
train gradient:  0.15455614431290726
iteration : 11574
train acc:  0.828125
train loss:  0.4055977761745453
train gradient:  0.1029596800015021
iteration : 11575
train acc:  0.71875
train loss:  0.4852268397808075
train gradient:  0.10954146088812901
iteration : 11576
train acc:  0.7734375
train loss:  0.42337656021118164
train gradient:  0.09144880044771697
iteration : 11577
train acc:  0.71875
train loss:  0.5303795337677002
train gradient:  0.16293873359880387
iteration : 11578
train acc:  0.734375
train loss:  0.5031870603561401
train gradient:  0.1532111287887092
iteration : 11579
train acc:  0.71875
train loss:  0.4363575577735901
train gradient:  0.0856383911003851
iteration : 11580
train acc:  0.6015625
train loss:  0.6101638674736023
train gradient:  0.16412247031174593
iteration : 11581
train acc:  0.765625
train loss:  0.45861977338790894
train gradient:  0.1037813638522349
iteration : 11582
train acc:  0.765625
train loss:  0.45421960949897766
train gradient:  0.08859496170952394
iteration : 11583
train acc:  0.703125
train loss:  0.4929409325122833
train gradient:  0.10581935951510123
iteration : 11584
train acc:  0.734375
train loss:  0.4979540705680847
train gradient:  0.1335000664876149
iteration : 11585
train acc:  0.7421875
train loss:  0.47045400738716125
train gradient:  0.15018217481180873
iteration : 11586
train acc:  0.78125
train loss:  0.47790199518203735
train gradient:  0.1319468328719937
iteration : 11587
train acc:  0.796875
train loss:  0.37185680866241455
train gradient:  0.08395687112222197
iteration : 11588
train acc:  0.71875
train loss:  0.5435687303543091
train gradient:  0.14091272920468062
iteration : 11589
train acc:  0.71875
train loss:  0.49852317571640015
train gradient:  0.1302056161982872
iteration : 11590
train acc:  0.7578125
train loss:  0.4871484041213989
train gradient:  0.12433726983781261
iteration : 11591
train acc:  0.765625
train loss:  0.45457619428634644
train gradient:  0.10555608880096232
iteration : 11592
train acc:  0.7421875
train loss:  0.5131649971008301
train gradient:  0.1332869534936803
iteration : 11593
train acc:  0.7890625
train loss:  0.4920586943626404
train gradient:  0.12321209295162926
iteration : 11594
train acc:  0.6640625
train loss:  0.5139192342758179
train gradient:  0.15508638053241897
iteration : 11595
train acc:  0.7890625
train loss:  0.4178919196128845
train gradient:  0.12080089268844085
iteration : 11596
train acc:  0.7421875
train loss:  0.45137089490890503
train gradient:  0.08730716436219407
iteration : 11597
train acc:  0.6796875
train loss:  0.557594895362854
train gradient:  0.13139883291531473
iteration : 11598
train acc:  0.6640625
train loss:  0.5706231594085693
train gradient:  0.17894402107159552
iteration : 11599
train acc:  0.7578125
train loss:  0.4578929543495178
train gradient:  0.11596396578545383
iteration : 11600
train acc:  0.75
train loss:  0.4921850860118866
train gradient:  0.113611514363851
iteration : 11601
train acc:  0.6875
train loss:  0.5901503562927246
train gradient:  0.18631501403811307
iteration : 11602
train acc:  0.7265625
train loss:  0.5077003240585327
train gradient:  0.12517483105157512
iteration : 11603
train acc:  0.75
train loss:  0.5311089754104614
train gradient:  0.1635659453739131
iteration : 11604
train acc:  0.6953125
train loss:  0.5199353098869324
train gradient:  0.11905957407573105
iteration : 11605
train acc:  0.796875
train loss:  0.42837774753570557
train gradient:  0.10424497642658068
iteration : 11606
train acc:  0.75
train loss:  0.5067950487136841
train gradient:  0.1366847726493089
iteration : 11607
train acc:  0.7734375
train loss:  0.46094369888305664
train gradient:  0.10874824770821073
iteration : 11608
train acc:  0.7578125
train loss:  0.5017089247703552
train gradient:  0.1507140943908359
iteration : 11609
train acc:  0.7421875
train loss:  0.48499947786331177
train gradient:  0.12177409665938825
iteration : 11610
train acc:  0.734375
train loss:  0.5376233458518982
train gradient:  0.17167122728450662
iteration : 11611
train acc:  0.7109375
train loss:  0.5758515000343323
train gradient:  0.18160922728871637
iteration : 11612
train acc:  0.7265625
train loss:  0.4772922992706299
train gradient:  0.1478362293774536
iteration : 11613
train acc:  0.75
train loss:  0.47916167974472046
train gradient:  0.11975219875541193
iteration : 11614
train acc:  0.7421875
train loss:  0.49228373169898987
train gradient:  0.13117869973509594
iteration : 11615
train acc:  0.7578125
train loss:  0.49137505888938904
train gradient:  0.1605163057579335
iteration : 11616
train acc:  0.7734375
train loss:  0.46091344952583313
train gradient:  0.0980035325963094
iteration : 11617
train acc:  0.6953125
train loss:  0.5519053936004639
train gradient:  0.14698894217354122
iteration : 11618
train acc:  0.7265625
train loss:  0.5380241274833679
train gradient:  0.21480398253433092
iteration : 11619
train acc:  0.8125
train loss:  0.4666256010532379
train gradient:  0.10700362426455926
iteration : 11620
train acc:  0.7734375
train loss:  0.48823386430740356
train gradient:  0.10745999562506554
iteration : 11621
train acc:  0.7578125
train loss:  0.49673229455947876
train gradient:  0.1459804319145186
iteration : 11622
train acc:  0.71875
train loss:  0.5136619210243225
train gradient:  0.12421285425996455
iteration : 11623
train acc:  0.734375
train loss:  0.4651246666908264
train gradient:  0.1146099533005076
iteration : 11624
train acc:  0.75
train loss:  0.5112676620483398
train gradient:  0.14484564027047725
iteration : 11625
train acc:  0.75
train loss:  0.48803022503852844
train gradient:  0.13168951629084802
iteration : 11626
train acc:  0.7421875
train loss:  0.48188361525535583
train gradient:  0.10728622876169719
iteration : 11627
train acc:  0.71875
train loss:  0.5235437154769897
train gradient:  0.13464663059457485
iteration : 11628
train acc:  0.8046875
train loss:  0.46923962235450745
train gradient:  0.13782858922675537
iteration : 11629
train acc:  0.7265625
train loss:  0.4633171558380127
train gradient:  0.1064123054637435
iteration : 11630
train acc:  0.6953125
train loss:  0.5958935022354126
train gradient:  0.19128865155655175
iteration : 11631
train acc:  0.7265625
train loss:  0.5357217788696289
train gradient:  0.175058198360597
iteration : 11632
train acc:  0.6796875
train loss:  0.5731370449066162
train gradient:  0.18656794504551405
iteration : 11633
train acc:  0.7421875
train loss:  0.5034869313240051
train gradient:  0.1263552479048473
iteration : 11634
train acc:  0.71875
train loss:  0.49689117074012756
train gradient:  0.1663967322314776
iteration : 11635
train acc:  0.7265625
train loss:  0.4529041051864624
train gradient:  0.09540347920589125
iteration : 11636
train acc:  0.7890625
train loss:  0.45180660486221313
train gradient:  0.11451284668134677
iteration : 11637
train acc:  0.75
train loss:  0.43207597732543945
train gradient:  0.08917686078976059
iteration : 11638
train acc:  0.7421875
train loss:  0.49739620089530945
train gradient:  0.11568570710188454
iteration : 11639
train acc:  0.8203125
train loss:  0.4280264973640442
train gradient:  0.09310023294853573
iteration : 11640
train acc:  0.8125
train loss:  0.48507630825042725
train gradient:  0.12290047746664459
iteration : 11641
train acc:  0.703125
train loss:  0.5500104427337646
train gradient:  0.14405324164170308
iteration : 11642
train acc:  0.7578125
train loss:  0.4892636835575104
train gradient:  0.12058235678466969
iteration : 11643
train acc:  0.75
train loss:  0.47474348545074463
train gradient:  0.13577491240138845
iteration : 11644
train acc:  0.6328125
train loss:  0.6323175430297852
train gradient:  0.20440823108891554
iteration : 11645
train acc:  0.765625
train loss:  0.45517978072166443
train gradient:  0.11986856921325761
iteration : 11646
train acc:  0.7265625
train loss:  0.48008763790130615
train gradient:  0.16333584260527584
iteration : 11647
train acc:  0.75
train loss:  0.4272661805152893
train gradient:  0.08396801083268303
iteration : 11648
train acc:  0.734375
train loss:  0.509323000907898
train gradient:  0.12187984485595761
iteration : 11649
train acc:  0.7578125
train loss:  0.5000799298286438
train gradient:  0.13804135305518356
iteration : 11650
train acc:  0.7734375
train loss:  0.4516855478286743
train gradient:  0.11689665149975355
iteration : 11651
train acc:  0.734375
train loss:  0.4691437780857086
train gradient:  0.13119624514576878
iteration : 11652
train acc:  0.765625
train loss:  0.5266764163970947
train gradient:  0.15684293489280782
iteration : 11653
train acc:  0.7265625
train loss:  0.5141304135322571
train gradient:  0.11885014808812315
iteration : 11654
train acc:  0.7734375
train loss:  0.4545031189918518
train gradient:  0.12549244724267966
iteration : 11655
train acc:  0.734375
train loss:  0.4877682030200958
train gradient:  0.10503764344936939
iteration : 11656
train acc:  0.734375
train loss:  0.49677836894989014
train gradient:  0.13114926735783905
iteration : 11657
train acc:  0.796875
train loss:  0.4362209439277649
train gradient:  0.10735435297740938
iteration : 11658
train acc:  0.71875
train loss:  0.5859841704368591
train gradient:  0.1860289182308737
iteration : 11659
train acc:  0.75
train loss:  0.5164722204208374
train gradient:  0.12925260565788543
iteration : 11660
train acc:  0.8046875
train loss:  0.4269198179244995
train gradient:  0.09897890709864188
iteration : 11661
train acc:  0.828125
train loss:  0.3891144096851349
train gradient:  0.10745600337290351
iteration : 11662
train acc:  0.75
train loss:  0.4615901708602905
train gradient:  0.10463128112666181
iteration : 11663
train acc:  0.7578125
train loss:  0.4573059678077698
train gradient:  0.10061491819172484
iteration : 11664
train acc:  0.703125
train loss:  0.5494444370269775
train gradient:  0.16866267873727442
iteration : 11665
train acc:  0.7421875
train loss:  0.494498610496521
train gradient:  0.11227608482585413
iteration : 11666
train acc:  0.7421875
train loss:  0.48959046602249146
train gradient:  0.10865988412008772
iteration : 11667
train acc:  0.78125
train loss:  0.4700741171836853
train gradient:  0.11956739599250217
iteration : 11668
train acc:  0.7265625
train loss:  0.4978697896003723
train gradient:  0.12115714487756964
iteration : 11669
train acc:  0.75
train loss:  0.49626609683036804
train gradient:  0.12458489905060813
iteration : 11670
train acc:  0.7109375
train loss:  0.5378491878509521
train gradient:  0.14055718833213307
iteration : 11671
train acc:  0.765625
train loss:  0.49338507652282715
train gradient:  0.11479327841620518
iteration : 11672
train acc:  0.7109375
train loss:  0.4782581627368927
train gradient:  0.11857683743340465
iteration : 11673
train acc:  0.6640625
train loss:  0.5702043771743774
train gradient:  0.1586742290376776
iteration : 11674
train acc:  0.7578125
train loss:  0.49445247650146484
train gradient:  0.1322696235018967
iteration : 11675
train acc:  0.71875
train loss:  0.5515406131744385
train gradient:  0.21701138149439483
iteration : 11676
train acc:  0.6640625
train loss:  0.5384008288383484
train gradient:  0.15247059531920804
iteration : 11677
train acc:  0.7734375
train loss:  0.5051358342170715
train gradient:  0.1175427643080713
iteration : 11678
train acc:  0.6796875
train loss:  0.5608876347541809
train gradient:  0.16468907281061396
iteration : 11679
train acc:  0.7890625
train loss:  0.42446616291999817
train gradient:  0.14016946725996005
iteration : 11680
train acc:  0.8125
train loss:  0.3928344249725342
train gradient:  0.08976731979152645
iteration : 11681
train acc:  0.6796875
train loss:  0.5199311971664429
train gradient:  0.11885288760522524
iteration : 11682
train acc:  0.7578125
train loss:  0.5150394439697266
train gradient:  0.13026494516506915
iteration : 11683
train acc:  0.7421875
train loss:  0.46993154287338257
train gradient:  0.1274797223478195
iteration : 11684
train acc:  0.7734375
train loss:  0.45973289012908936
train gradient:  0.11184000354033398
iteration : 11685
train acc:  0.765625
train loss:  0.503680408000946
train gradient:  0.15279531579732486
iteration : 11686
train acc:  0.75
train loss:  0.5089594721794128
train gradient:  0.1656945867310628
iteration : 11687
train acc:  0.7421875
train loss:  0.5427006483078003
train gradient:  0.13325303804168268
iteration : 11688
train acc:  0.6640625
train loss:  0.5783685445785522
train gradient:  0.14631291010319622
iteration : 11689
train acc:  0.8125
train loss:  0.43501022458076477
train gradient:  0.15036833408290462
iteration : 11690
train acc:  0.75
train loss:  0.49429288506507874
train gradient:  0.10401408598536327
iteration : 11691
train acc:  0.75
train loss:  0.4921988248825073
train gradient:  0.15719844487342288
iteration : 11692
train acc:  0.7578125
train loss:  0.4822195768356323
train gradient:  0.12520366779908795
iteration : 11693
train acc:  0.6953125
train loss:  0.5030645132064819
train gradient:  0.11452909263238188
iteration : 11694
train acc:  0.7421875
train loss:  0.4773198962211609
train gradient:  0.1226490153171062
iteration : 11695
train acc:  0.7734375
train loss:  0.4971151649951935
train gradient:  0.12561320873156212
iteration : 11696
train acc:  0.7578125
train loss:  0.44704926013946533
train gradient:  0.10798722311987467
iteration : 11697
train acc:  0.8125
train loss:  0.4362086057662964
train gradient:  0.1631394817559747
iteration : 11698
train acc:  0.7890625
train loss:  0.43142080307006836
train gradient:  0.11757943884026713
iteration : 11699
train acc:  0.7890625
train loss:  0.41853946447372437
train gradient:  0.10758880938636266
iteration : 11700
train acc:  0.6640625
train loss:  0.5466994047164917
train gradient:  0.14710986109138308
iteration : 11701
train acc:  0.7578125
train loss:  0.4979667067527771
train gradient:  0.11803177428134894
iteration : 11702
train acc:  0.734375
train loss:  0.4902206361293793
train gradient:  0.15798687129721317
iteration : 11703
train acc:  0.7265625
train loss:  0.4643998444080353
train gradient:  0.10161490387138153
iteration : 11704
train acc:  0.859375
train loss:  0.40265756845474243
train gradient:  0.11024303880214834
iteration : 11705
train acc:  0.7421875
train loss:  0.484540194272995
train gradient:  0.10392036973676316
iteration : 11706
train acc:  0.6875
train loss:  0.556911826133728
train gradient:  0.1467145849078937
iteration : 11707
train acc:  0.765625
train loss:  0.4454706907272339
train gradient:  0.10947408324710438
iteration : 11708
train acc:  0.703125
train loss:  0.5419707298278809
train gradient:  0.1421025461985535
iteration : 11709
train acc:  0.75
train loss:  0.4714997112751007
train gradient:  0.12815973238902806
iteration : 11710
train acc:  0.65625
train loss:  0.5621187090873718
train gradient:  0.1485451429181252
iteration : 11711
train acc:  0.75
train loss:  0.462120920419693
train gradient:  0.10931417604238858
iteration : 11712
train acc:  0.7578125
train loss:  0.4508270025253296
train gradient:  0.13036286617253812
iteration : 11713
train acc:  0.7421875
train loss:  0.5488089323043823
train gradient:  0.12538779233084196
iteration : 11714
train acc:  0.7734375
train loss:  0.4674359858036041
train gradient:  0.15486318852785375
iteration : 11715
train acc:  0.7890625
train loss:  0.5008144378662109
train gradient:  0.13581222725625705
iteration : 11716
train acc:  0.7265625
train loss:  0.5281721353530884
train gradient:  0.12487556876003644
iteration : 11717
train acc:  0.7421875
train loss:  0.4770083427429199
train gradient:  0.1159364476857467
iteration : 11718
train acc:  0.71875
train loss:  0.4911521077156067
train gradient:  0.13979232253574175
iteration : 11719
train acc:  0.7109375
train loss:  0.5370407104492188
train gradient:  0.12913054732517404
iteration : 11720
train acc:  0.7421875
train loss:  0.47282469272613525
train gradient:  0.10100936137908756
iteration : 11721
train acc:  0.796875
train loss:  0.49367111921310425
train gradient:  0.1330948011617446
iteration : 11722
train acc:  0.734375
train loss:  0.46791592240333557
train gradient:  0.13302969613051727
iteration : 11723
train acc:  0.71875
train loss:  0.46582603454589844
train gradient:  0.12478752075915471
iteration : 11724
train acc:  0.75
train loss:  0.4616839289665222
train gradient:  0.10082558330109255
iteration : 11725
train acc:  0.75
train loss:  0.5288624167442322
train gradient:  0.16869384748667454
iteration : 11726
train acc:  0.7734375
train loss:  0.436465859413147
train gradient:  0.10559085359467969
iteration : 11727
train acc:  0.796875
train loss:  0.4580927789211273
train gradient:  0.1032618606471132
iteration : 11728
train acc:  0.8125
train loss:  0.3846909701824188
train gradient:  0.09404285410899671
iteration : 11729
train acc:  0.703125
train loss:  0.5002346038818359
train gradient:  0.13895768363847824
iteration : 11730
train acc:  0.7578125
train loss:  0.48154470324516296
train gradient:  0.09586513593661623
iteration : 11731
train acc:  0.7734375
train loss:  0.4671970009803772
train gradient:  0.11310889209823079
iteration : 11732
train acc:  0.796875
train loss:  0.43534231185913086
train gradient:  0.08316578556544633
iteration : 11733
train acc:  0.7421875
train loss:  0.480379581451416
train gradient:  0.10653838531367633
iteration : 11734
train acc:  0.71875
train loss:  0.5436590909957886
train gradient:  0.1471580396305891
iteration : 11735
train acc:  0.8046875
train loss:  0.4455963671207428
train gradient:  0.13384848807172195
iteration : 11736
train acc:  0.734375
train loss:  0.46813392639160156
train gradient:  0.10369625078036336
iteration : 11737
train acc:  0.7421875
train loss:  0.500933051109314
train gradient:  0.11087836795961925
iteration : 11738
train acc:  0.7578125
train loss:  0.4747042655944824
train gradient:  0.11853743244489819
iteration : 11739
train acc:  0.7578125
train loss:  0.4250222146511078
train gradient:  0.11853514739835616
iteration : 11740
train acc:  0.7734375
train loss:  0.47159284353256226
train gradient:  0.1297433624270269
iteration : 11741
train acc:  0.7109375
train loss:  0.520851194858551
train gradient:  0.15679024126644198
iteration : 11742
train acc:  0.75
train loss:  0.45370399951934814
train gradient:  0.1070325551281099
iteration : 11743
train acc:  0.71875
train loss:  0.5093852281570435
train gradient:  0.16563541350584293
iteration : 11744
train acc:  0.7578125
train loss:  0.5251286625862122
train gradient:  0.16848555465838666
iteration : 11745
train acc:  0.7578125
train loss:  0.47084909677505493
train gradient:  0.1317011863778998
iteration : 11746
train acc:  0.7578125
train loss:  0.44641339778900146
train gradient:  0.10111082715953598
iteration : 11747
train acc:  0.7265625
train loss:  0.46859118342399597
train gradient:  0.10190537389704137
iteration : 11748
train acc:  0.65625
train loss:  0.5706998705863953
train gradient:  0.1540849568913707
iteration : 11749
train acc:  0.703125
train loss:  0.508186399936676
train gradient:  0.12542357995263764
iteration : 11750
train acc:  0.7265625
train loss:  0.4894213080406189
train gradient:  0.1229217108324318
iteration : 11751
train acc:  0.703125
train loss:  0.48518818616867065
train gradient:  0.12886680961670288
iteration : 11752
train acc:  0.71875
train loss:  0.4868107736110687
train gradient:  0.10830122982321469
iteration : 11753
train acc:  0.828125
train loss:  0.42658481001853943
train gradient:  0.10269418801532139
iteration : 11754
train acc:  0.703125
train loss:  0.5137814283370972
train gradient:  0.10984922616987733
iteration : 11755
train acc:  0.75
train loss:  0.4768720865249634
train gradient:  0.16539624659636498
iteration : 11756
train acc:  0.734375
train loss:  0.5248457193374634
train gradient:  0.1497167813085205
iteration : 11757
train acc:  0.78125
train loss:  0.42140060663223267
train gradient:  0.09352269817759801
iteration : 11758
train acc:  0.75
train loss:  0.4678804278373718
train gradient:  0.1095171572905029
iteration : 11759
train acc:  0.7734375
train loss:  0.4382226765155792
train gradient:  0.09441973040059373
iteration : 11760
train acc:  0.671875
train loss:  0.5439572930335999
train gradient:  0.13651616898395189
iteration : 11761
train acc:  0.7421875
train loss:  0.5259488224983215
train gradient:  0.16662458152996917
iteration : 11762
train acc:  0.8046875
train loss:  0.4618786871433258
train gradient:  0.12958542094805842
iteration : 11763
train acc:  0.6640625
train loss:  0.5878760814666748
train gradient:  0.19366277608198526
iteration : 11764
train acc:  0.8203125
train loss:  0.4002763032913208
train gradient:  0.09666451646657363
iteration : 11765
train acc:  0.7734375
train loss:  0.439057320356369
train gradient:  0.12945082706815292
iteration : 11766
train acc:  0.8125
train loss:  0.4268355071544647
train gradient:  0.1015313394603397
iteration : 11767
train acc:  0.765625
train loss:  0.4546562135219574
train gradient:  0.11867312214196081
iteration : 11768
train acc:  0.6875
train loss:  0.5695108771324158
train gradient:  0.1838529152129688
iteration : 11769
train acc:  0.796875
train loss:  0.40240681171417236
train gradient:  0.08491915410860752
iteration : 11770
train acc:  0.75
train loss:  0.48255693912506104
train gradient:  0.11666671917914202
iteration : 11771
train acc:  0.796875
train loss:  0.42743173241615295
train gradient:  0.0975991786217773
iteration : 11772
train acc:  0.7421875
train loss:  0.4781646132469177
train gradient:  0.11166257994490965
iteration : 11773
train acc:  0.7578125
train loss:  0.4848959445953369
train gradient:  0.1098910045383818
iteration : 11774
train acc:  0.7265625
train loss:  0.5080791711807251
train gradient:  0.12305495899227925
iteration : 11775
train acc:  0.7265625
train loss:  0.5187194347381592
train gradient:  0.15144690133406408
iteration : 11776
train acc:  0.71875
train loss:  0.4843877851963043
train gradient:  0.10505878599955552
iteration : 11777
train acc:  0.7578125
train loss:  0.5266344547271729
train gradient:  0.1418740823674713
iteration : 11778
train acc:  0.765625
train loss:  0.4319778084754944
train gradient:  0.10958533865137587
iteration : 11779
train acc:  0.71875
train loss:  0.4594769775867462
train gradient:  0.11812691009561867
iteration : 11780
train acc:  0.7109375
train loss:  0.5334218740463257
train gradient:  0.14866511242005104
iteration : 11781
train acc:  0.71875
train loss:  0.46305590867996216
train gradient:  0.10305279770772183
iteration : 11782
train acc:  0.6953125
train loss:  0.5800348520278931
train gradient:  0.19904196106854877
iteration : 11783
train acc:  0.7421875
train loss:  0.4887923002243042
train gradient:  0.12066609258692003
iteration : 11784
train acc:  0.75
train loss:  0.5143768787384033
train gradient:  0.1502791889685341
iteration : 11785
train acc:  0.7421875
train loss:  0.4551440477371216
train gradient:  0.0888771277549213
iteration : 11786
train acc:  0.7421875
train loss:  0.5029782652854919
train gradient:  0.12169384932767555
iteration : 11787
train acc:  0.7421875
train loss:  0.546438455581665
train gradient:  0.17268913417652187
iteration : 11788
train acc:  0.7421875
train loss:  0.5665720701217651
train gradient:  0.19849530388192746
iteration : 11789
train acc:  0.6875
train loss:  0.5444207191467285
train gradient:  0.18220116748018061
iteration : 11790
train acc:  0.6953125
train loss:  0.5383322834968567
train gradient:  0.1440849980460102
iteration : 11791
train acc:  0.7734375
train loss:  0.5001386404037476
train gradient:  0.20107343277156242
iteration : 11792
train acc:  0.6796875
train loss:  0.5258452892303467
train gradient:  0.1318851949309432
iteration : 11793
train acc:  0.7421875
train loss:  0.4816893935203552
train gradient:  0.124268181565961
iteration : 11794
train acc:  0.7578125
train loss:  0.48098453879356384
train gradient:  0.11797840074228982
iteration : 11795
train acc:  0.734375
train loss:  0.5244141817092896
train gradient:  0.1682503191580319
iteration : 11796
train acc:  0.7890625
train loss:  0.5096913576126099
train gradient:  0.128866694983001
iteration : 11797
train acc:  0.7265625
train loss:  0.5073137283325195
train gradient:  0.12042519800153446
iteration : 11798
train acc:  0.7734375
train loss:  0.45190852880477905
train gradient:  0.10526266171024286
iteration : 11799
train acc:  0.75
train loss:  0.5018183588981628
train gradient:  0.18771540308966428
iteration : 11800
train acc:  0.7265625
train loss:  0.5516446232795715
train gradient:  0.1884360782807225
iteration : 11801
train acc:  0.7421875
train loss:  0.42507296800613403
train gradient:  0.11033031608132257
iteration : 11802
train acc:  0.75
train loss:  0.4903850257396698
train gradient:  0.1528552142460498
iteration : 11803
train acc:  0.734375
train loss:  0.5084100961685181
train gradient:  0.1214434232206322
iteration : 11804
train acc:  0.734375
train loss:  0.49157363176345825
train gradient:  0.13621772225219214
iteration : 11805
train acc:  0.75
train loss:  0.4718436598777771
train gradient:  0.11258223527410263
iteration : 11806
train acc:  0.7734375
train loss:  0.48871874809265137
train gradient:  0.12075569241488734
iteration : 11807
train acc:  0.6640625
train loss:  0.5599142909049988
train gradient:  0.16683340936154784
iteration : 11808
train acc:  0.6875
train loss:  0.49850916862487793
train gradient:  0.14364572249798363
iteration : 11809
train acc:  0.7421875
train loss:  0.5297822952270508
train gradient:  0.14806118297975407
iteration : 11810
train acc:  0.78125
train loss:  0.43362945318222046
train gradient:  0.0716394652391012
iteration : 11811
train acc:  0.765625
train loss:  0.4797396659851074
train gradient:  0.11953569061601133
iteration : 11812
train acc:  0.796875
train loss:  0.4356515109539032
train gradient:  0.11935923945248217
iteration : 11813
train acc:  0.7265625
train loss:  0.5191497206687927
train gradient:  0.16934019636124897
iteration : 11814
train acc:  0.8125
train loss:  0.45776236057281494
train gradient:  0.1246557989090367
iteration : 11815
train acc:  0.734375
train loss:  0.5160276889801025
train gradient:  0.14503873084030822
iteration : 11816
train acc:  0.75
train loss:  0.4983464181423187
train gradient:  0.13648877406018137
iteration : 11817
train acc:  0.7578125
train loss:  0.5144869089126587
train gradient:  0.16019199401666234
iteration : 11818
train acc:  0.7109375
train loss:  0.5156095623970032
train gradient:  0.15435606661487877
iteration : 11819
train acc:  0.75
train loss:  0.4779903292655945
train gradient:  0.13275176318482174
iteration : 11820
train acc:  0.7265625
train loss:  0.5484022498130798
train gradient:  0.12764647160362
iteration : 11821
train acc:  0.7421875
train loss:  0.5541776418685913
train gradient:  0.1480168987801807
iteration : 11822
train acc:  0.7265625
train loss:  0.474096417427063
train gradient:  0.12079757995246089
iteration : 11823
train acc:  0.7421875
train loss:  0.5305755138397217
train gradient:  0.1379350708887602
iteration : 11824
train acc:  0.7421875
train loss:  0.46103939414024353
train gradient:  0.1094550734423694
iteration : 11825
train acc:  0.765625
train loss:  0.4859960973262787
train gradient:  0.10782608920320762
iteration : 11826
train acc:  0.8125
train loss:  0.45794811844825745
train gradient:  0.1179966018540806
iteration : 11827
train acc:  0.7421875
train loss:  0.5036949515342712
train gradient:  0.125393524895475
iteration : 11828
train acc:  0.7421875
train loss:  0.5268464088439941
train gradient:  0.13883863661215778
iteration : 11829
train acc:  0.71875
train loss:  0.5424636006355286
train gradient:  0.1874302213664537
iteration : 11830
train acc:  0.8359375
train loss:  0.4111914038658142
train gradient:  0.08477522344776646
iteration : 11831
train acc:  0.828125
train loss:  0.4154806137084961
train gradient:  0.07506004509380536
iteration : 11832
train acc:  0.671875
train loss:  0.5469011664390564
train gradient:  0.12942310060779844
iteration : 11833
train acc:  0.7109375
train loss:  0.5071851015090942
train gradient:  0.1360912304759676
iteration : 11834
train acc:  0.640625
train loss:  0.6267200708389282
train gradient:  0.1953829921060608
iteration : 11835
train acc:  0.7421875
train loss:  0.4448976218700409
train gradient:  0.1032852731082212
iteration : 11836
train acc:  0.7890625
train loss:  0.446586549282074
train gradient:  0.14023320737272954
iteration : 11837
train acc:  0.7265625
train loss:  0.5141746401786804
train gradient:  0.13564410879491667
iteration : 11838
train acc:  0.78125
train loss:  0.41776299476623535
train gradient:  0.10147220375914534
iteration : 11839
train acc:  0.7578125
train loss:  0.44637274742126465
train gradient:  0.12514150943497615
iteration : 11840
train acc:  0.78125
train loss:  0.4651942551136017
train gradient:  0.10978231713747288
iteration : 11841
train acc:  0.734375
train loss:  0.5493384599685669
train gradient:  0.14670456975215163
iteration : 11842
train acc:  0.7578125
train loss:  0.4784448742866516
train gradient:  0.11431122032831491
iteration : 11843
train acc:  0.75
train loss:  0.5183465480804443
train gradient:  0.16216494072248622
iteration : 11844
train acc:  0.6640625
train loss:  0.5801690816879272
train gradient:  0.20262456398522533
iteration : 11845
train acc:  0.7578125
train loss:  0.5385261178016663
train gradient:  0.16627855788622076
iteration : 11846
train acc:  0.7890625
train loss:  0.4810756742954254
train gradient:  0.15045537939290132
iteration : 11847
train acc:  0.75
train loss:  0.4442436695098877
train gradient:  0.11062619511911588
iteration : 11848
train acc:  0.7265625
train loss:  0.4994814991950989
train gradient:  0.15609457812606103
iteration : 11849
train acc:  0.7734375
train loss:  0.4716831147670746
train gradient:  0.12246674603411357
iteration : 11850
train acc:  0.75
train loss:  0.5137069225311279
train gradient:  0.14327033407151057
iteration : 11851
train acc:  0.6796875
train loss:  0.5503081679344177
train gradient:  0.17167067685452309
iteration : 11852
train acc:  0.6953125
train loss:  0.4975992739200592
train gradient:  0.13075557896954382
iteration : 11853
train acc:  0.7578125
train loss:  0.49578332901000977
train gradient:  0.10496841146094782
iteration : 11854
train acc:  0.7421875
train loss:  0.540968656539917
train gradient:  0.17929071207070296
iteration : 11855
train acc:  0.6953125
train loss:  0.569644570350647
train gradient:  0.21070378171960288
iteration : 11856
train acc:  0.828125
train loss:  0.3808980882167816
train gradient:  0.07937627077236956
iteration : 11857
train acc:  0.6953125
train loss:  0.5875457525253296
train gradient:  0.16401861214314084
iteration : 11858
train acc:  0.71875
train loss:  0.5445595383644104
train gradient:  0.13963306092359443
iteration : 11859
train acc:  0.8203125
train loss:  0.4530579149723053
train gradient:  0.11456646155361906
iteration : 11860
train acc:  0.765625
train loss:  0.4873321056365967
train gradient:  0.13273941665056813
iteration : 11861
train acc:  0.765625
train loss:  0.45051369071006775
train gradient:  0.13294908561063434
iteration : 11862
train acc:  0.7578125
train loss:  0.4811130166053772
train gradient:  0.10674159622823345
iteration : 11863
train acc:  0.7734375
train loss:  0.48838740587234497
train gradient:  0.1322857163042011
iteration : 11864
train acc:  0.6796875
train loss:  0.5939286947250366
train gradient:  0.17627925649559245
iteration : 11865
train acc:  0.6875
train loss:  0.6228746175765991
train gradient:  0.1823616150757462
iteration : 11866
train acc:  0.703125
train loss:  0.5239180326461792
train gradient:  0.11196206158739509
iteration : 11867
train acc:  0.78125
train loss:  0.44296199083328247
train gradient:  0.12483513199334617
iteration : 11868
train acc:  0.71875
train loss:  0.52347332239151
train gradient:  0.14701131839538656
iteration : 11869
train acc:  0.78125
train loss:  0.46430641412734985
train gradient:  0.10125798056937042
iteration : 11870
train acc:  0.7578125
train loss:  0.475870817899704
train gradient:  0.11931416181545462
iteration : 11871
train acc:  0.78125
train loss:  0.45714086294174194
train gradient:  0.09791511050107923
iteration : 11872
train acc:  0.75
train loss:  0.4973270893096924
train gradient:  0.11941189426885365
iteration : 11873
train acc:  0.7265625
train loss:  0.4969514310359955
train gradient:  0.1098784789525991
iteration : 11874
train acc:  0.7890625
train loss:  0.4228439927101135
train gradient:  0.09080481783281616
iteration : 11875
train acc:  0.71875
train loss:  0.4952254891395569
train gradient:  0.1220364984478161
iteration : 11876
train acc:  0.765625
train loss:  0.4541738033294678
train gradient:  0.08321864993668972
iteration : 11877
train acc:  0.7109375
train loss:  0.5460851788520813
train gradient:  0.15396164525751943
iteration : 11878
train acc:  0.78125
train loss:  0.44707757234573364
train gradient:  0.10269092998993877
iteration : 11879
train acc:  0.7265625
train loss:  0.5186729431152344
train gradient:  0.1393324332012949
iteration : 11880
train acc:  0.71875
train loss:  0.574511706829071
train gradient:  0.19989863789501208
iteration : 11881
train acc:  0.765625
train loss:  0.4885152578353882
train gradient:  0.13284555241543855
iteration : 11882
train acc:  0.7578125
train loss:  0.4544159770011902
train gradient:  0.119613207864836
iteration : 11883
train acc:  0.7890625
train loss:  0.4534759521484375
train gradient:  0.11532706275464757
iteration : 11884
train acc:  0.6953125
train loss:  0.5720870494842529
train gradient:  0.17844969791993204
iteration : 11885
train acc:  0.734375
train loss:  0.46184998750686646
train gradient:  0.12933550488599888
iteration : 11886
train acc:  0.75
train loss:  0.49940812587738037
train gradient:  0.13843457957058908
iteration : 11887
train acc:  0.75
train loss:  0.4882167875766754
train gradient:  0.11821693758546517
iteration : 11888
train acc:  0.8671875
train loss:  0.33760982751846313
train gradient:  0.07244186382966782
iteration : 11889
train acc:  0.734375
train loss:  0.4937919080257416
train gradient:  0.1131613486727122
iteration : 11890
train acc:  0.78125
train loss:  0.45886993408203125
train gradient:  0.13766863120446082
iteration : 11891
train acc:  0.6953125
train loss:  0.49047744274139404
train gradient:  0.12231287380512436
iteration : 11892
train acc:  0.8046875
train loss:  0.47334927320480347
train gradient:  0.11606672749563994
iteration : 11893
train acc:  0.71875
train loss:  0.5276609659194946
train gradient:  0.14620221352844684
iteration : 11894
train acc:  0.7578125
train loss:  0.5153683423995972
train gradient:  0.14741651322869714
iteration : 11895
train acc:  0.671875
train loss:  0.5448887348175049
train gradient:  0.17691918467481682
iteration : 11896
train acc:  0.796875
train loss:  0.42394208908081055
train gradient:  0.10816587217293176
iteration : 11897
train acc:  0.7578125
train loss:  0.4927405118942261
train gradient:  0.17291055681617395
iteration : 11898
train acc:  0.7578125
train loss:  0.46904441714286804
train gradient:  0.1120346913499187
iteration : 11899
train acc:  0.7265625
train loss:  0.48585957288742065
train gradient:  0.10653024326426082
iteration : 11900
train acc:  0.765625
train loss:  0.4932149052619934
train gradient:  0.1289777393314236
iteration : 11901
train acc:  0.765625
train loss:  0.4478456377983093
train gradient:  0.10180195873451531
iteration : 11902
train acc:  0.7265625
train loss:  0.5035808682441711
train gradient:  0.14024486461400576
iteration : 11903
train acc:  0.6796875
train loss:  0.5654053688049316
train gradient:  0.19226614434604067
iteration : 11904
train acc:  0.78125
train loss:  0.4993775188922882
train gradient:  0.1453545260550151
iteration : 11905
train acc:  0.734375
train loss:  0.5106010437011719
train gradient:  0.14646552679304126
iteration : 11906
train acc:  0.734375
train loss:  0.49439114332199097
train gradient:  0.12865895744167774
iteration : 11907
train acc:  0.7109375
train loss:  0.5196887254714966
train gradient:  0.1407005344687238
iteration : 11908
train acc:  0.7109375
train loss:  0.5254844427108765
train gradient:  0.16095689268546887
iteration : 11909
train acc:  0.7734375
train loss:  0.4055449962615967
train gradient:  0.11468274926615854
iteration : 11910
train acc:  0.796875
train loss:  0.4830547571182251
train gradient:  0.12310627679351392
iteration : 11911
train acc:  0.7734375
train loss:  0.4620366096496582
train gradient:  0.11240987085023868
iteration : 11912
train acc:  0.8359375
train loss:  0.4355071783065796
train gradient:  0.0791665077195213
iteration : 11913
train acc:  0.796875
train loss:  0.42295390367507935
train gradient:  0.10141522576714618
iteration : 11914
train acc:  0.7421875
train loss:  0.5601268410682678
train gradient:  0.1896994844386612
iteration : 11915
train acc:  0.7890625
train loss:  0.4591883718967438
train gradient:  0.15868995631517302
iteration : 11916
train acc:  0.734375
train loss:  0.49175435304641724
train gradient:  0.12294124726948019
iteration : 11917
train acc:  0.7421875
train loss:  0.5056531429290771
train gradient:  0.09376336864195649
iteration : 11918
train acc:  0.734375
train loss:  0.4881162643432617
train gradient:  0.11906480069033008
iteration : 11919
train acc:  0.7421875
train loss:  0.5445249080657959
train gradient:  0.13529063590430523
iteration : 11920
train acc:  0.7109375
train loss:  0.5076000690460205
train gradient:  0.13263668176275312
iteration : 11921
train acc:  0.7578125
train loss:  0.5059587955474854
train gradient:  0.14116584438629246
iteration : 11922
train acc:  0.75
train loss:  0.5271540880203247
train gradient:  0.15113597802249765
iteration : 11923
train acc:  0.7421875
train loss:  0.49726176261901855
train gradient:  0.12632538186100978
iteration : 11924
train acc:  0.75
train loss:  0.4726206064224243
train gradient:  0.11298694072308806
iteration : 11925
train acc:  0.7265625
train loss:  0.514032244682312
train gradient:  0.1439180573088134
iteration : 11926
train acc:  0.75
train loss:  0.4904429316520691
train gradient:  0.13078973566011073
iteration : 11927
train acc:  0.734375
train loss:  0.5589650869369507
train gradient:  0.15529325061202426
iteration : 11928
train acc:  0.765625
train loss:  0.4602091312408447
train gradient:  0.12928022675923856
iteration : 11929
train acc:  0.734375
train loss:  0.5225151777267456
train gradient:  0.16213633780562875
iteration : 11930
train acc:  0.75
train loss:  0.48603564500808716
train gradient:  0.1564856816573803
iteration : 11931
train acc:  0.734375
train loss:  0.5101306438446045
train gradient:  0.1419794959521702
iteration : 11932
train acc:  0.7265625
train loss:  0.526995062828064
train gradient:  0.14379009347924665
iteration : 11933
train acc:  0.6484375
train loss:  0.5505074262619019
train gradient:  0.1281199392471376
iteration : 11934
train acc:  0.7578125
train loss:  0.49232327938079834
train gradient:  0.13038165943990876
iteration : 11935
train acc:  0.7890625
train loss:  0.46401214599609375
train gradient:  0.12321514630835365
iteration : 11936
train acc:  0.6796875
train loss:  0.5416774153709412
train gradient:  0.14170744557700504
iteration : 11937
train acc:  0.7734375
train loss:  0.4502142667770386
train gradient:  0.11120650783819962
iteration : 11938
train acc:  0.6875
train loss:  0.5463635921478271
train gradient:  0.13209056658137205
iteration : 11939
train acc:  0.734375
train loss:  0.5068922638893127
train gradient:  0.14746357048618242
iteration : 11940
train acc:  0.671875
train loss:  0.5065510272979736
train gradient:  0.13327930285198455
iteration : 11941
train acc:  0.703125
train loss:  0.5579268932342529
train gradient:  0.16881830527774339
iteration : 11942
train acc:  0.796875
train loss:  0.46011900901794434
train gradient:  0.14387300378023476
iteration : 11943
train acc:  0.765625
train loss:  0.4485000967979431
train gradient:  0.11322934458830813
iteration : 11944
train acc:  0.7578125
train loss:  0.4901246726512909
train gradient:  0.09968367573035156
iteration : 11945
train acc:  0.78125
train loss:  0.47507792711257935
train gradient:  0.11105059498065795
iteration : 11946
train acc:  0.7421875
train loss:  0.46039971709251404
train gradient:  0.09584376615569146
iteration : 11947
train acc:  0.734375
train loss:  0.5015910863876343
train gradient:  0.11866630409291043
iteration : 11948
train acc:  0.7890625
train loss:  0.48470768332481384
train gradient:  0.11731584425817773
iteration : 11949
train acc:  0.796875
train loss:  0.4593402147293091
train gradient:  0.10366930677534264
iteration : 11950
train acc:  0.671875
train loss:  0.5736013650894165
train gradient:  0.14089102475656395
iteration : 11951
train acc:  0.7421875
train loss:  0.4776277542114258
train gradient:  0.09986196390028093
iteration : 11952
train acc:  0.765625
train loss:  0.4644383490085602
train gradient:  0.10894592668619821
iteration : 11953
train acc:  0.7890625
train loss:  0.4798794984817505
train gradient:  0.12546241998446073
iteration : 11954
train acc:  0.7421875
train loss:  0.5012214183807373
train gradient:  0.12599258154096393
iteration : 11955
train acc:  0.7421875
train loss:  0.510289192199707
train gradient:  0.13147732710201565
iteration : 11956
train acc:  0.7109375
train loss:  0.5218617916107178
train gradient:  0.1661284156708958
iteration : 11957
train acc:  0.7265625
train loss:  0.4902101159095764
train gradient:  0.13248568965697866
iteration : 11958
train acc:  0.78125
train loss:  0.47742006182670593
train gradient:  0.09749831878719857
iteration : 11959
train acc:  0.71875
train loss:  0.5017626285552979
train gradient:  0.20635538211254967
iteration : 11960
train acc:  0.8125
train loss:  0.4430696964263916
train gradient:  0.10749504265716253
iteration : 11961
train acc:  0.75
train loss:  0.4773463308811188
train gradient:  0.10875003974001511
iteration : 11962
train acc:  0.734375
train loss:  0.48301824927330017
train gradient:  0.1493874393591787
iteration : 11963
train acc:  0.75
train loss:  0.4629901945590973
train gradient:  0.11920065414649544
iteration : 11964
train acc:  0.6875
train loss:  0.5207971930503845
train gradient:  0.154928970117363
iteration : 11965
train acc:  0.765625
train loss:  0.4922848343849182
train gradient:  0.15284226977492088
iteration : 11966
train acc:  0.71875
train loss:  0.5333635210990906
train gradient:  0.1323497448288305
iteration : 11967
train acc:  0.7890625
train loss:  0.4732656478881836
train gradient:  0.12763906906726313
iteration : 11968
train acc:  0.7421875
train loss:  0.5190974473953247
train gradient:  0.15073487417920542
iteration : 11969
train acc:  0.7421875
train loss:  0.4798925817012787
train gradient:  0.11639801535695897
iteration : 11970
train acc:  0.7109375
train loss:  0.5107196569442749
train gradient:  0.12179211364638388
iteration : 11971
train acc:  0.8125
train loss:  0.48553499579429626
train gradient:  0.1260650632710243
iteration : 11972
train acc:  0.734375
train loss:  0.5181385278701782
train gradient:  0.13795379804861718
iteration : 11973
train acc:  0.765625
train loss:  0.42563045024871826
train gradient:  0.10551932745570786
iteration : 11974
train acc:  0.75
train loss:  0.49733513593673706
train gradient:  0.15453129735419552
iteration : 11975
train acc:  0.71875
train loss:  0.416422963142395
train gradient:  0.09312227814191189
iteration : 11976
train acc:  0.7109375
train loss:  0.510644793510437
train gradient:  0.12487377167717766
iteration : 11977
train acc:  0.7421875
train loss:  0.5435566902160645
train gradient:  0.13093585227450844
iteration : 11978
train acc:  0.796875
train loss:  0.4096291959285736
train gradient:  0.10787271213148729
iteration : 11979
train acc:  0.6875
train loss:  0.5499390363693237
train gradient:  0.13815531273766313
iteration : 11980
train acc:  0.765625
train loss:  0.4326417148113251
train gradient:  0.09696760884308256
iteration : 11981
train acc:  0.7890625
train loss:  0.43845027685165405
train gradient:  0.10326471369349975
iteration : 11982
train acc:  0.8203125
train loss:  0.4175737500190735
train gradient:  0.08703887845049055
iteration : 11983
train acc:  0.8359375
train loss:  0.41371411085128784
train gradient:  0.10266585188580725
iteration : 11984
train acc:  0.75
train loss:  0.4622747302055359
train gradient:  0.09713538189773967
iteration : 11985
train acc:  0.7578125
train loss:  0.5050330758094788
train gradient:  0.13359224029690087
iteration : 11986
train acc:  0.6953125
train loss:  0.5288277864456177
train gradient:  0.1771327894051537
iteration : 11987
train acc:  0.6953125
train loss:  0.48125791549682617
train gradient:  0.10872845167365523
iteration : 11988
train acc:  0.734375
train loss:  0.4967805743217468
train gradient:  0.1178726566852757
iteration : 11989
train acc:  0.75
train loss:  0.48045429587364197
train gradient:  0.11989726337824977
iteration : 11990
train acc:  0.75
train loss:  0.46534568071365356
train gradient:  0.19649877242052494
iteration : 11991
train acc:  0.7734375
train loss:  0.4455646872520447
train gradient:  0.11299896219850172
iteration : 11992
train acc:  0.7890625
train loss:  0.43155962228775024
train gradient:  0.1394853443770696
iteration : 11993
train acc:  0.734375
train loss:  0.5256971716880798
train gradient:  0.13351477451325222
iteration : 11994
train acc:  0.7578125
train loss:  0.44945982098579407
train gradient:  0.10185894290155463
iteration : 11995
train acc:  0.8046875
train loss:  0.431084007024765
train gradient:  0.09900349217173451
iteration : 11996
train acc:  0.7265625
train loss:  0.5040034055709839
train gradient:  0.11112061429967947
iteration : 11997
train acc:  0.7109375
train loss:  0.5220150947570801
train gradient:  0.1475320086281581
iteration : 11998
train acc:  0.71875
train loss:  0.544644296169281
train gradient:  0.16967671261814166
iteration : 11999
train acc:  0.71875
train loss:  0.4782193601131439
train gradient:  0.12750139341000427
iteration : 12000
train acc:  0.7109375
train loss:  0.513302206993103
train gradient:  0.1329602632742034
iteration : 12001
train acc:  0.75
train loss:  0.48996520042419434
train gradient:  0.10375063128081156
iteration : 12002
train acc:  0.765625
train loss:  0.50215744972229
train gradient:  0.16158086632803859
iteration : 12003
train acc:  0.6640625
train loss:  0.495802640914917
train gradient:  0.10675414943431324
iteration : 12004
train acc:  0.796875
train loss:  0.42300981283187866
train gradient:  0.1065123739248618
iteration : 12005
train acc:  0.7578125
train loss:  0.45525264739990234
train gradient:  0.10049988398502807
iteration : 12006
train acc:  0.7578125
train loss:  0.5310750603675842
train gradient:  0.12008559932052984
iteration : 12007
train acc:  0.78125
train loss:  0.42784595489501953
train gradient:  0.10952482686546988
iteration : 12008
train acc:  0.8046875
train loss:  0.45681077241897583
train gradient:  0.10564737757512939
iteration : 12009
train acc:  0.84375
train loss:  0.46111536026000977
train gradient:  0.12687307188205804
iteration : 12010
train acc:  0.765625
train loss:  0.5018312931060791
train gradient:  0.15912723871896622
iteration : 12011
train acc:  0.734375
train loss:  0.519842803478241
train gradient:  0.13909332246084333
iteration : 12012
train acc:  0.6953125
train loss:  0.5115715265274048
train gradient:  0.18075876673791755
iteration : 12013
train acc:  0.7734375
train loss:  0.45664456486701965
train gradient:  0.11983333317270602
iteration : 12014
train acc:  0.765625
train loss:  0.5097326636314392
train gradient:  0.13181624018265742
iteration : 12015
train acc:  0.734375
train loss:  0.4448985457420349
train gradient:  0.12343669606997142
iteration : 12016
train acc:  0.78125
train loss:  0.45680657029151917
train gradient:  0.11280635669859795
iteration : 12017
train acc:  0.671875
train loss:  0.5669716000556946
train gradient:  0.16780046564118814
iteration : 12018
train acc:  0.7578125
train loss:  0.5183432102203369
train gradient:  0.13742397664582315
iteration : 12019
train acc:  0.7109375
train loss:  0.5093691945075989
train gradient:  0.16154296418611147
iteration : 12020
train acc:  0.796875
train loss:  0.44874587655067444
train gradient:  0.12665929966128386
iteration : 12021
train acc:  0.7109375
train loss:  0.5191659331321716
train gradient:  0.1396263720406859
iteration : 12022
train acc:  0.7734375
train loss:  0.43358099460601807
train gradient:  0.09181300817241787
iteration : 12023
train acc:  0.71875
train loss:  0.5152276754379272
train gradient:  0.14267516180653206
iteration : 12024
train acc:  0.6953125
train loss:  0.574201226234436
train gradient:  0.19113670137079447
iteration : 12025
train acc:  0.7265625
train loss:  0.5286923050880432
train gradient:  0.14026861013706535
iteration : 12026
train acc:  0.7421875
train loss:  0.48142537474632263
train gradient:  0.10611844772395918
iteration : 12027
train acc:  0.7578125
train loss:  0.48139527440071106
train gradient:  0.15010538400963763
iteration : 12028
train acc:  0.78125
train loss:  0.47318801283836365
train gradient:  0.11466119789143965
iteration : 12029
train acc:  0.71875
train loss:  0.5009638667106628
train gradient:  0.11448936745222406
iteration : 12030
train acc:  0.78125
train loss:  0.44758841395378113
train gradient:  0.11034023150161258
iteration : 12031
train acc:  0.7109375
train loss:  0.531440258026123
train gradient:  0.14794195315893077
iteration : 12032
train acc:  0.671875
train loss:  0.5471173524856567
train gradient:  0.18938882772906684
iteration : 12033
train acc:  0.75
train loss:  0.4683663249015808
train gradient:  0.14391938049796688
iteration : 12034
train acc:  0.7421875
train loss:  0.47624170780181885
train gradient:  0.1160512625944806
iteration : 12035
train acc:  0.75
train loss:  0.529732882976532
train gradient:  0.12702188684017118
iteration : 12036
train acc:  0.75
train loss:  0.48440906405448914
train gradient:  0.11200720192280268
iteration : 12037
train acc:  0.7734375
train loss:  0.5073552131652832
train gradient:  0.14030103955820067
iteration : 12038
train acc:  0.71875
train loss:  0.5375250577926636
train gradient:  0.1417137777185899
iteration : 12039
train acc:  0.7265625
train loss:  0.5094808340072632
train gradient:  0.12938920914281093
iteration : 12040
train acc:  0.7265625
train loss:  0.5175453424453735
train gradient:  0.1156895193805534
iteration : 12041
train acc:  0.765625
train loss:  0.48295679688453674
train gradient:  0.1227522554057715
iteration : 12042
train acc:  0.703125
train loss:  0.53313148021698
train gradient:  0.11369557819666042
iteration : 12043
train acc:  0.71875
train loss:  0.5347391366958618
train gradient:  0.13324140434298115
iteration : 12044
train acc:  0.7578125
train loss:  0.48714953660964966
train gradient:  0.12748354858730893
iteration : 12045
train acc:  0.734375
train loss:  0.4989423155784607
train gradient:  0.14438862158367144
iteration : 12046
train acc:  0.7421875
train loss:  0.45672982931137085
train gradient:  0.09407823577908693
iteration : 12047
train acc:  0.7109375
train loss:  0.5187355279922485
train gradient:  0.11569479893516554
iteration : 12048
train acc:  0.6484375
train loss:  0.5623195171356201
train gradient:  0.15588706653934714
iteration : 12049
train acc:  0.78125
train loss:  0.49046534299850464
train gradient:  0.10494387081177552
iteration : 12050
train acc:  0.7421875
train loss:  0.4892078936100006
train gradient:  0.10793129093133712
iteration : 12051
train acc:  0.765625
train loss:  0.4592057466506958
train gradient:  0.10694435370342037
iteration : 12052
train acc:  0.75
train loss:  0.5247335433959961
train gradient:  0.13347082983143066
iteration : 12053
train acc:  0.7421875
train loss:  0.44876617193222046
train gradient:  0.10044396781864003
iteration : 12054
train acc:  0.734375
train loss:  0.4616239070892334
train gradient:  0.10226257566289758
iteration : 12055
train acc:  0.765625
train loss:  0.5211634039878845
train gradient:  0.14376450532307733
iteration : 12056
train acc:  0.8515625
train loss:  0.39866429567337036
train gradient:  0.1032089998250817
iteration : 12057
train acc:  0.6640625
train loss:  0.5438930988311768
train gradient:  0.12646616260171947
iteration : 12058
train acc:  0.75
train loss:  0.5102449059486389
train gradient:  0.14545124412016275
iteration : 12059
train acc:  0.8203125
train loss:  0.41608890891075134
train gradient:  0.10571119592739135
iteration : 12060
train acc:  0.765625
train loss:  0.4584673047065735
train gradient:  0.11379863094035621
iteration : 12061
train acc:  0.734375
train loss:  0.454071581363678
train gradient:  0.1021419461015246
iteration : 12062
train acc:  0.7265625
train loss:  0.47412919998168945
train gradient:  0.12186460668934607
iteration : 12063
train acc:  0.7578125
train loss:  0.5042053461074829
train gradient:  0.14184453626523752
iteration : 12064
train acc:  0.7421875
train loss:  0.48689383268356323
train gradient:  0.1276550325072301
iteration : 12065
train acc:  0.734375
train loss:  0.4634038805961609
train gradient:  0.1173449093352232
iteration : 12066
train acc:  0.7578125
train loss:  0.44844818115234375
train gradient:  0.11520645071128532
iteration : 12067
train acc:  0.7109375
train loss:  0.50067138671875
train gradient:  0.09819346067809191
iteration : 12068
train acc:  0.6953125
train loss:  0.5228323340415955
train gradient:  0.12081630861211656
iteration : 12069
train acc:  0.7890625
train loss:  0.4463161826133728
train gradient:  0.10295123077783469
iteration : 12070
train acc:  0.6796875
train loss:  0.5733382701873779
train gradient:  0.22931909850750304
iteration : 12071
train acc:  0.78125
train loss:  0.4459332525730133
train gradient:  0.12225926097327733
iteration : 12072
train acc:  0.765625
train loss:  0.5361109972000122
train gradient:  0.14086913802424944
iteration : 12073
train acc:  0.7265625
train loss:  0.5923207998275757
train gradient:  0.16943533772549624
iteration : 12074
train acc:  0.78125
train loss:  0.4286040663719177
train gradient:  0.11687679551850283
iteration : 12075
train acc:  0.734375
train loss:  0.47454050183296204
train gradient:  0.11752936726979456
iteration : 12076
train acc:  0.7265625
train loss:  0.47058022022247314
train gradient:  0.12232095588214535
iteration : 12077
train acc:  0.75
train loss:  0.5028799772262573
train gradient:  0.1629357848159947
iteration : 12078
train acc:  0.703125
train loss:  0.5731971859931946
train gradient:  0.19567728372078536
iteration : 12079
train acc:  0.75
train loss:  0.5775285959243774
train gradient:  0.182288706358119
iteration : 12080
train acc:  0.75
train loss:  0.4753686785697937
train gradient:  0.13324410856089058
iteration : 12081
train acc:  0.8203125
train loss:  0.42529454827308655
train gradient:  0.11596050078014747
iteration : 12082
train acc:  0.7109375
train loss:  0.5367611050605774
train gradient:  0.17334374908531103
iteration : 12083
train acc:  0.75
train loss:  0.49599701166152954
train gradient:  0.12957452829646804
iteration : 12084
train acc:  0.7109375
train loss:  0.5649421215057373
train gradient:  0.1801424547897589
iteration : 12085
train acc:  0.8203125
train loss:  0.40415725111961365
train gradient:  0.07835194616243654
iteration : 12086
train acc:  0.78125
train loss:  0.44617903232574463
train gradient:  0.09540584513894312
iteration : 12087
train acc:  0.78125
train loss:  0.45051631331443787
train gradient:  0.09571264340648648
iteration : 12088
train acc:  0.703125
train loss:  0.5258821249008179
train gradient:  0.12982641633307856
iteration : 12089
train acc:  0.7890625
train loss:  0.40183261036872864
train gradient:  0.09361599016377439
iteration : 12090
train acc:  0.7109375
train loss:  0.4880744516849518
train gradient:  0.11636510220802544
iteration : 12091
train acc:  0.7734375
train loss:  0.5063588619232178
train gradient:  0.15508505069380846
iteration : 12092
train acc:  0.8203125
train loss:  0.41000285744667053
train gradient:  0.0872243991968049
iteration : 12093
train acc:  0.7734375
train loss:  0.5037968158721924
train gradient:  0.11548752422149221
iteration : 12094
train acc:  0.7109375
train loss:  0.5462172627449036
train gradient:  0.1420291151573067
iteration : 12095
train acc:  0.78125
train loss:  0.5129899382591248
train gradient:  0.1811925791285404
iteration : 12096
train acc:  0.75
train loss:  0.46853289008140564
train gradient:  0.11322476170999575
iteration : 12097
train acc:  0.734375
train loss:  0.5044846534729004
train gradient:  0.1766262873624308
iteration : 12098
train acc:  0.671875
train loss:  0.5560101270675659
train gradient:  0.15651077542115732
iteration : 12099
train acc:  0.71875
train loss:  0.4800574779510498
train gradient:  0.10222511097167741
iteration : 12100
train acc:  0.7109375
train loss:  0.5221940279006958
train gradient:  0.12327813243174987
iteration : 12101
train acc:  0.7421875
train loss:  0.4892424941062927
train gradient:  0.10824006812148881
iteration : 12102
train acc:  0.7890625
train loss:  0.4879170060157776
train gradient:  0.13001972846275883
iteration : 12103
train acc:  0.765625
train loss:  0.45026516914367676
train gradient:  0.11553377488784265
iteration : 12104
train acc:  0.7421875
train loss:  0.516362190246582
train gradient:  0.13871432492540497
iteration : 12105
train acc:  0.7421875
train loss:  0.4706423282623291
train gradient:  0.1212064974824566
iteration : 12106
train acc:  0.703125
train loss:  0.4953273832798004
train gradient:  0.14878021119661167
iteration : 12107
train acc:  0.7890625
train loss:  0.45941871404647827
train gradient:  0.09803765954210823
iteration : 12108
train acc:  0.6875
train loss:  0.5260724425315857
train gradient:  0.1972164963157176
iteration : 12109
train acc:  0.6796875
train loss:  0.5304973125457764
train gradient:  0.13662780716944367
iteration : 12110
train acc:  0.7578125
train loss:  0.47227153182029724
train gradient:  0.11364827441689927
iteration : 12111
train acc:  0.734375
train loss:  0.48958030343055725
train gradient:  0.12937603241616155
iteration : 12112
train acc:  0.75
train loss:  0.4993022084236145
train gradient:  0.11668373575785353
iteration : 12113
train acc:  0.8046875
train loss:  0.422257661819458
train gradient:  0.09266743505448673
iteration : 12114
train acc:  0.7734375
train loss:  0.42194849252700806
train gradient:  0.11872038040073195
iteration : 12115
train acc:  0.734375
train loss:  0.4815741777420044
train gradient:  0.14933919061978324
iteration : 12116
train acc:  0.71875
train loss:  0.5084781646728516
train gradient:  0.13750032044378613
iteration : 12117
train acc:  0.7109375
train loss:  0.5340837240219116
train gradient:  0.1706621280868772
iteration : 12118
train acc:  0.734375
train loss:  0.4944024980068207
train gradient:  0.118491818828239
iteration : 12119
train acc:  0.796875
train loss:  0.4625246226787567
train gradient:  0.09939690008687964
iteration : 12120
train acc:  0.78125
train loss:  0.476159006357193
train gradient:  0.13180700476615725
iteration : 12121
train acc:  0.7109375
train loss:  0.5232248902320862
train gradient:  0.11055899943611566
iteration : 12122
train acc:  0.7734375
train loss:  0.4122016429901123
train gradient:  0.08479844701269687
iteration : 12123
train acc:  0.8046875
train loss:  0.4635794758796692
train gradient:  0.1181553249293538
iteration : 12124
train acc:  0.6796875
train loss:  0.5959784984588623
train gradient:  0.17321803738871058
iteration : 12125
train acc:  0.71875
train loss:  0.527438759803772
train gradient:  0.162891784894618
iteration : 12126
train acc:  0.7890625
train loss:  0.4511170983314514
train gradient:  0.11522807967693555
iteration : 12127
train acc:  0.6953125
train loss:  0.5201813578605652
train gradient:  0.13613813729963264
iteration : 12128
train acc:  0.7421875
train loss:  0.5118772983551025
train gradient:  0.10869268930455792
iteration : 12129
train acc:  0.75
train loss:  0.47462522983551025
train gradient:  0.10936620584835538
iteration : 12130
train acc:  0.71875
train loss:  0.5666157007217407
train gradient:  0.1486921997165876
iteration : 12131
train acc:  0.7734375
train loss:  0.45219725370407104
train gradient:  0.10618661164992874
iteration : 12132
train acc:  0.734375
train loss:  0.449332594871521
train gradient:  0.11706293305801255
iteration : 12133
train acc:  0.75
train loss:  0.49138638377189636
train gradient:  0.13523231077165604
iteration : 12134
train acc:  0.7421875
train loss:  0.49206870794296265
train gradient:  0.14493750164620656
iteration : 12135
train acc:  0.8359375
train loss:  0.4444732666015625
train gradient:  0.09691080578743941
iteration : 12136
train acc:  0.75
train loss:  0.5029062032699585
train gradient:  0.13628700147608258
iteration : 12137
train acc:  0.78125
train loss:  0.46510815620422363
train gradient:  0.12224682440604909
iteration : 12138
train acc:  0.7109375
train loss:  0.47998327016830444
train gradient:  0.12218468217686804
iteration : 12139
train acc:  0.75
train loss:  0.4947981834411621
train gradient:  0.10993212622193722
iteration : 12140
train acc:  0.7734375
train loss:  0.41170626878738403
train gradient:  0.1082523795585628
iteration : 12141
train acc:  0.71875
train loss:  0.531109094619751
train gradient:  0.1588193797662275
iteration : 12142
train acc:  0.734375
train loss:  0.47727149724960327
train gradient:  0.13718060786555425
iteration : 12143
train acc:  0.734375
train loss:  0.47529417276382446
train gradient:  0.1259497497036876
iteration : 12144
train acc:  0.7890625
train loss:  0.47485780715942383
train gradient:  0.11745610552531155
iteration : 12145
train acc:  0.7734375
train loss:  0.4672086238861084
train gradient:  0.11335979067283326
iteration : 12146
train acc:  0.671875
train loss:  0.6775456666946411
train gradient:  0.19570358987616038
iteration : 12147
train acc:  0.75
train loss:  0.49200037121772766
train gradient:  0.13216098658918443
iteration : 12148
train acc:  0.765625
train loss:  0.4548531174659729
train gradient:  0.12328076586973928
iteration : 12149
train acc:  0.796875
train loss:  0.43916213512420654
train gradient:  0.09759639493167517
iteration : 12150
train acc:  0.671875
train loss:  0.5923141837120056
train gradient:  0.21636391397467475
iteration : 12151
train acc:  0.703125
train loss:  0.535086989402771
train gradient:  0.13971154873593505
iteration : 12152
train acc:  0.75
train loss:  0.5133224129676819
train gradient:  0.10722335747720366
iteration : 12153
train acc:  0.78125
train loss:  0.4814895987510681
train gradient:  0.12555804195996387
iteration : 12154
train acc:  0.75
train loss:  0.50385582447052
train gradient:  0.1370614556394728
iteration : 12155
train acc:  0.6796875
train loss:  0.5357085466384888
train gradient:  0.1507650445896518
iteration : 12156
train acc:  0.734375
train loss:  0.5159524083137512
train gradient:  0.15205296094364346
iteration : 12157
train acc:  0.78125
train loss:  0.4714754819869995
train gradient:  0.11767861563544889
iteration : 12158
train acc:  0.7265625
train loss:  0.5148500204086304
train gradient:  0.10964744525915496
iteration : 12159
train acc:  0.734375
train loss:  0.5019523501396179
train gradient:  0.12955250108968078
iteration : 12160
train acc:  0.7421875
train loss:  0.4316716194152832
train gradient:  0.1350545438467306
iteration : 12161
train acc:  0.7421875
train loss:  0.4924347698688507
train gradient:  0.12753315384088681
iteration : 12162
train acc:  0.703125
train loss:  0.5323233604431152
train gradient:  0.12907665089069703
iteration : 12163
train acc:  0.7421875
train loss:  0.561251163482666
train gradient:  0.152153722260757
iteration : 12164
train acc:  0.7578125
train loss:  0.47258424758911133
train gradient:  0.11705532175475152
iteration : 12165
train acc:  0.7578125
train loss:  0.510657548904419
train gradient:  0.1427928787679309
iteration : 12166
train acc:  0.765625
train loss:  0.46710044145584106
train gradient:  0.10824908094441077
iteration : 12167
train acc:  0.75
train loss:  0.44658178091049194
train gradient:  0.11938386597331914
iteration : 12168
train acc:  0.796875
train loss:  0.4592549800872803
train gradient:  0.1089393962761289
iteration : 12169
train acc:  0.734375
train loss:  0.5451703071594238
train gradient:  0.16333775915169063
iteration : 12170
train acc:  0.7109375
train loss:  0.49855053424835205
train gradient:  0.13187482962376817
iteration : 12171
train acc:  0.7109375
train loss:  0.49519822001457214
train gradient:  0.1634530675438436
iteration : 12172
train acc:  0.765625
train loss:  0.49384716153144836
train gradient:  0.11664059092543112
iteration : 12173
train acc:  0.7578125
train loss:  0.44620513916015625
train gradient:  0.09563110645794713
iteration : 12174
train acc:  0.8125
train loss:  0.4600655734539032
train gradient:  0.11180929925654463
iteration : 12175
train acc:  0.6875
train loss:  0.5165590047836304
train gradient:  0.16081586685355845
iteration : 12176
train acc:  0.7265625
train loss:  0.4821760952472687
train gradient:  0.13487644118628309
iteration : 12177
train acc:  0.703125
train loss:  0.518447756767273
train gradient:  0.12930553431538996
iteration : 12178
train acc:  0.765625
train loss:  0.48246634006500244
train gradient:  0.2179703489506314
iteration : 12179
train acc:  0.7421875
train loss:  0.5108550786972046
train gradient:  0.1610539693468504
iteration : 12180
train acc:  0.796875
train loss:  0.43157440423965454
train gradient:  0.08970625415647498
iteration : 12181
train acc:  0.7578125
train loss:  0.5332522988319397
train gradient:  0.18549510131966507
iteration : 12182
train acc:  0.796875
train loss:  0.41689470410346985
train gradient:  0.10537255221579148
iteration : 12183
train acc:  0.7578125
train loss:  0.47530972957611084
train gradient:  0.11049070954294438
iteration : 12184
train acc:  0.75
train loss:  0.49455884099006653
train gradient:  0.13153938303637724
iteration : 12185
train acc:  0.75
train loss:  0.49594223499298096
train gradient:  0.14729136013376348
iteration : 12186
train acc:  0.7734375
train loss:  0.46799325942993164
train gradient:  0.12488217593732018
iteration : 12187
train acc:  0.7578125
train loss:  0.5005438327789307
train gradient:  0.16960569368771813
iteration : 12188
train acc:  0.6875
train loss:  0.5454574823379517
train gradient:  0.204913441703375
iteration : 12189
train acc:  0.7109375
train loss:  0.5720087885856628
train gradient:  0.15684698373542538
iteration : 12190
train acc:  0.7265625
train loss:  0.5318253040313721
train gradient:  0.17835519587313797
iteration : 12191
train acc:  0.734375
train loss:  0.4821614623069763
train gradient:  0.10602587709116604
iteration : 12192
train acc:  0.71875
train loss:  0.45776641368865967
train gradient:  0.19011683730195128
iteration : 12193
train acc:  0.765625
train loss:  0.4520483911037445
train gradient:  0.1283651801086092
iteration : 12194
train acc:  0.7734375
train loss:  0.48428788781166077
train gradient:  0.11201487613241468
iteration : 12195
train acc:  0.78125
train loss:  0.4835885167121887
train gradient:  0.12259782584599667
iteration : 12196
train acc:  0.78125
train loss:  0.4596937298774719
train gradient:  0.13873236932699806
iteration : 12197
train acc:  0.78125
train loss:  0.4762818217277527
train gradient:  0.10620586201771623
iteration : 12198
train acc:  0.671875
train loss:  0.5181328654289246
train gradient:  0.1261398103295531
iteration : 12199
train acc:  0.640625
train loss:  0.5264568328857422
train gradient:  0.12894411678571432
iteration : 12200
train acc:  0.734375
train loss:  0.5041638016700745
train gradient:  0.1296400391302489
iteration : 12201
train acc:  0.7421875
train loss:  0.468112051486969
train gradient:  0.1262402845547084
iteration : 12202
train acc:  0.7734375
train loss:  0.43394988775253296
train gradient:  0.07668209588085508
iteration : 12203
train acc:  0.84375
train loss:  0.4109891653060913
train gradient:  0.10348438041975963
iteration : 12204
train acc:  0.7578125
train loss:  0.4722369909286499
train gradient:  0.10775803845463482
iteration : 12205
train acc:  0.7578125
train loss:  0.5108026266098022
train gradient:  0.152521082055959
iteration : 12206
train acc:  0.6875
train loss:  0.5375595092773438
train gradient:  0.1340271191319255
iteration : 12207
train acc:  0.71875
train loss:  0.5582212805747986
train gradient:  0.24530236992660148
iteration : 12208
train acc:  0.71875
train loss:  0.5117989182472229
train gradient:  0.14735674730449527
iteration : 12209
train acc:  0.7578125
train loss:  0.4860582649707794
train gradient:  0.13791442777087648
iteration : 12210
train acc:  0.78125
train loss:  0.5200665593147278
train gradient:  0.13496225712466878
iteration : 12211
train acc:  0.71875
train loss:  0.4976636469364166
train gradient:  0.14216090952374366
iteration : 12212
train acc:  0.734375
train loss:  0.5192572474479675
train gradient:  0.14049199292836542
iteration : 12213
train acc:  0.7109375
train loss:  0.528069019317627
train gradient:  0.11820598824964117
iteration : 12214
train acc:  0.7421875
train loss:  0.4915095865726471
train gradient:  0.1268799451513844
iteration : 12215
train acc:  0.6953125
train loss:  0.5458322763442993
train gradient:  0.1300137414296364
iteration : 12216
train acc:  0.7109375
train loss:  0.511389970779419
train gradient:  0.14320727012783002
iteration : 12217
train acc:  0.796875
train loss:  0.4851253628730774
train gradient:  0.12957658700430896
iteration : 12218
train acc:  0.734375
train loss:  0.4915510416030884
train gradient:  0.146542644682423
iteration : 12219
train acc:  0.8046875
train loss:  0.42484819889068604
train gradient:  0.08279694010490436
iteration : 12220
train acc:  0.6953125
train loss:  0.5407080054283142
train gradient:  0.14411161039290227
iteration : 12221
train acc:  0.7890625
train loss:  0.4377329349517822
train gradient:  0.11572364522921369
iteration : 12222
train acc:  0.7109375
train loss:  0.5310097932815552
train gradient:  0.14556443310548106
iteration : 12223
train acc:  0.734375
train loss:  0.5329028964042664
train gradient:  0.14258943139969987
iteration : 12224
train acc:  0.765625
train loss:  0.4917411804199219
train gradient:  0.12307711334056994
iteration : 12225
train acc:  0.78125
train loss:  0.4853416681289673
train gradient:  0.170242675225719
iteration : 12226
train acc:  0.7109375
train loss:  0.5271091461181641
train gradient:  0.1458593328190208
iteration : 12227
train acc:  0.703125
train loss:  0.4979509711265564
train gradient:  0.14023669382169793
iteration : 12228
train acc:  0.7734375
train loss:  0.4702911376953125
train gradient:  0.13839860579905439
iteration : 12229
train acc:  0.65625
train loss:  0.5703142881393433
train gradient:  0.23117874955302137
iteration : 12230
train acc:  0.7890625
train loss:  0.46907922625541687
train gradient:  0.13396199895696778
iteration : 12231
train acc:  0.6875
train loss:  0.538943886756897
train gradient:  0.12742039847678
iteration : 12232
train acc:  0.7734375
train loss:  0.44846758246421814
train gradient:  0.07907585629868472
iteration : 12233
train acc:  0.78125
train loss:  0.4799962639808655
train gradient:  0.17659639571077607
iteration : 12234
train acc:  0.703125
train loss:  0.5122268795967102
train gradient:  0.1297110599618925
iteration : 12235
train acc:  0.6640625
train loss:  0.5437296032905579
train gradient:  0.1394888808965732
iteration : 12236
train acc:  0.75
train loss:  0.4795685410499573
train gradient:  0.11881223828911189
iteration : 12237
train acc:  0.734375
train loss:  0.47529056668281555
train gradient:  0.11244039101165569
iteration : 12238
train acc:  0.6953125
train loss:  0.5234050750732422
train gradient:  0.13797352922837142
iteration : 12239
train acc:  0.7265625
train loss:  0.4770738482475281
train gradient:  0.16144686011906068
iteration : 12240
train acc:  0.7265625
train loss:  0.5131352543830872
train gradient:  0.16423537706133745
iteration : 12241
train acc:  0.765625
train loss:  0.4777200520038605
train gradient:  0.12370169396698666
iteration : 12242
train acc:  0.8046875
train loss:  0.44027024507522583
train gradient:  0.11573786660193779
iteration : 12243
train acc:  0.796875
train loss:  0.44605201482772827
train gradient:  0.09783608146441436
iteration : 12244
train acc:  0.7578125
train loss:  0.46531832218170166
train gradient:  0.11871610536086198
iteration : 12245
train acc:  0.8046875
train loss:  0.4799084961414337
train gradient:  0.14705469377290564
iteration : 12246
train acc:  0.75
train loss:  0.5212110280990601
train gradient:  0.14963241321877702
iteration : 12247
train acc:  0.703125
train loss:  0.4873998761177063
train gradient:  0.13539983563476585
iteration : 12248
train acc:  0.71875
train loss:  0.5691296458244324
train gradient:  0.152264097556777
iteration : 12249
train acc:  0.6875
train loss:  0.6003692746162415
train gradient:  0.14497845052541963
iteration : 12250
train acc:  0.7734375
train loss:  0.4916563332080841
train gradient:  0.1300225567120455
iteration : 12251
train acc:  0.7890625
train loss:  0.4668503403663635
train gradient:  0.1221507029860297
iteration : 12252
train acc:  0.734375
train loss:  0.5375434160232544
train gradient:  0.15604415384743342
iteration : 12253
train acc:  0.7734375
train loss:  0.4602013826370239
train gradient:  0.11566409069133739
iteration : 12254
train acc:  0.734375
train loss:  0.49763110280036926
train gradient:  0.16036900803271953
iteration : 12255
train acc:  0.6953125
train loss:  0.5166194438934326
train gradient:  0.13070629220437485
iteration : 12256
train acc:  0.78125
train loss:  0.45865267515182495
train gradient:  0.114577092262223
iteration : 12257
train acc:  0.7734375
train loss:  0.4647819995880127
train gradient:  0.11366277274085343
iteration : 12258
train acc:  0.734375
train loss:  0.483021080493927
train gradient:  0.1269658636414282
iteration : 12259
train acc:  0.8203125
train loss:  0.4569644033908844
train gradient:  0.11138743137201346
iteration : 12260
train acc:  0.765625
train loss:  0.5231858491897583
train gradient:  0.14780552688718063
iteration : 12261
train acc:  0.7578125
train loss:  0.5078034400939941
train gradient:  0.11901224440307338
iteration : 12262
train acc:  0.7421875
train loss:  0.4745226204395294
train gradient:  0.1262729928461938
iteration : 12263
train acc:  0.7265625
train loss:  0.4997681975364685
train gradient:  0.1527113121601692
iteration : 12264
train acc:  0.7578125
train loss:  0.5415201187133789
train gradient:  0.1618141397886071
iteration : 12265
train acc:  0.8125
train loss:  0.41358819603919983
train gradient:  0.09041003854660218
iteration : 12266
train acc:  0.7890625
train loss:  0.42756882309913635
train gradient:  0.11019384073614741
iteration : 12267
train acc:  0.7734375
train loss:  0.4306419789791107
train gradient:  0.08563151582358956
iteration : 12268
train acc:  0.7890625
train loss:  0.50444495677948
train gradient:  0.1300296208447243
iteration : 12269
train acc:  0.828125
train loss:  0.37887993454933167
train gradient:  0.12716002413054484
iteration : 12270
train acc:  0.734375
train loss:  0.49342185258865356
train gradient:  0.15043700179972758
iteration : 12271
train acc:  0.78125
train loss:  0.4388706684112549
train gradient:  0.11631278930439064
iteration : 12272
train acc:  0.8125
train loss:  0.41715919971466064
train gradient:  0.10445639415079755
iteration : 12273
train acc:  0.7578125
train loss:  0.44063249230384827
train gradient:  0.11953718620190662
iteration : 12274
train acc:  0.7890625
train loss:  0.4594513773918152
train gradient:  0.13754001887639192
iteration : 12275
train acc:  0.8046875
train loss:  0.4514009952545166
train gradient:  0.0937987938703027
iteration : 12276
train acc:  0.71875
train loss:  0.5527144074440002
train gradient:  0.16776817450524414
iteration : 12277
train acc:  0.828125
train loss:  0.36438918113708496
train gradient:  0.08563556134337087
iteration : 12278
train acc:  0.734375
train loss:  0.4568593502044678
train gradient:  0.12042791044176651
iteration : 12279
train acc:  0.734375
train loss:  0.5342352390289307
train gradient:  0.1377084988552844
iteration : 12280
train acc:  0.6953125
train loss:  0.5465904474258423
train gradient:  0.13577125484401248
iteration : 12281
train acc:  0.7265625
train loss:  0.48393088579177856
train gradient:  0.1150394237399599
iteration : 12282
train acc:  0.7421875
train loss:  0.4743194878101349
train gradient:  0.1212432142502552
iteration : 12283
train acc:  0.7734375
train loss:  0.4638996422290802
train gradient:  0.11794027164140296
iteration : 12284
train acc:  0.765625
train loss:  0.4714425802230835
train gradient:  0.11250044415597646
iteration : 12285
train acc:  0.703125
train loss:  0.591585636138916
train gradient:  0.19629464673337538
iteration : 12286
train acc:  0.7578125
train loss:  0.5046945810317993
train gradient:  0.1192924927954354
iteration : 12287
train acc:  0.7578125
train loss:  0.46100467443466187
train gradient:  0.12271729118758015
iteration : 12288
train acc:  0.7421875
train loss:  0.49351391196250916
train gradient:  0.13168976063978388
iteration : 12289
train acc:  0.8515625
train loss:  0.4119393229484558
train gradient:  0.10887279132387045
iteration : 12290
train acc:  0.765625
train loss:  0.4784305691719055
train gradient:  0.11729867420341371
iteration : 12291
train acc:  0.78125
train loss:  0.4427824020385742
train gradient:  0.11236916202808198
iteration : 12292
train acc:  0.703125
train loss:  0.5479398965835571
train gradient:  0.16963214113117725
iteration : 12293
train acc:  0.8046875
train loss:  0.423845499753952
train gradient:  0.11437196408867754
iteration : 12294
train acc:  0.7578125
train loss:  0.45255136489868164
train gradient:  0.09658105302665244
iteration : 12295
train acc:  0.7109375
train loss:  0.5473559498786926
train gradient:  0.19638669683303794
iteration : 12296
train acc:  0.765625
train loss:  0.4645743668079376
train gradient:  0.11261141643517426
iteration : 12297
train acc:  0.7265625
train loss:  0.5227863788604736
train gradient:  0.14900328783287953
iteration : 12298
train acc:  0.7265625
train loss:  0.47532373666763306
train gradient:  0.09531435789659357
iteration : 12299
train acc:  0.734375
train loss:  0.46610191464424133
train gradient:  0.10058901532546959
iteration : 12300
train acc:  0.7734375
train loss:  0.45706239342689514
train gradient:  0.11156314741275677
iteration : 12301
train acc:  0.7109375
train loss:  0.5277859568595886
train gradient:  0.13510057403652675
iteration : 12302
train acc:  0.796875
train loss:  0.4567963480949402
train gradient:  0.09637059356646974
iteration : 12303
train acc:  0.7265625
train loss:  0.4831486940383911
train gradient:  0.13194294980380822
iteration : 12304
train acc:  0.7421875
train loss:  0.4838353097438812
train gradient:  0.1444220328847789
iteration : 12305
train acc:  0.671875
train loss:  0.5806195735931396
train gradient:  0.15990976564413428
iteration : 12306
train acc:  0.734375
train loss:  0.49725013971328735
train gradient:  0.1195132672296516
iteration : 12307
train acc:  0.71875
train loss:  0.5223580598831177
train gradient:  0.15803777734267305
iteration : 12308
train acc:  0.734375
train loss:  0.4691537022590637
train gradient:  0.11853387256388057
iteration : 12309
train acc:  0.703125
train loss:  0.5710512399673462
train gradient:  0.16622681550339075
iteration : 12310
train acc:  0.7734375
train loss:  0.4987899661064148
train gradient:  0.13387925709986914
iteration : 12311
train acc:  0.75
train loss:  0.4699963927268982
train gradient:  0.1158955739493578
iteration : 12312
train acc:  0.6796875
train loss:  0.5738624334335327
train gradient:  0.1475469746087254
iteration : 12313
train acc:  0.703125
train loss:  0.4831656217575073
train gradient:  0.1730289028958713
iteration : 12314
train acc:  0.71875
train loss:  0.518028974533081
train gradient:  0.16182391165806842
iteration : 12315
train acc:  0.734375
train loss:  0.4969726800918579
train gradient:  0.13105393862637885
iteration : 12316
train acc:  0.7421875
train loss:  0.4790821075439453
train gradient:  0.11485304691910175
iteration : 12317
train acc:  0.734375
train loss:  0.4927116334438324
train gradient:  0.11917858968601372
iteration : 12318
train acc:  0.7578125
train loss:  0.478035569190979
train gradient:  0.12245512762561366
iteration : 12319
train acc:  0.6171875
train loss:  0.6197408437728882
train gradient:  0.1788641321642644
iteration : 12320
train acc:  0.75
train loss:  0.5296683311462402
train gradient:  0.14945735129015983
iteration : 12321
train acc:  0.7421875
train loss:  0.5242847204208374
train gradient:  0.15505976063919596
iteration : 12322
train acc:  0.71875
train loss:  0.49134397506713867
train gradient:  0.13913692126468846
iteration : 12323
train acc:  0.7890625
train loss:  0.4514455199241638
train gradient:  0.19120550371108286
iteration : 12324
train acc:  0.8046875
train loss:  0.41647574305534363
train gradient:  0.09858068492658867
iteration : 12325
train acc:  0.734375
train loss:  0.5094090700149536
train gradient:  0.27050321834955343
iteration : 12326
train acc:  0.75
train loss:  0.4942586123943329
train gradient:  0.11650602883545598
iteration : 12327
train acc:  0.7734375
train loss:  0.5243169069290161
train gradient:  0.14082170268758257
iteration : 12328
train acc:  0.6796875
train loss:  0.582072377204895
train gradient:  0.16237042680892136
iteration : 12329
train acc:  0.75
train loss:  0.45930784940719604
train gradient:  0.11238215464943592
iteration : 12330
train acc:  0.703125
train loss:  0.4954770505428314
train gradient:  0.11274055963790944
iteration : 12331
train acc:  0.78125
train loss:  0.47276338934898376
train gradient:  0.12927543589312213
iteration : 12332
train acc:  0.8203125
train loss:  0.4151155948638916
train gradient:  0.09009100794176918
iteration : 12333
train acc:  0.7890625
train loss:  0.4379415512084961
train gradient:  0.14141907613442858
iteration : 12334
train acc:  0.7734375
train loss:  0.4750083088874817
train gradient:  0.09108205507038732
iteration : 12335
train acc:  0.7890625
train loss:  0.4875611960887909
train gradient:  0.11287237711864885
iteration : 12336
train acc:  0.78125
train loss:  0.45354127883911133
train gradient:  0.08531273714945746
iteration : 12337
train acc:  0.7890625
train loss:  0.4663862884044647
train gradient:  0.12845735510940115
iteration : 12338
train acc:  0.7578125
train loss:  0.47417956590652466
train gradient:  0.09803500736311499
iteration : 12339
train acc:  0.6875
train loss:  0.5373034477233887
train gradient:  0.13658053260182973
iteration : 12340
train acc:  0.734375
train loss:  0.5123383402824402
train gradient:  0.12438542142285788
iteration : 12341
train acc:  0.7421875
train loss:  0.6030963659286499
train gradient:  0.17055599237313993
iteration : 12342
train acc:  0.7734375
train loss:  0.44832614064216614
train gradient:  0.11727457739224627
iteration : 12343
train acc:  0.78125
train loss:  0.49214327335357666
train gradient:  0.17409798689616157
iteration : 12344
train acc:  0.75
train loss:  0.4696458578109741
train gradient:  0.14909510537529097
iteration : 12345
train acc:  0.75
train loss:  0.5091876983642578
train gradient:  0.10872243904871308
iteration : 12346
train acc:  0.7109375
train loss:  0.5890644788742065
train gradient:  0.18090097364490054
iteration : 12347
train acc:  0.78125
train loss:  0.43753430247306824
train gradient:  0.0953605159961033
iteration : 12348
train acc:  0.65625
train loss:  0.6283596754074097
train gradient:  0.211208898346525
iteration : 12349
train acc:  0.7265625
train loss:  0.5367426872253418
train gradient:  0.1575794001883859
iteration : 12350
train acc:  0.796875
train loss:  0.42058995366096497
train gradient:  0.09769034508606021
iteration : 12351
train acc:  0.765625
train loss:  0.47113728523254395
train gradient:  0.10794734779888467
iteration : 12352
train acc:  0.765625
train loss:  0.4374479055404663
train gradient:  0.1170934121680495
iteration : 12353
train acc:  0.765625
train loss:  0.5125167369842529
train gradient:  0.1398669838959243
iteration : 12354
train acc:  0.796875
train loss:  0.5079424381256104
train gradient:  0.11839532731530635
iteration : 12355
train acc:  0.75
train loss:  0.47089284658432007
train gradient:  0.10645338360577496
iteration : 12356
train acc:  0.734375
train loss:  0.48939645290374756
train gradient:  0.12361274844115969
iteration : 12357
train acc:  0.7265625
train loss:  0.500314474105835
train gradient:  0.13029584997414206
iteration : 12358
train acc:  0.7265625
train loss:  0.5168403387069702
train gradient:  0.1819639431464521
iteration : 12359
train acc:  0.7265625
train loss:  0.5125492811203003
train gradient:  0.11691010716336848
iteration : 12360
train acc:  0.6640625
train loss:  0.6030075550079346
train gradient:  0.16971896013560514
iteration : 12361
train acc:  0.71875
train loss:  0.5435591340065002
train gradient:  0.15153053068020145
iteration : 12362
train acc:  0.7890625
train loss:  0.42139753699302673
train gradient:  0.10342040232102262
iteration : 12363
train acc:  0.7421875
train loss:  0.4638866186141968
train gradient:  0.11702764451638159
iteration : 12364
train acc:  0.734375
train loss:  0.5670987963676453
train gradient:  0.13975509226713856
iteration : 12365
train acc:  0.6796875
train loss:  0.5287065505981445
train gradient:  0.14901176286471188
iteration : 12366
train acc:  0.7890625
train loss:  0.4398875832557678
train gradient:  0.09879897693735303
iteration : 12367
train acc:  0.65625
train loss:  0.5807006359100342
train gradient:  0.18018329058290447
iteration : 12368
train acc:  0.7421875
train loss:  0.4900640845298767
train gradient:  0.11038916468444476
iteration : 12369
train acc:  0.7734375
train loss:  0.47774356603622437
train gradient:  0.11810853327110989
iteration : 12370
train acc:  0.71875
train loss:  0.5158919095993042
train gradient:  0.13690941352451033
iteration : 12371
train acc:  0.6171875
train loss:  0.5715891122817993
train gradient:  0.18373212879671505
iteration : 12372
train acc:  0.734375
train loss:  0.5304332971572876
train gradient:  0.09480826111606408
iteration : 12373
train acc:  0.859375
train loss:  0.39499005675315857
train gradient:  0.10508574982724299
iteration : 12374
train acc:  0.765625
train loss:  0.4731007218360901
train gradient:  0.10022545636080758
iteration : 12375
train acc:  0.75
train loss:  0.4898565113544464
train gradient:  0.12506090461311925
iteration : 12376
train acc:  0.765625
train loss:  0.4730832576751709
train gradient:  0.1204474001167585
iteration : 12377
train acc:  0.7734375
train loss:  0.48994535207748413
train gradient:  0.12342337237339958
iteration : 12378
train acc:  0.7109375
train loss:  0.486476331949234
train gradient:  0.1262400163613245
iteration : 12379
train acc:  0.75
train loss:  0.5318868160247803
train gradient:  0.143800760275475
iteration : 12380
train acc:  0.796875
train loss:  0.42286133766174316
train gradient:  0.09308767252644183
iteration : 12381
train acc:  0.7265625
train loss:  0.5031139254570007
train gradient:  0.14723122806183556
iteration : 12382
train acc:  0.765625
train loss:  0.4917318820953369
train gradient:  0.15535588755392132
iteration : 12383
train acc:  0.65625
train loss:  0.6101664304733276
train gradient:  0.2159172340941287
iteration : 12384
train acc:  0.75
train loss:  0.5032631754875183
train gradient:  0.13844600675562665
iteration : 12385
train acc:  0.8046875
train loss:  0.45642250776290894
train gradient:  0.11010517631382945
iteration : 12386
train acc:  0.7578125
train loss:  0.4660506844520569
train gradient:  0.10894513995498623
iteration : 12387
train acc:  0.7421875
train loss:  0.5172230005264282
train gradient:  0.15377655912927674
iteration : 12388
train acc:  0.796875
train loss:  0.48969870805740356
train gradient:  0.12350532047269758
iteration : 12389
train acc:  0.703125
train loss:  0.44573578238487244
train gradient:  0.1085994532802832
iteration : 12390
train acc:  0.7734375
train loss:  0.4825039505958557
train gradient:  0.12083261676872117
iteration : 12391
train acc:  0.6796875
train loss:  0.5412189960479736
train gradient:  0.14098709485362262
iteration : 12392
train acc:  0.765625
train loss:  0.45446744561195374
train gradient:  0.10451432262950973
iteration : 12393
train acc:  0.703125
train loss:  0.5925471186637878
train gradient:  0.17849010145714306
iteration : 12394
train acc:  0.6796875
train loss:  0.5297918319702148
train gradient:  0.14475806313504852
iteration : 12395
train acc:  0.765625
train loss:  0.5149677991867065
train gradient:  0.1657415810432416
iteration : 12396
train acc:  0.796875
train loss:  0.42414867877960205
train gradient:  0.09559206649502795
iteration : 12397
train acc:  0.7109375
train loss:  0.4941844642162323
train gradient:  0.11980864609426015
iteration : 12398
train acc:  0.765625
train loss:  0.5149505734443665
train gradient:  0.14076259788097112
iteration : 12399
train acc:  0.8046875
train loss:  0.43770137429237366
train gradient:  0.08967142018590614
iteration : 12400
train acc:  0.8046875
train loss:  0.4004765748977661
train gradient:  0.07988847758541012
iteration : 12401
train acc:  0.71875
train loss:  0.5693696737289429
train gradient:  0.1590511212729279
iteration : 12402
train acc:  0.6875
train loss:  0.5025362372398376
train gradient:  0.11258567321151164
iteration : 12403
train acc:  0.828125
train loss:  0.39776191115379333
train gradient:  0.09141189105823393
iteration : 12404
train acc:  0.796875
train loss:  0.43481969833374023
train gradient:  0.08967639540721634
iteration : 12405
train acc:  0.796875
train loss:  0.43229007720947266
train gradient:  0.14712852577742594
iteration : 12406
train acc:  0.7421875
train loss:  0.48528456687927246
train gradient:  0.13785457865964224
iteration : 12407
train acc:  0.7578125
train loss:  0.47808289527893066
train gradient:  0.12792807685118524
iteration : 12408
train acc:  0.765625
train loss:  0.4731653928756714
train gradient:  0.1084726684394083
iteration : 12409
train acc:  0.6640625
train loss:  0.5512502193450928
train gradient:  0.17510839798796396
iteration : 12410
train acc:  0.7421875
train loss:  0.5051644444465637
train gradient:  0.11620597319640741
iteration : 12411
train acc:  0.7109375
train loss:  0.5429731011390686
train gradient:  0.14827045065682243
iteration : 12412
train acc:  0.75
train loss:  0.5189166069030762
train gradient:  0.13546811488806118
iteration : 12413
train acc:  0.7734375
train loss:  0.479465126991272
train gradient:  0.10606350346872212
iteration : 12414
train acc:  0.7734375
train loss:  0.45811644196510315
train gradient:  0.08978413582309933
iteration : 12415
train acc:  0.7109375
train loss:  0.5308734774589539
train gradient:  0.11838394201136927
iteration : 12416
train acc:  0.7265625
train loss:  0.5161075592041016
train gradient:  0.15886924278790288
iteration : 12417
train acc:  0.65625
train loss:  0.5758445262908936
train gradient:  0.15487011866659633
iteration : 12418
train acc:  0.75
train loss:  0.5415185689926147
train gradient:  0.1351267070841698
iteration : 12419
train acc:  0.734375
train loss:  0.5038947463035583
train gradient:  0.12885371703271647
iteration : 12420
train acc:  0.7421875
train loss:  0.4796740710735321
train gradient:  0.10450552404144962
iteration : 12421
train acc:  0.671875
train loss:  0.5560765266418457
train gradient:  0.18022983209359766
iteration : 12422
train acc:  0.71875
train loss:  0.5227263569831848
train gradient:  0.12410341299847874
iteration : 12423
train acc:  0.7109375
train loss:  0.5055409669876099
train gradient:  0.1265050365443559
iteration : 12424
train acc:  0.8125
train loss:  0.45563027262687683
train gradient:  0.11029590284621801
iteration : 12425
train acc:  0.75
train loss:  0.5228846073150635
train gradient:  0.12901486917497806
iteration : 12426
train acc:  0.7578125
train loss:  0.4825747013092041
train gradient:  0.10201357460717954
iteration : 12427
train acc:  0.765625
train loss:  0.4786030650138855
train gradient:  0.12070435501749471
iteration : 12428
train acc:  0.71875
train loss:  0.5125504732131958
train gradient:  0.12802334897804982
iteration : 12429
train acc:  0.734375
train loss:  0.4504934251308441
train gradient:  0.10427086814756614
iteration : 12430
train acc:  0.6953125
train loss:  0.51027512550354
train gradient:  0.1273049255632528
iteration : 12431
train acc:  0.8046875
train loss:  0.4359250068664551
train gradient:  0.1026245927468828
iteration : 12432
train acc:  0.7109375
train loss:  0.5298564434051514
train gradient:  0.11241123935837309
iteration : 12433
train acc:  0.8125
train loss:  0.446233868598938
train gradient:  0.11701414446043594
iteration : 12434
train acc:  0.71875
train loss:  0.49757975339889526
train gradient:  0.12692128116105061
iteration : 12435
train acc:  0.765625
train loss:  0.44996845722198486
train gradient:  0.08856090778919296
iteration : 12436
train acc:  0.7734375
train loss:  0.4401036500930786
train gradient:  0.08926272724176613
iteration : 12437
train acc:  0.796875
train loss:  0.43987181782722473
train gradient:  0.09518815906823602
iteration : 12438
train acc:  0.671875
train loss:  0.5332800149917603
train gradient:  0.12428204242684775
iteration : 12439
train acc:  0.796875
train loss:  0.480867862701416
train gradient:  0.11713177070930621
iteration : 12440
train acc:  0.75
train loss:  0.5466570854187012
train gradient:  0.2020851311414062
iteration : 12441
train acc:  0.703125
train loss:  0.5210227370262146
train gradient:  0.15801167599926896
iteration : 12442
train acc:  0.703125
train loss:  0.5493135452270508
train gradient:  0.18584804676172517
iteration : 12443
train acc:  0.7734375
train loss:  0.47207874059677124
train gradient:  0.1338692583903525
iteration : 12444
train acc:  0.6875
train loss:  0.5438306331634521
train gradient:  0.1649761520379449
iteration : 12445
train acc:  0.7578125
train loss:  0.5409409403800964
train gradient:  0.16002406674331587
iteration : 12446
train acc:  0.7109375
train loss:  0.5054064393043518
train gradient:  0.10675731667633351
iteration : 12447
train acc:  0.609375
train loss:  0.6768555641174316
train gradient:  0.2292008309808823
iteration : 12448
train acc:  0.7578125
train loss:  0.48791298270225525
train gradient:  0.11889609060076393
iteration : 12449
train acc:  0.78125
train loss:  0.48312366008758545
train gradient:  0.1343492635556725
iteration : 12450
train acc:  0.7421875
train loss:  0.4711318910121918
train gradient:  0.12754868880284426
iteration : 12451
train acc:  0.6875
train loss:  0.5302239060401917
train gradient:  0.1611226388114697
iteration : 12452
train acc:  0.71875
train loss:  0.5291553735733032
train gradient:  0.1400068356367195
iteration : 12453
train acc:  0.7421875
train loss:  0.4836040735244751
train gradient:  0.11553730499629636
iteration : 12454
train acc:  0.71875
train loss:  0.5174940824508667
train gradient:  0.16053295092652017
iteration : 12455
train acc:  0.71875
train loss:  0.5327415466308594
train gradient:  0.13366462433943155
iteration : 12456
train acc:  0.7734375
train loss:  0.5034675002098083
train gradient:  0.13126017973203652
iteration : 12457
train acc:  0.7265625
train loss:  0.5320576429367065
train gradient:  0.16956856363776113
iteration : 12458
train acc:  0.734375
train loss:  0.5030593872070312
train gradient:  0.11953892385753272
iteration : 12459
train acc:  0.7578125
train loss:  0.48101019859313965
train gradient:  0.10888132736343309
iteration : 12460
train acc:  0.7578125
train loss:  0.4496228098869324
train gradient:  0.10622924457579798
iteration : 12461
train acc:  0.7578125
train loss:  0.4759610891342163
train gradient:  0.10299793997466056
iteration : 12462
train acc:  0.6796875
train loss:  0.5309380292892456
train gradient:  0.154645712979017
iteration : 12463
train acc:  0.8203125
train loss:  0.40978002548217773
train gradient:  0.07633387871835833
iteration : 12464
train acc:  0.7421875
train loss:  0.47422683238983154
train gradient:  0.10530426367963586
iteration : 12465
train acc:  0.75
train loss:  0.47204476594924927
train gradient:  0.1476074208066048
iteration : 12466
train acc:  0.7421875
train loss:  0.5027647018432617
train gradient:  0.14133783582062737
iteration : 12467
train acc:  0.7109375
train loss:  0.4972992539405823
train gradient:  0.17221032470069117
iteration : 12468
train acc:  0.734375
train loss:  0.5048238039016724
train gradient:  0.13623518894000708
iteration : 12469
train acc:  0.734375
train loss:  0.5225613713264465
train gradient:  0.1344433238738238
iteration : 12470
train acc:  0.703125
train loss:  0.5002896785736084
train gradient:  0.1410131016746109
iteration : 12471
train acc:  0.765625
train loss:  0.46922963857650757
train gradient:  0.14009308654317565
iteration : 12472
train acc:  0.765625
train loss:  0.46811723709106445
train gradient:  0.10556012866752
iteration : 12473
train acc:  0.8203125
train loss:  0.45877817273139954
train gradient:  0.11312743257499236
iteration : 12474
train acc:  0.734375
train loss:  0.5250403881072998
train gradient:  0.12856934444888402
iteration : 12475
train acc:  0.7734375
train loss:  0.4611404836177826
train gradient:  0.12581577867075372
iteration : 12476
train acc:  0.75
train loss:  0.46653491258621216
train gradient:  0.12721865706986352
iteration : 12477
train acc:  0.765625
train loss:  0.4664044976234436
train gradient:  0.08801535263345124
iteration : 12478
train acc:  0.7421875
train loss:  0.498803973197937
train gradient:  0.1471648888897027
iteration : 12479
train acc:  0.6953125
train loss:  0.5290546417236328
train gradient:  0.13680894762886914
iteration : 12480
train acc:  0.7265625
train loss:  0.535880446434021
train gradient:  0.1590810992133314
iteration : 12481
train acc:  0.6953125
train loss:  0.48604536056518555
train gradient:  0.1360054060246758
iteration : 12482
train acc:  0.703125
train loss:  0.5260448455810547
train gradient:  0.14615146690061204
iteration : 12483
train acc:  0.7421875
train loss:  0.48649871349334717
train gradient:  0.1300414622679295
iteration : 12484
train acc:  0.7578125
train loss:  0.4768981337547302
train gradient:  0.10677211603720727
iteration : 12485
train acc:  0.734375
train loss:  0.5328116416931152
train gradient:  0.14343211372378184
iteration : 12486
train acc:  0.6953125
train loss:  0.6021337509155273
train gradient:  0.17833835529250586
iteration : 12487
train acc:  0.8046875
train loss:  0.5056813955307007
train gradient:  0.15925285619368978
iteration : 12488
train acc:  0.7890625
train loss:  0.4264569580554962
train gradient:  0.08743146754977632
iteration : 12489
train acc:  0.765625
train loss:  0.4312424957752228
train gradient:  0.09381894094801449
iteration : 12490
train acc:  0.6640625
train loss:  0.5849895477294922
train gradient:  0.1549704045622921
iteration : 12491
train acc:  0.765625
train loss:  0.4532105028629303
train gradient:  0.09195619707002939
iteration : 12492
train acc:  0.7109375
train loss:  0.5305467844009399
train gradient:  0.19115404868554176
iteration : 12493
train acc:  0.6953125
train loss:  0.5167038440704346
train gradient:  0.1400330034444325
iteration : 12494
train acc:  0.734375
train loss:  0.5306825637817383
train gradient:  0.16015937283961118
iteration : 12495
train acc:  0.765625
train loss:  0.5110204815864563
train gradient:  0.1250670839973455
iteration : 12496
train acc:  0.78125
train loss:  0.4234941601753235
train gradient:  0.09102865711311568
iteration : 12497
train acc:  0.7578125
train loss:  0.4796907901763916
train gradient:  0.1305880657691133
iteration : 12498
train acc:  0.7421875
train loss:  0.4915074110031128
train gradient:  0.11752045237062908
iteration : 12499
train acc:  0.7265625
train loss:  0.49111366271972656
train gradient:  0.106270996804294
iteration : 12500
train acc:  0.75
train loss:  0.4851004183292389
train gradient:  0.11661412180019817
iteration : 12501
train acc:  0.6875
train loss:  0.5544285774230957
train gradient:  0.1631581089180078
iteration : 12502
train acc:  0.734375
train loss:  0.45400941371917725
train gradient:  0.13208696850396073
iteration : 12503
train acc:  0.7734375
train loss:  0.41787534952163696
train gradient:  0.10604914147518495
iteration : 12504
train acc:  0.703125
train loss:  0.5299032926559448
train gradient:  0.1177540685853342
iteration : 12505
train acc:  0.734375
train loss:  0.5105804204940796
train gradient:  0.13947591235273116
iteration : 12506
train acc:  0.7265625
train loss:  0.4929957389831543
train gradient:  0.12806270783118
iteration : 12507
train acc:  0.6953125
train loss:  0.5685403347015381
train gradient:  0.16447862625898124
iteration : 12508
train acc:  0.6953125
train loss:  0.524232029914856
train gradient:  0.14258199845707806
iteration : 12509
train acc:  0.734375
train loss:  0.4881032407283783
train gradient:  0.10331177404120064
iteration : 12510
train acc:  0.7578125
train loss:  0.4662397503852844
train gradient:  0.14162418065569732
iteration : 12511
train acc:  0.734375
train loss:  0.49236974120140076
train gradient:  0.10228050946506703
iteration : 12512
train acc:  0.71875
train loss:  0.5107685327529907
train gradient:  0.10550637592877403
iteration : 12513
train acc:  0.8046875
train loss:  0.4249875545501709
train gradient:  0.1017601980200942
iteration : 12514
train acc:  0.6953125
train loss:  0.48720020055770874
train gradient:  0.11532316174952859
iteration : 12515
train acc:  0.7265625
train loss:  0.5545169115066528
train gradient:  0.1528777217342844
iteration : 12516
train acc:  0.7421875
train loss:  0.5055743455886841
train gradient:  0.1433654553063603
iteration : 12517
train acc:  0.765625
train loss:  0.4557211995124817
train gradient:  0.1032703069210988
iteration : 12518
train acc:  0.765625
train loss:  0.4681181013584137
train gradient:  0.098366336540079
iteration : 12519
train acc:  0.78125
train loss:  0.48534947633743286
train gradient:  0.08592486741332271
iteration : 12520
train acc:  0.671875
train loss:  0.5468326807022095
train gradient:  0.15056316539811931
iteration : 12521
train acc:  0.71875
train loss:  0.4725778102874756
train gradient:  0.10298878586097597
iteration : 12522
train acc:  0.75
train loss:  0.436676949262619
train gradient:  0.10042676048401286
iteration : 12523
train acc:  0.7578125
train loss:  0.43741199374198914
train gradient:  0.10833605507980808
iteration : 12524
train acc:  0.7421875
train loss:  0.5093546509742737
train gradient:  0.13173948298649968
iteration : 12525
train acc:  0.7265625
train loss:  0.4944208562374115
train gradient:  0.12484453255010512
iteration : 12526
train acc:  0.78125
train loss:  0.4486362338066101
train gradient:  0.09856010074364918
iteration : 12527
train acc:  0.7265625
train loss:  0.5330795049667358
train gradient:  0.1409470763972166
iteration : 12528
train acc:  0.7421875
train loss:  0.5336166620254517
train gradient:  0.1754099604478946
iteration : 12529
train acc:  0.734375
train loss:  0.49397212266921997
train gradient:  0.11686886461113587
iteration : 12530
train acc:  0.765625
train loss:  0.49527955055236816
train gradient:  0.12203443308436523
iteration : 12531
train acc:  0.7421875
train loss:  0.4596918523311615
train gradient:  0.11911876962263879
iteration : 12532
train acc:  0.7578125
train loss:  0.47527819871902466
train gradient:  0.14418118221756868
iteration : 12533
train acc:  0.78125
train loss:  0.45599818229675293
train gradient:  0.10217809889403008
iteration : 12534
train acc:  0.6953125
train loss:  0.5424907207489014
train gradient:  0.14156778709876525
iteration : 12535
train acc:  0.75
train loss:  0.4902285933494568
train gradient:  0.14242075754276579
iteration : 12536
train acc:  0.734375
train loss:  0.5116675496101379
train gradient:  0.15536224910036556
iteration : 12537
train acc:  0.7421875
train loss:  0.5019978284835815
train gradient:  0.13334594409380593
iteration : 12538
train acc:  0.734375
train loss:  0.48065465688705444
train gradient:  0.10576712045501031
iteration : 12539
train acc:  0.765625
train loss:  0.464513897895813
train gradient:  0.09415117538457762
iteration : 12540
train acc:  0.7421875
train loss:  0.4875730872154236
train gradient:  0.12977749035106456
iteration : 12541
train acc:  0.7265625
train loss:  0.5223588347434998
train gradient:  0.13351033022024814
iteration : 12542
train acc:  0.7421875
train loss:  0.4688210189342499
train gradient:  0.10021758957085639
iteration : 12543
train acc:  0.7265625
train loss:  0.47654950618743896
train gradient:  0.11948023267003353
iteration : 12544
train acc:  0.78125
train loss:  0.452879935503006
train gradient:  0.10833361531719848
iteration : 12545
train acc:  0.6875
train loss:  0.5352093577384949
train gradient:  0.14748076813436356
iteration : 12546
train acc:  0.7421875
train loss:  0.5747585296630859
train gradient:  0.22369596431522792
iteration : 12547
train acc:  0.7578125
train loss:  0.4445236325263977
train gradient:  0.1063563121079201
iteration : 12548
train acc:  0.7890625
train loss:  0.43200042843818665
train gradient:  0.0888972383631731
iteration : 12549
train acc:  0.6953125
train loss:  0.5113009214401245
train gradient:  0.12635375366146723
iteration : 12550
train acc:  0.7265625
train loss:  0.4894478917121887
train gradient:  0.10676056256179678
iteration : 12551
train acc:  0.7421875
train loss:  0.4854583740234375
train gradient:  0.11916537145209269
iteration : 12552
train acc:  0.8046875
train loss:  0.4345337748527527
train gradient:  0.09588586144070574
iteration : 12553
train acc:  0.7421875
train loss:  0.5210423469543457
train gradient:  0.12930928959916835
iteration : 12554
train acc:  0.7734375
train loss:  0.4947909116744995
train gradient:  0.1361447767528396
iteration : 12555
train acc:  0.75
train loss:  0.47935375571250916
train gradient:  0.1116216193722909
iteration : 12556
train acc:  0.734375
train loss:  0.516925573348999
train gradient:  0.12622879955910843
iteration : 12557
train acc:  0.78125
train loss:  0.4710155129432678
train gradient:  0.10497939930440987
iteration : 12558
train acc:  0.765625
train loss:  0.4084702134132385
train gradient:  0.10413279853397378
iteration : 12559
train acc:  0.703125
train loss:  0.553795337677002
train gradient:  0.17092943829793877
iteration : 12560
train acc:  0.765625
train loss:  0.4957199990749359
train gradient:  0.1399415649130876
iteration : 12561
train acc:  0.765625
train loss:  0.4848775863647461
train gradient:  0.10359833112030359
iteration : 12562
train acc:  0.7578125
train loss:  0.46664461493492126
train gradient:  0.09925000069796663
iteration : 12563
train acc:  0.7890625
train loss:  0.4310959577560425
train gradient:  0.07998326524687839
iteration : 12564
train acc:  0.734375
train loss:  0.46293455362319946
train gradient:  0.0925887900722143
iteration : 12565
train acc:  0.7421875
train loss:  0.4990547001361847
train gradient:  0.1164311591505359
iteration : 12566
train acc:  0.671875
train loss:  0.5173101425170898
train gradient:  0.1433208554460566
iteration : 12567
train acc:  0.796875
train loss:  0.4541233777999878
train gradient:  0.10847071899950797
iteration : 12568
train acc:  0.8046875
train loss:  0.4583856463432312
train gradient:  0.10139504268734673
iteration : 12569
train acc:  0.75
train loss:  0.5052541494369507
train gradient:  0.1132554329854935
iteration : 12570
train acc:  0.78125
train loss:  0.4550463557243347
train gradient:  0.10057098493486397
iteration : 12571
train acc:  0.703125
train loss:  0.49887537956237793
train gradient:  0.14060852981603478
iteration : 12572
train acc:  0.7109375
train loss:  0.5242674946784973
train gradient:  0.16712959642123143
iteration : 12573
train acc:  0.71875
train loss:  0.4818122982978821
train gradient:  0.12529454774145687
iteration : 12574
train acc:  0.828125
train loss:  0.4275946617126465
train gradient:  0.10375979443948655
iteration : 12575
train acc:  0.7421875
train loss:  0.5227473974227905
train gradient:  0.1185247783399884
iteration : 12576
train acc:  0.71875
train loss:  0.5142226219177246
train gradient:  0.11672922190714159
iteration : 12577
train acc:  0.7734375
train loss:  0.4773269593715668
train gradient:  0.1381407295996071
iteration : 12578
train acc:  0.7734375
train loss:  0.4556397795677185
train gradient:  0.10012043801190029
iteration : 12579
train acc:  0.78125
train loss:  0.4993346333503723
train gradient:  0.12279931613068962
iteration : 12580
train acc:  0.6953125
train loss:  0.5480293035507202
train gradient:  0.13887677633042145
iteration : 12581
train acc:  0.78125
train loss:  0.5244957208633423
train gradient:  0.12038290673064656
iteration : 12582
train acc:  0.703125
train loss:  0.4789261519908905
train gradient:  0.1338206943866594
iteration : 12583
train acc:  0.765625
train loss:  0.4388517141342163
train gradient:  0.11982196563712612
iteration : 12584
train acc:  0.7421875
train loss:  0.47684189677238464
train gradient:  0.11189003696276924
iteration : 12585
train acc:  0.7265625
train loss:  0.4700072109699249
train gradient:  0.10893356518287636
iteration : 12586
train acc:  0.75
train loss:  0.49337533116340637
train gradient:  0.15517172189508535
iteration : 12587
train acc:  0.7734375
train loss:  0.482300341129303
train gradient:  0.12381716254006292
iteration : 12588
train acc:  0.765625
train loss:  0.5288147330284119
train gradient:  0.13267628258899988
iteration : 12589
train acc:  0.734375
train loss:  0.5034126043319702
train gradient:  0.1450234834792516
iteration : 12590
train acc:  0.71875
train loss:  0.42857062816619873
train gradient:  0.08486875804245149
iteration : 12591
train acc:  0.796875
train loss:  0.44162189960479736
train gradient:  0.08847277785286826
iteration : 12592
train acc:  0.7421875
train loss:  0.4602566361427307
train gradient:  0.09269149787000798
iteration : 12593
train acc:  0.734375
train loss:  0.5143653750419617
train gradient:  0.1259545429221626
iteration : 12594
train acc:  0.7734375
train loss:  0.45385992527008057
train gradient:  0.12346787747237127
iteration : 12595
train acc:  0.7109375
train loss:  0.5145411491394043
train gradient:  0.14956737959684252
iteration : 12596
train acc:  0.6640625
train loss:  0.636099636554718
train gradient:  0.2328116341150065
iteration : 12597
train acc:  0.6796875
train loss:  0.5083364248275757
train gradient:  0.13740860837691699
iteration : 12598
train acc:  0.6640625
train loss:  0.5494806170463562
train gradient:  0.14407354220860438
iteration : 12599
train acc:  0.703125
train loss:  0.5209701061248779
train gradient:  0.12198655934603361
iteration : 12600
train acc:  0.796875
train loss:  0.447361022233963
train gradient:  0.12025642635143234
iteration : 12601
train acc:  0.7578125
train loss:  0.44853895902633667
train gradient:  0.10103483500327301
iteration : 12602
train acc:  0.71875
train loss:  0.5107877254486084
train gradient:  0.1389342156848806
iteration : 12603
train acc:  0.7734375
train loss:  0.5441412925720215
train gradient:  0.12168973327552866
iteration : 12604
train acc:  0.7578125
train loss:  0.4450988173484802
train gradient:  0.09230621265138972
iteration : 12605
train acc:  0.7265625
train loss:  0.5423722863197327
train gradient:  0.16037630112957907
iteration : 12606
train acc:  0.71875
train loss:  0.5138895511627197
train gradient:  0.11774622703762568
iteration : 12607
train acc:  0.7421875
train loss:  0.5030279755592346
train gradient:  0.1418414257477049
iteration : 12608
train acc:  0.7421875
train loss:  0.4363012909889221
train gradient:  0.09620248110097494
iteration : 12609
train acc:  0.703125
train loss:  0.5326116681098938
train gradient:  0.13821918711765124
iteration : 12610
train acc:  0.7890625
train loss:  0.4507274031639099
train gradient:  0.08642032414734929
iteration : 12611
train acc:  0.703125
train loss:  0.5348780751228333
train gradient:  0.14410015646264096
iteration : 12612
train acc:  0.7578125
train loss:  0.48424190282821655
train gradient:  0.11843783443750096
iteration : 12613
train acc:  0.6953125
train loss:  0.4723268449306488
train gradient:  0.09485766466052899
iteration : 12614
train acc:  0.7734375
train loss:  0.5058412551879883
train gradient:  0.1295976988894772
iteration : 12615
train acc:  0.7421875
train loss:  0.4766431748867035
train gradient:  0.10023948733475693
iteration : 12616
train acc:  0.8359375
train loss:  0.3805100917816162
train gradient:  0.08036503677154473
iteration : 12617
train acc:  0.75
train loss:  0.47335559129714966
train gradient:  0.11436137841397065
iteration : 12618
train acc:  0.7265625
train loss:  0.512134313583374
train gradient:  0.13515094181360876
iteration : 12619
train acc:  0.828125
train loss:  0.4436296820640564
train gradient:  0.10738465063264982
iteration : 12620
train acc:  0.6953125
train loss:  0.5584992170333862
train gradient:  0.16565468255147547
iteration : 12621
train acc:  0.7109375
train loss:  0.47459256649017334
train gradient:  0.13051769580202824
iteration : 12622
train acc:  0.71875
train loss:  0.5074751973152161
train gradient:  0.15049607058851983
iteration : 12623
train acc:  0.6953125
train loss:  0.5360681414604187
train gradient:  0.1292225490695142
iteration : 12624
train acc:  0.6953125
train loss:  0.5023666620254517
train gradient:  0.11650953900557139
iteration : 12625
train acc:  0.75
train loss:  0.4762895107269287
train gradient:  0.11646503657881116
iteration : 12626
train acc:  0.734375
train loss:  0.5007716417312622
train gradient:  0.12488738753539157
iteration : 12627
train acc:  0.7421875
train loss:  0.5000556111335754
train gradient:  0.1114039324573981
iteration : 12628
train acc:  0.71875
train loss:  0.49911314249038696
train gradient:  0.15341084682457906
iteration : 12629
train acc:  0.7421875
train loss:  0.4698108732700348
train gradient:  0.11975099389632345
iteration : 12630
train acc:  0.734375
train loss:  0.4619867205619812
train gradient:  0.10403266816419451
iteration : 12631
train acc:  0.703125
train loss:  0.5148400068283081
train gradient:  0.17807402568854339
iteration : 12632
train acc:  0.671875
train loss:  0.5385117530822754
train gradient:  0.14247343582359023
iteration : 12633
train acc:  0.75
train loss:  0.5537205934524536
train gradient:  0.14205702635404294
iteration : 12634
train acc:  0.7890625
train loss:  0.4535536468029022
train gradient:  0.10788767081445057
iteration : 12635
train acc:  0.796875
train loss:  0.4933846592903137
train gradient:  0.1560088303695261
iteration : 12636
train acc:  0.6953125
train loss:  0.521488606929779
train gradient:  0.13989572781788584
iteration : 12637
train acc:  0.7734375
train loss:  0.44145363569259644
train gradient:  0.09611452294274246
iteration : 12638
train acc:  0.6796875
train loss:  0.504684329032898
train gradient:  0.12700884004416796
iteration : 12639
train acc:  0.765625
train loss:  0.5060752034187317
train gradient:  0.15127334725909536
iteration : 12640
train acc:  0.75
train loss:  0.4839332699775696
train gradient:  0.10058469888350866
iteration : 12641
train acc:  0.7421875
train loss:  0.5350861549377441
train gradient:  0.1890078486935568
iteration : 12642
train acc:  0.7265625
train loss:  0.5567953586578369
train gradient:  0.15290999742751965
iteration : 12643
train acc:  0.71875
train loss:  0.4884628653526306
train gradient:  0.134783465758274
iteration : 12644
train acc:  0.765625
train loss:  0.4779647886753082
train gradient:  0.0965299320908507
iteration : 12645
train acc:  0.734375
train loss:  0.5384329557418823
train gradient:  0.16320660078077448
iteration : 12646
train acc:  0.734375
train loss:  0.4949340224266052
train gradient:  0.11768484327159019
iteration : 12647
train acc:  0.75
train loss:  0.4754965901374817
train gradient:  0.09553071214561576
iteration : 12648
train acc:  0.7890625
train loss:  0.4843330681324005
train gradient:  0.116294728054423
iteration : 12649
train acc:  0.734375
train loss:  0.43508243560791016
train gradient:  0.11777190744060853
iteration : 12650
train acc:  0.75
train loss:  0.5308228731155396
train gradient:  0.1346094889617707
iteration : 12651
train acc:  0.703125
train loss:  0.5214798450469971
train gradient:  0.12056227106184451
iteration : 12652
train acc:  0.71875
train loss:  0.4942474067211151
train gradient:  0.12206496190267291
iteration : 12653
train acc:  0.75
train loss:  0.5142430663108826
train gradient:  0.11910815316506529
iteration : 12654
train acc:  0.8203125
train loss:  0.47617632150650024
train gradient:  0.11957646044720131
iteration : 12655
train acc:  0.7890625
train loss:  0.45361050963401794
train gradient:  0.10178811153608353
iteration : 12656
train acc:  0.765625
train loss:  0.5125112533569336
train gradient:  0.11250672330224105
iteration : 12657
train acc:  0.8125
train loss:  0.42311349511146545
train gradient:  0.10904836532802811
iteration : 12658
train acc:  0.703125
train loss:  0.5464693307876587
train gradient:  0.14778865705206948
iteration : 12659
train acc:  0.7890625
train loss:  0.4522269666194916
train gradient:  0.10134799458499416
iteration : 12660
train acc:  0.71875
train loss:  0.5039584636688232
train gradient:  0.13424672158700418
iteration : 12661
train acc:  0.71875
train loss:  0.5328713655471802
train gradient:  0.25175313708424163
iteration : 12662
train acc:  0.7265625
train loss:  0.4837952256202698
train gradient:  0.12679705889095905
iteration : 12663
train acc:  0.7890625
train loss:  0.4646388292312622
train gradient:  0.14717704894210604
iteration : 12664
train acc:  0.7578125
train loss:  0.4734864830970764
train gradient:  0.11471948327887123
iteration : 12665
train acc:  0.765625
train loss:  0.5154540538787842
train gradient:  0.11165979216421501
iteration : 12666
train acc:  0.7890625
train loss:  0.4052894413471222
train gradient:  0.07460216939620475
iteration : 12667
train acc:  0.796875
train loss:  0.44070571660995483
train gradient:  0.09308002990403037
iteration : 12668
train acc:  0.7421875
train loss:  0.490019291639328
train gradient:  0.11110982810963256
iteration : 12669
train acc:  0.7265625
train loss:  0.5369244813919067
train gradient:  0.14243762638678997
iteration : 12670
train acc:  0.703125
train loss:  0.49124598503112793
train gradient:  0.1266375755063025
iteration : 12671
train acc:  0.7578125
train loss:  0.4528621435165405
train gradient:  0.10520648123518118
iteration : 12672
train acc:  0.7265625
train loss:  0.5009647011756897
train gradient:  0.10898416903514208
iteration : 12673
train acc:  0.7421875
train loss:  0.48567071557044983
train gradient:  0.10936474735772224
iteration : 12674
train acc:  0.6640625
train loss:  0.5699059963226318
train gradient:  0.14262182679057223
iteration : 12675
train acc:  0.7421875
train loss:  0.5144850015640259
train gradient:  0.1255329989142771
iteration : 12676
train acc:  0.6953125
train loss:  0.522868812084198
train gradient:  0.1926819737085732
iteration : 12677
train acc:  0.765625
train loss:  0.49301671981811523
train gradient:  0.11433628264867361
iteration : 12678
train acc:  0.7578125
train loss:  0.4649046063423157
train gradient:  0.09591893621831545
iteration : 12679
train acc:  0.71875
train loss:  0.5151576399803162
train gradient:  0.16187236170811664
iteration : 12680
train acc:  0.78125
train loss:  0.4874356985092163
train gradient:  0.17015696897349047
iteration : 12681
train acc:  0.734375
train loss:  0.49204275012016296
train gradient:  0.11950867156141687
iteration : 12682
train acc:  0.796875
train loss:  0.43543505668640137
train gradient:  0.09185287973562174
iteration : 12683
train acc:  0.7734375
train loss:  0.45575249195098877
train gradient:  0.09833129515768818
iteration : 12684
train acc:  0.703125
train loss:  0.520940899848938
train gradient:  0.1363526913323047
iteration : 12685
train acc:  0.765625
train loss:  0.45387890934944153
train gradient:  0.1144031870616587
iteration : 12686
train acc:  0.734375
train loss:  0.5109237432479858
train gradient:  0.12167493917243223
iteration : 12687
train acc:  0.734375
train loss:  0.4793989658355713
train gradient:  0.13023130740278938
iteration : 12688
train acc:  0.78125
train loss:  0.42447757720947266
train gradient:  0.15256390929527758
iteration : 12689
train acc:  0.75
train loss:  0.4814760088920593
train gradient:  0.11563373624491345
iteration : 12690
train acc:  0.7265625
train loss:  0.5051391124725342
train gradient:  0.16506794930005592
iteration : 12691
train acc:  0.6875
train loss:  0.5767683982849121
train gradient:  0.14525319958714963
iteration : 12692
train acc:  0.703125
train loss:  0.5241860151290894
train gradient:  0.17223757186935715
iteration : 12693
train acc:  0.734375
train loss:  0.49463599920272827
train gradient:  0.13630685737296216
iteration : 12694
train acc:  0.7109375
train loss:  0.5265197157859802
train gradient:  0.13331119588643187
iteration : 12695
train acc:  0.6640625
train loss:  0.5603724718093872
train gradient:  0.179638159558555
iteration : 12696
train acc:  0.7890625
train loss:  0.4546688199043274
train gradient:  0.11125601074563349
iteration : 12697
train acc:  0.7578125
train loss:  0.5490928292274475
train gradient:  0.1425820493737801
iteration : 12698
train acc:  0.71875
train loss:  0.5134038925170898
train gradient:  0.14909778716672034
iteration : 12699
train acc:  0.6953125
train loss:  0.4850195348262787
train gradient:  0.14457733954238836
iteration : 12700
train acc:  0.8203125
train loss:  0.44395869970321655
train gradient:  0.1012090481299278
iteration : 12701
train acc:  0.7265625
train loss:  0.4656209349632263
train gradient:  0.12874597997411796
iteration : 12702
train acc:  0.7421875
train loss:  0.508641242980957
train gradient:  0.13494288744202038
iteration : 12703
train acc:  0.796875
train loss:  0.48186978697776794
train gradient:  0.11228580545767711
iteration : 12704
train acc:  0.7265625
train loss:  0.4801640510559082
train gradient:  0.12298495444276135
iteration : 12705
train acc:  0.7578125
train loss:  0.5179685354232788
train gradient:  0.12958459806894967
iteration : 12706
train acc:  0.765625
train loss:  0.46538352966308594
train gradient:  0.11612506542609198
iteration : 12707
train acc:  0.703125
train loss:  0.5305415391921997
train gradient:  0.12716147958176718
iteration : 12708
train acc:  0.7734375
train loss:  0.46017172932624817
train gradient:  0.10667712934201504
iteration : 12709
train acc:  0.765625
train loss:  0.4612277150154114
train gradient:  0.10070163580200173
iteration : 12710
train acc:  0.734375
train loss:  0.48411887884140015
train gradient:  0.12799184972396052
iteration : 12711
train acc:  0.7265625
train loss:  0.5153452157974243
train gradient:  0.12679125695170054
iteration : 12712
train acc:  0.7109375
train loss:  0.5174441337585449
train gradient:  0.1275832518642152
iteration : 12713
train acc:  0.7265625
train loss:  0.478743314743042
train gradient:  0.14702112137011025
iteration : 12714
train acc:  0.75
train loss:  0.4741640090942383
train gradient:  0.1080869908538482
iteration : 12715
train acc:  0.75
train loss:  0.5196743011474609
train gradient:  0.1791531844052217
iteration : 12716
train acc:  0.734375
train loss:  0.46459949016571045
train gradient:  0.0961571731082554
iteration : 12717
train acc:  0.75
train loss:  0.4862860441207886
train gradient:  0.13947208979649928
iteration : 12718
train acc:  0.6640625
train loss:  0.5876919031143188
train gradient:  0.1814269909386121
iteration : 12719
train acc:  0.796875
train loss:  0.443187415599823
train gradient:  0.1152145031911909
iteration : 12720
train acc:  0.75
train loss:  0.4737091660499573
train gradient:  0.11674987911836075
iteration : 12721
train acc:  0.6953125
train loss:  0.5440464019775391
train gradient:  0.14054957423902914
iteration : 12722
train acc:  0.7890625
train loss:  0.42964881658554077
train gradient:  0.11620276695448584
iteration : 12723
train acc:  0.71875
train loss:  0.49719008803367615
train gradient:  0.15776193562549573
iteration : 12724
train acc:  0.6875
train loss:  0.5813984274864197
train gradient:  0.1843661693036392
iteration : 12725
train acc:  0.7578125
train loss:  0.44709593057632446
train gradient:  0.10193741414133951
iteration : 12726
train acc:  0.7578125
train loss:  0.5146372318267822
train gradient:  0.13444896714000396
iteration : 12727
train acc:  0.78125
train loss:  0.4410048723220825
train gradient:  0.13119217286856089
iteration : 12728
train acc:  0.78125
train loss:  0.44591060280799866
train gradient:  0.11029729227514216
iteration : 12729
train acc:  0.6953125
train loss:  0.5205304622650146
train gradient:  0.12429526532620473
iteration : 12730
train acc:  0.75
train loss:  0.47301092743873596
train gradient:  0.12849808378568917
iteration : 12731
train acc:  0.765625
train loss:  0.464211106300354
train gradient:  0.10647176680177348
iteration : 12732
train acc:  0.8125
train loss:  0.3885963559150696
train gradient:  0.08195923672822192
iteration : 12733
train acc:  0.703125
train loss:  0.5107875466346741
train gradient:  0.11723634123779582
iteration : 12734
train acc:  0.7421875
train loss:  0.5038619041442871
train gradient:  0.14510802313079516
iteration : 12735
train acc:  0.765625
train loss:  0.45321977138519287
train gradient:  0.10709054631768562
iteration : 12736
train acc:  0.703125
train loss:  0.5016579627990723
train gradient:  0.1333540753249836
iteration : 12737
train acc:  0.734375
train loss:  0.530666708946228
train gradient:  0.12347691153326074
iteration : 12738
train acc:  0.78125
train loss:  0.4216533303260803
train gradient:  0.10460993761568606
iteration : 12739
train acc:  0.71875
train loss:  0.537499189376831
train gradient:  0.1627913420066572
iteration : 12740
train acc:  0.8125
train loss:  0.4281514883041382
train gradient:  0.10710300751303793
iteration : 12741
train acc:  0.703125
train loss:  0.5389957427978516
train gradient:  0.19080414851334826
iteration : 12742
train acc:  0.7578125
train loss:  0.4472311735153198
train gradient:  0.10711344304528987
iteration : 12743
train acc:  0.7734375
train loss:  0.40194427967071533
train gradient:  0.07530762586306693
iteration : 12744
train acc:  0.734375
train loss:  0.48756924271583557
train gradient:  0.13410850849678652
iteration : 12745
train acc:  0.8125
train loss:  0.4348471760749817
train gradient:  0.10630385743769839
iteration : 12746
train acc:  0.640625
train loss:  0.6074270606040955
train gradient:  0.16665459913849034
iteration : 12747
train acc:  0.8203125
train loss:  0.46195274591445923
train gradient:  0.11537108159192544
iteration : 12748
train acc:  0.7578125
train loss:  0.4822649359703064
train gradient:  0.11222209000016267
iteration : 12749
train acc:  0.7890625
train loss:  0.44068294763565063
train gradient:  0.10914073491614462
iteration : 12750
train acc:  0.7265625
train loss:  0.5201492309570312
train gradient:  0.1349392875154759
iteration : 12751
train acc:  0.7421875
train loss:  0.48218441009521484
train gradient:  0.11512063621634484
iteration : 12752
train acc:  0.7734375
train loss:  0.48125550150871277
train gradient:  0.1076424499019773
iteration : 12753
train acc:  0.7734375
train loss:  0.46654924750328064
train gradient:  0.10780240358607271
iteration : 12754
train acc:  0.71875
train loss:  0.556535005569458
train gradient:  0.1425902513868851
iteration : 12755
train acc:  0.765625
train loss:  0.49634796380996704
train gradient:  0.10503097597155514
iteration : 12756
train acc:  0.7109375
train loss:  0.5128278732299805
train gradient:  0.12877173880309925
iteration : 12757
train acc:  0.7890625
train loss:  0.4395962357521057
train gradient:  0.10708032163798506
iteration : 12758
train acc:  0.734375
train loss:  0.5166220664978027
train gradient:  0.13385509914329002
iteration : 12759
train acc:  0.765625
train loss:  0.47962430119514465
train gradient:  0.13258693057729248
iteration : 12760
train acc:  0.7890625
train loss:  0.4447247385978699
train gradient:  0.11409268333177122
iteration : 12761
train acc:  0.7265625
train loss:  0.5075428485870361
train gradient:  0.1320054057568385
iteration : 12762
train acc:  0.7890625
train loss:  0.4133741557598114
train gradient:  0.0921484501761427
iteration : 12763
train acc:  0.703125
train loss:  0.5631395578384399
train gradient:  0.14562741980144223
iteration : 12764
train acc:  0.6796875
train loss:  0.5918569564819336
train gradient:  0.1490627074854189
iteration : 12765
train acc:  0.7265625
train loss:  0.47682851552963257
train gradient:  0.10707127619743166
iteration : 12766
train acc:  0.7734375
train loss:  0.5204556584358215
train gradient:  0.15783649837161356
iteration : 12767
train acc:  0.765625
train loss:  0.5601555109024048
train gradient:  0.18286749683906905
iteration : 12768
train acc:  0.7265625
train loss:  0.5697400569915771
train gradient:  0.18011957331046322
iteration : 12769
train acc:  0.7421875
train loss:  0.48446208238601685
train gradient:  0.1050229217931441
iteration : 12770
train acc:  0.7734375
train loss:  0.4852728247642517
train gradient:  0.12345948545313594
iteration : 12771
train acc:  0.78125
train loss:  0.4685920476913452
train gradient:  0.09328337982656053
iteration : 12772
train acc:  0.7421875
train loss:  0.45657792687416077
train gradient:  0.13089945612423876
iteration : 12773
train acc:  0.7265625
train loss:  0.506404459476471
train gradient:  0.13135330663931089
iteration : 12774
train acc:  0.7421875
train loss:  0.45667558908462524
train gradient:  0.10976398351369979
iteration : 12775
train acc:  0.7421875
train loss:  0.5215301513671875
train gradient:  0.14695119906710358
iteration : 12776
train acc:  0.7421875
train loss:  0.47487425804138184
train gradient:  0.12014375752556279
iteration : 12777
train acc:  0.7109375
train loss:  0.5337575674057007
train gradient:  0.1496373835026415
iteration : 12778
train acc:  0.71875
train loss:  0.5297192931175232
train gradient:  0.13009749109400098
iteration : 12779
train acc:  0.75
train loss:  0.48362088203430176
train gradient:  0.10589699201937383
iteration : 12780
train acc:  0.734375
train loss:  0.4645785689353943
train gradient:  0.08603045499933344
iteration : 12781
train acc:  0.75
train loss:  0.46208322048187256
train gradient:  0.11451750589474405
iteration : 12782
train acc:  0.7578125
train loss:  0.5153135061264038
train gradient:  0.14720521157100577
iteration : 12783
train acc:  0.7265625
train loss:  0.6056056022644043
train gradient:  0.1347893397626101
iteration : 12784
train acc:  0.6875
train loss:  0.5950940251350403
train gradient:  0.1918623627523046
iteration : 12785
train acc:  0.7734375
train loss:  0.4542006850242615
train gradient:  0.11004429921075043
iteration : 12786
train acc:  0.75
train loss:  0.4486236572265625
train gradient:  0.09141514186870121
iteration : 12787
train acc:  0.7265625
train loss:  0.4832550585269928
train gradient:  0.13327021976435893
iteration : 12788
train acc:  0.7265625
train loss:  0.5209746360778809
train gradient:  0.16776572979153145
iteration : 12789
train acc:  0.7421875
train loss:  0.45715683698654175
train gradient:  0.15945918086111566
iteration : 12790
train acc:  0.703125
train loss:  0.5049864649772644
train gradient:  0.14305653632102447
iteration : 12791
train acc:  0.75
train loss:  0.5277735590934753
train gradient:  0.11550523536500103
iteration : 12792
train acc:  0.71875
train loss:  0.5206544399261475
train gradient:  0.1249915980081103
iteration : 12793
train acc:  0.75
train loss:  0.5467406511306763
train gradient:  0.13537317763814782
iteration : 12794
train acc:  0.734375
train loss:  0.5180888175964355
train gradient:  0.13522700548317623
iteration : 12795
train acc:  0.6875
train loss:  0.5783658027648926
train gradient:  0.13416007528341362
iteration : 12796
train acc:  0.765625
train loss:  0.46314477920532227
train gradient:  0.09915065858931306
iteration : 12797
train acc:  0.7421875
train loss:  0.4674511253833771
train gradient:  0.12099793800011907
iteration : 12798
train acc:  0.7109375
train loss:  0.5339964628219604
train gradient:  0.1374754308603529
iteration : 12799
train acc:  0.765625
train loss:  0.46258246898651123
train gradient:  0.10018103308109706
iteration : 12800
train acc:  0.703125
train loss:  0.5590967535972595
train gradient:  0.12854939620952366
iteration : 12801
train acc:  0.7421875
train loss:  0.5288084149360657
train gradient:  0.13029092229314365
iteration : 12802
train acc:  0.734375
train loss:  0.4663487672805786
train gradient:  0.10456440625149842
iteration : 12803
train acc:  0.765625
train loss:  0.4282848536968231
train gradient:  0.08882467139580273
iteration : 12804
train acc:  0.78125
train loss:  0.48211053013801575
train gradient:  0.10637575479489661
iteration : 12805
train acc:  0.78125
train loss:  0.5149117708206177
train gradient:  0.13811864412968228
iteration : 12806
train acc:  0.7265625
train loss:  0.45098257064819336
train gradient:  0.10585056962928192
iteration : 12807
train acc:  0.8125
train loss:  0.4347187280654907
train gradient:  0.11159685334013958
iteration : 12808
train acc:  0.7734375
train loss:  0.4701690077781677
train gradient:  0.10557665470200789
iteration : 12809
train acc:  0.78125
train loss:  0.4489486813545227
train gradient:  0.1239308717512109
iteration : 12810
train acc:  0.78125
train loss:  0.4788311719894409
train gradient:  0.14206064691679235
iteration : 12811
train acc:  0.8359375
train loss:  0.5014503598213196
train gradient:  0.14715198336637053
iteration : 12812
train acc:  0.7578125
train loss:  0.47986263036727905
train gradient:  0.10873882249471588
iteration : 12813
train acc:  0.765625
train loss:  0.4770658612251282
train gradient:  0.12166790705275354
iteration : 12814
train acc:  0.828125
train loss:  0.39064788818359375
train gradient:  0.07696289997878414
iteration : 12815
train acc:  0.796875
train loss:  0.49339377880096436
train gradient:  0.12978939356725416
iteration : 12816
train acc:  0.75
train loss:  0.4974178075790405
train gradient:  0.14239488573584164
iteration : 12817
train acc:  0.7421875
train loss:  0.5133823752403259
train gradient:  0.13252468317124022
iteration : 12818
train acc:  0.7265625
train loss:  0.5241787433624268
train gradient:  0.13952617060737427
iteration : 12819
train acc:  0.78125
train loss:  0.4934519827365875
train gradient:  0.13019121560065788
iteration : 12820
train acc:  0.65625
train loss:  0.5648627877235413
train gradient:  0.14955307071714474
iteration : 12821
train acc:  0.6953125
train loss:  0.563941240310669
train gradient:  0.2056817377786031
iteration : 12822
train acc:  0.6875
train loss:  0.5102146863937378
train gradient:  0.13695432637510258
iteration : 12823
train acc:  0.765625
train loss:  0.44517040252685547
train gradient:  0.11503339997178519
iteration : 12824
train acc:  0.7421875
train loss:  0.522584855556488
train gradient:  0.13686400163140328
iteration : 12825
train acc:  0.6796875
train loss:  0.6269351243972778
train gradient:  0.20151479911143289
iteration : 12826
train acc:  0.765625
train loss:  0.48652026057243347
train gradient:  0.13726053757949358
iteration : 12827
train acc:  0.765625
train loss:  0.434746652841568
train gradient:  0.11636806301718844
iteration : 12828
train acc:  0.6875
train loss:  0.5229934453964233
train gradient:  0.13190905919113016
iteration : 12829
train acc:  0.765625
train loss:  0.44288402795791626
train gradient:  0.1054745739418864
iteration : 12830
train acc:  0.7109375
train loss:  0.5501455068588257
train gradient:  0.16124474981561787
iteration : 12831
train acc:  0.7109375
train loss:  0.4922507107257843
train gradient:  0.12196920306521053
iteration : 12832
train acc:  0.7578125
train loss:  0.4800192713737488
train gradient:  0.115669570251714
iteration : 12833
train acc:  0.7734375
train loss:  0.43745940923690796
train gradient:  0.0905333628163068
iteration : 12834
train acc:  0.765625
train loss:  0.49121901392936707
train gradient:  0.12747238938910327
iteration : 12835
train acc:  0.828125
train loss:  0.43040186166763306
train gradient:  0.11328025563338955
iteration : 12836
train acc:  0.7578125
train loss:  0.5087106823921204
train gradient:  0.12700702884632153
iteration : 12837
train acc:  0.7734375
train loss:  0.4515746831893921
train gradient:  0.12917820775751146
iteration : 12838
train acc:  0.7890625
train loss:  0.4751605987548828
train gradient:  0.13348191857828567
iteration : 12839
train acc:  0.6875
train loss:  0.5776849985122681
train gradient:  0.14503436509686518
iteration : 12840
train acc:  0.7578125
train loss:  0.47736024856567383
train gradient:  0.13234468646563885
iteration : 12841
train acc:  0.71875
train loss:  0.49432963132858276
train gradient:  0.14143681237059597
iteration : 12842
train acc:  0.703125
train loss:  0.5047543048858643
train gradient:  0.15011048678437156
iteration : 12843
train acc:  0.671875
train loss:  0.5656981468200684
train gradient:  0.1952408446671488
iteration : 12844
train acc:  0.75
train loss:  0.488614559173584
train gradient:  0.10653099415541693
iteration : 12845
train acc:  0.78125
train loss:  0.4518710970878601
train gradient:  0.1261189470023795
iteration : 12846
train acc:  0.6953125
train loss:  0.5485281944274902
train gradient:  0.18195700194037495
iteration : 12847
train acc:  0.7109375
train loss:  0.5208302140235901
train gradient:  0.11407814678125719
iteration : 12848
train acc:  0.78125
train loss:  0.4152098596096039
train gradient:  0.09885456492882129
iteration : 12849
train acc:  0.703125
train loss:  0.5235991477966309
train gradient:  0.170964326680124
iteration : 12850
train acc:  0.7265625
train loss:  0.4991222620010376
train gradient:  0.12129422538817211
iteration : 12851
train acc:  0.7578125
train loss:  0.47245144844055176
train gradient:  0.13395842204429748
iteration : 12852
train acc:  0.7890625
train loss:  0.44477471709251404
train gradient:  0.10973994834926999
iteration : 12853
train acc:  0.7421875
train loss:  0.47827115654945374
train gradient:  0.12922617785580287
iteration : 12854
train acc:  0.7890625
train loss:  0.48755162954330444
train gradient:  0.10694095038283219
iteration : 12855
train acc:  0.7578125
train loss:  0.4725213348865509
train gradient:  0.12276847681674136
iteration : 12856
train acc:  0.7890625
train loss:  0.48705437779426575
train gradient:  0.12888215764384212
iteration : 12857
train acc:  0.7421875
train loss:  0.48338621854782104
train gradient:  0.11268057945237825
iteration : 12858
train acc:  0.734375
train loss:  0.4883180856704712
train gradient:  0.12130638593293967
iteration : 12859
train acc:  0.7890625
train loss:  0.4412333369255066
train gradient:  0.10027510094707778
iteration : 12860
train acc:  0.703125
train loss:  0.5278260111808777
train gradient:  0.17613236883826372
iteration : 12861
train acc:  0.7890625
train loss:  0.45711082220077515
train gradient:  0.10961020974624403
iteration : 12862
train acc:  0.6875
train loss:  0.5670555233955383
train gradient:  0.16515811351844067
iteration : 12863
train acc:  0.6875
train loss:  0.4938608705997467
train gradient:  0.13367926365565833
iteration : 12864
train acc:  0.7578125
train loss:  0.5141959190368652
train gradient:  0.16211857399760365
iteration : 12865
train acc:  0.7890625
train loss:  0.4435569643974304
train gradient:  0.09350408993277434
iteration : 12866
train acc:  0.765625
train loss:  0.4888656735420227
train gradient:  0.10809270622724172
iteration : 12867
train acc:  0.7578125
train loss:  0.4479334354400635
train gradient:  0.12745177439204303
iteration : 12868
train acc:  0.7265625
train loss:  0.5122584700584412
train gradient:  0.13391966422099588
iteration : 12869
train acc:  0.7421875
train loss:  0.5593130588531494
train gradient:  0.14491947701144936
iteration : 12870
train acc:  0.6953125
train loss:  0.5338079929351807
train gradient:  0.18250438832735194
iteration : 12871
train acc:  0.734375
train loss:  0.502790093421936
train gradient:  0.11427546872849853
iteration : 12872
train acc:  0.6796875
train loss:  0.535224437713623
train gradient:  0.1458476880392819
iteration : 12873
train acc:  0.703125
train loss:  0.5212162733078003
train gradient:  0.13253759935432674
iteration : 12874
train acc:  0.734375
train loss:  0.4325712323188782
train gradient:  0.11211146077646489
iteration : 12875
train acc:  0.7890625
train loss:  0.48996901512145996
train gradient:  0.13397138653691532
iteration : 12876
train acc:  0.7109375
train loss:  0.49608737230300903
train gradient:  0.12781912575971996
iteration : 12877
train acc:  0.7421875
train loss:  0.4985395669937134
train gradient:  0.11260920338954605
iteration : 12878
train acc:  0.7734375
train loss:  0.47105762362480164
train gradient:  0.08917289830129597
iteration : 12879
train acc:  0.8046875
train loss:  0.4402746856212616
train gradient:  0.12703524424334328
iteration : 12880
train acc:  0.7265625
train loss:  0.5157681107521057
train gradient:  0.14146910961595027
iteration : 12881
train acc:  0.7421875
train loss:  0.5032174587249756
train gradient:  0.16383081424717982
iteration : 12882
train acc:  0.7265625
train loss:  0.5282939672470093
train gradient:  0.13023326636170177
iteration : 12883
train acc:  0.703125
train loss:  0.5220041275024414
train gradient:  0.14048829308243377
iteration : 12884
train acc:  0.71875
train loss:  0.49097317457199097
train gradient:  0.142699403080904
iteration : 12885
train acc:  0.7421875
train loss:  0.49651846289634705
train gradient:  0.13333782977988912
iteration : 12886
train acc:  0.7578125
train loss:  0.4615439176559448
train gradient:  0.09727465632269787
iteration : 12887
train acc:  0.703125
train loss:  0.5203533172607422
train gradient:  0.20383363433369228
iteration : 12888
train acc:  0.734375
train loss:  0.47216275334358215
train gradient:  0.12071190238434713
iteration : 12889
train acc:  0.78125
train loss:  0.4816179871559143
train gradient:  0.14093679011184146
iteration : 12890
train acc:  0.7734375
train loss:  0.4617270827293396
train gradient:  0.1030675452226522
iteration : 12891
train acc:  0.6875
train loss:  0.5122546553611755
train gradient:  0.1377049915918307
iteration : 12892
train acc:  0.7578125
train loss:  0.5027040243148804
train gradient:  0.13109771908132367
iteration : 12893
train acc:  0.765625
train loss:  0.4525391757488251
train gradient:  0.10981698780890943
iteration : 12894
train acc:  0.75
train loss:  0.458693265914917
train gradient:  0.10999944959364169
iteration : 12895
train acc:  0.8203125
train loss:  0.41197288036346436
train gradient:  0.09638066563346206
iteration : 12896
train acc:  0.734375
train loss:  0.5077999234199524
train gradient:  0.15481512665841546
iteration : 12897
train acc:  0.75
train loss:  0.5416167378425598
train gradient:  0.14390643927469704
iteration : 12898
train acc:  0.8125
train loss:  0.4223116338253021
train gradient:  0.08587790334608203
iteration : 12899
train acc:  0.7109375
train loss:  0.5090584754943848
train gradient:  0.1490578629469124
iteration : 12900
train acc:  0.7265625
train loss:  0.512560248374939
train gradient:  0.13907478190395517
iteration : 12901
train acc:  0.75
train loss:  0.45681503415107727
train gradient:  0.10930040805298998
iteration : 12902
train acc:  0.7421875
train loss:  0.5123171806335449
train gradient:  0.16606008563779534
iteration : 12903
train acc:  0.71875
train loss:  0.4989577829837799
train gradient:  0.1377175346637596
iteration : 12904
train acc:  0.7265625
train loss:  0.5228371620178223
train gradient:  0.1621177723330844
iteration : 12905
train acc:  0.671875
train loss:  0.5645749568939209
train gradient:  0.1712870760179045
iteration : 12906
train acc:  0.7265625
train loss:  0.47460848093032837
train gradient:  0.09557489587682144
iteration : 12907
train acc:  0.7109375
train loss:  0.5149162411689758
train gradient:  0.16201209740277295
iteration : 12908
train acc:  0.6953125
train loss:  0.45700135827064514
train gradient:  0.09783271927348806
iteration : 12909
train acc:  0.7734375
train loss:  0.46164244413375854
train gradient:  0.11261690450107653
iteration : 12910
train acc:  0.796875
train loss:  0.477675199508667
train gradient:  0.12657791387129258
iteration : 12911
train acc:  0.7578125
train loss:  0.4616510570049286
train gradient:  0.10025479078741462
iteration : 12912
train acc:  0.7421875
train loss:  0.5037457942962646
train gradient:  0.11361648394140013
iteration : 12913
train acc:  0.71875
train loss:  0.47708427906036377
train gradient:  0.09863641088783598
iteration : 12914
train acc:  0.7578125
train loss:  0.4621375799179077
train gradient:  0.10592498671226921
iteration : 12915
train acc:  0.7421875
train loss:  0.45891934633255005
train gradient:  0.11626902920498201
iteration : 12916
train acc:  0.7265625
train loss:  0.5212967991828918
train gradient:  0.138849240750961
iteration : 12917
train acc:  0.7421875
train loss:  0.5151488184928894
train gradient:  0.1549390905400656
iteration : 12918
train acc:  0.78125
train loss:  0.43490564823150635
train gradient:  0.0960068986943367
iteration : 12919
train acc:  0.75
train loss:  0.4748398959636688
train gradient:  0.10542724645624846
iteration : 12920
train acc:  0.703125
train loss:  0.5160433053970337
train gradient:  0.13518242303460254
iteration : 12921
train acc:  0.78125
train loss:  0.4526739716529846
train gradient:  0.10977520200702504
iteration : 12922
train acc:  0.734375
train loss:  0.4984912574291229
train gradient:  0.12263790874984144
iteration : 12923
train acc:  0.703125
train loss:  0.49943092465400696
train gradient:  0.1190805646261526
iteration : 12924
train acc:  0.7109375
train loss:  0.4881020784378052
train gradient:  0.11568816923959238
iteration : 12925
train acc:  0.734375
train loss:  0.5061482787132263
train gradient:  0.15062669334925477
iteration : 12926
train acc:  0.75
train loss:  0.5355706214904785
train gradient:  0.1431927250809717
iteration : 12927
train acc:  0.7578125
train loss:  0.5182533264160156
train gradient:  0.11583682656264088
iteration : 12928
train acc:  0.7734375
train loss:  0.4838002920150757
train gradient:  0.11334478685793647
iteration : 12929
train acc:  0.765625
train loss:  0.4361705183982849
train gradient:  0.09186764280298941
iteration : 12930
train acc:  0.7109375
train loss:  0.5062981247901917
train gradient:  0.13988611928676334
iteration : 12931
train acc:  0.765625
train loss:  0.4604485034942627
train gradient:  0.1010953191525846
iteration : 12932
train acc:  0.7265625
train loss:  0.5414635539054871
train gradient:  0.17297152249036424
iteration : 12933
train acc:  0.6875
train loss:  0.5285722017288208
train gradient:  0.15323252846597574
iteration : 12934
train acc:  0.7578125
train loss:  0.4616418480873108
train gradient:  0.10109158195691959
iteration : 12935
train acc:  0.7421875
train loss:  0.5165956616401672
train gradient:  0.1321015086727792
iteration : 12936
train acc:  0.7578125
train loss:  0.5171787738800049
train gradient:  0.14262897646998693
iteration : 12937
train acc:  0.7734375
train loss:  0.47180598974227905
train gradient:  0.10437326986201337
iteration : 12938
train acc:  0.6875
train loss:  0.5262714624404907
train gradient:  0.1339762438791302
iteration : 12939
train acc:  0.796875
train loss:  0.40416190028190613
train gradient:  0.09461268270165625
iteration : 12940
train acc:  0.7109375
train loss:  0.5385668277740479
train gradient:  0.18541010774613642
iteration : 12941
train acc:  0.6640625
train loss:  0.5316004157066345
train gradient:  0.1292724926908314
iteration : 12942
train acc:  0.7421875
train loss:  0.5128201246261597
train gradient:  0.2044983508806893
iteration : 12943
train acc:  0.75
train loss:  0.490392804145813
train gradient:  0.11043914136758694
iteration : 12944
train acc:  0.65625
train loss:  0.5594726800918579
train gradient:  0.1733107132056358
iteration : 12945
train acc:  0.8359375
train loss:  0.43027976155281067
train gradient:  0.10215560187248719
iteration : 12946
train acc:  0.7890625
train loss:  0.4727838635444641
train gradient:  0.11290072632269803
iteration : 12947
train acc:  0.796875
train loss:  0.46909767389297485
train gradient:  0.0811855284605514
iteration : 12948
train acc:  0.7421875
train loss:  0.48164114356040955
train gradient:  0.14097075743040266
iteration : 12949
train acc:  0.71875
train loss:  0.4879764914512634
train gradient:  0.11557772132873441
iteration : 12950
train acc:  0.8203125
train loss:  0.4322611391544342
train gradient:  0.09412949454359537
iteration : 12951
train acc:  0.71875
train loss:  0.5019707679748535
train gradient:  0.12502314341261384
iteration : 12952
train acc:  0.78125
train loss:  0.41757339239120483
train gradient:  0.07397081942948415
iteration : 12953
train acc:  0.796875
train loss:  0.4670277237892151
train gradient:  0.11282751797075562
iteration : 12954
train acc:  0.8203125
train loss:  0.4121571183204651
train gradient:  0.09934268086587958
iteration : 12955
train acc:  0.765625
train loss:  0.4686495065689087
train gradient:  0.12673221699561804
iteration : 12956
train acc:  0.78125
train loss:  0.45964086055755615
train gradient:  0.12378978119168436
iteration : 12957
train acc:  0.6875
train loss:  0.4976757764816284
train gradient:  0.14958836518708385
iteration : 12958
train acc:  0.765625
train loss:  0.4500979483127594
train gradient:  0.12828618178247253
iteration : 12959
train acc:  0.8125
train loss:  0.4431489408016205
train gradient:  0.11691629946727317
iteration : 12960
train acc:  0.6640625
train loss:  0.5621157884597778
train gradient:  0.18129039895466975
iteration : 12961
train acc:  0.734375
train loss:  0.48871368169784546
train gradient:  0.12872262090829237
iteration : 12962
train acc:  0.765625
train loss:  0.45582419633865356
train gradient:  0.09633060749227237
iteration : 12963
train acc:  0.7421875
train loss:  0.4705551266670227
train gradient:  0.12830010639196338
iteration : 12964
train acc:  0.71875
train loss:  0.49436813592910767
train gradient:  0.14029507813437692
iteration : 12965
train acc:  0.7421875
train loss:  0.515112042427063
train gradient:  0.14027633524947297
iteration : 12966
train acc:  0.734375
train loss:  0.558590292930603
train gradient:  0.17930532635179414
iteration : 12967
train acc:  0.765625
train loss:  0.4826723337173462
train gradient:  0.13572506697522596
iteration : 12968
train acc:  0.765625
train loss:  0.47728896141052246
train gradient:  0.1429549018074812
iteration : 12969
train acc:  0.734375
train loss:  0.5225660800933838
train gradient:  0.14876888158974846
iteration : 12970
train acc:  0.7265625
train loss:  0.5492676496505737
train gradient:  0.1855852752944971
iteration : 12971
train acc:  0.7109375
train loss:  0.508224368095398
train gradient:  0.13816212440305045
iteration : 12972
train acc:  0.6796875
train loss:  0.5070526599884033
train gradient:  0.11392948029205663
iteration : 12973
train acc:  0.6328125
train loss:  0.597511887550354
train gradient:  0.17177705947485966
iteration : 12974
train acc:  0.6953125
train loss:  0.4870741665363312
train gradient:  0.1199614478416338
iteration : 12975
train acc:  0.78125
train loss:  0.47968608140945435
train gradient:  0.12534933980562113
iteration : 12976
train acc:  0.7578125
train loss:  0.43604806065559387
train gradient:  0.09016659247678104
iteration : 12977
train acc:  0.7890625
train loss:  0.47182750701904297
train gradient:  0.12040946422414806
iteration : 12978
train acc:  0.7109375
train loss:  0.5408529043197632
train gradient:  0.13566769636833434
iteration : 12979
train acc:  0.6953125
train loss:  0.5925325751304626
train gradient:  0.1825806828398786
iteration : 12980
train acc:  0.7421875
train loss:  0.4935779273509979
train gradient:  0.12364520121152331
iteration : 12981
train acc:  0.7734375
train loss:  0.42537635564804077
train gradient:  0.0943619551274286
iteration : 12982
train acc:  0.734375
train loss:  0.5051549673080444
train gradient:  0.1482528346980057
iteration : 12983
train acc:  0.75
train loss:  0.5078047513961792
train gradient:  0.13523842956466775
iteration : 12984
train acc:  0.71875
train loss:  0.49020183086395264
train gradient:  0.12130988446358923
iteration : 12985
train acc:  0.7265625
train loss:  0.4967362880706787
train gradient:  0.19420672975564618
iteration : 12986
train acc:  0.75
train loss:  0.4747026264667511
train gradient:  0.10158545030034609
iteration : 12987
train acc:  0.75
train loss:  0.4475351572036743
train gradient:  0.11893723809375056
iteration : 12988
train acc:  0.7265625
train loss:  0.4793326258659363
train gradient:  0.109635654078753
iteration : 12989
train acc:  0.7109375
train loss:  0.4615408182144165
train gradient:  0.10328923425279976
iteration : 12990
train acc:  0.7421875
train loss:  0.4751247763633728
train gradient:  0.09768561912476781
iteration : 12991
train acc:  0.765625
train loss:  0.4648975431919098
train gradient:  0.11403920844950906
iteration : 12992
train acc:  0.7734375
train loss:  0.48104214668273926
train gradient:  0.1473129293212113
iteration : 12993
train acc:  0.7265625
train loss:  0.4788845479488373
train gradient:  0.10480818800201068
iteration : 12994
train acc:  0.78125
train loss:  0.4046229124069214
train gradient:  0.10645076686271031
iteration : 12995
train acc:  0.71875
train loss:  0.5314077734947205
train gradient:  0.14106307341891594
iteration : 12996
train acc:  0.734375
train loss:  0.504012942314148
train gradient:  0.1397940006771934
iteration : 12997
train acc:  0.78125
train loss:  0.5030310750007629
train gradient:  0.1841287336819969
iteration : 12998
train acc:  0.71875
train loss:  0.5082976818084717
train gradient:  0.13813035691582534
iteration : 12999
train acc:  0.734375
train loss:  0.5002167224884033
train gradient:  0.12655638458119606
iteration : 13000
train acc:  0.671875
train loss:  0.5513467788696289
train gradient:  0.13344833657809357
iteration : 13001
train acc:  0.75
train loss:  0.5052911639213562
train gradient:  0.14032795939364864
iteration : 13002
train acc:  0.8125
train loss:  0.4245315194129944
train gradient:  0.11361340384142594
iteration : 13003
train acc:  0.765625
train loss:  0.5259056687355042
train gradient:  0.17890806086678313
iteration : 13004
train acc:  0.765625
train loss:  0.5036773681640625
train gradient:  0.13928128649345728
iteration : 13005
train acc:  0.78125
train loss:  0.4534216523170471
train gradient:  0.13622349934279315
iteration : 13006
train acc:  0.7265625
train loss:  0.5259491801261902
train gradient:  0.143711730377092
iteration : 13007
train acc:  0.796875
train loss:  0.4799480438232422
train gradient:  0.09950624715960606
iteration : 13008
train acc:  0.7890625
train loss:  0.46939849853515625
train gradient:  0.1271056679001813
iteration : 13009
train acc:  0.7890625
train loss:  0.4353201389312744
train gradient:  0.09363537120970324
iteration : 13010
train acc:  0.6796875
train loss:  0.5554244518280029
train gradient:  0.12666603441477733
iteration : 13011
train acc:  0.71875
train loss:  0.4596762955188751
train gradient:  0.09719872289963484
iteration : 13012
train acc:  0.65625
train loss:  0.6033467054367065
train gradient:  0.2412837961339524
iteration : 13013
train acc:  0.7421875
train loss:  0.4716700613498688
train gradient:  0.10595438262116048
iteration : 13014
train acc:  0.75
train loss:  0.5196477174758911
train gradient:  0.13720067615429674
iteration : 13015
train acc:  0.7265625
train loss:  0.486216276884079
train gradient:  0.0937190818500886
iteration : 13016
train acc:  0.7421875
train loss:  0.4896109998226166
train gradient:  0.12081550879868415
iteration : 13017
train acc:  0.6484375
train loss:  0.5911878347396851
train gradient:  0.15452015527679164
iteration : 13018
train acc:  0.7734375
train loss:  0.4641020596027374
train gradient:  0.10489190898792992
iteration : 13019
train acc:  0.7734375
train loss:  0.5055447220802307
train gradient:  0.11929930201265983
iteration : 13020
train acc:  0.6328125
train loss:  0.5524776577949524
train gradient:  0.19672437894777203
iteration : 13021
train acc:  0.78125
train loss:  0.4245271682739258
train gradient:  0.09891823047255968
iteration : 13022
train acc:  0.75
train loss:  0.4642786383628845
train gradient:  0.11067598805538838
iteration : 13023
train acc:  0.671875
train loss:  0.5163311958312988
train gradient:  0.12052537195841678
iteration : 13024
train acc:  0.7421875
train loss:  0.474973201751709
train gradient:  0.10862412246715429
iteration : 13025
train acc:  0.765625
train loss:  0.531062662601471
train gradient:  0.15154735512183595
iteration : 13026
train acc:  0.7265625
train loss:  0.4835391044616699
train gradient:  0.10699888301722732
iteration : 13027
train acc:  0.7265625
train loss:  0.5119246244430542
train gradient:  0.11796696745497172
iteration : 13028
train acc:  0.7734375
train loss:  0.4439321756362915
train gradient:  0.1577945596239578
iteration : 13029
train acc:  0.703125
train loss:  0.5361447930335999
train gradient:  0.1513248666649183
iteration : 13030
train acc:  0.7734375
train loss:  0.4684576988220215
train gradient:  0.1348451881001635
iteration : 13031
train acc:  0.7421875
train loss:  0.46050500869750977
train gradient:  0.10194591528906448
iteration : 13032
train acc:  0.7890625
train loss:  0.432235985994339
train gradient:  0.10042839572715975
iteration : 13033
train acc:  0.703125
train loss:  0.508170485496521
train gradient:  0.159389342718199
iteration : 13034
train acc:  0.7578125
train loss:  0.4619596600532532
train gradient:  0.10344917706169854
iteration : 13035
train acc:  0.7890625
train loss:  0.47873905301094055
train gradient:  0.1414780320145655
iteration : 13036
train acc:  0.7734375
train loss:  0.4535752236843109
train gradient:  0.10370424455418385
iteration : 13037
train acc:  0.7734375
train loss:  0.4518589377403259
train gradient:  0.09420305726663851
iteration : 13038
train acc:  0.796875
train loss:  0.4720161557197571
train gradient:  0.09936063476317278
iteration : 13039
train acc:  0.7578125
train loss:  0.48385560512542725
train gradient:  0.11189798464795331
iteration : 13040
train acc:  0.6875
train loss:  0.5288751125335693
train gradient:  0.1342870423154749
iteration : 13041
train acc:  0.7265625
train loss:  0.5558699369430542
train gradient:  0.13016702721317752
iteration : 13042
train acc:  0.796875
train loss:  0.49389714002609253
train gradient:  0.13886766337400175
iteration : 13043
train acc:  0.796875
train loss:  0.44903627038002014
train gradient:  0.08006247858672748
iteration : 13044
train acc:  0.71875
train loss:  0.5545386075973511
train gradient:  0.1650279506973455
iteration : 13045
train acc:  0.7265625
train loss:  0.4943770170211792
train gradient:  0.13117519263775335
iteration : 13046
train acc:  0.75
train loss:  0.4621152877807617
train gradient:  0.11069579387156908
iteration : 13047
train acc:  0.7734375
train loss:  0.4440286159515381
train gradient:  0.12260214661215792
iteration : 13048
train acc:  0.7265625
train loss:  0.5304394960403442
train gradient:  0.1595680288582701
iteration : 13049
train acc:  0.765625
train loss:  0.4594295024871826
train gradient:  0.12625210106173645
iteration : 13050
train acc:  0.7265625
train loss:  0.488213449716568
train gradient:  0.10765918179168443
iteration : 13051
train acc:  0.75
train loss:  0.49591678380966187
train gradient:  0.1447479754805991
iteration : 13052
train acc:  0.6875
train loss:  0.5757265686988831
train gradient:  0.14741349323939074
iteration : 13053
train acc:  0.765625
train loss:  0.47335517406463623
train gradient:  0.12047856111400032
iteration : 13054
train acc:  0.7109375
train loss:  0.5197978019714355
train gradient:  0.1183424160177123
iteration : 13055
train acc:  0.765625
train loss:  0.48401403427124023
train gradient:  0.13368148367795257
iteration : 13056
train acc:  0.71875
train loss:  0.5050283670425415
train gradient:  0.1251524134435526
iteration : 13057
train acc:  0.7421875
train loss:  0.4684244990348816
train gradient:  0.10153280346461735
iteration : 13058
train acc:  0.765625
train loss:  0.4568476378917694
train gradient:  0.08132632741314215
iteration : 13059
train acc:  0.75
train loss:  0.4873700439929962
train gradient:  0.12216889824172815
iteration : 13060
train acc:  0.7109375
train loss:  0.5403952598571777
train gradient:  0.1331357986389784
iteration : 13061
train acc:  0.71875
train loss:  0.5596604347229004
train gradient:  0.16654532309180575
iteration : 13062
train acc:  0.703125
train loss:  0.5144368410110474
train gradient:  0.14533393016367507
iteration : 13063
train acc:  0.78125
train loss:  0.43346965312957764
train gradient:  0.09673990437965214
iteration : 13064
train acc:  0.7890625
train loss:  0.4475695490837097
train gradient:  0.1119149532621447
iteration : 13065
train acc:  0.75
train loss:  0.48556703329086304
train gradient:  0.13104890680495482
iteration : 13066
train acc:  0.796875
train loss:  0.4520779848098755
train gradient:  0.10111099384478618
iteration : 13067
train acc:  0.796875
train loss:  0.43205103278160095
train gradient:  0.09521932965109517
iteration : 13068
train acc:  0.7734375
train loss:  0.4644429683685303
train gradient:  0.10166820469764909
iteration : 13069
train acc:  0.7421875
train loss:  0.5195667743682861
train gradient:  0.14365640420279197
iteration : 13070
train acc:  0.796875
train loss:  0.4552879333496094
train gradient:  0.11635095888966976
iteration : 13071
train acc:  0.765625
train loss:  0.45405036211013794
train gradient:  0.12369962416720995
iteration : 13072
train acc:  0.71875
train loss:  0.4806887209415436
train gradient:  0.1081936226782669
iteration : 13073
train acc:  0.75
train loss:  0.5379093289375305
train gradient:  0.14622333881125926
iteration : 13074
train acc:  0.71875
train loss:  0.5085353255271912
train gradient:  0.12007264837605221
iteration : 13075
train acc:  0.859375
train loss:  0.3692084550857544
train gradient:  0.07461823607110876
iteration : 13076
train acc:  0.75
train loss:  0.48408013582229614
train gradient:  0.13307578712319812
iteration : 13077
train acc:  0.75
train loss:  0.47373634576797485
train gradient:  0.15394969876458695
iteration : 13078
train acc:  0.71875
train loss:  0.5171203017234802
train gradient:  0.1359909264587797
iteration : 13079
train acc:  0.734375
train loss:  0.533397376537323
train gradient:  0.1485842921842461
iteration : 13080
train acc:  0.7890625
train loss:  0.4509935975074768
train gradient:  0.09150716652894132
iteration : 13081
train acc:  0.6953125
train loss:  0.5349695682525635
train gradient:  0.16195410516171177
iteration : 13082
train acc:  0.7578125
train loss:  0.47366732358932495
train gradient:  0.11203738352219664
iteration : 13083
train acc:  0.6796875
train loss:  0.5228149890899658
train gradient:  0.16749550410632424
iteration : 13084
train acc:  0.6953125
train loss:  0.5063214302062988
train gradient:  0.13100863422865455
iteration : 13085
train acc:  0.75
train loss:  0.4851394593715668
train gradient:  0.11155970819721923
iteration : 13086
train acc:  0.78125
train loss:  0.43897417187690735
train gradient:  0.09042301186781139
iteration : 13087
train acc:  0.75
train loss:  0.51964271068573
train gradient:  0.15236659232563768
iteration : 13088
train acc:  0.65625
train loss:  0.5796692967414856
train gradient:  0.14653806862221505
iteration : 13089
train acc:  0.734375
train loss:  0.4531788229942322
train gradient:  0.10236560834033376
iteration : 13090
train acc:  0.78125
train loss:  0.4841959774494171
train gradient:  0.1295857698372471
iteration : 13091
train acc:  0.8359375
train loss:  0.40087786316871643
train gradient:  0.08675525577347701
iteration : 13092
train acc:  0.7265625
train loss:  0.4835757613182068
train gradient:  0.12284454914905549
iteration : 13093
train acc:  0.734375
train loss:  0.49966317415237427
train gradient:  0.14891848677525638
iteration : 13094
train acc:  0.7578125
train loss:  0.4862157702445984
train gradient:  0.13581664636259946
iteration : 13095
train acc:  0.7265625
train loss:  0.5004350543022156
train gradient:  0.15885691840588867
iteration : 13096
train acc:  0.7578125
train loss:  0.4504240155220032
train gradient:  0.1071997543362337
iteration : 13097
train acc:  0.7265625
train loss:  0.5432349443435669
train gradient:  0.17979292566425686
iteration : 13098
train acc:  0.7421875
train loss:  0.4906042218208313
train gradient:  0.11408608020950374
iteration : 13099
train acc:  0.7421875
train loss:  0.5188141465187073
train gradient:  0.16406761234665196
iteration : 13100
train acc:  0.7578125
train loss:  0.4573248028755188
train gradient:  0.10958425208175519
iteration : 13101
train acc:  0.7578125
train loss:  0.47574105858802795
train gradient:  0.11336191605940299
iteration : 13102
train acc:  0.7109375
train loss:  0.5198649168014526
train gradient:  0.13497301744679172
iteration : 13103
train acc:  0.7109375
train loss:  0.5027505159378052
train gradient:  0.13260805391389835
iteration : 13104
train acc:  0.78125
train loss:  0.4968116283416748
train gradient:  0.13155297346899603
iteration : 13105
train acc:  0.7578125
train loss:  0.46596601605415344
train gradient:  0.11609968948626348
iteration : 13106
train acc:  0.7578125
train loss:  0.4842223525047302
train gradient:  0.13292534423133037
iteration : 13107
train acc:  0.7734375
train loss:  0.46142125129699707
train gradient:  0.11005747885650222
iteration : 13108
train acc:  0.71875
train loss:  0.49801501631736755
train gradient:  0.11789336981955206
iteration : 13109
train acc:  0.7421875
train loss:  0.4911741614341736
train gradient:  0.10213906849198247
iteration : 13110
train acc:  0.7890625
train loss:  0.4266148805618286
train gradient:  0.10997674097407882
iteration : 13111
train acc:  0.6875
train loss:  0.5275835394859314
train gradient:  0.13272817995193192
iteration : 13112
train acc:  0.7734375
train loss:  0.46670496463775635
train gradient:  0.14281914552728137
iteration : 13113
train acc:  0.75
train loss:  0.506879985332489
train gradient:  0.1295826300364898
iteration : 13114
train acc:  0.765625
train loss:  0.4639524817466736
train gradient:  0.10879845639216425
iteration : 13115
train acc:  0.7578125
train loss:  0.4488169252872467
train gradient:  0.10409743124247463
iteration : 13116
train acc:  0.796875
train loss:  0.448642373085022
train gradient:  0.09985564304546478
iteration : 13117
train acc:  0.734375
train loss:  0.4655674695968628
train gradient:  0.10874095434030233
iteration : 13118
train acc:  0.7578125
train loss:  0.5027680397033691
train gradient:  0.11457505726545227
iteration : 13119
train acc:  0.75
train loss:  0.5044223666191101
train gradient:  0.13913063821038504
iteration : 13120
train acc:  0.7890625
train loss:  0.4585552215576172
train gradient:  0.13320501219812286
iteration : 13121
train acc:  0.75
train loss:  0.512939453125
train gradient:  0.13166838188750288
iteration : 13122
train acc:  0.796875
train loss:  0.4163408875465393
train gradient:  0.10612716241027836
iteration : 13123
train acc:  0.75
train loss:  0.5095528960227966
train gradient:  0.12449868388373724
iteration : 13124
train acc:  0.7109375
train loss:  0.5093175172805786
train gradient:  0.12536935176724048
iteration : 13125
train acc:  0.78125
train loss:  0.45662081241607666
train gradient:  0.1202917571496436
iteration : 13126
train acc:  0.75
train loss:  0.5299475193023682
train gradient:  0.12121818021518019
iteration : 13127
train acc:  0.765625
train loss:  0.5069278478622437
train gradient:  0.16495512351588043
iteration : 13128
train acc:  0.7578125
train loss:  0.47388190031051636
train gradient:  0.131367929855031
iteration : 13129
train acc:  0.6484375
train loss:  0.5312310457229614
train gradient:  0.14335872988827886
iteration : 13130
train acc:  0.78125
train loss:  0.41457369923591614
train gradient:  0.08752729314704527
iteration : 13131
train acc:  0.7890625
train loss:  0.4624004662036896
train gradient:  0.12826146690393853
iteration : 13132
train acc:  0.7578125
train loss:  0.4612901210784912
train gradient:  0.12157638575953429
iteration : 13133
train acc:  0.6796875
train loss:  0.5640895366668701
train gradient:  0.1543563272220223
iteration : 13134
train acc:  0.734375
train loss:  0.5127716064453125
train gradient:  0.10774356195130559
iteration : 13135
train acc:  0.7265625
train loss:  0.5675889253616333
train gradient:  0.1555718789562067
iteration : 13136
train acc:  0.734375
train loss:  0.5368952751159668
train gradient:  0.1574513919251002
iteration : 13137
train acc:  0.765625
train loss:  0.48707103729248047
train gradient:  0.09518562579669311
iteration : 13138
train acc:  0.75
train loss:  0.4632362127304077
train gradient:  0.11664453368184473
iteration : 13139
train acc:  0.7578125
train loss:  0.4668514132499695
train gradient:  0.11545801507269245
iteration : 13140
train acc:  0.7578125
train loss:  0.5019261240959167
train gradient:  0.1786010547466217
iteration : 13141
train acc:  0.7265625
train loss:  0.5351422429084778
train gradient:  0.16253094915674632
iteration : 13142
train acc:  0.7265625
train loss:  0.5064437389373779
train gradient:  0.13292175036286297
iteration : 13143
train acc:  0.7890625
train loss:  0.45164912939071655
train gradient:  0.11598722159744641
iteration : 13144
train acc:  0.7265625
train loss:  0.5262464880943298
train gradient:  0.16551931580135143
iteration : 13145
train acc:  0.796875
train loss:  0.46808862686157227
train gradient:  0.1180686936013284
iteration : 13146
train acc:  0.7265625
train loss:  0.5307480096817017
train gradient:  0.17972541332138156
iteration : 13147
train acc:  0.7109375
train loss:  0.49782466888427734
train gradient:  0.13769296968726427
iteration : 13148
train acc:  0.75
train loss:  0.4853099584579468
train gradient:  0.12140282301497579
iteration : 13149
train acc:  0.703125
train loss:  0.49138352274894714
train gradient:  0.1380710943297438
iteration : 13150
train acc:  0.7265625
train loss:  0.4898255467414856
train gradient:  0.15467360093583696
iteration : 13151
train acc:  0.734375
train loss:  0.5065774321556091
train gradient:  0.12773580639116858
iteration : 13152
train acc:  0.7421875
train loss:  0.46716275811195374
train gradient:  0.1167649146972477
iteration : 13153
train acc:  0.703125
train loss:  0.5026000738143921
train gradient:  0.11425022526595577
iteration : 13154
train acc:  0.734375
train loss:  0.424681156873703
train gradient:  0.099679965196322
iteration : 13155
train acc:  0.7109375
train loss:  0.59031081199646
train gradient:  0.13845771549590463
iteration : 13156
train acc:  0.78125
train loss:  0.4528915286064148
train gradient:  0.09720435513808745
iteration : 13157
train acc:  0.71875
train loss:  0.4976213574409485
train gradient:  0.11490221004379338
iteration : 13158
train acc:  0.765625
train loss:  0.5014071464538574
train gradient:  0.125798434145411
iteration : 13159
train acc:  0.6875
train loss:  0.5441709756851196
train gradient:  0.15504285633328002
iteration : 13160
train acc:  0.6953125
train loss:  0.5433148145675659
train gradient:  0.14892339436779367
iteration : 13161
train acc:  0.734375
train loss:  0.5203336477279663
train gradient:  0.13831807952529954
iteration : 13162
train acc:  0.8125
train loss:  0.4354313611984253
train gradient:  0.1072651143458596
iteration : 13163
train acc:  0.7890625
train loss:  0.4497823417186737
train gradient:  0.10326955411214732
iteration : 13164
train acc:  0.7734375
train loss:  0.466640830039978
train gradient:  0.13943645638610186
iteration : 13165
train acc:  0.7578125
train loss:  0.5421949625015259
train gradient:  0.1456204008269295
iteration : 13166
train acc:  0.78125
train loss:  0.4440498352050781
train gradient:  0.11067074396227856
iteration : 13167
train acc:  0.671875
train loss:  0.5384625196456909
train gradient:  0.1577916301741183
iteration : 13168
train acc:  0.796875
train loss:  0.4265095293521881
train gradient:  0.1257038623049418
iteration : 13169
train acc:  0.7578125
train loss:  0.5015622973442078
train gradient:  0.12672690403203038
iteration : 13170
train acc:  0.765625
train loss:  0.4591277241706848
train gradient:  0.11865565627218946
iteration : 13171
train acc:  0.7265625
train loss:  0.485444039106369
train gradient:  0.10971069524284095
iteration : 13172
train acc:  0.7265625
train loss:  0.4995368719100952
train gradient:  0.12242521440317705
iteration : 13173
train acc:  0.7578125
train loss:  0.47199350595474243
train gradient:  0.12097757239653481
iteration : 13174
train acc:  0.7109375
train loss:  0.5409393310546875
train gradient:  0.12361909413180969
iteration : 13175
train acc:  0.765625
train loss:  0.4687979817390442
train gradient:  0.12039443769278176
iteration : 13176
train acc:  0.7265625
train loss:  0.5101269483566284
train gradient:  0.10962698972943308
iteration : 13177
train acc:  0.734375
train loss:  0.48974597454071045
train gradient:  0.12824496367220312
iteration : 13178
train acc:  0.65625
train loss:  0.5282630920410156
train gradient:  0.18105903047078947
iteration : 13179
train acc:  0.765625
train loss:  0.49765846133232117
train gradient:  0.12544880689891866
iteration : 13180
train acc:  0.7734375
train loss:  0.4446735978126526
train gradient:  0.12091033718252335
iteration : 13181
train acc:  0.7578125
train loss:  0.46299800276756287
train gradient:  0.07828608712010142
iteration : 13182
train acc:  0.8046875
train loss:  0.40710508823394775
train gradient:  0.10578258241463559
iteration : 13183
train acc:  0.796875
train loss:  0.44790756702423096
train gradient:  0.0918917081998307
iteration : 13184
train acc:  0.78125
train loss:  0.5075049996376038
train gradient:  0.17038221002495138
iteration : 13185
train acc:  0.78125
train loss:  0.424692839384079
train gradient:  0.1132632367412059
iteration : 13186
train acc:  0.7578125
train loss:  0.48041388392448425
train gradient:  0.10382598666423502
iteration : 13187
train acc:  0.7265625
train loss:  0.5511354207992554
train gradient:  0.13764543889490927
iteration : 13188
train acc:  0.7890625
train loss:  0.41174793243408203
train gradient:  0.09388345975505578
iteration : 13189
train acc:  0.75
train loss:  0.4707221984863281
train gradient:  0.11463394874106907
iteration : 13190
train acc:  0.7890625
train loss:  0.44514477252960205
train gradient:  0.115937631719177
iteration : 13191
train acc:  0.7109375
train loss:  0.4881044030189514
train gradient:  0.11034353210092385
iteration : 13192
train acc:  0.7109375
train loss:  0.5100707411766052
train gradient:  0.16547035373235247
iteration : 13193
train acc:  0.7890625
train loss:  0.45955991744995117
train gradient:  0.12247881144936759
iteration : 13194
train acc:  0.796875
train loss:  0.4268372058868408
train gradient:  0.08907947914633293
iteration : 13195
train acc:  0.765625
train loss:  0.45983952283859253
train gradient:  0.11264100469669289
iteration : 13196
train acc:  0.796875
train loss:  0.45335328578948975
train gradient:  0.11520964900979724
iteration : 13197
train acc:  0.78125
train loss:  0.47237709164619446
train gradient:  0.11866753281586709
iteration : 13198
train acc:  0.7109375
train loss:  0.5198172926902771
train gradient:  0.19857805292716293
iteration : 13199
train acc:  0.7109375
train loss:  0.5206883549690247
train gradient:  0.12348912642023023
iteration : 13200
train acc:  0.7421875
train loss:  0.4674530029296875
train gradient:  0.11063907087889623
iteration : 13201
train acc:  0.7265625
train loss:  0.4992098808288574
train gradient:  0.16318324399934003
iteration : 13202
train acc:  0.7109375
train loss:  0.5253610610961914
train gradient:  0.16984057459282154
iteration : 13203
train acc:  0.7578125
train loss:  0.4679347574710846
train gradient:  0.11266531877467192
iteration : 13204
train acc:  0.7578125
train loss:  0.47284677624702454
train gradient:  0.1218742562612642
iteration : 13205
train acc:  0.71875
train loss:  0.55894935131073
train gradient:  0.1713493230050529
iteration : 13206
train acc:  0.78125
train loss:  0.477512389421463
train gradient:  0.11369921098456094
iteration : 13207
train acc:  0.7109375
train loss:  0.5497514009475708
train gradient:  0.12647559671453476
iteration : 13208
train acc:  0.6875
train loss:  0.5163519382476807
train gradient:  0.1223229385178985
iteration : 13209
train acc:  0.7421875
train loss:  0.5100222826004028
train gradient:  0.2011287417842355
iteration : 13210
train acc:  0.78125
train loss:  0.4432176947593689
train gradient:  0.1049839585762285
iteration : 13211
train acc:  0.7890625
train loss:  0.44750964641571045
train gradient:  0.10903074933292932
iteration : 13212
train acc:  0.71875
train loss:  0.5420476794242859
train gradient:  0.15389772027755078
iteration : 13213
train acc:  0.7265625
train loss:  0.5563247203826904
train gradient:  0.17656538008194866
iteration : 13214
train acc:  0.65625
train loss:  0.5345731973648071
train gradient:  0.13998636975795414
iteration : 13215
train acc:  0.8203125
train loss:  0.4038858413696289
train gradient:  0.08796416170361811
iteration : 13216
train acc:  0.78125
train loss:  0.4764202833175659
train gradient:  0.10806620886583479
iteration : 13217
train acc:  0.78125
train loss:  0.434620201587677
train gradient:  0.09414656713121988
iteration : 13218
train acc:  0.8125
train loss:  0.39003637433052063
train gradient:  0.09883850692792566
iteration : 13219
train acc:  0.7890625
train loss:  0.43476831912994385
train gradient:  0.1064022646339494
iteration : 13220
train acc:  0.71875
train loss:  0.5763972997665405
train gradient:  0.16108319350679384
iteration : 13221
train acc:  0.7734375
train loss:  0.4605492949485779
train gradient:  0.10506041370606171
iteration : 13222
train acc:  0.703125
train loss:  0.5070492625236511
train gradient:  0.12571445800700248
iteration : 13223
train acc:  0.6953125
train loss:  0.5000598430633545
train gradient:  0.12649740901320447
iteration : 13224
train acc:  0.65625
train loss:  0.5235772132873535
train gradient:  0.14668702271642756
iteration : 13225
train acc:  0.7890625
train loss:  0.42281240224838257
train gradient:  0.12248165423616848
iteration : 13226
train acc:  0.671875
train loss:  0.5350082516670227
train gradient:  0.18138193408860692
iteration : 13227
train acc:  0.6875
train loss:  0.5810022354125977
train gradient:  0.16342743578258218
iteration : 13228
train acc:  0.7265625
train loss:  0.46520668268203735
train gradient:  0.10451617795331408
iteration : 13229
train acc:  0.7421875
train loss:  0.45493924617767334
train gradient:  0.10370672238577025
iteration : 13230
train acc:  0.7578125
train loss:  0.4824587404727936
train gradient:  0.11747778334748793
iteration : 13231
train acc:  0.7578125
train loss:  0.4746817946434021
train gradient:  0.11529642044895388
iteration : 13232
train acc:  0.75
train loss:  0.4549252390861511
train gradient:  0.11599756599642588
iteration : 13233
train acc:  0.734375
train loss:  0.5304906368255615
train gradient:  0.186343028613161
iteration : 13234
train acc:  0.796875
train loss:  0.42380112409591675
train gradient:  0.0849770930813462
iteration : 13235
train acc:  0.7734375
train loss:  0.5127596259117126
train gradient:  0.18137545641080352
iteration : 13236
train acc:  0.7265625
train loss:  0.518106997013092
train gradient:  0.15971439629579165
iteration : 13237
train acc:  0.7578125
train loss:  0.4560144543647766
train gradient:  0.10882221630784294
iteration : 13238
train acc:  0.671875
train loss:  0.5634557008743286
train gradient:  0.21184158617536536
iteration : 13239
train acc:  0.703125
train loss:  0.5519115924835205
train gradient:  0.16375239612214854
iteration : 13240
train acc:  0.7109375
train loss:  0.5211120247840881
train gradient:  0.12990775491979478
iteration : 13241
train acc:  0.6953125
train loss:  0.5011091232299805
train gradient:  0.12000781713145446
iteration : 13242
train acc:  0.765625
train loss:  0.46404218673706055
train gradient:  0.12129403250495308
iteration : 13243
train acc:  0.8359375
train loss:  0.440123975276947
train gradient:  0.1264082361884555
iteration : 13244
train acc:  0.8125
train loss:  0.4426078200340271
train gradient:  0.09772641343821113
iteration : 13245
train acc:  0.703125
train loss:  0.5149937868118286
train gradient:  0.12567677437312486
iteration : 13246
train acc:  0.7109375
train loss:  0.4898296892642975
train gradient:  0.11347760257111274
iteration : 13247
train acc:  0.71875
train loss:  0.47705012559890747
train gradient:  0.12637354691927744
iteration : 13248
train acc:  0.75
train loss:  0.4734962582588196
train gradient:  0.11330733613892843
iteration : 13249
train acc:  0.765625
train loss:  0.46054714918136597
train gradient:  0.12550228961242257
iteration : 13250
train acc:  0.734375
train loss:  0.5079318881034851
train gradient:  0.12244518106551455
iteration : 13251
train acc:  0.7109375
train loss:  0.5220360159873962
train gradient:  0.13959228365214932
iteration : 13252
train acc:  0.75
train loss:  0.507541298866272
train gradient:  0.1385158261205449
iteration : 13253
train acc:  0.78125
train loss:  0.4451431632041931
train gradient:  0.13522971539165085
iteration : 13254
train acc:  0.671875
train loss:  0.5670582056045532
train gradient:  0.1759239940045266
iteration : 13255
train acc:  0.640625
train loss:  0.5701011419296265
train gradient:  0.14439564030866886
iteration : 13256
train acc:  0.6953125
train loss:  0.5402502417564392
train gradient:  0.1627136033555315
iteration : 13257
train acc:  0.71875
train loss:  0.506605863571167
train gradient:  0.1325817212521396
iteration : 13258
train acc:  0.7109375
train loss:  0.5316269397735596
train gradient:  0.16469897202255213
iteration : 13259
train acc:  0.6875
train loss:  0.49334511160850525
train gradient:  0.11226186291766166
iteration : 13260
train acc:  0.78125
train loss:  0.4876266121864319
train gradient:  0.12419854340564608
iteration : 13261
train acc:  0.8515625
train loss:  0.3651372790336609
train gradient:  0.08113893005952572
iteration : 13262
train acc:  0.75
train loss:  0.5406894683837891
train gradient:  0.15073105103488552
iteration : 13263
train acc:  0.796875
train loss:  0.4258245825767517
train gradient:  0.09505475052371336
iteration : 13264
train acc:  0.765625
train loss:  0.48998212814331055
train gradient:  0.1535010126418562
iteration : 13265
train acc:  0.6640625
train loss:  0.5693220496177673
train gradient:  0.14273825122142558
iteration : 13266
train acc:  0.765625
train loss:  0.5113181471824646
train gradient:  0.15920931002798158
iteration : 13267
train acc:  0.7421875
train loss:  0.5620595216751099
train gradient:  0.14140054238135372
iteration : 13268
train acc:  0.8203125
train loss:  0.4296322762966156
train gradient:  0.11878148421492338
iteration : 13269
train acc:  0.7421875
train loss:  0.4994880259037018
train gradient:  0.16745074355414113
iteration : 13270
train acc:  0.7265625
train loss:  0.4761425852775574
train gradient:  0.12180888049079477
iteration : 13271
train acc:  0.7421875
train loss:  0.5465412139892578
train gradient:  0.2356347271182333
iteration : 13272
train acc:  0.75
train loss:  0.5124051570892334
train gradient:  0.11161731604383315
iteration : 13273
train acc:  0.7421875
train loss:  0.5000192523002625
train gradient:  0.12627754601550512
iteration : 13274
train acc:  0.7578125
train loss:  0.45998552441596985
train gradient:  0.09551068497705818
iteration : 13275
train acc:  0.7578125
train loss:  0.45931604504585266
train gradient:  0.12011961079682396
iteration : 13276
train acc:  0.7109375
train loss:  0.5338089466094971
train gradient:  0.13050908429674316
iteration : 13277
train acc:  0.796875
train loss:  0.41821447014808655
train gradient:  0.08825585494324546
iteration : 13278
train acc:  0.7734375
train loss:  0.45600631833076477
train gradient:  0.13801359566249405
iteration : 13279
train acc:  0.8125
train loss:  0.4306965470314026
train gradient:  0.10026199420823825
iteration : 13280
train acc:  0.703125
train loss:  0.46328920125961304
train gradient:  0.10825123172211189
iteration : 13281
train acc:  0.8203125
train loss:  0.43573641777038574
train gradient:  0.08779775523858258
iteration : 13282
train acc:  0.796875
train loss:  0.4663372039794922
train gradient:  0.09671133886658423
iteration : 13283
train acc:  0.7421875
train loss:  0.49319684505462646
train gradient:  0.14079677564499538
iteration : 13284
train acc:  0.7265625
train loss:  0.5406026840209961
train gradient:  0.22036525491700823
iteration : 13285
train acc:  0.734375
train loss:  0.5276662111282349
train gradient:  0.11499579789665007
iteration : 13286
train acc:  0.734375
train loss:  0.5499533414840698
train gradient:  0.15583877732034537
iteration : 13287
train acc:  0.734375
train loss:  0.492779403924942
train gradient:  0.10672752608064888
iteration : 13288
train acc:  0.765625
train loss:  0.4582604765892029
train gradient:  0.11709244464181477
iteration : 13289
train acc:  0.7109375
train loss:  0.5167919993400574
train gradient:  0.1420214496000346
iteration : 13290
train acc:  0.8125
train loss:  0.46648532152175903
train gradient:  0.10642711882320476
iteration : 13291
train acc:  0.7265625
train loss:  0.5067368745803833
train gradient:  0.14831941113531605
iteration : 13292
train acc:  0.8125
train loss:  0.4067336320877075
train gradient:  0.13517639975309825
iteration : 13293
train acc:  0.7578125
train loss:  0.4813917577266693
train gradient:  0.1329290566804838
iteration : 13294
train acc:  0.6953125
train loss:  0.5457161664962769
train gradient:  0.12955776376266956
iteration : 13295
train acc:  0.7578125
train loss:  0.46525245904922485
train gradient:  0.11275839822563019
iteration : 13296
train acc:  0.734375
train loss:  0.4841136038303375
train gradient:  0.11116249627367188
iteration : 13297
train acc:  0.765625
train loss:  0.5257161259651184
train gradient:  0.14270437935130584
iteration : 13298
train acc:  0.6796875
train loss:  0.5595575571060181
train gradient:  0.20698862944436353
iteration : 13299
train acc:  0.75
train loss:  0.4738052785396576
train gradient:  0.11025258440494425
iteration : 13300
train acc:  0.6875
train loss:  0.5642948746681213
train gradient:  0.19992469930173257
iteration : 13301
train acc:  0.7421875
train loss:  0.4633617401123047
train gradient:  0.11083553932283897
iteration : 13302
train acc:  0.7421875
train loss:  0.4995953142642975
train gradient:  0.11736518367392401
iteration : 13303
train acc:  0.7734375
train loss:  0.4598889946937561
train gradient:  0.13669520158367654
iteration : 13304
train acc:  0.7265625
train loss:  0.48347577452659607
train gradient:  0.1178606695847736
iteration : 13305
train acc:  0.765625
train loss:  0.5316668748855591
train gradient:  0.1353711905814317
iteration : 13306
train acc:  0.8203125
train loss:  0.4367784857749939
train gradient:  0.09732383306643635
iteration : 13307
train acc:  0.7109375
train loss:  0.45710504055023193
train gradient:  0.106929716342677
iteration : 13308
train acc:  0.75
train loss:  0.4677017629146576
train gradient:  0.10879646919234998
iteration : 13309
train acc:  0.7734375
train loss:  0.5229594707489014
train gradient:  0.09990140783478821
iteration : 13310
train acc:  0.78125
train loss:  0.43620729446411133
train gradient:  0.1269174766784613
iteration : 13311
train acc:  0.8046875
train loss:  0.46851080656051636
train gradient:  0.11923634319391634
iteration : 13312
train acc:  0.8046875
train loss:  0.4115443825721741
train gradient:  0.08978729054305103
iteration : 13313
train acc:  0.765625
train loss:  0.4840020537376404
train gradient:  0.10863510711886712
iteration : 13314
train acc:  0.6953125
train loss:  0.5637681484222412
train gradient:  0.15848331354775716
iteration : 13315
train acc:  0.703125
train loss:  0.5679076910018921
train gradient:  0.1856455206962664
iteration : 13316
train acc:  0.796875
train loss:  0.4172307252883911
train gradient:  0.10078907991851839
iteration : 13317
train acc:  0.7578125
train loss:  0.5048683881759644
train gradient:  0.11736368716399165
iteration : 13318
train acc:  0.8046875
train loss:  0.451133668422699
train gradient:  0.10789142546322579
iteration : 13319
train acc:  0.7109375
train loss:  0.4898088872432709
train gradient:  0.09836735973631457
iteration : 13320
train acc:  0.75
train loss:  0.49116024374961853
train gradient:  0.14894544068316357
iteration : 13321
train acc:  0.7265625
train loss:  0.502974271774292
train gradient:  0.122743006338342
iteration : 13322
train acc:  0.7109375
train loss:  0.5193907022476196
train gradient:  0.13784988920706065
iteration : 13323
train acc:  0.7890625
train loss:  0.41844668984413147
train gradient:  0.10920377225752335
iteration : 13324
train acc:  0.7265625
train loss:  0.52068692445755
train gradient:  0.18799441183650134
iteration : 13325
train acc:  0.8125
train loss:  0.4071797728538513
train gradient:  0.07902814392745222
iteration : 13326
train acc:  0.75
train loss:  0.47546154260635376
train gradient:  0.09768605288132609
iteration : 13327
train acc:  0.71875
train loss:  0.5291770100593567
train gradient:  0.1283723484572208
iteration : 13328
train acc:  0.78125
train loss:  0.47250813245773315
train gradient:  0.129636506226663
iteration : 13329
train acc:  0.8203125
train loss:  0.416686475276947
train gradient:  0.12173429139574585
iteration : 13330
train acc:  0.6796875
train loss:  0.5338548421859741
train gradient:  0.1742148381505268
iteration : 13331
train acc:  0.828125
train loss:  0.38651910424232483
train gradient:  0.10026956637851357
iteration : 13332
train acc:  0.78125
train loss:  0.4422150254249573
train gradient:  0.12011858442584963
iteration : 13333
train acc:  0.6640625
train loss:  0.5360493659973145
train gradient:  0.17723299009279944
iteration : 13334
train acc:  0.734375
train loss:  0.4904416501522064
train gradient:  0.13588192575327723
iteration : 13335
train acc:  0.7421875
train loss:  0.5016143321990967
train gradient:  0.1521413422090429
iteration : 13336
train acc:  0.7421875
train loss:  0.4606836438179016
train gradient:  0.11090315189376096
iteration : 13337
train acc:  0.765625
train loss:  0.4896391034126282
train gradient:  0.11878674758868288
iteration : 13338
train acc:  0.78125
train loss:  0.4992956519126892
train gradient:  0.1333538211513495
iteration : 13339
train acc:  0.7421875
train loss:  0.4760794937610626
train gradient:  0.14673228783951714
iteration : 13340
train acc:  0.7578125
train loss:  0.4471248388290405
train gradient:  0.11064374206691481
iteration : 13341
train acc:  0.734375
train loss:  0.5118683576583862
train gradient:  0.13080743023463134
iteration : 13342
train acc:  0.734375
train loss:  0.5474426746368408
train gradient:  0.12224764792665889
iteration : 13343
train acc:  0.71875
train loss:  0.4237461984157562
train gradient:  0.12481312992337078
iteration : 13344
train acc:  0.7265625
train loss:  0.5147369503974915
train gradient:  0.13493083126129585
iteration : 13345
train acc:  0.765625
train loss:  0.48553013801574707
train gradient:  0.13670099668389318
iteration : 13346
train acc:  0.703125
train loss:  0.502768874168396
train gradient:  0.14519770408585436
iteration : 13347
train acc:  0.8125
train loss:  0.4646868109703064
train gradient:  0.1141787493158099
iteration : 13348
train acc:  0.7265625
train loss:  0.5405339598655701
train gradient:  0.18983829711237282
iteration : 13349
train acc:  0.71875
train loss:  0.5187828540802002
train gradient:  0.1470110339898576
iteration : 13350
train acc:  0.7734375
train loss:  0.46105751395225525
train gradient:  0.11892404375399156
iteration : 13351
train acc:  0.796875
train loss:  0.4239733815193176
train gradient:  0.08819425380990567
iteration : 13352
train acc:  0.75
train loss:  0.46929049491882324
train gradient:  0.12410148567408626
iteration : 13353
train acc:  0.71875
train loss:  0.4795806109905243
train gradient:  0.10873960904073388
iteration : 13354
train acc:  0.7265625
train loss:  0.48154908418655396
train gradient:  0.13472694557032996
iteration : 13355
train acc:  0.75
train loss:  0.4722362756729126
train gradient:  0.1320236444337463
iteration : 13356
train acc:  0.765625
train loss:  0.4834091067314148
train gradient:  0.12679253512437444
iteration : 13357
train acc:  0.78125
train loss:  0.4352300763130188
train gradient:  0.08238610510415627
iteration : 13358
train acc:  0.703125
train loss:  0.5349500179290771
train gradient:  0.15857002060651004
iteration : 13359
train acc:  0.6875
train loss:  0.5790408849716187
train gradient:  0.17422126559940565
iteration : 13360
train acc:  0.84375
train loss:  0.4419136047363281
train gradient:  0.10108907374471607
iteration : 13361
train acc:  0.7578125
train loss:  0.4560037851333618
train gradient:  0.11791003738954794
iteration : 13362
train acc:  0.7734375
train loss:  0.4361535310745239
train gradient:  0.0867713202891302
iteration : 13363
train acc:  0.703125
train loss:  0.5074149966239929
train gradient:  0.11665801450316014
iteration : 13364
train acc:  0.7109375
train loss:  0.49647894501686096
train gradient:  0.12838384333168867
iteration : 13365
train acc:  0.7890625
train loss:  0.46655237674713135
train gradient:  0.1130282698736903
iteration : 13366
train acc:  0.7578125
train loss:  0.47841936349868774
train gradient:  0.12897293130410425
iteration : 13367
train acc:  0.7421875
train loss:  0.4659925401210785
train gradient:  0.10230166876326545
iteration : 13368
train acc:  0.796875
train loss:  0.47791069746017456
train gradient:  0.11313798361078639
iteration : 13369
train acc:  0.7265625
train loss:  0.5109348893165588
train gradient:  0.12735328678630153
iteration : 13370
train acc:  0.8046875
train loss:  0.4095017910003662
train gradient:  0.09252972372630991
iteration : 13371
train acc:  0.765625
train loss:  0.44314342737197876
train gradient:  0.11143448284480081
iteration : 13372
train acc:  0.6796875
train loss:  0.5745375156402588
train gradient:  0.14807629979146814
iteration : 13373
train acc:  0.7578125
train loss:  0.5098311901092529
train gradient:  0.12435752223773058
iteration : 13374
train acc:  0.7265625
train loss:  0.4685354232788086
train gradient:  0.11124287409743634
iteration : 13375
train acc:  0.7421875
train loss:  0.47137778997421265
train gradient:  0.10886630724825033
iteration : 13376
train acc:  0.7734375
train loss:  0.4629330635070801
train gradient:  0.13696631834806722
iteration : 13377
train acc:  0.8203125
train loss:  0.4594719111919403
train gradient:  0.11330405083711949
iteration : 13378
train acc:  0.6796875
train loss:  0.5140255689620972
train gradient:  0.1406111030310911
iteration : 13379
train acc:  0.7578125
train loss:  0.565734326839447
train gradient:  0.1802340561039812
iteration : 13380
train acc:  0.7890625
train loss:  0.4756729006767273
train gradient:  0.12737645242284537
iteration : 13381
train acc:  0.765625
train loss:  0.48472416400909424
train gradient:  0.13089900972937524
iteration : 13382
train acc:  0.7890625
train loss:  0.48996615409851074
train gradient:  0.1286871534463736
iteration : 13383
train acc:  0.7109375
train loss:  0.5635741353034973
train gradient:  0.1440361954368389
iteration : 13384
train acc:  0.6875
train loss:  0.554747462272644
train gradient:  0.14071998717774092
iteration : 13385
train acc:  0.7734375
train loss:  0.44310295581817627
train gradient:  0.11732301118483916
iteration : 13386
train acc:  0.75
train loss:  0.4673558175563812
train gradient:  0.10716228837794048
iteration : 13387
train acc:  0.7890625
train loss:  0.4367644190788269
train gradient:  0.10416344389553725
iteration : 13388
train acc:  0.703125
train loss:  0.5554256439208984
train gradient:  0.16222943361676606
iteration : 13389
train acc:  0.796875
train loss:  0.4387176036834717
train gradient:  0.11616408659156091
iteration : 13390
train acc:  0.7890625
train loss:  0.5170247554779053
train gradient:  0.1541427337602117
iteration : 13391
train acc:  0.75
train loss:  0.48279327154159546
train gradient:  0.1287142955508827
iteration : 13392
train acc:  0.7734375
train loss:  0.5120018720626831
train gradient:  0.14629712213514795
iteration : 13393
train acc:  0.7890625
train loss:  0.49215084314346313
train gradient:  0.14108488206392678
iteration : 13394
train acc:  0.703125
train loss:  0.5503747463226318
train gradient:  0.15492575925871654
iteration : 13395
train acc:  0.7734375
train loss:  0.4436054229736328
train gradient:  0.08888898771985344
iteration : 13396
train acc:  0.7734375
train loss:  0.430044561624527
train gradient:  0.09781109286435492
iteration : 13397
train acc:  0.734375
train loss:  0.5467413663864136
train gradient:  0.15774028145397806
iteration : 13398
train acc:  0.7890625
train loss:  0.5188906192779541
train gradient:  0.1374694698433315
iteration : 13399
train acc:  0.7578125
train loss:  0.4806211590766907
train gradient:  0.13046387359337466
iteration : 13400
train acc:  0.7578125
train loss:  0.4302408695220947
train gradient:  0.09803709336899954
iteration : 13401
train acc:  0.7578125
train loss:  0.485215425491333
train gradient:  0.11650789752714999
iteration : 13402
train acc:  0.7421875
train loss:  0.521549642086029
train gradient:  0.15858331275584558
iteration : 13403
train acc:  0.734375
train loss:  0.5254716873168945
train gradient:  0.13075661964777127
iteration : 13404
train acc:  0.7265625
train loss:  0.5590224266052246
train gradient:  0.1448857706278917
iteration : 13405
train acc:  0.8046875
train loss:  0.4344283640384674
train gradient:  0.11247139484308502
iteration : 13406
train acc:  0.6796875
train loss:  0.5316723585128784
train gradient:  0.15399736775939754
iteration : 13407
train acc:  0.7578125
train loss:  0.4813288450241089
train gradient:  0.1355789567997649
iteration : 13408
train acc:  0.734375
train loss:  0.49328964948654175
train gradient:  0.11831535906295294
iteration : 13409
train acc:  0.7734375
train loss:  0.46079331636428833
train gradient:  0.12349023957955325
iteration : 13410
train acc:  0.78125
train loss:  0.506184458732605
train gradient:  0.15403166718854283
iteration : 13411
train acc:  0.7109375
train loss:  0.5041179656982422
train gradient:  0.1090328636922486
iteration : 13412
train acc:  0.765625
train loss:  0.45057839155197144
train gradient:  0.09709340824268765
iteration : 13413
train acc:  0.765625
train loss:  0.395066499710083
train gradient:  0.08329411868674418
iteration : 13414
train acc:  0.7890625
train loss:  0.4289354979991913
train gradient:  0.14356493786700347
iteration : 13415
train acc:  0.7890625
train loss:  0.44523656368255615
train gradient:  0.10642727068503566
iteration : 13416
train acc:  0.78125
train loss:  0.44583871960639954
train gradient:  0.1166019244656332
iteration : 13417
train acc:  0.78125
train loss:  0.4809439182281494
train gradient:  0.11796676058309742
iteration : 13418
train acc:  0.8046875
train loss:  0.4378356337547302
train gradient:  0.12060295123325826
iteration : 13419
train acc:  0.8125
train loss:  0.42081743478775024
train gradient:  0.10301524376535807
iteration : 13420
train acc:  0.7578125
train loss:  0.475006639957428
train gradient:  0.11427917212702397
iteration : 13421
train acc:  0.7734375
train loss:  0.5010885000228882
train gradient:  0.1256807569160744
iteration : 13422
train acc:  0.7421875
train loss:  0.4558396339416504
train gradient:  0.1252329627323251
iteration : 13423
train acc:  0.734375
train loss:  0.5209742188453674
train gradient:  0.11897671808477524
iteration : 13424
train acc:  0.7890625
train loss:  0.44273003935813904
train gradient:  0.11287212647406025
iteration : 13425
train acc:  0.78125
train loss:  0.446850061416626
train gradient:  0.11215809745996733
iteration : 13426
train acc:  0.7734375
train loss:  0.4846404194831848
train gradient:  0.14825518926136522
iteration : 13427
train acc:  0.6953125
train loss:  0.510772168636322
train gradient:  0.1682033426404143
iteration : 13428
train acc:  0.7734375
train loss:  0.5041645765304565
train gradient:  0.15274516647650965
iteration : 13429
train acc:  0.8125
train loss:  0.38385358452796936
train gradient:  0.10922861972756717
iteration : 13430
train acc:  0.7578125
train loss:  0.4855281114578247
train gradient:  0.17236083451805678
iteration : 13431
train acc:  0.7421875
train loss:  0.48610183596611023
train gradient:  0.09972376869811568
iteration : 13432
train acc:  0.765625
train loss:  0.501162052154541
train gradient:  0.13213830897829654
iteration : 13433
train acc:  0.6640625
train loss:  0.5684365034103394
train gradient:  0.1857406587703647
iteration : 13434
train acc:  0.7578125
train loss:  0.46890586614608765
train gradient:  0.14242353128651125
iteration : 13435
train acc:  0.6796875
train loss:  0.5339301824569702
train gradient:  0.12289601608962415
iteration : 13436
train acc:  0.7421875
train loss:  0.4932844638824463
train gradient:  0.13438016260323943
iteration : 13437
train acc:  0.7890625
train loss:  0.45308685302734375
train gradient:  0.13624678989526567
iteration : 13438
train acc:  0.7734375
train loss:  0.4795992076396942
train gradient:  0.12895714037015815
iteration : 13439
train acc:  0.6953125
train loss:  0.5281407833099365
train gradient:  0.1500749164089491
iteration : 13440
train acc:  0.71875
train loss:  0.5161722898483276
train gradient:  0.13674124365616458
iteration : 13441
train acc:  0.6796875
train loss:  0.48620355129241943
train gradient:  0.12273132695727522
iteration : 13442
train acc:  0.6953125
train loss:  0.5088248252868652
train gradient:  0.11835741143643713
iteration : 13443
train acc:  0.796875
train loss:  0.5023696422576904
train gradient:  0.1311523363621686
iteration : 13444
train acc:  0.78125
train loss:  0.4686833322048187
train gradient:  0.14196478458618572
iteration : 13445
train acc:  0.7734375
train loss:  0.453147292137146
train gradient:  0.0963696975159152
iteration : 13446
train acc:  0.7734375
train loss:  0.4627407491207123
train gradient:  0.11425789745106997
iteration : 13447
train acc:  0.7578125
train loss:  0.4289790391921997
train gradient:  0.10027226069726164
iteration : 13448
train acc:  0.75
train loss:  0.4599745273590088
train gradient:  0.08638772382823796
iteration : 13449
train acc:  0.7265625
train loss:  0.48256123065948486
train gradient:  0.16334725703685316
iteration : 13450
train acc:  0.8046875
train loss:  0.45138004422187805
train gradient:  0.12884386589199173
iteration : 13451
train acc:  0.7890625
train loss:  0.40300533175468445
train gradient:  0.11910077151425005
iteration : 13452
train acc:  0.765625
train loss:  0.4760519862174988
train gradient:  0.12057820723534599
iteration : 13453
train acc:  0.6953125
train loss:  0.5262570381164551
train gradient:  0.12925543814095078
iteration : 13454
train acc:  0.78125
train loss:  0.4794108271598816
train gradient:  0.15127131830468887
iteration : 13455
train acc:  0.703125
train loss:  0.518622875213623
train gradient:  0.12511561872235177
iteration : 13456
train acc:  0.7578125
train loss:  0.4880828559398651
train gradient:  0.15537631508681182
iteration : 13457
train acc:  0.7578125
train loss:  0.41648048162460327
train gradient:  0.10162398157222423
iteration : 13458
train acc:  0.71875
train loss:  0.5410821437835693
train gradient:  0.14358684082916412
iteration : 13459
train acc:  0.7578125
train loss:  0.4546966552734375
train gradient:  0.11573948214099417
iteration : 13460
train acc:  0.6796875
train loss:  0.5762838125228882
train gradient:  0.17063639441143902
iteration : 13461
train acc:  0.7265625
train loss:  0.4875231981277466
train gradient:  0.12945646350251488
iteration : 13462
train acc:  0.7109375
train loss:  0.5191104412078857
train gradient:  0.17482147760377534
iteration : 13463
train acc:  0.703125
train loss:  0.4878007173538208
train gradient:  0.12883016635348332
iteration : 13464
train acc:  0.734375
train loss:  0.529913067817688
train gradient:  0.1600936320890719
iteration : 13465
train acc:  0.75
train loss:  0.47408294677734375
train gradient:  0.11538306395802427
iteration : 13466
train acc:  0.734375
train loss:  0.4874664843082428
train gradient:  0.10932541689170966
iteration : 13467
train acc:  0.75
train loss:  0.461646169424057
train gradient:  0.11443396101376455
iteration : 13468
train acc:  0.71875
train loss:  0.5182535648345947
train gradient:  0.13367899561683216
iteration : 13469
train acc:  0.7578125
train loss:  0.44016823172569275
train gradient:  0.08992322593153351
iteration : 13470
train acc:  0.7421875
train loss:  0.4667574465274811
train gradient:  0.11692083446854169
iteration : 13471
train acc:  0.75
train loss:  0.46231091022491455
train gradient:  0.11552491084978339
iteration : 13472
train acc:  0.7578125
train loss:  0.4840352237224579
train gradient:  0.13498536524734786
iteration : 13473
train acc:  0.703125
train loss:  0.5098220109939575
train gradient:  0.12936253799169675
iteration : 13474
train acc:  0.7734375
train loss:  0.43620121479034424
train gradient:  0.11355773370577005
iteration : 13475
train acc:  0.734375
train loss:  0.5073230266571045
train gradient:  0.12709473479357916
iteration : 13476
train acc:  0.71875
train loss:  0.5008014440536499
train gradient:  0.13143447224053373
iteration : 13477
train acc:  0.765625
train loss:  0.4757567048072815
train gradient:  0.12302220293412111
iteration : 13478
train acc:  0.734375
train loss:  0.5259868502616882
train gradient:  0.13367173940860314
iteration : 13479
train acc:  0.71875
train loss:  0.5110969543457031
train gradient:  0.1408976402094031
iteration : 13480
train acc:  0.7578125
train loss:  0.46698448061943054
train gradient:  0.09560107996337336
iteration : 13481
train acc:  0.7265625
train loss:  0.4772037863731384
train gradient:  0.14917227009464112
iteration : 13482
train acc:  0.7265625
train loss:  0.4992089867591858
train gradient:  0.1331812979980851
iteration : 13483
train acc:  0.6484375
train loss:  0.6102601885795593
train gradient:  0.1616917420597889
iteration : 13484
train acc:  0.7578125
train loss:  0.4717177152633667
train gradient:  0.13852878249256306
iteration : 13485
train acc:  0.734375
train loss:  0.48275062441825867
train gradient:  0.12354790651763146
iteration : 13486
train acc:  0.71875
train loss:  0.4642753601074219
train gradient:  0.12714076038037675
iteration : 13487
train acc:  0.71875
train loss:  0.47585177421569824
train gradient:  0.1228093870865075
iteration : 13488
train acc:  0.734375
train loss:  0.4893234372138977
train gradient:  0.13230757339182514
iteration : 13489
train acc:  0.8125
train loss:  0.44886088371276855
train gradient:  0.12892999817679324
iteration : 13490
train acc:  0.7890625
train loss:  0.49711912870407104
train gradient:  0.13613485377772283
iteration : 13491
train acc:  0.7890625
train loss:  0.42530739307403564
train gradient:  0.12865424476408518
iteration : 13492
train acc:  0.75
train loss:  0.4987156093120575
train gradient:  0.14018926985746707
iteration : 13493
train acc:  0.75
train loss:  0.4782027304172516
train gradient:  0.11714045837484507
iteration : 13494
train acc:  0.796875
train loss:  0.49967193603515625
train gradient:  0.13460512065094965
iteration : 13495
train acc:  0.6875
train loss:  0.5328384637832642
train gradient:  0.13314667931416352
iteration : 13496
train acc:  0.7421875
train loss:  0.47334742546081543
train gradient:  0.10485498923112133
iteration : 13497
train acc:  0.734375
train loss:  0.5451825857162476
train gradient:  0.1804524742779026
iteration : 13498
train acc:  0.734375
train loss:  0.4893742799758911
train gradient:  0.14580717019025685
iteration : 13499
train acc:  0.796875
train loss:  0.4285773038864136
train gradient:  0.09338477960427238
iteration : 13500
train acc:  0.6875
train loss:  0.5346107482910156
train gradient:  0.1607167103441219
iteration : 13501
train acc:  0.7734375
train loss:  0.4439494013786316
train gradient:  0.11904306380531586
iteration : 13502
train acc:  0.7578125
train loss:  0.46582746505737305
train gradient:  0.14720401915913012
iteration : 13503
train acc:  0.71875
train loss:  0.5219957828521729
train gradient:  0.11856041170511487
iteration : 13504
train acc:  0.7265625
train loss:  0.4663785398006439
train gradient:  0.10104201615144384
iteration : 13505
train acc:  0.7421875
train loss:  0.5170678496360779
train gradient:  0.14717231094179295
iteration : 13506
train acc:  0.734375
train loss:  0.46053630113601685
train gradient:  0.10048568644794256
iteration : 13507
train acc:  0.8125
train loss:  0.4635770916938782
train gradient:  0.1291114153030906
iteration : 13508
train acc:  0.7109375
train loss:  0.4798857569694519
train gradient:  0.14014063937555407
iteration : 13509
train acc:  0.71875
train loss:  0.48690396547317505
train gradient:  0.13116525113223015
iteration : 13510
train acc:  0.75
train loss:  0.5239425897598267
train gradient:  0.12934251652004913
iteration : 13511
train acc:  0.734375
train loss:  0.4792537987232208
train gradient:  0.1298233286416714
iteration : 13512
train acc:  0.734375
train loss:  0.5173336863517761
train gradient:  0.1276372859300614
iteration : 13513
train acc:  0.7109375
train loss:  0.5493755340576172
train gradient:  0.13889496996690048
iteration : 13514
train acc:  0.734375
train loss:  0.4797723889350891
train gradient:  0.11186689303815885
iteration : 13515
train acc:  0.75
train loss:  0.47206223011016846
train gradient:  0.13046441538863524
iteration : 13516
train acc:  0.6875
train loss:  0.5789148807525635
train gradient:  0.2327613448980639
iteration : 13517
train acc:  0.703125
train loss:  0.5108855962753296
train gradient:  0.13398485956212872
iteration : 13518
train acc:  0.7890625
train loss:  0.4313637614250183
train gradient:  0.11402837733125348
iteration : 13519
train acc:  0.7265625
train loss:  0.5142076015472412
train gradient:  0.123359329131236
iteration : 13520
train acc:  0.765625
train loss:  0.47548139095306396
train gradient:  0.1712961325241676
iteration : 13521
train acc:  0.8203125
train loss:  0.42717286944389343
train gradient:  0.11391223159788795
iteration : 13522
train acc:  0.7734375
train loss:  0.4392249584197998
train gradient:  0.13318486215337932
iteration : 13523
train acc:  0.6953125
train loss:  0.5342615842819214
train gradient:  0.12726868215220466
iteration : 13524
train acc:  0.734375
train loss:  0.485464870929718
train gradient:  0.12431016900925412
iteration : 13525
train acc:  0.7578125
train loss:  0.44652068614959717
train gradient:  0.09840659459250108
iteration : 13526
train acc:  0.734375
train loss:  0.4958626329898834
train gradient:  0.1227095198235264
iteration : 13527
train acc:  0.71875
train loss:  0.548499345779419
train gradient:  0.15332080715382784
iteration : 13528
train acc:  0.75
train loss:  0.4920673668384552
train gradient:  0.14367643730381718
iteration : 13529
train acc:  0.71875
train loss:  0.5049125552177429
train gradient:  0.10630536500996283
iteration : 13530
train acc:  0.7109375
train loss:  0.4798956513404846
train gradient:  0.13404969994718252
iteration : 13531
train acc:  0.703125
train loss:  0.5091171264648438
train gradient:  0.14369716786054199
iteration : 13532
train acc:  0.703125
train loss:  0.48741692304611206
train gradient:  0.128676476848618
iteration : 13533
train acc:  0.796875
train loss:  0.46360790729522705
train gradient:  0.13482175062134238
iteration : 13534
train acc:  0.7421875
train loss:  0.4847583472728729
train gradient:  0.1229622739909302
iteration : 13535
train acc:  0.7265625
train loss:  0.46214818954467773
train gradient:  0.11039471588144632
iteration : 13536
train acc:  0.71875
train loss:  0.5353823900222778
train gradient:  0.11307705255501735
iteration : 13537
train acc:  0.734375
train loss:  0.5135098695755005
train gradient:  0.15276728607530876
iteration : 13538
train acc:  0.8046875
train loss:  0.46559420228004456
train gradient:  0.11445003724788827
iteration : 13539
train acc:  0.7734375
train loss:  0.4777831435203552
train gradient:  0.1615010661089887
iteration : 13540
train acc:  0.8125
train loss:  0.4233825206756592
train gradient:  0.0995842290800269
iteration : 13541
train acc:  0.7109375
train loss:  0.5284668207168579
train gradient:  0.1278973967507388
iteration : 13542
train acc:  0.7265625
train loss:  0.4774245023727417
train gradient:  0.12675602226809501
iteration : 13543
train acc:  0.75
train loss:  0.4906461834907532
train gradient:  0.11116939644332749
iteration : 13544
train acc:  0.78125
train loss:  0.42366325855255127
train gradient:  0.10139252054141153
iteration : 13545
train acc:  0.875
train loss:  0.36637142300605774
train gradient:  0.0836255055868986
iteration : 13546
train acc:  0.7734375
train loss:  0.43362322449684143
train gradient:  0.0940453742274923
iteration : 13547
train acc:  0.71875
train loss:  0.5012613534927368
train gradient:  0.12462883312040869
iteration : 13548
train acc:  0.8046875
train loss:  0.4502454996109009
train gradient:  0.10463566614634265
iteration : 13549
train acc:  0.765625
train loss:  0.431302011013031
train gradient:  0.09813495824363068
iteration : 13550
train acc:  0.6328125
train loss:  0.6298560500144958
train gradient:  0.18561486568047691
iteration : 13551
train acc:  0.7578125
train loss:  0.4913504123687744
train gradient:  0.12566234032030194
iteration : 13552
train acc:  0.78125
train loss:  0.5200625658035278
train gradient:  0.12557492300383766
iteration : 13553
train acc:  0.78125
train loss:  0.4390854835510254
train gradient:  0.09626559649828259
iteration : 13554
train acc:  0.75
train loss:  0.49571093916893005
train gradient:  0.12955476319881348
iteration : 13555
train acc:  0.8125
train loss:  0.4216708242893219
train gradient:  0.12295005099737942
iteration : 13556
train acc:  0.75
train loss:  0.48542988300323486
train gradient:  0.1128611614410328
iteration : 13557
train acc:  0.671875
train loss:  0.5776997804641724
train gradient:  0.15971674615328957
iteration : 13558
train acc:  0.7890625
train loss:  0.4471528232097626
train gradient:  0.10496479005301354
iteration : 13559
train acc:  0.75
train loss:  0.4864373207092285
train gradient:  0.1260783774811634
iteration : 13560
train acc:  0.6953125
train loss:  0.5218391418457031
train gradient:  0.1255333688231633
iteration : 13561
train acc:  0.734375
train loss:  0.4863603711128235
train gradient:  0.17632593027705254
iteration : 13562
train acc:  0.671875
train loss:  0.5422353148460388
train gradient:  0.14732023481262344
iteration : 13563
train acc:  0.765625
train loss:  0.4589454233646393
train gradient:  0.1342153887685623
iteration : 13564
train acc:  0.7265625
train loss:  0.5172163248062134
train gradient:  0.13141253921187246
iteration : 13565
train acc:  0.75
train loss:  0.4606066346168518
train gradient:  0.10960411190943684
iteration : 13566
train acc:  0.6328125
train loss:  0.5736088156700134
train gradient:  0.20371421920754784
iteration : 13567
train acc:  0.75
train loss:  0.46857672929763794
train gradient:  0.12044229068975357
iteration : 13568
train acc:  0.78125
train loss:  0.4560096263885498
train gradient:  0.09196792268758901
iteration : 13569
train acc:  0.7109375
train loss:  0.4905809760093689
train gradient:  0.12085994012928952
iteration : 13570
train acc:  0.7734375
train loss:  0.5140918493270874
train gradient:  0.14984932593324934
iteration : 13571
train acc:  0.78125
train loss:  0.4156058430671692
train gradient:  0.1154950929873609
iteration : 13572
train acc:  0.765625
train loss:  0.4357607364654541
train gradient:  0.0943337347262139
iteration : 13573
train acc:  0.7265625
train loss:  0.5240753889083862
train gradient:  0.1348249179883308
iteration : 13574
train acc:  0.796875
train loss:  0.42647480964660645
train gradient:  0.08601120794901398
iteration : 13575
train acc:  0.828125
train loss:  0.42888343334198
train gradient:  0.09757651415677493
iteration : 13576
train acc:  0.71875
train loss:  0.5039979219436646
train gradient:  0.12004970671136986
iteration : 13577
train acc:  0.75
train loss:  0.5015835762023926
train gradient:  0.14733425955278107
iteration : 13578
train acc:  0.71875
train loss:  0.5731979608535767
train gradient:  0.21486067092510222
iteration : 13579
train acc:  0.78125
train loss:  0.4595016837120056
train gradient:  0.10223140131236852
iteration : 13580
train acc:  0.703125
train loss:  0.5425926446914673
train gradient:  0.14747470872149276
iteration : 13581
train acc:  0.7734375
train loss:  0.4362090826034546
train gradient:  0.09156758369576758
iteration : 13582
train acc:  0.765625
train loss:  0.4632878601551056
train gradient:  0.11333872139201884
iteration : 13583
train acc:  0.65625
train loss:  0.584396243095398
train gradient:  0.1785866356196067
iteration : 13584
train acc:  0.7421875
train loss:  0.47632741928100586
train gradient:  0.09988272416458294
iteration : 13585
train acc:  0.78125
train loss:  0.46412354707717896
train gradient:  0.09195565414008046
iteration : 13586
train acc:  0.703125
train loss:  0.5344541072845459
train gradient:  0.1798521554956133
iteration : 13587
train acc:  0.7109375
train loss:  0.5409460067749023
train gradient:  0.15833717652880663
iteration : 13588
train acc:  0.796875
train loss:  0.4300559163093567
train gradient:  0.14206855036672633
iteration : 13589
train acc:  0.765625
train loss:  0.5036277174949646
train gradient:  0.17240101260039448
iteration : 13590
train acc:  0.796875
train loss:  0.46892786026000977
train gradient:  0.17264034020121952
iteration : 13591
train acc:  0.7578125
train loss:  0.47696834802627563
train gradient:  0.12183179927829382
iteration : 13592
train acc:  0.7578125
train loss:  0.4488399028778076
train gradient:  0.1302775889968766
iteration : 13593
train acc:  0.765625
train loss:  0.45330339670181274
train gradient:  0.09824403491639716
iteration : 13594
train acc:  0.703125
train loss:  0.4955752193927765
train gradient:  0.12991930082642938
iteration : 13595
train acc:  0.7421875
train loss:  0.5079724788665771
train gradient:  0.16315914533480297
iteration : 13596
train acc:  0.796875
train loss:  0.43391773104667664
train gradient:  0.09625776277363071
iteration : 13597
train acc:  0.734375
train loss:  0.5055104494094849
train gradient:  0.1522433809362157
iteration : 13598
train acc:  0.6953125
train loss:  0.5205690860748291
train gradient:  0.15664262954990515
iteration : 13599
train acc:  0.71875
train loss:  0.4787943363189697
train gradient:  0.15138688986722854
iteration : 13600
train acc:  0.765625
train loss:  0.44849956035614014
train gradient:  0.11793886411935957
iteration : 13601
train acc:  0.703125
train loss:  0.5972614884376526
train gradient:  0.24055235602176564
iteration : 13602
train acc:  0.6953125
train loss:  0.5318975448608398
train gradient:  0.1301686488301626
iteration : 13603
train acc:  0.7265625
train loss:  0.4404104948043823
train gradient:  0.10612579713687881
iteration : 13604
train acc:  0.7265625
train loss:  0.5135185718536377
train gradient:  0.16923992176678576
iteration : 13605
train acc:  0.75
train loss:  0.4834230840206146
train gradient:  0.12366722861159048
iteration : 13606
train acc:  0.7109375
train loss:  0.5446051359176636
train gradient:  0.1705563199555739
iteration : 13607
train acc:  0.7109375
train loss:  0.5076457262039185
train gradient:  0.1116346410552898
iteration : 13608
train acc:  0.734375
train loss:  0.5067431926727295
train gradient:  0.16995518341792498
iteration : 13609
train acc:  0.703125
train loss:  0.5544086694717407
train gradient:  0.156197708498966
iteration : 13610
train acc:  0.734375
train loss:  0.5331783890724182
train gradient:  0.1558676155693275
iteration : 13611
train acc:  0.75
train loss:  0.5194897651672363
train gradient:  0.13682650815595104
iteration : 13612
train acc:  0.7578125
train loss:  0.44913041591644287
train gradient:  0.09819440429854825
iteration : 13613
train acc:  0.7421875
train loss:  0.523693323135376
train gradient:  0.15437074357328825
iteration : 13614
train acc:  0.7734375
train loss:  0.44248759746551514
train gradient:  0.11983529071603523
iteration : 13615
train acc:  0.765625
train loss:  0.48091259598731995
train gradient:  0.1172957030268231
iteration : 13616
train acc:  0.765625
train loss:  0.5042627453804016
train gradient:  0.15228924333198485
iteration : 13617
train acc:  0.7421875
train loss:  0.4572623372077942
train gradient:  0.18750106610265863
iteration : 13618
train acc:  0.7265625
train loss:  0.5069255828857422
train gradient:  0.11568846885947087
iteration : 13619
train acc:  0.765625
train loss:  0.47813764214515686
train gradient:  0.11234823265717699
iteration : 13620
train acc:  0.765625
train loss:  0.48825588822364807
train gradient:  0.1438613388801852
iteration : 13621
train acc:  0.75
train loss:  0.4467301666736603
train gradient:  0.11596077818870829
iteration : 13622
train acc:  0.703125
train loss:  0.5189724564552307
train gradient:  0.16044025174231225
iteration : 13623
train acc:  0.78125
train loss:  0.468717098236084
train gradient:  0.1190746093072119
iteration : 13624
train acc:  0.671875
train loss:  0.5705252885818481
train gradient:  0.15307931278144826
iteration : 13625
train acc:  0.75
train loss:  0.4936148524284363
train gradient:  0.11677312445660444
iteration : 13626
train acc:  0.7109375
train loss:  0.583480715751648
train gradient:  0.17839357150178817
iteration : 13627
train acc:  0.65625
train loss:  0.5677497386932373
train gradient:  0.18692145323991738
iteration : 13628
train acc:  0.734375
train loss:  0.5280478000640869
train gradient:  0.17402137012723579
iteration : 13629
train acc:  0.7578125
train loss:  0.4998701810836792
train gradient:  0.14352219366416508
iteration : 13630
train acc:  0.7734375
train loss:  0.4679718017578125
train gradient:  0.10496732017103706
iteration : 13631
train acc:  0.78125
train loss:  0.49825751781463623
train gradient:  0.13046563306174097
iteration : 13632
train acc:  0.7578125
train loss:  0.49854761362075806
train gradient:  0.12207315949429294
iteration : 13633
train acc:  0.7421875
train loss:  0.5473354458808899
train gradient:  0.16469533942485304
iteration : 13634
train acc:  0.7578125
train loss:  0.494488000869751
train gradient:  0.15560699631385627
iteration : 13635
train acc:  0.734375
train loss:  0.4713209271430969
train gradient:  0.11408518282831512
iteration : 13636
train acc:  0.6875
train loss:  0.5558731555938721
train gradient:  0.17913826594702678
iteration : 13637
train acc:  0.734375
train loss:  0.4716665744781494
train gradient:  0.11848814738129394
iteration : 13638
train acc:  0.7578125
train loss:  0.4865802228450775
train gradient:  0.1008247414993422
iteration : 13639
train acc:  0.765625
train loss:  0.48949646949768066
train gradient:  0.11656891156838002
iteration : 13640
train acc:  0.75
train loss:  0.4891604483127594
train gradient:  0.17288694900834162
iteration : 13641
train acc:  0.7734375
train loss:  0.4745883643627167
train gradient:  0.12516559088478718
iteration : 13642
train acc:  0.7890625
train loss:  0.41948527097702026
train gradient:  0.10694024095020373
iteration : 13643
train acc:  0.671875
train loss:  0.5092552900314331
train gradient:  0.16104428783861796
iteration : 13644
train acc:  0.75
train loss:  0.4762401580810547
train gradient:  0.15137109254805547
iteration : 13645
train acc:  0.765625
train loss:  0.4775475263595581
train gradient:  0.16078643710469734
iteration : 13646
train acc:  0.7578125
train loss:  0.45217031240463257
train gradient:  0.09922749117896919
iteration : 13647
train acc:  0.7578125
train loss:  0.4820947051048279
train gradient:  0.11674108590161356
iteration : 13648
train acc:  0.7421875
train loss:  0.5002495050430298
train gradient:  0.13130605423380542
iteration : 13649
train acc:  0.75
train loss:  0.4774380028247833
train gradient:  0.11432542709578883
iteration : 13650
train acc:  0.7265625
train loss:  0.538187563419342
train gradient:  0.12830676558556373
iteration : 13651
train acc:  0.796875
train loss:  0.39912715554237366
train gradient:  0.1041599646987789
iteration : 13652
train acc:  0.7734375
train loss:  0.4489785432815552
train gradient:  0.12339300790712487
iteration : 13653
train acc:  0.6796875
train loss:  0.5348762273788452
train gradient:  0.12326433788469962
iteration : 13654
train acc:  0.7265625
train loss:  0.457568496465683
train gradient:  0.10714164376104938
iteration : 13655
train acc:  0.7734375
train loss:  0.45778408646583557
train gradient:  0.11164907817162652
iteration : 13656
train acc:  0.75
train loss:  0.4943437874317169
train gradient:  0.1623598397287982
iteration : 13657
train acc:  0.7578125
train loss:  0.4465862512588501
train gradient:  0.10397416271638946
iteration : 13658
train acc:  0.7734375
train loss:  0.4641224145889282
train gradient:  0.10984098195880106
iteration : 13659
train acc:  0.796875
train loss:  0.4755820333957672
train gradient:  0.12425951912178322
iteration : 13660
train acc:  0.8046875
train loss:  0.4026329517364502
train gradient:  0.09548526309123596
iteration : 13661
train acc:  0.703125
train loss:  0.5807379484176636
train gradient:  0.19008856133682486
iteration : 13662
train acc:  0.7421875
train loss:  0.46033114194869995
train gradient:  0.10427440236090255
iteration : 13663
train acc:  0.7265625
train loss:  0.49504461884498596
train gradient:  0.11504929672882527
iteration : 13664
train acc:  0.75
train loss:  0.4842301905155182
train gradient:  0.1291167925835143
iteration : 13665
train acc:  0.71875
train loss:  0.48850005865097046
train gradient:  0.16131529165692612
iteration : 13666
train acc:  0.765625
train loss:  0.4227989912033081
train gradient:  0.10639731719131439
iteration : 13667
train acc:  0.7265625
train loss:  0.5334262847900391
train gradient:  0.12791662141948912
iteration : 13668
train acc:  0.71875
train loss:  0.4897068738937378
train gradient:  0.13338539984225156
iteration : 13669
train acc:  0.7890625
train loss:  0.4535352289676666
train gradient:  0.12252064188627039
iteration : 13670
train acc:  0.6796875
train loss:  0.584881603717804
train gradient:  0.2244119277144235
iteration : 13671
train acc:  0.7421875
train loss:  0.5104008913040161
train gradient:  0.1294268551250021
iteration : 13672
train acc:  0.765625
train loss:  0.4533049464225769
train gradient:  0.10528281251206009
iteration : 13673
train acc:  0.8046875
train loss:  0.4712928533554077
train gradient:  0.11091778017434928
iteration : 13674
train acc:  0.7734375
train loss:  0.48388299345970154
train gradient:  0.10583431005787952
iteration : 13675
train acc:  0.765625
train loss:  0.4716905355453491
train gradient:  0.11280098659906818
iteration : 13676
train acc:  0.71875
train loss:  0.5210351943969727
train gradient:  0.13650880774952953
iteration : 13677
train acc:  0.8125
train loss:  0.4783819615840912
train gradient:  0.1191013809720016
iteration : 13678
train acc:  0.734375
train loss:  0.5037078857421875
train gradient:  0.1130765354682816
iteration : 13679
train acc:  0.6875
train loss:  0.5740569829940796
train gradient:  0.16368018337638973
iteration : 13680
train acc:  0.6875
train loss:  0.5503377914428711
train gradient:  0.12378241062945447
iteration : 13681
train acc:  0.75
train loss:  0.5251836776733398
train gradient:  0.14931111125390129
iteration : 13682
train acc:  0.734375
train loss:  0.5197691917419434
train gradient:  0.10954530254860811
iteration : 13683
train acc:  0.8046875
train loss:  0.4617644250392914
train gradient:  0.1274384783965205
iteration : 13684
train acc:  0.8046875
train loss:  0.4928174614906311
train gradient:  0.15373238903231243
iteration : 13685
train acc:  0.7734375
train loss:  0.4796105623245239
train gradient:  0.15604537995177414
iteration : 13686
train acc:  0.7578125
train loss:  0.4921936094760895
train gradient:  0.13315444682514688
iteration : 13687
train acc:  0.7421875
train loss:  0.48817169666290283
train gradient:  0.1411101365980738
iteration : 13688
train acc:  0.7109375
train loss:  0.4784308671951294
train gradient:  0.16991387509037686
iteration : 13689
train acc:  0.75
train loss:  0.482510507106781
train gradient:  0.1131007956910836
iteration : 13690
train acc:  0.7421875
train loss:  0.4805758595466614
train gradient:  0.10783958511141598
iteration : 13691
train acc:  0.765625
train loss:  0.46469056606292725
train gradient:  0.10664400605015373
iteration : 13692
train acc:  0.7265625
train loss:  0.47352027893066406
train gradient:  0.1274171081092607
iteration : 13693
train acc:  0.8515625
train loss:  0.4404441714286804
train gradient:  0.09761420864724903
iteration : 13694
train acc:  0.765625
train loss:  0.46968021988868713
train gradient:  0.1142127053999647
iteration : 13695
train acc:  0.7265625
train loss:  0.4834986627101898
train gradient:  0.10898466618704733
iteration : 13696
train acc:  0.734375
train loss:  0.48991382122039795
train gradient:  0.11625317737093151
iteration : 13697
train acc:  0.8359375
train loss:  0.3910760283470154
train gradient:  0.07688329221099818
iteration : 13698
train acc:  0.7734375
train loss:  0.47761958837509155
train gradient:  0.13553920993617002
iteration : 13699
train acc:  0.78125
train loss:  0.42786574363708496
train gradient:  0.09935573270456727
iteration : 13700
train acc:  0.7890625
train loss:  0.5202330350875854
train gradient:  0.12233467008709646
iteration : 13701
train acc:  0.6953125
train loss:  0.5286068916320801
train gradient:  0.1363396313464952
iteration : 13702
train acc:  0.734375
train loss:  0.5035508275032043
train gradient:  0.13818889188505185
iteration : 13703
train acc:  0.7578125
train loss:  0.49729493260383606
train gradient:  0.13775142038850682
iteration : 13704
train acc:  0.765625
train loss:  0.4494887590408325
train gradient:  0.09164157654699599
iteration : 13705
train acc:  0.734375
train loss:  0.5699285864830017
train gradient:  0.14407444302047745
iteration : 13706
train acc:  0.78125
train loss:  0.48646366596221924
train gradient:  0.1788024505781209
iteration : 13707
train acc:  0.75
train loss:  0.4693675935268402
train gradient:  0.11338096563121741
iteration : 13708
train acc:  0.7421875
train loss:  0.48528388142585754
train gradient:  0.1227971068555343
iteration : 13709
train acc:  0.75
train loss:  0.5385072827339172
train gradient:  0.13955829230458272
iteration : 13710
train acc:  0.78125
train loss:  0.4906694293022156
train gradient:  0.13471605089245192
iteration : 13711
train acc:  0.7421875
train loss:  0.47362178564071655
train gradient:  0.1405507549954128
iteration : 13712
train acc:  0.7265625
train loss:  0.5229068994522095
train gradient:  0.12697295982333143
iteration : 13713
train acc:  0.7578125
train loss:  0.46084412932395935
train gradient:  0.10298839938070299
iteration : 13714
train acc:  0.8046875
train loss:  0.4072619676589966
train gradient:  0.10207642421617996
iteration : 13715
train acc:  0.6953125
train loss:  0.5375810861587524
train gradient:  0.14923479349474883
iteration : 13716
train acc:  0.8203125
train loss:  0.3950381278991699
train gradient:  0.08164795572461411
iteration : 13717
train acc:  0.828125
train loss:  0.44062694907188416
train gradient:  0.1087790592973187
iteration : 13718
train acc:  0.8203125
train loss:  0.4272328019142151
train gradient:  0.125418229788324
iteration : 13719
train acc:  0.75
train loss:  0.4535215198993683
train gradient:  0.12236274938072236
iteration : 13720
train acc:  0.796875
train loss:  0.41105902194976807
train gradient:  0.07329162034671087
iteration : 13721
train acc:  0.6953125
train loss:  0.5313740968704224
train gradient:  0.1479216963061134
iteration : 13722
train acc:  0.75
train loss:  0.5371994376182556
train gradient:  0.14158890714844224
iteration : 13723
train acc:  0.7109375
train loss:  0.48161518573760986
train gradient:  0.11091313978370639
iteration : 13724
train acc:  0.765625
train loss:  0.4221463203430176
train gradient:  0.090410875947946
iteration : 13725
train acc:  0.7578125
train loss:  0.48046639561653137
train gradient:  0.10419511832177021
iteration : 13726
train acc:  0.78125
train loss:  0.4278104305267334
train gradient:  0.10149363255588345
iteration : 13727
train acc:  0.78125
train loss:  0.4589044451713562
train gradient:  0.11433986555717002
iteration : 13728
train acc:  0.7421875
train loss:  0.4902591109275818
train gradient:  0.1165211710691014
iteration : 13729
train acc:  0.6875
train loss:  0.5364547967910767
train gradient:  0.1322286629264608
iteration : 13730
train acc:  0.6796875
train loss:  0.561739981174469
train gradient:  0.1580603847453243
iteration : 13731
train acc:  0.703125
train loss:  0.5335568189620972
train gradient:  0.15253760333232821
iteration : 13732
train acc:  0.7265625
train loss:  0.5339668989181519
train gradient:  0.16430791826892382
iteration : 13733
train acc:  0.734375
train loss:  0.487693190574646
train gradient:  0.1093345056220084
iteration : 13734
train acc:  0.7734375
train loss:  0.5117666721343994
train gradient:  0.13832092132651458
iteration : 13735
train acc:  0.7578125
train loss:  0.47965842485427856
train gradient:  0.10384150949086085
iteration : 13736
train acc:  0.7109375
train loss:  0.5235892534255981
train gradient:  0.153974216763189
iteration : 13737
train acc:  0.78125
train loss:  0.4534131586551666
train gradient:  0.10611643815272508
iteration : 13738
train acc:  0.7265625
train loss:  0.5318756103515625
train gradient:  0.16719147463702566
iteration : 13739
train acc:  0.7578125
train loss:  0.4533449113368988
train gradient:  0.12887040894420804
iteration : 13740
train acc:  0.7578125
train loss:  0.4707571268081665
train gradient:  0.1138600394689844
iteration : 13741
train acc:  0.734375
train loss:  0.5039628148078918
train gradient:  0.1252793244562631
iteration : 13742
train acc:  0.765625
train loss:  0.470997154712677
train gradient:  0.11767123826642309
iteration : 13743
train acc:  0.78125
train loss:  0.46726927161216736
train gradient:  0.13255763434660905
iteration : 13744
train acc:  0.7265625
train loss:  0.4748936891555786
train gradient:  0.11736895229406002
iteration : 13745
train acc:  0.703125
train loss:  0.5420225858688354
train gradient:  0.12197955792711335
iteration : 13746
train acc:  0.7890625
train loss:  0.4468857944011688
train gradient:  0.0971289809214686
iteration : 13747
train acc:  0.78125
train loss:  0.4658863842487335
train gradient:  0.10698436705448132
iteration : 13748
train acc:  0.734375
train loss:  0.46104326844215393
train gradient:  0.10985413784411112
iteration : 13749
train acc:  0.7421875
train loss:  0.48429906368255615
train gradient:  0.1252479455685504
iteration : 13750
train acc:  0.7109375
train loss:  0.5078017711639404
train gradient:  0.1471711305866914
iteration : 13751
train acc:  0.78125
train loss:  0.44542595744132996
train gradient:  0.11496637988609132
iteration : 13752
train acc:  0.8125
train loss:  0.42786943912506104
train gradient:  0.10677636601617996
iteration : 13753
train acc:  0.765625
train loss:  0.48577257990837097
train gradient:  0.11979417484777166
iteration : 13754
train acc:  0.796875
train loss:  0.4309927821159363
train gradient:  0.08662188709445831
iteration : 13755
train acc:  0.765625
train loss:  0.4702054262161255
train gradient:  0.11427235408752015
iteration : 13756
train acc:  0.75
train loss:  0.49661844968795776
train gradient:  0.1641976758237539
iteration : 13757
train acc:  0.78125
train loss:  0.44519561529159546
train gradient:  0.11270581416290462
iteration : 13758
train acc:  0.71875
train loss:  0.6119776964187622
train gradient:  0.20606366917246183
iteration : 13759
train acc:  0.75
train loss:  0.4573396146297455
train gradient:  0.10066688517526966
iteration : 13760
train acc:  0.734375
train loss:  0.49857544898986816
train gradient:  0.12801812572965676
iteration : 13761
train acc:  0.7890625
train loss:  0.46672242879867554
train gradient:  0.10371004146079346
iteration : 13762
train acc:  0.7578125
train loss:  0.46492236852645874
train gradient:  0.13602461051839587
iteration : 13763
train acc:  0.8046875
train loss:  0.4202662706375122
train gradient:  0.11217784284753542
iteration : 13764
train acc:  0.7578125
train loss:  0.4719763398170471
train gradient:  0.11761638761827255
iteration : 13765
train acc:  0.7421875
train loss:  0.4793270230293274
train gradient:  0.12740024456829233
iteration : 13766
train acc:  0.75
train loss:  0.47148627042770386
train gradient:  0.12645783186542292
iteration : 13767
train acc:  0.7421875
train loss:  0.5373095870018005
train gradient:  0.13588850241942535
iteration : 13768
train acc:  0.6328125
train loss:  0.5689777731895447
train gradient:  0.17327177557209283
iteration : 13769
train acc:  0.75
train loss:  0.5137345194816589
train gradient:  0.13698602352616174
iteration : 13770
train acc:  0.703125
train loss:  0.5287692546844482
train gradient:  0.14085919764901167
iteration : 13771
train acc:  0.703125
train loss:  0.5445232391357422
train gradient:  0.15125673553101948
iteration : 13772
train acc:  0.7734375
train loss:  0.4508363604545593
train gradient:  0.14361733724463585
iteration : 13773
train acc:  0.75
train loss:  0.4627610743045807
train gradient:  0.10315518575464631
iteration : 13774
train acc:  0.7734375
train loss:  0.49247923493385315
train gradient:  0.11012990162254105
iteration : 13775
train acc:  0.71875
train loss:  0.4903299808502197
train gradient:  0.14837586728853697
iteration : 13776
train acc:  0.71875
train loss:  0.5084301233291626
train gradient:  0.1126340341115492
iteration : 13777
train acc:  0.765625
train loss:  0.543035626411438
train gradient:  0.14921587227975314
iteration : 13778
train acc:  0.7578125
train loss:  0.4828692674636841
train gradient:  0.12373190448650451
iteration : 13779
train acc:  0.6875
train loss:  0.5622096061706543
train gradient:  0.18721157768424204
iteration : 13780
train acc:  0.7421875
train loss:  0.4193631708621979
train gradient:  0.07835668667006057
iteration : 13781
train acc:  0.6796875
train loss:  0.502129316329956
train gradient:  0.1146550280368
iteration : 13782
train acc:  0.7734375
train loss:  0.47525665163993835
train gradient:  0.13341623282858633
iteration : 13783
train acc:  0.734375
train loss:  0.5097067356109619
train gradient:  0.1357822211479838
iteration : 13784
train acc:  0.734375
train loss:  0.48411130905151367
train gradient:  0.12222919641118231
iteration : 13785
train acc:  0.8046875
train loss:  0.4164961874485016
train gradient:  0.09475488549457342
iteration : 13786
train acc:  0.7734375
train loss:  0.48057928681373596
train gradient:  0.13375896898167272
iteration : 13787
train acc:  0.71875
train loss:  0.49369722604751587
train gradient:  0.13086976274068357
iteration : 13788
train acc:  0.7734375
train loss:  0.45353075861930847
train gradient:  0.10662191542403619
iteration : 13789
train acc:  0.8203125
train loss:  0.3938371241092682
train gradient:  0.06798266699291308
iteration : 13790
train acc:  0.78125
train loss:  0.48275619745254517
train gradient:  0.11325752811509718
iteration : 13791
train acc:  0.71875
train loss:  0.4902803599834442
train gradient:  0.12789858865750958
iteration : 13792
train acc:  0.7109375
train loss:  0.5567871332168579
train gradient:  0.16242770439999904
iteration : 13793
train acc:  0.75
train loss:  0.47437286376953125
train gradient:  0.09469836078354599
iteration : 13794
train acc:  0.7265625
train loss:  0.5442023277282715
train gradient:  0.16488123446560907
iteration : 13795
train acc:  0.71875
train loss:  0.5562623739242554
train gradient:  0.15871855129573198
iteration : 13796
train acc:  0.7890625
train loss:  0.4356360137462616
train gradient:  0.11255881045059324
iteration : 13797
train acc:  0.7890625
train loss:  0.40394535660743713
train gradient:  0.0933393805982208
iteration : 13798
train acc:  0.734375
train loss:  0.5062915086746216
train gradient:  0.1379543212286188
iteration : 13799
train acc:  0.7890625
train loss:  0.42021504044532776
train gradient:  0.10020405018145184
iteration : 13800
train acc:  0.7109375
train loss:  0.5194450616836548
train gradient:  0.1274568653149586
iteration : 13801
train acc:  0.7734375
train loss:  0.4797385632991791
train gradient:  0.11408318410850811
iteration : 13802
train acc:  0.7578125
train loss:  0.4395483136177063
train gradient:  0.1137437658592573
iteration : 13803
train acc:  0.7421875
train loss:  0.5159540176391602
train gradient:  0.10638471456517219
iteration : 13804
train acc:  0.7265625
train loss:  0.5323631763458252
train gradient:  0.135358644581281
iteration : 13805
train acc:  0.78125
train loss:  0.47824627161026
train gradient:  0.10571828339959113
iteration : 13806
train acc:  0.75
train loss:  0.48495763540267944
train gradient:  0.1353716099148814
iteration : 13807
train acc:  0.71875
train loss:  0.5230971574783325
train gradient:  0.13039433201200112
iteration : 13808
train acc:  0.75
train loss:  0.4907839894294739
train gradient:  0.12699928855224257
iteration : 13809
train acc:  0.7578125
train loss:  0.45985591411590576
train gradient:  0.139600491342259
iteration : 13810
train acc:  0.7578125
train loss:  0.43606552481651306
train gradient:  0.1083822477381901
iteration : 13811
train acc:  0.765625
train loss:  0.4841960668563843
train gradient:  0.11128002134107248
iteration : 13812
train acc:  0.7578125
train loss:  0.43509405851364136
train gradient:  0.0978970098124904
iteration : 13813
train acc:  0.6640625
train loss:  0.5446626543998718
train gradient:  0.15797882077922382
iteration : 13814
train acc:  0.75
train loss:  0.475655734539032
train gradient:  0.1335400383619941
iteration : 13815
train acc:  0.7421875
train loss:  0.48292380571365356
train gradient:  0.11511071969335313
iteration : 13816
train acc:  0.78125
train loss:  0.46539992094039917
train gradient:  0.10539027753389743
iteration : 13817
train acc:  0.7421875
train loss:  0.4772200584411621
train gradient:  0.1630442351634917
iteration : 13818
train acc:  0.7421875
train loss:  0.4902433156967163
train gradient:  0.119590318967632
iteration : 13819
train acc:  0.6953125
train loss:  0.5532031655311584
train gradient:  0.13841097045333198
iteration : 13820
train acc:  0.7265625
train loss:  0.4846738874912262
train gradient:  0.11317691836739535
iteration : 13821
train acc:  0.78125
train loss:  0.4733474552631378
train gradient:  0.10328816794680178
iteration : 13822
train acc:  0.71875
train loss:  0.5747753381729126
train gradient:  0.1477971376150261
iteration : 13823
train acc:  0.78125
train loss:  0.42661887407302856
train gradient:  0.07772014161082726
iteration : 13824
train acc:  0.75
train loss:  0.4861370921134949
train gradient:  0.14246974695873965
iteration : 13825
train acc:  0.734375
train loss:  0.5148626565933228
train gradient:  0.1146038939196559
iteration : 13826
train acc:  0.734375
train loss:  0.5001183152198792
train gradient:  0.15907952935283523
iteration : 13827
train acc:  0.7734375
train loss:  0.4213074743747711
train gradient:  0.10978898930203355
iteration : 13828
train acc:  0.734375
train loss:  0.524572491645813
train gradient:  0.1387520557309764
iteration : 13829
train acc:  0.7265625
train loss:  0.5272473096847534
train gradient:  0.12628729931704397
iteration : 13830
train acc:  0.734375
train loss:  0.48603373765945435
train gradient:  0.11088546828273771
iteration : 13831
train acc:  0.78125
train loss:  0.42984429001808167
train gradient:  0.09676840442113886
iteration : 13832
train acc:  0.7890625
train loss:  0.4358171820640564
train gradient:  0.10460788920275937
iteration : 13833
train acc:  0.7421875
train loss:  0.5017592310905457
train gradient:  0.1651288916474069
iteration : 13834
train acc:  0.703125
train loss:  0.525010883808136
train gradient:  0.1600932040551555
iteration : 13835
train acc:  0.7734375
train loss:  0.4617980122566223
train gradient:  0.15956249168251044
iteration : 13836
train acc:  0.75
train loss:  0.4905126094818115
train gradient:  0.1213518607589701
iteration : 13837
train acc:  0.8125
train loss:  0.42768460512161255
train gradient:  0.11427140656821293
iteration : 13838
train acc:  0.71875
train loss:  0.5226082801818848
train gradient:  0.15250610902483133
iteration : 13839
train acc:  0.71875
train loss:  0.5581415891647339
train gradient:  0.1864487998887761
iteration : 13840
train acc:  0.7421875
train loss:  0.5241551399230957
train gradient:  0.15206174186785948
iteration : 13841
train acc:  0.796875
train loss:  0.4641089141368866
train gradient:  0.12737944567680776
iteration : 13842
train acc:  0.7734375
train loss:  0.4524065852165222
train gradient:  0.09405970979055969
iteration : 13843
train acc:  0.8125
train loss:  0.426896870136261
train gradient:  0.09687678461200055
iteration : 13844
train acc:  0.6953125
train loss:  0.5438162088394165
train gradient:  0.12627197024197578
iteration : 13845
train acc:  0.703125
train loss:  0.5156253576278687
train gradient:  0.13725482793332128
iteration : 13846
train acc:  0.78125
train loss:  0.47206249833106995
train gradient:  0.10739147062502077
iteration : 13847
train acc:  0.8203125
train loss:  0.45092400908470154
train gradient:  0.12990905449279572
iteration : 13848
train acc:  0.7421875
train loss:  0.5069337487220764
train gradient:  0.12170849347747066
iteration : 13849
train acc:  0.7265625
train loss:  0.4942469000816345
train gradient:  0.1304185586554254
iteration : 13850
train acc:  0.734375
train loss:  0.4707708954811096
train gradient:  0.11292758950349317
iteration : 13851
train acc:  0.7578125
train loss:  0.43032652139663696
train gradient:  0.0958404521412668
iteration : 13852
train acc:  0.7578125
train loss:  0.4972543716430664
train gradient:  0.12703811629003975
iteration : 13853
train acc:  0.75
train loss:  0.4916975498199463
train gradient:  0.13607648655659815
iteration : 13854
train acc:  0.7578125
train loss:  0.4884117841720581
train gradient:  0.1294822905773655
iteration : 13855
train acc:  0.7578125
train loss:  0.4544086158275604
train gradient:  0.1495787579921639
iteration : 13856
train acc:  0.7265625
train loss:  0.4807547628879547
train gradient:  0.10296637211221318
iteration : 13857
train acc:  0.6875
train loss:  0.5434423685073853
train gradient:  0.14202689079687913
iteration : 13858
train acc:  0.8046875
train loss:  0.45271098613739014
train gradient:  0.13135411482373205
iteration : 13859
train acc:  0.7734375
train loss:  0.4595763683319092
train gradient:  0.11340189781724905
iteration : 13860
train acc:  0.75
train loss:  0.47384536266326904
train gradient:  0.13008691902867608
iteration : 13861
train acc:  0.6875
train loss:  0.5644489526748657
train gradient:  0.17090917425137367
iteration : 13862
train acc:  0.734375
train loss:  0.49186110496520996
train gradient:  0.1285286874685989
iteration : 13863
train acc:  0.6640625
train loss:  0.5203565359115601
train gradient:  0.11473176949455238
iteration : 13864
train acc:  0.734375
train loss:  0.44746512174606323
train gradient:  0.11787958998062191
iteration : 13865
train acc:  0.796875
train loss:  0.45713305473327637
train gradient:  0.090686702720853
iteration : 13866
train acc:  0.7109375
train loss:  0.5984135866165161
train gradient:  0.20220700224293806
iteration : 13867
train acc:  0.796875
train loss:  0.45253226161003113
train gradient:  0.11751248087490956
iteration : 13868
train acc:  0.7109375
train loss:  0.5192152857780457
train gradient:  0.21348985949801924
iteration : 13869
train acc:  0.765625
train loss:  0.46248161792755127
train gradient:  0.16407038166993776
iteration : 13870
train acc:  0.7421875
train loss:  0.48133522272109985
train gradient:  0.11576641324229939
iteration : 13871
train acc:  0.796875
train loss:  0.4442251920700073
train gradient:  0.10838308171667596
iteration : 13872
train acc:  0.734375
train loss:  0.4873497188091278
train gradient:  0.13209911309719422
iteration : 13873
train acc:  0.6875
train loss:  0.5144956111907959
train gradient:  0.1347562075303706
iteration : 13874
train acc:  0.8125
train loss:  0.472764253616333
train gradient:  0.12505806309841544
iteration : 13875
train acc:  0.75
train loss:  0.5339205265045166
train gradient:  0.1300110158338103
iteration : 13876
train acc:  0.734375
train loss:  0.5011463165283203
train gradient:  0.14065527466795452
iteration : 13877
train acc:  0.7265625
train loss:  0.45717036724090576
train gradient:  0.0948762030486601
iteration : 13878
train acc:  0.8125
train loss:  0.4237845242023468
train gradient:  0.11825790790749284
iteration : 13879
train acc:  0.7890625
train loss:  0.4329826235771179
train gradient:  0.10470996475493731
iteration : 13880
train acc:  0.71875
train loss:  0.49902206659317017
train gradient:  0.11691461072617619
iteration : 13881
train acc:  0.7890625
train loss:  0.4691862463951111
train gradient:  0.12520221038056734
iteration : 13882
train acc:  0.7578125
train loss:  0.4790734052658081
train gradient:  0.11711060244756488
iteration : 13883
train acc:  0.6953125
train loss:  0.5381091237068176
train gradient:  0.13925601000440913
iteration : 13884
train acc:  0.7109375
train loss:  0.5796095728874207
train gradient:  0.15714617993422594
iteration : 13885
train acc:  0.65625
train loss:  0.5232158899307251
train gradient:  0.14780044441835805
iteration : 13886
train acc:  0.7734375
train loss:  0.4745386838912964
train gradient:  0.10632112000240529
iteration : 13887
train acc:  0.7734375
train loss:  0.47681474685668945
train gradient:  0.1256803486286761
iteration : 13888
train acc:  0.828125
train loss:  0.40658897161483765
train gradient:  0.09133978856143395
iteration : 13889
train acc:  0.6953125
train loss:  0.5710124969482422
train gradient:  0.17342100085338716
iteration : 13890
train acc:  0.71875
train loss:  0.5925251841545105
train gradient:  0.16699207580700345
iteration : 13891
train acc:  0.6953125
train loss:  0.5591350793838501
train gradient:  0.18652475391667342
iteration : 13892
train acc:  0.71875
train loss:  0.4535873830318451
train gradient:  0.08558183159163392
iteration : 13893
train acc:  0.6875
train loss:  0.5920010805130005
train gradient:  0.19654711249799903
iteration : 13894
train acc:  0.75
train loss:  0.4853789806365967
train gradient:  0.18130115529334828
iteration : 13895
train acc:  0.6796875
train loss:  0.4942905306816101
train gradient:  0.11535828270631801
iteration : 13896
train acc:  0.8046875
train loss:  0.47657185792922974
train gradient:  0.14679607615251394
iteration : 13897
train acc:  0.71875
train loss:  0.5429677367210388
train gradient:  0.11582678829361237
iteration : 13898
train acc:  0.7890625
train loss:  0.45972830057144165
train gradient:  0.10130762458076102
iteration : 13899
train acc:  0.7578125
train loss:  0.4558013081550598
train gradient:  0.10454743018364214
iteration : 13900
train acc:  0.6328125
train loss:  0.6001267433166504
train gradient:  0.20785510307029034
iteration : 13901
train acc:  0.7734375
train loss:  0.4470987021923065
train gradient:  0.129926488387402
iteration : 13902
train acc:  0.765625
train loss:  0.44204846024513245
train gradient:  0.10894289089523133
iteration : 13903
train acc:  0.765625
train loss:  0.46746599674224854
train gradient:  0.1426064450212606
iteration : 13904
train acc:  0.7109375
train loss:  0.571640133857727
train gradient:  0.14918764558783248
iteration : 13905
train acc:  0.7578125
train loss:  0.5182857513427734
train gradient:  0.13133916341092616
iteration : 13906
train acc:  0.8046875
train loss:  0.44401347637176514
train gradient:  0.10308193459909458
iteration : 13907
train acc:  0.7734375
train loss:  0.4748375713825226
train gradient:  0.12354023600359768
iteration : 13908
train acc:  0.703125
train loss:  0.5432487726211548
train gradient:  0.1558344211628201
iteration : 13909
train acc:  0.7109375
train loss:  0.4895671010017395
train gradient:  0.10797242768298272
iteration : 13910
train acc:  0.7265625
train loss:  0.4827788472175598
train gradient:  0.12473167692084476
iteration : 13911
train acc:  0.6875
train loss:  0.5514440536499023
train gradient:  0.20402324086758955
iteration : 13912
train acc:  0.7109375
train loss:  0.5549121499061584
train gradient:  0.1573205915214969
iteration : 13913
train acc:  0.7421875
train loss:  0.4806572198867798
train gradient:  0.1336456783870468
iteration : 13914
train acc:  0.75
train loss:  0.4108664393424988
train gradient:  0.08559450828395958
iteration : 13915
train acc:  0.796875
train loss:  0.40899503231048584
train gradient:  0.08788313727926911
iteration : 13916
train acc:  0.7421875
train loss:  0.48245522379875183
train gradient:  0.12136604568751354
iteration : 13917
train acc:  0.734375
train loss:  0.5046967267990112
train gradient:  0.13400832335980142
iteration : 13918
train acc:  0.78125
train loss:  0.5013337135314941
train gradient:  0.13946985026400263
iteration : 13919
train acc:  0.734375
train loss:  0.4752313792705536
train gradient:  0.1118009379183908
iteration : 13920
train acc:  0.796875
train loss:  0.44073808193206787
train gradient:  0.09330192694607842
iteration : 13921
train acc:  0.75
train loss:  0.541867196559906
train gradient:  0.18582263112892117
iteration : 13922
train acc:  0.8046875
train loss:  0.44132930040359497
train gradient:  0.1390891628111436
iteration : 13923
train acc:  0.703125
train loss:  0.5091750025749207
train gradient:  0.14211506718678904
iteration : 13924
train acc:  0.7109375
train loss:  0.5307711362838745
train gradient:  0.12344829373592615
iteration : 13925
train acc:  0.75
train loss:  0.47730332612991333
train gradient:  0.11197174857753088
iteration : 13926
train acc:  0.7265625
train loss:  0.5477365255355835
train gradient:  0.1589646511417595
iteration : 13927
train acc:  0.7734375
train loss:  0.43697160482406616
train gradient:  0.07766027625781802
iteration : 13928
train acc:  0.7734375
train loss:  0.47530436515808105
train gradient:  0.11368918096308912
iteration : 13929
train acc:  0.7734375
train loss:  0.46954670548439026
train gradient:  0.1484876170489488
iteration : 13930
train acc:  0.671875
train loss:  0.4886936545372009
train gradient:  0.09870200166433428
iteration : 13931
train acc:  0.7578125
train loss:  0.4767245650291443
train gradient:  0.10976131654644429
iteration : 13932
train acc:  0.734375
train loss:  0.4975942373275757
train gradient:  0.12183904287951965
iteration : 13933
train acc:  0.640625
train loss:  0.5786586999893188
train gradient:  0.15965708627066147
iteration : 13934
train acc:  0.75
train loss:  0.49600327014923096
train gradient:  0.11093294337373738
iteration : 13935
train acc:  0.7421875
train loss:  0.4855012893676758
train gradient:  0.11448635724330887
iteration : 13936
train acc:  0.7421875
train loss:  0.4943029582500458
train gradient:  0.13213068432276212
iteration : 13937
train acc:  0.75
train loss:  0.5027350187301636
train gradient:  0.1400533804826854
iteration : 13938
train acc:  0.7421875
train loss:  0.46714842319488525
train gradient:  0.12622944895242205
iteration : 13939
train acc:  0.78125
train loss:  0.4340690076351166
train gradient:  0.10757174625356762
iteration : 13940
train acc:  0.71875
train loss:  0.5291775465011597
train gradient:  0.1313142694201882
iteration : 13941
train acc:  0.734375
train loss:  0.49598509073257446
train gradient:  0.12354932555294908
iteration : 13942
train acc:  0.765625
train loss:  0.45200005173683167
train gradient:  0.1370052035120715
iteration : 13943
train acc:  0.7421875
train loss:  0.48932331800460815
train gradient:  0.12263529462641354
iteration : 13944
train acc:  0.6796875
train loss:  0.5633605718612671
train gradient:  0.18013786406930954
iteration : 13945
train acc:  0.7578125
train loss:  0.47034695744514465
train gradient:  0.10442385311688969
iteration : 13946
train acc:  0.7109375
train loss:  0.5404015183448792
train gradient:  0.21477216454466544
iteration : 13947
train acc:  0.6875
train loss:  0.525084376335144
train gradient:  0.13636912656792344
iteration : 13948
train acc:  0.828125
train loss:  0.4311599135398865
train gradient:  0.1156724220120864
iteration : 13949
train acc:  0.7734375
train loss:  0.41152074933052063
train gradient:  0.09321637305301406
iteration : 13950
train acc:  0.7265625
train loss:  0.5012272000312805
train gradient:  0.12357751529495736
iteration : 13951
train acc:  0.7265625
train loss:  0.540492594242096
train gradient:  0.12092124406898293
iteration : 13952
train acc:  0.71875
train loss:  0.5393282175064087
train gradient:  0.18708989397356637
iteration : 13953
train acc:  0.734375
train loss:  0.5444915890693665
train gradient:  0.14370756149447944
iteration : 13954
train acc:  0.6796875
train loss:  0.5685622096061707
train gradient:  0.18558140087108094
iteration : 13955
train acc:  0.71875
train loss:  0.5375012159347534
train gradient:  0.14108607579163784
iteration : 13956
train acc:  0.6875
train loss:  0.4811190962791443
train gradient:  0.11100941360050871
iteration : 13957
train acc:  0.7734375
train loss:  0.4735044836997986
train gradient:  0.1353391480376926
iteration : 13958
train acc:  0.78125
train loss:  0.4839097857475281
train gradient:  0.14305663530642793
iteration : 13959
train acc:  0.6953125
train loss:  0.5307456851005554
train gradient:  0.16605367741905747
iteration : 13960
train acc:  0.7890625
train loss:  0.5059792399406433
train gradient:  0.14207768326063297
iteration : 13961
train acc:  0.7734375
train loss:  0.4768487811088562
train gradient:  0.10378098492898262
iteration : 13962
train acc:  0.7421875
train loss:  0.5184537172317505
train gradient:  0.138918293087414
iteration : 13963
train acc:  0.7890625
train loss:  0.4953150153160095
train gradient:  0.12832153235022611
iteration : 13964
train acc:  0.7421875
train loss:  0.49303391575813293
train gradient:  0.12292245560111444
iteration : 13965
train acc:  0.7109375
train loss:  0.48090046644210815
train gradient:  0.11704537249996406
iteration : 13966
train acc:  0.7578125
train loss:  0.4346582591533661
train gradient:  0.10657827223060401
iteration : 13967
train acc:  0.7578125
train loss:  0.47250688076019287
train gradient:  0.10023332175288578
iteration : 13968
train acc:  0.7421875
train loss:  0.5342971086502075
train gradient:  0.13198406285254244
iteration : 13969
train acc:  0.734375
train loss:  0.5202838778495789
train gradient:  0.1330384955455991
iteration : 13970
train acc:  0.75
train loss:  0.5289797186851501
train gradient:  0.13285015591661758
iteration : 13971
train acc:  0.7265625
train loss:  0.5134844779968262
train gradient:  0.1656463859567064
iteration : 13972
train acc:  0.78125
train loss:  0.49503612518310547
train gradient:  0.11406955664040536
iteration : 13973
train acc:  0.8046875
train loss:  0.4040520489215851
train gradient:  0.07865571379018814
iteration : 13974
train acc:  0.796875
train loss:  0.44128096103668213
train gradient:  0.09997340899487649
iteration : 13975
train acc:  0.6875
train loss:  0.5468343496322632
train gradient:  0.16934422419246348
iteration : 13976
train acc:  0.7109375
train loss:  0.538968563079834
train gradient:  0.13375927696587586
iteration : 13977
train acc:  0.7890625
train loss:  0.4441055357456207
train gradient:  0.10347526484275113
iteration : 13978
train acc:  0.703125
train loss:  0.5279130935668945
train gradient:  0.11793590317554922
iteration : 13979
train acc:  0.7265625
train loss:  0.4839697480201721
train gradient:  0.09320117652637874
iteration : 13980
train acc:  0.7578125
train loss:  0.3967305123806
train gradient:  0.09075213510964562
iteration : 13981
train acc:  0.703125
train loss:  0.5295474529266357
train gradient:  0.1443292291346212
iteration : 13982
train acc:  0.734375
train loss:  0.5097277164459229
train gradient:  0.11290545334682707
iteration : 13983
train acc:  0.734375
train loss:  0.44524621963500977
train gradient:  0.09089019680358013
iteration : 13984
train acc:  0.7578125
train loss:  0.46500325202941895
train gradient:  0.10149010519705977
iteration : 13985
train acc:  0.78125
train loss:  0.4501098692417145
train gradient:  0.1274128494493164
iteration : 13986
train acc:  0.7734375
train loss:  0.4348808526992798
train gradient:  0.09063783915712227
iteration : 13987
train acc:  0.796875
train loss:  0.4181969463825226
train gradient:  0.1259622395232336
iteration : 13988
train acc:  0.734375
train loss:  0.4607604146003723
train gradient:  0.09064320934339555
iteration : 13989
train acc:  0.8125
train loss:  0.43690770864486694
train gradient:  0.11875263371761781
iteration : 13990
train acc:  0.7734375
train loss:  0.4934113323688507
train gradient:  0.12378965126518758
iteration : 13991
train acc:  0.734375
train loss:  0.5101312398910522
train gradient:  0.12018573180991586
iteration : 13992
train acc:  0.8046875
train loss:  0.4446341097354889
train gradient:  0.12530600206581022
iteration : 13993
train acc:  0.75
train loss:  0.4899817109107971
train gradient:  0.13265102881726304
iteration : 13994
train acc:  0.734375
train loss:  0.5017126202583313
train gradient:  0.1258419404114688
iteration : 13995
train acc:  0.78125
train loss:  0.3956841230392456
train gradient:  0.08112395303330655
iteration : 13996
train acc:  0.7734375
train loss:  0.4388940632343292
train gradient:  0.10443834538354896
iteration : 13997
train acc:  0.734375
train loss:  0.4842178225517273
train gradient:  0.13686861535123523
iteration : 13998
train acc:  0.71875
train loss:  0.5615653991699219
train gradient:  0.16182168487473242
iteration : 13999
train acc:  0.7578125
train loss:  0.48667412996292114
train gradient:  0.12912787164304862
iteration : 14000
train acc:  0.6953125
train loss:  0.5858392715454102
train gradient:  0.18182709600866268
iteration : 14001
train acc:  0.78125
train loss:  0.451054185628891
train gradient:  0.11682301986910212
iteration : 14002
train acc:  0.75
train loss:  0.5505861043930054
train gradient:  0.14305104942455052
iteration : 14003
train acc:  0.7578125
train loss:  0.44277292490005493
train gradient:  0.0959183931520763
iteration : 14004
train acc:  0.6875
train loss:  0.5743419528007507
train gradient:  0.1766678244802983
iteration : 14005
train acc:  0.796875
train loss:  0.5053936839103699
train gradient:  0.17137004401471406
iteration : 14006
train acc:  0.7734375
train loss:  0.4833373725414276
train gradient:  0.14186068266165008
iteration : 14007
train acc:  0.7890625
train loss:  0.42637041211128235
train gradient:  0.10249691671339137
iteration : 14008
train acc:  0.7890625
train loss:  0.4873451590538025
train gradient:  0.1484972121390366
iteration : 14009
train acc:  0.828125
train loss:  0.37709322571754456
train gradient:  0.08328312529708704
iteration : 14010
train acc:  0.7734375
train loss:  0.4883820414543152
train gradient:  0.11767712600399124
iteration : 14011
train acc:  0.734375
train loss:  0.5013643503189087
train gradient:  0.1544535564519111
iteration : 14012
train acc:  0.6953125
train loss:  0.5205373167991638
train gradient:  0.14980112496570983
iteration : 14013
train acc:  0.75
train loss:  0.5315496921539307
train gradient:  0.16516434441297206
iteration : 14014
train acc:  0.78125
train loss:  0.5010973215103149
train gradient:  0.10114112281445799
iteration : 14015
train acc:  0.7421875
train loss:  0.45624300837516785
train gradient:  0.11449663627046427
iteration : 14016
train acc:  0.7578125
train loss:  0.4753417670726776
train gradient:  0.11972487622162789
iteration : 14017
train acc:  0.6796875
train loss:  0.5649397373199463
train gradient:  0.20301469316539533
iteration : 14018
train acc:  0.78125
train loss:  0.45885783433914185
train gradient:  0.11078888040654858
iteration : 14019
train acc:  0.7890625
train loss:  0.4644413888454437
train gradient:  0.10659210796552196
iteration : 14020
train acc:  0.7890625
train loss:  0.4912164807319641
train gradient:  0.1172683916547432
iteration : 14021
train acc:  0.765625
train loss:  0.5104478001594543
train gradient:  0.12523285197930784
iteration : 14022
train acc:  0.7890625
train loss:  0.5304511785507202
train gradient:  0.1664797337505557
iteration : 14023
train acc:  0.7890625
train loss:  0.5252076387405396
train gradient:  0.1362727569988309
iteration : 14024
train acc:  0.78125
train loss:  0.46951788663864136
train gradient:  0.14392935261453024
iteration : 14025
train acc:  0.828125
train loss:  0.37781521677970886
train gradient:  0.07995427080895393
iteration : 14026
train acc:  0.75
train loss:  0.4504595398902893
train gradient:  0.12498400699681052
iteration : 14027
train acc:  0.8203125
train loss:  0.44166624546051025
train gradient:  0.10896925679993516
iteration : 14028
train acc:  0.8125
train loss:  0.41358232498168945
train gradient:  0.09925434689355746
iteration : 14029
train acc:  0.7890625
train loss:  0.43973392248153687
train gradient:  0.0966491884371573
iteration : 14030
train acc:  0.7890625
train loss:  0.46183615922927856
train gradient:  0.10391502974974891
iteration : 14031
train acc:  0.734375
train loss:  0.4652895927429199
train gradient:  0.11346283183956149
iteration : 14032
train acc:  0.7578125
train loss:  0.482261061668396
train gradient:  0.13290151087127117
iteration : 14033
train acc:  0.765625
train loss:  0.5005975961685181
train gradient:  0.1396430411058156
iteration : 14034
train acc:  0.75
train loss:  0.450805127620697
train gradient:  0.10634503030677636
iteration : 14035
train acc:  0.78125
train loss:  0.4787549078464508
train gradient:  0.11016237453504335
iteration : 14036
train acc:  0.7109375
train loss:  0.5224817991256714
train gradient:  0.145042227529706
iteration : 14037
train acc:  0.7421875
train loss:  0.4711730480194092
train gradient:  0.1119582035155532
iteration : 14038
train acc:  0.703125
train loss:  0.5260835886001587
train gradient:  0.160326984857213
iteration : 14039
train acc:  0.703125
train loss:  0.5376306772232056
train gradient:  0.1467460433501214
iteration : 14040
train acc:  0.7109375
train loss:  0.505082368850708
train gradient:  0.12050402232276733
iteration : 14041
train acc:  0.765625
train loss:  0.4424951672554016
train gradient:  0.10071585510111179
iteration : 14042
train acc:  0.75
train loss:  0.49839383363723755
train gradient:  0.11017053093151263
iteration : 14043
train acc:  0.6875
train loss:  0.5274324417114258
train gradient:  0.10533601901361468
iteration : 14044
train acc:  0.7109375
train loss:  0.538613498210907
train gradient:  0.13535831495513762
iteration : 14045
train acc:  0.7421875
train loss:  0.5144851207733154
train gradient:  0.12673949070353924
iteration : 14046
train acc:  0.6875
train loss:  0.524476170539856
train gradient:  0.1297583271363613
iteration : 14047
train acc:  0.7265625
train loss:  0.4926014542579651
train gradient:  0.18466511957801127
iteration : 14048
train acc:  0.71875
train loss:  0.5379533767700195
train gradient:  0.12255361672181989
iteration : 14049
train acc:  0.65625
train loss:  0.5619626641273499
train gradient:  0.15609748583654287
iteration : 14050
train acc:  0.75
train loss:  0.4842541217803955
train gradient:  0.14532827749120736
iteration : 14051
train acc:  0.78125
train loss:  0.5297626256942749
train gradient:  0.1405151141975357
iteration : 14052
train acc:  0.71875
train loss:  0.5746138691902161
train gradient:  0.15712526091922452
iteration : 14053
train acc:  0.8046875
train loss:  0.4725394546985626
train gradient:  0.1215816589086344
iteration : 14054
train acc:  0.765625
train loss:  0.4962936043739319
train gradient:  0.112395221097827
iteration : 14055
train acc:  0.7578125
train loss:  0.47124722599983215
train gradient:  0.09124571843271538
iteration : 14056
train acc:  0.765625
train loss:  0.4999016523361206
train gradient:  0.1270414234440196
iteration : 14057
train acc:  0.78125
train loss:  0.4469708502292633
train gradient:  0.12210834463328887
iteration : 14058
train acc:  0.7109375
train loss:  0.5045434236526489
train gradient:  0.11959913502076447
iteration : 14059
train acc:  0.7890625
train loss:  0.45518767833709717
train gradient:  0.12754789278628714
iteration : 14060
train acc:  0.71875
train loss:  0.47060638666152954
train gradient:  0.12770186154239016
iteration : 14061
train acc:  0.703125
train loss:  0.48653167486190796
train gradient:  0.11355873942461892
iteration : 14062
train acc:  0.75
train loss:  0.4849085807800293
train gradient:  0.1450058743440647
iteration : 14063
train acc:  0.71875
train loss:  0.5225696563720703
train gradient:  0.12417903779187756
iteration : 14064
train acc:  0.75
train loss:  0.45577508211135864
train gradient:  0.10323811636078602
iteration : 14065
train acc:  0.7734375
train loss:  0.47036176919937134
train gradient:  0.09726354867887148
iteration : 14066
train acc:  0.796875
train loss:  0.4378625750541687
train gradient:  0.08975143863075168
iteration : 14067
train acc:  0.7265625
train loss:  0.5208498239517212
train gradient:  0.16337732932393267
iteration : 14068
train acc:  0.75
train loss:  0.5203649997711182
train gradient:  0.1395707381193066
iteration : 14069
train acc:  0.7421875
train loss:  0.4701184034347534
train gradient:  0.14154806498916772
iteration : 14070
train acc:  0.703125
train loss:  0.47898003458976746
train gradient:  0.1250347456180504
iteration : 14071
train acc:  0.7421875
train loss:  0.47672390937805176
train gradient:  0.13456575630544654
iteration : 14072
train acc:  0.7109375
train loss:  0.542270839214325
train gradient:  0.13114848332871054
iteration : 14073
train acc:  0.7578125
train loss:  0.47153159976005554
train gradient:  0.11212029783804203
iteration : 14074
train acc:  0.7421875
train loss:  0.5940972566604614
train gradient:  0.1797779651088634
iteration : 14075
train acc:  0.703125
train loss:  0.5463014841079712
train gradient:  0.14121656490697237
iteration : 14076
train acc:  0.7265625
train loss:  0.5082314610481262
train gradient:  0.13940210614071957
iteration : 14077
train acc:  0.734375
train loss:  0.47725969552993774
train gradient:  0.10737795949821773
iteration : 14078
train acc:  0.75
train loss:  0.511515736579895
train gradient:  0.1326574949415435
iteration : 14079
train acc:  0.7734375
train loss:  0.43784981966018677
train gradient:  0.10883862904799145
iteration : 14080
train acc:  0.765625
train loss:  0.46426695585250854
train gradient:  0.0968922219304024
iteration : 14081
train acc:  0.671875
train loss:  0.5559317469596863
train gradient:  0.1746594285408431
iteration : 14082
train acc:  0.7421875
train loss:  0.5476747751235962
train gradient:  0.15413748525174623
iteration : 14083
train acc:  0.7734375
train loss:  0.462515652179718
train gradient:  0.10039269925984132
iteration : 14084
train acc:  0.796875
train loss:  0.41666319966316223
train gradient:  0.07895975238737299
iteration : 14085
train acc:  0.75
train loss:  0.47278833389282227
train gradient:  0.11217672412640434
iteration : 14086
train acc:  0.71875
train loss:  0.5077884197235107
train gradient:  0.13737929749040526
iteration : 14087
train acc:  0.7109375
train loss:  0.5086814761161804
train gradient:  0.11544751668336578
iteration : 14088
train acc:  0.7734375
train loss:  0.4557797312736511
train gradient:  0.12616977817973787
iteration : 14089
train acc:  0.7109375
train loss:  0.5768574476242065
train gradient:  0.14993508102898684
iteration : 14090
train acc:  0.8125
train loss:  0.4119574725627899
train gradient:  0.0923436423328728
iteration : 14091
train acc:  0.7890625
train loss:  0.44305306673049927
train gradient:  0.12742831553247227
iteration : 14092
train acc:  0.703125
train loss:  0.48521214723587036
train gradient:  0.09855150736892068
iteration : 14093
train acc:  0.703125
train loss:  0.5217459797859192
train gradient:  0.11462666878003258
iteration : 14094
train acc:  0.8203125
train loss:  0.4241807162761688
train gradient:  0.09698932604302901
iteration : 14095
train acc:  0.7578125
train loss:  0.48361682891845703
train gradient:  0.1013418707800237
iteration : 14096
train acc:  0.6640625
train loss:  0.5667394995689392
train gradient:  0.1416138768735389
iteration : 14097
train acc:  0.7265625
train loss:  0.5022446513175964
train gradient:  0.14582008117811107
iteration : 14098
train acc:  0.7578125
train loss:  0.48573726415634155
train gradient:  0.11312303376395622
iteration : 14099
train acc:  0.71875
train loss:  0.5163695216178894
train gradient:  0.12047138154073596
iteration : 14100
train acc:  0.71875
train loss:  0.5212798714637756
train gradient:  0.155542700502273
iteration : 14101
train acc:  0.796875
train loss:  0.4420178532600403
train gradient:  0.10681403761971643
iteration : 14102
train acc:  0.7734375
train loss:  0.4622344970703125
train gradient:  0.11249087053997617
iteration : 14103
train acc:  0.7265625
train loss:  0.48034557700157166
train gradient:  0.10769359074109913
iteration : 14104
train acc:  0.703125
train loss:  0.5058764219284058
train gradient:  0.10851571122300502
iteration : 14105
train acc:  0.71875
train loss:  0.5113554000854492
train gradient:  0.12650919639027206
iteration : 14106
train acc:  0.765625
train loss:  0.47097963094711304
train gradient:  0.10843302484049301
iteration : 14107
train acc:  0.7890625
train loss:  0.43522191047668457
train gradient:  0.10757336118075954
iteration : 14108
train acc:  0.75
train loss:  0.48268795013427734
train gradient:  0.14518466996388274
iteration : 14109
train acc:  0.78125
train loss:  0.45346158742904663
train gradient:  0.08967105243328014
iteration : 14110
train acc:  0.7734375
train loss:  0.45903223752975464
train gradient:  0.11634251540684563
iteration : 14111
train acc:  0.734375
train loss:  0.5173962116241455
train gradient:  0.1403840538304353
iteration : 14112
train acc:  0.78125
train loss:  0.48758453130722046
train gradient:  0.11781874777397357
iteration : 14113
train acc:  0.6875
train loss:  0.5047339200973511
train gradient:  0.13797741080412024
iteration : 14114
train acc:  0.7265625
train loss:  0.5061783790588379
train gradient:  0.14367754901569368
iteration : 14115
train acc:  0.734375
train loss:  0.5270550847053528
train gradient:  0.12524911655934848
iteration : 14116
train acc:  0.78125
train loss:  0.44040125608444214
train gradient:  0.12122196689115274
iteration : 14117
train acc:  0.828125
train loss:  0.4318035840988159
train gradient:  0.08698173354372139
iteration : 14118
train acc:  0.7109375
train loss:  0.5653076171875
train gradient:  0.21271278254065085
iteration : 14119
train acc:  0.7734375
train loss:  0.45418688654899597
train gradient:  0.11178168166509413
iteration : 14120
train acc:  0.7421875
train loss:  0.5014603137969971
train gradient:  0.1764691103962134
iteration : 14121
train acc:  0.6875
train loss:  0.5655356645584106
train gradient:  0.17274461637674382
iteration : 14122
train acc:  0.7265625
train loss:  0.5000718832015991
train gradient:  0.10333057699884236
iteration : 14123
train acc:  0.6796875
train loss:  0.6035693287849426
train gradient:  0.2474359850303846
iteration : 14124
train acc:  0.765625
train loss:  0.5049111247062683
train gradient:  0.10713286352137043
iteration : 14125
train acc:  0.71875
train loss:  0.5188339948654175
train gradient:  0.13094440290461862
iteration : 14126
train acc:  0.7734375
train loss:  0.45744583010673523
train gradient:  0.11005141993375517
iteration : 14127
train acc:  0.75
train loss:  0.5835700631141663
train gradient:  0.1863981283352839
iteration : 14128
train acc:  0.765625
train loss:  0.4584551751613617
train gradient:  0.10278962988387672
iteration : 14129
train acc:  0.7578125
train loss:  0.4634491801261902
train gradient:  0.10704029784053921
iteration : 14130
train acc:  0.7734375
train loss:  0.45136919617652893
train gradient:  0.11264952192804914
iteration : 14131
train acc:  0.7890625
train loss:  0.48713910579681396
train gradient:  0.09697068877071444
iteration : 14132
train acc:  0.7421875
train loss:  0.5340492725372314
train gradient:  0.12354702022608274
iteration : 14133
train acc:  0.765625
train loss:  0.46448904275894165
train gradient:  0.10066702148552076
iteration : 14134
train acc:  0.7890625
train loss:  0.46927446126937866
train gradient:  0.16570680831645823
iteration : 14135
train acc:  0.796875
train loss:  0.42618805170059204
train gradient:  0.11520117953567216
iteration : 14136
train acc:  0.8046875
train loss:  0.39204859733581543
train gradient:  0.08619781667502455
iteration : 14137
train acc:  0.765625
train loss:  0.4234730899333954
train gradient:  0.09425346575524007
iteration : 14138
train acc:  0.640625
train loss:  0.5713929533958435
train gradient:  0.14136617658044515
iteration : 14139
train acc:  0.8203125
train loss:  0.41823810338974
train gradient:  0.10471024702288456
iteration : 14140
train acc:  0.78125
train loss:  0.4533481001853943
train gradient:  0.11524027442322703
iteration : 14141
train acc:  0.7265625
train loss:  0.5343369245529175
train gradient:  0.15984077661247537
iteration : 14142
train acc:  0.8125
train loss:  0.4546969532966614
train gradient:  0.09754709327469747
iteration : 14143
train acc:  0.734375
train loss:  0.5040603876113892
train gradient:  0.1465615683050629
iteration : 14144
train acc:  0.765625
train loss:  0.48468053340911865
train gradient:  0.17991958176692185
iteration : 14145
train acc:  0.7421875
train loss:  0.4605850577354431
train gradient:  0.09568212914777678
iteration : 14146
train acc:  0.75
train loss:  0.44599035382270813
train gradient:  0.07850239315053358
iteration : 14147
train acc:  0.703125
train loss:  0.6105894446372986
train gradient:  0.17857643560536107
iteration : 14148
train acc:  0.75
train loss:  0.5129121541976929
train gradient:  0.13895522361816265
iteration : 14149
train acc:  0.71875
train loss:  0.5104453563690186
train gradient:  0.1321417767009162
iteration : 14150
train acc:  0.7265625
train loss:  0.4544452130794525
train gradient:  0.11512680134976651
iteration : 14151
train acc:  0.7109375
train loss:  0.5141265392303467
train gradient:  0.11773786555065019
iteration : 14152
train acc:  0.7421875
train loss:  0.4677490293979645
train gradient:  0.1009866447730641
iteration : 14153
train acc:  0.78125
train loss:  0.46822217106819153
train gradient:  0.1032127945148695
iteration : 14154
train acc:  0.8046875
train loss:  0.45618191361427307
train gradient:  0.10902013909878822
iteration : 14155
train acc:  0.671875
train loss:  0.5534461736679077
train gradient:  0.15008931916771262
iteration : 14156
train acc:  0.6953125
train loss:  0.532789945602417
train gradient:  0.13183993317850579
iteration : 14157
train acc:  0.7421875
train loss:  0.4929024577140808
train gradient:  0.10769599293701505
iteration : 14158
train acc:  0.8046875
train loss:  0.43922287225723267
train gradient:  0.09454128770663518
iteration : 14159
train acc:  0.796875
train loss:  0.4255818724632263
train gradient:  0.11369835973805696
iteration : 14160
train acc:  0.7734375
train loss:  0.4571428894996643
train gradient:  0.1121146730102742
iteration : 14161
train acc:  0.6875
train loss:  0.5095734596252441
train gradient:  0.12811961881509704
iteration : 14162
train acc:  0.7421875
train loss:  0.49507564306259155
train gradient:  0.09244757758908903
iteration : 14163
train acc:  0.6953125
train loss:  0.5602452158927917
train gradient:  0.1658367190559381
iteration : 14164
train acc:  0.734375
train loss:  0.5065662264823914
train gradient:  0.11734193467264403
iteration : 14165
train acc:  0.703125
train loss:  0.5508596897125244
train gradient:  0.14243325842854018
iteration : 14166
train acc:  0.765625
train loss:  0.49532651901245117
train gradient:  0.11945361878568751
iteration : 14167
train acc:  0.7890625
train loss:  0.47480064630508423
train gradient:  0.13336638468856857
iteration : 14168
train acc:  0.75
train loss:  0.5015212893486023
train gradient:  0.1586252035893242
iteration : 14169
train acc:  0.6953125
train loss:  0.5701247453689575
train gradient:  0.1554775671421281
iteration : 14170
train acc:  0.8125
train loss:  0.4125547409057617
train gradient:  0.09542069668887139
iteration : 14171
train acc:  0.71875
train loss:  0.5249720811843872
train gradient:  0.1539501990597408
iteration : 14172
train acc:  0.796875
train loss:  0.438447505235672
train gradient:  0.08976439048487021
iteration : 14173
train acc:  0.7109375
train loss:  0.5394446849822998
train gradient:  0.14010337624913105
iteration : 14174
train acc:  0.8046875
train loss:  0.43699851632118225
train gradient:  0.11993792769239511
iteration : 14175
train acc:  0.7421875
train loss:  0.4778037369251251
train gradient:  0.11589923716901733
iteration : 14176
train acc:  0.7734375
train loss:  0.4911019802093506
train gradient:  0.0977980803988589
iteration : 14177
train acc:  0.828125
train loss:  0.4454723596572876
train gradient:  0.11065973071121392
iteration : 14178
train acc:  0.765625
train loss:  0.4786381125450134
train gradient:  0.11967942446656908
iteration : 14179
train acc:  0.78125
train loss:  0.45260483026504517
train gradient:  0.080700069271173
iteration : 14180
train acc:  0.7734375
train loss:  0.4529683589935303
train gradient:  0.1039760319175065
iteration : 14181
train acc:  0.765625
train loss:  0.46814343333244324
train gradient:  0.11780521896831853
iteration : 14182
train acc:  0.7578125
train loss:  0.5291761159896851
train gradient:  0.11117604648348203
iteration : 14183
train acc:  0.6640625
train loss:  0.5311117172241211
train gradient:  0.16091694528216416
iteration : 14184
train acc:  0.765625
train loss:  0.45787686109542847
train gradient:  0.11860828916565784
iteration : 14185
train acc:  0.703125
train loss:  0.5018309354782104
train gradient:  0.1266579702096619
iteration : 14186
train acc:  0.7777777777777778
train loss:  0.6447737812995911
train gradient:  1.6571268944037758
val acc:  0.7378866240746375
val f1:  0.7606094285449663
val confusion matrix:  [[63403 35207]
 [16487 82123]]

----------------------------------------new_epoch--------------------------------------

epoch:  1
iteration : 0
train acc:  0.703125
train loss:  0.5288828611373901
train gradient:  0.15033295633547167
iteration : 1
train acc:  0.7421875
train loss:  0.568314254283905
train gradient:  0.12908747757759464
iteration : 2
train acc:  0.6640625
train loss:  0.5857443809509277
train gradient:  0.16984420752309556
iteration : 3
train acc:  0.734375
train loss:  0.4938218891620636
train gradient:  0.1135057526082121
iteration : 4
train acc:  0.7421875
train loss:  0.5365210771560669
train gradient:  0.1871748322314578
iteration : 5
train acc:  0.7734375
train loss:  0.492328405380249
train gradient:  0.11857903631004821
iteration : 6
train acc:  0.765625
train loss:  0.4332699179649353
train gradient:  0.09972589678960608
iteration : 7
train acc:  0.7578125
train loss:  0.437786340713501
train gradient:  0.11198217657782054
iteration : 8
train acc:  0.6953125
train loss:  0.484382301568985
train gradient:  0.127667659904223
iteration : 9
train acc:  0.7734375
train loss:  0.4370376169681549
train gradient:  0.10461240710214993
iteration : 10
train acc:  0.7421875
train loss:  0.5035943388938904
train gradient:  0.11772360941885386
iteration : 11
train acc:  0.8359375
train loss:  0.44835326075553894
train gradient:  0.12247778451759413
iteration : 12
train acc:  0.78125
train loss:  0.4994688034057617
train gradient:  0.12913229282983374
iteration : 13
train acc:  0.75
train loss:  0.49459224939346313
train gradient:  0.12726894210615525
iteration : 14
train acc:  0.78125
train loss:  0.43779218196868896
train gradient:  0.09868742397644678
iteration : 15
train acc:  0.7421875
train loss:  0.4872024357318878
train gradient:  0.10974240901378204
iteration : 16
train acc:  0.8203125
train loss:  0.409684419631958
train gradient:  0.08958596164830132
iteration : 17
train acc:  0.8125
train loss:  0.42732760310173035
train gradient:  0.10423983588489905
iteration : 18
train acc:  0.765625
train loss:  0.41200536489486694
train gradient:  0.09118212217801469
iteration : 19
train acc:  0.765625
train loss:  0.4642176628112793
train gradient:  0.12527012026030504
iteration : 20
train acc:  0.75
train loss:  0.44768619537353516
train gradient:  0.09504264223090915
iteration : 21
train acc:  0.7265625
train loss:  0.509884238243103
train gradient:  0.1173107698715452
iteration : 22
train acc:  0.71875
train loss:  0.45845261216163635
train gradient:  0.11073486469597531
iteration : 23
train acc:  0.6796875
train loss:  0.5500664710998535
train gradient:  0.20780484504102603
iteration : 24
train acc:  0.8046875
train loss:  0.44090813398361206
train gradient:  0.11721144489932168
iteration : 25
train acc:  0.796875
train loss:  0.4095761775970459
train gradient:  0.07413099785375862
iteration : 26
train acc:  0.6953125
train loss:  0.5462239980697632
train gradient:  0.12290913033069928
iteration : 27
train acc:  0.734375
train loss:  0.522255539894104
train gradient:  0.14911557186435587
iteration : 28
train acc:  0.703125
train loss:  0.5140066742897034
train gradient:  0.1223499108241525
iteration : 29
train acc:  0.6640625
train loss:  0.5738928914070129
train gradient:  0.18405687668919135
iteration : 30
train acc:  0.7890625
train loss:  0.45347750186920166
train gradient:  0.10101355940565218
iteration : 31
train acc:  0.8125
train loss:  0.463911771774292
train gradient:  0.08716591080380452
iteration : 32
train acc:  0.734375
train loss:  0.5212234854698181
train gradient:  0.1346058207082455
iteration : 33
train acc:  0.734375
train loss:  0.4832000732421875
train gradient:  0.1196480438499512
iteration : 34
train acc:  0.8046875
train loss:  0.4166436791419983
train gradient:  0.09267955307569185
iteration : 35
train acc:  0.6796875
train loss:  0.5216394662857056
train gradient:  0.11017403383641637
iteration : 36
train acc:  0.765625
train loss:  0.4386024475097656
train gradient:  0.09596170219305092
iteration : 37
train acc:  0.7734375
train loss:  0.4687512218952179
train gradient:  0.15641643540526418
iteration : 38
train acc:  0.765625
train loss:  0.4146959185600281
train gradient:  0.0933480075623273
iteration : 39
train acc:  0.71875
train loss:  0.5005459785461426
train gradient:  0.11773059690051518
iteration : 40
train acc:  0.765625
train loss:  0.4169268310070038
train gradient:  0.09049641248122274
iteration : 41
train acc:  0.75
train loss:  0.4566577672958374
train gradient:  0.0997150934396619
iteration : 42
train acc:  0.78125
train loss:  0.5076521635055542
train gradient:  0.12262876979179392
iteration : 43
train acc:  0.7734375
train loss:  0.4629819691181183
train gradient:  0.11135578165353946
iteration : 44
train acc:  0.734375
train loss:  0.4747909605503082
train gradient:  0.1609779335497144
iteration : 45
train acc:  0.796875
train loss:  0.45322322845458984
train gradient:  0.14371237936379472
iteration : 46
train acc:  0.7421875
train loss:  0.473969429731369
train gradient:  0.09944300119359355
iteration : 47
train acc:  0.71875
train loss:  0.48100337386131287
train gradient:  0.1127635694719254
iteration : 48
train acc:  0.7890625
train loss:  0.4516037106513977
train gradient:  0.11123690533161458
iteration : 49
train acc:  0.6875
train loss:  0.5345399975776672
train gradient:  0.15447455504285246
iteration : 50
train acc:  0.7265625
train loss:  0.5370817184448242
train gradient:  0.15433046769616132
iteration : 51
train acc:  0.6796875
train loss:  0.5026204586029053
train gradient:  0.1036907223439155
iteration : 52
train acc:  0.6875
train loss:  0.6218059062957764
train gradient:  0.15968459264384613
iteration : 53
train acc:  0.78125
train loss:  0.4372612535953522
train gradient:  0.10336955838915733
iteration : 54
train acc:  0.7734375
train loss:  0.46475130319595337
train gradient:  0.11352050110819692
iteration : 55
train acc:  0.8046875
train loss:  0.47004443407058716
train gradient:  0.11781280811375132
iteration : 56
train acc:  0.7421875
train loss:  0.49379220604896545
train gradient:  0.10440625576740291
iteration : 57
train acc:  0.7109375
train loss:  0.5796823501586914
train gradient:  0.13611695735022844
iteration : 58
train acc:  0.75
train loss:  0.47791528701782227
train gradient:  0.17507385722524757
iteration : 59
train acc:  0.796875
train loss:  0.4021899700164795
train gradient:  0.10666152723820918
iteration : 60
train acc:  0.7578125
train loss:  0.4510856866836548
train gradient:  0.0999001201626979
iteration : 61
train acc:  0.7109375
train loss:  0.5215113759040833
train gradient:  0.10465617561536757
iteration : 62
train acc:  0.703125
train loss:  0.5448713302612305
train gradient:  0.13288937287252822
iteration : 63
train acc:  0.7421875
train loss:  0.5057302117347717
train gradient:  0.10566466174949926
iteration : 64
train acc:  0.734375
train loss:  0.4957786202430725
train gradient:  0.1135110857773598
iteration : 65
train acc:  0.7109375
train loss:  0.4851665496826172
train gradient:  0.1188024726109759
iteration : 66
train acc:  0.7265625
train loss:  0.517122745513916
train gradient:  0.14248249657248652
iteration : 67
train acc:  0.796875
train loss:  0.47553765773773193
train gradient:  0.11574129815960384
iteration : 68
train acc:  0.7578125
train loss:  0.4805598855018616
train gradient:  0.12202184954413647
iteration : 69
train acc:  0.71875
train loss:  0.5112566947937012
train gradient:  0.10346094177267115
iteration : 70
train acc:  0.7421875
train loss:  0.4841538667678833
train gradient:  0.15782514193979963
iteration : 71
train acc:  0.6875
train loss:  0.6007972359657288
train gradient:  0.18065930822511017
iteration : 72
train acc:  0.7578125
train loss:  0.5058437585830688
train gradient:  0.12889378434108584
iteration : 73
train acc:  0.703125
train loss:  0.5058479309082031
train gradient:  0.13623669238602537
iteration : 74
train acc:  0.765625
train loss:  0.5092667937278748
train gradient:  0.1461283780664507
iteration : 75
train acc:  0.8125
train loss:  0.3907829821109772
train gradient:  0.08187130479785563
iteration : 76
train acc:  0.75
train loss:  0.46392396092414856
train gradient:  0.1279418264320145
iteration : 77
train acc:  0.7890625
train loss:  0.45615696907043457
train gradient:  0.12564278558295822
iteration : 78
train acc:  0.75
train loss:  0.515522301197052
train gradient:  0.15281411291896185
iteration : 79
train acc:  0.7578125
train loss:  0.4427794814109802
train gradient:  0.0974082208911617
iteration : 80
train acc:  0.75
train loss:  0.47924894094467163
train gradient:  0.09381998672204268
iteration : 81
train acc:  0.78125
train loss:  0.4798419773578644
train gradient:  0.13815018652581001
iteration : 82
train acc:  0.765625
train loss:  0.4951494336128235
train gradient:  0.1078060994276393
iteration : 83
train acc:  0.734375
train loss:  0.44002765417099
train gradient:  0.0927660230887904
iteration : 84
train acc:  0.8046875
train loss:  0.42757415771484375
train gradient:  0.10516422651203261
iteration : 85
train acc:  0.7734375
train loss:  0.5176105499267578
train gradient:  0.13101735800700065
iteration : 86
train acc:  0.828125
train loss:  0.4210513234138489
train gradient:  0.10653793866528498
iteration : 87
train acc:  0.71875
train loss:  0.4778467118740082
train gradient:  0.1263668197888262
iteration : 88
train acc:  0.71875
train loss:  0.4658600091934204
train gradient:  0.137713446546818
iteration : 89
train acc:  0.78125
train loss:  0.47509104013442993
train gradient:  0.12234331854315823
iteration : 90
train acc:  0.703125
train loss:  0.4937993884086609
train gradient:  0.11629678999648735
iteration : 91
train acc:  0.7734375
train loss:  0.43043550848960876
train gradient:  0.09107176243595701
iteration : 92
train acc:  0.78125
train loss:  0.47697535157203674
train gradient:  0.15232510616326778
iteration : 93
train acc:  0.703125
train loss:  0.48411184549331665
train gradient:  0.12789759626049918
iteration : 94
train acc:  0.78125
train loss:  0.42475056648254395
train gradient:  0.09229385038364607
iteration : 95
train acc:  0.765625
train loss:  0.4633057713508606
train gradient:  0.11377643279216176
iteration : 96
train acc:  0.75
train loss:  0.47494927048683167
train gradient:  0.11110128171051192
iteration : 97
train acc:  0.8046875
train loss:  0.4680173993110657
train gradient:  0.10546108403008156
iteration : 98
train acc:  0.7265625
train loss:  0.5113228559494019
train gradient:  0.11102498658288308
iteration : 99
train acc:  0.7578125
train loss:  0.4720191955566406
train gradient:  0.10476739662250903
iteration : 100
train acc:  0.734375
train loss:  0.50032639503479
train gradient:  0.1402132472078913
iteration : 101
train acc:  0.734375
train loss:  0.4826969504356384
train gradient:  0.11464550649095098
iteration : 102
train acc:  0.703125
train loss:  0.5355566143989563
train gradient:  0.16841848452292502
iteration : 103
train acc:  0.6953125
train loss:  0.5822321772575378
train gradient:  0.17409649062279062
iteration : 104
train acc:  0.6875
train loss:  0.6033366918563843
train gradient:  0.14812496954919568
iteration : 105
train acc:  0.796875
train loss:  0.4229736924171448
train gradient:  0.10506845499211656
iteration : 106
train acc:  0.734375
train loss:  0.5399889349937439
train gradient:  0.16924330809001303
iteration : 107
train acc:  0.7734375
train loss:  0.41025668382644653
train gradient:  0.09007349318851961
iteration : 108
train acc:  0.7265625
train loss:  0.4830670654773712
train gradient:  0.15341334190650274
iteration : 109
train acc:  0.765625
train loss:  0.49494555592536926
train gradient:  0.1293060022193459
iteration : 110
train acc:  0.796875
train loss:  0.4142964482307434
train gradient:  0.09874515136391415
iteration : 111
train acc:  0.796875
train loss:  0.43752431869506836
train gradient:  0.1340834383502303
iteration : 112
train acc:  0.7734375
train loss:  0.4902232885360718
train gradient:  0.13775743343976593
iteration : 113
train acc:  0.734375
train loss:  0.5728683471679688
train gradient:  0.14374883097089586
iteration : 114
train acc:  0.7265625
train loss:  0.5569007396697998
train gradient:  0.20560402789668636
iteration : 115
train acc:  0.765625
train loss:  0.44775742292404175
train gradient:  0.11343498421509554
iteration : 116
train acc:  0.7109375
train loss:  0.5039315223693848
train gradient:  0.13357854266193936
iteration : 117
train acc:  0.75
train loss:  0.5208835601806641
train gradient:  0.15663782492184355
iteration : 118
train acc:  0.78125
train loss:  0.44543567299842834
train gradient:  0.09972177150503685
iteration : 119
train acc:  0.71875
train loss:  0.54658043384552
train gradient:  0.1554322219249648
iteration : 120
train acc:  0.8359375
train loss:  0.43039917945861816
train gradient:  0.09462602268360186
iteration : 121
train acc:  0.7109375
train loss:  0.5140842199325562
train gradient:  0.16868811124778546
iteration : 122
train acc:  0.7578125
train loss:  0.46341100335121155
train gradient:  0.14226087460026304
iteration : 123
train acc:  0.7578125
train loss:  0.4548262357711792
train gradient:  0.10069725482432079
iteration : 124
train acc:  0.7265625
train loss:  0.4721328616142273
train gradient:  0.10610669927695617
iteration : 125
train acc:  0.7109375
train loss:  0.5010942816734314
train gradient:  0.11910006451285352
iteration : 126
train acc:  0.7578125
train loss:  0.46741822361946106
train gradient:  0.12261388960701525
iteration : 127
train acc:  0.6484375
train loss:  0.592090904712677
train gradient:  0.17426485733500285
iteration : 128
train acc:  0.7890625
train loss:  0.41885805130004883
train gradient:  0.08805623111786097
iteration : 129
train acc:  0.7890625
train loss:  0.47330421209335327
train gradient:  0.10301004502987766
iteration : 130
train acc:  0.828125
train loss:  0.3716624677181244
train gradient:  0.07721017035078323
iteration : 131
train acc:  0.7265625
train loss:  0.49432113766670227
train gradient:  0.11910750918078734
iteration : 132
train acc:  0.75
train loss:  0.4558524489402771
train gradient:  0.10333636197087225
iteration : 133
train acc:  0.796875
train loss:  0.4723852872848511
train gradient:  0.13650357442147268
iteration : 134
train acc:  0.8046875
train loss:  0.4223606586456299
train gradient:  0.10152817212904303
iteration : 135
train acc:  0.6875
train loss:  0.5088801383972168
train gradient:  0.11258892966089845
iteration : 136
train acc:  0.7578125
train loss:  0.4771410822868347
train gradient:  0.12649467795817815
iteration : 137
train acc:  0.703125
train loss:  0.4945703446865082
train gradient:  0.17342271533203535
iteration : 138
train acc:  0.8125
train loss:  0.4259263277053833
train gradient:  0.09998372723687086
iteration : 139
train acc:  0.7109375
train loss:  0.48477914929389954
train gradient:  0.13436717981525764
iteration : 140
train acc:  0.734375
train loss:  0.49296075105667114
train gradient:  0.09912090148480873
iteration : 141
train acc:  0.71875
train loss:  0.5293331146240234
train gradient:  0.1465252363031051
iteration : 142
train acc:  0.734375
train loss:  0.5869009494781494
train gradient:  0.17515661757898413
iteration : 143
train acc:  0.734375
train loss:  0.5494205951690674
train gradient:  0.13132936666839745
iteration : 144
train acc:  0.78125
train loss:  0.43929198384284973
train gradient:  0.11417061281640709
iteration : 145
train acc:  0.7265625
train loss:  0.4710763990879059
train gradient:  0.13825377082992374
iteration : 146
train acc:  0.7578125
train loss:  0.49931448698043823
train gradient:  0.11442480724213028
iteration : 147
train acc:  0.7109375
train loss:  0.5375776290893555
train gradient:  0.17376679384790394
iteration : 148
train acc:  0.7734375
train loss:  0.5101665258407593
train gradient:  0.13184223525866062
iteration : 149
train acc:  0.7109375
train loss:  0.5069020390510559
train gradient:  0.15913693309314958
iteration : 150
train acc:  0.7109375
train loss:  0.5401034951210022
train gradient:  0.15865561386334978
iteration : 151
train acc:  0.8046875
train loss:  0.47275787591934204
train gradient:  0.1384480391688701
iteration : 152
train acc:  0.7421875
train loss:  0.49730491638183594
train gradient:  0.10855029541041515
iteration : 153
train acc:  0.8046875
train loss:  0.4903820753097534
train gradient:  0.1459754229976833
iteration : 154
train acc:  0.7734375
train loss:  0.49068009853363037
train gradient:  0.11737863714698725
iteration : 155
train acc:  0.78125
train loss:  0.4756515622138977
train gradient:  0.11943633669954405
iteration : 156
train acc:  0.78125
train loss:  0.4603266716003418
train gradient:  0.13338997119813578
iteration : 157
train acc:  0.796875
train loss:  0.4723263680934906
train gradient:  0.17910305219719808
iteration : 158
train acc:  0.7890625
train loss:  0.4614405930042267
train gradient:  0.11565902249592998
iteration : 159
train acc:  0.78125
train loss:  0.41307950019836426
train gradient:  0.11673608174960319
iteration : 160
train acc:  0.703125
train loss:  0.538691520690918
train gradient:  0.15640275093603345
iteration : 161
train acc:  0.71875
train loss:  0.4995947480201721
train gradient:  0.11636597599558492
iteration : 162
train acc:  0.7421875
train loss:  0.444732666015625
train gradient:  0.10047798254677633
iteration : 163
train acc:  0.7734375
train loss:  0.4249315559864044
train gradient:  0.12027303427107139
iteration : 164
train acc:  0.7421875
train loss:  0.48100051283836365
train gradient:  0.12533348119562407
iteration : 165
train acc:  0.796875
train loss:  0.45034635066986084
train gradient:  0.0970876412234201
iteration : 166
train acc:  0.7421875
train loss:  0.4659203290939331
train gradient:  0.12917082855824588
iteration : 167
train acc:  0.7265625
train loss:  0.5087783932685852
train gradient:  0.1514309030687807
iteration : 168
train acc:  0.8125
train loss:  0.4472033679485321
train gradient:  0.13610996929892247
iteration : 169
train acc:  0.765625
train loss:  0.4412379562854767
train gradient:  0.11162057961885019
iteration : 170
train acc:  0.6640625
train loss:  0.5999756455421448
train gradient:  0.19567929214182173
iteration : 171
train acc:  0.75
train loss:  0.483268141746521
train gradient:  0.15973491789447275
iteration : 172
train acc:  0.7109375
train loss:  0.552847683429718
train gradient:  0.16227511917151471
iteration : 173
train acc:  0.765625
train loss:  0.46546512842178345
train gradient:  0.1365307579174721
iteration : 174
train acc:  0.6796875
train loss:  0.5338090062141418
train gradient:  0.17373434018299552
iteration : 175
train acc:  0.7109375
train loss:  0.5180700421333313
train gradient:  0.13544073403018103
iteration : 176
train acc:  0.7109375
train loss:  0.5368102788925171
train gradient:  0.16104210823892362
iteration : 177
train acc:  0.7421875
train loss:  0.453679621219635
train gradient:  0.13592930016479476
iteration : 178
train acc:  0.796875
train loss:  0.4673604965209961
train gradient:  0.15355300842707403
iteration : 179
train acc:  0.7578125
train loss:  0.48288607597351074
train gradient:  0.13677898872595406
iteration : 180
train acc:  0.75
train loss:  0.5005054473876953
train gradient:  0.11651612044616308
iteration : 181
train acc:  0.7109375
train loss:  0.5029216408729553
train gradient:  0.13583777375718037
iteration : 182
train acc:  0.765625
train loss:  0.431649386882782
train gradient:  0.09260444453158125
iteration : 183
train acc:  0.8203125
train loss:  0.46971195936203003
train gradient:  0.11378447194053322
iteration : 184
train acc:  0.7265625
train loss:  0.4560701251029968
train gradient:  0.10814512479505586
iteration : 185
train acc:  0.640625
train loss:  0.5840400457382202
train gradient:  0.19889814689465274
iteration : 186
train acc:  0.7578125
train loss:  0.5029172897338867
train gradient:  0.09977251177717014
iteration : 187
train acc:  0.7734375
train loss:  0.4815664291381836
train gradient:  0.1008639106213958
iteration : 188
train acc:  0.7109375
train loss:  0.6090348362922668
train gradient:  0.15955976609249767
iteration : 189
train acc:  0.7578125
train loss:  0.4445567727088928
train gradient:  0.10031426510977905
iteration : 190
train acc:  0.7421875
train loss:  0.5235662460327148
train gradient:  0.1450729714231172
iteration : 191
train acc:  0.6875
train loss:  0.5425596237182617
train gradient:  0.14974842790179332
iteration : 192
train acc:  0.765625
train loss:  0.44500017166137695
train gradient:  0.10477132395323087
iteration : 193
train acc:  0.7578125
train loss:  0.4715161621570587
train gradient:  0.12439060425362236
iteration : 194
train acc:  0.703125
train loss:  0.5622902512550354
train gradient:  0.1769135897272271
iteration : 195
train acc:  0.6953125
train loss:  0.5572927594184875
train gradient:  0.16210560516950212
iteration : 196
train acc:  0.7265625
train loss:  0.4981502890586853
train gradient:  0.10968913743148463
iteration : 197
train acc:  0.78125
train loss:  0.4629702866077423
train gradient:  0.12129492578697398
iteration : 198
train acc:  0.703125
train loss:  0.5690438747406006
train gradient:  0.1789729663908322
iteration : 199
train acc:  0.7265625
train loss:  0.5264556407928467
train gradient:  0.12246875637768366
iteration : 200
train acc:  0.7890625
train loss:  0.43472152948379517
train gradient:  0.10510899749159713
iteration : 201
train acc:  0.765625
train loss:  0.4640026092529297
train gradient:  0.09866784017974539
iteration : 202
train acc:  0.7109375
train loss:  0.5157713890075684
train gradient:  0.11086983179314028
iteration : 203
train acc:  0.6875
train loss:  0.5395519733428955
train gradient:  0.17528756088643954
iteration : 204
train acc:  0.75
train loss:  0.5129663944244385
train gradient:  0.10601266112089495
iteration : 205
train acc:  0.7578125
train loss:  0.5005601644515991
train gradient:  0.12355421967578763
iteration : 206
train acc:  0.8203125
train loss:  0.4097350239753723
train gradient:  0.10937472973951745
iteration : 207
train acc:  0.6875
train loss:  0.5208472013473511
train gradient:  0.12056909244869068
iteration : 208
train acc:  0.765625
train loss:  0.49608510732650757
train gradient:  0.1115551042494443
iteration : 209
train acc:  0.7578125
train loss:  0.4754306674003601
train gradient:  0.1194667725558199
iteration : 210
train acc:  0.7578125
train loss:  0.4824299216270447
train gradient:  0.1362473740048631
iteration : 211
train acc:  0.7265625
train loss:  0.4980258047580719
train gradient:  0.141056015669087
iteration : 212
train acc:  0.8046875
train loss:  0.42940637469291687
train gradient:  0.11223947983362866
iteration : 213
train acc:  0.796875
train loss:  0.4131474792957306
train gradient:  0.09702470927218915
iteration : 214
train acc:  0.6953125
train loss:  0.5560801029205322
train gradient:  0.18655671363573306
iteration : 215
train acc:  0.734375
train loss:  0.4847812056541443
train gradient:  0.1576519837346072
iteration : 216
train acc:  0.734375
train loss:  0.496433287858963
train gradient:  0.11299394804643474
iteration : 217
train acc:  0.7578125
train loss:  0.5012514591217041
train gradient:  0.12779463353189888
iteration : 218
train acc:  0.703125
train loss:  0.4812840521335602
train gradient:  0.13961258045998287
iteration : 219
train acc:  0.7890625
train loss:  0.43191832304000854
train gradient:  0.09324834770230403
iteration : 220
train acc:  0.6875
train loss:  0.5310502648353577
train gradient:  0.11198210967423834
iteration : 221
train acc:  0.6640625
train loss:  0.5479351282119751
train gradient:  0.19559791101023064
iteration : 222
train acc:  0.7265625
train loss:  0.4892669916152954
train gradient:  0.13050197097635977
iteration : 223
train acc:  0.71875
train loss:  0.4809449315071106
train gradient:  0.1116335110197342
iteration : 224
train acc:  0.7421875
train loss:  0.4488465189933777
train gradient:  0.10502457782913102
iteration : 225
train acc:  0.75
train loss:  0.49511829018592834
train gradient:  0.13939218408143977
iteration : 226
train acc:  0.765625
train loss:  0.44569697976112366
train gradient:  0.1172991029768497
iteration : 227
train acc:  0.78125
train loss:  0.4191781282424927
train gradient:  0.09730968115099434
iteration : 228
train acc:  0.734375
train loss:  0.4888034462928772
train gradient:  0.13191987003008376
iteration : 229
train acc:  0.703125
train loss:  0.5447992086410522
train gradient:  0.18023266889330342
iteration : 230
train acc:  0.6875
train loss:  0.5735334157943726
train gradient:  0.1682728154972178
iteration : 231
train acc:  0.75
train loss:  0.4971258044242859
train gradient:  0.12162007027708872
iteration : 232
train acc:  0.6953125
train loss:  0.5127312541007996
train gradient:  0.1107186900448334
iteration : 233
train acc:  0.7578125
train loss:  0.4370030462741852
train gradient:  0.08197683777854409
iteration : 234
train acc:  0.7265625
train loss:  0.5045872330665588
train gradient:  0.13096488016678343
iteration : 235
train acc:  0.7421875
train loss:  0.5019136071205139
train gradient:  0.11913109165089393
iteration : 236
train acc:  0.8125
train loss:  0.45217108726501465
train gradient:  0.13366690336066425
iteration : 237
train acc:  0.7421875
train loss:  0.49072036147117615
train gradient:  0.10492374066616168
iteration : 238
train acc:  0.7890625
train loss:  0.4049307703971863
train gradient:  0.0982781852351564
iteration : 239
train acc:  0.8046875
train loss:  0.39360979199409485
train gradient:  0.0906543667041101
iteration : 240
train acc:  0.765625
train loss:  0.45546403527259827
train gradient:  0.09559300012708442
iteration : 241
train acc:  0.765625
train loss:  0.4704400300979614
train gradient:  0.13939230547377324
iteration : 242
train acc:  0.78125
train loss:  0.49517959356307983
train gradient:  0.1376694170205709
iteration : 243
train acc:  0.75
train loss:  0.44585663080215454
train gradient:  0.09036963611675852
iteration : 244
train acc:  0.734375
train loss:  0.5274273157119751
train gradient:  0.15780662022014674
iteration : 245
train acc:  0.7734375
train loss:  0.4633803963661194
train gradient:  0.11709549887832156
iteration : 246
train acc:  0.734375
train loss:  0.5092082619667053
train gradient:  0.15928543925975855
iteration : 247
train acc:  0.7109375
train loss:  0.5536416172981262
train gradient:  0.1632145251374978
iteration : 248
train acc:  0.796875
train loss:  0.4100390672683716
train gradient:  0.10658564061295643
iteration : 249
train acc:  0.7421875
train loss:  0.511916995048523
train gradient:  0.15710842217588467
iteration : 250
train acc:  0.71875
train loss:  0.4907855689525604
train gradient:  0.12305028516783713
iteration : 251
train acc:  0.7421875
train loss:  0.5355418920516968
train gradient:  0.15646877862345332
iteration : 252
train acc:  0.7109375
train loss:  0.4822811484336853
train gradient:  0.12860466520736125
iteration : 253
train acc:  0.8046875
train loss:  0.4137200713157654
train gradient:  0.11061787630772758
iteration : 254
train acc:  0.671875
train loss:  0.5419695377349854
train gradient:  0.16961279828564166
iteration : 255
train acc:  0.7109375
train loss:  0.5023897886276245
train gradient:  0.13548457994385282
iteration : 256
train acc:  0.78125
train loss:  0.45764830708503723
train gradient:  0.11785761204108468
iteration : 257
train acc:  0.703125
train loss:  0.5059323310852051
train gradient:  0.11838622040142964
iteration : 258
train acc:  0.7890625
train loss:  0.4646815061569214
train gradient:  0.10077501561959518
iteration : 259
train acc:  0.765625
train loss:  0.4320470094680786
train gradient:  0.12274810918919443
iteration : 260
train acc:  0.765625
train loss:  0.4786375164985657
train gradient:  0.12282886271466982
iteration : 261
train acc:  0.765625
train loss:  0.48418140411376953
train gradient:  0.10878548295662704
iteration : 262
train acc:  0.7578125
train loss:  0.4506359100341797
train gradient:  0.08909918322789384
iteration : 263
train acc:  0.7578125
train loss:  0.4663980007171631
train gradient:  0.1683308414334065
iteration : 264
train acc:  0.71875
train loss:  0.5385425090789795
train gradient:  0.15664117234285568
iteration : 265
train acc:  0.796875
train loss:  0.49703800678253174
train gradient:  0.12538868192181146
iteration : 266
train acc:  0.765625
train loss:  0.4654045104980469
train gradient:  0.12061125882704132
iteration : 267
train acc:  0.7421875
train loss:  0.4730454683303833
train gradient:  0.1285630687264659
iteration : 268
train acc:  0.7578125
train loss:  0.494011253118515
train gradient:  0.132703339116938
iteration : 269
train acc:  0.8046875
train loss:  0.44748082756996155
train gradient:  0.1089839985238158
iteration : 270
train acc:  0.6484375
train loss:  0.5175135135650635
train gradient:  0.13635074628472965
iteration : 271
train acc:  0.796875
train loss:  0.42039719223976135
train gradient:  0.12632367428768823
iteration : 272
train acc:  0.7421875
train loss:  0.47233039140701294
train gradient:  0.10354978267785593
iteration : 273
train acc:  0.7890625
train loss:  0.4371528625488281
train gradient:  0.08652370549132973
iteration : 274
train acc:  0.7421875
train loss:  0.4606325030326843
train gradient:  0.10537215718633282
iteration : 275
train acc:  0.6875
train loss:  0.5585364103317261
train gradient:  0.16677025591398525
iteration : 276
train acc:  0.7734375
train loss:  0.4644324481487274
train gradient:  0.12383621584047273
iteration : 277
train acc:  0.7578125
train loss:  0.5183690190315247
train gradient:  0.157662919084529
iteration : 278
train acc:  0.7265625
train loss:  0.4586870074272156
train gradient:  0.11074764705620198
iteration : 279
train acc:  0.7578125
train loss:  0.49220070242881775
train gradient:  0.18099233075722704
iteration : 280
train acc:  0.7265625
train loss:  0.4512157142162323
train gradient:  0.10867615661380567
iteration : 281
train acc:  0.7734375
train loss:  0.49519091844558716
train gradient:  0.10706443639586288
iteration : 282
train acc:  0.765625
train loss:  0.4834960699081421
train gradient:  0.1361442510589653
iteration : 283
train acc:  0.7890625
train loss:  0.4183421730995178
train gradient:  0.09927877379177849
iteration : 284
train acc:  0.7421875
train loss:  0.4780682623386383
train gradient:  0.1360094792002356
iteration : 285
train acc:  0.671875
train loss:  0.5595390200614929
train gradient:  0.175261379357063
iteration : 286
train acc:  0.7578125
train loss:  0.4636940360069275
train gradient:  0.11408531677878145
iteration : 287
train acc:  0.734375
train loss:  0.5048919916152954
train gradient:  0.13088547866980016
iteration : 288
train acc:  0.7578125
train loss:  0.4676899015903473
train gradient:  0.11329094988108128
iteration : 289
train acc:  0.6796875
train loss:  0.4970930814743042
train gradient:  0.13254922001004354
iteration : 290
train acc:  0.765625
train loss:  0.44570857286453247
train gradient:  0.16575556214425669
iteration : 291
train acc:  0.7421875
train loss:  0.5043344497680664
train gradient:  0.1362692796568383
iteration : 292
train acc:  0.7890625
train loss:  0.463166207075119
train gradient:  0.11615842160097256
iteration : 293
train acc:  0.765625
train loss:  0.43928998708724976
train gradient:  0.09702642005389255
iteration : 294
train acc:  0.7734375
train loss:  0.4710496664047241
train gradient:  0.11821384394184341
iteration : 295
train acc:  0.78125
train loss:  0.4718969464302063
train gradient:  0.10803116557888018
iteration : 296
train acc:  0.765625
train loss:  0.4768673777580261
train gradient:  0.11521377468584924
iteration : 297
train acc:  0.7734375
train loss:  0.4659830331802368
train gradient:  0.1362645055800732
iteration : 298
train acc:  0.765625
train loss:  0.4764706492424011
train gradient:  0.1382925298905328
iteration : 299
train acc:  0.7578125
train loss:  0.5364844799041748
train gradient:  0.1676568954959568
iteration : 300
train acc:  0.7890625
train loss:  0.4384493827819824
train gradient:  0.1072425512908529
iteration : 301
train acc:  0.8203125
train loss:  0.4378712773323059
train gradient:  0.09768594147937605
iteration : 302
train acc:  0.7734375
train loss:  0.41215693950653076
train gradient:  0.08328598345766346
iteration : 303
train acc:  0.7421875
train loss:  0.42900943756103516
train gradient:  0.11999959933419801
iteration : 304
train acc:  0.7890625
train loss:  0.48573946952819824
train gradient:  0.12236721886714248
iteration : 305
train acc:  0.7421875
train loss:  0.5331383943557739
train gradient:  0.14096958777849228
iteration : 306
train acc:  0.7734375
train loss:  0.45826035737991333
train gradient:  0.13170883062173314
iteration : 307
train acc:  0.7578125
train loss:  0.4138948321342468
train gradient:  0.0884557015345873
iteration : 308
train acc:  0.7109375
train loss:  0.5011541843414307
train gradient:  0.1626499364369044
iteration : 309
train acc:  0.7578125
train loss:  0.4777746796607971
train gradient:  0.13140603383981764
iteration : 310
train acc:  0.8203125
train loss:  0.4332910180091858
train gradient:  0.11677913213555582
iteration : 311
train acc:  0.7265625
train loss:  0.4849579632282257
train gradient:  0.10760605560918862
iteration : 312
train acc:  0.7578125
train loss:  0.47393614053726196
train gradient:  0.1205799867310319
iteration : 313
train acc:  0.796875
train loss:  0.4559842348098755
train gradient:  0.0961251149091665
iteration : 314
train acc:  0.71875
train loss:  0.5061891078948975
train gradient:  0.11202938076468692
iteration : 315
train acc:  0.765625
train loss:  0.4840396046638489
train gradient:  0.1506215478796717
iteration : 316
train acc:  0.734375
train loss:  0.49069738388061523
train gradient:  0.14052259948553436
iteration : 317
train acc:  0.8046875
train loss:  0.44955119490623474
train gradient:  0.12366644952286066
iteration : 318
train acc:  0.78125
train loss:  0.4696013331413269
train gradient:  0.11282312010948696
iteration : 319
train acc:  0.7421875
train loss:  0.4520307183265686
train gradient:  0.12780914884929065
iteration : 320
train acc:  0.7578125
train loss:  0.43655234575271606
train gradient:  0.10483248241465878
iteration : 321
train acc:  0.765625
train loss:  0.5480068922042847
train gradient:  0.17440267728223738
iteration : 322
train acc:  0.71875
train loss:  0.5130348205566406
train gradient:  0.13512633718330908
iteration : 323
train acc:  0.796875
train loss:  0.4272664487361908
train gradient:  0.10203865243320573
iteration : 324
train acc:  0.796875
train loss:  0.4475259780883789
train gradient:  0.11070171992398206
iteration : 325
train acc:  0.8203125
train loss:  0.4325864911079407
train gradient:  0.08650623704349836
iteration : 326
train acc:  0.7578125
train loss:  0.43573227524757385
train gradient:  0.10916647991156139
iteration : 327
train acc:  0.796875
train loss:  0.441936731338501
train gradient:  0.14298521741311537
iteration : 328
train acc:  0.7578125
train loss:  0.5003653764724731
train gradient:  0.14352470985017485
iteration : 329
train acc:  0.7890625
train loss:  0.4493118226528168
train gradient:  0.13042849876566043
iteration : 330
train acc:  0.765625
train loss:  0.4477030336856842
train gradient:  0.10586481463240521
iteration : 331
train acc:  0.71875
train loss:  0.4674014449119568
train gradient:  0.12020189441978557
iteration : 332
train acc:  0.703125
train loss:  0.545843780040741
train gradient:  0.1352578784080155
iteration : 333
train acc:  0.765625
train loss:  0.5513274073600769
train gradient:  0.2057299869812797
iteration : 334
train acc:  0.75
train loss:  0.5238730907440186
train gradient:  0.1568162949412143
iteration : 335
train acc:  0.796875
train loss:  0.4263530969619751
train gradient:  0.11064805747965396
iteration : 336
train acc:  0.6953125
train loss:  0.4792255163192749
train gradient:  0.1632627670585101
iteration : 337
train acc:  0.7734375
train loss:  0.4501315653324127
train gradient:  0.11307641363263328
iteration : 338
train acc:  0.78125
train loss:  0.4984830617904663
train gradient:  0.2007153716780762
iteration : 339
train acc:  0.71875
train loss:  0.547355055809021
train gradient:  0.16751663137377404
iteration : 340
train acc:  0.8359375
train loss:  0.4409393072128296
train gradient:  0.10806998420743802
iteration : 341
train acc:  0.7421875
train loss:  0.5087932348251343
train gradient:  0.13063202715017969
iteration : 342
train acc:  0.640625
train loss:  0.6010996103286743
train gradient:  0.20492663547555384
iteration : 343
train acc:  0.7265625
train loss:  0.5076666474342346
train gradient:  0.13164374924015473
iteration : 344
train acc:  0.765625
train loss:  0.4765932857990265
train gradient:  0.12277458781670798
iteration : 345
train acc:  0.703125
train loss:  0.4828713536262512
train gradient:  0.13033840395669982
iteration : 346
train acc:  0.671875
train loss:  0.5609201788902283
train gradient:  0.13811951204131726
iteration : 347
train acc:  0.7734375
train loss:  0.4949752688407898
train gradient:  0.12739098551896813
iteration : 348
train acc:  0.71875
train loss:  0.5081363320350647
train gradient:  0.12082307765510414
iteration : 349
train acc:  0.7265625
train loss:  0.4936169385910034
train gradient:  0.14990573959257714
iteration : 350
train acc:  0.71875
train loss:  0.5720809698104858
train gradient:  0.15583628973095737
iteration : 351
train acc:  0.7578125
train loss:  0.4776788353919983
train gradient:  0.1221093089661819
iteration : 352
train acc:  0.6953125
train loss:  0.5160453915596008
train gradient:  0.16091405209068493
iteration : 353
train acc:  0.7890625
train loss:  0.4737650156021118
train gradient:  0.12311832235222905
iteration : 354
train acc:  0.765625
train loss:  0.46248525381088257
train gradient:  0.11085835883884435
iteration : 355
train acc:  0.765625
train loss:  0.5005717277526855
train gradient:  0.12755002091585593
iteration : 356
train acc:  0.75
train loss:  0.49822336435317993
train gradient:  0.10976912747541
iteration : 357
train acc:  0.75
train loss:  0.47549551725387573
train gradient:  0.14685074183634478
iteration : 358
train acc:  0.7421875
train loss:  0.5205327868461609
train gradient:  0.17163592605989686
iteration : 359
train acc:  0.7890625
train loss:  0.46327051520347595
train gradient:  0.13340841461644545
iteration : 360
train acc:  0.765625
train loss:  0.4653724730014801
train gradient:  0.1079090144658968
iteration : 361
train acc:  0.765625
train loss:  0.44834834337234497
train gradient:  0.11029344817377329
iteration : 362
train acc:  0.7421875
train loss:  0.49993982911109924
train gradient:  0.10135179175923298
iteration : 363
train acc:  0.734375
train loss:  0.5034070014953613
train gradient:  0.11510894680545677
iteration : 364
train acc:  0.71875
train loss:  0.5152515769004822
train gradient:  0.13200930652177129
iteration : 365
train acc:  0.75
train loss:  0.4583730697631836
train gradient:  0.10918254048760863
iteration : 366
train acc:  0.765625
train loss:  0.4701058268547058
train gradient:  0.11309695676252737
iteration : 367
train acc:  0.75
train loss:  0.4690706431865692
train gradient:  0.11494811647542959
iteration : 368
train acc:  0.7109375
train loss:  0.5347194671630859
train gradient:  0.12391119998732307
iteration : 369
train acc:  0.7109375
train loss:  0.4791831970214844
train gradient:  0.12604868652603446
iteration : 370
train acc:  0.8203125
train loss:  0.4078507721424103
train gradient:  0.08255934532651131
iteration : 371
train acc:  0.7109375
train loss:  0.4896303713321686
train gradient:  0.13470968389827334
iteration : 372
train acc:  0.7578125
train loss:  0.44945400953292847
train gradient:  0.10601806948304203
iteration : 373
train acc:  0.703125
train loss:  0.4921712577342987
train gradient:  0.12088914389628024
iteration : 374
train acc:  0.8046875
train loss:  0.4008367657661438
train gradient:  0.08958224031954128
iteration : 375
train acc:  0.8125
train loss:  0.4310186803340912
train gradient:  0.08921948793125536
iteration : 376
train acc:  0.734375
train loss:  0.5013580918312073
train gradient:  0.12011638799666687
iteration : 377
train acc:  0.78125
train loss:  0.45890355110168457
train gradient:  0.11733784035031734
iteration : 378
train acc:  0.765625
train loss:  0.470095694065094
train gradient:  0.12120132088962228
iteration : 379
train acc:  0.7265625
train loss:  0.46753162145614624
train gradient:  0.12530773707848836
iteration : 380
train acc:  0.7109375
train loss:  0.5921992063522339
train gradient:  0.19151988859328473
iteration : 381
train acc:  0.75
train loss:  0.45742523670196533
train gradient:  0.10711325396135311
iteration : 382
train acc:  0.78125
train loss:  0.5055055618286133
train gradient:  0.12472978342448533
iteration : 383
train acc:  0.75
train loss:  0.45459216833114624
train gradient:  0.12958053406962405
iteration : 384
train acc:  0.7421875
train loss:  0.5507340431213379
train gradient:  0.1471582269725043
iteration : 385
train acc:  0.78125
train loss:  0.4715336561203003
train gradient:  0.12179528114812394
iteration : 386
train acc:  0.7265625
train loss:  0.5304347276687622
train gradient:  0.1462368826987508
iteration : 387
train acc:  0.7578125
train loss:  0.44705843925476074
train gradient:  0.12180289865242937
iteration : 388
train acc:  0.796875
train loss:  0.4301251769065857
train gradient:  0.13081378388182008
iteration : 389
train acc:  0.8203125
train loss:  0.4255419671535492
train gradient:  0.11164341030669359
iteration : 390
train acc:  0.7734375
train loss:  0.46482622623443604
train gradient:  0.11245163963857842
iteration : 391
train acc:  0.8046875
train loss:  0.44587117433547974
train gradient:  0.10226426858438104
iteration : 392
train acc:  0.765625
train loss:  0.4525126814842224
train gradient:  0.12020776301405396
iteration : 393
train acc:  0.7578125
train loss:  0.4512908458709717
train gradient:  0.0902196090481214
iteration : 394
train acc:  0.78125
train loss:  0.4489506483078003
train gradient:  0.10033262560156835
iteration : 395
train acc:  0.7421875
train loss:  0.47141480445861816
train gradient:  0.15312660275317977
iteration : 396
train acc:  0.78125
train loss:  0.4507797956466675
train gradient:  0.09557225829816035
iteration : 397
train acc:  0.734375
train loss:  0.49538737535476685
train gradient:  0.12030969721665387
iteration : 398
train acc:  0.765625
train loss:  0.4976038932800293
train gradient:  0.14020228312659805
iteration : 399
train acc:  0.8046875
train loss:  0.4405762553215027
train gradient:  0.10392413013592842
iteration : 400
train acc:  0.7578125
train loss:  0.44311872124671936
train gradient:  0.09881853265295078
iteration : 401
train acc:  0.71875
train loss:  0.5085365772247314
train gradient:  0.12701348883244645
iteration : 402
train acc:  0.7421875
train loss:  0.4609410762786865
train gradient:  0.13546745979295044
iteration : 403
train acc:  0.703125
train loss:  0.5335652232170105
train gradient:  0.16884502359549022
iteration : 404
train acc:  0.7890625
train loss:  0.44999855756759644
train gradient:  0.11073422862007548
iteration : 405
train acc:  0.6953125
train loss:  0.5448087453842163
train gradient:  0.13649816098483442
iteration : 406
train acc:  0.734375
train loss:  0.4671367406845093
train gradient:  0.11747723080991251
iteration : 407
train acc:  0.78125
train loss:  0.45232993364334106
train gradient:  0.11586163109569389
iteration : 408
train acc:  0.7578125
train loss:  0.4688969850540161
train gradient:  0.11531344891345288
iteration : 409
train acc:  0.7109375
train loss:  0.5797329545021057
train gradient:  0.19151001688991015
iteration : 410
train acc:  0.71875
train loss:  0.5017930865287781
train gradient:  0.18752445481917648
iteration : 411
train acc:  0.7109375
train loss:  0.49279844760894775
train gradient:  0.13040614508826565
iteration : 412
train acc:  0.7265625
train loss:  0.4964095950126648
train gradient:  0.1772856602885738
iteration : 413
train acc:  0.796875
train loss:  0.46672379970550537
train gradient:  0.13574640980293806
iteration : 414
train acc:  0.78125
train loss:  0.4554154872894287
train gradient:  0.12963143558669832
iteration : 415
train acc:  0.734375
train loss:  0.49010053277015686
train gradient:  0.13510217066257907
iteration : 416
train acc:  0.75
train loss:  0.45631468296051025
train gradient:  0.1004112570304337
iteration : 417
train acc:  0.6953125
train loss:  0.5478273630142212
train gradient:  0.19299756653553465
iteration : 418
train acc:  0.7578125
train loss:  0.4405516982078552
train gradient:  0.10693412942893928
iteration : 419
train acc:  0.7734375
train loss:  0.4581436812877655
train gradient:  0.13186667308887645
iteration : 420
train acc:  0.7421875
train loss:  0.49344924092292786
train gradient:  0.1584391961193139
iteration : 421
train acc:  0.7265625
train loss:  0.5081104040145874
train gradient:  0.14092622861454024
iteration : 422
train acc:  0.7578125
train loss:  0.4654879570007324
train gradient:  0.1051292484231681
iteration : 423
train acc:  0.78125
train loss:  0.4570404589176178
train gradient:  0.13106333140551313
iteration : 424
train acc:  0.6796875
train loss:  0.5923927426338196
train gradient:  0.18668833636039456
iteration : 425
train acc:  0.765625
train loss:  0.477215439081192
train gradient:  0.10289057909285368
iteration : 426
train acc:  0.703125
train loss:  0.47294431924819946
train gradient:  0.14568669451932686
iteration : 427
train acc:  0.78125
train loss:  0.4430856704711914
train gradient:  0.11393618308316134
iteration : 428
train acc:  0.7265625
train loss:  0.5034657120704651
train gradient:  0.1317109240807231
iteration : 429
train acc:  0.71875
train loss:  0.4875447750091553
train gradient:  0.15297103086431257
iteration : 430
train acc:  0.765625
train loss:  0.4435562491416931
train gradient:  0.12327994918648888
iteration : 431
train acc:  0.7734375
train loss:  0.43830275535583496
train gradient:  0.1257761325189028
iteration : 432
train acc:  0.734375
train loss:  0.5132150650024414
train gradient:  0.12936279305783163
iteration : 433
train acc:  0.6796875
train loss:  0.564932107925415
train gradient:  0.16272112025900443
iteration : 434
train acc:  0.78125
train loss:  0.47203928232192993
train gradient:  0.11433174295381535
iteration : 435
train acc:  0.7578125
train loss:  0.44188159704208374
train gradient:  0.09840799430256979
iteration : 436
train acc:  0.7421875
train loss:  0.4964427947998047
train gradient:  0.12192282712276624
iteration : 437
train acc:  0.78125
train loss:  0.4822536110877991
train gradient:  0.11057234976714517
iteration : 438
train acc:  0.7421875
train loss:  0.4709136486053467
train gradient:  0.09594886543164328
iteration : 439
train acc:  0.75
train loss:  0.48458221554756165
train gradient:  0.1189464017990904
iteration : 440
train acc:  0.7265625
train loss:  0.5105587840080261
train gradient:  0.1609804248849706
iteration : 441
train acc:  0.71875
train loss:  0.5329217314720154
train gradient:  0.13827068837494083
iteration : 442
train acc:  0.7578125
train loss:  0.45073074102401733
train gradient:  0.13347488767391913
iteration : 443
train acc:  0.7734375
train loss:  0.46438872814178467
train gradient:  0.14837755790759488
iteration : 444
train acc:  0.7421875
train loss:  0.4425857365131378
train gradient:  0.1053590283388267
iteration : 445
train acc:  0.7421875
train loss:  0.48758625984191895
train gradient:  0.15662263600018975
iteration : 446
train acc:  0.703125
train loss:  0.5278112888336182
train gradient:  0.1443735241453293
iteration : 447
train acc:  0.734375
train loss:  0.5519580841064453
train gradient:  0.1856412411806702
iteration : 448
train acc:  0.671875
train loss:  0.54020094871521
train gradient:  0.15621793291309966
iteration : 449
train acc:  0.71875
train loss:  0.514990508556366
train gradient:  0.16402541757088268
iteration : 450
train acc:  0.7421875
train loss:  0.4515348970890045
train gradient:  0.10523343770611955
iteration : 451
train acc:  0.734375
train loss:  0.5139387845993042
train gradient:  0.12069977760700366
iteration : 452
train acc:  0.7109375
train loss:  0.4870522618293762
train gradient:  0.13341357642209944
iteration : 453
train acc:  0.6953125
train loss:  0.5210353136062622
train gradient:  0.09804929986191957
iteration : 454
train acc:  0.7421875
train loss:  0.5193695425987244
train gradient:  0.12235784548249015
iteration : 455
train acc:  0.8125
train loss:  0.38814154267311096
train gradient:  0.07473234486726185
iteration : 456
train acc:  0.7421875
train loss:  0.4950138032436371
train gradient:  0.15275678566166084
iteration : 457
train acc:  0.796875
train loss:  0.4084056615829468
train gradient:  0.08791188031784694
iteration : 458
train acc:  0.75
train loss:  0.5393444299697876
train gradient:  0.14611214644265602
iteration : 459
train acc:  0.8046875
train loss:  0.42727601528167725
train gradient:  0.1026957061906823
iteration : 460
train acc:  0.7265625
train loss:  0.4928351640701294
train gradient:  0.13988065511947062
iteration : 461
train acc:  0.671875
train loss:  0.5370792150497437
train gradient:  0.16765261864555925
iteration : 462
train acc:  0.7734375
train loss:  0.46898624300956726
train gradient:  0.10161982548676225
iteration : 463
train acc:  0.8046875
train loss:  0.46963953971862793
train gradient:  0.10458265037092213
iteration : 464
train acc:  0.734375
train loss:  0.5330913066864014
train gradient:  0.15586809536950758
iteration : 465
train acc:  0.7578125
train loss:  0.48233669996261597
train gradient:  0.11913732531062776
iteration : 466
train acc:  0.78125
train loss:  0.4501103162765503
train gradient:  0.1313304456290227
iteration : 467
train acc:  0.7421875
train loss:  0.46745991706848145
train gradient:  0.11334004091430504
iteration : 468
train acc:  0.7109375
train loss:  0.510223925113678
train gradient:  0.14825292923173938
iteration : 469
train acc:  0.7734375
train loss:  0.5205923318862915
train gradient:  0.12681244307438483
iteration : 470
train acc:  0.75
train loss:  0.47021833062171936
train gradient:  0.1095450957302367
iteration : 471
train acc:  0.75
train loss:  0.48153042793273926
train gradient:  0.1266905599961174
iteration : 472
train acc:  0.7578125
train loss:  0.47041672468185425
train gradient:  0.11971280592149978
iteration : 473
train acc:  0.6640625
train loss:  0.5165549516677856
train gradient:  0.10465591517257407
iteration : 474
train acc:  0.8125
train loss:  0.4519444704055786
train gradient:  0.10825001064762246
iteration : 475
train acc:  0.7109375
train loss:  0.5213706493377686
train gradient:  0.11624723694228803
iteration : 476
train acc:  0.7578125
train loss:  0.4788670539855957
train gradient:  0.1190884324962892
iteration : 477
train acc:  0.859375
train loss:  0.4000796675682068
train gradient:  0.10123042509275801
iteration : 478
train acc:  0.7109375
train loss:  0.5487223863601685
train gradient:  0.19453753409204244
iteration : 479
train acc:  0.765625
train loss:  0.45469969511032104
train gradient:  0.10233158272267692
iteration : 480
train acc:  0.828125
train loss:  0.4132831394672394
train gradient:  0.08340995956486444
iteration : 481
train acc:  0.8359375
train loss:  0.3907434046268463
train gradient:  0.07703381539776068
iteration : 482
train acc:  0.75
train loss:  0.511906087398529
train gradient:  0.15641630118426866
iteration : 483
train acc:  0.7421875
train loss:  0.4208293855190277
train gradient:  0.11167662406177933
iteration : 484
train acc:  0.8359375
train loss:  0.4291045069694519
train gradient:  0.08313470090984565
iteration : 485
train acc:  0.7734375
train loss:  0.4695453643798828
train gradient:  0.12128542187308572
iteration : 486
train acc:  0.6953125
train loss:  0.5277776718139648
train gradient:  0.13375517467314613
iteration : 487
train acc:  0.7734375
train loss:  0.4467231035232544
train gradient:  0.10489285543249183
iteration : 488
train acc:  0.71875
train loss:  0.5388691425323486
train gradient:  0.13777666068671374
iteration : 489
train acc:  0.734375
train loss:  0.49475252628326416
train gradient:  0.11426574803674175
iteration : 490
train acc:  0.703125
train loss:  0.5728268623352051
train gradient:  0.19391667736615387
iteration : 491
train acc:  0.7734375
train loss:  0.4624718725681305
train gradient:  0.1218367909672543
iteration : 492
train acc:  0.7109375
train loss:  0.4817010760307312
train gradient:  0.14113360201684083
iteration : 493
train acc:  0.8125
train loss:  0.44849714636802673
train gradient:  0.10901465240196696
iteration : 494
train acc:  0.7421875
train loss:  0.5453131794929504
train gradient:  0.24971148340105917
iteration : 495
train acc:  0.7265625
train loss:  0.4810996353626251
train gradient:  0.15702734386055578
iteration : 496
train acc:  0.7421875
train loss:  0.512121319770813
train gradient:  0.12460132560742748
iteration : 497
train acc:  0.84375
train loss:  0.4158093333244324
train gradient:  0.09952112425693178
iteration : 498
train acc:  0.734375
train loss:  0.49228745698928833
train gradient:  0.13173305500022053
iteration : 499
train acc:  0.7734375
train loss:  0.5022901892662048
train gradient:  0.11698964576916572
iteration : 500
train acc:  0.8203125
train loss:  0.3805142045021057
train gradient:  0.09352366718070017
iteration : 501
train acc:  0.7265625
train loss:  0.5156581401824951
train gradient:  0.12719284961531852
iteration : 502
train acc:  0.7890625
train loss:  0.4355868697166443
train gradient:  0.09383597328382556
iteration : 503
train acc:  0.8359375
train loss:  0.40998831391334534
train gradient:  0.09252750261704629
iteration : 504
train acc:  0.7421875
train loss:  0.5040735006332397
train gradient:  0.13183662693040527
iteration : 505
train acc:  0.75
train loss:  0.4637123942375183
train gradient:  0.11359261636288018
iteration : 506
train acc:  0.765625
train loss:  0.5188714265823364
train gradient:  0.12848414452807916
iteration : 507
train acc:  0.703125
train loss:  0.4995203912258148
train gradient:  0.15184108652455425
iteration : 508
train acc:  0.7734375
train loss:  0.43167978525161743
train gradient:  0.10225289774325813
iteration : 509
train acc:  0.78125
train loss:  0.4544646143913269
train gradient:  0.11822324350292238
iteration : 510
train acc:  0.7421875
train loss:  0.4535139799118042
train gradient:  0.0909548404439693
iteration : 511
train acc:  0.8125
train loss:  0.44221231341362
train gradient:  0.0880091740534292
iteration : 512
train acc:  0.7265625
train loss:  0.5025843381881714
train gradient:  0.13907181925915407
iteration : 513
train acc:  0.78125
train loss:  0.4798872172832489
train gradient:  0.13481648862289847
iteration : 514
train acc:  0.78125
train loss:  0.4598461985588074
train gradient:  0.11972702368231358
iteration : 515
train acc:  0.7578125
train loss:  0.46871480345726013
train gradient:  0.10773814485984513
iteration : 516
train acc:  0.7265625
train loss:  0.4887371063232422
train gradient:  0.16141797469931227
iteration : 517
train acc:  0.734375
train loss:  0.5066526532173157
train gradient:  0.11754360706356344
iteration : 518
train acc:  0.703125
train loss:  0.5151422023773193
train gradient:  0.13505868554573142
iteration : 519
train acc:  0.84375
train loss:  0.3854261040687561
train gradient:  0.10114150633582662
iteration : 520
train acc:  0.7265625
train loss:  0.4985380172729492
train gradient:  0.1128973327928274
iteration : 521
train acc:  0.7421875
train loss:  0.4809745252132416
train gradient:  0.10864953873348242
iteration : 522
train acc:  0.765625
train loss:  0.5052521228790283
train gradient:  0.15179260742086553
iteration : 523
train acc:  0.75
train loss:  0.45533761382102966
train gradient:  0.10347449423901343
iteration : 524
train acc:  0.6953125
train loss:  0.5653022527694702
train gradient:  0.16103721941415722
iteration : 525
train acc:  0.734375
train loss:  0.5328483581542969
train gradient:  0.14089213798741923
iteration : 526
train acc:  0.75
train loss:  0.5504255294799805
train gradient:  0.16748914158958164
iteration : 527
train acc:  0.71875
train loss:  0.4906248450279236
train gradient:  0.11690716530337464
iteration : 528
train acc:  0.7421875
train loss:  0.5003715753555298
train gradient:  0.17285660855089646
iteration : 529
train acc:  0.7421875
train loss:  0.5006591081619263
train gradient:  0.12528163730362357
iteration : 530
train acc:  0.8125
train loss:  0.4044312834739685
train gradient:  0.0844840015322246
iteration : 531
train acc:  0.78125
train loss:  0.45004740357398987
train gradient:  0.15547879885400512
iteration : 532
train acc:  0.75
train loss:  0.4710122346878052
train gradient:  0.10615951647784656
iteration : 533
train acc:  0.6875
train loss:  0.4968824088573456
train gradient:  0.14479741152776798
iteration : 534
train acc:  0.75
train loss:  0.47395092248916626
train gradient:  0.11606899110691105
iteration : 535
train acc:  0.7578125
train loss:  0.44380611181259155
train gradient:  0.11247548991608618
iteration : 536
train acc:  0.78125
train loss:  0.48573029041290283
train gradient:  0.1463769355475168
iteration : 537
train acc:  0.703125
train loss:  0.5517866015434265
train gradient:  0.16337096693641767
iteration : 538
train acc:  0.765625
train loss:  0.5117703676223755
train gradient:  0.17566744644724958
iteration : 539
train acc:  0.75
train loss:  0.5424608588218689
train gradient:  0.14683407413440897
iteration : 540
train acc:  0.7109375
train loss:  0.5150488615036011
train gradient:  0.1469695633695737
iteration : 541
train acc:  0.78125
train loss:  0.4508773386478424
train gradient:  0.12971710312039897
iteration : 542
train acc:  0.7734375
train loss:  0.46908268332481384
train gradient:  0.12730368678588277
iteration : 543
train acc:  0.7421875
train loss:  0.5051819086074829
train gradient:  0.15010135301760075
iteration : 544
train acc:  0.734375
train loss:  0.4945974349975586
train gradient:  0.12945697387876673
iteration : 545
train acc:  0.796875
train loss:  0.42905211448669434
train gradient:  0.10361859997608092
iteration : 546
train acc:  0.734375
train loss:  0.524219274520874
train gradient:  0.13959159174492514
iteration : 547
train acc:  0.78125
train loss:  0.491976797580719
train gradient:  0.14115117659353948
iteration : 548
train acc:  0.75
train loss:  0.5069263577461243
train gradient:  0.136587099162293
iteration : 549
train acc:  0.7578125
train loss:  0.49141502380371094
train gradient:  0.11458664520024653
iteration : 550
train acc:  0.7578125
train loss:  0.5014919638633728
train gradient:  0.12513179330467034
iteration : 551
train acc:  0.75
train loss:  0.4329356253147125
train gradient:  0.09455962906192447
iteration : 552
train acc:  0.734375
train loss:  0.515855610370636
train gradient:  0.15856530167374971
iteration : 553
train acc:  0.78125
train loss:  0.501783549785614
train gradient:  0.1736254640718891
iteration : 554
train acc:  0.6328125
train loss:  0.5898628234863281
train gradient:  0.1591242623225111
iteration : 555
train acc:  0.6953125
train loss:  0.5453524589538574
train gradient:  0.16528585197420664
iteration : 556
train acc:  0.7734375
train loss:  0.45843762159347534
train gradient:  0.10891002078969438
iteration : 557
train acc:  0.75
train loss:  0.5335364937782288
train gradient:  0.12263022884428453
iteration : 558
train acc:  0.71875
train loss:  0.4801589846611023
train gradient:  0.13019578051985806
iteration : 559
train acc:  0.8125
train loss:  0.42314308881759644
train gradient:  0.10945137093408915
iteration : 560
train acc:  0.7421875
train loss:  0.46636074781417847
train gradient:  0.10921742554885389
iteration : 561
train acc:  0.75
train loss:  0.4552196264266968
train gradient:  0.11733140872654838
iteration : 562
train acc:  0.7578125
train loss:  0.4708554148674011
train gradient:  0.11047910009792493
iteration : 563
train acc:  0.7578125
train loss:  0.4502888321876526
train gradient:  0.12831373439378335
iteration : 564
train acc:  0.6796875
train loss:  0.5640764832496643
train gradient:  0.1306835575480692
iteration : 565
train acc:  0.765625
train loss:  0.4604678153991699
train gradient:  0.12786473228624523
iteration : 566
train acc:  0.7109375
train loss:  0.4875359535217285
train gradient:  0.10475674454677872
iteration : 567
train acc:  0.796875
train loss:  0.43114030361175537
train gradient:  0.11960669591952824
iteration : 568
train acc:  0.6875
train loss:  0.5358537435531616
train gradient:  0.15135462085325957
iteration : 569
train acc:  0.734375
train loss:  0.5148130655288696
train gradient:  0.16272962358987544
iteration : 570
train acc:  0.75
train loss:  0.47627437114715576
train gradient:  0.19006600029095644
iteration : 571
train acc:  0.6875
train loss:  0.5589035153388977
train gradient:  0.1757843533024019
iteration : 572
train acc:  0.78125
train loss:  0.4730970561504364
train gradient:  0.1375313070427271
iteration : 573
train acc:  0.734375
train loss:  0.5577964782714844
train gradient:  0.15710871210777005
iteration : 574
train acc:  0.796875
train loss:  0.4835337996482849
train gradient:  0.10624074835251784
iteration : 575
train acc:  0.640625
train loss:  0.6295311450958252
train gradient:  0.2001322857763385
iteration : 576
train acc:  0.78125
train loss:  0.45415717363357544
train gradient:  0.1031614859218182
iteration : 577
train acc:  0.734375
train loss:  0.43238699436187744
train gradient:  0.08895257632124084
iteration : 578
train acc:  0.703125
train loss:  0.5625734329223633
train gradient:  0.1559124327203834
iteration : 579
train acc:  0.703125
train loss:  0.5353876352310181
train gradient:  0.15044152699982177
iteration : 580
train acc:  0.8046875
train loss:  0.47065556049346924
train gradient:  0.11847539907602854
iteration : 581
train acc:  0.75
train loss:  0.5108583569526672
train gradient:  0.13806274671739974
iteration : 582
train acc:  0.734375
train loss:  0.46226391196250916
train gradient:  0.10466867718039692
iteration : 583
train acc:  0.7421875
train loss:  0.49116235971450806
train gradient:  0.14389191931907547
iteration : 584
train acc:  0.7421875
train loss:  0.4703604578971863
train gradient:  0.1002706174457682
iteration : 585
train acc:  0.7421875
train loss:  0.4596424102783203
train gradient:  0.13179169833460685
iteration : 586
train acc:  0.7578125
train loss:  0.46322008967399597
train gradient:  0.12288930721627697
iteration : 587
train acc:  0.75
train loss:  0.48103004693984985
train gradient:  0.13807721996004707
iteration : 588
train acc:  0.765625
train loss:  0.4423123598098755
train gradient:  0.09172113322616501
iteration : 589
train acc:  0.765625
train loss:  0.49188244342803955
train gradient:  0.10187359594395966
iteration : 590
train acc:  0.8046875
train loss:  0.4283652901649475
train gradient:  0.08542604445057554
iteration : 591
train acc:  0.8046875
train loss:  0.435530424118042
train gradient:  0.1089105865350866
iteration : 592
train acc:  0.7734375
train loss:  0.44077932834625244
train gradient:  0.13441269497536448
iteration : 593
train acc:  0.75
train loss:  0.4651884436607361
train gradient:  0.09298991619365951
iteration : 594
train acc:  0.7734375
train loss:  0.4496341347694397
train gradient:  0.1395345933816451
iteration : 595
train acc:  0.734375
train loss:  0.5064065456390381
train gradient:  0.1247574042895485
iteration : 596
train acc:  0.71875
train loss:  0.4718126654624939
train gradient:  0.14486012062432474
iteration : 597
train acc:  0.75
train loss:  0.46804022789001465
train gradient:  0.11184804084566018
iteration : 598
train acc:  0.734375
train loss:  0.4796149432659149
train gradient:  0.1334248665822328
iteration : 599
train acc:  0.7890625
train loss:  0.4588003158569336
train gradient:  0.1201251251917525
iteration : 600
train acc:  0.7890625
train loss:  0.47203636169433594
train gradient:  0.13488604437912666
iteration : 601
train acc:  0.7421875
train loss:  0.476387083530426
train gradient:  0.1226327606897462
iteration : 602
train acc:  0.7421875
train loss:  0.48465877771377563
train gradient:  0.14545358243942394
iteration : 603
train acc:  0.7109375
train loss:  0.4747471809387207
train gradient:  0.15882642142228381
iteration : 604
train acc:  0.734375
train loss:  0.5344722270965576
train gradient:  0.1916173933015839
iteration : 605
train acc:  0.7578125
train loss:  0.4628887474536896
train gradient:  0.09251264070435167
iteration : 606
train acc:  0.796875
train loss:  0.47737568616867065
train gradient:  0.10726707177389697
iteration : 607
train acc:  0.7265625
train loss:  0.5355543494224548
train gradient:  0.18960336702829658
iteration : 608
train acc:  0.7734375
train loss:  0.4515589475631714
train gradient:  0.1157569259602049
iteration : 609
train acc:  0.6875
train loss:  0.5538357496261597
train gradient:  0.16704315123631758
iteration : 610
train acc:  0.765625
train loss:  0.47250401973724365
train gradient:  0.14178734723700964
iteration : 611
train acc:  0.7421875
train loss:  0.48709559440612793
train gradient:  0.11451774726781841
iteration : 612
train acc:  0.75
train loss:  0.4719986617565155
train gradient:  0.11745488247080695
iteration : 613
train acc:  0.8125
train loss:  0.42799296975135803
train gradient:  0.09182182147532826
iteration : 614
train acc:  0.734375
train loss:  0.5737606883049011
train gradient:  0.137524284367433
iteration : 615
train acc:  0.703125
train loss:  0.5081725120544434
train gradient:  0.12194946726771883
iteration : 616
train acc:  0.734375
train loss:  0.5117545127868652
train gradient:  0.14275118693915875
iteration : 617
train acc:  0.78125
train loss:  0.4867159128189087
train gradient:  0.12977623230197738
iteration : 618
train acc:  0.8125
train loss:  0.481187105178833
train gradient:  0.12410193224473211
iteration : 619
train acc:  0.6953125
train loss:  0.5964781045913696
train gradient:  0.15150810333526216
iteration : 620
train acc:  0.75
train loss:  0.5291900634765625
train gradient:  0.14451140523255393
iteration : 621
train acc:  0.7421875
train loss:  0.5130507349967957
train gradient:  0.15372047565123603
iteration : 622
train acc:  0.78125
train loss:  0.4729389548301697
train gradient:  0.12101650972980808
iteration : 623
train acc:  0.703125
train loss:  0.5490186214447021
train gradient:  0.14189512362883933
iteration : 624
train acc:  0.7890625
train loss:  0.41084563732147217
train gradient:  0.0899540472967117
iteration : 625
train acc:  0.671875
train loss:  0.5530165433883667
train gradient:  0.14647376833078685
iteration : 626
train acc:  0.6875
train loss:  0.5629099011421204
train gradient:  0.14966638740678823
iteration : 627
train acc:  0.734375
train loss:  0.503039538860321
train gradient:  0.15060556133292696
iteration : 628
train acc:  0.734375
train loss:  0.4919363856315613
train gradient:  0.14249161081176198
iteration : 629
train acc:  0.765625
train loss:  0.484006404876709
train gradient:  0.10931707054189643
iteration : 630
train acc:  0.78125
train loss:  0.43126100301742554
train gradient:  0.0910332013946452
iteration : 631
train acc:  0.796875
train loss:  0.4621831476688385
train gradient:  0.10767250310741168
iteration : 632
train acc:  0.7734375
train loss:  0.500708818435669
train gradient:  0.10295887263158349
iteration : 633
train acc:  0.7421875
train loss:  0.48787355422973633
train gradient:  0.11224372779511396
iteration : 634
train acc:  0.7578125
train loss:  0.4756113588809967
train gradient:  0.1120038264885275
iteration : 635
train acc:  0.7109375
train loss:  0.5034652948379517
train gradient:  0.12331055317235956
iteration : 636
train acc:  0.703125
train loss:  0.49242544174194336
train gradient:  0.12645922187519604
iteration : 637
train acc:  0.734375
train loss:  0.48685622215270996
train gradient:  0.11368720736314505
iteration : 638
train acc:  0.75
train loss:  0.461769700050354
train gradient:  0.112069022636128
iteration : 639
train acc:  0.640625
train loss:  0.5403429269790649
train gradient:  0.144952930339308
iteration : 640
train acc:  0.7734375
train loss:  0.435547411441803
train gradient:  0.11535646589574101
iteration : 641
train acc:  0.78125
train loss:  0.47891485691070557
train gradient:  0.10541373997420332
iteration : 642
train acc:  0.7734375
train loss:  0.4750855565071106
train gradient:  0.16038512886153655
iteration : 643
train acc:  0.7734375
train loss:  0.4486842155456543
train gradient:  0.105111391554908
iteration : 644
train acc:  0.78125
train loss:  0.4247225821018219
train gradient:  0.10234144960196659
iteration : 645
train acc:  0.7578125
train loss:  0.4424629509449005
train gradient:  0.11008603202494205
iteration : 646
train acc:  0.7421875
train loss:  0.5185611248016357
train gradient:  0.14293972624545925
iteration : 647
train acc:  0.7734375
train loss:  0.4396895170211792
train gradient:  0.10055644709538056
iteration : 648
train acc:  0.7265625
train loss:  0.48547396063804626
train gradient:  0.13218603412587454
iteration : 649
train acc:  0.8046875
train loss:  0.4903876483440399
train gradient:  0.11448954919872911
iteration : 650
train acc:  0.765625
train loss:  0.46593064069747925
train gradient:  0.09671555338784271
iteration : 651
train acc:  0.7421875
train loss:  0.4912640154361725
train gradient:  0.1155687944654503
iteration : 652
train acc:  0.7578125
train loss:  0.4797143340110779
train gradient:  0.12071831768930592
iteration : 653
train acc:  0.7421875
train loss:  0.481221079826355
train gradient:  0.11853696050046425
iteration : 654
train acc:  0.78125
train loss:  0.5106013417243958
train gradient:  0.17215974188659006
iteration : 655
train acc:  0.75
train loss:  0.5367251634597778
train gradient:  0.2148624936429972
iteration : 656
train acc:  0.8046875
train loss:  0.46210718154907227
train gradient:  0.11135721204136331
iteration : 657
train acc:  0.828125
train loss:  0.40324562788009644
train gradient:  0.07495540135727817
iteration : 658
train acc:  0.71875
train loss:  0.5356507301330566
train gradient:  0.1623783446832554
iteration : 659
train acc:  0.7890625
train loss:  0.44221270084381104
train gradient:  0.09555625343975596
iteration : 660
train acc:  0.796875
train loss:  0.43349871039390564
train gradient:  0.09191434883319137
iteration : 661
train acc:  0.796875
train loss:  0.43278968334198
train gradient:  0.09831116306621081
iteration : 662
train acc:  0.765625
train loss:  0.5181925892829895
train gradient:  0.15954278498161234
iteration : 663
train acc:  0.640625
train loss:  0.5511042475700378
train gradient:  0.13942217662503453
iteration : 664
train acc:  0.796875
train loss:  0.4201517701148987
train gradient:  0.09213174052306869
iteration : 665
train acc:  0.71875
train loss:  0.5067940950393677
train gradient:  0.12170427430036468
iteration : 666
train acc:  0.796875
train loss:  0.4404359459877014
train gradient:  0.0931263869899909
iteration : 667
train acc:  0.71875
train loss:  0.5009121298789978
train gradient:  0.14199897561585967
iteration : 668
train acc:  0.75
train loss:  0.4834524989128113
train gradient:  0.13744406735516318
iteration : 669
train acc:  0.7109375
train loss:  0.5103831887245178
train gradient:  0.14942351703241025
iteration : 670
train acc:  0.703125
train loss:  0.5612521767616272
train gradient:  0.18935379108441514
iteration : 671
train acc:  0.78125
train loss:  0.42303895950317383
train gradient:  0.13272092034992428
iteration : 672
train acc:  0.8046875
train loss:  0.40804851055145264
train gradient:  0.09757471466361237
iteration : 673
train acc:  0.8515625
train loss:  0.3696081042289734
train gradient:  0.0852966352468445
iteration : 674
train acc:  0.6953125
train loss:  0.5534453392028809
train gradient:  0.14676637897001155
iteration : 675
train acc:  0.734375
train loss:  0.4647684693336487
train gradient:  0.10083064433216754
iteration : 676
train acc:  0.796875
train loss:  0.41058340668678284
train gradient:  0.08220334766920138
iteration : 677
train acc:  0.8125
train loss:  0.40808790922164917
train gradient:  0.08544910922522769
iteration : 678
train acc:  0.6796875
train loss:  0.5229500532150269
train gradient:  0.133486111617183
iteration : 679
train acc:  0.71875
train loss:  0.5033465027809143
train gradient:  0.11321290883276487
iteration : 680
train acc:  0.75
train loss:  0.4949401617050171
train gradient:  0.12327879809518623
iteration : 681
train acc:  0.7734375
train loss:  0.43218672275543213
train gradient:  0.10774196064631644
iteration : 682
train acc:  0.796875
train loss:  0.4131651520729065
train gradient:  0.10696380388777696
iteration : 683
train acc:  0.671875
train loss:  0.5675402283668518
train gradient:  0.18181603352796044
iteration : 684
train acc:  0.75
train loss:  0.5002013444900513
train gradient:  0.11749826520091665
iteration : 685
train acc:  0.84375
train loss:  0.3917372226715088
train gradient:  0.09842297753172122
iteration : 686
train acc:  0.796875
train loss:  0.46284914016723633
train gradient:  0.11694539100048694
iteration : 687
train acc:  0.703125
train loss:  0.508472740650177
train gradient:  0.12435667291979108
iteration : 688
train acc:  0.7109375
train loss:  0.5148324966430664
train gradient:  0.12613574767632277
iteration : 689
train acc:  0.7109375
train loss:  0.5096980333328247
train gradient:  0.13652910364105575
iteration : 690
train acc:  0.7421875
train loss:  0.49263858795166016
train gradient:  0.12181016448696096
iteration : 691
train acc:  0.765625
train loss:  0.45826926827430725
train gradient:  0.14515956176713107
iteration : 692
train acc:  0.7734375
train loss:  0.5148688554763794
train gradient:  0.12987145191482213
iteration : 693
train acc:  0.734375
train loss:  0.5198869705200195
train gradient:  0.13475176985574083
iteration : 694
train acc:  0.7421875
train loss:  0.5192864537239075
train gradient:  0.10701742869535544
iteration : 695
train acc:  0.765625
train loss:  0.45020079612731934
train gradient:  0.14162813254702283
iteration : 696
train acc:  0.7578125
train loss:  0.4599027633666992
train gradient:  0.1068538030028674
iteration : 697
train acc:  0.703125
train loss:  0.5040315389633179
train gradient:  0.12870570634834116
iteration : 698
train acc:  0.7734375
train loss:  0.4255286455154419
train gradient:  0.09308555263337742
iteration : 699
train acc:  0.8046875
train loss:  0.4147711396217346
train gradient:  0.1018747128253429
iteration : 700
train acc:  0.71875
train loss:  0.5584002137184143
train gradient:  0.14709634444604242
iteration : 701
train acc:  0.7578125
train loss:  0.4975072145462036
train gradient:  0.1175636375734009
iteration : 702
train acc:  0.8046875
train loss:  0.4336339235305786
train gradient:  0.11045082978257915
iteration : 703
train acc:  0.7734375
train loss:  0.4332249164581299
train gradient:  0.11214749351938941
iteration : 704
train acc:  0.734375
train loss:  0.5088257193565369
train gradient:  0.13967718181692296
iteration : 705
train acc:  0.6875
train loss:  0.5616825222969055
train gradient:  0.17144855658725144
iteration : 706
train acc:  0.859375
train loss:  0.39436453580856323
train gradient:  0.09853317227501607
iteration : 707
train acc:  0.78125
train loss:  0.42364728450775146
train gradient:  0.09880788351001951
iteration : 708
train acc:  0.71875
train loss:  0.5091185569763184
train gradient:  0.10653495595283083
iteration : 709
train acc:  0.71875
train loss:  0.4826999306678772
train gradient:  0.11529242292718367
iteration : 710
train acc:  0.8125
train loss:  0.43266087770462036
train gradient:  0.08782727056518373
iteration : 711
train acc:  0.796875
train loss:  0.4683941602706909
train gradient:  0.11355419768797562
iteration : 712
train acc:  0.7265625
train loss:  0.48699596524238586
train gradient:  0.12207960867715013
iteration : 713
train acc:  0.71875
train loss:  0.5199083685874939
train gradient:  0.13893528763952223
iteration : 714
train acc:  0.7421875
train loss:  0.4792250990867615
train gradient:  0.11682657064359213
iteration : 715
train acc:  0.75
train loss:  0.437540739774704
train gradient:  0.10734203388273764
iteration : 716
train acc:  0.75
train loss:  0.47129112482070923
train gradient:  0.1071473605704245
iteration : 717
train acc:  0.75
train loss:  0.4749400019645691
train gradient:  0.1314050452170196
iteration : 718
train acc:  0.6796875
train loss:  0.5046428442001343
train gradient:  0.1389507223572553
iteration : 719
train acc:  0.765625
train loss:  0.45266225934028625
train gradient:  0.11753516214982723
iteration : 720
train acc:  0.7734375
train loss:  0.44888919591903687
train gradient:  0.1314779671715
iteration : 721
train acc:  0.734375
train loss:  0.4813418686389923
train gradient:  0.1406064869543186
iteration : 722
train acc:  0.84375
train loss:  0.46019840240478516
train gradient:  0.17352937936197255
iteration : 723
train acc:  0.65625
train loss:  0.5491570830345154
train gradient:  0.16357930991891073
iteration : 724
train acc:  0.6328125
train loss:  0.6287093758583069
train gradient:  0.1852424670642411
iteration : 725
train acc:  0.8046875
train loss:  0.4173111915588379
train gradient:  0.10309504298004127
iteration : 726
train acc:  0.8046875
train loss:  0.4214712977409363
train gradient:  0.11456354405291869
iteration : 727
train acc:  0.7578125
train loss:  0.46710020303726196
train gradient:  0.0986731125063094
iteration : 728
train acc:  0.7890625
train loss:  0.45417284965515137
train gradient:  0.10100044550982477
iteration : 729
train acc:  0.8046875
train loss:  0.40571877360343933
train gradient:  0.09466269670784676
iteration : 730
train acc:  0.75
train loss:  0.5000759363174438
train gradient:  0.1672350755410656
iteration : 731
train acc:  0.640625
train loss:  0.6025910377502441
train gradient:  0.17363031035397786
iteration : 732
train acc:  0.703125
train loss:  0.529173731803894
train gradient:  0.1403493113076476
iteration : 733
train acc:  0.7578125
train loss:  0.48770779371261597
train gradient:  0.1311137685734124
iteration : 734
train acc:  0.78125
train loss:  0.48087626695632935
train gradient:  0.13681180965761597
iteration : 735
train acc:  0.65625
train loss:  0.560147762298584
train gradient:  0.14424527442442547
iteration : 736
train acc:  0.734375
train loss:  0.4709486961364746
train gradient:  0.11543334198266383
iteration : 737
train acc:  0.78125
train loss:  0.43931421637535095
train gradient:  0.09331967870389643
iteration : 738
train acc:  0.8125
train loss:  0.4222041666507721
train gradient:  0.10060544351884568
iteration : 739
train acc:  0.7734375
train loss:  0.45386895537376404
train gradient:  0.10736362387397137
iteration : 740
train acc:  0.703125
train loss:  0.4908342957496643
train gradient:  0.10001510285856699
iteration : 741
train acc:  0.7265625
train loss:  0.45442360639572144
train gradient:  0.10210831568021636
iteration : 742
train acc:  0.734375
train loss:  0.5486189126968384
train gradient:  0.16924865336700307
iteration : 743
train acc:  0.78125
train loss:  0.5297666788101196
train gradient:  0.10769928640069965
iteration : 744
train acc:  0.75
train loss:  0.49458327889442444
train gradient:  0.12411718652167225
iteration : 745
train acc:  0.734375
train loss:  0.5715332627296448
train gradient:  0.2066376060798337
iteration : 746
train acc:  0.6953125
train loss:  0.5760160088539124
train gradient:  0.14205030256601703
iteration : 747
train acc:  0.6953125
train loss:  0.5296364426612854
train gradient:  0.15635013591945168
iteration : 748
train acc:  0.7109375
train loss:  0.5151949524879456
train gradient:  0.13820715893888308
iteration : 749
train acc:  0.6953125
train loss:  0.5165784955024719
train gradient:  0.17844165568197493
iteration : 750
train acc:  0.7578125
train loss:  0.4953860342502594
train gradient:  0.16841589616419714
iteration : 751
train acc:  0.703125
train loss:  0.5263030529022217
train gradient:  0.11534329986887006
iteration : 752
train acc:  0.8046875
train loss:  0.442017138004303
train gradient:  0.11492531442821256
iteration : 753
train acc:  0.7578125
train loss:  0.491113543510437
train gradient:  0.1339420226251365
iteration : 754
train acc:  0.8125
train loss:  0.4278123676776886
train gradient:  0.10158644354531332
iteration : 755
train acc:  0.7890625
train loss:  0.4654258191585541
train gradient:  0.13539363463735388
iteration : 756
train acc:  0.7265625
train loss:  0.5313339233398438
train gradient:  0.15637737938009982
iteration : 757
train acc:  0.7578125
train loss:  0.46576711535453796
train gradient:  0.14102507652321744
iteration : 758
train acc:  0.734375
train loss:  0.5120222568511963
train gradient:  0.14326940945986333
iteration : 759
train acc:  0.765625
train loss:  0.49543261528015137
train gradient:  0.1193398941903641
iteration : 760
train acc:  0.78125
train loss:  0.4120330214500427
train gradient:  0.10353591355814681
iteration : 761
train acc:  0.796875
train loss:  0.4789019525051117
train gradient:  0.12152577180396702
iteration : 762
train acc:  0.78125
train loss:  0.4308992922306061
train gradient:  0.12315785810475831
iteration : 763
train acc:  0.7421875
train loss:  0.48151931166648865
train gradient:  0.12895789140133482
iteration : 764
train acc:  0.6796875
train loss:  0.5234324932098389
train gradient:  0.14334306831665483
iteration : 765
train acc:  0.8046875
train loss:  0.45170730352401733
train gradient:  0.10413239687452765
iteration : 766
train acc:  0.6953125
train loss:  0.506198525428772
train gradient:  0.11786661451639896
iteration : 767
train acc:  0.7421875
train loss:  0.5012609958648682
train gradient:  0.1540217675727958
iteration : 768
train acc:  0.7890625
train loss:  0.4215070605278015
train gradient:  0.10473454536893506
iteration : 769
train acc:  0.78125
train loss:  0.5159868001937866
train gradient:  0.14248486644888783
iteration : 770
train acc:  0.734375
train loss:  0.5513602495193481
train gradient:  0.1618853958840198
iteration : 771
train acc:  0.6953125
train loss:  0.49117788672447205
train gradient:  0.11370149793916359
iteration : 772
train acc:  0.78125
train loss:  0.42413243651390076
train gradient:  0.09825876728034558
iteration : 773
train acc:  0.7109375
train loss:  0.5488491654396057
train gradient:  0.16901327697935026
iteration : 774
train acc:  0.765625
train loss:  0.4551044702529907
train gradient:  0.12938043714027708
iteration : 775
train acc:  0.703125
train loss:  0.5821329951286316
train gradient:  0.1907171144234236
iteration : 776
train acc:  0.7890625
train loss:  0.39809784293174744
train gradient:  0.08075210165468766
iteration : 777
train acc:  0.6796875
train loss:  0.5037164688110352
train gradient:  0.10610165847148163
iteration : 778
train acc:  0.78125
train loss:  0.4361777603626251
train gradient:  0.09664552771201934
iteration : 779
train acc:  0.734375
train loss:  0.46954581141471863
train gradient:  0.11439975893515744
iteration : 780
train acc:  0.8046875
train loss:  0.43877410888671875
train gradient:  0.10217029486301792
iteration : 781
train acc:  0.6640625
train loss:  0.5945605039596558
train gradient:  0.19615909310815347
iteration : 782
train acc:  0.75
train loss:  0.5228515863418579
train gradient:  0.1414145722243879
iteration : 783
train acc:  0.75
train loss:  0.5153740644454956
train gradient:  0.15597979722382416
iteration : 784
train acc:  0.703125
train loss:  0.48401179909706116
train gradient:  0.12293776943434437
iteration : 785
train acc:  0.78125
train loss:  0.4979228377342224
train gradient:  0.12462913149259881
iteration : 786
train acc:  0.7421875
train loss:  0.5038691759109497
train gradient:  0.11047227863632504
iteration : 787
train acc:  0.6875
train loss:  0.5400157570838928
train gradient:  0.1124295690375932
iteration : 788
train acc:  0.65625
train loss:  0.6182498931884766
train gradient:  0.2148930181388226
iteration : 789
train acc:  0.6953125
train loss:  0.5078046917915344
train gradient:  0.1195040746945412
iteration : 790
train acc:  0.75
train loss:  0.45740342140197754
train gradient:  0.09853767240062289
iteration : 791
train acc:  0.8125
train loss:  0.4114643335342407
train gradient:  0.09535758544614373
iteration : 792
train acc:  0.796875
train loss:  0.46281754970550537
train gradient:  0.11497650102652027
iteration : 793
train acc:  0.671875
train loss:  0.5794629454612732
train gradient:  0.18620416682427948
iteration : 794
train acc:  0.765625
train loss:  0.49777188897132874
train gradient:  0.12704212448559488
iteration : 795
train acc:  0.71875
train loss:  0.5892701148986816
train gradient:  0.1761045371498355
iteration : 796
train acc:  0.765625
train loss:  0.5307809114456177
train gradient:  0.17791725793964952
iteration : 797
train acc:  0.71875
train loss:  0.48292219638824463
train gradient:  0.12069773488805008
iteration : 798
train acc:  0.765625
train loss:  0.49219873547554016
train gradient:  0.13601046896594537
iteration : 799
train acc:  0.78125
train loss:  0.4843924045562744
train gradient:  0.12715203369351452
iteration : 800
train acc:  0.75
train loss:  0.4712669849395752
train gradient:  0.1549872704883497
iteration : 801
train acc:  0.7578125
train loss:  0.5044957399368286
train gradient:  0.1410064027112355
iteration : 802
train acc:  0.734375
train loss:  0.4557344913482666
train gradient:  0.11869481786828118
iteration : 803
train acc:  0.703125
train loss:  0.5148874521255493
train gradient:  0.1668971710721735
iteration : 804
train acc:  0.765625
train loss:  0.44409048557281494
train gradient:  0.12002356795234069
iteration : 805
train acc:  0.78125
train loss:  0.472221314907074
train gradient:  0.1177497150040199
iteration : 806
train acc:  0.8203125
train loss:  0.47376903891563416
train gradient:  0.13079749484252523
iteration : 807
train acc:  0.78125
train loss:  0.3815768361091614
train gradient:  0.08441721636344429
iteration : 808
train acc:  0.796875
train loss:  0.44624239206314087
train gradient:  0.11188795747642925
iteration : 809
train acc:  0.6640625
train loss:  0.5550259947776794
train gradient:  0.13026964452379503
iteration : 810
train acc:  0.765625
train loss:  0.5233067274093628
train gradient:  0.22026686797191902
iteration : 811
train acc:  0.78125
train loss:  0.4273862838745117
train gradient:  0.07604032512333596
iteration : 812
train acc:  0.734375
train loss:  0.5131691098213196
train gradient:  0.13193639278817298
iteration : 813
train acc:  0.7109375
train loss:  0.4974988102912903
train gradient:  0.15646231325314391
iteration : 814
train acc:  0.734375
train loss:  0.49957674741744995
train gradient:  0.11477625731041519
iteration : 815
train acc:  0.7109375
train loss:  0.5246137380599976
train gradient:  0.13446109484131757
iteration : 816
train acc:  0.734375
train loss:  0.5037876963615417
train gradient:  0.13809298267247105
iteration : 817
train acc:  0.78125
train loss:  0.439578115940094
train gradient:  0.1225232648213916
iteration : 818
train acc:  0.8046875
train loss:  0.42554983496665955
train gradient:  0.09231785269638064
iteration : 819
train acc:  0.6796875
train loss:  0.5075546503067017
train gradient:  0.1244924556958546
iteration : 820
train acc:  0.78125
train loss:  0.43840500712394714
train gradient:  0.08522136613620805
iteration : 821
train acc:  0.7265625
train loss:  0.4947567880153656
train gradient:  0.14267092481894694
iteration : 822
train acc:  0.6953125
train loss:  0.5357533693313599
train gradient:  0.13894592965717845
iteration : 823
train acc:  0.7734375
train loss:  0.4876881241798401
train gradient:  0.11915842354547515
iteration : 824
train acc:  0.703125
train loss:  0.4941323399543762
train gradient:  0.129442588043352
iteration : 825
train acc:  0.7421875
train loss:  0.468325138092041
train gradient:  0.12981461177452
iteration : 826
train acc:  0.8046875
train loss:  0.4164593815803528
train gradient:  0.10657642971415231
iteration : 827
train acc:  0.75
train loss:  0.4789420962333679
train gradient:  0.1340548141181867
iteration : 828
train acc:  0.71875
train loss:  0.5042837858200073
train gradient:  0.1101473638412933
iteration : 829
train acc:  0.75
train loss:  0.5034059286117554
train gradient:  0.1238180496807524
iteration : 830
train acc:  0.7578125
train loss:  0.4958738088607788
train gradient:  0.15121622918653038
iteration : 831
train acc:  0.65625
train loss:  0.6058359742164612
train gradient:  0.18681354547048074
iteration : 832
train acc:  0.734375
train loss:  0.5025812387466431
train gradient:  0.12597691377754158
iteration : 833
train acc:  0.703125
train loss:  0.5223621129989624
train gradient:  0.1302643956981722
iteration : 834
train acc:  0.7578125
train loss:  0.4745585024356842
train gradient:  0.11664615903488336
iteration : 835
train acc:  0.6328125
train loss:  0.5508067011833191
train gradient:  0.13589981649539198
iteration : 836
train acc:  0.734375
train loss:  0.5052813291549683
train gradient:  0.16570032097934267
iteration : 837
train acc:  0.75
train loss:  0.49909210205078125
train gradient:  0.10514942443945864
iteration : 838
train acc:  0.78125
train loss:  0.43242549896240234
train gradient:  0.11744641394998193
iteration : 839
train acc:  0.8046875
train loss:  0.4549817442893982
train gradient:  0.11065890433957093
iteration : 840
train acc:  0.7578125
train loss:  0.4928150773048401
train gradient:  0.12407915462436406
iteration : 841
train acc:  0.7265625
train loss:  0.478544145822525
train gradient:  0.10613085254289714
iteration : 842
train acc:  0.71875
train loss:  0.5233738422393799
train gradient:  0.16818406872956865
iteration : 843
train acc:  0.78125
train loss:  0.5152712464332581
train gradient:  0.12974303644632207
iteration : 844
train acc:  0.75
train loss:  0.4719841182231903
train gradient:  0.1011069607541743
iteration : 845
train acc:  0.7578125
train loss:  0.5107455849647522
train gradient:  0.13890334293368384
iteration : 846
train acc:  0.8203125
train loss:  0.4026585817337036
train gradient:  0.08974198568097096
iteration : 847
train acc:  0.6953125
train loss:  0.6081104278564453
train gradient:  0.21465384990670894
iteration : 848
train acc:  0.7734375
train loss:  0.48372018337249756
train gradient:  0.20364442078464887
iteration : 849
train acc:  0.6328125
train loss:  0.5650396943092346
train gradient:  0.17785173616677152
iteration : 850
train acc:  0.859375
train loss:  0.3979855179786682
train gradient:  0.10579526620511605
iteration : 851
train acc:  0.734375
train loss:  0.46112319827079773
train gradient:  0.1070974992107155
iteration : 852
train acc:  0.7734375
train loss:  0.44218510389328003
train gradient:  0.11514615696198699
iteration : 853
train acc:  0.8046875
train loss:  0.4277060925960541
train gradient:  0.10202972298733941
iteration : 854
train acc:  0.6796875
train loss:  0.5228272080421448
train gradient:  0.11529590911361179
iteration : 855
train acc:  0.765625
train loss:  0.4712015390396118
train gradient:  0.11591938422422769
iteration : 856
train acc:  0.671875
train loss:  0.535384476184845
train gradient:  0.10734338261522383
iteration : 857
train acc:  0.703125
train loss:  0.5420038104057312
train gradient:  0.13586032715942364
iteration : 858
train acc:  0.7109375
train loss:  0.5268997550010681
train gradient:  0.13276177709576809
iteration : 859
train acc:  0.7578125
train loss:  0.47431614995002747
train gradient:  0.10755222842832955
iteration : 860
train acc:  0.71875
train loss:  0.5070736408233643
train gradient:  0.11929613391574946
iteration : 861
train acc:  0.7421875
train loss:  0.5023709535598755
train gradient:  0.21925284523560445
iteration : 862
train acc:  0.796875
train loss:  0.43469542264938354
train gradient:  0.1072071934486229
iteration : 863
train acc:  0.7890625
train loss:  0.4071609377861023
train gradient:  0.10891444920792119
iteration : 864
train acc:  0.7109375
train loss:  0.512330174446106
train gradient:  0.15335195092350148
iteration : 865
train acc:  0.7578125
train loss:  0.503881573677063
train gradient:  0.18577761695223405
iteration : 866
train acc:  0.8125
train loss:  0.47306275367736816
train gradient:  0.10538640768151057
iteration : 867
train acc:  0.78125
train loss:  0.4412184953689575
train gradient:  0.11468366179317284
iteration : 868
train acc:  0.796875
train loss:  0.40842127799987793
train gradient:  0.10738052725497704
iteration : 869
train acc:  0.7578125
train loss:  0.46982407569885254
train gradient:  0.11558459585109701
iteration : 870
train acc:  0.734375
train loss:  0.45402002334594727
train gradient:  0.12457780282339044
iteration : 871
train acc:  0.8046875
train loss:  0.48148196935653687
train gradient:  0.125877156649699
iteration : 872
train acc:  0.8046875
train loss:  0.4399370551109314
train gradient:  0.09958493047408322
iteration : 873
train acc:  0.796875
train loss:  0.4603022336959839
train gradient:  0.12301874027005387
iteration : 874
train acc:  0.7109375
train loss:  0.47916269302368164
train gradient:  0.12245837348875693
iteration : 875
train acc:  0.6953125
train loss:  0.48597490787506104
train gradient:  0.11441082282179255
iteration : 876
train acc:  0.71875
train loss:  0.4855470061302185
train gradient:  0.16364416690485384
iteration : 877
train acc:  0.7421875
train loss:  0.5189412832260132
train gradient:  0.14142806392070245
iteration : 878
train acc:  0.71875
train loss:  0.4997917413711548
train gradient:  0.11560132005360044
iteration : 879
train acc:  0.8203125
train loss:  0.4090942144393921
train gradient:  0.08975732157194277
iteration : 880
train acc:  0.7578125
train loss:  0.49578994512557983
train gradient:  0.11569883930015938
iteration : 881
train acc:  0.7734375
train loss:  0.4498715400695801
train gradient:  0.0979903391109257
iteration : 882
train acc:  0.8203125
train loss:  0.34719252586364746
train gradient:  0.07521178337074381
iteration : 883
train acc:  0.765625
train loss:  0.48222678899765015
train gradient:  0.12098077639953826
iteration : 884
train acc:  0.7890625
train loss:  0.4640234708786011
train gradient:  0.11967788178234591
iteration : 885
train acc:  0.7734375
train loss:  0.4955984652042389
train gradient:  0.1586861164404163
iteration : 886
train acc:  0.78125
train loss:  0.42232027649879456
train gradient:  0.0917720757650372
iteration : 887
train acc:  0.796875
train loss:  0.48342815041542053
train gradient:  0.124099637280448
iteration : 888
train acc:  0.65625
train loss:  0.5649591684341431
train gradient:  0.1493651317647905
iteration : 889
train acc:  0.7890625
train loss:  0.47189241647720337
train gradient:  0.11099558245986489
iteration : 890
train acc:  0.6796875
train loss:  0.5180655717849731
train gradient:  0.1276582255761317
iteration : 891
train acc:  0.703125
train loss:  0.4719235599040985
train gradient:  0.13226533610154256
iteration : 892
train acc:  0.6953125
train loss:  0.524010956287384
train gradient:  0.12943185338577362
iteration : 893
train acc:  0.71875
train loss:  0.49496734142303467
train gradient:  0.14539467316681115
iteration : 894
train acc:  0.734375
train loss:  0.5255861282348633
train gradient:  0.1377376843968076
iteration : 895
train acc:  0.765625
train loss:  0.5098206400871277
train gradient:  0.14884678548961403
iteration : 896
train acc:  0.7734375
train loss:  0.4462940990924835
train gradient:  0.09033318223609516
iteration : 897
train acc:  0.75
train loss:  0.44654256105422974
train gradient:  0.0974643924416684
iteration : 898
train acc:  0.7578125
train loss:  0.46966099739074707
train gradient:  0.1484892703887633
iteration : 899
train acc:  0.7421875
train loss:  0.44910046458244324
train gradient:  0.10835383662821932
iteration : 900
train acc:  0.8203125
train loss:  0.40092045068740845
train gradient:  0.09399679389040921
iteration : 901
train acc:  0.75
train loss:  0.4293828010559082
train gradient:  0.088206593627533
iteration : 902
train acc:  0.8125
train loss:  0.39577269554138184
train gradient:  0.08558107872245478
iteration : 903
train acc:  0.78125
train loss:  0.4717315435409546
train gradient:  0.12380219772135313
iteration : 904
train acc:  0.765625
train loss:  0.46450507640838623
train gradient:  0.11787561528126231
iteration : 905
train acc:  0.78125
train loss:  0.4479284882545471
train gradient:  0.09085836831240943
iteration : 906
train acc:  0.7109375
train loss:  0.5732826590538025
train gradient:  0.1700192524198269
iteration : 907
train acc:  0.875
train loss:  0.3712344169616699
train gradient:  0.08963595376934291
iteration : 908
train acc:  0.75
train loss:  0.4755682051181793
train gradient:  0.15147212270113775
iteration : 909
train acc:  0.7734375
train loss:  0.4751323461532593
train gradient:  0.15034828199053074
iteration : 910
train acc:  0.703125
train loss:  0.5448552370071411
train gradient:  0.14850708279206531
iteration : 911
train acc:  0.78125
train loss:  0.5193419456481934
train gradient:  0.13526799861058114
iteration : 912
train acc:  0.7421875
train loss:  0.42577722668647766
train gradient:  0.10178845173066628
iteration : 913
train acc:  0.7734375
train loss:  0.5022841691970825
train gradient:  0.12044360306285518
iteration : 914
train acc:  0.7265625
train loss:  0.49850404262542725
train gradient:  0.11292750618495327
iteration : 915
train acc:  0.796875
train loss:  0.44015705585479736
train gradient:  0.13037430538774042
iteration : 916
train acc:  0.7265625
train loss:  0.47418031096458435
train gradient:  0.11114093477965749
iteration : 917
train acc:  0.7578125
train loss:  0.4774508774280548
train gradient:  0.14887948579535049
iteration : 918
train acc:  0.703125
train loss:  0.5336122512817383
train gradient:  0.14556478936080602
iteration : 919
train acc:  0.7578125
train loss:  0.43650227785110474
train gradient:  0.11065779359260945
iteration : 920
train acc:  0.7109375
train loss:  0.5351718664169312
train gradient:  0.1535209614829099
iteration : 921
train acc:  0.7109375
train loss:  0.5517798662185669
train gradient:  0.14252287468699626
iteration : 922
train acc:  0.8359375
train loss:  0.4446254372596741
train gradient:  0.0937930860399065
iteration : 923
train acc:  0.6796875
train loss:  0.5402606725692749
train gradient:  0.15793119006319967
iteration : 924
train acc:  0.7578125
train loss:  0.48082953691482544
train gradient:  0.16149656919402447
iteration : 925
train acc:  0.7421875
train loss:  0.4653910994529724
train gradient:  0.11374417174514835
iteration : 926
train acc:  0.765625
train loss:  0.49795979261398315
train gradient:  0.1285560745893028
iteration : 927
train acc:  0.796875
train loss:  0.4340311288833618
train gradient:  0.09979831377704544
iteration : 928
train acc:  0.84375
train loss:  0.35453879833221436
train gradient:  0.08108863199029027
iteration : 929
train acc:  0.7265625
train loss:  0.5063601136207581
train gradient:  0.139028569456425
iteration : 930
train acc:  0.7421875
train loss:  0.4688517451286316
train gradient:  0.11101241686283723
iteration : 931
train acc:  0.78125
train loss:  0.47957468032836914
train gradient:  0.12657462874058317
iteration : 932
train acc:  0.7265625
train loss:  0.4766450524330139
train gradient:  0.10416447715902498
iteration : 933
train acc:  0.796875
train loss:  0.4439440369606018
train gradient:  0.1039537410064512
iteration : 934
train acc:  0.7421875
train loss:  0.5624594688415527
train gradient:  0.15664690606904463
iteration : 935
train acc:  0.75
train loss:  0.46835339069366455
train gradient:  0.14371688530575116
iteration : 936
train acc:  0.8046875
train loss:  0.4254618287086487
train gradient:  0.09210208284861687
iteration : 937
train acc:  0.78125
train loss:  0.4357134699821472
train gradient:  0.11841252375647729
iteration : 938
train acc:  0.7578125
train loss:  0.4972233176231384
train gradient:  0.141461198649101
iteration : 939
train acc:  0.765625
train loss:  0.4092601537704468
train gradient:  0.07320434679871583
iteration : 940
train acc:  0.7265625
train loss:  0.47190529108047485
train gradient:  0.10038843843409571
iteration : 941
train acc:  0.8125
train loss:  0.44111722707748413
train gradient:  0.12569573348565
iteration : 942
train acc:  0.75
train loss:  0.46286511421203613
train gradient:  0.09943314795645454
iteration : 943
train acc:  0.7890625
train loss:  0.4412442445755005
train gradient:  0.09893934035096631
iteration : 944
train acc:  0.765625
train loss:  0.4922650456428528
train gradient:  0.19400569956214853
iteration : 945
train acc:  0.8125
train loss:  0.44368037581443787
train gradient:  0.1095386200567375
iteration : 946
train acc:  0.8046875
train loss:  0.4338645339012146
train gradient:  0.11239470649774795
iteration : 947
train acc:  0.703125
train loss:  0.4884188175201416
train gradient:  0.1184003095907783
iteration : 948
train acc:  0.7109375
train loss:  0.5239219665527344
train gradient:  0.1633219374460883
iteration : 949
train acc:  0.75
train loss:  0.49135178327560425
train gradient:  0.12342638761380685
iteration : 950
train acc:  0.6875
train loss:  0.5425691604614258
train gradient:  0.15290137604790013
iteration : 951
train acc:  0.8359375
train loss:  0.3959326446056366
train gradient:  0.10731154457869313
iteration : 952
train acc:  0.7734375
train loss:  0.47906187176704407
train gradient:  0.11316670517715804
iteration : 953
train acc:  0.734375
train loss:  0.5199869871139526
train gradient:  0.15029188246739755
iteration : 954
train acc:  0.78125
train loss:  0.4559764266014099
train gradient:  0.12139819656808169
iteration : 955
train acc:  0.7421875
train loss:  0.5176376700401306
train gradient:  0.13714438824672276
iteration : 956
train acc:  0.7578125
train loss:  0.4354446530342102
train gradient:  0.11055723029379996
iteration : 957
train acc:  0.6875
train loss:  0.5098356008529663
train gradient:  0.1735622427568508
iteration : 958
train acc:  0.734375
train loss:  0.4979831874370575
train gradient:  0.17310745713967576
iteration : 959
train acc:  0.7109375
train loss:  0.5114647150039673
train gradient:  0.1374573422963075
iteration : 960
train acc:  0.71875
train loss:  0.5164914131164551
train gradient:  0.15209716793162442
iteration : 961
train acc:  0.7578125
train loss:  0.4541771411895752
train gradient:  0.09113306967632535
iteration : 962
train acc:  0.6796875
train loss:  0.4968740940093994
train gradient:  0.12147696795147789
iteration : 963
train acc:  0.8046875
train loss:  0.40611159801483154
train gradient:  0.10923920269824855
iteration : 964
train acc:  0.7578125
train loss:  0.4581325352191925
train gradient:  0.13039073321871913
iteration : 965
train acc:  0.71875
train loss:  0.4929015040397644
train gradient:  0.14132807943609366
iteration : 966
train acc:  0.734375
train loss:  0.5361806154251099
train gradient:  0.1464688192269395
iteration : 967
train acc:  0.7578125
train loss:  0.513217568397522
train gradient:  0.11633471037899484
iteration : 968
train acc:  0.6875
train loss:  0.5405963659286499
train gradient:  0.1794346733140751
iteration : 969
train acc:  0.7265625
train loss:  0.5528818368911743
train gradient:  0.16197819214044057
iteration : 970
train acc:  0.7109375
train loss:  0.4891514778137207
train gradient:  0.14291433489136085
iteration : 971
train acc:  0.7890625
train loss:  0.4772786498069763
train gradient:  0.15250920488339512
iteration : 972
train acc:  0.7421875
train loss:  0.5057530403137207
train gradient:  0.12977845062820886
iteration : 973
train acc:  0.7578125
train loss:  0.4574195146560669
train gradient:  0.138602546852568
iteration : 974
train acc:  0.7734375
train loss:  0.43269169330596924
train gradient:  0.11552937160338712
iteration : 975
train acc:  0.796875
train loss:  0.47369542717933655
train gradient:  0.13841782701774297
iteration : 976
train acc:  0.7734375
train loss:  0.46732527017593384
train gradient:  0.11017774922648961
iteration : 977
train acc:  0.6640625
train loss:  0.5883991718292236
train gradient:  0.15182054150864538
iteration : 978
train acc:  0.7421875
train loss:  0.5126996636390686
train gradient:  0.21188974878526376
iteration : 979
train acc:  0.7109375
train loss:  0.5565512180328369
train gradient:  0.15014914041923286
iteration : 980
train acc:  0.71875
train loss:  0.5335688591003418
train gradient:  0.1367076222308048
iteration : 981
train acc:  0.75
train loss:  0.45217907428741455
train gradient:  0.10484069878601203
iteration : 982
train acc:  0.7578125
train loss:  0.4824497103691101
train gradient:  0.09565551860104816
iteration : 983
train acc:  0.7578125
train loss:  0.43948477506637573
train gradient:  0.09962216813027108
iteration : 984
train acc:  0.78125
train loss:  0.47363078594207764
train gradient:  0.12653258762081598
iteration : 985
train acc:  0.7421875
train loss:  0.5257754325866699
train gradient:  0.1553349507828214
iteration : 986
train acc:  0.7421875
train loss:  0.49520668387413025
train gradient:  0.12703563562322573
iteration : 987
train acc:  0.8359375
train loss:  0.42513883113861084
train gradient:  0.08279401181300779
iteration : 988
train acc:  0.7734375
train loss:  0.5107150673866272
train gradient:  0.1401052563259971
iteration : 989
train acc:  0.75
train loss:  0.462431401014328
train gradient:  0.11984828689530636
iteration : 990
train acc:  0.765625
train loss:  0.4470595121383667
train gradient:  0.11019072913781432
iteration : 991
train acc:  0.703125
train loss:  0.5219403505325317
train gradient:  0.1344640110623574
iteration : 992
train acc:  0.7890625
train loss:  0.45288217067718506
train gradient:  0.09857677511437496
iteration : 993
train acc:  0.7578125
train loss:  0.5130477547645569
train gradient:  0.13726541658916747
iteration : 994
train acc:  0.71875
train loss:  0.5384933948516846
train gradient:  0.1773015974973502
iteration : 995
train acc:  0.765625
train loss:  0.49416401982307434
train gradient:  0.1365522976542946
iteration : 996
train acc:  0.78125
train loss:  0.4530799984931946
train gradient:  0.11411155847166285
iteration : 997
train acc:  0.7578125
train loss:  0.5281779766082764
train gradient:  0.15077763132296051
iteration : 998
train acc:  0.8203125
train loss:  0.42999324202537537
train gradient:  0.10698067576168824
iteration : 999
train acc:  0.7421875
train loss:  0.5085587501525879
train gradient:  0.1438400352268211
iteration : 1000
train acc:  0.7578125
train loss:  0.4680502414703369
train gradient:  0.1252993098174882
iteration : 1001
train acc:  0.734375
train loss:  0.5211579203605652
train gradient:  0.17781145100070805
iteration : 1002
train acc:  0.7890625
train loss:  0.45240044593811035
train gradient:  0.11261606206576373
iteration : 1003
train acc:  0.78125
train loss:  0.4808058738708496
train gradient:  0.12581983828697035
iteration : 1004
train acc:  0.7265625
train loss:  0.5162349939346313
train gradient:  0.1462349085471652
iteration : 1005
train acc:  0.8125
train loss:  0.431949645280838
train gradient:  0.12152341848319853
iteration : 1006
train acc:  0.734375
train loss:  0.5063628554344177
train gradient:  0.10403050111852075
iteration : 1007
train acc:  0.734375
train loss:  0.48721960186958313
train gradient:  0.14593489391296413
iteration : 1008
train acc:  0.640625
train loss:  0.5786442756652832
train gradient:  0.15097717022564056
iteration : 1009
train acc:  0.7265625
train loss:  0.45870354771614075
train gradient:  0.1293990223675475
iteration : 1010
train acc:  0.703125
train loss:  0.4987933933734894
train gradient:  0.13074039661247677
iteration : 1011
train acc:  0.7578125
train loss:  0.44680044054985046
train gradient:  0.10896327328727716
iteration : 1012
train acc:  0.8046875
train loss:  0.4308457672595978
train gradient:  0.13828862107981715
iteration : 1013
train acc:  0.78125
train loss:  0.41309672594070435
train gradient:  0.10011998466488053
iteration : 1014
train acc:  0.828125
train loss:  0.41607001423835754
train gradient:  0.09692882486297909
iteration : 1015
train acc:  0.7421875
train loss:  0.454774409532547
train gradient:  0.11764316821710734
iteration : 1016
train acc:  0.6796875
train loss:  0.545802652835846
train gradient:  0.16529497944310323
iteration : 1017
train acc:  0.6953125
train loss:  0.5944353938102722
train gradient:  0.15477393125071148
iteration : 1018
train acc:  0.6953125
train loss:  0.4960649907588959
train gradient:  0.12824966620221648
iteration : 1019
train acc:  0.6875
train loss:  0.5264476537704468
train gradient:  0.16937216664627408
iteration : 1020
train acc:  0.8203125
train loss:  0.46931886672973633
train gradient:  0.1278455661319336
iteration : 1021
train acc:  0.6796875
train loss:  0.5307585000991821
train gradient:  0.12378733358731189
iteration : 1022
train acc:  0.7421875
train loss:  0.48093676567077637
train gradient:  0.09585643846731741
iteration : 1023
train acc:  0.7890625
train loss:  0.45872676372528076
train gradient:  0.13193231566786706
iteration : 1024
train acc:  0.7109375
train loss:  0.5297184586524963
train gradient:  0.14142866389972764
iteration : 1025
train acc:  0.75
train loss:  0.4777985215187073
train gradient:  0.1148636750573297
iteration : 1026
train acc:  0.703125
train loss:  0.521541178226471
train gradient:  0.1447199079447703
iteration : 1027
train acc:  0.765625
train loss:  0.46723952889442444
train gradient:  0.12360928224390795
iteration : 1028
train acc:  0.8203125
train loss:  0.387655109167099
train gradient:  0.09354908349141404
iteration : 1029
train acc:  0.7734375
train loss:  0.4460979104042053
train gradient:  0.1153328376608141
iteration : 1030
train acc:  0.7578125
train loss:  0.46436649560928345
train gradient:  0.11009660110163223
iteration : 1031
train acc:  0.75
train loss:  0.5102666020393372
train gradient:  0.12574337496505422
iteration : 1032
train acc:  0.7890625
train loss:  0.44746914505958557
train gradient:  0.08627909433514093
iteration : 1033
train acc:  0.734375
train loss:  0.5043883919715881
train gradient:  0.14586407267479756
iteration : 1034
train acc:  0.6796875
train loss:  0.5332173705101013
train gradient:  0.16871748192333041
iteration : 1035
train acc:  0.765625
train loss:  0.4716818928718567
train gradient:  0.10680680741408886
iteration : 1036
train acc:  0.734375
train loss:  0.486314594745636
train gradient:  0.1677668087693978
iteration : 1037
train acc:  0.75
train loss:  0.48476743698120117
train gradient:  0.1228445804998733
iteration : 1038
train acc:  0.734375
train loss:  0.4612935781478882
train gradient:  0.10783382790952988
iteration : 1039
train acc:  0.7578125
train loss:  0.44455111026763916
train gradient:  0.11890862105276334
iteration : 1040
train acc:  0.703125
train loss:  0.577171802520752
train gradient:  0.14899982596540054
iteration : 1041
train acc:  0.7734375
train loss:  0.45130693912506104
train gradient:  0.11020111148572168
iteration : 1042
train acc:  0.7578125
train loss:  0.4867071509361267
train gradient:  0.1145182099572049
iteration : 1043
train acc:  0.703125
train loss:  0.5295293927192688
train gradient:  0.15157412992365735
iteration : 1044
train acc:  0.6875
train loss:  0.5318598747253418
train gradient:  0.15402247989330714
iteration : 1045
train acc:  0.75
train loss:  0.48371779918670654
train gradient:  0.13047103109185837
iteration : 1046
train acc:  0.8125
train loss:  0.4646967947483063
train gradient:  0.10939355177442699
iteration : 1047
train acc:  0.7890625
train loss:  0.4442903995513916
train gradient:  0.1284264011324977
iteration : 1048
train acc:  0.796875
train loss:  0.44194209575653076
train gradient:  0.10670640492931663
iteration : 1049
train acc:  0.75
train loss:  0.47335755825042725
train gradient:  0.12542250004152894
iteration : 1050
train acc:  0.6953125
train loss:  0.5582913756370544
train gradient:  0.16664487245281226
iteration : 1051
train acc:  0.7421875
train loss:  0.49939200282096863
train gradient:  0.11250896970591012
iteration : 1052
train acc:  0.7109375
train loss:  0.5203875303268433
train gradient:  0.11399993094449608
iteration : 1053
train acc:  0.7265625
train loss:  0.5451942682266235
train gradient:  0.15520029655067086
iteration : 1054
train acc:  0.7265625
train loss:  0.5364709496498108
train gradient:  0.13636551774538835
iteration : 1055
train acc:  0.7890625
train loss:  0.42633605003356934
train gradient:  0.10666628469340199
iteration : 1056
train acc:  0.7734375
train loss:  0.45690375566482544
train gradient:  0.15169957488821573
iteration : 1057
train acc:  0.71875
train loss:  0.5024714469909668
train gradient:  0.13327983655754527
iteration : 1058
train acc:  0.734375
train loss:  0.48216187953948975
train gradient:  0.11788292314007331
iteration : 1059
train acc:  0.75
train loss:  0.5221439003944397
train gradient:  0.1506633243585192
iteration : 1060
train acc:  0.7578125
train loss:  0.4797174632549286
train gradient:  0.13260117641181543
iteration : 1061
train acc:  0.6484375
train loss:  0.5272824764251709
train gradient:  0.13790622580238254
iteration : 1062
train acc:  0.7578125
train loss:  0.4705654978752136
train gradient:  0.12416319276423139
iteration : 1063
train acc:  0.7421875
train loss:  0.4979857802391052
train gradient:  0.15496739769936868
iteration : 1064
train acc:  0.75
train loss:  0.48559314012527466
train gradient:  0.13539958797219087
iteration : 1065
train acc:  0.796875
train loss:  0.5223979353904724
train gradient:  0.12780809128972087
iteration : 1066
train acc:  0.71875
train loss:  0.5154008865356445
train gradient:  0.16559908216625868
iteration : 1067
train acc:  0.8046875
train loss:  0.4822619557380676
train gradient:  0.13412004714250086
iteration : 1068
train acc:  0.75
train loss:  0.4576660096645355
train gradient:  0.12292880902402259
iteration : 1069
train acc:  0.7265625
train loss:  0.47420284152030945
train gradient:  0.14718093498435908
iteration : 1070
train acc:  0.71875
train loss:  0.4849277138710022
train gradient:  0.12976252314650333
iteration : 1071
train acc:  0.78125
train loss:  0.43757814168930054
train gradient:  0.08910379912800752
iteration : 1072
train acc:  0.734375
train loss:  0.5549547076225281
train gradient:  0.20186776276099244
iteration : 1073
train acc:  0.7421875
train loss:  0.49922290444374084
train gradient:  0.1737467352279462
iteration : 1074
train acc:  0.6796875
train loss:  0.5496431589126587
train gradient:  0.1378228366470972
iteration : 1075
train acc:  0.765625
train loss:  0.4475718140602112
train gradient:  0.09156263916836377
iteration : 1076
train acc:  0.7578125
train loss:  0.4238481819629669
train gradient:  0.08365929059101877
iteration : 1077
train acc:  0.71875
train loss:  0.4692775011062622
train gradient:  0.10561350763697548
iteration : 1078
train acc:  0.765625
train loss:  0.4893381893634796
train gradient:  0.14789906284553705
iteration : 1079
train acc:  0.7890625
train loss:  0.47826021909713745
train gradient:  0.11747797145225096
iteration : 1080
train acc:  0.75
train loss:  0.4804326295852661
train gradient:  0.12100461018301736
iteration : 1081
train acc:  0.75
train loss:  0.49041682481765747
train gradient:  0.1473182522099994
iteration : 1082
train acc:  0.7578125
train loss:  0.48083072900772095
train gradient:  0.12341871704608656
iteration : 1083
train acc:  0.78125
train loss:  0.48280423879623413
train gradient:  0.11578590196667077
iteration : 1084
train acc:  0.671875
train loss:  0.554477334022522
train gradient:  0.1597511200200869
iteration : 1085
train acc:  0.7109375
train loss:  0.5113775730133057
train gradient:  0.13748846360695377
iteration : 1086
train acc:  0.7421875
train loss:  0.45271992683410645
train gradient:  0.09856360603779499
iteration : 1087
train acc:  0.7421875
train loss:  0.4673413038253784
train gradient:  0.12321299152977577
iteration : 1088
train acc:  0.71875
train loss:  0.5242646336555481
train gradient:  0.15165788880999692
iteration : 1089
train acc:  0.7265625
train loss:  0.5360152721405029
train gradient:  0.1360215317290912
iteration : 1090
train acc:  0.6640625
train loss:  0.5627804398536682
train gradient:  0.1441348010304332
iteration : 1091
train acc:  0.7265625
train loss:  0.4824497699737549
train gradient:  0.16531945477935456
iteration : 1092
train acc:  0.75
train loss:  0.497122585773468
train gradient:  0.14210533433309708
iteration : 1093
train acc:  0.765625
train loss:  0.4665710926055908
train gradient:  0.10818836393414343
iteration : 1094
train acc:  0.7578125
train loss:  0.5188711881637573
train gradient:  0.1949946809089953
iteration : 1095
train acc:  0.7890625
train loss:  0.44942060112953186
train gradient:  0.09250128736336467
iteration : 1096
train acc:  0.78125
train loss:  0.4175504148006439
train gradient:  0.09773325739158639
iteration : 1097
train acc:  0.7109375
train loss:  0.5114380121231079
train gradient:  0.18126057307697402
iteration : 1098
train acc:  0.75
train loss:  0.5146119594573975
train gradient:  0.14964577532908246
iteration : 1099
train acc:  0.71875
train loss:  0.5181921720504761
train gradient:  0.11397568851524226
iteration : 1100
train acc:  0.7421875
train loss:  0.44590890407562256
train gradient:  0.09975867120847125
iteration : 1101
train acc:  0.78125
train loss:  0.4393554925918579
train gradient:  0.10639759052484885
iteration : 1102
train acc:  0.6875
train loss:  0.5585947036743164
train gradient:  0.17585950368928058
iteration : 1103
train acc:  0.703125
train loss:  0.5578780174255371
train gradient:  0.14950661825052108
iteration : 1104
train acc:  0.7578125
train loss:  0.4116062521934509
train gradient:  0.10082399727876656
iteration : 1105
train acc:  0.78125
train loss:  0.5052149295806885
train gradient:  0.12715256937763025
iteration : 1106
train acc:  0.7578125
train loss:  0.45230159163475037
train gradient:  0.10996383620345142
iteration : 1107
train acc:  0.75
train loss:  0.43977850675582886
train gradient:  0.10494292011017552
iteration : 1108
train acc:  0.7421875
train loss:  0.5233772993087769
train gradient:  0.13730080356997337
iteration : 1109
train acc:  0.7734375
train loss:  0.41977277398109436
train gradient:  0.0907022891030959
iteration : 1110
train acc:  0.7890625
train loss:  0.48549824953079224
train gradient:  0.11279493296531506
iteration : 1111
train acc:  0.6640625
train loss:  0.538083016872406
train gradient:  0.15367445235415875
iteration : 1112
train acc:  0.7421875
train loss:  0.4764399230480194
train gradient:  0.10811874430863247
iteration : 1113
train acc:  0.671875
train loss:  0.5744423270225525
train gradient:  0.20172002465565253
iteration : 1114
train acc:  0.71875
train loss:  0.5432685613632202
train gradient:  0.14867849320163778
iteration : 1115
train acc:  0.765625
train loss:  0.4676375985145569
train gradient:  0.11982836688941813
iteration : 1116
train acc:  0.7890625
train loss:  0.46933621168136597
train gradient:  0.13252340111463146
iteration : 1117
train acc:  0.6796875
train loss:  0.5620526075363159
train gradient:  0.15798024214980677
iteration : 1118
train acc:  0.6328125
train loss:  0.6147202253341675
train gradient:  0.18174443370789278
iteration : 1119
train acc:  0.765625
train loss:  0.44589585065841675
train gradient:  0.11680005878541151
iteration : 1120
train acc:  0.7578125
train loss:  0.4635249674320221
train gradient:  0.09437288988312054
iteration : 1121
train acc:  0.7734375
train loss:  0.4440268874168396
train gradient:  0.10750945063339717
iteration : 1122
train acc:  0.7578125
train loss:  0.477368026971817
train gradient:  0.10042444449004487
iteration : 1123
train acc:  0.8046875
train loss:  0.4454503059387207
train gradient:  0.13267986696789347
iteration : 1124
train acc:  0.796875
train loss:  0.42454802989959717
train gradient:  0.09062172850637494
iteration : 1125
train acc:  0.7421875
train loss:  0.46290045976638794
train gradient:  0.10968914966256041
iteration : 1126
train acc:  0.671875
train loss:  0.521027684211731
train gradient:  0.14720810409430485
iteration : 1127
train acc:  0.7421875
train loss:  0.5018962025642395
train gradient:  0.12187399964773508
iteration : 1128
train acc:  0.7578125
train loss:  0.4622340500354767
train gradient:  0.13151438227072448
iteration : 1129
train acc:  0.75
train loss:  0.5691061615943909
train gradient:  0.17670502888059725
iteration : 1130
train acc:  0.796875
train loss:  0.40835532546043396
train gradient:  0.09946245031295402
iteration : 1131
train acc:  0.703125
train loss:  0.5450758337974548
train gradient:  0.13791240474582606
iteration : 1132
train acc:  0.7421875
train loss:  0.5014171004295349
train gradient:  0.12497647341674292
iteration : 1133
train acc:  0.7109375
train loss:  0.5162565112113953
train gradient:  0.11607794425843522
iteration : 1134
train acc:  0.7265625
train loss:  0.5228385925292969
train gradient:  0.16547517123538305
iteration : 1135
train acc:  0.7421875
train loss:  0.5122277736663818
train gradient:  0.13758447443138713
iteration : 1136
train acc:  0.75
train loss:  0.5526745319366455
train gradient:  0.14585301140292584
iteration : 1137
train acc:  0.75
train loss:  0.4384181499481201
train gradient:  0.09594802348657096
iteration : 1138
train acc:  0.78125
train loss:  0.49548786878585815
train gradient:  0.10572112391690565
iteration : 1139
train acc:  0.703125
train loss:  0.5404447317123413
train gradient:  0.14169111317869648
iteration : 1140
train acc:  0.71875
train loss:  0.46489500999450684
train gradient:  0.09736612614517189
iteration : 1141
train acc:  0.6953125
train loss:  0.5086337924003601
train gradient:  0.11481903601089084
iteration : 1142
train acc:  0.71875
train loss:  0.5197449326515198
train gradient:  0.14075116139715838
iteration : 1143
train acc:  0.75
train loss:  0.4869515299797058
train gradient:  0.12221984323960723
iteration : 1144
train acc:  0.7421875
train loss:  0.49825817346572876
train gradient:  0.13188064545108497
iteration : 1145
train acc:  0.6796875
train loss:  0.49905136227607727
train gradient:  0.12456978193323706
iteration : 1146
train acc:  0.765625
train loss:  0.44905805587768555
train gradient:  0.10393442354318155
iteration : 1147
train acc:  0.828125
train loss:  0.42321673035621643
train gradient:  0.07435166357995396
iteration : 1148
train acc:  0.8046875
train loss:  0.45422226190567017
train gradient:  0.11882565064876159
iteration : 1149
train acc:  0.7578125
train loss:  0.4481163024902344
train gradient:  0.10101032026057824
iteration : 1150
train acc:  0.7890625
train loss:  0.4671059250831604
train gradient:  0.11261393338995748
iteration : 1151
train acc:  0.8046875
train loss:  0.4710593521595001
train gradient:  0.10921515399606192
iteration : 1152
train acc:  0.8046875
train loss:  0.4105035662651062
train gradient:  0.09878798514041472
iteration : 1153
train acc:  0.703125
train loss:  0.5083311200141907
train gradient:  0.13199611769694863
iteration : 1154
train acc:  0.7421875
train loss:  0.514906644821167
train gradient:  0.2219233697457039
iteration : 1155
train acc:  0.6796875
train loss:  0.5969350337982178
train gradient:  0.14863023245621337
iteration : 1156
train acc:  0.7109375
train loss:  0.5138234496116638
train gradient:  0.15300413619811581
iteration : 1157
train acc:  0.7734375
train loss:  0.467682421207428
train gradient:  0.10209538218482936
iteration : 1158
train acc:  0.796875
train loss:  0.41878795623779297
train gradient:  0.08071805814174905
iteration : 1159
train acc:  0.65625
train loss:  0.6073517799377441
train gradient:  0.18480413644881633
iteration : 1160
train acc:  0.734375
train loss:  0.5032825469970703
train gradient:  0.11928468814672855
iteration : 1161
train acc:  0.8046875
train loss:  0.44638901948928833
train gradient:  0.09344166032393743
iteration : 1162
train acc:  0.71875
train loss:  0.5146787166595459
train gradient:  0.1496887073650263
iteration : 1163
train acc:  0.71875
train loss:  0.48597609996795654
train gradient:  0.1224824765241494
iteration : 1164
train acc:  0.71875
train loss:  0.5373048782348633
train gradient:  0.19070051649664857
iteration : 1165
train acc:  0.8359375
train loss:  0.42883574962615967
train gradient:  0.09523120042692866
iteration : 1166
train acc:  0.6953125
train loss:  0.534491777420044
train gradient:  0.12983662513418257
iteration : 1167
train acc:  0.671875
train loss:  0.544954776763916
train gradient:  0.14538005737251014
iteration : 1168
train acc:  0.75
train loss:  0.4671023190021515
train gradient:  0.1262938623127119
iteration : 1169
train acc:  0.71875
train loss:  0.48707151412963867
train gradient:  0.12706800268774843
iteration : 1170
train acc:  0.625
train loss:  0.649343729019165
train gradient:  0.28068833190689846
iteration : 1171
train acc:  0.71875
train loss:  0.5301160216331482
train gradient:  0.11841122852352186
iteration : 1172
train acc:  0.6953125
train loss:  0.5218529105186462
train gradient:  0.15920977059982774
iteration : 1173
train acc:  0.7890625
train loss:  0.42167001962661743
train gradient:  0.09392410949846224
iteration : 1174
train acc:  0.75
train loss:  0.44332587718963623
train gradient:  0.10004211852145363
iteration : 1175
train acc:  0.75
train loss:  0.47247403860092163
train gradient:  0.10887505629859484
iteration : 1176
train acc:  0.7421875
train loss:  0.4710954427719116
train gradient:  0.10525711969769376
iteration : 1177
train acc:  0.734375
train loss:  0.4997861981391907
train gradient:  0.13760346684512198
iteration : 1178
train acc:  0.7421875
train loss:  0.506041407585144
train gradient:  0.14994601651058426
iteration : 1179
train acc:  0.703125
train loss:  0.4868306517601013
train gradient:  0.11563899900937354
iteration : 1180
train acc:  0.78125
train loss:  0.4233524203300476
train gradient:  0.1276476196652031
iteration : 1181
train acc:  0.71875
train loss:  0.47627389430999756
train gradient:  0.09878440108234025
iteration : 1182
train acc:  0.75
train loss:  0.531274139881134
train gradient:  0.13074754586702847
iteration : 1183
train acc:  0.7890625
train loss:  0.42348071932792664
train gradient:  0.15084235722011347
iteration : 1184
train acc:  0.7578125
train loss:  0.5109221935272217
train gradient:  0.1365928376330525
iteration : 1185
train acc:  0.7421875
train loss:  0.5244407653808594
train gradient:  0.12962388501604125
iteration : 1186
train acc:  0.7578125
train loss:  0.5380105376243591
train gradient:  0.17859396432692937
iteration : 1187
train acc:  0.71875
train loss:  0.4965013563632965
train gradient:  0.11157130763267191
iteration : 1188
train acc:  0.7734375
train loss:  0.453085720539093
train gradient:  0.12383479854062794
iteration : 1189
train acc:  0.7890625
train loss:  0.4415377974510193
train gradient:  0.11474975120370047
iteration : 1190
train acc:  0.671875
train loss:  0.5539299845695496
train gradient:  0.15343899837299912
iteration : 1191
train acc:  0.75
train loss:  0.44965219497680664
train gradient:  0.093489641069093
iteration : 1192
train acc:  0.71875
train loss:  0.5148845314979553
train gradient:  0.14398908420231882
iteration : 1193
train acc:  0.8046875
train loss:  0.4513533413410187
train gradient:  0.1275374970489196
iteration : 1194
train acc:  0.734375
train loss:  0.474879652261734
train gradient:  0.12168266941980677
iteration : 1195
train acc:  0.796875
train loss:  0.4357771873474121
train gradient:  0.09083036875873587
iteration : 1196
train acc:  0.7421875
train loss:  0.5213271975517273
train gradient:  0.13549024583950758
iteration : 1197
train acc:  0.6796875
train loss:  0.5833549499511719
train gradient:  0.17465011301350936
iteration : 1198
train acc:  0.7578125
train loss:  0.5012627840042114
train gradient:  0.09967505738959508
iteration : 1199
train acc:  0.78125
train loss:  0.47597894072532654
train gradient:  0.11156408354636067
iteration : 1200
train acc:  0.7109375
train loss:  0.5231294631958008
train gradient:  0.14642257864812747
iteration : 1201
train acc:  0.75
train loss:  0.4828428030014038
train gradient:  0.10710132654654439
iteration : 1202
train acc:  0.734375
train loss:  0.4777683615684509
train gradient:  0.13443932186345947
iteration : 1203
train acc:  0.7109375
train loss:  0.5365356206893921
train gradient:  0.1258634343436253
iteration : 1204
train acc:  0.7734375
train loss:  0.4451119303703308
train gradient:  0.11008225002200135
iteration : 1205
train acc:  0.78125
train loss:  0.42334744334220886
train gradient:  0.10275547149549867
iteration : 1206
train acc:  0.8046875
train loss:  0.446583092212677
train gradient:  0.09221196042948104
iteration : 1207
train acc:  0.7578125
train loss:  0.5053387880325317
train gradient:  0.134452163935453
iteration : 1208
train acc:  0.734375
train loss:  0.50139319896698
train gradient:  0.14140945293122403
iteration : 1209
train acc:  0.7421875
train loss:  0.47017931938171387
train gradient:  0.12125084759447753
iteration : 1210
train acc:  0.8203125
train loss:  0.4003276228904724
train gradient:  0.09080150296646518
iteration : 1211
train acc:  0.7421875
train loss:  0.4658828377723694
train gradient:  0.11503798452571572
iteration : 1212
train acc:  0.765625
train loss:  0.48153015971183777
train gradient:  0.11940241788390156
iteration : 1213
train acc:  0.6953125
train loss:  0.5309777855873108
train gradient:  0.16592727386466302
iteration : 1214
train acc:  0.75
train loss:  0.5501789450645447
train gradient:  0.14770923245680373
iteration : 1215
train acc:  0.71875
train loss:  0.5437909364700317
train gradient:  0.1292184653735514
iteration : 1216
train acc:  0.75
train loss:  0.5451461672782898
train gradient:  0.1544297725127891
iteration : 1217
train acc:  0.796875
train loss:  0.46277618408203125
train gradient:  0.12226707729662757
iteration : 1218
train acc:  0.75
train loss:  0.45141640305519104
train gradient:  0.09928549130193523
iteration : 1219
train acc:  0.78125
train loss:  0.43098604679107666
train gradient:  0.11614903133687023
iteration : 1220
train acc:  0.796875
train loss:  0.4394209086894989
train gradient:  0.09687440973311479
iteration : 1221
train acc:  0.7421875
train loss:  0.5039798617362976
train gradient:  0.1436460224924833
iteration : 1222
train acc:  0.7578125
train loss:  0.5136591792106628
train gradient:  0.15498174604663884
iteration : 1223
train acc:  0.8046875
train loss:  0.42602235078811646
train gradient:  0.09067316744040785
iteration : 1224
train acc:  0.7265625
train loss:  0.4498171806335449
train gradient:  0.11093812150138041
iteration : 1225
train acc:  0.7265625
train loss:  0.49445605278015137
train gradient:  0.11988182186573297
iteration : 1226
train acc:  0.7890625
train loss:  0.43302398920059204
train gradient:  0.09848957558769925
iteration : 1227
train acc:  0.6875
train loss:  0.5475074052810669
train gradient:  0.14389128518487523
iteration : 1228
train acc:  0.78125
train loss:  0.43869438767433167
train gradient:  0.11415890146971357
iteration : 1229
train acc:  0.765625
train loss:  0.46810853481292725
train gradient:  0.11804538780127083
iteration : 1230
train acc:  0.78125
train loss:  0.5147558450698853
train gradient:  0.15358653464333205
iteration : 1231
train acc:  0.7109375
train loss:  0.5374643206596375
train gradient:  0.1474480270644769
iteration : 1232
train acc:  0.75
train loss:  0.49218305945396423
train gradient:  0.18091826687166165
iteration : 1233
train acc:  0.6953125
train loss:  0.5435154438018799
train gradient:  0.12043849504532145
iteration : 1234
train acc:  0.75
train loss:  0.43797892332077026
train gradient:  0.10263758359421289
iteration : 1235
train acc:  0.6875
train loss:  0.5818859338760376
train gradient:  0.14198420739631806
iteration : 1236
train acc:  0.7265625
train loss:  0.4796786308288574
train gradient:  0.12082657625430987
iteration : 1237
train acc:  0.703125
train loss:  0.519209623336792
train gradient:  0.14877126801664226
iteration : 1238
train acc:  0.7109375
train loss:  0.5029560923576355
train gradient:  0.12088602569767586
iteration : 1239
train acc:  0.71875
train loss:  0.5081208944320679
train gradient:  0.1426928196550563
iteration : 1240
train acc:  0.8203125
train loss:  0.4258185625076294
train gradient:  0.08996404321069114
iteration : 1241
train acc:  0.75
train loss:  0.4909341633319855
train gradient:  0.13383357038234162
iteration : 1242
train acc:  0.765625
train loss:  0.45477598905563354
train gradient:  0.12460328691924816
iteration : 1243
train acc:  0.78125
train loss:  0.48621779680252075
train gradient:  0.12152676625272066
iteration : 1244
train acc:  0.7734375
train loss:  0.45213526487350464
train gradient:  0.1266653189199825
iteration : 1245
train acc:  0.765625
train loss:  0.408063143491745
train gradient:  0.12945072045584954
iteration : 1246
train acc:  0.7734375
train loss:  0.47466427087783813
train gradient:  0.15847858796331635
iteration : 1247
train acc:  0.6953125
train loss:  0.5433951616287231
train gradient:  0.15293382448322235
iteration : 1248
train acc:  0.75
train loss:  0.4820120334625244
train gradient:  0.15516209341433995
iteration : 1249
train acc:  0.75
train loss:  0.521456241607666
train gradient:  0.1585218849510513
iteration : 1250
train acc:  0.796875
train loss:  0.4570057988166809
train gradient:  0.10189415152859146
iteration : 1251
train acc:  0.7890625
train loss:  0.45307284593582153
train gradient:  0.11154932170685723
iteration : 1252
train acc:  0.7265625
train loss:  0.5217933654785156
train gradient:  0.1210877237675141
iteration : 1253
train acc:  0.8125
train loss:  0.4530360698699951
train gradient:  0.1274716746418345
iteration : 1254
train acc:  0.78125
train loss:  0.47748953104019165
train gradient:  0.13268656554277947
iteration : 1255
train acc:  0.8046875
train loss:  0.4206288158893585
train gradient:  0.11151867656869571
iteration : 1256
train acc:  0.828125
train loss:  0.46301332116127014
train gradient:  0.13880292822032889
iteration : 1257
train acc:  0.8359375
train loss:  0.3787943124771118
train gradient:  0.08246361348731493
iteration : 1258
train acc:  0.765625
train loss:  0.48020005226135254
train gradient:  0.11538983205792627
iteration : 1259
train acc:  0.6953125
train loss:  0.549670398235321
train gradient:  0.1451503994852359
iteration : 1260
train acc:  0.8203125
train loss:  0.4079653322696686
train gradient:  0.09883076087798134
iteration : 1261
train acc:  0.7890625
train loss:  0.426383376121521
train gradient:  0.09789370701826602
iteration : 1262
train acc:  0.75
train loss:  0.4927731156349182
train gradient:  0.11786750821739246
iteration : 1263
train acc:  0.7265625
train loss:  0.4839227795600891
train gradient:  0.10630111161783717
iteration : 1264
train acc:  0.7421875
train loss:  0.4677252173423767
train gradient:  0.12029832032623124
iteration : 1265
train acc:  0.7265625
train loss:  0.5051800608634949
train gradient:  0.13281770301254714
iteration : 1266
train acc:  0.7734375
train loss:  0.42338621616363525
train gradient:  0.11370202696781466
iteration : 1267
train acc:  0.703125
train loss:  0.47458019852638245
train gradient:  0.10720809327077956
iteration : 1268
train acc:  0.828125
train loss:  0.42886391282081604
train gradient:  0.13340025244559134
iteration : 1269
train acc:  0.8046875
train loss:  0.43108275532722473
train gradient:  0.12109016914625112
iteration : 1270
train acc:  0.6484375
train loss:  0.5312813520431519
train gradient:  0.13366928432377112
iteration : 1271
train acc:  0.734375
train loss:  0.49532055854797363
train gradient:  0.12129081625018051
iteration : 1272
train acc:  0.75
train loss:  0.46539735794067383
train gradient:  0.14630649348226818
iteration : 1273
train acc:  0.75
train loss:  0.4585512578487396
train gradient:  0.10933965040543288
iteration : 1274
train acc:  0.6640625
train loss:  0.5231232047080994
train gradient:  0.12425569212129098
iteration : 1275
train acc:  0.6953125
train loss:  0.5105016231536865
train gradient:  0.11904157853190021
iteration : 1276
train acc:  0.8046875
train loss:  0.42019349336624146
train gradient:  0.09071969005367356
iteration : 1277
train acc:  0.75
train loss:  0.5006124973297119
train gradient:  0.14874662054182303
iteration : 1278
train acc:  0.7578125
train loss:  0.4850959777832031
train gradient:  0.1412921286543239
iteration : 1279
train acc:  0.7421875
train loss:  0.499040424823761
train gradient:  0.11032434163985662
iteration : 1280
train acc:  0.71875
train loss:  0.5030776858329773
train gradient:  0.15515326913365157
iteration : 1281
train acc:  0.71875
train loss:  0.4562512934207916
train gradient:  0.11835885709490307
iteration : 1282
train acc:  0.796875
train loss:  0.4531480669975281
train gradient:  0.10378923852154891
iteration : 1283
train acc:  0.7734375
train loss:  0.4263002872467041
train gradient:  0.09026256292257495
iteration : 1284
train acc:  0.7734375
train loss:  0.5109566450119019
train gradient:  0.1318011436024974
iteration : 1285
train acc:  0.7109375
train loss:  0.5237366557121277
train gradient:  0.17129864328505062
iteration : 1286
train acc:  0.6484375
train loss:  0.5352650880813599
train gradient:  0.1340369670518021
iteration : 1287
train acc:  0.7265625
train loss:  0.510709285736084
train gradient:  0.1439012497949288
iteration : 1288
train acc:  0.6953125
train loss:  0.4996233284473419
train gradient:  0.1517506056796109
iteration : 1289
train acc:  0.7421875
train loss:  0.5055924654006958
train gradient:  0.11683689708954377
iteration : 1290
train acc:  0.7421875
train loss:  0.4721953868865967
train gradient:  0.11787634086748348
iteration : 1291
train acc:  0.75
train loss:  0.4642641544342041
train gradient:  0.10254034252898561
iteration : 1292
train acc:  0.7109375
train loss:  0.4634929597377777
train gradient:  0.10407839780753864
iteration : 1293
train acc:  0.8046875
train loss:  0.4384017586708069
train gradient:  0.11896766036509099
iteration : 1294
train acc:  0.7265625
train loss:  0.5125547647476196
train gradient:  0.14267897499426802
iteration : 1295
train acc:  0.8203125
train loss:  0.41406548023223877
train gradient:  0.10156276984451613
iteration : 1296
train acc:  0.8203125
train loss:  0.4071778953075409
train gradient:  0.08244788685523571
iteration : 1297
train acc:  0.765625
train loss:  0.4754895865917206
train gradient:  0.10743268228850768
iteration : 1298
train acc:  0.7890625
train loss:  0.4484526515007019
train gradient:  0.098029491001875
iteration : 1299
train acc:  0.7890625
train loss:  0.45564818382263184
train gradient:  0.12058501581388756
iteration : 1300
train acc:  0.7421875
train loss:  0.47314441204071045
train gradient:  0.104727652951419
iteration : 1301
train acc:  0.84375
train loss:  0.3954961895942688
train gradient:  0.0734837555769901
iteration : 1302
train acc:  0.75
train loss:  0.48552870750427246
train gradient:  0.13206730140110762
iteration : 1303
train acc:  0.6953125
train loss:  0.5020651817321777
train gradient:  0.162655881532858
iteration : 1304
train acc:  0.703125
train loss:  0.5460938215255737
train gradient:  0.1408511287699472
iteration : 1305
train acc:  0.6953125
train loss:  0.4893242418766022
train gradient:  0.11304947274424104
iteration : 1306
train acc:  0.7734375
train loss:  0.42044690251350403
train gradient:  0.12562780234549117
iteration : 1307
train acc:  0.7265625
train loss:  0.47369301319122314
train gradient:  0.09934510334777896
iteration : 1308
train acc:  0.7265625
train loss:  0.4824577569961548
train gradient:  0.11901357498814327
iteration : 1309
train acc:  0.71875
train loss:  0.5329211950302124
train gradient:  0.16884285788026904
iteration : 1310
train acc:  0.75
train loss:  0.49799683690071106
train gradient:  0.12783508272582889
iteration : 1311
train acc:  0.703125
train loss:  0.5450612306594849
train gradient:  0.17289253908898145
iteration : 1312
train acc:  0.7734375
train loss:  0.4537780284881592
train gradient:  0.10869564541035737
iteration : 1313
train acc:  0.7578125
train loss:  0.47837525606155396
train gradient:  0.11067222004238823
iteration : 1314
train acc:  0.734375
train loss:  0.4504709839820862
train gradient:  0.10572058789088358
iteration : 1315
train acc:  0.75
train loss:  0.48356807231903076
train gradient:  0.10799226592144755
iteration : 1316
train acc:  0.734375
train loss:  0.49445658922195435
train gradient:  0.15834722947122212
iteration : 1317
train acc:  0.8046875
train loss:  0.42068368196487427
train gradient:  0.07545443919898141
iteration : 1318
train acc:  0.7421875
train loss:  0.47623294591903687
train gradient:  0.13349794346688726
iteration : 1319
train acc:  0.765625
train loss:  0.4733102321624756
train gradient:  0.10277197107917714
iteration : 1320
train acc:  0.765625
train loss:  0.47061020135879517
train gradient:  0.11070000052554478
iteration : 1321
train acc:  0.703125
train loss:  0.5658122301101685
train gradient:  0.13412752751004062
iteration : 1322
train acc:  0.78125
train loss:  0.4725291132926941
train gradient:  0.15088703254118874
iteration : 1323
train acc:  0.7578125
train loss:  0.5043754577636719
train gradient:  0.1603466165040847
iteration : 1324
train acc:  0.734375
train loss:  0.5040773153305054
train gradient:  0.12211881191312375
iteration : 1325
train acc:  0.7265625
train loss:  0.45302000641822815
train gradient:  0.10200628552108042
iteration : 1326
train acc:  0.734375
train loss:  0.5232175588607788
train gradient:  0.14675394645319345
iteration : 1327
train acc:  0.75
train loss:  0.46733295917510986
train gradient:  0.12843277785283952
iteration : 1328
train acc:  0.6953125
train loss:  0.6078006029129028
train gradient:  0.14683801563876508
iteration : 1329
train acc:  0.78125
train loss:  0.4478141665458679
train gradient:  0.09872586799684197
iteration : 1330
train acc:  0.7421875
train loss:  0.469599187374115
train gradient:  0.1260744956136213
iteration : 1331
train acc:  0.734375
train loss:  0.4759053885936737
train gradient:  0.15174133092512154
iteration : 1332
train acc:  0.734375
train loss:  0.4703350365161896
train gradient:  0.13957340067487845
iteration : 1333
train acc:  0.703125
train loss:  0.5557196140289307
train gradient:  0.1738447531891658
iteration : 1334
train acc:  0.7734375
train loss:  0.4178619384765625
train gradient:  0.09460862287513427
iteration : 1335
train acc:  0.8046875
train loss:  0.47360312938690186
train gradient:  0.11893922471436048
iteration : 1336
train acc:  0.7265625
train loss:  0.4991191029548645
train gradient:  0.15586062670100745
iteration : 1337
train acc:  0.8046875
train loss:  0.41915905475616455
train gradient:  0.13089567864818594
iteration : 1338
train acc:  0.7109375
train loss:  0.5084513425827026
train gradient:  0.13858405847655675
iteration : 1339
train acc:  0.6875
train loss:  0.5304431319236755
train gradient:  0.115746148815891
iteration : 1340
train acc:  0.8046875
train loss:  0.4513285756111145
train gradient:  0.12325486346763559
iteration : 1341
train acc:  0.703125
train loss:  0.5055539608001709
train gradient:  0.13477144444560885
iteration : 1342
train acc:  0.7734375
train loss:  0.4648109972476959
train gradient:  0.1257799316142197
iteration : 1343
train acc:  0.765625
train loss:  0.45003587007522583
train gradient:  0.1082561583086931
iteration : 1344
train acc:  0.765625
train loss:  0.4380565285682678
train gradient:  0.09600654307444847
iteration : 1345
train acc:  0.78125
train loss:  0.45367032289505005
train gradient:  0.10274869571549082
iteration : 1346
train acc:  0.734375
train loss:  0.5030256509780884
train gradient:  0.12382331951674005
iteration : 1347
train acc:  0.765625
train loss:  0.536697506904602
train gradient:  0.12770748601268944
iteration : 1348
train acc:  0.6875
train loss:  0.5189513564109802
train gradient:  0.11496121111585811
iteration : 1349
train acc:  0.765625
train loss:  0.453854501247406
train gradient:  0.14741258267942667
iteration : 1350
train acc:  0.8046875
train loss:  0.41382938623428345
train gradient:  0.12106028117718101
iteration : 1351
train acc:  0.7734375
train loss:  0.4980583190917969
train gradient:  0.1373998934186202
iteration : 1352
train acc:  0.734375
train loss:  0.5212389826774597
train gradient:  0.14010691164728714
iteration : 1353
train acc:  0.7265625
train loss:  0.4710748493671417
train gradient:  0.128731737176641
iteration : 1354
train acc:  0.7578125
train loss:  0.48267000913619995
train gradient:  0.12095745540561892
iteration : 1355
train acc:  0.828125
train loss:  0.4116026759147644
train gradient:  0.08531158730673614
iteration : 1356
train acc:  0.78125
train loss:  0.4604395031929016
train gradient:  0.11129149923843966
iteration : 1357
train acc:  0.734375
train loss:  0.4937640130519867
train gradient:  0.11565646644199651
iteration : 1358
train acc:  0.796875
train loss:  0.4470391869544983
train gradient:  0.16022509281503713
iteration : 1359
train acc:  0.734375
train loss:  0.4851086437702179
train gradient:  0.1175574614881117
iteration : 1360
train acc:  0.75
train loss:  0.5108073949813843
train gradient:  0.14550955381497183
iteration : 1361
train acc:  0.7109375
train loss:  0.5639918446540833
train gradient:  0.15003346108456622
iteration : 1362
train acc:  0.8046875
train loss:  0.4997953772544861
train gradient:  0.11711177180744235
iteration : 1363
train acc:  0.7421875
train loss:  0.4690423309803009
train gradient:  0.0990326808106536
iteration : 1364
train acc:  0.71875
train loss:  0.5592498779296875
train gradient:  0.16015633880248387
iteration : 1365
train acc:  0.7734375
train loss:  0.4830792248249054
train gradient:  0.13160290492106758
iteration : 1366
train acc:  0.8203125
train loss:  0.40056610107421875
train gradient:  0.08961707650570205
iteration : 1367
train acc:  0.7265625
train loss:  0.521199643611908
train gradient:  0.13544281317197704
iteration : 1368
train acc:  0.71875
train loss:  0.5660873055458069
train gradient:  0.16771604072224794
iteration : 1369
train acc:  0.7578125
train loss:  0.4794953465461731
train gradient:  0.12966828185303766
iteration : 1370
train acc:  0.796875
train loss:  0.43246695399284363
train gradient:  0.08947083596540382
iteration : 1371
train acc:  0.7578125
train loss:  0.46868541836738586
train gradient:  0.12060702866682169
iteration : 1372
train acc:  0.734375
train loss:  0.5229578018188477
train gradient:  0.13958797731717337
iteration : 1373
train acc:  0.7109375
train loss:  0.5129510760307312
train gradient:  0.11982383057802411
iteration : 1374
train acc:  0.7734375
train loss:  0.45515716075897217
train gradient:  0.10888762019015623
iteration : 1375
train acc:  0.734375
train loss:  0.44311073422431946
train gradient:  0.09685488911813374
iteration : 1376
train acc:  0.765625
train loss:  0.4698461890220642
train gradient:  0.15138841527282734
iteration : 1377
train acc:  0.7421875
train loss:  0.5277423858642578
train gradient:  0.12157443458461344
iteration : 1378
train acc:  0.78125
train loss:  0.4356638789176941
train gradient:  0.09500538145599968
iteration : 1379
train acc:  0.6640625
train loss:  0.5980008840560913
train gradient:  0.17857011609612294
iteration : 1380
train acc:  0.7109375
train loss:  0.5116274356842041
train gradient:  0.1186931718906474
iteration : 1381
train acc:  0.7734375
train loss:  0.4621444344520569
train gradient:  0.07410310755845723
iteration : 1382
train acc:  0.7734375
train loss:  0.4772987365722656
train gradient:  0.12599087989241936
iteration : 1383
train acc:  0.7421875
train loss:  0.5472424030303955
train gradient:  0.14728367183379432
iteration : 1384
train acc:  0.7734375
train loss:  0.4582520127296448
train gradient:  0.11860195026022836
iteration : 1385
train acc:  0.765625
train loss:  0.47507840394973755
train gradient:  0.13225423636527384
iteration : 1386
train acc:  0.71875
train loss:  0.546205461025238
train gradient:  0.12735636846671494
iteration : 1387
train acc:  0.734375
train loss:  0.45798608660697937
train gradient:  0.15200208230350298
iteration : 1388
train acc:  0.7890625
train loss:  0.47533857822418213
train gradient:  0.12631705309738
iteration : 1389
train acc:  0.75
train loss:  0.48503679037094116
train gradient:  0.1283825373843126
iteration : 1390
train acc:  0.75
train loss:  0.5236284732818604
train gradient:  0.14235045142012795
iteration : 1391
train acc:  0.7578125
train loss:  0.5266410708427429
train gradient:  0.14155771930489286
iteration : 1392
train acc:  0.8046875
train loss:  0.45299288630485535
train gradient:  0.10135298351300352
iteration : 1393
train acc:  0.7421875
train loss:  0.49188724160194397
train gradient:  0.12399924293462114
iteration : 1394
train acc:  0.7890625
train loss:  0.43764498829841614
train gradient:  0.11316077535039269
iteration : 1395
train acc:  0.7890625
train loss:  0.46901533007621765
train gradient:  0.14279634215703949
iteration : 1396
train acc:  0.6640625
train loss:  0.5339001417160034
train gradient:  0.15220956537084096
iteration : 1397
train acc:  0.7734375
train loss:  0.4678669273853302
train gradient:  0.10748660676428601
iteration : 1398
train acc:  0.734375
train loss:  0.4603513479232788
train gradient:  0.10540256405076194
iteration : 1399
train acc:  0.7734375
train loss:  0.45205458998680115
train gradient:  0.1264101232086765
iteration : 1400
train acc:  0.8203125
train loss:  0.44321441650390625
train gradient:  0.15648640799467034
iteration : 1401
train acc:  0.796875
train loss:  0.4307549297809601
train gradient:  0.13380823892182644
iteration : 1402
train acc:  0.75
train loss:  0.4946259558200836
train gradient:  0.1360909448094766
iteration : 1403
train acc:  0.6640625
train loss:  0.510046124458313
train gradient:  0.15097837547782827
iteration : 1404
train acc:  0.734375
train loss:  0.4930275082588196
train gradient:  0.0987455579623339
iteration : 1405
train acc:  0.7578125
train loss:  0.5010715126991272
train gradient:  0.1304276373145598
iteration : 1406
train acc:  0.7734375
train loss:  0.43957996368408203
train gradient:  0.12069900267932293
iteration : 1407
train acc:  0.8125
train loss:  0.44404351711273193
train gradient:  0.11767802534353154
iteration : 1408
train acc:  0.828125
train loss:  0.4137400984764099
train gradient:  0.10201457366878318
iteration : 1409
train acc:  0.7578125
train loss:  0.47491833567619324
train gradient:  0.08741626805724763
iteration : 1410
train acc:  0.7109375
train loss:  0.5334911942481995
train gradient:  0.12896865725327694
iteration : 1411
train acc:  0.6640625
train loss:  0.5551513433456421
train gradient:  0.17925745797957143
iteration : 1412
train acc:  0.7578125
train loss:  0.4920758306980133
train gradient:  0.11344709218703256
iteration : 1413
train acc:  0.7421875
train loss:  0.5291213989257812
train gradient:  0.13750340281385542
iteration : 1414
train acc:  0.7734375
train loss:  0.45674586296081543
train gradient:  0.10338159783038271
iteration : 1415
train acc:  0.734375
train loss:  0.48693448305130005
train gradient:  0.12808400426946392
iteration : 1416
train acc:  0.7578125
train loss:  0.4974154233932495
train gradient:  0.17927129971633912
iteration : 1417
train acc:  0.734375
train loss:  0.43976089358329773
train gradient:  0.11853765454201931
iteration : 1418
train acc:  0.734375
train loss:  0.47538161277770996
train gradient:  0.11406501800539823
iteration : 1419
train acc:  0.703125
train loss:  0.5403259992599487
train gradient:  0.20433202405810239
iteration : 1420
train acc:  0.7734375
train loss:  0.4792201519012451
train gradient:  0.12517372217422515
iteration : 1421
train acc:  0.75
train loss:  0.5058227777481079
train gradient:  0.14556154646867586
iteration : 1422
train acc:  0.7109375
train loss:  0.49931684136390686
train gradient:  0.12949639845120003
iteration : 1423
train acc:  0.78125
train loss:  0.411223828792572
train gradient:  0.0826341014000017
iteration : 1424
train acc:  0.78125
train loss:  0.426726371049881
train gradient:  0.09003673296886462
iteration : 1425
train acc:  0.7734375
train loss:  0.4818442761898041
train gradient:  0.1300045858134771
iteration : 1426
train acc:  0.78125
train loss:  0.4445393979549408
train gradient:  0.12768984982163933
iteration : 1427
train acc:  0.734375
train loss:  0.45706313848495483
train gradient:  0.1361501234722058
iteration : 1428
train acc:  0.71875
train loss:  0.5528947114944458
train gradient:  0.14894932925986357
iteration : 1429
train acc:  0.6953125
train loss:  0.5167267322540283
train gradient:  0.12088365770992979
iteration : 1430
train acc:  0.78125
train loss:  0.4281037449836731
train gradient:  0.10319733762502044
iteration : 1431
train acc:  0.7109375
train loss:  0.5307941436767578
train gradient:  0.14760678055564022
iteration : 1432
train acc:  0.734375
train loss:  0.5089248418807983
train gradient:  0.13305059791655877
iteration : 1433
train acc:  0.7578125
train loss:  0.4816964864730835
train gradient:  0.12087484874505894
iteration : 1434
train acc:  0.6953125
train loss:  0.5227158069610596
train gradient:  0.14952994985332196
iteration : 1435
train acc:  0.671875
train loss:  0.534064531326294
train gradient:  0.1816523969859289
iteration : 1436
train acc:  0.7109375
train loss:  0.5003714561462402
train gradient:  0.14348386859675272
iteration : 1437
train acc:  0.7578125
train loss:  0.5286392569541931
train gradient:  0.18288883274162687
iteration : 1438
train acc:  0.78125
train loss:  0.46157121658325195
train gradient:  0.10427311296853127
iteration : 1439
train acc:  0.71875
train loss:  0.5124015808105469
train gradient:  0.1170333162536969
iteration : 1440
train acc:  0.796875
train loss:  0.41590917110443115
train gradient:  0.08759796395042155
iteration : 1441
train acc:  0.7890625
train loss:  0.46206337213516235
train gradient:  0.1024279908805412
iteration : 1442
train acc:  0.7734375
train loss:  0.46795669198036194
train gradient:  0.11303303877414196
iteration : 1443
train acc:  0.7578125
train loss:  0.4536021947860718
train gradient:  0.08884852608548165
iteration : 1444
train acc:  0.8046875
train loss:  0.3853316009044647
train gradient:  0.10562915540826272
iteration : 1445
train acc:  0.7578125
train loss:  0.4761655330657959
train gradient:  0.11956928734162854
iteration : 1446
train acc:  0.7578125
train loss:  0.4769211411476135
train gradient:  0.13462308507873244
iteration : 1447
train acc:  0.6640625
train loss:  0.5827577114105225
train gradient:  0.16663524255209966
iteration : 1448
train acc:  0.7734375
train loss:  0.4742407202720642
train gradient:  0.09732997981402036
iteration : 1449
train acc:  0.7265625
train loss:  0.4788464605808258
train gradient:  0.1006748535154519
iteration : 1450
train acc:  0.7265625
train loss:  0.49962782859802246
train gradient:  0.10946871827370147
iteration : 1451
train acc:  0.765625
train loss:  0.5233186483383179
train gradient:  0.13936504342638817
iteration : 1452
train acc:  0.7890625
train loss:  0.472190260887146
train gradient:  0.11370393142971569
iteration : 1453
train acc:  0.7734375
train loss:  0.4960719347000122
train gradient:  0.14426674822386581
iteration : 1454
train acc:  0.828125
train loss:  0.40488362312316895
train gradient:  0.08320213620371003
iteration : 1455
train acc:  0.7421875
train loss:  0.48187321424484253
train gradient:  0.1291469897577064
iteration : 1456
train acc:  0.703125
train loss:  0.5099632740020752
train gradient:  0.15081656728906073
iteration : 1457
train acc:  0.7109375
train loss:  0.4985254406929016
train gradient:  0.173582149684192
iteration : 1458
train acc:  0.703125
train loss:  0.5411655902862549
train gradient:  0.16815009423468597
iteration : 1459
train acc:  0.7734375
train loss:  0.4644055962562561
train gradient:  0.12787898250592628
iteration : 1460
train acc:  0.7421875
train loss:  0.5107529163360596
train gradient:  0.12968309522795896
iteration : 1461
train acc:  0.75
train loss:  0.4512043595314026
train gradient:  0.14355351169114794
iteration : 1462
train acc:  0.7734375
train loss:  0.42965370416641235
train gradient:  0.08052355750153006
iteration : 1463
train acc:  0.78125
train loss:  0.46425676345825195
train gradient:  0.13978770072508845
iteration : 1464
train acc:  0.75
train loss:  0.413451611995697
train gradient:  0.08602725213046339
iteration : 1465
train acc:  0.7421875
train loss:  0.530147910118103
train gradient:  0.10778772498394192
iteration : 1466
train acc:  0.7421875
train loss:  0.5216836929321289
train gradient:  0.1416877996601989
iteration : 1467
train acc:  0.8515625
train loss:  0.4074273109436035
train gradient:  0.11005154761099477
iteration : 1468
train acc:  0.734375
train loss:  0.5271899700164795
train gradient:  0.12336509070675326
iteration : 1469
train acc:  0.7109375
train loss:  0.507788360118866
train gradient:  0.12612083946474875
iteration : 1470
train acc:  0.75
train loss:  0.4749002456665039
train gradient:  0.10463432583449188
iteration : 1471
train acc:  0.7578125
train loss:  0.4939895272254944
train gradient:  0.11799730994065621
iteration : 1472
train acc:  0.7421875
train loss:  0.45719772577285767
train gradient:  0.11971195388984822
iteration : 1473
train acc:  0.7421875
train loss:  0.4485931992530823
train gradient:  0.091591801157816
iteration : 1474
train acc:  0.703125
train loss:  0.5096398591995239
train gradient:  0.13291749725465457
iteration : 1475
train acc:  0.78125
train loss:  0.4750017523765564
train gradient:  0.13546557540903142
iteration : 1476
train acc:  0.765625
train loss:  0.45979440212249756
train gradient:  0.12150783450386829
iteration : 1477
train acc:  0.6953125
train loss:  0.5041291117668152
train gradient:  0.13201623563604753
iteration : 1478
train acc:  0.7734375
train loss:  0.4441129267215729
train gradient:  0.09264857505588676
iteration : 1479
train acc:  0.7265625
train loss:  0.5206713080406189
train gradient:  0.16759824970683737
iteration : 1480
train acc:  0.734375
train loss:  0.5183179974555969
train gradient:  0.1541867256372601
iteration : 1481
train acc:  0.7890625
train loss:  0.42643219232559204
train gradient:  0.07103934032969487
iteration : 1482
train acc:  0.7890625
train loss:  0.452608585357666
train gradient:  0.12488949098860114
iteration : 1483
train acc:  0.7578125
train loss:  0.44009315967559814
train gradient:  0.09140215883990828
iteration : 1484
train acc:  0.6875
train loss:  0.55813068151474
train gradient:  0.18215581188988242
iteration : 1485
train acc:  0.7734375
train loss:  0.4627377390861511
train gradient:  0.1549982892445504
iteration : 1486
train acc:  0.7734375
train loss:  0.47205016016960144
train gradient:  0.14173336754350302
iteration : 1487
train acc:  0.734375
train loss:  0.5421586036682129
train gradient:  0.1325761014603285
iteration : 1488
train acc:  0.8203125
train loss:  0.4732004404067993
train gradient:  0.13492510557877405
iteration : 1489
train acc:  0.78125
train loss:  0.4335958957672119
train gradient:  0.08238622068034543
iteration : 1490
train acc:  0.78125
train loss:  0.48313188552856445
train gradient:  0.11884797691076696
iteration : 1491
train acc:  0.75
train loss:  0.4899546504020691
train gradient:  0.12039100338392883
iteration : 1492
train acc:  0.796875
train loss:  0.4608817398548126
train gradient:  0.0962904123609666
iteration : 1493
train acc:  0.7734375
train loss:  0.4154230058193207
train gradient:  0.09401612150306468
iteration : 1494
train acc:  0.796875
train loss:  0.4279692471027374
train gradient:  0.1192103993574098
iteration : 1495
train acc:  0.7890625
train loss:  0.48606812953948975
train gradient:  0.11422414067791639
iteration : 1496
train acc:  0.734375
train loss:  0.4874728322029114
train gradient:  0.14591309979706285
iteration : 1497
train acc:  0.7890625
train loss:  0.45980456471443176
train gradient:  0.11990605394752715
iteration : 1498
train acc:  0.796875
train loss:  0.4881620407104492
train gradient:  0.1610014499960743
iteration : 1499
train acc:  0.78125
train loss:  0.472234308719635
train gradient:  0.11474099614182896
iteration : 1500
train acc:  0.7421875
train loss:  0.4805675745010376
train gradient:  0.12223841710067693
iteration : 1501
train acc:  0.75
train loss:  0.44317013025283813
train gradient:  0.10339816779754522
iteration : 1502
train acc:  0.7421875
train loss:  0.5076296925544739
train gradient:  0.1072979712538238
iteration : 1503
train acc:  0.71875
train loss:  0.5142595171928406
train gradient:  0.14143743168847983
iteration : 1504
train acc:  0.796875
train loss:  0.4279935956001282
train gradient:  0.10768202102840409
iteration : 1505
train acc:  0.765625
train loss:  0.4981229901313782
train gradient:  0.1240580116582742
iteration : 1506
train acc:  0.78125
train loss:  0.43564972281455994
train gradient:  0.100803863870626
iteration : 1507
train acc:  0.7265625
train loss:  0.5463047027587891
train gradient:  0.14320794449927945
iteration : 1508
train acc:  0.7890625
train loss:  0.4470279812812805
train gradient:  0.09554652546597099
iteration : 1509
train acc:  0.734375
train loss:  0.48467111587524414
train gradient:  0.13815073359438312
iteration : 1510
train acc:  0.7578125
train loss:  0.510342001914978
train gradient:  0.12187135301327469
iteration : 1511
train acc:  0.6796875
train loss:  0.5490460991859436
train gradient:  0.15075918758308737
iteration : 1512
train acc:  0.7890625
train loss:  0.4140443205833435
train gradient:  0.10157991005016748
iteration : 1513
train acc:  0.7734375
train loss:  0.4683079421520233
train gradient:  0.1382906248834478
iteration : 1514
train acc:  0.75
train loss:  0.45424947142601013
train gradient:  0.11473255416204058
iteration : 1515
train acc:  0.6875
train loss:  0.5361964702606201
train gradient:  0.19068817736060265
iteration : 1516
train acc:  0.75
train loss:  0.48320841789245605
train gradient:  0.0948833832405935
iteration : 1517
train acc:  0.796875
train loss:  0.4168710708618164
train gradient:  0.09148778043988483
iteration : 1518
train acc:  0.7578125
train loss:  0.5119619369506836
train gradient:  0.1186810722379436
iteration : 1519
train acc:  0.7265625
train loss:  0.5143547058105469
train gradient:  0.1783411913750823
iteration : 1520
train acc:  0.703125
train loss:  0.5027278065681458
train gradient:  0.11815912619266004
iteration : 1521
train acc:  0.7421875
train loss:  0.48885223269462585
train gradient:  0.12165280437407312
iteration : 1522
train acc:  0.7890625
train loss:  0.4126626253128052
train gradient:  0.09276948645004275
iteration : 1523
train acc:  0.765625
train loss:  0.5082412958145142
train gradient:  0.14785402861153019
iteration : 1524
train acc:  0.796875
train loss:  0.4421612024307251
train gradient:  0.0863203988779334
iteration : 1525
train acc:  0.7265625
train loss:  0.5052249431610107
train gradient:  0.13978274346366204
iteration : 1526
train acc:  0.7265625
train loss:  0.5412334203720093
train gradient:  0.15682281714711788
iteration : 1527
train acc:  0.796875
train loss:  0.47593098878860474
train gradient:  0.12429231665759537
iteration : 1528
train acc:  0.75
train loss:  0.49871957302093506
train gradient:  0.12776880319396178
iteration : 1529
train acc:  0.7421875
train loss:  0.4763234257698059
train gradient:  0.1118441337037801
iteration : 1530
train acc:  0.7421875
train loss:  0.4768194258213043
train gradient:  0.10560993922821496
iteration : 1531
train acc:  0.7421875
train loss:  0.5163280963897705
train gradient:  0.16469419266444535
iteration : 1532
train acc:  0.7421875
train loss:  0.5301809310913086
train gradient:  0.1471388141988192
iteration : 1533
train acc:  0.7421875
train loss:  0.49082323908805847
train gradient:  0.12666263032158925
iteration : 1534
train acc:  0.8203125
train loss:  0.3902440369129181
train gradient:  0.07404999899819001
iteration : 1535
train acc:  0.7734375
train loss:  0.5206587314605713
train gradient:  0.13967756938326348
iteration : 1536
train acc:  0.7890625
train loss:  0.4277857542037964
train gradient:  0.11079952972364865
iteration : 1537
train acc:  0.7265625
train loss:  0.5204901695251465
train gradient:  0.14055995978492386
iteration : 1538
train acc:  0.828125
train loss:  0.3940587639808655
train gradient:  0.07724144822965606
iteration : 1539
train acc:  0.75
train loss:  0.4668739438056946
train gradient:  0.1034868760775223
iteration : 1540
train acc:  0.796875
train loss:  0.4826432466506958
train gradient:  0.14057659415850296
iteration : 1541
train acc:  0.765625
train loss:  0.5130535364151001
train gradient:  0.13156617398719062
iteration : 1542
train acc:  0.796875
train loss:  0.4301864802837372
train gradient:  0.09336662796509275
iteration : 1543
train acc:  0.7109375
train loss:  0.5230364203453064
train gradient:  0.11709240594403376
iteration : 1544
train acc:  0.7421875
train loss:  0.48098844289779663
train gradient:  0.10212281134199903
iteration : 1545
train acc:  0.7734375
train loss:  0.4443812668323517
train gradient:  0.11158761369819327
iteration : 1546
train acc:  0.7890625
train loss:  0.45921415090560913
train gradient:  0.11967163045363308
iteration : 1547
train acc:  0.8359375
train loss:  0.3947608172893524
train gradient:  0.10010314655087385
iteration : 1548
train acc:  0.734375
train loss:  0.4477357268333435
train gradient:  0.10766100242026153
iteration : 1549
train acc:  0.7265625
train loss:  0.49072718620300293
train gradient:  0.1282646016297313
iteration : 1550
train acc:  0.7890625
train loss:  0.442410409450531
train gradient:  0.14103027517082228
iteration : 1551
train acc:  0.7265625
train loss:  0.5237407684326172
train gradient:  0.16178501246339044
iteration : 1552
train acc:  0.7734375
train loss:  0.41932618618011475
train gradient:  0.10531646935874653
iteration : 1553
train acc:  0.75
train loss:  0.5307525992393494
train gradient:  0.19095455838692682
iteration : 1554
train acc:  0.7421875
train loss:  0.538705050945282
train gradient:  0.13203509805301405
iteration : 1555
train acc:  0.7109375
train loss:  0.47215357422828674
train gradient:  0.14201000148998605
iteration : 1556
train acc:  0.703125
train loss:  0.583818793296814
train gradient:  0.1848627209522459
iteration : 1557
train acc:  0.7578125
train loss:  0.4790921211242676
train gradient:  0.11947442452742728
iteration : 1558
train acc:  0.7578125
train loss:  0.44956910610198975
train gradient:  0.10895608026611472
iteration : 1559
train acc:  0.6953125
train loss:  0.581452488899231
train gradient:  0.1536809469358016
iteration : 1560
train acc:  0.7890625
train loss:  0.42950552701950073
train gradient:  0.08669198166326593
iteration : 1561
train acc:  0.7734375
train loss:  0.49448931217193604
train gradient:  0.13234637037089778
iteration : 1562
train acc:  0.7421875
train loss:  0.46283385157585144
train gradient:  0.12951218473354428
iteration : 1563
train acc:  0.6640625
train loss:  0.5619422197341919
train gradient:  0.19503996322359968
iteration : 1564
train acc:  0.7265625
train loss:  0.4718177616596222
train gradient:  0.12332717918765435
iteration : 1565
train acc:  0.8046875
train loss:  0.42329007387161255
train gradient:  0.09432284098785643
iteration : 1566
train acc:  0.71875
train loss:  0.4961051344871521
train gradient:  0.1500226462108949
iteration : 1567
train acc:  0.796875
train loss:  0.43341726064682007
train gradient:  0.09281460802345429
iteration : 1568
train acc:  0.734375
train loss:  0.4748672842979431
train gradient:  0.11127530569656051
iteration : 1569
train acc:  0.765625
train loss:  0.46332401037216187
train gradient:  0.10536931638773309
iteration : 1570
train acc:  0.7578125
train loss:  0.449618935585022
train gradient:  0.1060930822656711
iteration : 1571
train acc:  0.75
train loss:  0.5359886884689331
train gradient:  0.1321226547759895
iteration : 1572
train acc:  0.7734375
train loss:  0.46588367223739624
train gradient:  0.11799729275179689
iteration : 1573
train acc:  0.78125
train loss:  0.45791617035865784
train gradient:  0.11074162390415442
iteration : 1574
train acc:  0.75
train loss:  0.4430091977119446
train gradient:  0.11027773743684549
iteration : 1575
train acc:  0.7734375
train loss:  0.43365389108657837
train gradient:  0.08213963424925147
iteration : 1576
train acc:  0.6875
train loss:  0.5206496715545654
train gradient:  0.15336843827682195
iteration : 1577
train acc:  0.765625
train loss:  0.5143945217132568
train gradient:  0.11806079387033573
iteration : 1578
train acc:  0.75
train loss:  0.48010265827178955
train gradient:  0.10994006309639541
iteration : 1579
train acc:  0.7421875
train loss:  0.47241300344467163
train gradient:  0.10289817588422843
iteration : 1580
train acc:  0.7578125
train loss:  0.4843601584434509
train gradient:  0.1408194470328314
iteration : 1581
train acc:  0.765625
train loss:  0.49153590202331543
train gradient:  0.11502420182981701
iteration : 1582
train acc:  0.7578125
train loss:  0.47536689043045044
train gradient:  0.14721160997068963
iteration : 1583
train acc:  0.765625
train loss:  0.4439290165901184
train gradient:  0.10072656014533626
iteration : 1584
train acc:  0.734375
train loss:  0.5637190937995911
train gradient:  0.14355501885992988
iteration : 1585
train acc:  0.796875
train loss:  0.47313907742500305
train gradient:  0.10233974195172066
iteration : 1586
train acc:  0.7734375
train loss:  0.4636143445968628
train gradient:  0.12327440186454652
iteration : 1587
train acc:  0.78125
train loss:  0.46491163969039917
train gradient:  0.11277442072999605
iteration : 1588
train acc:  0.75
train loss:  0.4414938986301422
train gradient:  0.09121916198878435
iteration : 1589
train acc:  0.75
train loss:  0.45538634061813354
train gradient:  0.09757414699126395
iteration : 1590
train acc:  0.7421875
train loss:  0.4442225396633148
train gradient:  0.12853852535443355
iteration : 1591
train acc:  0.625
train loss:  0.5469865798950195
train gradient:  0.17202770453443017
iteration : 1592
train acc:  0.734375
train loss:  0.48360663652420044
train gradient:  0.12606332866780895
iteration : 1593
train acc:  0.7578125
train loss:  0.4677625000476837
train gradient:  0.1071622738625578
iteration : 1594
train acc:  0.7421875
train loss:  0.5003219246864319
train gradient:  0.1482804838217089
iteration : 1595
train acc:  0.734375
train loss:  0.47209155559539795
train gradient:  0.1129248079024103
iteration : 1596
train acc:  0.7109375
train loss:  0.4752616584300995
train gradient:  0.14162583657906214
iteration : 1597
train acc:  0.8046875
train loss:  0.4492962956428528
train gradient:  0.13383833123423775
iteration : 1598
train acc:  0.78125
train loss:  0.4851459860801697
train gradient:  0.11784297123003823
iteration : 1599
train acc:  0.796875
train loss:  0.4436318874359131
train gradient:  0.08390601008525314
iteration : 1600
train acc:  0.7578125
train loss:  0.4467483162879944
train gradient:  0.11342285206053748
iteration : 1601
train acc:  0.75
train loss:  0.4784267246723175
train gradient:  0.12858161357573156
iteration : 1602
train acc:  0.7734375
train loss:  0.45390763878822327
train gradient:  0.08832825116803694
iteration : 1603
train acc:  0.8203125
train loss:  0.42957550287246704
train gradient:  0.10685751341154714
iteration : 1604
train acc:  0.734375
train loss:  0.5406492948532104
train gradient:  0.14511555554307276
iteration : 1605
train acc:  0.6953125
train loss:  0.5030466318130493
train gradient:  0.10715631109108831
iteration : 1606
train acc:  0.796875
train loss:  0.461961030960083
train gradient:  0.11704266585235039
iteration : 1607
train acc:  0.765625
train loss:  0.46681326627731323
train gradient:  0.10572054648674312
iteration : 1608
train acc:  0.8046875
train loss:  0.44284147024154663
train gradient:  0.10543939239056052
iteration : 1609
train acc:  0.75
train loss:  0.487320214509964
train gradient:  0.1320678546471813
iteration : 1610
train acc:  0.7734375
train loss:  0.4573119878768921
train gradient:  0.12273394017935577
iteration : 1611
train acc:  0.6796875
train loss:  0.508736789226532
train gradient:  0.13790537369302258
iteration : 1612
train acc:  0.7890625
train loss:  0.46090835332870483
train gradient:  0.11670611899547707
iteration : 1613
train acc:  0.7734375
train loss:  0.504192054271698
train gradient:  0.13985860706670866
iteration : 1614
train acc:  0.7578125
train loss:  0.4560128450393677
train gradient:  0.09999521443426655
iteration : 1615
train acc:  0.7578125
train loss:  0.48885226249694824
train gradient:  0.13676163307362543
iteration : 1616
train acc:  0.703125
train loss:  0.5712578296661377
train gradient:  0.16003716660877212
iteration : 1617
train acc:  0.7734375
train loss:  0.43622398376464844
train gradient:  0.1070536173060045
iteration : 1618
train acc:  0.7421875
train loss:  0.4454522728919983
train gradient:  0.09278799049111146
iteration : 1619
train acc:  0.7265625
train loss:  0.5839123725891113
train gradient:  0.17445484752477022
iteration : 1620
train acc:  0.7265625
train loss:  0.4617469310760498
train gradient:  0.12611150133422014
iteration : 1621
train acc:  0.75
train loss:  0.47488221526145935
train gradient:  0.13270806419993383
iteration : 1622
train acc:  0.71875
train loss:  0.5099057555198669
train gradient:  0.13011138412439255
iteration : 1623
train acc:  0.7890625
train loss:  0.4590659737586975
train gradient:  0.13915095133128003
iteration : 1624
train acc:  0.75
train loss:  0.4811221659183502
train gradient:  0.16615979650851886
iteration : 1625
train acc:  0.828125
train loss:  0.4533563256263733
train gradient:  0.10270776202886527
iteration : 1626
train acc:  0.8125
train loss:  0.44374513626098633
train gradient:  0.0966184008358856
iteration : 1627
train acc:  0.765625
train loss:  0.5082026720046997
train gradient:  0.17830803583734972
iteration : 1628
train acc:  0.7421875
train loss:  0.4615079164505005
train gradient:  0.12566359506416444
iteration : 1629
train acc:  0.7265625
train loss:  0.5298953056335449
train gradient:  0.15828029245552955
iteration : 1630
train acc:  0.796875
train loss:  0.45525333285331726
train gradient:  0.12930034146976932
iteration : 1631
train acc:  0.7734375
train loss:  0.4755278527736664
train gradient:  0.10148886392475615
iteration : 1632
train acc:  0.734375
train loss:  0.5226725339889526
train gradient:  0.1429937659038622
iteration : 1633
train acc:  0.7421875
train loss:  0.49457550048828125
train gradient:  0.16054885039362787
iteration : 1634
train acc:  0.7421875
train loss:  0.4813143014907837
train gradient:  0.12302949435892163
iteration : 1635
train acc:  0.7421875
train loss:  0.5340355634689331
train gradient:  0.1612630170121505
iteration : 1636
train acc:  0.7421875
train loss:  0.5156208276748657
train gradient:  0.10269629983272154
iteration : 1637
train acc:  0.796875
train loss:  0.4530026316642761
train gradient:  0.12320556801335705
iteration : 1638
train acc:  0.8125
train loss:  0.370964378118515
train gradient:  0.07240866972385042
iteration : 1639
train acc:  0.7734375
train loss:  0.464926540851593
train gradient:  0.13811198702237548
iteration : 1640
train acc:  0.65625
train loss:  0.5542516708374023
train gradient:  0.16136956453157955
iteration : 1641
train acc:  0.734375
train loss:  0.512194037437439
train gradient:  0.14184609343499663
iteration : 1642
train acc:  0.78125
train loss:  0.5051307678222656
train gradient:  0.12033582756617842
iteration : 1643
train acc:  0.671875
train loss:  0.5427197217941284
train gradient:  0.15057414848907158
iteration : 1644
train acc:  0.8046875
train loss:  0.4207471013069153
train gradient:  0.09056609771922693
iteration : 1645
train acc:  0.7109375
train loss:  0.4763198494911194
train gradient:  0.1206483306187085
iteration : 1646
train acc:  0.8203125
train loss:  0.41165128350257874
train gradient:  0.10966153247742702
iteration : 1647
train acc:  0.8046875
train loss:  0.47693008184432983
train gradient:  0.1731984150458487
iteration : 1648
train acc:  0.7734375
train loss:  0.5141723155975342
train gradient:  0.1206399205946162
iteration : 1649
train acc:  0.7890625
train loss:  0.43761515617370605
train gradient:  0.0922569829052605
iteration : 1650
train acc:  0.8359375
train loss:  0.3925408720970154
train gradient:  0.09457589242964035
iteration : 1651
train acc:  0.8046875
train loss:  0.41772860288619995
train gradient:  0.11643623122258112
iteration : 1652
train acc:  0.78125
train loss:  0.46645790338516235
train gradient:  0.10181712286652236
iteration : 1653
train acc:  0.75
train loss:  0.48753899335861206
train gradient:  0.11223365490085836
iteration : 1654
train acc:  0.796875
train loss:  0.42596226930618286
train gradient:  0.10695705348235234
iteration : 1655
train acc:  0.75
train loss:  0.4837493300437927
train gradient:  0.11461855658626643
iteration : 1656
train acc:  0.765625
train loss:  0.4530513286590576
train gradient:  0.1147259257780711
iteration : 1657
train acc:  0.7578125
train loss:  0.46259915828704834
train gradient:  0.13222289431682957
iteration : 1658
train acc:  0.71875
train loss:  0.5032759308815002
train gradient:  0.12563902662406706
iteration : 1659
train acc:  0.75
train loss:  0.46763163805007935
train gradient:  0.10607035114020012
iteration : 1660
train acc:  0.71875
train loss:  0.5353814363479614
train gradient:  0.1877781937333693
iteration : 1661
train acc:  0.734375
train loss:  0.4963148236274719
train gradient:  0.17664727886025697
iteration : 1662
train acc:  0.765625
train loss:  0.49975448846817017
train gradient:  0.13163318866460333
iteration : 1663
train acc:  0.7578125
train loss:  0.43841463327407837
train gradient:  0.10152814949349166
iteration : 1664
train acc:  0.6796875
train loss:  0.5772414803504944
train gradient:  0.14535478952846193
iteration : 1665
train acc:  0.765625
train loss:  0.5119712352752686
train gradient:  0.17004821219100022
iteration : 1666
train acc:  0.78125
train loss:  0.43788397312164307
train gradient:  0.09889394273149237
iteration : 1667
train acc:  0.6953125
train loss:  0.5559417009353638
train gradient:  0.20486060475667525
iteration : 1668
train acc:  0.71875
train loss:  0.4760315716266632
train gradient:  0.1616244525360437
iteration : 1669
train acc:  0.7890625
train loss:  0.435441792011261
train gradient:  0.1317649108126075
iteration : 1670
train acc:  0.78125
train loss:  0.4599483609199524
train gradient:  0.11071889677156424
iteration : 1671
train acc:  0.7265625
train loss:  0.5007860660552979
train gradient:  0.16421503976538981
iteration : 1672
train acc:  0.6953125
train loss:  0.5449321866035461
train gradient:  0.17308839617985342
iteration : 1673
train acc:  0.7890625
train loss:  0.4373427629470825
train gradient:  0.09171793766858644
iteration : 1674
train acc:  0.7890625
train loss:  0.4725567102432251
train gradient:  0.16009255708533637
iteration : 1675
train acc:  0.7109375
train loss:  0.5030931830406189
train gradient:  0.11364082408647741
iteration : 1676
train acc:  0.734375
train loss:  0.47718489170074463
train gradient:  0.11438075319235329
iteration : 1677
train acc:  0.6640625
train loss:  0.6123527884483337
train gradient:  0.20073441685923363
iteration : 1678
train acc:  0.78125
train loss:  0.44898879528045654
train gradient:  0.09786171931381346
iteration : 1679
train acc:  0.71875
train loss:  0.4893936812877655
train gradient:  0.1461788794598729
iteration : 1680
train acc:  0.7109375
train loss:  0.49043673276901245
train gradient:  0.126004196488054
iteration : 1681
train acc:  0.734375
train loss:  0.48566487431526184
train gradient:  0.12201934539573836
iteration : 1682
train acc:  0.7734375
train loss:  0.445753812789917
train gradient:  0.12229844932111561
iteration : 1683
train acc:  0.7265625
train loss:  0.5071032047271729
train gradient:  0.15170746444280028
iteration : 1684
train acc:  0.734375
train loss:  0.5192398428916931
train gradient:  0.11964849376919891
iteration : 1685
train acc:  0.7265625
train loss:  0.45929259061813354
train gradient:  0.12249537805583245
iteration : 1686
train acc:  0.7109375
train loss:  0.5042481422424316
train gradient:  0.1262674975113824
iteration : 1687
train acc:  0.7578125
train loss:  0.4593627452850342
train gradient:  0.10618306022494653
iteration : 1688
train acc:  0.8125
train loss:  0.430144727230072
train gradient:  0.09271260773559573
iteration : 1689
train acc:  0.7578125
train loss:  0.5021381974220276
train gradient:  0.14815383882749095
iteration : 1690
train acc:  0.75
train loss:  0.47849056124687195
train gradient:  0.10387839246593544
iteration : 1691
train acc:  0.7734375
train loss:  0.4435329735279083
train gradient:  0.11173217669360494
iteration : 1692
train acc:  0.78125
train loss:  0.4550931453704834
train gradient:  0.0952282987013018
iteration : 1693
train acc:  0.75
train loss:  0.4749069809913635
train gradient:  0.11968336753883281
iteration : 1694
train acc:  0.703125
train loss:  0.5518056154251099
train gradient:  0.19009824048846724
iteration : 1695
train acc:  0.75
train loss:  0.5429985523223877
train gradient:  0.1465893852258007
iteration : 1696
train acc:  0.640625
train loss:  0.5744692087173462
train gradient:  0.13214757424332912
iteration : 1697
train acc:  0.6953125
train loss:  0.4673601984977722
train gradient:  0.09947743916047912
iteration : 1698
train acc:  0.703125
train loss:  0.4809926748275757
train gradient:  0.15955244753823553
iteration : 1699
train acc:  0.765625
train loss:  0.46863827109336853
train gradient:  0.11242055228793682
iteration : 1700
train acc:  0.7421875
train loss:  0.40901651978492737
train gradient:  0.07086019654511068
iteration : 1701
train acc:  0.703125
train loss:  0.5176349878311157
train gradient:  0.13264716751206146
iteration : 1702
train acc:  0.7109375
train loss:  0.5080256462097168
train gradient:  0.1441945646548145
iteration : 1703
train acc:  0.6953125
train loss:  0.5573073625564575
train gradient:  0.13987899501366072
iteration : 1704
train acc:  0.734375
train loss:  0.4897778332233429
train gradient:  0.14104880857559987
iteration : 1705
train acc:  0.7421875
train loss:  0.5345645546913147
train gradient:  0.1697884529971692
iteration : 1706
train acc:  0.828125
train loss:  0.44675499200820923
train gradient:  0.11853947254871293
iteration : 1707
train acc:  0.8046875
train loss:  0.46122536063194275
train gradient:  0.10960852189248153
iteration : 1708
train acc:  0.6953125
train loss:  0.5104935765266418
train gradient:  0.12637528611013304
iteration : 1709
train acc:  0.7421875
train loss:  0.4811623990535736
train gradient:  0.11434626804895698
iteration : 1710
train acc:  0.75
train loss:  0.48630350828170776
train gradient:  0.13130979259338613
iteration : 1711
train acc:  0.7578125
train loss:  0.4914723336696625
train gradient:  0.1479203996072993
iteration : 1712
train acc:  0.7734375
train loss:  0.5230543613433838
train gradient:  0.16387484884404424
iteration : 1713
train acc:  0.78125
train loss:  0.4903704524040222
train gradient:  0.1355018803725984
iteration : 1714
train acc:  0.734375
train loss:  0.48957931995391846
train gradient:  0.12123288626523945
iteration : 1715
train acc:  0.6875
train loss:  0.5069797039031982
train gradient:  0.13251986243677316
iteration : 1716
train acc:  0.734375
train loss:  0.5161110162734985
train gradient:  0.2086009723945472
iteration : 1717
train acc:  0.8046875
train loss:  0.4238477945327759
train gradient:  0.10380385551068134
iteration : 1718
train acc:  0.7734375
train loss:  0.5025609731674194
train gradient:  0.13147280810287537
iteration : 1719
train acc:  0.828125
train loss:  0.4029693007469177
train gradient:  0.11076667190009555
iteration : 1720
train acc:  0.7734375
train loss:  0.4475666284561157
train gradient:  0.10437773773400792
iteration : 1721
train acc:  0.796875
train loss:  0.4896771013736725
train gradient:  0.10970983550637585
iteration : 1722
train acc:  0.7578125
train loss:  0.4473358690738678
train gradient:  0.08912938713345293
iteration : 1723
train acc:  0.765625
train loss:  0.5093725919723511
train gradient:  0.15620541617747305
iteration : 1724
train acc:  0.7578125
train loss:  0.4419865012168884
train gradient:  0.10221457418817889
iteration : 1725
train acc:  0.7421875
train loss:  0.4982013702392578
train gradient:  0.14680495752398853
iteration : 1726
train acc:  0.78125
train loss:  0.4542832374572754
train gradient:  0.12451679468553987
iteration : 1727
train acc:  0.6640625
train loss:  0.5723915100097656
train gradient:  0.19174292527027728
iteration : 1728
train acc:  0.734375
train loss:  0.47980034351348877
train gradient:  0.13741092060986848
iteration : 1729
train acc:  0.7265625
train loss:  0.5144627094268799
train gradient:  0.13078450632570987
iteration : 1730
train acc:  0.7421875
train loss:  0.5114526748657227
train gradient:  0.13015781161993623
iteration : 1731
train acc:  0.7578125
train loss:  0.5276928544044495
train gradient:  0.1781350192278846
iteration : 1732
train acc:  0.78125
train loss:  0.4248337149620056
train gradient:  0.1165553752723008
iteration : 1733
train acc:  0.8359375
train loss:  0.5188142657279968
train gradient:  0.15600190609131082
iteration : 1734
train acc:  0.7734375
train loss:  0.4478261470794678
train gradient:  0.11427326011952385
iteration : 1735
train acc:  0.7578125
train loss:  0.49136555194854736
train gradient:  0.14460305668973425
iteration : 1736
train acc:  0.78125
train loss:  0.44964373111724854
train gradient:  0.11205509821967218
iteration : 1737
train acc:  0.7421875
train loss:  0.5846917629241943
train gradient:  0.18893011197935894
iteration : 1738
train acc:  0.734375
train loss:  0.48415660858154297
train gradient:  0.12035605073994368
iteration : 1739
train acc:  0.734375
train loss:  0.5101893544197083
train gradient:  0.09622869633101913
iteration : 1740
train acc:  0.734375
train loss:  0.487199068069458
train gradient:  0.12247353058589218
iteration : 1741
train acc:  0.765625
train loss:  0.4189746379852295
train gradient:  0.1015854117513681
iteration : 1742
train acc:  0.75
train loss:  0.4435635209083557
train gradient:  0.11775090651363694
iteration : 1743
train acc:  0.7265625
train loss:  0.48970407247543335
train gradient:  0.12416637199757662
iteration : 1744
train acc:  0.703125
train loss:  0.5367498397827148
train gradient:  0.16109750735568001
iteration : 1745
train acc:  0.78125
train loss:  0.4512678384780884
train gradient:  0.11896906573117716
iteration : 1746
train acc:  0.796875
train loss:  0.420615017414093
train gradient:  0.09509206260126048
iteration : 1747
train acc:  0.75
train loss:  0.4714292287826538
train gradient:  0.12076240410402675
iteration : 1748
train acc:  0.7578125
train loss:  0.43991249799728394
train gradient:  0.10100837476058462
iteration : 1749
train acc:  0.7578125
train loss:  0.45572733879089355
train gradient:  0.10698404406170837
iteration : 1750
train acc:  0.75
train loss:  0.47166550159454346
train gradient:  0.13709730806633302
iteration : 1751
train acc:  0.7578125
train loss:  0.47078651189804077
train gradient:  0.09920520306573793
iteration : 1752
train acc:  0.7109375
train loss:  0.494881272315979
train gradient:  0.14435676612725412
iteration : 1753
train acc:  0.8046875
train loss:  0.4068877696990967
train gradient:  0.08455255086197173
iteration : 1754
train acc:  0.7578125
train loss:  0.4627397358417511
train gradient:  0.1074783423758729
iteration : 1755
train acc:  0.6875
train loss:  0.512024998664856
train gradient:  0.12280425226945406
iteration : 1756
train acc:  0.7265625
train loss:  0.507217526435852
train gradient:  0.11649193018175252
iteration : 1757
train acc:  0.78125
train loss:  0.48273903131484985
train gradient:  0.1462907487725938
iteration : 1758
train acc:  0.7578125
train loss:  0.43197664618492126
train gradient:  0.09644830432335331
iteration : 1759
train acc:  0.7421875
train loss:  0.462924063205719
train gradient:  0.11147255952329495
iteration : 1760
train acc:  0.796875
train loss:  0.41121578216552734
train gradient:  0.10573423686710405
iteration : 1761
train acc:  0.7421875
train loss:  0.4709549844264984
train gradient:  0.10242526073201497
iteration : 1762
train acc:  0.7578125
train loss:  0.4990452229976654
train gradient:  0.11967644304046823
iteration : 1763
train acc:  0.71875
train loss:  0.5199442505836487
train gradient:  0.1336619922974182
iteration : 1764
train acc:  0.8203125
train loss:  0.40930861234664917
train gradient:  0.08132876905090279
iteration : 1765
train acc:  0.7265625
train loss:  0.5931888818740845
train gradient:  0.14339880889061654
iteration : 1766
train acc:  0.71875
train loss:  0.5061585903167725
train gradient:  0.13425795542254784
iteration : 1767
train acc:  0.6953125
train loss:  0.5322887897491455
train gradient:  0.15583110703603337
iteration : 1768
train acc:  0.7421875
train loss:  0.4912193715572357
train gradient:  0.12278509578814084
iteration : 1769
train acc:  0.78125
train loss:  0.4158785343170166
train gradient:  0.09678233388494252
iteration : 1770
train acc:  0.7265625
train loss:  0.5240939855575562
train gradient:  0.15948571954861368
iteration : 1771
train acc:  0.7890625
train loss:  0.41608327627182007
train gradient:  0.11310676062104937
iteration : 1772
train acc:  0.765625
train loss:  0.47304749488830566
train gradient:  0.10351367146968043
iteration : 1773
train acc:  0.765625
train loss:  0.4988211393356323
train gradient:  0.11416754551172915
iteration : 1774
train acc:  0.734375
train loss:  0.47378450632095337
train gradient:  0.10544063315337088
iteration : 1775
train acc:  0.703125
train loss:  0.5323638319969177
train gradient:  0.1575129992629996
iteration : 1776
train acc:  0.75
train loss:  0.43086719512939453
train gradient:  0.15534461796001525
iteration : 1777
train acc:  0.703125
train loss:  0.5254738926887512
train gradient:  0.1937822229190942
iteration : 1778
train acc:  0.71875
train loss:  0.5351145267486572
train gradient:  0.19375460475340545
iteration : 1779
train acc:  0.7109375
train loss:  0.4871513247489929
train gradient:  0.1114676413073945
iteration : 1780
train acc:  0.7578125
train loss:  0.5334048271179199
train gradient:  0.1662979738420748
iteration : 1781
train acc:  0.78125
train loss:  0.4407021999359131
train gradient:  0.12024820844721473
iteration : 1782
train acc:  0.7421875
train loss:  0.5184912085533142
train gradient:  0.16322936954909356
iteration : 1783
train acc:  0.734375
train loss:  0.5138426423072815
train gradient:  0.16157471140631832
iteration : 1784
train acc:  0.828125
train loss:  0.3718152642250061
train gradient:  0.09312720584826846
iteration : 1785
train acc:  0.7421875
train loss:  0.4750017821788788
train gradient:  0.11748700781213856
iteration : 1786
train acc:  0.765625
train loss:  0.49299144744873047
train gradient:  0.15963651140615248
iteration : 1787
train acc:  0.7109375
train loss:  0.4865753650665283
train gradient:  0.12943162442007272
iteration : 1788
train acc:  0.6796875
train loss:  0.5420773029327393
train gradient:  0.13565240498669595
iteration : 1789
train acc:  0.7890625
train loss:  0.4630040228366852
train gradient:  0.11153244358398948
iteration : 1790
train acc:  0.765625
train loss:  0.4732234477996826
train gradient:  0.12791320697750644
iteration : 1791
train acc:  0.765625
train loss:  0.44227564334869385
train gradient:  0.13309995625937404
iteration : 1792
train acc:  0.7890625
train loss:  0.4743195176124573
train gradient:  0.12683462683988903
iteration : 1793
train acc:  0.78125
train loss:  0.5293079018592834
train gradient:  0.1511315467597486
iteration : 1794
train acc:  0.78125
train loss:  0.463728666305542
train gradient:  0.11388438020615069
iteration : 1795
train acc:  0.7421875
train loss:  0.4908754229545593
train gradient:  0.14735169236279932
iteration : 1796
train acc:  0.6796875
train loss:  0.5241618156433105
train gradient:  0.14314268462058843
iteration : 1797
train acc:  0.765625
train loss:  0.48554369807243347
train gradient:  0.12019255529382475
iteration : 1798
train acc:  0.75
train loss:  0.4634971022605896
train gradient:  0.12819225112877594
iteration : 1799
train acc:  0.7421875
train loss:  0.4571637213230133
train gradient:  0.09553609850618505
iteration : 1800
train acc:  0.78125
train loss:  0.5192354917526245
train gradient:  0.1366375388696215
iteration : 1801
train acc:  0.8203125
train loss:  0.43363773822784424
train gradient:  0.08685449396430218
iteration : 1802
train acc:  0.796875
train loss:  0.4296717345714569
train gradient:  0.14501212043063627
iteration : 1803
train acc:  0.78125
train loss:  0.4119243621826172
train gradient:  0.09393825698454517
iteration : 1804
train acc:  0.78125
train loss:  0.49187326431274414
train gradient:  0.10585914524859541
iteration : 1805
train acc:  0.7734375
train loss:  0.4755622446537018
train gradient:  0.1279543541159595
iteration : 1806
train acc:  0.7421875
train loss:  0.5219376087188721
train gradient:  0.1257337879471893
iteration : 1807
train acc:  0.7578125
train loss:  0.44769105315208435
train gradient:  0.10497614141567758
iteration : 1808
train acc:  0.71875
train loss:  0.5481584072113037
train gradient:  0.12910036537221053
iteration : 1809
train acc:  0.7421875
train loss:  0.5292915105819702
train gradient:  0.13484833546925
iteration : 1810
train acc:  0.7578125
train loss:  0.5003939270973206
train gradient:  0.09738535236687582
iteration : 1811
train acc:  0.703125
train loss:  0.5617144107818604
train gradient:  0.15104484500885187
iteration : 1812
train acc:  0.71875
train loss:  0.5054699778556824
train gradient:  0.15030200321509352
iteration : 1813
train acc:  0.703125
train loss:  0.5071078538894653
train gradient:  0.14326862310253907
iteration : 1814
train acc:  0.7265625
train loss:  0.5060656666755676
train gradient:  0.11813345506625098
iteration : 1815
train acc:  0.7265625
train loss:  0.5188531875610352
train gradient:  0.17112896678581807
iteration : 1816
train acc:  0.71875
train loss:  0.5482311248779297
train gradient:  0.13546091080382805
iteration : 1817
train acc:  0.703125
train loss:  0.48906370997428894
train gradient:  0.13135543330241706
iteration : 1818
train acc:  0.7421875
train loss:  0.4666885733604431
train gradient:  0.12935578064073022
iteration : 1819
train acc:  0.7890625
train loss:  0.4659934639930725
train gradient:  0.12092206634677378
iteration : 1820
train acc:  0.7734375
train loss:  0.49214282631874084
train gradient:  0.10570982990373577
iteration : 1821
train acc:  0.7734375
train loss:  0.4231991767883301
train gradient:  0.12345798556630475
iteration : 1822
train acc:  0.7421875
train loss:  0.47014319896698
train gradient:  0.1124820227533161
iteration : 1823
train acc:  0.7578125
train loss:  0.4633457660675049
train gradient:  0.11395725365342788
iteration : 1824
train acc:  0.703125
train loss:  0.5052155256271362
train gradient:  0.11701378480869395
iteration : 1825
train acc:  0.71875
train loss:  0.5191136598587036
train gradient:  0.14011795770370064
iteration : 1826
train acc:  0.7421875
train loss:  0.5360549688339233
train gradient:  0.14709361347255306
iteration : 1827
train acc:  0.828125
train loss:  0.40553221106529236
train gradient:  0.08063791875042896
iteration : 1828
train acc:  0.78125
train loss:  0.4137864112854004
train gradient:  0.09084125895015374
iteration : 1829
train acc:  0.7265625
train loss:  0.4662742018699646
train gradient:  0.12321992923096516
iteration : 1830
train acc:  0.7578125
train loss:  0.4703347682952881
train gradient:  0.11082314770713644
iteration : 1831
train acc:  0.7265625
train loss:  0.48812246322631836
train gradient:  0.11649075239547477
iteration : 1832
train acc:  0.7421875
train loss:  0.5114197731018066
train gradient:  0.1729778268458541
iteration : 1833
train acc:  0.71875
train loss:  0.573881983757019
train gradient:  0.17957969847047633
iteration : 1834
train acc:  0.75
train loss:  0.4568464159965515
train gradient:  0.09953948816790262
iteration : 1835
train acc:  0.765625
train loss:  0.45085611939430237
train gradient:  0.09346494253783441
iteration : 1836
train acc:  0.765625
train loss:  0.5220195055007935
train gradient:  0.11501337264284059
iteration : 1837
train acc:  0.8046875
train loss:  0.421662837266922
train gradient:  0.09830240919982458
iteration : 1838
train acc:  0.7109375
train loss:  0.5516950488090515
train gradient:  0.15865832998103707
iteration : 1839
train acc:  0.7578125
train loss:  0.523422122001648
train gradient:  0.16162845407585222
iteration : 1840
train acc:  0.7109375
train loss:  0.5258401036262512
train gradient:  0.12788409116927268
iteration : 1841
train acc:  0.828125
train loss:  0.4461390972137451
train gradient:  0.1001787420448002
iteration : 1842
train acc:  0.75
train loss:  0.4320855140686035
train gradient:  0.0935769027340741
iteration : 1843
train acc:  0.65625
train loss:  0.6002534627914429
train gradient:  0.19192276106986603
iteration : 1844
train acc:  0.796875
train loss:  0.47345107793807983
train gradient:  0.13772447215389594
iteration : 1845
train acc:  0.7578125
train loss:  0.5193331241607666
train gradient:  0.13694475546388274
iteration : 1846
train acc:  0.7421875
train loss:  0.5046593546867371
train gradient:  0.14722164362404208
iteration : 1847
train acc:  0.71875
train loss:  0.4823351502418518
train gradient:  0.1042996488348014
iteration : 1848
train acc:  0.796875
train loss:  0.47911885380744934
train gradient:  0.11970529028242952
iteration : 1849
train acc:  0.8359375
train loss:  0.42453575134277344
train gradient:  0.09792408607337963
iteration : 1850
train acc:  0.7890625
train loss:  0.5087636113166809
train gradient:  0.1036791840732837
iteration : 1851
train acc:  0.71875
train loss:  0.5257161855697632
train gradient:  0.14379787371218533
iteration : 1852
train acc:  0.6875
train loss:  0.5836902856826782
train gradient:  0.12796810204480602
iteration : 1853
train acc:  0.703125
train loss:  0.5231894254684448
train gradient:  0.12715537549619044
iteration : 1854
train acc:  0.6875
train loss:  0.5226773023605347
train gradient:  0.11735816830642311
iteration : 1855
train acc:  0.765625
train loss:  0.4905570149421692
train gradient:  0.10723768702711764
iteration : 1856
train acc:  0.8125
train loss:  0.43447357416152954
train gradient:  0.10532076552833163
iteration : 1857
train acc:  0.7109375
train loss:  0.49036213755607605
train gradient:  0.11446028900265072
iteration : 1858
train acc:  0.7734375
train loss:  0.43064332008361816
train gradient:  0.09337126863020713
iteration : 1859
train acc:  0.78125
train loss:  0.44546666741371155
train gradient:  0.09309440656398707
iteration : 1860
train acc:  0.7421875
train loss:  0.5020762085914612
train gradient:  0.1275435750128301
iteration : 1861
train acc:  0.75
train loss:  0.47366243600845337
train gradient:  0.12391299051041633
iteration : 1862
train acc:  0.765625
train loss:  0.508700430393219
train gradient:  0.12216362336811289
iteration : 1863
train acc:  0.7734375
train loss:  0.43698710203170776
train gradient:  0.09699945972240734
iteration : 1864
train acc:  0.765625
train loss:  0.4579095244407654
train gradient:  0.10802997766918135
iteration : 1865
train acc:  0.6796875
train loss:  0.5289901494979858
train gradient:  0.11932533888444612
iteration : 1866
train acc:  0.8203125
train loss:  0.4250063896179199
train gradient:  0.09453899491555164
iteration : 1867
train acc:  0.796875
train loss:  0.42558053135871887
train gradient:  0.09768900422236258
iteration : 1868
train acc:  0.7421875
train loss:  0.5487567782402039
train gradient:  0.14419495202662885
iteration : 1869
train acc:  0.71875
train loss:  0.5020322203636169
train gradient:  0.15956514270484345
iteration : 1870
train acc:  0.75
train loss:  0.46902698278427124
train gradient:  0.11544971146540352
iteration : 1871
train acc:  0.7265625
train loss:  0.5126587152481079
train gradient:  0.1354319003457568
iteration : 1872
train acc:  0.75
train loss:  0.49442148208618164
train gradient:  0.14372607916674507
iteration : 1873
train acc:  0.78125
train loss:  0.5356520414352417
train gradient:  0.1412004618868333
iteration : 1874
train acc:  0.796875
train loss:  0.5067099332809448
train gradient:  0.12580280441570513
iteration : 1875
train acc:  0.75
train loss:  0.47831571102142334
train gradient:  0.13948624247664423
iteration : 1876
train acc:  0.7265625
train loss:  0.5060871839523315
train gradient:  0.14372191790575095
iteration : 1877
train acc:  0.7578125
train loss:  0.44161471724510193
train gradient:  0.10902830043341473
iteration : 1878
train acc:  0.7734375
train loss:  0.4659722149372101
train gradient:  0.10387359561094152
iteration : 1879
train acc:  0.6796875
train loss:  0.5146663188934326
train gradient:  0.1234657994551834
iteration : 1880
train acc:  0.7265625
train loss:  0.5750967264175415
train gradient:  0.18000264533844207
iteration : 1881
train acc:  0.7890625
train loss:  0.4521264433860779
train gradient:  0.09846591801502166
iteration : 1882
train acc:  0.7109375
train loss:  0.5464601516723633
train gradient:  0.1364759220965922
iteration : 1883
train acc:  0.8046875
train loss:  0.4375734329223633
train gradient:  0.0925935937338154
iteration : 1884
train acc:  0.7421875
train loss:  0.522644579410553
train gradient:  0.16329451155503222
iteration : 1885
train acc:  0.75
train loss:  0.5021865367889404
train gradient:  0.12284292778530179
iteration : 1886
train acc:  0.734375
train loss:  0.5232076644897461
train gradient:  0.10800641640092275
iteration : 1887
train acc:  0.78125
train loss:  0.47510552406311035
train gradient:  0.12484577492476727
iteration : 1888
train acc:  0.71875
train loss:  0.5334041714668274
train gradient:  0.18246646870526176
iteration : 1889
train acc:  0.84375
train loss:  0.4269161522388458
train gradient:  0.09279900166942387
iteration : 1890
train acc:  0.7734375
train loss:  0.4525187909603119
train gradient:  0.1064956853081319
iteration : 1891
train acc:  0.796875
train loss:  0.39650505781173706
train gradient:  0.0799353191515093
iteration : 1892
train acc:  0.703125
train loss:  0.5521530508995056
train gradient:  0.14461614433268488
iteration : 1893
train acc:  0.7734375
train loss:  0.49986377358436584
train gradient:  0.12017193178817888
iteration : 1894
train acc:  0.7265625
train loss:  0.4809421896934509
train gradient:  0.10269252437286985
iteration : 1895
train acc:  0.6875
train loss:  0.5094380378723145
train gradient:  0.11138700827865439
iteration : 1896
train acc:  0.6875
train loss:  0.5166277885437012
train gradient:  0.16188990712083684
iteration : 1897
train acc:  0.734375
train loss:  0.4837498068809509
train gradient:  0.10196772932127149
iteration : 1898
train acc:  0.8046875
train loss:  0.48041656613349915
train gradient:  0.11562692818148662
iteration : 1899
train acc:  0.6875
train loss:  0.5463231801986694
train gradient:  0.16443111490020107
iteration : 1900
train acc:  0.78125
train loss:  0.4551076292991638
train gradient:  0.099366744951256
iteration : 1901
train acc:  0.7578125
train loss:  0.5035095810890198
train gradient:  0.1176543804444771
iteration : 1902
train acc:  0.78125
train loss:  0.4349801242351532
train gradient:  0.08426557389414993
iteration : 1903
train acc:  0.765625
train loss:  0.48329880833625793
train gradient:  0.12286419236597573
iteration : 1904
train acc:  0.703125
train loss:  0.5181946754455566
train gradient:  0.15597393421063327
iteration : 1905
train acc:  0.734375
train loss:  0.4801361560821533
train gradient:  0.11283432353544995
iteration : 1906
train acc:  0.765625
train loss:  0.43684810400009155
train gradient:  0.09770893551503945
iteration : 1907
train acc:  0.7265625
train loss:  0.5158044695854187
train gradient:  0.14181997383032058
iteration : 1908
train acc:  0.7578125
train loss:  0.43825212121009827
train gradient:  0.09207941186931601
iteration : 1909
train acc:  0.7421875
train loss:  0.5238423347473145
train gradient:  0.14627405656991488
iteration : 1910
train acc:  0.6953125
train loss:  0.5041513442993164
train gradient:  0.1276562132326068
iteration : 1911
train acc:  0.8046875
train loss:  0.4083242416381836
train gradient:  0.09008816758338208
iteration : 1912
train acc:  0.78125
train loss:  0.4472283124923706
train gradient:  0.09223324546336872
iteration : 1913
train acc:  0.734375
train loss:  0.5040014982223511
train gradient:  0.10588144560448494
iteration : 1914
train acc:  0.8125
train loss:  0.43124133348464966
train gradient:  0.12249852813374967
iteration : 1915
train acc:  0.7109375
train loss:  0.5596723556518555
train gradient:  0.1514975001411924
iteration : 1916
train acc:  0.734375
train loss:  0.4643588066101074
train gradient:  0.11551040509968258
iteration : 1917
train acc:  0.828125
train loss:  0.42567259073257446
train gradient:  0.08746159680185865
iteration : 1918
train acc:  0.734375
train loss:  0.5595601201057434
train gradient:  0.1482772629712214
iteration : 1919
train acc:  0.84375
train loss:  0.40301549434661865
train gradient:  0.0944121469314022
iteration : 1920
train acc:  0.7734375
train loss:  0.5096067190170288
train gradient:  0.12989409008388966
iteration : 1921
train acc:  0.703125
train loss:  0.4945101737976074
train gradient:  0.14772615181602905
iteration : 1922
train acc:  0.75
train loss:  0.5328746438026428
train gradient:  0.14420053955857343
iteration : 1923
train acc:  0.75
train loss:  0.4562273621559143
train gradient:  0.11304877266744914
iteration : 1924
train acc:  0.796875
train loss:  0.45535725355148315
train gradient:  0.111751590206965
iteration : 1925
train acc:  0.7890625
train loss:  0.48249486088752747
train gradient:  0.13664973390888896
iteration : 1926
train acc:  0.6953125
train loss:  0.5590510368347168
train gradient:  0.1506836992792036
iteration : 1927
train acc:  0.75
train loss:  0.5151396989822388
train gradient:  0.12094122501730589
iteration : 1928
train acc:  0.7265625
train loss:  0.5355170965194702
train gradient:  0.15252210631239996
iteration : 1929
train acc:  0.765625
train loss:  0.4540212154388428
train gradient:  0.10723897339971654
iteration : 1930
train acc:  0.7421875
train loss:  0.49433380365371704
train gradient:  0.1158735530125631
iteration : 1931
train acc:  0.7734375
train loss:  0.4186447858810425
train gradient:  0.09927032371795297
iteration : 1932
train acc:  0.78125
train loss:  0.4641377627849579
train gradient:  0.10167028869542473
iteration : 1933
train acc:  0.765625
train loss:  0.44913744926452637
train gradient:  0.07909511044993832
iteration : 1934
train acc:  0.8125
train loss:  0.44126570224761963
train gradient:  0.09268751347644033
iteration : 1935
train acc:  0.7578125
train loss:  0.45512986183166504
train gradient:  0.11681896606888163
iteration : 1936
train acc:  0.8359375
train loss:  0.39625903964042664
train gradient:  0.08088494069008906
iteration : 1937
train acc:  0.703125
train loss:  0.4905235767364502
train gradient:  0.11479603168087767
iteration : 1938
train acc:  0.78125
train loss:  0.4955730736255646
train gradient:  0.11979581790724404
iteration : 1939
train acc:  0.7421875
train loss:  0.4324437975883484
train gradient:  0.08642936708540869
iteration : 1940
train acc:  0.75
train loss:  0.4593856930732727
train gradient:  0.11026730045100028
iteration : 1941
train acc:  0.7265625
train loss:  0.5055086016654968
train gradient:  0.12011029300434109
iteration : 1942
train acc:  0.7265625
train loss:  0.4930269122123718
train gradient:  0.11047316541743438
iteration : 1943
train acc:  0.75
train loss:  0.5015080571174622
train gradient:  0.10859596494059325
iteration : 1944
train acc:  0.78125
train loss:  0.4443063735961914
train gradient:  0.09119264960297475
iteration : 1945
train acc:  0.7890625
train loss:  0.46377313137054443
train gradient:  0.10848584608856782
iteration : 1946
train acc:  0.78125
train loss:  0.4319459795951843
train gradient:  0.09829761850799712
iteration : 1947
train acc:  0.734375
train loss:  0.5031663775444031
train gradient:  0.10333068260100814
iteration : 1948
train acc:  0.8515625
train loss:  0.39851975440979004
train gradient:  0.08914959937460473
iteration : 1949
train acc:  0.71875
train loss:  0.5169707536697388
train gradient:  0.1140407561882552
iteration : 1950
train acc:  0.765625
train loss:  0.5230103731155396
train gradient:  0.12893900726591007
iteration : 1951
train acc:  0.78125
train loss:  0.4685080945491791
train gradient:  0.11153662199121275
iteration : 1952
train acc:  0.8203125
train loss:  0.42032700777053833
train gradient:  0.0999130693734484
iteration : 1953
train acc:  0.734375
train loss:  0.49184274673461914
train gradient:  0.105958470194363
iteration : 1954
train acc:  0.734375
train loss:  0.5045000314712524
train gradient:  0.14257457984024424
iteration : 1955
train acc:  0.7578125
train loss:  0.4742375314235687
train gradient:  0.13460201328879623
iteration : 1956
train acc:  0.734375
train loss:  0.5059069395065308
train gradient:  0.12279337081451443
iteration : 1957
train acc:  0.765625
train loss:  0.4466775357723236
train gradient:  0.13972690645581293
iteration : 1958
train acc:  0.671875
train loss:  0.5529139041900635
train gradient:  0.16266975805831768
iteration : 1959
train acc:  0.7265625
train loss:  0.4885164201259613
train gradient:  0.13085587242257002
iteration : 1960
train acc:  0.71875
train loss:  0.5108575820922852
train gradient:  0.11230979135125607
iteration : 1961
train acc:  0.7265625
train loss:  0.5092048645019531
train gradient:  0.13523047354821865
iteration : 1962
train acc:  0.7421875
train loss:  0.48742222785949707
train gradient:  0.16114249156049953
iteration : 1963
train acc:  0.765625
train loss:  0.4649233818054199
train gradient:  0.13818144879792305
iteration : 1964
train acc:  0.6875
train loss:  0.47559720277786255
train gradient:  0.11083128569395444
iteration : 1965
train acc:  0.7421875
train loss:  0.48488718271255493
train gradient:  0.10132980914580578
iteration : 1966
train acc:  0.71875
train loss:  0.45653969049453735
train gradient:  0.13116219065585555
iteration : 1967
train acc:  0.734375
train loss:  0.49956807494163513
train gradient:  0.12799961546454658
iteration : 1968
train acc:  0.7421875
train loss:  0.49587681889533997
train gradient:  0.1366767402804164
iteration : 1969
train acc:  0.734375
train loss:  0.4726960062980652
train gradient:  0.12064414811929802
iteration : 1970
train acc:  0.71875
train loss:  0.5021705031394958
train gradient:  0.1285040957976481
iteration : 1971
train acc:  0.75
train loss:  0.5425800085067749
train gradient:  0.11277686209310897
iteration : 1972
train acc:  0.7421875
train loss:  0.47718149423599243
train gradient:  0.1108195057256583
iteration : 1973
train acc:  0.765625
train loss:  0.45140984654426575
train gradient:  0.09576461420783823
iteration : 1974
train acc:  0.84375
train loss:  0.41055670380592346
train gradient:  0.10141369477217957
iteration : 1975
train acc:  0.7890625
train loss:  0.4329708218574524
train gradient:  0.08215996298178004
iteration : 1976
train acc:  0.703125
train loss:  0.5119065642356873
train gradient:  0.1495040903994555
iteration : 1977
train acc:  0.7578125
train loss:  0.4728069007396698
train gradient:  0.14694379309652306
iteration : 1978
train acc:  0.7734375
train loss:  0.46383732557296753
train gradient:  0.08354613493356411
iteration : 1979
train acc:  0.75
train loss:  0.48291727900505066
train gradient:  0.12416031942424452
iteration : 1980
train acc:  0.703125
train loss:  0.5339851379394531
train gradient:  0.18184454396575161
iteration : 1981
train acc:  0.6796875
train loss:  0.5391538739204407
train gradient:  0.1365327490866511
iteration : 1982
train acc:  0.75
train loss:  0.40679627656936646
train gradient:  0.09398239965927208
iteration : 1983
train acc:  0.7265625
train loss:  0.49918627738952637
train gradient:  0.1117093863280774
iteration : 1984
train acc:  0.7265625
train loss:  0.48874542117118835
train gradient:  0.11357153594678361
iteration : 1985
train acc:  0.703125
train loss:  0.5224987268447876
train gradient:  0.16335384371774897
iteration : 1986
train acc:  0.8203125
train loss:  0.41772523522377014
train gradient:  0.10322750174545021
iteration : 1987
train acc:  0.7578125
train loss:  0.4923766255378723
train gradient:  0.14133639307494522
iteration : 1988
train acc:  0.796875
train loss:  0.41019901633262634
train gradient:  0.09670826059467715
iteration : 1989
train acc:  0.8125
train loss:  0.4147096276283264
train gradient:  0.07464150965405161
iteration : 1990
train acc:  0.78125
train loss:  0.4239109456539154
train gradient:  0.07658798397435243
iteration : 1991
train acc:  0.78125
train loss:  0.46804600954055786
train gradient:  0.12084641583168453
iteration : 1992
train acc:  0.7734375
train loss:  0.46474689245224
train gradient:  0.09721782309880031
iteration : 1993
train acc:  0.765625
train loss:  0.4278213381767273
train gradient:  0.11340847201611554
iteration : 1994
train acc:  0.703125
train loss:  0.5409404635429382
train gradient:  0.20477128693545413
iteration : 1995
train acc:  0.7578125
train loss:  0.4636630117893219
train gradient:  0.13652381938292424
iteration : 1996
train acc:  0.7109375
train loss:  0.508845329284668
train gradient:  0.14514190323244003
iteration : 1997
train acc:  0.796875
train loss:  0.4650336802005768
train gradient:  0.09499696767807717
iteration : 1998
train acc:  0.765625
train loss:  0.46936535835266113
train gradient:  0.11462110481595883
iteration : 1999
train acc:  0.7421875
train loss:  0.4696621298789978
train gradient:  0.15039360490616469
iteration : 2000
train acc:  0.859375
train loss:  0.36122971773147583
train gradient:  0.09154776384534603
iteration : 2001
train acc:  0.7421875
train loss:  0.5187786817550659
train gradient:  0.14570478012518018
iteration : 2002
train acc:  0.7421875
train loss:  0.47220736742019653
train gradient:  0.13554301927926699
iteration : 2003
train acc:  0.75
train loss:  0.4688369035720825
train gradient:  0.11964469242043471
iteration : 2004
train acc:  0.7578125
train loss:  0.5144000053405762
train gradient:  0.12375343211007547
iteration : 2005
train acc:  0.7265625
train loss:  0.5295323133468628
train gradient:  0.13233737870559098
iteration : 2006
train acc:  0.640625
train loss:  0.5694650411605835
train gradient:  0.14290584610388585
iteration : 2007
train acc:  0.75
train loss:  0.45130735635757446
train gradient:  0.08556502394421323
iteration : 2008
train acc:  0.75
train loss:  0.5088472366333008
train gradient:  0.11911915748226658
iteration : 2009
train acc:  0.7265625
train loss:  0.5324385166168213
train gradient:  0.12462800587355942
iteration : 2010
train acc:  0.7890625
train loss:  0.4862864017486572
train gradient:  0.11601258968503961
iteration : 2011
train acc:  0.75
train loss:  0.49453258514404297
train gradient:  0.1259585746872503
iteration : 2012
train acc:  0.703125
train loss:  0.512823760509491
train gradient:  0.13061528988523247
iteration : 2013
train acc:  0.7421875
train loss:  0.4219435453414917
train gradient:  0.08825299078947534
iteration : 2014
train acc:  0.7265625
train loss:  0.5750576257705688
train gradient:  0.1517734649079884
iteration : 2015
train acc:  0.7421875
train loss:  0.5327062606811523
train gradient:  0.19604830487697406
iteration : 2016
train acc:  0.7109375
train loss:  0.500170111656189
train gradient:  0.13214423846505038
iteration : 2017
train acc:  0.765625
train loss:  0.4676744341850281
train gradient:  0.13295432346170144
iteration : 2018
train acc:  0.7109375
train loss:  0.5637928247451782
train gradient:  0.16477128464712065
iteration : 2019
train acc:  0.7734375
train loss:  0.451222687959671
train gradient:  0.1036774692289499
iteration : 2020
train acc:  0.71875
train loss:  0.5182217359542847
train gradient:  0.13557406697554752
iteration : 2021
train acc:  0.71875
train loss:  0.5276618599891663
train gradient:  0.1426317547314102
iteration : 2022
train acc:  0.703125
train loss:  0.5361330509185791
train gradient:  0.13765228416627767
iteration : 2023
train acc:  0.7421875
train loss:  0.4794991612434387
train gradient:  0.10307437645360029
iteration : 2024
train acc:  0.75
train loss:  0.47544437646865845
train gradient:  0.10587902866884873
iteration : 2025
train acc:  0.6796875
train loss:  0.5426487326622009
train gradient:  0.1526342655647182
iteration : 2026
train acc:  0.7421875
train loss:  0.5125155448913574
train gradient:  0.12189510485111382
iteration : 2027
train acc:  0.8203125
train loss:  0.43655070662498474
train gradient:  0.10801253744574228
iteration : 2028
train acc:  0.7890625
train loss:  0.46575096249580383
train gradient:  0.08892146541170812
iteration : 2029
train acc:  0.7421875
train loss:  0.5107142925262451
train gradient:  0.16987351014338165
iteration : 2030
train acc:  0.828125
train loss:  0.4059866666793823
train gradient:  0.09166298740862809
iteration : 2031
train acc:  0.7734375
train loss:  0.45579302310943604
train gradient:  0.09215549856833774
iteration : 2032
train acc:  0.75
train loss:  0.4532244801521301
train gradient:  0.10981785139495921
iteration : 2033
train acc:  0.7578125
train loss:  0.4541296362876892
train gradient:  0.10659117119154765
iteration : 2034
train acc:  0.7421875
train loss:  0.4906841516494751
train gradient:  0.14153636124245306
iteration : 2035
train acc:  0.703125
train loss:  0.4839526116847992
train gradient:  0.11766165579357629
iteration : 2036
train acc:  0.7734375
train loss:  0.4621555209159851
train gradient:  0.12093643022026238
iteration : 2037
train acc:  0.7578125
train loss:  0.5028255581855774
train gradient:  0.1336639858210296
iteration : 2038
train acc:  0.765625
train loss:  0.42385920882225037
train gradient:  0.11056692156097898
iteration : 2039
train acc:  0.7890625
train loss:  0.47551536560058594
train gradient:  0.12071180466812206
iteration : 2040
train acc:  0.6953125
train loss:  0.49386030435562134
train gradient:  0.12202795071949628
iteration : 2041
train acc:  0.7265625
train loss:  0.5429588556289673
train gradient:  0.15314043178464715
iteration : 2042
train acc:  0.7734375
train loss:  0.4445774257183075
train gradient:  0.07726840064211389
iteration : 2043
train acc:  0.7890625
train loss:  0.4244718551635742
train gradient:  0.0846831863840574
iteration : 2044
train acc:  0.734375
train loss:  0.49433356523513794
train gradient:  0.1514779002517804
iteration : 2045
train acc:  0.8046875
train loss:  0.4707317650318146
train gradient:  0.12512054381930918
iteration : 2046
train acc:  0.765625
train loss:  0.5230591893196106
train gradient:  0.16366204022168535
iteration : 2047
train acc:  0.71875
train loss:  0.4829815626144409
train gradient:  0.10699647334487732
iteration : 2048
train acc:  0.6796875
train loss:  0.5306339263916016
train gradient:  0.1307856372788669
iteration : 2049
train acc:  0.78125
train loss:  0.49494534730911255
train gradient:  0.11156157201594606
iteration : 2050
train acc:  0.6875
train loss:  0.5150845050811768
train gradient:  0.14054000479278922
iteration : 2051
train acc:  0.6953125
train loss:  0.543311357498169
train gradient:  0.14499911575149838
iteration : 2052
train acc:  0.75
train loss:  0.4727286696434021
train gradient:  0.10658590820374977
iteration : 2053
train acc:  0.734375
train loss:  0.4773837625980377
train gradient:  0.12094994580087867
iteration : 2054
train acc:  0.71875
train loss:  0.5262694954872131
train gradient:  0.13131048719380775
iteration : 2055
train acc:  0.796875
train loss:  0.4424744248390198
train gradient:  0.10015066007661924
iteration : 2056
train acc:  0.734375
train loss:  0.4754455089569092
train gradient:  0.11370770024307182
iteration : 2057
train acc:  0.8046875
train loss:  0.44006723165512085
train gradient:  0.10218134412985222
iteration : 2058
train acc:  0.7421875
train loss:  0.4884624481201172
train gradient:  0.11015439278128263
iteration : 2059
train acc:  0.78125
train loss:  0.514438271522522
train gradient:  0.14549265773798076
iteration : 2060
train acc:  0.734375
train loss:  0.48365598917007446
train gradient:  0.14909138480951561
iteration : 2061
train acc:  0.765625
train loss:  0.4528210163116455
train gradient:  0.10899076286527856
iteration : 2062
train acc:  0.7734375
train loss:  0.45959120988845825
train gradient:  0.09633357397533819
iteration : 2063
train acc:  0.796875
train loss:  0.44440925121307373
train gradient:  0.10461647927876715
iteration : 2064
train acc:  0.7421875
train loss:  0.47401928901672363
train gradient:  0.10508497035019555
iteration : 2065
train acc:  0.8359375
train loss:  0.47535690665245056
train gradient:  0.17568138575203968
iteration : 2066
train acc:  0.7734375
train loss:  0.4499398469924927
train gradient:  0.09406973445306006
iteration : 2067
train acc:  0.75
train loss:  0.4824604392051697
train gradient:  0.15005723370695068
iteration : 2068
train acc:  0.7265625
train loss:  0.48571568727493286
train gradient:  0.15578482905000718
iteration : 2069
train acc:  0.765625
train loss:  0.5310038328170776
train gradient:  0.1546810553573379
iteration : 2070
train acc:  0.734375
train loss:  0.4739775061607361
train gradient:  0.12194577196303019
iteration : 2071
train acc:  0.7734375
train loss:  0.43560534715652466
train gradient:  0.1023095290499739
iteration : 2072
train acc:  0.7265625
train loss:  0.4508809447288513
train gradient:  0.10830270736475786
iteration : 2073
train acc:  0.75
train loss:  0.5076231956481934
train gradient:  0.10914437666537954
iteration : 2074
train acc:  0.7578125
train loss:  0.4774499535560608
train gradient:  0.11740068864475423
iteration : 2075
train acc:  0.75
train loss:  0.4882988929748535
train gradient:  0.12633846462710113
iteration : 2076
train acc:  0.734375
train loss:  0.5002105832099915
train gradient:  0.13909969473034411
iteration : 2077
train acc:  0.71875
train loss:  0.49912047386169434
train gradient:  0.14346805145388522
iteration : 2078
train acc:  0.6640625
train loss:  0.5545639395713806
train gradient:  0.15025185043702327
iteration : 2079
train acc:  0.7734375
train loss:  0.4985928237438202
train gradient:  0.11045068684542247
iteration : 2080
train acc:  0.78125
train loss:  0.4560242295265198
train gradient:  0.14881645821808764
iteration : 2081
train acc:  0.78125
train loss:  0.468736469745636
train gradient:  0.10839480078519657
iteration : 2082
train acc:  0.7734375
train loss:  0.4971117675304413
train gradient:  0.10453998953885775
iteration : 2083
train acc:  0.6796875
train loss:  0.5754298567771912
train gradient:  0.16511947273208877
iteration : 2084
train acc:  0.734375
train loss:  0.522174596786499
train gradient:  0.13927011526796396
iteration : 2085
train acc:  0.765625
train loss:  0.47031861543655396
train gradient:  0.1144090647586676
iteration : 2086
train acc:  0.71875
train loss:  0.5094307661056519
train gradient:  0.12306397174049144
iteration : 2087
train acc:  0.7421875
train loss:  0.44597291946411133
train gradient:  0.10675479474283683
iteration : 2088
train acc:  0.734375
train loss:  0.4895298182964325
train gradient:  0.12447220529603514
iteration : 2089
train acc:  0.703125
train loss:  0.4808877110481262
train gradient:  0.116887875356616
iteration : 2090
train acc:  0.7421875
train loss:  0.5101510286331177
train gradient:  0.12004382074854453
iteration : 2091
train acc:  0.6953125
train loss:  0.5246317982673645
train gradient:  0.12752602385806724
iteration : 2092
train acc:  0.7734375
train loss:  0.40973377227783203
train gradient:  0.08613335263253805
iteration : 2093
train acc:  0.7734375
train loss:  0.47037625312805176
train gradient:  0.11527172636121982
iteration : 2094
train acc:  0.703125
train loss:  0.5715705156326294
train gradient:  0.16294700037713977
iteration : 2095
train acc:  0.734375
train loss:  0.5011658668518066
train gradient:  0.13834627785959935
iteration : 2096
train acc:  0.7890625
train loss:  0.4444265365600586
train gradient:  0.13065235954015686
iteration : 2097
train acc:  0.703125
train loss:  0.5288699269294739
train gradient:  0.13100903804226086
iteration : 2098
train acc:  0.7578125
train loss:  0.4393135905265808
train gradient:  0.09884507622502701
iteration : 2099
train acc:  0.7265625
train loss:  0.5038636326789856
train gradient:  0.15022125086369575
iteration : 2100
train acc:  0.796875
train loss:  0.42937713861465454
train gradient:  0.11271642270059445
iteration : 2101
train acc:  0.7890625
train loss:  0.42946815490722656
train gradient:  0.09978514505696766
iteration : 2102
train acc:  0.71875
train loss:  0.5049694180488586
train gradient:  0.1614873917527527
iteration : 2103
train acc:  0.765625
train loss:  0.4623914957046509
train gradient:  0.10279831663092798
iteration : 2104
train acc:  0.75
train loss:  0.48626425862312317
train gradient:  0.10314572393893173
iteration : 2105
train acc:  0.7421875
train loss:  0.5046166181564331
train gradient:  0.15801751692150715
iteration : 2106
train acc:  0.765625
train loss:  0.4584697484970093
train gradient:  0.1291830324385867
iteration : 2107
train acc:  0.765625
train loss:  0.4544145464897156
train gradient:  0.1090891663134025
iteration : 2108
train acc:  0.796875
train loss:  0.4115244746208191
train gradient:  0.08027667795066096
iteration : 2109
train acc:  0.703125
train loss:  0.5150827765464783
train gradient:  0.12367960980295306
iteration : 2110
train acc:  0.7734375
train loss:  0.48099350929260254
train gradient:  0.13223245432965902
iteration : 2111
train acc:  0.703125
train loss:  0.5203864574432373
train gradient:  0.113530439086579
iteration : 2112
train acc:  0.75
train loss:  0.5035840272903442
train gradient:  0.1267061262715205
iteration : 2113
train acc:  0.671875
train loss:  0.5041601657867432
train gradient:  0.12797869771530812
iteration : 2114
train acc:  0.6953125
train loss:  0.5818849802017212
train gradient:  0.1776281179203238
iteration : 2115
train acc:  0.7265625
train loss:  0.47111040353775024
train gradient:  0.11692172703243088
iteration : 2116
train acc:  0.6328125
train loss:  0.6479589939117432
train gradient:  0.21650534695400256
iteration : 2117
train acc:  0.8125
train loss:  0.4050307273864746
train gradient:  0.09417906170797237
iteration : 2118
train acc:  0.7890625
train loss:  0.4237966537475586
train gradient:  0.1006138541380726
iteration : 2119
train acc:  0.6796875
train loss:  0.5579004287719727
train gradient:  0.1402110722460535
iteration : 2120
train acc:  0.7734375
train loss:  0.5132335424423218
train gradient:  0.13144624166956295
iteration : 2121
train acc:  0.796875
train loss:  0.4563485383987427
train gradient:  0.103537779463054
iteration : 2122
train acc:  0.75
train loss:  0.5454641580581665
train gradient:  0.16211048022661628
iteration : 2123
train acc:  0.765625
train loss:  0.5124527812004089
train gradient:  0.13542586192054296
iteration : 2124
train acc:  0.78125
train loss:  0.472552090883255
train gradient:  0.12713822972183497
iteration : 2125
train acc:  0.7265625
train loss:  0.5218702554702759
train gradient:  0.156536432095564
iteration : 2126
train acc:  0.703125
train loss:  0.47803691029548645
train gradient:  0.13620892206949162
iteration : 2127
train acc:  0.7109375
train loss:  0.5443141460418701
train gradient:  0.14641689264176064
iteration : 2128
train acc:  0.7265625
train loss:  0.47800642251968384
train gradient:  0.12212982022941098
iteration : 2129
train acc:  0.828125
train loss:  0.3810780346393585
train gradient:  0.06729883471703949
iteration : 2130
train acc:  0.6953125
train loss:  0.48292797803878784
train gradient:  0.10590625201980594
iteration : 2131
train acc:  0.75
train loss:  0.47825461626052856
train gradient:  0.11095784944878
iteration : 2132
train acc:  0.765625
train loss:  0.44230878353118896
train gradient:  0.1051595740373589
iteration : 2133
train acc:  0.75
train loss:  0.4592120945453644
train gradient:  0.13178234172101608
iteration : 2134
train acc:  0.703125
train loss:  0.5559632182121277
train gradient:  0.15512768035735697
iteration : 2135
train acc:  0.71875
train loss:  0.48218730092048645
train gradient:  0.0969655234189829
iteration : 2136
train acc:  0.75
train loss:  0.47040826082229614
train gradient:  0.10643036451224445
iteration : 2137
train acc:  0.734375
train loss:  0.5691779255867004
train gradient:  0.177574408869601
iteration : 2138
train acc:  0.796875
train loss:  0.442621648311615
train gradient:  0.12484459527116841
iteration : 2139
train acc:  0.796875
train loss:  0.45724618434906006
train gradient:  0.11617415471066395
iteration : 2140
train acc:  0.7578125
train loss:  0.48683521151542664
train gradient:  0.17503897621395345
iteration : 2141
train acc:  0.6953125
train loss:  0.5397993922233582
train gradient:  0.17652615561120738
iteration : 2142
train acc:  0.71875
train loss:  0.5411304235458374
train gradient:  0.15609187654853968
iteration : 2143
train acc:  0.7578125
train loss:  0.46038490533828735
train gradient:  0.09394958688704334
iteration : 2144
train acc:  0.75
train loss:  0.4741029143333435
train gradient:  0.12587762235666833
iteration : 2145
train acc:  0.7890625
train loss:  0.43937206268310547
train gradient:  0.09717894967452041
iteration : 2146
train acc:  0.7734375
train loss:  0.4267870783805847
train gradient:  0.11001404046388684
iteration : 2147
train acc:  0.75
train loss:  0.4889727830886841
train gradient:  0.13116041102537424
iteration : 2148
train acc:  0.7265625
train loss:  0.4980127513408661
train gradient:  0.14581116743652944
iteration : 2149
train acc:  0.7109375
train loss:  0.5897519588470459
train gradient:  0.18591089678362427
iteration : 2150
train acc:  0.765625
train loss:  0.5243083238601685
train gradient:  0.10495662153841857
iteration : 2151
train acc:  0.7578125
train loss:  0.4379347562789917
train gradient:  0.1420534319898971
iteration : 2152
train acc:  0.6875
train loss:  0.5424370169639587
train gradient:  0.1499282086044911
iteration : 2153
train acc:  0.765625
train loss:  0.4759330153465271
train gradient:  0.12737758881567784
iteration : 2154
train acc:  0.7109375
train loss:  0.5160627365112305
train gradient:  0.14793734966626954
iteration : 2155
train acc:  0.6953125
train loss:  0.5330190658569336
train gradient:  0.1304642812860114
iteration : 2156
train acc:  0.7421875
train loss:  0.49984562397003174
train gradient:  0.14111186190236802
iteration : 2157
train acc:  0.6875
train loss:  0.5328359603881836
train gradient:  0.1575557005773729
iteration : 2158
train acc:  0.7265625
train loss:  0.4758839011192322
train gradient:  0.1032377639973501
iteration : 2159
train acc:  0.6875
train loss:  0.5136744976043701
train gradient:  0.12089358289316769
iteration : 2160
train acc:  0.7109375
train loss:  0.5784111022949219
train gradient:  0.2095813025113062
iteration : 2161
train acc:  0.75
train loss:  0.49989545345306396
train gradient:  0.1320136058672719
iteration : 2162
train acc:  0.75
train loss:  0.4578278362751007
train gradient:  0.13878342427381696
iteration : 2163
train acc:  0.7265625
train loss:  0.4745386242866516
train gradient:  0.1262840896345378
iteration : 2164
train acc:  0.84375
train loss:  0.37963593006134033
train gradient:  0.09387024885276009
iteration : 2165
train acc:  0.7578125
train loss:  0.4522320330142975
train gradient:  0.10057062380874301
iteration : 2166
train acc:  0.71875
train loss:  0.5753815174102783
train gradient:  0.15279855531131342
iteration : 2167
train acc:  0.7734375
train loss:  0.44079673290252686
train gradient:  0.10500448300075975
iteration : 2168
train acc:  0.734375
train loss:  0.47376471757888794
train gradient:  0.131351843031764
iteration : 2169
train acc:  0.75
train loss:  0.4452955424785614
train gradient:  0.10189905847041947
iteration : 2170
train acc:  0.7265625
train loss:  0.5065283179283142
train gradient:  0.11468657055232548
iteration : 2171
train acc:  0.7421875
train loss:  0.4700331687927246
train gradient:  0.09141613891436917
iteration : 2172
train acc:  0.765625
train loss:  0.45278412103652954
train gradient:  0.11084096753385367
iteration : 2173
train acc:  0.7109375
train loss:  0.5143183469772339
train gradient:  0.15791791791575926
iteration : 2174
train acc:  0.734375
train loss:  0.47963738441467285
train gradient:  0.09957902908478213
iteration : 2175
train acc:  0.7421875
train loss:  0.47060590982437134
train gradient:  0.1376933697546322
iteration : 2176
train acc:  0.75
train loss:  0.4663158655166626
train gradient:  0.1263266966662674
iteration : 2177
train acc:  0.7421875
train loss:  0.4585002660751343
train gradient:  0.10921905717898077
iteration : 2178
train acc:  0.734375
train loss:  0.44325074553489685
train gradient:  0.10478160520369438
iteration : 2179
train acc:  0.7265625
train loss:  0.537521243095398
train gradient:  0.12513533404633237
iteration : 2180
train acc:  0.75
train loss:  0.5075291395187378
train gradient:  0.11936870271268872
iteration : 2181
train acc:  0.75
train loss:  0.46137115359306335
train gradient:  0.11257402253504005
iteration : 2182
train acc:  0.703125
train loss:  0.5504717230796814
train gradient:  0.1653024488494448
iteration : 2183
train acc:  0.7421875
train loss:  0.5022275447845459
train gradient:  0.10668400547143088
iteration : 2184
train acc:  0.7578125
train loss:  0.4488597512245178
train gradient:  0.10883894383320766
iteration : 2185
train acc:  0.6796875
train loss:  0.5790033340454102
train gradient:  0.2179069503402834
iteration : 2186
train acc:  0.8046875
train loss:  0.4075782895088196
train gradient:  0.09479471247214191
iteration : 2187
train acc:  0.7734375
train loss:  0.4563247561454773
train gradient:  0.09196662255744516
iteration : 2188
train acc:  0.703125
train loss:  0.5620514154434204
train gradient:  0.11747292140153766
iteration : 2189
train acc:  0.7578125
train loss:  0.45866602659225464
train gradient:  0.09378360419038917
iteration : 2190
train acc:  0.7578125
train loss:  0.49947142601013184
train gradient:  0.11960786607476508
iteration : 2191
train acc:  0.8046875
train loss:  0.48429539799690247
train gradient:  0.1273798939720793
iteration : 2192
train acc:  0.7109375
train loss:  0.5220533609390259
train gradient:  0.142239120428431
iteration : 2193
train acc:  0.75
train loss:  0.465696781873703
train gradient:  0.10622108062162855
iteration : 2194
train acc:  0.765625
train loss:  0.45544058084487915
train gradient:  0.09399564019783194
iteration : 2195
train acc:  0.75
train loss:  0.4777722656726837
train gradient:  0.1106049006990648
iteration : 2196
train acc:  0.6953125
train loss:  0.5524647235870361
train gradient:  0.14406682740404042
iteration : 2197
train acc:  0.71875
train loss:  0.5429774522781372
train gradient:  0.14101253912758935
iteration : 2198
train acc:  0.7421875
train loss:  0.4626142978668213
train gradient:  0.12654312216188943
iteration : 2199
train acc:  0.703125
train loss:  0.5107355117797852
train gradient:  0.13118284580575668
iteration : 2200
train acc:  0.7734375
train loss:  0.4506053924560547
train gradient:  0.12012432219946709
iteration : 2201
train acc:  0.7109375
train loss:  0.5889009237289429
train gradient:  0.2525235795893285
iteration : 2202
train acc:  0.75
train loss:  0.4419040083885193
train gradient:  0.08176378200714014
iteration : 2203
train acc:  0.7578125
train loss:  0.49446094036102295
train gradient:  0.115098314003573
iteration : 2204
train acc:  0.71875
train loss:  0.5381220579147339
train gradient:  0.14494002481490237
iteration : 2205
train acc:  0.75
train loss:  0.44999390840530396
train gradient:  0.10801683877228602
iteration : 2206
train acc:  0.7421875
train loss:  0.45956093072891235
train gradient:  0.12209745584244827
iteration : 2207
train acc:  0.765625
train loss:  0.4371975064277649
train gradient:  0.10144668595778474
iteration : 2208
train acc:  0.7578125
train loss:  0.45793765783309937
train gradient:  0.11324467324823786
iteration : 2209
train acc:  0.7265625
train loss:  0.5328512191772461
train gradient:  0.14018552381277471
iteration : 2210
train acc:  0.765625
train loss:  0.430997759103775
train gradient:  0.1043017820428831
iteration : 2211
train acc:  0.7109375
train loss:  0.4489312767982483
train gradient:  0.1059971907922075
iteration : 2212
train acc:  0.78125
train loss:  0.45340901613235474
train gradient:  0.10956794096400523
iteration : 2213
train acc:  0.8046875
train loss:  0.48268693685531616
train gradient:  0.10444487413016426
iteration : 2214
train acc:  0.75
train loss:  0.4807029366493225
train gradient:  0.113100550027158
iteration : 2215
train acc:  0.8046875
train loss:  0.4096665382385254
train gradient:  0.1115911297398153
iteration : 2216
train acc:  0.8046875
train loss:  0.47179317474365234
train gradient:  0.11162188858785725
iteration : 2217
train acc:  0.78125
train loss:  0.4445042610168457
train gradient:  0.09235776626833404
iteration : 2218
train acc:  0.71875
train loss:  0.5210916996002197
train gradient:  0.13013532451420018
iteration : 2219
train acc:  0.71875
train loss:  0.4640711545944214
train gradient:  0.1070241313236531
iteration : 2220
train acc:  0.765625
train loss:  0.4913727343082428
train gradient:  0.18989181843510533
iteration : 2221
train acc:  0.7265625
train loss:  0.4871138632297516
train gradient:  0.1193296800062764
iteration : 2222
train acc:  0.78125
train loss:  0.4608715772628784
train gradient:  0.09284203710472233
iteration : 2223
train acc:  0.7734375
train loss:  0.4713980555534363
train gradient:  0.12801723642363028
iteration : 2224
train acc:  0.6953125
train loss:  0.5032056570053101
train gradient:  0.1058537815325797
iteration : 2225
train acc:  0.78125
train loss:  0.5195566415786743
train gradient:  0.12750922809642562
iteration : 2226
train acc:  0.71875
train loss:  0.4984624981880188
train gradient:  0.12182564010880051
iteration : 2227
train acc:  0.796875
train loss:  0.4130379557609558
train gradient:  0.07284632664123954
iteration : 2228
train acc:  0.71875
train loss:  0.48270127177238464
train gradient:  0.09817283688559388
iteration : 2229
train acc:  0.71875
train loss:  0.4928668737411499
train gradient:  0.1707807863302393
iteration : 2230
train acc:  0.7578125
train loss:  0.48429709672927856
train gradient:  0.1367650217082364
iteration : 2231
train acc:  0.78125
train loss:  0.4181918799877167
train gradient:  0.0973952239065747
iteration : 2232
train acc:  0.7109375
train loss:  0.5536463856697083
train gradient:  0.163123266619618
iteration : 2233
train acc:  0.7421875
train loss:  0.5086215734481812
train gradient:  0.11071085994800653
iteration : 2234
train acc:  0.71875
train loss:  0.5250712037086487
train gradient:  0.12492350607074204
iteration : 2235
train acc:  0.7578125
train loss:  0.46559280157089233
train gradient:  0.09248019619706754
iteration : 2236
train acc:  0.7265625
train loss:  0.5097156763076782
train gradient:  0.13180265983613376
iteration : 2237
train acc:  0.7109375
train loss:  0.501144528388977
train gradient:  0.1595695145930891
iteration : 2238
train acc:  0.7421875
train loss:  0.5044565200805664
train gradient:  0.1450577599838715
iteration : 2239
train acc:  0.7421875
train loss:  0.4104071259498596
train gradient:  0.08538429297615713
iteration : 2240
train acc:  0.734375
train loss:  0.4712311923503876
train gradient:  0.11639640994306284
iteration : 2241
train acc:  0.765625
train loss:  0.45001092553138733
train gradient:  0.13054727149010797
iteration : 2242
train acc:  0.8046875
train loss:  0.4225536286830902
train gradient:  0.09010924549570645
iteration : 2243
train acc:  0.8046875
train loss:  0.431050568819046
train gradient:  0.11644070820017449
iteration : 2244
train acc:  0.7265625
train loss:  0.5062074661254883
train gradient:  0.14063515260507925
iteration : 2245
train acc:  0.75
train loss:  0.4685397148132324
train gradient:  0.11253643562067249
iteration : 2246
train acc:  0.796875
train loss:  0.40877917408943176
train gradient:  0.09431169206629214
iteration : 2247
train acc:  0.75
train loss:  0.49008721113204956
train gradient:  0.11745591737750312
iteration : 2248
train acc:  0.703125
train loss:  0.5582386255264282
train gradient:  0.14929042305152357
iteration : 2249
train acc:  0.7265625
train loss:  0.44293713569641113
train gradient:  0.1031386473129281
iteration : 2250
train acc:  0.7890625
train loss:  0.421642541885376
train gradient:  0.09185289842925241
iteration : 2251
train acc:  0.7734375
train loss:  0.5116562843322754
train gradient:  0.11206245388665653
iteration : 2252
train acc:  0.71875
train loss:  0.5358620882034302
train gradient:  0.17816127504732765
iteration : 2253
train acc:  0.75
train loss:  0.45671433210372925
train gradient:  0.1057690556641039
iteration : 2254
train acc:  0.7109375
train loss:  0.48817044496536255
train gradient:  0.15203401617035311
iteration : 2255
train acc:  0.75
train loss:  0.4849640130996704
train gradient:  0.11853028915596552
iteration : 2256
train acc:  0.8046875
train loss:  0.4362972676753998
train gradient:  0.1382227589984315
iteration : 2257
train acc:  0.796875
train loss:  0.49273183941841125
train gradient:  0.12709714710771286
iteration : 2258
train acc:  0.765625
train loss:  0.39523109793663025
train gradient:  0.1056521142620762
iteration : 2259
train acc:  0.75
train loss:  0.5102806091308594
train gradient:  0.15082408804025865
iteration : 2260
train acc:  0.8125
train loss:  0.44200730323791504
train gradient:  0.18835965477169198
iteration : 2261
train acc:  0.7109375
train loss:  0.5180918574333191
train gradient:  0.14338430026762478
iteration : 2262
train acc:  0.7890625
train loss:  0.45205533504486084
train gradient:  0.12034074504705006
iteration : 2263
train acc:  0.78125
train loss:  0.43775227665901184
train gradient:  0.10093104863730759
iteration : 2264
train acc:  0.6875
train loss:  0.516818642616272
train gradient:  0.1184000436457058
iteration : 2265
train acc:  0.7890625
train loss:  0.4534081816673279
train gradient:  0.10913421087772955
iteration : 2266
train acc:  0.734375
train loss:  0.47749799489974976
train gradient:  0.11943050103322472
iteration : 2267
train acc:  0.7578125
train loss:  0.527569591999054
train gradient:  0.1275477394583137
iteration : 2268
train acc:  0.765625
train loss:  0.42417484521865845
train gradient:  0.0987115748591299
iteration : 2269
train acc:  0.765625
train loss:  0.44485169649124146
train gradient:  0.08065696458978706
iteration : 2270
train acc:  0.7578125
train loss:  0.4687275290489197
train gradient:  0.11716653194041203
iteration : 2271
train acc:  0.78125
train loss:  0.4684961140155792
train gradient:  0.11369891984399284
iteration : 2272
train acc:  0.7265625
train loss:  0.4814577102661133
train gradient:  0.11526010203263605
iteration : 2273
train acc:  0.75
train loss:  0.5136716365814209
train gradient:  0.15047716901432756
iteration : 2274
train acc:  0.765625
train loss:  0.49785810708999634
train gradient:  0.13959410240476494
iteration : 2275
train acc:  0.75
train loss:  0.4619881510734558
train gradient:  0.10793677638220422
iteration : 2276
train acc:  0.7890625
train loss:  0.4651581645011902
train gradient:  0.1206006681736438
iteration : 2277
train acc:  0.71875
train loss:  0.4823794960975647
train gradient:  0.105308755168447
iteration : 2278
train acc:  0.7578125
train loss:  0.4587411880493164
train gradient:  0.12174891087585091
iteration : 2279
train acc:  0.734375
train loss:  0.513698399066925
train gradient:  0.13058716004147572
iteration : 2280
train acc:  0.7578125
train loss:  0.49417558312416077
train gradient:  0.15399719706204074
iteration : 2281
train acc:  0.7890625
train loss:  0.4304378032684326
train gradient:  0.11183904609992802
iteration : 2282
train acc:  0.71875
train loss:  0.5309512615203857
train gradient:  0.14092856510844276
iteration : 2283
train acc:  0.7578125
train loss:  0.4955737590789795
train gradient:  0.12757492094491724
iteration : 2284
train acc:  0.7421875
train loss:  0.477847158908844
train gradient:  0.10115838588328344
iteration : 2285
train acc:  0.71875
train loss:  0.573375403881073
train gradient:  0.17263120974325968
iteration : 2286
train acc:  0.75
train loss:  0.49060630798339844
train gradient:  0.11678887204229967
iteration : 2287
train acc:  0.7109375
train loss:  0.5404608249664307
train gradient:  0.1618472436372835
iteration : 2288
train acc:  0.7578125
train loss:  0.45782744884490967
train gradient:  0.11163822738517588
iteration : 2289
train acc:  0.7265625
train loss:  0.5550168752670288
train gradient:  0.18396485951455013
iteration : 2290
train acc:  0.71875
train loss:  0.5131492614746094
train gradient:  0.12500335599206838
iteration : 2291
train acc:  0.7734375
train loss:  0.4393385946750641
train gradient:  0.10489401389888645
iteration : 2292
train acc:  0.7421875
train loss:  0.5062413215637207
train gradient:  0.1101075024428596
iteration : 2293
train acc:  0.7734375
train loss:  0.4583089351654053
train gradient:  0.1195519716841685
iteration : 2294
train acc:  0.71875
train loss:  0.484650582075119
train gradient:  0.14001430528795178
iteration : 2295
train acc:  0.734375
train loss:  0.5011972188949585
train gradient:  0.1495091839087776
iteration : 2296
train acc:  0.71875
train loss:  0.5346722602844238
train gradient:  0.1646170616668821
iteration : 2297
train acc:  0.7421875
train loss:  0.48924943804740906
train gradient:  0.12250601929057568
iteration : 2298
train acc:  0.671875
train loss:  0.5607350468635559
train gradient:  0.15299586113393576
iteration : 2299
train acc:  0.796875
train loss:  0.41301214694976807
train gradient:  0.08731817250333082
iteration : 2300
train acc:  0.7734375
train loss:  0.4506172239780426
train gradient:  0.0939094192902059
iteration : 2301
train acc:  0.7578125
train loss:  0.4849146008491516
train gradient:  0.11680404470015629
iteration : 2302
train acc:  0.734375
train loss:  0.47126471996307373
train gradient:  0.1060444777252368
iteration : 2303
train acc:  0.7890625
train loss:  0.4951986074447632
train gradient:  0.12843769238183816
iteration : 2304
train acc:  0.7421875
train loss:  0.5287616848945618
train gradient:  0.11402469358282033
iteration : 2305
train acc:  0.8125
train loss:  0.4477117359638214
train gradient:  0.10064267760118825
iteration : 2306
train acc:  0.75
train loss:  0.484078973531723
train gradient:  0.11246577929772343
iteration : 2307
train acc:  0.7734375
train loss:  0.435164213180542
train gradient:  0.11215660684030843
iteration : 2308
train acc:  0.7109375
train loss:  0.5483207702636719
train gradient:  0.14336336468360772
iteration : 2309
train acc:  0.7734375
train loss:  0.43898606300354004
train gradient:  0.10425529317610448
iteration : 2310
train acc:  0.7109375
train loss:  0.5425264835357666
train gradient:  0.1357335310812356
iteration : 2311
train acc:  0.671875
train loss:  0.5434144735336304
train gradient:  0.1560391533196946
iteration : 2312
train acc:  0.8046875
train loss:  0.41119274497032166
train gradient:  0.08511466685659828
iteration : 2313
train acc:  0.6171875
train loss:  0.6591746807098389
train gradient:  0.21092926802022496
iteration : 2314
train acc:  0.7421875
train loss:  0.5713487267494202
train gradient:  0.13711718053842814
iteration : 2315
train acc:  0.8046875
train loss:  0.4358522295951843
train gradient:  0.08880916287522948
iteration : 2316
train acc:  0.875
train loss:  0.3956368565559387
train gradient:  0.08611731703513222
iteration : 2317
train acc:  0.71875
train loss:  0.544669508934021
train gradient:  0.15180185893350373
iteration : 2318
train acc:  0.7578125
train loss:  0.45646488666534424
train gradient:  0.12565960168947615
iteration : 2319
train acc:  0.7265625
train loss:  0.5148073434829712
train gradient:  0.1333488255750659
iteration : 2320
train acc:  0.7421875
train loss:  0.5770261883735657
train gradient:  0.1320111721935532
iteration : 2321
train acc:  0.71875
train loss:  0.5031423568725586
train gradient:  0.11333252127001368
iteration : 2322
train acc:  0.75
train loss:  0.5116357207298279
train gradient:  0.14024948098156123
iteration : 2323
train acc:  0.7421875
train loss:  0.43132781982421875
train gradient:  0.07916007798705764
iteration : 2324
train acc:  0.75
train loss:  0.5283153653144836
train gradient:  0.11724510488523392
iteration : 2325
train acc:  0.71875
train loss:  0.48376309871673584
train gradient:  0.12593208305345083
iteration : 2326
train acc:  0.6953125
train loss:  0.5951244831085205
train gradient:  0.1754985583632284
iteration : 2327
train acc:  0.7421875
train loss:  0.4727317690849304
train gradient:  0.09919347537538599
iteration : 2328
train acc:  0.78125
train loss:  0.44564390182495117
train gradient:  0.13737197732560144
iteration : 2329
train acc:  0.7109375
train loss:  0.5146164894104004
train gradient:  0.11970908970840362
iteration : 2330
train acc:  0.75
train loss:  0.5497596263885498
train gradient:  0.13246197705267257
iteration : 2331
train acc:  0.75
train loss:  0.528231680393219
train gradient:  0.131670454006674
iteration : 2332
train acc:  0.703125
train loss:  0.5495716333389282
train gradient:  0.14499191251807209
iteration : 2333
train acc:  0.75
train loss:  0.4635433554649353
train gradient:  0.09605248997651626
iteration : 2334
train acc:  0.71875
train loss:  0.5386959314346313
train gradient:  0.1258157873241912
iteration : 2335
train acc:  0.7890625
train loss:  0.46519505977630615
train gradient:  0.11920485276697254
iteration : 2336
train acc:  0.7265625
train loss:  0.4955592453479767
train gradient:  0.12880942595069828
iteration : 2337
train acc:  0.796875
train loss:  0.40179306268692017
train gradient:  0.08110176233443712
iteration : 2338
train acc:  0.796875
train loss:  0.4473446011543274
train gradient:  0.09925117973838972
iteration : 2339
train acc:  0.7421875
train loss:  0.4934893250465393
train gradient:  0.10948329108236794
iteration : 2340
train acc:  0.7578125
train loss:  0.4614257216453552
train gradient:  0.09822915258085058
iteration : 2341
train acc:  0.7421875
train loss:  0.5047909021377563
train gradient:  0.1416175884867329
iteration : 2342
train acc:  0.7734375
train loss:  0.4305708408355713
train gradient:  0.07987707111402517
iteration : 2343
train acc:  0.6953125
train loss:  0.48115241527557373
train gradient:  0.13567878320197393
iteration : 2344
train acc:  0.6953125
train loss:  0.5696958899497986
train gradient:  0.177826297053398
iteration : 2345
train acc:  0.8046875
train loss:  0.46340957283973694
train gradient:  0.11023943981957399
iteration : 2346
train acc:  0.734375
train loss:  0.5156300067901611
train gradient:  0.12326962361903562
iteration : 2347
train acc:  0.75
train loss:  0.4681547284126282
train gradient:  0.10513544385239447
iteration : 2348
train acc:  0.6796875
train loss:  0.5996429920196533
train gradient:  0.1580005604484873
iteration : 2349
train acc:  0.7890625
train loss:  0.45946136116981506
train gradient:  0.10958001072841977
iteration : 2350
train acc:  0.7109375
train loss:  0.523226261138916
train gradient:  0.11922695496744394
iteration : 2351
train acc:  0.7578125
train loss:  0.5515131950378418
train gradient:  0.1491975454854127
iteration : 2352
train acc:  0.75
train loss:  0.48368173837661743
train gradient:  0.1269567621454512
iteration : 2353
train acc:  0.78125
train loss:  0.495317667722702
train gradient:  0.13420096402656306
iteration : 2354
train acc:  0.7265625
train loss:  0.5033596158027649
train gradient:  0.10883094774890656
iteration : 2355
train acc:  0.7265625
train loss:  0.5842475891113281
train gradient:  0.1935440516143186
iteration : 2356
train acc:  0.765625
train loss:  0.5205948352813721
train gradient:  0.13453502823031077
iteration : 2357
train acc:  0.734375
train loss:  0.4479241371154785
train gradient:  0.11671080926808586
iteration : 2358
train acc:  0.7578125
train loss:  0.49611860513687134
train gradient:  0.1291617539507414
iteration : 2359
train acc:  0.8203125
train loss:  0.4288843870162964
train gradient:  0.11465968244905413
iteration : 2360
train acc:  0.78125
train loss:  0.48709362745285034
train gradient:  0.10822802727966335
iteration : 2361
train acc:  0.7890625
train loss:  0.43970948457717896
train gradient:  0.0904417608473324
iteration : 2362
train acc:  0.765625
train loss:  0.43245208263397217
train gradient:  0.08114727768468041
iteration : 2363
train acc:  0.78125
train loss:  0.447873055934906
train gradient:  0.10641167614524448
iteration : 2364
train acc:  0.7265625
train loss:  0.5012229084968567
train gradient:  0.14319488320816506
iteration : 2365
train acc:  0.78125
train loss:  0.4193788170814514
train gradient:  0.11195285651605849
iteration : 2366
train acc:  0.7421875
train loss:  0.4972388744354248
train gradient:  0.10027584322625875
iteration : 2367
train acc:  0.7265625
train loss:  0.505255401134491
train gradient:  0.12330092074666796
iteration : 2368
train acc:  0.8046875
train loss:  0.42000100016593933
train gradient:  0.08626331506570464
iteration : 2369
train acc:  0.7734375
train loss:  0.4185786843299866
train gradient:  0.08838934853791341
iteration : 2370
train acc:  0.7578125
train loss:  0.4575869143009186
train gradient:  0.11001659300705534
iteration : 2371
train acc:  0.78125
train loss:  0.46261921525001526
train gradient:  0.12642974374773483
iteration : 2372
train acc:  0.7578125
train loss:  0.47128042578697205
train gradient:  0.11520554747601709
iteration : 2373
train acc:  0.703125
train loss:  0.567776083946228
train gradient:  0.14365885069545442
iteration : 2374
train acc:  0.6875
train loss:  0.517796516418457
train gradient:  0.14039884542610143
iteration : 2375
train acc:  0.71875
train loss:  0.49424082040786743
train gradient:  0.11276858693032986
iteration : 2376
train acc:  0.6953125
train loss:  0.6165056824684143
train gradient:  0.1637958204715409
iteration : 2377
train acc:  0.75
train loss:  0.46433520317077637
train gradient:  0.10213541423107025
iteration : 2378
train acc:  0.75
train loss:  0.46339213848114014
train gradient:  0.10319837552974258
iteration : 2379
train acc:  0.796875
train loss:  0.4375447630882263
train gradient:  0.08710178402474635
iteration : 2380
train acc:  0.71875
train loss:  0.5558048486709595
train gradient:  0.17662451493635345
iteration : 2381
train acc:  0.75
train loss:  0.46031659841537476
train gradient:  0.08759669728136642
iteration : 2382
train acc:  0.78125
train loss:  0.44442588090896606
train gradient:  0.10655271995922706
iteration : 2383
train acc:  0.7734375
train loss:  0.45563292503356934
train gradient:  0.10547915689844632
iteration : 2384
train acc:  0.7578125
train loss:  0.46523916721343994
train gradient:  0.0847747881560783
iteration : 2385
train acc:  0.75
train loss:  0.49639421701431274
train gradient:  0.13815862411462554
iteration : 2386
train acc:  0.78125
train loss:  0.42598575353622437
train gradient:  0.09852632465569316
iteration : 2387
train acc:  0.78125
train loss:  0.4444981515407562
train gradient:  0.0932201105160594
iteration : 2388
train acc:  0.734375
train loss:  0.457235187292099
train gradient:  0.12089982401219293
iteration : 2389
train acc:  0.7421875
train loss:  0.4685850143432617
train gradient:  0.0980220761848962
iteration : 2390
train acc:  0.6484375
train loss:  0.58579421043396
train gradient:  0.1611318359294558
iteration : 2391
train acc:  0.7578125
train loss:  0.4981898069381714
train gradient:  0.1209329400022508
iteration : 2392
train acc:  0.78125
train loss:  0.4041152596473694
train gradient:  0.0841443456421306
iteration : 2393
train acc:  0.78125
train loss:  0.4503400921821594
train gradient:  0.08355517753150131
iteration : 2394
train acc:  0.6875
train loss:  0.5525035858154297
train gradient:  0.18240265796271898
iteration : 2395
train acc:  0.734375
train loss:  0.4886056184768677
train gradient:  0.1021493904237461
iteration : 2396
train acc:  0.7265625
train loss:  0.4618912935256958
train gradient:  0.10060761901614312
iteration : 2397
train acc:  0.75
train loss:  0.45169055461883545
train gradient:  0.12958321050406324
iteration : 2398
train acc:  0.8046875
train loss:  0.43076202273368835
train gradient:  0.0863521658862801
iteration : 2399
train acc:  0.8046875
train loss:  0.4138970971107483
train gradient:  0.09507108457180591
iteration : 2400
train acc:  0.7890625
train loss:  0.47098425030708313
train gradient:  0.10803306627036521
iteration : 2401
train acc:  0.734375
train loss:  0.493746817111969
train gradient:  0.11577776448989952
iteration : 2402
train acc:  0.7890625
train loss:  0.49737831950187683
train gradient:  0.1324476901240842
iteration : 2403
train acc:  0.7578125
train loss:  0.5910770893096924
train gradient:  0.20311550283646304
iteration : 2404
train acc:  0.796875
train loss:  0.43454545736312866
train gradient:  0.11102493579775113
iteration : 2405
train acc:  0.8046875
train loss:  0.4262785315513611
train gradient:  0.1041857653696078
iteration : 2406
train acc:  0.75
train loss:  0.47163277864456177
train gradient:  0.11960701884666719
iteration : 2407
train acc:  0.7265625
train loss:  0.49397680163383484
train gradient:  0.12580300506245468
iteration : 2408
train acc:  0.703125
train loss:  0.5358144044876099
train gradient:  0.17923765759416602
iteration : 2409
train acc:  0.765625
train loss:  0.45369672775268555
train gradient:  0.10223599244348502
iteration : 2410
train acc:  0.796875
train loss:  0.46795907616615295
train gradient:  0.1230524840075006
iteration : 2411
train acc:  0.7421875
train loss:  0.49059802293777466
train gradient:  0.10178356376223312
iteration : 2412
train acc:  0.71875
train loss:  0.5123065710067749
train gradient:  0.13575117139209153
iteration : 2413
train acc:  0.765625
train loss:  0.4888286590576172
train gradient:  0.13221404226173647
iteration : 2414
train acc:  0.7265625
train loss:  0.5187249183654785
train gradient:  0.13258945069458739
iteration : 2415
train acc:  0.7734375
train loss:  0.47764450311660767
train gradient:  0.10012962787450316
iteration : 2416
train acc:  0.7265625
train loss:  0.46920812129974365
train gradient:  0.12581328638968403
iteration : 2417
train acc:  0.7265625
train loss:  0.49001678824424744
train gradient:  0.10666974278974056
iteration : 2418
train acc:  0.765625
train loss:  0.4556528329849243
train gradient:  0.09845589318165739
iteration : 2419
train acc:  0.703125
train loss:  0.5023022890090942
train gradient:  0.12489005053518332
iteration : 2420
train acc:  0.734375
train loss:  0.4658154845237732
train gradient:  0.12012468832281324
iteration : 2421
train acc:  0.7421875
train loss:  0.48297688364982605
train gradient:  0.13112935932718428
iteration : 2422
train acc:  0.78125
train loss:  0.4609958827495575
train gradient:  0.11089595345840797
iteration : 2423
train acc:  0.75
train loss:  0.4536508023738861
train gradient:  0.11025352907936962
iteration : 2424
train acc:  0.734375
train loss:  0.5260952711105347
train gradient:  0.11529188499217113
iteration : 2425
train acc:  0.7734375
train loss:  0.41605567932128906
train gradient:  0.0889373123732572
iteration : 2426
train acc:  0.7109375
train loss:  0.5373765230178833
train gradient:  0.11338108715267183
iteration : 2427
train acc:  0.703125
train loss:  0.48699674010276794
train gradient:  0.11897755865617221
iteration : 2428
train acc:  0.78125
train loss:  0.485605388879776
train gradient:  0.09630849824073426
iteration : 2429
train acc:  0.6875
train loss:  0.5400108098983765
train gradient:  0.13513500279052032
iteration : 2430
train acc:  0.7734375
train loss:  0.4435567259788513
train gradient:  0.09397076673026603
iteration : 2431
train acc:  0.7109375
train loss:  0.49544864892959595
train gradient:  0.13976593989952013
iteration : 2432
train acc:  0.7890625
train loss:  0.4213431775569916
train gradient:  0.0985047276947704
iteration : 2433
train acc:  0.7734375
train loss:  0.4859614670276642
train gradient:  0.10695642390786662
iteration : 2434
train acc:  0.71875
train loss:  0.4788496494293213
train gradient:  0.10234009695504027
iteration : 2435
train acc:  0.78125
train loss:  0.450888067483902
train gradient:  0.10718207416540435
iteration : 2436
train acc:  0.78125
train loss:  0.452306866645813
train gradient:  0.12409132419091298
iteration : 2437
train acc:  0.6875
train loss:  0.5438116788864136
train gradient:  0.12165392601023785
iteration : 2438
train acc:  0.6796875
train loss:  0.5536564588546753
train gradient:  0.13319729350678716
iteration : 2439
train acc:  0.7734375
train loss:  0.4818384051322937
train gradient:  0.1421144273125709
iteration : 2440
train acc:  0.7578125
train loss:  0.4308009147644043
train gradient:  0.10958548010373155
iteration : 2441
train acc:  0.7421875
train loss:  0.49581781029701233
train gradient:  0.14083605126007964
iteration : 2442
train acc:  0.7109375
train loss:  0.5101376175880432
train gradient:  0.09986711004272512
iteration : 2443
train acc:  0.7890625
train loss:  0.4600713849067688
train gradient:  0.11298414544826106
iteration : 2444
train acc:  0.7421875
train loss:  0.48009181022644043
train gradient:  0.10438839235144329
iteration : 2445
train acc:  0.7265625
train loss:  0.47504639625549316
train gradient:  0.12100026204573418
iteration : 2446
train acc:  0.7890625
train loss:  0.4233570098876953
train gradient:  0.08461122037298781
iteration : 2447
train acc:  0.7734375
train loss:  0.4866083264350891
train gradient:  0.12651126753137348
iteration : 2448
train acc:  0.75
train loss:  0.4702445864677429
train gradient:  0.09382150255057892
iteration : 2449
train acc:  0.8046875
train loss:  0.510469377040863
train gradient:  0.12274753871651906
iteration : 2450
train acc:  0.7265625
train loss:  0.4950435161590576
train gradient:  0.1283613807280266
iteration : 2451
train acc:  0.6875
train loss:  0.5321519374847412
train gradient:  0.1735136144605166
iteration : 2452
train acc:  0.7734375
train loss:  0.39203038811683655
train gradient:  0.07926675048797041
iteration : 2453
train acc:  0.734375
train loss:  0.5616318583488464
train gradient:  0.15699427954184658
iteration : 2454
train acc:  0.6953125
train loss:  0.49356940388679504
train gradient:  0.11585700216835736
iteration : 2455
train acc:  0.7109375
train loss:  0.5123637914657593
train gradient:  0.11187969988627496
iteration : 2456
train acc:  0.6953125
train loss:  0.5102641582489014
train gradient:  0.1339443148843052
iteration : 2457
train acc:  0.859375
train loss:  0.38835853338241577
train gradient:  0.07890375277863781
iteration : 2458
train acc:  0.7578125
train loss:  0.47434201836586
train gradient:  0.12341397431445501
iteration : 2459
train acc:  0.7578125
train loss:  0.4847235679626465
train gradient:  0.10269390211737574
iteration : 2460
train acc:  0.8046875
train loss:  0.43362462520599365
train gradient:  0.09926169076976048
iteration : 2461
train acc:  0.7265625
train loss:  0.42826181650161743
train gradient:  0.09444023695751187
iteration : 2462
train acc:  0.7265625
train loss:  0.5073058009147644
train gradient:  0.12165100909478764
iteration : 2463
train acc:  0.7265625
train loss:  0.5131973028182983
train gradient:  0.1566408549980155
iteration : 2464
train acc:  0.734375
train loss:  0.5277566909790039
train gradient:  0.1283709793093405
iteration : 2465
train acc:  0.7578125
train loss:  0.4573351740837097
train gradient:  0.13119207333551547
iteration : 2466
train acc:  0.7265625
train loss:  0.5106323957443237
train gradient:  0.12977071367465218
iteration : 2467
train acc:  0.78125
train loss:  0.4214058220386505
train gradient:  0.09516947249594312
iteration : 2468
train acc:  0.71875
train loss:  0.5137428045272827
train gradient:  0.12857770844233402
iteration : 2469
train acc:  0.7421875
train loss:  0.45781761407852173
train gradient:  0.14562130127513687
iteration : 2470
train acc:  0.8046875
train loss:  0.4324382245540619
train gradient:  0.0998891590836246
iteration : 2471
train acc:  0.734375
train loss:  0.5598043203353882
train gradient:  0.16143157681168127
iteration : 2472
train acc:  0.75
train loss:  0.48108214139938354
train gradient:  0.11213915919519982
iteration : 2473
train acc:  0.7265625
train loss:  0.5246943235397339
train gradient:  0.12575235705397514
iteration : 2474
train acc:  0.7890625
train loss:  0.4075801968574524
train gradient:  0.08742061327727525
iteration : 2475
train acc:  0.7421875
train loss:  0.5692481994628906
train gradient:  0.18638799336054346
iteration : 2476
train acc:  0.765625
train loss:  0.44688180088996887
train gradient:  0.10633615989406241
iteration : 2477
train acc:  0.71875
train loss:  0.4811466932296753
train gradient:  0.13542628605718732
iteration : 2478
train acc:  0.734375
train loss:  0.46911072731018066
train gradient:  0.11052927294377539
iteration : 2479
train acc:  0.8125
train loss:  0.4077482521533966
train gradient:  0.1145333269429937
iteration : 2480
train acc:  0.7734375
train loss:  0.43471789360046387
train gradient:  0.10623487327649521
iteration : 2481
train acc:  0.78125
train loss:  0.4971936345100403
train gradient:  0.15651088061353857
iteration : 2482
train acc:  0.7734375
train loss:  0.45626959204673767
train gradient:  0.12101651837615444
iteration : 2483
train acc:  0.6953125
train loss:  0.5274181365966797
train gradient:  0.12264653321356085
iteration : 2484
train acc:  0.7265625
train loss:  0.5529896020889282
train gradient:  0.176629771162107
iteration : 2485
train acc:  0.703125
train loss:  0.520376443862915
train gradient:  0.13666316008502544
iteration : 2486
train acc:  0.8203125
train loss:  0.39156946539878845
train gradient:  0.08573886780135326
iteration : 2487
train acc:  0.78125
train loss:  0.47739893198013306
train gradient:  0.10579459061191272
iteration : 2488
train acc:  0.7734375
train loss:  0.4798620939254761
train gradient:  0.11410205420835111
iteration : 2489
train acc:  0.8203125
train loss:  0.4676862955093384
train gradient:  0.10110092502110801
iteration : 2490
train acc:  0.75
train loss:  0.45998072624206543
train gradient:  0.09407824483247185
iteration : 2491
train acc:  0.7421875
train loss:  0.527739942073822
train gradient:  0.1631772114771633
iteration : 2492
train acc:  0.734375
train loss:  0.4820857346057892
train gradient:  0.0951561218086365
iteration : 2493
train acc:  0.75
train loss:  0.462223619222641
train gradient:  0.12738196880866734
iteration : 2494
train acc:  0.859375
train loss:  0.41067931056022644
train gradient:  0.09796137344266094
iteration : 2495
train acc:  0.7734375
train loss:  0.4749651849269867
train gradient:  0.13039284258597933
iteration : 2496
train acc:  0.7578125
train loss:  0.4584228992462158
train gradient:  0.0933232551482104
iteration : 2497
train acc:  0.7421875
train loss:  0.4908735156059265
train gradient:  0.1103478208770491
iteration : 2498
train acc:  0.7890625
train loss:  0.4734272360801697
train gradient:  0.11322289094265889
iteration : 2499
train acc:  0.7578125
train loss:  0.5408921241760254
train gradient:  0.14785219775084119
iteration : 2500
train acc:  0.734375
train loss:  0.47411882877349854
train gradient:  0.10197287402671881
iteration : 2501
train acc:  0.8125
train loss:  0.41645681858062744
train gradient:  0.1255267393582915
iteration : 2502
train acc:  0.7734375
train loss:  0.4837762415409088
train gradient:  0.11463182655761775
iteration : 2503
train acc:  0.71875
train loss:  0.5138477087020874
train gradient:  0.12057463179145593
iteration : 2504
train acc:  0.7109375
train loss:  0.5488905310630798
train gradient:  0.16031668625423312
iteration : 2505
train acc:  0.7421875
train loss:  0.49700450897216797
train gradient:  0.1506206301675473
iteration : 2506
train acc:  0.734375
train loss:  0.5416862368583679
train gradient:  0.1253219539290295
iteration : 2507
train acc:  0.71875
train loss:  0.5079860687255859
train gradient:  0.13641046571587284
iteration : 2508
train acc:  0.71875
train loss:  0.5162164568901062
train gradient:  0.1326085193162793
iteration : 2509
train acc:  0.6953125
train loss:  0.5470765233039856
train gradient:  0.13047587844451072
iteration : 2510
train acc:  0.6953125
train loss:  0.5809531211853027
train gradient:  0.16142536661585635
iteration : 2511
train acc:  0.7578125
train loss:  0.4626261591911316
train gradient:  0.1101719506990528
iteration : 2512
train acc:  0.7421875
train loss:  0.5120956897735596
train gradient:  0.13201598805657472
iteration : 2513
train acc:  0.796875
train loss:  0.38976404070854187
train gradient:  0.09220730895179867
iteration : 2514
train acc:  0.796875
train loss:  0.4277459383010864
train gradient:  0.08651123411838632
iteration : 2515
train acc:  0.6953125
train loss:  0.5638920068740845
train gradient:  0.14011325691070925
iteration : 2516
train acc:  0.71875
train loss:  0.5182659029960632
train gradient:  0.13534417357735912
iteration : 2517
train acc:  0.75
train loss:  0.4438990354537964
train gradient:  0.08891452555514648
iteration : 2518
train acc:  0.7734375
train loss:  0.48912203311920166
train gradient:  0.12455406917310148
iteration : 2519
train acc:  0.75
train loss:  0.4577588438987732
train gradient:  0.10245430402612785
iteration : 2520
train acc:  0.78125
train loss:  0.4420211911201477
train gradient:  0.11832176626455938
iteration : 2521
train acc:  0.640625
train loss:  0.5540571212768555
train gradient:  0.1305793585593714
iteration : 2522
train acc:  0.796875
train loss:  0.4079641103744507
train gradient:  0.09911096398011286
iteration : 2523
train acc:  0.7578125
train loss:  0.499106764793396
train gradient:  0.12663405121201154
iteration : 2524
train acc:  0.7578125
train loss:  0.428127646446228
train gradient:  0.11932900505429679
iteration : 2525
train acc:  0.734375
train loss:  0.5316060185432434
train gradient:  0.15217206308469924
iteration : 2526
train acc:  0.7109375
train loss:  0.5054048895835876
train gradient:  0.11993211609840619
iteration : 2527
train acc:  0.7734375
train loss:  0.45920687913894653
train gradient:  0.11699670630674902
iteration : 2528
train acc:  0.765625
train loss:  0.4935677647590637
train gradient:  0.15988915388184555
iteration : 2529
train acc:  0.7265625
train loss:  0.4514639377593994
train gradient:  0.09771013727915515
iteration : 2530
train acc:  0.7578125
train loss:  0.4832639694213867
train gradient:  0.09598517818760297
iteration : 2531
train acc:  0.859375
train loss:  0.37870556116104126
train gradient:  0.06714479409457762
iteration : 2532
train acc:  0.7421875
train loss:  0.509727418422699
train gradient:  0.13027760819285256
iteration : 2533
train acc:  0.828125
train loss:  0.3998696208000183
train gradient:  0.07955833802564702
iteration : 2534
train acc:  0.796875
train loss:  0.4535355567932129
train gradient:  0.10292139968076944
iteration : 2535
train acc:  0.75
train loss:  0.48465901613235474
train gradient:  0.12486427066449256
iteration : 2536
train acc:  0.7421875
train loss:  0.45761311054229736
train gradient:  0.10950801012149408
iteration : 2537
train acc:  0.734375
train loss:  0.4811391532421112
train gradient:  0.1419617569226841
iteration : 2538
train acc:  0.765625
train loss:  0.4795040786266327
train gradient:  0.10782922730656837
iteration : 2539
train acc:  0.765625
train loss:  0.5137534141540527
train gradient:  0.12619793079858888
iteration : 2540
train acc:  0.765625
train loss:  0.4130343496799469
train gradient:  0.10787963248156288
iteration : 2541
train acc:  0.78125
train loss:  0.4864073097705841
train gradient:  0.10084480868970176
iteration : 2542
train acc:  0.765625
train loss:  0.5164484977722168
train gradient:  0.1374293535476481
iteration : 2543
train acc:  0.75
train loss:  0.44336768984794617
train gradient:  0.0975966002551188
iteration : 2544
train acc:  0.7578125
train loss:  0.4717062711715698
train gradient:  0.12087162709835643
iteration : 2545
train acc:  0.71875
train loss:  0.5475263595581055
train gradient:  0.14962962991267004
iteration : 2546
train acc:  0.8203125
train loss:  0.4223690629005432
train gradient:  0.11116764470691337
iteration : 2547
train acc:  0.7265625
train loss:  0.5270509719848633
train gradient:  0.15737633822876934
iteration : 2548
train acc:  0.734375
train loss:  0.48554521799087524
train gradient:  0.09911763066446443
iteration : 2549
train acc:  0.734375
train loss:  0.48886996507644653
train gradient:  0.13125119125169124
iteration : 2550
train acc:  0.7109375
train loss:  0.48134562373161316
train gradient:  0.15073575114322196
iteration : 2551
train acc:  0.7734375
train loss:  0.4382480978965759
train gradient:  0.08666987002864102
iteration : 2552
train acc:  0.7109375
train loss:  0.5155825018882751
train gradient:  0.14290572342436714
iteration : 2553
train acc:  0.7421875
train loss:  0.5221849679946899
train gradient:  0.14829172584991152
iteration : 2554
train acc:  0.7578125
train loss:  0.4844452142715454
train gradient:  0.14046540908230548
iteration : 2555
train acc:  0.7734375
train loss:  0.47499412298202515
train gradient:  0.11251431742509053
iteration : 2556
train acc:  0.7578125
train loss:  0.45475125312805176
train gradient:  0.1070603133659445
iteration : 2557
train acc:  0.7578125
train loss:  0.43663743138313293
train gradient:  0.09926357434002334
iteration : 2558
train acc:  0.78125
train loss:  0.444560170173645
train gradient:  0.11259185817577437
iteration : 2559
train acc:  0.7421875
train loss:  0.5440323948860168
train gradient:  0.1842703593360862
iteration : 2560
train acc:  0.734375
train loss:  0.505962610244751
train gradient:  0.14143574244658383
iteration : 2561
train acc:  0.6953125
train loss:  0.5571607351303101
train gradient:  0.1764585558219794
iteration : 2562
train acc:  0.8125
train loss:  0.41113537549972534
train gradient:  0.08471344841954297
iteration : 2563
train acc:  0.765625
train loss:  0.4619976580142975
train gradient:  0.12675215909553128
iteration : 2564
train acc:  0.765625
train loss:  0.47708746790885925
train gradient:  0.13540469516550385
iteration : 2565
train acc:  0.7890625
train loss:  0.45461025834083557
train gradient:  0.12207376387810868
iteration : 2566
train acc:  0.7890625
train loss:  0.4188242256641388
train gradient:  0.12260344387860231
iteration : 2567
train acc:  0.8046875
train loss:  0.44864964485168457
train gradient:  0.10343991750590524
iteration : 2568
train acc:  0.75
train loss:  0.48776447772979736
train gradient:  0.13183362553228262
iteration : 2569
train acc:  0.6875
train loss:  0.5123657584190369
train gradient:  0.13632136424727856
iteration : 2570
train acc:  0.7265625
train loss:  0.5114363431930542
train gradient:  0.13652881438803713
iteration : 2571
train acc:  0.6875
train loss:  0.6007930040359497
train gradient:  0.1748217330281735
iteration : 2572
train acc:  0.7265625
train loss:  0.49272894859313965
train gradient:  0.1345050152365446
iteration : 2573
train acc:  0.7734375
train loss:  0.46301567554473877
train gradient:  0.13781539626219064
iteration : 2574
train acc:  0.765625
train loss:  0.4610867500305176
train gradient:  0.12081027950923093
iteration : 2575
train acc:  0.765625
train loss:  0.44206175208091736
train gradient:  0.09973026997589722
iteration : 2576
train acc:  0.7265625
train loss:  0.520482063293457
train gradient:  0.12818948244927209
iteration : 2577
train acc:  0.671875
train loss:  0.5240790843963623
train gradient:  0.12516295105041012
iteration : 2578
train acc:  0.625
train loss:  0.5479528307914734
train gradient:  0.13462297320652686
iteration : 2579
train acc:  0.7109375
train loss:  0.47442692518234253
train gradient:  0.11754192525227491
iteration : 2580
train acc:  0.7421875
train loss:  0.45766890048980713
train gradient:  0.12040391957362968
iteration : 2581
train acc:  0.7890625
train loss:  0.44124674797058105
train gradient:  0.12087919889087004
iteration : 2582
train acc:  0.7890625
train loss:  0.40555936098098755
train gradient:  0.09616003544475366
iteration : 2583
train acc:  0.7265625
train loss:  0.4793786406517029
train gradient:  0.13483700844037266
iteration : 2584
train acc:  0.7578125
train loss:  0.45257270336151123
train gradient:  0.09160677598691006
iteration : 2585
train acc:  0.7734375
train loss:  0.4575643539428711
train gradient:  0.12438738416774643
iteration : 2586
train acc:  0.703125
train loss:  0.4664422571659088
train gradient:  0.1669682938269742
iteration : 2587
train acc:  0.703125
train loss:  0.567244291305542
train gradient:  0.1776005608501629
iteration : 2588
train acc:  0.7734375
train loss:  0.41266632080078125
train gradient:  0.09429875893534027
iteration : 2589
train acc:  0.7890625
train loss:  0.4142916798591614
train gradient:  0.08048616650136231
iteration : 2590
train acc:  0.7890625
train loss:  0.48237818479537964
train gradient:  0.1218922823571082
iteration : 2591
train acc:  0.765625
train loss:  0.4392353296279907
train gradient:  0.1268228267508668
iteration : 2592
train acc:  0.7421875
train loss:  0.49548906087875366
train gradient:  0.12647747936196357
iteration : 2593
train acc:  0.71875
train loss:  0.49040576815605164
train gradient:  0.12205221873342291
iteration : 2594
train acc:  0.7421875
train loss:  0.47753918170928955
train gradient:  0.11888533004089553
iteration : 2595
train acc:  0.75
train loss:  0.49147745966911316
train gradient:  0.11981639424772171
iteration : 2596
train acc:  0.734375
train loss:  0.4991910457611084
train gradient:  0.11282281544935158
iteration : 2597
train acc:  0.78125
train loss:  0.45861271023750305
train gradient:  0.12364167209635984
iteration : 2598
train acc:  0.7578125
train loss:  0.4386752247810364
train gradient:  0.10407174308034593
iteration : 2599
train acc:  0.765625
train loss:  0.4558402895927429
train gradient:  0.1343034805659321
iteration : 2600
train acc:  0.71875
train loss:  0.5023419857025146
train gradient:  0.11892243209317704
iteration : 2601
train acc:  0.703125
train loss:  0.5108053684234619
train gradient:  0.14687026435749273
iteration : 2602
train acc:  0.78125
train loss:  0.40665900707244873
train gradient:  0.08019748536335473
iteration : 2603
train acc:  0.6796875
train loss:  0.5200549960136414
train gradient:  0.15689659967379146
iteration : 2604
train acc:  0.8046875
train loss:  0.42841464281082153
train gradient:  0.10897776278047451
iteration : 2605
train acc:  0.7265625
train loss:  0.5167890787124634
train gradient:  0.12789241137052307
iteration : 2606
train acc:  0.7265625
train loss:  0.5355470180511475
train gradient:  0.12951641747836412
iteration : 2607
train acc:  0.7109375
train loss:  0.5443081855773926
train gradient:  0.14049395726919034
iteration : 2608
train acc:  0.765625
train loss:  0.4738062024116516
train gradient:  0.13942129080287224
iteration : 2609
train acc:  0.734375
train loss:  0.502483606338501
train gradient:  0.1453993394516681
iteration : 2610
train acc:  0.7890625
train loss:  0.48542851209640503
train gradient:  0.10653015775689176
iteration : 2611
train acc:  0.8671875
train loss:  0.36269623041152954
train gradient:  0.06469661500565908
iteration : 2612
train acc:  0.78125
train loss:  0.4499781131744385
train gradient:  0.0942594178291165
iteration : 2613
train acc:  0.796875
train loss:  0.39557909965515137
train gradient:  0.08677259103521925
iteration : 2614
train acc:  0.71875
train loss:  0.5172218084335327
train gradient:  0.14613692710361953
iteration : 2615
train acc:  0.71875
train loss:  0.4885331392288208
train gradient:  0.11923718346496473
iteration : 2616
train acc:  0.703125
train loss:  0.48582571744918823
train gradient:  0.13564387701000968
iteration : 2617
train acc:  0.7890625
train loss:  0.4335513710975647
train gradient:  0.11595633927606196
iteration : 2618
train acc:  0.765625
train loss:  0.4383009970188141
train gradient:  0.13059115519930198
iteration : 2619
train acc:  0.7421875
train loss:  0.4992053508758545
train gradient:  0.12242801684805425
iteration : 2620
train acc:  0.7578125
train loss:  0.4464162588119507
train gradient:  0.09913514798223153
iteration : 2621
train acc:  0.7578125
train loss:  0.4833276867866516
train gradient:  0.11529288493679218
iteration : 2622
train acc:  0.7265625
train loss:  0.511562168598175
train gradient:  0.1468197213195313
iteration : 2623
train acc:  0.8046875
train loss:  0.4301201403141022
train gradient:  0.09564698887323649
iteration : 2624
train acc:  0.734375
train loss:  0.5143774747848511
train gradient:  0.12485390136643962
iteration : 2625
train acc:  0.6875
train loss:  0.5564484000205994
train gradient:  0.12129851144049669
iteration : 2626
train acc:  0.75
train loss:  0.46761077642440796
train gradient:  0.11924848340953952
iteration : 2627
train acc:  0.71875
train loss:  0.4930311441421509
train gradient:  0.13075059966659403
iteration : 2628
train acc:  0.765625
train loss:  0.5081825852394104
train gradient:  0.1859423115768021
iteration : 2629
train acc:  0.7578125
train loss:  0.48873770236968994
train gradient:  0.15586226436467837
iteration : 2630
train acc:  0.78125
train loss:  0.525765061378479
train gradient:  0.15221155224259608
iteration : 2631
train acc:  0.7109375
train loss:  0.5593815445899963
train gradient:  0.15001183455947695
iteration : 2632
train acc:  0.8046875
train loss:  0.43081822991371155
train gradient:  0.11109092155943068
iteration : 2633
train acc:  0.78125
train loss:  0.4878358840942383
train gradient:  0.13120617101497573
iteration : 2634
train acc:  0.734375
train loss:  0.47296714782714844
train gradient:  0.12423561714599966
iteration : 2635
train acc:  0.7578125
train loss:  0.48756974935531616
train gradient:  0.11993487943291363
iteration : 2636
train acc:  0.75
train loss:  0.515419065952301
train gradient:  0.1506968095824236
iteration : 2637
train acc:  0.7734375
train loss:  0.44709402322769165
train gradient:  0.10654210443229056
iteration : 2638
train acc:  0.734375
train loss:  0.5130285620689392
train gradient:  0.16727938780872442
iteration : 2639
train acc:  0.6796875
train loss:  0.5264170169830322
train gradient:  0.15293754711275614
iteration : 2640
train acc:  0.765625
train loss:  0.47365185618400574
train gradient:  0.16331237843395743
iteration : 2641
train acc:  0.796875
train loss:  0.4197707772254944
train gradient:  0.09951099233226561
iteration : 2642
train acc:  0.6796875
train loss:  0.5753411054611206
train gradient:  0.18773191003762796
iteration : 2643
train acc:  0.765625
train loss:  0.47208553552627563
train gradient:  0.11554033307106731
iteration : 2644
train acc:  0.765625
train loss:  0.4984143376350403
train gradient:  0.14364522614467193
iteration : 2645
train acc:  0.75
train loss:  0.4484418034553528
train gradient:  0.08509503594228986
iteration : 2646
train acc:  0.71875
train loss:  0.5165463089942932
train gradient:  0.1522246024307995
iteration : 2647
train acc:  0.7890625
train loss:  0.4254046082496643
train gradient:  0.1076712255083282
iteration : 2648
train acc:  0.7265625
train loss:  0.4645867347717285
train gradient:  0.09710876720861711
iteration : 2649
train acc:  0.75
train loss:  0.4846714735031128
train gradient:  0.11831199879734185
iteration : 2650
train acc:  0.75
train loss:  0.4612065851688385
train gradient:  0.1375888648725026
iteration : 2651
train acc:  0.7578125
train loss:  0.4922608733177185
train gradient:  0.15555516126989027
iteration : 2652
train acc:  0.765625
train loss:  0.5133568048477173
train gradient:  0.13824019227719545
iteration : 2653
train acc:  0.734375
train loss:  0.5054558515548706
train gradient:  0.1522318920020286
iteration : 2654
train acc:  0.7734375
train loss:  0.44231536984443665
train gradient:  0.08452218591379529
iteration : 2655
train acc:  0.8125
train loss:  0.45369085669517517
train gradient:  0.09541989568879491
iteration : 2656
train acc:  0.7421875
train loss:  0.5302721858024597
train gradient:  0.13138340264102594
iteration : 2657
train acc:  0.7578125
train loss:  0.47835928201675415
train gradient:  0.12771950385948644
iteration : 2658
train acc:  0.734375
train loss:  0.4985761046409607
train gradient:  0.13023577928615104
iteration : 2659
train acc:  0.71875
train loss:  0.5394235849380493
train gradient:  0.13059795757126635
iteration : 2660
train acc:  0.78125
train loss:  0.43072229623794556
train gradient:  0.08118595830171993
iteration : 2661
train acc:  0.7734375
train loss:  0.42763328552246094
train gradient:  0.09094354433794494
iteration : 2662
train acc:  0.734375
train loss:  0.4474615156650543
train gradient:  0.09367642348180248
iteration : 2663
train acc:  0.7890625
train loss:  0.49518704414367676
train gradient:  0.11616503898448229
iteration : 2664
train acc:  0.7578125
train loss:  0.46019983291625977
train gradient:  0.09779157257677779
iteration : 2665
train acc:  0.7578125
train loss:  0.48273810744285583
train gradient:  0.14208909424777433
iteration : 2666
train acc:  0.7734375
train loss:  0.515061616897583
train gradient:  0.11835481000667188
iteration : 2667
train acc:  0.765625
train loss:  0.4329656958580017
train gradient:  0.09708685660786585
iteration : 2668
train acc:  0.78125
train loss:  0.4849972724914551
train gradient:  0.13379942264879002
iteration : 2669
train acc:  0.6796875
train loss:  0.5523456931114197
train gradient:  0.15528337604564527
iteration : 2670
train acc:  0.84375
train loss:  0.4058533012866974
train gradient:  0.07469687574423038
iteration : 2671
train acc:  0.703125
train loss:  0.5369534492492676
train gradient:  0.1434596708054971
iteration : 2672
train acc:  0.796875
train loss:  0.5004463791847229
train gradient:  0.1690559996956818
iteration : 2673
train acc:  0.765625
train loss:  0.4155769944190979
train gradient:  0.09812877631402649
iteration : 2674
train acc:  0.78125
train loss:  0.4823480248451233
train gradient:  0.13759480219665213
iteration : 2675
train acc:  0.6953125
train loss:  0.5136410593986511
train gradient:  0.13584928516650252
iteration : 2676
train acc:  0.7734375
train loss:  0.4742698073387146
train gradient:  0.0903586776021096
iteration : 2677
train acc:  0.7421875
train loss:  0.5020676851272583
train gradient:  0.12731448122182726
iteration : 2678
train acc:  0.7890625
train loss:  0.4461551606655121
train gradient:  0.11480792390553228
iteration : 2679
train acc:  0.8359375
train loss:  0.39617061614990234
train gradient:  0.07759944728650513
iteration : 2680
train acc:  0.828125
train loss:  0.3840043544769287
train gradient:  0.07652641722250819
iteration : 2681
train acc:  0.7109375
train loss:  0.535269558429718
train gradient:  0.16239960474641996
iteration : 2682
train acc:  0.7890625
train loss:  0.42118000984191895
train gradient:  0.08000901100705704
iteration : 2683
train acc:  0.7890625
train loss:  0.455894410610199
train gradient:  0.11904741671928987
iteration : 2684
train acc:  0.7734375
train loss:  0.45815467834472656
train gradient:  0.10712529631448489
iteration : 2685
train acc:  0.734375
train loss:  0.5212177634239197
train gradient:  0.11479673791141283
iteration : 2686
train acc:  0.71875
train loss:  0.5136367082595825
train gradient:  0.13496635310157776
iteration : 2687
train acc:  0.8046875
train loss:  0.41855692863464355
train gradient:  0.09933910256982002
iteration : 2688
train acc:  0.7265625
train loss:  0.4732999801635742
train gradient:  0.11568525583332144
iteration : 2689
train acc:  0.6796875
train loss:  0.5329138040542603
train gradient:  0.16756279541973335
iteration : 2690
train acc:  0.765625
train loss:  0.4901866912841797
train gradient:  0.10703837567289277
iteration : 2691
train acc:  0.6953125
train loss:  0.583582878112793
train gradient:  0.14196757281986216
iteration : 2692
train acc:  0.7890625
train loss:  0.4276190996170044
train gradient:  0.08281967984691627
iteration : 2693
train acc:  0.7421875
train loss:  0.4910105764865875
train gradient:  0.1064698765143729
iteration : 2694
train acc:  0.7890625
train loss:  0.4740317463874817
train gradient:  0.12464510716395162
iteration : 2695
train acc:  0.7890625
train loss:  0.383181095123291
train gradient:  0.09881072610386676
iteration : 2696
train acc:  0.734375
train loss:  0.46458375453948975
train gradient:  0.10421742272214034
iteration : 2697
train acc:  0.7265625
train loss:  0.5175204873085022
train gradient:  0.11577600832461402
iteration : 2698
train acc:  0.75
train loss:  0.4832475185394287
train gradient:  0.13474735847805644
iteration : 2699
train acc:  0.7734375
train loss:  0.4239282011985779
train gradient:  0.09960559672402124
iteration : 2700
train acc:  0.71875
train loss:  0.4940502643585205
train gradient:  0.11989593266265143
iteration : 2701
train acc:  0.8125
train loss:  0.4425276517868042
train gradient:  0.10358497249109999
iteration : 2702
train acc:  0.7265625
train loss:  0.4951587915420532
train gradient:  0.11722357558255879
iteration : 2703
train acc:  0.828125
train loss:  0.43087702989578247
train gradient:  0.10179032521429611
iteration : 2704
train acc:  0.7421875
train loss:  0.5117882490158081
train gradient:  0.12129729049771266
iteration : 2705
train acc:  0.765625
train loss:  0.5067583322525024
train gradient:  0.10580419603245106
iteration : 2706
train acc:  0.7578125
train loss:  0.452443391084671
train gradient:  0.13251802026870507
iteration : 2707
train acc:  0.7578125
train loss:  0.4692816138267517
train gradient:  0.15205003032416317
iteration : 2708
train acc:  0.703125
train loss:  0.5341199636459351
train gradient:  0.1548521274143146
iteration : 2709
train acc:  0.7421875
train loss:  0.5020720362663269
train gradient:  0.1359793235124857
iteration : 2710
train acc:  0.6953125
train loss:  0.4930766224861145
train gradient:  0.11983013047575411
iteration : 2711
train acc:  0.75
train loss:  0.4787747263908386
train gradient:  0.0881062879451466
iteration : 2712
train acc:  0.765625
train loss:  0.48252442479133606
train gradient:  0.10963843950393168
iteration : 2713
train acc:  0.7578125
train loss:  0.4698590934276581
train gradient:  0.10550439435862778
iteration : 2714
train acc:  0.78125
train loss:  0.46622276306152344
train gradient:  0.09586364411930381
iteration : 2715
train acc:  0.7890625
train loss:  0.42353469133377075
train gradient:  0.09720602731757204
iteration : 2716
train acc:  0.8046875
train loss:  0.40217602252960205
train gradient:  0.11244054984178335
iteration : 2717
train acc:  0.7578125
train loss:  0.476412832736969
train gradient:  0.1276220584152313
iteration : 2718
train acc:  0.7578125
train loss:  0.4687823951244354
train gradient:  0.13279856893923556
iteration : 2719
train acc:  0.765625
train loss:  0.4472880959510803
train gradient:  0.10722608720422552
iteration : 2720
train acc:  0.7109375
train loss:  0.5469520092010498
train gradient:  0.1446749467150335
iteration : 2721
train acc:  0.7265625
train loss:  0.48052772879600525
train gradient:  0.10772038575711515
iteration : 2722
train acc:  0.7890625
train loss:  0.4638819694519043
train gradient:  0.10417774027255915
iteration : 2723
train acc:  0.734375
train loss:  0.49670088291168213
train gradient:  0.1156239139593621
iteration : 2724
train acc:  0.71875
train loss:  0.4738633632659912
train gradient:  0.14868505930465767
iteration : 2725
train acc:  0.8359375
train loss:  0.4064099192619324
train gradient:  0.0762834284301061
iteration : 2726
train acc:  0.7734375
train loss:  0.42971229553222656
train gradient:  0.10483875464204342
iteration : 2727
train acc:  0.765625
train loss:  0.4930415749549866
train gradient:  0.1477918530486517
iteration : 2728
train acc:  0.7421875
train loss:  0.5126636028289795
train gradient:  0.18231367182567842
iteration : 2729
train acc:  0.7421875
train loss:  0.5152392983436584
train gradient:  0.14582713043721537
iteration : 2730
train acc:  0.671875
train loss:  0.5231227278709412
train gradient:  0.15553857822541814
iteration : 2731
train acc:  0.765625
train loss:  0.480108380317688
train gradient:  0.12699361911188717
iteration : 2732
train acc:  0.78125
train loss:  0.45283621549606323
train gradient:  0.09054150098519792
iteration : 2733
train acc:  0.7734375
train loss:  0.4306808114051819
train gradient:  0.10006644831846905
iteration : 2734
train acc:  0.765625
train loss:  0.5003371238708496
train gradient:  0.14114351807512432
iteration : 2735
train acc:  0.78125
train loss:  0.46998584270477295
train gradient:  0.12210591943267499
iteration : 2736
train acc:  0.625
train loss:  0.6152872443199158
train gradient:  0.19414207996666166
iteration : 2737
train acc:  0.7578125
train loss:  0.5039658546447754
train gradient:  0.13799610181427668
iteration : 2738
train acc:  0.7421875
train loss:  0.47592830657958984
train gradient:  0.12064766877858732
iteration : 2739
train acc:  0.734375
train loss:  0.4787202775478363
train gradient:  0.11254585826147906
iteration : 2740
train acc:  0.7578125
train loss:  0.4223993420600891
train gradient:  0.08049718315808724
iteration : 2741
train acc:  0.75
train loss:  0.5110529661178589
train gradient:  0.16104047075081424
iteration : 2742
train acc:  0.7578125
train loss:  0.42640531063079834
train gradient:  0.11911782056704945
iteration : 2743
train acc:  0.8046875
train loss:  0.4435361623764038
train gradient:  0.09522125562421062
iteration : 2744
train acc:  0.828125
train loss:  0.4412440061569214
train gradient:  0.09763375446220866
iteration : 2745
train acc:  0.796875
train loss:  0.4369000792503357
train gradient:  0.10609351220313176
iteration : 2746
train acc:  0.6640625
train loss:  0.5125682353973389
train gradient:  0.1170420992797182
iteration : 2747
train acc:  0.7890625
train loss:  0.4873093366622925
train gradient:  0.12636316882893317
iteration : 2748
train acc:  0.7734375
train loss:  0.4531535506248474
train gradient:  0.10230627846188953
iteration : 2749
train acc:  0.734375
train loss:  0.4805741012096405
train gradient:  0.11046610820794904
iteration : 2750
train acc:  0.7734375
train loss:  0.4555325508117676
train gradient:  0.11227903597175272
iteration : 2751
train acc:  0.6953125
train loss:  0.5430464744567871
train gradient:  0.1550141191193627
iteration : 2752
train acc:  0.6875
train loss:  0.539799153804779
train gradient:  0.15880459143929992
iteration : 2753
train acc:  0.703125
train loss:  0.47883760929107666
train gradient:  0.12395364666484836
iteration : 2754
train acc:  0.78125
train loss:  0.44965267181396484
train gradient:  0.10640454864528609
iteration : 2755
train acc:  0.7109375
train loss:  0.5330122709274292
train gradient:  0.15130667025029126
iteration : 2756
train acc:  0.7890625
train loss:  0.4692799150943756
train gradient:  0.13068451065216324
iteration : 2757
train acc:  0.7421875
train loss:  0.4922751784324646
train gradient:  0.13168413223425585
iteration : 2758
train acc:  0.7578125
train loss:  0.4862534999847412
train gradient:  0.11258602119915423
iteration : 2759
train acc:  0.7578125
train loss:  0.5391414761543274
train gradient:  0.17553207222292655
iteration : 2760
train acc:  0.7265625
train loss:  0.48159146308898926
train gradient:  0.13670963225914168
iteration : 2761
train acc:  0.7734375
train loss:  0.46036723256111145
train gradient:  0.12083077085389882
iteration : 2762
train acc:  0.71875
train loss:  0.5163853764533997
train gradient:  0.12323792678560354
iteration : 2763
train acc:  0.6640625
train loss:  0.538370668888092
train gradient:  0.1389009741725784
iteration : 2764
train acc:  0.8203125
train loss:  0.4191114902496338
train gradient:  0.10882919588946943
iteration : 2765
train acc:  0.6953125
train loss:  0.544510006904602
train gradient:  0.15082218147698886
iteration : 2766
train acc:  0.6953125
train loss:  0.5612384676933289
train gradient:  0.148885986451317
iteration : 2767
train acc:  0.75
train loss:  0.46206146478652954
train gradient:  0.10804306042420021
iteration : 2768
train acc:  0.7734375
train loss:  0.505478024482727
train gradient:  0.1147178953641866
iteration : 2769
train acc:  0.75
train loss:  0.5025588870048523
train gradient:  0.10919331094517382
iteration : 2770
train acc:  0.6640625
train loss:  0.6021737456321716
train gradient:  0.19320625479764125
iteration : 2771
train acc:  0.75
train loss:  0.42914921045303345
train gradient:  0.07282565884833758
iteration : 2772
train acc:  0.7265625
train loss:  0.5082296133041382
train gradient:  0.12862864877348348
iteration : 2773
train acc:  0.7109375
train loss:  0.5110474228858948
train gradient:  0.15174345113074567
iteration : 2774
train acc:  0.7890625
train loss:  0.47064268589019775
train gradient:  0.12292606460761418
iteration : 2775
train acc:  0.703125
train loss:  0.4783191680908203
train gradient:  0.1164878438948788
iteration : 2776
train acc:  0.8125
train loss:  0.4199720025062561
train gradient:  0.14885887503881368
iteration : 2777
train acc:  0.7421875
train loss:  0.5251498222351074
train gradient:  0.13614890463146345
iteration : 2778
train acc:  0.7578125
train loss:  0.47321587800979614
train gradient:  0.10497272278417882
iteration : 2779
train acc:  0.7578125
train loss:  0.4805195927619934
train gradient:  0.10988902709411458
iteration : 2780
train acc:  0.71875
train loss:  0.5004512071609497
train gradient:  0.10088393022322738
iteration : 2781
train acc:  0.75
train loss:  0.45501405000686646
train gradient:  0.1261058998425657
iteration : 2782
train acc:  0.71875
train loss:  0.508968710899353
train gradient:  0.13191525858141362
iteration : 2783
train acc:  0.765625
train loss:  0.46490341424942017
train gradient:  0.1085968504354762
iteration : 2784
train acc:  0.796875
train loss:  0.4742588996887207
train gradient:  0.12562758525894308
iteration : 2785
train acc:  0.78125
train loss:  0.4713825285434723
train gradient:  0.1173382177703902
iteration : 2786
train acc:  0.7578125
train loss:  0.45310747623443604
train gradient:  0.11338563184593155
iteration : 2787
train acc:  0.7109375
train loss:  0.5617306232452393
train gradient:  0.19034141130147814
iteration : 2788
train acc:  0.703125
train loss:  0.5817242860794067
train gradient:  0.16565032137662752
iteration : 2789
train acc:  0.7265625
train loss:  0.5257788896560669
train gradient:  0.1509112796651189
iteration : 2790
train acc:  0.71875
train loss:  0.5083186626434326
train gradient:  0.14190020856961072
iteration : 2791
train acc:  0.7421875
train loss:  0.48521479964256287
train gradient:  0.11549525041778569
iteration : 2792
train acc:  0.75
train loss:  0.5094653367996216
train gradient:  0.11020235822334801
iteration : 2793
train acc:  0.78125
train loss:  0.45846059918403625
train gradient:  0.09731755352225527
iteration : 2794
train acc:  0.7578125
train loss:  0.49487626552581787
train gradient:  0.10915560105354262
iteration : 2795
train acc:  0.7265625
train loss:  0.4997023344039917
train gradient:  0.11077612757013171
iteration : 2796
train acc:  0.765625
train loss:  0.4926305115222931
train gradient:  0.11332353156374551
iteration : 2797
train acc:  0.7578125
train loss:  0.4644969403743744
train gradient:  0.11483731756301188
iteration : 2798
train acc:  0.7578125
train loss:  0.48965147137641907
train gradient:  0.1610551904187025
iteration : 2799
train acc:  0.78125
train loss:  0.47104597091674805
train gradient:  0.12821015148784162
iteration : 2800
train acc:  0.75
train loss:  0.5214642882347107
train gradient:  0.14923631278857513
iteration : 2801
train acc:  0.796875
train loss:  0.4095795750617981
train gradient:  0.09445958276162218
iteration : 2802
train acc:  0.7734375
train loss:  0.45620524883270264
train gradient:  0.10567136471608264
iteration : 2803
train acc:  0.7578125
train loss:  0.47740060091018677
train gradient:  0.08850093525190379
iteration : 2804
train acc:  0.8046875
train loss:  0.4125239849090576
train gradient:  0.07764749649748962
iteration : 2805
train acc:  0.75
train loss:  0.46887069940567017
train gradient:  0.1125529669609779
iteration : 2806
train acc:  0.7578125
train loss:  0.47553354501724243
train gradient:  0.11463642112089581
iteration : 2807
train acc:  0.7734375
train loss:  0.4542025923728943
train gradient:  0.10207968734949784
iteration : 2808
train acc:  0.75
train loss:  0.48057106137275696
train gradient:  0.10692925787397381
iteration : 2809
train acc:  0.6953125
train loss:  0.5425113439559937
train gradient:  0.1492166534944224
iteration : 2810
train acc:  0.734375
train loss:  0.519148051738739
train gradient:  0.13241910566854206
iteration : 2811
train acc:  0.8515625
train loss:  0.4060060381889343
train gradient:  0.09906277501514806
iteration : 2812
train acc:  0.703125
train loss:  0.43506115674972534
train gradient:  0.09110833912745406
iteration : 2813
train acc:  0.75
train loss:  0.46188443899154663
train gradient:  0.12663412008779099
iteration : 2814
train acc:  0.75
train loss:  0.4813382625579834
train gradient:  0.18528319389348685
iteration : 2815
train acc:  0.765625
train loss:  0.47267234325408936
train gradient:  0.14308989784278076
iteration : 2816
train acc:  0.734375
train loss:  0.4614439308643341
train gradient:  0.10110025463576465
iteration : 2817
train acc:  0.6796875
train loss:  0.5488333106040955
train gradient:  0.14090739491214427
iteration : 2818
train acc:  0.734375
train loss:  0.5009641647338867
train gradient:  0.1203249852871477
iteration : 2819
train acc:  0.6953125
train loss:  0.5675673484802246
train gradient:  0.14393655059609128
iteration : 2820
train acc:  0.734375
train loss:  0.45227694511413574
train gradient:  0.1395285206687237
iteration : 2821
train acc:  0.8125
train loss:  0.39573752880096436
train gradient:  0.09480067808174922
iteration : 2822
train acc:  0.71875
train loss:  0.5114370584487915
train gradient:  0.1288957311189579
iteration : 2823
train acc:  0.7265625
train loss:  0.5250717997550964
train gradient:  0.14476841586494432
iteration : 2824
train acc:  0.7265625
train loss:  0.4999274015426636
train gradient:  0.14610570684487353
iteration : 2825
train acc:  0.7578125
train loss:  0.4388914108276367
train gradient:  0.10419806117580496
iteration : 2826
train acc:  0.7265625
train loss:  0.473153293132782
train gradient:  0.11394180252737122
iteration : 2827
train acc:  0.7890625
train loss:  0.46493422985076904
train gradient:  0.1177308652243244
iteration : 2828
train acc:  0.734375
train loss:  0.4842302203178406
train gradient:  0.15512297134353198
iteration : 2829
train acc:  0.8046875
train loss:  0.41818174719810486
train gradient:  0.09076721748968748
iteration : 2830
train acc:  0.78125
train loss:  0.42196083068847656
train gradient:  0.09950161215558719
iteration : 2831
train acc:  0.78125
train loss:  0.46224743127822876
train gradient:  0.10525038768329731
iteration : 2832
train acc:  0.828125
train loss:  0.40403681993484497
train gradient:  0.08313503965260209
iteration : 2833
train acc:  0.71875
train loss:  0.5148884057998657
train gradient:  0.1307584756511101
