program start:
num_rounds= 0
node_emb_dim= 32

----------------------------------------new_epoch--------------------------------------

epoch:  0
iteration : 0
train acc:  0.5546875
train loss:  0.6880925893783569
train gradient:  0.4000349207412287
iteration : 1
train acc:  0.4765625
train loss:  0.7096403241157532
train gradient:  0.4621350002704722
iteration : 2
train acc:  0.5546875
train loss:  0.6996251344680786
train gradient:  0.4709886712085825
iteration : 3
train acc:  0.5625
train loss:  0.677983283996582
train gradient:  0.25128417222895805
iteration : 4
train acc:  0.5703125
train loss:  0.706573486328125
train gradient:  0.27352317656034925
iteration : 5
train acc:  0.578125
train loss:  0.7047000527381897
train gradient:  0.2374632777192602
iteration : 6
train acc:  0.6015625
train loss:  0.6567739844322205
train gradient:  0.24907242415098163
iteration : 7
train acc:  0.578125
train loss:  0.6874667406082153
train gradient:  0.25875595433529913
iteration : 8
train acc:  0.5703125
train loss:  0.6972235441207886
train gradient:  0.3040994143123306
iteration : 9
train acc:  0.5546875
train loss:  0.688194990158081
train gradient:  0.2568319128417808
iteration : 10
train acc:  0.59375
train loss:  0.6934371590614319
train gradient:  0.32892464923328235
iteration : 11
train acc:  0.5546875
train loss:  0.7164970636367798
train gradient:  0.30759346832821144
iteration : 12
train acc:  0.5859375
train loss:  0.6900911331176758
train gradient:  0.2510600353767056
iteration : 13
train acc:  0.6171875
train loss:  0.6874745488166809
train gradient:  0.22787263104957378
iteration : 14
train acc:  0.53125
train loss:  0.7052503824234009
train gradient:  0.25375558491487415
iteration : 15
train acc:  0.5859375
train loss:  0.6787033081054688
train gradient:  0.2974781026193798
iteration : 16
train acc:  0.65625
train loss:  0.6370770931243896
train gradient:  0.1701502894054315
iteration : 17
train acc:  0.5703125
train loss:  0.7124443054199219
train gradient:  0.24488069045055022
iteration : 18
train acc:  0.625
train loss:  0.6283440589904785
train gradient:  0.19370195919715316
iteration : 19
train acc:  0.6640625
train loss:  0.6460342407226562
train gradient:  0.2490585022291164
iteration : 20
train acc:  0.6640625
train loss:  0.6303380727767944
train gradient:  0.20550756344995155
iteration : 21
train acc:  0.6640625
train loss:  0.6301205158233643
train gradient:  0.178045433092068
iteration : 22
train acc:  0.6953125
train loss:  0.6206020712852478
train gradient:  0.16560415981241744
iteration : 23
train acc:  0.609375
train loss:  0.6680024266242981
train gradient:  0.2857033242761453
iteration : 24
train acc:  0.5703125
train loss:  0.6939883828163147
train gradient:  0.4306164922292499
iteration : 25
train acc:  0.6015625
train loss:  0.6253977417945862
train gradient:  0.16263166571783239
iteration : 26
train acc:  0.671875
train loss:  0.6471211314201355
train gradient:  0.145714009723712
iteration : 27
train acc:  0.6484375
train loss:  0.6195656061172485
train gradient:  0.16465335666187833
iteration : 28
train acc:  0.6171875
train loss:  0.658156156539917
train gradient:  0.215757690187794
iteration : 29
train acc:  0.578125
train loss:  0.6515635251998901
train gradient:  0.18927918472386715
iteration : 30
train acc:  0.6171875
train loss:  0.6518759727478027
train gradient:  0.17065645890900516
iteration : 31
train acc:  0.578125
train loss:  0.6813713312149048
train gradient:  0.2640120858217597
iteration : 32
train acc:  0.578125
train loss:  0.6542314291000366
train gradient:  0.2528299228692028
iteration : 33
train acc:  0.5625
train loss:  0.6765785217285156
train gradient:  0.24554099445698
iteration : 34
train acc:  0.640625
train loss:  0.6410699486732483
train gradient:  0.20259478123939112
iteration : 35
train acc:  0.6640625
train loss:  0.6317658424377441
train gradient:  0.18540923803298975
iteration : 36
train acc:  0.71875
train loss:  0.6109225749969482
train gradient:  0.19401020384220868
iteration : 37
train acc:  0.6171875
train loss:  0.654611349105835
train gradient:  0.17192124127469216
iteration : 38
train acc:  0.6171875
train loss:  0.6652605533599854
train gradient:  0.1965203394892193
iteration : 39
train acc:  0.5859375
train loss:  0.6395037174224854
train gradient:  0.16938220071307317
iteration : 40
train acc:  0.5859375
train loss:  0.6725018620491028
train gradient:  0.20998000208521492
iteration : 41
train acc:  0.625
train loss:  0.6693308353424072
train gradient:  0.19413056133629342
iteration : 42
train acc:  0.6484375
train loss:  0.6434733271598816
train gradient:  0.17320235170857812
iteration : 43
train acc:  0.6796875
train loss:  0.6249335408210754
train gradient:  0.18266937548198267
iteration : 44
train acc:  0.5546875
train loss:  0.6708939671516418
train gradient:  0.19095218930409624
iteration : 45
train acc:  0.609375
train loss:  0.6645070910453796
train gradient:  0.18298345719835535
iteration : 46
train acc:  0.5859375
train loss:  0.6942614316940308
train gradient:  0.23655576746619414
iteration : 47
train acc:  0.5859375
train loss:  0.678128719329834
train gradient:  0.1864462735057539
iteration : 48
train acc:  0.7421875
train loss:  0.6061363220214844
train gradient:  0.16977493026574506
iteration : 49
train acc:  0.671875
train loss:  0.6204047203063965
train gradient:  0.1396268793661935
iteration : 50
train acc:  0.6171875
train loss:  0.6543020009994507
train gradient:  0.16646238085943654
iteration : 51
train acc:  0.59375
train loss:  0.6640505194664001
train gradient:  0.20938618881163704
iteration : 52
train acc:  0.609375
train loss:  0.6475505828857422
train gradient:  0.3534679179906718
iteration : 53
train acc:  0.671875
train loss:  0.6221541166305542
train gradient:  0.14648138787714524
iteration : 54
train acc:  0.5859375
train loss:  0.6637468338012695
train gradient:  0.16388615312358315
iteration : 55
train acc:  0.5703125
train loss:  0.6469296216964722
train gradient:  0.16426887832820128
iteration : 56
train acc:  0.6171875
train loss:  0.6436152458190918
train gradient:  0.13160753012639065
iteration : 57
train acc:  0.609375
train loss:  0.6583737730979919
train gradient:  0.17447846103021575
iteration : 58
train acc:  0.59375
train loss:  0.6345124244689941
train gradient:  0.21901310132548116
iteration : 59
train acc:  0.609375
train loss:  0.6363074779510498
train gradient:  0.21158391571088409
iteration : 60
train acc:  0.6328125
train loss:  0.6598203182220459
train gradient:  0.18887474988414765
iteration : 61
train acc:  0.6171875
train loss:  0.6680009365081787
train gradient:  0.1813794955780874
iteration : 62
train acc:  0.65625
train loss:  0.6320830583572388
train gradient:  0.17678693119617017
iteration : 63
train acc:  0.609375
train loss:  0.6578809022903442
train gradient:  0.24633246796393873
iteration : 64
train acc:  0.71875
train loss:  0.6047589778900146
train gradient:  0.26767003920391613
iteration : 65
train acc:  0.671875
train loss:  0.6248611211776733
train gradient:  0.14617254603730045
iteration : 66
train acc:  0.5625
train loss:  0.6624029278755188
train gradient:  0.1748099364652427
iteration : 67
train acc:  0.5859375
train loss:  0.6649175882339478
train gradient:  0.18059079552058138
iteration : 68
train acc:  0.625
train loss:  0.6491363048553467
train gradient:  0.38483773434104085
iteration : 69
train acc:  0.640625
train loss:  0.6492787599563599
train gradient:  0.20428479733471855
iteration : 70
train acc:  0.6015625
train loss:  0.6516488790512085
train gradient:  0.17955643857283693
iteration : 71
train acc:  0.6328125
train loss:  0.6216542720794678
train gradient:  0.1971768298174667
iteration : 72
train acc:  0.6796875
train loss:  0.6126927137374878
train gradient:  0.2602727992304688
iteration : 73
train acc:  0.5703125
train loss:  0.6633192300796509
train gradient:  0.20299304040316785
iteration : 74
train acc:  0.625
train loss:  0.6353667974472046
train gradient:  0.22957633376276
iteration : 75
train acc:  0.5625
train loss:  0.6689488887786865
train gradient:  0.20153541524757237
iteration : 76
train acc:  0.59375
train loss:  0.6429728269577026
train gradient:  0.1632196693368851
iteration : 77
train acc:  0.6015625
train loss:  0.6684278249740601
train gradient:  0.1642249792730856
iteration : 78
train acc:  0.6171875
train loss:  0.6382904052734375
train gradient:  0.15690271065472333
iteration : 79
train acc:  0.671875
train loss:  0.6203327178955078
train gradient:  0.26203580664301085
iteration : 80
train acc:  0.578125
train loss:  0.6820251941680908
train gradient:  0.25173082266532654
iteration : 81
train acc:  0.6015625
train loss:  0.6931029558181763
train gradient:  0.2707468465849863
iteration : 82
train acc:  0.6171875
train loss:  0.6371477842330933
train gradient:  0.22858743900995093
iteration : 83
train acc:  0.6171875
train loss:  0.6370803117752075
train gradient:  0.1985584078746251
iteration : 84
train acc:  0.6171875
train loss:  0.6136789321899414
train gradient:  0.14477135813312858
iteration : 85
train acc:  0.640625
train loss:  0.6213316917419434
train gradient:  0.15793922461401821
iteration : 86
train acc:  0.6015625
train loss:  0.6830649375915527
train gradient:  0.23816944420862296
iteration : 87
train acc:  0.609375
train loss:  0.6724545955657959
train gradient:  0.2655257872673994
iteration : 88
train acc:  0.65625
train loss:  0.6285922527313232
train gradient:  0.18622874971298073
iteration : 89
train acc:  0.6484375
train loss:  0.6301375031471252
train gradient:  0.1452033449480356
iteration : 90
train acc:  0.671875
train loss:  0.6203451156616211
train gradient:  0.18817647210815489
iteration : 91
train acc:  0.6171875
train loss:  0.6512000560760498
train gradient:  0.16493919469625706
iteration : 92
train acc:  0.6875
train loss:  0.6209213137626648
train gradient:  0.2019360187875228
iteration : 93
train acc:  0.6875
train loss:  0.5956810712814331
train gradient:  0.17407904989549908
iteration : 94
train acc:  0.6015625
train loss:  0.6598738431930542
train gradient:  0.17533782399753053
iteration : 95
train acc:  0.578125
train loss:  0.6833411455154419
train gradient:  0.2524900162545467
iteration : 96
train acc:  0.65625
train loss:  0.5934880971908569
train gradient:  0.1813444893025447
iteration : 97
train acc:  0.59375
train loss:  0.6530650854110718
train gradient:  0.15926470709005164
iteration : 98
train acc:  0.5703125
train loss:  0.6629657745361328
train gradient:  0.19549488413220573
iteration : 99
train acc:  0.625
train loss:  0.6424463391304016
train gradient:  0.14298001552178324
iteration : 100
train acc:  0.5703125
train loss:  0.6682717800140381
train gradient:  0.2077083638478341
iteration : 101
train acc:  0.6875
train loss:  0.5955916047096252
train gradient:  0.1259081443256595
iteration : 102
train acc:  0.6171875
train loss:  0.6653823256492615
train gradient:  0.20071649828653068
iteration : 103
train acc:  0.6328125
train loss:  0.621295690536499
train gradient:  0.18007383657332793
iteration : 104
train acc:  0.640625
train loss:  0.6341265439987183
train gradient:  0.19108436555186303
iteration : 105
train acc:  0.546875
train loss:  0.6661351919174194
train gradient:  0.16399472762574874
iteration : 106
train acc:  0.6171875
train loss:  0.6420009136199951
train gradient:  0.18300854174129166
iteration : 107
train acc:  0.578125
train loss:  0.6529237627983093
train gradient:  0.15877573749350388
iteration : 108
train acc:  0.640625
train loss:  0.6344095468521118
train gradient:  0.16419933383268548
iteration : 109
train acc:  0.671875
train loss:  0.6262509822845459
train gradient:  0.13891115705003487
iteration : 110
train acc:  0.625
train loss:  0.6246671676635742
train gradient:  0.13788476322146323
iteration : 111
train acc:  0.625
train loss:  0.607759952545166
train gradient:  0.1594931751060372
iteration : 112
train acc:  0.6875
train loss:  0.5832977294921875
train gradient:  0.16034868146305975
iteration : 113
train acc:  0.6015625
train loss:  0.6373984813690186
train gradient:  0.21343556406231312
iteration : 114
train acc:  0.59375
train loss:  0.6644582748413086
train gradient:  0.15429288293915538
iteration : 115
train acc:  0.671875
train loss:  0.620204508304596
train gradient:  0.1489673334679445
iteration : 116
train acc:  0.6484375
train loss:  0.6470799446105957
train gradient:  0.21048016034798162
iteration : 117
train acc:  0.6171875
train loss:  0.6287119388580322
train gradient:  0.16708810608656555
iteration : 118
train acc:  0.703125
train loss:  0.5959010124206543
train gradient:  0.1391243762319213
iteration : 119
train acc:  0.5859375
train loss:  0.6483889818191528
train gradient:  0.22078183499849008
iteration : 120
train acc:  0.6484375
train loss:  0.6315415501594543
train gradient:  0.14228145474736578
iteration : 121
train acc:  0.6875
train loss:  0.6090371608734131
train gradient:  0.191648704806682
iteration : 122
train acc:  0.578125
train loss:  0.6380800008773804
train gradient:  0.18151385084237154
iteration : 123
train acc:  0.578125
train loss:  0.6492092609405518
train gradient:  0.15233082434624096
iteration : 124
train acc:  0.703125
train loss:  0.5762951374053955
train gradient:  0.13147559103345435
iteration : 125
train acc:  0.5703125
train loss:  0.6641455292701721
train gradient:  0.21421656122603588
iteration : 126
train acc:  0.6875
train loss:  0.6090898513793945
train gradient:  0.15591424631751544
iteration : 127
train acc:  0.7265625
train loss:  0.5954344272613525
train gradient:  0.16789733071251206
iteration : 128
train acc:  0.5703125
train loss:  0.6933887004852295
train gradient:  0.30111861622614156
iteration : 129
train acc:  0.6484375
train loss:  0.5964335799217224
train gradient:  0.1359459537011229
iteration : 130
train acc:  0.6015625
train loss:  0.6289253830909729
train gradient:  0.14972510519745597
iteration : 131
train acc:  0.640625
train loss:  0.6148507595062256
train gradient:  0.16700005498100334
iteration : 132
train acc:  0.609375
train loss:  0.6543453335762024
train gradient:  0.20125544889607938
iteration : 133
train acc:  0.609375
train loss:  0.6324645280838013
train gradient:  0.14791049889425356
iteration : 134
train acc:  0.6640625
train loss:  0.578781008720398
train gradient:  0.17005315997365789
iteration : 135
train acc:  0.59375
train loss:  0.6650694012641907
train gradient:  0.28377247301675224
iteration : 136
train acc:  0.5859375
train loss:  0.6707165837287903
train gradient:  0.1612211346693676
iteration : 137
train acc:  0.578125
train loss:  0.6914304494857788
train gradient:  0.2556461029889499
iteration : 138
train acc:  0.578125
train loss:  0.6601899862289429
train gradient:  0.16619924645337464
iteration : 139
train acc:  0.6875
train loss:  0.6165080070495605
train gradient:  0.1596533684937707
iteration : 140
train acc:  0.53125
train loss:  0.680776834487915
train gradient:  0.26982629478251685
iteration : 141
train acc:  0.640625
train loss:  0.6213070154190063
train gradient:  0.3557429461934546
iteration : 142
train acc:  0.59375
train loss:  0.6479213237762451
train gradient:  0.2672380215315943
iteration : 143
train acc:  0.6171875
train loss:  0.6405107975006104
train gradient:  0.16658953346477773
iteration : 144
train acc:  0.6484375
train loss:  0.6210668683052063
train gradient:  0.11887746344117858
iteration : 145
train acc:  0.6484375
train loss:  0.5910594463348389
train gradient:  0.1871459301205355
iteration : 146
train acc:  0.6796875
train loss:  0.6217689514160156
train gradient:  0.1308335669612049
iteration : 147
train acc:  0.6171875
train loss:  0.6546421647071838
train gradient:  0.15126468907978424
iteration : 148
train acc:  0.6640625
train loss:  0.5815465450286865
train gradient:  0.17931111643846803
iteration : 149
train acc:  0.6484375
train loss:  0.6546221971511841
train gradient:  0.22914531764683177
iteration : 150
train acc:  0.59375
train loss:  0.614707350730896
train gradient:  0.18510169640707597
iteration : 151
train acc:  0.59375
train loss:  0.6536747217178345
train gradient:  0.1488280519959458
iteration : 152
train acc:  0.609375
train loss:  0.6576451063156128
train gradient:  0.14972873701341508
iteration : 153
train acc:  0.6484375
train loss:  0.6386445164680481
train gradient:  0.23273143449575226
iteration : 154
train acc:  0.671875
train loss:  0.6042379140853882
train gradient:  0.17075204869403646
iteration : 155
train acc:  0.640625
train loss:  0.6261060833930969
train gradient:  0.25495950321427496
iteration : 156
train acc:  0.609375
train loss:  0.6406018137931824
train gradient:  0.1715841425665003
iteration : 157
train acc:  0.6328125
train loss:  0.625702440738678
train gradient:  0.1717499421703037
iteration : 158
train acc:  0.625
train loss:  0.6475052237510681
train gradient:  0.17286938371171115
iteration : 159
train acc:  0.6328125
train loss:  0.6075844168663025
train gradient:  0.15652438135409869
iteration : 160
train acc:  0.625
train loss:  0.649718165397644
train gradient:  0.16759916663025648
iteration : 161
train acc:  0.609375
train loss:  0.618341863155365
train gradient:  0.12464432999185246
iteration : 162
train acc:  0.6640625
train loss:  0.6255254745483398
train gradient:  0.17955893405515522
iteration : 163
train acc:  0.609375
train loss:  0.6404598951339722
train gradient:  0.1250134480148116
iteration : 164
train acc:  0.5625
train loss:  0.6598243117332458
train gradient:  0.1509153496097349
iteration : 165
train acc:  0.59375
train loss:  0.6372668743133545
train gradient:  0.1951778371316373
iteration : 166
train acc:  0.6015625
train loss:  0.6300288438796997
train gradient:  0.2820438518396527
iteration : 167
train acc:  0.640625
train loss:  0.6049784421920776
train gradient:  0.1601513067792552
iteration : 168
train acc:  0.5703125
train loss:  0.6934428215026855
train gradient:  0.190491604666879
iteration : 169
train acc:  0.6484375
train loss:  0.6335510015487671
train gradient:  0.17687184963418068
iteration : 170
train acc:  0.671875
train loss:  0.5967212915420532
train gradient:  0.21385514486399296
iteration : 171
train acc:  0.6015625
train loss:  0.6906494498252869
train gradient:  0.23474535647329814
iteration : 172
train acc:  0.6640625
train loss:  0.6375175714492798
train gradient:  0.3413331541833033
iteration : 173
train acc:  0.6484375
train loss:  0.6278327703475952
train gradient:  0.15521631760128507
iteration : 174
train acc:  0.65625
train loss:  0.5997533798217773
train gradient:  0.1509622609483913
iteration : 175
train acc:  0.5546875
train loss:  0.6593097448348999
train gradient:  0.15914233175237358
iteration : 176
train acc:  0.6484375
train loss:  0.6227178573608398
train gradient:  0.16230709638288093
iteration : 177
train acc:  0.6796875
train loss:  0.6167376041412354
train gradient:  0.1865263445747672
iteration : 178
train acc:  0.609375
train loss:  0.6397913694381714
train gradient:  0.16741862319474177
iteration : 179
train acc:  0.625
train loss:  0.6579982042312622
train gradient:  0.18519700357603797
iteration : 180
train acc:  0.609375
train loss:  0.6345809698104858
train gradient:  0.17233892851913502
iteration : 181
train acc:  0.6015625
train loss:  0.6422141194343567
train gradient:  0.15334787249090875
iteration : 182
train acc:  0.6875
train loss:  0.6053871512413025
train gradient:  0.14445727344546838
iteration : 183
train acc:  0.6640625
train loss:  0.6440674066543579
train gradient:  0.16457448527193308
iteration : 184
train acc:  0.6875
train loss:  0.6063095331192017
train gradient:  0.1947530105507818
iteration : 185
train acc:  0.625
train loss:  0.6756443381309509
train gradient:  0.14332960171299725
iteration : 186
train acc:  0.671875
train loss:  0.61179119348526
train gradient:  0.1462173655513344
iteration : 187
train acc:  0.578125
train loss:  0.6721329689025879
train gradient:  0.165728437107867
iteration : 188
train acc:  0.75
train loss:  0.5641930103302002
train gradient:  0.1586803161065217
iteration : 189
train acc:  0.65625
train loss:  0.6264349818229675
train gradient:  0.1339605879709928
iteration : 190
train acc:  0.6171875
train loss:  0.6526467800140381
train gradient:  0.1738685558918411
iteration : 191
train acc:  0.703125
train loss:  0.5930087566375732
train gradient:  0.1574666170067875
iteration : 192
train acc:  0.609375
train loss:  0.6583412289619446
train gradient:  0.1652415253326048
iteration : 193
train acc:  0.6640625
train loss:  0.6189336180686951
train gradient:  0.14069453231993664
iteration : 194
train acc:  0.6796875
train loss:  0.5955057740211487
train gradient:  0.16974007326373586
iteration : 195
train acc:  0.6953125
train loss:  0.59576815366745
train gradient:  0.1789584584013854
iteration : 196
train acc:  0.7109375
train loss:  0.6202532052993774
train gradient:  0.15268538285388106
iteration : 197
train acc:  0.6796875
train loss:  0.5904905200004578
train gradient:  0.15076886602890102
iteration : 198
train acc:  0.6640625
train loss:  0.6389514207839966
train gradient:  0.17515613497163535
iteration : 199
train acc:  0.6875
train loss:  0.5853760242462158
train gradient:  0.1365070676751113
iteration : 200
train acc:  0.609375
train loss:  0.6475173234939575
train gradient:  0.15377931610209294
iteration : 201
train acc:  0.6484375
train loss:  0.6129482984542847
train gradient:  0.13457361455811462
iteration : 202
train acc:  0.7265625
train loss:  0.5646997690200806
train gradient:  0.14304486237794845
iteration : 203
train acc:  0.75
train loss:  0.5639246702194214
train gradient:  0.14849948605494218
iteration : 204
train acc:  0.6640625
train loss:  0.6234106421470642
train gradient:  0.16600240949052808
iteration : 205
train acc:  0.5859375
train loss:  0.639864981174469
train gradient:  0.17102750083567814
iteration : 206
train acc:  0.625
train loss:  0.6267966032028198
train gradient:  0.18989933140227994
iteration : 207
train acc:  0.6796875
train loss:  0.6090126037597656
train gradient:  0.1558552510812546
iteration : 208
train acc:  0.6875
train loss:  0.60633385181427
train gradient:  0.1284464015009174
iteration : 209
train acc:  0.6015625
train loss:  0.6552420258522034
train gradient:  0.15665095824511255
iteration : 210
train acc:  0.59375
train loss:  0.6472073793411255
train gradient:  0.23089472389435856
iteration : 211
train acc:  0.671875
train loss:  0.6017231941223145
train gradient:  0.18051713443396697
iteration : 212
train acc:  0.59375
train loss:  0.6312820911407471
train gradient:  0.1721026052370996
iteration : 213
train acc:  0.6640625
train loss:  0.6407378911972046
train gradient:  0.14394675702447304
iteration : 214
train acc:  0.609375
train loss:  0.6505222916603088
train gradient:  0.17747902799680357
iteration : 215
train acc:  0.6796875
train loss:  0.5810238718986511
train gradient:  0.17985966267937248
iteration : 216
train acc:  0.6796875
train loss:  0.6044493913650513
train gradient:  0.1379237417045518
iteration : 217
train acc:  0.6171875
train loss:  0.6547936797142029
train gradient:  0.21828189199335324
iteration : 218
train acc:  0.6171875
train loss:  0.6516071557998657
train gradient:  0.15264778861442918
iteration : 219
train acc:  0.6640625
train loss:  0.5887899398803711
train gradient:  0.1235585300606774
iteration : 220
train acc:  0.640625
train loss:  0.6333779096603394
train gradient:  0.20241350261231283
iteration : 221
train acc:  0.65625
train loss:  0.6062917709350586
train gradient:  0.24488629697658776
iteration : 222
train acc:  0.578125
train loss:  0.6544815301895142
train gradient:  0.16457457776488743
iteration : 223
train acc:  0.6015625
train loss:  0.617813766002655
train gradient:  0.149131379095215
iteration : 224
train acc:  0.7578125
train loss:  0.5448562502861023
train gradient:  0.1331132786989414
iteration : 225
train acc:  0.59375
train loss:  0.6510944366455078
train gradient:  0.16648146241937145
iteration : 226
train acc:  0.6484375
train loss:  0.6040757894515991
train gradient:  0.14511831162117145
iteration : 227
train acc:  0.65625
train loss:  0.6178844571113586
train gradient:  0.2526793698767029
iteration : 228
train acc:  0.6484375
train loss:  0.5903136730194092
train gradient:  0.2845785891534065
iteration : 229
train acc:  0.6328125
train loss:  0.6091437339782715
train gradient:  0.21471471870694603
iteration : 230
train acc:  0.703125
train loss:  0.5726617574691772
train gradient:  0.11492886003189515
iteration : 231
train acc:  0.6484375
train loss:  0.6206516623497009
train gradient:  0.2924098350790616
iteration : 232
train acc:  0.6875
train loss:  0.5943560004234314
train gradient:  0.12871237508360997
iteration : 233
train acc:  0.7421875
train loss:  0.5773863792419434
train gradient:  0.15579468873735214
iteration : 234
train acc:  0.625
train loss:  0.6369539499282837
train gradient:  0.17899049002840867
iteration : 235
train acc:  0.609375
train loss:  0.6330745220184326
train gradient:  0.1686179930785411
iteration : 236
train acc:  0.671875
train loss:  0.630508542060852
train gradient:  0.1538258444346352
iteration : 237
train acc:  0.5859375
train loss:  0.6406896114349365
train gradient:  0.17522685501624746
iteration : 238
train acc:  0.59375
train loss:  0.6669027209281921
train gradient:  0.16873926491783026
iteration : 239
train acc:  0.6484375
train loss:  0.6086757183074951
train gradient:  0.19075314307527663
iteration : 240
train acc:  0.6328125
train loss:  0.6303403973579407
train gradient:  0.1722388708619986
iteration : 241
train acc:  0.6328125
train loss:  0.5796979069709778
train gradient:  0.15563466070061993
iteration : 242
train acc:  0.640625
train loss:  0.6003091335296631
train gradient:  0.12696443403038413
iteration : 243
train acc:  0.6953125
train loss:  0.586492657661438
train gradient:  0.16296248113515935
iteration : 244
train acc:  0.671875
train loss:  0.607803463935852
train gradient:  0.14201168565291578
iteration : 245
train acc:  0.6171875
train loss:  0.5940696001052856
train gradient:  0.1844229943865845
iteration : 246
train acc:  0.6484375
train loss:  0.6438462138175964
train gradient:  0.17759780330456992
iteration : 247
train acc:  0.640625
train loss:  0.6369712352752686
train gradient:  0.15723923097799583
iteration : 248
train acc:  0.6875
train loss:  0.5982270240783691
train gradient:  0.11013320438063424
iteration : 249
train acc:  0.5859375
train loss:  0.6608448624610901
train gradient:  0.18492604523787506
iteration : 250
train acc:  0.6953125
train loss:  0.5624910593032837
train gradient:  0.14575886216935263
iteration : 251
train acc:  0.59375
train loss:  0.6448866128921509
train gradient:  0.23803131278784226
iteration : 252
train acc:  0.6484375
train loss:  0.6161603331565857
train gradient:  0.18714283218292904
iteration : 253
train acc:  0.671875
train loss:  0.6183754205703735
train gradient:  0.17288819835001557
iteration : 254
train acc:  0.6953125
train loss:  0.579534649848938
train gradient:  0.1484288119713344
iteration : 255
train acc:  0.640625
train loss:  0.6534629464149475
train gradient:  0.1721164080063119
iteration : 256
train acc:  0.6171875
train loss:  0.6258841753005981
train gradient:  0.18658069225238272
iteration : 257
train acc:  0.6171875
train loss:  0.6458846926689148
train gradient:  0.15217546213342537
iteration : 258
train acc:  0.6171875
train loss:  0.5909762978553772
train gradient:  0.17339105714307473
iteration : 259
train acc:  0.640625
train loss:  0.6330212950706482
train gradient:  0.13832429518838316
iteration : 260
train acc:  0.6328125
train loss:  0.6307100057601929
train gradient:  0.15647494368666587
iteration : 261
train acc:  0.6328125
train loss:  0.620407223701477
train gradient:  0.16151527788820597
iteration : 262
train acc:  0.59375
train loss:  0.6985350847244263
train gradient:  0.17797969391198865
iteration : 263
train acc:  0.5546875
train loss:  0.6891971826553345
train gradient:  0.2104167148819874
iteration : 264
train acc:  0.5625
train loss:  0.6611343622207642
train gradient:  0.18540191198382178
iteration : 265
train acc:  0.6640625
train loss:  0.6255978941917419
train gradient:  0.1605212048576417
iteration : 266
train acc:  0.671875
train loss:  0.5972551107406616
train gradient:  0.12552570146313052
iteration : 267
train acc:  0.671875
train loss:  0.596907913684845
train gradient:  0.18263849759173156
iteration : 268
train acc:  0.6796875
train loss:  0.5858697891235352
train gradient:  0.17113392004296568
iteration : 269
train acc:  0.7578125
train loss:  0.5746213793754578
train gradient:  0.14082567530530304
iteration : 270
train acc:  0.6328125
train loss:  0.5960078239440918
train gradient:  0.1561058381205026
iteration : 271
train acc:  0.59375
train loss:  0.6210824251174927
train gradient:  0.2165261506748179
iteration : 272
train acc:  0.6015625
train loss:  0.6304042935371399
train gradient:  0.23760591210341864
iteration : 273
train acc:  0.6484375
train loss:  0.6244786381721497
train gradient:  0.17510556887305004
iteration : 274
train acc:  0.5859375
train loss:  0.6644551753997803
train gradient:  0.18532068173309196
iteration : 275
train acc:  0.703125
train loss:  0.5794253349304199
train gradient:  0.1378955874514592
iteration : 276
train acc:  0.6484375
train loss:  0.6204797029495239
train gradient:  0.18684070470060177
iteration : 277
train acc:  0.6171875
train loss:  0.6543424129486084
train gradient:  0.17805244904265236
iteration : 278
train acc:  0.6640625
train loss:  0.597122073173523
train gradient:  0.15810732111259793
iteration : 279
train acc:  0.59375
train loss:  0.6585767269134521
train gradient:  0.17747700914807918
iteration : 280
train acc:  0.6484375
train loss:  0.6441364288330078
train gradient:  0.18185642162525836
iteration : 281
train acc:  0.6640625
train loss:  0.6011356115341187
train gradient:  0.18399362275333753
iteration : 282
train acc:  0.609375
train loss:  0.6250604391098022
train gradient:  0.14720630381623273
iteration : 283
train acc:  0.59375
train loss:  0.6405086517333984
train gradient:  0.19919617651422428
iteration : 284
train acc:  0.640625
train loss:  0.6174784302711487
train gradient:  0.16999577119976833
iteration : 285
train acc:  0.65625
train loss:  0.64324951171875
train gradient:  0.13130052132471928
iteration : 286
train acc:  0.65625
train loss:  0.6254152059555054
train gradient:  0.178364486191502
iteration : 287
train acc:  0.6875
train loss:  0.5992096066474915
train gradient:  0.16977510571885984
iteration : 288
train acc:  0.6484375
train loss:  0.6082853078842163
train gradient:  0.17427434575081285
iteration : 289
train acc:  0.6875
train loss:  0.6050389409065247
train gradient:  0.1459992503909916
iteration : 290
train acc:  0.671875
train loss:  0.5989883542060852
train gradient:  0.15338044437449727
iteration : 291
train acc:  0.671875
train loss:  0.5999179482460022
train gradient:  0.16800250963785723
iteration : 292
train acc:  0.6015625
train loss:  0.6460973024368286
train gradient:  0.1656738241043338
iteration : 293
train acc:  0.6015625
train loss:  0.6310465335845947
train gradient:  0.18028481225114626
iteration : 294
train acc:  0.6171875
train loss:  0.6268432140350342
train gradient:  0.17877346479119444
iteration : 295
train acc:  0.59375
train loss:  0.6561474204063416
train gradient:  0.18403577844587393
iteration : 296
train acc:  0.5703125
train loss:  0.6655873656272888
train gradient:  0.1473141557167584
iteration : 297
train acc:  0.65625
train loss:  0.6193380355834961
train gradient:  0.15979946038692566
iteration : 298
train acc:  0.6953125
train loss:  0.585540771484375
train gradient:  0.19382756215041586
iteration : 299
train acc:  0.625
train loss:  0.6315834522247314
train gradient:  0.20206156868713587
iteration : 300
train acc:  0.6796875
train loss:  0.6053502559661865
train gradient:  0.1321673015684508
iteration : 301
train acc:  0.6171875
train loss:  0.6102421283721924
train gradient:  0.1624951543047479
iteration : 302
train acc:  0.6015625
train loss:  0.6322222948074341
train gradient:  0.2043932413179539
iteration : 303
train acc:  0.65625
train loss:  0.6079764366149902
train gradient:  0.14237447358061517
iteration : 304
train acc:  0.671875
train loss:  0.6041406393051147
train gradient:  0.1446167188446401
iteration : 305
train acc:  0.625
train loss:  0.650335431098938
train gradient:  0.1789890172041485
iteration : 306
train acc:  0.703125
train loss:  0.5943319797515869
train gradient:  0.15999216259251592
iteration : 307
train acc:  0.65625
train loss:  0.5925118923187256
train gradient:  0.13119170972602395
iteration : 308
train acc:  0.65625
train loss:  0.6317689418792725
train gradient:  0.21295321468451162
iteration : 309
train acc:  0.6953125
train loss:  0.5947854518890381
train gradient:  0.13326174552569095
iteration : 310
train acc:  0.6953125
train loss:  0.5879036784172058
train gradient:  0.1991122218455143
iteration : 311
train acc:  0.640625
train loss:  0.6174279451370239
train gradient:  0.40540525647342934
iteration : 312
train acc:  0.703125
train loss:  0.5958921909332275
train gradient:  0.17138249669006617
iteration : 313
train acc:  0.6796875
train loss:  0.5992656946182251
train gradient:  0.18028584261090724
iteration : 314
train acc:  0.6875
train loss:  0.5902292132377625
train gradient:  0.1806170957665637
iteration : 315
train acc:  0.640625
train loss:  0.6270262002944946
train gradient:  0.2017157065561913
iteration : 316
train acc:  0.65625
train loss:  0.6405232548713684
train gradient:  0.20543666774810415
iteration : 317
train acc:  0.71875
train loss:  0.6012989282608032
train gradient:  0.1436540909876536
iteration : 318
train acc:  0.6796875
train loss:  0.6226269006729126
train gradient:  0.15328020183307473
iteration : 319
train acc:  0.6015625
train loss:  0.6395090818405151
train gradient:  0.2834210781431388
iteration : 320
train acc:  0.6171875
train loss:  0.6302530169487
train gradient:  0.1628875593841166
iteration : 321
train acc:  0.671875
train loss:  0.6112164855003357
train gradient:  0.1856028565374553
iteration : 322
train acc:  0.6015625
train loss:  0.6439955234527588
train gradient:  0.21338335984279166
iteration : 323
train acc:  0.6875
train loss:  0.5991157293319702
train gradient:  0.11883478790679759
iteration : 324
train acc:  0.5703125
train loss:  0.6761494874954224
train gradient:  0.3124806792864518
iteration : 325
train acc:  0.625
train loss:  0.610328197479248
train gradient:  0.14161214808165312
iteration : 326
train acc:  0.765625
train loss:  0.5517639517784119
train gradient:  0.14641046900969895
iteration : 327
train acc:  0.640625
train loss:  0.6163321137428284
train gradient:  0.1638397899481734
iteration : 328
train acc:  0.65625
train loss:  0.6063162088394165
train gradient:  0.1562832834918349
iteration : 329
train acc:  0.734375
train loss:  0.5602132678031921
train gradient:  0.17369920233198524
iteration : 330
train acc:  0.671875
train loss:  0.5723511576652527
train gradient:  0.1353273106216608
iteration : 331
train acc:  0.6953125
train loss:  0.5708752274513245
train gradient:  0.1406537341874155
iteration : 332
train acc:  0.7421875
train loss:  0.5499235391616821
train gradient:  0.12178390705689973
iteration : 333
train acc:  0.71875
train loss:  0.6080390214920044
train gradient:  0.14502855450883848
iteration : 334
train acc:  0.6171875
train loss:  0.6419789791107178
train gradient:  0.23759372415438942
iteration : 335
train acc:  0.6953125
train loss:  0.5956577062606812
train gradient:  0.1241766308695849
iteration : 336
train acc:  0.6875
train loss:  0.5772347450256348
train gradient:  0.14777654950443087
iteration : 337
train acc:  0.6328125
train loss:  0.6160823106765747
train gradient:  0.22515606989326178
iteration : 338
train acc:  0.640625
train loss:  0.6076130867004395
train gradient:  0.1475974161632762
iteration : 339
train acc:  0.640625
train loss:  0.6295523047447205
train gradient:  0.1981809026992465
iteration : 340
train acc:  0.6640625
train loss:  0.566773533821106
train gradient:  0.13066029443673144
iteration : 341
train acc:  0.65625
train loss:  0.5708591341972351
train gradient:  0.20107183244514565
iteration : 342
train acc:  0.625
train loss:  0.6378052234649658
train gradient:  0.16002005814455428
iteration : 343
train acc:  0.671875
train loss:  0.5753272771835327
train gradient:  0.2254341363410415
iteration : 344
train acc:  0.6484375
train loss:  0.6046035289764404
train gradient:  0.17047435187907395
iteration : 345
train acc:  0.671875
train loss:  0.5701324939727783
train gradient:  0.13800966008792795
iteration : 346
train acc:  0.6796875
train loss:  0.5812841653823853
train gradient:  0.16277861787126052
iteration : 347
train acc:  0.75
train loss:  0.549496054649353
train gradient:  0.16510439449193332
iteration : 348
train acc:  0.6640625
train loss:  0.5944268703460693
train gradient:  0.2527064971679144
iteration : 349
train acc:  0.71875
train loss:  0.5870932936668396
train gradient:  0.1240076050942296
iteration : 350
train acc:  0.640625
train loss:  0.6112779974937439
train gradient:  0.17719797093985099
iteration : 351
train acc:  0.6328125
train loss:  0.6000936031341553
train gradient:  0.16300454933327374
iteration : 352
train acc:  0.671875
train loss:  0.6085962057113647
train gradient:  0.1722036422930126
iteration : 353
train acc:  0.7109375
train loss:  0.6005512475967407
train gradient:  0.39485854565985085
iteration : 354
train acc:  0.6640625
train loss:  0.6284246444702148
train gradient:  0.17330634019600558
iteration : 355
train acc:  0.6171875
train loss:  0.6368335485458374
train gradient:  0.21903671747218148
iteration : 356
train acc:  0.6015625
train loss:  0.6289645433425903
train gradient:  0.1870237510967481
iteration : 357
train acc:  0.6171875
train loss:  0.6139364242553711
train gradient:  0.1939726371888564
iteration : 358
train acc:  0.6171875
train loss:  0.6325763463973999
train gradient:  0.2793632707818176
iteration : 359
train acc:  0.6328125
train loss:  0.6648381352424622
train gradient:  0.26090480630514995
iteration : 360
train acc:  0.6484375
train loss:  0.6138898134231567
train gradient:  0.15501170376596268
iteration : 361
train acc:  0.65625
train loss:  0.6453688144683838
train gradient:  0.19996960118894697
iteration : 362
train acc:  0.5703125
train loss:  0.656967043876648
train gradient:  0.1748810477436716
iteration : 363
train acc:  0.6953125
train loss:  0.5956188440322876
train gradient:  0.23737227470137887
iteration : 364
train acc:  0.6640625
train loss:  0.6004773378372192
train gradient:  0.13158397429438223
iteration : 365
train acc:  0.7109375
train loss:  0.5771833658218384
train gradient:  0.16314798097212818
iteration : 366
train acc:  0.6640625
train loss:  0.5949801206588745
train gradient:  0.13115347555746093
iteration : 367
train acc:  0.625
train loss:  0.6185462474822998
train gradient:  0.2159967580632824
iteration : 368
train acc:  0.6796875
train loss:  0.547911524772644
train gradient:  0.1286030774704558
iteration : 369
train acc:  0.6171875
train loss:  0.6167566776275635
train gradient:  0.19332511085085202
iteration : 370
train acc:  0.6796875
train loss:  0.585946261882782
train gradient:  0.16392783755352103
iteration : 371
train acc:  0.59375
train loss:  0.6152368783950806
train gradient:  0.1751954128887565
iteration : 372
train acc:  0.703125
train loss:  0.5549663305282593
train gradient:  0.1620087546202063
iteration : 373
train acc:  0.6953125
train loss:  0.5950599908828735
train gradient:  0.16770717361544613
iteration : 374
train acc:  0.7734375
train loss:  0.5567059516906738
train gradient:  0.172453280404668
iteration : 375
train acc:  0.65625
train loss:  0.6045300364494324
train gradient:  0.15692853888831582
iteration : 376
train acc:  0.6640625
train loss:  0.634355902671814
train gradient:  0.20808996516465023
iteration : 377
train acc:  0.6640625
train loss:  0.6033433079719543
train gradient:  0.13310265325431064
iteration : 378
train acc:  0.640625
train loss:  0.6368605494499207
train gradient:  0.1774656351668971
iteration : 379
train acc:  0.640625
train loss:  0.6014008522033691
train gradient:  0.2363427388028246
iteration : 380
train acc:  0.6171875
train loss:  0.6011607646942139
train gradient:  0.14563197585151944
iteration : 381
train acc:  0.6484375
train loss:  0.609637975692749
train gradient:  0.1489814688236584
iteration : 382
train acc:  0.7265625
train loss:  0.5797704458236694
train gradient:  0.16015964267820518
iteration : 383
train acc:  0.6015625
train loss:  0.6294701099395752
train gradient:  0.21701595160427678
iteration : 384
train acc:  0.640625
train loss:  0.6116795539855957
train gradient:  0.20891961286571792
iteration : 385
train acc:  0.765625
train loss:  0.5794740915298462
train gradient:  0.16067316292483852
iteration : 386
train acc:  0.703125
train loss:  0.5745531916618347
train gradient:  0.16926213428978235
iteration : 387
train acc:  0.6796875
train loss:  0.6261063814163208
train gradient:  0.1422939587418564
iteration : 388
train acc:  0.6484375
train loss:  0.5938340425491333
train gradient:  0.24363220876773897
iteration : 389
train acc:  0.671875
train loss:  0.6250267028808594
train gradient:  0.17417375230212334
iteration : 390
train acc:  0.625
train loss:  0.6580591797828674
train gradient:  0.41088035111196236
iteration : 391
train acc:  0.6484375
train loss:  0.6266674995422363
train gradient:  0.1952233657022887
iteration : 392
train acc:  0.59375
train loss:  0.6467925310134888
train gradient:  0.2122770274941987
iteration : 393
train acc:  0.6640625
train loss:  0.5844507217407227
train gradient:  0.1476735523777032
iteration : 394
train acc:  0.703125
train loss:  0.5680966377258301
train gradient:  0.14848061336350002
iteration : 395
train acc:  0.625
train loss:  0.6272522807121277
train gradient:  0.20601831457025574
iteration : 396
train acc:  0.6484375
train loss:  0.6187978982925415
train gradient:  0.1732366093834443
iteration : 397
train acc:  0.65625
train loss:  0.6369521617889404
train gradient:  0.16798036112013975
iteration : 398
train acc:  0.6875
train loss:  0.6350463628768921
train gradient:  0.16512114010495837
iteration : 399
train acc:  0.6796875
train loss:  0.6100668907165527
train gradient:  0.15611939433135832
iteration : 400
train acc:  0.6875
train loss:  0.5883179903030396
train gradient:  0.21632265660903327
iteration : 401
train acc:  0.65625
train loss:  0.6015735864639282
train gradient:  0.14974678772198674
iteration : 402
train acc:  0.703125
train loss:  0.5930399298667908
train gradient:  0.17599142913449506
iteration : 403
train acc:  0.7265625
train loss:  0.5535085797309875
train gradient:  0.14470647745025378
iteration : 404
train acc:  0.609375
train loss:  0.6266013383865356
train gradient:  0.19020961528212932
iteration : 405
train acc:  0.7734375
train loss:  0.5580792427062988
train gradient:  0.20973384112054366
iteration : 406
train acc:  0.6328125
train loss:  0.5975874066352844
train gradient:  0.14939318618333852
iteration : 407
train acc:  0.7265625
train loss:  0.5873196125030518
train gradient:  0.18721065836191417
iteration : 408
train acc:  0.6796875
train loss:  0.5991674661636353
train gradient:  0.15815267158230875
iteration : 409
train acc:  0.765625
train loss:  0.5854235887527466
train gradient:  0.14292494005162487
iteration : 410
train acc:  0.6796875
train loss:  0.5944039225578308
train gradient:  0.15143239425524005
iteration : 411
train acc:  0.6640625
train loss:  0.5798030495643616
train gradient:  0.1711945742974988
iteration : 412
train acc:  0.6171875
train loss:  0.6158851385116577
train gradient:  0.16808525711541722
iteration : 413
train acc:  0.6796875
train loss:  0.6159691214561462
train gradient:  0.1666417288041679
iteration : 414
train acc:  0.6171875
train loss:  0.6439718008041382
train gradient:  0.1831384517743601
iteration : 415
train acc:  0.75
train loss:  0.5377942323684692
train gradient:  0.14438078828609086
iteration : 416
train acc:  0.703125
train loss:  0.5994409918785095
train gradient:  0.16606252461323995
iteration : 417
train acc:  0.6953125
train loss:  0.5784729719161987
train gradient:  0.11901336539299986
iteration : 418
train acc:  0.6171875
train loss:  0.6335194706916809
train gradient:  0.2812711119394419
iteration : 419
train acc:  0.5546875
train loss:  0.667512059211731
train gradient:  0.22465994199318404
iteration : 420
train acc:  0.6796875
train loss:  0.5941077470779419
train gradient:  0.1761650586440816
iteration : 421
train acc:  0.7421875
train loss:  0.5211342573165894
train gradient:  0.1258886693275274
iteration : 422
train acc:  0.625
train loss:  0.62613445520401
train gradient:  0.17574839769006684
iteration : 423
train acc:  0.671875
train loss:  0.59433513879776
train gradient:  0.14342284433835967
iteration : 424
train acc:  0.7265625
train loss:  0.5800824761390686
train gradient:  0.16628730395237634
iteration : 425
train acc:  0.6484375
train loss:  0.612047016620636
train gradient:  0.163983452532229
iteration : 426
train acc:  0.7265625
train loss:  0.5824112892150879
train gradient:  0.19227157600441508
iteration : 427
train acc:  0.6640625
train loss:  0.5635278224945068
train gradient:  0.13661835117839244
iteration : 428
train acc:  0.6796875
train loss:  0.5974376797676086
train gradient:  0.14946817065431764
iteration : 429
train acc:  0.703125
train loss:  0.594458818435669
train gradient:  0.10672143667372588
iteration : 430
train acc:  0.640625
train loss:  0.6083276867866516
train gradient:  0.18246809155302923
iteration : 431
train acc:  0.6796875
train loss:  0.6132739186286926
train gradient:  0.15509523926499585
iteration : 432
train acc:  0.703125
train loss:  0.5407556891441345
train gradient:  0.17253810010473078
iteration : 433
train acc:  0.765625
train loss:  0.5375394225120544
train gradient:  0.11682287122628308
iteration : 434
train acc:  0.6484375
train loss:  0.6136103868484497
train gradient:  0.18487890506973792
iteration : 435
train acc:  0.6875
train loss:  0.5980294942855835
train gradient:  0.1955484018332723
iteration : 436
train acc:  0.6015625
train loss:  0.6418741345405579
train gradient:  0.2266170592749645
iteration : 437
train acc:  0.7109375
train loss:  0.5822439789772034
train gradient:  0.20010790552480087
iteration : 438
train acc:  0.6328125
train loss:  0.5974079966545105
train gradient:  0.18194925790380834
iteration : 439
train acc:  0.671875
train loss:  0.5953097343444824
train gradient:  0.1834809414360874
iteration : 440
train acc:  0.7265625
train loss:  0.5598487854003906
train gradient:  0.16664604996929416
iteration : 441
train acc:  0.6171875
train loss:  0.6360557675361633
train gradient:  0.26641642228596024
iteration : 442
train acc:  0.6875
train loss:  0.5886925458908081
train gradient:  0.16614820959562038
iteration : 443
train acc:  0.6796875
train loss:  0.6221837997436523
train gradient:  0.1551635570793395
iteration : 444
train acc:  0.640625
train loss:  0.6275776624679565
train gradient:  0.189582746837204
iteration : 445
train acc:  0.578125
train loss:  0.6527796983718872
train gradient:  0.16658719606389671
iteration : 446
train acc:  0.6875
train loss:  0.5615248680114746
train gradient:  0.12599224732272246
iteration : 447
train acc:  0.6484375
train loss:  0.5876410007476807
train gradient:  0.1787879988135368
iteration : 448
train acc:  0.6875
train loss:  0.5833553075790405
train gradient:  0.14848031066040435
iteration : 449
train acc:  0.734375
train loss:  0.5786739587783813
train gradient:  0.12491479157508913
iteration : 450
train acc:  0.65625
train loss:  0.6265997886657715
train gradient:  0.16262584000608168
iteration : 451
train acc:  0.6953125
train loss:  0.5556068420410156
train gradient:  0.14840218392482613
iteration : 452
train acc:  0.6875
train loss:  0.5750980377197266
train gradient:  0.19914840341752277
iteration : 453
train acc:  0.6328125
train loss:  0.6102489829063416
train gradient:  0.1748483306025853
iteration : 454
train acc:  0.65625
train loss:  0.6259509325027466
train gradient:  0.24241024071283268
iteration : 455
train acc:  0.7109375
train loss:  0.556405246257782
train gradient:  0.1537536150854117
iteration : 456
train acc:  0.625
train loss:  0.6207208633422852
train gradient:  0.18387278698888568
iteration : 457
train acc:  0.640625
train loss:  0.6107615232467651
train gradient:  0.1876305542525406
iteration : 458
train acc:  0.7421875
train loss:  0.5702488422393799
train gradient:  0.1347208878527915
iteration : 459
train acc:  0.6484375
train loss:  0.602358341217041
train gradient:  0.12991070695310233
iteration : 460
train acc:  0.703125
train loss:  0.5709570050239563
train gradient:  0.17845625687256444
iteration : 461
train acc:  0.6328125
train loss:  0.6203104257583618
train gradient:  0.13312022280440994
iteration : 462
train acc:  0.640625
train loss:  0.6069348454475403
train gradient:  0.13218043393596424
iteration : 463
train acc:  0.640625
train loss:  0.6471135020256042
train gradient:  0.16885714694025045
iteration : 464
train acc:  0.7421875
train loss:  0.5602956414222717
train gradient:  0.1717323502393709
iteration : 465
train acc:  0.6484375
train loss:  0.5989328622817993
train gradient:  0.14805740493349057
iteration : 466
train acc:  0.65625
train loss:  0.5896520018577576
train gradient:  0.17804260356882526
iteration : 467
train acc:  0.7421875
train loss:  0.5399845838546753
train gradient:  0.13138679154858152
iteration : 468
train acc:  0.6328125
train loss:  0.6521241664886475
train gradient:  0.4872167775994669
iteration : 469
train acc:  0.7109375
train loss:  0.5905235409736633
train gradient:  0.23217761349411858
iteration : 470
train acc:  0.734375
train loss:  0.534172534942627
train gradient:  0.13589652725568308
iteration : 471
train acc:  0.6171875
train loss:  0.6710960865020752
train gradient:  0.19415385518243794
iteration : 472
train acc:  0.671875
train loss:  0.5801956057548523
train gradient:  0.143629810821654
iteration : 473
train acc:  0.6640625
train loss:  0.6005445718765259
train gradient:  0.24236504960027866
iteration : 474
train acc:  0.65625
train loss:  0.5874479413032532
train gradient:  0.16454507482395292
iteration : 475
train acc:  0.65625
train loss:  0.5787635445594788
train gradient:  0.15551803374539575
iteration : 476
train acc:  0.703125
train loss:  0.5859826803207397
train gradient:  0.16101326311320419
iteration : 477
train acc:  0.6875
train loss:  0.5880706906318665
train gradient:  0.16678805104283256
iteration : 478
train acc:  0.6640625
train loss:  0.5737438797950745
train gradient:  0.15661305105377127
iteration : 479
train acc:  0.640625
train loss:  0.5871699452400208
train gradient:  0.1401331100375473
iteration : 480
train acc:  0.71875
train loss:  0.5443397760391235
train gradient:  0.16186507439681608
iteration : 481
train acc:  0.671875
train loss:  0.6431320905685425
train gradient:  0.15669187868191217
iteration : 482
train acc:  0.7421875
train loss:  0.5624229907989502
train gradient:  0.18859577260670604
iteration : 483
train acc:  0.6953125
train loss:  0.5821544528007507
train gradient:  0.16166844027391292
iteration : 484
train acc:  0.6953125
train loss:  0.5685611963272095
train gradient:  0.15346417064338552
iteration : 485
train acc:  0.65625
train loss:  0.6365097761154175
train gradient:  0.21193044108770273
iteration : 486
train acc:  0.671875
train loss:  0.6225709319114685
train gradient:  0.23531693879155713
iteration : 487
train acc:  0.6328125
train loss:  0.6489882469177246
train gradient:  0.17261523651919175
iteration : 488
train acc:  0.6953125
train loss:  0.562038779258728
train gradient:  0.2232920497370087
iteration : 489
train acc:  0.6171875
train loss:  0.6281936168670654
train gradient:  0.19587187446413679
iteration : 490
train acc:  0.6875
train loss:  0.5885987877845764
train gradient:  0.1280067719123742
iteration : 491
train acc:  0.671875
train loss:  0.6291897296905518
train gradient:  0.22289422479317714
iteration : 492
train acc:  0.71875
train loss:  0.5632840394973755
train gradient:  0.14985487389793134
iteration : 493
train acc:  0.6953125
train loss:  0.5954296588897705
train gradient:  0.215027928744481
iteration : 494
train acc:  0.7109375
train loss:  0.5628295540809631
train gradient:  0.20232796944014603
iteration : 495
train acc:  0.6640625
train loss:  0.6177295446395874
train gradient:  0.17495871714050054
iteration : 496
train acc:  0.7734375
train loss:  0.4935455322265625
train gradient:  0.1438960202138393
iteration : 497
train acc:  0.6640625
train loss:  0.5807983875274658
train gradient:  0.1887149533914665
iteration : 498
train acc:  0.71875
train loss:  0.5816003084182739
train gradient:  0.14855068184609382
iteration : 499
train acc:  0.546875
train loss:  0.6487414836883545
train gradient:  0.1744655813663098
iteration : 500
train acc:  0.6015625
train loss:  0.6265020966529846
train gradient:  0.20854898994476706
iteration : 501
train acc:  0.765625
train loss:  0.5543389320373535
train gradient:  0.16779112324271345
iteration : 502
train acc:  0.6328125
train loss:  0.6112791299819946
train gradient:  0.13325440673585762
iteration : 503
train acc:  0.6953125
train loss:  0.573914647102356
train gradient:  0.1352275711742687
iteration : 504
train acc:  0.5859375
train loss:  0.6444337964057922
train gradient:  0.19817831661950427
iteration : 505
train acc:  0.6875
train loss:  0.5687006711959839
train gradient:  0.16644157974724016
iteration : 506
train acc:  0.7109375
train loss:  0.5616816878318787
train gradient:  0.15278078103780623
iteration : 507
train acc:  0.7578125
train loss:  0.5353413224220276
train gradient:  0.15627114293610792
iteration : 508
train acc:  0.6796875
train loss:  0.5814099311828613
train gradient:  0.18393793814174547
iteration : 509
train acc:  0.546875
train loss:  0.6753913164138794
train gradient:  0.21530152251116247
iteration : 510
train acc:  0.734375
train loss:  0.5541967153549194
train gradient:  0.15617486543821807
iteration : 511
train acc:  0.7109375
train loss:  0.579667866230011
train gradient:  0.17598808082790368
iteration : 512
train acc:  0.671875
train loss:  0.6121892929077148
train gradient:  0.1798944332892073
iteration : 513
train acc:  0.671875
train loss:  0.5921916961669922
train gradient:  0.16718067328436537
iteration : 514
train acc:  0.671875
train loss:  0.5659462213516235
train gradient:  0.19480376647005582
iteration : 515
train acc:  0.6171875
train loss:  0.6516916751861572
train gradient:  0.2051354846555885
iteration : 516
train acc:  0.671875
train loss:  0.6372483968734741
train gradient:  0.18763425682306906
iteration : 517
train acc:  0.6640625
train loss:  0.5971251130104065
train gradient:  0.21986131964934136
iteration : 518
train acc:  0.6640625
train loss:  0.5838820934295654
train gradient:  0.1496236278227288
iteration : 519
train acc:  0.65625
train loss:  0.6272701025009155
train gradient:  0.1357085963440734
iteration : 520
train acc:  0.6875
train loss:  0.5769343376159668
train gradient:  0.2248994631287778
iteration : 521
train acc:  0.734375
train loss:  0.5336018204689026
train gradient:  0.13201079039066382
iteration : 522
train acc:  0.703125
train loss:  0.5872443318367004
train gradient:  0.1363182091598266
iteration : 523
train acc:  0.71875
train loss:  0.5687918663024902
train gradient:  0.15699220015504695
iteration : 524
train acc:  0.71875
train loss:  0.5581504106521606
train gradient:  0.14783546936160477
iteration : 525
train acc:  0.6796875
train loss:  0.6017390489578247
train gradient:  0.20729588483323408
iteration : 526
train acc:  0.6328125
train loss:  0.5937589406967163
train gradient:  0.17956133499371552
iteration : 527
train acc:  0.65625
train loss:  0.6155329942703247
train gradient:  0.2645299418645227
iteration : 528
train acc:  0.7421875
train loss:  0.5491135120391846
train gradient:  0.1663237713354316
iteration : 529
train acc:  0.6796875
train loss:  0.5674799680709839
train gradient:  0.1475998320236206
iteration : 530
train acc:  0.7578125
train loss:  0.518173336982727
train gradient:  0.16113555139776073
iteration : 531
train acc:  0.6875
train loss:  0.5799276828765869
train gradient:  0.1660969397720693
iteration : 532
train acc:  0.703125
train loss:  0.5852011442184448
train gradient:  0.18123637821772737
iteration : 533
train acc:  0.703125
train loss:  0.5796782970428467
train gradient:  0.21455168475091926
iteration : 534
train acc:  0.6640625
train loss:  0.5603647232055664
train gradient:  0.14705611080819544
iteration : 535
train acc:  0.625
train loss:  0.6064065098762512
train gradient:  0.1377216458879308
iteration : 536
train acc:  0.7109375
train loss:  0.5618163347244263
train gradient:  0.1888411912419649
iteration : 537
train acc:  0.6640625
train loss:  0.5832565426826477
train gradient:  0.14124848892403602
iteration : 538
train acc:  0.6796875
train loss:  0.5757508277893066
train gradient:  0.14605867831540834
iteration : 539
train acc:  0.7265625
train loss:  0.5477579832077026
train gradient:  0.1456533079097611
iteration : 540
train acc:  0.6953125
train loss:  0.5561847686767578
train gradient:  0.27779559794008046
iteration : 541
train acc:  0.71875
train loss:  0.5586057901382446
train gradient:  0.1719372300339307
iteration : 542
train acc:  0.640625
train loss:  0.608704686164856
train gradient:  0.1803652932158007
iteration : 543
train acc:  0.703125
train loss:  0.5834238529205322
train gradient:  0.1556936312110683
iteration : 544
train acc:  0.625
train loss:  0.6213943958282471
train gradient:  0.2131949070142502
iteration : 545
train acc:  0.6171875
train loss:  0.6371277570724487
train gradient:  0.1775502692113284
iteration : 546
train acc:  0.671875
train loss:  0.5562586784362793
train gradient:  0.13910759489232638
iteration : 547
train acc:  0.71875
train loss:  0.5379050374031067
train gradient:  0.12779175191477674
iteration : 548
train acc:  0.6953125
train loss:  0.570692777633667
train gradient:  0.1799317350343917
iteration : 549
train acc:  0.65625
train loss:  0.5994362235069275
train gradient:  0.18829126197971124
iteration : 550
train acc:  0.7109375
train loss:  0.5597087144851685
train gradient:  0.1819566768705208
iteration : 551
train acc:  0.6875
train loss:  0.5732340812683105
train gradient:  0.16036404329716192
iteration : 552
train acc:  0.7109375
train loss:  0.5968068242073059
train gradient:  0.3125795870946073
iteration : 553
train acc:  0.640625
train loss:  0.6204800605773926
train gradient:  0.17653028931089598
iteration : 554
train acc:  0.671875
train loss:  0.6294854879379272
train gradient:  0.2048720429737685
iteration : 555
train acc:  0.6796875
train loss:  0.5852470397949219
train gradient:  0.12391780649298174
iteration : 556
train acc:  0.6640625
train loss:  0.6178750991821289
train gradient:  0.15123425608089133
iteration : 557
train acc:  0.5859375
train loss:  0.6268627643585205
train gradient:  0.20053178799672813
iteration : 558
train acc:  0.671875
train loss:  0.5962890386581421
train gradient:  0.1621243252779882
iteration : 559
train acc:  0.6640625
train loss:  0.607653021812439
train gradient:  0.3237260130089132
iteration : 560
train acc:  0.6640625
train loss:  0.5789961814880371
train gradient:  0.21224815589885324
iteration : 561
train acc:  0.7265625
train loss:  0.5484192371368408
train gradient:  0.13537883838137404
iteration : 562
train acc:  0.7265625
train loss:  0.5638033151626587
train gradient:  0.2304030083523339
iteration : 563
train acc:  0.6953125
train loss:  0.5708740949630737
train gradient:  0.13787000355663231
iteration : 564
train acc:  0.6875
train loss:  0.570630669593811
train gradient:  0.25108175866974114
iteration : 565
train acc:  0.6171875
train loss:  0.6523783802986145
train gradient:  0.18906441058505813
iteration : 566
train acc:  0.6484375
train loss:  0.6031093597412109
train gradient:  0.14929047069546586
iteration : 567
train acc:  0.6796875
train loss:  0.5892771482467651
train gradient:  0.13553427717927521
iteration : 568
train acc:  0.6171875
train loss:  0.6629525423049927
train gradient:  0.28367879757605186
iteration : 569
train acc:  0.6640625
train loss:  0.6104649305343628
train gradient:  0.17417563938960942
iteration : 570
train acc:  0.6484375
train loss:  0.6050674915313721
train gradient:  0.15429573162006677
iteration : 571
train acc:  0.640625
train loss:  0.6292091012001038
train gradient:  0.16401030130474892
iteration : 572
train acc:  0.6796875
train loss:  0.5517889261245728
train gradient:  0.1277971461548012
iteration : 573
train acc:  0.640625
train loss:  0.5937714576721191
train gradient:  0.22631951670272143
iteration : 574
train acc:  0.71875
train loss:  0.5107574462890625
train gradient:  0.13237461006429846
iteration : 575
train acc:  0.703125
train loss:  0.5958895087242126
train gradient:  0.2236859434771059
iteration : 576
train acc:  0.6796875
train loss:  0.577887237071991
train gradient:  0.16733236077194574
iteration : 577
train acc:  0.6953125
train loss:  0.59070885181427
train gradient:  0.16638463822666497
iteration : 578
train acc:  0.671875
train loss:  0.5686660408973694
train gradient:  0.11641058820016323
iteration : 579
train acc:  0.609375
train loss:  0.6547236442565918
train gradient:  0.16298829715411325
iteration : 580
train acc:  0.6875
train loss:  0.5874884724617004
train gradient:  0.18030922035203178
iteration : 581
train acc:  0.7421875
train loss:  0.5354623794555664
train gradient:  0.1933762887315234
iteration : 582
train acc:  0.703125
train loss:  0.5746424198150635
train gradient:  0.12987298214886922
iteration : 583
train acc:  0.6328125
train loss:  0.6250618696212769
train gradient:  0.18974298097074388
iteration : 584
train acc:  0.6953125
train loss:  0.5747010707855225
train gradient:  0.12943477522312388
iteration : 585
train acc:  0.640625
train loss:  0.6105642318725586
train gradient:  0.15343564710382782
iteration : 586
train acc:  0.703125
train loss:  0.5905410647392273
train gradient:  0.1940166728038796
iteration : 587
train acc:  0.6796875
train loss:  0.5788646936416626
train gradient:  0.1540710372203905
iteration : 588
train acc:  0.65625
train loss:  0.6187161207199097
train gradient:  0.19012522253413927
iteration : 589
train acc:  0.609375
train loss:  0.6337466835975647
train gradient:  0.21843170552761315
iteration : 590
train acc:  0.6640625
train loss:  0.6388854384422302
train gradient:  0.1887950436078078
iteration : 591
train acc:  0.6796875
train loss:  0.5913891196250916
train gradient:  0.16156052061888193
iteration : 592
train acc:  0.6640625
train loss:  0.5973988175392151
train gradient:  0.17884064868832383
iteration : 593
train acc:  0.65625
train loss:  0.6131969690322876
train gradient:  0.1702504278646143
iteration : 594
train acc:  0.6875
train loss:  0.6018399000167847
train gradient:  0.1732041578393807
iteration : 595
train acc:  0.609375
train loss:  0.6180422306060791
train gradient:  0.36317234206392013
iteration : 596
train acc:  0.6640625
train loss:  0.5952284336090088
train gradient:  0.14416074669466206
iteration : 597
train acc:  0.6796875
train loss:  0.5915156006813049
train gradient:  0.127877253811164
iteration : 598
train acc:  0.6875
train loss:  0.610312283039093
train gradient:  0.21325829970071863
iteration : 599
train acc:  0.6875
train loss:  0.5675290822982788
train gradient:  0.12898545688087937
iteration : 600
train acc:  0.6796875
train loss:  0.5843396186828613
train gradient:  0.18132958889717893
iteration : 601
train acc:  0.7109375
train loss:  0.5365480780601501
train gradient:  0.17994349846150814
iteration : 602
train acc:  0.5859375
train loss:  0.6522279977798462
train gradient:  0.29134111059986256
iteration : 603
train acc:  0.6875
train loss:  0.5968047976493835
train gradient:  0.2507753932164064
iteration : 604
train acc:  0.6796875
train loss:  0.5759800672531128
train gradient:  0.14935916040579614
iteration : 605
train acc:  0.6953125
train loss:  0.579132080078125
train gradient:  0.14738710265295468
iteration : 606
train acc:  0.640625
train loss:  0.5804797410964966
train gradient:  0.21247506207486394
iteration : 607
train acc:  0.78125
train loss:  0.5222769975662231
train gradient:  0.14318814903863097
iteration : 608
train acc:  0.7265625
train loss:  0.5455517768859863
train gradient:  0.1442155030390242
iteration : 609
train acc:  0.734375
train loss:  0.5133785009384155
train gradient:  0.14283087072256884
iteration : 610
train acc:  0.7265625
train loss:  0.5526297092437744
train gradient:  0.21071555800969294
iteration : 611
train acc:  0.7578125
train loss:  0.5366196632385254
train gradient:  0.13381143236194717
iteration : 612
train acc:  0.75
train loss:  0.5697466135025024
train gradient:  0.12928904496405885
iteration : 613
train acc:  0.6640625
train loss:  0.6154181361198425
train gradient:  0.15651619957117496
iteration : 614
train acc:  0.6796875
train loss:  0.5682113766670227
train gradient:  0.11672203459673113
iteration : 615
train acc:  0.6171875
train loss:  0.6371526122093201
train gradient:  0.18069522927475282
iteration : 616
train acc:  0.671875
train loss:  0.60731440782547
train gradient:  0.21208370901401255
iteration : 617
train acc:  0.6953125
train loss:  0.575365424156189
train gradient:  0.12824027180931155
iteration : 618
train acc:  0.6875
train loss:  0.5627114176750183
train gradient:  0.17722042816126993
iteration : 619
train acc:  0.703125
train loss:  0.5632532238960266
train gradient:  0.1862116678753027
iteration : 620
train acc:  0.71875
train loss:  0.5641000270843506
train gradient:  0.14333101642742824
iteration : 621
train acc:  0.6953125
train loss:  0.5984097719192505
train gradient:  0.13854922415535464
iteration : 622
train acc:  0.734375
train loss:  0.559364914894104
train gradient:  0.13574848512993462
iteration : 623
train acc:  0.671875
train loss:  0.6222010850906372
train gradient:  0.21123385664786412
iteration : 624
train acc:  0.6171875
train loss:  0.6153111457824707
train gradient:  0.22088730824669497
iteration : 625
train acc:  0.6875
train loss:  0.5710564851760864
train gradient:  0.17271758376306484
iteration : 626
train acc:  0.65625
train loss:  0.5929427742958069
train gradient:  0.1486261496790748
iteration : 627
train acc:  0.703125
train loss:  0.542330801486969
train gradient:  0.2119891573635059
iteration : 628
train acc:  0.71875
train loss:  0.5507562160491943
train gradient:  0.16864165800914754
iteration : 629
train acc:  0.6328125
train loss:  0.6417704820632935
train gradient:  0.19875752128701563
iteration : 630
train acc:  0.640625
train loss:  0.609268844127655
train gradient:  0.15677433761540027
iteration : 631
train acc:  0.625
train loss:  0.626477837562561
train gradient:  0.19651135556285088
iteration : 632
train acc:  0.671875
train loss:  0.6037879586219788
train gradient:  0.21900598265519788
iteration : 633
train acc:  0.6796875
train loss:  0.605017900466919
train gradient:  0.17018145883543545
iteration : 634
train acc:  0.6640625
train loss:  0.5758490562438965
train gradient:  0.16580572379400577
iteration : 635
train acc:  0.671875
train loss:  0.5945154428482056
train gradient:  0.2869911616756643
iteration : 636
train acc:  0.7109375
train loss:  0.5473357439041138
train gradient:  0.13703996175708377
iteration : 637
train acc:  0.6796875
train loss:  0.6236178874969482
train gradient:  0.194188570534573
iteration : 638
train acc:  0.671875
train loss:  0.6177091598510742
train gradient:  0.16038613095126802
iteration : 639
train acc:  0.6953125
train loss:  0.5445647239685059
train gradient:  0.17306278208469056
iteration : 640
train acc:  0.6796875
train loss:  0.5667724609375
train gradient:  0.1473219565226248
iteration : 641
train acc:  0.609375
train loss:  0.6639395952224731
train gradient:  0.2903828362604853
iteration : 642
train acc:  0.6640625
train loss:  0.5910367965698242
train gradient:  0.1726780225552984
iteration : 643
train acc:  0.6484375
train loss:  0.5907777547836304
train gradient:  0.1634466806897335
iteration : 644
train acc:  0.7734375
train loss:  0.5396802425384521
train gradient:  0.1495259488661904
iteration : 645
train acc:  0.65625
train loss:  0.5421788692474365
train gradient:  0.1457310945347162
iteration : 646
train acc:  0.6171875
train loss:  0.6129995584487915
train gradient:  0.17556779212456713
iteration : 647
train acc:  0.6953125
train loss:  0.5821632146835327
train gradient:  0.15600669489780777
iteration : 648
train acc:  0.6875
train loss:  0.5580669641494751
train gradient:  0.19281088671986238
iteration : 649
train acc:  0.703125
train loss:  0.5926253199577332
train gradient:  0.19502618722351997
iteration : 650
train acc:  0.6796875
train loss:  0.6207913160324097
train gradient:  0.16518931250634356
iteration : 651
train acc:  0.703125
train loss:  0.579399824142456
train gradient:  0.16132754159844728
iteration : 652
train acc:  0.7109375
train loss:  0.5762898921966553
train gradient:  0.19598022094490827
iteration : 653
train acc:  0.6875
train loss:  0.6105068922042847
train gradient:  0.447029637032213
iteration : 654
train acc:  0.71875
train loss:  0.5536530017852783
train gradient:  0.20978626113207083
iteration : 655
train acc:  0.59375
train loss:  0.6087360382080078
train gradient:  0.191392814990606
iteration : 656
train acc:  0.6640625
train loss:  0.6053575277328491
train gradient:  0.1503850206237261
iteration : 657
train acc:  0.703125
train loss:  0.5685240030288696
train gradient:  0.20796664847374263
iteration : 658
train acc:  0.640625
train loss:  0.6249357461929321
train gradient:  0.15207522248301114
iteration : 659
train acc:  0.65625
train loss:  0.6040385961532593
train gradient:  0.2226042398436945
iteration : 660
train acc:  0.6796875
train loss:  0.565926194190979
train gradient:  0.16833582464895885
iteration : 661
train acc:  0.734375
train loss:  0.5441808700561523
train gradient:  0.12695005471361778
iteration : 662
train acc:  0.6328125
train loss:  0.6170388460159302
train gradient:  0.14245050106220636
iteration : 663
train acc:  0.6484375
train loss:  0.5811169147491455
train gradient:  0.17308220439601513
iteration : 664
train acc:  0.6171875
train loss:  0.6551468372344971
train gradient:  0.395453844872634
iteration : 665
train acc:  0.6328125
train loss:  0.6358323097229004
train gradient:  0.16860037219074914
iteration : 666
train acc:  0.6640625
train loss:  0.5797417759895325
train gradient:  0.2515211245054727
iteration : 667
train acc:  0.6640625
train loss:  0.6237731575965881
train gradient:  0.39165022757063994
iteration : 668
train acc:  0.7109375
train loss:  0.5864547491073608
train gradient:  0.16228955533359846
iteration : 669
train acc:  0.625
train loss:  0.6163382530212402
train gradient:  0.17145403374606732
iteration : 670
train acc:  0.6484375
train loss:  0.6254048347473145
train gradient:  0.1734692752572275
iteration : 671
train acc:  0.65625
train loss:  0.5923756957054138
train gradient:  0.1829309327866187
iteration : 672
train acc:  0.6640625
train loss:  0.6191577911376953
train gradient:  0.2254004213787596
iteration : 673
train acc:  0.734375
train loss:  0.5821846723556519
train gradient:  0.1327760997600877
iteration : 674
train acc:  0.65625
train loss:  0.6344844102859497
train gradient:  0.20453720602225847
iteration : 675
train acc:  0.6796875
train loss:  0.5608870983123779
train gradient:  0.1610694832751089
iteration : 676
train acc:  0.578125
train loss:  0.6278971433639526
train gradient:  0.23852716352662218
iteration : 677
train acc:  0.6796875
train loss:  0.6036916971206665
train gradient:  0.13820713790261038
iteration : 678
train acc:  0.734375
train loss:  0.5643080472946167
train gradient:  0.13851345806149987
iteration : 679
train acc:  0.640625
train loss:  0.6192803382873535
train gradient:  0.19665256352449673
iteration : 680
train acc:  0.6875
train loss:  0.5988439917564392
train gradient:  0.16762786077893144
iteration : 681
train acc:  0.671875
train loss:  0.5292734503746033
train gradient:  0.130979300578963
iteration : 682
train acc:  0.6953125
train loss:  0.5866868495941162
train gradient:  0.16521276904137416
iteration : 683
train acc:  0.6953125
train loss:  0.5768778324127197
train gradient:  0.11524645932438475
iteration : 684
train acc:  0.703125
train loss:  0.5484304428100586
train gradient:  0.2200810014228884
iteration : 685
train acc:  0.65625
train loss:  0.6022887229919434
train gradient:  0.16107495963519358
iteration : 686
train acc:  0.734375
train loss:  0.5581721067428589
train gradient:  0.28217406246671406
iteration : 687
train acc:  0.625
train loss:  0.6452863812446594
train gradient:  0.19036783564720114
iteration : 688
train acc:  0.6171875
train loss:  0.6558173894882202
train gradient:  0.18442690936125405
iteration : 689
train acc:  0.734375
train loss:  0.5356759428977966
train gradient:  0.14410301049980617
iteration : 690
train acc:  0.578125
train loss:  0.6455541253089905
train gradient:  0.18157302575026077
iteration : 691
train acc:  0.640625
train loss:  0.6105755567550659
train gradient:  0.15032429048048035
iteration : 692
train acc:  0.65625
train loss:  0.594944953918457
train gradient:  0.16884937509164377
iteration : 693
train acc:  0.703125
train loss:  0.5398750901222229
train gradient:  0.14579815003258484
iteration : 694
train acc:  0.6953125
train loss:  0.5752389430999756
train gradient:  0.1435976212203185
iteration : 695
train acc:  0.6796875
train loss:  0.5998457670211792
train gradient:  0.23618243160783653
iteration : 696
train acc:  0.71875
train loss:  0.5375002026557922
train gradient:  0.13468525291811645
iteration : 697
train acc:  0.671875
train loss:  0.5840846300125122
train gradient:  0.1982184066035152
iteration : 698
train acc:  0.6796875
train loss:  0.5943069458007812
train gradient:  0.12893230882680995
iteration : 699
train acc:  0.7578125
train loss:  0.5230882167816162
train gradient:  0.17754761399376123
iteration : 700
train acc:  0.7265625
train loss:  0.5216575264930725
train gradient:  0.13192341607489871
iteration : 701
train acc:  0.7109375
train loss:  0.5423078536987305
train gradient:  0.14257497357553195
iteration : 702
train acc:  0.6640625
train loss:  0.5911737680435181
train gradient:  0.18416867277621407
iteration : 703
train acc:  0.703125
train loss:  0.5434989333152771
train gradient:  0.1671082455755504
iteration : 704
train acc:  0.6796875
train loss:  0.5775065422058105
train gradient:  0.1554727459645917
iteration : 705
train acc:  0.6328125
train loss:  0.6458147764205933
train gradient:  0.21763271306161547
iteration : 706
train acc:  0.671875
train loss:  0.6058175563812256
train gradient:  0.1560678034827795
iteration : 707
train acc:  0.671875
train loss:  0.5737051963806152
train gradient:  0.1944425301339483
iteration : 708
train acc:  0.765625
train loss:  0.5418999195098877
train gradient:  0.19624363072151246
iteration : 709
train acc:  0.7265625
train loss:  0.5930873155593872
train gradient:  0.2294858199978852
iteration : 710
train acc:  0.75
train loss:  0.5435892343521118
train gradient:  0.1651679709850491
iteration : 711
train acc:  0.7265625
train loss:  0.5338243246078491
train gradient:  0.13087232035858593
iteration : 712
train acc:  0.6953125
train loss:  0.5767762660980225
train gradient:  0.15983709849721905
iteration : 713
train acc:  0.6015625
train loss:  0.6510385274887085
train gradient:  0.2470283256286132
iteration : 714
train acc:  0.703125
train loss:  0.5497937202453613
train gradient:  0.13495237182925338
iteration : 715
train acc:  0.609375
train loss:  0.6198418140411377
train gradient:  0.20651876730470597
iteration : 716
train acc:  0.65625
train loss:  0.5737219452857971
train gradient:  0.12667912185396096
iteration : 717
train acc:  0.71875
train loss:  0.5371794700622559
train gradient:  0.19379439118560457
iteration : 718
train acc:  0.6328125
train loss:  0.605204164981842
train gradient:  0.1885909393981569
iteration : 719
train acc:  0.6796875
train loss:  0.5802855491638184
train gradient:  0.17585403195097635
iteration : 720
train acc:  0.6796875
train loss:  0.622341513633728
train gradient:  0.21569732013912812
iteration : 721
train acc:  0.7265625
train loss:  0.5550664663314819
train gradient:  0.150215214597955
iteration : 722
train acc:  0.6328125
train loss:  0.6192117929458618
train gradient:  0.18639358034174883
iteration : 723
train acc:  0.671875
train loss:  0.5923524498939514
train gradient:  0.17311517724913994
iteration : 724
train acc:  0.6484375
train loss:  0.6254348158836365
train gradient:  0.33633692157052403
iteration : 725
train acc:  0.6953125
train loss:  0.5609918832778931
train gradient:  0.12621226500564753
iteration : 726
train acc:  0.6953125
train loss:  0.6077030897140503
train gradient:  0.2827743792691725
iteration : 727
train acc:  0.671875
train loss:  0.5821751356124878
train gradient:  0.1402960651584013
iteration : 728
train acc:  0.6875
train loss:  0.5988777875900269
train gradient:  0.23567395251084156
iteration : 729
train acc:  0.671875
train loss:  0.576970100402832
train gradient:  0.1725107227698965
iteration : 730
train acc:  0.6796875
train loss:  0.5555621385574341
train gradient:  0.15070777153650516
iteration : 731
train acc:  0.6171875
train loss:  0.5936613082885742
train gradient:  0.1819472080979817
iteration : 732
train acc:  0.671875
train loss:  0.5893310904502869
train gradient:  0.15317941870257265
iteration : 733
train acc:  0.75
train loss:  0.5242390632629395
train gradient:  0.1500741707803599
iteration : 734
train acc:  0.6484375
train loss:  0.5789835453033447
train gradient:  0.18256831860969305
iteration : 735
train acc:  0.6875
train loss:  0.5839288234710693
train gradient:  0.18020144236282246
iteration : 736
train acc:  0.671875
train loss:  0.5848000049591064
train gradient:  0.21912124157784296
iteration : 737
train acc:  0.6875
train loss:  0.579109787940979
train gradient:  0.21402867922532154
iteration : 738
train acc:  0.703125
train loss:  0.6203022003173828
train gradient:  0.17466776568929904
iteration : 739
train acc:  0.71875
train loss:  0.5654563903808594
train gradient:  0.15679210638458244
iteration : 740
train acc:  0.6328125
train loss:  0.6130452156066895
train gradient:  0.18556102567626154
iteration : 741
train acc:  0.71875
train loss:  0.5246686935424805
train gradient:  0.12829564599971183
iteration : 742
train acc:  0.6796875
train loss:  0.6071006059646606
train gradient:  0.15538378038626044
iteration : 743
train acc:  0.703125
train loss:  0.5753464698791504
train gradient:  0.24189197302476234
iteration : 744
train acc:  0.625
train loss:  0.6249895095825195
train gradient:  0.1722396559796429
iteration : 745
train acc:  0.71875
train loss:  0.5717315077781677
train gradient:  0.14019292933838345
iteration : 746
train acc:  0.671875
train loss:  0.6242338418960571
train gradient:  0.18503137280890786
iteration : 747
train acc:  0.7109375
train loss:  0.5087223052978516
train gradient:  0.12624248410053468
iteration : 748
train acc:  0.6640625
train loss:  0.5935459136962891
train gradient:  0.2692432179871965
iteration : 749
train acc:  0.6953125
train loss:  0.5676835775375366
train gradient:  0.14806694097340461
iteration : 750
train acc:  0.75
train loss:  0.5157872438430786
train gradient:  0.13301854453958392
iteration : 751
train acc:  0.734375
train loss:  0.554045557975769
train gradient:  0.12973421236674298
iteration : 752
train acc:  0.6953125
train loss:  0.5612808465957642
train gradient:  0.15142370805272418
iteration : 753
train acc:  0.7578125
train loss:  0.5190401077270508
train gradient:  0.13865790899218974
iteration : 754
train acc:  0.6640625
train loss:  0.6074852347373962
train gradient:  0.14754718525097182
iteration : 755
train acc:  0.6328125
train loss:  0.6013635396957397
train gradient:  0.1878110386551873
iteration : 756
train acc:  0.734375
train loss:  0.5586511492729187
train gradient:  0.12593466608824133
iteration : 757
train acc:  0.703125
train loss:  0.5683807730674744
train gradient:  0.15400842369853823
iteration : 758
train acc:  0.703125
train loss:  0.5860447883605957
train gradient:  0.17584044004576543
iteration : 759
train acc:  0.71875
train loss:  0.552852988243103
train gradient:  0.14893195159744993
iteration : 760
train acc:  0.7109375
train loss:  0.591494083404541
train gradient:  0.16997618247655816
iteration : 761
train acc:  0.671875
train loss:  0.5498489141464233
train gradient:  0.18059909481100517
iteration : 762
train acc:  0.671875
train loss:  0.5927212238311768
train gradient:  0.2024766857106929
iteration : 763
train acc:  0.7109375
train loss:  0.5385438203811646
train gradient:  0.14800238200575905
iteration : 764
train acc:  0.6171875
train loss:  0.5949799418449402
train gradient:  0.19268237530796084
iteration : 765
train acc:  0.6484375
train loss:  0.5840355753898621
train gradient:  0.24830257540912237
iteration : 766
train acc:  0.71875
train loss:  0.5626361966133118
train gradient:  0.14789265589308573
iteration : 767
train acc:  0.65625
train loss:  0.5706030130386353
train gradient:  0.18014749648774553
iteration : 768
train acc:  0.6640625
train loss:  0.5867170691490173
train gradient:  0.218596456918228
iteration : 769
train acc:  0.6875
train loss:  0.5846400856971741
train gradient:  0.13921787663292695
iteration : 770
train acc:  0.671875
train loss:  0.5748732089996338
train gradient:  0.18612549090793443
iteration : 771
train acc:  0.6796875
train loss:  0.5926221609115601
train gradient:  0.1498226017211982
iteration : 772
train acc:  0.6875
train loss:  0.5961658954620361
train gradient:  0.15896934883310984
iteration : 773
train acc:  0.5859375
train loss:  0.657050371170044
train gradient:  0.25659505043750813
iteration : 774
train acc:  0.6640625
train loss:  0.6467795968055725
train gradient:  0.20062150679851135
iteration : 775
train acc:  0.703125
train loss:  0.5542065501213074
train gradient:  0.13128724114188337
iteration : 776
train acc:  0.6875
train loss:  0.606050431728363
train gradient:  0.1607267894657663
iteration : 777
train acc:  0.671875
train loss:  0.5464829206466675
train gradient:  0.15904215222588958
iteration : 778
train acc:  0.7421875
train loss:  0.5219095945358276
train gradient:  0.15418554973722798
iteration : 779
train acc:  0.6171875
train loss:  0.6210906505584717
train gradient:  0.1688912438337089
iteration : 780
train acc:  0.6953125
train loss:  0.5423652529716492
train gradient:  0.24725032550550824
iteration : 781
train acc:  0.6796875
train loss:  0.574384331703186
train gradient:  0.1577986042663017
iteration : 782
train acc:  0.6953125
train loss:  0.5679042339324951
train gradient:  0.21991328610751035
iteration : 783
train acc:  0.609375
train loss:  0.6290901899337769
train gradient:  0.1799167464197523
iteration : 784
train acc:  0.6328125
train loss:  0.5610828995704651
train gradient:  0.14012170023413306
iteration : 785
train acc:  0.6875
train loss:  0.6302995681762695
train gradient:  0.25732136750365525
iteration : 786
train acc:  0.7109375
train loss:  0.5779591798782349
train gradient:  0.16775191339675133
iteration : 787
train acc:  0.703125
train loss:  0.5775834321975708
train gradient:  0.16677701331961234
iteration : 788
train acc:  0.7265625
train loss:  0.5312000513076782
train gradient:  0.14002179167137657
iteration : 789
train acc:  0.640625
train loss:  0.6427987813949585
train gradient:  0.205398527191822
iteration : 790
train acc:  0.6953125
train loss:  0.5393525958061218
train gradient:  0.1873122947910149
iteration : 791
train acc:  0.671875
train loss:  0.5941566228866577
train gradient:  0.21697870188803736
iteration : 792
train acc:  0.671875
train loss:  0.6174200177192688
train gradient:  0.15563491569091442
iteration : 793
train acc:  0.71875
train loss:  0.5540002584457397
train gradient:  0.16896957680835925
iteration : 794
train acc:  0.6640625
train loss:  0.5762922763824463
train gradient:  0.1812769610690721
iteration : 795
train acc:  0.703125
train loss:  0.5819835662841797
train gradient:  0.1830816550003676
iteration : 796
train acc:  0.671875
train loss:  0.5860437750816345
train gradient:  0.1917679524808248
iteration : 797
train acc:  0.6796875
train loss:  0.5524734258651733
train gradient:  0.16834924111930716
iteration : 798
train acc:  0.6875
train loss:  0.6202621459960938
train gradient:  0.18782945175035745
iteration : 799
train acc:  0.7265625
train loss:  0.5404670834541321
train gradient:  0.16829283033170747
iteration : 800
train acc:  0.65625
train loss:  0.5925523042678833
train gradient:  0.2047379106288263
iteration : 801
train acc:  0.640625
train loss:  0.5616650581359863
train gradient:  0.2138303930834648
iteration : 802
train acc:  0.65625
train loss:  0.5914945602416992
train gradient:  0.17855499532124708
iteration : 803
train acc:  0.6328125
train loss:  0.6352277398109436
train gradient:  0.1801097913960097
iteration : 804
train acc:  0.6640625
train loss:  0.5953487753868103
train gradient:  0.11781984757371117
iteration : 805
train acc:  0.6953125
train loss:  0.5801286697387695
train gradient:  0.1921472558428896
iteration : 806
train acc:  0.6640625
train loss:  0.6087251305580139
train gradient:  0.15791907794303817
iteration : 807
train acc:  0.609375
train loss:  0.6128467321395874
train gradient:  0.18063841381637585
iteration : 808
train acc:  0.625
train loss:  0.5760113596916199
train gradient:  0.23421572810737484
iteration : 809
train acc:  0.703125
train loss:  0.5597645044326782
train gradient:  0.2222314066904003
iteration : 810
train acc:  0.7109375
train loss:  0.575426459312439
train gradient:  0.1758708503796902
iteration : 811
train acc:  0.734375
train loss:  0.5365533828735352
train gradient:  0.16748429725667308
iteration : 812
train acc:  0.7421875
train loss:  0.5014567375183105
train gradient:  0.12341613732331927
iteration : 813
train acc:  0.6328125
train loss:  0.6017122268676758
train gradient:  0.1846087094700456
iteration : 814
train acc:  0.6640625
train loss:  0.6123713254928589
train gradient:  0.2074698525612158
iteration : 815
train acc:  0.671875
train loss:  0.57667475938797
train gradient:  0.14139775210421554
iteration : 816
train acc:  0.6484375
train loss:  0.6002976894378662
train gradient:  0.21433016316894185
iteration : 817
train acc:  0.6484375
train loss:  0.6118384599685669
train gradient:  0.17580618319229338
iteration : 818
train acc:  0.6953125
train loss:  0.54802405834198
train gradient:  0.1255649512613927
iteration : 819
train acc:  0.6875
train loss:  0.5729981660842896
train gradient:  0.16792393309291748
iteration : 820
train acc:  0.7421875
train loss:  0.5231252908706665
train gradient:  0.1192393024358944
iteration : 821
train acc:  0.75
train loss:  0.5266745686531067
train gradient:  0.13045343824605665
iteration : 822
train acc:  0.6640625
train loss:  0.5725447535514832
train gradient:  0.18266974805680097
iteration : 823
train acc:  0.625
train loss:  0.615330696105957
train gradient:  0.2152964972440618
iteration : 824
train acc:  0.6796875
train loss:  0.5687940120697021
train gradient:  0.1735316442267598
iteration : 825
train acc:  0.671875
train loss:  0.6034115552902222
train gradient:  0.17903118122191747
iteration : 826
train acc:  0.6953125
train loss:  0.5735863447189331
train gradient:  0.19908932123656292
iteration : 827
train acc:  0.6953125
train loss:  0.5435236096382141
train gradient:  0.15495440690448736
iteration : 828
train acc:  0.640625
train loss:  0.5906438827514648
train gradient:  0.13412684418858906
iteration : 829
train acc:  0.6875
train loss:  0.5858123898506165
train gradient:  0.24197726144143972
iteration : 830
train acc:  0.703125
train loss:  0.5478527545928955
train gradient:  0.1235091278426855
iteration : 831
train acc:  0.71875
train loss:  0.4991922974586487
train gradient:  0.14936004194439073
iteration : 832
train acc:  0.65625
train loss:  0.6140229105949402
train gradient:  0.2001653228504499
iteration : 833
train acc:  0.640625
train loss:  0.6486117243766785
train gradient:  0.24366430518432208
iteration : 834
train acc:  0.7109375
train loss:  0.538753092288971
train gradient:  0.15477506981149003
iteration : 835
train acc:  0.6953125
train loss:  0.5522990226745605
train gradient:  0.14629006169237063
iteration : 836
train acc:  0.6875
train loss:  0.5574210286140442
train gradient:  0.13043569255742116
iteration : 837
train acc:  0.6015625
train loss:  0.6502864360809326
train gradient:  0.21588004045289083
iteration : 838
train acc:  0.65625
train loss:  0.5835530161857605
train gradient:  0.1824048191622396
iteration : 839
train acc:  0.7109375
train loss:  0.5405768156051636
train gradient:  0.12373374642951854
iteration : 840
train acc:  0.671875
train loss:  0.6144843101501465
train gradient:  0.15948003410954648
iteration : 841
train acc:  0.6953125
train loss:  0.5786164999008179
train gradient:  0.16787703050279779
iteration : 842
train acc:  0.6328125
train loss:  0.6353359818458557
train gradient:  0.2835624782217563
iteration : 843
train acc:  0.7265625
train loss:  0.5296199917793274
train gradient:  0.11492033691659502
iteration : 844
train acc:  0.7421875
train loss:  0.544892430305481
train gradient:  0.1351002186477117
iteration : 845
train acc:  0.65625
train loss:  0.5933835506439209
train gradient:  0.2884874062288199
iteration : 846
train acc:  0.6640625
train loss:  0.5580223798751831
train gradient:  0.15257620848066308
iteration : 847
train acc:  0.640625
train loss:  0.6003426909446716
train gradient:  0.19362947022224827
iteration : 848
train acc:  0.703125
train loss:  0.5620560050010681
train gradient:  0.1491634250449119
iteration : 849
train acc:  0.703125
train loss:  0.5709182620048523
train gradient:  0.18234489046418603
iteration : 850
train acc:  0.75
train loss:  0.5526460409164429
train gradient:  0.16982885127790504
iteration : 851
train acc:  0.7265625
train loss:  0.5744865536689758
train gradient:  0.21703989137577195
iteration : 852
train acc:  0.6953125
train loss:  0.5480384826660156
train gradient:  0.14466807029094347
iteration : 853
train acc:  0.6875
train loss:  0.5757763385772705
train gradient:  0.15776906594243767
iteration : 854
train acc:  0.71875
train loss:  0.5273451805114746
train gradient:  0.11962721885961379
iteration : 855
train acc:  0.65625
train loss:  0.6300867795944214
train gradient:  0.1510095065130644
iteration : 856
train acc:  0.65625
train loss:  0.5860215425491333
train gradient:  0.17571549818902227
iteration : 857
train acc:  0.6640625
train loss:  0.5816957950592041
train gradient:  0.15374032945657418
iteration : 858
train acc:  0.640625
train loss:  0.604935884475708
train gradient:  0.1813016180336588
iteration : 859
train acc:  0.6484375
train loss:  0.6123727560043335
train gradient:  0.1997535485798646
iteration : 860
train acc:  0.6875
train loss:  0.6057174205780029
train gradient:  0.20678122418104206
iteration : 861
train acc:  0.734375
train loss:  0.5914371013641357
train gradient:  0.25846475536224345
iteration : 862
train acc:  0.7421875
train loss:  0.5489327907562256
train gradient:  0.18321768766570928
iteration : 863
train acc:  0.6015625
train loss:  0.6438248157501221
train gradient:  0.21569699185302563
iteration : 864
train acc:  0.7109375
train loss:  0.5525544881820679
train gradient:  0.15035464371005547
iteration : 865
train acc:  0.703125
train loss:  0.5647505521774292
train gradient:  0.14384681103265495
iteration : 866
train acc:  0.65625
train loss:  0.6024575233459473
train gradient:  0.17150656087230765
iteration : 867
train acc:  0.625
train loss:  0.6059698462486267
train gradient:  0.16583380769522182
iteration : 868
train acc:  0.703125
train loss:  0.5455127954483032
train gradient:  0.14147508845564125
iteration : 869
train acc:  0.71875
train loss:  0.5688771605491638
train gradient:  0.18445330769348506
iteration : 870
train acc:  0.6640625
train loss:  0.6166382431983948
train gradient:  0.2070271668713352
iteration : 871
train acc:  0.6875
train loss:  0.6009795069694519
train gradient:  0.19277262939958423
iteration : 872
train acc:  0.7734375
train loss:  0.5154674649238586
train gradient:  0.22010723706616425
iteration : 873
train acc:  0.703125
train loss:  0.5870761871337891
train gradient:  0.14470251150899488
iteration : 874
train acc:  0.6875
train loss:  0.5813175439834595
train gradient:  0.20705640478381912
iteration : 875
train acc:  0.7265625
train loss:  0.5414254069328308
train gradient:  0.15511891785652748
iteration : 876
train acc:  0.6640625
train loss:  0.6384023427963257
train gradient:  0.3306324477955829
iteration : 877
train acc:  0.75
train loss:  0.5061625838279724
train gradient:  0.13279676204073448
iteration : 878
train acc:  0.7421875
train loss:  0.5310144424438477
train gradient:  0.19273588346246023
iteration : 879
train acc:  0.625
train loss:  0.6276310682296753
train gradient:  0.3012736364790789
iteration : 880
train acc:  0.6875
train loss:  0.5761637091636658
train gradient:  0.1877274427428886
iteration : 881
train acc:  0.7109375
train loss:  0.5559876561164856
train gradient:  0.13934404433431075
iteration : 882
train acc:  0.6484375
train loss:  0.619495153427124
train gradient:  0.2089750843244862
iteration : 883
train acc:  0.703125
train loss:  0.6252255439758301
train gradient:  0.19269631164686757
iteration : 884
train acc:  0.671875
train loss:  0.5762571692466736
train gradient:  0.17548095221325255
iteration : 885
train acc:  0.6484375
train loss:  0.6265623569488525
train gradient:  0.29791102886931925
iteration : 886
train acc:  0.6796875
train loss:  0.5485584139823914
train gradient:  0.14413810943981634
iteration : 887
train acc:  0.734375
train loss:  0.5334488153457642
train gradient:  0.1450184752622104
iteration : 888
train acc:  0.6640625
train loss:  0.6269409656524658
train gradient:  0.23033144690646148
iteration : 889
train acc:  0.6328125
train loss:  0.6082165241241455
train gradient:  0.21466208093813866
iteration : 890
train acc:  0.703125
train loss:  0.5965043306350708
train gradient:  0.1706268697634336
iteration : 891
train acc:  0.6875
train loss:  0.5707735419273376
train gradient:  0.12641367781147195
iteration : 892
train acc:  0.84375
train loss:  0.46067291498184204
train gradient:  0.1332174719026131
iteration : 893
train acc:  0.6953125
train loss:  0.6045037508010864
train gradient:  0.1883214470542211
iteration : 894
train acc:  0.7109375
train loss:  0.5344544649124146
train gradient:  0.1413479975383115
iteration : 895
train acc:  0.6796875
train loss:  0.5853656530380249
train gradient:  0.12319186344346611
iteration : 896
train acc:  0.6953125
train loss:  0.57673579454422
train gradient:  0.20302049096083163
iteration : 897
train acc:  0.6875
train loss:  0.575300931930542
train gradient:  0.15698833092757383
iteration : 898
train acc:  0.765625
train loss:  0.5227518081665039
train gradient:  0.1720379905954484
iteration : 899
train acc:  0.734375
train loss:  0.5133923292160034
train gradient:  0.14873371318997844
iteration : 900
train acc:  0.7109375
train loss:  0.5753034949302673
train gradient:  0.11647142227216196
iteration : 901
train acc:  0.640625
train loss:  0.5907043218612671
train gradient:  0.1427598671939161
iteration : 902
train acc:  0.6640625
train loss:  0.6215289235115051
train gradient:  0.1980492752759453
iteration : 903
train acc:  0.7109375
train loss:  0.5953845381736755
train gradient:  0.20251234912535013
iteration : 904
train acc:  0.78125
train loss:  0.500540554523468
train gradient:  0.16160094824688342
iteration : 905
train acc:  0.734375
train loss:  0.5722019672393799
train gradient:  0.19299735522679556
iteration : 906
train acc:  0.6875
train loss:  0.5792263746261597
train gradient:  0.15022168878946351
iteration : 907
train acc:  0.6875
train loss:  0.6091158390045166
train gradient:  0.19096361061564981
iteration : 908
train acc:  0.671875
train loss:  0.5829312205314636
train gradient:  0.15815652613971576
iteration : 909
train acc:  0.703125
train loss:  0.5684592127799988
train gradient:  0.15554262068010366
iteration : 910
train acc:  0.6640625
train loss:  0.5749897360801697
train gradient:  0.14189938017788065
iteration : 911
train acc:  0.7578125
train loss:  0.5363140106201172
train gradient:  0.16158150754808653
iteration : 912
train acc:  0.625
train loss:  0.6023120880126953
train gradient:  0.14205650493969008
iteration : 913
train acc:  0.6796875
train loss:  0.6118698120117188
train gradient:  0.28782647882201007
iteration : 914
train acc:  0.6875
train loss:  0.5812168717384338
train gradient:  0.21129941826892634
iteration : 915
train acc:  0.7578125
train loss:  0.5071198344230652
train gradient:  0.14642482433458626
iteration : 916
train acc:  0.7109375
train loss:  0.5288493633270264
train gradient:  0.2636455392991253
iteration : 917
train acc:  0.6640625
train loss:  0.5891915559768677
train gradient:  0.15601324282719273
iteration : 918
train acc:  0.6953125
train loss:  0.545854926109314
train gradient:  0.13931052116943105
iteration : 919
train acc:  0.6640625
train loss:  0.6122591495513916
train gradient:  0.16571303690658523
iteration : 920
train acc:  0.703125
train loss:  0.5867459774017334
train gradient:  0.18699967279508165
iteration : 921
train acc:  0.6640625
train loss:  0.5896013975143433
train gradient:  0.1746382236116974
iteration : 922
train acc:  0.7109375
train loss:  0.5894770622253418
train gradient:  0.17444135341501998
iteration : 923
train acc:  0.7265625
train loss:  0.5438189506530762
train gradient:  0.11876128183815457
iteration : 924
train acc:  0.6796875
train loss:  0.5576949119567871
train gradient:  0.13872264990899247
iteration : 925
train acc:  0.6171875
train loss:  0.6161510348320007
train gradient:  0.1566913528003141
iteration : 926
train acc:  0.6796875
train loss:  0.5482836365699768
train gradient:  0.15586686569422498
iteration : 927
train acc:  0.6171875
train loss:  0.5912368893623352
train gradient:  0.14057311196345446
iteration : 928
train acc:  0.703125
train loss:  0.570548951625824
train gradient:  0.13672176319705787
iteration : 929
train acc:  0.625
train loss:  0.5831084251403809
train gradient:  0.1730863670601172
iteration : 930
train acc:  0.7421875
train loss:  0.538711667060852
train gradient:  0.15653865688243962
iteration : 931
train acc:  0.6484375
train loss:  0.5963864326477051
train gradient:  0.1761076904144293
iteration : 932
train acc:  0.6796875
train loss:  0.5828111171722412
train gradient:  0.17125174599850826
iteration : 933
train acc:  0.65625
train loss:  0.6032000780105591
train gradient:  0.21183219263770398
iteration : 934
train acc:  0.7421875
train loss:  0.5253417491912842
train gradient:  0.1512288250637506
iteration : 935
train acc:  0.6484375
train loss:  0.6014829874038696
train gradient:  0.1902186748737768
iteration : 936
train acc:  0.734375
train loss:  0.5267528295516968
train gradient:  0.15810795347330145
iteration : 937
train acc:  0.7109375
train loss:  0.5433032512664795
train gradient:  0.2010832394795572
iteration : 938
train acc:  0.671875
train loss:  0.5644059777259827
train gradient:  0.1757885469765081
iteration : 939
train acc:  0.7421875
train loss:  0.5398577451705933
train gradient:  0.1384799239069783
iteration : 940
train acc:  0.65625
train loss:  0.5616737604141235
train gradient:  0.15886171606062505
iteration : 941
train acc:  0.578125
train loss:  0.6379762887954712
train gradient:  0.19063787855397485
iteration : 942
train acc:  0.65625
train loss:  0.5890853404998779
train gradient:  0.16564362550830714
iteration : 943
train acc:  0.671875
train loss:  0.5823885798454285
train gradient:  0.19481401750526728
iteration : 944
train acc:  0.6796875
train loss:  0.595706045627594
train gradient:  0.2528393003923469
iteration : 945
train acc:  0.75
train loss:  0.527311384677887
train gradient:  0.14521928080217983
iteration : 946
train acc:  0.703125
train loss:  0.5786656737327576
train gradient:  0.1973427260299423
iteration : 947
train acc:  0.7109375
train loss:  0.5218409895896912
train gradient:  0.13762086610179539
iteration : 948
train acc:  0.7109375
train loss:  0.555383563041687
train gradient:  0.15750287477680086
iteration : 949
train acc:  0.640625
train loss:  0.6264196634292603
train gradient:  0.16902173547908877
iteration : 950
train acc:  0.7109375
train loss:  0.580033540725708
train gradient:  0.18524999694513822
iteration : 951
train acc:  0.6640625
train loss:  0.5665412545204163
train gradient:  0.1736856295703837
iteration : 952
train acc:  0.6484375
train loss:  0.5850105285644531
train gradient:  0.1789203975201924
iteration : 953
train acc:  0.7109375
train loss:  0.5400053262710571
train gradient:  0.21446533741760404
iteration : 954
train acc:  0.6171875
train loss:  0.6563661098480225
train gradient:  0.2107444932717518
iteration : 955
train acc:  0.640625
train loss:  0.5866326093673706
train gradient:  0.22456633243356983
iteration : 956
train acc:  0.671875
train loss:  0.5741186141967773
train gradient:  0.15305686838777477
iteration : 957
train acc:  0.765625
train loss:  0.5431345105171204
train gradient:  0.12870298210179731
iteration : 958
train acc:  0.671875
train loss:  0.5730153322219849
train gradient:  0.17988884933131133
iteration : 959
train acc:  0.671875
train loss:  0.6051300764083862
train gradient:  0.3482777734700414
iteration : 960
train acc:  0.671875
train loss:  0.6252530813217163
train gradient:  0.15633422505192243
iteration : 961
train acc:  0.7734375
train loss:  0.5023518204689026
train gradient:  0.15079385424170472
iteration : 962
train acc:  0.671875
train loss:  0.5959211587905884
train gradient:  0.23765038948265388
iteration : 963
train acc:  0.6796875
train loss:  0.5782508254051208
train gradient:  0.15908511030969583
iteration : 964
train acc:  0.6484375
train loss:  0.6063486337661743
train gradient:  0.17475904628068653
iteration : 965
train acc:  0.6796875
train loss:  0.5962770581245422
train gradient:  0.142505595800902
iteration : 966
train acc:  0.6875
train loss:  0.5596158504486084
train gradient:  0.13868798886954206
iteration : 967
train acc:  0.6875
train loss:  0.5485393404960632
train gradient:  0.21936216938110062
iteration : 968
train acc:  0.6953125
train loss:  0.5411060452461243
train gradient:  0.15598828434379788
iteration : 969
train acc:  0.6796875
train loss:  0.5472110509872437
train gradient:  0.16445509406433528
iteration : 970
train acc:  0.734375
train loss:  0.5295484066009521
train gradient:  0.11904259822716999
iteration : 971
train acc:  0.6640625
train loss:  0.5900726318359375
train gradient:  0.24852847131200773
iteration : 972
train acc:  0.71875
train loss:  0.5418890714645386
train gradient:  0.13484006847208896
iteration : 973
train acc:  0.65625
train loss:  0.5613514184951782
train gradient:  0.20416653278184244
iteration : 974
train acc:  0.6171875
train loss:  0.5920686721801758
train gradient:  0.2228391984580724
iteration : 975
train acc:  0.703125
train loss:  0.5452144742012024
train gradient:  0.1342167185223062
iteration : 976
train acc:  0.703125
train loss:  0.5704522132873535
train gradient:  0.19934951173867296
iteration : 977
train acc:  0.65625
train loss:  0.5998733043670654
train gradient:  0.15917804960967527
iteration : 978
train acc:  0.6640625
train loss:  0.5755701661109924
train gradient:  0.21204527927530836
iteration : 979
train acc:  0.75
train loss:  0.5221738815307617
train gradient:  0.1424316662831165
iteration : 980
train acc:  0.7421875
train loss:  0.5209075212478638
train gradient:  0.15641473790097898
iteration : 981
train acc:  0.703125
train loss:  0.5525249242782593
train gradient:  0.14038675668918718
iteration : 982
train acc:  0.6796875
train loss:  0.5762245655059814
train gradient:  0.1723210169828256
iteration : 983
train acc:  0.65625
train loss:  0.5563666224479675
train gradient:  0.18897752466003492
iteration : 984
train acc:  0.703125
train loss:  0.5184578895568848
train gradient:  0.1412818267379524
iteration : 985
train acc:  0.7265625
train loss:  0.5227752923965454
train gradient:  0.13352225803960796
iteration : 986
train acc:  0.734375
train loss:  0.5410059690475464
train gradient:  0.1967097515861205
iteration : 987
train acc:  0.7734375
train loss:  0.5084972381591797
train gradient:  0.11559248645077205
iteration : 988
train acc:  0.6875
train loss:  0.5617363452911377
train gradient:  0.1812096377157691
iteration : 989
train acc:  0.6953125
train loss:  0.5564143657684326
train gradient:  0.18626195871218937
iteration : 990
train acc:  0.765625
train loss:  0.5055495500564575
train gradient:  0.1478970804722171
iteration : 991
train acc:  0.703125
train loss:  0.5291924476623535
train gradient:  0.21470718547155548
iteration : 992
train acc:  0.71875
train loss:  0.5560022592544556
train gradient:  0.14727108410037532
iteration : 993
train acc:  0.65625
train loss:  0.5799683332443237
train gradient:  0.19940490213817896
iteration : 994
train acc:  0.71875
train loss:  0.562606155872345
train gradient:  0.18557883898482297
iteration : 995
train acc:  0.7109375
train loss:  0.5650625228881836
train gradient:  0.15786265986521006
iteration : 996
train acc:  0.640625
train loss:  0.6067886352539062
train gradient:  0.18398676735524105
iteration : 997
train acc:  0.6484375
train loss:  0.5941231846809387
train gradient:  0.2547329282467242
iteration : 998
train acc:  0.703125
train loss:  0.5491642951965332
train gradient:  0.18313710424229324
iteration : 999
train acc:  0.6640625
train loss:  0.5689203143119812
train gradient:  0.19044126630397645
iteration : 1000
train acc:  0.6015625
train loss:  0.6405184864997864
train gradient:  0.23918494358556647
iteration : 1001
train acc:  0.671875
train loss:  0.5986577272415161
train gradient:  0.1541761699800455
iteration : 1002
train acc:  0.6484375
train loss:  0.5944169163703918
train gradient:  0.21175625696268016
iteration : 1003
train acc:  0.75
train loss:  0.5083706974983215
train gradient:  0.12315574074866273
iteration : 1004
train acc:  0.71875
train loss:  0.5860114097595215
train gradient:  0.2336976652293275
iteration : 1005
train acc:  0.8203125
train loss:  0.47961872816085815
train gradient:  0.14954405369212867
iteration : 1006
train acc:  0.75
train loss:  0.5099245309829712
train gradient:  0.12862297920043553
iteration : 1007
train acc:  0.625
train loss:  0.5868412256240845
train gradient:  0.21241157905660996
iteration : 1008
train acc:  0.671875
train loss:  0.5654851794242859
train gradient:  0.17400839831251036
iteration : 1009
train acc:  0.6953125
train loss:  0.5606013536453247
train gradient:  0.1707279546747535
iteration : 1010
train acc:  0.671875
train loss:  0.5884902477264404
train gradient:  0.20533064966375644
iteration : 1011
train acc:  0.7578125
train loss:  0.5484897494316101
train gradient:  0.195078488595619
iteration : 1012
train acc:  0.796875
train loss:  0.5287479162216187
train gradient:  0.17156117842781307
iteration : 1013
train acc:  0.71875
train loss:  0.5421922206878662
train gradient:  0.13137318484944366
iteration : 1014
train acc:  0.609375
train loss:  0.6955264210700989
train gradient:  0.28586522493473804
iteration : 1015
train acc:  0.7109375
train loss:  0.5828073024749756
train gradient:  0.21143102224197513
iteration : 1016
train acc:  0.71875
train loss:  0.5650577545166016
train gradient:  0.16655137237644901
iteration : 1017
train acc:  0.6640625
train loss:  0.6116713881492615
train gradient:  0.23571294547303512
iteration : 1018
train acc:  0.6484375
train loss:  0.6294512748718262
train gradient:  0.1669104597924218
iteration : 1019
train acc:  0.7265625
train loss:  0.5645639300346375
train gradient:  0.19769702517027984
iteration : 1020
train acc:  0.6796875
train loss:  0.5778680443763733
train gradient:  0.22665922475550448
iteration : 1021
train acc:  0.609375
train loss:  0.6199126243591309
train gradient:  0.2834932060516009
iteration : 1022
train acc:  0.640625
train loss:  0.6011263132095337
train gradient:  0.17896357992173764
iteration : 1023
train acc:  0.71875
train loss:  0.5461652278900146
train gradient:  0.16053141579001168
iteration : 1024
train acc:  0.6484375
train loss:  0.5947723388671875
train gradient:  0.20553394865848737
iteration : 1025
train acc:  0.6953125
train loss:  0.5746515989303589
train gradient:  0.19917874576725808
iteration : 1026
train acc:  0.703125
train loss:  0.5450642108917236
train gradient:  0.1714139648635059
iteration : 1027
train acc:  0.640625
train loss:  0.6073634624481201
train gradient:  0.21435722210823216
iteration : 1028
train acc:  0.703125
train loss:  0.5521284937858582
train gradient:  0.16590070205250257
iteration : 1029
train acc:  0.640625
train loss:  0.5960490107536316
train gradient:  0.16395281572087364
iteration : 1030
train acc:  0.6015625
train loss:  0.6553800702095032
train gradient:  0.2907722127118891
iteration : 1031
train acc:  0.7265625
train loss:  0.5364220142364502
train gradient:  0.14470650686167308
iteration : 1032
train acc:  0.6640625
train loss:  0.5989964604377747
train gradient:  0.16558564574029155
iteration : 1033
train acc:  0.703125
train loss:  0.5724260807037354
train gradient:  0.1422914461113532
iteration : 1034
train acc:  0.7109375
train loss:  0.5252429246902466
train gradient:  0.13562140542888257
iteration : 1035
train acc:  0.7578125
train loss:  0.518947958946228
train gradient:  0.1515601445812927
iteration : 1036
train acc:  0.703125
train loss:  0.547152042388916
train gradient:  0.14335553409825696
iteration : 1037
train acc:  0.6875
train loss:  0.5622228384017944
train gradient:  0.1979328799724379
iteration : 1038
train acc:  0.75
train loss:  0.5090230107307434
train gradient:  0.14765058045051424
iteration : 1039
train acc:  0.65625
train loss:  0.5868251323699951
train gradient:  0.21028490238018127
iteration : 1040
train acc:  0.6875
train loss:  0.5531865954399109
train gradient:  0.15829593742362857
iteration : 1041
train acc:  0.734375
train loss:  0.5569061040878296
train gradient:  0.14782331783997793
iteration : 1042
train acc:  0.71875
train loss:  0.5616327524185181
train gradient:  0.16570642151579784
iteration : 1043
train acc:  0.7734375
train loss:  0.532895565032959
train gradient:  0.2091826439215847
iteration : 1044
train acc:  0.65625
train loss:  0.6236507892608643
train gradient:  0.2375450908152918
iteration : 1045
train acc:  0.59375
train loss:  0.6592671871185303
train gradient:  0.2684066618069245
iteration : 1046
train acc:  0.6953125
train loss:  0.540205717086792
train gradient:  0.2046241325070965
iteration : 1047
train acc:  0.71875
train loss:  0.544611930847168
train gradient:  0.18013965543750815
iteration : 1048
train acc:  0.703125
train loss:  0.5565275549888611
train gradient:  0.19054050499707287
iteration : 1049
train acc:  0.8046875
train loss:  0.4748958349227905
train gradient:  0.15520480131019904
iteration : 1050
train acc:  0.6875
train loss:  0.5530755519866943
train gradient:  0.18981065112243445
iteration : 1051
train acc:  0.71875
train loss:  0.557992696762085
train gradient:  0.13001018803493636
iteration : 1052
train acc:  0.703125
train loss:  0.5515327453613281
train gradient:  0.16328660797976266
iteration : 1053
train acc:  0.7265625
train loss:  0.5475437641143799
train gradient:  0.15354557600502788
iteration : 1054
train acc:  0.6328125
train loss:  0.6247057914733887
train gradient:  0.25218618731424997
iteration : 1055
train acc:  0.71875
train loss:  0.5197311639785767
train gradient:  0.11257668147839167
iteration : 1056
train acc:  0.7109375
train loss:  0.5417778491973877
train gradient:  0.16234377699299965
iteration : 1057
train acc:  0.71875
train loss:  0.5413358211517334
train gradient:  0.12633681376741607
iteration : 1058
train acc:  0.65625
train loss:  0.6725641489028931
train gradient:  0.2178104742484885
iteration : 1059
train acc:  0.703125
train loss:  0.5465594530105591
train gradient:  0.2716282817536664
iteration : 1060
train acc:  0.703125
train loss:  0.5590308904647827
train gradient:  0.17796735168854383
iteration : 1061
train acc:  0.6796875
train loss:  0.5966300368309021
train gradient:  0.16178596891658986
iteration : 1062
train acc:  0.6953125
train loss:  0.567667543888092
train gradient:  0.28283392725473444
iteration : 1063
train acc:  0.7109375
train loss:  0.5325219631195068
train gradient:  0.21592929756967968
iteration : 1064
train acc:  0.7265625
train loss:  0.5368157029151917
train gradient:  0.17476231297879552
iteration : 1065
train acc:  0.6328125
train loss:  0.591256856918335
train gradient:  0.17940093379515887
iteration : 1066
train acc:  0.734375
train loss:  0.5033611059188843
train gradient:  0.1257312767729465
iteration : 1067
train acc:  0.671875
train loss:  0.5846218466758728
train gradient:  0.20717399337685033
iteration : 1068
train acc:  0.71875
train loss:  0.5233105421066284
train gradient:  0.1350177817953421
iteration : 1069
train acc:  0.625
train loss:  0.6459482908248901
train gradient:  0.24976597951531393
iteration : 1070
train acc:  0.671875
train loss:  0.5668424963951111
train gradient:  0.16770574813615974
iteration : 1071
train acc:  0.7734375
train loss:  0.5115073919296265
train gradient:  0.15867751892764698
iteration : 1072
train acc:  0.6953125
train loss:  0.5434858202934265
train gradient:  0.12737880347231317
iteration : 1073
train acc:  0.6328125
train loss:  0.5959858894348145
train gradient:  0.14336726567291136
iteration : 1074
train acc:  0.703125
train loss:  0.5839557647705078
train gradient:  0.2971267695205556
iteration : 1075
train acc:  0.6328125
train loss:  0.6122031807899475
train gradient:  0.3014793908117138
iteration : 1076
train acc:  0.71875
train loss:  0.5710266828536987
train gradient:  0.18377837542448985
iteration : 1077
train acc:  0.71875
train loss:  0.5442161560058594
train gradient:  0.13689525930865087
iteration : 1078
train acc:  0.6640625
train loss:  0.6207102537155151
train gradient:  0.20500325936010505
iteration : 1079
train acc:  0.734375
train loss:  0.5370384454727173
train gradient:  0.1256067954808231
iteration : 1080
train acc:  0.7265625
train loss:  0.4987703561782837
train gradient:  0.15598169813506302
iteration : 1081
train acc:  0.734375
train loss:  0.5215564966201782
train gradient:  0.15558001855482237
iteration : 1082
train acc:  0.8203125
train loss:  0.4925873279571533
train gradient:  0.15062426999631356
iteration : 1083
train acc:  0.6484375
train loss:  0.6703557968139648
train gradient:  0.2674928114194854
iteration : 1084
train acc:  0.625
train loss:  0.646761953830719
train gradient:  0.23761901064831664
iteration : 1085
train acc:  0.6953125
train loss:  0.5344146490097046
train gradient:  0.2542119323600395
iteration : 1086
train acc:  0.7109375
train loss:  0.5545576810836792
train gradient:  0.1534453001859162
iteration : 1087
train acc:  0.6875
train loss:  0.5592293739318848
train gradient:  0.17448360611026875
iteration : 1088
train acc:  0.671875
train loss:  0.5583209991455078
train gradient:  0.141273329509501
iteration : 1089
train acc:  0.75
train loss:  0.5149863958358765
train gradient:  0.14812941775411748
iteration : 1090
train acc:  0.671875
train loss:  0.6028420925140381
train gradient:  0.17065627572455083
iteration : 1091
train acc:  0.71875
train loss:  0.547266960144043
train gradient:  0.19949686851363346
iteration : 1092
train acc:  0.6953125
train loss:  0.5657445192337036
train gradient:  0.16796233752024392
iteration : 1093
train acc:  0.65625
train loss:  0.611291766166687
train gradient:  0.19380698264680035
iteration : 1094
train acc:  0.8046875
train loss:  0.4749186635017395
train gradient:  0.16370295740464624
iteration : 1095
train acc:  0.7109375
train loss:  0.5621711015701294
train gradient:  0.16553616542563188
iteration : 1096
train acc:  0.65625
train loss:  0.6102660894393921
train gradient:  0.24016370104596452
iteration : 1097
train acc:  0.703125
train loss:  0.5500108599662781
train gradient:  0.1426053277935326
iteration : 1098
train acc:  0.6171875
train loss:  0.6323353052139282
train gradient:  0.23930741031298353
iteration : 1099
train acc:  0.65625
train loss:  0.5460425615310669
train gradient:  0.18733715004542734
iteration : 1100
train acc:  0.65625
train loss:  0.6208770871162415
train gradient:  0.25173978986511775
iteration : 1101
train acc:  0.75
train loss:  0.552049994468689
train gradient:  0.14155541326540505
iteration : 1102
train acc:  0.6640625
train loss:  0.5841989517211914
train gradient:  0.18141964433891172
iteration : 1103
train acc:  0.75
train loss:  0.5025293231010437
train gradient:  0.1554522344575222
iteration : 1104
train acc:  0.6796875
train loss:  0.5876266360282898
train gradient:  0.16689997047298555
iteration : 1105
train acc:  0.7109375
train loss:  0.5528432130813599
train gradient:  0.17759689460939787
iteration : 1106
train acc:  0.7109375
train loss:  0.5587178468704224
train gradient:  0.17092225955177154
iteration : 1107
train acc:  0.734375
train loss:  0.5264732241630554
train gradient:  0.1854698397619067
iteration : 1108
train acc:  0.625
train loss:  0.6095471382141113
train gradient:  0.24096869538466403
iteration : 1109
train acc:  0.65625
train loss:  0.6311982870101929
train gradient:  0.24873620776365385
iteration : 1110
train acc:  0.7421875
train loss:  0.5329366326332092
train gradient:  0.20348577079989183
iteration : 1111
train acc:  0.703125
train loss:  0.5493190884590149
train gradient:  0.17967601891442533
iteration : 1112
train acc:  0.65625
train loss:  0.6255708932876587
train gradient:  0.17896060707160918
iteration : 1113
train acc:  0.7109375
train loss:  0.5367422103881836
train gradient:  0.1656230323956918
iteration : 1114
train acc:  0.703125
train loss:  0.5834618806838989
train gradient:  0.18996067388414883
iteration : 1115
train acc:  0.6953125
train loss:  0.5738710165023804
train gradient:  0.2160579089491828
iteration : 1116
train acc:  0.6640625
train loss:  0.6028211116790771
train gradient:  0.1599616160395131
iteration : 1117
train acc:  0.6796875
train loss:  0.6043181419372559
train gradient:  0.1826026089715243
iteration : 1118
train acc:  0.6796875
train loss:  0.5810515880584717
train gradient:  0.1802628979756056
iteration : 1119
train acc:  0.65625
train loss:  0.5817514061927795
train gradient:  0.2328452985476695
iteration : 1120
train acc:  0.71875
train loss:  0.5186148881912231
train gradient:  0.17544323925485866
iteration : 1121
train acc:  0.671875
train loss:  0.5886679887771606
train gradient:  0.16356237843738092
iteration : 1122
train acc:  0.6640625
train loss:  0.6084144115447998
train gradient:  0.18269407671249413
iteration : 1123
train acc:  0.640625
train loss:  0.6020132303237915
train gradient:  0.17146513186030482
iteration : 1124
train acc:  0.703125
train loss:  0.553953230381012
train gradient:  0.17082697095254834
iteration : 1125
train acc:  0.7421875
train loss:  0.5196669101715088
train gradient:  0.12921187378055327
iteration : 1126
train acc:  0.671875
train loss:  0.5645291805267334
train gradient:  0.20625082230799685
iteration : 1127
train acc:  0.609375
train loss:  0.6136035919189453
train gradient:  0.20613215601875
iteration : 1128
train acc:  0.6484375
train loss:  0.6080124378204346
train gradient:  0.16439903054651242
iteration : 1129
train acc:  0.6796875
train loss:  0.5888352990150452
train gradient:  0.16023274432988227
iteration : 1130
train acc:  0.7265625
train loss:  0.5055604577064514
train gradient:  0.27114728355371015
iteration : 1131
train acc:  0.7265625
train loss:  0.5499685406684875
train gradient:  0.16818176673858964
iteration : 1132
train acc:  0.6875
train loss:  0.5628117918968201
train gradient:  0.19228097995067583
iteration : 1133
train acc:  0.6796875
train loss:  0.5341095924377441
train gradient:  0.19037021790035996
iteration : 1134
train acc:  0.65625
train loss:  0.5737991333007812
train gradient:  0.13828629534484246
iteration : 1135
train acc:  0.75
train loss:  0.4970093369483948
train gradient:  0.1676200818652971
iteration : 1136
train acc:  0.6171875
train loss:  0.5971328616142273
train gradient:  0.18986482797567691
iteration : 1137
train acc:  0.6953125
train loss:  0.5540772676467896
train gradient:  0.1798551755923466
iteration : 1138
train acc:  0.765625
train loss:  0.5357664227485657
train gradient:  0.1541431034073933
iteration : 1139
train acc:  0.7421875
train loss:  0.5276750326156616
train gradient:  0.12878318548756645
iteration : 1140
train acc:  0.6953125
train loss:  0.546970009803772
train gradient:  0.1490241065742724
iteration : 1141
train acc:  0.703125
train loss:  0.5643348097801208
train gradient:  0.18198403643330308
iteration : 1142
train acc:  0.6015625
train loss:  0.654899001121521
train gradient:  0.3858471418828602
iteration : 1143
train acc:  0.71875
train loss:  0.5401475429534912
train gradient:  0.1532122456822097
iteration : 1144
train acc:  0.65625
train loss:  0.5965818166732788
train gradient:  0.18208066457004307
iteration : 1145
train acc:  0.75
train loss:  0.4971844255924225
train gradient:  0.15165403808876315
iteration : 1146
train acc:  0.703125
train loss:  0.5660306215286255
train gradient:  0.18024254363239828
iteration : 1147
train acc:  0.7890625
train loss:  0.49417778849601746
train gradient:  0.13508287070841188
iteration : 1148
train acc:  0.6796875
train loss:  0.5940916538238525
train gradient:  0.19002715760758873
iteration : 1149
train acc:  0.6953125
train loss:  0.5398221611976624
train gradient:  0.14064210720859643
iteration : 1150
train acc:  0.6953125
train loss:  0.5571061372756958
train gradient:  0.2245548444792354
iteration : 1151
train acc:  0.71875
train loss:  0.5535421967506409
train gradient:  0.16142920725828597
iteration : 1152
train acc:  0.6796875
train loss:  0.5785908699035645
train gradient:  0.18439437131531367
iteration : 1153
train acc:  0.703125
train loss:  0.5566648244857788
train gradient:  0.128478241781176
iteration : 1154
train acc:  0.65625
train loss:  0.5956230759620667
train gradient:  0.16078054581659487
iteration : 1155
train acc:  0.7109375
train loss:  0.5303204655647278
train gradient:  0.14201951304105706
iteration : 1156
train acc:  0.6953125
train loss:  0.6217019557952881
train gradient:  0.2208657028958426
iteration : 1157
train acc:  0.6484375
train loss:  0.6239626407623291
train gradient:  0.21553289973979073
iteration : 1158
train acc:  0.7109375
train loss:  0.5538343191146851
train gradient:  0.15997598625265416
iteration : 1159
train acc:  0.6796875
train loss:  0.5863316655158997
train gradient:  0.14420860471968994
iteration : 1160
train acc:  0.6953125
train loss:  0.5267960429191589
train gradient:  0.16595271341246007
iteration : 1161
train acc:  0.7109375
train loss:  0.5463606119155884
train gradient:  0.1358339229941288
iteration : 1162
train acc:  0.6484375
train loss:  0.5889403820037842
train gradient:  0.18486266942251156
iteration : 1163
train acc:  0.6015625
train loss:  0.6293405294418335
train gradient:  0.22299071309768037
iteration : 1164
train acc:  0.6875
train loss:  0.5691245794296265
train gradient:  0.1597612306697879
iteration : 1165
train acc:  0.6484375
train loss:  0.5978726148605347
train gradient:  0.2198702257257495
iteration : 1166
train acc:  0.609375
train loss:  0.6291174292564392
train gradient:  0.22517728014891306
iteration : 1167
train acc:  0.671875
train loss:  0.5885066390037537
train gradient:  0.1821125946613627
iteration : 1168
train acc:  0.6953125
train loss:  0.6115307211875916
train gradient:  0.15867679536336976
iteration : 1169
train acc:  0.7109375
train loss:  0.5846824049949646
train gradient:  0.1694087238647845
iteration : 1170
train acc:  0.6796875
train loss:  0.5928938984870911
train gradient:  0.17712343920099047
iteration : 1171
train acc:  0.703125
train loss:  0.6040905117988586
train gradient:  0.2784548860563792
iteration : 1172
train acc:  0.6875
train loss:  0.534136176109314
train gradient:  0.1701109246079094
iteration : 1173
train acc:  0.65625
train loss:  0.6218969821929932
train gradient:  0.284247312573988
iteration : 1174
train acc:  0.734375
train loss:  0.5295426845550537
train gradient:  0.16260423615625735
iteration : 1175
train acc:  0.671875
train loss:  0.5508183240890503
train gradient:  0.19220648212091007
iteration : 1176
train acc:  0.6875
train loss:  0.5792524814605713
train gradient:  0.18353258999783073
iteration : 1177
train acc:  0.7109375
train loss:  0.5488432049751282
train gradient:  0.16324356999405132
iteration : 1178
train acc:  0.6953125
train loss:  0.5597735643386841
train gradient:  0.16107591860905557
iteration : 1179
train acc:  0.65625
train loss:  0.6300523281097412
train gradient:  0.20656520252219257
iteration : 1180
train acc:  0.609375
train loss:  0.6423007249832153
train gradient:  0.21617471035365238
iteration : 1181
train acc:  0.6875
train loss:  0.5895686745643616
train gradient:  0.22026033659211608
iteration : 1182
train acc:  0.734375
train loss:  0.5556793212890625
train gradient:  0.1855212015676135
iteration : 1183
train acc:  0.671875
train loss:  0.5615749359130859
train gradient:  0.14510068725804492
iteration : 1184
train acc:  0.703125
train loss:  0.5984733700752258
train gradient:  0.16749412387284626
iteration : 1185
train acc:  0.796875
train loss:  0.4995001554489136
train gradient:  0.16809592498097992
iteration : 1186
train acc:  0.6875
train loss:  0.5856417417526245
train gradient:  0.1876415650977481
iteration : 1187
train acc:  0.6875
train loss:  0.5510566234588623
train gradient:  0.12818300928495693
iteration : 1188
train acc:  0.7421875
train loss:  0.5251030325889587
train gradient:  0.16765156884562876
iteration : 1189
train acc:  0.71875
train loss:  0.525098979473114
train gradient:  0.12336589020254564
iteration : 1190
train acc:  0.7578125
train loss:  0.5143126249313354
train gradient:  0.2007607440086586
iteration : 1191
train acc:  0.6953125
train loss:  0.5507002472877502
train gradient:  0.1286526720604193
iteration : 1192
train acc:  0.671875
train loss:  0.5934242010116577
train gradient:  0.1986180243733944
iteration : 1193
train acc:  0.6640625
train loss:  0.6416977643966675
train gradient:  0.19849256612759525
iteration : 1194
train acc:  0.671875
train loss:  0.5802855491638184
train gradient:  0.15990206133215767
iteration : 1195
train acc:  0.6171875
train loss:  0.6873234510421753
train gradient:  0.29729325039727666
iteration : 1196
train acc:  0.6875
train loss:  0.5617533922195435
train gradient:  0.28930650337309616
iteration : 1197
train acc:  0.71875
train loss:  0.5559655427932739
train gradient:  0.17826689425096243
iteration : 1198
train acc:  0.6484375
train loss:  0.6289111971855164
train gradient:  0.19045087752895434
iteration : 1199
train acc:  0.7890625
train loss:  0.5484424829483032
train gradient:  0.1462276919461739
iteration : 1200
train acc:  0.6953125
train loss:  0.6068224906921387
train gradient:  0.17723656455215217
iteration : 1201
train acc:  0.6640625
train loss:  0.5975663065910339
train gradient:  0.1919633061391446
iteration : 1202
train acc:  0.703125
train loss:  0.5778299570083618
train gradient:  0.21627083746535009
iteration : 1203
train acc:  0.75
train loss:  0.5316054224967957
train gradient:  0.15788830514199786
iteration : 1204
train acc:  0.671875
train loss:  0.5840436220169067
train gradient:  0.18906573941752475
iteration : 1205
train acc:  0.6953125
train loss:  0.5554435849189758
train gradient:  0.16188899048723152
iteration : 1206
train acc:  0.6875
train loss:  0.596023678779602
train gradient:  0.15386687071363359
iteration : 1207
train acc:  0.6796875
train loss:  0.5426804423332214
train gradient:  0.1411446421674884
iteration : 1208
train acc:  0.65625
train loss:  0.5980066061019897
train gradient:  0.18712089399652632
iteration : 1209
train acc:  0.6953125
train loss:  0.5388565063476562
train gradient:  0.2788658212091913
iteration : 1210
train acc:  0.6953125
train loss:  0.5542776584625244
train gradient:  0.14461894992449503
iteration : 1211
train acc:  0.7265625
train loss:  0.5628745555877686
train gradient:  0.197415814410195
iteration : 1212
train acc:  0.71875
train loss:  0.5574604868888855
train gradient:  0.23736900540489383
iteration : 1213
train acc:  0.671875
train loss:  0.5677168369293213
train gradient:  0.16569768153214964
iteration : 1214
train acc:  0.765625
train loss:  0.5321581363677979
train gradient:  0.12494761349668373
iteration : 1215
train acc:  0.703125
train loss:  0.5575951337814331
train gradient:  0.1832879068939613
iteration : 1216
train acc:  0.65625
train loss:  0.5846283435821533
train gradient:  0.1862945678797813
iteration : 1217
train acc:  0.75
train loss:  0.5189733505249023
train gradient:  0.13265381082278582
iteration : 1218
train acc:  0.6328125
train loss:  0.6086969375610352
train gradient:  0.2599727459329883
iteration : 1219
train acc:  0.7109375
train loss:  0.5690173506736755
train gradient:  0.18868699253850876
iteration : 1220
train acc:  0.6953125
train loss:  0.5638655424118042
train gradient:  0.14610766644306006
iteration : 1221
train acc:  0.7109375
train loss:  0.5579418540000916
train gradient:  0.1338189745402772
iteration : 1222
train acc:  0.6328125
train loss:  0.6001964807510376
train gradient:  0.24334017120436685
iteration : 1223
train acc:  0.71875
train loss:  0.5196064710617065
train gradient:  0.15875211125600738
iteration : 1224
train acc:  0.671875
train loss:  0.5838593244552612
train gradient:  0.146240069042475
iteration : 1225
train acc:  0.65625
train loss:  0.5799732208251953
train gradient:  0.15799677397312584
iteration : 1226
train acc:  0.78125
train loss:  0.5207814574241638
train gradient:  0.15565811610060515
iteration : 1227
train acc:  0.6484375
train loss:  0.5800668001174927
train gradient:  0.19497572176103142
iteration : 1228
train acc:  0.7265625
train loss:  0.5492233037948608
train gradient:  0.15242824763151755
iteration : 1229
train acc:  0.671875
train loss:  0.6057196855545044
train gradient:  0.1415355500529128
iteration : 1230
train acc:  0.671875
train loss:  0.6244136095046997
train gradient:  0.24716421169443353
iteration : 1231
train acc:  0.734375
train loss:  0.5432036519050598
train gradient:  0.17301119640077434
iteration : 1232
train acc:  0.7265625
train loss:  0.514105498790741
train gradient:  0.2466151846700529
iteration : 1233
train acc:  0.703125
train loss:  0.589830219745636
train gradient:  0.14691932832553398
iteration : 1234
train acc:  0.6875
train loss:  0.5513733625411987
train gradient:  0.1481212913470622
iteration : 1235
train acc:  0.7109375
train loss:  0.5673619508743286
train gradient:  0.1603949833581249
iteration : 1236
train acc:  0.7421875
train loss:  0.5259539484977722
train gradient:  0.1710914932287554
iteration : 1237
train acc:  0.75
train loss:  0.5139861106872559
train gradient:  0.20011990736300683
iteration : 1238
train acc:  0.625
train loss:  0.6092957258224487
train gradient:  0.16915635406477725
iteration : 1239
train acc:  0.6640625
train loss:  0.5737771987915039
train gradient:  0.2126546730696689
iteration : 1240
train acc:  0.6640625
train loss:  0.5878455638885498
train gradient:  0.15247097335543408
iteration : 1241
train acc:  0.6953125
train loss:  0.5902577638626099
train gradient:  0.17044258513715008
iteration : 1242
train acc:  0.6875
train loss:  0.6024213433265686
train gradient:  0.16938121542024367
iteration : 1243
train acc:  0.7265625
train loss:  0.5588644742965698
train gradient:  0.17228629695614928
iteration : 1244
train acc:  0.6875
train loss:  0.5822147727012634
train gradient:  0.2255352790462173
iteration : 1245
train acc:  0.65625
train loss:  0.5670769214630127
train gradient:  0.1488147909726369
iteration : 1246
train acc:  0.7109375
train loss:  0.5168280601501465
train gradient:  0.1461914392067808
iteration : 1247
train acc:  0.6171875
train loss:  0.6001982688903809
train gradient:  0.23218687963965723
iteration : 1248
train acc:  0.6875
train loss:  0.5799790024757385
train gradient:  0.2336451147250329
iteration : 1249
train acc:  0.703125
train loss:  0.5884986519813538
train gradient:  0.14409399148204072
iteration : 1250
train acc:  0.7109375
train loss:  0.5384693145751953
train gradient:  0.13618931857283412
iteration : 1251
train acc:  0.71875
train loss:  0.5353855490684509
train gradient:  0.1343841359372782
iteration : 1252
train acc:  0.7421875
train loss:  0.5373528599739075
train gradient:  0.1468577409902686
iteration : 1253
train acc:  0.7109375
train loss:  0.5478560924530029
train gradient:  0.16486056807999538
iteration : 1254
train acc:  0.640625
train loss:  0.6535787582397461
train gradient:  0.22899446910022775
iteration : 1255
train acc:  0.6796875
train loss:  0.5946468114852905
train gradient:  0.19605006488266835
iteration : 1256
train acc:  0.703125
train loss:  0.5978980660438538
train gradient:  0.15920700072721422
iteration : 1257
train acc:  0.6875
train loss:  0.5194449424743652
train gradient:  0.14449507924896457
iteration : 1258
train acc:  0.6484375
train loss:  0.6208905577659607
train gradient:  0.1498453951803792
iteration : 1259
train acc:  0.703125
train loss:  0.5839182734489441
train gradient:  0.17835156288321857
iteration : 1260
train acc:  0.6640625
train loss:  0.6399108171463013
train gradient:  0.22522332790957839
iteration : 1261
train acc:  0.71875
train loss:  0.5486463308334351
train gradient:  0.16217671799935493
iteration : 1262
train acc:  0.7421875
train loss:  0.4566810429096222
train gradient:  0.11674694771556254
iteration : 1263
train acc:  0.7265625
train loss:  0.5800157189369202
train gradient:  0.17869065137302217
iteration : 1264
train acc:  0.640625
train loss:  0.5853378772735596
train gradient:  0.19648503734280517
iteration : 1265
train acc:  0.6328125
train loss:  0.5965758562088013
train gradient:  0.15149039201039094
iteration : 1266
train acc:  0.6953125
train loss:  0.564217746257782
train gradient:  0.13723622411099326
iteration : 1267
train acc:  0.6328125
train loss:  0.6477769017219543
train gradient:  0.19477581469992905
iteration : 1268
train acc:  0.6640625
train loss:  0.5885939598083496
train gradient:  0.15352802255374934
iteration : 1269
train acc:  0.7421875
train loss:  0.5623040199279785
train gradient:  0.193934253159887
iteration : 1270
train acc:  0.6953125
train loss:  0.5315917134284973
train gradient:  0.14539354533241394
iteration : 1271
train acc:  0.671875
train loss:  0.5837181210517883
train gradient:  0.1569429528496598
iteration : 1272
train acc:  0.7421875
train loss:  0.554549515247345
train gradient:  0.15547856216067782
iteration : 1273
train acc:  0.6328125
train loss:  0.5816763639450073
train gradient:  0.18025732507562328
iteration : 1274
train acc:  0.7265625
train loss:  0.5258692502975464
train gradient:  0.15410885072596717
iteration : 1275
train acc:  0.7421875
train loss:  0.5633811950683594
train gradient:  0.15396866355356767
iteration : 1276
train acc:  0.734375
train loss:  0.510454535484314
train gradient:  0.13552154955930107
iteration : 1277
train acc:  0.6875
train loss:  0.5426115989685059
train gradient:  0.18437307055142932
iteration : 1278
train acc:  0.6953125
train loss:  0.5281257629394531
train gradient:  0.15644303040598467
iteration : 1279
train acc:  0.671875
train loss:  0.5634220838546753
train gradient:  0.15590822554530528
iteration : 1280
train acc:  0.7421875
train loss:  0.4957476854324341
train gradient:  0.13379208978362492
iteration : 1281
train acc:  0.75
train loss:  0.5154056549072266
train gradient:  0.16076806382001263
iteration : 1282
train acc:  0.7578125
train loss:  0.537545919418335
train gradient:  0.2174328380450265
iteration : 1283
train acc:  0.703125
train loss:  0.5297154784202576
train gradient:  0.1352135134723284
iteration : 1284
train acc:  0.6875
train loss:  0.5374199151992798
train gradient:  0.18348858372709487
iteration : 1285
train acc:  0.671875
train loss:  0.5607789754867554
train gradient:  0.13207061499163547
iteration : 1286
train acc:  0.6875
train loss:  0.5494139194488525
train gradient:  0.1533418145000175
iteration : 1287
train acc:  0.84375
train loss:  0.4347193241119385
train gradient:  0.14319516282758155
iteration : 1288
train acc:  0.734375
train loss:  0.5086333751678467
train gradient:  0.1358390501565401
iteration : 1289
train acc:  0.6875
train loss:  0.5585456490516663
train gradient:  0.15636663806309584
iteration : 1290
train acc:  0.7109375
train loss:  0.5603828430175781
train gradient:  0.2052560036581229
iteration : 1291
train acc:  0.6953125
train loss:  0.549841046333313
train gradient:  0.13478166529144875
iteration : 1292
train acc:  0.625
train loss:  0.5874185562133789
train gradient:  0.1639923765049737
iteration : 1293
train acc:  0.6875
train loss:  0.5693283677101135
train gradient:  0.1845319382271618
iteration : 1294
train acc:  0.7578125
train loss:  0.49104583263397217
train gradient:  0.13950145716462473
iteration : 1295
train acc:  0.71875
train loss:  0.5282238721847534
train gradient:  0.19274309798370715
iteration : 1296
train acc:  0.6875
train loss:  0.5593501925468445
train gradient:  0.16647468274501837
iteration : 1297
train acc:  0.7578125
train loss:  0.5197990536689758
train gradient:  0.1553167650279567
iteration : 1298
train acc:  0.6875
train loss:  0.6155266165733337
train gradient:  0.17889077044493698
iteration : 1299
train acc:  0.703125
train loss:  0.5660905838012695
train gradient:  0.15604023530418243
iteration : 1300
train acc:  0.6875
train loss:  0.5601774454116821
train gradient:  0.16803274314102873
iteration : 1301
train acc:  0.6640625
train loss:  0.5826534032821655
train gradient:  0.1771658146294393
iteration : 1302
train acc:  0.6171875
train loss:  0.6277055740356445
train gradient:  0.19372046293535006
iteration : 1303
train acc:  0.671875
train loss:  0.5604546070098877
train gradient:  0.1812323264561499
iteration : 1304
train acc:  0.6875
train loss:  0.5641989707946777
train gradient:  0.1745797808682163
iteration : 1305
train acc:  0.75
train loss:  0.5544281601905823
train gradient:  0.23487266108261923
iteration : 1306
train acc:  0.703125
train loss:  0.5457034111022949
train gradient:  0.15413171026342132
iteration : 1307
train acc:  0.5859375
train loss:  0.664962649345398
train gradient:  0.2342437673629357
iteration : 1308
train acc:  0.7265625
train loss:  0.4960021674633026
train gradient:  0.140419415942033
iteration : 1309
train acc:  0.71875
train loss:  0.5506477952003479
train gradient:  0.18466739803629195
iteration : 1310
train acc:  0.6953125
train loss:  0.5528481006622314
train gradient:  0.22187529452563157
iteration : 1311
train acc:  0.78125
train loss:  0.49589622020721436
train gradient:  0.17237556803735782
iteration : 1312
train acc:  0.75
train loss:  0.5020458698272705
train gradient:  0.1541057157319352
iteration : 1313
train acc:  0.6796875
train loss:  0.5423643589019775
train gradient:  0.1597613401746425
iteration : 1314
train acc:  0.65625
train loss:  0.6297414898872375
train gradient:  0.20791104216030523
iteration : 1315
train acc:  0.7265625
train loss:  0.5566339492797852
train gradient:  0.1762230035815001
iteration : 1316
train acc:  0.671875
train loss:  0.606339156627655
train gradient:  0.20090051079886168
iteration : 1317
train acc:  0.7265625
train loss:  0.5207326412200928
train gradient:  0.10435293915238537
iteration : 1318
train acc:  0.703125
train loss:  0.5831089019775391
train gradient:  0.20027922532145095
iteration : 1319
train acc:  0.7265625
train loss:  0.5839399695396423
train gradient:  0.23534679377720294
iteration : 1320
train acc:  0.671875
train loss:  0.5820428133010864
train gradient:  0.188102841118535
iteration : 1321
train acc:  0.71875
train loss:  0.5528601408004761
train gradient:  0.1416410855305278
iteration : 1322
train acc:  0.6171875
train loss:  0.6015578508377075
train gradient:  0.18914509632329723
iteration : 1323
train acc:  0.6171875
train loss:  0.6327693462371826
train gradient:  0.2673961119389073
iteration : 1324
train acc:  0.6171875
train loss:  0.6076970100402832
train gradient:  0.22337774951633627
iteration : 1325
train acc:  0.71875
train loss:  0.5586959719657898
train gradient:  0.16134605734995144
iteration : 1326
train acc:  0.6953125
train loss:  0.5595014095306396
train gradient:  0.15733493998778508
iteration : 1327
train acc:  0.671875
train loss:  0.5348129868507385
train gradient:  0.1468377840188006
iteration : 1328
train acc:  0.6875
train loss:  0.5827732682228088
train gradient:  0.180416815569783
iteration : 1329
train acc:  0.6640625
train loss:  0.5932357311248779
train gradient:  0.23264070625275213
iteration : 1330
train acc:  0.7578125
train loss:  0.5030159950256348
train gradient:  0.11475886913331029
iteration : 1331
train acc:  0.7265625
train loss:  0.5806565880775452
train gradient:  0.2069448815537935
iteration : 1332
train acc:  0.65625
train loss:  0.5521999597549438
train gradient:  0.19415633273606198
iteration : 1333
train acc:  0.703125
train loss:  0.5676302909851074
train gradient:  0.14691247375625982
iteration : 1334
train acc:  0.6953125
train loss:  0.5381032228469849
train gradient:  0.22472424933477952
iteration : 1335
train acc:  0.71875
train loss:  0.5285087823867798
train gradient:  0.17732631378321978
iteration : 1336
train acc:  0.7265625
train loss:  0.5370287895202637
train gradient:  0.13978393575397266
iteration : 1337
train acc:  0.6796875
train loss:  0.551042377948761
train gradient:  0.18973308249272527
iteration : 1338
train acc:  0.7578125
train loss:  0.5406606197357178
train gradient:  0.16586749458915226
iteration : 1339
train acc:  0.65625
train loss:  0.6109747290611267
train gradient:  0.19974140159904652
iteration : 1340
train acc:  0.65625
train loss:  0.5831579566001892
train gradient:  0.17448442293138955
iteration : 1341
train acc:  0.765625
train loss:  0.5205259323120117
train gradient:  0.1649555857474435
iteration : 1342
train acc:  0.7265625
train loss:  0.5258498191833496
train gradient:  0.17697559067068025
iteration : 1343
train acc:  0.6875
train loss:  0.5788024067878723
train gradient:  0.17680329886955046
iteration : 1344
train acc:  0.6171875
train loss:  0.6402958631515503
train gradient:  0.19144775580113393
iteration : 1345
train acc:  0.734375
train loss:  0.5562930703163147
train gradient:  0.14725470321690312
iteration : 1346
train acc:  0.71875
train loss:  0.5325668454170227
train gradient:  0.13618714270168486
iteration : 1347
train acc:  0.6640625
train loss:  0.6049160957336426
train gradient:  0.1486163451014568
iteration : 1348
train acc:  0.703125
train loss:  0.5746266841888428
train gradient:  0.17755862773901934
iteration : 1349
train acc:  0.7265625
train loss:  0.53000807762146
train gradient:  0.1937463800564803
iteration : 1350
train acc:  0.6640625
train loss:  0.5609837770462036
train gradient:  0.14973232166385944
iteration : 1351
train acc:  0.71875
train loss:  0.5096901655197144
train gradient:  0.14727831799130797
iteration : 1352
train acc:  0.6796875
train loss:  0.56829434633255
train gradient:  0.18105448263602741
iteration : 1353
train acc:  0.6953125
train loss:  0.5293107032775879
train gradient:  0.2280942621957832
iteration : 1354
train acc:  0.6875
train loss:  0.5710943937301636
train gradient:  0.11853043397964205
iteration : 1355
train acc:  0.734375
train loss:  0.5243310332298279
train gradient:  0.14707812523597058
iteration : 1356
train acc:  0.7265625
train loss:  0.5862648487091064
train gradient:  0.15569817064008007
iteration : 1357
train acc:  0.7578125
train loss:  0.532237708568573
train gradient:  0.1467067001129361
iteration : 1358
train acc:  0.6640625
train loss:  0.5895596742630005
train gradient:  0.15726925737737307
iteration : 1359
train acc:  0.7265625
train loss:  0.5429107546806335
train gradient:  0.21344435511739054
iteration : 1360
train acc:  0.7109375
train loss:  0.5493535995483398
train gradient:  0.15391263925825116
iteration : 1361
train acc:  0.6796875
train loss:  0.5623112916946411
train gradient:  0.20965896627783082
iteration : 1362
train acc:  0.75
train loss:  0.5219844579696655
train gradient:  0.19383547401553686
iteration : 1363
train acc:  0.671875
train loss:  0.6034225225448608
train gradient:  0.16918002973455532
iteration : 1364
train acc:  0.7265625
train loss:  0.5425147414207458
train gradient:  0.172635697780818
iteration : 1365
train acc:  0.6484375
train loss:  0.5553573966026306
train gradient:  0.1993143854843895
iteration : 1366
train acc:  0.6875
train loss:  0.6072708368301392
train gradient:  0.20399853491967634
iteration : 1367
train acc:  0.6875
train loss:  0.5776569843292236
train gradient:  0.14299448457913994
iteration : 1368
train acc:  0.7265625
train loss:  0.5634759664535522
train gradient:  0.19713424326331844
iteration : 1369
train acc:  0.6484375
train loss:  0.5805650949478149
train gradient:  0.16341365966933166
iteration : 1370
train acc:  0.703125
train loss:  0.5404167175292969
train gradient:  0.17803869086564755
iteration : 1371
train acc:  0.671875
train loss:  0.53470778465271
train gradient:  0.21207990424675843
iteration : 1372
train acc:  0.6953125
train loss:  0.5390244722366333
train gradient:  0.1386618028464504
iteration : 1373
train acc:  0.6484375
train loss:  0.6177569031715393
train gradient:  0.1797468207282194
iteration : 1374
train acc:  0.703125
train loss:  0.5479081273078918
train gradient:  0.1568448603436025
iteration : 1375
train acc:  0.6796875
train loss:  0.546676754951477
train gradient:  0.14742079015189619
iteration : 1376
train acc:  0.671875
train loss:  0.5880042314529419
train gradient:  0.18749519978538084
iteration : 1377
train acc:  0.7578125
train loss:  0.4996878504753113
train gradient:  0.13871782271658717
iteration : 1378
train acc:  0.734375
train loss:  0.5019612908363342
train gradient:  0.12977661856417577
iteration : 1379
train acc:  0.6875
train loss:  0.5261110663414001
train gradient:  0.1433444396649528
iteration : 1380
train acc:  0.75
train loss:  0.5136982202529907
train gradient:  0.2156312014013853
iteration : 1381
train acc:  0.6953125
train loss:  0.5619679689407349
train gradient:  0.14378754075469852
iteration : 1382
train acc:  0.703125
train loss:  0.5827754735946655
train gradient:  0.18197181212516764
iteration : 1383
train acc:  0.8046875
train loss:  0.4771367609500885
train gradient:  0.12043992314070529
iteration : 1384
train acc:  0.6875
train loss:  0.5878157019615173
train gradient:  0.16824133797497415
iteration : 1385
train acc:  0.6015625
train loss:  0.6396003365516663
train gradient:  0.24126366634166546
iteration : 1386
train acc:  0.671875
train loss:  0.5788065195083618
train gradient:  0.22184311422475927
iteration : 1387
train acc:  0.6953125
train loss:  0.5792738199234009
train gradient:  0.22853319329017746
iteration : 1388
train acc:  0.734375
train loss:  0.5588449239730835
train gradient:  0.22825889147200573
iteration : 1389
train acc:  0.6953125
train loss:  0.6179294586181641
train gradient:  0.24139446789632518
iteration : 1390
train acc:  0.6015625
train loss:  0.6267563104629517
train gradient:  0.19311220099345516
iteration : 1391
train acc:  0.6953125
train loss:  0.5435670614242554
train gradient:  0.15508473127546496
iteration : 1392
train acc:  0.6875
train loss:  0.5096096992492676
train gradient:  0.18353664261271319
iteration : 1393
train acc:  0.6328125
train loss:  0.6006119251251221
train gradient:  0.22739990960941595
iteration : 1394
train acc:  0.6953125
train loss:  0.5765100121498108
train gradient:  0.2326393906649467
iteration : 1395
train acc:  0.7265625
train loss:  0.5049005746841431
train gradient:  0.14420066613832638
iteration : 1396
train acc:  0.6640625
train loss:  0.6327844262123108
train gradient:  0.2241222826142829
iteration : 1397
train acc:  0.7265625
train loss:  0.5491512417793274
train gradient:  0.17259874738709413
iteration : 1398
train acc:  0.6875
train loss:  0.5820534229278564
train gradient:  0.20419736292269597
iteration : 1399
train acc:  0.7421875
train loss:  0.5740851759910583
train gradient:  0.23168887426713713
iteration : 1400
train acc:  0.6796875
train loss:  0.556936502456665
train gradient:  0.21982640693690358
iteration : 1401
train acc:  0.7265625
train loss:  0.5524842739105225
train gradient:  0.16826166039451135
iteration : 1402
train acc:  0.734375
train loss:  0.5111397504806519
train gradient:  0.16859902146665157
iteration : 1403
train acc:  0.71875
train loss:  0.5080996751785278
train gradient:  0.18291087481669382
iteration : 1404
train acc:  0.71875
train loss:  0.5254786014556885
train gradient:  0.1255415093135673
iteration : 1405
train acc:  0.71875
train loss:  0.5493638515472412
train gradient:  0.1605675433990904
iteration : 1406
train acc:  0.6875
train loss:  0.5918751358985901
train gradient:  0.15353472996816475
iteration : 1407
train acc:  0.7265625
train loss:  0.5516998171806335
train gradient:  0.1978442345088316
iteration : 1408
train acc:  0.6875
train loss:  0.5849367380142212
train gradient:  0.19743055545979005
iteration : 1409
train acc:  0.6875
train loss:  0.5665611028671265
train gradient:  0.14642720240139673
iteration : 1410
train acc:  0.7578125
train loss:  0.512580156326294
train gradient:  0.17477321129118176
iteration : 1411
train acc:  0.609375
train loss:  0.6517983675003052
train gradient:  0.2128650255423021
iteration : 1412
train acc:  0.71875
train loss:  0.5120306015014648
train gradient:  0.14559679169843068
iteration : 1413
train acc:  0.640625
train loss:  0.5987359285354614
train gradient:  0.2578923694566408
iteration : 1414
train acc:  0.71875
train loss:  0.5529831051826477
train gradient:  0.2560521005570525
iteration : 1415
train acc:  0.65625
train loss:  0.5961395502090454
train gradient:  0.1924274522119306
iteration : 1416
train acc:  0.734375
train loss:  0.5226972103118896
train gradient:  0.18068676725502153
iteration : 1417
train acc:  0.7265625
train loss:  0.5351923704147339
train gradient:  0.1448691986068682
iteration : 1418
train acc:  0.671875
train loss:  0.6331120729446411
train gradient:  0.20300842462422652
iteration : 1419
train acc:  0.7265625
train loss:  0.5160053968429565
train gradient:  0.2356013198107328
iteration : 1420
train acc:  0.765625
train loss:  0.5074489712715149
train gradient:  0.13278260890474453
iteration : 1421
train acc:  0.734375
train loss:  0.5637128949165344
train gradient:  0.1660709945796195
iteration : 1422
train acc:  0.765625
train loss:  0.5095311403274536
train gradient:  0.16013042613682948
iteration : 1423
train acc:  0.7109375
train loss:  0.5392285585403442
train gradient:  0.1662004093327809
iteration : 1424
train acc:  0.7109375
train loss:  0.5498205423355103
train gradient:  0.17796285986754795
iteration : 1425
train acc:  0.7109375
train loss:  0.5391379594802856
train gradient:  0.13455891704939787
iteration : 1426
train acc:  0.734375
train loss:  0.5700377225875854
train gradient:  0.1766113372919475
iteration : 1427
train acc:  0.7265625
train loss:  0.5369188785552979
train gradient:  0.16801835978937998
iteration : 1428
train acc:  0.6796875
train loss:  0.5595666766166687
train gradient:  0.1709462151076886
iteration : 1429
train acc:  0.734375
train loss:  0.5764577984809875
train gradient:  0.30480050083857774
iteration : 1430
train acc:  0.75
train loss:  0.5140653848648071
train gradient:  0.15321053955914302
iteration : 1431
train acc:  0.703125
train loss:  0.5614131689071655
train gradient:  0.23105099242583627
iteration : 1432
train acc:  0.765625
train loss:  0.4907139837741852
train gradient:  0.14165876973354125
iteration : 1433
train acc:  0.765625
train loss:  0.4848825931549072
train gradient:  0.11960269295784841
iteration : 1434
train acc:  0.7578125
train loss:  0.5318724513053894
train gradient:  0.20802640455385846
iteration : 1435
train acc:  0.6640625
train loss:  0.5884680151939392
train gradient:  0.161167393730303
iteration : 1436
train acc:  0.6796875
train loss:  0.5631896257400513
train gradient:  0.16381472994947105
iteration : 1437
train acc:  0.6484375
train loss:  0.5727148652076721
train gradient:  0.15424653837348445
iteration : 1438
train acc:  0.640625
train loss:  0.6133238077163696
train gradient:  0.1779246416324452
iteration : 1439
train acc:  0.6484375
train loss:  0.5601635575294495
train gradient:  0.18636413072197544
iteration : 1440
train acc:  0.7578125
train loss:  0.5292918086051941
train gradient:  0.14107237914312948
iteration : 1441
train acc:  0.71875
train loss:  0.5201333165168762
train gradient:  0.17843220296736398
iteration : 1442
train acc:  0.703125
train loss:  0.5632117986679077
train gradient:  0.15238742366125155
iteration : 1443
train acc:  0.6328125
train loss:  0.629586935043335
train gradient:  0.34671057449733844
iteration : 1444
train acc:  0.6953125
train loss:  0.5816407203674316
train gradient:  0.17418827559220798
iteration : 1445
train acc:  0.7265625
train loss:  0.5386935472488403
train gradient:  0.14580393152740662
iteration : 1446
train acc:  0.71875
train loss:  0.5413782596588135
train gradient:  0.12494953405808926
iteration : 1447
train acc:  0.6640625
train loss:  0.592011570930481
train gradient:  0.1974755141918249
iteration : 1448
train acc:  0.8046875
train loss:  0.5073860287666321
train gradient:  0.14714414864276917
iteration : 1449
train acc:  0.6640625
train loss:  0.5733969807624817
train gradient:  0.17727147138594512
iteration : 1450
train acc:  0.703125
train loss:  0.5895510911941528
train gradient:  0.1992798140466137
iteration : 1451
train acc:  0.65625
train loss:  0.6080592274665833
train gradient:  0.24048451405200394
iteration : 1452
train acc:  0.7109375
train loss:  0.5277206301689148
train gradient:  0.14276326056572713
iteration : 1453
train acc:  0.71875
train loss:  0.5509222149848938
train gradient:  0.1362872275540205
iteration : 1454
train acc:  0.7265625
train loss:  0.5309468507766724
train gradient:  0.15686240586133948
iteration : 1455
train acc:  0.7578125
train loss:  0.5259814262390137
train gradient:  0.1273863293396001
iteration : 1456
train acc:  0.703125
train loss:  0.5385720729827881
train gradient:  0.19253129046179096
iteration : 1457
train acc:  0.6953125
train loss:  0.5126744508743286
train gradient:  0.175973401768653
iteration : 1458
train acc:  0.65625
train loss:  0.5874612331390381
train gradient:  0.16586630565790508
iteration : 1459
train acc:  0.6875
train loss:  0.5856253504753113
train gradient:  0.17813984833975754
iteration : 1460
train acc:  0.75
train loss:  0.5173689723014832
train gradient:  0.1604519092248165
iteration : 1461
train acc:  0.734375
train loss:  0.5477234721183777
train gradient:  0.2963727522436417
iteration : 1462
train acc:  0.734375
train loss:  0.5340379476547241
train gradient:  0.18612886611990714
iteration : 1463
train acc:  0.7265625
train loss:  0.5212832093238831
train gradient:  0.14831889935425732
iteration : 1464
train acc:  0.703125
train loss:  0.5388416647911072
train gradient:  0.16624851933822932
iteration : 1465
train acc:  0.6796875
train loss:  0.5904622077941895
train gradient:  0.22145927652223438
iteration : 1466
train acc:  0.6796875
train loss:  0.5724121928215027
train gradient:  0.16709374669457921
iteration : 1467
train acc:  0.671875
train loss:  0.5668478012084961
train gradient:  0.1595449008230825
iteration : 1468
train acc:  0.71875
train loss:  0.5482491850852966
train gradient:  0.12723099206266078
iteration : 1469
train acc:  0.734375
train loss:  0.499539315700531
train gradient:  0.2406362687988277
iteration : 1470
train acc:  0.6796875
train loss:  0.5574310421943665
train gradient:  0.19814524286497995
iteration : 1471
train acc:  0.7109375
train loss:  0.5241726040840149
train gradient:  0.16781468296619861
iteration : 1472
train acc:  0.8046875
train loss:  0.4705480933189392
train gradient:  0.15015179775198845
iteration : 1473
train acc:  0.703125
train loss:  0.5684369802474976
train gradient:  0.18500740393667797
iteration : 1474
train acc:  0.640625
train loss:  0.5817164182662964
train gradient:  0.22539029779174824
iteration : 1475
train acc:  0.75
train loss:  0.5230072736740112
train gradient:  0.16961031546044802
iteration : 1476
train acc:  0.796875
train loss:  0.4940391182899475
train gradient:  0.16650951142092496
iteration : 1477
train acc:  0.6640625
train loss:  0.6191726922988892
train gradient:  0.32510200096217023
iteration : 1478
train acc:  0.703125
train loss:  0.5326203107833862
train gradient:  0.1592723405959866
iteration : 1479
train acc:  0.6484375
train loss:  0.6645916700363159
train gradient:  0.20274310302216247
iteration : 1480
train acc:  0.6953125
train loss:  0.5799428224563599
train gradient:  0.2208794407112184
iteration : 1481
train acc:  0.703125
train loss:  0.5837775468826294
train gradient:  0.22625803985854065
iteration : 1482
train acc:  0.6640625
train loss:  0.6219324469566345
train gradient:  0.22711733483565924
iteration : 1483
train acc:  0.671875
train loss:  0.5652288198471069
train gradient:  0.23882765948625667
iteration : 1484
train acc:  0.6796875
train loss:  0.5448821783065796
train gradient:  0.15593227432324525
iteration : 1485
train acc:  0.765625
train loss:  0.5174075961112976
train gradient:  0.20192914480152885
iteration : 1486
train acc:  0.703125
train loss:  0.521850049495697
train gradient:  0.159243554821629
iteration : 1487
train acc:  0.703125
train loss:  0.5391045808792114
train gradient:  0.2456678576101376
iteration : 1488
train acc:  0.6953125
train loss:  0.5545032620429993
train gradient:  0.19410650145762853
iteration : 1489
train acc:  0.734375
train loss:  0.5200256109237671
train gradient:  0.13807845775910152
iteration : 1490
train acc:  0.6640625
train loss:  0.6002416014671326
train gradient:  0.17105733084758185
iteration : 1491
train acc:  0.6875
train loss:  0.6417378187179565
train gradient:  0.20846646065802707
iteration : 1492
train acc:  0.7109375
train loss:  0.5614001750946045
train gradient:  0.2559894421257004
iteration : 1493
train acc:  0.7421875
train loss:  0.5255556106567383
train gradient:  0.1668078506148551
iteration : 1494
train acc:  0.6640625
train loss:  0.6149370074272156
train gradient:  0.2055712726584816
iteration : 1495
train acc:  0.6953125
train loss:  0.5796165466308594
train gradient:  0.19402733875919065
iteration : 1496
train acc:  0.6875
train loss:  0.5583178997039795
train gradient:  0.26285663444092966
iteration : 1497
train acc:  0.765625
train loss:  0.551658034324646
train gradient:  0.1750610262437558
iteration : 1498
train acc:  0.640625
train loss:  0.5775464177131653
train gradient:  0.19751183536082803
iteration : 1499
train acc:  0.703125
train loss:  0.5298904180526733
train gradient:  0.1496517235016838
iteration : 1500
train acc:  0.734375
train loss:  0.5245968103408813
train gradient:  0.1563351433068259
iteration : 1501
train acc:  0.6953125
train loss:  0.5821035504341125
train gradient:  0.21586224121921532
iteration : 1502
train acc:  0.6640625
train loss:  0.5880053639411926
train gradient:  0.1941283070031522
iteration : 1503
train acc:  0.7109375
train loss:  0.5428272485733032
train gradient:  0.1418746491940597
iteration : 1504
train acc:  0.765625
train loss:  0.5162786245346069
train gradient:  0.12664569423944055
iteration : 1505
train acc:  0.65625
train loss:  0.5999671816825867
train gradient:  0.1735912427366668
iteration : 1506
train acc:  0.640625
train loss:  0.6033455729484558
train gradient:  0.21197900126238062
iteration : 1507
train acc:  0.640625
train loss:  0.59086012840271
train gradient:  0.17468636583809066
iteration : 1508
train acc:  0.640625
train loss:  0.5828401446342468
train gradient:  0.3613003487624423
iteration : 1509
train acc:  0.6796875
train loss:  0.6120010614395142
train gradient:  0.1968516986242207
iteration : 1510
train acc:  0.6953125
train loss:  0.5747175216674805
train gradient:  0.18244833771723729
iteration : 1511
train acc:  0.7265625
train loss:  0.5658236742019653
train gradient:  0.1584253574202739
iteration : 1512
train acc:  0.7109375
train loss:  0.536198616027832
train gradient:  0.12538519606763826
iteration : 1513
train acc:  0.6953125
train loss:  0.5882421135902405
train gradient:  0.24134251884507907
iteration : 1514
train acc:  0.6953125
train loss:  0.5376796722412109
train gradient:  0.16685304798546613
iteration : 1515
train acc:  0.671875
train loss:  0.601753830909729
train gradient:  0.1976130899753648
iteration : 1516
train acc:  0.6796875
train loss:  0.5802046060562134
train gradient:  0.19651082179381094
iteration : 1517
train acc:  0.7265625
train loss:  0.5319292545318604
train gradient:  0.19190863282880388
iteration : 1518
train acc:  0.71875
train loss:  0.516745388507843
train gradient:  0.14558460210330743
iteration : 1519
train acc:  0.6796875
train loss:  0.5686771273612976
train gradient:  0.16805591778062545
iteration : 1520
train acc:  0.734375
train loss:  0.5407589077949524
train gradient:  0.231678718087364
iteration : 1521
train acc:  0.75
train loss:  0.5272732377052307
train gradient:  0.18464186087937506
iteration : 1522
train acc:  0.671875
train loss:  0.5756579041481018
train gradient:  0.18308528963621024
iteration : 1523
train acc:  0.703125
train loss:  0.5562118291854858
train gradient:  0.15123422132003372
iteration : 1524
train acc:  0.734375
train loss:  0.5088649392127991
train gradient:  0.15346642238508004
iteration : 1525
train acc:  0.6875
train loss:  0.5510591268539429
train gradient:  0.16131890164722235
iteration : 1526
train acc:  0.6640625
train loss:  0.5835975408554077
train gradient:  0.22527789792130654
iteration : 1527
train acc:  0.703125
train loss:  0.5373287200927734
train gradient:  0.17780048049374325
iteration : 1528
train acc:  0.7109375
train loss:  0.5418509244918823
train gradient:  0.1607947067531395
iteration : 1529
train acc:  0.71875
train loss:  0.5384902954101562
train gradient:  0.1280465914991315
iteration : 1530
train acc:  0.78125
train loss:  0.5078239440917969
train gradient:  0.16510153347983508
iteration : 1531
train acc:  0.65625
train loss:  0.5681432485580444
train gradient:  0.1881604908987653
iteration : 1532
train acc:  0.7421875
train loss:  0.5124053955078125
train gradient:  0.19473712238198387
iteration : 1533
train acc:  0.6796875
train loss:  0.6367799043655396
train gradient:  0.2428147401922477
iteration : 1534
train acc:  0.6796875
train loss:  0.5914416313171387
train gradient:  0.17241607489555677
iteration : 1535
train acc:  0.7265625
train loss:  0.5274704694747925
train gradient:  0.1500558607489309
iteration : 1536
train acc:  0.71875
train loss:  0.5107542872428894
train gradient:  0.17486505057217067
iteration : 1537
train acc:  0.65625
train loss:  0.6227197647094727
train gradient:  0.2124665795832516
iteration : 1538
train acc:  0.7421875
train loss:  0.5253108739852905
train gradient:  0.16519011917806795
iteration : 1539
train acc:  0.7578125
train loss:  0.5125282406806946
train gradient:  0.15584453613548027
iteration : 1540
train acc:  0.5859375
train loss:  0.6462833881378174
train gradient:  0.24872535539815616
iteration : 1541
train acc:  0.6953125
train loss:  0.5215407609939575
train gradient:  0.1462414858496322
iteration : 1542
train acc:  0.6796875
train loss:  0.572161078453064
train gradient:  0.1866602542112363
iteration : 1543
train acc:  0.6875
train loss:  0.5665234327316284
train gradient:  0.19109927528662626
iteration : 1544
train acc:  0.7421875
train loss:  0.5124254822731018
train gradient:  0.1649929282228828
iteration : 1545
train acc:  0.703125
train loss:  0.5371974110603333
train gradient:  0.21697067758679559
iteration : 1546
train acc:  0.703125
train loss:  0.5396884679794312
train gradient:  0.1383355413505917
iteration : 1547
train acc:  0.6171875
train loss:  0.6315433979034424
train gradient:  0.17633820181969606
iteration : 1548
train acc:  0.640625
train loss:  0.616771936416626
train gradient:  0.208217257844853
iteration : 1549
train acc:  0.7109375
train loss:  0.5362699031829834
train gradient:  0.16905543368946951
iteration : 1550
train acc:  0.703125
train loss:  0.5520647764205933
train gradient:  0.1511072453480342
iteration : 1551
train acc:  0.7265625
train loss:  0.5310440063476562
train gradient:  0.16545979492991575
iteration : 1552
train acc:  0.6953125
train loss:  0.5878685712814331
train gradient:  0.18290137812216706
iteration : 1553
train acc:  0.6171875
train loss:  0.6252290606498718
train gradient:  0.21941249154562908
iteration : 1554
train acc:  0.734375
train loss:  0.5040515661239624
train gradient:  0.1682999735543908
iteration : 1555
train acc:  0.6796875
train loss:  0.535999596118927
train gradient:  0.1658859099794452
iteration : 1556
train acc:  0.7421875
train loss:  0.5426915287971497
train gradient:  0.1631631288431727
iteration : 1557
train acc:  0.6171875
train loss:  0.6178971529006958
train gradient:  0.192235002686937
iteration : 1558
train acc:  0.6796875
train loss:  0.6034783124923706
train gradient:  0.20404545843470007
iteration : 1559
train acc:  0.6953125
train loss:  0.5675441026687622
train gradient:  0.2630204364859205
iteration : 1560
train acc:  0.6796875
train loss:  0.5123605728149414
train gradient:  0.1941458638773284
iteration : 1561
train acc:  0.7421875
train loss:  0.5334572792053223
train gradient:  0.15877760143557723
iteration : 1562
train acc:  0.6640625
train loss:  0.5888648629188538
train gradient:  0.18836815557338618
iteration : 1563
train acc:  0.703125
train loss:  0.53752601146698
train gradient:  0.1728127365225185
iteration : 1564
train acc:  0.671875
train loss:  0.5232862234115601
train gradient:  0.16139145602204483
iteration : 1565
train acc:  0.6953125
train loss:  0.5520659685134888
train gradient:  0.15738667402227147
iteration : 1566
train acc:  0.6953125
train loss:  0.5676501989364624
train gradient:  0.20099476095558683
iteration : 1567
train acc:  0.625
train loss:  0.6379727721214294
train gradient:  0.2335266699460961
iteration : 1568
train acc:  0.609375
train loss:  0.6486276388168335
train gradient:  0.2234915705811822
iteration : 1569
train acc:  0.6953125
train loss:  0.5141201019287109
train gradient:  0.19222570439800107
iteration : 1570
train acc:  0.71875
train loss:  0.5431951284408569
train gradient:  0.1410341951211142
iteration : 1571
train acc:  0.671875
train loss:  0.5760420560836792
train gradient:  0.15768073515178696
iteration : 1572
train acc:  0.625
train loss:  0.5873206853866577
train gradient:  0.21539498293485837
iteration : 1573
train acc:  0.7578125
train loss:  0.5212067365646362
train gradient:  0.15284915679056382
iteration : 1574
train acc:  0.671875
train loss:  0.548172116279602
train gradient:  0.15571487767568967
iteration : 1575
train acc:  0.765625
train loss:  0.5312429070472717
train gradient:  0.12817819783484669
iteration : 1576
train acc:  0.7109375
train loss:  0.5355451107025146
train gradient:  0.14637682890735942
iteration : 1577
train acc:  0.71875
train loss:  0.5145628452301025
train gradient:  0.23616492472569361
iteration : 1578
train acc:  0.734375
train loss:  0.5407918691635132
train gradient:  0.18149393201716763
iteration : 1579
train acc:  0.7578125
train loss:  0.48733410239219666
train gradient:  0.1370027874657901
iteration : 1580
train acc:  0.6953125
train loss:  0.54705810546875
train gradient:  0.17474368018616757
iteration : 1581
train acc:  0.625
train loss:  0.6761237978935242
train gradient:  0.2804243353293723
iteration : 1582
train acc:  0.6875
train loss:  0.559897780418396
train gradient:  0.16481852548083487
iteration : 1583
train acc:  0.703125
train loss:  0.5404983758926392
train gradient:  0.1648829339767363
iteration : 1584
train acc:  0.75
train loss:  0.5101808905601501
train gradient:  0.14097158054442732
iteration : 1585
train acc:  0.7734375
train loss:  0.4694164991378784
train gradient:  0.15944621150738447
iteration : 1586
train acc:  0.6640625
train loss:  0.5677700042724609
train gradient:  0.1568183670962691
iteration : 1587
train acc:  0.6796875
train loss:  0.575166642665863
train gradient:  0.2360945396682672
iteration : 1588
train acc:  0.734375
train loss:  0.5273847579956055
train gradient:  0.14306296112057687
iteration : 1589
train acc:  0.7109375
train loss:  0.5459204912185669
train gradient:  0.14833792278260294
iteration : 1590
train acc:  0.765625
train loss:  0.5281705260276794
train gradient:  0.2191001193311537
iteration : 1591
train acc:  0.6796875
train loss:  0.5852736234664917
train gradient:  0.16184270201122603
iteration : 1592
train acc:  0.6953125
train loss:  0.558052659034729
train gradient:  0.1537240155800113
iteration : 1593
train acc:  0.6796875
train loss:  0.5916519165039062
train gradient:  0.19557917907045264
iteration : 1594
train acc:  0.6328125
train loss:  0.6206855773925781
train gradient:  0.24789107175606578
iteration : 1595
train acc:  0.671875
train loss:  0.6040327548980713
train gradient:  0.16646486001542127
iteration : 1596
train acc:  0.6640625
train loss:  0.5540509223937988
train gradient:  0.20752521906991384
iteration : 1597
train acc:  0.6328125
train loss:  0.5916678309440613
train gradient:  0.20348711411507206
iteration : 1598
train acc:  0.75
train loss:  0.5274035930633545
train gradient:  0.1740877660968111
iteration : 1599
train acc:  0.640625
train loss:  0.5820119380950928
train gradient:  0.1820750359731556
iteration : 1600
train acc:  0.671875
train loss:  0.5707595348358154
train gradient:  0.23630048556758942
iteration : 1601
train acc:  0.6953125
train loss:  0.5993860960006714
train gradient:  0.23989432752929946
iteration : 1602
train acc:  0.75
train loss:  0.5275862812995911
train gradient:  0.13058432765589392
iteration : 1603
train acc:  0.734375
train loss:  0.5045346021652222
train gradient:  0.16085871839247323
iteration : 1604
train acc:  0.703125
train loss:  0.5564306974411011
train gradient:  0.20814444308950084
iteration : 1605
train acc:  0.765625
train loss:  0.5131363868713379
train gradient:  0.15000517955609355
iteration : 1606
train acc:  0.625
train loss:  0.581080436706543
train gradient:  0.20365585179879403
iteration : 1607
train acc:  0.6328125
train loss:  0.5970532298088074
train gradient:  0.21580725449511284
iteration : 1608
train acc:  0.7734375
train loss:  0.521706223487854
train gradient:  0.16197644540271663
iteration : 1609
train acc:  0.6875
train loss:  0.5277560949325562
train gradient:  0.15660162807121097
iteration : 1610
train acc:  0.75
train loss:  0.531654953956604
train gradient:  0.13055714063590673
iteration : 1611
train acc:  0.71875
train loss:  0.5468403697013855
train gradient:  0.21863387264575984
iteration : 1612
train acc:  0.6484375
train loss:  0.643127977848053
train gradient:  0.21976589015012704
iteration : 1613
train acc:  0.7421875
train loss:  0.5218080282211304
train gradient:  0.14365113925483874
iteration : 1614
train acc:  0.6953125
train loss:  0.568306565284729
train gradient:  0.16602513668400376
iteration : 1615
train acc:  0.671875
train loss:  0.6012300252914429
train gradient:  0.15054725825884285
iteration : 1616
train acc:  0.7421875
train loss:  0.5028029680252075
train gradient:  0.18293801453117553
iteration : 1617
train acc:  0.734375
train loss:  0.5490053296089172
train gradient:  0.17106339873141263
iteration : 1618
train acc:  0.671875
train loss:  0.5577887892723083
train gradient:  0.13584228485215874
iteration : 1619
train acc:  0.734375
train loss:  0.5711777210235596
train gradient:  0.24415098747075975
iteration : 1620
train acc:  0.7109375
train loss:  0.572426974773407
train gradient:  0.22935580410600873
iteration : 1621
train acc:  0.6640625
train loss:  0.6197443008422852
train gradient:  0.2683431299768512
iteration : 1622
train acc:  0.6796875
train loss:  0.6111116409301758
train gradient:  0.17775922085659432
iteration : 1623
train acc:  0.640625
train loss:  0.6564856767654419
train gradient:  0.2270362077071417
iteration : 1624
train acc:  0.7109375
train loss:  0.5241000652313232
train gradient:  0.201439970890062
iteration : 1625
train acc:  0.6953125
train loss:  0.5640761256217957
train gradient:  0.18473262091513118
iteration : 1626
train acc:  0.7265625
train loss:  0.5487980842590332
train gradient:  0.19356196065404163
iteration : 1627
train acc:  0.671875
train loss:  0.5636294484138489
train gradient:  0.22246211087622508
iteration : 1628
train acc:  0.7265625
train loss:  0.5027101635932922
train gradient:  0.1480544801047095
iteration : 1629
train acc:  0.625
train loss:  0.6243943572044373
train gradient:  0.1907173874192038
iteration : 1630
train acc:  0.671875
train loss:  0.5890639424324036
train gradient:  0.18147926737896763
iteration : 1631
train acc:  0.65625
train loss:  0.6422557830810547
train gradient:  0.19442445727009774
iteration : 1632
train acc:  0.7109375
train loss:  0.5521884560585022
train gradient:  0.23981260346386657
iteration : 1633
train acc:  0.7421875
train loss:  0.5153236389160156
train gradient:  0.1684775639475538
iteration : 1634
train acc:  0.6328125
train loss:  0.629006028175354
train gradient:  0.24399467111975737
iteration : 1635
train acc:  0.6171875
train loss:  0.6241697072982788
train gradient:  0.20261963022212742
iteration : 1636
train acc:  0.7109375
train loss:  0.5509651899337769
train gradient:  0.16121480587275538
iteration : 1637
train acc:  0.671875
train loss:  0.5904083251953125
train gradient:  0.19176246751806791
iteration : 1638
train acc:  0.7109375
train loss:  0.5084568858146667
train gradient:  0.15924571804757734
iteration : 1639
train acc:  0.6796875
train loss:  0.5854043960571289
train gradient:  0.22618059816037256
iteration : 1640
train acc:  0.6484375
train loss:  0.6047629117965698
train gradient:  0.19004413721554364
iteration : 1641
train acc:  0.71875
train loss:  0.5809780955314636
train gradient:  0.17866938486065265
iteration : 1642
train acc:  0.7578125
train loss:  0.5008942484855652
train gradient:  0.1447519447991067
iteration : 1643
train acc:  0.6953125
train loss:  0.6027003526687622
train gradient:  0.23946048300513995
iteration : 1644
train acc:  0.65625
train loss:  0.5937076807022095
train gradient:  0.22254958568628957
iteration : 1645
train acc:  0.6953125
train loss:  0.5368772745132446
train gradient:  0.1559112659893663
iteration : 1646
train acc:  0.734375
train loss:  0.5370161533355713
train gradient:  0.13354487588526984
iteration : 1647
train acc:  0.734375
train loss:  0.5119695067405701
train gradient:  0.13685253177594459
iteration : 1648
train acc:  0.640625
train loss:  0.5739187002182007
train gradient:  0.20770747611312879
iteration : 1649
train acc:  0.6875
train loss:  0.5536779165267944
train gradient:  0.17732447605817458
iteration : 1650
train acc:  0.6484375
train loss:  0.5674163103103638
train gradient:  0.18732057666716329
iteration : 1651
train acc:  0.703125
train loss:  0.5582907199859619
train gradient:  0.19895486220659908
iteration : 1652
train acc:  0.734375
train loss:  0.5333856344223022
train gradient:  0.1750578942815979
iteration : 1653
train acc:  0.6328125
train loss:  0.6184145212173462
train gradient:  0.21219200625639212
iteration : 1654
train acc:  0.6328125
train loss:  0.592952311038971
train gradient:  0.15116474577698122
iteration : 1655
train acc:  0.7265625
train loss:  0.5668462514877319
train gradient:  0.18860315642570397
iteration : 1656
train acc:  0.6875
train loss:  0.5585803389549255
train gradient:  0.17396461742423802
iteration : 1657
train acc:  0.75
train loss:  0.4982689619064331
train gradient:  0.1554713573005244
iteration : 1658
train acc:  0.65625
train loss:  0.6222563982009888
train gradient:  0.21184023934954233
iteration : 1659
train acc:  0.6796875
train loss:  0.5266295075416565
train gradient:  0.1253279562887623
iteration : 1660
train acc:  0.671875
train loss:  0.576995849609375
train gradient:  0.17706398697067793
iteration : 1661
train acc:  0.75
train loss:  0.512934684753418
train gradient:  0.1490191003659458
iteration : 1662
train acc:  0.734375
train loss:  0.5472047328948975
train gradient:  0.17306516949752984
iteration : 1663
train acc:  0.6796875
train loss:  0.5412384271621704
train gradient:  0.1318986995052528
iteration : 1664
train acc:  0.78125
train loss:  0.5148486495018005
train gradient:  0.16355848411100493
iteration : 1665
train acc:  0.6796875
train loss:  0.5613455176353455
train gradient:  0.1645039098681548
iteration : 1666
train acc:  0.71875
train loss:  0.5580002069473267
train gradient:  0.18191865626127182
iteration : 1667
train acc:  0.734375
train loss:  0.5379159450531006
train gradient:  0.22108102701912707
iteration : 1668
train acc:  0.71875
train loss:  0.5774952173233032
train gradient:  0.2005356595113948
iteration : 1669
train acc:  0.7109375
train loss:  0.5278607606887817
train gradient:  0.20203175606422996
iteration : 1670
train acc:  0.7109375
train loss:  0.5445094704627991
train gradient:  0.2598567510355448
iteration : 1671
train acc:  0.625
train loss:  0.5899699330329895
train gradient:  0.20498652258329828
iteration : 1672
train acc:  0.6953125
train loss:  0.5398099422454834
train gradient:  0.1913569567532636
iteration : 1673
train acc:  0.703125
train loss:  0.5235431790351868
train gradient:  0.14354179035722994
iteration : 1674
train acc:  0.65625
train loss:  0.6190431118011475
train gradient:  0.22485932960330002
iteration : 1675
train acc:  0.6796875
train loss:  0.540723443031311
train gradient:  0.14232929437744712
iteration : 1676
train acc:  0.6875
train loss:  0.5744603872299194
train gradient:  0.17104863722738126
iteration : 1677
train acc:  0.765625
train loss:  0.5107827186584473
train gradient:  0.13588200334272654
iteration : 1678
train acc:  0.640625
train loss:  0.6152317523956299
train gradient:  0.25420528483119764
iteration : 1679
train acc:  0.703125
train loss:  0.5668607354164124
train gradient:  0.171827756117288
iteration : 1680
train acc:  0.6953125
train loss:  0.5751587152481079
train gradient:  0.19762093139759923
iteration : 1681
train acc:  0.6484375
train loss:  0.5687263011932373
train gradient:  0.22864882613543447
iteration : 1682
train acc:  0.703125
train loss:  0.5490332841873169
train gradient:  0.19487557299440017
iteration : 1683
train acc:  0.6640625
train loss:  0.6035351753234863
train gradient:  0.22849578369243018
iteration : 1684
train acc:  0.6953125
train loss:  0.5694810748100281
train gradient:  0.13735369292895835
iteration : 1685
train acc:  0.7578125
train loss:  0.545170247554779
train gradient:  0.15445416197214068
iteration : 1686
train acc:  0.71875
train loss:  0.5122709274291992
train gradient:  0.14954983045035936
iteration : 1687
train acc:  0.6953125
train loss:  0.5601301193237305
train gradient:  0.20752491893637406
iteration : 1688
train acc:  0.7265625
train loss:  0.5233805179595947
train gradient:  0.15595136229624912
iteration : 1689
train acc:  0.7578125
train loss:  0.5261641144752502
train gradient:  0.159891347292614
iteration : 1690
train acc:  0.71875
train loss:  0.5364834666252136
train gradient:  0.1689129875419218
iteration : 1691
train acc:  0.703125
train loss:  0.5258842706680298
train gradient:  0.1257584626662157
iteration : 1692
train acc:  0.75
train loss:  0.49141812324523926
train gradient:  0.1346946689197884
iteration : 1693
train acc:  0.6796875
train loss:  0.5579768419265747
train gradient:  0.1583635044659073
iteration : 1694
train acc:  0.71875
train loss:  0.538094699382782
train gradient:  0.18150943322769136
iteration : 1695
train acc:  0.6796875
train loss:  0.5689920783042908
train gradient:  0.1519847268197928
iteration : 1696
train acc:  0.6328125
train loss:  0.5828300714492798
train gradient:  0.17472672680734697
iteration : 1697
train acc:  0.6796875
train loss:  0.5552678108215332
train gradient:  0.15501795669194643
iteration : 1698
train acc:  0.7265625
train loss:  0.5218154191970825
train gradient:  0.14786607760730855
iteration : 1699
train acc:  0.6796875
train loss:  0.5408890247344971
train gradient:  0.18487196409584444
iteration : 1700
train acc:  0.703125
train loss:  0.5122227072715759
train gradient:  0.13959213853498864
iteration : 1701
train acc:  0.6640625
train loss:  0.5299504399299622
train gradient:  0.2387359495183263
iteration : 1702
train acc:  0.6875
train loss:  0.5876375436782837
train gradient:  0.21323879036140514
iteration : 1703
train acc:  0.640625
train loss:  0.6292942762374878
train gradient:  0.2319061785864049
iteration : 1704
train acc:  0.671875
train loss:  0.5682206153869629
train gradient:  0.17369799349255816
iteration : 1705
train acc:  0.6875
train loss:  0.5395783185958862
train gradient:  0.34586334755719517
iteration : 1706
train acc:  0.6953125
train loss:  0.5340961813926697
train gradient:  0.1463970533682856
iteration : 1707
train acc:  0.640625
train loss:  0.6140364408493042
train gradient:  0.19470042119138947
iteration : 1708
train acc:  0.6796875
train loss:  0.5736575126647949
train gradient:  0.17743548111378962
iteration : 1709
train acc:  0.6484375
train loss:  0.6297569274902344
train gradient:  0.23329921197541384
iteration : 1710
train acc:  0.7265625
train loss:  0.5023046731948853
train gradient:  0.15376580109372767
iteration : 1711
train acc:  0.71875
train loss:  0.5848128795623779
train gradient:  0.17511133238100696
iteration : 1712
train acc:  0.640625
train loss:  0.5993310213088989
train gradient:  0.17236436918288744
iteration : 1713
train acc:  0.6796875
train loss:  0.5588898658752441
train gradient:  0.170874048398776
iteration : 1714
train acc:  0.6796875
train loss:  0.5257179737091064
train gradient:  0.17326833670547853
iteration : 1715
train acc:  0.703125
train loss:  0.5528712272644043
train gradient:  0.16835163561550934
iteration : 1716
train acc:  0.7421875
train loss:  0.5283631086349487
train gradient:  0.15811403932239718
iteration : 1717
train acc:  0.75
train loss:  0.5022743344306946
train gradient:  0.15093175423406932
iteration : 1718
train acc:  0.6953125
train loss:  0.5557041168212891
train gradient:  0.21535245686918017
iteration : 1719
train acc:  0.7578125
train loss:  0.5186909437179565
train gradient:  0.1502817313098
iteration : 1720
train acc:  0.7265625
train loss:  0.5420922040939331
train gradient:  0.14764505983130471
iteration : 1721
train acc:  0.7109375
train loss:  0.5348997116088867
train gradient:  0.14404552362621548
iteration : 1722
train acc:  0.6875
train loss:  0.5519381165504456
train gradient:  0.14660637543366062
iteration : 1723
train acc:  0.6484375
train loss:  0.6040617227554321
train gradient:  0.3091615969530884
iteration : 1724
train acc:  0.6796875
train loss:  0.6066181063652039
train gradient:  0.1990180866842658
iteration : 1725
train acc:  0.7265625
train loss:  0.549663245677948
train gradient:  0.15339750442729358
iteration : 1726
train acc:  0.6328125
train loss:  0.6086841225624084
train gradient:  0.18142349914751765
iteration : 1727
train acc:  0.7265625
train loss:  0.5297518968582153
train gradient:  0.20938241916798253
iteration : 1728
train acc:  0.75
train loss:  0.539484977722168
train gradient:  0.14259047658566457
iteration : 1729
train acc:  0.703125
train loss:  0.5128729343414307
train gradient:  0.14585982355347327
iteration : 1730
train acc:  0.703125
train loss:  0.5561199188232422
train gradient:  0.17132822956386068
iteration : 1731
train acc:  0.7109375
train loss:  0.5824427604675293
train gradient:  0.19929704717528307
iteration : 1732
train acc:  0.7265625
train loss:  0.5388094186782837
train gradient:  0.14768851796775456
iteration : 1733
train acc:  0.7109375
train loss:  0.5700702667236328
train gradient:  0.2891678235483807
iteration : 1734
train acc:  0.671875
train loss:  0.5735639333724976
train gradient:  0.1469258877717852
iteration : 1735
train acc:  0.7265625
train loss:  0.5568111538887024
train gradient:  0.15659110792011866
iteration : 1736
train acc:  0.625
train loss:  0.6086569428443909
train gradient:  0.17797404436304215
iteration : 1737
train acc:  0.75
train loss:  0.49479490518569946
train gradient:  0.19050730204591002
iteration : 1738
train acc:  0.703125
train loss:  0.5701665878295898
train gradient:  0.15261536773304896
iteration : 1739
train acc:  0.7578125
train loss:  0.536376953125
train gradient:  0.1772895003814484
iteration : 1740
train acc:  0.734375
train loss:  0.5648878216743469
train gradient:  0.15053728527835028
iteration : 1741
train acc:  0.6953125
train loss:  0.5760772824287415
train gradient:  0.19622976162341124
iteration : 1742
train acc:  0.65625
train loss:  0.6149641275405884
train gradient:  0.18202382174375206
iteration : 1743
train acc:  0.7109375
train loss:  0.508099377155304
train gradient:  0.12990055008052798
iteration : 1744
train acc:  0.703125
train loss:  0.5435006618499756
train gradient:  0.19885675745235265
iteration : 1745
train acc:  0.6796875
train loss:  0.561028003692627
train gradient:  0.1314456694831503
iteration : 1746
train acc:  0.6875
train loss:  0.5723727941513062
train gradient:  0.19641029002316934
iteration : 1747
train acc:  0.7734375
train loss:  0.5155088901519775
train gradient:  0.18554156385085946
iteration : 1748
train acc:  0.7109375
train loss:  0.5305474400520325
train gradient:  0.1504612671569493
iteration : 1749
train acc:  0.7578125
train loss:  0.4802533686161041
train gradient:  0.12322454259578788
iteration : 1750
train acc:  0.796875
train loss:  0.48647063970565796
train gradient:  0.12118062100304994
iteration : 1751
train acc:  0.6484375
train loss:  0.5676613450050354
train gradient:  0.15651566257950622
iteration : 1752
train acc:  0.7109375
train loss:  0.6210690140724182
train gradient:  0.24626681667568098
iteration : 1753
train acc:  0.8203125
train loss:  0.48396942019462585
train gradient:  0.18324542039920755
iteration : 1754
train acc:  0.765625
train loss:  0.5052530765533447
train gradient:  0.17559290943902162
iteration : 1755
train acc:  0.6875
train loss:  0.5404835939407349
train gradient:  0.15068140682378756
iteration : 1756
train acc:  0.75
train loss:  0.4987133741378784
train gradient:  0.14567228917248365
iteration : 1757
train acc:  0.625
train loss:  0.6185871362686157
train gradient:  0.2016661724620328
iteration : 1758
train acc:  0.703125
train loss:  0.5835814476013184
train gradient:  0.2513858660538495
iteration : 1759
train acc:  0.71875
train loss:  0.5640420913696289
train gradient:  0.18680372033498124
iteration : 1760
train acc:  0.6875
train loss:  0.5574336051940918
train gradient:  0.14539664765649588
iteration : 1761
train acc:  0.6640625
train loss:  0.6086767911911011
train gradient:  0.2841640344706882
iteration : 1762
train acc:  0.703125
train loss:  0.5432128310203552
train gradient:  0.13916847153143933
iteration : 1763
train acc:  0.7265625
train loss:  0.5883432626724243
train gradient:  0.16195609058352373
iteration : 1764
train acc:  0.7109375
train loss:  0.5165249109268188
train gradient:  0.17892227910431677
iteration : 1765
train acc:  0.6796875
train loss:  0.5317800045013428
train gradient:  0.14409361429772946
iteration : 1766
train acc:  0.71875
train loss:  0.5240928530693054
train gradient:  0.15611091168771585
iteration : 1767
train acc:  0.734375
train loss:  0.53346186876297
train gradient:  0.18978830934057472
iteration : 1768
train acc:  0.6484375
train loss:  0.6134629845619202
train gradient:  0.21034655228233562
iteration : 1769
train acc:  0.6875
train loss:  0.5878984332084656
train gradient:  0.23166565298003516
iteration : 1770
train acc:  0.703125
train loss:  0.5543802976608276
train gradient:  0.1799031144662532
iteration : 1771
train acc:  0.6484375
train loss:  0.5651940107345581
train gradient:  0.19489759900445058
iteration : 1772
train acc:  0.7890625
train loss:  0.4817538857460022
train gradient:  0.1904516594593842
iteration : 1773
train acc:  0.765625
train loss:  0.5034504532814026
train gradient:  0.19214008927296575
iteration : 1774
train acc:  0.6875
train loss:  0.5750054121017456
train gradient:  0.16116660320240356
iteration : 1775
train acc:  0.65625
train loss:  0.5756911039352417
train gradient:  0.20618135612115018
iteration : 1776
train acc:  0.609375
train loss:  0.6200953722000122
train gradient:  0.21575310172252898
iteration : 1777
train acc:  0.6953125
train loss:  0.6023793816566467
train gradient:  0.24186813864629514
iteration : 1778
train acc:  0.6640625
train loss:  0.5803070068359375
train gradient:  0.2274794764054771
iteration : 1779
train acc:  0.6953125
train loss:  0.513361930847168
train gradient:  0.1616140553539454
iteration : 1780
train acc:  0.6953125
train loss:  0.5472736358642578
train gradient:  0.15978755955491958
iteration : 1781
train acc:  0.78125
train loss:  0.5232365131378174
train gradient:  0.13858977032143152
iteration : 1782
train acc:  0.6875
train loss:  0.557016134262085
train gradient:  0.13470903454733366
iteration : 1783
train acc:  0.6953125
train loss:  0.5473600029945374
train gradient:  0.1536800168904911
iteration : 1784
train acc:  0.8046875
train loss:  0.45724913477897644
train gradient:  0.1325618911018338
iteration : 1785
train acc:  0.6484375
train loss:  0.563739538192749
train gradient:  0.15034399139486343
iteration : 1786
train acc:  0.734375
train loss:  0.5366063117980957
train gradient:  0.16682426742862666
iteration : 1787
train acc:  0.7265625
train loss:  0.5052047967910767
train gradient:  0.14469446418791632
iteration : 1788
train acc:  0.71875
train loss:  0.49846044182777405
train gradient:  0.12805972411084093
iteration : 1789
train acc:  0.6796875
train loss:  0.5628862380981445
train gradient:  0.1606884547969682
iteration : 1790
train acc:  0.75
train loss:  0.5258922576904297
train gradient:  0.12439007736701065
iteration : 1791
train acc:  0.671875
train loss:  0.5314702391624451
train gradient:  0.1511370139047198
iteration : 1792
train acc:  0.7734375
train loss:  0.48115023970603943
train gradient:  0.1485608214578712
iteration : 1793
train acc:  0.7421875
train loss:  0.5486441850662231
train gradient:  0.15885754070643157
iteration : 1794
train acc:  0.671875
train loss:  0.550511360168457
train gradient:  0.165501928155823
iteration : 1795
train acc:  0.7421875
train loss:  0.5074388980865479
train gradient:  0.13116665318130072
iteration : 1796
train acc:  0.6796875
train loss:  0.5689078569412231
train gradient:  0.15686539119597026
iteration : 1797
train acc:  0.6640625
train loss:  0.5490385890007019
train gradient:  0.21008678661375965
iteration : 1798
train acc:  0.7265625
train loss:  0.49719494581222534
train gradient:  0.15675169166526898
iteration : 1799
train acc:  0.6796875
train loss:  0.614017128944397
train gradient:  0.2084890993658094
iteration : 1800
train acc:  0.6875
train loss:  0.5504258871078491
train gradient:  0.23596949290169666
iteration : 1801
train acc:  0.65625
train loss:  0.5771462917327881
train gradient:  0.20285049565523922
iteration : 1802
train acc:  0.7265625
train loss:  0.5229200124740601
train gradient:  0.12459807356613703
iteration : 1803
train acc:  0.7578125
train loss:  0.49734511971473694
train gradient:  0.142680874727307
iteration : 1804
train acc:  0.6640625
train loss:  0.5780137181282043
train gradient:  0.1775505980668308
iteration : 1805
train acc:  0.7421875
train loss:  0.5511947274208069
train gradient:  0.164156869828041
iteration : 1806
train acc:  0.734375
train loss:  0.5505136251449585
train gradient:  0.13002122327728416
iteration : 1807
train acc:  0.6875
train loss:  0.5437315702438354
train gradient:  0.13263613699527227
iteration : 1808
train acc:  0.671875
train loss:  0.609157145023346
train gradient:  0.16286992618488988
iteration : 1809
train acc:  0.734375
train loss:  0.5235872864723206
train gradient:  0.15745725792595655
iteration : 1810
train acc:  0.671875
train loss:  0.5815904140472412
train gradient:  0.19274310542500486
iteration : 1811
train acc:  0.6484375
train loss:  0.5987046957015991
train gradient:  0.21256066293873077
iteration : 1812
train acc:  0.734375
train loss:  0.5261244177818298
train gradient:  0.1431521460905017
iteration : 1813
train acc:  0.6796875
train loss:  0.5636003613471985
train gradient:  0.22427980535928294
iteration : 1814
train acc:  0.625
train loss:  0.6483926773071289
train gradient:  0.210215580643202
iteration : 1815
train acc:  0.765625
train loss:  0.5416502952575684
train gradient:  0.1500034123090681
iteration : 1816
train acc:  0.75
train loss:  0.5182020664215088
train gradient:  0.14392810454541172
iteration : 1817
train acc:  0.6484375
train loss:  0.570208728313446
train gradient:  0.14291360102901748
iteration : 1818
train acc:  0.71875
train loss:  0.5587762594223022
train gradient:  0.17006724510376928
iteration : 1819
train acc:  0.7109375
train loss:  0.5196027755737305
train gradient:  0.18430583971907014
iteration : 1820
train acc:  0.7265625
train loss:  0.5101288557052612
train gradient:  0.1307241715982025
iteration : 1821
train acc:  0.6484375
train loss:  0.5896987915039062
train gradient:  0.17618928862829764
iteration : 1822
train acc:  0.6953125
train loss:  0.5425839424133301
train gradient:  0.15332415420341425
iteration : 1823
train acc:  0.71875
train loss:  0.5498873591423035
train gradient:  0.1668872544652363
iteration : 1824
train acc:  0.703125
train loss:  0.5513649582862854
train gradient:  0.13621335881422297
iteration : 1825
train acc:  0.71875
train loss:  0.5078721642494202
train gradient:  0.14703711262703428
iteration : 1826
train acc:  0.734375
train loss:  0.5592924952507019
train gradient:  0.20724203980722808
iteration : 1827
train acc:  0.6484375
train loss:  0.5720946788787842
train gradient:  0.2255170968238339
iteration : 1828
train acc:  0.671875
train loss:  0.5907399654388428
train gradient:  0.3156830250866291
iteration : 1829
train acc:  0.703125
train loss:  0.5099948048591614
train gradient:  0.14474237494423314
iteration : 1830
train acc:  0.7265625
train loss:  0.5597842931747437
train gradient:  0.1395227395140097
iteration : 1831
train acc:  0.6796875
train loss:  0.5980456471443176
train gradient:  0.18557771717046456
iteration : 1832
train acc:  0.703125
train loss:  0.5513792037963867
train gradient:  0.17460625883006542
iteration : 1833
train acc:  0.65625
train loss:  0.6195837259292603
train gradient:  0.2654481007105345
iteration : 1834
train acc:  0.6640625
train loss:  0.6276102066040039
train gradient:  0.24126411082507895
iteration : 1835
train acc:  0.7265625
train loss:  0.5258800387382507
train gradient:  0.15358618604601065
iteration : 1836
train acc:  0.7890625
train loss:  0.5151194930076599
train gradient:  0.1550015394690098
iteration : 1837
train acc:  0.734375
train loss:  0.5148884654045105
train gradient:  0.1252339843271582
iteration : 1838
train acc:  0.671875
train loss:  0.5646858215332031
train gradient:  0.2276327608979516
iteration : 1839
train acc:  0.7578125
train loss:  0.5135806202888489
train gradient:  0.16469437667619463
iteration : 1840
train acc:  0.75
train loss:  0.5010089874267578
train gradient:  0.19957659811617467
iteration : 1841
train acc:  0.75
train loss:  0.5220125913619995
train gradient:  0.2035912029516425
iteration : 1842
train acc:  0.671875
train loss:  0.5971292853355408
train gradient:  0.24000952056200447
iteration : 1843
train acc:  0.765625
train loss:  0.5281758904457092
train gradient:  0.12839207711967093
iteration : 1844
train acc:  0.734375
train loss:  0.5381398797035217
train gradient:  0.14498029340929058
iteration : 1845
train acc:  0.75
train loss:  0.5524523854255676
train gradient:  0.22920851096152212
iteration : 1846
train acc:  0.703125
train loss:  0.5422794818878174
train gradient:  0.18253517019884202
iteration : 1847
train acc:  0.6640625
train loss:  0.5816330909729004
train gradient:  0.23225467013371445
iteration : 1848
train acc:  0.765625
train loss:  0.49977076053619385
train gradient:  0.20420363551856002
iteration : 1849
train acc:  0.6171875
train loss:  0.6188369989395142
train gradient:  0.234546480664806
iteration : 1850
train acc:  0.734375
train loss:  0.5250392556190491
train gradient:  0.211697215295387
iteration : 1851
train acc:  0.65625
train loss:  0.5760672092437744
train gradient:  0.2120788163127736
iteration : 1852
train acc:  0.6640625
train loss:  0.5725704431533813
train gradient:  0.18212844676977913
iteration : 1853
train acc:  0.671875
train loss:  0.5796070694923401
train gradient:  0.1542345539391793
iteration : 1854
train acc:  0.7109375
train loss:  0.5038909912109375
train gradient:  0.10714814194726356
iteration : 1855
train acc:  0.671875
train loss:  0.591106653213501
train gradient:  0.1682195548526308
iteration : 1856
train acc:  0.71875
train loss:  0.5158106684684753
train gradient:  0.14414366336298676
iteration : 1857
train acc:  0.6796875
train loss:  0.5752681493759155
train gradient:  0.2105818464058214
iteration : 1858
train acc:  0.703125
train loss:  0.5314968824386597
train gradient:  0.14075881613866487
iteration : 1859
train acc:  0.71875
train loss:  0.5362389087677002
train gradient:  0.15772912960809976
iteration : 1860
train acc:  0.71875
train loss:  0.5024837255477905
train gradient:  0.13127660145904435
iteration : 1861
train acc:  0.78125
train loss:  0.5222740173339844
train gradient:  0.18545507062407268
iteration : 1862
train acc:  0.75
train loss:  0.485201358795166
train gradient:  0.1303873892641989
iteration : 1863
train acc:  0.734375
train loss:  0.5151714086532593
train gradient:  0.11681231867551017
iteration : 1864
train acc:  0.703125
train loss:  0.5397716760635376
train gradient:  0.1536099322770157
iteration : 1865
train acc:  0.7578125
train loss:  0.4782513976097107
train gradient:  0.20317233384258082
iteration : 1866
train acc:  0.71875
train loss:  0.5030073523521423
train gradient:  0.19066602793400972
iteration : 1867
train acc:  0.734375
train loss:  0.48798060417175293
train gradient:  0.1448669924674439
iteration : 1868
train acc:  0.7421875
train loss:  0.4994203448295593
train gradient:  0.13231735622716362
iteration : 1869
train acc:  0.7109375
train loss:  0.5711877942085266
train gradient:  0.2084157078658724
iteration : 1870
train acc:  0.6640625
train loss:  0.5601788759231567
train gradient:  0.2129224401623499
iteration : 1871
train acc:  0.7265625
train loss:  0.5026835203170776
train gradient:  0.14890205086433111
iteration : 1872
train acc:  0.6796875
train loss:  0.575660765171051
train gradient:  0.2044108222369708
iteration : 1873
train acc:  0.734375
train loss:  0.5212684869766235
train gradient:  0.13568941516098565
iteration : 1874
train acc:  0.703125
train loss:  0.5847574472427368
train gradient:  0.2782219202533815
iteration : 1875
train acc:  0.7578125
train loss:  0.4993288516998291
train gradient:  0.13912950748516847
iteration : 1876
train acc:  0.6875
train loss:  0.5812026262283325
train gradient:  0.211689885369726
iteration : 1877
train acc:  0.703125
train loss:  0.5231550335884094
train gradient:  0.13856823823614844
iteration : 1878
train acc:  0.6328125
train loss:  0.6319259405136108
train gradient:  0.23255883039811284
iteration : 1879
train acc:  0.7109375
train loss:  0.5089643001556396
train gradient:  0.15149199571503297
iteration : 1880
train acc:  0.625
train loss:  0.6024495363235474
train gradient:  0.1766946429124349
iteration : 1881
train acc:  0.65625
train loss:  0.5560463070869446
train gradient:  0.16238500673620088
iteration : 1882
train acc:  0.71875
train loss:  0.4946836829185486
train gradient:  0.16070971804765624
iteration : 1883
train acc:  0.59375
train loss:  0.6456577181816101
train gradient:  0.2515544837784839
iteration : 1884
train acc:  0.609375
train loss:  0.6555452346801758
train gradient:  0.2090696502145894
iteration : 1885
train acc:  0.7578125
train loss:  0.5156171321868896
train gradient:  0.14020995765913408
iteration : 1886
train acc:  0.7109375
train loss:  0.5042424201965332
train gradient:  0.15757785819096634
iteration : 1887
train acc:  0.78125
train loss:  0.5457007884979248
train gradient:  0.13973542978587528
iteration : 1888
train acc:  0.6875
train loss:  0.5800739526748657
train gradient:  0.21558792773054553
iteration : 1889
train acc:  0.75
train loss:  0.5415239334106445
train gradient:  0.16535580133397532
iteration : 1890
train acc:  0.7265625
train loss:  0.5242646336555481
train gradient:  0.14892237831228033
iteration : 1891
train acc:  0.6796875
train loss:  0.5636863708496094
train gradient:  0.1844163887115019
iteration : 1892
train acc:  0.6875
train loss:  0.5388509631156921
train gradient:  0.1893525672565827
iteration : 1893
train acc:  0.6484375
train loss:  0.5955870747566223
train gradient:  0.22260846308818732
iteration : 1894
train acc:  0.71875
train loss:  0.5510543584823608
train gradient:  0.15885456142561766
iteration : 1895
train acc:  0.8203125
train loss:  0.46129798889160156
train gradient:  0.13411412328620184
iteration : 1896
train acc:  0.75
train loss:  0.4865405857563019
train gradient:  0.16607317936578825
iteration : 1897
train acc:  0.734375
train loss:  0.5296496152877808
train gradient:  0.1558947323330331
iteration : 1898
train acc:  0.71875
train loss:  0.5338678956031799
train gradient:  0.20633365914539328
iteration : 1899
train acc:  0.71875
train loss:  0.531384289264679
train gradient:  0.3132747535204122
iteration : 1900
train acc:  0.7890625
train loss:  0.4902544617652893
train gradient:  0.16617877973212009
iteration : 1901
train acc:  0.7265625
train loss:  0.5478310585021973
train gradient:  0.14542235735698977
iteration : 1902
train acc:  0.7265625
train loss:  0.5166638493537903
train gradient:  0.1803524760702595
iteration : 1903
train acc:  0.734375
train loss:  0.5331498980522156
train gradient:  0.1310486736624779
iteration : 1904
train acc:  0.65625
train loss:  0.5930884480476379
train gradient:  0.21849526242357314
iteration : 1905
train acc:  0.7734375
train loss:  0.518891453742981
train gradient:  0.18309487165365795
iteration : 1906
train acc:  0.6640625
train loss:  0.5526617169380188
train gradient:  0.1349472075658979
iteration : 1907
train acc:  0.6796875
train loss:  0.556114673614502
train gradient:  0.14825412768323104
iteration : 1908
train acc:  0.6640625
train loss:  0.5376884937286377
train gradient:  0.1368938331112208
iteration : 1909
train acc:  0.6328125
train loss:  0.623942494392395
train gradient:  0.27452989929899535
iteration : 1910
train acc:  0.6875
train loss:  0.6011292934417725
train gradient:  0.26272916856682965
iteration : 1911
train acc:  0.703125
train loss:  0.5690983533859253
train gradient:  0.1553939774381008
iteration : 1912
train acc:  0.703125
train loss:  0.5478560924530029
train gradient:  0.23172080332306982
iteration : 1913
train acc:  0.6953125
train loss:  0.5428884029388428
train gradient:  0.15699819159535833
iteration : 1914
train acc:  0.6328125
train loss:  0.5891798138618469
train gradient:  0.24342462796776643
iteration : 1915
train acc:  0.7265625
train loss:  0.5480120182037354
train gradient:  0.19228620288847093
iteration : 1916
train acc:  0.7109375
train loss:  0.5154038667678833
train gradient:  0.18518874937900998
iteration : 1917
train acc:  0.6875
train loss:  0.567380428314209
train gradient:  0.16780896197569023
iteration : 1918
train acc:  0.6953125
train loss:  0.6047952175140381
train gradient:  0.2040052579834019
iteration : 1919
train acc:  0.75
train loss:  0.49418187141418457
train gradient:  0.14256779164916686
iteration : 1920
train acc:  0.734375
train loss:  0.5344266295433044
train gradient:  0.12679819696989
iteration : 1921
train acc:  0.65625
train loss:  0.5821595191955566
train gradient:  0.21574760999768927
iteration : 1922
train acc:  0.703125
train loss:  0.593141496181488
train gradient:  0.1875903022616025
iteration : 1923
train acc:  0.75
train loss:  0.5037209987640381
train gradient:  0.13171953086126792
iteration : 1924
train acc:  0.7421875
train loss:  0.5043278336524963
train gradient:  0.13761605802675064
iteration : 1925
train acc:  0.7109375
train loss:  0.5320038795471191
train gradient:  0.14381783745275167
iteration : 1926
train acc:  0.703125
train loss:  0.590171217918396
train gradient:  0.17143291883177916
iteration : 1927
train acc:  0.734375
train loss:  0.5735054016113281
train gradient:  0.2669955252069577
iteration : 1928
train acc:  0.671875
train loss:  0.5900417566299438
train gradient:  0.28028705342661686
iteration : 1929
train acc:  0.703125
train loss:  0.5242562890052795
train gradient:  0.13970187719038635
iteration : 1930
train acc:  0.6953125
train loss:  0.5397238731384277
train gradient:  0.16498493164452352
iteration : 1931
train acc:  0.6796875
train loss:  0.5605189204216003
train gradient:  0.18095121889674015
iteration : 1932
train acc:  0.6484375
train loss:  0.5639014840126038
train gradient:  0.2108685789159572
iteration : 1933
train acc:  0.6875
train loss:  0.5597668290138245
train gradient:  0.14714959898895213
iteration : 1934
train acc:  0.7265625
train loss:  0.5049946308135986
train gradient:  0.18710957691193045
iteration : 1935
train acc:  0.7265625
train loss:  0.53465735912323
train gradient:  0.14843244458729143
iteration : 1936
train acc:  0.640625
train loss:  0.6405602693557739
train gradient:  0.2109159389788627
iteration : 1937
train acc:  0.765625
train loss:  0.4946858286857605
train gradient:  0.14016701286311142
iteration : 1938
train acc:  0.734375
train loss:  0.5176314115524292
train gradient:  0.13814908742543353
iteration : 1939
train acc:  0.7421875
train loss:  0.4881237745285034
train gradient:  0.15542864818250446
iteration : 1940
train acc:  0.703125
train loss:  0.5375298261642456
train gradient:  0.1347340745203223
iteration : 1941
train acc:  0.71875
train loss:  0.5377593040466309
train gradient:  0.16244480731243388
iteration : 1942
train acc:  0.703125
train loss:  0.5370662808418274
train gradient:  0.1377044016958346
iteration : 1943
train acc:  0.7109375
train loss:  0.5513806939125061
train gradient:  0.1602206463053965
iteration : 1944
train acc:  0.71875
train loss:  0.5275671482086182
train gradient:  0.17326831667398176
iteration : 1945
train acc:  0.640625
train loss:  0.5752724409103394
train gradient:  0.15498371843053804
iteration : 1946
train acc:  0.7265625
train loss:  0.5265614986419678
train gradient:  0.15669677130265508
iteration : 1947
train acc:  0.8046875
train loss:  0.4838449954986572
train gradient:  0.15016515374874298
iteration : 1948
train acc:  0.6953125
train loss:  0.540160596370697
train gradient:  0.18100395629448263
iteration : 1949
train acc:  0.71875
train loss:  0.5442928075790405
train gradient:  0.1652913791825987
iteration : 1950
train acc:  0.7109375
train loss:  0.534480094909668
train gradient:  0.22009180116152782
iteration : 1951
train acc:  0.6953125
train loss:  0.540245532989502
train gradient:  0.13520075529657705
iteration : 1952
train acc:  0.671875
train loss:  0.5833359956741333
train gradient:  0.21946651677950835
iteration : 1953
train acc:  0.6484375
train loss:  0.57999187707901
train gradient:  0.16635659270849984
iteration : 1954
train acc:  0.671875
train loss:  0.5820119976997375
train gradient:  0.24108967350415123
iteration : 1955
train acc:  0.6875
train loss:  0.5699647665023804
train gradient:  0.17797784566403008
iteration : 1956
train acc:  0.7265625
train loss:  0.5559290647506714
train gradient:  0.1609808738037125
iteration : 1957
train acc:  0.7265625
train loss:  0.5315791368484497
train gradient:  0.20044196196371727
iteration : 1958
train acc:  0.6953125
train loss:  0.5835679769515991
train gradient:  0.22602380947151396
iteration : 1959
train acc:  0.75
train loss:  0.5107330083847046
train gradient:  0.20571944165405553
iteration : 1960
train acc:  0.6953125
train loss:  0.5646255016326904
train gradient:  0.19029810993778307
iteration : 1961
train acc:  0.7265625
train loss:  0.5385690331459045
train gradient:  0.17472816589685902
iteration : 1962
train acc:  0.609375
train loss:  0.6832228899002075
train gradient:  0.219517145266643
iteration : 1963
train acc:  0.7421875
train loss:  0.5232903957366943
train gradient:  0.16693254766814028
iteration : 1964
train acc:  0.65625
train loss:  0.550506591796875
train gradient:  0.15002515863330892
iteration : 1965
train acc:  0.7421875
train loss:  0.5144562125205994
train gradient:  0.17285276674911737
iteration : 1966
train acc:  0.7578125
train loss:  0.5381450653076172
train gradient:  0.22984421920036924
iteration : 1967
train acc:  0.7421875
train loss:  0.5557160377502441
train gradient:  0.20022122274631343
iteration : 1968
train acc:  0.6875
train loss:  0.5358612537384033
train gradient:  0.14024649471132378
iteration : 1969
train acc:  0.703125
train loss:  0.5703859925270081
train gradient:  0.14610500535563564
iteration : 1970
train acc:  0.734375
train loss:  0.572083592414856
train gradient:  0.17230413821266394
iteration : 1971
train acc:  0.7421875
train loss:  0.49168622493743896
train gradient:  0.12757099356169826
iteration : 1972
train acc:  0.6796875
train loss:  0.6080669164657593
train gradient:  0.22134768293334073
iteration : 1973
train acc:  0.6953125
train loss:  0.576047420501709
train gradient:  0.17148059174157418
iteration : 1974
train acc:  0.71875
train loss:  0.5181056261062622
train gradient:  0.1619764536129854
iteration : 1975
train acc:  0.734375
train loss:  0.5197286009788513
train gradient:  0.15021522212513735
iteration : 1976
train acc:  0.71875
train loss:  0.5374218225479126
train gradient:  0.18458558928785282
iteration : 1977
train acc:  0.7109375
train loss:  0.5185551047325134
train gradient:  0.16349304244919544
iteration : 1978
train acc:  0.6796875
train loss:  0.5660839676856995
train gradient:  0.17346759092184127
iteration : 1979
train acc:  0.6875
train loss:  0.6365861892700195
train gradient:  0.2084723648075218
iteration : 1980
train acc:  0.7890625
train loss:  0.45526450872421265
train gradient:  0.13811206531355952
iteration : 1981
train acc:  0.78125
train loss:  0.5052928924560547
train gradient:  0.17257468956177013
iteration : 1982
train acc:  0.7265625
train loss:  0.5782062411308289
train gradient:  0.17251969061648403
iteration : 1983
train acc:  0.6953125
train loss:  0.5757766366004944
train gradient:  0.2129213245786417
iteration : 1984
train acc:  0.6953125
train loss:  0.5815701484680176
train gradient:  0.18394413429343465
iteration : 1985
train acc:  0.71875
train loss:  0.4902147054672241
train gradient:  0.21905559233269434
iteration : 1986
train acc:  0.75
train loss:  0.49323585629463196
train gradient:  0.20931915388459632
iteration : 1987
train acc:  0.75
train loss:  0.5028641223907471
train gradient:  0.11374930161478171
iteration : 1988
train acc:  0.7578125
train loss:  0.5385968089103699
train gradient:  0.1841509432497081
iteration : 1989
train acc:  0.7109375
train loss:  0.5402778387069702
train gradient:  0.14091885373890395
iteration : 1990
train acc:  0.7734375
train loss:  0.49276748299598694
train gradient:  0.1280438161526689
iteration : 1991
train acc:  0.6796875
train loss:  0.5753811597824097
train gradient:  0.18499603405549625
iteration : 1992
train acc:  0.75
train loss:  0.5659617185592651
train gradient:  0.1759989334870233
iteration : 1993
train acc:  0.7578125
train loss:  0.4659878611564636
train gradient:  0.11496794300189057
iteration : 1994
train acc:  0.75
train loss:  0.5010039806365967
train gradient:  0.14701831931441378
iteration : 1995
train acc:  0.734375
train loss:  0.5420913696289062
train gradient:  0.19234934355188152
iteration : 1996
train acc:  0.671875
train loss:  0.5711735486984253
train gradient:  0.15389129879533758
iteration : 1997
train acc:  0.7265625
train loss:  0.5517009496688843
train gradient:  0.15860503406676385
iteration : 1998
train acc:  0.734375
train loss:  0.5115852355957031
train gradient:  0.14070765598892676
iteration : 1999
train acc:  0.7265625
train loss:  0.5483256578445435
train gradient:  0.16144781828225535
iteration : 2000
train acc:  0.6875
train loss:  0.5460906028747559
train gradient:  0.14378329044380767
iteration : 2001
train acc:  0.7265625
train loss:  0.5114349126815796
train gradient:  0.19437707053454695
iteration : 2002
train acc:  0.71875
train loss:  0.5006612539291382
train gradient:  0.16510760049205506
iteration : 2003
train acc:  0.6953125
train loss:  0.5791224241256714
train gradient:  0.23477007729866745
iteration : 2004
train acc:  0.6796875
train loss:  0.5905805826187134
train gradient:  0.1857004280541055
iteration : 2005
train acc:  0.6796875
train loss:  0.6011439561843872
train gradient:  0.1652572383340016
iteration : 2006
train acc:  0.6953125
train loss:  0.5569151639938354
train gradient:  0.13462502600951037
iteration : 2007
train acc:  0.703125
train loss:  0.5167752504348755
train gradient:  0.1376618263348286
iteration : 2008
train acc:  0.65625
train loss:  0.5473646521568298
train gradient:  0.16317365741906306
iteration : 2009
train acc:  0.71875
train loss:  0.5561320185661316
train gradient:  0.21536520030659012
iteration : 2010
train acc:  0.7109375
train loss:  0.5315444469451904
train gradient:  0.1617523137624533
iteration : 2011
train acc:  0.7109375
train loss:  0.5651112198829651
train gradient:  0.20137760429560436
iteration : 2012
train acc:  0.7421875
train loss:  0.5219592452049255
train gradient:  0.16020221849366711
iteration : 2013
train acc:  0.703125
train loss:  0.5444879531860352
train gradient:  0.22707254793803597
iteration : 2014
train acc:  0.6796875
train loss:  0.6255491971969604
train gradient:  0.18069081906925982
iteration : 2015
train acc:  0.734375
train loss:  0.5701333284378052
train gradient:  0.18154108027602128
iteration : 2016
train acc:  0.6484375
train loss:  0.6083199977874756
train gradient:  0.2010152101132867
iteration : 2017
train acc:  0.6875
train loss:  0.5796290636062622
train gradient:  0.160446437950294
iteration : 2018
train acc:  0.6640625
train loss:  0.5217758417129517
train gradient:  0.11421162462859662
iteration : 2019
train acc:  0.625
train loss:  0.5853276252746582
train gradient:  0.19502668952221236
iteration : 2020
train acc:  0.703125
train loss:  0.5584650039672852
train gradient:  0.30528791981822356
iteration : 2021
train acc:  0.75
train loss:  0.5000366568565369
train gradient:  0.12254994891575895
iteration : 2022
train acc:  0.765625
train loss:  0.491210013628006
train gradient:  0.15417146250888236
iteration : 2023
train acc:  0.640625
train loss:  0.6363849639892578
train gradient:  0.22250086869476116
iteration : 2024
train acc:  0.625
train loss:  0.612151026725769
train gradient:  0.19366077081747912
iteration : 2025
train acc:  0.765625
train loss:  0.48958802223205566
train gradient:  0.13632180346010997
iteration : 2026
train acc:  0.6953125
train loss:  0.5484186410903931
train gradient:  0.17216275577484152
iteration : 2027
train acc:  0.671875
train loss:  0.6203932166099548
train gradient:  0.20844543808271235
iteration : 2028
train acc:  0.734375
train loss:  0.5181030631065369
train gradient:  0.1528034020307617
iteration : 2029
train acc:  0.7890625
train loss:  0.4765682518482208
train gradient:  0.12568260824643085
iteration : 2030
train acc:  0.625
train loss:  0.6693636178970337
train gradient:  0.2960720735121955
iteration : 2031
train acc:  0.640625
train loss:  0.5938066244125366
train gradient:  0.21783299841460504
iteration : 2032
train acc:  0.7734375
train loss:  0.5233955979347229
train gradient:  0.14806035112581845
iteration : 2033
train acc:  0.6953125
train loss:  0.5558924674987793
train gradient:  0.14192377472011414
iteration : 2034
train acc:  0.6953125
train loss:  0.593183696269989
train gradient:  0.2608674081608815
iteration : 2035
train acc:  0.7109375
train loss:  0.5897890329360962
train gradient:  0.149828000192223
iteration : 2036
train acc:  0.6484375
train loss:  0.5821472406387329
train gradient:  0.19729623082512238
iteration : 2037
train acc:  0.7421875
train loss:  0.5175105333328247
train gradient:  0.14899541446372633
iteration : 2038
train acc:  0.671875
train loss:  0.603861391544342
train gradient:  0.21650018237792584
iteration : 2039
train acc:  0.6796875
train loss:  0.6053380966186523
train gradient:  0.2536916179300134
iteration : 2040
train acc:  0.7421875
train loss:  0.5190913677215576
train gradient:  0.1924220651979242
iteration : 2041
train acc:  0.6875
train loss:  0.6162121295928955
train gradient:  0.15949976658532475
iteration : 2042
train acc:  0.703125
train loss:  0.5101940035820007
train gradient:  0.13183248047926
iteration : 2043
train acc:  0.71875
train loss:  0.5466920137405396
train gradient:  0.1447290219239217
iteration : 2044
train acc:  0.6953125
train loss:  0.5681183338165283
train gradient:  0.15225597483330994
iteration : 2045
train acc:  0.7890625
train loss:  0.495688796043396
train gradient:  0.1544436824976133
iteration : 2046
train acc:  0.71875
train loss:  0.5763391256332397
train gradient:  0.1768402300729764
iteration : 2047
train acc:  0.6484375
train loss:  0.5707330107688904
train gradient:  0.15147168044732856
iteration : 2048
train acc:  0.6328125
train loss:  0.5883306264877319
train gradient:  0.22245114229703689
iteration : 2049
train acc:  0.6875
train loss:  0.544511079788208
train gradient:  0.20638355761024768
iteration : 2050
train acc:  0.6640625
train loss:  0.5630158185958862
train gradient:  0.15307755232728476
iteration : 2051
train acc:  0.7265625
train loss:  0.5265277624130249
train gradient:  0.15953509744247862
iteration : 2052
train acc:  0.7578125
train loss:  0.5007443428039551
train gradient:  0.12800782861408663
iteration : 2053
train acc:  0.6953125
train loss:  0.620018720626831
train gradient:  0.24063833522904807
iteration : 2054
train acc:  0.671875
train loss:  0.5472314953804016
train gradient:  0.2231341635457867
iteration : 2055
train acc:  0.65625
train loss:  0.6112087965011597
train gradient:  0.1914777008016205
iteration : 2056
train acc:  0.703125
train loss:  0.6043501496315002
train gradient:  0.21644086319079292
iteration : 2057
train acc:  0.6796875
train loss:  0.5418441295623779
train gradient:  0.1997100451001973
iteration : 2058
train acc:  0.6875
train loss:  0.5674929618835449
train gradient:  0.21135825849535964
iteration : 2059
train acc:  0.71875
train loss:  0.5180885791778564
train gradient:  0.2141936549045004
iteration : 2060
train acc:  0.7421875
train loss:  0.5047681927680969
train gradient:  0.1428074874170438
iteration : 2061
train acc:  0.6171875
train loss:  0.638296365737915
train gradient:  0.2591297854213652
iteration : 2062
train acc:  0.7890625
train loss:  0.49286139011383057
train gradient:  0.14096913067618183
iteration : 2063
train acc:  0.734375
train loss:  0.5133156776428223
train gradient:  0.1586345013166739
iteration : 2064
train acc:  0.7421875
train loss:  0.47980815172195435
train gradient:  0.1266581114149679
iteration : 2065
train acc:  0.71875
train loss:  0.5660117864608765
train gradient:  0.25260189932856325
iteration : 2066
train acc:  0.6796875
train loss:  0.6068103909492493
train gradient:  0.19458571124382992
iteration : 2067
train acc:  0.7109375
train loss:  0.5416164398193359
train gradient:  0.14193493513318975
iteration : 2068
train acc:  0.71875
train loss:  0.5912001132965088
train gradient:  0.182616865781841
iteration : 2069
train acc:  0.6796875
train loss:  0.5646110773086548
train gradient:  0.21094813011811978
iteration : 2070
train acc:  0.7578125
train loss:  0.49321994185447693
train gradient:  0.13016369287088492
iteration : 2071
train acc:  0.6796875
train loss:  0.569414496421814
train gradient:  0.18027952365375077
iteration : 2072
train acc:  0.7265625
train loss:  0.5694798827171326
train gradient:  0.16741994698175033
iteration : 2073
train acc:  0.7265625
train loss:  0.5210568904876709
train gradient:  0.14626233760515991
iteration : 2074
train acc:  0.7109375
train loss:  0.5533066987991333
train gradient:  0.13394728883838025
iteration : 2075
train acc:  0.6953125
train loss:  0.5430220365524292
train gradient:  0.16881352378620745
iteration : 2076
train acc:  0.703125
train loss:  0.5731674432754517
train gradient:  0.17383582066297118
iteration : 2077
train acc:  0.734375
train loss:  0.6006729602813721
train gradient:  0.1945974986895306
iteration : 2078
train acc:  0.6796875
train loss:  0.5519635081291199
train gradient:  0.20112616404860986
iteration : 2079
train acc:  0.6796875
train loss:  0.5559317469596863
train gradient:  0.20282898882529124
iteration : 2080
train acc:  0.6875
train loss:  0.5236392021179199
train gradient:  0.19758439138927547
iteration : 2081
train acc:  0.703125
train loss:  0.5055389404296875
train gradient:  0.1372783890362217
iteration : 2082
train acc:  0.7421875
train loss:  0.5221675634384155
train gradient:  0.16500054558925475
iteration : 2083
train acc:  0.7109375
train loss:  0.5054876804351807
train gradient:  0.1498793008159346
iteration : 2084
train acc:  0.65625
train loss:  0.5407403111457825
train gradient:  0.1702948753672739
iteration : 2085
train acc:  0.65625
train loss:  0.5926010012626648
train gradient:  0.19304239654237593
iteration : 2086
train acc:  0.6640625
train loss:  0.5897216200828552
train gradient:  0.22434959006668267
iteration : 2087
train acc:  0.734375
train loss:  0.5082157850265503
train gradient:  0.13178247828767767
iteration : 2088
train acc:  0.75
train loss:  0.5047913789749146
train gradient:  0.14177649145741222
iteration : 2089
train acc:  0.7421875
train loss:  0.5193294286727905
train gradient:  0.16864766437985657
iteration : 2090
train acc:  0.6953125
train loss:  0.5481157302856445
train gradient:  0.24685644650601307
iteration : 2091
train acc:  0.75
train loss:  0.550552487373352
train gradient:  0.20554753605405326
iteration : 2092
train acc:  0.75
train loss:  0.524652361869812
train gradient:  0.1593164383625121
iteration : 2093
train acc:  0.6328125
train loss:  0.6397244930267334
train gradient:  0.2419467917971601
iteration : 2094
train acc:  0.75
train loss:  0.5119083523750305
train gradient:  0.14059873251565858
iteration : 2095
train acc:  0.7578125
train loss:  0.47487592697143555
train gradient:  0.14793615116470615
iteration : 2096
train acc:  0.7109375
train loss:  0.5273801684379578
train gradient:  0.14673355652090087
iteration : 2097
train acc:  0.703125
train loss:  0.5548979043960571
train gradient:  0.15736907419313806
iteration : 2098
train acc:  0.6640625
train loss:  0.5910031199455261
train gradient:  0.22669789759372355
iteration : 2099
train acc:  0.7109375
train loss:  0.6110561490058899
train gradient:  0.2101782970138918
iteration : 2100
train acc:  0.7265625
train loss:  0.5498294234275818
train gradient:  0.16190937490150042
iteration : 2101
train acc:  0.7421875
train loss:  0.517296314239502
train gradient:  0.15857926680662998
iteration : 2102
train acc:  0.71875
train loss:  0.5273011326789856
train gradient:  0.150529729909619
iteration : 2103
train acc:  0.75
train loss:  0.5740138292312622
train gradient:  0.19047008492198986
iteration : 2104
train acc:  0.7265625
train loss:  0.508440375328064
train gradient:  0.12530739488176382
iteration : 2105
train acc:  0.6875
train loss:  0.5270684957504272
train gradient:  0.15546898513471724
iteration : 2106
train acc:  0.6953125
train loss:  0.5447463989257812
train gradient:  0.147576462774612
iteration : 2107
train acc:  0.7578125
train loss:  0.4974614083766937
train gradient:  0.13308372107965277
iteration : 2108
train acc:  0.703125
train loss:  0.5448585748672485
train gradient:  0.13432420564716235
iteration : 2109
train acc:  0.7734375
train loss:  0.5155521631240845
train gradient:  0.16579936210506846
iteration : 2110
train acc:  0.7265625
train loss:  0.5457897186279297
train gradient:  0.1983367372757473
iteration : 2111
train acc:  0.703125
train loss:  0.597183108329773
train gradient:  0.20239559185723205
iteration : 2112
train acc:  0.6953125
train loss:  0.5154844522476196
train gradient:  0.19880079321714234
iteration : 2113
train acc:  0.6875
train loss:  0.5784053802490234
train gradient:  0.14970087687939734
iteration : 2114
train acc:  0.6953125
train loss:  0.5989727973937988
train gradient:  0.19698836839680572
iteration : 2115
train acc:  0.703125
train loss:  0.537697434425354
train gradient:  0.1667520945417479
iteration : 2116
train acc:  0.71875
train loss:  0.5374200940132141
train gradient:  0.19696940329865725
iteration : 2117
train acc:  0.625
train loss:  0.5930654406547546
train gradient:  0.17099307493608665
iteration : 2118
train acc:  0.71875
train loss:  0.5347490310668945
train gradient:  0.12513982475269947
iteration : 2119
train acc:  0.7265625
train loss:  0.5441741943359375
train gradient:  0.188419232612819
iteration : 2120
train acc:  0.671875
train loss:  0.5453591346740723
train gradient:  0.2389423163385067
iteration : 2121
train acc:  0.609375
train loss:  0.6172753572463989
train gradient:  0.19123696743045937
iteration : 2122
train acc:  0.671875
train loss:  0.5707792043685913
train gradient:  0.2042157165331303
iteration : 2123
train acc:  0.6640625
train loss:  0.5832775235176086
train gradient:  0.1537562206688624
iteration : 2124
train acc:  0.6640625
train loss:  0.54335618019104
train gradient:  0.1857657613972225
iteration : 2125
train acc:  0.6875
train loss:  0.5409471392631531
train gradient:  0.14136024834216068
iteration : 2126
train acc:  0.6953125
train loss:  0.5489060878753662
train gradient:  0.17538498363242422
iteration : 2127
train acc:  0.7265625
train loss:  0.5041397213935852
train gradient:  0.126857385703103
iteration : 2128
train acc:  0.703125
train loss:  0.5619404315948486
train gradient:  0.1871379497177309
iteration : 2129
train acc:  0.734375
train loss:  0.4651804566383362
train gradient:  0.11970774289065775
iteration : 2130
train acc:  0.7109375
train loss:  0.571733295917511
train gradient:  0.17547876281470276
iteration : 2131
train acc:  0.71875
train loss:  0.4924912750720978
train gradient:  0.15059587636220895
iteration : 2132
train acc:  0.7421875
train loss:  0.49920105934143066
train gradient:  0.1543315702459267
iteration : 2133
train acc:  0.71875
train loss:  0.5730896592140198
train gradient:  0.15539847995541167
iteration : 2134
train acc:  0.8046875
train loss:  0.48128166794776917
train gradient:  0.1611350234459865
iteration : 2135
train acc:  0.71875
train loss:  0.5298622846603394
train gradient:  0.15769338754144308
iteration : 2136
train acc:  0.71875
train loss:  0.4814212918281555
train gradient:  0.12563832220087023
iteration : 2137
train acc:  0.703125
train loss:  0.5860044956207275
train gradient:  0.1587731391599564
iteration : 2138
train acc:  0.7578125
train loss:  0.4987506568431854
train gradient:  0.17064809909318873
iteration : 2139
train acc:  0.7421875
train loss:  0.5032006502151489
train gradient:  0.22478071494681068
iteration : 2140
train acc:  0.734375
train loss:  0.48147499561309814
train gradient:  0.15865633704161036
iteration : 2141
train acc:  0.5703125
train loss:  0.6942524909973145
train gradient:  0.3917527486598404
iteration : 2142
train acc:  0.75
train loss:  0.5278533697128296
train gradient:  0.1668868697522879
iteration : 2143
train acc:  0.734375
train loss:  0.5549779534339905
train gradient:  0.19366447321086988
iteration : 2144
train acc:  0.7421875
train loss:  0.4876067042350769
train gradient:  0.14540859396362937
iteration : 2145
train acc:  0.6953125
train loss:  0.6151295900344849
train gradient:  0.18631187987025258
iteration : 2146
train acc:  0.6484375
train loss:  0.6360158920288086
train gradient:  0.20953091511686756
iteration : 2147
train acc:  0.78125
train loss:  0.5146623849868774
train gradient:  0.16602546506773685
iteration : 2148
train acc:  0.765625
train loss:  0.4892406463623047
train gradient:  0.16899631758826833
iteration : 2149
train acc:  0.71875
train loss:  0.5874780416488647
train gradient:  0.23977830162193586
iteration : 2150
train acc:  0.7265625
train loss:  0.529090404510498
train gradient:  0.1685563723468454
iteration : 2151
train acc:  0.71875
train loss:  0.5486552715301514
train gradient:  0.14802090457596978
iteration : 2152
train acc:  0.7265625
train loss:  0.5410522222518921
train gradient:  0.17669786386795155
iteration : 2153
train acc:  0.71875
train loss:  0.5519576072692871
train gradient:  0.1450885634102398
iteration : 2154
train acc:  0.7265625
train loss:  0.5243388414382935
train gradient:  0.14117342739017416
iteration : 2155
train acc:  0.6875
train loss:  0.5631973147392273
train gradient:  0.36820057218395025
iteration : 2156
train acc:  0.75
train loss:  0.5135195851325989
train gradient:  0.1553101344916426
iteration : 2157
train acc:  0.65625
train loss:  0.5892921686172485
train gradient:  0.17090746083540978
iteration : 2158
train acc:  0.703125
train loss:  0.5599280595779419
train gradient:  0.19209592875546083
iteration : 2159
train acc:  0.7421875
train loss:  0.5230154991149902
train gradient:  0.17219955565637607
iteration : 2160
train acc:  0.703125
train loss:  0.5610450506210327
train gradient:  0.1538958220318481
iteration : 2161
train acc:  0.7109375
train loss:  0.5249856114387512
train gradient:  0.1412726474166216
iteration : 2162
train acc:  0.671875
train loss:  0.5448628067970276
train gradient:  0.18700823542331568
iteration : 2163
train acc:  0.625
train loss:  0.5861886739730835
train gradient:  0.2402843274420837
iteration : 2164
train acc:  0.7578125
train loss:  0.4926709532737732
train gradient:  0.12428325243167825
iteration : 2165
train acc:  0.7265625
train loss:  0.5744143724441528
train gradient:  0.16323316002713648
iteration : 2166
train acc:  0.7578125
train loss:  0.5126999616622925
train gradient:  0.15902583953518454
iteration : 2167
train acc:  0.6953125
train loss:  0.5752949118614197
train gradient:  0.18242761920232464
iteration : 2168
train acc:  0.734375
train loss:  0.5396672487258911
train gradient:  0.19917175318842273
iteration : 2169
train acc:  0.65625
train loss:  0.5972356200218201
train gradient:  0.17366638288116393
iteration : 2170
train acc:  0.71875
train loss:  0.5587153434753418
train gradient:  0.15398785134381657
iteration : 2171
train acc:  0.75
train loss:  0.5016797780990601
train gradient:  0.1861536300507322
iteration : 2172
train acc:  0.6484375
train loss:  0.5790313482284546
train gradient:  0.15944738210273632
iteration : 2173
train acc:  0.6796875
train loss:  0.5708770751953125
train gradient:  0.15868296830677386
iteration : 2174
train acc:  0.7265625
train loss:  0.5530316829681396
train gradient:  0.14829452569732163
iteration : 2175
train acc:  0.65625
train loss:  0.628997266292572
train gradient:  0.20874024340959463
iteration : 2176
train acc:  0.6796875
train loss:  0.5743640065193176
train gradient:  0.1751910311472617
iteration : 2177
train acc:  0.7578125
train loss:  0.5203832387924194
train gradient:  0.15691923128241642
iteration : 2178
train acc:  0.7109375
train loss:  0.5620783567428589
train gradient:  0.17279755075987757
iteration : 2179
train acc:  0.6875
train loss:  0.5769975781440735
train gradient:  0.18306661556676065
iteration : 2180
train acc:  0.7421875
train loss:  0.5073916912078857
train gradient:  0.1318666437144572
iteration : 2181
train acc:  0.6875
train loss:  0.5848813056945801
train gradient:  0.18684398528235044
iteration : 2182
train acc:  0.6796875
train loss:  0.5542539358139038
train gradient:  0.1812441769029621
iteration : 2183
train acc:  0.671875
train loss:  0.5468642115592957
train gradient:  0.16786491819610244
iteration : 2184
train acc:  0.7109375
train loss:  0.5456749796867371
train gradient:  0.27088133522709995
iteration : 2185
train acc:  0.78125
train loss:  0.5219789147377014
train gradient:  0.19571911443863238
iteration : 2186
train acc:  0.703125
train loss:  0.5328224897384644
train gradient:  0.1376982988396631
iteration : 2187
train acc:  0.703125
train loss:  0.5332262516021729
train gradient:  0.18579942582586798
iteration : 2188
train acc:  0.7421875
train loss:  0.5525022745132446
train gradient:  0.1877503981698395
iteration : 2189
train acc:  0.7421875
train loss:  0.5116326808929443
train gradient:  0.16158149857698212
iteration : 2190
train acc:  0.734375
train loss:  0.5210200548171997
train gradient:  0.1733920229429876
iteration : 2191
train acc:  0.6796875
train loss:  0.5917319655418396
train gradient:  0.19823701831707272
iteration : 2192
train acc:  0.6875
train loss:  0.5863521099090576
train gradient:  0.15986770648932072
iteration : 2193
train acc:  0.671875
train loss:  0.5910576581954956
train gradient:  0.2031029962853721
iteration : 2194
train acc:  0.78125
train loss:  0.521620512008667
train gradient:  0.14339105848463907
iteration : 2195
train acc:  0.734375
train loss:  0.5036506652832031
train gradient:  0.1779592617543187
iteration : 2196
train acc:  0.7578125
train loss:  0.5103498697280884
train gradient:  0.1631745839468334
iteration : 2197
train acc:  0.671875
train loss:  0.5880521535873413
train gradient:  0.210668240055071
iteration : 2198
train acc:  0.7734375
train loss:  0.501240074634552
train gradient:  0.17894770303933605
iteration : 2199
train acc:  0.671875
train loss:  0.5722231864929199
train gradient:  0.21026544559973298
iteration : 2200
train acc:  0.6796875
train loss:  0.5452285408973694
train gradient:  0.1344118319960943
iteration : 2201
train acc:  0.671875
train loss:  0.5956929922103882
train gradient:  0.2347484430429883
iteration : 2202
train acc:  0.625
train loss:  0.6329281330108643
train gradient:  0.3169412889202014
iteration : 2203
train acc:  0.703125
train loss:  0.5678048133850098
train gradient:  0.2038536665885965
iteration : 2204
train acc:  0.71875
train loss:  0.4967483878135681
train gradient:  0.155499950812654
iteration : 2205
train acc:  0.6796875
train loss:  0.570639431476593
train gradient:  0.2692944321581012
iteration : 2206
train acc:  0.734375
train loss:  0.4893920421600342
train gradient:  0.12861201333141223
iteration : 2207
train acc:  0.7578125
train loss:  0.5098410844802856
train gradient:  0.16777980655796376
iteration : 2208
train acc:  0.734375
train loss:  0.5406477451324463
train gradient:  0.20502845188107746
iteration : 2209
train acc:  0.671875
train loss:  0.561328649520874
train gradient:  0.19621740416477634
iteration : 2210
train acc:  0.7578125
train loss:  0.5122391581535339
train gradient:  0.11982346822482573
iteration : 2211
train acc:  0.75
train loss:  0.5150624513626099
train gradient:  0.15003337213063675
iteration : 2212
train acc:  0.6640625
train loss:  0.5548442602157593
train gradient:  0.17818328058720886
iteration : 2213
train acc:  0.6953125
train loss:  0.510584831237793
train gradient:  0.14280294711061167
iteration : 2214
train acc:  0.7265625
train loss:  0.5294942259788513
train gradient:  0.16787430204337486
iteration : 2215
train acc:  0.7109375
train loss:  0.521598219871521
train gradient:  0.23961162229988953
iteration : 2216
train acc:  0.6875
train loss:  0.5404995083808899
train gradient:  0.1521848290199503
iteration : 2217
train acc:  0.703125
train loss:  0.5417486429214478
train gradient:  0.1618011352342622
iteration : 2218
train acc:  0.6953125
train loss:  0.5220764875411987
train gradient:  0.13509759071140667
iteration : 2219
train acc:  0.7265625
train loss:  0.5220479965209961
train gradient:  0.29271938804510883
iteration : 2220
train acc:  0.6953125
train loss:  0.5439636707305908
train gradient:  0.20641396751631735
iteration : 2221
train acc:  0.7421875
train loss:  0.5088626146316528
train gradient:  0.1454521540550361
iteration : 2222
train acc:  0.75
train loss:  0.4992775619029999
train gradient:  0.15089646468487808
iteration : 2223
train acc:  0.7578125
train loss:  0.5286257863044739
train gradient:  0.14922489619209767
iteration : 2224
train acc:  0.6640625
train loss:  0.5860417485237122
train gradient:  0.162243550367062
iteration : 2225
train acc:  0.7421875
train loss:  0.5241062045097351
train gradient:  0.20096345056279202
iteration : 2226
train acc:  0.7265625
train loss:  0.5177967548370361
train gradient:  0.15963325514371918
iteration : 2227
train acc:  0.71875
train loss:  0.5059149265289307
train gradient:  0.16932611672783426
iteration : 2228
train acc:  0.765625
train loss:  0.5089980959892273
train gradient:  0.17269157687049433
iteration : 2229
train acc:  0.7109375
train loss:  0.5424281358718872
train gradient:  0.16316545623396006
iteration : 2230
train acc:  0.7109375
train loss:  0.5257636904716492
train gradient:  0.16085165416490804
iteration : 2231
train acc:  0.6640625
train loss:  0.5784022808074951
train gradient:  0.23453007300290218
iteration : 2232
train acc:  0.734375
train loss:  0.5341665148735046
train gradient:  0.16093601022052967
iteration : 2233
train acc:  0.75
train loss:  0.5082240700721741
train gradient:  0.1551745635493737
iteration : 2234
train acc:  0.7265625
train loss:  0.5402314066886902
train gradient:  0.15903956254954904
iteration : 2235
train acc:  0.7578125
train loss:  0.5267859101295471
train gradient:  0.16582472635993092
iteration : 2236
train acc:  0.7578125
train loss:  0.5171875953674316
train gradient:  0.1493009465398129
iteration : 2237
train acc:  0.7109375
train loss:  0.5601181983947754
train gradient:  0.1911807362269652
iteration : 2238
train acc:  0.7578125
train loss:  0.4787648320198059
train gradient:  0.1160324760810616
iteration : 2239
train acc:  0.65625
train loss:  0.5428977012634277
train gradient:  0.1393159316986
iteration : 2240
train acc:  0.7265625
train loss:  0.5124499201774597
train gradient:  0.1587427352272595
iteration : 2241
train acc:  0.7109375
train loss:  0.5086486339569092
train gradient:  0.1880669750488417
iteration : 2242
train acc:  0.71875
train loss:  0.5285789370536804
train gradient:  0.17758523943381904
iteration : 2243
train acc:  0.671875
train loss:  0.5724812746047974
train gradient:  0.19029727301708257
iteration : 2244
train acc:  0.671875
train loss:  0.5792146921157837
train gradient:  0.18525071368260837
iteration : 2245
train acc:  0.7734375
train loss:  0.5421665906906128
train gradient:  0.1911341206422555
iteration : 2246
train acc:  0.7421875
train loss:  0.5514493584632874
train gradient:  0.1563680250711629
iteration : 2247
train acc:  0.7109375
train loss:  0.5162540674209595
train gradient:  0.1484041081854045
iteration : 2248
train acc:  0.640625
train loss:  0.611251950263977
train gradient:  0.22456124207252612
iteration : 2249
train acc:  0.6484375
train loss:  0.5965107083320618
train gradient:  0.18668411678470698
iteration : 2250
train acc:  0.6796875
train loss:  0.5608140826225281
train gradient:  0.18003746743035043
iteration : 2251
train acc:  0.796875
train loss:  0.4786819815635681
train gradient:  0.16955669239190468
iteration : 2252
train acc:  0.6640625
train loss:  0.5998523235321045
train gradient:  0.2092641651137918
iteration : 2253
train acc:  0.734375
train loss:  0.5468631982803345
train gradient:  0.1496176828138433
iteration : 2254
train acc:  0.65625
train loss:  0.6323785781860352
train gradient:  0.250063465745172
iteration : 2255
train acc:  0.765625
train loss:  0.4682216942310333
train gradient:  0.13212752231335845
iteration : 2256
train acc:  0.75
train loss:  0.557759165763855
train gradient:  0.22062165640827058
iteration : 2257
train acc:  0.671875
train loss:  0.6138423681259155
train gradient:  0.268313258766637
iteration : 2258
train acc:  0.7109375
train loss:  0.5138988494873047
train gradient:  0.17098465930826084
iteration : 2259
train acc:  0.71875
train loss:  0.5353874564170837
train gradient:  0.18733837301777073
iteration : 2260
train acc:  0.7265625
train loss:  0.5233343243598938
train gradient:  0.17035377974318483
iteration : 2261
train acc:  0.6640625
train loss:  0.542502224445343
train gradient:  0.16028189077142085
iteration : 2262
train acc:  0.7578125
train loss:  0.47320425510406494
train gradient:  0.13470072251469567
iteration : 2263
train acc:  0.7109375
train loss:  0.5907130241394043
train gradient:  0.1950017980608457
iteration : 2264
train acc:  0.8046875
train loss:  0.502871036529541
train gradient:  0.14871103425849308
iteration : 2265
train acc:  0.6953125
train loss:  0.5758504271507263
train gradient:  0.17524845343275658
iteration : 2266
train acc:  0.7578125
train loss:  0.5401929616928101
train gradient:  0.15483471790562903
iteration : 2267
train acc:  0.6640625
train loss:  0.556443452835083
train gradient:  0.17238621537390558
iteration : 2268
train acc:  0.6875
train loss:  0.5755289793014526
train gradient:  0.20556642147732146
iteration : 2269
train acc:  0.7421875
train loss:  0.5153802633285522
train gradient:  0.2688527157167931
iteration : 2270
train acc:  0.6796875
train loss:  0.5778682231903076
train gradient:  0.18075988125503162
iteration : 2271
train acc:  0.7890625
train loss:  0.4915409982204437
train gradient:  0.11526511346650144
iteration : 2272
train acc:  0.75
train loss:  0.5074425935745239
train gradient:  0.15792145027799476
iteration : 2273
train acc:  0.7734375
train loss:  0.46947747468948364
train gradient:  0.14274916968170437
iteration : 2274
train acc:  0.640625
train loss:  0.576008677482605
train gradient:  0.21177417271492893
iteration : 2275
train acc:  0.703125
train loss:  0.5133138298988342
train gradient:  0.20687485329268174
iteration : 2276
train acc:  0.6640625
train loss:  0.5777971148490906
train gradient:  0.17183496069837317
iteration : 2277
train acc:  0.71875
train loss:  0.5456625819206238
train gradient:  0.13932529449363343
iteration : 2278
train acc:  0.7265625
train loss:  0.5381104350090027
train gradient:  0.18539730718578193
iteration : 2279
train acc:  0.71875
train loss:  0.5129379630088806
train gradient:  0.14678838807760541
iteration : 2280
train acc:  0.7265625
train loss:  0.5455046892166138
train gradient:  0.16724068716813867
iteration : 2281
train acc:  0.6953125
train loss:  0.5814318656921387
train gradient:  0.23046121885447673
iteration : 2282
train acc:  0.734375
train loss:  0.4947086572647095
train gradient:  0.15758825660652467
iteration : 2283
train acc:  0.6875
train loss:  0.6080145835876465
train gradient:  0.2380492115057764
iteration : 2284
train acc:  0.6796875
train loss:  0.5957754850387573
train gradient:  0.3348789314766432
iteration : 2285
train acc:  0.6953125
train loss:  0.5364123582839966
train gradient:  0.15383474646189788
iteration : 2286
train acc:  0.6796875
train loss:  0.551787257194519
train gradient:  0.16574765800616165
iteration : 2287
train acc:  0.6953125
train loss:  0.5653120875358582
train gradient:  0.22012804871945124
iteration : 2288
train acc:  0.78125
train loss:  0.4713825285434723
train gradient:  0.14137848869034086
iteration : 2289
train acc:  0.6328125
train loss:  0.6079273223876953
train gradient:  0.21647442788322668
iteration : 2290
train acc:  0.703125
train loss:  0.5252914428710938
train gradient:  0.14688943560756854
iteration : 2291
train acc:  0.734375
train loss:  0.5358154773712158
train gradient:  0.13793989926104716
iteration : 2292
train acc:  0.765625
train loss:  0.5360190272331238
train gradient:  0.13749174532580866
iteration : 2293
train acc:  0.6953125
train loss:  0.5611151456832886
train gradient:  0.17859531862991762
iteration : 2294
train acc:  0.7109375
train loss:  0.5139685273170471
train gradient:  0.18633765034553662
iteration : 2295
train acc:  0.6875
train loss:  0.5668720006942749
train gradient:  0.26976350965508633
iteration : 2296
train acc:  0.7421875
train loss:  0.5283526182174683
train gradient:  0.1456645598460469
iteration : 2297
train acc:  0.703125
train loss:  0.6007982492446899
train gradient:  0.2106080550702421
iteration : 2298
train acc:  0.6640625
train loss:  0.5725653171539307
train gradient:  0.1832631408924285
iteration : 2299
train acc:  0.7421875
train loss:  0.49848631024360657
train gradient:  0.16281481418670607
iteration : 2300
train acc:  0.7109375
train loss:  0.5039303302764893
train gradient:  0.15678286623255688
iteration : 2301
train acc:  0.7421875
train loss:  0.532960057258606
train gradient:  0.1471099992134141
iteration : 2302
train acc:  0.734375
train loss:  0.5199205875396729
train gradient:  0.12391600069860215
iteration : 2303
train acc:  0.671875
train loss:  0.5734494924545288
train gradient:  0.22192967449853837
iteration : 2304
train acc:  0.7109375
train loss:  0.5573490858078003
train gradient:  0.16945412064244753
iteration : 2305
train acc:  0.7265625
train loss:  0.5473151803016663
train gradient:  0.18126599409931993
iteration : 2306
train acc:  0.6484375
train loss:  0.5840921401977539
train gradient:  0.2826263349898677
iteration : 2307
train acc:  0.7265625
train loss:  0.5697288513183594
train gradient:  0.1764851664367253
iteration : 2308
train acc:  0.6875
train loss:  0.541836142539978
train gradient:  0.18951474706607174
iteration : 2309
train acc:  0.640625
train loss:  0.5463130474090576
train gradient:  0.16097718785040888
iteration : 2310
train acc:  0.703125
train loss:  0.5168907642364502
train gradient:  0.15955490178363474
iteration : 2311
train acc:  0.6796875
train loss:  0.5563414096832275
train gradient:  0.14509123133768695
iteration : 2312
train acc:  0.71875
train loss:  0.5333397388458252
train gradient:  0.13479003663322195
iteration : 2313
train acc:  0.7421875
train loss:  0.5139391422271729
train gradient:  0.15506431997825507
iteration : 2314
train acc:  0.7109375
train loss:  0.5709829330444336
train gradient:  0.177088940957519
iteration : 2315
train acc:  0.6875
train loss:  0.565148115158081
train gradient:  0.1632721492123567
iteration : 2316
train acc:  0.7265625
train loss:  0.5086756944656372
train gradient:  0.13363648980742576
iteration : 2317
train acc:  0.71875
train loss:  0.5418095588684082
train gradient:  0.18235431450461015
iteration : 2318
train acc:  0.6953125
train loss:  0.5674566626548767
train gradient:  0.16129857087127214
iteration : 2319
train acc:  0.7421875
train loss:  0.5641834735870361
train gradient:  0.16189106535868722
iteration : 2320
train acc:  0.7578125
train loss:  0.5134131908416748
train gradient:  0.14146687815897346
iteration : 2321
train acc:  0.6484375
train loss:  0.6160909533500671
train gradient:  0.21357738538285542
iteration : 2322
train acc:  0.65625
train loss:  0.6056013107299805
train gradient:  0.2109839961630064
iteration : 2323
train acc:  0.640625
train loss:  0.6270266771316528
train gradient:  0.26908903602404544
iteration : 2324
train acc:  0.6796875
train loss:  0.570533275604248
train gradient:  0.23488188092507845
iteration : 2325
train acc:  0.71875
train loss:  0.5062419176101685
train gradient:  0.19426826776901912
iteration : 2326
train acc:  0.6875
train loss:  0.5445154905319214
train gradient:  0.17957223129616073
iteration : 2327
train acc:  0.71875
train loss:  0.5044105052947998
train gradient:  0.13634587612373922
iteration : 2328
train acc:  0.6953125
train loss:  0.5334444642066956
train gradient:  0.15811418886061396
iteration : 2329
train acc:  0.703125
train loss:  0.5798738598823547
train gradient:  0.20703004985445547
iteration : 2330
train acc:  0.6796875
train loss:  0.5688177347183228
train gradient:  0.19559498500577482
iteration : 2331
train acc:  0.71875
train loss:  0.5357489585876465
train gradient:  0.15729206975622306
iteration : 2332
train acc:  0.7421875
train loss:  0.5121673941612244
train gradient:  0.17937731363729925
iteration : 2333
train acc:  0.6875
train loss:  0.5498836040496826
train gradient:  0.20911050336375145
iteration : 2334
train acc:  0.6953125
train loss:  0.5512597560882568
train gradient:  0.17038477297127252
iteration : 2335
train acc:  0.6875
train loss:  0.5247783064842224
train gradient:  0.14439818581173003
iteration : 2336
train acc:  0.734375
train loss:  0.5045926570892334
train gradient:  0.1577510489781877
iteration : 2337
train acc:  0.7265625
train loss:  0.5635741353034973
train gradient:  0.16603733243533858
iteration : 2338
train acc:  0.75
train loss:  0.5044638514518738
train gradient:  0.21131514466134638
iteration : 2339
train acc:  0.7421875
train loss:  0.5301425457000732
train gradient:  0.17601054961250356
iteration : 2340
train acc:  0.6875
train loss:  0.542643129825592
train gradient:  0.1641554881281349
iteration : 2341
train acc:  0.703125
train loss:  0.5388768911361694
train gradient:  0.15430547981564072
iteration : 2342
train acc:  0.6953125
train loss:  0.5161889791488647
train gradient:  0.17401844982808568
iteration : 2343
train acc:  0.78125
train loss:  0.4979121685028076
train gradient:  0.17972225997600869
iteration : 2344
train acc:  0.7734375
train loss:  0.48717358708381653
train gradient:  0.1387079635604771
iteration : 2345
train acc:  0.7265625
train loss:  0.5567722320556641
train gradient:  0.2021730031953868
iteration : 2346
train acc:  0.671875
train loss:  0.5303071737289429
train gradient:  0.1712235479293626
iteration : 2347
train acc:  0.7421875
train loss:  0.5653613209724426
train gradient:  0.16530369680109508
iteration : 2348
train acc:  0.765625
train loss:  0.48354125022888184
train gradient:  0.138577044142241
iteration : 2349
train acc:  0.6640625
train loss:  0.5766056776046753
train gradient:  0.19218143648490033
iteration : 2350
train acc:  0.7109375
train loss:  0.5188034176826477
train gradient:  0.17260558561749015
iteration : 2351
train acc:  0.75
train loss:  0.499076783657074
train gradient:  0.168629159547688
iteration : 2352
train acc:  0.71875
train loss:  0.5497490167617798
train gradient:  0.16183559185821653
iteration : 2353
train acc:  0.703125
train loss:  0.5373543500900269
train gradient:  0.1878222542333016
iteration : 2354
train acc:  0.7890625
train loss:  0.5161353945732117
train gradient:  0.17419854056498374
iteration : 2355
train acc:  0.6796875
train loss:  0.5618240833282471
train gradient:  0.19008184289371122
iteration : 2356
train acc:  0.8125
train loss:  0.447136789560318
train gradient:  0.1647660404887601
iteration : 2357
train acc:  0.734375
train loss:  0.5231221914291382
train gradient:  0.19487151037727404
iteration : 2358
train acc:  0.796875
train loss:  0.475337952375412
train gradient:  0.18205074527901358
iteration : 2359
train acc:  0.78125
train loss:  0.48769843578338623
train gradient:  0.12917676785880156
iteration : 2360
train acc:  0.7265625
train loss:  0.5068782567977905
train gradient:  0.14886276983906832
iteration : 2361
train acc:  0.7109375
train loss:  0.5487310886383057
train gradient:  0.18279123814376091
iteration : 2362
train acc:  0.6640625
train loss:  0.5988247394561768
train gradient:  0.2520017438022475
iteration : 2363
train acc:  0.765625
train loss:  0.48294588923454285
train gradient:  0.1448024787131163
iteration : 2364
train acc:  0.671875
train loss:  0.5504915714263916
train gradient:  0.17355008572307967
iteration : 2365
train acc:  0.703125
train loss:  0.5642545223236084
train gradient:  0.1880438670971506
iteration : 2366
train acc:  0.6484375
train loss:  0.5880342721939087
train gradient:  0.2026022444304192
iteration : 2367
train acc:  0.734375
train loss:  0.5325783491134644
train gradient:  0.1842308145948019
iteration : 2368
train acc:  0.6953125
train loss:  0.5827036499977112
train gradient:  0.21831371513091935
iteration : 2369
train acc:  0.6640625
train loss:  0.5724959969520569
train gradient:  0.27702420649519766
iteration : 2370
train acc:  0.7265625
train loss:  0.5201607942581177
train gradient:  0.16803651047944884
iteration : 2371
train acc:  0.6953125
train loss:  0.6267622709274292
train gradient:  0.2564293432080974
iteration : 2372
train acc:  0.703125
train loss:  0.5662121772766113
train gradient:  0.19136812920761473
iteration : 2373
train acc:  0.6640625
train loss:  0.5909247994422913
train gradient:  0.17055757827512202
iteration : 2374
train acc:  0.703125
train loss:  0.5237599015235901
train gradient:  0.15286126654213866
iteration : 2375
train acc:  0.6796875
train loss:  0.6080756187438965
train gradient:  0.21294786266883017
iteration : 2376
train acc:  0.734375
train loss:  0.5153904557228088
train gradient:  0.1550294733894858
iteration : 2377
train acc:  0.6875
train loss:  0.6520054340362549
train gradient:  0.32076796061136753
iteration : 2378
train acc:  0.734375
train loss:  0.5256040096282959
train gradient:  0.1673361794891042
iteration : 2379
train acc:  0.6796875
train loss:  0.4883881211280823
train gradient:  0.1474252723136436
iteration : 2380
train acc:  0.7734375
train loss:  0.5181653499603271
train gradient:  0.12920748894937237
iteration : 2381
train acc:  0.765625
train loss:  0.509926438331604
train gradient:  0.16309482187750232
iteration : 2382
train acc:  0.6328125
train loss:  0.6032554507255554
train gradient:  0.15801334228663128
iteration : 2383
train acc:  0.640625
train loss:  0.573636531829834
train gradient:  0.20109652948394613
iteration : 2384
train acc:  0.7421875
train loss:  0.49505576491355896
train gradient:  0.14166397851079085
iteration : 2385
train acc:  0.75
train loss:  0.5327390432357788
train gradient:  0.13507004330771877
iteration : 2386
train acc:  0.6484375
train loss:  0.6046748161315918
train gradient:  0.19207311569224228
iteration : 2387
train acc:  0.7265625
train loss:  0.5107712745666504
train gradient:  0.1516675995799393
iteration : 2388
train acc:  0.7421875
train loss:  0.4876682162284851
train gradient:  0.12006552895695047
iteration : 2389
train acc:  0.7109375
train loss:  0.5261415243148804
train gradient:  0.14757097051470494
iteration : 2390
train acc:  0.7578125
train loss:  0.5029231309890747
train gradient:  0.14194254266627304
iteration : 2391
train acc:  0.703125
train loss:  0.5036908984184265
train gradient:  0.14224336569809307
iteration : 2392
train acc:  0.7109375
train loss:  0.5379073023796082
train gradient:  0.18822513269794539
iteration : 2393
train acc:  0.734375
train loss:  0.5325260758399963
train gradient:  0.16298023534405287
iteration : 2394
train acc:  0.765625
train loss:  0.46735021471977234
train gradient:  0.1135173859223032
iteration : 2395
train acc:  0.6953125
train loss:  0.564005970954895
train gradient:  0.15144098754198265
iteration : 2396
train acc:  0.671875
train loss:  0.557948648929596
train gradient:  0.1732159714980346
iteration : 2397
train acc:  0.65625
train loss:  0.5378892421722412
train gradient:  0.1364586396091026
iteration : 2398
train acc:  0.7578125
train loss:  0.510668933391571
train gradient:  0.11997218277319407
iteration : 2399
train acc:  0.6875
train loss:  0.572399377822876
train gradient:  0.20866776533600281
iteration : 2400
train acc:  0.671875
train loss:  0.5421220660209656
train gradient:  0.21803354487593452
iteration : 2401
train acc:  0.734375
train loss:  0.5366207361221313
train gradient:  0.1513493405120413
iteration : 2402
train acc:  0.71875
train loss:  0.5437183976173401
train gradient:  0.16197860831123717
iteration : 2403
train acc:  0.640625
train loss:  0.5797542929649353
train gradient:  0.18030065125778333
iteration : 2404
train acc:  0.7265625
train loss:  0.5344202518463135
train gradient:  0.13127829775919425
iteration : 2405
train acc:  0.6796875
train loss:  0.5353732705116272
train gradient:  0.1528785572435015
iteration : 2406
train acc:  0.6953125
train loss:  0.5799214839935303
train gradient:  0.21159895062285905
iteration : 2407
train acc:  0.75
train loss:  0.5113809108734131
train gradient:  0.14562311242905573
iteration : 2408
train acc:  0.7421875
train loss:  0.5044280290603638
train gradient:  0.14570889298749212
iteration : 2409
train acc:  0.6328125
train loss:  0.5659438967704773
train gradient:  0.19848713343080093
iteration : 2410
train acc:  0.6796875
train loss:  0.6075193881988525
train gradient:  0.2004394375371627
iteration : 2411
train acc:  0.6640625
train loss:  0.5939111709594727
train gradient:  0.2054186167198962
iteration : 2412
train acc:  0.7421875
train loss:  0.6108654737472534
train gradient:  0.19443143482964775
iteration : 2413
train acc:  0.6953125
train loss:  0.5438920855522156
train gradient:  0.17128962219991797
iteration : 2414
train acc:  0.703125
train loss:  0.5476059913635254
train gradient:  0.1675246126278831
iteration : 2415
train acc:  0.7421875
train loss:  0.5304636359214783
train gradient:  0.13477950904777192
iteration : 2416
train acc:  0.7421875
train loss:  0.4954351782798767
train gradient:  0.12944610313425692
iteration : 2417
train acc:  0.6171875
train loss:  0.631862223148346
train gradient:  0.20826676798804958
iteration : 2418
train acc:  0.78125
train loss:  0.49822014570236206
train gradient:  0.17479607631869837
iteration : 2419
train acc:  0.671875
train loss:  0.57176673412323
train gradient:  0.2116756645340926
iteration : 2420
train acc:  0.75
train loss:  0.49828800559043884
train gradient:  0.10513734536296135
iteration : 2421
train acc:  0.7421875
train loss:  0.500472903251648
train gradient:  0.15023650013608172
iteration : 2422
train acc:  0.7421875
train loss:  0.5191217660903931
train gradient:  0.15004332776190527
iteration : 2423
train acc:  0.734375
train loss:  0.5309129953384399
train gradient:  0.16736849485164002
iteration : 2424
train acc:  0.7421875
train loss:  0.5158687233924866
train gradient:  0.12417156232175582
iteration : 2425
train acc:  0.7578125
train loss:  0.510261058807373
train gradient:  0.1864671695691527
iteration : 2426
train acc:  0.703125
train loss:  0.5383195877075195
train gradient:  0.1533078365946045
iteration : 2427
train acc:  0.765625
train loss:  0.5063523054122925
train gradient:  0.15723751379907455
iteration : 2428
train acc:  0.7421875
train loss:  0.5572323799133301
train gradient:  0.13416602521624937
iteration : 2429
train acc:  0.7265625
train loss:  0.5164528489112854
train gradient:  0.1377631307622812
iteration : 2430
train acc:  0.7734375
train loss:  0.5147380232810974
train gradient:  0.14775550981124455
iteration : 2431
train acc:  0.7421875
train loss:  0.5452923774719238
train gradient:  0.16377391449130219
iteration : 2432
train acc:  0.734375
train loss:  0.4669594168663025
train gradient:  0.14533488791906768
iteration : 2433
train acc:  0.65625
train loss:  0.6193063855171204
train gradient:  0.16282118831181194
iteration : 2434
train acc:  0.6953125
train loss:  0.5858441591262817
train gradient:  0.2559782312766382
iteration : 2435
train acc:  0.7578125
train loss:  0.526643693447113
train gradient:  0.17074120391839365
iteration : 2436
train acc:  0.6875
train loss:  0.6144660711288452
train gradient:  0.22077711493141844
iteration : 2437
train acc:  0.7109375
train loss:  0.5607728362083435
train gradient:  0.1365117750460949
iteration : 2438
train acc:  0.734375
train loss:  0.5239169001579285
train gradient:  0.19299750148760092
iteration : 2439
train acc:  0.703125
train loss:  0.5486931204795837
train gradient:  0.1532195731245164
iteration : 2440
train acc:  0.71875
train loss:  0.49709242582321167
train gradient:  0.13534695725241475
iteration : 2441
train acc:  0.7265625
train loss:  0.5616919994354248
train gradient:  0.16551063507850744
iteration : 2442
train acc:  0.6328125
train loss:  0.5833933353424072
train gradient:  0.21508403358746755
iteration : 2443
train acc:  0.7421875
train loss:  0.5027514696121216
train gradient:  0.15262293708188085
iteration : 2444
train acc:  0.671875
train loss:  0.5709616541862488
train gradient:  0.18880610882333765
iteration : 2445
train acc:  0.6875
train loss:  0.5744284391403198
train gradient:  0.20378052211137004
iteration : 2446
train acc:  0.7109375
train loss:  0.5851250886917114
train gradient:  0.23212959077487122
iteration : 2447
train acc:  0.7265625
train loss:  0.6072492599487305
train gradient:  0.2212088706571598
iteration : 2448
train acc:  0.6953125
train loss:  0.5473802089691162
train gradient:  0.169762613338748
iteration : 2449
train acc:  0.765625
train loss:  0.4938755929470062
train gradient:  0.1348156505756579
iteration : 2450
train acc:  0.703125
train loss:  0.5874114632606506
train gradient:  0.1935093441334187
iteration : 2451
train acc:  0.6953125
train loss:  0.5760557055473328
train gradient:  0.18354556181038245
iteration : 2452
train acc:  0.7265625
train loss:  0.5460063815116882
train gradient:  0.1454320346073821
iteration : 2453
train acc:  0.6640625
train loss:  0.5546622276306152
train gradient:  0.1242837308338246
iteration : 2454
train acc:  0.671875
train loss:  0.5640578269958496
train gradient:  0.20302416007137017
iteration : 2455
train acc:  0.7421875
train loss:  0.5310763716697693
train gradient:  0.15509185505055478
iteration : 2456
train acc:  0.8046875
train loss:  0.45950859785079956
train gradient:  0.13899379770476775
iteration : 2457
train acc:  0.734375
train loss:  0.5674039721488953
train gradient:  0.22849655060110952
iteration : 2458
train acc:  0.625
train loss:  0.6412302255630493
train gradient:  0.20402290535489054
iteration : 2459
train acc:  0.703125
train loss:  0.5545738935470581
train gradient:  0.1736397200585116
iteration : 2460
train acc:  0.65625
train loss:  0.6018576622009277
train gradient:  0.16586218807438674
iteration : 2461
train acc:  0.6484375
train loss:  0.6074185371398926
train gradient:  0.19940071113168284
iteration : 2462
train acc:  0.6796875
train loss:  0.5530627965927124
train gradient:  0.15001961964084579
iteration : 2463
train acc:  0.71875
train loss:  0.5600014925003052
train gradient:  0.17353418382944602
iteration : 2464
train acc:  0.7109375
train loss:  0.5168200731277466
train gradient:  0.16053016277144463
iteration : 2465
train acc:  0.6875
train loss:  0.5763247013092041
train gradient:  0.21935801437126512
iteration : 2466
train acc:  0.78125
train loss:  0.49795466661453247
train gradient:  0.15622839621281004
iteration : 2467
train acc:  0.75
train loss:  0.5192760229110718
train gradient:  0.16442364386160507
iteration : 2468
train acc:  0.65625
train loss:  0.5981539487838745
train gradient:  0.1982571603640938
iteration : 2469
train acc:  0.671875
train loss:  0.5552551746368408
train gradient:  0.1679623855014135
iteration : 2470
train acc:  0.6953125
train loss:  0.5644222497940063
train gradient:  0.15871485918355105
iteration : 2471
train acc:  0.7421875
train loss:  0.5069687366485596
train gradient:  0.1770736042074053
iteration : 2472
train acc:  0.6875
train loss:  0.5872118473052979
train gradient:  0.15689032473849696
iteration : 2473
train acc:  0.75
train loss:  0.4956831932067871
train gradient:  0.14284221539379977
iteration : 2474
train acc:  0.609375
train loss:  0.6684504151344299
train gradient:  0.22661208634238972
iteration : 2475
train acc:  0.6484375
train loss:  0.6791398525238037
train gradient:  0.3072888963215672
iteration : 2476
train acc:  0.671875
train loss:  0.5923354625701904
train gradient:  0.1725833279920876
iteration : 2477
train acc:  0.7265625
train loss:  0.5341622233390808
train gradient:  0.20375864167323843
iteration : 2478
train acc:  0.71875
train loss:  0.5003476142883301
train gradient:  0.11877971839263239
iteration : 2479
train acc:  0.6171875
train loss:  0.6335524320602417
train gradient:  0.18757670413114802
iteration : 2480
train acc:  0.7578125
train loss:  0.48007893562316895
train gradient:  0.12432592280400245
iteration : 2481
train acc:  0.7265625
train loss:  0.543756365776062
train gradient:  0.1957300127105528
iteration : 2482
train acc:  0.6640625
train loss:  0.568217396736145
train gradient:  0.17941764730286844
iteration : 2483
train acc:  0.734375
train loss:  0.521502673625946
train gradient:  0.19434025787995024
iteration : 2484
train acc:  0.7421875
train loss:  0.5144715309143066
train gradient:  0.14936913267914942
iteration : 2485
train acc:  0.703125
train loss:  0.5403119325637817
train gradient:  0.18125936589373326
iteration : 2486
train acc:  0.6875
train loss:  0.5375380516052246
train gradient:  0.13024527143347608
iteration : 2487
train acc:  0.671875
train loss:  0.5558795928955078
train gradient:  0.17067931031244193
iteration : 2488
train acc:  0.6484375
train loss:  0.5744886994361877
train gradient:  0.16453171738205968
iteration : 2489
train acc:  0.6640625
train loss:  0.6090185642242432
train gradient:  0.2390189761304272
iteration : 2490
train acc:  0.7265625
train loss:  0.5341707468032837
train gradient:  0.21960392049760594
iteration : 2491
train acc:  0.703125
train loss:  0.5265029668807983
train gradient:  0.13098952284713639
iteration : 2492
train acc:  0.71875
train loss:  0.519742488861084
train gradient:  0.17436506219666398
iteration : 2493
train acc:  0.71875
train loss:  0.5517829656600952
train gradient:  0.17859620209262217
iteration : 2494
train acc:  0.734375
train loss:  0.5029627084732056
train gradient:  0.14433577009059076
iteration : 2495
train acc:  0.7421875
train loss:  0.5502676367759705
train gradient:  0.13834825153447333
iteration : 2496
train acc:  0.78125
train loss:  0.4796062111854553
train gradient:  0.13431673046356907
iteration : 2497
train acc:  0.7109375
train loss:  0.5537392497062683
train gradient:  0.19947962924760174
iteration : 2498
train acc:  0.7109375
train loss:  0.5413019061088562
train gradient:  0.13954782633573476
iteration : 2499
train acc:  0.6875
train loss:  0.590490460395813
train gradient:  0.16890654233558222
iteration : 2500
train acc:  0.71875
train loss:  0.545369029045105
train gradient:  0.184070786969906
iteration : 2501
train acc:  0.71875
train loss:  0.5464884042739868
train gradient:  0.14340394208650406
iteration : 2502
train acc:  0.7265625
train loss:  0.5170235633850098
train gradient:  0.17156790410926326
iteration : 2503
train acc:  0.7421875
train loss:  0.49789825081825256
train gradient:  0.14707838025457864
iteration : 2504
train acc:  0.7265625
train loss:  0.5401672720909119
train gradient:  0.1729752351594172
iteration : 2505
train acc:  0.6875
train loss:  0.5597926378250122
train gradient:  0.1808131035036661
iteration : 2506
train acc:  0.75
train loss:  0.4967447519302368
train gradient:  0.15029709953182369
iteration : 2507
train acc:  0.7109375
train loss:  0.5299299955368042
train gradient:  0.18042749655712
iteration : 2508
train acc:  0.765625
train loss:  0.5014069080352783
train gradient:  0.19115660191685413
iteration : 2509
train acc:  0.671875
train loss:  0.5551942586898804
train gradient:  0.23386930764110914
iteration : 2510
train acc:  0.765625
train loss:  0.4782761037349701
train gradient:  0.14003615300920932
iteration : 2511
train acc:  0.65625
train loss:  0.5622854232788086
train gradient:  0.20281789597779187
iteration : 2512
train acc:  0.671875
train loss:  0.5902916193008423
train gradient:  0.21689601691792249
iteration : 2513
train acc:  0.765625
train loss:  0.4954429268836975
train gradient:  0.1266352844976099
iteration : 2514
train acc:  0.796875
train loss:  0.4794048070907593
train gradient:  0.16050717636736186
iteration : 2515
train acc:  0.71875
train loss:  0.521660566329956
train gradient:  0.12927121089879637
iteration : 2516
train acc:  0.734375
train loss:  0.5183122158050537
train gradient:  0.14513446314463171
iteration : 2517
train acc:  0.765625
train loss:  0.4911434054374695
train gradient:  0.14568880970518927
iteration : 2518
train acc:  0.6796875
train loss:  0.5441179275512695
train gradient:  0.20174089558178593
iteration : 2519
train acc:  0.6484375
train loss:  0.5765162706375122
train gradient:  0.24287629926244614
iteration : 2520
train acc:  0.6953125
train loss:  0.5772557258605957
train gradient:  0.19582894322063438
iteration : 2521
train acc:  0.7109375
train loss:  0.5503789782524109
train gradient:  0.1864909862291559
iteration : 2522
train acc:  0.703125
train loss:  0.5462446808815002
train gradient:  0.18200071412530477
iteration : 2523
train acc:  0.6953125
train loss:  0.5463230609893799
train gradient:  0.1503375630819604
iteration : 2524
train acc:  0.6875
train loss:  0.556302547454834
train gradient:  0.17973221267174339
iteration : 2525
train acc:  0.6875
train loss:  0.5607031583786011
train gradient:  0.1693763684247828
iteration : 2526
train acc:  0.734375
train loss:  0.49994781613349915
train gradient:  0.14407109481422384
iteration : 2527
train acc:  0.6953125
train loss:  0.5530238151550293
train gradient:  0.1456309620242459
iteration : 2528
train acc:  0.7265625
train loss:  0.560728132724762
train gradient:  0.18068746834788585
iteration : 2529
train acc:  0.6953125
train loss:  0.6044096946716309
train gradient:  0.21032215554747807
iteration : 2530
train acc:  0.6796875
train loss:  0.5182497501373291
train gradient:  0.13393170172720345
iteration : 2531
train acc:  0.7734375
train loss:  0.48453035950660706
train gradient:  0.267371730296305
iteration : 2532
train acc:  0.7265625
train loss:  0.5572937726974487
train gradient:  0.2035194902711976
iteration : 2533
train acc:  0.71875
train loss:  0.5080628395080566
train gradient:  0.15219007974206789
iteration : 2534
train acc:  0.7421875
train loss:  0.5030207633972168
train gradient:  0.1366696751483627
iteration : 2535
train acc:  0.6875
train loss:  0.5179110169410706
train gradient:  0.18488296645525037
iteration : 2536
train acc:  0.703125
train loss:  0.546197235584259
train gradient:  0.19536171207030467
iteration : 2537
train acc:  0.7421875
train loss:  0.5402989983558655
train gradient:  0.1546739489797004
iteration : 2538
train acc:  0.6640625
train loss:  0.6107481718063354
train gradient:  0.21776655531447786
iteration : 2539
train acc:  0.78125
train loss:  0.4772818088531494
train gradient:  0.10899249713407155
iteration : 2540
train acc:  0.7109375
train loss:  0.5506230592727661
train gradient:  0.13671368433313194
iteration : 2541
train acc:  0.7734375
train loss:  0.44959962368011475
train gradient:  0.14644844883332453
iteration : 2542
train acc:  0.7578125
train loss:  0.4761989712715149
train gradient:  0.13854458036636935
iteration : 2543
train acc:  0.7421875
train loss:  0.5122405290603638
train gradient:  0.16205141140426932
iteration : 2544
train acc:  0.6953125
train loss:  0.574073314666748
train gradient:  0.15699624683500163
iteration : 2545
train acc:  0.7265625
train loss:  0.5484607219696045
train gradient:  0.16114404692494128
iteration : 2546
train acc:  0.6875
train loss:  0.6033850908279419
train gradient:  0.18302542919063694
iteration : 2547
train acc:  0.7734375
train loss:  0.5001078248023987
train gradient:  0.14599944163360906
iteration : 2548
train acc:  0.75
train loss:  0.556736409664154
train gradient:  0.18677863032501116
iteration : 2549
train acc:  0.71875
train loss:  0.5493497848510742
train gradient:  0.18164702027384783
iteration : 2550
train acc:  0.6640625
train loss:  0.5708123445510864
train gradient:  0.1888445484443041
iteration : 2551
train acc:  0.6796875
train loss:  0.6146557331085205
train gradient:  0.20544113979855078
iteration : 2552
train acc:  0.703125
train loss:  0.5390822291374207
train gradient:  0.18273138543542605
iteration : 2553
train acc:  0.7421875
train loss:  0.5202211141586304
train gradient:  0.1954666089728499
iteration : 2554
train acc:  0.71875
train loss:  0.5348683595657349
train gradient:  0.1437306432721353
iteration : 2555
train acc:  0.6953125
train loss:  0.5249059200286865
train gradient:  0.17921587597939842
iteration : 2556
train acc:  0.75
train loss:  0.5121641755104065
train gradient:  0.15049662094983876
iteration : 2557
train acc:  0.6796875
train loss:  0.5500679612159729
train gradient:  0.17706825671642662
iteration : 2558
train acc:  0.65625
train loss:  0.5452617406845093
train gradient:  0.17575253012090436
iteration : 2559
train acc:  0.7578125
train loss:  0.5299067497253418
train gradient:  0.1617241685184902
iteration : 2560
train acc:  0.8046875
train loss:  0.4587770998477936
train gradient:  0.13603310097585514
iteration : 2561
train acc:  0.6875
train loss:  0.5130360126495361
train gradient:  0.1543034961892482
iteration : 2562
train acc:  0.703125
train loss:  0.5519019365310669
train gradient:  0.19525978017843354
iteration : 2563
train acc:  0.75
train loss:  0.47536882758140564
train gradient:  0.16353041822363112
iteration : 2564
train acc:  0.6796875
train loss:  0.6188921332359314
train gradient:  0.17310439221666435
iteration : 2565
train acc:  0.6796875
train loss:  0.5701066255569458
train gradient:  0.1907049884043242
iteration : 2566
train acc:  0.7109375
train loss:  0.510496973991394
train gradient:  0.22113234137297172
iteration : 2567
train acc:  0.7265625
train loss:  0.4555986821651459
train gradient:  0.10485205635316959
iteration : 2568
train acc:  0.796875
train loss:  0.4544665217399597
train gradient:  0.12643499564427035
iteration : 2569
train acc:  0.703125
train loss:  0.562567949295044
train gradient:  0.22050101732365046
iteration : 2570
train acc:  0.7421875
train loss:  0.49584120512008667
train gradient:  0.15127053763271023
iteration : 2571
train acc:  0.7265625
train loss:  0.5227400064468384
train gradient:  0.20864053374482894
iteration : 2572
train acc:  0.734375
train loss:  0.5421079397201538
train gradient:  0.18685249155897476
iteration : 2573
train acc:  0.7265625
train loss:  0.48039156198501587
train gradient:  0.1611821136284915
iteration : 2574
train acc:  0.7421875
train loss:  0.5368098020553589
train gradient:  0.1999190079642853
iteration : 2575
train acc:  0.7109375
train loss:  0.4908794164657593
train gradient:  0.14765386120113877
iteration : 2576
train acc:  0.6796875
train loss:  0.5717248916625977
train gradient:  0.20705107711561674
iteration : 2577
train acc:  0.6953125
train loss:  0.5432875752449036
train gradient:  0.1570286070530123
iteration : 2578
train acc:  0.625
train loss:  0.6404489278793335
train gradient:  0.19241177998580092
iteration : 2579
train acc:  0.65625
train loss:  0.611030101776123
train gradient:  0.2410989160157798
iteration : 2580
train acc:  0.671875
train loss:  0.5805706977844238
train gradient:  0.19329592103130225
iteration : 2581
train acc:  0.7421875
train loss:  0.5134299993515015
train gradient:  0.17023487699198278
iteration : 2582
train acc:  0.6484375
train loss:  0.5631282329559326
train gradient:  0.1596587825548338
iteration : 2583
train acc:  0.75
train loss:  0.46398597955703735
train gradient:  0.11705802794284251
iteration : 2584
train acc:  0.765625
train loss:  0.49058961868286133
train gradient:  0.14343771075251474
iteration : 2585
train acc:  0.640625
train loss:  0.587334156036377
train gradient:  0.18445442833124626
iteration : 2586
train acc:  0.7734375
train loss:  0.4810180366039276
train gradient:  0.17902675863218123
iteration : 2587
train acc:  0.7734375
train loss:  0.5374624729156494
train gradient:  0.19426624437030804
iteration : 2588
train acc:  0.7109375
train loss:  0.5457326173782349
train gradient:  0.15762060861719518
iteration : 2589
train acc:  0.6640625
train loss:  0.5830833315849304
train gradient:  0.20012129057280242
iteration : 2590
train acc:  0.6796875
train loss:  0.5454054474830627
train gradient:  0.18106668714978263
iteration : 2591
train acc:  0.6953125
train loss:  0.5460781455039978
train gradient:  0.1709830737859492
iteration : 2592
train acc:  0.734375
train loss:  0.5145174860954285
train gradient:  0.12485775886230356
iteration : 2593
train acc:  0.65625
train loss:  0.6330527067184448
train gradient:  0.22431085472936624
iteration : 2594
train acc:  0.6328125
train loss:  0.5838432908058167
train gradient:  0.2036135417352429
iteration : 2595
train acc:  0.6953125
train loss:  0.5375815033912659
train gradient:  0.193467363543063
iteration : 2596
train acc:  0.71875
train loss:  0.5053180456161499
train gradient:  0.15905663982151058
iteration : 2597
train acc:  0.7109375
train loss:  0.5410235524177551
train gradient:  0.1500985296587255
iteration : 2598
train acc:  0.8046875
train loss:  0.47806093096733093
train gradient:  0.13350854840191934
iteration : 2599
train acc:  0.71875
train loss:  0.544763445854187
train gradient:  0.17572947259993862
iteration : 2600
train acc:  0.75
train loss:  0.45018696784973145
train gradient:  0.13680028454680865
iteration : 2601
train acc:  0.671875
train loss:  0.5713590383529663
train gradient:  0.20254335435645449
iteration : 2602
train acc:  0.75
train loss:  0.5360488295555115
train gradient:  0.18625218554249118
iteration : 2603
train acc:  0.6875
train loss:  0.5907714366912842
train gradient:  0.17144823763993083
iteration : 2604
train acc:  0.6953125
train loss:  0.5551164746284485
train gradient:  0.1781675158806527
iteration : 2605
train acc:  0.7578125
train loss:  0.507093071937561
train gradient:  0.14982274852077287
iteration : 2606
train acc:  0.71875
train loss:  0.524682879447937
train gradient:  0.25580390989127877
iteration : 2607
train acc:  0.7578125
train loss:  0.4962354898452759
train gradient:  0.1788684542516909
iteration : 2608
train acc:  0.765625
train loss:  0.4942483901977539
train gradient:  0.14469994425376304
iteration : 2609
train acc:  0.7265625
train loss:  0.5192545652389526
train gradient:  0.16852522958105942
iteration : 2610
train acc:  0.7421875
train loss:  0.5166171789169312
train gradient:  0.15449223436105927
iteration : 2611
train acc:  0.7109375
train loss:  0.5067349672317505
train gradient:  0.20537089343175635
iteration : 2612
train acc:  0.7578125
train loss:  0.5427981019020081
train gradient:  0.2215280334707772
iteration : 2613
train acc:  0.6875
train loss:  0.5153511166572571
train gradient:  0.1866183658504207
iteration : 2614
train acc:  0.7265625
train loss:  0.5377877950668335
train gradient:  0.21135687386669574
iteration : 2615
train acc:  0.6640625
train loss:  0.5319860577583313
train gradient:  0.20870890456729374
iteration : 2616
train acc:  0.734375
train loss:  0.48035115003585815
train gradient:  0.1661258633727405
iteration : 2617
train acc:  0.75
train loss:  0.5159406661987305
train gradient:  0.15158482627201397
iteration : 2618
train acc:  0.7421875
train loss:  0.5247238874435425
train gradient:  0.17561768552684415
iteration : 2619
train acc:  0.6953125
train loss:  0.548337996006012
train gradient:  0.1681182260158058
iteration : 2620
train acc:  0.6953125
train loss:  0.580440104007721
train gradient:  0.19798560497499912
iteration : 2621
train acc:  0.75
train loss:  0.5211788415908813
train gradient:  0.1958511637306274
iteration : 2622
train acc:  0.7109375
train loss:  0.5742577314376831
train gradient:  0.19340209721044
iteration : 2623
train acc:  0.7109375
train loss:  0.5336277484893799
train gradient:  0.16029487597079284
iteration : 2624
train acc:  0.71875
train loss:  0.52354896068573
train gradient:  0.1392868345651122
iteration : 2625
train acc:  0.6796875
train loss:  0.5871723294258118
train gradient:  0.16597372140317412
iteration : 2626
train acc:  0.6640625
train loss:  0.5782833099365234
train gradient:  0.1722526660447412
iteration : 2627
train acc:  0.7109375
train loss:  0.5260663628578186
train gradient:  0.1888434686226011
iteration : 2628
train acc:  0.6953125
train loss:  0.5318752527236938
train gradient:  0.14068681164469843
iteration : 2629
train acc:  0.6171875
train loss:  0.5975438356399536
train gradient:  0.15628115005726545
iteration : 2630
train acc:  0.7578125
train loss:  0.5264867544174194
train gradient:  0.1327757280706312
iteration : 2631
train acc:  0.671875
train loss:  0.5604673027992249
train gradient:  0.1974605850990285
iteration : 2632
train acc:  0.7109375
train loss:  0.5129843950271606
train gradient:  0.16396775714553483
iteration : 2633
train acc:  0.7421875
train loss:  0.5314745903015137
train gradient:  0.18014315620914367
iteration : 2634
train acc:  0.7578125
train loss:  0.512433648109436
train gradient:  0.15181634025609347
iteration : 2635
train acc:  0.65625
train loss:  0.5896499156951904
train gradient:  0.25642540487313686
iteration : 2636
train acc:  0.7109375
train loss:  0.5416108965873718
train gradient:  0.17415158388978894
iteration : 2637
train acc:  0.7109375
train loss:  0.5654489994049072
train gradient:  0.23438002889002446
iteration : 2638
train acc:  0.671875
train loss:  0.5535873174667358
train gradient:  0.19018899940282644
iteration : 2639
train acc:  0.71875
train loss:  0.5351440906524658
train gradient:  0.18239649162404495
iteration : 2640
train acc:  0.6875
train loss:  0.5666717290878296
train gradient:  0.19961019654697748
iteration : 2641
train acc:  0.6171875
train loss:  0.668373703956604
train gradient:  0.22409962661194738
iteration : 2642
train acc:  0.65625
train loss:  0.6225616931915283
train gradient:  0.2044459397320777
iteration : 2643
train acc:  0.7890625
train loss:  0.508450984954834
train gradient:  0.17709014890371305
iteration : 2644
train acc:  0.7421875
train loss:  0.4904489815235138
train gradient:  0.13979621593320896
iteration : 2645
train acc:  0.7421875
train loss:  0.5366318821907043
train gradient:  0.15983542118364422
iteration : 2646
train acc:  0.8046875
train loss:  0.4922882318496704
train gradient:  0.1488074336947121
iteration : 2647
train acc:  0.65625
train loss:  0.5684969425201416
train gradient:  0.17100566366253991
iteration : 2648
train acc:  0.6953125
train loss:  0.5586502552032471
train gradient:  0.1598980918711733
iteration : 2649
train acc:  0.75
train loss:  0.5005131959915161
train gradient:  0.16409032096435827
iteration : 2650
train acc:  0.7109375
train loss:  0.5240611433982849
train gradient:  0.2032792750611712
iteration : 2651
train acc:  0.7265625
train loss:  0.5276085138320923
train gradient:  0.18333105144241146
iteration : 2652
train acc:  0.7109375
train loss:  0.5118456482887268
train gradient:  0.1840207679627186
iteration : 2653
train acc:  0.71875
train loss:  0.5240565538406372
train gradient:  0.13157224473632842
iteration : 2654
train acc:  0.7421875
train loss:  0.516399085521698
train gradient:  0.18705382334559206
iteration : 2655
train acc:  0.6875
train loss:  0.5890946388244629
train gradient:  0.18268679124763337
iteration : 2656
train acc:  0.6640625
train loss:  0.5989594459533691
train gradient:  0.2062515112423859
iteration : 2657
train acc:  0.7421875
train loss:  0.512358546257019
train gradient:  0.1510073692818541
iteration : 2658
train acc:  0.6640625
train loss:  0.5821046233177185
train gradient:  0.1888156659777573
iteration : 2659
train acc:  0.6953125
train loss:  0.5003414750099182
train gradient:  0.201640527626967
iteration : 2660
train acc:  0.7109375
train loss:  0.523746907711029
train gradient:  0.1830232560948791
iteration : 2661
train acc:  0.7109375
train loss:  0.5622957944869995
train gradient:  0.19006166992209228
iteration : 2662
train acc:  0.671875
train loss:  0.5932467579841614
train gradient:  0.17520368385359109
iteration : 2663
train acc:  0.7890625
train loss:  0.4607444405555725
train gradient:  0.1611095229239515
iteration : 2664
train acc:  0.671875
train loss:  0.5308346748352051
train gradient:  0.155550094841048
iteration : 2665
train acc:  0.71875
train loss:  0.5885332822799683
train gradient:  0.23507694726732947
iteration : 2666
train acc:  0.6875
train loss:  0.564032256603241
train gradient:  0.1880720373557935
iteration : 2667
train acc:  0.703125
train loss:  0.5576573014259338
train gradient:  0.21042875237185055
iteration : 2668
train acc:  0.7578125
train loss:  0.541827380657196
train gradient:  0.21222726978587
iteration : 2669
train acc:  0.6953125
train loss:  0.5347424149513245
train gradient:  0.15534087170590713
iteration : 2670
train acc:  0.6640625
train loss:  0.551048994064331
train gradient:  0.21574015855652007
iteration : 2671
train acc:  0.7265625
train loss:  0.5240147113800049
train gradient:  0.13990401453436296
iteration : 2672
train acc:  0.7265625
train loss:  0.5186805725097656
train gradient:  0.17348628345124886
iteration : 2673
train acc:  0.671875
train loss:  0.5524039268493652
train gradient:  0.18097947651039414
iteration : 2674
train acc:  0.6875
train loss:  0.5712467432022095
train gradient:  0.19441567144114968
iteration : 2675
train acc:  0.734375
train loss:  0.5718955993652344
train gradient:  0.2068398133799817
iteration : 2676
train acc:  0.6875
train loss:  0.5545254945755005
train gradient:  0.16350272876988337
iteration : 2677
train acc:  0.71875
train loss:  0.5558204650878906
train gradient:  0.1574692843027827
iteration : 2678
train acc:  0.7734375
train loss:  0.44419652223587036
train gradient:  0.11323924438831626
iteration : 2679
train acc:  0.75
train loss:  0.5095699429512024
train gradient:  0.19459682586858815
iteration : 2680
train acc:  0.640625
train loss:  0.559614360332489
train gradient:  0.22300385854877006
iteration : 2681
train acc:  0.71875
train loss:  0.5446590781211853
train gradient:  0.13662164208809724
iteration : 2682
train acc:  0.7421875
train loss:  0.5308414697647095
train gradient:  0.1995533250473012
iteration : 2683
train acc:  0.65625
train loss:  0.5736993551254272
train gradient:  0.20993213016234447
iteration : 2684
train acc:  0.734375
train loss:  0.5361242294311523
train gradient:  0.21871013042372833
iteration : 2685
train acc:  0.71875
train loss:  0.5204678773880005
train gradient:  0.14412614187121514
iteration : 2686
train acc:  0.6953125
train loss:  0.5661845207214355
train gradient:  0.1777219981031228
iteration : 2687
train acc:  0.7578125
train loss:  0.4899720251560211
train gradient:  0.14624110615598565
iteration : 2688
train acc:  0.78125
train loss:  0.4531175494194031
train gradient:  0.1337763093104121
iteration : 2689
train acc:  0.7421875
train loss:  0.487104594707489
train gradient:  0.12711410638771778
iteration : 2690
train acc:  0.7265625
train loss:  0.528590202331543
train gradient:  0.16728176937124822
iteration : 2691
train acc:  0.7265625
train loss:  0.5274885892868042
train gradient:  0.18656528805761569
iteration : 2692
train acc:  0.6796875
train loss:  0.5351855158805847
train gradient:  0.1442558714049929
iteration : 2693
train acc:  0.7265625
train loss:  0.5397233963012695
train gradient:  0.15011561519196187
iteration : 2694
train acc:  0.703125
train loss:  0.5411425828933716
train gradient:  0.14982324163877198
iteration : 2695
train acc:  0.7265625
train loss:  0.5088551640510559
train gradient:  0.14622930498724257
iteration : 2696
train acc:  0.734375
train loss:  0.5309861898422241
train gradient:  0.13696419444123437
iteration : 2697
train acc:  0.7578125
train loss:  0.4997122585773468
train gradient:  0.15476306554681207
iteration : 2698
train acc:  0.71875
train loss:  0.5384007096290588
train gradient:  0.16980775344669446
iteration : 2699
train acc:  0.7109375
train loss:  0.5201514959335327
train gradient:  0.1634312036477607
iteration : 2700
train acc:  0.609375
train loss:  0.6005898714065552
train gradient:  0.1614985268431354
iteration : 2701
train acc:  0.6953125
train loss:  0.5513061285018921
train gradient:  0.16551894482659155
iteration : 2702
train acc:  0.65625
train loss:  0.5753198266029358
train gradient:  0.17444966479227017
iteration : 2703
train acc:  0.7265625
train loss:  0.5742000341415405
train gradient:  0.1733724398624889
iteration : 2704
train acc:  0.7578125
train loss:  0.4975520670413971
train gradient:  0.18527583260933253
iteration : 2705
train acc:  0.7265625
train loss:  0.5341928601264954
train gradient:  0.14918326753655597
iteration : 2706
train acc:  0.6875
train loss:  0.582159161567688
train gradient:  0.26016754781125057
iteration : 2707
train acc:  0.7109375
train loss:  0.5336878299713135
train gradient:  0.20406146434795522
iteration : 2708
train acc:  0.765625
train loss:  0.5303415656089783
train gradient:  0.2619188185877031
iteration : 2709
train acc:  0.7265625
train loss:  0.5424935817718506
train gradient:  0.1751787929234819
iteration : 2710
train acc:  0.6640625
train loss:  0.5637210607528687
train gradient:  0.17520627354917717
iteration : 2711
train acc:  0.6875
train loss:  0.5344874858856201
train gradient:  0.14434913812371053
iteration : 2712
train acc:  0.71875
train loss:  0.5317473411560059
train gradient:  0.14200733313006864
iteration : 2713
train acc:  0.8125
train loss:  0.40702512860298157
train gradient:  0.11201549036323434
iteration : 2714
train acc:  0.6953125
train loss:  0.5361666679382324
train gradient:  0.14301384405552744
iteration : 2715
train acc:  0.7265625
train loss:  0.5320912599563599
train gradient:  0.13774449761180213
iteration : 2716
train acc:  0.6640625
train loss:  0.5361413359642029
train gradient:  0.1515290122716406
iteration : 2717
train acc:  0.8203125
train loss:  0.43637973070144653
train gradient:  0.11087896495399813
iteration : 2718
train acc:  0.7109375
train loss:  0.5255282521247864
train gradient:  0.15162176602059274
iteration : 2719
train acc:  0.7890625
train loss:  0.4695338308811188
train gradient:  0.1228313842612309
iteration : 2720
train acc:  0.734375
train loss:  0.5358330011367798
train gradient:  0.1821175285264747
iteration : 2721
train acc:  0.78125
train loss:  0.4876522719860077
train gradient:  0.1344154098344115
iteration : 2722
train acc:  0.7265625
train loss:  0.5219271183013916
train gradient:  0.16969624208642087
iteration : 2723
train acc:  0.7265625
train loss:  0.5064488649368286
train gradient:  0.12645348986300395
iteration : 2724
train acc:  0.734375
train loss:  0.578200101852417
train gradient:  0.17577491962197356
iteration : 2725
train acc:  0.7109375
train loss:  0.5418862700462341
train gradient:  0.14759830057915302
iteration : 2726
train acc:  0.7421875
train loss:  0.5263068079948425
train gradient:  0.16342866430928027
iteration : 2727
train acc:  0.765625
train loss:  0.4587973952293396
train gradient:  0.12379505540012156
iteration : 2728
train acc:  0.6953125
train loss:  0.5758734941482544
train gradient:  0.2615574703008203
iteration : 2729
train acc:  0.78125
train loss:  0.47205251455307007
train gradient:  0.13136530230145227
iteration : 2730
train acc:  0.640625
train loss:  0.6189045310020447
train gradient:  0.23704086089752846
iteration : 2731
train acc:  0.7578125
train loss:  0.5062687993049622
train gradient:  0.15937394673867822
iteration : 2732
train acc:  0.7734375
train loss:  0.5246742963790894
train gradient:  0.17214503459231556
iteration : 2733
train acc:  0.75
train loss:  0.5261607766151428
train gradient:  0.1482473968623148
iteration : 2734
train acc:  0.6953125
train loss:  0.5436984896659851
train gradient:  0.18983961444190092
iteration : 2735
train acc:  0.65625
train loss:  0.5851737260818481
train gradient:  0.1844315207603845
iteration : 2736
train acc:  0.703125
train loss:  0.56081223487854
train gradient:  0.17504181305161629
iteration : 2737
train acc:  0.703125
train loss:  0.5377150774002075
train gradient:  0.17506732811671089
iteration : 2738
train acc:  0.6640625
train loss:  0.5615309476852417
train gradient:  0.1665100781721885
iteration : 2739
train acc:  0.71875
train loss:  0.5539636611938477
train gradient:  0.1802376576001304
iteration : 2740
train acc:  0.6875
train loss:  0.5330684185028076
train gradient:  0.16445570778403176
iteration : 2741
train acc:  0.6640625
train loss:  0.5743391513824463
train gradient:  0.1802027043035378
iteration : 2742
train acc:  0.6171875
train loss:  0.6185992956161499
train gradient:  0.2740222554824644
iteration : 2743
train acc:  0.6796875
train loss:  0.5740896463394165
train gradient:  0.18085946493580046
iteration : 2744
train acc:  0.7109375
train loss:  0.531334638595581
train gradient:  0.15111932286071705
iteration : 2745
train acc:  0.6640625
train loss:  0.5520188808441162
train gradient:  0.13787933266986013
iteration : 2746
train acc:  0.765625
train loss:  0.5219913125038147
train gradient:  0.1456004793124824
iteration : 2747
train acc:  0.7734375
train loss:  0.48587656021118164
train gradient:  0.10372380523011669
iteration : 2748
train acc:  0.765625
train loss:  0.4725421071052551
train gradient:  0.15127905772745803
iteration : 2749
train acc:  0.65625
train loss:  0.5663594007492065
train gradient:  0.15988205405012765
iteration : 2750
train acc:  0.6640625
train loss:  0.5935077667236328
train gradient:  0.28924591407038003
iteration : 2751
train acc:  0.640625
train loss:  0.6470814943313599
train gradient:  0.22269962487905576
iteration : 2752
train acc:  0.7578125
train loss:  0.5017844438552856
train gradient:  0.16240897918863395
iteration : 2753
train acc:  0.6171875
train loss:  0.5868122577667236
train gradient:  0.2571070788942171
iteration : 2754
train acc:  0.71875
train loss:  0.5534074306488037
train gradient:  0.19006367181212852
iteration : 2755
train acc:  0.671875
train loss:  0.5508904457092285
train gradient:  0.14739444868572416
iteration : 2756
train acc:  0.734375
train loss:  0.5502980947494507
train gradient:  0.12947299797963197
iteration : 2757
train acc:  0.6875
train loss:  0.6597142815589905
train gradient:  0.259750365256296
iteration : 2758
train acc:  0.6953125
train loss:  0.5502280592918396
train gradient:  0.23030032441417336
iteration : 2759
train acc:  0.71875
train loss:  0.5030338764190674
train gradient:  0.15244775592650045
iteration : 2760
train acc:  0.6171875
train loss:  0.645297110080719
train gradient:  0.25181696280873617
iteration : 2761
train acc:  0.7109375
train loss:  0.5333495140075684
train gradient:  0.16574077203354703
iteration : 2762
train acc:  0.734375
train loss:  0.556648850440979
train gradient:  0.14509782048261521
iteration : 2763
train acc:  0.734375
train loss:  0.5156788229942322
train gradient:  0.1682667914142157
iteration : 2764
train acc:  0.671875
train loss:  0.6086962819099426
train gradient:  0.18795136983388908
iteration : 2765
train acc:  0.65625
train loss:  0.5456556081771851
train gradient:  0.22706679007884373
iteration : 2766
train acc:  0.734375
train loss:  0.541050374507904
train gradient:  0.15973191867702696
iteration : 2767
train acc:  0.7265625
train loss:  0.5096235871315002
train gradient:  0.18338653625499207
iteration : 2768
train acc:  0.7265625
train loss:  0.5452894568443298
train gradient:  0.14824879019134443
iteration : 2769
train acc:  0.703125
train loss:  0.5494128465652466
train gradient:  0.27388767825228655
iteration : 2770
train acc:  0.6875
train loss:  0.5652422904968262
train gradient:  0.16654427921720905
iteration : 2771
train acc:  0.7578125
train loss:  0.49302220344543457
train gradient:  0.16403083661319254
iteration : 2772
train acc:  0.703125
train loss:  0.5704768896102905
train gradient:  0.16100057939083867
iteration : 2773
train acc:  0.7109375
train loss:  0.5376497507095337
train gradient:  0.209994180853864
iteration : 2774
train acc:  0.671875
train loss:  0.6121073961257935
train gradient:  0.17986452518020768
iteration : 2775
train acc:  0.6796875
train loss:  0.5563823580741882
train gradient:  0.1852970942928256
iteration : 2776
train acc:  0.703125
train loss:  0.5356999635696411
train gradient:  0.13461978203113084
iteration : 2777
train acc:  0.6875
train loss:  0.5378062725067139
train gradient:  0.15332489193962562
iteration : 2778
train acc:  0.7421875
train loss:  0.5204811096191406
train gradient:  0.19050210889576377
iteration : 2779
train acc:  0.703125
train loss:  0.5184264779090881
train gradient:  0.1640934345997774
iteration : 2780
train acc:  0.7890625
train loss:  0.4857008159160614
train gradient:  0.1589364331674583
iteration : 2781
train acc:  0.640625
train loss:  0.5982301235198975
train gradient:  0.23427129662966245
iteration : 2782
train acc:  0.7109375
train loss:  0.534589409828186
train gradient:  0.1806314382865266
iteration : 2783
train acc:  0.671875
train loss:  0.5247315168380737
train gradient:  0.15009405651162577
iteration : 2784
train acc:  0.75
train loss:  0.4997975826263428
train gradient:  0.1412338782957353
iteration : 2785
train acc:  0.7265625
train loss:  0.5180273056030273
train gradient:  0.1992623836104921
iteration : 2786
train acc:  0.6328125
train loss:  0.5859788060188293
train gradient:  0.19841006900936464
iteration : 2787
train acc:  0.734375
train loss:  0.5777353644371033
train gradient:  0.15655868710032222
iteration : 2788
train acc:  0.7109375
train loss:  0.5644140243530273
train gradient:  0.16223452732605945
iteration : 2789
train acc:  0.6875
train loss:  0.5457809567451477
train gradient:  0.1804805986638533
iteration : 2790
train acc:  0.71875
train loss:  0.5617668628692627
train gradient:  0.18366773382032608
iteration : 2791
train acc:  0.6484375
train loss:  0.561699390411377
train gradient:  0.1421734237593375
iteration : 2792
train acc:  0.7578125
train loss:  0.477761834859848
train gradient:  0.14736750667557058
iteration : 2793
train acc:  0.734375
train loss:  0.5650675296783447
train gradient:  0.15599713267665005
iteration : 2794
train acc:  0.71875
train loss:  0.568480908870697
train gradient:  0.14034499840862488
iteration : 2795
train acc:  0.78125
train loss:  0.4970965087413788
train gradient:  0.13220421345965164
iteration : 2796
train acc:  0.7421875
train loss:  0.47360438108444214
train gradient:  0.1303026707993546
iteration : 2797
train acc:  0.6875
train loss:  0.5544564723968506
train gradient:  0.17897894612320941
iteration : 2798
train acc:  0.7265625
train loss:  0.49801546335220337
train gradient:  0.1714933464587101
iteration : 2799
train acc:  0.6953125
train loss:  0.5474302768707275
train gradient:  0.1903303806799784
iteration : 2800
train acc:  0.734375
train loss:  0.5053852796554565
train gradient:  0.12745059047949744
iteration : 2801
train acc:  0.6640625
train loss:  0.5723769664764404
train gradient:  0.19188811664095567
iteration : 2802
train acc:  0.7578125
train loss:  0.507767915725708
train gradient:  0.1374578852463038
iteration : 2803
train acc:  0.734375
train loss:  0.5440756678581238
train gradient:  0.17214387341105294
iteration : 2804
train acc:  0.7109375
train loss:  0.6165151000022888
train gradient:  0.34593963531325367
iteration : 2805
train acc:  0.734375
train loss:  0.5127481818199158
train gradient:  0.1401126151473996
iteration : 2806
train acc:  0.7890625
train loss:  0.48651593923568726
train gradient:  0.1309813769845914
iteration : 2807
train acc:  0.7265625
train loss:  0.5538359880447388
train gradient:  0.2189015031947087
iteration : 2808
train acc:  0.734375
train loss:  0.5185675621032715
train gradient:  0.1238710685223842
iteration : 2809
train acc:  0.640625
train loss:  0.6215078234672546
train gradient:  0.23794312074112087
iteration : 2810
train acc:  0.65625
train loss:  0.5802135467529297
train gradient:  0.21472714059640818
iteration : 2811
train acc:  0.7734375
train loss:  0.4914476275444031
train gradient:  0.12659562384853584
iteration : 2812
train acc:  0.765625
train loss:  0.5281562805175781
train gradient:  0.14134954599837604
iteration : 2813
train acc:  0.6953125
train loss:  0.5481076240539551
train gradient:  0.21526177043096736
iteration : 2814
train acc:  0.734375
train loss:  0.5371487140655518
train gradient:  0.13507702106364888
iteration : 2815
train acc:  0.7265625
train loss:  0.5585641860961914
train gradient:  0.2582507226546311
iteration : 2816
train acc:  0.75
train loss:  0.4978024661540985
train gradient:  0.14496266615549766
iteration : 2817
train acc:  0.8359375
train loss:  0.40028250217437744
train gradient:  0.1002073059778666
iteration : 2818
train acc:  0.6484375
train loss:  0.6086704730987549
train gradient:  0.2140448420246294
iteration : 2819
train acc:  0.65625
train loss:  0.5519906282424927
train gradient:  0.15406306366826217
iteration : 2820
train acc:  0.6796875
train loss:  0.6026822328567505
train gradient:  0.2275370983812745
iteration : 2821
train acc:  0.75
train loss:  0.5051816701889038
train gradient:  0.22218514594891892
iteration : 2822
train acc:  0.6796875
train loss:  0.5706306099891663
train gradient:  0.1524607737740376
iteration : 2823
train acc:  0.6953125
train loss:  0.5529747009277344
train gradient:  0.17358991952090558
iteration : 2824
train acc:  0.625
train loss:  0.611911416053772
train gradient:  0.22657614360492473
iteration : 2825
train acc:  0.71875
train loss:  0.5487348437309265
train gradient:  0.23743097698862764
iteration : 2826
train acc:  0.703125
train loss:  0.5729101300239563
train gradient:  0.19100758935587092
iteration : 2827
train acc:  0.734375
train loss:  0.4864727556705475
train gradient:  0.12029027502790836
iteration : 2828
train acc:  0.640625
train loss:  0.5977782607078552
train gradient:  0.20325248786091477
iteration : 2829
train acc:  0.65625
train loss:  0.574061930179596
train gradient:  0.16768525548654634
iteration : 2830
train acc:  0.6171875
train loss:  0.5840355753898621
train gradient:  0.1603876001110958
iteration : 2831
train acc:  0.6875
train loss:  0.5761153697967529
train gradient:  0.16688132157689461
iteration : 2832
train acc:  0.796875
train loss:  0.5189112424850464
train gradient:  0.17938831239721148
iteration : 2833
train acc:  0.71875
train loss:  0.5685895681381226
train gradient:  0.15739533382181087
iteration : 2834
train acc:  0.7109375
train loss:  0.5584874153137207
train gradient:  0.15637990333502985
iteration : 2835
train acc:  0.7421875
train loss:  0.519990086555481
train gradient:  0.14850427381937786
iteration : 2836
train acc:  0.640625
train loss:  0.5851314067840576
train gradient:  0.181146729237119
iteration : 2837
train acc:  0.7421875
train loss:  0.5042041540145874
train gradient:  0.15094768939015396
iteration : 2838
train acc:  0.7109375
train loss:  0.5102121233940125
train gradient:  0.13546890792637983
iteration : 2839
train acc:  0.65625
train loss:  0.5910043120384216
train gradient:  0.168874663973834
iteration : 2840
train acc:  0.6875
train loss:  0.535961389541626
train gradient:  0.19233484876749893
iteration : 2841
train acc:  0.75
train loss:  0.49921226501464844
train gradient:  0.1698524698227943
iteration : 2842
train acc:  0.6875
train loss:  0.5887107849121094
train gradient:  0.1658356922547658
iteration : 2843
train acc:  0.78125
train loss:  0.4789595603942871
train gradient:  0.18329163452513775
iteration : 2844
train acc:  0.796875
train loss:  0.4622013568878174
train gradient:  0.15573001362545796
iteration : 2845
train acc:  0.7578125
train loss:  0.5139986276626587
train gradient:  0.16490729984770036
iteration : 2846
train acc:  0.6796875
train loss:  0.5778883099555969
train gradient:  0.17747890113798637
iteration : 2847
train acc:  0.6796875
train loss:  0.5858017206192017
train gradient:  0.18916277908041784
iteration : 2848
train acc:  0.78125
train loss:  0.47720974683761597
train gradient:  0.15848290507817578
iteration : 2849
train acc:  0.6953125
train loss:  0.5566972494125366
train gradient:  0.15554300570942692
iteration : 2850
train acc:  0.7109375
train loss:  0.5518356561660767
train gradient:  0.1779243866215489
iteration : 2851
train acc:  0.78125
train loss:  0.49325013160705566
train gradient:  0.13517658626988363
iteration : 2852
train acc:  0.6640625
train loss:  0.5702859163284302
train gradient:  0.24142930789684125
iteration : 2853
train acc:  0.6953125
train loss:  0.5462396144866943
train gradient:  0.14621292638255243
iteration : 2854
train acc:  0.7734375
train loss:  0.5180904865264893
train gradient:  0.1411752059788394
iteration : 2855
train acc:  0.6796875
train loss:  0.5318073034286499
train gradient:  0.14275589767373933
iteration : 2856
train acc:  0.65625
train loss:  0.6243504285812378
train gradient:  0.22641871406049713
iteration : 2857
train acc:  0.640625
train loss:  0.5893325805664062
train gradient:  0.2261413509170178
iteration : 2858
train acc:  0.6328125
train loss:  0.5971161127090454
train gradient:  0.2302754753986998
iteration : 2859
train acc:  0.65625
train loss:  0.5865407586097717
train gradient:  0.1647826811642075
iteration : 2860
train acc:  0.7578125
train loss:  0.51929771900177
train gradient:  0.17130593048006182
iteration : 2861
train acc:  0.7109375
train loss:  0.530702531337738
train gradient:  0.14131499016171994
iteration : 2862
train acc:  0.6953125
train loss:  0.5267796516418457
train gradient:  0.17680622795481526
iteration : 2863
train acc:  0.7421875
train loss:  0.49699515104293823
train gradient:  0.16933423475057058
iteration : 2864
train acc:  0.7421875
train loss:  0.49451106786727905
train gradient:  0.15453324487379747
iteration : 2865
train acc:  0.6796875
train loss:  0.562673807144165
train gradient:  0.21768929899105616
iteration : 2866
train acc:  0.7265625
train loss:  0.5466068387031555
train gradient:  0.14455510661633778
iteration : 2867
train acc:  0.6640625
train loss:  0.5559461712837219
train gradient:  0.16655828055903701
iteration : 2868
train acc:  0.734375
train loss:  0.5263669490814209
train gradient:  0.14725573633704309
iteration : 2869
train acc:  0.671875
train loss:  0.5715065002441406
train gradient:  0.17120327398010898
iteration : 2870
train acc:  0.65625
train loss:  0.5598623752593994
train gradient:  0.18167377780378188
iteration : 2871
train acc:  0.6640625
train loss:  0.6051160097122192
train gradient:  0.17136398859928917
iteration : 2872
train acc:  0.734375
train loss:  0.5319708585739136
train gradient:  0.15011538181440123
iteration : 2873
train acc:  0.6484375
train loss:  0.5778800249099731
train gradient:  0.1966853320213185
iteration : 2874
train acc:  0.7109375
train loss:  0.5079824328422546
train gradient:  0.14366131447179842
iteration : 2875
train acc:  0.7734375
train loss:  0.5274744629859924
train gradient:  0.15350930193463092
iteration : 2876
train acc:  0.71875
train loss:  0.5042859315872192
train gradient:  0.13559463690905557
iteration : 2877
train acc:  0.734375
train loss:  0.516466498374939
train gradient:  0.1717835372107428
iteration : 2878
train acc:  0.78125
train loss:  0.4921124577522278
train gradient:  0.15736347133619788
iteration : 2879
train acc:  0.8125
train loss:  0.4406818449497223
train gradient:  0.1262572059104801
iteration : 2880
train acc:  0.8046875
train loss:  0.46517324447631836
train gradient:  0.16449292919173314
iteration : 2881
train acc:  0.7578125
train loss:  0.5123964548110962
train gradient:  0.15943710410182682
iteration : 2882
train acc:  0.703125
train loss:  0.5668348670005798
train gradient:  0.22145258736171503
iteration : 2883
train acc:  0.71875
train loss:  0.5158142447471619
train gradient:  0.13172092636719845
iteration : 2884
train acc:  0.75
train loss:  0.48935842514038086
train gradient:  0.14118170369958521
iteration : 2885
train acc:  0.7265625
train loss:  0.5307151079177856
train gradient:  0.1702091350181581
iteration : 2886
train acc:  0.671875
train loss:  0.5428388118743896
train gradient:  0.1811494004046998
iteration : 2887
train acc:  0.6796875
train loss:  0.5598925352096558
train gradient:  0.1448809153612713
iteration : 2888
train acc:  0.71875
train loss:  0.5089780688285828
train gradient:  0.14464776406012786
iteration : 2889
train acc:  0.734375
train loss:  0.5695047378540039
train gradient:  0.21987193761357302
iteration : 2890
train acc:  0.7421875
train loss:  0.5443785190582275
train gradient:  0.1447147509821995
iteration : 2891
train acc:  0.734375
train loss:  0.5254328846931458
train gradient:  0.15000822321207047
iteration : 2892
train acc:  0.78125
train loss:  0.49483543634414673
train gradient:  0.1363189300302976
iteration : 2893
train acc:  0.7265625
train loss:  0.5406923294067383
train gradient:  0.14436432137776908
iteration : 2894
train acc:  0.7265625
train loss:  0.5590510368347168
train gradient:  0.16307681645810107
iteration : 2895
train acc:  0.75
train loss:  0.47298288345336914
train gradient:  0.11153423074682325
iteration : 2896
train acc:  0.7109375
train loss:  0.5411414504051208
train gradient:  0.24060556040245065
iteration : 2897
train acc:  0.7109375
train loss:  0.545325517654419
train gradient:  0.1266561763890247
iteration : 2898
train acc:  0.75
train loss:  0.48471277952194214
train gradient:  0.14885440622750715
iteration : 2899
train acc:  0.6796875
train loss:  0.5648030042648315
train gradient:  0.17033201074563759
iteration : 2900
train acc:  0.671875
train loss:  0.5682475566864014
train gradient:  0.19594698517534329
iteration : 2901
train acc:  0.6875
train loss:  0.5187017917633057
train gradient:  0.1588504005702519
iteration : 2902
train acc:  0.6953125
train loss:  0.5106236934661865
train gradient:  0.19703030028513557
iteration : 2903
train acc:  0.734375
train loss:  0.501605212688446
train gradient:  0.14516171758865884
iteration : 2904
train acc:  0.7578125
train loss:  0.523409366607666
train gradient:  0.14681398036175444
iteration : 2905
train acc:  0.6953125
train loss:  0.5493066906929016
train gradient:  0.17942481327337978
iteration : 2906
train acc:  0.7265625
train loss:  0.5191636681556702
train gradient:  0.17266014066744761
iteration : 2907
train acc:  0.75
train loss:  0.5438898801803589
train gradient:  0.2115218491611522
iteration : 2908
train acc:  0.7421875
train loss:  0.5273404121398926
train gradient:  0.2534896865829095
iteration : 2909
train acc:  0.7109375
train loss:  0.5489405393600464
train gradient:  0.1315468824433501
iteration : 2910
train acc:  0.6953125
train loss:  0.5987749099731445
train gradient:  0.19217411601371043
iteration : 2911
train acc:  0.7421875
train loss:  0.5258913636207581
train gradient:  0.22417578692556045
iteration : 2912
train acc:  0.6875
train loss:  0.5154966115951538
train gradient:  0.15263457354093257
iteration : 2913
train acc:  0.71875
train loss:  0.5478111505508423
train gradient:  0.17264319209764364
iteration : 2914
train acc:  0.7265625
train loss:  0.5345321893692017
train gradient:  0.16022466031815139
iteration : 2915
train acc:  0.7265625
train loss:  0.4931393265724182
train gradient:  0.12583657549462401
iteration : 2916
train acc:  0.6796875
train loss:  0.5570685267448425
train gradient:  0.17672026810716202
iteration : 2917
train acc:  0.71875
train loss:  0.5428400039672852
train gradient:  0.16013118857636355
iteration : 2918
train acc:  0.6875
train loss:  0.5421580076217651
train gradient:  0.17223830864982792
iteration : 2919
train acc:  0.7109375
train loss:  0.4981197118759155
train gradient:  0.1417815455692459
iteration : 2920
train acc:  0.7421875
train loss:  0.5043191313743591
train gradient:  0.13013893375682395
iteration : 2921
train acc:  0.703125
train loss:  0.5746994614601135
train gradient:  0.16051306954410227
iteration : 2922
train acc:  0.6875
train loss:  0.567497968673706
train gradient:  0.17373316723346394
iteration : 2923
train acc:  0.6953125
train loss:  0.5432208776473999
train gradient:  0.1618733798278376
iteration : 2924
train acc:  0.75
train loss:  0.5171655416488647
train gradient:  0.23839136710351444
iteration : 2925
train acc:  0.6796875
train loss:  0.6025873422622681
train gradient:  0.262949589582646
iteration : 2926
train acc:  0.75
train loss:  0.5178028345108032
train gradient:  0.1428578552155122
iteration : 2927
train acc:  0.7265625
train loss:  0.4974345862865448
train gradient:  0.1533979026652003
iteration : 2928
train acc:  0.78125
train loss:  0.44484061002731323
train gradient:  0.1393510967713943
iteration : 2929
train acc:  0.7109375
train loss:  0.5210564136505127
train gradient:  0.15634288735074908
iteration : 2930
train acc:  0.734375
train loss:  0.5791691541671753
train gradient:  0.1861296493679903
iteration : 2931
train acc:  0.71875
train loss:  0.5231298208236694
train gradient:  0.18406947036710322
iteration : 2932
train acc:  0.65625
train loss:  0.560509204864502
train gradient:  0.15941981160161
iteration : 2933
train acc:  0.7421875
train loss:  0.4823042154312134
train gradient:  0.15088567016422627
iteration : 2934
train acc:  0.7265625
train loss:  0.5478538274765015
train gradient:  0.1947674188004977
iteration : 2935
train acc:  0.7578125
train loss:  0.4863744080066681
train gradient:  0.17020922799938898
iteration : 2936
train acc:  0.7421875
train loss:  0.5233978629112244
train gradient:  0.2261275677153517
iteration : 2937
train acc:  0.6640625
train loss:  0.55682373046875
train gradient:  0.20202694603237464
iteration : 2938
train acc:  0.6484375
train loss:  0.576129674911499
train gradient:  0.21408450538878215
iteration : 2939
train acc:  0.6875
train loss:  0.5466277599334717
train gradient:  0.28343386266393755
iteration : 2940
train acc:  0.703125
train loss:  0.5306994915008545
train gradient:  0.18164110128027855
iteration : 2941
train acc:  0.71875
train loss:  0.5418046712875366
train gradient:  0.16824629095396565
iteration : 2942
train acc:  0.6640625
train loss:  0.571814775466919
train gradient:  0.2737152535031643
iteration : 2943
train acc:  0.7421875
train loss:  0.5294198393821716
train gradient:  0.15061903967357826
iteration : 2944
train acc:  0.78125
train loss:  0.4637593626976013
train gradient:  0.14078752507899644
iteration : 2945
train acc:  0.7578125
train loss:  0.4878920316696167
train gradient:  0.19244757567094384
iteration : 2946
train acc:  0.734375
train loss:  0.5391733646392822
train gradient:  0.12772126419698104
iteration : 2947
train acc:  0.734375
train loss:  0.5336134433746338
train gradient:  0.16892531618830453
iteration : 2948
train acc:  0.765625
train loss:  0.51548171043396
train gradient:  0.16217259825777952
iteration : 2949
train acc:  0.6875
train loss:  0.5425669550895691
train gradient:  0.1404365096011204
iteration : 2950
train acc:  0.71875
train loss:  0.5455076694488525
train gradient:  0.19994027498565553
iteration : 2951
train acc:  0.6796875
train loss:  0.5511318445205688
train gradient:  0.16639277958254095
iteration : 2952
train acc:  0.765625
train loss:  0.49201852083206177
train gradient:  0.17865509817961728
iteration : 2953
train acc:  0.71875
train loss:  0.5729759931564331
train gradient:  0.18284112241319456
iteration : 2954
train acc:  0.640625
train loss:  0.6593517065048218
train gradient:  0.23372804462949204
iteration : 2955
train acc:  0.796875
train loss:  0.46135759353637695
train gradient:  0.13725571024589345
iteration : 2956
train acc:  0.78125
train loss:  0.47386878728866577
train gradient:  0.12445724008071234
iteration : 2957
train acc:  0.75
train loss:  0.5210543870925903
train gradient:  0.13670811431892518
iteration : 2958
train acc:  0.7265625
train loss:  0.6071873903274536
train gradient:  0.1697239447365097
iteration : 2959
train acc:  0.7734375
train loss:  0.4967518448829651
train gradient:  0.1559695532045353
iteration : 2960
train acc:  0.7578125
train loss:  0.4845421016216278
train gradient:  0.13262315263079666
iteration : 2961
train acc:  0.6953125
train loss:  0.5925621390342712
train gradient:  0.21179049536316585
iteration : 2962
train acc:  0.7265625
train loss:  0.511979341506958
train gradient:  0.13324927818068577
iteration : 2963
train acc:  0.703125
train loss:  0.5425078868865967
train gradient:  0.21287660775994904
iteration : 2964
train acc:  0.6875
train loss:  0.5598299503326416
train gradient:  0.16855516829815598
iteration : 2965
train acc:  0.7578125
train loss:  0.4459272623062134
train gradient:  0.12649905272270281
iteration : 2966
train acc:  0.75
train loss:  0.49892520904541016
train gradient:  0.14332989136844482
iteration : 2967
train acc:  0.6640625
train loss:  0.5634298324584961
train gradient:  0.15783287990419487
iteration : 2968
train acc:  0.765625
train loss:  0.5029495358467102
train gradient:  0.13741982885591436
iteration : 2969
train acc:  0.671875
train loss:  0.5710332989692688
train gradient:  0.16209124929597882
iteration : 2970
train acc:  0.734375
train loss:  0.5621617436408997
train gradient:  0.17413547779238453
iteration : 2971
train acc:  0.75
train loss:  0.4862775504589081
train gradient:  0.12712549377800217
iteration : 2972
train acc:  0.7265625
train loss:  0.54496830701828
train gradient:  0.2215681468059018
iteration : 2973
train acc:  0.8125
train loss:  0.43807563185691833
train gradient:  0.11357787910172887
iteration : 2974
train acc:  0.6875
train loss:  0.5591545701026917
train gradient:  0.19831844314615366
iteration : 2975
train acc:  0.75
train loss:  0.5295215845108032
train gradient:  0.25722482602471325
iteration : 2976
train acc:  0.6875
train loss:  0.6037784814834595
train gradient:  0.18622825990901054
iteration : 2977
train acc:  0.7109375
train loss:  0.5602941513061523
train gradient:  0.18342286887906722
iteration : 2978
train acc:  0.75
train loss:  0.5532729029655457
train gradient:  0.18553016231357566
iteration : 2979
train acc:  0.6875
train loss:  0.4953979551792145
train gradient:  0.18483689038687662
iteration : 2980
train acc:  0.7578125
train loss:  0.5151569247245789
train gradient:  0.13704490914657114
iteration : 2981
train acc:  0.7265625
train loss:  0.5623949766159058
train gradient:  0.2193680537536286
iteration : 2982
train acc:  0.7578125
train loss:  0.4939349293708801
train gradient:  0.1647322604923758
iteration : 2983
train acc:  0.734375
train loss:  0.5247660875320435
train gradient:  0.13788277667930549
iteration : 2984
train acc:  0.7734375
train loss:  0.4736139178276062
train gradient:  0.128690154756605
iteration : 2985
train acc:  0.6640625
train loss:  0.5570163726806641
train gradient:  0.16828394474116304
iteration : 2986
train acc:  0.75
train loss:  0.4825248122215271
train gradient:  0.1366768993454761
iteration : 2987
train acc:  0.6953125
train loss:  0.54008948802948
train gradient:  0.187209992499079
iteration : 2988
train acc:  0.796875
train loss:  0.4636117219924927
train gradient:  0.1080093565691146
iteration : 2989
train acc:  0.734375
train loss:  0.5158092975616455
train gradient:  0.14399685534783507
iteration : 2990
train acc:  0.7421875
train loss:  0.4844648838043213
train gradient:  0.12593237996997508
iteration : 2991
train acc:  0.7109375
train loss:  0.5236722230911255
train gradient:  0.19657522604495153
iteration : 2992
train acc:  0.71875
train loss:  0.4950314462184906
train gradient:  0.14592112871255392
iteration : 2993
train acc:  0.7578125
train loss:  0.5148351192474365
train gradient:  0.17590545094039478
iteration : 2994
train acc:  0.703125
train loss:  0.570949375629425
train gradient:  0.15982460481843527
iteration : 2995
train acc:  0.65625
train loss:  0.578367292881012
train gradient:  0.23931067871484224
iteration : 2996
train acc:  0.7265625
train loss:  0.5032024383544922
train gradient:  0.14786366376818472
iteration : 2997
train acc:  0.7109375
train loss:  0.5261436104774475
train gradient:  0.17585298880384198
iteration : 2998
train acc:  0.7109375
train loss:  0.5295348167419434
train gradient:  0.131567206850076
iteration : 2999
train acc:  0.78125
train loss:  0.4878174960613251
train gradient:  0.14998302064969682
iteration : 3000
train acc:  0.671875
train loss:  0.59474116563797
train gradient:  0.21986298527643974
iteration : 3001
train acc:  0.6953125
train loss:  0.5656110644340515
train gradient:  0.20785520215902575
iteration : 3002
train acc:  0.7265625
train loss:  0.5246987342834473
train gradient:  0.16272518263584335
iteration : 3003
train acc:  0.7109375
train loss:  0.514032781124115
train gradient:  0.13473530312066706
iteration : 3004
train acc:  0.7265625
train loss:  0.5103024244308472
train gradient:  0.15704275929202877
iteration : 3005
train acc:  0.7109375
train loss:  0.5513907670974731
train gradient:  0.15686803951233372
iteration : 3006
train acc:  0.7265625
train loss:  0.5440980195999146
train gradient:  0.22936207377043333
iteration : 3007
train acc:  0.7734375
train loss:  0.4977138340473175
train gradient:  0.15858615258614223
iteration : 3008
train acc:  0.796875
train loss:  0.44968926906585693
train gradient:  0.12471349165462348
iteration : 3009
train acc:  0.71875
train loss:  0.5586797595024109
train gradient:  0.20101822541796976
iteration : 3010
train acc:  0.7578125
train loss:  0.4977371096611023
train gradient:  0.15537735304815026
iteration : 3011
train acc:  0.6640625
train loss:  0.5758427381515503
train gradient:  0.2162436993319417
iteration : 3012
train acc:  0.625
train loss:  0.6424803733825684
train gradient:  0.20222029954743564
iteration : 3013
train acc:  0.71875
train loss:  0.5210304856300354
train gradient:  0.1834115315651855
iteration : 3014
train acc:  0.75
train loss:  0.5081030130386353
train gradient:  0.12696550057863232
iteration : 3015
train acc:  0.7421875
train loss:  0.5009996891021729
train gradient:  0.15243963973854965
iteration : 3016
train acc:  0.6875
train loss:  0.5637667179107666
train gradient:  0.17084744542253147
iteration : 3017
train acc:  0.671875
train loss:  0.5942277312278748
train gradient:  0.1567608441770129
iteration : 3018
train acc:  0.65625
train loss:  0.5642373561859131
train gradient:  0.21494512079606648
iteration : 3019
train acc:  0.7265625
train loss:  0.5013384819030762
train gradient:  0.16228035260375745
iteration : 3020
train acc:  0.625
train loss:  0.5788507461547852
train gradient:  0.15913716825624213
iteration : 3021
train acc:  0.734375
train loss:  0.524967610836029
train gradient:  0.12844558231920244
iteration : 3022
train acc:  0.7265625
train loss:  0.490449458360672
train gradient:  0.14440651978336277
iteration : 3023
train acc:  0.71875
train loss:  0.5270191431045532
train gradient:  0.14266904290975957
iteration : 3024
train acc:  0.75
train loss:  0.49143633246421814
train gradient:  0.1627881043822768
iteration : 3025
train acc:  0.75
train loss:  0.5580877065658569
train gradient:  0.17920762307453558
iteration : 3026
train acc:  0.7734375
train loss:  0.5149424076080322
train gradient:  0.16168433741299096
iteration : 3027
train acc:  0.6953125
train loss:  0.6048029065132141
train gradient:  0.2746898466292287
iteration : 3028
train acc:  0.734375
train loss:  0.546890377998352
train gradient:  0.14268931872888546
iteration : 3029
train acc:  0.6640625
train loss:  0.5986712574958801
train gradient:  0.23756156487171531
iteration : 3030
train acc:  0.75
train loss:  0.49640360474586487
train gradient:  0.14974184608947022
iteration : 3031
train acc:  0.71875
train loss:  0.527164876461029
train gradient:  0.16199293840587356
iteration : 3032
train acc:  0.765625
train loss:  0.48242729902267456
train gradient:  0.17401463830806377
iteration : 3033
train acc:  0.703125
train loss:  0.5462400913238525
train gradient:  0.15887857056441756
iteration : 3034
train acc:  0.6640625
train loss:  0.5554384589195251
train gradient:  0.19233604531931847
iteration : 3035
train acc:  0.6640625
train loss:  0.5700571537017822
train gradient:  0.25773476866490935
iteration : 3036
train acc:  0.6640625
train loss:  0.5526658296585083
train gradient:  0.18188037652847525
iteration : 3037
train acc:  0.6796875
train loss:  0.5183966755867004
train gradient:  0.15378740891555154
iteration : 3038
train acc:  0.7265625
train loss:  0.5338495969772339
train gradient:  0.23577513431512953
iteration : 3039
train acc:  0.75
train loss:  0.5345743298530579
train gradient:  0.18563238883091912
iteration : 3040
train acc:  0.6953125
train loss:  0.5651918649673462
train gradient:  0.1832376224025508
iteration : 3041
train acc:  0.71875
train loss:  0.5415910482406616
train gradient:  0.16024207217380015
iteration : 3042
train acc:  0.75
train loss:  0.5076490640640259
train gradient:  0.13936413397871344
iteration : 3043
train acc:  0.625
train loss:  0.6164360642433167
train gradient:  0.1939169975990922
iteration : 3044
train acc:  0.6953125
train loss:  0.5307945013046265
train gradient:  0.17237764184592477
iteration : 3045
train acc:  0.703125
train loss:  0.5364750623703003
train gradient:  0.15673046425568596
iteration : 3046
train acc:  0.640625
train loss:  0.6066797971725464
train gradient:  0.2365704653790826
iteration : 3047
train acc:  0.6484375
train loss:  0.6134528517723083
train gradient:  0.21312777027933455
iteration : 3048
train acc:  0.7890625
train loss:  0.4849965274333954
train gradient:  0.13366931539342086
iteration : 3049
train acc:  0.734375
train loss:  0.5263979434967041
train gradient:  0.15976683597450608
iteration : 3050
train acc:  0.7265625
train loss:  0.49544990062713623
train gradient:  0.15028489093134925
iteration : 3051
train acc:  0.6796875
train loss:  0.5315239429473877
train gradient:  0.1870653573704682
iteration : 3052
train acc:  0.71875
train loss:  0.5297279357910156
train gradient:  0.1576122515546649
iteration : 3053
train acc:  0.703125
train loss:  0.556705892086029
train gradient:  0.15296272742914002
iteration : 3054
train acc:  0.6796875
train loss:  0.5550909638404846
train gradient:  0.16950213412560808
iteration : 3055
train acc:  0.6875
train loss:  0.5927912592887878
train gradient:  0.1639195807629623
iteration : 3056
train acc:  0.6640625
train loss:  0.6002390384674072
train gradient:  0.1698725146089746
iteration : 3057
train acc:  0.6875
train loss:  0.523678183555603
train gradient:  0.12569731839903944
iteration : 3058
train acc:  0.703125
train loss:  0.5192826390266418
train gradient:  0.16023427524695827
iteration : 3059
train acc:  0.7265625
train loss:  0.5740372538566589
train gradient:  0.1875101856316144
iteration : 3060
train acc:  0.734375
train loss:  0.5131480693817139
train gradient:  0.2023459310879386
iteration : 3061
train acc:  0.71875
train loss:  0.5306448936462402
train gradient:  0.1276546052646238
iteration : 3062
train acc:  0.7109375
train loss:  0.5254477262496948
train gradient:  0.17881973866000694
iteration : 3063
train acc:  0.7421875
train loss:  0.5177090167999268
train gradient:  0.1386687957166317
iteration : 3064
train acc:  0.703125
train loss:  0.5499418377876282
train gradient:  0.12504123656145488
iteration : 3065
train acc:  0.734375
train loss:  0.5041273832321167
train gradient:  0.13911404901328572
iteration : 3066
train acc:  0.7578125
train loss:  0.5071746110916138
train gradient:  0.17539435938951645
iteration : 3067
train acc:  0.6953125
train loss:  0.5636765956878662
train gradient:  0.16802267069208585
iteration : 3068
train acc:  0.6328125
train loss:  0.635888934135437
train gradient:  0.20056466143820575
iteration : 3069
train acc:  0.734375
train loss:  0.49460381269454956
train gradient:  0.17274818264150826
iteration : 3070
train acc:  0.6796875
train loss:  0.5606724619865417
train gradient:  0.2112743304386075
iteration : 3071
train acc:  0.7421875
train loss:  0.5076587796211243
train gradient:  0.1537631123545655
iteration : 3072
train acc:  0.7109375
train loss:  0.5125146508216858
train gradient:  0.13839058635769305
iteration : 3073
train acc:  0.7421875
train loss:  0.46428442001342773
train gradient:  0.125495950626411
iteration : 3074
train acc:  0.6953125
train loss:  0.5209354162216187
train gradient:  0.1503980130226215
iteration : 3075
train acc:  0.75
train loss:  0.5099450349807739
train gradient:  0.17641515555511333
iteration : 3076
train acc:  0.7109375
train loss:  0.5443008542060852
train gradient:  0.15744474657844887
iteration : 3077
train acc:  0.65625
train loss:  0.5774614810943604
train gradient:  0.20461238225963277
iteration : 3078
train acc:  0.7265625
train loss:  0.534827470779419
train gradient:  0.2068289464011963
iteration : 3079
train acc:  0.734375
train loss:  0.49398428201675415
train gradient:  0.12696349513126712
iteration : 3080
train acc:  0.71875
train loss:  0.5604057908058167
train gradient:  0.2219400794308915
iteration : 3081
train acc:  0.6875
train loss:  0.5998172760009766
train gradient:  0.21117949065664948
iteration : 3082
train acc:  0.6171875
train loss:  0.597669243812561
train gradient:  0.21620706269291068
iteration : 3083
train acc:  0.7109375
train loss:  0.5624741911888123
train gradient:  0.17782288651406425
iteration : 3084
train acc:  0.6875
train loss:  0.563340425491333
train gradient:  0.1939899350903865
iteration : 3085
train acc:  0.765625
train loss:  0.5031790733337402
train gradient:  0.16286354509656986
iteration : 3086
train acc:  0.75
train loss:  0.47746196389198303
train gradient:  0.12196997561128751
iteration : 3087
train acc:  0.71875
train loss:  0.5749106407165527
train gradient:  0.1660682657446673
iteration : 3088
train acc:  0.71875
train loss:  0.5114052295684814
train gradient:  0.1812924880617158
iteration : 3089
train acc:  0.7421875
train loss:  0.505010187625885
train gradient:  0.12605297029628995
iteration : 3090
train acc:  0.6640625
train loss:  0.5920931696891785
train gradient:  0.1935121634560765
iteration : 3091
train acc:  0.75
train loss:  0.5465059280395508
train gradient:  0.144009286304896
iteration : 3092
train acc:  0.78125
train loss:  0.46820884943008423
train gradient:  0.13242902978058013
iteration : 3093
train acc:  0.71875
train loss:  0.5320069789886475
train gradient:  0.1701808654271283
iteration : 3094
train acc:  0.8203125
train loss:  0.4523494243621826
train gradient:  0.1863838575067891
iteration : 3095
train acc:  0.671875
train loss:  0.6087403297424316
train gradient:  0.2054298912184181
iteration : 3096
train acc:  0.6640625
train loss:  0.5667731761932373
train gradient:  0.2035664855531314
iteration : 3097
train acc:  0.765625
train loss:  0.4994359612464905
train gradient:  0.13421862274476087
iteration : 3098
train acc:  0.703125
train loss:  0.5231419801712036
train gradient:  0.1616606100744863
iteration : 3099
train acc:  0.71875
train loss:  0.5745573043823242
train gradient:  0.1671630539150229
iteration : 3100
train acc:  0.6953125
train loss:  0.5156638622283936
train gradient:  0.14269373064234225
iteration : 3101
train acc:  0.7109375
train loss:  0.4872484803199768
train gradient:  0.1391269295145303
iteration : 3102
train acc:  0.71875
train loss:  0.5463135838508606
train gradient:  0.18391483557841587
iteration : 3103
train acc:  0.71875
train loss:  0.5004630088806152
train gradient:  0.1424814568127059
iteration : 3104
train acc:  0.6640625
train loss:  0.544685423374176
train gradient:  0.17582120390702544
iteration : 3105
train acc:  0.734375
train loss:  0.49491992592811584
train gradient:  0.1713474396505808
iteration : 3106
train acc:  0.71875
train loss:  0.5570555925369263
train gradient:  0.18986346738487958
iteration : 3107
train acc:  0.671875
train loss:  0.5597790479660034
train gradient:  0.1990656973449632
iteration : 3108
train acc:  0.6953125
train loss:  0.527041494846344
train gradient:  0.1765520820731346
iteration : 3109
train acc:  0.703125
train loss:  0.5149025917053223
train gradient:  0.1532572785857234
iteration : 3110
train acc:  0.7109375
train loss:  0.5328589081764221
train gradient:  0.13262645982318158
iteration : 3111
train acc:  0.7421875
train loss:  0.5318206548690796
train gradient:  0.14832664401749251
iteration : 3112
train acc:  0.703125
train loss:  0.5714964866638184
train gradient:  0.17542275359853624
iteration : 3113
train acc:  0.7265625
train loss:  0.5233983993530273
train gradient:  0.12933920560765505
iteration : 3114
train acc:  0.7265625
train loss:  0.5633448362350464
train gradient:  0.22622982393895058
iteration : 3115
train acc:  0.7109375
train loss:  0.546007513999939
train gradient:  0.14514000204567845
iteration : 3116
train acc:  0.65625
train loss:  0.5326921939849854
train gradient:  0.16177334699306206
iteration : 3117
train acc:  0.7421875
train loss:  0.5220584869384766
train gradient:  0.18902865579222344
iteration : 3118
train acc:  0.6640625
train loss:  0.5848017930984497
train gradient:  0.17771217340744433
iteration : 3119
train acc:  0.6640625
train loss:  0.6080739498138428
train gradient:  0.19121358028003926
iteration : 3120
train acc:  0.6796875
train loss:  0.5881811380386353
train gradient:  0.1907365617707931
iteration : 3121
train acc:  0.6640625
train loss:  0.5826293230056763
train gradient:  0.18278343707334865
iteration : 3122
train acc:  0.734375
train loss:  0.5237473249435425
train gradient:  0.1462562059153783
iteration : 3123
train acc:  0.6875
train loss:  0.5579907894134521
train gradient:  0.20857743720141972
iteration : 3124
train acc:  0.65625
train loss:  0.5823022127151489
train gradient:  0.1863378017057647
iteration : 3125
train acc:  0.6796875
train loss:  0.5783144235610962
train gradient:  0.1671151327515257
iteration : 3126
train acc:  0.71875
train loss:  0.5222159624099731
train gradient:  0.21957685375637387
iteration : 3127
train acc:  0.71875
train loss:  0.5457484722137451
train gradient:  0.16612735612223095
iteration : 3128
train acc:  0.7578125
train loss:  0.5119694471359253
train gradient:  0.2101901978898924
iteration : 3129
train acc:  0.7734375
train loss:  0.5020120739936829
train gradient:  0.1646169727236398
iteration : 3130
train acc:  0.703125
train loss:  0.5572011470794678
train gradient:  0.19758119044510677
iteration : 3131
train acc:  0.65625
train loss:  0.5881974101066589
train gradient:  0.24921854936071391
iteration : 3132
train acc:  0.7578125
train loss:  0.4980563521385193
train gradient:  0.1380904496106503
iteration : 3133
train acc:  0.6953125
train loss:  0.5396480560302734
train gradient:  0.15020607371517813
iteration : 3134
train acc:  0.6953125
train loss:  0.5298774242401123
train gradient:  0.20358818931307038
iteration : 3135
train acc:  0.671875
train loss:  0.6295960545539856
train gradient:  0.2581270984449209
iteration : 3136
train acc:  0.7265625
train loss:  0.5241314768791199
train gradient:  0.17370934518289333
iteration : 3137
train acc:  0.7578125
train loss:  0.4983900785446167
train gradient:  0.1953857617315038
iteration : 3138
train acc:  0.75
train loss:  0.5003337264060974
train gradient:  0.13558649563215525
iteration : 3139
train acc:  0.71875
train loss:  0.5497994422912598
train gradient:  0.13751044865790257
iteration : 3140
train acc:  0.7109375
train loss:  0.5943266153335571
train gradient:  0.2102557322728674
iteration : 3141
train acc:  0.6796875
train loss:  0.5904282927513123
train gradient:  0.2166118951245261
iteration : 3142
train acc:  0.7421875
train loss:  0.5173500776290894
train gradient:  0.17548902818917628
iteration : 3143
train acc:  0.609375
train loss:  0.627737283706665
train gradient:  0.2512529655433639
iteration : 3144
train acc:  0.7265625
train loss:  0.5225033164024353
train gradient:  0.18894930284097794
iteration : 3145
train acc:  0.7734375
train loss:  0.4799625277519226
train gradient:  0.1500564817496515
iteration : 3146
train acc:  0.7265625
train loss:  0.5345300436019897
train gradient:  0.13687716249518242
iteration : 3147
train acc:  0.71875
train loss:  0.4919585585594177
train gradient:  0.13255158774391365
iteration : 3148
train acc:  0.71875
train loss:  0.5231049656867981
train gradient:  0.16985049931009102
iteration : 3149
train acc:  0.6640625
train loss:  0.6032842397689819
train gradient:  0.1808085089707455
iteration : 3150
train acc:  0.75
train loss:  0.5202187299728394
train gradient:  0.119412006625359
iteration : 3151
train acc:  0.765625
train loss:  0.5254510641098022
train gradient:  0.14528568188026766
iteration : 3152
train acc:  0.7421875
train loss:  0.5274772644042969
train gradient:  0.17020021232911242
iteration : 3153
train acc:  0.6953125
train loss:  0.5553699135780334
train gradient:  0.16970535449269475
iteration : 3154
train acc:  0.6640625
train loss:  0.5883287191390991
train gradient:  0.19102008358391942
iteration : 3155
train acc:  0.7109375
train loss:  0.5639811754226685
train gradient:  0.1811389580067946
iteration : 3156
train acc:  0.71875
train loss:  0.49917906522750854
train gradient:  0.19398703881419008
iteration : 3157
train acc:  0.7109375
train loss:  0.5250798463821411
train gradient:  0.15737303018432897
iteration : 3158
train acc:  0.6953125
train loss:  0.5753023624420166
train gradient:  0.15452441773812484
iteration : 3159
train acc:  0.7109375
train loss:  0.6260418891906738
train gradient:  0.33744427805604627
iteration : 3160
train acc:  0.7734375
train loss:  0.5295379757881165
train gradient:  0.15691007810173757
iteration : 3161
train acc:  0.796875
train loss:  0.4862740933895111
train gradient:  0.1864324337896281
iteration : 3162
train acc:  0.7421875
train loss:  0.5557745099067688
train gradient:  0.25717818732349834
iteration : 3163
train acc:  0.75
train loss:  0.5237817168235779
train gradient:  0.1799385043077949
iteration : 3164
train acc:  0.65625
train loss:  0.5731668472290039
train gradient:  0.18798837132590138
iteration : 3165
train acc:  0.7265625
train loss:  0.5411202907562256
train gradient:  0.18445036818632893
iteration : 3166
train acc:  0.765625
train loss:  0.5112317800521851
train gradient:  0.14777933718003256
iteration : 3167
train acc:  0.7421875
train loss:  0.5368660092353821
train gradient:  0.19919033797648822
iteration : 3168
train acc:  0.6796875
train loss:  0.5424699783325195
train gradient:  0.17119944578581947
iteration : 3169
train acc:  0.671875
train loss:  0.5636359453201294
train gradient:  0.20791992419910554
iteration : 3170
train acc:  0.703125
train loss:  0.5285962224006653
train gradient:  0.1978457551530369
iteration : 3171
train acc:  0.6875
train loss:  0.6151386499404907
train gradient:  0.2380147935479362
iteration : 3172
train acc:  0.7578125
train loss:  0.467159628868103
train gradient:  0.13962735703768708
iteration : 3173
train acc:  0.6796875
train loss:  0.5748910307884216
train gradient:  0.20073985687119555
iteration : 3174
train acc:  0.7265625
train loss:  0.49534785747528076
train gradient:  0.1354877212200793
iteration : 3175
train acc:  0.71875
train loss:  0.5047415494918823
train gradient:  0.1823558822993051
iteration : 3176
train acc:  0.6953125
train loss:  0.5239368677139282
train gradient:  0.1377995142586168
iteration : 3177
train acc:  0.8125
train loss:  0.48051753640174866
train gradient:  0.14287438127603125
iteration : 3178
train acc:  0.71875
train loss:  0.5304819941520691
train gradient:  0.14464182559111444
iteration : 3179
train acc:  0.71875
train loss:  0.513275146484375
train gradient:  0.17816678212968867
iteration : 3180
train acc:  0.7578125
train loss:  0.47965073585510254
train gradient:  0.1277907893591569
iteration : 3181
train acc:  0.6796875
train loss:  0.5712763071060181
train gradient:  0.1681631852136703
iteration : 3182
train acc:  0.765625
train loss:  0.48440295457839966
train gradient:  0.1661903005483583
iteration : 3183
train acc:  0.6484375
train loss:  0.5915780067443848
train gradient:  0.20423721266106476
iteration : 3184
train acc:  0.6953125
train loss:  0.5829195976257324
train gradient:  0.14442425631257522
iteration : 3185
train acc:  0.671875
train loss:  0.584433913230896
train gradient:  0.22695668156358334
iteration : 3186
train acc:  0.8125
train loss:  0.45007526874542236
train gradient:  0.1174476352593425
iteration : 3187
train acc:  0.734375
train loss:  0.5467197895050049
train gradient:  0.16450245007387648
iteration : 3188
train acc:  0.7109375
train loss:  0.5515204668045044
train gradient:  0.1547866700797459
iteration : 3189
train acc:  0.7265625
train loss:  0.5385668277740479
train gradient:  0.17990338223297214
iteration : 3190
train acc:  0.6953125
train loss:  0.509781002998352
train gradient:  0.1715731085703875
iteration : 3191
train acc:  0.75
train loss:  0.5196206569671631
train gradient:  0.19053073440990817
iteration : 3192
train acc:  0.7109375
train loss:  0.5526490211486816
train gradient:  0.14752725352500423
iteration : 3193
train acc:  0.7890625
train loss:  0.46034717559814453
train gradient:  0.13019172558719716
iteration : 3194
train acc:  0.703125
train loss:  0.5845772624015808
train gradient:  0.2128881931365138
iteration : 3195
train acc:  0.734375
train loss:  0.5237903594970703
train gradient:  0.1380390356236687
iteration : 3196
train acc:  0.7421875
train loss:  0.5673866271972656
train gradient:  0.2540991775199141
iteration : 3197
train acc:  0.8046875
train loss:  0.46708083152770996
train gradient:  0.13696457168296813
iteration : 3198
train acc:  0.6875
train loss:  0.5563660860061646
train gradient:  0.18965907550760802
iteration : 3199
train acc:  0.734375
train loss:  0.5330995917320251
train gradient:  0.18484970708279275
iteration : 3200
train acc:  0.7578125
train loss:  0.4712528586387634
train gradient:  0.15744347568679595
iteration : 3201
train acc:  0.6015625
train loss:  0.6310420036315918
train gradient:  0.20432383678307803
iteration : 3202
train acc:  0.6875
train loss:  0.5785108804702759
train gradient:  0.1745909911939393
iteration : 3203
train acc:  0.7421875
train loss:  0.49144917726516724
train gradient:  0.23478923884188768
iteration : 3204
train acc:  0.78125
train loss:  0.5174777507781982
train gradient:  0.152226791981434
iteration : 3205
train acc:  0.7265625
train loss:  0.5451254844665527
train gradient:  0.17115710156316705
iteration : 3206
train acc:  0.7109375
train loss:  0.4887264370918274
train gradient:  0.18901289129062904
iteration : 3207
train acc:  0.7109375
train loss:  0.5284233093261719
train gradient:  0.1629459718049536
iteration : 3208
train acc:  0.7109375
train loss:  0.5763754844665527
train gradient:  0.19891557867724569
iteration : 3209
train acc:  0.734375
train loss:  0.528874933719635
train gradient:  0.17992475502869115
iteration : 3210
train acc:  0.7109375
train loss:  0.514892578125
train gradient:  0.17473546972906434
iteration : 3211
train acc:  0.765625
train loss:  0.4536930024623871
train gradient:  0.15134615034670998
iteration : 3212
train acc:  0.703125
train loss:  0.5156152248382568
train gradient:  0.214821304261041
iteration : 3213
train acc:  0.6328125
train loss:  0.5590088963508606
train gradient:  0.14989545965714404
iteration : 3214
train acc:  0.671875
train loss:  0.5404825210571289
train gradient:  0.13815988087537473
iteration : 3215
train acc:  0.7890625
train loss:  0.4878040552139282
train gradient:  0.16539424338285952
iteration : 3216
train acc:  0.765625
train loss:  0.515629768371582
train gradient:  0.1842569593860412
iteration : 3217
train acc:  0.7265625
train loss:  0.5931901335716248
train gradient:  0.2158310592020718
iteration : 3218
train acc:  0.7265625
train loss:  0.5237998962402344
train gradient:  0.1724397387556647
iteration : 3219
train acc:  0.6796875
train loss:  0.5512230396270752
train gradient:  0.20442475550028466
iteration : 3220
train acc:  0.671875
train loss:  0.5698068141937256
train gradient:  0.22579816682011689
iteration : 3221
train acc:  0.7421875
train loss:  0.5312229990959167
train gradient:  0.15379457058628893
iteration : 3222
train acc:  0.8125
train loss:  0.44643545150756836
train gradient:  0.13973020985246956
iteration : 3223
train acc:  0.8125
train loss:  0.467145174741745
train gradient:  0.13574760970475822
iteration : 3224
train acc:  0.765625
train loss:  0.4793458878993988
train gradient:  0.13821124640310206
iteration : 3225
train acc:  0.7578125
train loss:  0.5107941627502441
train gradient:  0.1263011956025739
iteration : 3226
train acc:  0.6875
train loss:  0.537233829498291
train gradient:  0.1813282417751028
iteration : 3227
train acc:  0.75
train loss:  0.49571049213409424
train gradient:  0.12116981904850511
iteration : 3228
train acc:  0.7109375
train loss:  0.5421556234359741
train gradient:  0.16994183140244148
iteration : 3229
train acc:  0.7578125
train loss:  0.4537539482116699
train gradient:  0.13546845227042353
iteration : 3230
train acc:  0.75
train loss:  0.6008122563362122
train gradient:  0.2186082112959413
iteration : 3231
train acc:  0.75
train loss:  0.5206454396247864
train gradient:  0.1942540204367555
iteration : 3232
train acc:  0.7421875
train loss:  0.48614832758903503
train gradient:  0.12028814410483574
iteration : 3233
train acc:  0.7265625
train loss:  0.5150566101074219
train gradient:  0.13158856791379342
iteration : 3234
train acc:  0.7578125
train loss:  0.49393102526664734
train gradient:  0.12356673295437798
iteration : 3235
train acc:  0.7421875
train loss:  0.47513920068740845
train gradient:  0.14643460881454498
iteration : 3236
train acc:  0.7578125
train loss:  0.47451508045196533
train gradient:  0.13369740195930724
iteration : 3237
train acc:  0.6640625
train loss:  0.5736393928527832
train gradient:  0.2447328835248821
iteration : 3238
train acc:  0.71875
train loss:  0.5066651701927185
train gradient:  0.1323820681363515
iteration : 3239
train acc:  0.703125
train loss:  0.50734543800354
train gradient:  0.13027715940345475
iteration : 3240
train acc:  0.765625
train loss:  0.4682306945323944
train gradient:  0.16756353508497368
iteration : 3241
train acc:  0.6796875
train loss:  0.5884949564933777
train gradient:  0.24206909136100047
iteration : 3242
train acc:  0.734375
train loss:  0.5090979933738708
train gradient:  0.14307355505517252
iteration : 3243
train acc:  0.703125
train loss:  0.5751017332077026
train gradient:  0.19542792625661792
iteration : 3244
train acc:  0.7734375
train loss:  0.494451642036438
train gradient:  0.19795568236946892
iteration : 3245
train acc:  0.7890625
train loss:  0.48855656385421753
train gradient:  0.1403404265132579
iteration : 3246
train acc:  0.71875
train loss:  0.57981276512146
train gradient:  0.17398403992098133
iteration : 3247
train acc:  0.6875
train loss:  0.5310155749320984
train gradient:  0.22804966425412293
iteration : 3248
train acc:  0.78125
train loss:  0.4989897608757019
train gradient:  0.15316396214785838
iteration : 3249
train acc:  0.6875
train loss:  0.5066429376602173
train gradient:  0.13928001901450182
iteration : 3250
train acc:  0.734375
train loss:  0.5054030418395996
train gradient:  0.16202981501344602
iteration : 3251
train acc:  0.78125
train loss:  0.49814000725746155
train gradient:  0.15588783400176737
iteration : 3252
train acc:  0.65625
train loss:  0.6001068353652954
train gradient:  0.17122480852608923
iteration : 3253
train acc:  0.6875
train loss:  0.5577205419540405
train gradient:  0.19764088268605595
iteration : 3254
train acc:  0.7734375
train loss:  0.4750749468803406
train gradient:  0.15868919422159197
iteration : 3255
train acc:  0.703125
train loss:  0.5254566073417664
train gradient:  0.17465221840404266
iteration : 3256
train acc:  0.703125
train loss:  0.5438146591186523
train gradient:  0.19637929165068807
iteration : 3257
train acc:  0.6796875
train loss:  0.571451723575592
train gradient:  0.1883656591699443
iteration : 3258
train acc:  0.6796875
train loss:  0.6046303510665894
train gradient:  0.2322306840030849
iteration : 3259
train acc:  0.734375
train loss:  0.5295765399932861
train gradient:  0.22209030386850215
iteration : 3260
train acc:  0.6875
train loss:  0.5190623998641968
train gradient:  0.16364619032242111
iteration : 3261
train acc:  0.734375
train loss:  0.5198503732681274
train gradient:  0.1751942599841307
iteration : 3262
train acc:  0.734375
train loss:  0.5368372797966003
train gradient:  0.16453501697008927
iteration : 3263
train acc:  0.734375
train loss:  0.5194728374481201
train gradient:  0.1610216445890264
iteration : 3264
train acc:  0.65625
train loss:  0.598048210144043
train gradient:  0.1838761438902858
iteration : 3265
train acc:  0.75
train loss:  0.5209691524505615
train gradient:  0.15828236222314665
iteration : 3266
train acc:  0.765625
train loss:  0.47865957021713257
train gradient:  0.14602099820265071
iteration : 3267
train acc:  0.75
train loss:  0.5273861885070801
train gradient:  0.14587714119729647
iteration : 3268
train acc:  0.703125
train loss:  0.5592100620269775
train gradient:  0.16599337751142884
iteration : 3269
train acc:  0.6875
train loss:  0.568954348564148
train gradient:  0.17198077180462512
iteration : 3270
train acc:  0.78125
train loss:  0.4739070534706116
train gradient:  0.13954598666325752
iteration : 3271
train acc:  0.7109375
train loss:  0.5398809313774109
train gradient:  0.152387874635658
iteration : 3272
train acc:  0.6875
train loss:  0.5728789567947388
train gradient:  0.18632795621579956
iteration : 3273
train acc:  0.71875
train loss:  0.4719060957431793
train gradient:  0.14277711367174137
iteration : 3274
train acc:  0.703125
train loss:  0.5184388160705566
train gradient:  0.19818980768184602
iteration : 3275
train acc:  0.7109375
train loss:  0.5353603363037109
train gradient:  0.16030035875486787
iteration : 3276
train acc:  0.78125
train loss:  0.4589740037918091
train gradient:  0.1254931250234264
iteration : 3277
train acc:  0.6328125
train loss:  0.5960644483566284
train gradient:  0.28659583063083627
iteration : 3278
train acc:  0.71875
train loss:  0.5750685334205627
train gradient:  0.20898419075856695
iteration : 3279
train acc:  0.7265625
train loss:  0.5301170349121094
train gradient:  0.1671549994599693
iteration : 3280
train acc:  0.734375
train loss:  0.5524396896362305
train gradient:  0.21544393828270647
iteration : 3281
train acc:  0.765625
train loss:  0.47515004873275757
train gradient:  0.142276577593721
iteration : 3282
train acc:  0.7109375
train loss:  0.521965503692627
train gradient:  0.1767890370084895
iteration : 3283
train acc:  0.7421875
train loss:  0.5117164850234985
train gradient:  0.1390664260381195
iteration : 3284
train acc:  0.7734375
train loss:  0.466640830039978
train gradient:  0.23679967360777276
iteration : 3285
train acc:  0.640625
train loss:  0.5774394273757935
train gradient:  0.23677857152345383
iteration : 3286
train acc:  0.703125
train loss:  0.55682772397995
train gradient:  0.19098004850630573
iteration : 3287
train acc:  0.7734375
train loss:  0.46613577008247375
train gradient:  0.1482025390157044
iteration : 3288
train acc:  0.71875
train loss:  0.5028471946716309
train gradient:  0.19463830829340434
iteration : 3289
train acc:  0.6484375
train loss:  0.5534656643867493
train gradient:  0.21270233603673067
iteration : 3290
train acc:  0.71875
train loss:  0.49683550000190735
train gradient:  0.1424330708390995
iteration : 3291
train acc:  0.6875
train loss:  0.5991543531417847
train gradient:  0.2332840524638755
iteration : 3292
train acc:  0.640625
train loss:  0.6291673183441162
train gradient:  0.23103190930110318
iteration : 3293
train acc:  0.765625
train loss:  0.49281299114227295
train gradient:  0.15206535129431287
iteration : 3294
train acc:  0.703125
train loss:  0.5936388969421387
train gradient:  0.16826120060392952
iteration : 3295
train acc:  0.6328125
train loss:  0.6003782749176025
train gradient:  0.21684978098752483
iteration : 3296
train acc:  0.703125
train loss:  0.5627907514572144
train gradient:  0.19853701055390277
iteration : 3297
train acc:  0.7421875
train loss:  0.546783983707428
train gradient:  0.18045860906858513
iteration : 3298
train acc:  0.7734375
train loss:  0.5448581576347351
train gradient:  0.14012408222332498
iteration : 3299
train acc:  0.7578125
train loss:  0.5025079846382141
train gradient:  0.12911576906230426
iteration : 3300
train acc:  0.71875
train loss:  0.5103247761726379
train gradient:  0.1607015999234499
iteration : 3301
train acc:  0.7734375
train loss:  0.47733166813850403
train gradient:  0.13729681750669306
iteration : 3302
train acc:  0.75
train loss:  0.5490808486938477
train gradient:  0.19809505508534345
iteration : 3303
train acc:  0.671875
train loss:  0.5645067691802979
train gradient:  0.20503345790629063
iteration : 3304
train acc:  0.7265625
train loss:  0.5282102823257446
train gradient:  0.17551898039231428
iteration : 3305
train acc:  0.625
train loss:  0.5785881876945496
train gradient:  0.1990996121511605
iteration : 3306
train acc:  0.7421875
train loss:  0.5165081024169922
train gradient:  0.15284024457455436
iteration : 3307
train acc:  0.7890625
train loss:  0.4685521125793457
train gradient:  0.15807854780438813
iteration : 3308
train acc:  0.7109375
train loss:  0.5033599138259888
train gradient:  0.14467908549970077
iteration : 3309
train acc:  0.6796875
train loss:  0.5193203687667847
train gradient:  0.15628300320250293
iteration : 3310
train acc:  0.7421875
train loss:  0.5159732103347778
train gradient:  0.18915645937471354
iteration : 3311
train acc:  0.7421875
train loss:  0.5151962041854858
train gradient:  0.1762428010929528
iteration : 3312
train acc:  0.703125
train loss:  0.5676665902137756
train gradient:  0.18227968337168565
iteration : 3313
train acc:  0.7265625
train loss:  0.5348398089408875
train gradient:  0.1864620123671007
iteration : 3314
train acc:  0.71875
train loss:  0.5289280414581299
train gradient:  0.14866994893978008
iteration : 3315
train acc:  0.7421875
train loss:  0.49722525477409363
train gradient:  0.20555881827627226
iteration : 3316
train acc:  0.6953125
train loss:  0.5602643489837646
train gradient:  0.1758149467970937
iteration : 3317
train acc:  0.7578125
train loss:  0.5465615391731262
train gradient:  0.14708816422715565
iteration : 3318
train acc:  0.7265625
train loss:  0.5508365631103516
train gradient:  0.2272603837112686
iteration : 3319
train acc:  0.625
train loss:  0.6978334784507751
train gradient:  0.2964768821258307
iteration : 3320
train acc:  0.6953125
train loss:  0.5207222700119019
train gradient:  0.1823201510517484
iteration : 3321
train acc:  0.65625
train loss:  0.5492255091667175
train gradient:  0.17717478905361606
iteration : 3322
train acc:  0.7109375
train loss:  0.5375934839248657
train gradient:  0.17811258783638673
iteration : 3323
train acc:  0.75
train loss:  0.479105681180954
train gradient:  0.14666067177038616
iteration : 3324
train acc:  0.7109375
train loss:  0.5187621116638184
train gradient:  0.19427573886726957
iteration : 3325
train acc:  0.71875
train loss:  0.540490984916687
train gradient:  0.16000452345032595
iteration : 3326
train acc:  0.7578125
train loss:  0.5170019865036011
train gradient:  0.17160324329185905
iteration : 3327
train acc:  0.671875
train loss:  0.5870363712310791
train gradient:  0.18317891019621693
iteration : 3328
train acc:  0.6484375
train loss:  0.5815314054489136
train gradient:  0.19210466524709768
iteration : 3329
train acc:  0.7109375
train loss:  0.596564769744873
train gradient:  0.16100853589502545
iteration : 3330
train acc:  0.7109375
train loss:  0.5241847038269043
train gradient:  0.12510357178273615
iteration : 3331
train acc:  0.734375
train loss:  0.5107280015945435
train gradient:  0.1463009854398966
iteration : 3332
train acc:  0.703125
train loss:  0.5564358234405518
train gradient:  0.2711776924863269
iteration : 3333
train acc:  0.7265625
train loss:  0.5408733487129211
train gradient:  0.1826820420944888
iteration : 3334
train acc:  0.703125
train loss:  0.5207728743553162
train gradient:  0.18479807708076998
iteration : 3335
train acc:  0.7734375
train loss:  0.49541616439819336
train gradient:  0.11595050984111692
iteration : 3336
train acc:  0.7265625
train loss:  0.5112713575363159
train gradient:  0.13960617227973487
iteration : 3337
train acc:  0.796875
train loss:  0.5035790205001831
train gradient:  0.20917427977744246
iteration : 3338
train acc:  0.75
train loss:  0.4702531695365906
train gradient:  0.16341265773766053
iteration : 3339
train acc:  0.65625
train loss:  0.5548003911972046
train gradient:  0.16290514153337304
iteration : 3340
train acc:  0.7890625
train loss:  0.4666357934474945
train gradient:  0.15442325065336593
iteration : 3341
train acc:  0.7265625
train loss:  0.5039294958114624
train gradient:  0.14232677773789948
iteration : 3342
train acc:  0.75
train loss:  0.5025376081466675
train gradient:  0.15383642202278064
iteration : 3343
train acc:  0.7265625
train loss:  0.5313119888305664
train gradient:  0.1824870783266208
iteration : 3344
train acc:  0.6328125
train loss:  0.6122334003448486
train gradient:  0.21513284798077012
iteration : 3345
train acc:  0.7109375
train loss:  0.5379180908203125
train gradient:  0.18279855177215287
iteration : 3346
train acc:  0.71875
train loss:  0.5011878609657288
train gradient:  0.1611174391212612
iteration : 3347
train acc:  0.75
train loss:  0.47279128432273865
train gradient:  0.1215696652744936
iteration : 3348
train acc:  0.7109375
train loss:  0.5115702152252197
train gradient:  0.1556945760445274
iteration : 3349
train acc:  0.6875
train loss:  0.529504120349884
train gradient:  0.14612884812974503
iteration : 3350
train acc:  0.6953125
train loss:  0.5853152871131897
train gradient:  0.24184504485182617
iteration : 3351
train acc:  0.75
train loss:  0.5327891111373901
train gradient:  0.15090541664934684
iteration : 3352
train acc:  0.734375
train loss:  0.5454326868057251
train gradient:  0.1422113003430902
iteration : 3353
train acc:  0.6484375
train loss:  0.6114070415496826
train gradient:  0.18595529936341176
iteration : 3354
train acc:  0.71875
train loss:  0.5711718797683716
train gradient:  0.1419107371158896
iteration : 3355
train acc:  0.75
train loss:  0.5631674528121948
train gradient:  0.1558921226130186
iteration : 3356
train acc:  0.765625
train loss:  0.4894084334373474
train gradient:  0.12026504565774099
iteration : 3357
train acc:  0.703125
train loss:  0.5473546385765076
train gradient:  0.17260764376272883
iteration : 3358
train acc:  0.734375
train loss:  0.5040180683135986
train gradient:  0.13457397615056443
iteration : 3359
train acc:  0.7421875
train loss:  0.5415383577346802
train gradient:  0.1968583747575664
iteration : 3360
train acc:  0.7421875
train loss:  0.49380505084991455
train gradient:  0.12773731355181758
iteration : 3361
train acc:  0.671875
train loss:  0.5776678323745728
train gradient:  0.16303630621680046
iteration : 3362
train acc:  0.703125
train loss:  0.5521250367164612
train gradient:  0.15099759090667825
iteration : 3363
train acc:  0.640625
train loss:  0.571184515953064
train gradient:  0.1459349601869916
iteration : 3364
train acc:  0.6640625
train loss:  0.6361024379730225
train gradient:  0.18371705346594397
iteration : 3365
train acc:  0.6796875
train loss:  0.5251867771148682
train gradient:  0.18266031916509168
iteration : 3366
train acc:  0.6640625
train loss:  0.605774462223053
train gradient:  0.1782378198922937
iteration : 3367
train acc:  0.6640625
train loss:  0.5414631962776184
train gradient:  0.1410923235782196
iteration : 3368
train acc:  0.859375
train loss:  0.4222823977470398
train gradient:  0.10040671779435491
iteration : 3369
train acc:  0.8046875
train loss:  0.4826667606830597
train gradient:  0.12681377549758155
iteration : 3370
train acc:  0.7578125
train loss:  0.4800034165382385
train gradient:  0.12392043646179825
iteration : 3371
train acc:  0.75
train loss:  0.4943200945854187
train gradient:  0.15596590333613583
iteration : 3372
train acc:  0.6953125
train loss:  0.5315535664558411
train gradient:  0.12145289030553395
iteration : 3373
train acc:  0.671875
train loss:  0.5733981728553772
train gradient:  0.15180335337873235
iteration : 3374
train acc:  0.75
train loss:  0.4994087219238281
train gradient:  0.1312753364951807
iteration : 3375
train acc:  0.6796875
train loss:  0.5506981611251831
train gradient:  0.18915241611696976
iteration : 3376
train acc:  0.703125
train loss:  0.4979665279388428
train gradient:  0.16758566646039655
iteration : 3377
train acc:  0.703125
train loss:  0.5111843347549438
train gradient:  0.1819644250451178
iteration : 3378
train acc:  0.6328125
train loss:  0.552214503288269
train gradient:  0.16630399682569896
iteration : 3379
train acc:  0.6953125
train loss:  0.5626986026763916
train gradient:  0.22322487174673233
iteration : 3380
train acc:  0.734375
train loss:  0.48364347219467163
train gradient:  0.16913119398303395
iteration : 3381
train acc:  0.7421875
train loss:  0.5274085998535156
train gradient:  0.14805687426367606
iteration : 3382
train acc:  0.7421875
train loss:  0.44603192806243896
train gradient:  0.13350659154827485
iteration : 3383
train acc:  0.7734375
train loss:  0.4811983108520508
train gradient:  0.12987660873804102
iteration : 3384
train acc:  0.6875
train loss:  0.5406675338745117
train gradient:  0.13495977488737237
iteration : 3385
train acc:  0.71875
train loss:  0.5355711579322815
train gradient:  0.16045507754986688
iteration : 3386
train acc:  0.71875
train loss:  0.5419917106628418
train gradient:  0.13446266677772759
iteration : 3387
train acc:  0.609375
train loss:  0.6436406373977661
train gradient:  0.2559154793503457
iteration : 3388
train acc:  0.7109375
train loss:  0.5489835143089294
train gradient:  0.1380612357023704
iteration : 3389
train acc:  0.6796875
train loss:  0.5736329555511475
train gradient:  0.13772950035506726
iteration : 3390
train acc:  0.6875
train loss:  0.5341495871543884
train gradient:  0.1621634695024573
iteration : 3391
train acc:  0.734375
train loss:  0.5442830920219421
train gradient:  0.16002013483878685
iteration : 3392
train acc:  0.6953125
train loss:  0.5226097702980042
train gradient:  0.137682420979166
iteration : 3393
train acc:  0.71875
train loss:  0.480396032333374
train gradient:  0.12885474402873678
iteration : 3394
train acc:  0.71875
train loss:  0.5169380903244019
train gradient:  0.1165484898531297
iteration : 3395
train acc:  0.65625
train loss:  0.5716562271118164
train gradient:  0.15065742816758415
iteration : 3396
train acc:  0.71875
train loss:  0.5205815434455872
train gradient:  0.15651812061620346
iteration : 3397
train acc:  0.7265625
train loss:  0.5505732297897339
train gradient:  0.21056210555470317
iteration : 3398
train acc:  0.71875
train loss:  0.5167086124420166
train gradient:  0.16415917509259847
iteration : 3399
train acc:  0.6953125
train loss:  0.5249851942062378
train gradient:  0.16911481281718743
iteration : 3400
train acc:  0.7265625
train loss:  0.5242655277252197
train gradient:  0.15003114443724608
iteration : 3401
train acc:  0.75
train loss:  0.47518348693847656
train gradient:  0.1202042014365334
iteration : 3402
train acc:  0.796875
train loss:  0.4826948642730713
train gradient:  0.23486221438213034
iteration : 3403
train acc:  0.7421875
train loss:  0.5162674188613892
train gradient:  0.13959522622106652
iteration : 3404
train acc:  0.6953125
train loss:  0.5261615514755249
train gradient:  0.14093610470590162
iteration : 3405
train acc:  0.7265625
train loss:  0.5355093479156494
train gradient:  0.15534978246559827
iteration : 3406
train acc:  0.7265625
train loss:  0.49918332695961
train gradient:  0.16104591917233163
iteration : 3407
train acc:  0.7734375
train loss:  0.49415504932403564
train gradient:  0.1046322057203884
iteration : 3408
train acc:  0.6953125
train loss:  0.5272848606109619
train gradient:  0.16532335160998163
iteration : 3409
train acc:  0.6875
train loss:  0.5683405995368958
train gradient:  0.1991011771123663
iteration : 3410
train acc:  0.7578125
train loss:  0.4703179597854614
train gradient:  0.1425340492889261
iteration : 3411
train acc:  0.671875
train loss:  0.5844225883483887
train gradient:  0.24890701893394473
iteration : 3412
train acc:  0.6796875
train loss:  0.5685513019561768
train gradient:  0.20834280634924396
iteration : 3413
train acc:  0.7578125
train loss:  0.5007696747779846
train gradient:  0.1204142416247953
iteration : 3414
train acc:  0.65625
train loss:  0.5656291246414185
train gradient:  0.22446974990344512
iteration : 3415
train acc:  0.6796875
train loss:  0.5703604817390442
train gradient:  0.13546632951639195
iteration : 3416
train acc:  0.7421875
train loss:  0.5006585121154785
train gradient:  0.1543725582307982
iteration : 3417
train acc:  0.75
train loss:  0.5257042646408081
train gradient:  0.15158828928107015
iteration : 3418
train acc:  0.75
train loss:  0.4846755862236023
train gradient:  0.12556116754191612
iteration : 3419
train acc:  0.71875
train loss:  0.4917799234390259
train gradient:  0.13165158618065498
iteration : 3420
train acc:  0.7109375
train loss:  0.5220381617546082
train gradient:  0.15524208935158976
iteration : 3421
train acc:  0.765625
train loss:  0.5368285179138184
train gradient:  0.1458724421628393
iteration : 3422
train acc:  0.703125
train loss:  0.5542045831680298
train gradient:  0.21410894864157104
iteration : 3423
train acc:  0.7109375
train loss:  0.5609376430511475
train gradient:  0.1699562685917273
iteration : 3424
train acc:  0.7109375
train loss:  0.517523467540741
train gradient:  0.1618620798663491
iteration : 3425
train acc:  0.765625
train loss:  0.4824102520942688
train gradient:  0.13803072923457238
iteration : 3426
train acc:  0.734375
train loss:  0.4983702301979065
train gradient:  0.12475003555250122
iteration : 3427
train acc:  0.7109375
train loss:  0.5028558969497681
train gradient:  0.20181579755661894
iteration : 3428
train acc:  0.640625
train loss:  0.581790566444397
train gradient:  0.21665860596848308
iteration : 3429
train acc:  0.6796875
train loss:  0.6054039001464844
train gradient:  0.20552877764579125
iteration : 3430
train acc:  0.75
train loss:  0.48668766021728516
train gradient:  0.14855471812697402
iteration : 3431
train acc:  0.6953125
train loss:  0.5384799838066101
train gradient:  0.15803224791846932
iteration : 3432
train acc:  0.6953125
train loss:  0.5768389701843262
train gradient:  0.16499562650206304
iteration : 3433
train acc:  0.640625
train loss:  0.5741698741912842
train gradient:  0.18472629500710724
iteration : 3434
train acc:  0.6640625
train loss:  0.5806308388710022
train gradient:  0.15435593467089104
iteration : 3435
train acc:  0.7421875
train loss:  0.5570715665817261
train gradient:  0.1641533731572577
iteration : 3436
train acc:  0.703125
train loss:  0.5296016931533813
train gradient:  0.14328443175113065
iteration : 3437
train acc:  0.671875
train loss:  0.5919343829154968
train gradient:  0.2677885328578723
iteration : 3438
train acc:  0.765625
train loss:  0.47154945135116577
train gradient:  0.146854459292725
iteration : 3439
train acc:  0.6171875
train loss:  0.5902266502380371
train gradient:  0.3297743875677779
iteration : 3440
train acc:  0.703125
train loss:  0.5701112747192383
train gradient:  0.17662528153362264
iteration : 3441
train acc:  0.7265625
train loss:  0.5097256898880005
train gradient:  0.1490916680306223
iteration : 3442
train acc:  0.765625
train loss:  0.47505655884742737
train gradient:  0.12188418945334274
iteration : 3443
train acc:  0.7734375
train loss:  0.49946022033691406
train gradient:  0.16146505119599752
iteration : 3444
train acc:  0.6953125
train loss:  0.49458467960357666
train gradient:  0.12175178224969528
iteration : 3445
train acc:  0.671875
train loss:  0.56932532787323
train gradient:  0.16750857807340908
iteration : 3446
train acc:  0.71875
train loss:  0.49087899923324585
train gradient:  0.20556352904587094
iteration : 3447
train acc:  0.71875
train loss:  0.5312484502792358
train gradient:  0.16122137391103847
iteration : 3448
train acc:  0.7109375
train loss:  0.523147702217102
train gradient:  0.1768708710957301
iteration : 3449
train acc:  0.7421875
train loss:  0.5256221890449524
train gradient:  0.16810361892533296
iteration : 3450
train acc:  0.703125
train loss:  0.5020143985748291
train gradient:  0.18088226470823882
iteration : 3451
train acc:  0.6875
train loss:  0.5743095278739929
train gradient:  0.21203066671986465
iteration : 3452
train acc:  0.6875
train loss:  0.5545552968978882
train gradient:  0.16646754309777162
iteration : 3453
train acc:  0.75
train loss:  0.515691876411438
train gradient:  0.12530579915972567
iteration : 3454
train acc:  0.78125
train loss:  0.4955846965312958
train gradient:  0.1439726463772223
iteration : 3455
train acc:  0.625
train loss:  0.6086211204528809
train gradient:  0.2513470369645783
iteration : 3456
train acc:  0.7578125
train loss:  0.493267685174942
train gradient:  0.12804157757172402
iteration : 3457
train acc:  0.6953125
train loss:  0.5224583148956299
train gradient:  0.12571985129557678
iteration : 3458
train acc:  0.7578125
train loss:  0.48796766996383667
train gradient:  0.13118592766572568
iteration : 3459
train acc:  0.7265625
train loss:  0.5485888719558716
train gradient:  0.1875504308146121
iteration : 3460
train acc:  0.78125
train loss:  0.4981065094470978
train gradient:  0.15661153066311553
iteration : 3461
train acc:  0.7265625
train loss:  0.5241730213165283
train gradient:  0.1544164638411205
iteration : 3462
train acc:  0.71875
train loss:  0.5380123257637024
train gradient:  0.17629548578455473
iteration : 3463
train acc:  0.78125
train loss:  0.4905939996242523
train gradient:  0.16319964580311258
iteration : 3464
train acc:  0.71875
train loss:  0.5562947988510132
train gradient:  0.16044092196667495
iteration : 3465
train acc:  0.7109375
train loss:  0.5205731391906738
train gradient:  0.15827328946478583
iteration : 3466
train acc:  0.765625
train loss:  0.4632202982902527
train gradient:  0.12134826769458955
iteration : 3467
train acc:  0.7734375
train loss:  0.47385579347610474
train gradient:  0.1685844163968716
iteration : 3468
train acc:  0.703125
train loss:  0.5628503561019897
train gradient:  0.2022046729946823
iteration : 3469
train acc:  0.7578125
train loss:  0.4841599464416504
train gradient:  0.11158093659217455
iteration : 3470
train acc:  0.7265625
train loss:  0.5172375440597534
train gradient:  0.1428010938935873
iteration : 3471
train acc:  0.640625
train loss:  0.5654173493385315
train gradient:  0.17346094231060669
iteration : 3472
train acc:  0.7421875
train loss:  0.5005842447280884
train gradient:  0.15100259192571291
iteration : 3473
train acc:  0.703125
train loss:  0.51990807056427
train gradient:  0.16275676688195417
iteration : 3474
train acc:  0.7421875
train loss:  0.540060818195343
train gradient:  0.1565518519180119
iteration : 3475
train acc:  0.7109375
train loss:  0.5339938402175903
train gradient:  0.1303545440741009
iteration : 3476
train acc:  0.59375
train loss:  0.6584039926528931
train gradient:  0.2661183040130824
iteration : 3477
train acc:  0.703125
train loss:  0.5722330808639526
train gradient:  0.18110905352711093
iteration : 3478
train acc:  0.734375
train loss:  0.4917835295200348
train gradient:  0.12378313464677577
iteration : 3479
train acc:  0.703125
train loss:  0.5534743666648865
train gradient:  0.15800907620795046
iteration : 3480
train acc:  0.734375
train loss:  0.5535875558853149
train gradient:  0.21135815158291382
iteration : 3481
train acc:  0.7265625
train loss:  0.5445289611816406
train gradient:  0.1522089090173196
iteration : 3482
train acc:  0.765625
train loss:  0.43968236446380615
train gradient:  0.11261155480770722
iteration : 3483
train acc:  0.734375
train loss:  0.5429800748825073
train gradient:  0.1696566563519831
iteration : 3484
train acc:  0.734375
train loss:  0.5678149461746216
train gradient:  0.20460866982747938
iteration : 3485
train acc:  0.7578125
train loss:  0.4935169517993927
train gradient:  0.13041041117517774
iteration : 3486
train acc:  0.75
train loss:  0.47591426968574524
train gradient:  0.1627545937562263
iteration : 3487
train acc:  0.6796875
train loss:  0.6129121780395508
train gradient:  0.2547858082091236
iteration : 3488
train acc:  0.8125
train loss:  0.43931660056114197
train gradient:  0.11597177237530089
iteration : 3489
train acc:  0.6171875
train loss:  0.6009688377380371
train gradient:  0.1713807957330457
iteration : 3490
train acc:  0.6875
train loss:  0.5462247729301453
train gradient:  0.15491403130539663
iteration : 3491
train acc:  0.734375
train loss:  0.5348376035690308
train gradient:  0.170586983849324
iteration : 3492
train acc:  0.734375
train loss:  0.5211169719696045
train gradient:  0.1761183382199051
iteration : 3493
train acc:  0.765625
train loss:  0.5315901637077332
train gradient:  0.12376853515519189
iteration : 3494
train acc:  0.78125
train loss:  0.5135665535926819
train gradient:  0.1915428078835411
iteration : 3495
train acc:  0.75
train loss:  0.47379064559936523
train gradient:  0.26161019023531884
iteration : 3496
train acc:  0.703125
train loss:  0.516445517539978
train gradient:  0.13788290262214906
iteration : 3497
train acc:  0.7265625
train loss:  0.4798867702484131
train gradient:  0.13604877163632675
iteration : 3498
train acc:  0.6640625
train loss:  0.5477663278579712
train gradient:  0.20165320765519673
iteration : 3499
train acc:  0.671875
train loss:  0.5996360778808594
train gradient:  0.18884265893601931
iteration : 3500
train acc:  0.6953125
train loss:  0.5800000429153442
train gradient:  0.21369336066382588
iteration : 3501
train acc:  0.7890625
train loss:  0.4823460280895233
train gradient:  0.12402354921969472
iteration : 3502
train acc:  0.765625
train loss:  0.4491826593875885
train gradient:  0.12567879253551087
iteration : 3503
train acc:  0.7265625
train loss:  0.5215702056884766
train gradient:  0.18569143586015063
iteration : 3504
train acc:  0.71875
train loss:  0.5635554194450378
train gradient:  0.17374468387877356
iteration : 3505
train acc:  0.71875
train loss:  0.5435492992401123
train gradient:  0.251400479338762
iteration : 3506
train acc:  0.75
train loss:  0.5194218754768372
train gradient:  0.1606360646960042
iteration : 3507
train acc:  0.671875
train loss:  0.624428927898407
train gradient:  0.2257973046278348
iteration : 3508
train acc:  0.7734375
train loss:  0.4658527970314026
train gradient:  0.14089043044352212
iteration : 3509
train acc:  0.6953125
train loss:  0.5782015323638916
train gradient:  0.2085490337035945
iteration : 3510
train acc:  0.75
train loss:  0.5375392436981201
train gradient:  0.1755479281726292
iteration : 3511
train acc:  0.7109375
train loss:  0.5323662161827087
train gradient:  0.23699215322118147
iteration : 3512
train acc:  0.765625
train loss:  0.560200572013855
train gradient:  0.17402575390923014
iteration : 3513
train acc:  0.6953125
train loss:  0.5220715999603271
train gradient:  0.15326529962032664
iteration : 3514
train acc:  0.7578125
train loss:  0.5152448415756226
train gradient:  0.1533771576454217
iteration : 3515
train acc:  0.7421875
train loss:  0.517928957939148
train gradient:  0.16354969122296664
iteration : 3516
train acc:  0.7421875
train loss:  0.48607033491134644
train gradient:  0.14234493023404524
iteration : 3517
train acc:  0.6953125
train loss:  0.5579689741134644
train gradient:  0.16813353575589957
iteration : 3518
train acc:  0.7109375
train loss:  0.560567319393158
train gradient:  0.16086074141770668
iteration : 3519
train acc:  0.765625
train loss:  0.46703049540519714
train gradient:  0.13473781161386078
iteration : 3520
train acc:  0.71875
train loss:  0.5229430198669434
train gradient:  0.15040409515754605
iteration : 3521
train acc:  0.796875
train loss:  0.4889748692512512
train gradient:  0.13193405306185407
iteration : 3522
train acc:  0.6953125
train loss:  0.5577048063278198
train gradient:  0.1819171362738412
iteration : 3523
train acc:  0.6328125
train loss:  0.6001105904579163
train gradient:  0.23827704816498468
iteration : 3524
train acc:  0.71875
train loss:  0.5322349071502686
train gradient:  0.17903253250533196
iteration : 3525
train acc:  0.6796875
train loss:  0.5448790788650513
train gradient:  0.17953116802428204
iteration : 3526
train acc:  0.7421875
train loss:  0.4946352541446686
train gradient:  0.11551909175105878
iteration : 3527
train acc:  0.71875
train loss:  0.5351482629776001
train gradient:  0.18967721368917717
iteration : 3528
train acc:  0.671875
train loss:  0.6153913736343384
train gradient:  0.2429175943241055
iteration : 3529
train acc:  0.71875
train loss:  0.5294089317321777
train gradient:  0.16955624720935053
iteration : 3530
train acc:  0.75
train loss:  0.5068366527557373
train gradient:  0.1560696249072936
iteration : 3531
train acc:  0.7109375
train loss:  0.5514329671859741
train gradient:  0.16154309506750458
iteration : 3532
train acc:  0.703125
train loss:  0.5454490184783936
train gradient:  0.24366455029485778
iteration : 3533
train acc:  0.75
train loss:  0.5297665596008301
train gradient:  0.1835766273836611
iteration : 3534
train acc:  0.78125
train loss:  0.4686899781227112
train gradient:  0.17196205791456204
iteration : 3535
train acc:  0.734375
train loss:  0.5396300554275513
train gradient:  0.2495561949743019
iteration : 3536
train acc:  0.6875
train loss:  0.5445927381515503
train gradient:  0.1579907302317924
iteration : 3537
train acc:  0.65625
train loss:  0.5716472268104553
train gradient:  0.22582647558358154
iteration : 3538
train acc:  0.7265625
train loss:  0.5261330008506775
train gradient:  0.1875710225357754
iteration : 3539
train acc:  0.734375
train loss:  0.49072813987731934
train gradient:  0.15199047366625895
iteration : 3540
train acc:  0.703125
train loss:  0.5549411773681641
train gradient:  0.3290611459617463
iteration : 3541
train acc:  0.734375
train loss:  0.51296067237854
train gradient:  0.1510874441902372
iteration : 3542
train acc:  0.734375
train loss:  0.5404878854751587
train gradient:  0.14661456093356456
iteration : 3543
train acc:  0.703125
train loss:  0.558066189289093
train gradient:  0.1747334940969612
iteration : 3544
train acc:  0.734375
train loss:  0.5395511388778687
train gradient:  0.13487491705780819
iteration : 3545
train acc:  0.7578125
train loss:  0.4785654544830322
train gradient:  0.11794158057629282
iteration : 3546
train acc:  0.765625
train loss:  0.48644503951072693
train gradient:  0.1492618083848926
iteration : 3547
train acc:  0.71875
train loss:  0.5359845161437988
train gradient:  0.1620426726567023
iteration : 3548
train acc:  0.7578125
train loss:  0.5309498310089111
train gradient:  0.17017061712973558
iteration : 3549
train acc:  0.6796875
train loss:  0.5369600057601929
train gradient:  0.1973805306106767
iteration : 3550
train acc:  0.71875
train loss:  0.4728235602378845
train gradient:  0.12446691133712144
iteration : 3551
train acc:  0.7578125
train loss:  0.47178953886032104
train gradient:  0.14802588529001495
iteration : 3552
train acc:  0.71875
train loss:  0.5334852337837219
train gradient:  0.18204871473101736
iteration : 3553
train acc:  0.765625
train loss:  0.5070249438285828
train gradient:  0.13576284151609436
iteration : 3554
train acc:  0.6953125
train loss:  0.5530186891555786
train gradient:  0.1730489361219503
iteration : 3555
train acc:  0.8046875
train loss:  0.47778189182281494
train gradient:  0.1645154111018753
iteration : 3556
train acc:  0.6796875
train loss:  0.5636019706726074
train gradient:  0.17324062767287465
iteration : 3557
train acc:  0.7265625
train loss:  0.539627194404602
train gradient:  0.14437209987191263
iteration : 3558
train acc:  0.7421875
train loss:  0.5299546718597412
train gradient:  0.16227695599958578
iteration : 3559
train acc:  0.734375
train loss:  0.5500938892364502
train gradient:  0.17899922849189803
iteration : 3560
train acc:  0.734375
train loss:  0.4995231628417969
train gradient:  0.1296357322560538
iteration : 3561
train acc:  0.71875
train loss:  0.5290200710296631
train gradient:  0.18415442973914065
iteration : 3562
train acc:  0.7421875
train loss:  0.541599452495575
train gradient:  0.14852029571977288
iteration : 3563
train acc:  0.71875
train loss:  0.5640454292297363
train gradient:  0.16903713208466295
iteration : 3564
train acc:  0.71875
train loss:  0.49159878492355347
train gradient:  0.1272329760663769
iteration : 3565
train acc:  0.7265625
train loss:  0.5721579790115356
train gradient:  0.1615488430029931
iteration : 3566
train acc:  0.71875
train loss:  0.5132046341896057
train gradient:  0.13840058735194127
iteration : 3567
train acc:  0.703125
train loss:  0.518521249294281
train gradient:  0.14325533720679795
iteration : 3568
train acc:  0.7109375
train loss:  0.49720948934555054
train gradient:  0.14413403824487311
iteration : 3569
train acc:  0.734375
train loss:  0.4995728135108948
train gradient:  0.13736542113064454
iteration : 3570
train acc:  0.703125
train loss:  0.5819392204284668
train gradient:  0.19433232265144218
iteration : 3571
train acc:  0.734375
train loss:  0.5266180038452148
train gradient:  0.13125829711422882
iteration : 3572
train acc:  0.7421875
train loss:  0.5059982538223267
train gradient:  0.15259286882692874
iteration : 3573
train acc:  0.71875
train loss:  0.5310971736907959
train gradient:  0.12657690819525108
iteration : 3574
train acc:  0.7421875
train loss:  0.5117230415344238
train gradient:  0.14787716533230086
iteration : 3575
train acc:  0.71875
train loss:  0.5168694257736206
train gradient:  0.15585289008295114
iteration : 3576
train acc:  0.7578125
train loss:  0.5126146078109741
train gradient:  0.16482600767924077
iteration : 3577
train acc:  0.7734375
train loss:  0.4818821847438812
train gradient:  0.1221482513886077
iteration : 3578
train acc:  0.703125
train loss:  0.49402284622192383
train gradient:  0.12084154857512924
iteration : 3579
train acc:  0.7734375
train loss:  0.47729089856147766
train gradient:  0.13930198008487427
iteration : 3580
train acc:  0.6796875
train loss:  0.5120419263839722
train gradient:  0.18529091266712558
iteration : 3581
train acc:  0.7265625
train loss:  0.5518366694450378
train gradient:  0.18815111700163212
iteration : 3582
train acc:  0.6796875
train loss:  0.5029428005218506
train gradient:  0.13719805301213356
iteration : 3583
train acc:  0.6953125
train loss:  0.565205454826355
train gradient:  0.18167750938872557
iteration : 3584
train acc:  0.71875
train loss:  0.5103906393051147
train gradient:  0.15615170624006677
iteration : 3585
train acc:  0.734375
train loss:  0.5000351667404175
train gradient:  0.15328491177980452
iteration : 3586
train acc:  0.71875
train loss:  0.515034556388855
train gradient:  0.16035259911856684
iteration : 3587
train acc:  0.7265625
train loss:  0.515112042427063
train gradient:  0.14581534770940924
iteration : 3588
train acc:  0.6953125
train loss:  0.5235017538070679
train gradient:  0.1604836438355644
iteration : 3589
train acc:  0.71875
train loss:  0.5499559640884399
train gradient:  0.18176489280929992
iteration : 3590
train acc:  0.71875
train loss:  0.5516912341117859
train gradient:  0.18399610057368676
iteration : 3591
train acc:  0.796875
train loss:  0.4616153836250305
train gradient:  0.18509705290324857
iteration : 3592
train acc:  0.765625
train loss:  0.4534134864807129
train gradient:  0.12464163794897738
iteration : 3593
train acc:  0.671875
train loss:  0.5309982299804688
train gradient:  0.17762095502971745
iteration : 3594
train acc:  0.6015625
train loss:  0.6539789438247681
train gradient:  0.28716211744791204
iteration : 3595
train acc:  0.6328125
train loss:  0.6211216449737549
train gradient:  0.30416086931862424
iteration : 3596
train acc:  0.71875
train loss:  0.5509057641029358
train gradient:  0.13494418614297105
iteration : 3597
train acc:  0.6484375
train loss:  0.6342321634292603
train gradient:  0.2705324120938696
iteration : 3598
train acc:  0.71875
train loss:  0.5716801285743713
train gradient:  0.21995170879897818
iteration : 3599
train acc:  0.7109375
train loss:  0.5259066820144653
train gradient:  0.18865680955669614
iteration : 3600
train acc:  0.6796875
train loss:  0.5461647510528564
train gradient:  0.201338995383955
iteration : 3601
train acc:  0.6953125
train loss:  0.5528589487075806
train gradient:  0.2015598804686775
iteration : 3602
train acc:  0.71875
train loss:  0.5422738790512085
train gradient:  0.1644698338299303
iteration : 3603
train acc:  0.7734375
train loss:  0.44389745593070984
train gradient:  0.2031703988786482
iteration : 3604
train acc:  0.71875
train loss:  0.5610377192497253
train gradient:  0.1742947429437774
iteration : 3605
train acc:  0.765625
train loss:  0.5227339267730713
train gradient:  0.16964371455278965
iteration : 3606
train acc:  0.7265625
train loss:  0.5367665886878967
train gradient:  0.15504251913518938
iteration : 3607
train acc:  0.6640625
train loss:  0.6118074655532837
train gradient:  0.20366380997830127
iteration : 3608
train acc:  0.6796875
train loss:  0.5183303952217102
train gradient:  0.14125646736055858
iteration : 3609
train acc:  0.734375
train loss:  0.5045738816261292
train gradient:  0.18160120472013952
iteration : 3610
train acc:  0.7109375
train loss:  0.5416787266731262
train gradient:  0.2424426569883266
iteration : 3611
train acc:  0.6796875
train loss:  0.5640761852264404
train gradient:  0.19615751373880697
iteration : 3612
train acc:  0.6953125
train loss:  0.5148680806159973
train gradient:  0.1634063076087563
iteration : 3613
train acc:  0.78125
train loss:  0.5182813405990601
train gradient:  0.15338513980347684
iteration : 3614
train acc:  0.71875
train loss:  0.4892652928829193
train gradient:  0.15784562739596916
iteration : 3615
train acc:  0.6484375
train loss:  0.6053115129470825
train gradient:  0.22103353193024072
iteration : 3616
train acc:  0.6796875
train loss:  0.5783016681671143
train gradient:  0.16943178149779414
iteration : 3617
train acc:  0.71875
train loss:  0.5225788354873657
train gradient:  0.15590213624396324
iteration : 3618
train acc:  0.6875
train loss:  0.5683987736701965
train gradient:  0.17851772421059753
iteration : 3619
train acc:  0.7109375
train loss:  0.466268926858902
train gradient:  0.13861134796935415
iteration : 3620
train acc:  0.75
train loss:  0.5085573196411133
train gradient:  0.13397958142278749
iteration : 3621
train acc:  0.71875
train loss:  0.542121946811676
train gradient:  0.17636745190321867
iteration : 3622
train acc:  0.7265625
train loss:  0.521490752696991
train gradient:  0.16076336400223804
iteration : 3623
train acc:  0.7109375
train loss:  0.5199158787727356
train gradient:  0.20636015520755574
iteration : 3624
train acc:  0.71875
train loss:  0.4896889925003052
train gradient:  0.13308258908350895
iteration : 3625
train acc:  0.75
train loss:  0.49581414461135864
train gradient:  0.14199603379355197
iteration : 3626
train acc:  0.71875
train loss:  0.5149393081665039
train gradient:  0.16323388259817004
iteration : 3627
train acc:  0.78125
train loss:  0.4851904809474945
train gradient:  0.1445584542892815
iteration : 3628
train acc:  0.7578125
train loss:  0.51885586977005
train gradient:  0.15019016099066174
iteration : 3629
train acc:  0.78125
train loss:  0.4770997166633606
train gradient:  0.1623638742935783
iteration : 3630
train acc:  0.796875
train loss:  0.4811282157897949
train gradient:  0.18855831448563162
iteration : 3631
train acc:  0.7265625
train loss:  0.5156383514404297
train gradient:  0.14803346762313116
iteration : 3632
train acc:  0.6875
train loss:  0.5602798461914062
train gradient:  0.2385523923639712
iteration : 3633
train acc:  0.7109375
train loss:  0.5347344875335693
train gradient:  0.1922134286668107
iteration : 3634
train acc:  0.84375
train loss:  0.41188961267471313
train gradient:  0.1306718789324921
iteration : 3635
train acc:  0.7734375
train loss:  0.4752744436264038
train gradient:  0.16599044183541317
iteration : 3636
train acc:  0.7109375
train loss:  0.5547977685928345
train gradient:  0.16289698320253926
iteration : 3637
train acc:  0.640625
train loss:  0.5510601997375488
train gradient:  0.17535791533932163
iteration : 3638
train acc:  0.703125
train loss:  0.5489802956581116
train gradient:  0.17746103646434383
iteration : 3639
train acc:  0.7109375
train loss:  0.5563848614692688
train gradient:  0.14375126948830785
iteration : 3640
train acc:  0.65625
train loss:  0.5793246030807495
train gradient:  0.21381951333549898
iteration : 3641
train acc:  0.78125
train loss:  0.4576231837272644
train gradient:  0.11213430335608708
iteration : 3642
train acc:  0.7265625
train loss:  0.4844106435775757
train gradient:  0.14000940700421494
iteration : 3643
train acc:  0.7421875
train loss:  0.5107983946800232
train gradient:  0.14442325709865844
iteration : 3644
train acc:  0.71875
train loss:  0.539868175983429
train gradient:  0.15461222901734217
iteration : 3645
train acc:  0.734375
train loss:  0.4742013216018677
train gradient:  0.13338363161376232
iteration : 3646
train acc:  0.734375
train loss:  0.507549524307251
train gradient:  0.14917995260160302
iteration : 3647
train acc:  0.7421875
train loss:  0.5089141130447388
train gradient:  0.15366157318645224
iteration : 3648
train acc:  0.78125
train loss:  0.45163199305534363
train gradient:  0.1286295347031725
iteration : 3649
train acc:  0.7265625
train loss:  0.5097859501838684
train gradient:  0.2098907840525555
iteration : 3650
train acc:  0.7890625
train loss:  0.4765523076057434
train gradient:  0.1202803348888542
iteration : 3651
train acc:  0.734375
train loss:  0.49018406867980957
train gradient:  0.17535691911726448
iteration : 3652
train acc:  0.671875
train loss:  0.5841504335403442
train gradient:  0.21628791740594489
iteration : 3653
train acc:  0.734375
train loss:  0.49124690890312195
train gradient:  0.17884895272660445
iteration : 3654
train acc:  0.7421875
train loss:  0.4695320129394531
train gradient:  0.14113048379898993
iteration : 3655
train acc:  0.6875
train loss:  0.5464047193527222
train gradient:  0.19187415570633384
iteration : 3656
train acc:  0.703125
train loss:  0.5417143106460571
train gradient:  0.19220165429974054
iteration : 3657
train acc:  0.703125
train loss:  0.5432446002960205
train gradient:  0.25112841108362793
iteration : 3658
train acc:  0.703125
train loss:  0.5631527304649353
train gradient:  0.17173676302804256
iteration : 3659
train acc:  0.734375
train loss:  0.5222254991531372
train gradient:  0.16358710403038712
iteration : 3660
train acc:  0.703125
train loss:  0.5367554426193237
train gradient:  0.14309938245193565
iteration : 3661
train acc:  0.75
train loss:  0.5061733722686768
train gradient:  0.19814249572813095
iteration : 3662
train acc:  0.78125
train loss:  0.47197777032852173
train gradient:  0.12739221849500776
iteration : 3663
train acc:  0.734375
train loss:  0.5190043449401855
train gradient:  0.16660985001517364
iteration : 3664
train acc:  0.671875
train loss:  0.5523928999900818
train gradient:  0.17092085468842555
iteration : 3665
train acc:  0.75
train loss:  0.4817347228527069
train gradient:  0.14148395090792315
iteration : 3666
train acc:  0.7265625
train loss:  0.5495444536209106
train gradient:  0.15391878008185378
iteration : 3667
train acc:  0.71875
train loss:  0.5499446988105774
train gradient:  0.1613778007188473
iteration : 3668
train acc:  0.6796875
train loss:  0.586887001991272
train gradient:  0.20747724793069844
iteration : 3669
train acc:  0.7265625
train loss:  0.5104632377624512
train gradient:  0.16035010889594292
iteration : 3670
train acc:  0.7265625
train loss:  0.49335357546806335
train gradient:  0.15067996606732437
iteration : 3671
train acc:  0.6640625
train loss:  0.5643503665924072
train gradient:  0.21267263176908133
iteration : 3672
train acc:  0.8046875
train loss:  0.45140841603279114
train gradient:  0.11561466242084971
iteration : 3673
train acc:  0.71875
train loss:  0.5496459007263184
train gradient:  0.19152508590281297
iteration : 3674
train acc:  0.7421875
train loss:  0.5090804696083069
train gradient:  0.13485305166625858
iteration : 3675
train acc:  0.7421875
train loss:  0.49287787079811096
train gradient:  0.12642725674677746
iteration : 3676
train acc:  0.7890625
train loss:  0.49056050181388855
train gradient:  0.18745369186700528
iteration : 3677
train acc:  0.7578125
train loss:  0.5500437021255493
train gradient:  0.14549333937641667
iteration : 3678
train acc:  0.75
train loss:  0.5076897740364075
train gradient:  0.2181368437436762
iteration : 3679
train acc:  0.75
train loss:  0.5121545791625977
train gradient:  0.10391724700525255
iteration : 3680
train acc:  0.7265625
train loss:  0.4926254153251648
train gradient:  0.1568239968517429
iteration : 3681
train acc:  0.7265625
train loss:  0.5565798878669739
train gradient:  0.2482627901520919
iteration : 3682
train acc:  0.703125
train loss:  0.5538938045501709
train gradient:  0.20383890430577223
iteration : 3683
train acc:  0.734375
train loss:  0.519055962562561
train gradient:  0.14655260141255283
iteration : 3684
train acc:  0.78125
train loss:  0.5069978833198547
train gradient:  0.12884611130155788
iteration : 3685
train acc:  0.765625
train loss:  0.5141803026199341
train gradient:  0.15996990565669222
iteration : 3686
train acc:  0.6640625
train loss:  0.6390783786773682
train gradient:  0.26645464037454025
iteration : 3687
train acc:  0.6875
train loss:  0.5224804878234863
train gradient:  0.15512430889987727
iteration : 3688
train acc:  0.7421875
train loss:  0.49444931745529175
train gradient:  0.1408261488698451
iteration : 3689
train acc:  0.734375
train loss:  0.5062156319618225
train gradient:  0.16552611304924897
iteration : 3690
train acc:  0.703125
train loss:  0.5068302154541016
train gradient:  0.14845921142441076
iteration : 3691
train acc:  0.734375
train loss:  0.4866868555545807
train gradient:  0.15320851807011926
iteration : 3692
train acc:  0.703125
train loss:  0.6065874099731445
train gradient:  0.2081882564073598
iteration : 3693
train acc:  0.8046875
train loss:  0.44855087995529175
train gradient:  0.1911504699420755
iteration : 3694
train acc:  0.6953125
train loss:  0.5505911111831665
train gradient:  0.14310002585615217
iteration : 3695
train acc:  0.609375
train loss:  0.6259869933128357
train gradient:  0.19382526438900116
iteration : 3696
train acc:  0.8046875
train loss:  0.4597800672054291
train gradient:  0.11879259960314933
iteration : 3697
train acc:  0.7265625
train loss:  0.5593690276145935
train gradient:  0.14983710718053175
iteration : 3698
train acc:  0.75
train loss:  0.47449633479118347
train gradient:  0.16297129596623788
iteration : 3699
train acc:  0.7265625
train loss:  0.5301647782325745
train gradient:  0.1571105933088556
iteration : 3700
train acc:  0.6171875
train loss:  0.6054302453994751
train gradient:  0.24501062953627878
iteration : 3701
train acc:  0.78125
train loss:  0.5258570909500122
train gradient:  0.1968745543730569
iteration : 3702
train acc:  0.65625
train loss:  0.6263442039489746
train gradient:  0.23777111138351975
iteration : 3703
train acc:  0.6640625
train loss:  0.5519360303878784
train gradient:  0.21828625007942187
iteration : 3704
train acc:  0.6953125
train loss:  0.5739274621009827
train gradient:  0.18453669720574856
iteration : 3705
train acc:  0.6953125
train loss:  0.5252630114555359
train gradient:  0.15395097656393578
iteration : 3706
train acc:  0.6953125
train loss:  0.4993811249732971
train gradient:  0.13193135220846053
iteration : 3707
train acc:  0.6484375
train loss:  0.5633953809738159
train gradient:  0.17194283480163475
iteration : 3708
train acc:  0.75
train loss:  0.4947817027568817
train gradient:  0.13931974291493307
iteration : 3709
train acc:  0.765625
train loss:  0.5155126452445984
train gradient:  0.2035694929152374
iteration : 3710
train acc:  0.671875
train loss:  0.5476588010787964
train gradient:  0.23939978164811582
iteration : 3711
train acc:  0.7265625
train loss:  0.4967981278896332
train gradient:  0.14765964888647615
iteration : 3712
train acc:  0.6640625
train loss:  0.5512412786483765
train gradient:  0.2075160532617108
iteration : 3713
train acc:  0.734375
train loss:  0.49842360615730286
train gradient:  0.15364634274761974
iteration : 3714
train acc:  0.734375
train loss:  0.480728417634964
train gradient:  0.13687920842011697
iteration : 3715
train acc:  0.7421875
train loss:  0.49147647619247437
train gradient:  0.12328834623760031
iteration : 3716
train acc:  0.703125
train loss:  0.5425962209701538
train gradient:  0.19328265290917232
iteration : 3717
train acc:  0.7109375
train loss:  0.542262077331543
train gradient:  0.15414992117288706
iteration : 3718
train acc:  0.734375
train loss:  0.5709704160690308
train gradient:  0.16203278552076394
iteration : 3719
train acc:  0.6484375
train loss:  0.5639165043830872
train gradient:  0.18072827949660625
iteration : 3720
train acc:  0.7109375
train loss:  0.5697479248046875
train gradient:  0.18951053007930818
iteration : 3721
train acc:  0.625
train loss:  0.6095440983772278
train gradient:  0.1692788219530192
iteration : 3722
train acc:  0.625
train loss:  0.6086081266403198
train gradient:  0.24059360514702366
iteration : 3723
train acc:  0.6875
train loss:  0.5714288949966431
train gradient:  0.20860799384551235
iteration : 3724
train acc:  0.6640625
train loss:  0.6149810552597046
train gradient:  0.205782265235258
iteration : 3725
train acc:  0.75
train loss:  0.5047168135643005
train gradient:  0.15637173134484295
iteration : 3726
train acc:  0.7578125
train loss:  0.4837251901626587
train gradient:  0.1436092737065476
iteration : 3727
train acc:  0.7265625
train loss:  0.5214062929153442
train gradient:  0.16981210265291896
iteration : 3728
train acc:  0.78125
train loss:  0.49234306812286377
train gradient:  0.12855684282100813
iteration : 3729
train acc:  0.765625
train loss:  0.5365815162658691
train gradient:  0.20370531528880953
iteration : 3730
train acc:  0.75
train loss:  0.48947250843048096
train gradient:  0.12078098967689484
iteration : 3731
train acc:  0.765625
train loss:  0.4807111620903015
train gradient:  0.1397481888442051
iteration : 3732
train acc:  0.6953125
train loss:  0.5732450485229492
train gradient:  0.1903443620407147
iteration : 3733
train acc:  0.6640625
train loss:  0.5918530225753784
train gradient:  0.18559552888958525
iteration : 3734
train acc:  0.7265625
train loss:  0.4933505654335022
train gradient:  0.1539558059969292
iteration : 3735
train acc:  0.734375
train loss:  0.5114218592643738
train gradient:  0.17400229767514241
iteration : 3736
train acc:  0.6953125
train loss:  0.553840160369873
train gradient:  0.20901578463377324
iteration : 3737
train acc:  0.6953125
train loss:  0.5809717774391174
train gradient:  0.17664736429249467
iteration : 3738
train acc:  0.6953125
train loss:  0.5752366185188293
train gradient:  0.17238526611090071
iteration : 3739
train acc:  0.734375
train loss:  0.5276731252670288
train gradient:  0.16228563503405055
iteration : 3740
train acc:  0.7109375
train loss:  0.5414832830429077
train gradient:  0.158777088524698
iteration : 3741
train acc:  0.78125
train loss:  0.5019927620887756
train gradient:  0.1730126981976228
iteration : 3742
train acc:  0.7109375
train loss:  0.5181591510772705
train gradient:  0.18602447409680412
iteration : 3743
train acc:  0.7265625
train loss:  0.5722413659095764
train gradient:  0.19478919392400135
iteration : 3744
train acc:  0.7421875
train loss:  0.4636368453502655
train gradient:  0.11930509377100684
iteration : 3745
train acc:  0.703125
train loss:  0.5538275241851807
train gradient:  0.186923239441875
iteration : 3746
train acc:  0.6796875
train loss:  0.5807489156723022
train gradient:  0.19307037914236652
iteration : 3747
train acc:  0.7265625
train loss:  0.5217716693878174
train gradient:  0.19018634869930762
iteration : 3748
train acc:  0.6953125
train loss:  0.52952641248703
train gradient:  0.15096625751587053
iteration : 3749
train acc:  0.6953125
train loss:  0.5598440170288086
train gradient:  0.1501921443211063
iteration : 3750
train acc:  0.7578125
train loss:  0.5122904777526855
train gradient:  0.12957319578551013
iteration : 3751
train acc:  0.6875
train loss:  0.5503504276275635
train gradient:  0.20291983019875814
iteration : 3752
train acc:  0.734375
train loss:  0.5052682161331177
train gradient:  0.1567030408324169
iteration : 3753
train acc:  0.7265625
train loss:  0.5120738744735718
train gradient:  0.12844485542175763
iteration : 3754
train acc:  0.734375
train loss:  0.498046338558197
train gradient:  0.10961043690045581
iteration : 3755
train acc:  0.7109375
train loss:  0.5559345483779907
train gradient:  0.14126210065830747
iteration : 3756
train acc:  0.71875
train loss:  0.5408244729042053
train gradient:  0.13707728800133742
iteration : 3757
train acc:  0.6796875
train loss:  0.5866296887397766
train gradient:  0.17500802062507193
iteration : 3758
train acc:  0.71875
train loss:  0.5912675261497498
train gradient:  0.14080520024637516
iteration : 3759
train acc:  0.734375
train loss:  0.5017227530479431
train gradient:  0.16920159539122015
iteration : 3760
train acc:  0.6953125
train loss:  0.6076526045799255
train gradient:  0.16438395199472466
iteration : 3761
train acc:  0.7578125
train loss:  0.4919312596321106
train gradient:  0.13903916850864315
iteration : 3762
train acc:  0.75
train loss:  0.5070087909698486
train gradient:  0.14547857335364167
iteration : 3763
train acc:  0.6875
train loss:  0.575637698173523
train gradient:  0.16032173717518206
iteration : 3764
train acc:  0.7421875
train loss:  0.5547174215316772
train gradient:  0.19507967438208015
iteration : 3765
train acc:  0.734375
train loss:  0.5332372188568115
train gradient:  0.1747417687355249
iteration : 3766
train acc:  0.734375
train loss:  0.48789507150650024
train gradient:  0.1803475896061686
iteration : 3767
train acc:  0.6953125
train loss:  0.5483769178390503
train gradient:  0.2068332074158904
iteration : 3768
train acc:  0.7421875
train loss:  0.48715734481811523
train gradient:  0.12207192006591663
iteration : 3769
train acc:  0.78125
train loss:  0.5110711455345154
train gradient:  0.12341740553651462
iteration : 3770
train acc:  0.640625
train loss:  0.5973538756370544
train gradient:  0.16600388049069348
iteration : 3771
train acc:  0.765625
train loss:  0.48302626609802246
train gradient:  0.1223783855393376
iteration : 3772
train acc:  0.7421875
train loss:  0.5233134031295776
train gradient:  0.162600892439362
iteration : 3773
train acc:  0.703125
train loss:  0.4853878915309906
train gradient:  0.12717474742028523
iteration : 3774
train acc:  0.7421875
train loss:  0.48571541905403137
train gradient:  0.17613054234378916
iteration : 3775
train acc:  0.734375
train loss:  0.5676063299179077
train gradient:  0.16790183465159958
iteration : 3776
train acc:  0.734375
train loss:  0.5689826607704163
train gradient:  0.21828802584343832
iteration : 3777
train acc:  0.75
train loss:  0.5066234469413757
train gradient:  0.21091951886738292
iteration : 3778
train acc:  0.71875
train loss:  0.5177843570709229
train gradient:  0.22134663252872439
iteration : 3779
train acc:  0.71875
train loss:  0.5569627285003662
train gradient:  0.17811134662256833
iteration : 3780
train acc:  0.671875
train loss:  0.6106652021408081
train gradient:  0.1695006391449417
iteration : 3781
train acc:  0.75
train loss:  0.49529916048049927
train gradient:  0.17267961877716154
iteration : 3782
train acc:  0.734375
train loss:  0.5566161870956421
train gradient:  0.17223816815220783
iteration : 3783
train acc:  0.71875
train loss:  0.48635244369506836
train gradient:  0.11692862183719427
iteration : 3784
train acc:  0.6796875
train loss:  0.5412933826446533
train gradient:  0.1375445203517635
iteration : 3785
train acc:  0.6953125
train loss:  0.5349620580673218
train gradient:  0.18409126676424403
iteration : 3786
train acc:  0.7890625
train loss:  0.47691109776496887
train gradient:  0.12434635634780822
iteration : 3787
train acc:  0.75
train loss:  0.5327751636505127
train gradient:  0.16448479488537404
iteration : 3788
train acc:  0.6953125
train loss:  0.5348087549209595
train gradient:  0.2049990107228657
iteration : 3789
train acc:  0.6640625
train loss:  0.5928374528884888
train gradient:  0.2548823611006368
iteration : 3790
train acc:  0.6953125
train loss:  0.590238094329834
train gradient:  0.14320088193406152
iteration : 3791
train acc:  0.7578125
train loss:  0.5055037140846252
train gradient:  0.1967923337199231
iteration : 3792
train acc:  0.7734375
train loss:  0.4908080995082855
train gradient:  0.13363385355182228
iteration : 3793
train acc:  0.6640625
train loss:  0.645871639251709
train gradient:  0.19398943417153466
iteration : 3794
train acc:  0.7421875
train loss:  0.5265905261039734
train gradient:  0.13270989273241768
iteration : 3795
train acc:  0.71875
train loss:  0.49892762303352356
train gradient:  0.2053738227080943
iteration : 3796
train acc:  0.7421875
train loss:  0.5334437489509583
train gradient:  0.15314469003595654
iteration : 3797
train acc:  0.7890625
train loss:  0.4801163077354431
train gradient:  0.1735676044475708
iteration : 3798
train acc:  0.7421875
train loss:  0.5057721138000488
train gradient:  0.1609777459046416
iteration : 3799
train acc:  0.7265625
train loss:  0.5297481417655945
train gradient:  0.16477397367120228
iteration : 3800
train acc:  0.6484375
train loss:  0.6462200880050659
train gradient:  0.24112316393080785
iteration : 3801
train acc:  0.6875
train loss:  0.5744354128837585
train gradient:  0.1654710891062255
iteration : 3802
train acc:  0.640625
train loss:  0.5389475226402283
train gradient:  0.16519673641624097
iteration : 3803
train acc:  0.78125
train loss:  0.47185444831848145
train gradient:  0.14385433176391677
iteration : 3804
train acc:  0.7578125
train loss:  0.44994062185287476
train gradient:  0.14482823563192038
iteration : 3805
train acc:  0.7421875
train loss:  0.4798664450645447
train gradient:  0.13145639778671261
iteration : 3806
train acc:  0.6796875
train loss:  0.560935378074646
train gradient:  0.14789683396231534
iteration : 3807
train acc:  0.7109375
train loss:  0.552332878112793
train gradient:  0.1634832495998076
iteration : 3808
train acc:  0.6796875
train loss:  0.5327841639518738
train gradient:  0.14566727518411504
iteration : 3809
train acc:  0.7421875
train loss:  0.4924022853374481
train gradient:  0.12090762141303654
iteration : 3810
train acc:  0.703125
train loss:  0.5445375442504883
train gradient:  0.1584210978039367
iteration : 3811
train acc:  0.765625
train loss:  0.5045644044876099
train gradient:  0.1526584573209959
iteration : 3812
train acc:  0.71875
train loss:  0.535393238067627
train gradient:  0.12103395294463944
iteration : 3813
train acc:  0.75
train loss:  0.4871833324432373
train gradient:  0.1405400132823834
iteration : 3814
train acc:  0.765625
train loss:  0.46582555770874023
train gradient:  0.12884712420570366
iteration : 3815
train acc:  0.7109375
train loss:  0.5365457534790039
train gradient:  0.16038078353818985
iteration : 3816
train acc:  0.7109375
train loss:  0.5686136484146118
train gradient:  0.1603089359520942
iteration : 3817
train acc:  0.671875
train loss:  0.5829031467437744
train gradient:  0.1786912871634502
iteration : 3818
train acc:  0.640625
train loss:  0.5728144645690918
train gradient:  0.17144119692059145
iteration : 3819
train acc:  0.703125
train loss:  0.546539843082428
train gradient:  0.21004852048926395
iteration : 3820
train acc:  0.6875
train loss:  0.536465048789978
train gradient:  0.15392282602307
iteration : 3821
train acc:  0.6875
train loss:  0.600698709487915
train gradient:  0.20053925860352106
iteration : 3822
train acc:  0.734375
train loss:  0.5248704552650452
train gradient:  0.1990516094697239
iteration : 3823
train acc:  0.71875
train loss:  0.559354841709137
train gradient:  0.1790205732325476
iteration : 3824
train acc:  0.765625
train loss:  0.49532806873321533
train gradient:  0.15129222199667264
iteration : 3825
train acc:  0.71875
train loss:  0.5249515175819397
train gradient:  0.14818745300706027
iteration : 3826
train acc:  0.7109375
train loss:  0.5626664757728577
train gradient:  0.13914540375310222
iteration : 3827
train acc:  0.734375
train loss:  0.517927885055542
train gradient:  0.17564690671077532
iteration : 3828
train acc:  0.7578125
train loss:  0.5126168727874756
train gradient:  0.15440961839705697
iteration : 3829
train acc:  0.6953125
train loss:  0.5188974738121033
train gradient:  0.13244968872023996
iteration : 3830
train acc:  0.75
train loss:  0.5127251148223877
train gradient:  0.14398984702552775
iteration : 3831
train acc:  0.765625
train loss:  0.46828633546829224
train gradient:  0.12899777341869287
iteration : 3832
train acc:  0.765625
train loss:  0.5182637572288513
train gradient:  0.1464419992724193
iteration : 3833
train acc:  0.6640625
train loss:  0.5661999583244324
train gradient:  0.1574865798914113
iteration : 3834
train acc:  0.71875
train loss:  0.518656849861145
train gradient:  0.15199485359849318
iteration : 3835
train acc:  0.640625
train loss:  0.592214047908783
train gradient:  0.16306920999265875
iteration : 3836
train acc:  0.734375
train loss:  0.5315415263175964
train gradient:  0.1879523536505477
iteration : 3837
train acc:  0.6953125
train loss:  0.558838963508606
train gradient:  0.16207475873064842
iteration : 3838
train acc:  0.6875
train loss:  0.5499255657196045
train gradient:  0.17862275494591956
iteration : 3839
train acc:  0.7890625
train loss:  0.5134845972061157
train gradient:  0.15926901213114375
iteration : 3840
train acc:  0.71875
train loss:  0.5179370045661926
train gradient:  0.14457942773432803
iteration : 3841
train acc:  0.75
train loss:  0.4925447106361389
train gradient:  0.16903508960215308
iteration : 3842
train acc:  0.6953125
train loss:  0.5461543798446655
train gradient:  0.15529970887964703
iteration : 3843
train acc:  0.734375
train loss:  0.48312243819236755
train gradient:  0.14884996211168883
iteration : 3844
train acc:  0.6953125
train loss:  0.558322548866272
train gradient:  0.17185597503781316
iteration : 3845
train acc:  0.65625
train loss:  0.5596790313720703
train gradient:  0.1765545374210613
iteration : 3846
train acc:  0.6640625
train loss:  0.5664577484130859
train gradient:  0.14866013061906308
iteration : 3847
train acc:  0.65625
train loss:  0.6022064685821533
train gradient:  0.1801912817937087
iteration : 3848
train acc:  0.7265625
train loss:  0.5110703706741333
train gradient:  0.14854830888794912
iteration : 3849
train acc:  0.703125
train loss:  0.5556902885437012
train gradient:  0.14826902531861275
iteration : 3850
train acc:  0.734375
train loss:  0.5360350608825684
train gradient:  0.16666890224438813
iteration : 3851
train acc:  0.703125
train loss:  0.5189254283905029
train gradient:  0.1343432537531695
iteration : 3852
train acc:  0.7578125
train loss:  0.5120188593864441
train gradient:  0.1432492766218156
iteration : 3853
train acc:  0.78125
train loss:  0.4954293966293335
train gradient:  0.17812201645658313
iteration : 3854
train acc:  0.734375
train loss:  0.5038766860961914
train gradient:  0.1203621891049124
iteration : 3855
train acc:  0.7109375
train loss:  0.5293999314308167
train gradient:  0.15661667962911946
iteration : 3856
train acc:  0.71875
train loss:  0.48990219831466675
train gradient:  0.14433481816106492
iteration : 3857
train acc:  0.734375
train loss:  0.48278117179870605
train gradient:  0.14096824132543817
iteration : 3858
train acc:  0.6875
train loss:  0.5392324328422546
train gradient:  0.19583150869469013
iteration : 3859
train acc:  0.6484375
train loss:  0.5654668807983398
train gradient:  0.15070655014121523
iteration : 3860
train acc:  0.75
train loss:  0.4914408326148987
train gradient:  0.12922006085474755
iteration : 3861
train acc:  0.7109375
train loss:  0.5090963244438171
train gradient:  0.1301888711651211
iteration : 3862
train acc:  0.75
train loss:  0.48491328954696655
train gradient:  0.1318320601283949
iteration : 3863
train acc:  0.78125
train loss:  0.48933953046798706
train gradient:  0.11228981556530751
iteration : 3864
train acc:  0.671875
train loss:  0.6102016568183899
train gradient:  0.16763225600971066
iteration : 3865
train acc:  0.7890625
train loss:  0.4379947781562805
train gradient:  0.10583325042455757
iteration : 3866
train acc:  0.7578125
train loss:  0.48008084297180176
train gradient:  0.15753840913965417
iteration : 3867
train acc:  0.765625
train loss:  0.47180381417274475
train gradient:  0.1429326095497632
iteration : 3868
train acc:  0.765625
train loss:  0.4401477575302124
train gradient:  0.17185377359237677
iteration : 3869
train acc:  0.7421875
train loss:  0.5324071645736694
train gradient:  0.1485896781594837
iteration : 3870
train acc:  0.7265625
train loss:  0.5106667876243591
train gradient:  0.14071723871765995
iteration : 3871
train acc:  0.7421875
train loss:  0.5320743918418884
train gradient:  0.2235072262976917
iteration : 3872
train acc:  0.703125
train loss:  0.5392840504646301
train gradient:  0.15963550051317485
iteration : 3873
train acc:  0.7734375
train loss:  0.4726985692977905
train gradient:  0.10981457083159202
iteration : 3874
train acc:  0.7421875
train loss:  0.5310878157615662
train gradient:  0.19896315633725847
iteration : 3875
train acc:  0.78125
train loss:  0.4495980143547058
train gradient:  0.14313066213809228
iteration : 3876
train acc:  0.7890625
train loss:  0.46878838539123535
train gradient:  0.1197285081093742
iteration : 3877
train acc:  0.7265625
train loss:  0.4757464826107025
train gradient:  0.1734871329837235
iteration : 3878
train acc:  0.734375
train loss:  0.5360078811645508
train gradient:  0.1516880926349185
iteration : 3879
train acc:  0.578125
train loss:  0.6865224838256836
train gradient:  0.2554442053110338
iteration : 3880
train acc:  0.6640625
train loss:  0.5858991146087646
train gradient:  0.13872622433133297
iteration : 3881
train acc:  0.703125
train loss:  0.5023928880691528
train gradient:  0.1284460480024712
iteration : 3882
train acc:  0.6796875
train loss:  0.6241440773010254
train gradient:  0.24845588635118704
iteration : 3883
train acc:  0.703125
train loss:  0.5247863531112671
train gradient:  0.15396494051632936
iteration : 3884
train acc:  0.65625
train loss:  0.5825979709625244
train gradient:  0.20689688828965097
iteration : 3885
train acc:  0.734375
train loss:  0.5290251970291138
train gradient:  0.20547920293438698
iteration : 3886
train acc:  0.78125
train loss:  0.4392295479774475
train gradient:  0.1562215226206915
iteration : 3887
train acc:  0.71875
train loss:  0.5056635141372681
train gradient:  0.1546937782246982
iteration : 3888
train acc:  0.6875
train loss:  0.536865234375
train gradient:  0.13932588480606062
iteration : 3889
train acc:  0.671875
train loss:  0.5575817823410034
train gradient:  0.1773157114553869
iteration : 3890
train acc:  0.734375
train loss:  0.544495165348053
train gradient:  0.18459404459710055
iteration : 3891
train acc:  0.734375
train loss:  0.5395106077194214
train gradient:  0.14185314294820422
iteration : 3892
train acc:  0.7421875
train loss:  0.5277562141418457
train gradient:  0.19870799914363196
iteration : 3893
train acc:  0.6328125
train loss:  0.5861833691596985
train gradient:  0.16054581193734377
iteration : 3894
train acc:  0.671875
train loss:  0.5245032906532288
train gradient:  0.11787184303850903
iteration : 3895
train acc:  0.75
train loss:  0.49976471066474915
train gradient:  0.15107297166552563
iteration : 3896
train acc:  0.7578125
train loss:  0.5022339224815369
train gradient:  0.16115950720139352
iteration : 3897
train acc:  0.6484375
train loss:  0.6012434363365173
train gradient:  0.19240261251342533
iteration : 3898
train acc:  0.75
train loss:  0.5212999582290649
train gradient:  0.18843800011986256
iteration : 3899
train acc:  0.75
train loss:  0.49478551745414734
train gradient:  0.15646488304421824
iteration : 3900
train acc:  0.7890625
train loss:  0.4920397102832794
train gradient:  0.1853419481629719
iteration : 3901
train acc:  0.6796875
train loss:  0.5446863770484924
train gradient:  0.17510963894881904
iteration : 3902
train acc:  0.6484375
train loss:  0.568152904510498
train gradient:  0.17280018798140437
iteration : 3903
train acc:  0.65625
train loss:  0.5365291833877563
train gradient:  0.14089119545827233
iteration : 3904
train acc:  0.7265625
train loss:  0.5048985481262207
train gradient:  0.15840372974138853
iteration : 3905
train acc:  0.7109375
train loss:  0.5008800029754639
train gradient:  0.22274249491249193
iteration : 3906
train acc:  0.6953125
train loss:  0.5311548113822937
train gradient:  0.1630447365822073
iteration : 3907
train acc:  0.6875
train loss:  0.5670216083526611
train gradient:  0.2518345378408338
iteration : 3908
train acc:  0.6953125
train loss:  0.5358543395996094
train gradient:  0.17356986201904395
iteration : 3909
train acc:  0.7421875
train loss:  0.4828759431838989
train gradient:  0.1544796675505824
iteration : 3910
train acc:  0.6796875
train loss:  0.5537091493606567
train gradient:  0.14504486555312168
iteration : 3911
train acc:  0.734375
train loss:  0.4797261357307434
train gradient:  0.10558176357635474
iteration : 3912
train acc:  0.7890625
train loss:  0.4499104917049408
train gradient:  0.1324679523787832
iteration : 3913
train acc:  0.7578125
train loss:  0.4831455945968628
train gradient:  0.13781338954565436
iteration : 3914
train acc:  0.7734375
train loss:  0.477605938911438
train gradient:  0.1603130961236317
iteration : 3915
train acc:  0.6953125
train loss:  0.5115346908569336
train gradient:  0.20446674366178808
iteration : 3916
train acc:  0.734375
train loss:  0.513995349407196
train gradient:  0.17858045010339613
iteration : 3917
train acc:  0.7109375
train loss:  0.5129618048667908
train gradient:  0.14718361853610606
iteration : 3918
train acc:  0.6796875
train loss:  0.549476683139801
train gradient:  0.1850335602910543
iteration : 3919
train acc:  0.6953125
train loss:  0.5687736868858337
train gradient:  0.2324930164384153
iteration : 3920
train acc:  0.703125
train loss:  0.5230063199996948
train gradient:  0.16880241835537113
iteration : 3921
train acc:  0.71875
train loss:  0.49517926573753357
train gradient:  0.14275315596836036
iteration : 3922
train acc:  0.7109375
train loss:  0.551642894744873
train gradient:  0.13903501330627527
iteration : 3923
train acc:  0.7109375
train loss:  0.5414910316467285
train gradient:  0.17965550851250495
iteration : 3924
train acc:  0.7421875
train loss:  0.47238361835479736
train gradient:  0.12533831749927826
iteration : 3925
train acc:  0.65625
train loss:  0.6259572505950928
train gradient:  0.1828782988388085
iteration : 3926
train acc:  0.6640625
train loss:  0.5861219167709351
train gradient:  0.16510096850942135
iteration : 3927
train acc:  0.7578125
train loss:  0.506367564201355
train gradient:  0.13081777676224743
iteration : 3928
train acc:  0.7265625
train loss:  0.5735719203948975
train gradient:  0.1878143612697183
iteration : 3929
train acc:  0.71875
train loss:  0.5085068941116333
train gradient:  0.14144844939988893
iteration : 3930
train acc:  0.7421875
train loss:  0.5403774380683899
train gradient:  0.14571476460425606
iteration : 3931
train acc:  0.7578125
train loss:  0.4724183678627014
train gradient:  0.13984184589756288
iteration : 3932
train acc:  0.7265625
train loss:  0.5334436893463135
train gradient:  0.13770521503933017
iteration : 3933
train acc:  0.78125
train loss:  0.45157763361930847
train gradient:  0.153003593808173
iteration : 3934
train acc:  0.734375
train loss:  0.5560177564620972
train gradient:  0.16431212557072
iteration : 3935
train acc:  0.6875
train loss:  0.5299879312515259
train gradient:  0.15142721791946132
iteration : 3936
train acc:  0.734375
train loss:  0.4656810164451599
train gradient:  0.1328072473712061
iteration : 3937
train acc:  0.796875
train loss:  0.4760589599609375
train gradient:  0.11362495303821113
iteration : 3938
train acc:  0.7578125
train loss:  0.5259596705436707
train gradient:  0.13732070877150346
iteration : 3939
train acc:  0.75
train loss:  0.4458238482475281
train gradient:  0.1162897235073704
iteration : 3940
train acc:  0.75
train loss:  0.5275394320487976
train gradient:  0.13540713354118933
iteration : 3941
train acc:  0.71875
train loss:  0.5201091766357422
train gradient:  0.14786899934063621
iteration : 3942
train acc:  0.7421875
train loss:  0.46736761927604675
train gradient:  0.14329833350594695
iteration : 3943
train acc:  0.765625
train loss:  0.476948082447052
train gradient:  0.12902768970077286
iteration : 3944
train acc:  0.765625
train loss:  0.5152823328971863
train gradient:  0.16774380015819784
iteration : 3945
train acc:  0.65625
train loss:  0.5876140594482422
train gradient:  0.1750669912885302
iteration : 3946
train acc:  0.6953125
train loss:  0.5099183320999146
train gradient:  0.1719893783658486
iteration : 3947
train acc:  0.7578125
train loss:  0.49902957677841187
train gradient:  0.19734163778199743
iteration : 3948
train acc:  0.765625
train loss:  0.5147122740745544
train gradient:  0.14673969912051743
iteration : 3949
train acc:  0.7578125
train loss:  0.4683486223220825
train gradient:  0.13607054342779545
iteration : 3950
train acc:  0.65625
train loss:  0.5973111391067505
train gradient:  0.20337983459531062
iteration : 3951
train acc:  0.7421875
train loss:  0.49011093378067017
train gradient:  0.14553677589268912
iteration : 3952
train acc:  0.765625
train loss:  0.48899269104003906
train gradient:  0.14289556021500002
iteration : 3953
train acc:  0.7421875
train loss:  0.5217413902282715
train gradient:  0.16719298727673604
iteration : 3954
train acc:  0.734375
train loss:  0.5271375179290771
train gradient:  0.21411641646116633
iteration : 3955
train acc:  0.78125
train loss:  0.4483353793621063
train gradient:  0.14708578892692287
iteration : 3956
train acc:  0.671875
train loss:  0.5414239168167114
train gradient:  0.1579500079574853
iteration : 3957
train acc:  0.7421875
train loss:  0.5279770493507385
train gradient:  0.1485377237472828
iteration : 3958
train acc:  0.78125
train loss:  0.46316686272621155
train gradient:  0.12091173386051159
iteration : 3959
train acc:  0.765625
train loss:  0.5075773596763611
train gradient:  0.13566609231947732
iteration : 3960
train acc:  0.703125
train loss:  0.5270782709121704
train gradient:  0.17618624044815398
iteration : 3961
train acc:  0.75
train loss:  0.5367185473442078
train gradient:  0.1563352243698692
iteration : 3962
train acc:  0.7421875
train loss:  0.4850740432739258
train gradient:  0.12597831511249585
iteration : 3963
train acc:  0.734375
train loss:  0.5489363670349121
train gradient:  0.17364514781054835
iteration : 3964
train acc:  0.7109375
train loss:  0.5274064540863037
train gradient:  0.18703080668781294
iteration : 3965
train acc:  0.6796875
train loss:  0.5680602192878723
train gradient:  0.18673494734243432
iteration : 3966
train acc:  0.7421875
train loss:  0.5206360816955566
train gradient:  0.17097434465572195
iteration : 3967
train acc:  0.7421875
train loss:  0.5299971699714661
train gradient:  0.1739086853274125
iteration : 3968
train acc:  0.734375
train loss:  0.5567182898521423
train gradient:  0.17802396075653298
iteration : 3969
train acc:  0.7265625
train loss:  0.5115878582000732
train gradient:  0.12639224875655058
iteration : 3970
train acc:  0.75
train loss:  0.4765913188457489
train gradient:  0.1753830493698632
iteration : 3971
train acc:  0.765625
train loss:  0.4498746991157532
train gradient:  0.14066408135007277
iteration : 3972
train acc:  0.6875
train loss:  0.5649062395095825
train gradient:  0.1615755608125297
iteration : 3973
train acc:  0.7265625
train loss:  0.5114942789077759
train gradient:  0.1659733509317821
iteration : 3974
train acc:  0.7265625
train loss:  0.5377659797668457
train gradient:  0.165823353071915
iteration : 3975
train acc:  0.6875
train loss:  0.5705183744430542
train gradient:  0.18534092028907123
iteration : 3976
train acc:  0.7109375
train loss:  0.5207407474517822
train gradient:  0.1703685874294129
iteration : 3977
train acc:  0.6953125
train loss:  0.548431396484375
train gradient:  0.15458995492222036
iteration : 3978
train acc:  0.71875
train loss:  0.5258403420448303
train gradient:  0.13010111020731563
iteration : 3979
train acc:  0.765625
train loss:  0.4866069555282593
train gradient:  0.1679180069735122
iteration : 3980
train acc:  0.6953125
train loss:  0.5311951637268066
train gradient:  0.14582228669035258
iteration : 3981
train acc:  0.78125
train loss:  0.5058506727218628
train gradient:  0.17155866457836022
iteration : 3982
train acc:  0.7734375
train loss:  0.43885302543640137
train gradient:  0.15503061888640812
iteration : 3983
train acc:  0.6484375
train loss:  0.6250264048576355
train gradient:  0.2737935892561024
iteration : 3984
train acc:  0.640625
train loss:  0.5876809358596802
train gradient:  0.20731163667221736
iteration : 3985
train acc:  0.78125
train loss:  0.4841849207878113
train gradient:  0.12791113033555362
iteration : 3986
train acc:  0.7421875
train loss:  0.4751937985420227
train gradient:  0.193364270831607
iteration : 3987
train acc:  0.6953125
train loss:  0.5388809442520142
train gradient:  0.18880914938786136
iteration : 3988
train acc:  0.6953125
train loss:  0.5332366824150085
train gradient:  0.15569593503233334
iteration : 3989
train acc:  0.78125
train loss:  0.5093684196472168
train gradient:  0.15643690873271626
iteration : 3990
train acc:  0.734375
train loss:  0.5054399967193604
train gradient:  0.137378136279591
iteration : 3991
train acc:  0.7421875
train loss:  0.4866783618927002
train gradient:  0.17850934385572081
iteration : 3992
train acc:  0.6171875
train loss:  0.6639105081558228
train gradient:  0.20629079548568313
iteration : 3993
train acc:  0.765625
train loss:  0.47692805528640747
train gradient:  0.1398704595922215
iteration : 3994
train acc:  0.7109375
train loss:  0.5286271572113037
train gradient:  0.16902715288884484
iteration : 3995
train acc:  0.8203125
train loss:  0.48154789209365845
train gradient:  0.14566018945111203
iteration : 3996
train acc:  0.703125
train loss:  0.5200756788253784
train gradient:  0.1382165217996547
iteration : 3997
train acc:  0.7890625
train loss:  0.5123517513275146
train gradient:  0.15175037145943232
iteration : 3998
train acc:  0.7265625
train loss:  0.5173196792602539
train gradient:  0.16707540395704734
iteration : 3999
train acc:  0.6796875
train loss:  0.5960806012153625
train gradient:  0.2010931324881436
iteration : 4000
train acc:  0.6953125
train loss:  0.4935832619667053
train gradient:  0.16792925670647113
iteration : 4001
train acc:  0.6953125
train loss:  0.5575249195098877
train gradient:  0.16868622563038027
iteration : 4002
train acc:  0.6875
train loss:  0.5900734663009644
train gradient:  0.16038143007540168
iteration : 4003
train acc:  0.703125
train loss:  0.5402978658676147
train gradient:  0.22077813319743572
iteration : 4004
train acc:  0.7265625
train loss:  0.5232466459274292
train gradient:  0.1485768323738742
iteration : 4005
train acc:  0.734375
train loss:  0.49092599749565125
train gradient:  0.1412111283119064
iteration : 4006
train acc:  0.671875
train loss:  0.5701053738594055
train gradient:  0.20206464752974448
iteration : 4007
train acc:  0.78125
train loss:  0.4679142236709595
train gradient:  0.1280343079134414
iteration : 4008
train acc:  0.6953125
train loss:  0.5635090470314026
train gradient:  0.14715369835243736
iteration : 4009
train acc:  0.78125
train loss:  0.5035463571548462
train gradient:  0.1234142201167468
iteration : 4010
train acc:  0.75
train loss:  0.5013902187347412
train gradient:  0.17287281827365175
iteration : 4011
train acc:  0.71875
train loss:  0.5206965208053589
train gradient:  0.15192896461022234
iteration : 4012
train acc:  0.734375
train loss:  0.5716805458068848
train gradient:  0.15242341805264972
iteration : 4013
train acc:  0.75
train loss:  0.5150482058525085
train gradient:  0.15780091380975242
iteration : 4014
train acc:  0.7109375
train loss:  0.5333585143089294
train gradient:  0.15869475611738443
iteration : 4015
train acc:  0.75
train loss:  0.49795472621917725
train gradient:  0.13660436428257827
iteration : 4016
train acc:  0.71875
train loss:  0.5410124063491821
train gradient:  0.17879150618420311
iteration : 4017
train acc:  0.734375
train loss:  0.5062953233718872
train gradient:  0.1278705040559775
iteration : 4018
train acc:  0.65625
train loss:  0.5745002627372742
train gradient:  0.22798653610901912
iteration : 4019
train acc:  0.71875
train loss:  0.5523124933242798
train gradient:  0.19362622992659384
iteration : 4020
train acc:  0.6640625
train loss:  0.6341829299926758
train gradient:  0.20183272625574444
iteration : 4021
train acc:  0.75
train loss:  0.5074044466018677
train gradient:  0.18813428133073118
iteration : 4022
train acc:  0.71875
train loss:  0.5073333382606506
train gradient:  0.19442062887208092
iteration : 4023
train acc:  0.7265625
train loss:  0.585720419883728
train gradient:  0.2148155380872549
iteration : 4024
train acc:  0.7421875
train loss:  0.5282547473907471
train gradient:  0.18717565188421248
iteration : 4025
train acc:  0.75
train loss:  0.49575716257095337
train gradient:  0.11522855084384778
iteration : 4026
train acc:  0.78125
train loss:  0.48111021518707275
train gradient:  0.14189750944333573
iteration : 4027
train acc:  0.7421875
train loss:  0.5154985785484314
train gradient:  0.1656537040851381
iteration : 4028
train acc:  0.6875
train loss:  0.54508376121521
train gradient:  0.16594753922913125
iteration : 4029
train acc:  0.703125
train loss:  0.5066218376159668
train gradient:  0.15594136613107
iteration : 4030
train acc:  0.6796875
train loss:  0.5768746137619019
train gradient:  0.18257675978963378
iteration : 4031
train acc:  0.7109375
train loss:  0.5088021755218506
train gradient:  0.18662702081030552
iteration : 4032
train acc:  0.7578125
train loss:  0.477115273475647
train gradient:  0.1330675739790964
iteration : 4033
train acc:  0.6171875
train loss:  0.6336742639541626
train gradient:  0.2350296582094515
iteration : 4034
train acc:  0.640625
train loss:  0.5977541208267212
train gradient:  0.15552234114418667
iteration : 4035
train acc:  0.734375
train loss:  0.5447439551353455
train gradient:  0.13809931067060716
iteration : 4036
train acc:  0.7265625
train loss:  0.5566734075546265
train gradient:  0.2135232050546682
iteration : 4037
train acc:  0.7265625
train loss:  0.4977168142795563
train gradient:  0.13231076835716798
iteration : 4038
train acc:  0.7109375
train loss:  0.5104988217353821
train gradient:  0.12328040269496651
iteration : 4039
train acc:  0.6875
train loss:  0.5318174958229065
train gradient:  0.13117663986942302
iteration : 4040
train acc:  0.7109375
train loss:  0.5921581983566284
train gradient:  0.20700252977292566
iteration : 4041
train acc:  0.7109375
train loss:  0.48965173959732056
train gradient:  0.15037478248461944
iteration : 4042
train acc:  0.796875
train loss:  0.4708152413368225
train gradient:  0.1560325668979755
iteration : 4043
train acc:  0.6796875
train loss:  0.5414019227027893
train gradient:  0.1455752412543146
iteration : 4044
train acc:  0.7421875
train loss:  0.4969889521598816
train gradient:  0.1339521531988424
iteration : 4045
train acc:  0.6640625
train loss:  0.5660857558250427
train gradient:  0.17130107220478968
iteration : 4046
train acc:  0.7421875
train loss:  0.5104259252548218
train gradient:  0.14840053101062428
iteration : 4047
train acc:  0.6875
train loss:  0.532071053981781
train gradient:  0.18398767177510017
iteration : 4048
train acc:  0.671875
train loss:  0.5527386665344238
train gradient:  0.14573683944586907
iteration : 4049
train acc:  0.7578125
train loss:  0.4918696880340576
train gradient:  0.1432550654348025
iteration : 4050
train acc:  0.7578125
train loss:  0.5231215953826904
train gradient:  0.17434650454470713
iteration : 4051
train acc:  0.703125
train loss:  0.5855308771133423
train gradient:  0.16162947485711626
iteration : 4052
train acc:  0.71875
train loss:  0.5623729228973389
train gradient:  0.16346316675130568
iteration : 4053
train acc:  0.734375
train loss:  0.4756866693496704
train gradient:  0.14842866274862745
iteration : 4054
train acc:  0.703125
train loss:  0.534501314163208
train gradient:  0.13659370472813748
iteration : 4055
train acc:  0.6953125
train loss:  0.576798677444458
train gradient:  0.17092448337529895
iteration : 4056
train acc:  0.7109375
train loss:  0.5501362085342407
train gradient:  0.1496536108549358
iteration : 4057
train acc:  0.6875
train loss:  0.5301704406738281
train gradient:  0.19487699588235158
iteration : 4058
train acc:  0.6640625
train loss:  0.5700777769088745
train gradient:  0.17748039947675162
iteration : 4059
train acc:  0.703125
train loss:  0.5355159044265747
train gradient:  0.23712586420999365
iteration : 4060
train acc:  0.75
train loss:  0.5153726935386658
train gradient:  0.1272862120339746
iteration : 4061
train acc:  0.734375
train loss:  0.5114195942878723
train gradient:  0.20579172141333446
iteration : 4062
train acc:  0.75
train loss:  0.503244936466217
train gradient:  0.1698268622143692
iteration : 4063
train acc:  0.75
train loss:  0.5219104886054993
train gradient:  0.14162586985667697
iteration : 4064
train acc:  0.703125
train loss:  0.529268741607666
train gradient:  0.14926117073276973
iteration : 4065
train acc:  0.75
train loss:  0.5257151126861572
train gradient:  0.17365445003622754
iteration : 4066
train acc:  0.703125
train loss:  0.5464347004890442
train gradient:  0.17783687437549717
iteration : 4067
train acc:  0.7265625
train loss:  0.5191793441772461
train gradient:  0.17154485338623848
iteration : 4068
train acc:  0.734375
train loss:  0.5654546022415161
train gradient:  0.15010640201491793
iteration : 4069
train acc:  0.765625
train loss:  0.493866890668869
train gradient:  0.17018722987661722
iteration : 4070
train acc:  0.734375
train loss:  0.5573565363883972
train gradient:  0.1718813071221461
iteration : 4071
train acc:  0.7265625
train loss:  0.5084891319274902
train gradient:  0.1790874950471975
iteration : 4072
train acc:  0.703125
train loss:  0.5913752913475037
train gradient:  0.20914772916087412
iteration : 4073
train acc:  0.7578125
train loss:  0.5103927850723267
train gradient:  0.13345439242873652
iteration : 4074
train acc:  0.6953125
train loss:  0.5286353230476379
train gradient:  0.16204967996790187
iteration : 4075
train acc:  0.75
train loss:  0.467123806476593
train gradient:  0.12704365862862926
iteration : 4076
train acc:  0.6640625
train loss:  0.5702863931655884
train gradient:  0.17643453289942132
iteration : 4077
train acc:  0.7265625
train loss:  0.5257474184036255
train gradient:  0.18596331620085665
iteration : 4078
train acc:  0.7109375
train loss:  0.5165077447891235
train gradient:  0.1360412393883126
iteration : 4079
train acc:  0.7265625
train loss:  0.5465754270553589
train gradient:  0.2109599166549213
iteration : 4080
train acc:  0.75
train loss:  0.487079918384552
train gradient:  0.13210687952368563
iteration : 4081
train acc:  0.703125
train loss:  0.5538674592971802
train gradient:  0.18624590495693472
iteration : 4082
train acc:  0.75
train loss:  0.5094437599182129
train gradient:  0.14768818779519327
iteration : 4083
train acc:  0.7578125
train loss:  0.5255576372146606
train gradient:  0.15015555757293864
iteration : 4084
train acc:  0.71875
train loss:  0.5740169882774353
train gradient:  0.18115237988708613
iteration : 4085
train acc:  0.6796875
train loss:  0.5750046968460083
train gradient:  0.15831128638581146
iteration : 4086
train acc:  0.671875
train loss:  0.5624619722366333
train gradient:  0.13701187368057874
iteration : 4087
train acc:  0.671875
train loss:  0.5491414070129395
train gradient:  0.1829448193305699
iteration : 4088
train acc:  0.765625
train loss:  0.47706180810928345
train gradient:  0.20439380048688283
iteration : 4089
train acc:  0.65625
train loss:  0.5350335240364075
train gradient:  0.1760169318084921
iteration : 4090
train acc:  0.7578125
train loss:  0.4944186806678772
train gradient:  0.16949868455945677
iteration : 4091
train acc:  0.7578125
train loss:  0.5186862349510193
train gradient:  0.13229859402938016
iteration : 4092
train acc:  0.7109375
train loss:  0.5041525959968567
train gradient:  0.13760122202932779
iteration : 4093
train acc:  0.703125
train loss:  0.5209788680076599
train gradient:  0.13081606599325585
iteration : 4094
train acc:  0.7265625
train loss:  0.5288048982620239
train gradient:  0.13629109017733304
iteration : 4095
train acc:  0.6796875
train loss:  0.5205017328262329
train gradient:  0.1504227669891689
iteration : 4096
train acc:  0.65625
train loss:  0.6187301874160767
train gradient:  0.23806586939713792
iteration : 4097
train acc:  0.796875
train loss:  0.5115422606468201
train gradient:  0.16531006056315412
iteration : 4098
train acc:  0.7109375
train loss:  0.48917320370674133
train gradient:  0.10668559070603988
iteration : 4099
train acc:  0.671875
train loss:  0.5336228013038635
train gradient:  0.14494366702495265
iteration : 4100
train acc:  0.7734375
train loss:  0.4733063280582428
train gradient:  0.12312999456119521
iteration : 4101
train acc:  0.734375
train loss:  0.5249360203742981
train gradient:  0.12993555217588065
iteration : 4102
train acc:  0.703125
train loss:  0.554428219795227
train gradient:  0.21997474286117316
iteration : 4103
train acc:  0.65625
train loss:  0.6134868264198303
train gradient:  0.17064465636249493
iteration : 4104
train acc:  0.7421875
train loss:  0.487216055393219
train gradient:  0.13888994789091652
iteration : 4105
train acc:  0.640625
train loss:  0.5755259990692139
train gradient:  0.20867649847155662
iteration : 4106
train acc:  0.6796875
train loss:  0.549763560295105
train gradient:  0.16779526795885324
iteration : 4107
train acc:  0.734375
train loss:  0.5186773538589478
train gradient:  0.21693754550128516
iteration : 4108
train acc:  0.7578125
train loss:  0.48022428154945374
train gradient:  0.13892549941899396
iteration : 4109
train acc:  0.7578125
train loss:  0.5131227970123291
train gradient:  0.14526647690983785
iteration : 4110
train acc:  0.796875
train loss:  0.49581587314605713
train gradient:  0.1678777845737544
iteration : 4111
train acc:  0.7265625
train loss:  0.5138716697692871
train gradient:  0.14292350340738696
iteration : 4112
train acc:  0.7265625
train loss:  0.48893284797668457
train gradient:  0.13432001843754385
iteration : 4113
train acc:  0.75
train loss:  0.47079405188560486
train gradient:  0.15410535773513467
iteration : 4114
train acc:  0.7421875
train loss:  0.5051630735397339
train gradient:  0.14049570383002466
iteration : 4115
train acc:  0.7890625
train loss:  0.44889870285987854
train gradient:  0.13299147735949768
iteration : 4116
train acc:  0.8125
train loss:  0.463396817445755
train gradient:  0.1424448398411177
iteration : 4117
train acc:  0.7421875
train loss:  0.521582841873169
train gradient:  0.1556332478640629
iteration : 4118
train acc:  0.703125
train loss:  0.5800573825836182
train gradient:  0.18554051967078372
iteration : 4119
train acc:  0.734375
train loss:  0.5013426542282104
train gradient:  0.13777391008548517
iteration : 4120
train acc:  0.671875
train loss:  0.6065354347229004
train gradient:  0.1702932218535652
iteration : 4121
train acc:  0.7109375
train loss:  0.5626271367073059
train gradient:  0.19074973104787651
iteration : 4122
train acc:  0.7421875
train loss:  0.4859960377216339
train gradient:  0.11064027512460244
iteration : 4123
train acc:  0.75
train loss:  0.5294680595397949
train gradient:  0.16874695885095242
iteration : 4124
train acc:  0.828125
train loss:  0.42496222257614136
train gradient:  0.10870358866343592
iteration : 4125
train acc:  0.6875
train loss:  0.5379577875137329
train gradient:  0.17381295177285402
iteration : 4126
train acc:  0.625
train loss:  0.6268662810325623
train gradient:  0.17774027992959468
iteration : 4127
train acc:  0.7578125
train loss:  0.48378992080688477
train gradient:  0.1274779714664136
iteration : 4128
train acc:  0.75
train loss:  0.5133190751075745
train gradient:  0.16379468734519934
iteration : 4129
train acc:  0.75
train loss:  0.49361884593963623
train gradient:  0.15891599314189353
iteration : 4130
train acc:  0.703125
train loss:  0.5454728007316589
train gradient:  0.12891840922820902
iteration : 4131
train acc:  0.71875
train loss:  0.5049768686294556
train gradient:  0.11787825456413445
iteration : 4132
train acc:  0.6796875
train loss:  0.5327646732330322
train gradient:  0.20302202915431106
iteration : 4133
train acc:  0.7265625
train loss:  0.5737036466598511
train gradient:  0.19847776311537438
iteration : 4134
train acc:  0.6953125
train loss:  0.5643582940101624
train gradient:  0.16758567605916003
iteration : 4135
train acc:  0.7265625
train loss:  0.49678629636764526
train gradient:  0.13924327680143259
iteration : 4136
train acc:  0.71875
train loss:  0.4950556457042694
train gradient:  0.14877414122990207
iteration : 4137
train acc:  0.7890625
train loss:  0.47013938426971436
train gradient:  0.13056308062892413
iteration : 4138
train acc:  0.796875
train loss:  0.48523610830307007
train gradient:  0.12046490809415358
iteration : 4139
train acc:  0.6796875
train loss:  0.5532768964767456
train gradient:  0.18970937490409218
iteration : 4140
train acc:  0.75
train loss:  0.4962770938873291
train gradient:  0.14954837189481415
iteration : 4141
train acc:  0.6796875
train loss:  0.5266662836074829
train gradient:  0.14615946104838456
iteration : 4142
train acc:  0.75
train loss:  0.4819955825805664
train gradient:  0.13129928058431953
iteration : 4143
train acc:  0.765625
train loss:  0.486616313457489
train gradient:  0.13252729654701179
iteration : 4144
train acc:  0.71875
train loss:  0.5581471920013428
train gradient:  0.152513738130131
iteration : 4145
train acc:  0.7734375
train loss:  0.4462648034095764
train gradient:  0.1138483246929605
iteration : 4146
train acc:  0.75
train loss:  0.47888630628585815
train gradient:  0.2212621875176155
iteration : 4147
train acc:  0.75
train loss:  0.4863949418067932
train gradient:  0.1362885268540472
iteration : 4148
train acc:  0.703125
train loss:  0.5332765579223633
train gradient:  0.1934484496393982
iteration : 4149
train acc:  0.7421875
train loss:  0.4917088449001312
train gradient:  0.15602327650116518
iteration : 4150
train acc:  0.7578125
train loss:  0.47168710827827454
train gradient:  0.1490038292493658
iteration : 4151
train acc:  0.78125
train loss:  0.4760276973247528
train gradient:  0.12116527513128421
iteration : 4152
train acc:  0.75
train loss:  0.4492610692977905
train gradient:  0.16159985106055336
iteration : 4153
train acc:  0.6796875
train loss:  0.538733720779419
train gradient:  0.1697939772225917
iteration : 4154
train acc:  0.75
train loss:  0.5043128728866577
train gradient:  0.1703899228457441
iteration : 4155
train acc:  0.71875
train loss:  0.5604327917098999
train gradient:  0.15715970159216153
iteration : 4156
train acc:  0.671875
train loss:  0.5794988870620728
train gradient:  0.1866485894751788
iteration : 4157
train acc:  0.7421875
train loss:  0.49727270007133484
train gradient:  0.15185596511171362
iteration : 4158
train acc:  0.65625
train loss:  0.5801495313644409
train gradient:  0.1957166906881976
iteration : 4159
train acc:  0.8125
train loss:  0.41974061727523804
train gradient:  0.1484726772711226
iteration : 4160
train acc:  0.7265625
train loss:  0.5408118367195129
train gradient:  0.19412672553333987
iteration : 4161
train acc:  0.7421875
train loss:  0.5214738845825195
train gradient:  0.12688858670479156
iteration : 4162
train acc:  0.6953125
train loss:  0.5496386289596558
train gradient:  0.17442426343510514
iteration : 4163
train acc:  0.7109375
train loss:  0.5358089208602905
train gradient:  0.15629357775363112
iteration : 4164
train acc:  0.7578125
train loss:  0.4795045554637909
train gradient:  0.11870213200945565
iteration : 4165
train acc:  0.6796875
train loss:  0.5468001961708069
train gradient:  0.14753222377451738
iteration : 4166
train acc:  0.71875
train loss:  0.5610604882240295
train gradient:  0.16298057144722464
iteration : 4167
train acc:  0.7109375
train loss:  0.4560066759586334
train gradient:  0.1113543578409828
iteration : 4168
train acc:  0.7109375
train loss:  0.6301038265228271
train gradient:  0.21619064528577325
iteration : 4169
train acc:  0.71875
train loss:  0.5330745577812195
train gradient:  0.16045646791381163
iteration : 4170
train acc:  0.765625
train loss:  0.477986603975296
train gradient:  0.17309213652616157
iteration : 4171
train acc:  0.703125
train loss:  0.5093910098075867
train gradient:  0.1207219606426066
iteration : 4172
train acc:  0.7109375
train loss:  0.5756270885467529
train gradient:  0.20526674899202413
iteration : 4173
train acc:  0.640625
train loss:  0.6336631774902344
train gradient:  0.24585864902621063
iteration : 4174
train acc:  0.671875
train loss:  0.5205830335617065
train gradient:  0.1921284545065653
iteration : 4175
train acc:  0.8203125
train loss:  0.42041027545928955
train gradient:  0.15901163560881065
iteration : 4176
train acc:  0.6953125
train loss:  0.5308183431625366
train gradient:  0.14008238746409227
iteration : 4177
train acc:  0.75
train loss:  0.5707258582115173
train gradient:  0.15884844942114118
iteration : 4178
train acc:  0.8203125
train loss:  0.4601793587207794
train gradient:  0.22222036755267513
iteration : 4179
train acc:  0.71875
train loss:  0.5548019409179688
train gradient:  0.13900198017779747
iteration : 4180
train acc:  0.7109375
train loss:  0.555414617061615
train gradient:  0.17136738806656207
iteration : 4181
train acc:  0.7421875
train loss:  0.4977816045284271
train gradient:  0.21402599681959164
iteration : 4182
train acc:  0.75
train loss:  0.45216989517211914
train gradient:  0.18306346567514675
iteration : 4183
train acc:  0.65625
train loss:  0.614013671875
train gradient:  0.1858705630061736
iteration : 4184
train acc:  0.7265625
train loss:  0.5329185128211975
train gradient:  0.17013372799057128
iteration : 4185
train acc:  0.75
train loss:  0.5322970747947693
train gradient:  0.15124256467521435
iteration : 4186
train acc:  0.7421875
train loss:  0.545612096786499
train gradient:  0.15251952788637188
iteration : 4187
train acc:  0.640625
train loss:  0.5700353384017944
train gradient:  0.14142949525830878
iteration : 4188
train acc:  0.7734375
train loss:  0.48709535598754883
train gradient:  0.15638587864525158
iteration : 4189
train acc:  0.765625
train loss:  0.4904351234436035
train gradient:  0.11823127458164577
iteration : 4190
train acc:  0.6328125
train loss:  0.5867953300476074
train gradient:  0.2331100812892164
iteration : 4191
train acc:  0.75
train loss:  0.4583415687084198
train gradient:  0.11853739819042367
iteration : 4192
train acc:  0.7890625
train loss:  0.47563880681991577
train gradient:  0.12503466488514342
iteration : 4193
train acc:  0.7578125
train loss:  0.4977518916130066
train gradient:  0.15353822529839925
iteration : 4194
train acc:  0.703125
train loss:  0.536139965057373
train gradient:  0.1502687425237452
iteration : 4195
train acc:  0.6875
train loss:  0.5264661312103271
train gradient:  0.15497426887254104
iteration : 4196
train acc:  0.7421875
train loss:  0.5073032379150391
train gradient:  0.15330861827368625
iteration : 4197
train acc:  0.734375
train loss:  0.5229495763778687
train gradient:  0.18893749705972235
iteration : 4198
train acc:  0.734375
train loss:  0.521216094493866
train gradient:  0.1677337532195916
iteration : 4199
train acc:  0.734375
train loss:  0.48312491178512573
train gradient:  0.16038088458132527
iteration : 4200
train acc:  0.7890625
train loss:  0.46295568346977234
train gradient:  0.17862717357038682
iteration : 4201
train acc:  0.7421875
train loss:  0.4798583686351776
train gradient:  0.1253911785513082
iteration : 4202
train acc:  0.7578125
train loss:  0.511436939239502
train gradient:  0.17475423232632487
iteration : 4203
train acc:  0.7421875
train loss:  0.5607335567474365
train gradient:  0.14653757856884914
iteration : 4204
train acc:  0.7109375
train loss:  0.517805814743042
train gradient:  0.1667235262578116
iteration : 4205
train acc:  0.7265625
train loss:  0.48812299966812134
train gradient:  0.1725647066474423
iteration : 4206
train acc:  0.734375
train loss:  0.47631242871284485
train gradient:  0.12245043699125373
iteration : 4207
train acc:  0.6953125
train loss:  0.5609872341156006
train gradient:  0.14637350969723528
iteration : 4208
train acc:  0.6953125
train loss:  0.556450366973877
train gradient:  0.16485380145320871
iteration : 4209
train acc:  0.671875
train loss:  0.5511504411697388
train gradient:  0.18079586770577416
iteration : 4210
train acc:  0.6953125
train loss:  0.5175265073776245
train gradient:  0.15105977853755548
iteration : 4211
train acc:  0.671875
train loss:  0.5523654222488403
train gradient:  0.17270596077274447
iteration : 4212
train acc:  0.7421875
train loss:  0.46522167325019836
train gradient:  0.12428029527072854
iteration : 4213
train acc:  0.671875
train loss:  0.6041641235351562
train gradient:  0.27491253161278806
iteration : 4214
train acc:  0.734375
train loss:  0.481624573469162
train gradient:  0.1387917917074404
iteration : 4215
train acc:  0.734375
train loss:  0.5503409504890442
train gradient:  0.1344907184530821
iteration : 4216
train acc:  0.6640625
train loss:  0.5382232666015625
train gradient:  0.16365955961941092
iteration : 4217
train acc:  0.6640625
train loss:  0.5585236549377441
train gradient:  0.1662402604955864
iteration : 4218
train acc:  0.78125
train loss:  0.5095739960670471
train gradient:  0.13543987688464545
iteration : 4219
train acc:  0.75
train loss:  0.48803889751434326
train gradient:  0.1507621458876847
iteration : 4220
train acc:  0.8046875
train loss:  0.46243494749069214
train gradient:  0.14624692453942412
iteration : 4221
train acc:  0.71875
train loss:  0.5321479439735413
train gradient:  0.1449196969702355
iteration : 4222
train acc:  0.6796875
train loss:  0.5582147836685181
train gradient:  0.1993529037040335
iteration : 4223
train acc:  0.6875
train loss:  0.5499624013900757
train gradient:  0.16572373310247834
iteration : 4224
train acc:  0.71875
train loss:  0.5832165479660034
train gradient:  0.18966824721771414
iteration : 4225
train acc:  0.71875
train loss:  0.555978000164032
train gradient:  0.1812425979456167
iteration : 4226
train acc:  0.734375
train loss:  0.5147851705551147
train gradient:  0.1498486653201939
iteration : 4227
train acc:  0.6953125
train loss:  0.5611018538475037
train gradient:  0.16039485693339126
iteration : 4228
train acc:  0.6875
train loss:  0.5653814077377319
train gradient:  0.1542402136086788
iteration : 4229
train acc:  0.6796875
train loss:  0.6234924793243408
train gradient:  0.25414720863689105
iteration : 4230
train acc:  0.6328125
train loss:  0.5668320059776306
train gradient:  0.16100367089950593
iteration : 4231
train acc:  0.7265625
train loss:  0.4856058955192566
train gradient:  0.13775900518163128
iteration : 4232
train acc:  0.703125
train loss:  0.5302373170852661
train gradient:  0.18901905822983128
iteration : 4233
train acc:  0.7734375
train loss:  0.471469521522522
train gradient:  0.15649808721485792
iteration : 4234
train acc:  0.7578125
train loss:  0.46289777755737305
train gradient:  0.13980757866554325
iteration : 4235
train acc:  0.7578125
train loss:  0.5059533715248108
train gradient:  0.1571584258577517
iteration : 4236
train acc:  0.6796875
train loss:  0.5783311128616333
train gradient:  0.18152736596805136
iteration : 4237
train acc:  0.7265625
train loss:  0.5546083450317383
train gradient:  0.22191381489895068
iteration : 4238
train acc:  0.734375
train loss:  0.5537216067314148
train gradient:  0.15482421445550126
iteration : 4239
train acc:  0.703125
train loss:  0.545839250087738
train gradient:  0.15326452703909074
iteration : 4240
train acc:  0.734375
train loss:  0.5344879627227783
train gradient:  0.17595534576231286
iteration : 4241
train acc:  0.6875
train loss:  0.5451900959014893
train gradient:  0.16107710157256516
iteration : 4242
train acc:  0.7265625
train loss:  0.48576095700263977
train gradient:  0.1344199458645176
iteration : 4243
train acc:  0.78125
train loss:  0.46703577041625977
train gradient:  0.15022222204347602
iteration : 4244
train acc:  0.71875
train loss:  0.4970843195915222
train gradient:  0.12637007270511602
iteration : 4245
train acc:  0.734375
train loss:  0.5262279510498047
train gradient:  0.1533879250668571
iteration : 4246
train acc:  0.7578125
train loss:  0.533569872379303
train gradient:  0.1449688640158185
iteration : 4247
train acc:  0.765625
train loss:  0.49598202109336853
train gradient:  0.14817671878932664
iteration : 4248
train acc:  0.765625
train loss:  0.4735485911369324
train gradient:  0.14976113844850716
iteration : 4249
train acc:  0.6875
train loss:  0.5431587100028992
train gradient:  0.2206407189944663
iteration : 4250
train acc:  0.671875
train loss:  0.6144614219665527
train gradient:  0.2079033833270799
iteration : 4251
train acc:  0.75
train loss:  0.5395128130912781
train gradient:  0.1586705946001188
iteration : 4252
train acc:  0.7109375
train loss:  0.5773546099662781
train gradient:  0.22585949558582774
iteration : 4253
train acc:  0.671875
train loss:  0.574251651763916
train gradient:  0.17420509177893284
iteration : 4254
train acc:  0.71875
train loss:  0.5659041404724121
train gradient:  0.2031599196690182
iteration : 4255
train acc:  0.7265625
train loss:  0.5473244190216064
train gradient:  0.16610264536532182
iteration : 4256
train acc:  0.796875
train loss:  0.5087290406227112
train gradient:  0.20641713790811034
iteration : 4257
train acc:  0.6953125
train loss:  0.5250633955001831
train gradient:  0.13716738371678322
iteration : 4258
train acc:  0.7421875
train loss:  0.5381770133972168
train gradient:  0.14745043057261564
iteration : 4259
train acc:  0.6171875
train loss:  0.6615921258926392
train gradient:  0.2236818485844911
iteration : 4260
train acc:  0.7421875
train loss:  0.49606263637542725
train gradient:  0.14765201170196146
iteration : 4261
train acc:  0.75
train loss:  0.4913007915019989
train gradient:  0.18415103527192744
iteration : 4262
train acc:  0.6640625
train loss:  0.5447176694869995
train gradient:  0.17678492256917083
iteration : 4263
train acc:  0.7265625
train loss:  0.49006009101867676
train gradient:  0.12320150015919717
iteration : 4264
train acc:  0.7578125
train loss:  0.4776916205883026
train gradient:  0.11577100456082713
iteration : 4265
train acc:  0.7421875
train loss:  0.5265907049179077
train gradient:  0.13879641677427962
iteration : 4266
train acc:  0.765625
train loss:  0.5562330484390259
train gradient:  0.11026564344128181
iteration : 4267
train acc:  0.765625
train loss:  0.45977362990379333
train gradient:  0.11696621602891202
iteration : 4268
train acc:  0.6796875
train loss:  0.546001136302948
train gradient:  0.16645751797215097
iteration : 4269
train acc:  0.6796875
train loss:  0.5670167207717896
train gradient:  0.1608583989197711
iteration : 4270
train acc:  0.6640625
train loss:  0.5660242438316345
train gradient:  0.20544269770407633
iteration : 4271
train acc:  0.7578125
train loss:  0.489670991897583
train gradient:  0.13536073534720935
iteration : 4272
train acc:  0.6953125
train loss:  0.5715115666389465
train gradient:  0.1477054324802408
iteration : 4273
train acc:  0.6640625
train loss:  0.5537922382354736
train gradient:  0.18607368682799907
iteration : 4274
train acc:  0.78125
train loss:  0.525734543800354
train gradient:  0.1627856612165475
iteration : 4275
train acc:  0.7890625
train loss:  0.4976569414138794
train gradient:  0.13686065718756582
iteration : 4276
train acc:  0.6796875
train loss:  0.5979869365692139
train gradient:  0.19646785165527061
iteration : 4277
train acc:  0.7734375
train loss:  0.4690297245979309
train gradient:  0.15286069628266064
iteration : 4278
train acc:  0.7890625
train loss:  0.4751679599285126
train gradient:  0.12251238383267456
iteration : 4279
train acc:  0.71875
train loss:  0.5294813513755798
train gradient:  0.1427497473719265
iteration : 4280
train acc:  0.7421875
train loss:  0.5388646125793457
train gradient:  0.20494417707610718
iteration : 4281
train acc:  0.7578125
train loss:  0.4999030828475952
train gradient:  0.13678255993906194
iteration : 4282
train acc:  0.7265625
train loss:  0.49765703082084656
train gradient:  0.10823519214158366
iteration : 4283
train acc:  0.7109375
train loss:  0.5332019329071045
train gradient:  0.1845511805346679
iteration : 4284
train acc:  0.7734375
train loss:  0.4767437279224396
train gradient:  0.14992786789055954
iteration : 4285
train acc:  0.734375
train loss:  0.493135929107666
train gradient:  0.15458201725984166
iteration : 4286
train acc:  0.734375
train loss:  0.4970782399177551
train gradient:  0.1463382220226425
iteration : 4287
train acc:  0.6328125
train loss:  0.617430567741394
train gradient:  0.18309385631146435
iteration : 4288
train acc:  0.6875
train loss:  0.5508377552032471
train gradient:  0.16256714151810514
iteration : 4289
train acc:  0.6640625
train loss:  0.602632462978363
train gradient:  0.16454308462491996
iteration : 4290
train acc:  0.71875
train loss:  0.4814627766609192
train gradient:  0.12153684827239902
iteration : 4291
train acc:  0.765625
train loss:  0.5051084756851196
train gradient:  0.16396324912340954
iteration : 4292
train acc:  0.6640625
train loss:  0.5821598172187805
train gradient:  0.2079680766709699
iteration : 4293
train acc:  0.7578125
train loss:  0.4978531002998352
train gradient:  0.1162940774019295
iteration : 4294
train acc:  0.734375
train loss:  0.5582281947135925
train gradient:  0.1568214756938634
iteration : 4295
train acc:  0.703125
train loss:  0.5523129105567932
train gradient:  0.14920984745302135
iteration : 4296
train acc:  0.765625
train loss:  0.4937324523925781
train gradient:  0.1310402188602135
iteration : 4297
train acc:  0.671875
train loss:  0.5242563486099243
train gradient:  0.13346800217703028
iteration : 4298
train acc:  0.71875
train loss:  0.5169729590415955
train gradient:  0.16425610877262703
iteration : 4299
train acc:  0.71875
train loss:  0.5079084634780884
train gradient:  0.16638477991482434
iteration : 4300
train acc:  0.71875
train loss:  0.5417919158935547
train gradient:  0.13318857119267397
iteration : 4301
train acc:  0.640625
train loss:  0.653619647026062
train gradient:  0.2644421362079975
iteration : 4302
train acc:  0.6953125
train loss:  0.5533544421195984
train gradient:  0.17500325999855382
iteration : 4303
train acc:  0.625
train loss:  0.5909186601638794
train gradient:  0.23720627090678137
iteration : 4304
train acc:  0.65625
train loss:  0.5994954109191895
train gradient:  0.19727988551718412
iteration : 4305
train acc:  0.703125
train loss:  0.5277504324913025
train gradient:  0.1403043944517433
iteration : 4306
train acc:  0.796875
train loss:  0.4679553508758545
train gradient:  0.14503214863218927
iteration : 4307
train acc:  0.7265625
train loss:  0.5480474233627319
train gradient:  0.13852800074691887
iteration : 4308
train acc:  0.671875
train loss:  0.5560427904129028
train gradient:  0.15610630578515228
iteration : 4309
train acc:  0.6953125
train loss:  0.5651288032531738
train gradient:  0.16153729932778443
iteration : 4310
train acc:  0.765625
train loss:  0.44805029034614563
train gradient:  0.19299976130051644
iteration : 4311
train acc:  0.65625
train loss:  0.5607430934906006
train gradient:  0.175690036027756
iteration : 4312
train acc:  0.7578125
train loss:  0.4988209307193756
train gradient:  0.17366436938305727
iteration : 4313
train acc:  0.734375
train loss:  0.5851952433586121
train gradient:  0.21915115747734093
iteration : 4314
train acc:  0.6796875
train loss:  0.5291101932525635
train gradient:  0.14374939230702666
iteration : 4315
train acc:  0.7734375
train loss:  0.5008737444877625
train gradient:  0.18583075461820805
iteration : 4316
train acc:  0.703125
train loss:  0.5510690212249756
train gradient:  0.20389052252413614
iteration : 4317
train acc:  0.6875
train loss:  0.5466514825820923
train gradient:  0.1982441826286282
iteration : 4318
train acc:  0.703125
train loss:  0.5250407457351685
train gradient:  0.21381089188862973
iteration : 4319
train acc:  0.7578125
train loss:  0.49410632252693176
train gradient:  0.11184393146496126
iteration : 4320
train acc:  0.78125
train loss:  0.5012495517730713
train gradient:  0.12967606888611485
iteration : 4321
train acc:  0.7578125
train loss:  0.4767153263092041
train gradient:  0.11383135510179897
iteration : 4322
train acc:  0.71875
train loss:  0.5025120973587036
train gradient:  0.13054540871354092
iteration : 4323
train acc:  0.6328125
train loss:  0.5696909427642822
train gradient:  0.1799547493826147
iteration : 4324
train acc:  0.7890625
train loss:  0.4586859345436096
train gradient:  0.13747488679488493
iteration : 4325
train acc:  0.765625
train loss:  0.4769582450389862
train gradient:  0.17781942791838495
iteration : 4326
train acc:  0.703125
train loss:  0.5713820457458496
train gradient:  0.17477186952703128
iteration : 4327
train acc:  0.734375
train loss:  0.5297950506210327
train gradient:  0.15468493609827533
iteration : 4328
train acc:  0.7109375
train loss:  0.5143512487411499
train gradient:  0.1269711069587004
iteration : 4329
train acc:  0.7265625
train loss:  0.540216863155365
train gradient:  0.1857365295918717
iteration : 4330
train acc:  0.71875
train loss:  0.4945150911808014
train gradient:  0.14014988238882625
iteration : 4331
train acc:  0.7578125
train loss:  0.5035196542739868
train gradient:  0.16001681568699763
iteration : 4332
train acc:  0.7734375
train loss:  0.5044206380844116
train gradient:  0.15597531755946425
iteration : 4333
train acc:  0.703125
train loss:  0.5711884498596191
train gradient:  0.20081773057693666
iteration : 4334
train acc:  0.71875
train loss:  0.49399054050445557
train gradient:  0.1282161480432208
iteration : 4335
train acc:  0.6875
train loss:  0.536909818649292
train gradient:  0.1688860210427959
iteration : 4336
train acc:  0.6953125
train loss:  0.5508005619049072
train gradient:  0.15745351036905492
iteration : 4337
train acc:  0.734375
train loss:  0.5318446159362793
train gradient:  0.15633925265558002
iteration : 4338
train acc:  0.703125
train loss:  0.5454902648925781
train gradient:  0.19812350258427797
iteration : 4339
train acc:  0.6953125
train loss:  0.527396559715271
train gradient:  0.13952612274909293
iteration : 4340
train acc:  0.671875
train loss:  0.5460352897644043
train gradient:  0.20052989863585025
iteration : 4341
train acc:  0.765625
train loss:  0.5252377986907959
train gradient:  0.21928256601035734
iteration : 4342
train acc:  0.7265625
train loss:  0.5414261817932129
train gradient:  0.19638100380127593
iteration : 4343
train acc:  0.7421875
train loss:  0.5358977317810059
train gradient:  0.1614546851411335
iteration : 4344
train acc:  0.7578125
train loss:  0.4696956276893616
train gradient:  0.15399037804057147
iteration : 4345
train acc:  0.71875
train loss:  0.5447613000869751
train gradient:  0.16377692150886863
iteration : 4346
train acc:  0.71875
train loss:  0.5698336958885193
train gradient:  0.2062841830169591
iteration : 4347
train acc:  0.703125
train loss:  0.5656814575195312
train gradient:  0.13976167720329252
iteration : 4348
train acc:  0.6640625
train loss:  0.5553843975067139
train gradient:  0.2737123496668296
iteration : 4349
train acc:  0.78125
train loss:  0.49698710441589355
train gradient:  0.192183224869375
iteration : 4350
train acc:  0.7578125
train loss:  0.4783085882663727
train gradient:  0.1294213113208053
iteration : 4351
train acc:  0.7578125
train loss:  0.5049000978469849
train gradient:  0.1352623473670766
iteration : 4352
train acc:  0.7734375
train loss:  0.46844762563705444
train gradient:  0.12040648958558883
iteration : 4353
train acc:  0.703125
train loss:  0.5114427804946899
train gradient:  0.12647002123119172
iteration : 4354
train acc:  0.734375
train loss:  0.5050074458122253
train gradient:  0.13408750861792812
iteration : 4355
train acc:  0.75
train loss:  0.4992145299911499
train gradient:  0.1669191969295617
iteration : 4356
train acc:  0.6484375
train loss:  0.5250215530395508
train gradient:  0.16365057546469663
iteration : 4357
train acc:  0.8046875
train loss:  0.4792412519454956
train gradient:  0.147551586626739
iteration : 4358
train acc:  0.765625
train loss:  0.4737219512462616
train gradient:  0.15683839224643287
iteration : 4359
train acc:  0.7578125
train loss:  0.5105903148651123
train gradient:  0.15704267076604625
iteration : 4360
train acc:  0.8125
train loss:  0.45033711194992065
train gradient:  0.13452514943047578
iteration : 4361
train acc:  0.78125
train loss:  0.48503875732421875
train gradient:  0.15197129542579535
iteration : 4362
train acc:  0.671875
train loss:  0.5346534252166748
train gradient:  0.18063840239867457
iteration : 4363
train acc:  0.7265625
train loss:  0.5205480456352234
train gradient:  0.11399549267008852
iteration : 4364
train acc:  0.6953125
train loss:  0.5079341530799866
train gradient:  0.17350519041076073
iteration : 4365
train acc:  0.7421875
train loss:  0.5331586003303528
train gradient:  0.13459107397859005
iteration : 4366
train acc:  0.6796875
train loss:  0.5913833975791931
train gradient:  0.18237609731640564
iteration : 4367
train acc:  0.703125
train loss:  0.5961992144584656
train gradient:  0.20303972456455513
iteration : 4368
train acc:  0.7421875
train loss:  0.5000275373458862
train gradient:  0.13394391482081472
iteration : 4369
train acc:  0.734375
train loss:  0.5091496109962463
train gradient:  0.21387710334549426
iteration : 4370
train acc:  0.765625
train loss:  0.4539073705673218
train gradient:  0.12906381415433305
iteration : 4371
train acc:  0.7734375
train loss:  0.4627819359302521
train gradient:  0.11001289339926501
iteration : 4372
train acc:  0.703125
train loss:  0.5088870525360107
train gradient:  0.2056537499148548
iteration : 4373
train acc:  0.7109375
train loss:  0.48291581869125366
train gradient:  0.16904942695821806
iteration : 4374
train acc:  0.6796875
train loss:  0.5705584287643433
train gradient:  0.1808553785820845
iteration : 4375
train acc:  0.7265625
train loss:  0.5161996483802795
train gradient:  0.1886117445593204
iteration : 4376
train acc:  0.7265625
train loss:  0.47611644864082336
train gradient:  0.15100828590648804
iteration : 4377
train acc:  0.65625
train loss:  0.5726840496063232
train gradient:  0.20345254845680621
iteration : 4378
train acc:  0.703125
train loss:  0.5190127491950989
train gradient:  0.1661658654242047
iteration : 4379
train acc:  0.765625
train loss:  0.45649123191833496
train gradient:  0.12384865196482842
iteration : 4380
train acc:  0.6953125
train loss:  0.6005666851997375
train gradient:  0.16838495591844527
iteration : 4381
train acc:  0.6640625
train loss:  0.5447044372558594
train gradient:  0.14358956959416605
iteration : 4382
train acc:  0.71875
train loss:  0.4980945587158203
train gradient:  0.10357837414135034
iteration : 4383
train acc:  0.6796875
train loss:  0.5464588403701782
train gradient:  0.24858987922424813
iteration : 4384
train acc:  0.6875
train loss:  0.5126315951347351
train gradient:  0.13818369442799672
iteration : 4385
train acc:  0.8046875
train loss:  0.4793585538864136
train gradient:  0.11800610216341262
iteration : 4386
train acc:  0.6796875
train loss:  0.5615200996398926
train gradient:  0.1796539879058045
iteration : 4387
train acc:  0.71875
train loss:  0.5344853401184082
train gradient:  0.13538976192520627
iteration : 4388
train acc:  0.78125
train loss:  0.46053773164749146
train gradient:  0.12314075977200675
iteration : 4389
train acc:  0.6953125
train loss:  0.5235010385513306
train gradient:  0.18401482235054356
iteration : 4390
train acc:  0.7578125
train loss:  0.4934200644493103
train gradient:  0.14165026339535924
iteration : 4391
train acc:  0.671875
train loss:  0.5308966040611267
train gradient:  0.1415265870975681
iteration : 4392
train acc:  0.6796875
train loss:  0.5802481770515442
train gradient:  0.27251310028538167
iteration : 4393
train acc:  0.7109375
train loss:  0.5217922925949097
train gradient:  0.2096133707520807
iteration : 4394
train acc:  0.734375
train loss:  0.4855472445487976
train gradient:  0.136258361487907
iteration : 4395
train acc:  0.6875
train loss:  0.5323516130447388
train gradient:  0.16302059613895137
iteration : 4396
train acc:  0.734375
train loss:  0.4903669059276581
train gradient:  0.1358095377247129
iteration : 4397
train acc:  0.7578125
train loss:  0.48518049716949463
train gradient:  0.11931907322822957
iteration : 4398
train acc:  0.75
train loss:  0.46102917194366455
train gradient:  0.13909419398758133
iteration : 4399
train acc:  0.6953125
train loss:  0.545952558517456
train gradient:  0.21332358058268153
iteration : 4400
train acc:  0.7421875
train loss:  0.5108565092086792
train gradient:  0.13392019548842976
iteration : 4401
train acc:  0.7109375
train loss:  0.5622435808181763
train gradient:  0.19488261838090515
iteration : 4402
train acc:  0.6796875
train loss:  0.5408412218093872
train gradient:  0.16896303327725176
iteration : 4403
train acc:  0.7265625
train loss:  0.5419652462005615
train gradient:  0.2073906837184939
iteration : 4404
train acc:  0.6640625
train loss:  0.5611779093742371
train gradient:  0.19559555854105282
iteration : 4405
train acc:  0.7265625
train loss:  0.5079397559165955
train gradient:  0.1110494290615676
iteration : 4406
train acc:  0.7578125
train loss:  0.5269654989242554
train gradient:  0.14783404731063385
iteration : 4407
train acc:  0.703125
train loss:  0.505527138710022
train gradient:  0.1809570745365437
iteration : 4408
train acc:  0.796875
train loss:  0.45633041858673096
train gradient:  0.14541278785129047
iteration : 4409
train acc:  0.71875
train loss:  0.5077385902404785
train gradient:  0.1471208872577063
iteration : 4410
train acc:  0.8125
train loss:  0.46004337072372437
train gradient:  0.12281965435245
iteration : 4411
train acc:  0.734375
train loss:  0.5158520340919495
train gradient:  0.1870874815684008
iteration : 4412
train acc:  0.703125
train loss:  0.5551707148551941
train gradient:  0.14428679812631567
iteration : 4413
train acc:  0.7421875
train loss:  0.5064140558242798
train gradient:  0.14512586569238828
iteration : 4414
train acc:  0.6796875
train loss:  0.5796148777008057
train gradient:  0.196067773778885
iteration : 4415
train acc:  0.7421875
train loss:  0.5273574590682983
train gradient:  0.16800582766951783
iteration : 4416
train acc:  0.7421875
train loss:  0.5312321186065674
train gradient:  0.1789699660831639
iteration : 4417
train acc:  0.7109375
train loss:  0.5412924289703369
train gradient:  0.18173830962988569
iteration : 4418
train acc:  0.6875
train loss:  0.5730519890785217
train gradient:  0.19534824180376686
iteration : 4419
train acc:  0.703125
train loss:  0.5091843605041504
train gradient:  0.13884411498449925
iteration : 4420
train acc:  0.6953125
train loss:  0.5908839702606201
train gradient:  0.1587872943493111
iteration : 4421
train acc:  0.6796875
train loss:  0.5572109222412109
train gradient:  0.1605331255064416
iteration : 4422
train acc:  0.71875
train loss:  0.5070384740829468
train gradient:  0.13479586026648174
iteration : 4423
train acc:  0.734375
train loss:  0.5400339365005493
train gradient:  0.1359313003356979
iteration : 4424
train acc:  0.703125
train loss:  0.5265804529190063
train gradient:  0.14297014451288464
iteration : 4425
train acc:  0.7109375
train loss:  0.5477817058563232
train gradient:  0.1535950673880865
iteration : 4426
train acc:  0.6484375
train loss:  0.565401554107666
train gradient:  0.16869645746106898
iteration : 4427
train acc:  0.6875
train loss:  0.5266134738922119
train gradient:  0.21531195776835163
iteration : 4428
train acc:  0.7578125
train loss:  0.5235497355461121
train gradient:  0.1909309212127781
iteration : 4429
train acc:  0.6796875
train loss:  0.553048849105835
train gradient:  0.19406841505217148
iteration : 4430
train acc:  0.7578125
train loss:  0.5068157911300659
train gradient:  0.1641020974189421
iteration : 4431
train acc:  0.8203125
train loss:  0.43171313405036926
train gradient:  0.10311826407182156
iteration : 4432
train acc:  0.8203125
train loss:  0.4547536075115204
train gradient:  0.15687193533133037
iteration : 4433
train acc:  0.734375
train loss:  0.4874822497367859
train gradient:  0.19596514565129847
iteration : 4434
train acc:  0.71875
train loss:  0.5466739535331726
train gradient:  0.13932312551434656
iteration : 4435
train acc:  0.71875
train loss:  0.4895516037940979
train gradient:  0.15184338334335
iteration : 4436
train acc:  0.703125
train loss:  0.5169695019721985
train gradient:  0.13407628877652894
iteration : 4437
train acc:  0.7265625
train loss:  0.49330490827560425
train gradient:  0.11902814357932454
iteration : 4438
train acc:  0.7890625
train loss:  0.5254123210906982
train gradient:  0.1713537197596718
iteration : 4439
train acc:  0.7421875
train loss:  0.5498194694519043
train gradient:  0.15862309778171196
iteration : 4440
train acc:  0.6640625
train loss:  0.5949798226356506
train gradient:  0.15824367606493
iteration : 4441
train acc:  0.71875
train loss:  0.5168826580047607
train gradient:  0.13040732931471763
iteration : 4442
train acc:  0.765625
train loss:  0.5137877464294434
train gradient:  0.13352657297687287
iteration : 4443
train acc:  0.7421875
train loss:  0.4923381209373474
train gradient:  0.1812305135536848
iteration : 4444
train acc:  0.734375
train loss:  0.47270500659942627
train gradient:  0.13070579356450274
iteration : 4445
train acc:  0.7265625
train loss:  0.5129045248031616
train gradient:  0.12641618630177986
iteration : 4446
train acc:  0.703125
train loss:  0.5432264804840088
train gradient:  0.20176474187567955
iteration : 4447
train acc:  0.765625
train loss:  0.5145264267921448
train gradient:  0.16506007592429106
iteration : 4448
train acc:  0.8203125
train loss:  0.42865312099456787
train gradient:  0.12466535072985367
iteration : 4449
train acc:  0.7265625
train loss:  0.5261058807373047
train gradient:  0.1535116246152359
iteration : 4450
train acc:  0.703125
train loss:  0.4951760172843933
train gradient:  0.13173186981614207
iteration : 4451
train acc:  0.7421875
train loss:  0.5037177205085754
train gradient:  0.16432462015618132
iteration : 4452
train acc:  0.765625
train loss:  0.5319318771362305
train gradient:  0.19323823054875588
iteration : 4453
train acc:  0.6328125
train loss:  0.6257621645927429
train gradient:  0.24241419711679552
iteration : 4454
train acc:  0.671875
train loss:  0.6162935495376587
train gradient:  0.25819992378473716
iteration : 4455
train acc:  0.75
train loss:  0.5088869333267212
train gradient:  0.18464441516205943
iteration : 4456
train acc:  0.6875
train loss:  0.5433199405670166
train gradient:  0.18659782467518626
iteration : 4457
train acc:  0.7578125
train loss:  0.5122355222702026
train gradient:  0.12821907480680517
iteration : 4458
train acc:  0.7109375
train loss:  0.5237638354301453
train gradient:  0.1780492474162812
iteration : 4459
train acc:  0.7421875
train loss:  0.4701318144798279
train gradient:  0.13569382516126688
iteration : 4460
train acc:  0.671875
train loss:  0.515739917755127
train gradient:  0.2002416147518999
iteration : 4461
train acc:  0.7734375
train loss:  0.525100827217102
train gradient:  0.13403153327630868
iteration : 4462
train acc:  0.6640625
train loss:  0.5534413456916809
train gradient:  0.16675164406331738
iteration : 4463
train acc:  0.6953125
train loss:  0.5429583191871643
train gradient:  0.18872063906612785
iteration : 4464
train acc:  0.78125
train loss:  0.43957528471946716
train gradient:  0.10979343407964769
iteration : 4465
train acc:  0.7265625
train loss:  0.5117062926292419
train gradient:  0.15724920342321008
iteration : 4466
train acc:  0.6640625
train loss:  0.5774060487747192
train gradient:  0.16984591330157478
iteration : 4467
train acc:  0.75
train loss:  0.5194987654685974
train gradient:  0.20307547329506825
iteration : 4468
train acc:  0.734375
train loss:  0.4911229610443115
train gradient:  0.16035883903626474
iteration : 4469
train acc:  0.671875
train loss:  0.583640456199646
train gradient:  0.20084767092318778
iteration : 4470
train acc:  0.734375
train loss:  0.499796986579895
train gradient:  0.14581032190041907
iteration : 4471
train acc:  0.6953125
train loss:  0.5664815902709961
train gradient:  0.16822625430792568
iteration : 4472
train acc:  0.6953125
train loss:  0.5476059913635254
train gradient:  0.19747819331184513
iteration : 4473
train acc:  0.7265625
train loss:  0.5207918882369995
train gradient:  0.14665938929300906
iteration : 4474
train acc:  0.7734375
train loss:  0.485235333442688
train gradient:  0.1318107988073459
iteration : 4475
train acc:  0.7265625
train loss:  0.5272402763366699
train gradient:  0.17223981132410662
iteration : 4476
train acc:  0.7265625
train loss:  0.4733527898788452
train gradient:  0.12406294519042031
iteration : 4477
train acc:  0.6875
train loss:  0.5013946294784546
train gradient:  0.14845142060280941
iteration : 4478
train acc:  0.7578125
train loss:  0.49948960542678833
train gradient:  0.13441134860632145
iteration : 4479
train acc:  0.734375
train loss:  0.5191037654876709
train gradient:  0.16835650133741398
iteration : 4480
train acc:  0.71875
train loss:  0.5497394800186157
train gradient:  0.26822372411221446
iteration : 4481
train acc:  0.78125
train loss:  0.42021143436431885
train gradient:  0.10372166227426009
iteration : 4482
train acc:  0.7421875
train loss:  0.5209964513778687
train gradient:  0.13585415916339472
iteration : 4483
train acc:  0.734375
train loss:  0.5708994269371033
train gradient:  0.2661380191886311
iteration : 4484
train acc:  0.7578125
train loss:  0.4546942412853241
train gradient:  0.13936234317921448
iteration : 4485
train acc:  0.6875
train loss:  0.5366002321243286
train gradient:  0.18814928299578404
iteration : 4486
train acc:  0.6875
train loss:  0.5274733304977417
train gradient:  0.1717300321103595
iteration : 4487
train acc:  0.6640625
train loss:  0.5689563155174255
train gradient:  0.16212767828212743
iteration : 4488
train acc:  0.71875
train loss:  0.5292800068855286
train gradient:  0.15133776625706932
iteration : 4489
train acc:  0.7265625
train loss:  0.4949701428413391
train gradient:  0.15054341782205463
iteration : 4490
train acc:  0.6796875
train loss:  0.5787184238433838
train gradient:  0.2111493060333982
iteration : 4491
train acc:  0.765625
train loss:  0.4820619225502014
train gradient:  0.18815594842018968
iteration : 4492
train acc:  0.65625
train loss:  0.5826473236083984
train gradient:  0.2607643476455502
iteration : 4493
train acc:  0.765625
train loss:  0.4778168797492981
train gradient:  0.1419797628012781
iteration : 4494
train acc:  0.7578125
train loss:  0.4612641930580139
train gradient:  0.1341514084482815
iteration : 4495
train acc:  0.734375
train loss:  0.5306582450866699
train gradient:  0.14911501581563005
iteration : 4496
train acc:  0.734375
train loss:  0.4908450245857239
train gradient:  0.14883240165009518
iteration : 4497
train acc:  0.7734375
train loss:  0.4930461049079895
train gradient:  0.1743016777130329
iteration : 4498
train acc:  0.7265625
train loss:  0.48773908615112305
train gradient:  0.14995534472711758
iteration : 4499
train acc:  0.6640625
train loss:  0.5440284013748169
train gradient:  0.1334578423430072
iteration : 4500
train acc:  0.6640625
train loss:  0.5673561096191406
train gradient:  0.17074153015386415
iteration : 4501
train acc:  0.734375
train loss:  0.5236201286315918
train gradient:  0.15610055981736715
iteration : 4502
train acc:  0.78125
train loss:  0.4501507580280304
train gradient:  0.12274860545177312
iteration : 4503
train acc:  0.71875
train loss:  0.5234507918357849
train gradient:  0.1948355428839933
iteration : 4504
train acc:  0.7421875
train loss:  0.5128116607666016
train gradient:  0.14913261568765712
iteration : 4505
train acc:  0.7109375
train loss:  0.5406566858291626
train gradient:  0.18944455599841437
iteration : 4506
train acc:  0.7578125
train loss:  0.48861390352249146
train gradient:  0.15723661468746739
iteration : 4507
train acc:  0.6875
train loss:  0.5284407138824463
train gradient:  0.16694788827002205
iteration : 4508
train acc:  0.765625
train loss:  0.476237952709198
train gradient:  0.11485248499726157
iteration : 4509
train acc:  0.7109375
train loss:  0.5245831608772278
train gradient:  0.1868844523571394
iteration : 4510
train acc:  0.734375
train loss:  0.480776309967041
train gradient:  0.14663642243711894
iteration : 4511
train acc:  0.6875
train loss:  0.5918678045272827
train gradient:  0.35047230171168475
iteration : 4512
train acc:  0.71875
train loss:  0.531076192855835
train gradient:  0.19299956472397528
iteration : 4513
train acc:  0.6875
train loss:  0.5425487756729126
train gradient:  0.17918787705440822
iteration : 4514
train acc:  0.6953125
train loss:  0.6032959222793579
train gradient:  0.2043137712288638
iteration : 4515
train acc:  0.7421875
train loss:  0.5344725847244263
train gradient:  0.1517379434593527
iteration : 4516
train acc:  0.8125
train loss:  0.4342077970504761
train gradient:  0.11312887814957422
iteration : 4517
train acc:  0.7109375
train loss:  0.4951661229133606
train gradient:  0.1262024110725977
iteration : 4518
train acc:  0.7265625
train loss:  0.4831721782684326
train gradient:  0.21168784786326145
iteration : 4519
train acc:  0.78125
train loss:  0.4993264675140381
train gradient:  0.18024683893869703
iteration : 4520
train acc:  0.7265625
train loss:  0.5026458501815796
train gradient:  0.16692285387025124
iteration : 4521
train acc:  0.7578125
train loss:  0.5017408132553101
train gradient:  0.13197543426203057
iteration : 4522
train acc:  0.671875
train loss:  0.5747302770614624
train gradient:  0.1417152011695288
iteration : 4523
train acc:  0.6875
train loss:  0.5907857418060303
train gradient:  0.27002527924287406
iteration : 4524
train acc:  0.6640625
train loss:  0.5838808417320251
train gradient:  0.1816975017049029
iteration : 4525
train acc:  0.7890625
train loss:  0.48292276263237
train gradient:  0.1273945513999077
iteration : 4526
train acc:  0.765625
train loss:  0.5174949169158936
train gradient:  0.17037937547162485
iteration : 4527
train acc:  0.734375
train loss:  0.4478851556777954
train gradient:  0.11590302802549982
iteration : 4528
train acc:  0.703125
train loss:  0.520068883895874
train gradient:  0.1455730905181517
iteration : 4529
train acc:  0.7578125
train loss:  0.4655272364616394
train gradient:  0.12385641824856657
iteration : 4530
train acc:  0.7265625
train loss:  0.565803050994873
train gradient:  0.19873541151272628
iteration : 4531
train acc:  0.734375
train loss:  0.523455023765564
train gradient:  0.14161204338264238
iteration : 4532
train acc:  0.8046875
train loss:  0.4375283718109131
train gradient:  0.12965081885572655
iteration : 4533
train acc:  0.703125
train loss:  0.5624296069145203
train gradient:  0.18806639572513537
iteration : 4534
train acc:  0.6875
train loss:  0.5979620218276978
train gradient:  0.15524816304927777
iteration : 4535
train acc:  0.6328125
train loss:  0.5817462205886841
train gradient:  0.1561997919144775
iteration : 4536
train acc:  0.78125
train loss:  0.45570236444473267
train gradient:  0.15001357942524524
iteration : 4537
train acc:  0.7265625
train loss:  0.4801378548145294
train gradient:  0.16342323634377504
iteration : 4538
train acc:  0.7421875
train loss:  0.48469072580337524
train gradient:  0.1345129660418356
iteration : 4539
train acc:  0.703125
train loss:  0.5695242881774902
train gradient:  0.15925718643606468
iteration : 4540
train acc:  0.7421875
train loss:  0.4832218289375305
train gradient:  0.15534545203679806
iteration : 4541
train acc:  0.671875
train loss:  0.5521328449249268
train gradient:  0.16579170079983083
iteration : 4542
train acc:  0.703125
train loss:  0.5648890137672424
train gradient:  0.19583347835254913
iteration : 4543
train acc:  0.6953125
train loss:  0.5430574417114258
train gradient:  0.16878386138342685
iteration : 4544
train acc:  0.7578125
train loss:  0.4979798495769501
train gradient:  0.15448236345980712
iteration : 4545
train acc:  0.7734375
train loss:  0.47087162733078003
train gradient:  0.11102988026079885
iteration : 4546
train acc:  0.7421875
train loss:  0.5228384137153625
train gradient:  0.17042409856543206
iteration : 4547
train acc:  0.6796875
train loss:  0.5588796138763428
train gradient:  0.20933479544097183
iteration : 4548
train acc:  0.796875
train loss:  0.4641604423522949
train gradient:  0.13014420191572612
iteration : 4549
train acc:  0.7890625
train loss:  0.46144407987594604
train gradient:  0.10634076559153231
iteration : 4550
train acc:  0.71875
train loss:  0.5601871013641357
train gradient:  0.17512193902776219
iteration : 4551
train acc:  0.6875
train loss:  0.5552812218666077
train gradient:  0.1559069122195422
iteration : 4552
train acc:  0.734375
train loss:  0.5539690852165222
train gradient:  0.15783533876213535
iteration : 4553
train acc:  0.78125
train loss:  0.48025357723236084
train gradient:  0.14710674175851365
iteration : 4554
train acc:  0.7265625
train loss:  0.5425846576690674
train gradient:  0.2106248860694832
iteration : 4555
train acc:  0.7109375
train loss:  0.5345205068588257
train gradient:  0.1645125977290805
iteration : 4556
train acc:  0.7265625
train loss:  0.5309006571769714
train gradient:  0.16540837904770378
iteration : 4557
train acc:  0.8046875
train loss:  0.4818272590637207
train gradient:  0.12922891559179733
iteration : 4558
train acc:  0.7734375
train loss:  0.5052781701087952
train gradient:  0.12258596561187884
iteration : 4559
train acc:  0.734375
train loss:  0.5412772297859192
train gradient:  0.1838333214867659
iteration : 4560
train acc:  0.6953125
train loss:  0.5203143358230591
train gradient:  0.12810325699759542
iteration : 4561
train acc:  0.78125
train loss:  0.48221099376678467
train gradient:  0.11293803469115939
iteration : 4562
train acc:  0.796875
train loss:  0.4649388790130615
train gradient:  0.14778681548852
iteration : 4563
train acc:  0.8046875
train loss:  0.4800288677215576
train gradient:  0.159536898221334
iteration : 4564
train acc:  0.7421875
train loss:  0.5297732353210449
train gradient:  0.16106443526635372
iteration : 4565
train acc:  0.7421875
train loss:  0.5038025379180908
train gradient:  0.15192266603691615
iteration : 4566
train acc:  0.7265625
train loss:  0.5160907506942749
train gradient:  0.21135764633700838
iteration : 4567
train acc:  0.703125
train loss:  0.5123175978660583
train gradient:  0.1374042407315345
iteration : 4568
train acc:  0.6953125
train loss:  0.5793449878692627
train gradient:  0.19545528446260668
iteration : 4569
train acc:  0.7109375
train loss:  0.567121148109436
train gradient:  0.17759266931775602
iteration : 4570
train acc:  0.6484375
train loss:  0.5891794562339783
train gradient:  0.21036281231500464
iteration : 4571
train acc:  0.75
train loss:  0.4834643602371216
train gradient:  0.16766833105453177
iteration : 4572
train acc:  0.734375
train loss:  0.5415499210357666
train gradient:  0.17998770440489809
iteration : 4573
train acc:  0.7421875
train loss:  0.48869121074676514
train gradient:  0.14983613011693406
iteration : 4574
train acc:  0.734375
train loss:  0.5239726901054382
train gradient:  0.13876096409577024
iteration : 4575
train acc:  0.7265625
train loss:  0.5342618227005005
train gradient:  0.13927043995742405
iteration : 4576
train acc:  0.71875
train loss:  0.48376625776290894
train gradient:  0.1782741528341531
iteration : 4577
train acc:  0.765625
train loss:  0.5081827640533447
train gradient:  0.1466164876470123
iteration : 4578
train acc:  0.8125
train loss:  0.4213278591632843
train gradient:  0.11714361656292484
iteration : 4579
train acc:  0.78125
train loss:  0.48635876178741455
train gradient:  0.1673483608799137
iteration : 4580
train acc:  0.7890625
train loss:  0.45002663135528564
train gradient:  0.1286421549960929
iteration : 4581
train acc:  0.7109375
train loss:  0.5213838815689087
train gradient:  0.1663018503398608
iteration : 4582
train acc:  0.71875
train loss:  0.49610835313796997
train gradient:  0.1326263598527411
iteration : 4583
train acc:  0.734375
train loss:  0.490883469581604
train gradient:  0.1448138832941084
iteration : 4584
train acc:  0.75
train loss:  0.481054425239563
train gradient:  0.12672717326294997
iteration : 4585
train acc:  0.78125
train loss:  0.45784804224967957
train gradient:  0.11316258737309225
iteration : 4586
train acc:  0.7109375
train loss:  0.5095532536506653
train gradient:  0.16058839585572368
iteration : 4587
train acc:  0.6796875
train loss:  0.5696672201156616
train gradient:  0.23581517488705797
iteration : 4588
train acc:  0.6484375
train loss:  0.5946856737136841
train gradient:  0.2005766870753051
iteration : 4589
train acc:  0.609375
train loss:  0.5759099721908569
train gradient:  0.15035359187176514
iteration : 4590
train acc:  0.703125
train loss:  0.5383189916610718
train gradient:  0.18143991521642733
iteration : 4591
train acc:  0.7421875
train loss:  0.49783819913864136
train gradient:  0.1692636583005998
iteration : 4592
train acc:  0.640625
train loss:  0.6186692714691162
train gradient:  0.1955556713132774
iteration : 4593
train acc:  0.734375
train loss:  0.5272836685180664
train gradient:  0.22201387547104035
iteration : 4594
train acc:  0.671875
train loss:  0.551235020160675
train gradient:  0.1775051463833702
iteration : 4595
train acc:  0.828125
train loss:  0.45148834586143494
train gradient:  0.14314652132040795
iteration : 4596
train acc:  0.7265625
train loss:  0.5017305612564087
train gradient:  0.16660746489972056
iteration : 4597
train acc:  0.75
train loss:  0.543601393699646
train gradient:  0.151107279332435
iteration : 4598
train acc:  0.703125
train loss:  0.5255992412567139
train gradient:  0.13367433390427236
iteration : 4599
train acc:  0.765625
train loss:  0.505874514579773
train gradient:  0.12134884113784652
iteration : 4600
train acc:  0.6953125
train loss:  0.5759843587875366
train gradient:  0.18620411805223314
iteration : 4601
train acc:  0.703125
train loss:  0.5718725323677063
train gradient:  0.19349345604345436
iteration : 4602
train acc:  0.703125
train loss:  0.5210052728652954
train gradient:  0.19868838878199357
iteration : 4603
train acc:  0.765625
train loss:  0.47493016719818115
train gradient:  0.12318181957232706
iteration : 4604
train acc:  0.671875
train loss:  0.5607362985610962
train gradient:  0.14351143937915006
iteration : 4605
train acc:  0.71875
train loss:  0.5041883587837219
train gradient:  0.14595092264420603
iteration : 4606
train acc:  0.71875
train loss:  0.5230027437210083
train gradient:  0.15587690335502677
iteration : 4607
train acc:  0.7890625
train loss:  0.4691474437713623
train gradient:  0.14541642455728213
iteration : 4608
train acc:  0.78125
train loss:  0.46490442752838135
train gradient:  0.21336412716098074
iteration : 4609
train acc:  0.7421875
train loss:  0.5022846460342407
train gradient:  0.17204304320311695
iteration : 4610
train acc:  0.7421875
train loss:  0.4935409128665924
train gradient:  0.14087738752781748
iteration : 4611
train acc:  0.7265625
train loss:  0.5235663652420044
train gradient:  0.26695900453276544
iteration : 4612
train acc:  0.75
train loss:  0.5086501836776733
train gradient:  0.14730132302639112
iteration : 4613
train acc:  0.7421875
train loss:  0.4713650345802307
train gradient:  0.19631527189260323
iteration : 4614
train acc:  0.6796875
train loss:  0.5558924674987793
train gradient:  0.1851077707012629
iteration : 4615
train acc:  0.7421875
train loss:  0.4683607816696167
train gradient:  0.10372390604800945
iteration : 4616
train acc:  0.6875
train loss:  0.5307255983352661
train gradient:  0.1453259570734477
iteration : 4617
train acc:  0.6875
train loss:  0.581977367401123
train gradient:  0.19488384091820993
iteration : 4618
train acc:  0.7734375
train loss:  0.47743356227874756
train gradient:  0.14101658205678344
iteration : 4619
train acc:  0.6875
train loss:  0.5415146350860596
train gradient:  0.1521232105204282
iteration : 4620
train acc:  0.6875
train loss:  0.6027334928512573
train gradient:  0.17179251163005899
iteration : 4621
train acc:  0.7890625
train loss:  0.4360692501068115
train gradient:  0.09723634626725977
iteration : 4622
train acc:  0.71875
train loss:  0.5235380530357361
train gradient:  0.16646269093715688
iteration : 4623
train acc:  0.734375
train loss:  0.4535885453224182
train gradient:  0.13465990276559658
iteration : 4624
train acc:  0.671875
train loss:  0.5436701774597168
train gradient:  0.18457815082861478
iteration : 4625
train acc:  0.78125
train loss:  0.45157524943351746
train gradient:  0.11291110252120584
iteration : 4626
train acc:  0.703125
train loss:  0.5157055854797363
train gradient:  0.13416268336029746
iteration : 4627
train acc:  0.6953125
train loss:  0.546962320804596
train gradient:  0.17134575974221136
iteration : 4628
train acc:  0.71875
train loss:  0.533288836479187
train gradient:  0.15780132214242354
iteration : 4629
train acc:  0.6875
train loss:  0.5597492456436157
train gradient:  0.19639385204868576
iteration : 4630
train acc:  0.7265625
train loss:  0.5448317527770996
train gradient:  0.14292216122702348
iteration : 4631
train acc:  0.7734375
train loss:  0.45975518226623535
train gradient:  0.1288922455939975
iteration : 4632
train acc:  0.734375
train loss:  0.5199130773544312
train gradient:  0.1557014270721585
iteration : 4633
train acc:  0.7265625
train loss:  0.5070366859436035
train gradient:  0.15954204653125886
iteration : 4634
train acc:  0.671875
train loss:  0.5582618713378906
train gradient:  0.15317724926231507
iteration : 4635
train acc:  0.6796875
train loss:  0.5579102635383606
train gradient:  0.18367096991335077
iteration : 4636
train acc:  0.671875
train loss:  0.5867733955383301
train gradient:  0.20248821373437514
iteration : 4637
train acc:  0.734375
train loss:  0.5242695808410645
train gradient:  0.136843709277812
iteration : 4638
train acc:  0.734375
train loss:  0.5387645363807678
train gradient:  0.1525202671404685
iteration : 4639
train acc:  0.734375
train loss:  0.555072546005249
train gradient:  0.16187729486613514
iteration : 4640
train acc:  0.6953125
train loss:  0.5047672390937805
train gradient:  0.16480536396614648
iteration : 4641
train acc:  0.6875
train loss:  0.5338849425315857
train gradient:  0.16289056745854377
iteration : 4642
train acc:  0.765625
train loss:  0.46728312969207764
train gradient:  0.14152817421005864
iteration : 4643
train acc:  0.7578125
train loss:  0.4673723876476288
train gradient:  0.13866297349462242
iteration : 4644
train acc:  0.671875
train loss:  0.5413539409637451
train gradient:  0.15444895994179336
iteration : 4645
train acc:  0.6484375
train loss:  0.5491689443588257
train gradient:  0.1970603111942804
iteration : 4646
train acc:  0.75
train loss:  0.4438289403915405
train gradient:  0.13727080574663758
iteration : 4647
train acc:  0.7109375
train loss:  0.5327612161636353
train gradient:  0.17130852215061962
iteration : 4648
train acc:  0.734375
train loss:  0.47893276810646057
train gradient:  0.18271874409002856
iteration : 4649
train acc:  0.703125
train loss:  0.5503078699111938
train gradient:  0.16432114323529645
iteration : 4650
train acc:  0.7421875
train loss:  0.4798375368118286
train gradient:  0.13594045861449686
iteration : 4651
train acc:  0.796875
train loss:  0.45594707131385803
train gradient:  0.11558105145188702
iteration : 4652
train acc:  0.6640625
train loss:  0.5690433382987976
train gradient:  0.22151132744429286
iteration : 4653
train acc:  0.7109375
train loss:  0.5168393850326538
train gradient:  0.13851608347036515
iteration : 4654
train acc:  0.703125
train loss:  0.4998939335346222
train gradient:  0.2004677266749491
iteration : 4655
train acc:  0.6640625
train loss:  0.513990044593811
train gradient:  0.16541621448123922
iteration : 4656
train acc:  0.8046875
train loss:  0.4824047088623047
train gradient:  0.13439599795419577
iteration : 4657
train acc:  0.7109375
train loss:  0.5133931636810303
train gradient:  0.16353424114383633
iteration : 4658
train acc:  0.7734375
train loss:  0.4561169743537903
train gradient:  0.10978014033409512
iteration : 4659
train acc:  0.734375
train loss:  0.5174853801727295
train gradient:  0.15068401747248572
iteration : 4660
train acc:  0.6953125
train loss:  0.5694090127944946
train gradient:  0.18169761190357925
iteration : 4661
train acc:  0.703125
train loss:  0.519463300704956
train gradient:  0.13038107424011564
iteration : 4662
train acc:  0.7734375
train loss:  0.5205843448638916
train gradient:  0.15012700198542633
iteration : 4663
train acc:  0.796875
train loss:  0.4353366196155548
train gradient:  0.1410662191645487
iteration : 4664
train acc:  0.7734375
train loss:  0.49658969044685364
train gradient:  0.15102329837756828
iteration : 4665
train acc:  0.640625
train loss:  0.5829415321350098
train gradient:  0.20921822684293095
iteration : 4666
train acc:  0.6953125
train loss:  0.5093446373939514
train gradient:  0.14376833139811096
iteration : 4667
train acc:  0.71875
train loss:  0.517175555229187
train gradient:  0.14790834156135385
iteration : 4668
train acc:  0.765625
train loss:  0.4739856421947479
train gradient:  0.14201782834048826
iteration : 4669
train acc:  0.7578125
train loss:  0.48421016335487366
train gradient:  0.11504734602630053
iteration : 4670
train acc:  0.7265625
train loss:  0.5177140831947327
train gradient:  0.17528196295595022
iteration : 4671
train acc:  0.796875
train loss:  0.4840143918991089
train gradient:  0.125496398252338
iteration : 4672
train acc:  0.625
train loss:  0.537673830986023
train gradient:  0.2128178788323699
iteration : 4673
train acc:  0.7578125
train loss:  0.4726587235927582
train gradient:  0.15530405076363243
iteration : 4674
train acc:  0.765625
train loss:  0.4767833650112152
train gradient:  0.13129842077163828
iteration : 4675
train acc:  0.71875
train loss:  0.6217693090438843
train gradient:  0.25646836651974575
iteration : 4676
train acc:  0.7734375
train loss:  0.518625020980835
train gradient:  0.18378195640328798
iteration : 4677
train acc:  0.7734375
train loss:  0.556452214717865
train gradient:  0.15142313840430044
iteration : 4678
train acc:  0.7578125
train loss:  0.512238621711731
train gradient:  0.16026760370100016
iteration : 4679
train acc:  0.71875
train loss:  0.5151452422142029
train gradient:  0.134482195399122
iteration : 4680
train acc:  0.734375
train loss:  0.527990460395813
train gradient:  0.15629338363996859
iteration : 4681
train acc:  0.765625
train loss:  0.4740779399871826
train gradient:  0.1716261636003228
iteration : 4682
train acc:  0.7734375
train loss:  0.502238392829895
train gradient:  0.13650534530756636
iteration : 4683
train acc:  0.6953125
train loss:  0.5293498039245605
train gradient:  0.13704247465723612
iteration : 4684
train acc:  0.765625
train loss:  0.4784334897994995
train gradient:  0.14099191074525397
iteration : 4685
train acc:  0.6875
train loss:  0.6108794212341309
train gradient:  0.19278054639013453
iteration : 4686
train acc:  0.703125
train loss:  0.556238055229187
train gradient:  0.1690440477879046
iteration : 4687
train acc:  0.7734375
train loss:  0.4512631595134735
train gradient:  0.12793593014779175
iteration : 4688
train acc:  0.734375
train loss:  0.5363644361495972
train gradient:  0.16586731019281017
iteration : 4689
train acc:  0.6328125
train loss:  0.598968505859375
train gradient:  0.1878619236600172
iteration : 4690
train acc:  0.7421875
train loss:  0.48013412952423096
train gradient:  0.11108028480166073
iteration : 4691
train acc:  0.7890625
train loss:  0.4688456654548645
train gradient:  0.1279320857296522
iteration : 4692
train acc:  0.6796875
train loss:  0.5170681476593018
train gradient:  0.16671332200901923
iteration : 4693
train acc:  0.7265625
train loss:  0.5258411169052124
train gradient:  0.15957823609511784
iteration : 4694
train acc:  0.6953125
train loss:  0.5927675366401672
train gradient:  0.171640399930118
iteration : 4695
train acc:  0.796875
train loss:  0.4515491724014282
train gradient:  0.1023897200335194
iteration : 4696
train acc:  0.7109375
train loss:  0.5221948027610779
train gradient:  0.1555845967302329
iteration : 4697
train acc:  0.7265625
train loss:  0.513109564781189
train gradient:  0.15143761424430285
iteration : 4698
train acc:  0.734375
train loss:  0.5429918766021729
train gradient:  0.18602462814972648
iteration : 4699
train acc:  0.703125
train loss:  0.5496747493743896
train gradient:  0.17546785440703275
iteration : 4700
train acc:  0.71875
train loss:  0.5613709688186646
train gradient:  0.19287664046356132
iteration : 4701
train acc:  0.6640625
train loss:  0.5383679866790771
train gradient:  0.17169021963036651
iteration : 4702
train acc:  0.7109375
train loss:  0.5174821019172668
train gradient:  0.13931679374207367
iteration : 4703
train acc:  0.6875
train loss:  0.5329707860946655
train gradient:  0.22937160734525897
iteration : 4704
train acc:  0.7578125
train loss:  0.517334520816803
train gradient:  0.1848150618174893
iteration : 4705
train acc:  0.6796875
train loss:  0.5256438851356506
train gradient:  0.18583075849946065
iteration : 4706
train acc:  0.671875
train loss:  0.5988479852676392
train gradient:  0.23967024134820908
iteration : 4707
train acc:  0.640625
train loss:  0.5825390815734863
train gradient:  0.1833479131640144
iteration : 4708
train acc:  0.7109375
train loss:  0.4993871748447418
train gradient:  0.1376153131248812
iteration : 4709
train acc:  0.6875
train loss:  0.5457994937896729
train gradient:  0.150916912095114
iteration : 4710
train acc:  0.7265625
train loss:  0.5399876832962036
train gradient:  0.166091010451942
iteration : 4711
train acc:  0.703125
train loss:  0.4944227933883667
train gradient:  0.15842613209209966
iteration : 4712
train acc:  0.6484375
train loss:  0.5904414057731628
train gradient:  0.1971478870899311
iteration : 4713
train acc:  0.7734375
train loss:  0.4914686381816864
train gradient:  0.15138238382543973
iteration : 4714
train acc:  0.7109375
train loss:  0.5202249884605408
train gradient:  0.1519618842646555
iteration : 4715
train acc:  0.7578125
train loss:  0.47989344596862793
train gradient:  0.12634465550850857
iteration : 4716
train acc:  0.7734375
train loss:  0.48085400462150574
train gradient:  0.1306956283608557
iteration : 4717
train acc:  0.65625
train loss:  0.57331383228302
train gradient:  0.17129257622291888
iteration : 4718
train acc:  0.796875
train loss:  0.4266856610774994
train gradient:  0.15236703105952423
iteration : 4719
train acc:  0.765625
train loss:  0.46752646565437317
train gradient:  0.10346364421542711
iteration : 4720
train acc:  0.6640625
train loss:  0.602552056312561
train gradient:  0.21001888726815576
iteration : 4721
train acc:  0.6640625
train loss:  0.5523554682731628
train gradient:  0.18223679098317386
iteration : 4722
train acc:  0.765625
train loss:  0.4412905275821686
train gradient:  0.10455242124724928
iteration : 4723
train acc:  0.75
train loss:  0.4814775586128235
train gradient:  0.12668785053110038
iteration : 4724
train acc:  0.703125
train loss:  0.5619484186172485
train gradient:  0.184754737053729
iteration : 4725
train acc:  0.8046875
train loss:  0.4463264048099518
train gradient:  0.18855861951911268
iteration : 4726
train acc:  0.75
train loss:  0.49696585536003113
train gradient:  0.13679930772261467
iteration : 4727
train acc:  0.6875
train loss:  0.544552743434906
train gradient:  0.15203686963019614
iteration : 4728
train acc:  0.7265625
train loss:  0.5391966104507446
train gradient:  0.1409501093097667
iteration : 4729
train acc:  0.71875
train loss:  0.5126951336860657
train gradient:  0.1691204867766712
iteration : 4730
train acc:  0.7109375
train loss:  0.5120875239372253
train gradient:  0.15682349839872534
iteration : 4731
train acc:  0.7734375
train loss:  0.4392441213130951
train gradient:  0.1306861745616873
iteration : 4732
train acc:  0.7109375
train loss:  0.5326381921768188
train gradient:  0.16336911124808928
iteration : 4733
train acc:  0.6875
train loss:  0.5626773238182068
train gradient:  0.18809254184524404
iteration : 4734
train acc:  0.75
train loss:  0.4859541654586792
train gradient:  0.1463058259431697
iteration : 4735
train acc:  0.7578125
train loss:  0.4936731159687042
train gradient:  0.15970431377551852
iteration : 4736
train acc:  0.671875
train loss:  0.5104982256889343
train gradient:  0.13252971932852664
iteration : 4737
train acc:  0.734375
train loss:  0.4842758774757385
train gradient:  0.14450270693427736
iteration : 4738
train acc:  0.7421875
train loss:  0.4770253300666809
train gradient:  0.1338207472513277
iteration : 4739
train acc:  0.71875
train loss:  0.5206497311592102
train gradient:  0.14839270401929766
iteration : 4740
train acc:  0.75
train loss:  0.5018410682678223
train gradient:  0.1474339462611612
iteration : 4741
train acc:  0.78125
train loss:  0.4764382541179657
train gradient:  0.11119958748790487
iteration : 4742
train acc:  0.75
train loss:  0.47069159150123596
train gradient:  0.1019896137979188
iteration : 4743
train acc:  0.75
train loss:  0.5099925994873047
train gradient:  0.18170917073840506
iteration : 4744
train acc:  0.671875
train loss:  0.511753261089325
train gradient:  0.14873435344467484
iteration : 4745
train acc:  0.703125
train loss:  0.545610785484314
train gradient:  0.15879470389905365
iteration : 4746
train acc:  0.7578125
train loss:  0.4650987386703491
train gradient:  0.15795776001918876
iteration : 4747
train acc:  0.75
train loss:  0.5378572940826416
train gradient:  0.1875132066420528
iteration : 4748
train acc:  0.7734375
train loss:  0.4855029881000519
train gradient:  0.14817466305760324
iteration : 4749
train acc:  0.6875
train loss:  0.5773316621780396
train gradient:  0.17931344417742126
iteration : 4750
train acc:  0.71875
train loss:  0.5202019214630127
train gradient:  0.16923277972380069
iteration : 4751
train acc:  0.6953125
train loss:  0.5302416086196899
train gradient:  0.15359573291461454
iteration : 4752
train acc:  0.7421875
train loss:  0.46016401052474976
train gradient:  0.13699198450091182
iteration : 4753
train acc:  0.765625
train loss:  0.5156274437904358
train gradient:  0.13511747476690927
iteration : 4754
train acc:  0.7421875
train loss:  0.48636409640312195
train gradient:  0.1247558894775811
iteration : 4755
train acc:  0.796875
train loss:  0.49220985174179077
train gradient:  0.18062529386506282
iteration : 4756
train acc:  0.65625
train loss:  0.5765408873558044
train gradient:  0.1678891096058826
iteration : 4757
train acc:  0.6953125
train loss:  0.5387777090072632
train gradient:  0.1390429038284282
iteration : 4758
train acc:  0.7578125
train loss:  0.518645167350769
train gradient:  0.16748528053997838
iteration : 4759
train acc:  0.7265625
train loss:  0.5587708950042725
train gradient:  0.19357884624174743
iteration : 4760
train acc:  0.734375
train loss:  0.4964272677898407
train gradient:  0.14319328448276986
iteration : 4761
train acc:  0.828125
train loss:  0.4458705186843872
train gradient:  0.12621653233170466
iteration : 4762
train acc:  0.7421875
train loss:  0.47785845398902893
train gradient:  0.14435720344297415
iteration : 4763
train acc:  0.7421875
train loss:  0.5002614259719849
train gradient:  0.1833899401351049
iteration : 4764
train acc:  0.6875
train loss:  0.5844213962554932
train gradient:  0.2089778908855765
iteration : 4765
train acc:  0.7109375
train loss:  0.5404820442199707
train gradient:  0.16652752178018437
iteration : 4766
train acc:  0.78125
train loss:  0.48913130164146423
train gradient:  0.16515087434630354
iteration : 4767
train acc:  0.75
train loss:  0.46420818567276
train gradient:  0.152028134544826
iteration : 4768
train acc:  0.71875
train loss:  0.5123134851455688
train gradient:  0.16348528561844733
iteration : 4769
train acc:  0.75
train loss:  0.5008960366249084
train gradient:  0.14508442328717014
iteration : 4770
train acc:  0.703125
train loss:  0.4924145042896271
train gradient:  0.1487957818466369
iteration : 4771
train acc:  0.75
train loss:  0.4899091124534607
train gradient:  0.14569955921720285
iteration : 4772
train acc:  0.796875
train loss:  0.4674226641654968
train gradient:  0.16191096111123693
iteration : 4773
train acc:  0.6953125
train loss:  0.6014712452888489
train gradient:  0.21782411421394754
iteration : 4774
train acc:  0.7578125
train loss:  0.4729290008544922
train gradient:  0.12161144518397757
iteration : 4775
train acc:  0.640625
train loss:  0.534591794013977
train gradient:  0.20036664018705286
iteration : 4776
train acc:  0.6953125
train loss:  0.5169984698295593
train gradient:  0.13643386847402947
iteration : 4777
train acc:  0.6953125
train loss:  0.5186808109283447
train gradient:  0.14123833101880717
iteration : 4778
train acc:  0.734375
train loss:  0.5283894538879395
train gradient:  0.18864023814999215
iteration : 4779
train acc:  0.6640625
train loss:  0.6041628122329712
train gradient:  0.17450140467917077
iteration : 4780
train acc:  0.7109375
train loss:  0.5223459005355835
train gradient:  0.1457608953772305
iteration : 4781
train acc:  0.671875
train loss:  0.5796611309051514
train gradient:  0.15816350066598878
iteration : 4782
train acc:  0.7265625
train loss:  0.5566807985305786
train gradient:  0.16711520040556593
iteration : 4783
train acc:  0.65625
train loss:  0.5688439607620239
train gradient:  0.1785524446565966
iteration : 4784
train acc:  0.7265625
train loss:  0.4904780089855194
train gradient:  0.12878769595073503
iteration : 4785
train acc:  0.71875
train loss:  0.5307649374008179
train gradient:  0.1449893513103234
iteration : 4786
train acc:  0.734375
train loss:  0.49908357858657837
train gradient:  0.13981481859526657
iteration : 4787
train acc:  0.78125
train loss:  0.45633402466773987
train gradient:  0.13325702561199956
iteration : 4788
train acc:  0.6796875
train loss:  0.5691317915916443
train gradient:  0.18913572141053492
iteration : 4789
train acc:  0.7734375
train loss:  0.45135053992271423
train gradient:  0.1270173480547214
iteration : 4790
train acc:  0.7109375
train loss:  0.5571696758270264
train gradient:  0.19080373044281934
iteration : 4791
train acc:  0.734375
train loss:  0.5149409174919128
train gradient:  0.1452608678714663
iteration : 4792
train acc:  0.6953125
train loss:  0.5836825966835022
train gradient:  0.23068044787990755
iteration : 4793
train acc:  0.765625
train loss:  0.5119384527206421
train gradient:  0.1652532531080524
iteration : 4794
train acc:  0.75
train loss:  0.47244375944137573
train gradient:  0.11334105322579076
iteration : 4795
train acc:  0.7578125
train loss:  0.46485134959220886
train gradient:  0.1376760061514835
iteration : 4796
train acc:  0.7890625
train loss:  0.4298221468925476
train gradient:  0.11350548486332468
iteration : 4797
train acc:  0.6328125
train loss:  0.6012388467788696
train gradient:  0.20874254540265075
iteration : 4798
train acc:  0.6875
train loss:  0.5701583623886108
train gradient:  0.20133086293156272
iteration : 4799
train acc:  0.7109375
train loss:  0.5829786062240601
train gradient:  0.2592486288155987
iteration : 4800
train acc:  0.75
train loss:  0.4923993945121765
train gradient:  0.13805416792621658
iteration : 4801
train acc:  0.7421875
train loss:  0.5293300151824951
train gradient:  0.16953947519667023
iteration : 4802
train acc:  0.8125
train loss:  0.4747680425643921
train gradient:  0.14543246185699663
iteration : 4803
train acc:  0.78125
train loss:  0.48196637630462646
train gradient:  0.1361927001851439
iteration : 4804
train acc:  0.7578125
train loss:  0.47521913051605225
train gradient:  0.15272895726049054
iteration : 4805
train acc:  0.703125
train loss:  0.5699848532676697
train gradient:  0.22035721781323148
iteration : 4806
train acc:  0.7578125
train loss:  0.4963498115539551
train gradient:  0.19706524657668156
iteration : 4807
train acc:  0.71875
train loss:  0.5066908597946167
train gradient:  0.12266616436479383
iteration : 4808
train acc:  0.734375
train loss:  0.4736964702606201
train gradient:  0.13115012161897335
iteration : 4809
train acc:  0.65625
train loss:  0.5795483589172363
train gradient:  0.14598868487586775
iteration : 4810
train acc:  0.8125
train loss:  0.46303433179855347
train gradient:  0.12934729382755256
iteration : 4811
train acc:  0.78125
train loss:  0.48398852348327637
train gradient:  0.14380672851467324
iteration : 4812
train acc:  0.7734375
train loss:  0.4545849561691284
train gradient:  0.11031853860210186
iteration : 4813
train acc:  0.7421875
train loss:  0.4739186465740204
train gradient:  0.14131233343475183
iteration : 4814
train acc:  0.734375
train loss:  0.48550015687942505
train gradient:  0.12954622267979382
iteration : 4815
train acc:  0.7265625
train loss:  0.540075421333313
train gradient:  0.2538911171800024
iteration : 4816
train acc:  0.7421875
train loss:  0.5214979648590088
train gradient:  0.14534585242887982
iteration : 4817
train acc:  0.7421875
train loss:  0.5113762617111206
train gradient:  0.1710358934530291
iteration : 4818
train acc:  0.6875
train loss:  0.5938177108764648
train gradient:  0.19545818091617814
iteration : 4819
train acc:  0.7734375
train loss:  0.494990736246109
train gradient:  0.15561699140549204
iteration : 4820
train acc:  0.7421875
train loss:  0.503889799118042
train gradient:  0.145462767290976
iteration : 4821
train acc:  0.75
train loss:  0.48119857907295227
train gradient:  0.1225938886452901
iteration : 4822
train acc:  0.7421875
train loss:  0.4867475926876068
train gradient:  0.1406318353396363
iteration : 4823
train acc:  0.7421875
train loss:  0.4689098000526428
train gradient:  0.1664402539338401
iteration : 4824
train acc:  0.765625
train loss:  0.463645339012146
train gradient:  0.11195521802790162
iteration : 4825
train acc:  0.7265625
train loss:  0.5130155086517334
train gradient:  0.16157814687222363
iteration : 4826
train acc:  0.7109375
train loss:  0.5488023161888123
train gradient:  0.19131039496403363
iteration : 4827
train acc:  0.765625
train loss:  0.4684063792228699
train gradient:  0.12527620962052388
iteration : 4828
train acc:  0.703125
train loss:  0.5380333662033081
train gradient:  0.19334394269594052
iteration : 4829
train acc:  0.7734375
train loss:  0.49131131172180176
train gradient:  0.1132040220012679
iteration : 4830
train acc:  0.78125
train loss:  0.47991418838500977
train gradient:  0.13780876995446012
iteration : 4831
train acc:  0.6796875
train loss:  0.5520620942115784
train gradient:  0.17076891756002846
iteration : 4832
train acc:  0.78125
train loss:  0.4596741795539856
train gradient:  0.1262046021089816
iteration : 4833
train acc:  0.6796875
train loss:  0.5730785727500916
train gradient:  0.19693409368121112
iteration : 4834
train acc:  0.75
train loss:  0.49731022119522095
train gradient:  0.14793919249536047
iteration : 4835
train acc:  0.75
train loss:  0.47146326303482056
train gradient:  0.1184925158652255
iteration : 4836
train acc:  0.6953125
train loss:  0.5655168890953064
train gradient:  0.16094252301234577
iteration : 4837
train acc:  0.7265625
train loss:  0.531055748462677
train gradient:  0.15765767975300352
iteration : 4838
train acc:  0.734375
train loss:  0.506463348865509
train gradient:  0.14208156717657053
iteration : 4839
train acc:  0.6796875
train loss:  0.6163767576217651
train gradient:  0.21591265714626812
iteration : 4840
train acc:  0.7421875
train loss:  0.5150593519210815
train gradient:  0.15308653399699673
iteration : 4841
train acc:  0.7109375
train loss:  0.5345033407211304
train gradient:  0.1865257503658805
iteration : 4842
train acc:  0.7421875
train loss:  0.492632120847702
train gradient:  0.174705262555932
iteration : 4843
train acc:  0.7734375
train loss:  0.4519643783569336
train gradient:  0.1137633969243399
iteration : 4844
train acc:  0.71875
train loss:  0.4978577494621277
train gradient:  0.1260426457840117
iteration : 4845
train acc:  0.6953125
train loss:  0.6118265390396118
train gradient:  0.19943182278854507
iteration : 4846
train acc:  0.71875
train loss:  0.5111876726150513
train gradient:  0.14062993169117124
iteration : 4847
train acc:  0.7109375
train loss:  0.5326219201087952
train gradient:  0.1652201742234581
iteration : 4848
train acc:  0.78125
train loss:  0.5049425363540649
train gradient:  0.1515351806948213
iteration : 4849
train acc:  0.78125
train loss:  0.4851885437965393
train gradient:  0.18112966063373456
iteration : 4850
train acc:  0.6875
train loss:  0.5783374309539795
train gradient:  0.21069570513144997
iteration : 4851
train acc:  0.7734375
train loss:  0.5082660913467407
train gradient:  0.1741976819876886
iteration : 4852
train acc:  0.703125
train loss:  0.5107669830322266
train gradient:  0.14659313661138662
iteration : 4853
train acc:  0.6875
train loss:  0.6051020622253418
train gradient:  0.23654617898110597
iteration : 4854
train acc:  0.6953125
train loss:  0.4993698000907898
train gradient:  0.11998509458263647
iteration : 4855
train acc:  0.71875
train loss:  0.554961621761322
train gradient:  0.1716642488067769
iteration : 4856
train acc:  0.6875
train loss:  0.5850294828414917
train gradient:  0.17997663420316212
iteration : 4857
train acc:  0.75
train loss:  0.5254061222076416
train gradient:  0.17531802171485045
iteration : 4858
train acc:  0.7109375
train loss:  0.5292515158653259
train gradient:  0.18145994640816893
iteration : 4859
train acc:  0.765625
train loss:  0.47887229919433594
train gradient:  0.1525903641619772
iteration : 4860
train acc:  0.765625
train loss:  0.50367671251297
train gradient:  0.15636464859405672
iteration : 4861
train acc:  0.7265625
train loss:  0.535440981388092
train gradient:  0.18927544340892716
iteration : 4862
train acc:  0.75
train loss:  0.48818880319595337
train gradient:  0.20123310676120826
iteration : 4863
train acc:  0.7421875
train loss:  0.4844725430011749
train gradient:  0.11798703924568317
iteration : 4864
train acc:  0.734375
train loss:  0.4983478784561157
train gradient:  0.18261156211420282
iteration : 4865
train acc:  0.71875
train loss:  0.5730410814285278
train gradient:  0.14764591795797755
iteration : 4866
train acc:  0.71875
train loss:  0.5080654621124268
train gradient:  0.12581957033918806
iteration : 4867
train acc:  0.71875
train loss:  0.5046130418777466
train gradient:  0.1252819585943655
iteration : 4868
train acc:  0.734375
train loss:  0.5133598446846008
train gradient:  0.1743881619815852
iteration : 4869
train acc:  0.6953125
train loss:  0.5315589308738708
train gradient:  0.14601690240811005
iteration : 4870
train acc:  0.6953125
train loss:  0.5752252340316772
train gradient:  0.1674159075324252
iteration : 4871
train acc:  0.765625
train loss:  0.521407425403595
train gradient:  0.18134220214954933
iteration : 4872
train acc:  0.7109375
train loss:  0.5166608095169067
train gradient:  0.14840605335244159
iteration : 4873
train acc:  0.75
train loss:  0.4849582314491272
train gradient:  0.12228613703804157
iteration : 4874
train acc:  0.7265625
train loss:  0.4959242343902588
train gradient:  0.18938104876290007
iteration : 4875
train acc:  0.703125
train loss:  0.5789822936058044
train gradient:  0.14721102839891725
iteration : 4876
train acc:  0.78125
train loss:  0.47194260358810425
train gradient:  0.17569845165178333
iteration : 4877
train acc:  0.6953125
train loss:  0.5782613158226013
train gradient:  0.1809584416181236
iteration : 4878
train acc:  0.7421875
train loss:  0.4884140193462372
train gradient:  0.12157157235035142
iteration : 4879
train acc:  0.671875
train loss:  0.5887271165847778
train gradient:  0.16722884711636377
iteration : 4880
train acc:  0.75
train loss:  0.5091451406478882
train gradient:  0.15190175486941015
iteration : 4881
train acc:  0.75
train loss:  0.4844661355018616
train gradient:  0.1685510389071736
iteration : 4882
train acc:  0.7578125
train loss:  0.49276256561279297
train gradient:  0.15936593185794337
iteration : 4883
train acc:  0.71875
train loss:  0.5588451027870178
train gradient:  0.21692992546876433
iteration : 4884
train acc:  0.7421875
train loss:  0.5224863290786743
train gradient:  0.18469730121816652
iteration : 4885
train acc:  0.7578125
train loss:  0.49704793095588684
train gradient:  0.12772244972455332
iteration : 4886
train acc:  0.6875
train loss:  0.5702431201934814
train gradient:  0.25089172372200486
iteration : 4887
train acc:  0.703125
train loss:  0.553890585899353
train gradient:  0.16196035272338438
iteration : 4888
train acc:  0.6953125
train loss:  0.5340505242347717
train gradient:  0.1415496932268726
iteration : 4889
train acc:  0.7421875
train loss:  0.4904678463935852
train gradient:  0.14673289131066086
iteration : 4890
train acc:  0.703125
train loss:  0.561913013458252
train gradient:  0.22902175888126847
iteration : 4891
train acc:  0.7734375
train loss:  0.44579359889030457
train gradient:  0.1357694304712324
iteration : 4892
train acc:  0.8125
train loss:  0.4431552588939667
train gradient:  0.11756285728677852
iteration : 4893
train acc:  0.65625
train loss:  0.663925290107727
train gradient:  0.2597256925920544
iteration : 4894
train acc:  0.765625
train loss:  0.46207988262176514
train gradient:  0.14349446694441648
iteration : 4895
train acc:  0.6953125
train loss:  0.5612568855285645
train gradient:  0.17352475231771647
iteration : 4896
train acc:  0.6796875
train loss:  0.5516549348831177
train gradient:  0.16679310780904422
iteration : 4897
train acc:  0.671875
train loss:  0.5448500514030457
train gradient:  0.155563950596435
iteration : 4898
train acc:  0.6796875
train loss:  0.5420649647712708
train gradient:  0.13445132675236393
iteration : 4899
train acc:  0.734375
train loss:  0.5045267939567566
train gradient:  0.11907411108294222
iteration : 4900
train acc:  0.734375
train loss:  0.5245806574821472
train gradient:  0.1646893502843556
iteration : 4901
train acc:  0.7109375
train loss:  0.5093379020690918
train gradient:  0.1612476155361495
iteration : 4902
train acc:  0.640625
train loss:  0.5897946357727051
train gradient:  0.26036715852777964
iteration : 4903
train acc:  0.7421875
train loss:  0.44376733899116516
train gradient:  0.11650676367384981
iteration : 4904
train acc:  0.6484375
train loss:  0.5995758175849915
train gradient:  0.1979677855592194
iteration : 4905
train acc:  0.7734375
train loss:  0.4966224431991577
train gradient:  0.14090430978389132
iteration : 4906
train acc:  0.6484375
train loss:  0.5579171180725098
train gradient:  0.14076175854685918
iteration : 4907
train acc:  0.6640625
train loss:  0.5971826910972595
train gradient:  0.18114995983090268
iteration : 4908
train acc:  0.75
train loss:  0.5396629571914673
train gradient:  0.21383671242700103
iteration : 4909
train acc:  0.734375
train loss:  0.5646774768829346
train gradient:  0.15318046998566298
iteration : 4910
train acc:  0.7265625
train loss:  0.5108774900436401
train gradient:  0.14782333308309303
iteration : 4911
train acc:  0.7421875
train loss:  0.4611968696117401
train gradient:  0.12851535556975968
iteration : 4912
train acc:  0.6875
train loss:  0.5266481041908264
train gradient:  0.15452684559917437
iteration : 4913
train acc:  0.65625
train loss:  0.5395336747169495
train gradient:  0.17272844630659107
iteration : 4914
train acc:  0.703125
train loss:  0.557969868183136
train gradient:  0.20926924843351435
iteration : 4915
train acc:  0.71875
train loss:  0.5236910581588745
train gradient:  0.17762591126588184
iteration : 4916
train acc:  0.71875
train loss:  0.4894600510597229
train gradient:  0.13284712107868524
iteration : 4917
train acc:  0.7265625
train loss:  0.502883791923523
train gradient:  0.16295205711933236
iteration : 4918
train acc:  0.6875
train loss:  0.5350927114486694
train gradient:  0.15728406418827556
iteration : 4919
train acc:  0.7265625
train loss:  0.5154188871383667
train gradient:  0.16223076411374698
iteration : 4920
train acc:  0.7734375
train loss:  0.4560316503047943
train gradient:  0.11993487783701713
iteration : 4921
train acc:  0.765625
train loss:  0.505981981754303
train gradient:  0.157741218576036
iteration : 4922
train acc:  0.7578125
train loss:  0.4758860468864441
train gradient:  0.11909512880975591
iteration : 4923
train acc:  0.7578125
train loss:  0.5270954370498657
train gradient:  0.13340411325688667
iteration : 4924
train acc:  0.703125
train loss:  0.5815191268920898
train gradient:  0.20213158752502464
iteration : 4925
train acc:  0.7109375
train loss:  0.512962818145752
train gradient:  0.16941077381254133
iteration : 4926
train acc:  0.7421875
train loss:  0.49900731444358826
train gradient:  0.14116706389744013
iteration : 4927
train acc:  0.734375
train loss:  0.5304721593856812
train gradient:  0.1731493144249135
iteration : 4928
train acc:  0.78125
train loss:  0.480258047580719
train gradient:  0.1228657797486005
iteration : 4929
train acc:  0.703125
train loss:  0.5329381227493286
train gradient:  0.15054103275901387
iteration : 4930
train acc:  0.78125
train loss:  0.43194127082824707
train gradient:  0.09610967767707163
iteration : 4931
train acc:  0.7421875
train loss:  0.49746397137641907
train gradient:  0.16138717061114333
iteration : 4932
train acc:  0.6875
train loss:  0.5511199235916138
train gradient:  0.17789564387020024
iteration : 4933
train acc:  0.7421875
train loss:  0.4985997974872589
train gradient:  0.14425210420761478
iteration : 4934
train acc:  0.7890625
train loss:  0.5110483765602112
train gradient:  0.1248870858258029
iteration : 4935
train acc:  0.7109375
train loss:  0.5200876593589783
train gradient:  0.13697098377391398
iteration : 4936
train acc:  0.7421875
train loss:  0.5054370760917664
train gradient:  0.1373656289155272
iteration : 4937
train acc:  0.7421875
train loss:  0.48951685428619385
train gradient:  0.11200451020484012
iteration : 4938
train acc:  0.7734375
train loss:  0.459086149930954
train gradient:  0.11775316899958305
iteration : 4939
train acc:  0.7734375
train loss:  0.5125054121017456
train gradient:  0.15781598048100126
iteration : 4940
train acc:  0.71875
train loss:  0.5520544648170471
train gradient:  0.17773829801664737
iteration : 4941
train acc:  0.7109375
train loss:  0.5166007280349731
train gradient:  0.13635324504171675
iteration : 4942
train acc:  0.7265625
train loss:  0.5081442594528198
train gradient:  0.17158560854345922
iteration : 4943
train acc:  0.6953125
train loss:  0.5408638715744019
train gradient:  0.1506521790733918
iteration : 4944
train acc:  0.7265625
train loss:  0.5467062592506409
train gradient:  0.19559011969646195
iteration : 4945
train acc:  0.7421875
train loss:  0.5088396668434143
train gradient:  0.1525041431464652
iteration : 4946
train acc:  0.7265625
train loss:  0.5248799324035645
train gradient:  0.13433325885658892
iteration : 4947
train acc:  0.6875
train loss:  0.5115109086036682
train gradient:  0.12825104343440727
iteration : 4948
train acc:  0.7109375
train loss:  0.5544609427452087
train gradient:  0.14929820271407734
iteration : 4949
train acc:  0.75
train loss:  0.5220427513122559
train gradient:  0.2301810613042386
iteration : 4950
train acc:  0.7265625
train loss:  0.4906475245952606
train gradient:  0.1506222605131412
iteration : 4951
train acc:  0.7421875
train loss:  0.5005596876144409
train gradient:  0.14899318282707805
iteration : 4952
train acc:  0.6796875
train loss:  0.6199944019317627
train gradient:  0.2574229621184482
iteration : 4953
train acc:  0.7421875
train loss:  0.49683892726898193
train gradient:  0.13048095605547672
iteration : 4954
train acc:  0.6640625
train loss:  0.5386632680892944
train gradient:  0.16310952563892045
iteration : 4955
train acc:  0.7578125
train loss:  0.4623695909976959
train gradient:  0.1982573487089021
iteration : 4956
train acc:  0.71875
train loss:  0.49369364976882935
train gradient:  0.14635225745407926
iteration : 4957
train acc:  0.7109375
train loss:  0.533291220664978
train gradient:  0.16917665673454463
iteration : 4958
train acc:  0.7265625
train loss:  0.5424096584320068
train gradient:  0.1518655698191657
iteration : 4959
train acc:  0.65625
train loss:  0.595604658126831
train gradient:  0.17547224247213444
iteration : 4960
train acc:  0.671875
train loss:  0.5519264340400696
train gradient:  0.15976617137387017
iteration : 4961
train acc:  0.78125
train loss:  0.4691697359085083
train gradient:  0.10549733123023369
iteration : 4962
train acc:  0.7265625
train loss:  0.5785863399505615
train gradient:  0.17603158375172362
iteration : 4963
train acc:  0.8046875
train loss:  0.5087399482727051
train gradient:  0.16606709486471782
iteration : 4964
train acc:  0.6875
train loss:  0.49958232045173645
train gradient:  0.12152357917869576
iteration : 4965
train acc:  0.7734375
train loss:  0.4581332206726074
train gradient:  0.11245857046802898
iteration : 4966
train acc:  0.65625
train loss:  0.5931076407432556
train gradient:  0.20806485384507767
iteration : 4967
train acc:  0.7265625
train loss:  0.5235702395439148
train gradient:  0.14573168194847397
iteration : 4968
train acc:  0.703125
train loss:  0.5422860383987427
train gradient:  0.165350376689569
iteration : 4969
train acc:  0.7734375
train loss:  0.47147440910339355
train gradient:  0.1317346119250251
iteration : 4970
train acc:  0.6875
train loss:  0.5550313591957092
train gradient:  0.19126970230545057
iteration : 4971
train acc:  0.7578125
train loss:  0.5305896997451782
train gradient:  0.15312486592024732
iteration : 4972
train acc:  0.703125
train loss:  0.5326213836669922
train gradient:  0.18071272882889167
iteration : 4973
train acc:  0.71875
train loss:  0.5432618856430054
train gradient:  0.15484754390900385
iteration : 4974
train acc:  0.7734375
train loss:  0.4943656325340271
train gradient:  0.13325119199946978
iteration : 4975
train acc:  0.75
train loss:  0.4847193658351898
train gradient:  0.14839516021856505
iteration : 4976
train acc:  0.703125
train loss:  0.5584801435470581
train gradient:  0.15247374030248262
iteration : 4977
train acc:  0.75
train loss:  0.48675739765167236
train gradient:  0.13321544580853112
iteration : 4978
train acc:  0.7421875
train loss:  0.5092713832855225
train gradient:  0.15893734555363892
iteration : 4979
train acc:  0.7265625
train loss:  0.5479074716567993
train gradient:  0.18272419932004014
iteration : 4980
train acc:  0.734375
train loss:  0.5305505990982056
train gradient:  0.17532432491616046
iteration : 4981
train acc:  0.8046875
train loss:  0.44540658593177795
train gradient:  0.13515023473583668
iteration : 4982
train acc:  0.734375
train loss:  0.5150699019432068
train gradient:  0.2002959027386434
iteration : 4983
train acc:  0.7265625
train loss:  0.4956551790237427
train gradient:  0.12622979463695003
iteration : 4984
train acc:  0.6953125
train loss:  0.4946361482143402
train gradient:  0.20369615802846897
iteration : 4985
train acc:  0.796875
train loss:  0.44257423281669617
train gradient:  0.12408280524358303
iteration : 4986
train acc:  0.75
train loss:  0.4844735860824585
train gradient:  0.12292074065591983
iteration : 4987
train acc:  0.75
train loss:  0.47967782616615295
train gradient:  0.13249813361082166
iteration : 4988
train acc:  0.75
train loss:  0.4785720705986023
train gradient:  0.11149810802939507
iteration : 4989
train acc:  0.7421875
train loss:  0.5184398889541626
train gradient:  0.14898624350136502
iteration : 4990
train acc:  0.6953125
train loss:  0.515557587146759
train gradient:  0.14267677802280682
iteration : 4991
train acc:  0.71875
train loss:  0.491633802652359
train gradient:  0.13098734895385777
iteration : 4992
train acc:  0.7421875
train loss:  0.5066627264022827
train gradient:  0.14076894053547312
iteration : 4993
train acc:  0.7890625
train loss:  0.4079107344150543
train gradient:  0.09737445519263556
iteration : 4994
train acc:  0.625
train loss:  0.5835115909576416
train gradient:  0.19657220734228636
iteration : 4995
train acc:  0.703125
train loss:  0.5024648904800415
train gradient:  0.17838477913894407
iteration : 4996
train acc:  0.765625
train loss:  0.5024921894073486
train gradient:  0.14333754173695826
iteration : 4997
train acc:  0.7578125
train loss:  0.48286110162734985
train gradient:  0.11553239498500747
iteration : 4998
train acc:  0.7265625
train loss:  0.5513611435890198
train gradient:  0.15964420199382684
iteration : 4999
train acc:  0.75
train loss:  0.5102477073669434
train gradient:  0.18410033752785093
iteration : 5000
train acc:  0.7421875
train loss:  0.525032639503479
train gradient:  0.14818273791999814
iteration : 5001
train acc:  0.703125
train loss:  0.5070712566375732
train gradient:  0.14145373691153426
iteration : 5002
train acc:  0.671875
train loss:  0.5684723854064941
train gradient:  0.1793946532920084
iteration : 5003
train acc:  0.6953125
train loss:  0.5324792861938477
train gradient:  0.13689371094255665
iteration : 5004
train acc:  0.78125
train loss:  0.4796944260597229
train gradient:  0.1466086430336344
iteration : 5005
train acc:  0.6484375
train loss:  0.5902078151702881
train gradient:  0.1895693372515539
iteration : 5006
train acc:  0.6484375
train loss:  0.6311418414115906
train gradient:  0.17884035906345425
iteration : 5007
train acc:  0.75
train loss:  0.44288328289985657
train gradient:  0.12306491018673853
iteration : 5008
train acc:  0.7109375
train loss:  0.5144402384757996
train gradient:  0.13686698714119735
iteration : 5009
train acc:  0.7109375
train loss:  0.5305229425430298
train gradient:  0.1627716854082255
iteration : 5010
train acc:  0.7734375
train loss:  0.4833628833293915
train gradient:  0.13172902797097913
iteration : 5011
train acc:  0.703125
train loss:  0.5230591297149658
train gradient:  0.14513724326098731
iteration : 5012
train acc:  0.7265625
train loss:  0.49080488085746765
train gradient:  0.11473960831291755
iteration : 5013
train acc:  0.7109375
train loss:  0.5211436152458191
train gradient:  0.18302412836080698
iteration : 5014
train acc:  0.6953125
train loss:  0.5217173099517822
train gradient:  0.14540978290162365
iteration : 5015
train acc:  0.75
train loss:  0.5127378106117249
train gradient:  0.12612308071011058
iteration : 5016
train acc:  0.7421875
train loss:  0.5213724374771118
train gradient:  0.14064355882518198
iteration : 5017
train acc:  0.734375
train loss:  0.5219461917877197
train gradient:  0.12502769043386566
iteration : 5018
train acc:  0.765625
train loss:  0.5004812479019165
train gradient:  0.1273280694714162
iteration : 5019
train acc:  0.71875
train loss:  0.54250168800354
train gradient:  0.19471701901766061
iteration : 5020
train acc:  0.8046875
train loss:  0.4531311094760895
train gradient:  0.14618133603409256
iteration : 5021
train acc:  0.671875
train loss:  0.5449092388153076
train gradient:  0.18234419328143614
iteration : 5022
train acc:  0.78125
train loss:  0.47969669103622437
train gradient:  0.1368400538524131
iteration : 5023
train acc:  0.734375
train loss:  0.495239794254303
train gradient:  0.13566143684971543
iteration : 5024
train acc:  0.6953125
train loss:  0.5133640766143799
train gradient:  0.14057889833895526
iteration : 5025
train acc:  0.7734375
train loss:  0.461433470249176
train gradient:  0.15292898903199537
iteration : 5026
train acc:  0.7734375
train loss:  0.4209485650062561
train gradient:  0.12101151215723605
iteration : 5027
train acc:  0.65625
train loss:  0.658667802810669
train gradient:  0.1870307753701282
iteration : 5028
train acc:  0.671875
train loss:  0.5677087306976318
train gradient:  0.19769272080303762
iteration : 5029
train acc:  0.7421875
train loss:  0.47039273381233215
train gradient:  0.1141854471085404
iteration : 5030
train acc:  0.703125
train loss:  0.5102641582489014
train gradient:  0.14337430580695137
iteration : 5031
train acc:  0.703125
train loss:  0.5413540601730347
train gradient:  0.17200136170666858
iteration : 5032
train acc:  0.7734375
train loss:  0.4789731502532959
train gradient:  0.11999348142547196
iteration : 5033
train acc:  0.7421875
train loss:  0.47104278206825256
train gradient:  0.10333613112589912
iteration : 5034
train acc:  0.7265625
train loss:  0.5052034854888916
train gradient:  0.13238165059400647
iteration : 5035
train acc:  0.7734375
train loss:  0.4686804413795471
train gradient:  0.14580005601857052
iteration : 5036
train acc:  0.75
train loss:  0.49756765365600586
train gradient:  0.1677890359999045
iteration : 5037
train acc:  0.78125
train loss:  0.5523692965507507
train gradient:  0.16879227173530503
iteration : 5038
train acc:  0.71875
train loss:  0.49092209339141846
train gradient:  0.15195003602838897
iteration : 5039
train acc:  0.7890625
train loss:  0.4514462351799011
train gradient:  0.11274093077492135
iteration : 5040
train acc:  0.7109375
train loss:  0.5435481071472168
train gradient:  0.15424031603024374
iteration : 5041
train acc:  0.765625
train loss:  0.42896193265914917
train gradient:  0.11795162368634955
iteration : 5042
train acc:  0.7890625
train loss:  0.4507502317428589
train gradient:  0.1404273910948149
iteration : 5043
train acc:  0.6875
train loss:  0.5061984062194824
train gradient:  0.138433342424908
iteration : 5044
train acc:  0.6640625
train loss:  0.5848455429077148
train gradient:  0.16438584937580603
iteration : 5045
train acc:  0.7734375
train loss:  0.4774702191352844
train gradient:  0.12677971602408908
iteration : 5046
train acc:  0.7578125
train loss:  0.5447473526000977
train gradient:  0.14657414385597428
iteration : 5047
train acc:  0.7109375
train loss:  0.540399432182312
train gradient:  0.15862405238936628
iteration : 5048
train acc:  0.6875
train loss:  0.5392478704452515
train gradient:  0.15571924010539184
iteration : 5049
train acc:  0.796875
train loss:  0.4553843140602112
train gradient:  0.11450179704994384
iteration : 5050
train acc:  0.7109375
train loss:  0.5579701662063599
train gradient:  0.16543936246005106
iteration : 5051
train acc:  0.703125
train loss:  0.5946372151374817
train gradient:  0.17461556207925744
iteration : 5052
train acc:  0.71875
train loss:  0.501275897026062
train gradient:  0.12357264401632184
iteration : 5053
train acc:  0.6796875
train loss:  0.6118727922439575
train gradient:  0.18880839603336824
iteration : 5054
train acc:  0.7578125
train loss:  0.47103172540664673
train gradient:  0.13461681433475015
iteration : 5055
train acc:  0.6875
train loss:  0.5319753885269165
train gradient:  0.14004317716300657
iteration : 5056
train acc:  0.734375
train loss:  0.47480157017707825
train gradient:  0.12221203915962164
iteration : 5057
train acc:  0.734375
train loss:  0.503063440322876
train gradient:  0.11712379553609509
iteration : 5058
train acc:  0.6796875
train loss:  0.6028938293457031
train gradient:  0.247487852720076
iteration : 5059
train acc:  0.71875
train loss:  0.49969449639320374
train gradient:  0.14741847597505153
iteration : 5060
train acc:  0.6953125
train loss:  0.5319884419441223
train gradient:  0.13245254959479913
iteration : 5061
train acc:  0.796875
train loss:  0.450894296169281
train gradient:  0.12425793644702231
iteration : 5062
train acc:  0.7734375
train loss:  0.4810454845428467
train gradient:  0.12741712827920632
iteration : 5063
train acc:  0.7890625
train loss:  0.46983057260513306
train gradient:  0.14305415309629674
iteration : 5064
train acc:  0.71875
train loss:  0.4976428747177124
train gradient:  0.1733629353621795
iteration : 5065
train acc:  0.7421875
train loss:  0.4996730387210846
train gradient:  0.14712522887672685
iteration : 5066
train acc:  0.7109375
train loss:  0.5067676305770874
train gradient:  0.13908206886759966
iteration : 5067
train acc:  0.6875
train loss:  0.5949492454528809
train gradient:  0.2111782822433207
iteration : 5068
train acc:  0.7890625
train loss:  0.4182102680206299
train gradient:  0.10347417712387573
iteration : 5069
train acc:  0.8046875
train loss:  0.4499291777610779
train gradient:  0.11000349639815264
iteration : 5070
train acc:  0.671875
train loss:  0.5547472238540649
train gradient:  0.1541635703469653
iteration : 5071
train acc:  0.7265625
train loss:  0.5184953212738037
train gradient:  0.18855668763219796
iteration : 5072
train acc:  0.75
train loss:  0.5004411935806274
train gradient:  0.14196210488809363
iteration : 5073
train acc:  0.6796875
train loss:  0.5811726450920105
train gradient:  0.15913416251971935
iteration : 5074
train acc:  0.703125
train loss:  0.47693103551864624
train gradient:  0.13774121619489427
iteration : 5075
train acc:  0.75
train loss:  0.47865793108940125
train gradient:  0.16563485123365246
iteration : 5076
train acc:  0.7890625
train loss:  0.47474318742752075
train gradient:  0.1806773692592924
iteration : 5077
train acc:  0.703125
train loss:  0.5560003519058228
train gradient:  0.16526550925143063
iteration : 5078
train acc:  0.796875
train loss:  0.5299030542373657
train gradient:  0.13927314556796938
iteration : 5079
train acc:  0.6953125
train loss:  0.5289243459701538
train gradient:  0.1599026941977923
iteration : 5080
train acc:  0.7734375
train loss:  0.5111935138702393
train gradient:  0.1328639565607756
iteration : 5081
train acc:  0.734375
train loss:  0.521753191947937
train gradient:  0.1679605579577599
iteration : 5082
train acc:  0.6796875
train loss:  0.5181774497032166
train gradient:  0.13385762889698166
iteration : 5083
train acc:  0.7265625
train loss:  0.5088117122650146
train gradient:  0.1301394199062949
iteration : 5084
train acc:  0.8125
train loss:  0.4465342164039612
train gradient:  0.13258916973222035
iteration : 5085
train acc:  0.7734375
train loss:  0.49009159207344055
train gradient:  0.14021298267531518
iteration : 5086
train acc:  0.7578125
train loss:  0.48700883984565735
train gradient:  0.11587490977445895
iteration : 5087
train acc:  0.703125
train loss:  0.5410040020942688
train gradient:  0.18401303022160514
iteration : 5088
train acc:  0.75
train loss:  0.4891411066055298
train gradient:  0.1426816818695579
iteration : 5089
train acc:  0.71875
train loss:  0.5365782976150513
train gradient:  0.13434376414372573
iteration : 5090
train acc:  0.71875
train loss:  0.47391968965530396
train gradient:  0.1276631394421715
iteration : 5091
train acc:  0.71875
train loss:  0.543476939201355
train gradient:  0.15804565220813707
iteration : 5092
train acc:  0.8203125
train loss:  0.4425899386405945
train gradient:  0.11898766354999687
iteration : 5093
train acc:  0.796875
train loss:  0.4481287896633148
train gradient:  0.09534954837288807
iteration : 5094
train acc:  0.71875
train loss:  0.5535647869110107
train gradient:  0.1818584367176192
iteration : 5095
train acc:  0.7109375
train loss:  0.5468567609786987
train gradient:  0.1714076274287923
iteration : 5096
train acc:  0.71875
train loss:  0.5031421184539795
train gradient:  0.14142336055714747
iteration : 5097
train acc:  0.765625
train loss:  0.46373996138572693
train gradient:  0.1321885329790102
iteration : 5098
train acc:  0.7578125
train loss:  0.48789137601852417
train gradient:  0.15325555626950835
iteration : 5099
train acc:  0.7109375
train loss:  0.5317400097846985
train gradient:  0.1728953217252468
iteration : 5100
train acc:  0.7421875
train loss:  0.4920036494731903
train gradient:  0.20282814732287266
iteration : 5101
train acc:  0.7265625
train loss:  0.5270757675170898
train gradient:  0.14546269107216558
iteration : 5102
train acc:  0.65625
train loss:  0.5736607909202576
train gradient:  0.16926980243444797
iteration : 5103
train acc:  0.6484375
train loss:  0.5873061418533325
train gradient:  0.1972119895295235
iteration : 5104
train acc:  0.71875
train loss:  0.4846978485584259
train gradient:  0.1284837451326012
iteration : 5105
train acc:  0.6953125
train loss:  0.5617961883544922
train gradient:  0.18826172311356643
iteration : 5106
train acc:  0.75
train loss:  0.5240607261657715
train gradient:  0.16011852784691827
iteration : 5107
train acc:  0.7734375
train loss:  0.48114094138145447
train gradient:  0.16385450375726934
iteration : 5108
train acc:  0.734375
train loss:  0.5117092728614807
train gradient:  0.14205761954852691
iteration : 5109
train acc:  0.7578125
train loss:  0.5103633403778076
train gradient:  0.12066535808161553
iteration : 5110
train acc:  0.6640625
train loss:  0.553968608379364
train gradient:  0.15561233275869754
iteration : 5111
train acc:  0.734375
train loss:  0.48174506425857544
train gradient:  0.12625102488068232
iteration : 5112
train acc:  0.6796875
train loss:  0.5842792987823486
train gradient:  0.16237286794762695
iteration : 5113
train acc:  0.703125
train loss:  0.5256967544555664
train gradient:  0.15051657504208782
iteration : 5114
train acc:  0.7890625
train loss:  0.48955047130584717
train gradient:  0.12723111020287997
iteration : 5115
train acc:  0.75
train loss:  0.5051508545875549
train gradient:  0.13416133983099376
iteration : 5116
train acc:  0.703125
train loss:  0.5192997455596924
train gradient:  0.1935368317439068
iteration : 5117
train acc:  0.7578125
train loss:  0.4727243185043335
train gradient:  0.12285266782892693
iteration : 5118
train acc:  0.7109375
train loss:  0.4776685833930969
train gradient:  0.1464583955135604
iteration : 5119
train acc:  0.765625
train loss:  0.49895787239074707
train gradient:  0.15108722271767128
iteration : 5120
train acc:  0.75
train loss:  0.4861103892326355
train gradient:  0.12780694029911688
iteration : 5121
train acc:  0.765625
train loss:  0.4436699450016022
train gradient:  0.11863013367580705
iteration : 5122
train acc:  0.7109375
train loss:  0.5320186614990234
train gradient:  0.16029468432949212
iteration : 5123
train acc:  0.7109375
train loss:  0.5512508153915405
train gradient:  0.1386387358082413
iteration : 5124
train acc:  0.7265625
train loss:  0.5434280037879944
train gradient:  0.15653700580641344
iteration : 5125
train acc:  0.671875
train loss:  0.582064688205719
train gradient:  0.22910286588009415
iteration : 5126
train acc:  0.734375
train loss:  0.45899245142936707
train gradient:  0.13853164075590252
iteration : 5127
train acc:  0.6875
train loss:  0.5410309433937073
train gradient:  0.13599469479182652
iteration : 5128
train acc:  0.6953125
train loss:  0.5395667552947998
train gradient:  0.16681582855728988
iteration : 5129
train acc:  0.7734375
train loss:  0.47711533308029175
train gradient:  0.1504340454251576
iteration : 5130
train acc:  0.5859375
train loss:  0.6480622887611389
train gradient:  0.2272800497085078
iteration : 5131
train acc:  0.6953125
train loss:  0.5104930400848389
train gradient:  0.18980736466476478
iteration : 5132
train acc:  0.78125
train loss:  0.4655184745788574
train gradient:  0.15758245097727575
iteration : 5133
train acc:  0.78125
train loss:  0.4702513813972473
train gradient:  0.12500516931808964
iteration : 5134
train acc:  0.765625
train loss:  0.4812327027320862
train gradient:  0.12049609839136717
iteration : 5135
train acc:  0.7109375
train loss:  0.5623176097869873
train gradient:  0.17530896485852
iteration : 5136
train acc:  0.71875
train loss:  0.524219274520874
train gradient:  0.14796770497774533
iteration : 5137
train acc:  0.7109375
train loss:  0.4890364706516266
train gradient:  0.13670106756379488
iteration : 5138
train acc:  0.78125
train loss:  0.4822198152542114
train gradient:  0.137850165800336
iteration : 5139
train acc:  0.671875
train loss:  0.5884405970573425
train gradient:  0.1862645378121331
iteration : 5140
train acc:  0.765625
train loss:  0.46861016750335693
train gradient:  0.14411321497083046
iteration : 5141
train acc:  0.78125
train loss:  0.44136935472488403
train gradient:  0.11709903644850564
iteration : 5142
train acc:  0.7421875
train loss:  0.46656447649002075
train gradient:  0.12397142534753317
iteration : 5143
train acc:  0.7109375
train loss:  0.5113983750343323
train gradient:  0.16831307433059697
iteration : 5144
train acc:  0.7890625
train loss:  0.4857963025569916
train gradient:  0.13860648230043793
iteration : 5145
train acc:  0.6875
train loss:  0.5488404631614685
train gradient:  0.15334284168342296
iteration : 5146
train acc:  0.7265625
train loss:  0.5484816431999207
train gradient:  0.19873237548473333
iteration : 5147
train acc:  0.671875
train loss:  0.5907002687454224
train gradient:  0.19039792392570098
iteration : 5148
train acc:  0.703125
train loss:  0.5613020062446594
train gradient:  0.14001116717048667
iteration : 5149
train acc:  0.75
train loss:  0.5207124352455139
train gradient:  0.1624952410397214
iteration : 5150
train acc:  0.7421875
train loss:  0.45533132553100586
train gradient:  0.09794439137272903
iteration : 5151
train acc:  0.6484375
train loss:  0.5489827394485474
train gradient:  0.1266330324953455
iteration : 5152
train acc:  0.75
train loss:  0.4836532473564148
train gradient:  0.15680736189922603
iteration : 5153
train acc:  0.71875
train loss:  0.5538850426673889
train gradient:  0.2376477791035472
iteration : 5154
train acc:  0.734375
train loss:  0.5075246691703796
train gradient:  0.13139530637714944
iteration : 5155
train acc:  0.6953125
train loss:  0.5407687425613403
train gradient:  0.16582869541828943
iteration : 5156
train acc:  0.7421875
train loss:  0.5511616468429565
train gradient:  0.15142385788115947
iteration : 5157
train acc:  0.6875
train loss:  0.5648077726364136
train gradient:  0.1557412825830891
iteration : 5158
train acc:  0.75
train loss:  0.5024325847625732
train gradient:  0.11657180012994874
iteration : 5159
train acc:  0.6953125
train loss:  0.4855273365974426
train gradient:  0.1608007398389049
iteration : 5160
train acc:  0.6953125
train loss:  0.5444172620773315
train gradient:  0.20057042448051324
iteration : 5161
train acc:  0.71875
train loss:  0.5217670798301697
train gradient:  0.18960253858513393
iteration : 5162
train acc:  0.84375
train loss:  0.39605337381362915
train gradient:  0.11375410346264163
iteration : 5163
train acc:  0.703125
train loss:  0.48832082748413086
train gradient:  0.13452137345745338
iteration : 5164
train acc:  0.7890625
train loss:  0.47103381156921387
train gradient:  0.12432390168255666
iteration : 5165
train acc:  0.6875
train loss:  0.5329623818397522
train gradient:  0.1597392849315362
iteration : 5166
train acc:  0.734375
train loss:  0.5146546959877014
train gradient:  0.1592689329449239
iteration : 5167
train acc:  0.7578125
train loss:  0.47504737973213196
train gradient:  0.14415198530039697
iteration : 5168
train acc:  0.765625
train loss:  0.4884735345840454
train gradient:  0.15470256566176793
iteration : 5169
train acc:  0.71875
train loss:  0.5363242626190186
train gradient:  0.15418031816051364
iteration : 5170
train acc:  0.6796875
train loss:  0.6023688912391663
train gradient:  0.22853751834985037
iteration : 5171
train acc:  0.7109375
train loss:  0.5051445364952087
train gradient:  0.1588611300606983
iteration : 5172
train acc:  0.78125
train loss:  0.4629994034767151
train gradient:  0.14827235320357846
iteration : 5173
train acc:  0.6796875
train loss:  0.5726794004440308
train gradient:  0.1603474402295329
iteration : 5174
train acc:  0.765625
train loss:  0.5018613338470459
train gradient:  0.17507292221780318
iteration : 5175
train acc:  0.71875
train loss:  0.5566021203994751
train gradient:  0.16368141690886295
iteration : 5176
train acc:  0.734375
train loss:  0.5082489252090454
train gradient:  0.14594704438781209
iteration : 5177
train acc:  0.71875
train loss:  0.5244289040565491
train gradient:  0.19025585213912133
iteration : 5178
train acc:  0.765625
train loss:  0.4562772214412689
train gradient:  0.10158346491898773
iteration : 5179
train acc:  0.6640625
train loss:  0.5734301805496216
train gradient:  0.1815707661644349
iteration : 5180
train acc:  0.75
train loss:  0.4869600236415863
train gradient:  0.11286568192088543
iteration : 5181
train acc:  0.734375
train loss:  0.5450974702835083
train gradient:  0.15888320173263037
iteration : 5182
train acc:  0.7734375
train loss:  0.45186853408813477
train gradient:  0.11967446799861041
iteration : 5183
train acc:  0.6484375
train loss:  0.5833097696304321
train gradient:  0.1908835154152949
iteration : 5184
train acc:  0.6953125
train loss:  0.5330860614776611
train gradient:  0.1714301902649603
iteration : 5185
train acc:  0.6796875
train loss:  0.5614418983459473
train gradient:  0.16692522897833478
iteration : 5186
train acc:  0.6953125
train loss:  0.5438927412033081
train gradient:  0.15023022486763668
iteration : 5187
train acc:  0.6796875
train loss:  0.6063516139984131
train gradient:  0.17002665003685985
iteration : 5188
train acc:  0.78125
train loss:  0.48176759481430054
train gradient:  0.1350033727042398
iteration : 5189
train acc:  0.765625
train loss:  0.49503883719444275
train gradient:  0.17683166187642285
iteration : 5190
train acc:  0.765625
train loss:  0.4781815707683563
train gradient:  0.12880478227948527
iteration : 5191
train acc:  0.703125
train loss:  0.5436654686927795
train gradient:  0.17742102601998222
iteration : 5192
train acc:  0.6796875
train loss:  0.5235835909843445
train gradient:  0.1384679308687075
iteration : 5193
train acc:  0.75
train loss:  0.4478805661201477
train gradient:  0.11643751161711466
iteration : 5194
train acc:  0.78125
train loss:  0.46014928817749023
train gradient:  0.11086575051933778
iteration : 5195
train acc:  0.6796875
train loss:  0.5777678489685059
train gradient:  0.15174428926237538
iteration : 5196
train acc:  0.765625
train loss:  0.4891406297683716
train gradient:  0.15018830267292704
iteration : 5197
train acc:  0.75
train loss:  0.4502958059310913
train gradient:  0.1337768474792505
iteration : 5198
train acc:  0.703125
train loss:  0.4829939603805542
train gradient:  0.10809911166832932
iteration : 5199
train acc:  0.7109375
train loss:  0.5235379934310913
train gradient:  0.15202230131023542
iteration : 5200
train acc:  0.7421875
train loss:  0.5546849370002747
train gradient:  0.17929160759793647
iteration : 5201
train acc:  0.7734375
train loss:  0.44698625802993774
train gradient:  0.1275586566733219
iteration : 5202
train acc:  0.6640625
train loss:  0.5252730846405029
train gradient:  0.15909447548134908
iteration : 5203
train acc:  0.671875
train loss:  0.5682199001312256
train gradient:  0.19060863552516888
iteration : 5204
train acc:  0.7109375
train loss:  0.5833244323730469
train gradient:  0.19542650918374155
iteration : 5205
train acc:  0.78125
train loss:  0.4611765444278717
train gradient:  0.12355051597417818
iteration : 5206
train acc:  0.71875
train loss:  0.5259904861450195
train gradient:  0.16786576223276906
iteration : 5207
train acc:  0.7265625
train loss:  0.46390533447265625
train gradient:  0.14362143770441205
iteration : 5208
train acc:  0.7109375
train loss:  0.5557109117507935
train gradient:  0.16760181131079782
iteration : 5209
train acc:  0.7265625
train loss:  0.5138983130455017
train gradient:  0.1771040822477447
iteration : 5210
train acc:  0.75
train loss:  0.4911038279533386
train gradient:  0.12142734814314812
iteration : 5211
train acc:  0.7109375
train loss:  0.5255584716796875
train gradient:  0.1551223143162226
iteration : 5212
train acc:  0.65625
train loss:  0.553246021270752
train gradient:  0.1869899387229531
iteration : 5213
train acc:  0.7109375
train loss:  0.5493248701095581
train gradient:  0.19921380357464147
iteration : 5214
train acc:  0.6796875
train loss:  0.541662335395813
train gradient:  0.21003719269059745
iteration : 5215
train acc:  0.7109375
train loss:  0.5280886888504028
train gradient:  0.15716668471745351
iteration : 5216
train acc:  0.765625
train loss:  0.4546573758125305
train gradient:  0.12149339801287522
iteration : 5217
train acc:  0.6875
train loss:  0.5576305389404297
train gradient:  0.1793685296816449
iteration : 5218
train acc:  0.71875
train loss:  0.5020126700401306
train gradient:  0.12828112237454403
iteration : 5219
train acc:  0.7265625
train loss:  0.5358263254165649
train gradient:  0.17940881141800286
iteration : 5220
train acc:  0.78125
train loss:  0.43756479024887085
train gradient:  0.09944251571213171
iteration : 5221
train acc:  0.71875
train loss:  0.5043272972106934
train gradient:  0.1245168860557906
iteration : 5222
train acc:  0.703125
train loss:  0.5409834980964661
train gradient:  0.13549324322567483
iteration : 5223
train acc:  0.71875
train loss:  0.5419556498527527
train gradient:  0.14665125410186414
iteration : 5224
train acc:  0.8359375
train loss:  0.4328680634498596
train gradient:  0.14706936417881517
iteration : 5225
train acc:  0.7265625
train loss:  0.51246178150177
train gradient:  0.1502276574999058
iteration : 5226
train acc:  0.734375
train loss:  0.5570324659347534
train gradient:  0.1400292233713693
iteration : 5227
train acc:  0.75
train loss:  0.5150988101959229
train gradient:  0.14732649685148383
iteration : 5228
train acc:  0.7890625
train loss:  0.5044006109237671
train gradient:  0.16789987145041674
iteration : 5229
train acc:  0.75
train loss:  0.4922001361846924
train gradient:  0.1667968197571128
iteration : 5230
train acc:  0.75
train loss:  0.5024508237838745
train gradient:  0.13518441637503775
iteration : 5231
train acc:  0.7578125
train loss:  0.4848349392414093
train gradient:  0.14977867972638492
iteration : 5232
train acc:  0.734375
train loss:  0.5114543437957764
train gradient:  0.1211226669791134
iteration : 5233
train acc:  0.703125
train loss:  0.500155508518219
train gradient:  0.1780234155995849
iteration : 5234
train acc:  0.734375
train loss:  0.5178884863853455
train gradient:  0.13317873013598291
iteration : 5235
train acc:  0.7265625
train loss:  0.5377870798110962
train gradient:  0.17128817334649005
iteration : 5236
train acc:  0.6953125
train loss:  0.624681830406189
train gradient:  0.2548760693354708
iteration : 5237
train acc:  0.7734375
train loss:  0.4788382053375244
train gradient:  0.16304562300572903
iteration : 5238
train acc:  0.6640625
train loss:  0.5994740724563599
train gradient:  0.19631507181480556
iteration : 5239
train acc:  0.765625
train loss:  0.4579625725746155
train gradient:  0.1252648917309867
iteration : 5240
train acc:  0.71875
train loss:  0.4908328652381897
train gradient:  0.11827561564768581
iteration : 5241
train acc:  0.6875
train loss:  0.535956859588623
train gradient:  0.15792414283163816
iteration : 5242
train acc:  0.7421875
train loss:  0.49016904830932617
train gradient:  0.18967089136205567
iteration : 5243
train acc:  0.7734375
train loss:  0.4924125373363495
train gradient:  0.12742941168923239
iteration : 5244
train acc:  0.7109375
train loss:  0.49516138434410095
train gradient:  0.13699311705759099
iteration : 5245
train acc:  0.7578125
train loss:  0.49770697951316833
train gradient:  0.12904198067397404
iteration : 5246
train acc:  0.6796875
train loss:  0.5648272037506104
train gradient:  0.1497947496974936
iteration : 5247
train acc:  0.75
train loss:  0.5112922787666321
train gradient:  0.13693926829692504
iteration : 5248
train acc:  0.8046875
train loss:  0.44835910201072693
train gradient:  0.12855285589097834
iteration : 5249
train acc:  0.6875
train loss:  0.5703102350234985
train gradient:  0.18556858713742158
iteration : 5250
train acc:  0.7421875
train loss:  0.44109445810317993
train gradient:  0.10537089193657004
iteration : 5251
train acc:  0.71875
train loss:  0.5970408916473389
train gradient:  0.2625337339912979
iteration : 5252
train acc:  0.7109375
train loss:  0.5091021060943604
train gradient:  0.14459116654738977
iteration : 5253
train acc:  0.75
train loss:  0.48876509070396423
train gradient:  0.13305954130808761
iteration : 5254
train acc:  0.671875
train loss:  0.5827102065086365
train gradient:  0.17128987266827714
iteration : 5255
train acc:  0.7421875
train loss:  0.494029700756073
train gradient:  0.1373998106400052
iteration : 5256
train acc:  0.6875
train loss:  0.5875453948974609
train gradient:  0.22859979940823194
iteration : 5257
train acc:  0.6796875
train loss:  0.6303728222846985
train gradient:  0.2316089233946862
iteration : 5258
train acc:  0.703125
train loss:  0.5491563081741333
train gradient:  0.15414393314792368
iteration : 5259
train acc:  0.7421875
train loss:  0.48139601945877075
train gradient:  0.19537810125653715
iteration : 5260
train acc:  0.7421875
train loss:  0.4790046811103821
train gradient:  0.1149878964370485
iteration : 5261
train acc:  0.7265625
train loss:  0.5493491291999817
train gradient:  0.14305137766129564
iteration : 5262
train acc:  0.6953125
train loss:  0.5167383551597595
train gradient:  0.14497700789944667
iteration : 5263
train acc:  0.78125
train loss:  0.47433891892433167
train gradient:  0.12201274697471413
iteration : 5264
train acc:  0.7265625
train loss:  0.5074435472488403
train gradient:  0.12333394230027687
iteration : 5265
train acc:  0.765625
train loss:  0.47136813402175903
train gradient:  0.13200169494538377
iteration : 5266
train acc:  0.75
train loss:  0.5075963139533997
train gradient:  0.13540534644555932
iteration : 5267
train acc:  0.7265625
train loss:  0.5334172248840332
train gradient:  0.1765619815770868
iteration : 5268
train acc:  0.6328125
train loss:  0.5686216950416565
train gradient:  0.16645706604196236
iteration : 5269
train acc:  0.6875
train loss:  0.6107382774353027
train gradient:  0.18733033613452832
iteration : 5270
train acc:  0.7890625
train loss:  0.454660564661026
train gradient:  0.12869010498265177
iteration : 5271
train acc:  0.7109375
train loss:  0.5073229074478149
train gradient:  0.17489257174810371
iteration : 5272
train acc:  0.78125
train loss:  0.49048376083374023
train gradient:  0.12601958694532045
iteration : 5273
train acc:  0.6640625
train loss:  0.5723240375518799
train gradient:  0.19500511591357733
iteration : 5274
train acc:  0.71875
train loss:  0.5202666521072388
train gradient:  0.13535854206083342
iteration : 5275
train acc:  0.7734375
train loss:  0.46690404415130615
train gradient:  0.16684284317438358
iteration : 5276
train acc:  0.7109375
train loss:  0.5291393399238586
train gradient:  0.14063076936094085
iteration : 5277
train acc:  0.640625
train loss:  0.5742888450622559
train gradient:  0.15693198977473516
iteration : 5278
train acc:  0.765625
train loss:  0.4458693265914917
train gradient:  0.13005126810031736
iteration : 5279
train acc:  0.75
train loss:  0.5397242307662964
train gradient:  0.1519476361013007
iteration : 5280
train acc:  0.6953125
train loss:  0.5521108508110046
train gradient:  0.17573265900347354
iteration : 5281
train acc:  0.703125
train loss:  0.5648877620697021
train gradient:  0.2142409926455029
iteration : 5282
train acc:  0.7109375
train loss:  0.5487104654312134
train gradient:  0.17313160909986502
iteration : 5283
train acc:  0.71875
train loss:  0.4916549324989319
train gradient:  0.1630578812549635
iteration : 5284
train acc:  0.765625
train loss:  0.4816114902496338
train gradient:  0.15243121528571857
iteration : 5285
train acc:  0.703125
train loss:  0.556795597076416
train gradient:  0.16228455623496563
iteration : 5286
train acc:  0.7578125
train loss:  0.46346092224121094
train gradient:  0.11916902247551606
iteration : 5287
train acc:  0.734375
train loss:  0.4848870635032654
train gradient:  0.12272343790753688
iteration : 5288
train acc:  0.734375
train loss:  0.5720404982566833
train gradient:  0.1873730914748727
iteration : 5289
train acc:  0.671875
train loss:  0.5621978640556335
train gradient:  0.17426096225369037
iteration : 5290
train acc:  0.7890625
train loss:  0.438379168510437
train gradient:  0.1184285816486263
iteration : 5291
train acc:  0.6875
train loss:  0.530210018157959
train gradient:  0.15321060459718686
iteration : 5292
train acc:  0.75
train loss:  0.46371009945869446
train gradient:  0.12084476868470645
iteration : 5293
train acc:  0.734375
train loss:  0.5153949856758118
train gradient:  0.13407351085450445
iteration : 5294
train acc:  0.75
train loss:  0.4775466322898865
train gradient:  0.13008479051507654
iteration : 5295
train acc:  0.75
train loss:  0.47416776418685913
train gradient:  0.10869069725495702
iteration : 5296
train acc:  0.7265625
train loss:  0.5038448572158813
train gradient:  0.22448232245807337
iteration : 5297
train acc:  0.7265625
train loss:  0.5406217575073242
train gradient:  0.12828761109279807
iteration : 5298
train acc:  0.734375
train loss:  0.495356023311615
train gradient:  0.13125611101637627
iteration : 5299
train acc:  0.7265625
train loss:  0.5497910380363464
train gradient:  0.19192826315417838
iteration : 5300
train acc:  0.6796875
train loss:  0.5622828602790833
train gradient:  0.18710087653352275
iteration : 5301
train acc:  0.75
train loss:  0.46892184019088745
train gradient:  0.1316414653830825
iteration : 5302
train acc:  0.7734375
train loss:  0.4527952969074249
train gradient:  0.1012086919548891
iteration : 5303
train acc:  0.703125
train loss:  0.5094957947731018
train gradient:  0.12798335869638272
iteration : 5304
train acc:  0.7265625
train loss:  0.49884355068206787
train gradient:  0.1932910841604631
iteration : 5305
train acc:  0.7265625
train loss:  0.4837459623813629
train gradient:  0.13260403336761456
iteration : 5306
train acc:  0.7734375
train loss:  0.49454766511917114
train gradient:  0.15552369354151896
iteration : 5307
train acc:  0.765625
train loss:  0.5007352828979492
train gradient:  0.1635638116444374
iteration : 5308
train acc:  0.703125
train loss:  0.5375585556030273
train gradient:  0.13665415858801344
iteration : 5309
train acc:  0.7578125
train loss:  0.48301881551742554
train gradient:  0.1491715346852578
iteration : 5310
train acc:  0.65625
train loss:  0.5704339742660522
train gradient:  0.17251306627798774
iteration : 5311
train acc:  0.7578125
train loss:  0.5164377093315125
train gradient:  0.15085669442831234
iteration : 5312
train acc:  0.6640625
train loss:  0.590002179145813
train gradient:  0.16199289007010112
iteration : 5313
train acc:  0.7109375
train loss:  0.5698803067207336
train gradient:  0.1702233932267803
iteration : 5314
train acc:  0.6953125
train loss:  0.5478734970092773
train gradient:  0.22267059152797947
iteration : 5315
train acc:  0.7734375
train loss:  0.46107491850852966
train gradient:  0.13871172836093038
iteration : 5316
train acc:  0.6875
train loss:  0.5664832592010498
train gradient:  0.19111748992546979
iteration : 5317
train acc:  0.7265625
train loss:  0.5331081748008728
train gradient:  0.14269753767941876
iteration : 5318
train acc:  0.734375
train loss:  0.5264059901237488
train gradient:  0.21193834603436865
iteration : 5319
train acc:  0.6796875
train loss:  0.5601942539215088
train gradient:  0.16454389732988267
iteration : 5320
train acc:  0.6796875
train loss:  0.5878368020057678
train gradient:  0.179351428763328
iteration : 5321
train acc:  0.75
train loss:  0.502332866191864
train gradient:  0.14650065612013882
iteration : 5322
train acc:  0.75
train loss:  0.4864291548728943
train gradient:  0.140693036078464
iteration : 5323
train acc:  0.7421875
train loss:  0.49827152490615845
train gradient:  0.14069497967536487
iteration : 5324
train acc:  0.7578125
train loss:  0.5338687896728516
train gradient:  0.1409918092014733
iteration : 5325
train acc:  0.765625
train loss:  0.48464757204055786
train gradient:  0.13953631637292235
iteration : 5326
train acc:  0.765625
train loss:  0.45274752378463745
train gradient:  0.10462461808830349
iteration : 5327
train acc:  0.78125
train loss:  0.4901595115661621
train gradient:  0.10855024034844901
iteration : 5328
train acc:  0.7265625
train loss:  0.5312427282333374
train gradient:  0.1302328637022984
iteration : 5329
train acc:  0.7578125
train loss:  0.5009000301361084
train gradient:  0.11418536970764886
iteration : 5330
train acc:  0.703125
train loss:  0.5606902837753296
train gradient:  0.19546510130513473
iteration : 5331
train acc:  0.640625
train loss:  0.6532018184661865
train gradient:  0.30238410148803363
iteration : 5332
train acc:  0.703125
train loss:  0.503955602645874
train gradient:  0.1325912411888338
iteration : 5333
train acc:  0.6484375
train loss:  0.5458056330680847
train gradient:  0.1980002616407721
iteration : 5334
train acc:  0.734375
train loss:  0.512751579284668
train gradient:  0.15225231462726507
iteration : 5335
train acc:  0.734375
train loss:  0.5199080109596252
train gradient:  0.12808055247799088
iteration : 5336
train acc:  0.734375
train loss:  0.5284571051597595
train gradient:  0.20854938975884596
iteration : 5337
train acc:  0.75
train loss:  0.48319751024246216
train gradient:  0.1468172223430235
iteration : 5338
train acc:  0.703125
train loss:  0.5447283983230591
train gradient:  0.14208134169172376
iteration : 5339
train acc:  0.7734375
train loss:  0.48501044511795044
train gradient:  0.13785664370724687
iteration : 5340
train acc:  0.734375
train loss:  0.5792822241783142
train gradient:  0.19575233979855516
iteration : 5341
train acc:  0.7265625
train loss:  0.4934726357460022
train gradient:  0.10750078429599101
iteration : 5342
train acc:  0.7265625
train loss:  0.5241248607635498
train gradient:  0.12949628493742993
iteration : 5343
train acc:  0.78125
train loss:  0.4494374990463257
train gradient:  0.10081664389876702
iteration : 5344
train acc:  0.6640625
train loss:  0.5664306282997131
train gradient:  0.16140308532230152
iteration : 5345
train acc:  0.6953125
train loss:  0.5447658896446228
train gradient:  0.16237921859636895
iteration : 5346
train acc:  0.765625
train loss:  0.4881366789340973
train gradient:  0.1331315871066998
iteration : 5347
train acc:  0.703125
train loss:  0.5583273768424988
train gradient:  0.13276868264583955
iteration : 5348
train acc:  0.765625
train loss:  0.49608176946640015
train gradient:  0.1373755155403272
iteration : 5349
train acc:  0.7734375
train loss:  0.45230767130851746
train gradient:  0.12190543118328186
iteration : 5350
train acc:  0.828125
train loss:  0.4687168598175049
train gradient:  0.11375898225052855
iteration : 5351
train acc:  0.765625
train loss:  0.5048736929893494
train gradient:  0.11009807380859142
iteration : 5352
train acc:  0.765625
train loss:  0.48099905252456665
train gradient:  0.14513419064546074
iteration : 5353
train acc:  0.7578125
train loss:  0.48162972927093506
train gradient:  0.14527737947845487
iteration : 5354
train acc:  0.78125
train loss:  0.5052947998046875
train gradient:  0.1376293245680773
iteration : 5355
train acc:  0.7265625
train loss:  0.5053560733795166
train gradient:  0.17243707023217741
iteration : 5356
train acc:  0.734375
train loss:  0.5053342580795288
train gradient:  0.12177894851309569
iteration : 5357
train acc:  0.78125
train loss:  0.4843400716781616
train gradient:  0.13820459263247636
iteration : 5358
train acc:  0.7890625
train loss:  0.42182183265686035
train gradient:  0.12920940720880558
iteration : 5359
train acc:  0.8515625
train loss:  0.38544875383377075
train gradient:  0.11651420997731667
iteration : 5360
train acc:  0.8046875
train loss:  0.4369872808456421
train gradient:  0.15000874397788438
iteration : 5361
train acc:  0.7890625
train loss:  0.4516405761241913
train gradient:  0.1419736360910917
iteration : 5362
train acc:  0.6953125
train loss:  0.5386230945587158
train gradient:  0.14746209312591863
iteration : 5363
train acc:  0.6875
train loss:  0.5481372475624084
train gradient:  0.1579661384758265
iteration : 5364
train acc:  0.71875
train loss:  0.49215933680534363
train gradient:  0.11952178287896673
iteration : 5365
train acc:  0.6953125
train loss:  0.5220788717269897
train gradient:  0.13839369545230984
iteration : 5366
train acc:  0.78125
train loss:  0.49227115511894226
train gradient:  0.1370676742893125
iteration : 5367
train acc:  0.7421875
train loss:  0.5384706258773804
train gradient:  0.1413550465113944
iteration : 5368
train acc:  0.734375
train loss:  0.5507632493972778
train gradient:  0.13190154345196586
iteration : 5369
train acc:  0.625
train loss:  0.6292885541915894
train gradient:  0.29262426047536727
iteration : 5370
train acc:  0.7734375
train loss:  0.46639031171798706
train gradient:  0.14714261641767262
iteration : 5371
train acc:  0.7578125
train loss:  0.4613531231880188
train gradient:  0.10800711151530881
iteration : 5372
train acc:  0.6875
train loss:  0.5698435306549072
train gradient:  0.16111999975544
iteration : 5373
train acc:  0.75
train loss:  0.5345553755760193
train gradient:  0.14543015010997884
iteration : 5374
train acc:  0.75
train loss:  0.5136620402336121
train gradient:  0.17391579368023996
iteration : 5375
train acc:  0.6796875
train loss:  0.5482822060585022
train gradient:  0.16954426225954417
iteration : 5376
train acc:  0.6640625
train loss:  0.5764768719673157
train gradient:  0.21862868944119807
iteration : 5377
train acc:  0.796875
train loss:  0.4791066348552704
train gradient:  0.12045459617402698
iteration : 5378
train acc:  0.7734375
train loss:  0.4551311433315277
train gradient:  0.11912955063937866
iteration : 5379
train acc:  0.6796875
train loss:  0.5399315357208252
train gradient:  0.17734854383331072
iteration : 5380
train acc:  0.734375
train loss:  0.5310945510864258
train gradient:  0.18543600039214603
iteration : 5381
train acc:  0.7890625
train loss:  0.44590842723846436
train gradient:  0.12691417194496357
iteration : 5382
train acc:  0.734375
train loss:  0.5197422504425049
train gradient:  0.13223784431620633
iteration : 5383
train acc:  0.6953125
train loss:  0.5135014057159424
train gradient:  0.2050389270761887
iteration : 5384
train acc:  0.7265625
train loss:  0.5496355295181274
train gradient:  0.13375815829048793
iteration : 5385
train acc:  0.65625
train loss:  0.5782843828201294
train gradient:  0.15433176772138413
iteration : 5386
train acc:  0.7265625
train loss:  0.5220222473144531
train gradient:  0.13805906932958523
iteration : 5387
train acc:  0.7734375
train loss:  0.4608234763145447
train gradient:  0.09582226500322544
iteration : 5388
train acc:  0.734375
train loss:  0.4936884045600891
train gradient:  0.09882930624050293
iteration : 5389
train acc:  0.6640625
train loss:  0.5790059566497803
train gradient:  0.1547941291156682
iteration : 5390
train acc:  0.7578125
train loss:  0.4472940266132355
train gradient:  0.11194732588799658
iteration : 5391
train acc:  0.6875
train loss:  0.5893807411193848
train gradient:  0.19286091513577952
iteration : 5392
train acc:  0.6953125
train loss:  0.5188189148902893
train gradient:  0.13108141050254252
iteration : 5393
train acc:  0.765625
train loss:  0.45973479747772217
train gradient:  0.1008907212605945
iteration : 5394
train acc:  0.6796875
train loss:  0.5400614738464355
train gradient:  0.15263734698580078
iteration : 5395
train acc:  0.765625
train loss:  0.4989737570285797
train gradient:  0.1159656277272666
iteration : 5396
train acc:  0.7734375
train loss:  0.48304978013038635
train gradient:  0.1386019306259859
iteration : 5397
train acc:  0.734375
train loss:  0.4785022437572479
train gradient:  0.12126674439691765
iteration : 5398
train acc:  0.75
train loss:  0.5190990567207336
train gradient:  0.141718676816403
iteration : 5399
train acc:  0.7734375
train loss:  0.4322936534881592
train gradient:  0.1279073503066001
iteration : 5400
train acc:  0.6796875
train loss:  0.5643823146820068
train gradient:  0.15668696463176962
iteration : 5401
train acc:  0.703125
train loss:  0.5334056615829468
train gradient:  0.141917241899017
iteration : 5402
train acc:  0.7421875
train loss:  0.5227258205413818
train gradient:  0.14867461204085752
iteration : 5403
train acc:  0.6953125
train loss:  0.5293943881988525
train gradient:  0.13758547475308683
iteration : 5404
train acc:  0.65625
train loss:  0.5894194841384888
train gradient:  0.21915782876874096
iteration : 5405
train acc:  0.7421875
train loss:  0.5048636198043823
train gradient:  0.13699466474911767
iteration : 5406
train acc:  0.796875
train loss:  0.4481707513332367
train gradient:  0.13244032612134438
iteration : 5407
train acc:  0.7578125
train loss:  0.5054692625999451
train gradient:  0.13138758678637635
iteration : 5408
train acc:  0.7734375
train loss:  0.4388394355773926
train gradient:  0.09639073806899529
iteration : 5409
train acc:  0.71875
train loss:  0.5355558395385742
train gradient:  0.17241289417517064
iteration : 5410
train acc:  0.7265625
train loss:  0.5142196416854858
train gradient:  0.10684956112938117
iteration : 5411
train acc:  0.6640625
train loss:  0.5691225528717041
train gradient:  0.16323732846883093
iteration : 5412
train acc:  0.7109375
train loss:  0.5410434603691101
train gradient:  0.17109467915482718
iteration : 5413
train acc:  0.71875
train loss:  0.5430833697319031
train gradient:  0.15306297688497417
iteration : 5414
train acc:  0.6796875
train loss:  0.5835090279579163
train gradient:  0.19823784329144706
iteration : 5415
train acc:  0.7109375
train loss:  0.5278773307800293
train gradient:  0.11328258101973196
iteration : 5416
train acc:  0.6875
train loss:  0.5137108564376831
train gradient:  0.15436661599250506
iteration : 5417
train acc:  0.6953125
train loss:  0.5413217544555664
train gradient:  0.193678005904021
iteration : 5418
train acc:  0.75
train loss:  0.5190441608428955
train gradient:  0.17070295770191324
iteration : 5419
train acc:  0.7421875
train loss:  0.5266936421394348
train gradient:  0.21789597543227923
iteration : 5420
train acc:  0.7578125
train loss:  0.5445348620414734
train gradient:  0.17826553578119714
iteration : 5421
train acc:  0.765625
train loss:  0.5142911076545715
train gradient:  0.18460660962589118
iteration : 5422
train acc:  0.671875
train loss:  0.5658276081085205
train gradient:  0.1784223527188603
iteration : 5423
train acc:  0.7421875
train loss:  0.5053345561027527
train gradient:  0.18866079805091962
iteration : 5424
train acc:  0.765625
train loss:  0.5073009729385376
train gradient:  0.14261541386866636
iteration : 5425
train acc:  0.75
train loss:  0.5027974843978882
train gradient:  0.15026183551405387
iteration : 5426
train acc:  0.6953125
train loss:  0.5050735473632812
train gradient:  0.12506715361791115
iteration : 5427
train acc:  0.7109375
train loss:  0.4796414375305176
train gradient:  0.161917505278041
iteration : 5428
train acc:  0.78125
train loss:  0.4421089291572571
train gradient:  0.12665228146081914
iteration : 5429
train acc:  0.71875
train loss:  0.5298869013786316
train gradient:  0.16555115269460868
iteration : 5430
train acc:  0.765625
train loss:  0.491624116897583
train gradient:  0.14375040749805773
iteration : 5431
train acc:  0.71875
train loss:  0.5360949635505676
train gradient:  0.18527989702342323
iteration : 5432
train acc:  0.6640625
train loss:  0.5687556266784668
train gradient:  0.16184028951435367
iteration : 5433
train acc:  0.703125
train loss:  0.5498328804969788
train gradient:  0.1409630016383805
iteration : 5434
train acc:  0.7109375
train loss:  0.6040600538253784
train gradient:  0.17202559324805652
iteration : 5435
train acc:  0.734375
train loss:  0.49233734607696533
train gradient:  0.1387703279819845
iteration : 5436
train acc:  0.703125
train loss:  0.5166336297988892
train gradient:  0.10978519480031039
iteration : 5437
train acc:  0.8046875
train loss:  0.44397395849227905
train gradient:  0.13711722850944896
iteration : 5438
train acc:  0.7734375
train loss:  0.4600663185119629
train gradient:  0.10893763951100305
iteration : 5439
train acc:  0.671875
train loss:  0.5460283756256104
train gradient:  0.1673354858350213
iteration : 5440
train acc:  0.734375
train loss:  0.5305184125900269
train gradient:  0.14506050129994374
iteration : 5441
train acc:  0.671875
train loss:  0.6067277193069458
train gradient:  0.18801793086604152
iteration : 5442
train acc:  0.7578125
train loss:  0.46955832839012146
train gradient:  0.11152960477935514
iteration : 5443
train acc:  0.765625
train loss:  0.4881967306137085
train gradient:  0.15592608458974244
iteration : 5444
train acc:  0.7265625
train loss:  0.5337142944335938
train gradient:  0.17984156390146044
iteration : 5445
train acc:  0.6875
train loss:  0.5756711959838867
train gradient:  0.1787221848814595
iteration : 5446
train acc:  0.734375
train loss:  0.5189407467842102
train gradient:  0.15952963727746472
iteration : 5447
train acc:  0.71875
train loss:  0.5108001828193665
train gradient:  0.14989726163232722
iteration : 5448
train acc:  0.75
train loss:  0.5244348049163818
train gradient:  0.14282996107877932
iteration : 5449
train acc:  0.75
train loss:  0.5555610060691833
train gradient:  0.18841965749247225
iteration : 5450
train acc:  0.734375
train loss:  0.4876466691493988
train gradient:  0.1279102274006395
iteration : 5451
train acc:  0.6484375
train loss:  0.5703152418136597
train gradient:  0.16586268954263172
iteration : 5452
train acc:  0.7109375
train loss:  0.5732977390289307
train gradient:  0.15641249546970987
iteration : 5453
train acc:  0.75
train loss:  0.5267764329910278
train gradient:  0.12916463183449023
iteration : 5454
train acc:  0.6875
train loss:  0.6103580594062805
train gradient:  0.1906188423088373
iteration : 5455
train acc:  0.828125
train loss:  0.4049510359764099
train gradient:  0.1034604856846268
iteration : 5456
train acc:  0.65625
train loss:  0.5221641063690186
train gradient:  0.14353606600117597
iteration : 5457
train acc:  0.8125
train loss:  0.45164453983306885
train gradient:  0.11872000755024191
iteration : 5458
train acc:  0.7109375
train loss:  0.5730026960372925
train gradient:  0.21188837936877458
iteration : 5459
train acc:  0.6875
train loss:  0.5973953008651733
train gradient:  0.1907633990881467
iteration : 5460
train acc:  0.8125
train loss:  0.4396666884422302
train gradient:  0.10784668934828667
iteration : 5461
train acc:  0.75
train loss:  0.5085012912750244
train gradient:  0.13777442744052495
iteration : 5462
train acc:  0.71875
train loss:  0.5042724609375
train gradient:  0.13498899288446475
iteration : 5463
train acc:  0.7890625
train loss:  0.4287196695804596
train gradient:  0.11199750375983838
iteration : 5464
train acc:  0.734375
train loss:  0.46467116475105286
train gradient:  0.10686826481091562
iteration : 5465
train acc:  0.703125
train loss:  0.48424893617630005
train gradient:  0.13500096613481366
iteration : 5466
train acc:  0.7890625
train loss:  0.49084389209747314
train gradient:  0.1265462624117436
iteration : 5467
train acc:  0.6953125
train loss:  0.5688326954841614
train gradient:  0.17103904950048687
iteration : 5468
train acc:  0.71875
train loss:  0.5346896648406982
train gradient:  0.17633245237768574
iteration : 5469
train acc:  0.765625
train loss:  0.4638781249523163
train gradient:  0.1583009429441914
iteration : 5470
train acc:  0.75
train loss:  0.5220277309417725
train gradient:  0.14007908999300256
iteration : 5471
train acc:  0.765625
train loss:  0.5128304362297058
train gradient:  0.15982616017398477
iteration : 5472
train acc:  0.6796875
train loss:  0.5355750322341919
train gradient:  0.1766570656155707
iteration : 5473
train acc:  0.7578125
train loss:  0.49844416975975037
train gradient:  0.16661733226648334
iteration : 5474
train acc:  0.71875
train loss:  0.5319753885269165
train gradient:  0.1660643824807413
iteration : 5475
train acc:  0.734375
train loss:  0.5320248603820801
train gradient:  0.13049854909299996
iteration : 5476
train acc:  0.71875
train loss:  0.5243760347366333
train gradient:  0.14753865346795267
iteration : 5477
train acc:  0.7578125
train loss:  0.4713265895843506
train gradient:  0.10952154856639393
iteration : 5478
train acc:  0.796875
train loss:  0.4487817585468292
train gradient:  0.10825635767739289
iteration : 5479
train acc:  0.75
train loss:  0.469005286693573
train gradient:  0.11760652177450699
iteration : 5480
train acc:  0.7421875
train loss:  0.5108761787414551
train gradient:  0.1369349457808335
iteration : 5481
train acc:  0.7265625
train loss:  0.5087159872055054
train gradient:  0.12803753064522683
iteration : 5482
train acc:  0.6796875
train loss:  0.5309686660766602
train gradient:  0.16996379532808858
iteration : 5483
train acc:  0.7734375
train loss:  0.4898238778114319
train gradient:  0.1531170229265157
iteration : 5484
train acc:  0.7421875
train loss:  0.5110459327697754
train gradient:  0.12018822452305007
iteration : 5485
train acc:  0.71875
train loss:  0.5481822490692139
train gradient:  0.14999844564492076
iteration : 5486
train acc:  0.75
train loss:  0.5104470252990723
train gradient:  0.1534856415960454
iteration : 5487
train acc:  0.78125
train loss:  0.45795130729675293
train gradient:  0.133252560777488
iteration : 5488
train acc:  0.6796875
train loss:  0.5465062856674194
train gradient:  0.16682646044828725
iteration : 5489
train acc:  0.734375
train loss:  0.48613354563713074
train gradient:  0.13414219195071012
iteration : 5490
train acc:  0.75
train loss:  0.5120973587036133
train gradient:  0.13572694844169675
iteration : 5491
train acc:  0.765625
train loss:  0.47656065225601196
train gradient:  0.1505747448709316
iteration : 5492
train acc:  0.734375
train loss:  0.5133041143417358
train gradient:  0.14968438956760338
iteration : 5493
train acc:  0.671875
train loss:  0.5727041959762573
train gradient:  0.14446669030902556
iteration : 5494
train acc:  0.7578125
train loss:  0.522946834564209
train gradient:  0.1269215430661264
iteration : 5495
train acc:  0.7421875
train loss:  0.45304304361343384
train gradient:  0.12963982237062327
iteration : 5496
train acc:  0.734375
train loss:  0.4891747236251831
train gradient:  0.12763469708219144
iteration : 5497
train acc:  0.7578125
train loss:  0.4715379476547241
train gradient:  0.12424836060154024
iteration : 5498
train acc:  0.7421875
train loss:  0.4978719651699066
train gradient:  0.13948069119729012
iteration : 5499
train acc:  0.75
train loss:  0.5033803582191467
train gradient:  0.13968276048265493
iteration : 5500
train acc:  0.78125
train loss:  0.4738563299179077
train gradient:  0.1169638654829001
iteration : 5501
train acc:  0.78125
train loss:  0.4788479804992676
train gradient:  0.14528327895712267
iteration : 5502
train acc:  0.7109375
train loss:  0.50202476978302
train gradient:  0.13085592522814604
iteration : 5503
train acc:  0.7265625
train loss:  0.5313298106193542
train gradient:  0.1603713529467721
iteration : 5504
train acc:  0.734375
train loss:  0.5552588701248169
train gradient:  0.17915908128272837
iteration : 5505
train acc:  0.7109375
train loss:  0.49754300713539124
train gradient:  0.1627499131122898
iteration : 5506
train acc:  0.734375
train loss:  0.46399828791618347
train gradient:  0.10998110219656039
iteration : 5507
train acc:  0.6640625
train loss:  0.5931175947189331
train gradient:  0.1777554765969869
iteration : 5508
train acc:  0.75
train loss:  0.4818885624408722
train gradient:  0.13754483095753933
iteration : 5509
train acc:  0.703125
train loss:  0.5799720883369446
train gradient:  0.20014037561475845
iteration : 5510
train acc:  0.734375
train loss:  0.5287339687347412
train gradient:  0.14300773134660366
iteration : 5511
train acc:  0.7265625
train loss:  0.48849940299987793
train gradient:  0.13211978767552252
iteration : 5512
train acc:  0.7421875
train loss:  0.4768996238708496
train gradient:  0.12546546790364058
iteration : 5513
train acc:  0.6640625
train loss:  0.5500197410583496
train gradient:  0.19758924862295135
iteration : 5514
train acc:  0.6484375
train loss:  0.5648216605186462
train gradient:  0.14007847864439615
iteration : 5515
train acc:  0.7421875
train loss:  0.5133470296859741
train gradient:  0.14989795421462948
iteration : 5516
train acc:  0.7265625
train loss:  0.4612579047679901
train gradient:  0.12250416500695553
iteration : 5517
train acc:  0.734375
train loss:  0.5103964805603027
train gradient:  0.12199819445137129
iteration : 5518
train acc:  0.7265625
train loss:  0.5024856925010681
train gradient:  0.12268441766895623
iteration : 5519
train acc:  0.6484375
train loss:  0.5907173156738281
train gradient:  0.24326921459930628
iteration : 5520
train acc:  0.6640625
train loss:  0.6281497478485107
train gradient:  0.23687045995841627
iteration : 5521
train acc:  0.765625
train loss:  0.4835747182369232
train gradient:  0.12978950814665627
iteration : 5522
train acc:  0.703125
train loss:  0.5156135559082031
train gradient:  0.20694036930030524
iteration : 5523
train acc:  0.6796875
train loss:  0.5729576349258423
train gradient:  0.16282046941962997
iteration : 5524
train acc:  0.71875
train loss:  0.5238327980041504
train gradient:  0.14567512606001604
iteration : 5525
train acc:  0.734375
train loss:  0.5106191635131836
train gradient:  0.1526927558438559
iteration : 5526
train acc:  0.6953125
train loss:  0.5731380581855774
train gradient:  0.14722169734562374
iteration : 5527
train acc:  0.8203125
train loss:  0.42558521032333374
train gradient:  0.09595929318789626
iteration : 5528
train acc:  0.7421875
train loss:  0.4628911018371582
train gradient:  0.10630163189360901
iteration : 5529
train acc:  0.765625
train loss:  0.4605344235897064
train gradient:  0.11052739078447808
iteration : 5530
train acc:  0.640625
train loss:  0.5875840187072754
train gradient:  0.15936752150329034
iteration : 5531
train acc:  0.7109375
train loss:  0.5364698171615601
train gradient:  0.12638534598667978
iteration : 5532
train acc:  0.796875
train loss:  0.4749712347984314
train gradient:  0.15003352914541646
iteration : 5533
train acc:  0.6640625
train loss:  0.6100426912307739
train gradient:  0.17817416052660412
iteration : 5534
train acc:  0.7109375
train loss:  0.5422049760818481
train gradient:  0.19244304324340022
iteration : 5535
train acc:  0.7578125
train loss:  0.49235159158706665
train gradient:  0.11313613366411658
iteration : 5536
train acc:  0.75
train loss:  0.47965583205223083
train gradient:  0.12509763879240116
iteration : 5537
train acc:  0.75
train loss:  0.49626195430755615
train gradient:  0.15590963094745103
iteration : 5538
train acc:  0.7421875
train loss:  0.4949210286140442
train gradient:  0.10548211245745087
iteration : 5539
train acc:  0.765625
train loss:  0.4818071722984314
train gradient:  0.13780015797064366
iteration : 5540
train acc:  0.734375
train loss:  0.5012518763542175
train gradient:  0.1469704935660188
iteration : 5541
train acc:  0.734375
train loss:  0.570035994052887
train gradient:  0.18273054615655587
iteration : 5542
train acc:  0.78125
train loss:  0.4497421383857727
train gradient:  0.12979607971433738
iteration : 5543
train acc:  0.7109375
train loss:  0.535346508026123
train gradient:  0.15239462504398393
iteration : 5544
train acc:  0.71875
train loss:  0.5013574361801147
train gradient:  0.13400040536952684
iteration : 5545
train acc:  0.6796875
train loss:  0.5386636853218079
train gradient:  0.16137724709955403
iteration : 5546
train acc:  0.734375
train loss:  0.5047498345375061
train gradient:  0.14043558821750152
iteration : 5547
train acc:  0.75
train loss:  0.47318384051322937
train gradient:  0.1646962309461809
iteration : 5548
train acc:  0.6171875
train loss:  0.5719665884971619
train gradient:  0.1703794371171764
iteration : 5549
train acc:  0.796875
train loss:  0.4517305791378021
train gradient:  0.1353477696786137
iteration : 5550
train acc:  0.75
train loss:  0.4742898941040039
train gradient:  0.11849881276402033
iteration : 5551
train acc:  0.734375
train loss:  0.5189566612243652
train gradient:  0.14486136212044504
iteration : 5552
train acc:  0.78125
train loss:  0.43281495571136475
train gradient:  0.09860392238748011
iteration : 5553
train acc:  0.6640625
train loss:  0.5645647644996643
train gradient:  0.15516284272896816
iteration : 5554
train acc:  0.71875
train loss:  0.5098588466644287
train gradient:  0.1480107134294797
iteration : 5555
train acc:  0.765625
train loss:  0.45737218856811523
train gradient:  0.11064128929119801
iteration : 5556
train acc:  0.703125
train loss:  0.4790104329586029
train gradient:  0.13675132644534838
iteration : 5557
train acc:  0.75
train loss:  0.5218539237976074
train gradient:  0.1656555807008846
iteration : 5558
train acc:  0.7421875
train loss:  0.494073748588562
train gradient:  0.1581758262099035
iteration : 5559
train acc:  0.765625
train loss:  0.454730749130249
train gradient:  0.09642392848223497
iteration : 5560
train acc:  0.7734375
train loss:  0.44936254620552063
train gradient:  0.08929724837559373
iteration : 5561
train acc:  0.7421875
train loss:  0.5749999284744263
train gradient:  0.14886082100934392
iteration : 5562
train acc:  0.703125
train loss:  0.5595433712005615
train gradient:  0.14431132506031938
iteration : 5563
train acc:  0.6953125
train loss:  0.5343941450119019
train gradient:  0.13445190972351312
iteration : 5564
train acc:  0.71875
train loss:  0.4962118864059448
train gradient:  0.14760330383932901
iteration : 5565
train acc:  0.7890625
train loss:  0.4707433879375458
train gradient:  0.15933516375078277
iteration : 5566
train acc:  0.6875
train loss:  0.5738731622695923
train gradient:  0.1413274735181121
iteration : 5567
train acc:  0.6953125
train loss:  0.5603431463241577
train gradient:  0.14321543694574074
iteration : 5568
train acc:  0.6796875
train loss:  0.5502340793609619
train gradient:  0.18656953511464658
iteration : 5569
train acc:  0.6875
train loss:  0.536246657371521
train gradient:  0.14791589717094772
iteration : 5570
train acc:  0.765625
train loss:  0.45161008834838867
train gradient:  0.1446318969506259
iteration : 5571
train acc:  0.75
train loss:  0.5662248134613037
train gradient:  0.16000295706427647
iteration : 5572
train acc:  0.7578125
train loss:  0.49526917934417725
train gradient:  0.12461288079667877
iteration : 5573
train acc:  0.7265625
train loss:  0.529032826423645
train gradient:  0.15349835081629848
iteration : 5574
train acc:  0.75
train loss:  0.5138381719589233
train gradient:  0.15401808722430613
iteration : 5575
train acc:  0.75
train loss:  0.5005105137825012
train gradient:  0.09969161402882516
iteration : 5576
train acc:  0.7109375
train loss:  0.514991044998169
train gradient:  0.14170631989557614
iteration : 5577
train acc:  0.7265625
train loss:  0.5312105417251587
train gradient:  0.18619928987743073
iteration : 5578
train acc:  0.765625
train loss:  0.4242788851261139
train gradient:  0.09480608851703205
iteration : 5579
train acc:  0.7578125
train loss:  0.4899667799472809
train gradient:  0.1346298636972334
iteration : 5580
train acc:  0.734375
train loss:  0.5255321860313416
train gradient:  0.13561111527348813
iteration : 5581
train acc:  0.734375
train loss:  0.4795547425746918
train gradient:  0.145153598181886
iteration : 5582
train acc:  0.78125
train loss:  0.4508199095726013
train gradient:  0.1364417066709907
iteration : 5583
train acc:  0.7265625
train loss:  0.5512296557426453
train gradient:  0.19380805372619975
iteration : 5584
train acc:  0.71875
train loss:  0.5640733242034912
train gradient:  0.17123595238566564
iteration : 5585
train acc:  0.7265625
train loss:  0.5344017744064331
train gradient:  0.15010336514823083
iteration : 5586
train acc:  0.6875
train loss:  0.5383129119873047
train gradient:  0.16835766925230583
iteration : 5587
train acc:  0.75
train loss:  0.4957423210144043
train gradient:  0.14217918480691283
iteration : 5588
train acc:  0.734375
train loss:  0.5666337013244629
train gradient:  0.20079083660463398
iteration : 5589
train acc:  0.6875
train loss:  0.5719822645187378
train gradient:  0.2444769541734238
iteration : 5590
train acc:  0.734375
train loss:  0.5517902374267578
train gradient:  0.1752661377173573
iteration : 5591
train acc:  0.765625
train loss:  0.4883762001991272
train gradient:  0.15409973115363507
iteration : 5592
train acc:  0.6796875
train loss:  0.588045060634613
train gradient:  0.18911689289604128
iteration : 5593
train acc:  0.75
train loss:  0.5050313472747803
train gradient:  0.14153159554680755
iteration : 5594
train acc:  0.703125
train loss:  0.5701313614845276
train gradient:  0.1771499904300085
iteration : 5595
train acc:  0.6640625
train loss:  0.5923116207122803
train gradient:  0.18008909309283938
iteration : 5596
train acc:  0.703125
train loss:  0.5288590788841248
train gradient:  0.1721176260599132
iteration : 5597
train acc:  0.7421875
train loss:  0.47563374042510986
train gradient:  0.11075100242499353
iteration : 5598
train acc:  0.734375
train loss:  0.5668166875839233
train gradient:  0.19983467598586013
iteration : 5599
train acc:  0.7421875
train loss:  0.49560368061065674
train gradient:  0.1479118612432826
iteration : 5600
train acc:  0.6953125
train loss:  0.5256881713867188
train gradient:  0.153615522281266
iteration : 5601
train acc:  0.7109375
train loss:  0.5222499370574951
train gradient:  0.1345277603060067
iteration : 5602
train acc:  0.6953125
train loss:  0.5050953030586243
train gradient:  0.12306716084071148
iteration : 5603
train acc:  0.7421875
train loss:  0.5509231090545654
train gradient:  0.1451712919303283
iteration : 5604
train acc:  0.7265625
train loss:  0.488550066947937
train gradient:  0.13194268712839521
iteration : 5605
train acc:  0.6875
train loss:  0.5471676588058472
train gradient:  0.147061229390253
iteration : 5606
train acc:  0.7109375
train loss:  0.5455175042152405
train gradient:  0.15236173492421717
iteration : 5607
train acc:  0.765625
train loss:  0.5005013346672058
train gradient:  0.11367420119302284
iteration : 5608
train acc:  0.7265625
train loss:  0.5473846793174744
train gradient:  0.19238368788144528
iteration : 5609
train acc:  0.71875
train loss:  0.4931778609752655
train gradient:  0.13847081320209023
iteration : 5610
train acc:  0.7890625
train loss:  0.45273542404174805
train gradient:  0.11986459117802371
iteration : 5611
train acc:  0.7734375
train loss:  0.463711142539978
train gradient:  0.1341987748263747
iteration : 5612
train acc:  0.75
train loss:  0.5045854449272156
train gradient:  0.12548966075812373
iteration : 5613
train acc:  0.734375
train loss:  0.5226539373397827
train gradient:  0.18790506622072062
iteration : 5614
train acc:  0.6953125
train loss:  0.5255399942398071
train gradient:  0.17729355067721664
iteration : 5615
train acc:  0.734375
train loss:  0.5393027067184448
train gradient:  0.19278027146057838
iteration : 5616
train acc:  0.75
train loss:  0.4655430316925049
train gradient:  0.11493698881973033
iteration : 5617
train acc:  0.78125
train loss:  0.4832327961921692
train gradient:  0.11636935192721479
iteration : 5618
train acc:  0.7265625
train loss:  0.4669090211391449
train gradient:  0.12676887681153276
iteration : 5619
train acc:  0.7578125
train loss:  0.476923406124115
train gradient:  0.2019735768236434
iteration : 5620
train acc:  0.640625
train loss:  0.6599754095077515
train gradient:  0.19993650737110452
iteration : 5621
train acc:  0.7734375
train loss:  0.5412071943283081
train gradient:  0.15465715594666826
iteration : 5622
train acc:  0.65625
train loss:  0.5879532098770142
train gradient:  0.16929235283132793
iteration : 5623
train acc:  0.7890625
train loss:  0.48648712038993835
train gradient:  0.11960816344441791
iteration : 5624
train acc:  0.7890625
train loss:  0.4865899980068207
train gradient:  0.14764205332536298
iteration : 5625
train acc:  0.8203125
train loss:  0.4588114619255066
train gradient:  0.12997889740255225
iteration : 5626
train acc:  0.7890625
train loss:  0.4573812782764435
train gradient:  0.109257928906863
iteration : 5627
train acc:  0.6796875
train loss:  0.5276675224304199
train gradient:  0.19846625412002977
iteration : 5628
train acc:  0.7578125
train loss:  0.4723755121231079
train gradient:  0.11803540692723798
iteration : 5629
train acc:  0.734375
train loss:  0.5533195734024048
train gradient:  0.1563792528914226
iteration : 5630
train acc:  0.7109375
train loss:  0.5465086102485657
train gradient:  0.1425313777099092
iteration : 5631
train acc:  0.75
train loss:  0.47244948148727417
train gradient:  0.13735904687517364
iteration : 5632
train acc:  0.7421875
train loss:  0.5088125467300415
train gradient:  0.16916431837209644
iteration : 5633
train acc:  0.78125
train loss:  0.45280522108078003
train gradient:  0.13065713149070934
iteration : 5634
train acc:  0.6796875
train loss:  0.5466181635856628
train gradient:  0.2169960903916593
iteration : 5635
train acc:  0.765625
train loss:  0.48387253284454346
train gradient:  0.11134832509729554
iteration : 5636
train acc:  0.7421875
train loss:  0.4967358112335205
train gradient:  0.13634819095748613
iteration : 5637
train acc:  0.7890625
train loss:  0.5090224742889404
train gradient:  0.16068118611224358
iteration : 5638
train acc:  0.7734375
train loss:  0.43750500679016113
train gradient:  0.10557757298933151
iteration : 5639
train acc:  0.7109375
train loss:  0.5453704595565796
train gradient:  0.19215364224593065
iteration : 5640
train acc:  0.703125
train loss:  0.528443455696106
train gradient:  0.12388553411932432
iteration : 5641
train acc:  0.7890625
train loss:  0.4687512218952179
train gradient:  0.11598517445092911
iteration : 5642
train acc:  0.734375
train loss:  0.4654039740562439
train gradient:  0.1507959298500195
iteration : 5643
train acc:  0.6796875
train loss:  0.5152314901351929
train gradient:  0.18694185647789996
iteration : 5644
train acc:  0.71875
train loss:  0.5174868106842041
train gradient:  0.17695743303212652
iteration : 5645
train acc:  0.703125
train loss:  0.5270564556121826
train gradient:  0.16702323082731912
iteration : 5646
train acc:  0.7109375
train loss:  0.49455779790878296
train gradient:  0.13255803830764656
iteration : 5647
train acc:  0.78125
train loss:  0.4481477737426758
train gradient:  0.1122174443226954
iteration : 5648
train acc:  0.71875
train loss:  0.5306808948516846
train gradient:  0.1261348273561268
iteration : 5649
train acc:  0.7578125
train loss:  0.5200015902519226
train gradient:  0.13005771321330312
iteration : 5650
train acc:  0.7890625
train loss:  0.44192469120025635
train gradient:  0.14179929502882477
iteration : 5651
train acc:  0.7890625
train loss:  0.46546077728271484
train gradient:  0.15293112889981475
iteration : 5652
train acc:  0.71875
train loss:  0.5122314691543579
train gradient:  0.12759376616203533
iteration : 5653
train acc:  0.71875
train loss:  0.5472347140312195
train gradient:  0.1638062250665741
iteration : 5654
train acc:  0.7890625
train loss:  0.44961801171302795
train gradient:  0.10596014096193414
iteration : 5655
train acc:  0.734375
train loss:  0.5217294692993164
train gradient:  0.15794774163190278
iteration : 5656
train acc:  0.7421875
train loss:  0.47825193405151367
train gradient:  0.13292170141904763
iteration : 5657
train acc:  0.7578125
train loss:  0.5057744979858398
train gradient:  0.17748247932554279
iteration : 5658
train acc:  0.765625
train loss:  0.48697346448898315
train gradient:  0.15437043246847632
iteration : 5659
train acc:  0.765625
train loss:  0.5258210897445679
train gradient:  0.1709167072782059
iteration : 5660
train acc:  0.6796875
train loss:  0.6444904804229736
train gradient:  0.34103612889000845
iteration : 5661
train acc:  0.7734375
train loss:  0.4858155846595764
train gradient:  0.13718703914144426
iteration : 5662
train acc:  0.734375
train loss:  0.5042378306388855
train gradient:  0.12921950338829313
iteration : 5663
train acc:  0.6875
train loss:  0.5421358346939087
train gradient:  0.14315246880772206
iteration : 5664
train acc:  0.7109375
train loss:  0.5991747975349426
train gradient:  0.19437610550416712
iteration : 5665
train acc:  0.734375
train loss:  0.5115793943405151
train gradient:  0.12424798725870258
iteration : 5666
train acc:  0.734375
train loss:  0.5240622758865356
train gradient:  0.18740804812092224
iteration : 5667
train acc:  0.75
train loss:  0.46839553117752075
train gradient:  0.13829048762236695
iteration : 5668
train acc:  0.7421875
train loss:  0.5097376704216003
train gradient:  0.16754010692113794
iteration : 5669
train acc:  0.6796875
train loss:  0.5739081501960754
train gradient:  0.16493172878218643
iteration : 5670
train acc:  0.6953125
train loss:  0.5689045190811157
train gradient:  0.14499202577233591
iteration : 5671
train acc:  0.78125
train loss:  0.46370598673820496
train gradient:  0.13916648335332382
iteration : 5672
train acc:  0.6953125
train loss:  0.563430905342102
train gradient:  0.21796764751946945
iteration : 5673
train acc:  0.734375
train loss:  0.49594706296920776
train gradient:  0.15860464152981976
iteration : 5674
train acc:  0.6796875
train loss:  0.5527632832527161
train gradient:  0.17665572865933787
iteration : 5675
train acc:  0.7578125
train loss:  0.47691160440444946
train gradient:  0.12758657370139354
iteration : 5676
train acc:  0.7265625
train loss:  0.48900434374809265
train gradient:  0.13952491577995457
iteration : 5677
train acc:  0.765625
train loss:  0.4848344922065735
train gradient:  0.18632148761390932
iteration : 5678
train acc:  0.78125
train loss:  0.4383639097213745
train gradient:  0.12498780434060265
iteration : 5679
train acc:  0.7265625
train loss:  0.6045534014701843
train gradient:  0.23546866425384613
iteration : 5680
train acc:  0.6953125
train loss:  0.5136234164237976
train gradient:  0.13017968877802188
iteration : 5681
train acc:  0.7734375
train loss:  0.4893406927585602
train gradient:  0.18882045275617965
iteration : 5682
train acc:  0.6875
train loss:  0.5393691062927246
train gradient:  0.15915976487597336
iteration : 5683
train acc:  0.7109375
train loss:  0.5587965250015259
train gradient:  0.1447855392036399
iteration : 5684
train acc:  0.71875
train loss:  0.5048681497573853
train gradient:  0.15111971208020603
iteration : 5685
train acc:  0.703125
train loss:  0.5371494889259338
train gradient:  0.13734157736268984
iteration : 5686
train acc:  0.7734375
train loss:  0.48840028047561646
train gradient:  0.1709544327739278
iteration : 5687
train acc:  0.71875
train loss:  0.5042069554328918
train gradient:  0.14375398933651207
iteration : 5688
train acc:  0.6875
train loss:  0.5249990224838257
train gradient:  0.18161286889154526
iteration : 5689
train acc:  0.71875
train loss:  0.5230593085289001
train gradient:  0.13657518348269357
iteration : 5690
train acc:  0.7578125
train loss:  0.5021070241928101
train gradient:  0.18611152772154577
iteration : 5691
train acc:  0.7578125
train loss:  0.4777687191963196
train gradient:  0.24741327613552694
iteration : 5692
train acc:  0.75
train loss:  0.4746609628200531
train gradient:  0.15404947140064917
iteration : 5693
train acc:  0.796875
train loss:  0.4138026833534241
train gradient:  0.09506964427280011
iteration : 5694
train acc:  0.7734375
train loss:  0.4853598475456238
train gradient:  0.1539847620286885
iteration : 5695
train acc:  0.7109375
train loss:  0.5147711038589478
train gradient:  0.17304972675348781
iteration : 5696
train acc:  0.78125
train loss:  0.4838644862174988
train gradient:  0.13367238510606516
iteration : 5697
train acc:  0.65625
train loss:  0.5357447862625122
train gradient:  0.16867214184005028
iteration : 5698
train acc:  0.7578125
train loss:  0.484447717666626
train gradient:  0.13180703857326057
iteration : 5699
train acc:  0.703125
train loss:  0.5520999431610107
train gradient:  0.1569636947880239
iteration : 5700
train acc:  0.6796875
train loss:  0.5311906933784485
train gradient:  0.15012693146440925
iteration : 5701
train acc:  0.7109375
train loss:  0.49815934896469116
train gradient:  0.18786214464438392
iteration : 5702
train acc:  0.6875
train loss:  0.4995027482509613
train gradient:  0.16936751585964685
iteration : 5703
train acc:  0.6953125
train loss:  0.5753015279769897
train gradient:  0.20777676474948115
iteration : 5704
train acc:  0.734375
train loss:  0.4922648072242737
train gradient:  0.1954171433401721
iteration : 5705
train acc:  0.7578125
train loss:  0.5170769691467285
train gradient:  0.24112507764396202
iteration : 5706
train acc:  0.734375
train loss:  0.5262781381607056
train gradient:  0.16023887456789326
iteration : 5707
train acc:  0.6953125
train loss:  0.5619477033615112
train gradient:  0.14721823669148032
iteration : 5708
train acc:  0.765625
train loss:  0.474104106426239
train gradient:  0.13369885183017755
iteration : 5709
train acc:  0.7578125
train loss:  0.47697585821151733
train gradient:  0.13830899768696808
iteration : 5710
train acc:  0.75
train loss:  0.5407521724700928
train gradient:  0.12521006108174026
iteration : 5711
train acc:  0.6875
train loss:  0.5274655818939209
train gradient:  0.18390547755279102
iteration : 5712
train acc:  0.6875
train loss:  0.5423333048820496
train gradient:  0.15553970814146312
iteration : 5713
train acc:  0.7578125
train loss:  0.48715847730636597
train gradient:  0.1166268122451082
iteration : 5714
train acc:  0.7265625
train loss:  0.5217192769050598
train gradient:  0.1422122723755316
iteration : 5715
train acc:  0.640625
train loss:  0.5481666326522827
train gradient:  0.16954000206055664
iteration : 5716
train acc:  0.734375
train loss:  0.4933125376701355
train gradient:  0.14181591779865602
iteration : 5717
train acc:  0.7265625
train loss:  0.4879230260848999
train gradient:  0.11343995113141121
iteration : 5718
train acc:  0.6640625
train loss:  0.5542632341384888
train gradient:  0.12766231227151784
iteration : 5719
train acc:  0.7265625
train loss:  0.47718527913093567
train gradient:  0.13173978776709763
iteration : 5720
train acc:  0.7421875
train loss:  0.48936325311660767
train gradient:  0.17910422688628763
iteration : 5721
train acc:  0.6796875
train loss:  0.5087146759033203
train gradient:  0.1131719387669381
iteration : 5722
train acc:  0.6953125
train loss:  0.5288040041923523
train gradient:  0.17939958529845543
iteration : 5723
train acc:  0.6484375
train loss:  0.6094712615013123
train gradient:  0.19916034777844868
iteration : 5724
train acc:  0.765625
train loss:  0.48875150084495544
train gradient:  0.12772397888300346
iteration : 5725
train acc:  0.703125
train loss:  0.5438414812088013
train gradient:  0.33019953146462677
iteration : 5726
train acc:  0.75
train loss:  0.5172484517097473
train gradient:  0.17278146698657337
iteration : 5727
train acc:  0.7109375
train loss:  0.4795072674751282
train gradient:  0.1336833940062761
iteration : 5728
train acc:  0.7265625
train loss:  0.5738011002540588
train gradient:  0.158662278516309
iteration : 5729
train acc:  0.7421875
train loss:  0.5358219146728516
train gradient:  0.16175693658924373
iteration : 5730
train acc:  0.7265625
train loss:  0.4662734568119049
train gradient:  0.11012771450755905
iteration : 5731
train acc:  0.7890625
train loss:  0.44251546263694763
train gradient:  0.11732810856159806
iteration : 5732
train acc:  0.7421875
train loss:  0.47412705421447754
train gradient:  0.12633232345393242
iteration : 5733
train acc:  0.703125
train loss:  0.5631730556488037
train gradient:  0.14415658886293847
iteration : 5734
train acc:  0.7265625
train loss:  0.5019721984863281
train gradient:  0.13877952976100577
iteration : 5735
train acc:  0.7578125
train loss:  0.4600844085216522
train gradient:  0.09568059389035634
iteration : 5736
train acc:  0.734375
train loss:  0.5042102336883545
train gradient:  0.11338599361657156
iteration : 5737
train acc:  0.671875
train loss:  0.5447957515716553
train gradient:  0.1547543657479769
iteration : 5738
train acc:  0.6953125
train loss:  0.5046395063400269
train gradient:  0.12879811184322906
iteration : 5739
train acc:  0.6953125
train loss:  0.5473801493644714
train gradient:  0.15032894220676124
iteration : 5740
train acc:  0.78125
train loss:  0.5137226581573486
train gradient:  0.14556652384068713
iteration : 5741
train acc:  0.703125
train loss:  0.5414827466011047
train gradient:  0.16421013224369807
iteration : 5742
train acc:  0.71875
train loss:  0.4944985806941986
train gradient:  0.10846587551236826
iteration : 5743
train acc:  0.65625
train loss:  0.6106252670288086
train gradient:  0.1830193167481595
iteration : 5744
train acc:  0.765625
train loss:  0.4872562289237976
train gradient:  0.140154201730808
iteration : 5745
train acc:  0.7890625
train loss:  0.4730994701385498
train gradient:  0.12237098429615707
iteration : 5746
train acc:  0.7265625
train loss:  0.5542117953300476
train gradient:  0.1622669570006336
iteration : 5747
train acc:  0.7421875
train loss:  0.44297730922698975
train gradient:  0.10945245751689296
iteration : 5748
train acc:  0.671875
train loss:  0.5634713172912598
train gradient:  0.20295860951084566
iteration : 5749
train acc:  0.71875
train loss:  0.5480093955993652
train gradient:  0.1362524329060096
iteration : 5750
train acc:  0.796875
train loss:  0.49539536237716675
train gradient:  0.14969650918423177
iteration : 5751
train acc:  0.7734375
train loss:  0.4492679238319397
train gradient:  0.1479232455304006
iteration : 5752
train acc:  0.6953125
train loss:  0.5041263103485107
train gradient:  0.1389810039187671
iteration : 5753
train acc:  0.7265625
train loss:  0.5345748662948608
train gradient:  0.1744183552480537
iteration : 5754
train acc:  0.703125
train loss:  0.5534414052963257
train gradient:  0.1537084344418827
iteration : 5755
train acc:  0.765625
train loss:  0.4858464300632477
train gradient:  0.13610013029084256
iteration : 5756
train acc:  0.765625
train loss:  0.4906238615512848
train gradient:  0.13253703815735834
iteration : 5757
train acc:  0.703125
train loss:  0.54546058177948
train gradient:  0.1915012440024271
iteration : 5758
train acc:  0.7578125
train loss:  0.4902677536010742
train gradient:  0.13847402785471577
iteration : 5759
train acc:  0.7265625
train loss:  0.5327291488647461
train gradient:  0.1444787103953995
iteration : 5760
train acc:  0.765625
train loss:  0.5047603249549866
train gradient:  0.1588506186433005
iteration : 5761
train acc:  0.796875
train loss:  0.44817572832107544
train gradient:  0.11015020249143609
iteration : 5762
train acc:  0.6796875
train loss:  0.5619423389434814
train gradient:  0.1692204352811928
iteration : 5763
train acc:  0.71875
train loss:  0.5149767398834229
train gradient:  0.16103893828069402
iteration : 5764
train acc:  0.75
train loss:  0.4956565499305725
train gradient:  0.11381355682512365
iteration : 5765
train acc:  0.71875
train loss:  0.5349535942077637
train gradient:  0.12748712820158348
iteration : 5766
train acc:  0.71875
train loss:  0.5184635519981384
train gradient:  0.18494862698797282
iteration : 5767
train acc:  0.765625
train loss:  0.45698511600494385
train gradient:  0.1372115889070768
iteration : 5768
train acc:  0.6640625
train loss:  0.4933796226978302
train gradient:  0.12460918683868652
iteration : 5769
train acc:  0.703125
train loss:  0.5264906883239746
train gradient:  0.15706776679992768
iteration : 5770
train acc:  0.75
train loss:  0.534355640411377
train gradient:  0.12697861944330086
iteration : 5771
train acc:  0.6875
train loss:  0.5431827306747437
train gradient:  0.16865790554126187
iteration : 5772
train acc:  0.6796875
train loss:  0.5757132768630981
train gradient:  0.19000560565725752
iteration : 5773
train acc:  0.7890625
train loss:  0.43713799118995667
train gradient:  0.12800888808826605
iteration : 5774
train acc:  0.7109375
train loss:  0.5192192792892456
train gradient:  0.1420253905102317
iteration : 5775
train acc:  0.7109375
train loss:  0.5300971269607544
train gradient:  0.1791356402918982
iteration : 5776
train acc:  0.765625
train loss:  0.4718584716320038
train gradient:  0.12290338083714322
iteration : 5777
train acc:  0.71875
train loss:  0.5076844096183777
train gradient:  0.16055203413929048
iteration : 5778
train acc:  0.7734375
train loss:  0.4843562841415405
train gradient:  0.13721222339107017
iteration : 5779
train acc:  0.6953125
train loss:  0.5683082342147827
train gradient:  0.1660700670219643
iteration : 5780
train acc:  0.7109375
train loss:  0.514268159866333
train gradient:  0.14672130387324733
iteration : 5781
train acc:  0.7578125
train loss:  0.47554025053977966
train gradient:  0.13682936523818856
iteration : 5782
train acc:  0.6796875
train loss:  0.5442983508110046
train gradient:  0.16252606237389997
iteration : 5783
train acc:  0.625
train loss:  0.5816231966018677
train gradient:  0.18891777536661752
iteration : 5784
train acc:  0.7265625
train loss:  0.5450527667999268
train gradient:  0.13539844115421112
iteration : 5785
train acc:  0.75
train loss:  0.5150559544563293
train gradient:  0.1354792399863748
iteration : 5786
train acc:  0.75
train loss:  0.44523748755455017
train gradient:  0.09938280373213113
iteration : 5787
train acc:  0.71875
train loss:  0.4978155493736267
train gradient:  0.16062664298702306
iteration : 5788
train acc:  0.7109375
train loss:  0.5599027872085571
train gradient:  0.1943445057355164
iteration : 5789
train acc:  0.75
train loss:  0.4557345509529114
train gradient:  0.11981400640399319
iteration : 5790
train acc:  0.734375
train loss:  0.472551554441452
train gradient:  0.10850244942442859
iteration : 5791
train acc:  0.7109375
train loss:  0.5299707651138306
train gradient:  0.1523030684878878
iteration : 5792
train acc:  0.703125
train loss:  0.4968395233154297
train gradient:  0.11897872141185326
iteration : 5793
train acc:  0.6796875
train loss:  0.5658285617828369
train gradient:  0.16245934748236526
iteration : 5794
train acc:  0.8203125
train loss:  0.4556688070297241
train gradient:  0.11410253689953695
iteration : 5795
train acc:  0.765625
train loss:  0.448318749666214
train gradient:  0.14575613104860002
iteration : 5796
train acc:  0.671875
train loss:  0.584720253944397
train gradient:  0.1682353768112797
iteration : 5797
train acc:  0.7734375
train loss:  0.4804908037185669
train gradient:  0.14819858139996395
iteration : 5798
train acc:  0.8125
train loss:  0.4560820758342743
train gradient:  0.10158277427848737
iteration : 5799
train acc:  0.7265625
train loss:  0.501091718673706
train gradient:  0.12420548348758106
iteration : 5800
train acc:  0.75
train loss:  0.5009333491325378
train gradient:  0.14489687727410522
iteration : 5801
train acc:  0.7421875
train loss:  0.4927627444267273
train gradient:  0.110974843576317
iteration : 5802
train acc:  0.7421875
train loss:  0.5514543056488037
train gradient:  0.15398106758529084
iteration : 5803
train acc:  0.703125
train loss:  0.552159309387207
train gradient:  0.1663423442692314
iteration : 5804
train acc:  0.703125
train loss:  0.5102852582931519
train gradient:  0.18307605347388625
iteration : 5805
train acc:  0.6953125
train loss:  0.5195342302322388
train gradient:  0.1630252938247863
iteration : 5806
train acc:  0.7421875
train loss:  0.4722864627838135
train gradient:  0.13150307169908815
iteration : 5807
train acc:  0.7578125
train loss:  0.5093125104904175
train gradient:  0.12839640750667225
iteration : 5808
train acc:  0.75
train loss:  0.5014691352844238
train gradient:  0.11693494754038795
iteration : 5809
train acc:  0.7421875
train loss:  0.49249404668807983
train gradient:  0.12507895702366292
iteration : 5810
train acc:  0.765625
train loss:  0.477699339389801
train gradient:  0.13551801015078338
iteration : 5811
train acc:  0.75
train loss:  0.46578896045684814
train gradient:  0.14102771476603226
iteration : 5812
train acc:  0.6484375
train loss:  0.5941096544265747
train gradient:  0.2330948035232142
iteration : 5813
train acc:  0.796875
train loss:  0.4360121488571167
train gradient:  0.11502766804405082
iteration : 5814
train acc:  0.75
train loss:  0.5355063676834106
train gradient:  0.1726802648222535
iteration : 5815
train acc:  0.703125
train loss:  0.5659160614013672
train gradient:  0.1720835602091673
iteration : 5816
train acc:  0.7109375
train loss:  0.509742259979248
train gradient:  0.12164301369710044
iteration : 5817
train acc:  0.6875
train loss:  0.5382274389266968
train gradient:  0.15247312944978558
iteration : 5818
train acc:  0.734375
train loss:  0.49709397554397583
train gradient:  0.11968077925482817
iteration : 5819
train acc:  0.703125
train loss:  0.499362975358963
train gradient:  0.12519549119282414
iteration : 5820
train acc:  0.7265625
train loss:  0.5416689515113831
train gradient:  0.15761306363317928
iteration : 5821
train acc:  0.7734375
train loss:  0.4578168988227844
train gradient:  0.15132410100594856
iteration : 5822
train acc:  0.765625
train loss:  0.5112395882606506
train gradient:  0.13405674892142366
iteration : 5823
train acc:  0.8046875
train loss:  0.4564259648323059
train gradient:  0.117895890300315
iteration : 5824
train acc:  0.78125
train loss:  0.4640098810195923
train gradient:  0.12762647410888597
iteration : 5825
train acc:  0.6796875
train loss:  0.5274397134780884
train gradient:  0.14597000777799135
iteration : 5826
train acc:  0.671875
train loss:  0.5559788942337036
train gradient:  0.21710583109736997
iteration : 5827
train acc:  0.703125
train loss:  0.5517597794532776
train gradient:  0.1984436275495739
iteration : 5828
train acc:  0.8046875
train loss:  0.4570801854133606
train gradient:  0.12064435715423388
iteration : 5829
train acc:  0.7265625
train loss:  0.5018146634101868
train gradient:  0.12093494284611539
iteration : 5830
train acc:  0.75
train loss:  0.5341060757637024
train gradient:  0.2006603704379452
iteration : 5831
train acc:  0.7265625
train loss:  0.4931160807609558
train gradient:  0.13771308361362053
iteration : 5832
train acc:  0.7421875
train loss:  0.5097947120666504
train gradient:  0.15913838947474873
iteration : 5833
train acc:  0.7265625
train loss:  0.5059776306152344
train gradient:  0.1364718369796299
iteration : 5834
train acc:  0.7109375
train loss:  0.5040245652198792
train gradient:  0.1538108328297687
iteration : 5835
train acc:  0.7734375
train loss:  0.47985634207725525
train gradient:  0.12713998453753403
iteration : 5836
train acc:  0.75
train loss:  0.5034220814704895
train gradient:  0.13069585794852212
iteration : 5837
train acc:  0.75
train loss:  0.4960668087005615
train gradient:  0.1880843155644098
iteration : 5838
train acc:  0.734375
train loss:  0.4436221122741699
train gradient:  0.11172475654967579
iteration : 5839
train acc:  0.7421875
train loss:  0.49706411361694336
train gradient:  0.13156832361212878
iteration : 5840
train acc:  0.765625
train loss:  0.48625001311302185
train gradient:  0.18368878518784454
iteration : 5841
train acc:  0.7265625
train loss:  0.467278391122818
train gradient:  0.13321897636284002
iteration : 5842
train acc:  0.6953125
train loss:  0.5944744348526001
train gradient:  0.1919917004620061
iteration : 5843
train acc:  0.7734375
train loss:  0.5210400819778442
train gradient:  0.17705394954157422
iteration : 5844
train acc:  0.7265625
train loss:  0.5377134680747986
train gradient:  0.1502011307104581
iteration : 5845
train acc:  0.734375
train loss:  0.4997444748878479
train gradient:  0.15874091918277025
iteration : 5846
train acc:  0.640625
train loss:  0.6571273803710938
train gradient:  0.20481105701165223
iteration : 5847
train acc:  0.75
train loss:  0.48585134744644165
train gradient:  0.12304634514468184
iteration : 5848
train acc:  0.7421875
train loss:  0.49076569080352783
train gradient:  0.15519276783813946
iteration : 5849
train acc:  0.7421875
train loss:  0.5145811438560486
train gradient:  0.18248621344232613
iteration : 5850
train acc:  0.7109375
train loss:  0.552719235420227
train gradient:  0.16238730371129922
iteration : 5851
train acc:  0.7265625
train loss:  0.5240398645401001
train gradient:  0.13721406633256117
iteration : 5852
train acc:  0.703125
train loss:  0.5289693474769592
train gradient:  0.18718210450699518
iteration : 5853
train acc:  0.703125
train loss:  0.5236371159553528
train gradient:  0.13699363029018363
iteration : 5854
train acc:  0.71875
train loss:  0.515629768371582
train gradient:  0.15725658672090495
iteration : 5855
train acc:  0.7265625
train loss:  0.4992481470108032
train gradient:  0.12469292077914298
iteration : 5856
train acc:  0.7421875
train loss:  0.526884138584137
train gradient:  0.16860973819170108
iteration : 5857
train acc:  0.71875
train loss:  0.5055841207504272
train gradient:  0.16057310197271654
iteration : 5858
train acc:  0.6875
train loss:  0.5096110105514526
train gradient:  0.19004388312936848
iteration : 5859
train acc:  0.7734375
train loss:  0.47343116998672485
train gradient:  0.13293469147144632
iteration : 5860
train acc:  0.6953125
train loss:  0.5390846729278564
train gradient:  0.16527211999380342
iteration : 5861
train acc:  0.71875
train loss:  0.47322994470596313
train gradient:  0.13905865341938328
iteration : 5862
train acc:  0.71875
train loss:  0.5159051418304443
train gradient:  0.1362907735705395
iteration : 5863
train acc:  0.75
train loss:  0.4563104808330536
train gradient:  0.10207363248017161
iteration : 5864
train acc:  0.7265625
train loss:  0.5039915442466736
train gradient:  0.14123315901960218
iteration : 5865
train acc:  0.75
train loss:  0.48610132932662964
train gradient:  0.1464568455943574
iteration : 5866
train acc:  0.734375
train loss:  0.46743059158325195
train gradient:  0.14482060144163073
iteration : 5867
train acc:  0.6953125
train loss:  0.4967982769012451
train gradient:  0.1454000796430952
iteration : 5868
train acc:  0.7421875
train loss:  0.5072448253631592
train gradient:  0.1192497822122524
iteration : 5869
train acc:  0.7109375
train loss:  0.566213846206665
train gradient:  0.18920883381680256
iteration : 5870
train acc:  0.7578125
train loss:  0.484019935131073
train gradient:  0.13826349849715439
iteration : 5871
train acc:  0.671875
train loss:  0.5406988859176636
train gradient:  0.1322765995039661
iteration : 5872
train acc:  0.734375
train loss:  0.5009165406227112
train gradient:  0.1663120521695826
iteration : 5873
train acc:  0.703125
train loss:  0.5461380481719971
train gradient:  0.17972608876001273
iteration : 5874
train acc:  0.734375
train loss:  0.4852679371833801
train gradient:  0.1607553148892415
iteration : 5875
train acc:  0.671875
train loss:  0.6037536859512329
train gradient:  0.19981681912207824
iteration : 5876
train acc:  0.7578125
train loss:  0.47485536336898804
train gradient:  0.13088809877099283
iteration : 5877
train acc:  0.6875
train loss:  0.552384614944458
train gradient:  0.15036790282022783
iteration : 5878
train acc:  0.6953125
train loss:  0.5104441046714783
train gradient:  0.2134566909056383
iteration : 5879
train acc:  0.7265625
train loss:  0.5163829326629639
train gradient:  0.1541675839355447
iteration : 5880
train acc:  0.7265625
train loss:  0.5264076590538025
train gradient:  0.1496078800831866
iteration : 5881
train acc:  0.7578125
train loss:  0.5006784200668335
train gradient:  0.15393628320562325
iteration : 5882
train acc:  0.734375
train loss:  0.5232107639312744
train gradient:  0.13077924673462382
iteration : 5883
train acc:  0.7421875
train loss:  0.44618046283721924
train gradient:  0.11247458306968101
iteration : 5884
train acc:  0.765625
train loss:  0.49462011456489563
train gradient:  0.1443460890765295
iteration : 5885
train acc:  0.6875
train loss:  0.5315210819244385
train gradient:  0.18509294939111626
iteration : 5886
train acc:  0.7578125
train loss:  0.538155734539032
train gradient:  0.21214382153433164
iteration : 5887
train acc:  0.7734375
train loss:  0.4733557403087616
train gradient:  0.1179045741917637
iteration : 5888
train acc:  0.78125
train loss:  0.40587255358695984
train gradient:  0.1139303595375634
iteration : 5889
train acc:  0.7109375
train loss:  0.5345548987388611
train gradient:  0.13175166455172949
iteration : 5890
train acc:  0.765625
train loss:  0.47030502557754517
train gradient:  0.11551138509219347
iteration : 5891
train acc:  0.734375
train loss:  0.48863425850868225
train gradient:  0.12682759954464629
iteration : 5892
train acc:  0.75
train loss:  0.4875977039337158
train gradient:  0.13822008472432984
iteration : 5893
train acc:  0.671875
train loss:  0.5753188133239746
train gradient:  0.1849567921957303
iteration : 5894
train acc:  0.734375
train loss:  0.5040658116340637
train gradient:  0.1448861375493046
iteration : 5895
train acc:  0.7890625
train loss:  0.43213188648223877
train gradient:  0.11354108154033571
iteration : 5896
train acc:  0.7421875
train loss:  0.4504660665988922
train gradient:  0.1273320329726519
iteration : 5897
train acc:  0.703125
train loss:  0.5515643358230591
train gradient:  0.186751079801472
iteration : 5898
train acc:  0.8046875
train loss:  0.43830883502960205
train gradient:  0.11671394904640325
iteration : 5899
train acc:  0.765625
train loss:  0.4713004231452942
train gradient:  0.14975993424407452
iteration : 5900
train acc:  0.7109375
train loss:  0.5439310073852539
train gradient:  0.14615436378693303
iteration : 5901
train acc:  0.71875
train loss:  0.5162243843078613
train gradient:  0.156796191738739
iteration : 5902
train acc:  0.78125
train loss:  0.4490608274936676
train gradient:  0.1366044846990772
iteration : 5903
train acc:  0.796875
train loss:  0.44339075684547424
train gradient:  0.11379572501607087
iteration : 5904
train acc:  0.7578125
train loss:  0.5148226022720337
train gradient:  0.12591768077420096
iteration : 5905
train acc:  0.6953125
train loss:  0.5471893548965454
train gradient:  0.15387805769550747
iteration : 5906
train acc:  0.6875
train loss:  0.5463204383850098
train gradient:  0.15592901342749632
iteration : 5907
train acc:  0.7890625
train loss:  0.5005007982254028
train gradient:  0.15883245919706812
iteration : 5908
train acc:  0.7578125
train loss:  0.5064655542373657
train gradient:  0.12838578601194905
iteration : 5909
train acc:  0.7265625
train loss:  0.5314186215400696
train gradient:  0.15425568003478607
iteration : 5910
train acc:  0.7265625
train loss:  0.5252756476402283
train gradient:  0.13811007788912358
iteration : 5911
train acc:  0.703125
train loss:  0.5520710945129395
train gradient:  0.17414140855315807
iteration : 5912
train acc:  0.71875
train loss:  0.4768108129501343
train gradient:  0.12901800772113514
iteration : 5913
train acc:  0.7109375
train loss:  0.5509051084518433
train gradient:  0.1794390489193484
iteration : 5914
train acc:  0.7421875
train loss:  0.4716835021972656
train gradient:  0.10990615857343121
iteration : 5915
train acc:  0.75
train loss:  0.5202203392982483
train gradient:  0.19544634391216364
iteration : 5916
train acc:  0.78125
train loss:  0.502243161201477
train gradient:  0.15468084536819335
iteration : 5917
train acc:  0.6640625
train loss:  0.5654593706130981
train gradient:  0.16428492462495492
iteration : 5918
train acc:  0.734375
train loss:  0.4981843829154968
train gradient:  0.1582233961330229
iteration : 5919
train acc:  0.8046875
train loss:  0.4266229271888733
train gradient:  0.14318709924881867
iteration : 5920
train acc:  0.828125
train loss:  0.4357966482639313
train gradient:  0.1404420464190596
iteration : 5921
train acc:  0.6796875
train loss:  0.5702438354492188
train gradient:  0.15165366416255366
iteration : 5922
train acc:  0.7109375
train loss:  0.5491341352462769
train gradient:  0.14869319492402042
iteration : 5923
train acc:  0.7109375
train loss:  0.5094230771064758
train gradient:  0.14513230532259871
iteration : 5924
train acc:  0.6796875
train loss:  0.5090321898460388
train gradient:  0.12760801960972457
iteration : 5925
train acc:  0.7734375
train loss:  0.44144004583358765
train gradient:  0.1231356249996444
iteration : 5926
train acc:  0.7421875
train loss:  0.5056580901145935
train gradient:  0.14448188776180287
iteration : 5927
train acc:  0.71875
train loss:  0.5019346475601196
train gradient:  0.1496753777361281
iteration : 5928
train acc:  0.7578125
train loss:  0.4723811745643616
train gradient:  0.14295086643582003
iteration : 5929
train acc:  0.78125
train loss:  0.49164438247680664
train gradient:  0.1159613778875766
iteration : 5930
train acc:  0.765625
train loss:  0.5036303997039795
train gradient:  0.19220274819126826
iteration : 5931
train acc:  0.734375
train loss:  0.5036728382110596
train gradient:  0.1666218265469082
iteration : 5932
train acc:  0.6640625
train loss:  0.5861955285072327
train gradient:  0.1611054671912059
iteration : 5933
train acc:  0.734375
train loss:  0.4962714612483978
train gradient:  0.1738048642149671
iteration : 5934
train acc:  0.7265625
train loss:  0.515981912612915
train gradient:  0.16227719654991957
iteration : 5935
train acc:  0.7421875
train loss:  0.4936521053314209
train gradient:  0.12088677399626653
iteration : 5936
train acc:  0.7421875
train loss:  0.505629301071167
train gradient:  0.15707792078851432
iteration : 5937
train acc:  0.7578125
train loss:  0.5065673589706421
train gradient:  0.140697764273621
iteration : 5938
train acc:  0.7265625
train loss:  0.47886157035827637
train gradient:  0.14141511799002487
iteration : 5939
train acc:  0.6953125
train loss:  0.5321531295776367
train gradient:  0.15785300472717978
iteration : 5940
train acc:  0.734375
train loss:  0.5214799642562866
train gradient:  0.14166687142366496
iteration : 5941
train acc:  0.71875
train loss:  0.5629101991653442
train gradient:  0.17956508574269398
iteration : 5942
train acc:  0.734375
train loss:  0.5060731172561646
train gradient:  0.14510418617551257
iteration : 5943
train acc:  0.7421875
train loss:  0.44438958168029785
train gradient:  0.10656733353445064
iteration : 5944
train acc:  0.7109375
train loss:  0.5219319462776184
train gradient:  0.13327174855541216
iteration : 5945
train acc:  0.71875
train loss:  0.5007147192955017
train gradient:  0.13308387938581434
iteration : 5946
train acc:  0.7265625
train loss:  0.47052091360092163
train gradient:  0.12610318900653497
iteration : 5947
train acc:  0.78125
train loss:  0.461109459400177
train gradient:  0.1120840452980163
iteration : 5948
train acc:  0.71875
train loss:  0.45872414112091064
train gradient:  0.11175928841641285
iteration : 5949
train acc:  0.6953125
train loss:  0.5139451026916504
train gradient:  0.1558537085003862
iteration : 5950
train acc:  0.7890625
train loss:  0.43715447187423706
train gradient:  0.10471263041092806
iteration : 5951
train acc:  0.765625
train loss:  0.44535183906555176
train gradient:  0.12971386042783817
iteration : 5952
train acc:  0.7265625
train loss:  0.6021482944488525
train gradient:  0.18043766072187162
iteration : 5953
train acc:  0.703125
train loss:  0.5322132706642151
train gradient:  0.1417491028392674
iteration : 5954
train acc:  0.75
train loss:  0.47921648621559143
train gradient:  0.1651893012754324
iteration : 5955
train acc:  0.7109375
train loss:  0.5343604683876038
train gradient:  0.18437605443116503
iteration : 5956
train acc:  0.734375
train loss:  0.4818699061870575
train gradient:  0.12987857032669956
iteration : 5957
train acc:  0.6953125
train loss:  0.5380721092224121
train gradient:  0.17083609247571863
iteration : 5958
train acc:  0.734375
train loss:  0.5241403579711914
train gradient:  0.14817468712814436
iteration : 5959
train acc:  0.7265625
train loss:  0.5286479592323303
train gradient:  0.18484353009417628
iteration : 5960
train acc:  0.7265625
train loss:  0.5053313970565796
train gradient:  0.15808408950742367
iteration : 5961
train acc:  0.71875
train loss:  0.5255296230316162
train gradient:  0.13631145444518147
iteration : 5962
train acc:  0.7109375
train loss:  0.5998075008392334
train gradient:  0.18674288044276893
iteration : 5963
train acc:  0.796875
train loss:  0.43587616086006165
train gradient:  0.12750858358524111
iteration : 5964
train acc:  0.75
train loss:  0.5049201250076294
train gradient:  0.12840773242690778
iteration : 5965
train acc:  0.7265625
train loss:  0.5581550002098083
train gradient:  0.1901353640930031
iteration : 5966
train acc:  0.7265625
train loss:  0.5413177013397217
train gradient:  0.1274047486254557
iteration : 5967
train acc:  0.7734375
train loss:  0.48546871542930603
train gradient:  0.18961136943158485
iteration : 5968
train acc:  0.7265625
train loss:  0.5282490849494934
train gradient:  0.14520255668050175
iteration : 5969
train acc:  0.6953125
train loss:  0.5505728721618652
train gradient:  0.19595744458982142
iteration : 5970
train acc:  0.7265625
train loss:  0.5017247200012207
train gradient:  0.1173660638580308
iteration : 5971
train acc:  0.7421875
train loss:  0.5224725604057312
train gradient:  0.18978709224761026
iteration : 5972
train acc:  0.7734375
train loss:  0.48728126287460327
train gradient:  0.13975101375411703
iteration : 5973
train acc:  0.78125
train loss:  0.454251766204834
train gradient:  0.10905312133056197
iteration : 5974
train acc:  0.7421875
train loss:  0.5293183326721191
train gradient:  0.12930647551737312
iteration : 5975
train acc:  0.765625
train loss:  0.47418200969696045
train gradient:  0.14328031047508577
iteration : 5976
train acc:  0.703125
train loss:  0.5574975609779358
train gradient:  0.20421431326310302
iteration : 5977
train acc:  0.6875
train loss:  0.5284774899482727
train gradient:  0.19908767814870726
iteration : 5978
train acc:  0.7109375
train loss:  0.5083225965499878
train gradient:  0.13402637872715156
iteration : 5979
train acc:  0.671875
train loss:  0.626508355140686
train gradient:  0.18270108307163707
iteration : 5980
train acc:  0.6953125
train loss:  0.5789017677307129
train gradient:  0.19169817698180924
iteration : 5981
train acc:  0.734375
train loss:  0.5024900436401367
train gradient:  0.14634300893281155
iteration : 5982
train acc:  0.734375
train loss:  0.5062894225120544
train gradient:  0.14091092715384568
iteration : 5983
train acc:  0.765625
train loss:  0.4763312339782715
train gradient:  0.1340713373222177
iteration : 5984
train acc:  0.7734375
train loss:  0.4647268056869507
train gradient:  0.15569469052225043
iteration : 5985
train acc:  0.7578125
train loss:  0.4749254882335663
train gradient:  0.1312402244352625
iteration : 5986
train acc:  0.6875
train loss:  0.5655407905578613
train gradient:  0.14609465930744042
iteration : 5987
train acc:  0.796875
train loss:  0.4304203391075134
train gradient:  0.1090780110213259
iteration : 5988
train acc:  0.7734375
train loss:  0.516457200050354
train gradient:  0.15556387125706422
iteration : 5989
train acc:  0.75
train loss:  0.477257639169693
train gradient:  0.11181241861486042
iteration : 5990
train acc:  0.7734375
train loss:  0.540206789970398
train gradient:  0.14986574424775587
iteration : 5991
train acc:  0.75
train loss:  0.4780619740486145
train gradient:  0.11500040635735954
iteration : 5992
train acc:  0.703125
train loss:  0.5356191396713257
train gradient:  0.19762289644718786
iteration : 5993
train acc:  0.7265625
train loss:  0.5090072154998779
train gradient:  0.14321635392290674
iteration : 5994
train acc:  0.609375
train loss:  0.5934540033340454
train gradient:  0.2038461520457891
iteration : 5995
train acc:  0.6953125
train loss:  0.5364269018173218
train gradient:  0.13307116391895382
iteration : 5996
train acc:  0.796875
train loss:  0.4417220950126648
train gradient:  0.134562839712026
iteration : 5997
train acc:  0.765625
train loss:  0.4426652193069458
train gradient:  0.13071067247122942
iteration : 5998
train acc:  0.6796875
train loss:  0.5689970850944519
train gradient:  0.18827116786309891
iteration : 5999
train acc:  0.765625
train loss:  0.47368693351745605
train gradient:  0.14436558703399208
iteration : 6000
train acc:  0.6484375
train loss:  0.6337830424308777
train gradient:  0.21790086426057548
iteration : 6001
train acc:  0.734375
train loss:  0.5351371765136719
train gradient:  0.14474661118345952
iteration : 6002
train acc:  0.7265625
train loss:  0.5058737993240356
train gradient:  0.16053220701069443
iteration : 6003
train acc:  0.71875
train loss:  0.5457698106765747
train gradient:  0.134181418661248
iteration : 6004
train acc:  0.734375
train loss:  0.5082908868789673
train gradient:  0.14364888443995852
iteration : 6005
train acc:  0.7734375
train loss:  0.45846080780029297
train gradient:  0.16762590524022047
iteration : 6006
train acc:  0.75
train loss:  0.4695355296134949
train gradient:  0.17358675746446425
iteration : 6007
train acc:  0.6875
train loss:  0.5391091108322144
train gradient:  0.16826376447150504
iteration : 6008
train acc:  0.7578125
train loss:  0.4767078161239624
train gradient:  0.11907233179234572
iteration : 6009
train acc:  0.828125
train loss:  0.4114220142364502
train gradient:  0.1017237779918784
iteration : 6010
train acc:  0.7734375
train loss:  0.4547877311706543
train gradient:  0.12111757784886594
iteration : 6011
train acc:  0.6875
train loss:  0.6116812229156494
train gradient:  0.16288717188562535
iteration : 6012
train acc:  0.734375
train loss:  0.5446324348449707
train gradient:  0.16976086662015916
iteration : 6013
train acc:  0.6171875
train loss:  0.5420520305633545
train gradient:  0.16157373403502623
iteration : 6014
train acc:  0.7265625
train loss:  0.494197279214859
train gradient:  0.1639729904821186
iteration : 6015
train acc:  0.6796875
train loss:  0.5811156630516052
train gradient:  0.18206869929781827
iteration : 6016
train acc:  0.765625
train loss:  0.48021236062049866
train gradient:  0.1639598287781307
iteration : 6017
train acc:  0.71875
train loss:  0.4603338837623596
train gradient:  0.1211227047626087
iteration : 6018
train acc:  0.7734375
train loss:  0.4463493227958679
train gradient:  0.12440891376665238
iteration : 6019
train acc:  0.7265625
train loss:  0.5360890626907349
train gradient:  0.18672255336249122
iteration : 6020
train acc:  0.75
train loss:  0.45129698514938354
train gradient:  0.1565258597036932
iteration : 6021
train acc:  0.7265625
train loss:  0.5595256090164185
train gradient:  0.15085665662240855
iteration : 6022
train acc:  0.71875
train loss:  0.5110526084899902
train gradient:  0.19071914619469363
iteration : 6023
train acc:  0.7265625
train loss:  0.5113121271133423
train gradient:  0.15666427372421698
iteration : 6024
train acc:  0.734375
train loss:  0.5283649563789368
train gradient:  0.14805036691299006
iteration : 6025
train acc:  0.7109375
train loss:  0.5463871359825134
train gradient:  0.19751274542342434
iteration : 6026
train acc:  0.65625
train loss:  0.5550939440727234
train gradient:  0.16997975949420968
iteration : 6027
train acc:  0.765625
train loss:  0.5688558220863342
train gradient:  0.1674037919040947
iteration : 6028
train acc:  0.6953125
train loss:  0.5405087471008301
train gradient:  0.1520589396875071
iteration : 6029
train acc:  0.6875
train loss:  0.5789195895195007
train gradient:  0.1884563819612976
iteration : 6030
train acc:  0.6796875
train loss:  0.5716010332107544
train gradient:  0.15529743367206614
iteration : 6031
train acc:  0.640625
train loss:  0.560967206954956
train gradient:  0.13681023566755302
iteration : 6032
train acc:  0.734375
train loss:  0.48823943734169006
train gradient:  0.13058703037321218
iteration : 6033
train acc:  0.7421875
train loss:  0.4859004616737366
train gradient:  0.1477361636752595
iteration : 6034
train acc:  0.7578125
train loss:  0.49852457642555237
train gradient:  0.1178836951683154
iteration : 6035
train acc:  0.75
train loss:  0.461301326751709
train gradient:  0.15216526223973192
iteration : 6036
train acc:  0.796875
train loss:  0.46261757612228394
train gradient:  0.11220224797744885
iteration : 6037
train acc:  0.7265625
train loss:  0.5482289791107178
train gradient:  0.19337321356306458
iteration : 6038
train acc:  0.65625
train loss:  0.6483839154243469
train gradient:  0.29291389209736884
iteration : 6039
train acc:  0.71875
train loss:  0.5284549593925476
train gradient:  0.14084378898676184
iteration : 6040
train acc:  0.7265625
train loss:  0.500612735748291
train gradient:  0.13350439613285386
iteration : 6041
train acc:  0.6796875
train loss:  0.5645118355751038
train gradient:  0.1573264870787187
iteration : 6042
train acc:  0.6875
train loss:  0.5576867461204529
train gradient:  0.18183897663290016
iteration : 6043
train acc:  0.625
train loss:  0.5759401917457581
train gradient:  0.18772358138489653
iteration : 6044
train acc:  0.703125
train loss:  0.5747030973434448
train gradient:  0.18364520100979037
iteration : 6045
train acc:  0.7421875
train loss:  0.5058435201644897
train gradient:  0.13300588663165233
iteration : 6046
train acc:  0.7421875
train loss:  0.5157448649406433
train gradient:  0.17814850557184847
iteration : 6047
train acc:  0.6875
train loss:  0.5283982753753662
train gradient:  0.14326135343289215
iteration : 6048
train acc:  0.6953125
train loss:  0.5384559035301208
train gradient:  0.16524403506207336
iteration : 6049
train acc:  0.7265625
train loss:  0.4950178861618042
train gradient:  0.1443612212304715
iteration : 6050
train acc:  0.6796875
train loss:  0.5366728901863098
train gradient:  0.14798183663906855
iteration : 6051
train acc:  0.7421875
train loss:  0.507887601852417
train gradient:  0.13529738527375565
iteration : 6052
train acc:  0.734375
train loss:  0.46594950556755066
train gradient:  0.12259835361682486
iteration : 6053
train acc:  0.6796875
train loss:  0.6412235498428345
train gradient:  0.24206169705886643
iteration : 6054
train acc:  0.7421875
train loss:  0.4656658172607422
train gradient:  0.10782334730146692
iteration : 6055
train acc:  0.7578125
train loss:  0.5052295327186584
train gradient:  0.16855426234971882
iteration : 6056
train acc:  0.6875
train loss:  0.5213667154312134
train gradient:  0.1312155729575622
iteration : 6057
train acc:  0.6875
train loss:  0.5208380222320557
train gradient:  0.14897579158821825
iteration : 6058
train acc:  0.7890625
train loss:  0.4420474171638489
train gradient:  0.09892530881832919
iteration : 6059
train acc:  0.71875
train loss:  0.5172570943832397
train gradient:  0.1336670340009234
iteration : 6060
train acc:  0.7265625
train loss:  0.5300674438476562
train gradient:  0.13026541117877152
iteration : 6061
train acc:  0.7578125
train loss:  0.5125811696052551
train gradient:  0.14204766971618157
iteration : 6062
train acc:  0.6953125
train loss:  0.5246515274047852
train gradient:  0.1559196176393896
iteration : 6063
train acc:  0.6953125
train loss:  0.5110111832618713
train gradient:  0.11074691771726226
iteration : 6064
train acc:  0.7578125
train loss:  0.4433010220527649
train gradient:  0.12840428701947854
iteration : 6065
train acc:  0.734375
train loss:  0.5009633302688599
train gradient:  0.15063071499953457
iteration : 6066
train acc:  0.78125
train loss:  0.46325087547302246
train gradient:  0.16391395951795534
iteration : 6067
train acc:  0.7109375
train loss:  0.5406464338302612
train gradient:  0.13707419828918
iteration : 6068
train acc:  0.7265625
train loss:  0.4882538318634033
train gradient:  0.14492356922363014
iteration : 6069
train acc:  0.6953125
train loss:  0.5447301864624023
train gradient:  0.12885407984090652
iteration : 6070
train acc:  0.6796875
train loss:  0.5406979322433472
train gradient:  0.13158380181863288
iteration : 6071
train acc:  0.796875
train loss:  0.495897501707077
train gradient:  0.1491779706560838
iteration : 6072
train acc:  0.7265625
train loss:  0.5091016888618469
train gradient:  0.15259086407825445
iteration : 6073
train acc:  0.6953125
train loss:  0.5548497438430786
train gradient:  0.14214881058264195
iteration : 6074
train acc:  0.75
train loss:  0.4944485127925873
train gradient:  0.13934348574025182
iteration : 6075
train acc:  0.71875
train loss:  0.5027986764907837
train gradient:  0.14027507149615664
iteration : 6076
train acc:  0.7890625
train loss:  0.4414193034172058
train gradient:  0.10138624803612084
iteration : 6077
train acc:  0.7421875
train loss:  0.4794035851955414
train gradient:  0.10703696887854394
iteration : 6078
train acc:  0.7421875
train loss:  0.5057730674743652
train gradient:  0.12217589807438323
iteration : 6079
train acc:  0.7109375
train loss:  0.5258715748786926
train gradient:  0.12610269380599184
iteration : 6080
train acc:  0.7578125
train loss:  0.5171276330947876
train gradient:  0.12413961549610715
iteration : 6081
train acc:  0.7578125
train loss:  0.4619773030281067
train gradient:  0.11322058486896071
iteration : 6082
train acc:  0.7890625
train loss:  0.49326956272125244
train gradient:  0.15263269155478887
iteration : 6083
train acc:  0.8046875
train loss:  0.4241563081741333
train gradient:  0.12570202780195244
iteration : 6084
train acc:  0.7890625
train loss:  0.4669904112815857
train gradient:  0.1099436556146377
iteration : 6085
train acc:  0.7734375
train loss:  0.47931182384490967
train gradient:  0.10613072379605178
iteration : 6086
train acc:  0.7578125
train loss:  0.4967453181743622
train gradient:  0.13961433167707987
iteration : 6087
train acc:  0.75
train loss:  0.5263071060180664
train gradient:  0.1765518488949739
iteration : 6088
train acc:  0.7578125
train loss:  0.4779411554336548
train gradient:  0.17780882897161832
iteration : 6089
train acc:  0.7578125
train loss:  0.5202195048332214
train gradient:  0.1523087826258283
iteration : 6090
train acc:  0.7421875
train loss:  0.5056995749473572
train gradient:  0.12670932307666372
iteration : 6091
train acc:  0.7890625
train loss:  0.512174129486084
train gradient:  0.23893795657891595
iteration : 6092
train acc:  0.6796875
train loss:  0.6038675308227539
train gradient:  0.2447507364782558
iteration : 6093
train acc:  0.796875
train loss:  0.44795429706573486
train gradient:  0.11679361777826862
iteration : 6094
train acc:  0.6640625
train loss:  0.5340670347213745
train gradient:  0.14363612606007295
iteration : 6095
train acc:  0.7421875
train loss:  0.5254946947097778
train gradient:  0.1560856419981257
iteration : 6096
train acc:  0.7109375
train loss:  0.5567074418067932
train gradient:  0.15620223926248988
iteration : 6097
train acc:  0.75
train loss:  0.4858035445213318
train gradient:  0.14011228705284162
iteration : 6098
train acc:  0.7890625
train loss:  0.46791473031044006
train gradient:  0.1464285301702321
iteration : 6099
train acc:  0.65625
train loss:  0.575569748878479
train gradient:  0.15983588026883766
iteration : 6100
train acc:  0.6875
train loss:  0.581963062286377
train gradient:  0.18274242936794866
iteration : 6101
train acc:  0.734375
train loss:  0.5001990795135498
train gradient:  0.15104527673655382
iteration : 6102
train acc:  0.7578125
train loss:  0.4907456040382385
train gradient:  0.1333122438512197
iteration : 6103
train acc:  0.6953125
train loss:  0.5388380289077759
train gradient:  0.14771976328595732
iteration : 6104
train acc:  0.7265625
train loss:  0.4847383499145508
train gradient:  0.11009545559798858
iteration : 6105
train acc:  0.7890625
train loss:  0.4455476999282837
train gradient:  0.09317983805795946
iteration : 6106
train acc:  0.7265625
train loss:  0.48254725337028503
train gradient:  0.13947225186880086
iteration : 6107
train acc:  0.6796875
train loss:  0.5247969627380371
train gradient:  0.15116330128848
iteration : 6108
train acc:  0.765625
train loss:  0.4329153895378113
train gradient:  0.08986977618998145
iteration : 6109
train acc:  0.7421875
train loss:  0.4521026015281677
train gradient:  0.09677197569831865
iteration : 6110
train acc:  0.6953125
train loss:  0.5137847661972046
train gradient:  0.13062735091965033
iteration : 6111
train acc:  0.75
train loss:  0.4660460352897644
train gradient:  0.10633591700954242
iteration : 6112
train acc:  0.734375
train loss:  0.5226049423217773
train gradient:  0.12396430763198248
iteration : 6113
train acc:  0.765625
train loss:  0.5720160007476807
train gradient:  0.14172616179404673
iteration : 6114
train acc:  0.6953125
train loss:  0.5900134444236755
train gradient:  0.1633219234676896
iteration : 6115
train acc:  0.703125
train loss:  0.5170847177505493
train gradient:  0.11897450470390525
iteration : 6116
train acc:  0.6796875
train loss:  0.6337088942527771
train gradient:  0.29909590458442364
iteration : 6117
train acc:  0.6875
train loss:  0.5614432096481323
train gradient:  0.15123663154520833
iteration : 6118
train acc:  0.75
train loss:  0.489223837852478
train gradient:  0.15837672296327526
iteration : 6119
train acc:  0.765625
train loss:  0.4810689091682434
train gradient:  0.14499489371870178
iteration : 6120
train acc:  0.765625
train loss:  0.4411361813545227
train gradient:  0.09833592957129875
iteration : 6121
train acc:  0.734375
train loss:  0.5164496302604675
train gradient:  0.13275579199917742
iteration : 6122
train acc:  0.8046875
train loss:  0.4593859910964966
train gradient:  0.10278539212969823
iteration : 6123
train acc:  0.71875
train loss:  0.5156152248382568
train gradient:  0.1293477045876757
iteration : 6124
train acc:  0.6875
train loss:  0.5158379673957825
train gradient:  0.22087988127624963
iteration : 6125
train acc:  0.78125
train loss:  0.4333791732788086
train gradient:  0.11074001004913377
iteration : 6126
train acc:  0.71875
train loss:  0.47407639026641846
train gradient:  0.11759481230554882
iteration : 6127
train acc:  0.640625
train loss:  0.5619484186172485
train gradient:  0.1563805697026718
iteration : 6128
train acc:  0.6875
train loss:  0.5650932192802429
train gradient:  0.16032594878431
iteration : 6129
train acc:  0.71875
train loss:  0.5316235423088074
train gradient:  0.14614757273749332
iteration : 6130
train acc:  0.734375
train loss:  0.5266928672790527
train gradient:  0.15478728656177626
iteration : 6131
train acc:  0.7578125
train loss:  0.48093587160110474
train gradient:  0.15134914738833377
iteration : 6132
train acc:  0.7265625
train loss:  0.4862433671951294
train gradient:  0.12786324012231703
iteration : 6133
train acc:  0.734375
train loss:  0.5157548189163208
train gradient:  0.14991701458209516
iteration : 6134
train acc:  0.6953125
train loss:  0.5136998295783997
train gradient:  0.14150678746605586
iteration : 6135
train acc:  0.7421875
train loss:  0.5428776741027832
train gradient:  0.17569997266746828
iteration : 6136
train acc:  0.75
train loss:  0.4994906187057495
train gradient:  0.17552690811593302
iteration : 6137
train acc:  0.7109375
train loss:  0.5182198286056519
train gradient:  0.13094717834397585
iteration : 6138
train acc:  0.71875
train loss:  0.5123950242996216
train gradient:  0.12971896935047766
iteration : 6139
train acc:  0.78125
train loss:  0.4838830828666687
train gradient:  0.11229305644679824
iteration : 6140
train acc:  0.6640625
train loss:  0.5671684145927429
train gradient:  0.19275403581485095
iteration : 6141
train acc:  0.6796875
train loss:  0.5556877851486206
train gradient:  0.23256646687931853
iteration : 6142
train acc:  0.6953125
train loss:  0.5278632044792175
train gradient:  0.1529529337205739
iteration : 6143
train acc:  0.734375
train loss:  0.5305162668228149
train gradient:  0.155870280381263
iteration : 6144
train acc:  0.6640625
train loss:  0.5799109935760498
train gradient:  0.15678956819732076
iteration : 6145
train acc:  0.765625
train loss:  0.49258363246917725
train gradient:  0.13754572554219524
iteration : 6146
train acc:  0.6875
train loss:  0.5094238519668579
train gradient:  0.11623113370429572
iteration : 6147
train acc:  0.75
train loss:  0.5032841563224792
train gradient:  0.13205620934278525
iteration : 6148
train acc:  0.765625
train loss:  0.462765634059906
train gradient:  0.11362542722045503
iteration : 6149
train acc:  0.703125
train loss:  0.5003038048744202
train gradient:  0.12066547867406717
iteration : 6150
train acc:  0.7421875
train loss:  0.47439444065093994
train gradient:  0.12416807085334829
iteration : 6151
train acc:  0.75
train loss:  0.5485036373138428
train gradient:  0.15496279535281898
iteration : 6152
train acc:  0.75
train loss:  0.5023788213729858
train gradient:  0.18284441036912913
iteration : 6153
train acc:  0.765625
train loss:  0.5028475522994995
train gradient:  0.19918996921879145
iteration : 6154
train acc:  0.8125
train loss:  0.43583911657333374
train gradient:  0.10389987256621093
iteration : 6155
train acc:  0.7578125
train loss:  0.4881194829940796
train gradient:  0.14414553623962756
iteration : 6156
train acc:  0.7421875
train loss:  0.4959929585456848
train gradient:  0.14945290030794467
iteration : 6157
train acc:  0.671875
train loss:  0.5411813259124756
train gradient:  0.17075634984435256
iteration : 6158
train acc:  0.6953125
train loss:  0.5017340183258057
train gradient:  0.15227148767078713
iteration : 6159
train acc:  0.796875
train loss:  0.4414757490158081
train gradient:  0.11844217806102046
iteration : 6160
train acc:  0.765625
train loss:  0.4874371290206909
train gradient:  0.12743986943257485
iteration : 6161
train acc:  0.765625
train loss:  0.49007314443588257
train gradient:  0.18987423328320868
iteration : 6162
train acc:  0.7421875
train loss:  0.45875081419944763
train gradient:  0.14856148555894616
iteration : 6163
train acc:  0.703125
train loss:  0.578711211681366
train gradient:  0.18211353775550743
iteration : 6164
train acc:  0.765625
train loss:  0.49025726318359375
train gradient:  0.16114036818779748
iteration : 6165
train acc:  0.75
train loss:  0.4980049133300781
train gradient:  0.13389538239675405
iteration : 6166
train acc:  0.6953125
train loss:  0.6017636060714722
train gradient:  0.22012763344661268
iteration : 6167
train acc:  0.765625
train loss:  0.45277053117752075
train gradient:  0.10939319364300663
iteration : 6168
train acc:  0.765625
train loss:  0.5111145973205566
train gradient:  0.15396041743017364
iteration : 6169
train acc:  0.7421875
train loss:  0.49824559688568115
train gradient:  0.128822521300754
iteration : 6170
train acc:  0.78125
train loss:  0.43205907940864563
train gradient:  0.10924028627699027
iteration : 6171
train acc:  0.6953125
train loss:  0.5409848093986511
train gradient:  0.12168483766656561
iteration : 6172
train acc:  0.8203125
train loss:  0.4313405454158783
train gradient:  0.11180865377044233
iteration : 6173
train acc:  0.7265625
train loss:  0.5324418544769287
train gradient:  0.15581579782989582
iteration : 6174
train acc:  0.6875
train loss:  0.5562601089477539
train gradient:  0.15858134734298152
iteration : 6175
train acc:  0.6640625
train loss:  0.5751866698265076
train gradient:  0.16921228826336815
iteration : 6176
train acc:  0.6875
train loss:  0.5685746669769287
train gradient:  0.17773171276774924
iteration : 6177
train acc:  0.7421875
train loss:  0.5048285722732544
train gradient:  0.13409808882380236
iteration : 6178
train acc:  0.7421875
train loss:  0.5009980201721191
train gradient:  0.17343510162549014
iteration : 6179
train acc:  0.7890625
train loss:  0.42451608180999756
train gradient:  0.10633055048745965
iteration : 6180
train acc:  0.6953125
train loss:  0.5351380109786987
train gradient:  0.13229497136606827
iteration : 6181
train acc:  0.734375
train loss:  0.4740092158317566
train gradient:  0.13922302567612183
iteration : 6182
train acc:  0.703125
train loss:  0.5395704507827759
train gradient:  0.1539343298979642
iteration : 6183
train acc:  0.75
train loss:  0.49055343866348267
train gradient:  0.15328096433641142
iteration : 6184
train acc:  0.6875
train loss:  0.5453549027442932
train gradient:  0.23726936484355254
iteration : 6185
train acc:  0.703125
train loss:  0.503200888633728
train gradient:  0.15089293662996928
iteration : 6186
train acc:  0.703125
train loss:  0.5418574213981628
train gradient:  0.25928282337155484
iteration : 6187
train acc:  0.7734375
train loss:  0.48701411485671997
train gradient:  0.12791717440845018
iteration : 6188
train acc:  0.7578125
train loss:  0.4820551574230194
train gradient:  0.12516424504012053
iteration : 6189
train acc:  0.7734375
train loss:  0.449859082698822
train gradient:  0.12099710305640882
iteration : 6190
train acc:  0.703125
train loss:  0.5297060012817383
train gradient:  0.14307314041675598
iteration : 6191
train acc:  0.671875
train loss:  0.5682000517845154
train gradient:  0.1921208870150558
iteration : 6192
train acc:  0.7421875
train loss:  0.48479700088500977
train gradient:  0.13019447692787695
iteration : 6193
train acc:  0.7578125
train loss:  0.4986015558242798
train gradient:  0.17700695977773903
iteration : 6194
train acc:  0.703125
train loss:  0.514886736869812
train gradient:  0.13754565599144075
iteration : 6195
train acc:  0.7421875
train loss:  0.5339884757995605
train gradient:  0.17620430608452115
iteration : 6196
train acc:  0.734375
train loss:  0.47802820801734924
train gradient:  0.12434769278200185
iteration : 6197
train acc:  0.765625
train loss:  0.4826282858848572
train gradient:  0.12203022094071152
iteration : 6198
train acc:  0.75
train loss:  0.47440052032470703
train gradient:  0.11797363161886548
iteration : 6199
train acc:  0.6875
train loss:  0.5692851543426514
train gradient:  0.16232475727316079
iteration : 6200
train acc:  0.7421875
train loss:  0.557209312915802
train gradient:  0.15112190606387937
iteration : 6201
train acc:  0.78125
train loss:  0.44848543405532837
train gradient:  0.13935275861478744
iteration : 6202
train acc:  0.7734375
train loss:  0.4639155864715576
train gradient:  0.14281293090439623
iteration : 6203
train acc:  0.6484375
train loss:  0.58451247215271
train gradient:  0.1806357755805052
iteration : 6204
train acc:  0.7421875
train loss:  0.516865611076355
train gradient:  0.1296357685674614
iteration : 6205
train acc:  0.75
train loss:  0.5148202776908875
train gradient:  0.1416894224669118
iteration : 6206
train acc:  0.703125
train loss:  0.5484551191329956
train gradient:  0.16660019525317166
iteration : 6207
train acc:  0.7421875
train loss:  0.5623612999916077
train gradient:  0.16142802018077249
iteration : 6208
train acc:  0.6953125
train loss:  0.5741637349128723
train gradient:  0.17569806621438389
iteration : 6209
train acc:  0.7734375
train loss:  0.4665828347206116
train gradient:  0.10763044728868382
iteration : 6210
train acc:  0.75
train loss:  0.5125030279159546
train gradient:  0.16255691756061047
iteration : 6211
train acc:  0.6875
train loss:  0.5447522401809692
train gradient:  0.13389815888854978
iteration : 6212
train acc:  0.7265625
train loss:  0.5062859058380127
train gradient:  0.12172309691627255
iteration : 6213
train acc:  0.7265625
train loss:  0.5311870574951172
train gradient:  0.1851237208779204
iteration : 6214
train acc:  0.7265625
train loss:  0.5498663783073425
train gradient:  0.18003802584873546
iteration : 6215
train acc:  0.703125
train loss:  0.5711539387702942
train gradient:  0.16086208266115087
iteration : 6216
train acc:  0.6953125
train loss:  0.5313287973403931
train gradient:  0.16721449914225445
iteration : 6217
train acc:  0.640625
train loss:  0.6110342741012573
train gradient:  0.17654456716685538
iteration : 6218
train acc:  0.7734375
train loss:  0.4705328345298767
train gradient:  0.11606227781259007
iteration : 6219
train acc:  0.734375
train loss:  0.5054187774658203
train gradient:  0.1199018406987689
iteration : 6220
train acc:  0.7109375
train loss:  0.5202549695968628
train gradient:  0.15357576263363898
iteration : 6221
train acc:  0.7109375
train loss:  0.5107419490814209
train gradient:  0.12479077320861073
iteration : 6222
train acc:  0.6640625
train loss:  0.5515472888946533
train gradient:  0.22890578915509197
iteration : 6223
train acc:  0.6953125
train loss:  0.5627940893173218
train gradient:  0.14310238307487738
iteration : 6224
train acc:  0.6796875
train loss:  0.5625997185707092
train gradient:  0.17432438715457876
iteration : 6225
train acc:  0.7421875
train loss:  0.48558956384658813
train gradient:  0.14004186024787915
iteration : 6226
train acc:  0.7109375
train loss:  0.5289359092712402
train gradient:  0.14668827637334597
iteration : 6227
train acc:  0.6953125
train loss:  0.48325857520103455
train gradient:  0.11658478989207921
iteration : 6228
train acc:  0.7109375
train loss:  0.5456242561340332
train gradient:  0.1825295965565581
iteration : 6229
train acc:  0.75
train loss:  0.47211953997612
train gradient:  0.13155750989066067
iteration : 6230
train acc:  0.765625
train loss:  0.5001108646392822
train gradient:  0.13934216410496503
iteration : 6231
train acc:  0.75
train loss:  0.49345627427101135
train gradient:  0.13632969879263784
iteration : 6232
train acc:  0.8046875
train loss:  0.4177771806716919
train gradient:  0.1252994282480936
iteration : 6233
train acc:  0.703125
train loss:  0.4958525002002716
train gradient:  0.14265993146416178
iteration : 6234
train acc:  0.7265625
train loss:  0.5131196975708008
train gradient:  0.175617903593404
iteration : 6235
train acc:  0.6875
train loss:  0.556058406829834
train gradient:  0.16938318351995146
iteration : 6236
train acc:  0.796875
train loss:  0.4810529351234436
train gradient:  0.12367926382551693
iteration : 6237
train acc:  0.7578125
train loss:  0.4860755205154419
train gradient:  0.12174081414567386
iteration : 6238
train acc:  0.734375
train loss:  0.5180505514144897
train gradient:  0.13240930309625132
iteration : 6239
train acc:  0.71875
train loss:  0.4835299253463745
train gradient:  0.1216731981222828
iteration : 6240
train acc:  0.8125
train loss:  0.45881712436676025
train gradient:  0.1460029541549371
iteration : 6241
train acc:  0.734375
train loss:  0.5031332969665527
train gradient:  0.16874367152199662
iteration : 6242
train acc:  0.7578125
train loss:  0.4570329785346985
train gradient:  0.11665671741833764
iteration : 6243
train acc:  0.6953125
train loss:  0.5954536199569702
train gradient:  0.1508243956584171
iteration : 6244
train acc:  0.8046875
train loss:  0.46787333488464355
train gradient:  0.12644417783447914
iteration : 6245
train acc:  0.6875
train loss:  0.5451881289482117
train gradient:  0.1701890757509265
iteration : 6246
train acc:  0.703125
train loss:  0.5531831383705139
train gradient:  0.18414377878975624
iteration : 6247
train acc:  0.6796875
train loss:  0.543738603591919
train gradient:  0.12804538042380847
iteration : 6248
train acc:  0.7578125
train loss:  0.47358575463294983
train gradient:  0.13090489985811013
iteration : 6249
train acc:  0.6953125
train loss:  0.5213523507118225
train gradient:  0.13036543982819268
iteration : 6250
train acc:  0.765625
train loss:  0.4917636811733246
train gradient:  0.11509262657577327
iteration : 6251
train acc:  0.7265625
train loss:  0.4859442412853241
train gradient:  0.11204624749268857
iteration : 6252
train acc:  0.734375
train loss:  0.5600135922431946
train gradient:  0.15949880857996723
iteration : 6253
train acc:  0.734375
train loss:  0.49323785305023193
train gradient:  0.11509652955428334
iteration : 6254
train acc:  0.7421875
train loss:  0.48874562978744507
train gradient:  0.1308784974161608
iteration : 6255
train acc:  0.7109375
train loss:  0.5183491706848145
train gradient:  0.14915570326719826
iteration : 6256
train acc:  0.7578125
train loss:  0.4916749894618988
train gradient:  0.13730358092026726
iteration : 6257
train acc:  0.703125
train loss:  0.513629674911499
train gradient:  0.14214002193402286
iteration : 6258
train acc:  0.640625
train loss:  0.5849581956863403
train gradient:  0.18305769828494162
iteration : 6259
train acc:  0.7734375
train loss:  0.5054646134376526
train gradient:  0.1269854093514054
iteration : 6260
train acc:  0.7109375
train loss:  0.5645217895507812
train gradient:  0.19389205356725855
iteration : 6261
train acc:  0.8046875
train loss:  0.44824451208114624
train gradient:  0.13433663644696742
iteration : 6262
train acc:  0.703125
train loss:  0.5825433731079102
train gradient:  0.15342717892246477
iteration : 6263
train acc:  0.7734375
train loss:  0.48436981439590454
train gradient:  0.13739874206337757
iteration : 6264
train acc:  0.7265625
train loss:  0.5176149606704712
train gradient:  0.12105379624385727
iteration : 6265
train acc:  0.7890625
train loss:  0.4381418228149414
train gradient:  0.09801513928437404
iteration : 6266
train acc:  0.7265625
train loss:  0.48832887411117554
train gradient:  0.13392781614129534
iteration : 6267
train acc:  0.765625
train loss:  0.49832379817962646
train gradient:  0.13079570401924034
iteration : 6268
train acc:  0.703125
train loss:  0.5650092959403992
train gradient:  0.13415923668590046
iteration : 6269
train acc:  0.6953125
train loss:  0.5158419013023376
train gradient:  0.1198368647313435
iteration : 6270
train acc:  0.7890625
train loss:  0.4512998163700104
train gradient:  0.16613791683336798
iteration : 6271
train acc:  0.7890625
train loss:  0.4664042592048645
train gradient:  0.1234717554966421
iteration : 6272
train acc:  0.7578125
train loss:  0.520439863204956
train gradient:  0.13237375158619824
iteration : 6273
train acc:  0.765625
train loss:  0.48725906014442444
train gradient:  0.1170472364907342
iteration : 6274
train acc:  0.71875
train loss:  0.5297437906265259
train gradient:  0.13528414706295544
iteration : 6275
train acc:  0.7734375
train loss:  0.4642333388328552
train gradient:  0.17895489690426958
iteration : 6276
train acc:  0.6796875
train loss:  0.5209887027740479
train gradient:  0.1584942446098087
iteration : 6277
train acc:  0.75
train loss:  0.482013463973999
train gradient:  0.11870708306906883
iteration : 6278
train acc:  0.6953125
train loss:  0.5503906607627869
train gradient:  0.16921841710946092
iteration : 6279
train acc:  0.7109375
train loss:  0.5483360290527344
train gradient:  0.18933660889521403
iteration : 6280
train acc:  0.6796875
train loss:  0.5757785439491272
train gradient:  0.2012312640281875
iteration : 6281
train acc:  0.671875
train loss:  0.5298869013786316
train gradient:  0.16820418162249284
iteration : 6282
train acc:  0.78125
train loss:  0.4445229768753052
train gradient:  0.09682933166467782
iteration : 6283
train acc:  0.7109375
train loss:  0.539459764957428
train gradient:  0.16406963322914425
iteration : 6284
train acc:  0.75
train loss:  0.47612905502319336
train gradient:  0.09942461999637635
iteration : 6285
train acc:  0.734375
train loss:  0.5173776149749756
train gradient:  0.13694808223147348
iteration : 6286
train acc:  0.7421875
train loss:  0.4931131601333618
train gradient:  0.16728813302587472
iteration : 6287
train acc:  0.71875
train loss:  0.5422989130020142
train gradient:  0.17668647787104363
iteration : 6288
train acc:  0.765625
train loss:  0.5148054957389832
train gradient:  0.1440565232707552
iteration : 6289
train acc:  0.6484375
train loss:  0.5744692087173462
train gradient:  0.18052620011448267
iteration : 6290
train acc:  0.7265625
train loss:  0.5330387949943542
train gradient:  0.14717320740011275
iteration : 6291
train acc:  0.6953125
train loss:  0.5214075446128845
train gradient:  0.13576539645598024
iteration : 6292
train acc:  0.640625
train loss:  0.6065965890884399
train gradient:  0.17742251278250804
iteration : 6293
train acc:  0.75
train loss:  0.5001282691955566
train gradient:  0.11609360452919472
iteration : 6294
train acc:  0.7890625
train loss:  0.45377230644226074
train gradient:  0.14877420365969052
iteration : 6295
train acc:  0.6171875
train loss:  0.5892921686172485
train gradient:  0.1478078361220582
iteration : 6296
train acc:  0.734375
train loss:  0.5071383714675903
train gradient:  0.1451664080109454
iteration : 6297
train acc:  0.765625
train loss:  0.4915015995502472
train gradient:  0.16334736014616852
iteration : 6298
train acc:  0.6875
train loss:  0.4764693081378937
train gradient:  0.11018415085980658
iteration : 6299
train acc:  0.7109375
train loss:  0.5283005237579346
train gradient:  0.1752390106260126
iteration : 6300
train acc:  0.71875
train loss:  0.5774472951889038
train gradient:  0.18673872227752025
iteration : 6301
train acc:  0.765625
train loss:  0.5010305643081665
train gradient:  0.12212460264287708
iteration : 6302
train acc:  0.7265625
train loss:  0.47276660799980164
train gradient:  0.11956755363389283
iteration : 6303
train acc:  0.734375
train loss:  0.5145998001098633
train gradient:  0.14235635696697913
iteration : 6304
train acc:  0.78125
train loss:  0.4792498052120209
train gradient:  0.14934968930288428
iteration : 6305
train acc:  0.8125
train loss:  0.45671403408050537
train gradient:  0.10960236295476238
iteration : 6306
train acc:  0.765625
train loss:  0.48485639691352844
train gradient:  0.10405915655551347
iteration : 6307
train acc:  0.734375
train loss:  0.5418074131011963
train gradient:  0.18479034231321095
iteration : 6308
train acc:  0.7421875
train loss:  0.5228013396263123
train gradient:  0.151561486063955
iteration : 6309
train acc:  0.7265625
train loss:  0.47948959469795227
train gradient:  0.15794109783307375
iteration : 6310
train acc:  0.6953125
train loss:  0.5229448080062866
train gradient:  0.13039278487770833
iteration : 6311
train acc:  0.734375
train loss:  0.522165060043335
train gradient:  0.19690265784585098
iteration : 6312
train acc:  0.71875
train loss:  0.5250115394592285
train gradient:  0.14605696740983565
iteration : 6313
train acc:  0.734375
train loss:  0.4863482713699341
train gradient:  0.16015569128362722
iteration : 6314
train acc:  0.7109375
train loss:  0.49207445979118347
train gradient:  0.14107469515223015
iteration : 6315
train acc:  0.6796875
train loss:  0.555785059928894
train gradient:  0.1762429682866285
iteration : 6316
train acc:  0.7109375
train loss:  0.4925137162208557
train gradient:  0.17161555549445956
iteration : 6317
train acc:  0.7265625
train loss:  0.5095574855804443
train gradient:  0.16221639914181846
iteration : 6318
train acc:  0.75
train loss:  0.4996063709259033
train gradient:  0.12259293494351825
iteration : 6319
train acc:  0.6875
train loss:  0.5257593989372253
train gradient:  0.13194499787461378
iteration : 6320
train acc:  0.7109375
train loss:  0.5688788890838623
train gradient:  0.1425060993086211
iteration : 6321
train acc:  0.7421875
train loss:  0.5098671317100525
train gradient:  0.13338089984351761
iteration : 6322
train acc:  0.7265625
train loss:  0.515889585018158
train gradient:  0.17897485389396933
iteration : 6323
train acc:  0.734375
train loss:  0.5011271238327026
train gradient:  0.12261044640309372
iteration : 6324
train acc:  0.6875
train loss:  0.5464978218078613
train gradient:  0.16738099996303646
iteration : 6325
train acc:  0.7421875
train loss:  0.49801233410835266
train gradient:  0.12092687911922179
iteration : 6326
train acc:  0.6953125
train loss:  0.49314332008361816
train gradient:  0.1426916776958631
iteration : 6327
train acc:  0.7578125
train loss:  0.50252366065979
train gradient:  0.14292524723563454
iteration : 6328
train acc:  0.78125
train loss:  0.5040394067764282
train gradient:  0.1353721457326107
iteration : 6329
train acc:  0.7421875
train loss:  0.5295448899269104
train gradient:  0.14829474873832882
iteration : 6330
train acc:  0.6796875
train loss:  0.5529273748397827
train gradient:  0.20339007482462745
iteration : 6331
train acc:  0.71875
train loss:  0.5294267535209656
train gradient:  0.12957065293421827
iteration : 6332
train acc:  0.71875
train loss:  0.5171293020248413
train gradient:  0.11593491740881441
iteration : 6333
train acc:  0.7734375
train loss:  0.46952104568481445
train gradient:  0.1500961756801496
iteration : 6334
train acc:  0.75
train loss:  0.5674533843994141
train gradient:  0.1782856094553068
iteration : 6335
train acc:  0.7421875
train loss:  0.484126478433609
train gradient:  0.10785145744192201
iteration : 6336
train acc:  0.6796875
train loss:  0.5306509733200073
train gradient:  0.16011814907805394
iteration : 6337
train acc:  0.734375
train loss:  0.502199113368988
train gradient:  0.1293297767468694
iteration : 6338
train acc:  0.7890625
train loss:  0.44941815733909607
train gradient:  0.1008802201436477
iteration : 6339
train acc:  0.8359375
train loss:  0.45131731033325195
train gradient:  0.13187820471493727
iteration : 6340
train acc:  0.703125
train loss:  0.5377521514892578
train gradient:  0.1277752604086102
iteration : 6341
train acc:  0.7890625
train loss:  0.4769609570503235
train gradient:  0.13688819181654432
iteration : 6342
train acc:  0.7890625
train loss:  0.4314667582511902
train gradient:  0.0896607481814673
iteration : 6343
train acc:  0.6796875
train loss:  0.536128044128418
train gradient:  0.18471984564849475
iteration : 6344
train acc:  0.7578125
train loss:  0.519099235534668
train gradient:  0.12284160147884107
iteration : 6345
train acc:  0.7734375
train loss:  0.4953862428665161
train gradient:  0.10731367005177495
iteration : 6346
train acc:  0.796875
train loss:  0.4538325071334839
train gradient:  0.11375599192942591
iteration : 6347
train acc:  0.7734375
train loss:  0.457422137260437
train gradient:  0.12454513363339764
iteration : 6348
train acc:  0.765625
train loss:  0.44923198223114014
train gradient:  0.12099357493988522
iteration : 6349
train acc:  0.7578125
train loss:  0.5013854503631592
train gradient:  0.1153738509403152
iteration : 6350
train acc:  0.6796875
train loss:  0.5152364373207092
train gradient:  0.11730488557360792
iteration : 6351
train acc:  0.734375
train loss:  0.5225193500518799
train gradient:  0.11929603196197187
iteration : 6352
train acc:  0.71875
train loss:  0.4964276850223541
train gradient:  0.13243716073813333
iteration : 6353
train acc:  0.75
train loss:  0.5040372014045715
train gradient:  0.14799604624498555
iteration : 6354
train acc:  0.75
train loss:  0.49987557530403137
train gradient:  0.11897454221129568
iteration : 6355
train acc:  0.6875
train loss:  0.5691285133361816
train gradient:  0.17484502899777227
iteration : 6356
train acc:  0.765625
train loss:  0.5080999135971069
train gradient:  0.114614085348545
iteration : 6357
train acc:  0.7265625
train loss:  0.6036957502365112
train gradient:  0.18347407793364046
iteration : 6358
train acc:  0.75
train loss:  0.5167926549911499
train gradient:  0.13699333187033214
iteration : 6359
train acc:  0.796875
train loss:  0.4532114267349243
train gradient:  0.10288293701064712
iteration : 6360
train acc:  0.78125
train loss:  0.4745425581932068
train gradient:  0.13858437473492013
iteration : 6361
train acc:  0.7109375
train loss:  0.5124567151069641
train gradient:  0.176114551255442
iteration : 6362
train acc:  0.6953125
train loss:  0.5285627841949463
train gradient:  0.16150977582355136
iteration : 6363
train acc:  0.7265625
train loss:  0.5587191581726074
train gradient:  0.16516701722797783
iteration : 6364
train acc:  0.7421875
train loss:  0.5014088749885559
train gradient:  0.15064139655896555
iteration : 6365
train acc:  0.71875
train loss:  0.4769016206264496
train gradient:  0.13920283018928092
iteration : 6366
train acc:  0.734375
train loss:  0.48317188024520874
train gradient:  0.11800345007899779
iteration : 6367
train acc:  0.671875
train loss:  0.5399099588394165
train gradient:  0.14328556833715783
iteration : 6368
train acc:  0.7109375
train loss:  0.5247163772583008
train gradient:  0.1495273148751153
iteration : 6369
train acc:  0.6875
train loss:  0.5561869740486145
train gradient:  0.13913902367303926
iteration : 6370
train acc:  0.7265625
train loss:  0.532496452331543
train gradient:  0.15691420126165792
iteration : 6371
train acc:  0.7734375
train loss:  0.5011786818504333
train gradient:  0.15789783395463808
iteration : 6372
train acc:  0.765625
train loss:  0.49748557806015015
train gradient:  0.13495966278796614
iteration : 6373
train acc:  0.7578125
train loss:  0.49697598814964294
train gradient:  0.13473183914670492
iteration : 6374
train acc:  0.703125
train loss:  0.5651692748069763
train gradient:  0.15754232209956745
iteration : 6375
train acc:  0.703125
train loss:  0.5628747940063477
train gradient:  0.14796711222641468
iteration : 6376
train acc:  0.6796875
train loss:  0.5471174120903015
train gradient:  0.1489162435656091
iteration : 6377
train acc:  0.6953125
train loss:  0.49125945568084717
train gradient:  0.14213415010188188
iteration : 6378
train acc:  0.7734375
train loss:  0.5221443772315979
train gradient:  0.17955548925227705
iteration : 6379
train acc:  0.765625
train loss:  0.4605359733104706
train gradient:  0.1232200485595063
iteration : 6380
train acc:  0.765625
train loss:  0.49289578199386597
train gradient:  0.15008951788376218
iteration : 6381
train acc:  0.7265625
train loss:  0.5744227170944214
train gradient:  0.15896691229535975
iteration : 6382
train acc:  0.7265625
train loss:  0.48552048206329346
train gradient:  0.17180683954822923
iteration : 6383
train acc:  0.7265625
train loss:  0.5049707293510437
train gradient:  0.12754539840770618
iteration : 6384
train acc:  0.8359375
train loss:  0.3766099214553833
train gradient:  0.10817594305600142
iteration : 6385
train acc:  0.7734375
train loss:  0.45210587978363037
train gradient:  0.1037007894954971
iteration : 6386
train acc:  0.703125
train loss:  0.5237531661987305
train gradient:  0.1270905538756515
iteration : 6387
train acc:  0.6875
train loss:  0.543838620185852
train gradient:  0.1260613038522119
iteration : 6388
train acc:  0.7109375
train loss:  0.5375426411628723
train gradient:  0.1535889667133309
iteration : 6389
train acc:  0.6953125
train loss:  0.6031947135925293
train gradient:  0.1894860043528556
iteration : 6390
train acc:  0.765625
train loss:  0.48778724670410156
train gradient:  0.11774884488662815
iteration : 6391
train acc:  0.6796875
train loss:  0.6150511503219604
train gradient:  0.21582308787751983
iteration : 6392
train acc:  0.734375
train loss:  0.5276965498924255
train gradient:  0.18597487052762224
iteration : 6393
train acc:  0.75
train loss:  0.5279711484909058
train gradient:  0.13863934722188664
iteration : 6394
train acc:  0.7265625
train loss:  0.4635106921195984
train gradient:  0.09051364310159556
iteration : 6395
train acc:  0.671875
train loss:  0.4963563084602356
train gradient:  0.14460409409589847
iteration : 6396
train acc:  0.765625
train loss:  0.4898906946182251
train gradient:  0.1351525500803676
iteration : 6397
train acc:  0.6796875
train loss:  0.5777681469917297
train gradient:  0.1616061232664958
iteration : 6398
train acc:  0.7421875
train loss:  0.4996337294578552
train gradient:  0.11503086638128263
iteration : 6399
train acc:  0.703125
train loss:  0.5559008717536926
train gradient:  0.24608223522287
iteration : 6400
train acc:  0.6875
train loss:  0.5182762742042542
train gradient:  0.1121579867908512
iteration : 6401
train acc:  0.71875
train loss:  0.4918655753135681
train gradient:  0.10969903489028086
iteration : 6402
train acc:  0.703125
train loss:  0.5404503345489502
train gradient:  0.1804918703347319
iteration : 6403
train acc:  0.7890625
train loss:  0.4550907015800476
train gradient:  0.15450408791390588
iteration : 6404
train acc:  0.78125
train loss:  0.4568256437778473
train gradient:  0.120582519831431
iteration : 6405
train acc:  0.734375
train loss:  0.5111690759658813
train gradient:  0.14841621897270824
iteration : 6406
train acc:  0.7265625
train loss:  0.5301061868667603
train gradient:  0.14550557533074146
iteration : 6407
train acc:  0.78125
train loss:  0.471333384513855
train gradient:  0.12129364197175863
iteration : 6408
train acc:  0.6953125
train loss:  0.5232395529747009
train gradient:  0.1595332813360537
iteration : 6409
train acc:  0.7890625
train loss:  0.4657222330570221
train gradient:  0.10181418406896682
iteration : 6410
train acc:  0.7265625
train loss:  0.532402515411377
train gradient:  0.15109381370495353
iteration : 6411
train acc:  0.7578125
train loss:  0.4516771137714386
train gradient:  0.10735799692492837
iteration : 6412
train acc:  0.5859375
train loss:  0.6031016707420349
train gradient:  0.2262383425062956
iteration : 6413
train acc:  0.703125
train loss:  0.5620169043540955
train gradient:  0.1683410596186653
iteration : 6414
train acc:  0.765625
train loss:  0.447116494178772
train gradient:  0.10464096965064007
iteration : 6415
train acc:  0.734375
train loss:  0.4993441700935364
train gradient:  0.14087201341006167
iteration : 6416
train acc:  0.703125
train loss:  0.5435019731521606
train gradient:  0.14443700561072226
iteration : 6417
train acc:  0.734375
train loss:  0.5080219507217407
train gradient:  0.20195734713854802
iteration : 6418
train acc:  0.7421875
train loss:  0.5452864170074463
train gradient:  0.11942843158858178
iteration : 6419
train acc:  0.7265625
train loss:  0.49845388531684875
train gradient:  0.11437465353409373
iteration : 6420
train acc:  0.734375
train loss:  0.5543434619903564
train gradient:  0.21362125929404996
iteration : 6421
train acc:  0.6875
train loss:  0.5192772746086121
train gradient:  0.1548361869843683
iteration : 6422
train acc:  0.7421875
train loss:  0.5509114265441895
train gradient:  0.16804007993491635
iteration : 6423
train acc:  0.7109375
train loss:  0.5165388584136963
train gradient:  0.13811405817031752
iteration : 6424
train acc:  0.765625
train loss:  0.4860239326953888
train gradient:  0.1291249578274347
iteration : 6425
train acc:  0.7421875
train loss:  0.5239688158035278
train gradient:  0.14078316540485455
iteration : 6426
train acc:  0.6875
train loss:  0.5587413311004639
train gradient:  0.17710889564267907
iteration : 6427
train acc:  0.7265625
train loss:  0.5577270984649658
train gradient:  0.14702694269666666
iteration : 6428
train acc:  0.703125
train loss:  0.5079649686813354
train gradient:  0.15487007575570033
iteration : 6429
train acc:  0.765625
train loss:  0.463795006275177
train gradient:  0.17612880520748164
iteration : 6430
train acc:  0.7109375
train loss:  0.4806878864765167
train gradient:  0.12975703046992987
iteration : 6431
train acc:  0.71875
train loss:  0.533184289932251
train gradient:  0.17928854166410552
iteration : 6432
train acc:  0.734375
train loss:  0.502350389957428
train gradient:  0.13817100743454208
iteration : 6433
train acc:  0.7265625
train loss:  0.5590050220489502
train gradient:  0.16036171840959812
iteration : 6434
train acc:  0.6484375
train loss:  0.6125779747962952
train gradient:  0.16295619082754786
iteration : 6435
train acc:  0.78125
train loss:  0.49213969707489014
train gradient:  0.13763472379515984
iteration : 6436
train acc:  0.7265625
train loss:  0.5078619718551636
train gradient:  0.13504428515874567
iteration : 6437
train acc:  0.6875
train loss:  0.5415285229682922
train gradient:  0.1574076537074184
iteration : 6438
train acc:  0.7265625
train loss:  0.5074328780174255
train gradient:  0.1864790663104765
iteration : 6439
train acc:  0.671875
train loss:  0.5732879638671875
train gradient:  0.2741748425736875
iteration : 6440
train acc:  0.71875
train loss:  0.4925358295440674
train gradient:  0.1752911009822672
iteration : 6441
train acc:  0.78125
train loss:  0.44697636365890503
train gradient:  0.10305621612485938
iteration : 6442
train acc:  0.7265625
train loss:  0.5152562856674194
train gradient:  0.1261797828994985
iteration : 6443
train acc:  0.7265625
train loss:  0.507698655128479
train gradient:  0.11937240694813404
iteration : 6444
train acc:  0.71875
train loss:  0.525780200958252
train gradient:  0.11959976811567227
iteration : 6445
train acc:  0.7421875
train loss:  0.547970175743103
train gradient:  0.11566761951531743
iteration : 6446
train acc:  0.71875
train loss:  0.5336699485778809
train gradient:  0.1328358252114815
iteration : 6447
train acc:  0.7265625
train loss:  0.49903661012649536
train gradient:  0.1113172908879284
iteration : 6448
train acc:  0.6953125
train loss:  0.5345844626426697
train gradient:  0.14815271590264478
iteration : 6449
train acc:  0.75
train loss:  0.46663179993629456
train gradient:  0.10084410229310782
iteration : 6450
train acc:  0.671875
train loss:  0.503472626209259
train gradient:  0.12043498887620628
iteration : 6451
train acc:  0.71875
train loss:  0.5599997043609619
train gradient:  0.15642306381026327
iteration : 6452
train acc:  0.6875
train loss:  0.5797362923622131
train gradient:  0.2427338302114314
iteration : 6453
train acc:  0.671875
train loss:  0.5428200364112854
train gradient:  0.13970492173049198
iteration : 6454
train acc:  0.796875
train loss:  0.4872593283653259
train gradient:  0.10612107391257607
iteration : 6455
train acc:  0.8203125
train loss:  0.4506540298461914
train gradient:  0.11053885043948236
iteration : 6456
train acc:  0.7265625
train loss:  0.6008527278900146
train gradient:  0.21684028749577094
iteration : 6457
train acc:  0.7734375
train loss:  0.46734514832496643
train gradient:  0.12118852128037721
iteration : 6458
train acc:  0.671875
train loss:  0.5409063100814819
train gradient:  0.1631948314653371
iteration : 6459
train acc:  0.734375
train loss:  0.5265675783157349
train gradient:  0.17468287462735282
iteration : 6460
train acc:  0.7265625
train loss:  0.5772045850753784
train gradient:  0.2642716446628491
iteration : 6461
train acc:  0.703125
train loss:  0.5278953313827515
train gradient:  0.1193255312416523
iteration : 6462
train acc:  0.703125
train loss:  0.5602587461471558
train gradient:  0.17020602108644173
iteration : 6463
train acc:  0.765625
train loss:  0.4883739948272705
train gradient:  0.14893979339949565
iteration : 6464
train acc:  0.7421875
train loss:  0.4693060517311096
train gradient:  0.15229600410921615
iteration : 6465
train acc:  0.7578125
train loss:  0.4982331693172455
train gradient:  0.10110328819759726
iteration : 6466
train acc:  0.7890625
train loss:  0.4782281517982483
train gradient:  0.13508344444134618
iteration : 6467
train acc:  0.734375
train loss:  0.4617083668708801
train gradient:  0.1131406359095028
iteration : 6468
train acc:  0.7734375
train loss:  0.46443626284599304
train gradient:  0.13062692417073818
iteration : 6469
train acc:  0.6640625
train loss:  0.5287702679634094
train gradient:  0.15267561210177433
iteration : 6470
train acc:  0.7734375
train loss:  0.4833222031593323
train gradient:  0.1301675800867236
iteration : 6471
train acc:  0.75
train loss:  0.5025002360343933
train gradient:  0.1272536701822836
iteration : 6472
train acc:  0.7734375
train loss:  0.4694608747959137
train gradient:  0.13129872444984153
iteration : 6473
train acc:  0.703125
train loss:  0.5510069727897644
train gradient:  0.19220954415673097
iteration : 6474
train acc:  0.7265625
train loss:  0.4921238422393799
train gradient:  0.10499988396135748
iteration : 6475
train acc:  0.734375
train loss:  0.5121979713439941
train gradient:  0.13392204696972315
iteration : 6476
train acc:  0.7265625
train loss:  0.4983626902103424
train gradient:  0.12664129162128923
iteration : 6477
train acc:  0.75
train loss:  0.5133610963821411
train gradient:  0.15722444527254312
iteration : 6478
train acc:  0.71875
train loss:  0.5128209590911865
train gradient:  0.12881361843426553
iteration : 6479
train acc:  0.6875
train loss:  0.5481922626495361
train gradient:  0.1645486405815497
iteration : 6480
train acc:  0.7265625
train loss:  0.5119165778160095
train gradient:  0.11561091857982707
iteration : 6481
train acc:  0.6953125
train loss:  0.5195941925048828
train gradient:  0.16136967808964386
iteration : 6482
train acc:  0.7421875
train loss:  0.4662361145019531
train gradient:  0.11597440688186708
iteration : 6483
train acc:  0.71875
train loss:  0.5462429523468018
train gradient:  0.1679073809272637
iteration : 6484
train acc:  0.6953125
train loss:  0.5325568318367004
train gradient:  0.17457901148536942
iteration : 6485
train acc:  0.65625
train loss:  0.5489083528518677
train gradient:  0.1440557090263992
iteration : 6486
train acc:  0.734375
train loss:  0.5517145395278931
train gradient:  0.16666739397818162
iteration : 6487
train acc:  0.7109375
train loss:  0.5298522710800171
train gradient:  0.16793656840796356
iteration : 6488
train acc:  0.734375
train loss:  0.5538387894630432
train gradient:  0.19023116966832043
iteration : 6489
train acc:  0.734375
train loss:  0.5384446382522583
train gradient:  0.13122447441379498
iteration : 6490
train acc:  0.7109375
train loss:  0.520673930644989
train gradient:  0.1405216097157258
iteration : 6491
train acc:  0.765625
train loss:  0.5203801393508911
train gradient:  0.20839714877099302
iteration : 6492
train acc:  0.703125
train loss:  0.5293580293655396
train gradient:  0.11961093038116351
iteration : 6493
train acc:  0.703125
train loss:  0.5294839143753052
train gradient:  0.1078160021208464
iteration : 6494
train acc:  0.7578125
train loss:  0.4947146773338318
train gradient:  0.13266430222866277
iteration : 6495
train acc:  0.6796875
train loss:  0.552497386932373
train gradient:  0.16132478325938832
iteration : 6496
train acc:  0.75
train loss:  0.4817633628845215
train gradient:  0.13388912916867324
iteration : 6497
train acc:  0.7578125
train loss:  0.5195698738098145
train gradient:  0.1303946607425438
iteration : 6498
train acc:  0.71875
train loss:  0.5513784885406494
train gradient:  0.22901646390293837
iteration : 6499
train acc:  0.828125
train loss:  0.4156513810157776
train gradient:  0.12988358857242732
iteration : 6500
train acc:  0.703125
train loss:  0.5504986643791199
train gradient:  0.17836732208376038
iteration : 6501
train acc:  0.7734375
train loss:  0.47500932216644287
train gradient:  0.11576803881686781
iteration : 6502
train acc:  0.78125
train loss:  0.46733301877975464
train gradient:  0.1007299513189158
iteration : 6503
train acc:  0.734375
train loss:  0.5398602485656738
train gradient:  0.15355649441610522
iteration : 6504
train acc:  0.703125
train loss:  0.512169361114502
train gradient:  0.1261238677040687
iteration : 6505
train acc:  0.6953125
train loss:  0.5544112920761108
train gradient:  0.1890800737194377
iteration : 6506
train acc:  0.7578125
train loss:  0.4685114622116089
train gradient:  0.10336787010156981
iteration : 6507
train acc:  0.6953125
train loss:  0.46518710255622864
train gradient:  0.09601169736908559
iteration : 6508
train acc:  0.734375
train loss:  0.48658323287963867
train gradient:  0.11434697975588948
iteration : 6509
train acc:  0.78125
train loss:  0.4720895290374756
train gradient:  0.1302646326428633
iteration : 6510
train acc:  0.6796875
train loss:  0.5542506575584412
train gradient:  0.1909908580124372
iteration : 6511
train acc:  0.6328125
train loss:  0.6230740547180176
train gradient:  0.1953033372635789
iteration : 6512
train acc:  0.7578125
train loss:  0.4872940182685852
train gradient:  0.13896149776753675
iteration : 6513
train acc:  0.7109375
train loss:  0.5872312188148499
train gradient:  0.16793551620195613
iteration : 6514
train acc:  0.7265625
train loss:  0.4708237051963806
train gradient:  0.11500392954734019
iteration : 6515
train acc:  0.734375
train loss:  0.5276944637298584
train gradient:  0.15706386579734943
iteration : 6516
train acc:  0.75
train loss:  0.5307074189186096
train gradient:  0.11556880021771841
iteration : 6517
train acc:  0.7890625
train loss:  0.45255932211875916
train gradient:  0.09383962949070204
iteration : 6518
train acc:  0.8671875
train loss:  0.3974171280860901
train gradient:  0.09749236998512743
iteration : 6519
train acc:  0.7265625
train loss:  0.527395486831665
train gradient:  0.1565221373344231
iteration : 6520
train acc:  0.78125
train loss:  0.5090992450714111
train gradient:  0.12856528778086318
iteration : 6521
train acc:  0.7421875
train loss:  0.5211411714553833
train gradient:  0.1412377633732824
iteration : 6522
train acc:  0.7421875
train loss:  0.4678064286708832
train gradient:  0.11407928097880407
iteration : 6523
train acc:  0.6328125
train loss:  0.5168370604515076
train gradient:  0.12380105210166321
iteration : 6524
train acc:  0.703125
train loss:  0.5015053153038025
train gradient:  0.14561312527702952
iteration : 6525
train acc:  0.8046875
train loss:  0.4335746169090271
train gradient:  0.13671320013699587
iteration : 6526
train acc:  0.6796875
train loss:  0.5473385453224182
train gradient:  0.16009395747459698
iteration : 6527
train acc:  0.7578125
train loss:  0.5152090787887573
train gradient:  0.169847690102865
iteration : 6528
train acc:  0.71875
train loss:  0.5502959489822388
train gradient:  0.16475926529457846
iteration : 6529
train acc:  0.7265625
train loss:  0.4872998595237732
train gradient:  0.09705647595341729
iteration : 6530
train acc:  0.75
train loss:  0.5175336599349976
train gradient:  0.1447538367930581
iteration : 6531
train acc:  0.8203125
train loss:  0.4533059597015381
train gradient:  0.12197790698020915
iteration : 6532
train acc:  0.7421875
train loss:  0.4883633553981781
train gradient:  0.14417278198724034
iteration : 6533
train acc:  0.765625
train loss:  0.45131808519363403
train gradient:  0.08858030652940076
iteration : 6534
train acc:  0.7578125
train loss:  0.4931304156780243
train gradient:  0.13127995746314403
iteration : 6535
train acc:  0.6953125
train loss:  0.5668612122535706
train gradient:  0.20444327125998574
iteration : 6536
train acc:  0.75
train loss:  0.4798603355884552
train gradient:  0.1375769863163376
iteration : 6537
train acc:  0.6328125
train loss:  0.5856416821479797
train gradient:  0.19836344721475996
iteration : 6538
train acc:  0.6796875
train loss:  0.5552639961242676
train gradient:  0.15292917212193555
iteration : 6539
train acc:  0.75
train loss:  0.4762854278087616
train gradient:  0.1483228208965537
iteration : 6540
train acc:  0.734375
train loss:  0.5471936464309692
train gradient:  0.12370744552725754
iteration : 6541
train acc:  0.7734375
train loss:  0.4474583566188812
train gradient:  0.09240878423794752
iteration : 6542
train acc:  0.7421875
train loss:  0.5403646230697632
train gradient:  0.17178843063782756
iteration : 6543
train acc:  0.796875
train loss:  0.44973069429397583
train gradient:  0.12322085553234935
iteration : 6544
train acc:  0.7421875
train loss:  0.46830689907073975
train gradient:  0.11649617586412188
iteration : 6545
train acc:  0.75
train loss:  0.47453874349594116
train gradient:  0.1291520467692611
iteration : 6546
train acc:  0.734375
train loss:  0.5436297655105591
train gradient:  0.16200922774115575
iteration : 6547
train acc:  0.7421875
train loss:  0.5182955861091614
train gradient:  0.15778375227778468
iteration : 6548
train acc:  0.765625
train loss:  0.5052489042282104
train gradient:  0.18201615158763046
iteration : 6549
train acc:  0.7421875
train loss:  0.49515384435653687
train gradient:  0.1356179005829889
iteration : 6550
train acc:  0.671875
train loss:  0.5494703054428101
train gradient:  0.18505490731232416
iteration : 6551
train acc:  0.7890625
train loss:  0.4852069020271301
train gradient:  0.20534060712102611
iteration : 6552
train acc:  0.6640625
train loss:  0.5434533357620239
train gradient:  0.17733615164763133
iteration : 6553
train acc:  0.765625
train loss:  0.46313226222991943
train gradient:  0.13272776292459182
iteration : 6554
train acc:  0.7265625
train loss:  0.5137102007865906
train gradient:  0.1372428751088398
iteration : 6555
train acc:  0.7109375
train loss:  0.578490138053894
train gradient:  0.1770869593031124
iteration : 6556
train acc:  0.7421875
train loss:  0.4887968897819519
train gradient:  0.13032624065351997
iteration : 6557
train acc:  0.7109375
train loss:  0.5069153308868408
train gradient:  0.16028974125169423
iteration : 6558
train acc:  0.7109375
train loss:  0.501705527305603
train gradient:  0.12417238336733177
iteration : 6559
train acc:  0.703125
train loss:  0.5483055710792542
train gradient:  0.16061742411867103
iteration : 6560
train acc:  0.7265625
train loss:  0.4990435838699341
train gradient:  0.13242817584768923
iteration : 6561
train acc:  0.6484375
train loss:  0.591375470161438
train gradient:  0.2214623269530851
iteration : 6562
train acc:  0.765625
train loss:  0.4611133933067322
train gradient:  0.10252015628563095
iteration : 6563
train acc:  0.765625
train loss:  0.4542708098888397
train gradient:  0.1054708598853305
iteration : 6564
train acc:  0.71875
train loss:  0.47418779134750366
train gradient:  0.09819755906636217
iteration : 6565
train acc:  0.78125
train loss:  0.48105698823928833
train gradient:  0.138364348176987
iteration : 6566
train acc:  0.703125
train loss:  0.5771505832672119
train gradient:  0.23920678661177358
iteration : 6567
train acc:  0.7265625
train loss:  0.47233521938323975
train gradient:  0.11365875017259675
iteration : 6568
train acc:  0.6640625
train loss:  0.5426319241523743
train gradient:  0.15295311688517899
iteration : 6569
train acc:  0.8046875
train loss:  0.4969555735588074
train gradient:  0.14245595105949674
iteration : 6570
train acc:  0.78125
train loss:  0.47939157485961914
train gradient:  0.10968039387490412
iteration : 6571
train acc:  0.7421875
train loss:  0.524132490158081
train gradient:  0.14884426785612515
iteration : 6572
train acc:  0.71875
train loss:  0.5328221321105957
train gradient:  0.1263249874029002
iteration : 6573
train acc:  0.734375
train loss:  0.4806121587753296
train gradient:  0.13423497652334526
iteration : 6574
train acc:  0.765625
train loss:  0.42694228887557983
train gradient:  0.09489811485402806
iteration : 6575
train acc:  0.703125
train loss:  0.5641371011734009
train gradient:  0.16484844770037282
iteration : 6576
train acc:  0.734375
train loss:  0.5319576859474182
train gradient:  0.1353530376099461
iteration : 6577
train acc:  0.7421875
train loss:  0.5218595266342163
train gradient:  0.15556568595203418
iteration : 6578
train acc:  0.6953125
train loss:  0.5981829166412354
train gradient:  0.17920850480017267
iteration : 6579
train acc:  0.765625
train loss:  0.5300683975219727
train gradient:  0.17759320031744696
iteration : 6580
train acc:  0.765625
train loss:  0.45826399326324463
train gradient:  0.10385401348156222
iteration : 6581
train acc:  0.71875
train loss:  0.4783583879470825
train gradient:  0.12501236354222867
iteration : 6582
train acc:  0.7578125
train loss:  0.4559231400489807
train gradient:  0.09794018576293113
iteration : 6583
train acc:  0.7109375
train loss:  0.5694057941436768
train gradient:  0.18140279777773424
iteration : 6584
train acc:  0.71875
train loss:  0.46695390343666077
train gradient:  0.12377641465508256
iteration : 6585
train acc:  0.796875
train loss:  0.4797268807888031
train gradient:  0.13495437313440567
iteration : 6586
train acc:  0.7734375
train loss:  0.5292398929595947
train gradient:  0.16047315344803448
iteration : 6587
train acc:  0.71875
train loss:  0.5388996601104736
train gradient:  0.17559833315171341
iteration : 6588
train acc:  0.75
train loss:  0.46863675117492676
train gradient:  0.10770467244667084
iteration : 6589
train acc:  0.75
train loss:  0.5394034385681152
train gradient:  0.1983201711011476
iteration : 6590
train acc:  0.75
train loss:  0.5012418627738953
train gradient:  0.11950429476353172
iteration : 6591
train acc:  0.6640625
train loss:  0.5697762966156006
train gradient:  0.20064680485215747
iteration : 6592
train acc:  0.7109375
train loss:  0.5020852088928223
train gradient:  0.12933664985251525
iteration : 6593
train acc:  0.8046875
train loss:  0.4609358310699463
train gradient:  0.10116360286387303
iteration : 6594
train acc:  0.796875
train loss:  0.47184133529663086
train gradient:  0.13842146394243787
iteration : 6595
train acc:  0.75
train loss:  0.4697394371032715
train gradient:  0.12121482768816332
iteration : 6596
train acc:  0.7265625
train loss:  0.4885440170764923
train gradient:  0.16618980792262122
iteration : 6597
train acc:  0.6640625
train loss:  0.6572333574295044
train gradient:  0.24720538368765077
iteration : 6598
train acc:  0.765625
train loss:  0.4776788055896759
train gradient:  0.11029286548714567
iteration : 6599
train acc:  0.6953125
train loss:  0.5298588871955872
train gradient:  0.14945246478325183
iteration : 6600
train acc:  0.7421875
train loss:  0.4969526529312134
train gradient:  0.13293925875412235
iteration : 6601
train acc:  0.8046875
train loss:  0.4457353949546814
train gradient:  0.11389027512733267
iteration : 6602
train acc:  0.7734375
train loss:  0.4263158142566681
train gradient:  0.11077238205245457
iteration : 6603
train acc:  0.765625
train loss:  0.4578481614589691
train gradient:  0.10650524614401605
iteration : 6604
train acc:  0.671875
train loss:  0.5737708806991577
train gradient:  0.23018037538287311
iteration : 6605
train acc:  0.6640625
train loss:  0.5507655143737793
train gradient:  0.1382149885098112
iteration : 6606
train acc:  0.6953125
train loss:  0.5884736776351929
train gradient:  0.20243717436610742
iteration : 6607
train acc:  0.7578125
train loss:  0.49543309211730957
train gradient:  0.11553245765313279
iteration : 6608
train acc:  0.6953125
train loss:  0.5417528748512268
train gradient:  0.1634866182371455
iteration : 6609
train acc:  0.71875
train loss:  0.5345033407211304
train gradient:  0.16142526904068655
iteration : 6610
train acc:  0.75
train loss:  0.449417382478714
train gradient:  0.1319040089258896
iteration : 6611
train acc:  0.71875
train loss:  0.5179225206375122
train gradient:  0.12877362185588961
iteration : 6612
train acc:  0.7265625
train loss:  0.5280814170837402
train gradient:  0.17646226629497513
iteration : 6613
train acc:  0.703125
train loss:  0.5264739394187927
train gradient:  0.12343182453821415
iteration : 6614
train acc:  0.765625
train loss:  0.5016334056854248
train gradient:  0.15152990361461988
iteration : 6615
train acc:  0.78125
train loss:  0.4571564793586731
train gradient:  0.10095668449497924
iteration : 6616
train acc:  0.6953125
train loss:  0.5247547626495361
train gradient:  0.15949220086198546
iteration : 6617
train acc:  0.7734375
train loss:  0.47692710161209106
train gradient:  0.14911794518321553
iteration : 6618
train acc:  0.6953125
train loss:  0.5618069171905518
train gradient:  0.19294317135892358
iteration : 6619
train acc:  0.7734375
train loss:  0.46908625960350037
train gradient:  0.13047380137961417
iteration : 6620
train acc:  0.765625
train loss:  0.4466615319252014
train gradient:  0.12083169326147947
iteration : 6621
train acc:  0.765625
train loss:  0.473635196685791
train gradient:  0.10515127234309772
iteration : 6622
train acc:  0.7734375
train loss:  0.471737265586853
train gradient:  0.12835805457285143
iteration : 6623
train acc:  0.765625
train loss:  0.49940410256385803
train gradient:  0.10882429418808322
iteration : 6624
train acc:  0.609375
train loss:  0.6187219619750977
train gradient:  0.22527058269409778
iteration : 6625
train acc:  0.71875
train loss:  0.5478987693786621
train gradient:  0.15575605491011996
iteration : 6626
train acc:  0.71875
train loss:  0.5560636520385742
train gradient:  0.18890973943128064
iteration : 6627
train acc:  0.7421875
train loss:  0.5058004856109619
train gradient:  0.14069145739833688
iteration : 6628
train acc:  0.734375
train loss:  0.48481887578964233
train gradient:  0.15427053892935322
iteration : 6629
train acc:  0.703125
train loss:  0.5690919160842896
train gradient:  0.14237099347953303
iteration : 6630
train acc:  0.71875
train loss:  0.5183535814285278
train gradient:  0.13830161896268003
iteration : 6631
train acc:  0.7109375
train loss:  0.48902904987335205
train gradient:  0.13388228716724881
iteration : 6632
train acc:  0.765625
train loss:  0.4743577539920807
train gradient:  0.13150697676924833
iteration : 6633
train acc:  0.7421875
train loss:  0.4892662465572357
train gradient:  0.156714088415166
iteration : 6634
train acc:  0.703125
train loss:  0.49203604459762573
train gradient:  0.12707882177794771
iteration : 6635
train acc:  0.7109375
train loss:  0.5346535444259644
train gradient:  0.11706580393627969
iteration : 6636
train acc:  0.7578125
train loss:  0.4761817753314972
train gradient:  0.11394952349686809
iteration : 6637
train acc:  0.671875
train loss:  0.5582597255706787
train gradient:  0.20374532416511854
iteration : 6638
train acc:  0.734375
train loss:  0.5087783336639404
train gradient:  0.12703181777118827
iteration : 6639
train acc:  0.71875
train loss:  0.5734220147132874
train gradient:  0.17940755011599563
iteration : 6640
train acc:  0.8046875
train loss:  0.41073697805404663
train gradient:  0.0884281821899394
iteration : 6641
train acc:  0.640625
train loss:  0.5775610208511353
train gradient:  0.16818805983445181
iteration : 6642
train acc:  0.6953125
train loss:  0.5545902252197266
train gradient:  0.150457550730464
iteration : 6643
train acc:  0.8046875
train loss:  0.4747515618801117
train gradient:  0.11732778534519989
iteration : 6644
train acc:  0.796875
train loss:  0.478111207485199
train gradient:  0.14810579124200235
iteration : 6645
train acc:  0.7421875
train loss:  0.4929526448249817
train gradient:  0.11975040799855018
iteration : 6646
train acc:  0.6640625
train loss:  0.5449718236923218
train gradient:  0.17370410544755
iteration : 6647
train acc:  0.7109375
train loss:  0.5192490816116333
train gradient:  0.14244320195588026
iteration : 6648
train acc:  0.7421875
train loss:  0.5326864719390869
train gradient:  0.15700795986327465
iteration : 6649
train acc:  0.703125
train loss:  0.5034972429275513
train gradient:  0.16177472522690042
iteration : 6650
train acc:  0.7421875
train loss:  0.5193885564804077
train gradient:  0.13758426452022657
iteration : 6651
train acc:  0.734375
train loss:  0.5112363696098328
train gradient:  0.11497145659826305
iteration : 6652
train acc:  0.765625
train loss:  0.46971696615219116
train gradient:  0.10420397891775811
iteration : 6653
train acc:  0.796875
train loss:  0.46439066529273987
train gradient:  0.10402906031929998
iteration : 6654
train acc:  0.7578125
train loss:  0.4585804343223572
train gradient:  0.10055216607342529
iteration : 6655
train acc:  0.7109375
train loss:  0.5412883758544922
train gradient:  0.1341007081210745
iteration : 6656
train acc:  0.6875
train loss:  0.5079552531242371
train gradient:  0.13497862665826554
iteration : 6657
train acc:  0.7265625
train loss:  0.5382471084594727
train gradient:  0.1406039305627207
iteration : 6658
train acc:  0.6953125
train loss:  0.5357263088226318
train gradient:  0.17559373587130944
iteration : 6659
train acc:  0.765625
train loss:  0.4636159837245941
train gradient:  0.12114562412775713
iteration : 6660
train acc:  0.7734375
train loss:  0.4996207356452942
train gradient:  0.13745883847008628
iteration : 6661
train acc:  0.71875
train loss:  0.5203380584716797
train gradient:  0.11148453989773664
iteration : 6662
train acc:  0.703125
train loss:  0.5192734003067017
train gradient:  0.15722969232877107
iteration : 6663
train acc:  0.734375
train loss:  0.4957527816295624
train gradient:  0.14409520899175865
iteration : 6664
train acc:  0.7578125
train loss:  0.45639297366142273
train gradient:  0.11174559597472986
iteration : 6665
train acc:  0.8046875
train loss:  0.4648846983909607
train gradient:  0.1705641106873981
iteration : 6666
train acc:  0.6796875
train loss:  0.5780709981918335
train gradient:  0.16299534136245922
iteration : 6667
train acc:  0.7578125
train loss:  0.43276771903038025
train gradient:  0.11610044760561813
iteration : 6668
train acc:  0.703125
train loss:  0.5549485683441162
train gradient:  0.17771077058854712
iteration : 6669
train acc:  0.734375
train loss:  0.47752925753593445
train gradient:  0.10668610452360117
iteration : 6670
train acc:  0.75
train loss:  0.48480263352394104
train gradient:  0.13928276223055097
iteration : 6671
train acc:  0.796875
train loss:  0.43440961837768555
train gradient:  0.11153755157400085
iteration : 6672
train acc:  0.78125
train loss:  0.46808892488479614
train gradient:  0.11612535403281483
iteration : 6673
train acc:  0.75
train loss:  0.5231981873512268
train gradient:  0.14066717684187643
iteration : 6674
train acc:  0.703125
train loss:  0.48157036304473877
train gradient:  0.14187009769172063
iteration : 6675
train acc:  0.7578125
train loss:  0.4840767979621887
train gradient:  0.1366085879532852
iteration : 6676
train acc:  0.7578125
train loss:  0.4967535734176636
train gradient:  0.11368333895916345
iteration : 6677
train acc:  0.7265625
train loss:  0.5167408585548401
train gradient:  0.14367752240696244
iteration : 6678
train acc:  0.765625
train loss:  0.4843336343765259
train gradient:  0.1310245249190229
iteration : 6679
train acc:  0.7265625
train loss:  0.5474045872688293
train gradient:  0.16154993799873313
iteration : 6680
train acc:  0.7578125
train loss:  0.48640763759613037
train gradient:  0.13182680555736323
iteration : 6681
train acc:  0.7265625
train loss:  0.523647665977478
train gradient:  0.12256003250325935
iteration : 6682
train acc:  0.75
train loss:  0.4905374050140381
train gradient:  0.11515242129197469
iteration : 6683
train acc:  0.7109375
train loss:  0.5209157466888428
train gradient:  0.13143813939700583
iteration : 6684
train acc:  0.7265625
train loss:  0.485090970993042
train gradient:  0.129141125249143
iteration : 6685
train acc:  0.765625
train loss:  0.4813346266746521
train gradient:  0.13942309238836903
iteration : 6686
train acc:  0.71875
train loss:  0.47427472472190857
train gradient:  0.12423861293913552
iteration : 6687
train acc:  0.765625
train loss:  0.555020809173584
train gradient:  0.17500612524405246
iteration : 6688
train acc:  0.7109375
train loss:  0.5484737157821655
train gradient:  0.15727639123600665
iteration : 6689
train acc:  0.7109375
train loss:  0.5395563840866089
train gradient:  0.1421602934761491
iteration : 6690
train acc:  0.734375
train loss:  0.5140039920806885
train gradient:  0.1607749201141626
iteration : 6691
train acc:  0.6953125
train loss:  0.5187127590179443
train gradient:  0.12431385729138061
iteration : 6692
train acc:  0.7890625
train loss:  0.426455020904541
train gradient:  0.09696824905271675
iteration : 6693
train acc:  0.7421875
train loss:  0.5055493116378784
train gradient:  0.10521447770592708
iteration : 6694
train acc:  0.6953125
train loss:  0.5450669527053833
train gradient:  0.14587990646010857
iteration : 6695
train acc:  0.7421875
train loss:  0.48044246435165405
train gradient:  0.11616454379115596
iteration : 6696
train acc:  0.7265625
train loss:  0.5046564340591431
train gradient:  0.12593985767329297
iteration : 6697
train acc:  0.703125
train loss:  0.5091161131858826
train gradient:  0.1428984284568361
iteration : 6698
train acc:  0.8046875
train loss:  0.4378378391265869
train gradient:  0.10441939729626072
iteration : 6699
train acc:  0.71875
train loss:  0.5399639010429382
train gradient:  0.12567985398714204
iteration : 6700
train acc:  0.65625
train loss:  0.529734194278717
train gradient:  0.12952467964470787
iteration : 6701
train acc:  0.7421875
train loss:  0.4860037565231323
train gradient:  0.12804686056523068
iteration : 6702
train acc:  0.6796875
train loss:  0.5329582095146179
train gradient:  0.1514491046048922
iteration : 6703
train acc:  0.71875
train loss:  0.48695290088653564
train gradient:  0.15482573540562997
iteration : 6704
train acc:  0.7109375
train loss:  0.5652889013290405
train gradient:  0.1824267574048779
iteration : 6705
train acc:  0.7421875
train loss:  0.43818360567092896
train gradient:  0.09655696499394381
iteration : 6706
train acc:  0.7421875
train loss:  0.49119633436203003
train gradient:  0.1392399420851611
iteration : 6707
train acc:  0.7421875
train loss:  0.4698057472705841
train gradient:  0.1300627654914845
iteration : 6708
train acc:  0.7734375
train loss:  0.492708683013916
train gradient:  0.1547785538375985
iteration : 6709
train acc:  0.8203125
train loss:  0.4603407680988312
train gradient:  0.15624340891732913
iteration : 6710
train acc:  0.7265625
train loss:  0.5388358235359192
train gradient:  0.1667765166931
iteration : 6711
train acc:  0.7109375
train loss:  0.5361093878746033
train gradient:  0.19074092767921635
iteration : 6712
train acc:  0.7578125
train loss:  0.5110734105110168
train gradient:  0.20625038089030734
iteration : 6713
train acc:  0.765625
train loss:  0.4889522194862366
train gradient:  0.1383910729804076
iteration : 6714
train acc:  0.7265625
train loss:  0.5392721891403198
train gradient:  0.17737656539022073
iteration : 6715
train acc:  0.7265625
train loss:  0.5463730096817017
train gradient:  0.19450517467156764
iteration : 6716
train acc:  0.765625
train loss:  0.4691395163536072
train gradient:  0.15091159379891061
iteration : 6717
train acc:  0.71875
train loss:  0.5035131573677063
train gradient:  0.14293777877964128
iteration : 6718
train acc:  0.734375
train loss:  0.5347126722335815
train gradient:  0.1942041442767678
iteration : 6719
train acc:  0.6796875
train loss:  0.5894160270690918
train gradient:  0.16843946438987428
iteration : 6720
train acc:  0.6640625
train loss:  0.6229355931282043
train gradient:  0.206544166426545
iteration : 6721
train acc:  0.7578125
train loss:  0.50291508436203
train gradient:  0.18993842214115175
iteration : 6722
train acc:  0.734375
train loss:  0.5264968872070312
train gradient:  0.2610528418071686
iteration : 6723
train acc:  0.734375
train loss:  0.483084499835968
train gradient:  0.16804981349614323
iteration : 6724
train acc:  0.6953125
train loss:  0.5386312007904053
train gradient:  0.13683221237155535
iteration : 6725
train acc:  0.7578125
train loss:  0.49196481704711914
train gradient:  0.1310157468385859
iteration : 6726
train acc:  0.7578125
train loss:  0.5016470551490784
train gradient:  0.13642261351629698
iteration : 6727
train acc:  0.7421875
train loss:  0.5205360651016235
train gradient:  0.17031882361700923
iteration : 6728
train acc:  0.71875
train loss:  0.48453056812286377
train gradient:  0.15022724157069797
iteration : 6729
train acc:  0.671875
train loss:  0.5382829904556274
train gradient:  0.14659441193301528
iteration : 6730
train acc:  0.7265625
train loss:  0.5221513509750366
train gradient:  0.1269643787516656
iteration : 6731
train acc:  0.7734375
train loss:  0.4575608968734741
train gradient:  0.1274757547691937
iteration : 6732
train acc:  0.7578125
train loss:  0.49516168236732483
train gradient:  0.13736728544017573
iteration : 6733
train acc:  0.6796875
train loss:  0.534915030002594
train gradient:  0.15670062500405577
iteration : 6734
train acc:  0.7421875
train loss:  0.4925609230995178
train gradient:  0.14171732625796155
iteration : 6735
train acc:  0.71875
train loss:  0.5198912024497986
train gradient:  0.14804724622734766
iteration : 6736
train acc:  0.75
train loss:  0.5114297866821289
train gradient:  0.17107360049768822
iteration : 6737
train acc:  0.6953125
train loss:  0.5628809332847595
train gradient:  0.16914673622584653
iteration : 6738
train acc:  0.734375
train loss:  0.49489182233810425
train gradient:  0.14285949761277894
iteration : 6739
train acc:  0.6953125
train loss:  0.5805963277816772
train gradient:  0.1821417757344565
iteration : 6740
train acc:  0.6875
train loss:  0.5898175239562988
train gradient:  0.18056478849380792
iteration : 6741
train acc:  0.7421875
train loss:  0.443992555141449
train gradient:  0.10986942024247084
iteration : 6742
train acc:  0.671875
train loss:  0.552254855632782
train gradient:  0.1408830488097257
iteration : 6743
train acc:  0.734375
train loss:  0.48898595571517944
train gradient:  0.12328181802355719
iteration : 6744
train acc:  0.6875
train loss:  0.557172417640686
train gradient:  0.12855117309165665
iteration : 6745
train acc:  0.734375
train loss:  0.5331888198852539
train gradient:  0.1457795979364263
iteration : 6746
train acc:  0.7734375
train loss:  0.4821196496486664
train gradient:  0.12014278320773224
iteration : 6747
train acc:  0.71875
train loss:  0.5158405900001526
train gradient:  0.14674195772221715
iteration : 6748
train acc:  0.84375
train loss:  0.42421069741249084
train gradient:  0.11761555702163798
iteration : 6749
train acc:  0.7421875
train loss:  0.5080286264419556
train gradient:  0.1477704563837115
iteration : 6750
train acc:  0.6953125
train loss:  0.5462246537208557
train gradient:  0.1460488027966676
iteration : 6751
train acc:  0.734375
train loss:  0.47299903631210327
train gradient:  0.1178763796207524
iteration : 6752
train acc:  0.6796875
train loss:  0.5611477494239807
train gradient:  0.2295552647876012
iteration : 6753
train acc:  0.734375
train loss:  0.535779595375061
train gradient:  0.1676585967797225
iteration : 6754
train acc:  0.75
train loss:  0.5160714387893677
train gradient:  0.13632289841333878
iteration : 6755
train acc:  0.6953125
train loss:  0.5013719797134399
train gradient:  0.15279594317201917
iteration : 6756
train acc:  0.7734375
train loss:  0.4712459146976471
train gradient:  0.10465903102418807
iteration : 6757
train acc:  0.7578125
train loss:  0.5052905082702637
train gradient:  0.134954465079975
iteration : 6758
train acc:  0.734375
train loss:  0.49355584383010864
train gradient:  0.12204798066885979
iteration : 6759
train acc:  0.71875
train loss:  0.5091192722320557
train gradient:  0.15267164557831056
iteration : 6760
train acc:  0.7578125
train loss:  0.5330314636230469
train gradient:  0.14696957871141017
iteration : 6761
train acc:  0.7421875
train loss:  0.5269453525543213
train gradient:  0.14181058435297633
iteration : 6762
train acc:  0.75
train loss:  0.5285027027130127
train gradient:  0.1659240281780911
iteration : 6763
train acc:  0.71875
train loss:  0.5266480445861816
train gradient:  0.16953560857272193
iteration : 6764
train acc:  0.7578125
train loss:  0.5042150020599365
train gradient:  0.16357084184192366
iteration : 6765
train acc:  0.78125
train loss:  0.4664764106273651
train gradient:  0.148127595920453
iteration : 6766
train acc:  0.71875
train loss:  0.5326822996139526
train gradient:  0.1517644920469461
iteration : 6767
train acc:  0.71875
train loss:  0.5238444805145264
train gradient:  0.1411128289746245
iteration : 6768
train acc:  0.6875
train loss:  0.5495814085006714
train gradient:  0.1556996311243124
iteration : 6769
train acc:  0.765625
train loss:  0.49093347787857056
train gradient:  0.11937084392004776
iteration : 6770
train acc:  0.765625
train loss:  0.4572802186012268
train gradient:  0.1312026804228156
iteration : 6771
train acc:  0.6796875
train loss:  0.5751541256904602
train gradient:  0.15267838451939778
iteration : 6772
train acc:  0.7421875
train loss:  0.5019058585166931
train gradient:  0.12426596362428793
iteration : 6773
train acc:  0.65625
train loss:  0.5589418411254883
train gradient:  0.17979385194503056
iteration : 6774
train acc:  0.6484375
train loss:  0.5843583941459656
train gradient:  0.1574352725628808
iteration : 6775
train acc:  0.7421875
train loss:  0.49458247423171997
train gradient:  0.14373494778028328
iteration : 6776
train acc:  0.6953125
train loss:  0.5514123439788818
train gradient:  0.16863027922570992
iteration : 6777
train acc:  0.734375
train loss:  0.5581763982772827
train gradient:  0.184495561288069
iteration : 6778
train acc:  0.8046875
train loss:  0.46703189611434937
train gradient:  0.13760748960515784
iteration : 6779
train acc:  0.6953125
train loss:  0.5599185228347778
train gradient:  0.1502168831522282
iteration : 6780
train acc:  0.7265625
train loss:  0.5020264983177185
train gradient:  0.13492554189089218
iteration : 6781
train acc:  0.7421875
train loss:  0.49340516328811646
train gradient:  0.11519914471211452
iteration : 6782
train acc:  0.71875
train loss:  0.5278733968734741
train gradient:  0.15802441708859422
iteration : 6783
train acc:  0.75
train loss:  0.5238747000694275
train gradient:  0.138950542176274
iteration : 6784
train acc:  0.7421875
train loss:  0.5129778385162354
train gradient:  0.1331924222834903
iteration : 6785
train acc:  0.6875
train loss:  0.55986487865448
train gradient:  0.1620834950823514
iteration : 6786
train acc:  0.7734375
train loss:  0.45996588468551636
train gradient:  0.10932406138494744
iteration : 6787
train acc:  0.71875
train loss:  0.48514455556869507
train gradient:  0.1483383364760878
iteration : 6788
train acc:  0.75
train loss:  0.48752927780151367
train gradient:  0.13044754462824942
iteration : 6789
train acc:  0.71875
train loss:  0.5019100308418274
train gradient:  0.1501280611709336
iteration : 6790
train acc:  0.71875
train loss:  0.539933443069458
train gradient:  0.1486753110982332
iteration : 6791
train acc:  0.7109375
train loss:  0.539110004901886
train gradient:  0.13247604062258744
iteration : 6792
train acc:  0.7265625
train loss:  0.5361242294311523
train gradient:  0.14916343596511317
iteration : 6793
train acc:  0.7578125
train loss:  0.46503469347953796
train gradient:  0.13714247327667367
iteration : 6794
train acc:  0.7890625
train loss:  0.42867904901504517
train gradient:  0.09301472644455612
iteration : 6795
train acc:  0.796875
train loss:  0.4743878245353699
train gradient:  0.1348504350196445
iteration : 6796
train acc:  0.6875
train loss:  0.5339916348457336
train gradient:  0.12414133087466375
iteration : 6797
train acc:  0.734375
train loss:  0.4850557744503021
train gradient:  0.14392327338771038
iteration : 6798
train acc:  0.78125
train loss:  0.4874850809574127
train gradient:  0.14470400405863382
iteration : 6799
train acc:  0.7265625
train loss:  0.5093103647232056
train gradient:  0.1345101613064706
iteration : 6800
train acc:  0.7734375
train loss:  0.4713166356086731
train gradient:  0.13687872064036039
iteration : 6801
train acc:  0.6171875
train loss:  0.6199018955230713
train gradient:  0.20783178131286234
iteration : 6802
train acc:  0.78125
train loss:  0.46049878001213074
train gradient:  0.10152356785532449
iteration : 6803
train acc:  0.7421875
train loss:  0.5636684894561768
train gradient:  0.1779921752071948
iteration : 6804
train acc:  0.7421875
train loss:  0.4701157808303833
train gradient:  0.12570778965887786
iteration : 6805
train acc:  0.7578125
train loss:  0.47507840394973755
train gradient:  0.11659121569147038
iteration : 6806
train acc:  0.7265625
train loss:  0.5053389072418213
train gradient:  0.15053158413147677
iteration : 6807
train acc:  0.7109375
train loss:  0.5352953672409058
train gradient:  0.13424592451526762
iteration : 6808
train acc:  0.7578125
train loss:  0.5108835697174072
train gradient:  0.14554768108633237
iteration : 6809
train acc:  0.78125
train loss:  0.47706443071365356
train gradient:  0.12392358507435224
iteration : 6810
train acc:  0.734375
train loss:  0.5117661952972412
train gradient:  0.13025869789681144
iteration : 6811
train acc:  0.703125
train loss:  0.5615286827087402
train gradient:  0.14930893871034367
iteration : 6812
train acc:  0.7890625
train loss:  0.4404321312904358
train gradient:  0.12725047332341827
iteration : 6813
train acc:  0.7578125
train loss:  0.4813470244407654
train gradient:  0.13854861762110837
iteration : 6814
train acc:  0.734375
train loss:  0.4664474129676819
train gradient:  0.10264360320140875
iteration : 6815
train acc:  0.75
train loss:  0.5052585601806641
train gradient:  0.15078127524187718
iteration : 6816
train acc:  0.7890625
train loss:  0.45656818151474
train gradient:  0.10361644797348687
iteration : 6817
train acc:  0.7734375
train loss:  0.4690297544002533
train gradient:  0.12311999412195575
iteration : 6818
train acc:  0.65625
train loss:  0.5917608141899109
train gradient:  0.197731575103603
iteration : 6819
train acc:  0.7421875
train loss:  0.4867732524871826
train gradient:  0.15132053818593516
iteration : 6820
train acc:  0.78125
train loss:  0.49126794934272766
train gradient:  0.15463593795629327
iteration : 6821
train acc:  0.65625
train loss:  0.5786333680152893
train gradient:  0.19582607945226346
iteration : 6822
train acc:  0.7109375
train loss:  0.5456097722053528
train gradient:  0.19142159313830998
iteration : 6823
train acc:  0.703125
train loss:  0.529464840888977
train gradient:  0.12365908997161221
iteration : 6824
train acc:  0.7734375
train loss:  0.4794446527957916
train gradient:  0.14290811846076856
iteration : 6825
train acc:  0.7578125
train loss:  0.5023925304412842
train gradient:  0.12942180645374382
iteration : 6826
train acc:  0.7421875
train loss:  0.5162702798843384
train gradient:  0.10925600282518191
iteration : 6827
train acc:  0.796875
train loss:  0.4278842508792877
train gradient:  0.1081333966481242
iteration : 6828
train acc:  0.7265625
train loss:  0.49973544478416443
train gradient:  0.14380914380018733
iteration : 6829
train acc:  0.734375
train loss:  0.49414610862731934
train gradient:  0.12929850833379092
iteration : 6830
train acc:  0.796875
train loss:  0.46226853132247925
train gradient:  0.16299611720756585
iteration : 6831
train acc:  0.8359375
train loss:  0.4583500325679779
train gradient:  0.13720897701855866
iteration : 6832
train acc:  0.6796875
train loss:  0.5309572815895081
train gradient:  0.18440883674874373
iteration : 6833
train acc:  0.6953125
train loss:  0.5458431243896484
train gradient:  0.17418296605669048
iteration : 6834
train acc:  0.7890625
train loss:  0.46254870295524597
train gradient:  0.12153603976377651
iteration : 6835
train acc:  0.6953125
train loss:  0.5405434370040894
train gradient:  0.19994547850581618
iteration : 6836
train acc:  0.7265625
train loss:  0.5219081044197083
train gradient:  0.15580602245779168
iteration : 6837
train acc:  0.71875
train loss:  0.4997750520706177
train gradient:  0.11628721671154114
iteration : 6838
train acc:  0.7578125
train loss:  0.4271940588951111
train gradient:  0.1375499672311026
iteration : 6839
train acc:  0.75
train loss:  0.4984600841999054
train gradient:  0.15925641516654282
iteration : 6840
train acc:  0.765625
train loss:  0.4503215551376343
train gradient:  0.1068868794713063
iteration : 6841
train acc:  0.6484375
train loss:  0.5349656343460083
train gradient:  0.1547045275896139
iteration : 6842
train acc:  0.7265625
train loss:  0.5194540023803711
train gradient:  0.153209671604081
iteration : 6843
train acc:  0.765625
train loss:  0.4712487459182739
train gradient:  0.1173987904444509
iteration : 6844
train acc:  0.765625
train loss:  0.4829730987548828
train gradient:  0.17101955269222963
iteration : 6845
train acc:  0.71875
train loss:  0.5192462205886841
train gradient:  0.1724035509497348
iteration : 6846
train acc:  0.71875
train loss:  0.5304040908813477
train gradient:  0.1452120417955278
iteration : 6847
train acc:  0.796875
train loss:  0.4603477120399475
train gradient:  0.11603269139784052
iteration : 6848
train acc:  0.7734375
train loss:  0.48051953315734863
train gradient:  0.11407768690570516
iteration : 6849
train acc:  0.71875
train loss:  0.500933051109314
train gradient:  0.15879715281067638
iteration : 6850
train acc:  0.6640625
train loss:  0.5950237512588501
train gradient:  0.1652058289934326
iteration : 6851
train acc:  0.7421875
train loss:  0.5431640148162842
train gradient:  0.14623246076583546
iteration : 6852
train acc:  0.734375
train loss:  0.5489492416381836
train gradient:  0.1496710667179183
iteration : 6853
train acc:  0.7578125
train loss:  0.458462119102478
train gradient:  0.13927513031887917
iteration : 6854
train acc:  0.71875
train loss:  0.4810853898525238
train gradient:  0.1229322922039959
iteration : 6855
train acc:  0.7890625
train loss:  0.48417261242866516
train gradient:  0.12871813656077863
iteration : 6856
train acc:  0.671875
train loss:  0.6069856882095337
train gradient:  0.1742190376749977
iteration : 6857
train acc:  0.6953125
train loss:  0.547143816947937
train gradient:  0.14330315166157792
iteration : 6858
train acc:  0.71875
train loss:  0.5166580677032471
train gradient:  0.1611737562885681
iteration : 6859
train acc:  0.7578125
train loss:  0.4902976453304291
train gradient:  0.15430629554438652
iteration : 6860
train acc:  0.78125
train loss:  0.5008265376091003
train gradient:  0.1339453669302127
iteration : 6861
train acc:  0.7421875
train loss:  0.5168626308441162
train gradient:  0.14871563317277486
iteration : 6862
train acc:  0.65625
train loss:  0.60227370262146
train gradient:  0.22028659112881607
iteration : 6863
train acc:  0.75
train loss:  0.5058020353317261
train gradient:  0.1428823238514715
iteration : 6864
train acc:  0.75
train loss:  0.5238152742385864
train gradient:  0.14250105545112593
iteration : 6865
train acc:  0.8046875
train loss:  0.42929807305336
train gradient:  0.10776596342894426
iteration : 6866
train acc:  0.734375
train loss:  0.49983686208724976
train gradient:  0.12047100343234873
iteration : 6867
train acc:  0.6953125
train loss:  0.6429910659790039
train gradient:  0.248930632425583
iteration : 6868
train acc:  0.75
train loss:  0.5400873422622681
train gradient:  0.13334212610018142
iteration : 6869
train acc:  0.6875
train loss:  0.5355753898620605
train gradient:  0.17516328984523216
iteration : 6870
train acc:  0.8125
train loss:  0.444950133562088
train gradient:  0.12473016112195424
iteration : 6871
train acc:  0.75
train loss:  0.49213436245918274
train gradient:  0.16926400714315876
iteration : 6872
train acc:  0.8125
train loss:  0.399982213973999
train gradient:  0.0855149768952383
iteration : 6873
train acc:  0.7578125
train loss:  0.46480458974838257
train gradient:  0.11200600806912059
iteration : 6874
train acc:  0.75
train loss:  0.5447229146957397
train gradient:  0.19062153684603284
iteration : 6875
train acc:  0.78125
train loss:  0.48271095752716064
train gradient:  0.16337779466528823
iteration : 6876
train acc:  0.6953125
train loss:  0.505018413066864
train gradient:  0.15611075148141956
iteration : 6877
train acc:  0.703125
train loss:  0.5622128248214722
train gradient:  0.1494896340912334
iteration : 6878
train acc:  0.75
train loss:  0.48312515020370483
train gradient:  0.11721452127673637
iteration : 6879
train acc:  0.6171875
train loss:  0.6194103956222534
train gradient:  0.16278311175128996
iteration : 6880
train acc:  0.734375
train loss:  0.5035799145698547
train gradient:  0.18343051802341925
iteration : 6881
train acc:  0.6640625
train loss:  0.5519547462463379
train gradient:  0.20291421585199665
iteration : 6882
train acc:  0.671875
train loss:  0.5878620743751526
train gradient:  0.2256612662613111
iteration : 6883
train acc:  0.8125
train loss:  0.4290931522846222
train gradient:  0.11968748799717512
iteration : 6884
train acc:  0.7578125
train loss:  0.44456467032432556
train gradient:  0.13175829566750885
iteration : 6885
train acc:  0.75
train loss:  0.46248623728752136
train gradient:  0.12524574503645186
iteration : 6886
train acc:  0.734375
train loss:  0.5373775362968445
train gradient:  0.16464054140241083
iteration : 6887
train acc:  0.7109375
train loss:  0.4909017086029053
train gradient:  0.13209085059227016
iteration : 6888
train acc:  0.75
train loss:  0.4778183698654175
train gradient:  0.12104068410455468
iteration : 6889
train acc:  0.734375
train loss:  0.4875323176383972
train gradient:  0.16470081530383457
iteration : 6890
train acc:  0.6640625
train loss:  0.5651204586029053
train gradient:  0.19993480074395592
iteration : 6891
train acc:  0.671875
train loss:  0.6196938753128052
train gradient:  0.2545215695654303
iteration : 6892
train acc:  0.75
train loss:  0.45578068494796753
train gradient:  0.10843558421280551
iteration : 6893
train acc:  0.734375
train loss:  0.5485427379608154
train gradient:  0.16560647504598774
iteration : 6894
train acc:  0.71875
train loss:  0.486508309841156
train gradient:  0.13979253008716575
iteration : 6895
train acc:  0.7421875
train loss:  0.45599257946014404
train gradient:  0.11521068243943151
iteration : 6896
train acc:  0.7890625
train loss:  0.42311739921569824
train gradient:  0.11568018490597054
iteration : 6897
train acc:  0.8671875
train loss:  0.4338792860507965
train gradient:  0.17206467371750692
iteration : 6898
train acc:  0.734375
train loss:  0.515671968460083
train gradient:  0.13662046949884263
iteration : 6899
train acc:  0.7421875
train loss:  0.4755331873893738
train gradient:  0.11756378152933787
iteration : 6900
train acc:  0.6796875
train loss:  0.5353460907936096
train gradient:  0.1508462519709465
iteration : 6901
train acc:  0.6875
train loss:  0.5319717526435852
train gradient:  0.1506690676990931
iteration : 6902
train acc:  0.671875
train loss:  0.5838233232498169
train gradient:  0.19868815752980779
iteration : 6903
train acc:  0.6484375
train loss:  0.6181696653366089
train gradient:  0.22587085277007204
iteration : 6904
train acc:  0.7265625
train loss:  0.5296226143836975
train gradient:  0.14624003174346129
iteration : 6905
train acc:  0.7890625
train loss:  0.4803031086921692
train gradient:  0.13193418682148608
iteration : 6906
train acc:  0.703125
train loss:  0.5825878381729126
train gradient:  0.16556403283973176
iteration : 6907
train acc:  0.7421875
train loss:  0.5352243185043335
train gradient:  0.1610843335762931
iteration : 6908
train acc:  0.734375
train loss:  0.4889293313026428
train gradient:  0.15259610360620052
iteration : 6909
train acc:  0.6875
train loss:  0.5492938160896301
train gradient:  0.16839102410059226
iteration : 6910
train acc:  0.7890625
train loss:  0.5107139348983765
train gradient:  0.14259099202709777
iteration : 6911
train acc:  0.7109375
train loss:  0.5795456171035767
train gradient:  0.1660194147530868
iteration : 6912
train acc:  0.75
train loss:  0.45871010422706604
train gradient:  0.10928567443101997
iteration : 6913
train acc:  0.7890625
train loss:  0.4358628988265991
train gradient:  0.11895344725329145
iteration : 6914
train acc:  0.6796875
train loss:  0.6267694234848022
train gradient:  0.17444115477725666
iteration : 6915
train acc:  0.71875
train loss:  0.5496276617050171
train gradient:  0.20003731400326347
iteration : 6916
train acc:  0.7265625
train loss:  0.5324221849441528
train gradient:  0.12988465523022524
iteration : 6917
train acc:  0.7265625
train loss:  0.48846253752708435
train gradient:  0.13827905736880475
iteration : 6918
train acc:  0.7578125
train loss:  0.48244035243988037
train gradient:  0.1374285127362065
iteration : 6919
train acc:  0.7578125
train loss:  0.4789835810661316
train gradient:  0.09816664824048055
iteration : 6920
train acc:  0.7265625
train loss:  0.49834275245666504
train gradient:  0.1555341919207546
iteration : 6921
train acc:  0.78125
train loss:  0.4419029951095581
train gradient:  0.0935620541107759
iteration : 6922
train acc:  0.7265625
train loss:  0.4972251355648041
train gradient:  0.1297111817441185
iteration : 6923
train acc:  0.8046875
train loss:  0.480857789516449
train gradient:  0.11510604843527306
iteration : 6924
train acc:  0.765625
train loss:  0.497254341840744
train gradient:  0.12480872720971656
iteration : 6925
train acc:  0.7265625
train loss:  0.47966963052749634
train gradient:  0.11379578366698839
iteration : 6926
train acc:  0.6953125
train loss:  0.5644208192825317
train gradient:  0.18747656399111573
iteration : 6927
train acc:  0.7578125
train loss:  0.4333709478378296
train gradient:  0.08748538032516062
iteration : 6928
train acc:  0.78125
train loss:  0.45425206422805786
train gradient:  0.1038415150346446
iteration : 6929
train acc:  0.7421875
train loss:  0.4877850413322449
train gradient:  0.09472305160387465
iteration : 6930
train acc:  0.71875
train loss:  0.48979511857032776
train gradient:  0.141626330776503
iteration : 6931
train acc:  0.828125
train loss:  0.42662763595581055
train gradient:  0.09316602648392015
iteration : 6932
train acc:  0.734375
train loss:  0.5017980337142944
train gradient:  0.1330176233685839
iteration : 6933
train acc:  0.734375
train loss:  0.5248762369155884
train gradient:  0.1582435064860489
iteration : 6934
train acc:  0.7421875
train loss:  0.5472391843795776
train gradient:  0.2215273462678376
iteration : 6935
train acc:  0.7265625
train loss:  0.4763551354408264
train gradient:  0.12562166956716395
iteration : 6936
train acc:  0.734375
train loss:  0.46592608094215393
train gradient:  0.10171171506971598
iteration : 6937
train acc:  0.7578125
train loss:  0.5124874711036682
train gradient:  0.15167803067655614
iteration : 6938
train acc:  0.7109375
train loss:  0.4893116354942322
train gradient:  0.1503770374817323
iteration : 6939
train acc:  0.703125
train loss:  0.47134876251220703
train gradient:  0.12322708406163659
iteration : 6940
train acc:  0.6953125
train loss:  0.49779099225997925
train gradient:  0.14372870808035854
iteration : 6941
train acc:  0.7890625
train loss:  0.46978434920310974
train gradient:  0.11664679148729533
iteration : 6942
train acc:  0.734375
train loss:  0.4829487204551697
train gradient:  0.16075737588832856
iteration : 6943
train acc:  0.703125
train loss:  0.48515310883522034
train gradient:  0.12417501298463468
iteration : 6944
train acc:  0.703125
train loss:  0.5554628968238831
train gradient:  0.15600613155416193
iteration : 6945
train acc:  0.671875
train loss:  0.5818629264831543
train gradient:  0.2017010430476415
iteration : 6946
train acc:  0.765625
train loss:  0.4291976988315582
train gradient:  0.11056362351214634
iteration : 6947
train acc:  0.6796875
train loss:  0.532647967338562
train gradient:  0.14239199189640944
iteration : 6948
train acc:  0.734375
train loss:  0.5011152029037476
train gradient:  0.14878772597901874
iteration : 6949
train acc:  0.703125
train loss:  0.49780580401420593
train gradient:  0.14772161961877947
iteration : 6950
train acc:  0.7421875
train loss:  0.504979133605957
train gradient:  0.18346640902703445
iteration : 6951
train acc:  0.734375
train loss:  0.47528767585754395
train gradient:  0.13495360039536933
iteration : 6952
train acc:  0.6953125
train loss:  0.5733665227890015
train gradient:  0.1533633888639001
iteration : 6953
train acc:  0.6796875
train loss:  0.5712481737136841
train gradient:  0.15890063813580435
iteration : 6954
train acc:  0.7109375
train loss:  0.4981701374053955
train gradient:  0.15206358274308807
iteration : 6955
train acc:  0.7109375
train loss:  0.5149406790733337
train gradient:  0.158291616400781
iteration : 6956
train acc:  0.8125
train loss:  0.4368841350078583
train gradient:  0.13392915883650064
iteration : 6957
train acc:  0.796875
train loss:  0.4682309031486511
train gradient:  0.1305778906431067
iteration : 6958
train acc:  0.7109375
train loss:  0.5480633974075317
train gradient:  0.1522593252785967
iteration : 6959
train acc:  0.8046875
train loss:  0.44808149337768555
train gradient:  0.11328359029109918
iteration : 6960
train acc:  0.703125
train loss:  0.5486429929733276
train gradient:  0.20102206572263465
iteration : 6961
train acc:  0.734375
train loss:  0.46453505754470825
train gradient:  0.1258438997663558
iteration : 6962
train acc:  0.71875
train loss:  0.487731397151947
train gradient:  0.10730188338641103
iteration : 6963
train acc:  0.78125
train loss:  0.4050767421722412
train gradient:  0.12148258139467472
iteration : 6964
train acc:  0.7890625
train loss:  0.42514005303382874
train gradient:  0.09758242542097488
iteration : 6965
train acc:  0.7734375
train loss:  0.47119802236557007
train gradient:  0.12341676733021367
iteration : 6966
train acc:  0.734375
train loss:  0.5270566940307617
train gradient:  0.1636951730823874
iteration : 6967
train acc:  0.6953125
train loss:  0.5481734275817871
train gradient:  0.22488873652055116
iteration : 6968
train acc:  0.8203125
train loss:  0.415055513381958
train gradient:  0.0878423312851712
iteration : 6969
train acc:  0.765625
train loss:  0.4344211220741272
train gradient:  0.13353133142516266
iteration : 6970
train acc:  0.75
train loss:  0.48352646827697754
train gradient:  0.1537680894300688
iteration : 6971
train acc:  0.734375
train loss:  0.5136052966117859
train gradient:  0.1825586915500096
iteration : 6972
train acc:  0.71875
train loss:  0.5458167791366577
train gradient:  0.1452492618108922
iteration : 6973
train acc:  0.6953125
train loss:  0.4965566098690033
train gradient:  0.1494790147160778
iteration : 6974
train acc:  0.6953125
train loss:  0.5622584819793701
train gradient:  0.18303534285847406
iteration : 6975
train acc:  0.78125
train loss:  0.4654161334037781
train gradient:  0.14028482732907566
iteration : 6976
train acc:  0.71875
train loss:  0.4689438045024872
train gradient:  0.09993357812242684
iteration : 6977
train acc:  0.7265625
train loss:  0.5159209370613098
train gradient:  0.14205861918050539
iteration : 6978
train acc:  0.7421875
train loss:  0.524393618106842
train gradient:  0.13693662189984862
iteration : 6979
train acc:  0.671875
train loss:  0.5327718257904053
train gradient:  0.1677230378035276
iteration : 6980
train acc:  0.734375
train loss:  0.4993288516998291
train gradient:  0.15683604480910573
iteration : 6981
train acc:  0.6875
train loss:  0.5340186357498169
train gradient:  0.16834020653299744
iteration : 6982
train acc:  0.78125
train loss:  0.45389553904533386
train gradient:  0.1118350589177921
iteration : 6983
train acc:  0.71875
train loss:  0.5444292426109314
train gradient:  0.1986436966711289
iteration : 6984
train acc:  0.71875
train loss:  0.5214461088180542
train gradient:  0.1545338041860947
iteration : 6985
train acc:  0.734375
train loss:  0.5349891185760498
train gradient:  0.141395953327154
iteration : 6986
train acc:  0.703125
train loss:  0.4850388765335083
train gradient:  0.13697107762515257
iteration : 6987
train acc:  0.734375
train loss:  0.49152687191963196
train gradient:  0.1356584389967731
iteration : 6988
train acc:  0.734375
train loss:  0.4796849191188812
train gradient:  0.12766162961312555
iteration : 6989
train acc:  0.84375
train loss:  0.44260334968566895
train gradient:  0.1398734341001026
iteration : 6990
train acc:  0.7421875
train loss:  0.49290454387664795
train gradient:  0.12198936728440254
iteration : 6991
train acc:  0.6796875
train loss:  0.548650860786438
train gradient:  0.1767836890786923
iteration : 6992
train acc:  0.7734375
train loss:  0.48996323347091675
train gradient:  0.14519419843307652
iteration : 6993
train acc:  0.6953125
train loss:  0.5065587759017944
train gradient:  0.1231454731014781
iteration : 6994
train acc:  0.6328125
train loss:  0.5879441499710083
train gradient:  0.17016263510152838
iteration : 6995
train acc:  0.7421875
train loss:  0.5043976306915283
train gradient:  0.13129561140236448
iteration : 6996
train acc:  0.703125
train loss:  0.5008533000946045
train gradient:  0.14281448596136515
iteration : 6997
train acc:  0.75
train loss:  0.49236929416656494
train gradient:  0.1339885541845932
iteration : 6998
train acc:  0.7578125
train loss:  0.4573444426059723
train gradient:  0.11896927955081149
iteration : 6999
train acc:  0.7109375
train loss:  0.5560548305511475
train gradient:  0.1592963508724407
iteration : 7000
train acc:  0.8125
train loss:  0.4311433732509613
train gradient:  0.12010477784042388
iteration : 7001
train acc:  0.8046875
train loss:  0.45660296082496643
train gradient:  0.10572085610846266
iteration : 7002
train acc:  0.796875
train loss:  0.47607749700546265
train gradient:  0.15336077550459126
iteration : 7003
train acc:  0.7734375
train loss:  0.45025038719177246
train gradient:  0.12339768039225478
iteration : 7004
train acc:  0.7578125
train loss:  0.4976918399333954
train gradient:  0.14851038962294352
iteration : 7005
train acc:  0.71875
train loss:  0.5172523260116577
train gradient:  0.14106978288953553
iteration : 7006
train acc:  0.78125
train loss:  0.4284965395927429
train gradient:  0.10791738537742275
iteration : 7007
train acc:  0.6796875
train loss:  0.5157887935638428
train gradient:  0.1373192214309296
iteration : 7008
train acc:  0.7265625
train loss:  0.46638351678848267
train gradient:  0.13093604255290497
iteration : 7009
train acc:  0.703125
train loss:  0.5409486293792725
train gradient:  0.20622970279555147
iteration : 7010
train acc:  0.6953125
train loss:  0.5564447045326233
train gradient:  0.15030433104470198
iteration : 7011
train acc:  0.7109375
train loss:  0.5219425559043884
train gradient:  0.1597601887576039
iteration : 7012
train acc:  0.6796875
train loss:  0.5574297308921814
train gradient:  0.1852780562913416
iteration : 7013
train acc:  0.6953125
train loss:  0.5736775398254395
train gradient:  0.14907153107809318
iteration : 7014
train acc:  0.765625
train loss:  0.47966718673706055
train gradient:  0.11495437559793137
iteration : 7015
train acc:  0.75
train loss:  0.5162413716316223
train gradient:  0.17533367619758816
iteration : 7016
train acc:  0.703125
train loss:  0.609562337398529
train gradient:  0.190903505546421
iteration : 7017
train acc:  0.734375
train loss:  0.47373759746551514
train gradient:  0.1320058394644475
iteration : 7018
train acc:  0.6953125
train loss:  0.5246427059173584
train gradient:  0.1312048014037664
iteration : 7019
train acc:  0.78125
train loss:  0.4647989571094513
train gradient:  0.16118402011537747
iteration : 7020
train acc:  0.65625
train loss:  0.5776155591011047
train gradient:  0.2037263938436335
iteration : 7021
train acc:  0.765625
train loss:  0.510886013507843
train gradient:  0.1746751608021168
iteration : 7022
train acc:  0.734375
train loss:  0.5194306373596191
train gradient:  0.15813095531466959
iteration : 7023
train acc:  0.7265625
train loss:  0.5264595746994019
train gradient:  0.14600871665232118
iteration : 7024
train acc:  0.78125
train loss:  0.47572365403175354
train gradient:  0.12638181275851074
iteration : 7025
train acc:  0.7265625
train loss:  0.5022337436676025
train gradient:  0.1472167960883457
iteration : 7026
train acc:  0.71875
train loss:  0.5069363117218018
train gradient:  0.14524463520336
iteration : 7027
train acc:  0.734375
train loss:  0.4966312646865845
train gradient:  0.1235671616016086
iteration : 7028
train acc:  0.7890625
train loss:  0.48356038331985474
train gradient:  0.11118161975374105
iteration : 7029
train acc:  0.6953125
train loss:  0.5184856653213501
train gradient:  0.13278419328809637
iteration : 7030
train acc:  0.6796875
train loss:  0.49987709522247314
train gradient:  0.1377165621411032
iteration : 7031
train acc:  0.765625
train loss:  0.4573821723461151
train gradient:  0.13490438416668485
iteration : 7032
train acc:  0.7734375
train loss:  0.4790622293949127
train gradient:  0.14047570399360687
iteration : 7033
train acc:  0.734375
train loss:  0.5156302452087402
train gradient:  0.17509403969483434
iteration : 7034
train acc:  0.734375
train loss:  0.5203006267547607
train gradient:  0.1340229363900557
iteration : 7035
train acc:  0.71875
train loss:  0.5218439102172852
train gradient:  0.13974197317295187
iteration : 7036
train acc:  0.703125
train loss:  0.4934743344783783
train gradient:  0.10926205125403397
iteration : 7037
train acc:  0.75
train loss:  0.48733949661254883
train gradient:  0.11208684075776389
iteration : 7038
train acc:  0.6875
train loss:  0.555321455001831
train gradient:  0.1763574977976926
iteration : 7039
train acc:  0.703125
train loss:  0.5123394727706909
train gradient:  0.12993899689547161
iteration : 7040
train acc:  0.703125
train loss:  0.5131295919418335
train gradient:  0.14051494230437317
iteration : 7041
train acc:  0.78125
train loss:  0.45490801334381104
train gradient:  0.1721652752341409
iteration : 7042
train acc:  0.7265625
train loss:  0.5589032769203186
train gradient:  0.1554301827200002
iteration : 7043
train acc:  0.78125
train loss:  0.4419691264629364
train gradient:  0.13825307000891418
iteration : 7044
train acc:  0.75
train loss:  0.4697459042072296
train gradient:  0.12213699109057322
iteration : 7045
train acc:  0.75
train loss:  0.4690377414226532
train gradient:  0.17846542650558805
iteration : 7046
train acc:  0.78125
train loss:  0.489400714635849
train gradient:  0.1233701852548593
iteration : 7047
train acc:  0.7109375
train loss:  0.4872761368751526
train gradient:  0.13193672121496897
iteration : 7048
train acc:  0.7734375
train loss:  0.47580018639564514
train gradient:  0.16685956569958454
iteration : 7049
train acc:  0.8046875
train loss:  0.41022124886512756
train gradient:  0.0822407378667829
iteration : 7050
train acc:  0.7890625
train loss:  0.4901490807533264
train gradient:  0.13893150835702706
iteration : 7051
train acc:  0.75
train loss:  0.4551595449447632
train gradient:  0.12426160002873606
iteration : 7052
train acc:  0.7734375
train loss:  0.45429927110671997
train gradient:  0.12100853639520531
iteration : 7053
train acc:  0.75
train loss:  0.4471464157104492
train gradient:  0.13912263853477308
iteration : 7054
train acc:  0.7421875
train loss:  0.47697144746780396
train gradient:  0.13488411820488153
iteration : 7055
train acc:  0.7265625
train loss:  0.560620903968811
train gradient:  0.16851169499991492
iteration : 7056
train acc:  0.7578125
train loss:  0.4803493618965149
train gradient:  0.16000585814205356
iteration : 7057
train acc:  0.765625
train loss:  0.4418121576309204
train gradient:  0.12562122876949344
iteration : 7058
train acc:  0.7578125
train loss:  0.48395445942878723
train gradient:  0.12241487618829125
iteration : 7059
train acc:  0.7578125
train loss:  0.4491543769836426
train gradient:  0.11316836003901364
iteration : 7060
train acc:  0.734375
train loss:  0.48023027181625366
train gradient:  0.13767398377952444
iteration : 7061
train acc:  0.7734375
train loss:  0.48961102962493896
train gradient:  0.15897273893267477
iteration : 7062
train acc:  0.71875
train loss:  0.48996394872665405
train gradient:  0.11047600790482087
iteration : 7063
train acc:  0.6953125
train loss:  0.514090895652771
train gradient:  0.15006094379217855
iteration : 7064
train acc:  0.765625
train loss:  0.5148848295211792
train gradient:  0.12365526808226712
iteration : 7065
train acc:  0.71875
train loss:  0.5432723164558411
train gradient:  0.18985878515186444
iteration : 7066
train acc:  0.671875
train loss:  0.5646096467971802
train gradient:  0.19081336797633708
iteration : 7067
train acc:  0.7890625
train loss:  0.47610998153686523
train gradient:  0.13238848749516688
iteration : 7068
train acc:  0.75
train loss:  0.5282966494560242
train gradient:  0.1605329828448146
iteration : 7069
train acc:  0.65625
train loss:  0.5385489463806152
train gradient:  0.1871663488335784
iteration : 7070
train acc:  0.78125
train loss:  0.49871933460235596
train gradient:  0.11255141377743993
iteration : 7071
train acc:  0.671875
train loss:  0.605093240737915
train gradient:  0.19374758942793058
iteration : 7072
train acc:  0.765625
train loss:  0.45167040824890137
train gradient:  0.14874925585482957
iteration : 7073
train acc:  0.7109375
train loss:  0.490205854177475
train gradient:  0.1561630806640668
iteration : 7074
train acc:  0.7109375
train loss:  0.500152587890625
train gradient:  0.1315218947022077
iteration : 7075
train acc:  0.7421875
train loss:  0.4888550341129303
train gradient:  0.14219280554811964
iteration : 7076
train acc:  0.703125
train loss:  0.5214196443557739
train gradient:  0.1398604615264762
iteration : 7077
train acc:  0.703125
train loss:  0.5606948137283325
train gradient:  0.15549724256109776
iteration : 7078
train acc:  0.6875
train loss:  0.5603653192520142
train gradient:  0.13168267350177887
iteration : 7079
train acc:  0.7109375
train loss:  0.5028511881828308
train gradient:  0.15451090448136362
iteration : 7080
train acc:  0.671875
train loss:  0.5331606864929199
train gradient:  0.19862859920456938
iteration : 7081
train acc:  0.6796875
train loss:  0.5628270506858826
train gradient:  0.23616027813176424
iteration : 7082
train acc:  0.703125
train loss:  0.5432889461517334
train gradient:  0.17547833620155295
iteration : 7083
train acc:  0.765625
train loss:  0.5034594535827637
train gradient:  0.14454784674176263
iteration : 7084
train acc:  0.625
train loss:  0.6103161573410034
train gradient:  0.18139100052281837
iteration : 7085
train acc:  0.6875
train loss:  0.5439184308052063
train gradient:  0.15443614667578615
iteration : 7086
train acc:  0.7265625
train loss:  0.4731162488460541
train gradient:  0.1510351377247563
iteration : 7087
train acc:  0.78125
train loss:  0.5071624517440796
train gradient:  0.1388758427283372
iteration : 7088
train acc:  0.75
train loss:  0.513655960559845
train gradient:  0.16964622037917865
iteration : 7089
train acc:  0.7265625
train loss:  0.562208890914917
train gradient:  0.16975109126508942
iteration : 7090
train acc:  0.765625
train loss:  0.4625682830810547
train gradient:  0.14016613132911698
iteration : 7091
train acc:  0.734375
train loss:  0.48725488781929016
train gradient:  0.1497721317541962
iteration : 7092
train acc:  0.78125
train loss:  0.5054504871368408
train gradient:  0.1280368307452047
iteration : 7093
train acc:  0.7734375
train loss:  0.5182411074638367
train gradient:  0.1613501744381482
iteration : 7094
train acc:  0.7890625
train loss:  0.478116512298584
train gradient:  0.1326075320264322
iteration : 7095
train acc:  0.7734375
train loss:  0.46827948093414307
train gradient:  0.14411785168071706
iteration : 7096
train acc:  0.75
train loss:  0.47952091693878174
train gradient:  0.12291593274690449
iteration : 7097
train acc:  0.7578125
train loss:  0.4754962921142578
train gradient:  0.12424696684444125
iteration : 7098
train acc:  0.7578125
train loss:  0.4646601676940918
train gradient:  0.1320682938861911
iteration : 7099
train acc:  0.6796875
train loss:  0.5721784234046936
train gradient:  0.16397399181876482
iteration : 7100
train acc:  0.7734375
train loss:  0.4956648647785187
train gradient:  0.1589186805835866
iteration : 7101
train acc:  0.7109375
train loss:  0.5683367252349854
train gradient:  0.2242933746214953
iteration : 7102
train acc:  0.75
train loss:  0.49471497535705566
train gradient:  0.15524380050059466
iteration : 7103
train acc:  0.734375
train loss:  0.48140811920166016
train gradient:  0.11599271557459903
iteration : 7104
train acc:  0.8125
train loss:  0.45094573497772217
train gradient:  0.12154985694407966
iteration : 7105
train acc:  0.71875
train loss:  0.573457658290863
train gradient:  0.22715157646819434
iteration : 7106
train acc:  0.6875
train loss:  0.5320935249328613
train gradient:  0.1415676456581627
iteration : 7107
train acc:  0.7890625
train loss:  0.5333918333053589
train gradient:  0.15915055598859223
iteration : 7108
train acc:  0.671875
train loss:  0.5280917286872864
train gradient:  0.1899513180516963
iteration : 7109
train acc:  0.796875
train loss:  0.44274818897247314
train gradient:  0.09693195688050134
iteration : 7110
train acc:  0.734375
train loss:  0.46545809507369995
train gradient:  0.11433961217830842
iteration : 7111
train acc:  0.734375
train loss:  0.5187862515449524
train gradient:  0.17844395007913683
iteration : 7112
train acc:  0.7421875
train loss:  0.5166950821876526
train gradient:  0.16064780069715975
iteration : 7113
train acc:  0.703125
train loss:  0.5220848321914673
train gradient:  0.14142506790863582
iteration : 7114
train acc:  0.7421875
train loss:  0.5035269856452942
train gradient:  0.1142171969198006
iteration : 7115
train acc:  0.7265625
train loss:  0.5315349102020264
train gradient:  0.16319877309507014
iteration : 7116
train acc:  0.7421875
train loss:  0.4543907642364502
train gradient:  0.13707990811804466
iteration : 7117
train acc:  0.6875
train loss:  0.5823979377746582
train gradient:  0.17372879336084618
iteration : 7118
train acc:  0.734375
train loss:  0.482431024312973
train gradient:  0.15389480628103588
iteration : 7119
train acc:  0.75
train loss:  0.4717954993247986
train gradient:  0.1222800925980543
iteration : 7120
train acc:  0.78125
train loss:  0.46295928955078125
train gradient:  0.12165228426788667
iteration : 7121
train acc:  0.671875
train loss:  0.5086756944656372
train gradient:  0.11930213154119315
iteration : 7122
train acc:  0.7109375
train loss:  0.50042724609375
train gradient:  0.2089548075735005
iteration : 7123
train acc:  0.75
train loss:  0.4970379173755646
train gradient:  0.15530559441577704
iteration : 7124
train acc:  0.71875
train loss:  0.5440554618835449
train gradient:  0.15917519622705145
iteration : 7125
train acc:  0.703125
train loss:  0.5087168216705322
train gradient:  0.13558164605841594
iteration : 7126
train acc:  0.7890625
train loss:  0.44453150033950806
train gradient:  0.14340579433360506
iteration : 7127
train acc:  0.75
train loss:  0.5111316442489624
train gradient:  0.1283691386078542
iteration : 7128
train acc:  0.765625
train loss:  0.4921666979789734
train gradient:  0.13179242216937648
iteration : 7129
train acc:  0.703125
train loss:  0.5231016874313354
train gradient:  0.14508229987056293
iteration : 7130
train acc:  0.7109375
train loss:  0.5602527856826782
train gradient:  0.22305476496129104
iteration : 7131
train acc:  0.6953125
train loss:  0.5220291614532471
train gradient:  0.19816576831182098
iteration : 7132
train acc:  0.765625
train loss:  0.49637410044670105
train gradient:  0.15505031843906864
iteration : 7133
train acc:  0.765625
train loss:  0.4442647099494934
train gradient:  0.11616846610232144
iteration : 7134
train acc:  0.8125
train loss:  0.4217808246612549
train gradient:  0.08058585298604398
iteration : 7135
train acc:  0.7265625
train loss:  0.5013612508773804
train gradient:  0.15637406072394347
iteration : 7136
train acc:  0.7265625
train loss:  0.5357204675674438
train gradient:  0.21145110488429997
iteration : 7137
train acc:  0.7265625
train loss:  0.5120445489883423
train gradient:  0.15556961508627648
iteration : 7138
train acc:  0.765625
train loss:  0.460228830575943
train gradient:  0.12111835312346164
iteration : 7139
train acc:  0.7734375
train loss:  0.4644091725349426
train gradient:  0.12157995956997655
iteration : 7140
train acc:  0.7734375
train loss:  0.49897700548171997
train gradient:  0.14199007792355778
iteration : 7141
train acc:  0.7421875
train loss:  0.5062215328216553
train gradient:  0.1500637889723886
iteration : 7142
train acc:  0.71875
train loss:  0.5089056491851807
train gradient:  0.1265207532257024
iteration : 7143
train acc:  0.75
train loss:  0.5042089223861694
train gradient:  0.15012849522195568
iteration : 7144
train acc:  0.734375
train loss:  0.5143862962722778
train gradient:  0.1443190735846296
iteration : 7145
train acc:  0.796875
train loss:  0.4721839427947998
train gradient:  0.1413564502777931
iteration : 7146
train acc:  0.75
train loss:  0.4976537227630615
train gradient:  0.12760326554621215
iteration : 7147
train acc:  0.75
train loss:  0.4825335741043091
train gradient:  0.11761757637569097
iteration : 7148
train acc:  0.7421875
train loss:  0.49980542063713074
train gradient:  0.12832286993850098
iteration : 7149
train acc:  0.6953125
train loss:  0.5509079694747925
train gradient:  0.15738914459788428
iteration : 7150
train acc:  0.7265625
train loss:  0.5059711337089539
train gradient:  0.220693502013647
iteration : 7151
train acc:  0.6640625
train loss:  0.5588716268539429
train gradient:  0.18513529425615202
iteration : 7152
train acc:  0.7421875
train loss:  0.5224297046661377
train gradient:  0.14443771120777907
iteration : 7153
train acc:  0.78125
train loss:  0.44857096672058105
train gradient:  0.11256042988766744
iteration : 7154
train acc:  0.7265625
train loss:  0.5320155024528503
train gradient:  0.18667918740014522
iteration : 7155
train acc:  0.7578125
train loss:  0.4383552372455597
train gradient:  0.11418652570248142
iteration : 7156
train acc:  0.71875
train loss:  0.5286765098571777
train gradient:  0.23900184955541287
iteration : 7157
train acc:  0.734375
train loss:  0.5363772511482239
train gradient:  0.1386477541003701
iteration : 7158
train acc:  0.65625
train loss:  0.5584123134613037
train gradient:  0.14864924837274046
iteration : 7159
train acc:  0.7265625
train loss:  0.49956321716308594
train gradient:  0.1413106996538364
iteration : 7160
train acc:  0.7734375
train loss:  0.48431462049484253
train gradient:  0.13026920145765314
iteration : 7161
train acc:  0.7109375
train loss:  0.502564013004303
train gradient:  0.1418082315726004
iteration : 7162
train acc:  0.6640625
train loss:  0.5575089454650879
train gradient:  0.15056330642831448
iteration : 7163
train acc:  0.703125
train loss:  0.5099072456359863
train gradient:  0.14501802571006017
iteration : 7164
train acc:  0.640625
train loss:  0.5610904693603516
train gradient:  0.13852254843264983
iteration : 7165
train acc:  0.765625
train loss:  0.501794159412384
train gradient:  0.13402771034332528
iteration : 7166
train acc:  0.734375
train loss:  0.5511950254440308
train gradient:  0.1875779217349603
iteration : 7167
train acc:  0.8125
train loss:  0.4386238753795624
train gradient:  0.13002043117464585
iteration : 7168
train acc:  0.71875
train loss:  0.5155376195907593
train gradient:  0.13270871904068857
iteration : 7169
train acc:  0.6953125
train loss:  0.5114275217056274
train gradient:  0.13630461661379867
iteration : 7170
train acc:  0.796875
train loss:  0.46892863512039185
train gradient:  0.11466275592610514
iteration : 7171
train acc:  0.7109375
train loss:  0.5361289381980896
train gradient:  0.21321426737726573
iteration : 7172
train acc:  0.8203125
train loss:  0.45069682598114014
train gradient:  0.14965877752268952
iteration : 7173
train acc:  0.7734375
train loss:  0.464835524559021
train gradient:  0.12067718722170073
iteration : 7174
train acc:  0.796875
train loss:  0.4501146972179413
train gradient:  0.11688845262804187
iteration : 7175
train acc:  0.75
train loss:  0.4977107048034668
train gradient:  0.13199459461210264
iteration : 7176
train acc:  0.75
train loss:  0.47801536321640015
train gradient:  0.12417501659184753
iteration : 7177
train acc:  0.7734375
train loss:  0.4880399703979492
train gradient:  0.12165806405602354
iteration : 7178
train acc:  0.7265625
train loss:  0.5085426568984985
train gradient:  0.14055978164506402
iteration : 7179
train acc:  0.7421875
train loss:  0.4983019232749939
train gradient:  0.14105569288590408
iteration : 7180
train acc:  0.8203125
train loss:  0.4078458547592163
train gradient:  0.10006280899387408
iteration : 7181
train acc:  0.7578125
train loss:  0.49224236607551575
train gradient:  0.10764787571905138
iteration : 7182
train acc:  0.7109375
train loss:  0.5000129342079163
train gradient:  0.20371410647667668
iteration : 7183
train acc:  0.7109375
train loss:  0.5674121379852295
train gradient:  0.1480048072989643
iteration : 7184
train acc:  0.7578125
train loss:  0.453466534614563
train gradient:  0.16223407260019074
iteration : 7185
train acc:  0.8046875
train loss:  0.45959556102752686
train gradient:  0.12860230856371271
iteration : 7186
train acc:  0.765625
train loss:  0.5210092067718506
train gradient:  0.13537262692373403
iteration : 7187
train acc:  0.734375
train loss:  0.45195940136909485
train gradient:  0.12555604146604532
iteration : 7188
train acc:  0.6640625
train loss:  0.5672817826271057
train gradient:  0.16547029688463966
iteration : 7189
train acc:  0.8125
train loss:  0.4422500431537628
train gradient:  0.1254138412110164
iteration : 7190
train acc:  0.7109375
train loss:  0.5221033096313477
train gradient:  0.12493810211062605
iteration : 7191
train acc:  0.734375
train loss:  0.48209285736083984
train gradient:  0.10321641525480507
iteration : 7192
train acc:  0.78125
train loss:  0.5099610090255737
train gradient:  0.13382740464545273
iteration : 7193
train acc:  0.78125
train loss:  0.4740772247314453
train gradient:  0.11607937575518326
iteration : 7194
train acc:  0.734375
train loss:  0.5006639957427979
train gradient:  0.1463364903902857
iteration : 7195
train acc:  0.7421875
train loss:  0.4870647192001343
train gradient:  0.13086747389646286
iteration : 7196
train acc:  0.78125
train loss:  0.5064112544059753
train gradient:  0.17019999829799867
iteration : 7197
train acc:  0.78125
train loss:  0.45062530040740967
train gradient:  0.1084815672879963
iteration : 7198
train acc:  0.71875
train loss:  0.5156575441360474
train gradient:  0.17090703188730988
iteration : 7199
train acc:  0.703125
train loss:  0.5200257301330566
train gradient:  0.1603762642233004
iteration : 7200
train acc:  0.734375
train loss:  0.4916110634803772
train gradient:  0.14837652500061138
iteration : 7201
train acc:  0.7421875
train loss:  0.5163962841033936
train gradient:  0.16189070567990171
iteration : 7202
train acc:  0.7578125
train loss:  0.48581117391586304
train gradient:  0.1591708488311298
iteration : 7203
train acc:  0.8203125
train loss:  0.4367756247520447
train gradient:  0.18193266276598222
iteration : 7204
train acc:  0.7890625
train loss:  0.4443337619304657
train gradient:  0.1401086417021285
iteration : 7205
train acc:  0.734375
train loss:  0.512755811214447
train gradient:  0.13330806975498902
iteration : 7206
train acc:  0.796875
train loss:  0.47604668140411377
train gradient:  0.10817574372921486
iteration : 7207
train acc:  0.734375
train loss:  0.5392472743988037
train gradient:  0.17137921868178535
iteration : 7208
train acc:  0.7109375
train loss:  0.5179330110549927
train gradient:  0.1891602673751679
iteration : 7209
train acc:  0.75
train loss:  0.4579984247684479
train gradient:  0.13889174260830553
iteration : 7210
train acc:  0.765625
train loss:  0.5084728598594666
train gradient:  0.1529971234023923
iteration : 7211
train acc:  0.7265625
train loss:  0.511061429977417
train gradient:  0.15090839454462296
iteration : 7212
train acc:  0.71875
train loss:  0.5191584825515747
train gradient:  0.13730110451253488
iteration : 7213
train acc:  0.6640625
train loss:  0.5199572443962097
train gradient:  0.15031891783169735
iteration : 7214
train acc:  0.71875
train loss:  0.5258238911628723
train gradient:  0.15753206518436158
iteration : 7215
train acc:  0.7265625
train loss:  0.5462512969970703
train gradient:  0.13816519318486226
iteration : 7216
train acc:  0.71875
train loss:  0.5189789533615112
train gradient:  0.19054531261568525
iteration : 7217
train acc:  0.6953125
train loss:  0.492953360080719
train gradient:  0.1325346937202594
iteration : 7218
train acc:  0.78125
train loss:  0.5488051176071167
train gradient:  0.17648185959571538
iteration : 7219
train acc:  0.734375
train loss:  0.5515850186347961
train gradient:  0.17490085576751618
iteration : 7220
train acc:  0.75
train loss:  0.46499118208885193
train gradient:  0.1219978847398589
iteration : 7221
train acc:  0.8046875
train loss:  0.48031502962112427
train gradient:  0.16709525970994848
iteration : 7222
train acc:  0.8359375
train loss:  0.43155044317245483
train gradient:  0.1273762464518469
iteration : 7223
train acc:  0.7890625
train loss:  0.4254295825958252
train gradient:  0.12969307352611342
iteration : 7224
train acc:  0.75
train loss:  0.4670642912387848
train gradient:  0.13616919186054627
iteration : 7225
train acc:  0.7890625
train loss:  0.4710153043270111
train gradient:  0.13970160888126334
iteration : 7226
train acc:  0.78125
train loss:  0.45074912905693054
train gradient:  0.13005146881746493
iteration : 7227
train acc:  0.765625
train loss:  0.439877986907959
train gradient:  0.12754303947931228
iteration : 7228
train acc:  0.6875
train loss:  0.5879985690116882
train gradient:  0.1632326072674905
iteration : 7229
train acc:  0.75
train loss:  0.5086748600006104
train gradient:  0.14608215567813454
iteration : 7230
train acc:  0.6953125
train loss:  0.5487526655197144
train gradient:  0.19610865572392328
iteration : 7231
train acc:  0.625
train loss:  0.6187350153923035
train gradient:  0.24422862227198952
iteration : 7232
train acc:  0.7109375
train loss:  0.5244488716125488
train gradient:  0.157240182494812
iteration : 7233
train acc:  0.6875
train loss:  0.5084629654884338
train gradient:  0.16087526447170453
iteration : 7234
train acc:  0.796875
train loss:  0.44801703095436096
train gradient:  0.10308406434657054
iteration : 7235
train acc:  0.7109375
train loss:  0.5912818312644958
train gradient:  0.18982116852602032
iteration : 7236
train acc:  0.7109375
train loss:  0.45170143246650696
train gradient:  0.12186787994826889
iteration : 7237
train acc:  0.6953125
train loss:  0.5184195041656494
train gradient:  0.13013638467117045
iteration : 7238
train acc:  0.7265625
train loss:  0.4851301312446594
train gradient:  0.13982910959504663
iteration : 7239
train acc:  0.734375
train loss:  0.49579328298568726
train gradient:  0.14343554929182323
iteration : 7240
train acc:  0.7265625
train loss:  0.5114201307296753
train gradient:  0.17609943355986762
iteration : 7241
train acc:  0.796875
train loss:  0.45882752537727356
train gradient:  0.12045521404287474
iteration : 7242
train acc:  0.703125
train loss:  0.5413523316383362
train gradient:  0.14105879956166506
iteration : 7243
train acc:  0.7421875
train loss:  0.5251462459564209
train gradient:  0.15051874278747085
iteration : 7244
train acc:  0.703125
train loss:  0.5142306685447693
train gradient:  0.16235002109578772
iteration : 7245
train acc:  0.78125
train loss:  0.42984455823898315
train gradient:  0.13939934705430357
iteration : 7246
train acc:  0.7421875
train loss:  0.5406099557876587
train gradient:  0.2295271894881292
iteration : 7247
train acc:  0.75
train loss:  0.49460291862487793
train gradient:  0.1276849595982651
iteration : 7248
train acc:  0.703125
train loss:  0.5421332120895386
train gradient:  0.16145165098634096
iteration : 7249
train acc:  0.6875
train loss:  0.5590196251869202
train gradient:  0.13475034809124292
iteration : 7250
train acc:  0.7265625
train loss:  0.5146598815917969
train gradient:  0.14786420447354362
iteration : 7251
train acc:  0.7109375
train loss:  0.46620041131973267
train gradient:  0.1303609300785094
iteration : 7252
train acc:  0.8125
train loss:  0.44390594959259033
train gradient:  0.10674473566143033
iteration : 7253
train acc:  0.7734375
train loss:  0.5001025795936584
train gradient:  0.1494953072607313
iteration : 7254
train acc:  0.7265625
train loss:  0.5340716242790222
train gradient:  0.17760068772992776
iteration : 7255
train acc:  0.734375
train loss:  0.4786415100097656
train gradient:  0.14640262470166898
iteration : 7256
train acc:  0.7734375
train loss:  0.44319164752960205
train gradient:  0.11538293239269895
iteration : 7257
train acc:  0.78125
train loss:  0.4437562823295593
train gradient:  0.18385119992718382
iteration : 7258
train acc:  0.703125
train loss:  0.5326485633850098
train gradient:  0.15503423490096396
iteration : 7259
train acc:  0.734375
train loss:  0.5260703563690186
train gradient:  0.1303118381488148
iteration : 7260
train acc:  0.7421875
train loss:  0.490980327129364
train gradient:  0.1558302518998971
iteration : 7261
train acc:  0.7265625
train loss:  0.5000557899475098
train gradient:  0.1325710609128598
iteration : 7262
train acc:  0.6875
train loss:  0.5664013624191284
train gradient:  0.19648716402718952
iteration : 7263
train acc:  0.71875
train loss:  0.5048439502716064
train gradient:  0.11106074777132374
iteration : 7264
train acc:  0.78125
train loss:  0.4820811450481415
train gradient:  0.1370873276277397
iteration : 7265
train acc:  0.734375
train loss:  0.5630844831466675
train gradient:  0.17895786419312998
iteration : 7266
train acc:  0.71875
train loss:  0.5248055458068848
train gradient:  0.1378316526612982
iteration : 7267
train acc:  0.6875
train loss:  0.4898962378501892
train gradient:  0.1498737252087219
iteration : 7268
train acc:  0.78125
train loss:  0.42967647314071655
train gradient:  0.10952746836915508
iteration : 7269
train acc:  0.703125
train loss:  0.5077768564224243
train gradient:  0.15511498571950239
iteration : 7270
train acc:  0.6953125
train loss:  0.4928992986679077
train gradient:  0.14218674843184625
iteration : 7271
train acc:  0.7265625
train loss:  0.499138742685318
train gradient:  0.12294186504857621
iteration : 7272
train acc:  0.7421875
train loss:  0.4920065402984619
train gradient:  0.17260725981689543
iteration : 7273
train acc:  0.7109375
train loss:  0.48487091064453125
train gradient:  0.1531656668200404
iteration : 7274
train acc:  0.71875
train loss:  0.5091769695281982
train gradient:  0.14950630382520713
iteration : 7275
train acc:  0.7421875
train loss:  0.45429694652557373
train gradient:  0.11956784533892652
iteration : 7276
train acc:  0.78125
train loss:  0.47639143466949463
train gradient:  0.13450859757613726
iteration : 7277
train acc:  0.75
train loss:  0.45976901054382324
train gradient:  0.10183489086508068
iteration : 7278
train acc:  0.7265625
train loss:  0.5069020986557007
train gradient:  0.1825281201245147
iteration : 7279
train acc:  0.8125
train loss:  0.4439844489097595
train gradient:  0.12214431743723883
iteration : 7280
train acc:  0.765625
train loss:  0.5175155401229858
train gradient:  0.14805451673874048
iteration : 7281
train acc:  0.6953125
train loss:  0.5353845357894897
train gradient:  0.15303153822642623
iteration : 7282
train acc:  0.7734375
train loss:  0.4529370367527008
train gradient:  0.1067038036365475
iteration : 7283
train acc:  0.6875
train loss:  0.5691103339195251
train gradient:  0.19662288927365634
iteration : 7284
train acc:  0.7421875
train loss:  0.476177841424942
train gradient:  0.13478494790487072
iteration : 7285
train acc:  0.7265625
train loss:  0.55096834897995
train gradient:  0.17758928517193784
iteration : 7286
train acc:  0.6796875
train loss:  0.5410169363021851
train gradient:  0.16576206618212624
iteration : 7287
train acc:  0.6953125
train loss:  0.5206504464149475
train gradient:  0.17306699395203973
iteration : 7288
train acc:  0.6875
train loss:  0.587611973285675
train gradient:  0.16477499697244075
iteration : 7289
train acc:  0.734375
train loss:  0.48780933022499084
train gradient:  0.1164749629961233
iteration : 7290
train acc:  0.734375
train loss:  0.514854907989502
train gradient:  0.14313918402373466
iteration : 7291
train acc:  0.75
train loss:  0.504770040512085
train gradient:  0.1674452023781206
iteration : 7292
train acc:  0.7265625
train loss:  0.4985086917877197
train gradient:  0.14438319199567246
iteration : 7293
train acc:  0.765625
train loss:  0.5169883966445923
train gradient:  0.1540405558363256
iteration : 7294
train acc:  0.796875
train loss:  0.4282124936580658
train gradient:  0.09518670017560735
iteration : 7295
train acc:  0.703125
train loss:  0.5381031036376953
train gradient:  0.1449398166343862
iteration : 7296
train acc:  0.765625
train loss:  0.5080651640892029
train gradient:  0.13202690672165698
iteration : 7297
train acc:  0.6953125
train loss:  0.5617140531539917
train gradient:  0.13814986502762933
iteration : 7298
train acc:  0.703125
train loss:  0.5270365476608276
train gradient:  0.14552840654509022
iteration : 7299
train acc:  0.75
train loss:  0.5065122842788696
train gradient:  0.16431702014669602
iteration : 7300
train acc:  0.78125
train loss:  0.4512706995010376
train gradient:  0.1310059957039823
iteration : 7301
train acc:  0.734375
train loss:  0.5215204954147339
train gradient:  0.14418092689814557
iteration : 7302
train acc:  0.6953125
train loss:  0.5606659650802612
train gradient:  0.19232936107288365
iteration : 7303
train acc:  0.7890625
train loss:  0.4484802484512329
train gradient:  0.12561523835965036
iteration : 7304
train acc:  0.734375
train loss:  0.5062642097473145
train gradient:  0.13132049059654785
iteration : 7305
train acc:  0.7734375
train loss:  0.4476966857910156
train gradient:  0.11294838337184182
iteration : 7306
train acc:  0.7421875
train loss:  0.49523454904556274
train gradient:  0.11535472018891378
iteration : 7307
train acc:  0.7265625
train loss:  0.5210695266723633
train gradient:  0.28666385297472563
iteration : 7308
train acc:  0.7734375
train loss:  0.44691312313079834
train gradient:  0.11515758704320282
iteration : 7309
train acc:  0.7734375
train loss:  0.4720897078514099
train gradient:  0.15396321332344454
iteration : 7310
train acc:  0.8125
train loss:  0.4344434142112732
train gradient:  0.08398085352494994
iteration : 7311
train acc:  0.703125
train loss:  0.525746762752533
train gradient:  0.1672425595658664
iteration : 7312
train acc:  0.75
train loss:  0.48855069279670715
train gradient:  0.16388501117112045
iteration : 7313
train acc:  0.828125
train loss:  0.3965309262275696
train gradient:  0.09502286845974324
iteration : 7314
train acc:  0.7734375
train loss:  0.45288947224617004
train gradient:  0.12670553106168575
iteration : 7315
train acc:  0.6484375
train loss:  0.540827214717865
train gradient:  0.1653688936394308
iteration : 7316
train acc:  0.75
train loss:  0.4794921278953552
train gradient:  0.1270498985410234
iteration : 7317
train acc:  0.7109375
train loss:  0.5579196214675903
train gradient:  0.2317059229795754
iteration : 7318
train acc:  0.8125
train loss:  0.4429144859313965
train gradient:  0.09420546052804618
iteration : 7319
train acc:  0.7734375
train loss:  0.5093318223953247
train gradient:  0.1309225287310138
iteration : 7320
train acc:  0.75
train loss:  0.46876153349876404
train gradient:  0.13546810017875893
iteration : 7321
train acc:  0.7578125
train loss:  0.4789723753929138
train gradient:  0.15015591005553602
iteration : 7322
train acc:  0.7421875
train loss:  0.4892181158065796
train gradient:  0.15572222753776133
iteration : 7323
train acc:  0.6875
train loss:  0.5831203460693359
train gradient:  0.15299056452405116
iteration : 7324
train acc:  0.7421875
train loss:  0.460770845413208
train gradient:  0.10804504350226626
iteration : 7325
train acc:  0.78125
train loss:  0.4989434480667114
train gradient:  0.14123200427185995
iteration : 7326
train acc:  0.703125
train loss:  0.5123720169067383
train gradient:  0.13142647232493387
iteration : 7327
train acc:  0.6796875
train loss:  0.5418986082077026
train gradient:  0.16936959256601997
iteration : 7328
train acc:  0.703125
train loss:  0.5638529062271118
train gradient:  0.16285599177562676
iteration : 7329
train acc:  0.734375
train loss:  0.5101282596588135
train gradient:  0.15885500292848698
iteration : 7330
train acc:  0.7578125
train loss:  0.5065796375274658
train gradient:  0.1257380893229066
iteration : 7331
train acc:  0.78125
train loss:  0.44754233956336975
train gradient:  0.11411346998190125
iteration : 7332
train acc:  0.71875
train loss:  0.5153131484985352
train gradient:  0.1944438288117486
iteration : 7333
train acc:  0.78125
train loss:  0.4754089415073395
train gradient:  0.13550950434859543
iteration : 7334
train acc:  0.6953125
train loss:  0.535844087600708
train gradient:  0.20813346644180464
iteration : 7335
train acc:  0.6640625
train loss:  0.5984721183776855
train gradient:  0.15620675406883539
iteration : 7336
train acc:  0.71875
train loss:  0.5114315152168274
train gradient:  0.20283104244512296
iteration : 7337
train acc:  0.671875
train loss:  0.5426260232925415
train gradient:  0.16666910720517214
iteration : 7338
train acc:  0.8203125
train loss:  0.48707669973373413
train gradient:  0.16313845840626107
iteration : 7339
train acc:  0.6875
train loss:  0.5572555065155029
train gradient:  0.1740758774852083
iteration : 7340
train acc:  0.734375
train loss:  0.49483588337898254
train gradient:  0.13079066576154383
iteration : 7341
train acc:  0.7890625
train loss:  0.45266783237457275
train gradient:  0.11027142356546416
iteration : 7342
train acc:  0.7421875
train loss:  0.5321455001831055
train gradient:  0.1748027554818633
iteration : 7343
train acc:  0.7890625
train loss:  0.44697263836860657
train gradient:  0.12090136985770933
iteration : 7344
train acc:  0.78125
train loss:  0.46225816011428833
train gradient:  0.1128300505819597
iteration : 7345
train acc:  0.7265625
train loss:  0.4870196580886841
train gradient:  0.10400537930377395
iteration : 7346
train acc:  0.765625
train loss:  0.5089057683944702
train gradient:  0.1500158200286132
iteration : 7347
train acc:  0.7109375
train loss:  0.5803363919258118
train gradient:  0.24550525468000375
iteration : 7348
train acc:  0.7890625
train loss:  0.4663039445877075
train gradient:  0.10845884004385271
iteration : 7349
train acc:  0.7734375
train loss:  0.4446149468421936
train gradient:  0.1529626809309394
iteration : 7350
train acc:  0.7109375
train loss:  0.52538001537323
train gradient:  0.20647711634139493
iteration : 7351
train acc:  0.7109375
train loss:  0.5567988157272339
train gradient:  0.20275596410028784
iteration : 7352
train acc:  0.7734375
train loss:  0.4926431179046631
train gradient:  0.14886999652052393
iteration : 7353
train acc:  0.6875
train loss:  0.5650255680084229
train gradient:  0.18814393050610578
iteration : 7354
train acc:  0.7734375
train loss:  0.4600304067134857
train gradient:  0.12040460707292296
iteration : 7355
train acc:  0.6953125
train loss:  0.5563792586326599
train gradient:  0.15680549488906262
iteration : 7356
train acc:  0.734375
train loss:  0.5601667165756226
train gradient:  0.18382391593429154
iteration : 7357
train acc:  0.6875
train loss:  0.5690879225730896
train gradient:  0.15498395982814717
iteration : 7358
train acc:  0.6640625
train loss:  0.5330303907394409
train gradient:  0.15097127490935736
iteration : 7359
train acc:  0.7265625
train loss:  0.5235008597373962
train gradient:  0.181058440951968
iteration : 7360
train acc:  0.6796875
train loss:  0.5055755376815796
train gradient:  0.16553453664056822
iteration : 7361
train acc:  0.71875
train loss:  0.4936520457267761
train gradient:  0.17871227691345704
iteration : 7362
train acc:  0.7109375
train loss:  0.5428221821784973
train gradient:  0.15893457994779214
iteration : 7363
train acc:  0.734375
train loss:  0.49140384793281555
train gradient:  0.15434648774801546
iteration : 7364
train acc:  0.7578125
train loss:  0.4839046597480774
train gradient:  0.15401851431328198
iteration : 7365
train acc:  0.625
train loss:  0.6148492693901062
train gradient:  0.19463599874397358
iteration : 7366
train acc:  0.6796875
train loss:  0.6067895293235779
train gradient:  0.16829769852890356
iteration : 7367
train acc:  0.7109375
train loss:  0.5328558087348938
train gradient:  0.12851666653567828
iteration : 7368
train acc:  0.6484375
train loss:  0.6119570732116699
train gradient:  0.20998311491471305
iteration : 7369
train acc:  0.6015625
train loss:  0.6340703964233398
train gradient:  0.2050538103486938
iteration : 7370
train acc:  0.7265625
train loss:  0.4710846543312073
train gradient:  0.10931562714884313
iteration : 7371
train acc:  0.7421875
train loss:  0.5093144178390503
train gradient:  0.17309423361593215
iteration : 7372
train acc:  0.765625
train loss:  0.4698076844215393
train gradient:  0.13271470870969848
iteration : 7373
train acc:  0.7265625
train loss:  0.5970931053161621
train gradient:  0.164498790156567
iteration : 7374
train acc:  0.6640625
train loss:  0.575407862663269
train gradient:  0.17480541406037353
iteration : 7375
train acc:  0.75
train loss:  0.4488374590873718
train gradient:  0.11758436584818518
iteration : 7376
train acc:  0.734375
train loss:  0.49027711153030396
train gradient:  0.14400772800178246
iteration : 7377
train acc:  0.78125
train loss:  0.4936586022377014
train gradient:  0.1309747187842893
iteration : 7378
train acc:  0.7265625
train loss:  0.5039633512496948
train gradient:  0.15515183084237322
iteration : 7379
train acc:  0.8046875
train loss:  0.4262876510620117
train gradient:  0.10659610148776578
iteration : 7380
train acc:  0.71875
train loss:  0.559736967086792
train gradient:  0.17126023763260173
iteration : 7381
train acc:  0.78125
train loss:  0.48501378297805786
train gradient:  0.14277581580868992
iteration : 7382
train acc:  0.75
train loss:  0.5215879082679749
train gradient:  0.14579469452230748
iteration : 7383
train acc:  0.6171875
train loss:  0.6149153709411621
train gradient:  0.15927572000761164
iteration : 7384
train acc:  0.7734375
train loss:  0.486138254404068
train gradient:  0.10953198678628331
iteration : 7385
train acc:  0.8125
train loss:  0.39729565382003784
train gradient:  0.11179033926853803
iteration : 7386
train acc:  0.734375
train loss:  0.5494562387466431
train gradient:  0.1777775151321953
iteration : 7387
train acc:  0.7265625
train loss:  0.5649275779724121
train gradient:  0.14418281709900876
iteration : 7388
train acc:  0.75
train loss:  0.47034725546836853
train gradient:  0.16529462989903201
iteration : 7389
train acc:  0.71875
train loss:  0.5297638177871704
train gradient:  0.14969135727084548
iteration : 7390
train acc:  0.7578125
train loss:  0.471041202545166
train gradient:  0.1428701325895576
iteration : 7391
train acc:  0.8203125
train loss:  0.40343353152275085
train gradient:  0.1082048030462295
iteration : 7392
train acc:  0.671875
train loss:  0.5700684189796448
train gradient:  0.19854932278702783
iteration : 7393
train acc:  0.765625
train loss:  0.45847487449645996
train gradient:  0.16725471711316092
iteration : 7394
train acc:  0.7578125
train loss:  0.4634978175163269
train gradient:  0.11417587802326991
iteration : 7395
train acc:  0.7421875
train loss:  0.4761500656604767
train gradient:  0.14919151270319886
iteration : 7396
train acc:  0.6953125
train loss:  0.5304986238479614
train gradient:  0.11742971514746765
iteration : 7397
train acc:  0.734375
train loss:  0.5430737733840942
train gradient:  0.15387104468580723
iteration : 7398
train acc:  0.7109375
train loss:  0.5149674415588379
train gradient:  0.1194802250532594
iteration : 7399
train acc:  0.71875
train loss:  0.5437179803848267
train gradient:  0.16101747380697148
iteration : 7400
train acc:  0.71875
train loss:  0.5562572479248047
train gradient:  0.179441420639051
iteration : 7401
train acc:  0.7578125
train loss:  0.4684526026248932
train gradient:  0.11742526357054395
iteration : 7402
train acc:  0.7421875
train loss:  0.4951566159725189
train gradient:  0.15418809075119105
iteration : 7403
train acc:  0.765625
train loss:  0.4880621135234833
train gradient:  0.13316277027722945
iteration : 7404
train acc:  0.734375
train loss:  0.48112767934799194
train gradient:  0.14140300004747652
iteration : 7405
train acc:  0.796875
train loss:  0.4476260542869568
train gradient:  0.1036622771652542
iteration : 7406
train acc:  0.8203125
train loss:  0.45297151803970337
train gradient:  0.12520417831950376
iteration : 7407
train acc:  0.7421875
train loss:  0.4988821744918823
train gradient:  0.1338648424855714
iteration : 7408
train acc:  0.6796875
train loss:  0.5449557304382324
train gradient:  0.15649207997140235
iteration : 7409
train acc:  0.6953125
train loss:  0.5441271066665649
train gradient:  0.15046717625756637
iteration : 7410
train acc:  0.734375
train loss:  0.463378369808197
train gradient:  0.11775923285854457
iteration : 7411
train acc:  0.6875
train loss:  0.5557189583778381
train gradient:  0.18214511951526224
iteration : 7412
train acc:  0.71875
train loss:  0.5353493690490723
train gradient:  0.14220212262248494
iteration : 7413
train acc:  0.6953125
train loss:  0.5414800643920898
train gradient:  0.1342401936297328
iteration : 7414
train acc:  0.7109375
train loss:  0.5051460266113281
train gradient:  0.1277102683492073
iteration : 7415
train acc:  0.71875
train loss:  0.5271294116973877
train gradient:  0.14223634608863445
iteration : 7416
train acc:  0.7421875
train loss:  0.5404530167579651
train gradient:  0.1443852695059924
iteration : 7417
train acc:  0.765625
train loss:  0.4863787889480591
train gradient:  0.12858279916514925
iteration : 7418
train acc:  0.609375
train loss:  0.6150448322296143
train gradient:  0.19546623872293056
iteration : 7419
train acc:  0.7734375
train loss:  0.493172287940979
train gradient:  0.1784141923735204
iteration : 7420
train acc:  0.7265625
train loss:  0.49421069025993347
train gradient:  0.13435835927747034
iteration : 7421
train acc:  0.75
train loss:  0.47463148832321167
train gradient:  0.14172095584955197
iteration : 7422
train acc:  0.7109375
train loss:  0.5008895397186279
train gradient:  0.1605586006554401
iteration : 7423
train acc:  0.7265625
train loss:  0.5227329730987549
train gradient:  0.14722204284662432
iteration : 7424
train acc:  0.7109375
train loss:  0.5212609767913818
train gradient:  0.15629759245951375
iteration : 7425
train acc:  0.703125
train loss:  0.5387063026428223
train gradient:  0.1456793814106046
iteration : 7426
train acc:  0.828125
train loss:  0.4299289286136627
train gradient:  0.10547231167909925
iteration : 7427
train acc:  0.734375
train loss:  0.48926109075546265
train gradient:  0.12232754170357578
iteration : 7428
train acc:  0.75
train loss:  0.43516361713409424
train gradient:  0.12857891191049198
iteration : 7429
train acc:  0.78125
train loss:  0.4903044104576111
train gradient:  0.12223389373056781
iteration : 7430
train acc:  0.765625
train loss:  0.49062055349349976
train gradient:  0.13828861528101966
iteration : 7431
train acc:  0.7734375
train loss:  0.47240495681762695
train gradient:  0.11646161215081831
iteration : 7432
train acc:  0.75
train loss:  0.5383238792419434
train gradient:  0.12495235881311972
iteration : 7433
train acc:  0.7578125
train loss:  0.5009573101997375
train gradient:  0.15548824420901647
iteration : 7434
train acc:  0.765625
train loss:  0.47265300154685974
train gradient:  0.11547963763810574
iteration : 7435
train acc:  0.6796875
train loss:  0.562650740146637
train gradient:  0.18993115143098754
iteration : 7436
train acc:  0.71875
train loss:  0.4821082353591919
train gradient:  0.11277015012976924
iteration : 7437
train acc:  0.7265625
train loss:  0.5193958282470703
train gradient:  0.13096953983237253
iteration : 7438
train acc:  0.7578125
train loss:  0.46411895751953125
train gradient:  0.11762417865247206
iteration : 7439
train acc:  0.6875
train loss:  0.5681307911872864
train gradient:  0.1923685618524303
iteration : 7440
train acc:  0.765625
train loss:  0.48827409744262695
train gradient:  0.12038452949713041
iteration : 7441
train acc:  0.6875
train loss:  0.5638874173164368
train gradient:  0.1804366594729393
iteration : 7442
train acc:  0.7265625
train loss:  0.5261539220809937
train gradient:  0.1362604944174518
iteration : 7443
train acc:  0.6953125
train loss:  0.5490531921386719
train gradient:  0.18983418060352272
iteration : 7444
train acc:  0.7890625
train loss:  0.4576268196105957
train gradient:  0.11849780262793812
iteration : 7445
train acc:  0.765625
train loss:  0.5518481731414795
train gradient:  0.16778469958937564
iteration : 7446
train acc:  0.7109375
train loss:  0.5214335918426514
train gradient:  0.13653321102614246
iteration : 7447
train acc:  0.6953125
train loss:  0.5831438302993774
train gradient:  0.18236114569697226
iteration : 7448
train acc:  0.7421875
train loss:  0.4441436529159546
train gradient:  0.10825361309284032
iteration : 7449
train acc:  0.75
train loss:  0.4489203095436096
train gradient:  0.1151333149028014
iteration : 7450
train acc:  0.78125
train loss:  0.4546055197715759
train gradient:  0.10691383758700118
iteration : 7451
train acc:  0.71875
train loss:  0.5117051601409912
train gradient:  0.14140709694875142
iteration : 7452
train acc:  0.7421875
train loss:  0.5077596306800842
train gradient:  0.1750134631083374
iteration : 7453
train acc:  0.671875
train loss:  0.5566465854644775
train gradient:  0.13774929716801929
iteration : 7454
train acc:  0.78125
train loss:  0.46126872301101685
train gradient:  0.13993922465064831
iteration : 7455
train acc:  0.7421875
train loss:  0.5168017148971558
train gradient:  0.14913626140459996
iteration : 7456
train acc:  0.78125
train loss:  0.4888586103916168
train gradient:  0.1550206408181511
iteration : 7457
train acc:  0.765625
train loss:  0.4748871922492981
train gradient:  0.1290712555104681
iteration : 7458
train acc:  0.71875
train loss:  0.475751131772995
train gradient:  0.12281298446446919
iteration : 7459
train acc:  0.7578125
train loss:  0.4899858236312866
train gradient:  0.13518935224954032
iteration : 7460
train acc:  0.7265625
train loss:  0.5980390310287476
train gradient:  0.16741405167769904
iteration : 7461
train acc:  0.7421875
train loss:  0.4741499125957489
train gradient:  0.1117327867014306
iteration : 7462
train acc:  0.7734375
train loss:  0.4800058901309967
train gradient:  0.12313110571505909
iteration : 7463
train acc:  0.71875
train loss:  0.5441879034042358
train gradient:  0.1776602830049363
iteration : 7464
train acc:  0.765625
train loss:  0.4965742826461792
train gradient:  0.12423574520554179
iteration : 7465
train acc:  0.671875
train loss:  0.6052730083465576
train gradient:  0.1671954224532681
iteration : 7466
train acc:  0.765625
train loss:  0.4581296443939209
train gradient:  0.11862146812938865
iteration : 7467
train acc:  0.75
train loss:  0.5166529417037964
train gradient:  0.11453853691651457
iteration : 7468
train acc:  0.75
train loss:  0.4742687940597534
train gradient:  0.13054371757476543
iteration : 7469
train acc:  0.65625
train loss:  0.5305705666542053
train gradient:  0.15345986745650542
iteration : 7470
train acc:  0.6796875
train loss:  0.5471594333648682
train gradient:  0.17836931912900097
iteration : 7471
train acc:  0.8515625
train loss:  0.4012265205383301
train gradient:  0.11046016419907857
iteration : 7472
train acc:  0.671875
train loss:  0.5643327832221985
train gradient:  0.13051760292361475
iteration : 7473
train acc:  0.71875
train loss:  0.5021733045578003
train gradient:  0.14844700068693795
iteration : 7474
train acc:  0.6953125
train loss:  0.5724259614944458
train gradient:  0.1653736298004438
iteration : 7475
train acc:  0.75
train loss:  0.48272770643234253
train gradient:  0.11380191798272793
iteration : 7476
train acc:  0.765625
train loss:  0.47622349858283997
train gradient:  0.1204692177306841
iteration : 7477
train acc:  0.6953125
train loss:  0.554090142250061
train gradient:  0.14755668873904326
iteration : 7478
train acc:  0.75
train loss:  0.5371296405792236
train gradient:  0.1589502177549551
iteration : 7479
train acc:  0.71875
train loss:  0.589855432510376
train gradient:  0.22861358904824436
iteration : 7480
train acc:  0.671875
train loss:  0.5900800228118896
train gradient:  0.1354967729053212
iteration : 7481
train acc:  0.734375
train loss:  0.5103870630264282
train gradient:  0.13687860518386435
iteration : 7482
train acc:  0.71875
train loss:  0.49984878301620483
train gradient:  0.1336711029391976
iteration : 7483
train acc:  0.734375
train loss:  0.47821444272994995
train gradient:  0.1082917096399017
iteration : 7484
train acc:  0.78125
train loss:  0.48205745220184326
train gradient:  0.1295985449127771
iteration : 7485
train acc:  0.7578125
train loss:  0.4681609272956848
train gradient:  0.13837657316518465
iteration : 7486
train acc:  0.703125
train loss:  0.5822678804397583
train gradient:  0.1849913277930358
iteration : 7487
train acc:  0.65625
train loss:  0.5895446538925171
train gradient:  0.15114630855610725
iteration : 7488
train acc:  0.7109375
train loss:  0.5256484746932983
train gradient:  0.1505796751543847
iteration : 7489
train acc:  0.7890625
train loss:  0.4739791750907898
train gradient:  0.10038904345623262
iteration : 7490
train acc:  0.7265625
train loss:  0.5325855016708374
train gradient:  0.15650968425472628
iteration : 7491
train acc:  0.7734375
train loss:  0.44632017612457275
train gradient:  0.11605979095122348
iteration : 7492
train acc:  0.7421875
train loss:  0.469624400138855
train gradient:  0.1033416432571136
iteration : 7493
train acc:  0.7578125
train loss:  0.4492851793766022
train gradient:  0.08873153077707796
iteration : 7494
train acc:  0.71875
train loss:  0.5011753439903259
train gradient:  0.11782287733798205
iteration : 7495
train acc:  0.71875
train loss:  0.5220305323600769
train gradient:  0.14403873096912656
iteration : 7496
train acc:  0.6953125
train loss:  0.5340420007705688
train gradient:  0.1858000414323493
iteration : 7497
train acc:  0.75
train loss:  0.534805417060852
train gradient:  0.14557078017492325
iteration : 7498
train acc:  0.71875
train loss:  0.4972219467163086
train gradient:  0.12396730698396688
iteration : 7499
train acc:  0.75
train loss:  0.5088687539100647
train gradient:  0.1442610176106569
iteration : 7500
train acc:  0.7890625
train loss:  0.49958938360214233
train gradient:  0.13738935364370813
iteration : 7501
train acc:  0.75
train loss:  0.5302728414535522
train gradient:  0.14098212914406327
iteration : 7502
train acc:  0.7734375
train loss:  0.46618813276290894
train gradient:  0.1254884540470626
iteration : 7503
train acc:  0.75
train loss:  0.4843480885028839
train gradient:  0.12119609875568739
iteration : 7504
train acc:  0.703125
train loss:  0.5459001064300537
train gradient:  0.14179789114117763
iteration : 7505
train acc:  0.7578125
train loss:  0.515012264251709
train gradient:  0.14664673369653192
iteration : 7506
train acc:  0.75
train loss:  0.49872887134552
train gradient:  0.12341977153211235
iteration : 7507
train acc:  0.703125
train loss:  0.5173223614692688
train gradient:  0.15748939775976167
iteration : 7508
train acc:  0.7265625
train loss:  0.4939211905002594
train gradient:  0.14246648008024398
iteration : 7509
train acc:  0.7421875
train loss:  0.5061325430870056
train gradient:  0.13566212044429782
iteration : 7510
train acc:  0.6875
train loss:  0.5500286221504211
train gradient:  0.14670163651450852
iteration : 7511
train acc:  0.765625
train loss:  0.4407941699028015
train gradient:  0.10545719087556228
iteration : 7512
train acc:  0.6875
train loss:  0.5358400344848633
train gradient:  0.14863496428908646
iteration : 7513
train acc:  0.65625
train loss:  0.5555365681648254
train gradient:  0.1458251915754027
iteration : 7514
train acc:  0.7734375
train loss:  0.4656054377555847
train gradient:  0.13944011820251329
iteration : 7515
train acc:  0.7265625
train loss:  0.4994778633117676
train gradient:  0.1359867430886427
iteration : 7516
train acc:  0.7265625
train loss:  0.5425806641578674
train gradient:  0.15254023971306852
iteration : 7517
train acc:  0.6953125
train loss:  0.5231456756591797
train gradient:  0.13312007809449036
iteration : 7518
train acc:  0.7109375
train loss:  0.5259253978729248
train gradient:  0.15135959015556671
iteration : 7519
train acc:  0.71875
train loss:  0.5765683054924011
train gradient:  0.21101659765798259
iteration : 7520
train acc:  0.8046875
train loss:  0.45913076400756836
train gradient:  0.11358636239609532
iteration : 7521
train acc:  0.7265625
train loss:  0.5484463572502136
train gradient:  0.1615014364908499
iteration : 7522
train acc:  0.765625
train loss:  0.4989146888256073
train gradient:  0.13998535116309274
iteration : 7523
train acc:  0.7578125
train loss:  0.4819253087043762
train gradient:  0.12481544550392824
iteration : 7524
train acc:  0.7578125
train loss:  0.4772797226905823
train gradient:  0.12240648567402633
iteration : 7525
train acc:  0.734375
train loss:  0.5464394092559814
train gradient:  0.16663410901669157
iteration : 7526
train acc:  0.8046875
train loss:  0.45319294929504395
train gradient:  0.11850825476524927
iteration : 7527
train acc:  0.765625
train loss:  0.47041720151901245
train gradient:  0.1243375196253521
iteration : 7528
train acc:  0.75
train loss:  0.4558153748512268
train gradient:  0.11639373975763388
iteration : 7529
train acc:  0.703125
train loss:  0.46942079067230225
train gradient:  0.11909864671989154
iteration : 7530
train acc:  0.6953125
train loss:  0.5902327299118042
train gradient:  0.20487530827754505
iteration : 7531
train acc:  0.75
train loss:  0.4549424946308136
train gradient:  0.10779043201233655
iteration : 7532
train acc:  0.7578125
train loss:  0.49574995040893555
train gradient:  0.1310087754760969
iteration : 7533
train acc:  0.7578125
train loss:  0.45385146141052246
train gradient:  0.13145391916565843
iteration : 7534
train acc:  0.7265625
train loss:  0.5097808241844177
train gradient:  0.13189457238717958
iteration : 7535
train acc:  0.7109375
train loss:  0.4782843291759491
train gradient:  0.11403352144712763
iteration : 7536
train acc:  0.7265625
train loss:  0.5220373868942261
train gradient:  0.1318299503117608
iteration : 7537
train acc:  0.6796875
train loss:  0.5441458821296692
train gradient:  0.16442244718013024
iteration : 7538
train acc:  0.7421875
train loss:  0.48882853984832764
train gradient:  0.11118742220986187
iteration : 7539
train acc:  0.765625
train loss:  0.46616101264953613
train gradient:  0.11353055282211652
iteration : 7540
train acc:  0.7265625
train loss:  0.5023951530456543
train gradient:  0.1319344063452324
iteration : 7541
train acc:  0.671875
train loss:  0.584015429019928
train gradient:  0.15971955385094494
iteration : 7542
train acc:  0.65625
train loss:  0.584777295589447
train gradient:  0.1629422660191751
iteration : 7543
train acc:  0.7578125
train loss:  0.502842903137207
train gradient:  0.15503188031090162
iteration : 7544
train acc:  0.828125
train loss:  0.40358805656433105
train gradient:  0.10748716449358516
iteration : 7545
train acc:  0.6640625
train loss:  0.5507848858833313
train gradient:  0.14852308054868063
iteration : 7546
train acc:  0.7421875
train loss:  0.46162158250808716
train gradient:  0.12680106458980528
iteration : 7547
train acc:  0.71875
train loss:  0.5300290584564209
train gradient:  0.1641219184510594
iteration : 7548
train acc:  0.7265625
train loss:  0.5215024948120117
train gradient:  0.12936508068506425
iteration : 7549
train acc:  0.7421875
train loss:  0.5401986837387085
train gradient:  0.14009052325703253
iteration : 7550
train acc:  0.6484375
train loss:  0.5852509140968323
train gradient:  0.21204206092073002
iteration : 7551
train acc:  0.6953125
train loss:  0.551591157913208
train gradient:  0.17549437444074759
iteration : 7552
train acc:  0.7578125
train loss:  0.4850163161754608
train gradient:  0.17989428006323818
iteration : 7553
train acc:  0.765625
train loss:  0.4296005368232727
train gradient:  0.10653956974143669
iteration : 7554
train acc:  0.703125
train loss:  0.5349152088165283
train gradient:  0.15800809576599445
iteration : 7555
train acc:  0.6640625
train loss:  0.5501732230186462
train gradient:  0.19017007934226354
iteration : 7556
train acc:  0.7734375
train loss:  0.4882609248161316
train gradient:  0.15621899106016207
iteration : 7557
train acc:  0.7109375
train loss:  0.5202016830444336
train gradient:  0.16312510418811332
iteration : 7558
train acc:  0.8125
train loss:  0.4372129440307617
train gradient:  0.10576025154825891
iteration : 7559
train acc:  0.7734375
train loss:  0.4239445626735687
train gradient:  0.11487788796232046
iteration : 7560
train acc:  0.75
train loss:  0.5453501343727112
train gradient:  0.16161199506365648
iteration : 7561
train acc:  0.6796875
train loss:  0.5211793184280396
train gradient:  0.13410820722369865
iteration : 7562
train acc:  0.71875
train loss:  0.5540754795074463
train gradient:  0.1511786536174267
iteration : 7563
train acc:  0.671875
train loss:  0.545879602432251
train gradient:  0.18799189887827972
iteration : 7564
train acc:  0.734375
train loss:  0.49727576971054077
train gradient:  0.14737279149946614
iteration : 7565
train acc:  0.71875
train loss:  0.5269877314567566
train gradient:  0.1371032770248075
iteration : 7566
train acc:  0.75
train loss:  0.47319677472114563
train gradient:  0.13776275878387687
iteration : 7567
train acc:  0.75
train loss:  0.5110357999801636
train gradient:  0.14959261086000225
iteration : 7568
train acc:  0.796875
train loss:  0.4630180299282074
train gradient:  0.10400660368240859
iteration : 7569
train acc:  0.71875
train loss:  0.5299980640411377
train gradient:  0.18128836890026362
iteration : 7570
train acc:  0.71875
train loss:  0.5152326822280884
train gradient:  0.11895855289297147
iteration : 7571
train acc:  0.6640625
train loss:  0.5806018114089966
train gradient:  0.17422228026274264
iteration : 7572
train acc:  0.7421875
train loss:  0.49236345291137695
train gradient:  0.13651196994525758
iteration : 7573
train acc:  0.7421875
train loss:  0.4786264896392822
train gradient:  0.120218746617331
iteration : 7574
train acc:  0.734375
train loss:  0.5011643171310425
train gradient:  0.12099356930359435
iteration : 7575
train acc:  0.7421875
train loss:  0.47262245416641235
train gradient:  0.11269828817686756
iteration : 7576
train acc:  0.6484375
train loss:  0.6003212928771973
train gradient:  0.2094925101771846
iteration : 7577
train acc:  0.7265625
train loss:  0.5343046188354492
train gradient:  0.1326843631067211
iteration : 7578
train acc:  0.6953125
train loss:  0.47859594225883484
train gradient:  0.13313259864786603
iteration : 7579
train acc:  0.734375
train loss:  0.48431164026260376
train gradient:  0.11928142594983702
iteration : 7580
train acc:  0.7578125
train loss:  0.4468763470649719
train gradient:  0.12205352113567984
iteration : 7581
train acc:  0.7578125
train loss:  0.502778172492981
train gradient:  0.14792005621756693
iteration : 7582
train acc:  0.7734375
train loss:  0.48604732751846313
train gradient:  0.1741766059789291
iteration : 7583
train acc:  0.75
train loss:  0.4570874273777008
train gradient:  0.1152656383949639
iteration : 7584
train acc:  0.6328125
train loss:  0.5759458541870117
train gradient:  0.14781306860296123
iteration : 7585
train acc:  0.734375
train loss:  0.528770923614502
train gradient:  0.17586533601726617
iteration : 7586
train acc:  0.7421875
train loss:  0.47217223048210144
train gradient:  0.11385724322208303
iteration : 7587
train acc:  0.7109375
train loss:  0.5683446526527405
train gradient:  0.18626793406190173
iteration : 7588
train acc:  0.7578125
train loss:  0.48453289270401
train gradient:  0.13785055784318778
iteration : 7589
train acc:  0.734375
train loss:  0.5132992267608643
train gradient:  0.1556973432660304
iteration : 7590
train acc:  0.6640625
train loss:  0.5293506979942322
train gradient:  0.13632740697450502
iteration : 7591
train acc:  0.7421875
train loss:  0.4606538712978363
train gradient:  0.1383350789222393
iteration : 7592
train acc:  0.7265625
train loss:  0.5155812501907349
train gradient:  0.1378359921101549
iteration : 7593
train acc:  0.7734375
train loss:  0.4364302158355713
train gradient:  0.09388682814529048
iteration : 7594
train acc:  0.828125
train loss:  0.4124627113342285
train gradient:  0.14142854018407586
iteration : 7595
train acc:  0.7421875
train loss:  0.4527219235897064
train gradient:  0.09748146179158915
iteration : 7596
train acc:  0.7421875
train loss:  0.48514753580093384
train gradient:  0.16270821229134186
iteration : 7597
train acc:  0.734375
train loss:  0.5067325234413147
train gradient:  0.16287182939341044
iteration : 7598
train acc:  0.7734375
train loss:  0.5130411386489868
train gradient:  0.1291735079314233
iteration : 7599
train acc:  0.7109375
train loss:  0.5186600685119629
train gradient:  0.1691362506833044
iteration : 7600
train acc:  0.7578125
train loss:  0.47263389825820923
train gradient:  0.15238936781549844
iteration : 7601
train acc:  0.6953125
train loss:  0.5681449770927429
train gradient:  0.1931812900034673
iteration : 7602
train acc:  0.7421875
train loss:  0.5100858211517334
train gradient:  0.1325650077155037
iteration : 7603
train acc:  0.6875
train loss:  0.5542385578155518
train gradient:  0.1598129645697377
iteration : 7604
train acc:  0.6796875
train loss:  0.5668467879295349
train gradient:  0.19172119115392222
iteration : 7605
train acc:  0.7421875
train loss:  0.49791717529296875
train gradient:  0.11513112408437154
iteration : 7606
train acc:  0.75
train loss:  0.4874994158744812
train gradient:  0.12162044564095485
iteration : 7607
train acc:  0.734375
train loss:  0.46524477005004883
train gradient:  0.11763780905429326
iteration : 7608
train acc:  0.71875
train loss:  0.5016224980354309
train gradient:  0.13457644921486855
iteration : 7609
train acc:  0.7421875
train loss:  0.5012069940567017
train gradient:  0.11928611129533825
iteration : 7610
train acc:  0.71875
train loss:  0.4877210259437561
train gradient:  0.14170546676953938
iteration : 7611
train acc:  0.7109375
train loss:  0.5289863348007202
train gradient:  0.13278134927771396
iteration : 7612
train acc:  0.6875
train loss:  0.5244543552398682
train gradient:  0.2103622515880867
iteration : 7613
train acc:  0.75
train loss:  0.4943344295024872
train gradient:  0.1168405363406777
iteration : 7614
train acc:  0.8203125
train loss:  0.45901918411254883
train gradient:  0.12350772590714651
iteration : 7615
train acc:  0.796875
train loss:  0.44571125507354736
train gradient:  0.13097001380336828
iteration : 7616
train acc:  0.6953125
train loss:  0.5240277647972107
train gradient:  0.13592489454058465
iteration : 7617
train acc:  0.7109375
train loss:  0.5296777486801147
train gradient:  0.1719516272748526
iteration : 7618
train acc:  0.7265625
train loss:  0.4797890782356262
train gradient:  0.13946122955147577
iteration : 7619
train acc:  0.7890625
train loss:  0.46916529536247253
train gradient:  0.12287259355007774
iteration : 7620
train acc:  0.734375
train loss:  0.5208494663238525
train gradient:  0.1694833328415587
iteration : 7621
train acc:  0.734375
train loss:  0.5054169297218323
train gradient:  0.15351112932708372
iteration : 7622
train acc:  0.765625
train loss:  0.4606444835662842
train gradient:  0.14429863882379335
iteration : 7623
train acc:  0.703125
train loss:  0.524101972579956
train gradient:  0.12186052871551675
iteration : 7624
train acc:  0.7109375
train loss:  0.5412622690200806
train gradient:  0.19179211507951693
iteration : 7625
train acc:  0.75
train loss:  0.46111103892326355
train gradient:  0.1379260458145357
iteration : 7626
train acc:  0.765625
train loss:  0.4426915645599365
train gradient:  0.1170486143765991
iteration : 7627
train acc:  0.6640625
train loss:  0.5302850008010864
train gradient:  0.14732334133838226
iteration : 7628
train acc:  0.6875
train loss:  0.5406041145324707
train gradient:  0.1462758934522839
iteration : 7629
train acc:  0.6953125
train loss:  0.5905535817146301
train gradient:  0.1835182487035198
iteration : 7630
train acc:  0.703125
train loss:  0.5061860680580139
train gradient:  0.15022500867358501
iteration : 7631
train acc:  0.765625
train loss:  0.5035846829414368
train gradient:  0.21887746742702854
iteration : 7632
train acc:  0.7265625
train loss:  0.5075197219848633
train gradient:  0.18023460561003726
iteration : 7633
train acc:  0.7421875
train loss:  0.47770923376083374
train gradient:  0.1379980221041542
iteration : 7634
train acc:  0.75
train loss:  0.4937124252319336
train gradient:  0.14339826079535611
iteration : 7635
train acc:  0.7109375
train loss:  0.502213716506958
train gradient:  0.13377648759493208
iteration : 7636
train acc:  0.734375
train loss:  0.5146417617797852
train gradient:  0.15356482491417145
iteration : 7637
train acc:  0.6640625
train loss:  0.5552951097488403
train gradient:  0.20763969436639113
iteration : 7638
train acc:  0.796875
train loss:  0.4408425986766815
train gradient:  0.11783879881375252
iteration : 7639
train acc:  0.7265625
train loss:  0.5021942257881165
train gradient:  0.1500985641239898
iteration : 7640
train acc:  0.734375
train loss:  0.5172555446624756
train gradient:  0.11606535815720186
iteration : 7641
train acc:  0.671875
train loss:  0.5603574514389038
train gradient:  0.18561693331039342
iteration : 7642
train acc:  0.71875
train loss:  0.5380892157554626
train gradient:  0.20645344970501817
iteration : 7643
train acc:  0.78125
train loss:  0.4420737624168396
train gradient:  0.09039632003633429
iteration : 7644
train acc:  0.7421875
train loss:  0.4897603690624237
train gradient:  0.1287815220983014
iteration : 7645
train acc:  0.625
train loss:  0.6388669610023499
train gradient:  0.21260001400859863
iteration : 7646
train acc:  0.71875
train loss:  0.503216028213501
train gradient:  0.14097845135671405
iteration : 7647
train acc:  0.7578125
train loss:  0.469646155834198
train gradient:  0.13089007872257083
iteration : 7648
train acc:  0.7734375
train loss:  0.431906133890152
train gradient:  0.08626572629113012
iteration : 7649
train acc:  0.7421875
train loss:  0.525004506111145
train gradient:  0.1366510059128573
iteration : 7650
train acc:  0.6953125
train loss:  0.5665615797042847
train gradient:  0.18627741180499954
iteration : 7651
train acc:  0.671875
train loss:  0.6169363260269165
train gradient:  0.18066644393753006
iteration : 7652
train acc:  0.7578125
train loss:  0.5041531324386597
train gradient:  0.1361108722099959
iteration : 7653
train acc:  0.75
train loss:  0.49311745166778564
train gradient:  0.1635022623064708
iteration : 7654
train acc:  0.71875
train loss:  0.5476006269454956
train gradient:  0.17106718054970654
iteration : 7655
train acc:  0.734375
train loss:  0.5248434543609619
train gradient:  0.14982583557200002
iteration : 7656
train acc:  0.7734375
train loss:  0.48378217220306396
train gradient:  0.12693397751021251
iteration : 7657
train acc:  0.7421875
train loss:  0.5080404877662659
train gradient:  0.12437431349454856
iteration : 7658
train acc:  0.71875
train loss:  0.5327922105789185
train gradient:  0.1561090741460004
iteration : 7659
train acc:  0.734375
train loss:  0.49751365184783936
train gradient:  0.1201382605580229
iteration : 7660
train acc:  0.78125
train loss:  0.4551061987876892
train gradient:  0.13678729135426465
iteration : 7661
train acc:  0.828125
train loss:  0.42911839485168457
train gradient:  0.10513854433283283
iteration : 7662
train acc:  0.7734375
train loss:  0.4711456298828125
train gradient:  0.12321433546319958
iteration : 7663
train acc:  0.8046875
train loss:  0.4435293674468994
train gradient:  0.14400499791329902
iteration : 7664
train acc:  0.7890625
train loss:  0.4372848570346832
train gradient:  0.09694567038621317
iteration : 7665
train acc:  0.7109375
train loss:  0.5035643577575684
train gradient:  0.12824668652057686
iteration : 7666
train acc:  0.703125
train loss:  0.5583949089050293
train gradient:  0.1519013193811617
iteration : 7667
train acc:  0.7421875
train loss:  0.5216699838638306
train gradient:  0.14644720170622078
iteration : 7668
train acc:  0.7578125
train loss:  0.48905184864997864
train gradient:  0.16721182933693873
iteration : 7669
train acc:  0.8203125
train loss:  0.41708099842071533
train gradient:  0.11361685529669355
iteration : 7670
train acc:  0.7421875
train loss:  0.5303632616996765
train gradient:  0.14983611216600995
iteration : 7671
train acc:  0.8046875
train loss:  0.4113668203353882
train gradient:  0.11743291070950018
iteration : 7672
train acc:  0.7734375
train loss:  0.47282809019088745
train gradient:  0.11129392157334489
iteration : 7673
train acc:  0.7265625
train loss:  0.5571519136428833
train gradient:  0.14018079306582265
iteration : 7674
train acc:  0.7421875
train loss:  0.4992513060569763
train gradient:  0.12382011851304409
iteration : 7675
train acc:  0.7421875
train loss:  0.48659825325012207
train gradient:  0.10576567398444771
iteration : 7676
train acc:  0.7265625
train loss:  0.4766467809677124
train gradient:  0.11179127833106897
iteration : 7677
train acc:  0.7109375
train loss:  0.6513480544090271
train gradient:  0.2076766917604026
iteration : 7678
train acc:  0.71875
train loss:  0.5312026143074036
train gradient:  0.21109494755021507
iteration : 7679
train acc:  0.6640625
train loss:  0.6029179692268372
train gradient:  0.15769546451490674
iteration : 7680
train acc:  0.7578125
train loss:  0.4462762475013733
train gradient:  0.1210323932502691
iteration : 7681
train acc:  0.7421875
train loss:  0.4843640923500061
train gradient:  0.17346079936995706
iteration : 7682
train acc:  0.8046875
train loss:  0.4267754852771759
train gradient:  0.1060936173407771
iteration : 7683
train acc:  0.7265625
train loss:  0.4725108742713928
train gradient:  0.12805706925711113
iteration : 7684
train acc:  0.7265625
train loss:  0.4688248038291931
train gradient:  0.14091513483392726
iteration : 7685
train acc:  0.7890625
train loss:  0.48088985681533813
train gradient:  0.11841492962734733
iteration : 7686
train acc:  0.734375
train loss:  0.48140379786491394
train gradient:  0.14993544849755658
iteration : 7687
train acc:  0.7265625
train loss:  0.49053093791007996
train gradient:  0.12860551643442858
iteration : 7688
train acc:  0.734375
train loss:  0.5011839866638184
train gradient:  0.15000763802168263
iteration : 7689
train acc:  0.7265625
train loss:  0.5273333787918091
train gradient:  0.15424027850459232
iteration : 7690
train acc:  0.71875
train loss:  0.4667809307575226
train gradient:  0.13157104073732087
iteration : 7691
train acc:  0.7265625
train loss:  0.5435394048690796
train gradient:  0.15173156964791162
iteration : 7692
train acc:  0.7734375
train loss:  0.46643418073654175
train gradient:  0.136727648643184
iteration : 7693
train acc:  0.84375
train loss:  0.4287877082824707
train gradient:  0.1035121311254021
iteration : 7694
train acc:  0.7890625
train loss:  0.48011907935142517
train gradient:  0.12370166534532194
iteration : 7695
train acc:  0.6953125
train loss:  0.5580381155014038
train gradient:  0.16322144782490605
iteration : 7696
train acc:  0.6484375
train loss:  0.6043643355369568
train gradient:  0.20555527225685827
iteration : 7697
train acc:  0.7578125
train loss:  0.4823150634765625
train gradient:  0.13339775122894137
iteration : 7698
train acc:  0.7578125
train loss:  0.5039283633232117
train gradient:  0.1378613608157851
iteration : 7699
train acc:  0.75
train loss:  0.4346499741077423
train gradient:  0.10221168700354512
iteration : 7700
train acc:  0.78125
train loss:  0.48115330934524536
train gradient:  0.12403507869139681
iteration : 7701
train acc:  0.7109375
train loss:  0.58049476146698
train gradient:  0.21112872257332455
iteration : 7702
train acc:  0.6953125
train loss:  0.5368053317070007
train gradient:  0.1755234415130834
iteration : 7703
train acc:  0.7734375
train loss:  0.4231204390525818
train gradient:  0.08849928671695409
iteration : 7704
train acc:  0.6875
train loss:  0.5667963027954102
train gradient:  0.1660820472837493
iteration : 7705
train acc:  0.75
train loss:  0.4917446970939636
train gradient:  0.13217229271912162
iteration : 7706
train acc:  0.734375
train loss:  0.5381580591201782
train gradient:  0.14359684683149593
iteration : 7707
train acc:  0.703125
train loss:  0.5555276870727539
train gradient:  0.15555012259862522
iteration : 7708
train acc:  0.7421875
train loss:  0.4857473075389862
train gradient:  0.11992366106256686
iteration : 7709
train acc:  0.734375
train loss:  0.5166338682174683
train gradient:  0.15764588384545686
iteration : 7710
train acc:  0.6875
train loss:  0.5814394354820251
train gradient:  0.1753091477539767
iteration : 7711
train acc:  0.7578125
train loss:  0.5009059906005859
train gradient:  0.1319290740872111
iteration : 7712
train acc:  0.65625
train loss:  0.6077597141265869
train gradient:  0.17078229045230914
iteration : 7713
train acc:  0.703125
train loss:  0.5807595252990723
train gradient:  0.17308053260269596
iteration : 7714
train acc:  0.7265625
train loss:  0.5314493775367737
train gradient:  0.1799896157555132
iteration : 7715
train acc:  0.6875
train loss:  0.5588425993919373
train gradient:  0.13428930708739872
iteration : 7716
train acc:  0.671875
train loss:  0.6104552745819092
train gradient:  0.18561876069651712
iteration : 7717
train acc:  0.7734375
train loss:  0.46690142154693604
train gradient:  0.12099312587940218
iteration : 7718
train acc:  0.7109375
train loss:  0.5385661125183105
train gradient:  0.1560397111765085
iteration : 7719
train acc:  0.75
train loss:  0.46221354603767395
train gradient:  0.10034847174150592
iteration : 7720
train acc:  0.7890625
train loss:  0.459141343832016
train gradient:  0.12880060537498617
iteration : 7721
train acc:  0.734375
train loss:  0.49361729621887207
train gradient:  0.20850653653523746
iteration : 7722
train acc:  0.6875
train loss:  0.5340499877929688
train gradient:  0.17249809752984807
iteration : 7723
train acc:  0.7265625
train loss:  0.5258016586303711
train gradient:  0.17158692734684233
iteration : 7724
train acc:  0.6640625
train loss:  0.5579826235771179
train gradient:  0.1585498654102803
iteration : 7725
train acc:  0.7578125
train loss:  0.4987422227859497
train gradient:  0.15692670546152032
iteration : 7726
train acc:  0.7578125
train loss:  0.4773143529891968
train gradient:  0.1278293766831722
iteration : 7727
train acc:  0.703125
train loss:  0.5100182294845581
train gradient:  0.13743708914201974
iteration : 7728
train acc:  0.765625
train loss:  0.4873732924461365
train gradient:  0.11316464729471676
iteration : 7729
train acc:  0.765625
train loss:  0.4443952143192291
train gradient:  0.10651829853605137
iteration : 7730
train acc:  0.7109375
train loss:  0.49901747703552246
train gradient:  0.12674869800670657
iteration : 7731
train acc:  0.7578125
train loss:  0.503863513469696
train gradient:  0.15222839837722751
iteration : 7732
train acc:  0.6640625
train loss:  0.548033595085144
train gradient:  0.16696878218015554
iteration : 7733
train acc:  0.6953125
train loss:  0.5039986371994019
train gradient:  0.1333810823901686
iteration : 7734
train acc:  0.75
train loss:  0.5215851068496704
train gradient:  0.13667470869951015
iteration : 7735
train acc:  0.734375
train loss:  0.49862247705459595
train gradient:  0.15217181708553285
iteration : 7736
train acc:  0.640625
train loss:  0.5654016137123108
train gradient:  0.17001479764594102
iteration : 7737
train acc:  0.6796875
train loss:  0.5856160521507263
train gradient:  0.16093092025817207
iteration : 7738
train acc:  0.84375
train loss:  0.4005151093006134
train gradient:  0.08301581284232863
iteration : 7739
train acc:  0.71875
train loss:  0.4945104420185089
train gradient:  0.1343720974034472
iteration : 7740
train acc:  0.7578125
train loss:  0.4652944803237915
train gradient:  0.13752788199815016
iteration : 7741
train acc:  0.734375
train loss:  0.5254725217819214
train gradient:  0.12979559372210236
iteration : 7742
train acc:  0.765625
train loss:  0.46032941341400146
train gradient:  0.10134967124929894
iteration : 7743
train acc:  0.8125
train loss:  0.41852638125419617
train gradient:  0.11067405768390341
iteration : 7744
train acc:  0.6953125
train loss:  0.5605549812316895
train gradient:  0.1790012233319237
iteration : 7745
train acc:  0.734375
train loss:  0.49123233556747437
train gradient:  0.13075564901507797
iteration : 7746
train acc:  0.6953125
train loss:  0.5435635447502136
train gradient:  0.16429627892301873
iteration : 7747
train acc:  0.6640625
train loss:  0.5683964490890503
train gradient:  0.17950383596106373
iteration : 7748
train acc:  0.75
train loss:  0.4959298074245453
train gradient:  0.12495365668206385
iteration : 7749
train acc:  0.71875
train loss:  0.528580904006958
train gradient:  0.127669912815432
iteration : 7750
train acc:  0.734375
train loss:  0.48758336901664734
train gradient:  0.12433754964365984
iteration : 7751
train acc:  0.625
train loss:  0.6356152892112732
train gradient:  0.22154762971540992
iteration : 7752
train acc:  0.7421875
train loss:  0.45875635743141174
train gradient:  0.14096771033578293
iteration : 7753
train acc:  0.6796875
train loss:  0.5070945024490356
train gradient:  0.15953229421977355
iteration : 7754
train acc:  0.6875
train loss:  0.5615617036819458
train gradient:  0.15977768696958217
iteration : 7755
train acc:  0.7109375
train loss:  0.48150762915611267
train gradient:  0.1186072271737512
iteration : 7756
train acc:  0.7421875
train loss:  0.5106998682022095
train gradient:  0.136121573266069
iteration : 7757
train acc:  0.7421875
train loss:  0.5003552436828613
train gradient:  0.1191500683159869
iteration : 7758
train acc:  0.6796875
train loss:  0.5611557960510254
train gradient:  0.18543199595001136
iteration : 7759
train acc:  0.671875
train loss:  0.5970908403396606
train gradient:  0.14663956444317783
iteration : 7760
train acc:  0.703125
train loss:  0.5304879546165466
train gradient:  0.13893189017434354
iteration : 7761
train acc:  0.7734375
train loss:  0.5416697859764099
train gradient:  0.14398169883357026
iteration : 7762
train acc:  0.671875
train loss:  0.5551496744155884
train gradient:  0.18180523710881918
iteration : 7763
train acc:  0.6953125
train loss:  0.5618236064910889
train gradient:  0.15191164337572416
iteration : 7764
train acc:  0.7265625
train loss:  0.4828604459762573
train gradient:  0.14521449873499245
iteration : 7765
train acc:  0.7265625
train loss:  0.5078458189964294
train gradient:  0.10812826847336664
iteration : 7766
train acc:  0.71875
train loss:  0.5619562864303589
train gradient:  0.18863570513466715
iteration : 7767
train acc:  0.734375
train loss:  0.4998725652694702
train gradient:  0.1800555472444597
iteration : 7768
train acc:  0.6953125
train loss:  0.5348684787750244
train gradient:  0.15283521942101924
iteration : 7769
train acc:  0.75
train loss:  0.4761474132537842
train gradient:  0.11682770386183251
iteration : 7770
train acc:  0.765625
train loss:  0.4832106828689575
train gradient:  0.14782400604821438
iteration : 7771
train acc:  0.625
train loss:  0.6116975545883179
train gradient:  0.1925460635762638
iteration : 7772
train acc:  0.7265625
train loss:  0.4823291301727295
train gradient:  0.12313345858055863
iteration : 7773
train acc:  0.78125
train loss:  0.4806516468524933
train gradient:  0.12344243252115332
iteration : 7774
train acc:  0.75
train loss:  0.4988141655921936
train gradient:  0.1477680189882238
iteration : 7775
train acc:  0.796875
train loss:  0.4705103039741516
train gradient:  0.1215478644303329
iteration : 7776
train acc:  0.703125
train loss:  0.5837140083312988
train gradient:  0.2097202508932533
iteration : 7777
train acc:  0.8046875
train loss:  0.43958792090415955
train gradient:  0.10875742161130614
iteration : 7778
train acc:  0.796875
train loss:  0.49352747201919556
train gradient:  0.16566959885156776
iteration : 7779
train acc:  0.6328125
train loss:  0.6692250967025757
train gradient:  0.2078872364866317
iteration : 7780
train acc:  0.7578125
train loss:  0.4896853566169739
train gradient:  0.13915551250546584
iteration : 7781
train acc:  0.6953125
train loss:  0.5069657564163208
train gradient:  0.12010706439536749
iteration : 7782
train acc:  0.75
train loss:  0.4814237654209137
train gradient:  0.10042272132810467
iteration : 7783
train acc:  0.6953125
train loss:  0.5176255702972412
train gradient:  0.15780989335414985
iteration : 7784
train acc:  0.7890625
train loss:  0.5100618004798889
train gradient:  0.153694291505284
iteration : 7785
train acc:  0.640625
train loss:  0.5261468291282654
train gradient:  0.1778019670004202
iteration : 7786
train acc:  0.765625
train loss:  0.4638950228691101
train gradient:  0.11177374979369563
iteration : 7787
train acc:  0.7109375
train loss:  0.5552343130111694
train gradient:  0.14501894043685531
iteration : 7788
train acc:  0.75
train loss:  0.4899076223373413
train gradient:  0.1384228141105744
iteration : 7789
train acc:  0.796875
train loss:  0.46179792284965515
train gradient:  0.1680066484990198
iteration : 7790
train acc:  0.765625
train loss:  0.44259709119796753
train gradient:  0.12439424093320738
iteration : 7791
train acc:  0.7109375
train loss:  0.548747181892395
train gradient:  0.15697977311627545
iteration : 7792
train acc:  0.75
train loss:  0.4803485870361328
train gradient:  0.11081364302522367
iteration : 7793
train acc:  0.6796875
train loss:  0.5970037579536438
train gradient:  0.24807781975590182
iteration : 7794
train acc:  0.6484375
train loss:  0.5337510704994202
train gradient:  0.1652006771238767
iteration : 7795
train acc:  0.71875
train loss:  0.5133233070373535
train gradient:  0.14177333704816286
iteration : 7796
train acc:  0.734375
train loss:  0.4971652626991272
train gradient:  0.14761898410816499
iteration : 7797
train acc:  0.7578125
train loss:  0.48626697063446045
train gradient:  0.11588973675792555
iteration : 7798
train acc:  0.7421875
train loss:  0.4808233678340912
train gradient:  0.1359421823389748
iteration : 7799
train acc:  0.7734375
train loss:  0.4836658239364624
train gradient:  0.14098554490574264
iteration : 7800
train acc:  0.7890625
train loss:  0.4389796257019043
train gradient:  0.09793758807123018
iteration : 7801
train acc:  0.7421875
train loss:  0.5282096862792969
train gradient:  0.2091348590682623
iteration : 7802
train acc:  0.71875
train loss:  0.5391675233840942
train gradient:  0.1322378640961284
iteration : 7803
train acc:  0.7734375
train loss:  0.49077531695365906
train gradient:  0.12968233649865601
iteration : 7804
train acc:  0.7578125
train loss:  0.5128633975982666
train gradient:  0.12311670342358326
iteration : 7805
train acc:  0.7578125
train loss:  0.473072350025177
train gradient:  0.1512109420152179
iteration : 7806
train acc:  0.75
train loss:  0.5137032270431519
train gradient:  0.13159452579953862
iteration : 7807
train acc:  0.765625
train loss:  0.5244856476783752
train gradient:  0.14241143335402665
iteration : 7808
train acc:  0.7890625
train loss:  0.4803965091705322
train gradient:  0.13991930494056243
iteration : 7809
train acc:  0.734375
train loss:  0.5236994028091431
train gradient:  0.14157663024661893
iteration : 7810
train acc:  0.734375
train loss:  0.49026015400886536
train gradient:  0.13948182619543392
iteration : 7811
train acc:  0.8046875
train loss:  0.4346310496330261
train gradient:  0.1081350332926687
iteration : 7812
train acc:  0.7265625
train loss:  0.5066052675247192
train gradient:  0.14240919570197752
iteration : 7813
train acc:  0.78125
train loss:  0.48104414343833923
train gradient:  0.12616126991505977
iteration : 7814
train acc:  0.78125
train loss:  0.48206543922424316
train gradient:  0.16562613169662016
iteration : 7815
train acc:  0.7734375
train loss:  0.4739816188812256
train gradient:  0.15020701069584264
iteration : 7816
train acc:  0.7109375
train loss:  0.5290483832359314
train gradient:  0.13406768826109863
iteration : 7817
train acc:  0.7578125
train loss:  0.4786142110824585
train gradient:  0.11681496405128677
iteration : 7818
train acc:  0.8046875
train loss:  0.450056254863739
train gradient:  0.09155103162245849
iteration : 7819
train acc:  0.734375
train loss:  0.5308550596237183
train gradient:  0.12925299588248407
iteration : 7820
train acc:  0.71875
train loss:  0.496362566947937
train gradient:  0.11954526721872391
iteration : 7821
train acc:  0.7421875
train loss:  0.439403235912323
train gradient:  0.10636982719991275
iteration : 7822
train acc:  0.6953125
train loss:  0.5311926603317261
train gradient:  0.130501050207518
iteration : 7823
train acc:  0.7578125
train loss:  0.5121800303459167
train gradient:  0.1359869543882368
iteration : 7824
train acc:  0.8203125
train loss:  0.4401770234107971
train gradient:  0.11496710898764309
iteration : 7825
train acc:  0.65625
train loss:  0.5780449509620667
train gradient:  0.14603760474046096
iteration : 7826
train acc:  0.7421875
train loss:  0.5041072964668274
train gradient:  0.14640123384217252
iteration : 7827
train acc:  0.71875
train loss:  0.46689581871032715
train gradient:  0.12172841460029495
iteration : 7828
train acc:  0.7265625
train loss:  0.485548198223114
train gradient:  0.10323561413916234
iteration : 7829
train acc:  0.7421875
train loss:  0.5038965940475464
train gradient:  0.167876068635612
iteration : 7830
train acc:  0.75
train loss:  0.44582152366638184
train gradient:  0.1291531211604287
iteration : 7831
train acc:  0.71875
train loss:  0.51576828956604
train gradient:  0.12646523590641945
iteration : 7832
train acc:  0.7421875
train loss:  0.5098481774330139
train gradient:  0.23776066412235708
iteration : 7833
train acc:  0.7734375
train loss:  0.49425801634788513
train gradient:  0.13907116369153133
iteration : 7834
train acc:  0.71875
train loss:  0.5070290565490723
train gradient:  0.15086878512058166
iteration : 7835
train acc:  0.7734375
train loss:  0.45481374859809875
train gradient:  0.1405410924749531
iteration : 7836
train acc:  0.7421875
train loss:  0.46609145402908325
train gradient:  0.12253109400940239
iteration : 7837
train acc:  0.734375
train loss:  0.5090065002441406
train gradient:  0.15408211272394487
iteration : 7838
train acc:  0.7421875
train loss:  0.49876976013183594
train gradient:  0.14726517843077114
iteration : 7839
train acc:  0.703125
train loss:  0.5611796379089355
train gradient:  0.13961653812132585
iteration : 7840
train acc:  0.734375
train loss:  0.5092138051986694
train gradient:  0.14162506819696208
iteration : 7841
train acc:  0.734375
train loss:  0.4938770532608032
train gradient:  0.13675221535356719
iteration : 7842
train acc:  0.625
train loss:  0.5891194939613342
train gradient:  0.16193790382056444
iteration : 7843
train acc:  0.7265625
train loss:  0.5004992485046387
train gradient:  0.1350605968671202
iteration : 7844
train acc:  0.7578125
train loss:  0.4818439483642578
train gradient:  0.1326527679551283
iteration : 7845
train acc:  0.734375
train loss:  0.4620479345321655
train gradient:  0.12706197065535244
iteration : 7846
train acc:  0.734375
train loss:  0.5383678674697876
train gradient:  0.16737936107811024
iteration : 7847
train acc:  0.7109375
train loss:  0.5989396572113037
train gradient:  0.186939837739934
iteration : 7848
train acc:  0.7421875
train loss:  0.5443763732910156
train gradient:  0.16857582685930778
iteration : 7849
train acc:  0.71875
train loss:  0.49399763345718384
train gradient:  0.14057296820027831
iteration : 7850
train acc:  0.703125
train loss:  0.5098543167114258
train gradient:  0.15329163828077857
iteration : 7851
train acc:  0.734375
train loss:  0.4883861541748047
train gradient:  0.12228246862388376
iteration : 7852
train acc:  0.6796875
train loss:  0.5194048285484314
train gradient:  0.12893722195397292
iteration : 7853
train acc:  0.703125
train loss:  0.5383142828941345
train gradient:  0.13462592990488892
iteration : 7854
train acc:  0.8125
train loss:  0.4479149580001831
train gradient:  0.11766287046412084
iteration : 7855
train acc:  0.7421875
train loss:  0.5221750736236572
train gradient:  0.16064530899248253
iteration : 7856
train acc:  0.734375
train loss:  0.5242106914520264
train gradient:  0.1242686774664752
iteration : 7857
train acc:  0.7109375
train loss:  0.5397710204124451
train gradient:  0.1436525250404655
iteration : 7858
train acc:  0.7734375
train loss:  0.4998379349708557
train gradient:  0.13916626312018698
iteration : 7859
train acc:  0.6875
train loss:  0.542646050453186
train gradient:  0.1321176204430987
iteration : 7860
train acc:  0.703125
train loss:  0.5446531772613525
train gradient:  0.1323738054271755
iteration : 7861
train acc:  0.7734375
train loss:  0.49105748534202576
train gradient:  0.12676434873184025
iteration : 7862
train acc:  0.7421875
train loss:  0.4930437505245209
train gradient:  0.13292787979150766
iteration : 7863
train acc:  0.7734375
train loss:  0.44236525893211365
train gradient:  0.12668813756329264
iteration : 7864
train acc:  0.75
train loss:  0.5157932043075562
train gradient:  0.13510988442280977
iteration : 7865
train acc:  0.7421875
train loss:  0.4590476155281067
train gradient:  0.11510330766143066
iteration : 7866
train acc:  0.7421875
train loss:  0.5269680023193359
train gradient:  0.1552135048543843
iteration : 7867
train acc:  0.8203125
train loss:  0.42615702748298645
train gradient:  0.1006884305891789
iteration : 7868
train acc:  0.7578125
train loss:  0.47800102829933167
train gradient:  0.10677481795818906
iteration : 7869
train acc:  0.7890625
train loss:  0.42386651039123535
train gradient:  0.09006064606051006
iteration : 7870
train acc:  0.734375
train loss:  0.4793834686279297
train gradient:  0.1298131309250215
iteration : 7871
train acc:  0.7421875
train loss:  0.5320713520050049
train gradient:  0.12078341586798716
iteration : 7872
train acc:  0.7109375
train loss:  0.4860217273235321
train gradient:  0.09631561739577814
iteration : 7873
train acc:  0.765625
train loss:  0.480683296918869
train gradient:  0.11333346863527775
iteration : 7874
train acc:  0.71875
train loss:  0.4997161030769348
train gradient:  0.14322643830365622
iteration : 7875
train acc:  0.7578125
train loss:  0.4849657416343689
train gradient:  0.15878608627691038
iteration : 7876
train acc:  0.703125
train loss:  0.5450690388679504
train gradient:  0.1660815797636011
iteration : 7877
train acc:  0.78125
train loss:  0.45956936478614807
train gradient:  0.11537721512602403
iteration : 7878
train acc:  0.734375
train loss:  0.5167431235313416
train gradient:  0.11755307227170476
iteration : 7879
train acc:  0.6640625
train loss:  0.567649245262146
train gradient:  0.16282072366181355
iteration : 7880
train acc:  0.734375
train loss:  0.4991520643234253
train gradient:  0.16384193640455597
iteration : 7881
train acc:  0.6953125
train loss:  0.5400098562240601
train gradient:  0.17395165623663672
iteration : 7882
train acc:  0.7421875
train loss:  0.469149649143219
train gradient:  0.11684692114368764
iteration : 7883
train acc:  0.7265625
train loss:  0.5089696049690247
train gradient:  0.15754122760402722
iteration : 7884
train acc:  0.7578125
train loss:  0.45124828815460205
train gradient:  0.09692954758926227
iteration : 7885
train acc:  0.7265625
train loss:  0.5219615697860718
train gradient:  0.13356310645645886
iteration : 7886
train acc:  0.7265625
train loss:  0.5596208572387695
train gradient:  0.16794800637956
iteration : 7887
train acc:  0.7265625
train loss:  0.4699530303478241
train gradient:  0.1162947007288223
iteration : 7888
train acc:  0.7109375
train loss:  0.4960826635360718
train gradient:  0.16362964149409082
iteration : 7889
train acc:  0.75
train loss:  0.5194384455680847
train gradient:  0.12546070021123762
iteration : 7890
train acc:  0.796875
train loss:  0.4253283143043518
train gradient:  0.10854105567886087
iteration : 7891
train acc:  0.7109375
train loss:  0.5135804414749146
train gradient:  0.13883354900637154
iteration : 7892
train acc:  0.7265625
train loss:  0.5460183620452881
train gradient:  0.13935177672175916
iteration : 7893
train acc:  0.7734375
train loss:  0.44099360704421997
train gradient:  0.133548744199339
iteration : 7894
train acc:  0.796875
train loss:  0.455668568611145
train gradient:  0.12024373581690312
iteration : 7895
train acc:  0.8046875
train loss:  0.43649306893348694
train gradient:  0.10343263372223958
iteration : 7896
train acc:  0.7578125
train loss:  0.45291829109191895
train gradient:  0.13335768465815961
iteration : 7897
train acc:  0.7109375
train loss:  0.5024086236953735
train gradient:  0.13934918388344936
iteration : 7898
train acc:  0.765625
train loss:  0.4673253893852234
train gradient:  0.11030107091927781
iteration : 7899
train acc:  0.71875
train loss:  0.5506171584129333
train gradient:  0.19707633763841842
iteration : 7900
train acc:  0.78125
train loss:  0.5089031457901001
train gradient:  0.21967081672235314
iteration : 7901
train acc:  0.734375
train loss:  0.48344606161117554
train gradient:  0.11549038635301555
iteration : 7902
train acc:  0.78125
train loss:  0.5117247700691223
train gradient:  0.1483998469934731
iteration : 7903
train acc:  0.7109375
train loss:  0.49956363439559937
train gradient:  0.1862625128677416
iteration : 7904
train acc:  0.71875
train loss:  0.5210487842559814
train gradient:  0.15636015022507965
iteration : 7905
train acc:  0.8046875
train loss:  0.46808335185050964
train gradient:  0.11345787593988248
iteration : 7906
train acc:  0.75
train loss:  0.49642571806907654
train gradient:  0.15066316964301557
iteration : 7907
train acc:  0.7265625
train loss:  0.5038278102874756
train gradient:  0.19174712203614613
iteration : 7908
train acc:  0.75
train loss:  0.5289298892021179
train gradient:  0.13199752339270618
iteration : 7909
train acc:  0.7421875
train loss:  0.49238359928131104
train gradient:  0.15860418153087535
iteration : 7910
train acc:  0.703125
train loss:  0.5511765480041504
train gradient:  0.16168351508738776
iteration : 7911
train acc:  0.765625
train loss:  0.48991522192955017
train gradient:  0.12133090290772844
iteration : 7912
train acc:  0.734375
train loss:  0.5397800207138062
train gradient:  0.15411242788487794
iteration : 7913
train acc:  0.7890625
train loss:  0.47206801176071167
train gradient:  0.12048369620499558
iteration : 7914
train acc:  0.75
train loss:  0.4640744626522064
train gradient:  0.12127146567291802
iteration : 7915
train acc:  0.71875
train loss:  0.5577288866043091
train gradient:  0.17058768918962625
iteration : 7916
train acc:  0.7265625
train loss:  0.5323749780654907
train gradient:  0.16394176166157992
iteration : 7917
train acc:  0.7109375
train loss:  0.532524585723877
train gradient:  0.16563309802146425
iteration : 7918
train acc:  0.7734375
train loss:  0.42856425046920776
train gradient:  0.09164209530058208
iteration : 7919
train acc:  0.65625
train loss:  0.5313708782196045
train gradient:  0.1345429965304087
iteration : 7920
train acc:  0.7578125
train loss:  0.45959627628326416
train gradient:  0.1063557116858446
iteration : 7921
train acc:  0.796875
train loss:  0.46125125885009766
train gradient:  0.11858975631342845
iteration : 7922
train acc:  0.6953125
train loss:  0.5112780332565308
train gradient:  0.15315972506878175
iteration : 7923
train acc:  0.7734375
train loss:  0.48995161056518555
train gradient:  0.11587888208637256
iteration : 7924
train acc:  0.7109375
train loss:  0.5259344577789307
train gradient:  0.1392709395737454
iteration : 7925
train acc:  0.78125
train loss:  0.44495177268981934
train gradient:  0.1257990839986472
iteration : 7926
train acc:  0.7421875
train loss:  0.5086899995803833
train gradient:  0.17485296615458795
iteration : 7927
train acc:  0.75
train loss:  0.4799678921699524
train gradient:  0.13127666776174124
iteration : 7928
train acc:  0.84375
train loss:  0.3923720717430115
train gradient:  0.09203216646692908
iteration : 7929
train acc:  0.6171875
train loss:  0.6363447904586792
train gradient:  0.22865455622076197
iteration : 7930
train acc:  0.8359375
train loss:  0.4293769299983978
train gradient:  0.10945947461475351
iteration : 7931
train acc:  0.8203125
train loss:  0.44737833738327026
train gradient:  0.120632935282484
iteration : 7932
train acc:  0.765625
train loss:  0.5389175415039062
train gradient:  0.16113876066029764
iteration : 7933
train acc:  0.734375
train loss:  0.46720343828201294
train gradient:  0.1305664763524595
iteration : 7934
train acc:  0.7109375
train loss:  0.5316112637519836
train gradient:  0.19625694241944797
iteration : 7935
train acc:  0.7734375
train loss:  0.45310235023498535
train gradient:  0.11871933479724742
iteration : 7936
train acc:  0.7421875
train loss:  0.5243991613388062
train gradient:  0.16697197333227498
iteration : 7937
train acc:  0.734375
train loss:  0.5061653852462769
train gradient:  0.16645122621790798
iteration : 7938
train acc:  0.765625
train loss:  0.4888552725315094
train gradient:  0.1125973937536259
iteration : 7939
train acc:  0.7265625
train loss:  0.5164690613746643
train gradient:  0.14632970615962945
iteration : 7940
train acc:  0.6796875
train loss:  0.5923372507095337
train gradient:  0.19155689664030195
iteration : 7941
train acc:  0.71875
train loss:  0.5664876103401184
train gradient:  0.15396737781843228
iteration : 7942
train acc:  0.78125
train loss:  0.4347015917301178
train gradient:  0.1542475205617729
iteration : 7943
train acc:  0.671875
train loss:  0.5376165509223938
train gradient:  0.14789785436080605
iteration : 7944
train acc:  0.6796875
train loss:  0.5290871858596802
train gradient:  0.1661398944946072
iteration : 7945
train acc:  0.71875
train loss:  0.5066777467727661
train gradient:  0.14047383880253636
iteration : 7946
train acc:  0.7265625
train loss:  0.5482637882232666
train gradient:  0.19771189899887032
iteration : 7947
train acc:  0.7578125
train loss:  0.4996655583381653
train gradient:  0.14507276149524972
iteration : 7948
train acc:  0.796875
train loss:  0.4471476674079895
train gradient:  0.11268863917640437
iteration : 7949
train acc:  0.7734375
train loss:  0.4414917826652527
train gradient:  0.09468000004033542
iteration : 7950
train acc:  0.75
train loss:  0.47770869731903076
train gradient:  0.10936453431317422
iteration : 7951
train acc:  0.75
train loss:  0.5421487092971802
train gradient:  0.17048726194959474
iteration : 7952
train acc:  0.71875
train loss:  0.5232559442520142
train gradient:  0.14258755431671594
iteration : 7953
train acc:  0.7890625
train loss:  0.4673781991004944
train gradient:  0.1075182028484811
iteration : 7954
train acc:  0.71875
train loss:  0.5260084867477417
train gradient:  0.14336828343653013
iteration : 7955
train acc:  0.65625
train loss:  0.540397047996521
train gradient:  0.12579119399867186
iteration : 7956
train acc:  0.7578125
train loss:  0.5387897491455078
train gradient:  0.1698954400662434
iteration : 7957
train acc:  0.7890625
train loss:  0.47973573207855225
train gradient:  0.11272019288824765
iteration : 7958
train acc:  0.7265625
train loss:  0.5298763513565063
train gradient:  0.12519350180845537
iteration : 7959
train acc:  0.71875
train loss:  0.4884781837463379
train gradient:  0.10925312612738391
iteration : 7960
train acc:  0.734375
train loss:  0.5445581674575806
train gradient:  0.14795864796163893
iteration : 7961
train acc:  0.75
train loss:  0.4930657148361206
train gradient:  0.1418436402009159
iteration : 7962
train acc:  0.6875
train loss:  0.5248047113418579
train gradient:  0.1282288089416818
iteration : 7963
train acc:  0.734375
train loss:  0.4636285603046417
train gradient:  0.11012216353172884
iteration : 7964
train acc:  0.7734375
train loss:  0.47681519389152527
train gradient:  0.14848642587759053
iteration : 7965
train acc:  0.734375
train loss:  0.529597282409668
train gradient:  0.12622287688111583
iteration : 7966
train acc:  0.78125
train loss:  0.491971880197525
train gradient:  0.1332694968933164
iteration : 7967
train acc:  0.671875
train loss:  0.5618017315864563
train gradient:  0.14330149751578705
iteration : 7968
train acc:  0.6484375
train loss:  0.5457748174667358
train gradient:  0.14100453134463004
iteration : 7969
train acc:  0.75
train loss:  0.47268131375312805
train gradient:  0.12232987037773749
iteration : 7970
train acc:  0.6875
train loss:  0.5434699058532715
train gradient:  0.15402799160300984
iteration : 7971
train acc:  0.703125
train loss:  0.493183970451355
train gradient:  0.13442810004828673
iteration : 7972
train acc:  0.7578125
train loss:  0.48215222358703613
train gradient:  0.12557546170975467
iteration : 7973
train acc:  0.6953125
train loss:  0.5427877902984619
train gradient:  0.18322495093533603
iteration : 7974
train acc:  0.75
train loss:  0.45261579751968384
train gradient:  0.11644118257726438
iteration : 7975
train acc:  0.734375
train loss:  0.5090435147285461
train gradient:  0.1300877410116264
iteration : 7976
train acc:  0.6875
train loss:  0.4898582696914673
train gradient:  0.14252130043668376
iteration : 7977
train acc:  0.7109375
train loss:  0.5094667673110962
train gradient:  0.11295150109142436
iteration : 7978
train acc:  0.7734375
train loss:  0.42388099431991577
train gradient:  0.10513749182626045
iteration : 7979
train acc:  0.765625
train loss:  0.48356330394744873
train gradient:  0.17052048598509412
iteration : 7980
train acc:  0.75
train loss:  0.4731907844543457
train gradient:  0.1439289563212114
iteration : 7981
train acc:  0.6796875
train loss:  0.5656723976135254
train gradient:  0.17993057092561027
iteration : 7982
train acc:  0.7421875
train loss:  0.5297209024429321
train gradient:  0.1216122482709301
iteration : 7983
train acc:  0.765625
train loss:  0.47637253999710083
train gradient:  0.12519966957434464
iteration : 7984
train acc:  0.7109375
train loss:  0.506178617477417
train gradient:  0.11948250792246812
iteration : 7985
train acc:  0.765625
train loss:  0.5266888737678528
train gradient:  0.13908081446770867
iteration : 7986
train acc:  0.7265625
train loss:  0.5332751274108887
train gradient:  0.16196139657754688
iteration : 7987
train acc:  0.7578125
train loss:  0.4471893608570099
train gradient:  0.10984919067909178
iteration : 7988
train acc:  0.65625
train loss:  0.5165172815322876
train gradient:  0.14267109737463307
iteration : 7989
train acc:  0.6796875
train loss:  0.5940767526626587
train gradient:  0.13577548853644766
iteration : 7990
train acc:  0.7421875
train loss:  0.4926120638847351
train gradient:  0.16036683393586376
iteration : 7991
train acc:  0.7265625
train loss:  0.4774932265281677
train gradient:  0.1431067751953904
iteration : 7992
train acc:  0.7421875
train loss:  0.5308823585510254
train gradient:  0.17768714301181054
iteration : 7993
train acc:  0.765625
train loss:  0.4615563154220581
train gradient:  0.11140908192918471
iteration : 7994
train acc:  0.7578125
train loss:  0.5154669284820557
train gradient:  0.1461809614165655
iteration : 7995
train acc:  0.703125
train loss:  0.5243935585021973
train gradient:  0.12677995637061445
iteration : 7996
train acc:  0.7734375
train loss:  0.482599675655365
train gradient:  0.12464505574364317
iteration : 7997
train acc:  0.65625
train loss:  0.5324732065200806
train gradient:  0.12083670485579612
iteration : 7998
train acc:  0.71875
train loss:  0.4979156255722046
train gradient:  0.121753978305017
iteration : 7999
train acc:  0.7578125
train loss:  0.4715285301208496
train gradient:  0.15182701071958316
iteration : 8000
train acc:  0.7421875
train loss:  0.564433753490448
train gradient:  0.16933682974948777
iteration : 8001
train acc:  0.6875
train loss:  0.5088184475898743
train gradient:  0.13928437385143133
iteration : 8002
train acc:  0.7109375
train loss:  0.5526256561279297
train gradient:  0.15070553519350594
iteration : 8003
train acc:  0.765625
train loss:  0.45209693908691406
train gradient:  0.10832931801093362
iteration : 8004
train acc:  0.75
train loss:  0.4519142806529999
train gradient:  0.12440517001827217
iteration : 8005
train acc:  0.7109375
train loss:  0.5261794328689575
train gradient:  0.15917411189636932
iteration : 8006
train acc:  0.78125
train loss:  0.4194759726524353
train gradient:  0.09713911053607338
iteration : 8007
train acc:  0.75
train loss:  0.49955418705940247
train gradient:  0.14926267687189215
iteration : 8008
train acc:  0.65625
train loss:  0.5946120023727417
train gradient:  0.2185036589442922
iteration : 8009
train acc:  0.7578125
train loss:  0.5172652006149292
train gradient:  0.13024440085621158
iteration : 8010
train acc:  0.734375
train loss:  0.5292166471481323
train gradient:  0.14935295264330664
iteration : 8011
train acc:  0.78125
train loss:  0.4579973816871643
train gradient:  0.1262364097939766
iteration : 8012
train acc:  0.8125
train loss:  0.3983919620513916
train gradient:  0.07977134766191686
iteration : 8013
train acc:  0.71875
train loss:  0.5409212112426758
train gradient:  0.13220499808318453
iteration : 8014
train acc:  0.7578125
train loss:  0.4923499822616577
train gradient:  0.10508762259578741
iteration : 8015
train acc:  0.7109375
train loss:  0.5623525381088257
train gradient:  0.16500111323033478
iteration : 8016
train acc:  0.6171875
train loss:  0.5551806688308716
train gradient:  0.18931675148477842
iteration : 8017
train acc:  0.71875
train loss:  0.542596697807312
train gradient:  0.1823922052760338
iteration : 8018
train acc:  0.765625
train loss:  0.48946714401245117
train gradient:  0.11831314299563034
iteration : 8019
train acc:  0.796875
train loss:  0.474488765001297
train gradient:  0.11017907332950692
iteration : 8020
train acc:  0.6953125
train loss:  0.5430455207824707
train gradient:  0.1650500515175592
iteration : 8021
train acc:  0.78125
train loss:  0.4663432538509369
train gradient:  0.1100633652248635
iteration : 8022
train acc:  0.8046875
train loss:  0.45349228382110596
train gradient:  0.10006816197849551
iteration : 8023
train acc:  0.7265625
train loss:  0.48267221450805664
train gradient:  0.13440440217584448
iteration : 8024
train acc:  0.75
train loss:  0.4856188893318176
train gradient:  0.11557110322158115
iteration : 8025
train acc:  0.6640625
train loss:  0.5261009931564331
train gradient:  0.14873577007888605
iteration : 8026
train acc:  0.75
train loss:  0.4400379955768585
train gradient:  0.07811839268354326
iteration : 8027
train acc:  0.7421875
train loss:  0.5204131603240967
train gradient:  0.1731537937130337
iteration : 8028
train acc:  0.734375
train loss:  0.4878423810005188
train gradient:  0.13924825316468656
iteration : 8029
train acc:  0.765625
train loss:  0.5155792832374573
train gradient:  0.1432597310874275
iteration : 8030
train acc:  0.671875
train loss:  0.5854804515838623
train gradient:  0.18215198693893753
iteration : 8031
train acc:  0.78125
train loss:  0.4834764301776886
train gradient:  0.1412722333898408
iteration : 8032
train acc:  0.7578125
train loss:  0.4813442826271057
train gradient:  0.10971435111864272
iteration : 8033
train acc:  0.7890625
train loss:  0.44665515422821045
train gradient:  0.120698655103692
iteration : 8034
train acc:  0.71875
train loss:  0.5252348184585571
train gradient:  0.12644368732645078
iteration : 8035
train acc:  0.8125
train loss:  0.4239937961101532
train gradient:  0.1053102700640984
iteration : 8036
train acc:  0.7578125
train loss:  0.4425351917743683
train gradient:  0.14031762756267502
iteration : 8037
train acc:  0.7734375
train loss:  0.4955857992172241
train gradient:  0.15955153223129118
iteration : 8038
train acc:  0.6796875
train loss:  0.5301068425178528
train gradient:  0.14154400840718945
iteration : 8039
train acc:  0.7734375
train loss:  0.4795563817024231
train gradient:  0.1098011729382245
iteration : 8040
train acc:  0.7578125
train loss:  0.5569317936897278
train gradient:  0.1597018792729157
iteration : 8041
train acc:  0.71875
train loss:  0.48663976788520813
train gradient:  0.15038040057865248
iteration : 8042
train acc:  0.7578125
train loss:  0.5133907198905945
train gradient:  0.1034258496038166
iteration : 8043
train acc:  0.7578125
train loss:  0.4817489981651306
train gradient:  0.1219823585168687
iteration : 8044
train acc:  0.7578125
train loss:  0.5217368006706238
train gradient:  0.12292115691481197
iteration : 8045
train acc:  0.78125
train loss:  0.4401114881038666
train gradient:  0.10520472213261191
iteration : 8046
train acc:  0.734375
train loss:  0.5784961581230164
train gradient:  0.1652744367980072
iteration : 8047
train acc:  0.734375
train loss:  0.48312830924987793
train gradient:  0.1303588855422102
iteration : 8048
train acc:  0.765625
train loss:  0.478581041097641
train gradient:  0.11388549259908806
iteration : 8049
train acc:  0.7578125
train loss:  0.48514989018440247
train gradient:  0.1380005076188351
iteration : 8050
train acc:  0.6953125
train loss:  0.5601449012756348
train gradient:  0.17645761617746464
iteration : 8051
train acc:  0.6875
train loss:  0.6000044345855713
train gradient:  0.15796525171758852
iteration : 8052
train acc:  0.6953125
train loss:  0.5300737619400024
train gradient:  0.12246895180952218
iteration : 8053
train acc:  0.7734375
train loss:  0.4475677013397217
train gradient:  0.13625564234012943
iteration : 8054
train acc:  0.6953125
train loss:  0.5678117275238037
train gradient:  0.1705610775936396
iteration : 8055
train acc:  0.7890625
train loss:  0.4433495104312897
train gradient:  0.10705669613280554
iteration : 8056
train acc:  0.609375
train loss:  0.6223077774047852
train gradient:  0.17544241244615405
iteration : 8057
train acc:  0.8125
train loss:  0.4258561432361603
train gradient:  0.08294760433749239
iteration : 8058
train acc:  0.828125
train loss:  0.4231169819831848
train gradient:  0.09612348881993604
iteration : 8059
train acc:  0.75
train loss:  0.4892295002937317
train gradient:  0.11009349801721328
iteration : 8060
train acc:  0.7265625
train loss:  0.5152719020843506
train gradient:  0.1563127178270132
iteration : 8061
train acc:  0.7109375
train loss:  0.5382967591285706
train gradient:  0.15792109887778413
iteration : 8062
train acc:  0.7734375
train loss:  0.4726022183895111
train gradient:  0.11573622721322516
iteration : 8063
train acc:  0.71875
train loss:  0.5432840585708618
train gradient:  0.1666889962161
iteration : 8064
train acc:  0.7890625
train loss:  0.4770650267601013
train gradient:  0.1506972906492674
iteration : 8065
train acc:  0.7578125
train loss:  0.49772506952285767
train gradient:  0.11940065847439675
iteration : 8066
train acc:  0.7890625
train loss:  0.48616957664489746
train gradient:  0.12522747097069092
iteration : 8067
train acc:  0.71875
train loss:  0.5525844097137451
train gradient:  0.14717239609581617
iteration : 8068
train acc:  0.7265625
train loss:  0.5299066305160522
train gradient:  0.18181563524615557
iteration : 8069
train acc:  0.7265625
train loss:  0.4738149642944336
train gradient:  0.13222091230326793
iteration : 8070
train acc:  0.703125
train loss:  0.536562979221344
train gradient:  0.12892124110620837
iteration : 8071
train acc:  0.75
train loss:  0.49648982286453247
train gradient:  0.14282042783456372
iteration : 8072
train acc:  0.7734375
train loss:  0.4661077558994293
train gradient:  0.1058632349810462
iteration : 8073
train acc:  0.703125
train loss:  0.508160412311554
train gradient:  0.11961978010740762
iteration : 8074
train acc:  0.734375
train loss:  0.5140886902809143
train gradient:  0.1549010611437216
iteration : 8075
train acc:  0.75
train loss:  0.4597755968570709
train gradient:  0.17652013359952684
iteration : 8076
train acc:  0.734375
train loss:  0.4989730715751648
train gradient:  0.1273057603709315
iteration : 8077
train acc:  0.7578125
train loss:  0.4495246112346649
train gradient:  0.09621268332810523
iteration : 8078
train acc:  0.765625
train loss:  0.5058255791664124
train gradient:  0.1592444122437426
iteration : 8079
train acc:  0.7578125
train loss:  0.45042097568511963
train gradient:  0.09129168871257257
iteration : 8080
train acc:  0.7734375
train loss:  0.47681787610054016
train gradient:  0.10533186907579178
iteration : 8081
train acc:  0.7734375
train loss:  0.45349758863449097
train gradient:  0.14503800049402488
iteration : 8082
train acc:  0.75
train loss:  0.524436891078949
train gradient:  0.1732418185033594
iteration : 8083
train acc:  0.7109375
train loss:  0.5230215191841125
train gradient:  0.15467683829010537
iteration : 8084
train acc:  0.71875
train loss:  0.5461400151252747
train gradient:  0.15893445246261162
iteration : 8085
train acc:  0.7890625
train loss:  0.47398096323013306
train gradient:  0.19227924410848446
iteration : 8086
train acc:  0.7265625
train loss:  0.48899996280670166
train gradient:  0.12528878603524987
iteration : 8087
train acc:  0.71875
train loss:  0.5303128361701965
train gradient:  0.13174778442394153
iteration : 8088
train acc:  0.7890625
train loss:  0.4657171964645386
train gradient:  0.10756825466446202
iteration : 8089
train acc:  0.765625
train loss:  0.49781787395477295
train gradient:  0.11339715747220384
iteration : 8090
train acc:  0.7578125
train loss:  0.4932587444782257
train gradient:  0.11960773764161262
iteration : 8091
train acc:  0.7109375
train loss:  0.4968092441558838
train gradient:  0.1210477819228056
iteration : 8092
train acc:  0.6796875
train loss:  0.5446054339408875
train gradient:  0.15690870604698454
iteration : 8093
train acc:  0.7421875
train loss:  0.502586841583252
train gradient:  0.12975413503337668
iteration : 8094
train acc:  0.7109375
train loss:  0.537482500076294
train gradient:  0.13731851727255856
iteration : 8095
train acc:  0.7109375
train loss:  0.49948376417160034
train gradient:  0.13013836867754902
iteration : 8096
train acc:  0.7578125
train loss:  0.48409563302993774
train gradient:  0.1149031834121796
iteration : 8097
train acc:  0.71875
train loss:  0.5333940982818604
train gradient:  0.1460097615669073
iteration : 8098
train acc:  0.765625
train loss:  0.4563470482826233
train gradient:  0.13551771117395206
iteration : 8099
train acc:  0.6953125
train loss:  0.5337271094322205
train gradient:  0.14440189304382126
iteration : 8100
train acc:  0.765625
train loss:  0.47306159138679504
train gradient:  0.12731116414460547
iteration : 8101
train acc:  0.75
train loss:  0.498452365398407
train gradient:  0.14468514012998437
iteration : 8102
train acc:  0.7109375
train loss:  0.49429431557655334
train gradient:  0.13138598980878996
iteration : 8103
train acc:  0.8125
train loss:  0.43258872628211975
train gradient:  0.09971928143778787
iteration : 8104
train acc:  0.796875
train loss:  0.45636528730392456
train gradient:  0.10786503505327816
iteration : 8105
train acc:  0.7265625
train loss:  0.5007036924362183
train gradient:  0.18416445518015712
iteration : 8106
train acc:  0.6875
train loss:  0.5248563289642334
train gradient:  0.12219321851892274
iteration : 8107
train acc:  0.7265625
train loss:  0.5404124855995178
train gradient:  0.14347993858550934
iteration : 8108
train acc:  0.75
train loss:  0.47532105445861816
train gradient:  0.14906307827799503
iteration : 8109
train acc:  0.734375
train loss:  0.5130689144134521
train gradient:  0.14558035521922638
iteration : 8110
train acc:  0.71875
train loss:  0.4978960156440735
train gradient:  0.1453295690026374
iteration : 8111
train acc:  0.7734375
train loss:  0.4891602098941803
train gradient:  0.11926700209465915
iteration : 8112
train acc:  0.6875
train loss:  0.4838896691799164
train gradient:  0.1274248213001153
iteration : 8113
train acc:  0.6796875
train loss:  0.5216596126556396
train gradient:  0.14088539924524413
iteration : 8114
train acc:  0.8203125
train loss:  0.44594040513038635
train gradient:  0.12066632264599221
iteration : 8115
train acc:  0.75
train loss:  0.5303289890289307
train gradient:  0.14782238135365827
iteration : 8116
train acc:  0.7421875
train loss:  0.48365581035614014
train gradient:  0.1478705270314586
iteration : 8117
train acc:  0.7578125
train loss:  0.47610703110694885
train gradient:  0.12368640944995489
iteration : 8118
train acc:  0.796875
train loss:  0.45121461153030396
train gradient:  0.09602278615379868
iteration : 8119
train acc:  0.734375
train loss:  0.5423614382743835
train gradient:  0.18581771613314763
iteration : 8120
train acc:  0.7109375
train loss:  0.5168799161911011
train gradient:  0.14633664715451788
iteration : 8121
train acc:  0.7578125
train loss:  0.516959547996521
train gradient:  0.14498052941559858
iteration : 8122
train acc:  0.7578125
train loss:  0.5027971267700195
train gradient:  0.12880702694559093
iteration : 8123
train acc:  0.7421875
train loss:  0.462758332490921
train gradient:  0.10396930281562325
iteration : 8124
train acc:  0.6875
train loss:  0.5676419734954834
train gradient:  0.16373547742585398
iteration : 8125
train acc:  0.7109375
train loss:  0.4688894748687744
train gradient:  0.11032774619580565
iteration : 8126
train acc:  0.7578125
train loss:  0.49379611015319824
train gradient:  0.1318380599987127
iteration : 8127
train acc:  0.7421875
train loss:  0.47910434007644653
train gradient:  0.12422476224440443
iteration : 8128
train acc:  0.7578125
train loss:  0.45663803815841675
train gradient:  0.11311118206541508
iteration : 8129
train acc:  0.6796875
train loss:  0.595316469669342
train gradient:  0.15990453107126107
iteration : 8130
train acc:  0.828125
train loss:  0.40484780073165894
train gradient:  0.10724114157338992
iteration : 8131
train acc:  0.765625
train loss:  0.4708384573459625
train gradient:  0.1029121243791759
iteration : 8132
train acc:  0.703125
train loss:  0.4935583174228668
train gradient:  0.15003672559198356
iteration : 8133
train acc:  0.734375
train loss:  0.5430073738098145
train gradient:  0.14575978778494245
iteration : 8134
train acc:  0.75
train loss:  0.5144685506820679
train gradient:  0.16267964210752403
iteration : 8135
train acc:  0.75
train loss:  0.5214773416519165
train gradient:  0.13474684600412937
iteration : 8136
train acc:  0.7109375
train loss:  0.515362024307251
train gradient:  0.14173527703210897
iteration : 8137
train acc:  0.6953125
train loss:  0.4955139458179474
train gradient:  0.14609244866996585
iteration : 8138
train acc:  0.6796875
train loss:  0.539176344871521
train gradient:  0.14280212606051756
iteration : 8139
train acc:  0.7265625
train loss:  0.5105093121528625
train gradient:  0.15141154757339836
iteration : 8140
train acc:  0.765625
train loss:  0.5281908512115479
train gradient:  0.13918972464706747
iteration : 8141
train acc:  0.7109375
train loss:  0.513964831829071
train gradient:  0.14625000601519866
iteration : 8142
train acc:  0.7421875
train loss:  0.5155599117279053
train gradient:  0.14214982441959811
iteration : 8143
train acc:  0.7265625
train loss:  0.5235432982444763
train gradient:  0.13984388305326428
iteration : 8144
train acc:  0.6875
train loss:  0.5867325067520142
train gradient:  0.16713047565137346
iteration : 8145
train acc:  0.7265625
train loss:  0.5196849703788757
train gradient:  0.14107523499914004
iteration : 8146
train acc:  0.7578125
train loss:  0.45646485686302185
train gradient:  0.14054124363493337
iteration : 8147
train acc:  0.7421875
train loss:  0.48495957255363464
train gradient:  0.13633531997157153
iteration : 8148
train acc:  0.7578125
train loss:  0.4482860565185547
train gradient:  0.11850672356550246
iteration : 8149
train acc:  0.796875
train loss:  0.4671178460121155
train gradient:  0.1076721097230115
iteration : 8150
train acc:  0.765625
train loss:  0.47401684522628784
train gradient:  0.09659254336826466
iteration : 8151
train acc:  0.765625
train loss:  0.46723151206970215
train gradient:  0.10015374644079433
iteration : 8152
train acc:  0.7421875
train loss:  0.5220022201538086
train gradient:  0.1352831978839887
iteration : 8153
train acc:  0.78125
train loss:  0.4670258164405823
train gradient:  0.13943910768052942
iteration : 8154
train acc:  0.7578125
train loss:  0.47156041860580444
train gradient:  0.1184516828501827
iteration : 8155
train acc:  0.7734375
train loss:  0.46012818813323975
train gradient:  0.10225835146791223
iteration : 8156
train acc:  0.71875
train loss:  0.5355167984962463
train gradient:  0.1366743666918852
iteration : 8157
train acc:  0.7265625
train loss:  0.503414511680603
train gradient:  0.13721437149092663
iteration : 8158
train acc:  0.78125
train loss:  0.49165821075439453
train gradient:  0.1517884137764411
iteration : 8159
train acc:  0.7890625
train loss:  0.4962767958641052
train gradient:  0.13102451304915144
iteration : 8160
train acc:  0.7734375
train loss:  0.4723210036754608
train gradient:  0.14254528613982165
iteration : 8161
train acc:  0.7421875
train loss:  0.48510563373565674
train gradient:  0.12010807811458467
iteration : 8162
train acc:  0.75
train loss:  0.4937587380409241
train gradient:  0.16248111915408692
iteration : 8163
train acc:  0.7109375
train loss:  0.5574249029159546
train gradient:  0.16517930553629337
iteration : 8164
train acc:  0.71875
train loss:  0.5069069862365723
train gradient:  0.12311748222420922
iteration : 8165
train acc:  0.7109375
train loss:  0.5081570148468018
train gradient:  0.16323339802626186
iteration : 8166
train acc:  0.78125
train loss:  0.43536579608917236
train gradient:  0.10251193959920936
iteration : 8167
train acc:  0.796875
train loss:  0.48694729804992676
train gradient:  0.12214159922649026
iteration : 8168
train acc:  0.734375
train loss:  0.509925365447998
train gradient:  0.12282734275166163
iteration : 8169
train acc:  0.6953125
train loss:  0.47289568185806274
train gradient:  0.1411236332646652
iteration : 8170
train acc:  0.796875
train loss:  0.4945898652076721
train gradient:  0.14619203111386464
iteration : 8171
train acc:  0.671875
train loss:  0.5385475754737854
train gradient:  0.14993803537200168
iteration : 8172
train acc:  0.6953125
train loss:  0.5256111025810242
train gradient:  0.1725419058421086
iteration : 8173
train acc:  0.703125
train loss:  0.5251373052597046
train gradient:  0.12267860341211799
iteration : 8174
train acc:  0.75
train loss:  0.4718858599662781
train gradient:  0.11888795429307654
iteration : 8175
train acc:  0.84375
train loss:  0.4204796254634857
train gradient:  0.09594810917399749
iteration : 8176
train acc:  0.765625
train loss:  0.5032184720039368
train gradient:  0.12890004792723608
iteration : 8177
train acc:  0.7265625
train loss:  0.5494192242622375
train gradient:  0.15536100236917055
iteration : 8178
train acc:  0.7578125
train loss:  0.49151158332824707
train gradient:  0.1479321106818859
iteration : 8179
train acc:  0.7734375
train loss:  0.49713677167892456
train gradient:  0.15575730312322664
iteration : 8180
train acc:  0.7265625
train loss:  0.4932047724723816
train gradient:  0.14267609244931423
iteration : 8181
train acc:  0.6953125
train loss:  0.508769690990448
train gradient:  0.16736078571195234
iteration : 8182
train acc:  0.734375
train loss:  0.501211404800415
train gradient:  0.1293323322045453
iteration : 8183
train acc:  0.8125
train loss:  0.4077425003051758
train gradient:  0.10693399789247414
iteration : 8184
train acc:  0.7265625
train loss:  0.5538492202758789
train gradient:  0.13351160442233673
iteration : 8185
train acc:  0.71875
train loss:  0.5664500594139099
train gradient:  0.1636090144864215
iteration : 8186
train acc:  0.6953125
train loss:  0.561065673828125
train gradient:  0.15971918581947164
iteration : 8187
train acc:  0.7265625
train loss:  0.48576587438583374
train gradient:  0.15279693797406182
iteration : 8188
train acc:  0.671875
train loss:  0.5950483083724976
train gradient:  0.20584573882039625
iteration : 8189
train acc:  0.7890625
train loss:  0.4727610945701599
train gradient:  0.1161552802010615
iteration : 8190
train acc:  0.640625
train loss:  0.5888413190841675
train gradient:  0.15205665289613648
iteration : 8191
train acc:  0.6953125
train loss:  0.5172659754753113
train gradient:  0.1631402904678208
iteration : 8192
train acc:  0.7734375
train loss:  0.507192850112915
train gradient:  0.13396115853474488
iteration : 8193
train acc:  0.7578125
train loss:  0.4926755428314209
train gradient:  0.13079213006256626
iteration : 8194
train acc:  0.7421875
train loss:  0.47195231914520264
train gradient:  0.12558135101933968
iteration : 8195
train acc:  0.7109375
train loss:  0.5412558317184448
train gradient:  0.14850097876361332
iteration : 8196
train acc:  0.828125
train loss:  0.4336596131324768
train gradient:  0.1035766274587455
iteration : 8197
train acc:  0.734375
train loss:  0.5417712330818176
train gradient:  0.12398987959488167
iteration : 8198
train acc:  0.703125
train loss:  0.5451754331588745
train gradient:  0.20865588822151962
iteration : 8199
train acc:  0.71875
train loss:  0.5105529427528381
train gradient:  0.12778898588111148
iteration : 8200
train acc:  0.7578125
train loss:  0.47932684421539307
train gradient:  0.12612244147145682
iteration : 8201
train acc:  0.7578125
train loss:  0.46729493141174316
train gradient:  0.12587840373875792
iteration : 8202
train acc:  0.8046875
train loss:  0.491436243057251
train gradient:  0.12391439165637502
iteration : 8203
train acc:  0.7578125
train loss:  0.4803304672241211
train gradient:  0.13439922208914912
iteration : 8204
train acc:  0.7578125
train loss:  0.4981248378753662
train gradient:  0.1169881920021908
iteration : 8205
train acc:  0.6953125
train loss:  0.5482300519943237
train gradient:  0.14785943371890886
iteration : 8206
train acc:  0.71875
train loss:  0.56110680103302
train gradient:  0.20823335399978835
iteration : 8207
train acc:  0.7109375
train loss:  0.4985489547252655
train gradient:  0.15278536173165158
iteration : 8208
train acc:  0.75
train loss:  0.47362515330314636
train gradient:  0.11985107076738595
iteration : 8209
train acc:  0.734375
train loss:  0.5046732425689697
train gradient:  0.13922037949385102
iteration : 8210
train acc:  0.765625
train loss:  0.4685644209384918
train gradient:  0.1117684842631375
iteration : 8211
train acc:  0.7890625
train loss:  0.44334670901298523
train gradient:  0.11581936808419563
iteration : 8212
train acc:  0.7734375
train loss:  0.46711939573287964
train gradient:  0.10151909800985581
iteration : 8213
train acc:  0.75
train loss:  0.5505192875862122
train gradient:  0.1493165742691945
iteration : 8214
train acc:  0.6953125
train loss:  0.5189462900161743
train gradient:  0.11581481831595056
iteration : 8215
train acc:  0.7265625
train loss:  0.5084320306777954
train gradient:  0.16803830047928758
iteration : 8216
train acc:  0.75
train loss:  0.5049245953559875
train gradient:  0.1586566415168427
iteration : 8217
train acc:  0.796875
train loss:  0.48969292640686035
train gradient:  0.14865583481606592
iteration : 8218
train acc:  0.7890625
train loss:  0.45225152373313904
train gradient:  0.11401575363194914
iteration : 8219
train acc:  0.6953125
train loss:  0.5649082660675049
train gradient:  0.15135275539160414
iteration : 8220
train acc:  0.6875
train loss:  0.5562740564346313
train gradient:  0.15822397938244814
iteration : 8221
train acc:  0.71875
train loss:  0.5214561223983765
train gradient:  0.161753004453425
iteration : 8222
train acc:  0.7109375
train loss:  0.5489242076873779
train gradient:  0.15597200247400206
iteration : 8223
train acc:  0.734375
train loss:  0.5160133242607117
train gradient:  0.15017249375868919
iteration : 8224
train acc:  0.71875
train loss:  0.5253003239631653
train gradient:  0.1446076136300834
iteration : 8225
train acc:  0.7109375
train loss:  0.5601489543914795
train gradient:  0.17401827507623563
iteration : 8226
train acc:  0.7265625
train loss:  0.5267233848571777
train gradient:  0.17538890153317943
iteration : 8227
train acc:  0.7734375
train loss:  0.4587039053440094
train gradient:  0.11784517940483197
iteration : 8228
train acc:  0.6875
train loss:  0.5224469900131226
train gradient:  0.12692375974925815
iteration : 8229
train acc:  0.7421875
train loss:  0.4558253884315491
train gradient:  0.11717062745245943
iteration : 8230
train acc:  0.734375
train loss:  0.49536052346229553
train gradient:  0.11064261017362222
iteration : 8231
train acc:  0.7109375
train loss:  0.4845021367073059
train gradient:  0.12124918974038758
iteration : 8232
train acc:  0.7421875
train loss:  0.5425938367843628
train gradient:  0.14937430341809083
iteration : 8233
train acc:  0.7578125
train loss:  0.4661462903022766
train gradient:  0.11532795381763901
iteration : 8234
train acc:  0.71875
train loss:  0.5385037064552307
train gradient:  0.18042565616805112
iteration : 8235
train acc:  0.7265625
train loss:  0.5253981351852417
train gradient:  0.1472024175403857
iteration : 8236
train acc:  0.7265625
train loss:  0.5429884195327759
train gradient:  0.1291501870753262
iteration : 8237
train acc:  0.7890625
train loss:  0.4324988126754761
train gradient:  0.11728935095060374
iteration : 8238
train acc:  0.7265625
train loss:  0.5222355127334595
train gradient:  0.15940244767744666
iteration : 8239
train acc:  0.765625
train loss:  0.4877331256866455
train gradient:  0.1440227174653931
iteration : 8240
train acc:  0.6875
train loss:  0.5323964357376099
train gradient:  0.146028727503842
iteration : 8241
train acc:  0.765625
train loss:  0.5099015831947327
train gradient:  0.1271678570500579
iteration : 8242
train acc:  0.8125
train loss:  0.4552451968193054
train gradient:  0.12823067979935288
iteration : 8243
train acc:  0.7578125
train loss:  0.48120251297950745
train gradient:  0.1349001924959553
iteration : 8244
train acc:  0.7890625
train loss:  0.43838047981262207
train gradient:  0.10795000039336813
iteration : 8245
train acc:  0.75
train loss:  0.48411184549331665
train gradient:  0.12102818697449937
iteration : 8246
train acc:  0.6953125
train loss:  0.5116299390792847
train gradient:  0.19418218442403923
iteration : 8247
train acc:  0.7265625
train loss:  0.5801576375961304
train gradient:  0.1865546762571505
iteration : 8248
train acc:  0.796875
train loss:  0.4488534927368164
train gradient:  0.1430892115790578
iteration : 8249
train acc:  0.78125
train loss:  0.46749770641326904
train gradient:  0.13130023192851048
iteration : 8250
train acc:  0.7578125
train loss:  0.4509531855583191
train gradient:  0.10128989946384889
iteration : 8251
train acc:  0.7109375
train loss:  0.5220022201538086
train gradient:  0.12953142864342682
iteration : 8252
train acc:  0.8125
train loss:  0.4146387577056885
train gradient:  0.09511535727479081
iteration : 8253
train acc:  0.796875
train loss:  0.44699937105178833
train gradient:  0.11978531789752243
iteration : 8254
train acc:  0.7578125
train loss:  0.4847216308116913
train gradient:  0.11086995882432978
iteration : 8255
train acc:  0.703125
train loss:  0.4869941473007202
train gradient:  0.13270437111876116
iteration : 8256
train acc:  0.703125
train loss:  0.5593962669372559
train gradient:  0.18166221907007418
iteration : 8257
train acc:  0.78125
train loss:  0.43927377462387085
train gradient:  0.10479924294905589
iteration : 8258
train acc:  0.65625
train loss:  0.5418446063995361
train gradient:  0.1468351737982776
iteration : 8259
train acc:  0.734375
train loss:  0.5209592580795288
train gradient:  0.1423070400652759
iteration : 8260
train acc:  0.7421875
train loss:  0.47319310903549194
train gradient:  0.12767414639350058
iteration : 8261
train acc:  0.7109375
train loss:  0.6149818897247314
train gradient:  0.2238124411196168
iteration : 8262
train acc:  0.7578125
train loss:  0.495739609003067
train gradient:  0.13487735350094018
iteration : 8263
train acc:  0.734375
train loss:  0.5072720050811768
train gradient:  0.14794614473167994
iteration : 8264
train acc:  0.75
train loss:  0.477613627910614
train gradient:  0.1434271395496628
iteration : 8265
train acc:  0.703125
train loss:  0.5865002274513245
train gradient:  0.18815976151984753
iteration : 8266
train acc:  0.7421875
train loss:  0.522850513458252
train gradient:  0.15526448334543103
iteration : 8267
train acc:  0.7890625
train loss:  0.43028151988983154
train gradient:  0.09001443902880463
iteration : 8268
train acc:  0.7265625
train loss:  0.5013135671615601
train gradient:  0.12831280448464707
iteration : 8269
train acc:  0.7421875
train loss:  0.4597708582878113
train gradient:  0.11040230167248735
iteration : 8270
train acc:  0.75
train loss:  0.45929157733917236
train gradient:  0.11548128151736395
iteration : 8271
train acc:  0.7109375
train loss:  0.5011088848114014
train gradient:  0.12734713477574872
iteration : 8272
train acc:  0.7109375
train loss:  0.5106561183929443
train gradient:  0.1286933284100085
iteration : 8273
train acc:  0.6953125
train loss:  0.537903904914856
train gradient:  0.14855035252693946
iteration : 8274
train acc:  0.7265625
train loss:  0.5172145366668701
train gradient:  0.17817652743665474
iteration : 8275
train acc:  0.6796875
train loss:  0.5254596471786499
train gradient:  0.13245314012901666
iteration : 8276
train acc:  0.734375
train loss:  0.47760137915611267
train gradient:  0.1212434450837981
iteration : 8277
train acc:  0.8515625
train loss:  0.4234321117401123
train gradient:  0.08755517305283873
iteration : 8278
train acc:  0.7109375
train loss:  0.5456291437149048
train gradient:  0.17598021189893703
iteration : 8279
train acc:  0.7421875
train loss:  0.496060311794281
train gradient:  0.14063144848220194
iteration : 8280
train acc:  0.7109375
train loss:  0.5634196400642395
train gradient:  0.15089210291987823
iteration : 8281
train acc:  0.6953125
train loss:  0.5020465850830078
train gradient:  0.17472875513771308
iteration : 8282
train acc:  0.71875
train loss:  0.5130122303962708
train gradient:  0.12061128946605512
iteration : 8283
train acc:  0.7421875
train loss:  0.5206738114356995
train gradient:  0.1426748199519282
iteration : 8284
train acc:  0.7265625
train loss:  0.5015385150909424
train gradient:  0.1264293737951236
iteration : 8285
train acc:  0.7265625
train loss:  0.49564316868782043
train gradient:  0.14641431498964957
iteration : 8286
train acc:  0.7421875
train loss:  0.4634261131286621
train gradient:  0.1296147810391703
iteration : 8287
train acc:  0.7890625
train loss:  0.4551486372947693
train gradient:  0.11486523698702356
iteration : 8288
train acc:  0.7578125
train loss:  0.4510778486728668
train gradient:  0.13960241645354377
iteration : 8289
train acc:  0.6796875
train loss:  0.5535156726837158
train gradient:  0.14399365713326612
iteration : 8290
train acc:  0.734375
train loss:  0.4829352796077728
train gradient:  0.12159259663728876
iteration : 8291
train acc:  0.6796875
train loss:  0.5204223394393921
train gradient:  0.14559120908372103
iteration : 8292
train acc:  0.75
train loss:  0.5043675303459167
train gradient:  0.12852016410161282
iteration : 8293
train acc:  0.6796875
train loss:  0.5541755557060242
train gradient:  0.28105706927250207
iteration : 8294
train acc:  0.7265625
train loss:  0.4983383119106293
train gradient:  0.1519751824154277
iteration : 8295
train acc:  0.7421875
train loss:  0.4990983009338379
train gradient:  0.14213458335474768
iteration : 8296
train acc:  0.7109375
train loss:  0.48468366265296936
train gradient:  0.11014091099511146
iteration : 8297
train acc:  0.6484375
train loss:  0.5720224380493164
train gradient:  0.20230134511930312
iteration : 8298
train acc:  0.6875
train loss:  0.4996298849582672
train gradient:  0.13445885373173766
iteration : 8299
train acc:  0.7890625
train loss:  0.4352859854698181
train gradient:  0.11087975013388024
iteration : 8300
train acc:  0.7421875
train loss:  0.4934515357017517
train gradient:  0.11914979701375043
iteration : 8301
train acc:  0.734375
train loss:  0.4897367060184479
train gradient:  0.1306510163901172
iteration : 8302
train acc:  0.7265625
train loss:  0.48938822746276855
train gradient:  0.10881704924428841
iteration : 8303
train acc:  0.765625
train loss:  0.5007303357124329
train gradient:  0.14439983375013346
iteration : 8304
train acc:  0.734375
train loss:  0.49653834104537964
train gradient:  0.15540315584802666
iteration : 8305
train acc:  0.765625
train loss:  0.4639984965324402
train gradient:  0.09567328961657874
iteration : 8306
train acc:  0.734375
train loss:  0.543155312538147
train gradient:  0.16895878166903783
iteration : 8307
train acc:  0.71875
train loss:  0.520461916923523
train gradient:  0.1486846900935923
iteration : 8308
train acc:  0.71875
train loss:  0.5147393941879272
train gradient:  0.1242765636186409
iteration : 8309
train acc:  0.71875
train loss:  0.537343442440033
train gradient:  0.15096623121205383
iteration : 8310
train acc:  0.78125
train loss:  0.4893469214439392
train gradient:  0.1336612787225634
iteration : 8311
train acc:  0.78125
train loss:  0.434015691280365
train gradient:  0.09316006161422295
iteration : 8312
train acc:  0.765625
train loss:  0.5099148154258728
train gradient:  0.13148146818198247
iteration : 8313
train acc:  0.7265625
train loss:  0.5376085638999939
train gradient:  0.14019093608436095
iteration : 8314
train acc:  0.78125
train loss:  0.4419538974761963
train gradient:  0.12344254609684202
iteration : 8315
train acc:  0.625
train loss:  0.6203312873840332
train gradient:  0.1849479011189586
iteration : 8316
train acc:  0.765625
train loss:  0.43351784348487854
train gradient:  0.08546725291986884
iteration : 8317
train acc:  0.71875
train loss:  0.5022842884063721
train gradient:  0.14867768601189463
iteration : 8318
train acc:  0.796875
train loss:  0.48514673113822937
train gradient:  0.1301254618305417
iteration : 8319
train acc:  0.71875
train loss:  0.5448302626609802
train gradient:  0.19173986938444515
iteration : 8320
train acc:  0.6953125
train loss:  0.5172266364097595
train gradient:  0.15956146703595436
iteration : 8321
train acc:  0.75
train loss:  0.5219932198524475
train gradient:  0.13616134039619848
iteration : 8322
train acc:  0.7578125
train loss:  0.4485059976577759
train gradient:  0.1220934977149768
iteration : 8323
train acc:  0.703125
train loss:  0.5353950262069702
train gradient:  0.16289348704205114
iteration : 8324
train acc:  0.7421875
train loss:  0.5083378553390503
train gradient:  0.12892490890029631
iteration : 8325
train acc:  0.765625
train loss:  0.46868860721588135
train gradient:  0.1407605696405807
iteration : 8326
train acc:  0.6875
train loss:  0.5713725090026855
train gradient:  0.1640047620516384
iteration : 8327
train acc:  0.75
train loss:  0.4738907516002655
train gradient:  0.1313635944514532
iteration : 8328
train acc:  0.7421875
train loss:  0.509360671043396
train gradient:  0.13323405109808645
iteration : 8329
train acc:  0.734375
train loss:  0.49684152007102966
train gradient:  0.1302424385062836
iteration : 8330
train acc:  0.7578125
train loss:  0.4977952241897583
train gradient:  0.17144953761920037
iteration : 8331
train acc:  0.734375
train loss:  0.5075802803039551
train gradient:  0.12503672144394762
iteration : 8332
train acc:  0.7265625
train loss:  0.5252977609634399
train gradient:  0.18063830302713074
iteration : 8333
train acc:  0.7578125
train loss:  0.46783629059791565
train gradient:  0.11823284702333377
iteration : 8334
train acc:  0.8671875
train loss:  0.3658679127693176
train gradient:  0.07817334510895238
iteration : 8335
train acc:  0.7421875
train loss:  0.5831026434898376
train gradient:  0.15139905781254798
iteration : 8336
train acc:  0.78125
train loss:  0.45557695627212524
train gradient:  0.13128444113604265
iteration : 8337
train acc:  0.7265625
train loss:  0.5226081013679504
train gradient:  0.15850939582841345
iteration : 8338
train acc:  0.78125
train loss:  0.45454803109169006
train gradient:  0.10589149782253009
iteration : 8339
train acc:  0.796875
train loss:  0.45104098320007324
train gradient:  0.14176494653993385
iteration : 8340
train acc:  0.6796875
train loss:  0.5560838580131531
train gradient:  0.19012260936963904
iteration : 8341
train acc:  0.7734375
train loss:  0.4419807493686676
train gradient:  0.10225054756515885
iteration : 8342
train acc:  0.6875
train loss:  0.5007489919662476
train gradient:  0.1276412431817714
iteration : 8343
train acc:  0.703125
train loss:  0.525679349899292
train gradient:  0.18075000396029842
iteration : 8344
train acc:  0.71875
train loss:  0.5198550224304199
train gradient:  0.23175740928717342
iteration : 8345
train acc:  0.6875
train loss:  0.572182297706604
train gradient:  0.21594853642990094
iteration : 8346
train acc:  0.7109375
train loss:  0.515438973903656
train gradient:  0.17364129138239337
iteration : 8347
train acc:  0.71875
train loss:  0.48987147212028503
train gradient:  0.18632881556900338
iteration : 8348
train acc:  0.6875
train loss:  0.6226231455802917
train gradient:  0.227034782026312
iteration : 8349
train acc:  0.7421875
train loss:  0.4558870792388916
train gradient:  0.10486330705482501
iteration : 8350
train acc:  0.734375
train loss:  0.5492878556251526
train gradient:  0.18631579066625392
iteration : 8351
train acc:  0.6796875
train loss:  0.5720922946929932
train gradient:  0.17317940447294045
iteration : 8352
train acc:  0.7734375
train loss:  0.4886632561683655
train gradient:  0.17515693410867972
iteration : 8353
train acc:  0.78125
train loss:  0.4846493899822235
train gradient:  0.12677575285057735
iteration : 8354
train acc:  0.8125
train loss:  0.46366822719573975
train gradient:  0.12423173098567812
iteration : 8355
train acc:  0.703125
train loss:  0.5498876571655273
train gradient:  0.14654025038957347
iteration : 8356
train acc:  0.671875
train loss:  0.5600820779800415
train gradient:  0.20097867383198167
iteration : 8357
train acc:  0.703125
train loss:  0.5634939670562744
train gradient:  0.19921884511512533
iteration : 8358
train acc:  0.734375
train loss:  0.5186368227005005
train gradient:  0.16490847772566736
iteration : 8359
train acc:  0.8125
train loss:  0.480131059885025
train gradient:  0.11578330085516995
iteration : 8360
train acc:  0.8125
train loss:  0.4602735638618469
train gradient:  0.12402161145447069
iteration : 8361
train acc:  0.7890625
train loss:  0.4480140805244446
train gradient:  0.14408687492187103
iteration : 8362
train acc:  0.765625
train loss:  0.48858505487442017
train gradient:  0.1241115128477893
iteration : 8363
train acc:  0.7421875
train loss:  0.4985133707523346
train gradient:  0.12629614609355477
iteration : 8364
train acc:  0.7109375
train loss:  0.5213536620140076
train gradient:  0.13561711934294762
iteration : 8365
train acc:  0.765625
train loss:  0.4964321255683899
train gradient:  0.16177929838447785
iteration : 8366
train acc:  0.75
train loss:  0.5315269231796265
train gradient:  0.14035265144950126
iteration : 8367
train acc:  0.765625
train loss:  0.4642117917537689
train gradient:  0.1139068520672495
iteration : 8368
train acc:  0.7734375
train loss:  0.4335860311985016
train gradient:  0.12428972985446697
iteration : 8369
train acc:  0.6796875
train loss:  0.5970661044120789
train gradient:  0.20043432788229237
iteration : 8370
train acc:  0.78125
train loss:  0.450772762298584
train gradient:  0.11563580190203465
iteration : 8371
train acc:  0.765625
train loss:  0.4643784165382385
train gradient:  0.08780854113353914
iteration : 8372
train acc:  0.8046875
train loss:  0.42233964800834656
train gradient:  0.10495762155680095
iteration : 8373
train acc:  0.7265625
train loss:  0.5496573448181152
train gradient:  0.1179073232597042
iteration : 8374
train acc:  0.75
train loss:  0.4731943607330322
train gradient:  0.10158441710061417
iteration : 8375
train acc:  0.7890625
train loss:  0.470797598361969
train gradient:  0.11981328084586852
iteration : 8376
train acc:  0.7109375
train loss:  0.4876750409603119
train gradient:  0.14384055376973476
iteration : 8377
train acc:  0.7734375
train loss:  0.5294767618179321
train gradient:  0.12478059041240307
iteration : 8378
train acc:  0.703125
train loss:  0.562840461730957
train gradient:  0.18394773604265396
iteration : 8379
train acc:  0.765625
train loss:  0.49931836128234863
train gradient:  0.11188820518174232
iteration : 8380
train acc:  0.7421875
train loss:  0.5411450862884521
train gradient:  0.19862467242715875
iteration : 8381
train acc:  0.7421875
train loss:  0.5005331039428711
train gradient:  0.10468503273017642
iteration : 8382
train acc:  0.7578125
train loss:  0.4753313660621643
train gradient:  0.13386290062923756
iteration : 8383
train acc:  0.7265625
train loss:  0.5389242172241211
train gradient:  0.19042072995490386
iteration : 8384
train acc:  0.6640625
train loss:  0.5163434147834778
train gradient:  0.14084296730879567
iteration : 8385
train acc:  0.78125
train loss:  0.4330151081085205
train gradient:  0.0906936694368847
iteration : 8386
train acc:  0.7578125
train loss:  0.5157196521759033
train gradient:  0.15229721202520125
iteration : 8387
train acc:  0.7265625
train loss:  0.5407314896583557
train gradient:  0.25000137372325437
iteration : 8388
train acc:  0.734375
train loss:  0.5161026120185852
train gradient:  0.13433906183789995
iteration : 8389
train acc:  0.8046875
train loss:  0.4544855058193207
train gradient:  0.09450383154852794
iteration : 8390
train acc:  0.734375
train loss:  0.5517405867576599
train gradient:  0.19545118540519407
iteration : 8391
train acc:  0.7578125
train loss:  0.48797231912612915
train gradient:  0.11759710399179515
iteration : 8392
train acc:  0.75
train loss:  0.4607768654823303
train gradient:  0.10226008989464498
iteration : 8393
train acc:  0.765625
train loss:  0.48507362604141235
train gradient:  0.16754949680748626
iteration : 8394
train acc:  0.7421875
train loss:  0.5234847068786621
train gradient:  0.14295879771743103
iteration : 8395
train acc:  0.75
train loss:  0.5500259399414062
train gradient:  0.15716494035684198
iteration : 8396
train acc:  0.765625
train loss:  0.46200767159461975
train gradient:  0.09796866790101513
iteration : 8397
train acc:  0.6484375
train loss:  0.5863158702850342
train gradient:  0.1465560905794421
iteration : 8398
train acc:  0.703125
train loss:  0.5268298387527466
train gradient:  0.18302703756828753
iteration : 8399
train acc:  0.7578125
train loss:  0.4843883216381073
train gradient:  0.10862638793469308
iteration : 8400
train acc:  0.734375
train loss:  0.5031020641326904
train gradient:  0.131350645607546
iteration : 8401
train acc:  0.7578125
train loss:  0.5028285980224609
train gradient:  0.12250654194191322
iteration : 8402
train acc:  0.78125
train loss:  0.4433005452156067
train gradient:  0.10708009941940762
iteration : 8403
train acc:  0.7421875
train loss:  0.4442141652107239
train gradient:  0.11420300218420323
iteration : 8404
train acc:  0.7734375
train loss:  0.4636790156364441
train gradient:  0.12725652446628938
iteration : 8405
train acc:  0.7578125
train loss:  0.493791401386261
train gradient:  0.14639041928704305
iteration : 8406
train acc:  0.765625
train loss:  0.49036258459091187
train gradient:  0.12627160225303669
iteration : 8407
train acc:  0.8125
train loss:  0.39283400774002075
train gradient:  0.07984976675540757
iteration : 8408
train acc:  0.7265625
train loss:  0.516030490398407
train gradient:  0.12331598865063102
iteration : 8409
train acc:  0.7890625
train loss:  0.43688464164733887
train gradient:  0.08300439996969602
iteration : 8410
train acc:  0.7265625
train loss:  0.5775417685508728
train gradient:  0.18992198985879544
iteration : 8411
train acc:  0.796875
train loss:  0.44033223390579224
train gradient:  0.10410607084543766
iteration : 8412
train acc:  0.734375
train loss:  0.5029008388519287
train gradient:  0.10660082425872552
iteration : 8413
train acc:  0.734375
train loss:  0.5457783341407776
train gradient:  0.1472306047899467
iteration : 8414
train acc:  0.734375
train loss:  0.4662185311317444
train gradient:  0.10566695830278129
iteration : 8415
train acc:  0.75
train loss:  0.47774410247802734
train gradient:  0.1199450859240357
iteration : 8416
train acc:  0.6875
train loss:  0.5699547529220581
train gradient:  0.1919747150242132
iteration : 8417
train acc:  0.7734375
train loss:  0.44470474123954773
train gradient:  0.12575354677019085
iteration : 8418
train acc:  0.765625
train loss:  0.5046934485435486
train gradient:  0.1096583851703364
iteration : 8419
train acc:  0.703125
train loss:  0.5563760995864868
train gradient:  0.128376933556905
iteration : 8420
train acc:  0.6640625
train loss:  0.5785573720932007
train gradient:  0.1755370434959252
iteration : 8421
train acc:  0.703125
train loss:  0.5267871618270874
train gradient:  0.15983379731548691
iteration : 8422
train acc:  0.703125
train loss:  0.5435612797737122
train gradient:  0.13283847628737472
iteration : 8423
train acc:  0.71875
train loss:  0.4909141957759857
train gradient:  0.10546999462116662
iteration : 8424
train acc:  0.65625
train loss:  0.595772385597229
train gradient:  0.21996027301732363
iteration : 8425
train acc:  0.8125
train loss:  0.4325368106365204
train gradient:  0.10067608537216177
iteration : 8426
train acc:  0.734375
train loss:  0.4756048321723938
train gradient:  0.1384978610076332
iteration : 8427
train acc:  0.7109375
train loss:  0.5177077054977417
train gradient:  0.13306845146506963
iteration : 8428
train acc:  0.734375
train loss:  0.48559749126434326
train gradient:  0.1014845206723819
iteration : 8429
train acc:  0.7890625
train loss:  0.4289723038673401
train gradient:  0.1031883157695517
iteration : 8430
train acc:  0.7109375
train loss:  0.5441782474517822
train gradient:  0.2021380144680096
iteration : 8431
train acc:  0.7578125
train loss:  0.5153913497924805
train gradient:  0.12847089571443948
iteration : 8432
train acc:  0.7734375
train loss:  0.43162626028060913
train gradient:  0.12440927687280694
iteration : 8433
train acc:  0.7109375
train loss:  0.4913710355758667
train gradient:  0.16776070787027947
iteration : 8434
train acc:  0.734375
train loss:  0.49992936849594116
train gradient:  0.11778217084610779
iteration : 8435
train acc:  0.7421875
train loss:  0.5211265087127686
train gradient:  0.15026941930526044
iteration : 8436
train acc:  0.7109375
train loss:  0.500551164150238
train gradient:  0.1506898067273675
iteration : 8437
train acc:  0.6953125
train loss:  0.4681105315685272
train gradient:  0.09884155458343774
iteration : 8438
train acc:  0.7421875
train loss:  0.4618832767009735
train gradient:  0.12436496811399764
iteration : 8439
train acc:  0.7421875
train loss:  0.5120287537574768
train gradient:  0.14083864568848486
iteration : 8440
train acc:  0.7109375
train loss:  0.5294719338417053
train gradient:  0.15509492760784255
iteration : 8441
train acc:  0.703125
train loss:  0.5247479677200317
train gradient:  0.15492574233283168
iteration : 8442
train acc:  0.6875
train loss:  0.5697548985481262
train gradient:  0.2020805620688228
iteration : 8443
train acc:  0.703125
train loss:  0.49824079871177673
train gradient:  0.1630393699996061
iteration : 8444
train acc:  0.7109375
train loss:  0.5366514921188354
train gradient:  0.10643554994338172
iteration : 8445
train acc:  0.7265625
train loss:  0.5323793888092041
train gradient:  0.14259517933825214
iteration : 8446
train acc:  0.7578125
train loss:  0.458045095205307
train gradient:  0.12339239779962208
iteration : 8447
train acc:  0.734375
train loss:  0.5235145092010498
train gradient:  0.1280744108762394
iteration : 8448
train acc:  0.671875
train loss:  0.6058000326156616
train gradient:  0.19182159348002525
iteration : 8449
train acc:  0.765625
train loss:  0.4396281838417053
train gradient:  0.12418332537764504
iteration : 8450
train acc:  0.71875
train loss:  0.5526965856552124
train gradient:  0.13349412435392058
iteration : 8451
train acc:  0.78125
train loss:  0.5097815990447998
train gradient:  0.15171703196052252
iteration : 8452
train acc:  0.765625
train loss:  0.4842846989631653
train gradient:  0.13473035643034487
iteration : 8453
train acc:  0.7578125
train loss:  0.47122639417648315
train gradient:  0.10768116587024704
iteration : 8454
train acc:  0.6875
train loss:  0.5374549627304077
train gradient:  0.1363960236072801
iteration : 8455
train acc:  0.75
train loss:  0.4731682240962982
train gradient:  0.13187176927593108
iteration : 8456
train acc:  0.7265625
train loss:  0.5180894136428833
train gradient:  0.12800883776773853
iteration : 8457
train acc:  0.71875
train loss:  0.5402499437332153
train gradient:  0.14363594701598553
iteration : 8458
train acc:  0.734375
train loss:  0.538223922252655
train gradient:  0.20235732847450516
iteration : 8459
train acc:  0.7265625
train loss:  0.5556585788726807
train gradient:  0.1733296444350384
iteration : 8460
train acc:  0.703125
train loss:  0.5383760333061218
train gradient:  0.1990935291792585
iteration : 8461
train acc:  0.671875
train loss:  0.5309551954269409
train gradient:  0.1766100884971968
iteration : 8462
train acc:  0.6953125
train loss:  0.5485681295394897
train gradient:  0.15704704705686204
iteration : 8463
train acc:  0.78125
train loss:  0.4780014753341675
train gradient:  0.1379267821817321
iteration : 8464
train acc:  0.7421875
train loss:  0.5202568769454956
train gradient:  0.15679481150191266
iteration : 8465
train acc:  0.7890625
train loss:  0.46319371461868286
train gradient:  0.14260563100925389
iteration : 8466
train acc:  0.6796875
train loss:  0.553094744682312
train gradient:  0.15839873579179198
iteration : 8467
train acc:  0.734375
train loss:  0.533439576625824
train gradient:  0.15673101822472665
iteration : 8468
train acc:  0.734375
train loss:  0.4803614318370819
train gradient:  0.11313167000330235
iteration : 8469
train acc:  0.8046875
train loss:  0.4676785469055176
train gradient:  0.13568025906522674
iteration : 8470
train acc:  0.6796875
train loss:  0.5857924818992615
train gradient:  0.19443899607655069
iteration : 8471
train acc:  0.6953125
train loss:  0.6169048547744751
train gradient:  0.20525524131983308
iteration : 8472
train acc:  0.7578125
train loss:  0.4139832854270935
train gradient:  0.10103318755768875
iteration : 8473
train acc:  0.7265625
train loss:  0.4837583601474762
train gradient:  0.12653603548438447
iteration : 8474
train acc:  0.7734375
train loss:  0.49806085228919983
train gradient:  0.1349778486309089
iteration : 8475
train acc:  0.78125
train loss:  0.4659389555454254
train gradient:  0.11577403863344654
iteration : 8476
train acc:  0.71875
train loss:  0.5223058462142944
train gradient:  0.11826111066806293
iteration : 8477
train acc:  0.75
train loss:  0.4384193420410156
train gradient:  0.12784409282980363
iteration : 8478
train acc:  0.7421875
train loss:  0.4855921268463135
train gradient:  0.12197669992616565
iteration : 8479
train acc:  0.75
train loss:  0.4923626184463501
train gradient:  0.10520317124812321
iteration : 8480
train acc:  0.7578125
train loss:  0.5503653287887573
train gradient:  0.13488717375296044
iteration : 8481
train acc:  0.734375
train loss:  0.48939961194992065
train gradient:  0.13755881326045855
iteration : 8482
train acc:  0.671875
train loss:  0.5408719778060913
train gradient:  0.17444179858062234
iteration : 8483
train acc:  0.7421875
train loss:  0.5109058618545532
train gradient:  0.15025775970832234
iteration : 8484
train acc:  0.7890625
train loss:  0.44493770599365234
train gradient:  0.11294265124292076
iteration : 8485
train acc:  0.6796875
train loss:  0.5414775609970093
train gradient:  0.16149451381441693
iteration : 8486
train acc:  0.734375
train loss:  0.48223868012428284
train gradient:  0.1463434346530913
iteration : 8487
train acc:  0.7578125
train loss:  0.4846562445163727
train gradient:  0.13036147492527178
iteration : 8488
train acc:  0.8125
train loss:  0.4353867173194885
train gradient:  0.1027771106506299
iteration : 8489
train acc:  0.7265625
train loss:  0.47671064734458923
train gradient:  0.10438738768954725
iteration : 8490
train acc:  0.78125
train loss:  0.45036035776138306
train gradient:  0.10624848813524215
iteration : 8491
train acc:  0.671875
train loss:  0.5748129487037659
train gradient:  0.1522778069418042
iteration : 8492
train acc:  0.6875
train loss:  0.4998238682746887
train gradient:  0.10162689093152964
iteration : 8493
train acc:  0.703125
train loss:  0.5012053847312927
train gradient:  0.1362251516287143
iteration : 8494
train acc:  0.7421875
train loss:  0.5241379737854004
train gradient:  0.11863015078027997
iteration : 8495
train acc:  0.7265625
train loss:  0.5512951612472534
train gradient:  0.21266684914584089
iteration : 8496
train acc:  0.734375
train loss:  0.45165467262268066
train gradient:  0.1218574645109921
iteration : 8497
train acc:  0.734375
train loss:  0.5209232568740845
train gradient:  0.1323067509459292
iteration : 8498
train acc:  0.78125
train loss:  0.5160543918609619
train gradient:  0.15220315671941365
iteration : 8499
train acc:  0.765625
train loss:  0.48775577545166016
train gradient:  0.1594711382804315
iteration : 8500
train acc:  0.71875
train loss:  0.5104604959487915
train gradient:  0.1539537833959865
iteration : 8501
train acc:  0.75
train loss:  0.569533109664917
train gradient:  0.15921335083411553
iteration : 8502
train acc:  0.7734375
train loss:  0.4773368239402771
train gradient:  0.15877651228890396
iteration : 8503
train acc:  0.796875
train loss:  0.4519774615764618
train gradient:  0.11334853454338048
iteration : 8504
train acc:  0.6875
train loss:  0.5156542062759399
train gradient:  0.1560597175190634
iteration : 8505
train acc:  0.75
train loss:  0.4232689142227173
train gradient:  0.09525877329127692
iteration : 8506
train acc:  0.734375
train loss:  0.4985119104385376
train gradient:  0.14360080817972823
iteration : 8507
train acc:  0.703125
train loss:  0.4976944327354431
train gradient:  0.12422563042918115
iteration : 8508
train acc:  0.703125
train loss:  0.5003527998924255
train gradient:  0.1422268536317681
iteration : 8509
train acc:  0.734375
train loss:  0.5379966497421265
train gradient:  0.17942618663516546
iteration : 8510
train acc:  0.6953125
train loss:  0.6089389324188232
train gradient:  0.18503082648419975
iteration : 8511
train acc:  0.765625
train loss:  0.5054532289505005
train gradient:  0.13416361169592578
iteration : 8512
train acc:  0.7109375
train loss:  0.4944990277290344
train gradient:  0.12334119342701977
iteration : 8513
train acc:  0.7421875
train loss:  0.5089937448501587
train gradient:  0.17330253283215935
iteration : 8514
train acc:  0.734375
train loss:  0.5298048853874207
train gradient:  0.13406916599760005
iteration : 8515
train acc:  0.71875
train loss:  0.4908359944820404
train gradient:  0.13183663855505215
iteration : 8516
train acc:  0.7109375
train loss:  0.5079169273376465
train gradient:  0.1675775988905569
iteration : 8517
train acc:  0.6953125
train loss:  0.5194005966186523
train gradient:  0.14661671705807422
iteration : 8518
train acc:  0.7265625
train loss:  0.5131267309188843
train gradient:  0.1335034395213695
iteration : 8519
train acc:  0.6875
train loss:  0.5750266313552856
train gradient:  0.15326900597449733
iteration : 8520
train acc:  0.7734375
train loss:  0.4683073163032532
train gradient:  0.10888195632460382
iteration : 8521
train acc:  0.8046875
train loss:  0.4674640893936157
train gradient:  0.11797265455629999
iteration : 8522
train acc:  0.84375
train loss:  0.3945313096046448
train gradient:  0.07834765911513787
iteration : 8523
train acc:  0.6796875
train loss:  0.5809508562088013
train gradient:  0.18632229014725993
iteration : 8524
train acc:  0.78125
train loss:  0.4505310654640198
train gradient:  0.12988033166664525
iteration : 8525
train acc:  0.703125
train loss:  0.530136227607727
train gradient:  0.16960686162258373
iteration : 8526
train acc:  0.7421875
train loss:  0.5289168357849121
train gradient:  0.12702924007159175
iteration : 8527
train acc:  0.6953125
train loss:  0.5082556009292603
train gradient:  0.14161608758576988
iteration : 8528
train acc:  0.734375
train loss:  0.48611336946487427
train gradient:  0.12167631321675458
iteration : 8529
train acc:  0.78125
train loss:  0.48510172963142395
train gradient:  0.1145804760256401
iteration : 8530
train acc:  0.6953125
train loss:  0.5359588861465454
train gradient:  0.15200236448418103
iteration : 8531
train acc:  0.765625
train loss:  0.47292715311050415
train gradient:  0.11373523956600744
iteration : 8532
train acc:  0.75
train loss:  0.5016581416130066
train gradient:  0.11976554505829895
iteration : 8533
train acc:  0.71875
train loss:  0.48595091700553894
train gradient:  0.11705624898433707
iteration : 8534
train acc:  0.7578125
train loss:  0.5262192487716675
train gradient:  0.14701217520668436
iteration : 8535
train acc:  0.7890625
train loss:  0.4375006854534149
train gradient:  0.09636160195012
iteration : 8536
train acc:  0.6796875
train loss:  0.5533124208450317
train gradient:  0.12437201573773486
iteration : 8537
train acc:  0.6953125
train loss:  0.4938848614692688
train gradient:  0.11880116395670766
iteration : 8538
train acc:  0.6328125
train loss:  0.571483314037323
train gradient:  0.1533169775710571
iteration : 8539
train acc:  0.6640625
train loss:  0.6009786128997803
train gradient:  0.1490331381711813
iteration : 8540
train acc:  0.7734375
train loss:  0.43681520223617554
train gradient:  0.11808548173674951
iteration : 8541
train acc:  0.7265625
train loss:  0.5069688558578491
train gradient:  0.14662791050378413
iteration : 8542
train acc:  0.7734375
train loss:  0.47619718313217163
train gradient:  0.13243005143854727
iteration : 8543
train acc:  0.7109375
train loss:  0.5520300269126892
train gradient:  0.14761556359276795
iteration : 8544
train acc:  0.7421875
train loss:  0.46733689308166504
train gradient:  0.12273542617687148
iteration : 8545
train acc:  0.796875
train loss:  0.43016061186790466
train gradient:  0.1102285790937867
iteration : 8546
train acc:  0.78125
train loss:  0.45926129817962646
train gradient:  0.08898567421087392
iteration : 8547
train acc:  0.75
train loss:  0.5060830116271973
train gradient:  0.1316878695708425
iteration : 8548
train acc:  0.765625
train loss:  0.491788387298584
train gradient:  0.11953989958761088
iteration : 8549
train acc:  0.8046875
train loss:  0.4470345973968506
train gradient:  0.109880542603341
iteration : 8550
train acc:  0.7734375
train loss:  0.5335817337036133
train gradient:  0.1570082542300727
iteration : 8551
train acc:  0.796875
train loss:  0.428617000579834
train gradient:  0.1280844439296826
iteration : 8552
train acc:  0.7578125
train loss:  0.46490854024887085
train gradient:  0.10818465411537252
iteration : 8553
train acc:  0.7109375
train loss:  0.5279350280761719
train gradient:  0.12149843911507144
iteration : 8554
train acc:  0.6875
train loss:  0.5607679486274719
train gradient:  0.15942580813697332
iteration : 8555
train acc:  0.7578125
train loss:  0.5189002752304077
train gradient:  0.12697490784726476
iteration : 8556
train acc:  0.734375
train loss:  0.5632333755493164
train gradient:  0.16598953698596536
iteration : 8557
train acc:  0.75
train loss:  0.458636999130249
train gradient:  0.10544943264008291
iteration : 8558
train acc:  0.734375
train loss:  0.549512505531311
train gradient:  0.1345936113432703
iteration : 8559
train acc:  0.734375
train loss:  0.4977003037929535
train gradient:  0.10912944666848987
iteration : 8560
train acc:  0.734375
train loss:  0.4955577850341797
train gradient:  0.13099400165028102
iteration : 8561
train acc:  0.6953125
train loss:  0.5364961624145508
train gradient:  0.1524323068280513
iteration : 8562
train acc:  0.7578125
train loss:  0.4600166082382202
train gradient:  0.15047517708483013
iteration : 8563
train acc:  0.7421875
train loss:  0.5564422607421875
train gradient:  0.14475581561264309
iteration : 8564
train acc:  0.75
train loss:  0.48241037130355835
train gradient:  0.10909934275269408
iteration : 8565
train acc:  0.78125
train loss:  0.4601559340953827
train gradient:  0.09605299192892872
iteration : 8566
train acc:  0.75
train loss:  0.4735831022262573
train gradient:  0.14071929895340562
iteration : 8567
train acc:  0.8046875
train loss:  0.4780597388744354
train gradient:  0.12443999231749574
iteration : 8568
train acc:  0.78125
train loss:  0.4478839337825775
train gradient:  0.1304329137554988
iteration : 8569
train acc:  0.7265625
train loss:  0.489905446767807
train gradient:  0.15190646091351623
iteration : 8570
train acc:  0.7578125
train loss:  0.4213128685951233
train gradient:  0.12055834714132799
iteration : 8571
train acc:  0.7578125
train loss:  0.5266997218132019
train gradient:  0.15037822367413323
iteration : 8572
train acc:  0.78125
train loss:  0.4650192856788635
train gradient:  0.10222758990780169
iteration : 8573
train acc:  0.8125
train loss:  0.4993847608566284
train gradient:  0.15853377373464347
iteration : 8574
train acc:  0.6875
train loss:  0.5344294905662537
train gradient:  0.14816305006077396
iteration : 8575
train acc:  0.7421875
train loss:  0.5431451797485352
train gradient:  0.16492654811953827
iteration : 8576
train acc:  0.71875
train loss:  0.49802541732788086
train gradient:  0.1414327242330845
iteration : 8577
train acc:  0.640625
train loss:  0.6368172764778137
train gradient:  0.21504063561559061
iteration : 8578
train acc:  0.71875
train loss:  0.45893406867980957
train gradient:  0.10190627182090886
iteration : 8579
train acc:  0.75
train loss:  0.5093648433685303
train gradient:  0.14871267625641793
iteration : 8580
train acc:  0.75
train loss:  0.5104711055755615
train gradient:  0.14965559148900176
iteration : 8581
train acc:  0.7265625
train loss:  0.5173940062522888
train gradient:  0.14395600890072108
iteration : 8582
train acc:  0.765625
train loss:  0.48554110527038574
train gradient:  0.13503678516192985
iteration : 8583
train acc:  0.796875
train loss:  0.46550309658050537
train gradient:  0.09935718639554277
iteration : 8584
train acc:  0.6875
train loss:  0.6042131781578064
train gradient:  0.1785015738647605
iteration : 8585
train acc:  0.765625
train loss:  0.4795435070991516
train gradient:  0.11465249634417009
iteration : 8586
train acc:  0.7109375
train loss:  0.5159435868263245
train gradient:  0.13529217254525305
iteration : 8587
train acc:  0.7265625
train loss:  0.5243635177612305
train gradient:  0.11374851395589664
iteration : 8588
train acc:  0.7265625
train loss:  0.5174199342727661
train gradient:  0.12397512648695849
iteration : 8589
train acc:  0.75
train loss:  0.4788406789302826
train gradient:  0.12664654639615697
iteration : 8590
train acc:  0.7421875
train loss:  0.4924795627593994
train gradient:  0.14610422138458032
iteration : 8591
train acc:  0.7109375
train loss:  0.5412353277206421
train gradient:  0.12163426638667173
iteration : 8592
train acc:  0.71875
train loss:  0.5225986242294312
train gradient:  0.1356422429627794
iteration : 8593
train acc:  0.6875
train loss:  0.6086228489875793
train gradient:  0.1900829256021362
iteration : 8594
train acc:  0.75
train loss:  0.4851646423339844
train gradient:  0.14399832337135587
iteration : 8595
train acc:  0.7578125
train loss:  0.4596421420574188
train gradient:  0.10398839247285384
iteration : 8596
train acc:  0.75
train loss:  0.5321427583694458
train gradient:  0.1315901392151083
iteration : 8597
train acc:  0.765625
train loss:  0.44132962822914124
train gradient:  0.11189825865337126
iteration : 8598
train acc:  0.671875
train loss:  0.5960015654563904
train gradient:  0.15844068366748876
iteration : 8599
train acc:  0.75
train loss:  0.473227322101593
train gradient:  0.14311420303418398
iteration : 8600
train acc:  0.6796875
train loss:  0.5491757392883301
train gradient:  0.16135612616360478
iteration : 8601
train acc:  0.71875
train loss:  0.5193774104118347
train gradient:  0.14730073392751508
iteration : 8602
train acc:  0.71875
train loss:  0.6125168204307556
train gradient:  0.17944859629411564
iteration : 8603
train acc:  0.7421875
train loss:  0.5026266574859619
train gradient:  0.13516461862418777
iteration : 8604
train acc:  0.7421875
train loss:  0.502573549747467
train gradient:  0.12683196292121363
iteration : 8605
train acc:  0.78125
train loss:  0.5046259164810181
train gradient:  0.16383333553156498
iteration : 8606
train acc:  0.7109375
train loss:  0.4692649245262146
train gradient:  0.1220475163601134
iteration : 8607
train acc:  0.78125
train loss:  0.43380099534988403
train gradient:  0.1200233488378951
iteration : 8608
train acc:  0.7265625
train loss:  0.4905203878879547
train gradient:  0.12938447114147822
iteration : 8609
train acc:  0.7421875
train loss:  0.5480628609657288
train gradient:  0.13208817861359995
iteration : 8610
train acc:  0.7421875
train loss:  0.5014981627464294
train gradient:  0.13617142634304724
iteration : 8611
train acc:  0.765625
train loss:  0.503024697303772
train gradient:  0.13511131109367036
iteration : 8612
train acc:  0.75
train loss:  0.5360698103904724
train gradient:  0.14776544642388473
iteration : 8613
train acc:  0.7109375
train loss:  0.5141925811767578
train gradient:  0.1519352364316403
iteration : 8614
train acc:  0.765625
train loss:  0.4875032305717468
train gradient:  0.13648916821806262
iteration : 8615
train acc:  0.6875
train loss:  0.5092278718948364
train gradient:  0.1266833729293388
iteration : 8616
train acc:  0.75
train loss:  0.48109322786331177
train gradient:  0.12335533703780033
iteration : 8617
train acc:  0.703125
train loss:  0.5029476881027222
train gradient:  0.13353769982280916
iteration : 8618
train acc:  0.8046875
train loss:  0.4159711003303528
train gradient:  0.08140823178383068
iteration : 8619
train acc:  0.8359375
train loss:  0.4021887183189392
train gradient:  0.09644942450067114
iteration : 8620
train acc:  0.734375
train loss:  0.5176249742507935
train gradient:  0.12972565339952608
iteration : 8621
train acc:  0.8125
train loss:  0.4553712010383606
train gradient:  0.10871879280067177
iteration : 8622
train acc:  0.734375
train loss:  0.48569273948669434
train gradient:  0.1320624283663376
iteration : 8623
train acc:  0.7578125
train loss:  0.4583565592765808
train gradient:  0.13958743956866532
iteration : 8624
train acc:  0.71875
train loss:  0.5097484588623047
train gradient:  0.1520785338588351
iteration : 8625
train acc:  0.796875
train loss:  0.4330533742904663
train gradient:  0.152382436699655
iteration : 8626
train acc:  0.71875
train loss:  0.5001906156539917
train gradient:  0.12823042283411168
iteration : 8627
train acc:  0.7265625
train loss:  0.49946457147598267
train gradient:  0.12578638627161337
iteration : 8628
train acc:  0.796875
train loss:  0.4386013150215149
train gradient:  0.09471952486056524
iteration : 8629
train acc:  0.75
train loss:  0.5127402544021606
train gradient:  0.1282302781632642
iteration : 8630
train acc:  0.7734375
train loss:  0.47519683837890625
train gradient:  0.12320292661235803
iteration : 8631
train acc:  0.7578125
train loss:  0.5216387510299683
train gradient:  0.12755840258094525
iteration : 8632
train acc:  0.7578125
train loss:  0.5064314603805542
train gradient:  0.142185622309288
iteration : 8633
train acc:  0.78125
train loss:  0.4630718231201172
train gradient:  0.11093829365455582
iteration : 8634
train acc:  0.7578125
train loss:  0.5387473106384277
train gradient:  0.15457997060376427
iteration : 8635
train acc:  0.71875
train loss:  0.49747389554977417
train gradient:  0.13565669952322862
iteration : 8636
train acc:  0.7109375
train loss:  0.49572670459747314
train gradient:  0.14190166039978647
iteration : 8637
train acc:  0.703125
train loss:  0.5151948928833008
train gradient:  0.11198149865300976
iteration : 8638
train acc:  0.765625
train loss:  0.4763374626636505
train gradient:  0.11614575263953224
iteration : 8639
train acc:  0.7421875
train loss:  0.5000963807106018
train gradient:  0.14707706390910238
iteration : 8640
train acc:  0.78125
train loss:  0.4816230535507202
train gradient:  0.11521045585375941
iteration : 8641
train acc:  0.7265625
train loss:  0.5237667560577393
train gradient:  0.13960058585815022
iteration : 8642
train acc:  0.6328125
train loss:  0.5512399673461914
train gradient:  0.1404699888132805
iteration : 8643
train acc:  0.7734375
train loss:  0.4506879448890686
train gradient:  0.12829362194857835
iteration : 8644
train acc:  0.671875
train loss:  0.5577354431152344
train gradient:  0.15764950313864323
iteration : 8645
train acc:  0.6484375
train loss:  0.553463339805603
train gradient:  0.13441376919002093
iteration : 8646
train acc:  0.7421875
train loss:  0.5455794334411621
train gradient:  0.14292047207770525
iteration : 8647
train acc:  0.734375
train loss:  0.5130711793899536
train gradient:  0.1516145494007916
iteration : 8648
train acc:  0.7421875
train loss:  0.5070041418075562
train gradient:  0.12885113428352582
iteration : 8649
train acc:  0.7890625
train loss:  0.4842214584350586
train gradient:  0.1345713435269328
iteration : 8650
train acc:  0.7734375
train loss:  0.45276832580566406
train gradient:  0.13107425331121522
iteration : 8651
train acc:  0.734375
train loss:  0.5264419317245483
train gradient:  0.156024827670004
iteration : 8652
train acc:  0.65625
train loss:  0.5874297022819519
train gradient:  0.15134942892622721
iteration : 8653
train acc:  0.8359375
train loss:  0.42882728576660156
train gradient:  0.11851262878106625
iteration : 8654
train acc:  0.8046875
train loss:  0.4676220417022705
train gradient:  0.10801043667291621
iteration : 8655
train acc:  0.6171875
train loss:  0.6030350923538208
train gradient:  0.15387451175268818
iteration : 8656
train acc:  0.6953125
train loss:  0.5239946842193604
train gradient:  0.13793971694827384
iteration : 8657
train acc:  0.7109375
train loss:  0.5648918151855469
train gradient:  0.19683597759636534
iteration : 8658
train acc:  0.6953125
train loss:  0.5619397759437561
train gradient:  0.16600938685779926
iteration : 8659
train acc:  0.671875
train loss:  0.5930118560791016
train gradient:  0.16085675918826625
iteration : 8660
train acc:  0.78125
train loss:  0.5040605068206787
train gradient:  0.1442259266894096
iteration : 8661
train acc:  0.78125
train loss:  0.47381752729415894
train gradient:  0.18180871158118783
iteration : 8662
train acc:  0.7734375
train loss:  0.4633713662624359
train gradient:  0.13749663859458428
iteration : 8663
train acc:  0.7578125
train loss:  0.4961331784725189
train gradient:  0.13174287434334286
iteration : 8664
train acc:  0.7578125
train loss:  0.4669772982597351
train gradient:  0.1391987053939579
iteration : 8665
train acc:  0.765625
train loss:  0.5014650821685791
train gradient:  0.14878593040129334
iteration : 8666
train acc:  0.6953125
train loss:  0.522257924079895
train gradient:  0.1404461726487215
iteration : 8667
train acc:  0.75
train loss:  0.48378345370292664
train gradient:  0.12746258369267338
iteration : 8668
train acc:  0.7421875
train loss:  0.5025031566619873
train gradient:  0.1172711754541938
iteration : 8669
train acc:  0.7421875
train loss:  0.529210090637207
train gradient:  0.1360999902932577
iteration : 8670
train acc:  0.75
train loss:  0.4983627498149872
train gradient:  0.12574505930345342
iteration : 8671
train acc:  0.7109375
train loss:  0.4967985451221466
train gradient:  0.14327923926675368
iteration : 8672
train acc:  0.765625
train loss:  0.48976683616638184
train gradient:  0.12359823901724891
iteration : 8673
train acc:  0.7578125
train loss:  0.5005720853805542
train gradient:  0.13866052005061097
iteration : 8674
train acc:  0.703125
train loss:  0.5363450050354004
train gradient:  0.15611837842680776
iteration : 8675
train acc:  0.7109375
train loss:  0.5273165702819824
train gradient:  0.16767023411535387
iteration : 8676
train acc:  0.7734375
train loss:  0.4651370942592621
train gradient:  0.10709815091673923
iteration : 8677
train acc:  0.7109375
train loss:  0.5445094704627991
train gradient:  0.1675533348611469
iteration : 8678
train acc:  0.734375
train loss:  0.5010629892349243
train gradient:  0.11726275318666246
iteration : 8679
train acc:  0.7421875
train loss:  0.5158601999282837
train gradient:  0.1264476027606816
iteration : 8680
train acc:  0.7890625
train loss:  0.5100985765457153
train gradient:  0.14212654553034054
iteration : 8681
train acc:  0.703125
train loss:  0.5145480036735535
train gradient:  0.1526667159393294
iteration : 8682
train acc:  0.671875
train loss:  0.5990236401557922
train gradient:  0.17782968112346526
iteration : 8683
train acc:  0.78125
train loss:  0.4477992355823517
train gradient:  0.14134918195896656
iteration : 8684
train acc:  0.78125
train loss:  0.4388514757156372
train gradient:  0.09372704719936006
iteration : 8685
train acc:  0.65625
train loss:  0.6154509782791138
train gradient:  0.16377561799559504
iteration : 8686
train acc:  0.734375
train loss:  0.48042088747024536
train gradient:  0.12173611544477358
iteration : 8687
train acc:  0.7109375
train loss:  0.5366690754890442
train gradient:  0.13725835835788802
iteration : 8688
train acc:  0.71875
train loss:  0.5101982355117798
train gradient:  0.13061888266170163
iteration : 8689
train acc:  0.7578125
train loss:  0.504585325717926
train gradient:  0.13660237526766836
iteration : 8690
train acc:  0.7734375
train loss:  0.5038210153579712
train gradient:  0.13781467644368656
iteration : 8691
train acc:  0.7734375
train loss:  0.4671001434326172
train gradient:  0.12843856186038405
iteration : 8692
train acc:  0.6953125
train loss:  0.5356168150901794
train gradient:  0.12331368888840474
iteration : 8693
train acc:  0.71875
train loss:  0.5105307102203369
train gradient:  0.15197003038775925
iteration : 8694
train acc:  0.703125
train loss:  0.542387068271637
train gradient:  0.14915937124189013
iteration : 8695
train acc:  0.703125
train loss:  0.5266247987747192
train gradient:  0.14575316934704138
iteration : 8696
train acc:  0.7734375
train loss:  0.4624006152153015
train gradient:  0.10702879411956687
iteration : 8697
train acc:  0.7109375
train loss:  0.4932137131690979
train gradient:  0.11448671302111901
iteration : 8698
train acc:  0.71875
train loss:  0.5732643604278564
train gradient:  0.1423633361522207
iteration : 8699
train acc:  0.7421875
train loss:  0.47457486391067505
train gradient:  0.1164397673626088
iteration : 8700
train acc:  0.6875
train loss:  0.5881233811378479
train gradient:  0.1920302245114684
iteration : 8701
train acc:  0.6953125
train loss:  0.570114016532898
train gradient:  0.13314604045519246
iteration : 8702
train acc:  0.7265625
train loss:  0.5229583382606506
train gradient:  0.1276478932375371
iteration : 8703
train acc:  0.71875
train loss:  0.5570095777511597
train gradient:  0.18001581838223074
iteration : 8704
train acc:  0.6875
train loss:  0.5591278076171875
train gradient:  0.19154078299294164
iteration : 8705
train acc:  0.6640625
train loss:  0.5904961824417114
train gradient:  0.15244505984275986
iteration : 8706
train acc:  0.7578125
train loss:  0.45193353295326233
train gradient:  0.10200201782216149
iteration : 8707
train acc:  0.7265625
train loss:  0.5076707601547241
train gradient:  0.12257841435053184
iteration : 8708
train acc:  0.78125
train loss:  0.49683117866516113
train gradient:  0.12961210941484075
iteration : 8709
train acc:  0.78125
train loss:  0.4572452902793884
train gradient:  0.12259674695906185
iteration : 8710
train acc:  0.671875
train loss:  0.5421191453933716
train gradient:  0.13229646879161874
iteration : 8711
train acc:  0.7421875
train loss:  0.5969637632369995
train gradient:  0.15769250957683467
iteration : 8712
train acc:  0.7578125
train loss:  0.5537565350532532
train gradient:  0.16996261641527133
iteration : 8713
train acc:  0.7734375
train loss:  0.464898943901062
train gradient:  0.09552408634607584
iteration : 8714
train acc:  0.7734375
train loss:  0.5386162400245667
train gradient:  0.13869208277065798
iteration : 8715
train acc:  0.75
train loss:  0.4871237874031067
train gradient:  0.12266559513038575
iteration : 8716
train acc:  0.6484375
train loss:  0.6016404628753662
train gradient:  0.16270537379120464
iteration : 8717
train acc:  0.703125
train loss:  0.5286240577697754
train gradient:  0.13537899079573576
iteration : 8718
train acc:  0.703125
train loss:  0.48189878463745117
train gradient:  0.10284621019449583
iteration : 8719
train acc:  0.8203125
train loss:  0.43436795473098755
train gradient:  0.1054446531792295
iteration : 8720
train acc:  0.6875
train loss:  0.5670185089111328
train gradient:  0.17076301615126105
iteration : 8721
train acc:  0.6640625
train loss:  0.6070417165756226
train gradient:  0.22790778638095355
iteration : 8722
train acc:  0.796875
train loss:  0.4866674244403839
train gradient:  0.14254810400817797
iteration : 8723
train acc:  0.7421875
train loss:  0.5393610000610352
train gradient:  0.16173025736135738
iteration : 8724
train acc:  0.75
train loss:  0.45161518454551697
train gradient:  0.12844289769893058
iteration : 8725
train acc:  0.78125
train loss:  0.44611600041389465
train gradient:  0.09129339550748611
iteration : 8726
train acc:  0.734375
train loss:  0.48681163787841797
train gradient:  0.1162780448169878
iteration : 8727
train acc:  0.8125
train loss:  0.4516514837741852
train gradient:  0.12360371816994184
iteration : 8728
train acc:  0.6640625
train loss:  0.5661519765853882
train gradient:  0.1540379518369258
iteration : 8729
train acc:  0.8203125
train loss:  0.3986886143684387
train gradient:  0.07947291134738181
iteration : 8730
train acc:  0.703125
train loss:  0.5463980436325073
train gradient:  0.1470344580196922
iteration : 8731
train acc:  0.7421875
train loss:  0.4544130563735962
train gradient:  0.09654040510019182
iteration : 8732
train acc:  0.75
train loss:  0.5244298577308655
train gradient:  0.15121224529151417
iteration : 8733
train acc:  0.734375
train loss:  0.5166267156600952
train gradient:  0.14065273509615672
iteration : 8734
train acc:  0.7734375
train loss:  0.5051109194755554
train gradient:  0.14603354924241624
iteration : 8735
train acc:  0.703125
train loss:  0.5175164341926575
train gradient:  0.12369754356003577
iteration : 8736
train acc:  0.7734375
train loss:  0.4439518451690674
train gradient:  0.09673100288825462
iteration : 8737
train acc:  0.828125
train loss:  0.4326668679714203
train gradient:  0.10373487470092112
iteration : 8738
train acc:  0.75
train loss:  0.5006422400474548
train gradient:  0.11611766324073634
iteration : 8739
train acc:  0.7421875
train loss:  0.5285189151763916
train gradient:  0.1305841920722148
iteration : 8740
train acc:  0.703125
train loss:  0.5009654760360718
train gradient:  0.11264426172201894
iteration : 8741
train acc:  0.78125
train loss:  0.4586001932621002
train gradient:  0.1009135842396532
iteration : 8742
train acc:  0.75
train loss:  0.478953093290329
train gradient:  0.1796646169781066
iteration : 8743
train acc:  0.703125
train loss:  0.4967155158519745
train gradient:  0.12453548640575997
iteration : 8744
train acc:  0.6875
train loss:  0.5186885595321655
train gradient:  0.1589839525106411
iteration : 8745
train acc:  0.75
train loss:  0.5339664220809937
train gradient:  0.118626268811328
iteration : 8746
train acc:  0.7578125
train loss:  0.4924057722091675
train gradient:  0.16075090067188083
iteration : 8747
train acc:  0.8125
train loss:  0.42625075578689575
train gradient:  0.11263198421762749
iteration : 8748
train acc:  0.65625
train loss:  0.554582417011261
train gradient:  0.18596511674164995
iteration : 8749
train acc:  0.7578125
train loss:  0.5240160226821899
train gradient:  0.13703359441978347
iteration : 8750
train acc:  0.75
train loss:  0.5238966345787048
train gradient:  0.12977451037447985
iteration : 8751
train acc:  0.75
train loss:  0.4580725133419037
train gradient:  0.14627581361642156
iteration : 8752
train acc:  0.8203125
train loss:  0.43569982051849365
train gradient:  0.0927126984186745
iteration : 8753
train acc:  0.75
train loss:  0.5272583961486816
train gradient:  0.14917866587864606
iteration : 8754
train acc:  0.6796875
train loss:  0.5069143176078796
train gradient:  0.16361861639968656
iteration : 8755
train acc:  0.7734375
train loss:  0.46151918172836304
train gradient:  0.09175313765516234
iteration : 8756
train acc:  0.7265625
train loss:  0.4759014844894409
train gradient:  0.12361064755820263
iteration : 8757
train acc:  0.75
train loss:  0.47108834981918335
train gradient:  0.1575646950394326
iteration : 8758
train acc:  0.7578125
train loss:  0.4919191598892212
train gradient:  0.1403481577095791
iteration : 8759
train acc:  0.7734375
train loss:  0.47111976146698
train gradient:  0.14984271987859005
iteration : 8760
train acc:  0.796875
train loss:  0.4540300965309143
train gradient:  0.1271272693897601
iteration : 8761
train acc:  0.7109375
train loss:  0.4936220943927765
train gradient:  0.14499687134003644
iteration : 8762
train acc:  0.7109375
train loss:  0.5185976624488831
train gradient:  0.11755264234119286
iteration : 8763
train acc:  0.8203125
train loss:  0.42228230834007263
train gradient:  0.10404399120830482
iteration : 8764
train acc:  0.828125
train loss:  0.397380530834198
train gradient:  0.0886970796200631
iteration : 8765
train acc:  0.71875
train loss:  0.4748639762401581
train gradient:  0.11818387044458632
iteration : 8766
train acc:  0.75
train loss:  0.5193212032318115
train gradient:  0.12455931824147226
iteration : 8767
train acc:  0.78125
train loss:  0.43935179710388184
train gradient:  0.10020723975397922
iteration : 8768
train acc:  0.71875
train loss:  0.534164547920227
train gradient:  0.1697840458908953
iteration : 8769
train acc:  0.765625
train loss:  0.48542729020118713
train gradient:  0.11052106970829895
iteration : 8770
train acc:  0.6484375
train loss:  0.5697218179702759
train gradient:  0.17606119224287947
iteration : 8771
train acc:  0.75
train loss:  0.491555392742157
train gradient:  0.13794580098708623
iteration : 8772
train acc:  0.796875
train loss:  0.4803847372531891
train gradient:  0.17275723470796575
iteration : 8773
train acc:  0.78125
train loss:  0.4421358108520508
train gradient:  0.10953665446036157
iteration : 8774
train acc:  0.6953125
train loss:  0.5024368166923523
train gradient:  0.10494404170487681
iteration : 8775
train acc:  0.71875
train loss:  0.4910029172897339
train gradient:  0.1330011886425888
iteration : 8776
train acc:  0.6796875
train loss:  0.5821753740310669
train gradient:  0.19902006404405814
iteration : 8777
train acc:  0.7109375
train loss:  0.5245877504348755
train gradient:  0.13870290756078849
iteration : 8778
train acc:  0.7578125
train loss:  0.46168386936187744
train gradient:  0.1397542973699098
iteration : 8779
train acc:  0.703125
train loss:  0.542598307132721
train gradient:  0.1486571880492989
iteration : 8780
train acc:  0.7265625
train loss:  0.5140044093132019
train gradient:  0.1484484412514984
iteration : 8781
train acc:  0.671875
train loss:  0.5396077632904053
train gradient:  0.13289520154257708
iteration : 8782
train acc:  0.8203125
train loss:  0.4402044713497162
train gradient:  0.10083425972935145
iteration : 8783
train acc:  0.7734375
train loss:  0.5127097964286804
train gradient:  0.13673317632121995
iteration : 8784
train acc:  0.7734375
train loss:  0.4877905249595642
train gradient:  0.13674312095911173
iteration : 8785
train acc:  0.7734375
train loss:  0.47965770959854126
train gradient:  0.11084000015844528
iteration : 8786
train acc:  0.703125
train loss:  0.5413137674331665
train gradient:  0.15255143505192653
iteration : 8787
train acc:  0.7109375
train loss:  0.5407365560531616
train gradient:  0.14748612315127563
iteration : 8788
train acc:  0.71875
train loss:  0.531520426273346
train gradient:  0.13345914032133793
iteration : 8789
train acc:  0.7578125
train loss:  0.4897698163986206
train gradient:  0.142235625846664
iteration : 8790
train acc:  0.6953125
train loss:  0.48190736770629883
train gradient:  0.10725751478609577
iteration : 8791
train acc:  0.7109375
train loss:  0.5671662092208862
train gradient:  0.18614747996283992
iteration : 8792
train acc:  0.7421875
train loss:  0.49297475814819336
train gradient:  0.11413238228884463
iteration : 8793
train acc:  0.7265625
train loss:  0.5399198532104492
train gradient:  0.12824463588769533
iteration : 8794
train acc:  0.7421875
train loss:  0.46239492297172546
train gradient:  0.10445728446736306
iteration : 8795
train acc:  0.65625
train loss:  0.5285053253173828
train gradient:  0.15198217296484196
iteration : 8796
train acc:  0.796875
train loss:  0.502880334854126
train gradient:  0.15189124780466262
iteration : 8797
train acc:  0.75
train loss:  0.4790720045566559
train gradient:  0.11847713987520567
iteration : 8798
train acc:  0.6875
train loss:  0.5512458682060242
train gradient:  0.1571244812844872
iteration : 8799
train acc:  0.7109375
train loss:  0.5246917605400085
train gradient:  0.15859106351989433
iteration : 8800
train acc:  0.703125
train loss:  0.5038659572601318
train gradient:  0.12258828066195222
iteration : 8801
train acc:  0.7734375
train loss:  0.45541608333587646
train gradient:  0.09639914795197206
iteration : 8802
train acc:  0.7109375
train loss:  0.5568753480911255
train gradient:  0.21436922799441485
iteration : 8803
train acc:  0.7578125
train loss:  0.45339593291282654
train gradient:  0.14215363104996365
iteration : 8804
train acc:  0.7265625
train loss:  0.5422714352607727
train gradient:  0.16816836249045336
iteration : 8805
train acc:  0.7109375
train loss:  0.5011844635009766
train gradient:  0.13405636651890004
iteration : 8806
train acc:  0.78125
train loss:  0.42949986457824707
train gradient:  0.1329404801431192
iteration : 8807
train acc:  0.7578125
train loss:  0.4445895552635193
train gradient:  0.10707554513643806
iteration : 8808
train acc:  0.6953125
train loss:  0.5270155668258667
train gradient:  0.15618627364587456
iteration : 8809
train acc:  0.65625
train loss:  0.5641158819198608
train gradient:  0.12533037433698133
iteration : 8810
train acc:  0.765625
train loss:  0.4751245379447937
train gradient:  0.10326144700121083
iteration : 8811
train acc:  0.671875
train loss:  0.6112620830535889
train gradient:  0.2083307535301318
iteration : 8812
train acc:  0.8203125
train loss:  0.40470442175865173
train gradient:  0.09190598699251179
iteration : 8813
train acc:  0.734375
train loss:  0.477609246969223
train gradient:  0.1035040952965276
iteration : 8814
train acc:  0.7421875
train loss:  0.47086405754089355
train gradient:  0.13738433932908956
iteration : 8815
train acc:  0.6953125
train loss:  0.5583535432815552
train gradient:  0.15693132062358767
iteration : 8816
train acc:  0.8203125
train loss:  0.41302359104156494
train gradient:  0.0868145466166385
iteration : 8817
train acc:  0.828125
train loss:  0.4675978422164917
train gradient:  0.16666060101051022
iteration : 8818
train acc:  0.7265625
train loss:  0.5033067464828491
train gradient:  0.14823975160190994
iteration : 8819
train acc:  0.7578125
train loss:  0.47775331139564514
train gradient:  0.12501383204148658
iteration : 8820
train acc:  0.75
train loss:  0.49509650468826294
train gradient:  0.1440568232857326
iteration : 8821
train acc:  0.75
train loss:  0.44090139865875244
train gradient:  0.14054354317734613
iteration : 8822
train acc:  0.734375
train loss:  0.5463967323303223
train gradient:  0.2372288442997939
iteration : 8823
train acc:  0.7421875
train loss:  0.489035040140152
train gradient:  0.14763244614607574
iteration : 8824
train acc:  0.7578125
train loss:  0.48989999294281006
train gradient:  0.11063478100969591
iteration : 8825
train acc:  0.7109375
train loss:  0.47854554653167725
train gradient:  0.09234123943780066
iteration : 8826
train acc:  0.765625
train loss:  0.47478151321411133
train gradient:  0.12832729508244364
iteration : 8827
train acc:  0.7734375
train loss:  0.555266261100769
train gradient:  0.18259652530919956
iteration : 8828
train acc:  0.7265625
train loss:  0.47843360900878906
train gradient:  0.1228322144788861
iteration : 8829
train acc:  0.7421875
train loss:  0.48878562450408936
train gradient:  0.12686120779531274
iteration : 8830
train acc:  0.6875
train loss:  0.5512924194335938
train gradient:  0.14026713216948133
iteration : 8831
train acc:  0.703125
train loss:  0.5093649625778198
train gradient:  0.17342993601427537
iteration : 8832
train acc:  0.7265625
train loss:  0.505240797996521
train gradient:  0.14398569447798887
iteration : 8833
train acc:  0.734375
train loss:  0.4880993068218231
train gradient:  0.12924403768527554
iteration : 8834
train acc:  0.734375
train loss:  0.5074765682220459
train gradient:  0.1711270354785061
iteration : 8835
train acc:  0.6875
train loss:  0.5097824335098267
train gradient:  0.1377262875406008
iteration : 8836
train acc:  0.78125
train loss:  0.4499880075454712
train gradient:  0.11750883410360237
iteration : 8837
train acc:  0.6953125
train loss:  0.5367408394813538
train gradient:  0.16198352385685766
iteration : 8838
train acc:  0.6875
train loss:  0.5409029126167297
train gradient:  0.1506702350890161
iteration : 8839
train acc:  0.734375
train loss:  0.5269863605499268
train gradient:  0.1386772893842935
iteration : 8840
train acc:  0.7734375
train loss:  0.4693307876586914
train gradient:  0.10424453297229055
iteration : 8841
train acc:  0.8125
train loss:  0.47472500801086426
train gradient:  0.14052980766447726
iteration : 8842
train acc:  0.6953125
train loss:  0.5699818134307861
train gradient:  0.15927598800561965
iteration : 8843
train acc:  0.75
train loss:  0.48839130997657776
train gradient:  0.10815698120879984
iteration : 8844
train acc:  0.7890625
train loss:  0.47309428453445435
train gradient:  0.11994326891236608
iteration : 8845
train acc:  0.7578125
train loss:  0.5057412981987
train gradient:  0.12108035958646488
iteration : 8846
train acc:  0.7265625
train loss:  0.49541372060775757
train gradient:  0.12496462913673441
iteration : 8847
train acc:  0.78125
train loss:  0.48465484380722046
train gradient:  0.17855757685115947
iteration : 8848
train acc:  0.75
train loss:  0.4687882959842682
train gradient:  0.13040826418829277
iteration : 8849
train acc:  0.7578125
train loss:  0.4799851179122925
train gradient:  0.15106099012014734
iteration : 8850
train acc:  0.7265625
train loss:  0.4966451823711395
train gradient:  0.13829606739876704
iteration : 8851
train acc:  0.75
train loss:  0.5106480717658997
train gradient:  0.11892367502485224
iteration : 8852
train acc:  0.71875
train loss:  0.5001837015151978
train gradient:  0.13845121821092693
iteration : 8853
train acc:  0.8046875
train loss:  0.4538425803184509
train gradient:  0.12592363838579707
iteration : 8854
train acc:  0.765625
train loss:  0.4067555367946625
train gradient:  0.09736629959713472
iteration : 8855
train acc:  0.6796875
train loss:  0.605630099773407
train gradient:  0.2149231299992463
iteration : 8856
train acc:  0.7109375
train loss:  0.5494730472564697
train gradient:  0.17553834663657414
iteration : 8857
train acc:  0.703125
train loss:  0.5190382599830627
train gradient:  0.1295387323480955
iteration : 8858
train acc:  0.8046875
train loss:  0.4461362957954407
train gradient:  0.11670545690177535
iteration : 8859
train acc:  0.8046875
train loss:  0.4266001582145691
train gradient:  0.08536543459583479
iteration : 8860
train acc:  0.6796875
train loss:  0.4956797957420349
train gradient:  0.13143709492256112
iteration : 8861
train acc:  0.734375
train loss:  0.5314497947692871
train gradient:  0.19095251399301863
iteration : 8862
train acc:  0.6953125
train loss:  0.5860990285873413
train gradient:  0.1774446935908675
iteration : 8863
train acc:  0.765625
train loss:  0.47387972474098206
train gradient:  0.13895706964674362
iteration : 8864
train acc:  0.734375
train loss:  0.4734514355659485
train gradient:  0.11024314447431204
iteration : 8865
train acc:  0.765625
train loss:  0.4801700711250305
train gradient:  0.15542644891242152
iteration : 8866
train acc:  0.6640625
train loss:  0.5914404392242432
train gradient:  0.20134016127763385
iteration : 8867
train acc:  0.7109375
train loss:  0.48660239577293396
train gradient:  0.1269553190967227
iteration : 8868
train acc:  0.765625
train loss:  0.49585336446762085
train gradient:  0.10905168169472641
iteration : 8869
train acc:  0.765625
train loss:  0.49784067273139954
train gradient:  0.12789413990167361
iteration : 8870
train acc:  0.7265625
train loss:  0.48402899503707886
train gradient:  0.1319248734972787
iteration : 8871
train acc:  0.7578125
train loss:  0.5061633586883545
train gradient:  0.11647241552808735
iteration : 8872
train acc:  0.7421875
train loss:  0.5102617740631104
train gradient:  0.14340313747466407
iteration : 8873
train acc:  0.7265625
train loss:  0.6070858240127563
train gradient:  0.19740323460875642
iteration : 8874
train acc:  0.765625
train loss:  0.49010396003723145
train gradient:  0.17145745971174073
iteration : 8875
train acc:  0.640625
train loss:  0.5907419919967651
train gradient:  0.1663391144497125
iteration : 8876
train acc:  0.7265625
train loss:  0.5273929238319397
train gradient:  0.15005172278969833
iteration : 8877
train acc:  0.7109375
train loss:  0.49367576837539673
train gradient:  0.11928523522813417
iteration : 8878
train acc:  0.78125
train loss:  0.45461413264274597
train gradient:  0.10933356492193697
iteration : 8879
train acc:  0.703125
train loss:  0.5537372827529907
train gradient:  0.1399956160036422
iteration : 8880
train acc:  0.7734375
train loss:  0.49222928285598755
train gradient:  0.13886212058577457
iteration : 8881
train acc:  0.7890625
train loss:  0.4233209788799286
train gradient:  0.11263674087210854
iteration : 8882
train acc:  0.7734375
train loss:  0.4437134861946106
train gradient:  0.12264915931988887
iteration : 8883
train acc:  0.6484375
train loss:  0.555984377861023
train gradient:  0.15024772573959178
iteration : 8884
train acc:  0.78125
train loss:  0.45852944254875183
train gradient:  0.1325512793991072
iteration : 8885
train acc:  0.8046875
train loss:  0.4156937897205353
train gradient:  0.0946238845522126
iteration : 8886
train acc:  0.734375
train loss:  0.5259026288986206
train gradient:  0.13274177192815018
iteration : 8887
train acc:  0.7890625
train loss:  0.4680761694908142
train gradient:  0.1209765903139179
iteration : 8888
train acc:  0.7421875
train loss:  0.5321099162101746
train gradient:  0.18551312770785122
iteration : 8889
train acc:  0.8046875
train loss:  0.4693993926048279
train gradient:  0.10828300938587684
iteration : 8890
train acc:  0.7421875
train loss:  0.5002627372741699
train gradient:  0.12300957887471822
iteration : 8891
train acc:  0.7109375
train loss:  0.5044506788253784
train gradient:  0.12381187408656634
iteration : 8892
train acc:  0.71875
train loss:  0.5194019079208374
train gradient:  0.11841551701242477
iteration : 8893
train acc:  0.71875
train loss:  0.5930635929107666
train gradient:  0.18777055178742713
iteration : 8894
train acc:  0.6953125
train loss:  0.5585339069366455
train gradient:  0.16972679363208837
iteration : 8895
train acc:  0.78125
train loss:  0.42778000235557556
train gradient:  0.10552825611816805
iteration : 8896
train acc:  0.765625
train loss:  0.4924061894416809
train gradient:  0.10560198820419453
iteration : 8897
train acc:  0.7265625
train loss:  0.5493282079696655
train gradient:  0.15675514468384882
iteration : 8898
train acc:  0.75
train loss:  0.494853675365448
train gradient:  0.12745386719697258
iteration : 8899
train acc:  0.78125
train loss:  0.4458555579185486
train gradient:  0.11707297905704933
iteration : 8900
train acc:  0.671875
train loss:  0.6421023011207581
train gradient:  0.20317931066615347
iteration : 8901
train acc:  0.703125
train loss:  0.5299537181854248
train gradient:  0.13856185162887216
iteration : 8902
train acc:  0.734375
train loss:  0.4807724356651306
train gradient:  0.1707666058764007
iteration : 8903
train acc:  0.7734375
train loss:  0.45289623737335205
train gradient:  0.11602674462094689
iteration : 8904
train acc:  0.8125
train loss:  0.3971734642982483
train gradient:  0.10694746079331766
iteration : 8905
train acc:  0.78125
train loss:  0.40127676725387573
train gradient:  0.1083748868145925
iteration : 8906
train acc:  0.7109375
train loss:  0.47925907373428345
train gradient:  0.13354846268411757
iteration : 8907
train acc:  0.765625
train loss:  0.496620237827301
train gradient:  0.13734243388987655
iteration : 8908
train acc:  0.7578125
train loss:  0.49046099185943604
train gradient:  0.14041592098716055
iteration : 8909
train acc:  0.7734375
train loss:  0.5170557498931885
train gradient:  0.18086285963735163
iteration : 8910
train acc:  0.7265625
train loss:  0.4805772304534912
train gradient:  0.111429016295015
iteration : 8911
train acc:  0.734375
train loss:  0.5877021551132202
train gradient:  0.1816904451473738
iteration : 8912
train acc:  0.7109375
train loss:  0.525256872177124
train gradient:  0.11982718181874734
iteration : 8913
train acc:  0.6875
train loss:  0.5865877866744995
train gradient:  0.16016901424000296
iteration : 8914
train acc:  0.765625
train loss:  0.4782823920249939
train gradient:  0.17484669704348133
iteration : 8915
train acc:  0.7109375
train loss:  0.5321189165115356
train gradient:  0.12690347240741157
iteration : 8916
train acc:  0.7421875
train loss:  0.4668375551700592
train gradient:  0.12397252828846998
iteration : 8917
train acc:  0.828125
train loss:  0.43890929222106934
train gradient:  0.1088162995380416
iteration : 8918
train acc:  0.75
train loss:  0.4726724922657013
train gradient:  0.1217622310597156
iteration : 8919
train acc:  0.65625
train loss:  0.5790040493011475
train gradient:  0.19046849399405433
iteration : 8920
train acc:  0.6640625
train loss:  0.5509801506996155
train gradient:  0.14583539820594577
iteration : 8921
train acc:  0.7734375
train loss:  0.4556131958961487
train gradient:  0.1007232390289155
iteration : 8922
train acc:  0.7265625
train loss:  0.5179191827774048
train gradient:  0.13720994753220583
iteration : 8923
train acc:  0.7109375
train loss:  0.5385467410087585
train gradient:  0.14737007243478756
iteration : 8924
train acc:  0.8359375
train loss:  0.41141557693481445
train gradient:  0.10886805633794798
iteration : 8925
train acc:  0.7109375
train loss:  0.5161726474761963
train gradient:  0.1317320587490635
iteration : 8926
train acc:  0.7734375
train loss:  0.49680542945861816
train gradient:  0.1384248809274725
iteration : 8927
train acc:  0.7265625
train loss:  0.5039852857589722
train gradient:  0.13130103982465388
iteration : 8928
train acc:  0.7578125
train loss:  0.470664918422699
train gradient:  0.10905117334239867
iteration : 8929
train acc:  0.8046875
train loss:  0.45632773637771606
train gradient:  0.13136474384896357
iteration : 8930
train acc:  0.765625
train loss:  0.46906617283821106
train gradient:  0.1541304702067323
iteration : 8931
train acc:  0.6875
train loss:  0.5549044609069824
train gradient:  0.1642524531971607
iteration : 8932
train acc:  0.78125
train loss:  0.4493418037891388
train gradient:  0.1079002798809204
iteration : 8933
train acc:  0.671875
train loss:  0.6008248329162598
train gradient:  0.22469523442742967
iteration : 8934
train acc:  0.734375
train loss:  0.5219480991363525
train gradient:  0.148817127797908
iteration : 8935
train acc:  0.84375
train loss:  0.4194444715976715
train gradient:  0.10910035232123311
iteration : 8936
train acc:  0.71875
train loss:  0.556288480758667
train gradient:  0.1529640476272361
iteration : 8937
train acc:  0.8203125
train loss:  0.4400152564048767
train gradient:  0.12772895634758658
iteration : 8938
train acc:  0.75
train loss:  0.4714360237121582
train gradient:  0.14673403221603643
iteration : 8939
train acc:  0.765625
train loss:  0.4742821753025055
train gradient:  0.10623871742319287
iteration : 8940
train acc:  0.7734375
train loss:  0.47087499499320984
train gradient:  0.12796724155082534
iteration : 8941
train acc:  0.8046875
train loss:  0.44078677892684937
train gradient:  0.13128939587007377
iteration : 8942
train acc:  0.7265625
train loss:  0.49918633699417114
train gradient:  0.12526276608273745
iteration : 8943
train acc:  0.7421875
train loss:  0.4896865785121918
train gradient:  0.1658071123442887
iteration : 8944
train acc:  0.7265625
train loss:  0.5479691624641418
train gradient:  0.15944197961742046
iteration : 8945
train acc:  0.7421875
train loss:  0.5218184590339661
train gradient:  0.1335548108621476
iteration : 8946
train acc:  0.75
train loss:  0.492986261844635
train gradient:  0.13348940941077336
iteration : 8947
train acc:  0.7265625
train loss:  0.48933887481689453
train gradient:  0.12398674937845526
iteration : 8948
train acc:  0.7109375
train loss:  0.49271702766418457
train gradient:  0.1309752078777633
iteration : 8949
train acc:  0.765625
train loss:  0.4740009307861328
train gradient:  0.12275382041838317
iteration : 8950
train acc:  0.7578125
train loss:  0.47918277978897095
train gradient:  0.15224587694924974
iteration : 8951
train acc:  0.6796875
train loss:  0.5755966901779175
train gradient:  0.16984574481114
iteration : 8952
train acc:  0.75
train loss:  0.5400069952011108
train gradient:  0.16939117941209558
iteration : 8953
train acc:  0.671875
train loss:  0.5632873773574829
train gradient:  0.14740415757698633
iteration : 8954
train acc:  0.7421875
train loss:  0.5196071863174438
train gradient:  0.16774288286224212
iteration : 8955
train acc:  0.734375
train loss:  0.5014533996582031
train gradient:  0.12475982581116099
iteration : 8956
train acc:  0.703125
train loss:  0.5601624250411987
train gradient:  0.14760523115036756
iteration : 8957
train acc:  0.734375
train loss:  0.5071903467178345
train gradient:  0.1245593170956368
iteration : 8958
train acc:  0.6953125
train loss:  0.5554192662239075
train gradient:  0.17762035852884211
iteration : 8959
train acc:  0.78125
train loss:  0.4161872863769531
train gradient:  0.07683814070054214
iteration : 8960
train acc:  0.734375
train loss:  0.5037989616394043
train gradient:  0.12863159532567742
iteration : 8961
train acc:  0.7265625
train loss:  0.49143555760383606
train gradient:  0.13295420274954156
iteration : 8962
train acc:  0.703125
train loss:  0.5369259119033813
train gradient:  0.16519090566270506
iteration : 8963
train acc:  0.75
train loss:  0.44953304529190063
train gradient:  0.10534044412761251
iteration : 8964
train acc:  0.71875
train loss:  0.47660502791404724
train gradient:  0.122987212246964
iteration : 8965
train acc:  0.765625
train loss:  0.49327564239501953
train gradient:  0.16893004428165243
iteration : 8966
train acc:  0.7421875
train loss:  0.48627007007598877
train gradient:  0.11566968685499748
iteration : 8967
train acc:  0.765625
train loss:  0.4946887493133545
train gradient:  0.12078182470442456
iteration : 8968
train acc:  0.78125
train loss:  0.519278347492218
train gradient:  0.14767747707691586
iteration : 8969
train acc:  0.734375
train loss:  0.5402361154556274
train gradient:  0.20357831401346324
iteration : 8970
train acc:  0.734375
train loss:  0.5176014304161072
train gradient:  0.1702445722785601
iteration : 8971
train acc:  0.703125
train loss:  0.5124944448471069
train gradient:  0.16958918782760773
iteration : 8972
train acc:  0.75
train loss:  0.5137868523597717
train gradient:  0.13025502576814565
iteration : 8973
train acc:  0.65625
train loss:  0.548630952835083
train gradient:  0.17699387840751643
iteration : 8974
train acc:  0.765625
train loss:  0.4419596195220947
train gradient:  0.09692984306468867
iteration : 8975
train acc:  0.7578125
train loss:  0.47381144762039185
train gradient:  0.1242666580801642
iteration : 8976
train acc:  0.7578125
train loss:  0.49346017837524414
train gradient:  0.11135779142275197
iteration : 8977
train acc:  0.7109375
train loss:  0.5084719657897949
train gradient:  0.1321250572097329
iteration : 8978
train acc:  0.703125
train loss:  0.49488019943237305
train gradient:  0.1515642186286747
iteration : 8979
train acc:  0.765625
train loss:  0.4690157175064087
train gradient:  0.10461154875604266
iteration : 8980
train acc:  0.7421875
train loss:  0.4783563017845154
train gradient:  0.14171526790292766
iteration : 8981
train acc:  0.78125
train loss:  0.4486686587333679
train gradient:  0.10378239190508884
iteration : 8982
train acc:  0.671875
train loss:  0.5555877089500427
train gradient:  0.15578066754581965
iteration : 8983
train acc:  0.7265625
train loss:  0.4976429045200348
train gradient:  0.16089558801241088
iteration : 8984
train acc:  0.7578125
train loss:  0.5196873545646667
train gradient:  0.1435362791483352
iteration : 8985
train acc:  0.71875
train loss:  0.4707014858722687
train gradient:  0.14157222642065853
iteration : 8986
train acc:  0.8203125
train loss:  0.41723373532295227
train gradient:  0.08776873987467074
iteration : 8987
train acc:  0.71875
train loss:  0.5321812033653259
train gradient:  0.1606776555227542
iteration : 8988
train acc:  0.7578125
train loss:  0.46055757999420166
train gradient:  0.11391527819979974
iteration : 8989
train acc:  0.7109375
train loss:  0.5123326778411865
train gradient:  0.12309055251961959
iteration : 8990
train acc:  0.75
train loss:  0.4971436858177185
train gradient:  0.12129025416094338
iteration : 8991
train acc:  0.75
train loss:  0.4703519642353058
train gradient:  0.12842030754758443
iteration : 8992
train acc:  0.7421875
train loss:  0.5234125852584839
train gradient:  0.14188560427906732
iteration : 8993
train acc:  0.8125
train loss:  0.42987149953842163
train gradient:  0.10220562071284618
iteration : 8994
train acc:  0.6953125
train loss:  0.5354310274124146
train gradient:  0.18171352355222956
iteration : 8995
train acc:  0.7890625
train loss:  0.4958157539367676
train gradient:  0.13672965114035146
iteration : 8996
train acc:  0.7109375
train loss:  0.5164327025413513
train gradient:  0.17928960106203845
iteration : 8997
train acc:  0.734375
train loss:  0.47962942719459534
train gradient:  0.11943431809392516
iteration : 8998
train acc:  0.6796875
train loss:  0.5744222402572632
train gradient:  0.16498894980581688
iteration : 8999
train acc:  0.7265625
train loss:  0.506755530834198
train gradient:  0.16017566813887685
iteration : 9000
train acc:  0.6875
train loss:  0.5485259890556335
train gradient:  0.12282364116911304
iteration : 9001
train acc:  0.75
train loss:  0.5517066717147827
train gradient:  0.16322484159464273
iteration : 9002
train acc:  0.75
train loss:  0.48820120096206665
train gradient:  0.13223564798292015
iteration : 9003
train acc:  0.7578125
train loss:  0.5046000480651855
train gradient:  0.14058165280875567
iteration : 9004
train acc:  0.6953125
train loss:  0.5439778566360474
train gradient:  0.15234654997894076
iteration : 9005
train acc:  0.7421875
train loss:  0.5355761051177979
train gradient:  0.19037984922832882
iteration : 9006
train acc:  0.7890625
train loss:  0.48071396350860596
train gradient:  0.127714189968667
iteration : 9007
train acc:  0.734375
train loss:  0.5313243865966797
train gradient:  0.15351714983944792
iteration : 9008
train acc:  0.8046875
train loss:  0.4173493981361389
train gradient:  0.12448857256200536
iteration : 9009
train acc:  0.7421875
train loss:  0.4950380325317383
train gradient:  0.15455757225465566
iteration : 9010
train acc:  0.6328125
train loss:  0.5872487425804138
train gradient:  0.19665870413051445
iteration : 9011
train acc:  0.734375
train loss:  0.511293888092041
train gradient:  0.11999223743090183
iteration : 9012
train acc:  0.75
train loss:  0.4998885989189148
train gradient:  0.17932835802663255
iteration : 9013
train acc:  0.765625
train loss:  0.4409189224243164
train gradient:  0.11471607825311293
iteration : 9014
train acc:  0.71875
train loss:  0.48340725898742676
train gradient:  0.15375297951311295
iteration : 9015
train acc:  0.796875
train loss:  0.4430088698863983
train gradient:  0.12836434493208787
iteration : 9016
train acc:  0.75
train loss:  0.4972752034664154
train gradient:  0.16018793193191014
iteration : 9017
train acc:  0.6796875
train loss:  0.5402761101722717
train gradient:  0.1642177111051127
iteration : 9018
train acc:  0.7265625
train loss:  0.4980458617210388
train gradient:  0.13758362248918302
iteration : 9019
train acc:  0.765625
train loss:  0.45180147886276245
train gradient:  0.14246709165285742
iteration : 9020
train acc:  0.7578125
train loss:  0.4892124533653259
train gradient:  0.11591022597673122
iteration : 9021
train acc:  0.765625
train loss:  0.4889999032020569
train gradient:  0.11617340800068283
iteration : 9022
train acc:  0.703125
train loss:  0.5377438068389893
train gradient:  0.14853700583577062
iteration : 9023
train acc:  0.7109375
train loss:  0.5317159295082092
train gradient:  0.1383237900368341
iteration : 9024
train acc:  0.7734375
train loss:  0.4696968197822571
train gradient:  0.12607604656196286
iteration : 9025
train acc:  0.71875
train loss:  0.4758991003036499
train gradient:  0.1011508222234795
iteration : 9026
train acc:  0.75
train loss:  0.44768333435058594
train gradient:  0.12011924327931288
iteration : 9027
train acc:  0.7734375
train loss:  0.5246569514274597
train gradient:  0.15721719032405782
iteration : 9028
train acc:  0.7421875
train loss:  0.48986101150512695
train gradient:  0.11827932483582322
iteration : 9029
train acc:  0.703125
train loss:  0.5213098526000977
train gradient:  0.13904521727213798
iteration : 9030
train acc:  0.71875
train loss:  0.559512734413147
train gradient:  0.15172580666813518
iteration : 9031
train acc:  0.7421875
train loss:  0.4778723120689392
train gradient:  0.11302586067164343
iteration : 9032
train acc:  0.7109375
train loss:  0.5194233655929565
train gradient:  0.13428962331554684
iteration : 9033
train acc:  0.71875
train loss:  0.4674367308616638
train gradient:  0.1207816845307669
iteration : 9034
train acc:  0.75
train loss:  0.48315733671188354
train gradient:  0.13068752899072328
iteration : 9035
train acc:  0.75
train loss:  0.4592273533344269
train gradient:  0.12331539534669403
iteration : 9036
train acc:  0.7421875
train loss:  0.47717007994651794
train gradient:  0.13168588259564004
iteration : 9037
train acc:  0.8203125
train loss:  0.46158406138420105
train gradient:  0.14397796759261472
iteration : 9038
train acc:  0.796875
train loss:  0.4849740266799927
train gradient:  0.125229172686306
iteration : 9039
train acc:  0.78125
train loss:  0.44568994641304016
train gradient:  0.15190658106603172
iteration : 9040
train acc:  0.7421875
train loss:  0.5445443987846375
train gradient:  0.16334428331795608
iteration : 9041
train acc:  0.7421875
train loss:  0.4843372404575348
train gradient:  0.13981462922317445
iteration : 9042
train acc:  0.7421875
train loss:  0.48748862743377686
train gradient:  0.11607520814604853
iteration : 9043
train acc:  0.765625
train loss:  0.5243395566940308
train gradient:  0.15535567723809496
iteration : 9044
train acc:  0.7890625
train loss:  0.474570095539093
train gradient:  0.13163032081061438
iteration : 9045
train acc:  0.703125
train loss:  0.503610372543335
train gradient:  0.1131739767820636
iteration : 9046
train acc:  0.7421875
train loss:  0.49322211742401123
train gradient:  0.1293656065647972
iteration : 9047
train acc:  0.7265625
train loss:  0.5403811931610107
train gradient:  0.15011896454536516
iteration : 9048
train acc:  0.71875
train loss:  0.5343478918075562
train gradient:  0.14551467316813022
iteration : 9049
train acc:  0.765625
train loss:  0.509188175201416
train gradient:  0.15103045023875056
iteration : 9050
train acc:  0.7578125
train loss:  0.44271066784858704
train gradient:  0.1371776103874413
iteration : 9051
train acc:  0.7109375
train loss:  0.5320241451263428
train gradient:  0.1349349657094217
iteration : 9052
train acc:  0.71875
train loss:  0.4837646782398224
train gradient:  0.13169432291097632
iteration : 9053
train acc:  0.7578125
train loss:  0.49791279435157776
train gradient:  0.14627426411912264
iteration : 9054
train acc:  0.703125
train loss:  0.5657286643981934
train gradient:  0.16672453220993166
iteration : 9055
train acc:  0.75
train loss:  0.5172501802444458
train gradient:  0.1192235580876037
iteration : 9056
train acc:  0.734375
train loss:  0.5199999809265137
train gradient:  0.13950724182794544
iteration : 9057
train acc:  0.6875
train loss:  0.5273940563201904
train gradient:  0.13977324998440926
iteration : 9058
train acc:  0.71875
train loss:  0.5792174339294434
train gradient:  0.1763072202110762
iteration : 9059
train acc:  0.7421875
train loss:  0.465226411819458
train gradient:  0.1342151347639341
iteration : 9060
train acc:  0.65625
train loss:  0.5388221740722656
train gradient:  0.15059028092732196
iteration : 9061
train acc:  0.7578125
train loss:  0.5485244989395142
train gradient:  0.14323736683031885
iteration : 9062
train acc:  0.7734375
train loss:  0.45424169301986694
train gradient:  0.10357417595834328
iteration : 9063
train acc:  0.78125
train loss:  0.4448612928390503
train gradient:  0.10931577414934562
iteration : 9064
train acc:  0.796875
train loss:  0.4191303849220276
train gradient:  0.09440921923678937
iteration : 9065
train acc:  0.7734375
train loss:  0.4946044087409973
train gradient:  0.1238949941154195
iteration : 9066
train acc:  0.671875
train loss:  0.5250719785690308
train gradient:  0.13181906309285193
iteration : 9067
train acc:  0.7734375
train loss:  0.47551780939102173
train gradient:  0.12596226219168355
iteration : 9068
train acc:  0.6640625
train loss:  0.5478392839431763
train gradient:  0.17676693677510286
iteration : 9069
train acc:  0.65625
train loss:  0.5616834163665771
train gradient:  0.15523047913241123
iteration : 9070
train acc:  0.71875
train loss:  0.5029329061508179
train gradient:  0.13790724953843633
iteration : 9071
train acc:  0.6640625
train loss:  0.5363940000534058
train gradient:  0.15691748572011052
iteration : 9072
train acc:  0.7109375
train loss:  0.47789788246154785
train gradient:  0.1664405568917533
iteration : 9073
train acc:  0.71875
train loss:  0.5443750619888306
train gradient:  0.16544032345502344
iteration : 9074
train acc:  0.8046875
train loss:  0.47386634349823
train gradient:  0.09218016013992526
iteration : 9075
train acc:  0.7265625
train loss:  0.4673416316509247
train gradient:  0.1439256730346265
iteration : 9076
train acc:  0.703125
train loss:  0.4925716817378998
train gradient:  0.1031185636166934
iteration : 9077
train acc:  0.703125
train loss:  0.5055233836174011
train gradient:  0.1609874460449806
iteration : 9078
train acc:  0.6640625
train loss:  0.5974961519241333
train gradient:  0.1555941501706254
iteration : 9079
train acc:  0.6953125
train loss:  0.5157114863395691
train gradient:  0.1358441303003728
iteration : 9080
train acc:  0.6796875
train loss:  0.5616075992584229
train gradient:  0.15106171967578386
iteration : 9081
train acc:  0.765625
train loss:  0.488079309463501
train gradient:  0.17323593564948847
iteration : 9082
train acc:  0.7734375
train loss:  0.43057435750961304
train gradient:  0.08339172358462346
iteration : 9083
train acc:  0.7734375
train loss:  0.514512836933136
train gradient:  0.14419720983082884
iteration : 9084
train acc:  0.640625
train loss:  0.6427907943725586
train gradient:  0.254582594469548
iteration : 9085
train acc:  0.75
train loss:  0.49810129404067993
train gradient:  0.12595959698695003
iteration : 9086
train acc:  0.71875
train loss:  0.5356360673904419
train gradient:  0.12229461814912157
iteration : 9087
train acc:  0.71875
train loss:  0.5107505917549133
train gradient:  0.18922047334148184
iteration : 9088
train acc:  0.765625
train loss:  0.45048484206199646
train gradient:  0.11938703693058786
iteration : 9089
train acc:  0.7265625
train loss:  0.5020182132720947
train gradient:  0.19269345890428485
iteration : 9090
train acc:  0.75
train loss:  0.4729146659374237
train gradient:  0.11499088790838335
iteration : 9091
train acc:  0.6875
train loss:  0.5075542330741882
train gradient:  0.14876797344816528
iteration : 9092
train acc:  0.7578125
train loss:  0.4616982936859131
train gradient:  0.0848899103437537
iteration : 9093
train acc:  0.7578125
train loss:  0.48316264152526855
train gradient:  0.11676101362660216
iteration : 9094
train acc:  0.734375
train loss:  0.4994901120662689
train gradient:  0.12444723358074618
iteration : 9095
train acc:  0.6953125
train loss:  0.5491760969161987
train gradient:  0.15952120481689425
iteration : 9096
train acc:  0.703125
train loss:  0.5179586410522461
train gradient:  0.14683911690431764
iteration : 9097
train acc:  0.78125
train loss:  0.4547293782234192
train gradient:  0.15291453499883426
iteration : 9098
train acc:  0.703125
train loss:  0.5256441235542297
train gradient:  0.13508649276132761
iteration : 9099
train acc:  0.6953125
train loss:  0.5697176456451416
train gradient:  0.2830560169334474
iteration : 9100
train acc:  0.6875
train loss:  0.5420948266983032
train gradient:  0.1452845891128559
iteration : 9101
train acc:  0.7421875
train loss:  0.46599724888801575
train gradient:  0.11492622630935424
iteration : 9102
train acc:  0.7421875
train loss:  0.48158466815948486
train gradient:  0.11590250998027246
iteration : 9103
train acc:  0.75
train loss:  0.46771419048309326
train gradient:  0.10972594028481183
iteration : 9104
train acc:  0.671875
train loss:  0.5515118837356567
train gradient:  0.17993009053883763
iteration : 9105
train acc:  0.6953125
train loss:  0.5320237874984741
train gradient:  0.14093897878305284
iteration : 9106
train acc:  0.75
train loss:  0.5714605450630188
train gradient:  0.17454716496841782
iteration : 9107
train acc:  0.734375
train loss:  0.4341971278190613
train gradient:  0.17451094296560649
iteration : 9108
train acc:  0.71875
train loss:  0.5627961158752441
train gradient:  0.17593102809560487
iteration : 9109
train acc:  0.7265625
train loss:  0.5429157614707947
train gradient:  0.20658111886486363
iteration : 9110
train acc:  0.7578125
train loss:  0.47155505418777466
train gradient:  0.12428769453435788
iteration : 9111
train acc:  0.75
train loss:  0.4806784987449646
train gradient:  0.127814627585427
iteration : 9112
train acc:  0.7890625
train loss:  0.41237741708755493
train gradient:  0.08930924314549081
iteration : 9113
train acc:  0.671875
train loss:  0.5469688773155212
train gradient:  0.1267501976280002
iteration : 9114
train acc:  0.7109375
train loss:  0.489801824092865
train gradient:  0.1112622251353995
iteration : 9115
train acc:  0.71875
train loss:  0.5398094654083252
train gradient:  0.15510661461849534
iteration : 9116
train acc:  0.828125
train loss:  0.447063684463501
train gradient:  0.15008532964330507
iteration : 9117
train acc:  0.7578125
train loss:  0.48748353123664856
train gradient:  0.10821463724128463
iteration : 9118
train acc:  0.6640625
train loss:  0.56510329246521
train gradient:  0.16014089101201656
iteration : 9119
train acc:  0.6953125
train loss:  0.5362502336502075
train gradient:  0.1539407059109427
iteration : 9120
train acc:  0.6953125
train loss:  0.5025925636291504
train gradient:  0.16500748135490645
iteration : 9121
train acc:  0.734375
train loss:  0.5042091608047485
train gradient:  0.15638439245839061
iteration : 9122
train acc:  0.8046875
train loss:  0.43868717551231384
train gradient:  0.12561123682629066
iteration : 9123
train acc:  0.796875
train loss:  0.4569990634918213
train gradient:  0.11452077335624919
iteration : 9124
train acc:  0.7265625
train loss:  0.527310848236084
train gradient:  0.1457539345022105
iteration : 9125
train acc:  0.7265625
train loss:  0.5078128576278687
train gradient:  0.11436313010583524
iteration : 9126
train acc:  0.703125
train loss:  0.5607261061668396
train gradient:  0.19803386702875553
iteration : 9127
train acc:  0.75
train loss:  0.44769421219825745
train gradient:  0.09417547911925751
iteration : 9128
train acc:  0.7578125
train loss:  0.529675304889679
train gradient:  0.14788425144715278
iteration : 9129
train acc:  0.8046875
train loss:  0.4594850540161133
train gradient:  0.12625868615506758
iteration : 9130
train acc:  0.7734375
train loss:  0.4637500047683716
train gradient:  0.1089237906263723
iteration : 9131
train acc:  0.6875
train loss:  0.484978049993515
train gradient:  0.1213372822255991
iteration : 9132
train acc:  0.8125
train loss:  0.4472751319408417
train gradient:  0.09976672347036698
iteration : 9133
train acc:  0.734375
train loss:  0.5057921409606934
train gradient:  0.12902685600863212
iteration : 9134
train acc:  0.7109375
train loss:  0.5353895425796509
train gradient:  0.16567440653539228
iteration : 9135
train acc:  0.734375
train loss:  0.45681172609329224
train gradient:  0.09919378602884717
iteration : 9136
train acc:  0.75
train loss:  0.5029267072677612
train gradient:  0.11253755110103553
iteration : 9137
train acc:  0.7734375
train loss:  0.4511668086051941
train gradient:  0.12556107169335262
iteration : 9138
train acc:  0.703125
train loss:  0.5394263863563538
train gradient:  0.17601002284296158
iteration : 9139
train acc:  0.7421875
train loss:  0.5538274645805359
train gradient:  0.16895548417105413
iteration : 9140
train acc:  0.75
train loss:  0.4911166727542877
train gradient:  0.12549579524322557
iteration : 9141
train acc:  0.7578125
train loss:  0.47697126865386963
train gradient:  0.10027205874783178
iteration : 9142
train acc:  0.75
train loss:  0.545697033405304
train gradient:  0.14018228492365972
iteration : 9143
train acc:  0.7109375
train loss:  0.5163432359695435
train gradient:  0.1383265096358809
iteration : 9144
train acc:  0.7578125
train loss:  0.4751068949699402
train gradient:  0.12033165936837613
iteration : 9145
train acc:  0.703125
train loss:  0.5047227144241333
train gradient:  0.13743646935279094
iteration : 9146
train acc:  0.6484375
train loss:  0.6085948348045349
train gradient:  0.1854739317074034
iteration : 9147
train acc:  0.671875
train loss:  0.5199782848358154
train gradient:  0.14145766949043703
iteration : 9148
train acc:  0.765625
train loss:  0.4897752106189728
train gradient:  0.11822380381501338
iteration : 9149
train acc:  0.765625
train loss:  0.5181174278259277
train gradient:  0.1706141215721943
iteration : 9150
train acc:  0.8046875
train loss:  0.44485414028167725
train gradient:  0.10510372710025298
iteration : 9151
train acc:  0.7734375
train loss:  0.46207624673843384
train gradient:  0.10296504889187538
iteration : 9152
train acc:  0.7421875
train loss:  0.511228621006012
train gradient:  0.16014771439936262
iteration : 9153
train acc:  0.8046875
train loss:  0.48046135902404785
train gradient:  0.14159617225103738
iteration : 9154
train acc:  0.7421875
train loss:  0.48362159729003906
train gradient:  0.11409726270662371
iteration : 9155
train acc:  0.71875
train loss:  0.5033362507820129
train gradient:  0.11103214218366148
iteration : 9156
train acc:  0.734375
train loss:  0.4930118918418884
train gradient:  0.11948437315404949
iteration : 9157
train acc:  0.71875
train loss:  0.5211796760559082
train gradient:  0.18444304561553032
iteration : 9158
train acc:  0.7109375
train loss:  0.484478235244751
train gradient:  0.1273669987292122
iteration : 9159
train acc:  0.7890625
train loss:  0.4497634172439575
train gradient:  0.12213036218976442
iteration : 9160
train acc:  0.734375
train loss:  0.4977600574493408
train gradient:  0.1508192920179472
iteration : 9161
train acc:  0.75
train loss:  0.4855450391769409
train gradient:  0.10219460625208884
iteration : 9162
train acc:  0.7421875
train loss:  0.4634072780609131
train gradient:  0.11355540506479153
iteration : 9163
train acc:  0.671875
train loss:  0.5152627229690552
train gradient:  0.125785509686879
iteration : 9164
train acc:  0.7734375
train loss:  0.46494412422180176
train gradient:  0.1380342712925114
iteration : 9165
train acc:  0.7578125
train loss:  0.5248743295669556
train gradient:  0.1476290666494886
iteration : 9166
train acc:  0.734375
train loss:  0.5341379642486572
train gradient:  0.14557189315535712
iteration : 9167
train acc:  0.7734375
train loss:  0.4653806686401367
train gradient:  0.13159880480633185
iteration : 9168
train acc:  0.765625
train loss:  0.464153528213501
train gradient:  0.14245804681416907
iteration : 9169
train acc:  0.7734375
train loss:  0.46064913272857666
train gradient:  0.10696611375196784
iteration : 9170
train acc:  0.78125
train loss:  0.5199382305145264
train gradient:  0.14803016147660708
iteration : 9171
train acc:  0.78125
train loss:  0.5285024642944336
train gradient:  0.13430122135670985
iteration : 9172
train acc:  0.7109375
train loss:  0.5212860107421875
train gradient:  0.13957570529678448
iteration : 9173
train acc:  0.7265625
train loss:  0.5233623385429382
train gradient:  0.12318995603551365
iteration : 9174
train acc:  0.75
train loss:  0.5041255950927734
train gradient:  0.13068989437506828
iteration : 9175
train acc:  0.7578125
train loss:  0.4774453938007355
train gradient:  0.12385034564285817
iteration : 9176
train acc:  0.734375
train loss:  0.5115247964859009
train gradient:  0.11279370187552118
iteration : 9177
train acc:  0.6875
train loss:  0.5603520274162292
train gradient:  0.1374316484833154
iteration : 9178
train acc:  0.6953125
train loss:  0.5257980823516846
train gradient:  0.1767976101644959
iteration : 9179
train acc:  0.7265625
train loss:  0.5831663012504578
train gradient:  0.15156075683310477
iteration : 9180
train acc:  0.703125
train loss:  0.5467653274536133
train gradient:  0.16609457724603732
iteration : 9181
train acc:  0.765625
train loss:  0.48374664783477783
train gradient:  0.11995209814173188
iteration : 9182
train acc:  0.71875
train loss:  0.4700491428375244
train gradient:  0.1044492924793057
iteration : 9183
train acc:  0.6875
train loss:  0.5233577489852905
train gradient:  0.14298789229448064
iteration : 9184
train acc:  0.7109375
train loss:  0.5247213840484619
train gradient:  0.13403378601231503
iteration : 9185
train acc:  0.78125
train loss:  0.46515005826950073
train gradient:  0.12471176864517801
iteration : 9186
train acc:  0.734375
train loss:  0.554781436920166
train gradient:  0.14966382503236714
iteration : 9187
train acc:  0.7890625
train loss:  0.4146021604537964
train gradient:  0.09615736822268699
iteration : 9188
train acc:  0.7421875
train loss:  0.5297419428825378
train gradient:  0.1432024604420036
iteration : 9189
train acc:  0.765625
train loss:  0.4868747591972351
train gradient:  0.11433975995074212
iteration : 9190
train acc:  0.7265625
train loss:  0.4811573326587677
train gradient:  0.09698859586030716
iteration : 9191
train acc:  0.71875
train loss:  0.5520567893981934
train gradient:  0.14122039867016178
iteration : 9192
train acc:  0.6953125
train loss:  0.564523458480835
train gradient:  0.18593630087233562
iteration : 9193
train acc:  0.7265625
train loss:  0.5525214076042175
train gradient:  0.1517157357609316
iteration : 9194
train acc:  0.8203125
train loss:  0.4376264214515686
train gradient:  0.09149268562000458
iteration : 9195
train acc:  0.75
train loss:  0.512613832950592
train gradient:  0.14535313184233267
iteration : 9196
train acc:  0.765625
train loss:  0.4655850827693939
train gradient:  0.12390414834761881
iteration : 9197
train acc:  0.765625
train loss:  0.4769601821899414
train gradient:  0.10714852162577725
iteration : 9198
train acc:  0.6875
train loss:  0.5262201428413391
train gradient:  0.12274300274932004
iteration : 9199
train acc:  0.6953125
train loss:  0.5086939334869385
train gradient:  0.11596843182713927
iteration : 9200
train acc:  0.8046875
train loss:  0.4137997329235077
train gradient:  0.08993345364118452
iteration : 9201
train acc:  0.7109375
train loss:  0.5203570127487183
train gradient:  0.11066756956022586
iteration : 9202
train acc:  0.8125
train loss:  0.3975365161895752
train gradient:  0.10751114212753006
iteration : 9203
train acc:  0.734375
train loss:  0.4912060797214508
train gradient:  0.11342212966360685
iteration : 9204
train acc:  0.8046875
train loss:  0.4249840974807739
train gradient:  0.10674213515366711
iteration : 9205
train acc:  0.7421875
train loss:  0.4623306095600128
train gradient:  0.10781640433112272
iteration : 9206
train acc:  0.7734375
train loss:  0.4467940628528595
train gradient:  0.0984870582046804
iteration : 9207
train acc:  0.7421875
train loss:  0.45023030042648315
train gradient:  0.10409543681793036
iteration : 9208
train acc:  0.6875
train loss:  0.5341551303863525
train gradient:  0.1469860390674
iteration : 9209
train acc:  0.6875
train loss:  0.5293511748313904
train gradient:  0.14411125500490488
iteration : 9210
train acc:  0.71875
train loss:  0.5023969411849976
train gradient:  0.16373537945677843
iteration : 9211
train acc:  0.7421875
train loss:  0.5101560950279236
train gradient:  0.15839221046204727
iteration : 9212
train acc:  0.7421875
train loss:  0.4860551059246063
train gradient:  0.10991904059009865
iteration : 9213
train acc:  0.7421875
train loss:  0.46311017870903015
train gradient:  0.10070200055006809
iteration : 9214
train acc:  0.7421875
train loss:  0.4681282639503479
train gradient:  0.12042318703783626
iteration : 9215
train acc:  0.671875
train loss:  0.506497859954834
train gradient:  0.11547476155659506
iteration : 9216
train acc:  0.6953125
train loss:  0.5217456221580505
train gradient:  0.17004115938415276
iteration : 9217
train acc:  0.734375
train loss:  0.513414740562439
train gradient:  0.1316097245547711
iteration : 9218
train acc:  0.7421875
train loss:  0.510168731212616
train gradient:  0.1497084919634164
iteration : 9219
train acc:  0.6796875
train loss:  0.5404684543609619
train gradient:  0.16134984758840232
iteration : 9220
train acc:  0.7578125
train loss:  0.4768299460411072
train gradient:  0.1037655658180826
iteration : 9221
train acc:  0.765625
train loss:  0.5515576004981995
train gradient:  0.17266150452839002
iteration : 9222
train acc:  0.6171875
train loss:  0.6044648885726929
train gradient:  0.19713191212611908
iteration : 9223
train acc:  0.6953125
train loss:  0.5035022497177124
train gradient:  0.12160853726143125
iteration : 9224
train acc:  0.828125
train loss:  0.4086053967475891
train gradient:  0.09838620134846178
iteration : 9225
train acc:  0.78125
train loss:  0.4632496237754822
train gradient:  0.08738613761791858
iteration : 9226
train acc:  0.7421875
train loss:  0.47805076837539673
train gradient:  0.09341569288612069
iteration : 9227
train acc:  0.71875
train loss:  0.5222293734550476
train gradient:  0.1529843764558732
iteration : 9228
train acc:  0.765625
train loss:  0.47379663586616516
train gradient:  0.13307569269500336
iteration : 9229
train acc:  0.71875
train loss:  0.4903807044029236
train gradient:  0.12806167598712812
iteration : 9230
train acc:  0.7109375
train loss:  0.5407783389091492
train gradient:  0.2048618232931334
iteration : 9231
train acc:  0.796875
train loss:  0.445904016494751
train gradient:  0.12019564021981138
iteration : 9232
train acc:  0.7109375
train loss:  0.4782273769378662
train gradient:  0.10246657784149543
iteration : 9233
train acc:  0.7109375
train loss:  0.5617524981498718
train gradient:  0.1577964712554784
iteration : 9234
train acc:  0.6875
train loss:  0.5790300369262695
train gradient:  0.14342602549959887
iteration : 9235
train acc:  0.7265625
train loss:  0.5186285972595215
train gradient:  0.129105235112569
iteration : 9236
train acc:  0.7265625
train loss:  0.5015338063240051
train gradient:  0.10599618949140538
iteration : 9237
train acc:  0.7578125
train loss:  0.49474021792411804
train gradient:  0.14474962293632782
iteration : 9238
train acc:  0.6796875
train loss:  0.5769319534301758
train gradient:  0.15404676553659624
iteration : 9239
train acc:  0.6796875
train loss:  0.6141528487205505
train gradient:  0.20340775293024738
iteration : 9240
train acc:  0.734375
train loss:  0.4777600169181824
train gradient:  0.11009281094865787
iteration : 9241
train acc:  0.7578125
train loss:  0.5263068675994873
train gradient:  0.15092265444256037
iteration : 9242
train acc:  0.71875
train loss:  0.5970571637153625
train gradient:  0.17596305696194964
iteration : 9243
train acc:  0.765625
train loss:  0.4964619576931
train gradient:  0.15766334604412008
iteration : 9244
train acc:  0.734375
train loss:  0.4589584469795227
train gradient:  0.1230016552325802
iteration : 9245
train acc:  0.75
train loss:  0.46517622470855713
train gradient:  0.11605008149577785
iteration : 9246
train acc:  0.71875
train loss:  0.5113996267318726
train gradient:  0.11846105410850151
iteration : 9247
train acc:  0.671875
train loss:  0.5584763288497925
train gradient:  0.17448319630852382
iteration : 9248
train acc:  0.7890625
train loss:  0.42209112644195557
train gradient:  0.1273205627970185
iteration : 9249
train acc:  0.7734375
train loss:  0.4257166087627411
train gradient:  0.09628598241631497
iteration : 9250
train acc:  0.71875
train loss:  0.5407655239105225
train gradient:  0.16721387634860024
iteration : 9251
train acc:  0.7265625
train loss:  0.4912611246109009
train gradient:  0.1378761719367575
iteration : 9252
train acc:  0.7109375
train loss:  0.48476681113243103
train gradient:  0.10006422198342145
iteration : 9253
train acc:  0.734375
train loss:  0.5073655843734741
train gradient:  0.1347002383194863
iteration : 9254
train acc:  0.75
train loss:  0.49538588523864746
train gradient:  0.13141785578413098
iteration : 9255
train acc:  0.75
train loss:  0.5047338008880615
train gradient:  0.13194397675131303
iteration : 9256
train acc:  0.65625
train loss:  0.5776126384735107
train gradient:  0.1884302590179478
iteration : 9257
train acc:  0.703125
train loss:  0.5451573133468628
train gradient:  0.1660907727421423
iteration : 9258
train acc:  0.8203125
train loss:  0.4782029092311859
train gradient:  0.1258937855373098
iteration : 9259
train acc:  0.7734375
train loss:  0.47721534967422485
train gradient:  0.15573899947212427
iteration : 9260
train acc:  0.7109375
train loss:  0.5210146903991699
train gradient:  0.12158380910143404
iteration : 9261
train acc:  0.71875
train loss:  0.5171855688095093
train gradient:  0.1417706016317805
iteration : 9262
train acc:  0.78125
train loss:  0.4836799204349518
train gradient:  0.15666252504423275
iteration : 9263
train acc:  0.7578125
train loss:  0.5003191828727722
train gradient:  0.13081516555321943
iteration : 9264
train acc:  0.8046875
train loss:  0.45281437039375305
train gradient:  0.11017227862173956
iteration : 9265
train acc:  0.7421875
train loss:  0.504662275314331
train gradient:  0.14650693133276232
iteration : 9266
train acc:  0.7421875
train loss:  0.49382543563842773
train gradient:  0.11490563930641516
iteration : 9267
train acc:  0.7734375
train loss:  0.4682944715023041
train gradient:  0.11327460847084297
iteration : 9268
train acc:  0.7734375
train loss:  0.45533356070518494
train gradient:  0.09336469325433566
iteration : 9269
train acc:  0.7109375
train loss:  0.5709797143936157
train gradient:  0.155147829853786
iteration : 9270
train acc:  0.75
train loss:  0.506007730960846
train gradient:  0.173440673555529
iteration : 9271
train acc:  0.796875
train loss:  0.5268179774284363
train gradient:  0.1309742099048299
iteration : 9272
train acc:  0.7578125
train loss:  0.47556376457214355
train gradient:  0.16853109284596035
iteration : 9273
train acc:  0.7421875
train loss:  0.5373357534408569
train gradient:  0.14398215194735328
iteration : 9274
train acc:  0.6875
train loss:  0.5334614515304565
train gradient:  0.16901519063666925
iteration : 9275
train acc:  0.7578125
train loss:  0.5202274322509766
train gradient:  0.13571530846294466
iteration : 9276
train acc:  0.7421875
train loss:  0.4419265687465668
train gradient:  0.10349420773236419
iteration : 9277
train acc:  0.71875
train loss:  0.4883236885070801
train gradient:  0.14035374648179222
iteration : 9278
train acc:  0.7421875
train loss:  0.4898676872253418
train gradient:  0.1494931424604749
iteration : 9279
train acc:  0.6796875
train loss:  0.5564416646957397
train gradient:  0.17480815565859548
iteration : 9280
train acc:  0.6640625
train loss:  0.6079173684120178
train gradient:  0.19598552397293909
iteration : 9281
train acc:  0.78125
train loss:  0.44592946767807007
train gradient:  0.13387961981403657
iteration : 9282
train acc:  0.765625
train loss:  0.44123363494873047
train gradient:  0.10109297282961237
iteration : 9283
train acc:  0.78125
train loss:  0.4716353118419647
train gradient:  0.16196597538484775
iteration : 9284
train acc:  0.703125
train loss:  0.5495478510856628
train gradient:  0.18651631718706568
iteration : 9285
train acc:  0.703125
train loss:  0.5473059415817261
train gradient:  0.15442063031387276
iteration : 9286
train acc:  0.703125
train loss:  0.5340993404388428
train gradient:  0.13444415787093433
iteration : 9287
train acc:  0.7421875
train loss:  0.5018540024757385
train gradient:  0.15932309060226035
iteration : 9288
train acc:  0.734375
train loss:  0.47944337129592896
train gradient:  0.10817507060617976
iteration : 9289
train acc:  0.65625
train loss:  0.5252617597579956
train gradient:  0.13112056251981863
iteration : 9290
train acc:  0.8125
train loss:  0.44558092951774597
train gradient:  0.09516938716297668
iteration : 9291
train acc:  0.6875
train loss:  0.527265727519989
train gradient:  0.1381616350283348
iteration : 9292
train acc:  0.75
train loss:  0.46455565094947815
train gradient:  0.11351310699689121
iteration : 9293
train acc:  0.796875
train loss:  0.46356213092803955
train gradient:  0.11210523905533941
iteration : 9294
train acc:  0.7578125
train loss:  0.46066027879714966
train gradient:  0.1277033278428289
iteration : 9295
train acc:  0.6796875
train loss:  0.5080714225769043
train gradient:  0.1221184208244436
iteration : 9296
train acc:  0.75
train loss:  0.4933841824531555
train gradient:  0.1266239531837016
iteration : 9297
train acc:  0.75
train loss:  0.48846369981765747
train gradient:  0.14713666822795046
iteration : 9298
train acc:  0.6796875
train loss:  0.5593394637107849
train gradient:  0.14022472545994913
iteration : 9299
train acc:  0.75
train loss:  0.48648184537887573
train gradient:  0.1302517412096551
iteration : 9300
train acc:  0.734375
train loss:  0.49135032296180725
train gradient:  0.15115535163680505
iteration : 9301
train acc:  0.734375
train loss:  0.5238257646560669
train gradient:  0.13412939464679668
iteration : 9302
train acc:  0.75
train loss:  0.47326838970184326
train gradient:  0.11918621906155996
iteration : 9303
train acc:  0.734375
train loss:  0.47574561834335327
train gradient:  0.14979865009677745
iteration : 9304
train acc:  0.734375
train loss:  0.5252484679222107
train gradient:  0.13550170171950923
iteration : 9305
train acc:  0.7890625
train loss:  0.4872097969055176
train gradient:  0.14302634854070723
iteration : 9306
train acc:  0.7421875
train loss:  0.487596333026886
train gradient:  0.12293038317971165
iteration : 9307
train acc:  0.734375
train loss:  0.49291276931762695
train gradient:  0.16281043431931452
iteration : 9308
train acc:  0.7109375
train loss:  0.5084072947502136
train gradient:  0.1501777198030601
iteration : 9309
train acc:  0.6875
train loss:  0.4940105676651001
train gradient:  0.12996194448438148
iteration : 9310
train acc:  0.703125
train loss:  0.5083209276199341
train gradient:  0.11548328053064415
iteration : 9311
train acc:  0.609375
train loss:  0.6632800698280334
train gradient:  0.2486001821124969
iteration : 9312
train acc:  0.7421875
train loss:  0.49155810475349426
train gradient:  0.11780709229143034
iteration : 9313
train acc:  0.7578125
train loss:  0.5221596956253052
train gradient:  0.14144656141901438
iteration : 9314
train acc:  0.703125
train loss:  0.5551916360855103
train gradient:  0.16857736829447917
iteration : 9315
train acc:  0.7265625
train loss:  0.5213484764099121
train gradient:  0.16181794931972907
iteration : 9316
train acc:  0.71875
train loss:  0.49604561924934387
train gradient:  0.1391188748601729
iteration : 9317
train acc:  0.7421875
train loss:  0.4847431778907776
train gradient:  0.1440712959496452
iteration : 9318
train acc:  0.6953125
train loss:  0.5635923147201538
train gradient:  0.1589066709567898
iteration : 9319
train acc:  0.7109375
train loss:  0.5868173837661743
train gradient:  0.19397473393304282
iteration : 9320
train acc:  0.671875
train loss:  0.5436568260192871
train gradient:  0.12702132351516837
iteration : 9321
train acc:  0.6953125
train loss:  0.5046579837799072
train gradient:  0.1425963331912783
iteration : 9322
train acc:  0.6875
train loss:  0.5335982441902161
train gradient:  0.14982828143645271
iteration : 9323
train acc:  0.71875
train loss:  0.5462049841880798
train gradient:  0.15788349548886843
iteration : 9324
train acc:  0.7265625
train loss:  0.5353807210922241
train gradient:  0.12775353542291545
iteration : 9325
train acc:  0.734375
train loss:  0.49902018904685974
train gradient:  0.1191941002472955
iteration : 9326
train acc:  0.7421875
train loss:  0.4921918511390686
train gradient:  0.1198635610856629
iteration : 9327
train acc:  0.7578125
train loss:  0.4913598895072937
train gradient:  0.12131378605560035
iteration : 9328
train acc:  0.734375
train loss:  0.4587406516075134
train gradient:  0.0979948373367799
iteration : 9329
train acc:  0.71875
train loss:  0.48415300250053406
train gradient:  0.09939003875538115
iteration : 9330
train acc:  0.734375
train loss:  0.46599912643432617
train gradient:  0.1525794460701999
iteration : 9331
train acc:  0.734375
train loss:  0.5012645721435547
train gradient:  0.1396195729596438
iteration : 9332
train acc:  0.765625
train loss:  0.5020430088043213
train gradient:  0.16525240934678131
iteration : 9333
train acc:  0.75
train loss:  0.5111234188079834
train gradient:  0.13750753955759437
iteration : 9334
train acc:  0.796875
train loss:  0.4324401617050171
train gradient:  0.09780462994009141
iteration : 9335
train acc:  0.7265625
train loss:  0.5168508291244507
train gradient:  0.1193949187206151
iteration : 9336
train acc:  0.7578125
train loss:  0.4684966206550598
train gradient:  0.10802811077991503
iteration : 9337
train acc:  0.7265625
train loss:  0.47898125648498535
train gradient:  0.1504521383495157
iteration : 9338
train acc:  0.734375
train loss:  0.49523481726646423
train gradient:  0.12477297141011168
iteration : 9339
train acc:  0.7421875
train loss:  0.48283442854881287
train gradient:  0.11470131183679955
iteration : 9340
train acc:  0.6875
train loss:  0.553583562374115
train gradient:  0.17125987854474267
iteration : 9341
train acc:  0.7109375
train loss:  0.49242541193962097
train gradient:  0.16185423239298757
iteration : 9342
train acc:  0.765625
train loss:  0.4981308579444885
train gradient:  0.14873033427666374
iteration : 9343
train acc:  0.7578125
train loss:  0.45849210023880005
train gradient:  0.1279393662202511
iteration : 9344
train acc:  0.8125
train loss:  0.42048296332359314
train gradient:  0.09511989045366065
iteration : 9345
train acc:  0.6796875
train loss:  0.5977723598480225
train gradient:  0.17326841261465697
iteration : 9346
train acc:  0.7578125
train loss:  0.44184380769729614
train gradient:  0.11218712871068295
iteration : 9347
train acc:  0.71875
train loss:  0.5101722478866577
train gradient:  0.1541098063390895
iteration : 9348
train acc:  0.7578125
train loss:  0.4741058945655823
train gradient:  0.10977043335172645
iteration : 9349
train acc:  0.765625
train loss:  0.4510062634944916
train gradient:  0.11066934164998869
iteration : 9350
train acc:  0.7109375
train loss:  0.5052033066749573
train gradient:  0.1245615815747474
iteration : 9351
train acc:  0.7734375
train loss:  0.4401910901069641
train gradient:  0.10747130108995694
iteration : 9352
train acc:  0.7578125
train loss:  0.4995807409286499
train gradient:  0.11941283748418811
iteration : 9353
train acc:  0.796875
train loss:  0.4938178062438965
train gradient:  0.10232462037383837
iteration : 9354
train acc:  0.7109375
train loss:  0.4811647832393646
train gradient:  0.1279895978197328
iteration : 9355
train acc:  0.765625
train loss:  0.4692699909210205
train gradient:  0.14668309604693264
iteration : 9356
train acc:  0.703125
train loss:  0.5275763273239136
train gradient:  0.13004654763940526
iteration : 9357
train acc:  0.6796875
train loss:  0.5275962948799133
train gradient:  0.1511225339330102
iteration : 9358
train acc:  0.7734375
train loss:  0.5140700936317444
train gradient:  0.15215675210380952
iteration : 9359
train acc:  0.703125
train loss:  0.514254093170166
train gradient:  0.15329903271634293
iteration : 9360
train acc:  0.765625
train loss:  0.4447697401046753
train gradient:  0.1017424641773547
iteration : 9361
train acc:  0.796875
train loss:  0.42084696888923645
train gradient:  0.09580893780171314
iteration : 9362
train acc:  0.7890625
train loss:  0.43506520986557007
train gradient:  0.09028485867953637
iteration : 9363
train acc:  0.6796875
train loss:  0.5536471009254456
train gradient:  0.16135262546283272
iteration : 9364
train acc:  0.71875
train loss:  0.47533732652664185
train gradient:  0.11425270689533258
iteration : 9365
train acc:  0.71875
train loss:  0.5226194858551025
train gradient:  0.14152517558639593
iteration : 9366
train acc:  0.703125
train loss:  0.542722225189209
train gradient:  0.15798666911108267
iteration : 9367
train acc:  0.75
train loss:  0.4856484830379486
train gradient:  0.1339318932885038
iteration : 9368
train acc:  0.6953125
train loss:  0.5130372047424316
train gradient:  0.1509950131030231
iteration : 9369
train acc:  0.796875
train loss:  0.43517547845840454
train gradient:  0.11887471783431114
iteration : 9370
train acc:  0.7265625
train loss:  0.4651626646518707
train gradient:  0.11747401333220032
iteration : 9371
train acc:  0.6953125
train loss:  0.5302561521530151
train gradient:  0.14469084028541757
iteration : 9372
train acc:  0.6328125
train loss:  0.6265028715133667
train gradient:  0.1955488711298196
iteration : 9373
train acc:  0.75
train loss:  0.46636247634887695
train gradient:  0.13061870732236758
iteration : 9374
train acc:  0.734375
train loss:  0.5337337255477905
train gradient:  0.1332654282914536
iteration : 9375
train acc:  0.6953125
train loss:  0.568445086479187
train gradient:  0.1671703210392813
iteration : 9376
train acc:  0.796875
train loss:  0.4264994263648987
train gradient:  0.112972037939358
iteration : 9377
train acc:  0.828125
train loss:  0.3906572759151459
train gradient:  0.07597392090995751
iteration : 9378
train acc:  0.6875
train loss:  0.5320414304733276
train gradient:  0.15087902654442822
iteration : 9379
train acc:  0.765625
train loss:  0.48425790667533875
train gradient:  0.13931411156684104
iteration : 9380
train acc:  0.796875
train loss:  0.5069478750228882
train gradient:  0.12315141982241687
iteration : 9381
train acc:  0.7421875
train loss:  0.49359405040740967
train gradient:  0.12881990024649043
iteration : 9382
train acc:  0.7421875
train loss:  0.4716959297657013
train gradient:  0.13045964500219448
iteration : 9383
train acc:  0.7421875
train loss:  0.5045086145401001
train gradient:  0.1351277796045697
iteration : 9384
train acc:  0.7421875
train loss:  0.4247431755065918
train gradient:  0.0887416216777272
iteration : 9385
train acc:  0.734375
train loss:  0.5034464597702026
train gradient:  0.10643069694014393
iteration : 9386
train acc:  0.765625
train loss:  0.4232277572154999
train gradient:  0.09194266963826522
iteration : 9387
train acc:  0.6953125
train loss:  0.5616182088851929
train gradient:  0.16442603300623732
iteration : 9388
train acc:  0.6953125
train loss:  0.5331785678863525
train gradient:  0.14615126054372712
iteration : 9389
train acc:  0.765625
train loss:  0.45283758640289307
train gradient:  0.12592132028479486
iteration : 9390
train acc:  0.7265625
train loss:  0.5386343002319336
train gradient:  0.16320449169371715
iteration : 9391
train acc:  0.7578125
train loss:  0.5098245143890381
train gradient:  0.13900404679605805
iteration : 9392
train acc:  0.671875
train loss:  0.5323048830032349
train gradient:  0.14138204776219362
iteration : 9393
train acc:  0.7421875
train loss:  0.5069677829742432
train gradient:  0.13241609139582727
iteration : 9394
train acc:  0.7421875
train loss:  0.4883157014846802
train gradient:  0.12874802748123879
iteration : 9395
train acc:  0.6796875
train loss:  0.5438417196273804
train gradient:  0.17215905493739087
iteration : 9396
train acc:  0.75
train loss:  0.5046530365943909
train gradient:  0.11069681525368928
iteration : 9397
train acc:  0.734375
train loss:  0.494451642036438
train gradient:  0.13064183541425906
iteration : 9398
train acc:  0.75
train loss:  0.5024383068084717
train gradient:  0.1291209864145872
iteration : 9399
train acc:  0.7109375
train loss:  0.5136203765869141
train gradient:  0.15617224299099056
iteration : 9400
train acc:  0.7421875
train loss:  0.4682328999042511
train gradient:  0.104748348109377
iteration : 9401
train acc:  0.6953125
train loss:  0.5336730480194092
train gradient:  0.15651621461993154
iteration : 9402
train acc:  0.7265625
train loss:  0.5466524362564087
train gradient:  0.16145195273382715
iteration : 9403
train acc:  0.7734375
train loss:  0.44902482628822327
train gradient:  0.09491410840211413
iteration : 9404
train acc:  0.6796875
train loss:  0.6083065271377563
train gradient:  0.24830152733502484
iteration : 9405
train acc:  0.6796875
train loss:  0.49276354908943176
train gradient:  0.1385491153698747
iteration : 9406
train acc:  0.71875
train loss:  0.5225204229354858
train gradient:  0.10948434264884156
iteration : 9407
train acc:  0.734375
train loss:  0.45818012952804565
train gradient:  0.15964005469248088
iteration : 9408
train acc:  0.6484375
train loss:  0.5591310262680054
train gradient:  0.14545828994261795
iteration : 9409
train acc:  0.8359375
train loss:  0.41699954867362976
train gradient:  0.09058180070725888
iteration : 9410
train acc:  0.8203125
train loss:  0.4426673650741577
train gradient:  0.11065110149951067
iteration : 9411
train acc:  0.65625
train loss:  0.5285708904266357
train gradient:  0.16168677771141793
iteration : 9412
train acc:  0.7734375
train loss:  0.47540077567100525
train gradient:  0.09733663001286923
iteration : 9413
train acc:  0.75
train loss:  0.48013824224472046
train gradient:  0.10744926519099687
iteration : 9414
train acc:  0.8203125
train loss:  0.41715678572654724
train gradient:  0.11072963399831229
iteration : 9415
train acc:  0.75
train loss:  0.4885353446006775
train gradient:  0.12344504834682363
iteration : 9416
train acc:  0.703125
train loss:  0.5911868810653687
train gradient:  0.1890127447381375
iteration : 9417
train acc:  0.75
train loss:  0.472928524017334
train gradient:  0.13015191683320101
iteration : 9418
train acc:  0.7734375
train loss:  0.4243345260620117
train gradient:  0.11624030241104044
iteration : 9419
train acc:  0.7265625
train loss:  0.5289592742919922
train gradient:  0.12537998976250375
iteration : 9420
train acc:  0.671875
train loss:  0.5276864767074585
train gradient:  0.14510378122096257
iteration : 9421
train acc:  0.7265625
train loss:  0.52669358253479
train gradient:  0.15104401940280393
iteration : 9422
train acc:  0.703125
train loss:  0.5176523923873901
train gradient:  0.13074619601190163
iteration : 9423
train acc:  0.6953125
train loss:  0.4872818887233734
train gradient:  0.11712800195385847
iteration : 9424
train acc:  0.8046875
train loss:  0.42026692628860474
train gradient:  0.11861839187837282
iteration : 9425
train acc:  0.7265625
train loss:  0.5067517161369324
train gradient:  0.13941287219721188
iteration : 9426
train acc:  0.7421875
train loss:  0.532781183719635
train gradient:  0.1566274923371988
iteration : 9427
train acc:  0.7265625
train loss:  0.5035302639007568
train gradient:  0.12473591942906552
iteration : 9428
train acc:  0.78125
train loss:  0.451820969581604
train gradient:  0.10184053064211049
iteration : 9429
train acc:  0.640625
train loss:  0.5617682933807373
train gradient:  0.14277233274960252
iteration : 9430
train acc:  0.7578125
train loss:  0.5187298059463501
train gradient:  0.1511251028127481
iteration : 9431
train acc:  0.75
train loss:  0.4472324252128601
train gradient:  0.1195343939895937
iteration : 9432
train acc:  0.7734375
train loss:  0.4552418291568756
train gradient:  0.13222741561028656
iteration : 9433
train acc:  0.6484375
train loss:  0.5715895295143127
train gradient:  0.1752433387247485
iteration : 9434
train acc:  0.765625
train loss:  0.505341649055481
train gradient:  0.11590370014566856
iteration : 9435
train acc:  0.6953125
train loss:  0.5349395275115967
train gradient:  0.1508609206831822
iteration : 9436
train acc:  0.7109375
train loss:  0.48955366015434265
train gradient:  0.11456869955561867
iteration : 9437
train acc:  0.7578125
train loss:  0.5284480452537537
train gradient:  0.12209388716578154
iteration : 9438
train acc:  0.6953125
train loss:  0.5397745370864868
train gradient:  0.17009635332608286
iteration : 9439
train acc:  0.6640625
train loss:  0.5446147918701172
train gradient:  0.15427377576304258
iteration : 9440
train acc:  0.7109375
train loss:  0.5439824461936951
train gradient:  0.15958425112547292
iteration : 9441
train acc:  0.7578125
train loss:  0.44639211893081665
train gradient:  0.10882658350271923
iteration : 9442
train acc:  0.7421875
train loss:  0.4743284285068512
train gradient:  0.1316209187713422
iteration : 9443
train acc:  0.703125
train loss:  0.4873165488243103
train gradient:  0.1319256663719154
iteration : 9444
train acc:  0.7578125
train loss:  0.45977523922920227
train gradient:  0.1331344341031753
iteration : 9445
train acc:  0.78125
train loss:  0.4628015458583832
train gradient:  0.11840366914659825
iteration : 9446
train acc:  0.7265625
train loss:  0.5270609855651855
train gradient:  0.16976670309676556
iteration : 9447
train acc:  0.6953125
train loss:  0.5390359163284302
train gradient:  0.13368448611521194
iteration : 9448
train acc:  0.71875
train loss:  0.5028202533721924
train gradient:  0.146446592452477
iteration : 9449
train acc:  0.734375
train loss:  0.5066848993301392
train gradient:  0.1230189100783185
iteration : 9450
train acc:  0.7109375
train loss:  0.5408859252929688
train gradient:  0.1591036176468163
iteration : 9451
train acc:  0.7421875
train loss:  0.4777478575706482
train gradient:  0.13064215321532516
iteration : 9452
train acc:  0.7265625
train loss:  0.49565741419792175
train gradient:  0.12800208521360945
iteration : 9453
train acc:  0.6953125
train loss:  0.5165753364562988
train gradient:  0.12483133419145818
iteration : 9454
train acc:  0.71875
train loss:  0.5393922328948975
train gradient:  0.12366782846594312
iteration : 9455
train acc:  0.7109375
train loss:  0.5227149724960327
train gradient:  0.14720756861066445
iteration : 9456
train acc:  0.796875
train loss:  0.4691840410232544
train gradient:  0.11387795862057247
iteration : 9457
train acc:  0.7265625
train loss:  0.5116320848464966
train gradient:  0.1287667235188077
iteration : 9458
train acc:  0.8125
train loss:  0.4265735149383545
train gradient:  0.11411747504995473
iteration : 9459
train acc:  0.6796875
train loss:  0.5045709013938904
train gradient:  0.12420415773507025
iteration : 9460
train acc:  0.65625
train loss:  0.5887768268585205
train gradient:  0.17464812728847626
iteration : 9461
train acc:  0.703125
train loss:  0.48861992359161377
train gradient:  0.12808037705779174
iteration : 9462
train acc:  0.734375
train loss:  0.5092346668243408
train gradient:  0.15518621932834226
iteration : 9463
train acc:  0.765625
train loss:  0.42613381147384644
train gradient:  0.09600430174110025
iteration : 9464
train acc:  0.6875
train loss:  0.5418031215667725
train gradient:  0.1530321530191177
iteration : 9465
train acc:  0.7109375
train loss:  0.5204115509986877
train gradient:  0.13741587417556028
iteration : 9466
train acc:  0.6484375
train loss:  0.5704134106636047
train gradient:  0.18775033352546916
iteration : 9467
train acc:  0.8125
train loss:  0.4665378928184509
train gradient:  0.11022724865130214
iteration : 9468
train acc:  0.7890625
train loss:  0.4475083351135254
train gradient:  0.11156216401421538
iteration : 9469
train acc:  0.8046875
train loss:  0.40178346633911133
train gradient:  0.09693266779523131
iteration : 9470
train acc:  0.734375
train loss:  0.5058593153953552
train gradient:  0.14670130238456924
iteration : 9471
train acc:  0.8359375
train loss:  0.4447234272956848
train gradient:  0.1280052081683274
iteration : 9472
train acc:  0.7578125
train loss:  0.5035122036933899
train gradient:  0.13182463501632097
iteration : 9473
train acc:  0.7734375
train loss:  0.4876564145088196
train gradient:  0.11223953593523187
iteration : 9474
train acc:  0.734375
train loss:  0.494949609041214
train gradient:  0.17187747045491214
iteration : 9475
train acc:  0.8046875
train loss:  0.4532882273197174
train gradient:  0.13484024589948362
iteration : 9476
train acc:  0.765625
train loss:  0.4902381896972656
train gradient:  0.12664745163682123
iteration : 9477
train acc:  0.78125
train loss:  0.468906432390213
train gradient:  0.11992951972493704
iteration : 9478
train acc:  0.7421875
train loss:  0.48147961497306824
train gradient:  0.12010696764617361
iteration : 9479
train acc:  0.6875
train loss:  0.49708110094070435
train gradient:  0.12460214027896786
iteration : 9480
train acc:  0.75
train loss:  0.47764742374420166
train gradient:  0.10055547161619065
iteration : 9481
train acc:  0.75
train loss:  0.4736669361591339
train gradient:  0.11694288096845344
iteration : 9482
train acc:  0.734375
train loss:  0.5176748037338257
train gradient:  0.15352107680433358
iteration : 9483
train acc:  0.75
train loss:  0.4453366994857788
train gradient:  0.14201885736516123
iteration : 9484
train acc:  0.8046875
train loss:  0.44663766026496887
train gradient:  0.1022063681786025
iteration : 9485
train acc:  0.7578125
train loss:  0.5226203799247742
train gradient:  0.1355406942333249
iteration : 9486
train acc:  0.7265625
train loss:  0.4979236125946045
train gradient:  0.1201880121384408
iteration : 9487
train acc:  0.75
train loss:  0.4905240535736084
train gradient:  0.11823924170703069
iteration : 9488
train acc:  0.7734375
train loss:  0.46340620517730713
train gradient:  0.1470943909944184
iteration : 9489
train acc:  0.75
train loss:  0.5233046412467957
train gradient:  0.14137645793793419
iteration : 9490
train acc:  0.7421875
train loss:  0.4764223098754883
train gradient:  0.12852691764794968
iteration : 9491
train acc:  0.796875
train loss:  0.422526478767395
train gradient:  0.10487967761853959
iteration : 9492
train acc:  0.71875
train loss:  0.5090381503105164
train gradient:  0.17150818915178667
iteration : 9493
train acc:  0.7578125
train loss:  0.45351386070251465
train gradient:  0.12218865090529457
iteration : 9494
train acc:  0.6875
train loss:  0.5058111548423767
train gradient:  0.15030857460418146
iteration : 9495
train acc:  0.84375
train loss:  0.3952295184135437
train gradient:  0.07606877642754102
iteration : 9496
train acc:  0.7578125
train loss:  0.5249501466751099
train gradient:  0.17051756343028227
iteration : 9497
train acc:  0.734375
train loss:  0.47830384969711304
train gradient:  0.128070394960673
iteration : 9498
train acc:  0.765625
train loss:  0.5136436223983765
train gradient:  0.11871166688646391
iteration : 9499
train acc:  0.7109375
train loss:  0.4779566526412964
train gradient:  0.14381064184901646
iteration : 9500
train acc:  0.734375
train loss:  0.5154143571853638
train gradient:  0.13115618324240041
iteration : 9501
train acc:  0.765625
train loss:  0.4424699544906616
train gradient:  0.10617537496112227
iteration : 9502
train acc:  0.7265625
train loss:  0.4640032947063446
train gradient:  0.12750060008453817
iteration : 9503
train acc:  0.7109375
train loss:  0.5210158228874207
train gradient:  0.16282269421740198
iteration : 9504
train acc:  0.7734375
train loss:  0.4499014616012573
train gradient:  0.13753214818300186
iteration : 9505
train acc:  0.75
train loss:  0.48326992988586426
train gradient:  0.12316755997355935
iteration : 9506
train acc:  0.71875
train loss:  0.5275626182556152
train gradient:  0.1346757816844718
iteration : 9507
train acc:  0.796875
train loss:  0.4393102824687958
train gradient:  0.11173796763613432
iteration : 9508
train acc:  0.7578125
train loss:  0.5050755739212036
train gradient:  0.13936957861566754
iteration : 9509
train acc:  0.6875
train loss:  0.614611804485321
train gradient:  0.15577673402607506
iteration : 9510
train acc:  0.7109375
train loss:  0.5048361420631409
train gradient:  0.14499186141765763
iteration : 9511
train acc:  0.7265625
train loss:  0.5366033315658569
train gradient:  0.14521508284326265
iteration : 9512
train acc:  0.71875
train loss:  0.5267952680587769
train gradient:  0.14617990837905856
iteration : 9513
train acc:  0.734375
train loss:  0.527966320514679
train gradient:  0.15536280559455756
iteration : 9514
train acc:  0.796875
train loss:  0.44051438570022583
train gradient:  0.11298038963929492
iteration : 9515
train acc:  0.703125
train loss:  0.49930140376091003
train gradient:  0.1228056679177254
iteration : 9516
train acc:  0.75
train loss:  0.472322940826416
train gradient:  0.11341820999467397
iteration : 9517
train acc:  0.7109375
train loss:  0.5341678261756897
train gradient:  0.16057399282669468
iteration : 9518
train acc:  0.7578125
train loss:  0.4580705761909485
train gradient:  0.10959468673180325
iteration : 9519
train acc:  0.796875
train loss:  0.48589470982551575
train gradient:  0.11218051351485123
iteration : 9520
train acc:  0.6953125
train loss:  0.537205159664154
train gradient:  0.1408725545010322
iteration : 9521
train acc:  0.7421875
train loss:  0.4909162223339081
train gradient:  0.19012287186515084
iteration : 9522
train acc:  0.796875
train loss:  0.42551106214523315
train gradient:  0.12663965031818308
iteration : 9523
train acc:  0.75
train loss:  0.4885673522949219
train gradient:  0.11717077033279912
iteration : 9524
train acc:  0.703125
train loss:  0.48521870374679565
train gradient:  0.11353337253184288
iteration : 9525
train acc:  0.6875
train loss:  0.568540632724762
train gradient:  0.14685415425469078
iteration : 9526
train acc:  0.703125
train loss:  0.562919020652771
train gradient:  0.1677589138932097
iteration : 9527
train acc:  0.6796875
train loss:  0.5685108304023743
train gradient:  0.15898316061440976
iteration : 9528
train acc:  0.7890625
train loss:  0.44755643606185913
train gradient:  0.11083149209471273
iteration : 9529
train acc:  0.7265625
train loss:  0.5200711488723755
train gradient:  0.16072875369013612
iteration : 9530
train acc:  0.8125
train loss:  0.4251924455165863
train gradient:  0.0803410926417798
iteration : 9531
train acc:  0.7421875
train loss:  0.4950519800186157
train gradient:  0.148557149359957
iteration : 9532
train acc:  0.734375
train loss:  0.527094841003418
train gradient:  0.1764032476972869
iteration : 9533
train acc:  0.671875
train loss:  0.5339672565460205
train gradient:  0.13587483376076342
iteration : 9534
train acc:  0.7421875
train loss:  0.511965274810791
train gradient:  0.12922498949242706
iteration : 9535
train acc:  0.703125
train loss:  0.541394829750061
train gradient:  0.21294578771778638
iteration : 9536
train acc:  0.75
train loss:  0.47833725810050964
train gradient:  0.13935069910820674
iteration : 9537
train acc:  0.7578125
train loss:  0.46192067861557007
train gradient:  0.12081062748773096
iteration : 9538
train acc:  0.7109375
train loss:  0.543732762336731
train gradient:  0.15117815256168754
iteration : 9539
train acc:  0.765625
train loss:  0.45827001333236694
train gradient:  0.11570770345704429
iteration : 9540
train acc:  0.765625
train loss:  0.5061689019203186
train gradient:  0.13374409035238644
iteration : 9541
train acc:  0.765625
train loss:  0.47996291518211365
train gradient:  0.19855304966856763
iteration : 9542
train acc:  0.765625
train loss:  0.4642215073108673
train gradient:  0.127586780840641
iteration : 9543
train acc:  0.703125
train loss:  0.5288325548171997
train gradient:  0.1529179031015799
iteration : 9544
train acc:  0.6875
train loss:  0.5458475351333618
train gradient:  0.16619419909823324
iteration : 9545
train acc:  0.7421875
train loss:  0.47931790351867676
train gradient:  0.11789371526148107
iteration : 9546
train acc:  0.7421875
train loss:  0.5480828285217285
train gradient:  0.13414590709096136
iteration : 9547
train acc:  0.765625
train loss:  0.474866658449173
train gradient:  0.12138203236714593
iteration : 9548
train acc:  0.7421875
train loss:  0.5531102418899536
train gradient:  0.16412173897967625
iteration : 9549
train acc:  0.734375
train loss:  0.49229007959365845
train gradient:  0.12194402601131346
iteration : 9550
train acc:  0.7109375
train loss:  0.5160462856292725
train gradient:  0.13673970358609816
iteration : 9551
train acc:  0.734375
train loss:  0.49449580907821655
train gradient:  0.15261492679107466
iteration : 9552
train acc:  0.75
train loss:  0.4910351634025574
train gradient:  0.14642988958224906
iteration : 9553
train acc:  0.7578125
train loss:  0.44144344329833984
train gradient:  0.10065023928515499
iteration : 9554
train acc:  0.7421875
train loss:  0.493561327457428
train gradient:  0.14048311799614666
iteration : 9555
train acc:  0.75
train loss:  0.5469067692756653
train gradient:  0.15767316698402267
iteration : 9556
train acc:  0.7265625
train loss:  0.4609876573085785
train gradient:  0.1106179043779359
iteration : 9557
train acc:  0.65625
train loss:  0.49871692061424255
train gradient:  0.11465538694597933
iteration : 9558
train acc:  0.7578125
train loss:  0.4534764885902405
train gradient:  0.09806913778228321
iteration : 9559
train acc:  0.703125
train loss:  0.5455294251441956
train gradient:  0.12407728356921835
iteration : 9560
train acc:  0.7265625
train loss:  0.5435996055603027
train gradient:  0.1648909801995886
iteration : 9561
train acc:  0.78125
train loss:  0.4414776563644409
train gradient:  0.11902761122863381
iteration : 9562
train acc:  0.7421875
train loss:  0.4691638946533203
train gradient:  0.11523933123351549
iteration : 9563
train acc:  0.7109375
train loss:  0.5139473080635071
train gradient:  0.12825912449410787
iteration : 9564
train acc:  0.7421875
train loss:  0.5120404958724976
train gradient:  0.14202340471102382
iteration : 9565
train acc:  0.6953125
train loss:  0.5948476195335388
train gradient:  0.18466584629421678
iteration : 9566
train acc:  0.75
train loss:  0.5192065238952637
train gradient:  0.13912181696397125
iteration : 9567
train acc:  0.7734375
train loss:  0.4288504123687744
train gradient:  0.13223715346366732
iteration : 9568
train acc:  0.7109375
train loss:  0.5394023656845093
train gradient:  0.12645748999947592
iteration : 9569
train acc:  0.78125
train loss:  0.42697280645370483
train gradient:  0.11331355832006867
iteration : 9570
train acc:  0.8046875
train loss:  0.45232290029525757
train gradient:  0.10609240288762191
iteration : 9571
train acc:  0.75
train loss:  0.5260090827941895
train gradient:  0.13425990409972904
iteration : 9572
train acc:  0.75
train loss:  0.5011453628540039
train gradient:  0.12732046540112218
iteration : 9573
train acc:  0.796875
train loss:  0.43515846133232117
train gradient:  0.14515957998039233
iteration : 9574
train acc:  0.7109375
train loss:  0.5243399739265442
train gradient:  0.1348586264098488
iteration : 9575
train acc:  0.7265625
train loss:  0.49712902307510376
train gradient:  0.14330401090946265
iteration : 9576
train acc:  0.6640625
train loss:  0.585505485534668
train gradient:  0.1683649012236863
iteration : 9577
train acc:  0.6875
train loss:  0.5255756974220276
train gradient:  0.1486427475488466
iteration : 9578
train acc:  0.7109375
train loss:  0.5283032655715942
train gradient:  0.14701565809092998
iteration : 9579
train acc:  0.7421875
train loss:  0.49233078956604004
train gradient:  0.11522551870651916
iteration : 9580
train acc:  0.75
train loss:  0.49211448431015015
train gradient:  0.13552585302187467
iteration : 9581
train acc:  0.6796875
train loss:  0.56053626537323
train gradient:  0.18726433022568933
iteration : 9582
train acc:  0.78125
train loss:  0.4094564616680145
train gradient:  0.0884115335381582
iteration : 9583
train acc:  0.75
train loss:  0.4891619086265564
train gradient:  0.13888042820973853
iteration : 9584
train acc:  0.8046875
train loss:  0.4491855502128601
train gradient:  0.10154314271708226
iteration : 9585
train acc:  0.734375
train loss:  0.4973699748516083
train gradient:  0.14629381802289443
iteration : 9586
train acc:  0.7265625
train loss:  0.5285634994506836
train gradient:  0.14403958718159482
iteration : 9587
train acc:  0.6796875
train loss:  0.5545781850814819
train gradient:  0.16375767913982914
iteration : 9588
train acc:  0.75
train loss:  0.47619494795799255
train gradient:  0.13669031976615212
iteration : 9589
train acc:  0.703125
train loss:  0.528914213180542
train gradient:  0.12884236913427355
iteration : 9590
train acc:  0.640625
train loss:  0.6019160151481628
train gradient:  0.21185813065742687
iteration : 9591
train acc:  0.6875
train loss:  0.46409571170806885
train gradient:  0.1346106340569685
iteration : 9592
train acc:  0.703125
train loss:  0.5201090574264526
train gradient:  0.11805937625289195
iteration : 9593
train acc:  0.640625
train loss:  0.5999036431312561
train gradient:  0.1549243614508821
iteration : 9594
train acc:  0.6875
train loss:  0.5176839828491211
train gradient:  0.1572695171235661
iteration : 9595
train acc:  0.78125
train loss:  0.45902013778686523
train gradient:  0.10672761706808942
iteration : 9596
train acc:  0.765625
train loss:  0.4893837869167328
train gradient:  0.11096789839711614
iteration : 9597
train acc:  0.7890625
train loss:  0.4428936541080475
train gradient:  0.10278739641626213
iteration : 9598
train acc:  0.75
train loss:  0.45607101917266846
train gradient:  0.10429551227563874
iteration : 9599
train acc:  0.6953125
train loss:  0.541553795337677
train gradient:  0.192303265866541
iteration : 9600
train acc:  0.7109375
train loss:  0.5091285109519958
train gradient:  0.11853132411987481
iteration : 9601
train acc:  0.6796875
train loss:  0.5996767282485962
train gradient:  0.17348259317999531
iteration : 9602
train acc:  0.671875
train loss:  0.5994040966033936
train gradient:  0.22966685696628195
iteration : 9603
train acc:  0.734375
train loss:  0.45868539810180664
train gradient:  0.12720227718028915
iteration : 9604
train acc:  0.7421875
train loss:  0.48757022619247437
train gradient:  0.12183983248821828
iteration : 9605
train acc:  0.6953125
train loss:  0.5612422227859497
train gradient:  0.14087773690881056
iteration : 9606
train acc:  0.7109375
train loss:  0.5367031693458557
train gradient:  0.1539972049519776
iteration : 9607
train acc:  0.8046875
train loss:  0.4756917953491211
train gradient:  0.14867384763309957
iteration : 9608
train acc:  0.78125
train loss:  0.4986574053764343
train gradient:  0.12275699081945593
iteration : 9609
train acc:  0.6953125
train loss:  0.5373249053955078
train gradient:  0.17835319056863957
iteration : 9610
train acc:  0.75
train loss:  0.46235838532447815
train gradient:  0.11489879903987889
iteration : 9611
train acc:  0.765625
train loss:  0.5096197128295898
train gradient:  0.19085495087069326
iteration : 9612
train acc:  0.734375
train loss:  0.5385630130767822
train gradient:  0.15116198290349153
iteration : 9613
train acc:  0.75
train loss:  0.45728105306625366
train gradient:  0.09364975324958168
iteration : 9614
train acc:  0.71875
train loss:  0.500793993473053
train gradient:  0.10745417333759413
iteration : 9615
train acc:  0.71875
train loss:  0.4871169328689575
train gradient:  0.11526959835577053
iteration : 9616
train acc:  0.75
train loss:  0.48795443773269653
train gradient:  0.14376286366540447
iteration : 9617
train acc:  0.6953125
train loss:  0.5386112332344055
train gradient:  0.11653108429988812
iteration : 9618
train acc:  0.8125
train loss:  0.45409250259399414
train gradient:  0.09770571280018894
iteration : 9619
train acc:  0.6953125
train loss:  0.5162056684494019
train gradient:  0.11468065766180284
iteration : 9620
train acc:  0.765625
train loss:  0.5000220537185669
train gradient:  0.1378585107894702
iteration : 9621
train acc:  0.7578125
train loss:  0.44776099920272827
train gradient:  0.09972769860618283
iteration : 9622
train acc:  0.7578125
train loss:  0.4395986497402191
train gradient:  0.10795073544474358
iteration : 9623
train acc:  0.65625
train loss:  0.5312483906745911
train gradient:  0.156309111492193
iteration : 9624
train acc:  0.7421875
train loss:  0.4973284900188446
train gradient:  0.10849436142301228
iteration : 9625
train acc:  0.7578125
train loss:  0.49954697489738464
train gradient:  0.11752319598647319
iteration : 9626
train acc:  0.7578125
train loss:  0.46573710441589355
train gradient:  0.11284055811102374
iteration : 9627
train acc:  0.71875
train loss:  0.5480180382728577
train gradient:  0.1278943919672717
iteration : 9628
train acc:  0.7265625
train loss:  0.504022479057312
train gradient:  0.1086048893418351
iteration : 9629
train acc:  0.765625
train loss:  0.4302324950695038
train gradient:  0.09631804543590645
iteration : 9630
train acc:  0.7890625
train loss:  0.46800148487091064
train gradient:  0.1275782583975835
iteration : 9631
train acc:  0.765625
train loss:  0.5055158734321594
train gradient:  0.13007190053687487
iteration : 9632
train acc:  0.75
train loss:  0.46993961930274963
train gradient:  0.1078662540697589
iteration : 9633
train acc:  0.71875
train loss:  0.4934733211994171
train gradient:  0.11522287841357272
iteration : 9634
train acc:  0.7578125
train loss:  0.48946109414100647
train gradient:  0.11524948589261029
iteration : 9635
train acc:  0.75
train loss:  0.47423893213272095
train gradient:  0.12055602070218348
iteration : 9636
train acc:  0.7109375
train loss:  0.4999176561832428
train gradient:  0.12740020809026154
iteration : 9637
train acc:  0.7265625
train loss:  0.478402316570282
train gradient:  0.11743424207039413
iteration : 9638
train acc:  0.734375
train loss:  0.5554220676422119
train gradient:  0.19234172408512054
iteration : 9639
train acc:  0.6875
train loss:  0.54689621925354
train gradient:  0.13087464821295647
iteration : 9640
train acc:  0.765625
train loss:  0.4736945629119873
train gradient:  0.12915736221215335
iteration : 9641
train acc:  0.75
train loss:  0.5203021764755249
train gradient:  0.12874662957746708
iteration : 9642
train acc:  0.6953125
train loss:  0.503040611743927
train gradient:  0.1221535105151669
iteration : 9643
train acc:  0.8125
train loss:  0.4335375726222992
train gradient:  0.08910714951391813
iteration : 9644
train acc:  0.7109375
train loss:  0.5509213209152222
train gradient:  0.1397552624778884
iteration : 9645
train acc:  0.6953125
train loss:  0.5212605595588684
train gradient:  0.15700176786699974
iteration : 9646
train acc:  0.7578125
train loss:  0.4245046377182007
train gradient:  0.11996857841745029
iteration : 9647
train acc:  0.7734375
train loss:  0.42995795607566833
train gradient:  0.10004111361172703
iteration : 9648
train acc:  0.7265625
train loss:  0.5166437029838562
train gradient:  0.1306766210620139
iteration : 9649
train acc:  0.796875
train loss:  0.43373847007751465
train gradient:  0.11204142844716361
iteration : 9650
train acc:  0.78125
train loss:  0.4659944772720337
train gradient:  0.10832104807510064
iteration : 9651
train acc:  0.734375
train loss:  0.4648849666118622
train gradient:  0.1318985858020254
iteration : 9652
train acc:  0.7734375
train loss:  0.4663733243942261
train gradient:  0.13578820683316092
iteration : 9653
train acc:  0.734375
train loss:  0.5295467376708984
train gradient:  0.15783809052685216
iteration : 9654
train acc:  0.7421875
train loss:  0.4910653829574585
train gradient:  0.12443844151854887
iteration : 9655
train acc:  0.7734375
train loss:  0.4162805378437042
train gradient:  0.10718902369715227
iteration : 9656
train acc:  0.765625
train loss:  0.49349814653396606
train gradient:  0.11115263615948463
iteration : 9657
train acc:  0.78125
train loss:  0.479570209980011
train gradient:  0.13598055421159566
iteration : 9658
train acc:  0.7734375
train loss:  0.502880334854126
train gradient:  0.14107148277301979
iteration : 9659
train acc:  0.7265625
train loss:  0.5176249742507935
train gradient:  0.13340254934304321
iteration : 9660
train acc:  0.7578125
train loss:  0.4752742648124695
train gradient:  0.11005069091824418
iteration : 9661
train acc:  0.7578125
train loss:  0.46694520115852356
train gradient:  0.10849508844538416
iteration : 9662
train acc:  0.703125
train loss:  0.5173658132553101
train gradient:  0.11129270656253219
iteration : 9663
train acc:  0.71875
train loss:  0.5638926029205322
train gradient:  0.18361478666537828
iteration : 9664
train acc:  0.71875
train loss:  0.47180116176605225
train gradient:  0.13226921781091439
iteration : 9665
train acc:  0.71875
train loss:  0.48945486545562744
train gradient:  0.13830451093023938
iteration : 9666
train acc:  0.671875
train loss:  0.5499111413955688
train gradient:  0.13991851007153708
iteration : 9667
train acc:  0.7578125
train loss:  0.46463215351104736
train gradient:  0.11555206414049154
iteration : 9668
train acc:  0.78125
train loss:  0.4495392143726349
train gradient:  0.10493546488258707
iteration : 9669
train acc:  0.734375
train loss:  0.4920393228530884
train gradient:  0.15256142373713646
iteration : 9670
train acc:  0.75
train loss:  0.49529725313186646
train gradient:  0.10744961513828889
iteration : 9671
train acc:  0.671875
train loss:  0.5723940134048462
train gradient:  0.17319628397708364
iteration : 9672
train acc:  0.734375
train loss:  0.5072116851806641
train gradient:  0.13229273517933182
iteration : 9673
train acc:  0.8125
train loss:  0.3840941786766052
train gradient:  0.09262623057155167
iteration : 9674
train acc:  0.7265625
train loss:  0.5544236898422241
train gradient:  0.1484257129262963
iteration : 9675
train acc:  0.6796875
train loss:  0.5440143942832947
train gradient:  0.16313796930015537
iteration : 9676
train acc:  0.7578125
train loss:  0.4866398274898529
train gradient:  0.12064429513582893
iteration : 9677
train acc:  0.8046875
train loss:  0.4756884276866913
train gradient:  0.1188565672094192
iteration : 9678
train acc:  0.734375
train loss:  0.5367786884307861
train gradient:  0.1340042461345027
iteration : 9679
train acc:  0.6875
train loss:  0.5491247177124023
train gradient:  0.1675538371255808
iteration : 9680
train acc:  0.7265625
train loss:  0.48313796520233154
train gradient:  0.10195130271136083
iteration : 9681
train acc:  0.640625
train loss:  0.610060453414917
train gradient:  0.20535785306545884
iteration : 9682
train acc:  0.75
train loss:  0.5119234323501587
train gradient:  0.12269769245308096
iteration : 9683
train acc:  0.71875
train loss:  0.4966575503349304
train gradient:  0.159658741619997
iteration : 9684
train acc:  0.71875
train loss:  0.5281890630722046
train gradient:  0.13410635703126456
iteration : 9685
train acc:  0.734375
train loss:  0.48178717494010925
train gradient:  0.13788099872038118
iteration : 9686
train acc:  0.6953125
train loss:  0.5132997035980225
train gradient:  0.13771990696776343
iteration : 9687
train acc:  0.7265625
train loss:  0.544918954372406
train gradient:  0.143367504218734
iteration : 9688
train acc:  0.7265625
train loss:  0.4944595992565155
train gradient:  0.13080735589098003
iteration : 9689
train acc:  0.7265625
train loss:  0.4936050772666931
train gradient:  0.1400435412896179
iteration : 9690
train acc:  0.7421875
train loss:  0.4889792501926422
train gradient:  0.12070986193269206
iteration : 9691
train acc:  0.703125
train loss:  0.5594953894615173
train gradient:  0.1794016591231437
iteration : 9692
train acc:  0.7578125
train loss:  0.4841429889202118
train gradient:  0.14337061702888287
iteration : 9693
train acc:  0.7109375
train loss:  0.48717761039733887
train gradient:  0.15535643779760655
iteration : 9694
train acc:  0.7734375
train loss:  0.46696358919143677
train gradient:  0.11742876554805369
iteration : 9695
train acc:  0.71875
train loss:  0.4983370900154114
train gradient:  0.11671267517874516
iteration : 9696
train acc:  0.7421875
train loss:  0.49966463446617126
train gradient:  0.14955544512216148
iteration : 9697
train acc:  0.71875
train loss:  0.5293760895729065
train gradient:  0.14876879250534136
iteration : 9698
train acc:  0.6328125
train loss:  0.5655826330184937
train gradient:  0.17136885889179382
iteration : 9699
train acc:  0.6953125
train loss:  0.5078597068786621
train gradient:  0.13361742044850572
iteration : 9700
train acc:  0.765625
train loss:  0.4971740245819092
train gradient:  0.14052400837844203
iteration : 9701
train acc:  0.71875
train loss:  0.5294363498687744
train gradient:  0.14462790690978333
iteration : 9702
train acc:  0.796875
train loss:  0.41016536951065063
train gradient:  0.10477375091229509
iteration : 9703
train acc:  0.7109375
train loss:  0.4770582914352417
train gradient:  0.12056003542171219
iteration : 9704
train acc:  0.8046875
train loss:  0.4302816390991211
train gradient:  0.10694930403451068
iteration : 9705
train acc:  0.8046875
train loss:  0.4400363564491272
train gradient:  0.10078859890929333
iteration : 9706
train acc:  0.78125
train loss:  0.4561622142791748
train gradient:  0.10345322984143761
iteration : 9707
train acc:  0.7734375
train loss:  0.4495224356651306
train gradient:  0.10055699139191775
iteration : 9708
train acc:  0.7734375
train loss:  0.5039153099060059
train gradient:  0.17265075649112058
iteration : 9709
train acc:  0.6953125
train loss:  0.5382323861122131
train gradient:  0.22175114537690463
iteration : 9710
train acc:  0.71875
train loss:  0.5135736465454102
train gradient:  0.10904814210281925
iteration : 9711
train acc:  0.78125
train loss:  0.48020559549331665
train gradient:  0.1226423927410948
iteration : 9712
train acc:  0.765625
train loss:  0.49819415807724
train gradient:  0.12244320792356234
iteration : 9713
train acc:  0.734375
train loss:  0.49724268913269043
train gradient:  0.11493967739188353
iteration : 9714
train acc:  0.65625
train loss:  0.5782164335250854
train gradient:  0.1587515162266201
iteration : 9715
train acc:  0.7578125
train loss:  0.4410043954849243
train gradient:  0.09682366404463597
iteration : 9716
train acc:  0.765625
train loss:  0.489182710647583
train gradient:  0.12681083457568426
iteration : 9717
train acc:  0.7734375
train loss:  0.49799931049346924
train gradient:  0.13691404066354468
iteration : 9718
train acc:  0.6953125
train loss:  0.5477604866027832
train gradient:  0.22984070894892042
iteration : 9719
train acc:  0.7265625
train loss:  0.5659438371658325
train gradient:  0.16332586224040613
iteration : 9720
train acc:  0.7109375
train loss:  0.555037260055542
train gradient:  0.15802041766925345
iteration : 9721
train acc:  0.734375
train loss:  0.47030338644981384
train gradient:  0.10983408493527848
iteration : 9722
train acc:  0.734375
train loss:  0.5222566723823547
train gradient:  0.15695225957074174
iteration : 9723
train acc:  0.7578125
train loss:  0.4540935754776001
train gradient:  0.11018837909101
iteration : 9724
train acc:  0.7421875
train loss:  0.5217896699905396
train gradient:  0.15083081443446078
iteration : 9725
train acc:  0.71875
train loss:  0.45970040559768677
train gradient:  0.12275679031951343
iteration : 9726
train acc:  0.75
train loss:  0.46700459718704224
train gradient:  0.08905465565976106
iteration : 9727
train acc:  0.7890625
train loss:  0.46945029497146606
train gradient:  0.15988992377393096
iteration : 9728
train acc:  0.734375
train loss:  0.5408667325973511
train gradient:  0.12904506610092753
iteration : 9729
train acc:  0.703125
train loss:  0.5184530019760132
train gradient:  0.13469343018429358
iteration : 9730
train acc:  0.7421875
train loss:  0.4936707317829132
train gradient:  0.1199639221971604
iteration : 9731
train acc:  0.7421875
train loss:  0.5012364387512207
train gradient:  0.12663088944848008
iteration : 9732
train acc:  0.625
train loss:  0.5603901147842407
train gradient:  0.13716480472822867
iteration : 9733
train acc:  0.78125
train loss:  0.4963362216949463
train gradient:  0.1387957550424321
iteration : 9734
train acc:  0.78125
train loss:  0.44959795475006104
train gradient:  0.09298258495230163
iteration : 9735
train acc:  0.71875
train loss:  0.49470579624176025
train gradient:  0.14928792454218434
iteration : 9736
train acc:  0.6484375
train loss:  0.573192834854126
train gradient:  0.15741005327301172
iteration : 9737
train acc:  0.7265625
train loss:  0.5447161793708801
train gradient:  0.15550547485590643
iteration : 9738
train acc:  0.7109375
train loss:  0.5319302678108215
train gradient:  0.1751170881022733
iteration : 9739
train acc:  0.765625
train loss:  0.4683564603328705
train gradient:  0.11320790726885457
iteration : 9740
train acc:  0.765625
train loss:  0.4791952669620514
train gradient:  0.11755761054356619
iteration : 9741
train acc:  0.734375
train loss:  0.5449291467666626
train gradient:  0.15108807868376573
iteration : 9742
train acc:  0.7421875
train loss:  0.4777979850769043
train gradient:  0.17151030026496755
iteration : 9743
train acc:  0.796875
train loss:  0.46197015047073364
train gradient:  0.13312591839280719
iteration : 9744
train acc:  0.796875
train loss:  0.42059507966041565
train gradient:  0.10489822318650945
iteration : 9745
train acc:  0.7265625
train loss:  0.5075435638427734
train gradient:  0.12865105567982926
iteration : 9746
train acc:  0.765625
train loss:  0.48861634731292725
train gradient:  0.10243485761671055
iteration : 9747
train acc:  0.75
train loss:  0.47505128383636475
train gradient:  0.11974027450009733
iteration : 9748
train acc:  0.8046875
train loss:  0.42532968521118164
train gradient:  0.10891151157086901
iteration : 9749
train acc:  0.7265625
train loss:  0.5123390555381775
train gradient:  0.1496692479775129
iteration : 9750
train acc:  0.7421875
train loss:  0.5171029567718506
train gradient:  0.12592504109685698
iteration : 9751
train acc:  0.75
train loss:  0.511710524559021
train gradient:  0.1395594227619689
iteration : 9752
train acc:  0.71875
train loss:  0.5386568307876587
train gradient:  0.167314503019316
iteration : 9753
train acc:  0.765625
train loss:  0.4633544385433197
train gradient:  0.13425770754365804
iteration : 9754
train acc:  0.703125
train loss:  0.4887414872646332
train gradient:  0.09957892478646409
iteration : 9755
train acc:  0.71875
train loss:  0.5628876090049744
train gradient:  0.12876104122864157
iteration : 9756
train acc:  0.6953125
train loss:  0.5213145017623901
train gradient:  0.12865960617980998
iteration : 9757
train acc:  0.7421875
train loss:  0.555317759513855
train gradient:  0.14559802112700443
iteration : 9758
train acc:  0.734375
train loss:  0.5428047180175781
train gradient:  0.1328767190768051
iteration : 9759
train acc:  0.7734375
train loss:  0.4586399793624878
train gradient:  0.12998866037387505
iteration : 9760
train acc:  0.75
train loss:  0.46043047308921814
train gradient:  0.11328879137756692
iteration : 9761
train acc:  0.734375
train loss:  0.4695645570755005
train gradient:  0.10237572100986754
iteration : 9762
train acc:  0.7421875
train loss:  0.5544143319129944
train gradient:  0.17231479800873709
iteration : 9763
train acc:  0.78125
train loss:  0.4253183603286743
train gradient:  0.1014512899821376
iteration : 9764
train acc:  0.7421875
train loss:  0.5109227299690247
train gradient:  0.11975061627682138
iteration : 9765
train acc:  0.7578125
train loss:  0.48706528544425964
train gradient:  0.15290745325050215
iteration : 9766
train acc:  0.71875
train loss:  0.5101237893104553
train gradient:  0.13918766933930454
iteration : 9767
train acc:  0.6953125
train loss:  0.5186517238616943
train gradient:  0.12745759698125614
iteration : 9768
train acc:  0.734375
train loss:  0.5358665585517883
train gradient:  0.1370318054538064
iteration : 9769
train acc:  0.734375
train loss:  0.5203324556350708
train gradient:  0.15379573447660572
iteration : 9770
train acc:  0.7578125
train loss:  0.4587729573249817
train gradient:  0.10457673604430459
iteration : 9771
train acc:  0.7421875
train loss:  0.537219762802124
train gradient:  0.12457817200434966
iteration : 9772
train acc:  0.7109375
train loss:  0.5744980573654175
train gradient:  0.21627517305301358
iteration : 9773
train acc:  0.703125
train loss:  0.549780011177063
train gradient:  0.16149863098751954
iteration : 9774
train acc:  0.6953125
train loss:  0.5772767663002014
train gradient:  0.1831184739586529
iteration : 9775
train acc:  0.7109375
train loss:  0.5791475772857666
train gradient:  0.15610676759605907
iteration : 9776
train acc:  0.7109375
train loss:  0.5242955684661865
train gradient:  0.11945140889873232
iteration : 9777
train acc:  0.7578125
train loss:  0.505751371383667
train gradient:  0.14319800870200555
iteration : 9778
train acc:  0.7734375
train loss:  0.49820032715797424
train gradient:  0.11377919582847759
iteration : 9779
train acc:  0.6953125
train loss:  0.5525546669960022
train gradient:  0.15414017175976208
iteration : 9780
train acc:  0.7890625
train loss:  0.4691067039966583
train gradient:  0.0941764265344643
iteration : 9781
train acc:  0.7578125
train loss:  0.46337664127349854
train gradient:  0.09579505343935103
iteration : 9782
train acc:  0.828125
train loss:  0.4061621427536011
train gradient:  0.08310743784969178
iteration : 9783
train acc:  0.765625
train loss:  0.4404398798942566
train gradient:  0.1045241361621423
iteration : 9784
train acc:  0.71875
train loss:  0.5158063173294067
train gradient:  0.15518231345899225
iteration : 9785
train acc:  0.7265625
train loss:  0.5028782486915588
train gradient:  0.14774895228868143
iteration : 9786
train acc:  0.7578125
train loss:  0.5300078988075256
train gradient:  0.13838552337032145
iteration : 9787
train acc:  0.7734375
train loss:  0.4843907952308655
train gradient:  0.13008292882178912
iteration : 9788
train acc:  0.7578125
train loss:  0.5003093481063843
train gradient:  0.12686929963432922
iteration : 9789
train acc:  0.71875
train loss:  0.49310824275016785
train gradient:  0.13795637379996414
iteration : 9790
train acc:  0.7265625
train loss:  0.489788293838501
train gradient:  0.10894626399028609
iteration : 9791
train acc:  0.6875
train loss:  0.5343315601348877
train gradient:  0.14570287342724064
iteration : 9792
train acc:  0.6875
train loss:  0.49613717198371887
train gradient:  0.12421536559582715
iteration : 9793
train acc:  0.7890625
train loss:  0.4691934585571289
train gradient:  0.10817549603497255
iteration : 9794
train acc:  0.765625
train loss:  0.46313583850860596
train gradient:  0.1268229208920419
iteration : 9795
train acc:  0.765625
train loss:  0.47296494245529175
train gradient:  0.13584726169779326
iteration : 9796
train acc:  0.703125
train loss:  0.5371328592300415
train gradient:  0.1339796317553279
iteration : 9797
train acc:  0.7265625
train loss:  0.5542786121368408
train gradient:  0.1695435736692244
iteration : 9798
train acc:  0.75
train loss:  0.4678604006767273
train gradient:  0.10427749528418828
iteration : 9799
train acc:  0.78125
train loss:  0.4836563467979431
train gradient:  0.1146648561844541
iteration : 9800
train acc:  0.7109375
train loss:  0.5405532121658325
train gradient:  0.2117206035419954
iteration : 9801
train acc:  0.75
train loss:  0.49420005083084106
train gradient:  0.10652616963527899
iteration : 9802
train acc:  0.765625
train loss:  0.49877503514289856
train gradient:  0.1446633605762055
iteration : 9803
train acc:  0.75
train loss:  0.4649246335029602
train gradient:  0.09951391983233514
iteration : 9804
train acc:  0.75
train loss:  0.4903571605682373
train gradient:  0.12364124975660745
iteration : 9805
train acc:  0.6953125
train loss:  0.4935242533683777
train gradient:  0.1427442389792994
iteration : 9806
train acc:  0.734375
train loss:  0.486145943403244
train gradient:  0.1319474034757526
iteration : 9807
train acc:  0.6875
train loss:  0.5121137499809265
train gradient:  0.1858812727840597
iteration : 9808
train acc:  0.734375
train loss:  0.4619769752025604
train gradient:  0.11382099982957018
iteration : 9809
train acc:  0.796875
train loss:  0.4479929208755493
train gradient:  0.09610262400188761
iteration : 9810
train acc:  0.75
train loss:  0.51792311668396
train gradient:  0.11708867985260409
iteration : 9811
train acc:  0.703125
train loss:  0.5260767340660095
train gradient:  0.19857818759109752
iteration : 9812
train acc:  0.734375
train loss:  0.564409077167511
train gradient:  0.16739180790135472
iteration : 9813
train acc:  0.7890625
train loss:  0.4584693908691406
train gradient:  0.1250885493648037
iteration : 9814
train acc:  0.6953125
train loss:  0.5133748650550842
train gradient:  0.123540751988927
iteration : 9815
train acc:  0.7421875
train loss:  0.5135562419891357
train gradient:  0.13541231117644764
iteration : 9816
train acc:  0.7265625
train loss:  0.5362012386322021
train gradient:  0.18928371367652774
iteration : 9817
train acc:  0.7109375
train loss:  0.5197025537490845
train gradient:  0.16275719412849082
iteration : 9818
train acc:  0.7421875
train loss:  0.4827633500099182
train gradient:  0.10793241902723377
iteration : 9819
train acc:  0.75
train loss:  0.4409148693084717
train gradient:  0.1075086305877297
iteration : 9820
train acc:  0.8046875
train loss:  0.4324415922164917
train gradient:  0.09400461658284275
iteration : 9821
train acc:  0.84375
train loss:  0.4337771534919739
train gradient:  0.0988475843779716
iteration : 9822
train acc:  0.71875
train loss:  0.5248959064483643
train gradient:  0.12372246091325483
iteration : 9823
train acc:  0.734375
train loss:  0.491659551858902
train gradient:  0.16683369331119796
iteration : 9824
train acc:  0.75
train loss:  0.48513156175613403
train gradient:  0.1623735575300123
iteration : 9825
train acc:  0.765625
train loss:  0.48752641677856445
train gradient:  0.13406214708417283
iteration : 9826
train acc:  0.7265625
train loss:  0.5044739246368408
train gradient:  0.11317738734873116
iteration : 9827
train acc:  0.7890625
train loss:  0.46319714188575745
train gradient:  0.10146245139622173
iteration : 9828
train acc:  0.734375
train loss:  0.6455152034759521
train gradient:  0.19944295449116495
iteration : 9829
train acc:  0.7265625
train loss:  0.5588194131851196
train gradient:  0.15950149735446603
iteration : 9830
train acc:  0.765625
train loss:  0.472995400428772
train gradient:  0.09209499002318026
iteration : 9831
train acc:  0.71875
train loss:  0.5028717517852783
train gradient:  0.12734202973882539
iteration : 9832
train acc:  0.7578125
train loss:  0.46559813618659973
train gradient:  0.12364421989989897
iteration : 9833
train acc:  0.71875
train loss:  0.5024770498275757
train gradient:  0.13571782094929746
iteration : 9834
train acc:  0.7578125
train loss:  0.5138842463493347
train gradient:  0.1264300376473083
iteration : 9835
train acc:  0.78125
train loss:  0.4478924870491028
train gradient:  0.12881018334297134
iteration : 9836
train acc:  0.7109375
train loss:  0.5129926800727844
train gradient:  0.1379313028047655
iteration : 9837
train acc:  0.703125
train loss:  0.5564522743225098
train gradient:  0.15033354347567934
iteration : 9838
train acc:  0.7578125
train loss:  0.5081651210784912
train gradient:  0.12595389369142954
iteration : 9839
train acc:  0.7109375
train loss:  0.5525266528129578
train gradient:  0.13003051135542046
iteration : 9840
train acc:  0.765625
train loss:  0.4929562211036682
train gradient:  0.10876993433292687
iteration : 9841
train acc:  0.71875
train loss:  0.5042881965637207
train gradient:  0.12049439617438366
iteration : 9842
train acc:  0.7578125
train loss:  0.4916923940181732
train gradient:  0.11167374807601226
iteration : 9843
train acc:  0.71875
train loss:  0.4927481412887573
train gradient:  0.15270082309266414
iteration : 9844
train acc:  0.7578125
train loss:  0.4399827718734741
train gradient:  0.09741507616901701
iteration : 9845
train acc:  0.7578125
train loss:  0.5222242474555969
train gradient:  0.11796579670242804
iteration : 9846
train acc:  0.7109375
train loss:  0.4637659788131714
train gradient:  0.11394839451005734
iteration : 9847
train acc:  0.734375
train loss:  0.5514779090881348
train gradient:  0.16371747053463415
iteration : 9848
train acc:  0.75
train loss:  0.4642569422721863
train gradient:  0.09611563879532031
iteration : 9849
train acc:  0.7734375
train loss:  0.45100224018096924
train gradient:  0.0971278973473009
iteration : 9850
train acc:  0.7265625
train loss:  0.4601883888244629
train gradient:  0.1130314861322566
iteration : 9851
train acc:  0.6953125
train loss:  0.5355957746505737
train gradient:  0.13427844156778307
iteration : 9852
train acc:  0.71875
train loss:  0.49264365434646606
train gradient:  0.10748936144046518
iteration : 9853
train acc:  0.671875
train loss:  0.5611786842346191
train gradient:  0.17287381335793225
iteration : 9854
train acc:  0.6953125
train loss:  0.5477746725082397
train gradient:  0.16807536668256035
iteration : 9855
train acc:  0.765625
train loss:  0.47304418683052063
train gradient:  0.12496541702291795
iteration : 9856
train acc:  0.7109375
train loss:  0.48164448142051697
train gradient:  0.13834604451021348
iteration : 9857
train acc:  0.7421875
train loss:  0.5401431322097778
train gradient:  0.16200799335426763
iteration : 9858
train acc:  0.703125
train loss:  0.5224764943122864
train gradient:  0.1300552601149556
iteration : 9859
train acc:  0.78125
train loss:  0.43893545866012573
train gradient:  0.09260852183267412
iteration : 9860
train acc:  0.78125
train loss:  0.44061681628227234
train gradient:  0.11109477211055349
iteration : 9861
train acc:  0.7421875
train loss:  0.5225147008895874
train gradient:  0.14054819905868682
iteration : 9862
train acc:  0.71875
train loss:  0.5312671661376953
train gradient:  0.14549957729910773
iteration : 9863
train acc:  0.7109375
train loss:  0.5319530963897705
train gradient:  0.11499533263170929
iteration : 9864
train acc:  0.7578125
train loss:  0.5219395160675049
train gradient:  0.13701990833243338
iteration : 9865
train acc:  0.734375
train loss:  0.5177289247512817
train gradient:  0.15075833303887684
iteration : 9866
train acc:  0.7421875
train loss:  0.4767981767654419
train gradient:  0.13787258851885986
iteration : 9867
train acc:  0.8125
train loss:  0.46943050622940063
train gradient:  0.1266096394435335
iteration : 9868
train acc:  0.7421875
train loss:  0.5202889442443848
train gradient:  0.12635925864978464
iteration : 9869
train acc:  0.75
train loss:  0.5260601043701172
train gradient:  0.1338402115088715
iteration : 9870
train acc:  0.6953125
train loss:  0.596157431602478
train gradient:  0.19783901286350458
iteration : 9871
train acc:  0.765625
train loss:  0.4650600254535675
train gradient:  0.12073392151396917
iteration : 9872
train acc:  0.75
train loss:  0.4862377643585205
train gradient:  0.10139661316240094
iteration : 9873
train acc:  0.734375
train loss:  0.544019341468811
train gradient:  0.17145230169472694
iteration : 9874
train acc:  0.7265625
train loss:  0.506192684173584
train gradient:  0.14170443083799406
iteration : 9875
train acc:  0.7890625
train loss:  0.43561840057373047
train gradient:  0.11606451740717993
iteration : 9876
train acc:  0.7421875
train loss:  0.45809537172317505
train gradient:  0.1555336050750884
iteration : 9877
train acc:  0.8125
train loss:  0.4580822288990021
train gradient:  0.10764868337746672
iteration : 9878
train acc:  0.75
train loss:  0.5060129761695862
train gradient:  0.12392956058962122
iteration : 9879
train acc:  0.75
train loss:  0.5292291045188904
train gradient:  0.13585864662877595
iteration : 9880
train acc:  0.734375
train loss:  0.4718538522720337
train gradient:  0.0952549032931063
iteration : 9881
train acc:  0.765625
train loss:  0.4867268204689026
train gradient:  0.11781563527478131
iteration : 9882
train acc:  0.75
train loss:  0.537588894367218
train gradient:  0.14786199136934447
iteration : 9883
train acc:  0.7734375
train loss:  0.4924510717391968
train gradient:  0.11568541450592312
iteration : 9884
train acc:  0.6875
train loss:  0.49068278074264526
train gradient:  0.1466502474720704
iteration : 9885
train acc:  0.75
train loss:  0.5253498554229736
train gradient:  0.12130362128430747
iteration : 9886
train acc:  0.7109375
train loss:  0.5412359237670898
train gradient:  0.1525892997797948
iteration : 9887
train acc:  0.71875
train loss:  0.5372034311294556
train gradient:  0.13081878261296276
iteration : 9888
train acc:  0.828125
train loss:  0.4487616717815399
train gradient:  0.11822169055605063
iteration : 9889
train acc:  0.71875
train loss:  0.5507272481918335
train gradient:  0.13187922831324345
iteration : 9890
train acc:  0.7421875
train loss:  0.5026414394378662
train gradient:  0.12386472974590641
iteration : 9891
train acc:  0.734375
train loss:  0.5317584276199341
train gradient:  0.1372356847703785
iteration : 9892
train acc:  0.765625
train loss:  0.4405907690525055
train gradient:  0.11296365289585387
iteration : 9893
train acc:  0.75
train loss:  0.49395495653152466
train gradient:  0.131651352151603
iteration : 9894
train acc:  0.703125
train loss:  0.5157493948936462
train gradient:  0.13143459140737446
iteration : 9895
train acc:  0.765625
train loss:  0.45708903670310974
train gradient:  0.10333773104283517
iteration : 9896
train acc:  0.734375
train loss:  0.4715171456336975
train gradient:  0.13072307824317167
iteration : 9897
train acc:  0.7734375
train loss:  0.45006632804870605
train gradient:  0.10582739250795344
iteration : 9898
train acc:  0.6328125
train loss:  0.5750020742416382
train gradient:  0.14972579051883034
iteration : 9899
train acc:  0.7578125
train loss:  0.5210098624229431
train gradient:  0.1643602686884508
iteration : 9900
train acc:  0.7421875
train loss:  0.47442489862442017
train gradient:  0.11549729264472805
iteration : 9901
train acc:  0.84375
train loss:  0.4161067008972168
train gradient:  0.11065808368676155
iteration : 9902
train acc:  0.7734375
train loss:  0.45002448558807373
train gradient:  0.10127724010272844
iteration : 9903
train acc:  0.7265625
train loss:  0.5900651216506958
train gradient:  0.15294279503971037
iteration : 9904
train acc:  0.734375
train loss:  0.49825412034988403
train gradient:  0.13605605566532764
iteration : 9905
train acc:  0.7265625
train loss:  0.5224602818489075
train gradient:  0.20373723990926196
iteration : 9906
train acc:  0.7421875
train loss:  0.48761096596717834
train gradient:  0.13194953720831645
iteration : 9907
train acc:  0.6953125
train loss:  0.4788065254688263
train gradient:  0.14572489287224355
iteration : 9908
train acc:  0.8046875
train loss:  0.4542766809463501
train gradient:  0.12861214951340572
iteration : 9909
train acc:  0.796875
train loss:  0.4327664077281952
train gradient:  0.10366799112792537
iteration : 9910
train acc:  0.7421875
train loss:  0.5072017312049866
train gradient:  0.14527560944443035
iteration : 9911
train acc:  0.6875
train loss:  0.526545524597168
train gradient:  0.15552662371645726
iteration : 9912
train acc:  0.6953125
train loss:  0.5609040260314941
train gradient:  0.1773162740416535
iteration : 9913
train acc:  0.7734375
train loss:  0.4931354224681854
train gradient:  0.17093895913514856
iteration : 9914
train acc:  0.71875
train loss:  0.5377840995788574
train gradient:  0.16322009730680864
iteration : 9915
train acc:  0.7578125
train loss:  0.5063519477844238
train gradient:  0.11545907494021411
iteration : 9916
train acc:  0.6875
train loss:  0.5661734938621521
train gradient:  0.186538999829424
iteration : 9917
train acc:  0.765625
train loss:  0.4990413784980774
train gradient:  0.13042851880195439
iteration : 9918
train acc:  0.71875
train loss:  0.4924970865249634
train gradient:  0.13723312022075457
iteration : 9919
train acc:  0.765625
train loss:  0.46509745717048645
train gradient:  0.13941237225649383
iteration : 9920
train acc:  0.75
train loss:  0.4994809031486511
train gradient:  0.13456732153886458
iteration : 9921
train acc:  0.7578125
train loss:  0.4977240562438965
train gradient:  0.12999166961378172
iteration : 9922
train acc:  0.6953125
train loss:  0.5320615768432617
train gradient:  0.12491917996612657
iteration : 9923
train acc:  0.71875
train loss:  0.5225537419319153
train gradient:  0.13131793993998314
iteration : 9924
train acc:  0.71875
train loss:  0.462659627199173
train gradient:  0.11376771009061001
iteration : 9925
train acc:  0.6953125
train loss:  0.5215024352073669
train gradient:  0.13764310218114328
iteration : 9926
train acc:  0.7734375
train loss:  0.46259552240371704
train gradient:  0.10305285713392225
iteration : 9927
train acc:  0.640625
train loss:  0.5880886912345886
train gradient:  0.17190352487778265
iteration : 9928
train acc:  0.75
train loss:  0.5798119306564331
train gradient:  0.18633478953579902
iteration : 9929
train acc:  0.6875
train loss:  0.5222424268722534
train gradient:  0.16616929591678092
iteration : 9930
train acc:  0.7265625
train loss:  0.5205929279327393
train gradient:  0.14059694592330368
iteration : 9931
train acc:  0.765625
train loss:  0.4459470510482788
train gradient:  0.08258797858168845
iteration : 9932
train acc:  0.734375
train loss:  0.5270512104034424
train gradient:  0.11575159688518634
iteration : 9933
train acc:  0.703125
train loss:  0.5257911682128906
train gradient:  0.12378308689118245
iteration : 9934
train acc:  0.828125
train loss:  0.4319610893726349
train gradient:  0.10205909575987576
iteration : 9935
train acc:  0.765625
train loss:  0.48040422797203064
train gradient:  0.10134260072901831
iteration : 9936
train acc:  0.734375
train loss:  0.493291437625885
train gradient:  0.10771059260375679
iteration : 9937
train acc:  0.8046875
train loss:  0.4585416913032532
train gradient:  0.13014974292762344
iteration : 9938
train acc:  0.796875
train loss:  0.4642575979232788
train gradient:  0.13130923775571535
iteration : 9939
train acc:  0.6875
train loss:  0.5837485790252686
train gradient:  0.16716974709217763
iteration : 9940
train acc:  0.765625
train loss:  0.4459575414657593
train gradient:  0.10283378302948666
iteration : 9941
train acc:  0.71875
train loss:  0.49447381496429443
train gradient:  0.1255216060098226
iteration : 9942
train acc:  0.78125
train loss:  0.5319024920463562
train gradient:  0.1496837908583017
iteration : 9943
train acc:  0.78125
train loss:  0.4790438413619995
train gradient:  0.14426871962758608
iteration : 9944
train acc:  0.7265625
train loss:  0.49871933460235596
train gradient:  0.1367176329250569
iteration : 9945
train acc:  0.71875
train loss:  0.48646080493927
train gradient:  0.10084734636877912
iteration : 9946
train acc:  0.796875
train loss:  0.4442499876022339
train gradient:  0.11809790711237012
iteration : 9947
train acc:  0.7578125
train loss:  0.5237692594528198
train gradient:  0.1496969841873556
iteration : 9948
train acc:  0.71875
train loss:  0.525885820388794
train gradient:  0.13860130741613444
iteration : 9949
train acc:  0.7578125
train loss:  0.47175806760787964
train gradient:  0.13611923670657572
iteration : 9950
train acc:  0.6796875
train loss:  0.5907620787620544
train gradient:  0.17257613335601057
iteration : 9951
train acc:  0.7890625
train loss:  0.44350576400756836
train gradient:  0.12857408942743478
iteration : 9952
train acc:  0.6875
train loss:  0.528325080871582
train gradient:  0.1473116861574955
iteration : 9953
train acc:  0.734375
train loss:  0.4961179792881012
train gradient:  0.12145837310461995
iteration : 9954
train acc:  0.6875
train loss:  0.5858045220375061
train gradient:  0.18577008338816697
iteration : 9955
train acc:  0.828125
train loss:  0.4178861379623413
train gradient:  0.11762423055889425
iteration : 9956
train acc:  0.7109375
train loss:  0.541921854019165
train gradient:  0.15602228651778133
iteration : 9957
train acc:  0.703125
train loss:  0.501739501953125
train gradient:  0.1186965999586143
iteration : 9958
train acc:  0.7421875
train loss:  0.5070600509643555
train gradient:  0.1330282389595293
iteration : 9959
train acc:  0.75
train loss:  0.468688428401947
train gradient:  0.11953402882346655
iteration : 9960
train acc:  0.734375
train loss:  0.4642258584499359
train gradient:  0.12081002151038685
iteration : 9961
train acc:  0.7265625
train loss:  0.4717206656932831
train gradient:  0.1148162248199334
iteration : 9962
train acc:  0.703125
train loss:  0.48529475927352905
train gradient:  0.10506936992871947
iteration : 9963
train acc:  0.71875
train loss:  0.49564477801322937
train gradient:  0.11785529390980626
iteration : 9964
train acc:  0.796875
train loss:  0.4251384437084198
train gradient:  0.07986640173485293
iteration : 9965
train acc:  0.71875
train loss:  0.47466614842414856
train gradient:  0.13169941701183163
iteration : 9966
train acc:  0.7109375
train loss:  0.5186553001403809
train gradient:  0.17232663235714368
iteration : 9967
train acc:  0.7421875
train loss:  0.4438961446285248
train gradient:  0.11234659192556622
iteration : 9968
train acc:  0.7578125
train loss:  0.5149319767951965
train gradient:  0.12355829057624024
iteration : 9969
train acc:  0.7109375
train loss:  0.49525782465934753
train gradient:  0.1330018217842331
iteration : 9970
train acc:  0.8046875
train loss:  0.4733891189098358
train gradient:  0.1153821656057672
iteration : 9971
train acc:  0.6953125
train loss:  0.5386830568313599
train gradient:  0.16338002171332555
iteration : 9972
train acc:  0.7265625
train loss:  0.5795952081680298
train gradient:  0.14635908830687172
iteration : 9973
train acc:  0.71875
train loss:  0.5341648459434509
train gradient:  0.14691952560680643
iteration : 9974
train acc:  0.703125
train loss:  0.528033435344696
train gradient:  0.13580030976901797
iteration : 9975
train acc:  0.671875
train loss:  0.5506840944290161
train gradient:  0.1317521096872406
iteration : 9976
train acc:  0.734375
train loss:  0.45632052421569824
train gradient:  0.10673247582417532
iteration : 9977
train acc:  0.75
train loss:  0.45733708143234253
train gradient:  0.09942004411770773
iteration : 9978
train acc:  0.78125
train loss:  0.47367992997169495
train gradient:  0.11977087116408557
iteration : 9979
train acc:  0.7421875
train loss:  0.5039401650428772
train gradient:  0.1427290122791765
iteration : 9980
train acc:  0.6796875
train loss:  0.4866577982902527
train gradient:  0.13870700153325705
iteration : 9981
train acc:  0.75
train loss:  0.45594459772109985
train gradient:  0.10375948476371843
iteration : 9982
train acc:  0.7890625
train loss:  0.46968674659729004
train gradient:  0.12101148106829744
iteration : 9983
train acc:  0.7421875
train loss:  0.47253865003585815
train gradient:  0.10875297815158372
iteration : 9984
train acc:  0.765625
train loss:  0.4815887212753296
train gradient:  0.1519463550247453
iteration : 9985
train acc:  0.7421875
train loss:  0.5062419176101685
train gradient:  0.14699983643896178
iteration : 9986
train acc:  0.7109375
train loss:  0.5468442440032959
train gradient:  0.14191937396395093
iteration : 9987
train acc:  0.6796875
train loss:  0.6067719459533691
train gradient:  0.18408305029327743
iteration : 9988
train acc:  0.6953125
train loss:  0.5388441681861877
train gradient:  0.1716714681386714
iteration : 9989
train acc:  0.7265625
train loss:  0.45782095193862915
train gradient:  0.09840043319377836
iteration : 9990
train acc:  0.734375
train loss:  0.5312938690185547
train gradient:  0.17208122131170617
iteration : 9991
train acc:  0.6953125
train loss:  0.5423950552940369
train gradient:  0.14073868900716274
iteration : 9992
train acc:  0.71875
train loss:  0.5012895464897156
train gradient:  0.12764342575207233
iteration : 9993
train acc:  0.8046875
train loss:  0.49794670939445496
train gradient:  0.15359899804581847
iteration : 9994
train acc:  0.7578125
train loss:  0.49517321586608887
train gradient:  0.11019043833311677
iteration : 9995
train acc:  0.7265625
train loss:  0.5026394128799438
train gradient:  0.14274750877021275
iteration : 9996
train acc:  0.7578125
train loss:  0.47678789496421814
train gradient:  0.1646345341564041
iteration : 9997
train acc:  0.671875
train loss:  0.5109920501708984
train gradient:  0.13350229040343803
iteration : 9998
train acc:  0.7265625
train loss:  0.5044339895248413
train gradient:  0.15995953155051096
iteration : 9999
train acc:  0.765625
train loss:  0.514012336730957
train gradient:  0.15098193281517802
iteration : 10000
train acc:  0.71875
train loss:  0.5021065473556519
train gradient:  0.14378280387306297
iteration : 10001
train acc:  0.6875
train loss:  0.5279647707939148
train gradient:  0.13382872969298318
iteration : 10002
train acc:  0.75
train loss:  0.5170788764953613
train gradient:  0.18098104363217793
iteration : 10003
train acc:  0.734375
train loss:  0.5027469396591187
train gradient:  0.13197977330027252
iteration : 10004
train acc:  0.75
train loss:  0.5162673592567444
train gradient:  0.13015622113619596
iteration : 10005
train acc:  0.7265625
train loss:  0.5543485879898071
train gradient:  0.14895277219714198
iteration : 10006
train acc:  0.7734375
train loss:  0.5228605270385742
train gradient:  0.11217124160365492
iteration : 10007
train acc:  0.6796875
train loss:  0.5602214336395264
train gradient:  0.15442280582298332
iteration : 10008
train acc:  0.78125
train loss:  0.46561604738235474
train gradient:  0.13133799121994305
iteration : 10009
train acc:  0.734375
train loss:  0.49971216917037964
train gradient:  0.14050133886484079
iteration : 10010
train acc:  0.75
train loss:  0.44762885570526123
train gradient:  0.09694176723399076
iteration : 10011
train acc:  0.71875
train loss:  0.46500760316848755
train gradient:  0.12396896902040294
iteration : 10012
train acc:  0.6796875
train loss:  0.4767327904701233
train gradient:  0.1104985416563163
iteration : 10013
train acc:  0.7578125
train loss:  0.4992218017578125
train gradient:  0.11965509662993867
iteration : 10014
train acc:  0.6328125
train loss:  0.5958153605461121
train gradient:  0.21612250965761165
iteration : 10015
train acc:  0.6796875
train loss:  0.5528322458267212
train gradient:  0.17671058632029218
iteration : 10016
train acc:  0.7109375
train loss:  0.5029650926589966
train gradient:  0.1448234263518567
iteration : 10017
train acc:  0.78125
train loss:  0.4786844551563263
train gradient:  0.09930026045171547
iteration : 10018
train acc:  0.75
train loss:  0.47030431032180786
train gradient:  0.09966322537086912
iteration : 10019
train acc:  0.7265625
train loss:  0.5039644837379456
train gradient:  0.1514398644533923
iteration : 10020
train acc:  0.6796875
train loss:  0.5714046955108643
train gradient:  0.1406588028066795
iteration : 10021
train acc:  0.7265625
train loss:  0.48930323123931885
train gradient:  0.1394151140243416
iteration : 10022
train acc:  0.734375
train loss:  0.524067759513855
train gradient:  0.13065932489174625
iteration : 10023
train acc:  0.734375
train loss:  0.5332953333854675
train gradient:  0.13458706593616956
iteration : 10024
train acc:  0.75
train loss:  0.4901801645755768
train gradient:  0.11108314734402808
iteration : 10025
train acc:  0.765625
train loss:  0.45753049850463867
train gradient:  0.1266928274469468
iteration : 10026
train acc:  0.78125
train loss:  0.4708406627178192
train gradient:  0.11313993342614252
iteration : 10027
train acc:  0.765625
train loss:  0.48284876346588135
train gradient:  0.12644077088289202
iteration : 10028
train acc:  0.7421875
train loss:  0.4673047959804535
train gradient:  0.13567615308190067
iteration : 10029
train acc:  0.6875
train loss:  0.5153582692146301
train gradient:  0.11782778841132442
iteration : 10030
train acc:  0.75
train loss:  0.4748370349407196
train gradient:  0.13550766103565218
iteration : 10031
train acc:  0.75
train loss:  0.46830931305885315
train gradient:  0.14751473371490473
iteration : 10032
train acc:  0.7265625
train loss:  0.5424529314041138
train gradient:  0.1494049514862718
iteration : 10033
train acc:  0.7890625
train loss:  0.4275220036506653
train gradient:  0.1479254679035112
iteration : 10034
train acc:  0.7421875
train loss:  0.4691254794597626
train gradient:  0.1286464778022638
iteration : 10035
train acc:  0.7734375
train loss:  0.521503746509552
train gradient:  0.14091478194075172
iteration : 10036
train acc:  0.71875
train loss:  0.52303147315979
train gradient:  0.12707689407929956
iteration : 10037
train acc:  0.734375
train loss:  0.48004308342933655
train gradient:  0.14826429044242734
iteration : 10038
train acc:  0.6953125
train loss:  0.5919828414916992
train gradient:  0.17849623481603094
iteration : 10039
train acc:  0.796875
train loss:  0.43984419107437134
train gradient:  0.1400268638490257
iteration : 10040
train acc:  0.8125
train loss:  0.40000972151756287
train gradient:  0.11400530962197401
iteration : 10041
train acc:  0.6875
train loss:  0.5321657657623291
train gradient:  0.17607384385857028
iteration : 10042
train acc:  0.765625
train loss:  0.48776713013648987
train gradient:  0.1307304257278524
iteration : 10043
train acc:  0.703125
train loss:  0.5551795363426208
train gradient:  0.16417257680313418
iteration : 10044
train acc:  0.796875
train loss:  0.4530143141746521
train gradient:  0.09131403980747897
iteration : 10045
train acc:  0.75
train loss:  0.49259498715400696
train gradient:  0.12761591780739134
iteration : 10046
train acc:  0.796875
train loss:  0.45052027702331543
train gradient:  0.12400024786441223
iteration : 10047
train acc:  0.78125
train loss:  0.47903913259506226
train gradient:  0.13759792899035656
iteration : 10048
train acc:  0.6875
train loss:  0.5546340346336365
train gradient:  0.15410616292128138
iteration : 10049
train acc:  0.7265625
train loss:  0.47718605399131775
train gradient:  0.13656624539502185
iteration : 10050
train acc:  0.703125
train loss:  0.47510209679603577
train gradient:  0.10471204876174664
iteration : 10051
train acc:  0.7578125
train loss:  0.4624395966529846
train gradient:  0.10454885797789155
iteration : 10052
train acc:  0.75
train loss:  0.4982009530067444
train gradient:  0.14917017790939224
iteration : 10053
train acc:  0.7578125
train loss:  0.47531476616859436
train gradient:  0.11258084170587339
iteration : 10054
train acc:  0.75
train loss:  0.48317962884902954
train gradient:  0.11180362259326918
iteration : 10055
train acc:  0.7578125
train loss:  0.49432849884033203
train gradient:  0.14053046217364565
iteration : 10056
train acc:  0.7890625
train loss:  0.46130305528640747
train gradient:  0.1313988665057992
iteration : 10057
train acc:  0.796875
train loss:  0.42510953545570374
train gradient:  0.09945678159378628
iteration : 10058
train acc:  0.71875
train loss:  0.508208692073822
train gradient:  0.13581274796717352
iteration : 10059
train acc:  0.7734375
train loss:  0.46422046422958374
train gradient:  0.16520670034378376
iteration : 10060
train acc:  0.6875
train loss:  0.49382710456848145
train gradient:  0.13540254611617059
iteration : 10061
train acc:  0.75
train loss:  0.4546239376068115
train gradient:  0.1008004100202743
iteration : 10062
train acc:  0.6875
train loss:  0.5090828537940979
train gradient:  0.14623621432437953
iteration : 10063
train acc:  0.71875
train loss:  0.5291263461112976
train gradient:  0.13050110613122806
iteration : 10064
train acc:  0.765625
train loss:  0.48242050409317017
train gradient:  0.1426004517006838
iteration : 10065
train acc:  0.7734375
train loss:  0.4143913686275482
train gradient:  0.09332948797754012
iteration : 10066
train acc:  0.765625
train loss:  0.4772947430610657
train gradient:  0.12229125323016576
iteration : 10067
train acc:  0.78125
train loss:  0.4634922742843628
train gradient:  0.1260179572239154
iteration : 10068
train acc:  0.71875
train loss:  0.5022486448287964
train gradient:  0.1170043057752379
iteration : 10069
train acc:  0.734375
train loss:  0.5196102857589722
train gradient:  0.1336996012990343
iteration : 10070
train acc:  0.7421875
train loss:  0.45041507482528687
train gradient:  0.11452695273914346
iteration : 10071
train acc:  0.765625
train loss:  0.4951449930667877
train gradient:  0.1393708107476458
iteration : 10072
train acc:  0.8203125
train loss:  0.40677931904792786
train gradient:  0.1067030910532558
iteration : 10073
train acc:  0.703125
train loss:  0.5556063652038574
train gradient:  0.14347465540418153
iteration : 10074
train acc:  0.7265625
train loss:  0.5439085960388184
train gradient:  0.1441489431667765
iteration : 10075
train acc:  0.78125
train loss:  0.4485660791397095
train gradient:  0.11776545009339565
iteration : 10076
train acc:  0.7421875
train loss:  0.5482069253921509
train gradient:  0.150743432537852
iteration : 10077
train acc:  0.7109375
train loss:  0.4900400638580322
train gradient:  0.1353208027782838
iteration : 10078
train acc:  0.78125
train loss:  0.4661290645599365
train gradient:  0.12390568011240691
iteration : 10079
train acc:  0.6796875
train loss:  0.6229157447814941
train gradient:  0.23634036831208388
iteration : 10080
train acc:  0.7734375
train loss:  0.45486217737197876
train gradient:  0.10984871881859831
iteration : 10081
train acc:  0.765625
train loss:  0.533900260925293
train gradient:  0.19792957044968895
iteration : 10082
train acc:  0.734375
train loss:  0.5193560719490051
train gradient:  0.1364478342891834
iteration : 10083
train acc:  0.7578125
train loss:  0.4878999888896942
train gradient:  0.13444529094879337
iteration : 10084
train acc:  0.7265625
train loss:  0.5273017287254333
train gradient:  0.1405436218074841
iteration : 10085
train acc:  0.7578125
train loss:  0.44797444343566895
train gradient:  0.10496622129392003
iteration : 10086
train acc:  0.7890625
train loss:  0.4648008942604065
train gradient:  0.09650890856588244
iteration : 10087
train acc:  0.6640625
train loss:  0.5229175686836243
train gradient:  0.13505511297062642
iteration : 10088
train acc:  0.6796875
train loss:  0.5582484006881714
train gradient:  0.16806731006428016
iteration : 10089
train acc:  0.6875
train loss:  0.5208238959312439
train gradient:  0.14854714912695338
iteration : 10090
train acc:  0.7890625
train loss:  0.5089573860168457
train gradient:  0.15927811733055897
iteration : 10091
train acc:  0.6953125
train loss:  0.5099269151687622
train gradient:  0.13809718790056696
iteration : 10092
train acc:  0.7578125
train loss:  0.5136990547180176
train gradient:  0.12549103620315033
iteration : 10093
train acc:  0.71875
train loss:  0.5292685031890869
train gradient:  0.1902016215322272
iteration : 10094
train acc:  0.671875
train loss:  0.5506462454795837
train gradient:  0.15247700690098487
iteration : 10095
train acc:  0.7421875
train loss:  0.5631923675537109
train gradient:  0.16969570308818782
iteration : 10096
train acc:  0.7265625
train loss:  0.513338565826416
train gradient:  0.12524571306364313
iteration : 10097
train acc:  0.71875
train loss:  0.5470327138900757
train gradient:  0.1724102499435452
iteration : 10098
train acc:  0.75
train loss:  0.4694080352783203
train gradient:  0.09887248653243386
iteration : 10099
train acc:  0.765625
train loss:  0.412355899810791
train gradient:  0.10982096796788159
iteration : 10100
train acc:  0.7421875
train loss:  0.453923761844635
train gradient:  0.10783988156759318
iteration : 10101
train acc:  0.7578125
train loss:  0.46327075362205505
train gradient:  0.12868162140949557
iteration : 10102
train acc:  0.6796875
train loss:  0.50246262550354
train gradient:  0.12443618450023325
iteration : 10103
train acc:  0.75
train loss:  0.49212774634361267
train gradient:  0.1441687403968306
iteration : 10104
train acc:  0.8359375
train loss:  0.4283853769302368
train gradient:  0.1073710141701755
iteration : 10105
train acc:  0.8125
train loss:  0.4441526234149933
train gradient:  0.09645090714188279
iteration : 10106
train acc:  0.6484375
train loss:  0.556918740272522
train gradient:  0.13018030408193748
iteration : 10107
train acc:  0.7265625
train loss:  0.5414953231811523
train gradient:  0.17663963745249994
iteration : 10108
train acc:  0.6875
train loss:  0.5312374830245972
train gradient:  0.13174931526823763
iteration : 10109
train acc:  0.765625
train loss:  0.4568881392478943
train gradient:  0.1213881727947507
iteration : 10110
train acc:  0.65625
train loss:  0.5804784893989563
train gradient:  0.16259455979407036
iteration : 10111
train acc:  0.5546875
train loss:  0.5980609059333801
train gradient:  0.18318123568613728
iteration : 10112
train acc:  0.765625
train loss:  0.48572999238967896
train gradient:  0.13745501472750582
iteration : 10113
train acc:  0.7265625
train loss:  0.47296637296676636
train gradient:  0.12004402961119257
iteration : 10114
train acc:  0.7578125
train loss:  0.5088443756103516
train gradient:  0.12577480506845673
iteration : 10115
train acc:  0.7265625
train loss:  0.5390657782554626
train gradient:  0.13847094991032058
iteration : 10116
train acc:  0.75
train loss:  0.47616004943847656
train gradient:  0.1197151914175424
iteration : 10117
train acc:  0.6875
train loss:  0.5872355699539185
train gradient:  0.21046427156825184
iteration : 10118
train acc:  0.75
train loss:  0.5089408159255981
train gradient:  0.14825711508004724
iteration : 10119
train acc:  0.7734375
train loss:  0.4435768127441406
train gradient:  0.10605301460656445
iteration : 10120
train acc:  0.7421875
train loss:  0.5311003923416138
train gradient:  0.15179939390333536
iteration : 10121
train acc:  0.7578125
train loss:  0.5001392364501953
train gradient:  0.13841306178111612
iteration : 10122
train acc:  0.75
train loss:  0.5137802362442017
train gradient:  0.12425144449063434
iteration : 10123
train acc:  0.75
train loss:  0.47757163643836975
train gradient:  0.13736263256051048
iteration : 10124
train acc:  0.703125
train loss:  0.5325765609741211
train gradient:  0.1299447658365267
iteration : 10125
train acc:  0.7109375
train loss:  0.47487351298332214
train gradient:  0.1422645207789192
iteration : 10126
train acc:  0.78125
train loss:  0.4795882999897003
train gradient:  0.1380736905388344
iteration : 10127
train acc:  0.6875
train loss:  0.47402065992355347
train gradient:  0.09905438360538327
iteration : 10128
train acc:  0.734375
train loss:  0.4650347828865051
train gradient:  0.1127817963526081
iteration : 10129
train acc:  0.71875
train loss:  0.5515897870063782
train gradient:  0.15063017475549162
iteration : 10130
train acc:  0.734375
train loss:  0.5422667264938354
train gradient:  0.1740095489594714
iteration : 10131
train acc:  0.734375
train loss:  0.4852050244808197
train gradient:  0.10842995581515204
iteration : 10132
train acc:  0.8046875
train loss:  0.4603371024131775
train gradient:  0.09960885222287123
iteration : 10133
train acc:  0.75
train loss:  0.4853011965751648
train gradient:  0.1315787788353039
iteration : 10134
train acc:  0.75
train loss:  0.44332456588745117
train gradient:  0.11275843663355199
iteration : 10135
train acc:  0.796875
train loss:  0.4451114237308502
train gradient:  0.11917433789951554
iteration : 10136
train acc:  0.71875
train loss:  0.5154892206192017
train gradient:  0.12223976685571615
iteration : 10137
train acc:  0.7578125
train loss:  0.4540693759918213
train gradient:  0.1026248292097286
iteration : 10138
train acc:  0.7734375
train loss:  0.424063503742218
train gradient:  0.08536040593555039
iteration : 10139
train acc:  0.671875
train loss:  0.5209618806838989
train gradient:  0.12602492452895034
iteration : 10140
train acc:  0.6875
train loss:  0.45757120847702026
train gradient:  0.10999933964511541
iteration : 10141
train acc:  0.7578125
train loss:  0.4942304491996765
train gradient:  0.1375677501867298
iteration : 10142
train acc:  0.75
train loss:  0.4877622425556183
train gradient:  0.12243491456617596
iteration : 10143
train acc:  0.734375
train loss:  0.5362491607666016
train gradient:  0.1393725238123129
iteration : 10144
train acc:  0.6953125
train loss:  0.49556249380111694
train gradient:  0.11905826999216843
iteration : 10145
train acc:  0.734375
train loss:  0.45792096853256226
train gradient:  0.1149459321960134
iteration : 10146
train acc:  0.7578125
train loss:  0.4600709080696106
train gradient:  0.10853133213487134
iteration : 10147
train acc:  0.734375
train loss:  0.4927838444709778
train gradient:  0.12818748924892012
iteration : 10148
train acc:  0.7265625
train loss:  0.46725690364837646
train gradient:  0.1324442382809494
iteration : 10149
train acc:  0.78125
train loss:  0.44775810837745667
train gradient:  0.11241150933673602
iteration : 10150
train acc:  0.7734375
train loss:  0.45247727632522583
train gradient:  0.1164924899391137
iteration : 10151
train acc:  0.71875
train loss:  0.5053320527076721
train gradient:  0.15147909474236831
iteration : 10152
train acc:  0.71875
train loss:  0.5335909128189087
train gradient:  0.13635755351267634
iteration : 10153
train acc:  0.7109375
train loss:  0.5367839932441711
train gradient:  0.1897408211155967
iteration : 10154
train acc:  0.65625
train loss:  0.5461835861206055
train gradient:  0.14927508684896088
iteration : 10155
train acc:  0.71875
train loss:  0.5221225619316101
train gradient:  0.15125461526278308
iteration : 10156
train acc:  0.7265625
train loss:  0.5165872573852539
train gradient:  0.14727460344516802
iteration : 10157
train acc:  0.7421875
train loss:  0.5797652006149292
train gradient:  0.1547294761600394
iteration : 10158
train acc:  0.6796875
train loss:  0.5873143076896667
train gradient:  0.1856900684947566
iteration : 10159
train acc:  0.7578125
train loss:  0.45437049865722656
train gradient:  0.10770299286500915
iteration : 10160
train acc:  0.75
train loss:  0.4941778779029846
train gradient:  0.11482209433096927
iteration : 10161
train acc:  0.7578125
train loss:  0.5309338569641113
train gradient:  0.17641193988370776
iteration : 10162
train acc:  0.7265625
train loss:  0.4962499439716339
train gradient:  0.12944667518138817
iteration : 10163
train acc:  0.7890625
train loss:  0.433988094329834
train gradient:  0.08676508243839472
iteration : 10164
train acc:  0.7265625
train loss:  0.4874684512615204
train gradient:  0.1196291802383707
iteration : 10165
train acc:  0.765625
train loss:  0.4760865569114685
train gradient:  0.12863725494215256
iteration : 10166
train acc:  0.7421875
train loss:  0.4849177896976471
train gradient:  0.1168343238096349
iteration : 10167
train acc:  0.75
train loss:  0.48078152537345886
train gradient:  0.11519676528164813
iteration : 10168
train acc:  0.7421875
train loss:  0.4524242579936981
train gradient:  0.11560374664564542
iteration : 10169
train acc:  0.75
train loss:  0.4911996126174927
train gradient:  0.1141228455415592
iteration : 10170
train acc:  0.703125
train loss:  0.5257748365402222
train gradient:  0.13512672554046612
iteration : 10171
train acc:  0.7265625
train loss:  0.570868194103241
train gradient:  0.15483034931374817
iteration : 10172
train acc:  0.7421875
train loss:  0.5076875686645508
train gradient:  0.13997732920159595
iteration : 10173
train acc:  0.6328125
train loss:  0.585016131401062
train gradient:  0.1549166520031955
iteration : 10174
train acc:  0.7578125
train loss:  0.4401964545249939
train gradient:  0.09136005733575044
iteration : 10175
train acc:  0.71875
train loss:  0.5766552686691284
train gradient:  0.14393426422822464
iteration : 10176
train acc:  0.7421875
train loss:  0.4851263165473938
train gradient:  0.11038632604380072
iteration : 10177
train acc:  0.7265625
train loss:  0.5418606996536255
train gradient:  0.16011680766596853
iteration : 10178
train acc:  0.75
train loss:  0.4323272109031677
train gradient:  0.09016933943704515
iteration : 10179
train acc:  0.6875
train loss:  0.6146892309188843
train gradient:  0.22670806371792493
iteration : 10180
train acc:  0.8125
train loss:  0.44847923517227173
train gradient:  0.12930122029048738
iteration : 10181
train acc:  0.7265625
train loss:  0.49067583680152893
train gradient:  0.11972879454674222
iteration : 10182
train acc:  0.734375
train loss:  0.5081883668899536
train gradient:  0.10867459606254559
iteration : 10183
train acc:  0.7578125
train loss:  0.47286325693130493
train gradient:  0.11993289004983357
iteration : 10184
train acc:  0.8125
train loss:  0.42085355520248413
train gradient:  0.11878731415903779
iteration : 10185
train acc:  0.6953125
train loss:  0.5683712959289551
train gradient:  0.1368474899637906
iteration : 10186
train acc:  0.78125
train loss:  0.4783191382884979
train gradient:  0.10778575795073458
iteration : 10187
train acc:  0.7109375
train loss:  0.5403605699539185
train gradient:  0.1384740707061802
iteration : 10188
train acc:  0.7734375
train loss:  0.4237769842147827
train gradient:  0.12684339550839313
iteration : 10189
train acc:  0.7109375
train loss:  0.4951770305633545
train gradient:  0.12499724392920587
iteration : 10190
train acc:  0.78125
train loss:  0.4972236454486847
train gradient:  0.13910724036374522
iteration : 10191
train acc:  0.703125
train loss:  0.47556567192077637
train gradient:  0.11232960488826174
iteration : 10192
train acc:  0.6953125
train loss:  0.5209058523178101
train gradient:  0.11244924888189765
iteration : 10193
train acc:  0.7265625
train loss:  0.5179121494293213
train gradient:  0.15305238949712188
iteration : 10194
train acc:  0.7421875
train loss:  0.4444847106933594
train gradient:  0.10106270240412818
iteration : 10195
train acc:  0.7578125
train loss:  0.4629867374897003
train gradient:  0.12859816228295065
iteration : 10196
train acc:  0.75
train loss:  0.44716930389404297
train gradient:  0.10858825497800327
iteration : 10197
train acc:  0.7890625
train loss:  0.4665393829345703
train gradient:  0.1189054583520646
iteration : 10198
train acc:  0.765625
train loss:  0.5105082988739014
train gradient:  0.1251724009298503
iteration : 10199
train acc:  0.7265625
train loss:  0.5109670758247375
train gradient:  0.13680615647850525
iteration : 10200
train acc:  0.71875
train loss:  0.5061363577842712
train gradient:  0.156133809111905
iteration : 10201
train acc:  0.7109375
train loss:  0.5019554495811462
train gradient:  0.11706852253563786
iteration : 10202
train acc:  0.7734375
train loss:  0.48816072940826416
train gradient:  0.12040016377724004
iteration : 10203
train acc:  0.7578125
train loss:  0.4675840139389038
train gradient:  0.11253053042702625
iteration : 10204
train acc:  0.703125
train loss:  0.4843701124191284
train gradient:  0.10423525376470028
iteration : 10205
train acc:  0.7734375
train loss:  0.463881254196167
train gradient:  0.11392858609099668
iteration : 10206
train acc:  0.78125
train loss:  0.4573516547679901
train gradient:  0.12551588286372528
iteration : 10207
train acc:  0.7578125
train loss:  0.5072359442710876
train gradient:  0.13255233851149748
iteration : 10208
train acc:  0.8203125
train loss:  0.4171648323535919
train gradient:  0.10114133997916966
iteration : 10209
train acc:  0.78125
train loss:  0.43400508165359497
train gradient:  0.12129265675935232
iteration : 10210
train acc:  0.765625
train loss:  0.5387051105499268
train gradient:  0.1376312973542646
iteration : 10211
train acc:  0.6640625
train loss:  0.5321688055992126
train gradient:  0.13388397313327738
iteration : 10212
train acc:  0.7421875
train loss:  0.5200662016868591
train gradient:  0.16541183237805757
iteration : 10213
train acc:  0.7421875
train loss:  0.5153672695159912
train gradient:  0.11508150052479928
iteration : 10214
train acc:  0.65625
train loss:  0.5284570455551147
train gradient:  0.11271425226030561
iteration : 10215
train acc:  0.75
train loss:  0.5673381686210632
train gradient:  0.13023325917886217
iteration : 10216
train acc:  0.7890625
train loss:  0.4196184277534485
train gradient:  0.08967304947576184
iteration : 10217
train acc:  0.6953125
train loss:  0.5252219438552856
train gradient:  0.13477794520647152
iteration : 10218
train acc:  0.7265625
train loss:  0.5545697212219238
train gradient:  0.14503506435557034
iteration : 10219
train acc:  0.6484375
train loss:  0.579475462436676
train gradient:  0.1715571864363511
iteration : 10220
train acc:  0.71875
train loss:  0.4912963807582855
train gradient:  0.14376519024540668
iteration : 10221
train acc:  0.71875
train loss:  0.47374427318573
train gradient:  0.12973468656129672
iteration : 10222
train acc:  0.7578125
train loss:  0.4370685815811157
train gradient:  0.11565759462677705
iteration : 10223
train acc:  0.7578125
train loss:  0.4704466462135315
train gradient:  0.13177306365241037
iteration : 10224
train acc:  0.6953125
train loss:  0.530689001083374
train gradient:  0.12549886834927912
iteration : 10225
train acc:  0.7109375
train loss:  0.49389076232910156
train gradient:  0.13365204541187176
iteration : 10226
train acc:  0.7578125
train loss:  0.47127780318260193
train gradient:  0.10932924703203764
iteration : 10227
train acc:  0.65625
train loss:  0.5876351594924927
train gradient:  0.20432972144090414
iteration : 10228
train acc:  0.75
train loss:  0.5077666640281677
train gradient:  0.10882637569344353
iteration : 10229
train acc:  0.7265625
train loss:  0.5090057253837585
train gradient:  0.1088369746370992
iteration : 10230
train acc:  0.71875
train loss:  0.5341513156890869
train gradient:  0.1544304168777879
iteration : 10231
train acc:  0.8671875
train loss:  0.37930184602737427
train gradient:  0.10673630182043034
iteration : 10232
train acc:  0.6484375
train loss:  0.5675122737884521
train gradient:  0.1690673443798092
iteration : 10233
train acc:  0.6953125
train loss:  0.533098042011261
train gradient:  0.13976558642536743
iteration : 10234
train acc:  0.7734375
train loss:  0.4872562885284424
train gradient:  0.16208733785402274
iteration : 10235
train acc:  0.796875
train loss:  0.5069686770439148
train gradient:  0.1138748150441678
iteration : 10236
train acc:  0.734375
train loss:  0.46534520387649536
train gradient:  0.11853701401248246
iteration : 10237
train acc:  0.6953125
train loss:  0.5208412408828735
train gradient:  0.1258776718045324
iteration : 10238
train acc:  0.71875
train loss:  0.5528883934020996
train gradient:  0.12847496257007965
iteration : 10239
train acc:  0.765625
train loss:  0.4356873333454132
train gradient:  0.10077670345577361
iteration : 10240
train acc:  0.8046875
train loss:  0.44647711515426636
train gradient:  0.09930858926101743
iteration : 10241
train acc:  0.7109375
train loss:  0.5045273900032043
train gradient:  0.1619850825208321
iteration : 10242
train acc:  0.6953125
train loss:  0.5179325938224792
train gradient:  0.15608082672175144
iteration : 10243
train acc:  0.7109375
train loss:  0.5653771758079529
train gradient:  0.11658490221145969
iteration : 10244
train acc:  0.6484375
train loss:  0.5415430068969727
train gradient:  0.14794302348446875
iteration : 10245
train acc:  0.6953125
train loss:  0.4969436228275299
train gradient:  0.1572887421307178
iteration : 10246
train acc:  0.765625
train loss:  0.4793417453765869
train gradient:  0.11103318718044101
iteration : 10247
train acc:  0.78125
train loss:  0.4840846061706543
train gradient:  0.1553723530246604
iteration : 10248
train acc:  0.75
train loss:  0.5208287239074707
train gradient:  0.14760489770500998
iteration : 10249
train acc:  0.7890625
train loss:  0.44997256994247437
train gradient:  0.10401559714605477
iteration : 10250
train acc:  0.8046875
train loss:  0.44376999139785767
train gradient:  0.08003885147463813
iteration : 10251
train acc:  0.75
train loss:  0.4984455108642578
train gradient:  0.10665579702458808
iteration : 10252
train acc:  0.7890625
train loss:  0.46079763770103455
train gradient:  0.11126075391537943
iteration : 10253
train acc:  0.734375
train loss:  0.4821181893348694
train gradient:  0.145376023246039
iteration : 10254
train acc:  0.71875
train loss:  0.5342789888381958
train gradient:  0.1812286899135358
iteration : 10255
train acc:  0.6796875
train loss:  0.5681283473968506
train gradient:  0.15614927221416144
iteration : 10256
train acc:  0.765625
train loss:  0.46663716435432434
train gradient:  0.1039820266551724
iteration : 10257
train acc:  0.703125
train loss:  0.47012555599212646
train gradient:  0.12750734335265834
iteration : 10258
train acc:  0.703125
train loss:  0.5330511331558228
train gradient:  0.12509837720366737
iteration : 10259
train acc:  0.7265625
train loss:  0.548364520072937
train gradient:  0.1606252788259961
iteration : 10260
train acc:  0.765625
train loss:  0.4465521574020386
train gradient:  0.1018504183693838
iteration : 10261
train acc:  0.703125
train loss:  0.6302880644798279
train gradient:  0.19710415461123898
iteration : 10262
train acc:  0.765625
train loss:  0.5249546766281128
train gradient:  0.14658057592555163
iteration : 10263
train acc:  0.703125
train loss:  0.5598244667053223
train gradient:  0.14058293202051686
iteration : 10264
train acc:  0.8046875
train loss:  0.3820722699165344
train gradient:  0.07883944545486242
iteration : 10265
train acc:  0.75
train loss:  0.5106086730957031
train gradient:  0.1172161970640697
iteration : 10266
train acc:  0.7109375
train loss:  0.5104706883430481
train gradient:  0.10928425405765224
iteration : 10267
train acc:  0.734375
train loss:  0.4640057682991028
train gradient:  0.1614593754145594
iteration : 10268
train acc:  0.796875
train loss:  0.40210938453674316
train gradient:  0.09391893685390355
iteration : 10269
train acc:  0.703125
train loss:  0.4866202473640442
train gradient:  0.10814914227531718
iteration : 10270
train acc:  0.734375
train loss:  0.48920053243637085
train gradient:  0.10697190410859202
iteration : 10271
train acc:  0.7421875
train loss:  0.49737030267715454
train gradient:  0.15556437785150645
iteration : 10272
train acc:  0.7421875
train loss:  0.44763219356536865
train gradient:  0.10079983713021778
iteration : 10273
train acc:  0.796875
train loss:  0.46607527136802673
train gradient:  0.10303248876999331
iteration : 10274
train acc:  0.734375
train loss:  0.500186026096344
train gradient:  0.13267779193740298
iteration : 10275
train acc:  0.8046875
train loss:  0.4202513098716736
train gradient:  0.10334551866872285
iteration : 10276
train acc:  0.7578125
train loss:  0.4405694901943207
train gradient:  0.11621994070241479
iteration : 10277
train acc:  0.6640625
train loss:  0.5557669401168823
train gradient:  0.17343312757169665
iteration : 10278
train acc:  0.734375
train loss:  0.5621192455291748
train gradient:  0.15557606982164313
iteration : 10279
train acc:  0.7109375
train loss:  0.5293914079666138
train gradient:  0.14448424453023112
iteration : 10280
train acc:  0.78125
train loss:  0.4842124283313751
train gradient:  0.1137053768042843
iteration : 10281
train acc:  0.78125
train loss:  0.42105281352996826
train gradient:  0.09939074279613033
iteration : 10282
train acc:  0.6796875
train loss:  0.5320741534233093
train gradient:  0.1293423591206509
iteration : 10283
train acc:  0.8125
train loss:  0.43544334173202515
train gradient:  0.07936186343523861
iteration : 10284
train acc:  0.7421875
train loss:  0.5117660164833069
train gradient:  0.13666009877487928
iteration : 10285
train acc:  0.8125
train loss:  0.43696901202201843
train gradient:  0.09935974816650657
iteration : 10286
train acc:  0.7421875
train loss:  0.47487136721611023
train gradient:  0.0937295855411386
iteration : 10287
train acc:  0.7421875
train loss:  0.4907847046852112
train gradient:  0.13210034997563785
iteration : 10288
train acc:  0.71875
train loss:  0.4871925413608551
train gradient:  0.15178591869245844
iteration : 10289
train acc:  0.734375
train loss:  0.507853090763092
train gradient:  0.1814782734264821
iteration : 10290
train acc:  0.703125
train loss:  0.5580011010169983
train gradient:  0.14518354067699438
iteration : 10291
train acc:  0.765625
train loss:  0.4933909475803375
train gradient:  0.1304866853404053
iteration : 10292
train acc:  0.7265625
train loss:  0.5225656628608704
train gradient:  0.12225961464245731
iteration : 10293
train acc:  0.765625
train loss:  0.49690499901771545
train gradient:  0.15330590391237942
iteration : 10294
train acc:  0.734375
train loss:  0.5218309164047241
train gradient:  0.12634263663234885
iteration : 10295
train acc:  0.8515625
train loss:  0.39909645915031433
train gradient:  0.0908232370760552
iteration : 10296
train acc:  0.7421875
train loss:  0.46194469928741455
train gradient:  0.12452339891597491
iteration : 10297
train acc:  0.703125
train loss:  0.5317566394805908
train gradient:  0.16645067186432289
iteration : 10298
train acc:  0.765625
train loss:  0.4601905047893524
train gradient:  0.10329021110106282
iteration : 10299
train acc:  0.6953125
train loss:  0.5342949032783508
train gradient:  0.12603077893358772
iteration : 10300
train acc:  0.75
train loss:  0.47426867485046387
train gradient:  0.10644302174216547
iteration : 10301
train acc:  0.75
train loss:  0.49349245429039
train gradient:  0.11180776872283134
iteration : 10302
train acc:  0.75
train loss:  0.5115069150924683
train gradient:  0.1424783847195369
iteration : 10303
train acc:  0.7109375
train loss:  0.5156850814819336
train gradient:  0.12737109304905891
iteration : 10304
train acc:  0.7578125
train loss:  0.4539666175842285
train gradient:  0.10571082202699913
iteration : 10305
train acc:  0.7109375
train loss:  0.5090036392211914
train gradient:  0.12613630936107045
iteration : 10306
train acc:  0.734375
train loss:  0.4978458285331726
train gradient:  0.1362491912909628
iteration : 10307
train acc:  0.78125
train loss:  0.4646037817001343
train gradient:  0.10351952253280915
iteration : 10308
train acc:  0.7109375
train loss:  0.4804180860519409
train gradient:  0.1307956635068169
iteration : 10309
train acc:  0.765625
train loss:  0.48398035764694214
train gradient:  0.10888295162123461
iteration : 10310
train acc:  0.703125
train loss:  0.5010942220687866
train gradient:  0.13368395021325363
iteration : 10311
train acc:  0.734375
train loss:  0.505683183670044
train gradient:  0.14704357205707752
iteration : 10312
train acc:  0.8125
train loss:  0.43412071466445923
train gradient:  0.09208784225122188
iteration : 10313
train acc:  0.7265625
train loss:  0.517054557800293
train gradient:  0.15485680910592453
iteration : 10314
train acc:  0.7421875
train loss:  0.46590209007263184
train gradient:  0.12739697220157586
iteration : 10315
train acc:  0.7578125
train loss:  0.493914395570755
train gradient:  0.13751168271327768
iteration : 10316
train acc:  0.7421875
train loss:  0.45134711265563965
train gradient:  0.09748375652933301
iteration : 10317
train acc:  0.7578125
train loss:  0.5273088216781616
train gradient:  0.13886746002185058
iteration : 10318
train acc:  0.71875
train loss:  0.5133906602859497
train gradient:  0.1419418325606056
iteration : 10319
train acc:  0.625
train loss:  0.5861738324165344
train gradient:  0.17156327593551207
iteration : 10320
train acc:  0.7890625
train loss:  0.4434066414833069
train gradient:  0.09727971350468934
iteration : 10321
train acc:  0.7109375
train loss:  0.5712581872940063
train gradient:  0.15219329213613014
iteration : 10322
train acc:  0.78125
train loss:  0.43942734599113464
train gradient:  0.09292534978294664
iteration : 10323
train acc:  0.7109375
train loss:  0.5388659238815308
train gradient:  0.14346409524408166
iteration : 10324
train acc:  0.703125
train loss:  0.5959834456443787
train gradient:  0.1549127382224405
iteration : 10325
train acc:  0.6953125
train loss:  0.5113586783409119
train gradient:  0.13225122672956469
iteration : 10326
train acc:  0.75
train loss:  0.49999114871025085
train gradient:  0.1456027964006259
iteration : 10327
train acc:  0.765625
train loss:  0.42966023087501526
train gradient:  0.09043941109095775
iteration : 10328
train acc:  0.703125
train loss:  0.5366851091384888
train gradient:  0.10964450106781436
iteration : 10329
train acc:  0.7578125
train loss:  0.5086944103240967
train gradient:  0.11948003258154927
iteration : 10330
train acc:  0.6875
train loss:  0.49705666303634644
train gradient:  0.10522218600722114
iteration : 10331
train acc:  0.625
train loss:  0.6506218314170837
train gradient:  0.19032690670349334
iteration : 10332
train acc:  0.7265625
train loss:  0.48393183946609497
train gradient:  0.14756009603934137
iteration : 10333
train acc:  0.734375
train loss:  0.4561541676521301
train gradient:  0.10789354051981012
iteration : 10334
train acc:  0.6484375
train loss:  0.5534112453460693
train gradient:  0.15325471360715476
iteration : 10335
train acc:  0.7578125
train loss:  0.48075413703918457
train gradient:  0.11428124779804484
iteration : 10336
train acc:  0.7421875
train loss:  0.5257776379585266
train gradient:  0.14915117580902235
iteration : 10337
train acc:  0.7578125
train loss:  0.47871243953704834
train gradient:  0.11244911365940215
iteration : 10338
train acc:  0.7890625
train loss:  0.45162588357925415
train gradient:  0.1268177277850443
iteration : 10339
train acc:  0.7109375
train loss:  0.5178085565567017
train gradient:  0.12112521473179685
iteration : 10340
train acc:  0.65625
train loss:  0.536876916885376
train gradient:  0.137929304922302
iteration : 10341
train acc:  0.765625
train loss:  0.4681803584098816
train gradient:  0.12925935007217915
iteration : 10342
train acc:  0.8359375
train loss:  0.40000098943710327
train gradient:  0.07667494048247339
iteration : 10343
train acc:  0.7109375
train loss:  0.521579384803772
train gradient:  0.1524021390881582
iteration : 10344
train acc:  0.7578125
train loss:  0.44469520449638367
train gradient:  0.12122574378101372
iteration : 10345
train acc:  0.71875
train loss:  0.4870697259902954
train gradient:  0.11536351823439522
iteration : 10346
train acc:  0.8671875
train loss:  0.3488616645336151
train gradient:  0.08915877748937917
iteration : 10347
train acc:  0.6484375
train loss:  0.6133034229278564
train gradient:  0.17038258899031128
iteration : 10348
train acc:  0.7734375
train loss:  0.4775719940662384
train gradient:  0.123589532827141
iteration : 10349
train acc:  0.765625
train loss:  0.4764324128627777
train gradient:  0.10795496521833604
iteration : 10350
train acc:  0.6953125
train loss:  0.5479170083999634
train gradient:  0.12113755389179319
iteration : 10351
train acc:  0.8203125
train loss:  0.41521650552749634
train gradient:  0.07836068154621591
iteration : 10352
train acc:  0.7890625
train loss:  0.42823660373687744
train gradient:  0.09584771058136744
iteration : 10353
train acc:  0.6953125
train loss:  0.4731869697570801
train gradient:  0.12254187104518303
iteration : 10354
train acc:  0.828125
train loss:  0.3788985013961792
train gradient:  0.09747820572873594
iteration : 10355
train acc:  0.75
train loss:  0.46970564126968384
train gradient:  0.1319868137050703
iteration : 10356
train acc:  0.6875
train loss:  0.5471318960189819
train gradient:  0.14072763807363492
iteration : 10357
train acc:  0.7578125
train loss:  0.49352630972862244
train gradient:  0.1356346137343965
iteration : 10358
train acc:  0.71875
train loss:  0.4903479814529419
train gradient:  0.11855502886597628
iteration : 10359
train acc:  0.671875
train loss:  0.545685887336731
train gradient:  0.14605531415785
iteration : 10360
train acc:  0.7109375
train loss:  0.52032470703125
train gradient:  0.15265526054669068
iteration : 10361
train acc:  0.6953125
train loss:  0.490144282579422
train gradient:  0.10127280517334995
iteration : 10362
train acc:  0.765625
train loss:  0.49990808963775635
train gradient:  0.13554932654303414
iteration : 10363
train acc:  0.7109375
train loss:  0.5572537183761597
train gradient:  0.19491596362686675
iteration : 10364
train acc:  0.71875
train loss:  0.5223203301429749
train gradient:  0.12979432527575116
iteration : 10365
train acc:  0.7421875
train loss:  0.48178666830062866
train gradient:  0.13474538841390146
iteration : 10366
train acc:  0.7265625
train loss:  0.495850533246994
train gradient:  0.10838842152508264
iteration : 10367
train acc:  0.7421875
train loss:  0.4862774610519409
train gradient:  0.10247854451022102
iteration : 10368
train acc:  0.7578125
train loss:  0.47976842522621155
train gradient:  0.12342232119591488
iteration : 10369
train acc:  0.7265625
train loss:  0.5794672966003418
train gradient:  0.21633106882507228
iteration : 10370
train acc:  0.8046875
train loss:  0.4425315260887146
train gradient:  0.11224453292512197
iteration : 10371
train acc:  0.703125
train loss:  0.571252167224884
train gradient:  0.17276920487360048
iteration : 10372
train acc:  0.7890625
train loss:  0.45986127853393555
train gradient:  0.11890420029572052
iteration : 10373
train acc:  0.765625
train loss:  0.4368373155593872
train gradient:  0.13052121217574408
iteration : 10374
train acc:  0.75
train loss:  0.49877116084098816
train gradient:  0.10724274991563268
iteration : 10375
train acc:  0.78125
train loss:  0.4888961911201477
train gradient:  0.11892512280963317
iteration : 10376
train acc:  0.75
train loss:  0.4649915099143982
train gradient:  0.1010902993604732
iteration : 10377
train acc:  0.7734375
train loss:  0.5046403408050537
train gradient:  0.14068426231159725
iteration : 10378
train acc:  0.6875
train loss:  0.5609586238861084
train gradient:  0.14319128036864226
iteration : 10379
train acc:  0.7578125
train loss:  0.45879465341567993
train gradient:  0.09814863205094537
iteration : 10380
train acc:  0.71875
train loss:  0.5451965928077698
train gradient:  0.1415726145819165
iteration : 10381
train acc:  0.7578125
train loss:  0.47557008266448975
train gradient:  0.1082623286214373
iteration : 10382
train acc:  0.75
train loss:  0.4871138334274292
train gradient:  0.1108689946049255
iteration : 10383
train acc:  0.78125
train loss:  0.4789280891418457
train gradient:  0.11952416867013316
iteration : 10384
train acc:  0.75
train loss:  0.461026668548584
train gradient:  0.12385429565191372
iteration : 10385
train acc:  0.7109375
train loss:  0.5113100409507751
train gradient:  0.14083522053299824
iteration : 10386
train acc:  0.796875
train loss:  0.5036115646362305
train gradient:  0.16159131654345027
iteration : 10387
train acc:  0.7421875
train loss:  0.5000268220901489
train gradient:  0.1356560620880255
iteration : 10388
train acc:  0.765625
train loss:  0.45806995034217834
train gradient:  0.10224952218308583
iteration : 10389
train acc:  0.75
train loss:  0.5370128750801086
train gradient:  0.15695348059743708
iteration : 10390
train acc:  0.7421875
train loss:  0.524666428565979
train gradient:  0.15884743264689166
iteration : 10391
train acc:  0.75
train loss:  0.4474307894706726
train gradient:  0.10767733812618015
iteration : 10392
train acc:  0.7578125
train loss:  0.5129344463348389
train gradient:  0.15654233981988064
iteration : 10393
train acc:  0.7578125
train loss:  0.4746245741844177
train gradient:  0.13813256186434553
iteration : 10394
train acc:  0.71875
train loss:  0.4623986482620239
train gradient:  0.10841926272603354
iteration : 10395
train acc:  0.71875
train loss:  0.518930196762085
train gradient:  0.17634784008171012
iteration : 10396
train acc:  0.6953125
train loss:  0.5518338084220886
train gradient:  0.14954688838887953
iteration : 10397
train acc:  0.7109375
train loss:  0.5394452810287476
train gradient:  0.15112665085381607
iteration : 10398
train acc:  0.7734375
train loss:  0.48632705211639404
train gradient:  0.1181571253338715
iteration : 10399
train acc:  0.7109375
train loss:  0.5409789085388184
train gradient:  0.13797086159501423
iteration : 10400
train acc:  0.703125
train loss:  0.5137768983840942
train gradient:  0.1370872899710699
iteration : 10401
train acc:  0.75
train loss:  0.4823437035083771
train gradient:  0.11318606842110585
iteration : 10402
train acc:  0.7109375
train loss:  0.5453499555587769
train gradient:  0.13934758730115399
iteration : 10403
train acc:  0.7578125
train loss:  0.49020835757255554
train gradient:  0.13471045628694484
iteration : 10404
train acc:  0.7265625
train loss:  0.49599671363830566
train gradient:  0.1561216202797624
iteration : 10405
train acc:  0.75
train loss:  0.48549145460128784
train gradient:  0.12307915367598972
iteration : 10406
train acc:  0.734375
train loss:  0.5241116285324097
train gradient:  0.14236071414224033
iteration : 10407
train acc:  0.8203125
train loss:  0.45947742462158203
train gradient:  0.11200178013513849
iteration : 10408
train acc:  0.703125
train loss:  0.5127002000808716
train gradient:  0.1443532540768166
iteration : 10409
train acc:  0.7421875
train loss:  0.4929753541946411
train gradient:  0.11779922111998008
iteration : 10410
train acc:  0.734375
train loss:  0.5072855949401855
train gradient:  0.12231053566946286
iteration : 10411
train acc:  0.7578125
train loss:  0.45001840591430664
train gradient:  0.10025923249586374
iteration : 10412
train acc:  0.7109375
train loss:  0.5006394982337952
train gradient:  0.11510715639753538
iteration : 10413
train acc:  0.7421875
train loss:  0.4980167746543884
train gradient:  0.16428527952438632
iteration : 10414
train acc:  0.78125
train loss:  0.4819870591163635
train gradient:  0.12684475642881093
iteration : 10415
train acc:  0.7734375
train loss:  0.4328018128871918
train gradient:  0.08222204153044041
iteration : 10416
train acc:  0.71875
train loss:  0.5008518695831299
train gradient:  0.13515039768702963
iteration : 10417
train acc:  0.703125
train loss:  0.47805309295654297
train gradient:  0.12797027764884578
iteration : 10418
train acc:  0.7109375
train loss:  0.527743935585022
train gradient:  0.12949812070091446
iteration : 10419
train acc:  0.71875
train loss:  0.4985954165458679
train gradient:  0.1346446235455489
iteration : 10420
train acc:  0.6796875
train loss:  0.5343254208564758
train gradient:  0.14168065493317702
iteration : 10421
train acc:  0.7578125
train loss:  0.48247039318084717
train gradient:  0.10784074453287817
iteration : 10422
train acc:  0.796875
train loss:  0.5109630227088928
train gradient:  0.1375135656562797
iteration : 10423
train acc:  0.7734375
train loss:  0.4350082278251648
train gradient:  0.10916031961609154
iteration : 10424
train acc:  0.7421875
train loss:  0.4237858057022095
train gradient:  0.07809759929239628
iteration : 10425
train acc:  0.7578125
train loss:  0.49112915992736816
train gradient:  0.1207066997872307
iteration : 10426
train acc:  0.75
train loss:  0.4595111608505249
train gradient:  0.10452985677673397
iteration : 10427
train acc:  0.7734375
train loss:  0.45065319538116455
train gradient:  0.10770085819040703
iteration : 10428
train acc:  0.7421875
train loss:  0.47841909527778625
train gradient:  0.12198142364680062
iteration : 10429
train acc:  0.78125
train loss:  0.4411686658859253
train gradient:  0.11433434811135308
iteration : 10430
train acc:  0.7265625
train loss:  0.5148557424545288
train gradient:  0.14045452440628825
iteration : 10431
train acc:  0.7421875
train loss:  0.5073354244232178
train gradient:  0.13299093845210902
iteration : 10432
train acc:  0.703125
train loss:  0.5344481468200684
train gradient:  0.15301721754421396
iteration : 10433
train acc:  0.7734375
train loss:  0.47015273571014404
train gradient:  0.15350394942037965
iteration : 10434
train acc:  0.78125
train loss:  0.45208269357681274
train gradient:  0.10105388446369104
iteration : 10435
train acc:  0.6953125
train loss:  0.48040592670440674
train gradient:  0.11314162053580133
iteration : 10436
train acc:  0.734375
train loss:  0.47515854239463806
train gradient:  0.12958619709678615
iteration : 10437
train acc:  0.7265625
train loss:  0.4860283136367798
train gradient:  0.1128276230389279
iteration : 10438
train acc:  0.75
train loss:  0.459602415561676
train gradient:  0.1172051624991731
iteration : 10439
train acc:  0.6953125
train loss:  0.5836482048034668
train gradient:  0.16738295000894954
iteration : 10440
train acc:  0.8203125
train loss:  0.40342360734939575
train gradient:  0.1104948222469102
iteration : 10441
train acc:  0.734375
train loss:  0.515379786491394
train gradient:  0.16245828210884689
iteration : 10442
train acc:  0.7109375
train loss:  0.5117013454437256
train gradient:  0.13245403945054526
iteration : 10443
train acc:  0.8046875
train loss:  0.4531905949115753
train gradient:  0.11084982149720075
iteration : 10444
train acc:  0.71875
train loss:  0.4916381239891052
train gradient:  0.11076640095825922
iteration : 10445
train acc:  0.8515625
train loss:  0.44352513551712036
train gradient:  0.1297575872989611
iteration : 10446
train acc:  0.8125
train loss:  0.49389684200286865
train gradient:  0.18309552324325318
iteration : 10447
train acc:  0.7890625
train loss:  0.43546125292778015
train gradient:  0.10128846664446815
iteration : 10448
train acc:  0.7265625
train loss:  0.4968901574611664
train gradient:  0.10342783965302395
iteration : 10449
train acc:  0.75
train loss:  0.44722551107406616
train gradient:  0.09929354011037947
iteration : 10450
train acc:  0.75
train loss:  0.4834746718406677
train gradient:  0.18203000962567606
iteration : 10451
train acc:  0.734375
train loss:  0.4783872067928314
train gradient:  0.15536642021974922
iteration : 10452
train acc:  0.765625
train loss:  0.4937756061553955
train gradient:  0.11336831457803194
iteration : 10453
train acc:  0.75
train loss:  0.4983161687850952
train gradient:  0.1350573378759366
iteration : 10454
train acc:  0.75
train loss:  0.4870477020740509
train gradient:  0.12954669799203616
iteration : 10455
train acc:  0.7421875
train loss:  0.5125129222869873
train gradient:  0.14565694385693956
iteration : 10456
train acc:  0.7265625
train loss:  0.5151423811912537
train gradient:  0.13431955858145378
iteration : 10457
train acc:  0.828125
train loss:  0.41307541728019714
train gradient:  0.09579176278717735
iteration : 10458
train acc:  0.734375
train loss:  0.565629243850708
train gradient:  0.15392374561840638
iteration : 10459
train acc:  0.734375
train loss:  0.48821312189102173
train gradient:  0.14754677791788876
iteration : 10460
train acc:  0.7421875
train loss:  0.5424291491508484
train gradient:  0.1784353956957422
iteration : 10461
train acc:  0.7734375
train loss:  0.46235203742980957
train gradient:  0.11758194565740804
iteration : 10462
train acc:  0.7265625
train loss:  0.5165956020355225
train gradient:  0.14067357203496406
iteration : 10463
train acc:  0.7734375
train loss:  0.5080695152282715
train gradient:  0.12345169092202292
iteration : 10464
train acc:  0.703125
train loss:  0.5066499710083008
train gradient:  0.13960894072038157
iteration : 10465
train acc:  0.7734375
train loss:  0.5190749168395996
train gradient:  0.11182566804061929
iteration : 10466
train acc:  0.7890625
train loss:  0.4463590383529663
train gradient:  0.11170209453168237
iteration : 10467
train acc:  0.7578125
train loss:  0.4509231150150299
train gradient:  0.1259235714255394
iteration : 10468
train acc:  0.7890625
train loss:  0.42659246921539307
train gradient:  0.10411214083693637
iteration : 10469
train acc:  0.78125
train loss:  0.4449957609176636
train gradient:  0.10738930546524675
iteration : 10470
train acc:  0.7109375
train loss:  0.5282174348831177
train gradient:  0.1522891838440822
iteration : 10471
train acc:  0.75
train loss:  0.4475332498550415
train gradient:  0.10997700618260219
iteration : 10472
train acc:  0.734375
train loss:  0.4428015351295471
train gradient:  0.09259142652948489
iteration : 10473
train acc:  0.75
train loss:  0.5529902577400208
train gradient:  0.14602450021766816
iteration : 10474
train acc:  0.734375
train loss:  0.4928460717201233
train gradient:  0.13585031501222736
iteration : 10475
train acc:  0.75
train loss:  0.5059553384780884
train gradient:  0.14689108103801793
iteration : 10476
train acc:  0.765625
train loss:  0.43844377994537354
train gradient:  0.10269373827237788
iteration : 10477
train acc:  0.7734375
train loss:  0.46187227964401245
train gradient:  0.11279346380824425
iteration : 10478
train acc:  0.7578125
train loss:  0.48180750012397766
train gradient:  0.14698989210568164
iteration : 10479
train acc:  0.7578125
train loss:  0.42970597743988037
train gradient:  0.10465886734934526
iteration : 10480
train acc:  0.796875
train loss:  0.42069339752197266
train gradient:  0.12304386742509414
iteration : 10481
train acc:  0.7265625
train loss:  0.5207475423812866
train gradient:  0.14250472648932905
iteration : 10482
train acc:  0.71875
train loss:  0.5273707509040833
train gradient:  0.1437071775717544
iteration : 10483
train acc:  0.7578125
train loss:  0.5129482746124268
train gradient:  0.1100771898082519
iteration : 10484
train acc:  0.7265625
train loss:  0.4603845477104187
train gradient:  0.09989588269688039
iteration : 10485
train acc:  0.7109375
train loss:  0.4976705014705658
train gradient:  0.1318136206644956
iteration : 10486
train acc:  0.78125
train loss:  0.41800040006637573
train gradient:  0.10506340133248947
iteration : 10487
train acc:  0.7578125
train loss:  0.4777179956436157
train gradient:  0.13184581822179697
iteration : 10488
train acc:  0.75
train loss:  0.45702964067459106
train gradient:  0.09364845108926323
iteration : 10489
train acc:  0.7734375
train loss:  0.4497506022453308
train gradient:  0.11622168990001977
iteration : 10490
train acc:  0.7421875
train loss:  0.5272625088691711
train gradient:  0.15069541042826612
iteration : 10491
train acc:  0.734375
train loss:  0.46926477551460266
train gradient:  0.11748237084983502
iteration : 10492
train acc:  0.6953125
train loss:  0.5323607325553894
train gradient:  0.1738119465152707
iteration : 10493
train acc:  0.7890625
train loss:  0.4522439241409302
train gradient:  0.10510389187163607
iteration : 10494
train acc:  0.8046875
train loss:  0.4272637963294983
train gradient:  0.09890218149793845
iteration : 10495
train acc:  0.765625
train loss:  0.4755963683128357
train gradient:  0.11873720908999426
iteration : 10496
train acc:  0.8046875
train loss:  0.4527708888053894
train gradient:  0.12102570295219925
iteration : 10497
train acc:  0.8515625
train loss:  0.3859129846096039
train gradient:  0.09615706341442976
iteration : 10498
train acc:  0.7265625
train loss:  0.5496368408203125
train gradient:  0.16993683600399517
iteration : 10499
train acc:  0.7734375
train loss:  0.4795221984386444
train gradient:  0.10934539818054574
iteration : 10500
train acc:  0.75
train loss:  0.49350622296333313
train gradient:  0.14728368705134792
iteration : 10501
train acc:  0.7265625
train loss:  0.5458828210830688
train gradient:  0.17403246260869604
iteration : 10502
train acc:  0.7421875
train loss:  0.4896794259548187
train gradient:  0.11432767370126479
iteration : 10503
train acc:  0.7734375
train loss:  0.43009278178215027
train gradient:  0.0897603477907363
iteration : 10504
train acc:  0.7578125
train loss:  0.45595234632492065
train gradient:  0.15371210959346987
iteration : 10505
train acc:  0.7265625
train loss:  0.521297812461853
train gradient:  0.13857535523719364
iteration : 10506
train acc:  0.796875
train loss:  0.44165560603141785
train gradient:  0.1121368583472666
iteration : 10507
train acc:  0.71875
train loss:  0.5100778341293335
train gradient:  0.13825896860046988
iteration : 10508
train acc:  0.7890625
train loss:  0.46453380584716797
train gradient:  0.17550966670237872
iteration : 10509
train acc:  0.7421875
train loss:  0.4945758581161499
train gradient:  0.16328595147450173
iteration : 10510
train acc:  0.78125
train loss:  0.5031430721282959
train gradient:  0.14217305421304277
iteration : 10511
train acc:  0.734375
train loss:  0.49629801511764526
train gradient:  0.1241896108299055
iteration : 10512
train acc:  0.7421875
train loss:  0.5270836353302002
train gradient:  0.13732488680424856
iteration : 10513
train acc:  0.7578125
train loss:  0.46009892225265503
train gradient:  0.13870861454754824
iteration : 10514
train acc:  0.7421875
train loss:  0.49701061844825745
train gradient:  0.12790291723792563
iteration : 10515
train acc:  0.78125
train loss:  0.46342822909355164
train gradient:  0.11356522607333068
iteration : 10516
train acc:  0.71875
train loss:  0.4867972135543823
train gradient:  0.11355214223575644
iteration : 10517
train acc:  0.7109375
train loss:  0.5136061310768127
train gradient:  0.1540742242153547
iteration : 10518
train acc:  0.703125
train loss:  0.5057703852653503
train gradient:  0.13656861276451387
iteration : 10519
train acc:  0.75
train loss:  0.47729218006134033
train gradient:  0.13147376757725487
iteration : 10520
train acc:  0.71875
train loss:  0.5818533301353455
train gradient:  0.1930145135065638
iteration : 10521
train acc:  0.7734375
train loss:  0.43395140767097473
train gradient:  0.09203736387049719
iteration : 10522
train acc:  0.6328125
train loss:  0.6345421671867371
train gradient:  0.1746654491777916
iteration : 10523
train acc:  0.7734375
train loss:  0.4905635118484497
train gradient:  0.128651347760212
iteration : 10524
train acc:  0.734375
train loss:  0.5135164260864258
train gradient:  0.16379632188700383
iteration : 10525
train acc:  0.78125
train loss:  0.44711872935295105
train gradient:  0.1085836078227291
iteration : 10526
train acc:  0.71875
train loss:  0.5254383087158203
train gradient:  0.13352278739220136
iteration : 10527
train acc:  0.7734375
train loss:  0.46151065826416016
train gradient:  0.11120996362225907
iteration : 10528
train acc:  0.7109375
train loss:  0.5036126375198364
train gradient:  0.12682760788070557
iteration : 10529
train acc:  0.734375
train loss:  0.4957617223262787
train gradient:  0.1193688124295384
iteration : 10530
train acc:  0.734375
train loss:  0.4835394322872162
train gradient:  0.141740031242305
iteration : 10531
train acc:  0.75
train loss:  0.5270571708679199
train gradient:  0.1757493860284634
iteration : 10532
train acc:  0.6953125
train loss:  0.5463169813156128
train gradient:  0.13826995115688373
iteration : 10533
train acc:  0.734375
train loss:  0.5205862522125244
train gradient:  0.1522238544007451
iteration : 10534
train acc:  0.703125
train loss:  0.545427680015564
train gradient:  0.24529760191993533
iteration : 10535
train acc:  0.7734375
train loss:  0.489224910736084
train gradient:  0.1254482075484351
iteration : 10536
train acc:  0.7421875
train loss:  0.5259386301040649
train gradient:  0.15409458765683665
iteration : 10537
train acc:  0.7578125
train loss:  0.47474557161331177
train gradient:  0.14010738993688113
iteration : 10538
train acc:  0.6796875
train loss:  0.5244856476783752
train gradient:  0.181363329573342
iteration : 10539
train acc:  0.796875
train loss:  0.43202364444732666
train gradient:  0.10321453139669087
iteration : 10540
train acc:  0.7421875
train loss:  0.4543987512588501
train gradient:  0.14849885059387458
iteration : 10541
train acc:  0.75
train loss:  0.48844414949417114
train gradient:  0.11258387035126607
iteration : 10542
train acc:  0.78125
train loss:  0.5026873350143433
train gradient:  0.128997223374194
iteration : 10543
train acc:  0.6875
train loss:  0.5207016468048096
train gradient:  0.16891123815090986
iteration : 10544
train acc:  0.7109375
train loss:  0.4851508140563965
train gradient:  0.16412124514344928
iteration : 10545
train acc:  0.8359375
train loss:  0.4140720069408417
train gradient:  0.14050034783409485
iteration : 10546
train acc:  0.75
train loss:  0.4984411895275116
train gradient:  0.13931026149434528
iteration : 10547
train acc:  0.75
train loss:  0.54777991771698
train gradient:  0.16416413916883343
iteration : 10548
train acc:  0.796875
train loss:  0.44164979457855225
train gradient:  0.09759599782928773
iteration : 10549
train acc:  0.765625
train loss:  0.45900508761405945
train gradient:  0.11078083438646476
iteration : 10550
train acc:  0.734375
train loss:  0.4842488467693329
train gradient:  0.1173620748659366
iteration : 10551
train acc:  0.671875
train loss:  0.49798405170440674
train gradient:  0.14225454627957895
iteration : 10552
train acc:  0.7578125
train loss:  0.4974741041660309
train gradient:  0.12377556131447165
iteration : 10553
train acc:  0.7265625
train loss:  0.5336343050003052
train gradient:  0.14310137404704376
iteration : 10554
train acc:  0.734375
train loss:  0.5124746561050415
train gradient:  0.08966865050294896
iteration : 10555
train acc:  0.75
train loss:  0.46964752674102783
train gradient:  0.12294538906097859
iteration : 10556
train acc:  0.7890625
train loss:  0.42121315002441406
train gradient:  0.1081037567395738
iteration : 10557
train acc:  0.75
train loss:  0.47416722774505615
train gradient:  0.13684418202737342
iteration : 10558
train acc:  0.7890625
train loss:  0.4261753559112549
train gradient:  0.08886423918352561
iteration : 10559
train acc:  0.75
train loss:  0.4719971716403961
train gradient:  0.10735689541874756
iteration : 10560
train acc:  0.7109375
train loss:  0.5182632207870483
train gradient:  0.1864815887017157
iteration : 10561
train acc:  0.7578125
train loss:  0.48249727487564087
train gradient:  0.12265564349205663
iteration : 10562
train acc:  0.8125
train loss:  0.4593493342399597
train gradient:  0.1011715792212379
iteration : 10563
train acc:  0.703125
train loss:  0.541592001914978
train gradient:  0.19288964572312162
iteration : 10564
train acc:  0.75
train loss:  0.5588939189910889
train gradient:  0.1557296454670843
iteration : 10565
train acc:  0.734375
train loss:  0.5149608850479126
train gradient:  0.1336890251297882
iteration : 10566
train acc:  0.765625
train loss:  0.47707074880599976
train gradient:  0.11125552772111388
iteration : 10567
train acc:  0.734375
train loss:  0.5121932625770569
train gradient:  0.13311438741855702
iteration : 10568
train acc:  0.78125
train loss:  0.4250374138355255
train gradient:  0.11077728991849076
iteration : 10569
train acc:  0.75
train loss:  0.5284557938575745
train gradient:  0.14325545858330851
iteration : 10570
train acc:  0.75
train loss:  0.48236677050590515
train gradient:  0.12291602192970641
iteration : 10571
train acc:  0.8046875
train loss:  0.43095871806144714
train gradient:  0.12006846175292815
iteration : 10572
train acc:  0.7578125
train loss:  0.5061072111129761
train gradient:  0.10869099860643232
iteration : 10573
train acc:  0.71875
train loss:  0.4873576760292053
train gradient:  0.13326871402629512
iteration : 10574
train acc:  0.7578125
train loss:  0.457207053899765
train gradient:  0.1150747575647856
iteration : 10575
train acc:  0.7421875
train loss:  0.5133446455001831
train gradient:  0.15308093753985988
iteration : 10576
train acc:  0.7578125
train loss:  0.4853980541229248
train gradient:  0.14680793040194334
iteration : 10577
train acc:  0.78125
train loss:  0.47164565324783325
train gradient:  0.13122426020994865
iteration : 10578
train acc:  0.7421875
train loss:  0.47229984402656555
train gradient:  0.12279122215526148
iteration : 10579
train acc:  0.78125
train loss:  0.46790969371795654
train gradient:  0.13416460829470964
iteration : 10580
train acc:  0.7421875
train loss:  0.5085031390190125
train gradient:  0.14357943297179193
iteration : 10581
train acc:  0.7578125
train loss:  0.4547697901725769
train gradient:  0.1035247915913328
iteration : 10582
train acc:  0.734375
train loss:  0.49653470516204834
train gradient:  0.13832446351075778
iteration : 10583
train acc:  0.8125
train loss:  0.40506696701049805
train gradient:  0.0885300159407269
iteration : 10584
train acc:  0.7734375
train loss:  0.4567774534225464
train gradient:  0.11402510088332171
iteration : 10585
train acc:  0.6953125
train loss:  0.5458351969718933
train gradient:  0.20997380096438967
iteration : 10586
train acc:  0.671875
train loss:  0.5454012751579285
train gradient:  0.14509951833172902
iteration : 10587
train acc:  0.7578125
train loss:  0.486184298992157
train gradient:  0.1432609850025156
iteration : 10588
train acc:  0.7578125
train loss:  0.4970686137676239
train gradient:  0.14168146064220394
iteration : 10589
train acc:  0.7421875
train loss:  0.47251808643341064
train gradient:  0.09644688905242879
iteration : 10590
train acc:  0.71875
train loss:  0.4871407151222229
train gradient:  0.14213956202664296
iteration : 10591
train acc:  0.8046875
train loss:  0.4344415068626404
train gradient:  0.12834277411215433
iteration : 10592
train acc:  0.75
train loss:  0.4820512533187866
train gradient:  0.12636684530936815
iteration : 10593
train acc:  0.6875
train loss:  0.5627955198287964
train gradient:  0.15076931311627964
iteration : 10594
train acc:  0.734375
train loss:  0.473675400018692
train gradient:  0.11776870164903304
iteration : 10595
train acc:  0.765625
train loss:  0.4891544580459595
train gradient:  0.14663631557480927
iteration : 10596
train acc:  0.734375
train loss:  0.5372709631919861
train gradient:  0.15625171982182456
iteration : 10597
train acc:  0.71875
train loss:  0.48875001072883606
train gradient:  0.12368213630520777
iteration : 10598
train acc:  0.7421875
train loss:  0.4970991611480713
train gradient:  0.1258236436702586
iteration : 10599
train acc:  0.703125
train loss:  0.5366179943084717
train gradient:  0.15153391960753992
iteration : 10600
train acc:  0.765625
train loss:  0.4971303641796112
train gradient:  0.10758745736622646
iteration : 10601
train acc:  0.6953125
train loss:  0.5954204797744751
train gradient:  0.1932704570158163
iteration : 10602
train acc:  0.734375
train loss:  0.5372720956802368
train gradient:  0.13713485143499513
iteration : 10603
train acc:  0.6796875
train loss:  0.548387885093689
train gradient:  0.15327061518827467
iteration : 10604
train acc:  0.71875
train loss:  0.49300339818000793
train gradient:  0.1581332885697343
iteration : 10605
train acc:  0.75
train loss:  0.47458451986312866
train gradient:  0.119243319357174
iteration : 10606
train acc:  0.71875
train loss:  0.5011792778968811
train gradient:  0.1466041987352921
iteration : 10607
train acc:  0.8125
train loss:  0.4648594856262207
train gradient:  0.13070932262061724
iteration : 10608
train acc:  0.65625
train loss:  0.5341088771820068
train gradient:  0.1538238891472024
iteration : 10609
train acc:  0.7421875
train loss:  0.4648796021938324
train gradient:  0.1202967397835707
iteration : 10610
train acc:  0.7109375
train loss:  0.5364416241645813
train gradient:  0.13229886268640972
iteration : 10611
train acc:  0.703125
train loss:  0.5542194843292236
train gradient:  0.1733578791964614
iteration : 10612
train acc:  0.8046875
train loss:  0.49136626720428467
train gradient:  0.14437506115508508
iteration : 10613
train acc:  0.7265625
train loss:  0.5045599937438965
train gradient:  0.1416878363910983
iteration : 10614
train acc:  0.7265625
train loss:  0.5153974890708923
train gradient:  0.14247825541497255
iteration : 10615
train acc:  0.8046875
train loss:  0.4526204466819763
train gradient:  0.10810232902780048
iteration : 10616
train acc:  0.6796875
train loss:  0.5409747958183289
train gradient:  0.17058855831330977
iteration : 10617
train acc:  0.734375
train loss:  0.5215160846710205
train gradient:  0.15734164973057435
iteration : 10618
train acc:  0.7890625
train loss:  0.45971500873565674
train gradient:  0.1088985369446934
iteration : 10619
train acc:  0.7109375
train loss:  0.5122344493865967
train gradient:  0.13356057117294573
iteration : 10620
train acc:  0.78125
train loss:  0.43759703636169434
train gradient:  0.08894256584933492
iteration : 10621
train acc:  0.734375
train loss:  0.4912688434123993
train gradient:  0.13157944810989516
iteration : 10622
train acc:  0.8046875
train loss:  0.4530027508735657
train gradient:  0.12914122765200148
iteration : 10623
train acc:  0.6953125
train loss:  0.5064069628715515
train gradient:  0.15336949712000603
iteration : 10624
train acc:  0.7578125
train loss:  0.46574124693870544
train gradient:  0.11118954342668294
iteration : 10625
train acc:  0.7421875
train loss:  0.502338171005249
train gradient:  0.1719351294773086
iteration : 10626
train acc:  0.7421875
train loss:  0.49680766463279724
train gradient:  0.13006063287358294
iteration : 10627
train acc:  0.7890625
train loss:  0.5053330063819885
train gradient:  0.1369182934384091
iteration : 10628
train acc:  0.796875
train loss:  0.40733134746551514
train gradient:  0.0956633765029949
iteration : 10629
train acc:  0.71875
train loss:  0.5224640369415283
train gradient:  0.11637232304489281
iteration : 10630
train acc:  0.71875
train loss:  0.4859415292739868
train gradient:  0.1132277964443764
iteration : 10631
train acc:  0.7109375
train loss:  0.545189380645752
train gradient:  0.15641163780201367
iteration : 10632
train acc:  0.734375
train loss:  0.49777576327323914
train gradient:  0.13586196941584588
iteration : 10633
train acc:  0.71875
train loss:  0.46478408575057983
train gradient:  0.11586846914714326
iteration : 10634
train acc:  0.703125
train loss:  0.5203433632850647
train gradient:  0.14006412307427932
iteration : 10635
train acc:  0.7421875
train loss:  0.5273241400718689
train gradient:  0.1374058396478035
iteration : 10636
train acc:  0.796875
train loss:  0.4401344656944275
train gradient:  0.0966005699405466
iteration : 10637
train acc:  0.7109375
train loss:  0.5591975450515747
train gradient:  0.1827588187664716
iteration : 10638
train acc:  0.75
train loss:  0.5002938508987427
train gradient:  0.13629342850753529
iteration : 10639
train acc:  0.71875
train loss:  0.5057581663131714
train gradient:  0.1116224765764642
iteration : 10640
train acc:  0.796875
train loss:  0.4275952875614166
train gradient:  0.09031612314983636
iteration : 10641
train acc:  0.6953125
train loss:  0.5531781315803528
train gradient:  0.15138397646025387
iteration : 10642
train acc:  0.71875
train loss:  0.48954448103904724
train gradient:  0.10054835323144051
iteration : 10643
train acc:  0.703125
train loss:  0.5332474708557129
train gradient:  0.14177206832910969
iteration : 10644
train acc:  0.71875
train loss:  0.48767325282096863
train gradient:  0.13459890400177657
iteration : 10645
train acc:  0.75
train loss:  0.46139955520629883
train gradient:  0.11720568592993631
iteration : 10646
train acc:  0.7578125
train loss:  0.47005438804626465
train gradient:  0.11594102928766693
iteration : 10647
train acc:  0.7578125
train loss:  0.5012811422348022
train gradient:  0.12192865490075097
iteration : 10648
train acc:  0.71875
train loss:  0.5531717538833618
train gradient:  0.14262878388483988
iteration : 10649
train acc:  0.7421875
train loss:  0.46320223808288574
train gradient:  0.1159258136809452
iteration : 10650
train acc:  0.765625
train loss:  0.48371243476867676
train gradient:  0.14105962862562038
iteration : 10651
train acc:  0.6953125
train loss:  0.5178464651107788
train gradient:  0.16961464628489697
iteration : 10652
train acc:  0.671875
train loss:  0.5505149364471436
train gradient:  0.16536572563679675
iteration : 10653
train acc:  0.7265625
train loss:  0.4430844485759735
train gradient:  0.11985629633095907
iteration : 10654
train acc:  0.765625
train loss:  0.43517088890075684
train gradient:  0.1157827094025544
iteration : 10655
train acc:  0.765625
train loss:  0.43696755170822144
train gradient:  0.11676649986439905
iteration : 10656
train acc:  0.765625
train loss:  0.5261930227279663
train gradient:  0.1608192038814478
iteration : 10657
train acc:  0.6875
train loss:  0.5912737846374512
train gradient:  0.1840085588262581
iteration : 10658
train acc:  0.7109375
train loss:  0.49940866231918335
train gradient:  0.1295083676983953
iteration : 10659
train acc:  0.75
train loss:  0.4639918804168701
train gradient:  0.12330449230591164
iteration : 10660
train acc:  0.828125
train loss:  0.40962448716163635
train gradient:  0.08784900622826193
iteration : 10661
train acc:  0.796875
train loss:  0.4521740972995758
train gradient:  0.11510341774529562
iteration : 10662
train acc:  0.7734375
train loss:  0.5046172142028809
train gradient:  0.11807649381476153
iteration : 10663
train acc:  0.6640625
train loss:  0.5685949921607971
train gradient:  0.17435423989590923
iteration : 10664
train acc:  0.7265625
train loss:  0.5084803104400635
train gradient:  0.10991496850887386
iteration : 10665
train acc:  0.671875
train loss:  0.5355885624885559
train gradient:  0.14169277178622158
iteration : 10666
train acc:  0.71875
train loss:  0.4798872470855713
train gradient:  0.11816583436032864
iteration : 10667
train acc:  0.8046875
train loss:  0.4823939800262451
train gradient:  0.13279003770996464
iteration : 10668
train acc:  0.7734375
train loss:  0.47330352663993835
train gradient:  0.12798589077433592
iteration : 10669
train acc:  0.7578125
train loss:  0.4357842803001404
train gradient:  0.11300374791604796
iteration : 10670
train acc:  0.71875
train loss:  0.49982696771621704
train gradient:  0.12854485743198169
iteration : 10671
train acc:  0.6953125
train loss:  0.5055952072143555
train gradient:  0.13897210535735172
iteration : 10672
train acc:  0.6953125
train loss:  0.5858298540115356
train gradient:  0.16536210483933672
iteration : 10673
train acc:  0.796875
train loss:  0.4268125593662262
train gradient:  0.10138758630075158
iteration : 10674
train acc:  0.8203125
train loss:  0.4167884588241577
train gradient:  0.13247041046201516
iteration : 10675
train acc:  0.765625
train loss:  0.44915300607681274
train gradient:  0.10024204950533284
iteration : 10676
train acc:  0.71875
train loss:  0.49199378490448
train gradient:  0.14386824649865804
iteration : 10677
train acc:  0.6640625
train loss:  0.5887131690979004
train gradient:  0.18280741951483045
iteration : 10678
train acc:  0.734375
train loss:  0.5552370548248291
train gradient:  0.1514852850397168
iteration : 10679
train acc:  0.6875
train loss:  0.5469404458999634
train gradient:  0.19557661408194355
iteration : 10680
train acc:  0.734375
train loss:  0.5455340147018433
train gradient:  0.15600465316620277
iteration : 10681
train acc:  0.7109375
train loss:  0.532324492931366
train gradient:  0.15256860824576546
iteration : 10682
train acc:  0.78125
train loss:  0.43971681594848633
train gradient:  0.12286140538060013
iteration : 10683
train acc:  0.765625
train loss:  0.4884888827800751
train gradient:  0.139492613055886
iteration : 10684
train acc:  0.703125
train loss:  0.5002167224884033
train gradient:  0.14030956974536593
iteration : 10685
train acc:  0.7578125
train loss:  0.4451507031917572
train gradient:  0.1173295046703938
iteration : 10686
train acc:  0.703125
train loss:  0.5175120830535889
train gradient:  0.1472039405173276
iteration : 10687
train acc:  0.7109375
train loss:  0.5278333425521851
train gradient:  0.1326778258857411
iteration : 10688
train acc:  0.7109375
train loss:  0.5360171794891357
train gradient:  0.12097981510295157
iteration : 10689
train acc:  0.84375
train loss:  0.39633214473724365
train gradient:  0.09566892583523205
iteration : 10690
train acc:  0.71875
train loss:  0.5277834534645081
train gradient:  0.14830915108254705
iteration : 10691
train acc:  0.7421875
train loss:  0.5017446875572205
train gradient:  0.12129658131133755
iteration : 10692
train acc:  0.78125
train loss:  0.4690622091293335
train gradient:  0.13190555484163793
iteration : 10693
train acc:  0.6953125
train loss:  0.559330940246582
train gradient:  0.16640939435383867
iteration : 10694
train acc:  0.703125
train loss:  0.558887243270874
train gradient:  0.16205572482333402
iteration : 10695
train acc:  0.71875
train loss:  0.5051164031028748
train gradient:  0.13630519810836955
iteration : 10696
train acc:  0.734375
train loss:  0.4730876386165619
train gradient:  0.12533223797923446
iteration : 10697
train acc:  0.71875
train loss:  0.4913356304168701
train gradient:  0.15496193997231358
iteration : 10698
train acc:  0.7421875
train loss:  0.497724711894989
train gradient:  0.1272594243770499
iteration : 10699
train acc:  0.6875
train loss:  0.5284045934677124
train gradient:  0.1435597711007697
iteration : 10700
train acc:  0.7890625
train loss:  0.4771190881729126
train gradient:  0.11838720330637259
iteration : 10701
train acc:  0.71875
train loss:  0.5223308801651001
train gradient:  0.12854675530554138
iteration : 10702
train acc:  0.7421875
train loss:  0.5110485553741455
train gradient:  0.1314682722669111
iteration : 10703
train acc:  0.8046875
train loss:  0.4243561625480652
train gradient:  0.09792738645982191
iteration : 10704
train acc:  0.6953125
train loss:  0.5204383730888367
train gradient:  0.13312425268931571
iteration : 10705
train acc:  0.828125
train loss:  0.42962729930877686
train gradient:  0.08526185210585445
iteration : 10706
train acc:  0.7421875
train loss:  0.49995264410972595
train gradient:  0.1412710176958713
iteration : 10707
train acc:  0.75
train loss:  0.5013272166252136
train gradient:  0.13387312479717928
iteration : 10708
train acc:  0.7578125
train loss:  0.5021770596504211
train gradient:  0.09285435580900556
iteration : 10709
train acc:  0.7109375
train loss:  0.4808703064918518
train gradient:  0.11058564987334138
iteration : 10710
train acc:  0.75
train loss:  0.4487839341163635
train gradient:  0.10137827075273885
iteration : 10711
train acc:  0.734375
train loss:  0.48875683546066284
train gradient:  0.1183805851209233
iteration : 10712
train acc:  0.7578125
train loss:  0.5030200481414795
train gradient:  0.12217912014047666
iteration : 10713
train acc:  0.765625
train loss:  0.47312718629837036
train gradient:  0.10538926625992323
iteration : 10714
train acc:  0.75
train loss:  0.500380277633667
train gradient:  0.16438115349990648
iteration : 10715
train acc:  0.7578125
train loss:  0.4534388482570648
train gradient:  0.09936266159004682
iteration : 10716
train acc:  0.78125
train loss:  0.43581491708755493
train gradient:  0.09481789383811753
iteration : 10717
train acc:  0.796875
train loss:  0.4114718437194824
train gradient:  0.08136441758801494
iteration : 10718
train acc:  0.7734375
train loss:  0.4523981213569641
train gradient:  0.1382470574945177
iteration : 10719
train acc:  0.765625
train loss:  0.4944859743118286
train gradient:  0.13608255072882924
iteration : 10720
train acc:  0.7734375
train loss:  0.4650801420211792
train gradient:  0.12589481275802444
iteration : 10721
train acc:  0.7578125
train loss:  0.4919305443763733
train gradient:  0.1502550270430077
iteration : 10722
train acc:  0.734375
train loss:  0.4862583875656128
train gradient:  0.12030651144795809
iteration : 10723
train acc:  0.71875
train loss:  0.49086564779281616
train gradient:  0.12338026818318373
iteration : 10724
train acc:  0.765625
train loss:  0.45892906188964844
train gradient:  0.10265125777207705
iteration : 10725
train acc:  0.8203125
train loss:  0.407790869474411
train gradient:  0.10501574500102007
iteration : 10726
train acc:  0.7109375
train loss:  0.48940712213516235
train gradient:  0.11119037714035936
iteration : 10727
train acc:  0.7734375
train loss:  0.43739932775497437
train gradient:  0.11899792816678056
iteration : 10728
train acc:  0.7734375
train loss:  0.43442508578300476
train gradient:  0.10498018847652014
iteration : 10729
train acc:  0.71875
train loss:  0.5188261866569519
train gradient:  0.1357976224737285
iteration : 10730
train acc:  0.75
train loss:  0.4853264093399048
train gradient:  0.11499141238128135
iteration : 10731
train acc:  0.796875
train loss:  0.46289899945259094
train gradient:  0.11480554380004244
iteration : 10732
train acc:  0.7265625
train loss:  0.5313137769699097
train gradient:  0.11092485130606126
iteration : 10733
train acc:  0.78125
train loss:  0.4807189404964447
train gradient:  0.13526405052156248
iteration : 10734
train acc:  0.7890625
train loss:  0.46312350034713745
train gradient:  0.1254137649012721
iteration : 10735
train acc:  0.703125
train loss:  0.543573260307312
train gradient:  0.12977251354570885
iteration : 10736
train acc:  0.78125
train loss:  0.4749702513217926
train gradient:  0.11936688808916321
iteration : 10737
train acc:  0.7109375
train loss:  0.5292012691497803
train gradient:  0.11977765491755231
iteration : 10738
train acc:  0.7421875
train loss:  0.5140706896781921
train gradient:  0.15232342972987015
iteration : 10739
train acc:  0.7578125
train loss:  0.5086214542388916
train gradient:  0.11812013048272307
iteration : 10740
train acc:  0.7734375
train loss:  0.5085651874542236
train gradient:  0.13779432823545812
iteration : 10741
train acc:  0.7578125
train loss:  0.4352112114429474
train gradient:  0.11156957928820675
iteration : 10742
train acc:  0.71875
train loss:  0.5029226541519165
train gradient:  0.11822050824540584
iteration : 10743
train acc:  0.75
train loss:  0.484158992767334
train gradient:  0.11687663383519657
iteration : 10744
train acc:  0.7109375
train loss:  0.5400108098983765
train gradient:  0.1306262170986126
iteration : 10745
train acc:  0.7578125
train loss:  0.4652748703956604
train gradient:  0.13001282890903842
iteration : 10746
train acc:  0.7421875
train loss:  0.47079163789749146
train gradient:  0.10196625892110914
iteration : 10747
train acc:  0.7578125
train loss:  0.4702824354171753
train gradient:  0.10305469970321436
iteration : 10748
train acc:  0.7421875
train loss:  0.5076112747192383
train gradient:  0.12453447234101192
iteration : 10749
train acc:  0.78125
train loss:  0.47866737842559814
train gradient:  0.11043496071803179
iteration : 10750
train acc:  0.796875
train loss:  0.4336775243282318
train gradient:  0.10559114401132454
iteration : 10751
train acc:  0.6953125
train loss:  0.5364105105400085
train gradient:  0.15527746547069343
iteration : 10752
train acc:  0.7421875
train loss:  0.500389039516449
train gradient:  0.13809294887748969
iteration : 10753
train acc:  0.7734375
train loss:  0.4372670650482178
train gradient:  0.0879031318500027
iteration : 10754
train acc:  0.734375
train loss:  0.5362560153007507
train gradient:  0.1475699793716465
iteration : 10755
train acc:  0.7734375
train loss:  0.47478747367858887
train gradient:  0.11914463291238557
iteration : 10756
train acc:  0.7578125
train loss:  0.5262272953987122
train gradient:  0.12975362849835445
iteration : 10757
train acc:  0.765625
train loss:  0.4824807941913605
train gradient:  0.1394399081856369
iteration : 10758
train acc:  0.7890625
train loss:  0.4417065680027008
train gradient:  0.12943720950280163
iteration : 10759
train acc:  0.7421875
train loss:  0.471419095993042
train gradient:  0.11369939257426137
iteration : 10760
train acc:  0.6640625
train loss:  0.5634738206863403
train gradient:  0.14180321215007752
iteration : 10761
train acc:  0.75
train loss:  0.4763728976249695
train gradient:  0.09860817755064523
iteration : 10762
train acc:  0.6953125
train loss:  0.5105295181274414
train gradient:  0.15237711425729752
iteration : 10763
train acc:  0.7421875
train loss:  0.4524299204349518
train gradient:  0.0961038512552773
iteration : 10764
train acc:  0.734375
train loss:  0.47751584649086
train gradient:  0.11405948343600289
iteration : 10765
train acc:  0.765625
train loss:  0.4785116910934448
train gradient:  0.10868451470658276
iteration : 10766
train acc:  0.7578125
train loss:  0.4921421706676483
train gradient:  0.09723396875407746
iteration : 10767
train acc:  0.65625
train loss:  0.5673046112060547
train gradient:  0.17996787951205123
iteration : 10768
train acc:  0.78125
train loss:  0.4849061071872711
train gradient:  0.10669966870504148
iteration : 10769
train acc:  0.6875
train loss:  0.5600454211235046
train gradient:  0.1680979226477871
iteration : 10770
train acc:  0.7265625
train loss:  0.5019433498382568
train gradient:  0.16363928837611857
iteration : 10771
train acc:  0.7734375
train loss:  0.4574182629585266
train gradient:  0.10335388676069471
iteration : 10772
train acc:  0.7578125
train loss:  0.461336612701416
train gradient:  0.11721719213375684
iteration : 10773
train acc:  0.7421875
train loss:  0.51893550157547
train gradient:  0.1391872810007847
iteration : 10774
train acc:  0.7578125
train loss:  0.4839284420013428
train gradient:  0.1290626340255622
iteration : 10775
train acc:  0.8359375
train loss:  0.387776255607605
train gradient:  0.10719170807122787
iteration : 10776
train acc:  0.828125
train loss:  0.4227048456668854
train gradient:  0.09194005449560662
iteration : 10777
train acc:  0.71875
train loss:  0.4831641614437103
train gradient:  0.15769882860098416
iteration : 10778
train acc:  0.78125
train loss:  0.42429211735725403
train gradient:  0.11400617458267112
iteration : 10779
train acc:  0.6640625
train loss:  0.5369212627410889
train gradient:  0.14238806311008895
iteration : 10780
train acc:  0.75
train loss:  0.46546611189842224
train gradient:  0.1275183347396876
iteration : 10781
train acc:  0.734375
train loss:  0.4601408839225769
train gradient:  0.12558872033793284
iteration : 10782
train acc:  0.7734375
train loss:  0.5062195062637329
train gradient:  0.13352555143837583
iteration : 10783
train acc:  0.703125
train loss:  0.5439319014549255
train gradient:  0.15668091188624353
iteration : 10784
train acc:  0.7578125
train loss:  0.47296464443206787
train gradient:  0.10872106083378685
iteration : 10785
train acc:  0.6796875
train loss:  0.5174570083618164
train gradient:  0.1484629826581185
iteration : 10786
train acc:  0.7109375
train loss:  0.5227753520011902
train gradient:  0.1443617583880468
iteration : 10787
train acc:  0.7109375
train loss:  0.509145975112915
train gradient:  0.14297721686781345
iteration : 10788
train acc:  0.796875
train loss:  0.43884360790252686
train gradient:  0.10939101786800175
iteration : 10789
train acc:  0.6484375
train loss:  0.6331602334976196
train gradient:  0.20017379884688702
iteration : 10790
train acc:  0.7578125
train loss:  0.4466180205345154
train gradient:  0.0973212798569484
iteration : 10791
train acc:  0.703125
train loss:  0.44416865706443787
train gradient:  0.11825210548009366
iteration : 10792
train acc:  0.78125
train loss:  0.4347993731498718
train gradient:  0.09436899101189572
iteration : 10793
train acc:  0.765625
train loss:  0.4914894998073578
train gradient:  0.15864652796717926
iteration : 10794
train acc:  0.71875
train loss:  0.5325963497161865
train gradient:  0.17424963226937346
iteration : 10795
train acc:  0.6953125
train loss:  0.493011474609375
train gradient:  0.10998622759745587
iteration : 10796
train acc:  0.75
train loss:  0.5343411564826965
train gradient:  0.13294013318208034
iteration : 10797
train acc:  0.7421875
train loss:  0.47146615386009216
train gradient:  0.10855827686705671
iteration : 10798
train acc:  0.71875
train loss:  0.49610114097595215
train gradient:  0.13491839606654438
iteration : 10799
train acc:  0.78125
train loss:  0.49289441108703613
train gradient:  0.11621743264900672
iteration : 10800
train acc:  0.75
train loss:  0.4791069030761719
train gradient:  0.12430289910593875
iteration : 10801
train acc:  0.7734375
train loss:  0.47323012351989746
train gradient:  0.12014519927173577
iteration : 10802
train acc:  0.734375
train loss:  0.5194517374038696
train gradient:  0.17167400468083216
iteration : 10803
train acc:  0.7578125
train loss:  0.48018258810043335
train gradient:  0.11529645390846399
iteration : 10804
train acc:  0.71875
train loss:  0.5467386245727539
train gradient:  0.16055305557508648
iteration : 10805
train acc:  0.703125
train loss:  0.4927825927734375
train gradient:  0.13545539936453105
iteration : 10806
train acc:  0.7734375
train loss:  0.4803333580493927
train gradient:  0.11003059401028789
iteration : 10807
train acc:  0.71875
train loss:  0.5372182726860046
train gradient:  0.1489445162247391
iteration : 10808
train acc:  0.7109375
train loss:  0.5394881963729858
train gradient:  0.14201129987012137
iteration : 10809
train acc:  0.7109375
train loss:  0.5130783319473267
train gradient:  0.14519350412605725
iteration : 10810
train acc:  0.7890625
train loss:  0.44122809171676636
train gradient:  0.1431861799284386
iteration : 10811
train acc:  0.75
train loss:  0.4990807771682739
train gradient:  0.15723079422658737
iteration : 10812
train acc:  0.7421875
train loss:  0.4567638039588928
train gradient:  0.1568818721048003
iteration : 10813
train acc:  0.7265625
train loss:  0.5820290446281433
train gradient:  0.1862377491829697
iteration : 10814
train acc:  0.6796875
train loss:  0.530135452747345
train gradient:  0.14386723463552034
iteration : 10815
train acc:  0.7421875
train loss:  0.5042800903320312
train gradient:  0.139904348412205
iteration : 10816
train acc:  0.703125
train loss:  0.5250654220581055
train gradient:  0.14601544040982017
iteration : 10817
train acc:  0.7734375
train loss:  0.47202274203300476
train gradient:  0.11621051546738398
iteration : 10818
train acc:  0.71875
train loss:  0.5226744413375854
train gradient:  0.1609358590088239
iteration : 10819
train acc:  0.7265625
train loss:  0.5056873559951782
train gradient:  0.17185134447239175
iteration : 10820
train acc:  0.71875
train loss:  0.5251121520996094
train gradient:  0.1434397324475435
iteration : 10821
train acc:  0.78125
train loss:  0.46443185210227966
train gradient:  0.10802677087865625
iteration : 10822
train acc:  0.71875
train loss:  0.5650606155395508
train gradient:  0.17500038492142828
iteration : 10823
train acc:  0.8203125
train loss:  0.43473976850509644
train gradient:  0.11885114004362947
iteration : 10824
train acc:  0.7265625
train loss:  0.5299042463302612
train gradient:  0.12974734162160348
iteration : 10825
train acc:  0.7421875
train loss:  0.5277599096298218
train gradient:  0.1412919597071777
iteration : 10826
train acc:  0.75
train loss:  0.5000978708267212
train gradient:  0.1609914034640043
iteration : 10827
train acc:  0.796875
train loss:  0.43899399042129517
train gradient:  0.10964552929239213
iteration : 10828
train acc:  0.7421875
train loss:  0.5270001888275146
train gradient:  0.1336131266483867
iteration : 10829
train acc:  0.75
train loss:  0.4574996829032898
train gradient:  0.12531879377103533
iteration : 10830
train acc:  0.71875
train loss:  0.5078818202018738
train gradient:  0.12994926381301816
iteration : 10831
train acc:  0.71875
train loss:  0.5533535480499268
train gradient:  0.13835007373092378
iteration : 10832
train acc:  0.7578125
train loss:  0.4498952627182007
train gradient:  0.11577774560289275
iteration : 10833
train acc:  0.6875
train loss:  0.567168653011322
train gradient:  0.14977705773226935
iteration : 10834
train acc:  0.6875
train loss:  0.5661009550094604
train gradient:  0.1971696292336298
iteration : 10835
train acc:  0.7421875
train loss:  0.46478700637817383
train gradient:  0.11310732099570725
iteration : 10836
train acc:  0.75
train loss:  0.5488507747650146
train gradient:  0.1426683713127307
iteration : 10837
train acc:  0.7734375
train loss:  0.4786180853843689
train gradient:  0.13557750367532037
iteration : 10838
train acc:  0.7421875
train loss:  0.48741114139556885
train gradient:  0.13566446009667504
iteration : 10839
train acc:  0.71875
train loss:  0.5154463052749634
train gradient:  0.14388129576846484
iteration : 10840
train acc:  0.71875
train loss:  0.5237085223197937
train gradient:  0.13111890111432767
iteration : 10841
train acc:  0.7578125
train loss:  0.508823037147522
train gradient:  0.14310692040431056
iteration : 10842
train acc:  0.71875
train loss:  0.5411522388458252
train gradient:  0.13286944631307188
iteration : 10843
train acc:  0.75
train loss:  0.5097028613090515
train gradient:  0.13650157187239198
iteration : 10844
train acc:  0.7265625
train loss:  0.4679238498210907
train gradient:  0.1036336144847819
iteration : 10845
train acc:  0.8125
train loss:  0.4302061200141907
train gradient:  0.0994796540191717
iteration : 10846
train acc:  0.7109375
train loss:  0.512996256351471
train gradient:  0.12208857381448344
iteration : 10847
train acc:  0.703125
train loss:  0.5001925230026245
train gradient:  0.14425575309479413
iteration : 10848
train acc:  0.765625
train loss:  0.48789891600608826
train gradient:  0.14079778135575927
iteration : 10849
train acc:  0.75
train loss:  0.5078727006912231
train gradient:  0.14991148950255884
iteration : 10850
train acc:  0.7734375
train loss:  0.45200541615486145
train gradient:  0.10008562424069377
iteration : 10851
train acc:  0.734375
train loss:  0.5031201839447021
train gradient:  0.11727669768515198
iteration : 10852
train acc:  0.78125
train loss:  0.4243100583553314
train gradient:  0.0967966917683825
iteration : 10853
train acc:  0.78125
train loss:  0.44023624062538147
train gradient:  0.10843830084785297
iteration : 10854
train acc:  0.6796875
train loss:  0.5381504893302917
train gradient:  0.12363612892327296
iteration : 10855
train acc:  0.7578125
train loss:  0.47936344146728516
train gradient:  0.13667961713735644
iteration : 10856
train acc:  0.7265625
train loss:  0.5286567211151123
train gradient:  0.164586308986702
iteration : 10857
train acc:  0.71875
train loss:  0.5408402681350708
train gradient:  0.15536044402895516
iteration : 10858
train acc:  0.8046875
train loss:  0.4084666073322296
train gradient:  0.08214784087743428
iteration : 10859
train acc:  0.734375
train loss:  0.49390849471092224
train gradient:  0.11552267115811181
iteration : 10860
train acc:  0.703125
train loss:  0.5452988147735596
train gradient:  0.16274545253208725
iteration : 10861
train acc:  0.671875
train loss:  0.5594018697738647
train gradient:  0.1411358263127342
iteration : 10862
train acc:  0.7890625
train loss:  0.4251502752304077
train gradient:  0.10926655532091474
iteration : 10863
train acc:  0.734375
train loss:  0.5097298622131348
train gradient:  0.15082537318580852
iteration : 10864
train acc:  0.75
train loss:  0.47946012020111084
train gradient:  0.15913091992880324
iteration : 10865
train acc:  0.7421875
train loss:  0.5007045269012451
train gradient:  0.11592795589835418
iteration : 10866
train acc:  0.71875
train loss:  0.5145828723907471
train gradient:  0.1544887570515438
iteration : 10867
train acc:  0.75
train loss:  0.47963815927505493
train gradient:  0.12516637183204365
iteration : 10868
train acc:  0.75
train loss:  0.5413781404495239
train gradient:  0.1903296836194875
iteration : 10869
train acc:  0.71875
train loss:  0.47531840205192566
train gradient:  0.1185036820962068
iteration : 10870
train acc:  0.75
train loss:  0.4913990795612335
train gradient:  0.12936793513790265
iteration : 10871
train acc:  0.6875
train loss:  0.5126312375068665
train gradient:  0.12128580518572438
iteration : 10872
train acc:  0.75
train loss:  0.4870886206626892
train gradient:  0.12954929154751543
iteration : 10873
train acc:  0.75
train loss:  0.47196245193481445
train gradient:  0.10964209597274291
iteration : 10874
train acc:  0.765625
train loss:  0.4754638969898224
train gradient:  0.14528635735957063
iteration : 10875
train acc:  0.7265625
train loss:  0.5482275485992432
train gradient:  0.15229349393025116
iteration : 10876
train acc:  0.765625
train loss:  0.45248207449913025
train gradient:  0.1159477935034408
iteration : 10877
train acc:  0.765625
train loss:  0.49200499057769775
train gradient:  0.14897851902941067
iteration : 10878
train acc:  0.7109375
train loss:  0.474261075258255
train gradient:  0.10685665098376242
iteration : 10879
train acc:  0.6796875
train loss:  0.5326436161994934
train gradient:  0.1741329698254529
iteration : 10880
train acc:  0.703125
train loss:  0.4875384569168091
train gradient:  0.11138684332777474
iteration : 10881
train acc:  0.734375
train loss:  0.5294316411018372
train gradient:  0.14235060210780093
iteration : 10882
train acc:  0.78125
train loss:  0.4929140508174896
train gradient:  0.18520328917306755
iteration : 10883
train acc:  0.7578125
train loss:  0.47956767678260803
train gradient:  0.11463516516197572
iteration : 10884
train acc:  0.75
train loss:  0.4873579740524292
train gradient:  0.11198006760506102
iteration : 10885
train acc:  0.7578125
train loss:  0.5229851603507996
train gradient:  0.159180050511938
iteration : 10886
train acc:  0.7890625
train loss:  0.43307745456695557
train gradient:  0.10103260943661205
iteration : 10887
train acc:  0.78125
train loss:  0.46818748116493225
train gradient:  0.11162435100326833
iteration : 10888
train acc:  0.671875
train loss:  0.5132179856300354
train gradient:  0.14894975500441315
iteration : 10889
train acc:  0.8046875
train loss:  0.42662981152534485
train gradient:  0.1104956530377014
iteration : 10890
train acc:  0.6953125
train loss:  0.5683890581130981
train gradient:  0.17652382281709564
iteration : 10891
train acc:  0.734375
train loss:  0.4454193413257599
train gradient:  0.1178580445510903
iteration : 10892
train acc:  0.7734375
train loss:  0.4554789364337921
train gradient:  0.09468426903738258
iteration : 10893
train acc:  0.6953125
train loss:  0.5632662773132324
train gradient:  0.16370289245947542
iteration : 10894
train acc:  0.6796875
train loss:  0.542688250541687
train gradient:  0.16492298609950062
iteration : 10895
train acc:  0.765625
train loss:  0.4932708144187927
train gradient:  0.1264532602753592
iteration : 10896
train acc:  0.671875
train loss:  0.537410318851471
train gradient:  0.14422228244588375
iteration : 10897
train acc:  0.75
train loss:  0.5045864582061768
train gradient:  0.10412963064930922
iteration : 10898
train acc:  0.734375
train loss:  0.5440965294837952
train gradient:  0.1403848012668733
iteration : 10899
train acc:  0.7109375
train loss:  0.511405885219574
train gradient:  0.15000141910187584
iteration : 10900
train acc:  0.7421875
train loss:  0.5134537220001221
train gradient:  0.14453751660366876
iteration : 10901
train acc:  0.6875
train loss:  0.5301542282104492
train gradient:  0.1269563464606025
iteration : 10902
train acc:  0.7578125
train loss:  0.49912264943122864
train gradient:  0.13148267735408814
iteration : 10903
train acc:  0.7265625
train loss:  0.4929649531841278
train gradient:  0.11090752950208067
iteration : 10904
train acc:  0.6953125
train loss:  0.6193495988845825
train gradient:  0.20843067160206452
iteration : 10905
train acc:  0.734375
train loss:  0.4407927393913269
train gradient:  0.10121542277083528
iteration : 10906
train acc:  0.7578125
train loss:  0.5016155242919922
train gradient:  0.14397221587477696
iteration : 10907
train acc:  0.71875
train loss:  0.5069096088409424
train gradient:  0.1443768208541903
iteration : 10908
train acc:  0.7890625
train loss:  0.4364650249481201
train gradient:  0.09841975690107349
iteration : 10909
train acc:  0.6953125
train loss:  0.5254189968109131
train gradient:  0.138789502669797
iteration : 10910
train acc:  0.7890625
train loss:  0.4884580373764038
train gradient:  0.14730541190987412
iteration : 10911
train acc:  0.8046875
train loss:  0.46475061774253845
train gradient:  0.10180866878345411
iteration : 10912
train acc:  0.7890625
train loss:  0.4690382778644562
train gradient:  0.11648392259968882
iteration : 10913
train acc:  0.7265625
train loss:  0.5578286647796631
train gradient:  0.17718769592202466
iteration : 10914
train acc:  0.8203125
train loss:  0.43516790866851807
train gradient:  0.09003227480829061
iteration : 10915
train acc:  0.7265625
train loss:  0.6102719902992249
train gradient:  0.2050190637527739
iteration : 10916
train acc:  0.7578125
train loss:  0.4996386766433716
train gradient:  0.14090888279258845
iteration : 10917
train acc:  0.703125
train loss:  0.5358787775039673
train gradient:  0.15209817033435463
iteration : 10918
train acc:  0.7265625
train loss:  0.5267822742462158
train gradient:  0.13037178523604837
iteration : 10919
train acc:  0.71875
train loss:  0.5346903800964355
train gradient:  0.14663279799971368
iteration : 10920
train acc:  0.7421875
train loss:  0.460498571395874
train gradient:  0.09301194857368959
iteration : 10921
train acc:  0.7265625
train loss:  0.48519524931907654
train gradient:  0.12083175989259401
iteration : 10922
train acc:  0.7734375
train loss:  0.4954197108745575
train gradient:  0.13002882935976798
iteration : 10923
train acc:  0.7734375
train loss:  0.46072623133659363
train gradient:  0.12821489266397348
iteration : 10924
train acc:  0.7109375
train loss:  0.5104712247848511
train gradient:  0.1437719248915686
iteration : 10925
train acc:  0.7734375
train loss:  0.4753413796424866
train gradient:  0.13905863440611743
iteration : 10926
train acc:  0.765625
train loss:  0.4569207727909088
train gradient:  0.12265102539735875
iteration : 10927
train acc:  0.765625
train loss:  0.4656047224998474
train gradient:  0.12406578511027257
iteration : 10928
train acc:  0.796875
train loss:  0.45658135414123535
train gradient:  0.1094982200363393
iteration : 10929
train acc:  0.7265625
train loss:  0.46265751123428345
train gradient:  0.12176888716842263
iteration : 10930
train acc:  0.765625
train loss:  0.4572940468788147
train gradient:  0.12335919817355408
iteration : 10931
train acc:  0.7578125
train loss:  0.5069594979286194
train gradient:  0.16539217748029506
iteration : 10932
train acc:  0.765625
train loss:  0.4267883896827698
train gradient:  0.08723986632967243
iteration : 10933
train acc:  0.7109375
train loss:  0.5281460285186768
train gradient:  0.13169204734728246
iteration : 10934
train acc:  0.6953125
train loss:  0.5172858238220215
train gradient:  0.12016875017353279
iteration : 10935
train acc:  0.734375
train loss:  0.5436289310455322
train gradient:  0.1603241473161971
iteration : 10936
train acc:  0.78125
train loss:  0.5043697357177734
train gradient:  0.11677052741847495
iteration : 10937
train acc:  0.8046875
train loss:  0.47653520107269287
train gradient:  0.13120946808938416
iteration : 10938
train acc:  0.75
train loss:  0.4854784309864044
train gradient:  0.11920008937421865
iteration : 10939
train acc:  0.7109375
train loss:  0.5292052626609802
train gradient:  0.13884860411430097
iteration : 10940
train acc:  0.7421875
train loss:  0.5070624947547913
train gradient:  0.13015165953116847
iteration : 10941
train acc:  0.7265625
train loss:  0.5475425720214844
train gradient:  0.15093244587815086
iteration : 10942
train acc:  0.6953125
train loss:  0.4931105971336365
train gradient:  0.10855186412026743
iteration : 10943
train acc:  0.796875
train loss:  0.4356299638748169
train gradient:  0.09128320942344298
iteration : 10944
train acc:  0.71875
train loss:  0.49487653374671936
train gradient:  0.11246063643486756
iteration : 10945
train acc:  0.78125
train loss:  0.47114378213882446
train gradient:  0.09365768180167947
iteration : 10946
train acc:  0.734375
train loss:  0.5447123050689697
train gradient:  0.16710821479577592
iteration : 10947
train acc:  0.734375
train loss:  0.49438637495040894
train gradient:  0.12388411788217063
iteration : 10948
train acc:  0.734375
train loss:  0.5676572322845459
train gradient:  0.195472553448112
iteration : 10949
train acc:  0.71875
train loss:  0.5182075500488281
train gradient:  0.12488907808448926
iteration : 10950
train acc:  0.6875
train loss:  0.5055004358291626
train gradient:  0.1590928136864232
iteration : 10951
train acc:  0.71875
train loss:  0.48083364963531494
train gradient:  0.10238687593460812
iteration : 10952
train acc:  0.7578125
train loss:  0.47508129477500916
train gradient:  0.10103683874891484
iteration : 10953
train acc:  0.75
train loss:  0.4846476912498474
train gradient:  0.12437533632150066
iteration : 10954
train acc:  0.7734375
train loss:  0.49568137526512146
train gradient:  0.1639395496391074
iteration : 10955
train acc:  0.7421875
train loss:  0.4844157099723816
train gradient:  0.12490793612257922
iteration : 10956
train acc:  0.828125
train loss:  0.4428151845932007
train gradient:  0.10416616978526654
iteration : 10957
train acc:  0.75
train loss:  0.5160876512527466
train gradient:  0.14835413581674406
iteration : 10958
train acc:  0.7265625
train loss:  0.5433728694915771
train gradient:  0.1353449210422535
iteration : 10959
train acc:  0.6484375
train loss:  0.5455150008201599
train gradient:  0.13158709770063826
iteration : 10960
train acc:  0.8125
train loss:  0.4240715503692627
train gradient:  0.09585344725126153
iteration : 10961
train acc:  0.7890625
train loss:  0.4483468234539032
train gradient:  0.09756290813563866
iteration : 10962
train acc:  0.7578125
train loss:  0.5014818906784058
train gradient:  0.12379079644419615
iteration : 10963
train acc:  0.7109375
train loss:  0.5094912052154541
train gradient:  0.10648228890996993
iteration : 10964
train acc:  0.71875
train loss:  0.5455125570297241
train gradient:  0.1411119049154939
iteration : 10965
train acc:  0.75
train loss:  0.5213484764099121
train gradient:  0.14963137554289915
iteration : 10966
train acc:  0.765625
train loss:  0.44035112857818604
train gradient:  0.08973256231750885
iteration : 10967
train acc:  0.71875
train loss:  0.5356490612030029
train gradient:  0.1473457955198859
iteration : 10968
train acc:  0.7265625
train loss:  0.5082442164421082
train gradient:  0.13777404894495748
iteration : 10969
train acc:  0.6953125
train loss:  0.5477166175842285
train gradient:  0.14691211632049206
iteration : 10970
train acc:  0.734375
train loss:  0.4202924966812134
train gradient:  0.1028540852719358
iteration : 10971
train acc:  0.734375
train loss:  0.481935054063797
train gradient:  0.10935300914925315
iteration : 10972
train acc:  0.734375
train loss:  0.5240792036056519
train gradient:  0.1612462633347551
iteration : 10973
train acc:  0.7421875
train loss:  0.42864400148391724
train gradient:  0.1034207210632108
iteration : 10974
train acc:  0.6484375
train loss:  0.5648564696311951
train gradient:  0.17270881670415922
iteration : 10975
train acc:  0.7734375
train loss:  0.5090194344520569
train gradient:  0.10560920990650143
iteration : 10976
train acc:  0.75
train loss:  0.4905058741569519
train gradient:  0.131062767079599
iteration : 10977
train acc:  0.71875
train loss:  0.5837035179138184
train gradient:  0.16774927577212817
iteration : 10978
train acc:  0.6953125
train loss:  0.5415074229240417
train gradient:  0.20508894997473193
iteration : 10979
train acc:  0.703125
train loss:  0.48184704780578613
train gradient:  0.11003960799626421
iteration : 10980
train acc:  0.6953125
train loss:  0.5246024131774902
train gradient:  0.13335558546207493
iteration : 10981
train acc:  0.765625
train loss:  0.4795907139778137
train gradient:  0.10789141370643901
iteration : 10982
train acc:  0.7421875
train loss:  0.563151478767395
train gradient:  0.19564135758107704
iteration : 10983
train acc:  0.6875
train loss:  0.49843400716781616
train gradient:  0.12294927559474478
iteration : 10984
train acc:  0.75
train loss:  0.45930150151252747
train gradient:  0.13057680379611977
iteration : 10985
train acc:  0.75
train loss:  0.43730899691581726
train gradient:  0.09997729640290913
iteration : 10986
train acc:  0.6796875
train loss:  0.5694644451141357
train gradient:  0.16455688852358136
iteration : 10987
train acc:  0.671875
train loss:  0.516653835773468
train gradient:  0.16378028675132383
iteration : 10988
train acc:  0.734375
train loss:  0.45971429347991943
train gradient:  0.08527867849969967
iteration : 10989
train acc:  0.8046875
train loss:  0.44439926743507385
train gradient:  0.12001388041747446
iteration : 10990
train acc:  0.6796875
train loss:  0.57275390625
train gradient:  0.14741435449774004
iteration : 10991
train acc:  0.7734375
train loss:  0.4303015470504761
train gradient:  0.10275476315395399
iteration : 10992
train acc:  0.7265625
train loss:  0.5240849852561951
train gradient:  0.13742590046766698
iteration : 10993
train acc:  0.7578125
train loss:  0.4890076518058777
train gradient:  0.12762558063596172
iteration : 10994
train acc:  0.7109375
train loss:  0.5108001828193665
train gradient:  0.115689772069487
iteration : 10995
train acc:  0.7890625
train loss:  0.44294363260269165
train gradient:  0.10247091215279323
iteration : 10996
train acc:  0.7578125
train loss:  0.46389925479888916
train gradient:  0.12471708062384568
iteration : 10997
train acc:  0.7890625
train loss:  0.44529658555984497
train gradient:  0.0961876766430127
iteration : 10998
train acc:  0.71875
train loss:  0.47885990142822266
train gradient:  0.09748464580145749
iteration : 10999
train acc:  0.7578125
train loss:  0.4802256226539612
train gradient:  0.10092159062229578
iteration : 11000
train acc:  0.7734375
train loss:  0.4564834237098694
train gradient:  0.15435016544323166
iteration : 11001
train acc:  0.7578125
train loss:  0.5010485649108887
train gradient:  0.12032478981427287
iteration : 11002
train acc:  0.7421875
train loss:  0.47238263487815857
train gradient:  0.13295917474049535
iteration : 11003
train acc:  0.6796875
train loss:  0.5075291395187378
train gradient:  0.12060409643353195
iteration : 11004
train acc:  0.765625
train loss:  0.5050687789916992
train gradient:  0.1178297703142127
iteration : 11005
train acc:  0.7421875
train loss:  0.44137340784072876
train gradient:  0.11525855882743395
iteration : 11006
train acc:  0.7109375
train loss:  0.51414954662323
train gradient:  0.11931312397565282
iteration : 11007
train acc:  0.6484375
train loss:  0.5699118971824646
train gradient:  0.1407160864258601
iteration : 11008
train acc:  0.703125
train loss:  0.5473418235778809
train gradient:  0.1395858196688662
iteration : 11009
train acc:  0.734375
train loss:  0.5481512546539307
train gradient:  0.16076724487518457
iteration : 11010
train acc:  0.703125
train loss:  0.49179333448410034
train gradient:  0.11665149723188624
iteration : 11011
train acc:  0.765625
train loss:  0.5101761221885681
train gradient:  0.14540926143084673
iteration : 11012
train acc:  0.7109375
train loss:  0.569977343082428
train gradient:  0.17341006017852145
iteration : 11013
train acc:  0.8046875
train loss:  0.4409923255443573
train gradient:  0.10288523411514278
iteration : 11014
train acc:  0.8046875
train loss:  0.4550950527191162
train gradient:  0.10517798898256245
iteration : 11015
train acc:  0.7421875
train loss:  0.5187309384346008
train gradient:  0.13736904758627633
iteration : 11016
train acc:  0.71875
train loss:  0.5274649262428284
train gradient:  0.18157726781482908
iteration : 11017
train acc:  0.703125
train loss:  0.486461341381073
train gradient:  0.10435078596832076
iteration : 11018
train acc:  0.6953125
train loss:  0.5675438642501831
train gradient:  0.21696152836119814
iteration : 11019
train acc:  0.703125
train loss:  0.5062589049339294
train gradient:  0.13837501207715416
iteration : 11020
train acc:  0.7109375
train loss:  0.4525015950202942
train gradient:  0.10093447104175708
iteration : 11021
train acc:  0.6328125
train loss:  0.583810567855835
train gradient:  0.15953599711090893
iteration : 11022
train acc:  0.75
train loss:  0.47607895731925964
train gradient:  0.13037705581215636
iteration : 11023
train acc:  0.6484375
train loss:  0.5355383157730103
train gradient:  0.16615273073664782
iteration : 11024
train acc:  0.71875
train loss:  0.5092868208885193
train gradient:  0.14515420904250204
iteration : 11025
train acc:  0.765625
train loss:  0.4655799865722656
train gradient:  0.17036739354063757
iteration : 11026
train acc:  0.8046875
train loss:  0.42870986461639404
train gradient:  0.10976458897758365
iteration : 11027
train acc:  0.703125
train loss:  0.5480117797851562
train gradient:  0.17823947205376584
iteration : 11028
train acc:  0.734375
train loss:  0.4500054717063904
train gradient:  0.11764570232900792
iteration : 11029
train acc:  0.765625
train loss:  0.45277589559555054
train gradient:  0.10571014838661533
iteration : 11030
train acc:  0.765625
train loss:  0.5126047134399414
train gradient:  0.16734228105033688
iteration : 11031
train acc:  0.7109375
train loss:  0.5265552997589111
train gradient:  0.1304223221866313
iteration : 11032
train acc:  0.7578125
train loss:  0.4558979868888855
train gradient:  0.1287867549389522
iteration : 11033
train acc:  0.7734375
train loss:  0.45164042711257935
train gradient:  0.10739194246091908
iteration : 11034
train acc:  0.6953125
train loss:  0.5414144992828369
train gradient:  0.200073574307701
iteration : 11035
train acc:  0.8046875
train loss:  0.4159139394760132
train gradient:  0.08800029058520864
iteration : 11036
train acc:  0.7890625
train loss:  0.437098890542984
train gradient:  0.12122099363177241
iteration : 11037
train acc:  0.6640625
train loss:  0.5939364433288574
train gradient:  0.17052270776631961
iteration : 11038
train acc:  0.7890625
train loss:  0.41615957021713257
train gradient:  0.09754908301179954
iteration : 11039
train acc:  0.78125
train loss:  0.49457818269729614
train gradient:  0.1251071057265023
iteration : 11040
train acc:  0.765625
train loss:  0.46806880831718445
train gradient:  0.09050055064037749
iteration : 11041
train acc:  0.7578125
train loss:  0.5061699151992798
train gradient:  0.1412051374182624
iteration : 11042
train acc:  0.7109375
train loss:  0.562656044960022
train gradient:  0.1578604933930457
iteration : 11043
train acc:  0.7578125
train loss:  0.48431727290153503
train gradient:  0.11474486702754438
iteration : 11044
train acc:  0.7890625
train loss:  0.4125763773918152
train gradient:  0.08087519733114415
iteration : 11045
train acc:  0.6875
train loss:  0.5894039869308472
train gradient:  0.17998393244615313
iteration : 11046
train acc:  0.71875
train loss:  0.47868406772613525
train gradient:  0.10008021088381139
iteration : 11047
train acc:  0.75
train loss:  0.4948544502258301
train gradient:  0.14614945500984292
iteration : 11048
train acc:  0.7890625
train loss:  0.4608546495437622
train gradient:  0.11247799484759162
iteration : 11049
train acc:  0.7421875
train loss:  0.49788397550582886
train gradient:  0.12150738380011743
iteration : 11050
train acc:  0.796875
train loss:  0.48350679874420166
train gradient:  0.12142191698342975
iteration : 11051
train acc:  0.8046875
train loss:  0.42138391733169556
train gradient:  0.09648493639758861
iteration : 11052
train acc:  0.703125
train loss:  0.5349590182304382
train gradient:  0.1556631155377388
iteration : 11053
train acc:  0.7109375
train loss:  0.4831472337245941
train gradient:  0.11318964380174552
iteration : 11054
train acc:  0.7421875
train loss:  0.48676398396492004
train gradient:  0.17127662188968656
iteration : 11055
train acc:  0.7421875
train loss:  0.4660322070121765
train gradient:  0.12756847916496047
iteration : 11056
train acc:  0.7578125
train loss:  0.43888986110687256
train gradient:  0.08972942581476005
iteration : 11057
train acc:  0.765625
train loss:  0.5045334100723267
train gradient:  0.14158135357092383
iteration : 11058
train acc:  0.71875
train loss:  0.5250623226165771
train gradient:  0.13242294697277496
iteration : 11059
train acc:  0.6875
train loss:  0.5340024828910828
train gradient:  0.15118625012710468
iteration : 11060
train acc:  0.7578125
train loss:  0.4539905786514282
train gradient:  0.11498405915533214
iteration : 11061
train acc:  0.7578125
train loss:  0.4901844263076782
train gradient:  0.10702762433891108
iteration : 11062
train acc:  0.765625
train loss:  0.4492742717266083
train gradient:  0.12274958362698193
iteration : 11063
train acc:  0.71875
train loss:  0.5381040573120117
train gradient:  0.17147146132334556
iteration : 11064
train acc:  0.71875
train loss:  0.4995874762535095
train gradient:  0.12665751584616236
iteration : 11065
train acc:  0.78125
train loss:  0.4300631880760193
train gradient:  0.10149581458576311
iteration : 11066
train acc:  0.75
train loss:  0.43959909677505493
train gradient:  0.1083699744970954
iteration : 11067
train acc:  0.765625
train loss:  0.44275790452957153
train gradient:  0.10722487549480328
iteration : 11068
train acc:  0.71875
train loss:  0.5058766007423401
train gradient:  0.12835367844494228
iteration : 11069
train acc:  0.7578125
train loss:  0.46179938316345215
train gradient:  0.1095601665433806
iteration : 11070
train acc:  0.8203125
train loss:  0.407584011554718
train gradient:  0.09323559214682413
iteration : 11071
train acc:  0.765625
train loss:  0.47244536876678467
train gradient:  0.12186829329440256
iteration : 11072
train acc:  0.78125
train loss:  0.44397592544555664
train gradient:  0.10557012620357445
iteration : 11073
train acc:  0.7578125
train loss:  0.46939700841903687
train gradient:  0.10576296447432915
iteration : 11074
train acc:  0.7578125
train loss:  0.4815206527709961
train gradient:  0.11143682760383453
iteration : 11075
train acc:  0.7109375
train loss:  0.49480125308036804
train gradient:  0.13379056372212195
iteration : 11076
train acc:  0.7265625
train loss:  0.4686695337295532
train gradient:  0.12029202038151357
iteration : 11077
train acc:  0.7578125
train loss:  0.5067404508590698
train gradient:  0.11500606521897458
iteration : 11078
train acc:  0.7265625
train loss:  0.4681127667427063
train gradient:  0.13747085084020277
iteration : 11079
train acc:  0.6953125
train loss:  0.5481022596359253
train gradient:  0.1395200129716666
iteration : 11080
train acc:  0.765625
train loss:  0.4704676866531372
train gradient:  0.11864845351947025
iteration : 11081
train acc:  0.7265625
train loss:  0.5356037616729736
train gradient:  0.16760524227817336
iteration : 11082
train acc:  0.7109375
train loss:  0.5188452005386353
train gradient:  0.14631102973686527
iteration : 11083
train acc:  0.7734375
train loss:  0.5030770301818848
train gradient:  0.10687850771466277
iteration : 11084
train acc:  0.7109375
train loss:  0.54246985912323
train gradient:  0.14551529884698403
iteration : 11085
train acc:  0.7109375
train loss:  0.5211043357849121
train gradient:  0.1485255327142922
iteration : 11086
train acc:  0.7578125
train loss:  0.4550541639328003
train gradient:  0.10829650407932269
iteration : 11087
train acc:  0.71875
train loss:  0.5143300294876099
train gradient:  0.1312228567677301
iteration : 11088
train acc:  0.7578125
train loss:  0.4503984749317169
train gradient:  0.10875331893910868
iteration : 11089
train acc:  0.71875
train loss:  0.5457303524017334
train gradient:  0.14153462133802303
iteration : 11090
train acc:  0.6796875
train loss:  0.5482434034347534
train gradient:  0.170557150268699
iteration : 11091
train acc:  0.7109375
train loss:  0.5305097103118896
train gradient:  0.18044615302302408
iteration : 11092
train acc:  0.75
train loss:  0.48265984654426575
train gradient:  0.11647925493570574
iteration : 11093
train acc:  0.8125
train loss:  0.4119592010974884
train gradient:  0.10440041701490071
iteration : 11094
train acc:  0.7890625
train loss:  0.44313085079193115
train gradient:  0.10566928091949511
iteration : 11095
train acc:  0.7265625
train loss:  0.49727529287338257
train gradient:  0.1153292551539291
iteration : 11096
train acc:  0.71875
train loss:  0.5116199254989624
train gradient:  0.13442747076934547
iteration : 11097
train acc:  0.75
train loss:  0.4924127757549286
train gradient:  0.10329971450917252
iteration : 11098
train acc:  0.8359375
train loss:  0.392575740814209
train gradient:  0.10106642786159073
iteration : 11099
train acc:  0.75
train loss:  0.5036391615867615
train gradient:  0.13608507411248963
iteration : 11100
train acc:  0.734375
train loss:  0.5170607566833496
train gradient:  0.11570203411334752
iteration : 11101
train acc:  0.703125
train loss:  0.49937599897384644
train gradient:  0.14763001565465278
iteration : 11102
train acc:  0.6875
train loss:  0.5366154909133911
train gradient:  0.15101065853881346
iteration : 11103
train acc:  0.796875
train loss:  0.443539559841156
train gradient:  0.14101187999202977
iteration : 11104
train acc:  0.78125
train loss:  0.4963133633136749
train gradient:  0.13658158308346527
iteration : 11105
train acc:  0.7265625
train loss:  0.4781695604324341
train gradient:  0.1054147325912267
iteration : 11106
train acc:  0.734375
train loss:  0.49448585510253906
train gradient:  0.14281295976510608
iteration : 11107
train acc:  0.6484375
train loss:  0.5868649482727051
train gradient:  0.18155308155797018
iteration : 11108
train acc:  0.7109375
train loss:  0.48856204748153687
train gradient:  0.14309761469808607
iteration : 11109
train acc:  0.8203125
train loss:  0.44360971450805664
train gradient:  0.09982478277198799
iteration : 11110
train acc:  0.7578125
train loss:  0.49387824535369873
train gradient:  0.1367309009400147
iteration : 11111
train acc:  0.734375
train loss:  0.5546378493309021
train gradient:  0.1325391794426162
iteration : 11112
train acc:  0.7578125
train loss:  0.46316999197006226
train gradient:  0.1284885099545038
iteration : 11113
train acc:  0.7890625
train loss:  0.4316299855709076
train gradient:  0.10179854834853587
iteration : 11114
train acc:  0.734375
train loss:  0.4736650586128235
train gradient:  0.11672033341377579
iteration : 11115
train acc:  0.6796875
train loss:  0.5600404143333435
train gradient:  0.13301901469380747
iteration : 11116
train acc:  0.765625
train loss:  0.49868685007095337
train gradient:  0.16302082403112017
iteration : 11117
train acc:  0.7109375
train loss:  0.5414983630180359
train gradient:  0.18972413799161267
iteration : 11118
train acc:  0.796875
train loss:  0.43160516023635864
train gradient:  0.12342939391657673
iteration : 11119
train acc:  0.78125
train loss:  0.4741162955760956
train gradient:  0.09706262992401542
iteration : 11120
train acc:  0.765625
train loss:  0.4655347764492035
train gradient:  0.12595510357938924
iteration : 11121
train acc:  0.78125
train loss:  0.4562947154045105
train gradient:  0.12268583388979223
iteration : 11122
train acc:  0.7734375
train loss:  0.46209394931793213
train gradient:  0.1189028250854376
iteration : 11123
train acc:  0.7265625
train loss:  0.4941663444042206
train gradient:  0.13429394911559311
iteration : 11124
train acc:  0.75
train loss:  0.47907400131225586
train gradient:  0.10856346923741661
iteration : 11125
train acc:  0.8203125
train loss:  0.38078027963638306
train gradient:  0.08564048451940666
iteration : 11126
train acc:  0.75
train loss:  0.4655224680900574
train gradient:  0.1075671828389526
iteration : 11127
train acc:  0.6953125
train loss:  0.526587724685669
train gradient:  0.14648692798004836
iteration : 11128
train acc:  0.7734375
train loss:  0.44050687551498413
train gradient:  0.12080297972441516
iteration : 11129
train acc:  0.7265625
train loss:  0.5550291538238525
train gradient:  0.13698830239742837
iteration : 11130
train acc:  0.78125
train loss:  0.4411104619503021
train gradient:  0.09968166270924769
iteration : 11131
train acc:  0.7578125
train loss:  0.4538537561893463
train gradient:  0.12145007759087534
iteration : 11132
train acc:  0.7890625
train loss:  0.4470146894454956
train gradient:  0.09946418377191299
iteration : 11133
train acc:  0.7734375
train loss:  0.4803546071052551
train gradient:  0.10457585995349024
iteration : 11134
train acc:  0.75
train loss:  0.48949092626571655
train gradient:  0.1576153666345107
iteration : 11135
train acc:  0.765625
train loss:  0.48993510007858276
train gradient:  0.11298980742559878
iteration : 11136
train acc:  0.7109375
train loss:  0.4726177453994751
train gradient:  0.1320790447887118
iteration : 11137
train acc:  0.7578125
train loss:  0.4842866361141205
train gradient:  0.1101755644128528
iteration : 11138
train acc:  0.7265625
train loss:  0.4917180836200714
train gradient:  0.14408210583625713
iteration : 11139
train acc:  0.734375
train loss:  0.5175738334655762
train gradient:  0.12273546251260373
iteration : 11140
train acc:  0.6796875
train loss:  0.5375064611434937
train gradient:  0.22314210981229204
iteration : 11141
train acc:  0.6875
train loss:  0.5858595967292786
train gradient:  0.21562116505855175
iteration : 11142
train acc:  0.6640625
train loss:  0.5334897637367249
train gradient:  0.12207884379980759
iteration : 11143
train acc:  0.6796875
train loss:  0.5446910858154297
train gradient:  0.13795464878430697
iteration : 11144
train acc:  0.6953125
train loss:  0.5356100797653198
train gradient:  0.14204190675796632
iteration : 11145
train acc:  0.7578125
train loss:  0.5157530903816223
train gradient:  0.15759840978068457
iteration : 11146
train acc:  0.7890625
train loss:  0.485271692276001
train gradient:  0.1255538624095259
iteration : 11147
train acc:  0.765625
train loss:  0.4469422698020935
train gradient:  0.17588844982901214
iteration : 11148
train acc:  0.6875
train loss:  0.5372977256774902
train gradient:  0.1252861595979559
iteration : 11149
train acc:  0.7421875
train loss:  0.5111755132675171
train gradient:  0.15411298086085068
iteration : 11150
train acc:  0.765625
train loss:  0.4866296052932739
train gradient:  0.11412712910236535
iteration : 11151
train acc:  0.6796875
train loss:  0.5622841119766235
train gradient:  0.15529209202730485
iteration : 11152
train acc:  0.765625
train loss:  0.4577925205230713
train gradient:  0.11134406822801848
iteration : 11153
train acc:  0.765625
train loss:  0.49774667620658875
train gradient:  0.14809404424471442
iteration : 11154
train acc:  0.6953125
train loss:  0.52175372838974
train gradient:  0.17544195045016986
iteration : 11155
train acc:  0.7578125
train loss:  0.5077004432678223
train gradient:  0.11830241914061577
iteration : 11156
train acc:  0.703125
train loss:  0.5365909337997437
train gradient:  0.1384210909299513
iteration : 11157
train acc:  0.765625
train loss:  0.48345252871513367
train gradient:  0.11511579819645575
iteration : 11158
train acc:  0.734375
train loss:  0.485879123210907
train gradient:  0.13819193505703858
iteration : 11159
train acc:  0.734375
train loss:  0.46047261357307434
train gradient:  0.11445421231453914
iteration : 11160
train acc:  0.78125
train loss:  0.47813689708709717
train gradient:  0.10841949896095111
iteration : 11161
train acc:  0.7734375
train loss:  0.4798891246318817
train gradient:  0.10963164513994454
iteration : 11162
train acc:  0.7734375
train loss:  0.4758375883102417
train gradient:  0.10702360146671223
iteration : 11163
train acc:  0.734375
train loss:  0.46074649691581726
train gradient:  0.11078870558248169
iteration : 11164
train acc:  0.6640625
train loss:  0.5321532487869263
train gradient:  0.13602261958975084
iteration : 11165
train acc:  0.671875
train loss:  0.6205652952194214
train gradient:  0.19435071447798574
iteration : 11166
train acc:  0.6953125
train loss:  0.5149742364883423
train gradient:  0.13304284983481326
iteration : 11167
train acc:  0.703125
train loss:  0.5539798736572266
train gradient:  0.1444683159826201
iteration : 11168
train acc:  0.7421875
train loss:  0.5202775001525879
train gradient:  0.11494382846379954
iteration : 11169
train acc:  0.8046875
train loss:  0.4318186938762665
train gradient:  0.09852468273064503
iteration : 11170
train acc:  0.8125
train loss:  0.4226570129394531
train gradient:  0.09325391594490082
iteration : 11171
train acc:  0.7265625
train loss:  0.485329270362854
train gradient:  0.10153978478193988
iteration : 11172
train acc:  0.7890625
train loss:  0.44036799669265747
train gradient:  0.0994984533688077
iteration : 11173
train acc:  0.75
train loss:  0.44958022236824036
train gradient:  0.11334274346201349
iteration : 11174
train acc:  0.6484375
train loss:  0.5055007934570312
train gradient:  0.10342524931133215
iteration : 11175
train acc:  0.7265625
train loss:  0.5176990032196045
train gradient:  0.12123741923500388
iteration : 11176
train acc:  0.6953125
train loss:  0.5400673151016235
train gradient:  0.18658424168779206
iteration : 11177
train acc:  0.734375
train loss:  0.5293421149253845
train gradient:  0.1621754954863102
iteration : 11178
train acc:  0.7265625
train loss:  0.511722207069397
train gradient:  0.16323320654841145
iteration : 11179
train acc:  0.7265625
train loss:  0.4809108078479767
train gradient:  0.15702288537300968
iteration : 11180
train acc:  0.734375
train loss:  0.45000606775283813
train gradient:  0.11699872357997343
iteration : 11181
train acc:  0.71875
train loss:  0.44957947731018066
train gradient:  0.10853967318613801
iteration : 11182
train acc:  0.734375
train loss:  0.4820224344730377
train gradient:  0.11606552507967337
iteration : 11183
train acc:  0.734375
train loss:  0.47545501589775085
train gradient:  0.11760276119573061
iteration : 11184
train acc:  0.6953125
train loss:  0.495016872882843
train gradient:  0.13169791581018692
iteration : 11185
train acc:  0.78125
train loss:  0.45012348890304565
train gradient:  0.09241405393044502
iteration : 11186
train acc:  0.75
train loss:  0.4720859229564667
train gradient:  0.13418361265816037
iteration : 11187
train acc:  0.765625
train loss:  0.46722644567489624
train gradient:  0.10971602970326874
iteration : 11188
train acc:  0.8046875
train loss:  0.4762521982192993
train gradient:  0.13114324381327194
iteration : 11189
train acc:  0.71875
train loss:  0.5371588468551636
train gradient:  0.12702478234994408
iteration : 11190
train acc:  0.765625
train loss:  0.47389212250709534
train gradient:  0.11958446102994262
iteration : 11191
train acc:  0.75
train loss:  0.4534442722797394
train gradient:  0.12099985007017075
iteration : 11192
train acc:  0.71875
train loss:  0.47506842017173767
train gradient:  0.1287870493057685
iteration : 11193
train acc:  0.71875
train loss:  0.5018988251686096
train gradient:  0.14595621266931463
iteration : 11194
train acc:  0.734375
train loss:  0.48106610774993896
train gradient:  0.12397085358362524
iteration : 11195
train acc:  0.796875
train loss:  0.4115515351295471
train gradient:  0.11141115484691816
iteration : 11196
train acc:  0.75
train loss:  0.4868404269218445
train gradient:  0.16753922449082626
iteration : 11197
train acc:  0.7578125
train loss:  0.46368399262428284
train gradient:  0.13262238834369255
iteration : 11198
train acc:  0.734375
train loss:  0.5101432800292969
train gradient:  0.1306587362131893
iteration : 11199
train acc:  0.734375
train loss:  0.4761108160018921
train gradient:  0.11368273009500868
iteration : 11200
train acc:  0.8125
train loss:  0.47281137108802795
train gradient:  0.14358113909136233
iteration : 11201
train acc:  0.7109375
train loss:  0.4730667471885681
train gradient:  0.11642241689881003
iteration : 11202
train acc:  0.75
train loss:  0.4684532880783081
train gradient:  0.11965725009083865
iteration : 11203
train acc:  0.703125
train loss:  0.5454187393188477
train gradient:  0.12454280875986393
iteration : 11204
train acc:  0.8203125
train loss:  0.4148581922054291
train gradient:  0.11983432122912757
iteration : 11205
train acc:  0.75
train loss:  0.4523746967315674
train gradient:  0.09693090110326874
iteration : 11206
train acc:  0.78125
train loss:  0.4632267951965332
train gradient:  0.1176523277407177
iteration : 11207
train acc:  0.6953125
train loss:  0.49709099531173706
train gradient:  0.10079037031203696
iteration : 11208
train acc:  0.78125
train loss:  0.46609607338905334
train gradient:  0.11916449347214879
iteration : 11209
train acc:  0.7421875
train loss:  0.5080094933509827
train gradient:  0.11541283649090563
iteration : 11210
train acc:  0.71875
train loss:  0.4785541296005249
train gradient:  0.12098226049370761
iteration : 11211
train acc:  0.75
train loss:  0.44826504588127136
train gradient:  0.09537966521710067
iteration : 11212
train acc:  0.703125
train loss:  0.5253219604492188
train gradient:  0.1326948595761668
iteration : 11213
train acc:  0.7265625
train loss:  0.5276447534561157
train gradient:  0.15645760975708212
iteration : 11214
train acc:  0.703125
train loss:  0.5160328149795532
train gradient:  0.13484892835663037
iteration : 11215
train acc:  0.765625
train loss:  0.46218305826187134
train gradient:  0.11068906534651578
iteration : 11216
train acc:  0.796875
train loss:  0.46063724160194397
train gradient:  0.1074628206733357
iteration : 11217
train acc:  0.75
train loss:  0.4360632598400116
train gradient:  0.09341084401124562
iteration : 11218
train acc:  0.6875
train loss:  0.48951512575149536
train gradient:  0.1279038819394227
iteration : 11219
train acc:  0.734375
train loss:  0.5189002752304077
train gradient:  0.12124097364361429
iteration : 11220
train acc:  0.7109375
train loss:  0.5468090772628784
train gradient:  0.13043438374381366
iteration : 11221
train acc:  0.7421875
train loss:  0.4982048273086548
train gradient:  0.15402884050651475
iteration : 11222
train acc:  0.703125
train loss:  0.49770158529281616
train gradient:  0.1541255091533367
iteration : 11223
train acc:  0.7265625
train loss:  0.48598575592041016
train gradient:  0.10636533545975822
iteration : 11224
train acc:  0.7109375
train loss:  0.489183634519577
train gradient:  0.11021069240689015
iteration : 11225
train acc:  0.65625
train loss:  0.5358491539955139
train gradient:  0.14257202293337473
iteration : 11226
train acc:  0.7109375
train loss:  0.542885422706604
train gradient:  0.16678408387079546
iteration : 11227
train acc:  0.7578125
train loss:  0.4945296049118042
train gradient:  0.14259818144897943
iteration : 11228
train acc:  0.6953125
train loss:  0.5958200097084045
train gradient:  0.17572356681999118
iteration : 11229
train acc:  0.765625
train loss:  0.46009430289268494
train gradient:  0.10986487876434034
iteration : 11230
train acc:  0.6953125
train loss:  0.49477487802505493
train gradient:  0.12227511742433589
iteration : 11231
train acc:  0.8046875
train loss:  0.37908777594566345
train gradient:  0.07528217902467017
iteration : 11232
train acc:  0.7265625
train loss:  0.49294793605804443
train gradient:  0.14091877739364483
iteration : 11233
train acc:  0.703125
train loss:  0.5287448167800903
train gradient:  0.15616528859702922
iteration : 11234
train acc:  0.7890625
train loss:  0.43373745679855347
train gradient:  0.07749309416338415
iteration : 11235
train acc:  0.734375
train loss:  0.46654894948005676
train gradient:  0.12589950814764622
iteration : 11236
train acc:  0.7578125
train loss:  0.467179536819458
train gradient:  0.1357754974930741
iteration : 11237
train acc:  0.71875
train loss:  0.6100231409072876
train gradient:  0.1757568327422151
iteration : 11238
train acc:  0.7265625
train loss:  0.5194258689880371
train gradient:  0.13475461348862655
iteration : 11239
train acc:  0.7578125
train loss:  0.510764479637146
train gradient:  0.15031839096855143
iteration : 11240
train acc:  0.6875
train loss:  0.5935747623443604
train gradient:  0.16219531636110213
iteration : 11241
train acc:  0.7421875
train loss:  0.4835212528705597
train gradient:  0.1321918099728554
iteration : 11242
train acc:  0.7265625
train loss:  0.45655563473701477
train gradient:  0.11068664320618353
iteration : 11243
train acc:  0.78125
train loss:  0.47320353984832764
train gradient:  0.14865316368793952
iteration : 11244
train acc:  0.734375
train loss:  0.477139949798584
train gradient:  0.11012779436513154
iteration : 11245
train acc:  0.7265625
train loss:  0.5073994994163513
train gradient:  0.13305371216673123
iteration : 11246
train acc:  0.75
train loss:  0.4682040810585022
train gradient:  0.08769650898357241
iteration : 11247
train acc:  0.7421875
train loss:  0.4942063093185425
train gradient:  0.1482676252170501
iteration : 11248
train acc:  0.734375
train loss:  0.4806264638900757
train gradient:  0.11393830116791803
iteration : 11249
train acc:  0.7578125
train loss:  0.4828391671180725
train gradient:  0.11529701083866653
iteration : 11250
train acc:  0.734375
train loss:  0.5119130611419678
train gradient:  0.11509160784027189
iteration : 11251
train acc:  0.7890625
train loss:  0.4882103502750397
train gradient:  0.11800042457221448
iteration : 11252
train acc:  0.734375
train loss:  0.4971851408481598
train gradient:  0.14187409298247616
iteration : 11253
train acc:  0.71875
train loss:  0.5103600025177002
train gradient:  0.14781858153579774
iteration : 11254
train acc:  0.8125
train loss:  0.4299775958061218
train gradient:  0.10898643767595192
iteration : 11255
train acc:  0.6796875
train loss:  0.5946705341339111
train gradient:  0.17637418972762234
iteration : 11256
train acc:  0.7109375
train loss:  0.5075199604034424
train gradient:  0.126431997918431
iteration : 11257
train acc:  0.71875
train loss:  0.4509146213531494
train gradient:  0.12499345563515489
iteration : 11258
train acc:  0.703125
train loss:  0.5307958126068115
train gradient:  0.12110596573159885
iteration : 11259
train acc:  0.6796875
train loss:  0.5855574011802673
train gradient:  0.15530843050311793
iteration : 11260
train acc:  0.734375
train loss:  0.4543060064315796
train gradient:  0.0961235897674516
iteration : 11261
train acc:  0.75
train loss:  0.4656994342803955
train gradient:  0.09541519587163252
iteration : 11262
train acc:  0.8203125
train loss:  0.42647072672843933
train gradient:  0.11810615044490268
iteration : 11263
train acc:  0.7734375
train loss:  0.47374942898750305
train gradient:  0.12927646100044415
iteration : 11264
train acc:  0.78125
train loss:  0.46328771114349365
train gradient:  0.14779029582306202
iteration : 11265
train acc:  0.734375
train loss:  0.5351206064224243
train gradient:  0.15663083739667033
iteration : 11266
train acc:  0.765625
train loss:  0.4837932288646698
train gradient:  0.1316939616022065
iteration : 11267
train acc:  0.8125
train loss:  0.4170510768890381
train gradient:  0.13205683159711396
iteration : 11268
train acc:  0.75
train loss:  0.4945182800292969
train gradient:  0.11801354972751822
iteration : 11269
train acc:  0.7578125
train loss:  0.4896795451641083
train gradient:  0.11145757350966151
iteration : 11270
train acc:  0.765625
train loss:  0.45753318071365356
train gradient:  0.13859639179371028
iteration : 11271
train acc:  0.78125
train loss:  0.48796477913856506
train gradient:  0.1173200476448616
iteration : 11272
train acc:  0.7421875
train loss:  0.4393446147441864
train gradient:  0.118654406566514
iteration : 11273
train acc:  0.7734375
train loss:  0.4712919592857361
train gradient:  0.11097956755915746
iteration : 11274
train acc:  0.6875
train loss:  0.5284349918365479
train gradient:  0.12776590008889038
iteration : 11275
train acc:  0.78125
train loss:  0.45755207538604736
train gradient:  0.10702273077333553
iteration : 11276
train acc:  0.796875
train loss:  0.41749250888824463
train gradient:  0.12249570088043178
iteration : 11277
train acc:  0.71875
train loss:  0.49191245436668396
train gradient:  0.11773602726767995
iteration : 11278
train acc:  0.7734375
train loss:  0.4416091740131378
train gradient:  0.11483943319524366
iteration : 11279
train acc:  0.7578125
train loss:  0.4505764842033386
train gradient:  0.11262781194156299
iteration : 11280
train acc:  0.7265625
train loss:  0.5330841541290283
train gradient:  0.16619882563681598
iteration : 11281
train acc:  0.765625
train loss:  0.4497417211532593
train gradient:  0.12588551160912917
iteration : 11282
train acc:  0.75
train loss:  0.5128399729728699
train gradient:  0.14591972566609157
iteration : 11283
train acc:  0.7578125
train loss:  0.5252333879470825
train gradient:  0.15863073830748556
iteration : 11284
train acc:  0.765625
train loss:  0.4754786491394043
train gradient:  0.15469309958456165
iteration : 11285
train acc:  0.7421875
train loss:  0.5006839036941528
train gradient:  0.1307639767450516
iteration : 11286
train acc:  0.703125
train loss:  0.5017876029014587
train gradient:  0.11070934247452319
iteration : 11287
train acc:  0.7578125
train loss:  0.49583297967910767
train gradient:  0.11156359012926033
iteration : 11288
train acc:  0.7734375
train loss:  0.4616544842720032
train gradient:  0.09992844292891385
iteration : 11289
train acc:  0.7890625
train loss:  0.4455890953540802
train gradient:  0.11152884311015199
iteration : 11290
train acc:  0.765625
train loss:  0.48036032915115356
train gradient:  0.11944258948309336
iteration : 11291
train acc:  0.7578125
train loss:  0.4634206295013428
train gradient:  0.12522049613054184
iteration : 11292
train acc:  0.765625
train loss:  0.4607366919517517
train gradient:  0.14896854420734892
iteration : 11293
train acc:  0.7734375
train loss:  0.47860902547836304
train gradient:  0.1076914989645609
iteration : 11294
train acc:  0.765625
train loss:  0.4649563133716583
train gradient:  0.1037700648115752
iteration : 11295
train acc:  0.734375
train loss:  0.49738675355911255
train gradient:  0.12125369845252688
iteration : 11296
train acc:  0.7421875
train loss:  0.5010611414909363
train gradient:  0.1351651019196985
iteration : 11297
train acc:  0.7578125
train loss:  0.5072548389434814
train gradient:  0.1713302494877281
iteration : 11298
train acc:  0.7265625
train loss:  0.5469256639480591
train gradient:  0.1808536529461322
iteration : 11299
train acc:  0.703125
train loss:  0.5468331575393677
train gradient:  0.18251246164045168
iteration : 11300
train acc:  0.7265625
train loss:  0.49665969610214233
train gradient:  0.13043435347093413
iteration : 11301
train acc:  0.7421875
train loss:  0.5100829601287842
train gradient:  0.10654370856475055
iteration : 11302
train acc:  0.8046875
train loss:  0.4560472369194031
train gradient:  0.10721860494165247
iteration : 11303
train acc:  0.78125
train loss:  0.4835316240787506
train gradient:  0.12511058956664775
iteration : 11304
train acc:  0.78125
train loss:  0.43445050716400146
train gradient:  0.08464991519186899
iteration : 11305
train acc:  0.6875
train loss:  0.660872220993042
train gradient:  0.20516338935253436
iteration : 11306
train acc:  0.703125
train loss:  0.5567607879638672
train gradient:  0.15129273807316812
iteration : 11307
train acc:  0.7265625
train loss:  0.46896010637283325
train gradient:  0.11464868610605602
iteration : 11308
train acc:  0.7734375
train loss:  0.5159248113632202
train gradient:  0.14892734040038824
iteration : 11309
train acc:  0.71875
train loss:  0.48687684535980225
train gradient:  0.1285049541529797
iteration : 11310
train acc:  0.7578125
train loss:  0.46910810470581055
train gradient:  0.12637046967222154
iteration : 11311
train acc:  0.765625
train loss:  0.5447700023651123
train gradient:  0.18028818044255462
iteration : 11312
train acc:  0.7734375
train loss:  0.5000627636909485
train gradient:  0.12336588838111155
iteration : 11313
train acc:  0.765625
train loss:  0.4909110963344574
train gradient:  0.11606247559709573
iteration : 11314
train acc:  0.7734375
train loss:  0.4537680149078369
train gradient:  0.13471991880524137
iteration : 11315
train acc:  0.6953125
train loss:  0.5756674408912659
train gradient:  0.1511098943705561
iteration : 11316
train acc:  0.7265625
train loss:  0.5177403688430786
train gradient:  0.1728654794423541
iteration : 11317
train acc:  0.765625
train loss:  0.4168130159378052
train gradient:  0.1040466869932311
iteration : 11318
train acc:  0.7578125
train loss:  0.45988523960113525
train gradient:  0.11754020483311468
iteration : 11319
train acc:  0.6953125
train loss:  0.5274989008903503
train gradient:  0.15587332086267508
iteration : 11320
train acc:  0.7578125
train loss:  0.5155413150787354
train gradient:  0.15045812291574479
iteration : 11321
train acc:  0.7890625
train loss:  0.46605756878852844
train gradient:  0.1011610586572562
iteration : 11322
train acc:  0.75
train loss:  0.49945732951164246
train gradient:  0.14263892765444192
iteration : 11323
train acc:  0.765625
train loss:  0.46106863021850586
train gradient:  0.11627925586385865
iteration : 11324
train acc:  0.7109375
train loss:  0.4954991042613983
train gradient:  0.1483035005343855
iteration : 11325
train acc:  0.7734375
train loss:  0.49982744455337524
train gradient:  0.1308954631258437
iteration : 11326
train acc:  0.796875
train loss:  0.4350859522819519
train gradient:  0.11903014788714221
iteration : 11327
train acc:  0.7109375
train loss:  0.520962119102478
train gradient:  0.12561552865012993
iteration : 11328
train acc:  0.8046875
train loss:  0.4675690233707428
train gradient:  0.12875940619062803
iteration : 11329
train acc:  0.765625
train loss:  0.5506893396377563
train gradient:  0.12266190173034285
iteration : 11330
train acc:  0.6953125
train loss:  0.507392942905426
train gradient:  0.1282777063144963
iteration : 11331
train acc:  0.734375
train loss:  0.48187339305877686
train gradient:  0.12042411033291166
iteration : 11332
train acc:  0.7109375
train loss:  0.5267581343650818
train gradient:  0.12384984490075478
iteration : 11333
train acc:  0.6796875
train loss:  0.57109534740448
train gradient:  0.227060303108351
iteration : 11334
train acc:  0.7734375
train loss:  0.4821298122406006
train gradient:  0.1266458329131273
iteration : 11335
train acc:  0.7578125
train loss:  0.46485793590545654
train gradient:  0.10584005542635257
iteration : 11336
train acc:  0.765625
train loss:  0.44204530119895935
train gradient:  0.12141507566357819
iteration : 11337
train acc:  0.7734375
train loss:  0.47870293259620667
train gradient:  0.10788837726096714
iteration : 11338
train acc:  0.703125
train loss:  0.5297694206237793
train gradient:  0.14674249443240417
iteration : 11339
train acc:  0.8203125
train loss:  0.4028055965900421
train gradient:  0.09897705999667278
iteration : 11340
train acc:  0.7734375
train loss:  0.4847158193588257
train gradient:  0.1078446899875595
iteration : 11341
train acc:  0.8203125
train loss:  0.41734030842781067
train gradient:  0.10311634229919833
iteration : 11342
train acc:  0.7421875
train loss:  0.5064635872840881
train gradient:  0.13104908516303312
iteration : 11343
train acc:  0.7890625
train loss:  0.4352092742919922
train gradient:  0.11757675021080781
iteration : 11344
train acc:  0.71875
train loss:  0.4875488579273224
train gradient:  0.1322115032889964
iteration : 11345
train acc:  0.7265625
train loss:  0.5010703802108765
train gradient:  0.1632711847474482
iteration : 11346
train acc:  0.75
train loss:  0.47590577602386475
train gradient:  0.14784230284240846
iteration : 11347
train acc:  0.75
train loss:  0.5028478503227234
train gradient:  0.13438922992633545
iteration : 11348
train acc:  0.7578125
train loss:  0.48054707050323486
train gradient:  0.11573956183218091
iteration : 11349
train acc:  0.7734375
train loss:  0.4423937201499939
train gradient:  0.10091774670232886
iteration : 11350
train acc:  0.75
train loss:  0.48837077617645264
train gradient:  0.15378881348843315
iteration : 11351
train acc:  0.765625
train loss:  0.45165586471557617
train gradient:  0.1260290987661994
iteration : 11352
train acc:  0.7578125
train loss:  0.4748336970806122
train gradient:  0.11087687933013055
iteration : 11353
train acc:  0.6875
train loss:  0.5333269238471985
train gradient:  0.12572382910687524
iteration : 11354
train acc:  0.796875
train loss:  0.4374748766422272
train gradient:  0.1279738861631508
iteration : 11355
train acc:  0.6328125
train loss:  0.5863491296768188
train gradient:  0.19561049077621584
iteration : 11356
train acc:  0.75
train loss:  0.5411797165870667
train gradient:  0.16480995379103053
iteration : 11357
train acc:  0.765625
train loss:  0.46182677149772644
train gradient:  0.127460090759052
iteration : 11358
train acc:  0.7265625
train loss:  0.4656088054180145
train gradient:  0.11105682938081876
iteration : 11359
train acc:  0.796875
train loss:  0.44721508026123047
train gradient:  0.12042110889117977
iteration : 11360
train acc:  0.8046875
train loss:  0.42756396532058716
train gradient:  0.09703356494095434
iteration : 11361
train acc:  0.7890625
train loss:  0.4583248496055603
train gradient:  0.10016769041330148
iteration : 11362
train acc:  0.7890625
train loss:  0.4904516637325287
train gradient:  0.13545949894371673
iteration : 11363
train acc:  0.78125
train loss:  0.4696981906890869
train gradient:  0.11056139629468616
iteration : 11364
train acc:  0.78125
train loss:  0.4557829797267914
train gradient:  0.1416979777034807
iteration : 11365
train acc:  0.7734375
train loss:  0.5177919268608093
train gradient:  0.1519706113979286
iteration : 11366
train acc:  0.7421875
train loss:  0.5148047208786011
train gradient:  0.14169134064588748
iteration : 11367
train acc:  0.7890625
train loss:  0.46189430356025696
train gradient:  0.11740213981237266
iteration : 11368
train acc:  0.7578125
train loss:  0.45538604259490967
train gradient:  0.12780720341450683
iteration : 11369
train acc:  0.7734375
train loss:  0.4489107131958008
train gradient:  0.09981524968472467
iteration : 11370
train acc:  0.7578125
train loss:  0.45491933822631836
train gradient:  0.09337484639604125
iteration : 11371
train acc:  0.6875
train loss:  0.5345462560653687
train gradient:  0.16032319204556267
iteration : 11372
train acc:  0.703125
train loss:  0.5518772006034851
train gradient:  0.15921244429349912
iteration : 11373
train acc:  0.6796875
train loss:  0.6212419867515564
train gradient:  0.2156400096236003
iteration : 11374
train acc:  0.765625
train loss:  0.47232598066329956
train gradient:  0.12289459058330117
iteration : 11375
train acc:  0.7734375
train loss:  0.4898505210876465
train gradient:  0.1162152986894632
iteration : 11376
train acc:  0.796875
train loss:  0.413628488779068
train gradient:  0.08251053778971083
iteration : 11377
train acc:  0.765625
train loss:  0.442105770111084
train gradient:  0.0881764237064718
iteration : 11378
train acc:  0.734375
train loss:  0.5003237724304199
train gradient:  0.13344523612219508
iteration : 11379
train acc:  0.75
train loss:  0.48894503712654114
train gradient:  0.12923084353966036
iteration : 11380
train acc:  0.7421875
train loss:  0.5331234931945801
train gradient:  0.13894617384437513
iteration : 11381
train acc:  0.7890625
train loss:  0.4559631943702698
train gradient:  0.10401347952831254
iteration : 11382
train acc:  0.7734375
train loss:  0.4686625301837921
train gradient:  0.19634622739444063
iteration : 11383
train acc:  0.75
train loss:  0.49805524945259094
train gradient:  0.15302904997491082
iteration : 11384
train acc:  0.703125
train loss:  0.4823163151741028
train gradient:  0.12431269268883363
iteration : 11385
train acc:  0.796875
train loss:  0.4910203218460083
train gradient:  0.12026904533730448
iteration : 11386
train acc:  0.7265625
train loss:  0.515598475933075
train gradient:  0.1455081853258
iteration : 11387
train acc:  0.6875
train loss:  0.515364408493042
train gradient:  0.1269018461385505
iteration : 11388
train acc:  0.765625
train loss:  0.4783673882484436
train gradient:  0.13948398990936384
iteration : 11389
train acc:  0.734375
train loss:  0.4840468168258667
train gradient:  0.1426330054874129
iteration : 11390
train acc:  0.7265625
train loss:  0.5247353315353394
train gradient:  0.16210286116237893
iteration : 11391
train acc:  0.8046875
train loss:  0.4298723042011261
train gradient:  0.08831328703044324
iteration : 11392
train acc:  0.6875
train loss:  0.4897937774658203
train gradient:  0.1497771305291894
iteration : 11393
train acc:  0.796875
train loss:  0.43006330728530884
train gradient:  0.10599143148218758
iteration : 11394
train acc:  0.71875
train loss:  0.5808200836181641
train gradient:  0.21703227049477045
iteration : 11395
train acc:  0.7734375
train loss:  0.41911572217941284
train gradient:  0.09192104991118434
iteration : 11396
train acc:  0.71875
train loss:  0.48054587841033936
train gradient:  0.15845541356272025
iteration : 11397
train acc:  0.734375
train loss:  0.41761380434036255
train gradient:  0.08483748180476369
iteration : 11398
train acc:  0.7578125
train loss:  0.5200051069259644
train gradient:  0.13665365064583096
iteration : 11399
train acc:  0.6875
train loss:  0.504429280757904
train gradient:  0.13889807132618837
iteration : 11400
train acc:  0.765625
train loss:  0.4229891896247864
train gradient:  0.09554980597900639
iteration : 11401
train acc:  0.734375
train loss:  0.5622406005859375
train gradient:  0.17390336782974342
iteration : 11402
train acc:  0.7421875
train loss:  0.5008460879325867
train gradient:  0.13625675094226286
iteration : 11403
train acc:  0.8125
train loss:  0.4599531590938568
train gradient:  0.10620216440184213
iteration : 11404
train acc:  0.7578125
train loss:  0.4837483763694763
train gradient:  0.13951137322205226
iteration : 11405
train acc:  0.7421875
train loss:  0.4948720335960388
train gradient:  0.11983350373188215
iteration : 11406
train acc:  0.640625
train loss:  0.6063930988311768
train gradient:  0.16125587646537354
iteration : 11407
train acc:  0.734375
train loss:  0.5091335773468018
train gradient:  0.17112398349774696
iteration : 11408
train acc:  0.7734375
train loss:  0.47812288999557495
train gradient:  0.10362281263951931
iteration : 11409
train acc:  0.703125
train loss:  0.5189163684844971
train gradient:  0.1589675045182916
iteration : 11410
train acc:  0.7890625
train loss:  0.3932803273200989
train gradient:  0.08159472843559669
iteration : 11411
train acc:  0.8046875
train loss:  0.4654141366481781
train gradient:  0.11013637246381255
iteration : 11412
train acc:  0.796875
train loss:  0.4545213282108307
train gradient:  0.10374488094981178
iteration : 11413
train acc:  0.828125
train loss:  0.42354077100753784
train gradient:  0.10228108697341765
iteration : 11414
train acc:  0.75
train loss:  0.47323495149612427
train gradient:  0.1194146617102371
iteration : 11415
train acc:  0.7421875
train loss:  0.4806786775588989
train gradient:  0.11859201723386485
iteration : 11416
train acc:  0.671875
train loss:  0.5359176397323608
train gradient:  0.10751558933339403
iteration : 11417
train acc:  0.7265625
train loss:  0.5826746821403503
train gradient:  0.17183428052662492
iteration : 11418
train acc:  0.7265625
train loss:  0.5030192136764526
train gradient:  0.12851699429865523
iteration : 11419
train acc:  0.765625
train loss:  0.476719468832016
train gradient:  0.10117996896141791
iteration : 11420
train acc:  0.7578125
train loss:  0.45962053537368774
train gradient:  0.11804212940389094
iteration : 11421
train acc:  0.8203125
train loss:  0.4309164881706238
train gradient:  0.1499917089423009
iteration : 11422
train acc:  0.7265625
train loss:  0.5317890644073486
train gradient:  0.16162005204939134
iteration : 11423
train acc:  0.8125
train loss:  0.4549565613269806
train gradient:  0.11172051009747264
iteration : 11424
train acc:  0.6875
train loss:  0.5195348262786865
train gradient:  0.14910317706399268
iteration : 11425
train acc:  0.75
train loss:  0.43485233187675476
train gradient:  0.10138222051496128
iteration : 11426
train acc:  0.7578125
train loss:  0.5075074434280396
train gradient:  0.16039433828951505
iteration : 11427
train acc:  0.7109375
train loss:  0.492775559425354
train gradient:  0.11822811733799515
iteration : 11428
train acc:  0.7109375
train loss:  0.5022754073143005
train gradient:  0.13138724182876393
iteration : 11429
train acc:  0.6875
train loss:  0.5426430702209473
train gradient:  0.15792345174995268
iteration : 11430
train acc:  0.71875
train loss:  0.49826952815055847
train gradient:  0.11467775054363603
iteration : 11431
train acc:  0.7421875
train loss:  0.4927787780761719
train gradient:  0.13108169314980678
iteration : 11432
train acc:  0.734375
train loss:  0.5273381471633911
train gradient:  0.19346216654190695
iteration : 11433
train acc:  0.6796875
train loss:  0.5522341132164001
train gradient:  0.16355805001663132
iteration : 11434
train acc:  0.765625
train loss:  0.47654834389686584
train gradient:  0.09863981261831868
iteration : 11435
train acc:  0.734375
train loss:  0.4835043251514435
train gradient:  0.10197604330539713
iteration : 11436
train acc:  0.7578125
train loss:  0.46918803453445435
train gradient:  0.12369260123373567
iteration : 11437
train acc:  0.7890625
train loss:  0.4653988778591156
train gradient:  0.104245910048031
iteration : 11438
train acc:  0.7109375
train loss:  0.4975445866584778
train gradient:  0.1381081004731783
iteration : 11439
train acc:  0.75
train loss:  0.4767490029335022
train gradient:  0.12097197637431369
iteration : 11440
train acc:  0.8046875
train loss:  0.47165045142173767
train gradient:  0.10809322036246526
iteration : 11441
train acc:  0.765625
train loss:  0.448350191116333
train gradient:  0.11753443703948062
iteration : 11442
train acc:  0.703125
train loss:  0.5233442187309265
train gradient:  0.1423606785542575
iteration : 11443
train acc:  0.7421875
train loss:  0.4767864942550659
train gradient:  0.12622008676530078
iteration : 11444
train acc:  0.6875
train loss:  0.5499230027198792
train gradient:  0.1767963279119909
iteration : 11445
train acc:  0.71875
train loss:  0.579336404800415
train gradient:  0.18846050275424056
iteration : 11446
train acc:  0.6953125
train loss:  0.5448720455169678
train gradient:  0.1812886925770944
iteration : 11447
train acc:  0.734375
train loss:  0.5072487592697144
train gradient:  0.1276214178790399
iteration : 11448
train acc:  0.7734375
train loss:  0.5062972903251648
train gradient:  0.14658554546899644
iteration : 11449
train acc:  0.7421875
train loss:  0.49318450689315796
train gradient:  0.15134037760471708
iteration : 11450
train acc:  0.71875
train loss:  0.5528399348258972
train gradient:  0.1639855954962734
iteration : 11451
train acc:  0.734375
train loss:  0.5165066719055176
train gradient:  0.11820164796797068
iteration : 11452
train acc:  0.75
train loss:  0.4845113754272461
train gradient:  0.14507057785935767
iteration : 11453
train acc:  0.78125
train loss:  0.48618772625923157
train gradient:  0.1122096622400694
iteration : 11454
train acc:  0.7421875
train loss:  0.43805021047592163
train gradient:  0.0927228002846272
iteration : 11455
train acc:  0.7734375
train loss:  0.43326300382614136
train gradient:  0.10246563100614448
iteration : 11456
train acc:  0.734375
train loss:  0.511083722114563
train gradient:  0.21222074762022505
iteration : 11457
train acc:  0.7421875
train loss:  0.508339524269104
train gradient:  0.15491539410171284
iteration : 11458
train acc:  0.8125
train loss:  0.4147593677043915
train gradient:  0.1105357203007803
iteration : 11459
train acc:  0.7421875
train loss:  0.4695858359336853
train gradient:  0.14159653903597313
iteration : 11460
train acc:  0.6796875
train loss:  0.5521872043609619
train gradient:  0.14165480999094054
iteration : 11461
train acc:  0.734375
train loss:  0.5435891151428223
train gradient:  0.14015873948234675
iteration : 11462
train acc:  0.7578125
train loss:  0.5039217472076416
train gradient:  0.15872825833488252
iteration : 11463
train acc:  0.7578125
train loss:  0.4702048897743225
train gradient:  0.1322406064265229
iteration : 11464
train acc:  0.765625
train loss:  0.5341947078704834
train gradient:  0.15464234716947053
iteration : 11465
train acc:  0.75
train loss:  0.48751187324523926
train gradient:  0.12842327293823155
iteration : 11466
train acc:  0.734375
train loss:  0.5255454778671265
train gradient:  0.1755369552878846
iteration : 11467
train acc:  0.7421875
train loss:  0.49221065640449524
train gradient:  0.122592129446721
iteration : 11468
train acc:  0.7578125
train loss:  0.4993603229522705
train gradient:  0.15420327811741813
iteration : 11469
train acc:  0.7265625
train loss:  0.5056362152099609
train gradient:  0.14506705643603462
iteration : 11470
train acc:  0.7265625
train loss:  0.5090869069099426
train gradient:  0.13878042865827675
iteration : 11471
train acc:  0.6875
train loss:  0.519981324672699
train gradient:  0.14683127847568828
iteration : 11472
train acc:  0.8046875
train loss:  0.4809224009513855
train gradient:  0.12706155225588622
iteration : 11473
train acc:  0.703125
train loss:  0.6293870210647583
train gradient:  0.20939076117059344
iteration : 11474
train acc:  0.734375
train loss:  0.5086237788200378
train gradient:  0.1425350481562281
iteration : 11475
train acc:  0.7734375
train loss:  0.4701302945613861
train gradient:  0.1147801356570673
iteration : 11476
train acc:  0.71875
train loss:  0.5329362154006958
train gradient:  0.17127587672484934
iteration : 11477
train acc:  0.703125
train loss:  0.533379852771759
train gradient:  0.16151893686579588
iteration : 11478
train acc:  0.734375
train loss:  0.4758477807044983
train gradient:  0.13317506282313563
iteration : 11479
train acc:  0.7109375
train loss:  0.5005080699920654
train gradient:  0.15647148801661506
iteration : 11480
train acc:  0.7421875
train loss:  0.4419308304786682
train gradient:  0.10550720324141175
iteration : 11481
train acc:  0.75
train loss:  0.44262051582336426
train gradient:  0.0991390632719486
iteration : 11482
train acc:  0.7578125
train loss:  0.4397788643836975
train gradient:  0.13094089773107834
iteration : 11483
train acc:  0.765625
train loss:  0.49000227451324463
train gradient:  0.12209761041174694
iteration : 11484
train acc:  0.7578125
train loss:  0.5031214952468872
train gradient:  0.11774631488279315
iteration : 11485
train acc:  0.7265625
train loss:  0.49993693828582764
train gradient:  0.13518615950078447
iteration : 11486
train acc:  0.8046875
train loss:  0.423067182302475
train gradient:  0.11761614213504892
iteration : 11487
train acc:  0.71875
train loss:  0.47161102294921875
train gradient:  0.1117549401634556
iteration : 11488
train acc:  0.7578125
train loss:  0.447959303855896
train gradient:  0.11156573567100093
iteration : 11489
train acc:  0.7421875
train loss:  0.512254536151886
train gradient:  0.12471953490711166
iteration : 11490
train acc:  0.6953125
train loss:  0.5469415783882141
train gradient:  0.16504993461305478
iteration : 11491
train acc:  0.7421875
train loss:  0.5613725185394287
train gradient:  0.1515980295664648
iteration : 11492
train acc:  0.75
train loss:  0.46005603671073914
train gradient:  0.11646586439046842
iteration : 11493
train acc:  0.8046875
train loss:  0.4254302978515625
train gradient:  0.12236184412465799
iteration : 11494
train acc:  0.671875
train loss:  0.6366889476776123
train gradient:  0.259366519625917
iteration : 11495
train acc:  0.7421875
train loss:  0.5272859334945679
train gradient:  0.1743666279499056
iteration : 11496
train acc:  0.6328125
train loss:  0.5749253630638123
train gradient:  0.16624730512391384
iteration : 11497
train acc:  0.7265625
train loss:  0.5007848739624023
train gradient:  0.13255016940865846
iteration : 11498
train acc:  0.71875
train loss:  0.527741551399231
train gradient:  0.13939258966220863
iteration : 11499
train acc:  0.7890625
train loss:  0.4641563296318054
train gradient:  0.10104796981082838
iteration : 11500
train acc:  0.671875
train loss:  0.5111984610557556
train gradient:  0.14977369278874741
iteration : 11501
train acc:  0.734375
train loss:  0.5221337080001831
train gradient:  0.12088397078671252
iteration : 11502
train acc:  0.765625
train loss:  0.5044870376586914
train gradient:  0.17143606858851357
iteration : 11503
train acc:  0.765625
train loss:  0.5006691217422485
train gradient:  0.12244881224111555
iteration : 11504
train acc:  0.75
train loss:  0.5552961826324463
train gradient:  0.1549575910950653
iteration : 11505
train acc:  0.7265625
train loss:  0.5472615957260132
train gradient:  0.14535227504207707
iteration : 11506
train acc:  0.7734375
train loss:  0.4671569764614105
train gradient:  0.10754666165256066
iteration : 11507
train acc:  0.75
train loss:  0.46792101860046387
train gradient:  0.12945039136577996
iteration : 11508
train acc:  0.7421875
train loss:  0.4554097652435303
train gradient:  0.11909844006273684
iteration : 11509
train acc:  0.7421875
train loss:  0.471139520406723
train gradient:  0.1281619003780408
iteration : 11510
train acc:  0.734375
train loss:  0.4673326909542084
train gradient:  0.10916529377735221
iteration : 11511
train acc:  0.7578125
train loss:  0.49229758977890015
train gradient:  0.12326609065446639
iteration : 11512
train acc:  0.7734375
train loss:  0.5089266300201416
train gradient:  0.1073297812223657
iteration : 11513
train acc:  0.7421875
train loss:  0.5136916637420654
train gradient:  0.129756483907989
iteration : 11514
train acc:  0.7734375
train loss:  0.4727078080177307
train gradient:  0.12753257927779377
iteration : 11515
train acc:  0.8046875
train loss:  0.4758372902870178
train gradient:  0.18891602865291848
iteration : 11516
train acc:  0.75
train loss:  0.49461445212364197
train gradient:  0.1263170731211695
iteration : 11517
train acc:  0.7109375
train loss:  0.5058966875076294
train gradient:  0.11480776700271056
iteration : 11518
train acc:  0.7890625
train loss:  0.42078089714050293
train gradient:  0.08915248982128245
iteration : 11519
train acc:  0.71875
train loss:  0.5428208112716675
train gradient:  0.16772019598988425
iteration : 11520
train acc:  0.7265625
train loss:  0.5068198442459106
train gradient:  0.13386648621802094
iteration : 11521
train acc:  0.8046875
train loss:  0.4600197970867157
train gradient:  0.11645451180117503
iteration : 11522
train acc:  0.796875
train loss:  0.42663758993148804
train gradient:  0.10174600227771548
iteration : 11523
train acc:  0.671875
train loss:  0.5365286469459534
train gradient:  0.1514171516407895
iteration : 11524
train acc:  0.71875
train loss:  0.5163057446479797
train gradient:  0.13798767406363566
iteration : 11525
train acc:  0.734375
train loss:  0.5180184245109558
train gradient:  0.13457322969003255
iteration : 11526
train acc:  0.7109375
train loss:  0.5276132822036743
train gradient:  0.1549815722734239
iteration : 11527
train acc:  0.75
train loss:  0.4861530065536499
train gradient:  0.13652399601923987
iteration : 11528
train acc:  0.7734375
train loss:  0.46969157457351685
train gradient:  0.10788935786202344
iteration : 11529
train acc:  0.75
train loss:  0.4799596071243286
train gradient:  0.14144980861676731
iteration : 11530
train acc:  0.6953125
train loss:  0.5212956070899963
train gradient:  0.12966404840338788
iteration : 11531
train acc:  0.7734375
train loss:  0.4951023459434509
train gradient:  0.1293768038631638
iteration : 11532
train acc:  0.703125
train loss:  0.5707850456237793
train gradient:  0.18376270323572835
iteration : 11533
train acc:  0.7421875
train loss:  0.4599652886390686
train gradient:  0.12234831194583536
iteration : 11534
train acc:  0.6484375
train loss:  0.6219672560691833
train gradient:  0.19748954177086447
iteration : 11535
train acc:  0.796875
train loss:  0.4471742510795593
train gradient:  0.10392011139045809
iteration : 11536
train acc:  0.7421875
train loss:  0.5431656241416931
train gradient:  0.18459059175465697
iteration : 11537
train acc:  0.71875
train loss:  0.5277122259140015
train gradient:  0.15085602765625455
iteration : 11538
train acc:  0.75
train loss:  0.4816513955593109
train gradient:  0.11233709766676692
iteration : 11539
train acc:  0.6796875
train loss:  0.5808373689651489
train gradient:  0.16235520407662413
iteration : 11540
train acc:  0.7578125
train loss:  0.5666367411613464
train gradient:  0.19529243305401317
iteration : 11541
train acc:  0.796875
train loss:  0.4517987072467804
train gradient:  0.09936411380393795
iteration : 11542
train acc:  0.8125
train loss:  0.43117427825927734
train gradient:  0.08985594404800716
iteration : 11543
train acc:  0.640625
train loss:  0.5671306848526001
train gradient:  0.18060505124890536
iteration : 11544
train acc:  0.7109375
train loss:  0.5351243615150452
train gradient:  0.14017184572193997
iteration : 11545
train acc:  0.6796875
train loss:  0.5543630719184875
train gradient:  0.13858550906029304
iteration : 11546
train acc:  0.7421875
train loss:  0.4837924838066101
train gradient:  0.17543913628619173
iteration : 11547
train acc:  0.6796875
train loss:  0.5387898683547974
train gradient:  0.12611879483916266
iteration : 11548
train acc:  0.8046875
train loss:  0.4198455214500427
train gradient:  0.09973284076687679
iteration : 11549
train acc:  0.7421875
train loss:  0.5271751880645752
train gradient:  0.14364111919077027
iteration : 11550
train acc:  0.6640625
train loss:  0.5084414482116699
train gradient:  0.11845206470641519
iteration : 11551
train acc:  0.7109375
train loss:  0.5265493392944336
train gradient:  0.14859184328987715
iteration : 11552
train acc:  0.6484375
train loss:  0.6315120458602905
train gradient:  0.22057088437641925
iteration : 11553
train acc:  0.765625
train loss:  0.4963940382003784
train gradient:  0.11278799380630589
iteration : 11554
train acc:  0.7890625
train loss:  0.4550144672393799
train gradient:  0.16300611473787152
iteration : 11555
train acc:  0.75
train loss:  0.49804002046585083
train gradient:  0.14844552508984116
iteration : 11556
train acc:  0.6875
train loss:  0.5172813534736633
train gradient:  0.16484605131277374
iteration : 11557
train acc:  0.6953125
train loss:  0.5041648149490356
train gradient:  0.11262841034492937
iteration : 11558
train acc:  0.7421875
train loss:  0.4671008586883545
train gradient:  0.12284897500156534
iteration : 11559
train acc:  0.8203125
train loss:  0.4274424910545349
train gradient:  0.11156037042354712
iteration : 11560
train acc:  0.8359375
train loss:  0.38821738958358765
train gradient:  0.08539608771421796
iteration : 11561
train acc:  0.7265625
train loss:  0.5200287699699402
train gradient:  0.11647868657553924
iteration : 11562
train acc:  0.765625
train loss:  0.47405683994293213
train gradient:  0.13110453528161922
iteration : 11563
train acc:  0.7890625
train loss:  0.4210991859436035
train gradient:  0.10495264786156502
iteration : 11564
train acc:  0.734375
train loss:  0.46117472648620605
train gradient:  0.1275333207052063
iteration : 11565
train acc:  0.7734375
train loss:  0.45946651697158813
train gradient:  0.11129071726073489
iteration : 11566
train acc:  0.8203125
train loss:  0.4232800304889679
train gradient:  0.10942988322050319
iteration : 11567
train acc:  0.75
train loss:  0.5021989345550537
train gradient:  0.14509598021998413
iteration : 11568
train acc:  0.796875
train loss:  0.4463973343372345
train gradient:  0.0991883169242179
iteration : 11569
train acc:  0.7578125
train loss:  0.48653358221054077
train gradient:  0.10795418748834865
iteration : 11570
train acc:  0.7265625
train loss:  0.5097692012786865
train gradient:  0.1469112286100719
iteration : 11571
train acc:  0.7109375
train loss:  0.5385216474533081
train gradient:  0.13964061582446655
iteration : 11572
train acc:  0.78125
train loss:  0.43371152877807617
train gradient:  0.11482539077516923
iteration : 11573
train acc:  0.78125
train loss:  0.44422218203544617
train gradient:  0.10245896926977478
iteration : 11574
train acc:  0.7109375
train loss:  0.5118348598480225
train gradient:  0.13139097184406856
iteration : 11575
train acc:  0.765625
train loss:  0.507312536239624
train gradient:  0.1324879284991652
iteration : 11576
train acc:  0.75
train loss:  0.4822305142879486
train gradient:  0.11437495609669464
iteration : 11577
train acc:  0.71875
train loss:  0.48892831802368164
train gradient:  0.10501674882212114
iteration : 11578
train acc:  0.6953125
train loss:  0.5579816102981567
train gradient:  0.2103999884871764
iteration : 11579
train acc:  0.75
train loss:  0.4458615779876709
train gradient:  0.11848482782445635
iteration : 11580
train acc:  0.6953125
train loss:  0.5671720504760742
train gradient:  0.14635526524859901
iteration : 11581
train acc:  0.7578125
train loss:  0.5764667987823486
train gradient:  0.19312038569131657
iteration : 11582
train acc:  0.8046875
train loss:  0.4542970657348633
train gradient:  0.11520907958562293
iteration : 11583
train acc:  0.7578125
train loss:  0.4751430153846741
train gradient:  0.11322450800468017
iteration : 11584
train acc:  0.734375
train loss:  0.5042405128479004
train gradient:  0.12434090184632395
iteration : 11585
train acc:  0.75
train loss:  0.4872153103351593
train gradient:  0.1277387617460948
iteration : 11586
train acc:  0.765625
train loss:  0.48139625787734985
train gradient:  0.13067482606246325
iteration : 11587
train acc:  0.765625
train loss:  0.48351937532424927
train gradient:  0.12238693988967751
iteration : 11588
train acc:  0.734375
train loss:  0.5065212845802307
train gradient:  0.12437777680753075
iteration : 11589
train acc:  0.703125
train loss:  0.507131040096283
train gradient:  0.11461468041595557
iteration : 11590
train acc:  0.8203125
train loss:  0.4521997272968292
train gradient:  0.13299498474765598
iteration : 11591
train acc:  0.7421875
train loss:  0.5227223634719849
train gradient:  0.13413262971713125
iteration : 11592
train acc:  0.7734375
train loss:  0.4539465010166168
train gradient:  0.15494594915109738
iteration : 11593
train acc:  0.7421875
train loss:  0.4913688600063324
train gradient:  0.12332477054626612
iteration : 11594
train acc:  0.8359375
train loss:  0.4428805708885193
train gradient:  0.10560368681251366
iteration : 11595
train acc:  0.78125
train loss:  0.4175669550895691
train gradient:  0.10274967707758693
iteration : 11596
train acc:  0.78125
train loss:  0.46037930250167847
train gradient:  0.1340501183528346
iteration : 11597
train acc:  0.7890625
train loss:  0.43657925724983215
train gradient:  0.10217954995663128
iteration : 11598
train acc:  0.7578125
train loss:  0.44184058904647827
train gradient:  0.1058720914615661
iteration : 11599
train acc:  0.71875
train loss:  0.49575185775756836
train gradient:  0.13836240029541014
iteration : 11600
train acc:  0.7578125
train loss:  0.45864760875701904
train gradient:  0.10804441205804273
iteration : 11601
train acc:  0.7578125
train loss:  0.47467008233070374
train gradient:  0.12811755517407036
iteration : 11602
train acc:  0.765625
train loss:  0.47502297163009644
train gradient:  0.11179252791757448
iteration : 11603
train acc:  0.6875
train loss:  0.5470740795135498
train gradient:  0.16317547066113813
iteration : 11604
train acc:  0.6015625
train loss:  0.6179424524307251
train gradient:  0.17123303578457616
iteration : 11605
train acc:  0.765625
train loss:  0.4524982273578644
train gradient:  0.10572426288893762
iteration : 11606
train acc:  0.6640625
train loss:  0.5512983202934265
train gradient:  0.16963327255154498
iteration : 11607
train acc:  0.7578125
train loss:  0.5092993974685669
train gradient:  0.1393603847689993
iteration : 11608
train acc:  0.765625
train loss:  0.44061970710754395
train gradient:  0.09550150124790979
iteration : 11609
train acc:  0.734375
train loss:  0.5708276629447937
train gradient:  0.1551299082165945
iteration : 11610
train acc:  0.75
train loss:  0.4553975462913513
train gradient:  0.13510677327631168
iteration : 11611
train acc:  0.65625
train loss:  0.586029589176178
train gradient:  0.15904555735083276
iteration : 11612
train acc:  0.796875
train loss:  0.44690239429473877
train gradient:  0.1144072955488244
iteration : 11613
train acc:  0.8046875
train loss:  0.4587854743003845
train gradient:  0.1175894394177216
iteration : 11614
train acc:  0.7421875
train loss:  0.49306392669677734
train gradient:  0.12331846592429532
iteration : 11615
train acc:  0.7890625
train loss:  0.4294934868812561
train gradient:  0.12211592365557605
iteration : 11616
train acc:  0.7890625
train loss:  0.4624878168106079
train gradient:  0.11259320100353733
iteration : 11617
train acc:  0.78125
train loss:  0.4905114769935608
train gradient:  0.11661588002558086
iteration : 11618
train acc:  0.65625
train loss:  0.5941108465194702
train gradient:  0.17718636028119347
iteration : 11619
train acc:  0.8046875
train loss:  0.42788639664649963
train gradient:  0.097353090540992
iteration : 11620
train acc:  0.7578125
train loss:  0.4761425852775574
train gradient:  0.130329520456544
iteration : 11621
train acc:  0.6875
train loss:  0.5705214142799377
train gradient:  0.16102902463461305
iteration : 11622
train acc:  0.7734375
train loss:  0.4791331887245178
train gradient:  0.13398788056753913
iteration : 11623
train acc:  0.6796875
train loss:  0.5172305107116699
train gradient:  0.16341106232891084
iteration : 11624
train acc:  0.703125
train loss:  0.5160545110702515
train gradient:  0.15787444131988237
iteration : 11625
train acc:  0.7890625
train loss:  0.5000898838043213
train gradient:  0.116581277835344
iteration : 11626
train acc:  0.6953125
train loss:  0.5093932151794434
train gradient:  0.15422796624058777
iteration : 11627
train acc:  0.7109375
train loss:  0.5021417140960693
train gradient:  0.12783006870659408
iteration : 11628
train acc:  0.75
train loss:  0.5072987079620361
train gradient:  0.13006711703553842
iteration : 11629
train acc:  0.8203125
train loss:  0.38187384605407715
train gradient:  0.09418133946763263
iteration : 11630
train acc:  0.7578125
train loss:  0.44347161054611206
train gradient:  0.08554570092454537
iteration : 11631
train acc:  0.7734375
train loss:  0.44220834970474243
train gradient:  0.08994400394368897
iteration : 11632
train acc:  0.703125
train loss:  0.5440950393676758
train gradient:  0.14440584237725598
iteration : 11633
train acc:  0.6484375
train loss:  0.5703915357589722
train gradient:  0.15463476949096538
iteration : 11634
train acc:  0.796875
train loss:  0.46098923683166504
train gradient:  0.1206868558563413
iteration : 11635
train acc:  0.7890625
train loss:  0.4262046217918396
train gradient:  0.09243945442884281
iteration : 11636
train acc:  0.8125
train loss:  0.46167489886283875
train gradient:  0.1230461289801051
iteration : 11637
train acc:  0.765625
train loss:  0.5189263820648193
train gradient:  0.12372132590491638
iteration : 11638
train acc:  0.7578125
train loss:  0.4882975220680237
train gradient:  0.12301268135272542
iteration : 11639
train acc:  0.71875
train loss:  0.5293791890144348
train gradient:  0.12609072713984282
iteration : 11640
train acc:  0.796875
train loss:  0.468768835067749
train gradient:  0.1216869077964348
iteration : 11641
train acc:  0.75
train loss:  0.5142673254013062
train gradient:  0.16481550778829196
iteration : 11642
train acc:  0.7578125
train loss:  0.4822027087211609
train gradient:  0.13024215947799758
iteration : 11643
train acc:  0.7109375
train loss:  0.5080527067184448
train gradient:  0.10537543720104472
iteration : 11644
train acc:  0.7421875
train loss:  0.48240500688552856
train gradient:  0.1407960930253876
iteration : 11645
train acc:  0.75
train loss:  0.5279134511947632
train gradient:  0.12674649339969396
iteration : 11646
train acc:  0.7578125
train loss:  0.52387934923172
train gradient:  0.13659655262324896
iteration : 11647
train acc:  0.7265625
train loss:  0.5151453018188477
train gradient:  0.13125049470959974
iteration : 11648
train acc:  0.7578125
train loss:  0.463182657957077
train gradient:  0.13605896143999416
iteration : 11649
train acc:  0.78125
train loss:  0.45140641927719116
train gradient:  0.14333169976874177
iteration : 11650
train acc:  0.7421875
train loss:  0.4898216128349304
train gradient:  0.13055709247020375
iteration : 11651
train acc:  0.78125
train loss:  0.48434728384017944
train gradient:  0.11164041544708068
iteration : 11652
train acc:  0.703125
train loss:  0.5658068656921387
train gradient:  0.1532260019240084
iteration : 11653
train acc:  0.703125
train loss:  0.5438989400863647
train gradient:  0.11977185190876082
iteration : 11654
train acc:  0.7578125
train loss:  0.4991700351238251
train gradient:  0.12238602462460876
iteration : 11655
train acc:  0.7265625
train loss:  0.468700110912323
train gradient:  0.12481808441683738
iteration : 11656
train acc:  0.7421875
train loss:  0.5060079097747803
train gradient:  0.15899123597949946
iteration : 11657
train acc:  0.7578125
train loss:  0.4572036862373352
train gradient:  0.09992217611340205
iteration : 11658
train acc:  0.734375
train loss:  0.5183388590812683
train gradient:  0.15406029594623982
iteration : 11659
train acc:  0.734375
train loss:  0.5079509615898132
train gradient:  0.17110998718801035
iteration : 11660
train acc:  0.765625
train loss:  0.4897993803024292
train gradient:  0.14349277153585765
iteration : 11661
train acc:  0.78125
train loss:  0.4885106086730957
train gradient:  0.10638232461790469
iteration : 11662
train acc:  0.796875
train loss:  0.40091049671173096
train gradient:  0.07475706649112174
iteration : 11663
train acc:  0.7421875
train loss:  0.49047964811325073
train gradient:  0.12178444048140324
iteration : 11664
train acc:  0.7734375
train loss:  0.41721421480178833
train gradient:  0.09821180985340897
iteration : 11665
train acc:  0.71875
train loss:  0.49147987365722656
train gradient:  0.12589933632726846
iteration : 11666
train acc:  0.765625
train loss:  0.4132792353630066
train gradient:  0.10546239843470834
iteration : 11667
train acc:  0.6796875
train loss:  0.5449214577674866
train gradient:  0.16957256016172584
iteration : 11668
train acc:  0.7421875
train loss:  0.49696269631385803
train gradient:  0.10947836152310665
iteration : 11669
train acc:  0.7109375
train loss:  0.523597002029419
train gradient:  0.13465018881961255
iteration : 11670
train acc:  0.7265625
train loss:  0.4824676513671875
train gradient:  0.11447669556889227
iteration : 11671
train acc:  0.7734375
train loss:  0.44858941435813904
train gradient:  0.11377850387266415
iteration : 11672
train acc:  0.6796875
train loss:  0.557107150554657
train gradient:  0.1402701078203765
iteration : 11673
train acc:  0.75
train loss:  0.46315428614616394
train gradient:  0.13317627134278737
iteration : 11674
train acc:  0.7421875
train loss:  0.5362312197685242
train gradient:  0.14105146092092405
iteration : 11675
train acc:  0.78125
train loss:  0.4630797505378723
train gradient:  0.10640866161229112
iteration : 11676
train acc:  0.78125
train loss:  0.479735791683197
train gradient:  0.12200052950482008
iteration : 11677
train acc:  0.7421875
train loss:  0.491393506526947
train gradient:  0.15675916465795692
iteration : 11678
train acc:  0.7109375
train loss:  0.5157297849655151
train gradient:  0.12211351975982128
iteration : 11679
train acc:  0.75
train loss:  0.5011200308799744
train gradient:  0.13406783144547077
iteration : 11680
train acc:  0.7109375
train loss:  0.5311564207077026
train gradient:  0.13933031586453232
iteration : 11681
train acc:  0.7109375
train loss:  0.5444945096969604
train gradient:  0.13383760828243552
iteration : 11682
train acc:  0.75
train loss:  0.4946708381175995
train gradient:  0.1224081979442545
iteration : 11683
train acc:  0.7421875
train loss:  0.4929139316082001
train gradient:  0.12275543258626724
iteration : 11684
train acc:  0.7578125
train loss:  0.46373066306114197
train gradient:  0.11482085433522245
iteration : 11685
train acc:  0.75
train loss:  0.4658999443054199
train gradient:  0.11515894613358801
iteration : 11686
train acc:  0.7890625
train loss:  0.4386971592903137
train gradient:  0.11050630461013292
iteration : 11687
train acc:  0.7578125
train loss:  0.4557530879974365
train gradient:  0.09669410020853796
iteration : 11688
train acc:  0.7109375
train loss:  0.5292840600013733
train gradient:  0.11505216281786335
iteration : 11689
train acc:  0.7578125
train loss:  0.4993695318698883
train gradient:  0.12783791291457955
iteration : 11690
train acc:  0.765625
train loss:  0.4559035897254944
train gradient:  0.11773914498897985
iteration : 11691
train acc:  0.796875
train loss:  0.4472777545452118
train gradient:  0.10116933599420921
iteration : 11692
train acc:  0.703125
train loss:  0.5796384215354919
train gradient:  0.1395902315598568
iteration : 11693
train acc:  0.6484375
train loss:  0.5596217513084412
train gradient:  0.16098541428184834
iteration : 11694
train acc:  0.7109375
train loss:  0.5361737012863159
train gradient:  0.1392080940526857
iteration : 11695
train acc:  0.7578125
train loss:  0.5062575340270996
train gradient:  0.11230663978601739
iteration : 11696
train acc:  0.7578125
train loss:  0.46201324462890625
train gradient:  0.0966571490950927
iteration : 11697
train acc:  0.703125
train loss:  0.5326576232910156
train gradient:  0.1717018091445579
iteration : 11698
train acc:  0.6640625
train loss:  0.5351687669754028
train gradient:  0.1654180405968867
iteration : 11699
train acc:  0.7890625
train loss:  0.49945807456970215
train gradient:  0.14759910555686273
iteration : 11700
train acc:  0.7890625
train loss:  0.4914161264896393
train gradient:  0.1119059345118567
iteration : 11701
train acc:  0.7109375
train loss:  0.501140832901001
train gradient:  0.12764559502425554
iteration : 11702
train acc:  0.703125
train loss:  0.5105634331703186
train gradient:  0.1385156614395948
iteration : 11703
train acc:  0.765625
train loss:  0.47813770174980164
train gradient:  0.11084308029517648
iteration : 11704
train acc:  0.6875
train loss:  0.5457729697227478
train gradient:  0.14716387317556104
iteration : 11705
train acc:  0.734375
train loss:  0.5318412780761719
train gradient:  0.12336218930282569
iteration : 11706
train acc:  0.6875
train loss:  0.5273809432983398
train gradient:  0.1480704518716402
iteration : 11707
train acc:  0.765625
train loss:  0.5000799298286438
train gradient:  0.15195545197114488
iteration : 11708
train acc:  0.7265625
train loss:  0.4950519800186157
train gradient:  0.11410603132712589
iteration : 11709
train acc:  0.78125
train loss:  0.44315898418426514
train gradient:  0.10896829163193202
iteration : 11710
train acc:  0.765625
train loss:  0.49525508284568787
train gradient:  0.1299378285324293
iteration : 11711
train acc:  0.71875
train loss:  0.5327777862548828
train gradient:  0.10996178120805725
iteration : 11712
train acc:  0.7421875
train loss:  0.5149334669113159
train gradient:  0.11424960529017364
iteration : 11713
train acc:  0.703125
train loss:  0.502899169921875
train gradient:  0.1452491540466072
iteration : 11714
train acc:  0.6875
train loss:  0.5509829521179199
train gradient:  0.15637625339309408
iteration : 11715
train acc:  0.671875
train loss:  0.5491814613342285
train gradient:  0.15966643942474257
iteration : 11716
train acc:  0.75
train loss:  0.5090234279632568
train gradient:  0.1124188321676674
iteration : 11717
train acc:  0.765625
train loss:  0.5000426173210144
train gradient:  0.16329377909502618
iteration : 11718
train acc:  0.7890625
train loss:  0.4950745403766632
train gradient:  0.11774879662025857
iteration : 11719
train acc:  0.765625
train loss:  0.4730578064918518
train gradient:  0.10881781970022665
iteration : 11720
train acc:  0.7734375
train loss:  0.43057239055633545
train gradient:  0.09830533411513724
iteration : 11721
train acc:  0.7109375
train loss:  0.5069447159767151
train gradient:  0.13242832079180308
iteration : 11722
train acc:  0.7578125
train loss:  0.5116159915924072
train gradient:  0.13490637073372327
iteration : 11723
train acc:  0.703125
train loss:  0.4931301474571228
train gradient:  0.1531972390314965
iteration : 11724
train acc:  0.7578125
train loss:  0.4936625361442566
train gradient:  0.1293532571242227
iteration : 11725
train acc:  0.7890625
train loss:  0.46153903007507324
train gradient:  0.1036671962977022
iteration : 11726
train acc:  0.828125
train loss:  0.43679994344711304
train gradient:  0.13178276676509798
iteration : 11727
train acc:  0.765625
train loss:  0.4509793519973755
train gradient:  0.0990347204667991
iteration : 11728
train acc:  0.7734375
train loss:  0.44477468729019165
train gradient:  0.11626126928607403
iteration : 11729
train acc:  0.8046875
train loss:  0.4630436301231384
train gradient:  0.09030193190075035
iteration : 11730
train acc:  0.71875
train loss:  0.47954457998275757
train gradient:  0.12615216383276096
iteration : 11731
train acc:  0.7109375
train loss:  0.5041674971580505
train gradient:  0.1299752512221878
iteration : 11732
train acc:  0.7734375
train loss:  0.40488946437835693
train gradient:  0.07620311229370616
iteration : 11733
train acc:  0.8203125
train loss:  0.4250849187374115
train gradient:  0.1009093895025513
iteration : 11734
train acc:  0.78125
train loss:  0.40787410736083984
train gradient:  0.0894039479547228
iteration : 11735
train acc:  0.71875
train loss:  0.5085675716400146
train gradient:  0.12572770236885428
iteration : 11736
train acc:  0.765625
train loss:  0.4865017533302307
train gradient:  0.1035974180101877
iteration : 11737
train acc:  0.7890625
train loss:  0.42547133564949036
train gradient:  0.09317094140044434
iteration : 11738
train acc:  0.7734375
train loss:  0.42162322998046875
train gradient:  0.09919624082500385
iteration : 11739
train acc:  0.6796875
train loss:  0.5142364501953125
train gradient:  0.1390130202680211
iteration : 11740
train acc:  0.734375
train loss:  0.48163077235221863
train gradient:  0.10474634323580218
iteration : 11741
train acc:  0.734375
train loss:  0.4677765369415283
train gradient:  0.11284721017606646
iteration : 11742
train acc:  0.765625
train loss:  0.5264850854873657
train gradient:  0.15081356210829988
iteration : 11743
train acc:  0.6953125
train loss:  0.5794856548309326
train gradient:  0.17749711380735522
iteration : 11744
train acc:  0.7109375
train loss:  0.4701654613018036
train gradient:  0.11619969962337745
iteration : 11745
train acc:  0.7578125
train loss:  0.4753169119358063
train gradient:  0.10892248859223624
iteration : 11746
train acc:  0.703125
train loss:  0.5207075476646423
train gradient:  0.13549349874300082
iteration : 11747
train acc:  0.734375
train loss:  0.47361499071121216
train gradient:  0.11274357593831738
iteration : 11748
train acc:  0.75
train loss:  0.5238568782806396
train gradient:  0.13701077973357007
iteration : 11749
train acc:  0.71875
train loss:  0.4994127154350281
train gradient:  0.15363290615793196
iteration : 11750
train acc:  0.7578125
train loss:  0.46309009194374084
train gradient:  0.09586374798898052
iteration : 11751
train acc:  0.71875
train loss:  0.5131865739822388
train gradient:  0.12150150894604075
iteration : 11752
train acc:  0.7421875
train loss:  0.5384430885314941
train gradient:  0.13633833514555188
iteration : 11753
train acc:  0.8046875
train loss:  0.4291079640388489
train gradient:  0.09497201388799624
iteration : 11754
train acc:  0.734375
train loss:  0.5636768341064453
train gradient:  0.14661215335910088
iteration : 11755
train acc:  0.7265625
train loss:  0.46583813428878784
train gradient:  0.11846243512326816
iteration : 11756
train acc:  0.78125
train loss:  0.4623136520385742
train gradient:  0.10159105617371637
iteration : 11757
train acc:  0.7734375
train loss:  0.43455085158348083
train gradient:  0.133810577302789
iteration : 11758
train acc:  0.75
train loss:  0.5067175030708313
train gradient:  0.1560600629950124
iteration : 11759
train acc:  0.734375
train loss:  0.4585057497024536
train gradient:  0.09707327781303654
iteration : 11760
train acc:  0.796875
train loss:  0.4027201533317566
train gradient:  0.0923995240820619
iteration : 11761
train acc:  0.7421875
train loss:  0.502448320388794
train gradient:  0.12895058496132544
iteration : 11762
train acc:  0.734375
train loss:  0.5081431269645691
train gradient:  0.16396096059566403
iteration : 11763
train acc:  0.6953125
train loss:  0.538560688495636
train gradient:  0.1512397234178065
iteration : 11764
train acc:  0.734375
train loss:  0.4753528833389282
train gradient:  0.1451717512478446
iteration : 11765
train acc:  0.734375
train loss:  0.4697835445404053
train gradient:  0.12739592862298332
iteration : 11766
train acc:  0.7734375
train loss:  0.4655373692512512
train gradient:  0.1062972848008691
iteration : 11767
train acc:  0.7578125
train loss:  0.46566784381866455
train gradient:  0.10747672438067962
iteration : 11768
train acc:  0.765625
train loss:  0.48072686791419983
train gradient:  0.12791163076693662
iteration : 11769
train acc:  0.71875
train loss:  0.56727534532547
train gradient:  0.1608031037804719
iteration : 11770
train acc:  0.765625
train loss:  0.5099782943725586
train gradient:  0.16268205835210114
iteration : 11771
train acc:  0.7265625
train loss:  0.5056238770484924
train gradient:  0.12049829851790024
iteration : 11772
train acc:  0.7890625
train loss:  0.46701323986053467
train gradient:  0.1119157552150083
iteration : 11773
train acc:  0.8125
train loss:  0.45184579491615295
train gradient:  0.1357520789542716
iteration : 11774
train acc:  0.734375
train loss:  0.545253574848175
train gradient:  0.15616501038216488
iteration : 11775
train acc:  0.6953125
train loss:  0.5342625379562378
train gradient:  0.1383524077742574
iteration : 11776
train acc:  0.7109375
train loss:  0.5129132866859436
train gradient:  0.13217472398058722
iteration : 11777
train acc:  0.71875
train loss:  0.6152351498603821
train gradient:  0.16236232409129314
iteration : 11778
train acc:  0.78125
train loss:  0.4596213400363922
train gradient:  0.09530116687044698
iteration : 11779
train acc:  0.765625
train loss:  0.45557692646980286
train gradient:  0.11229882997724856
iteration : 11780
train acc:  0.7734375
train loss:  0.4405536651611328
train gradient:  0.11104626885360494
iteration : 11781
train acc:  0.765625
train loss:  0.4450020492076874
train gradient:  0.1064662739105623
iteration : 11782
train acc:  0.7734375
train loss:  0.4563084840774536
train gradient:  0.13902673044736358
iteration : 11783
train acc:  0.7890625
train loss:  0.4529840052127838
train gradient:  0.10988162980928268
iteration : 11784
train acc:  0.7109375
train loss:  0.563506543636322
train gradient:  0.13618536046856122
iteration : 11785
train acc:  0.8203125
train loss:  0.45656880736351013
train gradient:  0.09999407032004314
iteration : 11786
train acc:  0.7578125
train loss:  0.4981743097305298
train gradient:  0.16466804549930764
iteration : 11787
train acc:  0.7265625
train loss:  0.480951189994812
train gradient:  0.11576972120834651
iteration : 11788
train acc:  0.7421875
train loss:  0.4544826149940491
train gradient:  0.1095106985411958
iteration : 11789
train acc:  0.765625
train loss:  0.49854010343551636
train gradient:  0.14965640464858754
iteration : 11790
train acc:  0.7578125
train loss:  0.5310545563697815
train gradient:  0.1350821558422407
iteration : 11791
train acc:  0.765625
train loss:  0.46518027782440186
train gradient:  0.11461478812290304
iteration : 11792
train acc:  0.7734375
train loss:  0.4543396234512329
train gradient:  0.09655052121955533
iteration : 11793
train acc:  0.7265625
train loss:  0.502942681312561
train gradient:  0.1284596695952305
iteration : 11794
train acc:  0.7265625
train loss:  0.4768015146255493
train gradient:  0.10181541595221774
iteration : 11795
train acc:  0.7109375
train loss:  0.5414832234382629
train gradient:  0.1450776443564541
iteration : 11796
train acc:  0.734375
train loss:  0.4812646508216858
train gradient:  0.1234810598332446
iteration : 11797
train acc:  0.734375
train loss:  0.4773295521736145
train gradient:  0.11209889166300213
iteration : 11798
train acc:  0.6953125
train loss:  0.5124663710594177
train gradient:  0.1169607953926855
iteration : 11799
train acc:  0.75
train loss:  0.48043835163116455
train gradient:  0.1266968452449912
iteration : 11800
train acc:  0.71875
train loss:  0.4592689275741577
train gradient:  0.13888754401945658
iteration : 11801
train acc:  0.7734375
train loss:  0.48603665828704834
train gradient:  0.11060283991083196
iteration : 11802
train acc:  0.8125
train loss:  0.4278568625450134
train gradient:  0.10709200587953592
iteration : 11803
train acc:  0.71875
train loss:  0.5140556693077087
train gradient:  0.1651530821177022
iteration : 11804
train acc:  0.703125
train loss:  0.5407422780990601
train gradient:  0.13069830711619812
iteration : 11805
train acc:  0.765625
train loss:  0.46390414237976074
train gradient:  0.10072522834998535
iteration : 11806
train acc:  0.7421875
train loss:  0.5065542459487915
train gradient:  0.10837499635380368
iteration : 11807
train acc:  0.78125
train loss:  0.4305281341075897
train gradient:  0.09728240871519211
iteration : 11808
train acc:  0.828125
train loss:  0.4178808331489563
train gradient:  0.09058639522641844
iteration : 11809
train acc:  0.796875
train loss:  0.4693184494972229
train gradient:  0.11279541548770537
iteration : 11810
train acc:  0.796875
train loss:  0.44544485211372375
train gradient:  0.11026931418851038
iteration : 11811
train acc:  0.75
train loss:  0.41239404678344727
train gradient:  0.08947609689320939
iteration : 11812
train acc:  0.765625
train loss:  0.48265737295150757
train gradient:  0.15383340559916084
iteration : 11813
train acc:  0.6953125
train loss:  0.5739858150482178
train gradient:  0.16550836139850006
iteration : 11814
train acc:  0.7734375
train loss:  0.4978374242782593
train gradient:  0.15689882979620046
iteration : 11815
train acc:  0.765625
train loss:  0.4762435257434845
train gradient:  0.14433233988444763
iteration : 11816
train acc:  0.703125
train loss:  0.5291681885719299
train gradient:  0.13463929125503366
iteration : 11817
train acc:  0.796875
train loss:  0.4084262251853943
train gradient:  0.08802935614410394
iteration : 11818
train acc:  0.7109375
train loss:  0.4865127503871918
train gradient:  0.11400270641498635
iteration : 11819
train acc:  0.7734375
train loss:  0.46396809816360474
train gradient:  0.09818193845127236
iteration : 11820
train acc:  0.71875
train loss:  0.5077844858169556
train gradient:  0.1340014470255826
iteration : 11821
train acc:  0.7265625
train loss:  0.5047463774681091
train gradient:  0.11458105369668276
iteration : 11822
train acc:  0.7109375
train loss:  0.4983534812927246
train gradient:  0.14674009057760712
iteration : 11823
train acc:  0.7578125
train loss:  0.49042052030563354
train gradient:  0.12775650156724178
iteration : 11824
train acc:  0.7578125
train loss:  0.4622940719127655
train gradient:  0.1369383062221561
iteration : 11825
train acc:  0.7421875
train loss:  0.4842170476913452
train gradient:  0.1226778556196013
iteration : 11826
train acc:  0.7421875
train loss:  0.5311368107795715
train gradient:  0.13934422086366055
iteration : 11827
train acc:  0.7734375
train loss:  0.46412917971611023
train gradient:  0.11611411249253902
iteration : 11828
train acc:  0.6953125
train loss:  0.520190417766571
train gradient:  0.142286326477867
iteration : 11829
train acc:  0.703125
train loss:  0.5658885836601257
train gradient:  0.1346789121384192
iteration : 11830
train acc:  0.7578125
train loss:  0.48851463198661804
train gradient:  0.11511905599837974
iteration : 11831
train acc:  0.8203125
train loss:  0.43278583884239197
train gradient:  0.11718120053381445
iteration : 11832
train acc:  0.6640625
train loss:  0.6008793115615845
train gradient:  0.1992014218174364
iteration : 11833
train acc:  0.796875
train loss:  0.4525875449180603
train gradient:  0.10626704706606391
iteration : 11834
train acc:  0.7421875
train loss:  0.4946466088294983
train gradient:  0.11604469505644219
iteration : 11835
train acc:  0.734375
train loss:  0.5172550678253174
train gradient:  0.15757351331770936
iteration : 11836
train acc:  0.7734375
train loss:  0.4664662480354309
train gradient:  0.12681872341412165
iteration : 11837
train acc:  0.734375
train loss:  0.48942476511001587
train gradient:  0.1153149709374017
iteration : 11838
train acc:  0.6796875
train loss:  0.6293565034866333
train gradient:  0.1869421659091799
iteration : 11839
train acc:  0.7265625
train loss:  0.5072675943374634
train gradient:  0.13696578649263763
iteration : 11840
train acc:  0.78125
train loss:  0.47202903032302856
train gradient:  0.118002186774891
iteration : 11841
train acc:  0.7578125
train loss:  0.46619030833244324
train gradient:  0.10120151934065537
iteration : 11842
train acc:  0.7265625
train loss:  0.531602144241333
train gradient:  0.15943171569706038
iteration : 11843
train acc:  0.765625
train loss:  0.39761778712272644
train gradient:  0.08446403989681453
iteration : 11844
train acc:  0.7421875
train loss:  0.49137842655181885
train gradient:  0.12584469880058902
iteration : 11845
train acc:  0.71875
train loss:  0.5067664980888367
train gradient:  0.14767002029148157
iteration : 11846
train acc:  0.75
train loss:  0.49710169434547424
train gradient:  0.13367087275567602
iteration : 11847
train acc:  0.765625
train loss:  0.48865604400634766
train gradient:  0.15684216279223556
iteration : 11848
train acc:  0.75
train loss:  0.47529613971710205
train gradient:  0.1066347872270526
iteration : 11849
train acc:  0.65625
train loss:  0.6237460970878601
train gradient:  0.15281661085294612
iteration : 11850
train acc:  0.734375
train loss:  0.538029134273529
train gradient:  0.1643366069128716
iteration : 11851
train acc:  0.765625
train loss:  0.43012332916259766
train gradient:  0.09810378096245315
iteration : 11852
train acc:  0.75
train loss:  0.49246716499328613
train gradient:  0.1391736986426011
iteration : 11853
train acc:  0.7890625
train loss:  0.4702303409576416
train gradient:  0.11926114506947215
iteration : 11854
train acc:  0.8046875
train loss:  0.47930991649627686
train gradient:  0.141335769093005
iteration : 11855
train acc:  0.765625
train loss:  0.49912822246551514
train gradient:  0.10125685027064174
iteration : 11856
train acc:  0.7265625
train loss:  0.5439746975898743
train gradient:  0.14116939996218125
iteration : 11857
train acc:  0.7890625
train loss:  0.4760280251502991
train gradient:  0.11065540886465389
iteration : 11858
train acc:  0.6796875
train loss:  0.5442541241645813
train gradient:  0.13292032899470402
iteration : 11859
train acc:  0.703125
train loss:  0.49797552824020386
train gradient:  0.1499294123756436
iteration : 11860
train acc:  0.7421875
train loss:  0.4939974546432495
train gradient:  0.12600676394848007
iteration : 11861
train acc:  0.7890625
train loss:  0.44782471656799316
train gradient:  0.08984459118119702
iteration : 11862
train acc:  0.7890625
train loss:  0.4845343828201294
train gradient:  0.12202088632093107
iteration : 11863
train acc:  0.734375
train loss:  0.551132082939148
train gradient:  0.12004651784057907
iteration : 11864
train acc:  0.7421875
train loss:  0.5032781362533569
train gradient:  0.11636748781234402
iteration : 11865
train acc:  0.796875
train loss:  0.44276362657546997
train gradient:  0.10048352761045103
iteration : 11866
train acc:  0.78125
train loss:  0.40464264154434204
train gradient:  0.10506066315172916
iteration : 11867
train acc:  0.6875
train loss:  0.49312448501586914
train gradient:  0.115806186860989
iteration : 11868
train acc:  0.7890625
train loss:  0.41742244362831116
train gradient:  0.1033683761859429
iteration : 11869
train acc:  0.8046875
train loss:  0.42983877658843994
train gradient:  0.0875466773379417
iteration : 11870
train acc:  0.7265625
train loss:  0.5308208465576172
train gradient:  0.1177107714255668
iteration : 11871
train acc:  0.78125
train loss:  0.4699242413043976
train gradient:  0.14417229657181677
iteration : 11872
train acc:  0.7421875
train loss:  0.4355688989162445
train gradient:  0.08920879313333106
iteration : 11873
train acc:  0.7421875
train loss:  0.5193937420845032
train gradient:  0.11224072193734383
iteration : 11874
train acc:  0.8125
train loss:  0.43262094259262085
train gradient:  0.10186559308812
iteration : 11875
train acc:  0.71875
train loss:  0.4799662232398987
train gradient:  0.11307009907172652
iteration : 11876
train acc:  0.6953125
train loss:  0.5256906747817993
train gradient:  0.14363908586099072
iteration : 11877
train acc:  0.7734375
train loss:  0.48704761266708374
train gradient:  0.1292904512784588
iteration : 11878
train acc:  0.7578125
train loss:  0.4914272427558899
train gradient:  0.1313988656399419
iteration : 11879
train acc:  0.7421875
train loss:  0.4996061325073242
train gradient:  0.1444950838501919
iteration : 11880
train acc:  0.765625
train loss:  0.48731088638305664
train gradient:  0.1212433824139757
iteration : 11881
train acc:  0.734375
train loss:  0.43615013360977173
train gradient:  0.10833823308577505
iteration : 11882
train acc:  0.734375
train loss:  0.4999811053276062
train gradient:  0.11966550595737985
iteration : 11883
train acc:  0.6953125
train loss:  0.5376118421554565
train gradient:  0.15544468028590458
iteration : 11884
train acc:  0.7890625
train loss:  0.4964110553264618
train gradient:  0.13249436785159974
iteration : 11885
train acc:  0.6953125
train loss:  0.5530573129653931
train gradient:  0.11866106992750772
iteration : 11886
train acc:  0.78125
train loss:  0.509697675704956
train gradient:  0.11601985567347542
iteration : 11887
train acc:  0.765625
train loss:  0.5139819979667664
train gradient:  0.1550222286536348
iteration : 11888
train acc:  0.71875
train loss:  0.5224130153656006
train gradient:  0.13581190327975473
iteration : 11889
train acc:  0.703125
train loss:  0.49819523096084595
train gradient:  0.12032937096221813
iteration : 11890
train acc:  0.7578125
train loss:  0.43778902292251587
train gradient:  0.08878819286964591
iteration : 11891
train acc:  0.7890625
train loss:  0.41236191987991333
train gradient:  0.09217453954605304
iteration : 11892
train acc:  0.796875
train loss:  0.4717990756034851
train gradient:  0.13683990897351805
iteration : 11893
train acc:  0.7265625
train loss:  0.48334380984306335
train gradient:  0.11739855920785885
iteration : 11894
train acc:  0.71875
train loss:  0.5366378426551819
train gradient:  0.13695427109183625
iteration : 11895
train acc:  0.7421875
train loss:  0.5650960206985474
train gradient:  0.18208626165980737
iteration : 11896
train acc:  0.7421875
train loss:  0.5166335105895996
train gradient:  0.1231674014282453
iteration : 11897
train acc:  0.765625
train loss:  0.46412134170532227
train gradient:  0.13572402041939569
iteration : 11898
train acc:  0.7421875
train loss:  0.47738760709762573
train gradient:  0.1252175770161275
iteration : 11899
train acc:  0.703125
train loss:  0.5621072053909302
train gradient:  0.16177279277315176
iteration : 11900
train acc:  0.7265625
train loss:  0.4714394807815552
train gradient:  0.11595023231189315
iteration : 11901
train acc:  0.7421875
train loss:  0.5058293342590332
train gradient:  0.10279602517738275
iteration : 11902
train acc:  0.7421875
train loss:  0.4518112540245056
train gradient:  0.09206338432904092
iteration : 11903
train acc:  0.7421875
train loss:  0.4455469846725464
train gradient:  0.1266966164511572
iteration : 11904
train acc:  0.7734375
train loss:  0.43364858627319336
train gradient:  0.10748088195920041
iteration : 11905
train acc:  0.78125
train loss:  0.4480813145637512
train gradient:  0.1045618160081095
iteration : 11906
train acc:  0.7265625
train loss:  0.5063730478286743
train gradient:  0.1402592965034412
iteration : 11907
train acc:  0.7265625
train loss:  0.43878060579299927
train gradient:  0.08664526138035848
iteration : 11908
train acc:  0.78125
train loss:  0.4274044632911682
train gradient:  0.09309109851091432
iteration : 11909
train acc:  0.703125
train loss:  0.5403997898101807
train gradient:  0.11446965606585952
iteration : 11910
train acc:  0.78125
train loss:  0.43289482593536377
train gradient:  0.09689073308752633
iteration : 11911
train acc:  0.734375
train loss:  0.5146842002868652
train gradient:  0.12275173547558495
iteration : 11912
train acc:  0.6953125
train loss:  0.5557039976119995
train gradient:  0.17742589499916808
iteration : 11913
train acc:  0.7421875
train loss:  0.4968326687812805
train gradient:  0.1322539854592525
iteration : 11914
train acc:  0.7890625
train loss:  0.45249485969543457
train gradient:  0.1008675401469229
iteration : 11915
train acc:  0.765625
train loss:  0.4880676567554474
train gradient:  0.09604472689046631
iteration : 11916
train acc:  0.7109375
train loss:  0.5336962938308716
train gradient:  0.16156265261125607
iteration : 11917
train acc:  0.78125
train loss:  0.4858560562133789
train gradient:  0.1387705757386401
iteration : 11918
train acc:  0.75
train loss:  0.49001526832580566
train gradient:  0.18565329863776558
iteration : 11919
train acc:  0.75
train loss:  0.453202486038208
train gradient:  0.1215642554284317
iteration : 11920
train acc:  0.75
train loss:  0.5265243053436279
train gradient:  0.2098343334368955
iteration : 11921
train acc:  0.7890625
train loss:  0.452835351228714
train gradient:  0.10308347143891734
iteration : 11922
train acc:  0.7265625
train loss:  0.4745378792285919
train gradient:  0.13218991836887095
iteration : 11923
train acc:  0.78125
train loss:  0.511252224445343
train gradient:  0.10656471196395495
iteration : 11924
train acc:  0.71875
train loss:  0.5345191359519958
train gradient:  0.1638314827793773
iteration : 11925
train acc:  0.8359375
train loss:  0.3725971579551697
train gradient:  0.07140409208015905
iteration : 11926
train acc:  0.765625
train loss:  0.4428632855415344
train gradient:  0.10211499528379929
iteration : 11927
train acc:  0.7734375
train loss:  0.49054044485092163
train gradient:  0.15819248605730804
iteration : 11928
train acc:  0.7109375
train loss:  0.5597468614578247
train gradient:  0.16625081866348507
iteration : 11929
train acc:  0.6875
train loss:  0.5418869853019714
train gradient:  0.13966785351323485
iteration : 11930
train acc:  0.7265625
train loss:  0.5293488502502441
train gradient:  0.13003337801218384
iteration : 11931
train acc:  0.7421875
train loss:  0.5170718431472778
train gradient:  0.13521208055321515
iteration : 11932
train acc:  0.8046875
train loss:  0.4437878131866455
train gradient:  0.10898557195021123
iteration : 11933
train acc:  0.703125
train loss:  0.5863598585128784
train gradient:  0.16728868682026593
iteration : 11934
train acc:  0.7890625
train loss:  0.40306752920150757
train gradient:  0.08003387622755603
iteration : 11935
train acc:  0.7734375
train loss:  0.45611608028411865
train gradient:  0.10937483211264702
iteration : 11936
train acc:  0.8046875
train loss:  0.45413076877593994
train gradient:  0.11332087080206855
iteration : 11937
train acc:  0.734375
train loss:  0.4791201651096344
train gradient:  0.1014071875686336
iteration : 11938
train acc:  0.75
train loss:  0.5097232460975647
train gradient:  0.16693687294015275
iteration : 11939
train acc:  0.7578125
train loss:  0.46490609645843506
train gradient:  0.11011875732783774
iteration : 11940
train acc:  0.6328125
train loss:  0.5801724195480347
train gradient:  0.16159392841501286
iteration : 11941
train acc:  0.71875
train loss:  0.5100663900375366
train gradient:  0.1072866642384596
iteration : 11942
train acc:  0.796875
train loss:  0.4619520306587219
train gradient:  0.11544965414802028
iteration : 11943
train acc:  0.6953125
train loss:  0.5009055137634277
train gradient:  0.12130895453350303
iteration : 11944
train acc:  0.78125
train loss:  0.46646440029144287
train gradient:  0.10280162763639271
iteration : 11945
train acc:  0.7578125
train loss:  0.4937797486782074
train gradient:  0.1295314808500931
iteration : 11946
train acc:  0.78125
train loss:  0.40553468465805054
train gradient:  0.08227962872543158
iteration : 11947
train acc:  0.8125
train loss:  0.44928959012031555
train gradient:  0.1279273524536945
iteration : 11948
train acc:  0.765625
train loss:  0.49617087841033936
train gradient:  0.14432584444687896
iteration : 11949
train acc:  0.7734375
train loss:  0.41539543867111206
train gradient:  0.07545444165739647
iteration : 11950
train acc:  0.71875
train loss:  0.5411885976791382
train gradient:  0.1315042914423244
iteration : 11951
train acc:  0.8203125
train loss:  0.44224077463150024
train gradient:  0.10942239813778142
iteration : 11952
train acc:  0.8046875
train loss:  0.4585464596748352
train gradient:  0.1431702645720333
iteration : 11953
train acc:  0.6640625
train loss:  0.5879061222076416
train gradient:  0.1988191628757635
iteration : 11954
train acc:  0.71875
train loss:  0.4650757908821106
train gradient:  0.1128529038072601
iteration : 11955
train acc:  0.6953125
train loss:  0.5711063146591187
train gradient:  0.1545387849236744
iteration : 11956
train acc:  0.65625
train loss:  0.5857560634613037
train gradient:  0.18440659488433767
iteration : 11957
train acc:  0.65625
train loss:  0.5899412631988525
train gradient:  0.22808824915678297
iteration : 11958
train acc:  0.7890625
train loss:  0.465385377407074
train gradient:  0.12536822818119342
iteration : 11959
train acc:  0.734375
train loss:  0.480883926153183
train gradient:  0.128420046332337
iteration : 11960
train acc:  0.7578125
train loss:  0.5140476822853088
train gradient:  0.1568603257474145
iteration : 11961
train acc:  0.6953125
train loss:  0.5351575613021851
train gradient:  0.12508333490850213
iteration : 11962
train acc:  0.7421875
train loss:  0.4777012765407562
train gradient:  0.11781914369937067
iteration : 11963
train acc:  0.7890625
train loss:  0.45560115575790405
train gradient:  0.154463472189799
iteration : 11964
train acc:  0.796875
train loss:  0.41392743587493896
train gradient:  0.0982230189516546
iteration : 11965
train acc:  0.78125
train loss:  0.5430972576141357
train gradient:  0.1417105655966185
iteration : 11966
train acc:  0.734375
train loss:  0.5254763960838318
train gradient:  0.17723693977280564
iteration : 11967
train acc:  0.7265625
train loss:  0.4776964783668518
train gradient:  0.11289157333512141
iteration : 11968
train acc:  0.7109375
train loss:  0.5230295658111572
train gradient:  0.12805054325096735
iteration : 11969
train acc:  0.7578125
train loss:  0.43375933170318604
train gradient:  0.10164562549717682
iteration : 11970
train acc:  0.7421875
train loss:  0.47345203161239624
train gradient:  0.09909393983272091
iteration : 11971
train acc:  0.7109375
train loss:  0.5179947018623352
train gradient:  0.1148452833762657
iteration : 11972
train acc:  0.6875
train loss:  0.577183187007904
train gradient:  0.18422312422366613
iteration : 11973
train acc:  0.75
train loss:  0.5069060921669006
train gradient:  0.1289213898198296
iteration : 11974
train acc:  0.7890625
train loss:  0.4773790240287781
train gradient:  0.11114334910977718
iteration : 11975
train acc:  0.75
train loss:  0.5198230743408203
train gradient:  0.11881536035537887
iteration : 11976
train acc:  0.6953125
train loss:  0.5385512709617615
train gradient:  0.13525770637633794
iteration : 11977
train acc:  0.7734375
train loss:  0.4339561462402344
train gradient:  0.13869284143844657
iteration : 11978
train acc:  0.765625
train loss:  0.45098358392715454
train gradient:  0.1143193814014612
iteration : 11979
train acc:  0.78125
train loss:  0.47452569007873535
train gradient:  0.11452024009423808
iteration : 11980
train acc:  0.75
train loss:  0.5202803611755371
train gradient:  0.1419981232548823
iteration : 11981
train acc:  0.7421875
train loss:  0.49892738461494446
train gradient:  0.13982178616553292
iteration : 11982
train acc:  0.75
train loss:  0.48620957136154175
train gradient:  0.13486454743521173
iteration : 11983
train acc:  0.703125
train loss:  0.4998463988304138
train gradient:  0.14589688852713525
iteration : 11984
train acc:  0.6953125
train loss:  0.516227662563324
train gradient:  0.12152896893665052
iteration : 11985
train acc:  0.7109375
train loss:  0.48529672622680664
train gradient:  0.12836433568794597
iteration : 11986
train acc:  0.7578125
train loss:  0.5156825184822083
train gradient:  0.16208161699026458
iteration : 11987
train acc:  0.765625
train loss:  0.5031371712684631
train gradient:  0.13664439307085735
iteration : 11988
train acc:  0.7734375
train loss:  0.42848652601242065
train gradient:  0.09465266443588732
iteration : 11989
train acc:  0.6796875
train loss:  0.5285352468490601
train gradient:  0.12188950335823025
iteration : 11990
train acc:  0.7890625
train loss:  0.47657904028892517
train gradient:  0.13826772673756776
iteration : 11991
train acc:  0.78125
train loss:  0.5105456113815308
train gradient:  0.11966157175332631
iteration : 11992
train acc:  0.7890625
train loss:  0.4539458453655243
train gradient:  0.09204798334974061
iteration : 11993
train acc:  0.7265625
train loss:  0.5251085758209229
train gradient:  0.1269606307256183
iteration : 11994
train acc:  0.734375
train loss:  0.491856187582016
train gradient:  0.1278164707963459
iteration : 11995
train acc:  0.75
train loss:  0.4630294144153595
train gradient:  0.12747559221439717
iteration : 11996
train acc:  0.8125
train loss:  0.4703364968299866
train gradient:  0.10015012418228239
iteration : 11997
train acc:  0.7265625
train loss:  0.4920998811721802
train gradient:  0.13108844804689188
iteration : 11998
train acc:  0.8203125
train loss:  0.41567981243133545
train gradient:  0.07711991973711461
iteration : 11999
train acc:  0.6640625
train loss:  0.5525999665260315
train gradient:  0.1663484242789125
iteration : 12000
train acc:  0.703125
train loss:  0.48798811435699463
train gradient:  0.13336566682752177
iteration : 12001
train acc:  0.71875
train loss:  0.4929931163787842
train gradient:  0.130577713298744
iteration : 12002
train acc:  0.6796875
train loss:  0.532227635383606
train gradient:  0.17279233529698174
iteration : 12003
train acc:  0.7265625
train loss:  0.45702245831489563
train gradient:  0.13775332041404453
iteration : 12004
train acc:  0.734375
train loss:  0.5000215172767639
train gradient:  0.13653822628141132
iteration : 12005
train acc:  0.7109375
train loss:  0.5029807090759277
train gradient:  0.12251931277004355
iteration : 12006
train acc:  0.75
train loss:  0.4628704786300659
train gradient:  0.12727719940151427
iteration : 12007
train acc:  0.75
train loss:  0.4663871228694916
train gradient:  0.11031932492581689
iteration : 12008
train acc:  0.75
train loss:  0.4813041687011719
train gradient:  0.11545392424179987
iteration : 12009
train acc:  0.703125
train loss:  0.529521644115448
train gradient:  0.14977816150640044
iteration : 12010
train acc:  0.7109375
train loss:  0.5549881458282471
train gradient:  0.1530752690106606
iteration : 12011
train acc:  0.7421875
train loss:  0.4950965344905853
train gradient:  0.11035883202196473
iteration : 12012
train acc:  0.71875
train loss:  0.5176475048065186
train gradient:  0.1698699417613862
iteration : 12013
train acc:  0.8046875
train loss:  0.4158056676387787
train gradient:  0.0835917030510521
iteration : 12014
train acc:  0.765625
train loss:  0.5258455276489258
train gradient:  0.1789952578767447
iteration : 12015
train acc:  0.84375
train loss:  0.4075937271118164
train gradient:  0.11068648628108976
iteration : 12016
train acc:  0.7890625
train loss:  0.46981146931648254
train gradient:  0.15220749090352947
iteration : 12017
train acc:  0.703125
train loss:  0.5112677216529846
train gradient:  0.1243115585985863
iteration : 12018
train acc:  0.828125
train loss:  0.47275954484939575
train gradient:  0.153677613901139
iteration : 12019
train acc:  0.7578125
train loss:  0.47447067499160767
train gradient:  0.12828849328757935
iteration : 12020
train acc:  0.7265625
train loss:  0.5190858840942383
train gradient:  0.15780858894517225
iteration : 12021
train acc:  0.75
train loss:  0.4549356698989868
train gradient:  0.10376090847009327
iteration : 12022
train acc:  0.7265625
train loss:  0.5161483287811279
train gradient:  0.13319601600916492
iteration : 12023
train acc:  0.84375
train loss:  0.41888535022735596
train gradient:  0.10958971999842308
iteration : 12024
train acc:  0.7734375
train loss:  0.4438803493976593
train gradient:  0.10641297840280647
iteration : 12025
train acc:  0.765625
train loss:  0.43147027492523193
train gradient:  0.1279684741306964
iteration : 12026
train acc:  0.7578125
train loss:  0.4246845543384552
train gradient:  0.08434958204931818
iteration : 12027
train acc:  0.765625
train loss:  0.46092161536216736
train gradient:  0.12970873205938338
iteration : 12028
train acc:  0.734375
train loss:  0.45099568367004395
train gradient:  0.09898525312337596
iteration : 12029
train acc:  0.7109375
train loss:  0.5326442718505859
train gradient:  0.1111936587851452
iteration : 12030
train acc:  0.75
train loss:  0.4903886318206787
train gradient:  0.14604289482194033
iteration : 12031
train acc:  0.65625
train loss:  0.6027670502662659
train gradient:  0.19548640547709464
iteration : 12032
train acc:  0.8046875
train loss:  0.44219547510147095
train gradient:  0.12225299174850995
iteration : 12033
train acc:  0.7265625
train loss:  0.5400528311729431
train gradient:  0.1278704249519883
iteration : 12034
train acc:  0.765625
train loss:  0.440945565700531
train gradient:  0.11941410050279949
iteration : 12035
train acc:  0.703125
train loss:  0.47816675901412964
train gradient:  0.11816053036784285
iteration : 12036
train acc:  0.7890625
train loss:  0.4433055818080902
train gradient:  0.10871352628621611
iteration : 12037
train acc:  0.78125
train loss:  0.4273175001144409
train gradient:  0.08729362115313667
iteration : 12038
train acc:  0.796875
train loss:  0.4565329849720001
train gradient:  0.11408686592377981
iteration : 12039
train acc:  0.7109375
train loss:  0.5216182470321655
train gradient:  0.14230471858346727
iteration : 12040
train acc:  0.7578125
train loss:  0.5571874976158142
train gradient:  0.1961164995986583
iteration : 12041
train acc:  0.734375
train loss:  0.5028206706047058
train gradient:  0.11141469273972632
iteration : 12042
train acc:  0.734375
train loss:  0.5626448392868042
train gradient:  0.157325470770528
iteration : 12043
train acc:  0.8203125
train loss:  0.3930055499076843
train gradient:  0.10394353261600954
iteration : 12044
train acc:  0.7421875
train loss:  0.543056309223175
train gradient:  0.15044340693251773
iteration : 12045
train acc:  0.796875
train loss:  0.4803219735622406
train gradient:  0.12072968471474183
iteration : 12046
train acc:  0.78125
train loss:  0.511329174041748
train gradient:  0.13783308124869836
iteration : 12047
train acc:  0.671875
train loss:  0.531173586845398
train gradient:  0.1241041471453026
iteration : 12048
train acc:  0.65625
train loss:  0.5889981985092163
train gradient:  0.15041837536655783
iteration : 12049
train acc:  0.7578125
train loss:  0.46069324016571045
train gradient:  0.10421193215588628
iteration : 12050
train acc:  0.7578125
train loss:  0.4658278524875641
train gradient:  0.11307579716466025
iteration : 12051
train acc:  0.734375
train loss:  0.4917123019695282
train gradient:  0.1420356975073947
iteration : 12052
train acc:  0.7421875
train loss:  0.5277758836746216
train gradient:  0.12381438674311887
iteration : 12053
train acc:  0.75
train loss:  0.4978673756122589
train gradient:  0.13625138370528433
iteration : 12054
train acc:  0.7421875
train loss:  0.5018129348754883
train gradient:  0.15678299718809802
iteration : 12055
train acc:  0.7734375
train loss:  0.4874570965766907
train gradient:  0.14916574715384356
iteration : 12056
train acc:  0.703125
train loss:  0.5531487464904785
train gradient:  0.14518624737857075
iteration : 12057
train acc:  0.796875
train loss:  0.450092077255249
train gradient:  0.10530379966769719
iteration : 12058
train acc:  0.703125
train loss:  0.5396867394447327
train gradient:  0.15558785779108403
iteration : 12059
train acc:  0.734375
train loss:  0.503129243850708
train gradient:  0.12867607550515375
iteration : 12060
train acc:  0.6875
train loss:  0.5762630701065063
train gradient:  0.16976055488517383
iteration : 12061
train acc:  0.7265625
train loss:  0.48365288972854614
train gradient:  0.10167203077160256
iteration : 12062
train acc:  0.7421875
train loss:  0.514685332775116
train gradient:  0.11942453559613822
iteration : 12063
train acc:  0.7734375
train loss:  0.43600279092788696
train gradient:  0.11457827432875282
iteration : 12064
train acc:  0.796875
train loss:  0.4383842647075653
train gradient:  0.08399090311441354
iteration : 12065
train acc:  0.7734375
train loss:  0.469577431678772
train gradient:  0.08545314562548832
iteration : 12066
train acc:  0.8046875
train loss:  0.45153167843818665
train gradient:  0.11231404580622453
iteration : 12067
train acc:  0.734375
train loss:  0.4743104577064514
train gradient:  0.11069294076989591
iteration : 12068
train acc:  0.8046875
train loss:  0.44365519285202026
train gradient:  0.10466556069521728
iteration : 12069
train acc:  0.703125
train loss:  0.5164846181869507
train gradient:  0.13971901444077442
iteration : 12070
train acc:  0.765625
train loss:  0.4195738434791565
train gradient:  0.10495844619990861
iteration : 12071
train acc:  0.7734375
train loss:  0.4716919958591461
train gradient:  0.10064405886250415
iteration : 12072
train acc:  0.6953125
train loss:  0.5269221663475037
train gradient:  0.14309708010662556
iteration : 12073
train acc:  0.78125
train loss:  0.44415009021759033
train gradient:  0.10577362473072395
iteration : 12074
train acc:  0.796875
train loss:  0.41498008370399475
train gradient:  0.08432579590540869
iteration : 12075
train acc:  0.6796875
train loss:  0.5610415935516357
train gradient:  0.1465934695835275
iteration : 12076
train acc:  0.796875
train loss:  0.4518756568431854
train gradient:  0.11454191965515259
iteration : 12077
train acc:  0.71875
train loss:  0.5073387622833252
train gradient:  0.16749129537546548
iteration : 12078
train acc:  0.7890625
train loss:  0.4400269091129303
train gradient:  0.1217775454181357
iteration : 12079
train acc:  0.765625
train loss:  0.45773494243621826
train gradient:  0.12146685284290558
iteration : 12080
train acc:  0.7109375
train loss:  0.545554518699646
train gradient:  0.15384992209102477
iteration : 12081
train acc:  0.7578125
train loss:  0.49249589443206787
train gradient:  0.13595621474304592
iteration : 12082
train acc:  0.734375
train loss:  0.5129241943359375
train gradient:  0.13265800858464233
iteration : 12083
train acc:  0.7734375
train loss:  0.45104265213012695
train gradient:  0.09621618134836554
iteration : 12084
train acc:  0.75
train loss:  0.49183887243270874
train gradient:  0.13438683954618608
iteration : 12085
train acc:  0.78125
train loss:  0.46331048011779785
train gradient:  0.1297347759238317
iteration : 12086
train acc:  0.75
train loss:  0.5157803893089294
train gradient:  0.17196760765329425
iteration : 12087
train acc:  0.7421875
train loss:  0.4942116439342499
train gradient:  0.14632364908498713
iteration : 12088
train acc:  0.78125
train loss:  0.4522385001182556
train gradient:  0.13811410584962774
iteration : 12089
train acc:  0.7734375
train loss:  0.48860663175582886
train gradient:  0.09391259186423968
iteration : 12090
train acc:  0.75
train loss:  0.5328883528709412
train gradient:  0.13210138028786467
iteration : 12091
train acc:  0.71875
train loss:  0.4872662425041199
train gradient:  0.1434047939389338
iteration : 12092
train acc:  0.75
train loss:  0.4559325873851776
train gradient:  0.10789125587788254
iteration : 12093
train acc:  0.703125
train loss:  0.510474443435669
train gradient:  0.1451136641116323
iteration : 12094
train acc:  0.7890625
train loss:  0.482382595539093
train gradient:  0.13515774591018367
iteration : 12095
train acc:  0.7265625
train loss:  0.5003372430801392
train gradient:  0.15277490590922516
iteration : 12096
train acc:  0.7265625
train loss:  0.5152934789657593
train gradient:  0.13801808616458744
iteration : 12097
train acc:  0.75
train loss:  0.46213865280151367
train gradient:  0.13456091964752143
iteration : 12098
train acc:  0.7109375
train loss:  0.5038119554519653
train gradient:  0.17321230153677897
iteration : 12099
train acc:  0.734375
train loss:  0.47056952118873596
train gradient:  0.10531810795868864
iteration : 12100
train acc:  0.75
train loss:  0.5026286244392395
train gradient:  0.11369821782259835
iteration : 12101
train acc:  0.7578125
train loss:  0.5074614882469177
train gradient:  0.14638714715307988
iteration : 12102
train acc:  0.7109375
train loss:  0.5512368083000183
train gradient:  0.15952858112617227
iteration : 12103
train acc:  0.7109375
train loss:  0.5371408462524414
train gradient:  0.12890720076485945
iteration : 12104
train acc:  0.703125
train loss:  0.5181442499160767
train gradient:  0.1329833726536528
iteration : 12105
train acc:  0.78125
train loss:  0.47871434688568115
train gradient:  0.1391573294883373
iteration : 12106
train acc:  0.75
train loss:  0.4772927165031433
train gradient:  0.1507199309982613
iteration : 12107
train acc:  0.703125
train loss:  0.49632224440574646
train gradient:  0.13115739106727484
iteration : 12108
train acc:  0.78125
train loss:  0.42455756664276123
train gradient:  0.0885984151951109
iteration : 12109
train acc:  0.734375
train loss:  0.47841134667396545
train gradient:  0.1290329552076154
iteration : 12110
train acc:  0.8046875
train loss:  0.49412795901298523
train gradient:  0.13507654260883273
iteration : 12111
train acc:  0.734375
train loss:  0.5230659246444702
train gradient:  0.11577129698138103
iteration : 12112
train acc:  0.7109375
train loss:  0.5247554779052734
train gradient:  0.14770976109294726
iteration : 12113
train acc:  0.7890625
train loss:  0.4472777843475342
train gradient:  0.11207615077107756
iteration : 12114
train acc:  0.71875
train loss:  0.470051109790802
train gradient:  0.11603359342999933
iteration : 12115
train acc:  0.734375
train loss:  0.5229992270469666
train gradient:  0.12076767949283124
iteration : 12116
train acc:  0.7578125
train loss:  0.48016464710235596
train gradient:  0.11655506611988728
iteration : 12117
train acc:  0.71875
train loss:  0.5401381850242615
train gradient:  0.1611999149973068
iteration : 12118
train acc:  0.7734375
train loss:  0.43162837624549866
train gradient:  0.11147546296813358
iteration : 12119
train acc:  0.7265625
train loss:  0.6041250228881836
train gradient:  0.1626151420609277
iteration : 12120
train acc:  0.7265625
train loss:  0.4587640166282654
train gradient:  0.086042668773462
iteration : 12121
train acc:  0.7265625
train loss:  0.5203390121459961
train gradient:  0.12809880341806718
iteration : 12122
train acc:  0.71875
train loss:  0.5533665418624878
train gradient:  0.1602496316399475
iteration : 12123
train acc:  0.7890625
train loss:  0.43340104818344116
train gradient:  0.13056900561409157
iteration : 12124
train acc:  0.7890625
train loss:  0.444088876247406
train gradient:  0.10482966115645088
iteration : 12125
train acc:  0.7734375
train loss:  0.4500584900379181
train gradient:  0.09012755257941094
iteration : 12126
train acc:  0.6875
train loss:  0.5117583274841309
train gradient:  0.1468508920830919
iteration : 12127
train acc:  0.7265625
train loss:  0.49567246437072754
train gradient:  0.14685138838275438
iteration : 12128
train acc:  0.75
train loss:  0.48499324917793274
train gradient:  0.09453999374450082
iteration : 12129
train acc:  0.71875
train loss:  0.5207106471061707
train gradient:  0.12945330915906544
iteration : 12130
train acc:  0.6796875
train loss:  0.5409756898880005
train gradient:  0.12585225561023294
iteration : 12131
train acc:  0.75
train loss:  0.472914457321167
train gradient:  0.11719908072738404
iteration : 12132
train acc:  0.7734375
train loss:  0.47866395115852356
train gradient:  0.1406772364750662
iteration : 12133
train acc:  0.7109375
train loss:  0.5241214036941528
train gradient:  0.12374489663512657
iteration : 12134
train acc:  0.703125
train loss:  0.5616130232810974
train gradient:  0.13997648311216504
iteration : 12135
train acc:  0.71875
train loss:  0.5396758317947388
train gradient:  0.14076165073498015
iteration : 12136
train acc:  0.734375
train loss:  0.5387412309646606
train gradient:  0.13082064825713285
iteration : 12137
train acc:  0.7109375
train loss:  0.4993647336959839
train gradient:  0.11722403197301973
iteration : 12138
train acc:  0.7109375
train loss:  0.5544031858444214
train gradient:  0.15168466337478204
iteration : 12139
train acc:  0.8125
train loss:  0.4362129867076874
train gradient:  0.10244952179967622
iteration : 12140
train acc:  0.71875
train loss:  0.47186824679374695
train gradient:  0.1045073186748796
iteration : 12141
train acc:  0.8125
train loss:  0.451978474855423
train gradient:  0.10948140060293882
iteration : 12142
train acc:  0.7890625
train loss:  0.47280535101890564
train gradient:  0.13055578727672065
iteration : 12143
train acc:  0.7265625
train loss:  0.5417978167533875
train gradient:  0.14687234459594498
iteration : 12144
train acc:  0.7265625
train loss:  0.4782188832759857
train gradient:  0.09911253128697638
iteration : 12145
train acc:  0.796875
train loss:  0.4731854498386383
train gradient:  0.11846057242672116
iteration : 12146
train acc:  0.734375
train loss:  0.48051780462265015
train gradient:  0.10532886660440188
iteration : 12147
train acc:  0.7109375
train loss:  0.488534152507782
train gradient:  0.12990188986533507
iteration : 12148
train acc:  0.7890625
train loss:  0.4243853986263275
train gradient:  0.08834058179311517
iteration : 12149
train acc:  0.7421875
train loss:  0.524465024471283
train gradient:  0.13890761373606442
iteration : 12150
train acc:  0.8359375
train loss:  0.3639320135116577
train gradient:  0.0711382962655925
iteration : 12151
train acc:  0.8203125
train loss:  0.42020660638809204
train gradient:  0.09093876679297926
iteration : 12152
train acc:  0.7265625
train loss:  0.4945414662361145
train gradient:  0.14165570254673776
iteration : 12153
train acc:  0.765625
train loss:  0.46367770433425903
train gradient:  0.12952669628262728
iteration : 12154
train acc:  0.65625
train loss:  0.5589117407798767
train gradient:  0.17826460285012563
iteration : 12155
train acc:  0.78125
train loss:  0.44978800415992737
train gradient:  0.09806565818720935
iteration : 12156
train acc:  0.765625
train loss:  0.49255213141441345
train gradient:  0.1415028526138694
iteration : 12157
train acc:  0.71875
train loss:  0.5064635276794434
train gradient:  0.13253553899484793
iteration : 12158
train acc:  0.734375
train loss:  0.5241189002990723
train gradient:  0.13957599730241416
iteration : 12159
train acc:  0.703125
train loss:  0.546730637550354
train gradient:  0.15853105160907732
iteration : 12160
train acc:  0.7265625
train loss:  0.5019451379776001
train gradient:  0.14344771997485364
iteration : 12161
train acc:  0.7734375
train loss:  0.48910391330718994
train gradient:  0.14110211862764455
iteration : 12162
train acc:  0.7421875
train loss:  0.5202335119247437
train gradient:  0.11703456007033178
iteration : 12163
train acc:  0.734375
train loss:  0.5106003284454346
train gradient:  0.12640209340345
iteration : 12164
train acc:  0.7421875
train loss:  0.478662371635437
train gradient:  0.11038703771306518
iteration : 12165
train acc:  0.7421875
train loss:  0.4620685577392578
train gradient:  0.1136568676452257
iteration : 12166
train acc:  0.7265625
train loss:  0.4929654002189636
train gradient:  0.10244482588582329
iteration : 12167
train acc:  0.7578125
train loss:  0.46515533328056335
train gradient:  0.10780014996203226
iteration : 12168
train acc:  0.765625
train loss:  0.49310094118118286
train gradient:  0.11602540854808766
iteration : 12169
train acc:  0.75
train loss:  0.48966681957244873
train gradient:  0.11284610625603216
iteration : 12170
train acc:  0.71875
train loss:  0.5028549432754517
train gradient:  0.1395335632180159
iteration : 12171
train acc:  0.796875
train loss:  0.4644523561000824
train gradient:  0.1179121482540209
iteration : 12172
train acc:  0.7421875
train loss:  0.5041544437408447
train gradient:  0.1279368838291049
iteration : 12173
train acc:  0.71875
train loss:  0.48330920934677124
train gradient:  0.11799177354393879
iteration : 12174
train acc:  0.734375
train loss:  0.4595096707344055
train gradient:  0.11720320110979213
iteration : 12175
train acc:  0.71875
train loss:  0.535885214805603
train gradient:  0.1555212410108341
iteration : 12176
train acc:  0.78125
train loss:  0.4383704960346222
train gradient:  0.10220239989984053
iteration : 12177
train acc:  0.6640625
train loss:  0.5904518365859985
train gradient:  0.15416558349222087
iteration : 12178
train acc:  0.765625
train loss:  0.4954468607902527
train gradient:  0.12264316982749617
iteration : 12179
train acc:  0.734375
train loss:  0.4427955448627472
train gradient:  0.10038658051162425
iteration : 12180
train acc:  0.7578125
train loss:  0.5806713104248047
train gradient:  0.13100677703972827
iteration : 12181
train acc:  0.8046875
train loss:  0.44142571091651917
train gradient:  0.0913566959809404
iteration : 12182
train acc:  0.7265625
train loss:  0.44979873299598694
train gradient:  0.08965604405505379
iteration : 12183
train acc:  0.7265625
train loss:  0.5231919884681702
train gradient:  0.13294525183593164
iteration : 12184
train acc:  0.7421875
train loss:  0.5007349252700806
train gradient:  0.10508776097801414
iteration : 12185
train acc:  0.734375
train loss:  0.5019060373306274
train gradient:  0.12321602072003787
iteration : 12186
train acc:  0.7421875
train loss:  0.49063417315483093
train gradient:  0.11421868414355527
iteration : 12187
train acc:  0.78125
train loss:  0.46005964279174805
train gradient:  0.13267283345903347
iteration : 12188
train acc:  0.6640625
train loss:  0.5491375923156738
train gradient:  0.14098966414498754
iteration : 12189
train acc:  0.7734375
train loss:  0.4645509123802185
train gradient:  0.12226836026460472
iteration : 12190
train acc:  0.6875
train loss:  0.5837953090667725
train gradient:  0.15477760273466626
iteration : 12191
train acc:  0.703125
train loss:  0.5008342862129211
train gradient:  0.1284868654696772
iteration : 12192
train acc:  0.7109375
train loss:  0.5072376728057861
train gradient:  0.13518910625199515
iteration : 12193
train acc:  0.7734375
train loss:  0.45430803298950195
train gradient:  0.11016332724163162
iteration : 12194
train acc:  0.7109375
train loss:  0.5191740393638611
train gradient:  0.13410514238923688
iteration : 12195
train acc:  0.7421875
train loss:  0.5016986727714539
train gradient:  0.14029129599321633
iteration : 12196
train acc:  0.7578125
train loss:  0.4726412296295166
train gradient:  0.11691696650961038
iteration : 12197
train acc:  0.703125
train loss:  0.47841712832450867
train gradient:  0.09376565153073092
iteration : 12198
train acc:  0.6875
train loss:  0.49132928252220154
train gradient:  0.11287134699056114
iteration : 12199
train acc:  0.8515625
train loss:  0.3950048089027405
train gradient:  0.0849070971497411
iteration : 12200
train acc:  0.78125
train loss:  0.48537489771842957
train gradient:  0.14567425392696118
iteration : 12201
train acc:  0.734375
train loss:  0.4804421067237854
train gradient:  0.10147835220101535
iteration : 12202
train acc:  0.8125
train loss:  0.4072355628013611
train gradient:  0.09790517936296435
iteration : 12203
train acc:  0.7421875
train loss:  0.46529620885849
train gradient:  0.1182480686746837
iteration : 12204
train acc:  0.78125
train loss:  0.44969815015792847
train gradient:  0.12658668343473678
iteration : 12205
train acc:  0.8359375
train loss:  0.43557754158973694
train gradient:  0.10005633155700693
iteration : 12206
train acc:  0.7890625
train loss:  0.5315374135971069
train gradient:  0.17059857774265652
iteration : 12207
train acc:  0.7109375
train loss:  0.5269778966903687
train gradient:  0.1390666097949297
iteration : 12208
train acc:  0.71875
train loss:  0.5382946729660034
train gradient:  0.14185656662754512
iteration : 12209
train acc:  0.7734375
train loss:  0.4734204113483429
train gradient:  0.11328627725740413
iteration : 12210
train acc:  0.7421875
train loss:  0.4751206338405609
train gradient:  0.12286925618168433
iteration : 12211
train acc:  0.6953125
train loss:  0.5101211071014404
train gradient:  0.13423478274834444
iteration : 12212
train acc:  0.7578125
train loss:  0.5509185791015625
train gradient:  0.14475885362812435
iteration : 12213
train acc:  0.7890625
train loss:  0.46412891149520874
train gradient:  0.14269678760840337
iteration : 12214
train acc:  0.7265625
train loss:  0.4843093752861023
train gradient:  0.12220395293653702
iteration : 12215
train acc:  0.7734375
train loss:  0.48855364322662354
train gradient:  0.10804216107446282
iteration : 12216
train acc:  0.75
train loss:  0.5067708492279053
train gradient:  0.17772778953720528
iteration : 12217
train acc:  0.75
train loss:  0.4730433225631714
train gradient:  0.11527483076399155
iteration : 12218
train acc:  0.7578125
train loss:  0.47506165504455566
train gradient:  0.13258431065715579
iteration : 12219
train acc:  0.625
train loss:  0.6452706456184387
train gradient:  0.19568337722868118
iteration : 12220
train acc:  0.703125
train loss:  0.526870608329773
train gradient:  0.1293331472230337
iteration : 12221
train acc:  0.7109375
train loss:  0.5032016038894653
train gradient:  0.12227012654583126
iteration : 12222
train acc:  0.6953125
train loss:  0.4965225160121918
train gradient:  0.11845406404133609
iteration : 12223
train acc:  0.671875
train loss:  0.5501300096511841
train gradient:  0.15414759896703872
iteration : 12224
train acc:  0.765625
train loss:  0.4229086637496948
train gradient:  0.11057821353477944
iteration : 12225
train acc:  0.7109375
train loss:  0.5200966000556946
train gradient:  0.13936044506288978
iteration : 12226
train acc:  0.78125
train loss:  0.44264352321624756
train gradient:  0.10399389695582542
iteration : 12227
train acc:  0.7578125
train loss:  0.4815746247768402
train gradient:  0.11099202764847936
iteration : 12228
train acc:  0.71875
train loss:  0.47776156663894653
train gradient:  0.10503408235918085
iteration : 12229
train acc:  0.625
train loss:  0.5618191957473755
train gradient:  0.15732181021293326
iteration : 12230
train acc:  0.640625
train loss:  0.5074881315231323
train gradient:  0.12612401246054603
iteration : 12231
train acc:  0.7578125
train loss:  0.4882130026817322
train gradient:  0.11863013793685984
iteration : 12232
train acc:  0.6796875
train loss:  0.48585280776023865
train gradient:  0.08786733871073493
iteration : 12233
train acc:  0.7578125
train loss:  0.45879247784614563
train gradient:  0.10269411024549881
iteration : 12234
train acc:  0.7109375
train loss:  0.5414926409721375
train gradient:  0.1357028499942371
iteration : 12235
train acc:  0.734375
train loss:  0.44804924726486206
train gradient:  0.11157394836583306
iteration : 12236
train acc:  0.7421875
train loss:  0.47818589210510254
train gradient:  0.12392226040371938
iteration : 12237
train acc:  0.765625
train loss:  0.45552366971969604
train gradient:  0.09636801381862972
iteration : 12238
train acc:  0.75
train loss:  0.5036956071853638
train gradient:  0.12148936769714869
iteration : 12239
train acc:  0.71875
train loss:  0.556176483631134
train gradient:  0.13487736254633464
iteration : 12240
train acc:  0.765625
train loss:  0.47976481914520264
train gradient:  0.10586373291892144
iteration : 12241
train acc:  0.8359375
train loss:  0.4397907257080078
train gradient:  0.1257274656598223
iteration : 12242
train acc:  0.796875
train loss:  0.47238120436668396
train gradient:  0.10075660819647532
iteration : 12243
train acc:  0.734375
train loss:  0.5050109028816223
train gradient:  0.12347982076181148
iteration : 12244
train acc:  0.8046875
train loss:  0.4682755470275879
train gradient:  0.11089152182025366
iteration : 12245
train acc:  0.75
train loss:  0.5177381038665771
train gradient:  0.140800227475727
iteration : 12246
train acc:  0.6875
train loss:  0.5742349624633789
train gradient:  0.17317810798737088
iteration : 12247
train acc:  0.7421875
train loss:  0.4493153691291809
train gradient:  0.1266048637675435
iteration : 12248
train acc:  0.78125
train loss:  0.48401620984077454
train gradient:  0.11190035464462213
iteration : 12249
train acc:  0.7421875
train loss:  0.4866124391555786
train gradient:  0.10222715815095954
iteration : 12250
train acc:  0.75
train loss:  0.4826928973197937
train gradient:  0.11349395801623707
iteration : 12251
train acc:  0.796875
train loss:  0.4306592345237732
train gradient:  0.11006293696048068
iteration : 12252
train acc:  0.7109375
train loss:  0.5314038991928101
train gradient:  0.16638893888345768
iteration : 12253
train acc:  0.7734375
train loss:  0.4716196656227112
train gradient:  0.12343180134790725
iteration : 12254
train acc:  0.71875
train loss:  0.5287205576896667
train gradient:  0.12845269472416707
iteration : 12255
train acc:  0.75
train loss:  0.5352650880813599
train gradient:  0.1340523734787355
iteration : 12256
train acc:  0.671875
train loss:  0.5289071202278137
train gradient:  0.14281869759619312
iteration : 12257
train acc:  0.765625
train loss:  0.4461621642112732
train gradient:  0.09889259712612282
iteration : 12258
train acc:  0.7734375
train loss:  0.4588841199874878
train gradient:  0.10728659242017102
iteration : 12259
train acc:  0.71875
train loss:  0.46303266286849976
train gradient:  0.13050343115344776
iteration : 12260
train acc:  0.703125
train loss:  0.5520809888839722
train gradient:  0.13362492144512916
iteration : 12261
train acc:  0.8046875
train loss:  0.41630181670188904
train gradient:  0.09699308008750411
iteration : 12262
train acc:  0.75
train loss:  0.45163604617118835
train gradient:  0.1272850659712715
iteration : 12263
train acc:  0.71875
train loss:  0.5298513770103455
train gradient:  0.14863031586311523
iteration : 12264
train acc:  0.734375
train loss:  0.4957045316696167
train gradient:  0.1297905486775706
iteration : 12265
train acc:  0.765625
train loss:  0.4962221384048462
train gradient:  0.1437063680629561
iteration : 12266
train acc:  0.7890625
train loss:  0.4562969505786896
train gradient:  0.1038115124149947
iteration : 12267
train acc:  0.75
train loss:  0.4746667444705963
train gradient:  0.09886557076241582
iteration : 12268
train acc:  0.65625
train loss:  0.5124725103378296
train gradient:  0.1406562604565609
iteration : 12269
train acc:  0.7890625
train loss:  0.42681026458740234
train gradient:  0.08630797390190267
iteration : 12270
train acc:  0.7421875
train loss:  0.4981682598590851
train gradient:  0.11451154521714704
iteration : 12271
train acc:  0.6953125
train loss:  0.5662745237350464
train gradient:  0.15707958458094995
iteration : 12272
train acc:  0.765625
train loss:  0.46842488646507263
train gradient:  0.11100350555889633
iteration : 12273
train acc:  0.7265625
train loss:  0.5450217127799988
train gradient:  0.13301844286242243
iteration : 12274
train acc:  0.75
train loss:  0.5010635852813721
train gradient:  0.12412278870271441
iteration : 12275
train acc:  0.765625
train loss:  0.4890984892845154
train gradient:  0.16142935124787394
iteration : 12276
train acc:  0.703125
train loss:  0.5197622776031494
train gradient:  0.16497550308730258
iteration : 12277
train acc:  0.8046875
train loss:  0.46304285526275635
train gradient:  0.11570150256250605
iteration : 12278
train acc:  0.7265625
train loss:  0.5368150472640991
train gradient:  0.15324168496119028
iteration : 12279
train acc:  0.7734375
train loss:  0.44652414321899414
train gradient:  0.10227817637124435
iteration : 12280
train acc:  0.6953125
train loss:  0.5312708616256714
train gradient:  0.14683693121419905
iteration : 12281
train acc:  0.78125
train loss:  0.42697620391845703
train gradient:  0.1053471342312478
iteration : 12282
train acc:  0.7890625
train loss:  0.4416971802711487
train gradient:  0.10648954485312846
iteration : 12283
train acc:  0.6953125
train loss:  0.5265556573867798
train gradient:  0.14759080870179492
iteration : 12284
train acc:  0.7265625
train loss:  0.537568986415863
train gradient:  0.14163476854650986
iteration : 12285
train acc:  0.7421875
train loss:  0.4748724699020386
train gradient:  0.11019306325504141
iteration : 12286
train acc:  0.7109375
train loss:  0.5011366605758667
train gradient:  0.1181233035192767
iteration : 12287
train acc:  0.71875
train loss:  0.5430479645729065
train gradient:  0.12998758532557816
iteration : 12288
train acc:  0.7578125
train loss:  0.4792691767215729
train gradient:  0.10134383398631591
iteration : 12289
train acc:  0.71875
train loss:  0.49643778800964355
train gradient:  0.12980008236102736
iteration : 12290
train acc:  0.7265625
train loss:  0.4960496723651886
train gradient:  0.1283669570887075
iteration : 12291
train acc:  0.734375
train loss:  0.5073363780975342
train gradient:  0.12863309751684748
iteration : 12292
train acc:  0.7578125
train loss:  0.4526381492614746
train gradient:  0.11984988134970172
iteration : 12293
train acc:  0.7890625
train loss:  0.4236275255680084
train gradient:  0.08552766279831013
iteration : 12294
train acc:  0.71875
train loss:  0.5304739475250244
train gradient:  0.11293619066262596
iteration : 12295
train acc:  0.8125
train loss:  0.43926191329956055
train gradient:  0.1112879226680552
iteration : 12296
train acc:  0.8125
train loss:  0.4315888285636902
train gradient:  0.1038014872612338
iteration : 12297
train acc:  0.7109375
train loss:  0.5002937912940979
train gradient:  0.14749246659517395
iteration : 12298
train acc:  0.71875
train loss:  0.4645019769668579
train gradient:  0.1062145665184511
iteration : 12299
train acc:  0.7109375
train loss:  0.5481358766555786
train gradient:  0.1591620800186541
iteration : 12300
train acc:  0.78125
train loss:  0.4594551622867584
train gradient:  0.11619942571579603
iteration : 12301
train acc:  0.765625
train loss:  0.45717787742614746
train gradient:  0.11215224474528303
iteration : 12302
train acc:  0.71875
train loss:  0.5238878130912781
train gradient:  0.1263885203753003
iteration : 12303
train acc:  0.8046875
train loss:  0.3995824456214905
train gradient:  0.08229194038524586
iteration : 12304
train acc:  0.734375
train loss:  0.48089590668678284
train gradient:  0.1312526942795138
iteration : 12305
train acc:  0.75
train loss:  0.46901223063468933
train gradient:  0.15964573452116956
iteration : 12306
train acc:  0.7734375
train loss:  0.4776846170425415
train gradient:  0.10966355139915882
iteration : 12307
train acc:  0.7421875
train loss:  0.5150179266929626
train gradient:  0.13224829117964085
iteration : 12308
train acc:  0.71875
train loss:  0.6007579565048218
train gradient:  0.14743157202563384
iteration : 12309
train acc:  0.78125
train loss:  0.4919081926345825
train gradient:  0.13253562442943473
iteration : 12310
train acc:  0.75
train loss:  0.4624825417995453
train gradient:  0.1243828567263263
iteration : 12311
train acc:  0.7421875
train loss:  0.5140666961669922
train gradient:  0.11222328782379823
iteration : 12312
train acc:  0.703125
train loss:  0.4844839572906494
train gradient:  0.11144976100390998
iteration : 12313
train acc:  0.71875
train loss:  0.588781476020813
train gradient:  0.1765791747094349
iteration : 12314
train acc:  0.8125
train loss:  0.43066492676734924
train gradient:  0.10729564136393742
iteration : 12315
train acc:  0.7734375
train loss:  0.42834359407424927
train gradient:  0.12404756530351724
iteration : 12316
train acc:  0.734375
train loss:  0.49235889315605164
train gradient:  0.13228701978414303
iteration : 12317
train acc:  0.734375
train loss:  0.46577757596969604
train gradient:  0.12477584970882387
iteration : 12318
train acc:  0.71875
train loss:  0.45422446727752686
train gradient:  0.11372960717784629
iteration : 12319
train acc:  0.7265625
train loss:  0.49058616161346436
train gradient:  0.12299919584793538
iteration : 12320
train acc:  0.734375
train loss:  0.47638702392578125
train gradient:  0.15215887839026238
iteration : 12321
train acc:  0.7421875
train loss:  0.49558767676353455
train gradient:  0.1098900214597977
iteration : 12322
train acc:  0.71875
train loss:  0.5011252164840698
train gradient:  0.1327695512674752
iteration : 12323
train acc:  0.7578125
train loss:  0.4747529625892639
train gradient:  0.13890643213284942
iteration : 12324
train acc:  0.765625
train loss:  0.4842534065246582
train gradient:  0.1343254806973086
iteration : 12325
train acc:  0.6796875
train loss:  0.5616447329521179
train gradient:  0.19073055597836308
iteration : 12326
train acc:  0.8125
train loss:  0.4527396261692047
train gradient:  0.13576275562836276
iteration : 12327
train acc:  0.78125
train loss:  0.4859124720096588
train gradient:  0.12159466225291472
iteration : 12328
train acc:  0.78125
train loss:  0.46452823281288147
train gradient:  0.11038753336800278
iteration : 12329
train acc:  0.7109375
train loss:  0.5486786365509033
train gradient:  0.12570287113755296
iteration : 12330
train acc:  0.78125
train loss:  0.492113322019577
train gradient:  0.11706348541307325
iteration : 12331
train acc:  0.75
train loss:  0.5186804533004761
train gradient:  0.1431222031279233
iteration : 12332
train acc:  0.71875
train loss:  0.5345181822776794
train gradient:  0.14765207372681366
iteration : 12333
train acc:  0.6875
train loss:  0.5501904487609863
train gradient:  0.16336256272050637
iteration : 12334
train acc:  0.703125
train loss:  0.5075591802597046
train gradient:  0.15072807631187785
iteration : 12335
train acc:  0.7578125
train loss:  0.4635131061077118
train gradient:  0.12035187000757082
iteration : 12336
train acc:  0.7265625
train loss:  0.5352563858032227
train gradient:  0.13050772282765538
iteration : 12337
train acc:  0.7890625
train loss:  0.4619803726673126
train gradient:  0.1121327497523392
iteration : 12338
train acc:  0.7265625
train loss:  0.4715757966041565
train gradient:  0.095249636955275
iteration : 12339
train acc:  0.7421875
train loss:  0.4787174463272095
train gradient:  0.11740615476780446
iteration : 12340
train acc:  0.7421875
train loss:  0.5023635625839233
train gradient:  0.11752305935121436
iteration : 12341
train acc:  0.734375
train loss:  0.49918946623802185
train gradient:  0.19247624315190037
iteration : 12342
train acc:  0.84375
train loss:  0.3783375024795532
train gradient:  0.06752967460266593
iteration : 12343
train acc:  0.7578125
train loss:  0.5243211984634399
train gradient:  0.134802453466645
iteration : 12344
train acc:  0.703125
train loss:  0.48190370202064514
train gradient:  0.1430517223164562
iteration : 12345
train acc:  0.734375
train loss:  0.4819062352180481
train gradient:  0.0982036235600518
iteration : 12346
train acc:  0.8203125
train loss:  0.4262734651565552
train gradient:  0.12186800831561183
iteration : 12347
train acc:  0.6953125
train loss:  0.5228034257888794
train gradient:  0.1400800374453866
iteration : 12348
train acc:  0.78125
train loss:  0.48393553495407104
train gradient:  0.12086346021703546
iteration : 12349
train acc:  0.734375
train loss:  0.49525681138038635
train gradient:  0.10407484084857073
iteration : 12350
train acc:  0.71875
train loss:  0.49423134326934814
train gradient:  0.15055875347954273
iteration : 12351
train acc:  0.734375
train loss:  0.48065871000289917
train gradient:  0.11552522496725236
iteration : 12352
train acc:  0.796875
train loss:  0.48838111758232117
train gradient:  0.1064501771740091
iteration : 12353
train acc:  0.7890625
train loss:  0.4417445659637451
train gradient:  0.10193430325613524
iteration : 12354
train acc:  0.7421875
train loss:  0.4740777313709259
train gradient:  0.1083393109617435
iteration : 12355
train acc:  0.71875
train loss:  0.5246696472167969
train gradient:  0.15184718078257825
iteration : 12356
train acc:  0.7421875
train loss:  0.4779261648654938
train gradient:  0.13566988148414866
iteration : 12357
train acc:  0.75
train loss:  0.4680139124393463
train gradient:  0.1649029201796196
iteration : 12358
train acc:  0.6484375
train loss:  0.588887095451355
train gradient:  0.17457139272024375
iteration : 12359
train acc:  0.734375
train loss:  0.4690898060798645
train gradient:  0.1226430987880817
iteration : 12360
train acc:  0.75
train loss:  0.501067578792572
train gradient:  0.11202653748610814
iteration : 12361
train acc:  0.75
train loss:  0.483505517244339
train gradient:  0.1433346596299446
iteration : 12362
train acc:  0.6875
train loss:  0.5102605819702148
train gradient:  0.15673342654622796
iteration : 12363
train acc:  0.75
train loss:  0.49142271280288696
train gradient:  0.12543866649168234
iteration : 12364
train acc:  0.7421875
train loss:  0.48500022292137146
train gradient:  0.12208051836900653
iteration : 12365
train acc:  0.765625
train loss:  0.49837565422058105
train gradient:  0.11356961353754902
iteration : 12366
train acc:  0.7578125
train loss:  0.4429227411746979
train gradient:  0.09056644215337581
iteration : 12367
train acc:  0.78125
train loss:  0.4507855176925659
train gradient:  0.10577658964074893
iteration : 12368
train acc:  0.7109375
train loss:  0.5689679384231567
train gradient:  0.17270740505056156
iteration : 12369
train acc:  0.8125
train loss:  0.40412187576293945
train gradient:  0.09102787923827867
iteration : 12370
train acc:  0.734375
train loss:  0.5050868988037109
train gradient:  0.1289706278768163
iteration : 12371
train acc:  0.765625
train loss:  0.4596900939941406
train gradient:  0.10524479097444887
iteration : 12372
train acc:  0.6015625
train loss:  0.6268705725669861
train gradient:  0.18790828330813641
iteration : 12373
train acc:  0.75
train loss:  0.47656095027923584
train gradient:  0.10575086003770981
iteration : 12374
train acc:  0.75
train loss:  0.5502923727035522
train gradient:  0.16635384826566207
iteration : 12375
train acc:  0.734375
train loss:  0.4833599030971527
train gradient:  0.12766446705915996
iteration : 12376
train acc:  0.625
train loss:  0.6180658340454102
train gradient:  0.20050858760305806
iteration : 12377
train acc:  0.6953125
train loss:  0.5459645986557007
train gradient:  0.1535753291428879
iteration : 12378
train acc:  0.8046875
train loss:  0.44422900676727295
train gradient:  0.08491731730310471
iteration : 12379
train acc:  0.7734375
train loss:  0.4375370740890503
train gradient:  0.10891665815230762
iteration : 12380
train acc:  0.7890625
train loss:  0.44126659631729126
train gradient:  0.1023255788559148
iteration : 12381
train acc:  0.7578125
train loss:  0.44676733016967773
train gradient:  0.08704926038761569
iteration : 12382
train acc:  0.765625
train loss:  0.48185110092163086
train gradient:  0.12810113658436229
iteration : 12383
train acc:  0.8046875
train loss:  0.4342575967311859
train gradient:  0.09679546873931667
iteration : 12384
train acc:  0.703125
train loss:  0.5772171020507812
train gradient:  0.14874877317505947
iteration : 12385
train acc:  0.7109375
train loss:  0.5481472015380859
train gradient:  0.18158228619098815
iteration : 12386
train acc:  0.75
train loss:  0.5171180963516235
train gradient:  0.120010315736503
iteration : 12387
train acc:  0.7890625
train loss:  0.4520038366317749
train gradient:  0.12081467887367384
iteration : 12388
train acc:  0.7734375
train loss:  0.4853156805038452
train gradient:  0.09759710687575537
iteration : 12389
train acc:  0.703125
train loss:  0.5084462761878967
train gradient:  0.1283085786446552
iteration : 12390
train acc:  0.7578125
train loss:  0.4822826683521271
train gradient:  0.10472413807268059
iteration : 12391
train acc:  0.765625
train loss:  0.4576050937175751
train gradient:  0.12532486240251967
iteration : 12392
train acc:  0.6953125
train loss:  0.5066676139831543
train gradient:  0.18340621702209076
iteration : 12393
train acc:  0.6875
train loss:  0.46508342027664185
train gradient:  0.11864291392734998
iteration : 12394
train acc:  0.75
train loss:  0.45938026905059814
train gradient:  0.13070559553818809
iteration : 12395
train acc:  0.796875
train loss:  0.47443336248397827
train gradient:  0.12722698707814528
iteration : 12396
train acc:  0.71875
train loss:  0.5393093824386597
train gradient:  0.17322235133989905
iteration : 12397
train acc:  0.6875
train loss:  0.5196112394332886
train gradient:  0.10888790653325751
iteration : 12398
train acc:  0.7578125
train loss:  0.466716468334198
train gradient:  0.13857922705622455
iteration : 12399
train acc:  0.734375
train loss:  0.5261145830154419
train gradient:  0.14698000471315492
iteration : 12400
train acc:  0.796875
train loss:  0.4657742977142334
train gradient:  0.09189513587733024
iteration : 12401
train acc:  0.7578125
train loss:  0.4616204500198364
train gradient:  0.11937189978630401
iteration : 12402
train acc:  0.7578125
train loss:  0.44889184832572937
train gradient:  0.11861899444662581
iteration : 12403
train acc:  0.6953125
train loss:  0.5501120090484619
train gradient:  0.16697164812389648
iteration : 12404
train acc:  0.78125
train loss:  0.44509655237197876
train gradient:  0.09900444335972856
iteration : 12405
train acc:  0.765625
train loss:  0.47903382778167725
train gradient:  0.14054490847265294
iteration : 12406
train acc:  0.75
train loss:  0.5103199481964111
train gradient:  0.11489464330374015
iteration : 12407
train acc:  0.7578125
train loss:  0.48553305864334106
train gradient:  0.12138971485395697
iteration : 12408
train acc:  0.7890625
train loss:  0.4845033288002014
train gradient:  0.12757039445001514
iteration : 12409
train acc:  0.6875
train loss:  0.5629599094390869
train gradient:  0.14275714268657277
iteration : 12410
train acc:  0.796875
train loss:  0.4173768162727356
train gradient:  0.10670341573600883
iteration : 12411
train acc:  0.75
train loss:  0.4760613739490509
train gradient:  0.17164700016347212
iteration : 12412
train acc:  0.6796875
train loss:  0.5146206617355347
train gradient:  0.14178257197613764
iteration : 12413
train acc:  0.6796875
train loss:  0.5484349727630615
train gradient:  0.18204934069105388
iteration : 12414
train acc:  0.6328125
train loss:  0.5511385202407837
train gradient:  0.13514468403554014
iteration : 12415
train acc:  0.734375
train loss:  0.5287489295005798
train gradient:  0.1594548923792768
iteration : 12416
train acc:  0.7421875
train loss:  0.5008019804954529
train gradient:  0.12863959967557764
iteration : 12417
train acc:  0.7578125
train loss:  0.49790745973587036
train gradient:  0.12793293731969077
iteration : 12418
train acc:  0.75
train loss:  0.4812427759170532
train gradient:  0.10636090321300247
iteration : 12419
train acc:  0.78125
train loss:  0.42333394289016724
train gradient:  0.0964150629179951
iteration : 12420
train acc:  0.734375
train loss:  0.5489000082015991
train gradient:  0.17704852447470804
iteration : 12421
train acc:  0.734375
train loss:  0.48251253366470337
train gradient:  0.10385939549376755
iteration : 12422
train acc:  0.7109375
train loss:  0.550790548324585
train gradient:  0.13849724494101295
iteration : 12423
train acc:  0.734375
train loss:  0.4828217625617981
train gradient:  0.1090930225123649
iteration : 12424
train acc:  0.765625
train loss:  0.5029029846191406
train gradient:  0.12240998731394798
iteration : 12425
train acc:  0.7890625
train loss:  0.4382835328578949
train gradient:  0.09707703278778615
iteration : 12426
train acc:  0.7734375
train loss:  0.4311015009880066
train gradient:  0.10103102463645208
iteration : 12427
train acc:  0.7265625
train loss:  0.5310361385345459
train gradient:  0.17737831183276112
iteration : 12428
train acc:  0.75
train loss:  0.45048052072525024
train gradient:  0.08588679140284325
iteration : 12429
train acc:  0.7421875
train loss:  0.4609951972961426
train gradient:  0.11539683801934779
iteration : 12430
train acc:  0.7734375
train loss:  0.4538859724998474
train gradient:  0.09722453785368283
iteration : 12431
train acc:  0.6796875
train loss:  0.5354156494140625
train gradient:  0.10652984855820873
iteration : 12432
train acc:  0.8046875
train loss:  0.500298798084259
train gradient:  0.147123313764786
iteration : 12433
train acc:  0.71875
train loss:  0.4664421081542969
train gradient:  0.11176426030787903
iteration : 12434
train acc:  0.765625
train loss:  0.46440285444259644
train gradient:  0.13087878009505802
iteration : 12435
train acc:  0.7421875
train loss:  0.4437981843948364
train gradient:  0.10677794157525096
iteration : 12436
train acc:  0.7421875
train loss:  0.5099592208862305
train gradient:  0.1633521675242612
iteration : 12437
train acc:  0.765625
train loss:  0.43968433141708374
train gradient:  0.0913110189353436
iteration : 12438
train acc:  0.7890625
train loss:  0.4732190668582916
train gradient:  0.11551356803818424
iteration : 12439
train acc:  0.7578125
train loss:  0.5530313849449158
train gradient:  0.22489110114311328
iteration : 12440
train acc:  0.6953125
train loss:  0.5532271862030029
train gradient:  0.1267842961698601
iteration : 12441
train acc:  0.7265625
train loss:  0.49513569474220276
train gradient:  0.1378624793008023
iteration : 12442
train acc:  0.7734375
train loss:  0.4742824137210846
train gradient:  0.1332453967018937
iteration : 12443
train acc:  0.7578125
train loss:  0.4643324315547943
train gradient:  0.11704991755570811
iteration : 12444
train acc:  0.7265625
train loss:  0.45602044463157654
train gradient:  0.08622550096947806
iteration : 12445
train acc:  0.765625
train loss:  0.45659172534942627
train gradient:  0.09573393849937838
iteration : 12446
train acc:  0.7109375
train loss:  0.5272399187088013
train gradient:  0.13810518455191484
iteration : 12447
train acc:  0.765625
train loss:  0.4662413001060486
train gradient:  0.10091581077528075
iteration : 12448
train acc:  0.75
train loss:  0.49360597133636475
train gradient:  0.10656884936707615
iteration : 12449
train acc:  0.671875
train loss:  0.5834681987762451
train gradient:  0.14022748936576707
iteration : 12450
train acc:  0.6875
train loss:  0.4855930209159851
train gradient:  0.14317752333172162
iteration : 12451
train acc:  0.765625
train loss:  0.48982366919517517
train gradient:  0.12139244360620166
iteration : 12452
train acc:  0.7734375
train loss:  0.4654847979545593
train gradient:  0.10560539386203296
iteration : 12453
train acc:  0.734375
train loss:  0.4818289875984192
train gradient:  0.13435623586054338
iteration : 12454
train acc:  0.796875
train loss:  0.4458288550376892
train gradient:  0.11067234323375783
iteration : 12455
train acc:  0.7421875
train loss:  0.4867124557495117
train gradient:  0.11452714239284947
iteration : 12456
train acc:  0.7421875
train loss:  0.49087947607040405
train gradient:  0.13423050339610465
iteration : 12457
train acc:  0.7265625
train loss:  0.5028257966041565
train gradient:  0.1257554158569843
iteration : 12458
train acc:  0.7265625
train loss:  0.5155070424079895
train gradient:  0.13105705296720235
iteration : 12459
train acc:  0.7109375
train loss:  0.4936355948448181
train gradient:  0.1475496405186171
iteration : 12460
train acc:  0.71875
train loss:  0.44295406341552734
train gradient:  0.09624688057631159
iteration : 12461
train acc:  0.765625
train loss:  0.49869775772094727
train gradient:  0.11723128546967268
iteration : 12462
train acc:  0.7578125
train loss:  0.5351244211196899
train gradient:  0.12978328985247292
iteration : 12463
train acc:  0.6875
train loss:  0.5406962633132935
train gradient:  0.12489078396640563
iteration : 12464
train acc:  0.765625
train loss:  0.495658278465271
train gradient:  0.1271622700355065
iteration : 12465
train acc:  0.765625
train loss:  0.4930947721004486
train gradient:  0.11981108622682377
iteration : 12466
train acc:  0.7265625
train loss:  0.4977344870567322
train gradient:  0.1419568576419322
iteration : 12467
train acc:  0.7109375
train loss:  0.5477992296218872
train gradient:  0.14342343127574753
iteration : 12468
train acc:  0.7421875
train loss:  0.5346872806549072
train gradient:  0.11456174939101985
iteration : 12469
train acc:  0.7890625
train loss:  0.44619423151016235
train gradient:  0.12526838217765174
iteration : 12470
train acc:  0.7265625
train loss:  0.5198607444763184
train gradient:  0.12706911559137124
iteration : 12471
train acc:  0.75
train loss:  0.5042257308959961
train gradient:  0.11178592020860251
iteration : 12472
train acc:  0.8203125
train loss:  0.39473822712898254
train gradient:  0.08223614940175075
iteration : 12473
train acc:  0.6796875
train loss:  0.46928974986076355
train gradient:  0.1191830984292084
iteration : 12474
train acc:  0.71875
train loss:  0.5107739567756653
train gradient:  0.14583991332625001
iteration : 12475
train acc:  0.71875
train loss:  0.5132626891136169
train gradient:  0.12026167441675306
iteration : 12476
train acc:  0.7578125
train loss:  0.453332781791687
train gradient:  0.09953166472087739
iteration : 12477
train acc:  0.6875
train loss:  0.522020697593689
train gradient:  0.14902728258871528
iteration : 12478
train acc:  0.75
train loss:  0.4867718815803528
train gradient:  0.11138941812302391
iteration : 12479
train acc:  0.6953125
train loss:  0.4770466089248657
train gradient:  0.1002070236510548
iteration : 12480
train acc:  0.796875
train loss:  0.4852311611175537
train gradient:  0.12095591623521598
iteration : 12481
train acc:  0.7109375
train loss:  0.5626693964004517
train gradient:  0.19128161877524097
iteration : 12482
train acc:  0.7578125
train loss:  0.46011266112327576
train gradient:  0.09383056022406897
iteration : 12483
train acc:  0.71875
train loss:  0.49398091435432434
train gradient:  0.17652522615137517
iteration : 12484
train acc:  0.734375
train loss:  0.5161952376365662
train gradient:  0.13291453101995016
iteration : 12485
train acc:  0.7734375
train loss:  0.47657182812690735
train gradient:  0.10946970989191655
iteration : 12486
train acc:  0.765625
train loss:  0.4704633355140686
train gradient:  0.10141739403077295
iteration : 12487
train acc:  0.78125
train loss:  0.4831892251968384
train gradient:  0.10944181221056559
iteration : 12488
train acc:  0.8046875
train loss:  0.4362149238586426
train gradient:  0.1109277148987622
iteration : 12489
train acc:  0.734375
train loss:  0.4990871548652649
train gradient:  0.15254582117297236
iteration : 12490
train acc:  0.8125
train loss:  0.4151242971420288
train gradient:  0.09917922361815588
iteration : 12491
train acc:  0.6875
train loss:  0.5964778661727905
train gradient:  0.19846940981843997
iteration : 12492
train acc:  0.71875
train loss:  0.5040242671966553
train gradient:  0.11376092073437917
iteration : 12493
train acc:  0.703125
train loss:  0.548642635345459
train gradient:  0.16862085589466513
iteration : 12494
train acc:  0.765625
train loss:  0.5014546513557434
train gradient:  0.15288802890179082
iteration : 12495
train acc:  0.7265625
train loss:  0.519291341304779
train gradient:  0.14495308925333295
iteration : 12496
train acc:  0.6875
train loss:  0.481001615524292
train gradient:  0.09887706160751228
iteration : 12497
train acc:  0.7734375
train loss:  0.4899296164512634
train gradient:  0.13494357322927722
iteration : 12498
train acc:  0.765625
train loss:  0.4613589644432068
train gradient:  0.10173979421645803
iteration : 12499
train acc:  0.7734375
train loss:  0.4745703339576721
train gradient:  0.10870788987836631
iteration : 12500
train acc:  0.765625
train loss:  0.5114794373512268
train gradient:  0.11910184504405434
iteration : 12501
train acc:  0.7578125
train loss:  0.44623202085494995
train gradient:  0.10820764835925305
iteration : 12502
train acc:  0.7109375
train loss:  0.48808997869491577
train gradient:  0.09882036579707623
iteration : 12503
train acc:  0.75
train loss:  0.45524653792381287
train gradient:  0.11301080691404629
iteration : 12504
train acc:  0.7578125
train loss:  0.48333603143692017
train gradient:  0.14168738119871171
iteration : 12505
train acc:  0.796875
train loss:  0.44467514753341675
train gradient:  0.10832399930130795
iteration : 12506
train acc:  0.671875
train loss:  0.5321693420410156
train gradient:  0.11817614433446512
iteration : 12507
train acc:  0.7421875
train loss:  0.4953500032424927
train gradient:  0.11421201307892542
iteration : 12508
train acc:  0.6484375
train loss:  0.5486800670623779
train gradient:  0.12715589289486107
iteration : 12509
train acc:  0.6953125
train loss:  0.4881434142589569
train gradient:  0.09937731879590131
iteration : 12510
train acc:  0.7421875
train loss:  0.5023555755615234
train gradient:  0.12107521986423356
iteration : 12511
train acc:  0.765625
train loss:  0.49304547905921936
train gradient:  0.13442892962638284
iteration : 12512
train acc:  0.7265625
train loss:  0.4858120083808899
train gradient:  0.11358897507023408
iteration : 12513
train acc:  0.7890625
train loss:  0.49429845809936523
train gradient:  0.12162608568355515
iteration : 12514
train acc:  0.734375
train loss:  0.4765390455722809
train gradient:  0.13863687400800556
iteration : 12515
train acc:  0.75
train loss:  0.5206730365753174
train gradient:  0.12500956197693278
iteration : 12516
train acc:  0.8125
train loss:  0.44399574398994446
train gradient:  0.10398976413149753
iteration : 12517
train acc:  0.734375
train loss:  0.482805997133255
train gradient:  0.14059403002591522
iteration : 12518
train acc:  0.734375
train loss:  0.5011464357376099
train gradient:  0.14866826858708504
iteration : 12519
train acc:  0.734375
train loss:  0.527423620223999
train gradient:  0.13146924185079534
iteration : 12520
train acc:  0.7578125
train loss:  0.4890604615211487
train gradient:  0.15299462518762547
iteration : 12521
train acc:  0.7265625
train loss:  0.4912186861038208
train gradient:  0.13771558922255867
iteration : 12522
train acc:  0.7421875
train loss:  0.468944251537323
train gradient:  0.09328189202795052
iteration : 12523
train acc:  0.71875
train loss:  0.4939521551132202
train gradient:  0.13110274233668964
iteration : 12524
train acc:  0.78125
train loss:  0.49454283714294434
train gradient:  0.10342299316475977
iteration : 12525
train acc:  0.71875
train loss:  0.5057373046875
train gradient:  0.10133459196694021
iteration : 12526
train acc:  0.7421875
train loss:  0.45421546697616577
train gradient:  0.104320553655267
iteration : 12527
train acc:  0.7734375
train loss:  0.4683171510696411
train gradient:  0.11810268377348182
iteration : 12528
train acc:  0.6796875
train loss:  0.5101830959320068
train gradient:  0.1455287933498085
iteration : 12529
train acc:  0.734375
train loss:  0.4994691014289856
train gradient:  0.17628118424868927
iteration : 12530
train acc:  0.6953125
train loss:  0.6146195530891418
train gradient:  0.15801735175997056
iteration : 12531
train acc:  0.78125
train loss:  0.4187305271625519
train gradient:  0.14465815254751954
iteration : 12532
train acc:  0.75
train loss:  0.4490187168121338
train gradient:  0.10331400648238931
iteration : 12533
train acc:  0.7421875
train loss:  0.49448102712631226
train gradient:  0.13143711373289002
iteration : 12534
train acc:  0.6953125
train loss:  0.520108699798584
train gradient:  0.1214324848194858
iteration : 12535
train acc:  0.765625
train loss:  0.5403763055801392
train gradient:  0.1672699782528324
iteration : 12536
train acc:  0.7421875
train loss:  0.5539677143096924
train gradient:  0.13063783603225565
iteration : 12537
train acc:  0.75
train loss:  0.47192245721817017
train gradient:  0.11589629786657621
iteration : 12538
train acc:  0.7265625
train loss:  0.4941790699958801
train gradient:  0.11560748733749965
iteration : 12539
train acc:  0.8046875
train loss:  0.4162321984767914
train gradient:  0.10850283126353144
iteration : 12540
train acc:  0.7734375
train loss:  0.4749653935432434
train gradient:  0.13019691719369217
iteration : 12541
train acc:  0.734375
train loss:  0.5043352842330933
train gradient:  0.10524779222975823
iteration : 12542
train acc:  0.71875
train loss:  0.474916934967041
train gradient:  0.10798843045018625
iteration : 12543
train acc:  0.7421875
train loss:  0.4500429630279541
train gradient:  0.0934458552154466
iteration : 12544
train acc:  0.7578125
train loss:  0.4596492052078247
train gradient:  0.09871818752976753
iteration : 12545
train acc:  0.734375
train loss:  0.48701828718185425
train gradient:  0.1488669363315895
iteration : 12546
train acc:  0.734375
train loss:  0.4999355375766754
train gradient:  0.1271089558966516
iteration : 12547
train acc:  0.765625
train loss:  0.515475869178772
train gradient:  0.15963462187115898
iteration : 12548
train acc:  0.734375
train loss:  0.4999437928199768
train gradient:  0.13794039821659113
iteration : 12549
train acc:  0.7421875
train loss:  0.4744238257408142
train gradient:  0.15359973091219836
iteration : 12550
train acc:  0.6953125
train loss:  0.5444155931472778
train gradient:  0.13500757048777046
iteration : 12551
train acc:  0.7421875
train loss:  0.4592438042163849
train gradient:  0.13146157394012165
iteration : 12552
train acc:  0.78125
train loss:  0.44045519828796387
train gradient:  0.12034605560645581
iteration : 12553
train acc:  0.765625
train loss:  0.49864906072616577
train gradient:  0.1255973670158459
iteration : 12554
train acc:  0.734375
train loss:  0.49434512853622437
train gradient:  0.14173662052006797
iteration : 12555
train acc:  0.71875
train loss:  0.5560377240180969
train gradient:  0.13891478373243277
iteration : 12556
train acc:  0.75
train loss:  0.5411982536315918
train gradient:  0.13018886969839238
iteration : 12557
train acc:  0.7109375
train loss:  0.5078383684158325
train gradient:  0.12587104274233152
iteration : 12558
train acc:  0.7890625
train loss:  0.436434268951416
train gradient:  0.12272188648567668
iteration : 12559
train acc:  0.7109375
train loss:  0.4834953844547272
train gradient:  0.11562282486553815
iteration : 12560
train acc:  0.765625
train loss:  0.43460986018180847
train gradient:  0.1092205427839587
iteration : 12561
train acc:  0.7265625
train loss:  0.5268428325653076
train gradient:  0.1315924881147703
iteration : 12562
train acc:  0.6796875
train loss:  0.5404638648033142
train gradient:  0.1480008497159459
iteration : 12563
train acc:  0.734375
train loss:  0.5530503392219543
train gradient:  0.1619615163091216
iteration : 12564
train acc:  0.78125
train loss:  0.44853293895721436
train gradient:  0.08956892204464863
iteration : 12565
train acc:  0.7734375
train loss:  0.47239047288894653
train gradient:  0.0956006209980756
iteration : 12566
train acc:  0.796875
train loss:  0.4385383427143097
train gradient:  0.08093225889305038
iteration : 12567
train acc:  0.7421875
train loss:  0.5000010132789612
train gradient:  0.10897473562226047
iteration : 12568
train acc:  0.7578125
train loss:  0.49207764863967896
train gradient:  0.11590801211757487
iteration : 12569
train acc:  0.828125
train loss:  0.39683425426483154
train gradient:  0.08310581837753593
iteration : 12570
train acc:  0.75
train loss:  0.49037033319473267
train gradient:  0.12236487058816282
iteration : 12571
train acc:  0.6953125
train loss:  0.5286169648170471
train gradient:  0.11246472314451134
iteration : 12572
train acc:  0.7578125
train loss:  0.472867488861084
train gradient:  0.10075356650747229
iteration : 12573
train acc:  0.7421875
train loss:  0.5615382194519043
train gradient:  0.16006259802360734
iteration : 12574
train acc:  0.7421875
train loss:  0.5007915496826172
train gradient:  0.10916297528473227
iteration : 12575
train acc:  0.7890625
train loss:  0.4293792247772217
train gradient:  0.09185685460512813
iteration : 12576
train acc:  0.78125
train loss:  0.45507490634918213
train gradient:  0.1163917040554234
iteration : 12577
train acc:  0.75
train loss:  0.48621922731399536
train gradient:  0.18223347478944504
iteration : 12578
train acc:  0.7890625
train loss:  0.4116559326648712
train gradient:  0.08506239475650314
iteration : 12579
train acc:  0.828125
train loss:  0.43624943494796753
train gradient:  0.10289025413858971
iteration : 12580
train acc:  0.7265625
train loss:  0.4715014100074768
train gradient:  0.11656304250246798
iteration : 12581
train acc:  0.703125
train loss:  0.5230417251586914
train gradient:  0.1438750457973775
iteration : 12582
train acc:  0.671875
train loss:  0.5481168031692505
train gradient:  0.1392310918509933
iteration : 12583
train acc:  0.71875
train loss:  0.48015397787094116
train gradient:  0.13750752833209207
iteration : 12584
train acc:  0.6953125
train loss:  0.563575267791748
train gradient:  0.21877950165936044
iteration : 12585
train acc:  0.6875
train loss:  0.5821943283081055
train gradient:  0.1458020727046794
iteration : 12586
train acc:  0.7421875
train loss:  0.5037258863449097
train gradient:  0.13067168222520642
iteration : 12587
train acc:  0.7890625
train loss:  0.4518054723739624
train gradient:  0.11128463811163875
iteration : 12588
train acc:  0.7421875
train loss:  0.5256942510604858
train gradient:  0.166218644996872
iteration : 12589
train acc:  0.6796875
train loss:  0.5558656454086304
train gradient:  0.12875249402007338
iteration : 12590
train acc:  0.75
train loss:  0.49687206745147705
train gradient:  0.11025434992985125
iteration : 12591
train acc:  0.6328125
train loss:  0.6457440853118896
train gradient:  0.20299572454076975
iteration : 12592
train acc:  0.703125
train loss:  0.5325690507888794
train gradient:  0.1495562936655086
iteration : 12593
train acc:  0.7109375
train loss:  0.5325350165367126
train gradient:  0.1283266948073041
iteration : 12594
train acc:  0.75
train loss:  0.44780150055885315
train gradient:  0.12894925616317038
iteration : 12595
train acc:  0.7109375
train loss:  0.47620928287506104
train gradient:  0.1324153745349639
iteration : 12596
train acc:  0.7109375
train loss:  0.4964616298675537
train gradient:  0.15841813845362213
iteration : 12597
train acc:  0.7109375
train loss:  0.5092206597328186
train gradient:  0.13743417048493844
iteration : 12598
train acc:  0.671875
train loss:  0.5756447911262512
train gradient:  0.17340651079662478
iteration : 12599
train acc:  0.8203125
train loss:  0.430924654006958
train gradient:  0.08079408679862819
iteration : 12600
train acc:  0.7265625
train loss:  0.5042099356651306
train gradient:  0.09830321968985142
iteration : 12601
train acc:  0.7265625
train loss:  0.4939212501049042
train gradient:  0.12384397729187466
iteration : 12602
train acc:  0.6953125
train loss:  0.5357739925384521
train gradient:  0.13966876242258144
iteration : 12603
train acc:  0.6953125
train loss:  0.5166356563568115
train gradient:  0.1618873342135666
iteration : 12604
train acc:  0.71875
train loss:  0.5100553035736084
train gradient:  0.11880245174107705
iteration : 12605
train acc:  0.8046875
train loss:  0.44951188564300537
train gradient:  0.10403888298478892
iteration : 12606
train acc:  0.75
train loss:  0.4982227087020874
train gradient:  0.1441477929573473
iteration : 12607
train acc:  0.7109375
train loss:  0.5092607736587524
train gradient:  0.13021739325646303
iteration : 12608
train acc:  0.7265625
train loss:  0.48636338114738464
train gradient:  0.10179348921498965
iteration : 12609
train acc:  0.71875
train loss:  0.47428953647613525
train gradient:  0.12458176158870575
iteration : 12610
train acc:  0.75
train loss:  0.45258045196533203
train gradient:  0.10756862525000335
iteration : 12611
train acc:  0.71875
train loss:  0.4940534830093384
train gradient:  0.12451805912796328
iteration : 12612
train acc:  0.75
train loss:  0.5071743130683899
train gradient:  0.17765787221859283
iteration : 12613
train acc:  0.8203125
train loss:  0.4610375165939331
train gradient:  0.13277241817624508
iteration : 12614
train acc:  0.71875
train loss:  0.5155599117279053
train gradient:  0.1626162905820696
iteration : 12615
train acc:  0.765625
train loss:  0.504664421081543
train gradient:  0.1321640106374707
iteration : 12616
train acc:  0.71875
train loss:  0.524575412273407
train gradient:  0.11973275297872985
iteration : 12617
train acc:  0.7578125
train loss:  0.4783216118812561
train gradient:  0.12303091212135764
iteration : 12618
train acc:  0.7578125
train loss:  0.4555799067020416
train gradient:  0.08635077793117357
iteration : 12619
train acc:  0.7265625
train loss:  0.5335831046104431
train gradient:  0.1306081247494043
iteration : 12620
train acc:  0.734375
train loss:  0.4972282648086548
train gradient:  0.10421483884614811
iteration : 12621
train acc:  0.75
train loss:  0.4955642521381378
train gradient:  0.10468776957370488
iteration : 12622
train acc:  0.734375
train loss:  0.4941784739494324
train gradient:  0.1692775713143757
iteration : 12623
train acc:  0.7421875
train loss:  0.48562854528427124
train gradient:  0.13520850525326544
iteration : 12624
train acc:  0.75
train loss:  0.45076537132263184
train gradient:  0.15325760146515832
iteration : 12625
train acc:  0.7734375
train loss:  0.5110524892807007
train gradient:  0.10281652165899309
iteration : 12626
train acc:  0.7109375
train loss:  0.5507670640945435
train gradient:  0.1458237054109009
iteration : 12627
train acc:  0.78125
train loss:  0.42332756519317627
train gradient:  0.09172889688613506
iteration : 12628
train acc:  0.7890625
train loss:  0.4143410325050354
train gradient:  0.07668430198080231
iteration : 12629
train acc:  0.7421875
train loss:  0.5002056360244751
train gradient:  0.10505585000125044
iteration : 12630
train acc:  0.765625
train loss:  0.44614243507385254
train gradient:  0.11033299173549795
iteration : 12631
train acc:  0.734375
train loss:  0.48838329315185547
train gradient:  0.1253392045507996
iteration : 12632
train acc:  0.734375
train loss:  0.48523709177970886
train gradient:  0.13192141097349633
iteration : 12633
train acc:  0.6875
train loss:  0.51123046875
train gradient:  0.13225487350581527
iteration : 12634
train acc:  0.71875
train loss:  0.5234712362289429
train gradient:  0.12652486874632113
iteration : 12635
train acc:  0.734375
train loss:  0.4945908784866333
train gradient:  0.11216572041428634
iteration : 12636
train acc:  0.703125
train loss:  0.5164969563484192
train gradient:  0.13946206913535736
iteration : 12637
train acc:  0.703125
train loss:  0.5203815698623657
train gradient:  0.14656859561319913
iteration : 12638
train acc:  0.703125
train loss:  0.5490574836730957
train gradient:  0.14371470037864176
iteration : 12639
train acc:  0.765625
train loss:  0.47113797068595886
train gradient:  0.12228709347140694
iteration : 12640
train acc:  0.734375
train loss:  0.5255640745162964
train gradient:  0.12759437731255957
iteration : 12641
train acc:  0.703125
train loss:  0.5031120777130127
train gradient:  0.1321620820582869
iteration : 12642
train acc:  0.796875
train loss:  0.4220648407936096
train gradient:  0.10917358293994743
iteration : 12643
train acc:  0.7265625
train loss:  0.46644413471221924
train gradient:  0.09201859788168508
iteration : 12644
train acc:  0.734375
train loss:  0.48956334590911865
train gradient:  0.13753392372570927
iteration : 12645
train acc:  0.71875
train loss:  0.5027570724487305
train gradient:  0.13846711327091443
iteration : 12646
train acc:  0.765625
train loss:  0.4299778938293457
train gradient:  0.09543773095354505
iteration : 12647
train acc:  0.7578125
train loss:  0.4568707048892975
train gradient:  0.1197478457029759
iteration : 12648
train acc:  0.6953125
train loss:  0.49722230434417725
train gradient:  0.10860667967069747
iteration : 12649
train acc:  0.78125
train loss:  0.5071859359741211
train gradient:  0.13328535916721992
iteration : 12650
train acc:  0.75
train loss:  0.48565229773521423
train gradient:  0.13268311538843955
iteration : 12651
train acc:  0.7265625
train loss:  0.5049010515213013
train gradient:  0.14530791165478169
iteration : 12652
train acc:  0.7578125
train loss:  0.47045373916625977
train gradient:  0.10068477641573616
iteration : 12653
train acc:  0.7578125
train loss:  0.5120971202850342
train gradient:  0.14324406201363382
iteration : 12654
train acc:  0.6875
train loss:  0.48744016885757446
train gradient:  0.08970592741383358
iteration : 12655
train acc:  0.7734375
train loss:  0.4641660451889038
train gradient:  0.12884695684800423
iteration : 12656
train acc:  0.7265625
train loss:  0.49648451805114746
train gradient:  0.10507825818701752
iteration : 12657
train acc:  0.71875
train loss:  0.5231506824493408
train gradient:  0.11802962559227739
iteration : 12658
train acc:  0.796875
train loss:  0.4256194829940796
train gradient:  0.09595598581482925
iteration : 12659
train acc:  0.71875
train loss:  0.5837836861610413
train gradient:  0.22522484704569587
iteration : 12660
train acc:  0.7578125
train loss:  0.47095268964767456
train gradient:  0.10109784292541023
iteration : 12661
train acc:  0.6953125
train loss:  0.5632538795471191
train gradient:  0.14528529759952963
iteration : 12662
train acc:  0.734375
train loss:  0.4578743577003479
train gradient:  0.1013148642033155
iteration : 12663
train acc:  0.765625
train loss:  0.44786012172698975
train gradient:  0.1024612702477559
iteration : 12664
train acc:  0.734375
train loss:  0.5165644884109497
train gradient:  0.1363333449873887
iteration : 12665
train acc:  0.7421875
train loss:  0.4832704961299896
train gradient:  0.12777967422192948
iteration : 12666
train acc:  0.6953125
train loss:  0.49159300327301025
train gradient:  0.11635068163452261
iteration : 12667
train acc:  0.7890625
train loss:  0.4372944235801697
train gradient:  0.0975904501911286
iteration : 12668
train acc:  0.71875
train loss:  0.5606738328933716
train gradient:  0.1539833059405913
iteration : 12669
train acc:  0.6953125
train loss:  0.5669447183609009
train gradient:  0.1521249140287126
iteration : 12670
train acc:  0.7109375
train loss:  0.538083553314209
train gradient:  0.1274951896770935
iteration : 12671
train acc:  0.796875
train loss:  0.4056544303894043
train gradient:  0.10636768842354877
iteration : 12672
train acc:  0.765625
train loss:  0.5058091878890991
train gradient:  0.1515037754384348
iteration : 12673
train acc:  0.765625
train loss:  0.460863322019577
train gradient:  0.10653018900107503
iteration : 12674
train acc:  0.734375
train loss:  0.5053403377532959
train gradient:  0.11352745664965365
iteration : 12675
train acc:  0.7421875
train loss:  0.5144686698913574
train gradient:  0.12570151193143836
iteration : 12676
train acc:  0.78125
train loss:  0.46141767501831055
train gradient:  0.15343638169503052
iteration : 12677
train acc:  0.765625
train loss:  0.4763332009315491
train gradient:  0.09136635320344455
iteration : 12678
train acc:  0.71875
train loss:  0.5536028146743774
train gradient:  0.13607102395063836
iteration : 12679
train acc:  0.7890625
train loss:  0.44143471121788025
train gradient:  0.09549004423789115
iteration : 12680
train acc:  0.765625
train loss:  0.49621522426605225
train gradient:  0.12416446333108225
iteration : 12681
train acc:  0.7578125
train loss:  0.5203891396522522
train gradient:  0.12796445389166747
iteration : 12682
train acc:  0.703125
train loss:  0.48565956950187683
train gradient:  0.12070299063622593
iteration : 12683
train acc:  0.7890625
train loss:  0.40500104427337646
train gradient:  0.08625828049055503
iteration : 12684
train acc:  0.8046875
train loss:  0.48248961567878723
train gradient:  0.12121550616279689
iteration : 12685
train acc:  0.71875
train loss:  0.49502134323120117
train gradient:  0.1369571693971582
iteration : 12686
train acc:  0.7109375
train loss:  0.4768354296684265
train gradient:  0.11288038942267953
iteration : 12687
train acc:  0.7734375
train loss:  0.48430460691452026
train gradient:  0.1056062507946347
iteration : 12688
train acc:  0.6953125
train loss:  0.4695625603199005
train gradient:  0.1394362187613183
iteration : 12689
train acc:  0.734375
train loss:  0.5429477691650391
train gradient:  0.16131534367524403
iteration : 12690
train acc:  0.7578125
train loss:  0.45930325984954834
train gradient:  0.10306434363546803
iteration : 12691
train acc:  0.7578125
train loss:  0.49136993288993835
train gradient:  0.12286517693729865
iteration : 12692
train acc:  0.7109375
train loss:  0.5164343118667603
train gradient:  0.12912710631431606
iteration : 12693
train acc:  0.7734375
train loss:  0.4819476008415222
train gradient:  0.12379069849790672
iteration : 12694
train acc:  0.71875
train loss:  0.45267751812934875
train gradient:  0.11999291833225056
iteration : 12695
train acc:  0.7421875
train loss:  0.446245014667511
train gradient:  0.09228264428437669
iteration : 12696
train acc:  0.78125
train loss:  0.4371509552001953
train gradient:  0.11531736185585527
iteration : 12697
train acc:  0.7734375
train loss:  0.4469321668148041
train gradient:  0.13374028200300292
iteration : 12698
train acc:  0.75
train loss:  0.4731099605560303
train gradient:  0.10426516603914189
iteration : 12699
train acc:  0.7421875
train loss:  0.4457969665527344
train gradient:  0.11367753298465548
iteration : 12700
train acc:  0.7890625
train loss:  0.46265196800231934
train gradient:  0.12537264251092467
iteration : 12701
train acc:  0.7421875
train loss:  0.5500048995018005
train gradient:  0.1753335090112587
iteration : 12702
train acc:  0.703125
train loss:  0.49086400866508484
train gradient:  0.09621865833667022
iteration : 12703
train acc:  0.7578125
train loss:  0.4713022708892822
train gradient:  0.09855478827789727
iteration : 12704
train acc:  0.8203125
train loss:  0.43757617473602295
train gradient:  0.10927222160308997
iteration : 12705
train acc:  0.78125
train loss:  0.4162430763244629
train gradient:  0.09021366888981083
iteration : 12706
train acc:  0.734375
train loss:  0.5177294015884399
train gradient:  0.147860359902534
iteration : 12707
train acc:  0.7421875
train loss:  0.5400267839431763
train gradient:  0.18115697993908458
iteration : 12708
train acc:  0.734375
train loss:  0.5270416736602783
train gradient:  0.12080789278130424
iteration : 12709
train acc:  0.7578125
train loss:  0.5116446614265442
train gradient:  0.15001193830358073
iteration : 12710
train acc:  0.734375
train loss:  0.4678258001804352
train gradient:  0.10716387901731855
iteration : 12711
train acc:  0.71875
train loss:  0.48630818724632263
train gradient:  0.13314946602949324
iteration : 12712
train acc:  0.7578125
train loss:  0.44305509328842163
train gradient:  0.10315285282095178
iteration : 12713
train acc:  0.7265625
train loss:  0.5212993025779724
train gradient:  0.11331712534966239
iteration : 12714
train acc:  0.7421875
train loss:  0.45821624994277954
train gradient:  0.10855884312140358
iteration : 12715
train acc:  0.734375
train loss:  0.4748274087905884
train gradient:  0.11368007095152757
iteration : 12716
train acc:  0.703125
train loss:  0.5483760237693787
train gradient:  0.1963457391794488
iteration : 12717
train acc:  0.7578125
train loss:  0.525691032409668
train gradient:  0.14844752921321408
iteration : 12718
train acc:  0.6796875
train loss:  0.5564159750938416
train gradient:  0.1492477941298624
iteration : 12719
train acc:  0.65625
train loss:  0.5729919075965881
train gradient:  0.21593286404887665
iteration : 12720
train acc:  0.7421875
train loss:  0.4583483934402466
train gradient:  0.10146823842641954
iteration : 12721
train acc:  0.78125
train loss:  0.5103679299354553
train gradient:  0.17232616799449502
iteration : 12722
train acc:  0.7109375
train loss:  0.46934714913368225
train gradient:  0.12336289370621961
iteration : 12723
train acc:  0.765625
train loss:  0.4769391119480133
train gradient:  0.12136072772868292
iteration : 12724
train acc:  0.796875
train loss:  0.4106685519218445
train gradient:  0.08978950497360186
iteration : 12725
train acc:  0.75
train loss:  0.4377802610397339
train gradient:  0.1261229502408812
iteration : 12726
train acc:  0.7578125
train loss:  0.5230578184127808
train gradient:  0.14118965898544622
iteration : 12727
train acc:  0.71875
train loss:  0.5174961686134338
train gradient:  0.1353490825132735
iteration : 12728
train acc:  0.7421875
train loss:  0.5428950786590576
train gradient:  0.17692219759388766
iteration : 12729
train acc:  0.7265625
train loss:  0.46640273928642273
train gradient:  0.12492327988706221
iteration : 12730
train acc:  0.765625
train loss:  0.5079557299613953
train gradient:  0.13165884730497876
iteration : 12731
train acc:  0.765625
train loss:  0.44213011860847473
train gradient:  0.08741119183533186
iteration : 12732
train acc:  0.7109375
train loss:  0.5297768115997314
train gradient:  0.13356594310916298
iteration : 12733
train acc:  0.7890625
train loss:  0.4344834089279175
train gradient:  0.13014333249848004
iteration : 12734
train acc:  0.765625
train loss:  0.43193191289901733
train gradient:  0.09826213889989766
iteration : 12735
train acc:  0.7265625
train loss:  0.5216359496116638
train gradient:  0.12872884197416062
iteration : 12736
train acc:  0.8203125
train loss:  0.4763261675834656
train gradient:  0.15581384898791106
iteration : 12737
train acc:  0.7890625
train loss:  0.4283403754234314
train gradient:  0.10050115206020455
iteration : 12738
train acc:  0.7265625
train loss:  0.4821857511997223
train gradient:  0.10093684346673425
iteration : 12739
train acc:  0.734375
train loss:  0.5709863901138306
train gradient:  0.15720939879194185
iteration : 12740
train acc:  0.765625
train loss:  0.4741988182067871
train gradient:  0.10063128858943808
iteration : 12741
train acc:  0.7265625
train loss:  0.4999092221260071
train gradient:  0.11423488917123162
iteration : 12742
train acc:  0.75
train loss:  0.4568975269794464
train gradient:  0.09856439276527917
iteration : 12743
train acc:  0.7265625
train loss:  0.497083842754364
train gradient:  0.13543498923219227
iteration : 12744
train acc:  0.78125
train loss:  0.4657881259918213
train gradient:  0.18012594352083514
iteration : 12745
train acc:  0.7109375
train loss:  0.5225567817687988
train gradient:  0.1454010883860434
iteration : 12746
train acc:  0.6953125
train loss:  0.5397275686264038
train gradient:  0.12356929676211477
iteration : 12747
train acc:  0.7421875
train loss:  0.49314865469932556
train gradient:  0.12612612263724565
iteration : 12748
train acc:  0.8046875
train loss:  0.4076541066169739
train gradient:  0.09586218925658205
iteration : 12749
train acc:  0.7421875
train loss:  0.4827984571456909
train gradient:  0.1266478760589203
iteration : 12750
train acc:  0.71875
train loss:  0.5668481588363647
train gradient:  0.14458383958496374
iteration : 12751
train acc:  0.765625
train loss:  0.4494858980178833
train gradient:  0.10453118302121257
iteration : 12752
train acc:  0.796875
train loss:  0.4396345615386963
train gradient:  0.11209907224872227
iteration : 12753
train acc:  0.765625
train loss:  0.4517638087272644
train gradient:  0.0997234308647935
iteration : 12754
train acc:  0.7890625
train loss:  0.39611783623695374
train gradient:  0.0892014595120471
iteration : 12755
train acc:  0.7109375
train loss:  0.47931134700775146
train gradient:  0.11734975894400819
iteration : 12756
train acc:  0.75
train loss:  0.4973182678222656
train gradient:  0.1459983945014584
iteration : 12757
train acc:  0.7109375
train loss:  0.4881629943847656
train gradient:  0.13714560079841864
iteration : 12758
train acc:  0.75
train loss:  0.4890132546424866
train gradient:  0.148076793597135
iteration : 12759
train acc:  0.765625
train loss:  0.43432706594467163
train gradient:  0.09702838207791752
iteration : 12760
train acc:  0.734375
train loss:  0.5336213111877441
train gradient:  0.17378890638941746
iteration : 12761
train acc:  0.6953125
train loss:  0.5303083062171936
train gradient:  0.12065429468056726
iteration : 12762
train acc:  0.7265625
train loss:  0.5593951940536499
train gradient:  0.21455261641210746
iteration : 12763
train acc:  0.7734375
train loss:  0.4847569763660431
train gradient:  0.140546076693821
iteration : 12764
train acc:  0.7578125
train loss:  0.5166827440261841
train gradient:  0.1718348003156554
iteration : 12765
train acc:  0.7734375
train loss:  0.4765937626361847
train gradient:  0.11684611777465342
iteration : 12766
train acc:  0.75
train loss:  0.42369258403778076
train gradient:  0.1385625017292456
iteration : 12767
train acc:  0.71875
train loss:  0.5510279536247253
train gradient:  0.14416612431915615
iteration : 12768
train acc:  0.8046875
train loss:  0.4260202944278717
train gradient:  0.1025513054809941
iteration : 12769
train acc:  0.7734375
train loss:  0.45225679874420166
train gradient:  0.1315980595371643
iteration : 12770
train acc:  0.75
train loss:  0.4771454930305481
train gradient:  0.10631478539566612
iteration : 12771
train acc:  0.734375
train loss:  0.5041077136993408
train gradient:  0.14297095105897234
iteration : 12772
train acc:  0.7578125
train loss:  0.4583674669265747
train gradient:  0.12070453401778164
iteration : 12773
train acc:  0.7109375
train loss:  0.49317052960395813
train gradient:  0.10388897932025896
iteration : 12774
train acc:  0.7265625
train loss:  0.49241945147514343
train gradient:  0.11183222834181927
iteration : 12775
train acc:  0.7265625
train loss:  0.49834203720092773
train gradient:  0.12619065430094245
iteration : 12776
train acc:  0.734375
train loss:  0.4768729507923126
train gradient:  0.15985306460375487
iteration : 12777
train acc:  0.7578125
train loss:  0.47195330262184143
train gradient:  0.1194803261181869
iteration : 12778
train acc:  0.7890625
train loss:  0.38741007447242737
train gradient:  0.07950631867310548
iteration : 12779
train acc:  0.78125
train loss:  0.43468356132507324
train gradient:  0.10744172352254065
iteration : 12780
train acc:  0.78125
train loss:  0.44649291038513184
train gradient:  0.1181245850890663
iteration : 12781
train acc:  0.6796875
train loss:  0.4751046299934387
train gradient:  0.12135894894808841
iteration : 12782
train acc:  0.796875
train loss:  0.4800736606121063
train gradient:  0.12725143469047212
iteration : 12783
train acc:  0.71875
train loss:  0.48989126086235046
train gradient:  0.13263268487640095
iteration : 12784
train acc:  0.7421875
train loss:  0.43766307830810547
train gradient:  0.16047496799390748
iteration : 12785
train acc:  0.7421875
train loss:  0.4687627851963043
train gradient:  0.1010626186636479
iteration : 12786
train acc:  0.7578125
train loss:  0.4925750195980072
train gradient:  0.11782188764526154
iteration : 12787
train acc:  0.765625
train loss:  0.4968787729740143
train gradient:  0.10658258971296783
iteration : 12788
train acc:  0.75
train loss:  0.5464661121368408
train gradient:  0.15559548504679244
iteration : 12789
train acc:  0.734375
train loss:  0.4956583082675934
train gradient:  0.1194871638467256
iteration : 12790
train acc:  0.71875
train loss:  0.49138474464416504
train gradient:  0.14001765482281603
iteration : 12791
train acc:  0.734375
train loss:  0.5098899602890015
train gradient:  0.12453627387226716
iteration : 12792
train acc:  0.65625
train loss:  0.5677458643913269
train gradient:  0.13387675643614086
iteration : 12793
train acc:  0.7734375
train loss:  0.47162842750549316
train gradient:  0.11488130782248658
iteration : 12794
train acc:  0.7421875
train loss:  0.5346132516860962
train gradient:  0.15872274552529614
iteration : 12795
train acc:  0.6953125
train loss:  0.5287806987762451
train gradient:  0.12493125848406848
iteration : 12796
train acc:  0.7421875
train loss:  0.5095388889312744
train gradient:  0.120868162504375
iteration : 12797
train acc:  0.7734375
train loss:  0.46346163749694824
train gradient:  0.11857651397295349
iteration : 12798
train acc:  0.6796875
train loss:  0.5697567462921143
train gradient:  0.13102602862480744
iteration : 12799
train acc:  0.84375
train loss:  0.3897174596786499
train gradient:  0.07809772225271003
iteration : 12800
train acc:  0.6796875
train loss:  0.5522319078445435
train gradient:  0.15188043074274749
iteration : 12801
train acc:  0.765625
train loss:  0.4397682249546051
train gradient:  0.09533532081408311
iteration : 12802
train acc:  0.6796875
train loss:  0.5123897790908813
train gradient:  0.1113717634966977
iteration : 12803
train acc:  0.71875
train loss:  0.490502268075943
train gradient:  0.12284398366803384
iteration : 12804
train acc:  0.703125
train loss:  0.5139743685722351
train gradient:  0.1485376955272435
iteration : 12805
train acc:  0.7734375
train loss:  0.4906346797943115
train gradient:  0.13994873373879663
iteration : 12806
train acc:  0.765625
train loss:  0.46083900332450867
train gradient:  0.11950057171419795
iteration : 12807
train acc:  0.75
train loss:  0.4523812234401703
train gradient:  0.1052105454644751
iteration : 12808
train acc:  0.765625
train loss:  0.4793908894062042
train gradient:  0.15912855644854562
iteration : 12809
train acc:  0.6875
train loss:  0.5995274782180786
train gradient:  0.17837459864414312
iteration : 12810
train acc:  0.6875
train loss:  0.5516868233680725
train gradient:  0.2132508812944865
iteration : 12811
train acc:  0.765625
train loss:  0.4852251708507538
train gradient:  0.14029925530364096
iteration : 12812
train acc:  0.734375
train loss:  0.47564083337783813
train gradient:  0.12003433722461787
iteration : 12813
train acc:  0.78125
train loss:  0.4636646509170532
train gradient:  0.13273396880215063
iteration : 12814
train acc:  0.734375
train loss:  0.5394577383995056
train gradient:  0.12546480665200668
iteration : 12815
train acc:  0.7578125
train loss:  0.5522636771202087
train gradient:  0.16109668970971347
iteration : 12816
train acc:  0.765625
train loss:  0.47865960001945496
train gradient:  0.13853699585420187
iteration : 12817
train acc:  0.734375
train loss:  0.49732863903045654
train gradient:  0.1332011855407751
iteration : 12818
train acc:  0.7265625
train loss:  0.489745557308197
train gradient:  0.1266740823321627
iteration : 12819
train acc:  0.6953125
train loss:  0.48243480920791626
train gradient:  0.1326203720793247
iteration : 12820
train acc:  0.671875
train loss:  0.5156534314155579
train gradient:  0.14291181638708558
iteration : 12821
train acc:  0.7734375
train loss:  0.48664620518684387
train gradient:  0.11571104775643923
iteration : 12822
train acc:  0.6953125
train loss:  0.47867053747177124
train gradient:  0.11608377018017413
iteration : 12823
train acc:  0.7734375
train loss:  0.4907708168029785
train gradient:  0.10404118707905581
iteration : 12824
train acc:  0.84375
train loss:  0.4123597741127014
train gradient:  0.08328865491923887
iteration : 12825
train acc:  0.8046875
train loss:  0.4263260066509247
train gradient:  0.10638361727143462
iteration : 12826
train acc:  0.65625
train loss:  0.5486049056053162
train gradient:  0.15422351991890454
iteration : 12827
train acc:  0.6875
train loss:  0.5425294637680054
train gradient:  0.1610933890487015
iteration : 12828
train acc:  0.7421875
train loss:  0.5513491630554199
train gradient:  0.15837026603321577
iteration : 12829
train acc:  0.7578125
train loss:  0.45702412724494934
train gradient:  0.13016704403151352
iteration : 12830
train acc:  0.75
train loss:  0.4458000957965851
train gradient:  0.09916994479526414
iteration : 12831
train acc:  0.765625
train loss:  0.4696325361728668
train gradient:  0.10523930493522385
iteration : 12832
train acc:  0.78125
train loss:  0.41634052991867065
train gradient:  0.11259027539759016
iteration : 12833
train acc:  0.7265625
train loss:  0.5126799941062927
train gradient:  0.14132233350606843
iteration : 12834
train acc:  0.78125
train loss:  0.4336678385734558
train gradient:  0.09336117422766119
iteration : 12835
train acc:  0.78125
train loss:  0.4874808192253113
train gradient:  0.13114447945864102
iteration : 12836
train acc:  0.734375
train loss:  0.4500140845775604
train gradient:  0.10199165801977282
iteration : 12837
train acc:  0.734375
train loss:  0.5460650324821472
train gradient:  0.14701451204476107
iteration : 12838
train acc:  0.7109375
train loss:  0.525144636631012
train gradient:  0.12143643850304366
iteration : 12839
train acc:  0.765625
train loss:  0.46091437339782715
train gradient:  0.1045256662197308
iteration : 12840
train acc:  0.765625
train loss:  0.4856133460998535
train gradient:  0.15416383871963385
iteration : 12841
train acc:  0.7265625
train loss:  0.4966787099838257
train gradient:  0.17929554016478272
iteration : 12842
train acc:  0.734375
train loss:  0.5367912650108337
train gradient:  0.13275693547578232
iteration : 12843
train acc:  0.7734375
train loss:  0.49172312021255493
train gradient:  0.13211058813055065
iteration : 12844
train acc:  0.765625
train loss:  0.49917036294937134
train gradient:  0.12344716880540661
iteration : 12845
train acc:  0.8046875
train loss:  0.437751829624176
train gradient:  0.10821353407059102
iteration : 12846
train acc:  0.78125
train loss:  0.4867568910121918
train gradient:  0.11650495823185039
iteration : 12847
train acc:  0.703125
train loss:  0.5189252495765686
train gradient:  0.13674214559449616
iteration : 12848
train acc:  0.71875
train loss:  0.46854105591773987
train gradient:  0.11314417013100034
iteration : 12849
train acc:  0.7421875
train loss:  0.46035271883010864
train gradient:  0.10361363084209359
iteration : 12850
train acc:  0.671875
train loss:  0.509232759475708
train gradient:  0.15700381277030234
iteration : 12851
train acc:  0.78125
train loss:  0.47082072496414185
train gradient:  0.116637767458303
iteration : 12852
train acc:  0.7265625
train loss:  0.5140756368637085
train gradient:  0.14631271914873295
iteration : 12853
train acc:  0.734375
train loss:  0.4539218544960022
train gradient:  0.1183513828458937
iteration : 12854
train acc:  0.7109375
train loss:  0.5047439336776733
train gradient:  0.11769547437687006
iteration : 12855
train acc:  0.7890625
train loss:  0.48941782116889954
train gradient:  0.13227512571829778
iteration : 12856
train acc:  0.7109375
train loss:  0.5591210126876831
train gradient:  0.2151523380278965
iteration : 12857
train acc:  0.796875
train loss:  0.4387902319431305
train gradient:  0.10360535549893027
iteration : 12858
train acc:  0.765625
train loss:  0.4822009801864624
train gradient:  0.0988815857477202
iteration : 12859
train acc:  0.6875
train loss:  0.5552520751953125
train gradient:  0.1590188194169725
iteration : 12860
train acc:  0.671875
train loss:  0.6017650365829468
train gradient:  0.18856734397715064
iteration : 12861
train acc:  0.78125
train loss:  0.4524801969528198
train gradient:  0.1017421128168867
iteration : 12862
train acc:  0.8046875
train loss:  0.4139726161956787
train gradient:  0.09154464727836624
iteration : 12863
train acc:  0.78125
train loss:  0.43850821256637573
train gradient:  0.11429803002753344
iteration : 12864
train acc:  0.7265625
train loss:  0.5526626706123352
train gradient:  0.16161009906966045
iteration : 12865
train acc:  0.734375
train loss:  0.5190483331680298
train gradient:  0.14335315615974087
iteration : 12866
train acc:  0.6796875
train loss:  0.5709075927734375
train gradient:  0.19368946237336387
iteration : 12867
train acc:  0.7890625
train loss:  0.4387631416320801
train gradient:  0.10714999539133874
iteration : 12868
train acc:  0.71875
train loss:  0.5292497873306274
train gradient:  0.1414051100812556
iteration : 12869
train acc:  0.765625
train loss:  0.47283735871315
train gradient:  0.09908531100304406
iteration : 12870
train acc:  0.7421875
train loss:  0.5153825879096985
train gradient:  0.1273229621284287
iteration : 12871
train acc:  0.765625
train loss:  0.4694657623767853
train gradient:  0.1067817004560158
iteration : 12872
train acc:  0.7890625
train loss:  0.44031304121017456
train gradient:  0.0930940164718179
iteration : 12873
train acc:  0.7578125
train loss:  0.4724593758583069
train gradient:  0.1009839542070069
iteration : 12874
train acc:  0.75
train loss:  0.4359401762485504
train gradient:  0.08774805341658293
iteration : 12875
train acc:  0.8046875
train loss:  0.4435136616230011
train gradient:  0.10043750193331055
iteration : 12876
train acc:  0.6328125
train loss:  0.6101871728897095
train gradient:  0.19889639326248948
iteration : 12877
train acc:  0.7421875
train loss:  0.46885794401168823
train gradient:  0.11194148822091729
iteration : 12878
train acc:  0.671875
train loss:  0.5725977420806885
train gradient:  0.1555221470074804
iteration : 12879
train acc:  0.765625
train loss:  0.4525556266307831
train gradient:  0.08342557214828768
iteration : 12880
train acc:  0.75
train loss:  0.5328073501586914
train gradient:  0.1782872530227008
iteration : 12881
train acc:  0.6953125
train loss:  0.5520069599151611
train gradient:  0.16275994880979033
iteration : 12882
train acc:  0.6875
train loss:  0.5497399568557739
train gradient:  0.13976873671468693
iteration : 12883
train acc:  0.78125
train loss:  0.43467846512794495
train gradient:  0.10439095068607572
iteration : 12884
train acc:  0.6875
train loss:  0.5394890904426575
train gradient:  0.11438715289305688
iteration : 12885
train acc:  0.703125
train loss:  0.5450832843780518
train gradient:  0.15230393012744853
iteration : 12886
train acc:  0.7578125
train loss:  0.5167989134788513
train gradient:  0.1342807189924251
iteration : 12887
train acc:  0.8515625
train loss:  0.41327643394470215
train gradient:  0.10117318888178242
iteration : 12888
train acc:  0.7109375
train loss:  0.5012770891189575
train gradient:  0.13966787265548453
iteration : 12889
train acc:  0.8046875
train loss:  0.4458079934120178
train gradient:  0.11046140583114344
iteration : 12890
train acc:  0.671875
train loss:  0.5762326717376709
train gradient:  0.15329429933588046
iteration : 12891
train acc:  0.71875
train loss:  0.5141998529434204
train gradient:  0.1265998798133294
iteration : 12892
train acc:  0.7734375
train loss:  0.43074482679367065
train gradient:  0.11707390820702257
iteration : 12893
train acc:  0.7578125
train loss:  0.48797762393951416
train gradient:  0.10916665291509374
iteration : 12894
train acc:  0.71875
train loss:  0.45319631695747375
train gradient:  0.09010504338680773
iteration : 12895
train acc:  0.78125
train loss:  0.49856311082839966
train gradient:  0.12725841922097794
iteration : 12896
train acc:  0.8046875
train loss:  0.4548376500606537
train gradient:  0.1137752531828946
iteration : 12897
train acc:  0.8125
train loss:  0.42948707938194275
train gradient:  0.09264780574224311
iteration : 12898
train acc:  0.75
train loss:  0.5102630853652954
train gradient:  0.12169263592132826
iteration : 12899
train acc:  0.734375
train loss:  0.4987754821777344
train gradient:  0.1401487599856132
iteration : 12900
train acc:  0.734375
train loss:  0.4968343675136566
train gradient:  0.13085831587579932
iteration : 12901
train acc:  0.7734375
train loss:  0.5280023217201233
train gradient:  0.1320443458161896
iteration : 12902
train acc:  0.78125
train loss:  0.45316946506500244
train gradient:  0.10155664685044788
iteration : 12903
train acc:  0.8203125
train loss:  0.42187029123306274
train gradient:  0.0890597237587412
iteration : 12904
train acc:  0.75
train loss:  0.5131324529647827
train gradient:  0.11432564509029176
iteration : 12905
train acc:  0.8125
train loss:  0.4406740665435791
train gradient:  0.11804905186696515
iteration : 12906
train acc:  0.7734375
train loss:  0.4909449517726898
train gradient:  0.11837616838268078
iteration : 12907
train acc:  0.703125
train loss:  0.5146889686584473
train gradient:  0.11717938895647423
iteration : 12908
train acc:  0.7421875
train loss:  0.5036581158638
train gradient:  0.16561716632872142
iteration : 12909
train acc:  0.7578125
train loss:  0.47993043065071106
train gradient:  0.10149085208612459
iteration : 12910
train acc:  0.734375
train loss:  0.5828971862792969
train gradient:  0.1518792629193526
iteration : 12911
train acc:  0.796875
train loss:  0.4254853129386902
train gradient:  0.08351624123014963
iteration : 12912
train acc:  0.75
train loss:  0.4454261064529419
train gradient:  0.09684027887244434
iteration : 12913
train acc:  0.75
train loss:  0.48275691270828247
train gradient:  0.12187847349926877
iteration : 12914
train acc:  0.734375
train loss:  0.4714800715446472
train gradient:  0.10705328013075963
iteration : 12915
train acc:  0.7265625
train loss:  0.5406026244163513
train gradient:  0.13548783060817776
iteration : 12916
train acc:  0.7578125
train loss:  0.48997214436531067
train gradient:  0.12785041136997438
iteration : 12917
train acc:  0.703125
train loss:  0.5013169050216675
train gradient:  0.13284538143067248
iteration : 12918
train acc:  0.734375
train loss:  0.477276474237442
train gradient:  0.1030783015831398
iteration : 12919
train acc:  0.78125
train loss:  0.45186370611190796
train gradient:  0.11753180876563739
iteration : 12920
train acc:  0.71875
train loss:  0.5190621614456177
train gradient:  0.12811774784196162
iteration : 12921
train acc:  0.6875
train loss:  0.5592597126960754
train gradient:  0.11172203498833812
iteration : 12922
train acc:  0.796875
train loss:  0.4500676393508911
train gradient:  0.0978044944107698
iteration : 12923
train acc:  0.7265625
train loss:  0.4737687110900879
train gradient:  0.120119084562593
iteration : 12924
train acc:  0.7578125
train loss:  0.5132099390029907
train gradient:  0.11419867121517459
iteration : 12925
train acc:  0.7578125
train loss:  0.47838592529296875
train gradient:  0.10903985428456749
iteration : 12926
train acc:  0.78125
train loss:  0.4140687584877014
train gradient:  0.08109857330806171
iteration : 12927
train acc:  0.75
train loss:  0.4794814884662628
train gradient:  0.11021838957754929
iteration : 12928
train acc:  0.78125
train loss:  0.4434751272201538
train gradient:  0.09854598946369485
iteration : 12929
train acc:  0.71875
train loss:  0.5070230960845947
train gradient:  0.13320605675814517
iteration : 12930
train acc:  0.7421875
train loss:  0.48076412081718445
train gradient:  0.1276296107904919
iteration : 12931
train acc:  0.7109375
train loss:  0.5325510501861572
train gradient:  0.14927446131196626
iteration : 12932
train acc:  0.7734375
train loss:  0.46474283933639526
train gradient:  0.1199019208594328
iteration : 12933
train acc:  0.796875
train loss:  0.46216368675231934
train gradient:  0.10430747190902236
iteration : 12934
train acc:  0.7421875
train loss:  0.5259546637535095
train gradient:  0.12352771366428034
iteration : 12935
train acc:  0.7421875
train loss:  0.5034279227256775
train gradient:  0.11920580414254398
iteration : 12936
train acc:  0.8046875
train loss:  0.4498494267463684
train gradient:  0.10048337351885636
iteration : 12937
train acc:  0.6953125
train loss:  0.5410513877868652
train gradient:  0.15670700488656347
iteration : 12938
train acc:  0.7890625
train loss:  0.41697627305984497
train gradient:  0.08077014372443624
iteration : 12939
train acc:  0.7421875
train loss:  0.48198121786117554
train gradient:  0.12984219584856124
iteration : 12940
train acc:  0.703125
train loss:  0.5657318830490112
train gradient:  0.15444234378715194
iteration : 12941
train acc:  0.75
train loss:  0.4799184799194336
train gradient:  0.12209918105176676
iteration : 12942
train acc:  0.734375
train loss:  0.5475027561187744
train gradient:  0.15824624841668322
iteration : 12943
train acc:  0.7421875
train loss:  0.5100502967834473
train gradient:  0.12044157300104502
iteration : 12944
train acc:  0.7734375
train loss:  0.47542455792427063
train gradient:  0.09954576702062422
iteration : 12945
train acc:  0.75
train loss:  0.5517115592956543
train gradient:  0.15679558521248305
iteration : 12946
train acc:  0.75
train loss:  0.4864133894443512
train gradient:  0.13088118266605653
iteration : 12947
train acc:  0.828125
train loss:  0.43579041957855225
train gradient:  0.09777301454711529
iteration : 12948
train acc:  0.7578125
train loss:  0.44423580169677734
train gradient:  0.10497963778788308
iteration : 12949
train acc:  0.796875
train loss:  0.4289872348308563
train gradient:  0.0884378890007974
iteration : 12950
train acc:  0.7421875
train loss:  0.5223969221115112
train gradient:  0.15006283956177374
iteration : 12951
train acc:  0.7265625
train loss:  0.4919281303882599
train gradient:  0.15042684230686865
iteration : 12952
train acc:  0.7421875
train loss:  0.4754375219345093
train gradient:  0.10004060633961018
iteration : 12953
train acc:  0.75
train loss:  0.49375680088996887
train gradient:  0.12458016312825332
iteration : 12954
train acc:  0.734375
train loss:  0.4782140851020813
train gradient:  0.11141282372553905
iteration : 12955
train acc:  0.71875
train loss:  0.519188642501831
train gradient:  0.1397789675905392
iteration : 12956
train acc:  0.8125
train loss:  0.41369086503982544
train gradient:  0.1071838736221749
iteration : 12957
train acc:  0.7734375
train loss:  0.48006951808929443
train gradient:  0.14680760954003413
iteration : 12958
train acc:  0.78125
train loss:  0.4585537314414978
train gradient:  0.09673882312002118
iteration : 12959
train acc:  0.734375
train loss:  0.5162721872329712
train gradient:  0.14048838971886024
iteration : 12960
train acc:  0.734375
train loss:  0.5176740884780884
train gradient:  0.12740583408712763
iteration : 12961
train acc:  0.7890625
train loss:  0.43284738063812256
train gradient:  0.08217755942039985
iteration : 12962
train acc:  0.671875
train loss:  0.5285919904708862
train gradient:  0.12601557032773514
iteration : 12963
train acc:  0.6875
train loss:  0.5309778451919556
train gradient:  0.13783325265069613
iteration : 12964
train acc:  0.6953125
train loss:  0.5140909552574158
train gradient:  0.19641131409613252
iteration : 12965
train acc:  0.78125
train loss:  0.457889199256897
train gradient:  0.1115235993811852
iteration : 12966
train acc:  0.7265625
train loss:  0.5031120181083679
train gradient:  0.13994045998698804
iteration : 12967
train acc:  0.7265625
train loss:  0.4723760783672333
train gradient:  0.12109399151827485
iteration : 12968
train acc:  0.6640625
train loss:  0.6093071699142456
train gradient:  0.18949292082620417
iteration : 12969
train acc:  0.765625
train loss:  0.4272634983062744
train gradient:  0.11491391182911162
iteration : 12970
train acc:  0.6953125
train loss:  0.5288165211677551
train gradient:  0.15362898244642276
iteration : 12971
train acc:  0.6875
train loss:  0.5379481911659241
train gradient:  0.13225992122541932
iteration : 12972
train acc:  0.71875
train loss:  0.5030506253242493
train gradient:  0.13371817502406463
iteration : 12973
train acc:  0.7421875
train loss:  0.49199050664901733
train gradient:  0.15499146580547413
iteration : 12974
train acc:  0.7109375
train loss:  0.49189192056655884
train gradient:  0.12172632076706899
iteration : 12975
train acc:  0.7890625
train loss:  0.4410170912742615
train gradient:  0.10991563629772695
iteration : 12976
train acc:  0.7421875
train loss:  0.4704133868217468
train gradient:  0.11217633527766938
iteration : 12977
train acc:  0.6953125
train loss:  0.5473588705062866
train gradient:  0.14811058450771314
iteration : 12978
train acc:  0.6484375
train loss:  0.5738050937652588
train gradient:  0.19223589705232452
iteration : 12979
train acc:  0.78125
train loss:  0.4014759659767151
train gradient:  0.09127468625812926
iteration : 12980
train acc:  0.7578125
train loss:  0.47850391268730164
train gradient:  0.1336111780859164
iteration : 12981
train acc:  0.7421875
train loss:  0.4961041212081909
train gradient:  0.13827769797899125
iteration : 12982
train acc:  0.7578125
train loss:  0.474515438079834
train gradient:  0.1633660804896946
iteration : 12983
train acc:  0.703125
train loss:  0.5024033188819885
train gradient:  0.12786282686875194
iteration : 12984
train acc:  0.7890625
train loss:  0.42495834827423096
train gradient:  0.09183825119199264
iteration : 12985
train acc:  0.6953125
train loss:  0.5915457010269165
train gradient:  0.15200884794949066
iteration : 12986
train acc:  0.78125
train loss:  0.4471663534641266
train gradient:  0.10485790356029318
iteration : 12987
train acc:  0.7734375
train loss:  0.4519537687301636
train gradient:  0.09831236465225968
iteration : 12988
train acc:  0.7734375
train loss:  0.49127280712127686
train gradient:  0.10473895549676522
iteration : 12989
train acc:  0.8203125
train loss:  0.4116525948047638
train gradient:  0.11320325804448222
iteration : 12990
train acc:  0.703125
train loss:  0.5375745296478271
train gradient:  0.12764774156975622
iteration : 12991
train acc:  0.765625
train loss:  0.4503953754901886
train gradient:  0.1055697232929757
iteration : 12992
train acc:  0.7265625
train loss:  0.5079944133758545
train gradient:  0.14022129300289204
iteration : 12993
train acc:  0.78125
train loss:  0.4608950614929199
train gradient:  0.10463676652026353
iteration : 12994
train acc:  0.75
train loss:  0.5059847831726074
train gradient:  0.1521400687134288
iteration : 12995
train acc:  0.7265625
train loss:  0.4894813299179077
train gradient:  0.15599559313242325
iteration : 12996
train acc:  0.7734375
train loss:  0.49684032797813416
train gradient:  0.1087498451936553
iteration : 12997
train acc:  0.7265625
train loss:  0.5078600645065308
train gradient:  0.15813170232907825
iteration : 12998
train acc:  0.828125
train loss:  0.4758957028388977
train gradient:  0.11218042795642047
iteration : 12999
train acc:  0.7578125
train loss:  0.4724865257740021
train gradient:  0.12695541095326968
iteration : 13000
train acc:  0.6953125
train loss:  0.5348469018936157
train gradient:  0.14935967084259688
iteration : 13001
train acc:  0.78125
train loss:  0.4409233331680298
train gradient:  0.10551250715012604
iteration : 13002
train acc:  0.7890625
train loss:  0.4525376558303833
train gradient:  0.10009708044579535
iteration : 13003
train acc:  0.7734375
train loss:  0.4785333275794983
train gradient:  0.11383660700757377
iteration : 13004
train acc:  0.765625
train loss:  0.46934211254119873
train gradient:  0.11116869255264934
iteration : 13005
train acc:  0.7734375
train loss:  0.452686607837677
train gradient:  0.09046992369229039
iteration : 13006
train acc:  0.71875
train loss:  0.5413821935653687
train gradient:  0.12955770430835778
iteration : 13007
train acc:  0.7734375
train loss:  0.46497440338134766
train gradient:  0.10689502146125204
iteration : 13008
train acc:  0.7421875
train loss:  0.48093125224113464
train gradient:  0.11560274304326668
iteration : 13009
train acc:  0.7578125
train loss:  0.48907947540283203
train gradient:  0.14195238335339966
iteration : 13010
train acc:  0.78125
train loss:  0.4309730529785156
train gradient:  0.1003329155315586
iteration : 13011
train acc:  0.6796875
train loss:  0.554137110710144
train gradient:  0.15528923324515345
iteration : 13012
train acc:  0.7734375
train loss:  0.4990142583847046
train gradient:  0.1329079503881469
iteration : 13013
train acc:  0.7421875
train loss:  0.45961734652519226
train gradient:  0.11078563425126617
iteration : 13014
train acc:  0.7109375
train loss:  0.5823633074760437
train gradient:  0.1527852764623402
iteration : 13015
train acc:  0.734375
train loss:  0.5194388628005981
train gradient:  0.11572970111404488
iteration : 13016
train acc:  0.7109375
train loss:  0.4726935923099518
train gradient:  0.15352033831257167
iteration : 13017
train acc:  0.7578125
train loss:  0.5043359398841858
train gradient:  0.09999026123171031
iteration : 13018
train acc:  0.765625
train loss:  0.4826991558074951
train gradient:  0.1184482716641759
iteration : 13019
train acc:  0.78125
train loss:  0.4093710482120514
train gradient:  0.09086523933218178
iteration : 13020
train acc:  0.703125
train loss:  0.5376657843589783
train gradient:  0.12865647674585345
iteration : 13021
train acc:  0.7578125
train loss:  0.5071952939033508
train gradient:  0.1619081636678646
iteration : 13022
train acc:  0.7890625
train loss:  0.4490974247455597
train gradient:  0.11559853513748526
iteration : 13023
train acc:  0.7265625
train loss:  0.48701903223991394
train gradient:  0.12421128679766791
iteration : 13024
train acc:  0.7890625
train loss:  0.43258118629455566
train gradient:  0.09259855142840137
iteration : 13025
train acc:  0.703125
train loss:  0.5249834060668945
train gradient:  0.17440321564686212
iteration : 13026
train acc:  0.75
train loss:  0.5030624866485596
train gradient:  0.12386042704544649
iteration : 13027
train acc:  0.6875
train loss:  0.5227476358413696
train gradient:  0.148038927455451
iteration : 13028
train acc:  0.7421875
train loss:  0.4896419942378998
train gradient:  0.13225201378085838
iteration : 13029
train acc:  0.75
train loss:  0.4730626940727234
train gradient:  0.12374044831487388
iteration : 13030
train acc:  0.75
train loss:  0.46216264367103577
train gradient:  0.1137379012969443
iteration : 13031
train acc:  0.8046875
train loss:  0.4393128454685211
train gradient:  0.08587917567473223
iteration : 13032
train acc:  0.71875
train loss:  0.5272094011306763
train gradient:  0.16996845333815652
iteration : 13033
train acc:  0.7578125
train loss:  0.4695287048816681
train gradient:  0.11656795214520536
iteration : 13034
train acc:  0.7109375
train loss:  0.5020901560783386
train gradient:  0.10750140569466934
iteration : 13035
train acc:  0.7421875
train loss:  0.48528122901916504
train gradient:  0.09959404653255728
iteration : 13036
train acc:  0.8125
train loss:  0.4169071912765503
train gradient:  0.1230946407772003
iteration : 13037
train acc:  0.6953125
train loss:  0.5238680839538574
train gradient:  0.1546524921007877
iteration : 13038
train acc:  0.703125
train loss:  0.5602473020553589
train gradient:  0.13824499272586827
iteration : 13039
train acc:  0.734375
train loss:  0.4722285270690918
train gradient:  0.14261044989264798
iteration : 13040
train acc:  0.796875
train loss:  0.4309738874435425
train gradient:  0.09114498955258428
iteration : 13041
train acc:  0.6640625
train loss:  0.5801039934158325
train gradient:  0.18206360771276858
iteration : 13042
train acc:  0.765625
train loss:  0.46197783946990967
train gradient:  0.09116342820521542
iteration : 13043
train acc:  0.75
train loss:  0.48253148794174194
train gradient:  0.10443427194739874
iteration : 13044
train acc:  0.765625
train loss:  0.45680737495422363
train gradient:  0.12078036896268744
iteration : 13045
train acc:  0.734375
train loss:  0.4890247881412506
train gradient:  0.16695552919909556
iteration : 13046
train acc:  0.65625
train loss:  0.5639095902442932
train gradient:  0.18532858130991042
iteration : 13047
train acc:  0.7265625
train loss:  0.5181435942649841
train gradient:  0.14915108879687278
iteration : 13048
train acc:  0.7578125
train loss:  0.4865003824234009
train gradient:  0.1055767587084974
iteration : 13049
train acc:  0.6875
train loss:  0.5478789806365967
train gradient:  0.16128657653041079
iteration : 13050
train acc:  0.71875
train loss:  0.5825855731964111
train gradient:  0.15864308310712605
iteration : 13051
train acc:  0.734375
train loss:  0.467046320438385
train gradient:  0.0921485871795485
iteration : 13052
train acc:  0.78125
train loss:  0.48919302225112915
train gradient:  0.12905502725788828
iteration : 13053
train acc:  0.7890625
train loss:  0.5041382908821106
train gradient:  0.12370269702575157
iteration : 13054
train acc:  0.796875
train loss:  0.451820969581604
train gradient:  0.11931495734405072
iteration : 13055
train acc:  0.8125
train loss:  0.47142887115478516
train gradient:  0.13955323362963618
iteration : 13056
train acc:  0.78125
train loss:  0.5280391573905945
train gradient:  0.1429394009978141
iteration : 13057
train acc:  0.71875
train loss:  0.5023742914199829
train gradient:  0.11765347557443187
iteration : 13058
train acc:  0.8046875
train loss:  0.4417341947555542
train gradient:  0.09193914319311866
iteration : 13059
train acc:  0.7578125
train loss:  0.49869489669799805
train gradient:  0.11653169410985127
iteration : 13060
train acc:  0.78125
train loss:  0.4608108401298523
train gradient:  0.11895236682220552
iteration : 13061
train acc:  0.796875
train loss:  0.4199371933937073
train gradient:  0.10328135258041611
iteration : 13062
train acc:  0.75
train loss:  0.46316343545913696
train gradient:  0.10830062624795014
iteration : 13063
train acc:  0.828125
train loss:  0.44375941157341003
train gradient:  0.1078693861412928
iteration : 13064
train acc:  0.703125
train loss:  0.5328507423400879
train gradient:  0.16401619070163975
iteration : 13065
train acc:  0.6875
train loss:  0.5179650187492371
train gradient:  0.1368747528820779
iteration : 13066
train acc:  0.75
train loss:  0.4657236933708191
train gradient:  0.11294084066416098
iteration : 13067
train acc:  0.671875
train loss:  0.5989412665367126
train gradient:  0.20361826951688344
iteration : 13068
train acc:  0.7265625
train loss:  0.5144578218460083
train gradient:  0.11820391822570106
iteration : 13069
train acc:  0.7265625
train loss:  0.550844669342041
train gradient:  0.14826741830908036
iteration : 13070
train acc:  0.796875
train loss:  0.44426843523979187
train gradient:  0.11288314582934371
iteration : 13071
train acc:  0.71875
train loss:  0.49439144134521484
train gradient:  0.11284821232742813
iteration : 13072
train acc:  0.734375
train loss:  0.5051167011260986
train gradient:  0.1279772939708252
iteration : 13073
train acc:  0.7421875
train loss:  0.4968584179878235
train gradient:  0.10492107581840378
iteration : 13074
train acc:  0.7421875
train loss:  0.5108829736709595
train gradient:  0.12096529179632362
iteration : 13075
train acc:  0.734375
train loss:  0.5191582441329956
train gradient:  0.13014818706312536
iteration : 13076
train acc:  0.75
train loss:  0.5104874968528748
train gradient:  0.14694934692816042
iteration : 13077
train acc:  0.7578125
train loss:  0.4996817708015442
train gradient:  0.12246213862051854
iteration : 13078
train acc:  0.7734375
train loss:  0.47450536489486694
train gradient:  0.08741891000742491
iteration : 13079
train acc:  0.78125
train loss:  0.4395183324813843
train gradient:  0.09914030079528231
iteration : 13080
train acc:  0.7734375
train loss:  0.47394195199012756
train gradient:  0.09351501980774746
iteration : 13081
train acc:  0.734375
train loss:  0.44858670234680176
train gradient:  0.10038291848302683
iteration : 13082
train acc:  0.7421875
train loss:  0.4673920273780823
train gradient:  0.09178602857096281
iteration : 13083
train acc:  0.7578125
train loss:  0.525598406791687
train gradient:  0.10326315950476621
iteration : 13084
train acc:  0.765625
train loss:  0.5164775252342224
train gradient:  0.10689461286233398
iteration : 13085
train acc:  0.7578125
train loss:  0.49294060468673706
train gradient:  0.17950456878111865
iteration : 13086
train acc:  0.7578125
train loss:  0.44422072172164917
train gradient:  0.08814591256727558
iteration : 13087
train acc:  0.71875
train loss:  0.5529062747955322
train gradient:  0.14561437349393314
iteration : 13088
train acc:  0.7265625
train loss:  0.5090630054473877
train gradient:  0.13079955408039162
iteration : 13089
train acc:  0.71875
train loss:  0.47905248403549194
train gradient:  0.10308946108778974
iteration : 13090
train acc:  0.7890625
train loss:  0.4059636890888214
train gradient:  0.06635032529842354
iteration : 13091
train acc:  0.75
train loss:  0.47644197940826416
train gradient:  0.10665523176180272
iteration : 13092
train acc:  0.796875
train loss:  0.42759430408477783
train gradient:  0.08490736012723941
iteration : 13093
train acc:  0.8046875
train loss:  0.4175378680229187
train gradient:  0.08167085048730385
iteration : 13094
train acc:  0.6875
train loss:  0.5894736051559448
train gradient:  0.17095988650172933
iteration : 13095
train acc:  0.71875
train loss:  0.5085917115211487
train gradient:  0.1413095297539333
iteration : 13096
train acc:  0.75
train loss:  0.4831928312778473
train gradient:  0.11179623502461936
iteration : 13097
train acc:  0.6875
train loss:  0.5494126081466675
train gradient:  0.13180390769648356
iteration : 13098
train acc:  0.78125
train loss:  0.43636640906333923
train gradient:  0.09933743308776075
iteration : 13099
train acc:  0.75
train loss:  0.4885207414627075
train gradient:  0.14695961595054957
iteration : 13100
train acc:  0.7734375
train loss:  0.44779542088508606
train gradient:  0.11815471489757426
iteration : 13101
train acc:  0.7578125
train loss:  0.4635607600212097
train gradient:  0.1243662276046242
iteration : 13102
train acc:  0.796875
train loss:  0.44753938913345337
train gradient:  0.10583840103433648
iteration : 13103
train acc:  0.71875
train loss:  0.4867735505104065
train gradient:  0.12836351938880802
iteration : 13104
train acc:  0.7578125
train loss:  0.4557073712348938
train gradient:  0.09899382123116063
iteration : 13105
train acc:  0.734375
train loss:  0.4948352873325348
train gradient:  0.13663255756293874
iteration : 13106
train acc:  0.734375
train loss:  0.5492057800292969
train gradient:  0.1290336773985493
iteration : 13107
train acc:  0.8359375
train loss:  0.4136018753051758
train gradient:  0.08072037416168505
iteration : 13108
train acc:  0.765625
train loss:  0.4623766541481018
train gradient:  0.09855728198634298
iteration : 13109
train acc:  0.7265625
train loss:  0.46348655223846436
train gradient:  0.10758178725677457
iteration : 13110
train acc:  0.78125
train loss:  0.5059868097305298
train gradient:  0.13725810943020586
iteration : 13111
train acc:  0.734375
train loss:  0.5200825929641724
train gradient:  0.12630267144512725
iteration : 13112
train acc:  0.75
train loss:  0.5115429162979126
train gradient:  0.13338954256152027
iteration : 13113
train acc:  0.71875
train loss:  0.5011517405509949
train gradient:  0.1208015688450952
iteration : 13114
train acc:  0.7109375
train loss:  0.4964907765388489
train gradient:  0.11977502826188875
iteration : 13115
train acc:  0.7578125
train loss:  0.4711766839027405
train gradient:  0.11340906959266875
iteration : 13116
train acc:  0.7109375
train loss:  0.5400569438934326
train gradient:  0.18876436315089015
iteration : 13117
train acc:  0.7265625
train loss:  0.48511803150177
train gradient:  0.13222504380345207
iteration : 13118
train acc:  0.734375
train loss:  0.561596155166626
train gradient:  0.1514833836100371
iteration : 13119
train acc:  0.734375
train loss:  0.4608812928199768
train gradient:  0.08514370166270523
iteration : 13120
train acc:  0.78125
train loss:  0.4719063639640808
train gradient:  0.13078573835121277
iteration : 13121
train acc:  0.75
train loss:  0.4198513627052307
train gradient:  0.09310375827699795
iteration : 13122
train acc:  0.7265625
train loss:  0.5095816850662231
train gradient:  0.12440165900813926
iteration : 13123
train acc:  0.703125
train loss:  0.4795783758163452
train gradient:  0.14761374181463116
iteration : 13124
train acc:  0.7890625
train loss:  0.4818100035190582
train gradient:  0.10856642158639726
iteration : 13125
train acc:  0.7109375
train loss:  0.5357393622398376
train gradient:  0.12680361950754637
iteration : 13126
train acc:  0.7265625
train loss:  0.5108202695846558
train gradient:  0.12986693195644639
iteration : 13127
train acc:  0.75
train loss:  0.5356165766716003
train gradient:  0.14878395723999477
iteration : 13128
train acc:  0.703125
train loss:  0.5449126958847046
train gradient:  0.11528463398774226
iteration : 13129
train acc:  0.6875
train loss:  0.616110622882843
train gradient:  0.14355851442077144
iteration : 13130
train acc:  0.7109375
train loss:  0.5166254043579102
train gradient:  0.120855654724497
iteration : 13131
train acc:  0.734375
train loss:  0.49842336773872375
train gradient:  0.12461365770897026
iteration : 13132
train acc:  0.7265625
train loss:  0.43446657061576843
train gradient:  0.09131368066817283
iteration : 13133
train acc:  0.7734375
train loss:  0.47434306144714355
train gradient:  0.11948799335912696
iteration : 13134
train acc:  0.6875
train loss:  0.5557141304016113
train gradient:  0.1282948236026833
iteration : 13135
train acc:  0.734375
train loss:  0.4646461606025696
train gradient:  0.09732459900051722
iteration : 13136
train acc:  0.7890625
train loss:  0.4711827039718628
train gradient:  0.09431752601136546
iteration : 13137
train acc:  0.7265625
train loss:  0.4827480912208557
train gradient:  0.15175025848428433
iteration : 13138
train acc:  0.8125
train loss:  0.43692296743392944
train gradient:  0.1219353030969236
iteration : 13139
train acc:  0.6796875
train loss:  0.5172876119613647
train gradient:  0.13214316442371565
iteration : 13140
train acc:  0.8046875
train loss:  0.4582791030406952
train gradient:  0.12880159460972382
iteration : 13141
train acc:  0.734375
train loss:  0.45961418747901917
train gradient:  0.09898779099965431
iteration : 13142
train acc:  0.7734375
train loss:  0.44691070914268494
train gradient:  0.094512931705584
iteration : 13143
train acc:  0.796875
train loss:  0.41263818740844727
train gradient:  0.07807401272049733
iteration : 13144
train acc:  0.796875
train loss:  0.48038381338119507
train gradient:  0.1270905511377407
iteration : 13145
train acc:  0.7890625
train loss:  0.431500107049942
train gradient:  0.10259651170938135
iteration : 13146
train acc:  0.7734375
train loss:  0.448747843503952
train gradient:  0.11300038672988365
iteration : 13147
train acc:  0.734375
train loss:  0.4852793216705322
train gradient:  0.13190593084437668
iteration : 13148
train acc:  0.7890625
train loss:  0.4658423662185669
train gradient:  0.1636955222342547
iteration : 13149
train acc:  0.75
train loss:  0.5213188529014587
train gradient:  0.15413800287012397
iteration : 13150
train acc:  0.7265625
train loss:  0.4874109625816345
train gradient:  0.1347252710653763
iteration : 13151
train acc:  0.7421875
train loss:  0.5067839622497559
train gradient:  0.1429030734589335
iteration : 13152
train acc:  0.78125
train loss:  0.5161284804344177
train gradient:  0.14280274544630278
iteration : 13153
train acc:  0.765625
train loss:  0.4756660461425781
train gradient:  0.1504839617838597
iteration : 13154
train acc:  0.75
train loss:  0.5057675838470459
train gradient:  0.15226424914414505
iteration : 13155
train acc:  0.7421875
train loss:  0.46087974309921265
train gradient:  0.11800489870061052
iteration : 13156
train acc:  0.78125
train loss:  0.47959667444229126
train gradient:  0.12328755202154125
iteration : 13157
train acc:  0.7421875
train loss:  0.47543472051620483
train gradient:  0.11861011632138596
iteration : 13158
train acc:  0.7890625
train loss:  0.4044643044471741
train gradient:  0.0674688151789951
iteration : 13159
train acc:  0.7265625
train loss:  0.5057662725448608
train gradient:  0.11895032382863097
iteration : 13160
train acc:  0.75
train loss:  0.47322115302085876
train gradient:  0.09004826309652614
iteration : 13161
train acc:  0.765625
train loss:  0.4905681908130646
train gradient:  0.1330764409687572
iteration : 13162
train acc:  0.7890625
train loss:  0.46385490894317627
train gradient:  0.1029124268483736
iteration : 13163
train acc:  0.734375
train loss:  0.4746837913990021
train gradient:  0.11651076576403065
iteration : 13164
train acc:  0.7578125
train loss:  0.5140873193740845
train gradient:  0.14847541658278224
iteration : 13165
train acc:  0.71875
train loss:  0.5223473310470581
train gradient:  0.16047135633323356
iteration : 13166
train acc:  0.765625
train loss:  0.4858568608760834
train gradient:  0.12841352744973356
iteration : 13167
train acc:  0.71875
train loss:  0.5346611738204956
train gradient:  0.16496764037022
iteration : 13168
train acc:  0.7734375
train loss:  0.4713807702064514
train gradient:  0.11048240040156347
iteration : 13169
train acc:  0.71875
train loss:  0.4808669090270996
train gradient:  0.11912271161327526
iteration : 13170
train acc:  0.7109375
train loss:  0.5436248779296875
train gradient:  0.1478302388830898
iteration : 13171
train acc:  0.6953125
train loss:  0.5325554609298706
train gradient:  0.1315112014042824
iteration : 13172
train acc:  0.7578125
train loss:  0.4589705467224121
train gradient:  0.1281509473885154
iteration : 13173
train acc:  0.8046875
train loss:  0.42331185936927795
train gradient:  0.08648247739501022
iteration : 13174
train acc:  0.7421875
train loss:  0.49259644746780396
train gradient:  0.13407007979397698
iteration : 13175
train acc:  0.7734375
train loss:  0.46189451217651367
train gradient:  0.10028825974420391
iteration : 13176
train acc:  0.671875
train loss:  0.6323559284210205
train gradient:  0.17175815332345779
iteration : 13177
train acc:  0.7578125
train loss:  0.46990659832954407
train gradient:  0.11681077488931377
iteration : 13178
train acc:  0.734375
train loss:  0.47082334756851196
train gradient:  0.11882091340216008
iteration : 13179
train acc:  0.7421875
train loss:  0.45563623309135437
train gradient:  0.12085205868487245
iteration : 13180
train acc:  0.71875
train loss:  0.5188279747962952
train gradient:  0.11926684995484903
iteration : 13181
train acc:  0.7109375
train loss:  0.5318292379379272
train gradient:  0.11943739664675708
iteration : 13182
train acc:  0.7109375
train loss:  0.5099499821662903
train gradient:  0.125619423775829
iteration : 13183
train acc:  0.7421875
train loss:  0.46878212690353394
train gradient:  0.1178864659641365
iteration : 13184
train acc:  0.8125
train loss:  0.42209434509277344
train gradient:  0.09104992807717271
iteration : 13185
train acc:  0.703125
train loss:  0.5238357782363892
train gradient:  0.11769668647917565
iteration : 13186
train acc:  0.7109375
train loss:  0.5287892818450928
train gradient:  0.1189249561603963
iteration : 13187
train acc:  0.7109375
train loss:  0.5353636741638184
train gradient:  0.1097158002139319
iteration : 13188
train acc:  0.7578125
train loss:  0.5136752128601074
train gradient:  0.1184841985833102
iteration : 13189
train acc:  0.8359375
train loss:  0.41609448194503784
train gradient:  0.12590267574130137
iteration : 13190
train acc:  0.7890625
train loss:  0.4216182231903076
train gradient:  0.08713249991545571
iteration : 13191
train acc:  0.71875
train loss:  0.5266064405441284
train gradient:  0.11846326328977144
iteration : 13192
train acc:  0.7265625
train loss:  0.4881041347980499
train gradient:  0.1047999983071955
iteration : 13193
train acc:  0.78125
train loss:  0.476493775844574
train gradient:  0.1304748407941661
iteration : 13194
train acc:  0.734375
train loss:  0.5569182634353638
train gradient:  0.17570574724542368
iteration : 13195
train acc:  0.828125
train loss:  0.4059644341468811
train gradient:  0.10957571805569943
iteration : 13196
train acc:  0.78125
train loss:  0.5027303099632263
train gradient:  0.12298177058672984
iteration : 13197
train acc:  0.7421875
train loss:  0.489209920167923
train gradient:  0.14564567841164833
iteration : 13198
train acc:  0.8203125
train loss:  0.4520445168018341
train gradient:  0.12235632020653908
iteration : 13199
train acc:  0.75
train loss:  0.5095226764678955
train gradient:  0.18218532689201436
iteration : 13200
train acc:  0.7421875
train loss:  0.4940740168094635
train gradient:  0.1114884398299578
iteration : 13201
train acc:  0.78125
train loss:  0.43873757123947144
train gradient:  0.09813706226104023
iteration : 13202
train acc:  0.765625
train loss:  0.4235003888607025
train gradient:  0.08525937288829538
iteration : 13203
train acc:  0.7421875
train loss:  0.4737461805343628
train gradient:  0.1297088578359671
iteration : 13204
train acc:  0.7890625
train loss:  0.44919687509536743
train gradient:  0.11008049486003228
iteration : 13205
train acc:  0.8046875
train loss:  0.46292442083358765
train gradient:  0.1120097601024103
iteration : 13206
train acc:  0.84375
train loss:  0.3944374918937683
train gradient:  0.09595350653298707
iteration : 13207
train acc:  0.71875
train loss:  0.5119905471801758
train gradient:  0.13832489104229304
iteration : 13208
train acc:  0.75
train loss:  0.4956640601158142
train gradient:  0.11765431630403451
iteration : 13209
train acc:  0.796875
train loss:  0.4737757444381714
train gradient:  0.12453056893501535
iteration : 13210
train acc:  0.78125
train loss:  0.45914971828460693
train gradient:  0.10057496987725062
iteration : 13211
train acc:  0.703125
train loss:  0.5381523370742798
train gradient:  0.1365431669218765
iteration : 13212
train acc:  0.75
train loss:  0.45322728157043457
train gradient:  0.09270547801009664
iteration : 13213
train acc:  0.6953125
train loss:  0.5945376753807068
train gradient:  0.1833409807252694
iteration : 13214
train acc:  0.8515625
train loss:  0.4182402491569519
train gradient:  0.10861595731803012
iteration : 13215
train acc:  0.734375
train loss:  0.5140044689178467
train gradient:  0.13491546554349312
iteration : 13216
train acc:  0.8203125
train loss:  0.3833586275577545
train gradient:  0.07759329514210363
iteration : 13217
train acc:  0.703125
train loss:  0.5460596084594727
train gradient:  0.15429903578679438
iteration : 13218
train acc:  0.78125
train loss:  0.461702823638916
train gradient:  0.128314439272463
iteration : 13219
train acc:  0.7109375
train loss:  0.5449903607368469
train gradient:  0.15650737022201477
iteration : 13220
train acc:  0.7265625
train loss:  0.4521101117134094
train gradient:  0.09460385255003642
iteration : 13221
train acc:  0.7421875
train loss:  0.4670610725879669
train gradient:  0.11807694517592178
iteration : 13222
train acc:  0.7734375
train loss:  0.4663705825805664
train gradient:  0.118885579944547
iteration : 13223
train acc:  0.7109375
train loss:  0.49902740120887756
train gradient:  0.1271174890715233
iteration : 13224
train acc:  0.765625
train loss:  0.5056444406509399
train gradient:  0.14046383588036865
iteration : 13225
train acc:  0.8046875
train loss:  0.43979132175445557
train gradient:  0.09747678064560943
iteration : 13226
train acc:  0.78125
train loss:  0.448052316904068
train gradient:  0.11175292533375575
iteration : 13227
train acc:  0.75
train loss:  0.46170854568481445
train gradient:  0.15230932293452473
iteration : 13228
train acc:  0.78125
train loss:  0.44234591722488403
train gradient:  0.13268026969004892
iteration : 13229
train acc:  0.7734375
train loss:  0.47078073024749756
train gradient:  0.13383758777619265
iteration : 13230
train acc:  0.796875
train loss:  0.4898608326911926
train gradient:  0.14973599406653082
iteration : 13231
train acc:  0.765625
train loss:  0.4876038134098053
train gradient:  0.13509260740100118
iteration : 13232
train acc:  0.765625
train loss:  0.4764854907989502
train gradient:  0.11779301079864768
iteration : 13233
train acc:  0.828125
train loss:  0.3963661193847656
train gradient:  0.08611955113756517
iteration : 13234
train acc:  0.71875
train loss:  0.4729003310203552
train gradient:  0.09654147816995144
iteration : 13235
train acc:  0.7578125
train loss:  0.5010743141174316
train gradient:  0.14154757198796997
iteration : 13236
train acc:  0.71875
train loss:  0.48957717418670654
train gradient:  0.11652715812492838
iteration : 13237
train acc:  0.7421875
train loss:  0.4980089068412781
train gradient:  0.11544343645072314
iteration : 13238
train acc:  0.71875
train loss:  0.4761882424354553
train gradient:  0.13042930010047926
iteration : 13239
train acc:  0.71875
train loss:  0.4710733890533447
train gradient:  0.10085709072265087
iteration : 13240
train acc:  0.75
train loss:  0.4962591230869293
train gradient:  0.1258379145368526
iteration : 13241
train acc:  0.703125
train loss:  0.5539867877960205
train gradient:  0.1524239898375624
iteration : 13242
train acc:  0.8203125
train loss:  0.444640576839447
train gradient:  0.09192694110135133
iteration : 13243
train acc:  0.7734375
train loss:  0.44761836528778076
train gradient:  0.1237631902809971
iteration : 13244
train acc:  0.7734375
train loss:  0.4624820351600647
train gradient:  0.17028462738078903
iteration : 13245
train acc:  0.7578125
train loss:  0.4645770788192749
train gradient:  0.10640309104255156
iteration : 13246
train acc:  0.71875
train loss:  0.5556641817092896
train gradient:  0.1668360780267774
iteration : 13247
train acc:  0.7578125
train loss:  0.45045578479766846
train gradient:  0.12236708097852564
iteration : 13248
train acc:  0.671875
train loss:  0.5203450322151184
train gradient:  0.14833187919381857
iteration : 13249
train acc:  0.703125
train loss:  0.47831490635871887
train gradient:  0.12058889312183182
iteration : 13250
train acc:  0.6953125
train loss:  0.55525803565979
train gradient:  0.1387257326921909
iteration : 13251
train acc:  0.75
train loss:  0.47786301374435425
train gradient:  0.150613402679605
iteration : 13252
train acc:  0.7734375
train loss:  0.47026896476745605
train gradient:  0.11260640708154605
iteration : 13253
train acc:  0.7421875
train loss:  0.47154489159584045
train gradient:  0.12513075026317944
iteration : 13254
train acc:  0.7265625
train loss:  0.5009140372276306
train gradient:  0.10717963448694719
iteration : 13255
train acc:  0.796875
train loss:  0.45880961418151855
train gradient:  0.12835523792356343
iteration : 13256
train acc:  0.7265625
train loss:  0.5225917100906372
train gradient:  0.1241721474317348
iteration : 13257
train acc:  0.734375
train loss:  0.48056912422180176
train gradient:  0.1333679323720401
iteration : 13258
train acc:  0.7109375
train loss:  0.5672518014907837
train gradient:  0.19178541121910334
iteration : 13259
train acc:  0.7421875
train loss:  0.5563305616378784
train gradient:  0.15942156456886367
iteration : 13260
train acc:  0.78125
train loss:  0.4672147035598755
train gradient:  0.10208151712810098
iteration : 13261
train acc:  0.78125
train loss:  0.46174508333206177
train gradient:  0.12778937444578958
iteration : 13262
train acc:  0.7578125
train loss:  0.4907063841819763
train gradient:  0.1363403878935392
iteration : 13263
train acc:  0.734375
train loss:  0.524848222732544
train gradient:  0.12226805521543924
iteration : 13264
train acc:  0.7265625
train loss:  0.46779191493988037
train gradient:  0.12543745282821817
iteration : 13265
train acc:  0.7421875
train loss:  0.5333544015884399
train gradient:  0.12232416700755189
iteration : 13266
train acc:  0.765625
train loss:  0.45972105860710144
train gradient:  0.12949611211312106
iteration : 13267
train acc:  0.703125
train loss:  0.47641608119010925
train gradient:  0.10628602330415304
iteration : 13268
train acc:  0.7578125
train loss:  0.4890636205673218
train gradient:  0.140206867742471
iteration : 13269
train acc:  0.78125
train loss:  0.4591321647167206
train gradient:  0.11929136814389875
iteration : 13270
train acc:  0.75
train loss:  0.48441123962402344
train gradient:  0.10884854547343262
iteration : 13271
train acc:  0.7109375
train loss:  0.5616766214370728
train gradient:  0.15029823004252657
iteration : 13272
train acc:  0.734375
train loss:  0.5113170146942139
train gradient:  0.10806040456115008
iteration : 13273
train acc:  0.7265625
train loss:  0.5323818325996399
train gradient:  0.14004513503951496
iteration : 13274
train acc:  0.78125
train loss:  0.4810572862625122
train gradient:  0.1146295494385664
iteration : 13275
train acc:  0.734375
train loss:  0.5157148241996765
train gradient:  0.12557541737768005
iteration : 13276
train acc:  0.734375
train loss:  0.47824275493621826
train gradient:  0.11036673878459599
iteration : 13277
train acc:  0.6796875
train loss:  0.5193232297897339
train gradient:  0.15422416187532306
iteration : 13278
train acc:  0.7265625
train loss:  0.48256415128707886
train gradient:  0.14864381142929317
iteration : 13279
train acc:  0.6953125
train loss:  0.5487810969352722
train gradient:  0.17995640305556634
iteration : 13280
train acc:  0.703125
train loss:  0.5071889162063599
train gradient:  0.1517641021106941
iteration : 13281
train acc:  0.7265625
train loss:  0.48987433314323425
train gradient:  0.14398396243037953
iteration : 13282
train acc:  0.7890625
train loss:  0.444693922996521
train gradient:  0.1206112500527319
iteration : 13283
train acc:  0.8203125
train loss:  0.44563981890678406
train gradient:  0.10798866749376804
iteration : 13284
train acc:  0.8125
train loss:  0.4148428440093994
train gradient:  0.08385706465965258
iteration : 13285
train acc:  0.765625
train loss:  0.4578813314437866
train gradient:  0.1150788090205573
iteration : 13286
train acc:  0.734375
train loss:  0.5225080847740173
train gradient:  0.16187098705266653
iteration : 13287
train acc:  0.734375
train loss:  0.5071209669113159
train gradient:  0.13259187264405037
iteration : 13288
train acc:  0.7421875
train loss:  0.5483229160308838
train gradient:  0.17998834947210052
iteration : 13289
train acc:  0.78125
train loss:  0.4917917549610138
train gradient:  0.11939104983530975
iteration : 13290
train acc:  0.765625
train loss:  0.46704643964767456
train gradient:  0.12858003649730532
iteration : 13291
train acc:  0.7421875
train loss:  0.5053362250328064
train gradient:  0.16547237022405487
iteration : 13292
train acc:  0.7578125
train loss:  0.5306484699249268
train gradient:  0.13020796941776847
iteration : 13293
train acc:  0.71875
train loss:  0.5278031229972839
train gradient:  0.12120640540093439
iteration : 13294
train acc:  0.7421875
train loss:  0.5112741589546204
train gradient:  0.13692327374155058
iteration : 13295
train acc:  0.7578125
train loss:  0.47176656126976013
train gradient:  0.11984179359037349
iteration : 13296
train acc:  0.7265625
train loss:  0.5079468488693237
train gradient:  0.10892197486538951
iteration : 13297
train acc:  0.734375
train loss:  0.48025742173194885
train gradient:  0.13180470810679928
iteration : 13298
train acc:  0.7421875
train loss:  0.46297597885131836
train gradient:  0.10896620606414922
iteration : 13299
train acc:  0.765625
train loss:  0.5137466788291931
train gradient:  0.12361412264609352
iteration : 13300
train acc:  0.7734375
train loss:  0.43268099427223206
train gradient:  0.08954549837124613
iteration : 13301
train acc:  0.8046875
train loss:  0.42607271671295166
train gradient:  0.0939991018032717
iteration : 13302
train acc:  0.71875
train loss:  0.5545194745063782
train gradient:  0.14760128224337277
iteration : 13303
train acc:  0.75
train loss:  0.4647076725959778
train gradient:  0.1266425902178142
iteration : 13304
train acc:  0.78125
train loss:  0.4318489730358124
train gradient:  0.09961604266516287
iteration : 13305
train acc:  0.75
train loss:  0.5552384257316589
train gradient:  0.13827458211934984
iteration : 13306
train acc:  0.6953125
train loss:  0.4794481098651886
train gradient:  0.09441450263179181
iteration : 13307
train acc:  0.6796875
train loss:  0.5130682587623596
train gradient:  0.13855861933263341
iteration : 13308
train acc:  0.7421875
train loss:  0.4786791205406189
train gradient:  0.121683722777844
iteration : 13309
train acc:  0.7578125
train loss:  0.5003436803817749
train gradient:  0.1352780774767754
iteration : 13310
train acc:  0.6640625
train loss:  0.49578750133514404
train gradient:  0.10171699058522422
iteration : 13311
train acc:  0.6484375
train loss:  0.5568244457244873
train gradient:  0.1302974391214996
iteration : 13312
train acc:  0.7734375
train loss:  0.4526480734348297
train gradient:  0.11965383069536226
iteration : 13313
train acc:  0.7265625
train loss:  0.49964213371276855
train gradient:  0.14768497031782118
iteration : 13314
train acc:  0.6875
train loss:  0.5713311433792114
train gradient:  0.1565545002106566
iteration : 13315
train acc:  0.7890625
train loss:  0.4657561779022217
train gradient:  0.1793226190050622
iteration : 13316
train acc:  0.78125
train loss:  0.4747141897678375
train gradient:  0.11641360881807448
iteration : 13317
train acc:  0.78125
train loss:  0.4796603322029114
train gradient:  0.12260088098543438
iteration : 13318
train acc:  0.734375
train loss:  0.4794914722442627
train gradient:  0.10710876187552489
iteration : 13319
train acc:  0.7265625
train loss:  0.5392767786979675
train gradient:  0.12356905509297313
iteration : 13320
train acc:  0.7578125
train loss:  0.4888671636581421
train gradient:  0.1296758203131478
iteration : 13321
train acc:  0.8203125
train loss:  0.41363632678985596
train gradient:  0.08892849865476826
iteration : 13322
train acc:  0.6953125
train loss:  0.5040906667709351
train gradient:  0.1304264797711384
iteration : 13323
train acc:  0.75
train loss:  0.5182287693023682
train gradient:  0.15879501668923007
iteration : 13324
train acc:  0.7109375
train loss:  0.5103412866592407
train gradient:  0.12207314939390491
iteration : 13325
train acc:  0.78125
train loss:  0.48703813552856445
train gradient:  0.13596639100849295
iteration : 13326
train acc:  0.75
train loss:  0.5401397943496704
train gradient:  0.15085581482957527
iteration : 13327
train acc:  0.75
train loss:  0.47295546531677246
train gradient:  0.10118166064025426
iteration : 13328
train acc:  0.6875
train loss:  0.5246468186378479
train gradient:  0.12850595587642286
iteration : 13329
train acc:  0.7890625
train loss:  0.4823002815246582
train gradient:  0.13690409625499286
iteration : 13330
train acc:  0.8125
train loss:  0.4347172975540161
train gradient:  0.09844020008055886
iteration : 13331
train acc:  0.75
train loss:  0.5202460289001465
train gradient:  0.12723284115930733
iteration : 13332
train acc:  0.796875
train loss:  0.4116283059120178
train gradient:  0.08949294655240358
iteration : 13333
train acc:  0.7734375
train loss:  0.44113677740097046
train gradient:  0.12031646823202952
iteration : 13334
train acc:  0.8125
train loss:  0.40074482560157776
train gradient:  0.081114689733616
iteration : 13335
train acc:  0.703125
train loss:  0.5317926406860352
train gradient:  0.13275153095643463
iteration : 13336
train acc:  0.6640625
train loss:  0.593305766582489
train gradient:  0.2031411823458601
iteration : 13337
train acc:  0.7265625
train loss:  0.5074934959411621
train gradient:  0.12739141704015922
iteration : 13338
train acc:  0.7578125
train loss:  0.48533278703689575
train gradient:  0.14503970667102456
iteration : 13339
train acc:  0.7734375
train loss:  0.4184797406196594
train gradient:  0.10709074131071962
iteration : 13340
train acc:  0.7890625
train loss:  0.4499358534812927
train gradient:  0.11272418093382601
iteration : 13341
train acc:  0.7421875
train loss:  0.46089795231819153
train gradient:  0.09624368666028307
iteration : 13342
train acc:  0.6796875
train loss:  0.5347524881362915
train gradient:  0.15844588449361763
iteration : 13343
train acc:  0.765625
train loss:  0.464341938495636
train gradient:  0.12941198222980493
iteration : 13344
train acc:  0.8046875
train loss:  0.5066383481025696
train gradient:  0.10097438166500125
iteration : 13345
train acc:  0.7578125
train loss:  0.4968370497226715
train gradient:  0.10935804051056337
iteration : 13346
train acc:  0.7578125
train loss:  0.559113621711731
train gradient:  0.15391689638602196
iteration : 13347
train acc:  0.6953125
train loss:  0.6022360324859619
train gradient:  0.16799836099140714
iteration : 13348
train acc:  0.765625
train loss:  0.4415116012096405
train gradient:  0.09077904300631662
iteration : 13349
train acc:  0.7890625
train loss:  0.44198107719421387
train gradient:  0.08852474569386674
iteration : 13350
train acc:  0.7578125
train loss:  0.45531076192855835
train gradient:  0.1152187284483398
iteration : 13351
train acc:  0.8359375
train loss:  0.4195740222930908
train gradient:  0.08937301135290902
iteration : 13352
train acc:  0.7734375
train loss:  0.519634485244751
train gradient:  0.16014732612414345
iteration : 13353
train acc:  0.671875
train loss:  0.6275924444198608
train gradient:  0.20921407863921687
iteration : 13354
train acc:  0.6796875
train loss:  0.5449869632720947
train gradient:  0.1642046293644679
iteration : 13355
train acc:  0.7578125
train loss:  0.4282872676849365
train gradient:  0.0950118479534159
iteration : 13356
train acc:  0.7578125
train loss:  0.5073652863502502
train gradient:  0.1141320920200241
iteration : 13357
train acc:  0.765625
train loss:  0.4540507197380066
train gradient:  0.09565353899297759
iteration : 13358
train acc:  0.8125
train loss:  0.3715352416038513
train gradient:  0.07514984490863291
iteration : 13359
train acc:  0.796875
train loss:  0.4654991626739502
train gradient:  0.10696193969109478
iteration : 13360
train acc:  0.7265625
train loss:  0.49992358684539795
train gradient:  0.12498670465947817
iteration : 13361
train acc:  0.7265625
train loss:  0.4830304980278015
train gradient:  0.11003753460504877
iteration : 13362
train acc:  0.7734375
train loss:  0.45870494842529297
train gradient:  0.10047498632710665
iteration : 13363
train acc:  0.75
train loss:  0.4532882273197174
train gradient:  0.13269658066347306
iteration : 13364
train acc:  0.734375
train loss:  0.5621770620346069
train gradient:  0.13143598735466258
iteration : 13365
train acc:  0.7890625
train loss:  0.4684202969074249
train gradient:  0.12451535001544584
iteration : 13366
train acc:  0.765625
train loss:  0.46651721000671387
train gradient:  0.11029305483717886
iteration : 13367
train acc:  0.734375
train loss:  0.5224617123603821
train gradient:  0.11615664471496887
iteration : 13368
train acc:  0.7578125
train loss:  0.47496020793914795
train gradient:  0.1185136989224785
iteration : 13369
train acc:  0.78125
train loss:  0.41674861311912537
train gradient:  0.0933986633257951
iteration : 13370
train acc:  0.7109375
train loss:  0.4763205647468567
train gradient:  0.1086297616490605
iteration : 13371
train acc:  0.78125
train loss:  0.47824713587760925
train gradient:  0.14702450239844367
iteration : 13372
train acc:  0.734375
train loss:  0.4648101031780243
train gradient:  0.09964197061798526
iteration : 13373
train acc:  0.734375
train loss:  0.5338404178619385
train gradient:  0.15331732243663687
iteration : 13374
train acc:  0.765625
train loss:  0.4781638979911804
train gradient:  0.11405282362984161
iteration : 13375
train acc:  0.7890625
train loss:  0.44460558891296387
train gradient:  0.0893224174782164
iteration : 13376
train acc:  0.7109375
train loss:  0.5445864200592041
train gradient:  0.1699515489204098
iteration : 13377
train acc:  0.7734375
train loss:  0.46836182475090027
train gradient:  0.1148602599825116
iteration : 13378
train acc:  0.7890625
train loss:  0.5222851634025574
train gradient:  0.13884193587451724
iteration : 13379
train acc:  0.71875
train loss:  0.5044925212860107
train gradient:  0.11929153939620615
iteration : 13380
train acc:  0.734375
train loss:  0.49467945098876953
train gradient:  0.12879076746119883
iteration : 13381
train acc:  0.703125
train loss:  0.4729056656360626
train gradient:  0.11081521627376226
iteration : 13382
train acc:  0.734375
train loss:  0.5083401799201965
train gradient:  0.11836666573037283
iteration : 13383
train acc:  0.7265625
train loss:  0.48032593727111816
train gradient:  0.1268736684378029
iteration : 13384
train acc:  0.78125
train loss:  0.44190335273742676
train gradient:  0.10529274393643745
iteration : 13385
train acc:  0.71875
train loss:  0.4684171676635742
train gradient:  0.12260946380236548
iteration : 13386
train acc:  0.6875
train loss:  0.537335991859436
train gradient:  0.14090987611417724
iteration : 13387
train acc:  0.765625
train loss:  0.49940213561058044
train gradient:  0.10699346805944848
iteration : 13388
train acc:  0.7265625
train loss:  0.498813271522522
train gradient:  0.1258211194567688
iteration : 13389
train acc:  0.8125
train loss:  0.4147264063358307
train gradient:  0.09752035916770874
iteration : 13390
train acc:  0.7578125
train loss:  0.4776260554790497
train gradient:  0.09367906776343687
iteration : 13391
train acc:  0.75
train loss:  0.501393735408783
train gradient:  0.13133226488485092
iteration : 13392
train acc:  0.7578125
train loss:  0.4829676151275635
train gradient:  0.13012292210484364
iteration : 13393
train acc:  0.75
train loss:  0.5035736560821533
train gradient:  0.12854759680256733
iteration : 13394
train acc:  0.7734375
train loss:  0.4631190299987793
train gradient:  0.09916582214428615
iteration : 13395
train acc:  0.6953125
train loss:  0.5524786710739136
train gradient:  0.18762153179764812
iteration : 13396
train acc:  0.6953125
train loss:  0.5497773885726929
train gradient:  0.20945441174989704
iteration : 13397
train acc:  0.703125
train loss:  0.4893953800201416
train gradient:  0.13445643532529736
iteration : 13398
train acc:  0.7421875
train loss:  0.4566268026828766
train gradient:  0.10665530089969125
iteration : 13399
train acc:  0.6953125
train loss:  0.5293742418289185
train gradient:  0.12938584743074627
iteration : 13400
train acc:  0.8046875
train loss:  0.41092562675476074
train gradient:  0.09150426346439537
iteration : 13401
train acc:  0.7265625
train loss:  0.493108332157135
train gradient:  0.09567517552628574
iteration : 13402
train acc:  0.71875
train loss:  0.48799440264701843
train gradient:  0.11945968398020064
iteration : 13403
train acc:  0.7421875
train loss:  0.5200530886650085
train gradient:  0.1412444371014812
iteration : 13404
train acc:  0.7109375
train loss:  0.5123499631881714
train gradient:  0.12564227623973123
iteration : 13405
train acc:  0.7265625
train loss:  0.48097383975982666
train gradient:  0.11857091275741757
iteration : 13406
train acc:  0.765625
train loss:  0.46248698234558105
train gradient:  0.11444801023632781
iteration : 13407
train acc:  0.7109375
train loss:  0.5090885162353516
train gradient:  0.1303896847656908
iteration : 13408
train acc:  0.7421875
train loss:  0.4808708131313324
train gradient:  0.10591702129620054
iteration : 13409
train acc:  0.75
train loss:  0.47428619861602783
train gradient:  0.11145629682840784
iteration : 13410
train acc:  0.7734375
train loss:  0.41970616579055786
train gradient:  0.10580841407602588
iteration : 13411
train acc:  0.7421875
train loss:  0.4748903512954712
train gradient:  0.12076018602033549
iteration : 13412
train acc:  0.765625
train loss:  0.4525088667869568
train gradient:  0.10159536127859341
iteration : 13413
train acc:  0.734375
train loss:  0.5122920274734497
train gradient:  0.16376414452528543
iteration : 13414
train acc:  0.765625
train loss:  0.46536046266555786
train gradient:  0.12457332799576532
iteration : 13415
train acc:  0.7734375
train loss:  0.5020719766616821
train gradient:  0.12545588708178962
iteration : 13416
train acc:  0.75
train loss:  0.5120604634284973
train gradient:  0.15659652086013653
iteration : 13417
train acc:  0.7421875
train loss:  0.4635908901691437
train gradient:  0.11151432872687994
iteration : 13418
train acc:  0.7578125
train loss:  0.49144721031188965
train gradient:  0.13005182027329942
iteration : 13419
train acc:  0.78125
train loss:  0.4353151321411133
train gradient:  0.12314577188634236
iteration : 13420
train acc:  0.7109375
train loss:  0.5516520738601685
train gradient:  0.13769200090960254
iteration : 13421
train acc:  0.75
train loss:  0.495890736579895
train gradient:  0.11762503891403069
iteration : 13422
train acc:  0.7265625
train loss:  0.5223861336708069
train gradient:  0.14389927872049957
iteration : 13423
train acc:  0.8046875
train loss:  0.4305395483970642
train gradient:  0.11206885661721225
iteration : 13424
train acc:  0.8359375
train loss:  0.41623854637145996
train gradient:  0.1116428937762305
iteration : 13425
train acc:  0.7265625
train loss:  0.5126096606254578
train gradient:  0.158680316888813
iteration : 13426
train acc:  0.703125
train loss:  0.5551254749298096
train gradient:  0.1371408722971485
iteration : 13427
train acc:  0.6953125
train loss:  0.4867268204689026
train gradient:  0.12025887666480282
iteration : 13428
train acc:  0.765625
train loss:  0.48154228925704956
train gradient:  0.11851087313604591
iteration : 13429
train acc:  0.7421875
train loss:  0.43572819232940674
train gradient:  0.0962999992570644
iteration : 13430
train acc:  0.7265625
train loss:  0.4575352370738983
train gradient:  0.1112490543704838
iteration : 13431
train acc:  0.71875
train loss:  0.5024462342262268
train gradient:  0.12550004624945715
iteration : 13432
train acc:  0.7734375
train loss:  0.49491605162620544
train gradient:  0.12596179476626448
iteration : 13433
train acc:  0.640625
train loss:  0.6101768612861633
train gradient:  0.16476633887737108
iteration : 13434
train acc:  0.7578125
train loss:  0.5087274312973022
train gradient:  0.13265294341540554
iteration : 13435
train acc:  0.765625
train loss:  0.49268946051597595
train gradient:  0.1361862004752425
iteration : 13436
train acc:  0.796875
train loss:  0.3821447491645813
train gradient:  0.07999497415576204
iteration : 13437
train acc:  0.75
train loss:  0.47162914276123047
train gradient:  0.10658438119252912
iteration : 13438
train acc:  0.6796875
train loss:  0.562995433807373
train gradient:  0.14206403579353055
iteration : 13439
train acc:  0.7421875
train loss:  0.48625636100769043
train gradient:  0.10205277284616131
iteration : 13440
train acc:  0.75
train loss:  0.5059337615966797
train gradient:  0.15450048453170806
iteration : 13441
train acc:  0.734375
train loss:  0.5339064598083496
train gradient:  0.1498333519970318
iteration : 13442
train acc:  0.765625
train loss:  0.4274252653121948
train gradient:  0.09875948468493786
iteration : 13443
train acc:  0.8203125
train loss:  0.4159761667251587
train gradient:  0.11730481270559581
iteration : 13444
train acc:  0.7890625
train loss:  0.4082627296447754
train gradient:  0.07631358895997253
iteration : 13445
train acc:  0.734375
train loss:  0.5141171216964722
train gradient:  0.13007792605975185
iteration : 13446
train acc:  0.7578125
train loss:  0.5379277467727661
train gradient:  0.16066005709111203
iteration : 13447
train acc:  0.859375
train loss:  0.3679143488407135
train gradient:  0.10645436923889771
iteration : 13448
train acc:  0.6953125
train loss:  0.5634680390357971
train gradient:  0.14041135782112804
iteration : 13449
train acc:  0.6953125
train loss:  0.5251171588897705
train gradient:  0.14696184401116075
iteration : 13450
train acc:  0.7265625
train loss:  0.5231437683105469
train gradient:  0.12826201671523152
iteration : 13451
train acc:  0.78125
train loss:  0.4538896679878235
train gradient:  0.12484438924825189
iteration : 13452
train acc:  0.8046875
train loss:  0.4407169818878174
train gradient:  0.10460004359408434
iteration : 13453
train acc:  0.765625
train loss:  0.5138266086578369
train gradient:  0.13782711430856215
iteration : 13454
train acc:  0.7578125
train loss:  0.5115026235580444
train gradient:  0.13487193241797668
iteration : 13455
train acc:  0.7109375
train loss:  0.5581653118133545
train gradient:  0.1523061838716581
iteration : 13456
train acc:  0.734375
train loss:  0.507007360458374
train gradient:  0.14488821791008172
iteration : 13457
train acc:  0.8046875
train loss:  0.473092257976532
train gradient:  0.1327567208558662
iteration : 13458
train acc:  0.7578125
train loss:  0.4802511930465698
train gradient:  0.10944831894033893
iteration : 13459
train acc:  0.7421875
train loss:  0.4738815426826477
train gradient:  0.11175581015764365
iteration : 13460
train acc:  0.7734375
train loss:  0.4520799219608307
train gradient:  0.10678220376891141
iteration : 13461
train acc:  0.7734375
train loss:  0.4309994578361511
train gradient:  0.07444209251893534
iteration : 13462
train acc:  0.7265625
train loss:  0.5263864994049072
train gradient:  0.14402707773956758
iteration : 13463
train acc:  0.8046875
train loss:  0.41388407349586487
train gradient:  0.10749048201467187
iteration : 13464
train acc:  0.8203125
train loss:  0.3913591802120209
train gradient:  0.08465501943654828
iteration : 13465
train acc:  0.6953125
train loss:  0.5712519884109497
train gradient:  0.15025484237223127
iteration : 13466
train acc:  0.7265625
train loss:  0.4725029766559601
train gradient:  0.12096750715256055
iteration : 13467
train acc:  0.7734375
train loss:  0.4878157675266266
train gradient:  0.1193276995095135
iteration : 13468
train acc:  0.6875
train loss:  0.46927714347839355
train gradient:  0.11188275956305131
iteration : 13469
train acc:  0.7578125
train loss:  0.5058850049972534
train gradient:  0.11727960004950885
iteration : 13470
train acc:  0.7421875
train loss:  0.46927735209465027
train gradient:  0.12250952664229893
iteration : 13471
train acc:  0.7734375
train loss:  0.44214802980422974
train gradient:  0.09773530426560026
iteration : 13472
train acc:  0.7734375
train loss:  0.47421520948410034
train gradient:  0.09252203803011963
iteration : 13473
train acc:  0.703125
train loss:  0.5495346784591675
train gradient:  0.14858520973076117
iteration : 13474
train acc:  0.671875
train loss:  0.5348436236381531
train gradient:  0.12976990076391842
iteration : 13475
train acc:  0.796875
train loss:  0.46110230684280396
train gradient:  0.0860050894053081
iteration : 13476
train acc:  0.78125
train loss:  0.47167491912841797
train gradient:  0.1057851287490289
iteration : 13477
train acc:  0.71875
train loss:  0.5883309841156006
train gradient:  0.15484647470068474
iteration : 13478
train acc:  0.7890625
train loss:  0.44758328795433044
train gradient:  0.11208737615983713
iteration : 13479
train acc:  0.75
train loss:  0.4708270728588104
train gradient:  0.13000205665691955
iteration : 13480
train acc:  0.71875
train loss:  0.5054632425308228
train gradient:  0.12204379932437437
iteration : 13481
train acc:  0.7578125
train loss:  0.5023545026779175
train gradient:  0.1267565408416278
iteration : 13482
train acc:  0.7265625
train loss:  0.5458887815475464
train gradient:  0.1276274980755309
iteration : 13483
train acc:  0.765625
train loss:  0.46940380334854126
train gradient:  0.10535690433061669
iteration : 13484
train acc:  0.8046875
train loss:  0.4024654030799866
train gradient:  0.0925418671681449
iteration : 13485
train acc:  0.7265625
train loss:  0.5409156084060669
train gradient:  0.12839899289664558
iteration : 13486
train acc:  0.75
train loss:  0.47052785754203796
train gradient:  0.11479558242996013
iteration : 13487
train acc:  0.796875
train loss:  0.4468590319156647
train gradient:  0.09753246257022849
iteration : 13488
train acc:  0.7421875
train loss:  0.4380696415901184
train gradient:  0.1123107430884147
iteration : 13489
train acc:  0.7109375
train loss:  0.5351578593254089
train gradient:  0.16511142543970467
iteration : 13490
train acc:  0.75
train loss:  0.46082407236099243
train gradient:  0.10017272908243868
iteration : 13491
train acc:  0.7578125
train loss:  0.4193788170814514
train gradient:  0.08840440781446303
iteration : 13492
train acc:  0.734375
train loss:  0.5052636861801147
train gradient:  0.12446727732417018
iteration : 13493
train acc:  0.6796875
train loss:  0.5195693969726562
train gradient:  0.14052600660647596
iteration : 13494
train acc:  0.7421875
train loss:  0.4729894995689392
train gradient:  0.12369541919724164
iteration : 13495
train acc:  0.7578125
train loss:  0.4880855083465576
train gradient:  0.099658252983026
iteration : 13496
train acc:  0.71875
train loss:  0.47377222776412964
train gradient:  0.09369375313301685
iteration : 13497
train acc:  0.8046875
train loss:  0.44485652446746826
train gradient:  0.11008676584374186
iteration : 13498
train acc:  0.703125
train loss:  0.4901641607284546
train gradient:  0.13391812395223174
iteration : 13499
train acc:  0.796875
train loss:  0.45179104804992676
train gradient:  0.09612354799362702
iteration : 13500
train acc:  0.765625
train loss:  0.5042742490768433
train gradient:  0.14807803966314773
iteration : 13501
train acc:  0.75
train loss:  0.49723172187805176
train gradient:  0.15110934382552618
iteration : 13502
train acc:  0.765625
train loss:  0.5167884826660156
train gradient:  0.13156419760867527
iteration : 13503
train acc:  0.7734375
train loss:  0.44320669770240784
train gradient:  0.10771552787672599
iteration : 13504
train acc:  0.6953125
train loss:  0.534500241279602
train gradient:  0.16795563780652795
iteration : 13505
train acc:  0.7578125
train loss:  0.4302404224872589
train gradient:  0.08859320573183296
iteration : 13506
train acc:  0.75
train loss:  0.4803813695907593
train gradient:  0.09522179022825616
iteration : 13507
train acc:  0.765625
train loss:  0.5192431211471558
train gradient:  0.19029959109881464
iteration : 13508
train acc:  0.828125
train loss:  0.4151022136211395
train gradient:  0.10878048358184117
iteration : 13509
train acc:  0.765625
train loss:  0.46774452924728394
train gradient:  0.11762977634071944
iteration : 13510
train acc:  0.7890625
train loss:  0.4175912141799927
train gradient:  0.09669301828839312
iteration : 13511
train acc:  0.7890625
train loss:  0.4797281324863434
train gradient:  0.11167098946489938
iteration : 13512
train acc:  0.6796875
train loss:  0.5398582220077515
train gradient:  0.11681470787122594
iteration : 13513
train acc:  0.7734375
train loss:  0.5126988887786865
train gradient:  0.13987278314981338
iteration : 13514
train acc:  0.75
train loss:  0.48365017771720886
train gradient:  0.12087046961083082
iteration : 13515
train acc:  0.796875
train loss:  0.42102962732315063
train gradient:  0.1087822912438126
iteration : 13516
train acc:  0.796875
train loss:  0.4406360983848572
train gradient:  0.09068499654949588
iteration : 13517
train acc:  0.71875
train loss:  0.4870295226573944
train gradient:  0.11471921060676583
iteration : 13518
train acc:  0.7421875
train loss:  0.47236114740371704
train gradient:  0.12120747532355347
iteration : 13519
train acc:  0.671875
train loss:  0.6058732271194458
train gradient:  0.1707284862041114
iteration : 13520
train acc:  0.78125
train loss:  0.49760153889656067
train gradient:  0.12932458761324955
iteration : 13521
train acc:  0.75
train loss:  0.5109539031982422
train gradient:  0.1330777250556981
iteration : 13522
train acc:  0.7578125
train loss:  0.42531925439834595
train gradient:  0.10581165083001086
iteration : 13523
train acc:  0.734375
train loss:  0.4881444573402405
train gradient:  0.1355533812901095
iteration : 13524
train acc:  0.671875
train loss:  0.5442348122596741
train gradient:  0.18119683749143067
iteration : 13525
train acc:  0.71875
train loss:  0.5182796120643616
train gradient:  0.1445432236426963
iteration : 13526
train acc:  0.7890625
train loss:  0.46363595128059387
train gradient:  0.1445222988931439
iteration : 13527
train acc:  0.7890625
train loss:  0.41774487495422363
train gradient:  0.08843066354313718
iteration : 13528
train acc:  0.7578125
train loss:  0.46647438406944275
train gradient:  0.1043849130578775
iteration : 13529
train acc:  0.7578125
train loss:  0.44737231731414795
train gradient:  0.12182763490610239
iteration : 13530
train acc:  0.7578125
train loss:  0.4810238778591156
train gradient:  0.12524812262083007
iteration : 13531
train acc:  0.7109375
train loss:  0.4844282567501068
train gradient:  0.13192683536727956
iteration : 13532
train acc:  0.734375
train loss:  0.49541139602661133
train gradient:  0.11685067378688692
iteration : 13533
train acc:  0.71875
train loss:  0.527767539024353
train gradient:  0.1279669909280086
iteration : 13534
train acc:  0.7421875
train loss:  0.4762193560600281
train gradient:  0.12986703968311247
iteration : 13535
train acc:  0.671875
train loss:  0.5915461182594299
train gradient:  0.18636221319275992
iteration : 13536
train acc:  0.7265625
train loss:  0.4882723093032837
train gradient:  0.14187793781146685
iteration : 13537
train acc:  0.703125
train loss:  0.5405499339103699
train gradient:  0.10844814171756606
iteration : 13538
train acc:  0.7421875
train loss:  0.4686254858970642
train gradient:  0.13420911649137918
iteration : 13539
train acc:  0.765625
train loss:  0.4421188235282898
train gradient:  0.09598409614544687
iteration : 13540
train acc:  0.765625
train loss:  0.4992383122444153
train gradient:  0.13110807673757685
iteration : 13541
train acc:  0.75
train loss:  0.4676089882850647
train gradient:  0.1111509071025897
iteration : 13542
train acc:  0.7578125
train loss:  0.4524732232093811
train gradient:  0.09255736404906395
iteration : 13543
train acc:  0.8125
train loss:  0.45927608013153076
train gradient:  0.1281102669248219
iteration : 13544
train acc:  0.734375
train loss:  0.4544845521450043
train gradient:  0.12105479011300085
iteration : 13545
train acc:  0.734375
train loss:  0.47259098291397095
train gradient:  0.12942756303303227
iteration : 13546
train acc:  0.6953125
train loss:  0.4808487296104431
train gradient:  0.13718495488524696
iteration : 13547
train acc:  0.7578125
train loss:  0.39532047510147095
train gradient:  0.08126363892527534
iteration : 13548
train acc:  0.7421875
train loss:  0.5351008176803589
train gradient:  0.18553908523820237
iteration : 13549
train acc:  0.7890625
train loss:  0.4637887477874756
train gradient:  0.14376375336245734
iteration : 13550
train acc:  0.7578125
train loss:  0.5125285387039185
train gradient:  0.1338039321224013
iteration : 13551
train acc:  0.75
train loss:  0.4961651861667633
train gradient:  0.1569185800438233
iteration : 13552
train acc:  0.7109375
train loss:  0.5245901346206665
train gradient:  0.1278741397375114
iteration : 13553
train acc:  0.734375
train loss:  0.45905062556266785
train gradient:  0.12340714546724302
iteration : 13554
train acc:  0.7578125
train loss:  0.5005911588668823
train gradient:  0.1487908488648501
iteration : 13555
train acc:  0.71875
train loss:  0.5214717984199524
train gradient:  0.11550356702538865
iteration : 13556
train acc:  0.796875
train loss:  0.4673001766204834
train gradient:  0.10505140849633504
iteration : 13557
train acc:  0.78125
train loss:  0.43184933066368103
train gradient:  0.11592122232365298
iteration : 13558
train acc:  0.7109375
train loss:  0.4949694871902466
train gradient:  0.12978239341798722
iteration : 13559
train acc:  0.6484375
train loss:  0.5639728307723999
train gradient:  0.17042274148648395
iteration : 13560
train acc:  0.734375
train loss:  0.46138250827789307
train gradient:  0.11501224328393825
iteration : 13561
train acc:  0.6796875
train loss:  0.5515617728233337
train gradient:  0.14658247493651894
iteration : 13562
train acc:  0.765625
train loss:  0.43045538663864136
train gradient:  0.10477668502258439
iteration : 13563
train acc:  0.6484375
train loss:  0.5439478158950806
train gradient:  0.15304249342709103
iteration : 13564
train acc:  0.7578125
train loss:  0.4557766318321228
train gradient:  0.10421728647103573
iteration : 13565
train acc:  0.6171875
train loss:  0.6018223166465759
train gradient:  0.16859873540682135
iteration : 13566
train acc:  0.7421875
train loss:  0.47435951232910156
train gradient:  0.11082252732883206
iteration : 13567
train acc:  0.7734375
train loss:  0.49462005496025085
train gradient:  0.10227837688804163
iteration : 13568
train acc:  0.765625
train loss:  0.43340083956718445
train gradient:  0.11557244015453856
iteration : 13569
train acc:  0.8046875
train loss:  0.41959041357040405
train gradient:  0.12722331263402764
iteration : 13570
train acc:  0.8125
train loss:  0.46069782972335815
train gradient:  0.10413462545039874
iteration : 13571
train acc:  0.6875
train loss:  0.5719289779663086
train gradient:  0.14854214987022793
iteration : 13572
train acc:  0.734375
train loss:  0.4600880742073059
train gradient:  0.11058454268450511
iteration : 13573
train acc:  0.765625
train loss:  0.4827960729598999
train gradient:  0.1384902448590963
iteration : 13574
train acc:  0.765625
train loss:  0.43550020456314087
train gradient:  0.13061719512269493
iteration : 13575
train acc:  0.75
train loss:  0.44718536734580994
train gradient:  0.10368094596960017
iteration : 13576
train acc:  0.734375
train loss:  0.48032712936401367
train gradient:  0.11851356322523243
iteration : 13577
train acc:  0.8046875
train loss:  0.42944592237472534
train gradient:  0.08378748625051805
iteration : 13578
train acc:  0.796875
train loss:  0.5348014831542969
train gradient:  0.13850583505445002
iteration : 13579
train acc:  0.6953125
train loss:  0.5056347250938416
train gradient:  0.13248109557395588
iteration : 13580
train acc:  0.71875
train loss:  0.5605989098548889
train gradient:  0.19758971885342963
iteration : 13581
train acc:  0.7265625
train loss:  0.5008149743080139
train gradient:  0.1243412964322337
iteration : 13582
train acc:  0.78125
train loss:  0.4414939880371094
train gradient:  0.1002755654864701
iteration : 13583
train acc:  0.7265625
train loss:  0.5369240045547485
train gradient:  0.16927224661012794
iteration : 13584
train acc:  0.7421875
train loss:  0.5075705051422119
train gradient:  0.12227940240415401
iteration : 13585
train acc:  0.7578125
train loss:  0.5592145919799805
train gradient:  0.1467445062468487
iteration : 13586
train acc:  0.703125
train loss:  0.5034292340278625
train gradient:  0.12279124961967049
iteration : 13587
train acc:  0.6953125
train loss:  0.5190374851226807
train gradient:  0.16573430778076936
iteration : 13588
train acc:  0.75
train loss:  0.45218873023986816
train gradient:  0.0977332413671128
iteration : 13589
train acc:  0.7578125
train loss:  0.5013816356658936
train gradient:  0.11783300143293295
iteration : 13590
train acc:  0.7578125
train loss:  0.49780523777008057
train gradient:  0.13698360086103384
iteration : 13591
train acc:  0.6875
train loss:  0.505607545375824
train gradient:  0.13981654322353426
iteration : 13592
train acc:  0.7421875
train loss:  0.4642038345336914
train gradient:  0.13459394830278107
iteration : 13593
train acc:  0.7734375
train loss:  0.49232351779937744
train gradient:  0.13981267062502822
iteration : 13594
train acc:  0.7734375
train loss:  0.4771934449672699
train gradient:  0.12470794355838252
iteration : 13595
train acc:  0.671875
train loss:  0.5238293409347534
train gradient:  0.1147203079095009
iteration : 13596
train acc:  0.7421875
train loss:  0.45130497217178345
train gradient:  0.1222229190985061
iteration : 13597
train acc:  0.734375
train loss:  0.47259700298309326
train gradient:  0.11485537213243616
iteration : 13598
train acc:  0.71875
train loss:  0.5709959268569946
train gradient:  0.17674421861937045
iteration : 13599
train acc:  0.7734375
train loss:  0.42205268144607544
train gradient:  0.07976308179554416
iteration : 13600
train acc:  0.796875
train loss:  0.4351399838924408
train gradient:  0.08625920025708375
iteration : 13601
train acc:  0.7421875
train loss:  0.4661623239517212
train gradient:  0.09744273922156389
iteration : 13602
train acc:  0.7734375
train loss:  0.46670544147491455
train gradient:  0.11509914197448064
iteration : 13603
train acc:  0.8125
train loss:  0.40964454412460327
train gradient:  0.10716369684529096
iteration : 13604
train acc:  0.71875
train loss:  0.580516517162323
train gradient:  0.1566122369310582
iteration : 13605
train acc:  0.7265625
train loss:  0.5725094079971313
train gradient:  0.1587359304265456
iteration : 13606
train acc:  0.7421875
train loss:  0.5206022262573242
train gradient:  0.16569410713751856
iteration : 13607
train acc:  0.7890625
train loss:  0.4674701988697052
train gradient:  0.09375518478377795
iteration : 13608
train acc:  0.6796875
train loss:  0.5184739828109741
train gradient:  0.16124653027387514
iteration : 13609
train acc:  0.765625
train loss:  0.4842988848686218
train gradient:  0.17356119324982383
iteration : 13610
train acc:  0.703125
train loss:  0.5072344541549683
train gradient:  0.1444647155265319
iteration : 13611
train acc:  0.734375
train loss:  0.47196391224861145
train gradient:  0.10143165843519612
iteration : 13612
train acc:  0.765625
train loss:  0.509600043296814
train gradient:  0.15290460173765286
iteration : 13613
train acc:  0.734375
train loss:  0.5088067650794983
train gradient:  0.13479732784421133
iteration : 13614
train acc:  0.71875
train loss:  0.4803617000579834
train gradient:  0.12003020224558465
iteration : 13615
train acc:  0.8046875
train loss:  0.4038997292518616
train gradient:  0.09232247908498516
iteration : 13616
train acc:  0.65625
train loss:  0.6521174311637878
train gradient:  0.22074393555448935
iteration : 13617
train acc:  0.7421875
train loss:  0.46523335576057434
train gradient:  0.11560447692062331
iteration : 13618
train acc:  0.7578125
train loss:  0.45968353748321533
train gradient:  0.09572796311871128
iteration : 13619
train acc:  0.7890625
train loss:  0.4769161641597748
train gradient:  0.12201539214796885
iteration : 13620
train acc:  0.734375
train loss:  0.45304685831069946
train gradient:  0.11475936369656618
iteration : 13621
train acc:  0.765625
train loss:  0.408846378326416
train gradient:  0.0963403788531263
iteration : 13622
train acc:  0.78125
train loss:  0.44402194023132324
train gradient:  0.11816057134011855
iteration : 13623
train acc:  0.765625
train loss:  0.4449848532676697
train gradient:  0.11436419663121018
iteration : 13624
train acc:  0.7578125
train loss:  0.44435667991638184
train gradient:  0.09824410132151816
iteration : 13625
train acc:  0.7421875
train loss:  0.46983033418655396
train gradient:  0.11461254180085664
iteration : 13626
train acc:  0.703125
train loss:  0.4781051278114319
train gradient:  0.12209260977193309
iteration : 13627
train acc:  0.7265625
train loss:  0.5417792201042175
train gradient:  0.16846550204493305
iteration : 13628
train acc:  0.7109375
train loss:  0.523282527923584
train gradient:  0.16595938789802778
iteration : 13629
train acc:  0.7734375
train loss:  0.450403094291687
train gradient:  0.09441250102689538
iteration : 13630
train acc:  0.671875
train loss:  0.5754427909851074
train gradient:  0.15555960739038927
iteration : 13631
train acc:  0.765625
train loss:  0.4556264281272888
train gradient:  0.10924040540252004
iteration : 13632
train acc:  0.796875
train loss:  0.4752749800682068
train gradient:  0.14148428789033096
iteration : 13633
train acc:  0.734375
train loss:  0.5166899561882019
train gradient:  0.13797568427485274
iteration : 13634
train acc:  0.71875
train loss:  0.49916309118270874
train gradient:  0.13625593943592404
iteration : 13635
train acc:  0.734375
train loss:  0.46602481603622437
train gradient:  0.10744655991694754
iteration : 13636
train acc:  0.7109375
train loss:  0.5325063467025757
train gradient:  0.1412938509381989
iteration : 13637
train acc:  0.7734375
train loss:  0.5071797370910645
train gradient:  0.11703302929841078
iteration : 13638
train acc:  0.765625
train loss:  0.4674866497516632
train gradient:  0.10560824441108829
iteration : 13639
train acc:  0.71875
train loss:  0.570358157157898
train gradient:  0.14394516139685265
iteration : 13640
train acc:  0.734375
train loss:  0.48830097913742065
train gradient:  0.15334210612512886
iteration : 13641
train acc:  0.734375
train loss:  0.5325665473937988
train gradient:  0.13566663659903472
iteration : 13642
train acc:  0.6484375
train loss:  0.5553944706916809
train gradient:  0.14311499888482443
iteration : 13643
train acc:  0.71875
train loss:  0.47856616973876953
train gradient:  0.11269154278736197
iteration : 13644
train acc:  0.6640625
train loss:  0.5645991563796997
train gradient:  0.18317726462191763
iteration : 13645
train acc:  0.7734375
train loss:  0.46169742941856384
train gradient:  0.11363537035551577
iteration : 13646
train acc:  0.7734375
train loss:  0.493826687335968
train gradient:  0.13696515355378863
iteration : 13647
train acc:  0.703125
train loss:  0.5290266871452332
train gradient:  0.13918954012266904
iteration : 13648
train acc:  0.7578125
train loss:  0.46737298369407654
train gradient:  0.1359584486632548
iteration : 13649
train acc:  0.7421875
train loss:  0.4899899363517761
train gradient:  0.1401174050226969
iteration : 13650
train acc:  0.7265625
train loss:  0.505780816078186
train gradient:  0.12391311170100683
iteration : 13651
train acc:  0.7890625
train loss:  0.45617854595184326
train gradient:  0.14029459802443003
iteration : 13652
train acc:  0.7890625
train loss:  0.40798652172088623
train gradient:  0.10800297433150363
iteration : 13653
train acc:  0.75
train loss:  0.47191065549850464
train gradient:  0.11561478324036122
iteration : 13654
train acc:  0.7421875
train loss:  0.47871366143226624
train gradient:  0.12968109163440622
iteration : 13655
train acc:  0.78125
train loss:  0.4468291997909546
train gradient:  0.10777067319435901
iteration : 13656
train acc:  0.78125
train loss:  0.472328782081604
train gradient:  0.1125057717151783
iteration : 13657
train acc:  0.7734375
train loss:  0.46596214175224304
train gradient:  0.13863371227024285
iteration : 13658
train acc:  0.765625
train loss:  0.4588328003883362
train gradient:  0.09014450284778167
iteration : 13659
train acc:  0.703125
train loss:  0.5556519031524658
train gradient:  0.15962463985798497
iteration : 13660
train acc:  0.671875
train loss:  0.5260934829711914
train gradient:  0.15900833131451042
iteration : 13661
train acc:  0.796875
train loss:  0.41300445795059204
train gradient:  0.10041073372026374
iteration : 13662
train acc:  0.71875
train loss:  0.5529410243034363
train gradient:  0.16438610760012562
iteration : 13663
train acc:  0.78125
train loss:  0.4541139006614685
train gradient:  0.11028719893443732
iteration : 13664
train acc:  0.7734375
train loss:  0.41633251309394836
train gradient:  0.1204268967692436
iteration : 13665
train acc:  0.765625
train loss:  0.4540829658508301
train gradient:  0.112044375372443
iteration : 13666
train acc:  0.75
train loss:  0.4710673987865448
train gradient:  0.12712047341677735
iteration : 13667
train acc:  0.6875
train loss:  0.4646642506122589
train gradient:  0.0975713916068785
iteration : 13668
train acc:  0.7109375
train loss:  0.494754821062088
train gradient:  0.11756659231254944
iteration : 13669
train acc:  0.765625
train loss:  0.4334791302680969
train gradient:  0.09651338172298896
iteration : 13670
train acc:  0.71875
train loss:  0.5174502730369568
train gradient:  0.15179988934670907
iteration : 13671
train acc:  0.75
train loss:  0.470154732465744
train gradient:  0.10964934839883647
iteration : 13672
train acc:  0.6875
train loss:  0.5071561932563782
train gradient:  0.116917624828658
iteration : 13673
train acc:  0.71875
train loss:  0.5196008682250977
train gradient:  0.14488421334531923
iteration : 13674
train acc:  0.796875
train loss:  0.45503973960876465
train gradient:  0.0945577600934553
iteration : 13675
train acc:  0.7421875
train loss:  0.4857960045337677
train gradient:  0.11162322759602104
iteration : 13676
train acc:  0.796875
train loss:  0.4010047912597656
train gradient:  0.10162868850612918
iteration : 13677
train acc:  0.7421875
train loss:  0.5266194343566895
train gradient:  0.12042995648882455
iteration : 13678
train acc:  0.71875
train loss:  0.515994131565094
train gradient:  0.1327279692247675
iteration : 13679
train acc:  0.6875
train loss:  0.4962535500526428
train gradient:  0.13711398910477773
iteration : 13680
train acc:  0.71875
train loss:  0.5184574723243713
train gradient:  0.1314357841433825
iteration : 13681
train acc:  0.703125
train loss:  0.5270311236381531
train gradient:  0.17775086187269634
iteration : 13682
train acc:  0.8203125
train loss:  0.4473084807395935
train gradient:  0.09777799198392667
iteration : 13683
train acc:  0.7109375
train loss:  0.5482335090637207
train gradient:  0.19539321485145994
iteration : 13684
train acc:  0.703125
train loss:  0.5307141542434692
train gradient:  0.13470537692616424
iteration : 13685
train acc:  0.7109375
train loss:  0.5304089784622192
train gradient:  0.1874526568659668
iteration : 13686
train acc:  0.7421875
train loss:  0.4870428442955017
train gradient:  0.11828171654209423
iteration : 13687
train acc:  0.8046875
train loss:  0.4340827167034149
train gradient:  0.13637952742262038
iteration : 13688
train acc:  0.75
train loss:  0.5034008622169495
train gradient:  0.11559530637660334
iteration : 13689
train acc:  0.75
train loss:  0.5363202691078186
train gradient:  0.13719084845934992
iteration : 13690
train acc:  0.78125
train loss:  0.4948420822620392
train gradient:  0.1142417236091698
iteration : 13691
train acc:  0.6796875
train loss:  0.5332491397857666
train gradient:  0.13416022379408973
iteration : 13692
train acc:  0.78125
train loss:  0.4448610246181488
train gradient:  0.10486604816891461
iteration : 13693
train acc:  0.71875
train loss:  0.48595544695854187
train gradient:  0.12348152040859231
iteration : 13694
train acc:  0.765625
train loss:  0.5020249485969543
train gradient:  0.12850979853609398
iteration : 13695
train acc:  0.7734375
train loss:  0.4740932583808899
train gradient:  0.12681299649922956
iteration : 13696
train acc:  0.765625
train loss:  0.5086380243301392
train gradient:  0.144113408393366
iteration : 13697
train acc:  0.734375
train loss:  0.49586284160614014
train gradient:  0.12567465119201143
iteration : 13698
train acc:  0.7109375
train loss:  0.538674533367157
train gradient:  0.14827909756425484
iteration : 13699
train acc:  0.8203125
train loss:  0.4157711863517761
train gradient:  0.08422427815457482
iteration : 13700
train acc:  0.7265625
train loss:  0.5011422038078308
train gradient:  0.16245574513603545
iteration : 13701
train acc:  0.765625
train loss:  0.4521493911743164
train gradient:  0.0862275346970082
iteration : 13702
train acc:  0.7890625
train loss:  0.4586750864982605
train gradient:  0.11923531524141125
iteration : 13703
train acc:  0.703125
train loss:  0.5809897184371948
train gradient:  0.15386355999960377
iteration : 13704
train acc:  0.71875
train loss:  0.514473557472229
train gradient:  0.13771109969988143
iteration : 13705
train acc:  0.6796875
train loss:  0.5221145153045654
train gradient:  0.16316888224099085
iteration : 13706
train acc:  0.7578125
train loss:  0.4745027422904968
train gradient:  0.10051104779365055
iteration : 13707
train acc:  0.7890625
train loss:  0.46933531761169434
train gradient:  0.11550760044547428
iteration : 13708
train acc:  0.71875
train loss:  0.5267699956893921
train gradient:  0.18979473482948395
iteration : 13709
train acc:  0.734375
train loss:  0.48873060941696167
train gradient:  0.12189038812697085
iteration : 13710
train acc:  0.7890625
train loss:  0.43192028999328613
train gradient:  0.09536736257401993
iteration : 13711
train acc:  0.7734375
train loss:  0.45585018396377563
train gradient:  0.09265561675330704
iteration : 13712
train acc:  0.671875
train loss:  0.5822868347167969
train gradient:  0.16381080572738915
iteration : 13713
train acc:  0.7109375
train loss:  0.4860382378101349
train gradient:  0.1160187198254579
iteration : 13714
train acc:  0.7734375
train loss:  0.46353182196617126
train gradient:  0.11080413097790508
iteration : 13715
train acc:  0.78125
train loss:  0.4899064600467682
train gradient:  0.1268007741165753
iteration : 13716
train acc:  0.8203125
train loss:  0.4417015314102173
train gradient:  0.08844754994985221
iteration : 13717
train acc:  0.796875
train loss:  0.5104256868362427
train gradient:  0.13245013286640459
iteration : 13718
train acc:  0.75
train loss:  0.45131316781044006
train gradient:  0.10429342108442874
iteration : 13719
train acc:  0.7421875
train loss:  0.49636203050613403
train gradient:  0.1290069310903122
iteration : 13720
train acc:  0.734375
train loss:  0.4995162785053253
train gradient:  0.110713340390788
iteration : 13721
train acc:  0.71875
train loss:  0.5183005928993225
train gradient:  0.1448671303691954
iteration : 13722
train acc:  0.7421875
train loss:  0.5018997192382812
train gradient:  0.09306204557365089
iteration : 13723
train acc:  0.7421875
train loss:  0.5107139348983765
train gradient:  0.09773507722526267
iteration : 13724
train acc:  0.671875
train loss:  0.5027995109558105
train gradient:  0.13335329206769925
iteration : 13725
train acc:  0.734375
train loss:  0.5229052305221558
train gradient:  0.12220529314163012
iteration : 13726
train acc:  0.6796875
train loss:  0.5172183513641357
train gradient:  0.12616907696274426
iteration : 13727
train acc:  0.78125
train loss:  0.49695175886154175
train gradient:  0.11579063288689108
iteration : 13728
train acc:  0.7578125
train loss:  0.45261651277542114
train gradient:  0.11944701033970638
iteration : 13729
train acc:  0.71875
train loss:  0.48579323291778564
train gradient:  0.11911866502859055
iteration : 13730
train acc:  0.75
train loss:  0.4354284405708313
train gradient:  0.09562312403777785
iteration : 13731
train acc:  0.7578125
train loss:  0.5404267311096191
train gradient:  0.12653426806676155
iteration : 13732
train acc:  0.7890625
train loss:  0.43612295389175415
train gradient:  0.10941250384320449
iteration : 13733
train acc:  0.765625
train loss:  0.4618907570838928
train gradient:  0.15371245192072375
iteration : 13734
train acc:  0.734375
train loss:  0.4854165017604828
train gradient:  0.10948494413362712
iteration : 13735
train acc:  0.7109375
train loss:  0.49923670291900635
train gradient:  0.11221087871629552
iteration : 13736
train acc:  0.6953125
train loss:  0.5821206569671631
train gradient:  0.16001746342710627
iteration : 13737
train acc:  0.7578125
train loss:  0.4924481213092804
train gradient:  0.11262611572599172
iteration : 13738
train acc:  0.78125
train loss:  0.4488810896873474
train gradient:  0.09852369325760799
iteration : 13739
train acc:  0.7734375
train loss:  0.4680195152759552
train gradient:  0.10457911858565545
iteration : 13740
train acc:  0.7421875
train loss:  0.4593132436275482
train gradient:  0.11203558286415537
iteration : 13741
train acc:  0.7734375
train loss:  0.4379335045814514
train gradient:  0.0764692631274158
iteration : 13742
train acc:  0.703125
train loss:  0.5057430267333984
train gradient:  0.1191587490507495
iteration : 13743
train acc:  0.7734375
train loss:  0.42036372423171997
train gradient:  0.09857714360960762
iteration : 13744
train acc:  0.7578125
train loss:  0.49710169434547424
train gradient:  0.143411368323733
iteration : 13745
train acc:  0.7578125
train loss:  0.47579681873321533
train gradient:  0.11141918825634381
iteration : 13746
train acc:  0.7265625
train loss:  0.46726956963539124
train gradient:  0.09731803235071572
iteration : 13747
train acc:  0.78125
train loss:  0.4596598148345947
train gradient:  0.1240228443712751
iteration : 13748
train acc:  0.71875
train loss:  0.5149190425872803
train gradient:  0.13358086021251403
iteration : 13749
train acc:  0.7265625
train loss:  0.5009584426879883
train gradient:  0.11958288105362884
iteration : 13750
train acc:  0.6953125
train loss:  0.520020604133606
train gradient:  0.1148184662498025
iteration : 13751
train acc:  0.7265625
train loss:  0.5171122550964355
train gradient:  0.131118547812756
iteration : 13752
train acc:  0.7421875
train loss:  0.46180784702301025
train gradient:  0.12664503981429026
iteration : 13753
train acc:  0.71875
train loss:  0.535844624042511
train gradient:  0.14717980981051593
iteration : 13754
train acc:  0.765625
train loss:  0.47423678636550903
train gradient:  0.1341203706278114
iteration : 13755
train acc:  0.6953125
train loss:  0.553377628326416
train gradient:  0.15162309347374697
iteration : 13756
train acc:  0.78125
train loss:  0.49192553758621216
train gradient:  0.10539209597045993
iteration : 13757
train acc:  0.671875
train loss:  0.5059229135513306
train gradient:  0.1270186701813514
iteration : 13758
train acc:  0.78125
train loss:  0.45442646741867065
train gradient:  0.09517393152000711
iteration : 13759
train acc:  0.78125
train loss:  0.5228619575500488
train gradient:  0.19411231278574936
iteration : 13760
train acc:  0.78125
train loss:  0.4759959280490875
train gradient:  0.11363458949332696
iteration : 13761
train acc:  0.7421875
train loss:  0.509115993976593
train gradient:  0.1435214809877724
iteration : 13762
train acc:  0.7421875
train loss:  0.5345106720924377
train gradient:  0.15293355502219763
iteration : 13763
train acc:  0.7578125
train loss:  0.45753416419029236
train gradient:  0.10403728187896795
iteration : 13764
train acc:  0.78125
train loss:  0.4908710718154907
train gradient:  0.12243010697835409
iteration : 13765
train acc:  0.703125
train loss:  0.48614755272865295
train gradient:  0.11676663198250975
iteration : 13766
train acc:  0.734375
train loss:  0.4886375665664673
train gradient:  0.12198450273230999
iteration : 13767
train acc:  0.78125
train loss:  0.40416738390922546
train gradient:  0.09277680795841738
iteration : 13768
train acc:  0.7109375
train loss:  0.5014621019363403
train gradient:  0.1354959405588712
iteration : 13769
train acc:  0.7109375
train loss:  0.4840804934501648
train gradient:  0.110187476470755
iteration : 13770
train acc:  0.765625
train loss:  0.45566943287849426
train gradient:  0.13649361012076067
iteration : 13771
train acc:  0.8203125
train loss:  0.41125214099884033
train gradient:  0.10060223480174206
iteration : 13772
train acc:  0.7890625
train loss:  0.44209980964660645
train gradient:  0.10458643792745385
iteration : 13773
train acc:  0.6953125
train loss:  0.5587401390075684
train gradient:  0.1581144578874702
iteration : 13774
train acc:  0.78125
train loss:  0.4092257618904114
train gradient:  0.11511042024688496
iteration : 13775
train acc:  0.8046875
train loss:  0.41042864322662354
train gradient:  0.09673200239872262
iteration : 13776
train acc:  0.734375
train loss:  0.4914505183696747
train gradient:  0.1013579979432796
iteration : 13777
train acc:  0.7265625
train loss:  0.46484890580177307
train gradient:  0.138384157709769
iteration : 13778
train acc:  0.71875
train loss:  0.546795666217804
train gradient:  0.15273752638722482
iteration : 13779
train acc:  0.734375
train loss:  0.4689726233482361
train gradient:  0.11700053921774463
iteration : 13780
train acc:  0.6953125
train loss:  0.5631318092346191
train gradient:  0.1705122742882089
iteration : 13781
train acc:  0.7265625
train loss:  0.4844462275505066
train gradient:  0.1048457775085409
iteration : 13782
train acc:  0.75
train loss:  0.48067933320999146
train gradient:  0.1586239758181851
iteration : 13783
train acc:  0.765625
train loss:  0.4313013553619385
train gradient:  0.07937606203498168
iteration : 13784
train acc:  0.75
train loss:  0.5007528066635132
train gradient:  0.13559713396501036
iteration : 13785
train acc:  0.7734375
train loss:  0.4477389454841614
train gradient:  0.12244689876388623
iteration : 13786
train acc:  0.7421875
train loss:  0.5053912401199341
train gradient:  0.11660269223593993
iteration : 13787
train acc:  0.6640625
train loss:  0.5683450698852539
train gradient:  0.15342107338052363
iteration : 13788
train acc:  0.7109375
train loss:  0.5203071236610413
train gradient:  0.12771899297968559
iteration : 13789
train acc:  0.7578125
train loss:  0.46487390995025635
train gradient:  0.10176916940731895
iteration : 13790
train acc:  0.796875
train loss:  0.42643028497695923
train gradient:  0.090393295451584
iteration : 13791
train acc:  0.734375
train loss:  0.464931458234787
train gradient:  0.09594245802263385
iteration : 13792
train acc:  0.78125
train loss:  0.441800057888031
train gradient:  0.10466175583559435
iteration : 13793
train acc:  0.78125
train loss:  0.44157683849334717
train gradient:  0.09372597972317508
iteration : 13794
train acc:  0.7578125
train loss:  0.43139326572418213
train gradient:  0.09232339763481734
iteration : 13795
train acc:  0.6953125
train loss:  0.5320623517036438
train gradient:  0.13825967063172395
iteration : 13796
train acc:  0.71875
train loss:  0.5399105548858643
train gradient:  0.12283068840638213
iteration : 13797
train acc:  0.765625
train loss:  0.47972607612609863
train gradient:  0.11094890757284355
iteration : 13798
train acc:  0.7421875
train loss:  0.47330355644226074
train gradient:  0.10033800025283905
iteration : 13799
train acc:  0.6875
train loss:  0.5140724182128906
train gradient:  0.12328404427404181
iteration : 13800
train acc:  0.7109375
train loss:  0.5270782709121704
train gradient:  0.14192092136450668
iteration : 13801
train acc:  0.765625
train loss:  0.42446452379226685
train gradient:  0.1050539380375166
iteration : 13802
train acc:  0.7734375
train loss:  0.47147712111473083
train gradient:  0.09490732023791536
iteration : 13803
train acc:  0.7890625
train loss:  0.4638442397117615
train gradient:  0.1181507565137552
iteration : 13804
train acc:  0.75
train loss:  0.4588254988193512
train gradient:  0.11847110771435343
iteration : 13805
train acc:  0.7578125
train loss:  0.4886122941970825
train gradient:  0.14377629913408788
iteration : 13806
train acc:  0.765625
train loss:  0.4846760332584381
train gradient:  0.10182910081644407
iteration : 13807
train acc:  0.734375
train loss:  0.5630520582199097
train gradient:  0.15943856227307346
iteration : 13808
train acc:  0.671875
train loss:  0.5526832938194275
train gradient:  0.15025060365398324
iteration : 13809
train acc:  0.7734375
train loss:  0.42871785163879395
train gradient:  0.08911119894860182
iteration : 13810
train acc:  0.8203125
train loss:  0.45846930146217346
train gradient:  0.1319201228190019
iteration : 13811
train acc:  0.7734375
train loss:  0.46679145097732544
train gradient:  0.1165142224518724
iteration : 13812
train acc:  0.703125
train loss:  0.5138311386108398
train gradient:  0.1371849302013457
iteration : 13813
train acc:  0.765625
train loss:  0.45005595684051514
train gradient:  0.09308094732407779
iteration : 13814
train acc:  0.6640625
train loss:  0.5661577582359314
train gradient:  0.15043485211232555
iteration : 13815
train acc:  0.703125
train loss:  0.49031755328178406
train gradient:  0.12825865814931586
iteration : 13816
train acc:  0.7578125
train loss:  0.4505648612976074
train gradient:  0.11138603917306247
iteration : 13817
train acc:  0.78125
train loss:  0.44847697019577026
train gradient:  0.11839853742643067
iteration : 13818
train acc:  0.7890625
train loss:  0.46294671297073364
train gradient:  0.14090744146849465
iteration : 13819
train acc:  0.6953125
train loss:  0.527484118938446
train gradient:  0.11823204039518506
iteration : 13820
train acc:  0.734375
train loss:  0.567613422870636
train gradient:  0.1772977330597576
iteration : 13821
train acc:  0.7265625
train loss:  0.48653116822242737
train gradient:  0.12679257851186673
iteration : 13822
train acc:  0.7734375
train loss:  0.44100573658943176
train gradient:  0.11342721275776825
iteration : 13823
train acc:  0.78125
train loss:  0.4639064371585846
train gradient:  0.11227011253230834
iteration : 13824
train acc:  0.75
train loss:  0.4343215227127075
train gradient:  0.10131719538398853
iteration : 13825
train acc:  0.734375
train loss:  0.48613137006759644
train gradient:  0.12732341956504098
iteration : 13826
train acc:  0.78125
train loss:  0.44176042079925537
train gradient:  0.11082029280284295
iteration : 13827
train acc:  0.8046875
train loss:  0.38010507822036743
train gradient:  0.07856420196581694
iteration : 13828
train acc:  0.7421875
train loss:  0.46794018149375916
train gradient:  0.1220995648814265
iteration : 13829
train acc:  0.703125
train loss:  0.5082708597183228
train gradient:  0.1212410447741036
iteration : 13830
train acc:  0.6484375
train loss:  0.5645676255226135
train gradient:  0.1581808319641818
iteration : 13831
train acc:  0.6953125
train loss:  0.5665074586868286
train gradient:  0.14804901229814962
iteration : 13832
train acc:  0.7421875
train loss:  0.4657891094684601
train gradient:  0.10357865378517041
iteration : 13833
train acc:  0.8046875
train loss:  0.4624614417552948
train gradient:  0.10515627748260481
iteration : 13834
train acc:  0.734375
train loss:  0.5294839143753052
train gradient:  0.12410767556037416
iteration : 13835
train acc:  0.7109375
train loss:  0.493583083152771
train gradient:  0.13039746657071213
iteration : 13836
train acc:  0.7421875
train loss:  0.5124763250350952
train gradient:  0.1301810226067115
iteration : 13837
train acc:  0.671875
train loss:  0.5573729276657104
train gradient:  0.13281746737229405
iteration : 13838
train acc:  0.75
train loss:  0.4904305934906006
train gradient:  0.10512395430601612
iteration : 13839
train acc:  0.7890625
train loss:  0.4694003462791443
train gradient:  0.14351786855348103
iteration : 13840
train acc:  0.78125
train loss:  0.4444427788257599
train gradient:  0.09975761485018934
iteration : 13841
train acc:  0.78125
train loss:  0.4466845393180847
train gradient:  0.08872488019140849
iteration : 13842
train acc:  0.7421875
train loss:  0.5157909989356995
train gradient:  0.14478675027417282
iteration : 13843
train acc:  0.78125
train loss:  0.41786226630210876
train gradient:  0.0980505922659098
iteration : 13844
train acc:  0.7578125
train loss:  0.47477421164512634
train gradient:  0.10021964103850162
iteration : 13845
train acc:  0.6328125
train loss:  0.5771787166595459
train gradient:  0.1577880530845505
iteration : 13846
train acc:  0.71875
train loss:  0.5007120370864868
train gradient:  0.1295357305003922
iteration : 13847
train acc:  0.6484375
train loss:  0.5945050120353699
train gradient:  0.1861767118857964
iteration : 13848
train acc:  0.765625
train loss:  0.49024131894111633
train gradient:  0.11303915485340696
iteration : 13849
train acc:  0.7890625
train loss:  0.4674205183982849
train gradient:  0.12857589496541916
iteration : 13850
train acc:  0.8125
train loss:  0.4525725543498993
train gradient:  0.11186857825577984
iteration : 13851
train acc:  0.7578125
train loss:  0.4745609760284424
train gradient:  0.128784450504708
iteration : 13852
train acc:  0.78125
train loss:  0.4509275257587433
train gradient:  0.11933699639306339
iteration : 13853
train acc:  0.78125
train loss:  0.49622711539268494
train gradient:  0.1278905939730917
iteration : 13854
train acc:  0.7734375
train loss:  0.47714516520500183
train gradient:  0.10394059803905957
iteration : 13855
train acc:  0.6640625
train loss:  0.5735737085342407
train gradient:  0.10659783158176478
iteration : 13856
train acc:  0.6953125
train loss:  0.5452079772949219
train gradient:  0.16063376757852763
iteration : 13857
train acc:  0.6953125
train loss:  0.5043120980262756
train gradient:  0.13117176667990144
iteration : 13858
train acc:  0.75
train loss:  0.4860975742340088
train gradient:  0.13296263899128688
iteration : 13859
train acc:  0.734375
train loss:  0.5319540500640869
train gradient:  0.16447896110810623
iteration : 13860
train acc:  0.765625
train loss:  0.437826931476593
train gradient:  0.10568666127798547
iteration : 13861
train acc:  0.7734375
train loss:  0.4670898914337158
train gradient:  0.10917795211385363
iteration : 13862
train acc:  0.6796875
train loss:  0.5522027015686035
train gradient:  0.16925870405027904
iteration : 13863
train acc:  0.765625
train loss:  0.4475756883621216
train gradient:  0.10545157898288543
iteration : 13864
train acc:  0.7109375
train loss:  0.5538764595985413
train gradient:  0.12310417231732478
iteration : 13865
train acc:  0.75
train loss:  0.5042564868927002
train gradient:  0.11815175470192113
iteration : 13866
train acc:  0.75
train loss:  0.44847893714904785
train gradient:  0.10425766222478411
iteration : 13867
train acc:  0.703125
train loss:  0.5532646179199219
train gradient:  0.14839765918382464
iteration : 13868
train acc:  0.703125
train loss:  0.5361243486404419
train gradient:  0.1348883174905917
iteration : 13869
train acc:  0.7890625
train loss:  0.4076579213142395
train gradient:  0.08481063004051867
iteration : 13870
train acc:  0.7578125
train loss:  0.48283621668815613
train gradient:  0.1374196102010149
iteration : 13871
train acc:  0.7265625
train loss:  0.5558195114135742
train gradient:  0.16231264572522872
iteration : 13872
train acc:  0.7578125
train loss:  0.42933541536331177
train gradient:  0.09138551535165843
iteration : 13873
train acc:  0.734375
train loss:  0.4735161066055298
train gradient:  0.10878153610462386
iteration : 13874
train acc:  0.75
train loss:  0.44929808378219604
train gradient:  0.10232904308261809
iteration : 13875
train acc:  0.78125
train loss:  0.4370460510253906
train gradient:  0.12940873844992473
iteration : 13876
train acc:  0.6796875
train loss:  0.526626467704773
train gradient:  0.13769307757654697
iteration : 13877
train acc:  0.8046875
train loss:  0.42660221457481384
train gradient:  0.09301956904268405
iteration : 13878
train acc:  0.796875
train loss:  0.4544640779495239
train gradient:  0.1211426209744732
iteration : 13879
train acc:  0.796875
train loss:  0.42175722122192383
train gradient:  0.09303779934057388
iteration : 13880
train acc:  0.7265625
train loss:  0.5419961214065552
train gradient:  0.15663857550481655
iteration : 13881
train acc:  0.8046875
train loss:  0.49373477697372437
train gradient:  0.13044013012471595
iteration : 13882
train acc:  0.6796875
train loss:  0.5738698244094849
train gradient:  0.14325792868459894
iteration : 13883
train acc:  0.75
train loss:  0.5018369555473328
train gradient:  0.12548736626130444
iteration : 13884
train acc:  0.703125
train loss:  0.5118942260742188
train gradient:  0.12763780410238712
iteration : 13885
train acc:  0.75
train loss:  0.49054205417633057
train gradient:  0.12444695346500953
iteration : 13886
train acc:  0.765625
train loss:  0.5159292221069336
train gradient:  0.16386070288127985
iteration : 13887
train acc:  0.7421875
train loss:  0.4420270621776581
train gradient:  0.1083172435488088
iteration : 13888
train acc:  0.7734375
train loss:  0.48141998052597046
train gradient:  0.13051716670969873
iteration : 13889
train acc:  0.7890625
train loss:  0.43254733085632324
train gradient:  0.11045918602591105
iteration : 13890
train acc:  0.7578125
train loss:  0.5131409168243408
train gradient:  0.14711278127401622
iteration : 13891
train acc:  0.765625
train loss:  0.5014367699623108
train gradient:  0.1679203249058297
iteration : 13892
train acc:  0.703125
train loss:  0.5194382667541504
train gradient:  0.13222156105509988
iteration : 13893
train acc:  0.765625
train loss:  0.46125075221061707
train gradient:  0.10881631263759049
iteration : 13894
train acc:  0.765625
train loss:  0.449791818857193
train gradient:  0.12019735839357137
iteration : 13895
train acc:  0.7421875
train loss:  0.4476889371871948
train gradient:  0.10765812303200721
iteration : 13896
train acc:  0.6953125
train loss:  0.5201046466827393
train gradient:  0.12263712186721462
iteration : 13897
train acc:  0.7578125
train loss:  0.4679003655910492
train gradient:  0.10506374861438736
iteration : 13898
train acc:  0.7578125
train loss:  0.46540242433547974
train gradient:  0.10975783164003466
iteration : 13899
train acc:  0.734375
train loss:  0.4779374599456787
train gradient:  0.09242149803445047
iteration : 13900
train acc:  0.7890625
train loss:  0.4343996047973633
train gradient:  0.11791200592920115
iteration : 13901
train acc:  0.7890625
train loss:  0.4472496211528778
train gradient:  0.10876350464132914
iteration : 13902
train acc:  0.75
train loss:  0.49123454093933105
train gradient:  0.10600604208449768
iteration : 13903
train acc:  0.7109375
train loss:  0.5575655698776245
train gradient:  0.13844009302253535
iteration : 13904
train acc:  0.71875
train loss:  0.5243182182312012
train gradient:  0.13061495901837333
iteration : 13905
train acc:  0.7734375
train loss:  0.42094242572784424
train gradient:  0.0984838330702471
iteration : 13906
train acc:  0.7421875
train loss:  0.48856139183044434
train gradient:  0.1176047905283575
iteration : 13907
train acc:  0.78125
train loss:  0.4778280258178711
train gradient:  0.11414883200350812
iteration : 13908
train acc:  0.765625
train loss:  0.5104670524597168
train gradient:  0.12303003008194759
iteration : 13909
train acc:  0.71875
train loss:  0.4736623167991638
train gradient:  0.12564404077990837
iteration : 13910
train acc:  0.7109375
train loss:  0.4910566806793213
train gradient:  0.1070155647119716
iteration : 13911
train acc:  0.703125
train loss:  0.5731281638145447
train gradient:  0.1540201151651293
iteration : 13912
train acc:  0.765625
train loss:  0.46665558218955994
train gradient:  0.1074902181584554
iteration : 13913
train acc:  0.78125
train loss:  0.4303174614906311
train gradient:  0.11964071135773928
iteration : 13914
train acc:  0.71875
train loss:  0.48380839824676514
train gradient:  0.12145878105369247
iteration : 13915
train acc:  0.7421875
train loss:  0.4448067545890808
train gradient:  0.10748815504796679
iteration : 13916
train acc:  0.703125
train loss:  0.5718116760253906
train gradient:  0.16557297512724908
iteration : 13917
train acc:  0.734375
train loss:  0.5056343078613281
train gradient:  0.10274519957765818
iteration : 13918
train acc:  0.7421875
train loss:  0.4923642873764038
train gradient:  0.10807193425677375
iteration : 13919
train acc:  0.7265625
train loss:  0.5083810091018677
train gradient:  0.10370905802073198
iteration : 13920
train acc:  0.765625
train loss:  0.46045300364494324
train gradient:  0.12983112044631998
iteration : 13921
train acc:  0.7109375
train loss:  0.48689335584640503
train gradient:  0.12611996515116225
iteration : 13922
train acc:  0.75
train loss:  0.4769858717918396
train gradient:  0.12100030334936121
iteration : 13923
train acc:  0.7734375
train loss:  0.4917954206466675
train gradient:  0.13684774588010734
iteration : 13924
train acc:  0.7421875
train loss:  0.5016927123069763
train gradient:  0.15183157169979877
iteration : 13925
train acc:  0.6875
train loss:  0.5426993370056152
train gradient:  0.12083389418874865
iteration : 13926
train acc:  0.7890625
train loss:  0.4808861017227173
train gradient:  0.12850912425561953
iteration : 13927
train acc:  0.828125
train loss:  0.4373056888580322
train gradient:  0.1331948832011951
iteration : 13928
train acc:  0.796875
train loss:  0.43080154061317444
train gradient:  0.10867909432316217
iteration : 13929
train acc:  0.78125
train loss:  0.41989660263061523
train gradient:  0.09850252852192841
iteration : 13930
train acc:  0.71875
train loss:  0.4879259467124939
train gradient:  0.10343006000051264
iteration : 13931
train acc:  0.828125
train loss:  0.41356074810028076
train gradient:  0.09835987414185601
iteration : 13932
train acc:  0.75
train loss:  0.5025220513343811
train gradient:  0.10768119246450722
iteration : 13933
train acc:  0.7734375
train loss:  0.4760186970233917
train gradient:  0.10576140147417035
iteration : 13934
train acc:  0.7890625
train loss:  0.47189927101135254
train gradient:  0.12585507218079972
iteration : 13935
train acc:  0.7109375
train loss:  0.5125970840454102
train gradient:  0.12545614703903707
iteration : 13936
train acc:  0.6953125
train loss:  0.563744068145752
train gradient:  0.17267745242150218
iteration : 13937
train acc:  0.7265625
train loss:  0.516666054725647
train gradient:  0.12977111984398262
iteration : 13938
train acc:  0.75
train loss:  0.4843983054161072
train gradient:  0.15275643901400277
iteration : 13939
train acc:  0.703125
train loss:  0.5379936099052429
train gradient:  0.115258648961377
iteration : 13940
train acc:  0.765625
train loss:  0.4423046112060547
train gradient:  0.09585848607742223
iteration : 13941
train acc:  0.7578125
train loss:  0.48975902795791626
train gradient:  0.1363985508318442
iteration : 13942
train acc:  0.7421875
train loss:  0.5083276033401489
train gradient:  0.12368848032076425
iteration : 13943
train acc:  0.734375
train loss:  0.5071185827255249
train gradient:  0.12487590018867269
iteration : 13944
train acc:  0.7265625
train loss:  0.47740212082862854
train gradient:  0.10398873946219606
iteration : 13945
train acc:  0.8359375
train loss:  0.4177413284778595
train gradient:  0.09514042248842257
iteration : 13946
train acc:  0.71875
train loss:  0.5199524164199829
train gradient:  0.13055817353542365
iteration : 13947
train acc:  0.7578125
train loss:  0.4813026487827301
train gradient:  0.10390755485595787
iteration : 13948
train acc:  0.671875
train loss:  0.5540673136711121
train gradient:  0.1440847560876954
iteration : 13949
train acc:  0.6953125
train loss:  0.5057138204574585
train gradient:  0.15020950441730369
iteration : 13950
train acc:  0.765625
train loss:  0.4226001501083374
train gradient:  0.092377161688116
iteration : 13951
train acc:  0.75
train loss:  0.4781301021575928
train gradient:  0.12758013690464903
iteration : 13952
train acc:  0.7734375
train loss:  0.5297226905822754
train gradient:  0.1468131650701422
iteration : 13953
train acc:  0.75
train loss:  0.49534478783607483
train gradient:  0.13026183753770162
iteration : 13954
train acc:  0.75
train loss:  0.4944706857204437
train gradient:  0.14004621724482008
iteration : 13955
train acc:  0.8125
train loss:  0.4195384383201599
train gradient:  0.09017515979253511
iteration : 13956
train acc:  0.75
train loss:  0.4433991014957428
train gradient:  0.10480804398768621
iteration : 13957
train acc:  0.8125
train loss:  0.4606807827949524
train gradient:  0.11985978884393468
iteration : 13958
train acc:  0.7109375
train loss:  0.5348976850509644
train gradient:  0.15476143060890102
iteration : 13959
train acc:  0.78125
train loss:  0.44793450832366943
train gradient:  0.12110135123424451
iteration : 13960
train acc:  0.7578125
train loss:  0.46679484844207764
train gradient:  0.12803959161525774
iteration : 13961
train acc:  0.703125
train loss:  0.5413097739219666
train gradient:  0.15266562762378882
iteration : 13962
train acc:  0.765625
train loss:  0.47213131189346313
train gradient:  0.13832317593177695
iteration : 13963
train acc:  0.71875
train loss:  0.5147527456283569
train gradient:  0.13306754351000347
iteration : 13964
train acc:  0.796875
train loss:  0.42873913049697876
train gradient:  0.09188751794133869
iteration : 13965
train acc:  0.75
train loss:  0.511381208896637
train gradient:  0.13094232312175047
iteration : 13966
train acc:  0.7890625
train loss:  0.4399898052215576
train gradient:  0.09608988495976853
iteration : 13967
train acc:  0.7109375
train loss:  0.5225542783737183
train gradient:  0.1302971743540065
iteration : 13968
train acc:  0.71875
train loss:  0.4719690978527069
train gradient:  0.12293802215913316
iteration : 13969
train acc:  0.71875
train loss:  0.5082629919052124
train gradient:  0.12483368039008796
iteration : 13970
train acc:  0.71875
train loss:  0.5089078545570374
train gradient:  0.12437573050279944
iteration : 13971
train acc:  0.7265625
train loss:  0.5171751976013184
train gradient:  0.16029505737516492
iteration : 13972
train acc:  0.734375
train loss:  0.46465590596199036
train gradient:  0.10845531673127748
iteration : 13973
train acc:  0.7890625
train loss:  0.413547545671463
train gradient:  0.10325180924924947
iteration : 13974
train acc:  0.796875
train loss:  0.46366217732429504
train gradient:  0.11646946839697239
iteration : 13975
train acc:  0.703125
train loss:  0.5774931311607361
train gradient:  0.14928540864893813
iteration : 13976
train acc:  0.7421875
train loss:  0.5039970278739929
train gradient:  0.12070268803046305
iteration : 13977
train acc:  0.7890625
train loss:  0.443825364112854
train gradient:  0.1087803928233602
iteration : 13978
train acc:  0.8046875
train loss:  0.41976842284202576
train gradient:  0.10158521932708922
iteration : 13979
train acc:  0.7265625
train loss:  0.5050920844078064
train gradient:  0.125671021398991
iteration : 13980
train acc:  0.7734375
train loss:  0.4503869414329529
train gradient:  0.1063957989069011
iteration : 13981
train acc:  0.703125
train loss:  0.4748629033565521
train gradient:  0.11196365745775946
iteration : 13982
train acc:  0.734375
train loss:  0.5016403198242188
train gradient:  0.11666121280322228
iteration : 13983
train acc:  0.6953125
train loss:  0.5446125268936157
train gradient:  0.1382134494619463
iteration : 13984
train acc:  0.765625
train loss:  0.5159775018692017
train gradient:  0.13659913900708892
iteration : 13985
train acc:  0.75
train loss:  0.48629122972488403
train gradient:  0.11109334150428572
iteration : 13986
train acc:  0.65625
train loss:  0.5947388410568237
train gradient:  0.1795282338229161
iteration : 13987
train acc:  0.75
train loss:  0.47746115922927856
train gradient:  0.11570181078522875
iteration : 13988
train acc:  0.765625
train loss:  0.5119428038597107
train gradient:  0.14941371865798564
iteration : 13989
train acc:  0.765625
train loss:  0.47371864318847656
train gradient:  0.11672887015931649
iteration : 13990
train acc:  0.796875
train loss:  0.4049506187438965
train gradient:  0.07860564387793607
iteration : 13991
train acc:  0.671875
train loss:  0.5394159555435181
train gradient:  0.1170952748683722
iteration : 13992
train acc:  0.7421875
train loss:  0.4504295587539673
train gradient:  0.10566454874818947
iteration : 13993
train acc:  0.75
train loss:  0.47016435861587524
train gradient:  0.13315352531077396
iteration : 13994
train acc:  0.7421875
train loss:  0.5323704481124878
train gradient:  0.12794223298319482
iteration : 13995
train acc:  0.75
train loss:  0.518397867679596
train gradient:  0.12291415650694436
iteration : 13996
train acc:  0.734375
train loss:  0.4918212890625
train gradient:  0.15362753431179715
iteration : 13997
train acc:  0.78125
train loss:  0.46433013677597046
train gradient:  0.11260583986791567
iteration : 13998
train acc:  0.8125
train loss:  0.4202337861061096
train gradient:  0.0831563916830125
iteration : 13999
train acc:  0.7109375
train loss:  0.5414035320281982
train gradient:  0.1329410739524918
iteration : 14000
train acc:  0.8046875
train loss:  0.41474080085754395
train gradient:  0.08834003154264417
iteration : 14001
train acc:  0.7578125
train loss:  0.5165252685546875
train gradient:  0.16992547224704288
iteration : 14002
train acc:  0.8046875
train loss:  0.44052112102508545
train gradient:  0.1020397636649095
iteration : 14003
train acc:  0.796875
train loss:  0.42083418369293213
train gradient:  0.1262462097956493
iteration : 14004
train acc:  0.7734375
train loss:  0.493663489818573
train gradient:  0.131176658313497
iteration : 14005
train acc:  0.7578125
train loss:  0.4470212459564209
train gradient:  0.08244820116665953
iteration : 14006
train acc:  0.7421875
train loss:  0.4873713254928589
train gradient:  0.12756583357516227
iteration : 14007
train acc:  0.6953125
train loss:  0.5469600558280945
train gradient:  0.15153484719812124
iteration : 14008
train acc:  0.7578125
train loss:  0.4887322783470154
train gradient:  0.13544979470399207
iteration : 14009
train acc:  0.796875
train loss:  0.45208194851875305
train gradient:  0.14237658569752784
iteration : 14010
train acc:  0.7421875
train loss:  0.46976765990257263
train gradient:  0.13634901084781226
iteration : 14011
train acc:  0.7421875
train loss:  0.49849802255630493
train gradient:  0.11342330939542832
iteration : 14012
train acc:  0.7578125
train loss:  0.4871920645236969
train gradient:  0.12034007575401594
iteration : 14013
train acc:  0.6875
train loss:  0.6136613488197327
train gradient:  0.152487529331041
iteration : 14014
train acc:  0.6875
train loss:  0.5175864100456238
train gradient:  0.13479605058446156
iteration : 14015
train acc:  0.796875
train loss:  0.46291953325271606
train gradient:  0.1317564177316052
iteration : 14016
train acc:  0.734375
train loss:  0.5041693449020386
train gradient:  0.1163879814661307
iteration : 14017
train acc:  0.7421875
train loss:  0.5003542900085449
train gradient:  0.12737890411375508
iteration : 14018
train acc:  0.75
train loss:  0.4713061451911926
train gradient:  0.13147897267395606
iteration : 14019
train acc:  0.796875
train loss:  0.473929226398468
train gradient:  0.10894001104001745
iteration : 14020
train acc:  0.6796875
train loss:  0.5183706283569336
train gradient:  0.14782353718189722
iteration : 14021
train acc:  0.734375
train loss:  0.5216922760009766
train gradient:  0.10723721155293832
iteration : 14022
train acc:  0.765625
train loss:  0.4586101174354553
train gradient:  0.10100830209371545
iteration : 14023
train acc:  0.7421875
train loss:  0.4837751090526581
train gradient:  0.14354050177070385
iteration : 14024
train acc:  0.75
train loss:  0.48859184980392456
train gradient:  0.14381621825038152
iteration : 14025
train acc:  0.75
train loss:  0.4341362714767456
train gradient:  0.09401779114234017
iteration : 14026
train acc:  0.7890625
train loss:  0.4414425492286682
train gradient:  0.11298375862763152
iteration : 14027
train acc:  0.75
train loss:  0.4987889528274536
train gradient:  0.1257351397670225
iteration : 14028
train acc:  0.734375
train loss:  0.4754428565502167
train gradient:  0.1038912954069549
iteration : 14029
train acc:  0.8125
train loss:  0.3961617052555084
train gradient:  0.08109360246249166
iteration : 14030
train acc:  0.7265625
train loss:  0.48550620675086975
train gradient:  0.1005192711310807
iteration : 14031
train acc:  0.6875
train loss:  0.554753303527832
train gradient:  0.16532485897273885
iteration : 14032
train acc:  0.7734375
train loss:  0.41618940234184265
train gradient:  0.09233850557755319
iteration : 14033
train acc:  0.78125
train loss:  0.4464368224143982
train gradient:  0.11355050471595969
iteration : 14034
train acc:  0.734375
train loss:  0.5183094143867493
train gradient:  0.12991623000813868
iteration : 14035
train acc:  0.7734375
train loss:  0.46785861253738403
train gradient:  0.14189796353497913
iteration : 14036
train acc:  0.7109375
train loss:  0.5031530857086182
train gradient:  0.12748963208663136
iteration : 14037
train acc:  0.703125
train loss:  0.5080273151397705
train gradient:  0.13714005178556882
iteration : 14038
train acc:  0.765625
train loss:  0.4374046325683594
train gradient:  0.0918341623945589
iteration : 14039
train acc:  0.75
train loss:  0.48831480741500854
train gradient:  0.13014531803779944
iteration : 14040
train acc:  0.7109375
train loss:  0.5190098285675049
train gradient:  0.1358549158434138
iteration : 14041
train acc:  0.6953125
train loss:  0.5495984554290771
train gradient:  0.13347009799337262
iteration : 14042
train acc:  0.8046875
train loss:  0.3925367593765259
train gradient:  0.09944568288507821
iteration : 14043
train acc:  0.6875
train loss:  0.528248131275177
train gradient:  0.14126466271275814
iteration : 14044
train acc:  0.7890625
train loss:  0.4260132610797882
train gradient:  0.10133569480891018
iteration : 14045
train acc:  0.7421875
train loss:  0.4403158128261566
train gradient:  0.08399190145634336
iteration : 14046
train acc:  0.8203125
train loss:  0.4436352252960205
train gradient:  0.11308949109552374
iteration : 14047
train acc:  0.71875
train loss:  0.5092155933380127
train gradient:  0.12408199513490344
iteration : 14048
train acc:  0.71875
train loss:  0.5292898416519165
train gradient:  0.12140008760166801
iteration : 14049
train acc:  0.7421875
train loss:  0.4787082076072693
train gradient:  0.15579395249995626
iteration : 14050
train acc:  0.734375
train loss:  0.519079327583313
train gradient:  0.14847661824249783
iteration : 14051
train acc:  0.7109375
train loss:  0.5015926957130432
train gradient:  0.1538976540083135
iteration : 14052
train acc:  0.6953125
train loss:  0.5043234825134277
train gradient:  0.12525928695517444
iteration : 14053
train acc:  0.7109375
train loss:  0.5208088159561157
train gradient:  0.12768541203298295
iteration : 14054
train acc:  0.7578125
train loss:  0.48001348972320557
train gradient:  0.11594420827786027
iteration : 14055
train acc:  0.7890625
train loss:  0.47376981377601624
train gradient:  0.12708527861075253
iteration : 14056
train acc:  0.796875
train loss:  0.497736394405365
train gradient:  0.11649388591519778
iteration : 14057
train acc:  0.7421875
train loss:  0.5047128200531006
train gradient:  0.11518926974956922
iteration : 14058
train acc:  0.796875
train loss:  0.4304009675979614
train gradient:  0.10133939062476312
iteration : 14059
train acc:  0.7109375
train loss:  0.5192956328392029
train gradient:  0.12269930807095075
iteration : 14060
train acc:  0.7421875
train loss:  0.48321273922920227
train gradient:  0.12264098252580188
iteration : 14061
train acc:  0.7421875
train loss:  0.4983767867088318
train gradient:  0.12613952813179855
iteration : 14062
train acc:  0.6875
train loss:  0.4985000193119049
train gradient:  0.12300177198598278
iteration : 14063
train acc:  0.7734375
train loss:  0.49521148204803467
train gradient:  0.15802068513736978
iteration : 14064
train acc:  0.7578125
train loss:  0.47191083431243896
train gradient:  0.1370006066128126
iteration : 14065
train acc:  0.75
train loss:  0.5127989649772644
train gradient:  0.14245966485704498
iteration : 14066
train acc:  0.8125
train loss:  0.4235563278198242
train gradient:  0.10400338733519693
iteration : 14067
train acc:  0.765625
train loss:  0.4693342447280884
train gradient:  0.1005643601691782
iteration : 14068
train acc:  0.7890625
train loss:  0.489514023065567
train gradient:  0.11704973507614366
iteration : 14069
train acc:  0.765625
train loss:  0.4620838165283203
train gradient:  0.1497114611859087
iteration : 14070
train acc:  0.7421875
train loss:  0.4726133942604065
train gradient:  0.10617789187046338
iteration : 14071
train acc:  0.75
train loss:  0.49546095728874207
train gradient:  0.12718749286916833
iteration : 14072
train acc:  0.734375
train loss:  0.5184250473976135
train gradient:  0.138427548493867
iteration : 14073
train acc:  0.78125
train loss:  0.4230348467826843
train gradient:  0.08740746498032045
iteration : 14074
train acc:  0.7265625
train loss:  0.503236711025238
train gradient:  0.1489226717685951
iteration : 14075
train acc:  0.8203125
train loss:  0.42790675163269043
train gradient:  0.08711196130313568
iteration : 14076
train acc:  0.7890625
train loss:  0.45625197887420654
train gradient:  0.1189242390423223
iteration : 14077
train acc:  0.7890625
train loss:  0.41100436449050903
train gradient:  0.08616204224184136
iteration : 14078
train acc:  0.7890625
train loss:  0.41826122999191284
train gradient:  0.0940324325403583
iteration : 14079
train acc:  0.78125
train loss:  0.43624722957611084
train gradient:  0.09567380723988177
iteration : 14080
train acc:  0.7578125
train loss:  0.4396169185638428
train gradient:  0.0921617829281103
iteration : 14081
train acc:  0.78125
train loss:  0.4982024133205414
train gradient:  0.1837020794658959
iteration : 14082
train acc:  0.7578125
train loss:  0.4490330219268799
train gradient:  0.09234647295122074
iteration : 14083
train acc:  0.7265625
train loss:  0.465077668428421
train gradient:  0.11240119005686916
iteration : 14084
train acc:  0.703125
train loss:  0.5165612697601318
train gradient:  0.12333420997523939
iteration : 14085
train acc:  0.796875
train loss:  0.46375975012779236
train gradient:  0.09180393601803416
iteration : 14086
train acc:  0.765625
train loss:  0.498249351978302
train gradient:  0.12306479814261673
iteration : 14087
train acc:  0.6484375
train loss:  0.572195827960968
train gradient:  0.1659810557671734
iteration : 14088
train acc:  0.7109375
train loss:  0.5860245227813721
train gradient:  0.184689403678527
iteration : 14089
train acc:  0.7578125
train loss:  0.4301299452781677
train gradient:  0.08963956099741309
iteration : 14090
train acc:  0.734375
train loss:  0.46404388546943665
train gradient:  0.10700787251231085
iteration : 14091
train acc:  0.6953125
train loss:  0.6051511764526367
train gradient:  0.16547345983153966
iteration : 14092
train acc:  0.7890625
train loss:  0.44443920254707336
train gradient:  0.08949920396941011
iteration : 14093
train acc:  0.78125
train loss:  0.471648246049881
train gradient:  0.10739658583199445
iteration : 14094
train acc:  0.6953125
train loss:  0.5409724712371826
train gradient:  0.15043372111602954
iteration : 14095
train acc:  0.71875
train loss:  0.481851190328598
train gradient:  0.12049815017758847
iteration : 14096
train acc:  0.7734375
train loss:  0.4241654872894287
train gradient:  0.09444784225366752
iteration : 14097
train acc:  0.796875
train loss:  0.45542505383491516
train gradient:  0.10051562446263769
iteration : 14098
train acc:  0.6953125
train loss:  0.5339282751083374
train gradient:  0.14531731983682455
iteration : 14099
train acc:  0.7265625
train loss:  0.5128021240234375
train gradient:  0.13150003808972316
iteration : 14100
train acc:  0.75
train loss:  0.49214088916778564
train gradient:  0.15134373395556416
iteration : 14101
train acc:  0.7578125
train loss:  0.4533461034297943
train gradient:  0.10651550812957017
iteration : 14102
train acc:  0.765625
train loss:  0.4512593746185303
train gradient:  0.10190657974186694
iteration : 14103
train acc:  0.71875
train loss:  0.5498619079589844
train gradient:  0.17983722477799857
iteration : 14104
train acc:  0.6796875
train loss:  0.5823940634727478
train gradient:  0.19585725882064897
iteration : 14105
train acc:  0.7734375
train loss:  0.46583837270736694
train gradient:  0.10197333521745891
iteration : 14106
train acc:  0.796875
train loss:  0.4387512505054474
train gradient:  0.0971327294888575
iteration : 14107
train acc:  0.734375
train loss:  0.4465402364730835
train gradient:  0.1335446390031691
iteration : 14108
train acc:  0.75
train loss:  0.5219447612762451
train gradient:  0.1446360682573956
iteration : 14109
train acc:  0.71875
train loss:  0.500006914138794
train gradient:  0.1344562168646042
iteration : 14110
train acc:  0.7265625
train loss:  0.5243319272994995
train gradient:  0.14924648640894905
iteration : 14111
train acc:  0.71875
train loss:  0.5001403093338013
train gradient:  0.1355604693186273
iteration : 14112
train acc:  0.78125
train loss:  0.44161534309387207
train gradient:  0.1170322744536151
iteration : 14113
train acc:  0.7578125
train loss:  0.43945711851119995
train gradient:  0.12000333416374881
iteration : 14114
train acc:  0.7109375
train loss:  0.5696706175804138
train gradient:  0.15955409330922515
iteration : 14115
train acc:  0.7421875
train loss:  0.499306321144104
train gradient:  0.1113668009499979
iteration : 14116
train acc:  0.6875
train loss:  0.5272096991539001
train gradient:  0.13872009547518432
iteration : 14117
train acc:  0.640625
train loss:  0.6489990949630737
train gradient:  0.23565682838970228
iteration : 14118
train acc:  0.8125
train loss:  0.4330008029937744
train gradient:  0.08585764434741724
iteration : 14119
train acc:  0.75
train loss:  0.48043057322502136
train gradient:  0.1467272569702503
iteration : 14120
train acc:  0.6953125
train loss:  0.5232037305831909
train gradient:  0.12339566942572558
iteration : 14121
train acc:  0.765625
train loss:  0.4549922049045563
train gradient:  0.10442511259361852
iteration : 14122
train acc:  0.75
train loss:  0.47335168719291687
train gradient:  0.10879616789652505
iteration : 14123
train acc:  0.71875
train loss:  0.48122841119766235
train gradient:  0.10599036100999293
iteration : 14124
train acc:  0.6953125
train loss:  0.5505141019821167
train gradient:  0.1509287264673393
iteration : 14125
train acc:  0.734375
train loss:  0.48973023891448975
train gradient:  0.11714987883271032
iteration : 14126
train acc:  0.6953125
train loss:  0.5741549730300903
train gradient:  0.13894124009813275
iteration : 14127
train acc:  0.6953125
train loss:  0.500075101852417
train gradient:  0.14254723355359733
iteration : 14128
train acc:  0.796875
train loss:  0.38905277848243713
train gradient:  0.08956654030451068
iteration : 14129
train acc:  0.7890625
train loss:  0.4673345685005188
train gradient:  0.10914493349277328
iteration : 14130
train acc:  0.7734375
train loss:  0.4650280773639679
train gradient:  0.10787047891908055
iteration : 14131
train acc:  0.6953125
train loss:  0.4738873243331909
train gradient:  0.10323641028810711
iteration : 14132
train acc:  0.7734375
train loss:  0.4498261511325836
train gradient:  0.11956595404528858
iteration : 14133
train acc:  0.734375
train loss:  0.46185001730918884
train gradient:  0.10389086846607692
iteration : 14134
train acc:  0.671875
train loss:  0.5648248791694641
train gradient:  0.15575440983927397
iteration : 14135
train acc:  0.78125
train loss:  0.43915560841560364
train gradient:  0.0892090778846161
iteration : 14136
train acc:  0.765625
train loss:  0.4864898920059204
train gradient:  0.11147709447209236
iteration : 14137
train acc:  0.71875
train loss:  0.5498774647712708
train gradient:  0.1444216067855062
iteration : 14138
train acc:  0.7421875
train loss:  0.43504154682159424
train gradient:  0.10490834353478086
iteration : 14139
train acc:  0.7109375
train loss:  0.5310889482498169
train gradient:  0.12665039993209762
iteration : 14140
train acc:  0.71875
train loss:  0.5377654433250427
train gradient:  0.1521135760896162
iteration : 14141
train acc:  0.7109375
train loss:  0.5218848586082458
train gradient:  0.1288175602829544
iteration : 14142
train acc:  0.71875
train loss:  0.4902915358543396
train gradient:  0.12973584856388737
iteration : 14143
train acc:  0.7421875
train loss:  0.4897018074989319
train gradient:  0.13446334978676508
iteration : 14144
train acc:  0.796875
train loss:  0.47725093364715576
train gradient:  0.12198724872885292
iteration : 14145
train acc:  0.6875
train loss:  0.5199292898178101
train gradient:  0.1491286997550635
iteration : 14146
train acc:  0.7421875
train loss:  0.4871605634689331
train gradient:  0.13083241822263963
iteration : 14147
train acc:  0.734375
train loss:  0.49084746837615967
train gradient:  0.12027889227135531
iteration : 14148
train acc:  0.703125
train loss:  0.5188186764717102
train gradient:  0.1406294264830567
iteration : 14149
train acc:  0.734375
train loss:  0.51959627866745
train gradient:  0.14196160133484353
iteration : 14150
train acc:  0.7421875
train loss:  0.4477435350418091
train gradient:  0.12401603520989236
iteration : 14151
train acc:  0.71875
train loss:  0.4831812083721161
train gradient:  0.09867502870187368
iteration : 14152
train acc:  0.75
train loss:  0.5042880773544312
train gradient:  0.16000957362150237
iteration : 14153
train acc:  0.7109375
train loss:  0.5409311652183533
train gradient:  0.13772537056149992
iteration : 14154
train acc:  0.7265625
train loss:  0.5458579063415527
train gradient:  0.16571325359600236
iteration : 14155
train acc:  0.796875
train loss:  0.4307405948638916
train gradient:  0.09574417723181819
iteration : 14156
train acc:  0.75
train loss:  0.46612393856048584
train gradient:  0.09259881185059311
iteration : 14157
train acc:  0.71875
train loss:  0.5044625997543335
train gradient:  0.13989154605747817
iteration : 14158
train acc:  0.796875
train loss:  0.4206241965293884
train gradient:  0.10115170595186998
iteration : 14159
train acc:  0.7421875
train loss:  0.44474345445632935
train gradient:  0.13040166786126942
iteration : 14160
train acc:  0.7421875
train loss:  0.5061084032058716
train gradient:  0.11539903450005384
iteration : 14161
train acc:  0.703125
train loss:  0.503882884979248
train gradient:  0.10949751832539219
iteration : 14162
train acc:  0.7578125
train loss:  0.4402998387813568
train gradient:  0.09434463157941261
iteration : 14163
train acc:  0.6796875
train loss:  0.5609064102172852
train gradient:  0.13424894367943097
iteration : 14164
train acc:  0.75
train loss:  0.4785183072090149
train gradient:  0.12042384125156483
iteration : 14165
train acc:  0.6875
train loss:  0.49154430627822876
train gradient:  0.1155654382169312
iteration : 14166
train acc:  0.7734375
train loss:  0.514735221862793
train gradient:  0.14656505853409646
iteration : 14167
train acc:  0.7421875
train loss:  0.5012533068656921
train gradient:  0.12430606293109694
iteration : 14168
train acc:  0.796875
train loss:  0.41364067792892456
train gradient:  0.09451216663597091
iteration : 14169
train acc:  0.640625
train loss:  0.6109264492988586
train gradient:  0.21479002988635898
iteration : 14170
train acc:  0.8125
train loss:  0.4320204257965088
train gradient:  0.08926332860624198
iteration : 14171
train acc:  0.7890625
train loss:  0.4550386667251587
train gradient:  0.10500189841046628
iteration : 14172
train acc:  0.828125
train loss:  0.41925039887428284
train gradient:  0.09542301436769036
iteration : 14173
train acc:  0.7265625
train loss:  0.5215393304824829
train gradient:  0.11934884814919319
iteration : 14174
train acc:  0.703125
train loss:  0.5463645458221436
train gradient:  0.2032074181659813
iteration : 14175
train acc:  0.8125
train loss:  0.4411538541316986
train gradient:  0.11177973869686972
iteration : 14176
train acc:  0.7734375
train loss:  0.44383442401885986
train gradient:  0.08977407763051765
iteration : 14177
train acc:  0.71875
train loss:  0.47964978218078613
train gradient:  0.10857571868970181
iteration : 14178
train acc:  0.7421875
train loss:  0.51424241065979
train gradient:  0.14645717016326903
iteration : 14179
train acc:  0.765625
train loss:  0.42212915420532227
train gradient:  0.09703207493699405
iteration : 14180
train acc:  0.75
train loss:  0.46287450194358826
train gradient:  0.11307817893724273
iteration : 14181
train acc:  0.765625
train loss:  0.4833327829837799
train gradient:  0.1320598538383872
iteration : 14182
train acc:  0.765625
train loss:  0.441093772649765
train gradient:  0.10655775928287992
iteration : 14183
train acc:  0.7734375
train loss:  0.46679115295410156
train gradient:  0.13156668115557452
iteration : 14184
train acc:  0.71875
train loss:  0.5037592649459839
train gradient:  0.11982502206639244
iteration : 14185
train acc:  0.703125
train loss:  0.5345637798309326
train gradient:  0.1406088141851934
iteration : 14186
train acc:  0.7777777777777778
train loss:  0.41393133997917175
train gradient:  0.791892948945703
val acc:  0.7421914613122401
val f1:  0.7525682389983017
val confusion matrix:  [[69052 29558]
 [21287 77323]]

----------------------------------------new_epoch--------------------------------------

epoch:  1
iteration : 0
train acc:  0.734375
train loss:  0.4985556900501251
train gradient:  0.10178687963824587
iteration : 1
train acc:  0.8515625
train loss:  0.4033520817756653
train gradient:  0.1155647957999978
iteration : 2
train acc:  0.8359375
train loss:  0.37433165311813354
train gradient:  0.08083844861584956
iteration : 3
train acc:  0.8125
train loss:  0.4386381506919861
train gradient:  0.10633649075419695
iteration : 4
train acc:  0.8203125
train loss:  0.38201725482940674
train gradient:  0.09574874144445504
iteration : 5
train acc:  0.7109375
train loss:  0.5143733024597168
train gradient:  0.1319377239807808
iteration : 6
train acc:  0.75
train loss:  0.5025807619094849
train gradient:  0.13486955998096262
iteration : 7
train acc:  0.7578125
train loss:  0.505914568901062
train gradient:  0.1434126171468297
iteration : 8
train acc:  0.765625
train loss:  0.4812319278717041
train gradient:  0.12176151166690953
iteration : 9
train acc:  0.7734375
train loss:  0.4499940276145935
train gradient:  0.1278305972551462
iteration : 10
train acc:  0.703125
train loss:  0.5107808113098145
train gradient:  0.15687820373591682
iteration : 11
train acc:  0.796875
train loss:  0.46659785509109497
train gradient:  0.10756615081506277
iteration : 12
train acc:  0.703125
train loss:  0.5677393078804016
train gradient:  0.14988260894921773
iteration : 13
train acc:  0.765625
train loss:  0.4351268410682678
train gradient:  0.09183911999682166
iteration : 14
train acc:  0.78125
train loss:  0.46099451184272766
train gradient:  0.12486820263164987
iteration : 15
train acc:  0.7890625
train loss:  0.4552868902683258
train gradient:  0.10251191763351589
iteration : 16
train acc:  0.7421875
train loss:  0.449482798576355
train gradient:  0.11497505178319244
iteration : 17
train acc:  0.8046875
train loss:  0.40227818489074707
train gradient:  0.11202390494424948
iteration : 18
train acc:  0.703125
train loss:  0.5014982223510742
train gradient:  0.11677543037819797
iteration : 19
train acc:  0.6796875
train loss:  0.5394574999809265
train gradient:  0.12696547501481187
iteration : 20
train acc:  0.765625
train loss:  0.4441400170326233
train gradient:  0.09355737034906558
iteration : 21
train acc:  0.734375
train loss:  0.5331606268882751
train gradient:  0.1601516893531873
iteration : 22
train acc:  0.734375
train loss:  0.5208852887153625
train gradient:  0.13728812881091135
iteration : 23
train acc:  0.7734375
train loss:  0.4422187805175781
train gradient:  0.10600317664981984
iteration : 24
train acc:  0.71875
train loss:  0.4772137999534607
train gradient:  0.12139892682871509
iteration : 25
train acc:  0.671875
train loss:  0.5542275905609131
train gradient:  0.14025856263999736
iteration : 26
train acc:  0.7890625
train loss:  0.42787131667137146
train gradient:  0.09160267903136385
iteration : 27
train acc:  0.78125
train loss:  0.477241575717926
train gradient:  0.11390335634311996
iteration : 28
train acc:  0.6875
train loss:  0.551392138004303
train gradient:  0.16347150742492295
iteration : 29
train acc:  0.7421875
train loss:  0.4635135233402252
train gradient:  0.14461603808071027
iteration : 30
train acc:  0.6796875
train loss:  0.5222165584564209
train gradient:  0.12205228429880259
iteration : 31
train acc:  0.7109375
train loss:  0.5052919387817383
train gradient:  0.12536560796044954
iteration : 32
train acc:  0.71875
train loss:  0.5239688158035278
train gradient:  0.13600348015628214
iteration : 33
train acc:  0.71875
train loss:  0.5148605108261108
train gradient:  0.11730580345188865
iteration : 34
train acc:  0.703125
train loss:  0.5673750042915344
train gradient:  0.21114234844905114
iteration : 35
train acc:  0.75
train loss:  0.4838371276855469
train gradient:  0.14204117641843422
iteration : 36
train acc:  0.7578125
train loss:  0.4752253293991089
train gradient:  0.10046917843938764
iteration : 37
train acc:  0.7578125
train loss:  0.4942452609539032
train gradient:  0.12500375855760315
iteration : 38
train acc:  0.75
train loss:  0.5409324765205383
train gradient:  0.13491308291083676
iteration : 39
train acc:  0.78125
train loss:  0.4538496732711792
train gradient:  0.12327942149371907
iteration : 40
train acc:  0.7890625
train loss:  0.4419160783290863
train gradient:  0.10283379249239857
iteration : 41
train acc:  0.7421875
train loss:  0.5425088405609131
train gradient:  0.21305638851642436
iteration : 42
train acc:  0.7578125
train loss:  0.445125550031662
train gradient:  0.10291571839766817
iteration : 43
train acc:  0.7109375
train loss:  0.5093628168106079
train gradient:  0.12764334502463853
iteration : 44
train acc:  0.78125
train loss:  0.41737404465675354
train gradient:  0.08832420522328888
iteration : 45
train acc:  0.7578125
train loss:  0.4563847780227661
train gradient:  0.09898115913862068
iteration : 46
train acc:  0.7890625
train loss:  0.420519083738327
train gradient:  0.10161888153458344
iteration : 47
train acc:  0.71875
train loss:  0.5107097029685974
train gradient:  0.14065890665088224
iteration : 48
train acc:  0.6875
train loss:  0.5574187636375427
train gradient:  0.17460647319768874
iteration : 49
train acc:  0.78125
train loss:  0.43159857392311096
train gradient:  0.11924743753552852
iteration : 50
train acc:  0.796875
train loss:  0.42691540718078613
train gradient:  0.09234934415340969
iteration : 51
train acc:  0.765625
train loss:  0.44386932253837585
train gradient:  0.11925290967620045
iteration : 52
train acc:  0.734375
train loss:  0.4819566011428833
train gradient:  0.09386052806430793
iteration : 53
train acc:  0.765625
train loss:  0.4447423815727234
train gradient:  0.10606995300348152
iteration : 54
train acc:  0.7890625
train loss:  0.4322674870491028
train gradient:  0.09752894454170533
iteration : 55
train acc:  0.7421875
train loss:  0.4306958317756653
train gradient:  0.0797013141014507
iteration : 56
train acc:  0.7578125
train loss:  0.42314791679382324
train gradient:  0.08668276524493305
iteration : 57
train acc:  0.75
train loss:  0.48530328273773193
train gradient:  0.1499771571959085
iteration : 58
train acc:  0.7734375
train loss:  0.4613839089870453
train gradient:  0.10563565954167166
iteration : 59
train acc:  0.625
train loss:  0.5751395225524902
train gradient:  0.1495921891422014
iteration : 60
train acc:  0.765625
train loss:  0.4441671073436737
train gradient:  0.10289723629471444
iteration : 61
train acc:  0.7734375
train loss:  0.48120445013046265
train gradient:  0.11062083841036685
iteration : 62
train acc:  0.7578125
train loss:  0.4415861964225769
train gradient:  0.1208964069140756
iteration : 63
train acc:  0.75
train loss:  0.4981142282485962
train gradient:  0.14331324706704265
iteration : 64
train acc:  0.84375
train loss:  0.43137481808662415
train gradient:  0.08741852470255902
iteration : 65
train acc:  0.7421875
train loss:  0.49262702465057373
train gradient:  0.1337199598341679
iteration : 66
train acc:  0.703125
train loss:  0.5366734266281128
train gradient:  0.11607196785330832
iteration : 67
train acc:  0.7265625
train loss:  0.5095025300979614
train gradient:  0.2009442140347443
iteration : 68
train acc:  0.8046875
train loss:  0.43643897771835327
train gradient:  0.11715751913580572
iteration : 69
train acc:  0.7109375
train loss:  0.5255284309387207
train gradient:  0.15561848747928514
iteration : 70
train acc:  0.765625
train loss:  0.44147640466690063
train gradient:  0.10405056128532468
iteration : 71
train acc:  0.78125
train loss:  0.41991525888442993
train gradient:  0.09300141494161698
iteration : 72
train acc:  0.796875
train loss:  0.44506943225860596
train gradient:  0.08827344583383846
iteration : 73
train acc:  0.7890625
train loss:  0.41056180000305176
train gradient:  0.10272966956459198
iteration : 74
train acc:  0.7578125
train loss:  0.5475476980209351
train gradient:  0.14589987502010587
iteration : 75
train acc:  0.7578125
train loss:  0.455986887216568
train gradient:  0.10195480916181404
iteration : 76
train acc:  0.71875
train loss:  0.530077338218689
train gradient:  0.1610731639150489
iteration : 77
train acc:  0.7890625
train loss:  0.5378892421722412
train gradient:  0.13735638995500765
iteration : 78
train acc:  0.75
train loss:  0.5081146359443665
train gradient:  0.1281775877207284
iteration : 79
train acc:  0.7421875
train loss:  0.47480690479278564
train gradient:  0.15246100417738062
iteration : 80
train acc:  0.75
train loss:  0.5075297951698303
train gradient:  0.16824797528028157
iteration : 81
train acc:  0.7421875
train loss:  0.487609326839447
train gradient:  0.12405938042263763
iteration : 82
train acc:  0.7421875
train loss:  0.4773663878440857
train gradient:  0.13543952911857254
iteration : 83
train acc:  0.78125
train loss:  0.45915019512176514
train gradient:  0.11718310263083169
iteration : 84
train acc:  0.6953125
train loss:  0.4974774718284607
train gradient:  0.14569948585482595
iteration : 85
train acc:  0.7734375
train loss:  0.47169029712677
train gradient:  0.09615618735070847
iteration : 86
train acc:  0.734375
train loss:  0.4825921058654785
train gradient:  0.09991121615541806
iteration : 87
train acc:  0.7109375
train loss:  0.581385612487793
train gradient:  0.1501356302226886
iteration : 88
train acc:  0.8125
train loss:  0.4006749391555786
train gradient:  0.10613225286231545
iteration : 89
train acc:  0.6640625
train loss:  0.5121004581451416
train gradient:  0.11555974365558438
iteration : 90
train acc:  0.7265625
train loss:  0.43601083755493164
train gradient:  0.1125210267138843
iteration : 91
train acc:  0.6875
train loss:  0.5488497018814087
train gradient:  0.1449406005634953
iteration : 92
train acc:  0.765625
train loss:  0.49467533826828003
train gradient:  0.1405316636913702
iteration : 93
train acc:  0.7578125
train loss:  0.45279958844184875
train gradient:  0.1058270951415337
iteration : 94
train acc:  0.71875
train loss:  0.4467361569404602
train gradient:  0.09648871277324184
iteration : 95
train acc:  0.7265625
train loss:  0.521234393119812
train gradient:  0.14687374764200217
iteration : 96
train acc:  0.6640625
train loss:  0.6233879327774048
train gradient:  0.16542530240237968
iteration : 97
train acc:  0.734375
train loss:  0.4980417490005493
train gradient:  0.13292088832622562
iteration : 98
train acc:  0.7578125
train loss:  0.48750415444374084
train gradient:  0.14133832939575502
iteration : 99
train acc:  0.8125
train loss:  0.4232342839241028
train gradient:  0.12333954207954033
iteration : 100
train acc:  0.703125
train loss:  0.5323762893676758
train gradient:  0.13601321485654883
iteration : 101
train acc:  0.7109375
train loss:  0.5232079029083252
train gradient:  0.13079225235176584
iteration : 102
train acc:  0.734375
train loss:  0.5003504157066345
train gradient:  0.1612717927429771
iteration : 103
train acc:  0.7734375
train loss:  0.46205127239227295
train gradient:  0.1209097717152974
iteration : 104
train acc:  0.71875
train loss:  0.5096561312675476
train gradient:  0.16716554028289132
iteration : 105
train acc:  0.78125
train loss:  0.4662971496582031
train gradient:  0.13029058193646628
iteration : 106
train acc:  0.703125
train loss:  0.4950437545776367
train gradient:  0.14207196801036692
iteration : 107
train acc:  0.765625
train loss:  0.43945837020874023
train gradient:  0.1030615376003072
iteration : 108
train acc:  0.7109375
train loss:  0.49964672327041626
train gradient:  0.13578414495784763
iteration : 109
train acc:  0.78125
train loss:  0.4244186282157898
train gradient:  0.08424333424810163
iteration : 110
train acc:  0.7421875
train loss:  0.5082803964614868
train gradient:  0.15597769455353694
iteration : 111
train acc:  0.7265625
train loss:  0.5026087164878845
train gradient:  0.13228099931857173
iteration : 112
train acc:  0.8359375
train loss:  0.4032211899757385
train gradient:  0.1176306946402387
iteration : 113
train acc:  0.7578125
train loss:  0.4648612439632416
train gradient:  0.12465499155725786
iteration : 114
train acc:  0.7421875
train loss:  0.4904295802116394
train gradient:  0.1020655173821733
iteration : 115
train acc:  0.7890625
train loss:  0.4879600405693054
train gradient:  0.12151014268396759
iteration : 116
train acc:  0.78125
train loss:  0.42321839928627014
train gradient:  0.09389886550639742
iteration : 117
train acc:  0.7578125
train loss:  0.4490094780921936
train gradient:  0.11126111796593985
iteration : 118
train acc:  0.796875
train loss:  0.4712858200073242
train gradient:  0.13965351795007486
iteration : 119
train acc:  0.71875
train loss:  0.4692571759223938
train gradient:  0.11555283226762128
iteration : 120
train acc:  0.7265625
train loss:  0.5066951513290405
train gradient:  0.14425196458483142
iteration : 121
train acc:  0.765625
train loss:  0.501480221748352
train gradient:  0.12513730011295696
iteration : 122
train acc:  0.7578125
train loss:  0.4756001830101013
train gradient:  0.10551964181787099
iteration : 123
train acc:  0.8046875
train loss:  0.418824702501297
train gradient:  0.09756807920741528
iteration : 124
train acc:  0.6953125
train loss:  0.4721495509147644
train gradient:  0.11124438536930058
iteration : 125
train acc:  0.671875
train loss:  0.5286278128623962
train gradient:  0.14888167957226853
iteration : 126
train acc:  0.734375
train loss:  0.4716404974460602
train gradient:  0.12055511816840798
iteration : 127
train acc:  0.703125
train loss:  0.5036106109619141
train gradient:  0.13437803889126337
iteration : 128
train acc:  0.7734375
train loss:  0.433136522769928
train gradient:  0.10129420448496404
iteration : 129
train acc:  0.75
train loss:  0.46418917179107666
train gradient:  0.1576335048629969
iteration : 130
train acc:  0.703125
train loss:  0.5240564346313477
train gradient:  0.14114553108313382
iteration : 131
train acc:  0.7421875
train loss:  0.5250725746154785
train gradient:  0.13333581940023248
iteration : 132
train acc:  0.765625
train loss:  0.45370084047317505
train gradient:  0.11343036335038677
iteration : 133
train acc:  0.7578125
train loss:  0.4710101783275604
train gradient:  0.10096920885172048
iteration : 134
train acc:  0.828125
train loss:  0.4096235930919647
train gradient:  0.08365314533298156
iteration : 135
train acc:  0.7421875
train loss:  0.49059581756591797
train gradient:  0.12144783239843657
iteration : 136
train acc:  0.6953125
train loss:  0.5120654702186584
train gradient:  0.15877753060402894
iteration : 137
train acc:  0.7421875
train loss:  0.4861328601837158
train gradient:  0.12946352058778962
iteration : 138
train acc:  0.71875
train loss:  0.4741232395172119
train gradient:  0.13814583267498054
iteration : 139
train acc:  0.7109375
train loss:  0.48957663774490356
train gradient:  0.10345010123442759
iteration : 140
train acc:  0.7578125
train loss:  0.4929874539375305
train gradient:  0.12147646067451962
iteration : 141
train acc:  0.7890625
train loss:  0.47133776545524597
train gradient:  0.11826225672175628
iteration : 142
train acc:  0.7578125
train loss:  0.514934778213501
train gradient:  0.13481237934792825
iteration : 143
train acc:  0.7421875
train loss:  0.48771020770072937
train gradient:  0.10046239468478681
iteration : 144
train acc:  0.7890625
train loss:  0.4500436782836914
train gradient:  0.1164713553967334
iteration : 145
train acc:  0.6953125
train loss:  0.5370136499404907
train gradient:  0.11941050679479553
iteration : 146
train acc:  0.703125
train loss:  0.49289530515670776
train gradient:  0.14471251698602722
iteration : 147
train acc:  0.8046875
train loss:  0.45811983942985535
train gradient:  0.10304209962811425
iteration : 148
train acc:  0.71875
train loss:  0.5178921222686768
train gradient:  0.1431119339190765
iteration : 149
train acc:  0.71875
train loss:  0.49331897497177124
train gradient:  0.11957750854785026
iteration : 150
train acc:  0.6875
train loss:  0.49229860305786133
train gradient:  0.17725605066072442
iteration : 151
train acc:  0.75
train loss:  0.5009533762931824
train gradient:  0.11108858437152716
iteration : 152
train acc:  0.7578125
train loss:  0.4508427381515503
train gradient:  0.125009617952343
iteration : 153
train acc:  0.765625
train loss:  0.44361671805381775
train gradient:  0.09468769123667768
iteration : 154
train acc:  0.7265625
train loss:  0.515815258026123
train gradient:  0.13717712596575016
iteration : 155
train acc:  0.734375
train loss:  0.48048529028892517
train gradient:  0.13104330630837602
iteration : 156
train acc:  0.734375
train loss:  0.48161637783050537
train gradient:  0.1423000149394964
iteration : 157
train acc:  0.7265625
train loss:  0.5026834011077881
train gradient:  0.11633284203464984
iteration : 158
train acc:  0.8203125
train loss:  0.45335817337036133
train gradient:  0.10134384071105193
iteration : 159
train acc:  0.734375
train loss:  0.4732021987438202
train gradient:  0.11791847230662196
iteration : 160
train acc:  0.7578125
train loss:  0.4634511470794678
train gradient:  0.11343165192588281
iteration : 161
train acc:  0.703125
train loss:  0.5369347929954529
train gradient:  0.14510027423858451
iteration : 162
train acc:  0.7734375
train loss:  0.44070690870285034
train gradient:  0.13213767366922435
iteration : 163
train acc:  0.7578125
train loss:  0.4648768901824951
train gradient:  0.15074121275145008
iteration : 164
train acc:  0.6875
train loss:  0.5483559370040894
train gradient:  0.1458233233136444
iteration : 165
train acc:  0.703125
train loss:  0.5344349145889282
train gradient:  0.13593063987509046
iteration : 166
train acc:  0.7890625
train loss:  0.455929160118103
train gradient:  0.13005000628706892
iteration : 167
train acc:  0.765625
train loss:  0.5027693510055542
train gradient:  0.16658386558442342
iteration : 168
train acc:  0.828125
train loss:  0.41528064012527466
train gradient:  0.09992198213570849
iteration : 169
train acc:  0.8203125
train loss:  0.40775859355926514
train gradient:  0.09238487311201697
iteration : 170
train acc:  0.7578125
train loss:  0.4986204504966736
train gradient:  0.1218614252672652
iteration : 171
train acc:  0.765625
train loss:  0.4798581302165985
train gradient:  0.10791684307321948
iteration : 172
train acc:  0.7890625
train loss:  0.4480135440826416
train gradient:  0.11154329934339614
iteration : 173
train acc:  0.7265625
train loss:  0.5072381496429443
train gradient:  0.11032746791910784
iteration : 174
train acc:  0.7421875
train loss:  0.4915676712989807
train gradient:  0.11936469236524996
iteration : 175
train acc:  0.734375
train loss:  0.4679923355579376
train gradient:  0.10518621420423387
iteration : 176
train acc:  0.7421875
train loss:  0.43239113688468933
train gradient:  0.11012223291926879
iteration : 177
train acc:  0.7421875
train loss:  0.5084556341171265
train gradient:  0.14568466171257363
iteration : 178
train acc:  0.7578125
train loss:  0.4739850163459778
train gradient:  0.11177348298026363
iteration : 179
train acc:  0.7421875
train loss:  0.5206972360610962
train gradient:  0.1308368196667128
iteration : 180
train acc:  0.796875
train loss:  0.4239662289619446
train gradient:  0.10520703770275054
iteration : 181
train acc:  0.75
train loss:  0.45763179659843445
train gradient:  0.11653770438994578
iteration : 182
train acc:  0.8125
train loss:  0.4257941246032715
train gradient:  0.09050317922368047
iteration : 183
train acc:  0.6640625
train loss:  0.6068680286407471
train gradient:  0.2481044606181024
iteration : 184
train acc:  0.6875
train loss:  0.5541778802871704
train gradient:  0.16170646430014188
iteration : 185
train acc:  0.7421875
train loss:  0.4603160619735718
train gradient:  0.1219810478632217
iteration : 186
train acc:  0.7734375
train loss:  0.4364393353462219
train gradient:  0.12230265198994109
iteration : 187
train acc:  0.78125
train loss:  0.45135825872421265
train gradient:  0.12149610251662091
iteration : 188
train acc:  0.8046875
train loss:  0.3978739380836487
train gradient:  0.07195878845007532
iteration : 189
train acc:  0.7578125
train loss:  0.5034054517745972
train gradient:  0.11032040668944429
iteration : 190
train acc:  0.8046875
train loss:  0.43954792618751526
train gradient:  0.08264431105271099
iteration : 191
train acc:  0.796875
train loss:  0.4287574887275696
train gradient:  0.1033780669400422
iteration : 192
train acc:  0.765625
train loss:  0.48402804136276245
train gradient:  0.12133568619430432
iteration : 193
train acc:  0.7578125
train loss:  0.503639280796051
train gradient:  0.13349518659600612
iteration : 194
train acc:  0.7421875
train loss:  0.4902460277080536
train gradient:  0.11497108204765369
iteration : 195
train acc:  0.7890625
train loss:  0.4355267882347107
train gradient:  0.10638960435085183
iteration : 196
train acc:  0.8125
train loss:  0.4116572141647339
train gradient:  0.0918163070049916
iteration : 197
train acc:  0.734375
train loss:  0.4591476619243622
train gradient:  0.10618339055329214
iteration : 198
train acc:  0.6640625
train loss:  0.5822892189025879
train gradient:  0.15193934038378137
iteration : 199
train acc:  0.7578125
train loss:  0.44973665475845337
train gradient:  0.10529389368810323
iteration : 200
train acc:  0.75
train loss:  0.46540719270706177
train gradient:  0.11597493768769712
iteration : 201
train acc:  0.703125
train loss:  0.5394628047943115
train gradient:  0.13516632566353193
iteration : 202
train acc:  0.78125
train loss:  0.46445947885513306
train gradient:  0.14932269788715916
iteration : 203
train acc:  0.6875
train loss:  0.5270209908485413
train gradient:  0.1311237356057165
iteration : 204
train acc:  0.8046875
train loss:  0.4285527467727661
train gradient:  0.09384674663039963
iteration : 205
train acc:  0.703125
train loss:  0.555904746055603
train gradient:  0.1617251282020214
iteration : 206
train acc:  0.796875
train loss:  0.5091244578361511
train gradient:  0.15002400994953702
iteration : 207
train acc:  0.796875
train loss:  0.42472371459007263
train gradient:  0.09312986839050368
iteration : 208
train acc:  0.8046875
train loss:  0.4611455798149109
train gradient:  0.15507148610082727
iteration : 209
train acc:  0.7890625
train loss:  0.43498876690864563
train gradient:  0.08754137005933621
iteration : 210
train acc:  0.7265625
train loss:  0.5168541669845581
train gradient:  0.13924954233036435
iteration : 211
train acc:  0.7578125
train loss:  0.42373090982437134
train gradient:  0.11166200870538923
iteration : 212
train acc:  0.8046875
train loss:  0.44992774724960327
train gradient:  0.12002570632906744
iteration : 213
train acc:  0.7578125
train loss:  0.47595977783203125
train gradient:  0.11510731846181722
iteration : 214
train acc:  0.7421875
train loss:  0.484758585691452
train gradient:  0.12226572202439989
iteration : 215
train acc:  0.75
train loss:  0.46832722425460815
train gradient:  0.11342820147604646
iteration : 216
train acc:  0.7265625
train loss:  0.562221348285675
train gradient:  0.1619476037425473
iteration : 217
train acc:  0.7734375
train loss:  0.5195713639259338
train gradient:  0.16232158435005123
iteration : 218
train acc:  0.71875
train loss:  0.52031409740448
train gradient:  0.15228728920239284
iteration : 219
train acc:  0.6953125
train loss:  0.5708011388778687
train gradient:  0.17971153887524377
iteration : 220
train acc:  0.8125
train loss:  0.44559967517852783
train gradient:  0.10285737349945469
iteration : 221
train acc:  0.703125
train loss:  0.5246232748031616
train gradient:  0.13683940258667743
iteration : 222
train acc:  0.734375
train loss:  0.4273788332939148
train gradient:  0.0931538723886468
iteration : 223
train acc:  0.7265625
train loss:  0.5063914060592651
train gradient:  0.1387916349205342
iteration : 224
train acc:  0.703125
train loss:  0.5309001207351685
train gradient:  0.16679821504024478
iteration : 225
train acc:  0.7109375
train loss:  0.5589447617530823
train gradient:  0.21934975012945485
iteration : 226
train acc:  0.71875
train loss:  0.4859275817871094
train gradient:  0.10986974822228023
iteration : 227
train acc:  0.71875
train loss:  0.5130926370620728
train gradient:  0.13732429598528426
iteration : 228
train acc:  0.78125
train loss:  0.46900230646133423
train gradient:  0.09331252868927864
iteration : 229
train acc:  0.78125
train loss:  0.45173460245132446
train gradient:  0.11010420177309745
iteration : 230
train acc:  0.6796875
train loss:  0.564610481262207
train gradient:  0.14656556930095166
iteration : 231
train acc:  0.78125
train loss:  0.4351242184638977
train gradient:  0.10605665710150451
iteration : 232
train acc:  0.765625
train loss:  0.4563995599746704
train gradient:  0.0937403829994991
iteration : 233
train acc:  0.71875
train loss:  0.5212154388427734
train gradient:  0.15777754574710656
iteration : 234
train acc:  0.765625
train loss:  0.46743106842041016
train gradient:  0.10531986118333296
iteration : 235
train acc:  0.7734375
train loss:  0.4425431489944458
train gradient:  0.09669389115748571
iteration : 236
train acc:  0.7734375
train loss:  0.45288312435150146
train gradient:  0.10014822721935696
iteration : 237
train acc:  0.7890625
train loss:  0.4665052890777588
train gradient:  0.11092394100157109
iteration : 238
train acc:  0.7265625
train loss:  0.4759595990180969
train gradient:  0.12411273703882317
iteration : 239
train acc:  0.7890625
train loss:  0.4324633777141571
train gradient:  0.10879307611637569
iteration : 240
train acc:  0.7578125
train loss:  0.4940299391746521
train gradient:  0.1333579674433732
iteration : 241
train acc:  0.7734375
train loss:  0.4312211871147156
train gradient:  0.12462807844833937
iteration : 242
train acc:  0.765625
train loss:  0.4564799666404724
train gradient:  0.11669033912482624
iteration : 243
train acc:  0.78125
train loss:  0.44549983739852905
train gradient:  0.11100270896078611
iteration : 244
train acc:  0.71875
train loss:  0.5461188554763794
train gradient:  0.22697301406316192
iteration : 245
train acc:  0.7734375
train loss:  0.46444469690322876
train gradient:  0.11895290475263186
iteration : 246
train acc:  0.71875
train loss:  0.5030823349952698
train gradient:  0.12003967984674328
iteration : 247
train acc:  0.75
train loss:  0.4843917489051819
train gradient:  0.11613275720472835
iteration : 248
train acc:  0.75
train loss:  0.5038939714431763
train gradient:  0.13969720310609535
iteration : 249
train acc:  0.7421875
train loss:  0.5075480937957764
train gradient:  0.1544476967049704
iteration : 250
train acc:  0.7265625
train loss:  0.5154728293418884
train gradient:  0.15725853732545036
iteration : 251
train acc:  0.6953125
train loss:  0.5217113494873047
train gradient:  0.13134609737325237
iteration : 252
train acc:  0.6953125
train loss:  0.5593172311782837
train gradient:  0.16930973829631213
iteration : 253
train acc:  0.6875
train loss:  0.557921826839447
train gradient:  0.1439213938285222
iteration : 254
train acc:  0.8046875
train loss:  0.40926647186279297
train gradient:  0.11256963350298607
iteration : 255
train acc:  0.7890625
train loss:  0.4250359535217285
train gradient:  0.09749348880327219
iteration : 256
train acc:  0.7421875
train loss:  0.472982794046402
train gradient:  0.13312951105787518
iteration : 257
train acc:  0.7890625
train loss:  0.4185352325439453
train gradient:  0.08367334601286439
iteration : 258
train acc:  0.8515625
train loss:  0.40601181983947754
train gradient:  0.1394499327216274
iteration : 259
train acc:  0.7421875
train loss:  0.4854474365711212
train gradient:  0.10668943546745914
iteration : 260
train acc:  0.7421875
train loss:  0.4683089852333069
train gradient:  0.10789954407701606
iteration : 261
train acc:  0.7890625
train loss:  0.38696157932281494
train gradient:  0.10017404501978804
iteration : 262
train acc:  0.7734375
train loss:  0.4820719063282013
train gradient:  0.11238559966500661
iteration : 263
train acc:  0.7734375
train loss:  0.4742186665534973
train gradient:  0.11964726093158853
iteration : 264
train acc:  0.7265625
train loss:  0.4821230173110962
train gradient:  0.11143365305644408
iteration : 265
train acc:  0.703125
train loss:  0.4700964689254761
train gradient:  0.09657050330938764
iteration : 266
train acc:  0.8125
train loss:  0.37673279643058777
train gradient:  0.09099472872421338
iteration : 267
train acc:  0.765625
train loss:  0.47004610300064087
train gradient:  0.10217651422502867
iteration : 268
train acc:  0.7109375
train loss:  0.5738502740859985
train gradient:  0.14335390688382993
iteration : 269
train acc:  0.765625
train loss:  0.473678857088089
train gradient:  0.10341424003125207
iteration : 270
train acc:  0.7578125
train loss:  0.4610821008682251
train gradient:  0.12423068622464944
iteration : 271
train acc:  0.7109375
train loss:  0.5379664897918701
train gradient:  0.143775019320978
iteration : 272
train acc:  0.7890625
train loss:  0.5243546962738037
train gradient:  0.15025605511838064
iteration : 273
train acc:  0.765625
train loss:  0.4203392267227173
train gradient:  0.09548730618358597
iteration : 274
train acc:  0.7109375
train loss:  0.5252081155776978
train gradient:  0.15736350313812691
iteration : 275
train acc:  0.734375
train loss:  0.4827065169811249
train gradient:  0.11012602641886485
iteration : 276
train acc:  0.7578125
train loss:  0.4806218147277832
train gradient:  0.11065970653490954
iteration : 277
train acc:  0.7734375
train loss:  0.4255973994731903
train gradient:  0.07747811810968121
iteration : 278
train acc:  0.7265625
train loss:  0.49774348735809326
train gradient:  0.12540822548333008
iteration : 279
train acc:  0.640625
train loss:  0.5535396337509155
train gradient:  0.16127876609961994
iteration : 280
train acc:  0.6875
train loss:  0.5424761772155762
train gradient:  0.12008725407648785
iteration : 281
train acc:  0.7109375
train loss:  0.5293300151824951
train gradient:  0.17275031829542958
iteration : 282
train acc:  0.7265625
train loss:  0.47961509227752686
train gradient:  0.12562696132024156
iteration : 283
train acc:  0.734375
train loss:  0.5186548233032227
train gradient:  0.1369636202585366
iteration : 284
train acc:  0.78125
train loss:  0.41217297315597534
train gradient:  0.13171483919063157
iteration : 285
train acc:  0.7265625
train loss:  0.49699050188064575
train gradient:  0.1222753739132547
iteration : 286
train acc:  0.765625
train loss:  0.4699925184249878
train gradient:  0.13495945200346754
iteration : 287
train acc:  0.828125
train loss:  0.4034464955329895
train gradient:  0.10067648384054274
iteration : 288
train acc:  0.796875
train loss:  0.4613044261932373
train gradient:  0.11010594530636351
iteration : 289
train acc:  0.7578125
train loss:  0.5002009868621826
train gradient:  0.14524933562392223
iteration : 290
train acc:  0.6796875
train loss:  0.5405320525169373
train gradient:  0.1431169050421121
iteration : 291
train acc:  0.734375
train loss:  0.4850963056087494
train gradient:  0.11870483561044111
iteration : 292
train acc:  0.7109375
train loss:  0.4576227068901062
train gradient:  0.11932419015808068
iteration : 293
train acc:  0.7109375
train loss:  0.5083950757980347
train gradient:  0.10088828632248345
iteration : 294
train acc:  0.7421875
train loss:  0.4308202862739563
train gradient:  0.09007400649029974
iteration : 295
train acc:  0.8359375
train loss:  0.355145663022995
train gradient:  0.07511834936303058
iteration : 296
train acc:  0.7890625
train loss:  0.42822909355163574
train gradient:  0.12109305006576455
iteration : 297
train acc:  0.703125
train loss:  0.5659686326980591
train gradient:  0.18847509675752347
iteration : 298
train acc:  0.8046875
train loss:  0.42957860231399536
train gradient:  0.10941725967584695
iteration : 299
train acc:  0.765625
train loss:  0.46871477365493774
train gradient:  0.11890559825185071
iteration : 300
train acc:  0.7578125
train loss:  0.4806530475616455
train gradient:  0.1246405104636818
iteration : 301
train acc:  0.7734375
train loss:  0.4358580708503723
train gradient:  0.14712345589639472
iteration : 302
train acc:  0.7421875
train loss:  0.47379183769226074
train gradient:  0.09662739368976211
iteration : 303
train acc:  0.71875
train loss:  0.4980449974536896
train gradient:  0.11499704392396214
iteration : 304
train acc:  0.7734375
train loss:  0.4670550525188446
train gradient:  0.12683086304098945
iteration : 305
train acc:  0.828125
train loss:  0.44107896089553833
train gradient:  0.09817901804120484
iteration : 306
train acc:  0.71875
train loss:  0.5087752938270569
train gradient:  0.1295499326988959
iteration : 307
train acc:  0.7890625
train loss:  0.5176125764846802
train gradient:  0.13337891240946012
iteration : 308
train acc:  0.765625
train loss:  0.4377105236053467
train gradient:  0.09612485128283876
iteration : 309
train acc:  0.75
train loss:  0.4395173192024231
train gradient:  0.10533693881439271
iteration : 310
train acc:  0.7109375
train loss:  0.5571058988571167
train gradient:  0.13478252621160883
iteration : 311
train acc:  0.71875
train loss:  0.5122698545455933
train gradient:  0.1230473209959242
iteration : 312
train acc:  0.71875
train loss:  0.5338557958602905
train gradient:  0.14136288348086412
iteration : 313
train acc:  0.7578125
train loss:  0.463759183883667
train gradient:  0.10252273920850853
iteration : 314
train acc:  0.765625
train loss:  0.43216603994369507
train gradient:  0.1237670594206509
iteration : 315
train acc:  0.75
train loss:  0.5453025102615356
train gradient:  0.13814649304998433
iteration : 316
train acc:  0.78125
train loss:  0.44948965311050415
train gradient:  0.09630149637416084
iteration : 317
train acc:  0.8046875
train loss:  0.4431781768798828
train gradient:  0.10727052453424832
iteration : 318
train acc:  0.796875
train loss:  0.4281611740589142
train gradient:  0.09856770383776596
iteration : 319
train acc:  0.828125
train loss:  0.4093300700187683
train gradient:  0.08312650038977952
iteration : 320
train acc:  0.7109375
train loss:  0.5518262386322021
train gradient:  0.15263895964526736
iteration : 321
train acc:  0.859375
train loss:  0.39386701583862305
train gradient:  0.07121048131835134
iteration : 322
train acc:  0.75
train loss:  0.5180032849311829
train gradient:  0.13474802347059434
iteration : 323
train acc:  0.7421875
train loss:  0.4864721894264221
train gradient:  0.12003907407203067
iteration : 324
train acc:  0.78125
train loss:  0.4349994361400604
train gradient:  0.09926815442445082
iteration : 325
train acc:  0.65625
train loss:  0.5365538001060486
train gradient:  0.1355887652642641
iteration : 326
train acc:  0.8203125
train loss:  0.40424278378486633
train gradient:  0.08668796269260835
iteration : 327
train acc:  0.7265625
train loss:  0.4401299059391022
train gradient:  0.08766204205334464
iteration : 328
train acc:  0.6640625
train loss:  0.5633717775344849
train gradient:  0.1483286940655158
iteration : 329
train acc:  0.7734375
train loss:  0.4221569001674652
train gradient:  0.1014330198628203
iteration : 330
train acc:  0.71875
train loss:  0.47791603207588196
train gradient:  0.1261482022000067
iteration : 331
train acc:  0.7109375
train loss:  0.5063754320144653
train gradient:  0.1959251028414818
iteration : 332
train acc:  0.703125
train loss:  0.4994330406188965
train gradient:  0.11953346806988205
iteration : 333
train acc:  0.7421875
train loss:  0.5483874082565308
train gradient:  0.16103462835772042
iteration : 334
train acc:  0.6796875
train loss:  0.6062080264091492
train gradient:  0.18782657831172694
iteration : 335
train acc:  0.7734375
train loss:  0.42344146966934204
train gradient:  0.10205504277314027
iteration : 336
train acc:  0.765625
train loss:  0.4518837332725525
train gradient:  0.10379238741990117
iteration : 337
train acc:  0.765625
train loss:  0.47705546021461487
train gradient:  0.14234813651213535
iteration : 338
train acc:  0.7890625
train loss:  0.42108678817749023
train gradient:  0.1095231123435397
iteration : 339
train acc:  0.71875
train loss:  0.46961116790771484
train gradient:  0.12966375438178615
iteration : 340
train acc:  0.7890625
train loss:  0.44125282764434814
train gradient:  0.10745117911839189
iteration : 341
train acc:  0.7109375
train loss:  0.5429607033729553
train gradient:  0.15606821674595212
iteration : 342
train acc:  0.7421875
train loss:  0.49930721521377563
train gradient:  0.13422280894407645
iteration : 343
train acc:  0.7265625
train loss:  0.5156960487365723
train gradient:  0.12164594809341497
iteration : 344
train acc:  0.7578125
train loss:  0.48114827275276184
train gradient:  0.12580437628434438
iteration : 345
train acc:  0.75
train loss:  0.5581187009811401
train gradient:  0.17902930002492437
iteration : 346
train acc:  0.7421875
train loss:  0.4839175343513489
train gradient:  0.11613706351527041
iteration : 347
train acc:  0.765625
train loss:  0.4681808650493622
train gradient:  0.12577362876965295
iteration : 348
train acc:  0.765625
train loss:  0.4736260771751404
train gradient:  0.12401787481123684
iteration : 349
train acc:  0.7578125
train loss:  0.49985745549201965
train gradient:  0.13890079223626517
iteration : 350
train acc:  0.6953125
train loss:  0.5117826461791992
train gradient:  0.15060009329026583
iteration : 351
train acc:  0.75
train loss:  0.4915100634098053
train gradient:  0.12126637144327759
iteration : 352
train acc:  0.75
train loss:  0.46771329641342163
train gradient:  0.10151982662907058
iteration : 353
train acc:  0.6875
train loss:  0.5679283142089844
train gradient:  0.1511689465766417
iteration : 354
train acc:  0.7734375
train loss:  0.44142448902130127
train gradient:  0.11694832094160347
iteration : 355
train acc:  0.765625
train loss:  0.41012412309646606
train gradient:  0.11553818621988418
iteration : 356
train acc:  0.7578125
train loss:  0.48990508913993835
train gradient:  0.11752324028956937
iteration : 357
train acc:  0.734375
train loss:  0.4990749955177307
train gradient:  0.12509341499298948
iteration : 358
train acc:  0.671875
train loss:  0.5810601711273193
train gradient:  0.2269765972114218
iteration : 359
train acc:  0.796875
train loss:  0.4019756317138672
train gradient:  0.10599254708297778
iteration : 360
train acc:  0.7578125
train loss:  0.4880638122558594
train gradient:  0.12060304497781939
iteration : 361
train acc:  0.765625
train loss:  0.47044000029563904
train gradient:  0.12028227749816942
iteration : 362
train acc:  0.7734375
train loss:  0.4397910237312317
train gradient:  0.10634263743596646
iteration : 363
train acc:  0.765625
train loss:  0.47873711585998535
train gradient:  0.13460810783012384
iteration : 364
train acc:  0.703125
train loss:  0.5105273127555847
train gradient:  0.11663131545061686
iteration : 365
train acc:  0.703125
train loss:  0.5227916240692139
train gradient:  0.14497092752466176
iteration : 366
train acc:  0.7421875
train loss:  0.47119617462158203
train gradient:  0.14002345577286568
iteration : 367
train acc:  0.78125
train loss:  0.4415998160839081
train gradient:  0.10611265652533192
iteration : 368
train acc:  0.8125
train loss:  0.3986452519893646
train gradient:  0.08440111126561285
iteration : 369
train acc:  0.734375
train loss:  0.5351518392562866
train gradient:  0.13132087258411312
iteration : 370
train acc:  0.765625
train loss:  0.40761685371398926
train gradient:  0.10982046485467122
iteration : 371
train acc:  0.765625
train loss:  0.4658946990966797
train gradient:  0.132679699187717
iteration : 372
train acc:  0.7578125
train loss:  0.4728979766368866
train gradient:  0.1452093788817
iteration : 373
train acc:  0.765625
train loss:  0.4501081705093384
train gradient:  0.11893701074632566
iteration : 374
train acc:  0.7734375
train loss:  0.47564199566841125
train gradient:  0.14051536245838397
iteration : 375
train acc:  0.7421875
train loss:  0.5398370027542114
train gradient:  0.15487689801662552
iteration : 376
train acc:  0.703125
train loss:  0.48952528834342957
train gradient:  0.16177242110105955
iteration : 377
train acc:  0.796875
train loss:  0.40531405806541443
train gradient:  0.09964498518792206
iteration : 378
train acc:  0.765625
train loss:  0.4530622959136963
train gradient:  0.09598468132096549
iteration : 379
train acc:  0.734375
train loss:  0.5340808629989624
train gradient:  0.16030527648963933
iteration : 380
train acc:  0.765625
train loss:  0.46306005120277405
train gradient:  0.11089417109463218
iteration : 381
train acc:  0.7421875
train loss:  0.4263385534286499
train gradient:  0.09102875991213887
iteration : 382
train acc:  0.75
train loss:  0.49209359288215637
train gradient:  0.11771463745511401
iteration : 383
train acc:  0.734375
train loss:  0.4979199767112732
train gradient:  0.13234528288401323
iteration : 384
train acc:  0.7578125
train loss:  0.4427560567855835
train gradient:  0.12186680307238612
iteration : 385
train acc:  0.71875
train loss:  0.5082342624664307
train gradient:  0.1308675798412186
iteration : 386
train acc:  0.7265625
train loss:  0.5162168145179749
train gradient:  0.1618812580082754
iteration : 387
train acc:  0.734375
train loss:  0.536222517490387
train gradient:  0.14003729994118563
iteration : 388
train acc:  0.8046875
train loss:  0.4869145154953003
train gradient:  0.12813117632384474
iteration : 389
train acc:  0.7890625
train loss:  0.4427221417427063
train gradient:  0.1059590265344014
iteration : 390
train acc:  0.7421875
train loss:  0.48952192068099976
train gradient:  0.11872910969381029
iteration : 391
train acc:  0.6328125
train loss:  0.6290807723999023
train gradient:  0.20815226012894897
iteration : 392
train acc:  0.75
train loss:  0.46296292543411255
train gradient:  0.08913957443785198
iteration : 393
train acc:  0.7421875
train loss:  0.4760274291038513
train gradient:  0.10839546670229239
iteration : 394
train acc:  0.8515625
train loss:  0.3933858573436737
train gradient:  0.09057005434574299
iteration : 395
train acc:  0.7421875
train loss:  0.4982970952987671
train gradient:  0.1340855709991454
iteration : 396
train acc:  0.8046875
train loss:  0.48234832286834717
train gradient:  0.11695104285380335
iteration : 397
train acc:  0.734375
train loss:  0.525054395198822
train gradient:  0.13363538461581925
iteration : 398
train acc:  0.7109375
train loss:  0.5007485151290894
train gradient:  0.10310363768038092
iteration : 399
train acc:  0.7578125
train loss:  0.4620574116706848
train gradient:  0.11307783731117509
iteration : 400
train acc:  0.765625
train loss:  0.4755920171737671
train gradient:  0.1069969501066816
iteration : 401
train acc:  0.7109375
train loss:  0.5779812335968018
train gradient:  0.18748283857017534
iteration : 402
train acc:  0.7265625
train loss:  0.48525673151016235
train gradient:  0.14764687262573878
iteration : 403
train acc:  0.84375
train loss:  0.3862411081790924
train gradient:  0.08486393490475792
iteration : 404
train acc:  0.6640625
train loss:  0.6058773994445801
train gradient:  0.15729908259295364
iteration : 405
train acc:  0.7109375
train loss:  0.4898608922958374
train gradient:  0.11428400172463407
iteration : 406
train acc:  0.7265625
train loss:  0.5230592489242554
train gradient:  0.14665464937295555
iteration : 407
train acc:  0.71875
train loss:  0.5329843759536743
train gradient:  0.1449320160459554
iteration : 408
train acc:  0.75
train loss:  0.45447343587875366
train gradient:  0.11884035315305512
iteration : 409
train acc:  0.734375
train loss:  0.5215796232223511
train gradient:  0.13323869033992022
iteration : 410
train acc:  0.75
train loss:  0.5371503829956055
train gradient:  0.1834722209205591
iteration : 411
train acc:  0.7421875
train loss:  0.4696905016899109
train gradient:  0.11224325994767499
iteration : 412
train acc:  0.7421875
train loss:  0.5159199237823486
train gradient:  0.13272553785164834
iteration : 413
train acc:  0.71875
train loss:  0.4816025197505951
train gradient:  0.11290026301007793
iteration : 414
train acc:  0.75
train loss:  0.47215625643730164
train gradient:  0.13956432283066875
iteration : 415
train acc:  0.8203125
train loss:  0.4010281264781952
train gradient:  0.09128133720136829
iteration : 416
train acc:  0.7109375
train loss:  0.5275510549545288
train gradient:  0.14225512177030114
iteration : 417
train acc:  0.8203125
train loss:  0.39288967847824097
train gradient:  0.07536930401925586
iteration : 418
train acc:  0.78125
train loss:  0.46301889419555664
train gradient:  0.1464047418111507
iteration : 419
train acc:  0.7578125
train loss:  0.45460447669029236
train gradient:  0.1000280120205035
iteration : 420
train acc:  0.8046875
train loss:  0.4063692092895508
train gradient:  0.10143456056769937
iteration : 421
train acc:  0.6640625
train loss:  0.5595471858978271
train gradient:  0.14851479095207226
iteration : 422
train acc:  0.71875
train loss:  0.5094510316848755
train gradient:  0.1379841637361103
iteration : 423
train acc:  0.7109375
train loss:  0.528603732585907
train gradient:  0.1549725463769806
iteration : 424
train acc:  0.75
train loss:  0.529248833656311
train gradient:  0.13753249209665463
iteration : 425
train acc:  0.7890625
train loss:  0.4085540175437927
train gradient:  0.08720877245358237
iteration : 426
train acc:  0.7421875
train loss:  0.5287585258483887
train gradient:  0.12015116351880513
iteration : 427
train acc:  0.78125
train loss:  0.45260876417160034
train gradient:  0.10833788202135748
iteration : 428
train acc:  0.7734375
train loss:  0.5193545818328857
train gradient:  0.14558916310033976
iteration : 429
train acc:  0.796875
train loss:  0.4689326286315918
train gradient:  0.10288386439660888
iteration : 430
train acc:  0.6796875
train loss:  0.6213550567626953
train gradient:  0.19482902161799495
iteration : 431
train acc:  0.7890625
train loss:  0.39303338527679443
train gradient:  0.09705896222905482
iteration : 432
train acc:  0.734375
train loss:  0.48330676555633545
train gradient:  0.1566373454820104
iteration : 433
train acc:  0.71875
train loss:  0.545063853263855
train gradient:  0.12845895670001808
iteration : 434
train acc:  0.71875
train loss:  0.5031168460845947
train gradient:  0.11143598177093919
iteration : 435
train acc:  0.765625
train loss:  0.4892329275608063
train gradient:  0.1266471466978677
iteration : 436
train acc:  0.734375
train loss:  0.5294582843780518
train gradient:  0.13763527308740314
iteration : 437
train acc:  0.7421875
train loss:  0.46759214997291565
train gradient:  0.1280371216564214
iteration : 438
train acc:  0.7421875
train loss:  0.5219739079475403
train gradient:  0.13239504731370466
iteration : 439
train acc:  0.7578125
train loss:  0.47664499282836914
train gradient:  0.12379003460761535
iteration : 440
train acc:  0.7421875
train loss:  0.4124400019645691
train gradient:  0.0842831904409461
iteration : 441
train acc:  0.7890625
train loss:  0.44288408756256104
train gradient:  0.14893357545275154
iteration : 442
train acc:  0.8125
train loss:  0.3903042674064636
train gradient:  0.08047150738141036
iteration : 443
train acc:  0.703125
train loss:  0.4919842779636383
train gradient:  0.1042510301439932
iteration : 444
train acc:  0.7421875
train loss:  0.5331270098686218
train gradient:  0.14824315160040935
iteration : 445
train acc:  0.7265625
train loss:  0.4991823136806488
train gradient:  0.11562473382682095
iteration : 446
train acc:  0.6875
train loss:  0.5810302495956421
train gradient:  0.18057683720200368
iteration : 447
train acc:  0.7734375
train loss:  0.41442304849624634
train gradient:  0.09300592822164142
iteration : 448
train acc:  0.75
train loss:  0.4792230427265167
train gradient:  0.10740372513596118
iteration : 449
train acc:  0.7265625
train loss:  0.5125705599784851
train gradient:  0.12464585986191003
iteration : 450
train acc:  0.7734375
train loss:  0.4335843026638031
train gradient:  0.10341366977091672
iteration : 451
train acc:  0.7578125
train loss:  0.4339102506637573
train gradient:  0.09240449064104787
iteration : 452
train acc:  0.6953125
train loss:  0.5784369707107544
train gradient:  0.1558995088118974
iteration : 453
train acc:  0.7734375
train loss:  0.4402323365211487
train gradient:  0.11004721124213471
iteration : 454
train acc:  0.78125
train loss:  0.4703807830810547
train gradient:  0.10119583002909767
iteration : 455
train acc:  0.7265625
train loss:  0.5013455748558044
train gradient:  0.12312009636555597
iteration : 456
train acc:  0.7890625
train loss:  0.4081457853317261
train gradient:  0.10249428137642655
iteration : 457
train acc:  0.828125
train loss:  0.3770996928215027
train gradient:  0.09361953439143098
iteration : 458
train acc:  0.7421875
train loss:  0.4529086649417877
train gradient:  0.11341619315438682
iteration : 459
train acc:  0.734375
train loss:  0.4955524504184723
train gradient:  0.11909479861963906
iteration : 460
train acc:  0.7265625
train loss:  0.5283774137496948
train gradient:  0.12643625529637043
iteration : 461
train acc:  0.765625
train loss:  0.46977007389068604
train gradient:  0.10218398547117358
iteration : 462
train acc:  0.734375
train loss:  0.5264211893081665
train gradient:  0.20269326051476075
iteration : 463
train acc:  0.7265625
train loss:  0.48661017417907715
train gradient:  0.15472105160808014
iteration : 464
train acc:  0.765625
train loss:  0.5153170228004456
train gradient:  0.1497552719595973
iteration : 465
train acc:  0.75
train loss:  0.5074619054794312
train gradient:  0.11889612216869765
iteration : 466
train acc:  0.7109375
train loss:  0.5239195823669434
train gradient:  0.1328493016746146
iteration : 467
train acc:  0.7734375
train loss:  0.4436781704425812
train gradient:  0.11275951775341915
iteration : 468
train acc:  0.796875
train loss:  0.46245115995407104
train gradient:  0.12438326978073135
iteration : 469
train acc:  0.765625
train loss:  0.440004825592041
train gradient:  0.08859561609115005
iteration : 470
train acc:  0.7734375
train loss:  0.4326837956905365
train gradient:  0.15633218169227375
iteration : 471
train acc:  0.765625
train loss:  0.4616093635559082
train gradient:  0.11113754377394834
iteration : 472
train acc:  0.703125
train loss:  0.5750032663345337
train gradient:  0.16498512934684267
iteration : 473
train acc:  0.7421875
train loss:  0.4573833644390106
train gradient:  0.10091239962335463
iteration : 474
train acc:  0.765625
train loss:  0.445957750082016
train gradient:  0.11395773296988203
iteration : 475
train acc:  0.796875
train loss:  0.43010029196739197
train gradient:  0.08955962541718611
iteration : 476
train acc:  0.7890625
train loss:  0.4366513192653656
train gradient:  0.10286410826625564
iteration : 477
train acc:  0.7265625
train loss:  0.501427173614502
train gradient:  0.12290312030195437
iteration : 478
train acc:  0.7265625
train loss:  0.5030744075775146
train gradient:  0.12600071794173276
iteration : 479
train acc:  0.78125
train loss:  0.46832263469696045
train gradient:  0.12600633053578592
iteration : 480
train acc:  0.8125
train loss:  0.4205840229988098
train gradient:  0.1005962580809587
iteration : 481
train acc:  0.7578125
train loss:  0.4359607696533203
train gradient:  0.10128674631975451
iteration : 482
train acc:  0.7734375
train loss:  0.5034902095794678
train gradient:  0.12228221808093964
iteration : 483
train acc:  0.7734375
train loss:  0.4210206866264343
train gradient:  0.0946857499013722
iteration : 484
train acc:  0.7421875
train loss:  0.5024726390838623
train gradient:  0.12969765816677004
iteration : 485
train acc:  0.7265625
train loss:  0.588603138923645
train gradient:  0.1401059991145223
iteration : 486
train acc:  0.7265625
train loss:  0.4755708873271942
train gradient:  0.10362036227376102
iteration : 487
train acc:  0.765625
train loss:  0.46521109342575073
train gradient:  0.10943893303868973
iteration : 488
train acc:  0.7890625
train loss:  0.403866708278656
train gradient:  0.10112246749584275
iteration : 489
train acc:  0.71875
train loss:  0.5365649461746216
train gradient:  0.13503776689372263
iteration : 490
train acc:  0.734375
train loss:  0.44987472891807556
train gradient:  0.10515171623077522
iteration : 491
train acc:  0.78125
train loss:  0.3855745792388916
train gradient:  0.07787752220456982
iteration : 492
train acc:  0.7421875
train loss:  0.4560545086860657
train gradient:  0.106768296186896
iteration : 493
train acc:  0.7421875
train loss:  0.4918213188648224
train gradient:  0.11103739219143166
iteration : 494
train acc:  0.7265625
train loss:  0.5036527514457703
train gradient:  0.13844485392957967
iteration : 495
train acc:  0.7265625
train loss:  0.47484251856803894
train gradient:  0.11868184145539354
iteration : 496
train acc:  0.7109375
train loss:  0.4844582676887512
train gradient:  0.1302455923266872
iteration : 497
train acc:  0.75
train loss:  0.5083367228507996
train gradient:  0.11783969620173364
iteration : 498
train acc:  0.765625
train loss:  0.44108060002326965
train gradient:  0.10960612127482061
iteration : 499
train acc:  0.765625
train loss:  0.5330878496170044
train gradient:  0.15025265472450677
iteration : 500
train acc:  0.703125
train loss:  0.5023666620254517
train gradient:  0.13916444019660754
iteration : 501
train acc:  0.671875
train loss:  0.6021828651428223
train gradient:  0.1837563327943017
iteration : 502
train acc:  0.765625
train loss:  0.4053267240524292
train gradient:  0.08403100197470352
iteration : 503
train acc:  0.7421875
train loss:  0.5206854939460754
train gradient:  0.12479199210253841
iteration : 504
train acc:  0.7109375
train loss:  0.4939608573913574
train gradient:  0.13646215568796294
iteration : 505
train acc:  0.765625
train loss:  0.4967481195926666
train gradient:  0.14839150795845485
iteration : 506
train acc:  0.796875
train loss:  0.439169704914093
train gradient:  0.09494311822553447
iteration : 507
train acc:  0.7265625
train loss:  0.4988425672054291
train gradient:  0.1324286905359216
iteration : 508
train acc:  0.765625
train loss:  0.4767349064350128
train gradient:  0.10306314281338803
iteration : 509
train acc:  0.7890625
train loss:  0.45709505677223206
train gradient:  0.11156938139168462
iteration : 510
train acc:  0.765625
train loss:  0.43971705436706543
train gradient:  0.11327064490495452
iteration : 511
train acc:  0.78125
train loss:  0.4751690924167633
train gradient:  0.11781709557117885
iteration : 512
train acc:  0.71875
train loss:  0.5542787909507751
train gradient:  0.15899041493152616
iteration : 513
train acc:  0.6953125
train loss:  0.5434117913246155
train gradient:  0.14463245573681624
iteration : 514
train acc:  0.7109375
train loss:  0.5133848786354065
train gradient:  0.10719065360278796
iteration : 515
train acc:  0.734375
train loss:  0.5097837448120117
train gradient:  0.11311428394080876
iteration : 516
train acc:  0.7109375
train loss:  0.5066328644752502
train gradient:  0.14046320985900912
iteration : 517
train acc:  0.71875
train loss:  0.5084419250488281
train gradient:  0.1169323604589942
iteration : 518
train acc:  0.6796875
train loss:  0.5237550139427185
train gradient:  0.18402724878259885
iteration : 519
train acc:  0.75
train loss:  0.4363318383693695
train gradient:  0.10640424225296734
iteration : 520
train acc:  0.828125
train loss:  0.36189889907836914
train gradient:  0.08365277739518061
iteration : 521
train acc:  0.703125
train loss:  0.5689225196838379
train gradient:  0.19057602513069288
iteration : 522
train acc:  0.7265625
train loss:  0.5130760669708252
train gradient:  0.13068156690029825
iteration : 523
train acc:  0.78125
train loss:  0.4510796070098877
train gradient:  0.11037475348500286
iteration : 524
train acc:  0.796875
train loss:  0.4584157168865204
train gradient:  0.1181265871236528
iteration : 525
train acc:  0.7421875
train loss:  0.5245078206062317
train gradient:  0.15078921708424137
iteration : 526
train acc:  0.71875
train loss:  0.5028527975082397
train gradient:  0.14925802719452774
iteration : 527
train acc:  0.765625
train loss:  0.47154533863067627
train gradient:  0.11462675009051443
iteration : 528
train acc:  0.7578125
train loss:  0.44136112928390503
train gradient:  0.1145346125963244
iteration : 529
train acc:  0.78125
train loss:  0.46660879254341125
train gradient:  0.10303849600917907
iteration : 530
train acc:  0.7734375
train loss:  0.41128262877464294
train gradient:  0.09257750313684909
iteration : 531
train acc:  0.65625
train loss:  0.5413566827774048
train gradient:  0.12361848429501675
iteration : 532
train acc:  0.8046875
train loss:  0.4290340542793274
train gradient:  0.12166023571893436
iteration : 533
train acc:  0.765625
train loss:  0.4858345687389374
train gradient:  0.1694142432193574
iteration : 534
train acc:  0.796875
train loss:  0.48879456520080566
train gradient:  0.1321331970240059
iteration : 535
train acc:  0.765625
train loss:  0.4660607576370239
train gradient:  0.11263758558598158
iteration : 536
train acc:  0.71875
train loss:  0.47759491205215454
train gradient:  0.11718463331067987
iteration : 537
train acc:  0.6953125
train loss:  0.5556342601776123
train gradient:  0.12810397149814717
iteration : 538
train acc:  0.703125
train loss:  0.49399420619010925
train gradient:  0.12855857900892756
iteration : 539
train acc:  0.6796875
train loss:  0.5608035326004028
train gradient:  0.16236264013098614
iteration : 540
train acc:  0.75
train loss:  0.5030596256256104
train gradient:  0.12444650133527854
iteration : 541
train acc:  0.71875
train loss:  0.45566993951797485
train gradient:  0.11314067503888972
iteration : 542
train acc:  0.84375
train loss:  0.3807607889175415
train gradient:  0.08237199397637442
iteration : 543
train acc:  0.7265625
train loss:  0.5266842246055603
train gradient:  0.15696899840202272
iteration : 544
train acc:  0.71875
train loss:  0.4818193316459656
train gradient:  0.11660573927835413
iteration : 545
train acc:  0.765625
train loss:  0.4642969071865082
train gradient:  0.1042530425131841
iteration : 546
train acc:  0.796875
train loss:  0.4264034032821655
train gradient:  0.0989653711755898
iteration : 547
train acc:  0.7265625
train loss:  0.5215792655944824
train gradient:  0.14169472962548368
iteration : 548
train acc:  0.71875
train loss:  0.5364139080047607
train gradient:  0.16961318300391587
iteration : 549
train acc:  0.765625
train loss:  0.47062644362449646
train gradient:  0.1109217688040602
iteration : 550
train acc:  0.7578125
train loss:  0.49700480699539185
train gradient:  0.08802989967505655
iteration : 551
train acc:  0.7578125
train loss:  0.49564969539642334
train gradient:  0.1503792218855101
iteration : 552
train acc:  0.671875
train loss:  0.5389307737350464
train gradient:  0.1594766092508531
iteration : 553
train acc:  0.796875
train loss:  0.43761682510375977
train gradient:  0.09222233460267963
iteration : 554
train acc:  0.8046875
train loss:  0.4001885950565338
train gradient:  0.11106887381072543
iteration : 555
train acc:  0.7734375
train loss:  0.5127042531967163
train gradient:  0.1624787961287001
iteration : 556
train acc:  0.7578125
train loss:  0.44958773255348206
train gradient:  0.11416111922718136
iteration : 557
train acc:  0.6875
train loss:  0.5861083269119263
train gradient:  0.16426566216481828
iteration : 558
train acc:  0.7109375
train loss:  0.48769325017929077
train gradient:  0.1139330321330778
iteration : 559
train acc:  0.828125
train loss:  0.39319518208503723
train gradient:  0.08794699874289298
iteration : 560
train acc:  0.7421875
train loss:  0.5020520687103271
train gradient:  0.14806028723952053
iteration : 561
train acc:  0.8046875
train loss:  0.4507810175418854
train gradient:  0.10477079560606554
iteration : 562
train acc:  0.6875
train loss:  0.5099290013313293
train gradient:  0.14698890592144792
iteration : 563
train acc:  0.765625
train loss:  0.4573654532432556
train gradient:  0.12361092625474589
iteration : 564
train acc:  0.7578125
train loss:  0.44912299513816833
train gradient:  0.09836691900856658
iteration : 565
train acc:  0.7578125
train loss:  0.4793263077735901
train gradient:  0.14207656374047278
iteration : 566
train acc:  0.6953125
train loss:  0.5552728176116943
train gradient:  0.1580624773844424
iteration : 567
train acc:  0.7265625
train loss:  0.49090754985809326
train gradient:  0.1168965424232242
iteration : 568
train acc:  0.734375
train loss:  0.5300483107566833
train gradient:  0.13846122534476146
iteration : 569
train acc:  0.7734375
train loss:  0.520504891872406
train gradient:  0.1401808089166809
iteration : 570
train acc:  0.703125
train loss:  0.5884279608726501
train gradient:  0.1872195310961362
iteration : 571
train acc:  0.765625
train loss:  0.4472813010215759
train gradient:  0.0973423977630705
iteration : 572
train acc:  0.65625
train loss:  0.50559401512146
train gradient:  0.1156261566675083
iteration : 573
train acc:  0.7578125
train loss:  0.4829119145870209
train gradient:  0.1106054703737362
iteration : 574
train acc:  0.7421875
train loss:  0.45085835456848145
train gradient:  0.09486886629563435
iteration : 575
train acc:  0.796875
train loss:  0.3974958062171936
train gradient:  0.09302928067177912
iteration : 576
train acc:  0.7890625
train loss:  0.4379865229129791
train gradient:  0.1291106331403909
iteration : 577
train acc:  0.7734375
train loss:  0.4968832731246948
train gradient:  0.12586176587185025
iteration : 578
train acc:  0.8359375
train loss:  0.3792283236980438
train gradient:  0.08014698915606987
iteration : 579
train acc:  0.78125
train loss:  0.4292706847190857
train gradient:  0.10214390954125341
iteration : 580
train acc:  0.7421875
train loss:  0.47264695167541504
train gradient:  0.10391441466872481
iteration : 581
train acc:  0.8046875
train loss:  0.411227285861969
train gradient:  0.08628761749430029
iteration : 582
train acc:  0.7265625
train loss:  0.5311697125434875
train gradient:  0.149508393848802
iteration : 583
train acc:  0.7890625
train loss:  0.38370802998542786
train gradient:  0.08016755505661491
iteration : 584
train acc:  0.65625
train loss:  0.6434681415557861
train gradient:  0.21777953106514586
iteration : 585
train acc:  0.828125
train loss:  0.3990384340286255
train gradient:  0.09206577606503202
iteration : 586
train acc:  0.6484375
train loss:  0.5731908082962036
train gradient:  0.17317929694929196
iteration : 587
train acc:  0.71875
train loss:  0.4978070855140686
train gradient:  0.12107810564683831
iteration : 588
train acc:  0.7421875
train loss:  0.503495991230011
train gradient:  0.12736542508140605
iteration : 589
train acc:  0.796875
train loss:  0.4435136914253235
train gradient:  0.11203165194296227
iteration : 590
train acc:  0.8125
train loss:  0.4405142366886139
train gradient:  0.08646943333225897
iteration : 591
train acc:  0.703125
train loss:  0.5268435478210449
train gradient:  0.1502466022759149
iteration : 592
train acc:  0.7578125
train loss:  0.5324184894561768
train gradient:  0.12964373197729137
iteration : 593
train acc:  0.7421875
train loss:  0.5271886587142944
train gradient:  0.19307948244126602
iteration : 594
train acc:  0.7421875
train loss:  0.46564793586730957
train gradient:  0.18366423124399464
iteration : 595
train acc:  0.7890625
train loss:  0.4946466386318207
train gradient:  0.13487582732758707
iteration : 596
train acc:  0.84375
train loss:  0.3761937618255615
train gradient:  0.10625674902060546
iteration : 597
train acc:  0.6953125
train loss:  0.5037111043930054
train gradient:  0.10661399912863999
iteration : 598
train acc:  0.765625
train loss:  0.44230473041534424
train gradient:  0.12029390098773506
iteration : 599
train acc:  0.7578125
train loss:  0.44416189193725586
train gradient:  0.12031801755721958
iteration : 600
train acc:  0.7421875
train loss:  0.4880312979221344
train gradient:  0.11717973160212967
iteration : 601
train acc:  0.75
train loss:  0.4644894599914551
train gradient:  0.11341799808204439
iteration : 602
train acc:  0.703125
train loss:  0.5472204089164734
train gradient:  0.20453851959047548
iteration : 603
train acc:  0.765625
train loss:  0.44256407022476196
train gradient:  0.11219065866396277
iteration : 604
train acc:  0.75
train loss:  0.487944632768631
train gradient:  0.11491491455778444
iteration : 605
train acc:  0.703125
train loss:  0.5420944690704346
train gradient:  0.17592219558559863
iteration : 606
train acc:  0.7734375
train loss:  0.46584612131118774
train gradient:  0.117269095236638
iteration : 607
train acc:  0.7578125
train loss:  0.4527141749858856
train gradient:  0.10142175752846404
iteration : 608
train acc:  0.7578125
train loss:  0.4288892149925232
train gradient:  0.10287444794857425
iteration : 609
train acc:  0.7421875
train loss:  0.46411561965942383
train gradient:  0.13780988755958892
iteration : 610
train acc:  0.8515625
train loss:  0.3812487721443176
train gradient:  0.0870386606285413
iteration : 611
train acc:  0.6953125
train loss:  0.5981685519218445
train gradient:  0.19115170819526164
iteration : 612
train acc:  0.71875
train loss:  0.5772253274917603
train gradient:  0.20093544811436664
iteration : 613
train acc:  0.75
train loss:  0.5052461624145508
train gradient:  0.1250980986614038
iteration : 614
train acc:  0.734375
train loss:  0.5690478086471558
train gradient:  0.1519600359778353
iteration : 615
train acc:  0.71875
train loss:  0.4884864091873169
train gradient:  0.14630723370289128
iteration : 616
train acc:  0.8125
train loss:  0.465061217546463
train gradient:  0.13244214050116818
iteration : 617
train acc:  0.7578125
train loss:  0.466999351978302
train gradient:  0.09416296682272918
iteration : 618
train acc:  0.7890625
train loss:  0.4683811068534851
train gradient:  0.09928132508708863
iteration : 619
train acc:  0.7578125
train loss:  0.4432829022407532
train gradient:  0.0789123417306294
iteration : 620
train acc:  0.71875
train loss:  0.5363106727600098
train gradient:  0.1416471763540274
iteration : 621
train acc:  0.75
train loss:  0.4978368878364563
train gradient:  0.10935363023985725
iteration : 622
train acc:  0.7578125
train loss:  0.43804556131362915
train gradient:  0.11059416470949517
iteration : 623
train acc:  0.6875
train loss:  0.49986034631729126
train gradient:  0.1267770298109529
iteration : 624
train acc:  0.7578125
train loss:  0.45744043588638306
train gradient:  0.09731759059348549
iteration : 625
train acc:  0.7890625
train loss:  0.42873144149780273
train gradient:  0.101144575986293
iteration : 626
train acc:  0.796875
train loss:  0.42125439643859863
train gradient:  0.096658657782393
iteration : 627
train acc:  0.734375
train loss:  0.5011156797409058
train gradient:  0.1212627738485042
iteration : 628
train acc:  0.765625
train loss:  0.4436817169189453
train gradient:  0.09328730129874496
iteration : 629
train acc:  0.75
train loss:  0.5064921379089355
train gradient:  0.1289809492095426
iteration : 630
train acc:  0.734375
train loss:  0.4937759339809418
train gradient:  0.10965611604475961
iteration : 631
train acc:  0.7265625
train loss:  0.47893840074539185
train gradient:  0.1019241085849201
iteration : 632
train acc:  0.8125
train loss:  0.40708112716674805
train gradient:  0.10480749185385607
iteration : 633
train acc:  0.734375
train loss:  0.5321879386901855
train gradient:  0.15568245494518224
iteration : 634
train acc:  0.734375
train loss:  0.5287972688674927
train gradient:  0.11221478034153336
iteration : 635
train acc:  0.7578125
train loss:  0.50682532787323
train gradient:  0.24320230386401887
iteration : 636
train acc:  0.8125
train loss:  0.4172145426273346
train gradient:  0.09907176657480972
iteration : 637
train acc:  0.765625
train loss:  0.450417160987854
train gradient:  0.09888037694919671
iteration : 638
train acc:  0.78125
train loss:  0.41264182329177856
train gradient:  0.07761437123772373
iteration : 639
train acc:  0.828125
train loss:  0.4297870993614197
train gradient:  0.09871988595697406
iteration : 640
train acc:  0.7109375
train loss:  0.5080462694168091
train gradient:  0.1252878715965353
iteration : 641
train acc:  0.75
train loss:  0.48054707050323486
train gradient:  0.10452476517699484
iteration : 642
train acc:  0.6875
train loss:  0.5146716833114624
train gradient:  0.1296564044362189
iteration : 643
train acc:  0.7109375
train loss:  0.4996584355831146
train gradient:  0.10685986323228132
iteration : 644
train acc:  0.8046875
train loss:  0.40870165824890137
train gradient:  0.09902461988306453
iteration : 645
train acc:  0.703125
train loss:  0.5582853555679321
train gradient:  0.1684906240887427
iteration : 646
train acc:  0.7578125
train loss:  0.46110421419143677
train gradient:  0.11684960377340016
iteration : 647
train acc:  0.71875
train loss:  0.5162006616592407
train gradient:  0.1288914811323375
iteration : 648
train acc:  0.7578125
train loss:  0.46075934171676636
train gradient:  0.10980964948715101
iteration : 649
train acc:  0.734375
train loss:  0.48265427350997925
train gradient:  0.11078763295718846
iteration : 650
train acc:  0.7734375
train loss:  0.5061895847320557
train gradient:  0.12533426262172817
iteration : 651
train acc:  0.78125
train loss:  0.47581371665000916
train gradient:  0.10557384331061638
iteration : 652
train acc:  0.7265625
train loss:  0.5054678916931152
train gradient:  0.12043406766267203
iteration : 653
train acc:  0.7578125
train loss:  0.4801981449127197
train gradient:  0.11315036942246004
iteration : 654
train acc:  0.7421875
train loss:  0.5386263728141785
train gradient:  0.16404086425464964
iteration : 655
train acc:  0.8203125
train loss:  0.4354937970638275
train gradient:  0.10430749160651137
iteration : 656
train acc:  0.6875
train loss:  0.5267255306243896
train gradient:  0.1462279827798351
iteration : 657
train acc:  0.75
train loss:  0.49446865916252136
train gradient:  0.12205857654177389
iteration : 658
train acc:  0.8046875
train loss:  0.45222440361976624
train gradient:  0.11051941662185603
iteration : 659
train acc:  0.75
train loss:  0.4674036502838135
train gradient:  0.12608477640248833
iteration : 660
train acc:  0.75
train loss:  0.4790980815887451
train gradient:  0.13743197748769234
iteration : 661
train acc:  0.78125
train loss:  0.5007566809654236
train gradient:  0.15014298528109632
iteration : 662
train acc:  0.765625
train loss:  0.44419988989830017
train gradient:  0.10151995571304766
iteration : 663
train acc:  0.765625
train loss:  0.46166062355041504
train gradient:  0.10839046854320146
iteration : 664
train acc:  0.7578125
train loss:  0.4702378511428833
train gradient:  0.14111956527100333
iteration : 665
train acc:  0.71875
train loss:  0.4998117685317993
train gradient:  0.1526286876080401
iteration : 666
train acc:  0.7578125
train loss:  0.5006161332130432
train gradient:  0.10962502364037775
iteration : 667
train acc:  0.78125
train loss:  0.4423374533653259
train gradient:  0.09994392251681188
iteration : 668
train acc:  0.7578125
train loss:  0.48847922682762146
train gradient:  0.1049040299794393
iteration : 669
train acc:  0.7578125
train loss:  0.5056226849555969
train gradient:  0.1316440155618487
iteration : 670
train acc:  0.71875
train loss:  0.5028107166290283
train gradient:  0.13357011912747957
iteration : 671
train acc:  0.8671875
train loss:  0.3942737877368927
train gradient:  0.09526910803813128
iteration : 672
train acc:  0.7734375
train loss:  0.45450353622436523
train gradient:  0.11767784378441591
iteration : 673
train acc:  0.796875
train loss:  0.47173547744750977
train gradient:  0.10316370452763736
iteration : 674
train acc:  0.7578125
train loss:  0.48602598905563354
train gradient:  0.1325197526742306
iteration : 675
train acc:  0.7265625
train loss:  0.5149890184402466
train gradient:  0.12066165657928742
iteration : 676
train acc:  0.765625
train loss:  0.46772897243499756
train gradient:  0.11565734232362239
iteration : 677
train acc:  0.765625
train loss:  0.48927104473114014
train gradient:  0.10162746319369341
iteration : 678
train acc:  0.703125
train loss:  0.5511430501937866
train gradient:  0.1562499646112675
iteration : 679
train acc:  0.765625
train loss:  0.48053476214408875
train gradient:  0.11131108208000066
iteration : 680
train acc:  0.734375
train loss:  0.4856305718421936
train gradient:  0.12273654301396959
iteration : 681
train acc:  0.7265625
train loss:  0.5072311758995056
train gradient:  0.1514403721811171
iteration : 682
train acc:  0.7421875
train loss:  0.49613454937934875
train gradient:  0.16366230269942533
iteration : 683
train acc:  0.7890625
train loss:  0.47030141949653625
train gradient:  0.11447695916566956
iteration : 684
train acc:  0.71875
train loss:  0.5029118657112122
train gradient:  0.13851087669427803
iteration : 685
train acc:  0.765625
train loss:  0.4327769875526428
train gradient:  0.1061674320882504
iteration : 686
train acc:  0.8125
train loss:  0.4287939667701721
train gradient:  0.11644193230499886
iteration : 687
train acc:  0.7734375
train loss:  0.4200941324234009
train gradient:  0.07175121148124772
iteration : 688
train acc:  0.6796875
train loss:  0.5483859777450562
train gradient:  0.15180737292234187
iteration : 689
train acc:  0.734375
train loss:  0.5197681188583374
train gradient:  0.12718845592732997
iteration : 690
train acc:  0.7265625
train loss:  0.5251199007034302
train gradient:  0.15118651177047449
iteration : 691
train acc:  0.71875
train loss:  0.5157108306884766
train gradient:  0.12769454433697314
iteration : 692
train acc:  0.734375
train loss:  0.5146569609642029
train gradient:  0.1377198662364353
iteration : 693
train acc:  0.7421875
train loss:  0.4834832549095154
train gradient:  0.13318171953276814
iteration : 694
train acc:  0.765625
train loss:  0.45084089040756226
train gradient:  0.10257472798280164
iteration : 695
train acc:  0.71875
train loss:  0.523999810218811
train gradient:  0.12689822988156318
iteration : 696
train acc:  0.703125
train loss:  0.5426268577575684
train gradient:  0.13503965497224107
iteration : 697
train acc:  0.7734375
train loss:  0.4775182008743286
train gradient:  0.10381885249233716
iteration : 698
train acc:  0.7421875
train loss:  0.49113011360168457
train gradient:  0.1730763783695478
iteration : 699
train acc:  0.734375
train loss:  0.4260936975479126
train gradient:  0.11512525236599173
iteration : 700
train acc:  0.75
train loss:  0.44372302293777466
train gradient:  0.11765375199813372
iteration : 701
train acc:  0.6953125
train loss:  0.4829418957233429
train gradient:  0.11300705511813218
iteration : 702
train acc:  0.7578125
train loss:  0.5051240921020508
train gradient:  0.13186779387642222
iteration : 703
train acc:  0.8203125
train loss:  0.3883976340293884
train gradient:  0.08039693070841131
iteration : 704
train acc:  0.75
train loss:  0.4423724412918091
train gradient:  0.12942874452385172
iteration : 705
train acc:  0.75
train loss:  0.4463872015476227
train gradient:  0.11047168897236427
iteration : 706
train acc:  0.7421875
train loss:  0.49209314584732056
train gradient:  0.13630698959669024
iteration : 707
train acc:  0.7109375
train loss:  0.5406558513641357
train gradient:  0.15104257059672077
iteration : 708
train acc:  0.7734375
train loss:  0.4388130307197571
train gradient:  0.10300549558612648
iteration : 709
train acc:  0.7578125
train loss:  0.43775293231010437
train gradient:  0.09318474309539895
iteration : 710
train acc:  0.71875
train loss:  0.47582659125328064
train gradient:  0.11480118314102557
iteration : 711
train acc:  0.734375
train loss:  0.4939073324203491
train gradient:  0.13929264683260556
iteration : 712
train acc:  0.7734375
train loss:  0.5163925290107727
train gradient:  0.16268591495984697
iteration : 713
train acc:  0.734375
train loss:  0.4808867275714874
train gradient:  0.1247144276784494
iteration : 714
train acc:  0.7734375
train loss:  0.42400455474853516
train gradient:  0.0869027328008683
iteration : 715
train acc:  0.78125
train loss:  0.47641196846961975
train gradient:  0.1328807826482971
iteration : 716
train acc:  0.7421875
train loss:  0.47082599997520447
train gradient:  0.14127120449624914
iteration : 717
train acc:  0.7578125
train loss:  0.4531136751174927
train gradient:  0.09219061132312331
iteration : 718
train acc:  0.7734375
train loss:  0.44095832109451294
train gradient:  0.0809514912428595
iteration : 719
train acc:  0.7109375
train loss:  0.5016650557518005
train gradient:  0.14016388715271982
iteration : 720
train acc:  0.703125
train loss:  0.5389145612716675
train gradient:  0.1490268706486254
iteration : 721
train acc:  0.7265625
train loss:  0.5077940225601196
train gradient:  0.16336331805787013
iteration : 722
train acc:  0.8125
train loss:  0.4357949495315552
train gradient:  0.12004550010797799
iteration : 723
train acc:  0.765625
train loss:  0.46791720390319824
train gradient:  0.10064570767839585
iteration : 724
train acc:  0.734375
train loss:  0.47353434562683105
train gradient:  0.09941733378042905
iteration : 725
train acc:  0.71875
train loss:  0.5196026563644409
train gradient:  0.1293476724867543
iteration : 726
train acc:  0.765625
train loss:  0.4919704794883728
train gradient:  0.1354185416465834
iteration : 727
train acc:  0.7734375
train loss:  0.4832196831703186
train gradient:  0.11039450470106549
iteration : 728
train acc:  0.6953125
train loss:  0.5061712861061096
train gradient:  0.17707751976034314
iteration : 729
train acc:  0.7265625
train loss:  0.5292699337005615
train gradient:  0.15003342364520994
iteration : 730
train acc:  0.7578125
train loss:  0.43112897872924805
train gradient:  0.09498297493681541
iteration : 731
train acc:  0.7109375
train loss:  0.5337953567504883
train gradient:  0.12609522687156732
iteration : 732
train acc:  0.78125
train loss:  0.5295900106430054
train gradient:  0.14671459526203687
iteration : 733
train acc:  0.734375
train loss:  0.46125614643096924
train gradient:  0.11910485663178294
iteration : 734
train acc:  0.7734375
train loss:  0.4491370618343353
train gradient:  0.09919081819182729
iteration : 735
train acc:  0.7421875
train loss:  0.44873982667922974
train gradient:  0.10263028226382301
iteration : 736
train acc:  0.7421875
train loss:  0.5180819630622864
train gradient:  0.1267130981507884
iteration : 737
train acc:  0.765625
train loss:  0.46741554141044617
train gradient:  0.12714972156519877
iteration : 738
train acc:  0.765625
train loss:  0.4306882619857788
train gradient:  0.09204649673538266
iteration : 739
train acc:  0.8046875
train loss:  0.410553902387619
train gradient:  0.10022507812417451
iteration : 740
train acc:  0.75
train loss:  0.48929160833358765
train gradient:  0.13514358027373563
iteration : 741
train acc:  0.6796875
train loss:  0.5520752668380737
train gradient:  0.14763362624976412
iteration : 742
train acc:  0.7265625
train loss:  0.5574448108673096
train gradient:  0.1773729575015694
iteration : 743
train acc:  0.71875
train loss:  0.5403769016265869
train gradient:  0.13758294764494966
iteration : 744
train acc:  0.734375
train loss:  0.5003371238708496
train gradient:  0.12176088504607119
iteration : 745
train acc:  0.75
train loss:  0.4727048873901367
train gradient:  0.09881934756697273
iteration : 746
train acc:  0.7265625
train loss:  0.48874038457870483
train gradient:  0.12891925113332048
iteration : 747
train acc:  0.8359375
train loss:  0.4389777183532715
train gradient:  0.15589139346973424
iteration : 748
train acc:  0.828125
train loss:  0.3754764497280121
train gradient:  0.07608561837255469
iteration : 749
train acc:  0.7421875
train loss:  0.48589298129081726
train gradient:  0.10967521798872315
iteration : 750
train acc:  0.7421875
train loss:  0.5168704986572266
train gradient:  0.14373354090764834
iteration : 751
train acc:  0.78125
train loss:  0.4215373694896698
train gradient:  0.09427112707604249
iteration : 752
train acc:  0.78125
train loss:  0.518396258354187
train gradient:  0.11201903072141352
iteration : 753
train acc:  0.71875
train loss:  0.486806720495224
train gradient:  0.11314585595209163
iteration : 754
train acc:  0.75
train loss:  0.4697974622249603
train gradient:  0.12795219761441703
iteration : 755
train acc:  0.7109375
train loss:  0.4894740581512451
train gradient:  0.10568532941121811
iteration : 756
train acc:  0.7734375
train loss:  0.46724849939346313
train gradient:  0.1157149988762491
iteration : 757
train acc:  0.7578125
train loss:  0.4515484869480133
train gradient:  0.10215774079571315
iteration : 758
train acc:  0.7421875
train loss:  0.46145743131637573
train gradient:  0.10641829299109677
iteration : 759
train acc:  0.7421875
train loss:  0.5193463563919067
train gradient:  0.12763832846883902
iteration : 760
train acc:  0.78125
train loss:  0.4360189735889435
train gradient:  0.10621822899875472
iteration : 761
train acc:  0.8046875
train loss:  0.4232178330421448
train gradient:  0.10918993444967474
iteration : 762
train acc:  0.71875
train loss:  0.4851223826408386
train gradient:  0.12559767468290423
iteration : 763
train acc:  0.7109375
train loss:  0.49673885107040405
train gradient:  0.12050224701140501
iteration : 764
train acc:  0.7890625
train loss:  0.4505464434623718
train gradient:  0.10332141069593287
iteration : 765
train acc:  0.71875
train loss:  0.5324429869651794
train gradient:  0.1535270960499212
iteration : 766
train acc:  0.703125
train loss:  0.5174296498298645
train gradient:  0.15160338340998597
iteration : 767
train acc:  0.7265625
train loss:  0.49241557717323303
train gradient:  0.1521342499277542
iteration : 768
train acc:  0.7265625
train loss:  0.48265954852104187
train gradient:  0.13031544867405598
iteration : 769
train acc:  0.796875
train loss:  0.4254434108734131
train gradient:  0.10359532171282229
iteration : 770
train acc:  0.796875
train loss:  0.43976935744285583
train gradient:  0.09717747763209403
iteration : 771
train acc:  0.796875
train loss:  0.4916391968727112
train gradient:  0.12027187134815101
iteration : 772
train acc:  0.7109375
train loss:  0.49355244636535645
train gradient:  0.12160850516829112
iteration : 773
train acc:  0.7421875
train loss:  0.49490851163864136
train gradient:  0.14293949409555784
iteration : 774
train acc:  0.71875
train loss:  0.5503107309341431
train gradient:  0.17945421303927503
iteration : 775
train acc:  0.6796875
train loss:  0.5690844655036926
train gradient:  0.17725922487698412
iteration : 776
train acc:  0.7421875
train loss:  0.5147276520729065
train gradient:  0.15435098339215342
iteration : 777
train acc:  0.8125
train loss:  0.3815385699272156
train gradient:  0.08202281975064565
iteration : 778
train acc:  0.7109375
train loss:  0.499914288520813
train gradient:  0.11636885149247855
iteration : 779
train acc:  0.7890625
train loss:  0.48626044392585754
train gradient:  0.1140164868954475
iteration : 780
train acc:  0.7265625
train loss:  0.4526181221008301
train gradient:  0.10050218247664688
iteration : 781
train acc:  0.7578125
train loss:  0.5045090913772583
train gradient:  0.1338647691665179
iteration : 782
train acc:  0.7890625
train loss:  0.4232943058013916
train gradient:  0.10876220716298249
iteration : 783
train acc:  0.703125
train loss:  0.5942994952201843
train gradient:  0.15479700800088142
iteration : 784
train acc:  0.7109375
train loss:  0.46797680854797363
train gradient:  0.11250356708082179
iteration : 785
train acc:  0.75
train loss:  0.4809442460536957
train gradient:  0.09845293872346231
iteration : 786
train acc:  0.7421875
train loss:  0.4468495845794678
train gradient:  0.11580987702677029
iteration : 787
train acc:  0.7890625
train loss:  0.4149976670742035
train gradient:  0.09279260666224544
iteration : 788
train acc:  0.734375
train loss:  0.578874945640564
train gradient:  0.16101506252294034
iteration : 789
train acc:  0.71875
train loss:  0.49626243114471436
train gradient:  0.11360574177374186
iteration : 790
train acc:  0.8359375
train loss:  0.43285462260246277
train gradient:  0.09878569382131289
iteration : 791
train acc:  0.75
train loss:  0.4683498740196228
train gradient:  0.10652801277590426
iteration : 792
train acc:  0.7578125
train loss:  0.4536985456943512
train gradient:  0.10106583981105459
iteration : 793
train acc:  0.7265625
train loss:  0.5090638399124146
train gradient:  0.13756763334896172
iteration : 794
train acc:  0.71875
train loss:  0.513825535774231
train gradient:  0.1477818349471698
iteration : 795
train acc:  0.7734375
train loss:  0.44483616948127747
train gradient:  0.08678966594608277
iteration : 796
train acc:  0.7578125
train loss:  0.41698727011680603
train gradient:  0.08666335369684648
iteration : 797
train acc:  0.7578125
train loss:  0.44875985383987427
train gradient:  0.09886597042196027
iteration : 798
train acc:  0.75
train loss:  0.5192208290100098
train gradient:  0.14656306564268912
iteration : 799
train acc:  0.7578125
train loss:  0.44102710485458374
train gradient:  0.09912824303107079
iteration : 800
train acc:  0.7421875
train loss:  0.4779415726661682
train gradient:  0.12139618120703777
iteration : 801
train acc:  0.734375
train loss:  0.4581853747367859
train gradient:  0.1417218341821942
iteration : 802
train acc:  0.84375
train loss:  0.4308445155620575
train gradient:  0.11644697347088863
iteration : 803
train acc:  0.7578125
train loss:  0.49162307381629944
train gradient:  0.1277714842002804
iteration : 804
train acc:  0.7890625
train loss:  0.4612211287021637
train gradient:  0.1005392239567242
iteration : 805
train acc:  0.7265625
train loss:  0.48651647567749023
train gradient:  0.11156394631466504
iteration : 806
train acc:  0.7734375
train loss:  0.4392511248588562
train gradient:  0.11274297466507008
iteration : 807
train acc:  0.75
train loss:  0.548180878162384
train gradient:  0.169240184822013
iteration : 808
train acc:  0.6953125
train loss:  0.5340536832809448
train gradient:  0.17872351535877357
iteration : 809
train acc:  0.734375
train loss:  0.4752713441848755
train gradient:  0.09465690122302234
iteration : 810
train acc:  0.7265625
train loss:  0.46087875962257385
train gradient:  0.10355905853334887
iteration : 811
train acc:  0.7578125
train loss:  0.4642270505428314
train gradient:  0.12894164850706152
iteration : 812
train acc:  0.75
train loss:  0.505919337272644
train gradient:  0.1524942163506972
iteration : 813
train acc:  0.828125
train loss:  0.39618217945098877
train gradient:  0.07747557698754082
iteration : 814
train acc:  0.7734375
train loss:  0.45252102613449097
train gradient:  0.1159730409494863
iteration : 815
train acc:  0.6875
train loss:  0.4826523959636688
train gradient:  0.1498189677689082
iteration : 816
train acc:  0.7578125
train loss:  0.47408464550971985
train gradient:  0.11116371663226988
iteration : 817
train acc:  0.796875
train loss:  0.48562222719192505
train gradient:  0.11425582679288145
iteration : 818
train acc:  0.796875
train loss:  0.45225808024406433
train gradient:  0.11854383327915292
iteration : 819
train acc:  0.7421875
train loss:  0.4774755835533142
train gradient:  0.11965868535717285
iteration : 820
train acc:  0.7734375
train loss:  0.44038262963294983
train gradient:  0.11824594682540367
iteration : 821
train acc:  0.765625
train loss:  0.518768310546875
train gradient:  0.13516082416329994
iteration : 822
train acc:  0.671875
train loss:  0.5386053919792175
train gradient:  0.15283659839312508
iteration : 823
train acc:  0.7734375
train loss:  0.4343281090259552
train gradient:  0.1014816232162421
iteration : 824
train acc:  0.78125
train loss:  0.47856903076171875
train gradient:  0.12856564998450765
iteration : 825
train acc:  0.6796875
train loss:  0.5570091009140015
train gradient:  0.15043539794210192
iteration : 826
train acc:  0.734375
train loss:  0.490564227104187
train gradient:  0.11155048891776075
iteration : 827
train acc:  0.765625
train loss:  0.5074114799499512
train gradient:  0.13633218223267174
iteration : 828
train acc:  0.7421875
train loss:  0.5835869312286377
train gradient:  0.18473294518998556
iteration : 829
train acc:  0.734375
train loss:  0.47500938177108765
train gradient:  0.11236793575446094
iteration : 830
train acc:  0.6953125
train loss:  0.4884432554244995
train gradient:  0.1410838574384294
iteration : 831
train acc:  0.7265625
train loss:  0.47578057646751404
train gradient:  0.09446647183335168
iteration : 832
train acc:  0.7890625
train loss:  0.4393489956855774
train gradient:  0.1289573969309733
iteration : 833
train acc:  0.8046875
train loss:  0.455550879240036
train gradient:  0.13462085840028037
iteration : 834
train acc:  0.7421875
train loss:  0.5061270594596863
train gradient:  0.1251121383300253
iteration : 835
train acc:  0.7578125
train loss:  0.4352073669433594
train gradient:  0.1021614544882523
iteration : 836
train acc:  0.7734375
train loss:  0.511972188949585
train gradient:  0.12503253883060728
iteration : 837
train acc:  0.75
train loss:  0.48002302646636963
train gradient:  0.13452098978023086
iteration : 838
train acc:  0.7578125
train loss:  0.44474464654922485
train gradient:  0.08497390974974124
iteration : 839
train acc:  0.8046875
train loss:  0.4239717423915863
train gradient:  0.10791537082220058
iteration : 840
train acc:  0.7421875
train loss:  0.4447816014289856
train gradient:  0.10949850749857003
iteration : 841
train acc:  0.71875
train loss:  0.5462024211883545
train gradient:  0.1898844682320811
iteration : 842
train acc:  0.7734375
train loss:  0.4282965362071991
train gradient:  0.09201853129164356
iteration : 843
train acc:  0.71875
train loss:  0.5236172676086426
train gradient:  0.1654868516260803
iteration : 844
train acc:  0.71875
train loss:  0.4898066520690918
train gradient:  0.12857646012292467
iteration : 845
train acc:  0.75
train loss:  0.46337801218032837
train gradient:  0.11609073085660698
iteration : 846
train acc:  0.7578125
train loss:  0.4384281039237976
train gradient:  0.1419516149824086
iteration : 847
train acc:  0.8125
train loss:  0.4302842617034912
train gradient:  0.11032962170155435
iteration : 848
train acc:  0.796875
train loss:  0.4245518445968628
train gradient:  0.1009889395691527
iteration : 849
train acc:  0.78125
train loss:  0.4830712080001831
train gradient:  0.12512267464727883
iteration : 850
train acc:  0.6953125
train loss:  0.5499460697174072
train gradient:  0.16939211840199758
iteration : 851
train acc:  0.765625
train loss:  0.4236510992050171
train gradient:  0.09948676540070475
iteration : 852
train acc:  0.75
train loss:  0.48699671030044556
train gradient:  0.1165264363683122
iteration : 853
train acc:  0.828125
train loss:  0.4508417248725891
train gradient:  0.16350316788780378
iteration : 854
train acc:  0.6640625
train loss:  0.5867602229118347
train gradient:  0.17292634566676127
iteration : 855
train acc:  0.7109375
train loss:  0.5545048713684082
train gradient:  0.1566585570537105
iteration : 856
train acc:  0.6796875
train loss:  0.5421128869056702
train gradient:  0.15460790852242665
iteration : 857
train acc:  0.78125
train loss:  0.44757339358329773
train gradient:  0.10926533091869788
iteration : 858
train acc:  0.7578125
train loss:  0.4506238102912903
train gradient:  0.09644007254795173
iteration : 859
train acc:  0.6796875
train loss:  0.5354870557785034
train gradient:  0.143350457340652
iteration : 860
train acc:  0.7890625
train loss:  0.44699376821517944
train gradient:  0.11383573377525033
iteration : 861
train acc:  0.6875
train loss:  0.5984902381896973
train gradient:  0.17340428383722495
iteration : 862
train acc:  0.7578125
train loss:  0.4605237543582916
train gradient:  0.12631722440844012
iteration : 863
train acc:  0.75
train loss:  0.44378992915153503
train gradient:  0.12563039565469897
iteration : 864
train acc:  0.7109375
train loss:  0.5092912912368774
train gradient:  0.14404772171908842
iteration : 865
train acc:  0.78125
train loss:  0.4178203046321869
train gradient:  0.10204991229518791
iteration : 866
train acc:  0.734375
train loss:  0.5164631605148315
train gradient:  0.13582457733919864
iteration : 867
train acc:  0.796875
train loss:  0.42351391911506653
train gradient:  0.10732770174883763
iteration : 868
train acc:  0.7265625
train loss:  0.49293452501296997
train gradient:  0.135838187856683
iteration : 869
train acc:  0.7265625
train loss:  0.4478019177913666
train gradient:  0.10839414470655208
iteration : 870
train acc:  0.796875
train loss:  0.39551258087158203
train gradient:  0.07764824202535428
iteration : 871
train acc:  0.734375
train loss:  0.5067363381385803
train gradient:  0.0989429449461029
iteration : 872
train acc:  0.734375
train loss:  0.5218214988708496
train gradient:  0.13696788903858315
iteration : 873
train acc:  0.734375
train loss:  0.5377868413925171
train gradient:  0.15739624979002034
iteration : 874
train acc:  0.7734375
train loss:  0.4495868384838104
train gradient:  0.11368101749039317
iteration : 875
train acc:  0.6796875
train loss:  0.5230870246887207
train gradient:  0.11229082643177499
iteration : 876
train acc:  0.765625
train loss:  0.42610034346580505
train gradient:  0.08763970978769645
iteration : 877
train acc:  0.71875
train loss:  0.5125514268875122
train gradient:  0.12898858487365222
iteration : 878
train acc:  0.7421875
train loss:  0.460074782371521
train gradient:  0.1179864713457808
iteration : 879
train acc:  0.7109375
train loss:  0.5111445188522339
train gradient:  0.1518677720326478
iteration : 880
train acc:  0.7421875
train loss:  0.48566576838493347
train gradient:  0.12093961306470832
iteration : 881
train acc:  0.765625
train loss:  0.46013516187667847
train gradient:  0.10829236210972433
iteration : 882
train acc:  0.71875
train loss:  0.476895272731781
train gradient:  0.13430642051897457
iteration : 883
train acc:  0.7421875
train loss:  0.5566114187240601
train gradient:  0.17784567622011804
iteration : 884
train acc:  0.6640625
train loss:  0.5542500019073486
train gradient:  0.15456546037874502
iteration : 885
train acc:  0.75
train loss:  0.47085797786712646
train gradient:  0.10795299338689493
iteration : 886
train acc:  0.828125
train loss:  0.3886640965938568
train gradient:  0.09287956967947905
iteration : 887
train acc:  0.71875
train loss:  0.4781465530395508
train gradient:  0.12780841151088196
iteration : 888
train acc:  0.78125
train loss:  0.48835325241088867
train gradient:  0.12840775132315108
iteration : 889
train acc:  0.7578125
train loss:  0.506780743598938
train gradient:  0.14312153076096698
iteration : 890
train acc:  0.796875
train loss:  0.39164096117019653
train gradient:  0.09257914606213201
iteration : 891
train acc:  0.7265625
train loss:  0.5075052976608276
train gradient:  0.18844687629745727
iteration : 892
train acc:  0.796875
train loss:  0.4049742817878723
train gradient:  0.11620009978561828
iteration : 893
train acc:  0.796875
train loss:  0.4229894280433655
train gradient:  0.1231455996944672
iteration : 894
train acc:  0.734375
train loss:  0.5149257183074951
train gradient:  0.12524880627341178
iteration : 895
train acc:  0.8203125
train loss:  0.4112582206726074
train gradient:  0.08606058912498592
iteration : 896
train acc:  0.71875
train loss:  0.5442665815353394
train gradient:  0.14359438101314187
iteration : 897
train acc:  0.6953125
train loss:  0.5509948134422302
train gradient:  0.1378173417722432
iteration : 898
train acc:  0.75
train loss:  0.49286121129989624
train gradient:  0.12167639914380415
iteration : 899
train acc:  0.7734375
train loss:  0.48028671741485596
train gradient:  0.12097288522095574
iteration : 900
train acc:  0.703125
train loss:  0.486017107963562
train gradient:  0.11066097997424953
iteration : 901
train acc:  0.7421875
train loss:  0.48703452944755554
train gradient:  0.11840719692149192
iteration : 902
train acc:  0.765625
train loss:  0.49601200222969055
train gradient:  0.13840728950138345
iteration : 903
train acc:  0.7734375
train loss:  0.46181976795196533
train gradient:  0.10896974502510631
iteration : 904
train acc:  0.703125
train loss:  0.5072859525680542
train gradient:  0.11891730922540664
iteration : 905
train acc:  0.765625
train loss:  0.47496432065963745
train gradient:  0.12049818577771731
iteration : 906
train acc:  0.8046875
train loss:  0.4240388870239258
train gradient:  0.09462659902896987
iteration : 907
train acc:  0.8125
train loss:  0.45681098103523254
train gradient:  0.10104545841350603
iteration : 908
train acc:  0.7265625
train loss:  0.5186576843261719
train gradient:  0.12376312796142054
iteration : 909
train acc:  0.7421875
train loss:  0.5034059286117554
train gradient:  0.15319847847376872
iteration : 910
train acc:  0.8046875
train loss:  0.4422233998775482
train gradient:  0.09770604938116406
iteration : 911
train acc:  0.7265625
train loss:  0.5011171102523804
train gradient:  0.1276658202242091
iteration : 912
train acc:  0.7578125
train loss:  0.4956771731376648
train gradient:  0.15893824767708448
iteration : 913
train acc:  0.75
train loss:  0.510737419128418
train gradient:  0.15156855315669948
iteration : 914
train acc:  0.7109375
train loss:  0.5392611026763916
train gradient:  0.1530562766239683
iteration : 915
train acc:  0.671875
train loss:  0.5717898607254028
train gradient:  0.1535696876290407
iteration : 916
train acc:  0.71875
train loss:  0.47596922516822815
train gradient:  0.10343440470298172
iteration : 917
train acc:  0.7578125
train loss:  0.5535399913787842
train gradient:  0.14568176338701633
iteration : 918
train acc:  0.75
train loss:  0.4849205017089844
train gradient:  0.1591172133052296
iteration : 919
train acc:  0.7890625
train loss:  0.46022528409957886
train gradient:  0.12372339895886142
iteration : 920
train acc:  0.7734375
train loss:  0.47024479508399963
train gradient:  0.12013261474116471
iteration : 921
train acc:  0.71875
train loss:  0.5518902540206909
train gradient:  0.15519049706756655
iteration : 922
train acc:  0.796875
train loss:  0.4312550723552704
train gradient:  0.08660786915118651
iteration : 923
train acc:  0.7421875
train loss:  0.5289244651794434
train gradient:  0.12880044440769012
iteration : 924
train acc:  0.7734375
train loss:  0.49145838618278503
train gradient:  0.12906159409655255
iteration : 925
train acc:  0.7734375
train loss:  0.490328848361969
train gradient:  0.11356108760545525
iteration : 926
train acc:  0.6796875
train loss:  0.5166112184524536
train gradient:  0.13677648273340326
iteration : 927
train acc:  0.7734375
train loss:  0.4697992503643036
train gradient:  0.10368490973505741
iteration : 928
train acc:  0.6953125
train loss:  0.5189276337623596
train gradient:  0.12885385651383435
iteration : 929
train acc:  0.7421875
train loss:  0.47115981578826904
train gradient:  0.13470254801895112
iteration : 930
train acc:  0.7734375
train loss:  0.4407120943069458
train gradient:  0.1289398108271047
iteration : 931
train acc:  0.78125
train loss:  0.4293898344039917
train gradient:  0.08987674674876105
iteration : 932
train acc:  0.75
train loss:  0.49992871284484863
train gradient:  0.1402226090563654
iteration : 933
train acc:  0.8046875
train loss:  0.42046546936035156
train gradient:  0.11424726675338186
iteration : 934
train acc:  0.75
train loss:  0.4582648277282715
train gradient:  0.09171679823055236
iteration : 935
train acc:  0.8203125
train loss:  0.3959652781486511
train gradient:  0.07987950743665889
iteration : 936
train acc:  0.765625
train loss:  0.5021411776542664
train gradient:  0.15705396947433437
iteration : 937
train acc:  0.734375
train loss:  0.4741920828819275
train gradient:  0.10474655213245058
iteration : 938
train acc:  0.75
train loss:  0.4577443301677704
train gradient:  0.10830974348709174
iteration : 939
train acc:  0.6640625
train loss:  0.5731543302536011
train gradient:  0.1580343295424761
iteration : 940
train acc:  0.7421875
train loss:  0.5021987557411194
train gradient:  0.1241859623926213
iteration : 941
train acc:  0.734375
train loss:  0.46560242772102356
train gradient:  0.11579860782498098
iteration : 942
train acc:  0.75
train loss:  0.5087912082672119
train gradient:  0.15140987070051315
iteration : 943
train acc:  0.78125
train loss:  0.4958827495574951
train gradient:  0.1605797866849249
iteration : 944
train acc:  0.7890625
train loss:  0.4307984709739685
train gradient:  0.10618622314169875
iteration : 945
train acc:  0.7578125
train loss:  0.4473012089729309
train gradient:  0.10892211308557863
iteration : 946
train acc:  0.7265625
train loss:  0.4681910276412964
train gradient:  0.12875687848860434
iteration : 947
train acc:  0.765625
train loss:  0.47277987003326416
train gradient:  0.0919051278429996
iteration : 948
train acc:  0.7265625
train loss:  0.5535391569137573
train gradient:  0.17884896855199378
iteration : 949
train acc:  0.8125
train loss:  0.44596344232559204
train gradient:  0.09795803485811058
iteration : 950
train acc:  0.765625
train loss:  0.49580979347229004
train gradient:  0.10174230064860383
iteration : 951
train acc:  0.6875
train loss:  0.5013711452484131
train gradient:  0.11214226522745013
iteration : 952
train acc:  0.7265625
train loss:  0.5147332549095154
train gradient:  0.13132000266744692
iteration : 953
train acc:  0.6015625
train loss:  0.5809574723243713
train gradient:  0.1821582845416842
iteration : 954
train acc:  0.7265625
train loss:  0.5171161890029907
train gradient:  0.12746029690755703
iteration : 955
train acc:  0.7265625
train loss:  0.5250905752182007
train gradient:  0.1440536543672492
iteration : 956
train acc:  0.7421875
train loss:  0.5102928876876831
train gradient:  0.14873865269278902
iteration : 957
train acc:  0.7578125
train loss:  0.48978543281555176
train gradient:  0.11680715167570052
iteration : 958
train acc:  0.7578125
train loss:  0.45338818430900574
train gradient:  0.15294264629755863
iteration : 959
train acc:  0.7265625
train loss:  0.49758103489875793
train gradient:  0.1274682664602523
iteration : 960
train acc:  0.7578125
train loss:  0.4582992494106293
train gradient:  0.1125946208279576
iteration : 961
train acc:  0.78125
train loss:  0.43195945024490356
train gradient:  0.10179026256691542
iteration : 962
train acc:  0.7265625
train loss:  0.5142190456390381
train gradient:  0.14678602274847533
iteration : 963
train acc:  0.734375
train loss:  0.5667219161987305
train gradient:  0.20704625975702567
iteration : 964
train acc:  0.7734375
train loss:  0.4506198465824127
train gradient:  0.0968145000545013
iteration : 965
train acc:  0.7265625
train loss:  0.45859724283218384
train gradient:  0.10692984862988235
iteration : 966
train acc:  0.7890625
train loss:  0.4481154978275299
train gradient:  0.12767476981859818
iteration : 967
train acc:  0.7890625
train loss:  0.4901079833507538
train gradient:  0.12861893433468075
iteration : 968
train acc:  0.7265625
train loss:  0.494353711605072
train gradient:  0.13016724326723378
iteration : 969
train acc:  0.734375
train loss:  0.49006426334381104
train gradient:  0.14801169029023448
iteration : 970
train acc:  0.671875
train loss:  0.5427075028419495
train gradient:  0.13216112271090652
iteration : 971
train acc:  0.7265625
train loss:  0.5256587266921997
train gradient:  0.11101444035066918
iteration : 972
train acc:  0.7265625
train loss:  0.5099653005599976
train gradient:  0.13351032818992997
iteration : 973
train acc:  0.78125
train loss:  0.4481041431427002
train gradient:  0.12632630536536993
iteration : 974
train acc:  0.7265625
train loss:  0.505993664264679
train gradient:  0.14023304318453367
iteration : 975
train acc:  0.7578125
train loss:  0.49108603596687317
train gradient:  0.13296371467450552
iteration : 976
train acc:  0.75
train loss:  0.5550719499588013
train gradient:  0.16769133363268052
iteration : 977
train acc:  0.71875
train loss:  0.5004400610923767
train gradient:  0.14514656838941653
iteration : 978
train acc:  0.765625
train loss:  0.48557737469673157
train gradient:  0.11975361099522602
iteration : 979
train acc:  0.71875
train loss:  0.4968262016773224
train gradient:  0.12616187434097093
iteration : 980
train acc:  0.6875
train loss:  0.5746534466743469
train gradient:  0.17644166973148234
iteration : 981
train acc:  0.7578125
train loss:  0.46154773235321045
train gradient:  0.12485766009194699
iteration : 982
train acc:  0.78125
train loss:  0.4637274146080017
train gradient:  0.0937562403173412
iteration : 983
train acc:  0.71875
train loss:  0.5321190357208252
train gradient:  0.13230523890147233
iteration : 984
train acc:  0.8125
train loss:  0.44889146089553833
train gradient:  0.1037677251556645
iteration : 985
train acc:  0.7421875
train loss:  0.48694735765457153
train gradient:  0.11088369232722435
iteration : 986
train acc:  0.71875
train loss:  0.4790315628051758
train gradient:  0.11003670642272978
iteration : 987
train acc:  0.7421875
train loss:  0.4703015685081482
train gradient:  0.10553187472102808
iteration : 988
train acc:  0.7109375
train loss:  0.49350062012672424
train gradient:  0.14698265452741893
iteration : 989
train acc:  0.6953125
train loss:  0.5444210171699524
train gradient:  0.142623667576309
iteration : 990
train acc:  0.7421875
train loss:  0.48726511001586914
train gradient:  0.13081290534935958
iteration : 991
train acc:  0.71875
train loss:  0.49429598450660706
train gradient:  0.13262735987955107
iteration : 992
train acc:  0.75
train loss:  0.5526533126831055
train gradient:  0.15155839845740782
iteration : 993
train acc:  0.7109375
train loss:  0.562002420425415
train gradient:  0.12655465120846174
iteration : 994
train acc:  0.65625
train loss:  0.536675214767456
train gradient:  0.1507651026653234
iteration : 995
train acc:  0.6953125
train loss:  0.5563894510269165
train gradient:  0.13839353597573348
iteration : 996
train acc:  0.7578125
train loss:  0.4888577163219452
train gradient:  0.12916189446776932
iteration : 997
train acc:  0.7265625
train loss:  0.4904659390449524
train gradient:  0.13235266906474746
iteration : 998
train acc:  0.75
train loss:  0.4762830436229706
train gradient:  0.106489227204704
iteration : 999
train acc:  0.7734375
train loss:  0.4623600244522095
train gradient:  0.10147615832745732
iteration : 1000
train acc:  0.7265625
train loss:  0.5474700331687927
train gradient:  0.15248863019861503
iteration : 1001
train acc:  0.71875
train loss:  0.4911401569843292
train gradient:  0.10927420747157447
iteration : 1002
train acc:  0.7109375
train loss:  0.5334383845329285
train gradient:  0.13517535005211523
iteration : 1003
train acc:  0.7421875
train loss:  0.49667319655418396
train gradient:  0.11672855598942596
iteration : 1004
train acc:  0.734375
train loss:  0.4455645680427551
train gradient:  0.09346707209556954
iteration : 1005
train acc:  0.71875
train loss:  0.5326653718948364
train gradient:  0.12186117570360781
iteration : 1006
train acc:  0.7265625
train loss:  0.47396862506866455
train gradient:  0.12677650457503997
iteration : 1007
train acc:  0.7109375
train loss:  0.542796790599823
train gradient:  0.1706189624488698
iteration : 1008
train acc:  0.7109375
train loss:  0.5249629020690918
train gradient:  0.15224196370619603
iteration : 1009
train acc:  0.734375
train loss:  0.5256310105323792
train gradient:  0.14570358839543845
iteration : 1010
train acc:  0.7890625
train loss:  0.44576501846313477
train gradient:  0.0943841749668894
iteration : 1011
train acc:  0.765625
train loss:  0.4564209580421448
train gradient:  0.11389987117090049
iteration : 1012
train acc:  0.8046875
train loss:  0.4390223026275635
train gradient:  0.07622469145836235
iteration : 1013
train acc:  0.7109375
train loss:  0.47330760955810547
train gradient:  0.11782726442928987
iteration : 1014
train acc:  0.7734375
train loss:  0.5061867833137512
train gradient:  0.13290367455570196
iteration : 1015
train acc:  0.7578125
train loss:  0.46095001697540283
train gradient:  0.08967011392435073
iteration : 1016
train acc:  0.734375
train loss:  0.47308439016342163
train gradient:  0.09769126541340521
iteration : 1017
train acc:  0.734375
train loss:  0.5387909412384033
train gradient:  0.11733192082799787
iteration : 1018
train acc:  0.7265625
train loss:  0.44409698247909546
train gradient:  0.10558789670982843
iteration : 1019
train acc:  0.7265625
train loss:  0.5107333660125732
train gradient:  0.11014019963983307
iteration : 1020
train acc:  0.7890625
train loss:  0.44381773471832275
train gradient:  0.10800365444618985
iteration : 1021
train acc:  0.7734375
train loss:  0.4797755479812622
train gradient:  0.10148391715290077
iteration : 1022
train acc:  0.71875
train loss:  0.5266550779342651
train gradient:  0.13596764261782607
iteration : 1023
train acc:  0.8203125
train loss:  0.40202558040618896
train gradient:  0.0946931961272474
iteration : 1024
train acc:  0.765625
train loss:  0.4957045316696167
train gradient:  0.11565738466022807
iteration : 1025
train acc:  0.7890625
train loss:  0.49589666724205017
train gradient:  0.1330779820934205
iteration : 1026
train acc:  0.765625
train loss:  0.4426441490650177
train gradient:  0.08407314536575516
iteration : 1027
train acc:  0.7421875
train loss:  0.47760140895843506
train gradient:  0.10644781571342744
iteration : 1028
train acc:  0.75
train loss:  0.4790632426738739
train gradient:  0.11435657822047166
iteration : 1029
train acc:  0.734375
train loss:  0.4782469868659973
train gradient:  0.08625323804960107
iteration : 1030
train acc:  0.75
train loss:  0.4562346041202545
train gradient:  0.10666659672572286
iteration : 1031
train acc:  0.71875
train loss:  0.4641633629798889
train gradient:  0.09020228038992753
iteration : 1032
train acc:  0.7265625
train loss:  0.5257725715637207
train gradient:  0.14441985662110707
iteration : 1033
train acc:  0.71875
train loss:  0.5119434595108032
train gradient:  0.13673519831760272
iteration : 1034
train acc:  0.7265625
train loss:  0.5146141052246094
train gradient:  0.1434245905378072
iteration : 1035
train acc:  0.703125
train loss:  0.5507014393806458
train gradient:  0.1346137653232073
iteration : 1036
train acc:  0.7890625
train loss:  0.4430103302001953
train gradient:  0.11075183581919945
iteration : 1037
train acc:  0.8125
train loss:  0.3989955186843872
train gradient:  0.090260968152576
iteration : 1038
train acc:  0.8046875
train loss:  0.4718446433544159
train gradient:  0.11023849100030318
iteration : 1039
train acc:  0.7890625
train loss:  0.4437412619590759
train gradient:  0.09769946357633613
iteration : 1040
train acc:  0.84375
train loss:  0.4601095914840698
train gradient:  0.11238518272054059
iteration : 1041
train acc:  0.765625
train loss:  0.43890103697776794
train gradient:  0.12044349799331419
iteration : 1042
train acc:  0.78125
train loss:  0.4108203053474426
train gradient:  0.0985455262407513
iteration : 1043
train acc:  0.78125
train loss:  0.4853174686431885
train gradient:  0.12928621827301817
iteration : 1044
train acc:  0.7578125
train loss:  0.47127532958984375
train gradient:  0.1347225829723422
iteration : 1045
train acc:  0.703125
train loss:  0.5067476034164429
train gradient:  0.13508362554766934
iteration : 1046
train acc:  0.796875
train loss:  0.48724496364593506
train gradient:  0.11828729398354434
iteration : 1047
train acc:  0.75
train loss:  0.5040318965911865
train gradient:  0.11765272336190877
iteration : 1048
train acc:  0.6796875
train loss:  0.5118816494941711
train gradient:  0.12613383344683382
iteration : 1049
train acc:  0.7265625
train loss:  0.4927368760108948
train gradient:  0.11954373504071689
iteration : 1050
train acc:  0.6796875
train loss:  0.5457920432090759
train gradient:  0.14043890358023775
iteration : 1051
train acc:  0.734375
train loss:  0.5045677423477173
train gradient:  0.15169761451920327
iteration : 1052
train acc:  0.7578125
train loss:  0.5170707702636719
train gradient:  0.1469537264741399
iteration : 1053
train acc:  0.734375
train loss:  0.5166214108467102
train gradient:  0.14728500567211614
iteration : 1054
train acc:  0.671875
train loss:  0.5308009386062622
train gradient:  0.1406618538972792
iteration : 1055
train acc:  0.7734375
train loss:  0.4402349293231964
train gradient:  0.08965675830492896
iteration : 1056
train acc:  0.8125
train loss:  0.4261467158794403
train gradient:  0.09789262770156015
iteration : 1057
train acc:  0.703125
train loss:  0.5055955648422241
train gradient:  0.1257727293004488
iteration : 1058
train acc:  0.765625
train loss:  0.475788950920105
train gradient:  0.1268029042588647
iteration : 1059
train acc:  0.7890625
train loss:  0.47495755553245544
train gradient:  0.14257021748716414
iteration : 1060
train acc:  0.7109375
train loss:  0.5207276940345764
train gradient:  0.13954125644090587
iteration : 1061
train acc:  0.78125
train loss:  0.45527899265289307
train gradient:  0.09861723154003577
iteration : 1062
train acc:  0.75
train loss:  0.47453632950782776
train gradient:  0.10177348625469328
iteration : 1063
train acc:  0.7421875
train loss:  0.4955716133117676
train gradient:  0.13667236321768395
iteration : 1064
train acc:  0.734375
train loss:  0.48948657512664795
train gradient:  0.132339074740096
iteration : 1065
train acc:  0.6953125
train loss:  0.6149794459342957
train gradient:  0.1496663309371694
iteration : 1066
train acc:  0.7578125
train loss:  0.4868990480899811
train gradient:  0.12040318442699036
iteration : 1067
train acc:  0.6875
train loss:  0.5298278331756592
train gradient:  0.13963461973351038
iteration : 1068
train acc:  0.7734375
train loss:  0.43093135952949524
train gradient:  0.11521231006876767
iteration : 1069
train acc:  0.7734375
train loss:  0.4610154628753662
train gradient:  0.10654547515139912
iteration : 1070
train acc:  0.7890625
train loss:  0.47095414996147156
train gradient:  0.09291377361919337
iteration : 1071
train acc:  0.7109375
train loss:  0.5132291316986084
train gradient:  0.1435161451630035
iteration : 1072
train acc:  0.71875
train loss:  0.5250596404075623
train gradient:  0.13566069384885043
iteration : 1073
train acc:  0.7265625
train loss:  0.5511506199836731
train gradient:  0.14647710567623567
iteration : 1074
train acc:  0.703125
train loss:  0.5109353065490723
train gradient:  0.10663909112380164
iteration : 1075
train acc:  0.796875
train loss:  0.46007800102233887
train gradient:  0.11568849383913257
iteration : 1076
train acc:  0.734375
train loss:  0.524982213973999
train gradient:  0.17753607026480783
iteration : 1077
train acc:  0.75
train loss:  0.5165743827819824
train gradient:  0.11699233107667863
iteration : 1078
train acc:  0.6640625
train loss:  0.5651019215583801
train gradient:  0.15619741670511705
iteration : 1079
train acc:  0.7265625
train loss:  0.4767703115940094
train gradient:  0.10901688873401147
iteration : 1080
train acc:  0.7890625
train loss:  0.46742892265319824
train gradient:  0.12949217608067043
iteration : 1081
train acc:  0.6953125
train loss:  0.5549952983856201
train gradient:  0.20053021613810823
iteration : 1082
train acc:  0.765625
train loss:  0.4724322259426117
train gradient:  0.1271974299464727
iteration : 1083
train acc:  0.765625
train loss:  0.46908003091812134
train gradient:  0.10345394572220529
iteration : 1084
train acc:  0.7421875
train loss:  0.5632364153862
train gradient:  0.16162265453896585
iteration : 1085
train acc:  0.7265625
train loss:  0.5052293539047241
train gradient:  0.16794036438909207
iteration : 1086
train acc:  0.7265625
train loss:  0.4496506154537201
train gradient:  0.11787202358492606
iteration : 1087
train acc:  0.6640625
train loss:  0.5638800263404846
train gradient:  0.16346084611876127
iteration : 1088
train acc:  0.8125
train loss:  0.4035051465034485
train gradient:  0.09649184453351954
iteration : 1089
train acc:  0.75
train loss:  0.4878692626953125
train gradient:  0.11020781223801751
iteration : 1090
train acc:  0.765625
train loss:  0.47550833225250244
train gradient:  0.1468151431554981
iteration : 1091
train acc:  0.7890625
train loss:  0.48228734731674194
train gradient:  0.10426106540632334
iteration : 1092
train acc:  0.8125
train loss:  0.4060823917388916
train gradient:  0.08698149073691264
iteration : 1093
train acc:  0.7734375
train loss:  0.4376482367515564
train gradient:  0.1085808117789194
iteration : 1094
train acc:  0.7265625
train loss:  0.4995834529399872
train gradient:  0.1144632618436604
iteration : 1095
train acc:  0.78125
train loss:  0.4360283613204956
train gradient:  0.10254385346989386
iteration : 1096
train acc:  0.7578125
train loss:  0.5063172578811646
train gradient:  0.13601554966160218
iteration : 1097
train acc:  0.78125
train loss:  0.4803610146045685
train gradient:  0.1135001764554768
iteration : 1098
train acc:  0.6640625
train loss:  0.588935375213623
train gradient:  0.2406889143828824
iteration : 1099
train acc:  0.75
train loss:  0.4608730971813202
train gradient:  0.16889220527104792
iteration : 1100
train acc:  0.7421875
train loss:  0.5058380961418152
train gradient:  0.12252832877846827
iteration : 1101
train acc:  0.7421875
train loss:  0.4399069547653198
train gradient:  0.10284251436731974
iteration : 1102
train acc:  0.75
train loss:  0.4741504490375519
train gradient:  0.11178427093796764
iteration : 1103
train acc:  0.765625
train loss:  0.4605542719364166
train gradient:  0.11300394273105276
iteration : 1104
train acc:  0.75
train loss:  0.492171049118042
train gradient:  0.12787963191134577
iteration : 1105
train acc:  0.6796875
train loss:  0.5307538509368896
train gradient:  0.12552207399945092
iteration : 1106
train acc:  0.6875
train loss:  0.518846333026886
train gradient:  0.15635179568301816
iteration : 1107
train acc:  0.7890625
train loss:  0.413232684135437
train gradient:  0.10971088033368187
iteration : 1108
train acc:  0.765625
train loss:  0.4683818519115448
train gradient:  0.12393039313703078
iteration : 1109
train acc:  0.7734375
train loss:  0.45551109313964844
train gradient:  0.10832367373136742
iteration : 1110
train acc:  0.6640625
train loss:  0.5920584201812744
train gradient:  0.15371603794144179
iteration : 1111
train acc:  0.765625
train loss:  0.46605822443962097
train gradient:  0.1043268366990451
iteration : 1112
train acc:  0.71875
train loss:  0.5260853171348572
train gradient:  0.139355374229452
iteration : 1113
train acc:  0.6953125
train loss:  0.5492355227470398
train gradient:  0.14546182208003472
iteration : 1114
train acc:  0.7734375
train loss:  0.49424198269844055
train gradient:  0.12625818650965598
iteration : 1115
train acc:  0.8046875
train loss:  0.4650449752807617
train gradient:  0.1122816076373136
iteration : 1116
train acc:  0.7421875
train loss:  0.4605100154876709
train gradient:  0.0953949313984051
iteration : 1117
train acc:  0.7890625
train loss:  0.4427773356437683
train gradient:  0.08944740368936324
iteration : 1118
train acc:  0.75
train loss:  0.49933451414108276
train gradient:  0.12971796045148093
iteration : 1119
train acc:  0.6875
train loss:  0.5236063003540039
train gradient:  0.13818128490873577
iteration : 1120
train acc:  0.734375
train loss:  0.49069833755493164
train gradient:  0.11630966996191759
iteration : 1121
train acc:  0.71875
train loss:  0.4821162223815918
train gradient:  0.1143049668959962
iteration : 1122
train acc:  0.7578125
train loss:  0.4591814875602722
train gradient:  0.09450232940869899
iteration : 1123
train acc:  0.7578125
train loss:  0.4761323928833008
train gradient:  0.10838981822051601
iteration : 1124
train acc:  0.796875
train loss:  0.4211199879646301
train gradient:  0.12113164612506047
iteration : 1125
train acc:  0.7578125
train loss:  0.46776530146598816
train gradient:  0.11769616759785755
iteration : 1126
train acc:  0.734375
train loss:  0.4895758330821991
train gradient:  0.10334430585100994
iteration : 1127
train acc:  0.7890625
train loss:  0.48520684242248535
train gradient:  0.12111016722584299
iteration : 1128
train acc:  0.7265625
train loss:  0.5724765062332153
train gradient:  0.17391699283982515
iteration : 1129
train acc:  0.78125
train loss:  0.4090103209018707
train gradient:  0.09616924761220835
iteration : 1130
train acc:  0.7109375
train loss:  0.5119131803512573
train gradient:  0.14413101514873566
iteration : 1131
train acc:  0.7578125
train loss:  0.4925803542137146
train gradient:  0.14950545029388335
iteration : 1132
train acc:  0.765625
train loss:  0.49806246161460876
train gradient:  0.14456994277465693
iteration : 1133
train acc:  0.71875
train loss:  0.4626435339450836
train gradient:  0.09943377806930226
iteration : 1134
train acc:  0.7421875
train loss:  0.46798375248908997
train gradient:  0.11685360129592123
iteration : 1135
train acc:  0.75
train loss:  0.4567529261112213
train gradient:  0.08550651398645398
iteration : 1136
train acc:  0.7109375
train loss:  0.5032656788825989
train gradient:  0.11448878114902122
iteration : 1137
train acc:  0.734375
train loss:  0.49100440740585327
train gradient:  0.1387404345384925
iteration : 1138
train acc:  0.7265625
train loss:  0.5220365524291992
train gradient:  0.13012042846954722
iteration : 1139
train acc:  0.7421875
train loss:  0.45925718545913696
train gradient:  0.10542209274911449
iteration : 1140
train acc:  0.6953125
train loss:  0.5258195400238037
train gradient:  0.13065645197196596
iteration : 1141
train acc:  0.8203125
train loss:  0.4697822332382202
train gradient:  0.1447454892476279
iteration : 1142
train acc:  0.71875
train loss:  0.4847736060619354
train gradient:  0.0988554561428767
iteration : 1143
train acc:  0.7421875
train loss:  0.47301721572875977
train gradient:  0.11488077546179537
iteration : 1144
train acc:  0.765625
train loss:  0.45771390199661255
train gradient:  0.10180469767472045
iteration : 1145
train acc:  0.796875
train loss:  0.4575463533401489
train gradient:  0.09314965393062981
iteration : 1146
train acc:  0.7109375
train loss:  0.4980630874633789
train gradient:  0.1365530135576281
iteration : 1147
train acc:  0.71875
train loss:  0.4697095453739166
train gradient:  0.10503492125847225
iteration : 1148
train acc:  0.7578125
train loss:  0.4775424003601074
train gradient:  0.10521042982386034
iteration : 1149
train acc:  0.7109375
train loss:  0.5024136304855347
train gradient:  0.08502867878616165
iteration : 1150
train acc:  0.78125
train loss:  0.47668370604515076
train gradient:  0.09298758926355302
iteration : 1151
train acc:  0.7578125
train loss:  0.5128256678581238
train gradient:  0.13084862702038258
iteration : 1152
train acc:  0.75
train loss:  0.5113593339920044
train gradient:  0.12032922184532294
iteration : 1153
train acc:  0.7578125
train loss:  0.4664551019668579
train gradient:  0.10437780475472096
iteration : 1154
train acc:  0.7578125
train loss:  0.47243645787239075
train gradient:  0.12449148636921341
iteration : 1155
train acc:  0.7109375
train loss:  0.5308069586753845
train gradient:  0.14226813705978814
iteration : 1156
train acc:  0.8046875
train loss:  0.4640868902206421
train gradient:  0.11412481002003948
iteration : 1157
train acc:  0.8125
train loss:  0.40051305294036865
train gradient:  0.0836318311579967
iteration : 1158
train acc:  0.765625
train loss:  0.47970345616340637
train gradient:  0.09722142922342582
iteration : 1159
train acc:  0.7578125
train loss:  0.48880109190940857
train gradient:  0.1120438700767006
iteration : 1160
train acc:  0.765625
train loss:  0.4790032207965851
train gradient:  0.1140315619389672
iteration : 1161
train acc:  0.7734375
train loss:  0.44978809356689453
train gradient:  0.10084658388136811
iteration : 1162
train acc:  0.7109375
train loss:  0.5295168161392212
train gradient:  0.1460193808388306
iteration : 1163
train acc:  0.71875
train loss:  0.46436434984207153
train gradient:  0.11336000409000159
iteration : 1164
train acc:  0.75
train loss:  0.47103244066238403
train gradient:  0.12133036586250733
iteration : 1165
train acc:  0.7734375
train loss:  0.40512335300445557
train gradient:  0.09444723343095285
iteration : 1166
train acc:  0.8125
train loss:  0.44186708331108093
train gradient:  0.08185177360050298
iteration : 1167
train acc:  0.734375
train loss:  0.5095010995864868
train gradient:  0.14669860278934582
iteration : 1168
train acc:  0.7890625
train loss:  0.4480937123298645
train gradient:  0.12925746888102008
iteration : 1169
train acc:  0.8046875
train loss:  0.45301055908203125
train gradient:  0.08608499726864216
iteration : 1170
train acc:  0.734375
train loss:  0.43370747566223145
train gradient:  0.1037651683141813
iteration : 1171
train acc:  0.734375
train loss:  0.4551455080509186
train gradient:  0.11749348247258308
iteration : 1172
train acc:  0.7109375
train loss:  0.5432402491569519
train gradient:  0.1960898674516728
iteration : 1173
train acc:  0.765625
train loss:  0.47715720534324646
train gradient:  0.11062817870436666
iteration : 1174
train acc:  0.7578125
train loss:  0.4475119113922119
train gradient:  0.11710478455906194
iteration : 1175
train acc:  0.8046875
train loss:  0.4359184503555298
train gradient:  0.1020938670069455
iteration : 1176
train acc:  0.7421875
train loss:  0.45638060569763184
train gradient:  0.08542962227413185
iteration : 1177
train acc:  0.7109375
train loss:  0.5401715040206909
train gradient:  0.15431000065474532
iteration : 1178
train acc:  0.7890625
train loss:  0.5001547336578369
train gradient:  0.12377698753483758
iteration : 1179
train acc:  0.6875
train loss:  0.5019574165344238
train gradient:  0.11555428761578022
iteration : 1180
train acc:  0.7734375
train loss:  0.49223411083221436
train gradient:  0.14056396923903752
iteration : 1181
train acc:  0.796875
train loss:  0.4644568860530853
train gradient:  0.12654028728881933
iteration : 1182
train acc:  0.765625
train loss:  0.5140835046768188
train gradient:  0.14294175850179208
iteration : 1183
train acc:  0.78125
train loss:  0.4426255226135254
train gradient:  0.110542932258883
iteration : 1184
train acc:  0.765625
train loss:  0.4442586600780487
train gradient:  0.08207651590411556
iteration : 1185
train acc:  0.71875
train loss:  0.5231945514678955
train gradient:  0.1676967602047712
iteration : 1186
train acc:  0.8203125
train loss:  0.39790141582489014
train gradient:  0.09925878564663308
iteration : 1187
train acc:  0.8046875
train loss:  0.43871545791625977
train gradient:  0.1022968946616286
iteration : 1188
train acc:  0.7109375
train loss:  0.5226869583129883
train gradient:  0.14225592925248248
iteration : 1189
train acc:  0.703125
train loss:  0.5009610056877136
train gradient:  0.1168948789875391
iteration : 1190
train acc:  0.71875
train loss:  0.4467364549636841
train gradient:  0.09393209464979635
iteration : 1191
train acc:  0.7890625
train loss:  0.4382093846797943
train gradient:  0.10504533915195889
iteration : 1192
train acc:  0.75
train loss:  0.48246869444847107
train gradient:  0.11766579201784688
iteration : 1193
train acc:  0.7578125
train loss:  0.4915197491645813
train gradient:  0.1334416190888984
iteration : 1194
train acc:  0.7734375
train loss:  0.47594699263572693
train gradient:  0.10934920035336808
iteration : 1195
train acc:  0.765625
train loss:  0.4654645621776581
train gradient:  0.14936042892379925
iteration : 1196
train acc:  0.8125
train loss:  0.4442842900753021
train gradient:  0.11567746774357048
iteration : 1197
train acc:  0.7578125
train loss:  0.5099663734436035
train gradient:  0.11798705557958283
iteration : 1198
train acc:  0.65625
train loss:  0.5464324951171875
train gradient:  0.17739393329387215
iteration : 1199
train acc:  0.7265625
train loss:  0.5047948360443115
train gradient:  0.1366303794571777
iteration : 1200
train acc:  0.7265625
train loss:  0.5410646796226501
train gradient:  0.21685665464883536
iteration : 1201
train acc:  0.6953125
train loss:  0.538916826248169
train gradient:  0.13973979203165737
iteration : 1202
train acc:  0.6953125
train loss:  0.5612119436264038
train gradient:  0.16889406674337787
iteration : 1203
train acc:  0.78125
train loss:  0.4143601655960083
train gradient:  0.09759131695087744
iteration : 1204
train acc:  0.703125
train loss:  0.4819822609424591
train gradient:  0.1511397286064577
iteration : 1205
train acc:  0.7578125
train loss:  0.44848209619522095
train gradient:  0.1241228034222943
iteration : 1206
train acc:  0.7421875
train loss:  0.4688297510147095
train gradient:  0.11244453920240838
iteration : 1207
train acc:  0.734375
train loss:  0.5326085090637207
train gradient:  0.14382223601351046
iteration : 1208
train acc:  0.7265625
train loss:  0.5727716684341431
train gradient:  0.14430662051036117
iteration : 1209
train acc:  0.796875
train loss:  0.4745206832885742
train gradient:  0.12787829529524683
iteration : 1210
train acc:  0.7265625
train loss:  0.5490522384643555
train gradient:  0.14233491347298793
iteration : 1211
train acc:  0.71875
train loss:  0.5065785050392151
train gradient:  0.09882294818903295
iteration : 1212
train acc:  0.78125
train loss:  0.49092841148376465
train gradient:  0.1357858462442344
iteration : 1213
train acc:  0.765625
train loss:  0.4207126498222351
train gradient:  0.09434921879045673
iteration : 1214
train acc:  0.7578125
train loss:  0.4976769685745239
train gradient:  0.09716073349081172
iteration : 1215
train acc:  0.7578125
train loss:  0.467684268951416
train gradient:  0.09875221364800621
iteration : 1216
train acc:  0.796875
train loss:  0.422941654920578
train gradient:  0.08926232848282173
iteration : 1217
train acc:  0.7578125
train loss:  0.494040310382843
train gradient:  0.13089333586755106
iteration : 1218
train acc:  0.7421875
train loss:  0.48907020688056946
train gradient:  0.1381977752353055
iteration : 1219
train acc:  0.796875
train loss:  0.4503125548362732
train gradient:  0.09040092461583447
iteration : 1220
train acc:  0.71875
train loss:  0.48696720600128174
train gradient:  0.13907019922806957
iteration : 1221
train acc:  0.703125
train loss:  0.553633451461792
train gradient:  0.13902843270120185
iteration : 1222
train acc:  0.78125
train loss:  0.4735954999923706
train gradient:  0.132064660030129
iteration : 1223
train acc:  0.78125
train loss:  0.4325070381164551
train gradient:  0.0852636524280013
iteration : 1224
train acc:  0.734375
train loss:  0.4757826626300812
train gradient:  0.11175617935649204
iteration : 1225
train acc:  0.765625
train loss:  0.5048791766166687
train gradient:  0.13463574959423227
iteration : 1226
train acc:  0.71875
train loss:  0.5368770360946655
train gradient:  0.13618160443805816
iteration : 1227
train acc:  0.71875
train loss:  0.47239741683006287
train gradient:  0.13840524817146144
iteration : 1228
train acc:  0.734375
train loss:  0.48408079147338867
train gradient:  0.0985769627306136
iteration : 1229
train acc:  0.7734375
train loss:  0.4335477650165558
train gradient:  0.10017385532606629
iteration : 1230
train acc:  0.7421875
train loss:  0.469749391078949
train gradient:  0.08789220245451781
iteration : 1231
train acc:  0.7734375
train loss:  0.44956550002098083
train gradient:  0.08811431761240685
iteration : 1232
train acc:  0.6953125
train loss:  0.5789898633956909
train gradient:  0.1884736407971488
iteration : 1233
train acc:  0.765625
train loss:  0.48249518871307373
train gradient:  0.09626736443953837
iteration : 1234
train acc:  0.8046875
train loss:  0.39087343215942383
train gradient:  0.11062385774808296
iteration : 1235
train acc:  0.7265625
train loss:  0.4499098062515259
train gradient:  0.0944731078317365
iteration : 1236
train acc:  0.7578125
train loss:  0.4840664267539978
train gradient:  0.12850373790449515
iteration : 1237
train acc:  0.765625
train loss:  0.46780866384506226
train gradient:  0.1097051757347178
iteration : 1238
train acc:  0.78125
train loss:  0.44038763642311096
train gradient:  0.10348963521550243
iteration : 1239
train acc:  0.796875
train loss:  0.4629237949848175
train gradient:  0.09226569291976877
iteration : 1240
train acc:  0.7109375
train loss:  0.4792644679546356
train gradient:  0.1084068570259852
iteration : 1241
train acc:  0.765625
train loss:  0.4429705739021301
train gradient:  0.09942114644035246
iteration : 1242
train acc:  0.765625
train loss:  0.46967458724975586
train gradient:  0.12232869086222724
iteration : 1243
train acc:  0.7890625
train loss:  0.4157226085662842
train gradient:  0.09923176397581461
iteration : 1244
train acc:  0.7734375
train loss:  0.48235219717025757
train gradient:  0.11873452994260823
iteration : 1245
train acc:  0.75
train loss:  0.47660550475120544
train gradient:  0.11856431449224666
iteration : 1246
train acc:  0.78125
train loss:  0.46099215745925903
train gradient:  0.1373458411958563
iteration : 1247
train acc:  0.8046875
train loss:  0.44263046979904175
train gradient:  0.12976595167533406
iteration : 1248
train acc:  0.8203125
train loss:  0.42122259736061096
train gradient:  0.09448997455402015
iteration : 1249
train acc:  0.7421875
train loss:  0.5366723537445068
train gradient:  0.13653835994851626
iteration : 1250
train acc:  0.75
train loss:  0.4838982820510864
train gradient:  0.11110513241026782
iteration : 1251
train acc:  0.7421875
train loss:  0.4806520342826843
train gradient:  0.12058587710373657
iteration : 1252
train acc:  0.7421875
train loss:  0.4899911880493164
train gradient:  0.11640468391202138
iteration : 1253
train acc:  0.765625
train loss:  0.44011586904525757
train gradient:  0.11125245628671411
iteration : 1254
train acc:  0.7734375
train loss:  0.45976006984710693
train gradient:  0.1254143715399529
iteration : 1255
train acc:  0.7578125
train loss:  0.4678840935230255
train gradient:  0.11589622679727922
iteration : 1256
train acc:  0.828125
train loss:  0.44531503319740295
train gradient:  0.10883107314294532
iteration : 1257
train acc:  0.765625
train loss:  0.4828440845012665
train gradient:  0.12669282699767548
iteration : 1258
train acc:  0.6953125
train loss:  0.5331566333770752
train gradient:  0.11644123546040477
iteration : 1259
train acc:  0.703125
train loss:  0.46643179655075073
train gradient:  0.10900840188859104
iteration : 1260
train acc:  0.78125
train loss:  0.38638418912887573
train gradient:  0.07954552449206038
iteration : 1261
train acc:  0.796875
train loss:  0.40941354632377625
train gradient:  0.09132924860729831
iteration : 1262
train acc:  0.7109375
train loss:  0.5424379110336304
train gradient:  0.14723901013219903
iteration : 1263
train acc:  0.765625
train loss:  0.45692235231399536
train gradient:  0.1019436931703424
iteration : 1264
train acc:  0.8515625
train loss:  0.40948042273521423
train gradient:  0.09412508733330133
iteration : 1265
train acc:  0.78125
train loss:  0.4608822464942932
train gradient:  0.12329773802253206
iteration : 1266
train acc:  0.796875
train loss:  0.4518517851829529
train gradient:  0.11157891673379654
iteration : 1267
train acc:  0.71875
train loss:  0.5076982378959656
train gradient:  0.12919583115615552
iteration : 1268
train acc:  0.7421875
train loss:  0.5051982402801514
train gradient:  0.1336935736739522
iteration : 1269
train acc:  0.765625
train loss:  0.48023897409439087
train gradient:  0.13109761985618523
iteration : 1270
train acc:  0.78125
train loss:  0.466772198677063
train gradient:  0.12871469148534487
iteration : 1271
train acc:  0.7421875
train loss:  0.4866493344306946
train gradient:  0.10252876433522813
iteration : 1272
train acc:  0.6953125
train loss:  0.5240403413772583
train gradient:  0.14806008966578604
iteration : 1273
train acc:  0.7890625
train loss:  0.46549612283706665
train gradient:  0.13746022343882972
iteration : 1274
train acc:  0.6953125
train loss:  0.49827712774276733
train gradient:  0.11650807521765656
iteration : 1275
train acc:  0.828125
train loss:  0.41216152906417847
train gradient:  0.10490665604839075
iteration : 1276
train acc:  0.7578125
train loss:  0.5185258388519287
train gradient:  0.13615194955680793
iteration : 1277
train acc:  0.7109375
train loss:  0.5509511232376099
train gradient:  0.15956489007347252
iteration : 1278
train acc:  0.765625
train loss:  0.44810760021209717
train gradient:  0.10039603955353135
iteration : 1279
train acc:  0.6953125
train loss:  0.4950757324695587
train gradient:  0.08886270203684947
iteration : 1280
train acc:  0.734375
train loss:  0.4698295593261719
train gradient:  0.1102392965999697
iteration : 1281
train acc:  0.7890625
train loss:  0.43364495038986206
train gradient:  0.10426318797139086
iteration : 1282
train acc:  0.75
train loss:  0.4940275549888611
train gradient:  0.09674895930609384
iteration : 1283
train acc:  0.7421875
train loss:  0.4522462785243988
train gradient:  0.08808959145646358
iteration : 1284
train acc:  0.7421875
train loss:  0.4721986651420593
train gradient:  0.11231708968295699
iteration : 1285
train acc:  0.8046875
train loss:  0.3979259729385376
train gradient:  0.08837100435599296
iteration : 1286
train acc:  0.7890625
train loss:  0.4928775429725647
train gradient:  0.1301960935415672
iteration : 1287
train acc:  0.75
train loss:  0.4835388660430908
train gradient:  0.10720330180554542
iteration : 1288
train acc:  0.6875
train loss:  0.5277901291847229
train gradient:  0.12769975646202975
iteration : 1289
train acc:  0.765625
train loss:  0.5281316041946411
train gradient:  0.1308477522967927
iteration : 1290
train acc:  0.7890625
train loss:  0.4563206434249878
train gradient:  0.12191954697310683
iteration : 1291
train acc:  0.7890625
train loss:  0.44114434719085693
train gradient:  0.0910464204659454
iteration : 1292
train acc:  0.6875
train loss:  0.5444374084472656
train gradient:  0.1553105402409249
iteration : 1293
train acc:  0.734375
train loss:  0.4385868310928345
train gradient:  0.08900901795581002
iteration : 1294
train acc:  0.7578125
train loss:  0.4859008491039276
train gradient:  0.11830212388402414
iteration : 1295
train acc:  0.8046875
train loss:  0.4109638035297394
train gradient:  0.09680269420420913
iteration : 1296
train acc:  0.8046875
train loss:  0.4313744008541107
train gradient:  0.102771743886391
iteration : 1297
train acc:  0.71875
train loss:  0.49091586470603943
train gradient:  0.11409584391520221
iteration : 1298
train acc:  0.765625
train loss:  0.5339930653572083
train gradient:  0.14410983882230943
iteration : 1299
train acc:  0.6953125
train loss:  0.5547828674316406
train gradient:  0.1542238609133227
iteration : 1300
train acc:  0.8046875
train loss:  0.4383772611618042
train gradient:  0.12434219919360967
iteration : 1301
train acc:  0.765625
train loss:  0.44900667667388916
train gradient:  0.10196147253627962
iteration : 1302
train acc:  0.78125
train loss:  0.4759824275970459
train gradient:  0.12536464499899092
iteration : 1303
train acc:  0.71875
train loss:  0.5546727776527405
train gradient:  0.1561771243858746
iteration : 1304
train acc:  0.7734375
train loss:  0.45217615365982056
train gradient:  0.1048603823426833
iteration : 1305
train acc:  0.8125
train loss:  0.36285287141799927
train gradient:  0.08557902308913987
iteration : 1306
train acc:  0.8125
train loss:  0.4355700612068176
train gradient:  0.1079369772959714
iteration : 1307
train acc:  0.7265625
train loss:  0.4943356513977051
train gradient:  0.11418893718519336
iteration : 1308
train acc:  0.7578125
train loss:  0.46507155895233154
train gradient:  0.10575201455012109
iteration : 1309
train acc:  0.7109375
train loss:  0.539304792881012
train gradient:  0.16232114215066118
iteration : 1310
train acc:  0.71875
train loss:  0.4850590229034424
train gradient:  0.12873406521862907
iteration : 1311
train acc:  0.78125
train loss:  0.4194090664386749
train gradient:  0.08196232693357049
iteration : 1312
train acc:  0.7890625
train loss:  0.4863775372505188
train gradient:  0.10873110703768714
iteration : 1313
train acc:  0.7265625
train loss:  0.47594189643859863
train gradient:  0.14294712782028834
iteration : 1314
train acc:  0.78125
train loss:  0.4821043610572815
train gradient:  0.10698255377832559
iteration : 1315
train acc:  0.7578125
train loss:  0.4884053170681
train gradient:  0.10524730761415556
iteration : 1316
train acc:  0.7421875
train loss:  0.46478673815727234
train gradient:  0.10653690501091437
iteration : 1317
train acc:  0.7421875
train loss:  0.48491141200065613
train gradient:  0.10885677301656009
iteration : 1318
train acc:  0.7421875
train loss:  0.5373899340629578
train gradient:  0.1304446208077501
iteration : 1319
train acc:  0.7890625
train loss:  0.46382591128349304
train gradient:  0.10698584670379083
iteration : 1320
train acc:  0.7734375
train loss:  0.4808877110481262
train gradient:  0.13030464603489836
iteration : 1321
train acc:  0.765625
train loss:  0.46231794357299805
train gradient:  0.12750897076470277
iteration : 1322
train acc:  0.7890625
train loss:  0.44745469093322754
train gradient:  0.10092186590734788
iteration : 1323
train acc:  0.7265625
train loss:  0.5113809108734131
train gradient:  0.10707388238359886
iteration : 1324
train acc:  0.765625
train loss:  0.524509072303772
train gradient:  0.1330419353637024
iteration : 1325
train acc:  0.7890625
train loss:  0.44712716341018677
train gradient:  0.0984072101714524
iteration : 1326
train acc:  0.78125
train loss:  0.44487321376800537
train gradient:  0.09946306767816565
iteration : 1327
train acc:  0.734375
train loss:  0.4714648127555847
train gradient:  0.1090841292367845
iteration : 1328
train acc:  0.8203125
train loss:  0.37229740619659424
train gradient:  0.06322270757158938
iteration : 1329
train acc:  0.7265625
train loss:  0.47642195224761963
train gradient:  0.1199906336546237
iteration : 1330
train acc:  0.7421875
train loss:  0.480640172958374
train gradient:  0.14112574052252197
iteration : 1331
train acc:  0.6875
train loss:  0.5524778366088867
train gradient:  0.16345951010137028
iteration : 1332
train acc:  0.671875
train loss:  0.5535134077072144
train gradient:  0.17377276723887294
iteration : 1333
train acc:  0.7265625
train loss:  0.4732661545276642
train gradient:  0.11678123976916613
iteration : 1334
train acc:  0.6875
train loss:  0.4816422462463379
train gradient:  0.11225862668250625
iteration : 1335
train acc:  0.734375
train loss:  0.5398110151290894
train gradient:  0.1458978867951196
iteration : 1336
train acc:  0.71875
train loss:  0.5043455362319946
train gradient:  0.13918596634239055
iteration : 1337
train acc:  0.7578125
train loss:  0.4745378792285919
train gradient:  0.11606436620074868
iteration : 1338
train acc:  0.75
train loss:  0.4965941309928894
train gradient:  0.16198730280677862
iteration : 1339
train acc:  0.75
train loss:  0.4452705979347229
train gradient:  0.09378614496964965
iteration : 1340
train acc:  0.8125
train loss:  0.3984266519546509
train gradient:  0.07459829542126235
iteration : 1341
train acc:  0.6875
train loss:  0.5080620050430298
train gradient:  0.1338765383898043
iteration : 1342
train acc:  0.6796875
train loss:  0.6076456308364868
train gradient:  0.2232785741891214
iteration : 1343
train acc:  0.8203125
train loss:  0.4316366910934448
train gradient:  0.12605431971316733
iteration : 1344
train acc:  0.7734375
train loss:  0.44982919096946716
train gradient:  0.1206184168358622
iteration : 1345
train acc:  0.6875
train loss:  0.5292801856994629
train gradient:  0.14856631082878372
iteration : 1346
train acc:  0.6796875
train loss:  0.517303466796875
train gradient:  0.13693485695703572
iteration : 1347
train acc:  0.7890625
train loss:  0.4787023067474365
train gradient:  0.1346368468507364
iteration : 1348
train acc:  0.671875
train loss:  0.5335734486579895
train gradient:  0.16352583608277432
iteration : 1349
train acc:  0.6640625
train loss:  0.6334561109542847
train gradient:  0.16291186206047126
iteration : 1350
train acc:  0.78125
train loss:  0.47380101680755615
train gradient:  0.1246023504615307
iteration : 1351
train acc:  0.703125
train loss:  0.5108016133308411
train gradient:  0.11401767721269795
iteration : 1352
train acc:  0.796875
train loss:  0.42864668369293213
train gradient:  0.100446985055623
iteration : 1353
train acc:  0.703125
train loss:  0.5211439728736877
train gradient:  0.12905376531516066
iteration : 1354
train acc:  0.796875
train loss:  0.4133867025375366
train gradient:  0.08294453086472552
iteration : 1355
train acc:  0.734375
train loss:  0.49317270517349243
train gradient:  0.1312760659548929
iteration : 1356
train acc:  0.796875
train loss:  0.4348783493041992
train gradient:  0.09085114252250842
iteration : 1357
train acc:  0.8125
train loss:  0.41481706500053406
train gradient:  0.11418189417466224
iteration : 1358
train acc:  0.734375
train loss:  0.4918641448020935
train gradient:  0.10660507883658835
iteration : 1359
train acc:  0.7578125
train loss:  0.4728575050830841
train gradient:  0.14587647886343158
iteration : 1360
train acc:  0.6796875
train loss:  0.5272762179374695
train gradient:  0.20781171956949637
iteration : 1361
train acc:  0.765625
train loss:  0.47645798325538635
train gradient:  0.1469592774497958
iteration : 1362
train acc:  0.7421875
train loss:  0.4859105348587036
train gradient:  0.12131183462888573
iteration : 1363
train acc:  0.6640625
train loss:  0.5460912585258484
train gradient:  0.14089762737097405
iteration : 1364
train acc:  0.734375
train loss:  0.4733760356903076
train gradient:  0.11371909209134971
iteration : 1365
train acc:  0.7265625
train loss:  0.5075958967208862
train gradient:  0.13731042166405594
iteration : 1366
train acc:  0.7421875
train loss:  0.45874863862991333
train gradient:  0.10886730756239936
iteration : 1367
train acc:  0.6875
train loss:  0.5580069422721863
train gradient:  0.17253363528387758
iteration : 1368
train acc:  0.734375
train loss:  0.5442109107971191
train gradient:  0.13555453696975478
iteration : 1369
train acc:  0.7734375
train loss:  0.48540568351745605
train gradient:  0.12669289561712085
iteration : 1370
train acc:  0.7734375
train loss:  0.4435391426086426
train gradient:  0.07884174928026781
iteration : 1371
train acc:  0.75
train loss:  0.4403266906738281
train gradient:  0.0946290276287718
iteration : 1372
train acc:  0.7734375
train loss:  0.4806345999240875
train gradient:  0.10770140765435629
iteration : 1373
train acc:  0.734375
train loss:  0.5048795938491821
train gradient:  0.1458337383237348
iteration : 1374
train acc:  0.7109375
train loss:  0.5675511360168457
train gradient:  0.1707908350042209
iteration : 1375
train acc:  0.75
train loss:  0.5189127326011658
train gradient:  0.11559781189782217
iteration : 1376
train acc:  0.78125
train loss:  0.43488913774490356
train gradient:  0.08936386150042486
iteration : 1377
train acc:  0.6875
train loss:  0.5140483975410461
train gradient:  0.13648380236497681
iteration : 1378
train acc:  0.703125
train loss:  0.5154173374176025
train gradient:  0.1378511863650949
iteration : 1379
train acc:  0.734375
train loss:  0.48965728282928467
train gradient:  0.12140454050351682
iteration : 1380
train acc:  0.7890625
train loss:  0.44266292452812195
train gradient:  0.10175530992668849
iteration : 1381
train acc:  0.734375
train loss:  0.4646725058555603
train gradient:  0.12263334308661065
iteration : 1382
train acc:  0.7734375
train loss:  0.49032071232795715
train gradient:  0.1282582698583693
iteration : 1383
train acc:  0.8046875
train loss:  0.44319337606430054
train gradient:  0.1291448328809685
iteration : 1384
train acc:  0.8046875
train loss:  0.4403683543205261
train gradient:  0.1329410165414875
iteration : 1385
train acc:  0.7265625
train loss:  0.4819044768810272
train gradient:  0.12081901829088988
iteration : 1386
train acc:  0.765625
train loss:  0.47862857580184937
train gradient:  0.10488135406062811
iteration : 1387
train acc:  0.7734375
train loss:  0.4658041000366211
train gradient:  0.10515304609222981
iteration : 1388
train acc:  0.7578125
train loss:  0.4581640660762787
train gradient:  0.11494188516147223
iteration : 1389
train acc:  0.75
train loss:  0.47433602809906006
train gradient:  0.11044200299838756
iteration : 1390
train acc:  0.7421875
train loss:  0.5085465312004089
train gradient:  0.12554324024431124
iteration : 1391
train acc:  0.734375
train loss:  0.48729220032691956
train gradient:  0.12511237402013572
iteration : 1392
train acc:  0.796875
train loss:  0.4414371848106384
train gradient:  0.09798455341819959
iteration : 1393
train acc:  0.7265625
train loss:  0.490023136138916
train gradient:  0.1379044770071134
iteration : 1394
train acc:  0.7890625
train loss:  0.39572611451148987
train gradient:  0.08352990487303358
iteration : 1395
train acc:  0.7421875
train loss:  0.4814508855342865
train gradient:  0.1174323121734313
iteration : 1396
train acc:  0.796875
train loss:  0.45003217458724976
train gradient:  0.11128789555355902
iteration : 1397
train acc:  0.7109375
train loss:  0.522068977355957
train gradient:  0.16313199872266665
iteration : 1398
train acc:  0.6953125
train loss:  0.5146262049674988
train gradient:  0.12823165976753786
iteration : 1399
train acc:  0.7109375
train loss:  0.47369420528411865
train gradient:  0.1385962437308722
iteration : 1400
train acc:  0.8046875
train loss:  0.44855380058288574
train gradient:  0.12683999065603443
iteration : 1401
train acc:  0.8046875
train loss:  0.4345305860042572
train gradient:  0.10963463281582055
iteration : 1402
train acc:  0.703125
train loss:  0.5351085066795349
train gradient:  0.1382794524211354
iteration : 1403
train acc:  0.6796875
train loss:  0.5513407588005066
train gradient:  0.146829119466184
iteration : 1404
train acc:  0.7578125
train loss:  0.46319323778152466
train gradient:  0.11928858100358977
iteration : 1405
train acc:  0.6875
train loss:  0.5342464447021484
train gradient:  0.17720205337086414
iteration : 1406
train acc:  0.7578125
train loss:  0.49280768632888794
train gradient:  0.12401101617074217
iteration : 1407
train acc:  0.671875
train loss:  0.5971795320510864
train gradient:  0.15285399510714648
iteration : 1408
train acc:  0.78125
train loss:  0.42664939165115356
train gradient:  0.09245778447025924
iteration : 1409
train acc:  0.703125
train loss:  0.5100771188735962
train gradient:  0.13838824397268235
iteration : 1410
train acc:  0.75
train loss:  0.5096791386604309
train gradient:  0.1446742877023643
iteration : 1411
train acc:  0.7265625
train loss:  0.4636498987674713
train gradient:  0.11771189659350978
iteration : 1412
train acc:  0.8046875
train loss:  0.3986009359359741
train gradient:  0.0833490421928207
iteration : 1413
train acc:  0.7734375
train loss:  0.5141513347625732
train gradient:  0.13461324197123964
iteration : 1414
train acc:  0.75
train loss:  0.5329059362411499
train gradient:  0.1275968105884567
iteration : 1415
train acc:  0.7265625
train loss:  0.524188756942749
train gradient:  0.1372502172406334
iteration : 1416
train acc:  0.7421875
train loss:  0.47097545862197876
train gradient:  0.09621714405221982
iteration : 1417
train acc:  0.734375
train loss:  0.4968024790287018
train gradient:  0.13280562915161265
iteration : 1418
train acc:  0.7578125
train loss:  0.46516910195350647
train gradient:  0.12175065471666734
iteration : 1419
train acc:  0.734375
train loss:  0.5396854877471924
train gradient:  0.11925640083972114
iteration : 1420
train acc:  0.7265625
train loss:  0.4994036555290222
train gradient:  0.13667180290325512
iteration : 1421
train acc:  0.7265625
train loss:  0.4389018416404724
train gradient:  0.10731556359067819
iteration : 1422
train acc:  0.734375
train loss:  0.4857648015022278
train gradient:  0.1591903693075574
iteration : 1423
train acc:  0.7734375
train loss:  0.46608108282089233
train gradient:  0.13041380692349383
iteration : 1424
train acc:  0.7890625
train loss:  0.4326383173465729
train gradient:  0.08725053561362574
iteration : 1425
train acc:  0.734375
train loss:  0.557439923286438
train gradient:  0.13877557165734314
iteration : 1426
train acc:  0.75
train loss:  0.5391975045204163
train gradient:  0.15717054016104315
iteration : 1427
train acc:  0.7578125
train loss:  0.45251691341400146
train gradient:  0.1252141409184213
iteration : 1428
train acc:  0.78125
train loss:  0.4144105315208435
train gradient:  0.09045097773480133
iteration : 1429
train acc:  0.75
train loss:  0.4954496920108795
train gradient:  0.11693185168823261
iteration : 1430
train acc:  0.7109375
train loss:  0.5623346567153931
train gradient:  0.14999364453157404
iteration : 1431
train acc:  0.765625
train loss:  0.4993802309036255
train gradient:  0.1140955255081017
iteration : 1432
train acc:  0.7890625
train loss:  0.3889341354370117
train gradient:  0.09891887012721112
iteration : 1433
train acc:  0.7734375
train loss:  0.45172587037086487
train gradient:  0.10363219956854025
iteration : 1434
train acc:  0.7265625
train loss:  0.48257768154144287
train gradient:  0.11242667917585111
iteration : 1435
train acc:  0.7265625
train loss:  0.5475196838378906
train gradient:  0.13427114096652712
iteration : 1436
train acc:  0.78125
train loss:  0.46039968729019165
train gradient:  0.1053299386083721
iteration : 1437
train acc:  0.765625
train loss:  0.4376249313354492
train gradient:  0.10983836200552967
iteration : 1438
train acc:  0.7578125
train loss:  0.4604113698005676
train gradient:  0.10309515006301449
iteration : 1439
train acc:  0.7109375
train loss:  0.5589725375175476
train gradient:  0.15419337380838916
iteration : 1440
train acc:  0.7734375
train loss:  0.47500935196876526
train gradient:  0.13499075651178555
iteration : 1441
train acc:  0.796875
train loss:  0.47544369101524353
train gradient:  0.10099478548481808
iteration : 1442
train acc:  0.734375
train loss:  0.4808446764945984
train gradient:  0.11648212769933056
iteration : 1443
train acc:  0.7421875
train loss:  0.5602031350135803
train gradient:  0.16185224396227058
iteration : 1444
train acc:  0.6953125
train loss:  0.5071107149124146
train gradient:  0.12294304552181137
iteration : 1445
train acc:  0.65625
train loss:  0.587313175201416
train gradient:  0.173550063126093
iteration : 1446
train acc:  0.765625
train loss:  0.46276798844337463
train gradient:  0.09857794231345218
iteration : 1447
train acc:  0.6796875
train loss:  0.5130678415298462
train gradient:  0.12176134118352283
iteration : 1448
train acc:  0.8046875
train loss:  0.4629978537559509
train gradient:  0.11121343822817441
iteration : 1449
train acc:  0.703125
train loss:  0.4476467967033386
train gradient:  0.08524089581853445
iteration : 1450
train acc:  0.71875
train loss:  0.5305322408676147
train gradient:  0.14605308385065957
iteration : 1451
train acc:  0.703125
train loss:  0.5405914783477783
train gradient:  0.1240515372263001
iteration : 1452
train acc:  0.75
train loss:  0.4949483871459961
train gradient:  0.12838817763920268
iteration : 1453
train acc:  0.7265625
train loss:  0.5095462203025818
train gradient:  0.11839797652586795
iteration : 1454
train acc:  0.7734375
train loss:  0.48065829277038574
train gradient:  0.14978902458974436
iteration : 1455
train acc:  0.828125
train loss:  0.41391611099243164
train gradient:  0.096622116495343
iteration : 1456
train acc:  0.75
train loss:  0.5667802095413208
train gradient:  0.17497906061576368
iteration : 1457
train acc:  0.7421875
train loss:  0.49408233165740967
train gradient:  0.14589090589688922
iteration : 1458
train acc:  0.7109375
train loss:  0.46454018354415894
train gradient:  0.11604457545750602
iteration : 1459
train acc:  0.78125
train loss:  0.43408799171447754
train gradient:  0.1002766701264709
iteration : 1460
train acc:  0.75
train loss:  0.4839020073413849
train gradient:  0.11425347506267432
iteration : 1461
train acc:  0.7890625
train loss:  0.48076313734054565
train gradient:  0.13910596370163392
iteration : 1462
train acc:  0.71875
train loss:  0.48608893156051636
train gradient:  0.131171605159802
iteration : 1463
train acc:  0.6796875
train loss:  0.5274943113327026
train gradient:  0.13444119912454675
iteration : 1464
train acc:  0.734375
train loss:  0.527222216129303
train gradient:  0.12651967514407364
iteration : 1465
train acc:  0.75
train loss:  0.4591253399848938
train gradient:  0.1156089905715828
iteration : 1466
train acc:  0.71875
train loss:  0.5000818967819214
train gradient:  0.10333087788213963
iteration : 1467
train acc:  0.7890625
train loss:  0.42082345485687256
train gradient:  0.10002821932854004
iteration : 1468
train acc:  0.796875
train loss:  0.4405210018157959
train gradient:  0.09524585270303419
iteration : 1469
train acc:  0.703125
train loss:  0.5319187641143799
train gradient:  0.12655387570727064
iteration : 1470
train acc:  0.7890625
train loss:  0.4566496014595032
train gradient:  0.1317445621193926
iteration : 1471
train acc:  0.7734375
train loss:  0.44336998462677
train gradient:  0.09662979396261667
iteration : 1472
train acc:  0.7421875
train loss:  0.5396993160247803
train gradient:  0.14596920711108793
iteration : 1473
train acc:  0.703125
train loss:  0.5352653861045837
train gradient:  0.135059220335434
iteration : 1474
train acc:  0.734375
train loss:  0.5179389715194702
train gradient:  0.15549910246652532
iteration : 1475
train acc:  0.8046875
train loss:  0.4286486506462097
train gradient:  0.1018103193713212
iteration : 1476
train acc:  0.7109375
train loss:  0.5085160136222839
train gradient:  0.11764527744006886
iteration : 1477
train acc:  0.75
train loss:  0.4542083144187927
train gradient:  0.10298936437293055
iteration : 1478
train acc:  0.703125
train loss:  0.5078850984573364
train gradient:  0.12162397452960434
iteration : 1479
train acc:  0.75
train loss:  0.4474918246269226
train gradient:  0.09056760254043505
iteration : 1480
train acc:  0.75
train loss:  0.4798728823661804
train gradient:  0.1207688901122252
iteration : 1481
train acc:  0.7578125
train loss:  0.46520426869392395
train gradient:  0.11374001285998588
iteration : 1482
train acc:  0.734375
train loss:  0.49454835057258606
train gradient:  0.11941497375002283
iteration : 1483
train acc:  0.7421875
train loss:  0.48154354095458984
train gradient:  0.1525762380812286
iteration : 1484
train acc:  0.75
train loss:  0.4871681332588196
train gradient:  0.10405447409975455
iteration : 1485
train acc:  0.7109375
train loss:  0.5591490864753723
train gradient:  0.1957768361677681
iteration : 1486
train acc:  0.796875
train loss:  0.45278358459472656
train gradient:  0.12026234146237982
iteration : 1487
train acc:  0.703125
train loss:  0.5142963528633118
train gradient:  0.1514903497703502
iteration : 1488
train acc:  0.75
train loss:  0.4674358665943146
train gradient:  0.12144808834167843
iteration : 1489
train acc:  0.75
train loss:  0.4498044550418854
train gradient:  0.08772623146404915
iteration : 1490
train acc:  0.7578125
train loss:  0.45800474286079407
train gradient:  0.0949874545931441
iteration : 1491
train acc:  0.7734375
train loss:  0.4489750266075134
train gradient:  0.10747194333749618
iteration : 1492
train acc:  0.78125
train loss:  0.46085265278816223
train gradient:  0.11582064600553729
iteration : 1493
train acc:  0.78125
train loss:  0.4497196078300476
train gradient:  0.1111116627578987
iteration : 1494
train acc:  0.78125
train loss:  0.4623585045337677
train gradient:  0.1176268210200807
iteration : 1495
train acc:  0.8046875
train loss:  0.4253659248352051
train gradient:  0.10914752331884245
iteration : 1496
train acc:  0.75
train loss:  0.48123234510421753
train gradient:  0.10684068927826981
iteration : 1497
train acc:  0.7421875
train loss:  0.48057252168655396
train gradient:  0.11235502266982797
iteration : 1498
train acc:  0.78125
train loss:  0.45728349685668945
train gradient:  0.10256187765310638
iteration : 1499
train acc:  0.765625
train loss:  0.49900737404823303
train gradient:  0.12244406210754559
iteration : 1500
train acc:  0.6953125
train loss:  0.5077369213104248
train gradient:  0.11229992832291684
iteration : 1501
train acc:  0.7578125
train loss:  0.47252899408340454
train gradient:  0.12433121771407663
iteration : 1502
train acc:  0.7578125
train loss:  0.4739722013473511
train gradient:  0.1115949505482691
iteration : 1503
train acc:  0.71875
train loss:  0.4738009572029114
train gradient:  0.09723942401155299
iteration : 1504
train acc:  0.75
train loss:  0.46296507120132446
train gradient:  0.08840672237002406
iteration : 1505
train acc:  0.7578125
train loss:  0.49494025111198425
train gradient:  0.12284901253303732
iteration : 1506
train acc:  0.75
train loss:  0.5391932129859924
train gradient:  0.12232531665499412
iteration : 1507
train acc:  0.78125
train loss:  0.4422748386859894
train gradient:  0.1038872102764593
iteration : 1508
train acc:  0.734375
train loss:  0.5543476343154907
train gradient:  0.14639845803525547
iteration : 1509
train acc:  0.75
train loss:  0.4653759300708771
train gradient:  0.11649157652873486
iteration : 1510
train acc:  0.7578125
train loss:  0.4750732183456421
train gradient:  0.12014931113207175
iteration : 1511
train acc:  0.6640625
train loss:  0.5801995992660522
train gradient:  0.16146024810922704
iteration : 1512
train acc:  0.7734375
train loss:  0.49279990792274475
train gradient:  0.12965421257971044
iteration : 1513
train acc:  0.703125
train loss:  0.4987373948097229
train gradient:  0.14366496826856595
iteration : 1514
train acc:  0.75
train loss:  0.4949805736541748
train gradient:  0.153447937690962
iteration : 1515
train acc:  0.7421875
train loss:  0.5020873546600342
train gradient:  0.13196871156526585
iteration : 1516
train acc:  0.734375
train loss:  0.5458005666732788
train gradient:  0.16684646461379393
iteration : 1517
train acc:  0.765625
train loss:  0.49710971117019653
train gradient:  0.14439530508212434
iteration : 1518
train acc:  0.859375
train loss:  0.4074321985244751
train gradient:  0.07772542240054726
iteration : 1519
train acc:  0.765625
train loss:  0.45756444334983826
train gradient:  0.11843724157131555
iteration : 1520
train acc:  0.7421875
train loss:  0.4873703718185425
train gradient:  0.11632720730411969
iteration : 1521
train acc:  0.7578125
train loss:  0.44925108551979065
train gradient:  0.0978530614894464
iteration : 1522
train acc:  0.734375
train loss:  0.4962265193462372
train gradient:  0.1386164118307184
iteration : 1523
train acc:  0.765625
train loss:  0.47355684638023376
train gradient:  0.0879214737874886
iteration : 1524
train acc:  0.8046875
train loss:  0.4767150282859802
train gradient:  0.12544065748599031
iteration : 1525
train acc:  0.703125
train loss:  0.4831349551677704
train gradient:  0.11104082744457817
iteration : 1526
train acc:  0.6875
train loss:  0.5162737369537354
train gradient:  0.09823164910796141
iteration : 1527
train acc:  0.7734375
train loss:  0.4808168113231659
train gradient:  0.09980468405543375
iteration : 1528
train acc:  0.78125
train loss:  0.4932194948196411
train gradient:  0.1217336445613251
iteration : 1529
train acc:  0.7421875
train loss:  0.4915386438369751
train gradient:  0.11094223029904526
iteration : 1530
train acc:  0.7578125
train loss:  0.4989280700683594
train gradient:  0.1321755107818925
iteration : 1531
train acc:  0.7421875
train loss:  0.5287485122680664
train gradient:  0.15146914402764838
iteration : 1532
train acc:  0.7578125
train loss:  0.4373008906841278
train gradient:  0.09724973028439798
iteration : 1533
train acc:  0.71875
train loss:  0.5036090016365051
train gradient:  0.17715816126225198
iteration : 1534
train acc:  0.7734375
train loss:  0.4286711513996124
train gradient:  0.09592383235643781
iteration : 1535
train acc:  0.8046875
train loss:  0.42724576592445374
train gradient:  0.08094635004025183
iteration : 1536
train acc:  0.796875
train loss:  0.4338041841983795
train gradient:  0.1241497797340803
iteration : 1537
train acc:  0.7734375
train loss:  0.46265310049057007
train gradient:  0.09708781189963252
iteration : 1538
train acc:  0.796875
train loss:  0.46586892008781433
train gradient:  0.11846172148615614
iteration : 1539
train acc:  0.75
train loss:  0.4971000850200653
train gradient:  0.12922024795793952
iteration : 1540
train acc:  0.765625
train loss:  0.4463128447532654
train gradient:  0.10841498079339974
iteration : 1541
train acc:  0.703125
train loss:  0.4983394742012024
train gradient:  0.16881271419462693
iteration : 1542
train acc:  0.8359375
train loss:  0.36898648738861084
train gradient:  0.0742995101556246
iteration : 1543
train acc:  0.7421875
train loss:  0.4612789750099182
train gradient:  0.11110799999802451
iteration : 1544
train acc:  0.765625
train loss:  0.4882235825061798
train gradient:  0.1322547596557892
iteration : 1545
train acc:  0.6875
train loss:  0.536746084690094
train gradient:  0.13830147162541895
iteration : 1546
train acc:  0.734375
train loss:  0.5021086931228638
train gradient:  0.14091711152338793
iteration : 1547
train acc:  0.7421875
train loss:  0.4473227262496948
train gradient:  0.11782552915824133
iteration : 1548
train acc:  0.7890625
train loss:  0.4772765636444092
train gradient:  0.11924852319554693
iteration : 1549
train acc:  0.75
train loss:  0.44794392585754395
train gradient:  0.10385909784489714
iteration : 1550
train acc:  0.7421875
train loss:  0.4951919913291931
train gradient:  0.1321083394317174
iteration : 1551
train acc:  0.71875
train loss:  0.4921560287475586
train gradient:  0.12232807479803515
iteration : 1552
train acc:  0.71875
train loss:  0.5428633689880371
train gradient:  0.13780260894209312
iteration : 1553
train acc:  0.7265625
train loss:  0.5850849151611328
train gradient:  0.15350523019195078
iteration : 1554
train acc:  0.78125
train loss:  0.4083944261074066
train gradient:  0.10801194451442553
iteration : 1555
train acc:  0.734375
train loss:  0.4825579822063446
train gradient:  0.1179012841210807
iteration : 1556
train acc:  0.6953125
train loss:  0.5525326728820801
train gradient:  0.17779739944781842
iteration : 1557
train acc:  0.75
train loss:  0.4789640009403229
train gradient:  0.11289695366985118
iteration : 1558
train acc:  0.78125
train loss:  0.40599310398101807
train gradient:  0.09478916859693448
iteration : 1559
train acc:  0.734375
train loss:  0.4597329795360565
train gradient:  0.1044236139199648
iteration : 1560
train acc:  0.7890625
train loss:  0.42878222465515137
train gradient:  0.0897243636256332
iteration : 1561
train acc:  0.75
train loss:  0.4768056869506836
train gradient:  0.10239036126799728
iteration : 1562
train acc:  0.765625
train loss:  0.46439841389656067
train gradient:  0.08548845269544658
iteration : 1563
train acc:  0.7109375
train loss:  0.4990745186805725
train gradient:  0.13356600065763757
iteration : 1564
train acc:  0.7265625
train loss:  0.4881224036216736
train gradient:  0.11312826276789327
iteration : 1565
train acc:  0.7578125
train loss:  0.4813196361064911
train gradient:  0.14064725634346098
iteration : 1566
train acc:  0.6796875
train loss:  0.5801474452018738
train gradient:  0.21350615374407036
iteration : 1567
train acc:  0.7109375
train loss:  0.48414748907089233
train gradient:  0.13004697260871412
iteration : 1568
train acc:  0.7421875
train loss:  0.509611964225769
train gradient:  0.1140057415465908
iteration : 1569
train acc:  0.828125
train loss:  0.4508723020553589
train gradient:  0.12748510770657917
iteration : 1570
train acc:  0.7734375
train loss:  0.4268686771392822
train gradient:  0.09511159246271589
iteration : 1571
train acc:  0.765625
train loss:  0.4845176041126251
train gradient:  0.10172040605613429
iteration : 1572
train acc:  0.734375
train loss:  0.63929283618927
train gradient:  0.1905933881383569
iteration : 1573
train acc:  0.84375
train loss:  0.4211413264274597
train gradient:  0.09088613860404895
iteration : 1574
train acc:  0.6953125
train loss:  0.5049909353256226
train gradient:  0.14863669481261554
iteration : 1575
train acc:  0.7421875
train loss:  0.5310744047164917
train gradient:  0.12590714979623854
iteration : 1576
train acc:  0.6953125
train loss:  0.5286824107170105
train gradient:  0.1352630640816591
iteration : 1577
train acc:  0.6796875
train loss:  0.5405040979385376
train gradient:  0.13849413715494557
iteration : 1578
train acc:  0.7734375
train loss:  0.45888859033584595
train gradient:  0.10812518070785739
iteration : 1579
train acc:  0.71875
train loss:  0.47187450528144836
train gradient:  0.11710336676166472
iteration : 1580
train acc:  0.7578125
train loss:  0.46507585048675537
train gradient:  0.1212821725795267
iteration : 1581
train acc:  0.75
train loss:  0.43392324447631836
train gradient:  0.09366313671886879
iteration : 1582
train acc:  0.75
train loss:  0.5167806148529053
train gradient:  0.13547047728641554
iteration : 1583
train acc:  0.765625
train loss:  0.4488029181957245
train gradient:  0.11574720564943271
iteration : 1584
train acc:  0.765625
train loss:  0.4707767367362976
train gradient:  0.10711462958896587
iteration : 1585
train acc:  0.78125
train loss:  0.4489785432815552
train gradient:  0.11203676728717579
iteration : 1586
train acc:  0.78125
train loss:  0.43545660376548767
train gradient:  0.09623379376615715
iteration : 1587
train acc:  0.7109375
train loss:  0.5382289290428162
train gradient:  0.1362463164069406
iteration : 1588
train acc:  0.703125
train loss:  0.4783487319946289
train gradient:  0.11837537099250353
iteration : 1589
train acc:  0.71875
train loss:  0.5013325214385986
train gradient:  0.12734943063405085
iteration : 1590
train acc:  0.8203125
train loss:  0.4792766571044922
train gradient:  0.11853453347385637
iteration : 1591
train acc:  0.7578125
train loss:  0.4532437324523926
train gradient:  0.1002952599470895
iteration : 1592
train acc:  0.7734375
train loss:  0.4882659316062927
train gradient:  0.13435080717657533
iteration : 1593
train acc:  0.71875
train loss:  0.46009689569473267
train gradient:  0.10278231583941677
iteration : 1594
train acc:  0.7734375
train loss:  0.4852626919746399
train gradient:  0.11291394693665682
iteration : 1595
train acc:  0.7890625
train loss:  0.45072466135025024
train gradient:  0.09062433977949705
iteration : 1596
train acc:  0.7109375
train loss:  0.5457175970077515
train gradient:  0.13785870385705262
iteration : 1597
train acc:  0.7578125
train loss:  0.4983861744403839
train gradient:  0.1371156730730122
iteration : 1598
train acc:  0.7734375
train loss:  0.5040668249130249
train gradient:  0.1354245091954785
iteration : 1599
train acc:  0.71875
train loss:  0.5207406282424927
train gradient:  0.19849114049605687
iteration : 1600
train acc:  0.71875
train loss:  0.49082010984420776
train gradient:  0.09422430079292357
iteration : 1601
train acc:  0.7890625
train loss:  0.43135643005371094
train gradient:  0.0954418005005927
iteration : 1602
train acc:  0.7734375
train loss:  0.4716147482395172
train gradient:  0.101131449472955
iteration : 1603
train acc:  0.7734375
train loss:  0.46518999338150024
train gradient:  0.12696159787455297
iteration : 1604
train acc:  0.75
train loss:  0.49697253108024597
train gradient:  0.12291777854806751
iteration : 1605
train acc:  0.7578125
train loss:  0.4556252062320709
train gradient:  0.1386108150416876
iteration : 1606
train acc:  0.8125
train loss:  0.41076916456222534
train gradient:  0.09792359553929689
iteration : 1607
train acc:  0.7109375
train loss:  0.5364493727684021
train gradient:  0.15271155919975352
iteration : 1608
train acc:  0.8125
train loss:  0.4722140431404114
train gradient:  0.11898611884886989
iteration : 1609
train acc:  0.6875
train loss:  0.5119630098342896
train gradient:  0.14837962197723156
iteration : 1610
train acc:  0.7109375
train loss:  0.5185635089874268
train gradient:  0.14682886835877879
iteration : 1611
train acc:  0.703125
train loss:  0.5543924570083618
train gradient:  0.13576067699994626
iteration : 1612
train acc:  0.78125
train loss:  0.4778106212615967
train gradient:  0.12254988696685233
iteration : 1613
train acc:  0.71875
train loss:  0.503318190574646
train gradient:  0.11090613669884397
iteration : 1614
train acc:  0.7109375
train loss:  0.5802462100982666
train gradient:  0.19737632365309818
iteration : 1615
train acc:  0.71875
train loss:  0.5322545766830444
train gradient:  0.15078956001486807
iteration : 1616
train acc:  0.8125
train loss:  0.43032076954841614
train gradient:  0.12045195764102805
iteration : 1617
train acc:  0.7265625
train loss:  0.4484702944755554
train gradient:  0.11389948139971172
iteration : 1618
train acc:  0.7421875
train loss:  0.5011423230171204
train gradient:  0.11900043813850676
iteration : 1619
train acc:  0.734375
train loss:  0.49286821484565735
train gradient:  0.13647863248746753
iteration : 1620
train acc:  0.7578125
train loss:  0.42883720993995667
train gradient:  0.0851173650211539
iteration : 1621
train acc:  0.7578125
train loss:  0.4321465790271759
train gradient:  0.10015251483720806
iteration : 1622
train acc:  0.734375
train loss:  0.4745333194732666
train gradient:  0.11413689708384119
iteration : 1623
train acc:  0.84375
train loss:  0.41164860129356384
train gradient:  0.08167783278475121
iteration : 1624
train acc:  0.7109375
train loss:  0.48522862792015076
train gradient:  0.10212262313318196
iteration : 1625
train acc:  0.765625
train loss:  0.43076786398887634
train gradient:  0.10605595005599586
iteration : 1626
train acc:  0.8203125
train loss:  0.37685078382492065
train gradient:  0.06793314522870793
iteration : 1627
train acc:  0.765625
train loss:  0.4443877339363098
train gradient:  0.11659210971100885
iteration : 1628
train acc:  0.734375
train loss:  0.46889734268188477
train gradient:  0.11916117717337087
iteration : 1629
train acc:  0.7734375
train loss:  0.4794900417327881
train gradient:  0.12669356693472583
iteration : 1630
train acc:  0.7421875
train loss:  0.47676679491996765
train gradient:  0.11246472100037204
iteration : 1631
train acc:  0.78125
train loss:  0.46811431646347046
train gradient:  0.11205665481031811
iteration : 1632
train acc:  0.8203125
train loss:  0.42937272787094116
train gradient:  0.07907157437013823
iteration : 1633
train acc:  0.8046875
train loss:  0.45337343215942383
train gradient:  0.12655523839769073
iteration : 1634
train acc:  0.78125
train loss:  0.4649287164211273
train gradient:  0.0977382807318466
iteration : 1635
train acc:  0.78125
train loss:  0.4945096969604492
train gradient:  0.12144337000693997
iteration : 1636
train acc:  0.8046875
train loss:  0.469470739364624
train gradient:  0.14403672718678634
iteration : 1637
train acc:  0.7578125
train loss:  0.4983709752559662
train gradient:  0.11682753601937408
iteration : 1638
train acc:  0.7578125
train loss:  0.5323035717010498
train gradient:  0.146658704922063
iteration : 1639
train acc:  0.71875
train loss:  0.49905693531036377
train gradient:  0.12511597088688076
iteration : 1640
train acc:  0.765625
train loss:  0.5074408054351807
train gradient:  0.1523358623942927
iteration : 1641
train acc:  0.765625
train loss:  0.4571366310119629
train gradient:  0.12147075000062671
iteration : 1642
train acc:  0.8125
train loss:  0.4239741563796997
train gradient:  0.10128325795332573
iteration : 1643
train acc:  0.71875
train loss:  0.4995785355567932
train gradient:  0.11214369627977289
iteration : 1644
train acc:  0.734375
train loss:  0.49882662296295166
train gradient:  0.11032332086372566
iteration : 1645
train acc:  0.703125
train loss:  0.49134182929992676
train gradient:  0.1001806646355789
iteration : 1646
train acc:  0.84375
train loss:  0.4156869649887085
train gradient:  0.09388601397691061
iteration : 1647
train acc:  0.7890625
train loss:  0.4029995799064636
train gradient:  0.09362327536366964
iteration : 1648
train acc:  0.765625
train loss:  0.4704582095146179
train gradient:  0.10029291523751506
iteration : 1649
train acc:  0.796875
train loss:  0.41820448637008667
train gradient:  0.11887840929445163
iteration : 1650
train acc:  0.7890625
train loss:  0.4597105383872986
train gradient:  0.12140011240296959
iteration : 1651
train acc:  0.671875
train loss:  0.5505335927009583
train gradient:  0.1595075850148402
iteration : 1652
train acc:  0.796875
train loss:  0.48044949769973755
train gradient:  0.14663428141408338
iteration : 1653
train acc:  0.734375
train loss:  0.47038036584854126
train gradient:  0.12264931719548047
iteration : 1654
train acc:  0.71875
train loss:  0.47434645891189575
train gradient:  0.10332926421397595
iteration : 1655
train acc:  0.7109375
train loss:  0.4987936019897461
train gradient:  0.12464731698215176
iteration : 1656
train acc:  0.71875
train loss:  0.5155036449432373
train gradient:  0.14469547268068272
iteration : 1657
train acc:  0.796875
train loss:  0.45300331711769104
train gradient:  0.0936369781802783
iteration : 1658
train acc:  0.7578125
train loss:  0.47032004594802856
train gradient:  0.10343816706159376
iteration : 1659
train acc:  0.75
train loss:  0.45498523116111755
train gradient:  0.0885164202361119
iteration : 1660
train acc:  0.7421875
train loss:  0.4602876901626587
train gradient:  0.1120105290839488
iteration : 1661
train acc:  0.6796875
train loss:  0.5784612894058228
train gradient:  0.18017077143990634
iteration : 1662
train acc:  0.6796875
train loss:  0.5523132085800171
train gradient:  0.1421277513667943
iteration : 1663
train acc:  0.7734375
train loss:  0.44997113943099976
train gradient:  0.10607019699600563
iteration : 1664
train acc:  0.75
train loss:  0.4602607190608978
train gradient:  0.11595325462598338
iteration : 1665
train acc:  0.7421875
train loss:  0.481087327003479
train gradient:  0.11806930878550337
iteration : 1666
train acc:  0.7109375
train loss:  0.5186710953712463
train gradient:  0.1191367526765716
iteration : 1667
train acc:  0.75
train loss:  0.46657413244247437
train gradient:  0.0975659492247902
iteration : 1668
train acc:  0.7578125
train loss:  0.4709344208240509
train gradient:  0.11741325657525437
iteration : 1669
train acc:  0.8046875
train loss:  0.41156435012817383
train gradient:  0.09677543054047658
iteration : 1670
train acc:  0.8203125
train loss:  0.36746108531951904
train gradient:  0.06625992986564236
iteration : 1671
train acc:  0.765625
train loss:  0.47272148728370667
train gradient:  0.1251931260874803
iteration : 1672
train acc:  0.6953125
train loss:  0.47855818271636963
train gradient:  0.12476204519418067
iteration : 1673
train acc:  0.734375
train loss:  0.4752967357635498
train gradient:  0.1083792900717259
iteration : 1674
train acc:  0.703125
train loss:  0.6074366569519043
train gradient:  0.22610318248109865
iteration : 1675
train acc:  0.7578125
train loss:  0.5325300693511963
train gradient:  0.14835141671917879
iteration : 1676
train acc:  0.7734375
train loss:  0.45319750905036926
train gradient:  0.13400799930192808
iteration : 1677
train acc:  0.765625
train loss:  0.43944987654685974
train gradient:  0.10634016640950911
iteration : 1678
train acc:  0.8203125
train loss:  0.4017222821712494
train gradient:  0.07781353454349031
iteration : 1679
train acc:  0.7578125
train loss:  0.4560985863208771
train gradient:  0.1035864483857781
iteration : 1680
train acc:  0.6953125
train loss:  0.5518243312835693
train gradient:  0.12805538808766365
iteration : 1681
train acc:  0.6953125
train loss:  0.4993184208869934
train gradient:  0.12075292949699763
iteration : 1682
train acc:  0.8046875
train loss:  0.4032658338546753
train gradient:  0.08619334120764989
iteration : 1683
train acc:  0.734375
train loss:  0.527193009853363
train gradient:  0.1345293820488106
iteration : 1684
train acc:  0.7578125
train loss:  0.5098973512649536
train gradient:  0.13979044949679592
iteration : 1685
train acc:  0.75
train loss:  0.47582584619522095
train gradient:  0.14170216314404938
iteration : 1686
train acc:  0.8046875
train loss:  0.430643230676651
train gradient:  0.11444652275372107
iteration : 1687
train acc:  0.8046875
train loss:  0.46035999059677124
train gradient:  0.10068354671994001
iteration : 1688
train acc:  0.6953125
train loss:  0.6081564426422119
train gradient:  0.16439432587796404
iteration : 1689
train acc:  0.7578125
train loss:  0.4327641725540161
train gradient:  0.08538622800932524
iteration : 1690
train acc:  0.6796875
train loss:  0.543941080570221
train gradient:  0.1495832808099082
iteration : 1691
train acc:  0.75
train loss:  0.4603779911994934
train gradient:  0.10046829139457622
iteration : 1692
train acc:  0.734375
train loss:  0.5207264423370361
train gradient:  0.13413389322738312
iteration : 1693
train acc:  0.765625
train loss:  0.4943433105945587
train gradient:  0.12337535108251309
iteration : 1694
train acc:  0.7578125
train loss:  0.4987674355506897
train gradient:  0.15581720717648095
iteration : 1695
train acc:  0.7421875
train loss:  0.5027111768722534
train gradient:  0.12681049634097483
iteration : 1696
train acc:  0.765625
train loss:  0.5368943214416504
train gradient:  0.1270761142802962
iteration : 1697
train acc:  0.78125
train loss:  0.49888670444488525
train gradient:  0.11042965762032833
iteration : 1698
train acc:  0.71875
train loss:  0.4823414087295532
train gradient:  0.12814016346794738
iteration : 1699
train acc:  0.734375
train loss:  0.44330355525016785
train gradient:  0.09820103218358157
iteration : 1700
train acc:  0.7578125
train loss:  0.489213228225708
train gradient:  0.12347997164202298
iteration : 1701
train acc:  0.7578125
train loss:  0.45190882682800293
train gradient:  0.09848916352521815
iteration : 1702
train acc:  0.78125
train loss:  0.5024756193161011
train gradient:  0.0923105544083087
iteration : 1703
train acc:  0.8203125
train loss:  0.409079909324646
train gradient:  0.0827535904453874
iteration : 1704
train acc:  0.7265625
train loss:  0.5037969350814819
train gradient:  0.14730900377724546
iteration : 1705
train acc:  0.6953125
train loss:  0.49294978380203247
train gradient:  0.1459193944712741
iteration : 1706
train acc:  0.703125
train loss:  0.5394994020462036
train gradient:  0.11450071259421928
iteration : 1707
train acc:  0.8046875
train loss:  0.3748415410518646
train gradient:  0.08604381442452397
iteration : 1708
train acc:  0.8125
train loss:  0.4136585295200348
train gradient:  0.08720476698532072
iteration : 1709
train acc:  0.796875
train loss:  0.419930636882782
train gradient:  0.10195708184266314
iteration : 1710
train acc:  0.71875
train loss:  0.5312946438789368
train gradient:  0.1426757234287681
iteration : 1711
train acc:  0.6875
train loss:  0.5155949592590332
train gradient:  0.12165042987529837
iteration : 1712
train acc:  0.6640625
train loss:  0.5825520157814026
train gradient:  0.16738658218977787
iteration : 1713
train acc:  0.78125
train loss:  0.4810490310192108
train gradient:  0.11028583950709878
iteration : 1714
train acc:  0.765625
train loss:  0.46766579151153564
train gradient:  0.11115488615470538
iteration : 1715
train acc:  0.6796875
train loss:  0.5241692066192627
train gradient:  0.12675763373133062
iteration : 1716
train acc:  0.7265625
train loss:  0.5033338665962219
train gradient:  0.11341627016224878
iteration : 1717
train acc:  0.71875
train loss:  0.5062190294265747
train gradient:  0.15626522538663068
iteration : 1718
train acc:  0.765625
train loss:  0.48863664269447327
train gradient:  0.14600894691275373
iteration : 1719
train acc:  0.71875
train loss:  0.48504024744033813
train gradient:  0.1328055773166787
iteration : 1720
train acc:  0.7109375
train loss:  0.5230678915977478
train gradient:  0.23246202705515012
iteration : 1721
train acc:  0.734375
train loss:  0.5110342502593994
train gradient:  0.15672113114381753
iteration : 1722
train acc:  0.765625
train loss:  0.5196316838264465
train gradient:  0.12559698013217074
iteration : 1723
train acc:  0.71875
train loss:  0.4765239357948303
train gradient:  0.12772562076129965
iteration : 1724
train acc:  0.78125
train loss:  0.43238645792007446
train gradient:  0.07926860217206894
iteration : 1725
train acc:  0.7109375
train loss:  0.5142680406570435
train gradient:  0.15610793794950825
iteration : 1726
train acc:  0.6875
train loss:  0.6035950779914856
train gradient:  0.15005256794096905
iteration : 1727
train acc:  0.734375
train loss:  0.49315324425697327
train gradient:  0.11898854142943575
iteration : 1728
train acc:  0.7109375
train loss:  0.537102222442627
train gradient:  0.13493655294946164
iteration : 1729
train acc:  0.7109375
train loss:  0.5283408164978027
train gradient:  0.187629764246616
iteration : 1730
train acc:  0.6953125
train loss:  0.5302302837371826
train gradient:  0.14311217134341284
iteration : 1731
train acc:  0.78125
train loss:  0.44425857067108154
train gradient:  0.09196776939436163
iteration : 1732
train acc:  0.6796875
train loss:  0.577573299407959
train gradient:  0.21213498589150737
iteration : 1733
train acc:  0.7265625
train loss:  0.46117109060287476
train gradient:  0.08845934003913464
iteration : 1734
train acc:  0.7890625
train loss:  0.4987098276615143
train gradient:  0.11314862505758434
iteration : 1735
train acc:  0.7734375
train loss:  0.4654202461242676
train gradient:  0.09967771231700825
iteration : 1736
train acc:  0.6953125
train loss:  0.5252639651298523
train gradient:  0.1590132916165044
iteration : 1737
train acc:  0.765625
train loss:  0.5027483701705933
train gradient:  0.12339486155686297
iteration : 1738
train acc:  0.734375
train loss:  0.4837225675582886
train gradient:  0.10213172882378126
iteration : 1739
train acc:  0.6796875
train loss:  0.6013898849487305
train gradient:  0.17734797442514666
iteration : 1740
train acc:  0.75
train loss:  0.4808245897293091
train gradient:  0.13354798047628816
iteration : 1741
train acc:  0.796875
train loss:  0.47705793380737305
train gradient:  0.12569724418012296
iteration : 1742
train acc:  0.7578125
train loss:  0.4502476453781128
train gradient:  0.10621856242642069
iteration : 1743
train acc:  0.71875
train loss:  0.4963265061378479
train gradient:  0.12996483094133418
iteration : 1744
train acc:  0.7109375
train loss:  0.49026960134506226
train gradient:  0.11096609907159354
iteration : 1745
train acc:  0.8046875
train loss:  0.4205856919288635
train gradient:  0.10910849013091581
iteration : 1746
train acc:  0.6953125
train loss:  0.5456936359405518
train gradient:  0.15107089566506632
iteration : 1747
train acc:  0.828125
train loss:  0.39590099453926086
train gradient:  0.08958382283266275
iteration : 1748
train acc:  0.6796875
train loss:  0.5554338097572327
train gradient:  0.15858635047592246
iteration : 1749
train acc:  0.796875
train loss:  0.4372655749320984
train gradient:  0.0935710390243198
iteration : 1750
train acc:  0.75
train loss:  0.45161885023117065
train gradient:  0.10608393429193963
iteration : 1751
train acc:  0.71875
train loss:  0.5071724653244019
train gradient:  0.14532218968284227
iteration : 1752
train acc:  0.75
train loss:  0.47040730714797974
train gradient:  0.10715547381005339
iteration : 1753
train acc:  0.703125
train loss:  0.5837128162384033
train gradient:  0.17949386320309973
iteration : 1754
train acc:  0.7421875
train loss:  0.5144277215003967
train gradient:  0.11064019458956255
iteration : 1755
train acc:  0.7109375
train loss:  0.5081484913825989
train gradient:  0.13264382063985308
iteration : 1756
train acc:  0.703125
train loss:  0.5892904996871948
train gradient:  0.20981268482867205
iteration : 1757
train acc:  0.734375
train loss:  0.5102637410163879
train gradient:  0.1503340163001241
iteration : 1758
train acc:  0.703125
train loss:  0.515945315361023
train gradient:  0.1452866652422019
iteration : 1759
train acc:  0.7890625
train loss:  0.436633825302124
train gradient:  0.1315866922201704
iteration : 1760
train acc:  0.75
train loss:  0.4953642785549164
train gradient:  0.13427051651744287
iteration : 1761
train acc:  0.7265625
train loss:  0.48329588770866394
train gradient:  0.10231241334609124
iteration : 1762
train acc:  0.71875
train loss:  0.49310675263404846
train gradient:  0.11497379019210052
iteration : 1763
train acc:  0.703125
train loss:  0.5598345994949341
train gradient:  0.12902496917706258
iteration : 1764
train acc:  0.7421875
train loss:  0.494159996509552
train gradient:  0.13295789080623407
iteration : 1765
train acc:  0.8203125
train loss:  0.405695378780365
train gradient:  0.09098515885929993
iteration : 1766
train acc:  0.7578125
train loss:  0.4843568801879883
train gradient:  0.14093599479131347
iteration : 1767
train acc:  0.7421875
train loss:  0.4439469575881958
train gradient:  0.09595407947194587
iteration : 1768
train acc:  0.75
train loss:  0.4978177845478058
train gradient:  0.11765398067658178
iteration : 1769
train acc:  0.7578125
train loss:  0.48305100202560425
train gradient:  0.11965057042082303
iteration : 1770
train acc:  0.7265625
train loss:  0.49815788865089417
train gradient:  0.10907337510572593
iteration : 1771
train acc:  0.7578125
train loss:  0.41832947731018066
train gradient:  0.08362496774250362
iteration : 1772
train acc:  0.71875
train loss:  0.49350932240486145
train gradient:  0.1342118259523814
iteration : 1773
train acc:  0.765625
train loss:  0.51980060338974
train gradient:  0.12338220933309807
iteration : 1774
train acc:  0.78125
train loss:  0.45816299319267273
train gradient:  0.09982327447922172
iteration : 1775
train acc:  0.78125
train loss:  0.41426193714141846
train gradient:  0.1003375891475402
iteration : 1776
train acc:  0.6953125
train loss:  0.49370864033699036
train gradient:  0.12145314045132513
iteration : 1777
train acc:  0.7734375
train loss:  0.5085693597793579
train gradient:  0.13698634278932564
iteration : 1778
train acc:  0.75
train loss:  0.48320046067237854
train gradient:  0.13525234014583487
iteration : 1779
train acc:  0.8125
train loss:  0.41404908895492554
train gradient:  0.07414104445258671
iteration : 1780
train acc:  0.796875
train loss:  0.4763774871826172
train gradient:  0.11542512695388547
iteration : 1781
train acc:  0.734375
train loss:  0.5501822233200073
train gradient:  0.14757557464800425
iteration : 1782
train acc:  0.796875
train loss:  0.42387109994888306
train gradient:  0.10219151838106619
iteration : 1783
train acc:  0.8125
train loss:  0.435453861951828
train gradient:  0.12359339529194512
iteration : 1784
train acc:  0.8046875
train loss:  0.4149542450904846
train gradient:  0.09089100458103969
iteration : 1785
train acc:  0.7421875
train loss:  0.46288859844207764
train gradient:  0.10209838354378159
iteration : 1786
train acc:  0.8125
train loss:  0.4484066069126129
train gradient:  0.1397787204353057
iteration : 1787
train acc:  0.8046875
train loss:  0.4156380295753479
train gradient:  0.07241556200033399
iteration : 1788
train acc:  0.7265625
train loss:  0.5137888193130493
train gradient:  0.12565274960617312
iteration : 1789
train acc:  0.75
train loss:  0.5201730728149414
train gradient:  0.13115448048494385
iteration : 1790
train acc:  0.65625
train loss:  0.592135488986969
train gradient:  0.15231381486443724
iteration : 1791
train acc:  0.7265625
train loss:  0.5032724738121033
train gradient:  0.10927468020919381
iteration : 1792
train acc:  0.7734375
train loss:  0.47873586416244507
train gradient:  0.15484386301841485
iteration : 1793
train acc:  0.6953125
train loss:  0.49598240852355957
train gradient:  0.12581895382010944
iteration : 1794
train acc:  0.7890625
train loss:  0.4458487629890442
train gradient:  0.10648840749996671
iteration : 1795
train acc:  0.75
train loss:  0.48544174432754517
train gradient:  0.08911907823938113
iteration : 1796
train acc:  0.796875
train loss:  0.4314311444759369
train gradient:  0.13416296590442234
iteration : 1797
train acc:  0.765625
train loss:  0.42090219259262085
train gradient:  0.09729849266279796
iteration : 1798
train acc:  0.7734375
train loss:  0.47151046991348267
train gradient:  0.11595018203880632
iteration : 1799
train acc:  0.8203125
train loss:  0.41240814328193665
train gradient:  0.12049287278357415
iteration : 1800
train acc:  0.7421875
train loss:  0.471666157245636
train gradient:  0.12598829722476335
iteration : 1801
train acc:  0.734375
train loss:  0.5141388177871704
train gradient:  0.11726072978636816
iteration : 1802
train acc:  0.71875
train loss:  0.4887324571609497
train gradient:  0.11016490443295018
iteration : 1803
train acc:  0.765625
train loss:  0.4431987404823303
train gradient:  0.12331297369304138
iteration : 1804
train acc:  0.7734375
train loss:  0.4909825921058655
train gradient:  0.109595510782558
iteration : 1805
train acc:  0.703125
train loss:  0.5569127798080444
train gradient:  0.135211563505944
iteration : 1806
train acc:  0.765625
train loss:  0.5028868913650513
train gradient:  0.18036510465988054
iteration : 1807
train acc:  0.71875
train loss:  0.5597072839736938
train gradient:  0.12119451888849155
iteration : 1808
train acc:  0.7578125
train loss:  0.4499836564064026
train gradient:  0.11582508809730879
iteration : 1809
train acc:  0.765625
train loss:  0.4767509698867798
train gradient:  0.11373162119568087
iteration : 1810
train acc:  0.765625
train loss:  0.515697717666626
train gradient:  0.11395958691486233
iteration : 1811
train acc:  0.71875
train loss:  0.4985010325908661
train gradient:  0.1345812680089149
iteration : 1812
train acc:  0.8359375
train loss:  0.40776050090789795
train gradient:  0.08775114961317
iteration : 1813
train acc:  0.8046875
train loss:  0.386447548866272
train gradient:  0.07951324111611774
iteration : 1814
train acc:  0.7421875
train loss:  0.5182348489761353
train gradient:  0.1279574119117032
iteration : 1815
train acc:  0.734375
train loss:  0.48166269063949585
train gradient:  0.09703949068836477
iteration : 1816
train acc:  0.6875
train loss:  0.5485061407089233
train gradient:  0.14617913273730593
iteration : 1817
train acc:  0.6796875
train loss:  0.5947158336639404
train gradient:  0.1625404202771834
iteration : 1818
train acc:  0.75
train loss:  0.47506168484687805
train gradient:  0.10730518110786477
iteration : 1819
train acc:  0.75
train loss:  0.45095276832580566
train gradient:  0.11136943409848257
iteration : 1820
train acc:  0.6953125
train loss:  0.5274535417556763
train gradient:  0.13002623654932433
iteration : 1821
train acc:  0.75
train loss:  0.4564626216888428
train gradient:  0.12150607776526297
iteration : 1822
train acc:  0.734375
train loss:  0.4701419472694397
train gradient:  0.11784008828669518
iteration : 1823
train acc:  0.765625
train loss:  0.5129945278167725
train gradient:  0.11871404444678728
iteration : 1824
train acc:  0.7109375
train loss:  0.5551502704620361
train gradient:  0.15724482530086673
iteration : 1825
train acc:  0.734375
train loss:  0.5251984000205994
train gradient:  0.1253810705248699
iteration : 1826
train acc:  0.7890625
train loss:  0.4889169931411743
train gradient:  0.11137134556273028
iteration : 1827
train acc:  0.7421875
train loss:  0.4832812547683716
train gradient:  0.0990379316418171
iteration : 1828
train acc:  0.734375
train loss:  0.5107097625732422
train gradient:  0.12073022006718953
iteration : 1829
train acc:  0.78125
train loss:  0.49446403980255127
train gradient:  0.08730832475366489
iteration : 1830
train acc:  0.8125
train loss:  0.4227970242500305
train gradient:  0.10042048129017372
iteration : 1831
train acc:  0.7421875
train loss:  0.5546150207519531
train gradient:  0.1589821964020982
iteration : 1832
train acc:  0.7421875
train loss:  0.46857017278671265
train gradient:  0.122486781555299
iteration : 1833
train acc:  0.7890625
train loss:  0.39388909935951233
train gradient:  0.0836800581625276
iteration : 1834
train acc:  0.6796875
train loss:  0.5826554894447327
train gradient:  0.1385731608882355
iteration : 1835
train acc:  0.8125
train loss:  0.4361761808395386
train gradient:  0.09708257307425525
iteration : 1836
train acc:  0.734375
train loss:  0.520770788192749
train gradient:  0.12840521677143202
iteration : 1837
train acc:  0.6953125
train loss:  0.5027360916137695
train gradient:  0.1123334060274215
iteration : 1838
train acc:  0.7265625
train loss:  0.5589151382446289
train gradient:  0.1680284338822412
iteration : 1839
train acc:  0.7265625
train loss:  0.558980405330658
train gradient:  0.16229134255899103
iteration : 1840
train acc:  0.765625
train loss:  0.4500303268432617
train gradient:  0.08335172433252196
iteration : 1841
train acc:  0.7578125
train loss:  0.4601065516471863
train gradient:  0.13274420288655614
iteration : 1842
train acc:  0.7578125
train loss:  0.4960147738456726
train gradient:  0.11290228024603234
iteration : 1843
train acc:  0.78125
train loss:  0.45218783617019653
train gradient:  0.10782046245249075
iteration : 1844
train acc:  0.7265625
train loss:  0.5374565124511719
train gradient:  0.18086345877568583
iteration : 1845
train acc:  0.78125
train loss:  0.46278876066207886
train gradient:  0.10108513823909049
iteration : 1846
train acc:  0.734375
train loss:  0.5054295063018799
train gradient:  0.11282822054471783
iteration : 1847
train acc:  0.7421875
train loss:  0.46253207325935364
train gradient:  0.11193734832502625
iteration : 1848
train acc:  0.75
train loss:  0.5209398865699768
train gradient:  0.14360746922657547
iteration : 1849
train acc:  0.71875
train loss:  0.5253928899765015
train gradient:  0.12225007066330251
iteration : 1850
train acc:  0.8046875
train loss:  0.45493555068969727
train gradient:  0.1448107587137607
iteration : 1851
train acc:  0.8359375
train loss:  0.43315112590789795
train gradient:  0.08930375059595368
iteration : 1852
train acc:  0.734375
train loss:  0.4631441533565521
train gradient:  0.11713769134818285
iteration : 1853
train acc:  0.78125
train loss:  0.4557128846645355
train gradient:  0.10448266681505172
iteration : 1854
train acc:  0.703125
train loss:  0.47663170099258423
train gradient:  0.11537063003856639
iteration : 1855
train acc:  0.78125
train loss:  0.47482430934906006
train gradient:  0.12473619696500178
iteration : 1856
train acc:  0.765625
train loss:  0.4652099013328552
train gradient:  0.11917241287420885
iteration : 1857
train acc:  0.7421875
train loss:  0.49247926473617554
train gradient:  0.10737196613512362
iteration : 1858
train acc:  0.734375
train loss:  0.5218194723129272
train gradient:  0.11393428731728121
iteration : 1859
train acc:  0.765625
train loss:  0.44057393074035645
train gradient:  0.12543319857087482
iteration : 1860
train acc:  0.734375
train loss:  0.5112424492835999
train gradient:  0.109962240401552
iteration : 1861
train acc:  0.734375
train loss:  0.4741143584251404
train gradient:  0.13240312545550148
iteration : 1862
train acc:  0.7421875
train loss:  0.45495733618736267
train gradient:  0.10658211365595098
iteration : 1863
train acc:  0.7265625
train loss:  0.5223039984703064
train gradient:  0.09450976342582645
iteration : 1864
train acc:  0.875
train loss:  0.41416436433792114
train gradient:  0.09752111921549064
iteration : 1865
train acc:  0.7890625
train loss:  0.41697436571121216
train gradient:  0.09956188014194285
iteration : 1866
train acc:  0.734375
train loss:  0.5246736407279968
train gradient:  0.12088478826629301
iteration : 1867
train acc:  0.703125
train loss:  0.556875467300415
train gradient:  0.14719861323706018
iteration : 1868
train acc:  0.7265625
train loss:  0.4763137102127075
train gradient:  0.12080766943402324
iteration : 1869
train acc:  0.7890625
train loss:  0.4748542904853821
train gradient:  0.0955400923125944
iteration : 1870
train acc:  0.671875
train loss:  0.5245466232299805
train gradient:  0.17050614044464613
iteration : 1871
train acc:  0.8125
train loss:  0.42732343077659607
train gradient:  0.08974519339752844
iteration : 1872
train acc:  0.796875
train loss:  0.4507099390029907
train gradient:  0.10036747990638936
iteration : 1873
train acc:  0.78125
train loss:  0.43708962202072144
train gradient:  0.08699618521328362
iteration : 1874
train acc:  0.7265625
train loss:  0.4943152666091919
train gradient:  0.12933413356240975
iteration : 1875
train acc:  0.75
train loss:  0.48141685128211975
train gradient:  0.0993872968283439
iteration : 1876
train acc:  0.7734375
train loss:  0.459928035736084
train gradient:  0.1325646759442411
iteration : 1877
train acc:  0.7421875
train loss:  0.5033694505691528
train gradient:  0.0978884000626429
iteration : 1878
train acc:  0.75
train loss:  0.4946273863315582
train gradient:  0.11601699051725055
iteration : 1879
train acc:  0.765625
train loss:  0.45193567872047424
train gradient:  0.1190509646377733
iteration : 1880
train acc:  0.6953125
train loss:  0.5174566507339478
train gradient:  0.13057972119027467
iteration : 1881
train acc:  0.78125
train loss:  0.5086131691932678
train gradient:  0.1244120778848604
iteration : 1882
train acc:  0.7578125
train loss:  0.5214583873748779
train gradient:  0.13132372066755116
iteration : 1883
train acc:  0.7578125
train loss:  0.4543369710445404
train gradient:  0.12889294708049653
iteration : 1884
train acc:  0.796875
train loss:  0.43549075722694397
train gradient:  0.07664657298434681
iteration : 1885
train acc:  0.6640625
train loss:  0.529358983039856
train gradient:  0.1744178313242501
iteration : 1886
train acc:  0.7265625
train loss:  0.47877931594848633
train gradient:  0.10361970299065248
iteration : 1887
train acc:  0.78125
train loss:  0.4320896863937378
train gradient:  0.10581098488659478
iteration : 1888
train acc:  0.7734375
train loss:  0.4747486710548401
train gradient:  0.11433309178939607
iteration : 1889
train acc:  0.78125
train loss:  0.4430016875267029
train gradient:  0.0992310563823714
iteration : 1890
train acc:  0.765625
train loss:  0.4726634621620178
train gradient:  0.1337291804984007
iteration : 1891
train acc:  0.78125
train loss:  0.43977564573287964
train gradient:  0.12053460219680938
iteration : 1892
train acc:  0.7734375
train loss:  0.5166769623756409
train gradient:  0.14260210616033933
iteration : 1893
train acc:  0.7890625
train loss:  0.4342377483844757
train gradient:  0.087894116240212
iteration : 1894
train acc:  0.8203125
train loss:  0.41946589946746826
train gradient:  0.09445047933129333
iteration : 1895
train acc:  0.7578125
train loss:  0.46349766850471497
train gradient:  0.10651161290330453
iteration : 1896
train acc:  0.765625
train loss:  0.48730722069740295
train gradient:  0.11490616135082103
iteration : 1897
train acc:  0.7265625
train loss:  0.5297364592552185
train gradient:  0.13325037479858487
iteration : 1898
train acc:  0.7109375
train loss:  0.5134588479995728
train gradient:  0.13277991364928354
iteration : 1899
train acc:  0.828125
train loss:  0.38911932706832886
train gradient:  0.09838556303310228
iteration : 1900
train acc:  0.703125
train loss:  0.4690835177898407
train gradient:  0.09799468403192386
iteration : 1901
train acc:  0.8046875
train loss:  0.47035855054855347
train gradient:  0.10946143947529058
iteration : 1902
train acc:  0.765625
train loss:  0.44667667150497437
train gradient:  0.10324557384024212
iteration : 1903
train acc:  0.7734375
train loss:  0.5019564032554626
train gradient:  0.12262045497241739
iteration : 1904
train acc:  0.7265625
train loss:  0.5077508687973022
train gradient:  0.0968320440717031
iteration : 1905
train acc:  0.734375
train loss:  0.5116408467292786
train gradient:  0.13062977724410577
iteration : 1906
train acc:  0.765625
train loss:  0.49186259508132935
train gradient:  0.16518342455206542
iteration : 1907
train acc:  0.765625
train loss:  0.4736243784427643
train gradient:  0.12934935573978124
iteration : 1908
train acc:  0.8046875
train loss:  0.4172317087650299
train gradient:  0.10690793710496199
iteration : 1909
train acc:  0.765625
train loss:  0.487981915473938
train gradient:  0.12522157162557998
iteration : 1910
train acc:  0.7421875
train loss:  0.4952729046344757
train gradient:  0.11212072676595768
iteration : 1911
train acc:  0.703125
train loss:  0.4981071949005127
train gradient:  0.10480900123128628
iteration : 1912
train acc:  0.6796875
train loss:  0.5220600962638855
train gradient:  0.12120501334892321
iteration : 1913
train acc:  0.78125
train loss:  0.4299500584602356
train gradient:  0.12292342715770434
iteration : 1914
train acc:  0.75
train loss:  0.4411705732345581
train gradient:  0.1222357121580495
iteration : 1915
train acc:  0.75
train loss:  0.4622877240180969
train gradient:  0.1155204096146733
iteration : 1916
train acc:  0.6953125
train loss:  0.5476410388946533
train gradient:  0.16527796115168006
iteration : 1917
train acc:  0.7421875
train loss:  0.5295220613479614
train gradient:  0.13022427921759458
iteration : 1918
train acc:  0.7421875
train loss:  0.4372035264968872
train gradient:  0.09889049863758315
iteration : 1919
train acc:  0.75
train loss:  0.5324687957763672
train gradient:  0.14902622117217112
iteration : 1920
train acc:  0.7578125
train loss:  0.511576771736145
train gradient:  0.1459719821554969
iteration : 1921
train acc:  0.7109375
train loss:  0.5078735947608948
train gradient:  0.141041516154628
iteration : 1922
train acc:  0.734375
train loss:  0.5013861656188965
train gradient:  0.1163267075687691
iteration : 1923
train acc:  0.8203125
train loss:  0.44294053316116333
train gradient:  0.07597820486413155
iteration : 1924
train acc:  0.7265625
train loss:  0.48143619298934937
train gradient:  0.11433291783007235
iteration : 1925
train acc:  0.765625
train loss:  0.510582447052002
train gradient:  0.12159460652073187
iteration : 1926
train acc:  0.7578125
train loss:  0.487927109003067
train gradient:  0.12229879232061848
iteration : 1927
train acc:  0.7421875
train loss:  0.48012930154800415
train gradient:  0.12130173685563897
iteration : 1928
train acc:  0.703125
train loss:  0.5279635190963745
train gradient:  0.13608490546820223
iteration : 1929
train acc:  0.75
train loss:  0.4732532501220703
train gradient:  0.0994044844297253
iteration : 1930
train acc:  0.7421875
train loss:  0.5263286828994751
train gradient:  0.11012808219979503
iteration : 1931
train acc:  0.6796875
train loss:  0.5318924784660339
train gradient:  0.14446417249763788
iteration : 1932
train acc:  0.625
train loss:  0.5683436393737793
train gradient:  0.1313471147326949
iteration : 1933
train acc:  0.7265625
train loss:  0.4903666377067566
train gradient:  0.12276793712932203
iteration : 1934
train acc:  0.75
train loss:  0.4887481927871704
train gradient:  0.11310715121905572
iteration : 1935
train acc:  0.75
train loss:  0.48675721883773804
train gradient:  0.10255990252596707
iteration : 1936
train acc:  0.703125
train loss:  0.4874972403049469
train gradient:  0.10902353884102149
iteration : 1937
train acc:  0.75
train loss:  0.4274805188179016
train gradient:  0.10831289375911185
iteration : 1938
train acc:  0.7265625
train loss:  0.47221440076828003
train gradient:  0.12308928987485557
iteration : 1939
train acc:  0.7265625
train loss:  0.4799760580062866
train gradient:  0.10457320403315662
iteration : 1940
train acc:  0.671875
train loss:  0.5436192750930786
train gradient:  0.14484664488176222
iteration : 1941
train acc:  0.7421875
train loss:  0.4720580577850342
train gradient:  0.09976221919414052
iteration : 1942
train acc:  0.71875
train loss:  0.5179562568664551
train gradient:  0.12540481875756054
iteration : 1943
train acc:  0.734375
train loss:  0.4904499053955078
train gradient:  0.12241579219282996
iteration : 1944
train acc:  0.7578125
train loss:  0.4972684979438782
train gradient:  0.09470369998783837
iteration : 1945
train acc:  0.703125
train loss:  0.5418466329574585
train gradient:  0.14210666072065747
iteration : 1946
train acc:  0.7734375
train loss:  0.445970743894577
train gradient:  0.09495695115682923
iteration : 1947
train acc:  0.75
train loss:  0.5021225214004517
train gradient:  0.1272821581583268
iteration : 1948
train acc:  0.7578125
train loss:  0.4834041893482208
train gradient:  0.09749604157730525
iteration : 1949
train acc:  0.734375
train loss:  0.46249985694885254
train gradient:  0.0944627190019113
iteration : 1950
train acc:  0.71875
train loss:  0.5517603158950806
train gradient:  0.1730373222079088
iteration : 1951
train acc:  0.703125
train loss:  0.5014055967330933
train gradient:  0.11346687329240972
iteration : 1952
train acc:  0.734375
train loss:  0.4797208607196808
train gradient:  0.11396408223168178
iteration : 1953
train acc:  0.7421875
train loss:  0.5175228714942932
train gradient:  0.12194693563875139
iteration : 1954
train acc:  0.7265625
train loss:  0.44396859407424927
train gradient:  0.09361284524421404
iteration : 1955
train acc:  0.78125
train loss:  0.43044403195381165
train gradient:  0.08729789896370613
iteration : 1956
train acc:  0.6875
train loss:  0.511236310005188
train gradient:  0.13558522986989563
iteration : 1957
train acc:  0.7421875
train loss:  0.47997167706489563
train gradient:  0.1150906715634236
iteration : 1958
train acc:  0.734375
train loss:  0.5100326538085938
train gradient:  0.14131092954812347
iteration : 1959
train acc:  0.796875
train loss:  0.45475733280181885
train gradient:  0.12788162029007605
iteration : 1960
train acc:  0.75
train loss:  0.48353326320648193
train gradient:  0.12371513831639147
iteration : 1961
train acc:  0.828125
train loss:  0.3897920846939087
train gradient:  0.1004966085056332
iteration : 1962
train acc:  0.796875
train loss:  0.412798672914505
train gradient:  0.08555931268500373
iteration : 1963
train acc:  0.7890625
train loss:  0.4205286502838135
train gradient:  0.08972044755759903
iteration : 1964
train acc:  0.7890625
train loss:  0.4750842750072479
train gradient:  0.0984854298054063
iteration : 1965
train acc:  0.734375
train loss:  0.47725582122802734
train gradient:  0.12759928550325617
iteration : 1966
train acc:  0.7890625
train loss:  0.43888914585113525
train gradient:  0.10516360429314253
iteration : 1967
train acc:  0.6875
train loss:  0.5664328336715698
train gradient:  0.18928124142989247
iteration : 1968
train acc:  0.7421875
train loss:  0.47963929176330566
train gradient:  0.14746727545150906
iteration : 1969
train acc:  0.7578125
train loss:  0.4357432723045349
train gradient:  0.08490569171859647
iteration : 1970
train acc:  0.6640625
train loss:  0.5557525753974915
train gradient:  0.1797140277207868
iteration : 1971
train acc:  0.8046875
train loss:  0.45297810435295105
train gradient:  0.0963117084714659
iteration : 1972
train acc:  0.8046875
train loss:  0.43905380368232727
train gradient:  0.09449847375282909
iteration : 1973
train acc:  0.75
train loss:  0.49747389554977417
train gradient:  0.11529763329729632
iteration : 1974
train acc:  0.6953125
train loss:  0.5206490755081177
train gradient:  0.13347362503975368
iteration : 1975
train acc:  0.7734375
train loss:  0.4381884038448334
train gradient:  0.0949620960778115
iteration : 1976
train acc:  0.6875
train loss:  0.5304169654846191
train gradient:  0.1493364897263954
iteration : 1977
train acc:  0.75
train loss:  0.4436774253845215
train gradient:  0.11618637338857755
iteration : 1978
train acc:  0.8515625
train loss:  0.3638126850128174
train gradient:  0.08820573024589747
iteration : 1979
train acc:  0.703125
train loss:  0.49760720133781433
train gradient:  0.12251392438562617
iteration : 1980
train acc:  0.7421875
train loss:  0.49118804931640625
train gradient:  0.13461619058299612
iteration : 1981
train acc:  0.75
train loss:  0.4641299843788147
train gradient:  0.12395850701944808
iteration : 1982
train acc:  0.71875
train loss:  0.47740381956100464
train gradient:  0.11372691806274061
iteration : 1983
train acc:  0.7109375
train loss:  0.49826350808143616
train gradient:  0.11107773503618341
iteration : 1984
train acc:  0.7421875
train loss:  0.5396584272384644
train gradient:  0.1271656974673281
iteration : 1985
train acc:  0.7421875
train loss:  0.48949670791625977
train gradient:  0.16794686279984272
iteration : 1986
train acc:  0.765625
train loss:  0.4486592411994934
train gradient:  0.12819652643866908
iteration : 1987
train acc:  0.703125
train loss:  0.5487642884254456
train gradient:  0.240162314525773
iteration : 1988
train acc:  0.7265625
train loss:  0.534693717956543
train gradient:  0.14286614746108994
iteration : 1989
train acc:  0.7890625
train loss:  0.413404643535614
train gradient:  0.06736397095354672
iteration : 1990
train acc:  0.8125
train loss:  0.41265252232551575
train gradient:  0.11494439560157585
iteration : 1991
train acc:  0.7734375
train loss:  0.4697147607803345
train gradient:  0.11244108550918241
iteration : 1992
train acc:  0.7890625
train loss:  0.46316203474998474
train gradient:  0.1287474516881815
iteration : 1993
train acc:  0.6875
train loss:  0.504576563835144
train gradient:  0.12659097428170368
iteration : 1994
train acc:  0.7578125
train loss:  0.4878445267677307
train gradient:  0.1445525085730474
iteration : 1995
train acc:  0.796875
train loss:  0.40628600120544434
train gradient:  0.1358460359334575
iteration : 1996
train acc:  0.7578125
train loss:  0.44356125593185425
train gradient:  0.10928795213311282
iteration : 1997
train acc:  0.859375
train loss:  0.3845231831073761
train gradient:  0.089173747980378
iteration : 1998
train acc:  0.7890625
train loss:  0.4384145438671112
train gradient:  0.11254243457432131
iteration : 1999
train acc:  0.8046875
train loss:  0.40514957904815674
train gradient:  0.08152486662732107
iteration : 2000
train acc:  0.734375
train loss:  0.4926549792289734
train gradient:  0.10393858140043225
iteration : 2001
train acc:  0.671875
train loss:  0.5699673891067505
train gradient:  0.13646733206316383
iteration : 2002
train acc:  0.75
train loss:  0.496582955121994
train gradient:  0.11589922660511699
iteration : 2003
train acc:  0.7421875
train loss:  0.4637082815170288
train gradient:  0.11632046449593524
iteration : 2004
train acc:  0.8203125
train loss:  0.41541311144828796
train gradient:  0.11650633677030711
iteration : 2005
train acc:  0.734375
train loss:  0.4656398296356201
train gradient:  0.14025198442701903
iteration : 2006
train acc:  0.7734375
train loss:  0.47993162274360657
train gradient:  0.1477406554689103
iteration : 2007
train acc:  0.75
train loss:  0.4695553779602051
train gradient:  0.09498415820815986
iteration : 2008
train acc:  0.828125
train loss:  0.40279003977775574
train gradient:  0.0857992132878899
iteration : 2009
train acc:  0.75
train loss:  0.48754286766052246
train gradient:  0.12184553236108103
iteration : 2010
train acc:  0.7578125
train loss:  0.5229766368865967
train gradient:  0.1595338991890206
iteration : 2011
train acc:  0.71875
train loss:  0.5229289531707764
train gradient:  0.13390989070976372
iteration : 2012
train acc:  0.71875
train loss:  0.48009878396987915
train gradient:  0.14096342216358615
iteration : 2013
train acc:  0.828125
train loss:  0.33815592527389526
train gradient:  0.06763212551366103
iteration : 2014
train acc:  0.75
train loss:  0.5048401951789856
train gradient:  0.12445963872386882
iteration : 2015
train acc:  0.75
train loss:  0.4799051284790039
train gradient:  0.12307773333302624
iteration : 2016
train acc:  0.7890625
train loss:  0.4368210434913635
train gradient:  0.11488225823271128
iteration : 2017
train acc:  0.75
train loss:  0.4532606601715088
train gradient:  0.11763594778193202
iteration : 2018
train acc:  0.8046875
train loss:  0.4632987380027771
train gradient:  0.10820245831828981
iteration : 2019
train acc:  0.734375
train loss:  0.5092479586601257
train gradient:  0.12978663094274784
iteration : 2020
train acc:  0.765625
train loss:  0.46634864807128906
train gradient:  0.12793597002363613
iteration : 2021
train acc:  0.6953125
train loss:  0.5320774912834167
train gradient:  0.12444696267436493
iteration : 2022
train acc:  0.6875
train loss:  0.5048970580101013
train gradient:  0.15893239071210952
iteration : 2023
train acc:  0.7890625
train loss:  0.4355887770652771
train gradient:  0.10529456272028559
iteration : 2024
train acc:  0.7734375
train loss:  0.5051138997077942
train gradient:  0.13249799835972478
iteration : 2025
train acc:  0.78125
train loss:  0.43453454971313477
train gradient:  0.1079046820499229
iteration : 2026
train acc:  0.78125
train loss:  0.4336954355239868
train gradient:  0.10262253634772375
iteration : 2027
train acc:  0.7890625
train loss:  0.41684168577194214
train gradient:  0.08758122379231374
iteration : 2028
train acc:  0.765625
train loss:  0.46856990456581116
train gradient:  0.12092758274268643
iteration : 2029
train acc:  0.796875
train loss:  0.48255178332328796
train gradient:  0.11997515030995234
iteration : 2030
train acc:  0.71875
train loss:  0.5347573757171631
train gradient:  0.15347324581570254
iteration : 2031
train acc:  0.765625
train loss:  0.5032720565795898
train gradient:  0.15120102891205373
iteration : 2032
train acc:  0.8046875
train loss:  0.44465312361717224
train gradient:  0.10865857806027004
iteration : 2033
train acc:  0.8125
train loss:  0.4317339062690735
train gradient:  0.11359530778795035
iteration : 2034
train acc:  0.75
train loss:  0.4685247838497162
train gradient:  0.11662071576249429
iteration : 2035
train acc:  0.7421875
train loss:  0.4579954147338867
train gradient:  0.0929679873109415
iteration : 2036
train acc:  0.7578125
train loss:  0.5003544688224792
train gradient:  0.1289944180510499
iteration : 2037
train acc:  0.6796875
train loss:  0.6228591203689575
train gradient:  0.21292779801123768
iteration : 2038
train acc:  0.6953125
train loss:  0.5709906816482544
train gradient:  0.13795139876122686
iteration : 2039
train acc:  0.7265625
train loss:  0.5243003964424133
train gradient:  0.1380260738626643
iteration : 2040
train acc:  0.734375
train loss:  0.47431328892707825
train gradient:  0.1342277344851649
iteration : 2041
train acc:  0.7890625
train loss:  0.4136885404586792
train gradient:  0.0934960440578818
iteration : 2042
train acc:  0.796875
train loss:  0.4298255741596222
train gradient:  0.09623460425133937
iteration : 2043
train acc:  0.765625
train loss:  0.4413241147994995
train gradient:  0.08282803270706623
iteration : 2044
train acc:  0.765625
train loss:  0.4729229211807251
train gradient:  0.11423970669228306
iteration : 2045
train acc:  0.734375
train loss:  0.46692439913749695
train gradient:  0.10557145154724236
iteration : 2046
train acc:  0.765625
train loss:  0.41985616087913513
train gradient:  0.09324690590960164
iteration : 2047
train acc:  0.7421875
train loss:  0.47792667150497437
train gradient:  0.1481670678776133
iteration : 2048
train acc:  0.7265625
train loss:  0.5029438734054565
train gradient:  0.12770015320573322
iteration : 2049
train acc:  0.7421875
train loss:  0.4413091838359833
train gradient:  0.11265532919593577
iteration : 2050
train acc:  0.703125
train loss:  0.5262657403945923
train gradient:  0.14153380946772467
iteration : 2051
train acc:  0.796875
train loss:  0.43617427349090576
train gradient:  0.09530249754749337
iteration : 2052
train acc:  0.796875
train loss:  0.4034275412559509
train gradient:  0.09592560819316091
iteration : 2053
train acc:  0.7578125
train loss:  0.501120924949646
train gradient:  0.12293861161888896
iteration : 2054
train acc:  0.75
train loss:  0.44888582825660706
train gradient:  0.09467487858005362
iteration : 2055
train acc:  0.7421875
train loss:  0.46790364384651184
train gradient:  0.111676791546127
iteration : 2056
train acc:  0.78125
train loss:  0.39408835768699646
train gradient:  0.07439022901330793
iteration : 2057
train acc:  0.7734375
train loss:  0.4593920111656189
train gradient:  0.0991890768173814
iteration : 2058
train acc:  0.75
train loss:  0.5227707624435425
train gradient:  0.13995220708008982
iteration : 2059
train acc:  0.7421875
train loss:  0.533599853515625
train gradient:  0.152543933917134
iteration : 2060
train acc:  0.7734375
train loss:  0.42195960879325867
train gradient:  0.10432354648392797
iteration : 2061
train acc:  0.71875
train loss:  0.48679882287979126
train gradient:  0.1504940310795238
iteration : 2062
train acc:  0.7890625
train loss:  0.4340970516204834
train gradient:  0.10912301788008784
iteration : 2063
train acc:  0.796875
train loss:  0.4290516972541809
train gradient:  0.0907312531040749
iteration : 2064
train acc:  0.7890625
train loss:  0.47632327675819397
train gradient:  0.09875277276208547
iteration : 2065
train acc:  0.828125
train loss:  0.3872784376144409
train gradient:  0.09521651552652975
iteration : 2066
train acc:  0.7421875
train loss:  0.45308980345726013
train gradient:  0.13569661362367577
iteration : 2067
train acc:  0.765625
train loss:  0.5109298229217529
train gradient:  0.12537199629249596
iteration : 2068
train acc:  0.75
train loss:  0.4548630714416504
train gradient:  0.11304300701959576
iteration : 2069
train acc:  0.765625
train loss:  0.47830986976623535
train gradient:  0.12217689415745447
iteration : 2070
train acc:  0.765625
train loss:  0.4438343942165375
train gradient:  0.1063176020230065
iteration : 2071
train acc:  0.703125
train loss:  0.44181665778160095
train gradient:  0.110313620233311
iteration : 2072
train acc:  0.6953125
train loss:  0.5391956567764282
train gradient:  0.1239281391841532
iteration : 2073
train acc:  0.703125
train loss:  0.541581392288208
train gradient:  0.11892635731058313
iteration : 2074
train acc:  0.703125
train loss:  0.5411840081214905
train gradient:  0.1626902634210489
iteration : 2075
train acc:  0.78125
train loss:  0.4602767825126648
train gradient:  0.11929571097936392
iteration : 2076
train acc:  0.7734375
train loss:  0.4788297414779663
train gradient:  0.122901833713552
iteration : 2077
train acc:  0.7578125
train loss:  0.49323660135269165
train gradient:  0.1065847839605235
iteration : 2078
train acc:  0.8046875
train loss:  0.4216539263725281
train gradient:  0.11481605401616242
iteration : 2079
train acc:  0.8125
train loss:  0.45930472016334534
train gradient:  0.09380940655243929
iteration : 2080
train acc:  0.75
train loss:  0.5141556262969971
train gradient:  0.1456500691932826
iteration : 2081
train acc:  0.8046875
train loss:  0.38823598623275757
train gradient:  0.0799813774182128
iteration : 2082
train acc:  0.7421875
train loss:  0.47285765409469604
train gradient:  0.1304019490826282
iteration : 2083
train acc:  0.7734375
train loss:  0.4825219511985779
train gradient:  0.11272603249614611
iteration : 2084
train acc:  0.765625
train loss:  0.4822576642036438
train gradient:  0.1347790100438952
iteration : 2085
train acc:  0.71875
train loss:  0.48148155212402344
train gradient:  0.12306942334487411
iteration : 2086
train acc:  0.65625
train loss:  0.5577712059020996
train gradient:  0.1473350773349985
iteration : 2087
train acc:  0.8203125
train loss:  0.4285871088504791
train gradient:  0.09484887696937616
iteration : 2088
train acc:  0.71875
train loss:  0.5170710682868958
train gradient:  0.13428840819925533
iteration : 2089
train acc:  0.765625
train loss:  0.4629923403263092
train gradient:  0.09351519996589819
iteration : 2090
train acc:  0.7421875
train loss:  0.4582578241825104
train gradient:  0.10685626212841066
iteration : 2091
train acc:  0.75
train loss:  0.4572219252586365
train gradient:  0.1087410377655123
iteration : 2092
train acc:  0.7109375
train loss:  0.48168569803237915
train gradient:  0.14062043429145193
iteration : 2093
train acc:  0.7109375
train loss:  0.5302145481109619
train gradient:  0.1541957839048079
iteration : 2094
train acc:  0.6484375
train loss:  0.5582442283630371
train gradient:  0.1633817103835003
iteration : 2095
train acc:  0.7734375
train loss:  0.4776810109615326
train gradient:  0.13143756298886194
iteration : 2096
train acc:  0.7578125
train loss:  0.4124719500541687
train gradient:  0.09368729019724538
iteration : 2097
train acc:  0.71875
train loss:  0.47441214323043823
train gradient:  0.11385301052487867
iteration : 2098
train acc:  0.7578125
train loss:  0.44725143909454346
train gradient:  0.11441884251495524
iteration : 2099
train acc:  0.7265625
train loss:  0.4762688875198364
train gradient:  0.13284549270497464
iteration : 2100
train acc:  0.78125
train loss:  0.44413986802101135
train gradient:  0.09070559875310506
iteration : 2101
train acc:  0.7265625
train loss:  0.44490545988082886
train gradient:  0.08925389761119602
iteration : 2102
train acc:  0.796875
train loss:  0.4779272675514221
train gradient:  0.13643455346490407
iteration : 2103
train acc:  0.8359375
train loss:  0.42933881282806396
train gradient:  0.09427992131497838
iteration : 2104
train acc:  0.71875
train loss:  0.5282484292984009
train gradient:  0.15009389503040826
iteration : 2105
train acc:  0.78125
train loss:  0.4699501693248749
train gradient:  0.10801989023195481
iteration : 2106
train acc:  0.6875
train loss:  0.5713150501251221
train gradient:  0.1572584232876112
iteration : 2107
train acc:  0.78125
train loss:  0.45457157492637634
train gradient:  0.12150327286389194
iteration : 2108
train acc:  0.7578125
train loss:  0.4940727949142456
train gradient:  0.13481219220237245
iteration : 2109
train acc:  0.75
train loss:  0.44206011295318604
train gradient:  0.10441671453697514
iteration : 2110
train acc:  0.75
train loss:  0.5054076910018921
train gradient:  0.12793751051605465
iteration : 2111
train acc:  0.703125
train loss:  0.47301703691482544
train gradient:  0.11332458960818143
iteration : 2112
train acc:  0.75
train loss:  0.4838521480560303
train gradient:  0.1039701761614114
iteration : 2113
train acc:  0.7734375
train loss:  0.4450058341026306
train gradient:  0.13607914596739878
iteration : 2114
train acc:  0.8125
train loss:  0.42084091901779175
train gradient:  0.10129528573593442
iteration : 2115
train acc:  0.765625
train loss:  0.444087952375412
train gradient:  0.09181585812689308
iteration : 2116
train acc:  0.8046875
train loss:  0.41634273529052734
train gradient:  0.09744591375493057
iteration : 2117
train acc:  0.734375
train loss:  0.5071269869804382
train gradient:  0.11677545532172057
iteration : 2118
train acc:  0.7890625
train loss:  0.4468914270401001
train gradient:  0.11210683684170542
iteration : 2119
train acc:  0.6953125
train loss:  0.5063561201095581
train gradient:  0.11131287415982764
iteration : 2120
train acc:  0.7578125
train loss:  0.4958338141441345
train gradient:  0.14121301401840405
iteration : 2121
train acc:  0.75
train loss:  0.47177359461784363
train gradient:  0.12797483559024914
iteration : 2122
train acc:  0.78125
train loss:  0.4241502583026886
train gradient:  0.0869226068752235
iteration : 2123
train acc:  0.7890625
train loss:  0.43882933259010315
train gradient:  0.09664112247627776
iteration : 2124
train acc:  0.796875
train loss:  0.45973822474479675
train gradient:  0.11806957035128308
iteration : 2125
train acc:  0.6953125
train loss:  0.5238081812858582
train gradient:  0.1244583346097583
iteration : 2126
train acc:  0.7421875
train loss:  0.48719412088394165
train gradient:  0.10869852745264763
iteration : 2127
train acc:  0.703125
train loss:  0.5131163001060486
train gradient:  0.11964614599729637
iteration : 2128
train acc:  0.703125
train loss:  0.563395082950592
train gradient:  0.17929512535241449
iteration : 2129
train acc:  0.71875
train loss:  0.49897852540016174
train gradient:  0.14640101783886733
iteration : 2130
train acc:  0.765625
train loss:  0.44753965735435486
train gradient:  0.11747061105015902
iteration : 2131
train acc:  0.859375
train loss:  0.3769828975200653
train gradient:  0.05949506825693335
iteration : 2132
train acc:  0.7265625
train loss:  0.47215571999549866
train gradient:  0.1336194404976754
iteration : 2133
train acc:  0.8046875
train loss:  0.4111706018447876
train gradient:  0.10496611090744272
iteration : 2134
train acc:  0.765625
train loss:  0.43686550855636597
train gradient:  0.09849485408240499
iteration : 2135
train acc:  0.78125
train loss:  0.49687913060188293
train gradient:  0.11834783261799162
iteration : 2136
train acc:  0.703125
train loss:  0.5575422048568726
train gradient:  0.13287429398410294
iteration : 2137
train acc:  0.78125
train loss:  0.4207979440689087
train gradient:  0.08949269600923404
iteration : 2138
train acc:  0.7890625
train loss:  0.4204561710357666
train gradient:  0.10376896107562625
iteration : 2139
train acc:  0.7578125
train loss:  0.4950792193412781
train gradient:  0.13320142574222196
iteration : 2140
train acc:  0.703125
train loss:  0.5271207690238953
train gradient:  0.13780633653319388
iteration : 2141
train acc:  0.7265625
train loss:  0.46622413396835327
train gradient:  0.09885368749701669
iteration : 2142
train acc:  0.703125
train loss:  0.5080157518386841
train gradient:  0.14415485298880354
iteration : 2143
train acc:  0.8203125
train loss:  0.3788336515426636
train gradient:  0.08715705139125464
iteration : 2144
train acc:  0.7890625
train loss:  0.4277177155017853
train gradient:  0.09969949483663611
iteration : 2145
train acc:  0.71875
train loss:  0.5531392097473145
train gradient:  0.13679456889159838
iteration : 2146
train acc:  0.703125
train loss:  0.5388528108596802
train gradient:  0.13615279434756516
iteration : 2147
train acc:  0.7109375
train loss:  0.5095683932304382
train gradient:  0.16882285404738956
iteration : 2148
train acc:  0.7265625
train loss:  0.48727840185165405
train gradient:  0.11061763327267816
iteration : 2149
train acc:  0.734375
train loss:  0.49238479137420654
train gradient:  0.12060815015246704
iteration : 2150
train acc:  0.7109375
train loss:  0.5064937472343445
train gradient:  0.15204938548422825
iteration : 2151
train acc:  0.75
train loss:  0.5048079490661621
train gradient:  0.11830201859863078
iteration : 2152
train acc:  0.8125
train loss:  0.5032027363777161
train gradient:  0.13547303301331326
iteration : 2153
train acc:  0.7890625
train loss:  0.4860934019088745
train gradient:  0.13651570373932684
iteration : 2154
train acc:  0.71875
train loss:  0.5081459879875183
train gradient:  0.0972451307785132
iteration : 2155
train acc:  0.7578125
train loss:  0.4700879752635956
train gradient:  0.12391306786816973
iteration : 2156
train acc:  0.7421875
train loss:  0.46373575925827026
train gradient:  0.09757603569150289
iteration : 2157
train acc:  0.71875
train loss:  0.4536302983760834
train gradient:  0.09288894675535335
iteration : 2158
train acc:  0.7578125
train loss:  0.4819188416004181
train gradient:  0.11283964566048832
iteration : 2159
train acc:  0.7109375
train loss:  0.5904815196990967
train gradient:  0.16903841439553177
iteration : 2160
train acc:  0.765625
train loss:  0.4488670825958252
train gradient:  0.11205133801121753
iteration : 2161
train acc:  0.75
train loss:  0.4424903392791748
train gradient:  0.09848630053570234
iteration : 2162
train acc:  0.7265625
train loss:  0.5060684680938721
train gradient:  0.12430969286722206
iteration : 2163
train acc:  0.703125
train loss:  0.5185571312904358
train gradient:  0.16944805429148707
iteration : 2164
train acc:  0.7578125
train loss:  0.47266334295272827
train gradient:  0.09854087435049695
iteration : 2165
train acc:  0.75
train loss:  0.5053979754447937
train gradient:  0.13249273803902137
iteration : 2166
train acc:  0.7109375
train loss:  0.5146327018737793
train gradient:  0.14994605374478667
iteration : 2167
train acc:  0.7578125
train loss:  0.4582448899745941
train gradient:  0.11154027271080667
iteration : 2168
train acc:  0.7578125
train loss:  0.4943961203098297
train gradient:  0.12794020709384496
iteration : 2169
train acc:  0.734375
train loss:  0.5013731122016907
train gradient:  0.15008628559835252
iteration : 2170
train acc:  0.6796875
train loss:  0.5313851833343506
train gradient:  0.13510537185724258
iteration : 2171
train acc:  0.7109375
train loss:  0.5598081946372986
train gradient:  0.1704802062010396
iteration : 2172
train acc:  0.796875
train loss:  0.41015565395355225
train gradient:  0.07556497981166331
iteration : 2173
train acc:  0.7578125
train loss:  0.5417252779006958
train gradient:  0.18809301123136274
iteration : 2174
train acc:  0.75
train loss:  0.5084384679794312
train gradient:  0.13320506511710467
iteration : 2175
train acc:  0.765625
train loss:  0.449080228805542
train gradient:  0.11242133103199187
iteration : 2176
train acc:  0.7734375
train loss:  0.4455636143684387
train gradient:  0.0932240121090372
iteration : 2177
train acc:  0.7578125
train loss:  0.4850822687149048
train gradient:  0.15115337565890136
iteration : 2178
train acc:  0.78125
train loss:  0.4992092549800873
train gradient:  0.1294186947030685
iteration : 2179
train acc:  0.6640625
train loss:  0.5209530591964722
train gradient:  0.145117045739207
iteration : 2180
train acc:  0.78125
train loss:  0.4182334542274475
train gradient:  0.09112379381519452
iteration : 2181
train acc:  0.7890625
train loss:  0.4794923663139343
train gradient:  0.13102587664453497
iteration : 2182
train acc:  0.71875
train loss:  0.5042111873626709
train gradient:  0.14372737026567237
iteration : 2183
train acc:  0.796875
train loss:  0.44698840379714966
train gradient:  0.08366295294065874
iteration : 2184
train acc:  0.7421875
train loss:  0.45282307267189026
train gradient:  0.10446334267219205
iteration : 2185
train acc:  0.734375
train loss:  0.5002239346504211
train gradient:  0.10791467002721954
iteration : 2186
train acc:  0.75
train loss:  0.46401485800743103
train gradient:  0.09388646795694733
iteration : 2187
train acc:  0.7734375
train loss:  0.4500093162059784
train gradient:  0.11457870257102977
iteration : 2188
train acc:  0.7734375
train loss:  0.4798659682273865
train gradient:  0.13505940549285445
iteration : 2189
train acc:  0.703125
train loss:  0.44751402735710144
train gradient:  0.10497317052591588
iteration : 2190
train acc:  0.7734375
train loss:  0.4860996901988983
train gradient:  0.1078796978096489
iteration : 2191
train acc:  0.78125
train loss:  0.4599258303642273
train gradient:  0.10067678576725637
iteration : 2192
train acc:  0.8046875
train loss:  0.4379367232322693
train gradient:  0.0847886872854756
iteration : 2193
train acc:  0.734375
train loss:  0.46251773834228516
train gradient:  0.10600887776075454
iteration : 2194
train acc:  0.6953125
train loss:  0.47198593616485596
train gradient:  0.1297440191108512
iteration : 2195
train acc:  0.796875
train loss:  0.42468899488449097
train gradient:  0.07628212353190016
iteration : 2196
train acc:  0.75
train loss:  0.49035969376564026
train gradient:  0.12046426789380615
iteration : 2197
train acc:  0.8203125
train loss:  0.4625944495201111
train gradient:  0.11499311052570749
iteration : 2198
train acc:  0.65625
train loss:  0.5337799191474915
train gradient:  0.16099978906359375
iteration : 2199
train acc:  0.7421875
train loss:  0.486444354057312
train gradient:  0.12996784302093425
iteration : 2200
train acc:  0.7890625
train loss:  0.5472933650016785
train gradient:  0.18198638723008503
iteration : 2201
train acc:  0.7734375
train loss:  0.44640713930130005
train gradient:  0.09520189517226815
iteration : 2202
train acc:  0.71875
train loss:  0.537787914276123
train gradient:  0.12935061859947516
iteration : 2203
train acc:  0.7265625
train loss:  0.4878993332386017
train gradient:  0.11937529711349161
iteration : 2204
train acc:  0.765625
train loss:  0.479615718126297
train gradient:  0.13602264338298292
iteration : 2205
train acc:  0.796875
train loss:  0.4245617389678955
train gradient:  0.09760878196472425
iteration : 2206
train acc:  0.796875
train loss:  0.42854878306388855
train gradient:  0.10279977888610774
iteration : 2207
train acc:  0.75
train loss:  0.49861273169517517
train gradient:  0.1179631125261812
iteration : 2208
train acc:  0.765625
train loss:  0.45017462968826294
train gradient:  0.10795186015231649
iteration : 2209
train acc:  0.734375
train loss:  0.45456284284591675
train gradient:  0.10579246429131145
iteration : 2210
train acc:  0.734375
train loss:  0.552206814289093
train gradient:  0.14981543180660412
iteration : 2211
train acc:  0.7265625
train loss:  0.4888201653957367
train gradient:  0.10702704688754382
iteration : 2212
train acc:  0.734375
train loss:  0.5248401165008545
train gradient:  0.12442417855155861
iteration : 2213
train acc:  0.6953125
train loss:  0.49547943472862244
train gradient:  0.11396138433116235
iteration : 2214
train acc:  0.7421875
train loss:  0.5295098423957825
train gradient:  0.131271666923578
iteration : 2215
train acc:  0.7265625
train loss:  0.5593153238296509
train gradient:  0.14631016369176114
iteration : 2216
train acc:  0.6796875
train loss:  0.557086169719696
train gradient:  0.13510777621010212
iteration : 2217
train acc:  0.7421875
train loss:  0.5183953642845154
train gradient:  0.13695021105827926
iteration : 2218
train acc:  0.734375
train loss:  0.4787536859512329
train gradient:  0.11539104226691654
iteration : 2219
train acc:  0.75
train loss:  0.49062949419021606
train gradient:  0.11770509383633725
iteration : 2220
train acc:  0.765625
train loss:  0.4585101306438446
train gradient:  0.1216545571385325
iteration : 2221
train acc:  0.75
train loss:  0.4858037829399109
train gradient:  0.11652392171681103
iteration : 2222
train acc:  0.7578125
train loss:  0.4495593309402466
train gradient:  0.10740949596323844
iteration : 2223
train acc:  0.734375
train loss:  0.5054645538330078
train gradient:  0.156240997559645
iteration : 2224
train acc:  0.7265625
train loss:  0.4899086356163025
train gradient:  0.11193542933992495
iteration : 2225
train acc:  0.765625
train loss:  0.48539865016937256
train gradient:  0.14721193468625315
iteration : 2226
train acc:  0.8125
train loss:  0.4234725534915924
train gradient:  0.10833617101811265
iteration : 2227
train acc:  0.7578125
train loss:  0.4558209180831909
train gradient:  0.11307997691541997
iteration : 2228
train acc:  0.796875
train loss:  0.4756466746330261
train gradient:  0.08909121548486344
iteration : 2229
train acc:  0.71875
train loss:  0.5316442251205444
train gradient:  0.1450457837678955
iteration : 2230
train acc:  0.7421875
train loss:  0.4755709767341614
train gradient:  0.15744611428296112
iteration : 2231
train acc:  0.8046875
train loss:  0.4229625463485718
train gradient:  0.10150666016076869
iteration : 2232
train acc:  0.7265625
train loss:  0.4995771050453186
train gradient:  0.10580882569966545
iteration : 2233
train acc:  0.734375
train loss:  0.4627763032913208
train gradient:  0.10076357462355638
iteration : 2234
train acc:  0.765625
train loss:  0.5002962350845337
train gradient:  0.12048894260681595
iteration : 2235
train acc:  0.765625
train loss:  0.5055006146430969
train gradient:  0.12206260924627178
iteration : 2236
train acc:  0.7265625
train loss:  0.5298119187355042
train gradient:  0.12740545201747946
iteration : 2237
train acc:  0.765625
train loss:  0.478434681892395
train gradient:  0.10021486542012036
iteration : 2238
train acc:  0.75
train loss:  0.49503833055496216
train gradient:  0.12193331729471794
iteration : 2239
train acc:  0.75
train loss:  0.44446811079978943
train gradient:  0.10618373757345763
iteration : 2240
train acc:  0.765625
train loss:  0.5352211594581604
train gradient:  0.1826899647409873
iteration : 2241
train acc:  0.828125
train loss:  0.3985351026058197
train gradient:  0.08604960260345826
iteration : 2242
train acc:  0.7421875
train loss:  0.4763050377368927
train gradient:  0.11960451322923998
iteration : 2243
train acc:  0.78125
train loss:  0.44727206230163574
train gradient:  0.12112459087531915
iteration : 2244
train acc:  0.796875
train loss:  0.41627562046051025
train gradient:  0.08649399171122087
iteration : 2245
train acc:  0.78125
train loss:  0.418947696685791
train gradient:  0.09200365267724488
iteration : 2246
train acc:  0.7265625
train loss:  0.5032001733779907
train gradient:  0.16450730649647421
iteration : 2247
train acc:  0.78125
train loss:  0.4238559603691101
train gradient:  0.09885075222575014
iteration : 2248
train acc:  0.7734375
train loss:  0.5068805813789368
train gradient:  0.13245310902801408
iteration : 2249
train acc:  0.7421875
train loss:  0.49949508905410767
train gradient:  0.11802478896642062
iteration : 2250
train acc:  0.78125
train loss:  0.42240720987319946
train gradient:  0.11929136297159082
iteration : 2251
train acc:  0.75
train loss:  0.48891550302505493
train gradient:  0.10526585435390695
iteration : 2252
train acc:  0.734375
train loss:  0.4663102626800537
train gradient:  0.11472741619334474
iteration : 2253
train acc:  0.7109375
train loss:  0.4822692275047302
train gradient:  0.1343331356732081
iteration : 2254
train acc:  0.7578125
train loss:  0.48155325651168823
train gradient:  0.11993057035809408
iteration : 2255
train acc:  0.6875
train loss:  0.517403244972229
train gradient:  0.14988737665978102
iteration : 2256
train acc:  0.703125
train loss:  0.46251046657562256
train gradient:  0.11236240009956115
iteration : 2257
train acc:  0.765625
train loss:  0.4255164861679077
train gradient:  0.09131538209036066
iteration : 2258
train acc:  0.703125
train loss:  0.5488090515136719
train gradient:  0.1589473833666376
iteration : 2259
train acc:  0.8125
train loss:  0.41106900572776794
train gradient:  0.09249708355208798
iteration : 2260
train acc:  0.78125
train loss:  0.4673808813095093
train gradient:  0.09880588436249191
iteration : 2261
train acc:  0.7109375
train loss:  0.5375018119812012
train gradient:  0.17512197411146882
iteration : 2262
train acc:  0.7265625
train loss:  0.5677019953727722
train gradient:  0.18947754564028305
iteration : 2263
train acc:  0.8046875
train loss:  0.4274096190929413
train gradient:  0.0974926657011234
iteration : 2264
train acc:  0.6953125
train loss:  0.5034854412078857
train gradient:  0.12258197748131679
iteration : 2265
train acc:  0.8046875
train loss:  0.44062450528144836
train gradient:  0.10378149826197318
iteration : 2266
train acc:  0.75
train loss:  0.4401428699493408
train gradient:  0.09667760660787215
iteration : 2267
train acc:  0.7734375
train loss:  0.4820868968963623
train gradient:  0.11383386722399266
iteration : 2268
train acc:  0.7578125
train loss:  0.4688723087310791
train gradient:  0.09730285756735159
iteration : 2269
train acc:  0.75
train loss:  0.4459581971168518
train gradient:  0.10355660833929316
iteration : 2270
train acc:  0.734375
train loss:  0.5338982939720154
train gradient:  0.1743163748897919
iteration : 2271
train acc:  0.71875
train loss:  0.5534920692443848
train gradient:  0.1687334450151804
iteration : 2272
train acc:  0.84375
train loss:  0.4573137164115906
train gradient:  0.15973225723920598
iteration : 2273
train acc:  0.78125
train loss:  0.47411537170410156
train gradient:  0.11664822048779588
iteration : 2274
train acc:  0.7421875
train loss:  0.4774572253227234
train gradient:  0.1063611381365705
iteration : 2275
train acc:  0.765625
train loss:  0.48987990617752075
train gradient:  0.11059083327293003
iteration : 2276
train acc:  0.78125
train loss:  0.428509384393692
train gradient:  0.11442632736104749
iteration : 2277
train acc:  0.7578125
train loss:  0.48521357774734497
train gradient:  0.12146279548027425
iteration : 2278
train acc:  0.7265625
train loss:  0.5182993412017822
train gradient:  0.1480067113449743
iteration : 2279
train acc:  0.71875
train loss:  0.5135926604270935
train gradient:  0.125144136662851
iteration : 2280
train acc:  0.78125
train loss:  0.5415515303611755
train gradient:  0.1612174167312343
iteration : 2281
train acc:  0.7421875
train loss:  0.4872934818267822
train gradient:  0.09622606338049552
iteration : 2282
train acc:  0.7890625
train loss:  0.4488694965839386
train gradient:  0.11357371769591464
iteration : 2283
train acc:  0.6953125
train loss:  0.5211832523345947
train gradient:  0.11807583353773411
iteration : 2284
train acc:  0.765625
train loss:  0.48913121223449707
train gradient:  0.10439501950119423
iteration : 2285
train acc:  0.7890625
train loss:  0.45813268423080444
train gradient:  0.11811451738202486
iteration : 2286
train acc:  0.8203125
train loss:  0.425833523273468
train gradient:  0.1188806844480908
iteration : 2287
train acc:  0.7265625
train loss:  0.47619426250457764
train gradient:  0.13115226040052397
iteration : 2288
train acc:  0.7734375
train loss:  0.4349362850189209
train gradient:  0.10243551279314188
iteration : 2289
train acc:  0.8203125
train loss:  0.44700223207473755
train gradient:  0.09594975615633157
iteration : 2290
train acc:  0.7421875
train loss:  0.4994962811470032
train gradient:  0.12793803394510098
iteration : 2291
train acc:  0.734375
train loss:  0.464204877614975
train gradient:  0.09287095024797229
iteration : 2292
train acc:  0.7109375
train loss:  0.5909051895141602
train gradient:  0.1926491556112315
iteration : 2293
train acc:  0.7578125
train loss:  0.4385772943496704
train gradient:  0.09770234856395642
iteration : 2294
train acc:  0.734375
train loss:  0.4875930845737457
train gradient:  0.1621960419845728
iteration : 2295
train acc:  0.7265625
train loss:  0.4876215159893036
train gradient:  0.1350223030740193
iteration : 2296
train acc:  0.7734375
train loss:  0.4705864489078522
train gradient:  0.12252778061137777
iteration : 2297
train acc:  0.6953125
train loss:  0.5700289011001587
train gradient:  0.13655649406875622
iteration : 2298
train acc:  0.6875
train loss:  0.568002462387085
train gradient:  0.17879952212826467
iteration : 2299
train acc:  0.734375
train loss:  0.514094352722168
train gradient:  0.12251343476077117
iteration : 2300
train acc:  0.7109375
train loss:  0.5315879583358765
train gradient:  0.13008812339531867
iteration : 2301
train acc:  0.7265625
train loss:  0.5224945545196533
train gradient:  0.1105749341014124
iteration : 2302
train acc:  0.78125
train loss:  0.42093420028686523
train gradient:  0.08226328886625167
iteration : 2303
train acc:  0.71875
train loss:  0.5282894968986511
train gradient:  0.1411500486902127
iteration : 2304
train acc:  0.7578125
train loss:  0.45541054010391235
train gradient:  0.10579598103957623
iteration : 2305
train acc:  0.7109375
train loss:  0.5466858148574829
train gradient:  0.12733490063860883
iteration : 2306
train acc:  0.7265625
train loss:  0.5073238611221313
train gradient:  0.2100917264570132
iteration : 2307
train acc:  0.7109375
train loss:  0.5141421556472778
train gradient:  0.12054194789693543
iteration : 2308
train acc:  0.8046875
train loss:  0.39715057611465454
train gradient:  0.08558595865690022
iteration : 2309
train acc:  0.7734375
train loss:  0.42172491550445557
train gradient:  0.09492091753408012
iteration : 2310
train acc:  0.75
train loss:  0.45635178685188293
train gradient:  0.11571189757933607
iteration : 2311
train acc:  0.765625
train loss:  0.4753752648830414
train gradient:  0.12334179093164638
iteration : 2312
train acc:  0.8125
train loss:  0.40609273314476013
train gradient:  0.10298421645691559
iteration : 2313
train acc:  0.7421875
train loss:  0.483919620513916
train gradient:  0.13212742280009465
iteration : 2314
train acc:  0.796875
train loss:  0.49192896485328674
train gradient:  0.1186927487815103
iteration : 2315
train acc:  0.7734375
train loss:  0.48039400577545166
train gradient:  0.10308807539188082
iteration : 2316
train acc:  0.8359375
train loss:  0.40506741404533386
train gradient:  0.0969749227187154
iteration : 2317
train acc:  0.765625
train loss:  0.45111051201820374
train gradient:  0.10583414047111346
iteration : 2318
train acc:  0.796875
train loss:  0.4109290540218353
train gradient:  0.08151953401402602
iteration : 2319
train acc:  0.8046875
train loss:  0.4169124364852905
train gradient:  0.0854429152510651
iteration : 2320
train acc:  0.75
train loss:  0.46525251865386963
train gradient:  0.13265377096823733
iteration : 2321
train acc:  0.71875
train loss:  0.5043209791183472
train gradient:  0.13052618718122236
iteration : 2322
train acc:  0.734375
train loss:  0.5191254615783691
train gradient:  0.11167965138326046
iteration : 2323
train acc:  0.7421875
train loss:  0.42534008622169495
train gradient:  0.09249840503579952
iteration : 2324
train acc:  0.8203125
train loss:  0.37962305545806885
train gradient:  0.06793729722200406
iteration : 2325
train acc:  0.7890625
train loss:  0.49077385663986206
train gradient:  0.1380500794651044
iteration : 2326
train acc:  0.7578125
train loss:  0.4991356134414673
train gradient:  0.11925019721949799
iteration : 2327
train acc:  0.796875
train loss:  0.4133269190788269
train gradient:  0.07773879089920588
iteration : 2328
train acc:  0.796875
train loss:  0.4526370167732239
train gradient:  0.10534824658952853
iteration : 2329
train acc:  0.8125
train loss:  0.43361273407936096
train gradient:  0.08752577910078221
iteration : 2330
train acc:  0.8359375
train loss:  0.45599573850631714
train gradient:  0.09471519556220118
iteration : 2331
train acc:  0.7265625
train loss:  0.5013408064842224
train gradient:  0.15053233156628215
iteration : 2332
train acc:  0.8125
train loss:  0.3907870650291443
train gradient:  0.08019207526853353
iteration : 2333
train acc:  0.7421875
train loss:  0.4494779109954834
train gradient:  0.09487295290433513
iteration : 2334
train acc:  0.7265625
train loss:  0.4736771583557129
train gradient:  0.11290041923342865
iteration : 2335
train acc:  0.7578125
train loss:  0.47850847244262695
train gradient:  0.10518601792169677
iteration : 2336
train acc:  0.6875
train loss:  0.4927385747432709
train gradient:  0.1246539441587671
iteration : 2337
train acc:  0.71875
train loss:  0.5007466077804565
train gradient:  0.1291368538210793
iteration : 2338
train acc:  0.765625
train loss:  0.48211151361465454
train gradient:  0.1227226426518775
iteration : 2339
train acc:  0.7890625
train loss:  0.39663881063461304
train gradient:  0.07765088836130976
iteration : 2340
train acc:  0.8203125
train loss:  0.43056032061576843
train gradient:  0.09825251726774659
iteration : 2341
train acc:  0.7890625
train loss:  0.39848989248275757
train gradient:  0.10105903985211091
iteration : 2342
train acc:  0.765625
train loss:  0.4682861566543579
train gradient:  0.1414956545742319
iteration : 2343
train acc:  0.78125
train loss:  0.4250723123550415
train gradient:  0.08673049211960082
iteration : 2344
train acc:  0.765625
train loss:  0.48221439123153687
train gradient:  0.12815778221520197
iteration : 2345
train acc:  0.6953125
train loss:  0.5011146068572998
train gradient:  0.1378862625908776
iteration : 2346
train acc:  0.75
train loss:  0.46749037504196167
train gradient:  0.11736756932604028
iteration : 2347
train acc:  0.765625
train loss:  0.4961681663990021
train gradient:  0.11828799236767537
iteration : 2348
train acc:  0.734375
train loss:  0.4913502335548401
train gradient:  0.13767291345397292
iteration : 2349
train acc:  0.71875
train loss:  0.5743354558944702
train gradient:  0.17001913862372559
iteration : 2350
train acc:  0.8046875
train loss:  0.43936988711357117
train gradient:  0.10009534858936935
iteration : 2351
train acc:  0.7890625
train loss:  0.4310654103755951
train gradient:  0.11464682543400995
iteration : 2352
train acc:  0.7890625
train loss:  0.44212135672569275
train gradient:  0.11050988985788936
iteration : 2353
train acc:  0.7734375
train loss:  0.4628852903842926
train gradient:  0.1390532372708797
iteration : 2354
train acc:  0.7421875
train loss:  0.522153377532959
train gradient:  0.12564074913413043
iteration : 2355
train acc:  0.7265625
train loss:  0.49562007188796997
train gradient:  0.13182007721633682
iteration : 2356
train acc:  0.7109375
train loss:  0.48389846086502075
train gradient:  0.1348016169259135
iteration : 2357
train acc:  0.765625
train loss:  0.466697633266449
train gradient:  0.1418763048989002
iteration : 2358
train acc:  0.7890625
train loss:  0.4378446936607361
train gradient:  0.11063761493030803
iteration : 2359
train acc:  0.7578125
train loss:  0.4576042890548706
train gradient:  0.11653664542818021
iteration : 2360
train acc:  0.7109375
train loss:  0.5214799642562866
train gradient:  0.15663829830719783
iteration : 2361
train acc:  0.7421875
train loss:  0.5014406442642212
train gradient:  0.11849109527150492
iteration : 2362
train acc:  0.671875
train loss:  0.49404263496398926
train gradient:  0.14729253276949092
iteration : 2363
train acc:  0.7578125
train loss:  0.46203863620758057
train gradient:  0.11602984621796106
iteration : 2364
train acc:  0.7109375
train loss:  0.4786902070045471
train gradient:  0.13179773658473617
iteration : 2365
train acc:  0.7421875
train loss:  0.5041191577911377
train gradient:  0.12764611872279275
iteration : 2366
train acc:  0.7421875
train loss:  0.5130680203437805
train gradient:  0.13296833320823195
iteration : 2367
train acc:  0.7421875
train loss:  0.47161155939102173
train gradient:  0.14435744443563486
iteration : 2368
train acc:  0.7421875
train loss:  0.48287856578826904
train gradient:  0.12954243266834947
iteration : 2369
train acc:  0.765625
train loss:  0.4815053343772888
train gradient:  0.12243287040701653
iteration : 2370
train acc:  0.796875
train loss:  0.421322226524353
train gradient:  0.10822123807590019
iteration : 2371
train acc:  0.703125
train loss:  0.5237104892730713
train gradient:  0.134642494225644
iteration : 2372
train acc:  0.828125
train loss:  0.3956789970397949
train gradient:  0.0995324348533317
iteration : 2373
train acc:  0.7421875
train loss:  0.49885135889053345
train gradient:  0.12455770780802157
iteration : 2374
train acc:  0.7890625
train loss:  0.4424784183502197
train gradient:  0.11150335634624292
iteration : 2375
train acc:  0.7109375
train loss:  0.5262212753295898
train gradient:  0.19324335893710712
iteration : 2376
train acc:  0.734375
train loss:  0.48308518528938293
train gradient:  0.11832970687962585
iteration : 2377
train acc:  0.8046875
train loss:  0.40640079975128174
train gradient:  0.1014949393891949
iteration : 2378
train acc:  0.734375
train loss:  0.5059723854064941
train gradient:  0.11735885884031452
iteration : 2379
train acc:  0.65625
train loss:  0.5256004333496094
train gradient:  0.15184590209539423
iteration : 2380
train acc:  0.7734375
train loss:  0.5443841814994812
train gradient:  0.1904374630792872
iteration : 2381
train acc:  0.7734375
train loss:  0.46301400661468506
train gradient:  0.09969283927610696
iteration : 2382
train acc:  0.78125
train loss:  0.4330960512161255
train gradient:  0.11819486722519859
iteration : 2383
train acc:  0.765625
train loss:  0.5056002140045166
train gradient:  0.1274442629462417
iteration : 2384
train acc:  0.6796875
train loss:  0.5579545497894287
train gradient:  0.1629221415598119
iteration : 2385
train acc:  0.75
train loss:  0.48322969675064087
train gradient:  0.15668293144412018
iteration : 2386
train acc:  0.7109375
train loss:  0.5547780990600586
train gradient:  0.14317780441757172
iteration : 2387
train acc:  0.7734375
train loss:  0.500636100769043
train gradient:  0.13405799187861217
iteration : 2388
train acc:  0.6875
train loss:  0.5166488885879517
train gradient:  0.12818793035978787
iteration : 2389
train acc:  0.828125
train loss:  0.3684360682964325
train gradient:  0.07359309938878748
iteration : 2390
train acc:  0.765625
train loss:  0.4284723997116089
train gradient:  0.09109523290711231
iteration : 2391
train acc:  0.7578125
train loss:  0.4955218732357025
train gradient:  0.16748206700181273
iteration : 2392
train acc:  0.7890625
train loss:  0.4581664800643921
train gradient:  0.13678059980379187
iteration : 2393
train acc:  0.7578125
train loss:  0.4253128468990326
train gradient:  0.1002962236227241
iteration : 2394
train acc:  0.703125
train loss:  0.5200582146644592
train gradient:  0.13625898061945707
iteration : 2395
train acc:  0.71875
train loss:  0.5182720422744751
train gradient:  0.13964553440034672
iteration : 2396
train acc:  0.7578125
train loss:  0.40928202867507935
train gradient:  0.10133355116954286
iteration : 2397
train acc:  0.7578125
train loss:  0.47080767154693604
train gradient:  0.10461877575950645
iteration : 2398
train acc:  0.765625
train loss:  0.446698933839798
train gradient:  0.09726454077825879
iteration : 2399
train acc:  0.78125
train loss:  0.4263388514518738
train gradient:  0.11090284843703455
iteration : 2400
train acc:  0.75
train loss:  0.5040885210037231
train gradient:  0.11653385260796804
iteration : 2401
train acc:  0.765625
train loss:  0.44683152437210083
train gradient:  0.09722363453229334
iteration : 2402
train acc:  0.765625
train loss:  0.46084174513816833
train gradient:  0.12360792926761242
iteration : 2403
train acc:  0.65625
train loss:  0.5372464656829834
train gradient:  0.12896462975210782
iteration : 2404
train acc:  0.8046875
train loss:  0.47629880905151367
train gradient:  0.09863103462486916
iteration : 2405
train acc:  0.75
train loss:  0.4771796464920044
train gradient:  0.147972969675526
iteration : 2406
train acc:  0.71875
train loss:  0.5607401728630066
train gradient:  0.1982244203455859
iteration : 2407
train acc:  0.75
train loss:  0.4895470440387726
train gradient:  0.12645612646780402
iteration : 2408
train acc:  0.78125
train loss:  0.4484272301197052
train gradient:  0.12412292462050568
iteration : 2409
train acc:  0.734375
train loss:  0.5008859038352966
train gradient:  0.13594673921539763
iteration : 2410
train acc:  0.734375
train loss:  0.5008467435836792
train gradient:  0.12002770230871807
iteration : 2411
train acc:  0.78125
train loss:  0.4284217357635498
train gradient:  0.10112962409876378
iteration : 2412
train acc:  0.8203125
train loss:  0.40760424733161926
train gradient:  0.07667462541668374
iteration : 2413
train acc:  0.796875
train loss:  0.42158395051956177
train gradient:  0.11574402095408827
iteration : 2414
train acc:  0.7890625
train loss:  0.46164005994796753
train gradient:  0.11702498078743397
iteration : 2415
train acc:  0.796875
train loss:  0.4417872428894043
train gradient:  0.10735804916945135
iteration : 2416
train acc:  0.7421875
train loss:  0.4902125597000122
train gradient:  0.13895617302585855
iteration : 2417
train acc:  0.8046875
train loss:  0.43330860137939453
train gradient:  0.10168751318735372
iteration : 2418
train acc:  0.671875
train loss:  0.5582616329193115
train gradient:  0.13217913790324068
iteration : 2419
train acc:  0.7421875
train loss:  0.5166717171669006
train gradient:  0.14397612532760337
iteration : 2420
train acc:  0.7734375
train loss:  0.4622495174407959
train gradient:  0.10283477388458347
iteration : 2421
train acc:  0.7421875
train loss:  0.4694827198982239
train gradient:  0.11791876312460074
iteration : 2422
train acc:  0.75
train loss:  0.4590179920196533
train gradient:  0.1445936780662739
iteration : 2423
train acc:  0.7578125
train loss:  0.5094250440597534
train gradient:  0.1369738419206521
iteration : 2424
train acc:  0.765625
train loss:  0.44752931594848633
train gradient:  0.11202787826929754
iteration : 2425
train acc:  0.7734375
train loss:  0.5007807016372681
train gradient:  0.10937790315550712
iteration : 2426
train acc:  0.796875
train loss:  0.43493157625198364
train gradient:  0.09795136473837934
iteration : 2427
train acc:  0.71875
train loss:  0.44520753622055054
train gradient:  0.10773391073086273
iteration : 2428
train acc:  0.765625
train loss:  0.4526018500328064
train gradient:  0.1018875082721318
iteration : 2429
train acc:  0.84375
train loss:  0.4259341359138489
train gradient:  0.09557030595975327
iteration : 2430
train acc:  0.8046875
train loss:  0.4366161525249481
train gradient:  0.11014117108324332
iteration : 2431
train acc:  0.6796875
train loss:  0.5361238718032837
train gradient:  0.17091167534517637
iteration : 2432
train acc:  0.7421875
train loss:  0.5257576704025269
train gradient:  0.12749811974660033
iteration : 2433
train acc:  0.765625
train loss:  0.45461779832839966
train gradient:  0.12475614603969944
iteration : 2434
train acc:  0.78125
train loss:  0.5468289852142334
train gradient:  0.14264480062495832
iteration : 2435
train acc:  0.8046875
train loss:  0.421305775642395
train gradient:  0.10487207310382113
iteration : 2436
train acc:  0.78125
train loss:  0.42664623260498047
train gradient:  0.0917495705534577
iteration : 2437
train acc:  0.7734375
train loss:  0.42846107482910156
train gradient:  0.08887904370192408
iteration : 2438
train acc:  0.765625
train loss:  0.4694511294364929
train gradient:  0.13450359183525112
iteration : 2439
train acc:  0.78125
train loss:  0.5100653171539307
train gradient:  0.12261737588562673
iteration : 2440
train acc:  0.6640625
train loss:  0.610249400138855
train gradient:  0.15862600263957422
iteration : 2441
train acc:  0.765625
train loss:  0.4248654246330261
train gradient:  0.09507426506279708
iteration : 2442
train acc:  0.828125
train loss:  0.41480565071105957
train gradient:  0.09793738723690565
iteration : 2443
train acc:  0.7734375
train loss:  0.4825021028518677
train gradient:  0.17848156349773003
iteration : 2444
train acc:  0.78125
train loss:  0.44497019052505493
train gradient:  0.1069779854344253
iteration : 2445
train acc:  0.7109375
train loss:  0.49676981568336487
train gradient:  0.12357512984877668
iteration : 2446
train acc:  0.71875
train loss:  0.4850766658782959
train gradient:  0.1177917963000233
iteration : 2447
train acc:  0.75
train loss:  0.48922598361968994
train gradient:  0.1361607195071261
iteration : 2448
train acc:  0.8359375
train loss:  0.3966357707977295
train gradient:  0.06876690735121226
iteration : 2449
train acc:  0.7109375
train loss:  0.5196410417556763
train gradient:  0.12677897094326623
iteration : 2450
train acc:  0.65625
train loss:  0.5419203042984009
train gradient:  0.14257167162514256
iteration : 2451
train acc:  0.71875
train loss:  0.5008683800697327
train gradient:  0.12352170326814425
iteration : 2452
train acc:  0.6953125
train loss:  0.5231496095657349
train gradient:  0.12665620367519562
iteration : 2453
train acc:  0.765625
train loss:  0.4731312096118927
train gradient:  0.1162427771205467
iteration : 2454
train acc:  0.796875
train loss:  0.45196035504341125
train gradient:  0.13537265551692476
iteration : 2455
train acc:  0.734375
train loss:  0.5211517810821533
train gradient:  0.14688454129373346
iteration : 2456
train acc:  0.71875
train loss:  0.5165055990219116
train gradient:  0.14528824890162195
iteration : 2457
train acc:  0.7578125
train loss:  0.45412200689315796
train gradient:  0.10800239268954967
iteration : 2458
train acc:  0.7421875
train loss:  0.5432583689689636
train gradient:  0.1617149670630328
iteration : 2459
train acc:  0.7578125
train loss:  0.45553117990493774
train gradient:  0.0994617784826846
iteration : 2460
train acc:  0.671875
train loss:  0.520734429359436
train gradient:  0.12593838465873516
iteration : 2461
train acc:  0.7734375
train loss:  0.44988760352134705
train gradient:  0.10007849487750932
iteration : 2462
train acc:  0.765625
train loss:  0.4812973737716675
train gradient:  0.12285447730945402
iteration : 2463
train acc:  0.6953125
train loss:  0.520307183265686
train gradient:  0.13573012185289707
iteration : 2464
train acc:  0.7109375
train loss:  0.49339354038238525
train gradient:  0.10324891218493475
iteration : 2465
train acc:  0.7890625
train loss:  0.45449042320251465
train gradient:  0.10300310834149065
iteration : 2466
train acc:  0.7421875
train loss:  0.481313019990921
train gradient:  0.12768195789548487
iteration : 2467
train acc:  0.7734375
train loss:  0.5121420621871948
train gradient:  0.1392849135228457
iteration : 2468
train acc:  0.7578125
train loss:  0.4720310568809509
train gradient:  0.12088222367524856
iteration : 2469
train acc:  0.78125
train loss:  0.42131859064102173
train gradient:  0.09284253532355986
iteration : 2470
train acc:  0.7421875
train loss:  0.4602980613708496
train gradient:  0.10369980821191273
iteration : 2471
train acc:  0.78125
train loss:  0.45735400915145874
train gradient:  0.09548246901424819
iteration : 2472
train acc:  0.734375
train loss:  0.46768689155578613
train gradient:  0.0947879466428632
iteration : 2473
train acc:  0.7265625
train loss:  0.5061843395233154
train gradient:  0.1415888847833251
iteration : 2474
train acc:  0.7421875
train loss:  0.5304126143455505
train gradient:  0.1321072441516981
iteration : 2475
train acc:  0.7734375
train loss:  0.4272514581680298
train gradient:  0.13487857724815186
iteration : 2476
train acc:  0.796875
train loss:  0.4844205975532532
train gradient:  0.11776399085772817
iteration : 2477
train acc:  0.8125
train loss:  0.3958909809589386
train gradient:  0.08934373995600989
iteration : 2478
train acc:  0.75
train loss:  0.4127456545829773
train gradient:  0.10232713065223602
iteration : 2479
train acc:  0.75
train loss:  0.5177278518676758
train gradient:  0.13443317181312303
iteration : 2480
train acc:  0.8046875
train loss:  0.4341316223144531
train gradient:  0.09618443256176454
iteration : 2481
train acc:  0.796875
train loss:  0.4112080931663513
train gradient:  0.07484626922652453
iteration : 2482
train acc:  0.7265625
train loss:  0.5013114213943481
train gradient:  0.13115351353771496
iteration : 2483
train acc:  0.7265625
train loss:  0.5142553448677063
train gradient:  0.14445528261623383
iteration : 2484
train acc:  0.7109375
train loss:  0.47598162293434143
train gradient:  0.10245180519617837
iteration : 2485
train acc:  0.78125
train loss:  0.46880656480789185
train gradient:  0.13254755788577477
iteration : 2486
train acc:  0.7265625
train loss:  0.48738086223602295
train gradient:  0.12349709196580115
iteration : 2487
train acc:  0.6953125
train loss:  0.513958215713501
train gradient:  0.13290783615834367
iteration : 2488
train acc:  0.7578125
train loss:  0.4794366955757141
train gradient:  0.11660247203041196
iteration : 2489
train acc:  0.7109375
train loss:  0.4962407946586609
train gradient:  0.11449103464464117
iteration : 2490
train acc:  0.71875
train loss:  0.49673911929130554
train gradient:  0.12436842134246089
iteration : 2491
train acc:  0.8203125
train loss:  0.4225519895553589
train gradient:  0.08499664932035456
iteration : 2492
train acc:  0.75
train loss:  0.46207430958747864
train gradient:  0.18540100640230894
iteration : 2493
train acc:  0.7890625
train loss:  0.4054441750049591
train gradient:  0.07970695375024738
iteration : 2494
train acc:  0.7109375
train loss:  0.4769555628299713
train gradient:  0.09762142765163782
iteration : 2495
train acc:  0.7109375
train loss:  0.5051915645599365
train gradient:  0.1122235531001157
iteration : 2496
train acc:  0.796875
train loss:  0.44827643036842346
train gradient:  0.09396033751580532
iteration : 2497
train acc:  0.6953125
train loss:  0.4958741068840027
train gradient:  0.11634675732574123
iteration : 2498
train acc:  0.7578125
train loss:  0.4726122319698334
train gradient:  0.11813719834729308
iteration : 2499
train acc:  0.7265625
train loss:  0.49030548334121704
train gradient:  0.12323741327295368
iteration : 2500
train acc:  0.7421875
train loss:  0.48020410537719727
train gradient:  0.12547388431302633
iteration : 2501
train acc:  0.75
train loss:  0.43934953212738037
train gradient:  0.11717873585045391
iteration : 2502
train acc:  0.6953125
train loss:  0.5309492349624634
train gradient:  0.13275081266246314
iteration : 2503
train acc:  0.6640625
train loss:  0.55998694896698
train gradient:  0.1579591015729615
iteration : 2504
train acc:  0.7265625
train loss:  0.4886150360107422
train gradient:  0.13996727486671465
iteration : 2505
train acc:  0.7109375
train loss:  0.4933982789516449
train gradient:  0.13827866918265472
iteration : 2506
train acc:  0.765625
train loss:  0.5312762260437012
train gradient:  0.1470610819849047
iteration : 2507
train acc:  0.7578125
train loss:  0.5134137868881226
train gradient:  0.11648457429599864
iteration : 2508
train acc:  0.6953125
train loss:  0.5355994701385498
train gradient:  0.1256010531937075
iteration : 2509
train acc:  0.6796875
train loss:  0.5734277367591858
train gradient:  0.14100167851719295
iteration : 2510
train acc:  0.734375
train loss:  0.47805941104888916
train gradient:  0.12831816026563075
iteration : 2511
train acc:  0.6875
train loss:  0.5410312414169312
train gradient:  0.1893080946358091
iteration : 2512
train acc:  0.78125
train loss:  0.49003520607948303
train gradient:  0.12364531704618154
iteration : 2513
train acc:  0.78125
train loss:  0.4593545198440552
train gradient:  0.09916080240428249
iteration : 2514
train acc:  0.703125
train loss:  0.5158868432044983
train gradient:  0.12520466139468256
iteration : 2515
train acc:  0.7421875
train loss:  0.48520129919052124
train gradient:  0.13675686086761646
iteration : 2516
train acc:  0.8125
train loss:  0.4731842577457428
train gradient:  0.13580981191854413
iteration : 2517
train acc:  0.7421875
train loss:  0.47407305240631104
train gradient:  0.109742527736581
iteration : 2518
train acc:  0.7734375
train loss:  0.4499185383319855
train gradient:  0.0964600102992591
iteration : 2519
train acc:  0.7265625
train loss:  0.5036139488220215
train gradient:  0.13471886437516328
iteration : 2520
train acc:  0.78125
train loss:  0.48464435338974
train gradient:  0.12344122843743195
iteration : 2521
train acc:  0.7421875
train loss:  0.5291386246681213
train gradient:  0.13673360753969388
iteration : 2522
train acc:  0.75
train loss:  0.4781477153301239
train gradient:  0.1278086535976572
iteration : 2523
train acc:  0.796875
train loss:  0.4348326623439789
train gradient:  0.11541904936599567
iteration : 2524
train acc:  0.71875
train loss:  0.5129521489143372
train gradient:  0.11758797630204808
iteration : 2525
train acc:  0.734375
train loss:  0.48213183879852295
train gradient:  0.12021361934342657
iteration : 2526
train acc:  0.7890625
train loss:  0.4048457741737366
train gradient:  0.10679225160389862
iteration : 2527
train acc:  0.7734375
train loss:  0.45623084902763367
train gradient:  0.10755266590036809
iteration : 2528
train acc:  0.765625
train loss:  0.4749930799007416
train gradient:  0.11472096256262915
iteration : 2529
train acc:  0.78125
train loss:  0.46796494722366333
train gradient:  0.13081734153210633
iteration : 2530
train acc:  0.6875
train loss:  0.5638942718505859
train gradient:  0.13485793977780525
iteration : 2531
train acc:  0.78125
train loss:  0.418414831161499
train gradient:  0.07422758574686247
iteration : 2532
train acc:  0.7890625
train loss:  0.45766016840934753
train gradient:  0.1161672033875238
iteration : 2533
train acc:  0.7421875
train loss:  0.542061984539032
train gradient:  0.1994661407355547
iteration : 2534
train acc:  0.75
train loss:  0.5048376321792603
train gradient:  0.11127931465303056
iteration : 2535
train acc:  0.671875
train loss:  0.5285855531692505
train gradient:  0.1371076322039409
iteration : 2536
train acc:  0.7265625
train loss:  0.5002598166465759
train gradient:  0.14273886644940414
iteration : 2537
train acc:  0.796875
train loss:  0.3983718156814575
train gradient:  0.09428943626283191
iteration : 2538
train acc:  0.703125
train loss:  0.5386092662811279
train gradient:  0.1307117113159413
iteration : 2539
train acc:  0.75
train loss:  0.4998878836631775
train gradient:  0.1249589769885917
iteration : 2540
train acc:  0.6875
train loss:  0.5383821725845337
train gradient:  0.1420073813426096
iteration : 2541
train acc:  0.71875
train loss:  0.5200631618499756
train gradient:  0.16760501721399435
iteration : 2542
train acc:  0.765625
train loss:  0.4477022886276245
train gradient:  0.12588357945705234
iteration : 2543
train acc:  0.765625
train loss:  0.4068982005119324
train gradient:  0.09141277509995929
iteration : 2544
train acc:  0.71875
train loss:  0.5176482200622559
train gradient:  0.11592241083400368
iteration : 2545
train acc:  0.765625
train loss:  0.4772907495498657
train gradient:  0.14015567678475951
iteration : 2546
train acc:  0.7265625
train loss:  0.5273239612579346
train gradient:  0.10279928878011002
iteration : 2547
train acc:  0.78125
train loss:  0.40715092420578003
train gradient:  0.08212605179727482
iteration : 2548
train acc:  0.71875
train loss:  0.48028630018234253
train gradient:  0.10459018786406953
iteration : 2549
train acc:  0.796875
train loss:  0.4472191333770752
train gradient:  0.10106490532534304
iteration : 2550
train acc:  0.6953125
train loss:  0.5069803595542908
train gradient:  0.10546344841814445
iteration : 2551
train acc:  0.796875
train loss:  0.45790553092956543
train gradient:  0.08754979675745526
iteration : 2552
train acc:  0.7578125
train loss:  0.44368070363998413
train gradient:  0.09040802217021819
iteration : 2553
train acc:  0.734375
train loss:  0.479885995388031
train gradient:  0.13153249086587154
iteration : 2554
train acc:  0.7265625
train loss:  0.5429600477218628
train gradient:  0.14685174723094063
iteration : 2555
train acc:  0.7734375
train loss:  0.5136440992355347
train gradient:  0.16321109910314097
iteration : 2556
train acc:  0.734375
train loss:  0.4685264527797699
train gradient:  0.10698874078702361
iteration : 2557
train acc:  0.78125
train loss:  0.4911050796508789
train gradient:  0.10353639805833215
iteration : 2558
train acc:  0.75
train loss:  0.5123578310012817
train gradient:  0.12856433850932714
iteration : 2559
train acc:  0.78125
train loss:  0.4695361852645874
train gradient:  0.10266332057463262
iteration : 2560
train acc:  0.7890625
train loss:  0.41606956720352173
train gradient:  0.09575408553217662
iteration : 2561
train acc:  0.71875
train loss:  0.49544548988342285
train gradient:  0.12023424294847071
iteration : 2562
train acc:  0.8046875
train loss:  0.43948593735694885
train gradient:  0.0939471827690406
iteration : 2563
train acc:  0.765625
train loss:  0.4577093720436096
train gradient:  0.09883402699526049
iteration : 2564
train acc:  0.7265625
train loss:  0.5557098388671875
train gradient:  0.13364040155752777
iteration : 2565
train acc:  0.75
train loss:  0.44778385758399963
train gradient:  0.09288416472063558
iteration : 2566
train acc:  0.8046875
train loss:  0.44592711329460144
train gradient:  0.14079270717753792
iteration : 2567
train acc:  0.8046875
train loss:  0.43260762095451355
train gradient:  0.08881654793090449
iteration : 2568
train acc:  0.796875
train loss:  0.42997705936431885
train gradient:  0.09456684885720344
iteration : 2569
train acc:  0.7578125
train loss:  0.46235179901123047
train gradient:  0.11817118970506925
iteration : 2570
train acc:  0.7578125
train loss:  0.4182260036468506
train gradient:  0.09414684688035797
iteration : 2571
train acc:  0.7734375
train loss:  0.49246352910995483
train gradient:  0.10135152517366003
iteration : 2572
train acc:  0.8046875
train loss:  0.4231882095336914
train gradient:  0.11403345999500149
iteration : 2573
train acc:  0.6796875
train loss:  0.5362405776977539
train gradient:  0.17113491379912393
iteration : 2574
train acc:  0.7890625
train loss:  0.4213937520980835
train gradient:  0.07166339841996307
iteration : 2575
train acc:  0.703125
train loss:  0.48820507526397705
train gradient:  0.11906398302570985
iteration : 2576
train acc:  0.7421875
train loss:  0.5012426376342773
train gradient:  0.12694978592329892
iteration : 2577
train acc:  0.75
train loss:  0.47205016016960144
train gradient:  0.12222915580832121
iteration : 2578
train acc:  0.7578125
train loss:  0.4637257754802704
train gradient:  0.10266918254636531
iteration : 2579
train acc:  0.7734375
train loss:  0.41533854603767395
train gradient:  0.09569827180789366
iteration : 2580
train acc:  0.6953125
train loss:  0.5416936874389648
train gradient:  0.14762919320599044
iteration : 2581
train acc:  0.765625
train loss:  0.46280696988105774
train gradient:  0.09709122483148523
iteration : 2582
train acc:  0.6953125
train loss:  0.542300820350647
train gradient:  0.13921424551925005
iteration : 2583
train acc:  0.78125
train loss:  0.4705256223678589
train gradient:  0.12884054997530822
iteration : 2584
train acc:  0.765625
train loss:  0.46707725524902344
train gradient:  0.10643431488382259
iteration : 2585
train acc:  0.7421875
train loss:  0.4727800488471985
train gradient:  0.12390948288514585
iteration : 2586
train acc:  0.7890625
train loss:  0.4308011531829834
train gradient:  0.08880583662955932
iteration : 2587
train acc:  0.703125
train loss:  0.4866189956665039
train gradient:  0.10223065455208721
iteration : 2588
train acc:  0.734375
train loss:  0.4428587555885315
train gradient:  0.093689854841578
iteration : 2589
train acc:  0.7265625
train loss:  0.47045791149139404
train gradient:  0.11942121275976131
iteration : 2590
train acc:  0.7109375
train loss:  0.4892286956310272
train gradient:  0.12816520059146003
iteration : 2591
train acc:  0.8046875
train loss:  0.4173572063446045
train gradient:  0.09263423377751182
iteration : 2592
train acc:  0.7421875
train loss:  0.4581448435783386
train gradient:  0.1020769124617154
iteration : 2593
train acc:  0.671875
train loss:  0.5263592004776001
train gradient:  0.12403674814288246
iteration : 2594
train acc:  0.6875
train loss:  0.5442520380020142
train gradient:  0.14716549060221232
iteration : 2595
train acc:  0.765625
train loss:  0.44220656156539917
train gradient:  0.10656934980004486
iteration : 2596
train acc:  0.71875
train loss:  0.5506924390792847
train gradient:  0.17945294972428655
iteration : 2597
train acc:  0.7734375
train loss:  0.4902452528476715
train gradient:  0.17238949782407476
iteration : 2598
train acc:  0.7578125
train loss:  0.5172433257102966
train gradient:  0.11555181988986901
iteration : 2599
train acc:  0.7890625
train loss:  0.4620929956436157
train gradient:  0.12046021100980595
iteration : 2600
train acc:  0.7109375
train loss:  0.47057145833969116
train gradient:  0.10633649448452001
iteration : 2601
train acc:  0.734375
train loss:  0.4878823161125183
train gradient:  0.13847514988669823
iteration : 2602
train acc:  0.796875
train loss:  0.43398356437683105
train gradient:  0.11188304242378865
iteration : 2603
train acc:  0.8046875
train loss:  0.42358970642089844
train gradient:  0.09252881912927208
iteration : 2604
train acc:  0.796875
train loss:  0.42001986503601074
train gradient:  0.15034016318400145
iteration : 2605
train acc:  0.765625
train loss:  0.4507976770401001
train gradient:  0.09787165356833158
iteration : 2606
train acc:  0.6875
train loss:  0.4880889654159546
train gradient:  0.12192417675287649
iteration : 2607
train acc:  0.7578125
train loss:  0.4817658066749573
train gradient:  0.11359604463790908
iteration : 2608
train acc:  0.734375
train loss:  0.4894147515296936
train gradient:  0.13903645951881877
iteration : 2609
train acc:  0.765625
train loss:  0.4824323356151581
train gradient:  0.12909877496584893
iteration : 2610
train acc:  0.7265625
train loss:  0.4892577528953552
train gradient:  0.14939310053687796
iteration : 2611
train acc:  0.796875
train loss:  0.4831402003765106
train gradient:  0.1375720314690611
iteration : 2612
train acc:  0.796875
train loss:  0.4405190348625183
train gradient:  0.11094668636475136
iteration : 2613
train acc:  0.6796875
train loss:  0.5511401891708374
train gradient:  0.14484918745690675
iteration : 2614
train acc:  0.75
train loss:  0.46735695004463196
train gradient:  0.11605904215382203
iteration : 2615
train acc:  0.7890625
train loss:  0.4582858681678772
train gradient:  0.10524031111979402
iteration : 2616
train acc:  0.75
train loss:  0.49440306425094604
train gradient:  0.14687966723210127
iteration : 2617
train acc:  0.7890625
train loss:  0.3808661699295044
train gradient:  0.07055761062383466
iteration : 2618
train acc:  0.734375
train loss:  0.5078941583633423
train gradient:  0.11253057735276562
iteration : 2619
train acc:  0.78125
train loss:  0.44875627756118774
train gradient:  0.10688364215563115
iteration : 2620
train acc:  0.703125
train loss:  0.577704906463623
train gradient:  0.16920042434723737
iteration : 2621
train acc:  0.7109375
train loss:  0.506733238697052
train gradient:  0.14959359186328158
iteration : 2622
train acc:  0.8046875
train loss:  0.4436555504798889
train gradient:  0.09275551744860658
iteration : 2623
train acc:  0.7734375
train loss:  0.4748144745826721
train gradient:  0.11118121146747689
iteration : 2624
train acc:  0.6875
train loss:  0.5976759791374207
train gradient:  0.13510924141206326
iteration : 2625
train acc:  0.7578125
train loss:  0.4340047240257263
train gradient:  0.08255454175233087
iteration : 2626
train acc:  0.78125
train loss:  0.4109528064727783
train gradient:  0.08282981352345625
iteration : 2627
train acc:  0.7265625
train loss:  0.5293482542037964
train gradient:  0.13337559052605025
iteration : 2628
train acc:  0.734375
train loss:  0.4571976065635681
train gradient:  0.08666617602272995
iteration : 2629
train acc:  0.7265625
train loss:  0.5098057985305786
train gradient:  0.11814746819831898
iteration : 2630
train acc:  0.75
train loss:  0.4944637715816498
train gradient:  0.11848791070679762
iteration : 2631
train acc:  0.7734375
train loss:  0.4118975102901459
train gradient:  0.08368861336148084
iteration : 2632
train acc:  0.71875
train loss:  0.5147894620895386
train gradient:  0.1195240819262323
iteration : 2633
train acc:  0.8046875
train loss:  0.4141561985015869
train gradient:  0.07814674320572353
iteration : 2634
train acc:  0.8125
train loss:  0.4436793327331543
train gradient:  0.10632189902583028
iteration : 2635
train acc:  0.6484375
train loss:  0.5855532884597778
train gradient:  0.17319123244449958
iteration : 2636
train acc:  0.6875
train loss:  0.5428860187530518
train gradient:  0.13105797923819856
iteration : 2637
train acc:  0.7578125
train loss:  0.4523773193359375
train gradient:  0.12796147805643254
iteration : 2638
train acc:  0.7890625
train loss:  0.4532475769519806
train gradient:  0.09561161391606662
iteration : 2639
train acc:  0.6953125
train loss:  0.5378146171569824
train gradient:  0.15846003153046384
iteration : 2640
train acc:  0.7421875
train loss:  0.4974541664123535
train gradient:  0.12829937019451795
iteration : 2641
train acc:  0.78125
train loss:  0.4328451156616211
train gradient:  0.12234152900513628
iteration : 2642
train acc:  0.796875
train loss:  0.3976525068283081
train gradient:  0.09445416882070365
iteration : 2643
train acc:  0.796875
train loss:  0.46240293979644775
train gradient:  0.12314025472857712
iteration : 2644
train acc:  0.8125
train loss:  0.4084162414073944
train gradient:  0.09338271535649856
iteration : 2645
train acc:  0.7265625
train loss:  0.5125778317451477
train gradient:  0.12777721700307243
iteration : 2646
train acc:  0.7578125
train loss:  0.45190590620040894
train gradient:  0.09215845756992562
iteration : 2647
train acc:  0.734375
train loss:  0.4985218048095703
train gradient:  0.1309567830541628
iteration : 2648
train acc:  0.7421875
train loss:  0.5652620196342468
train gradient:  0.13280340954164577
iteration : 2649
train acc:  0.7734375
train loss:  0.4349788725376129
train gradient:  0.15294623600381085
iteration : 2650
train acc:  0.765625
train loss:  0.47954440116882324
train gradient:  0.0992508130255931
iteration : 2651
train acc:  0.7734375
train loss:  0.4183027148246765
train gradient:  0.10653071747750487
iteration : 2652
train acc:  0.671875
train loss:  0.5834308862686157
train gradient:  0.1503081409744752
iteration : 2653
train acc:  0.75
train loss:  0.502657413482666
train gradient:  0.1339772061566703
iteration : 2654
train acc:  0.7421875
train loss:  0.47559869289398193
train gradient:  0.1078141837081686
iteration : 2655
train acc:  0.78125
train loss:  0.44691675901412964
train gradient:  0.09060856424514234
iteration : 2656
train acc:  0.7578125
train loss:  0.4663243293762207
train gradient:  0.11027912122675025
iteration : 2657
train acc:  0.6875
train loss:  0.5104012489318848
train gradient:  0.13923477029437112
iteration : 2658
train acc:  0.828125
train loss:  0.37635207176208496
train gradient:  0.07724463776888105
iteration : 2659
train acc:  0.796875
train loss:  0.41339921951293945
train gradient:  0.09047842128626768
iteration : 2660
train acc:  0.7578125
train loss:  0.44887447357177734
train gradient:  0.10052973188618498
iteration : 2661
train acc:  0.734375
train loss:  0.4762076139450073
train gradient:  0.11759091836298365
iteration : 2662
train acc:  0.7421875
train loss:  0.45497071743011475
train gradient:  0.10970768587069074
iteration : 2663
train acc:  0.78125
train loss:  0.4739712178707123
train gradient:  0.12061426763373051
iteration : 2664
train acc:  0.765625
train loss:  0.45619019865989685
train gradient:  0.12025488416877862
iteration : 2665
train acc:  0.7265625
train loss:  0.488567978143692
train gradient:  0.11515561641812937
iteration : 2666
train acc:  0.7421875
train loss:  0.4590829014778137
train gradient:  0.12070216966515519
iteration : 2667
train acc:  0.7890625
train loss:  0.47937965393066406
train gradient:  0.10744317271471801
iteration : 2668
train acc:  0.71875
train loss:  0.48330044746398926
train gradient:  0.12242281387160363
iteration : 2669
train acc:  0.75
train loss:  0.47898727655410767
train gradient:  0.10229453355343805
iteration : 2670
train acc:  0.703125
train loss:  0.5497034788131714
train gradient:  0.1666320569132834
iteration : 2671
train acc:  0.703125
train loss:  0.5303524136543274
train gradient:  0.13246628699174723
iteration : 2672
train acc:  0.78125
train loss:  0.4167378544807434
train gradient:  0.09400474039852634
iteration : 2673
train acc:  0.7578125
train loss:  0.5083569288253784
train gradient:  0.13505306608936313
iteration : 2674
train acc:  0.7421875
train loss:  0.5283496379852295
train gradient:  0.20304561527744894
iteration : 2675
train acc:  0.6796875
train loss:  0.5144338607788086
train gradient:  0.16920383863697652
iteration : 2676
train acc:  0.75
train loss:  0.45658811926841736
train gradient:  0.1218144400469899
iteration : 2677
train acc:  0.859375
train loss:  0.3849446177482605
train gradient:  0.09193984056291918
iteration : 2678
train acc:  0.7890625
train loss:  0.43433117866516113
train gradient:  0.08777324644817998
iteration : 2679
train acc:  0.640625
train loss:  0.602645993232727
train gradient:  0.18172355680885005
iteration : 2680
train acc:  0.796875
train loss:  0.4505265951156616
train gradient:  0.1013874110722512
iteration : 2681
train acc:  0.7265625
train loss:  0.46060270071029663
train gradient:  0.13550057530978438
iteration : 2682
train acc:  0.7265625
train loss:  0.47927263379096985
train gradient:  0.10748065002507708
iteration : 2683
train acc:  0.734375
train loss:  0.5223372578620911
train gradient:  0.14535350196439262
iteration : 2684
train acc:  0.765625
train loss:  0.44993093609809875
train gradient:  0.11166006090754878
iteration : 2685
train acc:  0.78125
train loss:  0.46249479055404663
train gradient:  0.11524859960782691
iteration : 2686
train acc:  0.6953125
train loss:  0.5195587873458862
train gradient:  0.14399121903582474
iteration : 2687
train acc:  0.7578125
train loss:  0.5085617303848267
train gradient:  0.12337401123439583
iteration : 2688
train acc:  0.75
train loss:  0.46285390853881836
train gradient:  0.13103189578278462
iteration : 2689
train acc:  0.7734375
train loss:  0.46006113290786743
train gradient:  0.0988650877765966
iteration : 2690
train acc:  0.8046875
train loss:  0.3841171860694885
train gradient:  0.0721852975349313
iteration : 2691
train acc:  0.75
train loss:  0.43819111585617065
train gradient:  0.09584729202810502
iteration : 2692
train acc:  0.7734375
train loss:  0.4497002959251404
train gradient:  0.09303663229973892
iteration : 2693
train acc:  0.78125
train loss:  0.42013755440711975
train gradient:  0.08537624169447038
iteration : 2694
train acc:  0.734375
train loss:  0.5337209105491638
train gradient:  0.12864963818377365
iteration : 2695
train acc:  0.734375
train loss:  0.4799681305885315
train gradient:  0.0885253064472211
iteration : 2696
train acc:  0.703125
train loss:  0.4869399964809418
train gradient:  0.10730294378807154
iteration : 2697
train acc:  0.7578125
train loss:  0.5142847299575806
train gradient:  0.12425650711788255
iteration : 2698
train acc:  0.71875
train loss:  0.49978673458099365
train gradient:  0.10521362494824907
iteration : 2699
train acc:  0.828125
train loss:  0.42271125316619873
train gradient:  0.0887728750600198
iteration : 2700
train acc:  0.7421875
train loss:  0.4700528383255005
train gradient:  0.09981718777403437
iteration : 2701
train acc:  0.7109375
train loss:  0.5175207853317261
train gradient:  0.15362995856271472
iteration : 2702
train acc:  0.8125
train loss:  0.4056229889392853
train gradient:  0.08479742167082237
iteration : 2703
train acc:  0.734375
train loss:  0.5258145332336426
train gradient:  0.13409446791889496
iteration : 2704
train acc:  0.7578125
train loss:  0.466459721326828
train gradient:  0.10049391763290322
iteration : 2705
train acc:  0.75
train loss:  0.49976134300231934
train gradient:  0.15140472977355318
iteration : 2706
train acc:  0.78125
train loss:  0.3994886577129364
train gradient:  0.07007247327758176
iteration : 2707
train acc:  0.7734375
train loss:  0.4574376344680786
train gradient:  0.10360380771139249
iteration : 2708
train acc:  0.71875
train loss:  0.5137978792190552
train gradient:  0.1415484806015885
iteration : 2709
train acc:  0.75
train loss:  0.45471423864364624
train gradient:  0.09098641146882162
iteration : 2710
train acc:  0.8046875
train loss:  0.43181830644607544
train gradient:  0.08943604256499006
iteration : 2711
train acc:  0.8359375
train loss:  0.36128050088882446
train gradient:  0.07412823555746512
iteration : 2712
train acc:  0.6875
train loss:  0.49895596504211426
train gradient:  0.14224792347554166
iteration : 2713
train acc:  0.7421875
train loss:  0.4768984317779541
train gradient:  0.11414333040536769
iteration : 2714
train acc:  0.765625
train loss:  0.4359206557273865
train gradient:  0.09865892571925529
iteration : 2715
train acc:  0.8046875
train loss:  0.417202889919281
train gradient:  0.0791456373079619
iteration : 2716
train acc:  0.7421875
train loss:  0.47335943579673767
train gradient:  0.106780408894685
iteration : 2717
train acc:  0.7109375
train loss:  0.5081595182418823
train gradient:  0.13476169564950485
iteration : 2718
train acc:  0.7734375
train loss:  0.43933814764022827
train gradient:  0.09890560563872572
iteration : 2719
train acc:  0.703125
train loss:  0.5065315961837769
train gradient:  0.11886631639548545
iteration : 2720
train acc:  0.7734375
train loss:  0.4389335811138153
train gradient:  0.11730063917251005
iteration : 2721
train acc:  0.6796875
train loss:  0.5373275279998779
train gradient:  0.1591646711649653
iteration : 2722
train acc:  0.6796875
train loss:  0.5295491814613342
train gradient:  0.12736381474888922
iteration : 2723
train acc:  0.8125
train loss:  0.4539097249507904
train gradient:  0.10324409247570239
iteration : 2724
train acc:  0.7734375
train loss:  0.4540659189224243
train gradient:  0.1326556852180984
iteration : 2725
train acc:  0.7734375
train loss:  0.40929776430130005
train gradient:  0.10447886945404199
iteration : 2726
train acc:  0.7265625
train loss:  0.4975402355194092
train gradient:  0.11832795712882871
iteration : 2727
train acc:  0.765625
train loss:  0.45421546697616577
train gradient:  0.10318013457533796
iteration : 2728
train acc:  0.796875
train loss:  0.4601745307445526
train gradient:  0.10508675163759003
iteration : 2729
train acc:  0.7578125
train loss:  0.4783918261528015
train gradient:  0.12807925316280042
iteration : 2730
train acc:  0.75
train loss:  0.4209887981414795
train gradient:  0.08941908099885276
iteration : 2731
train acc:  0.75
train loss:  0.4551662802696228
train gradient:  0.09628814884591012
iteration : 2732
train acc:  0.78125
train loss:  0.46454963088035583
train gradient:  0.09796188637867706
iteration : 2733
train acc:  0.6328125
train loss:  0.5715186595916748
train gradient:  0.1631114746720954
iteration : 2734
train acc:  0.7578125
train loss:  0.518255352973938
train gradient:  0.16558424817143994
iteration : 2735
train acc:  0.8046875
train loss:  0.47650277614593506
train gradient:  0.1027498879903129
iteration : 2736
train acc:  0.65625
train loss:  0.5978527665138245
train gradient:  0.2095080603550853
iteration : 2737
train acc:  0.828125
train loss:  0.41572558879852295
train gradient:  0.1154296785152128
iteration : 2738
train acc:  0.734375
train loss:  0.5279157161712646
train gradient:  0.17616163386493655
iteration : 2739
train acc:  0.7890625
train loss:  0.45685508847236633
train gradient:  0.10056609488114354
iteration : 2740
train acc:  0.734375
train loss:  0.48407530784606934
train gradient:  0.13659680144435737
iteration : 2741
train acc:  0.7734375
train loss:  0.4591817259788513
train gradient:  0.13472281873408998
iteration : 2742
train acc:  0.765625
train loss:  0.4266679286956787
train gradient:  0.10299465927687294
iteration : 2743
train acc:  0.7265625
train loss:  0.5078060030937195
train gradient:  0.1834904256822879
iteration : 2744
train acc:  0.7578125
train loss:  0.49685412645339966
train gradient:  0.11420296825541157
iteration : 2745
train acc:  0.75
train loss:  0.47344279289245605
train gradient:  0.11872423027606427
iteration : 2746
train acc:  0.7734375
train loss:  0.4660359025001526
train gradient:  0.11600634121515269
iteration : 2747
train acc:  0.8125
train loss:  0.4400325417518616
train gradient:  0.10144212528590381
iteration : 2748
train acc:  0.7578125
train loss:  0.5011168718338013
train gradient:  0.16726169607072583
iteration : 2749
train acc:  0.765625
train loss:  0.4739987254142761
train gradient:  0.1109656368353109
iteration : 2750
train acc:  0.828125
train loss:  0.4124736189842224
train gradient:  0.09579569771965266
iteration : 2751
train acc:  0.7578125
train loss:  0.4619252681732178
train gradient:  0.10783143426508739
iteration : 2752
train acc:  0.78125
train loss:  0.4453820586204529
train gradient:  0.10984746533039402
iteration : 2753
train acc:  0.7890625
train loss:  0.4238179624080658
train gradient:  0.09892140107959976
iteration : 2754
train acc:  0.796875
train loss:  0.46862757205963135
train gradient:  0.11768922372490877
iteration : 2755
train acc:  0.75
train loss:  0.510195255279541
train gradient:  0.11260019024217507
iteration : 2756
train acc:  0.765625
train loss:  0.4385853409767151
train gradient:  0.08462381571451937
iteration : 2757
train acc:  0.7109375
train loss:  0.5030888319015503
train gradient:  0.1004930519705854
iteration : 2758
train acc:  0.765625
train loss:  0.4352622628211975
train gradient:  0.10335621581442764
iteration : 2759
train acc:  0.796875
train loss:  0.4504341185092926
train gradient:  0.11093362158723268
iteration : 2760
train acc:  0.78125
train loss:  0.4772355556488037
train gradient:  0.12282300574426486
iteration : 2761
train acc:  0.7734375
train loss:  0.4221113920211792
train gradient:  0.08518297944722623
iteration : 2762
train acc:  0.7734375
train loss:  0.40945231914520264
train gradient:  0.07976152424375461
iteration : 2763
train acc:  0.7421875
train loss:  0.4607457220554352
train gradient:  0.110711652094331
iteration : 2764
train acc:  0.71875
train loss:  0.44473302364349365
train gradient:  0.113237106559439
iteration : 2765
train acc:  0.71875
train loss:  0.4945768713951111
train gradient:  0.11443421249513054
iteration : 2766
train acc:  0.75
train loss:  0.48287567496299744
train gradient:  0.1315026309046815
iteration : 2767
train acc:  0.765625
train loss:  0.479478120803833
train gradient:  0.1428671273730377
iteration : 2768
train acc:  0.7734375
train loss:  0.4981089234352112
train gradient:  0.1302479324117467
iteration : 2769
train acc:  0.765625
train loss:  0.45622718334198
train gradient:  0.1076503236420199
iteration : 2770
train acc:  0.671875
train loss:  0.5553356409072876
train gradient:  0.14347298392208213
iteration : 2771
train acc:  0.7890625
train loss:  0.4292856454849243
train gradient:  0.11593808270009597
iteration : 2772
train acc:  0.796875
train loss:  0.41032618284225464
train gradient:  0.11806975093632056
iteration : 2773
train acc:  0.8046875
train loss:  0.41017526388168335
train gradient:  0.0932522012993067
iteration : 2774
train acc:  0.765625
train loss:  0.4949573278427124
train gradient:  0.11948538728038152
iteration : 2775
train acc:  0.796875
train loss:  0.4590907096862793
train gradient:  0.12648500178351152
iteration : 2776
train acc:  0.734375
train loss:  0.4583492875099182
train gradient:  0.11972854861000268
iteration : 2777
train acc:  0.6875
train loss:  0.5337362885475159
train gradient:  0.13671435124807202
iteration : 2778
train acc:  0.8125
train loss:  0.42514634132385254
train gradient:  0.10828971427985758
iteration : 2779
train acc:  0.78125
train loss:  0.4256296157836914
train gradient:  0.08875339152806729
iteration : 2780
train acc:  0.78125
train loss:  0.4102085828781128
train gradient:  0.09196540030197188
iteration : 2781
train acc:  0.75
train loss:  0.48675358295440674
train gradient:  0.11869436776163768
iteration : 2782
train acc:  0.78125
train loss:  0.42719364166259766
train gradient:  0.10031855673114654
iteration : 2783
train acc:  0.7578125
train loss:  0.4462173581123352
train gradient:  0.1023077103138913
iteration : 2784
train acc:  0.6953125
train loss:  0.5680075883865356
train gradient:  0.1843804455186994
iteration : 2785
train acc:  0.734375
train loss:  0.46574103832244873
train gradient:  0.11936518184230978
iteration : 2786
train acc:  0.75
train loss:  0.45029959082603455
train gradient:  0.10493105411797539
iteration : 2787
train acc:  0.71875
train loss:  0.4893662929534912
train gradient:  0.1081414065401299
iteration : 2788
train acc:  0.7734375
train loss:  0.43224120140075684
train gradient:  0.09468487909214253
iteration : 2789
train acc:  0.796875
train loss:  0.4189380407333374
train gradient:  0.09476326194133518
iteration : 2790
train acc:  0.7890625
train loss:  0.4505244791507721
train gradient:  0.14705336337002056
iteration : 2791
train acc:  0.796875
train loss:  0.38336604833602905
train gradient:  0.09030999779953522
iteration : 2792
train acc:  0.796875
train loss:  0.4384205937385559
train gradient:  0.10688782945138932
iteration : 2793
train acc:  0.7265625
train loss:  0.5430249571800232
train gradient:  0.16205874159388153
iteration : 2794
train acc:  0.8046875
train loss:  0.42909300327301025
train gradient:  0.08932825532521958
iteration : 2795
train acc:  0.6875
train loss:  0.5422362685203552
train gradient:  0.14900063421541737
iteration : 2796
train acc:  0.7578125
train loss:  0.41382521390914917
train gradient:  0.094130675286587
iteration : 2797
train acc:  0.7109375
train loss:  0.5238943696022034
train gradient:  0.15653901511957657
iteration : 2798
train acc:  0.7421875
train loss:  0.5209522843360901
train gradient:  0.14224359633574324
iteration : 2799
train acc:  0.7265625
train loss:  0.49575015902519226
train gradient:  0.13054848612092979
iteration : 2800
train acc:  0.7734375
train loss:  0.44599640369415283
train gradient:  0.0974085844090918
iteration : 2801
train acc:  0.703125
train loss:  0.5401532649993896
train gradient:  0.13650078267925017
iteration : 2802
train acc:  0.7421875
train loss:  0.45185425877571106
train gradient:  0.10316520996638348
iteration : 2803
train acc:  0.6953125
train loss:  0.5064674019813538
train gradient:  0.13443976009716574
iteration : 2804
train acc:  0.7578125
train loss:  0.4379981756210327
train gradient:  0.106041917417104
iteration : 2805
train acc:  0.796875
train loss:  0.4236605167388916
train gradient:  0.10810224194460902
iteration : 2806
train acc:  0.7890625
train loss:  0.41893643140792847
train gradient:  0.11081679275936225
iteration : 2807
train acc:  0.71875
train loss:  0.5530527234077454
train gradient:  0.1805479408796944
iteration : 2808
train acc:  0.71875
train loss:  0.49051713943481445
train gradient:  0.1494817581841065
iteration : 2809
train acc:  0.7578125
train loss:  0.5105812549591064
train gradient:  0.1324487280731395
iteration : 2810
train acc:  0.71875
train loss:  0.5272666215896606
train gradient:  0.12205955204514714
iteration : 2811
train acc:  0.75
train loss:  0.5115672945976257
train gradient:  0.1281540859515819
iteration : 2812
train acc:  0.78125
train loss:  0.43308454751968384
train gradient:  0.12608540150399528
iteration : 2813
train acc:  0.765625
train loss:  0.48465806245803833
train gradient:  0.1037042249524361
iteration : 2814
train acc:  0.75
train loss:  0.4678959548473358
train gradient:  0.11215539559928771
iteration : 2815
train acc:  0.6953125
train loss:  0.5769402384757996
train gradient:  0.21301616358105552
iteration : 2816
train acc:  0.78125
train loss:  0.4622517228126526
train gradient:  0.12217705028947873
iteration : 2817
train acc:  0.765625
train loss:  0.4705193042755127
train gradient:  0.14171552021671113
iteration : 2818
train acc:  0.765625
train loss:  0.4825423061847687
train gradient:  0.13992612801264856
iteration : 2819
train acc:  0.765625
train loss:  0.4536442756652832
train gradient:  0.10034024340919782
iteration : 2820
train acc:  0.71875
train loss:  0.5161430239677429
train gradient:  0.15468412024148226
iteration : 2821
train acc:  0.7890625
train loss:  0.43755701184272766
train gradient:  0.10796607230104599
iteration : 2822
train acc:  0.7578125
train loss:  0.5069910287857056
train gradient:  0.1455901093390763
iteration : 2823
train acc:  0.75
train loss:  0.48042941093444824
train gradient:  0.11482865253720562
iteration : 2824
train acc:  0.7421875
train loss:  0.5331703424453735
train gradient:  0.13104846640648043
iteration : 2825
train acc:  0.765625
train loss:  0.4913562536239624
train gradient:  0.1539933099480369
iteration : 2826
train acc:  0.8125
train loss:  0.4175621271133423
train gradient:  0.0879518641815526
iteration : 2827
train acc:  0.7734375
train loss:  0.45272424817085266
train gradient:  0.09011518417055724
iteration : 2828
train acc:  0.71875
train loss:  0.4965209662914276
train gradient:  0.10910976903553056
iteration : 2829
train acc:  0.6953125
train loss:  0.5774374008178711
train gradient:  0.13925182453001206
iteration : 2830
train acc:  0.7578125
train loss:  0.44716668128967285
train gradient:  0.10789050793749871
iteration : 2831
train acc:  0.75
train loss:  0.5343047976493835
train gradient:  0.1892705240157216
iteration : 2832
train acc:  0.7265625
train loss:  0.46938174962997437
train gradient:  0.10198706020381798
iteration : 2833
train acc:  0.78125
train loss:  0.4299177825450897
train gradient:  0.11117628038454297
iteration : 2834
train acc:  0.671875
train loss:  0.5471358895301819
train gradient:  0.1507127844164835
iteration : 2835
train acc:  0.7890625
train loss:  0.4221421778202057
train gradient:  0.08238190654807587
iteration : 2836
train acc:  0.6484375
train loss:  0.5863116979598999
train gradient:  0.15306839178642923
iteration : 2837
train acc:  0.6796875
train loss:  0.573336660861969
train gradient:  0.14769568626033108
iteration : 2838
train acc:  0.7734375
train loss:  0.4308456480503082
train gradient:  0.1077209558477544
iteration : 2839
train acc:  0.7265625
train loss:  0.47629010677337646
train gradient:  0.12406714262888553
iteration : 2840
train acc:  0.7109375
train loss:  0.551422655582428
train gradient:  0.1673468621761151
iteration : 2841
train acc:  0.7734375
train loss:  0.47704774141311646
train gradient:  0.1186870215805439
iteration : 2842
train acc:  0.734375
train loss:  0.47061794996261597
train gradient:  0.12273741160381207
iteration : 2843
train acc:  0.8046875
train loss:  0.4488140940666199
train gradient:  0.08818799226577945
iteration : 2844
train acc:  0.796875
train loss:  0.4373660683631897
train gradient:  0.09728180615315786
iteration : 2845
train acc:  0.7578125
train loss:  0.4172172546386719
train gradient:  0.08172821836576097
iteration : 2846
train acc:  0.78125
train loss:  0.4804903268814087
train gradient:  0.1012066790255956
iteration : 2847
train acc:  0.7578125
train loss:  0.4929533004760742
train gradient:  0.13502634652193263
iteration : 2848
train acc:  0.734375
train loss:  0.49118372797966003
train gradient:  0.10560855523190826
iteration : 2849
train acc:  0.7578125
train loss:  0.5081666111946106
train gradient:  0.1705084416745061
iteration : 2850
train acc:  0.734375
train loss:  0.5357491374015808
train gradient:  0.1557713266825661
iteration : 2851
train acc:  0.75
train loss:  0.5132695436477661
train gradient:  0.14643374868500544
iteration : 2852
train acc:  0.78125
train loss:  0.49226194620132446
train gradient:  0.11707891961824736
iteration : 2853
train acc:  0.7421875
train loss:  0.5095815658569336
train gradient:  0.11917407176887197
iteration : 2854
train acc:  0.828125
train loss:  0.39432376623153687
train gradient:  0.08865770065602842
iteration : 2855
train acc:  0.6640625
train loss:  0.5664032697677612
train gradient:  0.17199184376165905
iteration : 2856
train acc:  0.6640625
train loss:  0.5340908765792847
train gradient:  0.13895474568726773
iteration : 2857
train acc:  0.7734375
train loss:  0.4718751907348633
train gradient:  0.12614089713663823
iteration : 2858
train acc:  0.765625
train loss:  0.40763425827026367
train gradient:  0.07712882300831357
iteration : 2859
train acc:  0.796875
train loss:  0.4082964062690735
train gradient:  0.09286807141803122
iteration : 2860
train acc:  0.7265625
train loss:  0.5103805065155029
train gradient:  0.1394838407678361
iteration : 2861
train acc:  0.78125
train loss:  0.4737169146537781
train gradient:  0.09420564818130596
iteration : 2862
train acc:  0.75
train loss:  0.470771461725235
train gradient:  0.09357467173697448
iteration : 2863
train acc:  0.7578125
train loss:  0.5026919841766357
train gradient:  0.1298457500788277
iteration : 2864
train acc:  0.7734375
train loss:  0.45693439245224
train gradient:  0.10079848009149264
iteration : 2865
train acc:  0.8125
train loss:  0.40846431255340576
train gradient:  0.08357942807807062
iteration : 2866
train acc:  0.6328125
train loss:  0.5917856097221375
train gradient:  0.16993517019892468
iteration : 2867
train acc:  0.7734375
train loss:  0.4165562093257904
train gradient:  0.09270225505792219
iteration : 2868
train acc:  0.7421875
train loss:  0.43940603733062744
train gradient:  0.08254771469788172
iteration : 2869
train acc:  0.75
train loss:  0.44785794615745544
train gradient:  0.11295837945675638
iteration : 2870
train acc:  0.7109375
train loss:  0.5605469346046448
train gradient:  0.14313367620717654
iteration : 2871
train acc:  0.796875
train loss:  0.4422939717769623
train gradient:  0.1085971115591099
iteration : 2872
train acc:  0.7734375
train loss:  0.4625609815120697
train gradient:  0.09595811155045794
iteration : 2873
train acc:  0.7734375
train loss:  0.44642338156700134
train gradient:  0.09065907823542557
iteration : 2874
train acc:  0.75
train loss:  0.4901876747608185
train gradient:  0.11576017967283045
iteration : 2875
train acc:  0.7578125
train loss:  0.43996453285217285
train gradient:  0.1271220264148301
iteration : 2876
train acc:  0.8046875
train loss:  0.4447403848171234
train gradient:  0.09063653514008659
iteration : 2877
train acc:  0.765625
train loss:  0.4856685996055603
train gradient:  0.14188230011909753
iteration : 2878
train acc:  0.78125
train loss:  0.467072069644928
train gradient:  0.10968196219224992
iteration : 2879
train acc:  0.8125
train loss:  0.407879501581192
train gradient:  0.10190669508579112
iteration : 2880
train acc:  0.7578125
train loss:  0.5186749696731567
train gradient:  0.1143444697555263
iteration : 2881
train acc:  0.765625
train loss:  0.48152923583984375
train gradient:  0.11116532925229534
iteration : 2882
train acc:  0.7265625
train loss:  0.5609536170959473
train gradient:  0.16412335039543013
iteration : 2883
train acc:  0.8125
train loss:  0.4023856222629547
train gradient:  0.07870538578660899
iteration : 2884
train acc:  0.796875
train loss:  0.4294576346874237
train gradient:  0.09892963073417656
iteration : 2885
train acc:  0.8125
train loss:  0.4156999886035919
train gradient:  0.11882119091712597
iteration : 2886
train acc:  0.734375
train loss:  0.4802653193473816
train gradient:  0.12018131662060866
iteration : 2887
train acc:  0.78125
train loss:  0.44939368963241577
train gradient:  0.08353854048949251
iteration : 2888
train acc:  0.78125
train loss:  0.4795546233654022
train gradient:  0.14166141453828635
iteration : 2889
train acc:  0.703125
train loss:  0.5091991424560547
train gradient:  0.13765182366208417
iteration : 2890
train acc:  0.6796875
train loss:  0.5642809867858887
train gradient:  0.18345951499306873
iteration : 2891
train acc:  0.8203125
train loss:  0.4753911793231964
train gradient:  0.11903409913398012
iteration : 2892
train acc:  0.7265625
train loss:  0.5973818302154541
train gradient:  0.15627751864780715
iteration : 2893
train acc:  0.7578125
train loss:  0.4433465301990509
train gradient:  0.11679341704999963
iteration : 2894
train acc:  0.796875
train loss:  0.49567174911499023
train gradient:  0.17368769158547914
iteration : 2895
train acc:  0.7109375
train loss:  0.5376400947570801
train gradient:  0.16061719962883075
iteration : 2896
train acc:  0.7421875
train loss:  0.5057121515274048
train gradient:  0.11883388664436371
iteration : 2897
train acc:  0.7578125
train loss:  0.4292720556259155
train gradient:  0.08922426647845323
iteration : 2898
train acc:  0.6875
train loss:  0.5227658748626709
train gradient:  0.1395462004846224
iteration : 2899
train acc:  0.7265625
train loss:  0.4883240759372711
train gradient:  0.14227847588851178
iteration : 2900
train acc:  0.75
train loss:  0.46403032541275024
train gradient:  0.09230411699692667
iteration : 2901
train acc:  0.78125
train loss:  0.455269992351532
train gradient:  0.10000441769927848
iteration : 2902
train acc:  0.8125
train loss:  0.4630296528339386
train gradient:  0.09475607734906794
iteration : 2903
train acc:  0.7265625
train loss:  0.5168588161468506
train gradient:  0.16711968656489223
iteration : 2904
train acc:  0.734375
train loss:  0.4991295039653778
train gradient:  0.12871610998649574
iteration : 2905
train acc:  0.7109375
train loss:  0.5138115286827087
train gradient:  0.1448373924153421
iteration : 2906
train acc:  0.7265625
train loss:  0.49404966831207275
train gradient:  0.1541847339583705
iteration : 2907
train acc:  0.7890625
train loss:  0.44042184948921204
train gradient:  0.10079002576925263
iteration : 2908
train acc:  0.7734375
train loss:  0.4317425787448883
train gradient:  0.12243110884723299
iteration : 2909
train acc:  0.7734375
train loss:  0.46173664927482605
train gradient:  0.0964841767759821
iteration : 2910
train acc:  0.7265625
train loss:  0.4684855341911316
train gradient:  0.12512740723803814
iteration : 2911
train acc:  0.78125
train loss:  0.47799569368362427
train gradient:  0.12955489898728265
iteration : 2912
train acc:  0.796875
train loss:  0.41584840416908264
train gradient:  0.0934556260803239
iteration : 2913
train acc:  0.703125
train loss:  0.5178760290145874
train gradient:  0.1280426517587962
iteration : 2914
train acc:  0.78125
train loss:  0.47757869958877563
train gradient:  0.10585907299910102
iteration : 2915
train acc:  0.7109375
train loss:  0.5291621685028076
train gradient:  0.12769323170168373
iteration : 2916
train acc:  0.7265625
train loss:  0.4995916187763214
train gradient:  0.12133049551691895
iteration : 2917
train acc:  0.78125
train loss:  0.4972488284111023
train gradient:  0.14779950338063214
iteration : 2918
train acc:  0.78125
train loss:  0.47183993458747864
train gradient:  0.10460606476289117
iteration : 2919
train acc:  0.828125
train loss:  0.38706716895103455
train gradient:  0.08914013478100699
iteration : 2920
train acc:  0.75
train loss:  0.4920177459716797
train gradient:  0.12494938374354228
iteration : 2921
train acc:  0.765625
train loss:  0.460058331489563
train gradient:  0.13563935530809185
iteration : 2922
train acc:  0.765625
train loss:  0.49213907122612
train gradient:  0.13501118251429647
iteration : 2923
train acc:  0.7734375
train loss:  0.47742679715156555
train gradient:  0.11290660793383178
iteration : 2924
train acc:  0.78125
train loss:  0.4251152276992798
train gradient:  0.1038268328386842
iteration : 2925
train acc:  0.7578125
train loss:  0.4570833444595337
train gradient:  0.09811915577949995
iteration : 2926
train acc:  0.71875
train loss:  0.5348008871078491
train gradient:  0.13489249006183804
iteration : 2927
train acc:  0.7578125
train loss:  0.5315424799919128
train gradient:  0.13954542450486013
iteration : 2928
train acc:  0.78125
train loss:  0.46868759393692017
train gradient:  0.11443632639047722
iteration : 2929
train acc:  0.7265625
train loss:  0.5058318972587585
train gradient:  0.17331630097077805
iteration : 2930
train acc:  0.78125
train loss:  0.43097609281539917
train gradient:  0.10106067878731038
iteration : 2931
train acc:  0.75
train loss:  0.46943631768226624
train gradient:  0.10979383377350317
iteration : 2932
train acc:  0.7734375
train loss:  0.45398521423339844
train gradient:  0.10683722528814857
iteration : 2933
train acc:  0.7734375
train loss:  0.4374215602874756
train gradient:  0.10261805389013805
iteration : 2934
train acc:  0.734375
train loss:  0.46638333797454834
train gradient:  0.10057017172327869
iteration : 2935
train acc:  0.6953125
train loss:  0.5119327902793884
train gradient:  0.1159775880484362
iteration : 2936
train acc:  0.75
train loss:  0.4517369866371155
train gradient:  0.11939104388284859
iteration : 2937
train acc:  0.7578125
train loss:  0.500685453414917
train gradient:  0.15283487086696834
iteration : 2938
train acc:  0.734375
train loss:  0.4610496461391449
train gradient:  0.13489924724599095
iteration : 2939
train acc:  0.7578125
train loss:  0.5079332590103149
train gradient:  0.11301053212079754
iteration : 2940
train acc:  0.7265625
train loss:  0.5009246468544006
train gradient:  0.13232236403920997
iteration : 2941
train acc:  0.7421875
train loss:  0.46459177136421204
train gradient:  0.13126414734013891
iteration : 2942
train acc:  0.7265625
train loss:  0.4597036838531494
train gradient:  0.12056789431470913
iteration : 2943
train acc:  0.7890625
train loss:  0.47051599621772766
train gradient:  0.1257887293295687
iteration : 2944
train acc:  0.7734375
train loss:  0.4485163986682892
train gradient:  0.11240965136677754
iteration : 2945
train acc:  0.703125
train loss:  0.4891108572483063
train gradient:  0.11682943964386287
iteration : 2946
train acc:  0.78125
train loss:  0.5093039274215698
train gradient:  0.14395665640548033
iteration : 2947
train acc:  0.78125
train loss:  0.45509761571884155
train gradient:  0.11533987452129356
iteration : 2948
train acc:  0.8046875
train loss:  0.41549402475357056
train gradient:  0.1071795569806334
iteration : 2949
train acc:  0.7578125
train loss:  0.4507124423980713
train gradient:  0.12867059030038897
iteration : 2950
train acc:  0.75
train loss:  0.5015065670013428
train gradient:  0.11730747594014158
iteration : 2951
train acc:  0.7421875
train loss:  0.48798590898513794
train gradient:  0.11486632942555433
iteration : 2952
train acc:  0.7109375
train loss:  0.5292116403579712
train gradient:  0.17149601750245613
iteration : 2953
train acc:  0.7421875
train loss:  0.4622995853424072
train gradient:  0.11685245716002553
iteration : 2954
train acc:  0.7421875
train loss:  0.5053325891494751
train gradient:  0.12353696969968507
iteration : 2955
train acc:  0.765625
train loss:  0.510912299156189
train gradient:  0.11971135225969595
iteration : 2956
train acc:  0.7734375
train loss:  0.4842233955860138
train gradient:  0.1127907784794053
iteration : 2957
train acc:  0.796875
train loss:  0.4644455909729004
train gradient:  0.12345117872704835
iteration : 2958
train acc:  0.71875
train loss:  0.4831763505935669
train gradient:  0.14364889884518647
iteration : 2959
train acc:  0.71875
train loss:  0.5580569505691528
train gradient:  0.17268996889693627
iteration : 2960
train acc:  0.7734375
train loss:  0.4099714159965515
train gradient:  0.08433021152616253
iteration : 2961
train acc:  0.75
train loss:  0.5094393491744995
train gradient:  0.12704028519877003
iteration : 2962
train acc:  0.6953125
train loss:  0.49924203753471375
train gradient:  0.12503889184495082
iteration : 2963
train acc:  0.8359375
train loss:  0.3977070748806
train gradient:  0.09324487796160193
iteration : 2964
train acc:  0.765625
train loss:  0.4984346330165863
train gradient:  0.1571617690945726
iteration : 2965
train acc:  0.78125
train loss:  0.44656479358673096
train gradient:  0.1070467973727884
iteration : 2966
train acc:  0.796875
train loss:  0.44163814187049866
train gradient:  0.09917783961357796
iteration : 2967
train acc:  0.7734375
train loss:  0.4714246392250061
train gradient:  0.11618556765560198
iteration : 2968
train acc:  0.7890625
train loss:  0.41925761103630066
train gradient:  0.09177921077650714
iteration : 2969
train acc:  0.8359375
train loss:  0.384170264005661
train gradient:  0.07248024720011328
iteration : 2970
train acc:  0.7421875
train loss:  0.4747774302959442
train gradient:  0.1364836162600792
iteration : 2971
train acc:  0.7421875
train loss:  0.4641135632991791
train gradient:  0.10453145140274422
iteration : 2972
