program start:
num_rounds= 0
node_emb_dim= 32

----------------------------------------new_epoch--------------------------------------

epoch:  0
iteration : 0
train acc:  0.4375
train loss:  0.7247984409332275
train gradient:  0.2824326755859624
iteration : 1
train acc:  0.453125
train loss:  0.7161508798599243
train gradient:  0.407051293878428
iteration : 2
train acc:  0.578125
train loss:  0.6950597763061523
train gradient:  0.3648141195931621
iteration : 3
train acc:  0.46875
train loss:  0.7148504853248596
train gradient:  0.21753037960756616
iteration : 4
train acc:  0.546875
train loss:  0.6881037354469299
train gradient:  0.325977461363806
iteration : 5
train acc:  0.5
train loss:  0.7098019123077393
train gradient:  0.3539086283308267
iteration : 6
train acc:  0.5390625
train loss:  0.6937357187271118
train gradient:  0.22818657729552605
iteration : 7
train acc:  0.609375
train loss:  0.6630963087081909
train gradient:  0.22734975833991145
iteration : 8
train acc:  0.609375
train loss:  0.6667479276657104
train gradient:  0.22960369020303817
iteration : 9
train acc:  0.5859375
train loss:  0.6623627543449402
train gradient:  0.23276999365791637
iteration : 10
train acc:  0.5546875
train loss:  0.6977825164794922
train gradient:  0.20055701384660116
iteration : 11
train acc:  0.640625
train loss:  0.6376547813415527
train gradient:  0.23435245877819677
iteration : 12
train acc:  0.65625
train loss:  0.6448858976364136
train gradient:  0.21803034471197658
iteration : 13
train acc:  0.6015625
train loss:  0.6734898090362549
train gradient:  0.20762930211213831
iteration : 14
train acc:  0.6015625
train loss:  0.6612135767936707
train gradient:  0.19106346092078652
iteration : 15
train acc:  0.546875
train loss:  0.7191145420074463
train gradient:  0.2406956815077826
iteration : 16
train acc:  0.546875
train loss:  0.6632803082466125
train gradient:  0.23142108339907944
iteration : 17
train acc:  0.5859375
train loss:  0.7040554285049438
train gradient:  0.2662841724761496
iteration : 18
train acc:  0.6640625
train loss:  0.6143292188644409
train gradient:  0.2795956848582016
iteration : 19
train acc:  0.6171875
train loss:  0.6565871238708496
train gradient:  0.20815640206988764
iteration : 20
train acc:  0.59375
train loss:  0.691726803779602
train gradient:  0.21332316564829717
iteration : 21
train acc:  0.6171875
train loss:  0.6750951409339905
train gradient:  0.171765808241548
iteration : 22
train acc:  0.640625
train loss:  0.6562931537628174
train gradient:  0.38590474676159414
iteration : 23
train acc:  0.59375
train loss:  0.6583843231201172
train gradient:  0.21773194755603792
iteration : 24
train acc:  0.6171875
train loss:  0.6692593693733215
train gradient:  0.2944751333675132
iteration : 25
train acc:  0.6328125
train loss:  0.6503024101257324
train gradient:  0.257996837376976
iteration : 26
train acc:  0.5703125
train loss:  0.6636791825294495
train gradient:  0.2585310932007389
iteration : 27
train acc:  0.6015625
train loss:  0.6526056528091431
train gradient:  0.1803864073040981
iteration : 28
train acc:  0.578125
train loss:  0.6703685522079468
train gradient:  0.2536638304365879
iteration : 29
train acc:  0.5625
train loss:  0.6776794195175171
train gradient:  0.340204433570754
iteration : 30
train acc:  0.6328125
train loss:  0.6586844325065613
train gradient:  0.2561067755609754
iteration : 31
train acc:  0.6171875
train loss:  0.652255654335022
train gradient:  0.23805144680635576
iteration : 32
train acc:  0.625
train loss:  0.6718993186950684
train gradient:  0.1630873200433774
iteration : 33
train acc:  0.6171875
train loss:  0.640061616897583
train gradient:  0.29850692030846926
iteration : 34
train acc:  0.59375
train loss:  0.679115355014801
train gradient:  0.24400092844889434
iteration : 35
train acc:  0.6171875
train loss:  0.6590315699577332
train gradient:  0.19492260110432758
iteration : 36
train acc:  0.6171875
train loss:  0.6725665330886841
train gradient:  0.17274059000831665
iteration : 37
train acc:  0.5703125
train loss:  0.6467679738998413
train gradient:  0.15836191520049153
iteration : 38
train acc:  0.59375
train loss:  0.6270149946212769
train gradient:  0.22283405224947933
iteration : 39
train acc:  0.609375
train loss:  0.6712682247161865
train gradient:  0.1691222969971843
iteration : 40
train acc:  0.609375
train loss:  0.6483170986175537
train gradient:  0.17815949955385807
iteration : 41
train acc:  0.5546875
train loss:  0.6606522798538208
train gradient:  0.18279134122086219
iteration : 42
train acc:  0.7109375
train loss:  0.5976923704147339
train gradient:  0.1802144446372536
iteration : 43
train acc:  0.6328125
train loss:  0.6339888572692871
train gradient:  0.15879250371710957
iteration : 44
train acc:  0.6484375
train loss:  0.6214087009429932
train gradient:  0.15397144404215918
iteration : 45
train acc:  0.65625
train loss:  0.6523560285568237
train gradient:  0.273996731666381
iteration : 46
train acc:  0.6953125
train loss:  0.6117346882820129
train gradient:  0.2559275948456529
iteration : 47
train acc:  0.625
train loss:  0.6278660297393799
train gradient:  0.17606291672952573
iteration : 48
train acc:  0.6796875
train loss:  0.6143069863319397
train gradient:  0.17412330189343342
iteration : 49
train acc:  0.6171875
train loss:  0.6669037342071533
train gradient:  0.1543534970059166
iteration : 50
train acc:  0.6640625
train loss:  0.6026961803436279
train gradient:  0.19316858615720794
iteration : 51
train acc:  0.625
train loss:  0.6696903109550476
train gradient:  0.2004931112740874
iteration : 52
train acc:  0.6953125
train loss:  0.5964665412902832
train gradient:  0.14463019440893837
iteration : 53
train acc:  0.5546875
train loss:  0.6687161922454834
train gradient:  0.24546882503558543
iteration : 54
train acc:  0.5390625
train loss:  0.7042397260665894
train gradient:  0.20829206846612353
iteration : 55
train acc:  0.5859375
train loss:  0.6965335607528687
train gradient:  0.18458485471966413
iteration : 56
train acc:  0.65625
train loss:  0.6480225324630737
train gradient:  0.19690289710328673
iteration : 57
train acc:  0.5546875
train loss:  0.6964463591575623
train gradient:  0.3114407737330172
iteration : 58
train acc:  0.5859375
train loss:  0.6791967153549194
train gradient:  0.2083509137015938
iteration : 59
train acc:  0.515625
train loss:  0.7108663320541382
train gradient:  0.2554838037983095
iteration : 60
train acc:  0.6328125
train loss:  0.6246859431266785
train gradient:  0.15880871644855998
iteration : 61
train acc:  0.6328125
train loss:  0.6668525338172913
train gradient:  0.21112086284453727
iteration : 62
train acc:  0.65625
train loss:  0.6299360990524292
train gradient:  0.15386681340480787
iteration : 63
train acc:  0.640625
train loss:  0.6157789826393127
train gradient:  0.256621707764996
iteration : 64
train acc:  0.609375
train loss:  0.6423719525337219
train gradient:  0.17137158721698403
iteration : 65
train acc:  0.671875
train loss:  0.6365754008293152
train gradient:  0.1426023567685576
iteration : 66
train acc:  0.6640625
train loss:  0.609483003616333
train gradient:  0.14622353579692163
iteration : 67
train acc:  0.6484375
train loss:  0.6333922147750854
train gradient:  0.22281474662661083
iteration : 68
train acc:  0.703125
train loss:  0.5883815288543701
train gradient:  0.1872642053605993
iteration : 69
train acc:  0.671875
train loss:  0.6308519840240479
train gradient:  0.17225353727004103
iteration : 70
train acc:  0.65625
train loss:  0.6350268721580505
train gradient:  0.1613563257498452
iteration : 71
train acc:  0.578125
train loss:  0.6685147285461426
train gradient:  0.14250029860531727
iteration : 72
train acc:  0.578125
train loss:  0.6302388310432434
train gradient:  0.26360629057089374
iteration : 73
train acc:  0.671875
train loss:  0.6217413544654846
train gradient:  0.17614894263943304
iteration : 74
train acc:  0.5390625
train loss:  0.6856406927108765
train gradient:  0.16415273691938556
iteration : 75
train acc:  0.59375
train loss:  0.6758794784545898
train gradient:  0.2203132545443445
iteration : 76
train acc:  0.671875
train loss:  0.6206459999084473
train gradient:  0.12960364414414222
iteration : 77
train acc:  0.6796875
train loss:  0.6069962382316589
train gradient:  0.2104346152680587
iteration : 78
train acc:  0.609375
train loss:  0.6196954846382141
train gradient:  0.15421343160122758
iteration : 79
train acc:  0.5859375
train loss:  0.6314552426338196
train gradient:  0.16685058644689477
iteration : 80
train acc:  0.5859375
train loss:  0.6702348589897156
train gradient:  0.17579269422509863
iteration : 81
train acc:  0.6328125
train loss:  0.6549303531646729
train gradient:  0.20008440905583533
iteration : 82
train acc:  0.625
train loss:  0.6179577708244324
train gradient:  0.17140683951677005
iteration : 83
train acc:  0.6640625
train loss:  0.5964603424072266
train gradient:  0.15106343812656037
iteration : 84
train acc:  0.671875
train loss:  0.6115639209747314
train gradient:  0.20827557201912264
iteration : 85
train acc:  0.578125
train loss:  0.6670297980308533
train gradient:  0.1721777903596094
iteration : 86
train acc:  0.625
train loss:  0.6383072137832642
train gradient:  0.15480694608783518
iteration : 87
train acc:  0.6484375
train loss:  0.6660668849945068
train gradient:  0.23392342816918685
iteration : 88
train acc:  0.6796875
train loss:  0.6103814840316772
train gradient:  0.1805395863512549
iteration : 89
train acc:  0.6171875
train loss:  0.6632202863693237
train gradient:  0.24699870645518904
iteration : 90
train acc:  0.703125
train loss:  0.5891225337982178
train gradient:  0.20575632426062793
iteration : 91
train acc:  0.5234375
train loss:  0.6853877305984497
train gradient:  0.25426996541789243
iteration : 92
train acc:  0.6640625
train loss:  0.6233521699905396
train gradient:  0.15630663633011388
iteration : 93
train acc:  0.6640625
train loss:  0.6068049073219299
train gradient:  0.2100122860294602
iteration : 94
train acc:  0.625
train loss:  0.6313797235488892
train gradient:  0.263729727731042
iteration : 95
train acc:  0.625
train loss:  0.6221356391906738
train gradient:  0.1330211302551505
iteration : 96
train acc:  0.6875
train loss:  0.5763624906539917
train gradient:  0.1588437837661803
iteration : 97
train acc:  0.6328125
train loss:  0.6093800067901611
train gradient:  0.15791753607693515
iteration : 98
train acc:  0.5625
train loss:  0.6976660490036011
train gradient:  0.19069059940716415
iteration : 99
train acc:  0.59375
train loss:  0.6517284512519836
train gradient:  0.18948945281388213
iteration : 100
train acc:  0.6484375
train loss:  0.6478380560874939
train gradient:  0.23641318076962548
iteration : 101
train acc:  0.625
train loss:  0.6368910074234009
train gradient:  0.15063165630772227
iteration : 102
train acc:  0.578125
train loss:  0.6527531147003174
train gradient:  0.17854648897199252
iteration : 103
train acc:  0.6640625
train loss:  0.6154205799102783
train gradient:  0.14163536095031296
iteration : 104
train acc:  0.671875
train loss:  0.618045449256897
train gradient:  0.15499132750598094
iteration : 105
train acc:  0.671875
train loss:  0.621253490447998
train gradient:  0.15032854065224563
iteration : 106
train acc:  0.671875
train loss:  0.628189206123352
train gradient:  0.1378614709755028
iteration : 107
train acc:  0.65625
train loss:  0.6392180919647217
train gradient:  0.21923747980444755
iteration : 108
train acc:  0.6015625
train loss:  0.634637713432312
train gradient:  0.15998650861588692
iteration : 109
train acc:  0.6171875
train loss:  0.6586769819259644
train gradient:  0.21883150336587637
iteration : 110
train acc:  0.71875
train loss:  0.5625494718551636
train gradient:  0.15336176433410784
iteration : 111
train acc:  0.671875
train loss:  0.6301194429397583
train gradient:  0.1571740702335742
iteration : 112
train acc:  0.65625
train loss:  0.6269400119781494
train gradient:  0.16568503925601663
iteration : 113
train acc:  0.546875
train loss:  0.6939460039138794
train gradient:  0.2358734469975958
iteration : 114
train acc:  0.6484375
train loss:  0.6289625763893127
train gradient:  0.18556619543423042
iteration : 115
train acc:  0.6171875
train loss:  0.6391094326972961
train gradient:  0.1268223042486045
iteration : 116
train acc:  0.6484375
train loss:  0.6187487840652466
train gradient:  0.16559302176011187
iteration : 117
train acc:  0.6484375
train loss:  0.6181834936141968
train gradient:  0.2039261461682004
iteration : 118
train acc:  0.65625
train loss:  0.6180702447891235
train gradient:  0.17519569539830948
iteration : 119
train acc:  0.6328125
train loss:  0.6298644542694092
train gradient:  0.17720651174892452
iteration : 120
train acc:  0.625
train loss:  0.6749526262283325
train gradient:  0.22553507413688795
iteration : 121
train acc:  0.6484375
train loss:  0.6058935523033142
train gradient:  0.1486341467784021
iteration : 122
train acc:  0.640625
train loss:  0.6264222860336304
train gradient:  0.15911914114292855
iteration : 123
train acc:  0.6015625
train loss:  0.6333776712417603
train gradient:  0.17905629128530556
iteration : 124
train acc:  0.6484375
train loss:  0.6275159120559692
train gradient:  0.2046831254458146
iteration : 125
train acc:  0.65625
train loss:  0.592987596988678
train gradient:  0.2213986195366361
iteration : 126
train acc:  0.640625
train loss:  0.6560181975364685
train gradient:  0.1852819714728577
iteration : 127
train acc:  0.6875
train loss:  0.5970981121063232
train gradient:  0.16048806636415253
iteration : 128
train acc:  0.6953125
train loss:  0.6147671937942505
train gradient:  0.15235081203208317
iteration : 129
train acc:  0.6484375
train loss:  0.6366950869560242
train gradient:  0.1494890123199572
iteration : 130
train acc:  0.65625
train loss:  0.6166936159133911
train gradient:  0.1773529991199437
iteration : 131
train acc:  0.703125
train loss:  0.597541093826294
train gradient:  0.16917957035357217
iteration : 132
train acc:  0.65625
train loss:  0.5752778053283691
train gradient:  0.16358222396815222
iteration : 133
train acc:  0.609375
train loss:  0.6665037870407104
train gradient:  0.20963539481011234
iteration : 134
train acc:  0.5859375
train loss:  0.6792820692062378
train gradient:  0.14153990119454635
iteration : 135
train acc:  0.65625
train loss:  0.6078541278839111
train gradient:  0.1629596003062528
iteration : 136
train acc:  0.5859375
train loss:  0.6284113526344299
train gradient:  0.25033705393604905
iteration : 137
train acc:  0.671875
train loss:  0.6245293617248535
train gradient:  0.17833279979257255
iteration : 138
train acc:  0.6796875
train loss:  0.5882898569107056
train gradient:  0.14443995200619286
iteration : 139
train acc:  0.640625
train loss:  0.6337940692901611
train gradient:  0.20016164512662732
iteration : 140
train acc:  0.625
train loss:  0.6417851448059082
train gradient:  0.21945294682527997
iteration : 141
train acc:  0.578125
train loss:  0.6338798999786377
train gradient:  0.14512703342781041
iteration : 142
train acc:  0.625
train loss:  0.6352777481079102
train gradient:  0.17124603894552765
iteration : 143
train acc:  0.6796875
train loss:  0.605014443397522
train gradient:  0.1642619637033489
iteration : 144
train acc:  0.703125
train loss:  0.5648527145385742
train gradient:  0.15131509355878336
iteration : 145
train acc:  0.6875
train loss:  0.6105806231498718
train gradient:  0.16344821198848505
iteration : 146
train acc:  0.6796875
train loss:  0.599442720413208
train gradient:  0.19455420087323783
iteration : 147
train acc:  0.6484375
train loss:  0.6240301728248596
train gradient:  0.1829826438245507
iteration : 148
train acc:  0.640625
train loss:  0.6313976049423218
train gradient:  0.14686153886840456
iteration : 149
train acc:  0.671875
train loss:  0.628074049949646
train gradient:  0.14773293649129304
iteration : 150
train acc:  0.75
train loss:  0.5620337128639221
train gradient:  0.19834129248617344
iteration : 151
train acc:  0.65625
train loss:  0.6376228332519531
train gradient:  0.15192993064350332
iteration : 152
train acc:  0.6015625
train loss:  0.6449304819107056
train gradient:  0.14776547322806893
iteration : 153
train acc:  0.6171875
train loss:  0.6459134221076965
train gradient:  0.21964586381135123
iteration : 154
train acc:  0.640625
train loss:  0.662004828453064
train gradient:  0.24560908222149203
iteration : 155
train acc:  0.7109375
train loss:  0.5784285068511963
train gradient:  0.16346667263429526
iteration : 156
train acc:  0.671875
train loss:  0.5903198719024658
train gradient:  0.1332038406794653
iteration : 157
train acc:  0.671875
train loss:  0.6261396408081055
train gradient:  0.1381666965584307
iteration : 158
train acc:  0.578125
train loss:  0.6662763357162476
train gradient:  0.18581044195325447
iteration : 159
train acc:  0.625
train loss:  0.618331253528595
train gradient:  0.15614599695176357
iteration : 160
train acc:  0.640625
train loss:  0.6615692377090454
train gradient:  0.19408830188508647
iteration : 161
train acc:  0.6640625
train loss:  0.6316355466842651
train gradient:  0.1366849813706978
iteration : 162
train acc:  0.7265625
train loss:  0.5656757354736328
train gradient:  0.22925282856607238
iteration : 163
train acc:  0.6640625
train loss:  0.5884021520614624
train gradient:  0.12861616869559478
iteration : 164
train acc:  0.703125
train loss:  0.6158234477043152
train gradient:  0.18582982912681045
iteration : 165
train acc:  0.625
train loss:  0.6610580086708069
train gradient:  0.15581145691080106
iteration : 166
train acc:  0.6484375
train loss:  0.6399624347686768
train gradient:  0.23032978803570942
iteration : 167
train acc:  0.6484375
train loss:  0.624133825302124
train gradient:  0.1888444824486742
iteration : 168
train acc:  0.671875
train loss:  0.6009489297866821
train gradient:  0.1498769071754607
iteration : 169
train acc:  0.7109375
train loss:  0.5635771751403809
train gradient:  0.16007067466010477
iteration : 170
train acc:  0.65625
train loss:  0.5875198841094971
train gradient:  0.12796971720642972
iteration : 171
train acc:  0.765625
train loss:  0.5466715097427368
train gradient:  0.17122025722502138
iteration : 172
train acc:  0.6875
train loss:  0.6141470074653625
train gradient:  0.1723110732314251
iteration : 173
train acc:  0.6484375
train loss:  0.6267456412315369
train gradient:  0.20241862133030702
iteration : 174
train acc:  0.625
train loss:  0.6191316843032837
train gradient:  0.16938099585675997
iteration : 175
train acc:  0.6875
train loss:  0.5609215497970581
train gradient:  0.1289360822626287
iteration : 176
train acc:  0.6328125
train loss:  0.6528077721595764
train gradient:  0.18106364152898935
iteration : 177
train acc:  0.6484375
train loss:  0.6100271940231323
train gradient:  0.18452686958116848
iteration : 178
train acc:  0.671875
train loss:  0.5880130529403687
train gradient:  0.25290614617804325
iteration : 179
train acc:  0.6015625
train loss:  0.6472370624542236
train gradient:  0.19917324286017254
iteration : 180
train acc:  0.703125
train loss:  0.5878241658210754
train gradient:  0.20367909252139993
iteration : 181
train acc:  0.65625
train loss:  0.6047192811965942
train gradient:  0.13886472262962252
iteration : 182
train acc:  0.6796875
train loss:  0.6027966737747192
train gradient:  0.16363470571279964
iteration : 183
train acc:  0.6484375
train loss:  0.6192806959152222
train gradient:  0.19527021339706807
iteration : 184
train acc:  0.5859375
train loss:  0.666775107383728
train gradient:  0.23555279211412342
iteration : 185
train acc:  0.5859375
train loss:  0.658420205116272
train gradient:  0.18569540453141214
iteration : 186
train acc:  0.6484375
train loss:  0.5871514081954956
train gradient:  0.144677452349588
iteration : 187
train acc:  0.71875
train loss:  0.5953723192214966
train gradient:  0.1726288933015796
iteration : 188
train acc:  0.6328125
train loss:  0.6184236407279968
train gradient:  0.19292774082956388
iteration : 189
train acc:  0.671875
train loss:  0.6361711025238037
train gradient:  0.15037947601861268
iteration : 190
train acc:  0.640625
train loss:  0.6204164028167725
train gradient:  0.22390801149576986
iteration : 191
train acc:  0.640625
train loss:  0.6453771591186523
train gradient:  0.17635684066210094
iteration : 192
train acc:  0.65625
train loss:  0.6015892028808594
train gradient:  0.17083521403514473
iteration : 193
train acc:  0.6484375
train loss:  0.6136561632156372
train gradient:  0.23461511777608804
iteration : 194
train acc:  0.703125
train loss:  0.5856232643127441
train gradient:  0.1240787492051601
iteration : 195
train acc:  0.671875
train loss:  0.6012552976608276
train gradient:  0.13627262381885744
iteration : 196
train acc:  0.6328125
train loss:  0.6214780211448669
train gradient:  0.16245592081146004
iteration : 197
train acc:  0.6796875
train loss:  0.6161753535270691
train gradient:  0.17315460685440648
iteration : 198
train acc:  0.6796875
train loss:  0.5900028347969055
train gradient:  0.1500064724355691
iteration : 199
train acc:  0.671875
train loss:  0.5705258846282959
train gradient:  0.1172549537629132
iteration : 200
train acc:  0.6171875
train loss:  0.595247745513916
train gradient:  0.13925124136567893
iteration : 201
train acc:  0.5625
train loss:  0.6219849586486816
train gradient:  0.31609855270617043
iteration : 202
train acc:  0.71875
train loss:  0.5852354764938354
train gradient:  0.17152709574587727
iteration : 203
train acc:  0.5703125
train loss:  0.6653931140899658
train gradient:  0.33567714144144656
iteration : 204
train acc:  0.6640625
train loss:  0.6162189245223999
train gradient:  0.1406462859718731
iteration : 205
train acc:  0.6953125
train loss:  0.5799216032028198
train gradient:  0.1297035420655656
iteration : 206
train acc:  0.71875
train loss:  0.5578727722167969
train gradient:  0.1988459315373847
iteration : 207
train acc:  0.625
train loss:  0.6573401689529419
train gradient:  0.21173738832564498
iteration : 208
train acc:  0.578125
train loss:  0.653517484664917
train gradient:  0.14642140777242252
iteration : 209
train acc:  0.625
train loss:  0.6425572037696838
train gradient:  0.18145344749749504
iteration : 210
train acc:  0.6640625
train loss:  0.616880476474762
train gradient:  0.18123588611377256
iteration : 211
train acc:  0.6015625
train loss:  0.6575007438659668
train gradient:  0.19918466085598205
iteration : 212
train acc:  0.59375
train loss:  0.6117328405380249
train gradient:  0.14109822895722335
iteration : 213
train acc:  0.609375
train loss:  0.662714958190918
train gradient:  0.27018300968769315
iteration : 214
train acc:  0.6875
train loss:  0.5922377109527588
train gradient:  0.15026716688337574
iteration : 215
train acc:  0.734375
train loss:  0.5727270841598511
train gradient:  0.13781822528238769
iteration : 216
train acc:  0.625
train loss:  0.6294444799423218
train gradient:  0.1717620634517909
iteration : 217
train acc:  0.640625
train loss:  0.6396560668945312
train gradient:  0.1601285164113549
iteration : 218
train acc:  0.7109375
train loss:  0.5743817090988159
train gradient:  0.14342849201841212
iteration : 219
train acc:  0.6640625
train loss:  0.6286362409591675
train gradient:  0.1574668099239749
iteration : 220
train acc:  0.6953125
train loss:  0.6073326468467712
train gradient:  0.15643272095945732
iteration : 221
train acc:  0.703125
train loss:  0.5361239910125732
train gradient:  0.17843438906261777
iteration : 222
train acc:  0.6953125
train loss:  0.6064240336418152
train gradient:  0.20523689944351553
iteration : 223
train acc:  0.6328125
train loss:  0.6097316741943359
train gradient:  0.19274563383335042
iteration : 224
train acc:  0.640625
train loss:  0.6188368797302246
train gradient:  0.17403120111285497
iteration : 225
train acc:  0.640625
train loss:  0.6253455281257629
train gradient:  0.21640333566999959
iteration : 226
train acc:  0.6484375
train loss:  0.5937262773513794
train gradient:  0.14498543135351538
iteration : 227
train acc:  0.640625
train loss:  0.634995698928833
train gradient:  0.12494802392025864
iteration : 228
train acc:  0.71875
train loss:  0.5643741488456726
train gradient:  0.17013389374975585
iteration : 229
train acc:  0.6015625
train loss:  0.6368460655212402
train gradient:  0.24759131901305692
iteration : 230
train acc:  0.71875
train loss:  0.5737115740776062
train gradient:  0.20396276868600194
iteration : 231
train acc:  0.5859375
train loss:  0.6430937051773071
train gradient:  0.16396892104614258
iteration : 232
train acc:  0.671875
train loss:  0.6139168739318848
train gradient:  0.13768624167465768
iteration : 233
train acc:  0.6875
train loss:  0.5867787599563599
train gradient:  0.15426897414790103
iteration : 234
train acc:  0.6328125
train loss:  0.6479652523994446
train gradient:  0.16251706141183414
iteration : 235
train acc:  0.6171875
train loss:  0.6078198552131653
train gradient:  0.15675638458571217
iteration : 236
train acc:  0.71875
train loss:  0.5881827473640442
train gradient:  0.14059376684885772
iteration : 237
train acc:  0.6484375
train loss:  0.6437078714370728
train gradient:  0.2311499032447658
iteration : 238
train acc:  0.6796875
train loss:  0.5670288801193237
train gradient:  0.17012909001322937
iteration : 239
train acc:  0.671875
train loss:  0.6000497937202454
train gradient:  0.16079106710580515
iteration : 240
train acc:  0.6640625
train loss:  0.6034247875213623
train gradient:  0.17149813271394243
iteration : 241
train acc:  0.6328125
train loss:  0.6231253147125244
train gradient:  0.1843725019710133
iteration : 242
train acc:  0.6796875
train loss:  0.6036708354949951
train gradient:  0.1906317052686729
iteration : 243
train acc:  0.65625
train loss:  0.6251434087753296
train gradient:  0.2266502143265664
iteration : 244
train acc:  0.625
train loss:  0.6383453011512756
train gradient:  0.16282187236183662
iteration : 245
train acc:  0.703125
train loss:  0.557632565498352
train gradient:  0.17076445799421286
iteration : 246
train acc:  0.71875
train loss:  0.5969458818435669
train gradient:  0.1570149369002758
iteration : 247
train acc:  0.65625
train loss:  0.6041874885559082
train gradient:  0.17191360833409436
iteration : 248
train acc:  0.578125
train loss:  0.6594754457473755
train gradient:  0.20913763440230143
iteration : 249
train acc:  0.6875
train loss:  0.6384425759315491
train gradient:  0.1444660954432475
iteration : 250
train acc:  0.625
train loss:  0.6498876810073853
train gradient:  0.17786027799832965
iteration : 251
train acc:  0.7734375
train loss:  0.5422701239585876
train gradient:  0.13939062266539842
iteration : 252
train acc:  0.6796875
train loss:  0.5989611148834229
train gradient:  0.13571836027068934
iteration : 253
train acc:  0.6171875
train loss:  0.6292351484298706
train gradient:  0.1687798826058226
iteration : 254
train acc:  0.640625
train loss:  0.6648108959197998
train gradient:  0.19887970606385363
iteration : 255
train acc:  0.6796875
train loss:  0.5955451726913452
train gradient:  0.2002648432091052
iteration : 256
train acc:  0.7109375
train loss:  0.5847230553627014
train gradient:  0.1853888914099388
iteration : 257
train acc:  0.7109375
train loss:  0.5594143867492676
train gradient:  0.1450733912026741
iteration : 258
train acc:  0.6640625
train loss:  0.5701497793197632
train gradient:  0.22939399084484863
iteration : 259
train acc:  0.703125
train loss:  0.6001346111297607
train gradient:  0.13243211788576142
iteration : 260
train acc:  0.6171875
train loss:  0.6448086500167847
train gradient:  0.16753708048701577
iteration : 261
train acc:  0.6015625
train loss:  0.6119743585586548
train gradient:  0.17341518798096384
iteration : 262
train acc:  0.5703125
train loss:  0.6360273361206055
train gradient:  0.12827468655234553
iteration : 263
train acc:  0.625
train loss:  0.61576247215271
train gradient:  0.2347274745947991
iteration : 264
train acc:  0.7109375
train loss:  0.5683702826499939
train gradient:  0.13216487102168717
iteration : 265
train acc:  0.7109375
train loss:  0.6059821844100952
train gradient:  0.17914239817417915
iteration : 266
train acc:  0.6328125
train loss:  0.5912672281265259
train gradient:  0.15472282611609686
iteration : 267
train acc:  0.671875
train loss:  0.6060274839401245
train gradient:  0.1545170443350028
iteration : 268
train acc:  0.625
train loss:  0.6415287256240845
train gradient:  0.2019841115726703
iteration : 269
train acc:  0.7265625
train loss:  0.5632303953170776
train gradient:  0.18125609076355545
iteration : 270
train acc:  0.671875
train loss:  0.6158808469772339
train gradient:  0.13726735832154255
iteration : 271
train acc:  0.6640625
train loss:  0.5816044211387634
train gradient:  0.15983893140890504
iteration : 272
train acc:  0.609375
train loss:  0.6500354409217834
train gradient:  0.19074160137358442
iteration : 273
train acc:  0.640625
train loss:  0.6228725910186768
train gradient:  0.20548966512001388
iteration : 274
train acc:  0.75
train loss:  0.5410667657852173
train gradient:  0.15976196623616523
iteration : 275
train acc:  0.7265625
train loss:  0.5684137344360352
train gradient:  0.1666309533689413
iteration : 276
train acc:  0.65625
train loss:  0.6413127183914185
train gradient:  0.2114472092828616
iteration : 277
train acc:  0.6171875
train loss:  0.6724147796630859
train gradient:  0.18487725259976853
iteration : 278
train acc:  0.5859375
train loss:  0.6705619096755981
train gradient:  0.20734593881309166
iteration : 279
train acc:  0.59375
train loss:  0.660983145236969
train gradient:  0.18354661920988657
iteration : 280
train acc:  0.6875
train loss:  0.5924476385116577
train gradient:  0.17423026845783485
iteration : 281
train acc:  0.7109375
train loss:  0.5547958612442017
train gradient:  0.15522170073927294
iteration : 282
train acc:  0.6640625
train loss:  0.617354691028595
train gradient:  0.28757081250509997
iteration : 283
train acc:  0.703125
train loss:  0.5944387912750244
train gradient:  0.17544696446217733
iteration : 284
train acc:  0.703125
train loss:  0.5879589319229126
train gradient:  0.1775365810429153
iteration : 285
train acc:  0.7109375
train loss:  0.5707287788391113
train gradient:  0.14903985276732212
iteration : 286
train acc:  0.625
train loss:  0.6404868364334106
train gradient:  0.1781953987104516
iteration : 287
train acc:  0.6015625
train loss:  0.6146430969238281
train gradient:  0.21703271111899064
iteration : 288
train acc:  0.671875
train loss:  0.5870658159255981
train gradient:  0.17644281508024756
iteration : 289
train acc:  0.6640625
train loss:  0.6060186624526978
train gradient:  0.15781407794336139
iteration : 290
train acc:  0.6328125
train loss:  0.6307166218757629
train gradient:  0.1820206378540782
iteration : 291
train acc:  0.6953125
train loss:  0.594581127166748
train gradient:  0.14266732466635035
iteration : 292
train acc:  0.6875
train loss:  0.5704188942909241
train gradient:  0.1338204718967361
iteration : 293
train acc:  0.6875
train loss:  0.6092365384101868
train gradient:  0.13549353838472708
iteration : 294
train acc:  0.7578125
train loss:  0.5621639490127563
train gradient:  0.21081595949268178
iteration : 295
train acc:  0.6875
train loss:  0.5706468820571899
train gradient:  0.1630829308022832
iteration : 296
train acc:  0.6484375
train loss:  0.592448353767395
train gradient:  0.15795282736117225
iteration : 297
train acc:  0.65625
train loss:  0.5923489332199097
train gradient:  0.18263555721770497
iteration : 298
train acc:  0.6796875
train loss:  0.601906418800354
train gradient:  0.14285497893725807
iteration : 299
train acc:  0.71875
train loss:  0.5523079037666321
train gradient:  0.13714673886742132
iteration : 300
train acc:  0.6171875
train loss:  0.6327227354049683
train gradient:  0.16903246385253562
iteration : 301
train acc:  0.65625
train loss:  0.6141003370285034
train gradient:  0.22734518303181506
iteration : 302
train acc:  0.6015625
train loss:  0.6358656883239746
train gradient:  0.1954796114823387
iteration : 303
train acc:  0.640625
train loss:  0.5884103178977966
train gradient:  0.2281049576165378
iteration : 304
train acc:  0.640625
train loss:  0.6180996894836426
train gradient:  0.20931434760774448
iteration : 305
train acc:  0.6171875
train loss:  0.6475959420204163
train gradient:  0.2942213040647745
iteration : 306
train acc:  0.671875
train loss:  0.5938426852226257
train gradient:  0.16755709805587937
iteration : 307
train acc:  0.6875
train loss:  0.6142874360084534
train gradient:  0.21947575256446722
iteration : 308
train acc:  0.6796875
train loss:  0.6213362216949463
train gradient:  0.17078997803572996
iteration : 309
train acc:  0.5859375
train loss:  0.6431193947792053
train gradient:  0.19517920843143954
iteration : 310
train acc:  0.7109375
train loss:  0.5891541838645935
train gradient:  0.13929999781955418
iteration : 311
train acc:  0.625
train loss:  0.619294285774231
train gradient:  0.12459058147753366
iteration : 312
train acc:  0.6640625
train loss:  0.6252226829528809
train gradient:  0.17311454060856424
iteration : 313
train acc:  0.515625
train loss:  0.6946555376052856
train gradient:  0.2448476351582957
iteration : 314
train acc:  0.6328125
train loss:  0.6491149663925171
train gradient:  0.19149881963261195
iteration : 315
train acc:  0.6015625
train loss:  0.6351779699325562
train gradient:  0.16451477410429433
iteration : 316
train acc:  0.6484375
train loss:  0.6215243339538574
train gradient:  0.21179385161134678
iteration : 317
train acc:  0.71875
train loss:  0.5714707374572754
train gradient:  0.18341094681095527
iteration : 318
train acc:  0.6328125
train loss:  0.6384406089782715
train gradient:  0.28862934865375034
iteration : 319
train acc:  0.546875
train loss:  0.6962289214134216
train gradient:  0.21836986971065175
iteration : 320
train acc:  0.6953125
train loss:  0.6223371028900146
train gradient:  0.13522083637494636
iteration : 321
train acc:  0.6796875
train loss:  0.6245717406272888
train gradient:  0.18587898962315802
iteration : 322
train acc:  0.6484375
train loss:  0.6067469716072083
train gradient:  0.1549936475184912
iteration : 323
train acc:  0.6640625
train loss:  0.5947195887565613
train gradient:  0.1631056701887376
iteration : 324
train acc:  0.6796875
train loss:  0.5730714201927185
train gradient:  0.11851841086329803
iteration : 325
train acc:  0.5625
train loss:  0.6504466533660889
train gradient:  0.19457390412290843
iteration : 326
train acc:  0.640625
train loss:  0.6109503507614136
train gradient:  0.18050066026967343
iteration : 327
train acc:  0.7109375
train loss:  0.5780149698257446
train gradient:  0.1608843627109297
iteration : 328
train acc:  0.6953125
train loss:  0.6218523383140564
train gradient:  0.19411144727743773
iteration : 329
train acc:  0.6171875
train loss:  0.6132193803787231
train gradient:  0.15267777917580438
iteration : 330
train acc:  0.640625
train loss:  0.6209039688110352
train gradient:  0.20142616396786334
iteration : 331
train acc:  0.6484375
train loss:  0.6307238936424255
train gradient:  0.1300247852715783
iteration : 332
train acc:  0.6015625
train loss:  0.624833881855011
train gradient:  0.22136570013749174
iteration : 333
train acc:  0.6953125
train loss:  0.5889749526977539
train gradient:  0.14514693090177044
iteration : 334
train acc:  0.6640625
train loss:  0.5958133935928345
train gradient:  0.18921789122087923
iteration : 335
train acc:  0.75
train loss:  0.5449205636978149
train gradient:  0.1731989878293308
iteration : 336
train acc:  0.6796875
train loss:  0.6154180765151978
train gradient:  0.18964902287819185
iteration : 337
train acc:  0.6875
train loss:  0.6027690172195435
train gradient:  0.17744296413412225
iteration : 338
train acc:  0.6953125
train loss:  0.5875349044799805
train gradient:  0.23953852590300412
iteration : 339
train acc:  0.6875
train loss:  0.6054424047470093
train gradient:  0.17286605227938262
iteration : 340
train acc:  0.5859375
train loss:  0.6508816480636597
train gradient:  0.21631449733790264
iteration : 341
train acc:  0.6796875
train loss:  0.593568742275238
train gradient:  0.16180626804381792
iteration : 342
train acc:  0.6328125
train loss:  0.6136312484741211
train gradient:  0.17677368790749684
iteration : 343
train acc:  0.671875
train loss:  0.6037115454673767
train gradient:  0.16285013857885344
iteration : 344
train acc:  0.7109375
train loss:  0.5752751231193542
train gradient:  0.19254130888772347
iteration : 345
train acc:  0.65625
train loss:  0.6045037508010864
train gradient:  0.18927164239377178
iteration : 346
train acc:  0.703125
train loss:  0.6110432147979736
train gradient:  0.1539337395334835
iteration : 347
train acc:  0.6640625
train loss:  0.5969751477241516
train gradient:  0.1719707561988828
iteration : 348
train acc:  0.609375
train loss:  0.6483547687530518
train gradient:  0.2335776734487009
iteration : 349
train acc:  0.65625
train loss:  0.6190882325172424
train gradient:  0.1838673647965658
iteration : 350
train acc:  0.5859375
train loss:  0.652357816696167
train gradient:  0.17388712426842
iteration : 351
train acc:  0.609375
train loss:  0.6576188206672668
train gradient:  0.20677897033815115
iteration : 352
train acc:  0.734375
train loss:  0.574752926826477
train gradient:  0.2406572673279593
iteration : 353
train acc:  0.609375
train loss:  0.6373856067657471
train gradient:  0.22148450815694404
iteration : 354
train acc:  0.6328125
train loss:  0.6160664558410645
train gradient:  0.20526862934971823
iteration : 355
train acc:  0.6796875
train loss:  0.5856568217277527
train gradient:  0.15389435020277553
iteration : 356
train acc:  0.6875
train loss:  0.5838384032249451
train gradient:  0.17615233766705995
iteration : 357
train acc:  0.7421875
train loss:  0.5399172306060791
train gradient:  0.17760593783851134
iteration : 358
train acc:  0.6875
train loss:  0.5913665890693665
train gradient:  0.17216314877126387
iteration : 359
train acc:  0.65625
train loss:  0.6186556816101074
train gradient:  0.210357192299576
iteration : 360
train acc:  0.6484375
train loss:  0.6380465030670166
train gradient:  0.16123859424449877
iteration : 361
train acc:  0.6640625
train loss:  0.5891513824462891
train gradient:  0.14267189009380146
iteration : 362
train acc:  0.703125
train loss:  0.5828336477279663
train gradient:  0.23266639876414152
iteration : 363
train acc:  0.671875
train loss:  0.5824012756347656
train gradient:  0.2692634982147473
iteration : 364
train acc:  0.6484375
train loss:  0.5954259037971497
train gradient:  0.16977043469706515
iteration : 365
train acc:  0.6796875
train loss:  0.6141427159309387
train gradient:  0.1584060577630147
iteration : 366
train acc:  0.6484375
train loss:  0.5748286247253418
train gradient:  0.20935579363894474
iteration : 367
train acc:  0.7265625
train loss:  0.589428722858429
train gradient:  0.1729313264863514
iteration : 368
train acc:  0.7578125
train loss:  0.5780860185623169
train gradient:  0.13074021167913938
iteration : 369
train acc:  0.6953125
train loss:  0.5737298727035522
train gradient:  0.19152184096858887
iteration : 370
train acc:  0.703125
train loss:  0.5939756631851196
train gradient:  0.12990073164711127
iteration : 371
train acc:  0.6328125
train loss:  0.5951166152954102
train gradient:  0.2050556697120774
iteration : 372
train acc:  0.6640625
train loss:  0.5917419195175171
train gradient:  0.18001823578717552
iteration : 373
train acc:  0.640625
train loss:  0.6146504282951355
train gradient:  0.21463940425543312
iteration : 374
train acc:  0.6640625
train loss:  0.6202561855316162
train gradient:  0.18927475452765186
iteration : 375
train acc:  0.703125
train loss:  0.5488247275352478
train gradient:  0.17198136532060443
iteration : 376
train acc:  0.6015625
train loss:  0.6377772092819214
train gradient:  0.23467178864655291
iteration : 377
train acc:  0.6171875
train loss:  0.6332595348358154
train gradient:  0.16492103517731257
iteration : 378
train acc:  0.7109375
train loss:  0.560337245464325
train gradient:  0.18896611177063638
iteration : 379
train acc:  0.6640625
train loss:  0.569635272026062
train gradient:  0.13289349188030897
iteration : 380
train acc:  0.609375
train loss:  0.6273547410964966
train gradient:  0.15761964803482453
iteration : 381
train acc:  0.6796875
train loss:  0.6134268641471863
train gradient:  0.1378068763260001
iteration : 382
train acc:  0.71875
train loss:  0.6042128205299377
train gradient:  0.17580007536296516
iteration : 383
train acc:  0.671875
train loss:  0.576162576675415
train gradient:  0.14552624020872998
iteration : 384
train acc:  0.625
train loss:  0.6179508566856384
train gradient:  0.18887877553413712
iteration : 385
train acc:  0.6953125
train loss:  0.5973759293556213
train gradient:  0.1340030512096401
iteration : 386
train acc:  0.6796875
train loss:  0.6448509693145752
train gradient:  0.22427441021011613
iteration : 387
train acc:  0.6796875
train loss:  0.6013889312744141
train gradient:  0.14327587599615504
iteration : 388
train acc:  0.625
train loss:  0.6409831047058105
train gradient:  0.2415025725334456
iteration : 389
train acc:  0.6328125
train loss:  0.623245358467102
train gradient:  0.16128204697825116
iteration : 390
train acc:  0.6484375
train loss:  0.596876859664917
train gradient:  0.1525564324152971
iteration : 391
train acc:  0.6796875
train loss:  0.5973044633865356
train gradient:  0.1712768658611145
iteration : 392
train acc:  0.703125
train loss:  0.5685856342315674
train gradient:  0.16071113781959057
iteration : 393
train acc:  0.5625
train loss:  0.6454275846481323
train gradient:  0.24754763399412477
iteration : 394
train acc:  0.671875
train loss:  0.6121358871459961
train gradient:  0.2655094681749305
iteration : 395
train acc:  0.6953125
train loss:  0.567398190498352
train gradient:  0.1608905162897264
iteration : 396
train acc:  0.703125
train loss:  0.5614446401596069
train gradient:  0.12374562951590651
iteration : 397
train acc:  0.640625
train loss:  0.6164990663528442
train gradient:  0.1296280633662005
iteration : 398
train acc:  0.6640625
train loss:  0.5930922031402588
train gradient:  0.14668772895979593
iteration : 399
train acc:  0.703125
train loss:  0.5859301090240479
train gradient:  0.16829032732285468
iteration : 400
train acc:  0.7265625
train loss:  0.5496246814727783
train gradient:  0.1495169000804774
iteration : 401
train acc:  0.703125
train loss:  0.5714919567108154
train gradient:  0.19559633461888243
iteration : 402
train acc:  0.65625
train loss:  0.5730676651000977
train gradient:  0.15113195010824687
iteration : 403
train acc:  0.640625
train loss:  0.6120744943618774
train gradient:  0.20080109435477325
iteration : 404
train acc:  0.6796875
train loss:  0.5832270979881287
train gradient:  0.18558469362865387
iteration : 405
train acc:  0.7109375
train loss:  0.5875025987625122
train gradient:  0.17821512567703798
iteration : 406
train acc:  0.6796875
train loss:  0.6161547899246216
train gradient:  0.1919660403938692
iteration : 407
train acc:  0.6640625
train loss:  0.5849413871765137
train gradient:  0.13347028631496383
iteration : 408
train acc:  0.6640625
train loss:  0.5934628844261169
train gradient:  0.15399421735282337
iteration : 409
train acc:  0.625
train loss:  0.6279009580612183
train gradient:  0.1573433782495627
iteration : 410
train acc:  0.703125
train loss:  0.5968286991119385
train gradient:  0.1392845003111154
iteration : 411
train acc:  0.6640625
train loss:  0.5886613130569458
train gradient:  0.14737695825321256
iteration : 412
train acc:  0.6484375
train loss:  0.6129364967346191
train gradient:  0.13491331532295642
iteration : 413
train acc:  0.703125
train loss:  0.5660847425460815
train gradient:  0.1883317641020199
iteration : 414
train acc:  0.7109375
train loss:  0.5796113014221191
train gradient:  0.13944096949590384
iteration : 415
train acc:  0.6328125
train loss:  0.626828134059906
train gradient:  0.17107018667245388
iteration : 416
train acc:  0.703125
train loss:  0.5868419408798218
train gradient:  0.1611823156428291
iteration : 417
train acc:  0.71875
train loss:  0.5780414342880249
train gradient:  0.1339846115638389
iteration : 418
train acc:  0.7109375
train loss:  0.5286169648170471
train gradient:  0.1440996105627357
iteration : 419
train acc:  0.6796875
train loss:  0.6030406951904297
train gradient:  0.17162841658190106
iteration : 420
train acc:  0.6875
train loss:  0.5606650114059448
train gradient:  0.17261924174138074
iteration : 421
train acc:  0.640625
train loss:  0.5999020338058472
train gradient:  0.2298242579458301
iteration : 422
train acc:  0.6875
train loss:  0.5547707080841064
train gradient:  0.18628648674966164
iteration : 423
train acc:  0.6484375
train loss:  0.604526937007904
train gradient:  0.18099070187594096
iteration : 424
train acc:  0.671875
train loss:  0.5916128158569336
train gradient:  0.17790630552268716
iteration : 425
train acc:  0.6640625
train loss:  0.6059753894805908
train gradient:  0.28121775043241787
iteration : 426
train acc:  0.6796875
train loss:  0.5968499779701233
train gradient:  0.17667716353346716
iteration : 427
train acc:  0.578125
train loss:  0.6412818431854248
train gradient:  0.2110737598184313
iteration : 428
train acc:  0.671875
train loss:  0.5973880290985107
train gradient:  0.19576251638873005
iteration : 429
train acc:  0.6484375
train loss:  0.6414073705673218
train gradient:  0.3089994210465295
iteration : 430
train acc:  0.71875
train loss:  0.5710184574127197
train gradient:  0.15791899515202396
iteration : 431
train acc:  0.6875
train loss:  0.583369791507721
train gradient:  0.1517141852713169
iteration : 432
train acc:  0.609375
train loss:  0.6002955436706543
train gradient:  0.170910446196815
iteration : 433
train acc:  0.65625
train loss:  0.6230376958847046
train gradient:  0.19848455150878558
iteration : 434
train acc:  0.703125
train loss:  0.6039568185806274
train gradient:  0.1346229331950528
iteration : 435
train acc:  0.6484375
train loss:  0.6062151193618774
train gradient:  0.17124893331105268
iteration : 436
train acc:  0.6953125
train loss:  0.6042599081993103
train gradient:  0.218083527061877
iteration : 437
train acc:  0.6796875
train loss:  0.5957444906234741
train gradient:  0.16469954488291372
iteration : 438
train acc:  0.65625
train loss:  0.5711960792541504
train gradient:  0.126880521596488
iteration : 439
train acc:  0.640625
train loss:  0.6027276515960693
train gradient:  0.13751234600366324
iteration : 440
train acc:  0.703125
train loss:  0.562347948551178
train gradient:  0.19594202367742886
iteration : 441
train acc:  0.6875
train loss:  0.5650621056556702
train gradient:  0.1585554495676012
iteration : 442
train acc:  0.71875
train loss:  0.603881299495697
train gradient:  0.1876743194195164
iteration : 443
train acc:  0.640625
train loss:  0.5941947102546692
train gradient:  0.13155535689809592
iteration : 444
train acc:  0.703125
train loss:  0.5927464962005615
train gradient:  0.13472812606783532
iteration : 445
train acc:  0.71875
train loss:  0.5349755883216858
train gradient:  0.2530056479539418
iteration : 446
train acc:  0.625
train loss:  0.6241140365600586
train gradient:  0.18045457041784846
iteration : 447
train acc:  0.6953125
train loss:  0.5781211853027344
train gradient:  0.17177504528899135
iteration : 448
train acc:  0.6484375
train loss:  0.5808528661727905
train gradient:  0.1607586275061625
iteration : 449
train acc:  0.7265625
train loss:  0.5907186269760132
train gradient:  0.13935063969895578
iteration : 450
train acc:  0.71875
train loss:  0.592136025428772
train gradient:  0.16092242131790693
iteration : 451
train acc:  0.6796875
train loss:  0.5877927541732788
train gradient:  0.16609398649101642
iteration : 452
train acc:  0.6953125
train loss:  0.5810494422912598
train gradient:  0.16068586205301147
iteration : 453
train acc:  0.640625
train loss:  0.6236239671707153
train gradient:  0.16301982958897407
iteration : 454
train acc:  0.6796875
train loss:  0.6403058767318726
train gradient:  0.2423876232404283
iteration : 455
train acc:  0.5859375
train loss:  0.6444275379180908
train gradient:  0.18534040351000694
iteration : 456
train acc:  0.6640625
train loss:  0.6147150993347168
train gradient:  0.14349446533483978
iteration : 457
train acc:  0.75
train loss:  0.5383214950561523
train gradient:  0.1427181354625539
iteration : 458
train acc:  0.7578125
train loss:  0.5318856835365295
train gradient:  0.22977200034009176
iteration : 459
train acc:  0.6640625
train loss:  0.5810961723327637
train gradient:  0.14060141505370824
iteration : 460
train acc:  0.7109375
train loss:  0.6090481281280518
train gradient:  0.16677685986755042
iteration : 461
train acc:  0.6328125
train loss:  0.6231136918067932
train gradient:  0.23836058764847906
iteration : 462
train acc:  0.6328125
train loss:  0.5895088911056519
train gradient:  0.20597166167867303
iteration : 463
train acc:  0.671875
train loss:  0.6145145893096924
train gradient:  0.18617398142898584
iteration : 464
train acc:  0.6875
train loss:  0.5952598452568054
train gradient:  0.18680055094797582
iteration : 465
train acc:  0.6953125
train loss:  0.5775575637817383
train gradient:  0.14479508773092264
iteration : 466
train acc:  0.6796875
train loss:  0.5726168751716614
train gradient:  0.1643996539425803
iteration : 467
train acc:  0.703125
train loss:  0.5539958477020264
train gradient:  0.17701630840018412
iteration : 468
train acc:  0.6796875
train loss:  0.5735765695571899
train gradient:  0.14457858075484964
iteration : 469
train acc:  0.703125
train loss:  0.5386102795600891
train gradient:  0.18870606392881137
iteration : 470
train acc:  0.671875
train loss:  0.6317969560623169
train gradient:  0.17881238782464007
iteration : 471
train acc:  0.6171875
train loss:  0.587756872177124
train gradient:  0.20937856605964167
iteration : 472
train acc:  0.7265625
train loss:  0.5724149942398071
train gradient:  0.17066495618117866
iteration : 473
train acc:  0.6796875
train loss:  0.6252018213272095
train gradient:  0.19232301010929875
iteration : 474
train acc:  0.6796875
train loss:  0.5850430727005005
train gradient:  0.17994608710839521
iteration : 475
train acc:  0.6953125
train loss:  0.5639817714691162
train gradient:  0.1344579971878047
iteration : 476
train acc:  0.640625
train loss:  0.6190617680549622
train gradient:  0.24195626771789164
iteration : 477
train acc:  0.6484375
train loss:  0.6333829164505005
train gradient:  0.14508769967076637
iteration : 478
train acc:  0.671875
train loss:  0.5825125575065613
train gradient:  0.21852268330790817
iteration : 479
train acc:  0.6640625
train loss:  0.5663334727287292
train gradient:  0.12998370342279691
iteration : 480
train acc:  0.71875
train loss:  0.5666975975036621
train gradient:  0.151501555518832
iteration : 481
train acc:  0.703125
train loss:  0.5538759827613831
train gradient:  0.23011106256078556
iteration : 482
train acc:  0.671875
train loss:  0.6081728935241699
train gradient:  0.15368812611256227
iteration : 483
train acc:  0.625
train loss:  0.6129220724105835
train gradient:  0.1928167131724514
iteration : 484
train acc:  0.65625
train loss:  0.6218637228012085
train gradient:  0.21910063832686466
iteration : 485
train acc:  0.6953125
train loss:  0.580267071723938
train gradient:  0.17420746093662826
iteration : 486
train acc:  0.7109375
train loss:  0.5652745962142944
train gradient:  0.16105512114173587
iteration : 487
train acc:  0.6953125
train loss:  0.6000235080718994
train gradient:  0.1634265862519388
iteration : 488
train acc:  0.7265625
train loss:  0.5443844199180603
train gradient:  0.152235905063495
iteration : 489
train acc:  0.625
train loss:  0.6174033880233765
train gradient:  0.1370010010402597
iteration : 490
train acc:  0.6875
train loss:  0.5932335257530212
train gradient:  0.20924318456959032
iteration : 491
train acc:  0.703125
train loss:  0.5866661071777344
train gradient:  0.17187581132340582
iteration : 492
train acc:  0.6484375
train loss:  0.6393508315086365
train gradient:  0.161852858021548
iteration : 493
train acc:  0.6953125
train loss:  0.5926893949508667
train gradient:  0.15429859317753347
iteration : 494
train acc:  0.6328125
train loss:  0.6101891398429871
train gradient:  0.23066579445351493
iteration : 495
train acc:  0.7109375
train loss:  0.5841127038002014
train gradient:  0.33507816168700033
iteration : 496
train acc:  0.625
train loss:  0.6008726358413696
train gradient:  0.1575374254076514
iteration : 497
train acc:  0.734375
train loss:  0.5574758648872375
train gradient:  0.161656366672132
iteration : 498
train acc:  0.703125
train loss:  0.598697304725647
train gradient:  0.1540027683136776
iteration : 499
train acc:  0.65625
train loss:  0.6097501516342163
train gradient:  0.13864975026139825
iteration : 500
train acc:  0.59375
train loss:  0.653105616569519
train gradient:  0.19447262508609253
iteration : 501
train acc:  0.6171875
train loss:  0.6105127334594727
train gradient:  0.20589911819636
iteration : 502
train acc:  0.734375
train loss:  0.5606443881988525
train gradient:  0.17220809574594553
iteration : 503
train acc:  0.6484375
train loss:  0.6001192331314087
train gradient:  0.18452697487814174
iteration : 504
train acc:  0.6953125
train loss:  0.5791865587234497
train gradient:  0.18442734293394408
iteration : 505
train acc:  0.7578125
train loss:  0.5435729622840881
train gradient:  0.14021859595527392
iteration : 506
train acc:  0.65625
train loss:  0.5823730826377869
train gradient:  0.1213555109923122
iteration : 507
train acc:  0.640625
train loss:  0.6058011651039124
train gradient:  0.1936535605602409
iteration : 508
train acc:  0.71875
train loss:  0.5764666199684143
train gradient:  0.1334828457478717
iteration : 509
train acc:  0.703125
train loss:  0.5772765874862671
train gradient:  0.16013686958987738
iteration : 510
train acc:  0.7109375
train loss:  0.5403250455856323
train gradient:  0.15322997736141453
iteration : 511
train acc:  0.6484375
train loss:  0.5902807712554932
train gradient:  0.14219190465646625
iteration : 512
train acc:  0.734375
train loss:  0.5413292646408081
train gradient:  0.13239536864110818
iteration : 513
train acc:  0.625
train loss:  0.5785942673683167
train gradient:  0.17517129300573925
iteration : 514
train acc:  0.671875
train loss:  0.6049811244010925
train gradient:  0.16176043350032407
iteration : 515
train acc:  0.5859375
train loss:  0.6485919952392578
train gradient:  0.1397665148523088
iteration : 516
train acc:  0.734375
train loss:  0.5341240763664246
train gradient:  0.18772898529120075
iteration : 517
train acc:  0.6015625
train loss:  0.6037911772727966
train gradient:  0.21010398913898304
iteration : 518
train acc:  0.6640625
train loss:  0.583670973777771
train gradient:  0.18481941598347157
iteration : 519
train acc:  0.6484375
train loss:  0.5766056180000305
train gradient:  0.15080958025305807
iteration : 520
train acc:  0.6171875
train loss:  0.6333910226821899
train gradient:  0.18819297321445594
iteration : 521
train acc:  0.6484375
train loss:  0.6330664157867432
train gradient:  0.22576470118647213
iteration : 522
train acc:  0.625
train loss:  0.5965247750282288
train gradient:  0.21688666211746
iteration : 523
train acc:  0.6484375
train loss:  0.6120332479476929
train gradient:  0.19093615482890802
iteration : 524
train acc:  0.6953125
train loss:  0.5640190839767456
train gradient:  0.1513249493050909
iteration : 525
train acc:  0.6640625
train loss:  0.6250629425048828
train gradient:  0.18195503216110975
iteration : 526
train acc:  0.7578125
train loss:  0.498964786529541
train gradient:  0.1350436755057653
iteration : 527
train acc:  0.6875
train loss:  0.5673613548278809
train gradient:  0.15018905433455426
iteration : 528
train acc:  0.65625
train loss:  0.635654091835022
train gradient:  0.24051853494748038
iteration : 529
train acc:  0.7578125
train loss:  0.49220603704452515
train gradient:  0.21060560466345973
iteration : 530
train acc:  0.625
train loss:  0.6323080062866211
train gradient:  0.1881391165478819
iteration : 531
train acc:  0.625
train loss:  0.6355099081993103
train gradient:  0.14299740412656284
iteration : 532
train acc:  0.671875
train loss:  0.6252713203430176
train gradient:  0.1670084178834356
iteration : 533
train acc:  0.703125
train loss:  0.538914144039154
train gradient:  0.1810184693936217
iteration : 534
train acc:  0.78125
train loss:  0.5248860716819763
train gradient:  0.24361259975653948
iteration : 535
train acc:  0.640625
train loss:  0.5748998522758484
train gradient:  0.21029375767097275
iteration : 536
train acc:  0.6796875
train loss:  0.5864949226379395
train gradient:  0.16820980937479152
iteration : 537
train acc:  0.6796875
train loss:  0.5940208435058594
train gradient:  0.14398329078267358
iteration : 538
train acc:  0.65625
train loss:  0.6136118769645691
train gradient:  0.25435486314755035
iteration : 539
train acc:  0.6953125
train loss:  0.5945303440093994
train gradient:  0.19637140744687714
iteration : 540
train acc:  0.765625
train loss:  0.545543909072876
train gradient:  0.12506552085281397
iteration : 541
train acc:  0.671875
train loss:  0.5904536247253418
train gradient:  0.2122195711494983
iteration : 542
train acc:  0.75
train loss:  0.5686860680580139
train gradient:  0.1730512346841745
iteration : 543
train acc:  0.6796875
train loss:  0.5762729644775391
train gradient:  0.21498938023800507
iteration : 544
train acc:  0.6484375
train loss:  0.5748173594474792
train gradient:  0.2098537696016352
iteration : 545
train acc:  0.6328125
train loss:  0.6204280257225037
train gradient:  0.14161499569512853
iteration : 546
train acc:  0.65625
train loss:  0.5995216369628906
train gradient:  0.1876840517248835
iteration : 547
train acc:  0.6640625
train loss:  0.6106933355331421
train gradient:  0.2245338670783718
iteration : 548
train acc:  0.671875
train loss:  0.5790133476257324
train gradient:  0.15381176231366736
iteration : 549
train acc:  0.71875
train loss:  0.5725898742675781
train gradient:  0.1531460233375354
iteration : 550
train acc:  0.6328125
train loss:  0.5823038220405579
train gradient:  0.1457306031122783
iteration : 551
train acc:  0.609375
train loss:  0.6653863191604614
train gradient:  0.24154017382840712
iteration : 552
train acc:  0.6640625
train loss:  0.5819620490074158
train gradient:  0.18053366570231077
iteration : 553
train acc:  0.6796875
train loss:  0.5676498413085938
train gradient:  0.172817898168482
iteration : 554
train acc:  0.6640625
train loss:  0.5995981693267822
train gradient:  0.1603603712507643
iteration : 555
train acc:  0.6015625
train loss:  0.6137242317199707
train gradient:  0.27529863065467003
iteration : 556
train acc:  0.6796875
train loss:  0.620439887046814
train gradient:  0.1820951668560345
iteration : 557
train acc:  0.7265625
train loss:  0.5559473037719727
train gradient:  0.17623649713607994
iteration : 558
train acc:  0.7109375
train loss:  0.5898789167404175
train gradient:  0.19896243781030287
iteration : 559
train acc:  0.6484375
train loss:  0.6133156418800354
train gradient:  0.14184964177226633
iteration : 560
train acc:  0.734375
train loss:  0.5531650185585022
train gradient:  0.15181000338130912
iteration : 561
train acc:  0.640625
train loss:  0.6271896958351135
train gradient:  0.1942200181341936
iteration : 562
train acc:  0.6640625
train loss:  0.5652456283569336
train gradient:  0.21968899769222383
iteration : 563
train acc:  0.6484375
train loss:  0.6504366397857666
train gradient:  0.3856333134918492
iteration : 564
train acc:  0.6328125
train loss:  0.6388962268829346
train gradient:  0.2820701744104858
iteration : 565
train acc:  0.7265625
train loss:  0.5429489016532898
train gradient:  0.18235290893479372
iteration : 566
train acc:  0.625
train loss:  0.6365754008293152
train gradient:  0.17971445645862316
iteration : 567
train acc:  0.7109375
train loss:  0.5689946413040161
train gradient:  0.16816837605688156
iteration : 568
train acc:  0.65625
train loss:  0.6155126094818115
train gradient:  0.151078251640473
iteration : 569
train acc:  0.6328125
train loss:  0.5659555792808533
train gradient:  0.15336157549953966
iteration : 570
train acc:  0.640625
train loss:  0.6511549949645996
train gradient:  0.19827965954205157
iteration : 571
train acc:  0.7265625
train loss:  0.5113768577575684
train gradient:  0.11857579422289938
iteration : 572
train acc:  0.6328125
train loss:  0.6178581714630127
train gradient:  0.18818202578225002
iteration : 573
train acc:  0.71875
train loss:  0.5437131524085999
train gradient:  0.14389545495552253
iteration : 574
train acc:  0.75
train loss:  0.5680386424064636
train gradient:  0.15769951790310682
iteration : 575
train acc:  0.6796875
train loss:  0.5978724956512451
train gradient:  0.19163209557576488
iteration : 576
train acc:  0.6328125
train loss:  0.6563177108764648
train gradient:  0.1866500040219725
iteration : 577
train acc:  0.5859375
train loss:  0.6360347270965576
train gradient:  0.16814656870678724
iteration : 578
train acc:  0.671875
train loss:  0.5884864330291748
train gradient:  0.15794205963310412
iteration : 579
train acc:  0.609375
train loss:  0.6240109205245972
train gradient:  0.17305314790164805
iteration : 580
train acc:  0.6328125
train loss:  0.6256875991821289
train gradient:  0.2042115321404892
iteration : 581
train acc:  0.6953125
train loss:  0.6339058876037598
train gradient:  0.21571271891672572
iteration : 582
train acc:  0.6796875
train loss:  0.5941460132598877
train gradient:  0.14210157957190453
iteration : 583
train acc:  0.6875
train loss:  0.6154627799987793
train gradient:  0.22833256574799105
iteration : 584
train acc:  0.6875
train loss:  0.6151204109191895
train gradient:  0.1751869870092599
iteration : 585
train acc:  0.71875
train loss:  0.5853550434112549
train gradient:  0.12475486918490716
iteration : 586
train acc:  0.6015625
train loss:  0.6334993243217468
train gradient:  0.2030400784562536
iteration : 587
train acc:  0.7421875
train loss:  0.5716884136199951
train gradient:  0.19783927114947245
iteration : 588
train acc:  0.703125
train loss:  0.5717122554779053
train gradient:  0.20554605006458984
iteration : 589
train acc:  0.6796875
train loss:  0.5721118450164795
train gradient:  0.16251125428076366
iteration : 590
train acc:  0.671875
train loss:  0.5844488143920898
train gradient:  0.22640918253912912
iteration : 591
train acc:  0.6953125
train loss:  0.5702604651451111
train gradient:  0.12712030798324064
iteration : 592
train acc:  0.734375
train loss:  0.5675089359283447
train gradient:  0.13177771189379772
iteration : 593
train acc:  0.703125
train loss:  0.5768387317657471
train gradient:  0.12808718552424
iteration : 594
train acc:  0.671875
train loss:  0.6065297722816467
train gradient:  0.1873579733920085
iteration : 595
train acc:  0.765625
train loss:  0.5856271982192993
train gradient:  0.13875941491340088
iteration : 596
train acc:  0.71875
train loss:  0.52513587474823
train gradient:  0.12335664615944472
iteration : 597
train acc:  0.765625
train loss:  0.5395602583885193
train gradient:  0.12405557444763769
iteration : 598
train acc:  0.703125
train loss:  0.5897988080978394
train gradient:  0.18222778040289905
iteration : 599
train acc:  0.7421875
train loss:  0.5541787147521973
train gradient:  0.13366482135132635
iteration : 600
train acc:  0.6796875
train loss:  0.6044580936431885
train gradient:  0.164168092584495
iteration : 601
train acc:  0.71875
train loss:  0.5746203064918518
train gradient:  0.12989084702050185
iteration : 602
train acc:  0.6484375
train loss:  0.6000692844390869
train gradient:  0.1739823707247739
iteration : 603
train acc:  0.671875
train loss:  0.5794252157211304
train gradient:  0.14798276597978977
iteration : 604
train acc:  0.71875
train loss:  0.5563740730285645
train gradient:  0.11954177751652255
iteration : 605
train acc:  0.75
train loss:  0.5537508726119995
train gradient:  0.15744018801260687
iteration : 606
train acc:  0.6484375
train loss:  0.596367359161377
train gradient:  0.19581456504633438
iteration : 607
train acc:  0.6328125
train loss:  0.6056524515151978
train gradient:  0.13020295646205476
iteration : 608
train acc:  0.765625
train loss:  0.5352945327758789
train gradient:  0.109103691936117
iteration : 609
train acc:  0.6640625
train loss:  0.5795920491218567
train gradient:  0.16093163534232136
iteration : 610
train acc:  0.65625
train loss:  0.5936663150787354
train gradient:  0.1653977149899315
iteration : 611
train acc:  0.640625
train loss:  0.5826340913772583
train gradient:  0.24337509825422188
iteration : 612
train acc:  0.671875
train loss:  0.595379114151001
train gradient:  0.2400152492957269
iteration : 613
train acc:  0.7109375
train loss:  0.555443525314331
train gradient:  0.14101093104214
iteration : 614
train acc:  0.7109375
train loss:  0.5647357106208801
train gradient:  0.19785074449808554
iteration : 615
train acc:  0.640625
train loss:  0.5754584074020386
train gradient:  0.12385163045456997
iteration : 616
train acc:  0.65625
train loss:  0.5993167757987976
train gradient:  0.12401701737851976
iteration : 617
train acc:  0.65625
train loss:  0.6497294902801514
train gradient:  0.1510369423998193
iteration : 618
train acc:  0.7265625
train loss:  0.526532769203186
train gradient:  0.12393825057848358
iteration : 619
train acc:  0.7109375
train loss:  0.5432710647583008
train gradient:  0.1447797245163603
iteration : 620
train acc:  0.7109375
train loss:  0.5837842226028442
train gradient:  0.17673808793243134
iteration : 621
train acc:  0.609375
train loss:  0.6311047077178955
train gradient:  0.2205706235489402
iteration : 622
train acc:  0.71875
train loss:  0.559632420539856
train gradient:  0.20124349713384196
iteration : 623
train acc:  0.671875
train loss:  0.5933215022087097
train gradient:  0.16023816052917142
iteration : 624
train acc:  0.7421875
train loss:  0.5570782423019409
train gradient:  0.14994901439147787
iteration : 625
train acc:  0.6640625
train loss:  0.6021677255630493
train gradient:  0.18942313464449545
iteration : 626
train acc:  0.703125
train loss:  0.5347272157669067
train gradient:  0.23305947856763048
iteration : 627
train acc:  0.6328125
train loss:  0.6276810169219971
train gradient:  0.2151225636290299
iteration : 628
train acc:  0.6796875
train loss:  0.6099636554718018
train gradient:  0.22952746986612294
iteration : 629
train acc:  0.6484375
train loss:  0.6197056770324707
train gradient:  0.246731815691416
iteration : 630
train acc:  0.703125
train loss:  0.5352545976638794
train gradient:  0.174102065788949
iteration : 631
train acc:  0.6875
train loss:  0.5910731554031372
train gradient:  0.21248981310236947
iteration : 632
train acc:  0.703125
train loss:  0.5394001007080078
train gradient:  0.1453100964810623
iteration : 633
train acc:  0.6875
train loss:  0.6068276166915894
train gradient:  0.18223899996552337
iteration : 634
train acc:  0.71875
train loss:  0.5645995736122131
train gradient:  0.1361822354699475
iteration : 635
train acc:  0.734375
train loss:  0.5615355372428894
train gradient:  0.22736175340791437
iteration : 636
train acc:  0.6796875
train loss:  0.581184983253479
train gradient:  0.1134431977205758
iteration : 637
train acc:  0.6953125
train loss:  0.5785549283027649
train gradient:  0.1488969090689956
iteration : 638
train acc:  0.703125
train loss:  0.5704062581062317
train gradient:  0.12178976681699784
iteration : 639
train acc:  0.6640625
train loss:  0.5794714689254761
train gradient:  0.17991773640004022
iteration : 640
train acc:  0.703125
train loss:  0.6031186580657959
train gradient:  0.24616585558356868
iteration : 641
train acc:  0.6875
train loss:  0.5999195575714111
train gradient:  0.1834298634433488
iteration : 642
train acc:  0.640625
train loss:  0.584420919418335
train gradient:  0.13351120706839015
iteration : 643
train acc:  0.71875
train loss:  0.545809268951416
train gradient:  0.13066018601583773
iteration : 644
train acc:  0.6796875
train loss:  0.600321352481842
train gradient:  0.17382384312191188
iteration : 645
train acc:  0.65625
train loss:  0.6070972681045532
train gradient:  0.16659567973264944
iteration : 646
train acc:  0.671875
train loss:  0.6064085364341736
train gradient:  0.13211113935811336
iteration : 647
train acc:  0.65625
train loss:  0.5973350405693054
train gradient:  0.14095435045439553
iteration : 648
train acc:  0.6484375
train loss:  0.5914955139160156
train gradient:  0.14026157989652233
iteration : 649
train acc:  0.6953125
train loss:  0.5831648111343384
train gradient:  0.15862341301422628
iteration : 650
train acc:  0.640625
train loss:  0.6393037438392639
train gradient:  0.17439531691086757
iteration : 651
train acc:  0.59375
train loss:  0.6177968978881836
train gradient:  0.23964401030052535
iteration : 652
train acc:  0.6796875
train loss:  0.5602328777313232
train gradient:  0.17730043119489458
iteration : 653
train acc:  0.671875
train loss:  0.584932804107666
train gradient:  0.13606731831423213
iteration : 654
train acc:  0.703125
train loss:  0.5489856600761414
train gradient:  0.21636293757251412
iteration : 655
train acc:  0.65625
train loss:  0.5822885036468506
train gradient:  0.14437803489707185
iteration : 656
train acc:  0.578125
train loss:  0.6577811241149902
train gradient:  0.23303317883471847
iteration : 657
train acc:  0.6484375
train loss:  0.5941658020019531
train gradient:  0.16530060395714152
iteration : 658
train acc:  0.6875
train loss:  0.5668313503265381
train gradient:  0.17369303589980906
iteration : 659
train acc:  0.6484375
train loss:  0.6592909693717957
train gradient:  0.21325910824667388
iteration : 660
train acc:  0.6328125
train loss:  0.6313489675521851
train gradient:  0.15920837190012097
iteration : 661
train acc:  0.703125
train loss:  0.5350760221481323
train gradient:  0.1450579744407655
iteration : 662
train acc:  0.65625
train loss:  0.632622480392456
train gradient:  0.2991929022592561
iteration : 663
train acc:  0.7421875
train loss:  0.5576099753379822
train gradient:  0.12122506938368709
iteration : 664
train acc:  0.6640625
train loss:  0.5562793016433716
train gradient:  0.16514690830763945
iteration : 665
train acc:  0.734375
train loss:  0.507614254951477
train gradient:  0.19306471210632953
iteration : 666
train acc:  0.6484375
train loss:  0.5693361759185791
train gradient:  0.20838032515153715
iteration : 667
train acc:  0.71875
train loss:  0.5708813071250916
train gradient:  0.1566280046720516
iteration : 668
train acc:  0.640625
train loss:  0.6016190648078918
train gradient:  0.1797963192132051
iteration : 669
train acc:  0.65625
train loss:  0.5709228515625
train gradient:  0.22766756419472856
iteration : 670
train acc:  0.6796875
train loss:  0.5633088946342468
train gradient:  0.10997904746283971
iteration : 671
train acc:  0.578125
train loss:  0.6755078434944153
train gradient:  0.22305494411766696
iteration : 672
train acc:  0.6953125
train loss:  0.5865541696548462
train gradient:  0.1801242079701888
iteration : 673
train acc:  0.6796875
train loss:  0.5740125179290771
train gradient:  0.1489643305090015
iteration : 674
train acc:  0.7109375
train loss:  0.569127082824707
train gradient:  0.19411997867075914
iteration : 675
train acc:  0.65625
train loss:  0.6366555690765381
train gradient:  0.26495912481439365
iteration : 676
train acc:  0.6640625
train loss:  0.5801446437835693
train gradient:  0.24374415332357524
iteration : 677
train acc:  0.7109375
train loss:  0.5911091566085815
train gradient:  0.17188204774320454
iteration : 678
train acc:  0.6953125
train loss:  0.5830124020576477
train gradient:  0.1456150093402243
iteration : 679
train acc:  0.7734375
train loss:  0.5250570774078369
train gradient:  0.14128906816387782
iteration : 680
train acc:  0.703125
train loss:  0.5465396046638489
train gradient:  0.17052438987725543
iteration : 681
train acc:  0.65625
train loss:  0.6088989973068237
train gradient:  0.15318715280672732
iteration : 682
train acc:  0.6640625
train loss:  0.6316089630126953
train gradient:  0.20869415773829408
iteration : 683
train acc:  0.6953125
train loss:  0.6034930944442749
train gradient:  0.15428082521115338
iteration : 684
train acc:  0.6875
train loss:  0.5499821305274963
train gradient:  0.14241284216777927
iteration : 685
train acc:  0.71875
train loss:  0.5458539128303528
train gradient:  0.12433369443144815
iteration : 686
train acc:  0.6640625
train loss:  0.5878217220306396
train gradient:  0.14238987707116182
iteration : 687
train acc:  0.7890625
train loss:  0.5195635557174683
train gradient:  0.1356761409593642
iteration : 688
train acc:  0.6875
train loss:  0.6171579360961914
train gradient:  0.21388730613714635
iteration : 689
train acc:  0.625
train loss:  0.5944865345954895
train gradient:  0.14329458351294502
iteration : 690
train acc:  0.6796875
train loss:  0.5905197858810425
train gradient:  0.1914141883235947
iteration : 691
train acc:  0.6484375
train loss:  0.6168128252029419
train gradient:  0.1951122471931151
iteration : 692
train acc:  0.6328125
train loss:  0.6135938167572021
train gradient:  0.2518183191355865
iteration : 693
train acc:  0.65625
train loss:  0.6230764389038086
train gradient:  0.1833987296018646
iteration : 694
train acc:  0.703125
train loss:  0.5848127603530884
train gradient:  0.1637137085713083
iteration : 695
train acc:  0.7421875
train loss:  0.527511477470398
train gradient:  0.1463853207340308
iteration : 696
train acc:  0.703125
train loss:  0.5535809993743896
train gradient:  0.18346845848612975
iteration : 697
train acc:  0.609375
train loss:  0.6102079749107361
train gradient:  0.2609651885089524
iteration : 698
train acc:  0.6953125
train loss:  0.5486570596694946
train gradient:  0.1371980038710855
iteration : 699
train acc:  0.7421875
train loss:  0.5739803314208984
train gradient:  0.16869329442251202
iteration : 700
train acc:  0.6953125
train loss:  0.5893647074699402
train gradient:  0.18887981648874397
iteration : 701
train acc:  0.65625
train loss:  0.6121176481246948
train gradient:  0.15637129653923487
iteration : 702
train acc:  0.6796875
train loss:  0.5643957257270813
train gradient:  0.1988032124613811
iteration : 703
train acc:  0.671875
train loss:  0.5968451499938965
train gradient:  0.1526400675564672
iteration : 704
train acc:  0.7265625
train loss:  0.5594159364700317
train gradient:  0.13941958691338063
iteration : 705
train acc:  0.671875
train loss:  0.5697954893112183
train gradient:  0.165322344959772
iteration : 706
train acc:  0.640625
train loss:  0.6266595125198364
train gradient:  0.16060970279682318
iteration : 707
train acc:  0.6875
train loss:  0.5766198635101318
train gradient:  0.18647784401896433
iteration : 708
train acc:  0.671875
train loss:  0.581797182559967
train gradient:  0.16407519449721936
iteration : 709
train acc:  0.703125
train loss:  0.554826557636261
train gradient:  0.1442935669783133
iteration : 710
train acc:  0.71875
train loss:  0.5751841068267822
train gradient:  0.12218320448180561
iteration : 711
train acc:  0.65625
train loss:  0.6173862218856812
train gradient:  0.17311965983796623
iteration : 712
train acc:  0.65625
train loss:  0.5721052885055542
train gradient:  0.1438689042780841
iteration : 713
train acc:  0.6953125
train loss:  0.5656299591064453
train gradient:  0.1411470769626399
iteration : 714
train acc:  0.6328125
train loss:  0.5949980020523071
train gradient:  0.13980854249877675
iteration : 715
train acc:  0.7265625
train loss:  0.5584650635719299
train gradient:  0.15397446728242115
iteration : 716
train acc:  0.7421875
train loss:  0.5452411770820618
train gradient:  0.127077357052054
iteration : 717
train acc:  0.6796875
train loss:  0.5745573043823242
train gradient:  0.3004353680592921
iteration : 718
train acc:  0.71875
train loss:  0.5814270973205566
train gradient:  0.173193337418909
iteration : 719
train acc:  0.671875
train loss:  0.6103880405426025
train gradient:  0.1589427356070567
iteration : 720
train acc:  0.65625
train loss:  0.5764286518096924
train gradient:  0.15711442863032765
iteration : 721
train acc:  0.7109375
train loss:  0.5596661567687988
train gradient:  0.21701646432028193
iteration : 722
train acc:  0.71875
train loss:  0.5180400609970093
train gradient:  0.1443635487782461
iteration : 723
train acc:  0.6875
train loss:  0.5897580981254578
train gradient:  0.1610440810667923
iteration : 724
train acc:  0.65625
train loss:  0.6767536401748657
train gradient:  0.2980004528191181
iteration : 725
train acc:  0.78125
train loss:  0.5083713531494141
train gradient:  0.14399132517753221
iteration : 726
train acc:  0.640625
train loss:  0.6249516010284424
train gradient:  0.20574031471059467
iteration : 727
train acc:  0.6875
train loss:  0.5865601301193237
train gradient:  0.1727131636828676
iteration : 728
train acc:  0.6796875
train loss:  0.5877298712730408
train gradient:  0.16034866020981253
iteration : 729
train acc:  0.6328125
train loss:  0.5797348022460938
train gradient:  0.1653812690352239
iteration : 730
train acc:  0.6953125
train loss:  0.5790085792541504
train gradient:  0.1432615416798177
iteration : 731
train acc:  0.6171875
train loss:  0.6563624143600464
train gradient:  0.355421488046327
iteration : 732
train acc:  0.7421875
train loss:  0.5630303621292114
train gradient:  0.12828820422690984
iteration : 733
train acc:  0.6484375
train loss:  0.6230979561805725
train gradient:  0.24659807562543562
iteration : 734
train acc:  0.6875
train loss:  0.5418096780776978
train gradient:  0.15269815030467077
iteration : 735
train acc:  0.7265625
train loss:  0.5432298183441162
train gradient:  0.18139232666126354
iteration : 736
train acc:  0.7421875
train loss:  0.5149521827697754
train gradient:  0.14897673368611827
iteration : 737
train acc:  0.765625
train loss:  0.5338019132614136
train gradient:  0.1519093028614666
iteration : 738
train acc:  0.71875
train loss:  0.5450526475906372
train gradient:  0.1899683644029289
iteration : 739
train acc:  0.7421875
train loss:  0.5332000255584717
train gradient:  0.1102135618923668
iteration : 740
train acc:  0.6796875
train loss:  0.5823540091514587
train gradient:  0.13527441364242496
iteration : 741
train acc:  0.625
train loss:  0.5929878950119019
train gradient:  0.1741697420960866
iteration : 742
train acc:  0.6484375
train loss:  0.6193795204162598
train gradient:  0.18726323531776862
iteration : 743
train acc:  0.78125
train loss:  0.5341903567314148
train gradient:  0.1596642367572978
iteration : 744
train acc:  0.765625
train loss:  0.489751398563385
train gradient:  0.16278613137362005
iteration : 745
train acc:  0.671875
train loss:  0.5693309903144836
train gradient:  0.17135699665260473
iteration : 746
train acc:  0.7265625
train loss:  0.540306568145752
train gradient:  0.1314934834134793
iteration : 747
train acc:  0.7265625
train loss:  0.5519413948059082
train gradient:  0.16982434005793093
iteration : 748
train acc:  0.6796875
train loss:  0.5805824995040894
train gradient:  0.14276221411244788
iteration : 749
train acc:  0.65625
train loss:  0.6231475472450256
train gradient:  0.20797501734794105
iteration : 750
train acc:  0.6640625
train loss:  0.5941734313964844
train gradient:  0.204021118056766
iteration : 751
train acc:  0.59375
train loss:  0.6492318511009216
train gradient:  0.19526336612514933
iteration : 752
train acc:  0.6875
train loss:  0.5585604310035706
train gradient:  0.2222188423072407
iteration : 753
train acc:  0.6875
train loss:  0.609418511390686
train gradient:  0.1630952450120685
iteration : 754
train acc:  0.7109375
train loss:  0.5840444564819336
train gradient:  0.18710437538052282
iteration : 755
train acc:  0.7265625
train loss:  0.5451126098632812
train gradient:  0.14369230329417232
iteration : 756
train acc:  0.6796875
train loss:  0.5789997577667236
train gradient:  0.1837563005333569
iteration : 757
train acc:  0.7109375
train loss:  0.5384114384651184
train gradient:  0.18543231892725048
iteration : 758
train acc:  0.703125
train loss:  0.5812457203865051
train gradient:  0.2320681065760008
iteration : 759
train acc:  0.6875
train loss:  0.5900804996490479
train gradient:  0.20462941287148612
iteration : 760
train acc:  0.6484375
train loss:  0.6241255402565002
train gradient:  0.29316511375352833
iteration : 761
train acc:  0.671875
train loss:  0.5978508591651917
train gradient:  0.17554896272993994
iteration : 762
train acc:  0.71875
train loss:  0.5536196231842041
train gradient:  0.15202617725307385
iteration : 763
train acc:  0.640625
train loss:  0.6210908889770508
train gradient:  0.20369120335378457
iteration : 764
train acc:  0.71875
train loss:  0.5613964796066284
train gradient:  0.22764849764687722
iteration : 765
train acc:  0.6953125
train loss:  0.5690798163414001
train gradient:  0.19747624295790372
iteration : 766
train acc:  0.625
train loss:  0.6474413275718689
train gradient:  0.19280789319807878
iteration : 767
train acc:  0.671875
train loss:  0.5534654855728149
train gradient:  0.1390912799934037
iteration : 768
train acc:  0.78125
train loss:  0.5349345207214355
train gradient:  0.1386997985902872
iteration : 769
train acc:  0.7109375
train loss:  0.5385788679122925
train gradient:  0.17162968536481124
iteration : 770
train acc:  0.6796875
train loss:  0.5932334661483765
train gradient:  0.27357051920215253
iteration : 771
train acc:  0.6875
train loss:  0.5568357706069946
train gradient:  0.13337779007247907
iteration : 772
train acc:  0.71875
train loss:  0.5460398197174072
train gradient:  0.11883310660371266
iteration : 773
train acc:  0.7265625
train loss:  0.5652927160263062
train gradient:  0.1588625971617706
iteration : 774
train acc:  0.734375
train loss:  0.5402730703353882
train gradient:  0.11963480805083498
iteration : 775
train acc:  0.609375
train loss:  0.6024664640426636
train gradient:  0.24949158462136495
iteration : 776
train acc:  0.71875
train loss:  0.5271612405776978
train gradient:  0.1630062282873745
iteration : 777
train acc:  0.6328125
train loss:  0.6000354886054993
train gradient:  0.16404501810173266
iteration : 778
train acc:  0.7265625
train loss:  0.5527117252349854
train gradient:  0.1217382102382128
iteration : 779
train acc:  0.6875
train loss:  0.5853205919265747
train gradient:  0.1439238741731342
iteration : 780
train acc:  0.6953125
train loss:  0.5500306487083435
train gradient:  0.14851337045875462
iteration : 781
train acc:  0.6640625
train loss:  0.5918824672698975
train gradient:  0.19589917568571721
iteration : 782
train acc:  0.6484375
train loss:  0.6320427656173706
train gradient:  0.18742075148187998
iteration : 783
train acc:  0.609375
train loss:  0.6435641050338745
train gradient:  0.23002994272638677
iteration : 784
train acc:  0.703125
train loss:  0.541427493095398
train gradient:  0.18165489506002697
iteration : 785
train acc:  0.6484375
train loss:  0.6116493344306946
train gradient:  0.26082390506918396
iteration : 786
train acc:  0.6796875
train loss:  0.6495053768157959
train gradient:  0.17179405012138388
iteration : 787
train acc:  0.625
train loss:  0.6269962787628174
train gradient:  0.27898072028346854
iteration : 788
train acc:  0.6796875
train loss:  0.6269043684005737
train gradient:  0.2204257406128141
iteration : 789
train acc:  0.6796875
train loss:  0.6199384927749634
train gradient:  0.20625987910821894
iteration : 790
train acc:  0.6484375
train loss:  0.6069414615631104
train gradient:  0.17589465307457663
iteration : 791
train acc:  0.703125
train loss:  0.546333372592926
train gradient:  0.14479476592652965
iteration : 792
train acc:  0.6953125
train loss:  0.5445693135261536
train gradient:  0.13824119299767346
iteration : 793
train acc:  0.7109375
train loss:  0.6181281208992004
train gradient:  0.1821423904400117
iteration : 794
train acc:  0.65625
train loss:  0.5873736143112183
train gradient:  0.17744379895671908
iteration : 795
train acc:  0.71875
train loss:  0.5106329917907715
train gradient:  0.13564577755863408
iteration : 796
train acc:  0.6171875
train loss:  0.6592347621917725
train gradient:  0.22468112294953685
iteration : 797
train acc:  0.7265625
train loss:  0.5289014577865601
train gradient:  0.15109933556520466
iteration : 798
train acc:  0.640625
train loss:  0.624173641204834
train gradient:  0.16322969947326
iteration : 799
train acc:  0.6953125
train loss:  0.6061536073684692
train gradient:  0.15841858650897755
iteration : 800
train acc:  0.6015625
train loss:  0.6391692757606506
train gradient:  0.2092543584940233
iteration : 801
train acc:  0.6328125
train loss:  0.5986580848693848
train gradient:  0.18753043888699222
iteration : 802
train acc:  0.7421875
train loss:  0.5022658109664917
train gradient:  0.12340982227274917
iteration : 803
train acc:  0.6640625
train loss:  0.6246458888053894
train gradient:  0.2032349554573139
iteration : 804
train acc:  0.7265625
train loss:  0.5300683975219727
train gradient:  0.21296499202302255
iteration : 805
train acc:  0.6953125
train loss:  0.5683154463768005
train gradient:  0.2299035623311883
iteration : 806
train acc:  0.65625
train loss:  0.6327382922172546
train gradient:  0.17722154905236537
iteration : 807
train acc:  0.7109375
train loss:  0.5653537511825562
train gradient:  0.1421885937812744
iteration : 808
train acc:  0.6484375
train loss:  0.5487633943557739
train gradient:  0.14919995215167484
iteration : 809
train acc:  0.65625
train loss:  0.5813412666320801
train gradient:  0.21232437725570225
iteration : 810
train acc:  0.640625
train loss:  0.558394730091095
train gradient:  0.1521044362750763
iteration : 811
train acc:  0.6328125
train loss:  0.6250091791152954
train gradient:  0.24031512029477678
iteration : 812
train acc:  0.6953125
train loss:  0.6037440299987793
train gradient:  0.1731845399628489
iteration : 813
train acc:  0.6875
train loss:  0.5845212340354919
train gradient:  0.14946169881619192
iteration : 814
train acc:  0.7265625
train loss:  0.5605636239051819
train gradient:  0.14440589203751744
iteration : 815
train acc:  0.6796875
train loss:  0.6022104620933533
train gradient:  0.1541581834588387
iteration : 816
train acc:  0.703125
train loss:  0.5742888450622559
train gradient:  0.19721465704974245
iteration : 817
train acc:  0.6875
train loss:  0.5639417171478271
train gradient:  0.13559532905587146
iteration : 818
train acc:  0.609375
train loss:  0.5884491801261902
train gradient:  0.22383088352052055
iteration : 819
train acc:  0.703125
train loss:  0.5731942653656006
train gradient:  0.14352348125773834
iteration : 820
train acc:  0.671875
train loss:  0.6208237409591675
train gradient:  0.1982289667058909
iteration : 821
train acc:  0.65625
train loss:  0.6044801473617554
train gradient:  0.18923935820059387
iteration : 822
train acc:  0.75
train loss:  0.5515447854995728
train gradient:  0.1433927177879139
iteration : 823
train acc:  0.7109375
train loss:  0.5711667537689209
train gradient:  0.13709775385835943
iteration : 824
train acc:  0.7265625
train loss:  0.5493138432502747
train gradient:  0.2001892514087953
iteration : 825
train acc:  0.5859375
train loss:  0.6240317821502686
train gradient:  0.17970870927904006
iteration : 826
train acc:  0.7265625
train loss:  0.5593011975288391
train gradient:  0.1476036029380933
iteration : 827
train acc:  0.7578125
train loss:  0.5378193855285645
train gradient:  0.2436098292540732
iteration : 828
train acc:  0.5859375
train loss:  0.6443396806716919
train gradient:  0.1944359527951676
iteration : 829
train acc:  0.609375
train loss:  0.6635065078735352
train gradient:  0.26978637552288404
iteration : 830
train acc:  0.6640625
train loss:  0.5826057195663452
train gradient:  0.17041382358681167
iteration : 831
train acc:  0.59375
train loss:  0.6880739331245422
train gradient:  0.2029198640725631
iteration : 832
train acc:  0.6875
train loss:  0.5467672348022461
train gradient:  0.2059585113182144
iteration : 833
train acc:  0.6953125
train loss:  0.5435565710067749
train gradient:  0.16002396209119105
iteration : 834
train acc:  0.6953125
train loss:  0.613297700881958
train gradient:  0.18292877404037516
iteration : 835
train acc:  0.625
train loss:  0.5823151469230652
train gradient:  0.12519124666841386
iteration : 836
train acc:  0.703125
train loss:  0.5735956430435181
train gradient:  0.13253744688730956
iteration : 837
train acc:  0.6875
train loss:  0.55686354637146
train gradient:  0.1767060933458589
iteration : 838
train acc:  0.65625
train loss:  0.6136668920516968
train gradient:  0.23949412252062527
iteration : 839
train acc:  0.65625
train loss:  0.5876659750938416
train gradient:  0.15131298713121427
iteration : 840
train acc:  0.65625
train loss:  0.5601123571395874
train gradient:  0.18462454031432643
iteration : 841
train acc:  0.703125
train loss:  0.5604468584060669
train gradient:  0.17117720477826182
iteration : 842
train acc:  0.6171875
train loss:  0.6606911420822144
train gradient:  0.24263415146249245
iteration : 843
train acc:  0.6953125
train loss:  0.623395562171936
train gradient:  0.13994112236887657
iteration : 844
train acc:  0.7265625
train loss:  0.5672249794006348
train gradient:  0.19677312684193396
iteration : 845
train acc:  0.6640625
train loss:  0.5564512014389038
train gradient:  0.12241340085622757
iteration : 846
train acc:  0.6875
train loss:  0.5685228109359741
train gradient:  0.15295301693954916
iteration : 847
train acc:  0.65625
train loss:  0.5852361917495728
train gradient:  0.16686855534055378
iteration : 848
train acc:  0.671875
train loss:  0.5757946968078613
train gradient:  0.17273952288519745
iteration : 849
train acc:  0.75
train loss:  0.531560480594635
train gradient:  0.11496922677223115
iteration : 850
train acc:  0.7734375
train loss:  0.5627783536911011
train gradient:  0.19697342108178895
iteration : 851
train acc:  0.75
train loss:  0.5215736627578735
train gradient:  0.13945806398905608
iteration : 852
train acc:  0.71875
train loss:  0.5473408699035645
train gradient:  0.20611780306528188
iteration : 853
train acc:  0.6328125
train loss:  0.5870239734649658
train gradient:  0.14564359769903615
iteration : 854
train acc:  0.734375
train loss:  0.543350100517273
train gradient:  0.20934555963217938
iteration : 855
train acc:  0.7109375
train loss:  0.568793773651123
train gradient:  0.1429302239574381
iteration : 856
train acc:  0.65625
train loss:  0.6050806045532227
train gradient:  0.1677633924263738
iteration : 857
train acc:  0.6953125
train loss:  0.5758647918701172
train gradient:  0.1628711261956925
iteration : 858
train acc:  0.671875
train loss:  0.568821907043457
train gradient:  0.1393715439577955
iteration : 859
train acc:  0.6328125
train loss:  0.6260498762130737
train gradient:  0.14173438026900861
iteration : 860
train acc:  0.6328125
train loss:  0.5817840099334717
train gradient:  0.17674380800277548
iteration : 861
train acc:  0.6953125
train loss:  0.5802425146102905
train gradient:  0.18220199746014176
iteration : 862
train acc:  0.7109375
train loss:  0.5482184886932373
train gradient:  0.20481304983803533
iteration : 863
train acc:  0.703125
train loss:  0.56929612159729
train gradient:  0.12936181259807772
iteration : 864
train acc:  0.671875
train loss:  0.5859041213989258
train gradient:  0.1981784759999618
iteration : 865
train acc:  0.6640625
train loss:  0.5904155969619751
train gradient:  0.20335273931454398
iteration : 866
train acc:  0.703125
train loss:  0.5779657959938049
train gradient:  0.1805315933878591
iteration : 867
train acc:  0.7265625
train loss:  0.5725890398025513
train gradient:  0.13429195425870388
iteration : 868
train acc:  0.640625
train loss:  0.6134829521179199
train gradient:  0.1433802322834539
iteration : 869
train acc:  0.6875
train loss:  0.6075201034545898
train gradient:  0.2096471292351636
iteration : 870
train acc:  0.6796875
train loss:  0.6247866153717041
train gradient:  0.18713131064326566
iteration : 871
train acc:  0.671875
train loss:  0.5949440002441406
train gradient:  0.16313543474180198
iteration : 872
train acc:  0.7421875
train loss:  0.5443075299263
train gradient:  0.14744153327147275
iteration : 873
train acc:  0.734375
train loss:  0.552122175693512
train gradient:  0.17269774797255305
iteration : 874
train acc:  0.71875
train loss:  0.5403964519500732
train gradient:  0.13634868090776886
iteration : 875
train acc:  0.703125
train loss:  0.5692567229270935
train gradient:  0.1457124978021323
iteration : 876
train acc:  0.640625
train loss:  0.6079152822494507
train gradient:  0.19904819915028793
iteration : 877
train acc:  0.734375
train loss:  0.5191460847854614
train gradient:  0.11477491598406818
iteration : 878
train acc:  0.6171875
train loss:  0.6458568572998047
train gradient:  0.16245377633606362
iteration : 879
train acc:  0.734375
train loss:  0.5121931433677673
train gradient:  0.1677148750915088
iteration : 880
train acc:  0.6796875
train loss:  0.5759645104408264
train gradient:  0.15584287310361486
iteration : 881
train acc:  0.6484375
train loss:  0.5741378664970398
train gradient:  0.15090813529227856
iteration : 882
train acc:  0.640625
train loss:  0.6101408004760742
train gradient:  0.15749184652031534
iteration : 883
train acc:  0.6875
train loss:  0.5770965814590454
train gradient:  0.27887925512861983
iteration : 884
train acc:  0.703125
train loss:  0.5237770676612854
train gradient:  0.1547723040128532
iteration : 885
train acc:  0.6640625
train loss:  0.59178227186203
train gradient:  0.15503003997588247
iteration : 886
train acc:  0.625
train loss:  0.6420079469680786
train gradient:  0.20160545656189965
iteration : 887
train acc:  0.71875
train loss:  0.61744624376297
train gradient:  0.1903853573385465
iteration : 888
train acc:  0.609375
train loss:  0.6206164360046387
train gradient:  0.21384279354044933
iteration : 889
train acc:  0.671875
train loss:  0.5832705497741699
train gradient:  0.17331161504808956
iteration : 890
train acc:  0.6875
train loss:  0.5994962453842163
train gradient:  0.21227539643309307
iteration : 891
train acc:  0.703125
train loss:  0.5593501329421997
train gradient:  0.13341712198289515
iteration : 892
train acc:  0.71875
train loss:  0.5587185621261597
train gradient:  0.19119749168823733
iteration : 893
train acc:  0.6640625
train loss:  0.6158859729766846
train gradient:  0.16752667661919904
iteration : 894
train acc:  0.7421875
train loss:  0.543491542339325
train gradient:  0.14249290821405386
iteration : 895
train acc:  0.65625
train loss:  0.5950804948806763
train gradient:  0.17258453920726097
iteration : 896
train acc:  0.7265625
train loss:  0.5479472875595093
train gradient:  0.1523479270088406
iteration : 897
train acc:  0.703125
train loss:  0.5847949981689453
train gradient:  0.1534113752688454
iteration : 898
train acc:  0.640625
train loss:  0.5643903017044067
train gradient:  0.18732902357568976
iteration : 899
train acc:  0.609375
train loss:  0.6209737062454224
train gradient:  0.20726408628060342
iteration : 900
train acc:  0.703125
train loss:  0.5687707662582397
train gradient:  0.12924090483694606
iteration : 901
train acc:  0.6875
train loss:  0.6461763381958008
train gradient:  0.24204742101634236
iteration : 902
train acc:  0.6953125
train loss:  0.5608818531036377
train gradient:  0.18789555738168023
iteration : 903
train acc:  0.6953125
train loss:  0.5766825079917908
train gradient:  0.140343450341448
iteration : 904
train acc:  0.75
train loss:  0.5160158276557922
train gradient:  0.143960150754055
iteration : 905
train acc:  0.71875
train loss:  0.6072328090667725
train gradient:  0.2595737408304288
iteration : 906
train acc:  0.6796875
train loss:  0.5795351266860962
train gradient:  0.21513631830897156
iteration : 907
train acc:  0.671875
train loss:  0.5808089971542358
train gradient:  0.12167138902810773
iteration : 908
train acc:  0.65625
train loss:  0.5965210199356079
train gradient:  0.19545875774637178
iteration : 909
train acc:  0.71875
train loss:  0.5406829118728638
train gradient:  0.14748741976435564
iteration : 910
train acc:  0.6796875
train loss:  0.5616196393966675
train gradient:  0.16408766521235146
iteration : 911
train acc:  0.609375
train loss:  0.6048426628112793
train gradient:  0.14889395736110908
iteration : 912
train acc:  0.734375
train loss:  0.5436554551124573
train gradient:  0.17039076586010488
iteration : 913
train acc:  0.6953125
train loss:  0.5310471057891846
train gradient:  0.18697754714148168
iteration : 914
train acc:  0.7421875
train loss:  0.5233015418052673
train gradient:  0.1632888127757947
iteration : 915
train acc:  0.6953125
train loss:  0.5737712383270264
train gradient:  0.2853607976611624
iteration : 916
train acc:  0.75
train loss:  0.5238502025604248
train gradient:  0.18137221258032235
iteration : 917
train acc:  0.7109375
train loss:  0.5508875846862793
train gradient:  0.16432222969391527
iteration : 918
train acc:  0.65625
train loss:  0.62608802318573
train gradient:  0.1941024295642848
iteration : 919
train acc:  0.6953125
train loss:  0.5402930974960327
train gradient:  0.1680221531968601
iteration : 920
train acc:  0.6640625
train loss:  0.6003803014755249
train gradient:  0.1688769099693152
iteration : 921
train acc:  0.6796875
train loss:  0.6121991276741028
train gradient:  0.15694683198739906
iteration : 922
train acc:  0.6796875
train loss:  0.6429654955863953
train gradient:  0.16951881922298265
iteration : 923
train acc:  0.65625
train loss:  0.6091902256011963
train gradient:  0.18054118509578235
iteration : 924
train acc:  0.671875
train loss:  0.569495677947998
train gradient:  0.17798507294727206
iteration : 925
train acc:  0.7265625
train loss:  0.5342096090316772
train gradient:  0.13787583613262522
iteration : 926
train acc:  0.65625
train loss:  0.5956317782402039
train gradient:  0.13440484652470344
iteration : 927
train acc:  0.625
train loss:  0.6193650364875793
train gradient:  0.20911558205225672
iteration : 928
train acc:  0.6796875
train loss:  0.5685387849807739
train gradient:  0.14914087595904046
iteration : 929
train acc:  0.6640625
train loss:  0.5401705503463745
train gradient:  0.16577702305041453
iteration : 930
train acc:  0.7109375
train loss:  0.5451334714889526
train gradient:  0.12978790850978328
iteration : 931
train acc:  0.7265625
train loss:  0.5504012107849121
train gradient:  0.1394251104619663
iteration : 932
train acc:  0.7421875
train loss:  0.5341556072235107
train gradient:  0.19242689893724663
iteration : 933
train acc:  0.6328125
train loss:  0.6471953392028809
train gradient:  0.21537112847704218
iteration : 934
train acc:  0.6796875
train loss:  0.5824200510978699
train gradient:  0.16781462639848665
iteration : 935
train acc:  0.703125
train loss:  0.515571653842926
train gradient:  0.13969408770078215
iteration : 936
train acc:  0.6953125
train loss:  0.5344994068145752
train gradient:  0.20208607874127535
iteration : 937
train acc:  0.71875
train loss:  0.5502762794494629
train gradient:  0.15874438250120215
iteration : 938
train acc:  0.7421875
train loss:  0.5283656120300293
train gradient:  0.1124455040976606
iteration : 939
train acc:  0.703125
train loss:  0.5703238248825073
train gradient:  0.1440284515394793
iteration : 940
train acc:  0.7109375
train loss:  0.5671975612640381
train gradient:  0.15239020482245963
iteration : 941
train acc:  0.6640625
train loss:  0.5796808004379272
train gradient:  0.1976292765070729
iteration : 942
train acc:  0.6640625
train loss:  0.6476079225540161
train gradient:  0.2099522547380644
iteration : 943
train acc:  0.7734375
train loss:  0.5102845430374146
train gradient:  0.13076999803132405
iteration : 944
train acc:  0.65625
train loss:  0.5834718346595764
train gradient:  0.17222618428027245
iteration : 945
train acc:  0.71875
train loss:  0.5879995822906494
train gradient:  0.1817181793097441
iteration : 946
train acc:  0.671875
train loss:  0.5665050148963928
train gradient:  0.1393760090515762
iteration : 947
train acc:  0.7265625
train loss:  0.547885537147522
train gradient:  0.1760508621128739
iteration : 948
train acc:  0.640625
train loss:  0.5944246053695679
train gradient:  0.18555339128297202
iteration : 949
train acc:  0.6171875
train loss:  0.6008627414703369
train gradient:  0.17460440896229396
iteration : 950
train acc:  0.6796875
train loss:  0.5835453271865845
train gradient:  0.17827719903461584
iteration : 951
train acc:  0.65625
train loss:  0.5806081295013428
train gradient:  0.22914236828598183
iteration : 952
train acc:  0.6640625
train loss:  0.5601823925971985
train gradient:  0.15445800721519365
iteration : 953
train acc:  0.671875
train loss:  0.5981885194778442
train gradient:  0.13920168927058324
iteration : 954
train acc:  0.6953125
train loss:  0.5429760813713074
train gradient:  0.17358663845858677
iteration : 955
train acc:  0.703125
train loss:  0.5569421648979187
train gradient:  0.1553131423687778
iteration : 956
train acc:  0.640625
train loss:  0.6294595003128052
train gradient:  0.20497818111836952
iteration : 957
train acc:  0.65625
train loss:  0.6188677549362183
train gradient:  0.17602761465521516
iteration : 958
train acc:  0.6796875
train loss:  0.6001180410385132
train gradient:  0.23323614434551693
iteration : 959
train acc:  0.671875
train loss:  0.625703752040863
train gradient:  0.1664356975177778
iteration : 960
train acc:  0.671875
train loss:  0.5809460878372192
train gradient:  0.1361721020494685
iteration : 961
train acc:  0.640625
train loss:  0.6229427456855774
train gradient:  0.14656158223635063
iteration : 962
train acc:  0.71875
train loss:  0.5673526525497437
train gradient:  0.14795138779788475
iteration : 963
train acc:  0.7109375
train loss:  0.5523391366004944
train gradient:  0.17766576617027863
iteration : 964
train acc:  0.7578125
train loss:  0.5520412921905518
train gradient:  0.19148845666811282
iteration : 965
train acc:  0.671875
train loss:  0.588884711265564
train gradient:  0.16918381724103623
iteration : 966
train acc:  0.640625
train loss:  0.6243533492088318
train gradient:  0.21684367197259016
iteration : 967
train acc:  0.6796875
train loss:  0.5594441294670105
train gradient:  0.17737444088432652
iteration : 968
train acc:  0.65625
train loss:  0.5970315933227539
train gradient:  0.1875759113324283
iteration : 969
train acc:  0.640625
train loss:  0.63032066822052
train gradient:  0.2112987520224463
iteration : 970
train acc:  0.625
train loss:  0.6042963266372681
train gradient:  0.17427392669093852
iteration : 971
train acc:  0.78125
train loss:  0.5441656112670898
train gradient:  0.16738082334637916
iteration : 972
train acc:  0.671875
train loss:  0.5704675912857056
train gradient:  0.20935515823421091
iteration : 973
train acc:  0.703125
train loss:  0.5691312551498413
train gradient:  0.17693159332700684
iteration : 974
train acc:  0.6953125
train loss:  0.5819187164306641
train gradient:  0.2056648621994716
iteration : 975
train acc:  0.640625
train loss:  0.5685005187988281
train gradient:  0.14390404581279478
iteration : 976
train acc:  0.6953125
train loss:  0.5543041229248047
train gradient:  0.17058020099356816
iteration : 977
train acc:  0.71875
train loss:  0.5695441961288452
train gradient:  0.1525322154767363
iteration : 978
train acc:  0.6484375
train loss:  0.5556466579437256
train gradient:  0.21665037357258904
iteration : 979
train acc:  0.6484375
train loss:  0.5811767578125
train gradient:  0.19679956031723275
iteration : 980
train acc:  0.703125
train loss:  0.5518270134925842
train gradient:  0.23586410197702154
iteration : 981
train acc:  0.671875
train loss:  0.592726469039917
train gradient:  0.14519211936687243
iteration : 982
train acc:  0.71875
train loss:  0.5221248865127563
train gradient:  0.14186522542636448
iteration : 983
train acc:  0.6796875
train loss:  0.561633288860321
train gradient:  0.19748947357477048
iteration : 984
train acc:  0.6953125
train loss:  0.5343613624572754
train gradient:  0.12789037434535644
iteration : 985
train acc:  0.6875
train loss:  0.5685980319976807
train gradient:  0.10621647129370361
iteration : 986
train acc:  0.6796875
train loss:  0.5344078540802002
train gradient:  0.11251313935651275
iteration : 987
train acc:  0.6796875
train loss:  0.6331897974014282
train gradient:  0.17403954817452383
iteration : 988
train acc:  0.6796875
train loss:  0.5711142420768738
train gradient:  0.1560327538455804
iteration : 989
train acc:  0.7421875
train loss:  0.5434262752532959
train gradient:  0.11756890644073628
iteration : 990
train acc:  0.671875
train loss:  0.5886476039886475
train gradient:  0.1637345015158574
iteration : 991
train acc:  0.71875
train loss:  0.5645449161529541
train gradient:  0.15676824596330144
iteration : 992
train acc:  0.78125
train loss:  0.5094499588012695
train gradient:  0.14430786915246574
iteration : 993
train acc:  0.59375
train loss:  0.610209584236145
train gradient:  0.19949132354875318
iteration : 994
train acc:  0.7265625
train loss:  0.554114818572998
train gradient:  0.19663015087858976
iteration : 995
train acc:  0.6484375
train loss:  0.6175636053085327
train gradient:  0.22375708265600358
iteration : 996
train acc:  0.6640625
train loss:  0.5935950875282288
train gradient:  0.15783937060449463
iteration : 997
train acc:  0.6484375
train loss:  0.5636829137802124
train gradient:  0.12870665010779858
iteration : 998
train acc:  0.671875
train loss:  0.5997028350830078
train gradient:  0.1733646962542334
iteration : 999
train acc:  0.6796875
train loss:  0.5904192924499512
train gradient:  0.1829779838676162
iteration : 1000
train acc:  0.671875
train loss:  0.6026228666305542
train gradient:  0.18024835280630336
iteration : 1001
train acc:  0.7109375
train loss:  0.5517501831054688
train gradient:  0.18412453086877398
iteration : 1002
train acc:  0.734375
train loss:  0.5118905305862427
train gradient:  0.19938131112527502
iteration : 1003
train acc:  0.6484375
train loss:  0.6183342337608337
train gradient:  0.2589297412787914
iteration : 1004
train acc:  0.7109375
train loss:  0.5466017127037048
train gradient:  0.2008724527671049
iteration : 1005
train acc:  0.7109375
train loss:  0.5755770802497864
train gradient:  0.13906536702066785
iteration : 1006
train acc:  0.6796875
train loss:  0.5693463087081909
train gradient:  0.13630841717041056
iteration : 1007
train acc:  0.75
train loss:  0.5336451530456543
train gradient:  0.1392039926147934
iteration : 1008
train acc:  0.7421875
train loss:  0.5215864181518555
train gradient:  0.16172604446087707
iteration : 1009
train acc:  0.7109375
train loss:  0.590739905834198
train gradient:  0.20650378128546099
iteration : 1010
train acc:  0.7890625
train loss:  0.4568040370941162
train gradient:  0.1157560371624765
iteration : 1011
train acc:  0.71875
train loss:  0.5388194918632507
train gradient:  0.1313470845685391
iteration : 1012
train acc:  0.71875
train loss:  0.5475343465805054
train gradient:  0.18096336665078105
iteration : 1013
train acc:  0.7265625
train loss:  0.5550444722175598
train gradient:  0.12455964713863395
iteration : 1014
train acc:  0.671875
train loss:  0.587882936000824
train gradient:  0.20952408125443334
iteration : 1015
train acc:  0.6875
train loss:  0.5952858924865723
train gradient:  0.17212382362063106
iteration : 1016
train acc:  0.671875
train loss:  0.5969390273094177
train gradient:  0.16609982755440758
iteration : 1017
train acc:  0.7421875
train loss:  0.5470153093338013
train gradient:  0.19510605288318356
iteration : 1018
train acc:  0.65625
train loss:  0.5869134664535522
train gradient:  0.13869698881638076
iteration : 1019
train acc:  0.6015625
train loss:  0.6797153949737549
train gradient:  0.23826922270876305
iteration : 1020
train acc:  0.6171875
train loss:  0.6175380349159241
train gradient:  0.18653124196546433
iteration : 1021
train acc:  0.7421875
train loss:  0.565666913986206
train gradient:  0.14671098089293166
iteration : 1022
train acc:  0.765625
train loss:  0.5339705348014832
train gradient:  0.15266829246664604
iteration : 1023
train acc:  0.6875
train loss:  0.6093429327011108
train gradient:  0.20848699747323537
iteration : 1024
train acc:  0.7109375
train loss:  0.5767490863800049
train gradient:  0.15319746131468975
iteration : 1025
train acc:  0.6953125
train loss:  0.5766141414642334
train gradient:  0.13662738412676106
iteration : 1026
train acc:  0.6328125
train loss:  0.6115195155143738
train gradient:  0.18248044413785083
iteration : 1027
train acc:  0.7734375
train loss:  0.5199658274650574
train gradient:  0.11536796760485415
iteration : 1028
train acc:  0.6640625
train loss:  0.5706268548965454
train gradient:  0.2505834047371043
iteration : 1029
train acc:  0.7578125
train loss:  0.4888613224029541
train gradient:  0.19797073333853216
iteration : 1030
train acc:  0.7265625
train loss:  0.5265496373176575
train gradient:  0.1512695561755501
iteration : 1031
train acc:  0.6640625
train loss:  0.5869104862213135
train gradient:  0.16409402863887695
iteration : 1032
train acc:  0.6875
train loss:  0.6031633615493774
train gradient:  0.17336374176069164
iteration : 1033
train acc:  0.7109375
train loss:  0.5666790008544922
train gradient:  0.16435220603649192
iteration : 1034
train acc:  0.5859375
train loss:  0.6560467481613159
train gradient:  0.1890746503395898
iteration : 1035
train acc:  0.7578125
train loss:  0.5095400810241699
train gradient:  0.13701385042493408
iteration : 1036
train acc:  0.6640625
train loss:  0.5814480781555176
train gradient:  0.17230302524023522
iteration : 1037
train acc:  0.625
train loss:  0.6121799349784851
train gradient:  0.14848966203977132
iteration : 1038
train acc:  0.703125
train loss:  0.566298246383667
train gradient:  0.14747743864977642
iteration : 1039
train acc:  0.6640625
train loss:  0.6114279627799988
train gradient:  0.1965241441168764
iteration : 1040
train acc:  0.65625
train loss:  0.5860230922698975
train gradient:  0.15519766610234428
iteration : 1041
train acc:  0.6796875
train loss:  0.5661200881004333
train gradient:  0.15915913306553792
iteration : 1042
train acc:  0.7421875
train loss:  0.538896918296814
train gradient:  0.14020142720043627
iteration : 1043
train acc:  0.7109375
train loss:  0.5706851482391357
train gradient:  0.1272587891909033
iteration : 1044
train acc:  0.6484375
train loss:  0.6690775752067566
train gradient:  0.260965401736043
iteration : 1045
train acc:  0.71875
train loss:  0.5376148223876953
train gradient:  0.1592991448128182
iteration : 1046
train acc:  0.625
train loss:  0.6399540305137634
train gradient:  0.20175847105029757
iteration : 1047
train acc:  0.703125
train loss:  0.563430666923523
train gradient:  0.1849940250547819
iteration : 1048
train acc:  0.703125
train loss:  0.5762758255004883
train gradient:  0.1990389137613221
iteration : 1049
train acc:  0.65625
train loss:  0.5721192955970764
train gradient:  0.1769371632509028
iteration : 1050
train acc:  0.625
train loss:  0.5704026222229004
train gradient:  0.1981913272207897
iteration : 1051
train acc:  0.734375
train loss:  0.5798635482788086
train gradient:  0.23608586424722652
iteration : 1052
train acc:  0.734375
train loss:  0.5335707664489746
train gradient:  0.17309954566125751
iteration : 1053
train acc:  0.6953125
train loss:  0.5233837366104126
train gradient:  0.14283201495340545
iteration : 1054
train acc:  0.765625
train loss:  0.5345287322998047
train gradient:  0.14211380919545527
iteration : 1055
train acc:  0.6796875
train loss:  0.5968039631843567
train gradient:  0.1627458537275846
iteration : 1056
train acc:  0.6328125
train loss:  0.5852646827697754
train gradient:  0.22745342858036272
iteration : 1057
train acc:  0.7265625
train loss:  0.5330719351768494
train gradient:  0.11867908932090084
iteration : 1058
train acc:  0.6953125
train loss:  0.570368230342865
train gradient:  0.16434221061856746
iteration : 1059
train acc:  0.71875
train loss:  0.5411351919174194
train gradient:  0.12557649105069812
iteration : 1060
train acc:  0.7734375
train loss:  0.47728055715560913
train gradient:  0.12278088402764295
iteration : 1061
train acc:  0.65625
train loss:  0.5884683728218079
train gradient:  0.15672921370104131
iteration : 1062
train acc:  0.6796875
train loss:  0.5771135091781616
train gradient:  0.16367290535551293
iteration : 1063
train acc:  0.6875
train loss:  0.5421379208564758
train gradient:  0.12407733513838899
iteration : 1064
train acc:  0.7109375
train loss:  0.5257331728935242
train gradient:  0.24420130945141233
iteration : 1065
train acc:  0.6875
train loss:  0.58028244972229
train gradient:  0.21999240408594867
iteration : 1066
train acc:  0.671875
train loss:  0.5987215042114258
train gradient:  0.24864029800260834
iteration : 1067
train acc:  0.6875
train loss:  0.6004939079284668
train gradient:  0.1961882873066742
iteration : 1068
train acc:  0.75
train loss:  0.5443654656410217
train gradient:  0.12125183014250518
iteration : 1069
train acc:  0.6953125
train loss:  0.5475841760635376
train gradient:  0.1682345904261522
iteration : 1070
train acc:  0.6640625
train loss:  0.5525073409080505
train gradient:  0.17150718405484028
iteration : 1071
train acc:  0.734375
train loss:  0.5139170289039612
train gradient:  0.1328061257465441
iteration : 1072
train acc:  0.7421875
train loss:  0.5313924551010132
train gradient:  0.1414879434042892
iteration : 1073
train acc:  0.734375
train loss:  0.5275516510009766
train gradient:  0.1498094490219287
iteration : 1074
train acc:  0.6796875
train loss:  0.5729074478149414
train gradient:  0.24114322123592158
iteration : 1075
train acc:  0.734375
train loss:  0.5363556146621704
train gradient:  0.13917072826140187
iteration : 1076
train acc:  0.6875
train loss:  0.5556342601776123
train gradient:  0.15282362962510335
iteration : 1077
train acc:  0.6328125
train loss:  0.6003733277320862
train gradient:  0.21221880322002995
iteration : 1078
train acc:  0.6875
train loss:  0.5639499425888062
train gradient:  0.13537684672937117
iteration : 1079
train acc:  0.640625
train loss:  0.5764977931976318
train gradient:  0.1412199488911377
iteration : 1080
train acc:  0.7265625
train loss:  0.5685309171676636
train gradient:  0.1869440133542905
iteration : 1081
train acc:  0.6015625
train loss:  0.5951825380325317
train gradient:  0.18492784726922323
iteration : 1082
train acc:  0.6953125
train loss:  0.576157808303833
train gradient:  0.20577653209250893
iteration : 1083
train acc:  0.703125
train loss:  0.5632827281951904
train gradient:  0.27899118087913016
iteration : 1084
train acc:  0.71875
train loss:  0.5254623293876648
train gradient:  0.1264942312981006
iteration : 1085
train acc:  0.6640625
train loss:  0.5544226765632629
train gradient:  0.16966168658332842
iteration : 1086
train acc:  0.6953125
train loss:  0.5298056602478027
train gradient:  0.1437801789469461
iteration : 1087
train acc:  0.75
train loss:  0.5458449125289917
train gradient:  0.14121567893142944
iteration : 1088
train acc:  0.7109375
train loss:  0.538357138633728
train gradient:  0.25564904043438086
iteration : 1089
train acc:  0.671875
train loss:  0.5739895105361938
train gradient:  0.12647459277690065
iteration : 1090
train acc:  0.7421875
train loss:  0.5325894355773926
train gradient:  0.26323929085487696
iteration : 1091
train acc:  0.7265625
train loss:  0.5455825328826904
train gradient:  0.1490695118010919
iteration : 1092
train acc:  0.6484375
train loss:  0.608975887298584
train gradient:  0.19731218742913903
iteration : 1093
train acc:  0.7421875
train loss:  0.5456215739250183
train gradient:  0.17935320602014856
iteration : 1094
train acc:  0.71875
train loss:  0.5246345400810242
train gradient:  0.12001781342614767
iteration : 1095
train acc:  0.703125
train loss:  0.511131227016449
train gradient:  0.11567785752721871
iteration : 1096
train acc:  0.671875
train loss:  0.5400853157043457
train gradient:  0.159128082839343
iteration : 1097
train acc:  0.6875
train loss:  0.58488929271698
train gradient:  0.197483405198022
iteration : 1098
train acc:  0.8046875
train loss:  0.47402915358543396
train gradient:  0.1440482724275612
iteration : 1099
train acc:  0.6953125
train loss:  0.5567549467086792
train gradient:  0.18187657515643707
iteration : 1100
train acc:  0.6796875
train loss:  0.5439542531967163
train gradient:  0.16061960166012318
iteration : 1101
train acc:  0.734375
train loss:  0.541400671005249
train gradient:  0.1273733224028561
iteration : 1102
train acc:  0.6953125
train loss:  0.5708682537078857
train gradient:  0.13892578020421154
iteration : 1103
train acc:  0.671875
train loss:  0.6303916573524475
train gradient:  0.29113258482394316
iteration : 1104
train acc:  0.734375
train loss:  0.5541558265686035
train gradient:  0.19814690630354426
iteration : 1105
train acc:  0.7109375
train loss:  0.5260322690010071
train gradient:  0.14569699421902207
iteration : 1106
train acc:  0.6875
train loss:  0.5555492639541626
train gradient:  0.15654155664957878
iteration : 1107
train acc:  0.75
train loss:  0.507107138633728
train gradient:  0.14850804200822687
iteration : 1108
train acc:  0.65625
train loss:  0.5840206146240234
train gradient:  0.1729261204511945
iteration : 1109
train acc:  0.6640625
train loss:  0.6113452911376953
train gradient:  0.25690192637598336
iteration : 1110
train acc:  0.734375
train loss:  0.5286308526992798
train gradient:  0.14699996025211565
iteration : 1111
train acc:  0.6875
train loss:  0.5309479236602783
train gradient:  0.14910320430740448
iteration : 1112
train acc:  0.6953125
train loss:  0.6045912504196167
train gradient:  0.2654761211316833
iteration : 1113
train acc:  0.6875
train loss:  0.5471688508987427
train gradient:  0.19101127768369824
iteration : 1114
train acc:  0.6875
train loss:  0.5863276124000549
train gradient:  0.1416641659028562
iteration : 1115
train acc:  0.6875
train loss:  0.5673255324363708
train gradient:  0.18568901960834316
iteration : 1116
train acc:  0.6171875
train loss:  0.6354101896286011
train gradient:  0.17031621541759848
iteration : 1117
train acc:  0.765625
train loss:  0.5251022577285767
train gradient:  0.12768477908019177
iteration : 1118
train acc:  0.7578125
train loss:  0.505213737487793
train gradient:  0.1618904621859899
iteration : 1119
train acc:  0.6796875
train loss:  0.6006819009780884
train gradient:  0.1867214719232001
iteration : 1120
train acc:  0.734375
train loss:  0.5191806554794312
train gradient:  0.18274214285213825
iteration : 1121
train acc:  0.6796875
train loss:  0.5761677026748657
train gradient:  0.2217985086394354
iteration : 1122
train acc:  0.71875
train loss:  0.5328459739685059
train gradient:  0.17989785594913596
iteration : 1123
train acc:  0.6796875
train loss:  0.5794578790664673
train gradient:  0.15640712348287752
iteration : 1124
train acc:  0.6875
train loss:  0.5330605506896973
train gradient:  0.2140054664426017
iteration : 1125
train acc:  0.7109375
train loss:  0.558381199836731
train gradient:  0.20747608413699223
iteration : 1126
train acc:  0.703125
train loss:  0.5672895312309265
train gradient:  0.163160840755102
iteration : 1127
train acc:  0.7109375
train loss:  0.5440452694892883
train gradient:  0.19063242682540266
iteration : 1128
train acc:  0.6875
train loss:  0.5605401992797852
train gradient:  0.15084657883979893
iteration : 1129
train acc:  0.671875
train loss:  0.5861743092536926
train gradient:  0.1354633483698457
iteration : 1130
train acc:  0.71875
train loss:  0.5466017127037048
train gradient:  0.1835941884311094
iteration : 1131
train acc:  0.765625
train loss:  0.4996885657310486
train gradient:  0.12606825448783246
iteration : 1132
train acc:  0.71875
train loss:  0.5406095385551453
train gradient:  0.18838892616775868
iteration : 1133
train acc:  0.6953125
train loss:  0.548121452331543
train gradient:  0.1513029622756709
iteration : 1134
train acc:  0.6953125
train loss:  0.5546855926513672
train gradient:  0.1380039052711665
iteration : 1135
train acc:  0.6796875
train loss:  0.6181670427322388
train gradient:  0.16559708259783
iteration : 1136
train acc:  0.71875
train loss:  0.5485450625419617
train gradient:  0.15634979624205558
iteration : 1137
train acc:  0.7265625
train loss:  0.5318042039871216
train gradient:  0.1422383070420069
iteration : 1138
train acc:  0.671875
train loss:  0.5634628534317017
train gradient:  0.16561045420582754
iteration : 1139
train acc:  0.7265625
train loss:  0.5283340215682983
train gradient:  0.14917017862546705
iteration : 1140
train acc:  0.7109375
train loss:  0.5539162158966064
train gradient:  0.166010788039345
iteration : 1141
train acc:  0.640625
train loss:  0.6088970899581909
train gradient:  0.17510650625829566
iteration : 1142
train acc:  0.6484375
train loss:  0.5962597131729126
train gradient:  0.21410782329607558
iteration : 1143
train acc:  0.6796875
train loss:  0.593708336353302
train gradient:  0.15003446751786953
iteration : 1144
train acc:  0.6953125
train loss:  0.5611802339553833
train gradient:  0.1508401932464271
iteration : 1145
train acc:  0.6796875
train loss:  0.6231116056442261
train gradient:  0.22388835813271873
iteration : 1146
train acc:  0.734375
train loss:  0.518601655960083
train gradient:  0.15305907897976034
iteration : 1147
train acc:  0.6953125
train loss:  0.5366694927215576
train gradient:  0.1661802861163848
iteration : 1148
train acc:  0.65625
train loss:  0.6137136816978455
train gradient:  0.250397461670933
iteration : 1149
train acc:  0.609375
train loss:  0.6313647031784058
train gradient:  0.17890747006568708
iteration : 1150
train acc:  0.671875
train loss:  0.5620407462120056
train gradient:  0.1443025153114489
iteration : 1151
train acc:  0.7265625
train loss:  0.47493934631347656
train gradient:  0.11301822200381728
iteration : 1152
train acc:  0.65625
train loss:  0.6288352012634277
train gradient:  0.21878989193941306
iteration : 1153
train acc:  0.7421875
train loss:  0.49394339323043823
train gradient:  0.12720237304089954
iteration : 1154
train acc:  0.734375
train loss:  0.5514182448387146
train gradient:  0.22728644072583665
iteration : 1155
train acc:  0.640625
train loss:  0.609272301197052
train gradient:  0.21558726950369267
iteration : 1156
train acc:  0.640625
train loss:  0.5874643325805664
train gradient:  0.24314204974258474
iteration : 1157
train acc:  0.703125
train loss:  0.5823954343795776
train gradient:  0.18821664595298337
iteration : 1158
train acc:  0.6875
train loss:  0.513268768787384
train gradient:  0.14304382235217022
iteration : 1159
train acc:  0.7109375
train loss:  0.523983895778656
train gradient:  0.12311305452594598
iteration : 1160
train acc:  0.6484375
train loss:  0.5662256479263306
train gradient:  0.1929099994388715
iteration : 1161
train acc:  0.7265625
train loss:  0.5145959854125977
train gradient:  0.1398804568637724
iteration : 1162
train acc:  0.6875
train loss:  0.5761672258377075
train gradient:  0.2004831274902587
iteration : 1163
train acc:  0.6171875
train loss:  0.6465115547180176
train gradient:  0.19203714544445077
iteration : 1164
train acc:  0.7265625
train loss:  0.5879452228546143
train gradient:  0.15490861237068715
iteration : 1165
train acc:  0.734375
train loss:  0.5059380531311035
train gradient:  0.17098039439969656
iteration : 1166
train acc:  0.6484375
train loss:  0.5808019638061523
train gradient:  0.15984939212148036
iteration : 1167
train acc:  0.6640625
train loss:  0.5851958990097046
train gradient:  0.17425384239013664
iteration : 1168
train acc:  0.75
train loss:  0.526659369468689
train gradient:  0.1650254948371288
iteration : 1169
train acc:  0.6640625
train loss:  0.5627750158309937
train gradient:  0.20293142268527878
iteration : 1170
train acc:  0.640625
train loss:  0.5909013748168945
train gradient:  0.13625705642057695
iteration : 1171
train acc:  0.7109375
train loss:  0.5743138194084167
train gradient:  0.2085016255260474
iteration : 1172
train acc:  0.7421875
train loss:  0.5402594208717346
train gradient:  0.14554715677300145
iteration : 1173
train acc:  0.734375
train loss:  0.5580878853797913
train gradient:  0.24676304086344725
iteration : 1174
train acc:  0.625
train loss:  0.6000673770904541
train gradient:  0.20742911508273498
iteration : 1175
train acc:  0.6953125
train loss:  0.5734291076660156
train gradient:  0.1488450523686488
iteration : 1176
train acc:  0.6875
train loss:  0.5575625896453857
train gradient:  0.16448278655229806
iteration : 1177
train acc:  0.71875
train loss:  0.5336856842041016
train gradient:  0.1672232646487657
iteration : 1178
train acc:  0.65625
train loss:  0.5345228910446167
train gradient:  0.12302748592469472
iteration : 1179
train acc:  0.6640625
train loss:  0.577278733253479
train gradient:  0.16104111897057855
iteration : 1180
train acc:  0.7109375
train loss:  0.5803201794624329
train gradient:  0.16644380033757958
iteration : 1181
train acc:  0.6640625
train loss:  0.6011466383934021
train gradient:  0.1924463306889225
iteration : 1182
train acc:  0.71875
train loss:  0.5384396314620972
train gradient:  0.17815600533141498
iteration : 1183
train acc:  0.7109375
train loss:  0.5917590260505676
train gradient:  0.18770140696099283
iteration : 1184
train acc:  0.734375
train loss:  0.5698264837265015
train gradient:  0.2286837395289048
iteration : 1185
train acc:  0.65625
train loss:  0.6124584674835205
train gradient:  0.26275490906854987
iteration : 1186
train acc:  0.6875
train loss:  0.5853512287139893
train gradient:  0.14362017868201488
iteration : 1187
train acc:  0.734375
train loss:  0.5261697769165039
train gradient:  0.13818708442416988
iteration : 1188
train acc:  0.6171875
train loss:  0.6586200594902039
train gradient:  0.2850337418626572
iteration : 1189
train acc:  0.6875
train loss:  0.5719619393348694
train gradient:  0.15164507137373603
iteration : 1190
train acc:  0.6953125
train loss:  0.5798541307449341
train gradient:  0.1683418060642317
iteration : 1191
train acc:  0.65625
train loss:  0.5863439440727234
train gradient:  0.20226362324764707
iteration : 1192
train acc:  0.6875
train loss:  0.5721533298492432
train gradient:  0.18152546922404716
iteration : 1193
train acc:  0.6328125
train loss:  0.5574377775192261
train gradient:  0.13439059366714978
iteration : 1194
train acc:  0.7890625
train loss:  0.4831821024417877
train gradient:  0.14513534692851865
iteration : 1195
train acc:  0.765625
train loss:  0.4766436219215393
train gradient:  0.156575498154631
iteration : 1196
train acc:  0.6875
train loss:  0.5731956362724304
train gradient:  0.22272616065440698
iteration : 1197
train acc:  0.765625
train loss:  0.5612507462501526
train gradient:  0.21716658889458346
iteration : 1198
train acc:  0.6640625
train loss:  0.585852861404419
train gradient:  0.1319433833443816
iteration : 1199
train acc:  0.78125
train loss:  0.5065121650695801
train gradient:  0.14099682593706803
iteration : 1200
train acc:  0.640625
train loss:  0.6033058166503906
train gradient:  0.1597479878355636
iteration : 1201
train acc:  0.71875
train loss:  0.577958881855011
train gradient:  0.18472037992164622
iteration : 1202
train acc:  0.6796875
train loss:  0.5423522591590881
train gradient:  0.1299738986346553
iteration : 1203
train acc:  0.640625
train loss:  0.601069450378418
train gradient:  0.21341336193887647
iteration : 1204
train acc:  0.6640625
train loss:  0.5717281103134155
train gradient:  0.16779831306093826
iteration : 1205
train acc:  0.7734375
train loss:  0.5172784328460693
train gradient:  0.13766871384931573
iteration : 1206
train acc:  0.6796875
train loss:  0.5707465410232544
train gradient:  0.1829050259115379
iteration : 1207
train acc:  0.78125
train loss:  0.48608750104904175
train gradient:  0.16627392397316965
iteration : 1208
train acc:  0.671875
train loss:  0.5693837404251099
train gradient:  0.14922507482849354
iteration : 1209
train acc:  0.6484375
train loss:  0.5976632833480835
train gradient:  0.1557643717062026
iteration : 1210
train acc:  0.6484375
train loss:  0.6182242035865784
train gradient:  0.2164807323052106
iteration : 1211
train acc:  0.59375
train loss:  0.7200809121131897
train gradient:  0.31665336795636484
iteration : 1212
train acc:  0.7265625
train loss:  0.5141534209251404
train gradient:  0.19026017088215408
iteration : 1213
train acc:  0.7265625
train loss:  0.576055109500885
train gradient:  0.17561092671706738
iteration : 1214
train acc:  0.6796875
train loss:  0.6003144979476929
train gradient:  0.14626046516558366
iteration : 1215
train acc:  0.6953125
train loss:  0.5969351530075073
train gradient:  0.1774595817406197
iteration : 1216
train acc:  0.734375
train loss:  0.5273182392120361
train gradient:  0.16749257720766741
iteration : 1217
train acc:  0.7109375
train loss:  0.5713673830032349
train gradient:  0.15937608064126702
iteration : 1218
train acc:  0.6484375
train loss:  0.5917419195175171
train gradient:  0.15202388785529153
iteration : 1219
train acc:  0.6796875
train loss:  0.5691385269165039
train gradient:  0.15290349886583116
iteration : 1220
train acc:  0.71875
train loss:  0.5296963453292847
train gradient:  0.1206940133891542
iteration : 1221
train acc:  0.71875
train loss:  0.5305928587913513
train gradient:  0.13055513813078073
iteration : 1222
train acc:  0.625
train loss:  0.6084085702896118
train gradient:  0.17176254922302575
iteration : 1223
train acc:  0.65625
train loss:  0.5542793273925781
train gradient:  0.14200377084362015
iteration : 1224
train acc:  0.6796875
train loss:  0.6034834980964661
train gradient:  0.15686615705079304
iteration : 1225
train acc:  0.6875
train loss:  0.5822051763534546
train gradient:  0.16748052378733058
iteration : 1226
train acc:  0.640625
train loss:  0.669150173664093
train gradient:  0.2888934489419356
iteration : 1227
train acc:  0.65625
train loss:  0.5715610980987549
train gradient:  0.1340066296555897
iteration : 1228
train acc:  0.640625
train loss:  0.6044579148292542
train gradient:  0.2233423271778379
iteration : 1229
train acc:  0.6875
train loss:  0.6081218719482422
train gradient:  0.19905008572352778
iteration : 1230
train acc:  0.7109375
train loss:  0.5457993745803833
train gradient:  0.13339005342980786
iteration : 1231
train acc:  0.6796875
train loss:  0.5792636871337891
train gradient:  0.1849938656840469
iteration : 1232
train acc:  0.6875
train loss:  0.5590490102767944
train gradient:  0.251882781416827
iteration : 1233
train acc:  0.6640625
train loss:  0.5419265031814575
train gradient:  0.14263100920951383
iteration : 1234
train acc:  0.7578125
train loss:  0.5426079034805298
train gradient:  0.1311534181307856
iteration : 1235
train acc:  0.7265625
train loss:  0.4996446669101715
train gradient:  0.13499674159721328
iteration : 1236
train acc:  0.65625
train loss:  0.5965045690536499
train gradient:  0.1812046950566204
iteration : 1237
train acc:  0.65625
train loss:  0.5852608680725098
train gradient:  0.1494212510111524
iteration : 1238
train acc:  0.703125
train loss:  0.529897928237915
train gradient:  0.1295257080172164
iteration : 1239
train acc:  0.6796875
train loss:  0.5843843221664429
train gradient:  0.19352574817612286
iteration : 1240
train acc:  0.671875
train loss:  0.5581286549568176
train gradient:  0.1754458266834235
iteration : 1241
train acc:  0.6953125
train loss:  0.5773059725761414
train gradient:  0.1940271710675004
iteration : 1242
train acc:  0.6484375
train loss:  0.5625555515289307
train gradient:  0.14224973189728818
iteration : 1243
train acc:  0.6953125
train loss:  0.5403066873550415
train gradient:  0.10895730078193754
iteration : 1244
train acc:  0.6640625
train loss:  0.6138396263122559
train gradient:  0.2527990237975009
iteration : 1245
train acc:  0.703125
train loss:  0.5544312000274658
train gradient:  0.14590609381575523
iteration : 1246
train acc:  0.6640625
train loss:  0.5941559076309204
train gradient:  0.23527416885345406
iteration : 1247
train acc:  0.6875
train loss:  0.5701519250869751
train gradient:  0.16566076565183352
iteration : 1248
train acc:  0.65625
train loss:  0.57012939453125
train gradient:  0.16443547855882373
iteration : 1249
train acc:  0.71875
train loss:  0.5584924817085266
train gradient:  0.1463043315658995
iteration : 1250
train acc:  0.640625
train loss:  0.6277123093605042
train gradient:  0.1956340380924864
iteration : 1251
train acc:  0.6953125
train loss:  0.5690041780471802
train gradient:  0.21061821594326902
iteration : 1252
train acc:  0.6796875
train loss:  0.5609298348426819
train gradient:  0.1490673368420865
iteration : 1253
train acc:  0.671875
train loss:  0.5397703647613525
train gradient:  0.1209476561537531
iteration : 1254
train acc:  0.71875
train loss:  0.5670268535614014
train gradient:  0.16858225663661086
iteration : 1255
train acc:  0.6328125
train loss:  0.5753307342529297
train gradient:  0.16908349483046298
iteration : 1256
train acc:  0.6640625
train loss:  0.574649453163147
train gradient:  0.1818714788651416
iteration : 1257
train acc:  0.7109375
train loss:  0.6149088144302368
train gradient:  0.15260963491294247
iteration : 1258
train acc:  0.703125
train loss:  0.5575762987136841
train gradient:  0.12081232586806884
iteration : 1259
train acc:  0.71875
train loss:  0.5527870059013367
train gradient:  0.14138640598789828
iteration : 1260
train acc:  0.6328125
train loss:  0.6302530765533447
train gradient:  0.2043827482454172
iteration : 1261
train acc:  0.7109375
train loss:  0.5628647804260254
train gradient:  0.19009332181883626
iteration : 1262
train acc:  0.734375
train loss:  0.5066795945167542
train gradient:  0.11115216852637723
iteration : 1263
train acc:  0.6015625
train loss:  0.6534792184829712
train gradient:  0.2616148295119871
iteration : 1264
train acc:  0.703125
train loss:  0.5821670293807983
train gradient:  0.15679761697137254
iteration : 1265
train acc:  0.6328125
train loss:  0.5978834629058838
train gradient:  0.18714875567993844
iteration : 1266
train acc:  0.6953125
train loss:  0.5744057893753052
train gradient:  0.15472701091673924
iteration : 1267
train acc:  0.6953125
train loss:  0.5388153195381165
train gradient:  0.12157425917983554
iteration : 1268
train acc:  0.703125
train loss:  0.5384031534194946
train gradient:  0.1338945970246148
iteration : 1269
train acc:  0.6875
train loss:  0.563218355178833
train gradient:  0.1376775635306957
iteration : 1270
train acc:  0.671875
train loss:  0.5639505386352539
train gradient:  0.16631742365594077
iteration : 1271
train acc:  0.6875
train loss:  0.5808478593826294
train gradient:  0.14844446709036307
iteration : 1272
train acc:  0.671875
train loss:  0.5874627828598022
train gradient:  0.1617338426133379
iteration : 1273
train acc:  0.7578125
train loss:  0.49079978466033936
train gradient:  0.22422968937436455
iteration : 1274
train acc:  0.71875
train loss:  0.5456138849258423
train gradient:  0.1694314739771865
iteration : 1275
train acc:  0.7734375
train loss:  0.4778597950935364
train gradient:  0.18517692356611012
iteration : 1276
train acc:  0.671875
train loss:  0.5689976215362549
train gradient:  0.15660675421277487
iteration : 1277
train acc:  0.6796875
train loss:  0.5869947671890259
train gradient:  0.20016574826547603
iteration : 1278
train acc:  0.703125
train loss:  0.5562531352043152
train gradient:  0.12090061578000465
iteration : 1279
train acc:  0.7578125
train loss:  0.5224857926368713
train gradient:  0.25092300682656465
iteration : 1280
train acc:  0.7421875
train loss:  0.557906985282898
train gradient:  0.14012866355663853
iteration : 1281
train acc:  0.7109375
train loss:  0.5583729147911072
train gradient:  0.1314950587958551
iteration : 1282
train acc:  0.734375
train loss:  0.5413330793380737
train gradient:  0.13944486109211118
iteration : 1283
train acc:  0.703125
train loss:  0.552756130695343
train gradient:  0.14523409329164377
iteration : 1284
train acc:  0.65625
train loss:  0.5940247178077698
train gradient:  0.17831729685452577
iteration : 1285
train acc:  0.6796875
train loss:  0.584867000579834
train gradient:  0.16642176921903984
iteration : 1286
train acc:  0.65625
train loss:  0.6026847958564758
train gradient:  0.14733515063894137
iteration : 1287
train acc:  0.6796875
train loss:  0.5697597861289978
train gradient:  0.18638772201179185
iteration : 1288
train acc:  0.71875
train loss:  0.5286396145820618
train gradient:  0.1546356739035577
iteration : 1289
train acc:  0.7109375
train loss:  0.498253732919693
train gradient:  0.13357930566810872
iteration : 1290
train acc:  0.6953125
train loss:  0.57011878490448
train gradient:  0.15056104538711096
iteration : 1291
train acc:  0.6953125
train loss:  0.5746700167655945
train gradient:  0.24333126021400028
iteration : 1292
train acc:  0.7578125
train loss:  0.5229296088218689
train gradient:  0.14232784887180625
iteration : 1293
train acc:  0.640625
train loss:  0.6018883585929871
train gradient:  0.18403078572348047
iteration : 1294
train acc:  0.7265625
train loss:  0.5266882181167603
train gradient:  0.18114064827120063
iteration : 1295
train acc:  0.6484375
train loss:  0.542206883430481
train gradient:  0.13794535299284252
iteration : 1296
train acc:  0.734375
train loss:  0.5142651796340942
train gradient:  0.21972509827481657
iteration : 1297
train acc:  0.7578125
train loss:  0.49406883120536804
train gradient:  0.12580895904534944
iteration : 1298
train acc:  0.703125
train loss:  0.5620704889297485
train gradient:  0.19667415891978465
iteration : 1299
train acc:  0.6875
train loss:  0.570838212966919
train gradient:  0.14683312726085065
iteration : 1300
train acc:  0.6875
train loss:  0.5729358792304993
train gradient:  0.16635212103150004
iteration : 1301
train acc:  0.6953125
train loss:  0.5983676910400391
train gradient:  0.19766162142193017
iteration : 1302
train acc:  0.765625
train loss:  0.548766016960144
train gradient:  0.18903776521496352
iteration : 1303
train acc:  0.7421875
train loss:  0.49979764223098755
train gradient:  0.11080445905138744
iteration : 1304
train acc:  0.71875
train loss:  0.5520297884941101
train gradient:  0.15020565920392098
iteration : 1305
train acc:  0.6640625
train loss:  0.5915738940238953
train gradient:  0.1615172610289401
iteration : 1306
train acc:  0.7109375
train loss:  0.5447297096252441
train gradient:  0.19847147404897741
iteration : 1307
train acc:  0.65625
train loss:  0.5652133226394653
train gradient:  0.14361424749760265
iteration : 1308
train acc:  0.703125
train loss:  0.5440610647201538
train gradient:  0.1833412355689727
iteration : 1309
train acc:  0.734375
train loss:  0.5374106168746948
train gradient:  0.20309286443820757
iteration : 1310
train acc:  0.7109375
train loss:  0.5660237669944763
train gradient:  0.15729183066487437
iteration : 1311
train acc:  0.703125
train loss:  0.57969069480896
train gradient:  0.1966174142546978
iteration : 1312
train acc:  0.6796875
train loss:  0.5798184871673584
train gradient:  0.19236378834196477
iteration : 1313
train acc:  0.703125
train loss:  0.5822380185127258
train gradient:  0.1949899798542196
iteration : 1314
train acc:  0.6875
train loss:  0.588790237903595
train gradient:  0.19795556312443785
iteration : 1315
train acc:  0.671875
train loss:  0.5968433022499084
train gradient:  0.18719309262272443
iteration : 1316
train acc:  0.65625
train loss:  0.6387304067611694
train gradient:  0.2206772657303204
iteration : 1317
train acc:  0.7734375
train loss:  0.5672652125358582
train gradient:  0.14160755467329886
iteration : 1318
train acc:  0.6796875
train loss:  0.5483336448669434
train gradient:  0.1402711721365134
iteration : 1319
train acc:  0.6171875
train loss:  0.6148769855499268
train gradient:  0.21139047952717338
iteration : 1320
train acc:  0.6484375
train loss:  0.5899933576583862
train gradient:  0.1981872178206565
iteration : 1321
train acc:  0.6953125
train loss:  0.5456907749176025
train gradient:  0.14815092532877405
iteration : 1322
train acc:  0.75
train loss:  0.5556775331497192
train gradient:  0.18874611418165976
iteration : 1323
train acc:  0.671875
train loss:  0.5909987688064575
train gradient:  0.16385002799003243
iteration : 1324
train acc:  0.7265625
train loss:  0.5473120808601379
train gradient:  0.14109167668637532
iteration : 1325
train acc:  0.6796875
train loss:  0.5413469076156616
train gradient:  0.1282340605148683
iteration : 1326
train acc:  0.65625
train loss:  0.5911093950271606
train gradient:  0.18385937637568006
iteration : 1327
train acc:  0.671875
train loss:  0.5763477683067322
train gradient:  0.14962373821143066
iteration : 1328
train acc:  0.703125
train loss:  0.567533016204834
train gradient:  0.14647577212258284
iteration : 1329
train acc:  0.625
train loss:  0.5844929218292236
train gradient:  0.1959810982845079
iteration : 1330
train acc:  0.703125
train loss:  0.564711332321167
train gradient:  0.1337823850960932
iteration : 1331
train acc:  0.71875
train loss:  0.5555084943771362
train gradient:  0.15855875291127403
iteration : 1332
train acc:  0.7421875
train loss:  0.4800630807876587
train gradient:  0.13443075637012059
iteration : 1333
train acc:  0.6015625
train loss:  0.5833057761192322
train gradient:  0.19922580974734383
iteration : 1334
train acc:  0.6484375
train loss:  0.6052975654602051
train gradient:  0.1750629788838677
iteration : 1335
train acc:  0.7578125
train loss:  0.5284020900726318
train gradient:  0.12623239866103914
iteration : 1336
train acc:  0.671875
train loss:  0.5562149286270142
train gradient:  0.19015353428958348
iteration : 1337
train acc:  0.7421875
train loss:  0.5151881575584412
train gradient:  0.19294233492405344
iteration : 1338
train acc:  0.65625
train loss:  0.6432837247848511
train gradient:  0.1791728670067665
iteration : 1339
train acc:  0.65625
train loss:  0.6249682903289795
train gradient:  0.19103363120924982
iteration : 1340
train acc:  0.78125
train loss:  0.5213516354560852
train gradient:  0.11811930438871639
iteration : 1341
train acc:  0.6953125
train loss:  0.5584115982055664
train gradient:  0.17701516403889817
iteration : 1342
train acc:  0.671875
train loss:  0.5864713788032532
train gradient:  0.19670440186945426
iteration : 1343
train acc:  0.7109375
train loss:  0.5420366525650024
train gradient:  0.14682940875771727
iteration : 1344
train acc:  0.6953125
train loss:  0.5681778192520142
train gradient:  0.2721123365651287
iteration : 1345
train acc:  0.7734375
train loss:  0.5046318173408508
train gradient:  0.12847584720766442
iteration : 1346
train acc:  0.7109375
train loss:  0.5860271453857422
train gradient:  0.22807310052491941
iteration : 1347
train acc:  0.734375
train loss:  0.4871023893356323
train gradient:  0.14774799508194658
iteration : 1348
train acc:  0.6484375
train loss:  0.6133707165718079
train gradient:  0.1950560772275999
iteration : 1349
train acc:  0.6953125
train loss:  0.550456166267395
train gradient:  0.19591248613305784
iteration : 1350
train acc:  0.640625
train loss:  0.5880019664764404
train gradient:  0.1912532981885779
iteration : 1351
train acc:  0.7109375
train loss:  0.5822012424468994
train gradient:  0.1986724854428037
iteration : 1352
train acc:  0.671875
train loss:  0.5950911045074463
train gradient:  0.17422006590110622
iteration : 1353
train acc:  0.7421875
train loss:  0.5593894720077515
train gradient:  0.18909949121548375
iteration : 1354
train acc:  0.734375
train loss:  0.5177856683731079
train gradient:  0.17063395830036637
iteration : 1355
train acc:  0.6640625
train loss:  0.5475170612335205
train gradient:  0.20903278750238263
iteration : 1356
train acc:  0.6640625
train loss:  0.5693793296813965
train gradient:  0.1468177262086099
iteration : 1357
train acc:  0.765625
train loss:  0.5003480911254883
train gradient:  0.26599812294342773
iteration : 1358
train acc:  0.734375
train loss:  0.5104330778121948
train gradient:  0.17651487048652736
iteration : 1359
train acc:  0.734375
train loss:  0.5607530474662781
train gradient:  0.26126750611470595
iteration : 1360
train acc:  0.7265625
train loss:  0.5104426145553589
train gradient:  0.1769912455621201
iteration : 1361
train acc:  0.65625
train loss:  0.6301792860031128
train gradient:  0.32175096236804746
iteration : 1362
train acc:  0.6796875
train loss:  0.5485464930534363
train gradient:  0.12280519529456643
iteration : 1363
train acc:  0.734375
train loss:  0.5637108683586121
train gradient:  0.16614519257601845
iteration : 1364
train acc:  0.71875
train loss:  0.5467191934585571
train gradient:  0.14995098239103594
iteration : 1365
train acc:  0.7109375
train loss:  0.5189094543457031
train gradient:  0.1258693363905243
iteration : 1366
train acc:  0.671875
train loss:  0.5764279961585999
train gradient:  0.15521154915676225
iteration : 1367
train acc:  0.7265625
train loss:  0.5491070747375488
train gradient:  0.1927183774712719
iteration : 1368
train acc:  0.6484375
train loss:  0.567691445350647
train gradient:  0.1518808377403801
iteration : 1369
train acc:  0.6796875
train loss:  0.640781581401825
train gradient:  0.3932490110368256
iteration : 1370
train acc:  0.6875
train loss:  0.5203040838241577
train gradient:  0.1818294658018637
iteration : 1371
train acc:  0.7265625
train loss:  0.5392991900444031
train gradient:  0.15568068172133936
iteration : 1372
train acc:  0.65625
train loss:  0.6244332194328308
train gradient:  0.19239856456417626
iteration : 1373
train acc:  0.625
train loss:  0.634997546672821
train gradient:  0.2130577287227984
iteration : 1374
train acc:  0.7265625
train loss:  0.5279556512832642
train gradient:  0.16136878112934971
iteration : 1375
train acc:  0.640625
train loss:  0.6000913977622986
train gradient:  0.22209815297349128
iteration : 1376
train acc:  0.734375
train loss:  0.5144251585006714
train gradient:  0.12037549317666246
iteration : 1377
train acc:  0.71875
train loss:  0.5576883554458618
train gradient:  0.1539221162122774
iteration : 1378
train acc:  0.65625
train loss:  0.6187468767166138
train gradient:  0.15730276646136018
iteration : 1379
train acc:  0.7109375
train loss:  0.5400241017341614
train gradient:  0.15100911948129497
iteration : 1380
train acc:  0.7578125
train loss:  0.5427279472351074
train gradient:  0.12660462322302557
iteration : 1381
train acc:  0.78125
train loss:  0.507211446762085
train gradient:  0.14899474094933612
iteration : 1382
train acc:  0.7109375
train loss:  0.5437753200531006
train gradient:  0.18550597402006552
iteration : 1383
train acc:  0.734375
train loss:  0.5200076699256897
train gradient:  0.16482981759248894
iteration : 1384
train acc:  0.671875
train loss:  0.5550323128700256
train gradient:  0.16929687551661846
iteration : 1385
train acc:  0.703125
train loss:  0.5607783198356628
train gradient:  0.18885141570860048
iteration : 1386
train acc:  0.6796875
train loss:  0.5611247420310974
train gradient:  0.17645430395226439
iteration : 1387
train acc:  0.7890625
train loss:  0.49705132842063904
train gradient:  0.1607838037478733
iteration : 1388
train acc:  0.625
train loss:  0.5848884582519531
train gradient:  0.15081043852822162
iteration : 1389
train acc:  0.671875
train loss:  0.5646672248840332
train gradient:  0.20113734914491246
iteration : 1390
train acc:  0.6640625
train loss:  0.5553691387176514
train gradient:  0.14646091879876494
iteration : 1391
train acc:  0.6484375
train loss:  0.5740859508514404
train gradient:  0.16953572561013416
iteration : 1392
train acc:  0.6796875
train loss:  0.5498838424682617
train gradient:  0.20328946313550783
iteration : 1393
train acc:  0.703125
train loss:  0.5193332433700562
train gradient:  0.10771380363318579
iteration : 1394
train acc:  0.6328125
train loss:  0.5988079905509949
train gradient:  0.22214898586137063
iteration : 1395
train acc:  0.734375
train loss:  0.5493687391281128
train gradient:  0.1536640903037298
iteration : 1396
train acc:  0.7421875
train loss:  0.524768054485321
train gradient:  0.1377113269490457
iteration : 1397
train acc:  0.7421875
train loss:  0.5114990472793579
train gradient:  0.17758332613627398
iteration : 1398
train acc:  0.6640625
train loss:  0.5768054723739624
train gradient:  0.1345832805341296
iteration : 1399
train acc:  0.671875
train loss:  0.5950294733047485
train gradient:  0.15481844061925784
iteration : 1400
train acc:  0.6953125
train loss:  0.5994807481765747
train gradient:  0.22702545457800588
iteration : 1401
train acc:  0.6796875
train loss:  0.5156148672103882
train gradient:  0.1453512923997971
iteration : 1402
train acc:  0.625
train loss:  0.6366258859634399
train gradient:  0.17434078584886875
iteration : 1403
train acc:  0.7109375
train loss:  0.6043044328689575
train gradient:  0.1468638846405892
iteration : 1404
train acc:  0.7265625
train loss:  0.5240523815155029
train gradient:  0.16295877678210294
iteration : 1405
train acc:  0.7265625
train loss:  0.5031071305274963
train gradient:  0.25395887950119694
iteration : 1406
train acc:  0.7578125
train loss:  0.49541184306144714
train gradient:  0.18234319594923146
iteration : 1407
train acc:  0.703125
train loss:  0.5494006872177124
train gradient:  0.19634686065660015
iteration : 1408
train acc:  0.703125
train loss:  0.5976872444152832
train gradient:  0.24249554833584308
iteration : 1409
train acc:  0.65625
train loss:  0.6157479286193848
train gradient:  0.3492117158455273
iteration : 1410
train acc:  0.71875
train loss:  0.5679298639297485
train gradient:  0.16087647956797085
iteration : 1411
train acc:  0.640625
train loss:  0.6122715473175049
train gradient:  0.2055420147699356
iteration : 1412
train acc:  0.7421875
train loss:  0.5699460506439209
train gradient:  0.15141313229562212
iteration : 1413
train acc:  0.671875
train loss:  0.5686939358711243
train gradient:  0.1533630877668037
iteration : 1414
train acc:  0.6328125
train loss:  0.6351844072341919
train gradient:  0.18938902187011392
iteration : 1415
train acc:  0.6953125
train loss:  0.559508740901947
train gradient:  0.15373287370635952
iteration : 1416
train acc:  0.78125
train loss:  0.5158980488777161
train gradient:  0.15612591908148013
iteration : 1417
train acc:  0.7421875
train loss:  0.4746377468109131
train gradient:  0.13353833527302272
iteration : 1418
train acc:  0.71875
train loss:  0.5512988567352295
train gradient:  0.2083670054429863
iteration : 1419
train acc:  0.6875
train loss:  0.5243932008743286
train gradient:  0.14345804780420307
iteration : 1420
train acc:  0.703125
train loss:  0.5798648595809937
train gradient:  0.17054197749549732
iteration : 1421
train acc:  0.7109375
train loss:  0.51506108045578
train gradient:  0.15146209143088687
iteration : 1422
train acc:  0.65625
train loss:  0.5802633762359619
train gradient:  0.1413927572521717
iteration : 1423
train acc:  0.703125
train loss:  0.5440530180931091
train gradient:  0.16993506885673768
iteration : 1424
train acc:  0.6796875
train loss:  0.562676191329956
train gradient:  0.189541275886447
iteration : 1425
train acc:  0.6875
train loss:  0.5884872674942017
train gradient:  0.17662139832657237
iteration : 1426
train acc:  0.6484375
train loss:  0.5976207852363586
train gradient:  0.17346636848862898
iteration : 1427
train acc:  0.765625
train loss:  0.5102393627166748
train gradient:  0.1485975396432777
iteration : 1428
train acc:  0.6796875
train loss:  0.5581738352775574
train gradient:  0.15045357842496654
iteration : 1429
train acc:  0.6484375
train loss:  0.5700284242630005
train gradient:  0.16405181742970154
iteration : 1430
train acc:  0.671875
train loss:  0.5707345008850098
train gradient:  0.2436707546633966
iteration : 1431
train acc:  0.796875
train loss:  0.4717514216899872
train gradient:  0.1645465282851838
iteration : 1432
train acc:  0.6640625
train loss:  0.5499222874641418
train gradient:  0.22108615988232905
iteration : 1433
train acc:  0.6640625
train loss:  0.5731991529464722
train gradient:  0.16289709468912456
iteration : 1434
train acc:  0.734375
train loss:  0.5127761960029602
train gradient:  0.1258170763200256
iteration : 1435
train acc:  0.6640625
train loss:  0.6105371713638306
train gradient:  0.2082861878458056
iteration : 1436
train acc:  0.6875
train loss:  0.5555303692817688
train gradient:  0.13313230436441847
iteration : 1437
train acc:  0.71875
train loss:  0.5557436943054199
train gradient:  0.16691123667380992
iteration : 1438
train acc:  0.609375
train loss:  0.6938517093658447
train gradient:  0.22037626108438882
iteration : 1439
train acc:  0.71875
train loss:  0.5105348825454712
train gradient:  0.15086375559172058
iteration : 1440
train acc:  0.7109375
train loss:  0.6297231912612915
train gradient:  0.19523244725116112
iteration : 1441
train acc:  0.7109375
train loss:  0.5409625768661499
train gradient:  0.13692126390243758
iteration : 1442
train acc:  0.734375
train loss:  0.5571497678756714
train gradient:  0.14193187119835815
iteration : 1443
train acc:  0.6796875
train loss:  0.5925772786140442
train gradient:  0.21695759798374153
iteration : 1444
train acc:  0.6953125
train loss:  0.544140100479126
train gradient:  0.18076352429014073
iteration : 1445
train acc:  0.7109375
train loss:  0.537408709526062
train gradient:  0.15262830893493395
iteration : 1446
train acc:  0.6484375
train loss:  0.5978519916534424
train gradient:  0.20146003794610295
iteration : 1447
train acc:  0.7734375
train loss:  0.519498348236084
train gradient:  0.12702083652045856
iteration : 1448
train acc:  0.71875
train loss:  0.5434504747390747
train gradient:  0.1294705085930894
iteration : 1449
train acc:  0.6484375
train loss:  0.5734431743621826
train gradient:  0.14654768357015535
iteration : 1450
train acc:  0.6484375
train loss:  0.5854212045669556
train gradient:  0.2198847596477506
iteration : 1451
train acc:  0.765625
train loss:  0.515144944190979
train gradient:  0.18414399218959676
iteration : 1452
train acc:  0.65625
train loss:  0.6045172810554504
train gradient:  0.1764610360317546
iteration : 1453
train acc:  0.7421875
train loss:  0.5276836156845093
train gradient:  0.16282815210676965
iteration : 1454
train acc:  0.6875
train loss:  0.5626790523529053
train gradient:  0.16129516416257933
iteration : 1455
train acc:  0.71875
train loss:  0.5864492058753967
train gradient:  0.21361546665669323
iteration : 1456
train acc:  0.71875
train loss:  0.5201840996742249
train gradient:  0.11746073715995482
iteration : 1457
train acc:  0.6875
train loss:  0.5665159225463867
train gradient:  0.20194323966448763
iteration : 1458
train acc:  0.6640625
train loss:  0.606812596321106
train gradient:  0.15553582336941973
iteration : 1459
train acc:  0.7421875
train loss:  0.5330026149749756
train gradient:  0.1499869889098218
iteration : 1460
train acc:  0.7109375
train loss:  0.5229532122612
train gradient:  0.143189179417576
iteration : 1461
train acc:  0.7421875
train loss:  0.5116274952888489
train gradient:  0.20309987807296437
iteration : 1462
train acc:  0.6640625
train loss:  0.5601270198822021
train gradient:  0.16893002448774924
iteration : 1463
train acc:  0.65625
train loss:  0.5949453115463257
train gradient:  0.1618062482210498
iteration : 1464
train acc:  0.703125
train loss:  0.5545073747634888
train gradient:  0.15279596199615586
iteration : 1465
train acc:  0.71875
train loss:  0.5919630527496338
train gradient:  0.17697010773350721
iteration : 1466
train acc:  0.671875
train loss:  0.5968003273010254
train gradient:  0.18826073445945965
iteration : 1467
train acc:  0.6640625
train loss:  0.5808881521224976
train gradient:  0.18604805254340784
iteration : 1468
train acc:  0.71875
train loss:  0.52362459897995
train gradient:  0.12753612562599742
iteration : 1469
train acc:  0.7109375
train loss:  0.5100198984146118
train gradient:  0.14080130800302873
iteration : 1470
train acc:  0.7421875
train loss:  0.5147395730018616
train gradient:  0.12953590059637726
iteration : 1471
train acc:  0.671875
train loss:  0.5623635053634644
train gradient:  0.17835474129816392
iteration : 1472
train acc:  0.7109375
train loss:  0.5462397336959839
train gradient:  0.12180787439095565
iteration : 1473
train acc:  0.7578125
train loss:  0.5185400247573853
train gradient:  0.1496780336512996
iteration : 1474
train acc:  0.703125
train loss:  0.5482565760612488
train gradient:  0.15805276944829866
iteration : 1475
train acc:  0.640625
train loss:  0.6399676203727722
train gradient:  0.17229756103489774
iteration : 1476
train acc:  0.640625
train loss:  0.622687578201294
train gradient:  0.20415010430540872
iteration : 1477
train acc:  0.671875
train loss:  0.5666286945343018
train gradient:  0.15665026531375503
iteration : 1478
train acc:  0.75
train loss:  0.5291110277175903
train gradient:  0.16355443849087684
iteration : 1479
train acc:  0.75
train loss:  0.5043188333511353
train gradient:  0.13446776061010113
iteration : 1480
train acc:  0.734375
train loss:  0.5465430021286011
train gradient:  0.1751817292324263
iteration : 1481
train acc:  0.6796875
train loss:  0.5828758478164673
train gradient:  0.15426039864676716
iteration : 1482
train acc:  0.7265625
train loss:  0.5505867004394531
train gradient:  0.15009689142616434
iteration : 1483
train acc:  0.6328125
train loss:  0.6033729314804077
train gradient:  0.16108018100083246
iteration : 1484
train acc:  0.796875
train loss:  0.45408234000205994
train gradient:  0.14747634323756292
iteration : 1485
train acc:  0.703125
train loss:  0.5325090885162354
train gradient:  0.13587558317520912
iteration : 1486
train acc:  0.71875
train loss:  0.5800561904907227
train gradient:  0.14757516224026063
iteration : 1487
train acc:  0.6875
train loss:  0.5602357387542725
train gradient:  0.14940674400902434
iteration : 1488
train acc:  0.671875
train loss:  0.5672348737716675
train gradient:  0.16723532291623588
iteration : 1489
train acc:  0.671875
train loss:  0.5918518304824829
train gradient:  0.14192914270485585
iteration : 1490
train acc:  0.6875
train loss:  0.5503026843070984
train gradient:  0.1498809851877509
iteration : 1491
train acc:  0.6484375
train loss:  0.6358761191368103
train gradient:  0.20065278807993842
iteration : 1492
train acc:  0.6953125
train loss:  0.5226389169692993
train gradient:  0.1381554675639971
iteration : 1493
train acc:  0.7109375
train loss:  0.5500672459602356
train gradient:  0.13495070869520975
iteration : 1494
train acc:  0.6953125
train loss:  0.5394442081451416
train gradient:  0.19512878899700198
iteration : 1495
train acc:  0.671875
train loss:  0.5507609844207764
train gradient:  0.11868794539134683
iteration : 1496
train acc:  0.71875
train loss:  0.5792680382728577
train gradient:  0.18297220288494737
iteration : 1497
train acc:  0.71875
train loss:  0.547167181968689
train gradient:  0.137512491608162
iteration : 1498
train acc:  0.6796875
train loss:  0.5374117493629456
train gradient:  0.18470926056075315
iteration : 1499
train acc:  0.625
train loss:  0.6150251626968384
train gradient:  0.17280686698618908
iteration : 1500
train acc:  0.7421875
train loss:  0.5446364283561707
train gradient:  0.20103680561378742
iteration : 1501
train acc:  0.6328125
train loss:  0.5928440690040588
train gradient:  0.18357233526807526
iteration : 1502
train acc:  0.7265625
train loss:  0.5894219875335693
train gradient:  0.17627515241857766
iteration : 1503
train acc:  0.71875
train loss:  0.5686647891998291
train gradient:  0.17411741363038313
iteration : 1504
train acc:  0.6796875
train loss:  0.5851020216941833
train gradient:  0.14252611531057546
iteration : 1505
train acc:  0.7109375
train loss:  0.6012880802154541
train gradient:  0.23426658032587364
iteration : 1506
train acc:  0.6640625
train loss:  0.5617966055870056
train gradient:  0.2037577408197577
iteration : 1507
train acc:  0.7734375
train loss:  0.4932456314563751
train gradient:  0.13605137784720303
iteration : 1508
train acc:  0.71875
train loss:  0.5540462732315063
train gradient:  0.1855467003736423
iteration : 1509
train acc:  0.734375
train loss:  0.5023819208145142
train gradient:  0.1301000465736778
iteration : 1510
train acc:  0.6953125
train loss:  0.5382787585258484
train gradient:  0.13034760823983532
iteration : 1511
train acc:  0.6796875
train loss:  0.5341745018959045
train gradient:  0.18748003776440803
iteration : 1512
train acc:  0.6640625
train loss:  0.5842256546020508
train gradient:  0.13071100394744153
iteration : 1513
train acc:  0.671875
train loss:  0.5748611688613892
train gradient:  0.19949002583463993
iteration : 1514
train acc:  0.7109375
train loss:  0.5576902627944946
train gradient:  0.23569880983851604
iteration : 1515
train acc:  0.7734375
train loss:  0.46916359663009644
train gradient:  0.1401021164625415
iteration : 1516
train acc:  0.796875
train loss:  0.4981306195259094
train gradient:  0.1512656401707937
iteration : 1517
train acc:  0.7578125
train loss:  0.5412223935127258
train gradient:  0.16812331379047912
iteration : 1518
train acc:  0.734375
train loss:  0.5129925608634949
train gradient:  0.2086904707710387
iteration : 1519
train acc:  0.6484375
train loss:  0.6098629236221313
train gradient:  0.17564146876003967
iteration : 1520
train acc:  0.7109375
train loss:  0.5717005729675293
train gradient:  0.17253146970891933
iteration : 1521
train acc:  0.75
train loss:  0.5475873947143555
train gradient:  0.2307069493866566
iteration : 1522
train acc:  0.6640625
train loss:  0.5598784685134888
train gradient:  0.1536338184024761
iteration : 1523
train acc:  0.6953125
train loss:  0.5643728375434875
train gradient:  0.16963132944243126
iteration : 1524
train acc:  0.7734375
train loss:  0.5095336437225342
train gradient:  0.1474420934606538
iteration : 1525
train acc:  0.7578125
train loss:  0.48928678035736084
train gradient:  0.15709222623800345
iteration : 1526
train acc:  0.671875
train loss:  0.5651456713676453
train gradient:  0.16939308133092112
iteration : 1527
train acc:  0.7421875
train loss:  0.5384787917137146
train gradient:  0.17929109590543696
iteration : 1528
train acc:  0.75
train loss:  0.5405378937721252
train gradient:  0.14617344682997158
iteration : 1529
train acc:  0.65625
train loss:  0.6099503040313721
train gradient:  0.1706375671608093
iteration : 1530
train acc:  0.7265625
train loss:  0.5382082462310791
train gradient:  0.2199937766366763
iteration : 1531
train acc:  0.6953125
train loss:  0.5329237580299377
train gradient:  0.15307178896821472
iteration : 1532
train acc:  0.7421875
train loss:  0.5356477499008179
train gradient:  0.23313769688079236
iteration : 1533
train acc:  0.71875
train loss:  0.571041464805603
train gradient:  0.15412091491419921
iteration : 1534
train acc:  0.71875
train loss:  0.546052873134613
train gradient:  0.11270833201927795
iteration : 1535
train acc:  0.6875
train loss:  0.6011937260627747
train gradient:  0.16525988552615448
iteration : 1536
train acc:  0.6640625
train loss:  0.6323895454406738
train gradient:  0.20057486524746776
iteration : 1537
train acc:  0.75
train loss:  0.5656338930130005
train gradient:  0.18273176267219476
iteration : 1538
train acc:  0.671875
train loss:  0.6107439398765564
train gradient:  0.22532319998759187
iteration : 1539
train acc:  0.7265625
train loss:  0.5312660932540894
train gradient:  0.2279356779373543
iteration : 1540
train acc:  0.6875
train loss:  0.5644130706787109
train gradient:  0.2625691600217405
iteration : 1541
train acc:  0.625
train loss:  0.627128005027771
train gradient:  0.20024383768630283
iteration : 1542
train acc:  0.6953125
train loss:  0.603309154510498
train gradient:  0.18208064155177514
iteration : 1543
train acc:  0.7421875
train loss:  0.4956059753894806
train gradient:  0.13569001923361013
iteration : 1544
train acc:  0.75
train loss:  0.5065571069717407
train gradient:  0.14270768043037654
iteration : 1545
train acc:  0.7109375
train loss:  0.527138352394104
train gradient:  0.11384725975883515
iteration : 1546
train acc:  0.6953125
train loss:  0.5894051194190979
train gradient:  0.18496508647497958
iteration : 1547
train acc:  0.734375
train loss:  0.47197723388671875
train gradient:  0.09361549221674065
iteration : 1548
train acc:  0.7421875
train loss:  0.5898739099502563
train gradient:  0.29465413706599103
iteration : 1549
train acc:  0.6953125
train loss:  0.5223737955093384
train gradient:  0.11307901015299257
iteration : 1550
train acc:  0.75
train loss:  0.5484323501586914
train gradient:  0.17595211582651854
iteration : 1551
train acc:  0.71875
train loss:  0.576387345790863
train gradient:  0.23418749037246622
iteration : 1552
train acc:  0.609375
train loss:  0.6860755681991577
train gradient:  0.2517948214958984
iteration : 1553
train acc:  0.6953125
train loss:  0.5730395317077637
train gradient:  0.16605584548712665
iteration : 1554
train acc:  0.734375
train loss:  0.5286024808883667
train gradient:  0.14076077040415877
iteration : 1555
train acc:  0.703125
train loss:  0.5559602975845337
train gradient:  0.2725185771639031
iteration : 1556
train acc:  0.75
train loss:  0.49927258491516113
train gradient:  0.19675156360267027
iteration : 1557
train acc:  0.7421875
train loss:  0.5182057023048401
train gradient:  0.12491522327720121
iteration : 1558
train acc:  0.7890625
train loss:  0.4738750755786896
train gradient:  0.09945081022544756
iteration : 1559
train acc:  0.7265625
train loss:  0.5286756753921509
train gradient:  0.18561204717324115
iteration : 1560
train acc:  0.6640625
train loss:  0.5476226210594177
train gradient:  0.14502708500017802
iteration : 1561
train acc:  0.7890625
train loss:  0.4661848545074463
train gradient:  0.12450699367986348
iteration : 1562
train acc:  0.7578125
train loss:  0.4927879869937897
train gradient:  0.14805783411153034
iteration : 1563
train acc:  0.703125
train loss:  0.5914636850357056
train gradient:  0.1948011155250456
iteration : 1564
train acc:  0.6875
train loss:  0.5498787760734558
train gradient:  0.1433822576417913
iteration : 1565
train acc:  0.6640625
train loss:  0.6019904613494873
train gradient:  0.18841818670981414
iteration : 1566
train acc:  0.78125
train loss:  0.4677078127861023
train gradient:  0.2063727829522845
iteration : 1567
train acc:  0.78125
train loss:  0.49906685948371887
train gradient:  0.11617913953617294
iteration : 1568
train acc:  0.734375
train loss:  0.5571535229682922
train gradient:  0.1790283909148168
iteration : 1569
train acc:  0.7109375
train loss:  0.5424743890762329
train gradient:  0.1842003040893887
iteration : 1570
train acc:  0.7421875
train loss:  0.5293058156967163
train gradient:  0.13925558087529497
iteration : 1571
train acc:  0.6796875
train loss:  0.5457979440689087
train gradient:  0.15513736088094415
iteration : 1572
train acc:  0.6953125
train loss:  0.561560869216919
train gradient:  0.11740343908167704
iteration : 1573
train acc:  0.65625
train loss:  0.5481942892074585
train gradient:  0.14620797358238152
iteration : 1574
train acc:  0.7265625
train loss:  0.5405351519584656
train gradient:  0.15462541730497084
iteration : 1575
train acc:  0.59375
train loss:  0.6329469084739685
train gradient:  0.211032693891374
iteration : 1576
train acc:  0.6875
train loss:  0.5518680810928345
train gradient:  0.1383694715579452
iteration : 1577
train acc:  0.7109375
train loss:  0.5474689602851868
train gradient:  0.1334237311720607
iteration : 1578
train acc:  0.7109375
train loss:  0.531059205532074
train gradient:  0.13763303731834675
iteration : 1579
train acc:  0.65625
train loss:  0.5961108207702637
train gradient:  0.25678788654862095
iteration : 1580
train acc:  0.6953125
train loss:  0.5500214695930481
train gradient:  0.1353496907050215
iteration : 1581
train acc:  0.671875
train loss:  0.593677818775177
train gradient:  0.2219800272238271
iteration : 1582
train acc:  0.6796875
train loss:  0.6150075793266296
train gradient:  0.2527229884604001
iteration : 1583
train acc:  0.6640625
train loss:  0.5156602263450623
train gradient:  0.15233726555160088
iteration : 1584
train acc:  0.6875
train loss:  0.5841618180274963
train gradient:  0.19871265795077178
iteration : 1585
train acc:  0.671875
train loss:  0.6120910048484802
train gradient:  0.19970515678948594
iteration : 1586
train acc:  0.71875
train loss:  0.5465755462646484
train gradient:  0.15194783345413573
iteration : 1587
train acc:  0.6953125
train loss:  0.5418080687522888
train gradient:  0.16782283827367475
iteration : 1588
train acc:  0.7890625
train loss:  0.47787511348724365
train gradient:  0.15506048196987293
iteration : 1589
train acc:  0.7265625
train loss:  0.526152491569519
train gradient:  0.15411671981989136
iteration : 1590
train acc:  0.703125
train loss:  0.5319989323616028
train gradient:  0.16066919435527854
iteration : 1591
train acc:  0.6796875
train loss:  0.5664671063423157
train gradient:  0.18541537303538513
iteration : 1592
train acc:  0.765625
train loss:  0.5227652788162231
train gradient:  0.18461360864518161
iteration : 1593
train acc:  0.7578125
train loss:  0.5419768691062927
train gradient:  0.1269287350237125
iteration : 1594
train acc:  0.71875
train loss:  0.5606553554534912
train gradient:  0.19601781441931337
iteration : 1595
train acc:  0.6796875
train loss:  0.561474621295929
train gradient:  0.1651844072686129
iteration : 1596
train acc:  0.78125
train loss:  0.4805741310119629
train gradient:  0.1631263295294706
iteration : 1597
train acc:  0.703125
train loss:  0.5273165702819824
train gradient:  0.17661019338963696
iteration : 1598
train acc:  0.65625
train loss:  0.5773248672485352
train gradient:  0.17183925893242374
iteration : 1599
train acc:  0.734375
train loss:  0.5244308710098267
train gradient:  0.1210095325235984
iteration : 1600
train acc:  0.78125
train loss:  0.5343843102455139
train gradient:  0.1481075444518948
iteration : 1601
train acc:  0.7578125
train loss:  0.5098502039909363
train gradient:  0.1448162179181875
iteration : 1602
train acc:  0.640625
train loss:  0.6025846600532532
train gradient:  0.17913223351254864
iteration : 1603
train acc:  0.6953125
train loss:  0.5562467575073242
train gradient:  0.19615382342049542
iteration : 1604
train acc:  0.671875
train loss:  0.5740903615951538
train gradient:  0.17594260948154172
iteration : 1605
train acc:  0.7421875
train loss:  0.525829553604126
train gradient:  0.15837881698480033
iteration : 1606
train acc:  0.7890625
train loss:  0.47526830434799194
train gradient:  0.1252519618868268
iteration : 1607
train acc:  0.6640625
train loss:  0.5773371458053589
train gradient:  0.17097377390163743
iteration : 1608
train acc:  0.703125
train loss:  0.5784496068954468
train gradient:  0.21072397229620435
iteration : 1609
train acc:  0.7265625
train loss:  0.5030341148376465
train gradient:  0.12058823445316194
iteration : 1610
train acc:  0.6796875
train loss:  0.5378586649894714
train gradient:  0.21406075267942032
iteration : 1611
train acc:  0.7109375
train loss:  0.539094090461731
train gradient:  0.17746428774179548
iteration : 1612
train acc:  0.734375
train loss:  0.5518869161605835
train gradient:  0.13574582341414554
iteration : 1613
train acc:  0.7109375
train loss:  0.6030561923980713
train gradient:  0.20661152327192917
iteration : 1614
train acc:  0.71875
train loss:  0.5351002216339111
train gradient:  0.14346154306224762
iteration : 1615
train acc:  0.8203125
train loss:  0.4522169232368469
train gradient:  0.1454650677415515
iteration : 1616
train acc:  0.6796875
train loss:  0.5787532329559326
train gradient:  0.1809859765985582
iteration : 1617
train acc:  0.765625
train loss:  0.5177463293075562
train gradient:  0.14106961126448725
iteration : 1618
train acc:  0.6875
train loss:  0.5756309628486633
train gradient:  0.1717631759048836
iteration : 1619
train acc:  0.609375
train loss:  0.6135355830192566
train gradient:  0.17863048064743167
iteration : 1620
train acc:  0.6953125
train loss:  0.552406907081604
train gradient:  0.14205635545063733
iteration : 1621
train acc:  0.71875
train loss:  0.5203922390937805
train gradient:  0.14288457032430926
iteration : 1622
train acc:  0.6171875
train loss:  0.6230533123016357
train gradient:  0.19757128856184164
iteration : 1623
train acc:  0.6953125
train loss:  0.5925710797309875
train gradient:  0.2355382220195908
iteration : 1624
train acc:  0.7109375
train loss:  0.5503857135772705
train gradient:  0.16868340679866506
iteration : 1625
train acc:  0.7421875
train loss:  0.49978432059288025
train gradient:  0.15832935187872343
iteration : 1626
train acc:  0.640625
train loss:  0.5707129240036011
train gradient:  0.15774770829448012
iteration : 1627
train acc:  0.6875
train loss:  0.5532640218734741
train gradient:  0.14417417431691826
iteration : 1628
train acc:  0.6953125
train loss:  0.5762100219726562
train gradient:  0.19570020850877584
iteration : 1629
train acc:  0.65625
train loss:  0.6039361953735352
train gradient:  0.2926778898187559
iteration : 1630
train acc:  0.6484375
train loss:  0.5945050120353699
train gradient:  0.16734085397394088
iteration : 1631
train acc:  0.75
train loss:  0.5178800821304321
train gradient:  0.16545623076644775
iteration : 1632
train acc:  0.71875
train loss:  0.5266225337982178
train gradient:  0.1563565124559726
iteration : 1633
train acc:  0.734375
train loss:  0.5239836573600769
train gradient:  0.16487598120086222
iteration : 1634
train acc:  0.6875
train loss:  0.5499768853187561
train gradient:  0.13818737968728323
iteration : 1635
train acc:  0.78125
train loss:  0.4770420491695404
train gradient:  0.13601264237214905
iteration : 1636
train acc:  0.7265625
train loss:  0.5692222118377686
train gradient:  0.22179995279432163
iteration : 1637
train acc:  0.6796875
train loss:  0.6078972220420837
train gradient:  0.21192977088648574
iteration : 1638
train acc:  0.65625
train loss:  0.6072039604187012
train gradient:  0.17731649146144157
iteration : 1639
train acc:  0.6875
train loss:  0.5512468218803406
train gradient:  0.14752400680020417
iteration : 1640
train acc:  0.703125
train loss:  0.5865627527236938
train gradient:  0.17686230673793685
iteration : 1641
train acc:  0.7421875
train loss:  0.5660346746444702
train gradient:  0.15194845568831175
iteration : 1642
train acc:  0.6484375
train loss:  0.5610062479972839
train gradient:  0.23932451500764368
iteration : 1643
train acc:  0.6640625
train loss:  0.6049233078956604
train gradient:  0.17843109722744965
iteration : 1644
train acc:  0.703125
train loss:  0.5502159595489502
train gradient:  0.17457804516729267
iteration : 1645
train acc:  0.734375
train loss:  0.5008416175842285
train gradient:  0.2071006703027034
iteration : 1646
train acc:  0.7109375
train loss:  0.5320236682891846
train gradient:  0.18131497214454617
iteration : 1647
train acc:  0.7265625
train loss:  0.5411697626113892
train gradient:  0.16690843132504762
iteration : 1648
train acc:  0.640625
train loss:  0.5987498760223389
train gradient:  0.20020413754520583
iteration : 1649
train acc:  0.6640625
train loss:  0.5726300477981567
train gradient:  0.17951350131217925
iteration : 1650
train acc:  0.71875
train loss:  0.5636056661605835
train gradient:  0.15132606259824738
iteration : 1651
train acc:  0.734375
train loss:  0.5487882494926453
train gradient:  0.2016128128796011
iteration : 1652
train acc:  0.71875
train loss:  0.5711058378219604
train gradient:  0.1447711133801588
iteration : 1653
train acc:  0.7421875
train loss:  0.5009775161743164
train gradient:  0.13068419139482032
iteration : 1654
train acc:  0.7421875
train loss:  0.4860951006412506
train gradient:  0.1188848000677839
iteration : 1655
train acc:  0.6796875
train loss:  0.5600286722183228
train gradient:  0.14907405993848205
iteration : 1656
train acc:  0.6796875
train loss:  0.5589281320571899
train gradient:  0.15990182882374582
iteration : 1657
train acc:  0.6796875
train loss:  0.5634581446647644
train gradient:  0.1577119638954419
iteration : 1658
train acc:  0.671875
train loss:  0.600658655166626
train gradient:  0.21556754342594675
iteration : 1659
train acc:  0.703125
train loss:  0.5359737873077393
train gradient:  0.1480039415196752
iteration : 1660
train acc:  0.765625
train loss:  0.49451202154159546
train gradient:  0.1467296417170862
iteration : 1661
train acc:  0.734375
train loss:  0.5057714581489563
train gradient:  0.15619105543716574
iteration : 1662
train acc:  0.6484375
train loss:  0.6173223853111267
train gradient:  0.1780113115949997
iteration : 1663
train acc:  0.671875
train loss:  0.5661193132400513
train gradient:  0.15669759114212062
iteration : 1664
train acc:  0.671875
train loss:  0.5608946084976196
train gradient:  0.24205765773778032
iteration : 1665
train acc:  0.671875
train loss:  0.6368827819824219
train gradient:  0.18164532952196577
iteration : 1666
train acc:  0.6953125
train loss:  0.5621678829193115
train gradient:  0.1520049159760336
iteration : 1667
train acc:  0.734375
train loss:  0.5561776161193848
train gradient:  0.13940363711859927
iteration : 1668
train acc:  0.6796875
train loss:  0.5934447646141052
train gradient:  0.2009655486782057
iteration : 1669
train acc:  0.7421875
train loss:  0.5708135366439819
train gradient:  0.16835817881125187
iteration : 1670
train acc:  0.7421875
train loss:  0.5612784624099731
train gradient:  0.167175235187134
iteration : 1671
train acc:  0.734375
train loss:  0.5309967398643494
train gradient:  0.14219580974064738
iteration : 1672
train acc:  0.734375
train loss:  0.5499426126480103
train gradient:  0.16015918432915588
iteration : 1673
train acc:  0.671875
train loss:  0.5629174709320068
train gradient:  0.23516409873957805
iteration : 1674
train acc:  0.6328125
train loss:  0.6131315231323242
train gradient:  0.20300527515226602
iteration : 1675
train acc:  0.703125
train loss:  0.5696057081222534
train gradient:  0.17148011701641458
iteration : 1676
train acc:  0.6875
train loss:  0.5373585224151611
train gradient:  0.14515689547542726
iteration : 1677
train acc:  0.78125
train loss:  0.5099063515663147
train gradient:  0.13101359054910977
iteration : 1678
train acc:  0.609375
train loss:  0.6499540209770203
train gradient:  0.19195470838328121
iteration : 1679
train acc:  0.6484375
train loss:  0.5748804807662964
train gradient:  0.1926238013909286
iteration : 1680
train acc:  0.6953125
train loss:  0.5964161157608032
train gradient:  0.15391848998562926
iteration : 1681
train acc:  0.78125
train loss:  0.4615222215652466
train gradient:  0.10198567011865697
iteration : 1682
train acc:  0.6953125
train loss:  0.5882582068443298
train gradient:  0.19506434792822683
iteration : 1683
train acc:  0.7109375
train loss:  0.558129072189331
train gradient:  0.15036533672020252
iteration : 1684
train acc:  0.7421875
train loss:  0.4942789673805237
train gradient:  0.1479587137930658
iteration : 1685
train acc:  0.7734375
train loss:  0.5402032136917114
train gradient:  0.13732411061525646
iteration : 1686
train acc:  0.6171875
train loss:  0.5877323150634766
train gradient:  0.1605146277766265
iteration : 1687
train acc:  0.7109375
train loss:  0.5360242128372192
train gradient:  0.12657830755306232
iteration : 1688
train acc:  0.6796875
train loss:  0.5351351499557495
train gradient:  0.22229725773628198
iteration : 1689
train acc:  0.671875
train loss:  0.5681182146072388
train gradient:  0.20153689199607056
iteration : 1690
train acc:  0.6875
train loss:  0.5488988757133484
train gradient:  0.14913446581114953
iteration : 1691
train acc:  0.6953125
train loss:  0.5730540752410889
train gradient:  0.18784271410840464
iteration : 1692
train acc:  0.6796875
train loss:  0.5799120664596558
train gradient:  0.16806684250372309
iteration : 1693
train acc:  0.7421875
train loss:  0.5387600064277649
train gradient:  0.1390118009556044
iteration : 1694
train acc:  0.734375
train loss:  0.5397320985794067
train gradient:  0.13248466365679124
iteration : 1695
train acc:  0.6640625
train loss:  0.5999455451965332
train gradient:  0.22283571287288395
iteration : 1696
train acc:  0.7421875
train loss:  0.5290659666061401
train gradient:  0.1352998182042747
iteration : 1697
train acc:  0.7265625
train loss:  0.48653823137283325
train gradient:  0.1595353424896549
iteration : 1698
train acc:  0.71875
train loss:  0.558759331703186
train gradient:  0.16352538475934222
iteration : 1699
train acc:  0.7265625
train loss:  0.5257474184036255
train gradient:  0.13035139179747746
iteration : 1700
train acc:  0.703125
train loss:  0.6002624034881592
train gradient:  0.18274346164061098
iteration : 1701
train acc:  0.7109375
train loss:  0.5166027545928955
train gradient:  0.1466774324054605
iteration : 1702
train acc:  0.71875
train loss:  0.5030598640441895
train gradient:  0.15919234327573967
iteration : 1703
train acc:  0.671875
train loss:  0.5794689655303955
train gradient:  0.22660021464342944
iteration : 1704
train acc:  0.6875
train loss:  0.515012800693512
train gradient:  0.15881281730496694
iteration : 1705
train acc:  0.7265625
train loss:  0.5123315453529358
train gradient:  0.20642620811835843
iteration : 1706
train acc:  0.71875
train loss:  0.5566798448562622
train gradient:  0.1987280246855139
iteration : 1707
train acc:  0.703125
train loss:  0.5469642281532288
train gradient:  0.16560996921037144
iteration : 1708
train acc:  0.6796875
train loss:  0.5743865966796875
train gradient:  0.21233774113910475
iteration : 1709
train acc:  0.65625
train loss:  0.6124652028083801
train gradient:  0.15277542527203047
iteration : 1710
train acc:  0.6640625
train loss:  0.5782228708267212
train gradient:  0.1749392400313024
iteration : 1711
train acc:  0.71875
train loss:  0.5148022174835205
train gradient:  0.15135769025946227
iteration : 1712
train acc:  0.6953125
train loss:  0.5637956857681274
train gradient:  0.15731852367390597
iteration : 1713
train acc:  0.703125
train loss:  0.5634636878967285
train gradient:  0.16135534228687845
iteration : 1714
train acc:  0.609375
train loss:  0.6451482772827148
train gradient:  0.20657071020348727
iteration : 1715
train acc:  0.6796875
train loss:  0.5763009786605835
train gradient:  0.2136558321695248
iteration : 1716
train acc:  0.734375
train loss:  0.5253881216049194
train gradient:  0.1836216688511252
iteration : 1717
train acc:  0.671875
train loss:  0.6207931041717529
train gradient:  0.2012478858582598
iteration : 1718
train acc:  0.7109375
train loss:  0.5462938547134399
train gradient:  0.16338922988262639
iteration : 1719
train acc:  0.734375
train loss:  0.5290923118591309
train gradient:  0.17454414769873283
iteration : 1720
train acc:  0.734375
train loss:  0.5092654228210449
train gradient:  0.13290905179050566
iteration : 1721
train acc:  0.6875
train loss:  0.5798251628875732
train gradient:  0.25843775697692406
iteration : 1722
train acc:  0.6953125
train loss:  0.5524792671203613
train gradient:  0.19758530891173137
iteration : 1723
train acc:  0.703125
train loss:  0.5755528211593628
train gradient:  0.16655183627538733
iteration : 1724
train acc:  0.75
train loss:  0.5346701741218567
train gradient:  0.15889227907976514
iteration : 1725
train acc:  0.7109375
train loss:  0.5126922130584717
train gradient:  0.15215997092457975
iteration : 1726
train acc:  0.78125
train loss:  0.5111554265022278
train gradient:  0.1785679139836054
iteration : 1727
train acc:  0.6875
train loss:  0.5888726115226746
train gradient:  0.23172617658003813
iteration : 1728
train acc:  0.71875
train loss:  0.510381817817688
train gradient:  0.1149655180700813
iteration : 1729
train acc:  0.71875
train loss:  0.546448826789856
train gradient:  0.1518155121713445
iteration : 1730
train acc:  0.7265625
train loss:  0.5478143692016602
train gradient:  0.14744238702101267
iteration : 1731
train acc:  0.6953125
train loss:  0.5512651205062866
train gradient:  0.1385833675487045
iteration : 1732
train acc:  0.734375
train loss:  0.5118989944458008
train gradient:  0.1245874437156431
iteration : 1733
train acc:  0.7578125
train loss:  0.5359064340591431
train gradient:  0.19643593730001174
iteration : 1734
train acc:  0.6640625
train loss:  0.5678350329399109
train gradient:  0.17329129314802816
iteration : 1735
train acc:  0.6796875
train loss:  0.5977829098701477
train gradient:  0.19802828220266416
iteration : 1736
train acc:  0.71875
train loss:  0.5208529233932495
train gradient:  0.12232176048465848
iteration : 1737
train acc:  0.6875
train loss:  0.5330213904380798
train gradient:  0.14632225872873464
iteration : 1738
train acc:  0.734375
train loss:  0.5511153340339661
train gradient:  0.1698056816128401
iteration : 1739
train acc:  0.671875
train loss:  0.5508140921592712
train gradient:  0.18188645514808296
iteration : 1740
train acc:  0.6953125
train loss:  0.5646764636039734
train gradient:  0.1877527451719317
iteration : 1741
train acc:  0.71875
train loss:  0.5653814673423767
train gradient:  0.14356530639971832
iteration : 1742
train acc:  0.671875
train loss:  0.5356932878494263
train gradient:  0.15513464769868174
iteration : 1743
train acc:  0.7421875
train loss:  0.5012953281402588
train gradient:  0.12022068888059785
iteration : 1744
train acc:  0.7890625
train loss:  0.5132912993431091
train gradient:  0.14808757195913225
iteration : 1745
train acc:  0.671875
train loss:  0.5639788508415222
train gradient:  0.20621011486476937
iteration : 1746
train acc:  0.671875
train loss:  0.5885084867477417
train gradient:  0.2355238211174398
iteration : 1747
train acc:  0.671875
train loss:  0.5586977601051331
train gradient:  0.16539255694937893
iteration : 1748
train acc:  0.671875
train loss:  0.5266966819763184
train gradient:  0.10939343623566845
iteration : 1749
train acc:  0.734375
train loss:  0.5518091320991516
train gradient:  0.14523930337067548
iteration : 1750
train acc:  0.765625
train loss:  0.5209923386573792
train gradient:  0.14118611917315926
iteration : 1751
train acc:  0.71875
train loss:  0.5248442888259888
train gradient:  0.1902435081841791
iteration : 1752
train acc:  0.6015625
train loss:  0.6342054605484009
train gradient:  0.23883680312106703
iteration : 1753
train acc:  0.703125
train loss:  0.5331663489341736
train gradient:  0.15896994103865306
iteration : 1754
train acc:  0.6796875
train loss:  0.5941816568374634
train gradient:  0.2158801711113857
iteration : 1755
train acc:  0.71875
train loss:  0.5655120611190796
train gradient:  0.14884973276535202
iteration : 1756
train acc:  0.65625
train loss:  0.607965350151062
train gradient:  0.16853982460179978
iteration : 1757
train acc:  0.75
train loss:  0.5045098066329956
train gradient:  0.14598423094375365
iteration : 1758
train acc:  0.7421875
train loss:  0.49684953689575195
train gradient:  0.09857377667770184
iteration : 1759
train acc:  0.7421875
train loss:  0.4901030361652374
train gradient:  0.10657712344912006
iteration : 1760
train acc:  0.703125
train loss:  0.5230271816253662
train gradient:  0.13635457956819733
iteration : 1761
train acc:  0.671875
train loss:  0.6037620306015015
train gradient:  0.2301000617652304
iteration : 1762
train acc:  0.6953125
train loss:  0.5469552278518677
train gradient:  0.21717970482790216
iteration : 1763
train acc:  0.765625
train loss:  0.5197302103042603
train gradient:  0.11655234731942547
iteration : 1764
train acc:  0.6640625
train loss:  0.534386157989502
train gradient:  0.1651383543393365
iteration : 1765
train acc:  0.71875
train loss:  0.5907676815986633
train gradient:  0.19244075151423226
iteration : 1766
train acc:  0.671875
train loss:  0.589486300945282
train gradient:  0.17284812831077917
iteration : 1767
train acc:  0.6875
train loss:  0.5471402406692505
train gradient:  0.12868732275011457
iteration : 1768
train acc:  0.7109375
train loss:  0.5623444318771362
train gradient:  0.23318841397445378
iteration : 1769
train acc:  0.703125
train loss:  0.5968508124351501
train gradient:  0.1894787344349179
iteration : 1770
train acc:  0.6796875
train loss:  0.5348421335220337
train gradient:  0.13103189849451885
iteration : 1771
train acc:  0.671875
train loss:  0.5652120113372803
train gradient:  0.1839211689520288
iteration : 1772
train acc:  0.6796875
train loss:  0.5740683674812317
train gradient:  0.1732888310006653
iteration : 1773
train acc:  0.640625
train loss:  0.5748582482337952
train gradient:  0.16692633180310612
iteration : 1774
train acc:  0.7421875
train loss:  0.5524465441703796
train gradient:  0.21118858920171124
iteration : 1775
train acc:  0.7109375
train loss:  0.5368413925170898
train gradient:  0.3395997886483431
iteration : 1776
train acc:  0.7265625
train loss:  0.5494226217269897
train gradient:  0.139235868018301
iteration : 1777
train acc:  0.6875
train loss:  0.5551245212554932
train gradient:  0.1594402969946265
iteration : 1778
train acc:  0.6640625
train loss:  0.5846145153045654
train gradient:  0.16210842093986233
iteration : 1779
train acc:  0.71875
train loss:  0.5602320432662964
train gradient:  0.15243229772596853
iteration : 1780
train acc:  0.71875
train loss:  0.5737899541854858
train gradient:  0.17724904203505737
iteration : 1781
train acc:  0.734375
train loss:  0.4947151839733124
train gradient:  0.11232310729981915
iteration : 1782
train acc:  0.734375
train loss:  0.5129188299179077
train gradient:  0.15028272619599065
iteration : 1783
train acc:  0.71875
train loss:  0.5443044304847717
train gradient:  0.17023166307473686
iteration : 1784
train acc:  0.6875
train loss:  0.5498383045196533
train gradient:  0.15424447642179545
iteration : 1785
train acc:  0.7109375
train loss:  0.5329058170318604
train gradient:  0.11674891699571696
iteration : 1786
train acc:  0.6640625
train loss:  0.626614511013031
train gradient:  0.21049126762512765
iteration : 1787
train acc:  0.6796875
train loss:  0.5828014612197876
train gradient:  0.13949872397804702
iteration : 1788
train acc:  0.7578125
train loss:  0.5339058637619019
train gradient:  0.193915535294516
iteration : 1789
train acc:  0.625
train loss:  0.5924265384674072
train gradient:  0.14528682230406043
iteration : 1790
train acc:  0.6640625
train loss:  0.5787041187286377
train gradient:  0.19744550953160134
iteration : 1791
train acc:  0.6953125
train loss:  0.5648770928382874
train gradient:  0.2672681694704861
iteration : 1792
train acc:  0.71875
train loss:  0.5101804733276367
train gradient:  0.13612488036514903
iteration : 1793
train acc:  0.703125
train loss:  0.569048285484314
train gradient:  0.18863039748740004
iteration : 1794
train acc:  0.6953125
train loss:  0.5380737781524658
train gradient:  0.1588405858006159
iteration : 1795
train acc:  0.734375
train loss:  0.5053803324699402
train gradient:  0.11526365913384287
iteration : 1796
train acc:  0.6484375
train loss:  0.5953003764152527
train gradient:  0.18351451443352665
iteration : 1797
train acc:  0.6640625
train loss:  0.5800201296806335
train gradient:  0.1769856504288731
iteration : 1798
train acc:  0.765625
train loss:  0.543358564376831
train gradient:  0.1882508304501315
iteration : 1799
train acc:  0.7421875
train loss:  0.47605475783348083
train gradient:  0.1599904319954904
iteration : 1800
train acc:  0.6953125
train loss:  0.5414537191390991
train gradient:  0.2379351513408837
iteration : 1801
train acc:  0.71875
train loss:  0.557165265083313
train gradient:  0.1760738843226098
iteration : 1802
train acc:  0.703125
train loss:  0.5443658828735352
train gradient:  0.12079073043119201
iteration : 1803
train acc:  0.7265625
train loss:  0.529143214225769
train gradient:  0.13732951017972245
iteration : 1804
train acc:  0.6640625
train loss:  0.5480877161026001
train gradient:  0.13616677117379977
iteration : 1805
train acc:  0.671875
train loss:  0.5399118661880493
train gradient:  0.11793655807134504
iteration : 1806
train acc:  0.71875
train loss:  0.5138201713562012
train gradient:  0.13510639116234735
iteration : 1807
train acc:  0.65625
train loss:  0.6008754968643188
train gradient:  0.20119911911392158
iteration : 1808
train acc:  0.65625
train loss:  0.5989071130752563
train gradient:  0.20545964356437718
iteration : 1809
train acc:  0.734375
train loss:  0.5294215083122253
train gradient:  0.3574429848798873
iteration : 1810
train acc:  0.7265625
train loss:  0.5401153564453125
train gradient:  0.18353493010826585
iteration : 1811
train acc:  0.71875
train loss:  0.5259459018707275
train gradient:  0.15786447572311568
iteration : 1812
train acc:  0.703125
train loss:  0.5636371374130249
train gradient:  0.17297463704499177
iteration : 1813
train acc:  0.734375
train loss:  0.5299296975135803
train gradient:  0.15564150065110285
iteration : 1814
train acc:  0.625
train loss:  0.5632023215293884
train gradient:  0.1432434718499253
iteration : 1815
train acc:  0.71875
train loss:  0.5104678273200989
train gradient:  0.13619020401873067
iteration : 1816
train acc:  0.765625
train loss:  0.5166153907775879
train gradient:  0.11720654213171085
iteration : 1817
train acc:  0.640625
train loss:  0.5838836431503296
train gradient:  0.147076122622581
iteration : 1818
train acc:  0.7265625
train loss:  0.5557756423950195
train gradient:  0.1395633475524145
iteration : 1819
train acc:  0.6484375
train loss:  0.5948306322097778
train gradient:  0.1837726065270008
iteration : 1820
train acc:  0.6796875
train loss:  0.595693826675415
train gradient:  0.25703600995288645
iteration : 1821
train acc:  0.7109375
train loss:  0.5804488062858582
train gradient:  0.15168852151006698
iteration : 1822
train acc:  0.6640625
train loss:  0.5390243530273438
train gradient:  0.15551168889487466
iteration : 1823
train acc:  0.6953125
train loss:  0.5281786322593689
train gradient:  0.13903202305251372
iteration : 1824
train acc:  0.734375
train loss:  0.5234744548797607
train gradient:  0.13253437325350426
iteration : 1825
train acc:  0.7421875
train loss:  0.5508530139923096
train gradient:  0.19590395856708787
iteration : 1826
train acc:  0.6484375
train loss:  0.6173515319824219
train gradient:  0.26274603037877153
iteration : 1827
train acc:  0.734375
train loss:  0.5380029678344727
train gradient:  0.2541935988658763
iteration : 1828
train acc:  0.6484375
train loss:  0.5683133602142334
train gradient:  0.15042019007327406
iteration : 1829
train acc:  0.75
train loss:  0.5051382780075073
train gradient:  0.14110389492193381
iteration : 1830
train acc:  0.6640625
train loss:  0.5909744501113892
train gradient:  0.18629384030876794
iteration : 1831
train acc:  0.75
train loss:  0.5120639801025391
train gradient:  0.13761043484628493
iteration : 1832
train acc:  0.6484375
train loss:  0.5941791534423828
train gradient:  0.16685309248085867
iteration : 1833
train acc:  0.6875
train loss:  0.571395993232727
train gradient:  0.2009154474586499
iteration : 1834
train acc:  0.78125
train loss:  0.48101097345352173
train gradient:  0.1360141059141135
iteration : 1835
train acc:  0.6953125
train loss:  0.5528407096862793
train gradient:  0.20202922898819703
iteration : 1836
train acc:  0.6953125
train loss:  0.5568604469299316
train gradient:  0.1646879122926303
iteration : 1837
train acc:  0.6875
train loss:  0.5308670997619629
train gradient:  0.14416159647584842
iteration : 1838
train acc:  0.7109375
train loss:  0.5424937009811401
train gradient:  0.16861748598739432
iteration : 1839
train acc:  0.7109375
train loss:  0.551537275314331
train gradient:  0.12860175975331117
iteration : 1840
train acc:  0.765625
train loss:  0.5243098735809326
train gradient:  0.16612740731015233
iteration : 1841
train acc:  0.7109375
train loss:  0.6059499382972717
train gradient:  0.21029959896616507
iteration : 1842
train acc:  0.7265625
train loss:  0.5511329174041748
train gradient:  0.14655794756831356
iteration : 1843
train acc:  0.703125
train loss:  0.5510998964309692
train gradient:  0.19601956305944584
iteration : 1844
train acc:  0.6875
train loss:  0.5887947082519531
train gradient:  0.1891578550806011
iteration : 1845
train acc:  0.6875
train loss:  0.5520923137664795
train gradient:  0.17757348973934767
iteration : 1846
train acc:  0.6328125
train loss:  0.595426619052887
train gradient:  0.18260747156121224
iteration : 1847
train acc:  0.671875
train loss:  0.611663281917572
train gradient:  0.22511817814439694
iteration : 1848
train acc:  0.734375
train loss:  0.5368132591247559
train gradient:  0.13809691913297426
iteration : 1849
train acc:  0.703125
train loss:  0.5344911813735962
train gradient:  0.1349810764185786
iteration : 1850
train acc:  0.734375
train loss:  0.58402019739151
train gradient:  0.17649006701280778
iteration : 1851
train acc:  0.6796875
train loss:  0.5931581854820251
train gradient:  0.23122509185685813
iteration : 1852
train acc:  0.765625
train loss:  0.5333934426307678
train gradient:  0.15852343494840115
iteration : 1853
train acc:  0.7265625
train loss:  0.5257778763771057
train gradient:  0.1626742562877102
iteration : 1854
train acc:  0.671875
train loss:  0.5984382629394531
train gradient:  0.26603639836814574
iteration : 1855
train acc:  0.703125
train loss:  0.5115945339202881
train gradient:  0.14714827496293711
iteration : 1856
train acc:  0.6484375
train loss:  0.599381685256958
train gradient:  0.17439853076043077
iteration : 1857
train acc:  0.7421875
train loss:  0.4973708987236023
train gradient:  0.13143944578202982
iteration : 1858
train acc:  0.703125
train loss:  0.5270722508430481
train gradient:  0.25662855578420574
iteration : 1859
train acc:  0.6640625
train loss:  0.5459426045417786
train gradient:  0.12894065085914103
iteration : 1860
train acc:  0.7109375
train loss:  0.5819348096847534
train gradient:  0.15607241482394357
iteration : 1861
train acc:  0.6640625
train loss:  0.5840984582901001
train gradient:  0.15452691192160167
iteration : 1862
train acc:  0.65625
train loss:  0.6210527420043945
train gradient:  0.16752506571722947
iteration : 1863
train acc:  0.7421875
train loss:  0.5326354503631592
train gradient:  0.19173556657632654
iteration : 1864
train acc:  0.71875
train loss:  0.5079923868179321
train gradient:  0.18590787818071114
iteration : 1865
train acc:  0.6875
train loss:  0.5559468865394592
train gradient:  0.22219798815176572
iteration : 1866
train acc:  0.7265625
train loss:  0.510245144367218
train gradient:  0.15847101710556247
iteration : 1867
train acc:  0.765625
train loss:  0.4956296682357788
train gradient:  0.148815887917011
iteration : 1868
train acc:  0.71875
train loss:  0.5469828248023987
train gradient:  0.1806012523974988
iteration : 1869
train acc:  0.7265625
train loss:  0.5129679441452026
train gradient:  0.15234911235831086
iteration : 1870
train acc:  0.7421875
train loss:  0.5364863872528076
train gradient:  0.16014894926506934
iteration : 1871
train acc:  0.6796875
train loss:  0.5567338466644287
train gradient:  0.15412638806469997
iteration : 1872
train acc:  0.6328125
train loss:  0.6204897165298462
train gradient:  0.32277871860048774
iteration : 1873
train acc:  0.7109375
train loss:  0.5547981262207031
train gradient:  0.1539766974851921
iteration : 1874
train acc:  0.7109375
train loss:  0.5564279556274414
train gradient:  0.1369108682705874
iteration : 1875
train acc:  0.6875
train loss:  0.5247008800506592
train gradient:  0.12133034591789202
iteration : 1876
train acc:  0.6875
train loss:  0.5197270512580872
train gradient:  0.14107345423892564
iteration : 1877
train acc:  0.6875
train loss:  0.569912850856781
train gradient:  0.15623932037552427
iteration : 1878
train acc:  0.7578125
train loss:  0.5014719367027283
train gradient:  0.17524688644248548
iteration : 1879
train acc:  0.6484375
train loss:  0.5644091367721558
train gradient:  0.16595114706204667
iteration : 1880
train acc:  0.7578125
train loss:  0.515790581703186
train gradient:  0.14273044472784624
iteration : 1881
train acc:  0.71875
train loss:  0.5454092025756836
train gradient:  0.17910578278619171
iteration : 1882
train acc:  0.6484375
train loss:  0.615329384803772
train gradient:  0.21001107037493932
iteration : 1883
train acc:  0.6953125
train loss:  0.5542829036712646
train gradient:  0.12722306981909987
iteration : 1884
train acc:  0.6953125
train loss:  0.5997523069381714
train gradient:  0.1556548095318491
iteration : 1885
train acc:  0.7265625
train loss:  0.5570435523986816
train gradient:  0.14903720229064724
iteration : 1886
train acc:  0.703125
train loss:  0.5686107277870178
train gradient:  0.17560545163798744
iteration : 1887
train acc:  0.75
train loss:  0.49417251348495483
train gradient:  0.14252420115075193
iteration : 1888
train acc:  0.703125
train loss:  0.5091685056686401
train gradient:  0.11231614189200147
iteration : 1889
train acc:  0.71875
train loss:  0.49091020226478577
train gradient:  0.1395226955702622
iteration : 1890
train acc:  0.71875
train loss:  0.5466445684432983
train gradient:  0.1490292686524824
iteration : 1891
train acc:  0.7265625
train loss:  0.5075753927230835
train gradient:  0.14033869405456628
iteration : 1892
train acc:  0.671875
train loss:  0.5650001764297485
train gradient:  0.1635946474948313
iteration : 1893
train acc:  0.7734375
train loss:  0.5357424020767212
train gradient:  0.12724492185647718
iteration : 1894
train acc:  0.7578125
train loss:  0.47119563817977905
train gradient:  0.1251867190680049
iteration : 1895
train acc:  0.6953125
train loss:  0.545036256313324
train gradient:  0.17840893972670768
iteration : 1896
train acc:  0.7265625
train loss:  0.5134342908859253
train gradient:  0.14923475965265043
iteration : 1897
train acc:  0.6953125
train loss:  0.574401319026947
train gradient:  0.1392846941592496
iteration : 1898
train acc:  0.734375
train loss:  0.5029460191726685
train gradient:  0.21463637844200587
iteration : 1899
train acc:  0.6953125
train loss:  0.570013165473938
train gradient:  0.1632061482182592
iteration : 1900
train acc:  0.7578125
train loss:  0.514714241027832
train gradient:  0.1643064520279957
iteration : 1901
train acc:  0.7265625
train loss:  0.5053677558898926
train gradient:  0.15071328400111156
iteration : 1902
train acc:  0.6640625
train loss:  0.5680146217346191
train gradient:  0.16567112153861938
iteration : 1903
train acc:  0.7265625
train loss:  0.5666658878326416
train gradient:  0.16960987179924397
iteration : 1904
train acc:  0.625
train loss:  0.627576470375061
train gradient:  0.1854059021227439
iteration : 1905
train acc:  0.7734375
train loss:  0.48645082116127014
train gradient:  0.10567043783438532
iteration : 1906
train acc:  0.59375
train loss:  0.6476589441299438
train gradient:  0.2222814104652552
iteration : 1907
train acc:  0.71875
train loss:  0.5671690702438354
train gradient:  0.16252770298264702
iteration : 1908
train acc:  0.75
train loss:  0.532743513584137
train gradient:  0.15793791659794365
iteration : 1909
train acc:  0.6484375
train loss:  0.6028776168823242
train gradient:  0.16497294210366498
iteration : 1910
train acc:  0.7421875
train loss:  0.5458275079727173
train gradient:  0.11509076161625025
iteration : 1911
train acc:  0.6796875
train loss:  0.5480577945709229
train gradient:  0.1696368893339838
iteration : 1912
train acc:  0.7265625
train loss:  0.5450360774993896
train gradient:  0.16008471969467014
iteration : 1913
train acc:  0.671875
train loss:  0.599886417388916
train gradient:  0.19866385932113137
iteration : 1914
train acc:  0.6484375
train loss:  0.5710796117782593
train gradient:  0.15263796318651734
iteration : 1915
train acc:  0.7421875
train loss:  0.5370199084281921
train gradient:  0.182978519173763
iteration : 1916
train acc:  0.7421875
train loss:  0.523387610912323
train gradient:  0.16700255794771574
iteration : 1917
train acc:  0.6640625
train loss:  0.5892219543457031
train gradient:  0.18388169206860255
iteration : 1918
train acc:  0.71875
train loss:  0.52678382396698
train gradient:  0.14202644794549646
iteration : 1919
train acc:  0.671875
train loss:  0.5470676422119141
train gradient:  0.1103184001760692
iteration : 1920
train acc:  0.71875
train loss:  0.4930908679962158
train gradient:  0.1782158317516359
iteration : 1921
train acc:  0.6875
train loss:  0.5612540245056152
train gradient:  0.14201241651049387
iteration : 1922
train acc:  0.75
train loss:  0.505833089351654
train gradient:  0.13186809515598197
iteration : 1923
train acc:  0.6953125
train loss:  0.58930903673172
train gradient:  0.19008179501915734
iteration : 1924
train acc:  0.71875
train loss:  0.5513348579406738
train gradient:  0.1936109701260265
iteration : 1925
train acc:  0.734375
train loss:  0.5138417482376099
train gradient:  0.10251967122248101
iteration : 1926
train acc:  0.6953125
train loss:  0.5539663434028625
train gradient:  0.1613165988763161
iteration : 1927
train acc:  0.7265625
train loss:  0.5120497941970825
train gradient:  0.1664115245971497
iteration : 1928
train acc:  0.796875
train loss:  0.48337599635124207
train gradient:  0.11811452404416001
iteration : 1929
train acc:  0.734375
train loss:  0.5547723174095154
train gradient:  0.15891528710766395
iteration : 1930
train acc:  0.734375
train loss:  0.4596971869468689
train gradient:  0.11308385519468916
iteration : 1931
train acc:  0.734375
train loss:  0.5088468790054321
train gradient:  0.13492212516786048
iteration : 1932
train acc:  0.6484375
train loss:  0.5975830554962158
train gradient:  0.22922767178695463
iteration : 1933
train acc:  0.7265625
train loss:  0.5883464813232422
train gradient:  0.24979194657854714
iteration : 1934
train acc:  0.703125
train loss:  0.518713653087616
train gradient:  0.15404888564846742
iteration : 1935
train acc:  0.625
train loss:  0.6025362014770508
train gradient:  0.1793407137279045
iteration : 1936
train acc:  0.6953125
train loss:  0.5572943687438965
train gradient:  0.1476901851813736
iteration : 1937
train acc:  0.734375
train loss:  0.510445237159729
train gradient:  0.14395113987713487
iteration : 1938
train acc:  0.671875
train loss:  0.568398118019104
train gradient:  0.17192980218832798
iteration : 1939
train acc:  0.7421875
train loss:  0.5318586826324463
train gradient:  0.20715628725638818
iteration : 1940
train acc:  0.6796875
train loss:  0.5284646153450012
train gradient:  0.13725267120471654
iteration : 1941
train acc:  0.71875
train loss:  0.5246431827545166
train gradient:  0.16319559645509413
iteration : 1942
train acc:  0.7265625
train loss:  0.5139551162719727
train gradient:  0.13655700059468062
iteration : 1943
train acc:  0.734375
train loss:  0.5076704621315002
train gradient:  0.1554984603830411
iteration : 1944
train acc:  0.7578125
train loss:  0.5079197883605957
train gradient:  0.14317253500171756
iteration : 1945
train acc:  0.6875
train loss:  0.5816433429718018
train gradient:  0.15519104338589307
iteration : 1946
train acc:  0.640625
train loss:  0.5855447053909302
train gradient:  0.16613886749170237
iteration : 1947
train acc:  0.7109375
train loss:  0.5705857276916504
train gradient:  0.14856576705489233
iteration : 1948
train acc:  0.6875
train loss:  0.5583120584487915
train gradient:  0.12332641233503477
iteration : 1949
train acc:  0.7265625
train loss:  0.5289602279663086
train gradient:  0.14250706591989987
iteration : 1950
train acc:  0.7421875
train loss:  0.5154955387115479
train gradient:  0.13817155988757426
iteration : 1951
train acc:  0.6875
train loss:  0.5840283036231995
train gradient:  0.1691551923470369
iteration : 1952
train acc:  0.703125
train loss:  0.5847205519676208
train gradient:  0.25901905664798347
iteration : 1953
train acc:  0.6640625
train loss:  0.6129772663116455
train gradient:  0.20880528487419406
iteration : 1954
train acc:  0.6796875
train loss:  0.5650637149810791
train gradient:  0.13401536022482263
iteration : 1955
train acc:  0.6796875
train loss:  0.570919394493103
train gradient:  0.16903695954548897
iteration : 1956
train acc:  0.75
train loss:  0.4967416226863861
train gradient:  0.13645710150508095
iteration : 1957
train acc:  0.7265625
train loss:  0.5264654755592346
train gradient:  0.21820630052042847
iteration : 1958
train acc:  0.6796875
train loss:  0.5634024739265442
train gradient:  0.19968447262948102
iteration : 1959
train acc:  0.71875
train loss:  0.561998188495636
train gradient:  0.14018397663912613
iteration : 1960
train acc:  0.65625
train loss:  0.5662478804588318
train gradient:  0.17367900912906234
iteration : 1961
train acc:  0.640625
train loss:  0.6142837405204773
train gradient:  0.20467420975794398
iteration : 1962
train acc:  0.625
train loss:  0.596555233001709
train gradient:  0.18074555943417564
iteration : 1963
train acc:  0.7421875
train loss:  0.5082505345344543
train gradient:  0.1370954511936333
iteration : 1964
train acc:  0.6875
train loss:  0.5666924715042114
train gradient:  0.12841798198197824
iteration : 1965
train acc:  0.734375
train loss:  0.5028555989265442
train gradient:  0.13731095704573265
iteration : 1966
train acc:  0.7265625
train loss:  0.5667915940284729
train gradient:  0.2152766985208852
iteration : 1967
train acc:  0.7109375
train loss:  0.5659364461898804
train gradient:  0.19200471246099976
iteration : 1968
train acc:  0.7421875
train loss:  0.5038236975669861
train gradient:  0.15138284752566739
iteration : 1969
train acc:  0.6953125
train loss:  0.5661349892616272
train gradient:  0.359000117619705
iteration : 1970
train acc:  0.8125
train loss:  0.44935017824172974
train gradient:  0.12289361591695967
iteration : 1971
train acc:  0.7421875
train loss:  0.5111408233642578
train gradient:  0.1420733476774309
iteration : 1972
train acc:  0.7265625
train loss:  0.5111944675445557
train gradient:  0.13635011159885063
iteration : 1973
train acc:  0.7734375
train loss:  0.5039492845535278
train gradient:  0.14501327241408318
iteration : 1974
train acc:  0.7734375
train loss:  0.47536760568618774
train gradient:  0.13393743905346733
iteration : 1975
train acc:  0.6328125
train loss:  0.6064409017562866
train gradient:  0.2269344141744331
iteration : 1976
train acc:  0.671875
train loss:  0.6170127987861633
train gradient:  0.17997171983778476
iteration : 1977
train acc:  0.703125
train loss:  0.5610768795013428
train gradient:  0.14652393300411293
iteration : 1978
train acc:  0.6953125
train loss:  0.5853604674339294
train gradient:  0.14966111644137475
iteration : 1979
train acc:  0.6875
train loss:  0.5942433476448059
train gradient:  0.19774733229180125
iteration : 1980
train acc:  0.6484375
train loss:  0.5646950006484985
train gradient:  0.17521888392618196
iteration : 1981
train acc:  0.65625
train loss:  0.5684475898742676
train gradient:  0.23009637266634578
iteration : 1982
train acc:  0.7109375
train loss:  0.5430329442024231
train gradient:  0.12291250483224177
iteration : 1983
train acc:  0.7265625
train loss:  0.5377582311630249
train gradient:  0.17330076962800428
iteration : 1984
train acc:  0.765625
train loss:  0.500262975692749
train gradient:  0.17188308323980794
iteration : 1985
train acc:  0.703125
train loss:  0.553291916847229
train gradient:  0.14216794689256368
iteration : 1986
train acc:  0.7265625
train loss:  0.5645926594734192
train gradient:  0.20295060964872924
iteration : 1987
train acc:  0.734375
train loss:  0.5276198983192444
train gradient:  0.13548347007022282
iteration : 1988
train acc:  0.7265625
train loss:  0.5407906770706177
train gradient:  0.13881469772657903
iteration : 1989
train acc:  0.7109375
train loss:  0.5474950075149536
train gradient:  0.16488418082924355
iteration : 1990
train acc:  0.6953125
train loss:  0.5624127984046936
train gradient:  0.2504196053945883
iteration : 1991
train acc:  0.7578125
train loss:  0.4896031618118286
train gradient:  0.11422200320445133
iteration : 1992
train acc:  0.796875
train loss:  0.45302778482437134
train gradient:  0.1203180948100134
iteration : 1993
train acc:  0.6796875
train loss:  0.5708305835723877
train gradient:  0.15020184693843003
iteration : 1994
train acc:  0.671875
train loss:  0.5515268445014954
train gradient:  0.16195709523274082
iteration : 1995
train acc:  0.703125
train loss:  0.5357755422592163
train gradient:  0.14866424982708368
iteration : 1996
train acc:  0.703125
train loss:  0.6014947891235352
train gradient:  0.18460297925470728
iteration : 1997
train acc:  0.765625
train loss:  0.5194352269172668
train gradient:  0.12445258643686151
iteration : 1998
train acc:  0.6875
train loss:  0.5374414324760437
train gradient:  0.18876298899831712
iteration : 1999
train acc:  0.671875
train loss:  0.6088221073150635
train gradient:  0.18557387892649602
iteration : 2000
train acc:  0.71875
train loss:  0.5319955348968506
train gradient:  0.161250022871342
iteration : 2001
train acc:  0.7421875
train loss:  0.46894413232803345
train gradient:  0.15734408651563853
iteration : 2002
train acc:  0.671875
train loss:  0.5939159393310547
train gradient:  0.16676875667437183
iteration : 2003
train acc:  0.75
train loss:  0.5360841751098633
train gradient:  0.11307429552076549
iteration : 2004
train acc:  0.6875
train loss:  0.5575659871101379
train gradient:  0.15057862519226334
iteration : 2005
train acc:  0.7421875
train loss:  0.5203497409820557
train gradient:  0.1618936455673379
iteration : 2006
train acc:  0.7265625
train loss:  0.5179017782211304
train gradient:  0.14224755524837762
iteration : 2007
train acc:  0.7734375
train loss:  0.48442938923835754
train gradient:  0.15827456535884396
iteration : 2008
train acc:  0.6484375
train loss:  0.5985949039459229
train gradient:  0.17410432578954554
iteration : 2009
train acc:  0.6171875
train loss:  0.694952130317688
train gradient:  0.2378641044851701
iteration : 2010
train acc:  0.7265625
train loss:  0.5110342502593994
train gradient:  0.14187161996424488
iteration : 2011
train acc:  0.6953125
train loss:  0.5832604169845581
train gradient:  0.1636858266500399
iteration : 2012
train acc:  0.7421875
train loss:  0.5089514255523682
train gradient:  0.15147531959647026
iteration : 2013
train acc:  0.7421875
train loss:  0.5078721642494202
train gradient:  0.12598776508922083
iteration : 2014
train acc:  0.71875
train loss:  0.5728853940963745
train gradient:  0.18356797053067278
iteration : 2015
train acc:  0.671875
train loss:  0.5726875066757202
train gradient:  0.21824133172914012
iteration : 2016
train acc:  0.7265625
train loss:  0.5384590029716492
train gradient:  0.11524005401805898
iteration : 2017
train acc:  0.7421875
train loss:  0.5495362281799316
train gradient:  0.2399084032980638
iteration : 2018
train acc:  0.6875
train loss:  0.558894693851471
train gradient:  0.13622591666967082
iteration : 2019
train acc:  0.703125
train loss:  0.6042997241020203
train gradient:  0.1585858497505538
iteration : 2020
train acc:  0.6171875
train loss:  0.627289354801178
train gradient:  0.21482094237228666
iteration : 2021
train acc:  0.734375
train loss:  0.5547822713851929
train gradient:  0.21902169994005877
iteration : 2022
train acc:  0.7734375
train loss:  0.48673057556152344
train gradient:  0.16522561519624526
iteration : 2023
train acc:  0.703125
train loss:  0.5157797336578369
train gradient:  0.12914459174923315
iteration : 2024
train acc:  0.75
train loss:  0.5362329483032227
train gradient:  0.14781758323983207
iteration : 2025
train acc:  0.734375
train loss:  0.5079398155212402
train gradient:  0.15214063371856418
iteration : 2026
train acc:  0.6796875
train loss:  0.5695428848266602
train gradient:  0.18004375066299186
iteration : 2027
train acc:  0.671875
train loss:  0.5486403703689575
train gradient:  0.21879260893668923
iteration : 2028
train acc:  0.71875
train loss:  0.5079131126403809
train gradient:  0.13958677906143335
iteration : 2029
train acc:  0.6640625
train loss:  0.590017557144165
train gradient:  0.18550232891012852
iteration : 2030
train acc:  0.71875
train loss:  0.5339021682739258
train gradient:  0.1615727882088262
iteration : 2031
train acc:  0.65625
train loss:  0.6128875613212585
train gradient:  0.16766457614165792
iteration : 2032
train acc:  0.765625
train loss:  0.5057837963104248
train gradient:  0.14289609375199858
iteration : 2033
train acc:  0.7421875
train loss:  0.5199415683746338
train gradient:  0.13919467754364073
iteration : 2034
train acc:  0.6953125
train loss:  0.5395135879516602
train gradient:  0.1203813347292527
iteration : 2035
train acc:  0.71875
train loss:  0.5015554428100586
train gradient:  0.16116733793680543
iteration : 2036
train acc:  0.71875
train loss:  0.5260601043701172
train gradient:  0.18892956305336794
iteration : 2037
train acc:  0.65625
train loss:  0.5991536974906921
train gradient:  0.19350970859513972
iteration : 2038
train acc:  0.7421875
train loss:  0.5281463861465454
train gradient:  0.17458657283767415
iteration : 2039
train acc:  0.671875
train loss:  0.5468924641609192
train gradient:  0.1357350907234827
iteration : 2040
train acc:  0.65625
train loss:  0.5648481845855713
train gradient:  0.15659552606622268
iteration : 2041
train acc:  0.7109375
train loss:  0.5322461128234863
train gradient:  0.15462236266240215
iteration : 2042
train acc:  0.796875
train loss:  0.48769551515579224
train gradient:  0.11911231391295374
iteration : 2043
train acc:  0.7109375
train loss:  0.5337347388267517
train gradient:  0.11099266122447807
iteration : 2044
train acc:  0.7421875
train loss:  0.4855634272098541
train gradient:  0.16907065760515963
iteration : 2045
train acc:  0.6640625
train loss:  0.5862547755241394
train gradient:  0.17374603228043117
iteration : 2046
train acc:  0.734375
train loss:  0.4928671419620514
train gradient:  0.13174393430679385
iteration : 2047
train acc:  0.65625
train loss:  0.6208894848823547
train gradient:  0.25022976054467283
iteration : 2048
train acc:  0.6796875
train loss:  0.5253586769104004
train gradient:  0.1398525764228694
iteration : 2049
train acc:  0.6875
train loss:  0.5474580526351929
train gradient:  0.17412587196093432
iteration : 2050
train acc:  0.71875
train loss:  0.5380879640579224
train gradient:  0.12980397186342943
iteration : 2051
train acc:  0.703125
train loss:  0.5444710850715637
train gradient:  0.1620521955871777
iteration : 2052
train acc:  0.734375
train loss:  0.5120502710342407
train gradient:  0.12935566857378306
iteration : 2053
train acc:  0.609375
train loss:  0.5942025780677795
train gradient:  0.17153257317034198
iteration : 2054
train acc:  0.7578125
train loss:  0.5373917818069458
train gradient:  0.19477380430520455
iteration : 2055
train acc:  0.6953125
train loss:  0.5346778631210327
train gradient:  0.1320444135204532
iteration : 2056
train acc:  0.7109375
train loss:  0.5033851265907288
train gradient:  0.12476789841520648
iteration : 2057
train acc:  0.7421875
train loss:  0.5051366090774536
train gradient:  0.1261108711374521
iteration : 2058
train acc:  0.6796875
train loss:  0.5750443935394287
train gradient:  0.22605202140306419
iteration : 2059
train acc:  0.703125
train loss:  0.5312588810920715
train gradient:  0.1304676890441266
iteration : 2060
train acc:  0.7265625
train loss:  0.50670325756073
train gradient:  0.2377945075218451
iteration : 2061
train acc:  0.765625
train loss:  0.5013562440872192
train gradient:  0.12586768243653562
iteration : 2062
train acc:  0.7421875
train loss:  0.5316939949989319
train gradient:  0.14898109510305563
iteration : 2063
train acc:  0.7578125
train loss:  0.5002105832099915
train gradient:  0.13398751030241224
iteration : 2064
train acc:  0.703125
train loss:  0.5595164895057678
train gradient:  0.14672392409922974
iteration : 2065
train acc:  0.6796875
train loss:  0.5976910591125488
train gradient:  0.2394127398321554
iteration : 2066
train acc:  0.671875
train loss:  0.6190672516822815
train gradient:  0.20669985411488506
iteration : 2067
train acc:  0.6796875
train loss:  0.5622701644897461
train gradient:  0.18318457344306435
iteration : 2068
train acc:  0.65625
train loss:  0.5724510550498962
train gradient:  0.18621517814975913
iteration : 2069
train acc:  0.6796875
train loss:  0.5027326345443726
train gradient:  0.15825024655346775
iteration : 2070
train acc:  0.6796875
train loss:  0.6081020832061768
train gradient:  0.21907901408780672
iteration : 2071
train acc:  0.7265625
train loss:  0.4614628553390503
train gradient:  0.1327057514437403
iteration : 2072
train acc:  0.6640625
train loss:  0.5700289607048035
train gradient:  0.24207929575717996
iteration : 2073
train acc:  0.734375
train loss:  0.5465035438537598
train gradient:  0.2141851638725586
iteration : 2074
train acc:  0.671875
train loss:  0.5434029698371887
train gradient:  0.1667684995187661
iteration : 2075
train acc:  0.6875
train loss:  0.5251964926719666
train gradient:  0.16042859480467475
iteration : 2076
train acc:  0.7109375
train loss:  0.6010593175888062
train gradient:  0.16281076978399323
iteration : 2077
train acc:  0.6640625
train loss:  0.5831853747367859
train gradient:  0.1814203301460039
iteration : 2078
train acc:  0.7265625
train loss:  0.5480663180351257
train gradient:  0.15326573579761693
iteration : 2079
train acc:  0.71875
train loss:  0.5673363208770752
train gradient:  0.18985636528178118
iteration : 2080
train acc:  0.7265625
train loss:  0.5549825429916382
train gradient:  0.1933882438291543
iteration : 2081
train acc:  0.765625
train loss:  0.5063676834106445
train gradient:  0.14366182361671495
iteration : 2082
train acc:  0.6875
train loss:  0.5669001340866089
train gradient:  0.13717645106893223
iteration : 2083
train acc:  0.8046875
train loss:  0.4855589270591736
train gradient:  0.15025039804201618
iteration : 2084
train acc:  0.6953125
train loss:  0.5967634320259094
train gradient:  0.1660973908029541
iteration : 2085
train acc:  0.671875
train loss:  0.5242612361907959
train gradient:  0.21757164431148523
iteration : 2086
train acc:  0.703125
train loss:  0.5524940490722656
train gradient:  0.1628532531795221
iteration : 2087
train acc:  0.7265625
train loss:  0.5442386865615845
train gradient:  0.1495560514052723
iteration : 2088
train acc:  0.6875
train loss:  0.6030523180961609
train gradient:  0.16663527166610437
iteration : 2089
train acc:  0.7109375
train loss:  0.560921847820282
train gradient:  0.19145231635784085
iteration : 2090
train acc:  0.6953125
train loss:  0.5091679096221924
train gradient:  0.13099775148878612
iteration : 2091
train acc:  0.671875
train loss:  0.5704177618026733
train gradient:  0.170426431896339
iteration : 2092
train acc:  0.6875
train loss:  0.5488985776901245
train gradient:  0.16581779355849297
iteration : 2093
train acc:  0.7109375
train loss:  0.5061386823654175
train gradient:  0.1402207184765424
iteration : 2094
train acc:  0.6640625
train loss:  0.5756307244300842
train gradient:  0.17543554351173335
iteration : 2095
train acc:  0.7421875
train loss:  0.49587273597717285
train gradient:  0.11926873126956954
iteration : 2096
train acc:  0.7578125
train loss:  0.49692434072494507
train gradient:  0.14708182123718128
iteration : 2097
train acc:  0.765625
train loss:  0.4922342002391815
train gradient:  0.14775728058583837
iteration : 2098
train acc:  0.71875
train loss:  0.556627631187439
train gradient:  0.14528298159568986
iteration : 2099
train acc:  0.6796875
train loss:  0.5679222345352173
train gradient:  0.15340599500642882
iteration : 2100
train acc:  0.78125
train loss:  0.4707273542881012
train gradient:  0.15176290983773305
iteration : 2101
train acc:  0.734375
train loss:  0.5316623449325562
train gradient:  0.1368952079294363
iteration : 2102
train acc:  0.7265625
train loss:  0.5265102386474609
train gradient:  0.16016158693552113
iteration : 2103
train acc:  0.71875
train loss:  0.5607881546020508
train gradient:  0.24089130721722535
iteration : 2104
train acc:  0.7421875
train loss:  0.5101330280303955
train gradient:  0.14512571609772723
iteration : 2105
train acc:  0.734375
train loss:  0.54254549741745
train gradient:  0.162059649206158
iteration : 2106
train acc:  0.6953125
train loss:  0.5840162038803101
train gradient:  0.21196402584722496
iteration : 2107
train acc:  0.6484375
train loss:  0.5555016398429871
train gradient:  0.20182203426140652
iteration : 2108
train acc:  0.7578125
train loss:  0.4968079626560211
train gradient:  0.1684160899175161
iteration : 2109
train acc:  0.6953125
train loss:  0.619652509689331
train gradient:  0.24614600080792914
iteration : 2110
train acc:  0.7578125
train loss:  0.518710732460022
train gradient:  0.14437648453544855
iteration : 2111
train acc:  0.625
train loss:  0.6413086652755737
train gradient:  0.22725234473644607
iteration : 2112
train acc:  0.71875
train loss:  0.5387195348739624
train gradient:  0.25632574928704493
iteration : 2113
train acc:  0.6953125
train loss:  0.5355861783027649
train gradient:  0.142339541040291
iteration : 2114
train acc:  0.6640625
train loss:  0.5592505931854248
train gradient:  0.13689501235630763
iteration : 2115
train acc:  0.6796875
train loss:  0.5453217029571533
train gradient:  0.14151477771736395
iteration : 2116
train acc:  0.703125
train loss:  0.5511388778686523
train gradient:  0.15877660053057818
iteration : 2117
train acc:  0.6640625
train loss:  0.5854096412658691
train gradient:  0.23795818713575084
iteration : 2118
train acc:  0.71875
train loss:  0.48401618003845215
train gradient:  0.12162917052635341
iteration : 2119
train acc:  0.671875
train loss:  0.5681445598602295
train gradient:  0.2253507123274897
iteration : 2120
train acc:  0.703125
train loss:  0.5711119174957275
train gradient:  0.17370593652122185
iteration : 2121
train acc:  0.7109375
train loss:  0.5272362232208252
train gradient:  0.1880430569552674
iteration : 2122
train acc:  0.65625
train loss:  0.5556470155715942
train gradient:  0.1633770101674193
iteration : 2123
train acc:  0.671875
train loss:  0.540389895439148
train gradient:  0.20959282628929846
iteration : 2124
train acc:  0.6328125
train loss:  0.6565598249435425
train gradient:  0.27673596988616556
iteration : 2125
train acc:  0.7109375
train loss:  0.5346769094467163
train gradient:  0.16598198332822864
iteration : 2126
train acc:  0.7890625
train loss:  0.48085349798202515
train gradient:  0.14726870051963148
iteration : 2127
train acc:  0.6796875
train loss:  0.538108229637146
train gradient:  0.15848119746976308
iteration : 2128
train acc:  0.7265625
train loss:  0.5277080535888672
train gradient:  0.15129184748202454
iteration : 2129
train acc:  0.7265625
train loss:  0.5303978323936462
train gradient:  0.1434368657311838
iteration : 2130
train acc:  0.6875
train loss:  0.5594353079795837
train gradient:  0.2211103978000765
iteration : 2131
train acc:  0.7109375
train loss:  0.5262728333473206
train gradient:  0.17238526765486042
iteration : 2132
train acc:  0.671875
train loss:  0.5400752425193787
train gradient:  0.15428781721051682
iteration : 2133
train acc:  0.7265625
train loss:  0.5280761122703552
train gradient:  0.14596043593888094
iteration : 2134
train acc:  0.7421875
train loss:  0.4967656433582306
train gradient:  0.13964013311439982
iteration : 2135
train acc:  0.703125
train loss:  0.5842398405075073
train gradient:  0.18541410363453956
iteration : 2136
train acc:  0.7109375
train loss:  0.5220198631286621
train gradient:  0.13877287648029207
iteration : 2137
train acc:  0.75
train loss:  0.5084302425384521
train gradient:  0.1392193466589644
iteration : 2138
train acc:  0.7578125
train loss:  0.5065138936042786
train gradient:  0.11164221749669934
iteration : 2139
train acc:  0.65625
train loss:  0.5685968399047852
train gradient:  0.204343959535282
iteration : 2140
train acc:  0.7265625
train loss:  0.5035883188247681
train gradient:  0.1627581272844007
iteration : 2141
train acc:  0.703125
train loss:  0.5123597383499146
train gradient:  0.14095664989098644
iteration : 2142
train acc:  0.765625
train loss:  0.5043448805809021
train gradient:  0.13779676699136997
iteration : 2143
train acc:  0.7578125
train loss:  0.5027040839195251
train gradient:  0.14851855058675167
iteration : 2144
train acc:  0.7265625
train loss:  0.5274856090545654
train gradient:  0.1393489093373495
iteration : 2145
train acc:  0.6875
train loss:  0.6039153933525085
train gradient:  0.16843330481675678
iteration : 2146
train acc:  0.7578125
train loss:  0.5248401761054993
train gradient:  0.1780938273862997
iteration : 2147
train acc:  0.765625
train loss:  0.483095645904541
train gradient:  0.13321562422535813
iteration : 2148
train acc:  0.7265625
train loss:  0.507379412651062
train gradient:  0.12111360832817929
iteration : 2149
train acc:  0.7265625
train loss:  0.540485680103302
train gradient:  0.14699017983163312
iteration : 2150
train acc:  0.703125
train loss:  0.5502293109893799
train gradient:  0.17859080334292726
iteration : 2151
train acc:  0.7578125
train loss:  0.5183342099189758
train gradient:  0.12783279717878218
iteration : 2152
train acc:  0.7265625
train loss:  0.5050550699234009
train gradient:  0.14340602748474945
iteration : 2153
train acc:  0.7421875
train loss:  0.5275187492370605
train gradient:  0.16849744193251553
iteration : 2154
train acc:  0.6875
train loss:  0.5549412369728088
train gradient:  0.14164240948647566
iteration : 2155
train acc:  0.75
train loss:  0.5210362076759338
train gradient:  0.14077552772317176
iteration : 2156
train acc:  0.7265625
train loss:  0.5436978340148926
train gradient:  0.14443801438975218
iteration : 2157
train acc:  0.7890625
train loss:  0.47074955701828003
train gradient:  0.10619877732643833
iteration : 2158
train acc:  0.7421875
train loss:  0.5580471754074097
train gradient:  0.22521933736634864
iteration : 2159
train acc:  0.75
train loss:  0.5175652503967285
train gradient:  0.1288049253966323
iteration : 2160
train acc:  0.7265625
train loss:  0.5183724164962769
train gradient:  0.14021553454463945
iteration : 2161
train acc:  0.734375
train loss:  0.5127426981925964
train gradient:  0.14858743869313923
iteration : 2162
train acc:  0.78125
train loss:  0.4604872465133667
train gradient:  0.1445999123269614
iteration : 2163
train acc:  0.6171875
train loss:  0.6429256796836853
train gradient:  0.25558597224916546
iteration : 2164
train acc:  0.7421875
train loss:  0.5561550855636597
train gradient:  0.16994991951042054
iteration : 2165
train acc:  0.71875
train loss:  0.5497986078262329
train gradient:  0.1539659286051765
iteration : 2166
train acc:  0.71875
train loss:  0.5176607966423035
train gradient:  0.14246871818198958
iteration : 2167
train acc:  0.734375
train loss:  0.49490371346473694
train gradient:  0.14449943013583832
iteration : 2168
train acc:  0.7109375
train loss:  0.5741081237792969
train gradient:  0.1626566879638007
iteration : 2169
train acc:  0.703125
train loss:  0.4906265139579773
train gradient:  0.13285908103823668
iteration : 2170
train acc:  0.7421875
train loss:  0.5426446199417114
train gradient:  0.15540127560636505
iteration : 2171
train acc:  0.828125
train loss:  0.44881585240364075
train gradient:  0.12075272607317485
iteration : 2172
train acc:  0.8046875
train loss:  0.4779423475265503
train gradient:  0.13224660779645003
iteration : 2173
train acc:  0.7734375
train loss:  0.4710903465747833
train gradient:  0.11602558513135401
iteration : 2174
train acc:  0.7265625
train loss:  0.5884562730789185
train gradient:  0.20847612289706596
iteration : 2175
train acc:  0.734375
train loss:  0.528998076915741
train gradient:  0.16570104756088763
iteration : 2176
train acc:  0.6796875
train loss:  0.5554811954498291
train gradient:  0.1932456497583343
iteration : 2177
train acc:  0.7734375
train loss:  0.49123045802116394
train gradient:  0.1578757706237705
iteration : 2178
train acc:  0.6796875
train loss:  0.5283701419830322
train gradient:  0.14986115268420475
iteration : 2179
train acc:  0.765625
train loss:  0.4738576114177704
train gradient:  0.11127944561538552
iteration : 2180
train acc:  0.71875
train loss:  0.5185455083847046
train gradient:  0.18114526901190992
iteration : 2181
train acc:  0.703125
train loss:  0.5662593245506287
train gradient:  0.17502243629393904
iteration : 2182
train acc:  0.7109375
train loss:  0.5586941242218018
train gradient:  0.17340213393919213
iteration : 2183
train acc:  0.6640625
train loss:  0.6000877022743225
train gradient:  0.1864237591074359
iteration : 2184
train acc:  0.671875
train loss:  0.5677881240844727
train gradient:  0.1481544295125311
iteration : 2185
train acc:  0.7421875
train loss:  0.5270619988441467
train gradient:  0.16743631968181
iteration : 2186
train acc:  0.71875
train loss:  0.5282828211784363
train gradient:  0.14149791310863175
iteration : 2187
train acc:  0.7109375
train loss:  0.5251684188842773
train gradient:  0.18371937972829488
iteration : 2188
train acc:  0.6875
train loss:  0.5184587240219116
train gradient:  0.1304140702500508
iteration : 2189
train acc:  0.7421875
train loss:  0.4895210862159729
train gradient:  0.1583815032938392
iteration : 2190
train acc:  0.7109375
train loss:  0.5168187618255615
train gradient:  0.1790632050260048
iteration : 2191
train acc:  0.671875
train loss:  0.5863242745399475
train gradient:  0.1999382035230795
iteration : 2192
train acc:  0.765625
train loss:  0.5009302496910095
train gradient:  0.14782182330066862
iteration : 2193
train acc:  0.7265625
train loss:  0.5067413449287415
train gradient:  0.1636580804847892
iteration : 2194
train acc:  0.71875
train loss:  0.5861625671386719
train gradient:  0.16901267589138302
iteration : 2195
train acc:  0.6953125
train loss:  0.5863618850708008
train gradient:  0.21664808112753361
iteration : 2196
train acc:  0.6953125
train loss:  0.5338740348815918
train gradient:  0.17269067446849937
iteration : 2197
train acc:  0.703125
train loss:  0.5445699691772461
train gradient:  0.18239304678752088
iteration : 2198
train acc:  0.78125
train loss:  0.5004002451896667
train gradient:  0.21339063427409788
iteration : 2199
train acc:  0.75
train loss:  0.5377164483070374
train gradient:  0.1606065093020606
iteration : 2200
train acc:  0.6875
train loss:  0.536436140537262
train gradient:  0.1449443358971269
iteration : 2201
train acc:  0.7109375
train loss:  0.5445681810379028
train gradient:  0.1564747483865741
iteration : 2202
train acc:  0.6875
train loss:  0.5650684237480164
train gradient:  0.16280171113199535
iteration : 2203
train acc:  0.671875
train loss:  0.5967206954956055
train gradient:  0.20735620266289406
iteration : 2204
train acc:  0.75
train loss:  0.525473952293396
train gradient:  0.1491770950326463
iteration : 2205
train acc:  0.7109375
train loss:  0.5287129878997803
train gradient:  0.24404446733172105
iteration : 2206
train acc:  0.625
train loss:  0.6407034397125244
train gradient:  0.22247543975399384
iteration : 2207
train acc:  0.7265625
train loss:  0.5243873596191406
train gradient:  0.1370851707179625
iteration : 2208
train acc:  0.6953125
train loss:  0.5892524719238281
train gradient:  0.19207014873829825
iteration : 2209
train acc:  0.6875
train loss:  0.5400475263595581
train gradient:  0.1406839652020321
iteration : 2210
train acc:  0.7265625
train loss:  0.4918186664581299
train gradient:  0.16989464793941372
iteration : 2211
train acc:  0.6953125
train loss:  0.5336648225784302
train gradient:  0.25026712448125255
iteration : 2212
train acc:  0.671875
train loss:  0.5697528719902039
train gradient:  0.17535728082225088
iteration : 2213
train acc:  0.6796875
train loss:  0.5915126204490662
train gradient:  0.3119633634947231
iteration : 2214
train acc:  0.6953125
train loss:  0.5417167544364929
train gradient:  0.1700510322820448
iteration : 2215
train acc:  0.6796875
train loss:  0.54582679271698
train gradient:  0.16518489581695964
iteration : 2216
train acc:  0.71875
train loss:  0.5216704607009888
train gradient:  0.1535053393326479
iteration : 2217
train acc:  0.7578125
train loss:  0.5256633758544922
train gradient:  0.16224827721918078
iteration : 2218
train acc:  0.6953125
train loss:  0.5983529090881348
train gradient:  0.2339915437817917
iteration : 2219
train acc:  0.8046875
train loss:  0.4439162611961365
train gradient:  0.10647588115943113
iteration : 2220
train acc:  0.6640625
train loss:  0.5887312889099121
train gradient:  0.3035969168387065
iteration : 2221
train acc:  0.6796875
train loss:  0.5590184926986694
train gradient:  0.19040613510314697
iteration : 2222
train acc:  0.7734375
train loss:  0.47830942273139954
train gradient:  0.13725153327032458
iteration : 2223
train acc:  0.703125
train loss:  0.5196807980537415
train gradient:  0.18381645201359792
iteration : 2224
train acc:  0.6640625
train loss:  0.584348738193512
train gradient:  0.19443875955945547
iteration : 2225
train acc:  0.78125
train loss:  0.49060994386672974
train gradient:  0.13486440627435872
iteration : 2226
train acc:  0.7890625
train loss:  0.5017542243003845
train gradient:  0.23902766327460323
iteration : 2227
train acc:  0.703125
train loss:  0.5037450790405273
train gradient:  0.1539490493243033
iteration : 2228
train acc:  0.609375
train loss:  0.6104921698570251
train gradient:  0.2284804819268682
iteration : 2229
train acc:  0.7265625
train loss:  0.5385020971298218
train gradient:  0.15374893325554956
iteration : 2230
train acc:  0.7890625
train loss:  0.4688599109649658
train gradient:  0.12451749422949299
iteration : 2231
train acc:  0.78125
train loss:  0.49173417687416077
train gradient:  0.12516645461622317
iteration : 2232
train acc:  0.734375
train loss:  0.4998623728752136
train gradient:  0.1677293549614336
iteration : 2233
train acc:  0.671875
train loss:  0.5735752582550049
train gradient:  0.15554903343612486
iteration : 2234
train acc:  0.65625
train loss:  0.5538715124130249
train gradient:  0.1840874990826557
iteration : 2235
train acc:  0.796875
train loss:  0.4736221432685852
train gradient:  0.12307094274704052
iteration : 2236
train acc:  0.6953125
train loss:  0.5743292570114136
train gradient:  0.1864950456052013
iteration : 2237
train acc:  0.7109375
train loss:  0.5359986424446106
train gradient:  0.13875752142113393
iteration : 2238
train acc:  0.6640625
train loss:  0.6022607684135437
train gradient:  0.19437063908217672
iteration : 2239
train acc:  0.671875
train loss:  0.5605121850967407
train gradient:  0.21341602030190365
iteration : 2240
train acc:  0.71875
train loss:  0.5789257287979126
train gradient:  0.19702966727303303
iteration : 2241
train acc:  0.71875
train loss:  0.5679583549499512
train gradient:  0.16188701086914653
iteration : 2242
train acc:  0.7578125
train loss:  0.5059052109718323
train gradient:  0.1733106183388291
iteration : 2243
train acc:  0.7109375
train loss:  0.5039948225021362
train gradient:  0.18591546741874948
iteration : 2244
train acc:  0.7109375
train loss:  0.5639768838882446
train gradient:  0.26599405283930766
iteration : 2245
train acc:  0.6875
train loss:  0.553045392036438
train gradient:  0.17014965119807202
iteration : 2246
train acc:  0.75
train loss:  0.4835571348667145
train gradient:  0.13121189657251522
iteration : 2247
train acc:  0.7109375
train loss:  0.5234112739562988
train gradient:  0.15439923045192655
iteration : 2248
train acc:  0.7109375
train loss:  0.5071612596511841
train gradient:  0.15149714895013094
iteration : 2249
train acc:  0.6953125
train loss:  0.5633516311645508
train gradient:  0.18429866162280972
iteration : 2250
train acc:  0.609375
train loss:  0.626868486404419
train gradient:  0.30780735592723757
iteration : 2251
train acc:  0.8046875
train loss:  0.46671080589294434
train gradient:  0.1452649268300702
iteration : 2252
train acc:  0.7578125
train loss:  0.5128918886184692
train gradient:  0.16779796656332832
iteration : 2253
train acc:  0.7109375
train loss:  0.5332660675048828
train gradient:  0.19770460815949314
iteration : 2254
train acc:  0.625
train loss:  0.6242958307266235
train gradient:  0.19838294770230297
iteration : 2255
train acc:  0.7734375
train loss:  0.4904562830924988
train gradient:  0.1310567237272599
iteration : 2256
train acc:  0.765625
train loss:  0.4863205850124359
train gradient:  0.15894843625674837
iteration : 2257
train acc:  0.7109375
train loss:  0.5546237230300903
train gradient:  0.16042557185969
iteration : 2258
train acc:  0.7265625
train loss:  0.5030907988548279
train gradient:  0.19770372187943505
iteration : 2259
train acc:  0.65625
train loss:  0.600528359413147
train gradient:  0.20811243238602345
iteration : 2260
train acc:  0.7265625
train loss:  0.5357502698898315
train gradient:  0.2116237259462922
iteration : 2261
train acc:  0.65625
train loss:  0.5851999521255493
train gradient:  0.2245029985820967
iteration : 2262
train acc:  0.734375
train loss:  0.49644869565963745
train gradient:  0.14748327766209368
iteration : 2263
train acc:  0.734375
train loss:  0.5101633071899414
train gradient:  0.16605862007223315
iteration : 2264
train acc:  0.703125
train loss:  0.5356181859970093
train gradient:  0.17249103646999062
iteration : 2265
train acc:  0.6484375
train loss:  0.6112076640129089
train gradient:  0.2198350756012346
iteration : 2266
train acc:  0.7109375
train loss:  0.5242437124252319
train gradient:  0.14721556429460966
iteration : 2267
train acc:  0.703125
train loss:  0.5763450264930725
train gradient:  0.15160828570044926
iteration : 2268
train acc:  0.6953125
train loss:  0.5316071510314941
train gradient:  0.18492379913601042
iteration : 2269
train acc:  0.640625
train loss:  0.6421388387680054
train gradient:  0.19098393152478188
iteration : 2270
train acc:  0.65625
train loss:  0.5243578553199768
train gradient:  0.16851428389171963
iteration : 2271
train acc:  0.671875
train loss:  0.5241788625717163
train gradient:  0.15109939621429075
iteration : 2272
train acc:  0.78125
train loss:  0.4990222454071045
train gradient:  0.17601143362135874
iteration : 2273
train acc:  0.6484375
train loss:  0.5970457792282104
train gradient:  0.18616313495554337
iteration : 2274
train acc:  0.7421875
train loss:  0.5145848989486694
train gradient:  0.17198623617892234
iteration : 2275
train acc:  0.7265625
train loss:  0.5084042549133301
train gradient:  0.23797815870015693
iteration : 2276
train acc:  0.75
train loss:  0.5355809926986694
train gradient:  0.20089842637122968
iteration : 2277
train acc:  0.7109375
train loss:  0.5506371259689331
train gradient:  0.19694435639822266
iteration : 2278
train acc:  0.6953125
train loss:  0.6153408288955688
train gradient:  0.32829701170419456
iteration : 2279
train acc:  0.65625
train loss:  0.5533171892166138
train gradient:  0.18047612027000026
iteration : 2280
train acc:  0.671875
train loss:  0.5880987644195557
train gradient:  0.1604865457987047
iteration : 2281
train acc:  0.7578125
train loss:  0.4803767204284668
train gradient:  0.12514519681562564
iteration : 2282
train acc:  0.7734375
train loss:  0.5203103423118591
train gradient:  0.13654780434183025
iteration : 2283
train acc:  0.703125
train loss:  0.541918158531189
train gradient:  0.18460176975406223
iteration : 2284
train acc:  0.6875
train loss:  0.5692538022994995
train gradient:  0.24566420698793967
iteration : 2285
train acc:  0.71875
train loss:  0.5721001029014587
train gradient:  0.20872963273286338
iteration : 2286
train acc:  0.6875
train loss:  0.6629368662834167
train gradient:  0.25641183185789485
iteration : 2287
train acc:  0.7578125
train loss:  0.5160508155822754
train gradient:  0.12503005006300563
iteration : 2288
train acc:  0.765625
train loss:  0.4600028395652771
train gradient:  0.1352957746067837
iteration : 2289
train acc:  0.6953125
train loss:  0.5754191279411316
train gradient:  0.17058528965974293
iteration : 2290
train acc:  0.6484375
train loss:  0.5900450944900513
train gradient:  0.1559012878617194
iteration : 2291
train acc:  0.734375
train loss:  0.5404333472251892
train gradient:  0.21714685412972135
iteration : 2292
train acc:  0.7421875
train loss:  0.5063767433166504
train gradient:  0.2326779719351955
iteration : 2293
train acc:  0.75
train loss:  0.503589928150177
train gradient:  0.1458082250798513
iteration : 2294
train acc:  0.7265625
train loss:  0.49832883477211
train gradient:  0.12754289178568176
iteration : 2295
train acc:  0.7265625
train loss:  0.5241094827651978
train gradient:  0.17772793906809392
iteration : 2296
train acc:  0.65625
train loss:  0.551742434501648
train gradient:  0.14964637740908865
iteration : 2297
train acc:  0.703125
train loss:  0.5328829884529114
train gradient:  0.16629272664874944
iteration : 2298
train acc:  0.6796875
train loss:  0.5376414656639099
train gradient:  0.19836389330202536
iteration : 2299
train acc:  0.7109375
train loss:  0.5370454788208008
train gradient:  0.23800297184228938
iteration : 2300
train acc:  0.71875
train loss:  0.5510299205780029
train gradient:  0.1600480211683911
iteration : 2301
train acc:  0.6796875
train loss:  0.5820236206054688
train gradient:  0.1633211510412511
iteration : 2302
train acc:  0.7734375
train loss:  0.5031775236129761
train gradient:  0.12029249702424476
iteration : 2303
train acc:  0.703125
train loss:  0.5606874227523804
train gradient:  0.18380594278442705
iteration : 2304
train acc:  0.6796875
train loss:  0.5448077321052551
train gradient:  0.17230773774593727
iteration : 2305
train acc:  0.71875
train loss:  0.547696053981781
train gradient:  0.14889058244231415
iteration : 2306
train acc:  0.6953125
train loss:  0.5260769128799438
train gradient:  0.16605842662253603
iteration : 2307
train acc:  0.6015625
train loss:  0.6411529183387756
train gradient:  0.23338760683998055
iteration : 2308
train acc:  0.65625
train loss:  0.5879620313644409
train gradient:  0.15458203569017828
iteration : 2309
train acc:  0.71875
train loss:  0.5597650408744812
train gradient:  0.16186587521729734
iteration : 2310
train acc:  0.7578125
train loss:  0.4969295859336853
train gradient:  0.16631632558169973
iteration : 2311
train acc:  0.7734375
train loss:  0.4665609300136566
train gradient:  0.15963812752923365
iteration : 2312
train acc:  0.75
train loss:  0.5334843397140503
train gradient:  0.1962891144986844
iteration : 2313
train acc:  0.71875
train loss:  0.5075453519821167
train gradient:  0.15511125933860082
iteration : 2314
train acc:  0.7109375
train loss:  0.606174111366272
train gradient:  0.2270042722258963
iteration : 2315
train acc:  0.703125
train loss:  0.5680893659591675
train gradient:  0.22642541922908527
iteration : 2316
train acc:  0.6953125
train loss:  0.5786367058753967
train gradient:  0.16883393963372406
iteration : 2317
train acc:  0.6640625
train loss:  0.5785073041915894
train gradient:  0.2804462709231226
iteration : 2318
train acc:  0.7109375
train loss:  0.540634036064148
train gradient:  0.14907928706134496
iteration : 2319
train acc:  0.6953125
train loss:  0.551470935344696
train gradient:  0.204620705837668
iteration : 2320
train acc:  0.71875
train loss:  0.5416631698608398
train gradient:  0.13978490382813494
iteration : 2321
train acc:  0.6796875
train loss:  0.6269173622131348
train gradient:  0.21296021823723202
iteration : 2322
train acc:  0.7265625
train loss:  0.49773335456848145
train gradient:  0.12117004188218353
iteration : 2323
train acc:  0.6875
train loss:  0.552852988243103
train gradient:  0.1955922293273848
iteration : 2324
train acc:  0.6953125
train loss:  0.5739617347717285
train gradient:  0.19337703564458875
iteration : 2325
train acc:  0.6484375
train loss:  0.6135320663452148
train gradient:  0.2007029538179831
iteration : 2326
train acc:  0.734375
train loss:  0.5246211290359497
train gradient:  0.17219364573277207
iteration : 2327
train acc:  0.75
train loss:  0.5014744997024536
train gradient:  0.13479277646745375
iteration : 2328
train acc:  0.78125
train loss:  0.5016077756881714
train gradient:  0.1402528220946308
iteration : 2329
train acc:  0.6796875
train loss:  0.5466029047966003
train gradient:  0.17649542173015392
iteration : 2330
train acc:  0.7421875
train loss:  0.5053056478500366
train gradient:  0.12970612889633476
iteration : 2331
train acc:  0.640625
train loss:  0.581066370010376
train gradient:  0.20793222328975156
iteration : 2332
train acc:  0.7265625
train loss:  0.5065608024597168
train gradient:  0.12459475502219627
iteration : 2333
train acc:  0.65625
train loss:  0.5870221853256226
train gradient:  0.167086649839642
iteration : 2334
train acc:  0.6875
train loss:  0.5491016507148743
train gradient:  0.1811037512863442
iteration : 2335
train acc:  0.7109375
train loss:  0.5393561720848083
train gradient:  0.1477487830203577
iteration : 2336
train acc:  0.6484375
train loss:  0.5749305486679077
train gradient:  0.17192389109927408
iteration : 2337
train acc:  0.6796875
train loss:  0.5264101028442383
train gradient:  0.134837893621132
iteration : 2338
train acc:  0.7265625
train loss:  0.5506386756896973
train gradient:  0.2247434283350356
iteration : 2339
train acc:  0.6640625
train loss:  0.571833610534668
train gradient:  0.16278027020917313
iteration : 2340
train acc:  0.640625
train loss:  0.668931245803833
train gradient:  0.32872406930793513
iteration : 2341
train acc:  0.6953125
train loss:  0.5075217485427856
train gradient:  0.14393941999365853
iteration : 2342
train acc:  0.6796875
train loss:  0.585538387298584
train gradient:  0.185380174948204
iteration : 2343
train acc:  0.8046875
train loss:  0.450484961271286
train gradient:  0.1347998804522449
iteration : 2344
train acc:  0.71875
train loss:  0.5700455904006958
train gradient:  0.14702313217665264
iteration : 2345
train acc:  0.78125
train loss:  0.4460489749908447
train gradient:  0.1557044441267182
iteration : 2346
train acc:  0.7109375
train loss:  0.5565879344940186
train gradient:  0.16631573025103924
iteration : 2347
train acc:  0.7109375
train loss:  0.5305467247962952
train gradient:  0.13315405381778064
iteration : 2348
train acc:  0.6953125
train loss:  0.5478993654251099
train gradient:  0.15409973850820208
iteration : 2349
train acc:  0.6953125
train loss:  0.5511571168899536
train gradient:  0.1652097142203732
iteration : 2350
train acc:  0.671875
train loss:  0.549540638923645
train gradient:  0.1459650072827589
iteration : 2351
train acc:  0.7890625
train loss:  0.48900479078292847
train gradient:  0.12794442736025466
iteration : 2352
train acc:  0.703125
train loss:  0.5331768989562988
train gradient:  0.15638078267619812
iteration : 2353
train acc:  0.734375
train loss:  0.5650044083595276
train gradient:  0.20290718617313486
iteration : 2354
train acc:  0.6328125
train loss:  0.6422343254089355
train gradient:  0.197234622941577
iteration : 2355
train acc:  0.7109375
train loss:  0.5454869270324707
train gradient:  0.1628948967010673
iteration : 2356
train acc:  0.7109375
train loss:  0.5588104128837585
train gradient:  0.13361918484829827
iteration : 2357
train acc:  0.7734375
train loss:  0.4828583598136902
train gradient:  0.14192722010761855
iteration : 2358
train acc:  0.75
train loss:  0.5018105506896973
train gradient:  0.13465832062170446
iteration : 2359
train acc:  0.71875
train loss:  0.577695369720459
train gradient:  0.2224380886398193
iteration : 2360
train acc:  0.734375
train loss:  0.4859766960144043
train gradient:  0.11984002830617717
iteration : 2361
train acc:  0.796875
train loss:  0.44799959659576416
train gradient:  0.10080113913839549
iteration : 2362
train acc:  0.7578125
train loss:  0.49588871002197266
train gradient:  0.12607097700412237
iteration : 2363
train acc:  0.640625
train loss:  0.6050578355789185
train gradient:  0.1873910389790035
iteration : 2364
train acc:  0.7265625
train loss:  0.5259150862693787
train gradient:  0.1507216991144532
iteration : 2365
train acc:  0.6875
train loss:  0.5347616672515869
train gradient:  0.14268106238205286
iteration : 2366
train acc:  0.703125
train loss:  0.5424250364303589
train gradient:  0.18642424453928771
iteration : 2367
train acc:  0.7109375
train loss:  0.5664732456207275
train gradient:  0.1776501768358836
iteration : 2368
train acc:  0.6875
train loss:  0.6420296430587769
train gradient:  0.23473031571315361
iteration : 2369
train acc:  0.75
train loss:  0.535183846950531
train gradient:  0.128066526502025
iteration : 2370
train acc:  0.7109375
train loss:  0.5571794509887695
train gradient:  0.16871909043266503
iteration : 2371
train acc:  0.6484375
train loss:  0.6049507856369019
train gradient:  0.1549657586322402
iteration : 2372
train acc:  0.6796875
train loss:  0.5939874053001404
train gradient:  0.1916059619946651
iteration : 2373
train acc:  0.7578125
train loss:  0.45564207434654236
train gradient:  0.13259736753618617
iteration : 2374
train acc:  0.7421875
train loss:  0.5414755344390869
train gradient:  0.14944455605613477
iteration : 2375
train acc:  0.7109375
train loss:  0.5431519746780396
train gradient:  0.1479568329439901
iteration : 2376
train acc:  0.7734375
train loss:  0.506449818611145
train gradient:  0.13553113305000994
iteration : 2377
train acc:  0.7265625
train loss:  0.5318211317062378
train gradient:  0.18979370227888875
iteration : 2378
train acc:  0.703125
train loss:  0.5494029521942139
train gradient:  0.19233368852646407
iteration : 2379
train acc:  0.7109375
train loss:  0.5771905183792114
train gradient:  0.1840582358215782
iteration : 2380
train acc:  0.78125
train loss:  0.47813689708709717
train gradient:  0.13886908165371986
iteration : 2381
train acc:  0.640625
train loss:  0.6151354312896729
train gradient:  0.20651257163824224
iteration : 2382
train acc:  0.703125
train loss:  0.5423579216003418
train gradient:  0.13017332279759328
iteration : 2383
train acc:  0.6328125
train loss:  0.6263834238052368
train gradient:  0.29315104039003237
iteration : 2384
train acc:  0.765625
train loss:  0.47478869557380676
train gradient:  0.1340932886134926
iteration : 2385
train acc:  0.6640625
train loss:  0.5666110515594482
train gradient:  0.14729309366660964
iteration : 2386
train acc:  0.6796875
train loss:  0.5240679383277893
train gradient:  0.16995092703834647
iteration : 2387
train acc:  0.6953125
train loss:  0.576740026473999
train gradient:  0.1581098127975767
iteration : 2388
train acc:  0.7421875
train loss:  0.5338521599769592
train gradient:  0.16066723590376486
iteration : 2389
train acc:  0.78125
train loss:  0.47773370146751404
train gradient:  0.14641385257622747
iteration : 2390
train acc:  0.7265625
train loss:  0.5171126127243042
train gradient:  0.12323365290129557
iteration : 2391
train acc:  0.7109375
train loss:  0.5139728784561157
train gradient:  0.13119698092272164
iteration : 2392
train acc:  0.7109375
train loss:  0.5371212363243103
train gradient:  0.19299050364919815
iteration : 2393
train acc:  0.7109375
train loss:  0.5365045070648193
train gradient:  0.1987544851604744
iteration : 2394
train acc:  0.6953125
train loss:  0.5498863458633423
train gradient:  0.15042377435733784
iteration : 2395
train acc:  0.65625
train loss:  0.5564823746681213
train gradient:  0.22248468567044394
iteration : 2396
train acc:  0.75
train loss:  0.5306841135025024
train gradient:  0.1447949836809962
iteration : 2397
train acc:  0.6484375
train loss:  0.6028934121131897
train gradient:  0.17505211590621544
iteration : 2398
train acc:  0.7734375
train loss:  0.5248006582260132
train gradient:  0.12185568581479574
iteration : 2399
train acc:  0.6484375
train loss:  0.5950344800949097
train gradient:  0.21207109721238127
iteration : 2400
train acc:  0.765625
train loss:  0.5056453943252563
train gradient:  0.156907660660507
iteration : 2401
train acc:  0.703125
train loss:  0.5459650754928589
train gradient:  0.17240579321142158
iteration : 2402
train acc:  0.6796875
train loss:  0.585579514503479
train gradient:  0.1552251594691536
iteration : 2403
train acc:  0.7890625
train loss:  0.4895249307155609
train gradient:  0.14109653845596898
iteration : 2404
train acc:  0.71875
train loss:  0.5718180537223816
train gradient:  0.18863964843191003
iteration : 2405
train acc:  0.734375
train loss:  0.5230501890182495
train gradient:  0.15663806575105618
iteration : 2406
train acc:  0.7265625
train loss:  0.5491199493408203
train gradient:  0.19673194852473702
iteration : 2407
train acc:  0.703125
train loss:  0.5330644249916077
train gradient:  0.11496362703384205
iteration : 2408
train acc:  0.8046875
train loss:  0.476242333650589
train gradient:  0.15785365199723403
iteration : 2409
train acc:  0.71875
train loss:  0.4608903229236603
train gradient:  0.13231200308141625
iteration : 2410
train acc:  0.71875
train loss:  0.5246158838272095
train gradient:  0.14504587613038053
iteration : 2411
train acc:  0.7265625
train loss:  0.5443004369735718
train gradient:  0.14060624788097914
iteration : 2412
train acc:  0.71875
train loss:  0.5511434674263
train gradient:  0.19235079573792574
iteration : 2413
train acc:  0.7265625
train loss:  0.4847757816314697
train gradient:  0.12967407981536988
iteration : 2414
train acc:  0.75
train loss:  0.5052974224090576
train gradient:  0.15476414782382825
iteration : 2415
train acc:  0.7578125
train loss:  0.4657714366912842
train gradient:  0.14789803048143346
iteration : 2416
train acc:  0.7421875
train loss:  0.5360784530639648
train gradient:  0.1753884678457048
iteration : 2417
train acc:  0.703125
train loss:  0.5615174174308777
train gradient:  0.15608902068299735
iteration : 2418
train acc:  0.7421875
train loss:  0.5072622895240784
train gradient:  0.15435557624584012
iteration : 2419
train acc:  0.7578125
train loss:  0.4886324405670166
train gradient:  0.14930271755410346
iteration : 2420
train acc:  0.7109375
train loss:  0.5096917152404785
train gradient:  0.16113068805404118
iteration : 2421
train acc:  0.71875
train loss:  0.48836082220077515
train gradient:  0.1612488345129723
iteration : 2422
train acc:  0.6875
train loss:  0.590599775314331
train gradient:  0.19646337646885975
iteration : 2423
train acc:  0.6953125
train loss:  0.5568166971206665
train gradient:  0.18058570500118712
iteration : 2424
train acc:  0.6484375
train loss:  0.5952361226081848
train gradient:  0.182232939391905
iteration : 2425
train acc:  0.703125
train loss:  0.540654718875885
train gradient:  0.2206598920164802
iteration : 2426
train acc:  0.6328125
train loss:  0.554863691329956
train gradient:  0.2314441176346115
iteration : 2427
train acc:  0.734375
train loss:  0.5101995468139648
train gradient:  0.21021655613399157
iteration : 2428
train acc:  0.71875
train loss:  0.5072091221809387
train gradient:  0.1585600012042884
iteration : 2429
train acc:  0.7734375
train loss:  0.5272751450538635
train gradient:  0.15695231258918438
iteration : 2430
train acc:  0.78125
train loss:  0.5065265893936157
train gradient:  0.1786680165652633
iteration : 2431
train acc:  0.75
train loss:  0.5164022445678711
train gradient:  0.20128611634195923
iteration : 2432
train acc:  0.6953125
train loss:  0.5439388155937195
train gradient:  0.16555686859243665
iteration : 2433
train acc:  0.6953125
train loss:  0.5109964609146118
train gradient:  0.14255098414205442
iteration : 2434
train acc:  0.7265625
train loss:  0.5193368196487427
train gradient:  0.17524230541644306
iteration : 2435
train acc:  0.75
train loss:  0.5323777198791504
train gradient:  0.16423844485100708
iteration : 2436
train acc:  0.7109375
train loss:  0.5751222968101501
train gradient:  0.17081944365155738
iteration : 2437
train acc:  0.6640625
train loss:  0.6213287115097046
train gradient:  0.26324855634920014
iteration : 2438
train acc:  0.734375
train loss:  0.5356419086456299
train gradient:  0.16723299809497444
iteration : 2439
train acc:  0.71875
train loss:  0.5435889959335327
train gradient:  0.22463834551939638
iteration : 2440
train acc:  0.65625
train loss:  0.604625940322876
train gradient:  0.19088433911014996
iteration : 2441
train acc:  0.6875
train loss:  0.5509124994277954
train gradient:  0.15521524332225073
iteration : 2442
train acc:  0.75
train loss:  0.4897708296775818
train gradient:  0.1229288930135097
iteration : 2443
train acc:  0.734375
train loss:  0.5399169921875
train gradient:  0.139180013354606
iteration : 2444
train acc:  0.7265625
train loss:  0.5639711618423462
train gradient:  0.18770495030753906
iteration : 2445
train acc:  0.7109375
train loss:  0.603431761264801
train gradient:  0.16328523103125622
iteration : 2446
train acc:  0.765625
train loss:  0.5061072111129761
train gradient:  0.14833141593619056
iteration : 2447
train acc:  0.7734375
train loss:  0.48476654291152954
train gradient:  0.11311668642798466
iteration : 2448
train acc:  0.734375
train loss:  0.5232784748077393
train gradient:  0.13702419176866992
iteration : 2449
train acc:  0.734375
train loss:  0.5240229368209839
train gradient:  0.21155485418509978
iteration : 2450
train acc:  0.65625
train loss:  0.5337766408920288
train gradient:  0.19508851377893605
iteration : 2451
train acc:  0.6875
train loss:  0.557780385017395
train gradient:  0.15302394205812975
iteration : 2452
train acc:  0.7578125
train loss:  0.4639524221420288
train gradient:  0.14488072584489242
iteration : 2453
train acc:  0.71875
train loss:  0.5405653715133667
train gradient:  0.1810166404685013
iteration : 2454
train acc:  0.7578125
train loss:  0.4778478741645813
train gradient:  0.13683589232243498
iteration : 2455
train acc:  0.7109375
train loss:  0.5181403756141663
train gradient:  0.15733612936176644
iteration : 2456
train acc:  0.703125
train loss:  0.5545684099197388
train gradient:  0.14755098271927342
iteration : 2457
train acc:  0.75
train loss:  0.5228278636932373
train gradient:  0.11543001216240727
iteration : 2458
train acc:  0.671875
train loss:  0.5664857625961304
train gradient:  0.1644691927707536
iteration : 2459
train acc:  0.6875
train loss:  0.5432602167129517
train gradient:  0.15551502957184743
iteration : 2460
train acc:  0.6796875
train loss:  0.5857516527175903
train gradient:  0.26082745312695405
iteration : 2461
train acc:  0.734375
train loss:  0.5026801824569702
train gradient:  0.10745424178991549
iteration : 2462
train acc:  0.7421875
train loss:  0.4875800907611847
train gradient:  0.18299174540315954
iteration : 2463
train acc:  0.765625
train loss:  0.47865307331085205
train gradient:  0.18974061973599893
iteration : 2464
train acc:  0.6875
train loss:  0.5602390170097351
train gradient:  0.17193856826848497
iteration : 2465
train acc:  0.625
train loss:  0.5972352623939514
train gradient:  0.18556033319759824
iteration : 2466
train acc:  0.7109375
train loss:  0.533654510974884
train gradient:  0.19495334444949503
iteration : 2467
train acc:  0.8125
train loss:  0.5301387906074524
train gradient:  0.29920511849591275
iteration : 2468
train acc:  0.640625
train loss:  0.5751081705093384
train gradient:  0.15212637760455364
iteration : 2469
train acc:  0.734375
train loss:  0.5551092028617859
train gradient:  0.14995983446747724
iteration : 2470
train acc:  0.703125
train loss:  0.5667314529418945
train gradient:  0.17575198874169579
iteration : 2471
train acc:  0.7421875
train loss:  0.5183042883872986
train gradient:  0.15892172813680805
iteration : 2472
train acc:  0.71875
train loss:  0.4924658238887787
train gradient:  0.1487486352752228
iteration : 2473
train acc:  0.6953125
train loss:  0.5678516626358032
train gradient:  0.2691981279529801
iteration : 2474
train acc:  0.6796875
train loss:  0.5512217283248901
train gradient:  0.14502689200128152
iteration : 2475
train acc:  0.7265625
train loss:  0.5331311225891113
train gradient:  0.14243060385189196
iteration : 2476
train acc:  0.6875
train loss:  0.5559333562850952
train gradient:  0.18642593646211458
iteration : 2477
train acc:  0.765625
train loss:  0.5073234438896179
train gradient:  0.17265545746686253
iteration : 2478
train acc:  0.7734375
train loss:  0.5320518612861633
train gradient:  0.17662115572827303
iteration : 2479
train acc:  0.75
train loss:  0.5295968055725098
train gradient:  0.22661679611187546
iteration : 2480
train acc:  0.6875
train loss:  0.5858380198478699
train gradient:  0.18557967190506067
iteration : 2481
train acc:  0.7421875
train loss:  0.4859710931777954
train gradient:  0.1449669578559644
iteration : 2482
train acc:  0.765625
train loss:  0.4932292103767395
train gradient:  0.18113765746225965
iteration : 2483
train acc:  0.625
train loss:  0.6127289533615112
train gradient:  0.2066906899965055
iteration : 2484
train acc:  0.7109375
train loss:  0.5571180582046509
train gradient:  0.1883184387626599
iteration : 2485
train acc:  0.7265625
train loss:  0.5351351499557495
train gradient:  0.1508512233054652
iteration : 2486
train acc:  0.625
train loss:  0.594811737537384
train gradient:  0.22212890011086733
iteration : 2487
train acc:  0.703125
train loss:  0.5285016894340515
train gradient:  0.15447926380857338
iteration : 2488
train acc:  0.6875
train loss:  0.5580911636352539
train gradient:  0.22579458956516002
iteration : 2489
train acc:  0.7421875
train loss:  0.5442769527435303
train gradient:  0.2002223468261483
iteration : 2490
train acc:  0.6875
train loss:  0.5748749375343323
train gradient:  0.15119263111694253
iteration : 2491
train acc:  0.6328125
train loss:  0.570531964302063
train gradient:  0.18804092227618258
iteration : 2492
train acc:  0.71875
train loss:  0.5803282260894775
train gradient:  0.18817002389450133
iteration : 2493
train acc:  0.7578125
train loss:  0.5247964859008789
train gradient:  0.20892143486297102
iteration : 2494
train acc:  0.7421875
train loss:  0.5058407783508301
train gradient:  0.1709529257745711
iteration : 2495
train acc:  0.703125
train loss:  0.5456792712211609
train gradient:  0.18301873371191013
iteration : 2496
train acc:  0.6953125
train loss:  0.5248993635177612
train gradient:  0.17463112746101125
iteration : 2497
train acc:  0.734375
train loss:  0.5663295984268188
train gradient:  0.1524207432452499
iteration : 2498
train acc:  0.7109375
train loss:  0.5493457317352295
train gradient:  0.20225040415896572
iteration : 2499
train acc:  0.671875
train loss:  0.5696287751197815
train gradient:  0.21454581044370347
iteration : 2500
train acc:  0.703125
train loss:  0.5567335486412048
train gradient:  0.1680357168922036
iteration : 2501
train acc:  0.71875
train loss:  0.522405207157135
train gradient:  0.16898142980491215
iteration : 2502
train acc:  0.7265625
train loss:  0.5124557018280029
train gradient:  0.14338555873761774
iteration : 2503
train acc:  0.671875
train loss:  0.6160687208175659
train gradient:  0.25175018838602853
iteration : 2504
train acc:  0.7578125
train loss:  0.5000455379486084
train gradient:  0.15670381023332738
iteration : 2505
train acc:  0.75
train loss:  0.5303032398223877
train gradient:  0.16117494925952686
iteration : 2506
train acc:  0.703125
train loss:  0.5549033284187317
train gradient:  0.23445312797388101
iteration : 2507
train acc:  0.734375
train loss:  0.5878012776374817
train gradient:  0.18984941362213076
iteration : 2508
train acc:  0.75
train loss:  0.5043760538101196
train gradient:  0.15215350669894995
iteration : 2509
train acc:  0.6953125
train loss:  0.5417548418045044
train gradient:  0.2373307046119598
iteration : 2510
train acc:  0.6875
train loss:  0.5687755942344666
train gradient:  0.16610937809216844
iteration : 2511
train acc:  0.75
train loss:  0.4849655032157898
train gradient:  0.14334477367823578
iteration : 2512
train acc:  0.71875
train loss:  0.5239591598510742
train gradient:  0.13557007187493897
iteration : 2513
train acc:  0.6875
train loss:  0.5658235549926758
train gradient:  0.1462603702129195
iteration : 2514
train acc:  0.7421875
train loss:  0.5159361958503723
train gradient:  0.16116701217067453
iteration : 2515
train acc:  0.6953125
train loss:  0.5264798998832703
train gradient:  0.2348190802056905
iteration : 2516
train acc:  0.6796875
train loss:  0.5301716327667236
train gradient:  0.1489626411143376
iteration : 2517
train acc:  0.75
train loss:  0.49136146903038025
train gradient:  0.14513022174077278
iteration : 2518
train acc:  0.7109375
train loss:  0.5435546040534973
train gradient:  0.2053629552896336
iteration : 2519
train acc:  0.75
train loss:  0.4955023527145386
train gradient:  0.17952531379994768
iteration : 2520
train acc:  0.7421875
train loss:  0.542996883392334
train gradient:  0.16224192427768735
iteration : 2521
train acc:  0.6796875
train loss:  0.5533642768859863
train gradient:  0.15936473416630187
iteration : 2522
train acc:  0.7421875
train loss:  0.5018770694732666
train gradient:  0.12837525885049217
iteration : 2523
train acc:  0.71875
train loss:  0.5691155791282654
train gradient:  0.2296487912736749
iteration : 2524
train acc:  0.7109375
train loss:  0.5809549689292908
train gradient:  0.23328938504615704
iteration : 2525
train acc:  0.6796875
train loss:  0.6037102341651917
train gradient:  0.14573104654258257
iteration : 2526
train acc:  0.765625
train loss:  0.49371084570884705
train gradient:  0.12474923849279447
iteration : 2527
train acc:  0.75
train loss:  0.49955421686172485
train gradient:  0.11614220071219654
iteration : 2528
train acc:  0.734375
train loss:  0.5233811140060425
train gradient:  0.1887583061252769
iteration : 2529
train acc:  0.7265625
train loss:  0.5338383913040161
train gradient:  0.18375504572197848
iteration : 2530
train acc:  0.6171875
train loss:  0.5886881351470947
train gradient:  0.20232530618088432
iteration : 2531
train acc:  0.765625
train loss:  0.5264385938644409
train gradient:  0.17847009472286474
iteration : 2532
train acc:  0.6640625
train loss:  0.5689587593078613
train gradient:  0.141069865641956
iteration : 2533
train acc:  0.6875
train loss:  0.5437809228897095
train gradient:  0.16021184747717956
iteration : 2534
train acc:  0.7109375
train loss:  0.5169786810874939
train gradient:  0.19511010013006602
iteration : 2535
train acc:  0.734375
train loss:  0.5227672457695007
train gradient:  0.21218502346223872
iteration : 2536
train acc:  0.6484375
train loss:  0.6187263131141663
train gradient:  0.26789298614531826
iteration : 2537
train acc:  0.640625
train loss:  0.6008603572845459
train gradient:  0.18829175914504914
iteration : 2538
train acc:  0.7421875
train loss:  0.537429928779602
train gradient:  0.17234043980507824
iteration : 2539
train acc:  0.71875
train loss:  0.5705909729003906
train gradient:  0.1607450480313421
iteration : 2540
train acc:  0.7421875
train loss:  0.5049319863319397
train gradient:  0.24569440983697133
iteration : 2541
train acc:  0.6796875
train loss:  0.5426633954048157
train gradient:  0.16868668539839868
iteration : 2542
train acc:  0.6796875
train loss:  0.5629659295082092
train gradient:  0.17627253394833092
iteration : 2543
train acc:  0.7578125
train loss:  0.5083576440811157
train gradient:  0.16193760021660009
iteration : 2544
train acc:  0.703125
train loss:  0.5529592037200928
train gradient:  0.19292326126675585
iteration : 2545
train acc:  0.7421875
train loss:  0.5341857671737671
train gradient:  0.1424344033690714
iteration : 2546
train acc:  0.6484375
train loss:  0.5795705318450928
train gradient:  0.211437095247577
iteration : 2547
train acc:  0.6953125
train loss:  0.5844270586967468
train gradient:  0.26473940140725866
iteration : 2548
train acc:  0.7265625
train loss:  0.5681056380271912
train gradient:  0.23288775921734384
iteration : 2549
train acc:  0.7421875
train loss:  0.5241709351539612
train gradient:  0.13468977705492174
iteration : 2550
train acc:  0.75
train loss:  0.49752330780029297
train gradient:  0.1252264097854836
iteration : 2551
train acc:  0.78125
train loss:  0.522442102432251
train gradient:  0.17089186129214498
iteration : 2552
train acc:  0.7265625
train loss:  0.5558480024337769
train gradient:  0.1842539142247653
iteration : 2553
train acc:  0.71875
train loss:  0.587622880935669
train gradient:  0.171354487118803
iteration : 2554
train acc:  0.734375
train loss:  0.47826483845710754
train gradient:  0.10564887612581007
iteration : 2555
train acc:  0.6796875
train loss:  0.5654951930046082
train gradient:  0.16239910075771558
iteration : 2556
train acc:  0.703125
train loss:  0.5117365121841431
train gradient:  0.16603058925347525
iteration : 2557
train acc:  0.625
train loss:  0.6013083457946777
train gradient:  0.14475511514970235
iteration : 2558
train acc:  0.7265625
train loss:  0.5235268473625183
train gradient:  0.14178911295312877
iteration : 2559
train acc:  0.7578125
train loss:  0.49103572964668274
train gradient:  0.1713080028104752
iteration : 2560
train acc:  0.734375
train loss:  0.5220245122909546
train gradient:  0.18268611458056094
iteration : 2561
train acc:  0.71875
train loss:  0.5735024213790894
train gradient:  0.18526783879488876
iteration : 2562
train acc:  0.7109375
train loss:  0.5162698030471802
train gradient:  0.15277371774996767
iteration : 2563
train acc:  0.7265625
train loss:  0.5320215821266174
train gradient:  0.1393476212308999
iteration : 2564
train acc:  0.6328125
train loss:  0.6352182626724243
train gradient:  0.230795333660822
iteration : 2565
train acc:  0.6875
train loss:  0.5791257619857788
train gradient:  0.2407735246602194
iteration : 2566
train acc:  0.625
train loss:  0.6178597211837769
train gradient:  0.18816567155394634
iteration : 2567
train acc:  0.6953125
train loss:  0.5711403489112854
train gradient:  0.15860406729802723
iteration : 2568
train acc:  0.7578125
train loss:  0.5334961414337158
train gradient:  0.13796253604403136
iteration : 2569
train acc:  0.7421875
train loss:  0.4749947786331177
train gradient:  0.14943847791604878
iteration : 2570
train acc:  0.6796875
train loss:  0.5253397822380066
train gradient:  0.15698367710311906
iteration : 2571
train acc:  0.7734375
train loss:  0.45837777853012085
train gradient:  0.12702775165032332
iteration : 2572
train acc:  0.7109375
train loss:  0.5327829122543335
train gradient:  0.17407979625168657
iteration : 2573
train acc:  0.71875
train loss:  0.5428944826126099
train gradient:  0.12956887235026784
iteration : 2574
train acc:  0.6796875
train loss:  0.5509631633758545
train gradient:  0.19749476545990569
iteration : 2575
train acc:  0.734375
train loss:  0.5420570373535156
train gradient:  0.15799882102326598
iteration : 2576
train acc:  0.6953125
train loss:  0.545632004737854
train gradient:  0.1888106709931055
iteration : 2577
train acc:  0.765625
train loss:  0.5162627696990967
train gradient:  0.19924644814221967
iteration : 2578
train acc:  0.6796875
train loss:  0.5637956857681274
train gradient:  0.17323045210586002
iteration : 2579
train acc:  0.6484375
train loss:  0.6001956462860107
train gradient:  0.16858953254347447
iteration : 2580
train acc:  0.71875
train loss:  0.540254533290863
train gradient:  0.17011261013693335
iteration : 2581
train acc:  0.71875
train loss:  0.5177811980247498
train gradient:  0.2232390701253852
iteration : 2582
train acc:  0.6953125
train loss:  0.6116479635238647
train gradient:  0.17613899823788842
iteration : 2583
train acc:  0.6796875
train loss:  0.5582878589630127
train gradient:  0.15830401768177843
iteration : 2584
train acc:  0.7265625
train loss:  0.5344345569610596
train gradient:  0.16261824876410752
iteration : 2585
train acc:  0.640625
train loss:  0.634230375289917
train gradient:  0.19175001494722974
iteration : 2586
train acc:  0.6875
train loss:  0.5853714942932129
train gradient:  0.1490315144367699
iteration : 2587
train acc:  0.7109375
train loss:  0.5374886989593506
train gradient:  0.15243428082067845
iteration : 2588
train acc:  0.7265625
train loss:  0.5085506439208984
train gradient:  0.14153761148501115
iteration : 2589
train acc:  0.640625
train loss:  0.5940855145454407
train gradient:  0.19600023936553515
iteration : 2590
train acc:  0.7578125
train loss:  0.4566457271575928
train gradient:  0.0992678716872032
iteration : 2591
train acc:  0.71875
train loss:  0.5303975343704224
train gradient:  0.18134018643274252
iteration : 2592
train acc:  0.75
train loss:  0.4885212182998657
train gradient:  0.14565065232811888
iteration : 2593
train acc:  0.671875
train loss:  0.5784406661987305
train gradient:  0.17044073978000734
iteration : 2594
train acc:  0.6953125
train loss:  0.5933234095573425
train gradient:  0.16596574581353782
iteration : 2595
train acc:  0.6796875
train loss:  0.5803465843200684
train gradient:  0.1719215964019954
iteration : 2596
train acc:  0.734375
train loss:  0.5390477776527405
train gradient:  0.17204365952984924
iteration : 2597
train acc:  0.7265625
train loss:  0.5171682238578796
train gradient:  0.13368310395979868
iteration : 2598
train acc:  0.671875
train loss:  0.5653896331787109
train gradient:  0.17942487414534936
iteration : 2599
train acc:  0.71875
train loss:  0.5933331251144409
train gradient:  0.16669033809724682
iteration : 2600
train acc:  0.6796875
train loss:  0.6020503044128418
train gradient:  0.22547542480409202
iteration : 2601
train acc:  0.6796875
train loss:  0.6024038195610046
train gradient:  0.16597815810244324
iteration : 2602
train acc:  0.7578125
train loss:  0.5091454982757568
train gradient:  0.15345741263972484
iteration : 2603
train acc:  0.7890625
train loss:  0.47752654552459717
train gradient:  0.1806540850587266
iteration : 2604
train acc:  0.7265625
train loss:  0.5244168639183044
train gradient:  0.12923528554913438
iteration : 2605
train acc:  0.7265625
train loss:  0.5287399291992188
train gradient:  0.16274336637796671
iteration : 2606
train acc:  0.75
train loss:  0.5146688222885132
train gradient:  0.15468915116405962
iteration : 2607
train acc:  0.75
train loss:  0.46867290139198303
train gradient:  0.11975530981665623
iteration : 2608
train acc:  0.75
train loss:  0.49009591341018677
train gradient:  0.2075821884704937
iteration : 2609
train acc:  0.6796875
train loss:  0.5781412124633789
train gradient:  0.1513678620612428
iteration : 2610
train acc:  0.6640625
train loss:  0.5358975529670715
train gradient:  0.1645215322208191
iteration : 2611
train acc:  0.7109375
train loss:  0.5383846163749695
train gradient:  0.11649289827843923
iteration : 2612
train acc:  0.671875
train loss:  0.6151344776153564
train gradient:  0.19294477562920145
iteration : 2613
train acc:  0.734375
train loss:  0.5349857807159424
train gradient:  0.17959532469734013
iteration : 2614
train acc:  0.765625
train loss:  0.4916907548904419
train gradient:  0.14766013156727598
iteration : 2615
train acc:  0.7421875
train loss:  0.5333925485610962
train gradient:  0.16702024239217217
iteration : 2616
train acc:  0.6953125
train loss:  0.605464518070221
train gradient:  0.18316807242414465
iteration : 2617
train acc:  0.71875
train loss:  0.6231417655944824
train gradient:  0.31348229388140697
iteration : 2618
train acc:  0.6796875
train loss:  0.5600660443305969
train gradient:  0.17368538801510536
iteration : 2619
train acc:  0.734375
train loss:  0.5569599866867065
train gradient:  0.18441038359092177
iteration : 2620
train acc:  0.75
train loss:  0.579250693321228
train gradient:  0.15497627384438226
iteration : 2621
train acc:  0.6953125
train loss:  0.5435910820960999
train gradient:  0.14067595974725028
iteration : 2622
train acc:  0.734375
train loss:  0.5195883512496948
train gradient:  0.1333796553500885
iteration : 2623
train acc:  0.7578125
train loss:  0.5147826075553894
train gradient:  0.15654434369332187
iteration : 2624
train acc:  0.6953125
train loss:  0.5338326692581177
train gradient:  0.14996446065090613
iteration : 2625
train acc:  0.7109375
train loss:  0.5624996423721313
train gradient:  0.22723118680539645
iteration : 2626
train acc:  0.6953125
train loss:  0.560632050037384
train gradient:  0.18628652537159884
iteration : 2627
train acc:  0.640625
train loss:  0.5689999461174011
train gradient:  0.14858796733925375
iteration : 2628
train acc:  0.6953125
train loss:  0.5477066040039062
train gradient:  0.1716558270583231
iteration : 2629
train acc:  0.7421875
train loss:  0.5570080876350403
train gradient:  0.18400428946316327
iteration : 2630
train acc:  0.6640625
train loss:  0.5922304391860962
train gradient:  0.2330114731747096
iteration : 2631
train acc:  0.625
train loss:  0.6225154399871826
train gradient:  0.15318436336637758
iteration : 2632
train acc:  0.7421875
train loss:  0.5152268409729004
train gradient:  0.15296108374176787
iteration : 2633
train acc:  0.75
train loss:  0.49116650223731995
train gradient:  0.12536157879186333
iteration : 2634
train acc:  0.734375
train loss:  0.5283806920051575
train gradient:  0.1524113602767471
iteration : 2635
train acc:  0.71875
train loss:  0.5696399211883545
train gradient:  0.1685699984808435
iteration : 2636
train acc:  0.7109375
train loss:  0.5296326875686646
train gradient:  0.14802760300898793
iteration : 2637
train acc:  0.71875
train loss:  0.51741623878479
train gradient:  0.11768530393969791
iteration : 2638
train acc:  0.703125
train loss:  0.5257554650306702
train gradient:  0.1257906167190493
iteration : 2639
train acc:  0.6875
train loss:  0.5711870193481445
train gradient:  0.17749998627765595
iteration : 2640
train acc:  0.6953125
train loss:  0.5276964902877808
train gradient:  0.1313928222551104
iteration : 2641
train acc:  0.7109375
train loss:  0.49431079626083374
train gradient:  0.11633704945373263
iteration : 2642
train acc:  0.6953125
train loss:  0.5574723482131958
train gradient:  0.1773135689228425
iteration : 2643
train acc:  0.703125
train loss:  0.5248412489891052
train gradient:  0.12903702169668635
iteration : 2644
train acc:  0.7734375
train loss:  0.4811699390411377
train gradient:  0.13494103610064012
iteration : 2645
train acc:  0.7734375
train loss:  0.5160245299339294
train gradient:  0.1552814305322
iteration : 2646
train acc:  0.75
train loss:  0.5077836513519287
train gradient:  0.2204952673591865
iteration : 2647
train acc:  0.6953125
train loss:  0.5565366148948669
train gradient:  0.1514864645560246
iteration : 2648
train acc:  0.75
train loss:  0.5118889212608337
train gradient:  0.12501025153952428
iteration : 2649
train acc:  0.7421875
train loss:  0.5060673952102661
train gradient:  0.15127625153513652
iteration : 2650
train acc:  0.671875
train loss:  0.5873733758926392
train gradient:  0.22235978703932746
iteration : 2651
train acc:  0.7421875
train loss:  0.5005278587341309
train gradient:  0.12380669930549373
iteration : 2652
train acc:  0.703125
train loss:  0.5948132872581482
train gradient:  0.153914403550774
iteration : 2653
train acc:  0.6484375
train loss:  0.5955329537391663
train gradient:  0.19579394805536013
iteration : 2654
train acc:  0.7265625
train loss:  0.5387205481529236
train gradient:  0.14715904088380716
iteration : 2655
train acc:  0.7421875
train loss:  0.5108071565628052
train gradient:  0.14661048136171223
iteration : 2656
train acc:  0.640625
train loss:  0.5783618688583374
train gradient:  0.1445662484965416
iteration : 2657
train acc:  0.6640625
train loss:  0.5477398037910461
train gradient:  0.1838401485183841
iteration : 2658
train acc:  0.703125
train loss:  0.5281630158424377
train gradient:  0.16281303682451026
iteration : 2659
train acc:  0.7578125
train loss:  0.48013508319854736
train gradient:  0.1685190461356177
iteration : 2660
train acc:  0.7578125
train loss:  0.4745483696460724
train gradient:  0.1341897245525945
iteration : 2661
train acc:  0.6796875
train loss:  0.6193119287490845
train gradient:  0.22926218420669708
iteration : 2662
train acc:  0.671875
train loss:  0.5691115856170654
train gradient:  0.1406485134854436
iteration : 2663
train acc:  0.71875
train loss:  0.5453132390975952
train gradient:  0.20253766641304988
iteration : 2664
train acc:  0.7109375
train loss:  0.5398772358894348
train gradient:  0.13796763021791408
iteration : 2665
train acc:  0.7578125
train loss:  0.4882469177246094
train gradient:  0.10513401296636685
iteration : 2666
train acc:  0.734375
train loss:  0.5419644713401794
train gradient:  0.13933545404787787
iteration : 2667
train acc:  0.71875
train loss:  0.5401382446289062
train gradient:  0.16452625273045657
iteration : 2668
train acc:  0.640625
train loss:  0.5929491519927979
train gradient:  0.20279372144817454
iteration : 2669
train acc:  0.6484375
train loss:  0.5445040464401245
train gradient:  0.1652992292689325
iteration : 2670
train acc:  0.703125
train loss:  0.528995931148529
train gradient:  0.18829947825427368
iteration : 2671
train acc:  0.703125
train loss:  0.5367628335952759
train gradient:  0.22125204955826006
iteration : 2672
train acc:  0.6640625
train loss:  0.6103909611701965
train gradient:  0.17341261553537202
iteration : 2673
train acc:  0.6640625
train loss:  0.598344624042511
train gradient:  0.16484925299870767
iteration : 2674
train acc:  0.7109375
train loss:  0.5222156047821045
train gradient:  0.10857185091184168
iteration : 2675
train acc:  0.6953125
train loss:  0.6079574823379517
train gradient:  0.20307875435889788
iteration : 2676
train acc:  0.71875
train loss:  0.5524123907089233
train gradient:  0.1692776264909257
iteration : 2677
train acc:  0.765625
train loss:  0.49769148230552673
train gradient:  0.1472649171442385
iteration : 2678
train acc:  0.8125
train loss:  0.4583233594894409
train gradient:  0.16598069150382339
iteration : 2679
train acc:  0.65625
train loss:  0.5775376558303833
train gradient:  0.17395932626126936
iteration : 2680
train acc:  0.765625
train loss:  0.49911797046661377
train gradient:  0.1502945513613726
iteration : 2681
train acc:  0.7109375
train loss:  0.5685670971870422
train gradient:  0.13360192836218937
iteration : 2682
train acc:  0.6875
train loss:  0.49806585907936096
train gradient:  0.16950120365643578
iteration : 2683
train acc:  0.7109375
train loss:  0.5132702589035034
train gradient:  0.1473490418190419
iteration : 2684
train acc:  0.625
train loss:  0.6211109757423401
train gradient:  0.25580548043404466
iteration : 2685
train acc:  0.6796875
train loss:  0.5702805519104004
train gradient:  0.16285655033208485
iteration : 2686
train acc:  0.7578125
train loss:  0.49778249859809875
train gradient:  0.12521424793313235
iteration : 2687
train acc:  0.7421875
train loss:  0.5037037134170532
train gradient:  0.12225220369956573
iteration : 2688
train acc:  0.6796875
train loss:  0.5824500322341919
train gradient:  0.16506308659881525
iteration : 2689
train acc:  0.703125
train loss:  0.5646932125091553
train gradient:  0.17428838702959915
iteration : 2690
train acc:  0.65625
train loss:  0.5454618334770203
train gradient:  0.23221947156157202
iteration : 2691
train acc:  0.65625
train loss:  0.576704204082489
train gradient:  0.1537409285033023
iteration : 2692
train acc:  0.609375
train loss:  0.6193335056304932
train gradient:  0.27069292365127434
iteration : 2693
train acc:  0.765625
train loss:  0.46119511127471924
train gradient:  0.09967444680255126
iteration : 2694
train acc:  0.7265625
train loss:  0.49034664034843445
train gradient:  0.16354854683166198
iteration : 2695
train acc:  0.6640625
train loss:  0.5406672954559326
train gradient:  0.1449382131199461
iteration : 2696
train acc:  0.7421875
train loss:  0.5287092924118042
train gradient:  0.16649977703531382
iteration : 2697
train acc:  0.75
train loss:  0.5751253366470337
train gradient:  0.16453491930876238
iteration : 2698
train acc:  0.7265625
train loss:  0.5460162162780762
train gradient:  0.21467235574172266
iteration : 2699
train acc:  0.671875
train loss:  0.5377074480056763
train gradient:  0.14583119196609856
iteration : 2700
train acc:  0.75
train loss:  0.5172083377838135
train gradient:  0.21803626612707688
iteration : 2701
train acc:  0.7578125
train loss:  0.5118794441223145
train gradient:  0.11037546708650064
iteration : 2702
train acc:  0.703125
train loss:  0.5547225475311279
train gradient:  0.2243686196484278
iteration : 2703
train acc:  0.7421875
train loss:  0.5291621088981628
train gradient:  0.1961002870371077
iteration : 2704
train acc:  0.6953125
train loss:  0.5710433125495911
train gradient:  0.16355679123102707
iteration : 2705
train acc:  0.703125
train loss:  0.5144690871238708
train gradient:  0.13027252275242668
iteration : 2706
train acc:  0.6640625
train loss:  0.6032372713088989
train gradient:  0.19902500002620116
iteration : 2707
train acc:  0.6796875
train loss:  0.5447242856025696
train gradient:  0.17933151213836745
iteration : 2708
train acc:  0.6796875
train loss:  0.5511467456817627
train gradient:  0.16295756327745875
iteration : 2709
train acc:  0.6953125
train loss:  0.5228040814399719
train gradient:  0.1629846903874661
iteration : 2710
train acc:  0.6796875
train loss:  0.57607501745224
train gradient:  0.16458134701453947
iteration : 2711
train acc:  0.7109375
train loss:  0.508101761341095
train gradient:  0.13950283452854578
iteration : 2712
train acc:  0.7421875
train loss:  0.5410170555114746
train gradient:  0.23647336245681783
iteration : 2713
train acc:  0.6484375
train loss:  0.6142836809158325
train gradient:  0.20941524375820708
iteration : 2714
train acc:  0.7578125
train loss:  0.5059019923210144
train gradient:  0.11899776270783716
iteration : 2715
train acc:  0.75
train loss:  0.5197880268096924
train gradient:  0.2009046885005989
iteration : 2716
train acc:  0.71875
train loss:  0.5657853484153748
train gradient:  0.13895661639443826
iteration : 2717
train acc:  0.703125
train loss:  0.536537766456604
train gradient:  0.1495330057311521
iteration : 2718
train acc:  0.7265625
train loss:  0.5641105771064758
train gradient:  0.28853227749595134
iteration : 2719
train acc:  0.7109375
train loss:  0.5511587858200073
train gradient:  0.1523096542108105
iteration : 2720
train acc:  0.71875
train loss:  0.542388916015625
train gradient:  0.1389549755579432
iteration : 2721
train acc:  0.7578125
train loss:  0.48534339666366577
train gradient:  0.12224945770511053
iteration : 2722
train acc:  0.6953125
train loss:  0.563008189201355
train gradient:  0.20212610166011863
iteration : 2723
train acc:  0.671875
train loss:  0.5777122974395752
train gradient:  0.16271717017126375
iteration : 2724
train acc:  0.6796875
train loss:  0.5696789622306824
train gradient:  0.17272089677818092
iteration : 2725
train acc:  0.671875
train loss:  0.5494229197502136
train gradient:  0.17658439251276925
iteration : 2726
train acc:  0.671875
train loss:  0.5552927255630493
train gradient:  0.16841663591049932
iteration : 2727
train acc:  0.7421875
train loss:  0.5167906880378723
train gradient:  0.13760752245422897
iteration : 2728
train acc:  0.7421875
train loss:  0.5447499752044678
train gradient:  0.2135235997362459
iteration : 2729
train acc:  0.7578125
train loss:  0.5280004143714905
train gradient:  0.13004869166876415
iteration : 2730
train acc:  0.765625
train loss:  0.5029984712600708
train gradient:  0.11196341836897339
iteration : 2731
train acc:  0.7265625
train loss:  0.5215587615966797
train gradient:  0.18396100810488364
iteration : 2732
train acc:  0.7578125
train loss:  0.5201648473739624
train gradient:  0.14996886137550358
iteration : 2733
train acc:  0.7890625
train loss:  0.43523189425468445
train gradient:  0.13914541541539815
iteration : 2734
train acc:  0.765625
train loss:  0.5071215629577637
train gradient:  0.15963067484181637
iteration : 2735
train acc:  0.6171875
train loss:  0.6299777626991272
train gradient:  0.19663666705681343
iteration : 2736
train acc:  0.703125
train loss:  0.5799345970153809
train gradient:  0.1926846779978771
iteration : 2737
train acc:  0.7734375
train loss:  0.5187375545501709
train gradient:  0.1552370724907807
iteration : 2738
train acc:  0.75
train loss:  0.5101789236068726
train gradient:  0.13942780089925416
iteration : 2739
train acc:  0.7578125
train loss:  0.4890691936016083
train gradient:  0.13675710524144652
iteration : 2740
train acc:  0.765625
train loss:  0.4675225615501404
train gradient:  0.12321340629078875
iteration : 2741
train acc:  0.6796875
train loss:  0.5383126735687256
train gradient:  0.1029834093584321
iteration : 2742
train acc:  0.6875
train loss:  0.5615876913070679
train gradient:  0.15341526821894574
iteration : 2743
train acc:  0.671875
train loss:  0.5483263731002808
train gradient:  0.14940998863494223
iteration : 2744
train acc:  0.6953125
train loss:  0.5401096940040588
train gradient:  0.17241361572904362
iteration : 2745
train acc:  0.703125
train loss:  0.549744725227356
train gradient:  0.15509601664880798
iteration : 2746
train acc:  0.796875
train loss:  0.4878312349319458
train gradient:  0.1341741052837452
iteration : 2747
train acc:  0.8046875
train loss:  0.4358617663383484
train gradient:  0.12377649946771177
iteration : 2748
train acc:  0.703125
train loss:  0.5293492078781128
train gradient:  0.16036730145652517
iteration : 2749
train acc:  0.7421875
train loss:  0.4760509729385376
train gradient:  0.12278276853196719
iteration : 2750
train acc:  0.765625
train loss:  0.47795701026916504
train gradient:  0.14829495228179368
iteration : 2751
train acc:  0.6796875
train loss:  0.5613921880722046
train gradient:  0.13994127279530363
iteration : 2752
train acc:  0.7578125
train loss:  0.49082717299461365
train gradient:  0.11826952167290301
iteration : 2753
train acc:  0.6796875
train loss:  0.5845694541931152
train gradient:  0.15272100518734144
iteration : 2754
train acc:  0.734375
train loss:  0.5179523825645447
train gradient:  0.11954680356589974
iteration : 2755
train acc:  0.78125
train loss:  0.458740234375
train gradient:  0.13113921125161093
iteration : 2756
train acc:  0.6875
train loss:  0.5297807455062866
train gradient:  0.1945909726624379
iteration : 2757
train acc:  0.71875
train loss:  0.5617213845252991
train gradient:  0.13952002099262206
iteration : 2758
train acc:  0.734375
train loss:  0.5587429404258728
train gradient:  0.1776083151130134
iteration : 2759
train acc:  0.703125
train loss:  0.5725146532058716
train gradient:  0.20582764331604558
iteration : 2760
train acc:  0.703125
train loss:  0.5802687406539917
train gradient:  0.18758653210361298
iteration : 2761
train acc:  0.7578125
train loss:  0.5296633839607239
train gradient:  0.15527178447419848
iteration : 2762
train acc:  0.6953125
train loss:  0.5508889555931091
train gradient:  0.16984215135861108
iteration : 2763
train acc:  0.7890625
train loss:  0.4638720750808716
train gradient:  0.13660211009774326
iteration : 2764
train acc:  0.703125
train loss:  0.536590039730072
train gradient:  0.16795321973402078
iteration : 2765
train acc:  0.734375
train loss:  0.5021272301673889
train gradient:  0.18496975450434275
iteration : 2766
train acc:  0.7734375
train loss:  0.4775523245334625
train gradient:  0.15392777174139843
iteration : 2767
train acc:  0.7578125
train loss:  0.494963675737381
train gradient:  0.1114350758703882
iteration : 2768
train acc:  0.734375
train loss:  0.5579462051391602
train gradient:  0.17361618954969205
iteration : 2769
train acc:  0.75
train loss:  0.4999179244041443
train gradient:  0.17513057174685423
iteration : 2770
train acc:  0.6796875
train loss:  0.5693429708480835
train gradient:  0.2141777586466086
iteration : 2771
train acc:  0.7265625
train loss:  0.571808397769928
train gradient:  0.18979119222363033
iteration : 2772
train acc:  0.7578125
train loss:  0.4750286936759949
train gradient:  0.1256006113686176
iteration : 2773
train acc:  0.7734375
train loss:  0.48793482780456543
train gradient:  0.12812993244614374
iteration : 2774
train acc:  0.75
train loss:  0.4736652374267578
train gradient:  0.13957770770426758
iteration : 2775
train acc:  0.71875
train loss:  0.5389188528060913
train gradient:  0.13060890811736472
iteration : 2776
train acc:  0.765625
train loss:  0.48367634415626526
train gradient:  0.16333849711911202
iteration : 2777
train acc:  0.71875
train loss:  0.5177589654922485
train gradient:  0.17825237944740543
iteration : 2778
train acc:  0.734375
train loss:  0.5215778350830078
train gradient:  0.1501820984088789
iteration : 2779
train acc:  0.6875
train loss:  0.5030531883239746
train gradient:  0.22221397792039355
iteration : 2780
train acc:  0.6875
train loss:  0.5400660037994385
train gradient:  0.13599808503830046
iteration : 2781
train acc:  0.7109375
train loss:  0.5859487056732178
train gradient:  0.1825851484471807
iteration : 2782
train acc:  0.7265625
train loss:  0.5044424533843994
train gradient:  0.14009758601132177
iteration : 2783
train acc:  0.6875
train loss:  0.5798596143722534
train gradient:  0.17138308336871322
iteration : 2784
train acc:  0.7265625
train loss:  0.5112193822860718
train gradient:  0.1780551134182954
iteration : 2785
train acc:  0.765625
train loss:  0.5008238554000854
train gradient:  0.13421155828929973
iteration : 2786
train acc:  0.75
train loss:  0.5163636207580566
train gradient:  0.15165431309304958
iteration : 2787
train acc:  0.6953125
train loss:  0.5352452993392944
train gradient:  0.1965358952352867
iteration : 2788
train acc:  0.7578125
train loss:  0.5109182596206665
train gradient:  0.14618066838281535
iteration : 2789
train acc:  0.6328125
train loss:  0.6187984347343445
train gradient:  0.18484182652293546
iteration : 2790
train acc:  0.6796875
train loss:  0.6149019598960876
train gradient:  0.19570160296271216
iteration : 2791
train acc:  0.671875
train loss:  0.558068037033081
train gradient:  0.18308392436586882
iteration : 2792
train acc:  0.6796875
train loss:  0.5587624311447144
train gradient:  0.1634429051552548
iteration : 2793
train acc:  0.6484375
train loss:  0.5470214486122131
train gradient:  0.188476064135077
iteration : 2794
train acc:  0.703125
train loss:  0.5143160820007324
train gradient:  0.15918012122587297
iteration : 2795
train acc:  0.6796875
train loss:  0.5670866966247559
train gradient:  0.13663381100270056
iteration : 2796
train acc:  0.75
train loss:  0.5397797226905823
train gradient:  0.21303681889515103
iteration : 2797
train acc:  0.6875
train loss:  0.5925782918930054
train gradient:  0.18181076396142007
iteration : 2798
train acc:  0.765625
train loss:  0.5518492460250854
train gradient:  0.14714435461361
iteration : 2799
train acc:  0.6875
train loss:  0.5991684794425964
train gradient:  0.19468053370598476
iteration : 2800
train acc:  0.6171875
train loss:  0.5913459658622742
train gradient:  0.15034727857145158
iteration : 2801
train acc:  0.6875
train loss:  0.6115440130233765
train gradient:  0.17234016045047656
iteration : 2802
train acc:  0.6796875
train loss:  0.5357261896133423
train gradient:  0.168785520593439
iteration : 2803
train acc:  0.7265625
train loss:  0.5357885360717773
train gradient:  0.16810391500159805
iteration : 2804
train acc:  0.7265625
train loss:  0.5093210339546204
train gradient:  0.1331289915240202
iteration : 2805
train acc:  0.7109375
train loss:  0.5117247700691223
train gradient:  0.14089635092358965
iteration : 2806
train acc:  0.71875
train loss:  0.5315909385681152
train gradient:  0.1930499636445353
iteration : 2807
train acc:  0.734375
train loss:  0.5059413313865662
train gradient:  0.12791913577262407
iteration : 2808
train acc:  0.6640625
train loss:  0.5473004579544067
train gradient:  0.1941197849841561
iteration : 2809
train acc:  0.71875
train loss:  0.5167824029922485
train gradient:  0.19002824260364004
iteration : 2810
train acc:  0.703125
train loss:  0.5345490574836731
train gradient:  0.16265737071638947
iteration : 2811
train acc:  0.7578125
train loss:  0.49184873700141907
train gradient:  0.13461038243305692
iteration : 2812
train acc:  0.6875
train loss:  0.5340965390205383
train gradient:  0.13540869765345642
iteration : 2813
train acc:  0.71875
train loss:  0.5416892170906067
train gradient:  0.15641614322242733
iteration : 2814
train acc:  0.6875
train loss:  0.5823790431022644
train gradient:  0.19988993125874827
iteration : 2815
train acc:  0.703125
train loss:  0.5550385117530823
train gradient:  0.1254792150964876
iteration : 2816
train acc:  0.7578125
train loss:  0.494768887758255
train gradient:  0.12238881076665124
iteration : 2817
train acc:  0.7109375
train loss:  0.5029988288879395
train gradient:  0.13540418251683645
iteration : 2818
train acc:  0.7109375
train loss:  0.5233123302459717
train gradient:  0.1278237770053225
iteration : 2819
train acc:  0.734375
train loss:  0.5123453140258789
train gradient:  0.13448826080825269
iteration : 2820
train acc:  0.609375
train loss:  0.6600900888442993
train gradient:  0.22706522379879182
iteration : 2821
train acc:  0.7265625
train loss:  0.5631301403045654
train gradient:  0.17018299424801428
iteration : 2822
train acc:  0.625
train loss:  0.6013661623001099
train gradient:  0.24699767595087696
iteration : 2823
train acc:  0.7109375
train loss:  0.5454521179199219
train gradient:  0.1862103789850491
iteration : 2824
train acc:  0.7578125
train loss:  0.500227153301239
train gradient:  0.17674599335500296
iteration : 2825
train acc:  0.7109375
train loss:  0.5412684679031372
train gradient:  0.157717598148255
iteration : 2826
train acc:  0.71875
train loss:  0.5381684899330139
train gradient:  0.14920445629706602
iteration : 2827
train acc:  0.734375
train loss:  0.5672601461410522
train gradient:  0.18633474548595386
iteration : 2828
train acc:  0.71875
train loss:  0.5723243355751038
train gradient:  0.1991202357764903
iteration : 2829
train acc:  0.7421875
train loss:  0.5309808254241943
train gradient:  0.15186913982076167
iteration : 2830
train acc:  0.6875
train loss:  0.5461908578872681
train gradient:  0.1089605460182392
iteration : 2831
train acc:  0.765625
train loss:  0.4709555506706238
train gradient:  0.1603008490175889
iteration : 2832
train acc:  0.640625
train loss:  0.60918790102005
train gradient:  0.19735516988042662
iteration : 2833
train acc:  0.6953125
train loss:  0.5775076150894165
train gradient:  0.1448581392828453
iteration : 2834
train acc:  0.796875
train loss:  0.5010339021682739
train gradient:  0.17916130084312326
iteration : 2835
train acc:  0.6953125
train loss:  0.5267293453216553
train gradient:  0.17581499700395828
iteration : 2836
train acc:  0.6953125
train loss:  0.5487714409828186
train gradient:  0.1396625711982714
iteration : 2837
train acc:  0.8125
train loss:  0.419685959815979
train gradient:  0.1065333137799559
iteration : 2838
train acc:  0.71875
train loss:  0.5402636528015137
train gradient:  0.15472633535809474
iteration : 2839
train acc:  0.6328125
train loss:  0.6222153902053833
train gradient:  0.1700518327939106
iteration : 2840
train acc:  0.671875
train loss:  0.5527277588844299
train gradient:  0.19466010284189195
iteration : 2841
train acc:  0.6484375
train loss:  0.6009883880615234
train gradient:  0.19275833408023435
iteration : 2842
train acc:  0.75
train loss:  0.5231157541275024
train gradient:  0.1721778433602112
iteration : 2843
train acc:  0.734375
train loss:  0.5197861194610596
train gradient:  0.142125659402957
iteration : 2844
train acc:  0.6953125
train loss:  0.5579903721809387
train gradient:  0.19185605358653113
iteration : 2845
train acc:  0.7265625
train loss:  0.5477346777915955
train gradient:  0.1426641985307825
iteration : 2846
train acc:  0.703125
train loss:  0.5564590692520142
train gradient:  0.2329265129152106
iteration : 2847
train acc:  0.7890625
train loss:  0.46709883213043213
train gradient:  0.09796476012349405
iteration : 2848
train acc:  0.7578125
train loss:  0.49559497833251953
train gradient:  0.12961189013878108
iteration : 2849
train acc:  0.765625
train loss:  0.5342345237731934
train gradient:  0.16059838956101696
iteration : 2850
train acc:  0.71875
train loss:  0.5043070316314697
train gradient:  0.13213749856434842
iteration : 2851
train acc:  0.7421875
train loss:  0.5263212323188782
train gradient:  0.20804037914393142
iteration : 2852
train acc:  0.703125
train loss:  0.5779730677604675
train gradient:  0.1594653763752888
iteration : 2853
train acc:  0.7890625
train loss:  0.48819583654403687
train gradient:  0.14146316510958398
iteration : 2854
train acc:  0.703125
train loss:  0.5148096084594727
train gradient:  0.12689277955636402
iteration : 2855
train acc:  0.671875
train loss:  0.5644057393074036
train gradient:  0.1550622321032804
iteration : 2856
train acc:  0.6875
train loss:  0.5770571231842041
train gradient:  0.20392564189852747
iteration : 2857
train acc:  0.703125
train loss:  0.5108685493469238
train gradient:  0.1296797235620334
iteration : 2858
train acc:  0.734375
train loss:  0.46853533387184143
train gradient:  0.1438674278770481
iteration : 2859
train acc:  0.640625
train loss:  0.563925564289093
train gradient:  0.15077226344940892
iteration : 2860
train acc:  0.7578125
train loss:  0.46513909101486206
train gradient:  0.11123560461278365
iteration : 2861
train acc:  0.75
train loss:  0.5025253891944885
train gradient:  0.13175967770602548
iteration : 2862
train acc:  0.71875
train loss:  0.5468841791152954
train gradient:  0.19606397006068507
iteration : 2863
train acc:  0.6875
train loss:  0.5829454660415649
train gradient:  0.17639687455216563
iteration : 2864
train acc:  0.6328125
train loss:  0.601143479347229
train gradient:  0.2253983420502459
iteration : 2865
train acc:  0.7109375
train loss:  0.5347572565078735
train gradient:  0.12854524799915978
iteration : 2866
train acc:  0.6640625
train loss:  0.5750913619995117
train gradient:  0.1851471664907121
iteration : 2867
train acc:  0.671875
train loss:  0.5676575303077698
train gradient:  0.17130571049558335
iteration : 2868
train acc:  0.7265625
train loss:  0.4713771939277649
train gradient:  0.15329415831093954
iteration : 2869
train acc:  0.75
train loss:  0.4969157576560974
train gradient:  0.16520085225162848
iteration : 2870
train acc:  0.78125
train loss:  0.48568400740623474
train gradient:  0.12195089748135648
iteration : 2871
train acc:  0.6484375
train loss:  0.5730187892913818
train gradient:  0.19143369652729836
iteration : 2872
train acc:  0.7578125
train loss:  0.5121376514434814
train gradient:  0.12836864232195394
iteration : 2873
train acc:  0.6875
train loss:  0.5514532327651978
train gradient:  0.15552140729426484
iteration : 2874
train acc:  0.7109375
train loss:  0.5468010902404785
train gradient:  0.16395706565013568
iteration : 2875
train acc:  0.765625
train loss:  0.5338433980941772
train gradient:  0.1559882163157303
iteration : 2876
train acc:  0.734375
train loss:  0.5446008443832397
train gradient:  0.15913222819059641
iteration : 2877
train acc:  0.6640625
train loss:  0.5682363510131836
train gradient:  0.18643584512826378
iteration : 2878
train acc:  0.734375
train loss:  0.5065087676048279
train gradient:  0.11757156620395963
iteration : 2879
train acc:  0.7265625
train loss:  0.5633087158203125
train gradient:  0.3042577714609246
iteration : 2880
train acc:  0.703125
train loss:  0.544248104095459
train gradient:  0.12372908419433999
iteration : 2881
train acc:  0.75
train loss:  0.48702341318130493
train gradient:  0.13409892547824317
iteration : 2882
train acc:  0.765625
train loss:  0.49627986550331116
train gradient:  0.13532676745442818
iteration : 2883
train acc:  0.75
train loss:  0.49870383739471436
train gradient:  0.14265720001258747
iteration : 2884
train acc:  0.703125
train loss:  0.5180628895759583
train gradient:  0.14366361046388648
iteration : 2885
train acc:  0.6953125
train loss:  0.6082800626754761
train gradient:  0.222537139161597
iteration : 2886
train acc:  0.7421875
train loss:  0.5284911394119263
train gradient:  0.14021042047771942
iteration : 2887
train acc:  0.7109375
train loss:  0.5540264844894409
train gradient:  0.1686263150433444
iteration : 2888
train acc:  0.78125
train loss:  0.52638840675354
train gradient:  0.1528370979357778
iteration : 2889
train acc:  0.796875
train loss:  0.4763602018356323
train gradient:  0.13369178646579505
iteration : 2890
train acc:  0.6953125
train loss:  0.5553910732269287
train gradient:  0.20713623192756753
iteration : 2891
train acc:  0.6953125
train loss:  0.5334872007369995
train gradient:  0.1615816689500125
iteration : 2892
train acc:  0.7578125
train loss:  0.5225079655647278
train gradient:  0.1302064332981151
iteration : 2893
train acc:  0.6796875
train loss:  0.5614749789237976
train gradient:  0.15192065997242543
iteration : 2894
train acc:  0.6875
train loss:  0.5708814263343811
train gradient:  0.1368471031454756
iteration : 2895
train acc:  0.6953125
train loss:  0.5611977577209473
train gradient:  0.1737789698454203
iteration : 2896
train acc:  0.703125
train loss:  0.5406966805458069
train gradient:  0.1454784828358183
iteration : 2897
train acc:  0.6484375
train loss:  0.5787070989608765
train gradient:  0.22834074121746117
iteration : 2898
train acc:  0.7578125
train loss:  0.4862116575241089
train gradient:  0.13339958017732662
iteration : 2899
train acc:  0.7265625
train loss:  0.5309291481971741
train gradient:  0.15299855000110363
iteration : 2900
train acc:  0.6875
train loss:  0.6050025224685669
train gradient:  0.24290643488275793
iteration : 2901
train acc:  0.78125
train loss:  0.4787130355834961
train gradient:  0.14000372595926297
iteration : 2902
train acc:  0.6796875
train loss:  0.5428524017333984
train gradient:  0.13268811588018958
iteration : 2903
train acc:  0.671875
train loss:  0.5881739854812622
train gradient:  0.2106124415770041
iteration : 2904
train acc:  0.6796875
train loss:  0.5921382308006287
train gradient:  0.17904327460427719
iteration : 2905
train acc:  0.7265625
train loss:  0.5202826261520386
train gradient:  0.26615325779591553
iteration : 2906
train acc:  0.78125
train loss:  0.45097616314888
train gradient:  0.11378918960874414
iteration : 2907
train acc:  0.671875
train loss:  0.564149022102356
train gradient:  0.15901549156472375
iteration : 2908
train acc:  0.7265625
train loss:  0.49579381942749023
train gradient:  0.16718468942880732
iteration : 2909
train acc:  0.7265625
train loss:  0.5571861863136292
train gradient:  0.15332579519771197
iteration : 2910
train acc:  0.7421875
train loss:  0.5423637628555298
train gradient:  0.16034427640290624
iteration : 2911
train acc:  0.7265625
train loss:  0.5565896034240723
train gradient:  0.17318363609470805
iteration : 2912
train acc:  0.7109375
train loss:  0.528866708278656
train gradient:  0.1848613867650728
iteration : 2913
train acc:  0.7109375
train loss:  0.5453706383705139
train gradient:  0.15937411906638857
iteration : 2914
train acc:  0.734375
train loss:  0.5517350435256958
train gradient:  0.15776121437708635
iteration : 2915
train acc:  0.7578125
train loss:  0.5211824178695679
train gradient:  0.16718058179791748
iteration : 2916
train acc:  0.765625
train loss:  0.45888325572013855
train gradient:  0.16016676009972158
iteration : 2917
train acc:  0.7421875
train loss:  0.48896101117134094
train gradient:  0.15090010304807497
iteration : 2918
train acc:  0.75
train loss:  0.4895976781845093
train gradient:  0.1309471262527267
iteration : 2919
train acc:  0.6484375
train loss:  0.5649285912513733
train gradient:  0.14865837313694152
iteration : 2920
train acc:  0.71875
train loss:  0.542178750038147
train gradient:  0.14881514096962567
iteration : 2921
train acc:  0.7109375
train loss:  0.4992232918739319
train gradient:  0.15715741513185658
iteration : 2922
train acc:  0.6796875
train loss:  0.52149897813797
train gradient:  0.12093221098539375
iteration : 2923
train acc:  0.734375
train loss:  0.5734854936599731
train gradient:  0.15779235786795703
iteration : 2924
train acc:  0.6875
train loss:  0.5349234938621521
train gradient:  0.1728896759366496
iteration : 2925
train acc:  0.734375
train loss:  0.5522762537002563
train gradient:  0.15873203189962826
iteration : 2926
train acc:  0.6875
train loss:  0.6076050400733948
train gradient:  0.1820381677408741
iteration : 2927
train acc:  0.71875
train loss:  0.5101503729820251
train gradient:  0.1399311855407849
iteration : 2928
train acc:  0.6484375
train loss:  0.6043585538864136
train gradient:  0.19324482902839907
iteration : 2929
train acc:  0.640625
train loss:  0.5988654494285583
train gradient:  0.15898431404039287
iteration : 2930
train acc:  0.6953125
train loss:  0.5655676126480103
train gradient:  0.15797336032611517
iteration : 2931
train acc:  0.640625
train loss:  0.5946319699287415
train gradient:  0.23739880127675972
iteration : 2932
train acc:  0.7109375
train loss:  0.5412071347236633
train gradient:  0.17690136400460219
iteration : 2933
train acc:  0.6953125
train loss:  0.5375147461891174
train gradient:  0.12640425669785604
iteration : 2934
train acc:  0.7265625
train loss:  0.5215246081352234
train gradient:  0.1498841861331892
iteration : 2935
train acc:  0.7421875
train loss:  0.6177302002906799
train gradient:  0.1903944661771103
iteration : 2936
train acc:  0.71875
train loss:  0.513899564743042
train gradient:  0.1704068019104582
iteration : 2937
train acc:  0.734375
train loss:  0.566804051399231
train gradient:  0.1433169816560066
iteration : 2938
train acc:  0.71875
train loss:  0.5708893537521362
train gradient:  0.16047631282441527
iteration : 2939
train acc:  0.6640625
train loss:  0.5975040197372437
train gradient:  0.2138953755363749
iteration : 2940
train acc:  0.7734375
train loss:  0.5189509391784668
train gradient:  0.14039103001722608
iteration : 2941
train acc:  0.7734375
train loss:  0.47910481691360474
train gradient:  0.12920473489656867
iteration : 2942
train acc:  0.6953125
train loss:  0.5373826026916504
train gradient:  0.14582250745910408
iteration : 2943
train acc:  0.7890625
train loss:  0.5014910697937012
train gradient:  0.14678246688531266
iteration : 2944
train acc:  0.7265625
train loss:  0.5337411761283875
train gradient:  0.14433754865490458
iteration : 2945
train acc:  0.671875
train loss:  0.5503247976303101
train gradient:  0.15959507128927236
iteration : 2946
train acc:  0.6875
train loss:  0.595109224319458
train gradient:  0.194174693399839
iteration : 2947
train acc:  0.6796875
train loss:  0.5593452453613281
train gradient:  0.16183386823391543
iteration : 2948
train acc:  0.7578125
train loss:  0.5000715255737305
train gradient:  0.1552810970562396
iteration : 2949
train acc:  0.734375
train loss:  0.4998568594455719
train gradient:  0.12695707553371438
iteration : 2950
train acc:  0.6171875
train loss:  0.6218464374542236
train gradient:  0.2222735686321804
iteration : 2951
train acc:  0.7109375
train loss:  0.5157576203346252
train gradient:  0.13103017689863114
iteration : 2952
train acc:  0.796875
train loss:  0.48576706647872925
train gradient:  0.13584515999140082
iteration : 2953
train acc:  0.8125
train loss:  0.4716959595680237
train gradient:  0.1300328366844758
iteration : 2954
train acc:  0.640625
train loss:  0.5570293664932251
train gradient:  0.13561741877349376
iteration : 2955
train acc:  0.6796875
train loss:  0.5698890686035156
train gradient:  0.15878107431059238
iteration : 2956
train acc:  0.75
train loss:  0.49883443117141724
train gradient:  0.15725058887190083
iteration : 2957
train acc:  0.734375
train loss:  0.48892462253570557
train gradient:  0.16685264598224117
iteration : 2958
train acc:  0.7265625
train loss:  0.5220376253128052
train gradient:  0.15310497449257
iteration : 2959
train acc:  0.71875
train loss:  0.5669729709625244
train gradient:  0.1539260460985868
iteration : 2960
train acc:  0.734375
train loss:  0.5229679346084595
train gradient:  0.11566022755979968
iteration : 2961
train acc:  0.7265625
train loss:  0.5004919767379761
train gradient:  0.12042518245097614
iteration : 2962
train acc:  0.6796875
train loss:  0.5278793573379517
train gradient:  0.1529154193035305
iteration : 2963
train acc:  0.6953125
train loss:  0.5638525485992432
train gradient:  0.1541576367007579
iteration : 2964
train acc:  0.6484375
train loss:  0.5643222332000732
train gradient:  0.16901605720563445
iteration : 2965
train acc:  0.734375
train loss:  0.5494396090507507
train gradient:  0.12719440460968512
iteration : 2966
train acc:  0.765625
train loss:  0.49258795380592346
train gradient:  0.1491084920139819
iteration : 2967
train acc:  0.6796875
train loss:  0.5074806809425354
train gradient:  0.12669341408711804
iteration : 2968
train acc:  0.6640625
train loss:  0.5671882629394531
train gradient:  0.16354664971010516
iteration : 2969
train acc:  0.6796875
train loss:  0.5316476225852966
train gradient:  0.14562127409906128
iteration : 2970
train acc:  0.6640625
train loss:  0.5470467805862427
train gradient:  0.15663305666305757
iteration : 2971
train acc:  0.7421875
train loss:  0.5300847887992859
train gradient:  0.1354631952058357
iteration : 2972
train acc:  0.640625
train loss:  0.6161067485809326
train gradient:  0.1858038636780494
iteration : 2973
train acc:  0.7265625
train loss:  0.5166129469871521
train gradient:  0.1536988676130246
iteration : 2974
train acc:  0.640625
train loss:  0.563826858997345
train gradient:  0.1489235942558223
iteration : 2975
train acc:  0.65625
train loss:  0.5606777667999268
train gradient:  0.1657908124407258
iteration : 2976
train acc:  0.765625
train loss:  0.46623098850250244
train gradient:  0.131546756605312
iteration : 2977
train acc:  0.7421875
train loss:  0.5353733897209167
train gradient:  0.16092654875432913
iteration : 2978
train acc:  0.765625
train loss:  0.4968379735946655
train gradient:  0.1690854657527774
iteration : 2979
train acc:  0.6953125
train loss:  0.5386710166931152
train gradient:  0.1601716705376556
iteration : 2980
train acc:  0.6640625
train loss:  0.593705415725708
train gradient:  0.2434945471385307
iteration : 2981
train acc:  0.7734375
train loss:  0.5203120112419128
train gradient:  0.1351660263636212
iteration : 2982
train acc:  0.65625
train loss:  0.5967438817024231
train gradient:  0.20026513281568165
iteration : 2983
train acc:  0.6875
train loss:  0.5344364643096924
train gradient:  0.1626614130436148
iteration : 2984
train acc:  0.734375
train loss:  0.5013061165809631
train gradient:  0.1110477796540713
iteration : 2985
train acc:  0.7109375
train loss:  0.5432868599891663
train gradient:  0.1521555435190891
iteration : 2986
train acc:  0.7109375
train loss:  0.5530756711959839
train gradient:  0.12484693057901487
iteration : 2987
train acc:  0.7265625
train loss:  0.5324641466140747
train gradient:  0.13292020355578507
iteration : 2988
train acc:  0.6953125
train loss:  0.5770789384841919
train gradient:  0.17269336396116594
iteration : 2989
train acc:  0.7109375
train loss:  0.5583083629608154
train gradient:  0.18943662168512104
iteration : 2990
train acc:  0.6953125
train loss:  0.5398774147033691
train gradient:  0.14487010049040935
iteration : 2991
train acc:  0.75
train loss:  0.5037006139755249
train gradient:  0.19498024032503186
iteration : 2992
train acc:  0.734375
train loss:  0.5424871444702148
train gradient:  0.183344590399674
iteration : 2993
train acc:  0.7265625
train loss:  0.5048568248748779
train gradient:  0.16415400473292696
iteration : 2994
train acc:  0.7265625
train loss:  0.5767495036125183
train gradient:  0.1910894767978903
iteration : 2995
train acc:  0.75
train loss:  0.5033687353134155
train gradient:  0.14469166317058657
iteration : 2996
train acc:  0.75
train loss:  0.5343225002288818
train gradient:  0.17894599914136328
iteration : 2997
train acc:  0.7578125
train loss:  0.5135072469711304
train gradient:  0.1319724540271409
iteration : 2998
train acc:  0.734375
train loss:  0.49598225951194763
train gradient:  0.1967264131548903
iteration : 2999
train acc:  0.78125
train loss:  0.5014725923538208
train gradient:  0.1567170909882403
iteration : 3000
train acc:  0.6875
train loss:  0.5681024789810181
train gradient:  0.1325688003790932
iteration : 3001
train acc:  0.7890625
train loss:  0.45729026198387146
train gradient:  0.12832012901630704
iteration : 3002
train acc:  0.6640625
train loss:  0.5678065419197083
train gradient:  0.13713414628299914
iteration : 3003
train acc:  0.7734375
train loss:  0.4562339782714844
train gradient:  0.13074271814673916
iteration : 3004
train acc:  0.7109375
train loss:  0.5005190372467041
train gradient:  0.21489156272886864
iteration : 3005
train acc:  0.6875
train loss:  0.5645373463630676
train gradient:  0.18848974863421508
iteration : 3006
train acc:  0.7734375
train loss:  0.5343359708786011
train gradient:  0.1376201548752642
iteration : 3007
train acc:  0.734375
train loss:  0.5029748678207397
train gradient:  0.12551182445330758
iteration : 3008
train acc:  0.7421875
train loss:  0.5123108625411987
train gradient:  0.11895712851194729
iteration : 3009
train acc:  0.7578125
train loss:  0.5558226108551025
train gradient:  0.196861752415379
iteration : 3010
train acc:  0.6328125
train loss:  0.6771880388259888
train gradient:  0.28871983193632667
iteration : 3011
train acc:  0.7265625
train loss:  0.5624727606773376
train gradient:  0.1861996178077976
iteration : 3012
train acc:  0.765625
train loss:  0.5235162377357483
train gradient:  0.1520294921765208
iteration : 3013
train acc:  0.703125
train loss:  0.5687037706375122
train gradient:  0.19680566516863796
iteration : 3014
train acc:  0.7265625
train loss:  0.5201817154884338
train gradient:  0.2069574838899626
iteration : 3015
train acc:  0.734375
train loss:  0.5165431499481201
train gradient:  0.14866059625298061
iteration : 3016
train acc:  0.7734375
train loss:  0.46773725748062134
train gradient:  0.1255755114548942
iteration : 3017
train acc:  0.734375
train loss:  0.48552432656288147
train gradient:  0.14080759847228266
iteration : 3018
train acc:  0.734375
train loss:  0.5084139704704285
train gradient:  0.16085840486575975
iteration : 3019
train acc:  0.7265625
train loss:  0.5299364328384399
train gradient:  0.14476309943733706
iteration : 3020
train acc:  0.75
train loss:  0.5127614736557007
train gradient:  0.11363720890168745
iteration : 3021
train acc:  0.640625
train loss:  0.6012243628501892
train gradient:  0.16188346408244403
iteration : 3022
train acc:  0.765625
train loss:  0.4776461124420166
train gradient:  0.12317184789910973
iteration : 3023
train acc:  0.734375
train loss:  0.5676596760749817
train gradient:  0.15244270171083757
iteration : 3024
train acc:  0.65625
train loss:  0.6280087828636169
train gradient:  0.21223386679968043
iteration : 3025
train acc:  0.6484375
train loss:  0.5839725732803345
train gradient:  0.16667578171083802
iteration : 3026
train acc:  0.6875
train loss:  0.5835486054420471
train gradient:  0.13619711270276486
iteration : 3027
train acc:  0.765625
train loss:  0.47760048508644104
train gradient:  0.13904747804997386
iteration : 3028
train acc:  0.6875
train loss:  0.5249107480049133
train gradient:  0.17619289393552723
iteration : 3029
train acc:  0.7734375
train loss:  0.4887453615665436
train gradient:  0.13463250258495046
iteration : 3030
train acc:  0.7265625
train loss:  0.5249086618423462
train gradient:  0.12949315782101456
iteration : 3031
train acc:  0.703125
train loss:  0.5217039585113525
train gradient:  0.16712585967044896
iteration : 3032
train acc:  0.765625
train loss:  0.500715434551239
train gradient:  0.16292189579510163
iteration : 3033
train acc:  0.703125
train loss:  0.5906537175178528
train gradient:  0.17442664255258458
iteration : 3034
train acc:  0.6484375
train loss:  0.6227208375930786
train gradient:  0.2104681970664301
iteration : 3035
train acc:  0.7421875
train loss:  0.5173323154449463
train gradient:  0.16257237550021914
iteration : 3036
train acc:  0.7265625
train loss:  0.5057276487350464
train gradient:  0.1497421860970292
iteration : 3037
train acc:  0.703125
train loss:  0.5197544097900391
train gradient:  0.13980141119992104
iteration : 3038
train acc:  0.7265625
train loss:  0.558068037033081
train gradient:  0.21156506501614308
iteration : 3039
train acc:  0.6875
train loss:  0.5454714298248291
train gradient:  0.16902837274429128
iteration : 3040
train acc:  0.6953125
train loss:  0.5182529091835022
train gradient:  0.1737078622381616
iteration : 3041
train acc:  0.75
train loss:  0.513721227645874
train gradient:  0.12854850804410617
iteration : 3042
train acc:  0.734375
train loss:  0.5586650371551514
train gradient:  0.20601588930388015
iteration : 3043
train acc:  0.7109375
train loss:  0.5050415992736816
train gradient:  0.16397963071660931
iteration : 3044
train acc:  0.734375
train loss:  0.5434839725494385
train gradient:  0.20715562767824786
iteration : 3045
train acc:  0.7265625
train loss:  0.5097389221191406
train gradient:  0.1271375967791294
iteration : 3046
train acc:  0.6875
train loss:  0.4704168438911438
train gradient:  0.10945587084473421
iteration : 3047
train acc:  0.7265625
train loss:  0.49273431301116943
train gradient:  0.13045569703866447
iteration : 3048
train acc:  0.7109375
train loss:  0.5201234817504883
train gradient:  0.1456551631998134
iteration : 3049
train acc:  0.6640625
train loss:  0.6041927337646484
train gradient:  0.20311157099126148
iteration : 3050
train acc:  0.7109375
train loss:  0.5205497145652771
train gradient:  0.17041641500505617
iteration : 3051
train acc:  0.78125
train loss:  0.48711538314819336
train gradient:  0.16525032013850677
iteration : 3052
train acc:  0.671875
train loss:  0.5811553001403809
train gradient:  0.16303321696924516
iteration : 3053
train acc:  0.734375
train loss:  0.5093079209327698
train gradient:  0.14717476659225373
iteration : 3054
train acc:  0.75
train loss:  0.4788540005683899
train gradient:  0.11816736589300296
iteration : 3055
train acc:  0.71875
train loss:  0.5626983642578125
train gradient:  0.1461892385252186
iteration : 3056
train acc:  0.671875
train loss:  0.5918934345245361
train gradient:  0.20103943515200362
iteration : 3057
train acc:  0.765625
train loss:  0.527219295501709
train gradient:  0.1449812521003534
iteration : 3058
train acc:  0.6953125
train loss:  0.4908398687839508
train gradient:  0.1389775557300247
iteration : 3059
train acc:  0.65625
train loss:  0.5573682188987732
train gradient:  0.19857468277330037
iteration : 3060
train acc:  0.6875
train loss:  0.5656685829162598
train gradient:  0.183870251749286
iteration : 3061
train acc:  0.65625
train loss:  0.5772626399993896
train gradient:  0.21327432450567463
iteration : 3062
train acc:  0.796875
train loss:  0.46024852991104126
train gradient:  0.13133323268905717
iteration : 3063
train acc:  0.6796875
train loss:  0.524046778678894
train gradient:  0.12021580745322877
iteration : 3064
train acc:  0.734375
train loss:  0.5277801752090454
train gradient:  0.18528303624573988
iteration : 3065
train acc:  0.6953125
train loss:  0.5776355266571045
train gradient:  0.18339227348886483
iteration : 3066
train acc:  0.71875
train loss:  0.49649757146835327
train gradient:  0.13055020248575291
iteration : 3067
train acc:  0.78125
train loss:  0.49054908752441406
train gradient:  0.1710598703866481
iteration : 3068
train acc:  0.765625
train loss:  0.4876938462257385
train gradient:  0.15887817986668476
iteration : 3069
train acc:  0.75
train loss:  0.5091773271560669
train gradient:  0.1645961073127965
iteration : 3070
train acc:  0.6875
train loss:  0.5498403310775757
train gradient:  0.14068145699356516
iteration : 3071
train acc:  0.7265625
train loss:  0.545574426651001
train gradient:  0.1735954656120336
iteration : 3072
train acc:  0.6875
train loss:  0.5002422332763672
train gradient:  0.14295123358443426
iteration : 3073
train acc:  0.671875
train loss:  0.5752172470092773
train gradient:  0.18152824833557063
iteration : 3074
train acc:  0.671875
train loss:  0.5926731824874878
train gradient:  0.17854465294359156
iteration : 3075
train acc:  0.7421875
train loss:  0.5096608996391296
train gradient:  0.14681576829306447
iteration : 3076
train acc:  0.6640625
train loss:  0.5796353220939636
train gradient:  0.16518913862791162
iteration : 3077
train acc:  0.71875
train loss:  0.5357944965362549
train gradient:  0.14502922045900946
iteration : 3078
train acc:  0.6953125
train loss:  0.5840203166007996
train gradient:  0.17459322875813804
iteration : 3079
train acc:  0.6875
train loss:  0.5837904214859009
train gradient:  0.1753807394563696
iteration : 3080
train acc:  0.734375
train loss:  0.5285193920135498
train gradient:  0.20996271902586638
iteration : 3081
train acc:  0.6796875
train loss:  0.563683807849884
train gradient:  0.1467911038747724
iteration : 3082
train acc:  0.640625
train loss:  0.5839804410934448
train gradient:  0.27992776096066535
iteration : 3083
train acc:  0.6875
train loss:  0.5683631896972656
train gradient:  0.20638514801205884
iteration : 3084
train acc:  0.671875
train loss:  0.5731853246688843
train gradient:  0.16719520564575507
iteration : 3085
train acc:  0.734375
train loss:  0.5205878019332886
train gradient:  0.15240730656791412
iteration : 3086
train acc:  0.65625
train loss:  0.5746123790740967
train gradient:  0.17220961595532386
iteration : 3087
train acc:  0.7578125
train loss:  0.5228172540664673
train gradient:  0.14614260144776414
iteration : 3088
train acc:  0.6640625
train loss:  0.5432701110839844
train gradient:  0.1699552452031163
iteration : 3089
train acc:  0.7109375
train loss:  0.5300381779670715
train gradient:  0.24598549585981588
iteration : 3090
train acc:  0.734375
train loss:  0.5218599438667297
train gradient:  0.1288909853569929
iteration : 3091
train acc:  0.71875
train loss:  0.5026070475578308
train gradient:  0.17398125078821
iteration : 3092
train acc:  0.7421875
train loss:  0.5669057965278625
train gradient:  0.18326419327030058
iteration : 3093
train acc:  0.703125
train loss:  0.5825982093811035
train gradient:  0.17941582226632774
iteration : 3094
train acc:  0.71875
train loss:  0.4939310550689697
train gradient:  0.12881962591294074
iteration : 3095
train acc:  0.7265625
train loss:  0.5173441767692566
train gradient:  0.11626430986561548
iteration : 3096
train acc:  0.6796875
train loss:  0.5891371965408325
train gradient:  0.21412122278379164
iteration : 3097
train acc:  0.7421875
train loss:  0.5317681431770325
train gradient:  0.14846787886921325
iteration : 3098
train acc:  0.7578125
train loss:  0.49107691645622253
train gradient:  0.14927467473209932
iteration : 3099
train acc:  0.703125
train loss:  0.5246185660362244
train gradient:  0.14714507902785118
iteration : 3100
train acc:  0.671875
train loss:  0.5625736713409424
train gradient:  0.2229926824464783
iteration : 3101
train acc:  0.703125
train loss:  0.510654866695404
train gradient:  0.1577253566271518
iteration : 3102
train acc:  0.7265625
train loss:  0.5262894630432129
train gradient:  0.13530012168490013
iteration : 3103
train acc:  0.71875
train loss:  0.5990817546844482
train gradient:  0.18254789249483475
iteration : 3104
train acc:  0.6640625
train loss:  0.5537471771240234
train gradient:  0.18216413832862138
iteration : 3105
train acc:  0.6875
train loss:  0.5506002306938171
train gradient:  0.13896053178702386
iteration : 3106
train acc:  0.796875
train loss:  0.4583662152290344
train gradient:  0.13361597702642392
iteration : 3107
train acc:  0.765625
train loss:  0.5197563171386719
train gradient:  0.17651596727746116
iteration : 3108
train acc:  0.75
train loss:  0.5436270833015442
train gradient:  0.1567542814054928
iteration : 3109
train acc:  0.703125
train loss:  0.5490649938583374
train gradient:  0.1833212115527334
iteration : 3110
train acc:  0.6484375
train loss:  0.5806829929351807
train gradient:  0.21078511942828693
iteration : 3111
train acc:  0.71875
train loss:  0.5511629581451416
train gradient:  0.2096079278267896
iteration : 3112
train acc:  0.7109375
train loss:  0.5371793508529663
train gradient:  0.14776538746007467
iteration : 3113
train acc:  0.6875
train loss:  0.5551902055740356
train gradient:  0.1257719867980668
iteration : 3114
train acc:  0.75
train loss:  0.5026077032089233
train gradient:  0.10567790859602917
iteration : 3115
train acc:  0.7578125
train loss:  0.48399853706359863
train gradient:  0.13979780043852308
iteration : 3116
train acc:  0.71875
train loss:  0.542729377746582
train gradient:  0.15856965679860427
iteration : 3117
train acc:  0.6953125
train loss:  0.5213814973831177
train gradient:  0.13150874216367575
iteration : 3118
train acc:  0.7421875
train loss:  0.5445666313171387
train gradient:  0.1851035496255251
iteration : 3119
train acc:  0.75
train loss:  0.5285509824752808
train gradient:  0.18545155902723526
iteration : 3120
train acc:  0.6953125
train loss:  0.5542423725128174
train gradient:  0.17205811643967717
iteration : 3121
train acc:  0.7421875
train loss:  0.4786812961101532
train gradient:  0.13207546656614888
iteration : 3122
train acc:  0.7421875
train loss:  0.5634844899177551
train gradient:  0.17276792895371146
iteration : 3123
train acc:  0.75
train loss:  0.5268641114234924
train gradient:  0.1326570452305376
iteration : 3124
train acc:  0.6796875
train loss:  0.5648996233940125
train gradient:  0.19123025835719165
iteration : 3125
train acc:  0.75
train loss:  0.5433493256568909
train gradient:  0.1596734361401049
iteration : 3126
train acc:  0.6796875
train loss:  0.5365814566612244
train gradient:  0.15361062109292928
iteration : 3127
train acc:  0.703125
train loss:  0.5181418657302856
train gradient:  0.1531248056111209
iteration : 3128
train acc:  0.734375
train loss:  0.502529501914978
train gradient:  0.16208126311190235
iteration : 3129
train acc:  0.7109375
train loss:  0.5386191606521606
train gradient:  0.1445621800128601
iteration : 3130
train acc:  0.671875
train loss:  0.5574350357055664
train gradient:  0.15488340417227797
iteration : 3131
train acc:  0.703125
train loss:  0.5590326189994812
train gradient:  0.20609728586968407
iteration : 3132
train acc:  0.6953125
train loss:  0.5337841510772705
train gradient:  0.15207093875957906
iteration : 3133
train acc:  0.765625
train loss:  0.48131537437438965
train gradient:  0.14142690153473658
iteration : 3134
train acc:  0.6640625
train loss:  0.5198053121566772
train gradient:  0.13607255899274778
iteration : 3135
train acc:  0.6953125
train loss:  0.5446425080299377
train gradient:  0.1571825593986831
iteration : 3136
train acc:  0.71875
train loss:  0.5406038761138916
train gradient:  0.2312557783599804
iteration : 3137
train acc:  0.78125
train loss:  0.4640069007873535
train gradient:  0.1752478698165187
iteration : 3138
train acc:  0.71875
train loss:  0.5463283061981201
train gradient:  0.1539572777520185
iteration : 3139
train acc:  0.6484375
train loss:  0.5801975727081299
train gradient:  0.16261603346947315
iteration : 3140
train acc:  0.671875
train loss:  0.5442414283752441
train gradient:  0.1662800893085456
iteration : 3141
train acc:  0.6796875
train loss:  0.5570802092552185
train gradient:  0.1979452426817146
iteration : 3142
train acc:  0.7421875
train loss:  0.5533344149589539
train gradient:  0.12795184173689
iteration : 3143
train acc:  0.65625
train loss:  0.5937117338180542
train gradient:  0.15072571580201713
iteration : 3144
train acc:  0.7265625
train loss:  0.5182098150253296
train gradient:  0.18469047225417523
iteration : 3145
train acc:  0.78125
train loss:  0.48314934968948364
train gradient:  0.14877735561290603
iteration : 3146
train acc:  0.71875
train loss:  0.5527758598327637
train gradient:  0.19766634166823888
iteration : 3147
train acc:  0.6796875
train loss:  0.5912629961967468
train gradient:  0.18815445483811427
iteration : 3148
train acc:  0.7109375
train loss:  0.5434934496879578
train gradient:  0.1508411476914935
iteration : 3149
train acc:  0.7734375
train loss:  0.5015413761138916
train gradient:  0.21789717675003473
iteration : 3150
train acc:  0.71875
train loss:  0.5037437677383423
train gradient:  0.14268306299158995
iteration : 3151
train acc:  0.75
train loss:  0.4974533021450043
train gradient:  0.1378316211926271
iteration : 3152
train acc:  0.71875
train loss:  0.5321873426437378
train gradient:  0.12368496942886283
iteration : 3153
train acc:  0.71875
train loss:  0.535443902015686
train gradient:  0.18855216136585712
iteration : 3154
train acc:  0.7265625
train loss:  0.5380187034606934
train gradient:  0.1166513697578241
iteration : 3155
train acc:  0.734375
train loss:  0.5140999555587769
train gradient:  0.14662818477474648
iteration : 3156
train acc:  0.7265625
train loss:  0.5293967723846436
train gradient:  0.15046276293762528
iteration : 3157
train acc:  0.7109375
train loss:  0.5087311267852783
train gradient:  0.1845301801154291
iteration : 3158
train acc:  0.6953125
train loss:  0.5200090408325195
train gradient:  0.1523390237501216
iteration : 3159
train acc:  0.7109375
train loss:  0.5338578224182129
train gradient:  0.18443103881571798
iteration : 3160
train acc:  0.6796875
train loss:  0.6061705946922302
train gradient:  0.1947826526252795
iteration : 3161
train acc:  0.7265625
train loss:  0.5332708358764648
train gradient:  0.17013402835490923
iteration : 3162
train acc:  0.6328125
train loss:  0.5977871417999268
train gradient:  0.20152149865497876
iteration : 3163
train acc:  0.6953125
train loss:  0.5683724284172058
train gradient:  0.18639666213901074
iteration : 3164
train acc:  0.7265625
train loss:  0.5342648029327393
train gradient:  0.146100453989161
iteration : 3165
train acc:  0.7421875
train loss:  0.5252946615219116
train gradient:  0.15341836453179558
iteration : 3166
train acc:  0.6796875
train loss:  0.5869859457015991
train gradient:  0.1766573498540637
iteration : 3167
train acc:  0.671875
train loss:  0.5568124651908875
train gradient:  0.17511721624612187
iteration : 3168
train acc:  0.765625
train loss:  0.47754907608032227
train gradient:  0.15757389365283753
iteration : 3169
train acc:  0.734375
train loss:  0.56749427318573
train gradient:  0.20136532136642837
iteration : 3170
train acc:  0.6953125
train loss:  0.5426617860794067
train gradient:  0.1292334107669726
iteration : 3171
train acc:  0.734375
train loss:  0.4847796857357025
train gradient:  0.12310827670091296
iteration : 3172
train acc:  0.7734375
train loss:  0.4955952763557434
train gradient:  0.14989257933229277
iteration : 3173
train acc:  0.6875
train loss:  0.558504581451416
train gradient:  0.18859456177144907
iteration : 3174
train acc:  0.7265625
train loss:  0.5279183387756348
train gradient:  0.1311955497737633
iteration : 3175
train acc:  0.71875
train loss:  0.5105839967727661
train gradient:  0.14423941698878529
iteration : 3176
train acc:  0.6953125
train loss:  0.5694382190704346
train gradient:  0.21088794862687105
iteration : 3177
train acc:  0.7109375
train loss:  0.5542224645614624
train gradient:  0.18625806059817518
iteration : 3178
train acc:  0.6484375
train loss:  0.5703203082084656
train gradient:  0.19258211592029678
iteration : 3179
train acc:  0.84375
train loss:  0.4552057981491089
train gradient:  0.15476000663004558
iteration : 3180
train acc:  0.7421875
train loss:  0.4892164170742035
train gradient:  0.16238607271671862
iteration : 3181
train acc:  0.7421875
train loss:  0.49187397956848145
train gradient:  0.15617402910379216
iteration : 3182
train acc:  0.75
train loss:  0.4982553720474243
train gradient:  0.1373975439941837
iteration : 3183
train acc:  0.65625
train loss:  0.5687676668167114
train gradient:  0.23215686944014546
iteration : 3184
train acc:  0.6875
train loss:  0.5784247517585754
train gradient:  0.1558287155957927
iteration : 3185
train acc:  0.7890625
train loss:  0.43836715817451477
train gradient:  0.13091992427098065
iteration : 3186
train acc:  0.6875
train loss:  0.5876253247261047
train gradient:  0.19161899394448317
iteration : 3187
train acc:  0.7265625
train loss:  0.4968518316745758
train gradient:  0.11268594173126087
iteration : 3188
train acc:  0.6953125
train loss:  0.5194031000137329
train gradient:  0.16853667110177256
iteration : 3189
train acc:  0.7421875
train loss:  0.5310965776443481
train gradient:  0.15743857542187167
iteration : 3190
train acc:  0.734375
train loss:  0.51164710521698
train gradient:  0.12848088919659012
iteration : 3191
train acc:  0.6640625
train loss:  0.5774887800216675
train gradient:  0.15692830265200802
iteration : 3192
train acc:  0.7109375
train loss:  0.5170031189918518
train gradient:  0.1521228667841232
iteration : 3193
train acc:  0.7109375
train loss:  0.5511574745178223
train gradient:  0.16209773671195815
iteration : 3194
train acc:  0.625
train loss:  0.6038459539413452
train gradient:  0.24096686743610693
iteration : 3195
train acc:  0.65625
train loss:  0.5748324394226074
train gradient:  0.19346470167776697
iteration : 3196
train acc:  0.7890625
train loss:  0.4623662829399109
train gradient:  0.16509173016389478
iteration : 3197
train acc:  0.6640625
train loss:  0.574184775352478
train gradient:  0.23004217547857722
iteration : 3198
train acc:  0.6875
train loss:  0.5791793465614319
train gradient:  0.1590463542588049
iteration : 3199
train acc:  0.765625
train loss:  0.5368898510932922
train gradient:  0.19836099113638012
iteration : 3200
train acc:  0.71875
train loss:  0.5102943778038025
train gradient:  0.1574566832733721
iteration : 3201
train acc:  0.65625
train loss:  0.5668689012527466
train gradient:  0.1700482824587573
iteration : 3202
train acc:  0.71875
train loss:  0.5180009007453918
train gradient:  0.17236320352734436
iteration : 3203
train acc:  0.734375
train loss:  0.5249617695808411
train gradient:  0.14089782990677951
iteration : 3204
train acc:  0.7578125
train loss:  0.5535846948623657
train gradient:  0.17309269041871445
iteration : 3205
train acc:  0.7578125
train loss:  0.5187919735908508
train gradient:  0.15767461336906474
iteration : 3206
train acc:  0.7265625
train loss:  0.5607037544250488
train gradient:  0.1621022615947385
iteration : 3207
train acc:  0.6796875
train loss:  0.5401504635810852
train gradient:  0.17954551287235831
iteration : 3208
train acc:  0.7734375
train loss:  0.4710896611213684
train gradient:  0.12948295213861272
iteration : 3209
train acc:  0.6171875
train loss:  0.5966988801956177
train gradient:  0.17524036017896175
iteration : 3210
train acc:  0.6328125
train loss:  0.5670574903488159
train gradient:  0.14824987179493176
iteration : 3211
train acc:  0.75
train loss:  0.5506950616836548
train gradient:  0.1351432258059137
iteration : 3212
train acc:  0.7578125
train loss:  0.5171760320663452
train gradient:  0.13357562251478877
iteration : 3213
train acc:  0.6953125
train loss:  0.5355116724967957
train gradient:  0.14319343036638627
iteration : 3214
train acc:  0.75
train loss:  0.536444365978241
train gradient:  0.17769420986676696
iteration : 3215
train acc:  0.71875
train loss:  0.5280951261520386
train gradient:  0.1725302119513066
iteration : 3216
train acc:  0.75
train loss:  0.49581122398376465
train gradient:  0.10833731802375078
iteration : 3217
train acc:  0.6640625
train loss:  0.6195994019508362
train gradient:  0.1907798802797097
iteration : 3218
train acc:  0.7109375
train loss:  0.5421724915504456
train gradient:  0.18640110234500704
iteration : 3219
train acc:  0.7109375
train loss:  0.5078845024108887
train gradient:  0.12905246636132145
iteration : 3220
train acc:  0.7421875
train loss:  0.5264505743980408
train gradient:  0.15913046087737226
iteration : 3221
train acc:  0.7265625
train loss:  0.5386947989463806
train gradient:  0.14783689904061942
iteration : 3222
train acc:  0.7421875
train loss:  0.47126299142837524
train gradient:  0.13472808841096068
iteration : 3223
train acc:  0.6640625
train loss:  0.5651884078979492
train gradient:  0.13695764950994532
iteration : 3224
train acc:  0.6875
train loss:  0.5872949957847595
train gradient:  0.16888213204468422
iteration : 3225
train acc:  0.7265625
train loss:  0.5106773376464844
train gradient:  0.15036875423245377
iteration : 3226
train acc:  0.8046875
train loss:  0.46937018632888794
train gradient:  0.1707118617071543
iteration : 3227
train acc:  0.7109375
train loss:  0.5478657484054565
train gradient:  0.13069599429489853
iteration : 3228
train acc:  0.6640625
train loss:  0.5731678605079651
train gradient:  0.14431889553928512
iteration : 3229
train acc:  0.7265625
train loss:  0.5119841694831848
train gradient:  0.1336840950359231
iteration : 3230
train acc:  0.703125
train loss:  0.5503474473953247
train gradient:  0.13282493541223653
iteration : 3231
train acc:  0.6484375
train loss:  0.5524604320526123
train gradient:  0.14144819313911772
iteration : 3232
train acc:  0.65625
train loss:  0.5588768720626831
train gradient:  0.14314984314821044
iteration : 3233
train acc:  0.6953125
train loss:  0.5545133948326111
train gradient:  0.1459141870787185
iteration : 3234
train acc:  0.6875
train loss:  0.560015857219696
train gradient:  0.18715998937098152
iteration : 3235
train acc:  0.65625
train loss:  0.5423473715782166
train gradient:  0.1756953696432154
iteration : 3236
train acc:  0.7109375
train loss:  0.5495809316635132
train gradient:  0.1801626061179735
iteration : 3237
train acc:  0.7734375
train loss:  0.5150526762008667
train gradient:  0.1452014320437977
iteration : 3238
train acc:  0.7109375
train loss:  0.5238305926322937
train gradient:  0.1437305427020083
iteration : 3239
train acc:  0.734375
train loss:  0.5207418203353882
train gradient:  0.12611457314957625
iteration : 3240
train acc:  0.71875
train loss:  0.5292730927467346
train gradient:  0.1379463074132773
iteration : 3241
train acc:  0.7578125
train loss:  0.48075300455093384
train gradient:  0.12285542854210388
iteration : 3242
train acc:  0.671875
train loss:  0.5655943155288696
train gradient:  0.1793681224115903
iteration : 3243
train acc:  0.671875
train loss:  0.5723801851272583
train gradient:  0.17366216758611797
iteration : 3244
train acc:  0.7265625
train loss:  0.5004222393035889
train gradient:  0.13969642600134724
iteration : 3245
train acc:  0.7890625
train loss:  0.4854319095611572
train gradient:  0.12876925761809616
iteration : 3246
train acc:  0.734375
train loss:  0.5187643766403198
train gradient:  0.11394783163901785
iteration : 3247
train acc:  0.640625
train loss:  0.6444966197013855
train gradient:  0.21904008577432707
iteration : 3248
train acc:  0.6640625
train loss:  0.5820309519767761
train gradient:  0.18694008492306288
iteration : 3249
train acc:  0.671875
train loss:  0.5761159658432007
train gradient:  0.1558030528675285
iteration : 3250
train acc:  0.6953125
train loss:  0.5827610492706299
train gradient:  0.20312763638316134
iteration : 3251
train acc:  0.6484375
train loss:  0.6098424792289734
train gradient:  0.22787809129007336
iteration : 3252
train acc:  0.734375
train loss:  0.533522367477417
train gradient:  0.1448676122509892
iteration : 3253
train acc:  0.734375
train loss:  0.5052125453948975
train gradient:  0.15048413605586436
iteration : 3254
train acc:  0.6953125
train loss:  0.578123927116394
train gradient:  0.16653483200930408
iteration : 3255
train acc:  0.6875
train loss:  0.5976346731185913
train gradient:  0.15646758603276603
iteration : 3256
train acc:  0.734375
train loss:  0.5198549032211304
train gradient:  0.19145446170572428
iteration : 3257
train acc:  0.734375
train loss:  0.47609636187553406
train gradient:  0.10912103673924485
iteration : 3258
train acc:  0.6953125
train loss:  0.5428258776664734
train gradient:  0.12236161406003071
iteration : 3259
train acc:  0.609375
train loss:  0.6097240447998047
train gradient:  0.19848383921210017
iteration : 3260
train acc:  0.71875
train loss:  0.564702033996582
train gradient:  0.13297718128688182
iteration : 3261
train acc:  0.7421875
train loss:  0.4760514795780182
train gradient:  0.13824136556122457
iteration : 3262
train acc:  0.7109375
train loss:  0.5444990992546082
train gradient:  0.18250276827801826
iteration : 3263
train acc:  0.75
train loss:  0.48826807737350464
train gradient:  0.13606905914488054
iteration : 3264
train acc:  0.71875
train loss:  0.5041612386703491
train gradient:  0.15825407922343765
iteration : 3265
train acc:  0.7578125
train loss:  0.56125408411026
train gradient:  0.14858046952196977
iteration : 3266
train acc:  0.671875
train loss:  0.5530012845993042
train gradient:  0.15407200253225017
iteration : 3267
train acc:  0.6875
train loss:  0.536933958530426
train gradient:  0.14673361340620203
iteration : 3268
train acc:  0.7890625
train loss:  0.5281982421875
train gradient:  0.15134356225968665
iteration : 3269
train acc:  0.71875
train loss:  0.5547229051589966
train gradient:  0.15171853047678685
iteration : 3270
train acc:  0.6953125
train loss:  0.5814273357391357
train gradient:  0.17038926534429494
iteration : 3271
train acc:  0.7578125
train loss:  0.5017557144165039
train gradient:  0.1819655217214665
iteration : 3272
train acc:  0.7265625
train loss:  0.5177115201950073
train gradient:  0.16022693689135167
iteration : 3273
train acc:  0.6796875
train loss:  0.5224325656890869
train gradient:  0.20084401457945902
iteration : 3274
train acc:  0.78125
train loss:  0.4887121319770813
train gradient:  0.1481825216590718
iteration : 3275
train acc:  0.7109375
train loss:  0.5248391032218933
train gradient:  0.15812364988519578
iteration : 3276
train acc:  0.6953125
train loss:  0.5614942312240601
train gradient:  0.14735763365019108
iteration : 3277
train acc:  0.75
train loss:  0.4903067946434021
train gradient:  0.12413508685307344
iteration : 3278
train acc:  0.7109375
train loss:  0.6004953384399414
train gradient:  0.17153328148011898
iteration : 3279
train acc:  0.765625
train loss:  0.5137443542480469
train gradient:  0.15862475126205422
iteration : 3280
train acc:  0.578125
train loss:  0.6325751543045044
train gradient:  0.19587282667249264
iteration : 3281
train acc:  0.765625
train loss:  0.49497300386428833
train gradient:  0.1556039374243584
iteration : 3282
train acc:  0.65625
train loss:  0.6060326099395752
train gradient:  0.20750461454951136
iteration : 3283
train acc:  0.7734375
train loss:  0.5156896114349365
train gradient:  0.11450150778782743
iteration : 3284
train acc:  0.703125
train loss:  0.5585667490959167
train gradient:  0.19379939529294737
iteration : 3285
train acc:  0.703125
train loss:  0.5165218114852905
train gradient:  0.12429101929027628
iteration : 3286
train acc:  0.75
train loss:  0.5234217047691345
train gradient:  0.1450176787298076
iteration : 3287
train acc:  0.6875
train loss:  0.5363956689834595
train gradient:  0.21438606243454006
iteration : 3288
train acc:  0.734375
train loss:  0.5459739565849304
train gradient:  0.11727283258257527
iteration : 3289
train acc:  0.6953125
train loss:  0.51087886095047
train gradient:  0.12923995294326274
iteration : 3290
train acc:  0.703125
train loss:  0.5197298526763916
train gradient:  0.13960909850046233
iteration : 3291
train acc:  0.75
train loss:  0.4848020672798157
train gradient:  0.1909497738527045
iteration : 3292
train acc:  0.734375
train loss:  0.4930700957775116
train gradient:  0.12567891051656513
iteration : 3293
train acc:  0.7421875
train loss:  0.5167921185493469
train gradient:  0.12203176213571242
iteration : 3294
train acc:  0.7109375
train loss:  0.5338788628578186
train gradient:  0.1300435669306085
iteration : 3295
train acc:  0.734375
train loss:  0.5086150169372559
train gradient:  0.11528746652888038
iteration : 3296
train acc:  0.734375
train loss:  0.5736973881721497
train gradient:  0.16460448718258164
iteration : 3297
train acc:  0.7734375
train loss:  0.5292172431945801
train gradient:  0.16746676857205667
iteration : 3298
train acc:  0.7578125
train loss:  0.5034868717193604
train gradient:  0.12599092201292503
iteration : 3299
train acc:  0.7421875
train loss:  0.5013102293014526
train gradient:  0.14933983458813882
iteration : 3300
train acc:  0.6875
train loss:  0.5448927283287048
train gradient:  0.1297864839292649
iteration : 3301
train acc:  0.6953125
train loss:  0.5072716474533081
train gradient:  0.15279313539230518
iteration : 3302
train acc:  0.8125
train loss:  0.46328240633010864
train gradient:  0.11798478594231492
iteration : 3303
train acc:  0.734375
train loss:  0.521073579788208
train gradient:  0.14357488257852674
iteration : 3304
train acc:  0.7421875
train loss:  0.5437616109848022
train gradient:  0.19656324998360236
iteration : 3305
train acc:  0.7421875
train loss:  0.5284110307693481
train gradient:  0.16798297235256138
iteration : 3306
train acc:  0.703125
train loss:  0.5413752794265747
train gradient:  0.181569381814761
iteration : 3307
train acc:  0.6796875
train loss:  0.5760607719421387
train gradient:  0.15615838711137048
iteration : 3308
train acc:  0.6640625
train loss:  0.5244842171669006
train gradient:  0.15136639668027668
iteration : 3309
train acc:  0.7109375
train loss:  0.5540958642959595
train gradient:  0.16449414033077026
iteration : 3310
train acc:  0.6875
train loss:  0.5693194270133972
train gradient:  0.1708487556724934
iteration : 3311
train acc:  0.671875
train loss:  0.5458388924598694
train gradient:  0.15336229606201185
iteration : 3312
train acc:  0.75
train loss:  0.5250223875045776
train gradient:  0.15857807481115932
iteration : 3313
train acc:  0.71875
train loss:  0.5584553480148315
train gradient:  0.14731383399987674
iteration : 3314
train acc:  0.640625
train loss:  0.6779119372367859
train gradient:  0.33824090445231786
iteration : 3315
train acc:  0.765625
train loss:  0.4335663914680481
train gradient:  0.18425450321819614
iteration : 3316
train acc:  0.7578125
train loss:  0.5250474214553833
train gradient:  0.20882576652909535
iteration : 3317
train acc:  0.7109375
train loss:  0.5430114269256592
train gradient:  0.15336605837506279
iteration : 3318
train acc:  0.7109375
train loss:  0.517536997795105
train gradient:  0.1271862129326124
iteration : 3319
train acc:  0.6796875
train loss:  0.5591757297515869
train gradient:  0.17559173180431054
iteration : 3320
train acc:  0.671875
train loss:  0.5927964448928833
train gradient:  0.23673091570383092
iteration : 3321
train acc:  0.84375
train loss:  0.4035062789916992
train gradient:  0.11844498904519019
iteration : 3322
train acc:  0.7109375
train loss:  0.5100724697113037
train gradient:  0.13710226153417174
iteration : 3323
train acc:  0.6875
train loss:  0.5211676955223083
train gradient:  0.13631789440138672
iteration : 3324
train acc:  0.7421875
train loss:  0.5044877529144287
train gradient:  0.16393775387040477
iteration : 3325
train acc:  0.71875
train loss:  0.5206260085105896
train gradient:  0.15172386264568105
iteration : 3326
train acc:  0.7421875
train loss:  0.47764039039611816
train gradient:  0.13727086342834327
iteration : 3327
train acc:  0.7265625
train loss:  0.5049099326133728
train gradient:  0.1549753118204057
iteration : 3328
train acc:  0.65625
train loss:  0.6207852363586426
train gradient:  0.21574223787424363
iteration : 3329
train acc:  0.7265625
train loss:  0.5128108859062195
train gradient:  0.1629982285942812
iteration : 3330
train acc:  0.6953125
train loss:  0.5234328508377075
train gradient:  0.15820007578427928
iteration : 3331
train acc:  0.7890625
train loss:  0.45774632692337036
train gradient:  0.17003686518006528
iteration : 3332
train acc:  0.7734375
train loss:  0.5322515964508057
train gradient:  0.15643312369887857
iteration : 3333
train acc:  0.7578125
train loss:  0.4694426655769348
train gradient:  0.13400563222460615
iteration : 3334
train acc:  0.7421875
train loss:  0.5739432573318481
train gradient:  0.17073029134766785
iteration : 3335
train acc:  0.6484375
train loss:  0.5954192876815796
train gradient:  0.17829004006928512
iteration : 3336
train acc:  0.703125
train loss:  0.5058804750442505
train gradient:  0.11192350655372693
iteration : 3337
train acc:  0.734375
train loss:  0.5641403198242188
train gradient:  0.16968274457947924
iteration : 3338
train acc:  0.734375
train loss:  0.5305452346801758
train gradient:  0.14993813599258893
iteration : 3339
train acc:  0.7265625
train loss:  0.5238362550735474
train gradient:  0.16882321199159067
iteration : 3340
train acc:  0.6953125
train loss:  0.5458998680114746
train gradient:  0.18405269577570543
iteration : 3341
train acc:  0.703125
train loss:  0.507193922996521
train gradient:  0.12643382870485131
iteration : 3342
train acc:  0.6953125
train loss:  0.5461959838867188
train gradient:  0.20289058408060212
iteration : 3343
train acc:  0.734375
train loss:  0.5094169974327087
train gradient:  0.18158500726116594
iteration : 3344
train acc:  0.6875
train loss:  0.5381850004196167
train gradient:  0.1460913027662611
iteration : 3345
train acc:  0.7109375
train loss:  0.5026926398277283
train gradient:  0.1305788139792001
iteration : 3346
train acc:  0.734375
train loss:  0.520244836807251
train gradient:  0.14382411107436746
iteration : 3347
train acc:  0.6875
train loss:  0.5447202920913696
train gradient:  0.18739322155694638
iteration : 3348
train acc:  0.7890625
train loss:  0.4835261106491089
train gradient:  0.1136743105677522
iteration : 3349
train acc:  0.734375
train loss:  0.5077019929885864
train gradient:  0.1603272816277532
iteration : 3350
train acc:  0.640625
train loss:  0.6171669960021973
train gradient:  0.2241341577628826
iteration : 3351
train acc:  0.6953125
train loss:  0.5373983383178711
train gradient:  0.18037128035609779
iteration : 3352
train acc:  0.734375
train loss:  0.4967449903488159
train gradient:  0.11307462448015386
iteration : 3353
train acc:  0.765625
train loss:  0.4592726230621338
train gradient:  0.13028258579639584
iteration : 3354
train acc:  0.6875
train loss:  0.538070559501648
train gradient:  0.17818042197759318
iteration : 3355
train acc:  0.7421875
train loss:  0.503844141960144
train gradient:  0.13544977375115697
iteration : 3356
train acc:  0.75
train loss:  0.4792914390563965
train gradient:  0.145913394315352
iteration : 3357
train acc:  0.75
train loss:  0.5353944301605225
train gradient:  0.1235476323404209
iteration : 3358
train acc:  0.6640625
train loss:  0.5726796388626099
train gradient:  0.19005407013272657
iteration : 3359
train acc:  0.7265625
train loss:  0.5557714104652405
train gradient:  0.18979950499366488
iteration : 3360
train acc:  0.671875
train loss:  0.5658165812492371
train gradient:  0.15623422713104776
iteration : 3361
train acc:  0.7421875
train loss:  0.5000976920127869
train gradient:  0.13151445007037915
iteration : 3362
train acc:  0.78125
train loss:  0.5106296539306641
train gradient:  0.14062665975188987
iteration : 3363
train acc:  0.71875
train loss:  0.5035915970802307
train gradient:  0.13756469113832637
iteration : 3364
train acc:  0.8046875
train loss:  0.5295314788818359
train gradient:  0.2014787473219036
iteration : 3365
train acc:  0.75
train loss:  0.5127449631690979
train gradient:  0.14373338023074383
iteration : 3366
train acc:  0.7265625
train loss:  0.5067298412322998
train gradient:  0.14592369124982824
iteration : 3367
train acc:  0.75
train loss:  0.4760878086090088
train gradient:  0.1356842587325336
iteration : 3368
train acc:  0.71875
train loss:  0.501065731048584
train gradient:  0.16664573763091758
iteration : 3369
train acc:  0.6953125
train loss:  0.5976130962371826
train gradient:  0.20711522439122076
iteration : 3370
train acc:  0.6953125
train loss:  0.5567461252212524
train gradient:  0.16356357472225339
iteration : 3371
train acc:  0.7265625
train loss:  0.5119760036468506
train gradient:  0.1460030773760121
iteration : 3372
train acc:  0.703125
train loss:  0.5351704359054565
train gradient:  0.13624477923652303
iteration : 3373
train acc:  0.6875
train loss:  0.5389672517776489
train gradient:  0.154152132920167
iteration : 3374
train acc:  0.71875
train loss:  0.5418074131011963
train gradient:  0.13827413086338175
iteration : 3375
train acc:  0.7421875
train loss:  0.4972291588783264
train gradient:  0.12472552248912182
iteration : 3376
train acc:  0.7734375
train loss:  0.4839184880256653
train gradient:  0.13033330207614846
iteration : 3377
train acc:  0.8046875
train loss:  0.46241191029548645
train gradient:  0.14755585546403718
iteration : 3378
train acc:  0.7421875
train loss:  0.47745323181152344
train gradient:  0.1408768343685326
iteration : 3379
train acc:  0.7109375
train loss:  0.5282635688781738
train gradient:  0.14307531833839873
iteration : 3380
train acc:  0.765625
train loss:  0.48982346057891846
train gradient:  0.12845282611990755
iteration : 3381
train acc:  0.75
train loss:  0.5586634874343872
train gradient:  0.1949813552566227
iteration : 3382
train acc:  0.7734375
train loss:  0.46982836723327637
train gradient:  0.12152931666071173
iteration : 3383
train acc:  0.6953125
train loss:  0.5444225668907166
train gradient:  0.1669755505144829
iteration : 3384
train acc:  0.671875
train loss:  0.5360013246536255
train gradient:  0.17965388323693737
iteration : 3385
train acc:  0.765625
train loss:  0.48605650663375854
train gradient:  0.14437039680268804
iteration : 3386
train acc:  0.78125
train loss:  0.4467209577560425
train gradient:  0.10997417171210898
iteration : 3387
train acc:  0.703125
train loss:  0.5467173457145691
train gradient:  0.1686695753952412
iteration : 3388
train acc:  0.6640625
train loss:  0.5615495443344116
train gradient:  0.1603605349784648
iteration : 3389
train acc:  0.7421875
train loss:  0.5141651034355164
train gradient:  0.13039443954852462
iteration : 3390
train acc:  0.65625
train loss:  0.5713843107223511
train gradient:  0.19462235753079887
iteration : 3391
train acc:  0.6796875
train loss:  0.5420281291007996
train gradient:  0.15843244011578075
iteration : 3392
train acc:  0.671875
train loss:  0.5648003220558167
train gradient:  0.19988750932021532
iteration : 3393
train acc:  0.8203125
train loss:  0.43665868043899536
train gradient:  0.10377491543707548
iteration : 3394
train acc:  0.7109375
train loss:  0.5466330051422119
train gradient:  0.15609417349563282
iteration : 3395
train acc:  0.6953125
train loss:  0.5979742407798767
train gradient:  0.2112135741338989
iteration : 3396
train acc:  0.7578125
train loss:  0.45516437292099
train gradient:  0.1705553528420619
iteration : 3397
train acc:  0.7265625
train loss:  0.48999083042144775
train gradient:  0.12958396508644365
iteration : 3398
train acc:  0.7109375
train loss:  0.5683673620223999
train gradient:  0.15233884581582124
iteration : 3399
train acc:  0.71875
train loss:  0.5490886569023132
train gradient:  0.18404509918541695
iteration : 3400
train acc:  0.6875
train loss:  0.5698361992835999
train gradient:  0.1871516990075613
iteration : 3401
train acc:  0.6640625
train loss:  0.5665853023529053
train gradient:  0.1929271710271399
iteration : 3402
train acc:  0.734375
train loss:  0.4853752553462982
train gradient:  0.1401944817775429
iteration : 3403
train acc:  0.6796875
train loss:  0.5841579437255859
train gradient:  0.25915971105494473
iteration : 3404
train acc:  0.6953125
train loss:  0.5865105390548706
train gradient:  0.21998245239022002
iteration : 3405
train acc:  0.71875
train loss:  0.5321415066719055
train gradient:  0.14857823564416464
iteration : 3406
train acc:  0.734375
train loss:  0.5454380512237549
train gradient:  0.15713880774598402
iteration : 3407
train acc:  0.7578125
train loss:  0.4837949573993683
train gradient:  0.12376564806812122
iteration : 3408
train acc:  0.6953125
train loss:  0.5268787145614624
train gradient:  0.12884758019890025
iteration : 3409
train acc:  0.671875
train loss:  0.5827066898345947
train gradient:  0.203469100475078
iteration : 3410
train acc:  0.6875
train loss:  0.5144196152687073
train gradient:  0.12946133519320552
iteration : 3411
train acc:  0.7578125
train loss:  0.4896640479564667
train gradient:  0.18575707642053846
iteration : 3412
train acc:  0.7421875
train loss:  0.5035421848297119
train gradient:  0.14589637132492356
iteration : 3413
train acc:  0.78125
train loss:  0.4528645873069763
train gradient:  0.10624638205041138
iteration : 3414
train acc:  0.6640625
train loss:  0.5624318718910217
train gradient:  0.20608063908429608
iteration : 3415
train acc:  0.8046875
train loss:  0.425403356552124
train gradient:  0.10668369816546866
iteration : 3416
train acc:  0.7421875
train loss:  0.4966341257095337
train gradient:  0.1559885421411172
iteration : 3417
train acc:  0.734375
train loss:  0.5476089715957642
train gradient:  0.13878244112261953
iteration : 3418
train acc:  0.7578125
train loss:  0.4823899269104004
train gradient:  0.17916745104625176
iteration : 3419
train acc:  0.75
train loss:  0.5063138604164124
train gradient:  0.13601059045763253
iteration : 3420
train acc:  0.71875
train loss:  0.530983030796051
train gradient:  0.14267968483408366
iteration : 3421
train acc:  0.7421875
train loss:  0.49801480770111084
train gradient:  0.1793802536165406
iteration : 3422
train acc:  0.6796875
train loss:  0.6029357314109802
train gradient:  0.22701915111621518
iteration : 3423
train acc:  0.734375
train loss:  0.5287232398986816
train gradient:  0.1559336261302495
iteration : 3424
train acc:  0.6640625
train loss:  0.5845794081687927
train gradient:  0.15193586353538446
iteration : 3425
train acc:  0.71875
train loss:  0.5355595946311951
train gradient:  0.14255710349763678
iteration : 3426
train acc:  0.71875
train loss:  0.46653681993484497
train gradient:  0.13690590175634515
iteration : 3427
train acc:  0.734375
train loss:  0.5546942353248596
train gradient:  0.19654353923975276
iteration : 3428
train acc:  0.703125
train loss:  0.5340322852134705
train gradient:  0.14984030016879424
iteration : 3429
train acc:  0.7421875
train loss:  0.505363404750824
train gradient:  0.17840832674642276
iteration : 3430
train acc:  0.71875
train loss:  0.5554466247558594
train gradient:  0.18946242679398687
iteration : 3431
train acc:  0.734375
train loss:  0.5289106369018555
train gradient:  0.15362713706208322
iteration : 3432
train acc:  0.6875
train loss:  0.561503529548645
train gradient:  0.21584861663355012
iteration : 3433
train acc:  0.734375
train loss:  0.48585039377212524
train gradient:  0.13112647534503008
iteration : 3434
train acc:  0.640625
train loss:  0.606298565864563
train gradient:  0.16578309893643198
iteration : 3435
train acc:  0.75
train loss:  0.47814881801605225
train gradient:  0.13730330424309503
iteration : 3436
train acc:  0.71875
train loss:  0.5440104603767395
train gradient:  0.19043657212832588
iteration : 3437
train acc:  0.7421875
train loss:  0.5292686223983765
train gradient:  0.15047100987160966
iteration : 3438
train acc:  0.71875
train loss:  0.5601104497909546
train gradient:  0.22067540106140526
iteration : 3439
train acc:  0.7109375
train loss:  0.552465558052063
train gradient:  0.13773890144985967
iteration : 3440
train acc:  0.6640625
train loss:  0.5832387208938599
train gradient:  0.14654984802196303
iteration : 3441
train acc:  0.796875
train loss:  0.46485793590545654
train gradient:  0.1124261384564235
iteration : 3442
train acc:  0.71875
train loss:  0.5221920013427734
train gradient:  0.1430479376091376
iteration : 3443
train acc:  0.6953125
train loss:  0.538533627986908
train gradient:  0.2082445288610438
iteration : 3444
train acc:  0.7578125
train loss:  0.4876748323440552
train gradient:  0.13901312850003075
iteration : 3445
train acc:  0.78125
train loss:  0.47659891843795776
train gradient:  0.13293932449020013
iteration : 3446
train acc:  0.6953125
train loss:  0.5572540760040283
train gradient:  0.20502139121827806
iteration : 3447
train acc:  0.6796875
train loss:  0.5647009611129761
train gradient:  0.19227103368088128
iteration : 3448
train acc:  0.703125
train loss:  0.5287437438964844
train gradient:  0.22482207656209374
iteration : 3449
train acc:  0.7109375
train loss:  0.5035388469696045
train gradient:  0.13718934795218615
iteration : 3450
train acc:  0.6640625
train loss:  0.5986935496330261
train gradient:  0.1685175578688712
iteration : 3451
train acc:  0.7578125
train loss:  0.4807835817337036
train gradient:  0.13705503250331486
iteration : 3452
train acc:  0.7265625
train loss:  0.5252538919448853
train gradient:  0.18624119712730183
iteration : 3453
train acc:  0.7109375
train loss:  0.5457273125648499
train gradient:  0.17137501604352542
iteration : 3454
train acc:  0.71875
train loss:  0.5511285066604614
train gradient:  0.17364343748135985
iteration : 3455
train acc:  0.7578125
train loss:  0.49737849831581116
train gradient:  0.13693090709411587
iteration : 3456
train acc:  0.765625
train loss:  0.5171276330947876
train gradient:  0.1715688145798719
iteration : 3457
train acc:  0.7734375
train loss:  0.5228681564331055
train gradient:  0.1552089340794175
iteration : 3458
train acc:  0.7734375
train loss:  0.45569533109664917
train gradient:  0.14368280580655535
iteration : 3459
train acc:  0.703125
train loss:  0.5435267686843872
train gradient:  0.17680954529810022
iteration : 3460
train acc:  0.71875
train loss:  0.5305608510971069
train gradient:  0.2009763611503539
iteration : 3461
train acc:  0.7109375
train loss:  0.5282899141311646
train gradient:  0.22063060796838363
iteration : 3462
train acc:  0.6328125
train loss:  0.6108499765396118
train gradient:  0.20446605507286753
iteration : 3463
train acc:  0.65625
train loss:  0.5795354843139648
train gradient:  0.20203443220882455
iteration : 3464
train acc:  0.765625
train loss:  0.507581889629364
train gradient:  0.12944554169915984
iteration : 3465
train acc:  0.7265625
train loss:  0.5169876217842102
train gradient:  0.175812160381788
iteration : 3466
train acc:  0.7421875
train loss:  0.5373602509498596
train gradient:  0.2020483018801707
iteration : 3467
train acc:  0.765625
train loss:  0.5077221393585205
train gradient:  0.14598669184367233
iteration : 3468
train acc:  0.8125
train loss:  0.46244049072265625
train gradient:  0.11831673507398317
iteration : 3469
train acc:  0.71875
train loss:  0.5282104015350342
train gradient:  0.1593569468461487
iteration : 3470
train acc:  0.734375
train loss:  0.4809378981590271
train gradient:  0.11608657144394216
iteration : 3471
train acc:  0.7265625
train loss:  0.5393941402435303
train gradient:  0.13364947161986218
iteration : 3472
train acc:  0.71875
train loss:  0.5354586839675903
train gradient:  0.16525217084051788
iteration : 3473
train acc:  0.7734375
train loss:  0.48062488436698914
train gradient:  0.15496832229194402
iteration : 3474
train acc:  0.71875
train loss:  0.5291472673416138
train gradient:  0.14766132167868892
iteration : 3475
train acc:  0.703125
train loss:  0.5155933499336243
train gradient:  0.1787197063363251
iteration : 3476
train acc:  0.6484375
train loss:  0.6309922933578491
train gradient:  0.2640185870687068
iteration : 3477
train acc:  0.765625
train loss:  0.48196941614151
train gradient:  0.1267655293455917
iteration : 3478
train acc:  0.796875
train loss:  0.47348934412002563
train gradient:  0.13740852314887508
iteration : 3479
train acc:  0.7421875
train loss:  0.5379207730293274
train gradient:  0.1603339721835005
iteration : 3480
train acc:  0.7265625
train loss:  0.49405932426452637
train gradient:  0.14954107330832622
iteration : 3481
train acc:  0.6953125
train loss:  0.5747585892677307
train gradient:  0.16943304754153554
iteration : 3482
train acc:  0.7578125
train loss:  0.49414771795272827
train gradient:  0.1324035038724257
iteration : 3483
train acc:  0.6796875
train loss:  0.5656114220619202
train gradient:  0.15695909265520136
iteration : 3484
train acc:  0.7734375
train loss:  0.5701104402542114
train gradient:  0.16614749803298617
iteration : 3485
train acc:  0.6875
train loss:  0.5600554943084717
train gradient:  0.1502924694190055
iteration : 3486
train acc:  0.75
train loss:  0.5182363986968994
train gradient:  0.15130705709661846
iteration : 3487
train acc:  0.765625
train loss:  0.5441917777061462
train gradient:  0.15162597943598938
iteration : 3488
train acc:  0.6796875
train loss:  0.5420089960098267
train gradient:  0.18943073968672286
iteration : 3489
train acc:  0.6953125
train loss:  0.5121819972991943
train gradient:  0.1823691747941834
iteration : 3490
train acc:  0.7109375
train loss:  0.5934010744094849
train gradient:  0.1971373278162301
iteration : 3491
train acc:  0.7109375
train loss:  0.5658384561538696
train gradient:  0.17576956413664752
iteration : 3492
train acc:  0.6953125
train loss:  0.5674192905426025
train gradient:  0.19836010332072795
iteration : 3493
train acc:  0.65625
train loss:  0.6388209462165833
train gradient:  0.19048468675048252
iteration : 3494
train acc:  0.75
train loss:  0.505280613899231
train gradient:  0.1416797824285393
iteration : 3495
train acc:  0.734375
train loss:  0.5339241623878479
train gradient:  0.13950316870531723
iteration : 3496
train acc:  0.796875
train loss:  0.4575587809085846
train gradient:  0.13045978052502968
iteration : 3497
train acc:  0.703125
train loss:  0.5348329544067383
train gradient:  0.14471944721886262
iteration : 3498
train acc:  0.6484375
train loss:  0.6032240390777588
train gradient:  0.20869791701919005
iteration : 3499
train acc:  0.59375
train loss:  0.6571924686431885
train gradient:  0.19133979982133317
iteration : 3500
train acc:  0.71875
train loss:  0.5421701073646545
train gradient:  0.18426373325179712
iteration : 3501
train acc:  0.6953125
train loss:  0.5924384593963623
train gradient:  0.1691151751229123
iteration : 3502
train acc:  0.703125
train loss:  0.5625500679016113
train gradient:  0.1631364135857672
iteration : 3503
train acc:  0.6953125
train loss:  0.5464394092559814
train gradient:  0.13388408728989398
iteration : 3504
train acc:  0.78125
train loss:  0.47881948947906494
train gradient:  0.10771336276533736
iteration : 3505
train acc:  0.703125
train loss:  0.5252441167831421
train gradient:  0.14196050076484262
iteration : 3506
train acc:  0.6328125
train loss:  0.5899258852005005
train gradient:  0.17771267662447818
iteration : 3507
train acc:  0.71875
train loss:  0.5629324316978455
train gradient:  0.19425733335444473
iteration : 3508
train acc:  0.7109375
train loss:  0.5281788110733032
train gradient:  0.15357919165904843
iteration : 3509
train acc:  0.6953125
train loss:  0.5694364309310913
train gradient:  0.1830311967914346
iteration : 3510
train acc:  0.734375
train loss:  0.5730912685394287
train gradient:  0.18229778370961172
iteration : 3511
train acc:  0.7109375
train loss:  0.5320050716400146
train gradient:  0.20323235322571942
iteration : 3512
train acc:  0.7734375
train loss:  0.4798971116542816
train gradient:  0.10333073881415158
iteration : 3513
train acc:  0.7421875
train loss:  0.49754980206489563
train gradient:  0.1515269667058196
iteration : 3514
train acc:  0.75
train loss:  0.5465922355651855
train gradient:  0.22859514278854562
iteration : 3515
train acc:  0.6953125
train loss:  0.5535860061645508
train gradient:  0.1615927932242356
iteration : 3516
train acc:  0.671875
train loss:  0.5186216831207275
train gradient:  0.1446856561718211
iteration : 3517
train acc:  0.71875
train loss:  0.5197418928146362
train gradient:  0.16696215717355997
iteration : 3518
train acc:  0.703125
train loss:  0.529546856880188
train gradient:  0.14809419921951228
iteration : 3519
train acc:  0.7734375
train loss:  0.4980495274066925
train gradient:  0.12049575804453899
iteration : 3520
train acc:  0.734375
train loss:  0.5407383441925049
train gradient:  0.1400213658738069
iteration : 3521
train acc:  0.765625
train loss:  0.4955146908760071
train gradient:  0.13022439220977455
iteration : 3522
train acc:  0.703125
train loss:  0.5604209899902344
train gradient:  0.16665305252113433
iteration : 3523
train acc:  0.6875
train loss:  0.5789002180099487
train gradient:  0.20203905625741336
iteration : 3524
train acc:  0.703125
train loss:  0.509056568145752
train gradient:  0.13439134147180654
iteration : 3525
train acc:  0.734375
train loss:  0.5051144957542419
train gradient:  0.16582676406966243
iteration : 3526
train acc:  0.796875
train loss:  0.4904214143753052
train gradient:  0.1272962894420609
iteration : 3527
train acc:  0.78125
train loss:  0.47288650274276733
train gradient:  0.11319152726497228
iteration : 3528
train acc:  0.7109375
train loss:  0.5255235433578491
train gradient:  0.15338444261764073
iteration : 3529
train acc:  0.703125
train loss:  0.5611209869384766
train gradient:  0.13991203062923427
iteration : 3530
train acc:  0.7578125
train loss:  0.48899707198143005
train gradient:  0.13643185209051828
iteration : 3531
train acc:  0.671875
train loss:  0.6022006273269653
train gradient:  0.17993735043948111
iteration : 3532
train acc:  0.75
train loss:  0.49227476119995117
train gradient:  0.12075775565992936
iteration : 3533
train acc:  0.734375
train loss:  0.5093954801559448
train gradient:  0.11945169394512953
iteration : 3534
train acc:  0.609375
train loss:  0.59855055809021
train gradient:  0.16465128534950402
iteration : 3535
train acc:  0.7421875
train loss:  0.5286881923675537
train gradient:  0.2106052180999572
iteration : 3536
train acc:  0.7265625
train loss:  0.5286792516708374
train gradient:  0.140867537750624
iteration : 3537
train acc:  0.78125
train loss:  0.5114520788192749
train gradient:  0.1537288192634804
iteration : 3538
train acc:  0.71875
train loss:  0.5172104239463806
train gradient:  0.13918742127839634
iteration : 3539
train acc:  0.6953125
train loss:  0.566021203994751
train gradient:  0.1643672313616218
iteration : 3540
train acc:  0.6796875
train loss:  0.5532411336898804
train gradient:  0.17476314142759303
iteration : 3541
train acc:  0.7421875
train loss:  0.5322670340538025
train gradient:  0.16112966648893176
iteration : 3542
train acc:  0.7421875
train loss:  0.5065194964408875
train gradient:  0.14498866583493814
iteration : 3543
train acc:  0.6484375
train loss:  0.6200754642486572
train gradient:  0.1967281093341336
iteration : 3544
train acc:  0.65625
train loss:  0.5880199670791626
train gradient:  0.151830161201574
iteration : 3545
train acc:  0.640625
train loss:  0.6387648582458496
train gradient:  0.2246145014185066
iteration : 3546
train acc:  0.6015625
train loss:  0.5777249336242676
train gradient:  0.20857118256279605
iteration : 3547
train acc:  0.734375
train loss:  0.5013616681098938
train gradient:  0.12138254252729631
iteration : 3548
train acc:  0.6953125
train loss:  0.548498272895813
train gradient:  0.14680542327383456
iteration : 3549
train acc:  0.7109375
train loss:  0.52443528175354
train gradient:  0.15736300065997355
iteration : 3550
train acc:  0.71875
train loss:  0.5209463834762573
train gradient:  0.1254639796156226
iteration : 3551
train acc:  0.7578125
train loss:  0.45580148696899414
train gradient:  0.11826545726522149
iteration : 3552
train acc:  0.7265625
train loss:  0.577788233757019
train gradient:  0.20800238233169682
iteration : 3553
train acc:  0.6953125
train loss:  0.5854372978210449
train gradient:  0.1892442019962248
iteration : 3554
train acc:  0.671875
train loss:  0.6049051284790039
train gradient:  0.15558421483317353
iteration : 3555
train acc:  0.6953125
train loss:  0.5487372875213623
train gradient:  0.15833471240152425
iteration : 3556
train acc:  0.6796875
train loss:  0.5943801403045654
train gradient:  0.1522586144641092
iteration : 3557
train acc:  0.71875
train loss:  0.5653451681137085
train gradient:  0.19653479949202446
iteration : 3558
train acc:  0.7890625
train loss:  0.4487099051475525
train gradient:  0.09929642416850665
iteration : 3559
train acc:  0.6875
train loss:  0.5230340957641602
train gradient:  0.14556000939449543
iteration : 3560
train acc:  0.6875
train loss:  0.49901992082595825
train gradient:  0.13245003937234523
iteration : 3561
train acc:  0.734375
train loss:  0.5054715275764465
train gradient:  0.14664245816726884
iteration : 3562
train acc:  0.75
train loss:  0.477298378944397
train gradient:  0.19023140536463232
iteration : 3563
train acc:  0.828125
train loss:  0.465957909822464
train gradient:  0.139465969170183
iteration : 3564
train acc:  0.6875
train loss:  0.5696386098861694
train gradient:  0.16331050055029933
iteration : 3565
train acc:  0.7265625
train loss:  0.5272172093391418
train gradient:  0.1565102123083896
iteration : 3566
train acc:  0.7109375
train loss:  0.550018310546875
train gradient:  0.12466037991002964
iteration : 3567
train acc:  0.796875
train loss:  0.48758259415626526
train gradient:  0.16370027750294203
iteration : 3568
train acc:  0.734375
train loss:  0.5084741115570068
train gradient:  0.11769132765274211
iteration : 3569
train acc:  0.7578125
train loss:  0.46870917081832886
train gradient:  0.09848269186679447
iteration : 3570
train acc:  0.78125
train loss:  0.5173537731170654
train gradient:  0.15432114513144768
iteration : 3571
train acc:  0.6796875
train loss:  0.5721218585968018
train gradient:  0.12074190470666026
iteration : 3572
train acc:  0.75
train loss:  0.5472407341003418
train gradient:  0.12361222571679817
iteration : 3573
train acc:  0.6875
train loss:  0.522484540939331
train gradient:  0.13208026269677925
iteration : 3574
train acc:  0.6953125
train loss:  0.5090197324752808
train gradient:  0.14198360371335694
iteration : 3575
train acc:  0.796875
train loss:  0.5357832908630371
train gradient:  0.19161563434494921
iteration : 3576
train acc:  0.7578125
train loss:  0.5577657222747803
train gradient:  0.15632346029089406
iteration : 3577
train acc:  0.6796875
train loss:  0.5360089540481567
train gradient:  0.12733656765096574
iteration : 3578
train acc:  0.7265625
train loss:  0.5191874504089355
train gradient:  0.16427587727169868
iteration : 3579
train acc:  0.6953125
train loss:  0.5683343410491943
train gradient:  0.16243281103858204
iteration : 3580
train acc:  0.8125
train loss:  0.47201254963874817
train gradient:  0.12189486713227538
iteration : 3581
train acc:  0.7578125
train loss:  0.5108347535133362
train gradient:  0.16978733107448535
iteration : 3582
train acc:  0.734375
train loss:  0.4942461848258972
train gradient:  0.12838719927513453
iteration : 3583
train acc:  0.6953125
train loss:  0.49473923444747925
train gradient:  0.1543954245368088
iteration : 3584
train acc:  0.7421875
train loss:  0.50788414478302
train gradient:  0.13862320459286948
iteration : 3585
train acc:  0.6875
train loss:  0.5400294065475464
train gradient:  0.2018630227003021
iteration : 3586
train acc:  0.71875
train loss:  0.5419235825538635
train gradient:  0.1679939315909582
iteration : 3587
train acc:  0.7109375
train loss:  0.49571043252944946
train gradient:  0.12157688915098187
iteration : 3588
train acc:  0.7734375
train loss:  0.48373788595199585
train gradient:  0.11278761456064045
iteration : 3589
train acc:  0.6796875
train loss:  0.5959027409553528
train gradient:  0.20003670955460467
iteration : 3590
train acc:  0.6640625
train loss:  0.5587076544761658
train gradient:  0.16523366006826928
iteration : 3591
train acc:  0.6953125
train loss:  0.5151995420455933
train gradient:  0.12216719267901009
iteration : 3592
train acc:  0.734375
train loss:  0.5083473324775696
train gradient:  0.1410300791588913
iteration : 3593
train acc:  0.75
train loss:  0.5143406391143799
train gradient:  0.1360986729333854
iteration : 3594
train acc:  0.7109375
train loss:  0.5613064765930176
train gradient:  0.14556766710772018
iteration : 3595
train acc:  0.671875
train loss:  0.5514616966247559
train gradient:  0.16475680697181416
iteration : 3596
train acc:  0.7578125
train loss:  0.4962593913078308
train gradient:  0.1146914782129579
iteration : 3597
train acc:  0.703125
train loss:  0.5637844800949097
train gradient:  0.22256901536552404
iteration : 3598
train acc:  0.6484375
train loss:  0.5624032616615295
train gradient:  0.16085806840253739
iteration : 3599
train acc:  0.7890625
train loss:  0.47529858350753784
train gradient:  0.13389092255000595
iteration : 3600
train acc:  0.7578125
train loss:  0.46296000480651855
train gradient:  0.10369238485599018
iteration : 3601
train acc:  0.75
train loss:  0.4937494397163391
train gradient:  0.19879524670852244
iteration : 3602
train acc:  0.6796875
train loss:  0.5895786285400391
train gradient:  0.21488898777266957
iteration : 3603
train acc:  0.7890625
train loss:  0.4623958468437195
train gradient:  0.10359695890441048
iteration : 3604
train acc:  0.7265625
train loss:  0.5391628742218018
train gradient:  0.17805954354150583
iteration : 3605
train acc:  0.71875
train loss:  0.49639376997947693
train gradient:  0.12085891758328421
iteration : 3606
train acc:  0.7109375
train loss:  0.49454838037490845
train gradient:  0.12983728280217366
iteration : 3607
train acc:  0.703125
train loss:  0.5521892309188843
train gradient:  0.17933670581714575
iteration : 3608
train acc:  0.7109375
train loss:  0.5405907034873962
train gradient:  0.1799912278178869
iteration : 3609
train acc:  0.7421875
train loss:  0.5461106896400452
train gradient:  0.13944174896129763
iteration : 3610
train acc:  0.734375
train loss:  0.5024741291999817
train gradient:  0.13954521650069168
iteration : 3611
train acc:  0.7265625
train loss:  0.5408250093460083
train gradient:  0.1465892061239384
iteration : 3612
train acc:  0.71875
train loss:  0.5320374369621277
train gradient:  0.14500797433440765
iteration : 3613
train acc:  0.6953125
train loss:  0.5344551801681519
train gradient:  0.20791177005140743
iteration : 3614
train acc:  0.703125
train loss:  0.5773717761039734
train gradient:  0.14638821627219623
iteration : 3615
train acc:  0.703125
train loss:  0.5853984951972961
train gradient:  0.18558594321112287
iteration : 3616
train acc:  0.6875
train loss:  0.5421210527420044
train gradient:  0.17898379854608365
iteration : 3617
train acc:  0.640625
train loss:  0.6023271083831787
train gradient:  0.1768860894476113
iteration : 3618
train acc:  0.7734375
train loss:  0.5070073008537292
train gradient:  0.14830124253724053
iteration : 3619
train acc:  0.6953125
train loss:  0.550041139125824
train gradient:  0.145716925739185
iteration : 3620
train acc:  0.7265625
train loss:  0.5619503855705261
train gradient:  0.18504204889457848
iteration : 3621
train acc:  0.6875
train loss:  0.5550047159194946
train gradient:  0.14838409883552062
iteration : 3622
train acc:  0.765625
train loss:  0.501481831073761
train gradient:  0.13130812946850867
iteration : 3623
train acc:  0.671875
train loss:  0.5549304485321045
train gradient:  0.13361165194935226
iteration : 3624
train acc:  0.734375
train loss:  0.519476592540741
train gradient:  0.14260700887540434
iteration : 3625
train acc:  0.6953125
train loss:  0.5478308200836182
train gradient:  0.15686878569938617
iteration : 3626
train acc:  0.734375
train loss:  0.5157371759414673
train gradient:  0.14598501395727526
iteration : 3627
train acc:  0.7890625
train loss:  0.49079203605651855
train gradient:  0.13126368572166003
iteration : 3628
train acc:  0.7421875
train loss:  0.49796628952026367
train gradient:  0.11722931958088731
iteration : 3629
train acc:  0.71875
train loss:  0.5146341919898987
train gradient:  0.1175917624113635
iteration : 3630
train acc:  0.7109375
train loss:  0.5117725133895874
train gradient:  0.1454212159658671
iteration : 3631
train acc:  0.6875
train loss:  0.5233194828033447
train gradient:  0.18102931015857904
iteration : 3632
train acc:  0.640625
train loss:  0.6057606339454651
train gradient:  0.2193575904813861
iteration : 3633
train acc:  0.7265625
train loss:  0.530381977558136
train gradient:  0.12836687724129492
iteration : 3634
train acc:  0.71875
train loss:  0.5360507369041443
train gradient:  0.15212874710973165
iteration : 3635
train acc:  0.7578125
train loss:  0.5145778656005859
train gradient:  0.12502554124268872
iteration : 3636
train acc:  0.7421875
train loss:  0.5346670746803284
train gradient:  0.14755487322763466
iteration : 3637
train acc:  0.7890625
train loss:  0.4658474326133728
train gradient:  0.11069035089037696
iteration : 3638
train acc:  0.671875
train loss:  0.5471084117889404
train gradient:  0.12516636913911278
iteration : 3639
train acc:  0.7421875
train loss:  0.4955110549926758
train gradient:  0.141592035742027
iteration : 3640
train acc:  0.734375
train loss:  0.5371603965759277
train gradient:  0.1531116753901047
iteration : 3641
train acc:  0.7578125
train loss:  0.5112363696098328
train gradient:  0.15084955567252578
iteration : 3642
train acc:  0.734375
train loss:  0.5038604736328125
train gradient:  0.14507340954773293
iteration : 3643
train acc:  0.7421875
train loss:  0.5222402811050415
train gradient:  0.14873552471342866
iteration : 3644
train acc:  0.734375
train loss:  0.5000936985015869
train gradient:  0.14760603936459046
iteration : 3645
train acc:  0.703125
train loss:  0.5159928798675537
train gradient:  0.1615990823295314
iteration : 3646
train acc:  0.6484375
train loss:  0.5479446053504944
train gradient:  0.2899184151566736
iteration : 3647
train acc:  0.75
train loss:  0.5086886882781982
train gradient:  0.14460933278631588
iteration : 3648
train acc:  0.671875
train loss:  0.5814117193222046
train gradient:  0.18250018888287306
iteration : 3649
train acc:  0.75
train loss:  0.5000631213188171
train gradient:  0.126187463087234
iteration : 3650
train acc:  0.6875
train loss:  0.5702953934669495
train gradient:  0.17410498880226777
iteration : 3651
train acc:  0.6640625
train loss:  0.5069179534912109
train gradient:  0.23032124443967703
iteration : 3652
train acc:  0.6796875
train loss:  0.6082382202148438
train gradient:  0.18610905630148208
iteration : 3653
train acc:  0.703125
train loss:  0.5503091216087341
train gradient:  0.22895146966360258
iteration : 3654
train acc:  0.71875
train loss:  0.5073251724243164
train gradient:  0.1225198025571618
iteration : 3655
train acc:  0.65625
train loss:  0.5337523221969604
train gradient:  0.19033790506485876
iteration : 3656
train acc:  0.703125
train loss:  0.5348185300827026
train gradient:  0.14501333912059922
iteration : 3657
train acc:  0.7265625
train loss:  0.5000287890434265
train gradient:  0.15809619516346374
iteration : 3658
train acc:  0.6796875
train loss:  0.5383286476135254
train gradient:  0.200821172070835
iteration : 3659
train acc:  0.71875
train loss:  0.523016631603241
train gradient:  0.19275332906830975
iteration : 3660
train acc:  0.703125
train loss:  0.5739293098449707
train gradient:  0.24354206150664126
iteration : 3661
train acc:  0.7265625
train loss:  0.5586607456207275
train gradient:  0.29320023186026134
iteration : 3662
train acc:  0.734375
train loss:  0.5332631468772888
train gradient:  0.13496319933418643
iteration : 3663
train acc:  0.703125
train loss:  0.5352652072906494
train gradient:  0.1818093757108677
iteration : 3664
train acc:  0.6484375
train loss:  0.5786380767822266
train gradient:  0.16322826833169746
iteration : 3665
train acc:  0.703125
train loss:  0.5675311088562012
train gradient:  0.17625060261975292
iteration : 3666
train acc:  0.6875
train loss:  0.5797722935676575
train gradient:  0.16021082792026986
iteration : 3667
train acc:  0.7890625
train loss:  0.5045463442802429
train gradient:  0.15565378255451734
iteration : 3668
train acc:  0.6953125
train loss:  0.5214345455169678
train gradient:  0.13842292607444953
iteration : 3669
train acc:  0.703125
train loss:  0.5224453806877136
train gradient:  0.13338871656457815
iteration : 3670
train acc:  0.734375
train loss:  0.49590590596199036
train gradient:  0.13254127546305183
iteration : 3671
train acc:  0.7109375
train loss:  0.5088551044464111
train gradient:  0.14942995281605712
iteration : 3672
train acc:  0.71875
train loss:  0.5321841835975647
train gradient:  0.13846335108154695
iteration : 3673
train acc:  0.75
train loss:  0.4872651696205139
train gradient:  0.11619207544106543
iteration : 3674
train acc:  0.8359375
train loss:  0.4624733030796051
train gradient:  0.16836696342074287
iteration : 3675
train acc:  0.7109375
train loss:  0.5546295046806335
train gradient:  0.1563571838800586
iteration : 3676
train acc:  0.78125
train loss:  0.4993753731250763
train gradient:  0.15116326502752875
iteration : 3677
train acc:  0.7421875
train loss:  0.49928298592567444
train gradient:  0.14067731577302042
iteration : 3678
train acc:  0.734375
train loss:  0.5286000967025757
train gradient:  0.12327933509905217
iteration : 3679
train acc:  0.828125
train loss:  0.44687220454216003
train gradient:  0.10738132839673677
iteration : 3680
train acc:  0.625
train loss:  0.5993751287460327
train gradient:  0.17671325008522204
iteration : 3681
train acc:  0.6171875
train loss:  0.6519263386726379
train gradient:  0.24297282614517118
iteration : 3682
train acc:  0.7109375
train loss:  0.5315558910369873
train gradient:  0.1760043753593355
iteration : 3683
train acc:  0.71875
train loss:  0.5568510293960571
train gradient:  0.1688001962155313
iteration : 3684
train acc:  0.7421875
train loss:  0.5154348611831665
train gradient:  0.14090291878222427
iteration : 3685
train acc:  0.703125
train loss:  0.5546650290489197
train gradient:  0.15269505973183026
iteration : 3686
train acc:  0.671875
train loss:  0.5593608617782593
train gradient:  0.15548020790870798
iteration : 3687
train acc:  0.78125
train loss:  0.5054311752319336
train gradient:  0.15417167893491046
iteration : 3688
train acc:  0.7734375
train loss:  0.45792919397354126
train gradient:  0.12144532985977684
iteration : 3689
train acc:  0.6953125
train loss:  0.518258810043335
train gradient:  0.16803146428726073
iteration : 3690
train acc:  0.75
train loss:  0.49060362577438354
train gradient:  0.18332115747736777
iteration : 3691
train acc:  0.7734375
train loss:  0.517802894115448
train gradient:  0.1251531650088457
iteration : 3692
train acc:  0.6953125
train loss:  0.530204713344574
train gradient:  0.20376004754992855
iteration : 3693
train acc:  0.7734375
train loss:  0.4823499619960785
train gradient:  0.16940722774556147
iteration : 3694
train acc:  0.7421875
train loss:  0.5159441232681274
train gradient:  0.14921509214930617
iteration : 3695
train acc:  0.75
train loss:  0.46623754501342773
train gradient:  0.10999514836578216
iteration : 3696
train acc:  0.734375
train loss:  0.49571284651756287
train gradient:  0.11502795564254109
iteration : 3697
train acc:  0.7734375
train loss:  0.4833718538284302
train gradient:  0.12205754432102937
iteration : 3698
train acc:  0.703125
train loss:  0.5638815760612488
train gradient:  0.14819562867917696
iteration : 3699
train acc:  0.7265625
train loss:  0.5484400987625122
train gradient:  0.14649345728191018
iteration : 3700
train acc:  0.671875
train loss:  0.577278196811676
train gradient:  0.18101688433207563
iteration : 3701
train acc:  0.7578125
train loss:  0.4705546200275421
train gradient:  0.10771814158428737
iteration : 3702
train acc:  0.6875
train loss:  0.5833147764205933
train gradient:  0.21947763758210725
iteration : 3703
train acc:  0.6875
train loss:  0.5401347875595093
train gradient:  0.16474023426533949
iteration : 3704
train acc:  0.6796875
train loss:  0.5526823997497559
train gradient:  0.13384287071038087
iteration : 3705
train acc:  0.6953125
train loss:  0.5374628901481628
train gradient:  0.12638244171970842
iteration : 3706
train acc:  0.6953125
train loss:  0.5264890193939209
train gradient:  0.14955774693788748
iteration : 3707
train acc:  0.6953125
train loss:  0.5733046531677246
train gradient:  0.17856958577259946
iteration : 3708
train acc:  0.7421875
train loss:  0.5222160816192627
train gradient:  0.14781427186671717
iteration : 3709
train acc:  0.703125
train loss:  0.5100182890892029
train gradient:  0.14532722414614704
iteration : 3710
train acc:  0.71875
train loss:  0.5189254283905029
train gradient:  0.12363366728004872
iteration : 3711
train acc:  0.6328125
train loss:  0.6238953471183777
train gradient:  0.2377165067082737
iteration : 3712
train acc:  0.765625
train loss:  0.5092493295669556
train gradient:  0.14379077780941862
iteration : 3713
train acc:  0.75
train loss:  0.49005478620529175
train gradient:  0.1307203714284901
iteration : 3714
train acc:  0.6875
train loss:  0.5767728090286255
train gradient:  0.1470036037671809
iteration : 3715
train acc:  0.7578125
train loss:  0.46906518936157227
train gradient:  0.12311949503267329
iteration : 3716
train acc:  0.734375
train loss:  0.5118684768676758
train gradient:  0.1658671204039736
iteration : 3717
train acc:  0.7109375
train loss:  0.6007319688796997
train gradient:  0.1913493118755914
iteration : 3718
train acc:  0.6640625
train loss:  0.555901288986206
train gradient:  0.16278856955617557
iteration : 3719
train acc:  0.7265625
train loss:  0.4909125566482544
train gradient:  0.13722447043647923
iteration : 3720
train acc:  0.6875
train loss:  0.5701815485954285
train gradient:  0.1443016593984357
iteration : 3721
train acc:  0.6875
train loss:  0.5531449317932129
train gradient:  0.13753285822064093
iteration : 3722
train acc:  0.7265625
train loss:  0.5484512448310852
train gradient:  0.1597093792646775
iteration : 3723
train acc:  0.7265625
train loss:  0.5884824991226196
train gradient:  0.17229783574519392
iteration : 3724
train acc:  0.6796875
train loss:  0.5184732675552368
train gradient:  0.14980034200533726
iteration : 3725
train acc:  0.734375
train loss:  0.5028532147407532
train gradient:  0.16845695040771552
iteration : 3726
train acc:  0.6796875
train loss:  0.5027140378952026
train gradient:  0.1292664510205368
iteration : 3727
train acc:  0.703125
train loss:  0.5399543046951294
train gradient:  0.17585953952662936
iteration : 3728
train acc:  0.7265625
train loss:  0.5092387795448303
train gradient:  0.1567211919488925
iteration : 3729
train acc:  0.7109375
train loss:  0.5438525676727295
train gradient:  0.18609727419331074
iteration : 3730
train acc:  0.6875
train loss:  0.5793554782867432
train gradient:  0.19289712723518776
iteration : 3731
train acc:  0.78125
train loss:  0.5224124193191528
train gradient:  0.15711904249309316
iteration : 3732
train acc:  0.7109375
train loss:  0.5128638744354248
train gradient:  0.1361711262513564
iteration : 3733
train acc:  0.7109375
train loss:  0.549454391002655
train gradient:  0.17543048149249757
iteration : 3734
train acc:  0.6640625
train loss:  0.5727527141571045
train gradient:  0.22105479826187174
iteration : 3735
train acc:  0.78125
train loss:  0.5189576745033264
train gradient:  0.12247012353255289
iteration : 3736
train acc:  0.734375
train loss:  0.51230788230896
train gradient:  0.12997849471306455
iteration : 3737
train acc:  0.6796875
train loss:  0.5088824033737183
train gradient:  0.1508958393663817
iteration : 3738
train acc:  0.7265625
train loss:  0.5664352178573608
train gradient:  0.1711198714496705
iteration : 3739
train acc:  0.6953125
train loss:  0.5248294472694397
train gradient:  0.15203001098677177
iteration : 3740
train acc:  0.7265625
train loss:  0.5035814046859741
train gradient:  0.1502799683153928
iteration : 3741
train acc:  0.7265625
train loss:  0.5201232433319092
train gradient:  0.1545732709894434
iteration : 3742
train acc:  0.75
train loss:  0.5291743278503418
train gradient:  0.17828964255645802
iteration : 3743
train acc:  0.6875
train loss:  0.6256710886955261
train gradient:  0.1856579347482356
iteration : 3744
train acc:  0.6875
train loss:  0.5362147092819214
train gradient:  0.1306475592615462
iteration : 3745
train acc:  0.7578125
train loss:  0.4897090196609497
train gradient:  0.15254825875540856
iteration : 3746
train acc:  0.6953125
train loss:  0.5115426182746887
train gradient:  0.15487426558662915
iteration : 3747
train acc:  0.7109375
train loss:  0.5336135625839233
train gradient:  0.13122531336231413
iteration : 3748
train acc:  0.7265625
train loss:  0.5415533781051636
train gradient:  0.16644543163885805
iteration : 3749
train acc:  0.7421875
train loss:  0.5379651784896851
train gradient:  0.15488506360554927
iteration : 3750
train acc:  0.75
train loss:  0.5111532211303711
train gradient:  0.17652494473531766
iteration : 3751
train acc:  0.671875
train loss:  0.5685602426528931
train gradient:  0.23533692739835887
iteration : 3752
train acc:  0.6875
train loss:  0.5624588131904602
train gradient:  0.16961534491632668
iteration : 3753
train acc:  0.640625
train loss:  0.5946559906005859
train gradient:  0.21290604896482324
iteration : 3754
train acc:  0.765625
train loss:  0.4709915518760681
train gradient:  0.1333764485194559
iteration : 3755
train acc:  0.703125
train loss:  0.5422689318656921
train gradient:  0.16136419598602264
iteration : 3756
train acc:  0.765625
train loss:  0.45159709453582764
train gradient:  0.12472390317487511
iteration : 3757
train acc:  0.6875
train loss:  0.5948075652122498
train gradient:  0.19353081749691936
iteration : 3758
train acc:  0.703125
train loss:  0.5342282056808472
train gradient:  0.1386216039560998
iteration : 3759
train acc:  0.7421875
train loss:  0.481775164604187
train gradient:  0.13569422778259257
iteration : 3760
train acc:  0.7265625
train loss:  0.4853228032588959
train gradient:  0.1612875509174379
iteration : 3761
train acc:  0.734375
train loss:  0.5088640451431274
train gradient:  0.12596947457943566
iteration : 3762
train acc:  0.7421875
train loss:  0.49119123816490173
train gradient:  0.12971861367928478
iteration : 3763
train acc:  0.7265625
train loss:  0.4881809651851654
train gradient:  0.14908676724626246
iteration : 3764
train acc:  0.6484375
train loss:  0.6672033071517944
train gradient:  0.23840458317816896
iteration : 3765
train acc:  0.6640625
train loss:  0.5822873115539551
train gradient:  0.2166040333177977
iteration : 3766
train acc:  0.7265625
train loss:  0.5146157741546631
train gradient:  0.17467378256517604
iteration : 3767
train acc:  0.6875
train loss:  0.575884222984314
train gradient:  0.15179535148262635
iteration : 3768
train acc:  0.765625
train loss:  0.48154157400131226
train gradient:  0.11838344613165191
iteration : 3769
train acc:  0.7421875
train loss:  0.5578719973564148
train gradient:  0.13522846316329357
iteration : 3770
train acc:  0.7265625
train loss:  0.5151635408401489
train gradient:  0.13680780360183242
iteration : 3771
train acc:  0.75
train loss:  0.5060397386550903
train gradient:  0.15675480994677832
iteration : 3772
train acc:  0.65625
train loss:  0.6079461574554443
train gradient:  0.20234184823260587
iteration : 3773
train acc:  0.71875
train loss:  0.5205298662185669
train gradient:  0.12164444946613182
iteration : 3774
train acc:  0.75
train loss:  0.5473099946975708
train gradient:  0.14765429096285382
iteration : 3775
train acc:  0.671875
train loss:  0.5538631677627563
train gradient:  0.18599777935843273
iteration : 3776
train acc:  0.6796875
train loss:  0.5473491549491882
train gradient:  0.18704349069287288
iteration : 3777
train acc:  0.671875
train loss:  0.57389235496521
train gradient:  0.22000795602172857
iteration : 3778
train acc:  0.671875
train loss:  0.5302305221557617
train gradient:  0.19312429256021263
iteration : 3779
train acc:  0.7421875
train loss:  0.5085235834121704
train gradient:  0.15804386562056932
iteration : 3780
train acc:  0.734375
train loss:  0.5245808362960815
train gradient:  0.18913524146580857
iteration : 3781
train acc:  0.640625
train loss:  0.5692447423934937
train gradient:  0.14602348643410956
iteration : 3782
train acc:  0.75
train loss:  0.45948177576065063
train gradient:  0.10250480113461469
iteration : 3783
train acc:  0.734375
train loss:  0.5367509126663208
train gradient:  0.16733174242635152
iteration : 3784
train acc:  0.71875
train loss:  0.5653426647186279
train gradient:  0.14684306023981689
iteration : 3785
train acc:  0.71875
train loss:  0.5192385911941528
train gradient:  0.16779951588528177
iteration : 3786
train acc:  0.65625
train loss:  0.6003684997558594
train gradient:  0.20507506785785737
iteration : 3787
train acc:  0.6875
train loss:  0.586578369140625
train gradient:  0.18438451666347094
iteration : 3788
train acc:  0.7265625
train loss:  0.5222890377044678
train gradient:  0.1270803253772132
iteration : 3789
train acc:  0.6640625
train loss:  0.5744720697402954
train gradient:  0.1373503010003821
iteration : 3790
train acc:  0.703125
train loss:  0.5344091653823853
train gradient:  0.12371007618164538
iteration : 3791
train acc:  0.6875
train loss:  0.5538037419319153
train gradient:  0.1574809270364566
iteration : 3792
train acc:  0.7734375
train loss:  0.49595212936401367
train gradient:  0.13887185653937195
iteration : 3793
train acc:  0.75
train loss:  0.4547378420829773
train gradient:  0.12422608174922982
iteration : 3794
train acc:  0.8046875
train loss:  0.5052502155303955
train gradient:  0.19257700790926058
iteration : 3795
train acc:  0.703125
train loss:  0.5437538027763367
train gradient:  0.1973080096995738
iteration : 3796
train acc:  0.7109375
train loss:  0.4817291498184204
train gradient:  0.12443025342155983
iteration : 3797
train acc:  0.6640625
train loss:  0.6023141741752625
train gradient:  0.24046195533308906
iteration : 3798
train acc:  0.796875
train loss:  0.46593213081359863
train gradient:  0.12238838399874483
iteration : 3799
train acc:  0.6796875
train loss:  0.5079869627952576
train gradient:  0.11622991938218599
iteration : 3800
train acc:  0.765625
train loss:  0.4888759255409241
train gradient:  0.11087833290463088
iteration : 3801
train acc:  0.7109375
train loss:  0.5559733510017395
train gradient:  0.18039765982102035
iteration : 3802
train acc:  0.609375
train loss:  0.63361656665802
train gradient:  0.28133345747346045
iteration : 3803
train acc:  0.703125
train loss:  0.5400977730751038
train gradient:  0.18810383685150037
iteration : 3804
train acc:  0.6328125
train loss:  0.6053512096405029
train gradient:  0.18062121338689935
iteration : 3805
train acc:  0.7578125
train loss:  0.4928867518901825
train gradient:  0.11512970420041545
iteration : 3806
train acc:  0.7890625
train loss:  0.47974538803100586
train gradient:  0.16491116736724687
iteration : 3807
train acc:  0.734375
train loss:  0.5002107620239258
train gradient:  0.15591735550489294
iteration : 3808
train acc:  0.7578125
train loss:  0.49625444412231445
train gradient:  0.1848184583473254
iteration : 3809
train acc:  0.703125
train loss:  0.5595963597297668
train gradient:  0.17699707406388265
iteration : 3810
train acc:  0.8046875
train loss:  0.48288023471832275
train gradient:  0.14494403553156038
iteration : 3811
train acc:  0.6875
train loss:  0.540993332862854
train gradient:  0.16962947472410855
iteration : 3812
train acc:  0.71875
train loss:  0.5554766058921814
train gradient:  0.15960770117056558
iteration : 3813
train acc:  0.6953125
train loss:  0.5400140285491943
train gradient:  0.17691105134492477
iteration : 3814
train acc:  0.7265625
train loss:  0.5580736994743347
train gradient:  0.22115712146114003
iteration : 3815
train acc:  0.671875
train loss:  0.5292035341262817
train gradient:  0.15567000669847664
iteration : 3816
train acc:  0.7421875
train loss:  0.4803733825683594
train gradient:  0.12513989514099383
iteration : 3817
train acc:  0.7109375
train loss:  0.519944429397583
train gradient:  0.1402962043666135
iteration : 3818
train acc:  0.7421875
train loss:  0.5136425495147705
train gradient:  0.12221986349544041
iteration : 3819
train acc:  0.703125
train loss:  0.6185911893844604
train gradient:  0.20666698778581735
iteration : 3820
train acc:  0.75
train loss:  0.52248215675354
train gradient:  0.16380980394477024
iteration : 3821
train acc:  0.75
train loss:  0.5051926374435425
train gradient:  0.1267219841852999
iteration : 3822
train acc:  0.71875
train loss:  0.525432825088501
train gradient:  0.12921683215977475
iteration : 3823
train acc:  0.7109375
train loss:  0.526972770690918
train gradient:  0.1441314217385929
iteration : 3824
train acc:  0.6953125
train loss:  0.5758442878723145
train gradient:  0.1511760180355367
iteration : 3825
train acc:  0.6796875
train loss:  0.5214680433273315
train gradient:  0.116638416544821
iteration : 3826
train acc:  0.71875
train loss:  0.5043890476226807
train gradient:  0.1515901911575367
iteration : 3827
train acc:  0.7421875
train loss:  0.49316275119781494
train gradient:  0.1345747578260545
iteration : 3828
train acc:  0.71875
train loss:  0.5338160991668701
train gradient:  0.1523904104409394
iteration : 3829
train acc:  0.671875
train loss:  0.6014703512191772
train gradient:  0.20484387636544896
iteration : 3830
train acc:  0.7421875
train loss:  0.5654757618904114
train gradient:  0.17091306987984972
iteration : 3831
train acc:  0.7890625
train loss:  0.4339442253112793
train gradient:  0.11143451077292107
iteration : 3832
train acc:  0.703125
train loss:  0.5325910449028015
train gradient:  0.18615104137900026
iteration : 3833
train acc:  0.671875
train loss:  0.585692286491394
train gradient:  0.1734998460651146
iteration : 3834
train acc:  0.765625
train loss:  0.47949114441871643
train gradient:  0.14014111071237573
iteration : 3835
train acc:  0.7578125
train loss:  0.47748929262161255
train gradient:  0.13095685072800553
iteration : 3836
train acc:  0.7421875
train loss:  0.5267720222473145
train gradient:  0.15452279624012433
iteration : 3837
train acc:  0.734375
train loss:  0.49652400612831116
train gradient:  0.16503288335505678
iteration : 3838
train acc:  0.703125
train loss:  0.53194260597229
train gradient:  0.2042848529138731
iteration : 3839
train acc:  0.703125
train loss:  0.5269967913627625
train gradient:  0.14220021938609173
iteration : 3840
train acc:  0.71875
train loss:  0.5477009415626526
train gradient:  0.15464204941603238
iteration : 3841
train acc:  0.765625
train loss:  0.4937388598918915
train gradient:  0.1310232177827352
iteration : 3842
train acc:  0.7109375
train loss:  0.6110690236091614
train gradient:  0.24047997899379586
iteration : 3843
train acc:  0.7109375
train loss:  0.5442814826965332
train gradient:  0.14073159655906242
iteration : 3844
train acc:  0.703125
train loss:  0.565309464931488
train gradient:  0.18834442954449276
iteration : 3845
train acc:  0.7578125
train loss:  0.4813656210899353
train gradient:  0.20128245814503573
iteration : 3846
train acc:  0.7890625
train loss:  0.46605780720710754
train gradient:  0.12655384712738554
iteration : 3847
train acc:  0.7890625
train loss:  0.47266650199890137
train gradient:  0.16921766512748435
iteration : 3848
train acc:  0.640625
train loss:  0.6318178772926331
train gradient:  0.28174736328460986
iteration : 3849
train acc:  0.734375
train loss:  0.5088024139404297
train gradient:  0.12168346484672547
iteration : 3850
train acc:  0.6484375
train loss:  0.5504754781723022
train gradient:  0.1417445996892273
iteration : 3851
train acc:  0.6796875
train loss:  0.5080406665802002
train gradient:  0.1803816787955454
iteration : 3852
train acc:  0.7578125
train loss:  0.509672999382019
train gradient:  0.21373953190829087
iteration : 3853
train acc:  0.6875
train loss:  0.5764755606651306
train gradient:  0.17728537281959303
iteration : 3854
train acc:  0.7890625
train loss:  0.4413054585456848
train gradient:  0.1265250351341743
iteration : 3855
train acc:  0.7890625
train loss:  0.4431268274784088
train gradient:  0.10725065396682797
iteration : 3856
train acc:  0.671875
train loss:  0.5556316375732422
train gradient:  0.12721805922423013
iteration : 3857
train acc:  0.703125
train loss:  0.5403251647949219
train gradient:  0.14553404122665464
iteration : 3858
train acc:  0.703125
train loss:  0.6000077128410339
train gradient:  0.15573820771254288
iteration : 3859
train acc:  0.71875
train loss:  0.5038949251174927
train gradient:  0.13573110832705593
iteration : 3860
train acc:  0.7578125
train loss:  0.48980647325515747
train gradient:  0.13763807158031077
iteration : 3861
train acc:  0.7265625
train loss:  0.5016903281211853
train gradient:  0.13643772947194593
iteration : 3862
train acc:  0.734375
train loss:  0.4868093430995941
train gradient:  0.12879515938523345
iteration : 3863
train acc:  0.7578125
train loss:  0.47748202085494995
train gradient:  0.10880950394799956
iteration : 3864
train acc:  0.75
train loss:  0.44316530227661133
train gradient:  0.14314543783098937
iteration : 3865
train acc:  0.703125
train loss:  0.5213018655776978
train gradient:  0.18494556352242336
iteration : 3866
train acc:  0.71875
train loss:  0.47176188230514526
train gradient:  0.14571348399222528
iteration : 3867
train acc:  0.71875
train loss:  0.5413103103637695
train gradient:  0.1470396160815261
iteration : 3868
train acc:  0.7734375
train loss:  0.4777278006076813
train gradient:  0.11639091461669242
iteration : 3869
train acc:  0.796875
train loss:  0.4528118669986725
train gradient:  0.14909875178390553
iteration : 3870
train acc:  0.7265625
train loss:  0.5455224514007568
train gradient:  0.18914192241560085
iteration : 3871
train acc:  0.703125
train loss:  0.5554479360580444
train gradient:  0.15680230684048418
iteration : 3872
train acc:  0.765625
train loss:  0.5252255201339722
train gradient:  0.13277557810477675
iteration : 3873
train acc:  0.6875
train loss:  0.6062707901000977
train gradient:  0.20616907952597324
iteration : 3874
train acc:  0.78125
train loss:  0.4919583797454834
train gradient:  0.17827510684487868
iteration : 3875
train acc:  0.671875
train loss:  0.5505300164222717
train gradient:  0.14289777052524416
iteration : 3876
train acc:  0.7421875
train loss:  0.5087128281593323
train gradient:  0.15008514792938785
iteration : 3877
train acc:  0.734375
train loss:  0.5245934724807739
train gradient:  0.13368781072539399
iteration : 3878
train acc:  0.703125
train loss:  0.5628279447555542
train gradient:  0.14684491043110545
iteration : 3879
train acc:  0.71875
train loss:  0.5503240823745728
train gradient:  0.11993429261247494
iteration : 3880
train acc:  0.6953125
train loss:  0.5476518869400024
train gradient:  0.17285531697741066
iteration : 3881
train acc:  0.7578125
train loss:  0.45033538341522217
train gradient:  0.11776972257470081
iteration : 3882
train acc:  0.734375
train loss:  0.5896041989326477
train gradient:  0.1958672032635138
iteration : 3883
train acc:  0.7890625
train loss:  0.4682934284210205
train gradient:  0.11715246601666601
iteration : 3884
train acc:  0.6953125
train loss:  0.5300202369689941
train gradient:  0.12574629400071347
iteration : 3885
train acc:  0.7265625
train loss:  0.47712427377700806
train gradient:  0.12083100087417646
iteration : 3886
train acc:  0.7421875
train loss:  0.5324304103851318
train gradient:  0.23320191175882404
iteration : 3887
train acc:  0.734375
train loss:  0.5130923986434937
train gradient:  0.1268242615086168
iteration : 3888
train acc:  0.75
train loss:  0.5292486548423767
train gradient:  0.19535026559907398
iteration : 3889
train acc:  0.7265625
train loss:  0.5173895359039307
train gradient:  0.1523961809083214
iteration : 3890
train acc:  0.7578125
train loss:  0.4992454946041107
train gradient:  0.11284231063600347
iteration : 3891
train acc:  0.7421875
train loss:  0.47919198870658875
train gradient:  0.10594863661534834
iteration : 3892
train acc:  0.78125
train loss:  0.44714316725730896
train gradient:  0.13439895993360698
iteration : 3893
train acc:  0.75
train loss:  0.5074260234832764
train gradient:  0.1255776382847754
iteration : 3894
train acc:  0.7734375
train loss:  0.4480748176574707
train gradient:  0.11030075802606536
iteration : 3895
train acc:  0.7578125
train loss:  0.5177765488624573
train gradient:  0.1554783344837131
iteration : 3896
train acc:  0.75
train loss:  0.49523767828941345
train gradient:  0.13166910212072264
iteration : 3897
train acc:  0.7109375
train loss:  0.5761352181434631
train gradient:  0.26347431186604225
iteration : 3898
train acc:  0.6953125
train loss:  0.5158531665802002
train gradient:  0.14217530203962048
iteration : 3899
train acc:  0.671875
train loss:  0.5953594446182251
train gradient:  0.20795200030940403
iteration : 3900
train acc:  0.703125
train loss:  0.5265500545501709
train gradient:  0.10701024651120575
iteration : 3901
train acc:  0.765625
train loss:  0.48312389850616455
train gradient:  0.12805251449892482
iteration : 3902
train acc:  0.671875
train loss:  0.5727127194404602
train gradient:  0.16708990922262756
iteration : 3903
train acc:  0.6953125
train loss:  0.5229270458221436
train gradient:  0.15547509591319775
iteration : 3904
train acc:  0.7578125
train loss:  0.4951319098472595
train gradient:  0.15080550687185165
iteration : 3905
train acc:  0.6875
train loss:  0.517844021320343
train gradient:  0.14448535790581454
iteration : 3906
train acc:  0.7109375
train loss:  0.5342347621917725
train gradient:  0.17721923825540226
iteration : 3907
train acc:  0.7421875
train loss:  0.5056689977645874
train gradient:  0.125984688084989
iteration : 3908
train acc:  0.7109375
train loss:  0.49288588762283325
train gradient:  0.1254017425057357
iteration : 3909
train acc:  0.6796875
train loss:  0.557529091835022
train gradient:  0.13210396133432617
iteration : 3910
train acc:  0.75
train loss:  0.5205330848693848
train gradient:  0.12422713334338892
iteration : 3911
train acc:  0.7265625
train loss:  0.4866933524608612
train gradient:  0.13137573282006385
iteration : 3912
train acc:  0.71875
train loss:  0.5279536247253418
train gradient:  0.14612818725649185
iteration : 3913
train acc:  0.71875
train loss:  0.5660686492919922
train gradient:  0.20732328355059515
iteration : 3914
train acc:  0.7734375
train loss:  0.5019810199737549
train gradient:  0.1263674009582492
iteration : 3915
train acc:  0.7109375
train loss:  0.47964587807655334
train gradient:  0.11216221478867787
iteration : 3916
train acc:  0.7109375
train loss:  0.5505586862564087
train gradient:  0.20167976896942508
iteration : 3917
train acc:  0.6953125
train loss:  0.5236023664474487
train gradient:  0.17296969435729131
iteration : 3918
train acc:  0.765625
train loss:  0.4644468426704407
train gradient:  0.15642912211540758
iteration : 3919
train acc:  0.7578125
train loss:  0.5146393775939941
train gradient:  0.16023066177112216
iteration : 3920
train acc:  0.7734375
train loss:  0.47278526425361633
train gradient:  0.13035432994670818
iteration : 3921
train acc:  0.7890625
train loss:  0.48247745633125305
train gradient:  0.15311778698844603
iteration : 3922
train acc:  0.7421875
train loss:  0.4729079306125641
train gradient:  0.16462197935595418
iteration : 3923
train acc:  0.703125
train loss:  0.5088341236114502
train gradient:  0.11917150173735325
iteration : 3924
train acc:  0.765625
train loss:  0.48738688230514526
train gradient:  0.14843026052827996
iteration : 3925
train acc:  0.765625
train loss:  0.4992617070674896
train gradient:  0.21570900641042345
iteration : 3926
train acc:  0.7109375
train loss:  0.5856002569198608
train gradient:  0.23073482042478785
iteration : 3927
train acc:  0.734375
train loss:  0.502025306224823
train gradient:  0.1422194860490233
iteration : 3928
train acc:  0.796875
train loss:  0.4575119912624359
train gradient:  0.09902055357365724
iteration : 3929
train acc:  0.7421875
train loss:  0.5367153286933899
train gradient:  0.19901785696228608
iteration : 3930
train acc:  0.703125
train loss:  0.5033008456230164
train gradient:  0.13799247030415032
iteration : 3931
train acc:  0.6953125
train loss:  0.5408744812011719
train gradient:  0.211078748901466
iteration : 3932
train acc:  0.6796875
train loss:  0.5804285407066345
train gradient:  0.1632221784826397
iteration : 3933
train acc:  0.8125
train loss:  0.46152549982070923
train gradient:  0.16383035634856696
iteration : 3934
train acc:  0.734375
train loss:  0.520859956741333
train gradient:  0.14131685672626454
iteration : 3935
train acc:  0.6875
train loss:  0.5241742730140686
train gradient:  0.16525072226116305
iteration : 3936
train acc:  0.71875
train loss:  0.5193653702735901
train gradient:  0.11458905212136972
iteration : 3937
train acc:  0.7109375
train loss:  0.5282930135726929
train gradient:  0.1471666133913188
iteration : 3938
train acc:  0.7734375
train loss:  0.4658932089805603
train gradient:  0.11042727681704387
iteration : 3939
train acc:  0.7265625
train loss:  0.5340999364852905
train gradient:  0.1544667656331412
iteration : 3940
train acc:  0.6953125
train loss:  0.5261358618736267
train gradient:  0.16924379673155504
iteration : 3941
train acc:  0.671875
train loss:  0.6293251514434814
train gradient:  0.2386549250720939
iteration : 3942
train acc:  0.6875
train loss:  0.5189681649208069
train gradient:  0.18973521092683127
iteration : 3943
train acc:  0.6953125
train loss:  0.5416773557662964
train gradient:  0.14516390400304136
iteration : 3944
train acc:  0.703125
train loss:  0.5741358995437622
train gradient:  0.18561779536146317
iteration : 3945
train acc:  0.7890625
train loss:  0.4554896652698517
train gradient:  0.12495229788560855
iteration : 3946
train acc:  0.7734375
train loss:  0.47919097542762756
train gradient:  0.12212600607174276
iteration : 3947
train acc:  0.7265625
train loss:  0.5012176632881165
train gradient:  0.11927765467246164
iteration : 3948
train acc:  0.734375
train loss:  0.4940480887889862
train gradient:  0.12934429009635878
iteration : 3949
train acc:  0.7421875
train loss:  0.5173180103302002
train gradient:  0.1444465773153441
iteration : 3950
train acc:  0.7265625
train loss:  0.5612730979919434
train gradient:  0.1740106818069973
iteration : 3951
train acc:  0.7109375
train loss:  0.5490668416023254
train gradient:  0.1723773554895201
iteration : 3952
train acc:  0.734375
train loss:  0.5149263143539429
train gradient:  0.2124365298889636
iteration : 3953
train acc:  0.6953125
train loss:  0.5350856184959412
train gradient:  0.2165285014792141
iteration : 3954
train acc:  0.6796875
train loss:  0.5613649487495422
train gradient:  0.18938217006322944
iteration : 3955
train acc:  0.7890625
train loss:  0.44887620210647583
train gradient:  0.15250754426896995
iteration : 3956
train acc:  0.734375
train loss:  0.5051134824752808
train gradient:  0.1483902090598986
iteration : 3957
train acc:  0.7109375
train loss:  0.5047635436058044
train gradient:  0.15455387723363412
iteration : 3958
train acc:  0.75
train loss:  0.5113729238510132
train gradient:  0.14850054842429738
iteration : 3959
train acc:  0.6484375
train loss:  0.5833197832107544
train gradient:  0.2311332439481094
iteration : 3960
train acc:  0.7109375
train loss:  0.5163879990577698
train gradient:  0.1744937508301727
iteration : 3961
train acc:  0.6796875
train loss:  0.5282717943191528
train gradient:  0.20621717728717454
iteration : 3962
train acc:  0.7578125
train loss:  0.4967876374721527
train gradient:  0.1255256195638974
iteration : 3963
train acc:  0.7578125
train loss:  0.4645349681377411
train gradient:  0.1662099031814232
iteration : 3964
train acc:  0.71875
train loss:  0.5325751900672913
train gradient:  0.217613296965109
iteration : 3965
train acc:  0.75
train loss:  0.4902591109275818
train gradient:  0.1334029429046906
iteration : 3966
train acc:  0.6484375
train loss:  0.6140483021736145
train gradient:  0.189278390770235
iteration : 3967
train acc:  0.703125
train loss:  0.5695152282714844
train gradient:  0.15903038966154787
iteration : 3968
train acc:  0.71875
train loss:  0.5123825073242188
train gradient:  0.10547246264341346
iteration : 3969
train acc:  0.6875
train loss:  0.5329210162162781
train gradient:  0.14912884271935511
iteration : 3970
train acc:  0.703125
train loss:  0.5723384618759155
train gradient:  0.23087605868851052
iteration : 3971
train acc:  0.6953125
train loss:  0.5614244937896729
train gradient:  0.20595520008193313
iteration : 3972
train acc:  0.6875
train loss:  0.5465817451477051
train gradient:  0.13230189737631715
iteration : 3973
train acc:  0.7109375
train loss:  0.5260813236236572
train gradient:  0.13965092112823999
iteration : 3974
train acc:  0.7109375
train loss:  0.5310657024383545
train gradient:  0.12839794793524534
iteration : 3975
train acc:  0.7421875
train loss:  0.5257927775382996
train gradient:  0.14700493484873287
iteration : 3976
train acc:  0.6953125
train loss:  0.4976541996002197
train gradient:  0.1465145891200707
iteration : 3977
train acc:  0.75
train loss:  0.5267119407653809
train gradient:  0.18126962189164392
iteration : 3978
train acc:  0.7421875
train loss:  0.5019521117210388
train gradient:  0.1865763525021355
iteration : 3979
train acc:  0.7421875
train loss:  0.5024549961090088
train gradient:  0.13671478769784126
iteration : 3980
train acc:  0.734375
train loss:  0.506266713142395
train gradient:  0.15280290295672797
iteration : 3981
train acc:  0.75
train loss:  0.5306146144866943
train gradient:  0.21627983650113175
iteration : 3982
train acc:  0.703125
train loss:  0.5864801406860352
train gradient:  0.193460034222811
iteration : 3983
train acc:  0.7109375
train loss:  0.520729660987854
train gradient:  0.1522517070281872
iteration : 3984
train acc:  0.6484375
train loss:  0.6069234609603882
train gradient:  0.21686729398075466
iteration : 3985
train acc:  0.6875
train loss:  0.5391739010810852
train gradient:  0.16666519782799893
iteration : 3986
train acc:  0.6484375
train loss:  0.6539603471755981
train gradient:  0.20768568345339272
iteration : 3987
train acc:  0.7109375
train loss:  0.5809890031814575
train gradient:  0.1750417790398744
iteration : 3988
train acc:  0.703125
train loss:  0.5637713670730591
train gradient:  0.19182667572358164
iteration : 3989
train acc:  0.75
train loss:  0.5097371339797974
train gradient:  0.12276030766409324
iteration : 3990
train acc:  0.765625
train loss:  0.5035437345504761
train gradient:  0.13586731042708847
iteration : 3991
train acc:  0.71875
train loss:  0.506450355052948
train gradient:  0.213697944356617
iteration : 3992
train acc:  0.7421875
train loss:  0.49673497676849365
train gradient:  0.14012567915880392
iteration : 3993
train acc:  0.75
train loss:  0.5132929086685181
train gradient:  0.14610821249898273
iteration : 3994
train acc:  0.71875
train loss:  0.521350622177124
train gradient:  0.16214083083237713
iteration : 3995
train acc:  0.703125
train loss:  0.537168025970459
train gradient:  0.1658483594040207
iteration : 3996
train acc:  0.765625
train loss:  0.5137171149253845
train gradient:  0.13470903180188068
iteration : 3997
train acc:  0.7734375
train loss:  0.46947169303894043
train gradient:  0.1376937063983238
iteration : 3998
train acc:  0.734375
train loss:  0.5113632678985596
train gradient:  0.1494046510513818
iteration : 3999
train acc:  0.78125
train loss:  0.5198010206222534
train gradient:  0.16194590886318722
iteration : 4000
train acc:  0.6796875
train loss:  0.5441321134567261
train gradient:  0.1416344911639638
iteration : 4001
train acc:  0.6328125
train loss:  0.6202911138534546
train gradient:  0.18743388196819546
iteration : 4002
train acc:  0.75
train loss:  0.4870777726173401
train gradient:  0.16332521746384915
iteration : 4003
train acc:  0.7421875
train loss:  0.506005585193634
train gradient:  0.19146238287672807
iteration : 4004
train acc:  0.65625
train loss:  0.5890092849731445
train gradient:  0.21044357203649647
iteration : 4005
train acc:  0.734375
train loss:  0.5015185475349426
train gradient:  0.14695812889549814
iteration : 4006
train acc:  0.703125
train loss:  0.56019127368927
train gradient:  0.18870169681027493
iteration : 4007
train acc:  0.7421875
train loss:  0.5129542350769043
train gradient:  0.12242074060601428
iteration : 4008
train acc:  0.703125
train loss:  0.5232508182525635
train gradient:  0.15454036721112327
iteration : 4009
train acc:  0.6875
train loss:  0.5656254887580872
train gradient:  0.17492976526689297
iteration : 4010
train acc:  0.671875
train loss:  0.5737395286560059
train gradient:  0.18194861963885048
iteration : 4011
train acc:  0.6484375
train loss:  0.5942445993423462
train gradient:  0.17817958569010345
iteration : 4012
train acc:  0.71875
train loss:  0.5413545966148376
train gradient:  0.13932188534514006
iteration : 4013
train acc:  0.6875
train loss:  0.5967637896537781
train gradient:  0.15841367943223036
iteration : 4014
train acc:  0.734375
train loss:  0.5008726119995117
train gradient:  0.12766166266844725
iteration : 4015
train acc:  0.765625
train loss:  0.4633371829986572
train gradient:  0.14506858529887634
iteration : 4016
train acc:  0.7578125
train loss:  0.488460898399353
train gradient:  0.12208190658175685
iteration : 4017
train acc:  0.75
train loss:  0.525296151638031
train gradient:  0.16808344455599503
iteration : 4018
train acc:  0.7421875
train loss:  0.503131628036499
train gradient:  0.1337351569782381
iteration : 4019
train acc:  0.765625
train loss:  0.49845606088638306
train gradient:  0.14593461653167134
iteration : 4020
train acc:  0.7578125
train loss:  0.5249114632606506
train gradient:  0.13309545965137626
iteration : 4021
train acc:  0.703125
train loss:  0.5509284138679504
train gradient:  0.14271737407365137
iteration : 4022
train acc:  0.7578125
train loss:  0.5241124629974365
train gradient:  0.1517017219741476
iteration : 4023
train acc:  0.6875
train loss:  0.5343849062919617
train gradient:  0.15395958929581602
iteration : 4024
train acc:  0.7109375
train loss:  0.5330995321273804
train gradient:  0.14578658204927103
iteration : 4025
train acc:  0.7265625
train loss:  0.5149127244949341
train gradient:  0.16773388302112996
iteration : 4026
train acc:  0.7109375
train loss:  0.5611467957496643
train gradient:  0.22443284322719378
iteration : 4027
train acc:  0.734375
train loss:  0.5140773057937622
train gradient:  0.1586245589868075
iteration : 4028
train acc:  0.6875
train loss:  0.6050476431846619
train gradient:  0.18537522554319266
iteration : 4029
train acc:  0.734375
train loss:  0.5110665559768677
train gradient:  0.1263055264703281
iteration : 4030
train acc:  0.703125
train loss:  0.5340859889984131
train gradient:  0.14039080999531495
iteration : 4031
train acc:  0.65625
train loss:  0.6060589551925659
train gradient:  0.19719260983946554
iteration : 4032
train acc:  0.6875
train loss:  0.6010526418685913
train gradient:  0.23547944017339917
iteration : 4033
train acc:  0.703125
train loss:  0.5030432343482971
train gradient:  0.16367653434348473
iteration : 4034
train acc:  0.7265625
train loss:  0.5385493040084839
train gradient:  0.19730769403593523
iteration : 4035
train acc:  0.7265625
train loss:  0.5408090949058533
train gradient:  0.1490387856540299
iteration : 4036
train acc:  0.75
train loss:  0.520422637462616
train gradient:  0.17586855097074405
iteration : 4037
train acc:  0.6796875
train loss:  0.6387143135070801
train gradient:  0.222102425858463
iteration : 4038
train acc:  0.7109375
train loss:  0.4927014410495758
train gradient:  0.13268800489881802
iteration : 4039
train acc:  0.6796875
train loss:  0.5437674522399902
train gradient:  0.21217257711005802
iteration : 4040
train acc:  0.7578125
train loss:  0.5262110829353333
train gradient:  0.16244693027618118
iteration : 4041
train acc:  0.75
train loss:  0.5352644920349121
train gradient:  0.1556202932268072
iteration : 4042
train acc:  0.7421875
train loss:  0.5060874223709106
train gradient:  0.13900406208580374
iteration : 4043
train acc:  0.6953125
train loss:  0.5282006859779358
train gradient:  0.13096692814924255
iteration : 4044
train acc:  0.640625
train loss:  0.5693917274475098
train gradient:  0.20245259336282323
iteration : 4045
train acc:  0.7578125
train loss:  0.4828934669494629
train gradient:  0.15851621093892931
iteration : 4046
train acc:  0.7421875
train loss:  0.5328183174133301
train gradient:  0.17113689260139492
iteration : 4047
train acc:  0.7109375
train loss:  0.5732142329216003
train gradient:  0.21992040192880452
iteration : 4048
train acc:  0.734375
train loss:  0.5314485430717468
train gradient:  0.12390076105513982
iteration : 4049
train acc:  0.671875
train loss:  0.5617984533309937
train gradient:  0.16567982916615975
iteration : 4050
train acc:  0.71875
train loss:  0.5039551258087158
train gradient:  0.179154521039862
iteration : 4051
train acc:  0.703125
train loss:  0.553559422492981
train gradient:  0.1818578570276141
iteration : 4052
train acc:  0.6640625
train loss:  0.604485034942627
train gradient:  0.2304340513113644
iteration : 4053
train acc:  0.6875
train loss:  0.5450448393821716
train gradient:  0.17451843643050133
iteration : 4054
train acc:  0.75
train loss:  0.5072720050811768
train gradient:  0.14246542120799127
iteration : 4055
train acc:  0.734375
train loss:  0.5373040437698364
train gradient:  0.1577617500640694
iteration : 4056
train acc:  0.640625
train loss:  0.567721962928772
train gradient:  0.16783459530542982
iteration : 4057
train acc:  0.78125
train loss:  0.49675559997558594
train gradient:  0.19495921339358724
iteration : 4058
train acc:  0.65625
train loss:  0.6331407427787781
train gradient:  0.23196601106752893
iteration : 4059
train acc:  0.7578125
train loss:  0.5022411346435547
train gradient:  0.13728396483758243
iteration : 4060
train acc:  0.6640625
train loss:  0.5892857909202576
train gradient:  0.15521851059984723
iteration : 4061
train acc:  0.6953125
train loss:  0.5550797581672668
train gradient:  0.155300140620464
iteration : 4062
train acc:  0.6640625
train loss:  0.577079713344574
train gradient:  0.17453090339028263
iteration : 4063
train acc:  0.671875
train loss:  0.5392636656761169
train gradient:  0.13262751219113006
iteration : 4064
train acc:  0.7421875
train loss:  0.4914896488189697
train gradient:  0.1294699468716235
iteration : 4065
train acc:  0.703125
train loss:  0.5406500101089478
train gradient:  0.16719618113727314
iteration : 4066
train acc:  0.6953125
train loss:  0.5791573524475098
train gradient:  0.24258864120745172
iteration : 4067
train acc:  0.7265625
train loss:  0.5203659534454346
train gradient:  0.13283948182459504
iteration : 4068
train acc:  0.625
train loss:  0.5833009481430054
train gradient:  0.2275838299974065
iteration : 4069
train acc:  0.7109375
train loss:  0.5367565155029297
train gradient:  0.22833571525250845
iteration : 4070
train acc:  0.7265625
train loss:  0.5511379241943359
train gradient:  0.14774817625030565
iteration : 4071
train acc:  0.796875
train loss:  0.44342488050460815
train gradient:  0.10099613225078943
iteration : 4072
train acc:  0.7265625
train loss:  0.5078718662261963
train gradient:  0.17246410441363447
iteration : 4073
train acc:  0.640625
train loss:  0.5844420790672302
train gradient:  0.18971766951464358
iteration : 4074
train acc:  0.7421875
train loss:  0.4961842894554138
train gradient:  0.1301200246021617
iteration : 4075
train acc:  0.7421875
train loss:  0.4991248548030853
train gradient:  0.18642573443601518
iteration : 4076
train acc:  0.7265625
train loss:  0.5291060209274292
train gradient:  0.1366781455303146
iteration : 4077
train acc:  0.734375
train loss:  0.5019364953041077
train gradient:  0.18613143699286855
iteration : 4078
train acc:  0.7890625
train loss:  0.4811421036720276
train gradient:  0.14546771985544815
iteration : 4079
train acc:  0.75
train loss:  0.523426353931427
train gradient:  0.1296685935935372
iteration : 4080
train acc:  0.734375
train loss:  0.49411889910697937
train gradient:  0.20965564688204774
iteration : 4081
train acc:  0.7421875
train loss:  0.5008528232574463
train gradient:  0.14317570560781084
iteration : 4082
train acc:  0.7265625
train loss:  0.47848278284072876
train gradient:  0.13219492749045542
iteration : 4083
train acc:  0.6796875
train loss:  0.5495260953903198
train gradient:  0.16231876172210993
iteration : 4084
train acc:  0.6328125
train loss:  0.5509778261184692
train gradient:  0.17603811147515655
iteration : 4085
train acc:  0.7109375
train loss:  0.5592896938323975
train gradient:  0.14011609208765607
iteration : 4086
train acc:  0.7734375
train loss:  0.5004360675811768
train gradient:  0.1602327708590875
iteration : 4087
train acc:  0.6875
train loss:  0.5803918838500977
train gradient:  0.16662993046382915
iteration : 4088
train acc:  0.65625
train loss:  0.623261570930481
train gradient:  0.22092148582968082
iteration : 4089
train acc:  0.765625
train loss:  0.47018054127693176
train gradient:  0.11505427504206572
iteration : 4090
train acc:  0.7421875
train loss:  0.5077226161956787
train gradient:  0.12872821974801682
iteration : 4091
train acc:  0.8125
train loss:  0.46585917472839355
train gradient:  0.11219576570748509
iteration : 4092
train acc:  0.7421875
train loss:  0.503057599067688
train gradient:  0.17227001180914636
iteration : 4093
train acc:  0.7578125
train loss:  0.4768187999725342
train gradient:  0.13378633686867614
iteration : 4094
train acc:  0.7421875
train loss:  0.5237331986427307
train gradient:  0.1799110023615552
iteration : 4095
train acc:  0.7109375
train loss:  0.5029884576797485
train gradient:  0.1212303452983165
iteration : 4096
train acc:  0.71875
train loss:  0.5293998122215271
train gradient:  0.1702773495922858
iteration : 4097
train acc:  0.828125
train loss:  0.4272053837776184
train gradient:  0.11055878849998504
iteration : 4098
train acc:  0.75
train loss:  0.5019049644470215
train gradient:  0.2417337669621699
iteration : 4099
train acc:  0.7265625
train loss:  0.5408076643943787
train gradient:  0.2784627702690034
iteration : 4100
train acc:  0.671875
train loss:  0.609290361404419
train gradient:  0.1635290534253023
iteration : 4101
train acc:  0.6640625
train loss:  0.597000777721405
train gradient:  0.20961826475683754
iteration : 4102
train acc:  0.78125
train loss:  0.4599030613899231
train gradient:  0.11895257266914035
iteration : 4103
train acc:  0.7734375
train loss:  0.48280370235443115
train gradient:  0.1275599154042577
iteration : 4104
train acc:  0.71875
train loss:  0.5873130559921265
train gradient:  0.22209987676316972
iteration : 4105
train acc:  0.6640625
train loss:  0.5541725158691406
train gradient:  0.15107350974061617
iteration : 4106
train acc:  0.7109375
train loss:  0.5309849977493286
train gradient:  0.17023265727117876
iteration : 4107
train acc:  0.7109375
train loss:  0.5670568943023682
train gradient:  0.13756670763027512
iteration : 4108
train acc:  0.671875
train loss:  0.5798141956329346
train gradient:  0.18789506301332656
iteration : 4109
train acc:  0.75
train loss:  0.5194284915924072
train gradient:  0.17835058342373192
iteration : 4110
train acc:  0.6796875
train loss:  0.5502675771713257
train gradient:  0.17622013036499704
iteration : 4111
train acc:  0.640625
train loss:  0.5882596969604492
train gradient:  0.1625045106706919
iteration : 4112
train acc:  0.765625
train loss:  0.4647684693336487
train gradient:  0.1314309221647233
iteration : 4113
train acc:  0.6875
train loss:  0.5313287973403931
train gradient:  0.20097555200844602
iteration : 4114
train acc:  0.6953125
train loss:  0.5613692998886108
train gradient:  0.2574543502054702
iteration : 4115
train acc:  0.7578125
train loss:  0.49848347902297974
train gradient:  0.16225978707896052
iteration : 4116
train acc:  0.6484375
train loss:  0.571341872215271
train gradient:  0.17985620625712573
iteration : 4117
train acc:  0.78125
train loss:  0.4490840435028076
train gradient:  0.16994908732413672
iteration : 4118
train acc:  0.71875
train loss:  0.5206850171089172
train gradient:  0.1649968944948242
iteration : 4119
train acc:  0.671875
train loss:  0.5579137206077576
train gradient:  0.16489226712263166
iteration : 4120
train acc:  0.7578125
train loss:  0.47043174505233765
train gradient:  0.1188771514228341
iteration : 4121
train acc:  0.671875
train loss:  0.5425114631652832
train gradient:  0.26273159482971337
iteration : 4122
train acc:  0.71875
train loss:  0.5461925864219666
train gradient:  0.1385926276933137
iteration : 4123
train acc:  0.78125
train loss:  0.4592534303665161
train gradient:  0.17977236210663078
iteration : 4124
train acc:  0.7265625
train loss:  0.5152548551559448
train gradient:  0.19005896875176004
iteration : 4125
train acc:  0.734375
train loss:  0.49098822474479675
train gradient:  0.15015498035339475
iteration : 4126
train acc:  0.7734375
train loss:  0.4608350992202759
train gradient:  0.10895720032604453
iteration : 4127
train acc:  0.765625
train loss:  0.47395506501197815
train gradient:  0.12391668723171827
iteration : 4128
train acc:  0.7734375
train loss:  0.46012938022613525
train gradient:  0.0978053112429704
iteration : 4129
train acc:  0.7421875
train loss:  0.542883038520813
train gradient:  0.12288329356327868
iteration : 4130
train acc:  0.7578125
train loss:  0.4784702658653259
train gradient:  0.13572602002029013
iteration : 4131
train acc:  0.75
train loss:  0.5107053518295288
train gradient:  0.14650052055233442
iteration : 4132
train acc:  0.71875
train loss:  0.539405107498169
train gradient:  0.13646506564022928
iteration : 4133
train acc:  0.6640625
train loss:  0.576032280921936
train gradient:  0.1866049498264366
iteration : 4134
train acc:  0.6484375
train loss:  0.5444644093513489
train gradient:  0.20263277902561616
iteration : 4135
train acc:  0.7109375
train loss:  0.5150758028030396
train gradient:  0.13579107377467875
iteration : 4136
train acc:  0.7734375
train loss:  0.48241516947746277
train gradient:  0.15577301085226236
iteration : 4137
train acc:  0.7734375
train loss:  0.5094720125198364
train gradient:  0.14337572432887258
iteration : 4138
train acc:  0.734375
train loss:  0.538544774055481
train gradient:  0.13924518417509696
iteration : 4139
train acc:  0.7734375
train loss:  0.45517516136169434
train gradient:  0.0898820368142188
iteration : 4140
train acc:  0.7421875
train loss:  0.5035531520843506
train gradient:  0.1439897866104522
iteration : 4141
train acc:  0.8046875
train loss:  0.4510885775089264
train gradient:  0.09603437711319619
iteration : 4142
train acc:  0.7421875
train loss:  0.5504424571990967
train gradient:  0.1720665993584784
iteration : 4143
train acc:  0.7265625
train loss:  0.5231409072875977
train gradient:  0.16338661499866414
iteration : 4144
train acc:  0.7421875
train loss:  0.47721901535987854
train gradient:  0.14528932655504107
iteration : 4145
train acc:  0.765625
train loss:  0.5106082558631897
train gradient:  0.11804417798517973
iteration : 4146
train acc:  0.734375
train loss:  0.507554292678833
train gradient:  0.14898580775703774
iteration : 4147
train acc:  0.71875
train loss:  0.5420269966125488
train gradient:  0.212305922886023
iteration : 4148
train acc:  0.6640625
train loss:  0.564224123954773
train gradient:  0.21193128014877874
iteration : 4149
train acc:  0.7578125
train loss:  0.5541791319847107
train gradient:  0.145752214001027
iteration : 4150
train acc:  0.6875
train loss:  0.5300147533416748
train gradient:  0.15522133581092373
iteration : 4151
train acc:  0.796875
train loss:  0.4462146759033203
train gradient:  0.17775200569254992
iteration : 4152
train acc:  0.6484375
train loss:  0.5723339915275574
train gradient:  0.1466337136786755
iteration : 4153
train acc:  0.75
train loss:  0.49621549248695374
train gradient:  0.12197775834012457
iteration : 4154
train acc:  0.8203125
train loss:  0.43992912769317627
train gradient:  0.11526383287989318
iteration : 4155
train acc:  0.7421875
train loss:  0.5338374376296997
train gradient:  0.11864761649042087
iteration : 4156
train acc:  0.75
train loss:  0.4969198703765869
train gradient:  0.1499494482554014
iteration : 4157
train acc:  0.6953125
train loss:  0.5177314281463623
train gradient:  0.1910244099873179
iteration : 4158
train acc:  0.75
train loss:  0.5098516941070557
train gradient:  0.12515402180655594
iteration : 4159
train acc:  0.6796875
train loss:  0.5872874855995178
train gradient:  0.20099035860112915
iteration : 4160
train acc:  0.7265625
train loss:  0.519625186920166
train gradient:  0.17357726279460187
iteration : 4161
train acc:  0.796875
train loss:  0.5070228576660156
train gradient:  0.14410113168833316
iteration : 4162
train acc:  0.671875
train loss:  0.5386399030685425
train gradient:  0.16685716228892256
iteration : 4163
train acc:  0.765625
train loss:  0.4753318727016449
train gradient:  0.1036283765805099
iteration : 4164
train acc:  0.7265625
train loss:  0.47637441754341125
train gradient:  0.13744390318418626
iteration : 4165
train acc:  0.71875
train loss:  0.5645776391029358
train gradient:  0.19509130698264648
iteration : 4166
train acc:  0.7421875
train loss:  0.5394466519355774
train gradient:  0.1478375227578036
iteration : 4167
train acc:  0.671875
train loss:  0.5293245315551758
train gradient:  0.16509704027605177
iteration : 4168
train acc:  0.6953125
train loss:  0.5229344367980957
train gradient:  0.16927992181651175
iteration : 4169
train acc:  0.71875
train loss:  0.5856504440307617
train gradient:  0.21115499502673798
iteration : 4170
train acc:  0.7734375
train loss:  0.4758334755897522
train gradient:  0.12636339118813136
iteration : 4171
train acc:  0.734375
train loss:  0.4826864004135132
train gradient:  0.12165172982482911
iteration : 4172
train acc:  0.7265625
train loss:  0.5412817597389221
train gradient:  0.16451349524942152
iteration : 4173
train acc:  0.796875
train loss:  0.43637406826019287
train gradient:  0.14969741478505766
iteration : 4174
train acc:  0.7109375
train loss:  0.5192286968231201
train gradient:  0.13867346126228497
iteration : 4175
train acc:  0.7265625
train loss:  0.5289106965065002
train gradient:  0.14496028559195537
iteration : 4176
train acc:  0.8046875
train loss:  0.496131956577301
train gradient:  0.12443539269577883
iteration : 4177
train acc:  0.6953125
train loss:  0.551261305809021
train gradient:  0.1643215831627216
iteration : 4178
train acc:  0.7421875
train loss:  0.4928617477416992
train gradient:  0.13489186487083737
iteration : 4179
train acc:  0.703125
train loss:  0.5500088930130005
train gradient:  0.17261059776864884
iteration : 4180
train acc:  0.765625
train loss:  0.5197044014930725
train gradient:  0.15429161566404448
iteration : 4181
train acc:  0.6171875
train loss:  0.6724361181259155
train gradient:  0.3859617899048899
iteration : 4182
train acc:  0.6953125
train loss:  0.5473799705505371
train gradient:  0.14893711352162825
iteration : 4183
train acc:  0.6640625
train loss:  0.5855108499526978
train gradient:  0.21799490646170783
iteration : 4184
train acc:  0.7890625
train loss:  0.4404316842556
train gradient:  0.1626189168578315
iteration : 4185
train acc:  0.6640625
train loss:  0.5992579460144043
train gradient:  0.17341154784871543
iteration : 4186
train acc:  0.765625
train loss:  0.5052670240402222
train gradient:  0.15902948657884697
iteration : 4187
train acc:  0.7109375
train loss:  0.5426274538040161
train gradient:  0.12787665883381893
iteration : 4188
train acc:  0.671875
train loss:  0.6259505748748779
train gradient:  0.21260691998358391
iteration : 4189
train acc:  0.703125
train loss:  0.5065560936927795
train gradient:  0.15339927156003436
iteration : 4190
train acc:  0.7265625
train loss:  0.4961414337158203
train gradient:  0.12805942913272733
iteration : 4191
train acc:  0.7109375
train loss:  0.5458998680114746
train gradient:  0.15976903136588033
iteration : 4192
train acc:  0.7265625
train loss:  0.5549224615097046
train gradient:  0.18450305619770643
iteration : 4193
train acc:  0.6953125
train loss:  0.5603570938110352
train gradient:  0.16671678164152315
iteration : 4194
train acc:  0.71875
train loss:  0.5229291915893555
train gradient:  0.21564745223672882
iteration : 4195
train acc:  0.734375
train loss:  0.524654746055603
train gradient:  0.1596100389797313
iteration : 4196
train acc:  0.7265625
train loss:  0.5318307280540466
train gradient:  0.17640707567503505
iteration : 4197
train acc:  0.71875
train loss:  0.5069116950035095
train gradient:  0.13143789624373675
iteration : 4198
train acc:  0.6796875
train loss:  0.5570282936096191
train gradient:  0.1329266789373147
iteration : 4199
train acc:  0.65625
train loss:  0.5709131360054016
train gradient:  0.15707303495627378
iteration : 4200
train acc:  0.765625
train loss:  0.47716376185417175
train gradient:  0.1377894103082486
iteration : 4201
train acc:  0.71875
train loss:  0.5163558721542358
train gradient:  0.12975166776583433
iteration : 4202
train acc:  0.7421875
train loss:  0.4927317500114441
train gradient:  0.1327708113020447
iteration : 4203
train acc:  0.65625
train loss:  0.6093668937683105
train gradient:  0.2026256707234495
iteration : 4204
train acc:  0.75
train loss:  0.534337043762207
train gradient:  0.1818065348979181
iteration : 4205
train acc:  0.7265625
train loss:  0.5336979627609253
train gradient:  0.16477805273079377
iteration : 4206
train acc:  0.6953125
train loss:  0.5862544775009155
train gradient:  0.1827990997058387
iteration : 4207
train acc:  0.734375
train loss:  0.5236960649490356
train gradient:  0.1352781727266214
iteration : 4208
train acc:  0.734375
train loss:  0.5413724184036255
train gradient:  0.1502397820835883
iteration : 4209
train acc:  0.7421875
train loss:  0.49524393677711487
train gradient:  0.11628496350484381
iteration : 4210
train acc:  0.703125
train loss:  0.5389167666435242
train gradient:  0.17083349563071687
iteration : 4211
train acc:  0.6796875
train loss:  0.5332843661308289
train gradient:  0.1410965597002704
iteration : 4212
train acc:  0.7109375
train loss:  0.565708339214325
train gradient:  0.15011204908531262
iteration : 4213
train acc:  0.6796875
train loss:  0.5783043503761292
train gradient:  0.21103164622256618
iteration : 4214
train acc:  0.71875
train loss:  0.529854416847229
train gradient:  0.17911879344785647
iteration : 4215
train acc:  0.78125
train loss:  0.4707842469215393
train gradient:  0.19677654356585106
iteration : 4216
train acc:  0.75
train loss:  0.48202696442604065
train gradient:  0.142917027010095
iteration : 4217
train acc:  0.7421875
train loss:  0.5254421830177307
train gradient:  0.1666710115713375
iteration : 4218
train acc:  0.78125
train loss:  0.47606393694877625
train gradient:  0.12440737626948882
iteration : 4219
train acc:  0.7890625
train loss:  0.4645773470401764
train gradient:  0.11253196484803726
iteration : 4220
train acc:  0.6875
train loss:  0.5327627658843994
train gradient:  0.2014762985318168
iteration : 4221
train acc:  0.6953125
train loss:  0.5528849959373474
train gradient:  0.14700809778452364
iteration : 4222
train acc:  0.6953125
train loss:  0.5133565664291382
train gradient:  0.2146454513610681
iteration : 4223
train acc:  0.6796875
train loss:  0.5617668628692627
train gradient:  0.17155167184718406
iteration : 4224
train acc:  0.71875
train loss:  0.5258075594902039
train gradient:  0.14295316496185884
iteration : 4225
train acc:  0.703125
train loss:  0.5156375169754028
train gradient:  0.25274911675468403
iteration : 4226
train acc:  0.6953125
train loss:  0.5240080952644348
train gradient:  0.18905280503150027
iteration : 4227
train acc:  0.734375
train loss:  0.5443095564842224
train gradient:  0.17038155892789958
iteration : 4228
train acc:  0.75
train loss:  0.4877447783946991
train gradient:  0.11246375851557638
iteration : 4229
train acc:  0.7578125
train loss:  0.49322664737701416
train gradient:  0.12822025084752114
iteration : 4230
train acc:  0.7578125
train loss:  0.5080956816673279
train gradient:  0.1589021762706852
iteration : 4231
train acc:  0.71875
train loss:  0.5233787894248962
train gradient:  0.17348640836374657
iteration : 4232
train acc:  0.8046875
train loss:  0.454347163438797
train gradient:  0.1646996724380331
iteration : 4233
train acc:  0.6484375
train loss:  0.6038089990615845
train gradient:  0.17866313921499427
iteration : 4234
train acc:  0.6953125
train loss:  0.5497011542320251
train gradient:  0.15856846719819995
iteration : 4235
train acc:  0.765625
train loss:  0.4365472197532654
train gradient:  0.13508555502418035
iteration : 4236
train acc:  0.7109375
train loss:  0.5518690943717957
train gradient:  0.16104598452043994
iteration : 4237
train acc:  0.6796875
train loss:  0.5626522302627563
train gradient:  0.19376923768028975
iteration : 4238
train acc:  0.7890625
train loss:  0.48085182905197144
train gradient:  0.1454266328330157
iteration : 4239
train acc:  0.7578125
train loss:  0.4734411835670471
train gradient:  0.1213638434422761
iteration : 4240
train acc:  0.6796875
train loss:  0.6251891851425171
train gradient:  0.22271971677125812
iteration : 4241
train acc:  0.703125
train loss:  0.5407910346984863
train gradient:  0.13053832214594607
iteration : 4242
train acc:  0.7265625
train loss:  0.4947989284992218
train gradient:  0.12935480302486024
iteration : 4243
train acc:  0.7421875
train loss:  0.47634339332580566
train gradient:  0.13087498282861118
iteration : 4244
train acc:  0.65625
train loss:  0.5580875873565674
train gradient:  0.1419030400846864
iteration : 4245
train acc:  0.71875
train loss:  0.5100266933441162
train gradient:  0.16157687406911653
iteration : 4246
train acc:  0.8125
train loss:  0.459297239780426
train gradient:  0.2752457228191839
iteration : 4247
train acc:  0.7734375
train loss:  0.4600961208343506
train gradient:  0.10953658110661342
iteration : 4248
train acc:  0.703125
train loss:  0.5242059230804443
train gradient:  0.174409306341263
iteration : 4249
train acc:  0.734375
train loss:  0.49478691816329956
train gradient:  0.15209886645099216
iteration : 4250
train acc:  0.6796875
train loss:  0.5691269636154175
train gradient:  0.21918396945354202
iteration : 4251
train acc:  0.734375
train loss:  0.4853162169456482
train gradient:  0.12198542369487299
iteration : 4252
train acc:  0.7734375
train loss:  0.5126145482063293
train gradient:  0.17932666407933948
iteration : 4253
train acc:  0.6953125
train loss:  0.6070583462715149
train gradient:  0.2695588647366434
iteration : 4254
train acc:  0.765625
train loss:  0.5246187448501587
train gradient:  0.1689570466034171
iteration : 4255
train acc:  0.6875
train loss:  0.5693812966346741
train gradient:  0.15549188909403835
iteration : 4256
train acc:  0.71875
train loss:  0.5707255601882935
train gradient:  0.1996520221131591
iteration : 4257
train acc:  0.7578125
train loss:  0.509148120880127
train gradient:  0.11267496112122552
iteration : 4258
train acc:  0.828125
train loss:  0.4444984793663025
train gradient:  0.13900051893907078
iteration : 4259
train acc:  0.7109375
train loss:  0.5555529594421387
train gradient:  0.16214626658391357
iteration : 4260
train acc:  0.7265625
train loss:  0.5361411571502686
train gradient:  0.17069948014242717
iteration : 4261
train acc:  0.7734375
train loss:  0.4588403105735779
train gradient:  0.13050539812305617
iteration : 4262
train acc:  0.6953125
train loss:  0.5621520280838013
train gradient:  0.142906723041731
iteration : 4263
train acc:  0.6796875
train loss:  0.5694693326950073
train gradient:  0.15969721358456185
iteration : 4264
train acc:  0.7265625
train loss:  0.5393432974815369
train gradient:  0.19993968007788254
iteration : 4265
train acc:  0.6875
train loss:  0.5459092259407043
train gradient:  0.13754216365821195
iteration : 4266
train acc:  0.734375
train loss:  0.5360554456710815
train gradient:  0.18200359905500316
iteration : 4267
train acc:  0.7421875
train loss:  0.5160058736801147
train gradient:  0.15043329021055885
iteration : 4268
train acc:  0.7109375
train loss:  0.570881187915802
train gradient:  0.13519482677821998
iteration : 4269
train acc:  0.765625
train loss:  0.5210873484611511
train gradient:  0.2001578604054159
iteration : 4270
train acc:  0.703125
train loss:  0.5279681086540222
train gradient:  0.14913643193449588
iteration : 4271
train acc:  0.734375
train loss:  0.5118204355239868
train gradient:  0.15650735542096583
iteration : 4272
train acc:  0.8046875
train loss:  0.4424360990524292
train gradient:  0.11188818613212813
iteration : 4273
train acc:  0.6953125
train loss:  0.5426465272903442
train gradient:  0.18602039070899518
iteration : 4274
train acc:  0.6953125
train loss:  0.4829733073711395
train gradient:  0.13969468767719345
iteration : 4275
train acc:  0.7265625
train loss:  0.5375136733055115
train gradient:  0.1729979536497357
iteration : 4276
train acc:  0.78125
train loss:  0.4810473620891571
train gradient:  0.16659629240301116
iteration : 4277
train acc:  0.71875
train loss:  0.5341033935546875
train gradient:  0.17222102669082198
iteration : 4278
train acc:  0.7265625
train loss:  0.5152762532234192
train gradient:  0.12187085100770303
iteration : 4279
train acc:  0.734375
train loss:  0.5098248720169067
train gradient:  0.1476213009557097
iteration : 4280
train acc:  0.6953125
train loss:  0.5301146507263184
train gradient:  0.16688755580586936
iteration : 4281
train acc:  0.703125
train loss:  0.49699607491493225
train gradient:  0.1136576905687804
iteration : 4282
train acc:  0.703125
train loss:  0.5453711748123169
train gradient:  0.15429424454050278
iteration : 4283
train acc:  0.703125
train loss:  0.5040614604949951
train gradient:  0.13639544873359863
iteration : 4284
train acc:  0.7109375
train loss:  0.49575889110565186
train gradient:  0.13454212620644945
iteration : 4285
train acc:  0.7109375
train loss:  0.5633967518806458
train gradient:  0.17702483082530782
iteration : 4286
train acc:  0.6953125
train loss:  0.5240890383720398
train gradient:  0.19618266003539475
iteration : 4287
train acc:  0.7421875
train loss:  0.5315533876419067
train gradient:  0.1622866333649889
iteration : 4288
train acc:  0.7578125
train loss:  0.4878194332122803
train gradient:  0.17160522974801706
iteration : 4289
train acc:  0.7421875
train loss:  0.5463987588882446
train gradient:  0.18658281038386132
iteration : 4290
train acc:  0.703125
train loss:  0.5656678080558777
train gradient:  0.17559011068603758
iteration : 4291
train acc:  0.7265625
train loss:  0.49543389678001404
train gradient:  0.13345048614015054
iteration : 4292
train acc:  0.7578125
train loss:  0.4897304177284241
train gradient:  0.14531006940470714
iteration : 4293
train acc:  0.765625
train loss:  0.49987494945526123
train gradient:  0.13841866037619371
iteration : 4294
train acc:  0.6875
train loss:  0.5662487745285034
train gradient:  0.19569839867205482
iteration : 4295
train acc:  0.7421875
train loss:  0.48356127738952637
train gradient:  0.16473653330211555
iteration : 4296
train acc:  0.7109375
train loss:  0.5380856990814209
train gradient:  0.18833982027747959
iteration : 4297
train acc:  0.71875
train loss:  0.5829616785049438
train gradient:  0.2753091470399694
iteration : 4298
train acc:  0.7421875
train loss:  0.49142128229141235
train gradient:  0.17064705121071005
iteration : 4299
train acc:  0.7109375
train loss:  0.5319024324417114
train gradient:  0.15265245129755667
iteration : 4300
train acc:  0.7109375
train loss:  0.5147374868392944
train gradient:  0.1450156263419766
iteration : 4301
train acc:  0.7109375
train loss:  0.5060646533966064
train gradient:  0.12177268989218949
iteration : 4302
train acc:  0.734375
train loss:  0.4667831063270569
train gradient:  0.15637861901503872
iteration : 4303
train acc:  0.7421875
train loss:  0.49399495124816895
train gradient:  0.19874416816584684
iteration : 4304
train acc:  0.6484375
train loss:  0.577792763710022
train gradient:  0.2036671973095221
iteration : 4305
train acc:  0.828125
train loss:  0.43970975279808044
train gradient:  0.11308153651061528
iteration : 4306
train acc:  0.7421875
train loss:  0.49185603857040405
train gradient:  0.14454017016817244
iteration : 4307
train acc:  0.6875
train loss:  0.5775889158248901
train gradient:  0.24706786644792222
iteration : 4308
train acc:  0.734375
train loss:  0.4866469204425812
train gradient:  0.12878940698560698
iteration : 4309
train acc:  0.734375
train loss:  0.5425693988800049
train gradient:  0.17454711161388503
iteration : 4310
train acc:  0.8046875
train loss:  0.4759175777435303
train gradient:  0.13816132682438623
iteration : 4311
train acc:  0.7109375
train loss:  0.5288784503936768
train gradient:  0.12543666575398932
iteration : 4312
train acc:  0.7421875
train loss:  0.4946117401123047
train gradient:  0.1451534502062228
iteration : 4313
train acc:  0.7109375
train loss:  0.5275517702102661
train gradient:  0.17100397156200708
iteration : 4314
train acc:  0.6796875
train loss:  0.5481884479522705
train gradient:  0.1404708524025688
iteration : 4315
train acc:  0.7421875
train loss:  0.5274611711502075
train gradient:  0.1879803169019449
iteration : 4316
train acc:  0.7265625
train loss:  0.4960373640060425
train gradient:  0.12250457138979855
iteration : 4317
train acc:  0.6796875
train loss:  0.5554623007774353
train gradient:  0.15527734908786217
iteration : 4318
train acc:  0.7109375
train loss:  0.5147757530212402
train gradient:  0.14193492999249002
iteration : 4319
train acc:  0.671875
train loss:  0.52873694896698
train gradient:  0.217393259457896
iteration : 4320
train acc:  0.7734375
train loss:  0.5469211339950562
train gradient:  0.23436137751126035
iteration : 4321
train acc:  0.734375
train loss:  0.5255681276321411
train gradient:  0.118799235241573
iteration : 4322
train acc:  0.6484375
train loss:  0.559089183807373
train gradient:  0.20835639211379614
iteration : 4323
train acc:  0.671875
train loss:  0.5769109725952148
train gradient:  0.22047295760214963
iteration : 4324
train acc:  0.7265625
train loss:  0.50760817527771
train gradient:  0.15365834931193068
iteration : 4325
train acc:  0.703125
train loss:  0.5340723395347595
train gradient:  0.12965259371614185
iteration : 4326
train acc:  0.7109375
train loss:  0.5538506507873535
train gradient:  0.18371059707698148
iteration : 4327
train acc:  0.7734375
train loss:  0.4597530663013458
train gradient:  0.12518687226520575
iteration : 4328
train acc:  0.7109375
train loss:  0.5176230669021606
train gradient:  0.12942892851976018
iteration : 4329
train acc:  0.7265625
train loss:  0.5411072969436646
train gradient:  0.1560855388147235
iteration : 4330
train acc:  0.625
train loss:  0.5808237791061401
train gradient:  0.17326532116259535
iteration : 4331
train acc:  0.71875
train loss:  0.510694146156311
train gradient:  0.14679607162909752
iteration : 4332
train acc:  0.703125
train loss:  0.5366266369819641
train gradient:  0.19882146308083198
iteration : 4333
train acc:  0.734375
train loss:  0.5237004160881042
train gradient:  0.16410236089945984
iteration : 4334
train acc:  0.7734375
train loss:  0.5274140238761902
train gradient:  0.17440167508843513
iteration : 4335
train acc:  0.71875
train loss:  0.48761218786239624
train gradient:  0.1359315010946248
iteration : 4336
train acc:  0.71875
train loss:  0.587849497795105
train gradient:  0.21483097525843992
iteration : 4337
train acc:  0.7109375
train loss:  0.5484737157821655
train gradient:  0.15676886731713385
iteration : 4338
train acc:  0.7265625
train loss:  0.5425186157226562
train gradient:  0.1979758744714153
iteration : 4339
train acc:  0.6875
train loss:  0.5311360955238342
train gradient:  0.16251863677658057
iteration : 4340
train acc:  0.6640625
train loss:  0.5707738399505615
train gradient:  0.21772862600324489
iteration : 4341
train acc:  0.71875
train loss:  0.549647331237793
train gradient:  0.18719867035574078
iteration : 4342
train acc:  0.7421875
train loss:  0.5511887669563293
train gradient:  0.16455621091946027
iteration : 4343
train acc:  0.765625
train loss:  0.5298293828964233
train gradient:  0.1524731400725376
iteration : 4344
train acc:  0.6796875
train loss:  0.5293344259262085
train gradient:  0.1431154368677071
iteration : 4345
train acc:  0.6875
train loss:  0.5639417171478271
train gradient:  0.1525842526159259
iteration : 4346
train acc:  0.765625
train loss:  0.4429735541343689
train gradient:  0.11760412808471475
iteration : 4347
train acc:  0.7265625
train loss:  0.5406301021575928
train gradient:  0.14884774952963042
iteration : 4348
train acc:  0.7421875
train loss:  0.49909961223602295
train gradient:  0.18812102561459015
iteration : 4349
train acc:  0.7578125
train loss:  0.4901713728904724
train gradient:  0.11546830529114144
iteration : 4350
train acc:  0.671875
train loss:  0.5796291828155518
train gradient:  0.17926292491259532
iteration : 4351
train acc:  0.71875
train loss:  0.5486757159233093
train gradient:  0.17183736830092863
iteration : 4352
train acc:  0.734375
train loss:  0.4721229374408722
train gradient:  0.1079291183428533
iteration : 4353
train acc:  0.71875
train loss:  0.5292450189590454
train gradient:  0.19210821564703884
iteration : 4354
train acc:  0.7421875
train loss:  0.5101848840713501
train gradient:  0.21951431672335875
iteration : 4355
train acc:  0.71875
train loss:  0.5422124266624451
train gradient:  0.14466669512569738
iteration : 4356
train acc:  0.765625
train loss:  0.46324479579925537
train gradient:  0.123448591673104
iteration : 4357
train acc:  0.71875
train loss:  0.5446504354476929
train gradient:  0.16925393174146763
iteration : 4358
train acc:  0.640625
train loss:  0.5978697538375854
train gradient:  0.2031690491205726
iteration : 4359
train acc:  0.671875
train loss:  0.5717775821685791
train gradient:  0.18780217266392613
iteration : 4360
train acc:  0.6796875
train loss:  0.5228065252304077
train gradient:  0.14770771891984347
iteration : 4361
train acc:  0.671875
train loss:  0.5459638237953186
train gradient:  0.13773836327794764
iteration : 4362
train acc:  0.7578125
train loss:  0.4620203971862793
train gradient:  0.13490573119060634
iteration : 4363
train acc:  0.71875
train loss:  0.5038182735443115
train gradient:  0.1936011185149139
iteration : 4364
train acc:  0.734375
train loss:  0.4749358296394348
train gradient:  0.15055867324602723
iteration : 4365
train acc:  0.6875
train loss:  0.5483064651489258
train gradient:  0.18497526249519308
iteration : 4366
train acc:  0.7265625
train loss:  0.5250991582870483
train gradient:  0.14253238998040987
iteration : 4367
train acc:  0.7734375
train loss:  0.5109773874282837
train gradient:  0.17923026047074994
iteration : 4368
train acc:  0.7421875
train loss:  0.5454516410827637
train gradient:  0.12636330774514415
iteration : 4369
train acc:  0.734375
train loss:  0.47775256633758545
train gradient:  0.13670294995400717
iteration : 4370
train acc:  0.6875
train loss:  0.5633798837661743
train gradient:  0.14638915358864835
iteration : 4371
train acc:  0.7109375
train loss:  0.6078152656555176
train gradient:  0.14873006741785255
iteration : 4372
train acc:  0.6875
train loss:  0.5983548164367676
train gradient:  0.20311212220105207
iteration : 4373
train acc:  0.65625
train loss:  0.5814668536186218
train gradient:  0.1601124086355231
iteration : 4374
train acc:  0.7421875
train loss:  0.49254870414733887
train gradient:  0.1335664654418229
iteration : 4375
train acc:  0.65625
train loss:  0.5744733810424805
train gradient:  0.16475129398496852
iteration : 4376
train acc:  0.7578125
train loss:  0.4717795252799988
train gradient:  0.12751751906537057
iteration : 4377
train acc:  0.640625
train loss:  0.5721018314361572
train gradient:  0.24357623835179965
iteration : 4378
train acc:  0.7421875
train loss:  0.5643613338470459
train gradient:  0.19959339269968301
iteration : 4379
train acc:  0.734375
train loss:  0.5223895311355591
train gradient:  0.1355346596072955
iteration : 4380
train acc:  0.71875
train loss:  0.4837080240249634
train gradient:  0.1665819445888773
iteration : 4381
train acc:  0.7109375
train loss:  0.5350397825241089
train gradient:  0.18618626377128367
iteration : 4382
train acc:  0.7421875
train loss:  0.5203622579574585
train gradient:  0.11437211704625565
iteration : 4383
train acc:  0.7265625
train loss:  0.4923608899116516
train gradient:  0.13285580395453678
iteration : 4384
train acc:  0.6875
train loss:  0.5135670304298401
train gradient:  0.15164858839229073
iteration : 4385
train acc:  0.6953125
train loss:  0.5211414694786072
train gradient:  0.1328536682419347
iteration : 4386
train acc:  0.7109375
train loss:  0.5138038396835327
train gradient:  0.22804427486684045
iteration : 4387
train acc:  0.734375
train loss:  0.5083461999893188
train gradient:  0.17048430815329632
iteration : 4388
train acc:  0.75
train loss:  0.4978022575378418
train gradient:  0.13972216617726463
iteration : 4389
train acc:  0.765625
train loss:  0.48357662558555603
train gradient:  0.11765659508984598
iteration : 4390
train acc:  0.796875
train loss:  0.4673025608062744
train gradient:  0.11510974698634055
iteration : 4391
train acc:  0.703125
train loss:  0.5305957794189453
train gradient:  0.13008207907703112
iteration : 4392
train acc:  0.84375
train loss:  0.4428321123123169
train gradient:  0.09780511553066405
iteration : 4393
train acc:  0.7734375
train loss:  0.4964909255504608
train gradient:  0.15248175011866077
iteration : 4394
train acc:  0.71875
train loss:  0.562947690486908
train gradient:  0.18844559521437845
iteration : 4395
train acc:  0.71875
train loss:  0.5009249448776245
train gradient:  0.11415701307062893
iteration : 4396
train acc:  0.703125
train loss:  0.5401550531387329
train gradient:  0.11310364221667728
iteration : 4397
train acc:  0.8046875
train loss:  0.5042712688446045
train gradient:  0.15350688745560181
iteration : 4398
train acc:  0.6953125
train loss:  0.5707463622093201
train gradient:  0.1550488789869553
iteration : 4399
train acc:  0.6640625
train loss:  0.6254477500915527
train gradient:  0.1857233524430808
iteration : 4400
train acc:  0.65625
train loss:  0.5635665655136108
train gradient:  0.16073147370866697
iteration : 4401
train acc:  0.7890625
train loss:  0.4850826859474182
train gradient:  0.15774232059526316
iteration : 4402
train acc:  0.75
train loss:  0.526674211025238
train gradient:  0.1315458953886911
iteration : 4403
train acc:  0.6875
train loss:  0.5594550967216492
train gradient:  0.1699736268415527
iteration : 4404
train acc:  0.7890625
train loss:  0.5032286047935486
train gradient:  0.12226590862885126
iteration : 4405
train acc:  0.7734375
train loss:  0.4820702075958252
train gradient:  0.15011822391026414
iteration : 4406
train acc:  0.6796875
train loss:  0.5512794256210327
train gradient:  0.14829275532054226
iteration : 4407
train acc:  0.7734375
train loss:  0.455091655254364
train gradient:  0.13804437196573388
iteration : 4408
train acc:  0.7734375
train loss:  0.4770098328590393
train gradient:  0.15048383951177052
iteration : 4409
train acc:  0.671875
train loss:  0.5758290886878967
train gradient:  0.16165508001876644
iteration : 4410
train acc:  0.8203125
train loss:  0.4470878541469574
train gradient:  0.14946148076426108
iteration : 4411
train acc:  0.7734375
train loss:  0.4589976966381073
train gradient:  0.1113851424726766
iteration : 4412
train acc:  0.6953125
train loss:  0.5555901527404785
train gradient:  0.14561774116627435
iteration : 4413
train acc:  0.734375
train loss:  0.5793571472167969
train gradient:  0.17976108189953605
iteration : 4414
train acc:  0.7421875
train loss:  0.5198465585708618
train gradient:  0.1692217082653862
iteration : 4415
train acc:  0.703125
train loss:  0.5136568546295166
train gradient:  0.15918725772499723
iteration : 4416
train acc:  0.6796875
train loss:  0.5822391510009766
train gradient:  0.17953130080580265
iteration : 4417
train acc:  0.75
train loss:  0.48312047123908997
train gradient:  0.11117298948854717
iteration : 4418
train acc:  0.6953125
train loss:  0.5546184778213501
train gradient:  0.15767880799465428
iteration : 4419
train acc:  0.6640625
train loss:  0.561636209487915
train gradient:  0.1877595495003737
iteration : 4420
train acc:  0.6953125
train loss:  0.5156075954437256
train gradient:  0.16118125778248954
iteration : 4421
train acc:  0.6953125
train loss:  0.5345078706741333
train gradient:  0.1614224791803007
iteration : 4422
train acc:  0.7265625
train loss:  0.557702898979187
train gradient:  0.1970322172621773
iteration : 4423
train acc:  0.6875
train loss:  0.4944390058517456
train gradient:  0.1556262596364515
iteration : 4424
train acc:  0.7421875
train loss:  0.48335301876068115
train gradient:  0.1280286790535691
iteration : 4425
train acc:  0.78125
train loss:  0.4476270079612732
train gradient:  0.11161921351722844
iteration : 4426
train acc:  0.7265625
train loss:  0.47284412384033203
train gradient:  0.1422648577770343
iteration : 4427
train acc:  0.796875
train loss:  0.42888057231903076
train gradient:  0.12179961648591871
iteration : 4428
train acc:  0.6953125
train loss:  0.618834376335144
train gradient:  0.21904251652427786
iteration : 4429
train acc:  0.765625
train loss:  0.44105154275894165
train gradient:  0.12104953168716864
iteration : 4430
train acc:  0.7265625
train loss:  0.4899594187736511
train gradient:  0.16641371958122025
iteration : 4431
train acc:  0.6484375
train loss:  0.5699458122253418
train gradient:  0.19925762044776257
iteration : 4432
train acc:  0.796875
train loss:  0.46471846103668213
train gradient:  0.13689650571190962
iteration : 4433
train acc:  0.6796875
train loss:  0.5630041360855103
train gradient:  0.14439626399912575
iteration : 4434
train acc:  0.734375
train loss:  0.5102711915969849
train gradient:  0.13999238755060034
iteration : 4435
train acc:  0.7421875
train loss:  0.5136998891830444
train gradient:  0.15789266997715445
iteration : 4436
train acc:  0.8125
train loss:  0.42897143959999084
train gradient:  0.11026071786143514
iteration : 4437
train acc:  0.765625
train loss:  0.501545250415802
train gradient:  0.12899101850423172
iteration : 4438
train acc:  0.7265625
train loss:  0.5265429019927979
train gradient:  0.2362740093832002
iteration : 4439
train acc:  0.7421875
train loss:  0.505872368812561
train gradient:  0.16357909640339988
iteration : 4440
train acc:  0.640625
train loss:  0.6086441874504089
train gradient:  0.2016717096191039
iteration : 4441
train acc:  0.7265625
train loss:  0.49884623289108276
train gradient:  0.1689758897640599
iteration : 4442
train acc:  0.6953125
train loss:  0.5559818744659424
train gradient:  0.1823812885139579
iteration : 4443
train acc:  0.7421875
train loss:  0.507642388343811
train gradient:  0.14422037527236065
iteration : 4444
train acc:  0.6875
train loss:  0.5307987928390503
train gradient:  0.17104314881655658
iteration : 4445
train acc:  0.78125
train loss:  0.4896632432937622
train gradient:  0.14056693840892615
iteration : 4446
train acc:  0.7734375
train loss:  0.4977286756038666
train gradient:  0.14886156417642027
iteration : 4447
train acc:  0.7421875
train loss:  0.525717556476593
train gradient:  0.13998640306486776
iteration : 4448
train acc:  0.671875
train loss:  0.5783290863037109
train gradient:  0.22499922009998863
iteration : 4449
train acc:  0.703125
train loss:  0.5456624031066895
train gradient:  0.1489649020489753
iteration : 4450
train acc:  0.7734375
train loss:  0.4657537639141083
train gradient:  0.1266885795151087
iteration : 4451
train acc:  0.7109375
train loss:  0.5437073707580566
train gradient:  0.1667437085575278
iteration : 4452
train acc:  0.7578125
train loss:  0.4749731421470642
train gradient:  0.12667514743987388
iteration : 4453
train acc:  0.7421875
train loss:  0.5340582132339478
train gradient:  0.15595605832092646
iteration : 4454
train acc:  0.7734375
train loss:  0.4447034001350403
train gradient:  0.09761043227215566
iteration : 4455
train acc:  0.6875
train loss:  0.585936427116394
train gradient:  0.19388989850023314
iteration : 4456
train acc:  0.7578125
train loss:  0.4816984534263611
train gradient:  0.1335970197437617
iteration : 4457
train acc:  0.75
train loss:  0.5103000402450562
train gradient:  0.1436084094176303
iteration : 4458
train acc:  0.734375
train loss:  0.471554160118103
train gradient:  0.10955537163576211
iteration : 4459
train acc:  0.625
train loss:  0.6231761574745178
train gradient:  0.1889881940345644
iteration : 4460
train acc:  0.6484375
train loss:  0.592040479183197
train gradient:  0.15174836990202412
iteration : 4461
train acc:  0.671875
train loss:  0.5576448440551758
train gradient:  0.1517982121241387
iteration : 4462
train acc:  0.6796875
train loss:  0.5963301658630371
train gradient:  0.24977426683103013
iteration : 4463
train acc:  0.71875
train loss:  0.5872161388397217
train gradient:  0.22761626900958065
iteration : 4464
train acc:  0.6953125
train loss:  0.5014486312866211
train gradient:  0.14497203810179093
iteration : 4465
train acc:  0.765625
train loss:  0.4986797571182251
train gradient:  0.1399240077345768
iteration : 4466
train acc:  0.734375
train loss:  0.49601486325263977
train gradient:  0.15224488898609984
iteration : 4467
train acc:  0.6953125
train loss:  0.5565658807754517
train gradient:  0.1378119962869793
iteration : 4468
train acc:  0.75
train loss:  0.5115516781806946
train gradient:  0.11618709845699207
iteration : 4469
train acc:  0.6953125
train loss:  0.4877166748046875
train gradient:  0.1764067080784381
iteration : 4470
train acc:  0.765625
train loss:  0.4830400049686432
train gradient:  0.13644551902456323
iteration : 4471
train acc:  0.7421875
train loss:  0.4934089183807373
train gradient:  0.1489862670333299
iteration : 4472
train acc:  0.6328125
train loss:  0.5544207692146301
train gradient:  0.141030440994205
iteration : 4473
train acc:  0.6796875
train loss:  0.5351189374923706
train gradient:  0.129076665169218
iteration : 4474
train acc:  0.7265625
train loss:  0.5346018075942993
train gradient:  0.15269692737441476
iteration : 4475
train acc:  0.765625
train loss:  0.4813012480735779
train gradient:  0.12937008243799458
iteration : 4476
train acc:  0.75
train loss:  0.4811209738254547
train gradient:  0.10452315041741805
iteration : 4477
train acc:  0.6953125
train loss:  0.5445322394371033
train gradient:  0.1827295614646136
iteration : 4478
train acc:  0.75
train loss:  0.5120956897735596
train gradient:  0.11547727016098543
iteration : 4479
train acc:  0.7421875
train loss:  0.5070488452911377
train gradient:  0.12043955379423267
iteration : 4480
train acc:  0.734375
train loss:  0.5081261992454529
train gradient:  0.131649872891241
iteration : 4481
train acc:  0.671875
train loss:  0.5591646432876587
train gradient:  0.15616353863008103
iteration : 4482
train acc:  0.6875
train loss:  0.5247345566749573
train gradient:  0.15810889151509472
iteration : 4483
train acc:  0.75
train loss:  0.46501389145851135
train gradient:  0.15541706170804007
iteration : 4484
train acc:  0.671875
train loss:  0.5623419284820557
train gradient:  0.21315055104957953
iteration : 4485
train acc:  0.7578125
train loss:  0.4441636800765991
train gradient:  0.09266777911224844
iteration : 4486
train acc:  0.734375
train loss:  0.5522706508636475
train gradient:  0.16416404623117395
iteration : 4487
train acc:  0.6953125
train loss:  0.5182998180389404
train gradient:  0.19116366743410818
iteration : 4488
train acc:  0.6953125
train loss:  0.5690760612487793
train gradient:  0.1593413274543419
iteration : 4489
train acc:  0.703125
train loss:  0.5979717969894409
train gradient:  0.1798729280192826
iteration : 4490
train acc:  0.7265625
train loss:  0.5074146389961243
train gradient:  0.1575080160249917
iteration : 4491
train acc:  0.734375
train loss:  0.5152895450592041
train gradient:  0.1267651026672752
iteration : 4492
train acc:  0.6875
train loss:  0.5522981286048889
train gradient:  0.12539889109839353
iteration : 4493
train acc:  0.71875
train loss:  0.5090501308441162
train gradient:  0.19731269662599737
iteration : 4494
train acc:  0.703125
train loss:  0.5427231192588806
train gradient:  0.13105656174911395
iteration : 4495
train acc:  0.6796875
train loss:  0.5195448994636536
train gradient:  0.11050571758878362
iteration : 4496
train acc:  0.7265625
train loss:  0.5416523218154907
train gradient:  0.16401303227902914
iteration : 4497
train acc:  0.7265625
train loss:  0.4875090718269348
train gradient:  0.12823830487430105
iteration : 4498
train acc:  0.671875
train loss:  0.5677744150161743
train gradient:  0.13942362746433512
iteration : 4499
train acc:  0.734375
train loss:  0.5003121495246887
train gradient:  0.1567460727069971
iteration : 4500
train acc:  0.6640625
train loss:  0.5524246096611023
train gradient:  0.1576505272536956
iteration : 4501
train acc:  0.75
train loss:  0.5808040499687195
train gradient:  0.15285926181910892
iteration : 4502
train acc:  0.78125
train loss:  0.4692704975605011
train gradient:  0.13892281617656144
iteration : 4503
train acc:  0.7265625
train loss:  0.4952704906463623
train gradient:  0.1850306518623279
iteration : 4504
train acc:  0.7265625
train loss:  0.5079549551010132
train gradient:  0.14983825145585922
iteration : 4505
train acc:  0.75
train loss:  0.5212240219116211
train gradient:  0.17255517548721386
iteration : 4506
train acc:  0.671875
train loss:  0.589121401309967
train gradient:  0.18250628845713013
iteration : 4507
train acc:  0.6875
train loss:  0.5360571146011353
train gradient:  0.16405073345903776
iteration : 4508
train acc:  0.65625
train loss:  0.5672022104263306
train gradient:  0.18075231208039794
iteration : 4509
train acc:  0.75
train loss:  0.47701191902160645
train gradient:  0.09037781040588456
iteration : 4510
train acc:  0.6875
train loss:  0.5692256093025208
train gradient:  0.16182449011272282
iteration : 4511
train acc:  0.765625
train loss:  0.4909151792526245
train gradient:  0.12494558572737112
iteration : 4512
train acc:  0.7265625
train loss:  0.5306621193885803
train gradient:  0.14544251928147525
iteration : 4513
train acc:  0.6953125
train loss:  0.5343170166015625
train gradient:  0.15403780521125734
iteration : 4514
train acc:  0.703125
train loss:  0.5231623649597168
train gradient:  0.14449895228448578
iteration : 4515
train acc:  0.6796875
train loss:  0.5769732594490051
train gradient:  0.2498228843411634
iteration : 4516
train acc:  0.7421875
train loss:  0.5381932258605957
train gradient:  0.15289573110978116
iteration : 4517
train acc:  0.7109375
train loss:  0.4890892505645752
train gradient:  0.11070265076451657
iteration : 4518
train acc:  0.7890625
train loss:  0.5435055494308472
train gradient:  0.17153790384953504
iteration : 4519
train acc:  0.6796875
train loss:  0.5773975849151611
train gradient:  0.17608540471583084
iteration : 4520
train acc:  0.6953125
train loss:  0.5445476770401001
train gradient:  0.1571513541652521
iteration : 4521
train acc:  0.7421875
train loss:  0.5557951331138611
train gradient:  0.16114986056334485
iteration : 4522
train acc:  0.75
train loss:  0.546497106552124
train gradient:  0.1822174385592869
iteration : 4523
train acc:  0.6953125
train loss:  0.5456128120422363
train gradient:  0.17651143937130642
iteration : 4524
train acc:  0.6640625
train loss:  0.6684333086013794
train gradient:  0.29946758491393316
iteration : 4525
train acc:  0.6953125
train loss:  0.5667792558670044
train gradient:  0.1771121600905486
iteration : 4526
train acc:  0.7421875
train loss:  0.4773789346218109
train gradient:  0.11404944768810234
iteration : 4527
train acc:  0.7734375
train loss:  0.451717734336853
train gradient:  0.11819139914003875
iteration : 4528
train acc:  0.6640625
train loss:  0.5886390209197998
train gradient:  0.16423422094520243
iteration : 4529
train acc:  0.703125
train loss:  0.5435833930969238
train gradient:  0.14306745470685317
iteration : 4530
train acc:  0.7109375
train loss:  0.5396873950958252
train gradient:  0.17607835424077567
iteration : 4531
train acc:  0.7109375
train loss:  0.5010489225387573
train gradient:  0.14435032230717615
iteration : 4532
train acc:  0.7265625
train loss:  0.5218241214752197
train gradient:  0.11246326878025957
iteration : 4533
train acc:  0.7734375
train loss:  0.4894639253616333
train gradient:  0.15722084822978294
iteration : 4534
train acc:  0.75
train loss:  0.47831764817237854
train gradient:  0.11764751373380634
iteration : 4535
train acc:  0.7421875
train loss:  0.5497077703475952
train gradient:  0.20776183316395447
iteration : 4536
train acc:  0.734375
train loss:  0.49671220779418945
train gradient:  0.11194394901817398
iteration : 4537
train acc:  0.734375
train loss:  0.5073423981666565
train gradient:  0.11422331731157677
iteration : 4538
train acc:  0.734375
train loss:  0.5262168645858765
train gradient:  0.14036791652207103
iteration : 4539
train acc:  0.703125
train loss:  0.5253716707229614
train gradient:  0.1513318449667388
iteration : 4540
train acc:  0.6875
train loss:  0.5194568037986755
train gradient:  0.14525861881878815
iteration : 4541
train acc:  0.7265625
train loss:  0.4714982509613037
train gradient:  0.10570457948005697
iteration : 4542
train acc:  0.7109375
train loss:  0.5536695718765259
train gradient:  0.15709944242292856
iteration : 4543
train acc:  0.71875
train loss:  0.5168988108634949
train gradient:  0.11431467831862963
iteration : 4544
train acc:  0.7421875
train loss:  0.49759408831596375
train gradient:  0.1301951231369374
iteration : 4545
train acc:  0.7265625
train loss:  0.5245406627655029
train gradient:  0.16731482128433872
iteration : 4546
train acc:  0.6953125
train loss:  0.5920405387878418
train gradient:  0.1937885066182355
iteration : 4547
train acc:  0.6953125
train loss:  0.5432521104812622
train gradient:  0.1671561976106068
iteration : 4548
train acc:  0.7265625
train loss:  0.5432173013687134
train gradient:  0.15776760585649915
iteration : 4549
train acc:  0.765625
train loss:  0.5082342028617859
train gradient:  0.12653891052410485
iteration : 4550
train acc:  0.734375
train loss:  0.48901909589767456
train gradient:  0.10792649075028964
iteration : 4551
train acc:  0.703125
train loss:  0.5266712307929993
train gradient:  0.1703105013478069
iteration : 4552
train acc:  0.7421875
train loss:  0.5163577198982239
train gradient:  0.13655065485635032
iteration : 4553
train acc:  0.7421875
train loss:  0.5176502466201782
train gradient:  0.13788521577925963
iteration : 4554
train acc:  0.71875
train loss:  0.5177494883537292
train gradient:  0.13036045244848674
iteration : 4555
train acc:  0.7265625
train loss:  0.47625190019607544
train gradient:  0.16530809079695627
iteration : 4556
train acc:  0.75
train loss:  0.5125512480735779
train gradient:  0.13453155405110695
iteration : 4557
train acc:  0.78125
train loss:  0.5041075944900513
train gradient:  0.14744321194187326
iteration : 4558
train acc:  0.65625
train loss:  0.6158968806266785
train gradient:  0.1873778583741138
iteration : 4559
train acc:  0.671875
train loss:  0.5299683809280396
train gradient:  0.12279213627468726
iteration : 4560
train acc:  0.7421875
train loss:  0.5405414700508118
train gradient:  0.17849979855468712
iteration : 4561
train acc:  0.7734375
train loss:  0.49035969376564026
train gradient:  0.13799166650852124
iteration : 4562
train acc:  0.71875
train loss:  0.5184187889099121
train gradient:  0.14566213460142202
iteration : 4563
train acc:  0.7265625
train loss:  0.5248206257820129
train gradient:  0.1485252172270814
iteration : 4564
train acc:  0.6953125
train loss:  0.5067944526672363
train gradient:  0.13336887543415488
iteration : 4565
train acc:  0.671875
train loss:  0.5411062836647034
train gradient:  0.11285700268132
iteration : 4566
train acc:  0.7421875
train loss:  0.5317604541778564
train gradient:  0.12654597589726838
iteration : 4567
train acc:  0.671875
train loss:  0.573877215385437
train gradient:  0.17854891405390455
iteration : 4568
train acc:  0.671875
train loss:  0.5693016052246094
train gradient:  0.17797876436633775
iteration : 4569
train acc:  0.796875
train loss:  0.436082661151886
train gradient:  0.10108509711513068
iteration : 4570
train acc:  0.7734375
train loss:  0.48307523131370544
train gradient:  0.1266040327712822
iteration : 4571
train acc:  0.734375
train loss:  0.502389669418335
train gradient:  0.18683928389004617
iteration : 4572
train acc:  0.6796875
train loss:  0.5814971923828125
train gradient:  0.15812998708192094
iteration : 4573
train acc:  0.703125
train loss:  0.5875017046928406
train gradient:  0.1425666355305409
iteration : 4574
train acc:  0.765625
train loss:  0.46703124046325684
train gradient:  0.14291816693919168
iteration : 4575
train acc:  0.6328125
train loss:  0.6164509057998657
train gradient:  0.17750574742258338
iteration : 4576
train acc:  0.71875
train loss:  0.4985218048095703
train gradient:  0.11818171844230398
iteration : 4577
train acc:  0.7578125
train loss:  0.47942906618118286
train gradient:  0.15959014833654672
iteration : 4578
train acc:  0.7578125
train loss:  0.4628077745437622
train gradient:  0.15725152208962317
iteration : 4579
train acc:  0.75
train loss:  0.4771712124347687
train gradient:  0.12154640517592799
iteration : 4580
train acc:  0.671875
train loss:  0.5408738255500793
train gradient:  0.23425988506564432
iteration : 4581
train acc:  0.7421875
train loss:  0.48784539103507996
train gradient:  0.10919105917000095
iteration : 4582
train acc:  0.65625
train loss:  0.5987540483474731
train gradient:  0.214325824012001
iteration : 4583
train acc:  0.7421875
train loss:  0.485732764005661
train gradient:  0.12536762275999574
iteration : 4584
train acc:  0.75
train loss:  0.49171677231788635
train gradient:  0.16486045338386185
iteration : 4585
train acc:  0.7109375
train loss:  0.5699042081832886
train gradient:  0.16724767412547248
iteration : 4586
train acc:  0.7421875
train loss:  0.4780946373939514
train gradient:  0.1300949151576341
iteration : 4587
train acc:  0.765625
train loss:  0.4566326141357422
train gradient:  0.13411822496797077
iteration : 4588
train acc:  0.7734375
train loss:  0.4978397488594055
train gradient:  0.15625334537425406
iteration : 4589
train acc:  0.78125
train loss:  0.4916457235813141
train gradient:  0.14426610290443875
iteration : 4590
train acc:  0.7265625
train loss:  0.5463402271270752
train gradient:  0.14892478521236724
iteration : 4591
train acc:  0.765625
train loss:  0.4699952006340027
train gradient:  0.166946113429674
iteration : 4592
train acc:  0.6796875
train loss:  0.5558651089668274
train gradient:  0.1705102947786175
iteration : 4593
train acc:  0.7734375
train loss:  0.49357903003692627
train gradient:  0.13531660066814538
iteration : 4594
train acc:  0.75
train loss:  0.4962155818939209
train gradient:  0.1326232892233552
iteration : 4595
train acc:  0.6796875
train loss:  0.5082138776779175
train gradient:  0.13001970852887018
iteration : 4596
train acc:  0.7265625
train loss:  0.5314635634422302
train gradient:  0.14138553399347506
iteration : 4597
train acc:  0.75
train loss:  0.5162352323532104
train gradient:  0.12314375701348802
iteration : 4598
train acc:  0.71875
train loss:  0.5159728527069092
train gradient:  0.1632060562314911
iteration : 4599
train acc:  0.7734375
train loss:  0.4751405715942383
train gradient:  0.13068217010181188
iteration : 4600
train acc:  0.7109375
train loss:  0.5204423069953918
train gradient:  0.1969435870181813
iteration : 4601
train acc:  0.7421875
train loss:  0.46770521998405457
train gradient:  0.13044401008978063
iteration : 4602
train acc:  0.7265625
train loss:  0.5724700689315796
train gradient:  0.20376367659348288
iteration : 4603
train acc:  0.7109375
train loss:  0.4950774312019348
train gradient:  0.1445337739911975
iteration : 4604
train acc:  0.75
train loss:  0.5222580432891846
train gradient:  0.13968322784091622
iteration : 4605
train acc:  0.6640625
train loss:  0.5664386749267578
train gradient:  0.1722516919660949
iteration : 4606
train acc:  0.7578125
train loss:  0.5244221091270447
train gradient:  0.13832808095054372
iteration : 4607
train acc:  0.6953125
train loss:  0.532569408416748
train gradient:  0.1590116259442228
iteration : 4608
train acc:  0.765625
train loss:  0.4924469590187073
train gradient:  0.12695513547838352
iteration : 4609
train acc:  0.765625
train loss:  0.48014041781425476
train gradient:  0.1359258000960989
iteration : 4610
train acc:  0.734375
train loss:  0.5099400877952576
train gradient:  0.17650845893150993
iteration : 4611
train acc:  0.734375
train loss:  0.4754454493522644
train gradient:  0.17975374186314588
iteration : 4612
train acc:  0.734375
train loss:  0.5551990866661072
train gradient:  0.18273187501252736
iteration : 4613
train acc:  0.71875
train loss:  0.524773120880127
train gradient:  0.14732631050320216
iteration : 4614
train acc:  0.7578125
train loss:  0.5253263711929321
train gradient:  0.15205855384292263
iteration : 4615
train acc:  0.6640625
train loss:  0.5682585835456848
train gradient:  0.1854629381270173
iteration : 4616
train acc:  0.7734375
train loss:  0.4539180397987366
train gradient:  0.13304909193099246
iteration : 4617
train acc:  0.6796875
train loss:  0.5620474219322205
train gradient:  0.1626332456087139
iteration : 4618
train acc:  0.703125
train loss:  0.5330151319503784
train gradient:  0.12847178516511767
iteration : 4619
train acc:  0.796875
train loss:  0.5011330842971802
train gradient:  0.15714091681383036
iteration : 4620
train acc:  0.75
train loss:  0.5209387540817261
train gradient:  0.16015230619286785
iteration : 4621
train acc:  0.7421875
train loss:  0.4798280894756317
train gradient:  0.10924558446369068
iteration : 4622
train acc:  0.6953125
train loss:  0.5798099637031555
train gradient:  0.2045372888716958
iteration : 4623
train acc:  0.75
train loss:  0.5022074580192566
train gradient:  0.10790822442629182
iteration : 4624
train acc:  0.75
train loss:  0.5218523740768433
train gradient:  0.14535689258345535
iteration : 4625
train acc:  0.7890625
train loss:  0.43353933095932007
train gradient:  0.104894846102709
iteration : 4626
train acc:  0.734375
train loss:  0.5442663431167603
train gradient:  0.16203296633913744
iteration : 4627
train acc:  0.71875
train loss:  0.5367186665534973
train gradient:  0.18799307623920541
iteration : 4628
train acc:  0.7421875
train loss:  0.5101543664932251
train gradient:  0.15546644167167747
iteration : 4629
train acc:  0.7578125
train loss:  0.48521149158477783
train gradient:  0.11198428522389674
iteration : 4630
train acc:  0.7109375
train loss:  0.5465482473373413
train gradient:  0.15233624319201877
iteration : 4631
train acc:  0.7578125
train loss:  0.45923006534576416
train gradient:  0.12281736802825045
iteration : 4632
train acc:  0.71875
train loss:  0.49120888113975525
train gradient:  0.12114660916348223
iteration : 4633
train acc:  0.6953125
train loss:  0.537987470626831
train gradient:  0.17253035630275623
iteration : 4634
train acc:  0.671875
train loss:  0.5859937071800232
train gradient:  0.17087274952398476
iteration : 4635
train acc:  0.734375
train loss:  0.5008798241615295
train gradient:  0.17342814398140105
iteration : 4636
train acc:  0.65625
train loss:  0.584936261177063
train gradient:  0.1946053336304312
iteration : 4637
train acc:  0.6171875
train loss:  0.5874823331832886
train gradient:  0.23278656395893418
iteration : 4638
train acc:  0.734375
train loss:  0.5018298029899597
train gradient:  0.14335260250323956
iteration : 4639
train acc:  0.7421875
train loss:  0.4870773255825043
train gradient:  0.17693997757290647
iteration : 4640
train acc:  0.7109375
train loss:  0.5625863075256348
train gradient:  0.1908806435708232
iteration : 4641
train acc:  0.78125
train loss:  0.4588000476360321
train gradient:  0.10869417570762334
iteration : 4642
train acc:  0.65625
train loss:  0.5598181486129761
train gradient:  0.1744554660634562
iteration : 4643
train acc:  0.7890625
train loss:  0.4634736478328705
train gradient:  0.1307966379450578
iteration : 4644
train acc:  0.6875
train loss:  0.5512058138847351
train gradient:  0.14539894050767177
iteration : 4645
train acc:  0.7578125
train loss:  0.5095993280410767
train gradient:  0.1468404982796489
iteration : 4646
train acc:  0.7578125
train loss:  0.4918052852153778
train gradient:  0.13552880610304174
iteration : 4647
train acc:  0.796875
train loss:  0.43272507190704346
train gradient:  0.12480174317468329
iteration : 4648
train acc:  0.765625
train loss:  0.4887842535972595
train gradient:  0.15281461916507355
iteration : 4649
train acc:  0.7734375
train loss:  0.5116399526596069
train gradient:  0.1680344599652154
iteration : 4650
train acc:  0.765625
train loss:  0.452025830745697
train gradient:  0.16689400402903976
iteration : 4651
train acc:  0.703125
train loss:  0.5382237434387207
train gradient:  0.18205751169086642
iteration : 4652
train acc:  0.703125
train loss:  0.5474039316177368
train gradient:  0.15153170370163088
iteration : 4653
train acc:  0.7421875
train loss:  0.4910283088684082
train gradient:  0.15457195243431077
iteration : 4654
train acc:  0.6875
train loss:  0.5512180328369141
train gradient:  0.15645335298553154
iteration : 4655
train acc:  0.703125
train loss:  0.561753511428833
train gradient:  0.17555985243410832
iteration : 4656
train acc:  0.734375
train loss:  0.5189758539199829
train gradient:  0.15317860210118692
iteration : 4657
train acc:  0.7890625
train loss:  0.4685400128364563
train gradient:  0.10623117572265527
iteration : 4658
train acc:  0.6796875
train loss:  0.580685555934906
train gradient:  0.26621820201122154
iteration : 4659
train acc:  0.7265625
train loss:  0.5119884014129639
train gradient:  0.13727849130263348
iteration : 4660
train acc:  0.734375
train loss:  0.561481237411499
train gradient:  0.1406472374865305
iteration : 4661
train acc:  0.765625
train loss:  0.47924095392227173
train gradient:  0.1693833858812717
iteration : 4662
train acc:  0.765625
train loss:  0.5186264514923096
train gradient:  0.17787339947946224
iteration : 4663
train acc:  0.765625
train loss:  0.4705769419670105
train gradient:  0.13369188026586648
iteration : 4664
train acc:  0.6875
train loss:  0.5170480012893677
train gradient:  0.14201018029736595
iteration : 4665
train acc:  0.75
train loss:  0.4891206920146942
train gradient:  0.14858773058354077
iteration : 4666
train acc:  0.7890625
train loss:  0.4417373538017273
train gradient:  0.1094782602843078
iteration : 4667
train acc:  0.734375
train loss:  0.4834759533405304
train gradient:  0.13216165575138356
iteration : 4668
train acc:  0.765625
train loss:  0.4933295249938965
train gradient:  0.1579028911072788
iteration : 4669
train acc:  0.7109375
train loss:  0.5345208048820496
train gradient:  0.19317664153480607
iteration : 4670
train acc:  0.71875
train loss:  0.5310853719711304
train gradient:  0.15604654761596462
iteration : 4671
train acc:  0.7421875
train loss:  0.5469250679016113
train gradient:  0.15517423308371858
iteration : 4672
train acc:  0.75
train loss:  0.539671778678894
train gradient:  0.15624097918339458
iteration : 4673
train acc:  0.734375
train loss:  0.4996296763420105
train gradient:  0.12497770547284343
iteration : 4674
train acc:  0.7578125
train loss:  0.5201593041419983
train gradient:  0.15911697313428336
iteration : 4675
train acc:  0.7421875
train loss:  0.5250882506370544
train gradient:  0.18276707512819645
iteration : 4676
train acc:  0.703125
train loss:  0.5748777389526367
train gradient:  0.24917114001042306
iteration : 4677
train acc:  0.7734375
train loss:  0.48558616638183594
train gradient:  0.1287212669700834
iteration : 4678
train acc:  0.7265625
train loss:  0.5184262990951538
train gradient:  0.13966579360218173
iteration : 4679
train acc:  0.7265625
train loss:  0.4721997082233429
train gradient:  0.17317626417752946
iteration : 4680
train acc:  0.7109375
train loss:  0.5792415142059326
train gradient:  0.2322197756860424
iteration : 4681
train acc:  0.703125
train loss:  0.5739539861679077
train gradient:  0.2010239120895536
iteration : 4682
train acc:  0.796875
train loss:  0.4495120048522949
train gradient:  0.14004017062138213
iteration : 4683
train acc:  0.6640625
train loss:  0.5152225494384766
train gradient:  0.11958109754857942
iteration : 4684
train acc:  0.75
train loss:  0.47726374864578247
train gradient:  0.12337062856879104
iteration : 4685
train acc:  0.7109375
train loss:  0.49187833070755005
train gradient:  0.1716418201970768
iteration : 4686
train acc:  0.7265625
train loss:  0.48028460144996643
train gradient:  0.11868205220092143
iteration : 4687
train acc:  0.765625
train loss:  0.5028374791145325
train gradient:  0.20263479772575937
iteration : 4688
train acc:  0.7265625
train loss:  0.519544243812561
train gradient:  0.13128122121433683
iteration : 4689
train acc:  0.7734375
train loss:  0.48928868770599365
train gradient:  0.1686810551433086
iteration : 4690
train acc:  0.7421875
train loss:  0.46910566091537476
train gradient:  0.12340828370078219
iteration : 4691
train acc:  0.6640625
train loss:  0.5359245538711548
train gradient:  0.16230144404484148
iteration : 4692
train acc:  0.7265625
train loss:  0.5179980993270874
train gradient:  0.155134523174066
iteration : 4693
train acc:  0.7265625
train loss:  0.5237083435058594
train gradient:  0.1487801809701377
iteration : 4694
train acc:  0.734375
train loss:  0.5059776306152344
train gradient:  0.16644525711860342
iteration : 4695
train acc:  0.8125
train loss:  0.45631009340286255
train gradient:  0.13408816819219166
iteration : 4696
train acc:  0.6875
train loss:  0.5480763912200928
train gradient:  0.17679665390490512
iteration : 4697
train acc:  0.6875
train loss:  0.5356603860855103
train gradient:  0.155002470762437
iteration : 4698
train acc:  0.7109375
train loss:  0.4685014486312866
train gradient:  0.14566550211533932
iteration : 4699
train acc:  0.7265625
train loss:  0.5014938116073608
train gradient:  0.12765384184803186
iteration : 4700
train acc:  0.703125
train loss:  0.5602186918258667
train gradient:  0.18328250776085014
iteration : 4701
train acc:  0.6953125
train loss:  0.5408551692962646
train gradient:  0.15687289210897934
iteration : 4702
train acc:  0.765625
train loss:  0.4954456388950348
train gradient:  0.12248367680490352
iteration : 4703
train acc:  0.7890625
train loss:  0.46208155155181885
train gradient:  0.11292933877091635
iteration : 4704
train acc:  0.6875
train loss:  0.5421056747436523
train gradient:  0.15637672106527006
iteration : 4705
train acc:  0.65625
train loss:  0.6111488342285156
train gradient:  0.1948761507390065
iteration : 4706
train acc:  0.8203125
train loss:  0.4232701361179352
train gradient:  0.12532096348724558
iteration : 4707
train acc:  0.7265625
train loss:  0.4951518774032593
train gradient:  0.16545103803654385
iteration : 4708
train acc:  0.7265625
train loss:  0.5176554322242737
train gradient:  0.14537876367679664
iteration : 4709
train acc:  0.71875
train loss:  0.48859432339668274
train gradient:  0.15189278593411593
iteration : 4710
train acc:  0.75
train loss:  0.4796832799911499
train gradient:  0.16074492477080615
iteration : 4711
train acc:  0.71875
train loss:  0.5532714128494263
train gradient:  0.1797330990779366
iteration : 4712
train acc:  0.75
train loss:  0.46216660737991333
train gradient:  0.160157565735743
iteration : 4713
train acc:  0.65625
train loss:  0.5603487491607666
train gradient:  0.211973269201655
iteration : 4714
train acc:  0.7734375
train loss:  0.49512189626693726
train gradient:  0.12594082830851216
iteration : 4715
train acc:  0.8046875
train loss:  0.4516691565513611
train gradient:  0.13482004726104774
iteration : 4716
train acc:  0.7109375
train loss:  0.5379796028137207
train gradient:  0.13011481557914756
iteration : 4717
train acc:  0.7578125
train loss:  0.5192404985427856
train gradient:  0.13380978338974148
iteration : 4718
train acc:  0.6796875
train loss:  0.5534513592720032
train gradient:  0.14109726934529765
iteration : 4719
train acc:  0.6953125
train loss:  0.571296215057373
train gradient:  0.24261757905435427
iteration : 4720
train acc:  0.65625
train loss:  0.5890045762062073
train gradient:  0.22912285276070213
iteration : 4721
train acc:  0.734375
train loss:  0.5061075687408447
train gradient:  0.15654972677767498
iteration : 4722
train acc:  0.7734375
train loss:  0.47026097774505615
train gradient:  0.13856050760094446
iteration : 4723
train acc:  0.734375
train loss:  0.4992009997367859
train gradient:  0.13671868369480766
iteration : 4724
train acc:  0.7109375
train loss:  0.5010227560997009
train gradient:  0.17931893095920878
iteration : 4725
train acc:  0.6953125
train loss:  0.5737642049789429
train gradient:  0.17956279328492625
iteration : 4726
train acc:  0.7265625
train loss:  0.4460083246231079
train gradient:  0.11892223406353467
iteration : 4727
train acc:  0.7421875
train loss:  0.5010963678359985
train gradient:  0.17294302287971985
iteration : 4728
train acc:  0.7265625
train loss:  0.5577137470245361
train gradient:  0.14466377436964917
iteration : 4729
train acc:  0.71875
train loss:  0.49272215366363525
train gradient:  0.1319188969935262
iteration : 4730
train acc:  0.7265625
train loss:  0.49311792850494385
train gradient:  0.12233936511100578
iteration : 4731
train acc:  0.78125
train loss:  0.45378541946411133
train gradient:  0.1295644940759536
iteration : 4732
train acc:  0.734375
train loss:  0.48429593443870544
train gradient:  0.12441481424386636
iteration : 4733
train acc:  0.7421875
train loss:  0.5380553603172302
train gradient:  0.23798577822154615
iteration : 4734
train acc:  0.71875
train loss:  0.5103465914726257
train gradient:  0.18855804465978437
iteration : 4735
train acc:  0.734375
train loss:  0.4802464544773102
train gradient:  0.13459759584701247
iteration : 4736
train acc:  0.71875
train loss:  0.5444509983062744
train gradient:  0.16437563908565317
iteration : 4737
train acc:  0.75
train loss:  0.5519282817840576
train gradient:  0.2503250398566046
iteration : 4738
train acc:  0.75
train loss:  0.5184789896011353
train gradient:  0.1652496178166245
iteration : 4739
train acc:  0.7421875
train loss:  0.5360046625137329
train gradient:  0.13440833859874063
iteration : 4740
train acc:  0.734375
train loss:  0.4795605540275574
train gradient:  0.1222919377178375
iteration : 4741
train acc:  0.6953125
train loss:  0.49857616424560547
train gradient:  0.15517431159197775
iteration : 4742
train acc:  0.7109375
train loss:  0.5126408338546753
train gradient:  0.14802583979608347
iteration : 4743
train acc:  0.7578125
train loss:  0.4845913052558899
train gradient:  0.15345711554526129
iteration : 4744
train acc:  0.703125
train loss:  0.5753723382949829
train gradient:  0.23483251273024758
iteration : 4745
train acc:  0.671875
train loss:  0.5769655704498291
train gradient:  0.1906152238875667
iteration : 4746
train acc:  0.7578125
train loss:  0.4480312168598175
train gradient:  0.12520662572823438
iteration : 4747
train acc:  0.765625
train loss:  0.4656194746494293
train gradient:  0.13563352713506696
iteration : 4748
train acc:  0.6484375
train loss:  0.6754426956176758
train gradient:  0.26519735969386443
iteration : 4749
train acc:  0.65625
train loss:  0.6229382753372192
train gradient:  0.185966799420465
iteration : 4750
train acc:  0.6796875
train loss:  0.5763904452323914
train gradient:  0.16815032981920808
iteration : 4751
train acc:  0.75
train loss:  0.5483806133270264
train gradient:  0.15023562234588567
iteration : 4752
train acc:  0.75
train loss:  0.5295038223266602
train gradient:  0.14039482074722476
iteration : 4753
train acc:  0.7734375
train loss:  0.5042575597763062
train gradient:  0.190245817116043
iteration : 4754
train acc:  0.7265625
train loss:  0.5015116930007935
train gradient:  0.10733838091457214
iteration : 4755
train acc:  0.71875
train loss:  0.543550431728363
train gradient:  0.1411869009183857
iteration : 4756
train acc:  0.640625
train loss:  0.5745558738708496
train gradient:  0.20832522470653766
iteration : 4757
train acc:  0.7109375
train loss:  0.5552403926849365
train gradient:  0.18002651175839784
iteration : 4758
train acc:  0.6640625
train loss:  0.5950146913528442
train gradient:  0.1687084549826499
iteration : 4759
train acc:  0.7578125
train loss:  0.5233494639396667
train gradient:  0.15475080955118464
iteration : 4760
train acc:  0.7421875
train loss:  0.5081912875175476
train gradient:  0.1423730282419416
iteration : 4761
train acc:  0.7109375
train loss:  0.5218896865844727
train gradient:  0.15999329606797344
iteration : 4762
train acc:  0.6875
train loss:  0.5082626342773438
train gradient:  0.12276236350635587
iteration : 4763
train acc:  0.7578125
train loss:  0.498515248298645
train gradient:  0.1572172314716292
iteration : 4764
train acc:  0.71875
train loss:  0.5341807007789612
train gradient:  0.1458208969237905
iteration : 4765
train acc:  0.6953125
train loss:  0.539959728717804
train gradient:  0.19951470105831925
iteration : 4766
train acc:  0.6796875
train loss:  0.580849826335907
train gradient:  0.1744220063580707
iteration : 4767
train acc:  0.7109375
train loss:  0.5245190858840942
train gradient:  0.12705002990017655
iteration : 4768
train acc:  0.71875
train loss:  0.5321789383888245
train gradient:  0.128628046280514
iteration : 4769
train acc:  0.7109375
train loss:  0.5019263029098511
train gradient:  0.14218092398949045
iteration : 4770
train acc:  0.7890625
train loss:  0.4618169069290161
train gradient:  0.1286259202140172
iteration : 4771
train acc:  0.7109375
train loss:  0.5260878801345825
train gradient:  0.1572696908899854
iteration : 4772
train acc:  0.8125
train loss:  0.3936973512172699
train gradient:  0.09595109393206967
iteration : 4773
train acc:  0.71875
train loss:  0.5476043224334717
train gradient:  0.17803664227770888
iteration : 4774
train acc:  0.703125
train loss:  0.5579514503479004
train gradient:  0.15572390553693324
iteration : 4775
train acc:  0.7109375
train loss:  0.5506763458251953
train gradient:  0.17143303923068987
iteration : 4776
train acc:  0.703125
train loss:  0.5570604801177979
train gradient:  0.1414261977362789
iteration : 4777
train acc:  0.734375
train loss:  0.46641239523887634
train gradient:  0.13308574598935552
iteration : 4778
train acc:  0.734375
train loss:  0.5206393003463745
train gradient:  0.12576896233935897
iteration : 4779
train acc:  0.7265625
train loss:  0.49627685546875
train gradient:  0.14619790395966678
iteration : 4780
train acc:  0.7578125
train loss:  0.5151041746139526
train gradient:  0.15694274642099604
iteration : 4781
train acc:  0.734375
train loss:  0.5542466044425964
train gradient:  0.16957442582889806
iteration : 4782
train acc:  0.703125
train loss:  0.5252766013145447
train gradient:  0.1298780283058725
iteration : 4783
train acc:  0.703125
train loss:  0.5201104879379272
train gradient:  0.14333701367862162
iteration : 4784
train acc:  0.6875
train loss:  0.5314422845840454
train gradient:  0.18507437281844488
iteration : 4785
train acc:  0.78125
train loss:  0.48220816254615784
train gradient:  0.15448616815239477
iteration : 4786
train acc:  0.6875
train loss:  0.5404738783836365
train gradient:  0.12274907543977168
iteration : 4787
train acc:  0.7109375
train loss:  0.5161270499229431
train gradient:  0.15441698403675053
iteration : 4788
train acc:  0.671875
train loss:  0.6188580989837646
train gradient:  0.2179214058621641
iteration : 4789
train acc:  0.734375
train loss:  0.5537174940109253
train gradient:  0.14186641925597196
iteration : 4790
train acc:  0.7421875
train loss:  0.5240079164505005
train gradient:  0.1684744103562143
iteration : 4791
train acc:  0.703125
train loss:  0.5389643311500549
train gradient:  0.17649810184804587
iteration : 4792
train acc:  0.7578125
train loss:  0.5269896388053894
train gradient:  0.13341104521186609
iteration : 4793
train acc:  0.671875
train loss:  0.5569806098937988
train gradient:  0.22046618583230362
iteration : 4794
train acc:  0.7109375
train loss:  0.5225327610969543
train gradient:  0.1645349123434673
iteration : 4795
train acc:  0.6875
train loss:  0.54576575756073
train gradient:  0.12774310159946412
iteration : 4796
train acc:  0.7421875
train loss:  0.5123828649520874
train gradient:  0.1290602943445262
iteration : 4797
train acc:  0.7265625
train loss:  0.5534108877182007
train gradient:  0.1628861014353477
iteration : 4798
train acc:  0.6953125
train loss:  0.545682430267334
train gradient:  0.1622988962463681
iteration : 4799
train acc:  0.734375
train loss:  0.4970046281814575
train gradient:  0.15681631187837783
iteration : 4800
train acc:  0.7890625
train loss:  0.5051349997520447
train gradient:  0.13164145054042675
iteration : 4801
train acc:  0.6640625
train loss:  0.6233277320861816
train gradient:  0.20413073365027906
iteration : 4802
train acc:  0.6640625
train loss:  0.6170437335968018
train gradient:  0.21256267357320469
iteration : 4803
train acc:  0.7421875
train loss:  0.47246503829956055
train gradient:  0.1638305441965623
iteration : 4804
train acc:  0.71875
train loss:  0.4821195900440216
train gradient:  0.11371831783071111
iteration : 4805
train acc:  0.75
train loss:  0.47066640853881836
train gradient:  0.11390127928993028
iteration : 4806
train acc:  0.7265625
train loss:  0.4906679391860962
train gradient:  0.1354689840984291
iteration : 4807
train acc:  0.734375
train loss:  0.48909497261047363
train gradient:  0.13707287864600087
iteration : 4808
train acc:  0.703125
train loss:  0.5174416303634644
train gradient:  0.13024030334344144
iteration : 4809
train acc:  0.734375
train loss:  0.5428544282913208
train gradient:  0.18256619257512607
iteration : 4810
train acc:  0.7109375
train loss:  0.5284087061882019
train gradient:  0.12379107002550481
iteration : 4811
train acc:  0.7265625
train loss:  0.5131906270980835
train gradient:  0.1730983197927987
iteration : 4812
train acc:  0.7578125
train loss:  0.4800404906272888
train gradient:  0.15799386531300658
iteration : 4813
train acc:  0.765625
train loss:  0.4519919753074646
train gradient:  0.10815090642500685
iteration : 4814
train acc:  0.640625
train loss:  0.5520107746124268
train gradient:  0.17406514834871423
iteration : 4815
train acc:  0.734375
train loss:  0.5016535520553589
train gradient:  0.15218434936457387
iteration : 4816
train acc:  0.7109375
train loss:  0.5658178329467773
train gradient:  0.16402508230453783
iteration : 4817
train acc:  0.7578125
train loss:  0.49197664856910706
train gradient:  0.11332656876755506
iteration : 4818
train acc:  0.7265625
train loss:  0.5442987680435181
train gradient:  0.19769661735414784
iteration : 4819
train acc:  0.75
train loss:  0.5154315233230591
train gradient:  0.1518368181300449
iteration : 4820
train acc:  0.65625
train loss:  0.6069095134735107
train gradient:  0.16260929788024478
iteration : 4821
train acc:  0.7734375
train loss:  0.4677489697933197
train gradient:  0.1370902596452624
iteration : 4822
train acc:  0.796875
train loss:  0.48624706268310547
train gradient:  0.15343403390273663
iteration : 4823
train acc:  0.7578125
train loss:  0.48694536089897156
train gradient:  0.13868723985796982
iteration : 4824
train acc:  0.765625
train loss:  0.49560683965682983
train gradient:  0.18389438349009452
iteration : 4825
train acc:  0.7734375
train loss:  0.45956695079803467
train gradient:  0.1025872686196176
iteration : 4826
train acc:  0.78125
train loss:  0.4713708460330963
train gradient:  0.12784476822194896
iteration : 4827
train acc:  0.7578125
train loss:  0.48426663875579834
train gradient:  0.1072359453778536
iteration : 4828
train acc:  0.7109375
train loss:  0.597813069820404
train gradient:  0.22261249614968626
iteration : 4829
train acc:  0.703125
train loss:  0.5486882925033569
train gradient:  0.17463663740509489
iteration : 4830
train acc:  0.7265625
train loss:  0.4873098134994507
train gradient:  0.1937631604361203
iteration : 4831
train acc:  0.703125
train loss:  0.5309774875640869
train gradient:  0.16079458976119387
iteration : 4832
train acc:  0.6875
train loss:  0.525503396987915
train gradient:  0.14995305743905485
iteration : 4833
train acc:  0.65625
train loss:  0.5530534982681274
train gradient:  0.16345037170190962
iteration : 4834
train acc:  0.6796875
train loss:  0.5525275468826294
train gradient:  0.1768792344421089
iteration : 4835
train acc:  0.71875
train loss:  0.5700787305831909
train gradient:  0.16144042139253484
iteration : 4836
train acc:  0.7109375
train loss:  0.5076555013656616
train gradient:  0.14142530345908533
iteration : 4837
train acc:  0.703125
train loss:  0.5730984210968018
train gradient:  0.16033971755965326
iteration : 4838
train acc:  0.7265625
train loss:  0.5754293203353882
train gradient:  0.13657555861604206
iteration : 4839
train acc:  0.7578125
train loss:  0.47712433338165283
train gradient:  0.11627044754366453
iteration : 4840
train acc:  0.796875
train loss:  0.4641880393028259
train gradient:  0.1199749906109995
iteration : 4841
train acc:  0.78125
train loss:  0.5076574087142944
train gradient:  0.1343021178227925
iteration : 4842
train acc:  0.640625
train loss:  0.5718854665756226
train gradient:  0.1733782993427463
iteration : 4843
train acc:  0.734375
train loss:  0.5278331637382507
train gradient:  0.14623331419998278
iteration : 4844
train acc:  0.7265625
train loss:  0.478841632604599
train gradient:  0.1335652673851534
iteration : 4845
train acc:  0.7109375
train loss:  0.5406717658042908
train gradient:  0.16434600715838743
iteration : 4846
train acc:  0.7421875
train loss:  0.46922627091407776
train gradient:  0.11619713905502929
iteration : 4847
train acc:  0.734375
train loss:  0.4792369306087494
train gradient:  0.1414375295061282
iteration : 4848
train acc:  0.703125
train loss:  0.5402172803878784
train gradient:  0.18541532642233682
iteration : 4849
train acc:  0.7734375
train loss:  0.456855833530426
train gradient:  0.11933312619395404
iteration : 4850
train acc:  0.625
train loss:  0.6289095878601074
train gradient:  0.25848875966745877
iteration : 4851
train acc:  0.75
train loss:  0.4915516674518585
train gradient:  0.1577826919793421
iteration : 4852
train acc:  0.7421875
train loss:  0.5228764414787292
train gradient:  0.12631852268410093
iteration : 4853
train acc:  0.7734375
train loss:  0.440368115901947
train gradient:  0.12121795570045733
iteration : 4854
train acc:  0.78125
train loss:  0.4601040780544281
train gradient:  0.1350398399849374
iteration : 4855
train acc:  0.7421875
train loss:  0.48368144035339355
train gradient:  0.1228692070942079
iteration : 4856
train acc:  0.75
train loss:  0.4757194519042969
train gradient:  0.11252900172588194
iteration : 4857
train acc:  0.734375
train loss:  0.5118335485458374
train gradient:  0.11529753365614767
iteration : 4858
train acc:  0.671875
train loss:  0.5913485288619995
train gradient:  0.17682112703523656
iteration : 4859
train acc:  0.7109375
train loss:  0.5544687509536743
train gradient:  0.20683221083410674
iteration : 4860
train acc:  0.7578125
train loss:  0.5147554278373718
train gradient:  0.16631344806684545
iteration : 4861
train acc:  0.6953125
train loss:  0.5223167538642883
train gradient:  0.15228802400565722
iteration : 4862
train acc:  0.7890625
train loss:  0.5058109760284424
train gradient:  0.13078534414986448
iteration : 4863
train acc:  0.75
train loss:  0.5073686838150024
train gradient:  0.11774616672793999
iteration : 4864
train acc:  0.6796875
train loss:  0.5393307209014893
train gradient:  0.17614129924887523
iteration : 4865
train acc:  0.703125
train loss:  0.5155611634254456
train gradient:  0.14620629683491415
iteration : 4866
train acc:  0.7109375
train loss:  0.518326997756958
train gradient:  0.1932695634808287
iteration : 4867
train acc:  0.734375
train loss:  0.48149919509887695
train gradient:  0.13173027509841712
iteration : 4868
train acc:  0.671875
train loss:  0.5370584726333618
train gradient:  0.17667250274960836
iteration : 4869
train acc:  0.6953125
train loss:  0.5242215991020203
train gradient:  0.15872226978430462
iteration : 4870
train acc:  0.6875
train loss:  0.5147568583488464
train gradient:  0.11974800538526105
iteration : 4871
train acc:  0.7265625
train loss:  0.5184557437896729
train gradient:  0.15939190091654037
iteration : 4872
train acc:  0.703125
train loss:  0.5497759580612183
train gradient:  0.13550914119905108
iteration : 4873
train acc:  0.734375
train loss:  0.5102066397666931
train gradient:  0.12092435234978215
iteration : 4874
train acc:  0.828125
train loss:  0.45119932293891907
train gradient:  0.10338942371066213
iteration : 4875
train acc:  0.75
train loss:  0.521144688129425
train gradient:  0.12757509316774696
iteration : 4876
train acc:  0.75
train loss:  0.47039204835891724
train gradient:  0.11383086990811926
iteration : 4877
train acc:  0.6796875
train loss:  0.5665968656539917
train gradient:  0.14886234616037242
iteration : 4878
train acc:  0.75
train loss:  0.5107368230819702
train gradient:  0.12417549778526236
iteration : 4879
train acc:  0.640625
train loss:  0.5628179907798767
train gradient:  0.16695031558066178
iteration : 4880
train acc:  0.71875
train loss:  0.5242642760276794
train gradient:  0.14090594926876807
iteration : 4881
train acc:  0.75
train loss:  0.47121113538742065
train gradient:  0.18377401687879258
iteration : 4882
train acc:  0.7109375
train loss:  0.5750222206115723
train gradient:  0.15981820069474123
iteration : 4883
train acc:  0.7265625
train loss:  0.5092108249664307
train gradient:  0.1305802178544417
iteration : 4884
train acc:  0.765625
train loss:  0.4937063753604889
train gradient:  0.17448165622035505
iteration : 4885
train acc:  0.7265625
train loss:  0.5151658654212952
train gradient:  0.1179311274178945
iteration : 4886
train acc:  0.6875
train loss:  0.58856201171875
train gradient:  0.18374812720885894
iteration : 4887
train acc:  0.6875
train loss:  0.5612150430679321
train gradient:  0.15690818392850583
iteration : 4888
train acc:  0.7265625
train loss:  0.5256462097167969
train gradient:  0.13419234489529147
iteration : 4889
train acc:  0.6953125
train loss:  0.5027120113372803
train gradient:  0.14276884604938433
iteration : 4890
train acc:  0.75
train loss:  0.48565226793289185
train gradient:  0.20031113716797094
iteration : 4891
train acc:  0.6875
train loss:  0.5564907193183899
train gradient:  0.16450677480403508
iteration : 4892
train acc:  0.703125
train loss:  0.5143917798995972
train gradient:  0.117445071345756
iteration : 4893
train acc:  0.6015625
train loss:  0.6097849607467651
train gradient:  0.20045343338205932
iteration : 4894
train acc:  0.7109375
train loss:  0.5292172431945801
train gradient:  0.17515666554094286
iteration : 4895
train acc:  0.7109375
train loss:  0.5359979867935181
train gradient:  0.14662489250788646
iteration : 4896
train acc:  0.7265625
train loss:  0.4885309338569641
train gradient:  0.13873434008792968
iteration : 4897
train acc:  0.71875
train loss:  0.5233033895492554
train gradient:  0.18214010459285063
iteration : 4898
train acc:  0.796875
train loss:  0.4634651243686676
train gradient:  0.13926881948257874
iteration : 4899
train acc:  0.734375
train loss:  0.48709461092948914
train gradient:  0.13511061556157314
iteration : 4900
train acc:  0.7265625
train loss:  0.5483615398406982
train gradient:  0.27112910128487816
iteration : 4901
train acc:  0.7265625
train loss:  0.5492212772369385
train gradient:  0.15562965971135256
iteration : 4902
train acc:  0.7265625
train loss:  0.5165847539901733
train gradient:  0.17531416465343252
iteration : 4903
train acc:  0.6953125
train loss:  0.5656432509422302
train gradient:  0.18375984578865062
iteration : 4904
train acc:  0.671875
train loss:  0.562562882900238
train gradient:  0.14904306987626798
iteration : 4905
train acc:  0.734375
train loss:  0.5333117842674255
train gradient:  0.16631885120679507
iteration : 4906
train acc:  0.78125
train loss:  0.4164278507232666
train gradient:  0.100318916533055
iteration : 4907
train acc:  0.7109375
train loss:  0.5093035697937012
train gradient:  0.16964726915152897
iteration : 4908
train acc:  0.71875
train loss:  0.5506219267845154
train gradient:  0.1564841897669378
iteration : 4909
train acc:  0.640625
train loss:  0.5941256880760193
train gradient:  0.15701998058521435
iteration : 4910
train acc:  0.6484375
train loss:  0.563530445098877
train gradient:  0.18222458503984684
iteration : 4911
train acc:  0.7265625
train loss:  0.5341717600822449
train gradient:  0.16812057890993112
iteration : 4912
train acc:  0.7734375
train loss:  0.4885396361351013
train gradient:  0.14296605868780532
iteration : 4913
train acc:  0.7265625
train loss:  0.493674635887146
train gradient:  0.19848866728910364
iteration : 4914
train acc:  0.71875
train loss:  0.5254675149917603
train gradient:  0.13798607912830782
iteration : 4915
train acc:  0.6796875
train loss:  0.5832937955856323
train gradient:  0.2215952806206181
iteration : 4916
train acc:  0.7890625
train loss:  0.48304685950279236
train gradient:  0.10660877958315493
iteration : 4917
train acc:  0.78125
train loss:  0.4647987484931946
train gradient:  0.11126119384238953
iteration : 4918
train acc:  0.75
train loss:  0.5405775308609009
train gradient:  0.18972158738130618
iteration : 4919
train acc:  0.8046875
train loss:  0.46669507026672363
train gradient:  0.12295181339669148
iteration : 4920
train acc:  0.75
train loss:  0.4752022624015808
train gradient:  0.10564753407377445
iteration : 4921
train acc:  0.6953125
train loss:  0.5383471250534058
train gradient:  0.14113221253764424
iteration : 4922
train acc:  0.734375
train loss:  0.5721941590309143
train gradient:  0.20123437241875008
iteration : 4923
train acc:  0.6953125
train loss:  0.5523514747619629
train gradient:  0.14846133941366974
iteration : 4924
train acc:  0.6484375
train loss:  0.5731275081634521
train gradient:  0.20038391806094624
iteration : 4925
train acc:  0.7578125
train loss:  0.5334107875823975
train gradient:  0.17077717420039537
iteration : 4926
train acc:  0.7421875
train loss:  0.5230668783187866
train gradient:  0.1518515257116381
iteration : 4927
train acc:  0.7109375
train loss:  0.5328377485275269
train gradient:  0.16726910498865238
iteration : 4928
train acc:  0.7421875
train loss:  0.4829616844654083
train gradient:  0.12543490290183412
iteration : 4929
train acc:  0.703125
train loss:  0.5719139575958252
train gradient:  0.17147917523745515
iteration : 4930
train acc:  0.703125
train loss:  0.5026362538337708
train gradient:  0.13576872200366383
iteration : 4931
train acc:  0.75
train loss:  0.4694203734397888
train gradient:  0.13139595403286516
iteration : 4932
train acc:  0.71875
train loss:  0.5190010070800781
train gradient:  0.13592744057122128
iteration : 4933
train acc:  0.7109375
train loss:  0.5459855794906616
train gradient:  0.14926783202192911
iteration : 4934
train acc:  0.765625
train loss:  0.504055917263031
train gradient:  0.14952775640705207
iteration : 4935
train acc:  0.71875
train loss:  0.4766732454299927
train gradient:  0.132111623834927
iteration : 4936
train acc:  0.6875
train loss:  0.5909531712532043
train gradient:  0.17935584392813264
iteration : 4937
train acc:  0.6953125
train loss:  0.5981227159500122
train gradient:  0.14868398479697476
iteration : 4938
train acc:  0.7578125
train loss:  0.48694708943367004
train gradient:  0.12188827412973974
iteration : 4939
train acc:  0.75
train loss:  0.45645424723625183
train gradient:  0.16851420800526445
iteration : 4940
train acc:  0.796875
train loss:  0.4826825261116028
train gradient:  0.1316640127116156
iteration : 4941
train acc:  0.75
train loss:  0.5113263130187988
train gradient:  0.15050313074045965
iteration : 4942
train acc:  0.671875
train loss:  0.5884703993797302
train gradient:  0.17761595787954648
iteration : 4943
train acc:  0.734375
train loss:  0.4971444606781006
train gradient:  0.13649310024042074
iteration : 4944
train acc:  0.75
train loss:  0.4957332909107208
train gradient:  0.13663637794273115
iteration : 4945
train acc:  0.7265625
train loss:  0.514049768447876
train gradient:  0.1380887463208843
iteration : 4946
train acc:  0.734375
train loss:  0.5146833658218384
train gradient:  0.1696342950147668
iteration : 4947
train acc:  0.75
train loss:  0.5172438621520996
train gradient:  0.13517163356393314
iteration : 4948
train acc:  0.734375
train loss:  0.47301802039146423
train gradient:  0.16423237957057296
iteration : 4949
train acc:  0.6796875
train loss:  0.5543468594551086
train gradient:  0.1508660390029199
iteration : 4950
train acc:  0.7265625
train loss:  0.5510112643241882
train gradient:  0.18566264272699468
iteration : 4951
train acc:  0.71875
train loss:  0.5748701095581055
train gradient:  0.16611641631260682
iteration : 4952
train acc:  0.7890625
train loss:  0.447540283203125
train gradient:  0.11423853946611999
iteration : 4953
train acc:  0.765625
train loss:  0.5034966468811035
train gradient:  0.14060762856336864
iteration : 4954
train acc:  0.7109375
train loss:  0.5340695381164551
train gradient:  0.13102959046637264
iteration : 4955
train acc:  0.7421875
train loss:  0.4829869270324707
train gradient:  0.1571672191258116
iteration : 4956
train acc:  0.7109375
train loss:  0.5516428351402283
train gradient:  0.15085895034720956
iteration : 4957
train acc:  0.703125
train loss:  0.541799783706665
train gradient:  0.1737812090174438
iteration : 4958
train acc:  0.7578125
train loss:  0.44589078426361084
train gradient:  0.09158263134678919
iteration : 4959
train acc:  0.78125
train loss:  0.47603631019592285
train gradient:  0.11496487689155964
iteration : 4960
train acc:  0.6875
train loss:  0.5797972679138184
train gradient:  0.13513731759916364
iteration : 4961
train acc:  0.703125
train loss:  0.5011231899261475
train gradient:  0.10523597287081321
iteration : 4962
train acc:  0.7578125
train loss:  0.4879876375198364
train gradient:  0.13470857205904385
iteration : 4963
train acc:  0.703125
train loss:  0.5600478053092957
train gradient:  0.1829486965440016
iteration : 4964
train acc:  0.6875
train loss:  0.5408226251602173
train gradient:  0.14728042992325358
iteration : 4965
train acc:  0.75
train loss:  0.4894903302192688
train gradient:  0.12119726928586265
iteration : 4966
train acc:  0.6796875
train loss:  0.5691492557525635
train gradient:  0.14815874903781873
iteration : 4967
train acc:  0.75
train loss:  0.48453113436698914
train gradient:  0.13622136459049666
iteration : 4968
train acc:  0.7421875
train loss:  0.4778716266155243
train gradient:  0.1470824233133946
iteration : 4969
train acc:  0.6875
train loss:  0.5233467221260071
train gradient:  0.13196118981713006
iteration : 4970
train acc:  0.7265625
train loss:  0.5293093323707581
train gradient:  0.1387733183730474
iteration : 4971
train acc:  0.71875
train loss:  0.5049880743026733
train gradient:  0.12906927075183175
iteration : 4972
train acc:  0.7265625
train loss:  0.5776590704917908
train gradient:  0.18297453053605905
iteration : 4973
train acc:  0.6796875
train loss:  0.5439655780792236
train gradient:  0.16847809593925026
iteration : 4974
train acc:  0.7578125
train loss:  0.49390485882759094
train gradient:  0.1260328086030863
iteration : 4975
train acc:  0.734375
train loss:  0.49772149324417114
train gradient:  0.15758932689174793
iteration : 4976
train acc:  0.71875
train loss:  0.5485262870788574
train gradient:  0.17890043078951795
iteration : 4977
train acc:  0.7578125
train loss:  0.4882696270942688
train gradient:  0.12285699954365485
iteration : 4978
train acc:  0.6953125
train loss:  0.530351459980011
train gradient:  0.1629029701843563
iteration : 4979
train acc:  0.7421875
train loss:  0.5796651840209961
train gradient:  0.19151270856759608
iteration : 4980
train acc:  0.7578125
train loss:  0.48799705505371094
train gradient:  0.12686727077559762
iteration : 4981
train acc:  0.7578125
train loss:  0.49699461460113525
train gradient:  0.1323174702352681
iteration : 4982
train acc:  0.75
train loss:  0.48940709233283997
train gradient:  0.12047152778240472
iteration : 4983
train acc:  0.8125
train loss:  0.4739059507846832
train gradient:  0.10838289546927045
iteration : 4984
train acc:  0.71875
train loss:  0.5357087254524231
train gradient:  0.14117746465273773
iteration : 4985
train acc:  0.71875
train loss:  0.4664028286933899
train gradient:  0.14329555721818854
iteration : 4986
train acc:  0.65625
train loss:  0.5636383891105652
train gradient:  0.15917033328450853
iteration : 4987
train acc:  0.6875
train loss:  0.5400902032852173
train gradient:  0.18836765944279743
iteration : 4988
train acc:  0.6796875
train loss:  0.5303895473480225
train gradient:  0.15751782731652172
iteration : 4989
train acc:  0.6953125
train loss:  0.586723804473877
train gradient:  0.2101983696170709
iteration : 4990
train acc:  0.75
train loss:  0.45544081926345825
train gradient:  0.12188294018051246
iteration : 4991
train acc:  0.7109375
train loss:  0.5032408833503723
train gradient:  0.10297147265016567
iteration : 4992
train acc:  0.8046875
train loss:  0.4378816783428192
train gradient:  0.15500404476208296
iteration : 4993
train acc:  0.7734375
train loss:  0.4482191205024719
train gradient:  0.1277637490000674
iteration : 4994
train acc:  0.7109375
train loss:  0.542350709438324
train gradient:  0.14203610784030285
iteration : 4995
train acc:  0.75
train loss:  0.47847047448158264
train gradient:  0.14920262920596
iteration : 4996
train acc:  0.6875
train loss:  0.5653692483901978
train gradient:  0.18959919099537376
iteration : 4997
train acc:  0.703125
train loss:  0.5342579483985901
train gradient:  0.18572059736273094
iteration : 4998
train acc:  0.75
train loss:  0.44480007886886597
train gradient:  0.13718079099125935
iteration : 4999
train acc:  0.6953125
train loss:  0.49079570174217224
train gradient:  0.14205567759081428
iteration : 5000
train acc:  0.6953125
train loss:  0.5239199995994568
train gradient:  0.1659813940472191
iteration : 5001
train acc:  0.75
train loss:  0.4786520004272461
train gradient:  0.11035922591438665
iteration : 5002
train acc:  0.75
train loss:  0.5027208924293518
train gradient:  0.13557303556313666
iteration : 5003
train acc:  0.765625
train loss:  0.4709226191043854
train gradient:  0.11582893046651593
iteration : 5004
train acc:  0.671875
train loss:  0.5758545994758606
train gradient:  0.21547419214478608
iteration : 5005
train acc:  0.71875
train loss:  0.47031378746032715
train gradient:  0.1489625761849579
iteration : 5006
train acc:  0.7265625
train loss:  0.5017185807228088
train gradient:  0.14597262595864827
iteration : 5007
train acc:  0.6953125
train loss:  0.5184373259544373
train gradient:  0.1881648551815105
iteration : 5008
train acc:  0.7890625
train loss:  0.46787652373313904
train gradient:  0.13186005130429607
iteration : 5009
train acc:  0.8046875
train loss:  0.4602248966693878
train gradient:  0.12853045953780684
iteration : 5010
train acc:  0.78125
train loss:  0.44892174005508423
train gradient:  0.12692680927550554
iteration : 5011
train acc:  0.671875
train loss:  0.5745947957038879
train gradient:  0.17601000133015504
iteration : 5012
train acc:  0.7578125
train loss:  0.4800378084182739
train gradient:  0.14550821154996021
iteration : 5013
train acc:  0.734375
train loss:  0.522599458694458
train gradient:  0.15477378612016915
iteration : 5014
train acc:  0.796875
train loss:  0.47726476192474365
train gradient:  0.13247280664637773
iteration : 5015
train acc:  0.6875
train loss:  0.5972448587417603
train gradient:  0.18541734909444746
iteration : 5016
train acc:  0.6171875
train loss:  0.5987643003463745
train gradient:  0.18796446479533357
iteration : 5017
train acc:  0.6796875
train loss:  0.5204862356185913
train gradient:  0.16890024208851437
iteration : 5018
train acc:  0.625
train loss:  0.5712644457817078
train gradient:  0.1761969578156921
iteration : 5019
train acc:  0.7421875
train loss:  0.4779937267303467
train gradient:  0.160699864025981
iteration : 5020
train acc:  0.796875
train loss:  0.4394320547580719
train gradient:  0.1236592559727072
iteration : 5021
train acc:  0.7578125
train loss:  0.48936355113983154
train gradient:  0.13852963029665816
iteration : 5022
train acc:  0.6796875
train loss:  0.558498203754425
train gradient:  0.1822429990937638
iteration : 5023
train acc:  0.7578125
train loss:  0.4829637408256531
train gradient:  0.1582788371366028
iteration : 5024
train acc:  0.6640625
train loss:  0.5719354152679443
train gradient:  0.14702978505092482
iteration : 5025
train acc:  0.765625
train loss:  0.5391864776611328
train gradient:  0.14659342071805834
iteration : 5026
train acc:  0.7421875
train loss:  0.4971899092197418
train gradient:  0.15009256142410415
iteration : 5027
train acc:  0.765625
train loss:  0.45717519521713257
train gradient:  0.14455587516488283
iteration : 5028
train acc:  0.75
train loss:  0.4694880247116089
train gradient:  0.12483064254328732
iteration : 5029
train acc:  0.7421875
train loss:  0.5504865050315857
train gradient:  0.22903136298301024
iteration : 5030
train acc:  0.734375
train loss:  0.5595134496688843
train gradient:  0.21593967551247417
iteration : 5031
train acc:  0.7734375
train loss:  0.5070031881332397
train gradient:  0.1617019918215894
iteration : 5032
train acc:  0.734375
train loss:  0.4861832559108734
train gradient:  0.13001115502367494
iteration : 5033
train acc:  0.75
train loss:  0.5125213861465454
train gradient:  0.1520363746451151
iteration : 5034
train acc:  0.734375
train loss:  0.528910756111145
train gradient:  0.15977466870269208
iteration : 5035
train acc:  0.7265625
train loss:  0.5417769551277161
train gradient:  0.134857882522597
iteration : 5036
train acc:  0.7265625
train loss:  0.5103601217269897
train gradient:  0.1695712241415966
iteration : 5037
train acc:  0.7890625
train loss:  0.418239951133728
train gradient:  0.11661689801207195
iteration : 5038
train acc:  0.6640625
train loss:  0.5767936706542969
train gradient:  0.16078604116548162
iteration : 5039
train acc:  0.71875
train loss:  0.5532619953155518
train gradient:  0.17442095612819877
iteration : 5040
train acc:  0.6484375
train loss:  0.6362899541854858
train gradient:  0.19194561834424045
iteration : 5041
train acc:  0.75
train loss:  0.4833928346633911
train gradient:  0.14701842770822388
iteration : 5042
train acc:  0.7109375
train loss:  0.5378051996231079
train gradient:  0.13166703892830162
iteration : 5043
train acc:  0.6875
train loss:  0.6149587631225586
train gradient:  0.26159501600102614
iteration : 5044
train acc:  0.7265625
train loss:  0.5381125807762146
train gradient:  0.23334668075841294
iteration : 5045
train acc:  0.7265625
train loss:  0.5241335034370422
train gradient:  0.17024381828904334
iteration : 5046
train acc:  0.734375
train loss:  0.5161471962928772
train gradient:  0.13282006809931957
iteration : 5047
train acc:  0.7421875
train loss:  0.4986593723297119
train gradient:  0.15833738388383886
iteration : 5048
train acc:  0.78125
train loss:  0.473863422870636
train gradient:  0.14108845002703851
iteration : 5049
train acc:  0.6953125
train loss:  0.5576534271240234
train gradient:  0.19663142750753965
iteration : 5050
train acc:  0.7265625
train loss:  0.5830156207084656
train gradient:  0.21554466593652466
iteration : 5051
train acc:  0.765625
train loss:  0.492136687040329
train gradient:  0.11837795120379739
iteration : 5052
train acc:  0.6953125
train loss:  0.5257523059844971
train gradient:  0.13751757120529995
iteration : 5053
train acc:  0.7421875
train loss:  0.47074735164642334
train gradient:  0.10198945533395787
iteration : 5054
train acc:  0.7578125
train loss:  0.47271931171417236
train gradient:  0.1420577919626992
iteration : 5055
train acc:  0.7265625
train loss:  0.5197249054908752
train gradient:  0.17328795902486807
iteration : 5056
train acc:  0.7734375
train loss:  0.4495227634906769
train gradient:  0.11746685900462164
iteration : 5057
train acc:  0.7734375
train loss:  0.5009737014770508
train gradient:  0.12377254504812356
iteration : 5058
train acc:  0.78125
train loss:  0.45247212052345276
train gradient:  0.1345762131072176
iteration : 5059
train acc:  0.7421875
train loss:  0.5345621109008789
train gradient:  0.17748761484031583
iteration : 5060
train acc:  0.7421875
train loss:  0.5360360145568848
train gradient:  0.17522078616068779
iteration : 5061
train acc:  0.75
train loss:  0.4909815788269043
train gradient:  0.12237987730276408
iteration : 5062
train acc:  0.75
train loss:  0.4490092396736145
train gradient:  0.11622079808147176
iteration : 5063
train acc:  0.7265625
train loss:  0.48328521847724915
train gradient:  0.15676115195721468
iteration : 5064
train acc:  0.734375
train loss:  0.5324954390525818
train gradient:  0.1773810191235467
iteration : 5065
train acc:  0.7109375
train loss:  0.46688634157180786
train gradient:  0.15318262177256425
iteration : 5066
train acc:  0.7734375
train loss:  0.49007338285446167
train gradient:  0.13732177287537808
iteration : 5067
train acc:  0.734375
train loss:  0.5173784494400024
train gradient:  0.13926411866655133
iteration : 5068
train acc:  0.6796875
train loss:  0.5681301355361938
train gradient:  0.21460997252368336
iteration : 5069
train acc:  0.6875
train loss:  0.6035271883010864
train gradient:  0.24123003578835328
iteration : 5070
train acc:  0.7109375
train loss:  0.5336470603942871
train gradient:  0.17791974884488768
iteration : 5071
train acc:  0.7734375
train loss:  0.4620576798915863
train gradient:  0.12963541660459438
iteration : 5072
train acc:  0.75
train loss:  0.45863664150238037
train gradient:  0.12527963329816463
iteration : 5073
train acc:  0.7578125
train loss:  0.5478157997131348
train gradient:  0.12894387853934228
iteration : 5074
train acc:  0.703125
train loss:  0.4863426387310028
train gradient:  0.12930910679649651
iteration : 5075
train acc:  0.734375
train loss:  0.5032677054405212
train gradient:  0.16860604861417566
iteration : 5076
train acc:  0.7109375
train loss:  0.5048079490661621
train gradient:  0.15997026808674658
iteration : 5077
train acc:  0.7109375
train loss:  0.5750621557235718
train gradient:  0.17565166118317582
iteration : 5078
train acc:  0.765625
train loss:  0.44680479168891907
train gradient:  0.11722996415845324
iteration : 5079
train acc:  0.7265625
train loss:  0.4909515976905823
train gradient:  0.13792220020850993
iteration : 5080
train acc:  0.6875
train loss:  0.5475373268127441
train gradient:  0.17541295217670466
iteration : 5081
train acc:  0.703125
train loss:  0.4972405731678009
train gradient:  0.10468090702921913
iteration : 5082
train acc:  0.7421875
train loss:  0.4786842167377472
train gradient:  0.16318469513472833
iteration : 5083
train acc:  0.6953125
train loss:  0.5409237742424011
train gradient:  0.16844060319898604
iteration : 5084
train acc:  0.7421875
train loss:  0.490908682346344
train gradient:  0.1250039450294533
iteration : 5085
train acc:  0.8203125
train loss:  0.442146897315979
train gradient:  0.11792452915394315
iteration : 5086
train acc:  0.765625
train loss:  0.5030474662780762
train gradient:  0.16772239694872465
iteration : 5087
train acc:  0.7421875
train loss:  0.4927225112915039
train gradient:  0.12604661090975372
iteration : 5088
train acc:  0.6953125
train loss:  0.5049727559089661
train gradient:  0.19572875166111386
iteration : 5089
train acc:  0.671875
train loss:  0.5307967066764832
train gradient:  0.2125949122263157
iteration : 5090
train acc:  0.6953125
train loss:  0.5769609808921814
train gradient:  0.15960165341210797
iteration : 5091
train acc:  0.765625
train loss:  0.45889681577682495
train gradient:  0.12274467747723901
iteration : 5092
train acc:  0.796875
train loss:  0.4320993423461914
train gradient:  0.11503237407891721
iteration : 5093
train acc:  0.7578125
train loss:  0.45535022020339966
train gradient:  0.1109649790742708
iteration : 5094
train acc:  0.6640625
train loss:  0.5623763203620911
train gradient:  0.21194499163686767
iteration : 5095
train acc:  0.78125
train loss:  0.515569269657135
train gradient:  0.15803400998747713
iteration : 5096
train acc:  0.8359375
train loss:  0.4127829670906067
train gradient:  0.1092176395078064
iteration : 5097
train acc:  0.703125
train loss:  0.5304016470909119
train gradient:  0.1455797939201599
iteration : 5098
train acc:  0.765625
train loss:  0.43214359879493713
train gradient:  0.14279707324593194
iteration : 5099
train acc:  0.7421875
train loss:  0.4932190179824829
train gradient:  0.15763556809166224
iteration : 5100
train acc:  0.703125
train loss:  0.5347268581390381
train gradient:  0.15824160418527033
iteration : 5101
train acc:  0.6796875
train loss:  0.6167870759963989
train gradient:  0.2755278005150534
iteration : 5102
train acc:  0.71875
train loss:  0.5077266097068787
train gradient:  0.14112687614925007
iteration : 5103
train acc:  0.7578125
train loss:  0.4779459834098816
train gradient:  0.11337607943432028
iteration : 5104
train acc:  0.703125
train loss:  0.538108229637146
train gradient:  0.12334899306536654
iteration : 5105
train acc:  0.765625
train loss:  0.4794151186943054
train gradient:  0.11889069064165542
iteration : 5106
train acc:  0.78125
train loss:  0.45390263199806213
train gradient:  0.12279583826106209
iteration : 5107
train acc:  0.78125
train loss:  0.4669073522090912
train gradient:  0.12160951969357622
iteration : 5108
train acc:  0.7109375
train loss:  0.5677351951599121
train gradient:  0.14730352274588926
iteration : 5109
train acc:  0.6796875
train loss:  0.5662114024162292
train gradient:  0.16541617604044478
iteration : 5110
train acc:  0.78125
train loss:  0.4231889843940735
train gradient:  0.12914951364450517
iteration : 5111
train acc:  0.6796875
train loss:  0.5747689604759216
train gradient:  0.17002038937867403
iteration : 5112
train acc:  0.71875
train loss:  0.5593834519386292
train gradient:  0.19315476505695922
iteration : 5113
train acc:  0.75
train loss:  0.5143190026283264
train gradient:  0.16842261805037362
iteration : 5114
train acc:  0.6875
train loss:  0.5575589537620544
train gradient:  0.20825843436454078
iteration : 5115
train acc:  0.6953125
train loss:  0.5624567866325378
train gradient:  0.2135091201905142
iteration : 5116
train acc:  0.734375
train loss:  0.5279744863510132
train gradient:  0.2021513303081262
iteration : 5117
train acc:  0.7421875
train loss:  0.5338189601898193
train gradient:  0.1909906296005362
iteration : 5118
train acc:  0.7265625
train loss:  0.51874178647995
train gradient:  0.15859265214462936
iteration : 5119
train acc:  0.7734375
train loss:  0.4833034574985504
train gradient:  0.13338956195629348
iteration : 5120
train acc:  0.6796875
train loss:  0.5888895988464355
train gradient:  0.16406363126834855
iteration : 5121
train acc:  0.6953125
train loss:  0.5898159742355347
train gradient:  0.17602107061946137
iteration : 5122
train acc:  0.71875
train loss:  0.5421106815338135
train gradient:  0.17510738506455145
iteration : 5123
train acc:  0.7109375
train loss:  0.5544341802597046
train gradient:  0.17529651949639616
iteration : 5124
train acc:  0.78125
train loss:  0.48999011516571045
train gradient:  0.13410017409616024
iteration : 5125
train acc:  0.7578125
train loss:  0.48944827914237976
train gradient:  0.11965262607558734
iteration : 5126
train acc:  0.765625
train loss:  0.46945247054100037
train gradient:  0.12355676181932009
iteration : 5127
train acc:  0.78125
train loss:  0.46393948793411255
train gradient:  0.1140756727152412
iteration : 5128
train acc:  0.765625
train loss:  0.4872649013996124
train gradient:  0.1560743033109423
iteration : 5129
train acc:  0.765625
train loss:  0.4995059370994568
train gradient:  0.116302394838221
iteration : 5130
train acc:  0.7265625
train loss:  0.530776858329773
train gradient:  0.13389890729038184
iteration : 5131
train acc:  0.71875
train loss:  0.5219520330429077
train gradient:  0.15370523041967532
iteration : 5132
train acc:  0.78125
train loss:  0.4913738965988159
train gradient:  0.10703273620367919
iteration : 5133
train acc:  0.6953125
train loss:  0.5428509712219238
train gradient:  0.15355387742725005
iteration : 5134
train acc:  0.734375
train loss:  0.5142220854759216
train gradient:  0.14569586055218742
iteration : 5135
train acc:  0.71875
train loss:  0.5062918663024902
train gradient:  0.14260936964926577
iteration : 5136
train acc:  0.703125
train loss:  0.5542669892311096
train gradient:  0.15533528943045277
iteration : 5137
train acc:  0.6875
train loss:  0.5750408172607422
train gradient:  0.2666785680061388
iteration : 5138
train acc:  0.7578125
train loss:  0.48870670795440674
train gradient:  0.13306777678089984
iteration : 5139
train acc:  0.71875
train loss:  0.5121158361434937
train gradient:  0.17966619439350534
iteration : 5140
train acc:  0.703125
train loss:  0.5401418209075928
train gradient:  0.15716568671280506
iteration : 5141
train acc:  0.734375
train loss:  0.48763683438301086
train gradient:  0.12725835728755563
iteration : 5142
train acc:  0.671875
train loss:  0.5310113430023193
train gradient:  0.1375811450509276
iteration : 5143
train acc:  0.75
train loss:  0.4418654441833496
train gradient:  0.12876916691940665
iteration : 5144
train acc:  0.7109375
train loss:  0.5107632279396057
train gradient:  0.15637495481762728
iteration : 5145
train acc:  0.8125
train loss:  0.44578519463539124
train gradient:  0.11362735279924624
iteration : 5146
train acc:  0.71875
train loss:  0.5839086771011353
train gradient:  0.19039258113059548
iteration : 5147
train acc:  0.7265625
train loss:  0.5470555424690247
train gradient:  0.15879422241952795
iteration : 5148
train acc:  0.6484375
train loss:  0.5819708108901978
train gradient:  0.2515510958052972
iteration : 5149
train acc:  0.6875
train loss:  0.5770463943481445
train gradient:  0.16707679883675755
iteration : 5150
train acc:  0.71875
train loss:  0.5079493522644043
train gradient:  0.17789144362777481
iteration : 5151
train acc:  0.7890625
train loss:  0.4398891031742096
train gradient:  0.1388907686702756
iteration : 5152
train acc:  0.6484375
train loss:  0.6160793900489807
train gradient:  0.2073669475887916
iteration : 5153
train acc:  0.71875
train loss:  0.49802663922309875
train gradient:  0.1533931457900541
iteration : 5154
train acc:  0.703125
train loss:  0.5486173033714294
train gradient:  0.17540792423356544
iteration : 5155
train acc:  0.734375
train loss:  0.4709368348121643
train gradient:  0.13046452755574306
iteration : 5156
train acc:  0.6796875
train loss:  0.5892974138259888
train gradient:  0.17012186409391328
iteration : 5157
train acc:  0.703125
train loss:  0.575950026512146
train gradient:  0.17557559099489828
iteration : 5158
train acc:  0.671875
train loss:  0.5289170742034912
train gradient:  0.13119320333673073
iteration : 5159
train acc:  0.7890625
train loss:  0.4564732015132904
train gradient:  0.14133251886344975
iteration : 5160
train acc:  0.6796875
train loss:  0.5447755455970764
train gradient:  0.14849186888794957
iteration : 5161
train acc:  0.7578125
train loss:  0.4707956910133362
train gradient:  0.11351756058430255
iteration : 5162
train acc:  0.703125
train loss:  0.5095967054367065
train gradient:  0.13487482811351997
iteration : 5163
train acc:  0.71875
train loss:  0.47952741384506226
train gradient:  0.15016847922792065
iteration : 5164
train acc:  0.734375
train loss:  0.5296691656112671
train gradient:  0.18137891447657828
iteration : 5165
train acc:  0.6953125
train loss:  0.5300372242927551
train gradient:  0.13573953468060157
iteration : 5166
train acc:  0.7265625
train loss:  0.5121456384658813
train gradient:  0.1725620959965783
iteration : 5167
train acc:  0.7109375
train loss:  0.5481777191162109
train gradient:  0.1680868778277786
iteration : 5168
train acc:  0.6953125
train loss:  0.5026752948760986
train gradient:  0.13402042306541043
iteration : 5169
train acc:  0.734375
train loss:  0.5000032186508179
train gradient:  0.1538327074089127
iteration : 5170
train acc:  0.7265625
train loss:  0.5357434153556824
train gradient:  0.18436089990700116
iteration : 5171
train acc:  0.7734375
train loss:  0.4681376814842224
train gradient:  0.1527777494205666
iteration : 5172
train acc:  0.765625
train loss:  0.5004348754882812
train gradient:  0.12209489638577603
iteration : 5173
train acc:  0.7265625
train loss:  0.5080482959747314
train gradient:  0.18816035899866085
iteration : 5174
train acc:  0.75
train loss:  0.48466601967811584
train gradient:  0.15047038587753725
iteration : 5175
train acc:  0.734375
train loss:  0.48774731159210205
train gradient:  0.1446933125923593
iteration : 5176
train acc:  0.765625
train loss:  0.4710319936275482
train gradient:  0.1436145358831441
iteration : 5177
train acc:  0.6796875
train loss:  0.5433996915817261
train gradient:  0.18103873772508522
iteration : 5178
train acc:  0.8125
train loss:  0.4577637016773224
train gradient:  0.11759702884116077
iteration : 5179
train acc:  0.6796875
train loss:  0.5642846822738647
train gradient:  0.20036179838733825
iteration : 5180
train acc:  0.7578125
train loss:  0.5231138467788696
train gradient:  0.17393081448068548
iteration : 5181
train acc:  0.734375
train loss:  0.5632051229476929
train gradient:  0.21634796875277806
iteration : 5182
train acc:  0.734375
train loss:  0.49856147170066833
train gradient:  0.19046117004865204
iteration : 5183
train acc:  0.7578125
train loss:  0.4814477562904358
train gradient:  0.12280605956935009
iteration : 5184
train acc:  0.71875
train loss:  0.5202599763870239
train gradient:  0.14941859801848398
iteration : 5185
train acc:  0.6953125
train loss:  0.5570732355117798
train gradient:  0.1399269153257941
iteration : 5186
train acc:  0.6953125
train loss:  0.5668362975120544
train gradient:  0.1490155328572832
iteration : 5187
train acc:  0.7421875
train loss:  0.527162492275238
train gradient:  0.1775074948677077
iteration : 5188
train acc:  0.734375
train loss:  0.5194129943847656
train gradient:  0.16834033368932605
iteration : 5189
train acc:  0.71875
train loss:  0.5572762489318848
train gradient:  0.16467670176026927
iteration : 5190
train acc:  0.6875
train loss:  0.5697499513626099
train gradient:  0.19754384137610237
iteration : 5191
train acc:  0.7421875
train loss:  0.47166845202445984
train gradient:  0.11060858277614632
iteration : 5192
train acc:  0.7421875
train loss:  0.5214312076568604
train gradient:  0.1572542802616977
iteration : 5193
train acc:  0.765625
train loss:  0.4617766737937927
train gradient:  0.14684669230526975
iteration : 5194
train acc:  0.734375
train loss:  0.4849596917629242
train gradient:  0.14528693903995754
iteration : 5195
train acc:  0.6875
train loss:  0.5964035987854004
train gradient:  0.20431484866169342
iteration : 5196
train acc:  0.75
train loss:  0.48460251092910767
train gradient:  0.1366119380441626
iteration : 5197
train acc:  0.7421875
train loss:  0.5058128237724304
train gradient:  0.2069866639058736
iteration : 5198
train acc:  0.7578125
train loss:  0.4675116539001465
train gradient:  0.1390156008359024
iteration : 5199
train acc:  0.75
train loss:  0.47932201623916626
train gradient:  0.1338365506748032
iteration : 5200
train acc:  0.7421875
train loss:  0.4894191026687622
train gradient:  0.1104561989896993
iteration : 5201
train acc:  0.6640625
train loss:  0.5374777317047119
train gradient:  0.14993704668601132
iteration : 5202
train acc:  0.734375
train loss:  0.5019623041152954
train gradient:  0.12925966404542155
iteration : 5203
train acc:  0.796875
train loss:  0.46861177682876587
train gradient:  0.11205026384366817
iteration : 5204
train acc:  0.7734375
train loss:  0.47512736916542053
train gradient:  0.13183316505849696
iteration : 5205
train acc:  0.7265625
train loss:  0.5545568466186523
train gradient:  0.17237363589202956
iteration : 5206
train acc:  0.7265625
train loss:  0.5056238770484924
train gradient:  0.11814051962221694
iteration : 5207
train acc:  0.671875
train loss:  0.5533212423324585
train gradient:  0.18156882901039711
iteration : 5208
train acc:  0.734375
train loss:  0.5192675590515137
train gradient:  0.1565941448686782
iteration : 5209
train acc:  0.71875
train loss:  0.5089679956436157
train gradient:  0.1589305533260113
iteration : 5210
train acc:  0.765625
train loss:  0.5353307127952576
train gradient:  0.13813529487502718
iteration : 5211
train acc:  0.6328125
train loss:  0.6014531254768372
train gradient:  0.1654455146161214
iteration : 5212
train acc:  0.6953125
train loss:  0.5341496467590332
train gradient:  0.14396876970347167
iteration : 5213
train acc:  0.6875
train loss:  0.5486502647399902
train gradient:  0.17252081336575992
iteration : 5214
train acc:  0.765625
train loss:  0.43478620052337646
train gradient:  0.1853098003973941
iteration : 5215
train acc:  0.7421875
train loss:  0.4844781756401062
train gradient:  0.12587252293767634
iteration : 5216
train acc:  0.7734375
train loss:  0.4660698175430298
train gradient:  0.12295535085261786
iteration : 5217
train acc:  0.7265625
train loss:  0.48586493730545044
train gradient:  0.11837618532714142
iteration : 5218
train acc:  0.796875
train loss:  0.45883482694625854
train gradient:  0.10983102999702836
iteration : 5219
train acc:  0.703125
train loss:  0.5852802991867065
train gradient:  0.20242504906805475
iteration : 5220
train acc:  0.734375
train loss:  0.4843379259109497
train gradient:  0.14169108066651198
iteration : 5221
train acc:  0.71875
train loss:  0.49034634232521057
train gradient:  0.1517953723689056
iteration : 5222
train acc:  0.765625
train loss:  0.42742985486984253
train gradient:  0.11924322064777683
iteration : 5223
train acc:  0.75
train loss:  0.4997255504131317
train gradient:  0.13898629062001655
iteration : 5224
train acc:  0.7265625
train loss:  0.5150273442268372
train gradient:  0.12716084396725347
iteration : 5225
train acc:  0.7890625
train loss:  0.4355396032333374
train gradient:  0.10603282129082028
iteration : 5226
train acc:  0.6875
train loss:  0.520193338394165
train gradient:  0.14113667919614073
iteration : 5227
train acc:  0.765625
train loss:  0.48076295852661133
train gradient:  0.13944647933227888
iteration : 5228
train acc:  0.765625
train loss:  0.47728678584098816
train gradient:  0.13514324661342536
iteration : 5229
train acc:  0.75
train loss:  0.562156081199646
train gradient:  0.18221926840114389
iteration : 5230
train acc:  0.6796875
train loss:  0.5593128204345703
train gradient:  0.19383123213082687
iteration : 5231
train acc:  0.765625
train loss:  0.4474368095397949
train gradient:  0.1648890763865618
iteration : 5232
train acc:  0.7578125
train loss:  0.49637970328330994
train gradient:  0.12669749705576117
iteration : 5233
train acc:  0.71875
train loss:  0.5115940570831299
train gradient:  0.17043806974798162
iteration : 5234
train acc:  0.6796875
train loss:  0.5846968293190002
train gradient:  0.20059302781215288
iteration : 5235
train acc:  0.7734375
train loss:  0.5111160278320312
train gradient:  0.14037933566512376
iteration : 5236
train acc:  0.734375
train loss:  0.489045113325119
train gradient:  0.16408733523004854
iteration : 5237
train acc:  0.7578125
train loss:  0.45883575081825256
train gradient:  0.13214624768034477
iteration : 5238
train acc:  0.7890625
train loss:  0.47221866250038147
train gradient:  0.21100569686691123
iteration : 5239
train acc:  0.703125
train loss:  0.5714350938796997
train gradient:  0.18287015278958346
iteration : 5240
train acc:  0.765625
train loss:  0.4937293827533722
train gradient:  0.18445534974215877
iteration : 5241
train acc:  0.703125
train loss:  0.5416471362113953
train gradient:  0.20334345485536504
iteration : 5242
train acc:  0.6875
train loss:  0.5691195726394653
train gradient:  0.22160016521692977
iteration : 5243
train acc:  0.7421875
train loss:  0.4702244699001312
train gradient:  0.11610023207442906
iteration : 5244
train acc:  0.71875
train loss:  0.49916842579841614
train gradient:  0.12964563592932565
iteration : 5245
train acc:  0.7109375
train loss:  0.574960470199585
train gradient:  0.18123960145339957
iteration : 5246
train acc:  0.8203125
train loss:  0.45266303420066833
train gradient:  0.1646400355245458
iteration : 5247
train acc:  0.7734375
train loss:  0.46853578090667725
train gradient:  0.12184706302224037
iteration : 5248
train acc:  0.828125
train loss:  0.40372398495674133
train gradient:  0.11437997342769428
iteration : 5249
train acc:  0.703125
train loss:  0.515226423740387
train gradient:  0.1916261469503734
iteration : 5250
train acc:  0.796875
train loss:  0.47876545786857605
train gradient:  0.12823972716646553
iteration : 5251
train acc:  0.734375
train loss:  0.49928516149520874
train gradient:  0.16033557190514663
iteration : 5252
train acc:  0.7109375
train loss:  0.5293538570404053
train gradient:  0.14721650411117893
iteration : 5253
train acc:  0.7890625
train loss:  0.4788777232170105
train gradient:  0.13978630616657914
iteration : 5254
train acc:  0.71875
train loss:  0.5256428718566895
train gradient:  0.17812831512450972
iteration : 5255
train acc:  0.71875
train loss:  0.5241097211837769
train gradient:  0.17314803981697013
iteration : 5256
train acc:  0.734375
train loss:  0.5325624346733093
train gradient:  0.16706287996036057
iteration : 5257
train acc:  0.8046875
train loss:  0.47304999828338623
train gradient:  0.14187240865085382
iteration : 5258
train acc:  0.6796875
train loss:  0.5860155820846558
train gradient:  0.2035456108530042
iteration : 5259
train acc:  0.71875
train loss:  0.5377217531204224
train gradient:  0.14961179407289893
iteration : 5260
train acc:  0.7265625
train loss:  0.49721771478652954
train gradient:  0.16632543359648283
iteration : 5261
train acc:  0.734375
train loss:  0.5179250240325928
train gradient:  0.14914907341641187
iteration : 5262
train acc:  0.75
train loss:  0.4850519597530365
train gradient:  0.13460733941808944
iteration : 5263
train acc:  0.7265625
train loss:  0.519767165184021
train gradient:  0.20035920756129066
iteration : 5264
train acc:  0.7578125
train loss:  0.46560344099998474
train gradient:  0.11812777204191365
iteration : 5265
train acc:  0.71875
train loss:  0.4975435435771942
train gradient:  0.13348783771218733
iteration : 5266
train acc:  0.7265625
train loss:  0.5188757181167603
train gradient:  0.1833626269526472
iteration : 5267
train acc:  0.734375
train loss:  0.5090566873550415
train gradient:  0.12922024681423844
iteration : 5268
train acc:  0.6953125
train loss:  0.5294635891914368
train gradient:  0.1690354200601376
iteration : 5269
train acc:  0.7265625
train loss:  0.5609378218650818
train gradient:  0.1781551983993685
iteration : 5270
train acc:  0.8125
train loss:  0.42655250430107117
train gradient:  0.13647723336381234
iteration : 5271
train acc:  0.6484375
train loss:  0.6588461399078369
train gradient:  0.28840337677513384
iteration : 5272
train acc:  0.6875
train loss:  0.5406569242477417
train gradient:  0.18429324262379443
iteration : 5273
train acc:  0.734375
train loss:  0.5115505456924438
train gradient:  0.13477118273548233
iteration : 5274
train acc:  0.7109375
train loss:  0.5513885021209717
train gradient:  0.22478957125801036
iteration : 5275
train acc:  0.78125
train loss:  0.47227972745895386
train gradient:  0.11678427138655231
iteration : 5276
train acc:  0.765625
train loss:  0.4579246938228607
train gradient:  0.12222629300733183
iteration : 5277
train acc:  0.71875
train loss:  0.4980586767196655
train gradient:  0.10484703436316829
iteration : 5278
train acc:  0.6640625
train loss:  0.5549708604812622
train gradient:  0.17783090641186255
iteration : 5279
train acc:  0.7265625
train loss:  0.5302942991256714
train gradient:  0.1593766587960958
iteration : 5280
train acc:  0.6953125
train loss:  0.5234122276306152
train gradient:  0.15093759170399496
iteration : 5281
train acc:  0.71875
train loss:  0.5434653162956238
train gradient:  0.15249243146622693
iteration : 5282
train acc:  0.78125
train loss:  0.519845187664032
train gradient:  0.1570693765004237
iteration : 5283
train acc:  0.75
train loss:  0.46876513957977295
train gradient:  0.1329868792889552
iteration : 5284
train acc:  0.7421875
train loss:  0.4957757890224457
train gradient:  0.1423427327814858
iteration : 5285
train acc:  0.765625
train loss:  0.5129950046539307
train gradient:  0.13417285423067066
iteration : 5286
train acc:  0.609375
train loss:  0.5960339307785034
train gradient:  0.1754987388361754
iteration : 5287
train acc:  0.7578125
train loss:  0.4761646091938019
train gradient:  0.11497191029653456
iteration : 5288
train acc:  0.7109375
train loss:  0.5552021861076355
train gradient:  0.1762048592007019
iteration : 5289
train acc:  0.7265625
train loss:  0.45667678117752075
train gradient:  0.11686714672106087
iteration : 5290
train acc:  0.7421875
train loss:  0.543487012386322
train gradient:  0.14633195238382923
iteration : 5291
train acc:  0.765625
train loss:  0.4885317087173462
train gradient:  0.14595578422005256
iteration : 5292
train acc:  0.6875
train loss:  0.5115102529525757
train gradient:  0.13779971198246277
iteration : 5293
train acc:  0.6640625
train loss:  0.5509415864944458
train gradient:  0.18478797046921058
iteration : 5294
train acc:  0.6796875
train loss:  0.5625385046005249
train gradient:  0.19563789413138205
iteration : 5295
train acc:  0.734375
train loss:  0.5020933747291565
train gradient:  0.1225066106468903
iteration : 5296
train acc:  0.671875
train loss:  0.6023162007331848
train gradient:  0.21337421956011532
iteration : 5297
train acc:  0.6875
train loss:  0.5545060634613037
train gradient:  0.17023904011831537
iteration : 5298
train acc:  0.7421875
train loss:  0.4960142970085144
train gradient:  0.12514580410013765
iteration : 5299
train acc:  0.6640625
train loss:  0.5934909582138062
train gradient:  0.21110786718545016
iteration : 5300
train acc:  0.7734375
train loss:  0.4522107243537903
train gradient:  0.109510252363219
iteration : 5301
train acc:  0.7265625
train loss:  0.5201794505119324
train gradient:  0.158557085457285
iteration : 5302
train acc:  0.734375
train loss:  0.4870983958244324
train gradient:  0.13703079691408682
iteration : 5303
train acc:  0.734375
train loss:  0.4991261959075928
train gradient:  0.14057244214116288
iteration : 5304
train acc:  0.7578125
train loss:  0.5214890241622925
train gradient:  0.14482438053472146
iteration : 5305
train acc:  0.765625
train loss:  0.5221797823905945
train gradient:  0.16491789597778986
iteration : 5306
train acc:  0.6328125
train loss:  0.6153837442398071
train gradient:  0.19621451085350622
iteration : 5307
train acc:  0.734375
train loss:  0.5204760432243347
train gradient:  0.1782891502005332
iteration : 5308
train acc:  0.78125
train loss:  0.5147169828414917
train gradient:  0.20123142445480946
iteration : 5309
train acc:  0.7421875
train loss:  0.5187710523605347
train gradient:  0.17452108393419752
iteration : 5310
train acc:  0.828125
train loss:  0.4962482154369354
train gradient:  0.1204634467730088
iteration : 5311
train acc:  0.7265625
train loss:  0.535934329032898
train gradient:  0.13166126857814398
iteration : 5312
train acc:  0.7734375
train loss:  0.4769030809402466
train gradient:  0.11947307925294129
iteration : 5313
train acc:  0.765625
train loss:  0.43684059381484985
train gradient:  0.10906068529561098
iteration : 5314
train acc:  0.796875
train loss:  0.47048425674438477
train gradient:  0.11923763554790197
iteration : 5315
train acc:  0.7265625
train loss:  0.5114220976829529
train gradient:  0.13815663099057768
iteration : 5316
train acc:  0.6484375
train loss:  0.6026827096939087
train gradient:  0.23771540644922984
iteration : 5317
train acc:  0.8125
train loss:  0.4093550145626068
train gradient:  0.11660350460263907
iteration : 5318
train acc:  0.7890625
train loss:  0.4969136714935303
train gradient:  0.1252675508115448
iteration : 5319
train acc:  0.703125
train loss:  0.504256010055542
train gradient:  0.12657390002367072
iteration : 5320
train acc:  0.6171875
train loss:  0.5921295285224915
train gradient:  0.17128761536908624
iteration : 5321
train acc:  0.734375
train loss:  0.46651533246040344
train gradient:  0.10924716814892473
iteration : 5322
train acc:  0.7578125
train loss:  0.47774580121040344
train gradient:  0.11845981449708669
iteration : 5323
train acc:  0.796875
train loss:  0.42723947763442993
train gradient:  0.12016148064658233
iteration : 5324
train acc:  0.7578125
train loss:  0.5012110471725464
train gradient:  0.15502845861694747
iteration : 5325
train acc:  0.703125
train loss:  0.507485568523407
train gradient:  0.13775291934322692
iteration : 5326
train acc:  0.671875
train loss:  0.5663091540336609
train gradient:  0.19084305341336952
iteration : 5327
train acc:  0.7265625
train loss:  0.5188145637512207
train gradient:  0.2553367814211193
iteration : 5328
train acc:  0.703125
train loss:  0.5685654878616333
train gradient:  0.163868632906978
iteration : 5329
train acc:  0.7421875
train loss:  0.4821820855140686
train gradient:  0.12347469779043675
iteration : 5330
train acc:  0.7890625
train loss:  0.46687671542167664
train gradient:  0.1466535466256937
iteration : 5331
train acc:  0.7578125
train loss:  0.47664403915405273
train gradient:  0.17482866889543996
iteration : 5332
train acc:  0.671875
train loss:  0.5967287421226501
train gradient:  0.2610559127402063
iteration : 5333
train acc:  0.78125
train loss:  0.4697682857513428
train gradient:  0.12763105230853328
iteration : 5334
train acc:  0.6796875
train loss:  0.5590634346008301
train gradient:  0.1343942696184259
iteration : 5335
train acc:  0.7578125
train loss:  0.47428616881370544
train gradient:  0.14106349806401497
iteration : 5336
train acc:  0.7421875
train loss:  0.5452165007591248
train gradient:  0.1502970421630776
iteration : 5337
train acc:  0.765625
train loss:  0.48624059557914734
train gradient:  0.13175726089702625
iteration : 5338
train acc:  0.734375
train loss:  0.5160462260246277
train gradient:  0.16095367613996397
iteration : 5339
train acc:  0.6796875
train loss:  0.5463241934776306
train gradient:  0.16881746631025682
iteration : 5340
train acc:  0.765625
train loss:  0.5200017690658569
train gradient:  0.16454241068371359
iteration : 5341
train acc:  0.7578125
train loss:  0.481946736574173
train gradient:  0.14666976872833487
iteration : 5342
train acc:  0.75
train loss:  0.45704564452171326
train gradient:  0.12979091684580454
iteration : 5343
train acc:  0.7265625
train loss:  0.4833786189556122
train gradient:  0.14435214457788498
iteration : 5344
train acc:  0.640625
train loss:  0.5904741883277893
train gradient:  0.19864438190755562
iteration : 5345
train acc:  0.6953125
train loss:  0.5384134650230408
train gradient:  0.16952481660646607
iteration : 5346
train acc:  0.6796875
train loss:  0.5727853775024414
train gradient:  0.15269231551980256
iteration : 5347
train acc:  0.7265625
train loss:  0.48202377557754517
train gradient:  0.14889318850193978
iteration : 5348
train acc:  0.734375
train loss:  0.4806012511253357
train gradient:  0.14696897529075148
iteration : 5349
train acc:  0.828125
train loss:  0.4576781988143921
train gradient:  0.145637857075532
iteration : 5350
train acc:  0.7578125
train loss:  0.47868844866752625
train gradient:  0.15202111148015998
iteration : 5351
train acc:  0.6796875
train loss:  0.5838148593902588
train gradient:  0.22801070633239878
iteration : 5352
train acc:  0.7265625
train loss:  0.49789556860923767
train gradient:  0.1548563254235716
iteration : 5353
train acc:  0.7421875
train loss:  0.5091423988342285
train gradient:  0.15018408330974703
iteration : 5354
train acc:  0.7265625
train loss:  0.5048662424087524
train gradient:  0.15527212214811317
iteration : 5355
train acc:  0.734375
train loss:  0.5433151125907898
train gradient:  0.14010481733105123
iteration : 5356
train acc:  0.6796875
train loss:  0.5643647313117981
train gradient:  0.19383831676035929
iteration : 5357
train acc:  0.734375
train loss:  0.4917256832122803
train gradient:  0.14040419064611978
iteration : 5358
train acc:  0.7265625
train loss:  0.4794794023036957
train gradient:  0.14787397379151518
iteration : 5359
train acc:  0.7109375
train loss:  0.5423288345336914
train gradient:  0.14725147894996424
iteration : 5360
train acc:  0.71875
train loss:  0.5342235565185547
train gradient:  0.1862359071951576
iteration : 5361
train acc:  0.7109375
train loss:  0.5641970634460449
train gradient:  0.2060345895975461
iteration : 5362
train acc:  0.65625
train loss:  0.5785740613937378
train gradient:  0.16092240117435885
iteration : 5363
train acc:  0.8203125
train loss:  0.44290173053741455
train gradient:  0.11778046955280937
iteration : 5364
train acc:  0.7890625
train loss:  0.4644739031791687
train gradient:  0.15285601398237741
iteration : 5365
train acc:  0.75
train loss:  0.5241985321044922
train gradient:  0.1807613742955227
iteration : 5366
train acc:  0.7109375
train loss:  0.5344763994216919
train gradient:  0.21459606388616315
iteration : 5367
train acc:  0.671875
train loss:  0.5403074026107788
train gradient:  0.13183542170696305
iteration : 5368
train acc:  0.6875
train loss:  0.6066734790802002
train gradient:  0.2966097830398815
iteration : 5369
train acc:  0.6953125
train loss:  0.528120756149292
train gradient:  0.15449786455655573
iteration : 5370
train acc:  0.671875
train loss:  0.5935719013214111
train gradient:  0.17341113826289994
iteration : 5371
train acc:  0.7265625
train loss:  0.49365103244781494
train gradient:  0.15393875887791614
iteration : 5372
train acc:  0.671875
train loss:  0.5266741514205933
train gradient:  0.14654428617936732
iteration : 5373
train acc:  0.65625
train loss:  0.5591964721679688
train gradient:  0.18242335671601798
iteration : 5374
train acc:  0.703125
train loss:  0.4970303177833557
train gradient:  0.1459949034107762
iteration : 5375
train acc:  0.78125
train loss:  0.4666055142879486
train gradient:  0.12781133739391112
iteration : 5376
train acc:  0.7734375
train loss:  0.4641945958137512
train gradient:  0.1256557218919045
iteration : 5377
train acc:  0.765625
train loss:  0.5165214538574219
train gradient:  0.1695465996650687
iteration : 5378
train acc:  0.71875
train loss:  0.4949195384979248
train gradient:  0.11751520690609382
iteration : 5379
train acc:  0.8125
train loss:  0.4405721426010132
train gradient:  0.11245483621457271
iteration : 5380
train acc:  0.78125
train loss:  0.4500317871570587
train gradient:  0.12932709506457118
iteration : 5381
train acc:  0.7421875
train loss:  0.4793711006641388
train gradient:  0.17190307712973754
iteration : 5382
train acc:  0.71875
train loss:  0.5280885696411133
train gradient:  0.19221963364773603
iteration : 5383
train acc:  0.734375
train loss:  0.47749006748199463
train gradient:  0.13441610697942752
iteration : 5384
train acc:  0.765625
train loss:  0.48449215292930603
train gradient:  0.1642170862175732
iteration : 5385
train acc:  0.7265625
train loss:  0.4896431267261505
train gradient:  0.11228625613550833
iteration : 5386
train acc:  0.734375
train loss:  0.5009148716926575
train gradient:  0.22350944846064327
iteration : 5387
train acc:  0.7578125
train loss:  0.4679199457168579
train gradient:  0.1223280218953629
iteration : 5388
train acc:  0.7109375
train loss:  0.5315638780593872
train gradient:  0.23123859752215803
iteration : 5389
train acc:  0.703125
train loss:  0.5614577531814575
train gradient:  0.2379119282938465
iteration : 5390
train acc:  0.7578125
train loss:  0.5230624675750732
train gradient:  0.14254365831054672
iteration : 5391
train acc:  0.6640625
train loss:  0.5442368984222412
train gradient:  0.127694390975768
iteration : 5392
train acc:  0.7890625
train loss:  0.4276342988014221
train gradient:  0.11514433278766625
iteration : 5393
train acc:  0.78125
train loss:  0.4644075036048889
train gradient:  0.11942590291858952
iteration : 5394
train acc:  0.703125
train loss:  0.5600447058677673
train gradient:  0.16907340940697174
iteration : 5395
train acc:  0.7265625
train loss:  0.512405514717102
train gradient:  0.18084503537364652
iteration : 5396
train acc:  0.75
train loss:  0.48758476972579956
train gradient:  0.1589142899390657
iteration : 5397
train acc:  0.6796875
train loss:  0.5539268851280212
train gradient:  0.147214428158572
iteration : 5398
train acc:  0.765625
train loss:  0.5088347792625427
train gradient:  0.14159034421678585
iteration : 5399
train acc:  0.75
train loss:  0.5031746625900269
train gradient:  0.17955362973303002
iteration : 5400
train acc:  0.6953125
train loss:  0.5152344703674316
train gradient:  0.15049901122702253
iteration : 5401
train acc:  0.7890625
train loss:  0.43586164712905884
train gradient:  0.1053712877434349
iteration : 5402
train acc:  0.7265625
train loss:  0.48236411809921265
train gradient:  0.155275373761791
iteration : 5403
train acc:  0.6640625
train loss:  0.5595398545265198
train gradient:  0.14361150571107595
iteration : 5404
train acc:  0.828125
train loss:  0.42729929089546204
train gradient:  0.10973350885635542
iteration : 5405
train acc:  0.7734375
train loss:  0.49296024441719055
train gradient:  0.13090328525238284
iteration : 5406
train acc:  0.6953125
train loss:  0.5696345567703247
train gradient:  0.15158006396947415
iteration : 5407
train acc:  0.6640625
train loss:  0.5836411714553833
train gradient:  0.21231794727583714
iteration : 5408
train acc:  0.7578125
train loss:  0.5193189382553101
train gradient:  0.15212233961400962
iteration : 5409
train acc:  0.7421875
train loss:  0.492918998003006
train gradient:  0.11469113820866424
iteration : 5410
train acc:  0.6484375
train loss:  0.5714197158813477
train gradient:  0.16282014252164545
iteration : 5411
train acc:  0.703125
train loss:  0.5359635353088379
train gradient:  0.1547339750499404
iteration : 5412
train acc:  0.640625
train loss:  0.5620070099830627
train gradient:  0.17899365451220883
iteration : 5413
train acc:  0.703125
train loss:  0.509589672088623
train gradient:  0.19020218495751742
iteration : 5414
train acc:  0.703125
train loss:  0.5192714333534241
train gradient:  0.14916769763550447
iteration : 5415
train acc:  0.65625
train loss:  0.5789995193481445
train gradient:  0.15367677722489143
iteration : 5416
train acc:  0.734375
train loss:  0.5234251618385315
train gradient:  0.18590963337602456
iteration : 5417
train acc:  0.6640625
train loss:  0.5221821665763855
train gradient:  0.14564588587225893
iteration : 5418
train acc:  0.6171875
train loss:  0.6068177819252014
train gradient:  0.2126617411623533
iteration : 5419
train acc:  0.7265625
train loss:  0.5729180574417114
train gradient:  0.14781854849865309
iteration : 5420
train acc:  0.6953125
train loss:  0.5153892040252686
train gradient:  0.16796879511622598
iteration : 5421
train acc:  0.78125
train loss:  0.519178569316864
train gradient:  0.15536301938889865
iteration : 5422
train acc:  0.7265625
train loss:  0.5226462483406067
train gradient:  0.14273172484126906
iteration : 5423
train acc:  0.6875
train loss:  0.5533162355422974
train gradient:  0.16399415574169257
iteration : 5424
train acc:  0.78125
train loss:  0.4391883611679077
train gradient:  0.08869185399974865
iteration : 5425
train acc:  0.8203125
train loss:  0.4029572308063507
train gradient:  0.09987903498530096
iteration : 5426
train acc:  0.828125
train loss:  0.42498111724853516
train gradient:  0.10770248901914684
iteration : 5427
train acc:  0.78125
train loss:  0.5208791494369507
train gradient:  0.1884130099586939
iteration : 5428
train acc:  0.6953125
train loss:  0.5099409818649292
train gradient:  0.14211443160998813
iteration : 5429
train acc:  0.71875
train loss:  0.49590742588043213
train gradient:  0.149419358322528
iteration : 5430
train acc:  0.71875
train loss:  0.49851951003074646
train gradient:  0.19994138834414507
iteration : 5431
train acc:  0.71875
train loss:  0.5917043685913086
train gradient:  0.1627186831323728
iteration : 5432
train acc:  0.6328125
train loss:  0.6063907146453857
train gradient:  0.18148282535872273
iteration : 5433
train acc:  0.765625
train loss:  0.48291468620300293
train gradient:  0.1328674480530151
iteration : 5434
train acc:  0.703125
train loss:  0.5267326831817627
train gradient:  0.1473922017238034
iteration : 5435
train acc:  0.7109375
train loss:  0.5294181108474731
train gradient:  0.1562318367156776
iteration : 5436
train acc:  0.703125
train loss:  0.5655636787414551
train gradient:  0.1469691531647454
iteration : 5437
train acc:  0.7109375
train loss:  0.4979678988456726
train gradient:  0.1337273862012453
iteration : 5438
train acc:  0.75
train loss:  0.4493453800678253
train gradient:  0.14290411677664278
iteration : 5439
train acc:  0.6875
train loss:  0.5390940308570862
train gradient:  0.1533950415876797
iteration : 5440
train acc:  0.734375
train loss:  0.5561689138412476
train gradient:  0.18965108969469613
iteration : 5441
train acc:  0.75
train loss:  0.5128664374351501
train gradient:  0.13714853358784598
iteration : 5442
train acc:  0.7578125
train loss:  0.47321900725364685
train gradient:  0.15593037706081597
iteration : 5443
train acc:  0.7734375
train loss:  0.5283051133155823
train gradient:  0.1915943889607491
iteration : 5444
train acc:  0.71875
train loss:  0.5002167224884033
train gradient:  0.1257497214638878
iteration : 5445
train acc:  0.71875
train loss:  0.5057032704353333
train gradient:  0.11835533049576998
iteration : 5446
train acc:  0.765625
train loss:  0.4582897424697876
train gradient:  0.126170146493988
iteration : 5447
train acc:  0.7421875
train loss:  0.45157259702682495
train gradient:  0.12925089624760427
iteration : 5448
train acc:  0.71875
train loss:  0.5002450942993164
train gradient:  0.1402917985692028
iteration : 5449
train acc:  0.703125
train loss:  0.5632359981536865
train gradient:  0.1621979315099327
iteration : 5450
train acc:  0.7578125
train loss:  0.49917593598365784
train gradient:  0.13953670479324148
iteration : 5451
train acc:  0.7265625
train loss:  0.47974175214767456
train gradient:  0.14416722255364822
iteration : 5452
train acc:  0.7421875
train loss:  0.5006473660469055
train gradient:  0.13126960404770055
iteration : 5453
train acc:  0.7578125
train loss:  0.48344528675079346
train gradient:  0.11552096558700685
iteration : 5454
train acc:  0.765625
train loss:  0.4748394787311554
train gradient:  0.10921419839361712
iteration : 5455
train acc:  0.7734375
train loss:  0.468119740486145
train gradient:  0.12171247992302675
iteration : 5456
train acc:  0.7109375
train loss:  0.5138497352600098
train gradient:  0.15280985663762875
iteration : 5457
train acc:  0.7578125
train loss:  0.4687806963920593
train gradient:  0.12865423596410125
iteration : 5458
train acc:  0.75
train loss:  0.5127760171890259
train gradient:  0.1541281495951869
iteration : 5459
train acc:  0.78125
train loss:  0.5033113360404968
train gradient:  0.12304658534438513
iteration : 5460
train acc:  0.75
train loss:  0.4774210453033447
train gradient:  0.13226286484671843
iteration : 5461
train acc:  0.6953125
train loss:  0.5423720479011536
train gradient:  0.15156207926472184
iteration : 5462
train acc:  0.71875
train loss:  0.4980625510215759
train gradient:  0.1401718335256201
iteration : 5463
train acc:  0.7578125
train loss:  0.4946734309196472
train gradient:  0.15654512871291065
iteration : 5464
train acc:  0.7421875
train loss:  0.48855718970298767
train gradient:  0.12343109530387796
iteration : 5465
train acc:  0.71875
train loss:  0.5287967920303345
train gradient:  0.15420550175385977
iteration : 5466
train acc:  0.75
train loss:  0.5144674777984619
train gradient:  0.13533769983730953
iteration : 5467
train acc:  0.78125
train loss:  0.4951144754886627
train gradient:  0.1193172669679886
iteration : 5468
train acc:  0.7109375
train loss:  0.5456398129463196
train gradient:  0.14104896467325084
iteration : 5469
train acc:  0.796875
train loss:  0.41313180327415466
train gradient:  0.11313247403241332
iteration : 5470
train acc:  0.7265625
train loss:  0.4862801432609558
train gradient:  0.1380181676404822
iteration : 5471
train acc:  0.734375
train loss:  0.5164345502853394
train gradient:  0.16357857289063166
iteration : 5472
train acc:  0.75
train loss:  0.4847903549671173
train gradient:  0.1453301017424328
iteration : 5473
train acc:  0.703125
train loss:  0.5158851146697998
train gradient:  0.16175239797278945
iteration : 5474
train acc:  0.78125
train loss:  0.4793199300765991
train gradient:  0.09996385606268907
iteration : 5475
train acc:  0.734375
train loss:  0.5153124928474426
train gradient:  0.14915167041863286
iteration : 5476
train acc:  0.7109375
train loss:  0.5185627937316895
train gradient:  0.20220544282576836
iteration : 5477
train acc:  0.7734375
train loss:  0.46663981676101685
train gradient:  0.1074368230005166
iteration : 5478
train acc:  0.7578125
train loss:  0.4765913784503937
train gradient:  0.2022240590987482
iteration : 5479
train acc:  0.6796875
train loss:  0.5247119665145874
train gradient:  0.1596612108441869
iteration : 5480
train acc:  0.6953125
train loss:  0.5416954755783081
train gradient:  0.168676165212286
iteration : 5481
train acc:  0.7421875
train loss:  0.5052975416183472
train gradient:  0.1340189389145439
iteration : 5482
train acc:  0.671875
train loss:  0.5737411379814148
train gradient:  0.20957592903213385
iteration : 5483
train acc:  0.71875
train loss:  0.5157199501991272
train gradient:  0.1634894858593245
iteration : 5484
train acc:  0.734375
train loss:  0.46446114778518677
train gradient:  0.13091407082085102
iteration : 5485
train acc:  0.7578125
train loss:  0.5252377986907959
train gradient:  0.16863795727183234
iteration : 5486
train acc:  0.7421875
train loss:  0.5060603618621826
train gradient:  0.12260282799478073
iteration : 5487
train acc:  0.7109375
train loss:  0.5402354001998901
train gradient:  0.13745676660202047
iteration : 5488
train acc:  0.734375
train loss:  0.4783290922641754
train gradient:  0.13926011711318392
iteration : 5489
train acc:  0.78125
train loss:  0.49508947134017944
train gradient:  0.14584275108872585
iteration : 5490
train acc:  0.7421875
train loss:  0.4983241558074951
train gradient:  0.14517322070629068
iteration : 5491
train acc:  0.71875
train loss:  0.5154248476028442
train gradient:  0.14578333347487527
iteration : 5492
train acc:  0.734375
train loss:  0.571061372756958
train gradient:  0.1630494969966932
iteration : 5493
train acc:  0.7578125
train loss:  0.5008764266967773
train gradient:  0.1567703025524946
iteration : 5494
train acc:  0.703125
train loss:  0.5316946506500244
train gradient:  0.1874886879325136
iteration : 5495
train acc:  0.7578125
train loss:  0.48234787583351135
train gradient:  0.20732708439142117
iteration : 5496
train acc:  0.6953125
train loss:  0.5686919689178467
train gradient:  0.264596107247929
iteration : 5497
train acc:  0.7109375
train loss:  0.5365524888038635
train gradient:  0.15258730943531162
iteration : 5498
train acc:  0.734375
train loss:  0.49544674158096313
train gradient:  0.17628942847893406
iteration : 5499
train acc:  0.7578125
train loss:  0.46067631244659424
train gradient:  0.143224613791736
iteration : 5500
train acc:  0.7734375
train loss:  0.45554590225219727
train gradient:  0.11260682377826503
iteration : 5501
train acc:  0.7578125
train loss:  0.5029253363609314
train gradient:  0.15793547733674773
iteration : 5502
train acc:  0.7421875
train loss:  0.4886903464794159
train gradient:  0.12981425131794722
iteration : 5503
train acc:  0.7578125
train loss:  0.4521094560623169
train gradient:  0.14333984058737748
iteration : 5504
train acc:  0.734375
train loss:  0.45150506496429443
train gradient:  0.13012544870472786
iteration : 5505
train acc:  0.78125
train loss:  0.4677583873271942
train gradient:  0.11225598264760595
iteration : 5506
train acc:  0.7265625
train loss:  0.5036362409591675
train gradient:  0.17474093426280002
iteration : 5507
train acc:  0.734375
train loss:  0.49720123410224915
train gradient:  0.15020305054727495
iteration : 5508
train acc:  0.6953125
train loss:  0.5305267572402954
train gradient:  0.208093842478026
iteration : 5509
train acc:  0.7265625
train loss:  0.4953124523162842
train gradient:  0.15158261051306704
iteration : 5510
train acc:  0.7734375
train loss:  0.49659693241119385
train gradient:  0.1834167126505022
iteration : 5511
train acc:  0.78125
train loss:  0.47144362330436707
train gradient:  0.12285359501611585
iteration : 5512
train acc:  0.703125
train loss:  0.5527374148368835
train gradient:  0.15385793313835822
iteration : 5513
train acc:  0.703125
train loss:  0.561040997505188
train gradient:  0.21186327401371863
iteration : 5514
train acc:  0.7578125
train loss:  0.4827636480331421
train gradient:  0.12028388784459228
iteration : 5515
train acc:  0.7578125
train loss:  0.5292056798934937
train gradient:  0.15083606737729927
iteration : 5516
train acc:  0.796875
train loss:  0.4431724548339844
train gradient:  0.1020213488669016
iteration : 5517
train acc:  0.6953125
train loss:  0.5259125828742981
train gradient:  0.17547187043294693
iteration : 5518
train acc:  0.71875
train loss:  0.5304288864135742
train gradient:  0.1317992659839831
iteration : 5519
train acc:  0.6640625
train loss:  0.5718052983283997
train gradient:  0.20408339911390416
iteration : 5520
train acc:  0.7734375
train loss:  0.46915745735168457
train gradient:  0.12818094599620758
iteration : 5521
train acc:  0.765625
train loss:  0.5050672292709351
train gradient:  0.13096674492768115
iteration : 5522
train acc:  0.7578125
train loss:  0.506216287612915
train gradient:  0.1357426875087852
iteration : 5523
train acc:  0.6953125
train loss:  0.5209691524505615
train gradient:  0.1422060281900293
iteration : 5524
train acc:  0.7578125
train loss:  0.45868435502052307
train gradient:  0.12012409994680828
iteration : 5525
train acc:  0.7265625
train loss:  0.48266857862472534
train gradient:  0.1375388317650271
iteration : 5526
train acc:  0.7265625
train loss:  0.5773921012878418
train gradient:  0.2168183888993698
iteration : 5527
train acc:  0.6640625
train loss:  0.5687435865402222
train gradient:  0.19191702118239096
iteration : 5528
train acc:  0.703125
train loss:  0.5149178504943848
train gradient:  0.15439841201282511
iteration : 5529
train acc:  0.7421875
train loss:  0.462026983499527
train gradient:  0.11993501636155238
iteration : 5530
train acc:  0.75
train loss:  0.4556180536746979
train gradient:  0.12095372193001457
iteration : 5531
train acc:  0.6875
train loss:  0.5234309434890747
train gradient:  0.17267113441217197
iteration : 5532
train acc:  0.6953125
train loss:  0.5479099154472351
train gradient:  0.15121089192428222
iteration : 5533
train acc:  0.7734375
train loss:  0.4632599949836731
train gradient:  0.11662234325993544
iteration : 5534
train acc:  0.703125
train loss:  0.5776721835136414
train gradient:  0.1795761237222121
iteration : 5535
train acc:  0.6875
train loss:  0.5907618999481201
train gradient:  0.17154188647775598
iteration : 5536
train acc:  0.7265625
train loss:  0.4879176616668701
train gradient:  0.1291009139437881
iteration : 5537
train acc:  0.703125
train loss:  0.5614817142486572
train gradient:  0.15731008829263235
iteration : 5538
train acc:  0.7421875
train loss:  0.5057960152626038
train gradient:  0.14910289768092305
iteration : 5539
train acc:  0.6640625
train loss:  0.5629530549049377
train gradient:  0.23522810257897275
iteration : 5540
train acc:  0.7109375
train loss:  0.4981607496738434
train gradient:  0.1357841380467115
iteration : 5541
train acc:  0.7109375
train loss:  0.4940881133079529
train gradient:  0.17629767941623115
iteration : 5542
train acc:  0.7265625
train loss:  0.513928234577179
train gradient:  0.12620522583238108
iteration : 5543
train acc:  0.8046875
train loss:  0.43912291526794434
train gradient:  0.10198639962517338
iteration : 5544
train acc:  0.703125
train loss:  0.5260277986526489
train gradient:  0.1655134834250498
iteration : 5545
train acc:  0.71875
train loss:  0.4993599057197571
train gradient:  0.12900002771287572
iteration : 5546
train acc:  0.6953125
train loss:  0.5371301174163818
train gradient:  0.17928166616420876
iteration : 5547
train acc:  0.796875
train loss:  0.4499671459197998
train gradient:  0.12280685571345153
iteration : 5548
train acc:  0.7265625
train loss:  0.4998489022254944
train gradient:  0.1298515382823114
iteration : 5549
train acc:  0.7109375
train loss:  0.5246656537055969
train gradient:  0.18629918286046201
iteration : 5550
train acc:  0.703125
train loss:  0.5372720956802368
train gradient:  0.14531757111766985
iteration : 5551
train acc:  0.7578125
train loss:  0.5230516791343689
train gradient:  0.17781457201097817
iteration : 5552
train acc:  0.7421875
train loss:  0.4830782413482666
train gradient:  0.1466445839825741
iteration : 5553
train acc:  0.640625
train loss:  0.6094648241996765
train gradient:  0.20079154638330282
iteration : 5554
train acc:  0.703125
train loss:  0.5404496192932129
train gradient:  0.18088071232677386
iteration : 5555
train acc:  0.7109375
train loss:  0.5827428102493286
train gradient:  0.1826690766473028
iteration : 5556
train acc:  0.7421875
train loss:  0.46216222643852234
train gradient:  0.11313508411777258
iteration : 5557
train acc:  0.65625
train loss:  0.6203435063362122
train gradient:  0.22131954684027766
iteration : 5558
train acc:  0.7109375
train loss:  0.5042245388031006
train gradient:  0.15281902484540985
iteration : 5559
train acc:  0.7265625
train loss:  0.5200147032737732
train gradient:  0.14811765206963248
iteration : 5560
train acc:  0.7265625
train loss:  0.5007039308547974
train gradient:  0.16431410764220644
iteration : 5561
train acc:  0.703125
train loss:  0.5385876893997192
train gradient:  0.17494365678026333
iteration : 5562
train acc:  0.7265625
train loss:  0.5210119485855103
train gradient:  0.18341424166904838
iteration : 5563
train acc:  0.7421875
train loss:  0.4720059335231781
train gradient:  0.1395246408058316
iteration : 5564
train acc:  0.7421875
train loss:  0.4735932946205139
train gradient:  0.1634287743212967
iteration : 5565
train acc:  0.8046875
train loss:  0.5060441493988037
train gradient:  0.13214067770994042
iteration : 5566
train acc:  0.7734375
train loss:  0.4814373850822449
train gradient:  0.13359925251754012
iteration : 5567
train acc:  0.765625
train loss:  0.5063682794570923
train gradient:  0.14682203408211203
iteration : 5568
train acc:  0.6953125
train loss:  0.5362643003463745
train gradient:  0.14717080542002353
iteration : 5569
train acc:  0.75
train loss:  0.49540647864341736
train gradient:  0.14788455115875265
iteration : 5570
train acc:  0.78125
train loss:  0.47933393716812134
train gradient:  0.12035377297433322
iteration : 5571
train acc:  0.765625
train loss:  0.45683610439300537
train gradient:  0.11618435721040374
iteration : 5572
train acc:  0.7265625
train loss:  0.49773889780044556
train gradient:  0.16517728752545163
iteration : 5573
train acc:  0.7421875
train loss:  0.4879538416862488
train gradient:  0.11524049851369288
iteration : 5574
train acc:  0.7578125
train loss:  0.48541057109832764
train gradient:  0.11540704102274953
iteration : 5575
train acc:  0.734375
train loss:  0.5337110757827759
train gradient:  0.1389694793446374
iteration : 5576
train acc:  0.734375
train loss:  0.4710328280925751
train gradient:  0.12712739194682332
iteration : 5577
train acc:  0.703125
train loss:  0.5309648513793945
train gradient:  0.1488078454192888
iteration : 5578
train acc:  0.7421875
train loss:  0.5031208992004395
train gradient:  0.13823769518939938
iteration : 5579
train acc:  0.75
train loss:  0.442635178565979
train gradient:  0.09912247304838881
iteration : 5580
train acc:  0.7734375
train loss:  0.47093796730041504
train gradient:  0.11426524687239055
iteration : 5581
train acc:  0.7421875
train loss:  0.4955483675003052
train gradient:  0.14578010521326074
iteration : 5582
train acc:  0.7265625
train loss:  0.48999956250190735
train gradient:  0.1293558866609336
iteration : 5583
train acc:  0.75
train loss:  0.4965452551841736
train gradient:  0.1622457871413079
iteration : 5584
train acc:  0.7578125
train loss:  0.5341265201568604
train gradient:  0.1493393442145157
iteration : 5585
train acc:  0.7265625
train loss:  0.5620169639587402
train gradient:  0.15617558172910428
iteration : 5586
train acc:  0.7421875
train loss:  0.4956684112548828
train gradient:  0.1324953428282759
iteration : 5587
train acc:  0.75
train loss:  0.4725850522518158
train gradient:  0.11181738593711507
iteration : 5588
train acc:  0.765625
train loss:  0.4766473174095154
train gradient:  0.12918798634164186
iteration : 5589
train acc:  0.7421875
train loss:  0.4809877276420593
train gradient:  0.18795603425221957
iteration : 5590
train acc:  0.7421875
train loss:  0.47504109144210815
train gradient:  0.1307524413044766
iteration : 5591
train acc:  0.71875
train loss:  0.48950931429862976
train gradient:  0.14431086068715332
iteration : 5592
train acc:  0.75
train loss:  0.47303226590156555
train gradient:  0.11680357284454221
iteration : 5593
train acc:  0.7734375
train loss:  0.5025824904441833
train gradient:  0.16219215094884545
iteration : 5594
train acc:  0.6875
train loss:  0.5826153755187988
train gradient:  0.16427888157906517
iteration : 5595
train acc:  0.7578125
train loss:  0.4746808409690857
train gradient:  0.18108014586569487
iteration : 5596
train acc:  0.7578125
train loss:  0.46392524242401123
train gradient:  0.1400120460055703
iteration : 5597
train acc:  0.6328125
train loss:  0.6643384099006653
train gradient:  0.3153227366631026
iteration : 5598
train acc:  0.671875
train loss:  0.5395991802215576
train gradient:  0.16833722761646264
iteration : 5599
train acc:  0.6953125
train loss:  0.519576907157898
train gradient:  0.12724590504238031
iteration : 5600
train acc:  0.7578125
train loss:  0.47754377126693726
train gradient:  0.12355333202314683
iteration : 5601
train acc:  0.7109375
train loss:  0.5142486095428467
train gradient:  0.2299743210629381
iteration : 5602
train acc:  0.75
train loss:  0.49471065402030945
train gradient:  0.13072855288436686
iteration : 5603
train acc:  0.7734375
train loss:  0.48744577169418335
train gradient:  0.11156995376703381
iteration : 5604
train acc:  0.71875
train loss:  0.5145356059074402
train gradient:  0.17831597829158025
iteration : 5605
train acc:  0.7265625
train loss:  0.5106269121170044
train gradient:  0.13174072685070687
iteration : 5606
train acc:  0.765625
train loss:  0.5004580020904541
train gradient:  0.15451626095533055
iteration : 5607
train acc:  0.6875
train loss:  0.5673163533210754
train gradient:  0.1585519048929832
iteration : 5608
train acc:  0.71875
train loss:  0.5207113027572632
train gradient:  0.15294794439361076
iteration : 5609
train acc:  0.71875
train loss:  0.5340782403945923
train gradient:  0.14411501801780402
iteration : 5610
train acc:  0.75
train loss:  0.4969538748264313
train gradient:  0.15507753825710024
iteration : 5611
train acc:  0.7578125
train loss:  0.4956128001213074
train gradient:  0.14223801779534923
iteration : 5612
train acc:  0.75
train loss:  0.507672905921936
train gradient:  0.15046960698889977
iteration : 5613
train acc:  0.7421875
train loss:  0.4999249577522278
train gradient:  0.15348918933837474
iteration : 5614
train acc:  0.7578125
train loss:  0.5081416368484497
train gradient:  0.14596006906537945
iteration : 5615
train acc:  0.7421875
train loss:  0.4870339035987854
train gradient:  0.13303617558768227
iteration : 5616
train acc:  0.6953125
train loss:  0.5508376359939575
train gradient:  0.1818555543140113
iteration : 5617
train acc:  0.765625
train loss:  0.4801986813545227
train gradient:  0.12106554909548388
iteration : 5618
train acc:  0.6796875
train loss:  0.6011982560157776
train gradient:  0.18972866697945326
iteration : 5619
train acc:  0.75
train loss:  0.4295077323913574
train gradient:  0.11270321250293143
iteration : 5620
train acc:  0.6875
train loss:  0.549649178981781
train gradient:  0.18005015056242907
iteration : 5621
train acc:  0.7734375
train loss:  0.4942576587200165
train gradient:  0.14533203679176987
iteration : 5622
train acc:  0.765625
train loss:  0.5054553747177124
train gradient:  0.13066680674193515
iteration : 5623
train acc:  0.6640625
train loss:  0.5675711631774902
train gradient:  0.19373548981977606
iteration : 5624
train acc:  0.78125
train loss:  0.5219858288764954
train gradient:  0.1468962129063404
iteration : 5625
train acc:  0.7109375
train loss:  0.5701316595077515
train gradient:  0.19119531653560798
iteration : 5626
train acc:  0.7421875
train loss:  0.4574132561683655
train gradient:  0.13265669190907675
iteration : 5627
train acc:  0.7734375
train loss:  0.4878336191177368
train gradient:  0.1395445808151241
iteration : 5628
train acc:  0.765625
train loss:  0.4920775592327118
train gradient:  0.14922210418817033
iteration : 5629
train acc:  0.7421875
train loss:  0.5081449747085571
train gradient:  0.11101992497406214
iteration : 5630
train acc:  0.796875
train loss:  0.4707016348838806
train gradient:  0.15587612900574943
iteration : 5631
train acc:  0.71875
train loss:  0.5152790546417236
train gradient:  0.13714652691373155
iteration : 5632
train acc:  0.6875
train loss:  0.5864275693893433
train gradient:  0.21030491208856916
iteration : 5633
train acc:  0.7265625
train loss:  0.49680113792419434
train gradient:  0.1201102069011296
iteration : 5634
train acc:  0.75
train loss:  0.43924224376678467
train gradient:  0.12108114967127571
iteration : 5635
train acc:  0.78125
train loss:  0.44059550762176514
train gradient:  0.10762574034732234
iteration : 5636
train acc:  0.7734375
train loss:  0.5113643407821655
train gradient:  0.1434267356403103
iteration : 5637
train acc:  0.71875
train loss:  0.4685632586479187
train gradient:  0.1251961754068715
iteration : 5638
train acc:  0.75
train loss:  0.5129475593566895
train gradient:  0.15630949777682254
iteration : 5639
train acc:  0.703125
train loss:  0.5552787780761719
train gradient:  0.18226842848180416
iteration : 5640
train acc:  0.7109375
train loss:  0.5310946702957153
train gradient:  0.19339961500551234
iteration : 5641
train acc:  0.703125
train loss:  0.5783983469009399
train gradient:  0.21069784342255943
iteration : 5642
train acc:  0.765625
train loss:  0.5026403665542603
train gradient:  0.15123446935741724
iteration : 5643
train acc:  0.7890625
train loss:  0.48607945442199707
train gradient:  0.1578400074241841
iteration : 5644
train acc:  0.78125
train loss:  0.4646035432815552
train gradient:  0.1314066268084834
iteration : 5645
train acc:  0.65625
train loss:  0.5546572208404541
train gradient:  0.21423745986922949
iteration : 5646
train acc:  0.78125
train loss:  0.48048079013824463
train gradient:  0.1291926320960833
iteration : 5647
train acc:  0.703125
train loss:  0.5183230638504028
train gradient:  0.1756605015627989
iteration : 5648
train acc:  0.8203125
train loss:  0.41988784074783325
train gradient:  0.13487291559059095
iteration : 5649
train acc:  0.6640625
train loss:  0.5462524890899658
train gradient:  0.15879035624501553
iteration : 5650
train acc:  0.75
train loss:  0.4848003089427948
train gradient:  0.1536511759415767
iteration : 5651
train acc:  0.7421875
train loss:  0.5122083425521851
train gradient:  0.12049032879058064
iteration : 5652
train acc:  0.75
train loss:  0.49129632115364075
train gradient:  0.11436014877559005
iteration : 5653
train acc:  0.765625
train loss:  0.4875336289405823
train gradient:  0.11804190746920859
iteration : 5654
train acc:  0.796875
train loss:  0.4554058313369751
train gradient:  0.1627019337111278
iteration : 5655
train acc:  0.7734375
train loss:  0.4899320602416992
train gradient:  0.16168682032641296
iteration : 5656
train acc:  0.7265625
train loss:  0.4930511713027954
train gradient:  0.13251085165095122
iteration : 5657
train acc:  0.7109375
train loss:  0.5647504329681396
train gradient:  0.17019909436662045
iteration : 5658
train acc:  0.7734375
train loss:  0.5083494782447815
train gradient:  0.13177876466965915
iteration : 5659
train acc:  0.7578125
train loss:  0.4905669093132019
train gradient:  0.15895156542015515
iteration : 5660
train acc:  0.71875
train loss:  0.5104074478149414
train gradient:  0.16838515509488874
iteration : 5661
train acc:  0.765625
train loss:  0.5288279056549072
train gradient:  0.21900901083102808
iteration : 5662
train acc:  0.75
train loss:  0.5026503205299377
train gradient:  0.14660326117156225
iteration : 5663
train acc:  0.6640625
train loss:  0.5829778909683228
train gradient:  0.1858082566198811
iteration : 5664
train acc:  0.7421875
train loss:  0.5516877174377441
train gradient:  0.14330286790042612
iteration : 5665
train acc:  0.78125
train loss:  0.4818306863307953
train gradient:  0.12434079373219036
iteration : 5666
train acc:  0.7421875
train loss:  0.5264052152633667
train gradient:  0.1711099112801546
iteration : 5667
train acc:  0.75
train loss:  0.44552794098854065
train gradient:  0.13693870504965083
iteration : 5668
train acc:  0.703125
train loss:  0.5569654703140259
train gradient:  0.21824525410629375
iteration : 5669
train acc:  0.765625
train loss:  0.5021263957023621
train gradient:  0.12369992668775892
iteration : 5670
train acc:  0.7109375
train loss:  0.5775126218795776
train gradient:  0.160093859205456
iteration : 5671
train acc:  0.7578125
train loss:  0.520041823387146
train gradient:  0.18696169363756265
iteration : 5672
train acc:  0.734375
train loss:  0.5230417251586914
train gradient:  0.14023427143243425
iteration : 5673
train acc:  0.71875
train loss:  0.5242981910705566
train gradient:  0.13504537241892345
iteration : 5674
train acc:  0.7265625
train loss:  0.5270680785179138
train gradient:  0.13457683068152218
iteration : 5675
train acc:  0.796875
train loss:  0.4434783458709717
train gradient:  0.12708747666572923
iteration : 5676
train acc:  0.75
train loss:  0.5190005302429199
train gradient:  0.1986570630419539
iteration : 5677
train acc:  0.8046875
train loss:  0.44255730509757996
train gradient:  0.10252557221494735
iteration : 5678
train acc:  0.7421875
train loss:  0.5167338848114014
train gradient:  0.13427039093267987
iteration : 5679
train acc:  0.734375
train loss:  0.5632461309432983
train gradient:  0.1828586208706957
iteration : 5680
train acc:  0.7421875
train loss:  0.5021693706512451
train gradient:  0.20319849926521866
iteration : 5681
train acc:  0.71875
train loss:  0.46372514963150024
train gradient:  0.11476819172226264
iteration : 5682
train acc:  0.7890625
train loss:  0.4634363651275635
train gradient:  0.12664300221888433
iteration : 5683
train acc:  0.7734375
train loss:  0.4642004668712616
train gradient:  0.11460935102486297
iteration : 5684
train acc:  0.734375
train loss:  0.5127612352371216
train gradient:  0.1523933258974382
iteration : 5685
train acc:  0.703125
train loss:  0.5091818571090698
train gradient:  0.11389361977153978
iteration : 5686
train acc:  0.78125
train loss:  0.490740567445755
train gradient:  0.15000735982160562
iteration : 5687
train acc:  0.734375
train loss:  0.5168359875679016
train gradient:  0.1404795246900197
iteration : 5688
train acc:  0.7265625
train loss:  0.5566439628601074
train gradient:  0.16397725364285903
iteration : 5689
train acc:  0.765625
train loss:  0.48384177684783936
train gradient:  0.1374462002875393
iteration : 5690
train acc:  0.734375
train loss:  0.4981617331504822
train gradient:  0.1802270468325352
iteration : 5691
train acc:  0.7734375
train loss:  0.488947331905365
train gradient:  0.1377969632887684
iteration : 5692
train acc:  0.7421875
train loss:  0.45136669278144836
train gradient:  0.155141891166695
iteration : 5693
train acc:  0.7265625
train loss:  0.48983627557754517
train gradient:  0.17844034948026885
iteration : 5694
train acc:  0.7734375
train loss:  0.47584208846092224
train gradient:  0.14601984193680062
iteration : 5695
train acc:  0.75
train loss:  0.4987955391407013
train gradient:  0.13396222410073055
iteration : 5696
train acc:  0.71875
train loss:  0.5277619361877441
train gradient:  0.12813960370423805
iteration : 5697
train acc:  0.7265625
train loss:  0.5140079259872437
train gradient:  0.15262084630835493
iteration : 5698
train acc:  0.703125
train loss:  0.5807244777679443
train gradient:  0.21436084535385413
iteration : 5699
train acc:  0.765625
train loss:  0.4845975637435913
train gradient:  0.11206835040824166
iteration : 5700
train acc:  0.6953125
train loss:  0.5900003910064697
train gradient:  0.17293977613206482
iteration : 5701
train acc:  0.71875
train loss:  0.4858133792877197
train gradient:  0.1479327807248444
iteration : 5702
train acc:  0.8125
train loss:  0.4143407940864563
train gradient:  0.10229309445976485
iteration : 5703
train acc:  0.8203125
train loss:  0.4447711706161499
train gradient:  0.1483734487397459
iteration : 5704
train acc:  0.8046875
train loss:  0.4383583068847656
train gradient:  0.11793778236786888
iteration : 5705
train acc:  0.7265625
train loss:  0.5472264289855957
train gradient:  0.21521441463599608
iteration : 5706
train acc:  0.765625
train loss:  0.4962562918663025
train gradient:  0.1414412304034326
iteration : 5707
train acc:  0.6875
train loss:  0.5402488112449646
train gradient:  0.16801365230762175
iteration : 5708
train acc:  0.7109375
train loss:  0.5309872031211853
train gradient:  0.15913650552820882
iteration : 5709
train acc:  0.796875
train loss:  0.46058857440948486
train gradient:  0.12835958631128996
iteration : 5710
train acc:  0.7421875
train loss:  0.4765266180038452
train gradient:  0.14251532771287606
iteration : 5711
train acc:  0.7109375
train loss:  0.4954705238342285
train gradient:  0.13327089387331664
iteration : 5712
train acc:  0.78125
train loss:  0.47874921560287476
train gradient:  0.11856146011364693
iteration : 5713
train acc:  0.703125
train loss:  0.5382431745529175
train gradient:  0.1560304575973241
iteration : 5714
train acc:  0.7421875
train loss:  0.5529077053070068
train gradient:  0.19667952173740522
iteration : 5715
train acc:  0.765625
train loss:  0.49220409989356995
train gradient:  0.11460260045474385
iteration : 5716
train acc:  0.671875
train loss:  0.567707359790802
train gradient:  0.158231919959428
iteration : 5717
train acc:  0.7421875
train loss:  0.49751564860343933
train gradient:  0.16230858539416856
iteration : 5718
train acc:  0.75
train loss:  0.476679265499115
train gradient:  0.10107692862055245
iteration : 5719
train acc:  0.6875
train loss:  0.5731215476989746
train gradient:  0.1763455111170546
iteration : 5720
train acc:  0.703125
train loss:  0.5261317491531372
train gradient:  0.13881024061271025
iteration : 5721
train acc:  0.71875
train loss:  0.5596930384635925
train gradient:  0.16503460493707878
iteration : 5722
train acc:  0.71875
train loss:  0.5497167110443115
train gradient:  0.20306363069296002
iteration : 5723
train acc:  0.7265625
train loss:  0.5155581831932068
train gradient:  0.15207080597633432
iteration : 5724
train acc:  0.7265625
train loss:  0.4700912833213806
train gradient:  0.13405346764836798
iteration : 5725
train acc:  0.6953125
train loss:  0.5587167739868164
train gradient:  0.17762104958951652
iteration : 5726
train acc:  0.671875
train loss:  0.5518761873245239
train gradient:  0.16278266511750106
iteration : 5727
train acc:  0.765625
train loss:  0.46400514245033264
train gradient:  0.14255239540266204
iteration : 5728
train acc:  0.734375
train loss:  0.5281836986541748
train gradient:  0.14878295144997317
iteration : 5729
train acc:  0.75
train loss:  0.489343523979187
train gradient:  0.16136799465894064
iteration : 5730
train acc:  0.7265625
train loss:  0.46861785650253296
train gradient:  0.16995957791006416
iteration : 5731
train acc:  0.7109375
train loss:  0.5231984257698059
train gradient:  0.1093531633667958
iteration : 5732
train acc:  0.8046875
train loss:  0.4083887040615082
train gradient:  0.1074239729747092
iteration : 5733
train acc:  0.78125
train loss:  0.47153419256210327
train gradient:  0.1374488043263003
iteration : 5734
train acc:  0.734375
train loss:  0.5079395174980164
train gradient:  0.13893171236055857
iteration : 5735
train acc:  0.8203125
train loss:  0.4204745888710022
train gradient:  0.10867472203430119
iteration : 5736
train acc:  0.75
train loss:  0.4823090434074402
train gradient:  0.13018149997771755
iteration : 5737
train acc:  0.78125
train loss:  0.481501042842865
train gradient:  0.18261373821821272
iteration : 5738
train acc:  0.6875
train loss:  0.5403186082839966
train gradient:  0.15406470550386114
iteration : 5739
train acc:  0.71875
train loss:  0.5550870299339294
train gradient:  0.16150335723080939
iteration : 5740
train acc:  0.7109375
train loss:  0.5031071305274963
train gradient:  0.15287937336460417
iteration : 5741
train acc:  0.734375
train loss:  0.4815560281276703
train gradient:  0.17648902115301462
iteration : 5742
train acc:  0.765625
train loss:  0.47129306197166443
train gradient:  0.16762425765643674
iteration : 5743
train acc:  0.7734375
train loss:  0.4319501221179962
train gradient:  0.12056789135589945
iteration : 5744
train acc:  0.7265625
train loss:  0.49349695444107056
train gradient:  0.13499873815077726
iteration : 5745
train acc:  0.71875
train loss:  0.5346961617469788
train gradient:  0.15895096654978236
iteration : 5746
train acc:  0.765625
train loss:  0.5283980965614319
train gradient:  0.19521532362862282
iteration : 5747
train acc:  0.7265625
train loss:  0.48794034123420715
train gradient:  0.11896938485802394
iteration : 5748
train acc:  0.7109375
train loss:  0.5221449136734009
train gradient:  0.17552514029762567
iteration : 5749
train acc:  0.71875
train loss:  0.5059838891029358
train gradient:  0.16355215275586643
iteration : 5750
train acc:  0.75
train loss:  0.5524579286575317
train gradient:  0.14946769656047149
iteration : 5751
train acc:  0.640625
train loss:  0.6360495090484619
train gradient:  0.20188009013359304
iteration : 5752
train acc:  0.7265625
train loss:  0.5642259120941162
train gradient:  0.20685504696812945
iteration : 5753
train acc:  0.8046875
train loss:  0.48458462953567505
train gradient:  0.12941801725245852
iteration : 5754
train acc:  0.71875
train loss:  0.5045621395111084
train gradient:  0.1623435185052194
iteration : 5755
train acc:  0.71875
train loss:  0.4821085035800934
train gradient:  0.11678780254811096
iteration : 5756
train acc:  0.7265625
train loss:  0.5090060830116272
train gradient:  0.14413044013793197
iteration : 5757
train acc:  0.71875
train loss:  0.5201713442802429
train gradient:  0.1498519863581686
iteration : 5758
train acc:  0.7734375
train loss:  0.4292224049568176
train gradient:  0.12013822951723921
iteration : 5759
train acc:  0.734375
train loss:  0.5532883405685425
train gradient:  0.16773024738326908
iteration : 5760
train acc:  0.7421875
train loss:  0.5066621899604797
train gradient:  0.13585841836793688
iteration : 5761
train acc:  0.71875
train loss:  0.5253302454948425
train gradient:  0.15651234056730884
iteration : 5762
train acc:  0.75
train loss:  0.4857460558414459
train gradient:  0.13660928346187665
iteration : 5763
train acc:  0.7734375
train loss:  0.4593961238861084
train gradient:  0.10120447444214391
iteration : 5764
train acc:  0.703125
train loss:  0.5618054866790771
train gradient:  0.18049030419513717
iteration : 5765
train acc:  0.7109375
train loss:  0.5153911113739014
train gradient:  0.14153346866664737
iteration : 5766
train acc:  0.734375
train loss:  0.4955853819847107
train gradient:  0.11562331398968491
iteration : 5767
train acc:  0.7265625
train loss:  0.5241871476173401
train gradient:  0.2243903456776653
iteration : 5768
train acc:  0.7734375
train loss:  0.4829946756362915
train gradient:  0.12482726269137343
iteration : 5769
train acc:  0.75
train loss:  0.5156652927398682
train gradient:  0.11463931125538865
iteration : 5770
train acc:  0.734375
train loss:  0.4931924045085907
train gradient:  0.11278676098266115
iteration : 5771
train acc:  0.75
train loss:  0.505534291267395
train gradient:  0.13511696610358187
iteration : 5772
train acc:  0.7421875
train loss:  0.5107429027557373
train gradient:  0.13523373807203315
iteration : 5773
train acc:  0.6875
train loss:  0.556418776512146
train gradient:  0.16559687149155405
iteration : 5774
train acc:  0.6875
train loss:  0.5419362783432007
train gradient:  0.14282039619515513
iteration : 5775
train acc:  0.7578125
train loss:  0.4950867295265198
train gradient:  0.12212315476028725
iteration : 5776
train acc:  0.8125
train loss:  0.46056538820266724
train gradient:  0.10584299126257678
iteration : 5777
train acc:  0.7578125
train loss:  0.4506123661994934
train gradient:  0.13214538209448193
iteration : 5778
train acc:  0.828125
train loss:  0.4241664409637451
train gradient:  0.11498151026483897
iteration : 5779
train acc:  0.7421875
train loss:  0.47374212741851807
train gradient:  0.10866246253819117
iteration : 5780
train acc:  0.7265625
train loss:  0.5037392973899841
train gradient:  0.11326264728167122
iteration : 5781
train acc:  0.828125
train loss:  0.42489907145500183
train gradient:  0.12151213294953862
iteration : 5782
train acc:  0.765625
train loss:  0.49073922634124756
train gradient:  0.1631156025212615
iteration : 5783
train acc:  0.765625
train loss:  0.5222514271736145
train gradient:  0.14519201748778437
iteration : 5784
train acc:  0.7578125
train loss:  0.46961432695388794
train gradient:  0.12101994374989902
iteration : 5785
train acc:  0.6796875
train loss:  0.5769725441932678
train gradient:  0.202988744299534
iteration : 5786
train acc:  0.7421875
train loss:  0.48526474833488464
train gradient:  0.15154855954347568
iteration : 5787
train acc:  0.71875
train loss:  0.5385223031044006
train gradient:  0.17871048085589636
iteration : 5788
train acc:  0.78125
train loss:  0.49167436361312866
train gradient:  0.15284327674319093
iteration : 5789
train acc:  0.6953125
train loss:  0.5455806255340576
train gradient:  0.1355429649395288
iteration : 5790
train acc:  0.765625
train loss:  0.43892285227775574
train gradient:  0.1353371616503927
iteration : 5791
train acc:  0.734375
train loss:  0.5575551986694336
train gradient:  0.14910172334133132
iteration : 5792
train acc:  0.703125
train loss:  0.568784773349762
train gradient:  0.2558266603570441
iteration : 5793
train acc:  0.78125
train loss:  0.4585493206977844
train gradient:  0.11757255362804232
iteration : 5794
train acc:  0.71875
train loss:  0.507138729095459
train gradient:  0.16262037805002097
iteration : 5795
train acc:  0.7734375
train loss:  0.4703570008277893
train gradient:  0.14943794690882417
iteration : 5796
train acc:  0.78125
train loss:  0.4511345326900482
train gradient:  0.12287760754791123
iteration : 5797
train acc:  0.734375
train loss:  0.557129442691803
train gradient:  0.16111365024231206
iteration : 5798
train acc:  0.78125
train loss:  0.45070990920066833
train gradient:  0.1435976974787252
iteration : 5799
train acc:  0.7734375
train loss:  0.5051846504211426
train gradient:  0.2378719769567219
iteration : 5800
train acc:  0.7734375
train loss:  0.4887683093547821
train gradient:  0.1268463007034292
iteration : 5801
train acc:  0.7890625
train loss:  0.474163293838501
train gradient:  0.14308895249494402
iteration : 5802
train acc:  0.75
train loss:  0.4572121798992157
train gradient:  0.11258093573451733
iteration : 5803
train acc:  0.7421875
train loss:  0.5358124375343323
train gradient:  0.18291738619544923
iteration : 5804
train acc:  0.7265625
train loss:  0.546442985534668
train gradient:  0.14328087232396283
iteration : 5805
train acc:  0.703125
train loss:  0.5102424025535583
train gradient:  0.1564490960081602
iteration : 5806
train acc:  0.7421875
train loss:  0.466287761926651
train gradient:  0.1505186684813098
iteration : 5807
train acc:  0.7265625
train loss:  0.4575345516204834
train gradient:  0.12136974114699121
iteration : 5808
train acc:  0.7734375
train loss:  0.4556071162223816
train gradient:  0.16007185576147634
iteration : 5809
train acc:  0.734375
train loss:  0.5455287098884583
train gradient:  0.20299017939409036
iteration : 5810
train acc:  0.71875
train loss:  0.5431463122367859
train gradient:  0.1382642188510918
iteration : 5811
train acc:  0.671875
train loss:  0.6081168055534363
train gradient:  0.20165021109311912
iteration : 5812
train acc:  0.7421875
train loss:  0.5214892625808716
train gradient:  0.1606546207883358
iteration : 5813
train acc:  0.703125
train loss:  0.553511381149292
train gradient:  0.1939118515574244
iteration : 5814
train acc:  0.734375
train loss:  0.4893229901790619
train gradient:  0.12507530812891762
iteration : 5815
train acc:  0.734375
train loss:  0.5275144577026367
train gradient:  0.13130005517718826
iteration : 5816
train acc:  0.7265625
train loss:  0.4729914665222168
train gradient:  0.12520812483146376
iteration : 5817
train acc:  0.75
train loss:  0.48255205154418945
train gradient:  0.12297134595153811
iteration : 5818
train acc:  0.71875
train loss:  0.5352806448936462
train gradient:  0.13743910801912065
iteration : 5819
train acc:  0.703125
train loss:  0.5479531288146973
train gradient:  0.18028477828510486
iteration : 5820
train acc:  0.765625
train loss:  0.48278680443763733
train gradient:  0.15941459819481207
iteration : 5821
train acc:  0.7734375
train loss:  0.5158089995384216
train gradient:  0.17275595218472528
iteration : 5822
train acc:  0.6875
train loss:  0.5478557348251343
train gradient:  0.1804039297476512
iteration : 5823
train acc:  0.7578125
train loss:  0.5026054382324219
train gradient:  0.1442194666052536
iteration : 5824
train acc:  0.7890625
train loss:  0.47169312834739685
train gradient:  0.12878435438661068
iteration : 5825
train acc:  0.6640625
train loss:  0.5679399967193604
train gradient:  0.19390028976351834
iteration : 5826
train acc:  0.7421875
train loss:  0.5009446144104004
train gradient:  0.17105817474546098
iteration : 5827
train acc:  0.6953125
train loss:  0.5534581542015076
train gradient:  0.15594509644321664
iteration : 5828
train acc:  0.7578125
train loss:  0.4978867173194885
train gradient:  0.162214111473158
iteration : 5829
train acc:  0.6796875
train loss:  0.5355730056762695
train gradient:  0.13080868193064404
iteration : 5830
train acc:  0.7109375
train loss:  0.49056512117385864
train gradient:  0.14458005440546381
iteration : 5831
train acc:  0.640625
train loss:  0.5772045850753784
train gradient:  0.1872772708283973
iteration : 5832
train acc:  0.75
train loss:  0.5478917360305786
train gradient:  0.16027814079596236
iteration : 5833
train acc:  0.7734375
train loss:  0.5110238194465637
train gradient:  0.13750099694374152
iteration : 5834
train acc:  0.6953125
train loss:  0.5959732532501221
train gradient:  0.20322961780123125
iteration : 5835
train acc:  0.796875
train loss:  0.4276513457298279
train gradient:  0.10975215523801926
iteration : 5836
train acc:  0.765625
train loss:  0.5141647458076477
train gradient:  0.14648030369113696
iteration : 5837
train acc:  0.75
train loss:  0.5090230703353882
train gradient:  0.14507831122205667
iteration : 5838
train acc:  0.8125
train loss:  0.41736310720443726
train gradient:  0.10746105883226811
iteration : 5839
train acc:  0.8046875
train loss:  0.48901471495628357
train gradient:  0.14947408921471475
iteration : 5840
train acc:  0.7109375
train loss:  0.5595191717147827
train gradient:  0.14506969670262432
iteration : 5841
train acc:  0.71875
train loss:  0.522781252861023
train gradient:  0.15808301166979616
iteration : 5842
train acc:  0.7421875
train loss:  0.4850102663040161
train gradient:  0.12350439836758163
iteration : 5843
train acc:  0.734375
train loss:  0.46870583295822144
train gradient:  0.12267969713636069
iteration : 5844
train acc:  0.703125
train loss:  0.545149564743042
train gradient:  0.16384351961889304
iteration : 5845
train acc:  0.734375
train loss:  0.518123209476471
train gradient:  0.16990429573433175
iteration : 5846
train acc:  0.78125
train loss:  0.4617113471031189
train gradient:  0.14194763493432794
iteration : 5847
train acc:  0.765625
train loss:  0.4950374364852905
train gradient:  0.14556062113369225
iteration : 5848
train acc:  0.796875
train loss:  0.472049742937088
train gradient:  0.13057570960376136
iteration : 5849
train acc:  0.7265625
train loss:  0.4989415407180786
train gradient:  0.13944541953045325
iteration : 5850
train acc:  0.8203125
train loss:  0.464601069688797
train gradient:  0.14174226453777541
iteration : 5851
train acc:  0.7265625
train loss:  0.5107346773147583
train gradient:  0.13313568891513228
iteration : 5852
train acc:  0.734375
train loss:  0.4842647910118103
train gradient:  0.16146709782176244
iteration : 5853
train acc:  0.734375
train loss:  0.5551619529724121
train gradient:  0.1720757007797296
iteration : 5854
train acc:  0.765625
train loss:  0.46464091539382935
train gradient:  0.1300545341198137
iteration : 5855
train acc:  0.7578125
train loss:  0.44927021861076355
train gradient:  0.09974852279861716
iteration : 5856
train acc:  0.765625
train loss:  0.472018301486969
train gradient:  0.12340960386386629
iteration : 5857
train acc:  0.6796875
train loss:  0.5431147813796997
train gradient:  0.1399668859777925
iteration : 5858
train acc:  0.7734375
train loss:  0.4408530294895172
train gradient:  0.1568621238658791
iteration : 5859
train acc:  0.75
train loss:  0.4808140695095062
train gradient:  0.12791626820521
iteration : 5860
train acc:  0.7265625
train loss:  0.5968844890594482
train gradient:  0.19945550450473498
iteration : 5861
train acc:  0.6796875
train loss:  0.5976498126983643
train gradient:  0.22386116364098102
iteration : 5862
train acc:  0.6953125
train loss:  0.5322893857955933
train gradient:  0.1443910695641736
iteration : 5863
train acc:  0.765625
train loss:  0.49348148703575134
train gradient:  0.13975226978554994
iteration : 5864
train acc:  0.703125
train loss:  0.4935581088066101
train gradient:  0.14603913716617828
iteration : 5865
train acc:  0.6640625
train loss:  0.5219885110855103
train gradient:  0.133448880166018
iteration : 5866
train acc:  0.7734375
train loss:  0.48200729489326477
train gradient:  0.1672053753239675
iteration : 5867
train acc:  0.703125
train loss:  0.5001399517059326
train gradient:  0.14704581584260545
iteration : 5868
train acc:  0.7265625
train loss:  0.5338460206985474
train gradient:  0.14884071502280832
iteration : 5869
train acc:  0.6875
train loss:  0.5021673440933228
train gradient:  0.1728401316816035
iteration : 5870
train acc:  0.75
train loss:  0.45579248666763306
train gradient:  0.12372955674059803
iteration : 5871
train acc:  0.765625
train loss:  0.4932721257209778
train gradient:  0.12640578222169963
iteration : 5872
train acc:  0.75
train loss:  0.47171565890312195
train gradient:  0.13602514699040918
iteration : 5873
train acc:  0.703125
train loss:  0.5239079594612122
train gradient:  0.15979013186959146
iteration : 5874
train acc:  0.703125
train loss:  0.5935969352722168
train gradient:  0.2316462567065019
iteration : 5875
train acc:  0.7265625
train loss:  0.5304487347602844
train gradient:  0.17892313461664416
iteration : 5876
train acc:  0.734375
train loss:  0.48779940605163574
train gradient:  0.13636669087473502
iteration : 5877
train acc:  0.703125
train loss:  0.5507669448852539
train gradient:  0.15976924289464983
iteration : 5878
train acc:  0.734375
train loss:  0.5196232199668884
train gradient:  0.1605192483890908
iteration : 5879
train acc:  0.703125
train loss:  0.515725314617157
train gradient:  0.14856276087781228
iteration : 5880
train acc:  0.7421875
train loss:  0.5603891611099243
train gradient:  0.17996925826873744
iteration : 5881
train acc:  0.7265625
train loss:  0.5335525870323181
train gradient:  0.17698301443894043
iteration : 5882
train acc:  0.7109375
train loss:  0.5341256856918335
train gradient:  0.1459647305473688
iteration : 5883
train acc:  0.78125
train loss:  0.4415961802005768
train gradient:  0.12008971220249094
iteration : 5884
train acc:  0.6953125
train loss:  0.5254433155059814
train gradient:  0.1584283111057987
iteration : 5885
train acc:  0.734375
train loss:  0.5047529935836792
train gradient:  0.11657859618885476
iteration : 5886
train acc:  0.71875
train loss:  0.5486161708831787
train gradient:  0.17692457493270747
iteration : 5887
train acc:  0.7265625
train loss:  0.5267820358276367
train gradient:  0.12597554901938113
iteration : 5888
train acc:  0.7421875
train loss:  0.4807433485984802
train gradient:  0.12106504013071667
iteration : 5889
train acc:  0.7265625
train loss:  0.517189621925354
train gradient:  0.187206785129659
iteration : 5890
train acc:  0.703125
train loss:  0.527498722076416
train gradient:  0.1595798461124995
iteration : 5891
train acc:  0.765625
train loss:  0.45919954776763916
train gradient:  0.09741537916862374
iteration : 5892
train acc:  0.671875
train loss:  0.50834059715271
train gradient:  0.1494074542310868
iteration : 5893
train acc:  0.71875
train loss:  0.5101348161697388
train gradient:  0.1267650614936829
iteration : 5894
train acc:  0.6640625
train loss:  0.561107873916626
train gradient:  0.17159881784507797
iteration : 5895
train acc:  0.7109375
train loss:  0.5301125645637512
train gradient:  0.164222750142274
iteration : 5896
train acc:  0.71875
train loss:  0.5252357721328735
train gradient:  0.13687205421106502
iteration : 5897
train acc:  0.7734375
train loss:  0.42987188696861267
train gradient:  0.10969860176288983
iteration : 5898
train acc:  0.71875
train loss:  0.507737398147583
train gradient:  0.14058382171682898
iteration : 5899
train acc:  0.6875
train loss:  0.5331169366836548
train gradient:  0.13809588084682994
iteration : 5900
train acc:  0.8046875
train loss:  0.4641859233379364
train gradient:  0.12371268150573876
iteration : 5901
train acc:  0.7890625
train loss:  0.4315241277217865
train gradient:  0.12342079750302527
iteration : 5902
train acc:  0.6640625
train loss:  0.5322912931442261
train gradient:  0.15205319857612765
iteration : 5903
train acc:  0.8125
train loss:  0.45439010858535767
train gradient:  0.11938960529547452
iteration : 5904
train acc:  0.7578125
train loss:  0.4273046851158142
train gradient:  0.12330192422950288
iteration : 5905
train acc:  0.7421875
train loss:  0.5090442895889282
train gradient:  0.18495724635554034
iteration : 5906
train acc:  0.7265625
train loss:  0.46658873558044434
train gradient:  0.11074478285120959
iteration : 5907
train acc:  0.734375
train loss:  0.47686219215393066
train gradient:  0.14487077156128275
iteration : 5908
train acc:  0.71875
train loss:  0.46211475133895874
train gradient:  0.11931034504092883
iteration : 5909
train acc:  0.7421875
train loss:  0.5046384334564209
train gradient:  0.13769424608405184
iteration : 5910
train acc:  0.71875
train loss:  0.511542797088623
train gradient:  0.16978988595066208
iteration : 5911
train acc:  0.765625
train loss:  0.5031543970108032
train gradient:  0.15795118473792266
iteration : 5912
train acc:  0.7421875
train loss:  0.4900367259979248
train gradient:  0.11965104700244064
iteration : 5913
train acc:  0.75
train loss:  0.491754949092865
train gradient:  0.10354164203436081
iteration : 5914
train acc:  0.671875
train loss:  0.572838544845581
train gradient:  0.15568187368494651
iteration : 5915
train acc:  0.7890625
train loss:  0.46743836998939514
train gradient:  0.13899132895331395
iteration : 5916
train acc:  0.7421875
train loss:  0.49374741315841675
train gradient:  0.12718241294646165
iteration : 5917
train acc:  0.75
train loss:  0.4709281325340271
train gradient:  0.11994648425891943
iteration : 5918
train acc:  0.6953125
train loss:  0.6034523248672485
train gradient:  0.18767277980403926
iteration : 5919
train acc:  0.8125
train loss:  0.42208927869796753
train gradient:  0.10389638354196816
iteration : 5920
train acc:  0.7578125
train loss:  0.592535674571991
train gradient:  0.17870140166836762
iteration : 5921
train acc:  0.734375
train loss:  0.5505471229553223
train gradient:  0.19387391628181366
iteration : 5922
train acc:  0.75
train loss:  0.5058068037033081
train gradient:  0.13791367042827585
iteration : 5923
train acc:  0.6796875
train loss:  0.5085195302963257
train gradient:  0.12225579048197031
iteration : 5924
train acc:  0.703125
train loss:  0.5214951634407043
train gradient:  0.1314858691003173
iteration : 5925
train acc:  0.765625
train loss:  0.5056003928184509
train gradient:  0.15884277477524295
iteration : 5926
train acc:  0.75
train loss:  0.5030645132064819
train gradient:  0.13933097287368718
iteration : 5927
train acc:  0.7421875
train loss:  0.4639081656932831
train gradient:  0.11114791908182364
iteration : 5928
train acc:  0.7890625
train loss:  0.4465746283531189
train gradient:  0.11780499713756798
iteration : 5929
train acc:  0.7578125
train loss:  0.47784489393234253
train gradient:  0.12242408514823529
iteration : 5930
train acc:  0.7421875
train loss:  0.5566126108169556
train gradient:  0.17157487120048187
iteration : 5931
train acc:  0.734375
train loss:  0.4759002923965454
train gradient:  0.14390858401855378
iteration : 5932
train acc:  0.75
train loss:  0.5247159004211426
train gradient:  0.1675070935031268
iteration : 5933
train acc:  0.7265625
train loss:  0.46562501788139343
train gradient:  0.1142605101885321
iteration : 5934
train acc:  0.6328125
train loss:  0.5826472640037537
train gradient:  0.16193687050350467
iteration : 5935
train acc:  0.734375
train loss:  0.5187499523162842
train gradient:  0.15534780408055615
iteration : 5936
train acc:  0.8125
train loss:  0.4688781499862671
train gradient:  0.1695506088082619
iteration : 5937
train acc:  0.671875
train loss:  0.5184531211853027
train gradient:  0.17674865860467343
iteration : 5938
train acc:  0.7734375
train loss:  0.4960530400276184
train gradient:  0.14964918670373945
iteration : 5939
train acc:  0.7734375
train loss:  0.5327721238136292
train gradient:  0.15878392630173332
iteration : 5940
train acc:  0.75
train loss:  0.539775550365448
train gradient:  0.1566899929175214
iteration : 5941
train acc:  0.75
train loss:  0.46255823969841003
train gradient:  0.11196632980573162
iteration : 5942
train acc:  0.71875
train loss:  0.5114946365356445
train gradient:  0.13278388712406117
iteration : 5943
train acc:  0.7265625
train loss:  0.5331358313560486
train gradient:  0.15710116469359492
iteration : 5944
train acc:  0.734375
train loss:  0.48935502767562866
train gradient:  0.1290397438716015
iteration : 5945
train acc:  0.7265625
train loss:  0.4755701422691345
train gradient:  0.1328698739468369
iteration : 5946
train acc:  0.7421875
train loss:  0.4335092306137085
train gradient:  0.09849702346662476
iteration : 5947
train acc:  0.6953125
train loss:  0.5523557662963867
train gradient:  0.16537022052705114
iteration : 5948
train acc:  0.7265625
train loss:  0.5305275321006775
train gradient:  0.17905440638243386
iteration : 5949
train acc:  0.7421875
train loss:  0.5304052829742432
train gradient:  0.1495465967310876
iteration : 5950
train acc:  0.65625
train loss:  0.5534303188323975
train gradient:  0.13934127546581696
iteration : 5951
train acc:  0.6875
train loss:  0.5835952758789062
train gradient:  0.15795604333483965
iteration : 5952
train acc:  0.7421875
train loss:  0.5042675733566284
train gradient:  0.13072634679087558
iteration : 5953
train acc:  0.7265625
train loss:  0.5244665145874023
train gradient:  0.14770124145980895
iteration : 5954
train acc:  0.71875
train loss:  0.5361381769180298
train gradient:  0.1442142432257359
iteration : 5955
train acc:  0.7578125
train loss:  0.5102713108062744
train gradient:  0.15709636002233435
iteration : 5956
train acc:  0.6640625
train loss:  0.5067847967147827
train gradient:  0.1553847153147458
iteration : 5957
train acc:  0.6953125
train loss:  0.5328902006149292
train gradient:  0.21880981271844646
iteration : 5958
train acc:  0.71875
train loss:  0.5106176137924194
train gradient:  0.16344022416050025
iteration : 5959
train acc:  0.7578125
train loss:  0.48916029930114746
train gradient:  0.15545649777334652
iteration : 5960
train acc:  0.828125
train loss:  0.4422270655632019
train gradient:  0.16585314702616777
iteration : 5961
train acc:  0.671875
train loss:  0.5546232461929321
train gradient:  0.16424264586920645
iteration : 5962
train acc:  0.7890625
train loss:  0.456529438495636
train gradient:  0.12819696228023175
iteration : 5963
train acc:  0.7734375
train loss:  0.5102826356887817
train gradient:  0.16407497752952238
iteration : 5964
train acc:  0.78125
train loss:  0.4575963020324707
train gradient:  0.10634575874351329
iteration : 5965
train acc:  0.734375
train loss:  0.4586273729801178
train gradient:  0.12090322814906311
iteration : 5966
train acc:  0.7109375
train loss:  0.544059157371521
train gradient:  0.21477964041253222
iteration : 5967
train acc:  0.75
train loss:  0.4870629906654358
train gradient:  0.15347160970834922
iteration : 5968
train acc:  0.734375
train loss:  0.5257257223129272
train gradient:  0.15941642071622386
iteration : 5969
train acc:  0.6953125
train loss:  0.5238404273986816
train gradient:  0.1408615398056195
iteration : 5970
train acc:  0.7109375
train loss:  0.4980328679084778
train gradient:  0.10861617893218335
iteration : 5971
train acc:  0.65625
train loss:  0.540489912033081
train gradient:  0.15918837960289475
iteration : 5972
train acc:  0.7578125
train loss:  0.44688576459884644
train gradient:  0.14367677607182755
iteration : 5973
train acc:  0.6875
train loss:  0.5713678598403931
train gradient:  0.14022583503848068
iteration : 5974
train acc:  0.6875
train loss:  0.562019944190979
train gradient:  0.14229401092809973
iteration : 5975
train acc:  0.6875
train loss:  0.5438865423202515
train gradient:  0.1761700548253755
iteration : 5976
train acc:  0.7265625
train loss:  0.4632309675216675
train gradient:  0.10960042148204956
iteration : 5977
train acc:  0.78125
train loss:  0.4490036070346832
train gradient:  0.12408763296379978
iteration : 5978
train acc:  0.6875
train loss:  0.5834661722183228
train gradient:  0.2411326614913479
iteration : 5979
train acc:  0.703125
train loss:  0.544346809387207
train gradient:  0.1459139971087085
iteration : 5980
train acc:  0.6796875
train loss:  0.568841814994812
train gradient:  0.17672083189574267
iteration : 5981
train acc:  0.6875
train loss:  0.559744119644165
train gradient:  0.2091193409885157
iteration : 5982
train acc:  0.609375
train loss:  0.6072725057601929
train gradient:  0.20574190956068752
iteration : 5983
train acc:  0.7421875
train loss:  0.5258586406707764
train gradient:  0.18631944872703698
iteration : 5984
train acc:  0.7421875
train loss:  0.4758649468421936
train gradient:  0.12158443569277944
iteration : 5985
train acc:  0.7265625
train loss:  0.5227668285369873
train gradient:  0.12034015165425609
iteration : 5986
train acc:  0.6953125
train loss:  0.5488053560256958
train gradient:  0.14934609728953305
iteration : 5987
train acc:  0.6796875
train loss:  0.527603030204773
train gradient:  0.14916570784568797
iteration : 5988
train acc:  0.78125
train loss:  0.4610413908958435
train gradient:  0.11883826614620192
iteration : 5989
train acc:  0.7578125
train loss:  0.5065725445747375
train gradient:  0.15166779685495074
iteration : 5990
train acc:  0.6953125
train loss:  0.5189084410667419
train gradient:  0.12988801808469783
iteration : 5991
train acc:  0.7578125
train loss:  0.509503960609436
train gradient:  0.13000948503129756
iteration : 5992
train acc:  0.71875
train loss:  0.45091408491134644
train gradient:  0.09762785766623544
iteration : 5993
train acc:  0.7890625
train loss:  0.47513341903686523
train gradient:  0.12182527161440561
iteration : 5994
train acc:  0.765625
train loss:  0.4824160933494568
train gradient:  0.15438324252955127
iteration : 5995
train acc:  0.6875
train loss:  0.5985205173492432
train gradient:  0.1863589694090289
iteration : 5996
train acc:  0.7421875
train loss:  0.5016567707061768
train gradient:  0.13792920348174942
iteration : 5997
train acc:  0.71875
train loss:  0.5985698699951172
train gradient:  0.17371932738622203
iteration : 5998
train acc:  0.6875
train loss:  0.518822968006134
train gradient:  0.12714404078951547
iteration : 5999
train acc:  0.7265625
train loss:  0.5456059575080872
train gradient:  0.1851853904667213
iteration : 6000
train acc:  0.7265625
train loss:  0.4890832304954529
train gradient:  0.12247239289699842
iteration : 6001
train acc:  0.78125
train loss:  0.5291742086410522
train gradient:  0.15562588821783113
iteration : 6002
train acc:  0.75
train loss:  0.4969959259033203
train gradient:  0.19007475648040756
iteration : 6003
train acc:  0.75
train loss:  0.45642343163490295
train gradient:  0.09598315481205769
iteration : 6004
train acc:  0.65625
train loss:  0.5556922554969788
train gradient:  0.2079972544190653
iteration : 6005
train acc:  0.6953125
train loss:  0.561081051826477
train gradient:  0.16774121657077545
iteration : 6006
train acc:  0.75
train loss:  0.500622034072876
train gradient:  0.15024606244489885
iteration : 6007
train acc:  0.71875
train loss:  0.5300027132034302
train gradient:  0.1391364505187464
iteration : 6008
train acc:  0.6953125
train loss:  0.4982230067253113
train gradient:  0.12156041386813465
iteration : 6009
train acc:  0.6953125
train loss:  0.5826395750045776
train gradient:  0.160339860620513
iteration : 6010
train acc:  0.703125
train loss:  0.5550740361213684
train gradient:  0.18850505940224233
iteration : 6011
train acc:  0.8046875
train loss:  0.5049355626106262
train gradient:  0.16446175810493602
iteration : 6012
train acc:  0.75
train loss:  0.5177656412124634
train gradient:  0.15336495033840844
iteration : 6013
train acc:  0.78125
train loss:  0.4821004569530487
train gradient:  0.12377382959512753
iteration : 6014
train acc:  0.75
train loss:  0.502637505531311
train gradient:  0.19595942479932105
iteration : 6015
train acc:  0.7578125
train loss:  0.5185344815254211
train gradient:  0.1555443906879751
iteration : 6016
train acc:  0.71875
train loss:  0.49369627237319946
train gradient:  0.17467757655736638
iteration : 6017
train acc:  0.7578125
train loss:  0.47407588362693787
train gradient:  0.12998444276633586
iteration : 6018
train acc:  0.7265625
train loss:  0.4980936050415039
train gradient:  0.14659525418128388
iteration : 6019
train acc:  0.7421875
train loss:  0.4737464189529419
train gradient:  0.13855801554298008
iteration : 6020
train acc:  0.6796875
train loss:  0.5136668086051941
train gradient:  0.11210883726470015
iteration : 6021
train acc:  0.6796875
train loss:  0.502349317073822
train gradient:  0.15119777579095442
iteration : 6022
train acc:  0.7265625
train loss:  0.5084870457649231
train gradient:  0.16944056395545104
iteration : 6023
train acc:  0.703125
train loss:  0.46171343326568604
train gradient:  0.12388772325591622
iteration : 6024
train acc:  0.71875
train loss:  0.5367791652679443
train gradient:  0.18657120375460462
iteration : 6025
train acc:  0.703125
train loss:  0.5383801460266113
train gradient:  0.16837766000187326
iteration : 6026
train acc:  0.703125
train loss:  0.5537764430046082
train gradient:  0.20853613394237142
iteration : 6027
train acc:  0.6328125
train loss:  0.5818058252334595
train gradient:  0.17817387019125502
iteration : 6028
train acc:  0.765625
train loss:  0.4751242995262146
train gradient:  0.11267396639649507
iteration : 6029
train acc:  0.765625
train loss:  0.49128806591033936
train gradient:  0.1308250053069338
iteration : 6030
train acc:  0.6796875
train loss:  0.5781005620956421
train gradient:  0.23418007800388213
iteration : 6031
train acc:  0.71875
train loss:  0.4828580915927887
train gradient:  0.12191844857007353
iteration : 6032
train acc:  0.71875
train loss:  0.5082240700721741
train gradient:  0.16792156204434383
iteration : 6033
train acc:  0.7578125
train loss:  0.4970654547214508
train gradient:  0.1401012649021297
iteration : 6034
train acc:  0.71875
train loss:  0.5251457691192627
train gradient:  0.1672367126381475
iteration : 6035
train acc:  0.765625
train loss:  0.493815541267395
train gradient:  0.12294789264105271
iteration : 6036
train acc:  0.6875
train loss:  0.5374810099601746
train gradient:  0.15749260922202563
iteration : 6037
train acc:  0.75
train loss:  0.46418604254722595
train gradient:  0.1138844716874565
iteration : 6038
train acc:  0.6953125
train loss:  0.5208432674407959
train gradient:  0.15338464660387646
iteration : 6039
train acc:  0.6953125
train loss:  0.5607826709747314
train gradient:  0.1519250624481367
iteration : 6040
train acc:  0.7109375
train loss:  0.549828827381134
train gradient:  0.1796116551479931
iteration : 6041
train acc:  0.71875
train loss:  0.5417265295982361
train gradient:  0.1599722739753422
iteration : 6042
train acc:  0.7109375
train loss:  0.5135797262191772
train gradient:  0.14869256290289373
iteration : 6043
train acc:  0.7578125
train loss:  0.4782406985759735
train gradient:  0.1334798557767537
iteration : 6044
train acc:  0.7265625
train loss:  0.5351306796073914
train gradient:  0.17722435829407962
iteration : 6045
train acc:  0.71875
train loss:  0.4752744138240814
train gradient:  0.11021541707381229
iteration : 6046
train acc:  0.75
train loss:  0.47624027729034424
train gradient:  0.09985788285362
iteration : 6047
train acc:  0.7109375
train loss:  0.5254417061805725
train gradient:  0.17207180297973812
iteration : 6048
train acc:  0.6953125
train loss:  0.5074357986450195
train gradient:  0.11551743256938385
iteration : 6049
train acc:  0.71875
train loss:  0.5236707925796509
train gradient:  0.17477354365165446
iteration : 6050
train acc:  0.734375
train loss:  0.5428462028503418
train gradient:  0.1589574192401647
iteration : 6051
train acc:  0.71875
train loss:  0.5285769701004028
train gradient:  0.1514821792818452
iteration : 6052
train acc:  0.7265625
train loss:  0.5446417331695557
train gradient:  0.24256832957551033
iteration : 6053
train acc:  0.734375
train loss:  0.5020561814308167
train gradient:  0.1134229743642931
iteration : 6054
train acc:  0.75
train loss:  0.46336477994918823
train gradient:  0.1204760602555492
iteration : 6055
train acc:  0.703125
train loss:  0.6441488265991211
train gradient:  0.21565495488610664
iteration : 6056
train acc:  0.78125
train loss:  0.4455608129501343
train gradient:  0.09539212527515016
iteration : 6057
train acc:  0.65625
train loss:  0.5910274386405945
train gradient:  0.22726534277269045
iteration : 6058
train acc:  0.765625
train loss:  0.503106951713562
train gradient:  0.11249883503385916
iteration : 6059
train acc:  0.7734375
train loss:  0.4742221534252167
train gradient:  0.1110620198909538
iteration : 6060
train acc:  0.7578125
train loss:  0.4886430501937866
train gradient:  0.12110269984407171
iteration : 6061
train acc:  0.7265625
train loss:  0.5337612628936768
train gradient:  0.13647179081846872
iteration : 6062
train acc:  0.6953125
train loss:  0.49855655431747437
train gradient:  0.15387806741848653
iteration : 6063
train acc:  0.78125
train loss:  0.44761228561401367
train gradient:  0.10229489157140303
iteration : 6064
train acc:  0.7265625
train loss:  0.509793758392334
train gradient:  0.16010716608432754
iteration : 6065
train acc:  0.6953125
train loss:  0.5097881555557251
train gradient:  0.13237452290907842
iteration : 6066
train acc:  0.6875
train loss:  0.555942177772522
train gradient:  0.13875151202361113
iteration : 6067
train acc:  0.8046875
train loss:  0.45700252056121826
train gradient:  0.14653509049433588
iteration : 6068
train acc:  0.7578125
train loss:  0.44137468934059143
train gradient:  0.09988887115372136
iteration : 6069
train acc:  0.7265625
train loss:  0.49161118268966675
train gradient:  0.12779186340684406
iteration : 6070
train acc:  0.7890625
train loss:  0.5201507210731506
train gradient:  0.19266628668680613
iteration : 6071
train acc:  0.7421875
train loss:  0.4795553982257843
train gradient:  0.13827016872388198
iteration : 6072
train acc:  0.7734375
train loss:  0.5062568187713623
train gradient:  0.13936034573524161
iteration : 6073
train acc:  0.7421875
train loss:  0.4950983226299286
train gradient:  0.11844192049722699
iteration : 6074
train acc:  0.765625
train loss:  0.4774686098098755
train gradient:  0.13237266770392223
iteration : 6075
train acc:  0.734375
train loss:  0.44948825240135193
train gradient:  0.09340823254712065
iteration : 6076
train acc:  0.734375
train loss:  0.482876718044281
train gradient:  0.10541214296511238
iteration : 6077
train acc:  0.7109375
train loss:  0.5184443593025208
train gradient:  0.1552591581491527
iteration : 6078
train acc:  0.734375
train loss:  0.49502646923065186
train gradient:  0.13240937738798975
iteration : 6079
train acc:  0.671875
train loss:  0.5563356876373291
train gradient:  0.17052503794845397
iteration : 6080
train acc:  0.75
train loss:  0.50613933801651
train gradient:  0.14726257445295432
iteration : 6081
train acc:  0.6875
train loss:  0.5490517616271973
train gradient:  0.2389966752518248
iteration : 6082
train acc:  0.734375
train loss:  0.5562267303466797
train gradient:  0.18805950272496785
iteration : 6083
train acc:  0.7265625
train loss:  0.49821311235427856
train gradient:  0.12369638512398719
iteration : 6084
train acc:  0.8125
train loss:  0.45445919036865234
train gradient:  0.12598900384086142
iteration : 6085
train acc:  0.78125
train loss:  0.4208250045776367
train gradient:  0.09611563685016186
iteration : 6086
train acc:  0.671875
train loss:  0.5772673487663269
train gradient:  0.19155399455815147
iteration : 6087
train acc:  0.7265625
train loss:  0.4998866617679596
train gradient:  0.1743228248211694
iteration : 6088
train acc:  0.765625
train loss:  0.48488059639930725
train gradient:  0.1238451807851729
iteration : 6089
train acc:  0.625
train loss:  0.5789842009544373
train gradient:  0.15927130531538156
iteration : 6090
train acc:  0.75
train loss:  0.4823387861251831
train gradient:  0.11553031344881916
iteration : 6091
train acc:  0.734375
train loss:  0.5296201109886169
train gradient:  0.16078845367652633
iteration : 6092
train acc:  0.734375
train loss:  0.5031400322914124
train gradient:  0.1435825781072169
iteration : 6093
train acc:  0.78125
train loss:  0.42861273884773254
train gradient:  0.1240152572346165
iteration : 6094
train acc:  0.7578125
train loss:  0.5397762656211853
train gradient:  0.14205251012260758
iteration : 6095
train acc:  0.765625
train loss:  0.4533272683620453
train gradient:  0.10704363981605469
iteration : 6096
train acc:  0.7890625
train loss:  0.4547569751739502
train gradient:  0.13346639221284928
iteration : 6097
train acc:  0.7890625
train loss:  0.47227469086647034
train gradient:  0.1207157339616459
iteration : 6098
train acc:  0.828125
train loss:  0.42349812388420105
train gradient:  0.12828046281935118
iteration : 6099
train acc:  0.7109375
train loss:  0.5410171747207642
train gradient:  0.1769077617561592
iteration : 6100
train acc:  0.7734375
train loss:  0.4891093373298645
train gradient:  0.1109272026136492
iteration : 6101
train acc:  0.7578125
train loss:  0.46638262271881104
train gradient:  0.09930886238904159
iteration : 6102
train acc:  0.7890625
train loss:  0.4776012897491455
train gradient:  0.13290503295739148
iteration : 6103
train acc:  0.7421875
train loss:  0.46192437410354614
train gradient:  0.13674824365449795
iteration : 6104
train acc:  0.71875
train loss:  0.49938100576400757
train gradient:  0.14067130314160226
iteration : 6105
train acc:  0.6953125
train loss:  0.5194219946861267
train gradient:  0.15590922997147544
iteration : 6106
train acc:  0.6484375
train loss:  0.5972238779067993
train gradient:  0.15994739779465827
iteration : 6107
train acc:  0.7578125
train loss:  0.4833019971847534
train gradient:  0.163162274339624
iteration : 6108
train acc:  0.65625
train loss:  0.6212925910949707
train gradient:  0.2952049153713265
iteration : 6109
train acc:  0.7734375
train loss:  0.49145129323005676
train gradient:  0.14107607489525714
iteration : 6110
train acc:  0.7265625
train loss:  0.4732770025730133
train gradient:  0.1288319428637354
iteration : 6111
train acc:  0.765625
train loss:  0.47115427255630493
train gradient:  0.11248278046866372
iteration : 6112
train acc:  0.703125
train loss:  0.522327184677124
train gradient:  0.18076779602828025
iteration : 6113
train acc:  0.703125
train loss:  0.5325363874435425
train gradient:  0.15742393824050988
iteration : 6114
train acc:  0.703125
train loss:  0.5487689971923828
train gradient:  0.14088045554304932
iteration : 6115
train acc:  0.6875
train loss:  0.5471864342689514
train gradient:  0.16927717406529041
iteration : 6116
train acc:  0.7265625
train loss:  0.5134434700012207
train gradient:  0.16938361867876162
iteration : 6117
train acc:  0.796875
train loss:  0.45430582761764526
train gradient:  0.11147776747068114
iteration : 6118
train acc:  0.65625
train loss:  0.6102570295333862
train gradient:  0.20746608040785475
iteration : 6119
train acc:  0.78125
train loss:  0.43546193838119507
train gradient:  0.12547716624175737
iteration : 6120
train acc:  0.7109375
train loss:  0.5364638566970825
train gradient:  0.15605458777952091
iteration : 6121
train acc:  0.6796875
train loss:  0.5407788753509521
train gradient:  0.1580471337161528
iteration : 6122
train acc:  0.65625
train loss:  0.5836089849472046
train gradient:  0.2266362863355882
iteration : 6123
train acc:  0.734375
train loss:  0.5119960308074951
train gradient:  0.13652345016254497
iteration : 6124
train acc:  0.71875
train loss:  0.5085837841033936
train gradient:  0.19200801828676134
iteration : 6125
train acc:  0.7734375
train loss:  0.5086183547973633
train gradient:  0.16309458589852244
iteration : 6126
train acc:  0.7421875
train loss:  0.4681588411331177
train gradient:  0.16121382217774582
iteration : 6127
train acc:  0.734375
train loss:  0.5187814235687256
train gradient:  0.12911208838107968
iteration : 6128
train acc:  0.71875
train loss:  0.5054010152816772
train gradient:  0.15174392157855943
iteration : 6129
train acc:  0.7265625
train loss:  0.5208624005317688
train gradient:  0.14371301106708753
iteration : 6130
train acc:  0.7109375
train loss:  0.5332354307174683
train gradient:  0.14839226394409372
iteration : 6131
train acc:  0.734375
train loss:  0.5059133172035217
train gradient:  0.14323451497550305
iteration : 6132
train acc:  0.8046875
train loss:  0.42572513222694397
train gradient:  0.1125744446678802
iteration : 6133
train acc:  0.7734375
train loss:  0.4460928440093994
train gradient:  0.10538809834087307
iteration : 6134
train acc:  0.78125
train loss:  0.47050997614860535
train gradient:  0.11443545320522204
iteration : 6135
train acc:  0.71875
train loss:  0.527401328086853
train gradient:  0.11994770537257451
iteration : 6136
train acc:  0.765625
train loss:  0.49311840534210205
train gradient:  0.1402531831333702
iteration : 6137
train acc:  0.71875
train loss:  0.5063760280609131
train gradient:  0.15153809209668986
iteration : 6138
train acc:  0.765625
train loss:  0.5305622220039368
train gradient:  0.14214576330965323
iteration : 6139
train acc:  0.703125
train loss:  0.5119463205337524
train gradient:  0.13115420722548576
iteration : 6140
train acc:  0.7578125
train loss:  0.5385316610336304
train gradient:  0.1623078182605228
iteration : 6141
train acc:  0.71875
train loss:  0.5478014945983887
train gradient:  0.19532232114355905
iteration : 6142
train acc:  0.7265625
train loss:  0.5115498900413513
train gradient:  0.23513813689312124
iteration : 6143
train acc:  0.765625
train loss:  0.4672665297985077
train gradient:  0.10481564936717142
iteration : 6144
train acc:  0.7109375
train loss:  0.5039362907409668
train gradient:  0.1201863098615914
iteration : 6145
train acc:  0.8046875
train loss:  0.46076589822769165
train gradient:  0.11426358404589909
iteration : 6146
train acc:  0.734375
train loss:  0.5286544561386108
train gradient:  0.1283533180694319
iteration : 6147
train acc:  0.7890625
train loss:  0.46394574642181396
train gradient:  0.10897574499866776
iteration : 6148
train acc:  0.7890625
train loss:  0.4514002799987793
train gradient:  0.09736957881974623
iteration : 6149
train acc:  0.7421875
train loss:  0.4963434338569641
train gradient:  0.13560253979674866
iteration : 6150
train acc:  0.7421875
train loss:  0.5320277214050293
train gradient:  0.17484916054589217
iteration : 6151
train acc:  0.7421875
train loss:  0.5376768708229065
train gradient:  0.20383835841103104
iteration : 6152
train acc:  0.796875
train loss:  0.42805957794189453
train gradient:  0.10935094571554578
iteration : 6153
train acc:  0.859375
train loss:  0.44669777154922485
train gradient:  0.08961620357571212
iteration : 6154
train acc:  0.6953125
train loss:  0.5400494933128357
train gradient:  0.12889993209813183
iteration : 6155
train acc:  0.6640625
train loss:  0.6036806106567383
train gradient:  0.2226287391069917
iteration : 6156
train acc:  0.7734375
train loss:  0.5057837963104248
train gradient:  0.11927592577247263
iteration : 6157
train acc:  0.6875
train loss:  0.5613505840301514
train gradient:  0.154390470330966
iteration : 6158
train acc:  0.78125
train loss:  0.4811377227306366
train gradient:  0.1440795404445452
iteration : 6159
train acc:  0.6875
train loss:  0.5888556838035583
train gradient:  0.14848111019952165
iteration : 6160
train acc:  0.6640625
train loss:  0.5423527956008911
train gradient:  0.18516567901380823
iteration : 6161
train acc:  0.6484375
train loss:  0.6004699468612671
train gradient:  0.17432067919332905
iteration : 6162
train acc:  0.765625
train loss:  0.4835411310195923
train gradient:  0.14082827295883577
iteration : 6163
train acc:  0.765625
train loss:  0.4530954360961914
train gradient:  0.11512747720697289
iteration : 6164
train acc:  0.7109375
train loss:  0.5628699660301208
train gradient:  0.13790996881467749
iteration : 6165
train acc:  0.7109375
train loss:  0.48865386843681335
train gradient:  0.12271443355572738
iteration : 6166
train acc:  0.796875
train loss:  0.4744586944580078
train gradient:  0.14321187230469723
iteration : 6167
train acc:  0.71875
train loss:  0.46941059827804565
train gradient:  0.10080092459154422
iteration : 6168
train acc:  0.765625
train loss:  0.48585647344589233
train gradient:  0.12561279304504097
iteration : 6169
train acc:  0.71875
train loss:  0.48685768246650696
train gradient:  0.10033482587184803
iteration : 6170
train acc:  0.6953125
train loss:  0.4959228038787842
train gradient:  0.12371611986383127
iteration : 6171
train acc:  0.765625
train loss:  0.45822638273239136
train gradient:  0.10083135371486973
iteration : 6172
train acc:  0.7265625
train loss:  0.5160119533538818
train gradient:  0.1579992947107236
iteration : 6173
train acc:  0.7109375
train loss:  0.5708281993865967
train gradient:  0.20065509956584127
iteration : 6174
train acc:  0.71875
train loss:  0.5113323330879211
train gradient:  0.13899971012756385
iteration : 6175
train acc:  0.78125
train loss:  0.455302357673645
train gradient:  0.11900123612113227
iteration : 6176
train acc:  0.8359375
train loss:  0.4040730595588684
train gradient:  0.11314569413833393
iteration : 6177
train acc:  0.671875
train loss:  0.5267118215560913
train gradient:  0.13089561657708076
iteration : 6178
train acc:  0.7109375
train loss:  0.5519654154777527
train gradient:  0.1399976173521385
iteration : 6179
train acc:  0.6953125
train loss:  0.5527356863021851
train gradient:  0.15265354320060787
iteration : 6180
train acc:  0.6640625
train loss:  0.5879843235015869
train gradient:  0.162797401377522
iteration : 6181
train acc:  0.6875
train loss:  0.586788535118103
train gradient:  0.18759399932163767
iteration : 6182
train acc:  0.7734375
train loss:  0.5278936624526978
train gradient:  0.14614380412050426
iteration : 6183
train acc:  0.71875
train loss:  0.531212329864502
train gradient:  0.1542742268628375
iteration : 6184
train acc:  0.78125
train loss:  0.4310925006866455
train gradient:  0.10945407979254389
iteration : 6185
train acc:  0.734375
train loss:  0.47385257482528687
train gradient:  0.12890684931386157
iteration : 6186
train acc:  0.7109375
train loss:  0.4999844431877136
train gradient:  0.17673626394159347
iteration : 6187
train acc:  0.7890625
train loss:  0.4424489736557007
train gradient:  0.1105556084498331
iteration : 6188
train acc:  0.71875
train loss:  0.5007293224334717
train gradient:  0.12061926155177607
iteration : 6189
train acc:  0.7734375
train loss:  0.4679540991783142
train gradient:  0.13875359107223922
iteration : 6190
train acc:  0.8046875
train loss:  0.46484795212745667
train gradient:  0.1058901295112976
iteration : 6191
train acc:  0.7578125
train loss:  0.4780583083629608
train gradient:  0.15287406897625863
iteration : 6192
train acc:  0.7578125
train loss:  0.49117806553840637
train gradient:  0.11764756912070472
iteration : 6193
train acc:  0.71875
train loss:  0.5247584581375122
train gradient:  0.1337901975557728
iteration : 6194
train acc:  0.75
train loss:  0.49683111906051636
train gradient:  0.14709207572346383
iteration : 6195
train acc:  0.8046875
train loss:  0.4422663450241089
train gradient:  0.13550604040104533
iteration : 6196
train acc:  0.71875
train loss:  0.5292693376541138
train gradient:  0.12107270083796737
iteration : 6197
train acc:  0.75
train loss:  0.46998628973960876
train gradient:  0.15943670760377374
iteration : 6198
train acc:  0.765625
train loss:  0.47643959522247314
train gradient:  0.12674337686623813
iteration : 6199
train acc:  0.7734375
train loss:  0.4903055429458618
train gradient:  0.10608116013328509
iteration : 6200
train acc:  0.6953125
train loss:  0.5378770232200623
train gradient:  0.13840806408434247
iteration : 6201
train acc:  0.671875
train loss:  0.5531297922134399
train gradient:  0.20347775267364657
iteration : 6202
train acc:  0.640625
train loss:  0.611495316028595
train gradient:  0.22609850367506282
iteration : 6203
train acc:  0.7421875
train loss:  0.5102694034576416
train gradient:  0.1622817530704504
iteration : 6204
train acc:  0.71875
train loss:  0.5264338254928589
train gradient:  0.1590462324354503
iteration : 6205
train acc:  0.734375
train loss:  0.5165576934814453
train gradient:  0.10882619064251127
iteration : 6206
train acc:  0.75
train loss:  0.4473271369934082
train gradient:  0.12000276438415561
iteration : 6207
train acc:  0.765625
train loss:  0.4616295397281647
train gradient:  0.14602788005312667
iteration : 6208
train acc:  0.6796875
train loss:  0.5499373078346252
train gradient:  0.17100559365667295
iteration : 6209
train acc:  0.78125
train loss:  0.4852462410926819
train gradient:  0.13937026852075607
iteration : 6210
train acc:  0.71875
train loss:  0.509976863861084
train gradient:  0.14740436976641053
iteration : 6211
train acc:  0.7421875
train loss:  0.4532202184200287
train gradient:  0.0864254172147835
iteration : 6212
train acc:  0.7265625
train loss:  0.5055276155471802
train gradient:  0.15139974987591268
iteration : 6213
train acc:  0.828125
train loss:  0.4386916160583496
train gradient:  0.0971200537843492
iteration : 6214
train acc:  0.7265625
train loss:  0.5360164046287537
train gradient:  0.19464721455041073
iteration : 6215
train acc:  0.7421875
train loss:  0.5492582321166992
train gradient:  0.2822282250214865
iteration : 6216
train acc:  0.8046875
train loss:  0.4252839684486389
train gradient:  0.11053107194453843
iteration : 6217
train acc:  0.75
train loss:  0.48873138427734375
train gradient:  0.1345298544580109
iteration : 6218
train acc:  0.765625
train loss:  0.4455960988998413
train gradient:  0.12362666626359481
iteration : 6219
train acc:  0.734375
train loss:  0.509804368019104
train gradient:  0.14456545760666836
iteration : 6220
train acc:  0.71875
train loss:  0.5415193438529968
train gradient:  0.14603484153966
iteration : 6221
train acc:  0.7109375
train loss:  0.4972739815711975
train gradient:  0.1573024553331182
iteration : 6222
train acc:  0.75
train loss:  0.5031569600105286
train gradient:  0.17896984932875554
iteration : 6223
train acc:  0.734375
train loss:  0.5161023139953613
train gradient:  0.15083389551887982
iteration : 6224
train acc:  0.7109375
train loss:  0.5368491411209106
train gradient:  0.17306706289364526
iteration : 6225
train acc:  0.6875
train loss:  0.5767350196838379
train gradient:  0.1402861538513942
iteration : 6226
train acc:  0.7578125
train loss:  0.494655579328537
train gradient:  0.12724487088558378
iteration : 6227
train acc:  0.7421875
train loss:  0.5289185643196106
train gradient:  0.14049614310503689
iteration : 6228
train acc:  0.7109375
train loss:  0.5837035179138184
train gradient:  0.1768029895112394
iteration : 6229
train acc:  0.765625
train loss:  0.4608263373374939
train gradient:  0.10538806454559335
iteration : 6230
train acc:  0.65625
train loss:  0.552823543548584
train gradient:  0.17861434835961476
iteration : 6231
train acc:  0.7421875
train loss:  0.4975617527961731
train gradient:  0.11293768241212422
iteration : 6232
train acc:  0.7890625
train loss:  0.4462048411369324
train gradient:  0.11184186528926372
iteration : 6233
train acc:  0.7578125
train loss:  0.49157530069351196
train gradient:  0.14128916786369256
iteration : 6234
train acc:  0.7734375
train loss:  0.46870356798171997
train gradient:  0.1378833578422251
iteration : 6235
train acc:  0.7578125
train loss:  0.4379494786262512
train gradient:  0.1295494315134932
iteration : 6236
train acc:  0.734375
train loss:  0.4836212396621704
train gradient:  0.11838384636051111
iteration : 6237
train acc:  0.7265625
train loss:  0.5008307695388794
train gradient:  0.16621177481254262
iteration : 6238
train acc:  0.703125
train loss:  0.5107253193855286
train gradient:  0.15289674847329626
iteration : 6239
train acc:  0.734375
train loss:  0.5073641538619995
train gradient:  0.14140046650563443
iteration : 6240
train acc:  0.71875
train loss:  0.5039973258972168
train gradient:  0.16602852820706343
iteration : 6241
train acc:  0.703125
train loss:  0.5782139301300049
train gradient:  0.14306679483397963
iteration : 6242
train acc:  0.7890625
train loss:  0.45262348651885986
train gradient:  0.102328624417885
iteration : 6243
train acc:  0.734375
train loss:  0.5322494506835938
train gradient:  0.15922449705967517
iteration : 6244
train acc:  0.6640625
train loss:  0.6413876414299011
train gradient:  0.17718743703571815
iteration : 6245
train acc:  0.6796875
train loss:  0.5427772402763367
train gradient:  0.13526339010672794
iteration : 6246
train acc:  0.7109375
train loss:  0.4638763666152954
train gradient:  0.10965152461668108
iteration : 6247
train acc:  0.71875
train loss:  0.5528017282485962
train gradient:  0.16909551437961642
iteration : 6248
train acc:  0.6875
train loss:  0.5471058487892151
train gradient:  0.17929233297876085
iteration : 6249
train acc:  0.7734375
train loss:  0.47551336884498596
train gradient:  0.12670047154896125
iteration : 6250
train acc:  0.671875
train loss:  0.5866881608963013
train gradient:  0.16659385649563974
iteration : 6251
train acc:  0.78125
train loss:  0.4285638928413391
train gradient:  0.0949859939200897
iteration : 6252
train acc:  0.6875
train loss:  0.4937123656272888
train gradient:  0.13103487563482655
iteration : 6253
train acc:  0.75
train loss:  0.5249480605125427
train gradient:  0.12586513571193597
iteration : 6254
train acc:  0.7578125
train loss:  0.49858832359313965
train gradient:  0.1300771717416825
iteration : 6255
train acc:  0.703125
train loss:  0.5348113775253296
train gradient:  0.1733131238566214
iteration : 6256
train acc:  0.7734375
train loss:  0.4521980881690979
train gradient:  0.11628218286437207
iteration : 6257
train acc:  0.71875
train loss:  0.5443971157073975
train gradient:  0.1677128395782158
iteration : 6258
train acc:  0.6953125
train loss:  0.5437723398208618
train gradient:  0.18005139766509012
iteration : 6259
train acc:  0.6953125
train loss:  0.5562047362327576
train gradient:  0.18267068023802502
iteration : 6260
train acc:  0.75
train loss:  0.5069381594657898
train gradient:  0.1467238661692073
iteration : 6261
train acc:  0.6328125
train loss:  0.6029706001281738
train gradient:  0.19787492489327507
iteration : 6262
train acc:  0.7890625
train loss:  0.4505687355995178
train gradient:  0.10824500707045336
iteration : 6263
train acc:  0.7265625
train loss:  0.5367894172668457
train gradient:  0.1367984689870685
iteration : 6264
train acc:  0.765625
train loss:  0.47822457551956177
train gradient:  0.12152811484322297
iteration : 6265
train acc:  0.78125
train loss:  0.48205941915512085
train gradient:  0.1446300903291525
iteration : 6266
train acc:  0.7890625
train loss:  0.43833041191101074
train gradient:  0.10452570492250642
iteration : 6267
train acc:  0.6875
train loss:  0.5689913034439087
train gradient:  0.15417058132364286
iteration : 6268
train acc:  0.7265625
train loss:  0.48349452018737793
train gradient:  0.11517020274744247
iteration : 6269
train acc:  0.7109375
train loss:  0.5407416820526123
train gradient:  0.11829355036306637
iteration : 6270
train acc:  0.796875
train loss:  0.45585229992866516
train gradient:  0.13112254413257593
iteration : 6271
train acc:  0.7578125
train loss:  0.4890093207359314
train gradient:  0.11927715672026437
iteration : 6272
train acc:  0.6875
train loss:  0.5317826271057129
train gradient:  0.1555899412077153
iteration : 6273
train acc:  0.7890625
train loss:  0.4374880790710449
train gradient:  0.10584640413798681
iteration : 6274
train acc:  0.6953125
train loss:  0.563308835029602
train gradient:  0.18020436226591494
iteration : 6275
train acc:  0.7109375
train loss:  0.5539348125457764
train gradient:  0.14575637986556755
iteration : 6276
train acc:  0.7578125
train loss:  0.4979501962661743
train gradient:  0.13673798032259687
iteration : 6277
train acc:  0.6953125
train loss:  0.5841569900512695
train gradient:  0.22594180769602584
iteration : 6278
train acc:  0.8359375
train loss:  0.41897791624069214
train gradient:  0.10953817329873076
iteration : 6279
train acc:  0.65625
train loss:  0.6034644246101379
train gradient:  0.16836312341016724
iteration : 6280
train acc:  0.7421875
train loss:  0.49703866243362427
train gradient:  0.12814317193081456
iteration : 6281
train acc:  0.6015625
train loss:  0.6070592403411865
train gradient:  0.18308329746445035
iteration : 6282
train acc:  0.671875
train loss:  0.5763060450553894
train gradient:  0.1396185290186922
iteration : 6283
train acc:  0.78125
train loss:  0.45956987142562866
train gradient:  0.10912317534287297
iteration : 6284
train acc:  0.703125
train loss:  0.5071295499801636
train gradient:  0.1648940089935858
iteration : 6285
train acc:  0.78125
train loss:  0.498115599155426
train gradient:  0.13063040126875317
iteration : 6286
train acc:  0.765625
train loss:  0.4855582118034363
train gradient:  0.13205103398805873
iteration : 6287
train acc:  0.7421875
train loss:  0.5577672123908997
train gradient:  0.1981936487096969
iteration : 6288
train acc:  0.78125
train loss:  0.4664263129234314
train gradient:  0.1173897022141887
iteration : 6289
train acc:  0.7421875
train loss:  0.49333542585372925
train gradient:  0.12963405528135558
iteration : 6290
train acc:  0.7421875
train loss:  0.47775694727897644
train gradient:  0.12369383496120896
iteration : 6291
train acc:  0.734375
train loss:  0.49853813648223877
train gradient:  0.14344902254917208
iteration : 6292
train acc:  0.7109375
train loss:  0.5154796838760376
train gradient:  0.12048237705533907
iteration : 6293
train acc:  0.7578125
train loss:  0.47679150104522705
train gradient:  0.1151122190642144
iteration : 6294
train acc:  0.6796875
train loss:  0.569847583770752
train gradient:  0.2283857408893566
iteration : 6295
train acc:  0.7109375
train loss:  0.5053892135620117
train gradient:  0.13551252930811458
iteration : 6296
train acc:  0.78125
train loss:  0.4661320149898529
train gradient:  0.10874496064250253
iteration : 6297
train acc:  0.765625
train loss:  0.5145460963249207
train gradient:  0.1719571198776073
iteration : 6298
train acc:  0.6953125
train loss:  0.5497902631759644
train gradient:  0.1462704102294749
iteration : 6299
train acc:  0.734375
train loss:  0.48500895500183105
train gradient:  0.11250948657646909
iteration : 6300
train acc:  0.75
train loss:  0.5240886807441711
train gradient:  0.14214181620257282
iteration : 6301
train acc:  0.65625
train loss:  0.5566167831420898
train gradient:  0.14654552079134858
iteration : 6302
train acc:  0.734375
train loss:  0.4776964485645294
train gradient:  0.10597095220893386
iteration : 6303
train acc:  0.6953125
train loss:  0.5950411558151245
train gradient:  0.15909843134981538
iteration : 6304
train acc:  0.765625
train loss:  0.46546807885169983
train gradient:  0.1122028322837142
iteration : 6305
train acc:  0.71875
train loss:  0.4972466826438904
train gradient:  0.15511612344406372
iteration : 6306
train acc:  0.734375
train loss:  0.47382837533950806
train gradient:  0.11877173257566775
iteration : 6307
train acc:  0.8046875
train loss:  0.4738517105579376
train gradient:  0.14047281532080452
iteration : 6308
train acc:  0.71875
train loss:  0.5400326251983643
train gradient:  0.1545678415885678
iteration : 6309
train acc:  0.7265625
train loss:  0.5443328619003296
train gradient:  0.16221846699824197
iteration : 6310
train acc:  0.71875
train loss:  0.5034016370773315
train gradient:  0.12724078227475522
iteration : 6311
train acc:  0.734375
train loss:  0.4861157536506653
train gradient:  0.12237355611310365
iteration : 6312
train acc:  0.75
train loss:  0.4927408695220947
train gradient:  0.10693240250645274
iteration : 6313
train acc:  0.671875
train loss:  0.6005294919013977
train gradient:  0.16390044358990205
iteration : 6314
train acc:  0.7734375
train loss:  0.5122123956680298
train gradient:  0.17459308006477164
iteration : 6315
train acc:  0.71875
train loss:  0.5061978697776794
train gradient:  0.16315952591870225
iteration : 6316
train acc:  0.7421875
train loss:  0.44745737314224243
train gradient:  0.13132749349663303
iteration : 6317
train acc:  0.75
train loss:  0.44718360900878906
train gradient:  0.12413257492848649
iteration : 6318
train acc:  0.71875
train loss:  0.539378821849823
train gradient:  0.16291392310509417
iteration : 6319
train acc:  0.765625
train loss:  0.5448479652404785
train gradient:  0.18111444010910407
iteration : 6320
train acc:  0.7578125
train loss:  0.4880942702293396
train gradient:  0.12594959225838065
iteration : 6321
train acc:  0.7265625
train loss:  0.5010086894035339
train gradient:  0.140037967938969
iteration : 6322
train acc:  0.7578125
train loss:  0.45856133103370667
train gradient:  0.14756312530203042
iteration : 6323
train acc:  0.7890625
train loss:  0.5357885360717773
train gradient:  0.1616719794296098
iteration : 6324
train acc:  0.78125
train loss:  0.45362791419029236
train gradient:  0.11894532044809333
iteration : 6325
train acc:  0.7734375
train loss:  0.44592049717903137
train gradient:  0.13168910469820783
iteration : 6326
train acc:  0.7265625
train loss:  0.5214684009552002
train gradient:  0.13955748114091554
iteration : 6327
train acc:  0.796875
train loss:  0.4445817172527313
train gradient:  0.10763086326629669
iteration : 6328
train acc:  0.75
train loss:  0.4509774148464203
train gradient:  0.11739123133594584
iteration : 6329
train acc:  0.7734375
train loss:  0.4685896337032318
train gradient:  0.10868925674804455
iteration : 6330
train acc:  0.6484375
train loss:  0.6190587878227234
train gradient:  0.21717202451915624
iteration : 6331
train acc:  0.7578125
train loss:  0.4907902479171753
train gradient:  0.10590490455890636
iteration : 6332
train acc:  0.671875
train loss:  0.5617908239364624
train gradient:  0.15408562360484315
iteration : 6333
train acc:  0.8125
train loss:  0.4361270070075989
train gradient:  0.11432180770821226
iteration : 6334
train acc:  0.7578125
train loss:  0.48915743827819824
train gradient:  0.11070323459896933
iteration : 6335
train acc:  0.703125
train loss:  0.5340589880943298
train gradient:  0.11120816285379281
iteration : 6336
train acc:  0.6953125
train loss:  0.5547044277191162
train gradient:  0.1528653499546466
iteration : 6337
train acc:  0.703125
train loss:  0.5509886741638184
train gradient:  0.20346072716906144
iteration : 6338
train acc:  0.7265625
train loss:  0.5209914445877075
train gradient:  0.16832669977271997
iteration : 6339
train acc:  0.71875
train loss:  0.5041077733039856
train gradient:  0.11350540600260846
iteration : 6340
train acc:  0.75
train loss:  0.48834916949272156
train gradient:  0.15350179386548005
iteration : 6341
train acc:  0.734375
train loss:  0.5317248702049255
train gradient:  0.16627979555839367
iteration : 6342
train acc:  0.6875
train loss:  0.5627207159996033
train gradient:  0.20609238559005083
iteration : 6343
train acc:  0.6953125
train loss:  0.5435571670532227
train gradient:  0.12307341984068561
iteration : 6344
train acc:  0.7890625
train loss:  0.4682024121284485
train gradient:  0.15771645200015988
iteration : 6345
train acc:  0.7109375
train loss:  0.480633407831192
train gradient:  0.139335211312561
iteration : 6346
train acc:  0.703125
train loss:  0.5222861170768738
train gradient:  0.1239662388470807
iteration : 6347
train acc:  0.71875
train loss:  0.5241028070449829
train gradient:  0.13408075863312985
iteration : 6348
train acc:  0.6796875
train loss:  0.5798977017402649
train gradient:  0.13759339534790455
iteration : 6349
train acc:  0.6875
train loss:  0.5591733455657959
train gradient:  0.1775176894624675
iteration : 6350
train acc:  0.6875
train loss:  0.4911060333251953
train gradient:  0.17072618595911168
iteration : 6351
train acc:  0.671875
train loss:  0.5444079637527466
train gradient:  0.13497552842389277
iteration : 6352
train acc:  0.75
train loss:  0.45943379402160645
train gradient:  0.11846907213923959
iteration : 6353
train acc:  0.6796875
train loss:  0.5781301259994507
train gradient:  0.16026648614399347
iteration : 6354
train acc:  0.71875
train loss:  0.5213380455970764
train gradient:  0.22085843717073278
iteration : 6355
train acc:  0.7265625
train loss:  0.5210901498794556
train gradient:  0.1431323022030179
iteration : 6356
train acc:  0.734375
train loss:  0.5234277248382568
train gradient:  0.1611847683408253
iteration : 6357
train acc:  0.6875
train loss:  0.5842304229736328
train gradient:  0.1643366176810928
iteration : 6358
train acc:  0.7578125
train loss:  0.49547433853149414
train gradient:  0.17869065935654704
iteration : 6359
train acc:  0.765625
train loss:  0.47604984045028687
train gradient:  0.13275823456407454
iteration : 6360
train acc:  0.703125
train loss:  0.528243899345398
train gradient:  0.15390008187727616
iteration : 6361
train acc:  0.7578125
train loss:  0.48470163345336914
train gradient:  0.11932482824200732
iteration : 6362
train acc:  0.75
train loss:  0.46662554144859314
train gradient:  0.1072722596850412
iteration : 6363
train acc:  0.7109375
train loss:  0.5470964908599854
train gradient:  0.1890255276568202
iteration : 6364
train acc:  0.6875
train loss:  0.5503323078155518
train gradient:  0.19594488213709937
iteration : 6365
train acc:  0.7265625
train loss:  0.4861738085746765
train gradient:  0.12216432548905495
iteration : 6366
train acc:  0.671875
train loss:  0.5580118894577026
train gradient:  0.15031239286051667
iteration : 6367
train acc:  0.7109375
train loss:  0.5216094255447388
train gradient:  0.15641832805731085
iteration : 6368
train acc:  0.671875
train loss:  0.5525938272476196
train gradient:  0.1461763430108578
iteration : 6369
train acc:  0.7734375
train loss:  0.45559996366500854
train gradient:  0.10774746762034178
iteration : 6370
train acc:  0.75
train loss:  0.5172836780548096
train gradient:  0.1862231346634658
iteration : 6371
train acc:  0.765625
train loss:  0.4804987609386444
train gradient:  0.1047508832799998
iteration : 6372
train acc:  0.765625
train loss:  0.4451436400413513
train gradient:  0.08877450964374714
iteration : 6373
train acc:  0.7109375
train loss:  0.5352351069450378
train gradient:  0.1768994502123158
iteration : 6374
train acc:  0.71875
train loss:  0.5645331144332886
train gradient:  0.20400420015953696
iteration : 6375
train acc:  0.765625
train loss:  0.4930872619152069
train gradient:  0.13841108991621467
iteration : 6376
train acc:  0.703125
train loss:  0.5618094801902771
train gradient:  0.15489086767830545
iteration : 6377
train acc:  0.7109375
train loss:  0.5230975151062012
train gradient:  0.1590085138288463
iteration : 6378
train acc:  0.734375
train loss:  0.49753105640411377
train gradient:  0.1379645417081916
iteration : 6379
train acc:  0.6796875
train loss:  0.6020399332046509
train gradient:  0.23030016073419213
iteration : 6380
train acc:  0.6875
train loss:  0.5783355236053467
train gradient:  0.13411171610615502
iteration : 6381
train acc:  0.7265625
train loss:  0.5469637513160706
train gradient:  0.14201859256246857
iteration : 6382
train acc:  0.6640625
train loss:  0.5414077043533325
train gradient:  0.17950134208977234
iteration : 6383
train acc:  0.765625
train loss:  0.4938163459300995
train gradient:  0.11766309740040172
iteration : 6384
train acc:  0.6953125
train loss:  0.5517109632492065
train gradient:  0.15653646456438214
iteration : 6385
train acc:  0.6875
train loss:  0.5457945466041565
train gradient:  0.17297287059931354
iteration : 6386
train acc:  0.734375
train loss:  0.4729160666465759
train gradient:  0.11804440151013855
iteration : 6387
train acc:  0.703125
train loss:  0.5143230557441711
train gradient:  0.14135250128057125
iteration : 6388
train acc:  0.765625
train loss:  0.5009135007858276
train gradient:  0.12197812415288847
iteration : 6389
train acc:  0.703125
train loss:  0.5067847967147827
train gradient:  0.10458120091748266
iteration : 6390
train acc:  0.671875
train loss:  0.5496765375137329
train gradient:  0.15592640178453565
iteration : 6391
train acc:  0.734375
train loss:  0.5247535109519958
train gradient:  0.14591935473058756
iteration : 6392
train acc:  0.8046875
train loss:  0.4399155378341675
train gradient:  0.11889683559539856
iteration : 6393
train acc:  0.6875
train loss:  0.5796527862548828
train gradient:  0.18940639136903636
iteration : 6394
train acc:  0.7421875
train loss:  0.5062538385391235
train gradient:  0.161729448374019
iteration : 6395
train acc:  0.7890625
train loss:  0.48728781938552856
train gradient:  0.12229462278497584
iteration : 6396
train acc:  0.7578125
train loss:  0.4647403657436371
train gradient:  0.11391770066778717
iteration : 6397
train acc:  0.703125
train loss:  0.5562586784362793
train gradient:  0.18185041646361383
iteration : 6398
train acc:  0.78125
train loss:  0.4428143799304962
train gradient:  0.09263999930000996
iteration : 6399
train acc:  0.7734375
train loss:  0.5201377272605896
train gradient:  0.13052549121971568
iteration : 6400
train acc:  0.7421875
train loss:  0.5307788252830505
train gradient:  0.14394045860122354
iteration : 6401
train acc:  0.7734375
train loss:  0.4560319781303406
train gradient:  0.1316360471513629
iteration : 6402
train acc:  0.6875
train loss:  0.5365511775016785
train gradient:  0.2197685000533573
iteration : 6403
train acc:  0.7734375
train loss:  0.4938558340072632
train gradient:  0.13654456936331022
iteration : 6404
train acc:  0.7265625
train loss:  0.5310451984405518
train gradient:  0.16566475138135034
iteration : 6405
train acc:  0.7578125
train loss:  0.47470855712890625
train gradient:  0.09730620556900572
iteration : 6406
train acc:  0.734375
train loss:  0.49491605162620544
train gradient:  0.13191823753752493
iteration : 6407
train acc:  0.6953125
train loss:  0.5230526924133301
train gradient:  0.1953335968782735
iteration : 6408
train acc:  0.7109375
train loss:  0.5138595700263977
train gradient:  0.16353005716783456
iteration : 6409
train acc:  0.71875
train loss:  0.5166910886764526
train gradient:  0.11370741426474996
iteration : 6410
train acc:  0.7421875
train loss:  0.5202503204345703
train gradient:  0.1410160380525738
iteration : 6411
train acc:  0.7265625
train loss:  0.5180552005767822
train gradient:  0.11964105092448453
iteration : 6412
train acc:  0.796875
train loss:  0.46873557567596436
train gradient:  0.09312655619580938
iteration : 6413
train acc:  0.6875
train loss:  0.5345888733863831
train gradient:  0.15255648447584058
iteration : 6414
train acc:  0.71875
train loss:  0.49345916509628296
train gradient:  0.12799750056047277
iteration : 6415
train acc:  0.7109375
train loss:  0.5371318459510803
train gradient:  0.1485787063859203
iteration : 6416
train acc:  0.7265625
train loss:  0.5378212928771973
train gradient:  0.19531415900101767
iteration : 6417
train acc:  0.71875
train loss:  0.5137523412704468
train gradient:  0.18274164904793927
iteration : 6418
train acc:  0.75
train loss:  0.5433454513549805
train gradient:  0.18558977953420308
iteration : 6419
train acc:  0.6953125
train loss:  0.5506896376609802
train gradient:  0.12387584831527733
iteration : 6420
train acc:  0.59375
train loss:  0.6016535758972168
train gradient:  0.2636495848413792
iteration : 6421
train acc:  0.6953125
train loss:  0.5256770849227905
train gradient:  0.14969160432680378
iteration : 6422
train acc:  0.7265625
train loss:  0.5115225315093994
train gradient:  0.18559176896711838
iteration : 6423
train acc:  0.671875
train loss:  0.5556799173355103
train gradient:  0.13858513784751586
iteration : 6424
train acc:  0.703125
train loss:  0.5279797911643982
train gradient:  0.14724444984534213
iteration : 6425
train acc:  0.6796875
train loss:  0.5858750343322754
train gradient:  0.16583783732511292
iteration : 6426
train acc:  0.75
train loss:  0.5047312378883362
train gradient:  0.1450496420404096
iteration : 6427
train acc:  0.7421875
train loss:  0.5037322044372559
train gradient:  0.13201251631531954
iteration : 6428
train acc:  0.796875
train loss:  0.42182934284210205
train gradient:  0.10226160797958282
iteration : 6429
train acc:  0.703125
train loss:  0.5756039619445801
train gradient:  0.1471354352023031
iteration : 6430
train acc:  0.71875
train loss:  0.5354430079460144
train gradient:  0.1929647123505927
iteration : 6431
train acc:  0.7734375
train loss:  0.4589785933494568
train gradient:  0.14699454947764254
iteration : 6432
train acc:  0.7421875
train loss:  0.513653576374054
train gradient:  0.11929427921528715
iteration : 6433
train acc:  0.703125
train loss:  0.5116590261459351
train gradient:  0.15476421857183204
iteration : 6434
train acc:  0.7890625
train loss:  0.44254863262176514
train gradient:  0.10415286753294761
iteration : 6435
train acc:  0.7734375
train loss:  0.4521094262599945
train gradient:  0.11167646379149557
iteration : 6436
train acc:  0.671875
train loss:  0.5418422222137451
train gradient:  0.12077043470448073
iteration : 6437
train acc:  0.7734375
train loss:  0.4743649959564209
train gradient:  0.122377331026262
iteration : 6438
train acc:  0.75
train loss:  0.4966541528701782
train gradient:  0.13355198912380106
iteration : 6439
train acc:  0.765625
train loss:  0.4975820779800415
train gradient:  0.1298719385616875
iteration : 6440
train acc:  0.8046875
train loss:  0.442050039768219
train gradient:  0.10580045967716276
iteration : 6441
train acc:  0.7109375
train loss:  0.5780796408653259
train gradient:  0.2051477826283289
iteration : 6442
train acc:  0.703125
train loss:  0.4966167211532593
train gradient:  0.1483390010042246
iteration : 6443
train acc:  0.7421875
train loss:  0.5146945714950562
train gradient:  0.14329171904576277
iteration : 6444
train acc:  0.703125
train loss:  0.5809403657913208
train gradient:  0.20710562601433924
iteration : 6445
train acc:  0.7734375
train loss:  0.4618298411369324
train gradient:  0.11797737071987292
iteration : 6446
train acc:  0.7109375
train loss:  0.514045000076294
train gradient:  0.1441396062726777
iteration : 6447
train acc:  0.765625
train loss:  0.4620188772678375
train gradient:  0.13530880995706604
iteration : 6448
train acc:  0.7578125
train loss:  0.49252986907958984
train gradient:  0.1282071908533582
iteration : 6449
train acc:  0.7734375
train loss:  0.4629271626472473
train gradient:  0.13268940613832636
iteration : 6450
train acc:  0.6875
train loss:  0.49196743965148926
train gradient:  0.10665821499210759
iteration : 6451
train acc:  0.703125
train loss:  0.5309379696846008
train gradient:  0.14266706527053058
iteration : 6452
train acc:  0.734375
train loss:  0.4915141463279724
train gradient:  0.12809945014077484
iteration : 6453
train acc:  0.7578125
train loss:  0.46605294942855835
train gradient:  0.10378380459255161
iteration : 6454
train acc:  0.8203125
train loss:  0.4194128215312958
train gradient:  0.09034857303831127
iteration : 6455
train acc:  0.6953125
train loss:  0.5967118740081787
train gradient:  0.17036315046300382
iteration : 6456
train acc:  0.7109375
train loss:  0.5346275568008423
train gradient:  0.16125919494745025
iteration : 6457
train acc:  0.7578125
train loss:  0.4985942244529724
train gradient:  0.19660080208699288
iteration : 6458
train acc:  0.703125
train loss:  0.5172250866889954
train gradient:  0.09875884268266223
iteration : 6459
train acc:  0.734375
train loss:  0.502945601940155
train gradient:  0.14250348463559465
iteration : 6460
train acc:  0.734375
train loss:  0.4839423596858978
train gradient:  0.15468860769586262
iteration : 6461
train acc:  0.7890625
train loss:  0.48781511187553406
train gradient:  0.13942833167522684
iteration : 6462
train acc:  0.734375
train loss:  0.4689226746559143
train gradient:  0.09181948806615262
iteration : 6463
train acc:  0.71875
train loss:  0.5096434354782104
train gradient:  0.10296891223266591
iteration : 6464
train acc:  0.7109375
train loss:  0.5003616213798523
train gradient:  0.11010350855696655
iteration : 6465
train acc:  0.7421875
train loss:  0.4758189916610718
train gradient:  0.11439646561919828
iteration : 6466
train acc:  0.734375
train loss:  0.49948978424072266
train gradient:  0.13379761785198152
iteration : 6467
train acc:  0.8046875
train loss:  0.42630454897880554
train gradient:  0.08501004121597495
iteration : 6468
train acc:  0.7265625
train loss:  0.5421817898750305
train gradient:  0.13984882948157917
iteration : 6469
train acc:  0.75
train loss:  0.528123140335083
train gradient:  0.17923110567211303
iteration : 6470
train acc:  0.71875
train loss:  0.5077863335609436
train gradient:  0.12346895842293425
iteration : 6471
train acc:  0.65625
train loss:  0.5354592800140381
train gradient:  0.17051698949717786
iteration : 6472
train acc:  0.7265625
train loss:  0.5309499502182007
train gradient:  0.13263554000650069
iteration : 6473
train acc:  0.7734375
train loss:  0.4882147014141083
train gradient:  0.12088420233731247
iteration : 6474
train acc:  0.7734375
train loss:  0.4834865629673004
train gradient:  0.1170107317800841
iteration : 6475
train acc:  0.7109375
train loss:  0.5468645095825195
train gradient:  0.22976247096442787
iteration : 6476
train acc:  0.7890625
train loss:  0.46216946840286255
train gradient:  0.124239079064497
iteration : 6477
train acc:  0.78125
train loss:  0.4728825092315674
train gradient:  0.11644227005349014
iteration : 6478
train acc:  0.71875
train loss:  0.5600450038909912
train gradient:  0.23520734220832074
iteration : 6479
train acc:  0.78125
train loss:  0.4676246643066406
train gradient:  0.10820619384864603
iteration : 6480
train acc:  0.7421875
train loss:  0.5056672096252441
train gradient:  0.24026799572260416
iteration : 6481
train acc:  0.6640625
train loss:  0.557774007320404
train gradient:  0.14579994069867427
iteration : 6482
train acc:  0.6875
train loss:  0.5577282309532166
train gradient:  0.1672599168004441
iteration : 6483
train acc:  0.7734375
train loss:  0.4573328495025635
train gradient:  0.11724932807638004
iteration : 6484
train acc:  0.7734375
train loss:  0.47744280099868774
train gradient:  0.11812374173372164
iteration : 6485
train acc:  0.796875
train loss:  0.45479416847229004
train gradient:  0.12648392659402607
iteration : 6486
train acc:  0.7265625
train loss:  0.5323264002799988
train gradient:  0.11304087743436671
iteration : 6487
train acc:  0.6484375
train loss:  0.5920875072479248
train gradient:  0.1764514333779254
iteration : 6488
train acc:  0.734375
train loss:  0.5001319646835327
train gradient:  0.09393935909534448
iteration : 6489
train acc:  0.75
train loss:  0.4754771590232849
train gradient:  0.12836415001047125
iteration : 6490
train acc:  0.7578125
train loss:  0.4557672142982483
train gradient:  0.10857810053716606
iteration : 6491
train acc:  0.71875
train loss:  0.5043590068817139
train gradient:  0.12781915800131322
iteration : 6492
train acc:  0.7421875
train loss:  0.48727482557296753
train gradient:  0.12692692076110393
iteration : 6493
train acc:  0.7265625
train loss:  0.49597805738449097
train gradient:  0.1464339231375621
iteration : 6494
train acc:  0.765625
train loss:  0.4819125235080719
train gradient:  0.1354763517892238
iteration : 6495
train acc:  0.6875
train loss:  0.5475961565971375
train gradient:  0.12894464460378893
iteration : 6496
train acc:  0.6796875
train loss:  0.5861374139785767
train gradient:  0.1899512074846264
iteration : 6497
train acc:  0.71875
train loss:  0.491720050573349
train gradient:  0.13088784220171149
iteration : 6498
train acc:  0.6796875
train loss:  0.6028479337692261
train gradient:  0.21878586795251026
iteration : 6499
train acc:  0.75
train loss:  0.4356781840324402
train gradient:  0.09077351726793993
iteration : 6500
train acc:  0.671875
train loss:  0.5552510023117065
train gradient:  0.17078577435825704
iteration : 6501
train acc:  0.8046875
train loss:  0.4431421160697937
train gradient:  0.10675363803616077
iteration : 6502
train acc:  0.734375
train loss:  0.5116322040557861
train gradient:  0.12812888454805038
iteration : 6503
train acc:  0.6796875
train loss:  0.5651554465293884
train gradient:  0.1957742344930788
iteration : 6504
train acc:  0.78125
train loss:  0.4694982171058655
train gradient:  0.11898999610070096
iteration : 6505
train acc:  0.796875
train loss:  0.4272387623786926
train gradient:  0.11088948010928064
iteration : 6506
train acc:  0.6796875
train loss:  0.5499316453933716
train gradient:  0.13482854204136083
iteration : 6507
train acc:  0.6875
train loss:  0.5373337864875793
train gradient:  0.14794193747457804
iteration : 6508
train acc:  0.734375
train loss:  0.4934309124946594
train gradient:  0.13061992114917642
iteration : 6509
train acc:  0.7421875
train loss:  0.4925290048122406
train gradient:  0.13430822189761768
iteration : 6510
train acc:  0.7265625
train loss:  0.5202266573905945
train gradient:  0.17467292747638202
iteration : 6511
train acc:  0.8203125
train loss:  0.40706363320350647
train gradient:  0.11814023370894747
iteration : 6512
train acc:  0.671875
train loss:  0.5519405603408813
train gradient:  0.14058286829118632
iteration : 6513
train acc:  0.703125
train loss:  0.5096806287765503
train gradient:  0.12350927514950946
iteration : 6514
train acc:  0.6953125
train loss:  0.5046051740646362
train gradient:  0.13606777711021842
iteration : 6515
train acc:  0.703125
train loss:  0.5324700474739075
train gradient:  0.12460114757364465
iteration : 6516
train acc:  0.7421875
train loss:  0.5234427452087402
train gradient:  0.1255464625197744
iteration : 6517
train acc:  0.7265625
train loss:  0.5523734092712402
train gradient:  0.16141084994386135
iteration : 6518
train acc:  0.71875
train loss:  0.6059571504592896
train gradient:  0.1719362212762961
iteration : 6519
train acc:  0.6953125
train loss:  0.5633882284164429
train gradient:  0.2016629665478264
iteration : 6520
train acc:  0.71875
train loss:  0.5206735730171204
train gradient:  0.11784659428094929
iteration : 6521
train acc:  0.6953125
train loss:  0.49292439222335815
train gradient:  0.12066863073836619
iteration : 6522
train acc:  0.7265625
train loss:  0.5095905065536499
train gradient:  0.11024469590334918
iteration : 6523
train acc:  0.7734375
train loss:  0.44617509841918945
train gradient:  0.1319928843450013
iteration : 6524
train acc:  0.734375
train loss:  0.5176007747650146
train gradient:  0.15732286388761502
iteration : 6525
train acc:  0.8125
train loss:  0.41769200563430786
train gradient:  0.10043439858639049
iteration : 6526
train acc:  0.75
train loss:  0.46884697675704956
train gradient:  0.12051685342596041
iteration : 6527
train acc:  0.734375
train loss:  0.5027951002120972
train gradient:  0.16879243744747544
iteration : 6528
train acc:  0.7734375
train loss:  0.5090551376342773
train gradient:  0.1289583492792091
iteration : 6529
train acc:  0.796875
train loss:  0.457260400056839
train gradient:  0.11348497488745886
iteration : 6530
train acc:  0.71875
train loss:  0.5049127340316772
train gradient:  0.12460440748949132
iteration : 6531
train acc:  0.71875
train loss:  0.5730429887771606
train gradient:  0.11952723246629755
iteration : 6532
train acc:  0.734375
train loss:  0.5017255544662476
train gradient:  0.11269421007292463
iteration : 6533
train acc:  0.640625
train loss:  0.6035784482955933
train gradient:  0.16282865343565112
iteration : 6534
train acc:  0.7265625
train loss:  0.520305871963501
train gradient:  0.14473619917511948
iteration : 6535
train acc:  0.734375
train loss:  0.5384752154350281
train gradient:  0.1332909900816071
iteration : 6536
train acc:  0.7578125
train loss:  0.5332698822021484
train gradient:  0.13173025069283195
iteration : 6537
train acc:  0.8203125
train loss:  0.4222242534160614
train gradient:  0.08963557894631677
iteration : 6538
train acc:  0.703125
train loss:  0.5899101495742798
train gradient:  0.14957333994091487
iteration : 6539
train acc:  0.7421875
train loss:  0.4845268130302429
train gradient:  0.1273709010701849
iteration : 6540
train acc:  0.6015625
train loss:  0.6792517304420471
train gradient:  0.23767422560619678
iteration : 6541
train acc:  0.75
train loss:  0.5025650858879089
train gradient:  0.11883187862104323
iteration : 6542
train acc:  0.7421875
train loss:  0.46929219365119934
train gradient:  0.10728579643228113
iteration : 6543
train acc:  0.765625
train loss:  0.48970624804496765
train gradient:  0.16118922668477353
iteration : 6544
train acc:  0.7734375
train loss:  0.49504074454307556
train gradient:  0.1462750502772655
iteration : 6545
train acc:  0.796875
train loss:  0.44804245233535767
train gradient:  0.09291700694875683
iteration : 6546
train acc:  0.71875
train loss:  0.5219001770019531
train gradient:  0.12847025465055767
iteration : 6547
train acc:  0.7890625
train loss:  0.42298901081085205
train gradient:  0.09466066759687894
iteration : 6548
train acc:  0.7578125
train loss:  0.5035543441772461
train gradient:  0.18010541019715018
iteration : 6549
train acc:  0.75
train loss:  0.513728141784668
train gradient:  0.1277524238838242
iteration : 6550
train acc:  0.6796875
train loss:  0.5368378758430481
train gradient:  0.14139146026737215
iteration : 6551
train acc:  0.7421875
train loss:  0.49293383955955505
train gradient:  0.11236009218191904
iteration : 6552
train acc:  0.6953125
train loss:  0.5358312129974365
train gradient:  0.11186151152254577
iteration : 6553
train acc:  0.71875
train loss:  0.5497909784317017
train gradient:  0.20603683423588065
iteration : 6554
train acc:  0.6875
train loss:  0.5271295309066772
train gradient:  0.16439671463576827
iteration : 6555
train acc:  0.7734375
train loss:  0.4260719120502472
train gradient:  0.1425735199209433
iteration : 6556
train acc:  0.6796875
train loss:  0.5687429308891296
train gradient:  0.15504757400715632
iteration : 6557
train acc:  0.7578125
train loss:  0.5448921918869019
train gradient:  0.12175251444993625
iteration : 6558
train acc:  0.8046875
train loss:  0.45523637533187866
train gradient:  0.10430534021942463
iteration : 6559
train acc:  0.8046875
train loss:  0.44826632738113403
train gradient:  0.12445793522774151
iteration : 6560
train acc:  0.7109375
train loss:  0.5353794097900391
train gradient:  0.13887805355038235
iteration : 6561
train acc:  0.7578125
train loss:  0.4913340210914612
train gradient:  0.14777652230231175
iteration : 6562
train acc:  0.75
train loss:  0.4661921560764313
train gradient:  0.1219992084235244
iteration : 6563
train acc:  0.7734375
train loss:  0.4840453267097473
train gradient:  0.12275492158813539
iteration : 6564
train acc:  0.75
train loss:  0.4942570626735687
train gradient:  0.11957363659190032
iteration : 6565
train acc:  0.671875
train loss:  0.5688637495040894
train gradient:  0.23848269026705482
iteration : 6566
train acc:  0.7734375
train loss:  0.5037444829940796
train gradient:  0.13784504997137298
iteration : 6567
train acc:  0.6640625
train loss:  0.6106281280517578
train gradient:  0.2464722270781143
iteration : 6568
train acc:  0.78125
train loss:  0.43840935826301575
train gradient:  0.11805544750609999
iteration : 6569
train acc:  0.71875
train loss:  0.511635959148407
train gradient:  0.12315300367763607
iteration : 6570
train acc:  0.6796875
train loss:  0.546744167804718
train gradient:  0.13530848180916233
iteration : 6571
train acc:  0.71875
train loss:  0.5081562399864197
train gradient:  0.12948953273995137
iteration : 6572
train acc:  0.8046875
train loss:  0.4611068367958069
train gradient:  0.10375756476524403
iteration : 6573
train acc:  0.734375
train loss:  0.5321999788284302
train gradient:  0.14177444548496648
iteration : 6574
train acc:  0.703125
train loss:  0.5151742100715637
train gradient:  0.1477406112049836
iteration : 6575
train acc:  0.8046875
train loss:  0.4641544818878174
train gradient:  0.0966140838167198
iteration : 6576
train acc:  0.6875
train loss:  0.5763218402862549
train gradient:  0.21987816874134614
iteration : 6577
train acc:  0.6796875
train loss:  0.5713562965393066
train gradient:  0.16424920985814656
iteration : 6578
train acc:  0.7421875
train loss:  0.49631911516189575
train gradient:  0.11826125545004573
iteration : 6579
train acc:  0.71875
train loss:  0.5201510190963745
train gradient:  0.11709708693689869
iteration : 6580
train acc:  0.71875
train loss:  0.5122886896133423
train gradient:  0.1386793759426197
iteration : 6581
train acc:  0.71875
train loss:  0.5366586446762085
train gradient:  0.17968444451450932
iteration : 6582
train acc:  0.7421875
train loss:  0.4858449101448059
train gradient:  0.11564843778715778
iteration : 6583
train acc:  0.78125
train loss:  0.4700883626937866
train gradient:  0.1257990888375372
iteration : 6584
train acc:  0.7734375
train loss:  0.4763435125350952
train gradient:  0.10862024208033876
iteration : 6585
train acc:  0.8203125
train loss:  0.4766578674316406
train gradient:  0.13605468306368756
iteration : 6586
train acc:  0.671875
train loss:  0.5361313223838806
train gradient:  0.14172838041334107
iteration : 6587
train acc:  0.71875
train loss:  0.5148270130157471
train gradient:  0.12606299761528245
iteration : 6588
train acc:  0.671875
train loss:  0.597123920917511
train gradient:  0.20915778880992913
iteration : 6589
train acc:  0.78125
train loss:  0.4616759121417999
train gradient:  0.10989770012952618
iteration : 6590
train acc:  0.6953125
train loss:  0.5186436772346497
train gradient:  0.14132501728001656
iteration : 6591
train acc:  0.734375
train loss:  0.49061617255210876
train gradient:  0.14103688085461175
iteration : 6592
train acc:  0.734375
train loss:  0.5013503432273865
train gradient:  0.153161942607363
iteration : 6593
train acc:  0.7265625
train loss:  0.5417218208312988
train gradient:  0.16481241585246909
iteration : 6594
train acc:  0.8046875
train loss:  0.41418391466140747
train gradient:  0.11591929657106703
iteration : 6595
train acc:  0.7734375
train loss:  0.4538193941116333
train gradient:  0.10659469248325543
iteration : 6596
train acc:  0.796875
train loss:  0.4424688220024109
train gradient:  0.1134942552477775
iteration : 6597
train acc:  0.7578125
train loss:  0.4300045371055603
train gradient:  0.11870560576835044
iteration : 6598
train acc:  0.71875
train loss:  0.5339634418487549
train gradient:  0.1812631643731718
iteration : 6599
train acc:  0.6796875
train loss:  0.5293368101119995
train gradient:  0.14657402469967792
iteration : 6600
train acc:  0.765625
train loss:  0.45501548051834106
train gradient:  0.10923141908886505
iteration : 6601
train acc:  0.7265625
train loss:  0.5204007625579834
train gradient:  0.13636051502739835
iteration : 6602
train acc:  0.7265625
train loss:  0.5069785714149475
train gradient:  0.12630231891469174
iteration : 6603
train acc:  0.734375
train loss:  0.5071605443954468
train gradient:  0.14829531317294567
iteration : 6604
train acc:  0.78125
train loss:  0.46995267271995544
train gradient:  0.11482705635440152
iteration : 6605
train acc:  0.7421875
train loss:  0.4925628900527954
train gradient:  0.1262590292622695
iteration : 6606
train acc:  0.7109375
train loss:  0.5437769889831543
train gradient:  0.14726552580350605
iteration : 6607
train acc:  0.6875
train loss:  0.5442447662353516
train gradient:  0.13244645311620848
iteration : 6608
train acc:  0.765625
train loss:  0.4714518189430237
train gradient:  0.17506738191458493
iteration : 6609
train acc:  0.75
train loss:  0.4575999975204468
train gradient:  0.14231755067088325
iteration : 6610
train acc:  0.7109375
train loss:  0.5307445526123047
train gradient:  0.13720057067740624
iteration : 6611
train acc:  0.6640625
train loss:  0.5401378273963928
train gradient:  0.1280910006432553
iteration : 6612
train acc:  0.7890625
train loss:  0.46447986364364624
train gradient:  0.09638403402953595
iteration : 6613
train acc:  0.703125
train loss:  0.523196816444397
train gradient:  0.13695305150162299
iteration : 6614
train acc:  0.640625
train loss:  0.6319368481636047
train gradient:  0.23313642423411773
iteration : 6615
train acc:  0.7109375
train loss:  0.5512834787368774
train gradient:  0.1699637870340442
iteration : 6616
train acc:  0.71875
train loss:  0.4962496757507324
train gradient:  0.14615165982380085
iteration : 6617
train acc:  0.7265625
train loss:  0.5808589458465576
train gradient:  0.1425345181931734
iteration : 6618
train acc:  0.7578125
train loss:  0.47748368978500366
train gradient:  0.2251478544071877
iteration : 6619
train acc:  0.71875
train loss:  0.5260938405990601
train gradient:  0.1600469805631311
iteration : 6620
train acc:  0.7890625
train loss:  0.4455760419368744
train gradient:  0.11889401543967344
iteration : 6621
train acc:  0.75
train loss:  0.5253170132637024
train gradient:  0.1568901303106402
iteration : 6622
train acc:  0.671875
train loss:  0.5741935968399048
train gradient:  0.1570189371061323
iteration : 6623
train acc:  0.6796875
train loss:  0.533091127872467
train gradient:  0.16431890807845212
iteration : 6624
train acc:  0.7421875
train loss:  0.47555607557296753
train gradient:  0.1186856512023647
iteration : 6625
train acc:  0.6796875
train loss:  0.46773120760917664
train gradient:  0.11997585858973608
iteration : 6626
train acc:  0.7890625
train loss:  0.4255059063434601
train gradient:  0.09321749888322706
iteration : 6627
train acc:  0.703125
train loss:  0.5354049205780029
train gradient:  0.12628487831053914
iteration : 6628
train acc:  0.75
train loss:  0.49019086360931396
train gradient:  0.1552566024249013
iteration : 6629
train acc:  0.796875
train loss:  0.43168771266937256
train gradient:  0.10606556114654975
iteration : 6630
train acc:  0.8046875
train loss:  0.44593536853790283
train gradient:  0.10003316436582739
iteration : 6631
train acc:  0.7265625
train loss:  0.5294444561004639
train gradient:  0.13685633567156846
iteration : 6632
train acc:  0.7421875
train loss:  0.541530966758728
train gradient:  0.14025432053435144
iteration : 6633
train acc:  0.6953125
train loss:  0.5039469003677368
train gradient:  0.130072370936926
iteration : 6634
train acc:  0.7109375
train loss:  0.5484798550605774
train gradient:  0.17497633008026153
iteration : 6635
train acc:  0.765625
train loss:  0.4510183334350586
train gradient:  0.13478866424351854
iteration : 6636
train acc:  0.7734375
train loss:  0.4784041941165924
train gradient:  0.10794739494295912
iteration : 6637
train acc:  0.734375
train loss:  0.5174136161804199
train gradient:  0.12028601139651401
iteration : 6638
train acc:  0.734375
train loss:  0.5598533153533936
train gradient:  0.14915355496191324
iteration : 6639
train acc:  0.765625
train loss:  0.47551822662353516
train gradient:  0.13196151628273056
iteration : 6640
train acc:  0.7578125
train loss:  0.48804235458374023
train gradient:  0.12332820603426953
iteration : 6641
train acc:  0.7109375
train loss:  0.5367813110351562
train gradient:  0.14145473439344697
iteration : 6642
train acc:  0.7734375
train loss:  0.4929288625717163
train gradient:  0.14323251958480243
iteration : 6643
train acc:  0.65625
train loss:  0.6083929538726807
train gradient:  0.19903689670227748
iteration : 6644
train acc:  0.6953125
train loss:  0.5642060041427612
train gradient:  0.20021468945382337
iteration : 6645
train acc:  0.7734375
train loss:  0.44526809453964233
train gradient:  0.13717331317054493
iteration : 6646
train acc:  0.6953125
train loss:  0.5610464215278625
train gradient:  0.1887024333347027
iteration : 6647
train acc:  0.7734375
train loss:  0.4814687669277191
train gradient:  0.1541663139320698
iteration : 6648
train acc:  0.7109375
train loss:  0.526789665222168
train gradient:  0.17051726077655416
iteration : 6649
train acc:  0.7265625
train loss:  0.4940390884876251
train gradient:  0.1209704115103557
iteration : 6650
train acc:  0.8046875
train loss:  0.4292186498641968
train gradient:  0.11113542660234717
iteration : 6651
train acc:  0.7734375
train loss:  0.5203272104263306
train gradient:  0.10947555670872772
iteration : 6652
train acc:  0.71875
train loss:  0.5141515731811523
train gradient:  0.1447334346403622
iteration : 6653
train acc:  0.7421875
train loss:  0.5062273144721985
train gradient:  0.12914334121508925
iteration : 6654
train acc:  0.75
train loss:  0.48480623960494995
train gradient:  0.14148102373818897
iteration : 6655
train acc:  0.7421875
train loss:  0.4851861596107483
train gradient:  0.13449957223099757
iteration : 6656
train acc:  0.7109375
train loss:  0.544832706451416
train gradient:  0.21052363260944945
iteration : 6657
train acc:  0.734375
train loss:  0.5031895041465759
train gradient:  0.15189358051813123
iteration : 6658
train acc:  0.71875
train loss:  0.5530533790588379
train gradient:  0.17742613451377326
iteration : 6659
train acc:  0.734375
train loss:  0.4576968848705292
train gradient:  0.15127924571813767
iteration : 6660
train acc:  0.734375
train loss:  0.45472195744514465
train gradient:  0.11636494926372676
iteration : 6661
train acc:  0.734375
train loss:  0.5022463798522949
train gradient:  0.1851737162228686
iteration : 6662
train acc:  0.7734375
train loss:  0.49626579880714417
train gradient:  0.12978225529943715
iteration : 6663
train acc:  0.703125
train loss:  0.5426099300384521
train gradient:  0.13694349503308473
iteration : 6664
train acc:  0.7265625
train loss:  0.5217272043228149
train gradient:  0.1353220052345575
iteration : 6665
train acc:  0.7421875
train loss:  0.5155644416809082
train gradient:  0.1581733611899892
iteration : 6666
train acc:  0.765625
train loss:  0.5065913796424866
train gradient:  0.16733408784076642
iteration : 6667
train acc:  0.7734375
train loss:  0.5045905113220215
train gradient:  0.14821290012538213
iteration : 6668
train acc:  0.6796875
train loss:  0.5335612893104553
train gradient:  0.14648050723217856
iteration : 6669
train acc:  0.75
train loss:  0.47795242071151733
train gradient:  0.13721786200493105
iteration : 6670
train acc:  0.7578125
train loss:  0.4551675319671631
train gradient:  0.11625131331967441
iteration : 6671
train acc:  0.7265625
train loss:  0.47441089153289795
train gradient:  0.12510637295315308
iteration : 6672
train acc:  0.7734375
train loss:  0.5335639119148254
train gradient:  0.1883615431565428
iteration : 6673
train acc:  0.703125
train loss:  0.5373838543891907
train gradient:  0.15447671392346113
iteration : 6674
train acc:  0.8125
train loss:  0.43407368659973145
train gradient:  0.0988532162041504
iteration : 6675
train acc:  0.7734375
train loss:  0.5108849406242371
train gradient:  0.10682291176044809
iteration : 6676
train acc:  0.703125
train loss:  0.5527728199958801
train gradient:  0.16900839907073242
iteration : 6677
train acc:  0.765625
train loss:  0.44963136315345764
train gradient:  0.12587944265303214
iteration : 6678
train acc:  0.7578125
train loss:  0.4901309609413147
train gradient:  0.12887999446665177
iteration : 6679
train acc:  0.796875
train loss:  0.45914584398269653
train gradient:  0.12266448032640605
iteration : 6680
train acc:  0.796875
train loss:  0.4524739980697632
train gradient:  0.11988853866216877
iteration : 6681
train acc:  0.75
train loss:  0.5117266774177551
train gradient:  0.16058088656353592
iteration : 6682
train acc:  0.7890625
train loss:  0.48937302827835083
train gradient:  0.14008264644409876
iteration : 6683
train acc:  0.7734375
train loss:  0.46143174171447754
train gradient:  0.13206624454413002
iteration : 6684
train acc:  0.8203125
train loss:  0.4195249676704407
train gradient:  0.11767633899121309
iteration : 6685
train acc:  0.734375
train loss:  0.5280500054359436
train gradient:  0.13254837102221878
iteration : 6686
train acc:  0.84375
train loss:  0.40538281202316284
train gradient:  0.1206606341763115
iteration : 6687
train acc:  0.765625
train loss:  0.4539202153682709
train gradient:  0.11391069960520243
iteration : 6688
train acc:  0.734375
train loss:  0.5145070552825928
train gradient:  0.13385569331541608
iteration : 6689
train acc:  0.71875
train loss:  0.5427603125572205
train gradient:  0.18070256543716226
iteration : 6690
train acc:  0.78125
train loss:  0.47555285692214966
train gradient:  0.11427841753341332
iteration : 6691
train acc:  0.7890625
train loss:  0.39985406398773193
train gradient:  0.11781722583719077
iteration : 6692
train acc:  0.7109375
train loss:  0.5282393097877502
train gradient:  0.1784467944037998
iteration : 6693
train acc:  0.8046875
train loss:  0.4242594242095947
train gradient:  0.11863581494329112
iteration : 6694
train acc:  0.71875
train loss:  0.547551155090332
train gradient:  0.15739669902167971
iteration : 6695
train acc:  0.703125
train loss:  0.5536363124847412
train gradient:  0.14876649834779612
iteration : 6696
train acc:  0.7734375
train loss:  0.4430890381336212
train gradient:  0.11232668128841261
iteration : 6697
train acc:  0.796875
train loss:  0.4430658221244812
train gradient:  0.12857851805435
iteration : 6698
train acc:  0.7265625
train loss:  0.5630438327789307
train gradient:  0.183000080773789
iteration : 6699
train acc:  0.6875
train loss:  0.5401571989059448
train gradient:  0.2099278111486505
iteration : 6700
train acc:  0.7109375
train loss:  0.5068625211715698
train gradient:  0.17377877911697115
iteration : 6701
train acc:  0.765625
train loss:  0.4578707814216614
train gradient:  0.10826656605470988
iteration : 6702
train acc:  0.7109375
train loss:  0.49735575914382935
train gradient:  0.1445736920896356
iteration : 6703
train acc:  0.7421875
train loss:  0.4825604557991028
train gradient:  0.1495063353296344
iteration : 6704
train acc:  0.75
train loss:  0.5138964653015137
train gradient:  0.16058574094942285
iteration : 6705
train acc:  0.671875
train loss:  0.5796105861663818
train gradient:  0.16896609476213242
iteration : 6706
train acc:  0.7734375
train loss:  0.4340352416038513
train gradient:  0.10103884531461164
iteration : 6707
train acc:  0.7109375
train loss:  0.5311363935470581
train gradient:  0.16473862851526333
iteration : 6708
train acc:  0.734375
train loss:  0.49695146083831787
train gradient:  0.13038515602666995
iteration : 6709
train acc:  0.6796875
train loss:  0.5525896549224854
train gradient:  0.14552798365538674
iteration : 6710
train acc:  0.6953125
train loss:  0.5294699668884277
train gradient:  0.13276993716937768
iteration : 6711
train acc:  0.7109375
train loss:  0.5024822950363159
train gradient:  0.1319960856189004
iteration : 6712
train acc:  0.796875
train loss:  0.45208263397216797
train gradient:  0.10982808065680934
iteration : 6713
train acc:  0.6796875
train loss:  0.5188396573066711
train gradient:  0.19166761724900644
iteration : 6714
train acc:  0.6953125
train loss:  0.5543563365936279
train gradient:  0.19748727608441022
iteration : 6715
train acc:  0.7578125
train loss:  0.44824934005737305
train gradient:  0.12581577068849625
iteration : 6716
train acc:  0.71875
train loss:  0.5619388818740845
train gradient:  0.15861904322920528
iteration : 6717
train acc:  0.703125
train loss:  0.5303459167480469
train gradient:  0.17917746544610846
iteration : 6718
train acc:  0.7109375
train loss:  0.5473082065582275
train gradient:  0.14666358815434888
iteration : 6719
train acc:  0.7265625
train loss:  0.47455984354019165
train gradient:  0.15239808563237534
iteration : 6720
train acc:  0.703125
train loss:  0.4866791367530823
train gradient:  0.11846160788901107
iteration : 6721
train acc:  0.7578125
train loss:  0.48880070447921753
train gradient:  0.14806589838204204
iteration : 6722
train acc:  0.6953125
train loss:  0.5259243845939636
train gradient:  0.17232487273106237
iteration : 6723
train acc:  0.8046875
train loss:  0.44785699248313904
train gradient:  0.0866555458708979
iteration : 6724
train acc:  0.703125
train loss:  0.5551575422286987
train gradient:  0.17038808214275386
iteration : 6725
train acc:  0.78125
train loss:  0.48051732778549194
train gradient:  0.12951646748096485
iteration : 6726
train acc:  0.7421875
train loss:  0.47465986013412476
train gradient:  0.14430951946405393
iteration : 6727
train acc:  0.7265625
train loss:  0.5239289999008179
train gradient:  0.1713794247788506
iteration : 6728
train acc:  0.671875
train loss:  0.613795280456543
train gradient:  0.2557863456350839
iteration : 6729
train acc:  0.78125
train loss:  0.4423977732658386
train gradient:  0.13545347077844413
iteration : 6730
train acc:  0.75
train loss:  0.493682861328125
train gradient:  0.1239603674742118
iteration : 6731
train acc:  0.71875
train loss:  0.5003699660301208
train gradient:  0.12110005898994482
iteration : 6732
train acc:  0.734375
train loss:  0.48790693283081055
train gradient:  0.14492212378929675
iteration : 6733
train acc:  0.7734375
train loss:  0.4798492193222046
train gradient:  0.1290340229285218
iteration : 6734
train acc:  0.703125
train loss:  0.4959006905555725
train gradient:  0.1423843453335535
iteration : 6735
train acc:  0.75
train loss:  0.500665545463562
train gradient:  0.15050224941153553
iteration : 6736
train acc:  0.7265625
train loss:  0.4795576333999634
train gradient:  0.128728270275704
iteration : 6737
train acc:  0.65625
train loss:  0.5967725515365601
train gradient:  0.20841336672844035
iteration : 6738
train acc:  0.734375
train loss:  0.5101592540740967
train gradient:  0.13981139901595813
iteration : 6739
train acc:  0.7421875
train loss:  0.47118231654167175
train gradient:  0.12005163540615528
iteration : 6740
train acc:  0.796875
train loss:  0.46140003204345703
train gradient:  0.09702194315546621
iteration : 6741
train acc:  0.703125
train loss:  0.5549788475036621
train gradient:  0.20642357258032012
iteration : 6742
train acc:  0.796875
train loss:  0.45382174849510193
train gradient:  0.1429577157848664
iteration : 6743
train acc:  0.703125
train loss:  0.5718203783035278
train gradient:  0.1754577057862376
iteration : 6744
train acc:  0.7734375
train loss:  0.4419932961463928
train gradient:  0.12039197057382242
iteration : 6745
train acc:  0.7578125
train loss:  0.4811611473560333
train gradient:  0.16519413217443135
iteration : 6746
train acc:  0.703125
train loss:  0.5693172216415405
train gradient:  0.17792448053329885
iteration : 6747
train acc:  0.6640625
train loss:  0.533161997795105
train gradient:  0.12653378527753822
iteration : 6748
train acc:  0.7734375
train loss:  0.5158168077468872
train gradient:  0.16038980305394762
iteration : 6749
train acc:  0.7421875
train loss:  0.5181515216827393
train gradient:  0.17292734957344386
iteration : 6750
train acc:  0.6796875
train loss:  0.5597528219223022
train gradient:  0.17832210792701236
iteration : 6751
train acc:  0.7421875
train loss:  0.4761073887348175
train gradient:  0.11323708834436474
iteration : 6752
train acc:  0.8046875
train loss:  0.4485991895198822
train gradient:  0.12845574093093415
iteration : 6753
train acc:  0.6171875
train loss:  0.6098182201385498
train gradient:  0.20011956441249953
iteration : 6754
train acc:  0.75
train loss:  0.4869910478591919
train gradient:  0.12644949650172504
iteration : 6755
train acc:  0.7734375
train loss:  0.4296067953109741
train gradient:  0.10900511751570441
iteration : 6756
train acc:  0.8046875
train loss:  0.4424656629562378
train gradient:  0.10986559305451966
iteration : 6757
train acc:  0.75
train loss:  0.5055513381958008
train gradient:  0.13808371581472764
iteration : 6758
train acc:  0.71875
train loss:  0.5242894887924194
train gradient:  0.11543172792370326
iteration : 6759
train acc:  0.7578125
train loss:  0.5411871671676636
train gradient:  0.158421893825864
iteration : 6760
train acc:  0.7109375
train loss:  0.5391508340835571
train gradient:  0.15047452673329242
iteration : 6761
train acc:  0.7421875
train loss:  0.46836015582084656
train gradient:  0.1316113098829306
iteration : 6762
train acc:  0.7265625
train loss:  0.48336994647979736
train gradient:  0.12728824483723122
iteration : 6763
train acc:  0.7734375
train loss:  0.4753902554512024
train gradient:  0.11399522924291156
iteration : 6764
train acc:  0.7578125
train loss:  0.46687135100364685
train gradient:  0.12399051303389083
iteration : 6765
train acc:  0.6953125
train loss:  0.5492003560066223
train gradient:  0.15334188655960657
iteration : 6766
train acc:  0.7734375
train loss:  0.45806801319122314
train gradient:  0.13988083647196464
iteration : 6767
train acc:  0.78125
train loss:  0.4613233804702759
train gradient:  0.1552639758259443
iteration : 6768
train acc:  0.7578125
train loss:  0.4485775828361511
train gradient:  0.0843071418531827
iteration : 6769
train acc:  0.7734375
train loss:  0.4474378824234009
train gradient:  0.11232893638813995
iteration : 6770
train acc:  0.7109375
train loss:  0.5330837965011597
train gradient:  0.13361409483604614
iteration : 6771
train acc:  0.796875
train loss:  0.45036107301712036
train gradient:  0.12207773527345107
iteration : 6772
train acc:  0.734375
train loss:  0.49593043327331543
train gradient:  0.10930597507063336
iteration : 6773
train acc:  0.7265625
train loss:  0.5099464654922485
train gradient:  0.1444286822844406
iteration : 6774
train acc:  0.71875
train loss:  0.4939240515232086
train gradient:  0.11768009420584344
iteration : 6775
train acc:  0.7734375
train loss:  0.5101783871650696
train gradient:  0.13424943704614184
iteration : 6776
train acc:  0.7578125
train loss:  0.49140065908432007
train gradient:  0.16824333515814188
iteration : 6777
train acc:  0.6640625
train loss:  0.5703885555267334
train gradient:  0.19523318759505615
iteration : 6778
train acc:  0.7265625
train loss:  0.5023881196975708
train gradient:  0.14033650353892926
iteration : 6779
train acc:  0.7109375
train loss:  0.5348937511444092
train gradient:  0.15127517260783235
iteration : 6780
train acc:  0.75
train loss:  0.499125212430954
train gradient:  0.17259455851132888
iteration : 6781
train acc:  0.6875
train loss:  0.5431342720985413
train gradient:  0.16817312421997288
iteration : 6782
train acc:  0.7421875
train loss:  0.5125437378883362
train gradient:  0.15786852662367684
iteration : 6783
train acc:  0.6953125
train loss:  0.5610030889511108
train gradient:  0.18677517019394557
iteration : 6784
train acc:  0.7109375
train loss:  0.5022702217102051
train gradient:  0.13745272736026148
iteration : 6785
train acc:  0.7578125
train loss:  0.4731907248497009
train gradient:  0.15438893278322174
iteration : 6786
train acc:  0.796875
train loss:  0.471327543258667
train gradient:  0.12985764950299628
iteration : 6787
train acc:  0.71875
train loss:  0.5108306407928467
train gradient:  0.11693731537137625
iteration : 6788
train acc:  0.6875
train loss:  0.5209832787513733
train gradient:  0.14612632419149246
iteration : 6789
train acc:  0.7421875
train loss:  0.5033224821090698
train gradient:  0.12450874553095177
iteration : 6790
train acc:  0.6875
train loss:  0.5713697671890259
train gradient:  0.21530178451923482
iteration : 6791
train acc:  0.7578125
train loss:  0.5024647116661072
train gradient:  0.12233589531503432
iteration : 6792
train acc:  0.765625
train loss:  0.445963591337204
train gradient:  0.12280660604776468
iteration : 6793
train acc:  0.796875
train loss:  0.4476303458213806
train gradient:  0.14950565991486986
iteration : 6794
train acc:  0.6953125
train loss:  0.501273512840271
train gradient:  0.15017441410717952
iteration : 6795
train acc:  0.7578125
train loss:  0.47172266244888306
train gradient:  0.14539978582143326
iteration : 6796
train acc:  0.75
train loss:  0.4781639575958252
train gradient:  0.13180663184693472
iteration : 6797
train acc:  0.7109375
train loss:  0.46502429246902466
train gradient:  0.12434334253623848
iteration : 6798
train acc:  0.7265625
train loss:  0.4571652412414551
train gradient:  0.11398427319408642
iteration : 6799
train acc:  0.78125
train loss:  0.49596258997917175
train gradient:  0.1333349293217455
iteration : 6800
train acc:  0.796875
train loss:  0.43303370475769043
train gradient:  0.14406216234382765
iteration : 6801
train acc:  0.7421875
train loss:  0.5384608507156372
train gradient:  0.14161193308496584
iteration : 6802
train acc:  0.7421875
train loss:  0.4922068119049072
train gradient:  0.12177860681132421
iteration : 6803
train acc:  0.75
train loss:  0.46633508801460266
train gradient:  0.1762448528074303
iteration : 6804
train acc:  0.7421875
train loss:  0.4538656771183014
train gradient:  0.11553237452277887
iteration : 6805
train acc:  0.734375
train loss:  0.5293025374412537
train gradient:  0.2316689465114323
iteration : 6806
train acc:  0.703125
train loss:  0.5146932601928711
train gradient:  0.15964897748902585
iteration : 6807
train acc:  0.7578125
train loss:  0.5451721549034119
train gradient:  0.15598283291947312
iteration : 6808
train acc:  0.7265625
train loss:  0.5404207706451416
train gradient:  0.12630672185019456
iteration : 6809
train acc:  0.7265625
train loss:  0.5615313053131104
train gradient:  0.14061314606936737
iteration : 6810
train acc:  0.6875
train loss:  0.5422272682189941
train gradient:  0.15311332162560665
iteration : 6811
train acc:  0.7734375
train loss:  0.48384833335876465
train gradient:  0.12881622768340473
iteration : 6812
train acc:  0.78125
train loss:  0.4248064160346985
train gradient:  0.11552014271034997
iteration : 6813
train acc:  0.6796875
train loss:  0.5310445427894592
train gradient:  0.1269728475126423
iteration : 6814
train acc:  0.7265625
train loss:  0.5680911540985107
train gradient:  0.1824488650714795
iteration : 6815
train acc:  0.75
train loss:  0.4721066951751709
train gradient:  0.13460708845512767
iteration : 6816
train acc:  0.734375
train loss:  0.48840048909187317
train gradient:  0.1351416058984694
iteration : 6817
train acc:  0.7109375
train loss:  0.4470483660697937
train gradient:  0.11969766973526708
iteration : 6818
train acc:  0.7265625
train loss:  0.5096808671951294
train gradient:  0.12709313041786252
iteration : 6819
train acc:  0.7578125
train loss:  0.4876742660999298
train gradient:  0.14523920319193123
iteration : 6820
train acc:  0.7734375
train loss:  0.476900190114975
train gradient:  0.11669338144873516
iteration : 6821
train acc:  0.75
train loss:  0.4985125660896301
train gradient:  0.14136560182917962
iteration : 6822
train acc:  0.75
train loss:  0.4981831908226013
train gradient:  0.1779404193342764
iteration : 6823
train acc:  0.7578125
train loss:  0.4503976106643677
train gradient:  0.13710763927503522
iteration : 6824
train acc:  0.6875
train loss:  0.5440067648887634
train gradient:  0.2279095317341595
iteration : 6825
train acc:  0.6796875
train loss:  0.6003307104110718
train gradient:  0.2212259998606695
iteration : 6826
train acc:  0.796875
train loss:  0.4661867022514343
train gradient:  0.11333685766992381
iteration : 6827
train acc:  0.7109375
train loss:  0.5354074239730835
train gradient:  0.16212388139699746
iteration : 6828
train acc:  0.7578125
train loss:  0.4713836908340454
train gradient:  0.12064757201701597
iteration : 6829
train acc:  0.765625
train loss:  0.45577526092529297
train gradient:  0.12611212881665515
iteration : 6830
train acc:  0.71875
train loss:  0.46415460109710693
train gradient:  0.1447704214199375
iteration : 6831
train acc:  0.796875
train loss:  0.4525773227214813
train gradient:  0.12063348310922019
iteration : 6832
train acc:  0.765625
train loss:  0.4886787533760071
train gradient:  0.15699705776173845
iteration : 6833
train acc:  0.7109375
train loss:  0.505829930305481
train gradient:  0.1616510763885905
iteration : 6834
train acc:  0.6875
train loss:  0.5567089319229126
train gradient:  0.13999664083963143
iteration : 6835
train acc:  0.7578125
train loss:  0.47921085357666016
train gradient:  0.11309945868555824
iteration : 6836
train acc:  0.6796875
train loss:  0.586487889289856
train gradient:  0.1898912537468594
iteration : 6837
train acc:  0.7578125
train loss:  0.5006980895996094
train gradient:  0.17088067947184116
iteration : 6838
train acc:  0.703125
train loss:  0.5740662813186646
train gradient:  0.22341702444059483
iteration : 6839
train acc:  0.765625
train loss:  0.5043513774871826
train gradient:  0.14161136136219687
iteration : 6840
train acc:  0.7578125
train loss:  0.5409069061279297
train gradient:  0.1672626413302632
iteration : 6841
train acc:  0.7265625
train loss:  0.494520902633667
train gradient:  0.1244323054176029
iteration : 6842
train acc:  0.796875
train loss:  0.4290451109409332
train gradient:  0.11550356558219604
iteration : 6843
train acc:  0.7265625
train loss:  0.5584156513214111
train gradient:  0.18182357553142697
iteration : 6844
train acc:  0.71875
train loss:  0.5316140651702881
train gradient:  0.13921771601627933
iteration : 6845
train acc:  0.765625
train loss:  0.4287022054195404
train gradient:  0.13218537285604676
iteration : 6846
train acc:  0.7578125
train loss:  0.49743732810020447
train gradient:  0.15510861903223455
iteration : 6847
train acc:  0.7109375
train loss:  0.5166102051734924
train gradient:  0.11565696707067072
iteration : 6848
train acc:  0.75
train loss:  0.4608823359012604
train gradient:  0.14992595691531088
iteration : 6849
train acc:  0.75
train loss:  0.47602561116218567
train gradient:  0.1097573189450753
iteration : 6850
train acc:  0.7265625
train loss:  0.53522789478302
train gradient:  0.161704635999778
iteration : 6851
train acc:  0.7421875
train loss:  0.5397692918777466
train gradient:  0.17095820029449832
iteration : 6852
train acc:  0.796875
train loss:  0.4802365303039551
train gradient:  0.1327850728142017
iteration : 6853
train acc:  0.7890625
train loss:  0.42516830563545227
train gradient:  0.1038063887191089
iteration : 6854
train acc:  0.7734375
train loss:  0.4550793468952179
train gradient:  0.11009579584256377
iteration : 6855
train acc:  0.8046875
train loss:  0.4870549738407135
train gradient:  0.17339976486780223
iteration : 6856
train acc:  0.7265625
train loss:  0.5047580003738403
train gradient:  0.1424193319763034
iteration : 6857
train acc:  0.765625
train loss:  0.4804477393627167
train gradient:  0.10159510196286192
iteration : 6858
train acc:  0.7109375
train loss:  0.4851630926132202
train gradient:  0.1168474293435494
iteration : 6859
train acc:  0.71875
train loss:  0.45077040791511536
train gradient:  0.13962021981013528
iteration : 6860
train acc:  0.7578125
train loss:  0.4786533713340759
train gradient:  0.11730903636746494
iteration : 6861
train acc:  0.7890625
train loss:  0.444667786359787
train gradient:  0.11696338767071567
iteration : 6862
train acc:  0.7421875
train loss:  0.4654722809791565
train gradient:  0.10627000953726468
iteration : 6863
train acc:  0.7734375
train loss:  0.4627275764942169
train gradient:  0.12514502572759664
iteration : 6864
train acc:  0.734375
train loss:  0.5246997475624084
train gradient:  0.12839539236473863
iteration : 6865
train acc:  0.7265625
train loss:  0.4739982485771179
train gradient:  0.10814785140787034
iteration : 6866
train acc:  0.765625
train loss:  0.5353298187255859
train gradient:  0.1301727356333099
iteration : 6867
train acc:  0.8046875
train loss:  0.44734224677085876
train gradient:  0.11940293615543911
iteration : 6868
train acc:  0.78125
train loss:  0.4369303286075592
train gradient:  0.10405741876449665
iteration : 6869
train acc:  0.6953125
train loss:  0.5609035491943359
train gradient:  0.1704756225995545
iteration : 6870
train acc:  0.7578125
train loss:  0.4867587387561798
train gradient:  0.1428321850224682
iteration : 6871
train acc:  0.796875
train loss:  0.4294884502887726
train gradient:  0.11692488271089982
iteration : 6872
train acc:  0.796875
train loss:  0.44732972979545593
train gradient:  0.12717620194872514
iteration : 6873
train acc:  0.7734375
train loss:  0.4691336154937744
train gradient:  0.1474043581254242
iteration : 6874
train acc:  0.6640625
train loss:  0.5545015931129456
train gradient:  0.15295625193621987
iteration : 6875
train acc:  0.6875
train loss:  0.5539757609367371
train gradient:  0.17453083316100737
iteration : 6876
train acc:  0.7578125
train loss:  0.483781099319458
train gradient:  0.1351169853586941
iteration : 6877
train acc:  0.703125
train loss:  0.5984129905700684
train gradient:  0.18874461367929768
iteration : 6878
train acc:  0.7265625
train loss:  0.5471470355987549
train gradient:  0.17560331994088835
iteration : 6879
train acc:  0.75
train loss:  0.48541003465652466
train gradient:  0.11280359043626151
iteration : 6880
train acc:  0.75
train loss:  0.4694802165031433
train gradient:  0.13883587277018533
iteration : 6881
train acc:  0.7421875
train loss:  0.5122527480125427
train gradient:  0.18106891436535846
iteration : 6882
train acc:  0.7578125
train loss:  0.46047908067703247
train gradient:  0.1543082559639439
iteration : 6883
train acc:  0.71875
train loss:  0.44334205985069275
train gradient:  0.11650492397470705
iteration : 6884
train acc:  0.7421875
train loss:  0.4801863431930542
train gradient:  0.16880428490936086
iteration : 6885
train acc:  0.703125
train loss:  0.5448644161224365
train gradient:  0.15291514221412308
iteration : 6886
train acc:  0.765625
train loss:  0.48281556367874146
train gradient:  0.12574168829166404
iteration : 6887
train acc:  0.734375
train loss:  0.5209266543388367
train gradient:  0.13631114937395222
iteration : 6888
train acc:  0.75
train loss:  0.49862608313560486
train gradient:  0.13290502671286009
iteration : 6889
train acc:  0.671875
train loss:  0.5709410905838013
train gradient:  0.17611784857650092
iteration : 6890
train acc:  0.7421875
train loss:  0.47246846556663513
train gradient:  0.1184446677207444
iteration : 6891
train acc:  0.6953125
train loss:  0.5372704267501831
train gradient:  0.17803396039111263
iteration : 6892
train acc:  0.6484375
train loss:  0.5514131784439087
train gradient:  0.17170150272554038
iteration : 6893
train acc:  0.75
train loss:  0.48244142532348633
train gradient:  0.14225787132600465
iteration : 6894
train acc:  0.765625
train loss:  0.4737393260002136
train gradient:  0.10825223030756963
iteration : 6895
train acc:  0.734375
train loss:  0.466526061296463
train gradient:  0.12152251086017345
iteration : 6896
train acc:  0.65625
train loss:  0.5521818399429321
train gradient:  0.18844404260882486
iteration : 6897
train acc:  0.734375
train loss:  0.5121740102767944
train gradient:  0.13871642036932813
iteration : 6898
train acc:  0.703125
train loss:  0.5240200757980347
train gradient:  0.14892032150518
iteration : 6899
train acc:  0.75
train loss:  0.5408608913421631
train gradient:  0.13902142075190643
iteration : 6900
train acc:  0.7734375
train loss:  0.47838813066482544
train gradient:  0.15573593385138784
iteration : 6901
train acc:  0.703125
train loss:  0.566511332988739
train gradient:  0.19020300139902485
iteration : 6902
train acc:  0.6875
train loss:  0.5725148916244507
train gradient:  0.2001251214738578
iteration : 6903
train acc:  0.71875
train loss:  0.5148338079452515
train gradient:  0.16275648294688194
iteration : 6904
train acc:  0.7421875
train loss:  0.4778923988342285
train gradient:  0.10418212870272199
iteration : 6905
train acc:  0.78125
train loss:  0.43877729773521423
train gradient:  0.12115452471109107
iteration : 6906
train acc:  0.71875
train loss:  0.5068098306655884
train gradient:  0.15389285905139177
iteration : 6907
train acc:  0.7578125
train loss:  0.5039927363395691
train gradient:  0.15520633237797885
iteration : 6908
train acc:  0.75
train loss:  0.4795076251029968
train gradient:  0.16250362134653423
iteration : 6909
train acc:  0.7890625
train loss:  0.4609431028366089
train gradient:  0.09673601081746727
iteration : 6910
train acc:  0.6953125
train loss:  0.5563327670097351
train gradient:  0.13757717946580794
iteration : 6911
train acc:  0.640625
train loss:  0.6237866878509521
train gradient:  0.20735693185108778
iteration : 6912
train acc:  0.765625
train loss:  0.4755081534385681
train gradient:  0.17195506525758925
iteration : 6913
train acc:  0.75
train loss:  0.5113407373428345
train gradient:  0.14639188073113635
iteration : 6914
train acc:  0.734375
train loss:  0.5130376815795898
train gradient:  0.1494896206946531
iteration : 6915
train acc:  0.6796875
train loss:  0.5009276866912842
train gradient:  0.13264477534423985
iteration : 6916
train acc:  0.8125
train loss:  0.48632335662841797
train gradient:  0.12041036433277856
iteration : 6917
train acc:  0.7109375
train loss:  0.48687320947647095
train gradient:  0.11779953933630558
iteration : 6918
train acc:  0.765625
train loss:  0.50242680311203
train gradient:  0.11783847905364034
iteration : 6919
train acc:  0.734375
train loss:  0.5336422324180603
train gradient:  0.1468569292070176
iteration : 6920
train acc:  0.7890625
train loss:  0.4960547089576721
train gradient:  0.1349476260979935
iteration : 6921
train acc:  0.734375
train loss:  0.5028897523880005
train gradient:  0.12380064994116177
iteration : 6922
train acc:  0.78125
train loss:  0.4619094431400299
train gradient:  0.12757940054004308
iteration : 6923
train acc:  0.78125
train loss:  0.4759558439254761
train gradient:  0.11872046692281518
iteration : 6924
train acc:  0.6875
train loss:  0.5253816843032837
train gradient:  0.16337962426730718
iteration : 6925
train acc:  0.7265625
train loss:  0.5135101079940796
train gradient:  0.1362400702027804
iteration : 6926
train acc:  0.8125
train loss:  0.4596210718154907
train gradient:  0.12342604266812145
iteration : 6927
train acc:  0.7109375
train loss:  0.5203494429588318
train gradient:  0.17023801746554515
iteration : 6928
train acc:  0.6796875
train loss:  0.5375277996063232
train gradient:  0.1665472269602593
iteration : 6929
train acc:  0.671875
train loss:  0.5279917120933533
train gradient:  0.1438049356151621
iteration : 6930
train acc:  0.7890625
train loss:  0.4550222158432007
train gradient:  0.13420822261183496
iteration : 6931
train acc:  0.796875
train loss:  0.46314311027526855
train gradient:  0.11868751062808756
iteration : 6932
train acc:  0.6953125
train loss:  0.575410008430481
train gradient:  0.17081037341498673
iteration : 6933
train acc:  0.6953125
train loss:  0.5685833692550659
train gradient:  0.20780801331264048
iteration : 6934
train acc:  0.7421875
train loss:  0.4703683853149414
train gradient:  0.11856880175702268
iteration : 6935
train acc:  0.75
train loss:  0.5159907341003418
train gradient:  0.1373427479239117
iteration : 6936
train acc:  0.78125
train loss:  0.4536030888557434
train gradient:  0.1080009048501195
iteration : 6937
train acc:  0.7578125
train loss:  0.5051557421684265
train gradient:  0.13388489947952875
iteration : 6938
train acc:  0.6875
train loss:  0.5801348686218262
train gradient:  0.18576331249859912
iteration : 6939
train acc:  0.65625
train loss:  0.5167304873466492
train gradient:  0.15976736460187874
iteration : 6940
train acc:  0.6953125
train loss:  0.5570772886276245
train gradient:  0.2046805116513174
iteration : 6941
train acc:  0.7109375
train loss:  0.5239672064781189
train gradient:  0.13747640567400457
iteration : 6942
train acc:  0.7421875
train loss:  0.47463592886924744
train gradient:  0.12504025912474165
iteration : 6943
train acc:  0.7734375
train loss:  0.44808337092399597
train gradient:  0.09884080563747577
iteration : 6944
train acc:  0.7421875
train loss:  0.48947572708129883
train gradient:  0.14949031482053007
iteration : 6945
train acc:  0.8203125
train loss:  0.41512036323547363
train gradient:  0.09268544260886233
iteration : 6946
train acc:  0.7109375
train loss:  0.5468292236328125
train gradient:  0.17817418984437416
iteration : 6947
train acc:  0.6953125
train loss:  0.5098962783813477
train gradient:  0.13231145250430199
iteration : 6948
train acc:  0.7109375
train loss:  0.5683891177177429
train gradient:  0.18382003222673768
iteration : 6949
train acc:  0.6875
train loss:  0.5369787812232971
train gradient:  0.23351773059116532
iteration : 6950
train acc:  0.7578125
train loss:  0.5123595595359802
train gradient:  0.14770324827954118
iteration : 6951
train acc:  0.703125
train loss:  0.5170232057571411
train gradient:  0.15034484041772767
iteration : 6952
train acc:  0.6875
train loss:  0.500897228717804
train gradient:  0.1510004293886598
iteration : 6953
train acc:  0.6953125
train loss:  0.5549856424331665
train gradient:  0.18182149987585555
iteration : 6954
train acc:  0.734375
train loss:  0.46289169788360596
train gradient:  0.11130689834802711
iteration : 6955
train acc:  0.6953125
train loss:  0.576911211013794
train gradient:  0.16547567753912445
iteration : 6956
train acc:  0.78125
train loss:  0.4604684114456177
train gradient:  0.12233738088847457
iteration : 6957
train acc:  0.796875
train loss:  0.4853677749633789
train gradient:  0.14069419573148667
iteration : 6958
train acc:  0.78125
train loss:  0.45925796031951904
train gradient:  0.11273458579082653
iteration : 6959
train acc:  0.7578125
train loss:  0.5221813917160034
train gradient:  0.14673812493194377
iteration : 6960
train acc:  0.7265625
train loss:  0.4895400106906891
train gradient:  0.1415534916288274
iteration : 6961
train acc:  0.6953125
train loss:  0.5455848574638367
train gradient:  0.13216680339453146
iteration : 6962
train acc:  0.765625
train loss:  0.4712819755077362
train gradient:  0.12127557210416677
iteration : 6963
train acc:  0.734375
train loss:  0.4737997353076935
train gradient:  0.12927326523443178
iteration : 6964
train acc:  0.78125
train loss:  0.45212888717651367
train gradient:  0.10026600203853604
iteration : 6965
train acc:  0.6953125
train loss:  0.5690081119537354
train gradient:  0.1865454787680072
iteration : 6966
train acc:  0.71875
train loss:  0.5132038593292236
train gradient:  0.149806524138074
iteration : 6967
train acc:  0.734375
train loss:  0.4678099453449249
train gradient:  0.12554958976725472
iteration : 6968
train acc:  0.7890625
train loss:  0.4349900484085083
train gradient:  0.10209054479531411
iteration : 6969
train acc:  0.6875
train loss:  0.517827033996582
train gradient:  0.16701939206022953
iteration : 6970
train acc:  0.8125
train loss:  0.4294823408126831
train gradient:  0.12509581171303075
iteration : 6971
train acc:  0.7734375
train loss:  0.5104357004165649
train gradient:  0.15098126843963003
iteration : 6972
train acc:  0.71875
train loss:  0.5552224516868591
train gradient:  0.15744941357830497
iteration : 6973
train acc:  0.734375
train loss:  0.5144407749176025
train gradient:  0.16655053048543023
iteration : 6974
train acc:  0.7265625
train loss:  0.4692865312099457
train gradient:  0.1552828926281843
iteration : 6975
train acc:  0.78125
train loss:  0.4688073396682739
train gradient:  0.13280775612834578
iteration : 6976
train acc:  0.734375
train loss:  0.5302399396896362
train gradient:  0.20245635810423662
iteration : 6977
train acc:  0.75
train loss:  0.4691983759403229
train gradient:  0.17641295460214912
iteration : 6978
train acc:  0.71875
train loss:  0.49623000621795654
train gradient:  0.15115363468043308
iteration : 6979
train acc:  0.671875
train loss:  0.5338243246078491
train gradient:  0.16295213900761413
iteration : 6980
train acc:  0.7109375
train loss:  0.5701713562011719
train gradient:  0.22541669588339186
iteration : 6981
train acc:  0.6953125
train loss:  0.5745041370391846
train gradient:  0.1638558751105911
iteration : 6982
train acc:  0.75
train loss:  0.5021817684173584
train gradient:  0.13559401030239893
iteration : 6983
train acc:  0.703125
train loss:  0.5372201204299927
train gradient:  0.14779254910994852
iteration : 6984
train acc:  0.703125
train loss:  0.5526071786880493
train gradient:  0.145736511568067
iteration : 6985
train acc:  0.75
train loss:  0.4474153518676758
train gradient:  0.11099294293956602
iteration : 6986
train acc:  0.765625
train loss:  0.42887768149375916
train gradient:  0.14480870182376013
iteration : 6987
train acc:  0.7421875
train loss:  0.4778357148170471
train gradient:  0.16215963813583922
iteration : 6988
train acc:  0.7734375
train loss:  0.45885956287384033
train gradient:  0.11783043302321738
iteration : 6989
train acc:  0.765625
train loss:  0.5159254670143127
train gradient:  0.19230692011650263
iteration : 6990
train acc:  0.8046875
train loss:  0.4890439510345459
train gradient:  0.13900707636702392
iteration : 6991
train acc:  0.703125
train loss:  0.5026147365570068
train gradient:  0.11498532936602132
iteration : 6992
train acc:  0.6796875
train loss:  0.5832687616348267
train gradient:  0.21216771350988467
iteration : 6993
train acc:  0.75
train loss:  0.4967871904373169
train gradient:  0.12634488931233287
iteration : 6994
train acc:  0.7265625
train loss:  0.5046718716621399
train gradient:  0.1550593691925799
iteration : 6995
train acc:  0.7578125
train loss:  0.45951247215270996
train gradient:  0.11178971055924625
iteration : 6996
train acc:  0.734375
train loss:  0.5176037549972534
train gradient:  0.1546843714654707
iteration : 6997
train acc:  0.75
train loss:  0.5391554832458496
train gradient:  0.17833892860798206
iteration : 6998
train acc:  0.765625
train loss:  0.4575420022010803
train gradient:  0.1084472201138407
iteration : 6999
train acc:  0.8046875
train loss:  0.4358156621456146
train gradient:  0.1192186541413036
iteration : 7000
train acc:  0.671875
train loss:  0.551384687423706
train gradient:  0.18412164294589262
iteration : 7001
train acc:  0.7578125
train loss:  0.4864421784877777
train gradient:  0.11022567623258515
iteration : 7002
train acc:  0.8046875
train loss:  0.46870648860931396
train gradient:  0.12280126632544762
iteration : 7003
train acc:  0.7265625
train loss:  0.49602073431015015
train gradient:  0.15987816725676657
iteration : 7004
train acc:  0.6796875
train loss:  0.5022817850112915
train gradient:  0.14107109602999257
iteration : 7005
train acc:  0.7890625
train loss:  0.4467375874519348
train gradient:  0.12226067969145259
iteration : 7006
train acc:  0.7578125
train loss:  0.46616971492767334
train gradient:  0.1325055759882668
iteration : 7007
train acc:  0.8203125
train loss:  0.4506481885910034
train gradient:  0.16047603325990528
iteration : 7008
train acc:  0.7109375
train loss:  0.5370307564735413
train gradient:  0.17104425836163684
iteration : 7009
train acc:  0.7109375
train loss:  0.528653621673584
train gradient:  0.14805512194568227
iteration : 7010
train acc:  0.6953125
train loss:  0.5564861297607422
train gradient:  0.1837440114937512
iteration : 7011
train acc:  0.7109375
train loss:  0.5657657980918884
train gradient:  0.14156503882490118
iteration : 7012
train acc:  0.7421875
train loss:  0.5497888326644897
train gradient:  0.16103046894466488
iteration : 7013
train acc:  0.7890625
train loss:  0.4234806299209595
train gradient:  0.10456675843987086
iteration : 7014
train acc:  0.6875
train loss:  0.4988051652908325
train gradient:  0.13461200049811065
iteration : 7015
train acc:  0.71875
train loss:  0.5182194709777832
train gradient:  0.13638488827618256
iteration : 7016
train acc:  0.7734375
train loss:  0.444641649723053
train gradient:  0.16179380653275083
iteration : 7017
train acc:  0.6953125
train loss:  0.5526355504989624
train gradient:  0.159024469817814
iteration : 7018
train acc:  0.7265625
train loss:  0.5164570212364197
train gradient:  0.14971077367839813
iteration : 7019
train acc:  0.6953125
train loss:  0.54087233543396
train gradient:  0.16582213640417287
iteration : 7020
train acc:  0.6875
train loss:  0.531341552734375
train gradient:  0.1543980056569041
iteration : 7021
train acc:  0.6953125
train loss:  0.5553975701332092
train gradient:  0.16432863613037285
iteration : 7022
train acc:  0.6953125
train loss:  0.5534067749977112
train gradient:  0.1622838637556433
iteration : 7023
train acc:  0.75
train loss:  0.4698965549468994
train gradient:  0.14900926517673307
iteration : 7024
train acc:  0.7109375
train loss:  0.504442036151886
train gradient:  0.149514747338474
iteration : 7025
train acc:  0.7265625
train loss:  0.5690549612045288
train gradient:  0.21467389531055633
iteration : 7026
train acc:  0.8125
train loss:  0.4298362135887146
train gradient:  0.12036730564851053
iteration : 7027
train acc:  0.765625
train loss:  0.5020653009414673
train gradient:  0.17072040617792095
iteration : 7028
train acc:  0.8203125
train loss:  0.42165887355804443
train gradient:  0.08862993623757749
iteration : 7029
train acc:  0.7421875
train loss:  0.47761082649230957
train gradient:  0.1447219927668425
iteration : 7030
train acc:  0.7578125
train loss:  0.4583529233932495
train gradient:  0.129299722804907
iteration : 7031
train acc:  0.84375
train loss:  0.4135800004005432
train gradient:  0.09966522836410537
iteration : 7032
train acc:  0.75
train loss:  0.48439261317253113
train gradient:  0.1374404584246805
iteration : 7033
train acc:  0.7109375
train loss:  0.540327250957489
train gradient:  0.20823128784323697
iteration : 7034
train acc:  0.7734375
train loss:  0.52147376537323
train gradient:  0.1382665474542391
iteration : 7035
train acc:  0.78125
train loss:  0.47774481773376465
train gradient:  0.15777463495003316
iteration : 7036
train acc:  0.7265625
train loss:  0.5104467868804932
train gradient:  0.13424609488033606
iteration : 7037
train acc:  0.7265625
train loss:  0.4675979018211365
train gradient:  0.10354663016027879
iteration : 7038
train acc:  0.71875
train loss:  0.5049315690994263
train gradient:  0.20136655292230254
iteration : 7039
train acc:  0.796875
train loss:  0.4433533251285553
train gradient:  0.11383960733584442
iteration : 7040
train acc:  0.6796875
train loss:  0.5285201668739319
train gradient:  0.13959791795023485
iteration : 7041
train acc:  0.7734375
train loss:  0.4704926609992981
train gradient:  0.1332901076315025
iteration : 7042
train acc:  0.796875
train loss:  0.4502832293510437
train gradient:  0.11785492955703543
iteration : 7043
train acc:  0.8125
train loss:  0.44029051065444946
train gradient:  0.1319037030113448
iteration : 7044
train acc:  0.7265625
train loss:  0.551117479801178
train gradient:  0.13901976688331075
iteration : 7045
train acc:  0.7421875
train loss:  0.4873434603214264
train gradient:  0.14492465825002243
iteration : 7046
train acc:  0.78125
train loss:  0.43585696816444397
train gradient:  0.10206440609055038
iteration : 7047
train acc:  0.7578125
train loss:  0.49281737208366394
train gradient:  0.13350221678651908
iteration : 7048
train acc:  0.7734375
train loss:  0.4456312656402588
train gradient:  0.11944215533601642
iteration : 7049
train acc:  0.734375
train loss:  0.5240994095802307
train gradient:  0.18486825867356438
iteration : 7050
train acc:  0.71875
train loss:  0.5280687808990479
train gradient:  0.16813598130541924
iteration : 7051
train acc:  0.765625
train loss:  0.47995859384536743
train gradient:  0.13648726504808148
iteration : 7052
train acc:  0.6953125
train loss:  0.5812398195266724
train gradient:  0.1764144276993839
iteration : 7053
train acc:  0.7265625
train loss:  0.4977496266365051
train gradient:  0.13537441477174372
iteration : 7054
train acc:  0.7734375
train loss:  0.4471740126609802
train gradient:  0.15927988714716695
iteration : 7055
train acc:  0.7578125
train loss:  0.5204479098320007
train gradient:  0.17704109235037413
iteration : 7056
train acc:  0.7109375
train loss:  0.520847737789154
train gradient:  0.18940777124553437
iteration : 7057
train acc:  0.78125
train loss:  0.4354526102542877
train gradient:  0.1148277590623118
iteration : 7058
train acc:  0.6875
train loss:  0.5988061428070068
train gradient:  0.22763988745729424
iteration : 7059
train acc:  0.71875
train loss:  0.5597749948501587
train gradient:  0.17173359199076366
iteration : 7060
train acc:  0.6953125
train loss:  0.5380129814147949
train gradient:  0.18750589111514981
iteration : 7061
train acc:  0.6953125
train loss:  0.5471470355987549
train gradient:  0.15765966568064738
iteration : 7062
train acc:  0.75
train loss:  0.5196480751037598
train gradient:  0.14877043934289724
iteration : 7063
train acc:  0.703125
train loss:  0.5077283382415771
train gradient:  0.1646872826814033
iteration : 7064
train acc:  0.6875
train loss:  0.5454128980636597
train gradient:  0.23489001346607147
iteration : 7065
train acc:  0.796875
train loss:  0.44312334060668945
train gradient:  0.10575059342121655
iteration : 7066
train acc:  0.7421875
train loss:  0.48268771171569824
train gradient:  0.1554830886182298
iteration : 7067
train acc:  0.78125
train loss:  0.4540833532810211
train gradient:  0.12225986318456956
iteration : 7068
train acc:  0.75
train loss:  0.5065133571624756
train gradient:  0.14995390779214351
iteration : 7069
train acc:  0.6953125
train loss:  0.5275318026542664
train gradient:  0.14266382663814564
iteration : 7070
train acc:  0.6796875
train loss:  0.564379870891571
train gradient:  0.17675243388406647
iteration : 7071
train acc:  0.734375
train loss:  0.5540622472763062
train gradient:  0.16675022955769708
iteration : 7072
train acc:  0.703125
train loss:  0.4637663960456848
train gradient:  0.1134752878610518
iteration : 7073
train acc:  0.7578125
train loss:  0.5271592140197754
train gradient:  0.15305505332763858
iteration : 7074
train acc:  0.6328125
train loss:  0.5591038465499878
train gradient:  0.21049150873855244
iteration : 7075
train acc:  0.6875
train loss:  0.5052267909049988
train gradient:  0.1304346127349751
iteration : 7076
train acc:  0.7578125
train loss:  0.5016310214996338
train gradient:  0.15831618271644493
iteration : 7077
train acc:  0.7578125
train loss:  0.4721131920814514
train gradient:  0.13185592835842774
iteration : 7078
train acc:  0.7734375
train loss:  0.4665626585483551
train gradient:  0.12268461432751886
iteration : 7079
train acc:  0.7109375
train loss:  0.5789408683776855
train gradient:  0.17042649868857962
iteration : 7080
train acc:  0.7421875
train loss:  0.48822838068008423
train gradient:  0.15183093867183933
iteration : 7081
train acc:  0.7734375
train loss:  0.45652908086776733
train gradient:  0.10738354422214251
iteration : 7082
train acc:  0.7734375
train loss:  0.4734886884689331
train gradient:  0.11754346391924503
iteration : 7083
train acc:  0.7265625
train loss:  0.49478691816329956
train gradient:  0.13709847433098055
iteration : 7084
train acc:  0.65625
train loss:  0.5908404588699341
train gradient:  0.1728590199398718
iteration : 7085
train acc:  0.7109375
train loss:  0.5223428010940552
train gradient:  0.1376991810940681
iteration : 7086
train acc:  0.7265625
train loss:  0.5361967086791992
train gradient:  0.13768963162943226
iteration : 7087
train acc:  0.734375
train loss:  0.48826462030410767
train gradient:  0.16925916037811412
iteration : 7088
train acc:  0.703125
train loss:  0.5376253128051758
train gradient:  0.21305029317867835
iteration : 7089
train acc:  0.6875
train loss:  0.5360543727874756
train gradient:  0.15562382231082242
iteration : 7090
train acc:  0.7890625
train loss:  0.4774474501609802
train gradient:  0.1614324435746054
iteration : 7091
train acc:  0.75
train loss:  0.5101379156112671
train gradient:  0.1320216237638298
iteration : 7092
train acc:  0.7578125
train loss:  0.4969736635684967
train gradient:  0.12741450753312455
iteration : 7093
train acc:  0.7734375
train loss:  0.5092111229896545
train gradient:  0.14003719877240983
iteration : 7094
train acc:  0.75
train loss:  0.4696139693260193
train gradient:  0.12789725406408614
iteration : 7095
train acc:  0.7265625
train loss:  0.5377622842788696
train gradient:  0.14618601592649383
iteration : 7096
train acc:  0.71875
train loss:  0.4927564263343811
train gradient:  0.1315782643947061
iteration : 7097
train acc:  0.6953125
train loss:  0.5338074564933777
train gradient:  0.12941263153896426
iteration : 7098
train acc:  0.7109375
train loss:  0.5249741077423096
train gradient:  0.16486608720220644
iteration : 7099
train acc:  0.7421875
train loss:  0.5219130516052246
train gradient:  0.1402803623168838
iteration : 7100
train acc:  0.6875
train loss:  0.5892971754074097
train gradient:  0.17729846863237858
iteration : 7101
train acc:  0.7109375
train loss:  0.5623529553413391
train gradient:  0.14269066645325418
iteration : 7102
train acc:  0.7421875
train loss:  0.4956813454627991
train gradient:  0.1105171096483461
iteration : 7103
train acc:  0.734375
train loss:  0.5118109583854675
train gradient:  0.15113424840289258
iteration : 7104
train acc:  0.71875
train loss:  0.5532054901123047
train gradient:  0.14846944098714798
iteration : 7105
train acc:  0.734375
train loss:  0.5521050691604614
train gradient:  0.15869181423324158
iteration : 7106
train acc:  0.6875
train loss:  0.5058729648590088
train gradient:  0.12369804883162486
iteration : 7107
train acc:  0.734375
train loss:  0.4761582911014557
train gradient:  0.12462260909027181
iteration : 7108
train acc:  0.6953125
train loss:  0.5313893556594849
train gradient:  0.16412833274191457
iteration : 7109
train acc:  0.6875
train loss:  0.5371025800704956
train gradient:  0.1469919693685198
iteration : 7110
train acc:  0.6875
train loss:  0.561582088470459
train gradient:  0.13954983220934575
iteration : 7111
train acc:  0.7109375
train loss:  0.5522860288619995
train gradient:  0.2108811327414215
iteration : 7112
train acc:  0.8125
train loss:  0.45182567834854126
train gradient:  0.11365441542294215
iteration : 7113
train acc:  0.8125
train loss:  0.44160133600234985
train gradient:  0.0968544699043781
iteration : 7114
train acc:  0.7265625
train loss:  0.4739442467689514
train gradient:  0.11033158548944975
iteration : 7115
train acc:  0.7421875
train loss:  0.45475852489471436
train gradient:  0.11004284089951143
iteration : 7116
train acc:  0.734375
train loss:  0.5276254415512085
train gradient:  0.15773999636667532
iteration : 7117
train acc:  0.7421875
train loss:  0.47002387046813965
train gradient:  0.09988696617517939
iteration : 7118
train acc:  0.7421875
train loss:  0.49032270908355713
train gradient:  0.10159156028855605
iteration : 7119
train acc:  0.671875
train loss:  0.5850648880004883
train gradient:  0.18869061124190062
iteration : 7120
train acc:  0.8125
train loss:  0.4494476914405823
train gradient:  0.13294746384298595
iteration : 7121
train acc:  0.75
train loss:  0.48058319091796875
train gradient:  0.11849499964732643
iteration : 7122
train acc:  0.6640625
train loss:  0.6106215119361877
train gradient:  0.22079073555508139
iteration : 7123
train acc:  0.75
train loss:  0.47716861963272095
train gradient:  0.14057890894842884
iteration : 7124
train acc:  0.7578125
train loss:  0.4927964210510254
train gradient:  0.12060526763428461
iteration : 7125
train acc:  0.8125
train loss:  0.44560253620147705
train gradient:  0.15729990632433377
iteration : 7126
train acc:  0.6484375
train loss:  0.5658885836601257
train gradient:  0.19167695745285718
iteration : 7127
train acc:  0.7578125
train loss:  0.5791671276092529
train gradient:  0.17260831297698598
iteration : 7128
train acc:  0.734375
train loss:  0.5522094368934631
train gradient:  0.16521029550054683
iteration : 7129
train acc:  0.8046875
train loss:  0.4465930759906769
train gradient:  0.15122055465116857
iteration : 7130
train acc:  0.7421875
train loss:  0.4846494793891907
train gradient:  0.1121871433445933
iteration : 7131
train acc:  0.6796875
train loss:  0.5410138964653015
train gradient:  0.17010153417581314
iteration : 7132
train acc:  0.796875
train loss:  0.4277881383895874
train gradient:  0.1071521805125252
iteration : 7133
train acc:  0.6953125
train loss:  0.5418460369110107
train gradient:  0.16133978180336744
iteration : 7134
train acc:  0.7265625
train loss:  0.4905734062194824
train gradient:  0.13436769357783906
iteration : 7135
train acc:  0.75
train loss:  0.5362964272499084
train gradient:  0.11634321824096026
iteration : 7136
train acc:  0.796875
train loss:  0.4252152144908905
train gradient:  0.09870617704798744
iteration : 7137
train acc:  0.6875
train loss:  0.586456835269928
train gradient:  0.15536844233696936
iteration : 7138
train acc:  0.71875
train loss:  0.5399882197380066
train gradient:  0.17364553723380116
iteration : 7139
train acc:  0.7421875
train loss:  0.48728111386299133
train gradient:  0.1224578252080602
iteration : 7140
train acc:  0.6640625
train loss:  0.5368109941482544
train gradient:  0.18406005684936683
iteration : 7141
train acc:  0.7265625
train loss:  0.5133804082870483
train gradient:  0.1436971221768548
iteration : 7142
train acc:  0.7265625
train loss:  0.5035432577133179
train gradient:  0.13191824302797844
iteration : 7143
train acc:  0.765625
train loss:  0.4649043083190918
train gradient:  0.10403677487914423
iteration : 7144
train acc:  0.734375
train loss:  0.5356093645095825
train gradient:  0.1465591667402093
iteration : 7145
train acc:  0.734375
train loss:  0.4530709981918335
train gradient:  0.12293774740954956
iteration : 7146
train acc:  0.6875
train loss:  0.5889829397201538
train gradient:  0.1832282428560893
iteration : 7147
train acc:  0.6953125
train loss:  0.5348774194717407
train gradient:  0.1302877630566221
iteration : 7148
train acc:  0.7109375
train loss:  0.5339655876159668
train gradient:  0.13657326320591773
iteration : 7149
train acc:  0.765625
train loss:  0.46251380443573
train gradient:  0.15659892085643495
iteration : 7150
train acc:  0.7578125
train loss:  0.4688947796821594
train gradient:  0.12934613994369737
iteration : 7151
train acc:  0.75
train loss:  0.5104418992996216
train gradient:  0.1691883693285035
iteration : 7152
train acc:  0.7578125
train loss:  0.5106984972953796
train gradient:  0.13408876202078235
iteration : 7153
train acc:  0.765625
train loss:  0.4840080142021179
train gradient:  0.1324946355091225
iteration : 7154
train acc:  0.796875
train loss:  0.461914598941803
train gradient:  0.11290836188547083
iteration : 7155
train acc:  0.71875
train loss:  0.47914886474609375
train gradient:  0.10933235666946386
iteration : 7156
train acc:  0.78125
train loss:  0.4684559106826782
train gradient:  0.11119939655841204
iteration : 7157
train acc:  0.7265625
train loss:  0.48389703035354614
train gradient:  0.12536319738276508
iteration : 7158
train acc:  0.8125
train loss:  0.44274890422821045
train gradient:  0.09694490375848959
iteration : 7159
train acc:  0.7578125
train loss:  0.5010070204734802
train gradient:  0.14848364385135535
iteration : 7160
train acc:  0.6640625
train loss:  0.544640302658081
train gradient:  0.1851712112023529
iteration : 7161
train acc:  0.75
train loss:  0.44608229398727417
train gradient:  0.11048843341183465
iteration : 7162
train acc:  0.7421875
train loss:  0.5159263014793396
train gradient:  0.1401994576021327
iteration : 7163
train acc:  0.6796875
train loss:  0.6343963146209717
train gradient:  0.18063582757090654
iteration : 7164
train acc:  0.703125
train loss:  0.518531858921051
train gradient:  0.14251478700264886
iteration : 7165
train acc:  0.6796875
train loss:  0.563122570514679
train gradient:  0.13511475295440395
iteration : 7166
train acc:  0.765625
train loss:  0.4951229989528656
train gradient:  0.10776948133686903
iteration : 7167
train acc:  0.765625
train loss:  0.4786025583744049
train gradient:  0.11183813623001683
iteration : 7168
train acc:  0.75
train loss:  0.4970301389694214
train gradient:  0.10775843441670276
iteration : 7169
train acc:  0.7734375
train loss:  0.46778666973114014
train gradient:  0.1155042896161776
iteration : 7170
train acc:  0.7421875
train loss:  0.4785115718841553
train gradient:  0.12108673049614546
iteration : 7171
train acc:  0.7109375
train loss:  0.5462812185287476
train gradient:  0.1248581509375482
iteration : 7172
train acc:  0.6953125
train loss:  0.4824715554714203
train gradient:  0.12575868590825298
iteration : 7173
train acc:  0.71875
train loss:  0.5243384838104248
train gradient:  0.12246072896144403
iteration : 7174
train acc:  0.671875
train loss:  0.5439971685409546
train gradient:  0.2338713482754939
iteration : 7175
train acc:  0.734375
train loss:  0.5093939304351807
train gradient:  0.1645731938931107
iteration : 7176
train acc:  0.7734375
train loss:  0.4718295633792877
train gradient:  0.13648028204839321
iteration : 7177
train acc:  0.7265625
train loss:  0.5258522033691406
train gradient:  0.16902034426538104
iteration : 7178
train acc:  0.71875
train loss:  0.5161372423171997
train gradient:  0.1677222984473769
iteration : 7179
train acc:  0.7109375
train loss:  0.5146973729133606
train gradient:  0.12993126885245748
iteration : 7180
train acc:  0.734375
train loss:  0.45753583312034607
train gradient:  0.11583301806366088
iteration : 7181
train acc:  0.765625
train loss:  0.47978195548057556
train gradient:  0.11535557408915965
iteration : 7182
train acc:  0.8125
train loss:  0.43970710039138794
train gradient:  0.1328486539406527
iteration : 7183
train acc:  0.7265625
train loss:  0.47836512327194214
train gradient:  0.1086034607372349
iteration : 7184
train acc:  0.7578125
train loss:  0.45295897126197815
train gradient:  0.11910969512895485
iteration : 7185
train acc:  0.65625
train loss:  0.6453319191932678
train gradient:  0.2534902673234704
iteration : 7186
train acc:  0.7578125
train loss:  0.5133682489395142
train gradient:  0.12420272363170497
iteration : 7187
train acc:  0.75
train loss:  0.5543003082275391
train gradient:  0.15155795005844783
iteration : 7188
train acc:  0.6953125
train loss:  0.5274158120155334
train gradient:  0.15241997486236925
iteration : 7189
train acc:  0.6875
train loss:  0.5128626227378845
train gradient:  0.17599701075109214
iteration : 7190
train acc:  0.7578125
train loss:  0.516497015953064
train gradient:  0.20743096029078303
iteration : 7191
train acc:  0.7109375
train loss:  0.49700042605400085
train gradient:  0.13005646861544864
iteration : 7192
train acc:  0.7890625
train loss:  0.5126349925994873
train gradient:  0.14235297652053444
iteration : 7193
train acc:  0.671875
train loss:  0.5776834487915039
train gradient:  0.17457999583419576
iteration : 7194
train acc:  0.75
train loss:  0.47045233845710754
train gradient:  0.10557903771156824
iteration : 7195
train acc:  0.7890625
train loss:  0.4573465585708618
train gradient:  0.10305815584909479
iteration : 7196
train acc:  0.7265625
train loss:  0.4612812399864197
train gradient:  0.10789984038118336
iteration : 7197
train acc:  0.7578125
train loss:  0.495665967464447
train gradient:  0.16228445711675984
iteration : 7198
train acc:  0.78125
train loss:  0.4513191282749176
train gradient:  0.12511886674588626
iteration : 7199
train acc:  0.7421875
train loss:  0.47539806365966797
train gradient:  0.11652238994538017
iteration : 7200
train acc:  0.7578125
train loss:  0.5322844982147217
train gradient:  0.15462473224323348
iteration : 7201
train acc:  0.7265625
train loss:  0.5245668888092041
train gradient:  0.1414140741987691
iteration : 7202
train acc:  0.6875
train loss:  0.5308375358581543
train gradient:  0.14985265212496532
iteration : 7203
train acc:  0.796875
train loss:  0.43910399079322815
train gradient:  0.1382106100216831
iteration : 7204
train acc:  0.7265625
train loss:  0.5188499689102173
train gradient:  0.16118737048269463
iteration : 7205
train acc:  0.7421875
train loss:  0.48136043548583984
train gradient:  0.15744753201353914
iteration : 7206
train acc:  0.7109375
train loss:  0.5059037804603577
train gradient:  0.13159401629604084
iteration : 7207
train acc:  0.78125
train loss:  0.46926963329315186
train gradient:  0.14170882749360042
iteration : 7208
train acc:  0.765625
train loss:  0.46864455938339233
train gradient:  0.10902732438452331
iteration : 7209
train acc:  0.8359375
train loss:  0.3878313899040222
train gradient:  0.09603917439471714
iteration : 7210
train acc:  0.7109375
train loss:  0.47605079412460327
train gradient:  0.12318938858166971
iteration : 7211
train acc:  0.703125
train loss:  0.5109832882881165
train gradient:  0.11120483163350059
iteration : 7212
train acc:  0.8046875
train loss:  0.4946292042732239
train gradient:  0.15418128582367602
iteration : 7213
train acc:  0.7109375
train loss:  0.5635015964508057
train gradient:  0.2532395167064232
iteration : 7214
train acc:  0.71875
train loss:  0.5602267980575562
train gradient:  0.1624907079667069
iteration : 7215
train acc:  0.71875
train loss:  0.5288547873497009
train gradient:  0.14854004128913037
iteration : 7216
train acc:  0.71875
train loss:  0.45461100339889526
train gradient:  0.11982825616624374
iteration : 7217
train acc:  0.7421875
train loss:  0.47813037037849426
train gradient:  0.12237204261027843
iteration : 7218
train acc:  0.6875
train loss:  0.5461791753768921
train gradient:  0.17411101570377216
iteration : 7219
train acc:  0.78125
train loss:  0.4229696989059448
train gradient:  0.1020629333787349
iteration : 7220
train acc:  0.703125
train loss:  0.521971583366394
train gradient:  0.1688539693143056
iteration : 7221
train acc:  0.6953125
train loss:  0.5177592635154724
train gradient:  0.16686635247281478
iteration : 7222
train acc:  0.7578125
train loss:  0.48521342873573303
train gradient:  0.11658293239853766
iteration : 7223
train acc:  0.75
train loss:  0.4439924359321594
train gradient:  0.10867539492829269
iteration : 7224
train acc:  0.75
train loss:  0.4673216640949249
train gradient:  0.14130871283795632
iteration : 7225
train acc:  0.7109375
train loss:  0.5130712389945984
train gradient:  0.14539765944056404
iteration : 7226
train acc:  0.703125
train loss:  0.5323637127876282
train gradient:  0.11866507340085315
iteration : 7227
train acc:  0.796875
train loss:  0.4318695664405823
train gradient:  0.10387239663569897
iteration : 7228
train acc:  0.734375
train loss:  0.48819512128829956
train gradient:  0.1843865121589368
iteration : 7229
train acc:  0.640625
train loss:  0.5198461413383484
train gradient:  0.1506229195406968
iteration : 7230
train acc:  0.6875
train loss:  0.5309262275695801
train gradient:  0.12947485454345808
iteration : 7231
train acc:  0.6875
train loss:  0.5255389213562012
train gradient:  0.16026604548182288
iteration : 7232
train acc:  0.703125
train loss:  0.49358683824539185
train gradient:  0.16450629379971732
iteration : 7233
train acc:  0.78125
train loss:  0.45165500044822693
train gradient:  0.10489725329428425
iteration : 7234
train acc:  0.7109375
train loss:  0.5610359311103821
train gradient:  0.2020631495392606
iteration : 7235
train acc:  0.7265625
train loss:  0.4907134175300598
train gradient:  0.12902179733402913
iteration : 7236
train acc:  0.796875
train loss:  0.4496619403362274
train gradient:  0.13584670164100085
iteration : 7237
train acc:  0.765625
train loss:  0.4594220519065857
train gradient:  0.15824419981042104
iteration : 7238
train acc:  0.78125
train loss:  0.43751364946365356
train gradient:  0.09921063272053891
iteration : 7239
train acc:  0.734375
train loss:  0.5226616263389587
train gradient:  0.15663346930193794
iteration : 7240
train acc:  0.7734375
train loss:  0.45991528034210205
train gradient:  0.10180224008529515
iteration : 7241
train acc:  0.765625
train loss:  0.469790518283844
train gradient:  0.15176325629366388
iteration : 7242
train acc:  0.734375
train loss:  0.49242985248565674
train gradient:  0.17049198270422494
iteration : 7243
train acc:  0.7421875
train loss:  0.5046190023422241
train gradient:  0.12928185895710925
iteration : 7244
train acc:  0.8203125
train loss:  0.44781580567359924
train gradient:  0.11066716995248142
iteration : 7245
train acc:  0.8046875
train loss:  0.432503342628479
train gradient:  0.10281709840355617
iteration : 7246
train acc:  0.7578125
train loss:  0.4691206216812134
train gradient:  0.11323945626317725
iteration : 7247
train acc:  0.7890625
train loss:  0.5009764432907104
train gradient:  0.12426141657232023
iteration : 7248
train acc:  0.765625
train loss:  0.4553869366645813
train gradient:  0.1558478850278915
iteration : 7249
train acc:  0.7421875
train loss:  0.5121535658836365
train gradient:  0.1487097093616996
iteration : 7250
train acc:  0.7265625
train loss:  0.5475852489471436
train gradient:  0.20972839559247514
iteration : 7251
train acc:  0.71875
train loss:  0.5455772876739502
train gradient:  0.15345722632630446
iteration : 7252
train acc:  0.734375
train loss:  0.5020316243171692
train gradient:  0.13785062629684328
iteration : 7253
train acc:  0.796875
train loss:  0.5013835430145264
train gradient:  0.1674388472843032
iteration : 7254
train acc:  0.7109375
train loss:  0.5534043312072754
train gradient:  0.17027223170068292
iteration : 7255
train acc:  0.765625
train loss:  0.4282085597515106
train gradient:  0.10014461126633267
iteration : 7256
train acc:  0.7265625
train loss:  0.510017991065979
train gradient:  0.11950530584149689
iteration : 7257
train acc:  0.734375
train loss:  0.5118767619132996
train gradient:  0.13291034540904234
iteration : 7258
train acc:  0.6953125
train loss:  0.547643780708313
train gradient:  0.15855589493709632
iteration : 7259
train acc:  0.796875
train loss:  0.4638270139694214
train gradient:  0.11379844390284234
iteration : 7260
train acc:  0.7421875
train loss:  0.5109661221504211
train gradient:  0.11231185439198556
iteration : 7261
train acc:  0.7734375
train loss:  0.45456475019454956
train gradient:  0.10237400943916596
iteration : 7262
train acc:  0.7265625
train loss:  0.549095869064331
train gradient:  0.2888557582660435
iteration : 7263
train acc:  0.71875
train loss:  0.6136873364448547
train gradient:  0.2007631472060623
iteration : 7264
train acc:  0.765625
train loss:  0.4906575679779053
train gradient:  0.13933318002449996
iteration : 7265
train acc:  0.71875
train loss:  0.5022139549255371
train gradient:  0.1465586732305506
iteration : 7266
train acc:  0.7734375
train loss:  0.46175825595855713
train gradient:  0.11033751703965755
iteration : 7267
train acc:  0.71875
train loss:  0.469271719455719
train gradient:  0.09513096135278921
iteration : 7268
train acc:  0.703125
train loss:  0.5192875266075134
train gradient:  0.15750559359646887
iteration : 7269
train acc:  0.7265625
train loss:  0.4672281742095947
train gradient:  0.12339158137448644
iteration : 7270
train acc:  0.8671875
train loss:  0.41338911652565
train gradient:  0.10261382219131591
iteration : 7271
train acc:  0.7578125
train loss:  0.4731477200984955
train gradient:  0.12518056639513142
iteration : 7272
train acc:  0.734375
train loss:  0.45178452134132385
train gradient:  0.10605346267473953
iteration : 7273
train acc:  0.734375
train loss:  0.4857184588909149
train gradient:  0.12280551668680811
iteration : 7274
train acc:  0.6484375
train loss:  0.5524558424949646
train gradient:  0.16576512895719134
iteration : 7275
train acc:  0.734375
train loss:  0.5099750757217407
train gradient:  0.1362198980589172
iteration : 7276
train acc:  0.75
train loss:  0.46972671151161194
train gradient:  0.1520812764587975
iteration : 7277
train acc:  0.71875
train loss:  0.5254579782485962
train gradient:  0.13562337924913642
iteration : 7278
train acc:  0.71875
train loss:  0.5357853770256042
train gradient:  0.13307491458201032
iteration : 7279
train acc:  0.6875
train loss:  0.5233994126319885
train gradient:  0.12781391359732586
iteration : 7280
train acc:  0.7890625
train loss:  0.47257477045059204
train gradient:  0.13115638711263453
iteration : 7281
train acc:  0.6953125
train loss:  0.5519939661026001
train gradient:  0.1625653288164961
iteration : 7282
train acc:  0.7578125
train loss:  0.5160946249961853
train gradient:  0.17245386651766142
iteration : 7283
train acc:  0.734375
train loss:  0.5415258407592773
train gradient:  0.16441946197468918
iteration : 7284
train acc:  0.6953125
train loss:  0.5012414455413818
train gradient:  0.13561512694469843
iteration : 7285
train acc:  0.734375
train loss:  0.5455195307731628
train gradient:  0.1627673112412356
iteration : 7286
train acc:  0.7265625
train loss:  0.4821856617927551
train gradient:  0.1408277581979708
iteration : 7287
train acc:  0.75
train loss:  0.46837449073791504
train gradient:  0.11059625194528501
iteration : 7288
train acc:  0.71875
train loss:  0.47908690571784973
train gradient:  0.10794828282685585
iteration : 7289
train acc:  0.7421875
train loss:  0.505117654800415
train gradient:  0.1747315066428102
iteration : 7290
train acc:  0.765625
train loss:  0.5435789823532104
train gradient:  0.19849831664372397
iteration : 7291
train acc:  0.8125
train loss:  0.4566046893596649
train gradient:  0.1455996912778731
iteration : 7292
train acc:  0.75
train loss:  0.4802275002002716
train gradient:  0.12456327923785586
iteration : 7293
train acc:  0.8046875
train loss:  0.4568193554878235
train gradient:  0.13208892546455883
iteration : 7294
train acc:  0.671875
train loss:  0.6439918279647827
train gradient:  0.24721987160525927
iteration : 7295
train acc:  0.7578125
train loss:  0.48308032751083374
train gradient:  0.10826281737725821
iteration : 7296
train acc:  0.703125
train loss:  0.5279157161712646
train gradient:  0.16961716921554387
iteration : 7297
train acc:  0.71875
train loss:  0.510089099407196
train gradient:  0.13074422517959036
iteration : 7298
train acc:  0.6484375
train loss:  0.5616385340690613
train gradient:  0.21967721069180066
iteration : 7299
train acc:  0.7265625
train loss:  0.5471771955490112
train gradient:  0.13449999210764801
iteration : 7300
train acc:  0.7578125
train loss:  0.5488218665122986
train gradient:  0.14487965317905274
iteration : 7301
train acc:  0.7109375
train loss:  0.4914212226867676
train gradient:  0.14407086864761853
iteration : 7302
train acc:  0.8046875
train loss:  0.49660393595695496
train gradient:  0.13971611372601667
iteration : 7303
train acc:  0.7578125
train loss:  0.41543835401535034
train gradient:  0.08337564196369157
iteration : 7304
train acc:  0.75
train loss:  0.43506717681884766
train gradient:  0.10236361164216776
iteration : 7305
train acc:  0.703125
train loss:  0.5430457592010498
train gradient:  0.14732453459091865
iteration : 7306
train acc:  0.7109375
train loss:  0.514262855052948
train gradient:  0.13570048368265125
iteration : 7307
train acc:  0.6796875
train loss:  0.5705454349517822
train gradient:  0.1835540800517125
iteration : 7308
train acc:  0.71875
train loss:  0.4749327003955841
train gradient:  0.10081949997296757
iteration : 7309
train acc:  0.7109375
train loss:  0.5272195935249329
train gradient:  0.1459579270460204
iteration : 7310
train acc:  0.7421875
train loss:  0.4893651306629181
train gradient:  0.1024405496011212
iteration : 7311
train acc:  0.6953125
train loss:  0.5829463005065918
train gradient:  0.24487724915320674
iteration : 7312
train acc:  0.765625
train loss:  0.5081143379211426
train gradient:  0.14052416453410466
iteration : 7313
train acc:  0.7734375
train loss:  0.47653013467788696
train gradient:  0.14179064616482995
iteration : 7314
train acc:  0.765625
train loss:  0.4809202253818512
train gradient:  0.13192025779201594
iteration : 7315
train acc:  0.75
train loss:  0.523335337638855
train gradient:  0.11472433423694259
iteration : 7316
train acc:  0.71875
train loss:  0.4758586287498474
train gradient:  0.10193614785148881
iteration : 7317
train acc:  0.734375
train loss:  0.5202677249908447
train gradient:  0.15351934949917018
iteration : 7318
train acc:  0.75
train loss:  0.47354215383529663
train gradient:  0.10263238396814625
iteration : 7319
train acc:  0.7578125
train loss:  0.5032773613929749
train gradient:  0.10362151758654617
iteration : 7320
train acc:  0.78125
train loss:  0.4694286584854126
train gradient:  0.1175814754033038
iteration : 7321
train acc:  0.7421875
train loss:  0.5142855048179626
train gradient:  0.12948302938789039
iteration : 7322
train acc:  0.78125
train loss:  0.4851018488407135
train gradient:  0.18039078564868455
iteration : 7323
train acc:  0.6328125
train loss:  0.5835118293762207
train gradient:  0.21562354334565553
iteration : 7324
train acc:  0.65625
train loss:  0.5790042877197266
train gradient:  0.16319185943221326
iteration : 7325
train acc:  0.765625
train loss:  0.4468774199485779
train gradient:  0.11616530122077383
iteration : 7326
train acc:  0.6953125
train loss:  0.5641674995422363
train gradient:  0.1793251673058717
iteration : 7327
train acc:  0.796875
train loss:  0.42607739567756653
train gradient:  0.10213264937986405
iteration : 7328
train acc:  0.75
train loss:  0.5228145718574524
train gradient:  0.13497184624353853
iteration : 7329
train acc:  0.7578125
train loss:  0.4901547133922577
train gradient:  0.18112612844577075
iteration : 7330
train acc:  0.796875
train loss:  0.47821149230003357
train gradient:  0.13163758755784583
iteration : 7331
train acc:  0.7265625
train loss:  0.47638970613479614
train gradient:  0.10367395071248198
iteration : 7332
train acc:  0.78125
train loss:  0.47614485025405884
train gradient:  0.15285801069071736
iteration : 7333
train acc:  0.7109375
train loss:  0.5449233055114746
train gradient:  0.14629403738150418
iteration : 7334
train acc:  0.6796875
train loss:  0.5640242099761963
train gradient:  0.22164838036855694
iteration : 7335
train acc:  0.6953125
train loss:  0.5635969638824463
train gradient:  0.1937755094861966
iteration : 7336
train acc:  0.6875
train loss:  0.546250581741333
train gradient:  0.1467749855382193
iteration : 7337
train acc:  0.765625
train loss:  0.4726114273071289
train gradient:  0.17883429915043098
iteration : 7338
train acc:  0.75
train loss:  0.4368208646774292
train gradient:  0.1247583910154244
iteration : 7339
train acc:  0.6875
train loss:  0.5732262134552002
train gradient:  0.20751246750575852
iteration : 7340
train acc:  0.7265625
train loss:  0.49570900201797485
train gradient:  0.13023331411984107
iteration : 7341
train acc:  0.765625
train loss:  0.45252281427383423
train gradient:  0.1068920514930961
iteration : 7342
train acc:  0.734375
train loss:  0.5320591330528259
train gradient:  0.13957562014828712
iteration : 7343
train acc:  0.7734375
train loss:  0.4811166822910309
train gradient:  0.1417044719765485
iteration : 7344
train acc:  0.7109375
train loss:  0.5063689351081848
train gradient:  0.13900623177370525
iteration : 7345
train acc:  0.796875
train loss:  0.41507697105407715
train gradient:  0.10309973351156945
iteration : 7346
train acc:  0.7578125
train loss:  0.4949730336666107
train gradient:  0.12458661785997893
iteration : 7347
train acc:  0.734375
train loss:  0.5413886308670044
train gradient:  0.13513971130373947
iteration : 7348
train acc:  0.7578125
train loss:  0.4987969398498535
train gradient:  0.13100621193057524
iteration : 7349
train acc:  0.7734375
train loss:  0.44574791193008423
train gradient:  0.13425159699150935
iteration : 7350
train acc:  0.7265625
train loss:  0.5046113133430481
train gradient:  0.15094063666586993
iteration : 7351
train acc:  0.71875
train loss:  0.5143718123435974
train gradient:  0.1685522505913121
iteration : 7352
train acc:  0.75
train loss:  0.5015076994895935
train gradient:  0.1362060172467781
iteration : 7353
train acc:  0.671875
train loss:  0.5157630443572998
train gradient:  0.14270857695264094
iteration : 7354
train acc:  0.6796875
train loss:  0.49792250990867615
train gradient:  0.12938229593202194
iteration : 7355
train acc:  0.671875
train loss:  0.5527300238609314
train gradient:  0.16143361285025964
iteration : 7356
train acc:  0.6796875
train loss:  0.5189529657363892
train gradient:  0.13791186427906127
iteration : 7357
train acc:  0.828125
train loss:  0.4123956561088562
train gradient:  0.11568490689887814
iteration : 7358
train acc:  0.7734375
train loss:  0.47939199209213257
train gradient:  0.1656591005697693
iteration : 7359
train acc:  0.7734375
train loss:  0.44709667563438416
train gradient:  0.11792098691744209
iteration : 7360
train acc:  0.7109375
train loss:  0.5165307521820068
train gradient:  0.14003839947237157
iteration : 7361
train acc:  0.71875
train loss:  0.4884440302848816
train gradient:  0.17478382028509842
iteration : 7362
train acc:  0.7421875
train loss:  0.45019030570983887
train gradient:  0.1023436777036608
iteration : 7363
train acc:  0.6953125
train loss:  0.5125546455383301
train gradient:  0.14546551752509812
iteration : 7364
train acc:  0.6875
train loss:  0.5628399848937988
train gradient:  0.15922607541698924
iteration : 7365
train acc:  0.7578125
train loss:  0.4683448374271393
train gradient:  0.12051282414626367
iteration : 7366
train acc:  0.75
train loss:  0.5278359651565552
train gradient:  0.142604897404719
iteration : 7367
train acc:  0.703125
train loss:  0.5846859812736511
train gradient:  0.18203494050649138
iteration : 7368
train acc:  0.6875
train loss:  0.5315116047859192
train gradient:  0.15112300521787547
iteration : 7369
train acc:  0.734375
train loss:  0.49751192331314087
train gradient:  0.16125652242051236
iteration : 7370
train acc:  0.7734375
train loss:  0.46808290481567383
train gradient:  0.10436851975447589
iteration : 7371
train acc:  0.6796875
train loss:  0.622567892074585
train gradient:  0.2905031750375112
iteration : 7372
train acc:  0.796875
train loss:  0.4611068665981293
train gradient:  0.11518551974253818
iteration : 7373
train acc:  0.7578125
train loss:  0.48932668566703796
train gradient:  0.11226743747125141
iteration : 7374
train acc:  0.7109375
train loss:  0.5092459917068481
train gradient:  0.11114688409603443
iteration : 7375
train acc:  0.78125
train loss:  0.4933795928955078
train gradient:  0.13990599917078977
iteration : 7376
train acc:  0.78125
train loss:  0.46284908056259155
train gradient:  0.10433623216236762
iteration : 7377
train acc:  0.7578125
train loss:  0.5068130493164062
train gradient:  0.138938271093844
iteration : 7378
train acc:  0.7109375
train loss:  0.5062665939331055
train gradient:  0.12778920986305403
iteration : 7379
train acc:  0.71875
train loss:  0.5536355972290039
train gradient:  0.14273600637119432
iteration : 7380
train acc:  0.8046875
train loss:  0.44969749450683594
train gradient:  0.1293661238382693
iteration : 7381
train acc:  0.71875
train loss:  0.5382943153381348
train gradient:  0.19952753224722614
iteration : 7382
train acc:  0.7421875
train loss:  0.5098885297775269
train gradient:  0.14219569174011892
iteration : 7383
train acc:  0.7890625
train loss:  0.4292929172515869
train gradient:  0.1361927198776792
iteration : 7384
train acc:  0.7109375
train loss:  0.5402733683586121
train gradient:  0.1377370231180382
iteration : 7385
train acc:  0.7578125
train loss:  0.5000200271606445
train gradient:  0.14002494376484756
iteration : 7386
train acc:  0.7578125
train loss:  0.4260667562484741
train gradient:  0.1129657600710452
iteration : 7387
train acc:  0.7421875
train loss:  0.4648497700691223
train gradient:  0.12516496677046468
iteration : 7388
train acc:  0.7578125
train loss:  0.5080315470695496
train gradient:  0.14063139042782094
iteration : 7389
train acc:  0.7578125
train loss:  0.4942471981048584
train gradient:  0.16764634852690696
iteration : 7390
train acc:  0.7421875
train loss:  0.507442057132721
train gradient:  0.12777530609396376
iteration : 7391
train acc:  0.7421875
train loss:  0.48492157459259033
train gradient:  0.12475090485301794
iteration : 7392
train acc:  0.7890625
train loss:  0.4733618497848511
train gradient:  0.11326029096816866
iteration : 7393
train acc:  0.7578125
train loss:  0.5037257671356201
train gradient:  0.1827954837935517
iteration : 7394
train acc:  0.7578125
train loss:  0.5219014883041382
train gradient:  0.12205326118651728
iteration : 7395
train acc:  0.75
train loss:  0.48012638092041016
train gradient:  0.1129671371158185
iteration : 7396
train acc:  0.7578125
train loss:  0.5281480550765991
train gradient:  0.1623591703436602
iteration : 7397
train acc:  0.703125
train loss:  0.5452377796173096
train gradient:  0.18925283430885145
iteration : 7398
train acc:  0.71875
train loss:  0.4915103316307068
train gradient:  0.10487356777175216
iteration : 7399
train acc:  0.7265625
train loss:  0.5307559967041016
train gradient:  0.145896035384208
iteration : 7400
train acc:  0.75
train loss:  0.4942495822906494
train gradient:  0.13505051901302115
iteration : 7401
train acc:  0.6875
train loss:  0.5152723789215088
train gradient:  0.16067456069574537
iteration : 7402
train acc:  0.7109375
train loss:  0.5355682969093323
train gradient:  0.12844936556808634
iteration : 7403
train acc:  0.7421875
train loss:  0.4787660837173462
train gradient:  0.1105981846309258
iteration : 7404
train acc:  0.8046875
train loss:  0.4694879651069641
train gradient:  0.13589909113955712
iteration : 7405
train acc:  0.6875
train loss:  0.5168390870094299
train gradient:  0.15279829798444017
iteration : 7406
train acc:  0.75
train loss:  0.4658704400062561
train gradient:  0.11132084216857217
iteration : 7407
train acc:  0.78125
train loss:  0.44754740595817566
train gradient:  0.09355636594371515
iteration : 7408
train acc:  0.71875
train loss:  0.5014692544937134
train gradient:  0.13730953719046302
iteration : 7409
train acc:  0.6953125
train loss:  0.5531771183013916
train gradient:  0.13248525848918252
iteration : 7410
train acc:  0.734375
train loss:  0.49825698137283325
train gradient:  0.1446474048366135
iteration : 7411
train acc:  0.765625
train loss:  0.5071107149124146
train gradient:  0.14119194316175004
iteration : 7412
train acc:  0.7265625
train loss:  0.4888525605201721
train gradient:  0.11312887554464496
iteration : 7413
train acc:  0.7109375
train loss:  0.5078425407409668
train gradient:  0.13773649674570165
iteration : 7414
train acc:  0.7890625
train loss:  0.46577292680740356
train gradient:  0.12874167878473158
iteration : 7415
train acc:  0.734375
train loss:  0.5000709891319275
train gradient:  0.09876363286526359
iteration : 7416
train acc:  0.65625
train loss:  0.5881906747817993
train gradient:  0.1594751425725084
iteration : 7417
train acc:  0.734375
train loss:  0.5081110000610352
train gradient:  0.13788373983969926
iteration : 7418
train acc:  0.7421875
train loss:  0.5430298447608948
train gradient:  0.1572211288032269
iteration : 7419
train acc:  0.7578125
train loss:  0.4772719740867615
train gradient:  0.13582954398285002
iteration : 7420
train acc:  0.703125
train loss:  0.5617644786834717
train gradient:  0.19208490564294
iteration : 7421
train acc:  0.734375
train loss:  0.501825749874115
train gradient:  0.15651109896091053
iteration : 7422
train acc:  0.7421875
train loss:  0.47128966450691223
train gradient:  0.1437418182117268
iteration : 7423
train acc:  0.7734375
train loss:  0.49673521518707275
train gradient:  0.16605809805314584
iteration : 7424
train acc:  0.7265625
train loss:  0.5305725336074829
train gradient:  0.1864414833288373
iteration : 7425
train acc:  0.75
train loss:  0.5676764249801636
train gradient:  0.19503030067187038
iteration : 7426
train acc:  0.7578125
train loss:  0.5060794353485107
train gradient:  0.14554243477084805
iteration : 7427
train acc:  0.75
train loss:  0.4255295693874359
train gradient:  0.1158666078544456
iteration : 7428
train acc:  0.796875
train loss:  0.41834741830825806
train gradient:  0.11792069737359392
iteration : 7429
train acc:  0.7421875
train loss:  0.48707816004753113
train gradient:  0.12909880807093904
iteration : 7430
train acc:  0.7578125
train loss:  0.5138524770736694
train gradient:  0.1204116785557526
iteration : 7431
train acc:  0.7109375
train loss:  0.5258796811103821
train gradient:  0.13560616910842377
iteration : 7432
train acc:  0.78125
train loss:  0.49210473895072937
train gradient:  0.15126255216189727
iteration : 7433
train acc:  0.7734375
train loss:  0.48598772287368774
train gradient:  0.1448212091899907
iteration : 7434
train acc:  0.6875
train loss:  0.5722675323486328
train gradient:  0.15087586420212867
iteration : 7435
train acc:  0.7109375
train loss:  0.5073158740997314
train gradient:  0.14602669809932378
iteration : 7436
train acc:  0.703125
train loss:  0.5144976377487183
train gradient:  0.1450397873942826
iteration : 7437
train acc:  0.71875
train loss:  0.479482501745224
train gradient:  0.11874232309881212
iteration : 7438
train acc:  0.71875
train loss:  0.5605989694595337
train gradient:  0.17692028671163465
iteration : 7439
train acc:  0.7265625
train loss:  0.511725664138794
train gradient:  0.17883889508991146
iteration : 7440
train acc:  0.75
train loss:  0.4718402624130249
train gradient:  0.12694684471622517
iteration : 7441
train acc:  0.75
train loss:  0.5108532905578613
train gradient:  0.1378478951179284
iteration : 7442
train acc:  0.703125
train loss:  0.5907201766967773
train gradient:  0.16515109173490733
iteration : 7443
train acc:  0.7734375
train loss:  0.4876191020011902
train gradient:  0.14057392360213258
iteration : 7444
train acc:  0.8125
train loss:  0.4510328471660614
train gradient:  0.14431899054659267
iteration : 7445
train acc:  0.7421875
train loss:  0.49033671617507935
train gradient:  0.15677013540506501
iteration : 7446
train acc:  0.671875
train loss:  0.5407531261444092
train gradient:  0.17030819325573673
iteration : 7447
train acc:  0.65625
train loss:  0.5372601747512817
train gradient:  0.14637803207760472
iteration : 7448
train acc:  0.6875
train loss:  0.5482705235481262
train gradient:  0.13364489459887985
iteration : 7449
train acc:  0.7578125
train loss:  0.5290207266807556
train gradient:  0.1610535182650119
iteration : 7450
train acc:  0.7265625
train loss:  0.46151846647262573
train gradient:  0.10316381095162892
iteration : 7451
train acc:  0.75
train loss:  0.4913058578968048
train gradient:  0.11310067280246583
iteration : 7452
train acc:  0.75
train loss:  0.5333254933357239
train gradient:  0.18348419409512312
iteration : 7453
train acc:  0.75
train loss:  0.5240462422370911
train gradient:  0.13024970648959258
iteration : 7454
train acc:  0.7265625
train loss:  0.5092335939407349
train gradient:  0.22173185365486595
iteration : 7455
train acc:  0.7890625
train loss:  0.45406094193458557
train gradient:  0.11524032792663987
iteration : 7456
train acc:  0.7265625
train loss:  0.5413545370101929
train gradient:  0.13400617137236387
iteration : 7457
train acc:  0.703125
train loss:  0.5166502594947815
train gradient:  0.18604589713543557
iteration : 7458
train acc:  0.7109375
train loss:  0.5077465772628784
train gradient:  0.13520580187654124
iteration : 7459
train acc:  0.7109375
train loss:  0.549433171749115
train gradient:  0.16857327645634565
iteration : 7460
train acc:  0.7578125
train loss:  0.5004595518112183
train gradient:  0.1695027860871433
iteration : 7461
train acc:  0.78125
train loss:  0.4863882064819336
train gradient:  0.14837378541444188
iteration : 7462
train acc:  0.765625
train loss:  0.4565506875514984
train gradient:  0.1339761940342018
iteration : 7463
train acc:  0.6953125
train loss:  0.5138726234436035
train gradient:  0.15382657938857325
iteration : 7464
train acc:  0.703125
train loss:  0.5063638687133789
train gradient:  0.12024515706030205
iteration : 7465
train acc:  0.71875
train loss:  0.5538002252578735
train gradient:  0.1374071228097598
iteration : 7466
train acc:  0.7265625
train loss:  0.5635737776756287
train gradient:  0.20355471437669515
iteration : 7467
train acc:  0.765625
train loss:  0.47986268997192383
train gradient:  0.13451913329188578
iteration : 7468
train acc:  0.703125
train loss:  0.5480788946151733
train gradient:  0.14607577881682543
iteration : 7469
train acc:  0.7421875
train loss:  0.5134413838386536
train gradient:  0.12770908656638247
iteration : 7470
train acc:  0.8125
train loss:  0.4349851608276367
train gradient:  0.11835466146804897
iteration : 7471
train acc:  0.765625
train loss:  0.5054653882980347
train gradient:  0.12410904084247237
iteration : 7472
train acc:  0.7578125
train loss:  0.4779399037361145
train gradient:  0.09914338832447991
iteration : 7473
train acc:  0.7421875
train loss:  0.47949865460395813
train gradient:  0.11626981227991323
iteration : 7474
train acc:  0.640625
train loss:  0.5820275545120239
train gradient:  0.14958807826289772
iteration : 7475
train acc:  0.796875
train loss:  0.4593091607093811
train gradient:  0.0908653203124728
iteration : 7476
train acc:  0.734375
train loss:  0.4519883990287781
train gradient:  0.08817322116414408
iteration : 7477
train acc:  0.6875
train loss:  0.5548234581947327
train gradient:  0.1462405699607438
iteration : 7478
train acc:  0.640625
train loss:  0.5371411442756653
train gradient:  0.1462134617222388
iteration : 7479
train acc:  0.8203125
train loss:  0.3866143822669983
train gradient:  0.08962494621838732
iteration : 7480
train acc:  0.703125
train loss:  0.5057024359703064
train gradient:  0.14379480403663755
iteration : 7481
train acc:  0.8046875
train loss:  0.41351479291915894
train gradient:  0.0951490792571056
iteration : 7482
train acc:  0.71875
train loss:  0.5600515007972717
train gradient:  0.16951187477468138
iteration : 7483
train acc:  0.7578125
train loss:  0.48982876539230347
train gradient:  0.11635124460136397
iteration : 7484
train acc:  0.765625
train loss:  0.515634298324585
train gradient:  0.14456364438703306
iteration : 7485
train acc:  0.6953125
train loss:  0.5256802439689636
train gradient:  0.14827109356614798
iteration : 7486
train acc:  0.7578125
train loss:  0.5082634687423706
train gradient:  0.1442894259971758
iteration : 7487
train acc:  0.703125
train loss:  0.5133010745048523
train gradient:  0.13269559843147855
iteration : 7488
train acc:  0.6953125
train loss:  0.5257601737976074
train gradient:  0.13804146389149763
iteration : 7489
train acc:  0.7109375
train loss:  0.5676325559616089
train gradient:  0.17849598447990261
iteration : 7490
train acc:  0.6875
train loss:  0.603021502494812
train gradient:  0.1815069232575657
iteration : 7491
train acc:  0.6796875
train loss:  0.5199179649353027
train gradient:  0.1439627914111295
iteration : 7492
train acc:  0.6953125
train loss:  0.5263311862945557
train gradient:  0.12171585594778635
iteration : 7493
train acc:  0.8125
train loss:  0.44528838992118835
train gradient:  0.11469623181297779
iteration : 7494
train acc:  0.78125
train loss:  0.4602597951889038
train gradient:  0.12706994928478538
iteration : 7495
train acc:  0.7109375
train loss:  0.5593751072883606
train gradient:  0.2622771275452539
iteration : 7496
train acc:  0.7578125
train loss:  0.5094463229179382
train gradient:  0.16575373253030184
iteration : 7497
train acc:  0.7265625
train loss:  0.5227065682411194
train gradient:  0.142792462921209
iteration : 7498
train acc:  0.7734375
train loss:  0.47411802411079407
train gradient:  0.14379145515012667
iteration : 7499
train acc:  0.7421875
train loss:  0.5419265031814575
train gradient:  0.14040982860077555
iteration : 7500
train acc:  0.765625
train loss:  0.48456430435180664
train gradient:  0.1215963579010577
iteration : 7501
train acc:  0.7578125
train loss:  0.4852938652038574
train gradient:  0.15116979432792238
iteration : 7502
train acc:  0.8203125
train loss:  0.41927942633628845
train gradient:  0.1146256403836929
iteration : 7503
train acc:  0.78125
train loss:  0.4727829098701477
train gradient:  0.14392360203268434
iteration : 7504
train acc:  0.7578125
train loss:  0.5098230838775635
train gradient:  0.11037250269588564
iteration : 7505
train acc:  0.734375
train loss:  0.5131251811981201
train gradient:  0.12097235825166495
iteration : 7506
train acc:  0.78125
train loss:  0.49049729108810425
train gradient:  0.12226945814349756
iteration : 7507
train acc:  0.7578125
train loss:  0.5139785408973694
train gradient:  0.11256552466505802
iteration : 7508
train acc:  0.734375
train loss:  0.5164434313774109
train gradient:  0.15620443501242154
iteration : 7509
train acc:  0.6796875
train loss:  0.5390621423721313
train gradient:  0.13792439597008493
iteration : 7510
train acc:  0.7109375
train loss:  0.5289944410324097
train gradient:  0.1267396683672085
iteration : 7511
train acc:  0.7421875
train loss:  0.5034182071685791
train gradient:  0.10397470929292053
iteration : 7512
train acc:  0.75
train loss:  0.4438106417655945
train gradient:  0.10396351073114478
iteration : 7513
train acc:  0.7421875
train loss:  0.5057571530342102
train gradient:  0.13742011427261713
iteration : 7514
train acc:  0.6640625
train loss:  0.5296003818511963
train gradient:  0.14184344808577334
iteration : 7515
train acc:  0.7734375
train loss:  0.4593208432197571
train gradient:  0.11756521273913265
iteration : 7516
train acc:  0.6796875
train loss:  0.5671780705451965
train gradient:  0.1922245537803539
iteration : 7517
train acc:  0.7578125
train loss:  0.5133613348007202
train gradient:  0.1196166535852448
iteration : 7518
train acc:  0.7265625
train loss:  0.5126254558563232
train gradient:  0.1357579486266494
iteration : 7519
train acc:  0.7265625
train loss:  0.493888795375824
train gradient:  0.13067002874720318
iteration : 7520
train acc:  0.75
train loss:  0.4925660192966461
train gradient:  0.1114956519841304
iteration : 7521
train acc:  0.671875
train loss:  0.5503270626068115
train gradient:  0.18033086358647193
iteration : 7522
train acc:  0.7734375
train loss:  0.5154926776885986
train gradient:  0.165839764579161
iteration : 7523
train acc:  0.828125
train loss:  0.41923922300338745
train gradient:  0.09491857913135485
iteration : 7524
train acc:  0.734375
train loss:  0.4890086054801941
train gradient:  0.1321877464454754
iteration : 7525
train acc:  0.7265625
train loss:  0.5057269334793091
train gradient:  0.12985447525373806
iteration : 7526
train acc:  0.6953125
train loss:  0.5307230353355408
train gradient:  0.12749454140537342
iteration : 7527
train acc:  0.6484375
train loss:  0.5491573810577393
train gradient:  0.14744816577258357
iteration : 7528
train acc:  0.7734375
train loss:  0.47664886713027954
train gradient:  0.12145007672989303
iteration : 7529
train acc:  0.84375
train loss:  0.36701759696006775
train gradient:  0.10562107605603724
iteration : 7530
train acc:  0.78125
train loss:  0.46953362226486206
train gradient:  0.12048072118329085
iteration : 7531
train acc:  0.7578125
train loss:  0.4804125428199768
train gradient:  0.1251213024090855
iteration : 7532
train acc:  0.765625
train loss:  0.4717237055301666
train gradient:  0.10007915158342039
iteration : 7533
train acc:  0.7421875
train loss:  0.5481790900230408
train gradient:  0.16879810262035577
iteration : 7534
train acc:  0.7578125
train loss:  0.4938594698905945
train gradient:  0.15901943766232865
iteration : 7535
train acc:  0.7734375
train loss:  0.44544702768325806
train gradient:  0.10535756246419173
iteration : 7536
train acc:  0.765625
train loss:  0.4635537266731262
train gradient:  0.0943808578094768
iteration : 7537
train acc:  0.7578125
train loss:  0.46207743883132935
train gradient:  0.12934034094014651
iteration : 7538
train acc:  0.7265625
train loss:  0.5054949522018433
train gradient:  0.1587982940305821
iteration : 7539
train acc:  0.6953125
train loss:  0.5490425825119019
train gradient:  0.13936635754314416
iteration : 7540
train acc:  0.7109375
train loss:  0.5239629149436951
train gradient:  0.1803461695561434
iteration : 7541
train acc:  0.7109375
train loss:  0.5497851371765137
train gradient:  0.1483815455298613
iteration : 7542
train acc:  0.75
train loss:  0.5005279779434204
train gradient:  0.17387538117129503
iteration : 7543
train acc:  0.71875
train loss:  0.5355228781700134
train gradient:  0.18522503627559012
iteration : 7544
train acc:  0.6484375
train loss:  0.5755599141120911
train gradient:  0.18126885532836656
iteration : 7545
train acc:  0.734375
train loss:  0.5380299687385559
train gradient:  0.1887431461712277
iteration : 7546
train acc:  0.734375
train loss:  0.47044986486434937
train gradient:  0.14524630811944
iteration : 7547
train acc:  0.8046875
train loss:  0.4222901463508606
train gradient:  0.11275641261089624
iteration : 7548
train acc:  0.71875
train loss:  0.5182881951332092
train gradient:  0.15818947990028537
iteration : 7549
train acc:  0.6953125
train loss:  0.5175069570541382
train gradient:  0.13927406448202284
iteration : 7550
train acc:  0.7109375
train loss:  0.5857568979263306
train gradient:  0.20364813609044463
iteration : 7551
train acc:  0.6953125
train loss:  0.5138615369796753
train gradient:  0.1351690794751953
iteration : 7552
train acc:  0.671875
train loss:  0.6092907190322876
train gradient:  0.16906777987023092
iteration : 7553
train acc:  0.765625
train loss:  0.46704813838005066
train gradient:  0.12127465735769953
iteration : 7554
train acc:  0.71875
train loss:  0.534109890460968
train gradient:  0.13870972931750788
iteration : 7555
train acc:  0.8046875
train loss:  0.45379745960235596
train gradient:  0.12201122914009985
iteration : 7556
train acc:  0.75
train loss:  0.49255892634391785
train gradient:  0.13068228305062898
iteration : 7557
train acc:  0.71875
train loss:  0.540556013584137
train gradient:  0.14805940337822165
iteration : 7558
train acc:  0.71875
train loss:  0.4863063395023346
train gradient:  0.12588378973462022
iteration : 7559
train acc:  0.6875
train loss:  0.4942224621772766
train gradient:  0.15301167581631372
iteration : 7560
train acc:  0.6875
train loss:  0.507306694984436
train gradient:  0.12640075003778117
iteration : 7561
train acc:  0.65625
train loss:  0.5382378101348877
train gradient:  0.1275130751553253
iteration : 7562
train acc:  0.75
train loss:  0.48993539810180664
train gradient:  0.12591551204207643
iteration : 7563
train acc:  0.7421875
train loss:  0.4929332733154297
train gradient:  0.13930120925382128
iteration : 7564
train acc:  0.6875
train loss:  0.5307809114456177
train gradient:  0.13803735873440934
iteration : 7565
train acc:  0.7734375
train loss:  0.4513495862483978
train gradient:  0.10156642055089436
iteration : 7566
train acc:  0.6640625
train loss:  0.5566980838775635
train gradient:  0.19365879549350473
iteration : 7567
train acc:  0.7109375
train loss:  0.509026288986206
train gradient:  0.1459457275236502
iteration : 7568
train acc:  0.7265625
train loss:  0.5732657313346863
train gradient:  0.1843860337731138
iteration : 7569
train acc:  0.75
train loss:  0.5627952218055725
train gradient:  0.17812655771336872
iteration : 7570
train acc:  0.7265625
train loss:  0.5196863412857056
train gradient:  0.13698758972982494
iteration : 7571
train acc:  0.765625
train loss:  0.47469618916511536
train gradient:  0.117623925246267
iteration : 7572
train acc:  0.8359375
train loss:  0.436105340719223
train gradient:  0.11408978154776912
iteration : 7573
train acc:  0.734375
train loss:  0.48679447174072266
train gradient:  0.12015022543618882
iteration : 7574
train acc:  0.7265625
train loss:  0.5235744714736938
train gradient:  0.1770561267583305
iteration : 7575
train acc:  0.796875
train loss:  0.4466897249221802
train gradient:  0.11892263216005668
iteration : 7576
train acc:  0.8046875
train loss:  0.4457317888736725
train gradient:  0.10515475432185156
iteration : 7577
train acc:  0.734375
train loss:  0.45711660385131836
train gradient:  0.11114612887438477
iteration : 7578
train acc:  0.7734375
train loss:  0.5047879219055176
train gradient:  0.12896897622741593
iteration : 7579
train acc:  0.75
train loss:  0.490732342004776
train gradient:  0.11212310434419542
iteration : 7580
train acc:  0.7421875
train loss:  0.5508487224578857
train gradient:  0.13479389897503075
iteration : 7581
train acc:  0.734375
train loss:  0.5266861915588379
train gradient:  0.13257713396260823
iteration : 7582
train acc:  0.7421875
train loss:  0.48696398735046387
train gradient:  0.13196582944667018
iteration : 7583
train acc:  0.7734375
train loss:  0.5028320550918579
train gradient:  0.11950441941590675
iteration : 7584
train acc:  0.75
train loss:  0.4791577160358429
train gradient:  0.09239586717721908
iteration : 7585
train acc:  0.8046875
train loss:  0.44655224680900574
train gradient:  0.1466209464706995
iteration : 7586
train acc:  0.78125
train loss:  0.4501728415489197
train gradient:  0.13184113632793482
iteration : 7587
train acc:  0.75
train loss:  0.48412027955055237
train gradient:  0.11034100433510803
iteration : 7588
train acc:  0.75
train loss:  0.4925028085708618
train gradient:  0.13424016437174546
iteration : 7589
train acc:  0.75
train loss:  0.45718175172805786
train gradient:  0.1189294233482441
iteration : 7590
train acc:  0.703125
train loss:  0.595429003238678
train gradient:  0.18465953506336502
iteration : 7591
train acc:  0.7890625
train loss:  0.4545952081680298
train gradient:  0.09914333079432465
iteration : 7592
train acc:  0.7109375
train loss:  0.5187772512435913
train gradient:  0.1430831100084407
iteration : 7593
train acc:  0.7734375
train loss:  0.4872470200061798
train gradient:  0.1246638584630465
iteration : 7594
train acc:  0.7734375
train loss:  0.46216657757759094
train gradient:  0.1210401382046837
iteration : 7595
train acc:  0.75
train loss:  0.5189443826675415
train gradient:  0.21452243943761845
iteration : 7596
train acc:  0.7421875
train loss:  0.5278996229171753
train gradient:  0.1471611532080251
iteration : 7597
train acc:  0.6796875
train loss:  0.5413162708282471
train gradient:  0.1537282755428691
iteration : 7598
train acc:  0.734375
train loss:  0.5855061411857605
train gradient:  0.21664237446968515
iteration : 7599
train acc:  0.78125
train loss:  0.45247507095336914
train gradient:  0.10921272225621433
iteration : 7600
train acc:  0.7421875
train loss:  0.555905818939209
train gradient:  0.1567852077209405
iteration : 7601
train acc:  0.7265625
train loss:  0.5142273306846619
train gradient:  0.21741693032053716
iteration : 7602
train acc:  0.71875
train loss:  0.503552258014679
train gradient:  0.12695996508145976
iteration : 7603
train acc:  0.7109375
train loss:  0.5057097673416138
train gradient:  0.14947130553557392
iteration : 7604
train acc:  0.7109375
train loss:  0.5423722267150879
train gradient:  0.13047513851103348
iteration : 7605
train acc:  0.7421875
train loss:  0.5244399905204773
train gradient:  0.1762714414385148
iteration : 7606
train acc:  0.71875
train loss:  0.4623238742351532
train gradient:  0.11038488431675537
iteration : 7607
train acc:  0.765625
train loss:  0.461495965719223
train gradient:  0.1148956991668506
iteration : 7608
train acc:  0.7421875
train loss:  0.5030581951141357
train gradient:  0.12838947368413378
iteration : 7609
train acc:  0.7578125
train loss:  0.5095872282981873
train gradient:  0.16286490127493053
iteration : 7610
train acc:  0.765625
train loss:  0.4473905861377716
train gradient:  0.11486952075049364
iteration : 7611
train acc:  0.7109375
train loss:  0.525263786315918
train gradient:  0.16616241337174031
iteration : 7612
train acc:  0.78125
train loss:  0.46208205819129944
train gradient:  0.16072405324742722
iteration : 7613
train acc:  0.75
train loss:  0.4835938513278961
train gradient:  0.12540610326472368
iteration : 7614
train acc:  0.71875
train loss:  0.5121325850486755
train gradient:  0.1495281925880825
iteration : 7615
train acc:  0.734375
train loss:  0.5068573951721191
train gradient:  0.15421329184048604
iteration : 7616
train acc:  0.6953125
train loss:  0.5130251049995422
train gradient:  0.14592223099795654
iteration : 7617
train acc:  0.7734375
train loss:  0.46326929330825806
train gradient:  0.09405273711216047
iteration : 7618
train acc:  0.765625
train loss:  0.4569743275642395
train gradient:  0.1405205972978696
iteration : 7619
train acc:  0.7734375
train loss:  0.49793797731399536
train gradient:  0.1338173402889309
iteration : 7620
train acc:  0.7578125
train loss:  0.47196030616760254
train gradient:  0.1621150550473791
iteration : 7621
train acc:  0.7421875
train loss:  0.48549404740333557
train gradient:  0.13151160205577955
iteration : 7622
train acc:  0.734375
train loss:  0.4881897568702698
train gradient:  0.12042160797857011
iteration : 7623
train acc:  0.7890625
train loss:  0.430591881275177
train gradient:  0.094024608187851
iteration : 7624
train acc:  0.6640625
train loss:  0.5619587898254395
train gradient:  0.1578348009601036
iteration : 7625
train acc:  0.75
train loss:  0.4699794054031372
train gradient:  0.15294511852354759
iteration : 7626
train acc:  0.7421875
train loss:  0.5069580078125
train gradient:  0.11387968425431483
iteration : 7627
train acc:  0.75
train loss:  0.5118304491043091
train gradient:  0.13161300934488868
iteration : 7628
train acc:  0.7734375
train loss:  0.4815714359283447
train gradient:  0.11272983536871543
iteration : 7629
train acc:  0.7734375
train loss:  0.4599660336971283
train gradient:  0.13988821396845832
iteration : 7630
train acc:  0.7890625
train loss:  0.42994004487991333
train gradient:  0.10811831892929928
iteration : 7631
train acc:  0.7734375
train loss:  0.45098817348480225
train gradient:  0.09843647239115635
iteration : 7632
train acc:  0.765625
train loss:  0.43761059641838074
train gradient:  0.08851939384526242
iteration : 7633
train acc:  0.765625
train loss:  0.4707200229167938
train gradient:  0.12253082795296986
iteration : 7634
train acc:  0.71875
train loss:  0.48883920907974243
train gradient:  0.11605275240669025
iteration : 7635
train acc:  0.7109375
train loss:  0.5349175333976746
train gradient:  0.160002015025233
iteration : 7636
train acc:  0.7578125
train loss:  0.4935036897659302
train gradient:  0.09958923457882607
iteration : 7637
train acc:  0.671875
train loss:  0.5421985983848572
train gradient:  0.146755019467041
iteration : 7638
train acc:  0.75
train loss:  0.5152740478515625
train gradient:  0.10972812172695436
iteration : 7639
train acc:  0.765625
train loss:  0.44938021898269653
train gradient:  0.10317043398769103
iteration : 7640
train acc:  0.75
train loss:  0.49591872096061707
train gradient:  0.15705835718149158
iteration : 7641
train acc:  0.8046875
train loss:  0.45947617292404175
train gradient:  0.13488720418256359
iteration : 7642
train acc:  0.6875
train loss:  0.5221654176712036
train gradient:  0.14323590663548957
iteration : 7643
train acc:  0.6484375
train loss:  0.5960246324539185
train gradient:  0.17470457592527958
iteration : 7644
train acc:  0.7578125
train loss:  0.4748676121234894
train gradient:  0.11454842703819314
iteration : 7645
train acc:  0.7578125
train loss:  0.43944627046585083
train gradient:  0.11127279404545701
iteration : 7646
train acc:  0.7421875
train loss:  0.4606032967567444
train gradient:  0.1173044726273413
iteration : 7647
train acc:  0.7421875
train loss:  0.5048784613609314
train gradient:  0.1431009000637206
iteration : 7648
train acc:  0.75
train loss:  0.5070608854293823
train gradient:  0.1810970685073085
iteration : 7649
train acc:  0.7890625
train loss:  0.5000970363616943
train gradient:  0.13283074333445255
iteration : 7650
train acc:  0.7265625
train loss:  0.46580541133880615
train gradient:  0.1229110215170305
iteration : 7651
train acc:  0.71875
train loss:  0.4982524514198303
train gradient:  0.12711907135796718
iteration : 7652
train acc:  0.71875
train loss:  0.49475786089897156
train gradient:  0.13496352497526365
iteration : 7653
train acc:  0.75
train loss:  0.4618832468986511
train gradient:  0.12308085001124612
iteration : 7654
train acc:  0.6796875
train loss:  0.6009868383407593
train gradient:  0.17011049822503732
iteration : 7655
train acc:  0.71875
train loss:  0.4818655252456665
train gradient:  0.11197091597248691
iteration : 7656
train acc:  0.765625
train loss:  0.46780622005462646
train gradient:  0.13594852552827985
iteration : 7657
train acc:  0.7890625
train loss:  0.47054433822631836
train gradient:  0.10665099904631538
iteration : 7658
train acc:  0.6796875
train loss:  0.5495861768722534
train gradient:  0.18644052071908773
iteration : 7659
train acc:  0.703125
train loss:  0.496398389339447
train gradient:  0.13558585605315704
iteration : 7660
train acc:  0.7109375
train loss:  0.5490087866783142
train gradient:  0.2186386589467686
iteration : 7661
train acc:  0.796875
train loss:  0.4130764603614807
train gradient:  0.08927736570465883
iteration : 7662
train acc:  0.7578125
train loss:  0.5008585453033447
train gradient:  0.11161384989590142
iteration : 7663
train acc:  0.765625
train loss:  0.46030426025390625
train gradient:  0.11670067415159581
iteration : 7664
train acc:  0.7734375
train loss:  0.4562740921974182
train gradient:  0.11053715568556333
iteration : 7665
train acc:  0.734375
train loss:  0.48957812786102295
train gradient:  0.13199034074926136
iteration : 7666
train acc:  0.6875
train loss:  0.5046015977859497
train gradient:  0.1248433617280418
iteration : 7667
train acc:  0.78125
train loss:  0.4301007091999054
train gradient:  0.09681684781848086
iteration : 7668
train acc:  0.703125
train loss:  0.5275076031684875
train gradient:  0.14760609244111977
iteration : 7669
train acc:  0.734375
train loss:  0.517348051071167
train gradient:  0.13421164457799095
iteration : 7670
train acc:  0.7890625
train loss:  0.42797547578811646
train gradient:  0.09659132307504277
iteration : 7671
train acc:  0.78125
train loss:  0.4701005220413208
train gradient:  0.11742907836219041
iteration : 7672
train acc:  0.75
train loss:  0.5173347592353821
train gradient:  0.17743735953631967
iteration : 7673
train acc:  0.7421875
train loss:  0.4520592987537384
train gradient:  0.13079872706784196
iteration : 7674
train acc:  0.734375
train loss:  0.4851658046245575
train gradient:  0.12381674618173684
iteration : 7675
train acc:  0.7265625
train loss:  0.52398681640625
train gradient:  0.13186976843466097
iteration : 7676
train acc:  0.671875
train loss:  0.5212329626083374
train gradient:  0.12994705253926897
iteration : 7677
train acc:  0.7578125
train loss:  0.45378583669662476
train gradient:  0.1310325070841924
iteration : 7678
train acc:  0.796875
train loss:  0.4779706597328186
train gradient:  0.12791593523960754
iteration : 7679
train acc:  0.71875
train loss:  0.515800952911377
train gradient:  0.14953616348174364
iteration : 7680
train acc:  0.6953125
train loss:  0.4897969961166382
train gradient:  0.11582266015594023
iteration : 7681
train acc:  0.7109375
train loss:  0.47109711170196533
train gradient:  0.13796226322568125
iteration : 7682
train acc:  0.7265625
train loss:  0.48424845933914185
train gradient:  0.15221011266359324
iteration : 7683
train acc:  0.765625
train loss:  0.49114951491355896
train gradient:  0.16073080006389245
iteration : 7684
train acc:  0.7890625
train loss:  0.4748404324054718
train gradient:  0.1636710312100409
iteration : 7685
train acc:  0.75
train loss:  0.507103681564331
train gradient:  0.12187956205338295
iteration : 7686
train acc:  0.65625
train loss:  0.5661165714263916
train gradient:  0.21975421991266247
iteration : 7687
train acc:  0.703125
train loss:  0.5293340086936951
train gradient:  0.1657529354303341
iteration : 7688
train acc:  0.765625
train loss:  0.49907177686691284
train gradient:  0.1573309289142745
iteration : 7689
train acc:  0.7890625
train loss:  0.4028654992580414
train gradient:  0.0977168456745782
iteration : 7690
train acc:  0.671875
train loss:  0.5610723495483398
train gradient:  0.16835493546850494
iteration : 7691
train acc:  0.734375
train loss:  0.46545517444610596
train gradient:  0.16613582736827087
iteration : 7692
train acc:  0.75
train loss:  0.4572522044181824
train gradient:  0.09385341637168666
iteration : 7693
train acc:  0.734375
train loss:  0.5330487489700317
train gradient:  0.15810572333827266
iteration : 7694
train acc:  0.75
train loss:  0.44721949100494385
train gradient:  0.10964151833512897
iteration : 7695
train acc:  0.796875
train loss:  0.4513046443462372
train gradient:  0.10893776056680068
iteration : 7696
train acc:  0.75
train loss:  0.5116515755653381
train gradient:  0.11832518368973802
iteration : 7697
train acc:  0.765625
train loss:  0.49120932817459106
train gradient:  0.17375058619573316
iteration : 7698
train acc:  0.6953125
train loss:  0.5475908517837524
train gradient:  0.15567901037253778
iteration : 7699
train acc:  0.7578125
train loss:  0.47887086868286133
train gradient:  0.14410674055632028
iteration : 7700
train acc:  0.671875
train loss:  0.5267365574836731
train gradient:  0.20744611949776415
iteration : 7701
train acc:  0.75
train loss:  0.49358272552490234
train gradient:  0.13682426621296745
iteration : 7702
train acc:  0.6484375
train loss:  0.6421080827713013
train gradient:  0.19579146138821835
iteration : 7703
train acc:  0.7109375
train loss:  0.5087324380874634
train gradient:  0.13651708540873964
iteration : 7704
train acc:  0.7265625
train loss:  0.5247975587844849
train gradient:  0.14599559847940413
iteration : 7705
train acc:  0.765625
train loss:  0.49334293603897095
train gradient:  0.13583628144712717
iteration : 7706
train acc:  0.71875
train loss:  0.5352872014045715
train gradient:  0.18610919322969285
iteration : 7707
train acc:  0.75
train loss:  0.4755643308162689
train gradient:  0.11818214803219453
iteration : 7708
train acc:  0.7734375
train loss:  0.46186214685440063
train gradient:  0.1124075558005553
iteration : 7709
train acc:  0.765625
train loss:  0.4628559350967407
train gradient:  0.10095284471722947
iteration : 7710
train acc:  0.7734375
train loss:  0.48424240946769714
train gradient:  0.13064984044780859
iteration : 7711
train acc:  0.75
train loss:  0.48694080114364624
train gradient:  0.10826550875559281
iteration : 7712
train acc:  0.7109375
train loss:  0.5003807544708252
train gradient:  0.12160552590936924
iteration : 7713
train acc:  0.703125
train loss:  0.5268704891204834
train gradient:  0.17379734625658733
iteration : 7714
train acc:  0.7265625
train loss:  0.5256871581077576
train gradient:  0.13629740837247967
iteration : 7715
train acc:  0.7890625
train loss:  0.5039014220237732
train gradient:  0.12648886948646393
iteration : 7716
train acc:  0.71875
train loss:  0.49736151099205017
train gradient:  0.12762271849881435
iteration : 7717
train acc:  0.734375
train loss:  0.44451749324798584
train gradient:  0.10994336622596597
iteration : 7718
train acc:  0.734375
train loss:  0.5329292416572571
train gradient:  0.1395960773808774
iteration : 7719
train acc:  0.7734375
train loss:  0.4600491225719452
train gradient:  0.11506621498190671
iteration : 7720
train acc:  0.7578125
train loss:  0.4597005248069763
train gradient:  0.10694807305349288
iteration : 7721
train acc:  0.765625
train loss:  0.4653453826904297
train gradient:  0.12585152969573044
iteration : 7722
train acc:  0.8046875
train loss:  0.44512251019477844
train gradient:  0.12407736050040441
iteration : 7723
train acc:  0.7734375
train loss:  0.4750640392303467
train gradient:  0.11355268583058765
iteration : 7724
train acc:  0.796875
train loss:  0.4142257571220398
train gradient:  0.10940701832718643
iteration : 7725
train acc:  0.734375
train loss:  0.4876534342765808
train gradient:  0.13635793111868366
iteration : 7726
train acc:  0.78125
train loss:  0.4710789620876312
train gradient:  0.10994915458984897
iteration : 7727
train acc:  0.7578125
train loss:  0.4869050085544586
train gradient:  0.14928314765510925
iteration : 7728
train acc:  0.7421875
train loss:  0.5231411457061768
train gradient:  0.1502473778863554
iteration : 7729
train acc:  0.7421875
train loss:  0.5309011340141296
train gradient:  0.17503911290713298
iteration : 7730
train acc:  0.734375
train loss:  0.5243350267410278
train gradient:  0.16808944006588666
iteration : 7731
train acc:  0.734375
train loss:  0.47367557883262634
train gradient:  0.12859336651362013
iteration : 7732
train acc:  0.75
train loss:  0.47240710258483887
train gradient:  0.14435897945157186
iteration : 7733
train acc:  0.7578125
train loss:  0.4653446674346924
train gradient:  0.12821656707185658
iteration : 7734
train acc:  0.78125
train loss:  0.43083515763282776
train gradient:  0.1244395376333579
iteration : 7735
train acc:  0.7421875
train loss:  0.4550197124481201
train gradient:  0.12258608588184702
iteration : 7736
train acc:  0.6875
train loss:  0.5583066344261169
train gradient:  0.18990398002835868
iteration : 7737
train acc:  0.7890625
train loss:  0.47368866205215454
train gradient:  0.20800387255537378
iteration : 7738
train acc:  0.6640625
train loss:  0.6038987636566162
train gradient:  0.19378936036233774
iteration : 7739
train acc:  0.7265625
train loss:  0.5132062435150146
train gradient:  0.18662601563052622
iteration : 7740
train acc:  0.75
train loss:  0.5137107372283936
train gradient:  0.17421494576475627
iteration : 7741
train acc:  0.71875
train loss:  0.562027096748352
train gradient:  0.22142160126194282
iteration : 7742
train acc:  0.7578125
train loss:  0.5101802349090576
train gradient:  0.1319594535194239
iteration : 7743
train acc:  0.8046875
train loss:  0.43068820238113403
train gradient:  0.13134909436778594
iteration : 7744
train acc:  0.7421875
train loss:  0.4912045896053314
train gradient:  0.1245773994770254
iteration : 7745
train acc:  0.7421875
train loss:  0.4409830868244171
train gradient:  0.10271932978869187
iteration : 7746
train acc:  0.7734375
train loss:  0.44355952739715576
train gradient:  0.11423141738267516
iteration : 7747
train acc:  0.71875
train loss:  0.5634171962738037
train gradient:  0.15784478609369207
iteration : 7748
train acc:  0.7109375
train loss:  0.522721529006958
train gradient:  0.14734450867580623
iteration : 7749
train acc:  0.71875
train loss:  0.5096597671508789
train gradient:  0.14196016471797152
iteration : 7750
train acc:  0.703125
train loss:  0.5207154750823975
train gradient:  0.14730980110869063
iteration : 7751
train acc:  0.7421875
train loss:  0.5074338912963867
train gradient:  0.1504054202972473
iteration : 7752
train acc:  0.7890625
train loss:  0.46948984265327454
train gradient:  0.1381633155801538
iteration : 7753
train acc:  0.6953125
train loss:  0.49695807695388794
train gradient:  0.12060472330974728
iteration : 7754
train acc:  0.78125
train loss:  0.46340465545654297
train gradient:  0.10579115032821564
iteration : 7755
train acc:  0.6640625
train loss:  0.5300354957580566
train gradient:  0.15026755871907296
iteration : 7756
train acc:  0.6875
train loss:  0.598701000213623
train gradient:  0.20368025630251602
iteration : 7757
train acc:  0.734375
train loss:  0.48465994000434875
train gradient:  0.13929882491966517
iteration : 7758
train acc:  0.65625
train loss:  0.607318639755249
train gradient:  0.17482001909020864
iteration : 7759
train acc:  0.8046875
train loss:  0.4554833173751831
train gradient:  0.11735319129191704
iteration : 7760
train acc:  0.7578125
train loss:  0.49038034677505493
train gradient:  0.1224180510811644
iteration : 7761
train acc:  0.7734375
train loss:  0.47072821855545044
train gradient:  0.1172586447418351
iteration : 7762
train acc:  0.7734375
train loss:  0.5395270586013794
train gradient:  0.1753441037959409
iteration : 7763
train acc:  0.7890625
train loss:  0.45723626017570496
train gradient:  0.15041119574617953
iteration : 7764
train acc:  0.7578125
train loss:  0.494854211807251
train gradient:  0.167546693008677
iteration : 7765
train acc:  0.6953125
train loss:  0.5661430358886719
train gradient:  0.20021262056919714
iteration : 7766
train acc:  0.7578125
train loss:  0.469654381275177
train gradient:  0.12037732090160588
iteration : 7767
train acc:  0.75
train loss:  0.5160763263702393
train gradient:  0.1515305638635746
iteration : 7768
train acc:  0.7265625
train loss:  0.5202039480209351
train gradient:  0.148750832578245
iteration : 7769
train acc:  0.6953125
train loss:  0.5859752893447876
train gradient:  0.208419581879785
iteration : 7770
train acc:  0.7578125
train loss:  0.5291063189506531
train gradient:  0.17677171668064856
iteration : 7771
train acc:  0.75
train loss:  0.45471328496932983
train gradient:  0.10720460719153672
iteration : 7772
train acc:  0.8203125
train loss:  0.38443636894226074
train gradient:  0.10814047866555528
iteration : 7773
train acc:  0.75
train loss:  0.5108489990234375
train gradient:  0.22994593230590116
iteration : 7774
train acc:  0.796875
train loss:  0.4746304750442505
train gradient:  0.1671635460522055
iteration : 7775
train acc:  0.7890625
train loss:  0.4476146399974823
train gradient:  0.18931709863865936
iteration : 7776
train acc:  0.7578125
train loss:  0.47874343395233154
train gradient:  0.13798808594439138
iteration : 7777
train acc:  0.7578125
train loss:  0.5135191082954407
train gradient:  0.14387446782569224
iteration : 7778
train acc:  0.71875
train loss:  0.5435940027236938
train gradient:  0.1605385504952948
iteration : 7779
train acc:  0.7421875
train loss:  0.49722039699554443
train gradient:  0.14667827202155825
iteration : 7780
train acc:  0.7109375
train loss:  0.4837042987346649
train gradient:  0.13684832458769383
iteration : 7781
train acc:  0.7265625
train loss:  0.4713283181190491
train gradient:  0.17639930512674834
iteration : 7782
train acc:  0.6953125
train loss:  0.5660070180892944
train gradient:  0.1695010106040825
iteration : 7783
train acc:  0.7265625
train loss:  0.46403223276138306
train gradient:  0.11858102628317438
iteration : 7784
train acc:  0.7265625
train loss:  0.5359875559806824
train gradient:  0.1799381282940145
iteration : 7785
train acc:  0.7265625
train loss:  0.47103336453437805
train gradient:  0.12532830864066152
iteration : 7786
train acc:  0.75
train loss:  0.4941776692867279
train gradient:  0.13309965837332943
iteration : 7787
train acc:  0.7890625
train loss:  0.4332250654697418
train gradient:  0.10067631285313122
iteration : 7788
train acc:  0.7109375
train loss:  0.5276253819465637
train gradient:  0.11688043129381735
iteration : 7789
train acc:  0.7421875
train loss:  0.55051589012146
train gradient:  0.18104531521714268
iteration : 7790
train acc:  0.75
train loss:  0.5171956419944763
train gradient:  0.17808475166138482
iteration : 7791
train acc:  0.71875
train loss:  0.5212023258209229
train gradient:  0.1376464619546142
iteration : 7792
train acc:  0.6875
train loss:  0.5377587676048279
train gradient:  0.17750135361957384
iteration : 7793
train acc:  0.7578125
train loss:  0.4884773790836334
train gradient:  0.12825688147546202
iteration : 7794
train acc:  0.7734375
train loss:  0.43413859605789185
train gradient:  0.1758425718373917
iteration : 7795
train acc:  0.7734375
train loss:  0.44191429018974304
train gradient:  0.1233333121240076
iteration : 7796
train acc:  0.71875
train loss:  0.5838043689727783
train gradient:  0.19083580092449304
iteration : 7797
train acc:  0.671875
train loss:  0.4845837652683258
train gradient:  0.17122671964464037
iteration : 7798
train acc:  0.765625
train loss:  0.43919041752815247
train gradient:  0.11728383089498493
iteration : 7799
train acc:  0.71875
train loss:  0.5402778387069702
train gradient:  0.14222158030865092
iteration : 7800
train acc:  0.671875
train loss:  0.5792196989059448
train gradient:  0.17238165631547864
iteration : 7801
train acc:  0.65625
train loss:  0.5741031765937805
train gradient:  0.17390996507729345
iteration : 7802
train acc:  0.6484375
train loss:  0.5939828157424927
train gradient:  0.15615336025016505
iteration : 7803
train acc:  0.6875
train loss:  0.5760999321937561
train gradient:  0.17602612593106637
iteration : 7804
train acc:  0.71875
train loss:  0.5692163705825806
train gradient:  0.16629514699539696
iteration : 7805
train acc:  0.7265625
train loss:  0.5133811831474304
train gradient:  0.11814373236339186
iteration : 7806
train acc:  0.796875
train loss:  0.4565143585205078
train gradient:  0.10158395616239839
iteration : 7807
train acc:  0.7734375
train loss:  0.5047311782836914
train gradient:  0.14707823059440256
iteration : 7808
train acc:  0.7578125
train loss:  0.466246098279953
train gradient:  0.12209965403171073
iteration : 7809
train acc:  0.765625
train loss:  0.5159105062484741
train gradient:  0.1097932315906451
iteration : 7810
train acc:  0.7109375
train loss:  0.4897291958332062
train gradient:  0.1414002653897643
iteration : 7811
train acc:  0.75
train loss:  0.5479753017425537
train gradient:  0.15963111977717087
iteration : 7812
train acc:  0.7109375
train loss:  0.5391021966934204
train gradient:  0.13369274955178953
iteration : 7813
train acc:  0.7421875
train loss:  0.5162997245788574
train gradient:  0.12773022307520482
iteration : 7814
train acc:  0.7421875
train loss:  0.4712558686733246
train gradient:  0.10208174885250468
iteration : 7815
train acc:  0.7265625
train loss:  0.504314661026001
train gradient:  0.13020657529140622
iteration : 7816
train acc:  0.71875
train loss:  0.5120440721511841
train gradient:  0.13219831736515472
iteration : 7817
train acc:  0.7578125
train loss:  0.4462379813194275
train gradient:  0.10168142212789938
iteration : 7818
train acc:  0.71875
train loss:  0.5099815130233765
train gradient:  0.17997182655064653
iteration : 7819
train acc:  0.7265625
train loss:  0.507727861404419
train gradient:  0.13486612085288713
iteration : 7820
train acc:  0.7734375
train loss:  0.48032283782958984
train gradient:  0.1302727115283498
iteration : 7821
train acc:  0.796875
train loss:  0.5154260396957397
train gradient:  0.1594496205405368
iteration : 7822
train acc:  0.671875
train loss:  0.5315428972244263
train gradient:  0.15751284173580168
iteration : 7823
train acc:  0.71875
train loss:  0.5302377939224243
train gradient:  0.21618570647045937
iteration : 7824
train acc:  0.71875
train loss:  0.48481273651123047
train gradient:  0.1347874867533379
iteration : 7825
train acc:  0.7734375
train loss:  0.5282561182975769
train gradient:  0.12364975180370041
iteration : 7826
train acc:  0.6171875
train loss:  0.6490574479103088
train gradient:  0.1932708250570973
iteration : 7827
train acc:  0.7578125
train loss:  0.5138382315635681
train gradient:  0.135714986511983
iteration : 7828
train acc:  0.765625
train loss:  0.4370928406715393
train gradient:  0.09834524239322433
iteration : 7829
train acc:  0.65625
train loss:  0.5875256061553955
train gradient:  0.2092767731329136
iteration : 7830
train acc:  0.7578125
train loss:  0.4935413599014282
train gradient:  0.2070715795900635
iteration : 7831
train acc:  0.7109375
train loss:  0.5125998258590698
train gradient:  0.1470990153728703
iteration : 7832
train acc:  0.75
train loss:  0.5146722793579102
train gradient:  0.12813048769881047
iteration : 7833
train acc:  0.765625
train loss:  0.5289151668548584
train gradient:  0.13127868862598396
iteration : 7834
train acc:  0.6875
train loss:  0.5029399394989014
train gradient:  0.13104697377435232
iteration : 7835
train acc:  0.7265625
train loss:  0.5139126777648926
train gradient:  0.13071029400478837
iteration : 7836
train acc:  0.765625
train loss:  0.4254114627838135
train gradient:  0.10174581395591879
iteration : 7837
train acc:  0.7890625
train loss:  0.47893401980400085
train gradient:  0.09704285199576806
iteration : 7838
train acc:  0.7109375
train loss:  0.5401052236557007
train gradient:  0.15785376094278944
iteration : 7839
train acc:  0.7890625
train loss:  0.4490719437599182
train gradient:  0.12862437122215517
iteration : 7840
train acc:  0.7109375
train loss:  0.5705562233924866
train gradient:  0.14831354591828796
iteration : 7841
train acc:  0.75
train loss:  0.5458219051361084
train gradient:  0.136189906524121
iteration : 7842
train acc:  0.734375
train loss:  0.5664520263671875
train gradient:  0.14769102400052475
iteration : 7843
train acc:  0.7578125
train loss:  0.4930008053779602
train gradient:  0.11061207763413697
iteration : 7844
train acc:  0.7890625
train loss:  0.4595503807067871
train gradient:  0.09917402865645
iteration : 7845
train acc:  0.6796875
train loss:  0.505603551864624
train gradient:  0.1683103917731688
iteration : 7846
train acc:  0.78125
train loss:  0.5088767409324646
train gradient:  0.14293914637303967
iteration : 7847
train acc:  0.765625
train loss:  0.5022484660148621
train gradient:  0.11515779699623732
iteration : 7848
train acc:  0.8046875
train loss:  0.42391055822372437
train gradient:  0.0992990254267072
iteration : 7849
train acc:  0.734375
train loss:  0.4966530203819275
train gradient:  0.11114496434174063
iteration : 7850
train acc:  0.7734375
train loss:  0.45353806018829346
train gradient:  0.10413511607656406
iteration : 7851
train acc:  0.734375
train loss:  0.5028225183486938
train gradient:  0.13571062358232316
iteration : 7852
train acc:  0.734375
train loss:  0.5384045839309692
train gradient:  0.1525978632212816
iteration : 7853
train acc:  0.765625
train loss:  0.482532262802124
train gradient:  0.12681873282107797
iteration : 7854
train acc:  0.734375
train loss:  0.4647870659828186
train gradient:  0.11393751027021537
iteration : 7855
train acc:  0.703125
train loss:  0.5239210724830627
train gradient:  0.14725367141139253
iteration : 7856
train acc:  0.78125
train loss:  0.4850778579711914
train gradient:  0.13220543315562389
iteration : 7857
train acc:  0.75
train loss:  0.5307965278625488
train gradient:  0.17345379963909052
iteration : 7858
train acc:  0.7109375
train loss:  0.5276991724967957
train gradient:  0.1250025446771185
iteration : 7859
train acc:  0.78125
train loss:  0.4740280210971832
train gradient:  0.1160363318946568
iteration : 7860
train acc:  0.671875
train loss:  0.5351176261901855
train gradient:  0.1405689554949741
iteration : 7861
train acc:  0.7578125
train loss:  0.4760570526123047
train gradient:  0.16426780416921377
iteration : 7862
train acc:  0.7109375
train loss:  0.49475425481796265
train gradient:  0.1154625790785743
iteration : 7863
train acc:  0.796875
train loss:  0.4330422878265381
train gradient:  0.10374885645044175
iteration : 7864
train acc:  0.765625
train loss:  0.5103820562362671
train gradient:  0.1149751842506804
iteration : 7865
train acc:  0.7265625
train loss:  0.5495307445526123
train gradient:  0.1413520467507589
iteration : 7866
train acc:  0.7421875
train loss:  0.49994802474975586
train gradient:  0.11509920534485772
iteration : 7867
train acc:  0.765625
train loss:  0.5269885063171387
train gradient:  0.14983807965052065
iteration : 7868
train acc:  0.765625
train loss:  0.4579491913318634
train gradient:  0.09628275626786192
iteration : 7869
train acc:  0.7265625
train loss:  0.5142539143562317
train gradient:  0.18455744833625332
iteration : 7870
train acc:  0.7421875
train loss:  0.4253844618797302
train gradient:  0.0996653400447835
iteration : 7871
train acc:  0.734375
train loss:  0.4979829490184784
train gradient:  0.15567832082921282
iteration : 7872
train acc:  0.6796875
train loss:  0.5855386257171631
train gradient:  0.19514839537013742
iteration : 7873
train acc:  0.671875
train loss:  0.5354146957397461
train gradient:  0.14293430446409228
iteration : 7874
train acc:  0.734375
train loss:  0.47111019492149353
train gradient:  0.12934591652775618
iteration : 7875
train acc:  0.65625
train loss:  0.5527504682540894
train gradient:  0.16220342192049333
iteration : 7876
train acc:  0.78125
train loss:  0.4348132610321045
train gradient:  0.1021066007766331
iteration : 7877
train acc:  0.75
train loss:  0.4699654281139374
train gradient:  0.11410379010436188
iteration : 7878
train acc:  0.7578125
train loss:  0.5020065307617188
train gradient:  0.1552790497625725
iteration : 7879
train acc:  0.8046875
train loss:  0.4534681737422943
train gradient:  0.11036973472795343
iteration : 7880
train acc:  0.734375
train loss:  0.45710355043411255
train gradient:  0.122272028871768
iteration : 7881
train acc:  0.734375
train loss:  0.48343342542648315
train gradient:  0.1278038683310208
iteration : 7882
train acc:  0.765625
train loss:  0.45296233892440796
train gradient:  0.11000538270504864
iteration : 7883
train acc:  0.6875
train loss:  0.5651299953460693
train gradient:  0.15247387937636397
iteration : 7884
train acc:  0.7578125
train loss:  0.47509121894836426
train gradient:  0.12516587000132795
iteration : 7885
train acc:  0.765625
train loss:  0.49072813987731934
train gradient:  0.110536083579561
iteration : 7886
train acc:  0.7265625
train loss:  0.5262857675552368
train gradient:  0.1548423099388591
iteration : 7887
train acc:  0.765625
train loss:  0.5074669718742371
train gradient:  0.18107150559683544
iteration : 7888
train acc:  0.7734375
train loss:  0.46714189648628235
train gradient:  0.12393675033320739
iteration : 7889
train acc:  0.734375
train loss:  0.4869631230831146
train gradient:  0.12882631863667143
iteration : 7890
train acc:  0.7890625
train loss:  0.46674099564552307
train gradient:  0.11651944526749794
iteration : 7891
train acc:  0.7578125
train loss:  0.5181717872619629
train gradient:  0.15251374549376356
iteration : 7892
train acc:  0.75
train loss:  0.5400770902633667
train gradient:  0.1351540277561801
iteration : 7893
train acc:  0.75
train loss:  0.4774059057235718
train gradient:  0.1052409204336104
iteration : 7894
train acc:  0.703125
train loss:  0.5689062476158142
train gradient:  0.1439640181233709
iteration : 7895
train acc:  0.7109375
train loss:  0.5434979200363159
train gradient:  0.14833694883520407
iteration : 7896
train acc:  0.78125
train loss:  0.4975864887237549
train gradient:  0.1089961074615458
iteration : 7897
train acc:  0.765625
train loss:  0.48884421586990356
train gradient:  0.12027411721966295
iteration : 7898
train acc:  0.6640625
train loss:  0.5446406602859497
train gradient:  0.16935332549481785
iteration : 7899
train acc:  0.6796875
train loss:  0.5819521546363831
train gradient:  0.1743224127082807
iteration : 7900
train acc:  0.7109375
train loss:  0.5726029872894287
train gradient:  0.15531269775271672
iteration : 7901
train acc:  0.7421875
train loss:  0.47938787937164307
train gradient:  0.1327708861335915
iteration : 7902
train acc:  0.7421875
train loss:  0.4819987416267395
train gradient:  0.13101511799826454
iteration : 7903
train acc:  0.75
train loss:  0.4458774924278259
train gradient:  0.0923915856667997
iteration : 7904
train acc:  0.734375
train loss:  0.48249000310897827
train gradient:  0.11262967081265499
iteration : 7905
train acc:  0.7578125
train loss:  0.4600767493247986
train gradient:  0.100106017630765
iteration : 7906
train acc:  0.703125
train loss:  0.5386204719543457
train gradient:  0.1655217912778379
iteration : 7907
train acc:  0.8046875
train loss:  0.47376713156700134
train gradient:  0.1360389748895731
iteration : 7908
train acc:  0.703125
train loss:  0.5189895629882812
train gradient:  0.1521978654113419
iteration : 7909
train acc:  0.7421875
train loss:  0.5161029696464539
train gradient:  0.1294137001939148
iteration : 7910
train acc:  0.7734375
train loss:  0.4814622104167938
train gradient:  0.0991568117684449
iteration : 7911
train acc:  0.75
train loss:  0.4677966833114624
train gradient:  0.13978985911611497
iteration : 7912
train acc:  0.78125
train loss:  0.4445330500602722
train gradient:  0.12225944970674685
iteration : 7913
train acc:  0.703125
train loss:  0.505329966545105
train gradient:  0.12974050277263519
iteration : 7914
train acc:  0.75
train loss:  0.49205201864242554
train gradient:  0.1185505333052671
iteration : 7915
train acc:  0.75
train loss:  0.5091897249221802
train gradient:  0.12604302212009114
iteration : 7916
train acc:  0.7890625
train loss:  0.47317036986351013
train gradient:  0.1322887789405614
iteration : 7917
train acc:  0.734375
train loss:  0.5073405504226685
train gradient:  0.13394330565120566
iteration : 7918
train acc:  0.78125
train loss:  0.4836963415145874
train gradient:  0.12758050248795774
iteration : 7919
train acc:  0.765625
train loss:  0.4968794584274292
train gradient:  0.12834113454484092
iteration : 7920
train acc:  0.7265625
train loss:  0.4908130168914795
train gradient:  0.13549674333429396
iteration : 7921
train acc:  0.7578125
train loss:  0.506783664226532
train gradient:  0.18538591739600857
iteration : 7922
train acc:  0.7578125
train loss:  0.49604111909866333
train gradient:  0.17016724613739043
iteration : 7923
train acc:  0.734375
train loss:  0.4972981810569763
train gradient:  0.1416928357612181
iteration : 7924
train acc:  0.6796875
train loss:  0.6137080192565918
train gradient:  0.19548977532795922
iteration : 7925
train acc:  0.75
train loss:  0.5215308666229248
train gradient:  0.14136166985301213
iteration : 7926
train acc:  0.7734375
train loss:  0.46819818019866943
train gradient:  0.13429819648068902
iteration : 7927
train acc:  0.6875
train loss:  0.5675239562988281
train gradient:  0.16798694976440398
iteration : 7928
train acc:  0.796875
train loss:  0.4749869704246521
train gradient:  0.13505780239075527
iteration : 7929
train acc:  0.75
train loss:  0.4655913710594177
train gradient:  0.12932273728561663
iteration : 7930
train acc:  0.7734375
train loss:  0.5149997472763062
train gradient:  0.14058739460548123
iteration : 7931
train acc:  0.7109375
train loss:  0.5768489837646484
train gradient:  0.16946854243651926
iteration : 7932
train acc:  0.78125
train loss:  0.49567118287086487
train gradient:  0.11619579652417006
iteration : 7933
train acc:  0.78125
train loss:  0.45909708738327026
train gradient:  0.1053510695511067
iteration : 7934
train acc:  0.7265625
train loss:  0.5023436546325684
train gradient:  0.13298184954233028
iteration : 7935
train acc:  0.625
train loss:  0.5592684745788574
train gradient:  0.15423763921915878
iteration : 7936
train acc:  0.7421875
train loss:  0.49227529764175415
train gradient:  0.1705017072652269
iteration : 7937
train acc:  0.71875
train loss:  0.5148534178733826
train gradient:  0.12375351687740893
iteration : 7938
train acc:  0.84375
train loss:  0.41122105717658997
train gradient:  0.10506230774437522
iteration : 7939
train acc:  0.7890625
train loss:  0.4355386793613434
train gradient:  0.10138365310302798
iteration : 7940
train acc:  0.7578125
train loss:  0.4754835069179535
train gradient:  0.10818743482887554
iteration : 7941
train acc:  0.71875
train loss:  0.4825085401535034
train gradient:  0.10997744157123876
iteration : 7942
train acc:  0.734375
train loss:  0.5659886598587036
train gradient:  0.18026154693670532
iteration : 7943
train acc:  0.7890625
train loss:  0.41020941734313965
train gradient:  0.09035870555847741
iteration : 7944
train acc:  0.7578125
train loss:  0.45809274911880493
train gradient:  0.09081575263093088
iteration : 7945
train acc:  0.6875
train loss:  0.4902764856815338
train gradient:  0.14525904158865155
iteration : 7946
train acc:  0.7578125
train loss:  0.5301451683044434
train gradient:  0.16923955589332124
iteration : 7947
train acc:  0.78125
train loss:  0.47333091497421265
train gradient:  0.09896168845795505
iteration : 7948
train acc:  0.6640625
train loss:  0.6077703237533569
train gradient:  0.1972547983717089
iteration : 7949
train acc:  0.703125
train loss:  0.5807980298995972
train gradient:  0.18034836251557906
iteration : 7950
train acc:  0.6953125
train loss:  0.5807287096977234
train gradient:  0.15855157189571806
iteration : 7951
train acc:  0.71875
train loss:  0.4941396713256836
train gradient:  0.14294151334294014
iteration : 7952
train acc:  0.65625
train loss:  0.5543370246887207
train gradient:  0.1592879840919615
iteration : 7953
train acc:  0.7890625
train loss:  0.4661138355731964
train gradient:  0.09140581122844398
iteration : 7954
train acc:  0.671875
train loss:  0.5265251398086548
train gradient:  0.11829260241641329
iteration : 7955
train acc:  0.71875
train loss:  0.5182255506515503
train gradient:  0.14368202598949875
iteration : 7956
train acc:  0.6796875
train loss:  0.5228269696235657
train gradient:  0.1281506348749623
iteration : 7957
train acc:  0.7421875
train loss:  0.4753555357456207
train gradient:  0.1172603813713923
iteration : 7958
train acc:  0.671875
train loss:  0.5423130989074707
train gradient:  0.1578280471311
iteration : 7959
train acc:  0.7109375
train loss:  0.5174506902694702
train gradient:  0.10328334746408407
iteration : 7960
train acc:  0.6484375
train loss:  0.5307665467262268
train gradient:  0.13292675400653478
iteration : 7961
train acc:  0.796875
train loss:  0.46214085817337036
train gradient:  0.1144213555171945
iteration : 7962
train acc:  0.7421875
train loss:  0.5041936635971069
train gradient:  0.13451490029198981
iteration : 7963
train acc:  0.7890625
train loss:  0.42635995149612427
train gradient:  0.0860235364453748
iteration : 7964
train acc:  0.8203125
train loss:  0.40816739201545715
train gradient:  0.09597531669362998
iteration : 7965
train acc:  0.7890625
train loss:  0.44155192375183105
train gradient:  0.11083304103691319
iteration : 7966
train acc:  0.7578125
train loss:  0.46681278944015503
train gradient:  0.1199124307273458
iteration : 7967
train acc:  0.7734375
train loss:  0.4462554454803467
train gradient:  0.08554728950751217
iteration : 7968
train acc:  0.7578125
train loss:  0.4881211519241333
train gradient:  0.11899511095906352
iteration : 7969
train acc:  0.78125
train loss:  0.4635852575302124
train gradient:  0.10680862085778219
iteration : 7970
train acc:  0.7578125
train loss:  0.4636971354484558
train gradient:  0.11303422538185473
iteration : 7971
train acc:  0.7109375
train loss:  0.5341620445251465
train gradient:  0.1376765854557082
iteration : 7972
train acc:  0.71875
train loss:  0.5216439962387085
train gradient:  0.11781052427911012
iteration : 7973
train acc:  0.734375
train loss:  0.5691272020339966
train gradient:  0.1876394793926618
iteration : 7974
train acc:  0.71875
train loss:  0.5784311294555664
train gradient:  0.18394902728078688
iteration : 7975
train acc:  0.71875
train loss:  0.526842474937439
train gradient:  0.16597239331956104
iteration : 7976
train acc:  0.7890625
train loss:  0.49190348386764526
train gradient:  0.1238727270876081
iteration : 7977
train acc:  0.7109375
train loss:  0.5380737781524658
train gradient:  0.11550677334128015
iteration : 7978
train acc:  0.7734375
train loss:  0.44580674171447754
train gradient:  0.09002319614987082
iteration : 7979
train acc:  0.7265625
train loss:  0.5049291849136353
train gradient:  0.13553501810434462
iteration : 7980
train acc:  0.703125
train loss:  0.49550437927246094
train gradient:  0.12425526531360852
iteration : 7981
train acc:  0.7265625
train loss:  0.5371455550193787
train gradient:  0.12600885775041776
iteration : 7982
train acc:  0.8046875
train loss:  0.4874510169029236
train gradient:  0.1352659802759535
iteration : 7983
train acc:  0.75
train loss:  0.485090434551239
train gradient:  0.16245829342988266
iteration : 7984
train acc:  0.7734375
train loss:  0.452069491147995
train gradient:  0.09852701965057896
iteration : 7985
train acc:  0.6953125
train loss:  0.5010745525360107
train gradient:  0.15244624712661786
iteration : 7986
train acc:  0.7109375
train loss:  0.5294415950775146
train gradient:  0.14497607348358701
iteration : 7987
train acc:  0.7265625
train loss:  0.552410364151001
train gradient:  0.1280865600777864
iteration : 7988
train acc:  0.796875
train loss:  0.47893404960632324
train gradient:  0.11025811072785106
iteration : 7989
train acc:  0.765625
train loss:  0.5256136059761047
train gradient:  0.12646708612413465
iteration : 7990
train acc:  0.75
train loss:  0.44094353914260864
train gradient:  0.10690937266816286
iteration : 7991
train acc:  0.75
train loss:  0.4835357069969177
train gradient:  0.11193192730549083
iteration : 7992
train acc:  0.7109375
train loss:  0.5015385150909424
train gradient:  0.12222612611544065
iteration : 7993
train acc:  0.703125
train loss:  0.48763492703437805
train gradient:  0.09727830572307467
iteration : 7994
train acc:  0.7265625
train loss:  0.5411620140075684
train gradient:  0.1494078877384809
iteration : 7995
train acc:  0.7734375
train loss:  0.46845728158950806
train gradient:  0.08720061841304437
iteration : 7996
train acc:  0.7421875
train loss:  0.5205203294754028
train gradient:  0.16150314530926624
iteration : 7997
train acc:  0.7109375
train loss:  0.49043023586273193
train gradient:  0.11256430880870984
iteration : 7998
train acc:  0.78125
train loss:  0.5000751614570618
train gradient:  0.1081207580499668
iteration : 7999
train acc:  0.75
train loss:  0.4766961932182312
train gradient:  0.11101868723717499
iteration : 8000
train acc:  0.7265625
train loss:  0.5646888017654419
train gradient:  0.1684091556485099
iteration : 8001
train acc:  0.7421875
train loss:  0.48493289947509766
train gradient:  0.12884160642960008
iteration : 8002
train acc:  0.7421875
train loss:  0.5016661882400513
train gradient:  0.12669907295665578
iteration : 8003
train acc:  0.71875
train loss:  0.5101168155670166
train gradient:  0.16384976027199252
iteration : 8004
train acc:  0.7265625
train loss:  0.5150891542434692
train gradient:  0.12686382559778353
iteration : 8005
train acc:  0.71875
train loss:  0.528466522693634
train gradient:  0.18746134959110666
iteration : 8006
train acc:  0.7109375
train loss:  0.4811567962169647
train gradient:  0.11805452952761741
iteration : 8007
train acc:  0.7734375
train loss:  0.47624391317367554
train gradient:  0.12935031819829834
iteration : 8008
train acc:  0.7890625
train loss:  0.48466944694519043
train gradient:  0.15130067283649412
iteration : 8009
train acc:  0.734375
train loss:  0.49398842453956604
train gradient:  0.12268021374450253
iteration : 8010
train acc:  0.6796875
train loss:  0.5140031576156616
train gradient:  0.21040152626470354
iteration : 8011
train acc:  0.765625
train loss:  0.47186732292175293
train gradient:  0.1734711857263076
iteration : 8012
train acc:  0.6640625
train loss:  0.5928604006767273
train gradient:  0.22662474975314414
iteration : 8013
train acc:  0.7265625
train loss:  0.4820549488067627
train gradient:  0.13052924509873826
iteration : 8014
train acc:  0.703125
train loss:  0.4926809072494507
train gradient:  0.11014214360889335
iteration : 8015
train acc:  0.78125
train loss:  0.4555768370628357
train gradient:  0.10003949642342738
iteration : 8016
train acc:  0.765625
train loss:  0.4875408411026001
train gradient:  0.11455217461707867
iteration : 8017
train acc:  0.75
train loss:  0.563136637210846
train gradient:  0.18829638265809584
iteration : 8018
train acc:  0.734375
train loss:  0.5076932907104492
train gradient:  0.17820293733127557
iteration : 8019
train acc:  0.7890625
train loss:  0.5143184661865234
train gradient:  0.127000674179713
iteration : 8020
train acc:  0.6953125
train loss:  0.5209296941757202
train gradient:  0.15801887627527372
iteration : 8021
train acc:  0.7734375
train loss:  0.5172159671783447
train gradient:  0.1537161075208882
iteration : 8022
train acc:  0.765625
train loss:  0.5017127394676208
train gradient:  0.11848881651678073
iteration : 8023
train acc:  0.7890625
train loss:  0.46836057305336
train gradient:  0.0941827353626977
iteration : 8024
train acc:  0.765625
train loss:  0.4894196391105652
train gradient:  0.11186748571072462
iteration : 8025
train acc:  0.78125
train loss:  0.4568096697330475
train gradient:  0.12795152338484864
iteration : 8026
train acc:  0.7890625
train loss:  0.43714639544487
train gradient:  0.1008953898567716
iteration : 8027
train acc:  0.71875
train loss:  0.5323895812034607
train gradient:  0.12574862039203172
iteration : 8028
train acc:  0.7265625
train loss:  0.5114781856536865
train gradient:  0.1168214474200667
iteration : 8029
train acc:  0.765625
train loss:  0.47856372594833374
train gradient:  0.12234988373084274
iteration : 8030
train acc:  0.671875
train loss:  0.5518689751625061
train gradient:  0.15517750668195662
iteration : 8031
train acc:  0.7578125
train loss:  0.4566313624382019
train gradient:  0.11438937585000034
iteration : 8032
train acc:  0.765625
train loss:  0.5163409113883972
train gradient:  0.14254072709471394
iteration : 8033
train acc:  0.8125
train loss:  0.4356209635734558
train gradient:  0.10502234668582403
iteration : 8034
train acc:  0.71875
train loss:  0.5332438945770264
train gradient:  0.14275528981339963
iteration : 8035
train acc:  0.7265625
train loss:  0.503728449344635
train gradient:  0.11709212559182004
iteration : 8036
train acc:  0.7265625
train loss:  0.5022287964820862
train gradient:  0.17050344232020487
iteration : 8037
train acc:  0.8203125
train loss:  0.4091341495513916
train gradient:  0.08611032403213394
iteration : 8038
train acc:  0.78125
train loss:  0.45995965600013733
train gradient:  0.14271031555280234
iteration : 8039
train acc:  0.7265625
train loss:  0.520925760269165
train gradient:  0.1305775142781352
iteration : 8040
train acc:  0.7265625
train loss:  0.496503621339798
train gradient:  0.11207102073195753
iteration : 8041
train acc:  0.765625
train loss:  0.4610370397567749
train gradient:  0.08918395256719049
iteration : 8042
train acc:  0.7578125
train loss:  0.5238164663314819
train gradient:  0.1359494170614949
iteration : 8043
train acc:  0.7109375
train loss:  0.5453327298164368
train gradient:  0.14348830935361007
iteration : 8044
train acc:  0.75
train loss:  0.48155131936073303
train gradient:  0.12942178240337904
iteration : 8045
train acc:  0.734375
train loss:  0.46181851625442505
train gradient:  0.12905909833089355
iteration : 8046
train acc:  0.75
train loss:  0.5007167458534241
train gradient:  0.1514862358889641
iteration : 8047
train acc:  0.734375
train loss:  0.5714179277420044
train gradient:  0.1741849828714116
iteration : 8048
train acc:  0.7109375
train loss:  0.5030783414840698
train gradient:  0.1300431862347599
iteration : 8049
train acc:  0.8359375
train loss:  0.3786484897136688
train gradient:  0.11050245213777796
iteration : 8050
train acc:  0.796875
train loss:  0.4885684847831726
train gradient:  0.12178661629805777
iteration : 8051
train acc:  0.7265625
train loss:  0.5239216089248657
train gradient:  0.12448864784732411
iteration : 8052
train acc:  0.6953125
train loss:  0.5697938203811646
train gradient:  0.18618394556799694
iteration : 8053
train acc:  0.7109375
train loss:  0.5289335250854492
train gradient:  0.15690938341218486
iteration : 8054
train acc:  0.765625
train loss:  0.4703320264816284
train gradient:  0.10931195063671313
iteration : 8055
train acc:  0.71875
train loss:  0.5421404242515564
train gradient:  0.1416258846102941
iteration : 8056
train acc:  0.7265625
train loss:  0.5576884746551514
train gradient:  0.14863018870157726
iteration : 8057
train acc:  0.6875
train loss:  0.542091965675354
train gradient:  0.14212050602980847
iteration : 8058
train acc:  0.7109375
train loss:  0.49220675230026245
train gradient:  0.11928056687108984
iteration : 8059
train acc:  0.7734375
train loss:  0.46000340580940247
train gradient:  0.109395853369534
iteration : 8060
train acc:  0.8125
train loss:  0.4377930462360382
train gradient:  0.11851986430474898
iteration : 8061
train acc:  0.78125
train loss:  0.42812323570251465
train gradient:  0.12530667384263605
iteration : 8062
train acc:  0.7734375
train loss:  0.5113590955734253
train gradient:  0.15237198293205367
iteration : 8063
train acc:  0.7734375
train loss:  0.46741440892219543
train gradient:  0.12431509811571546
iteration : 8064
train acc:  0.71875
train loss:  0.49661606550216675
train gradient:  0.1178010776163996
iteration : 8065
train acc:  0.7734375
train loss:  0.4889103174209595
train gradient:  0.1132493945948179
iteration : 8066
train acc:  0.765625
train loss:  0.4968247413635254
train gradient:  0.13874112902883362
iteration : 8067
train acc:  0.8515625
train loss:  0.3871694803237915
train gradient:  0.1214534385424941
iteration : 8068
train acc:  0.796875
train loss:  0.4552108645439148
train gradient:  0.11910683354152737
iteration : 8069
train acc:  0.7421875
train loss:  0.5110375881195068
train gradient:  0.14517388560116196
iteration : 8070
train acc:  0.7734375
train loss:  0.49648576974868774
train gradient:  0.15725297422886259
iteration : 8071
train acc:  0.703125
train loss:  0.5671156644821167
train gradient:  0.15586184581067591
iteration : 8072
train acc:  0.7890625
train loss:  0.4829440116882324
train gradient:  0.12167309236715423
iteration : 8073
train acc:  0.75
train loss:  0.5066349506378174
train gradient:  0.12071930563637487
iteration : 8074
train acc:  0.734375
train loss:  0.4766432046890259
train gradient:  0.1123720084761684
iteration : 8075
train acc:  0.734375
train loss:  0.4963882267475128
train gradient:  0.1322782127576913
iteration : 8076
train acc:  0.7265625
train loss:  0.585084855556488
train gradient:  0.17877363401958232
iteration : 8077
train acc:  0.7109375
train loss:  0.5294176340103149
train gradient:  0.1479507735476932
iteration : 8078
train acc:  0.703125
train loss:  0.5486443042755127
train gradient:  0.15132043590928712
iteration : 8079
train acc:  0.765625
train loss:  0.5217966437339783
train gradient:  0.16569711252119174
iteration : 8080
train acc:  0.7890625
train loss:  0.4387982487678528
train gradient:  0.10112224518566418
iteration : 8081
train acc:  0.765625
train loss:  0.5834295749664307
train gradient:  0.14643705164702348
iteration : 8082
train acc:  0.7109375
train loss:  0.534395694732666
train gradient:  0.19868608544910427
iteration : 8083
train acc:  0.7265625
train loss:  0.46438419818878174
train gradient:  0.1022201385251967
iteration : 8084
train acc:  0.7890625
train loss:  0.4711877703666687
train gradient:  0.12748817522430805
iteration : 8085
train acc:  0.703125
train loss:  0.5233147740364075
train gradient:  0.1411034097748364
iteration : 8086
train acc:  0.7578125
train loss:  0.5378267765045166
train gradient:  0.15529175748490257
iteration : 8087
train acc:  0.7421875
train loss:  0.5098217725753784
train gradient:  0.14039159935389173
iteration : 8088
train acc:  0.75
train loss:  0.4631322920322418
train gradient:  0.11775763393750698
iteration : 8089
train acc:  0.7265625
train loss:  0.5400872230529785
train gradient:  0.1739589107152298
iteration : 8090
train acc:  0.7578125
train loss:  0.4748271703720093
train gradient:  0.11406737613974609
iteration : 8091
train acc:  0.7109375
train loss:  0.5172849297523499
train gradient:  0.10684338675249645
iteration : 8092
train acc:  0.6796875
train loss:  0.5491257309913635
train gradient:  0.16663305432204428
iteration : 8093
train acc:  0.75
train loss:  0.4600842297077179
train gradient:  0.11291310663723081
iteration : 8094
train acc:  0.765625
train loss:  0.46922412514686584
train gradient:  0.1196292306755126
iteration : 8095
train acc:  0.71875
train loss:  0.5311987400054932
train gradient:  0.15726599750480436
iteration : 8096
train acc:  0.6875
train loss:  0.5717992782592773
train gradient:  0.15284203130324198
iteration : 8097
train acc:  0.6640625
train loss:  0.49025803804397583
train gradient:  0.14275725528859046
iteration : 8098
train acc:  0.71875
train loss:  0.5218784809112549
train gradient:  0.13514205217411662
iteration : 8099
train acc:  0.8125
train loss:  0.4163626432418823
train gradient:  0.10465535957696002
iteration : 8100
train acc:  0.6953125
train loss:  0.5078693628311157
train gradient:  0.1496060876982338
iteration : 8101
train acc:  0.7578125
train loss:  0.4431766867637634
train gradient:  0.11122876932609722
iteration : 8102
train acc:  0.78125
train loss:  0.4901556968688965
train gradient:  0.14507544401782474
iteration : 8103
train acc:  0.75
train loss:  0.5262542366981506
train gradient:  0.1465878947079496
iteration : 8104
train acc:  0.703125
train loss:  0.5416145920753479
train gradient:  0.14435289985854188
iteration : 8105
train acc:  0.7421875
train loss:  0.5023132562637329
train gradient:  0.11953905442635139
iteration : 8106
train acc:  0.75
train loss:  0.4760779142379761
train gradient:  0.10656937065977902
iteration : 8107
train acc:  0.734375
train loss:  0.4778290390968323
train gradient:  0.12875176528264087
iteration : 8108
train acc:  0.796875
train loss:  0.43719613552093506
train gradient:  0.1293580191685235
iteration : 8109
train acc:  0.75
train loss:  0.4883803725242615
train gradient:  0.15422964460583202
iteration : 8110
train acc:  0.703125
train loss:  0.5148031711578369
train gradient:  0.13221553577086637
iteration : 8111
train acc:  0.7578125
train loss:  0.4365141689777374
train gradient:  0.11819715798199124
iteration : 8112
train acc:  0.7265625
train loss:  0.5300378799438477
train gradient:  0.15216655778567528
iteration : 8113
train acc:  0.7734375
train loss:  0.45550984144210815
train gradient:  0.15422154740287963
iteration : 8114
train acc:  0.7109375
train loss:  0.5161401033401489
train gradient:  0.14221084396501482
iteration : 8115
train acc:  0.796875
train loss:  0.41022682189941406
train gradient:  0.10178343252600813
iteration : 8116
train acc:  0.7265625
train loss:  0.48741328716278076
train gradient:  0.1445505236450704
iteration : 8117
train acc:  0.7890625
train loss:  0.4763985574245453
train gradient:  0.1391990385472628
iteration : 8118
train acc:  0.8046875
train loss:  0.4717901945114136
train gradient:  0.13284244090922542
iteration : 8119
train acc:  0.6875
train loss:  0.5605694055557251
train gradient:  0.1589038817808548
iteration : 8120
train acc:  0.6953125
train loss:  0.5685377717018127
train gradient:  0.16574039507363736
iteration : 8121
train acc:  0.7421875
train loss:  0.509778618812561
train gradient:  0.15088217302746745
iteration : 8122
train acc:  0.71875
train loss:  0.5278382301330566
train gradient:  0.1339960928408487
iteration : 8123
train acc:  0.7421875
train loss:  0.5440824627876282
train gradient:  0.15083740536261603
iteration : 8124
train acc:  0.6875
train loss:  0.5791425704956055
train gradient:  0.16889552514986433
iteration : 8125
train acc:  0.7734375
train loss:  0.44959282875061035
train gradient:  0.12619448587334098
iteration : 8126
train acc:  0.78125
train loss:  0.44404149055480957
train gradient:  0.10979825663553744
iteration : 8127
train acc:  0.7109375
train loss:  0.5434958934783936
train gradient:  0.17306825517503183
iteration : 8128
train acc:  0.7421875
train loss:  0.531205415725708
train gradient:  0.13848193229635503
iteration : 8129
train acc:  0.7734375
train loss:  0.4274224042892456
train gradient:  0.11054971395643799
iteration : 8130
train acc:  0.75
train loss:  0.47263628244400024
train gradient:  0.14124578565158197
iteration : 8131
train acc:  0.7265625
train loss:  0.4931710958480835
train gradient:  0.14529146910151225
iteration : 8132
train acc:  0.7421875
train loss:  0.565926730632782
train gradient:  0.16094649140824255
iteration : 8133
train acc:  0.75
train loss:  0.4838101267814636
train gradient:  0.1270382473653059
iteration : 8134
train acc:  0.75
train loss:  0.482832133769989
train gradient:  0.12367212462989262
iteration : 8135
train acc:  0.6875
train loss:  0.5084352493286133
train gradient:  0.173839127328567
iteration : 8136
train acc:  0.796875
train loss:  0.44753921031951904
train gradient:  0.12701957353434462
iteration : 8137
train acc:  0.8203125
train loss:  0.4467686712741852
train gradient:  0.12677357789348892
iteration : 8138
train acc:  0.7109375
train loss:  0.5153646469116211
train gradient:  0.12737812007109223
iteration : 8139
train acc:  0.7578125
train loss:  0.47569239139556885
train gradient:  0.10987605443904629
iteration : 8140
train acc:  0.7265625
train loss:  0.4833514094352722
train gradient:  0.10618559132317411
iteration : 8141
train acc:  0.7421875
train loss:  0.5318698287010193
train gradient:  0.1584031027536026
iteration : 8142
train acc:  0.71875
train loss:  0.5442634224891663
train gradient:  0.15737110601922444
iteration : 8143
train acc:  0.734375
train loss:  0.5883594751358032
train gradient:  0.20135024539312432
iteration : 8144
train acc:  0.7109375
train loss:  0.5356554388999939
train gradient:  0.12764667922045364
iteration : 8145
train acc:  0.734375
train loss:  0.5548431873321533
train gradient:  0.14804566748956394
iteration : 8146
train acc:  0.75
train loss:  0.46192875504493713
train gradient:  0.12374457120819048
iteration : 8147
train acc:  0.6796875
train loss:  0.5597439408302307
train gradient:  0.13557364714019549
iteration : 8148
train acc:  0.7578125
train loss:  0.5018450617790222
train gradient:  0.14265559660019764
iteration : 8149
train acc:  0.6796875
train loss:  0.5721144080162048
train gradient:  0.18140988725917367
iteration : 8150
train acc:  0.7734375
train loss:  0.4441147744655609
train gradient:  0.10674652225499746
iteration : 8151
train acc:  0.734375
train loss:  0.47961390018463135
train gradient:  0.12968491902679374
iteration : 8152
train acc:  0.7578125
train loss:  0.5118619203567505
train gradient:  0.1216757088140954
iteration : 8153
train acc:  0.7265625
train loss:  0.4944089651107788
train gradient:  0.1325329632990433
iteration : 8154
train acc:  0.7578125
train loss:  0.4658007323741913
train gradient:  0.16513963332586945
iteration : 8155
train acc:  0.6953125
train loss:  0.5427857041358948
train gradient:  0.12858415411463955
iteration : 8156
train acc:  0.734375
train loss:  0.46801072359085083
train gradient:  0.10272926633812157
iteration : 8157
train acc:  0.7578125
train loss:  0.46474653482437134
train gradient:  0.09739911546239235
iteration : 8158
train acc:  0.71875
train loss:  0.5157619714736938
train gradient:  0.12988350045675495
iteration : 8159
train acc:  0.7265625
train loss:  0.5147576332092285
train gradient:  0.12717033636942138
iteration : 8160
train acc:  0.71875
train loss:  0.5077373385429382
train gradient:  0.1547956877288459
iteration : 8161
train acc:  0.6875
train loss:  0.5784978270530701
train gradient:  0.1643764723896441
iteration : 8162
train acc:  0.7421875
train loss:  0.48548436164855957
train gradient:  0.15307666709109308
iteration : 8163
train acc:  0.6875
train loss:  0.5257453918457031
train gradient:  0.14059855950031025
iteration : 8164
train acc:  0.734375
train loss:  0.5163925886154175
train gradient:  0.12165507405044473
iteration : 8165
train acc:  0.6875
train loss:  0.5689085721969604
train gradient:  0.14429317001109188
iteration : 8166
train acc:  0.7421875
train loss:  0.5020474195480347
train gradient:  0.12631091940699038
iteration : 8167
train acc:  0.703125
train loss:  0.5509713888168335
train gradient:  0.13936666893865068
iteration : 8168
train acc:  0.6953125
train loss:  0.5716301202774048
train gradient:  0.13614189392647733
iteration : 8169
train acc:  0.75
train loss:  0.48067426681518555
train gradient:  0.1282393694123132
iteration : 8170
train acc:  0.7265625
train loss:  0.5140236616134644
train gradient:  0.16800409103886726
iteration : 8171
train acc:  0.7109375
train loss:  0.553093433380127
train gradient:  0.16738696429307826
iteration : 8172
train acc:  0.71875
train loss:  0.5539658069610596
train gradient:  0.1778484949321431
iteration : 8173
train acc:  0.765625
train loss:  0.48401063680648804
train gradient:  0.12392192276708974
iteration : 8174
train acc:  0.7421875
train loss:  0.5210243463516235
train gradient:  0.15484606119217015
iteration : 8175
train acc:  0.8046875
train loss:  0.4297840893268585
train gradient:  0.08358275518484866
iteration : 8176
train acc:  0.6953125
train loss:  0.49164879322052
train gradient:  0.10140316359242502
iteration : 8177
train acc:  0.71875
train loss:  0.5262268781661987
train gradient:  0.1456128220952295
iteration : 8178
train acc:  0.7734375
train loss:  0.508216142654419
train gradient:  0.12463847847762502
iteration : 8179
train acc:  0.65625
train loss:  0.5250641107559204
train gradient:  0.16677487573450556
iteration : 8180
train acc:  0.7890625
train loss:  0.4788203835487366
train gradient:  0.10712612014038567
iteration : 8181
train acc:  0.6875
train loss:  0.4926359951496124
train gradient:  0.14066687612797385
iteration : 8182
train acc:  0.7890625
train loss:  0.4596881866455078
train gradient:  0.1196191480247922
iteration : 8183
train acc:  0.71875
train loss:  0.5067906379699707
train gradient:  0.13809064330936466
iteration : 8184
train acc:  0.7890625
train loss:  0.4429667592048645
train gradient:  0.09361284174191202
iteration : 8185
train acc:  0.8046875
train loss:  0.4650222659111023
train gradient:  0.1027772708790026
iteration : 8186
train acc:  0.7265625
train loss:  0.4924461841583252
train gradient:  0.14240379998002484
iteration : 8187
train acc:  0.6875
train loss:  0.5306789875030518
train gradient:  0.13350941211732248
iteration : 8188
train acc:  0.765625
train loss:  0.4378737807273865
train gradient:  0.09301465522155525
iteration : 8189
train acc:  0.703125
train loss:  0.5449008941650391
train gradient:  0.15250359592513468
iteration : 8190
train acc:  0.7578125
train loss:  0.484266459941864
train gradient:  0.11527997005994736
iteration : 8191
train acc:  0.75
train loss:  0.4937533736228943
train gradient:  0.12791459985600917
iteration : 8192
train acc:  0.7578125
train loss:  0.4486788213253021
train gradient:  0.10063921601346615
iteration : 8193
train acc:  0.6953125
train loss:  0.534119188785553
train gradient:  0.16724548930084232
iteration : 8194
train acc:  0.765625
train loss:  0.4739629030227661
train gradient:  0.1134125709949505
iteration : 8195
train acc:  0.7421875
train loss:  0.5211315751075745
train gradient:  0.138718193082166
iteration : 8196
train acc:  0.765625
train loss:  0.49975958466529846
train gradient:  0.12164030720464972
iteration : 8197
train acc:  0.71875
train loss:  0.5370116233825684
train gradient:  0.144077819196777
iteration : 8198
train acc:  0.78125
train loss:  0.4551035761833191
train gradient:  0.10987200929098302
iteration : 8199
train acc:  0.734375
train loss:  0.5003035068511963
train gradient:  0.13026064729894635
iteration : 8200
train acc:  0.8125
train loss:  0.4782705307006836
train gradient:  0.1250728976735019
iteration : 8201
train acc:  0.7265625
train loss:  0.5094040632247925
train gradient:  0.11300106385209623
iteration : 8202
train acc:  0.7890625
train loss:  0.4417157471179962
train gradient:  0.08943789345873243
iteration : 8203
train acc:  0.7578125
train loss:  0.45125794410705566
train gradient:  0.10763346264097501
iteration : 8204
train acc:  0.8203125
train loss:  0.4789031744003296
train gradient:  0.17929233229570107
iteration : 8205
train acc:  0.734375
train loss:  0.4964342415332794
train gradient:  0.1144370812943197
iteration : 8206
train acc:  0.7265625
train loss:  0.5415059924125671
train gradient:  0.15297154865701262
iteration : 8207
train acc:  0.7265625
train loss:  0.4901253879070282
train gradient:  0.11685948873881351
iteration : 8208
train acc:  0.7734375
train loss:  0.4605293869972229
train gradient:  0.14006379729302462
iteration : 8209
train acc:  0.8046875
train loss:  0.4696410298347473
train gradient:  0.11344206176029355
iteration : 8210
train acc:  0.7109375
train loss:  0.5446908473968506
train gradient:  0.1339154834575051
iteration : 8211
train acc:  0.7734375
train loss:  0.5414783358573914
train gradient:  0.1444105439639576
iteration : 8212
train acc:  0.7265625
train loss:  0.47277793288230896
train gradient:  0.14140467086620048
iteration : 8213
train acc:  0.6640625
train loss:  0.5529729127883911
train gradient:  0.16988075738646527
iteration : 8214
train acc:  0.828125
train loss:  0.44225746393203735
train gradient:  0.11967750125045712
iteration : 8215
train acc:  0.71875
train loss:  0.46317780017852783
train gradient:  0.09164340280092748
iteration : 8216
train acc:  0.7265625
train loss:  0.5016007423400879
train gradient:  0.11270938762339956
iteration : 8217
train acc:  0.7265625
train loss:  0.5147175788879395
train gradient:  0.13571063817444917
iteration : 8218
train acc:  0.7734375
train loss:  0.4605828821659088
train gradient:  0.11979880417228736
iteration : 8219
train acc:  0.703125
train loss:  0.5316029787063599
train gradient:  0.1644858757460994
iteration : 8220
train acc:  0.75
train loss:  0.5268272161483765
train gradient:  0.15244471877758753
iteration : 8221
train acc:  0.703125
train loss:  0.5509034395217896
train gradient:  0.13490768568957723
iteration : 8222
train acc:  0.75
train loss:  0.51607745885849
train gradient:  0.13745516080750242
iteration : 8223
train acc:  0.6953125
train loss:  0.56119704246521
train gradient:  0.19197758412373994
iteration : 8224
train acc:  0.703125
train loss:  0.5437041521072388
train gradient:  0.16732880195617697
iteration : 8225
train acc:  0.71875
train loss:  0.5839519500732422
train gradient:  0.1654979832135356
iteration : 8226
train acc:  0.671875
train loss:  0.5631518363952637
train gradient:  0.14085436668183565
iteration : 8227
train acc:  0.7890625
train loss:  0.4410507082939148
train gradient:  0.10168580890605945
iteration : 8228
train acc:  0.828125
train loss:  0.40787917375564575
train gradient:  0.09729671401387507
iteration : 8229
train acc:  0.7578125
train loss:  0.45432084798812866
train gradient:  0.11225652074448195
iteration : 8230
train acc:  0.703125
train loss:  0.5331846475601196
train gradient:  0.14495586000854033
iteration : 8231
train acc:  0.78125
train loss:  0.5004564523696899
train gradient:  0.10674646957178185
iteration : 8232
train acc:  0.6953125
train loss:  0.5093676447868347
train gradient:  0.1221791095706746
iteration : 8233
train acc:  0.71875
train loss:  0.5208973288536072
train gradient:  0.11974199894894548
iteration : 8234
train acc:  0.796875
train loss:  0.4201539158821106
train gradient:  0.0802263454472229
iteration : 8235
train acc:  0.671875
train loss:  0.51995849609375
train gradient:  0.11663858886728692
iteration : 8236
train acc:  0.7578125
train loss:  0.4351365268230438
train gradient:  0.11584735493579437
iteration : 8237
train acc:  0.75
train loss:  0.48787665367126465
train gradient:  0.13111094423196945
iteration : 8238
train acc:  0.6953125
train loss:  0.5166059136390686
train gradient:  0.1417581801168482
iteration : 8239
train acc:  0.7421875
train loss:  0.5021070837974548
train gradient:  0.13076468303595426
iteration : 8240
train acc:  0.7265625
train loss:  0.5105456709861755
train gradient:  0.14725998156652723
iteration : 8241
train acc:  0.75
train loss:  0.49358513951301575
train gradient:  0.13431655107104973
iteration : 8242
train acc:  0.7578125
train loss:  0.4543595314025879
train gradient:  0.10054683129764297
iteration : 8243
train acc:  0.7734375
train loss:  0.4336618185043335
train gradient:  0.08436641091606849
iteration : 8244
train acc:  0.765625
train loss:  0.5041815042495728
train gradient:  0.1584988725959138
iteration : 8245
train acc:  0.71875
train loss:  0.5334481000900269
train gradient:  0.15037718725436422
iteration : 8246
train acc:  0.7265625
train loss:  0.47515368461608887
train gradient:  0.14197669122906048
iteration : 8247
train acc:  0.7578125
train loss:  0.4658845365047455
train gradient:  0.11585720788600956
iteration : 8248
train acc:  0.6796875
train loss:  0.5365695953369141
train gradient:  0.1532748206604051
iteration : 8249
train acc:  0.7578125
train loss:  0.5088197588920593
train gradient:  0.1438137898820464
iteration : 8250
train acc:  0.6875
train loss:  0.5665077567100525
train gradient:  0.1659364089526691
iteration : 8251
train acc:  0.7578125
train loss:  0.4930567443370819
train gradient:  0.11688079156481909
iteration : 8252
train acc:  0.7265625
train loss:  0.5201314687728882
train gradient:  0.1266065954795247
iteration : 8253
train acc:  0.703125
train loss:  0.5245398879051208
train gradient:  0.15384896853292604
iteration : 8254
train acc:  0.78125
train loss:  0.4868263006210327
train gradient:  0.10977724979920697
iteration : 8255
train acc:  0.7734375
train loss:  0.4910167455673218
train gradient:  0.1393891981378127
iteration : 8256
train acc:  0.8046875
train loss:  0.4671291410923004
train gradient:  0.1299186288966157
iteration : 8257
train acc:  0.78125
train loss:  0.487131267786026
train gradient:  0.15346972979936324
iteration : 8258
train acc:  0.765625
train loss:  0.47570013999938965
train gradient:  0.13962560687917988
iteration : 8259
train acc:  0.7109375
train loss:  0.4950714111328125
train gradient:  0.10437608076885968
iteration : 8260
train acc:  0.78125
train loss:  0.4521282911300659
train gradient:  0.10655747442894037
iteration : 8261
train acc:  0.734375
train loss:  0.5425008535385132
train gradient:  0.16247810154735148
iteration : 8262
train acc:  0.765625
train loss:  0.463561087846756
train gradient:  0.12104403116740578
iteration : 8263
train acc:  0.7734375
train loss:  0.4224787950515747
train gradient:  0.08299691018571133
iteration : 8264
train acc:  0.7109375
train loss:  0.5473685264587402
train gradient:  0.1539536471818498
iteration : 8265
train acc:  0.7734375
train loss:  0.4304198920726776
train gradient:  0.10686903143912818
iteration : 8266
train acc:  0.7421875
train loss:  0.522438645362854
train gradient:  0.15140335660900545
iteration : 8267
train acc:  0.78125
train loss:  0.4706946909427643
train gradient:  0.14655145064968686
iteration : 8268
train acc:  0.75
train loss:  0.4762604832649231
train gradient:  0.13327217910480776
iteration : 8269
train acc:  0.765625
train loss:  0.4380852282047272
train gradient:  0.10623170769425457
iteration : 8270
train acc:  0.6796875
train loss:  0.6220322847366333
train gradient:  0.21448030434984217
iteration : 8271
train acc:  0.75
train loss:  0.48784199357032776
train gradient:  0.1127870497610434
iteration : 8272
train acc:  0.734375
train loss:  0.4843193292617798
train gradient:  0.10231426200742715
iteration : 8273
train acc:  0.765625
train loss:  0.4946366548538208
train gradient:  0.14507137915212814
iteration : 8274
train acc:  0.71875
train loss:  0.5451108813285828
train gradient:  0.1580832228529159
iteration : 8275
train acc:  0.796875
train loss:  0.47122782468795776
train gradient:  0.1345575545306934
iteration : 8276
train acc:  0.75
train loss:  0.4763186573982239
train gradient:  0.14388758510783417
iteration : 8277
train acc:  0.734375
train loss:  0.5075573325157166
train gradient:  0.11316765245378956
iteration : 8278
train acc:  0.734375
train loss:  0.48977208137512207
train gradient:  0.1110882319740416
iteration : 8279
train acc:  0.65625
train loss:  0.5553206205368042
train gradient:  0.1743938032250963
iteration : 8280
train acc:  0.75
train loss:  0.5062257647514343
train gradient:  0.14765728253741045
iteration : 8281
train acc:  0.6640625
train loss:  0.5793644785881042
train gradient:  0.15757726069682026
iteration : 8282
train acc:  0.734375
train loss:  0.499202698469162
train gradient:  0.1360525761418994
iteration : 8283
train acc:  0.734375
train loss:  0.48844975233078003
train gradient:  0.10979401122690977
iteration : 8284
train acc:  0.765625
train loss:  0.47783711552619934
train gradient:  0.11778160100312128
iteration : 8285
train acc:  0.75
train loss:  0.47405749559402466
train gradient:  0.10474691306240723
iteration : 8286
train acc:  0.7109375
train loss:  0.5368885397911072
train gradient:  0.16370497922998223
iteration : 8287
train acc:  0.8125
train loss:  0.43581387400627136
train gradient:  0.09659675780888907
iteration : 8288
train acc:  0.7421875
train loss:  0.5270880460739136
train gradient:  0.16325124900873028
iteration : 8289
train acc:  0.6796875
train loss:  0.5421990156173706
train gradient:  0.12520657371398614
iteration : 8290
train acc:  0.78125
train loss:  0.45736241340637207
train gradient:  0.11679305710624723
iteration : 8291
train acc:  0.7421875
train loss:  0.5384107828140259
train gradient:  0.15080737595673466
iteration : 8292
train acc:  0.7265625
train loss:  0.5225608348846436
train gradient:  0.15222365389919856
iteration : 8293
train acc:  0.7578125
train loss:  0.4841613471508026
train gradient:  0.13068385069642752
iteration : 8294
train acc:  0.796875
train loss:  0.4686981737613678
train gradient:  0.10428912373489109
iteration : 8295
train acc:  0.75
train loss:  0.495545893907547
train gradient:  0.13797437864351075
iteration : 8296
train acc:  0.7578125
train loss:  0.46474403142929077
train gradient:  0.1298054989632098
iteration : 8297
train acc:  0.7265625
train loss:  0.4897855818271637
train gradient:  0.10685983022473448
iteration : 8298
train acc:  0.734375
train loss:  0.4849541485309601
train gradient:  0.13132504648398186
iteration : 8299
train acc:  0.671875
train loss:  0.4991868734359741
train gradient:  0.12294400414513106
iteration : 8300
train acc:  0.71875
train loss:  0.49502623081207275
train gradient:  0.10751989875294603
iteration : 8301
train acc:  0.7734375
train loss:  0.4706838130950928
train gradient:  0.14409340614048297
iteration : 8302
train acc:  0.765625
train loss:  0.48081067204475403
train gradient:  0.1488379540271068
iteration : 8303
train acc:  0.6796875
train loss:  0.573786735534668
train gradient:  0.16982624000887903
iteration : 8304
train acc:  0.75
train loss:  0.4905847907066345
train gradient:  0.21482945915144813
iteration : 8305
train acc:  0.78125
train loss:  0.45232754945755005
train gradient:  0.10064207326257242
iteration : 8306
train acc:  0.8125
train loss:  0.4171473979949951
train gradient:  0.08726722826355067
iteration : 8307
train acc:  0.7265625
train loss:  0.49746114015579224
train gradient:  0.12324608126105552
iteration : 8308
train acc:  0.7265625
train loss:  0.5395814180374146
train gradient:  0.13553377501761937
iteration : 8309
train acc:  0.7265625
train loss:  0.5665432214736938
train gradient:  0.1607674503087918
iteration : 8310
train acc:  0.7265625
train loss:  0.5199702382087708
train gradient:  0.12609324140196088
iteration : 8311
train acc:  0.7734375
train loss:  0.4617561399936676
train gradient:  0.09837340582466901
iteration : 8312
train acc:  0.6875
train loss:  0.5806751251220703
train gradient:  0.14308389410191738
iteration : 8313
train acc:  0.71875
train loss:  0.5124590992927551
train gradient:  0.13915149219713316
iteration : 8314
train acc:  0.75
train loss:  0.519020140171051
train gradient:  0.12946133782243843
iteration : 8315
train acc:  0.8125
train loss:  0.41556936502456665
train gradient:  0.12173466312866205
iteration : 8316
train acc:  0.7578125
train loss:  0.5520280003547668
train gradient:  0.20210908768300576
iteration : 8317
train acc:  0.78125
train loss:  0.46216416358947754
train gradient:  0.12094627718979375
iteration : 8318
train acc:  0.78125
train loss:  0.4624807834625244
train gradient:  0.1034706308686978
iteration : 8319
train acc:  0.765625
train loss:  0.44160225987434387
train gradient:  0.10028061562394318
iteration : 8320
train acc:  0.8046875
train loss:  0.4430220425128937
train gradient:  0.13066654209232237
iteration : 8321
train acc:  0.7421875
train loss:  0.5266871452331543
train gradient:  0.163394515246026
iteration : 8322
train acc:  0.7109375
train loss:  0.5091171860694885
train gradient:  0.1168776359916003
iteration : 8323
train acc:  0.6953125
train loss:  0.5249627828598022
train gradient:  0.16610157266057723
iteration : 8324
train acc:  0.7734375
train loss:  0.45565298199653625
train gradient:  0.12705768444745408
iteration : 8325
train acc:  0.734375
train loss:  0.4601086974143982
train gradient:  0.10524328297201414
iteration : 8326
train acc:  0.734375
train loss:  0.4970739483833313
train gradient:  0.10925988806168029
iteration : 8327
train acc:  0.765625
train loss:  0.4586902856826782
train gradient:  0.12645579675770247
iteration : 8328
train acc:  0.7265625
train loss:  0.551926851272583
train gradient:  0.1523888361063243
iteration : 8329
train acc:  0.7265625
train loss:  0.5271826386451721
train gradient:  0.15275871508299765
iteration : 8330
train acc:  0.7109375
train loss:  0.4762437343597412
train gradient:  0.11290318802809142
iteration : 8331
train acc:  0.796875
train loss:  0.44768059253692627
train gradient:  0.13190204299763292
iteration : 8332
train acc:  0.7421875
train loss:  0.48899030685424805
train gradient:  0.10492831477929453
iteration : 8333
train acc:  0.7109375
train loss:  0.5438405275344849
train gradient:  0.15736955846862394
iteration : 8334
train acc:  0.78125
train loss:  0.5028685331344604
train gradient:  0.1498883787194066
iteration : 8335
train acc:  0.7578125
train loss:  0.4589858949184418
train gradient:  0.12072029611812404
iteration : 8336
train acc:  0.71875
train loss:  0.5442371368408203
train gradient:  0.14362567754056005
iteration : 8337
train acc:  0.765625
train loss:  0.49862954020500183
train gradient:  0.1180399719091333
iteration : 8338
train acc:  0.7421875
train loss:  0.4587956666946411
train gradient:  0.13151422367471793
iteration : 8339
train acc:  0.734375
train loss:  0.5144232511520386
train gradient:  0.11335249127299706
iteration : 8340
train acc:  0.765625
train loss:  0.5254818201065063
train gradient:  0.14481047925165164
iteration : 8341
train acc:  0.7578125
train loss:  0.4747835397720337
train gradient:  0.0895243025635083
iteration : 8342
train acc:  0.671875
train loss:  0.5553510189056396
train gradient:  0.14254806862720645
iteration : 8343
train acc:  0.734375
train loss:  0.5190310478210449
train gradient:  0.11902759903536148
iteration : 8344
train acc:  0.734375
train loss:  0.5141251683235168
train gradient:  0.18271239490761204
iteration : 8345
train acc:  0.7265625
train loss:  0.5318416357040405
train gradient:  0.13406133671077533
iteration : 8346
train acc:  0.734375
train loss:  0.5319892168045044
train gradient:  0.19402047464212974
iteration : 8347
train acc:  0.7578125
train loss:  0.4708516597747803
train gradient:  0.10566176848205801
iteration : 8348
train acc:  0.78125
train loss:  0.48830646276474
train gradient:  0.11794439044140562
iteration : 8349
train acc:  0.7421875
train loss:  0.48905330896377563
train gradient:  0.12437814474996917
iteration : 8350
train acc:  0.703125
train loss:  0.4976237416267395
train gradient:  0.13108780126764277
iteration : 8351
train acc:  0.734375
train loss:  0.48748278617858887
train gradient:  0.13594913771606326
iteration : 8352
train acc:  0.7578125
train loss:  0.5015949010848999
train gradient:  0.10756980191690772
iteration : 8353
train acc:  0.71875
train loss:  0.502395510673523
train gradient:  0.13692796417528236
iteration : 8354
train acc:  0.7109375
train loss:  0.5682432651519775
train gradient:  0.12404796957451295
iteration : 8355
train acc:  0.7578125
train loss:  0.5016398429870605
train gradient:  0.13829442056120334
iteration : 8356
train acc:  0.71875
train loss:  0.5212663412094116
train gradient:  0.12626044612758391
iteration : 8357
train acc:  0.7890625
train loss:  0.44176435470581055
train gradient:  0.09812960484043838
iteration : 8358
train acc:  0.734375
train loss:  0.5106214284896851
train gradient:  0.17098199631957667
iteration : 8359
train acc:  0.7578125
train loss:  0.4840819835662842
train gradient:  0.10792659550645611
iteration : 8360
train acc:  0.7421875
train loss:  0.5431140065193176
train gradient:  0.13936090769577064
iteration : 8361
train acc:  0.7265625
train loss:  0.5358493328094482
train gradient:  0.138488337588137
iteration : 8362
train acc:  0.765625
train loss:  0.45377495884895325
train gradient:  0.0983830385034083
iteration : 8363
train acc:  0.7109375
train loss:  0.5452271103858948
train gradient:  0.15080010578191289
iteration : 8364
train acc:  0.6953125
train loss:  0.6029527187347412
train gradient:  0.2500948924720552
iteration : 8365
train acc:  0.7734375
train loss:  0.477729469537735
train gradient:  0.1313137608020455
iteration : 8366
train acc:  0.65625
train loss:  0.5521011352539062
train gradient:  0.16226385397430593
iteration : 8367
train acc:  0.6796875
train loss:  0.6184963583946228
train gradient:  0.20588054985151655
iteration : 8368
train acc:  0.7109375
train loss:  0.4798978865146637
train gradient:  0.10402841749220229
iteration : 8369
train acc:  0.734375
train loss:  0.5152909755706787
train gradient:  0.13168140101216047
iteration : 8370
train acc:  0.75
train loss:  0.47467076778411865
train gradient:  0.08116573797805066
iteration : 8371
train acc:  0.765625
train loss:  0.4977235794067383
train gradient:  0.1139300991301012
iteration : 8372
train acc:  0.7421875
train loss:  0.4952225685119629
train gradient:  0.14208978088200308
iteration : 8373
train acc:  0.7890625
train loss:  0.4610205292701721
train gradient:  0.09710428991698748
iteration : 8374
train acc:  0.765625
train loss:  0.4812568128108978
train gradient:  0.14599780771470733
iteration : 8375
train acc:  0.765625
train loss:  0.497251033782959
train gradient:  0.142910457310147
iteration : 8376
train acc:  0.796875
train loss:  0.4953330159187317
train gradient:  0.1269716240131764
iteration : 8377
train acc:  0.78125
train loss:  0.4947730302810669
train gradient:  0.1205771531003662
iteration : 8378
train acc:  0.78125
train loss:  0.4598570168018341
train gradient:  0.09293553382015604
iteration : 8379
train acc:  0.7421875
train loss:  0.5073949098587036
train gradient:  0.1251185233048475
iteration : 8380
train acc:  0.7265625
train loss:  0.5385175943374634
train gradient:  0.14407802864697278
iteration : 8381
train acc:  0.734375
train loss:  0.4934784173965454
train gradient:  0.1264315716297436
iteration : 8382
train acc:  0.6953125
train loss:  0.5363985300064087
train gradient:  0.13200665350925417
iteration : 8383
train acc:  0.78125
train loss:  0.4704214930534363
train gradient:  0.15249903343277693
iteration : 8384
train acc:  0.671875
train loss:  0.5795640349388123
train gradient:  0.15548487086939727
iteration : 8385
train acc:  0.6953125
train loss:  0.5478770136833191
train gradient:  0.15986157329747203
iteration : 8386
train acc:  0.7265625
train loss:  0.4591858386993408
train gradient:  0.1086889832862189
iteration : 8387
train acc:  0.6796875
train loss:  0.5932207107543945
train gradient:  0.16934568872975148
iteration : 8388
train acc:  0.734375
train loss:  0.5572378635406494
train gradient:  0.1422946576047866
iteration : 8389
train acc:  0.71875
train loss:  0.5322081446647644
train gradient:  0.15638584122093446
iteration : 8390
train acc:  0.703125
train loss:  0.5860038995742798
train gradient:  0.14628973981947435
iteration : 8391
train acc:  0.6875
train loss:  0.5282310247421265
train gradient:  0.15407047861505657
iteration : 8392
train acc:  0.7734375
train loss:  0.5044020414352417
train gradient:  0.11913066356629098
iteration : 8393
train acc:  0.71875
train loss:  0.5081183314323425
train gradient:  0.12407006398777222
iteration : 8394
train acc:  0.75
train loss:  0.5540228486061096
train gradient:  0.15662811322089876
iteration : 8395
train acc:  0.7734375
train loss:  0.479134738445282
train gradient:  0.13804722231605876
iteration : 8396
train acc:  0.7578125
train loss:  0.4874386191368103
train gradient:  0.10288563164334917
iteration : 8397
train acc:  0.7265625
train loss:  0.5114908218383789
train gradient:  0.12066358443644278
iteration : 8398
train acc:  0.7265625
train loss:  0.5056585073471069
train gradient:  0.1194376877311071
iteration : 8399
train acc:  0.71875
train loss:  0.5079715847969055
train gradient:  0.12410186500861202
iteration : 8400
train acc:  0.796875
train loss:  0.42543095350265503
train gradient:  0.10201610759614957
iteration : 8401
train acc:  0.7421875
train loss:  0.5279064178466797
train gradient:  0.10039577702696753
iteration : 8402
train acc:  0.71875
train loss:  0.4719802737236023
train gradient:  0.11757939732763174
iteration : 8403
train acc:  0.7265625
train loss:  0.4982706904411316
train gradient:  0.11575243353562956
iteration : 8404
train acc:  0.7265625
train loss:  0.48560646176338196
train gradient:  0.12456931733353317
iteration : 8405
train acc:  0.6953125
train loss:  0.5564149618148804
train gradient:  0.15169907084994072
iteration : 8406
train acc:  0.7578125
train loss:  0.47025203704833984
train gradient:  0.08269398432654489
iteration : 8407
train acc:  0.7265625
train loss:  0.462466835975647
train gradient:  0.10362267870504814
iteration : 8408
train acc:  0.7265625
train loss:  0.5567871332168579
train gradient:  0.18260844710182728
iteration : 8409
train acc:  0.71875
train loss:  0.5190681219100952
train gradient:  0.1378914190310368
iteration : 8410
train acc:  0.828125
train loss:  0.3910626471042633
train gradient:  0.07622747665923064
iteration : 8411
train acc:  0.734375
train loss:  0.5117340683937073
train gradient:  0.11652528750729997
iteration : 8412
train acc:  0.6953125
train loss:  0.5247722864151001
train gradient:  0.1403174952401124
iteration : 8413
train acc:  0.7265625
train loss:  0.4909152090549469
train gradient:  0.13345304832575633
iteration : 8414
train acc:  0.7734375
train loss:  0.4626060128211975
train gradient:  0.11777460938772458
iteration : 8415
train acc:  0.7265625
train loss:  0.4946351647377014
train gradient:  0.12323347093936096
iteration : 8416
train acc:  0.796875
train loss:  0.47145983576774597
train gradient:  0.11726750909887128
iteration : 8417
train acc:  0.7109375
train loss:  0.5547690391540527
train gradient:  0.1521555528286096
iteration : 8418
train acc:  0.7734375
train loss:  0.436553418636322
train gradient:  0.09088084568418499
iteration : 8419
train acc:  0.7578125
train loss:  0.4687044620513916
train gradient:  0.11383474212276036
iteration : 8420
train acc:  0.7109375
train loss:  0.48116832971572876
train gradient:  0.11539161614424313
iteration : 8421
train acc:  0.734375
train loss:  0.45101630687713623
train gradient:  0.10883933352145835
iteration : 8422
train acc:  0.796875
train loss:  0.4238099157810211
train gradient:  0.09362559044510758
iteration : 8423
train acc:  0.6875
train loss:  0.5152193307876587
train gradient:  0.1258255234330021
iteration : 8424
train acc:  0.671875
train loss:  0.5254095792770386
train gradient:  0.1464283517363232
iteration : 8425
train acc:  0.78125
train loss:  0.47151920199394226
train gradient:  0.10669640947374281
iteration : 8426
train acc:  0.7421875
train loss:  0.44721752405166626
train gradient:  0.10208174927085005
iteration : 8427
train acc:  0.765625
train loss:  0.4708069860935211
train gradient:  0.12439719926480519
iteration : 8428
train acc:  0.703125
train loss:  0.5038979053497314
train gradient:  0.1114090911796008
iteration : 8429
train acc:  0.7578125
train loss:  0.46402183175086975
train gradient:  0.1248982198169558
iteration : 8430
train acc:  0.7265625
train loss:  0.4929754436016083
train gradient:  0.11830815543336566
iteration : 8431
train acc:  0.734375
train loss:  0.5101742744445801
train gradient:  0.1135510300084578
iteration : 8432
train acc:  0.734375
train loss:  0.4860087037086487
train gradient:  0.11158088192660814
iteration : 8433
train acc:  0.6953125
train loss:  0.5226287245750427
train gradient:  0.11500618196155506
iteration : 8434
train acc:  0.703125
train loss:  0.5776793360710144
train gradient:  0.12829462974674438
iteration : 8435
train acc:  0.75
train loss:  0.45507895946502686
train gradient:  0.10829581152874329
iteration : 8436
train acc:  0.78125
train loss:  0.4686082601547241
train gradient:  0.15532842220889154
iteration : 8437
train acc:  0.7578125
train loss:  0.5005709528923035
train gradient:  0.16465847042463017
iteration : 8438
train acc:  0.75
train loss:  0.48602011799812317
train gradient:  0.11425082797247742
iteration : 8439
train acc:  0.7109375
train loss:  0.5012423992156982
train gradient:  0.1113067046030816
iteration : 8440
train acc:  0.7734375
train loss:  0.4646679759025574
train gradient:  0.10906713528193111
iteration : 8441
train acc:  0.7109375
train loss:  0.4904879629611969
train gradient:  0.13245555315169127
iteration : 8442
train acc:  0.7421875
train loss:  0.4698236286640167
train gradient:  0.11216583825107977
iteration : 8443
train acc:  0.7421875
train loss:  0.4701366424560547
train gradient:  0.12288712119876315
iteration : 8444
train acc:  0.6796875
train loss:  0.5346877574920654
train gradient:  0.14619227451296102
iteration : 8445
train acc:  0.796875
train loss:  0.46779099106788635
train gradient:  0.11801364047815867
iteration : 8446
train acc:  0.78125
train loss:  0.4613064229488373
train gradient:  0.1146995735039882
iteration : 8447
train acc:  0.7109375
train loss:  0.48593127727508545
train gradient:  0.13937841537186313
iteration : 8448
train acc:  0.75
train loss:  0.4721331000328064
train gradient:  0.10370769434352857
iteration : 8449
train acc:  0.671875
train loss:  0.6172125935554504
train gradient:  0.1928787105527311
iteration : 8450
train acc:  0.6953125
train loss:  0.5604560375213623
train gradient:  0.19220656252057317
iteration : 8451
train acc:  0.75
train loss:  0.48707160353660583
train gradient:  0.11568685861263445
iteration : 8452
train acc:  0.7421875
train loss:  0.48371613025665283
train gradient:  0.14666500203472096
iteration : 8453
train acc:  0.796875
train loss:  0.4168805778026581
train gradient:  0.10294238859329496
iteration : 8454
train acc:  0.734375
train loss:  0.506223201751709
train gradient:  0.1626792972470511
iteration : 8455
train acc:  0.7421875
train loss:  0.4702613353729248
train gradient:  0.09239587047919415
iteration : 8456
train acc:  0.765625
train loss:  0.40918445587158203
train gradient:  0.088535991104523
iteration : 8457
train acc:  0.7578125
train loss:  0.4676998555660248
train gradient:  0.13646745978505687
iteration : 8458
train acc:  0.8125
train loss:  0.4341779351234436
train gradient:  0.10062477120111585
iteration : 8459
train acc:  0.7265625
train loss:  0.5001987218856812
train gradient:  0.13315624644076185
iteration : 8460
train acc:  0.7109375
train loss:  0.4818616807460785
train gradient:  0.1226011680885205
iteration : 8461
train acc:  0.734375
train loss:  0.532331109046936
train gradient:  0.1270221307824142
iteration : 8462
train acc:  0.7265625
train loss:  0.444419264793396
train gradient:  0.1155142222858278
iteration : 8463
train acc:  0.765625
train loss:  0.4855538606643677
train gradient:  0.11586172611417907
iteration : 8464
train acc:  0.71875
train loss:  0.4665994644165039
train gradient:  0.10830990881762433
iteration : 8465
train acc:  0.7734375
train loss:  0.4808478355407715
train gradient:  0.1149802940712175
iteration : 8466
train acc:  0.7109375
train loss:  0.5353742241859436
train gradient:  0.1718723033820807
iteration : 8467
train acc:  0.734375
train loss:  0.48756521940231323
train gradient:  0.12252124803128003
iteration : 8468
train acc:  0.828125
train loss:  0.4365522265434265
train gradient:  0.11462842072989973
iteration : 8469
train acc:  0.6953125
train loss:  0.565372109413147
train gradient:  0.20100365403462422
iteration : 8470
train acc:  0.75
train loss:  0.4921721816062927
train gradient:  0.12140488667038467
iteration : 8471
train acc:  0.7734375
train loss:  0.4448942542076111
train gradient:  0.10970598931998474
iteration : 8472
train acc:  0.7265625
train loss:  0.5170120596885681
train gradient:  0.14890051571750218
iteration : 8473
train acc:  0.7734375
train loss:  0.4915808439254761
train gradient:  0.13125290619000102
iteration : 8474
train acc:  0.7265625
train loss:  0.4800958037376404
train gradient:  0.13318158358529528
iteration : 8475
train acc:  0.734375
train loss:  0.5271364450454712
train gradient:  0.16213745652631995
iteration : 8476
train acc:  0.7421875
train loss:  0.4819086790084839
train gradient:  0.11274970901283277
iteration : 8477
train acc:  0.6953125
train loss:  0.5088262557983398
train gradient:  0.14619937119859938
iteration : 8478
train acc:  0.7734375
train loss:  0.510066032409668
train gradient:  0.17879967653447792
iteration : 8479
train acc:  0.7578125
train loss:  0.49749258160591125
train gradient:  0.11814468197345017
iteration : 8480
train acc:  0.65625
train loss:  0.6069091558456421
train gradient:  0.15958543885184343
iteration : 8481
train acc:  0.7421875
train loss:  0.44689834117889404
train gradient:  0.0845469525056298
iteration : 8482
train acc:  0.71875
train loss:  0.49553829431533813
train gradient:  0.13487394151214704
iteration : 8483
train acc:  0.6875
train loss:  0.5827341675758362
train gradient:  0.146189880771144
iteration : 8484
train acc:  0.703125
train loss:  0.5406731367111206
train gradient:  0.1359213826895686
iteration : 8485
train acc:  0.7890625
train loss:  0.4560968577861786
train gradient:  0.11029123346425598
iteration : 8486
train acc:  0.703125
train loss:  0.5554962158203125
train gradient:  0.1612201216117844
iteration : 8487
train acc:  0.7109375
train loss:  0.5515809655189514
train gradient:  0.16547888324324855
iteration : 8488
train acc:  0.7734375
train loss:  0.48033833503723145
train gradient:  0.13925752433520666
iteration : 8489
train acc:  0.671875
train loss:  0.5504566431045532
train gradient:  0.14534098836022105
iteration : 8490
train acc:  0.6640625
train loss:  0.5761674046516418
train gradient:  0.13928508744780949
iteration : 8491
train acc:  0.859375
train loss:  0.3598533570766449
train gradient:  0.08243769703897683
iteration : 8492
train acc:  0.65625
train loss:  0.5435144305229187
train gradient:  0.20878548960220875
iteration : 8493
train acc:  0.765625
train loss:  0.4654489755630493
train gradient:  0.1326507589922269
iteration : 8494
train acc:  0.75
train loss:  0.5300583839416504
train gradient:  0.15727676257405035
iteration : 8495
train acc:  0.75
train loss:  0.4754505753517151
train gradient:  0.12471164004064667
iteration : 8496
train acc:  0.7890625
train loss:  0.4747188687324524
train gradient:  0.10861241456326193
iteration : 8497
train acc:  0.7890625
train loss:  0.44222205877304077
train gradient:  0.09151153478034134
iteration : 8498
train acc:  0.796875
train loss:  0.5190364718437195
train gradient:  0.1310534273418522
iteration : 8499
train acc:  0.765625
train loss:  0.49014586210250854
train gradient:  0.16737147729246
iteration : 8500
train acc:  0.7109375
train loss:  0.5331825017929077
train gradient:  0.2054110969934132
iteration : 8501
train acc:  0.7734375
train loss:  0.4556978642940521
train gradient:  0.09570782801466411
iteration : 8502
train acc:  0.7578125
train loss:  0.4487847685813904
train gradient:  0.0930023856346938
iteration : 8503
train acc:  0.703125
train loss:  0.4818154275417328
train gradient:  0.12536330417716407
iteration : 8504
train acc:  0.7109375
train loss:  0.46985718607902527
train gradient:  0.10162410763715075
iteration : 8505
train acc:  0.765625
train loss:  0.4874202013015747
train gradient:  0.11218875073117882
iteration : 8506
train acc:  0.7578125
train loss:  0.46551281213760376
train gradient:  0.10659885535801625
iteration : 8507
train acc:  0.765625
train loss:  0.47856414318084717
train gradient:  0.1321254248357689
iteration : 8508
train acc:  0.6796875
train loss:  0.5377342700958252
train gradient:  0.15638060919440627
iteration : 8509
train acc:  0.71875
train loss:  0.5227937698364258
train gradient:  0.13235973806957518
iteration : 8510
train acc:  0.7421875
train loss:  0.4688376784324646
train gradient:  0.1461330730272793
iteration : 8511
train acc:  0.7265625
train loss:  0.5371823310852051
train gradient:  0.1333021375304
iteration : 8512
train acc:  0.71875
train loss:  0.5423153638839722
train gradient:  0.20356567803952869
iteration : 8513
train acc:  0.765625
train loss:  0.5191628336906433
train gradient:  0.11279607747208538
iteration : 8514
train acc:  0.796875
train loss:  0.4582042694091797
train gradient:  0.11522570349340103
iteration : 8515
train acc:  0.796875
train loss:  0.4229390621185303
train gradient:  0.08796284488251015
iteration : 8516
train acc:  0.734375
train loss:  0.4813305139541626
train gradient:  0.09826244660196781
iteration : 8517
train acc:  0.7265625
train loss:  0.5116918087005615
train gradient:  0.17188552490857506
iteration : 8518
train acc:  0.75
train loss:  0.4980562627315521
train gradient:  0.14854727759174757
iteration : 8519
train acc:  0.71875
train loss:  0.5175791382789612
train gradient:  0.14430010014990352
iteration : 8520
train acc:  0.7578125
train loss:  0.49429425597190857
train gradient:  0.1117576987644769
iteration : 8521
train acc:  0.734375
train loss:  0.46916598081588745
train gradient:  0.10747387376254602
iteration : 8522
train acc:  0.6796875
train loss:  0.5281789898872375
train gradient:  0.18862712784317542
iteration : 8523
train acc:  0.7265625
train loss:  0.4897720217704773
train gradient:  0.1473146694391422
iteration : 8524
train acc:  0.6796875
train loss:  0.5279213786125183
train gradient:  0.11787450297594274
iteration : 8525
train acc:  0.7734375
train loss:  0.4524326026439667
train gradient:  0.10059660639478221
iteration : 8526
train acc:  0.765625
train loss:  0.4521092176437378
train gradient:  0.10971274637211091
iteration : 8527
train acc:  0.7109375
train loss:  0.5214390754699707
train gradient:  0.126610934136245
iteration : 8528
train acc:  0.7421875
train loss:  0.5234161615371704
train gradient:  0.1423154807892556
iteration : 8529
train acc:  0.671875
train loss:  0.5958459377288818
train gradient:  0.18931403034634497
iteration : 8530
train acc:  0.6953125
train loss:  0.5538280010223389
train gradient:  0.19039541592547854
iteration : 8531
train acc:  0.7578125
train loss:  0.4785473644733429
train gradient:  0.12937095448698055
iteration : 8532
train acc:  0.796875
train loss:  0.4500960111618042
train gradient:  0.09880591049729975
iteration : 8533
train acc:  0.6328125
train loss:  0.541340172290802
train gradient:  0.14860130398223576
iteration : 8534
train acc:  0.75
train loss:  0.5035158395767212
train gradient:  0.11771619026011094
iteration : 8535
train acc:  0.71875
train loss:  0.5105398297309875
train gradient:  0.15676296864577305
iteration : 8536
train acc:  0.7734375
train loss:  0.46964141726493835
train gradient:  0.10982913962380875
iteration : 8537
train acc:  0.71875
train loss:  0.5733550786972046
train gradient:  0.15932264860158785
iteration : 8538
train acc:  0.8203125
train loss:  0.41892993450164795
train gradient:  0.10874281228652248
iteration : 8539
train acc:  0.734375
train loss:  0.4923391342163086
train gradient:  0.10094689702004286
iteration : 8540
train acc:  0.640625
train loss:  0.5443732738494873
train gradient:  0.14217577181192065
iteration : 8541
train acc:  0.6796875
train loss:  0.5443793535232544
train gradient:  0.1352112702347692
iteration : 8542
train acc:  0.7578125
train loss:  0.5384650230407715
train gradient:  0.14535017876589482
iteration : 8543
train acc:  0.7421875
train loss:  0.5196042656898499
train gradient:  0.13558640577632525
iteration : 8544
train acc:  0.6328125
train loss:  0.5822665691375732
train gradient:  0.1677353669392923
iteration : 8545
train acc:  0.6640625
train loss:  0.6108309030532837
train gradient:  0.19822486107159928
iteration : 8546
train acc:  0.7734375
train loss:  0.4824901819229126
train gradient:  0.12681648073174154
iteration : 8547
train acc:  0.7578125
train loss:  0.47995322942733765
train gradient:  0.10661802385293076
iteration : 8548
train acc:  0.796875
train loss:  0.4295238256454468
train gradient:  0.10221028978307366
iteration : 8549
train acc:  0.78125
train loss:  0.4370741546154022
train gradient:  0.09546254023066425
iteration : 8550
train acc:  0.7265625
train loss:  0.5241732597351074
train gradient:  0.12636989514192581
iteration : 8551
train acc:  0.703125
train loss:  0.5159962177276611
train gradient:  0.12768048907962926
iteration : 8552
train acc:  0.7578125
train loss:  0.5657244920730591
train gradient:  0.1656849074434377
iteration : 8553
train acc:  0.703125
train loss:  0.524246096611023
train gradient:  0.1504560408690651
iteration : 8554
train acc:  0.7734375
train loss:  0.44862210750579834
train gradient:  0.09866444368590464
iteration : 8555
train acc:  0.75
train loss:  0.4738352298736572
train gradient:  0.09508222524465927
iteration : 8556
train acc:  0.671875
train loss:  0.5671659708023071
train gradient:  0.18681212055592378
iteration : 8557
train acc:  0.734375
train loss:  0.5093068480491638
train gradient:  0.16111608207155176
iteration : 8558
train acc:  0.6953125
train loss:  0.5118563175201416
train gradient:  0.1376373956011543
iteration : 8559
train acc:  0.734375
train loss:  0.5223745107650757
train gradient:  0.12335918958440484
iteration : 8560
train acc:  0.6796875
train loss:  0.5173898935317993
train gradient:  0.11859250494136622
iteration : 8561
train acc:  0.7578125
train loss:  0.4510401487350464
train gradient:  0.0953544638987213
iteration : 8562
train acc:  0.7109375
train loss:  0.5046733617782593
train gradient:  0.12139369023186711
iteration : 8563
train acc:  0.7421875
train loss:  0.5287975668907166
train gradient:  0.13087703685223168
iteration : 8564
train acc:  0.71875
train loss:  0.5206458568572998
train gradient:  0.17640985708805185
iteration : 8565
train acc:  0.765625
train loss:  0.45255833864212036
train gradient:  0.12231692608718027
iteration : 8566
train acc:  0.703125
train loss:  0.531948447227478
train gradient:  0.15549305426164478
iteration : 8567
train acc:  0.71875
train loss:  0.5224869251251221
train gradient:  0.14460943759375472
iteration : 8568
train acc:  0.78125
train loss:  0.517749011516571
train gradient:  0.13098013379213003
iteration : 8569
train acc:  0.7578125
train loss:  0.4507547616958618
train gradient:  0.1046093846865362
iteration : 8570
train acc:  0.7421875
train loss:  0.5116558074951172
train gradient:  0.1464195045407095
iteration : 8571
train acc:  0.6875
train loss:  0.5164707899093628
train gradient:  0.1149754151299896
iteration : 8572
train acc:  0.7421875
train loss:  0.5176702737808228
train gradient:  0.14163903152777163
iteration : 8573
train acc:  0.6953125
train loss:  0.5757648944854736
train gradient:  0.14926503062247085
iteration : 8574
train acc:  0.703125
train loss:  0.550947368144989
train gradient:  0.15575241265376608
iteration : 8575
train acc:  0.78125
train loss:  0.4374966025352478
train gradient:  0.1038040217184409
iteration : 8576
train acc:  0.6796875
train loss:  0.586092472076416
train gradient:  0.19979277239472648
iteration : 8577
train acc:  0.78125
train loss:  0.45130348205566406
train gradient:  0.1148480565570368
iteration : 8578
train acc:  0.7578125
train loss:  0.4766424298286438
train gradient:  0.11994210691172953
iteration : 8579
train acc:  0.75
train loss:  0.48337987065315247
train gradient:  0.1180314076615952
iteration : 8580
train acc:  0.796875
train loss:  0.45156484842300415
train gradient:  0.08876878553049422
iteration : 8581
train acc:  0.8046875
train loss:  0.43016356229782104
train gradient:  0.08911095204387777
iteration : 8582
train acc:  0.7578125
train loss:  0.5127872228622437
train gradient:  0.2188115866469242
iteration : 8583
train acc:  0.7734375
train loss:  0.4510415494441986
train gradient:  0.11320618151046455
iteration : 8584
train acc:  0.71875
train loss:  0.4802020788192749
train gradient:  0.11048360604125915
iteration : 8585
train acc:  0.78125
train loss:  0.4355809986591339
train gradient:  0.12716200570612007
iteration : 8586
train acc:  0.7890625
train loss:  0.4174231290817261
train gradient:  0.09331969198694526
iteration : 8587
train acc:  0.765625
train loss:  0.45536792278289795
train gradient:  0.10714603595016545
iteration : 8588
train acc:  0.7578125
train loss:  0.5183053612709045
train gradient:  0.1407888232722426
iteration : 8589
train acc:  0.6875
train loss:  0.49616146087646484
train gradient:  0.10785620926804489
iteration : 8590
train acc:  0.71875
train loss:  0.4944343566894531
train gradient:  0.10498875219138605
iteration : 8591
train acc:  0.78125
train loss:  0.4967847466468811
train gradient:  0.1309790229270781
iteration : 8592
train acc:  0.703125
train loss:  0.6621714234352112
train gradient:  0.19813263586719093
iteration : 8593
train acc:  0.765625
train loss:  0.46520793437957764
train gradient:  0.12079902984744798
iteration : 8594
train acc:  0.78125
train loss:  0.48829007148742676
train gradient:  0.1487853702171257
iteration : 8595
train acc:  0.703125
train loss:  0.478326678276062
train gradient:  0.13130380757740076
iteration : 8596
train acc:  0.7109375
train loss:  0.5565983057022095
train gradient:  0.14916600204148633
iteration : 8597
train acc:  0.7578125
train loss:  0.46030572056770325
train gradient:  0.11908252150002828
iteration : 8598
train acc:  0.765625
train loss:  0.48048484325408936
train gradient:  0.16535104021951047
iteration : 8599
train acc:  0.71875
train loss:  0.47870010137557983
train gradient:  0.13313120347313787
iteration : 8600
train acc:  0.71875
train loss:  0.5682398080825806
train gradient:  0.13994539867036543
iteration : 8601
train acc:  0.796875
train loss:  0.4680395722389221
train gradient:  0.09870624503410869
iteration : 8602
train acc:  0.7109375
train loss:  0.6017764806747437
train gradient:  0.2572700331199231
iteration : 8603
train acc:  0.7109375
train loss:  0.5440917015075684
train gradient:  0.21614760268371086
iteration : 8604
train acc:  0.703125
train loss:  0.49894142150878906
train gradient:  0.14532243119245364
iteration : 8605
train acc:  0.765625
train loss:  0.4798198938369751
train gradient:  0.11736055411264987
iteration : 8606
train acc:  0.78125
train loss:  0.47833943367004395
train gradient:  0.12851168612965488
iteration : 8607
train acc:  0.734375
train loss:  0.49557793140411377
train gradient:  0.11211156475005803
iteration : 8608
train acc:  0.71875
train loss:  0.538170576095581
train gradient:  0.1204824537347092
iteration : 8609
train acc:  0.7109375
train loss:  0.5212399959564209
train gradient:  0.14324434138195774
iteration : 8610
train acc:  0.703125
train loss:  0.5197604298591614
train gradient:  0.1670969175117655
iteration : 8611
train acc:  0.734375
train loss:  0.5018064379692078
train gradient:  0.15537093009956132
iteration : 8612
train acc:  0.71875
train loss:  0.47044187784194946
train gradient:  0.12237386343901524
iteration : 8613
train acc:  0.7421875
train loss:  0.4971866011619568
train gradient:  0.13750387280423754
iteration : 8614
train acc:  0.75
train loss:  0.47933176159858704
train gradient:  0.1151514691439458
iteration : 8615
train acc:  0.734375
train loss:  0.553318977355957
train gradient:  0.1549359240359965
iteration : 8616
train acc:  0.8203125
train loss:  0.4557805359363556
train gradient:  0.14791447362810342
iteration : 8617
train acc:  0.78125
train loss:  0.4584687352180481
train gradient:  0.10917497357504448
iteration : 8618
train acc:  0.7890625
train loss:  0.4738428592681885
train gradient:  0.12085575942060392
iteration : 8619
train acc:  0.734375
train loss:  0.5244459509849548
train gradient:  0.1503588257517442
iteration : 8620
train acc:  0.796875
train loss:  0.424121618270874
train gradient:  0.10009031372241887
iteration : 8621
train acc:  0.765625
train loss:  0.49213775992393494
train gradient:  0.1404513325059917
iteration : 8622
train acc:  0.7265625
train loss:  0.5103573799133301
train gradient:  0.1381847699604637
iteration : 8623
train acc:  0.8359375
train loss:  0.42571717500686646
train gradient:  0.1059628404897396
iteration : 8624
train acc:  0.7578125
train loss:  0.4862823486328125
train gradient:  0.11905614291109204
iteration : 8625
train acc:  0.7109375
train loss:  0.5304735898971558
train gradient:  0.1300135430839459
iteration : 8626
train acc:  0.7109375
train loss:  0.5549274682998657
train gradient:  0.15041412085999883
iteration : 8627
train acc:  0.71875
train loss:  0.4814322590827942
train gradient:  0.10477045726181737
iteration : 8628
train acc:  0.75
train loss:  0.4993259012699127
train gradient:  0.16847014331371468
iteration : 8629
train acc:  0.75
train loss:  0.48369431495666504
train gradient:  0.10800685177678813
iteration : 8630
train acc:  0.7421875
train loss:  0.4915158152580261
train gradient:  0.12811227350594998
iteration : 8631
train acc:  0.7578125
train loss:  0.4891716241836548
train gradient:  0.16033620680272287
iteration : 8632
train acc:  0.7265625
train loss:  0.5093296766281128
train gradient:  0.14552787383994292
iteration : 8633
train acc:  0.7265625
train loss:  0.49386999011039734
train gradient:  0.10715043694157088
iteration : 8634
train acc:  0.765625
train loss:  0.45149144530296326
train gradient:  0.10185716538294212
iteration : 8635
train acc:  0.671875
train loss:  0.5678883194923401
train gradient:  0.12807140394712363
iteration : 8636
train acc:  0.78125
train loss:  0.49875009059906006
train gradient:  0.11630278573584074
iteration : 8637
train acc:  0.75
train loss:  0.4793907403945923
train gradient:  0.10934066239593293
iteration : 8638
train acc:  0.734375
train loss:  0.534587025642395
train gradient:  0.14411788259530922
iteration : 8639
train acc:  0.7578125
train loss:  0.41793763637542725
train gradient:  0.08849196540521567
iteration : 8640
train acc:  0.78125
train loss:  0.47098225355148315
train gradient:  0.11397807020069992
iteration : 8641
train acc:  0.7421875
train loss:  0.5303429365158081
train gradient:  0.12574398304131418
iteration : 8642
train acc:  0.8203125
train loss:  0.4112081825733185
train gradient:  0.07673215445464329
iteration : 8643
train acc:  0.7421875
train loss:  0.5054591298103333
train gradient:  0.1756777129895772
iteration : 8644
train acc:  0.75
train loss:  0.49561619758605957
train gradient:  0.11952180815262631
iteration : 8645
train acc:  0.7890625
train loss:  0.4395740032196045
train gradient:  0.08853428460014531
iteration : 8646
train acc:  0.7734375
train loss:  0.4376305937767029
train gradient:  0.13378044803270428
iteration : 8647
train acc:  0.7421875
train loss:  0.5254040956497192
train gradient:  0.12203056752750434
iteration : 8648
train acc:  0.6875
train loss:  0.551581621170044
train gradient:  0.15434532826898748
iteration : 8649
train acc:  0.78125
train loss:  0.4947469234466553
train gradient:  0.1416300062149885
iteration : 8650
train acc:  0.6953125
train loss:  0.5608397126197815
train gradient:  0.12001935593306429
iteration : 8651
train acc:  0.7421875
train loss:  0.48048579692840576
train gradient:  0.12816276552452244
iteration : 8652
train acc:  0.75
train loss:  0.49758780002593994
train gradient:  0.11814494199293585
iteration : 8653
train acc:  0.7265625
train loss:  0.4992086887359619
train gradient:  0.1278119981727412
iteration : 8654
train acc:  0.71875
train loss:  0.5139724016189575
train gradient:  0.11451824009444957
iteration : 8655
train acc:  0.7578125
train loss:  0.5021008253097534
train gradient:  0.15359704945914857
iteration : 8656
train acc:  0.7109375
train loss:  0.526821494102478
train gradient:  0.14252473464630813
iteration : 8657
train acc:  0.703125
train loss:  0.5225890278816223
train gradient:  0.1359645099971078
iteration : 8658
train acc:  0.796875
train loss:  0.4553551971912384
train gradient:  0.10233151759303606
iteration : 8659
train acc:  0.703125
train loss:  0.4794141948223114
train gradient:  0.1307599781829245
iteration : 8660
train acc:  0.75
train loss:  0.46263647079467773
train gradient:  0.13091605283706084
iteration : 8661
train acc:  0.6953125
train loss:  0.5492035746574402
train gradient:  0.1729141329094414
iteration : 8662
train acc:  0.71875
train loss:  0.5033191442489624
train gradient:  0.1183563567596331
iteration : 8663
train acc:  0.7578125
train loss:  0.4895712733268738
train gradient:  0.13366938620687788
iteration : 8664
train acc:  0.765625
train loss:  0.5057721138000488
train gradient:  0.169881204706813
iteration : 8665
train acc:  0.671875
train loss:  0.5986523032188416
train gradient:  0.15242960655198398
iteration : 8666
train acc:  0.75
train loss:  0.46442916989326477
train gradient:  0.10146667650134611
iteration : 8667
train acc:  0.7890625
train loss:  0.47555476427078247
train gradient:  0.1188489644198233
iteration : 8668
train acc:  0.7265625
train loss:  0.5246010422706604
train gradient:  0.14839315055877234
iteration : 8669
train acc:  0.6640625
train loss:  0.5704660415649414
train gradient:  0.1380148371893689
iteration : 8670
train acc:  0.8203125
train loss:  0.39229315519332886
train gradient:  0.08808047305690744
iteration : 8671
train acc:  0.7265625
train loss:  0.466329962015152
train gradient:  0.11307567005307295
iteration : 8672
train acc:  0.765625
train loss:  0.46503323316574097
train gradient:  0.1014889864906226
iteration : 8673
train acc:  0.78125
train loss:  0.48467111587524414
train gradient:  0.13060844084492043
iteration : 8674
train acc:  0.7265625
train loss:  0.5394427180290222
train gradient:  0.16427789296643747
iteration : 8675
train acc:  0.734375
train loss:  0.5143250226974487
train gradient:  0.12648744632814718
iteration : 8676
train acc:  0.703125
train loss:  0.5239028930664062
train gradient:  0.14437341712814733
iteration : 8677
train acc:  0.7734375
train loss:  0.49157118797302246
train gradient:  0.13164935231172276
iteration : 8678
train acc:  0.7109375
train loss:  0.5253846645355225
train gradient:  0.13445330935557878
iteration : 8679
train acc:  0.796875
train loss:  0.4617198705673218
train gradient:  0.11698144655358396
iteration : 8680
train acc:  0.765625
train loss:  0.4737774729728699
train gradient:  0.12381696211600278
iteration : 8681
train acc:  0.75
train loss:  0.4923418462276459
train gradient:  0.15289512774785136
iteration : 8682
train acc:  0.6484375
train loss:  0.5630591511726379
train gradient:  0.19053039666881616
iteration : 8683
train acc:  0.6953125
train loss:  0.5585839152336121
train gradient:  0.17970503607905414
iteration : 8684
train acc:  0.75
train loss:  0.47753793001174927
train gradient:  0.11200516655502774
iteration : 8685
train acc:  0.75
train loss:  0.5253547430038452
train gradient:  0.13044886777003148
iteration : 8686
train acc:  0.6796875
train loss:  0.5359683036804199
train gradient:  0.13656446482217185
iteration : 8687
train acc:  0.7421875
train loss:  0.5636566877365112
train gradient:  0.12707474222658127
iteration : 8688
train acc:  0.71875
train loss:  0.5203971266746521
train gradient:  0.15003771231333907
iteration : 8689
train acc:  0.7421875
train loss:  0.5146619081497192
train gradient:  0.14089784964485166
iteration : 8690
train acc:  0.78125
train loss:  0.5006791353225708
train gradient:  0.16617829231697556
iteration : 8691
train acc:  0.765625
train loss:  0.4895547032356262
train gradient:  0.11631014900348348
iteration : 8692
train acc:  0.6953125
train loss:  0.5223809480667114
train gradient:  0.1377001437680339
iteration : 8693
train acc:  0.7890625
train loss:  0.4552878737449646
train gradient:  0.14956360102837507
iteration : 8694
train acc:  0.703125
train loss:  0.5323082208633423
train gradient:  0.1312629322094331
iteration : 8695
train acc:  0.765625
train loss:  0.4863574206829071
train gradient:  0.12667334294461702
iteration : 8696
train acc:  0.8359375
train loss:  0.45209720730781555
train gradient:  0.091277486650899
iteration : 8697
train acc:  0.7421875
train loss:  0.44612759351730347
train gradient:  0.09349604122190935
iteration : 8698
train acc:  0.7890625
train loss:  0.4766310155391693
train gradient:  0.10912698971506941
iteration : 8699
train acc:  0.7265625
train loss:  0.4818212389945984
train gradient:  0.11027052363263017
iteration : 8700
train acc:  0.7265625
train loss:  0.5149948596954346
train gradient:  0.13857658251035393
iteration : 8701
train acc:  0.6953125
train loss:  0.5829097032546997
train gradient:  0.16040183216168938
iteration : 8702
train acc:  0.7578125
train loss:  0.5339394211769104
train gradient:  0.1259988525902131
iteration : 8703
train acc:  0.8125
train loss:  0.40923023223876953
train gradient:  0.08162759578035603
iteration : 8704
train acc:  0.7265625
train loss:  0.5327304005622864
train gradient:  0.12610581011597805
iteration : 8705
train acc:  0.71875
train loss:  0.542863667011261
train gradient:  0.11325715885671071
iteration : 8706
train acc:  0.7734375
train loss:  0.491321861743927
train gradient:  0.11577138925206942
iteration : 8707
train acc:  0.6875
train loss:  0.49902331829071045
train gradient:  0.13718961964950369
iteration : 8708
train acc:  0.7265625
train loss:  0.5690734386444092
train gradient:  0.13354652428426697
iteration : 8709
train acc:  0.734375
train loss:  0.49275636672973633
train gradient:  0.16344816992269132
iteration : 8710
train acc:  0.7265625
train loss:  0.4754337966442108
train gradient:  0.12095333219913314
iteration : 8711
train acc:  0.734375
train loss:  0.5053593516349792
train gradient:  0.17389783356029598
iteration : 8712
train acc:  0.7265625
train loss:  0.5127381086349487
train gradient:  0.2024012306369005
iteration : 8713
train acc:  0.7421875
train loss:  0.46156632900238037
train gradient:  0.10125473585012545
iteration : 8714
train acc:  0.7734375
train loss:  0.48521649837493896
train gradient:  0.09359061686339254
iteration : 8715
train acc:  0.75
train loss:  0.46371448040008545
train gradient:  0.12121846612261435
iteration : 8716
train acc:  0.6953125
train loss:  0.5345968008041382
train gradient:  0.15273443612460366
iteration : 8717
train acc:  0.7890625
train loss:  0.4842776656150818
train gradient:  0.14480915499976404
iteration : 8718
train acc:  0.7578125
train loss:  0.4325929284095764
train gradient:  0.10587659020389427
iteration : 8719
train acc:  0.6875
train loss:  0.5680540800094604
train gradient:  0.15496867964912747
iteration : 8720
train acc:  0.7890625
train loss:  0.45424914360046387
train gradient:  0.10195306436985346
iteration : 8721
train acc:  0.78125
train loss:  0.4337399899959564
train gradient:  0.08985800543492695
iteration : 8722
train acc:  0.7734375
train loss:  0.4501335322856903
train gradient:  0.09955118650496819
iteration : 8723
train acc:  0.71875
train loss:  0.49582546949386597
train gradient:  0.0939661964549744
iteration : 8724
train acc:  0.78125
train loss:  0.47406989336013794
train gradient:  0.10646843738378721
iteration : 8725
train acc:  0.78125
train loss:  0.44309258460998535
train gradient:  0.10970982486076242
iteration : 8726
train acc:  0.7421875
train loss:  0.45775458216667175
train gradient:  0.1028516624918217
iteration : 8727
train acc:  0.6640625
train loss:  0.613938570022583
train gradient:  0.17887885455856395
iteration : 8728
train acc:  0.7734375
train loss:  0.5165515542030334
train gradient:  0.12484794180599591
iteration : 8729
train acc:  0.734375
train loss:  0.4883645176887512
train gradient:  0.12130176915805038
iteration : 8730
train acc:  0.78125
train loss:  0.4648232161998749
train gradient:  0.10091517300021208
iteration : 8731
train acc:  0.75
train loss:  0.5315815210342407
train gradient:  0.17693615595222234
iteration : 8732
train acc:  0.734375
train loss:  0.5053570866584778
train gradient:  0.1510479159061324
iteration : 8733
train acc:  0.796875
train loss:  0.42314285039901733
train gradient:  0.13527618204739333
iteration : 8734
train acc:  0.703125
train loss:  0.4939444363117218
train gradient:  0.12287157993773754
iteration : 8735
train acc:  0.765625
train loss:  0.5192707777023315
train gradient:  0.1693361138891007
iteration : 8736
train acc:  0.765625
train loss:  0.530694305896759
train gradient:  0.15392580894500874
iteration : 8737
train acc:  0.7421875
train loss:  0.47224992513656616
train gradient:  0.11023843603380702
iteration : 8738
train acc:  0.71875
train loss:  0.4910626709461212
train gradient:  0.10097378357789694
iteration : 8739
train acc:  0.6640625
train loss:  0.5480837821960449
train gradient:  0.1470018457989533
iteration : 8740
train acc:  0.7578125
train loss:  0.47598233819007874
train gradient:  0.11472792977391372
iteration : 8741
train acc:  0.7578125
train loss:  0.5490909218788147
train gradient:  0.1455745170287945
iteration : 8742
train acc:  0.71875
train loss:  0.5258894562721252
train gradient:  0.13268886503469743
iteration : 8743
train acc:  0.7734375
train loss:  0.5081948637962341
train gradient:  0.15691716005783946
iteration : 8744
train acc:  0.7578125
train loss:  0.45177826285362244
train gradient:  0.13066551976829813
iteration : 8745
train acc:  0.6640625
train loss:  0.5644233226776123
train gradient:  0.16988544335016317
iteration : 8746
train acc:  0.7265625
train loss:  0.4854264259338379
train gradient:  0.11534665459268964
iteration : 8747
train acc:  0.8203125
train loss:  0.43207865953445435
train gradient:  0.10578657661167876
iteration : 8748
train acc:  0.7265625
train loss:  0.49458980560302734
train gradient:  0.1545274645741423
iteration : 8749
train acc:  0.7421875
train loss:  0.5072379112243652
train gradient:  0.12489156334255627
iteration : 8750
train acc:  0.7265625
train loss:  0.5193187594413757
train gradient:  0.11468190627270274
iteration : 8751
train acc:  0.7578125
train loss:  0.4436662197113037
train gradient:  0.09522034653825216
iteration : 8752
train acc:  0.765625
train loss:  0.470329225063324
train gradient:  0.1039179679417917
iteration : 8753
train acc:  0.734375
train loss:  0.5111700296401978
train gradient:  0.14276630538049276
iteration : 8754
train acc:  0.75
train loss:  0.5371640920639038
train gradient:  0.17145857961087047
iteration : 8755
train acc:  0.71875
train loss:  0.5047985315322876
train gradient:  0.14062669327130695
iteration : 8756
train acc:  0.703125
train loss:  0.5426471829414368
train gradient:  0.17609931756280578
iteration : 8757
train acc:  0.8203125
train loss:  0.3950662314891815
train gradient:  0.09052826460890427
iteration : 8758
train acc:  0.7734375
train loss:  0.4661242365837097
train gradient:  0.10824948213638329
iteration : 8759
train acc:  0.7578125
train loss:  0.49989673495292664
train gradient:  0.13625258452992764
iteration : 8760
train acc:  0.7578125
train loss:  0.45610231161117554
train gradient:  0.10679003845102528
iteration : 8761
train acc:  0.734375
train loss:  0.5227108001708984
train gradient:  0.12904749340503355
iteration : 8762
train acc:  0.7421875
train loss:  0.5047399997711182
train gradient:  0.11244317659874341
iteration : 8763
train acc:  0.75
train loss:  0.4592263102531433
train gradient:  0.11402131372950695
iteration : 8764
train acc:  0.6875
train loss:  0.4921090304851532
train gradient:  0.12056235212956815
iteration : 8765
train acc:  0.7421875
train loss:  0.522193193435669
train gradient:  0.13618064568878643
iteration : 8766
train acc:  0.7265625
train loss:  0.5424555540084839
train gradient:  0.15362757547676242
iteration : 8767
train acc:  0.7109375
train loss:  0.5464837551116943
train gradient:  0.12432533717558925
iteration : 8768
train acc:  0.7734375
train loss:  0.4602200984954834
train gradient:  0.13558704771485336
iteration : 8769
train acc:  0.734375
train loss:  0.5142566561698914
train gradient:  0.15861691276013234
iteration : 8770
train acc:  0.71875
train loss:  0.5202081203460693
train gradient:  0.15471527522096928
iteration : 8771
train acc:  0.8046875
train loss:  0.4392598271369934
train gradient:  0.10857634973710152
iteration : 8772
train acc:  0.6875
train loss:  0.4993826150894165
train gradient:  0.10760320117839624
iteration : 8773
train acc:  0.78125
train loss:  0.5044346451759338
train gradient:  0.13119404405583757
iteration : 8774
train acc:  0.7265625
train loss:  0.49097874760627747
train gradient:  0.1205188108808554
iteration : 8775
train acc:  0.7734375
train loss:  0.4573262631893158
train gradient:  0.18458401178793993
iteration : 8776
train acc:  0.7421875
train loss:  0.4984404444694519
train gradient:  0.11259163642729372
iteration : 8777
train acc:  0.75
train loss:  0.4551658034324646
train gradient:  0.1065579796557585
iteration : 8778
train acc:  0.7109375
train loss:  0.5366260409355164
train gradient:  0.17470742969045622
iteration : 8779
train acc:  0.75
train loss:  0.4871671199798584
train gradient:  0.12505548387933807
iteration : 8780
train acc:  0.7109375
train loss:  0.5322584509849548
train gradient:  0.1419875486385811
iteration : 8781
train acc:  0.7421875
train loss:  0.5051994323730469
train gradient:  0.11369355128432958
iteration : 8782
train acc:  0.6875
train loss:  0.5169341564178467
train gradient:  0.12399111177307638
iteration : 8783
train acc:  0.8046875
train loss:  0.4108368754386902
train gradient:  0.10107636532138016
iteration : 8784
train acc:  0.75
train loss:  0.5326786041259766
train gradient:  0.12120647267442299
iteration : 8785
train acc:  0.796875
train loss:  0.44660842418670654
train gradient:  0.10994466054124552
iteration : 8786
train acc:  0.6875
train loss:  0.5213012099266052
train gradient:  0.155782195400403
iteration : 8787
train acc:  0.75
train loss:  0.49295127391815186
train gradient:  0.13200660922775947
iteration : 8788
train acc:  0.7734375
train loss:  0.43774232268333435
train gradient:  0.11359494578314026
iteration : 8789
train acc:  0.7265625
train loss:  0.5077496767044067
train gradient:  0.118555499633269
iteration : 8790
train acc:  0.6875
train loss:  0.5081641674041748
train gradient:  0.09479944984972288
iteration : 8791
train acc:  0.7265625
train loss:  0.5176334381103516
train gradient:  0.12671555633440867
iteration : 8792
train acc:  0.765625
train loss:  0.5146734714508057
train gradient:  0.14025793960521313
iteration : 8793
train acc:  0.671875
train loss:  0.5651562213897705
train gradient:  0.15534144824846347
iteration : 8794
train acc:  0.78125
train loss:  0.4352548122406006
train gradient:  0.10321748195183697
iteration : 8795
train acc:  0.75
train loss:  0.4701520800590515
train gradient:  0.13751332401758423
iteration : 8796
train acc:  0.6953125
train loss:  0.5432406663894653
train gradient:  0.14150940736680478
iteration : 8797
train acc:  0.765625
train loss:  0.4591018557548523
train gradient:  0.10101069429849695
iteration : 8798
train acc:  0.6640625
train loss:  0.573395848274231
train gradient:  0.1500172268805865
iteration : 8799
train acc:  0.71875
train loss:  0.5436329245567322
train gradient:  0.1624687491265685
iteration : 8800
train acc:  0.7109375
train loss:  0.5300513505935669
train gradient:  0.1557975254010603
iteration : 8801
train acc:  0.7578125
train loss:  0.4656183123588562
train gradient:  0.1043983795183343
iteration : 8802
train acc:  0.7109375
train loss:  0.5499674677848816
train gradient:  0.13739388726738733
iteration : 8803
train acc:  0.734375
train loss:  0.46420302987098694
train gradient:  0.11801237546637575
iteration : 8804
train acc:  0.7734375
train loss:  0.49453336000442505
train gradient:  0.11511514919638312
iteration : 8805
train acc:  0.7421875
train loss:  0.4889683723449707
train gradient:  0.10172722006813627
iteration : 8806
train acc:  0.7578125
train loss:  0.49004384875297546
train gradient:  0.10279979958234814
iteration : 8807
train acc:  0.7421875
train loss:  0.43361896276474
train gradient:  0.09049456257209515
iteration : 8808
train acc:  0.71875
train loss:  0.491033136844635
train gradient:  0.12948817259029893
iteration : 8809
train acc:  0.796875
train loss:  0.4563782513141632
train gradient:  0.11962354371038426
iteration : 8810
train acc:  0.7265625
train loss:  0.5269366502761841
train gradient:  0.13899518013544243
iteration : 8811
train acc:  0.7109375
train loss:  0.5309208631515503
train gradient:  0.12419702821926333
iteration : 8812
train acc:  0.7890625
train loss:  0.43771013617515564
train gradient:  0.10003637979617783
iteration : 8813
train acc:  0.71875
train loss:  0.49152106046676636
train gradient:  0.1510665845447764
iteration : 8814
train acc:  0.640625
train loss:  0.6712782382965088
train gradient:  0.2424175946484498
iteration : 8815
train acc:  0.7734375
train loss:  0.4498125910758972
train gradient:  0.13205956827230153
iteration : 8816
train acc:  0.6953125
train loss:  0.5803888440132141
train gradient:  0.174546795811796
iteration : 8817
train acc:  0.7109375
train loss:  0.49217918515205383
train gradient:  0.13132254647436173
iteration : 8818
train acc:  0.78125
train loss:  0.4655218720436096
train gradient:  0.13059093344482386
iteration : 8819
train acc:  0.7421875
train loss:  0.4687463939189911
train gradient:  0.12884130016999318
iteration : 8820
train acc:  0.75
train loss:  0.45735543966293335
train gradient:  0.12734726667172008
iteration : 8821
train acc:  0.6953125
train loss:  0.5390938520431519
train gradient:  0.14468879349867597
iteration : 8822
train acc:  0.7265625
train loss:  0.5237303972244263
train gradient:  0.15721245022978853
iteration : 8823
train acc:  0.75
train loss:  0.49808651208877563
train gradient:  0.10168907459227947
iteration : 8824
train acc:  0.703125
train loss:  0.4853476285934448
train gradient:  0.14839391630002516
iteration : 8825
train acc:  0.65625
train loss:  0.5381426811218262
train gradient:  0.1317055149036385
iteration : 8826
train acc:  0.7265625
train loss:  0.5070167779922485
train gradient:  0.13077512268010325
iteration : 8827
train acc:  0.8125
train loss:  0.4300398826599121
train gradient:  0.1121982414364227
iteration : 8828
train acc:  0.7265625
train loss:  0.5048305988311768
train gradient:  0.11885244753312751
iteration : 8829
train acc:  0.7421875
train loss:  0.4869791269302368
train gradient:  0.14847154336734159
iteration : 8830
train acc:  0.7421875
train loss:  0.53495192527771
train gradient:  0.14405023750101198
iteration : 8831
train acc:  0.75
train loss:  0.43879806995391846
train gradient:  0.0973620895040898
iteration : 8832
train acc:  0.6875
train loss:  0.5421269536018372
train gradient:  0.13567416014800723
iteration : 8833
train acc:  0.7421875
train loss:  0.4955301582813263
train gradient:  0.133106623595174
iteration : 8834
train acc:  0.703125
train loss:  0.5691945552825928
train gradient:  0.16304193322059346
iteration : 8835
train acc:  0.7109375
train loss:  0.5355374813079834
train gradient:  0.21618718270622148
iteration : 8836
train acc:  0.6640625
train loss:  0.5392197370529175
train gradient:  0.14611893539362647
iteration : 8837
train acc:  0.7421875
train loss:  0.46948114037513733
train gradient:  0.12537605873524443
iteration : 8838
train acc:  0.734375
train loss:  0.5381882190704346
train gradient:  0.14357950135411912
iteration : 8839
train acc:  0.7578125
train loss:  0.4694613516330719
train gradient:  0.12027674369995164
iteration : 8840
train acc:  0.7734375
train loss:  0.4619491994380951
train gradient:  0.10369180116303663
iteration : 8841
train acc:  0.7265625
train loss:  0.5013492107391357
train gradient:  0.12726832437479094
iteration : 8842
train acc:  0.75
train loss:  0.516892671585083
train gradient:  0.14962026346156226
iteration : 8843
train acc:  0.7578125
train loss:  0.4838460385799408
train gradient:  0.15879379267973753
iteration : 8844
train acc:  0.6875
train loss:  0.5394191741943359
train gradient:  0.14060008483093178
iteration : 8845
train acc:  0.8046875
train loss:  0.4126608073711395
train gradient:  0.10155251270615172
iteration : 8846
train acc:  0.765625
train loss:  0.535362958908081
train gradient:  0.12737139735348327
iteration : 8847
train acc:  0.7265625
train loss:  0.5387229919433594
train gradient:  0.16739899455480073
iteration : 8848
train acc:  0.765625
train loss:  0.44739481806755066
train gradient:  0.1028799616888598
iteration : 8849
train acc:  0.703125
train loss:  0.6198223233222961
train gradient:  0.18369296478538474
iteration : 8850
train acc:  0.7109375
train loss:  0.5784058570861816
train gradient:  0.17770033528006743
iteration : 8851
train acc:  0.703125
train loss:  0.5554131269454956
train gradient:  0.18842819166782315
iteration : 8852
train acc:  0.671875
train loss:  0.5714205503463745
train gradient:  0.21663402398719261
iteration : 8853
train acc:  0.71875
train loss:  0.46816718578338623
train gradient:  0.11594115578947103
iteration : 8854
train acc:  0.703125
train loss:  0.5656437277793884
train gradient:  0.180748145243881
iteration : 8855
train acc:  0.6953125
train loss:  0.5292025804519653
train gradient:  0.203596043009532
iteration : 8856
train acc:  0.7421875
train loss:  0.5111279487609863
train gradient:  0.13178967392499102
iteration : 8857
train acc:  0.75
train loss:  0.5166141390800476
train gradient:  0.15413243276329597
iteration : 8858
train acc:  0.78125
train loss:  0.4715975522994995
train gradient:  0.0946725895189945
iteration : 8859
train acc:  0.78125
train loss:  0.4745326638221741
train gradient:  0.10229878668609885
iteration : 8860
train acc:  0.765625
train loss:  0.4674903452396393
train gradient:  0.11162659682809487
iteration : 8861
train acc:  0.7734375
train loss:  0.4464845061302185
train gradient:  0.09710773958383544
iteration : 8862
train acc:  0.65625
train loss:  0.5490623712539673
train gradient:  0.14006792512797317
iteration : 8863
train acc:  0.7265625
train loss:  0.5154070854187012
train gradient:  0.12498517466463441
iteration : 8864
train acc:  0.7890625
train loss:  0.42012709379196167
train gradient:  0.08576405371037583
iteration : 8865
train acc:  0.671875
train loss:  0.5444859266281128
train gradient:  0.13794129443310665
iteration : 8866
train acc:  0.671875
train loss:  0.5324587821960449
train gradient:  0.0984461159851823
iteration : 8867
train acc:  0.7578125
train loss:  0.5438872575759888
train gradient:  0.14713104284399467
iteration : 8868
train acc:  0.7421875
train loss:  0.5007387399673462
train gradient:  0.14395294934680714
iteration : 8869
train acc:  0.7890625
train loss:  0.4348582625389099
train gradient:  0.09964090677944862
iteration : 8870
train acc:  0.6953125
train loss:  0.5505214929580688
train gradient:  0.15570930030733582
iteration : 8871
train acc:  0.734375
train loss:  0.4831819534301758
train gradient:  0.1270056114974315
iteration : 8872
train acc:  0.734375
train loss:  0.4882974922657013
train gradient:  0.1424811386481288
iteration : 8873
train acc:  0.7109375
train loss:  0.5772688388824463
train gradient:  0.1572585125389708
iteration : 8874
train acc:  0.765625
train loss:  0.4550994634628296
train gradient:  0.09722699594270666
iteration : 8875
train acc:  0.7734375
train loss:  0.4927982687950134
train gradient:  0.11839739420485958
iteration : 8876
train acc:  0.71875
train loss:  0.5054072141647339
train gradient:  0.12353219710180374
iteration : 8877
train acc:  0.7578125
train loss:  0.4557759761810303
train gradient:  0.08677441573772003
iteration : 8878
train acc:  0.734375
train loss:  0.4797740876674652
train gradient:  0.10830424710842941
iteration : 8879
train acc:  0.7265625
train loss:  0.4733530879020691
train gradient:  0.10995570522980923
iteration : 8880
train acc:  0.7734375
train loss:  0.5377922654151917
train gradient:  0.15369663396269978
iteration : 8881
train acc:  0.765625
train loss:  0.47436419129371643
train gradient:  0.1003255962648748
iteration : 8882
train acc:  0.734375
train loss:  0.47823041677474976
train gradient:  0.10457414991442147
iteration : 8883
train acc:  0.7578125
train loss:  0.5019273161888123
train gradient:  0.11919508179348681
iteration : 8884
train acc:  0.7109375
train loss:  0.5135316848754883
train gradient:  0.13855101940787418
iteration : 8885
train acc:  0.7109375
train loss:  0.5561628341674805
train gradient:  0.1494694357669195
iteration : 8886
train acc:  0.71875
train loss:  0.5145788192749023
train gradient:  0.1249749845658026
iteration : 8887
train acc:  0.7265625
train loss:  0.49449625611305237
train gradient:  0.1370257018789024
iteration : 8888
train acc:  0.75
train loss:  0.43806686997413635
train gradient:  0.12143347265286912
iteration : 8889
train acc:  0.734375
train loss:  0.48948466777801514
train gradient:  0.12051252447669507
iteration : 8890
train acc:  0.765625
train loss:  0.4686388373374939
train gradient:  0.11821216359399621
iteration : 8891
train acc:  0.6875
train loss:  0.5401884317398071
train gradient:  0.1630199921944863
iteration : 8892
train acc:  0.6953125
train loss:  0.5350291728973389
train gradient:  0.13120150288958593
iteration : 8893
train acc:  0.7421875
train loss:  0.4874049127101898
train gradient:  0.14993918919094276
iteration : 8894
train acc:  0.7265625
train loss:  0.4944312870502472
train gradient:  0.13123021890837633
iteration : 8895
train acc:  0.703125
train loss:  0.5346860885620117
train gradient:  0.1425063475909339
iteration : 8896
train acc:  0.75
train loss:  0.4495261311531067
train gradient:  0.10767493909689507
iteration : 8897
train acc:  0.7578125
train loss:  0.5365560054779053
train gradient:  0.2142098755090641
iteration : 8898
train acc:  0.7578125
train loss:  0.4558279812335968
train gradient:  0.09944962838975013
iteration : 8899
train acc:  0.7265625
train loss:  0.5620831847190857
train gradient:  0.16039951281845927
iteration : 8900
train acc:  0.765625
train loss:  0.47905677556991577
train gradient:  0.1482759192565436
iteration : 8901
train acc:  0.6953125
train loss:  0.5666853189468384
train gradient:  0.13766847981766556
iteration : 8902
train acc:  0.7734375
train loss:  0.46379780769348145
train gradient:  0.14770529232099386
iteration : 8903
train acc:  0.78125
train loss:  0.4533272385597229
train gradient:  0.08888635256471043
iteration : 8904
train acc:  0.7265625
train loss:  0.5359451770782471
train gradient:  0.13744883148368908
iteration : 8905
train acc:  0.765625
train loss:  0.45462214946746826
train gradient:  0.11430051850829236
iteration : 8906
train acc:  0.78125
train loss:  0.43284401297569275
train gradient:  0.10477202141458815
iteration : 8907
train acc:  0.6796875
train loss:  0.5507479906082153
train gradient:  0.15258697892829137
iteration : 8908
train acc:  0.84375
train loss:  0.39728572964668274
train gradient:  0.10683451202354184
iteration : 8909
train acc:  0.734375
train loss:  0.4730801582336426
train gradient:  0.11849868273463382
iteration : 8910
train acc:  0.71875
train loss:  0.5044998526573181
train gradient:  0.13437781762440673
iteration : 8911
train acc:  0.6640625
train loss:  0.6084951162338257
train gradient:  0.17817213672216925
iteration : 8912
train acc:  0.7578125
train loss:  0.4569961130619049
train gradient:  0.1302710210652916
iteration : 8913
train acc:  0.6953125
train loss:  0.5360987186431885
train gradient:  0.17283847789247153
iteration : 8914
train acc:  0.8203125
train loss:  0.42121243476867676
train gradient:  0.10844740195619723
iteration : 8915
train acc:  0.7734375
train loss:  0.4532790780067444
train gradient:  0.11187594491555249
iteration : 8916
train acc:  0.796875
train loss:  0.44079360365867615
train gradient:  0.10957677931403192
iteration : 8917
train acc:  0.703125
train loss:  0.5347981452941895
train gradient:  0.1623896145054801
iteration : 8918
train acc:  0.75
train loss:  0.4466167092323303
train gradient:  0.104544642054031
iteration : 8919
train acc:  0.7578125
train loss:  0.45930713415145874
train gradient:  0.1245982149550036
iteration : 8920
train acc:  0.7890625
train loss:  0.4455636143684387
train gradient:  0.10093151292368548
iteration : 8921
train acc:  0.75
train loss:  0.5060786008834839
train gradient:  0.11631190687154386
iteration : 8922
train acc:  0.78125
train loss:  0.452770859003067
train gradient:  0.09459818139410335
iteration : 8923
train acc:  0.734375
train loss:  0.5156051516532898
train gradient:  0.12093223082656222
iteration : 8924
train acc:  0.7734375
train loss:  0.471102237701416
train gradient:  0.09878374019925375
iteration : 8925
train acc:  0.75
train loss:  0.5058192014694214
train gradient:  0.17023198721149652
iteration : 8926
train acc:  0.765625
train loss:  0.4939367473125458
train gradient:  0.1287373128120476
iteration : 8927
train acc:  0.75
train loss:  0.5525405406951904
train gradient:  0.1320674661254959
iteration : 8928
train acc:  0.671875
train loss:  0.5773419141769409
train gradient:  0.1913398518930326
iteration : 8929
train acc:  0.7421875
train loss:  0.5001280307769775
train gradient:  0.14715617844828024
iteration : 8930
train acc:  0.7265625
train loss:  0.5311062335968018
train gradient:  0.1338990024360292
iteration : 8931
train acc:  0.6875
train loss:  0.5622928142547607
train gradient:  0.12770447294467355
iteration : 8932
train acc:  0.7421875
train loss:  0.5056891441345215
train gradient:  0.13903094773452887
iteration : 8933
train acc:  0.7734375
train loss:  0.43500885367393494
train gradient:  0.0939068146623498
iteration : 8934
train acc:  0.7734375
train loss:  0.4918842315673828
train gradient:  0.11079901036411104
iteration : 8935
train acc:  0.7421875
train loss:  0.4802432656288147
train gradient:  0.1375746989144463
iteration : 8936
train acc:  0.7265625
train loss:  0.49215415120124817
train gradient:  0.11522726001541322
iteration : 8937
train acc:  0.7421875
train loss:  0.5100623369216919
train gradient:  0.1308348819593671
iteration : 8938
train acc:  0.75
train loss:  0.4949813783168793
train gradient:  0.10947309822958713
iteration : 8939
train acc:  0.6328125
train loss:  0.6056507229804993
train gradient:  0.17615393670165164
iteration : 8940
train acc:  0.6875
train loss:  0.5405941009521484
train gradient:  0.13543234600920107
iteration : 8941
train acc:  0.8125
train loss:  0.42413878440856934
train gradient:  0.1265146032522902
iteration : 8942
train acc:  0.71875
train loss:  0.4818909466266632
train gradient:  0.11228918001646802
iteration : 8943
train acc:  0.78125
train loss:  0.43402767181396484
train gradient:  0.11892133821711935
iteration : 8944
train acc:  0.75
train loss:  0.4762765169143677
train gradient:  0.11312168290368663
iteration : 8945
train acc:  0.7578125
train loss:  0.5124019384384155
train gradient:  0.18492572227032322
iteration : 8946
train acc:  0.796875
train loss:  0.42749762535095215
train gradient:  0.10309577565988907
iteration : 8947
train acc:  0.7109375
train loss:  0.4676910638809204
train gradient:  0.10171645129234765
iteration : 8948
train acc:  0.7578125
train loss:  0.4501109719276428
train gradient:  0.12400757852862636
iteration : 8949
train acc:  0.84375
train loss:  0.43518102169036865
train gradient:  0.1185806130397506
iteration : 8950
train acc:  0.75
train loss:  0.5152633190155029
train gradient:  0.16097874266016732
iteration : 8951
train acc:  0.7890625
train loss:  0.4588196873664856
train gradient:  0.11667082327014597
iteration : 8952
train acc:  0.6796875
train loss:  0.5661277770996094
train gradient:  0.13899444674496803
iteration : 8953
train acc:  0.765625
train loss:  0.48610690236091614
train gradient:  0.15865573582541714
iteration : 8954
train acc:  0.6953125
train loss:  0.5197234153747559
train gradient:  0.11527865009388459
iteration : 8955
train acc:  0.703125
train loss:  0.5202285051345825
train gradient:  0.15307899836319383
iteration : 8956
train acc:  0.71875
train loss:  0.5241701602935791
train gradient:  0.15451215725440942
iteration : 8957
train acc:  0.734375
train loss:  0.5038719177246094
train gradient:  0.14602988983391266
iteration : 8958
train acc:  0.7578125
train loss:  0.4683266282081604
train gradient:  0.12587929125972794
iteration : 8959
train acc:  0.65625
train loss:  0.5926681756973267
train gradient:  0.1653412312614152
iteration : 8960
train acc:  0.7578125
train loss:  0.4645480513572693
train gradient:  0.09846918443408488
iteration : 8961
train acc:  0.75
train loss:  0.4567262530326843
train gradient:  0.12805497523546486
iteration : 8962
train acc:  0.75
train loss:  0.5067330598831177
train gradient:  0.18412079933662717
iteration : 8963
train acc:  0.8046875
train loss:  0.4337449073791504
train gradient:  0.09479017633575322
iteration : 8964
train acc:  0.703125
train loss:  0.5519315600395203
train gradient:  0.14637146125327513
iteration : 8965
train acc:  0.75
train loss:  0.4783250093460083
train gradient:  0.16598367491437047
iteration : 8966
train acc:  0.7421875
train loss:  0.5567229390144348
train gradient:  0.15146436610616246
iteration : 8967
train acc:  0.75
train loss:  0.49729204177856445
train gradient:  0.14042578633116015
iteration : 8968
train acc:  0.6796875
train loss:  0.5361428260803223
train gradient:  0.1404339929599489
iteration : 8969
train acc:  0.6953125
train loss:  0.5786304473876953
train gradient:  0.1806673848826562
iteration : 8970
train acc:  0.828125
train loss:  0.4531884789466858
train gradient:  0.0930052065750701
iteration : 8971
train acc:  0.6796875
train loss:  0.5802072882652283
train gradient:  0.1538287073648734
iteration : 8972
train acc:  0.7109375
train loss:  0.5090539455413818
train gradient:  0.12799655062248783
iteration : 8973
train acc:  0.75
train loss:  0.4449573755264282
train gradient:  0.09270309480432497
iteration : 8974
train acc:  0.7578125
train loss:  0.4903392791748047
train gradient:  0.11145166433802785
iteration : 8975
train acc:  0.7421875
train loss:  0.4692915081977844
train gradient:  0.13362414024778352
iteration : 8976
train acc:  0.7265625
train loss:  0.5223073959350586
train gradient:  0.15151151725066142
iteration : 8977
train acc:  0.796875
train loss:  0.42210209369659424
train gradient:  0.09496355750729692
iteration : 8978
train acc:  0.78125
train loss:  0.45247817039489746
train gradient:  0.106269719521449
iteration : 8979
train acc:  0.7734375
train loss:  0.49838653206825256
train gradient:  0.1254171391098958
iteration : 8980
train acc:  0.7421875
train loss:  0.4967431426048279
train gradient:  0.1403866773808452
iteration : 8981
train acc:  0.75
train loss:  0.4798103868961334
train gradient:  0.16649944122108462
iteration : 8982
train acc:  0.796875
train loss:  0.5002782940864563
train gradient:  0.11773885604453793
iteration : 8983
train acc:  0.765625
train loss:  0.4533938467502594
train gradient:  0.1634158251805855
iteration : 8984
train acc:  0.734375
train loss:  0.49010321497917175
train gradient:  0.12937724824094682
iteration : 8985
train acc:  0.7265625
train loss:  0.48239874839782715
train gradient:  0.1432836540649492
iteration : 8986
train acc:  0.7890625
train loss:  0.4478387236595154
train gradient:  0.10543254786412566
iteration : 8987
train acc:  0.7109375
train loss:  0.5466576814651489
train gradient:  0.13269750667930558
iteration : 8988
train acc:  0.734375
train loss:  0.5172543525695801
train gradient:  0.13979471545693561
iteration : 8989
train acc:  0.7265625
train loss:  0.5095525979995728
train gradient:  0.13409652066372435
iteration : 8990
train acc:  0.71875
train loss:  0.5271275043487549
train gradient:  0.1978383929358843
iteration : 8991
train acc:  0.7421875
train loss:  0.4563436210155487
train gradient:  0.12221826789462999
iteration : 8992
train acc:  0.78125
train loss:  0.4848114848136902
train gradient:  0.13986278293541465
iteration : 8993
train acc:  0.7109375
train loss:  0.5158606767654419
train gradient:  0.14048112262759288
iteration : 8994
train acc:  0.765625
train loss:  0.4633601903915405
train gradient:  0.1082266080920114
iteration : 8995
train acc:  0.75
train loss:  0.4715392589569092
train gradient:  0.10523511020707389
iteration : 8996
train acc:  0.7265625
train loss:  0.5208112001419067
train gradient:  0.1367728065089082
iteration : 8997
train acc:  0.7421875
train loss:  0.48160916566848755
train gradient:  0.1271535081187578
iteration : 8998
train acc:  0.7421875
train loss:  0.4781014323234558
train gradient:  0.16158285204273237
iteration : 8999
train acc:  0.7109375
train loss:  0.5692684650421143
train gradient:  0.17932728105073875
iteration : 9000
train acc:  0.8046875
train loss:  0.4430702030658722
train gradient:  0.08955367909513293
iteration : 9001
train acc:  0.71875
train loss:  0.4960409700870514
train gradient:  0.11057540318217052
iteration : 9002
train acc:  0.7109375
train loss:  0.5438374280929565
train gradient:  0.13104276705411394
iteration : 9003
train acc:  0.734375
train loss:  0.49251607060432434
train gradient:  0.13449944918769485
iteration : 9004
train acc:  0.7890625
train loss:  0.3931315541267395
train gradient:  0.08093935859473331
iteration : 9005
train acc:  0.7421875
train loss:  0.4912440776824951
train gradient:  0.11100614257170134
iteration : 9006
train acc:  0.7734375
train loss:  0.4922204613685608
train gradient:  0.115408214358529
iteration : 9007
train acc:  0.7578125
train loss:  0.46285581588745117
train gradient:  0.09581192220218078
iteration : 9008
train acc:  0.7421875
train loss:  0.4738261103630066
train gradient:  0.1306950545858395
iteration : 9009
train acc:  0.6640625
train loss:  0.5580386519432068
train gradient:  0.14543728884747975
iteration : 9010
train acc:  0.75
train loss:  0.47112759947776794
train gradient:  0.12106310608288322
iteration : 9011
train acc:  0.7265625
train loss:  0.49014750123023987
train gradient:  0.15295395676253842
iteration : 9012
train acc:  0.765625
train loss:  0.43140608072280884
train gradient:  0.10203882418954888
iteration : 9013
train acc:  0.8125
train loss:  0.44110962748527527
train gradient:  0.10044380999234981
iteration : 9014
train acc:  0.75
train loss:  0.5043176412582397
train gradient:  0.13906926145647353
iteration : 9015
train acc:  0.7578125
train loss:  0.4918566644191742
train gradient:  0.11011713952519653
iteration : 9016
train acc:  0.78125
train loss:  0.4418809413909912
train gradient:  0.09451927821973335
iteration : 9017
train acc:  0.7734375
train loss:  0.44618597626686096
train gradient:  0.11971897333718444
iteration : 9018
train acc:  0.7421875
train loss:  0.5282604098320007
train gradient:  0.15116850580522245
iteration : 9019
train acc:  0.75
train loss:  0.47195616364479065
train gradient:  0.11898865835706005
iteration : 9020
train acc:  0.7890625
train loss:  0.4209612309932709
train gradient:  0.10960572342648127
iteration : 9021
train acc:  0.78125
train loss:  0.4736452102661133
train gradient:  0.12890635519508037
iteration : 9022
train acc:  0.7734375
train loss:  0.46425148844718933
train gradient:  0.17892834169274197
iteration : 9023
train acc:  0.7890625
train loss:  0.42253682017326355
train gradient:  0.09944993951994752
iteration : 9024
train acc:  0.7734375
train loss:  0.46736255288124084
train gradient:  0.1086329057346765
iteration : 9025
train acc:  0.8046875
train loss:  0.4208531975746155
train gradient:  0.09142360255973132
iteration : 9026
train acc:  0.7421875
train loss:  0.4969746470451355
train gradient:  0.14247598490371555
iteration : 9027
train acc:  0.7421875
train loss:  0.520093560218811
train gradient:  0.1271018405073829
iteration : 9028
train acc:  0.7734375
train loss:  0.4500569701194763
train gradient:  0.12538805550976487
iteration : 9029
train acc:  0.7421875
train loss:  0.4603627920150757
train gradient:  0.10923111276391238
iteration : 9030
train acc:  0.6953125
train loss:  0.5456557869911194
train gradient:  0.1419445077646043
iteration : 9031
train acc:  0.75
train loss:  0.4669181704521179
train gradient:  0.130608705809849
iteration : 9032
train acc:  0.71875
train loss:  0.4858894348144531
train gradient:  0.12825476788725937
iteration : 9033
train acc:  0.71875
train loss:  0.5030955076217651
train gradient:  0.1537211435790395
iteration : 9034
train acc:  0.71875
train loss:  0.5205873847007751
train gradient:  0.12910557877429446
iteration : 9035
train acc:  0.75
train loss:  0.44551509618759155
train gradient:  0.09303704016859377
iteration : 9036
train acc:  0.7421875
train loss:  0.5239565372467041
train gradient:  0.1409617441235788
iteration : 9037
train acc:  0.6953125
train loss:  0.4926545023918152
train gradient:  0.1208207880220913
iteration : 9038
train acc:  0.703125
train loss:  0.5405048131942749
train gradient:  0.13429624302840648
iteration : 9039
train acc:  0.796875
train loss:  0.4348757266998291
train gradient:  0.09638635156406922
iteration : 9040
train acc:  0.796875
train loss:  0.47323018312454224
train gradient:  0.13265128144140448
iteration : 9041
train acc:  0.75
train loss:  0.4494791328907013
train gradient:  0.10719015141965225
iteration : 9042
train acc:  0.71875
train loss:  0.5186178684234619
train gradient:  0.12899493403945883
iteration : 9043
train acc:  0.7421875
train loss:  0.4848653972148895
train gradient:  0.1610005134227424
iteration : 9044
train acc:  0.7265625
train loss:  0.5030670762062073
train gradient:  0.10581577658907365
iteration : 9045
train acc:  0.78125
train loss:  0.4529014825820923
train gradient:  0.1245260741385902
iteration : 9046
train acc:  0.8125
train loss:  0.4308670163154602
train gradient:  0.10952165839687343
iteration : 9047
train acc:  0.703125
train loss:  0.5546483397483826
train gradient:  0.1585805317215574
iteration : 9048
train acc:  0.7421875
train loss:  0.5152077674865723
train gradient:  0.13947825418345916
iteration : 9049
train acc:  0.7578125
train loss:  0.4564402997493744
train gradient:  0.11738640963509245
iteration : 9050
train acc:  0.7109375
train loss:  0.5702409744262695
train gradient:  0.14763640287567265
iteration : 9051
train acc:  0.75
train loss:  0.4840119779109955
train gradient:  0.148300793011786
iteration : 9052
train acc:  0.7734375
train loss:  0.5004244446754456
train gradient:  0.11554925581741962
iteration : 9053
train acc:  0.75
train loss:  0.4550941586494446
train gradient:  0.09711743102364055
iteration : 9054
train acc:  0.765625
train loss:  0.4591917395591736
train gradient:  0.09674704402147241
iteration : 9055
train acc:  0.703125
train loss:  0.47790300846099854
train gradient:  0.10446914703672762
iteration : 9056
train acc:  0.7109375
train loss:  0.5375691056251526
train gradient:  0.14254277453738173
iteration : 9057
train acc:  0.6953125
train loss:  0.5661108493804932
train gradient:  0.16627511221952623
iteration : 9058
train acc:  0.75
train loss:  0.4862501323223114
train gradient:  0.11057420322632769
iteration : 9059
train acc:  0.78125
train loss:  0.4292025566101074
train gradient:  0.11546956288098639
iteration : 9060
train acc:  0.75
train loss:  0.4845622479915619
train gradient:  0.11413126497603203
iteration : 9061
train acc:  0.6875
train loss:  0.556067943572998
train gradient:  0.22342066782005252
iteration : 9062
train acc:  0.75
train loss:  0.4947170317173004
train gradient:  0.13323526205426728
iteration : 9063
train acc:  0.7109375
train loss:  0.48521506786346436
train gradient:  0.1303074552999844
iteration : 9064
train acc:  0.7421875
train loss:  0.4680224061012268
train gradient:  0.11590423473143775
iteration : 9065
train acc:  0.765625
train loss:  0.49461010098457336
train gradient:  0.12878747535730597
iteration : 9066
train acc:  0.71875
train loss:  0.5088282823562622
train gradient:  0.11460764861882566
iteration : 9067
train acc:  0.75
train loss:  0.4936368763446808
train gradient:  0.10076508104796063
iteration : 9068
train acc:  0.7421875
train loss:  0.49858641624450684
train gradient:  0.12236138830256621
iteration : 9069
train acc:  0.7734375
train loss:  0.4766871929168701
train gradient:  0.12593071781379703
iteration : 9070
train acc:  0.78125
train loss:  0.4331951439380646
train gradient:  0.09984524607205855
iteration : 9071
train acc:  0.7109375
train loss:  0.5130501389503479
train gradient:  0.1900937570785476
iteration : 9072
train acc:  0.71875
train loss:  0.4874829649925232
train gradient:  0.11257685257205882
iteration : 9073
train acc:  0.75
train loss:  0.4775409400463104
train gradient:  0.1371459888267315
iteration : 9074
train acc:  0.75
train loss:  0.4349782466888428
train gradient:  0.10953092972063762
iteration : 9075
train acc:  0.8046875
train loss:  0.3991726338863373
train gradient:  0.15104631020813314
iteration : 9076
train acc:  0.6875
train loss:  0.5321378707885742
train gradient:  0.14885447103738775
iteration : 9077
train acc:  0.71875
train loss:  0.47362419962882996
train gradient:  0.10131782977170689
iteration : 9078
train acc:  0.75
train loss:  0.49205702543258667
train gradient:  0.13785325834438444
iteration : 9079
train acc:  0.765625
train loss:  0.4297718107700348
train gradient:  0.14131457106470804
iteration : 9080
train acc:  0.703125
train loss:  0.5464391112327576
train gradient:  0.14341687828304078
iteration : 9081
train acc:  0.7734375
train loss:  0.48152658343315125
train gradient:  0.1269300599573742
iteration : 9082
train acc:  0.796875
train loss:  0.43997955322265625
train gradient:  0.11821900704331352
iteration : 9083
train acc:  0.7421875
train loss:  0.49642306566238403
train gradient:  0.13917875190959067
iteration : 9084
train acc:  0.6796875
train loss:  0.5114864706993103
train gradient:  0.13398080819043695
iteration : 9085
train acc:  0.734375
train loss:  0.4936072826385498
train gradient:  0.13889746106816903
iteration : 9086
train acc:  0.765625
train loss:  0.43977832794189453
train gradient:  0.08931337007463014
iteration : 9087
train acc:  0.765625
train loss:  0.4972800016403198
train gradient:  0.13996720486693465
iteration : 9088
train acc:  0.7734375
train loss:  0.4631612300872803
train gradient:  0.117914499862315
iteration : 9089
train acc:  0.7265625
train loss:  0.5067927241325378
train gradient:  0.2046229272732661
iteration : 9090
train acc:  0.71875
train loss:  0.45636576414108276
train gradient:  0.10014333677052585
iteration : 9091
train acc:  0.75
train loss:  0.48060911893844604
train gradient:  0.1362720803825122
iteration : 9092
train acc:  0.8125
train loss:  0.4533800780773163
train gradient:  0.13480572861900705
iteration : 9093
train acc:  0.7265625
train loss:  0.4827686548233032
train gradient:  0.11781126510706114
iteration : 9094
train acc:  0.6796875
train loss:  0.5513651371002197
train gradient:  0.12411171020545969
iteration : 9095
train acc:  0.6328125
train loss:  0.6064409017562866
train gradient:  0.19975137784230976
iteration : 9096
train acc:  0.71875
train loss:  0.5202394723892212
train gradient:  0.13248315044357192
iteration : 9097
train acc:  0.7578125
train loss:  0.512787938117981
train gradient:  0.11803222222247699
iteration : 9098
train acc:  0.703125
train loss:  0.6079487204551697
train gradient:  0.21896866127303516
iteration : 9099
train acc:  0.78125
train loss:  0.45470064878463745
train gradient:  0.13975568256806178
iteration : 9100
train acc:  0.7734375
train loss:  0.46838098764419556
train gradient:  0.09469329747516844
iteration : 9101
train acc:  0.671875
train loss:  0.5722603797912598
train gradient:  0.16222251996149661
iteration : 9102
train acc:  0.78125
train loss:  0.4757916331291199
train gradient:  0.13276152332858343
iteration : 9103
train acc:  0.765625
train loss:  0.4596070647239685
train gradient:  0.13745213394762024
iteration : 9104
train acc:  0.734375
train loss:  0.4912117123603821
train gradient:  0.13443719891026457
iteration : 9105
train acc:  0.734375
train loss:  0.48425382375717163
train gradient:  0.14089146466244884
iteration : 9106
train acc:  0.71875
train loss:  0.5127972364425659
train gradient:  0.14030672361848745
iteration : 9107
train acc:  0.734375
train loss:  0.49387460947036743
train gradient:  0.1379692727504233
iteration : 9108
train acc:  0.7265625
train loss:  0.5333817601203918
train gradient:  0.1738956004146192
iteration : 9109
train acc:  0.734375
train loss:  0.5080980062484741
train gradient:  0.13564700220308076
iteration : 9110
train acc:  0.7421875
train loss:  0.5240463614463806
train gradient:  0.12827999360207734
iteration : 9111
train acc:  0.796875
train loss:  0.44424575567245483
train gradient:  0.08996273213230005
iteration : 9112
train acc:  0.7734375
train loss:  0.4779548645019531
train gradient:  0.13295066109088738
iteration : 9113
train acc:  0.734375
train loss:  0.46156978607177734
train gradient:  0.15710601253433248
iteration : 9114
train acc:  0.7734375
train loss:  0.49388590455055237
train gradient:  0.15204938086479922
iteration : 9115
train acc:  0.7421875
train loss:  0.5156773328781128
train gradient:  0.11779917047026721
iteration : 9116
train acc:  0.7578125
train loss:  0.4788416922092438
train gradient:  0.1371260163320635
iteration : 9117
train acc:  0.7265625
train loss:  0.5043582916259766
train gradient:  0.10018112406389018
iteration : 9118
train acc:  0.8125
train loss:  0.4208969473838806
train gradient:  0.1073037119724363
iteration : 9119
train acc:  0.7109375
train loss:  0.5431901812553406
train gradient:  0.1515534929460085
iteration : 9120
train acc:  0.703125
train loss:  0.4992189407348633
train gradient:  0.1180286096630946
iteration : 9121
train acc:  0.703125
train loss:  0.47949257493019104
train gradient:  0.1256545477772631
iteration : 9122
train acc:  0.6953125
train loss:  0.5415392518043518
train gradient:  0.1747575816308281
iteration : 9123
train acc:  0.71875
train loss:  0.5667669773101807
train gradient:  0.16209938997851675
iteration : 9124
train acc:  0.6953125
train loss:  0.5261269807815552
train gradient:  0.14931634627556634
iteration : 9125
train acc:  0.78125
train loss:  0.45673808455467224
train gradient:  0.10324062790721543
iteration : 9126
train acc:  0.7734375
train loss:  0.43612730503082275
train gradient:  0.12027239467133988
iteration : 9127
train acc:  0.78125
train loss:  0.4861699938774109
train gradient:  0.11221773821781605
iteration : 9128
train acc:  0.6953125
train loss:  0.547394871711731
train gradient:  0.15128457821105062
iteration : 9129
train acc:  0.703125
train loss:  0.4898402690887451
train gradient:  0.16463166293879772
iteration : 9130
train acc:  0.6953125
train loss:  0.49644818902015686
train gradient:  0.15745504674282518
iteration : 9131
train acc:  0.828125
train loss:  0.38761016726493835
train gradient:  0.0890165956972622
iteration : 9132
train acc:  0.7421875
train loss:  0.4744715094566345
train gradient:  0.14927583414205656
iteration : 9133
train acc:  0.7421875
train loss:  0.5365875959396362
train gradient:  0.17079747199199358
iteration : 9134
train acc:  0.78125
train loss:  0.4745369255542755
train gradient:  0.12170851740446725
iteration : 9135
train acc:  0.7109375
train loss:  0.50347900390625
train gradient:  0.11309353297065404
iteration : 9136
train acc:  0.8046875
train loss:  0.4351727366447449
train gradient:  0.1059198474890604
iteration : 9137
train acc:  0.7109375
train loss:  0.4905303418636322
train gradient:  0.12802711700949054
iteration : 9138
train acc:  0.7734375
train loss:  0.5141309499740601
train gradient:  0.18797341815765378
iteration : 9139
train acc:  0.6953125
train loss:  0.508259654045105
train gradient:  0.14796450421789228
iteration : 9140
train acc:  0.7109375
train loss:  0.5153070092201233
train gradient:  0.17022942389867157
iteration : 9141
train acc:  0.765625
train loss:  0.5269895792007446
train gradient:  0.1328501982848278
iteration : 9142
train acc:  0.7109375
train loss:  0.5034481287002563
train gradient:  0.13879011246445896
iteration : 9143
train acc:  0.734375
train loss:  0.5469731688499451
train gradient:  0.1949012066432187
iteration : 9144
train acc:  0.765625
train loss:  0.4573745131492615
train gradient:  0.104686687546309
iteration : 9145
train acc:  0.6484375
train loss:  0.6044824123382568
train gradient:  0.17468280270241368
iteration : 9146
train acc:  0.765625
train loss:  0.4848646819591522
train gradient:  0.12411504409219827
iteration : 9147
train acc:  0.765625
train loss:  0.4528244733810425
train gradient:  0.1082962279112389
iteration : 9148
train acc:  0.6875
train loss:  0.5371599197387695
train gradient:  0.12981791989478195
iteration : 9149
train acc:  0.828125
train loss:  0.452283650636673
train gradient:  0.10230480986740287
iteration : 9150
train acc:  0.7265625
train loss:  0.49658656120300293
train gradient:  0.11201171239033234
iteration : 9151
train acc:  0.734375
train loss:  0.4910326600074768
train gradient:  0.16526278894464644
iteration : 9152
train acc:  0.734375
train loss:  0.5184937715530396
train gradient:  0.1771241761930859
iteration : 9153
train acc:  0.78125
train loss:  0.5016990900039673
train gradient:  0.10785002741490242
iteration : 9154
train acc:  0.7578125
train loss:  0.4492952227592468
train gradient:  0.09762417938681263
iteration : 9155
train acc:  0.6796875
train loss:  0.5379731059074402
train gradient:  0.1371634285593516
iteration : 9156
train acc:  0.7109375
train loss:  0.5097160935401917
train gradient:  0.12062027593651074
iteration : 9157
train acc:  0.78125
train loss:  0.4922795295715332
train gradient:  0.10994536272399875
iteration : 9158
train acc:  0.75
train loss:  0.48611751198768616
train gradient:  0.13439072067259988
iteration : 9159
train acc:  0.7421875
train loss:  0.47212791442871094
train gradient:  0.1158701206909743
iteration : 9160
train acc:  0.7578125
train loss:  0.4831828474998474
train gradient:  0.14588487231037256
iteration : 9161
train acc:  0.8125
train loss:  0.40310612320899963
train gradient:  0.09645002843231802
iteration : 9162
train acc:  0.7421875
train loss:  0.5148000717163086
train gradient:  0.12915718758992398
iteration : 9163
train acc:  0.7578125
train loss:  0.4962086081504822
train gradient:  0.13069617347378992
iteration : 9164
train acc:  0.6796875
train loss:  0.5267603993415833
train gradient:  0.12715289980964306
iteration : 9165
train acc:  0.703125
train loss:  0.5095238089561462
train gradient:  0.14618584572209936
iteration : 9166
train acc:  0.734375
train loss:  0.4835672080516815
train gradient:  0.14508152680096403
iteration : 9167
train acc:  0.6953125
train loss:  0.5248121023178101
train gradient:  0.16980819516124113
iteration : 9168
train acc:  0.7890625
train loss:  0.4862271547317505
train gradient:  0.13169225351491226
iteration : 9169
train acc:  0.765625
train loss:  0.4771259129047394
train gradient:  0.18205096077615435
iteration : 9170
train acc:  0.734375
train loss:  0.4887159764766693
train gradient:  0.12702563552329058
iteration : 9171
train acc:  0.703125
train loss:  0.5432236194610596
train gradient:  0.15678048460734773
iteration : 9172
train acc:  0.7265625
train loss:  0.4982137084007263
train gradient:  0.14979595399990175
iteration : 9173
train acc:  0.7421875
train loss:  0.4769551753997803
train gradient:  0.14823463746913754
iteration : 9174
train acc:  0.828125
train loss:  0.40172186493873596
train gradient:  0.09108048861262114
iteration : 9175
train acc:  0.8359375
train loss:  0.38301920890808105
train gradient:  0.08711697007692451
iteration : 9176
train acc:  0.765625
train loss:  0.4860014021396637
train gradient:  0.11494073455002174
iteration : 9177
train acc:  0.8203125
train loss:  0.4533900022506714
train gradient:  0.10460236143443721
iteration : 9178
train acc:  0.7890625
train loss:  0.4800758957862854
train gradient:  0.09582161333783813
iteration : 9179
train acc:  0.7734375
train loss:  0.40444421768188477
train gradient:  0.10402317645554769
iteration : 9180
train acc:  0.6953125
train loss:  0.5450555682182312
train gradient:  0.13515642932300076
iteration : 9181
train acc:  0.734375
train loss:  0.46863842010498047
train gradient:  0.09144331888526909
iteration : 9182
train acc:  0.6875
train loss:  0.5538351535797119
train gradient:  0.18783604499591888
iteration : 9183
train acc:  0.7109375
train loss:  0.5194402933120728
train gradient:  0.12227456053262743
iteration : 9184
train acc:  0.765625
train loss:  0.46858692169189453
train gradient:  0.1311220403001614
iteration : 9185
train acc:  0.75
train loss:  0.5031717419624329
train gradient:  0.1356584907195002
iteration : 9186
train acc:  0.765625
train loss:  0.4746650457382202
train gradient:  0.13844425598453533
iteration : 9187
train acc:  0.703125
train loss:  0.5164066553115845
train gradient:  0.14403434564262538
iteration : 9188
train acc:  0.7890625
train loss:  0.43847572803497314
train gradient:  0.09132268886184387
iteration : 9189
train acc:  0.6796875
train loss:  0.5792170763015747
train gradient:  0.15049942552065979
iteration : 9190
train acc:  0.7109375
train loss:  0.48676416277885437
train gradient:  0.11778920993849123
iteration : 9191
train acc:  0.765625
train loss:  0.4632207155227661
train gradient:  0.11627031315160084
iteration : 9192
train acc:  0.78125
train loss:  0.4746232330799103
train gradient:  0.1296393084290679
iteration : 9193
train acc:  0.71875
train loss:  0.5252750515937805
train gradient:  0.11953552986432385
iteration : 9194
train acc:  0.7421875
train loss:  0.4564162492752075
train gradient:  0.11049025728606669
iteration : 9195
train acc:  0.7578125
train loss:  0.4731545150279999
train gradient:  0.10717942708483771
iteration : 9196
train acc:  0.6875
train loss:  0.5616642236709595
train gradient:  0.14513029892778334
iteration : 9197
train acc:  0.765625
train loss:  0.47997140884399414
train gradient:  0.12014821219858626
iteration : 9198
train acc:  0.7578125
train loss:  0.4503793716430664
train gradient:  0.1370436232177883
iteration : 9199
train acc:  0.734375
train loss:  0.5268027782440186
train gradient:  0.13153792361687183
iteration : 9200
train acc:  0.75
train loss:  0.4773096740245819
train gradient:  0.1349158560055795
iteration : 9201
train acc:  0.7265625
train loss:  0.5654792785644531
train gradient:  0.13280012273504777
iteration : 9202
train acc:  0.7734375
train loss:  0.4608284831047058
train gradient:  0.12655193372511658
iteration : 9203
train acc:  0.734375
train loss:  0.4916855990886688
train gradient:  0.11046386158437656
iteration : 9204
train acc:  0.703125
train loss:  0.5108359456062317
train gradient:  0.13659260343998375
iteration : 9205
train acc:  0.734375
train loss:  0.5383705496788025
train gradient:  0.13237141112153067
iteration : 9206
train acc:  0.7421875
train loss:  0.4880606532096863
train gradient:  0.12583238280620346
iteration : 9207
train acc:  0.7734375
train loss:  0.45532500743865967
train gradient:  0.1162060173318634
iteration : 9208
train acc:  0.75
train loss:  0.4742198884487152
train gradient:  0.09709677709720767
iteration : 9209
train acc:  0.796875
train loss:  0.44235825538635254
train gradient:  0.11707692050378198
iteration : 9210
train acc:  0.734375
train loss:  0.4849781394004822
train gradient:  0.13117251710713235
iteration : 9211
train acc:  0.734375
train loss:  0.5646074414253235
train gradient:  0.15127632005584898
iteration : 9212
train acc:  0.78125
train loss:  0.4464600682258606
train gradient:  0.11578413201448418
iteration : 9213
train acc:  0.703125
train loss:  0.5203245878219604
train gradient:  0.16536920796668098
iteration : 9214
train acc:  0.71875
train loss:  0.49640631675720215
train gradient:  0.18473857062495502
iteration : 9215
train acc:  0.7578125
train loss:  0.44588300585746765
train gradient:  0.12108321382031476
iteration : 9216
train acc:  0.78125
train loss:  0.43448030948638916
train gradient:  0.07708353047586079
iteration : 9217
train acc:  0.6953125
train loss:  0.5314481854438782
train gradient:  0.1501127750966331
iteration : 9218
train acc:  0.78125
train loss:  0.465120792388916
train gradient:  0.11977278885783667
iteration : 9219
train acc:  0.6640625
train loss:  0.6005969643592834
train gradient:  0.1985657836861051
iteration : 9220
train acc:  0.71875
train loss:  0.48099154233932495
train gradient:  0.13433653911233728
iteration : 9221
train acc:  0.7109375
train loss:  0.578704297542572
train gradient:  0.21277113238944956
iteration : 9222
train acc:  0.75
train loss:  0.5301254391670227
train gradient:  0.17909806326867905
iteration : 9223
train acc:  0.7265625
train loss:  0.5050255060195923
train gradient:  0.14345183416005802
iteration : 9224
train acc:  0.7890625
train loss:  0.4798518717288971
train gradient:  0.10143229294312746
iteration : 9225
train acc:  0.734375
train loss:  0.49104252457618713
train gradient:  0.12203324693976411
iteration : 9226
train acc:  0.765625
train loss:  0.5060393810272217
train gradient:  0.1560219489482979
iteration : 9227
train acc:  0.7734375
train loss:  0.4817662835121155
train gradient:  0.11560583850279228
iteration : 9228
train acc:  0.734375
train loss:  0.5369609594345093
train gradient:  0.1492013982939348
iteration : 9229
train acc:  0.75
train loss:  0.514835774898529
train gradient:  0.13944709814007766
iteration : 9230
train acc:  0.6796875
train loss:  0.5484378337860107
train gradient:  0.17464559452511902
iteration : 9231
train acc:  0.765625
train loss:  0.4469001293182373
train gradient:  0.13028438337090203
iteration : 9232
train acc:  0.71875
train loss:  0.5457898378372192
train gradient:  0.218023794854471
iteration : 9233
train acc:  0.7421875
train loss:  0.5066418647766113
train gradient:  0.1450091129675948
iteration : 9234
train acc:  0.7421875
train loss:  0.5214947462081909
train gradient:  0.14090415191205363
iteration : 9235
train acc:  0.7421875
train loss:  0.548846960067749
train gradient:  0.12356026112123805
iteration : 9236
train acc:  0.7734375
train loss:  0.44233423471450806
train gradient:  0.1313028299038825
iteration : 9237
train acc:  0.7265625
train loss:  0.5283429026603699
train gradient:  0.15017611056288377
iteration : 9238
train acc:  0.734375
train loss:  0.5004569292068481
train gradient:  0.15197463829730165
iteration : 9239
train acc:  0.6875
train loss:  0.5339388847351074
train gradient:  0.1496857572490793
iteration : 9240
train acc:  0.75
train loss:  0.4843152165412903
train gradient:  0.11287391646716888
iteration : 9241
train acc:  0.6640625
train loss:  0.5826243162155151
train gradient:  0.2688993189683376
iteration : 9242
train acc:  0.78125
train loss:  0.47138625383377075
train gradient:  0.12077150110380853
iteration : 9243
train acc:  0.765625
train loss:  0.41986560821533203
train gradient:  0.10275065863052066
iteration : 9244
train acc:  0.7578125
train loss:  0.47413286566734314
train gradient:  0.1388458038041915
iteration : 9245
train acc:  0.78125
train loss:  0.4672183692455292
train gradient:  0.11744005613542702
iteration : 9246
train acc:  0.7109375
train loss:  0.5589263439178467
train gradient:  0.20494634961594996
iteration : 9247
train acc:  0.8125
train loss:  0.40795326232910156
train gradient:  0.09453976739269927
iteration : 9248
train acc:  0.7578125
train loss:  0.4951666593551636
train gradient:  0.14586340984026994
iteration : 9249
train acc:  0.7734375
train loss:  0.4590114653110504
train gradient:  0.10026983103415141
iteration : 9250
train acc:  0.78125
train loss:  0.4898703694343567
train gradient:  0.14439014911619125
iteration : 9251
train acc:  0.65625
train loss:  0.5408362150192261
train gradient:  0.1731937732713208
iteration : 9252
train acc:  0.71875
train loss:  0.5157713890075684
train gradient:  0.16376814619297575
iteration : 9253
train acc:  0.71875
train loss:  0.5043885111808777
train gradient:  0.13628872149298743
iteration : 9254
train acc:  0.6875
train loss:  0.5289304256439209
train gradient:  0.12713637959080584
iteration : 9255
train acc:  0.7578125
train loss:  0.45972615480422974
train gradient:  0.11487095546246517
iteration : 9256
train acc:  0.7578125
train loss:  0.48318129777908325
train gradient:  0.15019859641191785
iteration : 9257
train acc:  0.78125
train loss:  0.44049498438835144
train gradient:  0.11136181830781412
iteration : 9258
train acc:  0.71875
train loss:  0.4962770342826843
train gradient:  0.11856862037529807
iteration : 9259
train acc:  0.8125
train loss:  0.4019588828086853
train gradient:  0.10121894988355482
iteration : 9260
train acc:  0.734375
train loss:  0.4861080050468445
train gradient:  0.12059003295265565
iteration : 9261
train acc:  0.7890625
train loss:  0.4504867196083069
train gradient:  0.09980567561770236
iteration : 9262
train acc:  0.796875
train loss:  0.4594377875328064
train gradient:  0.12180483843023131
iteration : 9263
train acc:  0.6796875
train loss:  0.5058485269546509
train gradient:  0.1435470354130518
iteration : 9264
train acc:  0.765625
train loss:  0.4756905138492584
train gradient:  0.1726721013249891
iteration : 9265
train acc:  0.7890625
train loss:  0.47969502210617065
train gradient:  0.11116991056810228
iteration : 9266
train acc:  0.7265625
train loss:  0.48943233489990234
train gradient:  0.1511738301148346
iteration : 9267
train acc:  0.7265625
train loss:  0.5067332983016968
train gradient:  0.15313188039649434
iteration : 9268
train acc:  0.7421875
train loss:  0.5229179859161377
train gradient:  0.17191702767486677
iteration : 9269
train acc:  0.765625
train loss:  0.49688565731048584
train gradient:  0.12659437051623634
iteration : 9270
train acc:  0.7421875
train loss:  0.4807411730289459
train gradient:  0.11842147750126547
iteration : 9271
train acc:  0.7265625
train loss:  0.5017679929733276
train gradient:  0.147419978192636
iteration : 9272
train acc:  0.75
train loss:  0.5495020747184753
train gradient:  0.16316522882262144
iteration : 9273
train acc:  0.75
train loss:  0.43502992391586304
train gradient:  0.10515876482570673
iteration : 9274
train acc:  0.7734375
train loss:  0.4920765459537506
train gradient:  0.11734568952127243
iteration : 9275
train acc:  0.7265625
train loss:  0.5396661758422852
train gradient:  0.1739375868915024
iteration : 9276
train acc:  0.78125
train loss:  0.41665539145469666
train gradient:  0.08957587400630294
iteration : 9277
train acc:  0.7109375
train loss:  0.5342791080474854
train gradient:  0.15270265692956406
iteration : 9278
train acc:  0.703125
train loss:  0.5189005732536316
train gradient:  0.1423815642768368
iteration : 9279
train acc:  0.734375
train loss:  0.5024123191833496
train gradient:  0.11914753174543079
iteration : 9280
train acc:  0.6796875
train loss:  0.5617502927780151
train gradient:  0.15232553709942764
iteration : 9281
train acc:  0.71875
train loss:  0.5046321153640747
train gradient:  0.13746321140895484
iteration : 9282
train acc:  0.734375
train loss:  0.46987444162368774
train gradient:  0.1195134406487237
iteration : 9283
train acc:  0.75
train loss:  0.47507208585739136
train gradient:  0.1257154333584155
iteration : 9284
train acc:  0.7578125
train loss:  0.4559568762779236
train gradient:  0.10263038833274893
iteration : 9285
train acc:  0.7109375
train loss:  0.5485125184059143
train gradient:  0.14782675785760485
iteration : 9286
train acc:  0.703125
train loss:  0.6278069019317627
train gradient:  0.22897085951180216
iteration : 9287
train acc:  0.734375
train loss:  0.4573904871940613
train gradient:  0.11302096211441469
iteration : 9288
train acc:  0.765625
train loss:  0.46297889947891235
train gradient:  0.11760968960822031
iteration : 9289
train acc:  0.734375
train loss:  0.4974638521671295
train gradient:  0.14164813159218176
iteration : 9290
train acc:  0.7578125
train loss:  0.4749302864074707
train gradient:  0.14470718469413846
iteration : 9291
train acc:  0.7578125
train loss:  0.4497382938861847
train gradient:  0.09694382251700182
iteration : 9292
train acc:  0.765625
train loss:  0.48532071709632874
train gradient:  0.1375436407933474
iteration : 9293
train acc:  0.796875
train loss:  0.4098265767097473
train gradient:  0.09583421596898424
iteration : 9294
train acc:  0.7421875
train loss:  0.4759255647659302
train gradient:  0.1033589713422802
iteration : 9295
train acc:  0.796875
train loss:  0.4277692139148712
train gradient:  0.08868345279129304
iteration : 9296
train acc:  0.7578125
train loss:  0.43803584575653076
train gradient:  0.10337165454081713
iteration : 9297
train acc:  0.765625
train loss:  0.4395361840724945
train gradient:  0.10038835676545334
iteration : 9298
train acc:  0.71875
train loss:  0.5562969446182251
train gradient:  0.12892063071482548
iteration : 9299
train acc:  0.765625
train loss:  0.5082804560661316
train gradient:  0.15440373452606618
iteration : 9300
train acc:  0.765625
train loss:  0.5219045281410217
train gradient:  0.13804345599640616
iteration : 9301
train acc:  0.7421875
train loss:  0.5031116008758545
train gradient:  0.14065737179835547
iteration : 9302
train acc:  0.703125
train loss:  0.5876117944717407
train gradient:  0.23666434545605952
iteration : 9303
train acc:  0.7421875
train loss:  0.4888288378715515
train gradient:  0.1250875484666104
iteration : 9304
train acc:  0.7578125
train loss:  0.47532129287719727
train gradient:  0.09969368957532057
iteration : 9305
train acc:  0.78125
train loss:  0.43802356719970703
train gradient:  0.10124231495326032
iteration : 9306
train acc:  0.7109375
train loss:  0.5356044769287109
train gradient:  0.1590637133604661
iteration : 9307
train acc:  0.734375
train loss:  0.44356319308280945
train gradient:  0.08618487726762451
iteration : 9308
train acc:  0.8046875
train loss:  0.4314573109149933
train gradient:  0.09583805259315306
iteration : 9309
train acc:  0.765625
train loss:  0.5037111043930054
train gradient:  0.13445525299262012
iteration : 9310
train acc:  0.6796875
train loss:  0.5543615221977234
train gradient:  0.15705921379172055
iteration : 9311
train acc:  0.75
train loss:  0.4720735251903534
train gradient:  0.11603893390613389
iteration : 9312
train acc:  0.703125
train loss:  0.5124017000198364
train gradient:  0.12473701014713583
iteration : 9313
train acc:  0.7578125
train loss:  0.49168485403060913
train gradient:  0.10822379758429525
iteration : 9314
train acc:  0.7890625
train loss:  0.5201985836029053
train gradient:  0.13718745487557016
iteration : 9315
train acc:  0.75
train loss:  0.45674920082092285
train gradient:  0.09599069960363789
iteration : 9316
train acc:  0.7734375
train loss:  0.4725337326526642
train gradient:  0.12310861038861987
iteration : 9317
train acc:  0.7421875
train loss:  0.4437612295150757
train gradient:  0.10307659561199362
iteration : 9318
train acc:  0.6640625
train loss:  0.5522226095199585
train gradient:  0.17186256808931566
iteration : 9319
train acc:  0.7421875
train loss:  0.5455775260925293
train gradient:  0.12956324594273289
iteration : 9320
train acc:  0.7265625
train loss:  0.524362325668335
train gradient:  0.13133704738921537
iteration : 9321
train acc:  0.75
train loss:  0.4780585765838623
train gradient:  0.11227671471186368
iteration : 9322
train acc:  0.7265625
train loss:  0.5272674560546875
train gradient:  0.15263288009121473
iteration : 9323
train acc:  0.78125
train loss:  0.4770495295524597
train gradient:  0.1255424366941079
iteration : 9324
train acc:  0.6875
train loss:  0.5507650375366211
train gradient:  0.13883123659820504
iteration : 9325
train acc:  0.7890625
train loss:  0.3986905813217163
train gradient:  0.10031471486310478
iteration : 9326
train acc:  0.6953125
train loss:  0.5548458099365234
train gradient:  0.16635389085772712
iteration : 9327
train acc:  0.71875
train loss:  0.47869592905044556
train gradient:  0.11512146942160487
iteration : 9328
train acc:  0.7578125
train loss:  0.47998443245887756
train gradient:  0.11226343022540713
iteration : 9329
train acc:  0.6953125
train loss:  0.47303375601768494
train gradient:  0.11783264896894384
iteration : 9330
train acc:  0.7265625
train loss:  0.494493305683136
train gradient:  0.11128223188574117
iteration : 9331
train acc:  0.703125
train loss:  0.5472304224967957
train gradient:  0.16307698092562173
iteration : 9332
train acc:  0.8046875
train loss:  0.4517452120780945
train gradient:  0.09718784461678986
iteration : 9333
train acc:  0.71875
train loss:  0.5570697784423828
train gradient:  0.15728497448217887
iteration : 9334
train acc:  0.6875
train loss:  0.5470958948135376
train gradient:  0.16315329798394101
iteration : 9335
train acc:  0.7421875
train loss:  0.5377050638198853
train gradient:  0.1191078515671456
iteration : 9336
train acc:  0.7421875
train loss:  0.46184229850769043
train gradient:  0.10781735012719548
iteration : 9337
train acc:  0.71875
train loss:  0.5537406206130981
train gradient:  0.13123497917738308
iteration : 9338
train acc:  0.6875
train loss:  0.5362852215766907
train gradient:  0.16754452641788475
iteration : 9339
train acc:  0.765625
train loss:  0.4333766996860504
train gradient:  0.10711731688101392
iteration : 9340
train acc:  0.7265625
train loss:  0.5277611613273621
train gradient:  0.13934542636874236
iteration : 9341
train acc:  0.6796875
train loss:  0.5145891308784485
train gradient:  0.14047957652372917
iteration : 9342
train acc:  0.7421875
train loss:  0.5616215467453003
train gradient:  0.2002536712595145
iteration : 9343
train acc:  0.7578125
train loss:  0.4633462429046631
train gradient:  0.09361953119963416
iteration : 9344
train acc:  0.75
train loss:  0.5259081125259399
train gradient:  0.15709006426939437
iteration : 9345
train acc:  0.7421875
train loss:  0.5190242528915405
train gradient:  0.11809455883242218
iteration : 9346
train acc:  0.7890625
train loss:  0.4644795060157776
train gradient:  0.12887985979754882
iteration : 9347
train acc:  0.625
train loss:  0.6305975317955017
train gradient:  0.20968477049483036
iteration : 9348
train acc:  0.7265625
train loss:  0.47548019886016846
train gradient:  0.11920151061495862
iteration : 9349
train acc:  0.7265625
train loss:  0.5503606796264648
train gradient:  0.1398541765671066
iteration : 9350
train acc:  0.71875
train loss:  0.5479272603988647
train gradient:  0.18362784912338986
iteration : 9351
train acc:  0.7734375
train loss:  0.4449201226234436
train gradient:  0.07709353055116479
iteration : 9352
train acc:  0.7109375
train loss:  0.5401859283447266
train gradient:  0.12541964838305425
iteration : 9353
train acc:  0.7421875
train loss:  0.5185021162033081
train gradient:  0.15302879433767808
iteration : 9354
train acc:  0.7421875
train loss:  0.47467970848083496
train gradient:  0.10368235331422668
iteration : 9355
train acc:  0.6875
train loss:  0.506161630153656
train gradient:  0.10551381243937906
iteration : 9356
train acc:  0.6875
train loss:  0.5900786519050598
train gradient:  0.16225316415525237
iteration : 9357
train acc:  0.7890625
train loss:  0.46405771374702454
train gradient:  0.10357745663471149
iteration : 9358
train acc:  0.859375
train loss:  0.396825909614563
train gradient:  0.07842169296569482
iteration : 9359
train acc:  0.765625
train loss:  0.49847671389579773
train gradient:  0.11909234664659114
iteration : 9360
train acc:  0.765625
train loss:  0.5074265003204346
train gradient:  0.15078660402470156
iteration : 9361
train acc:  0.734375
train loss:  0.5046321749687195
train gradient:  0.14689616305051467
iteration : 9362
train acc:  0.765625
train loss:  0.43837621808052063
train gradient:  0.09832200581098975
iteration : 9363
train acc:  0.7265625
train loss:  0.4988689720630646
train gradient:  0.10256808556199065
iteration : 9364
train acc:  0.75
train loss:  0.4771251082420349
train gradient:  0.12540304795298174
iteration : 9365
train acc:  0.765625
train loss:  0.4994991421699524
train gradient:  0.12252022309234287
iteration : 9366
train acc:  0.75
train loss:  0.46286648511886597
train gradient:  0.10812514629953734
iteration : 9367
train acc:  0.6796875
train loss:  0.5284579992294312
train gradient:  0.12328162029031985
iteration : 9368
train acc:  0.6953125
train loss:  0.5558474659919739
train gradient:  0.21851195791541567
iteration : 9369
train acc:  0.734375
train loss:  0.5272411108016968
train gradient:  0.14937867068486216
iteration : 9370
train acc:  0.75
train loss:  0.4649121165275574
train gradient:  0.11266692739172837
iteration : 9371
train acc:  0.796875
train loss:  0.46063432097435
train gradient:  0.11328157591536993
iteration : 9372
train acc:  0.71875
train loss:  0.517531156539917
train gradient:  0.13332958921751398
iteration : 9373
train acc:  0.7734375
train loss:  0.5216699838638306
train gradient:  0.16246281939097332
iteration : 9374
train acc:  0.75
train loss:  0.4911072552204132
train gradient:  0.10945798530441152
iteration : 9375
train acc:  0.703125
train loss:  0.5343841314315796
train gradient:  0.13403073433374132
iteration : 9376
train acc:  0.7421875
train loss:  0.47016793489456177
train gradient:  0.10935116307208505
iteration : 9377
train acc:  0.7578125
train loss:  0.5210510492324829
train gradient:  0.14112306905861255
iteration : 9378
train acc:  0.8203125
train loss:  0.43606406450271606
train gradient:  0.11015242677441286
iteration : 9379
train acc:  0.8125
train loss:  0.44420182704925537
train gradient:  0.13076746751392498
iteration : 9380
train acc:  0.7109375
train loss:  0.48411017656326294
train gradient:  0.1229113198558076
iteration : 9381
train acc:  0.71875
train loss:  0.49565818905830383
train gradient:  0.137297730119876
iteration : 9382
train acc:  0.8046875
train loss:  0.41402435302734375
train gradient:  0.10617613877900571
iteration : 9383
train acc:  0.65625
train loss:  0.5113252401351929
train gradient:  0.10441279400204467
iteration : 9384
train acc:  0.7421875
train loss:  0.5013808012008667
train gradient:  0.10855959156293568
iteration : 9385
train acc:  0.7265625
train loss:  0.5070140957832336
train gradient:  0.14487605019944017
iteration : 9386
train acc:  0.7421875
train loss:  0.5312803387641907
train gradient:  0.12102381285111037
iteration : 9387
train acc:  0.734375
train loss:  0.49306219816207886
train gradient:  0.11972448681267982
iteration : 9388
train acc:  0.7421875
train loss:  0.4919999837875366
train gradient:  0.13359818387501304
iteration : 9389
train acc:  0.8359375
train loss:  0.37757986783981323
train gradient:  0.10262183256281643
iteration : 9390
train acc:  0.7421875
train loss:  0.5432902574539185
train gradient:  0.18273143356522448
iteration : 9391
train acc:  0.7421875
train loss:  0.50480055809021
train gradient:  0.1593048004246394
iteration : 9392
train acc:  0.671875
train loss:  0.5184851884841919
train gradient:  0.12977835235209567
iteration : 9393
train acc:  0.734375
train loss:  0.5232151746749878
train gradient:  0.1487031160683684
iteration : 9394
train acc:  0.6875
train loss:  0.576561689376831
train gradient:  0.13209171552259535
iteration : 9395
train acc:  0.796875
train loss:  0.4651122987270355
train gradient:  0.10562419093133747
iteration : 9396
train acc:  0.7578125
train loss:  0.4846987724304199
train gradient:  0.10455115458286801
iteration : 9397
train acc:  0.7578125
train loss:  0.4678169786930084
train gradient:  0.12671679160888838
iteration : 9398
train acc:  0.7578125
train loss:  0.5172753930091858
train gradient:  0.1613005553983291
iteration : 9399
train acc:  0.7734375
train loss:  0.46366947889328003
train gradient:  0.13179500193796997
iteration : 9400
train acc:  0.71875
train loss:  0.5349428653717041
train gradient:  0.12589593290814066
iteration : 9401
train acc:  0.765625
train loss:  0.4602165222167969
train gradient:  0.13787201393217585
iteration : 9402
train acc:  0.71875
train loss:  0.5342279672622681
train gradient:  0.15670083359050369
iteration : 9403
train acc:  0.7890625
train loss:  0.4192644953727722
train gradient:  0.1054891416728788
iteration : 9404
train acc:  0.734375
train loss:  0.4843890368938446
train gradient:  0.11842666060514224
iteration : 9405
train acc:  0.796875
train loss:  0.47024238109588623
train gradient:  0.12530798859645992
iteration : 9406
train acc:  0.75
train loss:  0.5276249647140503
train gradient:  0.16054727048876188
iteration : 9407
train acc:  0.7109375
train loss:  0.5344679951667786
train gradient:  0.13538572727600867
iteration : 9408
train acc:  0.6796875
train loss:  0.5186740756034851
train gradient:  0.1250164207286531
iteration : 9409
train acc:  0.765625
train loss:  0.47484534978866577
train gradient:  0.096616641423162
iteration : 9410
train acc:  0.7421875
train loss:  0.4876139760017395
train gradient:  0.13534411941886645
iteration : 9411
train acc:  0.7109375
train loss:  0.5634394884109497
train gradient:  0.23683671960013175
iteration : 9412
train acc:  0.7578125
train loss:  0.5038176774978638
train gradient:  0.12987621367150842
iteration : 9413
train acc:  0.75
train loss:  0.4671775698661804
train gradient:  0.1314412984672056
iteration : 9414
train acc:  0.7890625
train loss:  0.45941582322120667
train gradient:  0.10743087044996884
iteration : 9415
train acc:  0.78125
train loss:  0.4588231146335602
train gradient:  0.10374708311845808
iteration : 9416
train acc:  0.6875
train loss:  0.5339679718017578
train gradient:  0.12996766682599314
iteration : 9417
train acc:  0.6953125
train loss:  0.597724437713623
train gradient:  0.17264803468212442
iteration : 9418
train acc:  0.765625
train loss:  0.4436423182487488
train gradient:  0.09083382566167374
iteration : 9419
train acc:  0.640625
train loss:  0.5864503383636475
train gradient:  0.191140466494094
iteration : 9420
train acc:  0.703125
train loss:  0.5445994138717651
train gradient:  0.14538367846863737
iteration : 9421
train acc:  0.8359375
train loss:  0.44005483388900757
train gradient:  0.11262265287584135
iteration : 9422
train acc:  0.7265625
train loss:  0.5296070575714111
train gradient:  0.14988831682262005
iteration : 9423
train acc:  0.6953125
train loss:  0.568584680557251
train gradient:  0.15457519003785564
iteration : 9424
train acc:  0.734375
train loss:  0.5009278655052185
train gradient:  0.1410045305151296
iteration : 9425
train acc:  0.796875
train loss:  0.4169027507305145
train gradient:  0.10241286171979787
iteration : 9426
train acc:  0.7578125
train loss:  0.4998480975627899
train gradient:  0.10720372625973934
iteration : 9427
train acc:  0.8203125
train loss:  0.42062294483184814
train gradient:  0.09540801468829438
iteration : 9428
train acc:  0.7421875
train loss:  0.5234142541885376
train gradient:  0.14077377023562532
iteration : 9429
train acc:  0.7890625
train loss:  0.4688635766506195
train gradient:  0.10221176812094798
iteration : 9430
train acc:  0.765625
train loss:  0.51075679063797
train gradient:  0.13938824617022644
iteration : 9431
train acc:  0.734375
train loss:  0.5083919763565063
train gradient:  0.1182028840970192
iteration : 9432
train acc:  0.7421875
train loss:  0.5222045183181763
train gradient:  0.1580132208304193
iteration : 9433
train acc:  0.7421875
train loss:  0.5002309083938599
train gradient:  0.11827290265207731
iteration : 9434
train acc:  0.6640625
train loss:  0.5252331495285034
train gradient:  0.15521361621811547
iteration : 9435
train acc:  0.796875
train loss:  0.426729291677475
train gradient:  0.0922637718042008
iteration : 9436
train acc:  0.75
train loss:  0.47654324769973755
train gradient:  0.13164245390319207
iteration : 9437
train acc:  0.765625
train loss:  0.506589949131012
train gradient:  0.11613092709135885
iteration : 9438
train acc:  0.828125
train loss:  0.44792312383651733
train gradient:  0.10095944220204037
iteration : 9439
train acc:  0.7109375
train loss:  0.5568240880966187
train gradient:  0.13040175340726934
iteration : 9440
train acc:  0.703125
train loss:  0.5407743453979492
train gradient:  0.15830466284746958
iteration : 9441
train acc:  0.734375
train loss:  0.5041966438293457
train gradient:  0.1442682515931087
iteration : 9442
train acc:  0.75
train loss:  0.5221905708312988
train gradient:  0.12772471104327696
iteration : 9443
train acc:  0.734375
train loss:  0.4829180836677551
train gradient:  0.11096887050495696
iteration : 9444
train acc:  0.71875
train loss:  0.508663535118103
train gradient:  0.11652096318821377
iteration : 9445
train acc:  0.703125
train loss:  0.4896160662174225
train gradient:  0.11897998745941796
iteration : 9446
train acc:  0.7265625
train loss:  0.5074597597122192
train gradient:  0.12414450289132294
iteration : 9447
train acc:  0.7421875
train loss:  0.487332284450531
train gradient:  0.1149068955676453
iteration : 9448
train acc:  0.7734375
train loss:  0.4199811816215515
train gradient:  0.10501821363586843
iteration : 9449
train acc:  0.6875
train loss:  0.5522289276123047
train gradient:  0.18238808937308665
iteration : 9450
train acc:  0.8046875
train loss:  0.409737229347229
train gradient:  0.10012261524612494
iteration : 9451
train acc:  0.78125
train loss:  0.49683594703674316
train gradient:  0.12043006303486761
iteration : 9452
train acc:  0.71875
train loss:  0.5010039806365967
train gradient:  0.15377212794552358
iteration : 9453
train acc:  0.7109375
train loss:  0.4960630536079407
train gradient:  0.1436805109693987
iteration : 9454
train acc:  0.765625
train loss:  0.5000574588775635
train gradient:  0.1151217940391187
iteration : 9455
train acc:  0.7890625
train loss:  0.471803218126297
train gradient:  0.11658874250592266
iteration : 9456
train acc:  0.7265625
train loss:  0.585472822189331
train gradient:  0.16532069580674819
iteration : 9457
train acc:  0.7109375
train loss:  0.5140005946159363
train gradient:  0.14868717887854477
iteration : 9458
train acc:  0.734375
train loss:  0.5044439435005188
train gradient:  0.14503512411782943
iteration : 9459
train acc:  0.6484375
train loss:  0.5790528059005737
train gradient:  0.17644799243919912
iteration : 9460
train acc:  0.7421875
train loss:  0.49088695645332336
train gradient:  0.16626814884159874
iteration : 9461
train acc:  0.796875
train loss:  0.496989369392395
train gradient:  0.1357915814685035
iteration : 9462
train acc:  0.7265625
train loss:  0.4810835123062134
train gradient:  0.16464388541323688
iteration : 9463
train acc:  0.7578125
train loss:  0.4738786518573761
train gradient:  0.11138792622721792
iteration : 9464
train acc:  0.7265625
train loss:  0.4661529064178467
train gradient:  0.11598994759756907
iteration : 9465
train acc:  0.8046875
train loss:  0.462830126285553
train gradient:  0.11855898410985347
iteration : 9466
train acc:  0.703125
train loss:  0.5565943121910095
train gradient:  0.1310045828675302
iteration : 9467
train acc:  0.78125
train loss:  0.465110719203949
train gradient:  0.12822764986487095
iteration : 9468
train acc:  0.78125
train loss:  0.4159958064556122
train gradient:  0.08417499541556966
iteration : 9469
train acc:  0.6796875
train loss:  0.5228958129882812
train gradient:  0.15011493236787954
iteration : 9470
train acc:  0.796875
train loss:  0.4646332561969757
train gradient:  0.11276340872153053
iteration : 9471
train acc:  0.6953125
train loss:  0.5525053143501282
train gradient:  0.18894720814929655
iteration : 9472
train acc:  0.6953125
train loss:  0.5524892210960388
train gradient:  0.15093261433820826
iteration : 9473
train acc:  0.7890625
train loss:  0.4032217264175415
train gradient:  0.10839990465335636
iteration : 9474
train acc:  0.8125
train loss:  0.433674693107605
train gradient:  0.10965926615163206
iteration : 9475
train acc:  0.7890625
train loss:  0.454370379447937
train gradient:  0.11250609909581143
iteration : 9476
train acc:  0.7734375
train loss:  0.4175757169723511
train gradient:  0.12599795136476882
iteration : 9477
train acc:  0.7734375
train loss:  0.5157715082168579
train gradient:  0.16527686759794194
iteration : 9478
train acc:  0.7109375
train loss:  0.4593096375465393
train gradient:  0.10657433025422006
iteration : 9479
train acc:  0.78125
train loss:  0.4564782679080963
train gradient:  0.1243958919672957
iteration : 9480
train acc:  0.71875
train loss:  0.5681254267692566
train gradient:  0.15969679099574818
iteration : 9481
train acc:  0.7734375
train loss:  0.44209012389183044
train gradient:  0.13611131359196696
iteration : 9482
train acc:  0.7890625
train loss:  0.4173922538757324
train gradient:  0.09168179682517302
iteration : 9483
train acc:  0.71875
train loss:  0.5600569248199463
train gradient:  0.15150865376771933
iteration : 9484
train acc:  0.7421875
train loss:  0.49373191595077515
train gradient:  0.13714857909621647
iteration : 9485
train acc:  0.7421875
train loss:  0.4680633246898651
train gradient:  0.12097317950949964
iteration : 9486
train acc:  0.796875
train loss:  0.42456501722335815
train gradient:  0.10284984917702222
iteration : 9487
train acc:  0.71875
train loss:  0.5019705891609192
train gradient:  0.13869596555512326
iteration : 9488
train acc:  0.703125
train loss:  0.5126780867576599
train gradient:  0.12318728625685968
iteration : 9489
train acc:  0.75
train loss:  0.5147614479064941
train gradient:  0.11887649767353853
iteration : 9490
train acc:  0.703125
train loss:  0.5287979245185852
train gradient:  0.14733503174305607
iteration : 9491
train acc:  0.6875
train loss:  0.5026346445083618
train gradient:  0.11409298088928613
iteration : 9492
train acc:  0.75
train loss:  0.5287027359008789
train gradient:  0.18822367494868736
iteration : 9493
train acc:  0.734375
train loss:  0.4945295751094818
train gradient:  0.12346706525087166
iteration : 9494
train acc:  0.6640625
train loss:  0.5931192636489868
train gradient:  0.1667046934868028
iteration : 9495
train acc:  0.734375
train loss:  0.5026285648345947
train gradient:  0.12562273735268648
iteration : 9496
train acc:  0.703125
train loss:  0.5335818529129028
train gradient:  0.15046259977140194
iteration : 9497
train acc:  0.703125
train loss:  0.5163665413856506
train gradient:  0.1472732212284254
iteration : 9498
train acc:  0.703125
train loss:  0.5176745653152466
train gradient:  0.12292961524498663
iteration : 9499
train acc:  0.78125
train loss:  0.4645533561706543
train gradient:  0.1956624043795535
iteration : 9500
train acc:  0.734375
train loss:  0.5116472840309143
train gradient:  0.13684919937616874
iteration : 9501
train acc:  0.6953125
train loss:  0.5601857900619507
train gradient:  0.15497363626409
iteration : 9502
train acc:  0.75
train loss:  0.4792539179325104
train gradient:  0.12475197111464174
iteration : 9503
train acc:  0.6875
train loss:  0.5564466714859009
train gradient:  0.18111020611669618
iteration : 9504
train acc:  0.8046875
train loss:  0.42228028178215027
train gradient:  0.09546853065967342
iteration : 9505
train acc:  0.71875
train loss:  0.5350192785263062
train gradient:  0.1385571558700649
iteration : 9506
train acc:  0.71875
train loss:  0.5099046230316162
train gradient:  0.14247475111358132
iteration : 9507
train acc:  0.65625
train loss:  0.5653961896896362
train gradient:  0.1794055122722465
iteration : 9508
train acc:  0.75
train loss:  0.4799962043762207
train gradient:  0.10108126248856064
iteration : 9509
train acc:  0.7578125
train loss:  0.4937337040901184
train gradient:  0.1265314730850671
iteration : 9510
train acc:  0.7734375
train loss:  0.46340492367744446
train gradient:  0.1023396551389316
iteration : 9511
train acc:  0.7578125
train loss:  0.4839213788509369
train gradient:  0.12323380234462919
iteration : 9512
train acc:  0.7578125
train loss:  0.4467315971851349
train gradient:  0.1215772269047155
iteration : 9513
train acc:  0.7578125
train loss:  0.4873398542404175
train gradient:  0.1606920544450015
iteration : 9514
train acc:  0.734375
train loss:  0.5012062788009644
train gradient:  0.12295282586480953
iteration : 9515
train acc:  0.7265625
train loss:  0.5282914042472839
train gradient:  0.13889354835726253
iteration : 9516
train acc:  0.75
train loss:  0.44670215249061584
train gradient:  0.101163189257304
iteration : 9517
train acc:  0.7578125
train loss:  0.46081238985061646
train gradient:  0.12572315232400333
iteration : 9518
train acc:  0.7890625
train loss:  0.48077136278152466
train gradient:  0.13440758443890785
iteration : 9519
train acc:  0.7890625
train loss:  0.48848986625671387
train gradient:  0.12164391997897647
iteration : 9520
train acc:  0.6875
train loss:  0.56773442029953
train gradient:  0.14297019516188367
iteration : 9521
train acc:  0.65625
train loss:  0.665505051612854
train gradient:  0.253025464618753
iteration : 9522
train acc:  0.734375
train loss:  0.4747556746006012
train gradient:  0.09192123914133182
iteration : 9523
train acc:  0.6953125
train loss:  0.5673280954360962
train gradient:  0.13839246546568112
iteration : 9524
train acc:  0.7109375
train loss:  0.5067740082740784
train gradient:  0.11071140561466324
iteration : 9525
train acc:  0.78125
train loss:  0.4970577359199524
train gradient:  0.12007680376809218
iteration : 9526
train acc:  0.71875
train loss:  0.5342357158660889
train gradient:  0.13262636734736832
iteration : 9527
train acc:  0.734375
train loss:  0.4917376637458801
train gradient:  0.12952532838683567
iteration : 9528
train acc:  0.7734375
train loss:  0.4368932247161865
train gradient:  0.09009321454028728
iteration : 9529
train acc:  0.765625
train loss:  0.4460783302783966
train gradient:  0.12966001058115084
iteration : 9530
train acc:  0.7734375
train loss:  0.4774435758590698
train gradient:  0.10979493500728509
iteration : 9531
train acc:  0.765625
train loss:  0.4767119288444519
train gradient:  0.10724893253596071
iteration : 9532
train acc:  0.75
train loss:  0.526744544506073
train gradient:  0.1279893378225263
iteration : 9533
train acc:  0.7578125
train loss:  0.4923337996006012
train gradient:  0.11048481076128142
iteration : 9534
train acc:  0.6953125
train loss:  0.5645332932472229
train gradient:  0.1379156506036138
iteration : 9535
train acc:  0.6953125
train loss:  0.5020250082015991
train gradient:  0.12216152853981439
iteration : 9536
train acc:  0.7421875
train loss:  0.48430371284484863
train gradient:  0.12873796832456028
iteration : 9537
train acc:  0.7265625
train loss:  0.5224957466125488
train gradient:  0.12721900011599974
iteration : 9538
train acc:  0.71875
train loss:  0.5058492422103882
train gradient:  0.10393257194359576
iteration : 9539
train acc:  0.703125
train loss:  0.5026553869247437
train gradient:  0.12203755396848177
iteration : 9540
train acc:  0.765625
train loss:  0.4484778642654419
train gradient:  0.11903028674626247
iteration : 9541
train acc:  0.7734375
train loss:  0.4092518091201782
train gradient:  0.09748556856770718
iteration : 9542
train acc:  0.6796875
train loss:  0.4865644574165344
train gradient:  0.11146941906150151
iteration : 9543
train acc:  0.796875
train loss:  0.4505313038825989
train gradient:  0.10012560059697681
iteration : 9544
train acc:  0.71875
train loss:  0.49143505096435547
train gradient:  0.12776381164570552
iteration : 9545
train acc:  0.7265625
train loss:  0.5199630260467529
train gradient:  0.12562516739523968
iteration : 9546
train acc:  0.7734375
train loss:  0.538171112537384
train gradient:  0.13528947456740031
iteration : 9547
train acc:  0.6796875
train loss:  0.49401989579200745
train gradient:  0.12020234531375955
iteration : 9548
train acc:  0.7421875
train loss:  0.5171834230422974
train gradient:  0.19025233585705853
iteration : 9549
train acc:  0.7265625
train loss:  0.5131626129150391
train gradient:  0.13134422742476815
iteration : 9550
train acc:  0.703125
train loss:  0.5451064109802246
train gradient:  0.14534581435898614
iteration : 9551
train acc:  0.7421875
train loss:  0.4426785707473755
train gradient:  0.12414329864713328
iteration : 9552
train acc:  0.75
train loss:  0.5379576683044434
train gradient:  0.14610343900410339
iteration : 9553
train acc:  0.8515625
train loss:  0.3675222098827362
train gradient:  0.09537441907016037
iteration : 9554
train acc:  0.7265625
train loss:  0.5217114686965942
train gradient:  0.12090770774475655
iteration : 9555
train acc:  0.7265625
train loss:  0.5079084038734436
train gradient:  0.11647195136304965
iteration : 9556
train acc:  0.71875
train loss:  0.5863973498344421
train gradient:  0.16738924361582674
iteration : 9557
train acc:  0.7578125
train loss:  0.4840976595878601
train gradient:  0.10581211421229371
iteration : 9558
train acc:  0.671875
train loss:  0.5601270198822021
train gradient:  0.14809157010788015
iteration : 9559
train acc:  0.765625
train loss:  0.511171817779541
train gradient:  0.14430831090905383
iteration : 9560
train acc:  0.8046875
train loss:  0.497774600982666
train gradient:  0.1290473357118078
iteration : 9561
train acc:  0.6875
train loss:  0.5648747086524963
train gradient:  0.13017124304813954
iteration : 9562
train acc:  0.7890625
train loss:  0.49061012268066406
train gradient:  0.15221138073233
iteration : 9563
train acc:  0.6875
train loss:  0.6177490949630737
train gradient:  0.2152119203927994
iteration : 9564
train acc:  0.6953125
train loss:  0.5156092643737793
train gradient:  0.12215944401761837
iteration : 9565
train acc:  0.7578125
train loss:  0.4652100205421448
train gradient:  0.12981308351754223
iteration : 9566
train acc:  0.6875
train loss:  0.5993585586547852
train gradient:  0.2051830816126317
iteration : 9567
train acc:  0.7265625
train loss:  0.4818669259548187
train gradient:  0.10208488608486975
iteration : 9568
train acc:  0.734375
train loss:  0.510949969291687
train gradient:  0.14149443333341666
iteration : 9569
train acc:  0.765625
train loss:  0.4640752375125885
train gradient:  0.15441840644908617
iteration : 9570
train acc:  0.765625
train loss:  0.4848358929157257
train gradient:  0.10558334093196932
iteration : 9571
train acc:  0.7734375
train loss:  0.4407121539115906
train gradient:  0.12460999668097786
iteration : 9572
train acc:  0.6953125
train loss:  0.510806143283844
train gradient:  0.13923109601816633
iteration : 9573
train acc:  0.6796875
train loss:  0.5318785905838013
train gradient:  0.1358238381999035
iteration : 9574
train acc:  0.7265625
train loss:  0.5182147026062012
train gradient:  0.15871826661149543
iteration : 9575
train acc:  0.703125
train loss:  0.511529803276062
train gradient:  0.14274714805051597
iteration : 9576
train acc:  0.765625
train loss:  0.4339604377746582
train gradient:  0.10857764488107619
iteration : 9577
train acc:  0.78125
train loss:  0.44537457823753357
train gradient:  0.10337366080409884
iteration : 9578
train acc:  0.8046875
train loss:  0.416883647441864
train gradient:  0.10116320349891834
iteration : 9579
train acc:  0.765625
train loss:  0.46861690282821655
train gradient:  0.12319386244425118
iteration : 9580
train acc:  0.6875
train loss:  0.5608205795288086
train gradient:  0.1558945842991959
iteration : 9581
train acc:  0.8359375
train loss:  0.4298795461654663
train gradient:  0.10126850704281523
iteration : 9582
train acc:  0.6875
train loss:  0.5518430471420288
train gradient:  0.17250872435686587
iteration : 9583
train acc:  0.7734375
train loss:  0.4589604139328003
train gradient:  0.11748623327777116
iteration : 9584
train acc:  0.765625
train loss:  0.518741250038147
train gradient:  0.14805270999929465
iteration : 9585
train acc:  0.6875
train loss:  0.5739400386810303
train gradient:  0.14796400536899962
iteration : 9586
train acc:  0.7578125
train loss:  0.44564035534858704
train gradient:  0.09123429427416624
iteration : 9587
train acc:  0.6875
train loss:  0.5381463170051575
train gradient:  0.13944903914137874
iteration : 9588
train acc:  0.765625
train loss:  0.5026320815086365
train gradient:  0.1131008169397133
iteration : 9589
train acc:  0.734375
train loss:  0.5716575384140015
train gradient:  0.189208898190235
iteration : 9590
train acc:  0.7109375
train loss:  0.5377089977264404
train gradient:  0.132913222369743
iteration : 9591
train acc:  0.7109375
train loss:  0.520900309085846
train gradient:  0.13780114379263436
iteration : 9592
train acc:  0.8046875
train loss:  0.4321845769882202
train gradient:  0.09477526207245779
iteration : 9593
train acc:  0.65625
train loss:  0.5549031496047974
train gradient:  0.15670267326950282
iteration : 9594
train acc:  0.78125
train loss:  0.48038214445114136
train gradient:  0.11565581102965761
iteration : 9595
train acc:  0.796875
train loss:  0.4881870746612549
train gradient:  0.1012845971728088
iteration : 9596
train acc:  0.7734375
train loss:  0.4423745274543762
train gradient:  0.109383397414315
iteration : 9597
train acc:  0.734375
train loss:  0.4766721725463867
train gradient:  0.14805622926811413
iteration : 9598
train acc:  0.6953125
train loss:  0.5600310564041138
train gradient:  0.16156470459908184
iteration : 9599
train acc:  0.6796875
train loss:  0.5881267786026001
train gradient:  0.1508274199738066
iteration : 9600
train acc:  0.6796875
train loss:  0.617550790309906
train gradient:  0.2001271089983396
iteration : 9601
train acc:  0.703125
train loss:  0.5656055212020874
train gradient:  0.1339492558437839
iteration : 9602
train acc:  0.7578125
train loss:  0.4516902565956116
train gradient:  0.1009607516243394
iteration : 9603
train acc:  0.7734375
train loss:  0.4956037998199463
train gradient:  0.0962683398789185
iteration : 9604
train acc:  0.7734375
train loss:  0.5079189538955688
train gradient:  0.14094292801248565
iteration : 9605
train acc:  0.7265625
train loss:  0.5155432224273682
train gradient:  0.14411591567670617
iteration : 9606
train acc:  0.734375
train loss:  0.5312033891677856
train gradient:  0.12559158602933795
iteration : 9607
train acc:  0.6953125
train loss:  0.5257810354232788
train gradient:  0.1670089447284086
iteration : 9608
train acc:  0.7265625
train loss:  0.46141916513442993
train gradient:  0.0957622753570073
iteration : 9609
train acc:  0.6484375
train loss:  0.5741058588027954
train gradient:  0.1401883368936087
iteration : 9610
train acc:  0.7265625
train loss:  0.48750317096710205
train gradient:  0.16141086494493917
iteration : 9611
train acc:  0.7734375
train loss:  0.4547217786312103
train gradient:  0.08716976147353464
iteration : 9612
train acc:  0.6953125
train loss:  0.5507844686508179
train gradient:  0.15435130164605754
iteration : 9613
train acc:  0.765625
train loss:  0.45804670453071594
train gradient:  0.1377288910015383
iteration : 9614
train acc:  0.7109375
train loss:  0.47033920884132385
train gradient:  0.09819620688703223
iteration : 9615
train acc:  0.7265625
train loss:  0.4942190945148468
train gradient:  0.14781161123697956
iteration : 9616
train acc:  0.71875
train loss:  0.5305902361869812
train gradient:  0.11374257115014492
iteration : 9617
train acc:  0.7890625
train loss:  0.4928455650806427
train gradient:  0.09106606176710935
iteration : 9618
train acc:  0.703125
train loss:  0.4814530611038208
train gradient:  0.10196243238336489
iteration : 9619
train acc:  0.7578125
train loss:  0.5224831104278564
train gradient:  0.1477553587189066
iteration : 9620
train acc:  0.796875
train loss:  0.44420918822288513
train gradient:  0.09236010720819679
iteration : 9621
train acc:  0.75
train loss:  0.5597209930419922
train gradient:  0.15541054969928014
iteration : 9622
train acc:  0.75
train loss:  0.4587116837501526
train gradient:  0.12287983817819427
iteration : 9623
train acc:  0.734375
train loss:  0.4898417890071869
train gradient:  0.11878154103962248
iteration : 9624
train acc:  0.7265625
train loss:  0.5267096161842346
train gradient:  0.12943165217797992
iteration : 9625
train acc:  0.765625
train loss:  0.4515632390975952
train gradient:  0.11124108677863749
iteration : 9626
train acc:  0.7421875
train loss:  0.4992595911026001
train gradient:  0.11277870175181817
iteration : 9627
train acc:  0.75
train loss:  0.5241703987121582
train gradient:  0.14482326451493796
iteration : 9628
train acc:  0.671875
train loss:  0.6046442985534668
train gradient:  0.15140760645354523
iteration : 9629
train acc:  0.671875
train loss:  0.5572777986526489
train gradient:  0.1350267443228482
iteration : 9630
train acc:  0.7109375
train loss:  0.544862687587738
train gradient:  0.13380643493424083
iteration : 9631
train acc:  0.7578125
train loss:  0.4553331136703491
train gradient:  0.10278351001859853
iteration : 9632
train acc:  0.75
train loss:  0.5099709630012512
train gradient:  0.14243979382380273
iteration : 9633
train acc:  0.6796875
train loss:  0.5779623985290527
train gradient:  0.1533348233827026
iteration : 9634
train acc:  0.6796875
train loss:  0.5144320726394653
train gradient:  0.15989456626521983
iteration : 9635
train acc:  0.796875
train loss:  0.4569564163684845
train gradient:  0.11109166772097039
iteration : 9636
train acc:  0.7734375
train loss:  0.4450990557670593
train gradient:  0.11725302868961074
iteration : 9637
train acc:  0.71875
train loss:  0.5131223201751709
train gradient:  0.1186912650397688
iteration : 9638
train acc:  0.703125
train loss:  0.5671018362045288
train gradient:  0.13516862189941314
iteration : 9639
train acc:  0.7265625
train loss:  0.4739176034927368
train gradient:  0.10801538692071183
iteration : 9640
train acc:  0.734375
train loss:  0.4809688925743103
train gradient:  0.12104139496596511
iteration : 9641
train acc:  0.78125
train loss:  0.4447703957557678
train gradient:  0.10248523316627563
iteration : 9642
train acc:  0.75
train loss:  0.46685096621513367
train gradient:  0.11046323165844119
iteration : 9643
train acc:  0.7578125
train loss:  0.45989546179771423
train gradient:  0.08783220440621164
iteration : 9644
train acc:  0.765625
train loss:  0.4763612747192383
train gradient:  0.13326467950034138
iteration : 9645
train acc:  0.71875
train loss:  0.5824479460716248
train gradient:  0.18314209784219226
iteration : 9646
train acc:  0.703125
train loss:  0.5146335959434509
train gradient:  0.12994142549589577
iteration : 9647
train acc:  0.75
train loss:  0.467838317155838
train gradient:  0.1205344121980788
iteration : 9648
train acc:  0.7421875
train loss:  0.5148944854736328
train gradient:  0.15775971040325837
iteration : 9649
train acc:  0.7265625
train loss:  0.5060707330703735
train gradient:  0.14480620494776408
iteration : 9650
train acc:  0.765625
train loss:  0.4545167088508606
train gradient:  0.12063519334827111
iteration : 9651
train acc:  0.7421875
train loss:  0.5330020785331726
train gradient:  0.14881859949216725
iteration : 9652
train acc:  0.703125
train loss:  0.5655559301376343
train gradient:  0.1500812509421306
iteration : 9653
train acc:  0.703125
train loss:  0.574492871761322
train gradient:  0.13666543422928917
iteration : 9654
train acc:  0.7421875
train loss:  0.49456650018692017
train gradient:  0.1318187906673068
iteration : 9655
train acc:  0.703125
train loss:  0.5491134524345398
train gradient:  0.12146419047881479
iteration : 9656
train acc:  0.7265625
train loss:  0.5523491501808167
train gradient:  0.13459145956155744
iteration : 9657
train acc:  0.7421875
train loss:  0.4751046895980835
train gradient:  0.10817569469190609
iteration : 9658
train acc:  0.7890625
train loss:  0.4596076011657715
train gradient:  0.10640298550468247
iteration : 9659
train acc:  0.765625
train loss:  0.4957059621810913
train gradient:  0.11305765851170763
iteration : 9660
train acc:  0.7890625
train loss:  0.40966397523880005
train gradient:  0.09382207436579273
iteration : 9661
train acc:  0.71875
train loss:  0.54517662525177
train gradient:  0.1678914994612008
iteration : 9662
train acc:  0.6953125
train loss:  0.5262452960014343
train gradient:  0.12233583815562905
iteration : 9663
train acc:  0.7421875
train loss:  0.48248809576034546
train gradient:  0.08727190402520899
iteration : 9664
train acc:  0.75
train loss:  0.4778320789337158
train gradient:  0.0994830141328877
iteration : 9665
train acc:  0.7109375
train loss:  0.5269296169281006
train gradient:  0.1180599453933733
iteration : 9666
train acc:  0.7109375
train loss:  0.5421804785728455
train gradient:  0.11727862723241406
iteration : 9667
train acc:  0.7265625
train loss:  0.5444122552871704
train gradient:  0.11822888777285907
iteration : 9668
train acc:  0.7734375
train loss:  0.44178128242492676
train gradient:  0.11683793776905523
iteration : 9669
train acc:  0.7109375
train loss:  0.4679400324821472
train gradient:  0.10581989542025744
iteration : 9670
train acc:  0.7265625
train loss:  0.5341370105743408
train gradient:  0.1624874089757361
iteration : 9671
train acc:  0.734375
train loss:  0.5038717985153198
train gradient:  0.11447568328405353
iteration : 9672
train acc:  0.734375
train loss:  0.485632061958313
train gradient:  0.11065362078793234
iteration : 9673
train acc:  0.734375
train loss:  0.45735132694244385
train gradient:  0.10236861495090137
iteration : 9674
train acc:  0.7109375
train loss:  0.501375138759613
train gradient:  0.1251395797565467
iteration : 9675
train acc:  0.7578125
train loss:  0.5063639283180237
train gradient:  0.21206879696606412
iteration : 9676
train acc:  0.6640625
train loss:  0.589340329170227
train gradient:  0.17645323467792667
iteration : 9677
train acc:  0.75
train loss:  0.4598681330680847
train gradient:  0.08756791960832834
iteration : 9678
train acc:  0.7421875
train loss:  0.45339593291282654
train gradient:  0.10246465279198276
iteration : 9679
train acc:  0.7578125
train loss:  0.5254049301147461
train gradient:  0.1828092171540301
iteration : 9680
train acc:  0.6953125
train loss:  0.5038280487060547
train gradient:  0.11890146787520986
iteration : 9681
train acc:  0.7265625
train loss:  0.4886274039745331
train gradient:  0.1492044343128776
iteration : 9682
train acc:  0.65625
train loss:  0.5744499564170837
train gradient:  0.15628728087071714
iteration : 9683
train acc:  0.7734375
train loss:  0.4390987157821655
train gradient:  0.1018766278363685
iteration : 9684
train acc:  0.6953125
train loss:  0.5095159411430359
train gradient:  0.1161861719475344
iteration : 9685
train acc:  0.7265625
train loss:  0.5337979793548584
train gradient:  0.11778623896055128
iteration : 9686
train acc:  0.7265625
train loss:  0.5296772718429565
train gradient:  0.14479549336746178
iteration : 9687
train acc:  0.75
train loss:  0.5021219849586487
train gradient:  0.12316529584278457
iteration : 9688
train acc:  0.71875
train loss:  0.48081934452056885
train gradient:  0.1070943707236801
iteration : 9689
train acc:  0.7109375
train loss:  0.5668789148330688
train gradient:  0.1972762046630533
iteration : 9690
train acc:  0.7109375
train loss:  0.5194844007492065
train gradient:  0.10716364924294697
iteration : 9691
train acc:  0.765625
train loss:  0.4479370415210724
train gradient:  0.08275102186641511
iteration : 9692
train acc:  0.703125
train loss:  0.5189900994300842
train gradient:  0.1201265346943023
iteration : 9693
train acc:  0.7265625
train loss:  0.5074928998947144
train gradient:  0.12105600253890907
iteration : 9694
train acc:  0.7578125
train loss:  0.44862478971481323
train gradient:  0.0894614852870388
iteration : 9695
train acc:  0.7734375
train loss:  0.4743275046348572
train gradient:  0.10090414323597428
iteration : 9696
train acc:  0.671875
train loss:  0.5689935684204102
train gradient:  0.14991760435997792
iteration : 9697
train acc:  0.671875
train loss:  0.574591875076294
train gradient:  0.15012804972723226
iteration : 9698
train acc:  0.7734375
train loss:  0.5130801796913147
train gradient:  0.1375348546112593
iteration : 9699
train acc:  0.78125
train loss:  0.4555418789386749
train gradient:  0.1127503969487377
iteration : 9700
train acc:  0.7265625
train loss:  0.47746509313583374
train gradient:  0.129964989555322
iteration : 9701
train acc:  0.7265625
train loss:  0.5108234882354736
train gradient:  0.14003435467470565
iteration : 9702
train acc:  0.75
train loss:  0.4642378091812134
train gradient:  0.09765055154591129
iteration : 9703
train acc:  0.78125
train loss:  0.4482855796813965
train gradient:  0.10476416397171948
iteration : 9704
train acc:  0.8203125
train loss:  0.43764495849609375
train gradient:  0.10587628336211115
iteration : 9705
train acc:  0.734375
train loss:  0.5080896615982056
train gradient:  0.1330281264049451
iteration : 9706
train acc:  0.65625
train loss:  0.5686371922492981
train gradient:  0.16867149333347695
iteration : 9707
train acc:  0.71875
train loss:  0.5514509677886963
train gradient:  0.12936158944049903
iteration : 9708
train acc:  0.8046875
train loss:  0.39556753635406494
train gradient:  0.08679347107498839
iteration : 9709
train acc:  0.7265625
train loss:  0.4820444881916046
train gradient:  0.10781179295599881
iteration : 9710
train acc:  0.7109375
train loss:  0.49745866656303406
train gradient:  0.11170054760138204
iteration : 9711
train acc:  0.734375
train loss:  0.5374453067779541
train gradient:  0.15417020944958382
iteration : 9712
train acc:  0.75
train loss:  0.483245849609375
train gradient:  0.11380141063492993
iteration : 9713
train acc:  0.796875
train loss:  0.4184802174568176
train gradient:  0.11322084918072114
iteration : 9714
train acc:  0.796875
train loss:  0.46938973665237427
train gradient:  0.09479404748345954
iteration : 9715
train acc:  0.75
train loss:  0.4582267105579376
train gradient:  0.10183614590384789
iteration : 9716
train acc:  0.734375
train loss:  0.5161589980125427
train gradient:  0.16845163324650425
iteration : 9717
train acc:  0.703125
train loss:  0.5431881546974182
train gradient:  0.15731317476746248
iteration : 9718
train acc:  0.7109375
train loss:  0.4803234934806824
train gradient:  0.1321734742094587
iteration : 9719
train acc:  0.6875
train loss:  0.5571169853210449
train gradient:  0.15693905851379328
iteration : 9720
train acc:  0.75
train loss:  0.5491660833358765
train gradient:  0.1499619537522704
iteration : 9721
train acc:  0.7265625
train loss:  0.4575514793395996
train gradient:  0.11725498456837584
iteration : 9722
train acc:  0.734375
train loss:  0.5150871276855469
train gradient:  0.14290698558049308
iteration : 9723
train acc:  0.7265625
train loss:  0.5160324573516846
train gradient:  0.14482733718210355
iteration : 9724
train acc:  0.7890625
train loss:  0.42200028896331787
train gradient:  0.0972015338074973
iteration : 9725
train acc:  0.7265625
train loss:  0.5105969309806824
train gradient:  0.1070481575230252
iteration : 9726
train acc:  0.734375
train loss:  0.503754734992981
train gradient:  0.12720473886709982
iteration : 9727
train acc:  0.796875
train loss:  0.4409547746181488
train gradient:  0.1565461408668839
iteration : 9728
train acc:  0.796875
train loss:  0.429568886756897
train gradient:  0.1139454180266274
iteration : 9729
train acc:  0.734375
train loss:  0.4849565625190735
train gradient:  0.10751960267962252
iteration : 9730
train acc:  0.7578125
train loss:  0.4978727698326111
train gradient:  0.09996725011047616
iteration : 9731
train acc:  0.796875
train loss:  0.4183345437049866
train gradient:  0.08409491587568872
iteration : 9732
train acc:  0.765625
train loss:  0.46368348598480225
train gradient:  0.11151105111261933
iteration : 9733
train acc:  0.75
train loss:  0.5354801416397095
train gradient:  0.13842308003851264
iteration : 9734
train acc:  0.7578125
train loss:  0.493930459022522
train gradient:  0.14471289534550505
iteration : 9735
train acc:  0.7109375
train loss:  0.5645490884780884
train gradient:  0.17698586302966873
iteration : 9736
train acc:  0.75
train loss:  0.4893069267272949
train gradient:  0.12214058714905703
iteration : 9737
train acc:  0.78125
train loss:  0.4249536693096161
train gradient:  0.10466556624726617
iteration : 9738
train acc:  0.8203125
train loss:  0.42452526092529297
train gradient:  0.10072284353320565
iteration : 9739
train acc:  0.8203125
train loss:  0.42612704634666443
train gradient:  0.08024699228971448
iteration : 9740
train acc:  0.71875
train loss:  0.4976346790790558
train gradient:  0.17373690343973128
iteration : 9741
train acc:  0.71875
train loss:  0.5257236361503601
train gradient:  0.10426857950556119
iteration : 9742
train acc:  0.7265625
train loss:  0.5315786004066467
train gradient:  0.11959478932875853
iteration : 9743
train acc:  0.7421875
train loss:  0.44146645069122314
train gradient:  0.09849631575283056
iteration : 9744
train acc:  0.7421875
train loss:  0.4810604155063629
train gradient:  0.09254366514630971
iteration : 9745
train acc:  0.6796875
train loss:  0.4955075979232788
train gradient:  0.09879308403058774
iteration : 9746
train acc:  0.6953125
train loss:  0.5013645887374878
train gradient:  0.11762823088311912
iteration : 9747
train acc:  0.71875
train loss:  0.550369143486023
train gradient:  0.16406032418830746
iteration : 9748
train acc:  0.703125
train loss:  0.5615894794464111
train gradient:  0.15048712072203185
iteration : 9749
train acc:  0.6796875
train loss:  0.539241373538971
train gradient:  0.1184628405411076
iteration : 9750
train acc:  0.7890625
train loss:  0.46554630994796753
train gradient:  0.11348389424343183
iteration : 9751
train acc:  0.7578125
train loss:  0.5189264416694641
train gradient:  0.1163004123538669
iteration : 9752
train acc:  0.8046875
train loss:  0.46996527910232544
train gradient:  0.0978563192197506
iteration : 9753
train acc:  0.71875
train loss:  0.5340354442596436
train gradient:  0.14315988945731245
iteration : 9754
train acc:  0.7265625
train loss:  0.5172914266586304
train gradient:  0.11453686278536161
iteration : 9755
train acc:  0.75
train loss:  0.5099178552627563
train gradient:  0.10935558840654784
iteration : 9756
train acc:  0.75
train loss:  0.5423890352249146
train gradient:  0.12867747089639714
iteration : 9757
train acc:  0.6875
train loss:  0.515789270401001
train gradient:  0.1418121394015124
iteration : 9758
train acc:  0.7734375
train loss:  0.45652148127555847
train gradient:  0.114053459562894
iteration : 9759
train acc:  0.6875
train loss:  0.5830228328704834
train gradient:  0.15487949187538386
iteration : 9760
train acc:  0.703125
train loss:  0.5225339531898499
train gradient:  0.11332158780335572
iteration : 9761
train acc:  0.6875
train loss:  0.5071783065795898
train gradient:  0.12452464484076864
iteration : 9762
train acc:  0.71875
train loss:  0.5344694256782532
train gradient:  0.13393061252625804
iteration : 9763
train acc:  0.828125
train loss:  0.4062594771385193
train gradient:  0.09291844706611552
iteration : 9764
train acc:  0.703125
train loss:  0.5346422791481018
train gradient:  0.11755879067700165
iteration : 9765
train acc:  0.671875
train loss:  0.5610030889511108
train gradient:  0.1390074105763987
iteration : 9766
train acc:  0.7578125
train loss:  0.49496883153915405
train gradient:  0.12652040695082606
iteration : 9767
train acc:  0.7109375
train loss:  0.5337316989898682
train gradient:  0.14722805111070542
iteration : 9768
train acc:  0.828125
train loss:  0.4083334803581238
train gradient:  0.08214667998288708
iteration : 9769
train acc:  0.71875
train loss:  0.5354844331741333
train gradient:  0.1401387504041291
iteration : 9770
train acc:  0.734375
train loss:  0.47099441289901733
train gradient:  0.127469767098508
iteration : 9771
train acc:  0.7734375
train loss:  0.4677421748638153
train gradient:  0.10882409328533858
iteration : 9772
train acc:  0.65625
train loss:  0.5753450393676758
train gradient:  0.15268685383714514
iteration : 9773
train acc:  0.6953125
train loss:  0.5610507726669312
train gradient:  0.1494559941403475
iteration : 9774
train acc:  0.7734375
train loss:  0.49340540170669556
train gradient:  0.10741567151392083
iteration : 9775
train acc:  0.6875
train loss:  0.5079360008239746
train gradient:  0.12988599928943256
iteration : 9776
train acc:  0.71875
train loss:  0.4814384877681732
train gradient:  0.13754452609003015
iteration : 9777
train acc:  0.8046875
train loss:  0.438628613948822
train gradient:  0.13561413240695272
iteration : 9778
train acc:  0.7734375
train loss:  0.4808134138584137
train gradient:  0.1267478410858458
iteration : 9779
train acc:  0.71875
train loss:  0.5071419477462769
train gradient:  0.1370573693055088
iteration : 9780
train acc:  0.796875
train loss:  0.45203596353530884
train gradient:  0.12277135101112574
iteration : 9781
train acc:  0.6875
train loss:  0.5145721435546875
train gradient:  0.12294428297561272
iteration : 9782
train acc:  0.6328125
train loss:  0.613574206829071
train gradient:  0.16696347376987486
iteration : 9783
train acc:  0.71875
train loss:  0.5667436122894287
train gradient:  0.16151039193070327
iteration : 9784
train acc:  0.671875
train loss:  0.6134728193283081
train gradient:  0.18543898100502462
iteration : 9785
train acc:  0.7421875
train loss:  0.4867807626724243
train gradient:  0.12449592227226403
iteration : 9786
train acc:  0.75
train loss:  0.5192381739616394
train gradient:  0.1143362775696628
iteration : 9787
train acc:  0.671875
train loss:  0.566652774810791
train gradient:  0.13800345697675165
iteration : 9788
train acc:  0.6796875
train loss:  0.56101393699646
train gradient:  0.1402375080783469
iteration : 9789
train acc:  0.75
train loss:  0.47670722007751465
train gradient:  0.11606896517676757
iteration : 9790
train acc:  0.78125
train loss:  0.47992169857025146
train gradient:  0.12558459943699662
iteration : 9791
train acc:  0.734375
train loss:  0.5042276382446289
train gradient:  0.11005992755969912
iteration : 9792
train acc:  0.75
train loss:  0.49962398409843445
train gradient:  0.1297119379628135
iteration : 9793
train acc:  0.7421875
train loss:  0.4666811227798462
train gradient:  0.08443929088103368
iteration : 9794
train acc:  0.75
train loss:  0.46867695450782776
train gradient:  0.12965014435240818
iteration : 9795
train acc:  0.7109375
train loss:  0.4857530891895294
train gradient:  0.13367319479274714
iteration : 9796
train acc:  0.796875
train loss:  0.42514342069625854
train gradient:  0.07539353136735147
iteration : 9797
train acc:  0.765625
train loss:  0.4977758228778839
train gradient:  0.15140401022991545
iteration : 9798
train acc:  0.703125
train loss:  0.5427963733673096
train gradient:  0.1215663851836873
iteration : 9799
train acc:  0.7421875
train loss:  0.48006919026374817
train gradient:  0.13143549053061462
iteration : 9800
train acc:  0.75
train loss:  0.5075820684432983
train gradient:  0.16413765363918836
iteration : 9801
train acc:  0.7265625
train loss:  0.4893580675125122
train gradient:  0.12730610264846898
iteration : 9802
train acc:  0.7265625
train loss:  0.49922895431518555
train gradient:  0.1324134650250744
iteration : 9803
train acc:  0.75
train loss:  0.5535155534744263
train gradient:  0.1447029707676042
iteration : 9804
train acc:  0.8125
train loss:  0.43156856298446655
train gradient:  0.09459941079944362
iteration : 9805
train acc:  0.8203125
train loss:  0.41969358921051025
train gradient:  0.09550741877145047
iteration : 9806
train acc:  0.765625
train loss:  0.48440372943878174
train gradient:  0.12298796325545819
iteration : 9807
train acc:  0.734375
train loss:  0.49891045689582825
train gradient:  0.11090077087082528
iteration : 9808
train acc:  0.734375
train loss:  0.4967387020587921
train gradient:  0.10518397564188586
iteration : 9809
train acc:  0.7421875
train loss:  0.5233720541000366
train gradient:  0.12421010312655249
iteration : 9810
train acc:  0.7265625
train loss:  0.5026311278343201
train gradient:  0.14755381610498686
iteration : 9811
train acc:  0.7265625
train loss:  0.4855540096759796
train gradient:  0.1117711575519979
iteration : 9812
train acc:  0.7578125
train loss:  0.4628686010837555
train gradient:  0.13571368344301482
iteration : 9813
train acc:  0.6875
train loss:  0.5689079761505127
train gradient:  0.14915291351949567
iteration : 9814
train acc:  0.7265625
train loss:  0.45804619789123535
train gradient:  0.10881154018961237
iteration : 9815
train acc:  0.734375
train loss:  0.4828732907772064
train gradient:  0.11849337393205722
iteration : 9816
train acc:  0.734375
train loss:  0.4738912284374237
train gradient:  0.12542488155424814
iteration : 9817
train acc:  0.7265625
train loss:  0.5553625822067261
train gradient:  0.19527678253488212
iteration : 9818
train acc:  0.703125
train loss:  0.5100153684616089
train gradient:  0.1333920278676936
iteration : 9819
train acc:  0.6953125
train loss:  0.4868640601634979
train gradient:  0.10494650434663295
iteration : 9820
train acc:  0.75
train loss:  0.4962009787559509
train gradient:  0.11780764320916455
iteration : 9821
train acc:  0.75
train loss:  0.4777390658855438
train gradient:  0.12948739821872984
iteration : 9822
train acc:  0.71875
train loss:  0.5289205312728882
train gradient:  0.12960483920712468
iteration : 9823
train acc:  0.796875
train loss:  0.4384698271751404
train gradient:  0.08644774027573483
iteration : 9824
train acc:  0.734375
train loss:  0.4806176722049713
train gradient:  0.1112365475434788
iteration : 9825
train acc:  0.75
train loss:  0.46456050872802734
train gradient:  0.11784310260059797
iteration : 9826
train acc:  0.703125
train loss:  0.5452845096588135
train gradient:  0.17437985052976515
iteration : 9827
train acc:  0.71875
train loss:  0.5512301325798035
train gradient:  0.13643212721915648
iteration : 9828
train acc:  0.7734375
train loss:  0.4483831226825714
train gradient:  0.09960614343102143
iteration : 9829
train acc:  0.78125
train loss:  0.4542613923549652
train gradient:  0.10839115801711997
iteration : 9830
train acc:  0.7421875
train loss:  0.4599965810775757
train gradient:  0.12237611945823475
iteration : 9831
train acc:  0.78125
train loss:  0.44700849056243896
train gradient:  0.08801467690995296
iteration : 9832
train acc:  0.703125
train loss:  0.5749874114990234
train gradient:  0.11948174917432795
iteration : 9833
train acc:  0.765625
train loss:  0.44823378324508667
train gradient:  0.1252220712475101
iteration : 9834
train acc:  0.7890625
train loss:  0.4586262106895447
train gradient:  0.12193312438074429
iteration : 9835
train acc:  0.796875
train loss:  0.46546292304992676
train gradient:  0.09411044436610641
iteration : 9836
train acc:  0.6875
train loss:  0.5066304206848145
train gradient:  0.10185595374181412
iteration : 9837
train acc:  0.7421875
train loss:  0.5329000353813171
train gradient:  0.13785594575268945
iteration : 9838
train acc:  0.7265625
train loss:  0.5009599924087524
train gradient:  0.1056118154659323
iteration : 9839
train acc:  0.734375
train loss:  0.5100204348564148
train gradient:  0.13171075024460127
iteration : 9840
train acc:  0.71875
train loss:  0.5186845064163208
train gradient:  0.12657920754648005
iteration : 9841
train acc:  0.7421875
train loss:  0.5007447600364685
train gradient:  0.12711313383836603
iteration : 9842
train acc:  0.7734375
train loss:  0.46986788511276245
train gradient:  0.13660287441806124
iteration : 9843
train acc:  0.765625
train loss:  0.4783278703689575
train gradient:  0.1086488420988747
iteration : 9844
train acc:  0.78125
train loss:  0.48303115367889404
train gradient:  0.11998462711255654
iteration : 9845
train acc:  0.7421875
train loss:  0.4994488060474396
train gradient:  0.11560804637036899
iteration : 9846
train acc:  0.7421875
train loss:  0.4942692220211029
train gradient:  0.1202847520379941
iteration : 9847
train acc:  0.6953125
train loss:  0.5539439916610718
train gradient:  0.1755534107342202
iteration : 9848
train acc:  0.734375
train loss:  0.4701301157474518
train gradient:  0.11216454470231425
iteration : 9849
train acc:  0.7578125
train loss:  0.4733154773712158
train gradient:  0.0857596614683584
iteration : 9850
train acc:  0.7890625
train loss:  0.47022655606269836
train gradient:  0.11237884902182774
iteration : 9851
train acc:  0.703125
train loss:  0.5710035562515259
train gradient:  0.14496195687223423
iteration : 9852
train acc:  0.734375
train loss:  0.4401397109031677
train gradient:  0.10441107774847004
iteration : 9853
train acc:  0.75
train loss:  0.463939368724823
train gradient:  0.12330109783919459
iteration : 9854
train acc:  0.734375
train loss:  0.4976796507835388
train gradient:  0.10932387145755693
iteration : 9855
train acc:  0.703125
train loss:  0.5158847570419312
train gradient:  0.12638374363417376
iteration : 9856
train acc:  0.734375
train loss:  0.5009809732437134
train gradient:  0.11653379451279085
iteration : 9857
train acc:  0.8203125
train loss:  0.4033045172691345
train gradient:  0.08360793863785643
iteration : 9858
train acc:  0.734375
train loss:  0.4614098370075226
train gradient:  0.16330351691245004
iteration : 9859
train acc:  0.765625
train loss:  0.5133127570152283
train gradient:  0.16135242777546724
iteration : 9860
train acc:  0.71875
train loss:  0.4858974814414978
train gradient:  0.09932764961711328
iteration : 9861
train acc:  0.6796875
train loss:  0.5503750443458557
train gradient:  0.14741209523228138
iteration : 9862
train acc:  0.6875
train loss:  0.5174585580825806
train gradient:  0.13962424189567696
iteration : 9863
train acc:  0.703125
train loss:  0.573824942111969
train gradient:  0.18264704071441712
iteration : 9864
train acc:  0.7890625
train loss:  0.4735420346260071
train gradient:  0.12926980768806634
iteration : 9865
train acc:  0.7890625
train loss:  0.43514400720596313
train gradient:  0.11670346950016752
iteration : 9866
train acc:  0.7109375
train loss:  0.4966060519218445
train gradient:  0.11719799613292935
iteration : 9867
train acc:  0.75
train loss:  0.465376615524292
train gradient:  0.1241286236990366
iteration : 9868
train acc:  0.75
train loss:  0.5833265781402588
train gradient:  0.17287439230452908
iteration : 9869
train acc:  0.703125
train loss:  0.5450310111045837
train gradient:  0.1519724700154153
iteration : 9870
train acc:  0.765625
train loss:  0.507771909236908
train gradient:  0.12325292619518935
iteration : 9871
train acc:  0.6875
train loss:  0.5364546775817871
train gradient:  0.10765103327506147
iteration : 9872
train acc:  0.71875
train loss:  0.5467559099197388
train gradient:  0.13593139841154978
iteration : 9873
train acc:  0.796875
train loss:  0.4698216915130615
train gradient:  0.09460093716137544
iteration : 9874
train acc:  0.765625
train loss:  0.48734813928604126
train gradient:  0.1321276638057417
iteration : 9875
train acc:  0.7109375
train loss:  0.5589678287506104
train gradient:  0.13661204823127868
iteration : 9876
train acc:  0.7265625
train loss:  0.5056632161140442
train gradient:  0.11957301660900783
iteration : 9877
train acc:  0.765625
train loss:  0.4819459319114685
train gradient:  0.11619214974815194
iteration : 9878
train acc:  0.7734375
train loss:  0.45820683240890503
train gradient:  0.10613790252218872
iteration : 9879
train acc:  0.7109375
train loss:  0.5061166286468506
train gradient:  0.11069109225792759
iteration : 9880
train acc:  0.7265625
train loss:  0.4959808588027954
train gradient:  0.1394064348293812
iteration : 9881
train acc:  0.6640625
train loss:  0.6093328595161438
train gradient:  0.132808793873638
iteration : 9882
train acc:  0.75
train loss:  0.45633113384246826
train gradient:  0.0974379554956457
iteration : 9883
train acc:  0.71875
train loss:  0.5463931560516357
train gradient:  0.12062635835112254
iteration : 9884
train acc:  0.7265625
train loss:  0.49006932973861694
train gradient:  0.09601345120558913
iteration : 9885
train acc:  0.7578125
train loss:  0.5151121020317078
train gradient:  0.135261523580661
iteration : 9886
train acc:  0.7578125
train loss:  0.4498075842857361
train gradient:  0.11665810633177194
iteration : 9887
train acc:  0.8125
train loss:  0.4202030301094055
train gradient:  0.0804374060312191
iteration : 9888
train acc:  0.7734375
train loss:  0.49907082319259644
train gradient:  0.12109402506252456
iteration : 9889
train acc:  0.8046875
train loss:  0.4444738030433655
train gradient:  0.11316895942027964
iteration : 9890
train acc:  0.7578125
train loss:  0.4941026270389557
train gradient:  0.10890758490478271
iteration : 9891
train acc:  0.7109375
train loss:  0.5021687150001526
train gradient:  0.11063449022687814
iteration : 9892
train acc:  0.78125
train loss:  0.48314231634140015
train gradient:  0.13153999549342826
iteration : 9893
train acc:  0.7421875
train loss:  0.502919614315033
train gradient:  0.1092501721646512
iteration : 9894
train acc:  0.734375
train loss:  0.49270784854888916
train gradient:  0.09912606245963279
iteration : 9895
train acc:  0.78125
train loss:  0.49533528089523315
train gradient:  0.11190944046835956
iteration : 9896
train acc:  0.8203125
train loss:  0.3802531361579895
train gradient:  0.09270916799256285
iteration : 9897
train acc:  0.703125
train loss:  0.546073317527771
train gradient:  0.17052690425322647
iteration : 9898
train acc:  0.71875
train loss:  0.5259844064712524
train gradient:  0.13915130637072384
iteration : 9899
train acc:  0.796875
train loss:  0.538127601146698
train gradient:  0.12767932253796838
iteration : 9900
train acc:  0.765625
train loss:  0.4983338713645935
train gradient:  0.11946103817327564
iteration : 9901
train acc:  0.7890625
train loss:  0.4674622416496277
train gradient:  0.09596908838485624
iteration : 9902
train acc:  0.7109375
train loss:  0.5139375329017639
train gradient:  0.10792693077953781
iteration : 9903
train acc:  0.6875
train loss:  0.5556649565696716
train gradient:  0.14827069349174327
iteration : 9904
train acc:  0.765625
train loss:  0.4473814070224762
train gradient:  0.11843121084128075
iteration : 9905
train acc:  0.7109375
train loss:  0.5409119129180908
train gradient:  0.15391279756637816
iteration : 9906
train acc:  0.7421875
train loss:  0.49244943261146545
train gradient:  0.10873551011496434
iteration : 9907
train acc:  0.78125
train loss:  0.4588785171508789
train gradient:  0.11484871071788554
iteration : 9908
train acc:  0.796875
train loss:  0.439983069896698
train gradient:  0.09844691557058499
iteration : 9909
train acc:  0.734375
train loss:  0.523331880569458
train gradient:  0.15428868247339042
iteration : 9910
train acc:  0.7265625
train loss:  0.483928918838501
train gradient:  0.1217750551773754
iteration : 9911
train acc:  0.7421875
train loss:  0.4990009367465973
train gradient:  0.13202004441470688
iteration : 9912
train acc:  0.78125
train loss:  0.468379408121109
train gradient:  0.12153695591134615
iteration : 9913
train acc:  0.7890625
train loss:  0.46781235933303833
train gradient:  0.09864050851848633
iteration : 9914
train acc:  0.6875
train loss:  0.5456314086914062
train gradient:  0.13398093180247372
iteration : 9915
train acc:  0.765625
train loss:  0.4538165032863617
train gradient:  0.10283405908668865
iteration : 9916
train acc:  0.75
train loss:  0.5163040161132812
train gradient:  0.14966235838079583
iteration : 9917
train acc:  0.765625
train loss:  0.48194727301597595
train gradient:  0.11130989023125556
iteration : 9918
train acc:  0.7890625
train loss:  0.4557979106903076
train gradient:  0.10989019870837739
iteration : 9919
train acc:  0.7734375
train loss:  0.4365123212337494
train gradient:  0.1173433874844023
iteration : 9920
train acc:  0.7421875
train loss:  0.5182464122772217
train gradient:  0.11716795325070581
iteration : 9921
train acc:  0.6953125
train loss:  0.5392027497291565
train gradient:  0.1608844586364317
iteration : 9922
train acc:  0.71875
train loss:  0.4843289852142334
train gradient:  0.11899097469725274
iteration : 9923
train acc:  0.7890625
train loss:  0.5017968416213989
train gradient:  0.12815616936626695
iteration : 9924
train acc:  0.8359375
train loss:  0.4175276756286621
train gradient:  0.09698049658676267
iteration : 9925
train acc:  0.78125
train loss:  0.5036904215812683
train gradient:  0.13059516701522744
iteration : 9926
train acc:  0.7421875
train loss:  0.4639873504638672
train gradient:  0.11357123633909705
iteration : 9927
train acc:  0.6953125
train loss:  0.511919379234314
train gradient:  0.15396051085989043
iteration : 9928
train acc:  0.734375
train loss:  0.4949547350406647
train gradient:  0.11665075095575457
iteration : 9929
train acc:  0.765625
train loss:  0.4835653305053711
train gradient:  0.1364484629290197
iteration : 9930
train acc:  0.765625
train loss:  0.43840423226356506
train gradient:  0.09871218902563363
iteration : 9931
train acc:  0.796875
train loss:  0.4407598674297333
train gradient:  0.10882287473709408
iteration : 9932
train acc:  0.6484375
train loss:  0.6265650987625122
train gradient:  0.1873550984963624
iteration : 9933
train acc:  0.7578125
train loss:  0.4560926556587219
train gradient:  0.1164182110449804
iteration : 9934
train acc:  0.75
train loss:  0.5055903196334839
train gradient:  0.11696768574307398
iteration : 9935
train acc:  0.765625
train loss:  0.4749234914779663
train gradient:  0.12102600392905129
iteration : 9936
train acc:  0.71875
train loss:  0.5262607336044312
train gradient:  0.12560468487556314
iteration : 9937
train acc:  0.6953125
train loss:  0.5452020168304443
train gradient:  0.1333831950717456
iteration : 9938
train acc:  0.7109375
train loss:  0.4958798289299011
train gradient:  0.12361614404293733
iteration : 9939
train acc:  0.75
train loss:  0.45755520462989807
train gradient:  0.14389282274328796
iteration : 9940
train acc:  0.7890625
train loss:  0.49693387746810913
train gradient:  0.13177308786974373
iteration : 9941
train acc:  0.6796875
train loss:  0.5683143734931946
train gradient:  0.1454067536080256
iteration : 9942
train acc:  0.75
train loss:  0.4864634871482849
train gradient:  0.1286559013277409
iteration : 9943
train acc:  0.734375
train loss:  0.5313949584960938
train gradient:  0.11752597274439774
iteration : 9944
train acc:  0.734375
train loss:  0.45389890670776367
train gradient:  0.13396501894637025
iteration : 9945
train acc:  0.8125
train loss:  0.46089571714401245
train gradient:  0.13053846126306212
iteration : 9946
train acc:  0.734375
train loss:  0.5019538402557373
train gradient:  0.12838497161766388
iteration : 9947
train acc:  0.7421875
train loss:  0.4796614348888397
train gradient:  0.11975778450559353
iteration : 9948
train acc:  0.734375
train loss:  0.4759976863861084
train gradient:  0.10373680099187206
iteration : 9949
train acc:  0.7890625
train loss:  0.5365395545959473
train gradient:  0.12432961460179176
iteration : 9950
train acc:  0.796875
train loss:  0.4712652266025543
train gradient:  0.09620428380529683
iteration : 9951
train acc:  0.78125
train loss:  0.4760833978652954
train gradient:  0.13479042459786333
iteration : 9952
train acc:  0.828125
train loss:  0.4202926754951477
train gradient:  0.09452636766301888
iteration : 9953
train acc:  0.75
train loss:  0.4584185481071472
train gradient:  0.111670199247718
iteration : 9954
train acc:  0.765625
train loss:  0.4510346055030823
train gradient:  0.10044219922265933
iteration : 9955
train acc:  0.7578125
train loss:  0.4626113176345825
train gradient:  0.1043949757273754
iteration : 9956
train acc:  0.78125
train loss:  0.46044275164604187
train gradient:  0.11196848302257811
iteration : 9957
train acc:  0.75
train loss:  0.4818112552165985
train gradient:  0.15111364005910322
iteration : 9958
train acc:  0.6875
train loss:  0.5822207927703857
train gradient:  0.1576943095464612
iteration : 9959
train acc:  0.7578125
train loss:  0.47313153743743896
train gradient:  0.10516466450965717
iteration : 9960
train acc:  0.7265625
train loss:  0.4849279224872589
train gradient:  0.11150847110711824
iteration : 9961
train acc:  0.7265625
train loss:  0.48411494493484497
train gradient:  0.1219363525976163
iteration : 9962
train acc:  0.7578125
train loss:  0.4925166964530945
train gradient:  0.1363081047330118
iteration : 9963
train acc:  0.78125
train loss:  0.44608595967292786
train gradient:  0.09183412535599393
iteration : 9964
train acc:  0.734375
train loss:  0.44557517766952515
train gradient:  0.0994396144774722
iteration : 9965
train acc:  0.6796875
train loss:  0.5591646432876587
train gradient:  0.14212593320359562
iteration : 9966
train acc:  0.765625
train loss:  0.475311815738678
train gradient:  0.1464654903467662
iteration : 9967
train acc:  0.7890625
train loss:  0.45416688919067383
train gradient:  0.11188839216817865
iteration : 9968
train acc:  0.75
train loss:  0.45857271552085876
train gradient:  0.09499233923769063
iteration : 9969
train acc:  0.75
train loss:  0.4943448305130005
train gradient:  0.13436204775222488
iteration : 9970
train acc:  0.7421875
train loss:  0.5093921422958374
train gradient:  0.13515943236676564
iteration : 9971
train acc:  0.71875
train loss:  0.5926116704940796
train gradient:  0.17020175826489597
iteration : 9972
train acc:  0.7109375
train loss:  0.546750545501709
train gradient:  0.16152349406629568
iteration : 9973
train acc:  0.71875
train loss:  0.5244103074073792
train gradient:  0.13989387054778551
iteration : 9974
train acc:  0.8125
train loss:  0.4615591764450073
train gradient:  0.09434849607714092
iteration : 9975
train acc:  0.7890625
train loss:  0.43366706371307373
train gradient:  0.13090780352079545
iteration : 9976
train acc:  0.75
train loss:  0.48290032148361206
train gradient:  0.1190350780888168
iteration : 9977
train acc:  0.828125
train loss:  0.40875905752182007
train gradient:  0.08958023378719761
iteration : 9978
train acc:  0.7578125
train loss:  0.5268943309783936
train gradient:  0.12804365076317187
iteration : 9979
train acc:  0.6875
train loss:  0.5332375764846802
train gradient:  0.15009610517439698
iteration : 9980
train acc:  0.7265625
train loss:  0.47622910141944885
train gradient:  0.10160123081391235
iteration : 9981
train acc:  0.734375
train loss:  0.4627496898174286
train gradient:  0.1153703053177563
iteration : 9982
train acc:  0.7265625
train loss:  0.4855126738548279
train gradient:  0.11662795106570926
iteration : 9983
train acc:  0.7421875
train loss:  0.532961368560791
train gradient:  0.18899835932358178
iteration : 9984
train acc:  0.8046875
train loss:  0.4322529137134552
train gradient:  0.11607338291281333
iteration : 9985
train acc:  0.7109375
train loss:  0.5395503640174866
train gradient:  0.18859913271636758
iteration : 9986
train acc:  0.7578125
train loss:  0.48452654480934143
train gradient:  0.1572609465906636
iteration : 9987
train acc:  0.765625
train loss:  0.48866942524909973
train gradient:  0.14317490604948988
iteration : 9988
train acc:  0.6796875
train loss:  0.5943633317947388
train gradient:  0.20762475613243792
iteration : 9989
train acc:  0.71875
train loss:  0.4808039665222168
train gradient:  0.13796883311661215
iteration : 9990
train acc:  0.7734375
train loss:  0.48677441477775574
train gradient:  0.12119681793328942
iteration : 9991
train acc:  0.6875
train loss:  0.5324487686157227
train gradient:  0.13309555858309163
iteration : 9992
train acc:  0.7578125
train loss:  0.46872347593307495
train gradient:  0.10986674672694256
iteration : 9993
train acc:  0.7265625
train loss:  0.5046690702438354
train gradient:  0.1276540306055754
iteration : 9994
train acc:  0.7578125
train loss:  0.482302725315094
train gradient:  0.118800397179165
iteration : 9995
train acc:  0.703125
train loss:  0.5415284633636475
train gradient:  0.11431418790054318
iteration : 9996
train acc:  0.75
train loss:  0.4922361969947815
train gradient:  0.12757775221197043
iteration : 9997
train acc:  0.6953125
train loss:  0.5001096725463867
train gradient:  0.14318635735617807
iteration : 9998
train acc:  0.71875
train loss:  0.4741061329841614
train gradient:  0.1486164740866749
iteration : 9999
train acc:  0.765625
train loss:  0.46052294969558716
train gradient:  0.12459120484577633
iteration : 10000
train acc:  0.734375
train loss:  0.5047821998596191
train gradient:  0.11274848433785838
iteration : 10001
train acc:  0.7265625
train loss:  0.48639997839927673
train gradient:  0.12763060324316935
iteration : 10002
train acc:  0.75
train loss:  0.45554041862487793
train gradient:  0.11701042671422657
iteration : 10003
train acc:  0.828125
train loss:  0.5108239650726318
train gradient:  0.14863060636180858
iteration : 10004
train acc:  0.71875
train loss:  0.49331679940223694
train gradient:  0.14033885342252922
iteration : 10005
train acc:  0.703125
train loss:  0.48259925842285156
train gradient:  0.12279735437728052
iteration : 10006
train acc:  0.75
train loss:  0.49802514910697937
train gradient:  0.13477287465956522
iteration : 10007
train acc:  0.78125
train loss:  0.4284488260746002
train gradient:  0.09921822344938099
iteration : 10008
train acc:  0.6953125
train loss:  0.5565346479415894
train gradient:  0.16507625879149584
iteration : 10009
train acc:  0.796875
train loss:  0.4776456356048584
train gradient:  0.13110979604225315
iteration : 10010
train acc:  0.7890625
train loss:  0.4235973656177521
train gradient:  0.09539428434875133
iteration : 10011
train acc:  0.6953125
train loss:  0.5060927867889404
train gradient:  0.11716295496821644
iteration : 10012
train acc:  0.8046875
train loss:  0.44663405418395996
train gradient:  0.12156612563875287
iteration : 10013
train acc:  0.625
train loss:  0.6177966594696045
train gradient:  0.16687592952900834
iteration : 10014
train acc:  0.7734375
train loss:  0.5225948095321655
train gradient:  0.1563073576411983
iteration : 10015
train acc:  0.7890625
train loss:  0.45104119181632996
train gradient:  0.1274479940253283
iteration : 10016
train acc:  0.8046875
train loss:  0.4383470118045807
train gradient:  0.10762256389177514
iteration : 10017
train acc:  0.7109375
train loss:  0.5353283882141113
train gradient:  0.13518830755069372
iteration : 10018
train acc:  0.6953125
train loss:  0.5556933879852295
train gradient:  0.18405393751193597
iteration : 10019
train acc:  0.6953125
train loss:  0.6225869655609131
train gradient:  0.19466177717636174
iteration : 10020
train acc:  0.8046875
train loss:  0.44022136926651
train gradient:  0.12008022100975964
iteration : 10021
train acc:  0.7578125
train loss:  0.4554196894168854
train gradient:  0.12503511259459482
iteration : 10022
train acc:  0.7578125
train loss:  0.49891000986099243
train gradient:  0.18874763724175875
iteration : 10023
train acc:  0.75
train loss:  0.5296597480773926
train gradient:  0.131290446759999
iteration : 10024
train acc:  0.828125
train loss:  0.41692253947257996
train gradient:  0.09090740089391713
iteration : 10025
train acc:  0.8203125
train loss:  0.4365229904651642
train gradient:  0.08765766998642076
iteration : 10026
train acc:  0.7265625
train loss:  0.570616602897644
train gradient:  0.15991307196340931
iteration : 10027
train acc:  0.7109375
train loss:  0.5749953985214233
train gradient:  0.19457183297048802
iteration : 10028
train acc:  0.7265625
train loss:  0.583705723285675
train gradient:  0.1589964865994083
iteration : 10029
train acc:  0.7109375
train loss:  0.5065836906433105
train gradient:  0.11177001327893141
iteration : 10030
train acc:  0.7734375
train loss:  0.49189186096191406
train gradient:  0.11352350686186861
iteration : 10031
train acc:  0.796875
train loss:  0.44119539856910706
train gradient:  0.11785949276816643
iteration : 10032
train acc:  0.703125
train loss:  0.5155206322669983
train gradient:  0.10473256783341521
iteration : 10033
train acc:  0.71875
train loss:  0.5444700717926025
train gradient:  0.14075530473671366
iteration : 10034
train acc:  0.8046875
train loss:  0.47626349329948425
train gradient:  0.09357561504833109
iteration : 10035
train acc:  0.796875
train loss:  0.4333839416503906
train gradient:  0.12140261194681952
iteration : 10036
train acc:  0.765625
train loss:  0.4935339093208313
train gradient:  0.12203117630559629
iteration : 10037
train acc:  0.765625
train loss:  0.44847196340560913
train gradient:  0.09930743153205793
iteration : 10038
train acc:  0.796875
train loss:  0.45498642325401306
train gradient:  0.10015080278386285
iteration : 10039
train acc:  0.765625
train loss:  0.4923420548439026
train gradient:  0.10742319171619799
iteration : 10040
train acc:  0.7734375
train loss:  0.48922717571258545
train gradient:  0.10506537074035839
iteration : 10041
train acc:  0.8125
train loss:  0.41548338532447815
train gradient:  0.10432078884686977
iteration : 10042
train acc:  0.828125
train loss:  0.4211965501308441
train gradient:  0.11054388125884056
iteration : 10043
train acc:  0.71875
train loss:  0.5243183374404907
train gradient:  0.11292576431905936
iteration : 10044
train acc:  0.8125
train loss:  0.4245668649673462
train gradient:  0.10546435898898314
iteration : 10045
train acc:  0.7265625
train loss:  0.5023270845413208
train gradient:  0.12670749835038758
iteration : 10046
train acc:  0.7890625
train loss:  0.41934025287628174
train gradient:  0.10578324159967732
iteration : 10047
train acc:  0.78125
train loss:  0.4711083769798279
train gradient:  0.10818729049379039
iteration : 10048
train acc:  0.6875
train loss:  0.5257852077484131
train gradient:  0.12859071162365876
iteration : 10049
train acc:  0.734375
train loss:  0.4510501027107239
train gradient:  0.10419967295505285
iteration : 10050
train acc:  0.7265625
train loss:  0.4566521644592285
train gradient:  0.09529826614029015
iteration : 10051
train acc:  0.765625
train loss:  0.4937824010848999
train gradient:  0.11264715791004204
iteration : 10052
train acc:  0.6875
train loss:  0.543501615524292
train gradient:  0.16515690916374626
iteration : 10053
train acc:  0.828125
train loss:  0.44173893332481384
train gradient:  0.13229811990578055
iteration : 10054
train acc:  0.7421875
train loss:  0.49980464577674866
train gradient:  0.10853935182067447
iteration : 10055
train acc:  0.734375
train loss:  0.5041970610618591
train gradient:  0.1305362933248851
iteration : 10056
train acc:  0.7421875
train loss:  0.48299992084503174
train gradient:  0.11204297923768755
iteration : 10057
train acc:  0.8515625
train loss:  0.40316182374954224
train gradient:  0.0755797997271737
iteration : 10058
train acc:  0.7109375
train loss:  0.5065330266952515
train gradient:  0.12145586998332167
iteration : 10059
train acc:  0.6953125
train loss:  0.485184907913208
train gradient:  0.10997972248660748
iteration : 10060
train acc:  0.8046875
train loss:  0.4405962824821472
train gradient:  0.11414803461970727
iteration : 10061
train acc:  0.71875
train loss:  0.5179952383041382
train gradient:  0.16569201317516297
iteration : 10062
train acc:  0.65625
train loss:  0.613652229309082
train gradient:  0.1872955834138882
iteration : 10063
train acc:  0.7265625
train loss:  0.4519633650779724
train gradient:  0.09830882496856386
iteration : 10064
train acc:  0.6953125
train loss:  0.5663903951644897
train gradient:  0.14487668718323238
iteration : 10065
train acc:  0.6875
train loss:  0.5184566974639893
train gradient:  0.1282974781445384
iteration : 10066
train acc:  0.7109375
train loss:  0.5661445260047913
train gradient:  0.16573796696621368
iteration : 10067
train acc:  0.7265625
train loss:  0.546566903591156
train gradient:  0.15384180836092512
iteration : 10068
train acc:  0.796875
train loss:  0.42878979444503784
train gradient:  0.09529388999637745
iteration : 10069
train acc:  0.7734375
train loss:  0.45394355058670044
train gradient:  0.09486718594910835
iteration : 10070
train acc:  0.7109375
train loss:  0.5293726921081543
train gradient:  0.1485899735121602
iteration : 10071
train acc:  0.8203125
train loss:  0.4255152642726898
train gradient:  0.12022860859240422
iteration : 10072
train acc:  0.7421875
train loss:  0.4890393018722534
train gradient:  0.12115301536490326
iteration : 10073
train acc:  0.8046875
train loss:  0.4864695370197296
train gradient:  0.1513321081491513
iteration : 10074
train acc:  0.78125
train loss:  0.4463667869567871
train gradient:  0.09447546341602871
iteration : 10075
train acc:  0.7578125
train loss:  0.47362953424453735
train gradient:  0.12214212310003641
iteration : 10076
train acc:  0.75
train loss:  0.5380315780639648
train gradient:  0.14311511420227307
iteration : 10077
train acc:  0.6953125
train loss:  0.5458900928497314
train gradient:  0.16526176613973886
iteration : 10078
train acc:  0.8203125
train loss:  0.39717960357666016
train gradient:  0.09265178879855471
iteration : 10079
train acc:  0.734375
train loss:  0.530113160610199
train gradient:  0.13491458790176775
iteration : 10080
train acc:  0.734375
train loss:  0.4694635272026062
train gradient:  0.11080974730330685
iteration : 10081
train acc:  0.7109375
train loss:  0.5136881470680237
train gradient:  0.13366758835277792
iteration : 10082
train acc:  0.7109375
train loss:  0.5259995460510254
train gradient:  0.14814177260802935
iteration : 10083
train acc:  0.7734375
train loss:  0.49924132227897644
train gradient:  0.09637513373885602
iteration : 10084
train acc:  0.703125
train loss:  0.5949037075042725
train gradient:  0.2011391848210976
iteration : 10085
train acc:  0.7578125
train loss:  0.4826095700263977
train gradient:  0.12280734691702358
iteration : 10086
train acc:  0.7578125
train loss:  0.492807000875473
train gradient:  0.11185495052002306
iteration : 10087
train acc:  0.7421875
train loss:  0.4658910036087036
train gradient:  0.10362919353450419
iteration : 10088
train acc:  0.7734375
train loss:  0.4588453769683838
train gradient:  0.12922625145259914
iteration : 10089
train acc:  0.7734375
train loss:  0.4989643692970276
train gradient:  0.11389859710369488
iteration : 10090
train acc:  0.7421875
train loss:  0.4799933433532715
train gradient:  0.13326436195364638
iteration : 10091
train acc:  0.7890625
train loss:  0.4971039593219757
train gradient:  0.11875477893572307
iteration : 10092
train acc:  0.765625
train loss:  0.4799535572528839
train gradient:  0.1319441809049103
iteration : 10093
train acc:  0.7734375
train loss:  0.47822830080986023
train gradient:  0.12508417804520167
iteration : 10094
train acc:  0.703125
train loss:  0.5167677998542786
train gradient:  0.15215943941048915
iteration : 10095
train acc:  0.796875
train loss:  0.42351749539375305
train gradient:  0.08224738714807699
iteration : 10096
train acc:  0.734375
train loss:  0.5495153665542603
train gradient:  0.15754147905616234
iteration : 10097
train acc:  0.7421875
train loss:  0.5081540942192078
train gradient:  0.1610861290965908
iteration : 10098
train acc:  0.6953125
train loss:  0.4934910535812378
train gradient:  0.14638147941288376
iteration : 10099
train acc:  0.8046875
train loss:  0.45654135942459106
train gradient:  0.1037542316046402
iteration : 10100
train acc:  0.703125
train loss:  0.5695619583129883
train gradient:  0.16060589140220327
iteration : 10101
train acc:  0.71875
train loss:  0.5551732778549194
train gradient:  0.11990480599637689
iteration : 10102
train acc:  0.7734375
train loss:  0.48500266671180725
train gradient:  0.11152450304796173
iteration : 10103
train acc:  0.6875
train loss:  0.56260085105896
train gradient:  0.1449524755241584
iteration : 10104
train acc:  0.703125
train loss:  0.566462516784668
train gradient:  0.16333824464558755
iteration : 10105
train acc:  0.71875
train loss:  0.5277012586593628
train gradient:  0.18410132434347576
iteration : 10106
train acc:  0.765625
train loss:  0.5340006351470947
train gradient:  0.15959491694431213
iteration : 10107
train acc:  0.7265625
train loss:  0.5217008590698242
train gradient:  0.14069254023776062
iteration : 10108
train acc:  0.71875
train loss:  0.539128839969635
train gradient:  0.17257663963250974
iteration : 10109
train acc:  0.6875
train loss:  0.4997483491897583
train gradient:  0.1516217108465429
iteration : 10110
train acc:  0.78125
train loss:  0.4490734040737152
train gradient:  0.10881572519841028
iteration : 10111
train acc:  0.734375
train loss:  0.4703393280506134
train gradient:  0.1478935024162829
iteration : 10112
train acc:  0.828125
train loss:  0.4545908272266388
train gradient:  0.10759820556477791
iteration : 10113
train acc:  0.6796875
train loss:  0.5581814646720886
train gradient:  0.13814833736270854
iteration : 10114
train acc:  0.703125
train loss:  0.5158651471138
train gradient:  0.12398512239439996
iteration : 10115
train acc:  0.7578125
train loss:  0.4771098494529724
train gradient:  0.130171562205688
iteration : 10116
train acc:  0.765625
train loss:  0.49716007709503174
train gradient:  0.15140760071269543
iteration : 10117
train acc:  0.7578125
train loss:  0.4873294234275818
train gradient:  0.1263896882253193
iteration : 10118
train acc:  0.7109375
train loss:  0.5187755823135376
train gradient:  0.11029953990865896
iteration : 10119
train acc:  0.7734375
train loss:  0.4851992130279541
train gradient:  0.14404941865146603
iteration : 10120
train acc:  0.71875
train loss:  0.542422890663147
train gradient:  0.14466852356647794
iteration : 10121
train acc:  0.6953125
train loss:  0.5452709197998047
train gradient:  0.12870025406025626
iteration : 10122
train acc:  0.7421875
train loss:  0.47383254766464233
train gradient:  0.11419145218251633
iteration : 10123
train acc:  0.765625
train loss:  0.44070494174957275
train gradient:  0.09698506241731368
iteration : 10124
train acc:  0.78125
train loss:  0.46114522218704224
train gradient:  0.12942596136773699
iteration : 10125
train acc:  0.78125
train loss:  0.45061296224594116
train gradient:  0.12327060334431579
iteration : 10126
train acc:  0.78125
train loss:  0.45660874247550964
train gradient:  0.11832010270955642
iteration : 10127
train acc:  0.734375
train loss:  0.5010031461715698
train gradient:  0.14229790063944003
iteration : 10128
train acc:  0.75
train loss:  0.5061249732971191
train gradient:  0.11374794710613494
iteration : 10129
train acc:  0.7421875
train loss:  0.5538774728775024
train gradient:  0.11808573311650654
iteration : 10130
train acc:  0.7890625
train loss:  0.45412272214889526
train gradient:  0.10800090063737555
iteration : 10131
train acc:  0.7578125
train loss:  0.49526187777519226
train gradient:  0.13343167848322685
iteration : 10132
train acc:  0.78125
train loss:  0.44098135828971863
train gradient:  0.09798377671537385
iteration : 10133
train acc:  0.6796875
train loss:  0.5120018720626831
train gradient:  0.11279207809346473
iteration : 10134
train acc:  0.7421875
train loss:  0.48917317390441895
train gradient:  0.19943802230883534
iteration : 10135
train acc:  0.7734375
train loss:  0.4340229034423828
train gradient:  0.11863792930507336
iteration : 10136
train acc:  0.6953125
train loss:  0.5228614807128906
train gradient:  0.15067031020194646
iteration : 10137
train acc:  0.7890625
train loss:  0.43370386958122253
train gradient:  0.08741159656395325
iteration : 10138
train acc:  0.703125
train loss:  0.5582939386367798
train gradient:  0.15637071271506814
iteration : 10139
train acc:  0.8046875
train loss:  0.42358413338661194
train gradient:  0.09928929119793936
iteration : 10140
train acc:  0.7578125
train loss:  0.5046525001525879
train gradient:  0.11059932101065177
iteration : 10141
train acc:  0.734375
train loss:  0.4900140166282654
train gradient:  0.15324236444544886
iteration : 10142
train acc:  0.7421875
train loss:  0.4917151927947998
train gradient:  0.13906719533431652
iteration : 10143
train acc:  0.828125
train loss:  0.45019033551216125
train gradient:  0.10457956351696798
iteration : 10144
train acc:  0.7109375
train loss:  0.5052319765090942
train gradient:  0.11265789961390843
iteration : 10145
train acc:  0.7421875
train loss:  0.45599666237831116
train gradient:  0.08535297143317888
iteration : 10146
train acc:  0.7421875
train loss:  0.48657941818237305
train gradient:  0.10408425651851931
iteration : 10147
train acc:  0.75
train loss:  0.5176377892494202
train gradient:  0.1355013777675419
iteration : 10148
train acc:  0.6875
train loss:  0.5042484998703003
train gradient:  0.14869225517188803
iteration : 10149
train acc:  0.71875
train loss:  0.5312066078186035
train gradient:  0.1525200971916128
iteration : 10150
train acc:  0.7109375
train loss:  0.5214783549308777
train gradient:  0.12389798871603955
iteration : 10151
train acc:  0.7578125
train loss:  0.5010355710983276
train gradient:  0.12286608779287414
iteration : 10152
train acc:  0.7578125
train loss:  0.473233163356781
train gradient:  0.13961370634013684
iteration : 10153
train acc:  0.671875
train loss:  0.5806552171707153
train gradient:  0.1612843482481595
iteration : 10154
train acc:  0.6953125
train loss:  0.524681568145752
train gradient:  0.166070760411964
iteration : 10155
train acc:  0.7890625
train loss:  0.47110190987586975
train gradient:  0.11395629477104427
iteration : 10156
train acc:  0.71875
train loss:  0.5552095174789429
train gradient:  0.1454280912491217
iteration : 10157
train acc:  0.8046875
train loss:  0.4443508982658386
train gradient:  0.10403556633431368
iteration : 10158
train acc:  0.703125
train loss:  0.5245293378829956
train gradient:  0.12729454062884932
iteration : 10159
train acc:  0.796875
train loss:  0.458074688911438
train gradient:  0.11532746318795467
iteration : 10160
train acc:  0.7421875
train loss:  0.4811171889305115
train gradient:  0.11787644589546639
iteration : 10161
train acc:  0.7421875
train loss:  0.4769200086593628
train gradient:  0.12120443691717339
iteration : 10162
train acc:  0.75
train loss:  0.5005136132240295
train gradient:  0.14763769108705088
iteration : 10163
train acc:  0.7265625
train loss:  0.48840126395225525
train gradient:  0.11763030744264505
iteration : 10164
train acc:  0.6875
train loss:  0.5814402103424072
train gradient:  0.15965955406233134
iteration : 10165
train acc:  0.765625
train loss:  0.44594693183898926
train gradient:  0.09403674431349363
iteration : 10166
train acc:  0.7421875
train loss:  0.5035161375999451
train gradient:  0.11978634747272932
iteration : 10167
train acc:  0.7265625
train loss:  0.5262151956558228
train gradient:  0.12060617803079647
iteration : 10168
train acc:  0.671875
train loss:  0.5242122411727905
train gradient:  0.1399363227681905
iteration : 10169
train acc:  0.75
train loss:  0.498457670211792
train gradient:  0.1496780081204545
iteration : 10170
train acc:  0.78125
train loss:  0.461325466632843
train gradient:  0.12637811157178064
iteration : 10171
train acc:  0.7265625
train loss:  0.4723723232746124
train gradient:  0.09898447540517305
iteration : 10172
train acc:  0.7265625
train loss:  0.5389933586120605
train gradient:  0.1589611934216394
iteration : 10173
train acc:  0.7734375
train loss:  0.5202164053916931
train gradient:  0.16603265529319677
iteration : 10174
train acc:  0.65625
train loss:  0.5516765117645264
train gradient:  0.13909808843406968
iteration : 10175
train acc:  0.7734375
train loss:  0.44928500056266785
train gradient:  0.10233663983886161
iteration : 10176
train acc:  0.78125
train loss:  0.43680110573768616
train gradient:  0.09767415384894045
iteration : 10177
train acc:  0.7890625
train loss:  0.4928041696548462
train gradient:  0.12276868406249765
iteration : 10178
train acc:  0.640625
train loss:  0.5388392210006714
train gradient:  0.1786010710165264
iteration : 10179
train acc:  0.765625
train loss:  0.4444929361343384
train gradient:  0.11746354031713531
iteration : 10180
train acc:  0.765625
train loss:  0.4628332257270813
train gradient:  0.12153390831790474
iteration : 10181
train acc:  0.8203125
train loss:  0.43774670362472534
train gradient:  0.11424573421715684
iteration : 10182
train acc:  0.734375
train loss:  0.4854232668876648
train gradient:  0.1238337183526147
iteration : 10183
train acc:  0.7421875
train loss:  0.4606427550315857
train gradient:  0.13732165424751602
iteration : 10184
train acc:  0.8046875
train loss:  0.46551305055618286
train gradient:  0.12401063075737531
iteration : 10185
train acc:  0.75
train loss:  0.4704950153827667
train gradient:  0.09411411079967265
iteration : 10186
train acc:  0.71875
train loss:  0.5047858953475952
train gradient:  0.11512730657478532
iteration : 10187
train acc:  0.7578125
train loss:  0.4786502420902252
train gradient:  0.1486567786134962
iteration : 10188
train acc:  0.765625
train loss:  0.48921406269073486
train gradient:  0.11730355679977675
iteration : 10189
train acc:  0.8125
train loss:  0.45581310987472534
train gradient:  0.1801099505006568
iteration : 10190
train acc:  0.7265625
train loss:  0.560163140296936
train gradient:  0.13069401222311638
iteration : 10191
train acc:  0.765625
train loss:  0.5066884756088257
train gradient:  0.16270521128417625
iteration : 10192
train acc:  0.7421875
train loss:  0.5034277439117432
train gradient:  0.16967110364298618
iteration : 10193
train acc:  0.765625
train loss:  0.4841623902320862
train gradient:  0.09796673284119299
iteration : 10194
train acc:  0.703125
train loss:  0.5516675710678101
train gradient:  0.16855322338387907
iteration : 10195
train acc:  0.765625
train loss:  0.4448119103908539
train gradient:  0.10746172096548533
iteration : 10196
train acc:  0.75
train loss:  0.49934273958206177
train gradient:  0.11734302899509151
iteration : 10197
train acc:  0.78125
train loss:  0.45971667766571045
train gradient:  0.10841065503457059
iteration : 10198
train acc:  0.7734375
train loss:  0.49187061190605164
train gradient:  0.10701338507796064
iteration : 10199
train acc:  0.7265625
train loss:  0.5838412046432495
train gradient:  0.14674791298816228
iteration : 10200
train acc:  0.78125
train loss:  0.44178301095962524
train gradient:  0.10476720401484531
iteration : 10201
train acc:  0.6953125
train loss:  0.5241844058036804
train gradient:  0.1380274287337276
iteration : 10202
train acc:  0.7578125
train loss:  0.4540260136127472
train gradient:  0.1057415261780342
iteration : 10203
train acc:  0.7265625
train loss:  0.48101329803466797
train gradient:  0.105631133618105
iteration : 10204
train acc:  0.703125
train loss:  0.5395454168319702
train gradient:  0.12102141872360868
iteration : 10205
train acc:  0.7734375
train loss:  0.45360124111175537
train gradient:  0.1264466819629293
iteration : 10206
train acc:  0.78125
train loss:  0.45945239067077637
train gradient:  0.14184619280611888
iteration : 10207
train acc:  0.7109375
train loss:  0.5582218170166016
train gradient:  0.17125893078329665
iteration : 10208
train acc:  0.6875
train loss:  0.5118265151977539
train gradient:  0.15618300171567884
iteration : 10209
train acc:  0.75
train loss:  0.4761373996734619
train gradient:  0.11592803997280597
iteration : 10210
train acc:  0.7109375
train loss:  0.5192595720291138
train gradient:  0.12225234956996137
iteration : 10211
train acc:  0.6875
train loss:  0.5206847190856934
train gradient:  0.16803765618324085
iteration : 10212
train acc:  0.7734375
train loss:  0.46445974707603455
train gradient:  0.10154129977631635
iteration : 10213
train acc:  0.7109375
train loss:  0.47152748703956604
train gradient:  0.11216544541931503
iteration : 10214
train acc:  0.8046875
train loss:  0.5228003263473511
train gradient:  0.13881120846669323
iteration : 10215
train acc:  0.8515625
train loss:  0.42793262004852295
train gradient:  0.12580829195629575
iteration : 10216
train acc:  0.7578125
train loss:  0.48829296231269836
train gradient:  0.11776978964049291
iteration : 10217
train acc:  0.71875
train loss:  0.4749869704246521
train gradient:  0.1126964764220293
iteration : 10218
train acc:  0.6796875
train loss:  0.48604249954223633
train gradient:  0.12844504741841173
iteration : 10219
train acc:  0.7109375
train loss:  0.5241104364395142
train gradient:  0.14373271004845428
iteration : 10220
train acc:  0.6875
train loss:  0.572597086429596
train gradient:  0.13570816743147884
iteration : 10221
train acc:  0.7265625
train loss:  0.48885899782180786
train gradient:  0.16226387462119457
iteration : 10222
train acc:  0.75
train loss:  0.5060076713562012
train gradient:  0.12083204004491842
iteration : 10223
train acc:  0.6875
train loss:  0.5330420136451721
train gradient:  0.13109808025310238
iteration : 10224
train acc:  0.75
train loss:  0.5085686445236206
train gradient:  0.11439569265952544
iteration : 10225
train acc:  0.7421875
train loss:  0.4744165241718292
train gradient:  0.15584759576524165
iteration : 10226
train acc:  0.71875
train loss:  0.5036370754241943
train gradient:  0.13011070200295335
iteration : 10227
train acc:  0.7578125
train loss:  0.4713328778743744
train gradient:  0.1646923795498354
iteration : 10228
train acc:  0.7109375
train loss:  0.5367532968521118
train gradient:  0.12012897361908911
iteration : 10229
train acc:  0.7265625
train loss:  0.5154724717140198
train gradient:  0.130877964609768
iteration : 10230
train acc:  0.671875
train loss:  0.5162785649299622
train gradient:  0.17898160332291174
iteration : 10231
train acc:  0.7578125
train loss:  0.4983721077442169
train gradient:  0.13381426652913098
iteration : 10232
train acc:  0.703125
train loss:  0.551099419593811
train gradient:  0.19494706949952934
iteration : 10233
train acc:  0.71875
train loss:  0.5185439586639404
train gradient:  0.1221173119240021
iteration : 10234
train acc:  0.71875
train loss:  0.5159841775894165
train gradient:  0.16646691151041887
iteration : 10235
train acc:  0.7890625
train loss:  0.4524708390235901
train gradient:  0.107010333467797
iteration : 10236
train acc:  0.7109375
train loss:  0.5160543918609619
train gradient:  0.09650535603466426
iteration : 10237
train acc:  0.7421875
train loss:  0.4598727226257324
train gradient:  0.10775987126048714
iteration : 10238
train acc:  0.703125
train loss:  0.5274333953857422
train gradient:  0.18528467722800263
iteration : 10239
train acc:  0.8125
train loss:  0.45235562324523926
train gradient:  0.1053865753055016
iteration : 10240
train acc:  0.75
train loss:  0.5071024298667908
train gradient:  0.1450814600026759
iteration : 10241
train acc:  0.75
train loss:  0.5078038573265076
train gradient:  0.13079664473549457
iteration : 10242
train acc:  0.71875
train loss:  0.5143485069274902
train gradient:  0.12814760795857832
iteration : 10243
train acc:  0.65625
train loss:  0.542419970035553
train gradient:  0.13764432983896271
iteration : 10244
train acc:  0.7109375
train loss:  0.5106270909309387
train gradient:  0.1300271499649968
iteration : 10245
train acc:  0.75
train loss:  0.5234947204589844
train gradient:  0.1421603452109343
iteration : 10246
train acc:  0.6875
train loss:  0.5092509984970093
train gradient:  0.14007991097258088
iteration : 10247
train acc:  0.7421875
train loss:  0.4948318600654602
train gradient:  0.11069000303774615
iteration : 10248
train acc:  0.7578125
train loss:  0.47912538051605225
train gradient:  0.12570385790940036
iteration : 10249
train acc:  0.7421875
train loss:  0.5304073095321655
train gradient:  0.13015139986640084
iteration : 10250
train acc:  0.703125
train loss:  0.5387482643127441
train gradient:  0.15796419899697933
iteration : 10251
train acc:  0.75
train loss:  0.4629588723182678
train gradient:  0.1382053256922708
iteration : 10252
train acc:  0.734375
train loss:  0.4736926555633545
train gradient:  0.1417718993721785
iteration : 10253
train acc:  0.7265625
train loss:  0.49619707465171814
train gradient:  0.11246270063192881
iteration : 10254
train acc:  0.75
train loss:  0.4588703513145447
train gradient:  0.11710987499701332
iteration : 10255
train acc:  0.765625
train loss:  0.5215520858764648
train gradient:  0.11584302876053576
iteration : 10256
train acc:  0.796875
train loss:  0.4495985209941864
train gradient:  0.110335034790135
iteration : 10257
train acc:  0.78125
train loss:  0.4574573040008545
train gradient:  0.11134345569704703
iteration : 10258
train acc:  0.734375
train loss:  0.5258234739303589
train gradient:  0.17727066207108794
iteration : 10259
train acc:  0.7421875
train loss:  0.47513335943222046
train gradient:  0.10410103558577928
iteration : 10260
train acc:  0.78125
train loss:  0.48621171712875366
train gradient:  0.13809145189834504
iteration : 10261
train acc:  0.7421875
train loss:  0.48320090770721436
train gradient:  0.11411042379115452
iteration : 10262
train acc:  0.75
train loss:  0.4710204005241394
train gradient:  0.15888250423385333
iteration : 10263
train acc:  0.71875
train loss:  0.5189511775970459
train gradient:  0.13866793955514584
iteration : 10264
train acc:  0.703125
train loss:  0.543151319026947
train gradient:  0.1568891484780871
iteration : 10265
train acc:  0.703125
train loss:  0.5609508156776428
train gradient:  0.14193409304646387
iteration : 10266
train acc:  0.703125
train loss:  0.5183154344558716
train gradient:  0.10971494214149943
iteration : 10267
train acc:  0.7578125
train loss:  0.4621690511703491
train gradient:  0.09305881193534947
iteration : 10268
train acc:  0.7421875
train loss:  0.5115729570388794
train gradient:  0.11638533158237367
iteration : 10269
train acc:  0.734375
train loss:  0.48355382680892944
train gradient:  0.13041071570484356
iteration : 10270
train acc:  0.7421875
train loss:  0.49527454376220703
train gradient:  0.14056955130758292
iteration : 10271
train acc:  0.7265625
train loss:  0.4884350895881653
train gradient:  0.1379883085507766
iteration : 10272
train acc:  0.6796875
train loss:  0.6272038817405701
train gradient:  0.22274863670012046
iteration : 10273
train acc:  0.8125
train loss:  0.47437983751296997
train gradient:  0.11671745326084136
iteration : 10274
train acc:  0.7734375
train loss:  0.455727219581604
train gradient:  0.10082437120942128
iteration : 10275
train acc:  0.7109375
train loss:  0.5024750232696533
train gradient:  0.141953475533583
iteration : 10276
train acc:  0.765625
train loss:  0.4829865097999573
train gradient:  0.10568438411304698
iteration : 10277
train acc:  0.7421875
train loss:  0.4773627817630768
train gradient:  0.0990178245368854
iteration : 10278
train acc:  0.7421875
train loss:  0.5241006016731262
train gradient:  0.12382730508298848
iteration : 10279
train acc:  0.796875
train loss:  0.3942907154560089
train gradient:  0.08686555262816419
iteration : 10280
train acc:  0.7890625
train loss:  0.438751220703125
train gradient:  0.09764704514079979
iteration : 10281
train acc:  0.7734375
train loss:  0.48210644721984863
train gradient:  0.11509370629498782
iteration : 10282
train acc:  0.7109375
train loss:  0.5370347499847412
train gradient:  0.17501645771788293
iteration : 10283
train acc:  0.7578125
train loss:  0.5020126104354858
train gradient:  0.10725524860053892
iteration : 10284
train acc:  0.796875
train loss:  0.477054238319397
train gradient:  0.11757248215148199
iteration : 10285
train acc:  0.6875
train loss:  0.5570180416107178
train gradient:  0.13734198704559836
iteration : 10286
train acc:  0.734375
train loss:  0.518610954284668
train gradient:  0.13754591824899187
iteration : 10287
train acc:  0.703125
train loss:  0.5453601479530334
train gradient:  0.13064432945320964
iteration : 10288
train acc:  0.71875
train loss:  0.5586771965026855
train gradient:  0.204290129341976
iteration : 10289
train acc:  0.7421875
train loss:  0.5037080645561218
train gradient:  0.10560082740346126
iteration : 10290
train acc:  0.7890625
train loss:  0.4517447352409363
train gradient:  0.09994089039565832
iteration : 10291
train acc:  0.7578125
train loss:  0.46301135420799255
train gradient:  0.11070273324731494
iteration : 10292
train acc:  0.765625
train loss:  0.45210015773773193
train gradient:  0.13962770509024675
iteration : 10293
train acc:  0.734375
train loss:  0.4854091703891754
train gradient:  0.0966795654872253
iteration : 10294
train acc:  0.6953125
train loss:  0.5611312985420227
train gradient:  0.15892170670128514
iteration : 10295
train acc:  0.703125
train loss:  0.5147749781608582
train gradient:  0.12754837370061575
iteration : 10296
train acc:  0.84375
train loss:  0.4030170440673828
train gradient:  0.09446067710790941
iteration : 10297
train acc:  0.734375
train loss:  0.45648670196533203
train gradient:  0.10407546894679803
iteration : 10298
train acc:  0.6875
train loss:  0.5142272710800171
train gradient:  0.12649737255275145
iteration : 10299
train acc:  0.6953125
train loss:  0.5341548323631287
train gradient:  0.20267125010012738
iteration : 10300
train acc:  0.7578125
train loss:  0.4457629323005676
train gradient:  0.09391675367195161
iteration : 10301
train acc:  0.7265625
train loss:  0.5213385224342346
train gradient:  0.13033193071280158
iteration : 10302
train acc:  0.765625
train loss:  0.4650024175643921
train gradient:  0.10796481692234305
iteration : 10303
train acc:  0.703125
train loss:  0.5209391713142395
train gradient:  0.1064891267613044
iteration : 10304
train acc:  0.7578125
train loss:  0.4581298232078552
train gradient:  0.1406923123027244
iteration : 10305
train acc:  0.7265625
train loss:  0.46484774351119995
train gradient:  0.11488701926686412
iteration : 10306
train acc:  0.703125
train loss:  0.5055847764015198
train gradient:  0.12494627812800208
iteration : 10307
train acc:  0.71875
train loss:  0.4904586374759674
train gradient:  0.09366603107176948
iteration : 10308
train acc:  0.6875
train loss:  0.5110920071601868
train gradient:  0.14733259111464414
iteration : 10309
train acc:  0.8203125
train loss:  0.42348581552505493
train gradient:  0.08812872375420125
iteration : 10310
train acc:  0.765625
train loss:  0.4494164288043976
train gradient:  0.08714368312780714
iteration : 10311
train acc:  0.7734375
train loss:  0.4557061791419983
train gradient:  0.10341597942792842
iteration : 10312
train acc:  0.796875
train loss:  0.47636955976486206
train gradient:  0.10641708136888321
iteration : 10313
train acc:  0.75
train loss:  0.5173498392105103
train gradient:  0.13197709777170338
iteration : 10314
train acc:  0.703125
train loss:  0.5224541425704956
train gradient:  0.12869341915634014
iteration : 10315
train acc:  0.7578125
train loss:  0.48355406522750854
train gradient:  0.14100021577268385
iteration : 10316
train acc:  0.7890625
train loss:  0.4507805109024048
train gradient:  0.11174878327533931
iteration : 10317
train acc:  0.6796875
train loss:  0.5353151559829712
train gradient:  0.14401019276450028
iteration : 10318
train acc:  0.703125
train loss:  0.6628631949424744
train gradient:  0.2609948040553153
iteration : 10319
train acc:  0.734375
train loss:  0.5047765374183655
train gradient:  0.12313846416351185
iteration : 10320
train acc:  0.7109375
train loss:  0.4918583929538727
train gradient:  0.10903749922708116
iteration : 10321
train acc:  0.71875
train loss:  0.510834813117981
train gradient:  0.1260835439688665
iteration : 10322
train acc:  0.796875
train loss:  0.43465566635131836
train gradient:  0.09546259270900131
iteration : 10323
train acc:  0.7421875
train loss:  0.5162531733512878
train gradient:  0.11339436902375051
iteration : 10324
train acc:  0.7578125
train loss:  0.46080631017684937
train gradient:  0.11261975887806241
iteration : 10325
train acc:  0.78125
train loss:  0.44955265522003174
train gradient:  0.09565160868176267
iteration : 10326
train acc:  0.734375
train loss:  0.46556758880615234
train gradient:  0.09637930139457875
iteration : 10327
train acc:  0.6875
train loss:  0.507867157459259
train gradient:  0.11182888889468534
iteration : 10328
train acc:  0.765625
train loss:  0.4300117790699005
train gradient:  0.0960288517027721
iteration : 10329
train acc:  0.7890625
train loss:  0.4542573392391205
train gradient:  0.10008992994766791
iteration : 10330
train acc:  0.7578125
train loss:  0.4774087369441986
train gradient:  0.11554535485655539
iteration : 10331
train acc:  0.7265625
train loss:  0.5030683279037476
train gradient:  0.11497662823610828
iteration : 10332
train acc:  0.765625
train loss:  0.4090644419193268
train gradient:  0.10715341055460854
iteration : 10333
train acc:  0.75
train loss:  0.49442657828330994
train gradient:  0.19725738143761884
iteration : 10334
train acc:  0.828125
train loss:  0.4186822175979614
train gradient:  0.08063480522444454
iteration : 10335
train acc:  0.6796875
train loss:  0.5998696088790894
train gradient:  0.18140277336700028
iteration : 10336
train acc:  0.7421875
train loss:  0.49549591541290283
train gradient:  0.1352677594829111
iteration : 10337
train acc:  0.703125
train loss:  0.5572882890701294
train gradient:  0.13062049221710675
iteration : 10338
train acc:  0.7734375
train loss:  0.48948705196380615
train gradient:  0.12402311064724002
iteration : 10339
train acc:  0.7734375
train loss:  0.4089197814464569
train gradient:  0.08828754345568364
iteration : 10340
train acc:  0.7734375
train loss:  0.46356022357940674
train gradient:  0.10471557920343999
iteration : 10341
train acc:  0.78125
train loss:  0.43393513560295105
train gradient:  0.1139122634733632
iteration : 10342
train acc:  0.75
train loss:  0.4759841561317444
train gradient:  0.11090405960062551
iteration : 10343
train acc:  0.7578125
train loss:  0.49728158116340637
train gradient:  0.12813477437572487
iteration : 10344
train acc:  0.78125
train loss:  0.4738219380378723
train gradient:  0.10675994633489423
iteration : 10345
train acc:  0.78125
train loss:  0.4403367042541504
train gradient:  0.09149117073857443
iteration : 10346
train acc:  0.7734375
train loss:  0.4634319543838501
train gradient:  0.11598194712811984
iteration : 10347
train acc:  0.7265625
train loss:  0.5001584887504578
train gradient:  0.12885863538277575
iteration : 10348
train acc:  0.6953125
train loss:  0.5310018062591553
train gradient:  0.12763456260586303
iteration : 10349
train acc:  0.671875
train loss:  0.5667430758476257
train gradient:  0.1585788925395053
iteration : 10350
train acc:  0.703125
train loss:  0.55696040391922
train gradient:  0.1768231052398999
iteration : 10351
train acc:  0.7734375
train loss:  0.4328862726688385
train gradient:  0.10640279917004211
iteration : 10352
train acc:  0.765625
train loss:  0.5280382037162781
train gradient:  0.13136746424822646
iteration : 10353
train acc:  0.8046875
train loss:  0.4305742383003235
train gradient:  0.10339748066784354
iteration : 10354
train acc:  0.765625
train loss:  0.476955771446228
train gradient:  0.0996146778035762
iteration : 10355
train acc:  0.6953125
train loss:  0.5158812999725342
train gradient:  0.12964239745211364
iteration : 10356
train acc:  0.8046875
train loss:  0.43616753816604614
train gradient:  0.10301704765059393
iteration : 10357
train acc:  0.7421875
train loss:  0.506805419921875
train gradient:  0.1330273871397852
iteration : 10358
train acc:  0.6875
train loss:  0.5499420166015625
train gradient:  0.18924712073300276
iteration : 10359
train acc:  0.7109375
train loss:  0.49320748448371887
train gradient:  0.11603345171835383
iteration : 10360
train acc:  0.734375
train loss:  0.5038919448852539
train gradient:  0.16196387153400293
iteration : 10361
train acc:  0.7265625
train loss:  0.5296961069107056
train gradient:  0.1410768166268774
iteration : 10362
train acc:  0.703125
train loss:  0.49628084897994995
train gradient:  0.1518772535570046
iteration : 10363
train acc:  0.734375
train loss:  0.49981534481048584
train gradient:  0.14409902315674278
iteration : 10364
train acc:  0.765625
train loss:  0.5041300654411316
train gradient:  0.1718128513549741
iteration : 10365
train acc:  0.7578125
train loss:  0.5129744410514832
train gradient:  0.13137661157961705
iteration : 10366
train acc:  0.7890625
train loss:  0.48545655608177185
train gradient:  0.11319223204512528
iteration : 10367
train acc:  0.734375
train loss:  0.5124256610870361
train gradient:  0.12156106997391482
iteration : 10368
train acc:  0.7890625
train loss:  0.49178773164749146
train gradient:  0.16859484669393074
iteration : 10369
train acc:  0.7421875
train loss:  0.49952054023742676
train gradient:  0.1237337085190336
iteration : 10370
train acc:  0.7578125
train loss:  0.473697304725647
train gradient:  0.08926183615934813
iteration : 10371
train acc:  0.734375
train loss:  0.47306111454963684
train gradient:  0.10305192444577967
iteration : 10372
train acc:  0.703125
train loss:  0.5202472805976868
train gradient:  0.14182085608539072
iteration : 10373
train acc:  0.75
train loss:  0.45640748739242554
train gradient:  0.09925068256444984
iteration : 10374
train acc:  0.75
train loss:  0.48270130157470703
train gradient:  0.12036729053332403
iteration : 10375
train acc:  0.734375
train loss:  0.457873672246933
train gradient:  0.11912654414526398
iteration : 10376
train acc:  0.71875
train loss:  0.5132191181182861
train gradient:  0.10801065827231937
iteration : 10377
train acc:  0.734375
train loss:  0.4772144854068756
train gradient:  0.119380637049759
iteration : 10378
train acc:  0.7578125
train loss:  0.48895329236984253
train gradient:  0.12538098289022198
iteration : 10379
train acc:  0.78125
train loss:  0.4719070792198181
train gradient:  0.09068900436578307
iteration : 10380
train acc:  0.796875
train loss:  0.49080511927604675
train gradient:  0.1438199944182691
iteration : 10381
train acc:  0.6875
train loss:  0.5226665735244751
train gradient:  0.11664381091405784
iteration : 10382
train acc:  0.75
train loss:  0.49832847714424133
train gradient:  0.1236933705677687
iteration : 10383
train acc:  0.7578125
train loss:  0.5048813819885254
train gradient:  0.11580896642039087
iteration : 10384
train acc:  0.734375
train loss:  0.49231159687042236
train gradient:  0.11937517837533973
iteration : 10385
train acc:  0.7421875
train loss:  0.4995768666267395
train gradient:  0.11382610292016863
iteration : 10386
train acc:  0.7265625
train loss:  0.5577670931816101
train gradient:  0.1452870226692033
iteration : 10387
train acc:  0.7578125
train loss:  0.45519763231277466
train gradient:  0.11459737376043255
iteration : 10388
train acc:  0.6875
train loss:  0.5758475661277771
train gradient:  0.18235896583808633
iteration : 10389
train acc:  0.7734375
train loss:  0.44685983657836914
train gradient:  0.0887028940852835
iteration : 10390
train acc:  0.71875
train loss:  0.5299857258796692
train gradient:  0.16032522101348315
iteration : 10391
train acc:  0.765625
train loss:  0.46088695526123047
train gradient:  0.09099842693323386
iteration : 10392
train acc:  0.7890625
train loss:  0.4365118145942688
train gradient:  0.1308087398675114
iteration : 10393
train acc:  0.7734375
train loss:  0.4199809432029724
train gradient:  0.09733877240532637
iteration : 10394
train acc:  0.7265625
train loss:  0.4727870225906372
train gradient:  0.12335408575440497
iteration : 10395
train acc:  0.734375
train loss:  0.5212435722351074
train gradient:  0.13454192977732343
iteration : 10396
train acc:  0.7421875
train loss:  0.4573875665664673
train gradient:  0.10897853238761424
iteration : 10397
train acc:  0.75
train loss:  0.4952925443649292
train gradient:  0.11252588085817729
iteration : 10398
train acc:  0.7421875
train loss:  0.48578301072120667
train gradient:  0.1305918670683451
iteration : 10399
train acc:  0.7265625
train loss:  0.5175417065620422
train gradient:  0.1301024666623457
iteration : 10400
train acc:  0.6953125
train loss:  0.5620898604393005
train gradient:  0.16177997438341862
iteration : 10401
train acc:  0.7734375
train loss:  0.44714677333831787
train gradient:  0.12838275055442844
iteration : 10402
train acc:  0.78125
train loss:  0.42306753993034363
train gradient:  0.0986066037151511
iteration : 10403
train acc:  0.75
train loss:  0.48064079880714417
train gradient:  0.10810812123737935
iteration : 10404
train acc:  0.703125
train loss:  0.5089789628982544
train gradient:  0.12002065744784382
iteration : 10405
train acc:  0.7734375
train loss:  0.49827635288238525
train gradient:  0.13146769222436552
iteration : 10406
train acc:  0.8671875
train loss:  0.42038780450820923
train gradient:  0.09898561191755335
iteration : 10407
train acc:  0.71875
train loss:  0.5051072239875793
train gradient:  0.13878888480764953
iteration : 10408
train acc:  0.703125
train loss:  0.5721954703330994
train gradient:  0.16513953339893084
iteration : 10409
train acc:  0.7734375
train loss:  0.4321664571762085
train gradient:  0.09244381596753645
iteration : 10410
train acc:  0.7578125
train loss:  0.5486671924591064
train gradient:  0.2538225577005012
iteration : 10411
train acc:  0.7265625
train loss:  0.48981860280036926
train gradient:  0.11453534674820076
iteration : 10412
train acc:  0.6328125
train loss:  0.6095651388168335
train gradient:  0.17649640245237547
iteration : 10413
train acc:  0.8125
train loss:  0.4365134835243225
train gradient:  0.09176108986532669
iteration : 10414
train acc:  0.71875
train loss:  0.5154364109039307
train gradient:  0.15433361418002384
iteration : 10415
train acc:  0.7578125
train loss:  0.4965657889842987
train gradient:  0.12101818053242881
iteration : 10416
train acc:  0.7890625
train loss:  0.4868554472923279
train gradient:  0.12284234225397786
iteration : 10417
train acc:  0.6953125
train loss:  0.5516626834869385
train gradient:  0.14194784139977062
iteration : 10418
train acc:  0.71875
train loss:  0.493386447429657
train gradient:  0.1505538327828237
iteration : 10419
train acc:  0.78125
train loss:  0.49091753363609314
train gradient:  0.12580733860526822
iteration : 10420
train acc:  0.734375
train loss:  0.4896998405456543
train gradient:  0.10954480325698573
iteration : 10421
train acc:  0.6796875
train loss:  0.5252756476402283
train gradient:  0.1531101106990313
iteration : 10422
train acc:  0.765625
train loss:  0.46464186906814575
train gradient:  0.10701140538362885
iteration : 10423
train acc:  0.765625
train loss:  0.45378196239471436
train gradient:  0.10802486064655516
iteration : 10424
train acc:  0.765625
train loss:  0.4728308916091919
train gradient:  0.10511635398211504
iteration : 10425
train acc:  0.703125
train loss:  0.563176155090332
train gradient:  0.1602578834321896
iteration : 10426
train acc:  0.765625
train loss:  0.44423750042915344
train gradient:  0.10867679780907687
iteration : 10427
train acc:  0.75
train loss:  0.4545658826828003
train gradient:  0.09775211245338525
iteration : 10428
train acc:  0.796875
train loss:  0.46962490677833557
train gradient:  0.10249346477364644
iteration : 10429
train acc:  0.7421875
train loss:  0.4921029210090637
train gradient:  0.10150571995518028
iteration : 10430
train acc:  0.8203125
train loss:  0.46665459871292114
train gradient:  0.11466707461329381
iteration : 10431
train acc:  0.7421875
train loss:  0.5077791810035706
train gradient:  0.13076882288058733
iteration : 10432
train acc:  0.671875
train loss:  0.5323291420936584
train gradient:  0.1136841962141528
iteration : 10433
train acc:  0.75
train loss:  0.4815293252468109
train gradient:  0.14350171264124095
iteration : 10434
train acc:  0.7734375
train loss:  0.43761488795280457
train gradient:  0.09237218524316979
iteration : 10435
train acc:  0.734375
train loss:  0.4768977463245392
train gradient:  0.1296901623010932
iteration : 10436
train acc:  0.7421875
train loss:  0.45650553703308105
train gradient:  0.0922150611288912
iteration : 10437
train acc:  0.703125
train loss:  0.5403253436088562
train gradient:  0.1362850138492877
iteration : 10438
train acc:  0.6796875
train loss:  0.5866256356239319
train gradient:  0.17992956426020829
iteration : 10439
train acc:  0.75
train loss:  0.49725818634033203
train gradient:  0.11709619912069905
iteration : 10440
train acc:  0.75
train loss:  0.4551907777786255
train gradient:  0.10751880271840625
iteration : 10441
train acc:  0.71875
train loss:  0.4996614456176758
train gradient:  0.11923027246440154
iteration : 10442
train acc:  0.6796875
train loss:  0.543171763420105
train gradient:  0.14836680296200916
iteration : 10443
train acc:  0.7421875
train loss:  0.48877954483032227
train gradient:  0.1275801778947782
iteration : 10444
train acc:  0.7109375
train loss:  0.5042247772216797
train gradient:  0.11447799694931703
iteration : 10445
train acc:  0.7890625
train loss:  0.4272597134113312
train gradient:  0.11675011544271517
iteration : 10446
train acc:  0.71875
train loss:  0.528710126876831
train gradient:  0.13452194245620103
iteration : 10447
train acc:  0.6796875
train loss:  0.5701290965080261
train gradient:  0.16032379049607925
iteration : 10448
train acc:  0.7578125
train loss:  0.47053247690200806
train gradient:  0.09734415631063047
iteration : 10449
train acc:  0.6640625
train loss:  0.5890896320343018
train gradient:  0.13952702463853228
iteration : 10450
train acc:  0.734375
train loss:  0.5466904640197754
train gradient:  0.11896835922549286
iteration : 10451
train acc:  0.6640625
train loss:  0.5708493590354919
train gradient:  0.17773246285035027
iteration : 10452
train acc:  0.7109375
train loss:  0.5132902264595032
train gradient:  0.12620390137497806
iteration : 10453
train acc:  0.6953125
train loss:  0.5173495411872864
train gradient:  0.10733624099143675
iteration : 10454
train acc:  0.703125
train loss:  0.5352233648300171
train gradient:  0.151379114454534
iteration : 10455
train acc:  0.6953125
train loss:  0.5626330971717834
train gradient:  0.1547353506046203
iteration : 10456
train acc:  0.6875
train loss:  0.5595126748085022
train gradient:  0.15613366248677796
iteration : 10457
train acc:  0.75
train loss:  0.45671892166137695
train gradient:  0.10094281576670926
iteration : 10458
train acc:  0.78125
train loss:  0.43733465671539307
train gradient:  0.11385916995617344
iteration : 10459
train acc:  0.6953125
train loss:  0.5058279037475586
train gradient:  0.11987504043811885
iteration : 10460
train acc:  0.75
train loss:  0.47151046991348267
train gradient:  0.08332294046465888
iteration : 10461
train acc:  0.765625
train loss:  0.47148552536964417
train gradient:  0.12311139643162533
iteration : 10462
train acc:  0.7734375
train loss:  0.4957726001739502
train gradient:  0.11804120272951915
iteration : 10463
train acc:  0.7109375
train loss:  0.5206959843635559
train gradient:  0.09883013219707923
iteration : 10464
train acc:  0.734375
train loss:  0.5708930492401123
train gradient:  0.14928117620569809
iteration : 10465
train acc:  0.7421875
train loss:  0.47016000747680664
train gradient:  0.13786566325937577
iteration : 10466
train acc:  0.7578125
train loss:  0.5509739518165588
train gradient:  0.12096503685198476
iteration : 10467
train acc:  0.7265625
train loss:  0.49240583181381226
train gradient:  0.13610576154069162
iteration : 10468
train acc:  0.71875
train loss:  0.5423959493637085
train gradient:  0.11844728206415195
iteration : 10469
train acc:  0.7890625
train loss:  0.4269471764564514
train gradient:  0.0954370835220606
iteration : 10470
train acc:  0.765625
train loss:  0.42764365673065186
train gradient:  0.09051418656454872
iteration : 10471
train acc:  0.7265625
train loss:  0.5174568891525269
train gradient:  0.14752546786808068
iteration : 10472
train acc:  0.796875
train loss:  0.4321138858795166
train gradient:  0.10365754449712483
iteration : 10473
train acc:  0.7421875
train loss:  0.48956775665283203
train gradient:  0.1350366277042553
iteration : 10474
train acc:  0.6484375
train loss:  0.5117619633674622
train gradient:  0.12780820879877836
iteration : 10475
train acc:  0.734375
train loss:  0.4947798252105713
train gradient:  0.11938065476864983
iteration : 10476
train acc:  0.765625
train loss:  0.45794564485549927
train gradient:  0.11185035528557277
iteration : 10477
train acc:  0.7578125
train loss:  0.48248374462127686
train gradient:  0.13076399451406145
iteration : 10478
train acc:  0.703125
train loss:  0.5572366714477539
train gradient:  0.1691437775586513
iteration : 10479
train acc:  0.7734375
train loss:  0.44865870475769043
train gradient:  0.09125692998750863
iteration : 10480
train acc:  0.7109375
train loss:  0.51795494556427
train gradient:  0.1556611258023085
iteration : 10481
train acc:  0.7265625
train loss:  0.5015795230865479
train gradient:  0.11617608152875884
iteration : 10482
train acc:  0.734375
train loss:  0.5249642133712769
train gradient:  0.12529862035434391
iteration : 10483
train acc:  0.71875
train loss:  0.5698540210723877
train gradient:  0.1509630347192999
iteration : 10484
train acc:  0.734375
train loss:  0.4704584777355194
train gradient:  0.10498203800600377
iteration : 10485
train acc:  0.7734375
train loss:  0.4218260645866394
train gradient:  0.11001405847453549
iteration : 10486
train acc:  0.7734375
train loss:  0.48946496844291687
train gradient:  0.12074857563044271
iteration : 10487
train acc:  0.6953125
train loss:  0.5121662020683289
train gradient:  0.13326479380735623
iteration : 10488
train acc:  0.7578125
train loss:  0.494344025850296
train gradient:  0.1144729041967547
iteration : 10489
train acc:  0.734375
train loss:  0.4701020419597626
train gradient:  0.10558552523256472
iteration : 10490
train acc:  0.765625
train loss:  0.4699484407901764
train gradient:  0.09857683549809186
iteration : 10491
train acc:  0.7578125
train loss:  0.5012118816375732
train gradient:  0.13187062428014823
iteration : 10492
train acc:  0.71875
train loss:  0.5148631930351257
train gradient:  0.13945676132030937
iteration : 10493
train acc:  0.796875
train loss:  0.4705250561237335
train gradient:  0.11785637552692693
iteration : 10494
train acc:  0.7421875
train loss:  0.4748464524745941
train gradient:  0.12094652745551825
iteration : 10495
train acc:  0.78125
train loss:  0.47568851709365845
train gradient:  0.10494376158542464
iteration : 10496
train acc:  0.765625
train loss:  0.47608232498168945
train gradient:  0.11197695524177542
iteration : 10497
train acc:  0.703125
train loss:  0.5249885320663452
train gradient:  0.14444603843194087
iteration : 10498
train acc:  0.671875
train loss:  0.5223376750946045
train gradient:  0.13642708123776426
iteration : 10499
train acc:  0.734375
train loss:  0.5208460092544556
train gradient:  0.13435199014716373
iteration : 10500
train acc:  0.7421875
train loss:  0.5205121040344238
train gradient:  0.1268173303657004
iteration : 10501
train acc:  0.7421875
train loss:  0.47485458850860596
train gradient:  0.1287879904832192
iteration : 10502
train acc:  0.703125
train loss:  0.53545081615448
train gradient:  0.15965269618362987
iteration : 10503
train acc:  0.75
train loss:  0.521738588809967
train gradient:  0.10648758848309531
iteration : 10504
train acc:  0.7265625
train loss:  0.49065887928009033
train gradient:  0.10679873346168949
iteration : 10505
train acc:  0.6953125
train loss:  0.5694790482521057
train gradient:  0.13872135758612963
iteration : 10506
train acc:  0.6484375
train loss:  0.5398646593093872
train gradient:  0.17152757180896663
iteration : 10507
train acc:  0.7421875
train loss:  0.48273009061813354
train gradient:  0.11461824370017361
iteration : 10508
train acc:  0.7421875
train loss:  0.4947936236858368
train gradient:  0.13611293648472733
iteration : 10509
train acc:  0.6875
train loss:  0.5629786849021912
train gradient:  0.142018963979056
iteration : 10510
train acc:  0.8125
train loss:  0.47211143374443054
train gradient:  0.13422913736592515
iteration : 10511
train acc:  0.8046875
train loss:  0.43230584263801575
train gradient:  0.09649772612652383
iteration : 10512
train acc:  0.7265625
train loss:  0.5154511332511902
train gradient:  0.12778701228188907
iteration : 10513
train acc:  0.75
train loss:  0.48030930757522583
train gradient:  0.09270242742279577
iteration : 10514
train acc:  0.7890625
train loss:  0.45607203245162964
train gradient:  0.09711131188042187
iteration : 10515
train acc:  0.75
train loss:  0.493806391954422
train gradient:  0.11997056847464868
iteration : 10516
train acc:  0.78125
train loss:  0.4333328604698181
train gradient:  0.09305381786831561
iteration : 10517
train acc:  0.8203125
train loss:  0.43382275104522705
train gradient:  0.08185366346278738
iteration : 10518
train acc:  0.6796875
train loss:  0.525545597076416
train gradient:  0.11935764851061674
iteration : 10519
train acc:  0.75
train loss:  0.48213210701942444
train gradient:  0.09795110503551559
iteration : 10520
train acc:  0.75
train loss:  0.4445018768310547
train gradient:  0.09389366370061503
iteration : 10521
train acc:  0.8046875
train loss:  0.4478464126586914
train gradient:  0.11544351741339777
iteration : 10522
train acc:  0.7265625
train loss:  0.5466579794883728
train gradient:  0.13328297115435972
iteration : 10523
train acc:  0.8046875
train loss:  0.4187367558479309
train gradient:  0.0821532825911078
iteration : 10524
train acc:  0.796875
train loss:  0.4388876259326935
train gradient:  0.1257679221002349
iteration : 10525
train acc:  0.734375
train loss:  0.4610932469367981
train gradient:  0.106516127400883
iteration : 10526
train acc:  0.796875
train loss:  0.44655054807662964
train gradient:  0.1028541134300421
iteration : 10527
train acc:  0.7265625
train loss:  0.4739624559879303
train gradient:  0.10366454035556033
iteration : 10528
train acc:  0.8046875
train loss:  0.4121251702308655
train gradient:  0.08139591210604728
iteration : 10529
train acc:  0.78125
train loss:  0.48883599042892456
train gradient:  0.11596687431073006
iteration : 10530
train acc:  0.7578125
train loss:  0.4720962643623352
train gradient:  0.10626368392838748
iteration : 10531
train acc:  0.671875
train loss:  0.4929201006889343
train gradient:  0.1325929175873147
iteration : 10532
train acc:  0.7578125
train loss:  0.4644128978252411
train gradient:  0.09715592667729876
iteration : 10533
train acc:  0.734375
train loss:  0.5222166180610657
train gradient:  0.12149774525350876
iteration : 10534
train acc:  0.671875
train loss:  0.5473189353942871
train gradient:  0.13937604564252049
iteration : 10535
train acc:  0.7890625
train loss:  0.44368311762809753
train gradient:  0.09360993836030738
iteration : 10536
train acc:  0.734375
train loss:  0.4783939719200134
train gradient:  0.12874052013268578
iteration : 10537
train acc:  0.765625
train loss:  0.4677599370479584
train gradient:  0.1296233020414606
iteration : 10538
train acc:  0.7734375
train loss:  0.44959908723831177
train gradient:  0.08849413493558442
iteration : 10539
train acc:  0.78125
train loss:  0.4372367262840271
train gradient:  0.09420747535667329
iteration : 10540
train acc:  0.765625
train loss:  0.4758123457431793
train gradient:  0.15095137016692095
iteration : 10541
train acc:  0.7734375
train loss:  0.44635096192359924
train gradient:  0.09808486928786322
iteration : 10542
train acc:  0.7734375
train loss:  0.4442395567893982
train gradient:  0.09730431243195134
iteration : 10543
train acc:  0.78125
train loss:  0.4785692095756531
train gradient:  0.11513819985139116
iteration : 10544
train acc:  0.671875
train loss:  0.5828046798706055
train gradient:  0.13870994391063388
iteration : 10545
train acc:  0.7421875
train loss:  0.48730963468551636
train gradient:  0.12162788517115865
iteration : 10546
train acc:  0.7734375
train loss:  0.4422217607498169
train gradient:  0.10239937704145162
iteration : 10547
train acc:  0.75
train loss:  0.5002710819244385
train gradient:  0.11844837274089763
iteration : 10548
train acc:  0.7265625
train loss:  0.5351148843765259
train gradient:  0.1270829659377236
iteration : 10549
train acc:  0.796875
train loss:  0.42280516028404236
train gradient:  0.10889276625479369
iteration : 10550
train acc:  0.75
train loss:  0.5359106659889221
train gradient:  0.11830213168965538
iteration : 10551
train acc:  0.671875
train loss:  0.5368763208389282
train gradient:  0.15336243953267453
iteration : 10552
train acc:  0.78125
train loss:  0.4386483430862427
train gradient:  0.1015490413902146
iteration : 10553
train acc:  0.7578125
train loss:  0.4704887270927429
train gradient:  0.12545578845936456
iteration : 10554
train acc:  0.8125
train loss:  0.4376404881477356
train gradient:  0.0883944467776718
iteration : 10555
train acc:  0.7890625
train loss:  0.464394211769104
train gradient:  0.11024735292234858
iteration : 10556
train acc:  0.7578125
train loss:  0.4904060959815979
train gradient:  0.13302718077827358
iteration : 10557
train acc:  0.7890625
train loss:  0.42316555976867676
train gradient:  0.11882526811331068
iteration : 10558
train acc:  0.640625
train loss:  0.562105655670166
train gradient:  0.16796060213595299
iteration : 10559
train acc:  0.7578125
train loss:  0.5090439319610596
train gradient:  0.10374375957002684
iteration : 10560
train acc:  0.703125
train loss:  0.5244724154472351
train gradient:  0.13130885999893915
iteration : 10561
train acc:  0.765625
train loss:  0.43293672800064087
train gradient:  0.11140348631223279
iteration : 10562
train acc:  0.7734375
train loss:  0.4294482469558716
train gradient:  0.08531797703027243
iteration : 10563
train acc:  0.7421875
train loss:  0.4740890562534332
train gradient:  0.14564092610237606
iteration : 10564
train acc:  0.765625
train loss:  0.4981886148452759
train gradient:  0.11707021782042608
iteration : 10565
train acc:  0.7890625
train loss:  0.46896180510520935
train gradient:  0.10718833318558564
iteration : 10566
train acc:  0.75
train loss:  0.467593252658844
train gradient:  0.1307437467341594
iteration : 10567
train acc:  0.671875
train loss:  0.560606837272644
train gradient:  0.19100751886986753
iteration : 10568
train acc:  0.8359375
train loss:  0.44662633538246155
train gradient:  0.10986171365209003
iteration : 10569
train acc:  0.78125
train loss:  0.4323302209377289
train gradient:  0.08572848388543493
iteration : 10570
train acc:  0.75
train loss:  0.5072026252746582
train gradient:  0.12426889696792522
iteration : 10571
train acc:  0.7890625
train loss:  0.5004428029060364
train gradient:  0.16483606255316247
iteration : 10572
train acc:  0.7734375
train loss:  0.44880712032318115
train gradient:  0.1370544591093964
iteration : 10573
train acc:  0.7890625
train loss:  0.44951242208480835
train gradient:  0.10502017251784906
iteration : 10574
train acc:  0.75
train loss:  0.4532701373100281
train gradient:  0.09121669965041007
iteration : 10575
train acc:  0.7109375
train loss:  0.5135127305984497
train gradient:  0.12847570815674794
iteration : 10576
train acc:  0.7578125
train loss:  0.5027046203613281
train gradient:  0.1386893813466421
iteration : 10577
train acc:  0.703125
train loss:  0.526191771030426
train gradient:  0.1270579518093602
iteration : 10578
train acc:  0.7734375
train loss:  0.49849027395248413
train gradient:  0.11724714464355526
iteration : 10579
train acc:  0.8203125
train loss:  0.4167037010192871
train gradient:  0.10180050245509943
iteration : 10580
train acc:  0.75
train loss:  0.49792370200157166
train gradient:  0.11030133545778513
iteration : 10581
train acc:  0.71875
train loss:  0.5197914838790894
train gradient:  0.1175185604657312
iteration : 10582
train acc:  0.75
train loss:  0.49106013774871826
train gradient:  0.12084093603985337
iteration : 10583
train acc:  0.75
train loss:  0.4501846432685852
train gradient:  0.07178883884263998
iteration : 10584
train acc:  0.78125
train loss:  0.4513804316520691
train gradient:  0.1234164188525031
iteration : 10585
train acc:  0.734375
train loss:  0.47604435682296753
train gradient:  0.1173072475246978
iteration : 10586
train acc:  0.796875
train loss:  0.46400707960128784
train gradient:  0.10492220966898623
iteration : 10587
train acc:  0.71875
train loss:  0.4593137800693512
train gradient:  0.1184444528651644
iteration : 10588
train acc:  0.7578125
train loss:  0.4686015248298645
train gradient:  0.11154833130286382
iteration : 10589
train acc:  0.7890625
train loss:  0.4750991463661194
train gradient:  0.13533460226285662
iteration : 10590
train acc:  0.65625
train loss:  0.5934549570083618
train gradient:  0.2201940179734047
iteration : 10591
train acc:  0.703125
train loss:  0.5299087166786194
train gradient:  0.1446774918622017
iteration : 10592
train acc:  0.734375
train loss:  0.5031746625900269
train gradient:  0.11922073770879937
iteration : 10593
train acc:  0.765625
train loss:  0.4667697250843048
train gradient:  0.12485476675998249
iteration : 10594
train acc:  0.8125
train loss:  0.42852291464805603
train gradient:  0.09529981427492529
iteration : 10595
train acc:  0.6953125
train loss:  0.5374441146850586
train gradient:  0.15415801024979436
iteration : 10596
train acc:  0.7265625
train loss:  0.4724876284599304
train gradient:  0.11953556423053281
iteration : 10597
train acc:  0.765625
train loss:  0.4944295585155487
train gradient:  0.1668186143899174
iteration : 10598
train acc:  0.7265625
train loss:  0.5400828719139099
train gradient:  0.10557564736506399
iteration : 10599
train acc:  0.75
train loss:  0.5075920820236206
train gradient:  0.15908528130537308
iteration : 10600
train acc:  0.671875
train loss:  0.6131525039672852
train gradient:  0.15011149996796355
iteration : 10601
train acc:  0.7578125
train loss:  0.46101313829421997
train gradient:  0.1261898759599299
iteration : 10602
train acc:  0.734375
train loss:  0.47734764218330383
train gradient:  0.10063136531356591
iteration : 10603
train acc:  0.7421875
train loss:  0.5036477446556091
train gradient:  0.12438782016950115
iteration : 10604
train acc:  0.7109375
train loss:  0.5016336441040039
train gradient:  0.12132742791891847
iteration : 10605
train acc:  0.7109375
train loss:  0.5165760517120361
train gradient:  0.1363742225068369
iteration : 10606
train acc:  0.7265625
train loss:  0.5260831117630005
train gradient:  0.13483732980637567
iteration : 10607
train acc:  0.796875
train loss:  0.4942272901535034
train gradient:  0.13430180336661857
iteration : 10608
train acc:  0.71875
train loss:  0.5197691321372986
train gradient:  0.11624598637328475
iteration : 10609
train acc:  0.734375
train loss:  0.48252880573272705
train gradient:  0.12926283993717497
iteration : 10610
train acc:  0.703125
train loss:  0.5690815448760986
train gradient:  0.14236606148497005
iteration : 10611
train acc:  0.6875
train loss:  0.5311377048492432
train gradient:  0.14097954793079998
iteration : 10612
train acc:  0.765625
train loss:  0.45795607566833496
train gradient:  0.11975349135441697
iteration : 10613
train acc:  0.7890625
train loss:  0.4781233072280884
train gradient:  0.11551132519242398
iteration : 10614
train acc:  0.8125
train loss:  0.4341944456100464
train gradient:  0.1273687375886706
iteration : 10615
train acc:  0.7109375
train loss:  0.5183006525039673
train gradient:  0.13792437109372568
iteration : 10616
train acc:  0.78125
train loss:  0.47132548689842224
train gradient:  0.14964587118346115
iteration : 10617
train acc:  0.765625
train loss:  0.5129083395004272
train gradient:  0.13453166902039893
iteration : 10618
train acc:  0.7421875
train loss:  0.492704838514328
train gradient:  0.12556229734405733
iteration : 10619
train acc:  0.7265625
train loss:  0.4749198257923126
train gradient:  0.11851118009774537
iteration : 10620
train acc:  0.796875
train loss:  0.43809807300567627
train gradient:  0.10450603279086185
iteration : 10621
train acc:  0.765625
train loss:  0.4291650354862213
train gradient:  0.11380799802278413
iteration : 10622
train acc:  0.734375
train loss:  0.49218228459358215
train gradient:  0.11787665158737065
iteration : 10623
train acc:  0.71875
train loss:  0.49187415838241577
train gradient:  0.11057634888895577
iteration : 10624
train acc:  0.765625
train loss:  0.45524492859840393
train gradient:  0.10007760552196221
iteration : 10625
train acc:  0.6875
train loss:  0.5036163330078125
train gradient:  0.1505992476204811
iteration : 10626
train acc:  0.7109375
train loss:  0.4715508818626404
train gradient:  0.10474570193415272
iteration : 10627
train acc:  0.7734375
train loss:  0.483938068151474
train gradient:  0.18240376785416235
iteration : 10628
train acc:  0.65625
train loss:  0.5800886154174805
train gradient:  0.14865447100701365
iteration : 10629
train acc:  0.78125
train loss:  0.4451008439064026
train gradient:  0.10086965458793005
iteration : 10630
train acc:  0.8515625
train loss:  0.3751474916934967
train gradient:  0.08967726260418164
iteration : 10631
train acc:  0.8515625
train loss:  0.37052929401397705
train gradient:  0.10990223699302783
iteration : 10632
train acc:  0.6875
train loss:  0.5526934862136841
train gradient:  0.15383539930679468
iteration : 10633
train acc:  0.6875
train loss:  0.5404950976371765
train gradient:  0.11988162606634313
iteration : 10634
train acc:  0.71875
train loss:  0.5074096918106079
train gradient:  0.15380696799373644
iteration : 10635
train acc:  0.75
train loss:  0.4367756247520447
train gradient:  0.09552985851492594
iteration : 10636
train acc:  0.7109375
train loss:  0.538577139377594
train gradient:  0.1290742656372838
iteration : 10637
train acc:  0.7109375
train loss:  0.5621861815452576
train gradient:  0.1467955159212714
iteration : 10638
train acc:  0.75
train loss:  0.44033241271972656
train gradient:  0.10567571758482067
iteration : 10639
train acc:  0.7734375
train loss:  0.4936518669128418
train gradient:  0.10498534974876234
iteration : 10640
train acc:  0.75
train loss:  0.5110927820205688
train gradient:  0.13669023119122814
iteration : 10641
train acc:  0.7734375
train loss:  0.5061606764793396
train gradient:  0.14607738494419265
iteration : 10642
train acc:  0.71875
train loss:  0.5325993895530701
train gradient:  0.13221567037743354
iteration : 10643
train acc:  0.7734375
train loss:  0.4767943322658539
train gradient:  0.12378467688516717
iteration : 10644
train acc:  0.65625
train loss:  0.5242840051651001
train gradient:  0.14116506725258346
iteration : 10645
train acc:  0.7578125
train loss:  0.45440149307250977
train gradient:  0.09401518372207697
iteration : 10646
train acc:  0.7734375
train loss:  0.46620088815689087
train gradient:  0.0858519365915708
iteration : 10647
train acc:  0.6953125
train loss:  0.48733773827552795
train gradient:  0.13467677508526787
iteration : 10648
train acc:  0.8125
train loss:  0.4125000238418579
train gradient:  0.07548014625469858
iteration : 10649
train acc:  0.71875
train loss:  0.5335269570350647
train gradient:  0.12876674782516553
iteration : 10650
train acc:  0.78125
train loss:  0.4872782528400421
train gradient:  0.14578937118507848
iteration : 10651
train acc:  0.7421875
train loss:  0.5247174501419067
train gradient:  0.1318765321161938
iteration : 10652
train acc:  0.8046875
train loss:  0.4316985011100769
train gradient:  0.09949056716584785
iteration : 10653
train acc:  0.8046875
train loss:  0.4448522925376892
train gradient:  0.11076869082518437
iteration : 10654
train acc:  0.75
train loss:  0.45121172070503235
train gradient:  0.11041705352624469
iteration : 10655
train acc:  0.6796875
train loss:  0.5493865609169006
train gradient:  0.1709215152258855
iteration : 10656
train acc:  0.7421875
train loss:  0.5011326670646667
train gradient:  0.1355309988492463
iteration : 10657
train acc:  0.7578125
train loss:  0.49327653646469116
train gradient:  0.1159283778267381
iteration : 10658
train acc:  0.765625
train loss:  0.48511677980422974
train gradient:  0.14097081901115227
iteration : 10659
train acc:  0.7734375
train loss:  0.49448996782302856
train gradient:  0.11628815295802196
iteration : 10660
train acc:  0.7890625
train loss:  0.48229286074638367
train gradient:  0.09099759636318594
iteration : 10661
train acc:  0.7109375
train loss:  0.5187856554985046
train gradient:  0.11583750329424268
iteration : 10662
train acc:  0.7421875
train loss:  0.5371562838554382
train gradient:  0.1451290697571695
iteration : 10663
train acc:  0.71875
train loss:  0.5136885643005371
train gradient:  0.13074570718433337
iteration : 10664
train acc:  0.7890625
train loss:  0.4536477327346802
train gradient:  0.09860727260402653
iteration : 10665
train acc:  0.796875
train loss:  0.4570003151893616
train gradient:  0.11677891233091697
iteration : 10666
train acc:  0.7578125
train loss:  0.5177170038223267
train gradient:  0.13919628125136274
iteration : 10667
train acc:  0.75
train loss:  0.47253167629241943
train gradient:  0.1585126549391957
iteration : 10668
train acc:  0.7109375
train loss:  0.4799618721008301
train gradient:  0.12244287650205475
iteration : 10669
train acc:  0.765625
train loss:  0.43150895833969116
train gradient:  0.09240000299391163
iteration : 10670
train acc:  0.6328125
train loss:  0.5821524858474731
train gradient:  0.1934463387060657
iteration : 10671
train acc:  0.765625
train loss:  0.4302107095718384
train gradient:  0.10308443593377163
iteration : 10672
train acc:  0.7734375
train loss:  0.42390739917755127
train gradient:  0.08968578590100754
iteration : 10673
train acc:  0.765625
train loss:  0.500070333480835
train gradient:  0.13062245475416762
iteration : 10674
train acc:  0.7890625
train loss:  0.438659131526947
train gradient:  0.11899479174735016
iteration : 10675
train acc:  0.8046875
train loss:  0.4493103623390198
train gradient:  0.12533836102603788
iteration : 10676
train acc:  0.78125
train loss:  0.4742315411567688
train gradient:  0.11591597650804415
iteration : 10677
train acc:  0.7421875
train loss:  0.47718432545661926
train gradient:  0.11635806955212813
iteration : 10678
train acc:  0.75
train loss:  0.4758937656879425
train gradient:  0.08986959360500227
iteration : 10679
train acc:  0.7265625
train loss:  0.5236101150512695
train gradient:  0.12292651564947625
iteration : 10680
train acc:  0.7109375
train loss:  0.538011372089386
train gradient:  0.13675572665851718
iteration : 10681
train acc:  0.6875
train loss:  0.5165674686431885
train gradient:  0.10748855505805834
iteration : 10682
train acc:  0.7578125
train loss:  0.44886308908462524
train gradient:  0.113927349722719
iteration : 10683
train acc:  0.7421875
train loss:  0.5328432321548462
train gradient:  0.1468652214095377
iteration : 10684
train acc:  0.65625
train loss:  0.5447422862052917
train gradient:  0.12467984733747658
iteration : 10685
train acc:  0.78125
train loss:  0.4807710647583008
train gradient:  0.13485893048473765
iteration : 10686
train acc:  0.7265625
train loss:  0.5657743215560913
train gradient:  0.18889472250667202
iteration : 10687
train acc:  0.7578125
train loss:  0.4861866533756256
train gradient:  0.11394463705066941
iteration : 10688
train acc:  0.7578125
train loss:  0.47731563448905945
train gradient:  0.12089207484036502
iteration : 10689
train acc:  0.7734375
train loss:  0.5083456635475159
train gradient:  0.12201385099657489
iteration : 10690
train acc:  0.7109375
train loss:  0.474173903465271
train gradient:  0.09275404180775111
iteration : 10691
train acc:  0.796875
train loss:  0.4518056809902191
train gradient:  0.13593659959707222
iteration : 10692
train acc:  0.6796875
train loss:  0.5627344846725464
train gradient:  0.1386534957953431
iteration : 10693
train acc:  0.78125
train loss:  0.4597965478897095
train gradient:  0.10397819061379371
iteration : 10694
train acc:  0.75
train loss:  0.5045442581176758
train gradient:  0.15655678841892784
iteration : 10695
train acc:  0.765625
train loss:  0.5022906064987183
train gradient:  0.13626513283326908
iteration : 10696
train acc:  0.7578125
train loss:  0.5208642482757568
train gradient:  0.1398693646296274
iteration : 10697
train acc:  0.734375
train loss:  0.4723837375640869
train gradient:  0.12461459576793231
iteration : 10698
train acc:  0.7578125
train loss:  0.4833320379257202
train gradient:  0.10887507289786558
iteration : 10699
train acc:  0.765625
train loss:  0.4874415695667267
train gradient:  0.137188473578213
iteration : 10700
train acc:  0.7578125
train loss:  0.4919499158859253
train gradient:  0.12228955906910721
iteration : 10701
train acc:  0.671875
train loss:  0.5696390867233276
train gradient:  0.1455614281412576
iteration : 10702
train acc:  0.7109375
train loss:  0.49805745482444763
train gradient:  0.09899466614982749
iteration : 10703
train acc:  0.6796875
train loss:  0.52801114320755
train gradient:  0.10579202173550831
iteration : 10704
train acc:  0.7109375
train loss:  0.5215181112289429
train gradient:  0.14855546366549321
iteration : 10705
train acc:  0.7734375
train loss:  0.44139763712882996
train gradient:  0.10820670820206182
iteration : 10706
train acc:  0.7734375
train loss:  0.44681307673454285
train gradient:  0.10034798029099602
iteration : 10707
train acc:  0.71875
train loss:  0.5080760717391968
train gradient:  0.12530694030462933
iteration : 10708
train acc:  0.734375
train loss:  0.49069395661354065
train gradient:  0.11289194047517362
iteration : 10709
train acc:  0.7265625
train loss:  0.523137092590332
train gradient:  0.12694790858658522
iteration : 10710
train acc:  0.6796875
train loss:  0.566259503364563
train gradient:  0.16580281955183374
iteration : 10711
train acc:  0.8125
train loss:  0.42147937417030334
train gradient:  0.09634989216042526
iteration : 10712
train acc:  0.7421875
train loss:  0.5075216293334961
train gradient:  0.1456769340750103
iteration : 10713
train acc:  0.78125
train loss:  0.4718959629535675
train gradient:  0.09203712636936058
iteration : 10714
train acc:  0.765625
train loss:  0.43919339776039124
train gradient:  0.11006607243451202
iteration : 10715
train acc:  0.7734375
train loss:  0.4766920804977417
train gradient:  0.12285395147000815
iteration : 10716
train acc:  0.75
train loss:  0.47604745626449585
train gradient:  0.11891191056146036
iteration : 10717
train acc:  0.7265625
train loss:  0.5207439064979553
train gradient:  0.13083557077052382
iteration : 10718
train acc:  0.7421875
train loss:  0.46747392416000366
train gradient:  0.10153523234663754
iteration : 10719
train acc:  0.796875
train loss:  0.4238446354866028
train gradient:  0.14455290596863077
iteration : 10720
train acc:  0.6953125
train loss:  0.5243819952011108
train gradient:  0.1495565484831058
iteration : 10721
train acc:  0.7421875
train loss:  0.48912087082862854
train gradient:  0.11847953227126498
iteration : 10722
train acc:  0.703125
train loss:  0.6263318657875061
train gradient:  0.1777007714340303
iteration : 10723
train acc:  0.7890625
train loss:  0.44616860151290894
train gradient:  0.09765624836061011
iteration : 10724
train acc:  0.7734375
train loss:  0.4721185564994812
train gradient:  0.12438919205516194
iteration : 10725
train acc:  0.6875
train loss:  0.511074423789978
train gradient:  0.13188808671349173
iteration : 10726
train acc:  0.7890625
train loss:  0.4478016495704651
train gradient:  0.08477573642992678
iteration : 10727
train acc:  0.7734375
train loss:  0.4729870557785034
train gradient:  0.11995105871540937
iteration : 10728
train acc:  0.7109375
train loss:  0.5021907091140747
train gradient:  0.1461344152805956
iteration : 10729
train acc:  0.71875
train loss:  0.545843780040741
train gradient:  0.1466839538413483
iteration : 10730
train acc:  0.671875
train loss:  0.6038007140159607
train gradient:  0.15791531544399395
iteration : 10731
train acc:  0.78125
train loss:  0.463654100894928
train gradient:  0.11249374985766354
iteration : 10732
train acc:  0.71875
train loss:  0.4744096100330353
train gradient:  0.12262563431810308
iteration : 10733
train acc:  0.703125
train loss:  0.5351725816726685
train gradient:  0.1426817547770504
iteration : 10734
train acc:  0.8046875
train loss:  0.40527039766311646
train gradient:  0.09311187176741802
iteration : 10735
train acc:  0.75
train loss:  0.48773157596588135
train gradient:  0.13274067828092753
iteration : 10736
train acc:  0.8046875
train loss:  0.4480922818183899
train gradient:  0.0957846501509053
iteration : 10737
train acc:  0.765625
train loss:  0.4775320887565613
train gradient:  0.10828596268676523
iteration : 10738
train acc:  0.71875
train loss:  0.5035029649734497
train gradient:  0.11395431247964068
iteration : 10739
train acc:  0.734375
train loss:  0.5190619230270386
train gradient:  0.1378210687350716
iteration : 10740
train acc:  0.71875
train loss:  0.5575861930847168
train gradient:  0.1768240348084492
iteration : 10741
train acc:  0.7578125
train loss:  0.5282584428787231
train gradient:  0.13187950037423513
iteration : 10742
train acc:  0.7734375
train loss:  0.4777366816997528
train gradient:  0.1120989586321241
iteration : 10743
train acc:  0.7265625
train loss:  0.49395325779914856
train gradient:  0.11041845657606601
iteration : 10744
train acc:  0.71875
train loss:  0.5459666848182678
train gradient:  0.17501356268714865
iteration : 10745
train acc:  0.734375
train loss:  0.4601143002510071
train gradient:  0.09683411199438559
iteration : 10746
train acc:  0.8125
train loss:  0.4339655637741089
train gradient:  0.11049606010826621
iteration : 10747
train acc:  0.78125
train loss:  0.4649531841278076
train gradient:  0.1233473621918009
iteration : 10748
train acc:  0.71875
train loss:  0.6036428213119507
train gradient:  0.21314482455644446
iteration : 10749
train acc:  0.75
train loss:  0.5717453360557556
train gradient:  0.13176519512209853
iteration : 10750
train acc:  0.765625
train loss:  0.5083280801773071
train gradient:  0.14404940646241315
iteration : 10751
train acc:  0.6875
train loss:  0.4972904622554779
train gradient:  0.12516320176532397
iteration : 10752
train acc:  0.6953125
train loss:  0.5048193335533142
train gradient:  0.12215804694860306
iteration : 10753
train acc:  0.7421875
train loss:  0.542456865310669
train gradient:  0.1311926126615935
iteration : 10754
train acc:  0.71875
train loss:  0.5257763862609863
train gradient:  0.17595157018923852
iteration : 10755
train acc:  0.7578125
train loss:  0.4735077917575836
train gradient:  0.1035047753691327
iteration : 10756
train acc:  0.7421875
train loss:  0.4669272303581238
train gradient:  0.10770509612375129
iteration : 10757
train acc:  0.765625
train loss:  0.4519537389278412
train gradient:  0.09682564844869258
iteration : 10758
train acc:  0.75
train loss:  0.5060596466064453
train gradient:  0.12588837279470827
iteration : 10759
train acc:  0.7578125
train loss:  0.5078862905502319
train gradient:  0.1323543449698193
iteration : 10760
train acc:  0.734375
train loss:  0.5418310165405273
train gradient:  0.13341146591003594
iteration : 10761
train acc:  0.7734375
train loss:  0.4968686103820801
train gradient:  0.1488693209878571
iteration : 10762
train acc:  0.7421875
train loss:  0.474016010761261
train gradient:  0.13178735419140164
iteration : 10763
train acc:  0.6875
train loss:  0.5813164114952087
train gradient:  0.15038157535994634
iteration : 10764
train acc:  0.75
train loss:  0.477992981672287
train gradient:  0.13031117596494396
iteration : 10765
train acc:  0.8359375
train loss:  0.3980199098587036
train gradient:  0.08244872433615442
iteration : 10766
train acc:  0.7421875
train loss:  0.4837408661842346
train gradient:  0.13068209703079553
iteration : 10767
train acc:  0.71875
train loss:  0.4730949103832245
train gradient:  0.10358226273677335
iteration : 10768
train acc:  0.78125
train loss:  0.4785599708557129
train gradient:  0.1360146405105968
iteration : 10769
train acc:  0.7265625
train loss:  0.5427505373954773
train gradient:  0.13721267076852506
iteration : 10770
train acc:  0.7265625
train loss:  0.506514310836792
train gradient:  0.11058606281433349
iteration : 10771
train acc:  0.703125
train loss:  0.5267033576965332
train gradient:  0.11631126649259224
iteration : 10772
train acc:  0.75
train loss:  0.45273321866989136
train gradient:  0.11923763861845485
iteration : 10773
train acc:  0.71875
train loss:  0.506891131401062
train gradient:  0.10400622364528249
iteration : 10774
train acc:  0.6796875
train loss:  0.5138580799102783
train gradient:  0.13137536412429945
iteration : 10775
train acc:  0.765625
train loss:  0.47915220260620117
train gradient:  0.10186770937297214
iteration : 10776
train acc:  0.7109375
train loss:  0.5385185480117798
train gradient:  0.1559114007954076
iteration : 10777
train acc:  0.7734375
train loss:  0.5091567039489746
train gradient:  0.13570724841127335
iteration : 10778
train acc:  0.765625
train loss:  0.46985751390457153
train gradient:  0.10280371627089746
iteration : 10779
train acc:  0.6875
train loss:  0.5637374520301819
train gradient:  0.1384364773183741
iteration : 10780
train acc:  0.7265625
train loss:  0.5597057342529297
train gradient:  0.1419564136170782
iteration : 10781
train acc:  0.7265625
train loss:  0.4939209818840027
train gradient:  0.14928756259869153
iteration : 10782
train acc:  0.8359375
train loss:  0.4080251157283783
train gradient:  0.07596249722515842
iteration : 10783
train acc:  0.78125
train loss:  0.44899559020996094
train gradient:  0.09804596672304751
iteration : 10784
train acc:  0.7421875
train loss:  0.4742509722709656
train gradient:  0.1011828771935267
iteration : 10785
train acc:  0.7265625
train loss:  0.4814531207084656
train gradient:  0.1304381361119185
iteration : 10786
train acc:  0.6953125
train loss:  0.5208779573440552
train gradient:  0.1550763746510466
iteration : 10787
train acc:  0.71875
train loss:  0.5645161271095276
train gradient:  0.17013463374260895
iteration : 10788
train acc:  0.765625
train loss:  0.47430145740509033
train gradient:  0.11617253346365708
iteration : 10789
train acc:  0.734375
train loss:  0.477360337972641
train gradient:  0.09380339197530439
iteration : 10790
train acc:  0.7421875
train loss:  0.4202357530593872
train gradient:  0.07770386464115515
iteration : 10791
train acc:  0.7109375
train loss:  0.5061482191085815
train gradient:  0.11618887722277856
iteration : 10792
train acc:  0.7734375
train loss:  0.48806577920913696
train gradient:  0.11752688046924742
iteration : 10793
train acc:  0.7890625
train loss:  0.4326004683971405
train gradient:  0.10138946385921864
iteration : 10794
train acc:  0.7421875
train loss:  0.4569668173789978
train gradient:  0.09152784062327246
iteration : 10795
train acc:  0.7578125
train loss:  0.4714852571487427
train gradient:  0.09598739553518007
iteration : 10796
train acc:  0.6875
train loss:  0.5939699411392212
train gradient:  0.1415840102959044
iteration : 10797
train acc:  0.734375
train loss:  0.4945734143257141
train gradient:  0.14950263706027056
iteration : 10798
train acc:  0.75
train loss:  0.47374773025512695
train gradient:  0.1250394978797006
iteration : 10799
train acc:  0.734375
train loss:  0.4904409945011139
train gradient:  0.12838035600050693
iteration : 10800
train acc:  0.703125
train loss:  0.5348842740058899
train gradient:  0.14261889584010534
iteration : 10801
train acc:  0.796875
train loss:  0.42988452315330505
train gradient:  0.10472538065372128
iteration : 10802
train acc:  0.7265625
train loss:  0.5027203559875488
train gradient:  0.1036851494517661
iteration : 10803
train acc:  0.7421875
train loss:  0.4472687244415283
train gradient:  0.10275794725519444
iteration : 10804
train acc:  0.703125
train loss:  0.5452154278755188
train gradient:  0.16523008519590238
iteration : 10805
train acc:  0.7265625
train loss:  0.47776612639427185
train gradient:  0.10235232263597827
iteration : 10806
train acc:  0.7578125
train loss:  0.5332491993904114
train gradient:  0.1432150236559492
iteration : 10807
train acc:  0.734375
train loss:  0.4837121367454529
train gradient:  0.14408634081722832
iteration : 10808
train acc:  0.7421875
train loss:  0.46716976165771484
train gradient:  0.12928980833841322
iteration : 10809
train acc:  0.703125
train loss:  0.494052529335022
train gradient:  0.09379109794941094
iteration : 10810
train acc:  0.71875
train loss:  0.46701133251190186
train gradient:  0.11718045497374052
iteration : 10811
train acc:  0.71875
train loss:  0.5389930009841919
train gradient:  0.11363249383555823
iteration : 10812
train acc:  0.7734375
train loss:  0.45501992106437683
train gradient:  0.10487663719157594
iteration : 10813
train acc:  0.7421875
train loss:  0.4730736315250397
train gradient:  0.10597876363276897
iteration : 10814
train acc:  0.765625
train loss:  0.5524348020553589
train gradient:  0.18041456093214034
iteration : 10815
train acc:  0.7578125
train loss:  0.49949079751968384
train gradient:  0.11918675989997103
iteration : 10816
train acc:  0.7734375
train loss:  0.4235025644302368
train gradient:  0.09914626389389995
iteration : 10817
train acc:  0.7109375
train loss:  0.5479816198348999
train gradient:  0.12233151588487612
iteration : 10818
train acc:  0.7578125
train loss:  0.46108895540237427
train gradient:  0.11419638417656594
iteration : 10819
train acc:  0.6953125
train loss:  0.5007618069648743
train gradient:  0.1152771519881652
iteration : 10820
train acc:  0.75
train loss:  0.5195866823196411
train gradient:  0.10296997665780029
iteration : 10821
train acc:  0.7578125
train loss:  0.4748615622520447
train gradient:  0.14967509468369683
iteration : 10822
train acc:  0.765625
train loss:  0.44486895203590393
train gradient:  0.09665533077854278
iteration : 10823
train acc:  0.7578125
train loss:  0.4654010832309723
train gradient:  0.10829300062440891
iteration : 10824
train acc:  0.7421875
train loss:  0.516437292098999
train gradient:  0.1255522992672899
iteration : 10825
train acc:  0.640625
train loss:  0.5682371258735657
train gradient:  0.13390369314662387
iteration : 10826
train acc:  0.7265625
train loss:  0.5035298466682434
train gradient:  0.15122347203158815
iteration : 10827
train acc:  0.734375
train loss:  0.49799054861068726
train gradient:  0.11513914219950021
iteration : 10828
train acc:  0.6796875
train loss:  0.5565332174301147
train gradient:  0.11224856369189022
iteration : 10829
train acc:  0.734375
train loss:  0.47831976413726807
train gradient:  0.1349539478629353
iteration : 10830
train acc:  0.7890625
train loss:  0.4439026117324829
train gradient:  0.10323952689956706
iteration : 10831
train acc:  0.734375
train loss:  0.46628332138061523
train gradient:  0.09413476857790161
iteration : 10832
train acc:  0.7265625
train loss:  0.4831840991973877
train gradient:  0.10881706335856257
iteration : 10833
train acc:  0.7421875
train loss:  0.4639190137386322
train gradient:  0.10157680012544232
iteration : 10834
train acc:  0.78125
train loss:  0.452561616897583
train gradient:  0.09634416743469847
iteration : 10835
train acc:  0.6875
train loss:  0.5481056571006775
train gradient:  0.14870649523284418
iteration : 10836
train acc:  0.7109375
train loss:  0.48552626371383667
train gradient:  0.11026738378195473
iteration : 10837
train acc:  0.765625
train loss:  0.4777352809906006
train gradient:  0.14762451419952283
iteration : 10838
train acc:  0.734375
train loss:  0.47233182191848755
train gradient:  0.10046056507906567
iteration : 10839
train acc:  0.765625
train loss:  0.4573727250099182
train gradient:  0.10486931630488201
iteration : 10840
train acc:  0.734375
train loss:  0.49945640563964844
train gradient:  0.11701359825076223
iteration : 10841
train acc:  0.734375
train loss:  0.5188876390457153
train gradient:  0.10408166532678483
iteration : 10842
train acc:  0.703125
train loss:  0.4840160012245178
train gradient:  0.11823655413554898
iteration : 10843
train acc:  0.7578125
train loss:  0.45676296949386597
train gradient:  0.10947767613080038
iteration : 10844
train acc:  0.7421875
train loss:  0.45827388763427734
train gradient:  0.09421430819412685
iteration : 10845
train acc:  0.7109375
train loss:  0.5258349180221558
train gradient:  0.11278820857094929
iteration : 10846
train acc:  0.7734375
train loss:  0.4762663245201111
train gradient:  0.17245461877913126
iteration : 10847
train acc:  0.765625
train loss:  0.44883492588996887
train gradient:  0.08956139716795913
iteration : 10848
train acc:  0.765625
train loss:  0.47734490036964417
train gradient:  0.1212206875817758
iteration : 10849
train acc:  0.71875
train loss:  0.5494861602783203
train gradient:  0.17745066953283328
iteration : 10850
train acc:  0.6875
train loss:  0.5042597055435181
train gradient:  0.10961113541730473
iteration : 10851
train acc:  0.6953125
train loss:  0.5194294452667236
train gradient:  0.13296132835319063
iteration : 10852
train acc:  0.7265625
train loss:  0.5483155250549316
train gradient:  0.13040776815264954
iteration : 10853
train acc:  0.703125
train loss:  0.5444363355636597
train gradient:  0.14809021450834486
iteration : 10854
train acc:  0.7734375
train loss:  0.45120319724082947
train gradient:  0.08990031529963029
iteration : 10855
train acc:  0.7109375
train loss:  0.5562666654586792
train gradient:  0.15303635472613664
iteration : 10856
train acc:  0.7734375
train loss:  0.40020811557769775
train gradient:  0.09702625113916276
iteration : 10857
train acc:  0.734375
train loss:  0.4766732454299927
train gradient:  0.10120618478811283
iteration : 10858
train acc:  0.703125
train loss:  0.48451775312423706
train gradient:  0.1315187749060357
iteration : 10859
train acc:  0.7734375
train loss:  0.44824525713920593
train gradient:  0.10427633332302874
iteration : 10860
train acc:  0.671875
train loss:  0.5620348453521729
train gradient:  0.14237140053998776
iteration : 10861
train acc:  0.828125
train loss:  0.39472970366477966
train gradient:  0.08504188169637202
iteration : 10862
train acc:  0.796875
train loss:  0.43098902702331543
train gradient:  0.10309306368076934
iteration : 10863
train acc:  0.6484375
train loss:  0.5251738429069519
train gradient:  0.11709558490281861
iteration : 10864
train acc:  0.75
train loss:  0.49458223581314087
train gradient:  0.15413539871352677
iteration : 10865
train acc:  0.734375
train loss:  0.485954225063324
train gradient:  0.1058736460148853
iteration : 10866
train acc:  0.734375
train loss:  0.5445724725723267
train gradient:  0.13145488462631075
iteration : 10867
train acc:  0.7578125
train loss:  0.46448904275894165
train gradient:  0.0945389004605697
iteration : 10868
train acc:  0.7265625
train loss:  0.5243619084358215
train gradient:  0.11474046987178363
iteration : 10869
train acc:  0.7421875
train loss:  0.46210336685180664
train gradient:  0.09996418858100095
iteration : 10870
train acc:  0.6484375
train loss:  0.540971040725708
train gradient:  0.12500485276365528
iteration : 10871
train acc:  0.84375
train loss:  0.4210887551307678
train gradient:  0.0935888237099179
iteration : 10872
train acc:  0.703125
train loss:  0.5668318271636963
train gradient:  0.17301336507242082
iteration : 10873
train acc:  0.6953125
train loss:  0.5412094593048096
train gradient:  0.12062831123895003
iteration : 10874
train acc:  0.6953125
train loss:  0.5051397085189819
train gradient:  0.12014219558020042
iteration : 10875
train acc:  0.75
train loss:  0.45508795976638794
train gradient:  0.10735982760397625
iteration : 10876
train acc:  0.7890625
train loss:  0.435874342918396
train gradient:  0.10859816176726193
iteration : 10877
train acc:  0.7421875
train loss:  0.49083903431892395
train gradient:  0.11807264389825228
iteration : 10878
train acc:  0.703125
train loss:  0.5359453558921814
train gradient:  0.16292594616769723
iteration : 10879
train acc:  0.75
train loss:  0.4728596806526184
train gradient:  0.1032270480074959
iteration : 10880
train acc:  0.7265625
train loss:  0.4887140691280365
train gradient:  0.11964043515468192
iteration : 10881
train acc:  0.7109375
train loss:  0.504479169845581
train gradient:  0.13501437310370856
iteration : 10882
train acc:  0.7421875
train loss:  0.4978597164154053
train gradient:  0.1575879271706561
iteration : 10883
train acc:  0.7578125
train loss:  0.4917283356189728
train gradient:  0.11427064535500721
iteration : 10884
train acc:  0.75
train loss:  0.4743461310863495
train gradient:  0.11331157034445992
iteration : 10885
train acc:  0.734375
train loss:  0.5927332639694214
train gradient:  0.21643355509778284
iteration : 10886
train acc:  0.734375
train loss:  0.47965240478515625
train gradient:  0.11453105685580134
iteration : 10887
train acc:  0.6875
train loss:  0.5699959993362427
train gradient:  0.20112765618858047
iteration : 10888
train acc:  0.75
train loss:  0.48572587966918945
train gradient:  0.13916428565905725
iteration : 10889
train acc:  0.734375
train loss:  0.4797828197479248
train gradient:  0.1123977465461404
iteration : 10890
train acc:  0.734375
train loss:  0.4969223737716675
train gradient:  0.14219568118653392
iteration : 10891
train acc:  0.7265625
train loss:  0.50750732421875
train gradient:  0.1344968887438875
iteration : 10892
train acc:  0.734375
train loss:  0.4852103292942047
train gradient:  0.15116308333885614
iteration : 10893
train acc:  0.78125
train loss:  0.45630943775177
train gradient:  0.11213358423643986
iteration : 10894
train acc:  0.6640625
train loss:  0.5837969779968262
train gradient:  0.15524442346118694
iteration : 10895
train acc:  0.6796875
train loss:  0.5285418033599854
train gradient:  0.15929434779673726
iteration : 10896
train acc:  0.8046875
train loss:  0.42009562253952026
train gradient:  0.08036238523799133
iteration : 10897
train acc:  0.734375
train loss:  0.4729689955711365
train gradient:  0.12491054319887264
iteration : 10898
train acc:  0.7421875
train loss:  0.4477912187576294
train gradient:  0.10813920794929834
iteration : 10899
train acc:  0.7578125
train loss:  0.4833921194076538
train gradient:  0.11897388420349078
iteration : 10900
train acc:  0.71875
train loss:  0.5446872711181641
train gradient:  0.1584320747404956
iteration : 10901
train acc:  0.7265625
train loss:  0.4784902036190033
train gradient:  0.11563035033424046
iteration : 10902
train acc:  0.7265625
train loss:  0.46865910291671753
train gradient:  0.0926329190285437
iteration : 10903
train acc:  0.765625
train loss:  0.4549793303012848
train gradient:  0.09489882797491964
iteration : 10904
train acc:  0.78125
train loss:  0.4958370327949524
train gradient:  0.11633391343630821
iteration : 10905
train acc:  0.7578125
train loss:  0.4987373948097229
train gradient:  0.10989175145967868
iteration : 10906
train acc:  0.7421875
train loss:  0.531255841255188
train gradient:  0.1327582221138679
iteration : 10907
train acc:  0.6484375
train loss:  0.6110485792160034
train gradient:  0.1745229484246303
iteration : 10908
train acc:  0.6875
train loss:  0.526455283164978
train gradient:  0.14393745284291956
iteration : 10909
train acc:  0.71875
train loss:  0.5442665815353394
train gradient:  0.13229912515826414
iteration : 10910
train acc:  0.7265625
train loss:  0.5264028906822205
train gradient:  0.14092377637266978
iteration : 10911
train acc:  0.7421875
train loss:  0.5162111520767212
train gradient:  0.12934251172737032
iteration : 10912
train acc:  0.7578125
train loss:  0.46448731422424316
train gradient:  0.11905330781536845
iteration : 10913
train acc:  0.71875
train loss:  0.5642449855804443
train gradient:  0.17197220096647983
iteration : 10914
train acc:  0.7109375
train loss:  0.49755197763442993
train gradient:  0.12007608468412237
iteration : 10915
train acc:  0.734375
train loss:  0.43722742795944214
train gradient:  0.11138980515447525
iteration : 10916
train acc:  0.703125
train loss:  0.5312254428863525
train gradient:  0.15825622859148702
iteration : 10917
train acc:  0.703125
train loss:  0.5533544421195984
train gradient:  0.15846320365546823
iteration : 10918
train acc:  0.6796875
train loss:  0.5213753581047058
train gradient:  0.1341454671929046
iteration : 10919
train acc:  0.7890625
train loss:  0.4934249222278595
train gradient:  0.1351367877128256
iteration : 10920
train acc:  0.6640625
train loss:  0.5585743188858032
train gradient:  0.12040433847942483
iteration : 10921
train acc:  0.7109375
train loss:  0.5420448780059814
train gradient:  0.1241574198859554
iteration : 10922
train acc:  0.6875
train loss:  0.5242310762405396
train gradient:  0.10627409864857108
iteration : 10923
train acc:  0.6953125
train loss:  0.5529757738113403
train gradient:  0.15168287735187275
iteration : 10924
train acc:  0.7890625
train loss:  0.4572605490684509
train gradient:  0.08537722115298207
iteration : 10925
train acc:  0.7578125
train loss:  0.5065643787384033
train gradient:  0.110408353582197
iteration : 10926
train acc:  0.6796875
train loss:  0.5312477350234985
train gradient:  0.15283303979817886
iteration : 10927
train acc:  0.7734375
train loss:  0.47449618577957153
train gradient:  0.10771262297977115
iteration : 10928
train acc:  0.7890625
train loss:  0.44194698333740234
train gradient:  0.12444755369072337
iteration : 10929
train acc:  0.765625
train loss:  0.4552037715911865
train gradient:  0.11755772835976509
iteration : 10930
train acc:  0.8125
train loss:  0.44314369559288025
train gradient:  0.11205602691701075
iteration : 10931
train acc:  0.7578125
train loss:  0.48169100284576416
train gradient:  0.153177040164302
iteration : 10932
train acc:  0.6484375
train loss:  0.5277849435806274
train gradient:  0.12949557519527588
iteration : 10933
train acc:  0.7890625
train loss:  0.46985572576522827
train gradient:  0.09331735703711873
iteration : 10934
train acc:  0.671875
train loss:  0.5619529485702515
train gradient:  0.16403536414296416
iteration : 10935
train acc:  0.7890625
train loss:  0.4095815122127533
train gradient:  0.07577231088299409
iteration : 10936
train acc:  0.78125
train loss:  0.4332464337348938
train gradient:  0.08665964445292963
iteration : 10937
train acc:  0.78125
train loss:  0.44379502534866333
train gradient:  0.10068653293263123
iteration : 10938
train acc:  0.7890625
train loss:  0.4256408214569092
train gradient:  0.08663719945779573
iteration : 10939
train acc:  0.828125
train loss:  0.4241618514060974
train gradient:  0.08457352028990849
iteration : 10940
train acc:  0.75
train loss:  0.4955776631832123
train gradient:  0.1548369960262604
iteration : 10941
train acc:  0.7890625
train loss:  0.44641372561454773
train gradient:  0.10405372608296881
iteration : 10942
train acc:  0.8046875
train loss:  0.4272104501724243
train gradient:  0.10394444742180924
iteration : 10943
train acc:  0.765625
train loss:  0.4800882041454315
train gradient:  0.11622828700740916
iteration : 10944
train acc:  0.7890625
train loss:  0.46645259857177734
train gradient:  0.12681676308291678
iteration : 10945
train acc:  0.7578125
train loss:  0.5251046419143677
train gradient:  0.12292549377150003
iteration : 10946
train acc:  0.75
train loss:  0.5018289089202881
train gradient:  0.1237219304698812
iteration : 10947
train acc:  0.828125
train loss:  0.4467862844467163
train gradient:  0.12804903105260862
iteration : 10948
train acc:  0.7578125
train loss:  0.5014472603797913
train gradient:  0.1228062647137679
iteration : 10949
train acc:  0.734375
train loss:  0.5192592144012451
train gradient:  0.14102152119554812
iteration : 10950
train acc:  0.6953125
train loss:  0.5318255424499512
train gradient:  0.1609151870519851
iteration : 10951
train acc:  0.7578125
train loss:  0.4858657121658325
train gradient:  0.11071255364746928
iteration : 10952
train acc:  0.6796875
train loss:  0.557510495185852
train gradient:  0.14819727010856248
iteration : 10953
train acc:  0.828125
train loss:  0.4198291301727295
train gradient:  0.08909693102837207
iteration : 10954
train acc:  0.765625
train loss:  0.4604380130767822
train gradient:  0.12410583163599372
iteration : 10955
train acc:  0.7578125
train loss:  0.41707801818847656
train gradient:  0.09455956924203729
iteration : 10956
train acc:  0.75
train loss:  0.46933114528656006
train gradient:  0.10804592198872497
iteration : 10957
train acc:  0.7421875
train loss:  0.48081716895103455
train gradient:  0.14021672073423763
iteration : 10958
train acc:  0.671875
train loss:  0.5537512302398682
train gradient:  0.13050801017295413
iteration : 10959
train acc:  0.71875
train loss:  0.481139212846756
train gradient:  0.11895569120611403
iteration : 10960
train acc:  0.7578125
train loss:  0.4725276827812195
train gradient:  0.14421133550561563
iteration : 10961
train acc:  0.703125
train loss:  0.5072572231292725
train gradient:  0.12920297596926258
iteration : 10962
train acc:  0.71875
train loss:  0.4978814423084259
train gradient:  0.11392811093193275
iteration : 10963
train acc:  0.734375
train loss:  0.4903705418109894
train gradient:  0.10851339177832313
iteration : 10964
train acc:  0.7265625
train loss:  0.5093128681182861
train gradient:  0.14944833373976696
iteration : 10965
train acc:  0.703125
train loss:  0.5125063061714172
train gradient:  0.13011573403507323
iteration : 10966
train acc:  0.75
train loss:  0.4741913378238678
train gradient:  0.112913692945239
iteration : 10967
train acc:  0.6875
train loss:  0.5208773612976074
train gradient:  0.13816833828114472
iteration : 10968
train acc:  0.7890625
train loss:  0.42656195163726807
train gradient:  0.09425809064689694
iteration : 10969
train acc:  0.7109375
train loss:  0.502251148223877
train gradient:  0.10832993012810913
iteration : 10970
train acc:  0.7734375
train loss:  0.4261713922023773
train gradient:  0.08851189847996573
iteration : 10971
train acc:  0.7578125
train loss:  0.4448431432247162
train gradient:  0.10453410588765595
iteration : 10972
train acc:  0.7109375
train loss:  0.4859941601753235
train gradient:  0.11469020354755306
iteration : 10973
train acc:  0.71875
train loss:  0.5189086198806763
train gradient:  0.20820891708808065
iteration : 10974
train acc:  0.796875
train loss:  0.46280813217163086
train gradient:  0.09285364021308079
iteration : 10975
train acc:  0.7578125
train loss:  0.44129449129104614
train gradient:  0.09149398049630934
iteration : 10976
train acc:  0.7578125
train loss:  0.5051983594894409
train gradient:  0.18361818834306887
iteration : 10977
train acc:  0.7578125
train loss:  0.4437781572341919
train gradient:  0.12507489501760238
iteration : 10978
train acc:  0.78125
train loss:  0.4859621226787567
train gradient:  0.1055924034115697
iteration : 10979
train acc:  0.7109375
train loss:  0.5147292613983154
train gradient:  0.14708261102864117
iteration : 10980
train acc:  0.8125
train loss:  0.44069939851760864
train gradient:  0.1298523628244978
iteration : 10981
train acc:  0.7578125
train loss:  0.5082675814628601
train gradient:  0.14414823389288642
iteration : 10982
train acc:  0.7734375
train loss:  0.43646490573883057
train gradient:  0.11416756005271957
iteration : 10983
train acc:  0.75
train loss:  0.49124112725257874
train gradient:  0.12913706228891203
iteration : 10984
train acc:  0.7421875
train loss:  0.5097927451133728
train gradient:  0.11278540607310923
iteration : 10985
train acc:  0.734375
train loss:  0.5014253258705139
train gradient:  0.1210311234299048
iteration : 10986
train acc:  0.703125
train loss:  0.5347887277603149
train gradient:  0.13517442972785995
iteration : 10987
train acc:  0.7578125
train loss:  0.5272006988525391
train gradient:  0.14933218910170853
iteration : 10988
train acc:  0.6484375
train loss:  0.6070476770401001
train gradient:  0.17509967954449862
iteration : 10989
train acc:  0.7578125
train loss:  0.4962157905101776
train gradient:  0.11989335486268922
iteration : 10990
train acc:  0.703125
train loss:  0.5160777568817139
train gradient:  0.1323820919392834
iteration : 10991
train acc:  0.8046875
train loss:  0.4017578363418579
train gradient:  0.08320861244697154
iteration : 10992
train acc:  0.7265625
train loss:  0.4698910713195801
train gradient:  0.11227143363008062
iteration : 10993
train acc:  0.8359375
train loss:  0.3856125473976135
train gradient:  0.0768658207514728
iteration : 10994
train acc:  0.6875
train loss:  0.5099198222160339
train gradient:  0.11272591206217493
iteration : 10995
train acc:  0.6875
train loss:  0.5604557991027832
train gradient:  0.1461594392999193
iteration : 10996
train acc:  0.7734375
train loss:  0.44065219163894653
train gradient:  0.11626056350438448
iteration : 10997
train acc:  0.7265625
train loss:  0.5307220816612244
train gradient:  0.09856278609860557
iteration : 10998
train acc:  0.7109375
train loss:  0.512007474899292
train gradient:  0.12403316586296827
iteration : 10999
train acc:  0.6875
train loss:  0.4734981656074524
train gradient:  0.1339110710530651
iteration : 11000
train acc:  0.734375
train loss:  0.46987447142601013
train gradient:  0.11388591702782945
iteration : 11001
train acc:  0.7109375
train loss:  0.5673458576202393
train gradient:  0.13586343790584
iteration : 11002
train acc:  0.765625
train loss:  0.504135251045227
train gradient:  0.13769259556988078
iteration : 11003
train acc:  0.7578125
train loss:  0.4287598729133606
train gradient:  0.10339882319379884
iteration : 11004
train acc:  0.6953125
train loss:  0.5174922347068787
train gradient:  0.1466935374463157
iteration : 11005
train acc:  0.7734375
train loss:  0.4635974168777466
train gradient:  0.12132360641020612
iteration : 11006
train acc:  0.6953125
train loss:  0.5015546083450317
train gradient:  0.09636701014170032
iteration : 11007
train acc:  0.75
train loss:  0.5026048421859741
train gradient:  0.12677508640301943
iteration : 11008
train acc:  0.7265625
train loss:  0.5163759589195251
train gradient:  0.13641155008156816
iteration : 11009
train acc:  0.671875
train loss:  0.5689110159873962
train gradient:  0.19246558623928633
iteration : 11010
train acc:  0.765625
train loss:  0.4141786992549896
train gradient:  0.09370889710951048
iteration : 11011
train acc:  0.6796875
train loss:  0.5989135503768921
train gradient:  0.1625790931983618
iteration : 11012
train acc:  0.7265625
train loss:  0.4948574900627136
train gradient:  0.13270645301139172
iteration : 11013
train acc:  0.7421875
train loss:  0.46649932861328125
train gradient:  0.12219524298271882
iteration : 11014
train acc:  0.71875
train loss:  0.5313687324523926
train gradient:  0.11774668229300322
iteration : 11015
train acc:  0.7265625
train loss:  0.5491276979446411
train gradient:  0.14214576888403224
iteration : 11016
train acc:  0.734375
train loss:  0.4718994200229645
train gradient:  0.11992909757500451
iteration : 11017
train acc:  0.7265625
train loss:  0.5235975980758667
train gradient:  0.13077308045877017
iteration : 11018
train acc:  0.765625
train loss:  0.48010551929473877
train gradient:  0.1306764596281232
iteration : 11019
train acc:  0.6953125
train loss:  0.5567446947097778
train gradient:  0.1208752747120034
iteration : 11020
train acc:  0.71875
train loss:  0.471190869808197
train gradient:  0.11396937424722406
iteration : 11021
train acc:  0.7890625
train loss:  0.405360609292984
train gradient:  0.06986741229509603
iteration : 11022
train acc:  0.71875
train loss:  0.5392566919326782
train gradient:  0.14121560313438686
iteration : 11023
train acc:  0.734375
train loss:  0.4745294749736786
train gradient:  0.10405979038534101
iteration : 11024
train acc:  0.71875
train loss:  0.5662838220596313
train gradient:  0.1638945136528974
iteration : 11025
train acc:  0.7734375
train loss:  0.42789313197135925
train gradient:  0.08367848006407212
iteration : 11026
train acc:  0.7890625
train loss:  0.41379839181900024
train gradient:  0.09702220567164745
iteration : 11027
train acc:  0.734375
train loss:  0.5451462268829346
train gradient:  0.13046954926086302
iteration : 11028
train acc:  0.78125
train loss:  0.46754950284957886
train gradient:  0.10120777018947978
iteration : 11029
train acc:  0.7421875
train loss:  0.47382378578186035
train gradient:  0.09161753873316467
iteration : 11030
train acc:  0.7421875
train loss:  0.4634261727333069
train gradient:  0.10841360743598386
iteration : 11031
train acc:  0.7578125
train loss:  0.459433913230896
train gradient:  0.13602504001272384
iteration : 11032
train acc:  0.703125
train loss:  0.5185332298278809
train gradient:  0.11758809791527651
iteration : 11033
train acc:  0.765625
train loss:  0.45056280493736267
train gradient:  0.10340643696301843
iteration : 11034
train acc:  0.7265625
train loss:  0.46139246225357056
train gradient:  0.10927208066251846
iteration : 11035
train acc:  0.7734375
train loss:  0.493561714887619
train gradient:  0.12794431940652928
iteration : 11036
train acc:  0.6875
train loss:  0.5357984304428101
train gradient:  0.1654808314967035
iteration : 11037
train acc:  0.7578125
train loss:  0.4985983967781067
train gradient:  0.15437078883545396
iteration : 11038
train acc:  0.7578125
train loss:  0.4365648031234741
train gradient:  0.10947263626496787
iteration : 11039
train acc:  0.734375
train loss:  0.5402318835258484
train gradient:  0.1494638102549741
iteration : 11040
train acc:  0.7421875
train loss:  0.48610323667526245
train gradient:  0.1125091380513729
iteration : 11041
train acc:  0.7265625
train loss:  0.48802798986434937
train gradient:  0.11550909733954776
iteration : 11042
train acc:  0.7421875
train loss:  0.47408366203308105
train gradient:  0.11097384514771926
iteration : 11043
train acc:  0.734375
train loss:  0.4992680251598358
train gradient:  0.12807324041558713
iteration : 11044
train acc:  0.7265625
train loss:  0.5053774118423462
train gradient:  0.1111315288376791
iteration : 11045
train acc:  0.75
train loss:  0.45379263162612915
train gradient:  0.10792200568784793
iteration : 11046
train acc:  0.7890625
train loss:  0.47674280405044556
train gradient:  0.11444158386813234
iteration : 11047
train acc:  0.7421875
train loss:  0.4904477298259735
train gradient:  0.10487903869895429
iteration : 11048
train acc:  0.796875
train loss:  0.4435398280620575
train gradient:  0.0982455439411932
iteration : 11049
train acc:  0.734375
train loss:  0.496027410030365
train gradient:  0.1191508754675864
iteration : 11050
train acc:  0.7109375
train loss:  0.524236798286438
train gradient:  0.1646686903744375
iteration : 11051
train acc:  0.6796875
train loss:  0.51734858751297
train gradient:  0.15626741387421417
iteration : 11052
train acc:  0.734375
train loss:  0.4567418694496155
train gradient:  0.1267681427946568
iteration : 11053
train acc:  0.7890625
train loss:  0.4334317743778229
train gradient:  0.10902159928807076
iteration : 11054
train acc:  0.7578125
train loss:  0.4311184883117676
train gradient:  0.10437195000100635
iteration : 11055
train acc:  0.71875
train loss:  0.5068594217300415
train gradient:  0.10897242366487342
iteration : 11056
train acc:  0.7578125
train loss:  0.43050146102905273
train gradient:  0.089369664020602
iteration : 11057
train acc:  0.7421875
train loss:  0.49935513734817505
train gradient:  0.1301873429824012
iteration : 11058
train acc:  0.6796875
train loss:  0.5697962045669556
train gradient:  0.17324730673827676
iteration : 11059
train acc:  0.8046875
train loss:  0.3782432973384857
train gradient:  0.0789192887990162
iteration : 11060
train acc:  0.703125
train loss:  0.5138351917266846
train gradient:  0.1186628648259257
iteration : 11061
train acc:  0.7109375
train loss:  0.5352491140365601
train gradient:  0.12254596216049098
iteration : 11062
train acc:  0.8046875
train loss:  0.41483062505722046
train gradient:  0.10081464663831868
iteration : 11063
train acc:  0.7421875
train loss:  0.4591350853443146
train gradient:  0.09728240664410526
iteration : 11064
train acc:  0.734375
train loss:  0.5539343357086182
train gradient:  0.17702311983080515
iteration : 11065
train acc:  0.75
train loss:  0.4920685291290283
train gradient:  0.1163077568331474
iteration : 11066
train acc:  0.7734375
train loss:  0.43964970111846924
train gradient:  0.1073389593078543
iteration : 11067
train acc:  0.7421875
train loss:  0.48276644945144653
train gradient:  0.12896467860080044
iteration : 11068
train acc:  0.765625
train loss:  0.44245994091033936
train gradient:  0.09287000238171414
iteration : 11069
train acc:  0.7421875
train loss:  0.4394700229167938
train gradient:  0.09511199102355775
iteration : 11070
train acc:  0.71875
train loss:  0.5634869337081909
train gradient:  0.16736447724006054
iteration : 11071
train acc:  0.734375
train loss:  0.5224185585975647
train gradient:  0.1558043748597506
iteration : 11072
train acc:  0.7578125
train loss:  0.49710550904273987
train gradient:  0.1163567694987657
iteration : 11073
train acc:  0.75
train loss:  0.47051936388015747
train gradient:  0.11348805687740789
iteration : 11074
train acc:  0.7421875
train loss:  0.49529892206192017
train gradient:  0.13588068635284023
iteration : 11075
train acc:  0.734375
train loss:  0.47084662318229675
train gradient:  0.14058013640735828
iteration : 11076
train acc:  0.75
train loss:  0.5397404432296753
train gradient:  0.11277329656932703
iteration : 11077
train acc:  0.765625
train loss:  0.47314566373825073
train gradient:  0.10174812355657453
iteration : 11078
train acc:  0.75
train loss:  0.49926117062568665
train gradient:  0.11249668429885912
iteration : 11079
train acc:  0.734375
train loss:  0.4984104335308075
train gradient:  0.10534299723153431
iteration : 11080
train acc:  0.78125
train loss:  0.49067914485931396
train gradient:  0.11047907472208204
iteration : 11081
train acc:  0.7890625
train loss:  0.4843229055404663
train gradient:  0.13394685044478763
iteration : 11082
train acc:  0.7578125
train loss:  0.4501936435699463
train gradient:  0.16344927488583685
iteration : 11083
train acc:  0.71875
train loss:  0.5345261096954346
train gradient:  0.1341810137250374
iteration : 11084
train acc:  0.7421875
train loss:  0.4839601516723633
train gradient:  0.1174621078436353
iteration : 11085
train acc:  0.7734375
train loss:  0.4279518723487854
train gradient:  0.09298375363704121
iteration : 11086
train acc:  0.71875
train loss:  0.5124374628067017
train gradient:  0.12661697003626715
iteration : 11087
train acc:  0.7734375
train loss:  0.4960338771343231
train gradient:  0.11152241724387242
iteration : 11088
train acc:  0.7265625
train loss:  0.4909976124763489
train gradient:  0.09643156388716342
iteration : 11089
train acc:  0.7890625
train loss:  0.4925728440284729
train gradient:  0.1421327891632162
iteration : 11090
train acc:  0.703125
train loss:  0.5205711126327515
train gradient:  0.14333269286297579
iteration : 11091
train acc:  0.75
train loss:  0.5142430067062378
train gradient:  0.13078155802120744
iteration : 11092
train acc:  0.7421875
train loss:  0.49361905455589294
train gradient:  0.15740291441035204
iteration : 11093
train acc:  0.796875
train loss:  0.46987664699554443
train gradient:  0.11339538024108341
iteration : 11094
train acc:  0.7734375
train loss:  0.43208420276641846
train gradient:  0.08543224046669844
iteration : 11095
train acc:  0.8046875
train loss:  0.4221486747264862
train gradient:  0.09494239320754193
iteration : 11096
train acc:  0.7734375
train loss:  0.44358187913894653
train gradient:  0.11843885521476721
iteration : 11097
train acc:  0.7109375
train loss:  0.5081171989440918
train gradient:  0.11896907133563947
iteration : 11098
train acc:  0.7890625
train loss:  0.4462639391422272
train gradient:  0.10534341881246702
iteration : 11099
train acc:  0.75
train loss:  0.4676254987716675
train gradient:  0.11873094480867562
iteration : 11100
train acc:  0.703125
train loss:  0.5110583305358887
train gradient:  0.13961628423784014
iteration : 11101
train acc:  0.765625
train loss:  0.483845978975296
train gradient:  0.10802633816708146
iteration : 11102
train acc:  0.703125
train loss:  0.5074935555458069
train gradient:  0.13477709426025136
iteration : 11103
train acc:  0.78125
train loss:  0.46634700894355774
train gradient:  0.12892976548575213
iteration : 11104
train acc:  0.671875
train loss:  0.5829328894615173
train gradient:  0.22748999729223346
iteration : 11105
train acc:  0.7734375
train loss:  0.4744655191898346
train gradient:  0.10503224472727554
iteration : 11106
train acc:  0.7421875
train loss:  0.47653624415397644
train gradient:  0.1232705119707713
iteration : 11107
train acc:  0.75
train loss:  0.4880842864513397
train gradient:  0.1198579003546241
iteration : 11108
train acc:  0.7421875
train loss:  0.5365729928016663
train gradient:  0.15539967927253967
iteration : 11109
train acc:  0.78125
train loss:  0.4763067066669464
train gradient:  0.1509780153648382
iteration : 11110
train acc:  0.7109375
train loss:  0.5294644832611084
train gradient:  0.1385638429707276
iteration : 11111
train acc:  0.703125
train loss:  0.5503919720649719
train gradient:  0.177423221902893
iteration : 11112
train acc:  0.734375
train loss:  0.47662246227264404
train gradient:  0.12103855440747091
iteration : 11113
train acc:  0.7578125
train loss:  0.484676718711853
train gradient:  0.11676135183025099
iteration : 11114
train acc:  0.75
train loss:  0.5210581421852112
train gradient:  0.1441028489530102
iteration : 11115
train acc:  0.7890625
train loss:  0.45788291096687317
train gradient:  0.1390239048924543
iteration : 11116
train acc:  0.796875
train loss:  0.43876582384109497
train gradient:  0.08433494083314755
iteration : 11117
train acc:  0.734375
train loss:  0.4646008014678955
train gradient:  0.11745350990124354
iteration : 11118
train acc:  0.75
train loss:  0.49039775133132935
train gradient:  0.11471331538955704
iteration : 11119
train acc:  0.7734375
train loss:  0.46901339292526245
train gradient:  0.1027382429786726
iteration : 11120
train acc:  0.8046875
train loss:  0.43460965156555176
train gradient:  0.10326482798585677
iteration : 11121
train acc:  0.78125
train loss:  0.44370266795158386
train gradient:  0.0917615162546573
iteration : 11122
train acc:  0.734375
train loss:  0.47834503650665283
train gradient:  0.10210961463254341
iteration : 11123
train acc:  0.7109375
train loss:  0.5364363789558411
train gradient:  0.13916488146750428
iteration : 11124
train acc:  0.7109375
train loss:  0.5191035270690918
train gradient:  0.1410448302526418
iteration : 11125
train acc:  0.7890625
train loss:  0.500937819480896
train gradient:  0.1220841108139417
iteration : 11126
train acc:  0.7734375
train loss:  0.5354130268096924
train gradient:  0.18038954278690839
iteration : 11127
train acc:  0.734375
train loss:  0.521031379699707
train gradient:  0.18352238779568147
iteration : 11128
train acc:  0.8046875
train loss:  0.43948549032211304
train gradient:  0.10703741608492014
iteration : 11129
train acc:  0.8046875
train loss:  0.46190983057022095
train gradient:  0.11602856303745467
iteration : 11130
train acc:  0.78125
train loss:  0.43811607360839844
train gradient:  0.09921890099779405
iteration : 11131
train acc:  0.7421875
train loss:  0.5148200392723083
train gradient:  0.17649079334559858
iteration : 11132
train acc:  0.640625
train loss:  0.5981652736663818
train gradient:  0.17534740430259071
iteration : 11133
train acc:  0.703125
train loss:  0.5280469655990601
train gradient:  0.14634277510507787
iteration : 11134
train acc:  0.6875
train loss:  0.5287013053894043
train gradient:  0.1320294115152727
iteration : 11135
train acc:  0.6640625
train loss:  0.5291988849639893
train gradient:  0.1399068446189925
iteration : 11136
train acc:  0.6875
train loss:  0.534226655960083
train gradient:  0.1491108879034575
iteration : 11137
train acc:  0.7109375
train loss:  0.549157440662384
train gradient:  0.1414809209497977
iteration : 11138
train acc:  0.6875
train loss:  0.5597445964813232
train gradient:  0.14798875932726208
iteration : 11139
train acc:  0.78125
train loss:  0.4737797677516937
train gradient:  0.1329463914939914
iteration : 11140
train acc:  0.7109375
train loss:  0.46695077419281006
train gradient:  0.1347878169281202
iteration : 11141
train acc:  0.75
train loss:  0.4783419370651245
train gradient:  0.10646354342373307
iteration : 11142
train acc:  0.8046875
train loss:  0.42888516187667847
train gradient:  0.08172647117918713
iteration : 11143
train acc:  0.6953125
train loss:  0.5414401888847351
train gradient:  0.15917887733259667
iteration : 11144
train acc:  0.75
train loss:  0.45958149433135986
train gradient:  0.1102631841715041
iteration : 11145
train acc:  0.8203125
train loss:  0.42063289880752563
train gradient:  0.10488077467447866
iteration : 11146
train acc:  0.734375
train loss:  0.48710668087005615
train gradient:  0.1266460381780123
iteration : 11147
train acc:  0.75
train loss:  0.4868810772895813
train gradient:  0.1320716389947605
iteration : 11148
train acc:  0.6875
train loss:  0.5363155603408813
train gradient:  0.11726003996178196
iteration : 11149
train acc:  0.703125
train loss:  0.5040931701660156
train gradient:  0.13745244191257178
iteration : 11150
train acc:  0.796875
train loss:  0.46556177735328674
train gradient:  0.1317003736165956
iteration : 11151
train acc:  0.734375
train loss:  0.5396147966384888
train gradient:  0.1495256908542274
iteration : 11152
train acc:  0.765625
train loss:  0.434487909078598
train gradient:  0.08519325648941603
iteration : 11153
train acc:  0.75
train loss:  0.4839826226234436
train gradient:  0.12295721805172977
iteration : 11154
train acc:  0.84375
train loss:  0.4084925651550293
train gradient:  0.10679952999968025
iteration : 11155
train acc:  0.8125
train loss:  0.45997804403305054
train gradient:  0.13529597587840453
iteration : 11156
train acc:  0.7421875
train loss:  0.4659663438796997
train gradient:  0.11934476708249785
iteration : 11157
train acc:  0.7265625
train loss:  0.548702597618103
train gradient:  0.18252104999639657
iteration : 11158
train acc:  0.734375
train loss:  0.4804760813713074
train gradient:  0.09568371435519661
iteration : 11159
train acc:  0.640625
train loss:  0.5511482954025269
train gradient:  0.17017604121331065
iteration : 11160
train acc:  0.78125
train loss:  0.5455297231674194
train gradient:  0.19540333870134047
iteration : 11161
train acc:  0.7109375
train loss:  0.5729319453239441
train gradient:  0.19055267217653965
iteration : 11162
train acc:  0.734375
train loss:  0.48908406496047974
train gradient:  0.10836236900300338
iteration : 11163
train acc:  0.7734375
train loss:  0.46668875217437744
train gradient:  0.12627997339056218
iteration : 11164
train acc:  0.7265625
train loss:  0.5217530131340027
train gradient:  0.14438391371757042
iteration : 11165
train acc:  0.75
train loss:  0.44119763374328613
train gradient:  0.07800257709293215
iteration : 11166
train acc:  0.6875
train loss:  0.5343509316444397
train gradient:  0.13687565575995508
iteration : 11167
train acc:  0.8046875
train loss:  0.40700846910476685
train gradient:  0.10128684578163333
iteration : 11168
train acc:  0.7890625
train loss:  0.4446994364261627
train gradient:  0.1375887264879575
iteration : 11169
train acc:  0.7421875
train loss:  0.5118056535720825
train gradient:  0.13687996684238732
iteration : 11170
train acc:  0.734375
train loss:  0.4840738773345947
train gradient:  0.1666295002628183
iteration : 11171
train acc:  0.7734375
train loss:  0.49898582696914673
train gradient:  0.13156695528876333
iteration : 11172
train acc:  0.75
train loss:  0.46455925703048706
train gradient:  0.09552190324972791
iteration : 11173
train acc:  0.7421875
train loss:  0.48310554027557373
train gradient:  0.12120279564572624
iteration : 11174
train acc:  0.7734375
train loss:  0.4867921471595764
train gradient:  0.12165904132811538
iteration : 11175
train acc:  0.7421875
train loss:  0.4585056006908417
train gradient:  0.10495576424378923
iteration : 11176
train acc:  0.765625
train loss:  0.4844101369380951
train gradient:  0.13998298184684893
iteration : 11177
train acc:  0.6875
train loss:  0.5322275757789612
train gradient:  0.14365711030422115
iteration : 11178
train acc:  0.7734375
train loss:  0.4672219157218933
train gradient:  0.12298268847811109
iteration : 11179
train acc:  0.765625
train loss:  0.509867787361145
train gradient:  0.12538832243590842
iteration : 11180
train acc:  0.8125
train loss:  0.4215273857116699
train gradient:  0.09585882850272391
iteration : 11181
train acc:  0.8203125
train loss:  0.47886061668395996
train gradient:  0.12515414879650494
iteration : 11182
train acc:  0.7421875
train loss:  0.4504898190498352
train gradient:  0.11007558365820624
iteration : 11183
train acc:  0.7578125
train loss:  0.5580844879150391
train gradient:  0.14718512977197767
iteration : 11184
train acc:  0.7734375
train loss:  0.5050966739654541
train gradient:  0.14399781925586252
iteration : 11185
train acc:  0.78125
train loss:  0.5153849124908447
train gradient:  0.1386925924866518
iteration : 11186
train acc:  0.6953125
train loss:  0.5279187560081482
train gradient:  0.14243334727148071
iteration : 11187
train acc:  0.734375
train loss:  0.49166184663772583
train gradient:  0.1257785266780812
iteration : 11188
train acc:  0.8046875
train loss:  0.4657175838947296
train gradient:  0.1311919591545863
iteration : 11189
train acc:  0.7109375
train loss:  0.5320987701416016
train gradient:  0.11193530769813613
iteration : 11190
train acc:  0.6796875
train loss:  0.5218538045883179
train gradient:  0.15659625554950357
iteration : 11191
train acc:  0.7734375
train loss:  0.4491369128227234
train gradient:  0.08794595985092249
iteration : 11192
train acc:  0.7578125
train loss:  0.4816407561302185
train gradient:  0.15608641959851213
iteration : 11193
train acc:  0.7421875
train loss:  0.5222257971763611
train gradient:  0.14311888145356746
iteration : 11194
train acc:  0.796875
train loss:  0.4789149761199951
train gradient:  0.14943125503134128
iteration : 11195
train acc:  0.7109375
train loss:  0.5454502105712891
train gradient:  0.1733079300385328
iteration : 11196
train acc:  0.71875
train loss:  0.5198652148246765
train gradient:  0.17644475977823093
iteration : 11197
train acc:  0.796875
train loss:  0.43245500326156616
train gradient:  0.11548085940768087
iteration : 11198
train acc:  0.765625
train loss:  0.4725481867790222
train gradient:  0.10854518428695747
iteration : 11199
train acc:  0.78125
train loss:  0.45747071504592896
train gradient:  0.10494381562037486
iteration : 11200
train acc:  0.7578125
train loss:  0.4893180727958679
train gradient:  0.1268273674809049
iteration : 11201
train acc:  0.6796875
train loss:  0.5440959930419922
train gradient:  0.143975161183181
iteration : 11202
train acc:  0.6953125
train loss:  0.5423852801322937
train gradient:  0.13551466633974363
iteration : 11203
train acc:  0.7265625
train loss:  0.5445078611373901
train gradient:  0.1468458171820814
iteration : 11204
train acc:  0.7890625
train loss:  0.48540711402893066
train gradient:  0.12327033055061286
iteration : 11205
train acc:  0.7734375
train loss:  0.45857298374176025
train gradient:  0.12132081911171225
iteration : 11206
train acc:  0.734375
train loss:  0.48640546202659607
train gradient:  0.12597055190970557
iteration : 11207
train acc:  0.828125
train loss:  0.43191689252853394
train gradient:  0.08326957056643675
iteration : 11208
train acc:  0.78125
train loss:  0.4959675073623657
train gradient:  0.16536589426007053
iteration : 11209
train acc:  0.6640625
train loss:  0.5078245401382446
train gradient:  0.13237826630338972
iteration : 11210
train acc:  0.78125
train loss:  0.43773627281188965
train gradient:  0.09561314517730427
iteration : 11211
train acc:  0.78125
train loss:  0.45250147581100464
train gradient:  0.0987192859594848
iteration : 11212
train acc:  0.7109375
train loss:  0.49789202213287354
train gradient:  0.12201692188638467
iteration : 11213
train acc:  0.6875
train loss:  0.5262664556503296
train gradient:  0.1531148518021372
iteration : 11214
train acc:  0.7734375
train loss:  0.4786996841430664
train gradient:  0.12294541268522349
iteration : 11215
train acc:  0.671875
train loss:  0.5081018805503845
train gradient:  0.12483947886910925
iteration : 11216
train acc:  0.765625
train loss:  0.46008509397506714
train gradient:  0.11842859833209195
iteration : 11217
train acc:  0.7578125
train loss:  0.48448067903518677
train gradient:  0.13935982711733874
iteration : 11218
train acc:  0.7265625
train loss:  0.4712870717048645
train gradient:  0.09718923503037294
iteration : 11219
train acc:  0.734375
train loss:  0.49178630113601685
train gradient:  0.0966244353379668
iteration : 11220
train acc:  0.7265625
train loss:  0.5238315463066101
train gradient:  0.13177972447434505
iteration : 11221
train acc:  0.6953125
train loss:  0.553638219833374
train gradient:  0.1629260816026386
iteration : 11222
train acc:  0.7265625
train loss:  0.5252946615219116
train gradient:  0.14431636017472066
iteration : 11223
train acc:  0.765625
train loss:  0.4446319341659546
train gradient:  0.10050976488704438
iteration : 11224
train acc:  0.78125
train loss:  0.481222540140152
train gradient:  0.10853447979815466
iteration : 11225
train acc:  0.703125
train loss:  0.4497597813606262
train gradient:  0.09575183566453473
iteration : 11226
train acc:  0.734375
train loss:  0.505972683429718
train gradient:  0.12589901896582523
iteration : 11227
train acc:  0.65625
train loss:  0.5719070434570312
train gradient:  0.17466133159158376
iteration : 11228
train acc:  0.7421875
train loss:  0.4723506569862366
train gradient:  0.09770136413894068
iteration : 11229
train acc:  0.8125
train loss:  0.4282344579696655
train gradient:  0.09561523393779497
iteration : 11230
train acc:  0.7734375
train loss:  0.4627039134502411
train gradient:  0.13032588628788366
iteration : 11231
train acc:  0.8671875
train loss:  0.35466843843460083
train gradient:  0.08576406903515342
iteration : 11232
train acc:  0.71875
train loss:  0.5163848400115967
train gradient:  0.14955318577788884
iteration : 11233
train acc:  0.8125
train loss:  0.4388091266155243
train gradient:  0.10122547231983309
iteration : 11234
train acc:  0.7734375
train loss:  0.4598158001899719
train gradient:  0.09081643768951563
iteration : 11235
train acc:  0.765625
train loss:  0.4792565107345581
train gradient:  0.13310924185999046
iteration : 11236
train acc:  0.8125
train loss:  0.4356919229030609
train gradient:  0.0977550316398521
iteration : 11237
train acc:  0.859375
train loss:  0.4234415292739868
train gradient:  0.10722394104280561
iteration : 11238
train acc:  0.6875
train loss:  0.5153948068618774
train gradient:  0.11638859434058726
iteration : 11239
train acc:  0.75
train loss:  0.49502313137054443
train gradient:  0.12231772726888145
iteration : 11240
train acc:  0.75
train loss:  0.4609462022781372
train gradient:  0.12216006023748598
iteration : 11241
train acc:  0.75
train loss:  0.45997440814971924
train gradient:  0.1340728378426858
iteration : 11242
train acc:  0.765625
train loss:  0.4273059666156769
train gradient:  0.09801089951457743
iteration : 11243
train acc:  0.78125
train loss:  0.46008914709091187
train gradient:  0.10083564348417627
iteration : 11244
train acc:  0.75
train loss:  0.5230679512023926
train gradient:  0.11450774386918516
iteration : 11245
train acc:  0.84375
train loss:  0.42370715737342834
train gradient:  0.09391477365645325
iteration : 11246
train acc:  0.75
train loss:  0.5112014412879944
train gradient:  0.15585695953797546
iteration : 11247
train acc:  0.75
train loss:  0.4727524220943451
train gradient:  0.11969589647046844
iteration : 11248
train acc:  0.796875
train loss:  0.3987147808074951
train gradient:  0.09252285037861092
iteration : 11249
train acc:  0.703125
train loss:  0.5198391675949097
train gradient:  0.13320586080324967
iteration : 11250
train acc:  0.7890625
train loss:  0.4704492390155792
train gradient:  0.13401987071653138
iteration : 11251
train acc:  0.7421875
train loss:  0.4842647910118103
train gradient:  0.13342538009781257
iteration : 11252
train acc:  0.7578125
train loss:  0.515418291091919
train gradient:  0.15254654901038883
iteration : 11253
train acc:  0.8046875
train loss:  0.456180214881897
train gradient:  0.10023663880506752
iteration : 11254
train acc:  0.75
train loss:  0.4638030230998993
train gradient:  0.11843305833254948
iteration : 11255
train acc:  0.828125
train loss:  0.4164508581161499
train gradient:  0.0983631587263112
iteration : 11256
train acc:  0.6953125
train loss:  0.5420452952384949
train gradient:  0.15243244266171208
iteration : 11257
train acc:  0.75
train loss:  0.4682399034500122
train gradient:  0.13652790900456763
iteration : 11258
train acc:  0.7421875
train loss:  0.49851274490356445
train gradient:  0.1072149741756612
iteration : 11259
train acc:  0.765625
train loss:  0.46981537342071533
train gradient:  0.11157249550372934
iteration : 11260
train acc:  0.765625
train loss:  0.45530009269714355
train gradient:  0.11985808118929903
iteration : 11261
train acc:  0.8359375
train loss:  0.40097641944885254
train gradient:  0.09389309358941347
iteration : 11262
train acc:  0.703125
train loss:  0.5013586282730103
train gradient:  0.14660536508960562
iteration : 11263
train acc:  0.765625
train loss:  0.45391392707824707
train gradient:  0.1425834599484171
iteration : 11264
train acc:  0.78125
train loss:  0.4730059504508972
train gradient:  0.12030436231893935
iteration : 11265
train acc:  0.78125
train loss:  0.45360708236694336
train gradient:  0.09365412378604791
iteration : 11266
train acc:  0.765625
train loss:  0.45807474851608276
train gradient:  0.10128216692431752
iteration : 11267
train acc:  0.7734375
train loss:  0.47332918643951416
train gradient:  0.10828381568321392
iteration : 11268
train acc:  0.765625
train loss:  0.49053657054901123
train gradient:  0.14551339636965333
iteration : 11269
train acc:  0.7421875
train loss:  0.4678870439529419
train gradient:  0.09993846183720871
iteration : 11270
train acc:  0.7265625
train loss:  0.48631808161735535
train gradient:  0.10568276136062452
iteration : 11271
train acc:  0.75
train loss:  0.4759658873081207
train gradient:  0.1070746385098231
iteration : 11272
train acc:  0.7265625
train loss:  0.543648362159729
train gradient:  0.13561476015072338
iteration : 11273
train acc:  0.6875
train loss:  0.5103633403778076
train gradient:  0.12601658295195622
iteration : 11274
train acc:  0.6875
train loss:  0.5639695525169373
train gradient:  0.18846928270379493
iteration : 11275
train acc:  0.6953125
train loss:  0.5579922199249268
train gradient:  0.16627442078056165
iteration : 11276
train acc:  0.703125
train loss:  0.5240837335586548
train gradient:  0.1605252178124144
iteration : 11277
train acc:  0.7578125
train loss:  0.4727290868759155
train gradient:  0.118287631715889
iteration : 11278
train acc:  0.7421875
train loss:  0.4639652967453003
train gradient:  0.12491864682876566
iteration : 11279
train acc:  0.7109375
train loss:  0.5300992131233215
train gradient:  0.18477613611981203
iteration : 11280
train acc:  0.7734375
train loss:  0.4739193320274353
train gradient:  0.10202652141613644
iteration : 11281
train acc:  0.6875
train loss:  0.5491832494735718
train gradient:  0.14317014973312647
iteration : 11282
train acc:  0.8125
train loss:  0.4577634334564209
train gradient:  0.112081768967495
iteration : 11283
train acc:  0.6953125
train loss:  0.5474661588668823
train gradient:  0.15121738272493496
iteration : 11284
train acc:  0.7734375
train loss:  0.4458090662956238
train gradient:  0.1110979718294576
iteration : 11285
train acc:  0.6640625
train loss:  0.5816981792449951
train gradient:  0.17077380844611528
iteration : 11286
train acc:  0.75
train loss:  0.525704026222229
train gradient:  0.1813210637075815
iteration : 11287
train acc:  0.6953125
train loss:  0.5281708240509033
train gradient:  0.16094503656435816
iteration : 11288
train acc:  0.8125
train loss:  0.42386236786842346
train gradient:  0.0894611988820144
iteration : 11289
train acc:  0.7734375
train loss:  0.4704102575778961
train gradient:  0.1315450015137127
iteration : 11290
train acc:  0.7265625
train loss:  0.46763426065444946
train gradient:  0.10378389180063123
iteration : 11291
train acc:  0.703125
train loss:  0.5369979739189148
train gradient:  0.1723521637059269
iteration : 11292
train acc:  0.75
train loss:  0.5225032567977905
train gradient:  0.12963076977676208
iteration : 11293
train acc:  0.765625
train loss:  0.48449715971946716
train gradient:  0.11218573073583972
iteration : 11294
train acc:  0.734375
train loss:  0.5145244002342224
train gradient:  0.12027852094466647
iteration : 11295
train acc:  0.703125
train loss:  0.5333386659622192
train gradient:  0.14038135074056116
iteration : 11296
train acc:  0.7265625
train loss:  0.5256350040435791
train gradient:  0.13573810448004023
iteration : 11297
train acc:  0.75
train loss:  0.4879087805747986
train gradient:  0.1347511793750298
iteration : 11298
train acc:  0.734375
train loss:  0.5098263621330261
train gradient:  0.11989101746828565
iteration : 11299
train acc:  0.7421875
train loss:  0.47390425205230713
train gradient:  0.1580658433225816
iteration : 11300
train acc:  0.7265625
train loss:  0.537572979927063
train gradient:  0.12256463005717666
iteration : 11301
train acc:  0.734375
train loss:  0.5011773705482483
train gradient:  0.13449330170537227
iteration : 11302
train acc:  0.7421875
train loss:  0.4912468194961548
train gradient:  0.12495064103598055
iteration : 11303
train acc:  0.78125
train loss:  0.44276583194732666
train gradient:  0.08429083018187043
iteration : 11304
train acc:  0.703125
train loss:  0.48911166191101074
train gradient:  0.10795794841852284
iteration : 11305
train acc:  0.71875
train loss:  0.5188698768615723
train gradient:  0.13932738267641892
iteration : 11306
train acc:  0.703125
train loss:  0.5376477241516113
train gradient:  0.14135825613838682
iteration : 11307
train acc:  0.7265625
train loss:  0.5008448958396912
train gradient:  0.14146700493747777
iteration : 11308
train acc:  0.7578125
train loss:  0.529930055141449
train gradient:  0.14883301664380844
iteration : 11309
train acc:  0.7265625
train loss:  0.5649080276489258
train gradient:  0.16158929326882962
iteration : 11310
train acc:  0.78125
train loss:  0.43526792526245117
train gradient:  0.11347498666576362
iteration : 11311
train acc:  0.8125
train loss:  0.42000657320022583
train gradient:  0.11022569275733538
iteration : 11312
train acc:  0.6953125
train loss:  0.5760848522186279
train gradient:  0.15777023336103982
iteration : 11313
train acc:  0.7109375
train loss:  0.46218758821487427
train gradient:  0.11445327439494364
iteration : 11314
train acc:  0.75
train loss:  0.4548379182815552
train gradient:  0.10734876616893513
iteration : 11315
train acc:  0.75
train loss:  0.45801040530204773
train gradient:  0.15396193697116722
iteration : 11316
train acc:  0.71875
train loss:  0.54917973279953
train gradient:  0.14500706625660859
iteration : 11317
train acc:  0.7109375
train loss:  0.5567300915718079
train gradient:  0.16822475767600775
iteration : 11318
train acc:  0.75
train loss:  0.4762883186340332
train gradient:  0.10318715785463949
iteration : 11319
train acc:  0.7109375
train loss:  0.5362311601638794
train gradient:  0.12245136319622327
iteration : 11320
train acc:  0.71875
train loss:  0.47211143374443054
train gradient:  0.11056773505829304
iteration : 11321
train acc:  0.7578125
train loss:  0.5047786831855774
train gradient:  0.13612788690882635
iteration : 11322
train acc:  0.703125
train loss:  0.5663488507270813
train gradient:  0.16018714540506768
iteration : 11323
train acc:  0.7578125
train loss:  0.49005401134490967
train gradient:  0.12445917882636956
iteration : 11324
train acc:  0.7734375
train loss:  0.432425856590271
train gradient:  0.14007636192597298
iteration : 11325
train acc:  0.7578125
train loss:  0.48481351137161255
train gradient:  0.11307695238497223
iteration : 11326
train acc:  0.7421875
train loss:  0.4643435776233673
train gradient:  0.10449701742878473
iteration : 11327
train acc:  0.7109375
train loss:  0.5069429874420166
train gradient:  0.15010848438187246
iteration : 11328
train acc:  0.75
train loss:  0.517145574092865
train gradient:  0.1355985370007936
iteration : 11329
train acc:  0.6875
train loss:  0.5966454744338989
train gradient:  0.14893877017106824
iteration : 11330
train acc:  0.71875
train loss:  0.45213907957077026
train gradient:  0.0864861563232002
iteration : 11331
train acc:  0.7734375
train loss:  0.4286887049674988
train gradient:  0.10121019959094718
iteration : 11332
train acc:  0.71875
train loss:  0.5515307188034058
train gradient:  0.14257442027505268
iteration : 11333
train acc:  0.7734375
train loss:  0.4421094059944153
train gradient:  0.11068410471553183
iteration : 11334
train acc:  0.7265625
train loss:  0.5219771862030029
train gradient:  0.11847919154036837
iteration : 11335
train acc:  0.7109375
train loss:  0.49697649478912354
train gradient:  0.1349248230356769
iteration : 11336
train acc:  0.7109375
train loss:  0.4915943443775177
train gradient:  0.11538365992021658
iteration : 11337
train acc:  0.78125
train loss:  0.48250287771224976
train gradient:  0.11766187395709636
iteration : 11338
train acc:  0.8046875
train loss:  0.47617974877357483
train gradient:  0.13089930632153712
iteration : 11339
train acc:  0.7734375
train loss:  0.4659818410873413
train gradient:  0.10459210973821066
iteration : 11340
train acc:  0.7109375
train loss:  0.5097644925117493
train gradient:  0.13138350920713882
iteration : 11341
train acc:  0.7265625
train loss:  0.4800287187099457
train gradient:  0.14727947815539819
iteration : 11342
train acc:  0.7265625
train loss:  0.4978598952293396
train gradient:  0.11014584483886633
iteration : 11343
train acc:  0.7734375
train loss:  0.43580904603004456
train gradient:  0.08869497501689208
iteration : 11344
train acc:  0.7421875
train loss:  0.5027844905853271
train gradient:  0.11288013592180511
iteration : 11345
train acc:  0.7578125
train loss:  0.5033285617828369
train gradient:  0.12024266816635538
iteration : 11346
train acc:  0.7421875
train loss:  0.4317093789577484
train gradient:  0.1122358859698093
iteration : 11347
train acc:  0.75
train loss:  0.4584749937057495
train gradient:  0.09915898650383632
iteration : 11348
train acc:  0.8515625
train loss:  0.3457162380218506
train gradient:  0.08390117207875163
iteration : 11349
train acc:  0.75
train loss:  0.48328229784965515
train gradient:  0.14021924424972082
iteration : 11350
train acc:  0.7890625
train loss:  0.434266597032547
train gradient:  0.11523149555230491
iteration : 11351
train acc:  0.796875
train loss:  0.43981295824050903
train gradient:  0.10293393238155286
iteration : 11352
train acc:  0.71875
train loss:  0.5176945924758911
train gradient:  0.15946794855432223
iteration : 11353
train acc:  0.703125
train loss:  0.5091264247894287
train gradient:  0.13710516931665412
iteration : 11354
train acc:  0.765625
train loss:  0.45703426003456116
train gradient:  0.10603741476487033
iteration : 11355
train acc:  0.6796875
train loss:  0.5879778265953064
train gradient:  0.15549652190046798
iteration : 11356
train acc:  0.6328125
train loss:  0.638494610786438
train gradient:  0.1860857800843215
iteration : 11357
train acc:  0.8125
train loss:  0.4317171573638916
train gradient:  0.09411906168783504
iteration : 11358
train acc:  0.7734375
train loss:  0.425859272480011
train gradient:  0.08003534963989882
iteration : 11359
train acc:  0.78125
train loss:  0.4448797106742859
train gradient:  0.1100062589003677
iteration : 11360
train acc:  0.828125
train loss:  0.4413715600967407
train gradient:  0.13693357520611096
iteration : 11361
train acc:  0.734375
train loss:  0.4461526572704315
train gradient:  0.10378766600798772
iteration : 11362
train acc:  0.765625
train loss:  0.4899742007255554
train gradient:  0.11775210018881473
iteration : 11363
train acc:  0.7578125
train loss:  0.4578213095664978
train gradient:  0.11985045352083631
iteration : 11364
train acc:  0.7421875
train loss:  0.5129714012145996
train gradient:  0.11678150733795965
iteration : 11365
train acc:  0.7890625
train loss:  0.4300267696380615
train gradient:  0.09279337172967886
iteration : 11366
train acc:  0.78125
train loss:  0.4607378840446472
train gradient:  0.13170533668202528
iteration : 11367
train acc:  0.8203125
train loss:  0.42487025260925293
train gradient:  0.09489480618030731
iteration : 11368
train acc:  0.78125
train loss:  0.4649985134601593
train gradient:  0.1141858139823751
iteration : 11369
train acc:  0.7890625
train loss:  0.4326893091201782
train gradient:  0.08822920576368971
iteration : 11370
train acc:  0.7578125
train loss:  0.44889959692955017
train gradient:  0.10917169820249152
iteration : 11371
train acc:  0.703125
train loss:  0.5503414869308472
train gradient:  0.1507375273495
iteration : 11372
train acc:  0.765625
train loss:  0.455791175365448
train gradient:  0.10334507714880561
iteration : 11373
train acc:  0.78125
train loss:  0.44143399596214294
train gradient:  0.09042862891199388
iteration : 11374
train acc:  0.78125
train loss:  0.44223764538764954
train gradient:  0.09311352038756848
iteration : 11375
train acc:  0.71875
train loss:  0.5474464893341064
train gradient:  0.2141333169049543
iteration : 11376
train acc:  0.703125
train loss:  0.5470567941665649
train gradient:  0.140654730830085
iteration : 11377
train acc:  0.75
train loss:  0.4846348762512207
train gradient:  0.11401682444546656
iteration : 11378
train acc:  0.7421875
train loss:  0.45561468601226807
train gradient:  0.10602295491536369
iteration : 11379
train acc:  0.703125
train loss:  0.5075145363807678
train gradient:  0.13385261653652766
iteration : 11380
train acc:  0.7421875
train loss:  0.4350603222846985
train gradient:  0.11221038911617615
iteration : 11381
train acc:  0.75
train loss:  0.5084618330001831
train gradient:  0.13153477769773425
iteration : 11382
train acc:  0.796875
train loss:  0.4266487956047058
train gradient:  0.10232332568309294
iteration : 11383
train acc:  0.75
train loss:  0.5063902139663696
train gradient:  0.17521167583947117
iteration : 11384
train acc:  0.6796875
train loss:  0.5654460787773132
train gradient:  0.16502586534137614
iteration : 11385
train acc:  0.765625
train loss:  0.4769198000431061
train gradient:  0.12910423327650739
iteration : 11386
train acc:  0.78125
train loss:  0.4666014313697815
train gradient:  0.10785268009116382
iteration : 11387
train acc:  0.7578125
train loss:  0.48925065994262695
train gradient:  0.12947637238878273
iteration : 11388
train acc:  0.765625
train loss:  0.45880573987960815
train gradient:  0.10681204513943111
iteration : 11389
train acc:  0.6484375
train loss:  0.5949386358261108
train gradient:  0.1567630254763584
iteration : 11390
train acc:  0.75
train loss:  0.48476096987724304
train gradient:  0.1305792889232386
iteration : 11391
train acc:  0.7890625
train loss:  0.47036951780319214
train gradient:  0.14138221976628745
iteration : 11392
train acc:  0.796875
train loss:  0.4250819683074951
train gradient:  0.11401806661582825
iteration : 11393
train acc:  0.734375
train loss:  0.5720433592796326
train gradient:  0.16702135293206305
iteration : 11394
train acc:  0.7109375
train loss:  0.5438128113746643
train gradient:  0.14341048860816827
iteration : 11395
train acc:  0.7890625
train loss:  0.4508533477783203
train gradient:  0.13274231244931717
iteration : 11396
train acc:  0.7421875
train loss:  0.45924609899520874
train gradient:  0.1261757211097616
iteration : 11397
train acc:  0.78125
train loss:  0.4493635296821594
train gradient:  0.1249011347265145
iteration : 11398
train acc:  0.8125
train loss:  0.425703763961792
train gradient:  0.1185502457528851
iteration : 11399
train acc:  0.7265625
train loss:  0.5176641941070557
train gradient:  0.12757952595928274
iteration : 11400
train acc:  0.75
train loss:  0.4937450885772705
train gradient:  0.13950337312550254
iteration : 11401
train acc:  0.8125
train loss:  0.4315434694290161
train gradient:  0.0873619348412649
iteration : 11402
train acc:  0.71875
train loss:  0.56271892786026
train gradient:  0.1719813317881519
iteration : 11403
train acc:  0.765625
train loss:  0.4731908440589905
train gradient:  0.11312225790132106
iteration : 11404
train acc:  0.7265625
train loss:  0.5081721544265747
train gradient:  0.14067912201413704
iteration : 11405
train acc:  0.703125
train loss:  0.510929524898529
train gradient:  0.19085201329596385
iteration : 11406
train acc:  0.796875
train loss:  0.43318647146224976
train gradient:  0.09475401307783482
iteration : 11407
train acc:  0.7265625
train loss:  0.4670131206512451
train gradient:  0.14032043461994687
iteration : 11408
train acc:  0.7578125
train loss:  0.5107775926589966
train gradient:  0.12991455130839977
iteration : 11409
train acc:  0.71875
train loss:  0.5134626626968384
train gradient:  0.13623354962091277
iteration : 11410
train acc:  0.765625
train loss:  0.44550108909606934
train gradient:  0.08713626170952878
iteration : 11411
train acc:  0.8046875
train loss:  0.45112913846969604
train gradient:  0.1286539493614463
iteration : 11412
train acc:  0.765625
train loss:  0.4191502332687378
train gradient:  0.0838922002325562
iteration : 11413
train acc:  0.734375
train loss:  0.5180879235267639
train gradient:  0.14529499301726684
iteration : 11414
train acc:  0.6875
train loss:  0.487643837928772
train gradient:  0.12310574745249109
iteration : 11415
train acc:  0.71875
train loss:  0.5494242310523987
train gradient:  0.17827093290643636
iteration : 11416
train acc:  0.7890625
train loss:  0.46848064661026
train gradient:  0.12707232146802355
iteration : 11417
train acc:  0.7734375
train loss:  0.48041170835494995
train gradient:  0.14849509445213369
iteration : 11418
train acc:  0.7890625
train loss:  0.4556882679462433
train gradient:  0.12398591708584757
iteration : 11419
train acc:  0.7578125
train loss:  0.47267860174179077
train gradient:  0.11130415581969628
iteration : 11420
train acc:  0.703125
train loss:  0.5610241293907166
train gradient:  0.15300127770548488
iteration : 11421
train acc:  0.7109375
train loss:  0.5199657082557678
train gradient:  0.1178332649738278
iteration : 11422
train acc:  0.703125
train loss:  0.6028087735176086
train gradient:  0.16624929982418046
iteration : 11423
train acc:  0.796875
train loss:  0.4599025249481201
train gradient:  0.13419837685064967
iteration : 11424
train acc:  0.796875
train loss:  0.41058963537216187
train gradient:  0.09961474999766676
iteration : 11425
train acc:  0.7734375
train loss:  0.46430397033691406
train gradient:  0.12191821808527838
iteration : 11426
train acc:  0.7578125
train loss:  0.5025218725204468
train gradient:  0.11527886065532461
iteration : 11427
train acc:  0.7578125
train loss:  0.46590402722358704
train gradient:  0.10939748454015842
iteration : 11428
train acc:  0.7578125
train loss:  0.4334631562232971
train gradient:  0.10453552990610514
iteration : 11429
train acc:  0.7265625
train loss:  0.49399101734161377
train gradient:  0.124069755861731
iteration : 11430
train acc:  0.703125
train loss:  0.4980449080467224
train gradient:  0.1741137508499162
iteration : 11431
train acc:  0.765625
train loss:  0.42867282032966614
train gradient:  0.09945864780907095
iteration : 11432
train acc:  0.7109375
train loss:  0.4893296957015991
train gradient:  0.1037732779700953
iteration : 11433
train acc:  0.78125
train loss:  0.44263896346092224
train gradient:  0.09166816918582914
iteration : 11434
train acc:  0.703125
train loss:  0.5594552755355835
train gradient:  0.15662043621621263
iteration : 11435
train acc:  0.734375
train loss:  0.4826510548591614
train gradient:  0.10613437889515699
iteration : 11436
train acc:  0.7421875
train loss:  0.4426490068435669
train gradient:  0.10450950050430205
iteration : 11437
train acc:  0.7890625
train loss:  0.43954598903656006
train gradient:  0.1232173642119457
iteration : 11438
train acc:  0.71875
train loss:  0.5094922184944153
train gradient:  0.17025670665390766
iteration : 11439
train acc:  0.7265625
train loss:  0.5346150398254395
train gradient:  0.14762437684752472
iteration : 11440
train acc:  0.796875
train loss:  0.48479828238487244
train gradient:  0.15703236698626155
iteration : 11441
train acc:  0.7265625
train loss:  0.5528063774108887
train gradient:  0.2042015135587727
iteration : 11442
train acc:  0.6796875
train loss:  0.5180776119232178
train gradient:  0.16357316902504065
iteration : 11443
train acc:  0.7421875
train loss:  0.4633883237838745
train gradient:  0.12184060310433416
iteration : 11444
train acc:  0.734375
train loss:  0.47229140996932983
train gradient:  0.11308592236177321
iteration : 11445
train acc:  0.71875
train loss:  0.4888078570365906
train gradient:  0.13339967767558125
iteration : 11446
train acc:  0.734375
train loss:  0.5197311639785767
train gradient:  0.13672779937695412
iteration : 11447
train acc:  0.7109375
train loss:  0.5343165397644043
train gradient:  0.14865131645200355
iteration : 11448
train acc:  0.703125
train loss:  0.5728143453598022
train gradient:  0.1856253959243182
iteration : 11449
train acc:  0.6953125
train loss:  0.5525436401367188
train gradient:  0.16040099332590535
iteration : 11450
train acc:  0.765625
train loss:  0.4838266670703888
train gradient:  0.11053787173754688
iteration : 11451
train acc:  0.7578125
train loss:  0.5047760605812073
train gradient:  0.12210865706710308
iteration : 11452
train acc:  0.765625
train loss:  0.4361720085144043
train gradient:  0.09537307658005299
iteration : 11453
train acc:  0.7734375
train loss:  0.4716007113456726
train gradient:  0.1135627788596352
iteration : 11454
train acc:  0.78125
train loss:  0.4511423707008362
train gradient:  0.11376913324829588
iteration : 11455
train acc:  0.71875
train loss:  0.519841194152832
train gradient:  0.14132126822805574
iteration : 11456
train acc:  0.75
train loss:  0.49224063754081726
train gradient:  0.14707690788126557
iteration : 11457
train acc:  0.7265625
train loss:  0.4927716553211212
train gradient:  0.1252301447609461
iteration : 11458
train acc:  0.6875
train loss:  0.5118367671966553
train gradient:  0.15810333786899877
iteration : 11459
train acc:  0.8046875
train loss:  0.4439091384410858
train gradient:  0.11163987801808033
iteration : 11460
train acc:  0.7890625
train loss:  0.4384472072124481
train gradient:  0.0974566974490516
iteration : 11461
train acc:  0.7578125
train loss:  0.45103204250335693
train gradient:  0.10105828167844563
iteration : 11462
train acc:  0.7265625
train loss:  0.5096503496170044
train gradient:  0.1288804149842796
iteration : 11463
train acc:  0.78125
train loss:  0.44779065251350403
train gradient:  0.11049330831239726
iteration : 11464
train acc:  0.7890625
train loss:  0.41594627499580383
train gradient:  0.09162325255495202
iteration : 11465
train acc:  0.71875
train loss:  0.5309039354324341
train gradient:  0.12377660000719427
iteration : 11466
train acc:  0.8359375
train loss:  0.41866350173950195
train gradient:  0.07722584089280006
iteration : 11467
train acc:  0.734375
train loss:  0.4554103910923004
train gradient:  0.1233573784880241
iteration : 11468
train acc:  0.765625
train loss:  0.48260772228240967
train gradient:  0.14035050218428588
iteration : 11469
train acc:  0.7578125
train loss:  0.48329782485961914
train gradient:  0.11767130529784557
iteration : 11470
train acc:  0.75
train loss:  0.4998854100704193
train gradient:  0.14266832796126694
iteration : 11471
train acc:  0.6953125
train loss:  0.6025851368904114
train gradient:  0.1550115066493417
iteration : 11472
train acc:  0.7421875
train loss:  0.4719163775444031
train gradient:  0.10824298019311844
iteration : 11473
train acc:  0.7578125
train loss:  0.4343847632408142
train gradient:  0.10396527127792292
iteration : 11474
train acc:  0.703125
train loss:  0.6110782623291016
train gradient:  0.1932051719844461
iteration : 11475
train acc:  0.71875
train loss:  0.4831337332725525
train gradient:  0.11476094079854454
iteration : 11476
train acc:  0.6875
train loss:  0.5339982509613037
train gradient:  0.13846628832752897
iteration : 11477
train acc:  0.8125
train loss:  0.44072675704956055
train gradient:  0.09745839150882891
iteration : 11478
train acc:  0.734375
train loss:  0.4662187695503235
train gradient:  0.11722511889049628
iteration : 11479
train acc:  0.7421875
train loss:  0.46112775802612305
train gradient:  0.1139688934608157
iteration : 11480
train acc:  0.7421875
train loss:  0.4723930358886719
train gradient:  0.08944339433744404
iteration : 11481
train acc:  0.71875
train loss:  0.5597035884857178
train gradient:  0.12895704808692876
iteration : 11482
train acc:  0.7578125
train loss:  0.4855746626853943
train gradient:  0.11546710410645268
iteration : 11483
train acc:  0.6875
train loss:  0.4817579388618469
train gradient:  0.15176909059507726
iteration : 11484
train acc:  0.65625
train loss:  0.6043064594268799
train gradient:  0.20049229465024
iteration : 11485
train acc:  0.7734375
train loss:  0.4666352868080139
train gradient:  0.11747359517728469
iteration : 11486
train acc:  0.765625
train loss:  0.4695145785808563
train gradient:  0.11043060904327247
iteration : 11487
train acc:  0.7734375
train loss:  0.45909419655799866
train gradient:  0.10965953952147049
iteration : 11488
train acc:  0.7421875
train loss:  0.49169519543647766
train gradient:  0.13295901959827344
iteration : 11489
train acc:  0.734375
train loss:  0.46523842215538025
train gradient:  0.11629479181491285
iteration : 11490
train acc:  0.7578125
train loss:  0.48722565174102783
train gradient:  0.1517980750423078
iteration : 11491
train acc:  0.7421875
train loss:  0.4837735891342163
train gradient:  0.13500170115208127
iteration : 11492
train acc:  0.75
train loss:  0.4310126304626465
train gradient:  0.08251944571200524
iteration : 11493
train acc:  0.8203125
train loss:  0.45017507672309875
train gradient:  0.1255023558284541
iteration : 11494
train acc:  0.734375
train loss:  0.4637035131454468
train gradient:  0.11370225134633884
iteration : 11495
train acc:  0.75
train loss:  0.5235518217086792
train gradient:  0.13533636484607228
iteration : 11496
train acc:  0.6796875
train loss:  0.5466662049293518
train gradient:  0.156921050658658
iteration : 11497
train acc:  0.7421875
train loss:  0.5117130279541016
train gradient:  0.12126605648689184
iteration : 11498
train acc:  0.765625
train loss:  0.48700469732284546
train gradient:  0.20822544992378816
iteration : 11499
train acc:  0.796875
train loss:  0.42233026027679443
train gradient:  0.09171573060701081
iteration : 11500
train acc:  0.7421875
train loss:  0.5458788871765137
train gradient:  0.17362497025342993
iteration : 11501
train acc:  0.71875
train loss:  0.5269978642463684
train gradient:  0.1596475243870594
iteration : 11502
train acc:  0.7734375
train loss:  0.4816420376300812
train gradient:  0.13387858147718046
iteration : 11503
train acc:  0.7265625
train loss:  0.5795164108276367
train gradient:  0.16358390246670726
iteration : 11504
train acc:  0.734375
train loss:  0.5059735178947449
train gradient:  0.13104027027463522
iteration : 11505
train acc:  0.7734375
train loss:  0.4969710111618042
train gradient:  0.11795608732786658
iteration : 11506
train acc:  0.796875
train loss:  0.41998475790023804
train gradient:  0.11909290563158954
iteration : 11507
train acc:  0.71875
train loss:  0.4824943244457245
train gradient:  0.11222488568851872
iteration : 11508
train acc:  0.7578125
train loss:  0.4368951916694641
train gradient:  0.09162326970439529
iteration : 11509
train acc:  0.7421875
train loss:  0.5005075931549072
train gradient:  0.12778013499731256
iteration : 11510
train acc:  0.7734375
train loss:  0.4780714213848114
train gradient:  0.14029084256466445
iteration : 11511
train acc:  0.765625
train loss:  0.4866037368774414
train gradient:  0.10362278164633697
iteration : 11512
train acc:  0.703125
train loss:  0.5026957392692566
train gradient:  0.11634417374727037
iteration : 11513
train acc:  0.6953125
train loss:  0.5468641519546509
train gradient:  0.1384344518356077
iteration : 11514
train acc:  0.734375
train loss:  0.47442004084587097
train gradient:  0.12780489815638046
iteration : 11515
train acc:  0.71875
train loss:  0.5640612840652466
train gradient:  0.22269125268287343
iteration : 11516
train acc:  0.7265625
train loss:  0.5549593567848206
train gradient:  0.13616102213555245
iteration : 11517
train acc:  0.734375
train loss:  0.49566757678985596
train gradient:  0.13489069540368262
iteration : 11518
train acc:  0.7578125
train loss:  0.46861952543258667
train gradient:  0.14197341024116972
iteration : 11519
train acc:  0.7734375
train loss:  0.4769734740257263
train gradient:  0.1358775074816616
iteration : 11520
train acc:  0.7265625
train loss:  0.4684752821922302
train gradient:  0.11111994274698117
iteration : 11521
train acc:  0.7421875
train loss:  0.5074194073677063
train gradient:  0.1319497279891897
iteration : 11522
train acc:  0.7734375
train loss:  0.4342144727706909
train gradient:  0.11175106558730538
iteration : 11523
train acc:  0.7734375
train loss:  0.4260644018650055
train gradient:  0.09720217570323164
iteration : 11524
train acc:  0.75
train loss:  0.5307274460792542
train gradient:  0.17792701500124175
iteration : 11525
train acc:  0.7734375
train loss:  0.45916736125946045
train gradient:  0.11722346286346187
iteration : 11526
train acc:  0.734375
train loss:  0.4714418053627014
train gradient:  0.12472983104596536
iteration : 11527
train acc:  0.7421875
train loss:  0.5226050019264221
train gradient:  0.14425018016319924
iteration : 11528
train acc:  0.71875
train loss:  0.5012631416320801
train gradient:  0.10736233361664829
iteration : 11529
train acc:  0.7734375
train loss:  0.47308820486068726
train gradient:  0.09884103344848268
iteration : 11530
train acc:  0.7421875
train loss:  0.5354444980621338
train gradient:  0.13697927762895007
iteration : 11531
train acc:  0.734375
train loss:  0.5055292248725891
train gradient:  0.105712200564078
iteration : 11532
train acc:  0.8125
train loss:  0.4185602068901062
train gradient:  0.10987157607738823
iteration : 11533
train acc:  0.71875
train loss:  0.5440541505813599
train gradient:  0.17064755571711754
iteration : 11534
train acc:  0.734375
train loss:  0.5201014280319214
train gradient:  0.10598967772435583
iteration : 11535
train acc:  0.765625
train loss:  0.47072944045066833
train gradient:  0.09876500490474538
iteration : 11536
train acc:  0.6953125
train loss:  0.5424036979675293
train gradient:  0.14645328140281588
iteration : 11537
train acc:  0.7421875
train loss:  0.550557017326355
train gradient:  0.18077505406299277
iteration : 11538
train acc:  0.7265625
train loss:  0.4649449288845062
train gradient:  0.12847472253359754
iteration : 11539
train acc:  0.7578125
train loss:  0.4726807773113251
train gradient:  0.12118191434268888
iteration : 11540
train acc:  0.6875
train loss:  0.5175260305404663
train gradient:  0.12176582594524928
iteration : 11541
train acc:  0.765625
train loss:  0.49992698431015015
train gradient:  0.11515239320623367
iteration : 11542
train acc:  0.84375
train loss:  0.3800414204597473
train gradient:  0.06764611345599567
iteration : 11543
train acc:  0.7578125
train loss:  0.44115138053894043
train gradient:  0.1099736523066819
iteration : 11544
train acc:  0.6953125
train loss:  0.5531499981880188
train gradient:  0.13550579673983798
iteration : 11545
train acc:  0.796875
train loss:  0.45729246735572815
train gradient:  0.12631739459049757
iteration : 11546
train acc:  0.765625
train loss:  0.49003326892852783
train gradient:  0.11644524558801085
iteration : 11547
train acc:  0.828125
train loss:  0.39607107639312744
train gradient:  0.08235313536998579
iteration : 11548
train acc:  0.71875
train loss:  0.5711124539375305
train gradient:  0.19842913796035322
iteration : 11549
train acc:  0.7109375
train loss:  0.5084692239761353
train gradient:  0.11157001455960093
iteration : 11550
train acc:  0.7890625
train loss:  0.4599178731441498
train gradient:  0.12010158053262203
iteration : 11551
train acc:  0.78125
train loss:  0.4631458520889282
train gradient:  0.09192899936940623
iteration : 11552
train acc:  0.734375
train loss:  0.435496985912323
train gradient:  0.12417114660423412
iteration : 11553
train acc:  0.7109375
train loss:  0.5026266574859619
train gradient:  0.12263741299303803
iteration : 11554
train acc:  0.75
train loss:  0.5067198276519775
train gradient:  0.1146921318451421
iteration : 11555
train acc:  0.6953125
train loss:  0.5808966159820557
train gradient:  0.19031598846271625
iteration : 11556
train acc:  0.7109375
train loss:  0.5074681639671326
train gradient:  0.14019560657471422
iteration : 11557
train acc:  0.6953125
train loss:  0.6110397577285767
train gradient:  0.17531725600115602
iteration : 11558
train acc:  0.8125
train loss:  0.41837257146835327
train gradient:  0.11382657351307449
iteration : 11559
train acc:  0.8125
train loss:  0.4989911913871765
train gradient:  0.17443191374863684
iteration : 11560
train acc:  0.7578125
train loss:  0.4757660925388336
train gradient:  0.09930178112115418
iteration : 11561
train acc:  0.734375
train loss:  0.4992954730987549
train gradient:  0.14773362088850406
iteration : 11562
train acc:  0.8359375
train loss:  0.3934789299964905
train gradient:  0.0818916266184591
iteration : 11563
train acc:  0.7109375
train loss:  0.502910315990448
train gradient:  0.16834388447974924
iteration : 11564
train acc:  0.75
train loss:  0.4887821078300476
train gradient:  0.11141469606732002
iteration : 11565
train acc:  0.71875
train loss:  0.6016252040863037
train gradient:  0.15951030395270055
iteration : 11566
train acc:  0.7109375
train loss:  0.4886673092842102
train gradient:  0.13766387937312857
iteration : 11567
train acc:  0.6796875
train loss:  0.5374754667282104
train gradient:  0.1962194908040766
iteration : 11568
train acc:  0.71875
train loss:  0.5394032001495361
train gradient:  0.15561360641247704
iteration : 11569
train acc:  0.6953125
train loss:  0.5719150304794312
train gradient:  0.12708778381306274
iteration : 11570
train acc:  0.7890625
train loss:  0.45252588391304016
train gradient:  0.09616516248345196
iteration : 11571
train acc:  0.7734375
train loss:  0.4638047218322754
train gradient:  0.10292720133178984
iteration : 11572
train acc:  0.765625
train loss:  0.47979992628097534
train gradient:  0.12714733091318045
iteration : 11573
train acc:  0.7421875
train loss:  0.5084196329116821
train gradient:  0.12202582323553757
iteration : 11574
train acc:  0.7265625
train loss:  0.5099150538444519
train gradient:  0.14133582499428077
iteration : 11575
train acc:  0.7265625
train loss:  0.46463543176651
train gradient:  0.11981837447613679
iteration : 11576
train acc:  0.765625
train loss:  0.4489786624908447
train gradient:  0.12081269274224132
iteration : 11577
train acc:  0.75
train loss:  0.4808844029903412
train gradient:  0.12021242949138634
iteration : 11578
train acc:  0.7421875
train loss:  0.5433634519577026
train gradient:  0.15089968624406574
iteration : 11579
train acc:  0.78125
train loss:  0.4547573924064636
train gradient:  0.10605149750223364
iteration : 11580
train acc:  0.7109375
train loss:  0.49888426065444946
train gradient:  0.12688093621293772
iteration : 11581
train acc:  0.734375
train loss:  0.4787806272506714
train gradient:  0.10849560711595291
iteration : 11582
train acc:  0.703125
train loss:  0.5177255868911743
train gradient:  0.1783789526590954
iteration : 11583
train acc:  0.671875
train loss:  0.5438624620437622
train gradient:  0.12384353794898863
iteration : 11584
train acc:  0.703125
train loss:  0.5395376682281494
train gradient:  0.13438421390632285
iteration : 11585
train acc:  0.765625
train loss:  0.465311199426651
train gradient:  0.0896412497664406
iteration : 11586
train acc:  0.765625
train loss:  0.4864545464515686
train gradient:  0.12413679565874224
iteration : 11587
train acc:  0.71875
train loss:  0.5276921391487122
train gradient:  0.1388444888821087
iteration : 11588
train acc:  0.7109375
train loss:  0.5054808855056763
train gradient:  0.1428217819770894
iteration : 11589
train acc:  0.7578125
train loss:  0.5327943563461304
train gradient:  0.1359824484729594
iteration : 11590
train acc:  0.796875
train loss:  0.43528926372528076
train gradient:  0.0967642490136432
iteration : 11591
train acc:  0.7578125
train loss:  0.48975327610969543
train gradient:  0.14467538053987192
iteration : 11592
train acc:  0.78125
train loss:  0.4485587477684021
train gradient:  0.11458679656896727
iteration : 11593
train acc:  0.8125
train loss:  0.41682642698287964
train gradient:  0.08168653821132135
iteration : 11594
train acc:  0.671875
train loss:  0.5174373388290405
train gradient:  0.1415794442825623
iteration : 11595
train acc:  0.7265625
train loss:  0.5406068563461304
train gradient:  0.1650337977856413
iteration : 11596
train acc:  0.734375
train loss:  0.46174922585487366
train gradient:  0.09254308199619281
iteration : 11597
train acc:  0.75
train loss:  0.4696606695652008
train gradient:  0.0969866338129078
iteration : 11598
train acc:  0.734375
train loss:  0.5255458354949951
train gradient:  0.13122357004909926
iteration : 11599
train acc:  0.703125
train loss:  0.49226808547973633
train gradient:  0.13909459386287165
iteration : 11600
train acc:  0.71875
train loss:  0.5080589056015015
train gradient:  0.11947079608562274
iteration : 11601
train acc:  0.71875
train loss:  0.5335923433303833
train gradient:  0.12960110195050906
iteration : 11602
train acc:  0.734375
train loss:  0.48289811611175537
train gradient:  0.10186803171003389
iteration : 11603
train acc:  0.734375
train loss:  0.46944478154182434
train gradient:  0.11176657302078247
iteration : 11604
train acc:  0.765625
train loss:  0.4503832757472992
train gradient:  0.10921583757160598
iteration : 11605
train acc:  0.796875
train loss:  0.46102648973464966
train gradient:  0.11655642964851093
iteration : 11606
train acc:  0.6875
train loss:  0.5329303741455078
train gradient:  0.1670339471331549
iteration : 11607
train acc:  0.8046875
train loss:  0.4351668059825897
train gradient:  0.08651172502355313
iteration : 11608
train acc:  0.6796875
train loss:  0.5366870760917664
train gradient:  0.1544603832580001
iteration : 11609
train acc:  0.7578125
train loss:  0.4880717992782593
train gradient:  0.1129721782261128
iteration : 11610
train acc:  0.78125
train loss:  0.4308047294616699
train gradient:  0.1587657062986207
iteration : 11611
train acc:  0.7265625
train loss:  0.4922938942909241
train gradient:  0.11960105853512222
iteration : 11612
train acc:  0.765625
train loss:  0.44443920254707336
train gradient:  0.10073884031250359
iteration : 11613
train acc:  0.7421875
train loss:  0.45928651094436646
train gradient:  0.11499653282198435
iteration : 11614
train acc:  0.703125
train loss:  0.5815120935440063
train gradient:  0.14055409662106982
iteration : 11615
train acc:  0.765625
train loss:  0.5053858160972595
train gradient:  0.12983801708273718
iteration : 11616
train acc:  0.7578125
train loss:  0.4542679190635681
train gradient:  0.1157272558781833
iteration : 11617
train acc:  0.71875
train loss:  0.4808092415332794
train gradient:  0.1295500579305478
iteration : 11618
train acc:  0.75
train loss:  0.45228737592697144
train gradient:  0.10264956106408721
iteration : 11619
train acc:  0.765625
train loss:  0.49497872591018677
train gradient:  0.11370224978801692
iteration : 11620
train acc:  0.6953125
train loss:  0.5656967759132385
train gradient:  0.19020222071483667
iteration : 11621
train acc:  0.7890625
train loss:  0.5195752382278442
train gradient:  0.15955716833614436
iteration : 11622
train acc:  0.7109375
train loss:  0.568355917930603
train gradient:  0.16065450345076834
iteration : 11623
train acc:  0.796875
train loss:  0.4641730785369873
train gradient:  0.10095027115721411
iteration : 11624
train acc:  0.734375
train loss:  0.4695083498954773
train gradient:  0.1287878040515497
iteration : 11625
train acc:  0.765625
train loss:  0.469829261302948
train gradient:  0.12365124603925048
iteration : 11626
train acc:  0.7890625
train loss:  0.43716880679130554
train gradient:  0.11054616822508626
iteration : 11627
train acc:  0.7421875
train loss:  0.5038689374923706
train gradient:  0.13074551496969977
iteration : 11628
train acc:  0.6875
train loss:  0.5186121463775635
train gradient:  0.13646734353563272
iteration : 11629
train acc:  0.7578125
train loss:  0.44067755341529846
train gradient:  0.1025211796376274
iteration : 11630
train acc:  0.7578125
train loss:  0.5151517987251282
train gradient:  0.11892619874370555
iteration : 11631
train acc:  0.703125
train loss:  0.5268924236297607
train gradient:  0.14092341399686198
iteration : 11632
train acc:  0.7265625
train loss:  0.476249098777771
train gradient:  0.11182670159801963
iteration : 11633
train acc:  0.7265625
train loss:  0.5258582830429077
train gradient:  0.12378794232573327
iteration : 11634
train acc:  0.765625
train loss:  0.4835629463195801
train gradient:  0.13599964491559347
iteration : 11635
train acc:  0.8046875
train loss:  0.4587324261665344
train gradient:  0.10040791739189713
iteration : 11636
train acc:  0.796875
train loss:  0.4969111979007721
train gradient:  0.12424164774705054
iteration : 11637
train acc:  0.7734375
train loss:  0.42554473876953125
train gradient:  0.10065798722468941
iteration : 11638
train acc:  0.7578125
train loss:  0.49865657091140747
train gradient:  0.1413258883746705
iteration : 11639
train acc:  0.78125
train loss:  0.47630536556243896
train gradient:  0.1474850701800634
iteration : 11640
train acc:  0.75
train loss:  0.4810495972633362
train gradient:  0.11865512207462253
iteration : 11641
train acc:  0.734375
train loss:  0.5446982383728027
train gradient:  0.12962182324578211
iteration : 11642
train acc:  0.734375
train loss:  0.47388580441474915
train gradient:  0.10981126748444756
iteration : 11643
train acc:  0.734375
train loss:  0.45612189173698425
train gradient:  0.11191256188326641
iteration : 11644
train acc:  0.6953125
train loss:  0.5265892148017883
train gradient:  0.12357411921179719
iteration : 11645
train acc:  0.71875
train loss:  0.5447031259536743
train gradient:  0.1649739983182422
iteration : 11646
train acc:  0.78125
train loss:  0.4116041958332062
train gradient:  0.10467906397690951
iteration : 11647
train acc:  0.7421875
train loss:  0.4682450294494629
train gradient:  0.11681765435455331
iteration : 11648
train acc:  0.75
train loss:  0.5104786157608032
train gradient:  0.1252212374202975
iteration : 11649
train acc:  0.6953125
train loss:  0.5191231369972229
train gradient:  0.14117767742224419
iteration : 11650
train acc:  0.75
train loss:  0.46964436769485474
train gradient:  0.09953233201027561
iteration : 11651
train acc:  0.75
train loss:  0.4533025324344635
train gradient:  0.09231956599158668
iteration : 11652
train acc:  0.8046875
train loss:  0.43358129262924194
train gradient:  0.10119010273465175
iteration : 11653
train acc:  0.7734375
train loss:  0.4839123487472534
train gradient:  0.12857138631870038
iteration : 11654
train acc:  0.7421875
train loss:  0.48682594299316406
train gradient:  0.21200499788014918
iteration : 11655
train acc:  0.6875
train loss:  0.5557459592819214
train gradient:  0.14514086543970267
iteration : 11656
train acc:  0.6953125
train loss:  0.5316622853279114
train gradient:  0.13452436161060216
iteration : 11657
train acc:  0.703125
train loss:  0.5437741875648499
train gradient:  0.13572374821418937
iteration : 11658
train acc:  0.7734375
train loss:  0.47849029302597046
train gradient:  0.13147747382072267
iteration : 11659
train acc:  0.8046875
train loss:  0.45441728830337524
train gradient:  0.13834514103950113
iteration : 11660
train acc:  0.7265625
train loss:  0.509697437286377
train gradient:  0.09844496932526836
iteration : 11661
train acc:  0.71875
train loss:  0.5254960060119629
train gradient:  0.13860307485137768
iteration : 11662
train acc:  0.7265625
train loss:  0.4732809066772461
train gradient:  0.12461043775947052
iteration : 11663
train acc:  0.734375
train loss:  0.5557125806808472
train gradient:  0.17294471328822306
iteration : 11664
train acc:  0.7421875
train loss:  0.49492722749710083
train gradient:  0.1225502718189729
iteration : 11665
train acc:  0.7578125
train loss:  0.5084046721458435
train gradient:  0.13938526153732336
iteration : 11666
train acc:  0.7265625
train loss:  0.5324631333351135
train gradient:  0.13322975225880984
iteration : 11667
train acc:  0.7578125
train loss:  0.4706113636493683
train gradient:  0.09195875216626576
iteration : 11668
train acc:  0.7734375
train loss:  0.4378643035888672
train gradient:  0.10718207526859878
iteration : 11669
train acc:  0.7265625
train loss:  0.5049864053726196
train gradient:  0.14354727406165463
iteration : 11670
train acc:  0.765625
train loss:  0.47401896119117737
train gradient:  0.10244855029467421
iteration : 11671
train acc:  0.7109375
train loss:  0.513249397277832
train gradient:  0.13946665630737623
iteration : 11672
train acc:  0.8359375
train loss:  0.4372982084751129
train gradient:  0.10142586186339295
iteration : 11673
train acc:  0.7421875
train loss:  0.5009867548942566
train gradient:  0.12336014616535919
iteration : 11674
train acc:  0.71875
train loss:  0.49000662565231323
train gradient:  0.11194727725683211
iteration : 11675
train acc:  0.7578125
train loss:  0.5003405809402466
train gradient:  0.18181715011816513
iteration : 11676
train acc:  0.7890625
train loss:  0.45885026454925537
train gradient:  0.12392063897173045
iteration : 11677
train acc:  0.7578125
train loss:  0.473960816860199
train gradient:  0.13678565750263832
iteration : 11678
train acc:  0.78125
train loss:  0.42608368396759033
train gradient:  0.09553185626607544
iteration : 11679
train acc:  0.65625
train loss:  0.565753698348999
train gradient:  0.14183740371981604
iteration : 11680
train acc:  0.8046875
train loss:  0.42209458351135254
train gradient:  0.08351699023355186
iteration : 11681
train acc:  0.7734375
train loss:  0.4678451418876648
train gradient:  0.11339485455654584
iteration : 11682
train acc:  0.7734375
train loss:  0.44805869460105896
train gradient:  0.15162652737093535
iteration : 11683
train acc:  0.7109375
train loss:  0.539764404296875
train gradient:  0.1606383043410689
iteration : 11684
train acc:  0.734375
train loss:  0.5234777927398682
train gradient:  0.11592966686549072
iteration : 11685
train acc:  0.7578125
train loss:  0.4666670560836792
train gradient:  0.13487296729800236
iteration : 11686
train acc:  0.765625
train loss:  0.4566385746002197
train gradient:  0.11088155854341346
iteration : 11687
train acc:  0.765625
train loss:  0.4884704649448395
train gradient:  0.10170009627733888
iteration : 11688
train acc:  0.734375
train loss:  0.4708026349544525
train gradient:  0.10241600770564901
iteration : 11689
train acc:  0.75
train loss:  0.4560757875442505
train gradient:  0.0877989484990292
iteration : 11690
train acc:  0.7890625
train loss:  0.4400375485420227
train gradient:  0.10769113160623625
iteration : 11691
train acc:  0.671875
train loss:  0.5092958807945251
train gradient:  0.13430191854399112
iteration : 11692
train acc:  0.75
train loss:  0.5069090127944946
train gradient:  0.09988036134321489
iteration : 11693
train acc:  0.796875
train loss:  0.4296662509441376
train gradient:  0.08630704518217353
iteration : 11694
train acc:  0.7421875
train loss:  0.5231420993804932
train gradient:  0.17454252841865706
iteration : 11695
train acc:  0.765625
train loss:  0.43575119972229004
train gradient:  0.09182197346775453
iteration : 11696
train acc:  0.765625
train loss:  0.4268190264701843
train gradient:  0.08523692819761212
iteration : 11697
train acc:  0.6796875
train loss:  0.5355804562568665
train gradient:  0.14359458529618568
iteration : 11698
train acc:  0.7578125
train loss:  0.5047018527984619
train gradient:  0.11715488014056036
iteration : 11699
train acc:  0.75
train loss:  0.49593663215637207
train gradient:  0.10660602386850895
iteration : 11700
train acc:  0.7421875
train loss:  0.5318245887756348
train gradient:  0.11767348777765141
iteration : 11701
train acc:  0.7890625
train loss:  0.4498659074306488
train gradient:  0.09358115558059771
iteration : 11702
train acc:  0.8359375
train loss:  0.4564513564109802
train gradient:  0.09626070539245427
iteration : 11703
train acc:  0.78125
train loss:  0.4730527400970459
train gradient:  0.12709009642343075
iteration : 11704
train acc:  0.6953125
train loss:  0.5028902888298035
train gradient:  0.12658504055798175
iteration : 11705
train acc:  0.765625
train loss:  0.4627957344055176
train gradient:  0.10052495179607543
iteration : 11706
train acc:  0.7109375
train loss:  0.5158112049102783
train gradient:  0.14015487456790193
iteration : 11707
train acc:  0.71875
train loss:  0.5176606178283691
train gradient:  0.1493077686547162
iteration : 11708
train acc:  0.7421875
train loss:  0.5026743412017822
train gradient:  0.11252291801991512
iteration : 11709
train acc:  0.703125
train loss:  0.5404962301254272
train gradient:  0.1461189738807942
iteration : 11710
train acc:  0.7734375
train loss:  0.46176549792289734
train gradient:  0.09951725928668902
iteration : 11711
train acc:  0.734375
train loss:  0.45415443181991577
train gradient:  0.12428080721292191
iteration : 11712
train acc:  0.7578125
train loss:  0.4654729962348938
train gradient:  0.11533654168640384
iteration : 11713
train acc:  0.6640625
train loss:  0.5676931738853455
train gradient:  0.17064387259304487
iteration : 11714
train acc:  0.7109375
train loss:  0.5050013661384583
train gradient:  0.1626610494406373
iteration : 11715
train acc:  0.71875
train loss:  0.5430867075920105
train gradient:  0.16016641326014502
iteration : 11716
train acc:  0.8203125
train loss:  0.4118363857269287
train gradient:  0.07773752588251966
iteration : 11717
train acc:  0.7734375
train loss:  0.5220761299133301
train gradient:  0.21469323413053656
iteration : 11718
train acc:  0.71875
train loss:  0.49773526191711426
train gradient:  0.11732694145561276
iteration : 11719
train acc:  0.8125
train loss:  0.4042237401008606
train gradient:  0.08597832317370424
iteration : 11720
train acc:  0.71875
train loss:  0.47042661905288696
train gradient:  0.12991006500634772
iteration : 11721
train acc:  0.7265625
train loss:  0.5230995416641235
train gradient:  0.19797501429488923
iteration : 11722
train acc:  0.75
train loss:  0.47135043144226074
train gradient:  0.12990027146009947
iteration : 11723
train acc:  0.765625
train loss:  0.4450046718120575
train gradient:  0.12004751539714882
iteration : 11724
train acc:  0.765625
train loss:  0.49556660652160645
train gradient:  0.15541632059667188
iteration : 11725
train acc:  0.7890625
train loss:  0.4792861342430115
train gradient:  0.11532580267080647
iteration : 11726
train acc:  0.71875
train loss:  0.5343328714370728
train gradient:  0.14555340562630334
iteration : 11727
train acc:  0.7734375
train loss:  0.48217782378196716
train gradient:  0.0977557285318168
iteration : 11728
train acc:  0.765625
train loss:  0.4963808059692383
train gradient:  0.12938232214738343
iteration : 11729
train acc:  0.7421875
train loss:  0.527235746383667
train gradient:  0.16703823230059456
iteration : 11730
train acc:  0.6953125
train loss:  0.5798549056053162
train gradient:  0.16418575420609383
iteration : 11731
train acc:  0.8046875
train loss:  0.43575847148895264
train gradient:  0.1059501774129851
iteration : 11732
train acc:  0.75
train loss:  0.4495811462402344
train gradient:  0.13461934961205552
iteration : 11733
train acc:  0.7578125
train loss:  0.5448529124259949
train gradient:  0.16956607400885576
iteration : 11734
train acc:  0.7265625
train loss:  0.47241994738578796
train gradient:  0.1243209134440694
iteration : 11735
train acc:  0.734375
train loss:  0.4807590842247009
train gradient:  0.12098017230989035
iteration : 11736
train acc:  0.7421875
train loss:  0.44688349962234497
train gradient:  0.12705697137419092
iteration : 11737
train acc:  0.7109375
train loss:  0.46527785062789917
train gradient:  0.11377200099351932
iteration : 11738
train acc:  0.71875
train loss:  0.5325849056243896
train gradient:  0.149989702040903
iteration : 11739
train acc:  0.71875
train loss:  0.4980878233909607
train gradient:  0.13282695221716986
iteration : 11740
train acc:  0.71875
train loss:  0.5227922201156616
train gradient:  0.13255964903627981
iteration : 11741
train acc:  0.671875
train loss:  0.5604956746101379
train gradient:  0.20904788334570124
iteration : 11742
train acc:  0.7890625
train loss:  0.4729844629764557
train gradient:  0.14649439188219682
iteration : 11743
train acc:  0.7421875
train loss:  0.4825749397277832
train gradient:  0.12618610850744405
iteration : 11744
train acc:  0.8125
train loss:  0.41203171014785767
train gradient:  0.09318882807038495
iteration : 11745
train acc:  0.71875
train loss:  0.5932540893554688
train gradient:  0.15406871728637828
iteration : 11746
train acc:  0.765625
train loss:  0.48669666051864624
train gradient:  0.12056504278372576
iteration : 11747
train acc:  0.6875
train loss:  0.5307362079620361
train gradient:  0.12867041210938662
iteration : 11748
train acc:  0.78125
train loss:  0.4857834577560425
train gradient:  0.12540108138898173
iteration : 11749
train acc:  0.8203125
train loss:  0.39215099811553955
train gradient:  0.07633264260421391
iteration : 11750
train acc:  0.7109375
train loss:  0.553945779800415
train gradient:  0.15540352213138833
iteration : 11751
train acc:  0.71875
train loss:  0.463667094707489
train gradient:  0.1290505911903963
iteration : 11752
train acc:  0.7890625
train loss:  0.4209873080253601
train gradient:  0.10647114785747121
iteration : 11753
train acc:  0.7109375
train loss:  0.5519328713417053
train gradient:  0.15510589304423
iteration : 11754
train acc:  0.75
train loss:  0.4995538890361786
train gradient:  0.11592235973469063
iteration : 11755
train acc:  0.7421875
train loss:  0.4683675169944763
train gradient:  0.10158642280780453
iteration : 11756
train acc:  0.7421875
train loss:  0.495530366897583
train gradient:  0.11819999898665326
iteration : 11757
train acc:  0.7578125
train loss:  0.4884645938873291
train gradient:  0.11907278743097888
iteration : 11758
train acc:  0.7890625
train loss:  0.4680699110031128
train gradient:  0.11970861067657028
iteration : 11759
train acc:  0.7109375
train loss:  0.4991472363471985
train gradient:  0.1139423652782638
iteration : 11760
train acc:  0.703125
train loss:  0.5370702743530273
train gradient:  0.13343391122357073
iteration : 11761
train acc:  0.75
train loss:  0.4654715955257416
train gradient:  0.11538793207710295
iteration : 11762
train acc:  0.7421875
train loss:  0.5187329649925232
train gradient:  0.12534057069091292
iteration : 11763
train acc:  0.78125
train loss:  0.5346677303314209
train gradient:  0.1521174323323108
iteration : 11764
train acc:  0.71875
train loss:  0.5060869455337524
train gradient:  0.12152060433718628
iteration : 11765
train acc:  0.7421875
train loss:  0.45883649587631226
train gradient:  0.11246905061647772
iteration : 11766
train acc:  0.703125
train loss:  0.4966979920864105
train gradient:  0.11762489200070345
iteration : 11767
train acc:  0.7890625
train loss:  0.4327068328857422
train gradient:  0.11022466446026005
iteration : 11768
train acc:  0.7265625
train loss:  0.5484142303466797
train gradient:  0.16806345411298917
iteration : 11769
train acc:  0.6953125
train loss:  0.5725780725479126
train gradient:  0.20679337386883206
iteration : 11770
train acc:  0.7421875
train loss:  0.46109312772750854
train gradient:  0.09975171475118905
iteration : 11771
train acc:  0.6875
train loss:  0.5152894854545593
train gradient:  0.14475374308038708
iteration : 11772
train acc:  0.7578125
train loss:  0.45796477794647217
train gradient:  0.11171531501730202
iteration : 11773
train acc:  0.7890625
train loss:  0.41718369722366333
train gradient:  0.10653908370196316
iteration : 11774
train acc:  0.8046875
train loss:  0.4633219838142395
train gradient:  0.09628879871743291
iteration : 11775
train acc:  0.8125
train loss:  0.42633217573165894
train gradient:  0.09627110071257373
iteration : 11776
train acc:  0.75
train loss:  0.46299219131469727
train gradient:  0.10958028560019784
iteration : 11777
train acc:  0.7109375
train loss:  0.5559129118919373
train gradient:  0.16867238655313277
iteration : 11778
train acc:  0.734375
train loss:  0.452201783657074
train gradient:  0.10493845630090907
iteration : 11779
train acc:  0.6953125
train loss:  0.514420211315155
train gradient:  0.13049096954462636
iteration : 11780
train acc:  0.796875
train loss:  0.443485826253891
train gradient:  0.10772164973851843
iteration : 11781
train acc:  0.7265625
train loss:  0.4950655996799469
train gradient:  0.1526245152780291
iteration : 11782
train acc:  0.7890625
train loss:  0.48574644327163696
train gradient:  0.10606821171722829
iteration : 11783
train acc:  0.7265625
train loss:  0.4939397871494293
train gradient:  0.11341467053376968
iteration : 11784
train acc:  0.71875
train loss:  0.5348094701766968
train gradient:  0.1387004654857127
iteration : 11785
train acc:  0.7734375
train loss:  0.4690669775009155
train gradient:  0.14420377417002594
iteration : 11786
train acc:  0.796875
train loss:  0.4448990821838379
train gradient:  0.09484870098327801
iteration : 11787
train acc:  0.7578125
train loss:  0.49429982900619507
train gradient:  0.11898050927905786
iteration : 11788
train acc:  0.7734375
train loss:  0.5139858722686768
train gradient:  0.11230545076234671
iteration : 11789
train acc:  0.75
train loss:  0.43390727043151855
train gradient:  0.09729754778283745
iteration : 11790
train acc:  0.703125
train loss:  0.4842078387737274
train gradient:  0.10441136276812368
iteration : 11791
train acc:  0.6875
train loss:  0.539788007736206
train gradient:  0.12137421299937376
iteration : 11792
train acc:  0.78125
train loss:  0.531234085559845
train gradient:  0.1618756108969786
iteration : 11793
train acc:  0.734375
train loss:  0.5143299698829651
train gradient:  0.13738488236869612
iteration : 11794
train acc:  0.7890625
train loss:  0.4509894847869873
train gradient:  0.10755482400594148
iteration : 11795
train acc:  0.6953125
train loss:  0.530449628829956
train gradient:  0.13412344125125664
iteration : 11796
train acc:  0.6796875
train loss:  0.5427305698394775
train gradient:  0.15499016403383048
iteration : 11797
train acc:  0.7109375
train loss:  0.5328962802886963
train gradient:  0.15247245723130215
iteration : 11798
train acc:  0.7421875
train loss:  0.5145754218101501
train gradient:  0.14437090448831616
iteration : 11799
train acc:  0.7578125
train loss:  0.47359731793403625
train gradient:  0.11427167797588136
iteration : 11800
train acc:  0.6953125
train loss:  0.5210426449775696
train gradient:  0.1502718771931455
iteration : 11801
train acc:  0.71875
train loss:  0.5567665696144104
train gradient:  0.1514376324491088
iteration : 11802
train acc:  0.7421875
train loss:  0.4803113639354706
train gradient:  0.1272812643964198
iteration : 11803
train acc:  0.71875
train loss:  0.5702196359634399
train gradient:  0.16757483126532963
iteration : 11804
train acc:  0.8203125
train loss:  0.44693538546562195
train gradient:  0.1061506488121544
iteration : 11805
train acc:  0.7578125
train loss:  0.47255939245224
train gradient:  0.1105293904545602
iteration : 11806
train acc:  0.7890625
train loss:  0.4548400640487671
train gradient:  0.0924159313597364
iteration : 11807
train acc:  0.765625
train loss:  0.47923463582992554
train gradient:  0.11193295393590746
iteration : 11808
train acc:  0.703125
train loss:  0.5227392315864563
train gradient:  0.13342768370341346
iteration : 11809
train acc:  0.7734375
train loss:  0.45704054832458496
train gradient:  0.08981257512001176
iteration : 11810
train acc:  0.75
train loss:  0.4656590223312378
train gradient:  0.09852691401382604
iteration : 11811
train acc:  0.6796875
train loss:  0.5793009996414185
train gradient:  0.1880158253905796
iteration : 11812
train acc:  0.7578125
train loss:  0.47691911458969116
train gradient:  0.11885497336395176
iteration : 11813
train acc:  0.765625
train loss:  0.5102413296699524
train gradient:  0.1535973540040655
iteration : 11814
train acc:  0.78125
train loss:  0.48216867446899414
train gradient:  0.14978696550954151
iteration : 11815
train acc:  0.78125
train loss:  0.4535254240036011
train gradient:  0.11390115873296473
iteration : 11816
train acc:  0.8203125
train loss:  0.4029066860675812
train gradient:  0.09046449173876173
iteration : 11817
train acc:  0.71875
train loss:  0.534674346446991
train gradient:  0.14052989589660195
iteration : 11818
train acc:  0.6796875
train loss:  0.5032534003257751
train gradient:  0.12247930944561183
iteration : 11819
train acc:  0.75
train loss:  0.4689461886882782
train gradient:  0.10912408294172785
iteration : 11820
train acc:  0.7109375
train loss:  0.44082605838775635
train gradient:  0.08941254478094303
iteration : 11821
train acc:  0.7265625
train loss:  0.5683289766311646
train gradient:  0.15647018358877735
iteration : 11822
train acc:  0.703125
train loss:  0.5251973271369934
train gradient:  0.1527771248415065
iteration : 11823
train acc:  0.78125
train loss:  0.4576144814491272
train gradient:  0.08705633934123155
iteration : 11824
train acc:  0.75
train loss:  0.44453850388526917
train gradient:  0.11362778658222931
iteration : 11825
train acc:  0.671875
train loss:  0.576542854309082
train gradient:  0.17158972344443973
iteration : 11826
train acc:  0.7890625
train loss:  0.45496439933776855
train gradient:  0.10052343002028997
iteration : 11827
train acc:  0.7421875
train loss:  0.45866551995277405
train gradient:  0.14582453821596109
iteration : 11828
train acc:  0.7578125
train loss:  0.4707210659980774
train gradient:  0.09511904806079811
iteration : 11829
train acc:  0.7578125
train loss:  0.4662543535232544
train gradient:  0.11632411781585589
iteration : 11830
train acc:  0.7734375
train loss:  0.42718803882598877
train gradient:  0.12330801404337115
iteration : 11831
train acc:  0.765625
train loss:  0.45380735397338867
train gradient:  0.11086031373183705
iteration : 11832
train acc:  0.8203125
train loss:  0.4037451148033142
train gradient:  0.12477508511416698
iteration : 11833
train acc:  0.78125
train loss:  0.4440346360206604
train gradient:  0.08387295276301002
iteration : 11834
train acc:  0.7265625
train loss:  0.5047948956489563
train gradient:  0.10756560049697175
iteration : 11835
train acc:  0.7421875
train loss:  0.5344727039337158
train gradient:  0.16125763925946748
iteration : 11836
train acc:  0.7578125
train loss:  0.46321016550064087
train gradient:  0.09143529770246295
iteration : 11837
train acc:  0.7890625
train loss:  0.4275363087654114
train gradient:  0.09845525957825094
iteration : 11838
train acc:  0.75
train loss:  0.4785401225090027
train gradient:  0.09529130814915228
iteration : 11839
train acc:  0.7734375
train loss:  0.44861018657684326
train gradient:  0.12435346333922022
iteration : 11840
train acc:  0.75
train loss:  0.45471447706222534
train gradient:  0.10842321733429906
iteration : 11841
train acc:  0.7578125
train loss:  0.46477505564689636
train gradient:  0.10193728524345815
iteration : 11842
train acc:  0.7734375
train loss:  0.47311460971832275
train gradient:  0.09110939918463239
iteration : 11843
train acc:  0.6953125
train loss:  0.5339893698692322
train gradient:  0.12940753351254436
iteration : 11844
train acc:  0.7421875
train loss:  0.4985451400279999
train gradient:  0.136237562973776
iteration : 11845
train acc:  0.765625
train loss:  0.4860817790031433
train gradient:  0.11568478199036195
iteration : 11846
train acc:  0.7265625
train loss:  0.5355669856071472
train gradient:  0.15692158649522966
iteration : 11847
train acc:  0.7109375
train loss:  0.5236548781394958
train gradient:  0.16267242213156236
iteration : 11848
train acc:  0.7578125
train loss:  0.5175370573997498
train gradient:  0.12406593785953884
iteration : 11849
train acc:  0.71875
train loss:  0.5387210845947266
train gradient:  0.1394764894367858
iteration : 11850
train acc:  0.71875
train loss:  0.47496291995048523
train gradient:  0.11734207524332636
iteration : 11851
train acc:  0.71875
train loss:  0.5104116797447205
train gradient:  0.1317236053398031
iteration : 11852
train acc:  0.75
train loss:  0.4567852020263672
train gradient:  0.10838536783770254
iteration : 11853
train acc:  0.7734375
train loss:  0.44232064485549927
train gradient:  0.0819105717256009
iteration : 11854
train acc:  0.7578125
train loss:  0.5696158409118652
train gradient:  0.15547933935848482
iteration : 11855
train acc:  0.703125
train loss:  0.5069971084594727
train gradient:  0.16758988205749298
iteration : 11856
train acc:  0.734375
train loss:  0.49968239665031433
train gradient:  0.10897744977424047
iteration : 11857
train acc:  0.71875
train loss:  0.5056703090667725
train gradient:  0.12371825895545556
iteration : 11858
train acc:  0.7578125
train loss:  0.4993619918823242
train gradient:  0.13693953248999557
iteration : 11859
train acc:  0.796875
train loss:  0.4485774338245392
train gradient:  0.1059758490932578
iteration : 11860
train acc:  0.7109375
train loss:  0.519098162651062
train gradient:  0.1346648053764089
iteration : 11861
train acc:  0.7421875
train loss:  0.4792894721031189
train gradient:  0.08855178888945442
iteration : 11862
train acc:  0.7265625
train loss:  0.4970628619194031
train gradient:  0.1260877068210582
iteration : 11863
train acc:  0.734375
train loss:  0.5242888927459717
train gradient:  0.1331820870886512
iteration : 11864
train acc:  0.7109375
train loss:  0.5136936902999878
train gradient:  0.14579118319169998
iteration : 11865
train acc:  0.734375
train loss:  0.45994484424591064
train gradient:  0.11499346784060982
iteration : 11866
train acc:  0.78125
train loss:  0.4340316653251648
train gradient:  0.10630621032228948
iteration : 11867
train acc:  0.71875
train loss:  0.5176130533218384
train gradient:  0.09480763502842823
iteration : 11868
train acc:  0.7421875
train loss:  0.4932052493095398
train gradient:  0.14774709169551686
iteration : 11869
train acc:  0.78125
train loss:  0.4557466208934784
train gradient:  0.11230150739155582
iteration : 11870
train acc:  0.7578125
train loss:  0.5288490056991577
train gradient:  0.17363894838884394
iteration : 11871
train acc:  0.7421875
train loss:  0.5198909044265747
train gradient:  0.11512555002584392
iteration : 11872
train acc:  0.78125
train loss:  0.4709320068359375
train gradient:  0.10906535062059741
iteration : 11873
train acc:  0.71875
train loss:  0.46840134263038635
train gradient:  0.09002917333088942
iteration : 11874
train acc:  0.78125
train loss:  0.43102869391441345
train gradient:  0.1097154703487176
iteration : 11875
train acc:  0.75
train loss:  0.47699040174484253
train gradient:  0.14077744673552706
iteration : 11876
train acc:  0.703125
train loss:  0.5275903344154358
train gradient:  0.13186528792910346
iteration : 11877
train acc:  0.796875
train loss:  0.4364105761051178
train gradient:  0.09222509403375523
iteration : 11878
train acc:  0.703125
train loss:  0.5774314403533936
train gradient:  0.13870140661822455
iteration : 11879
train acc:  0.7890625
train loss:  0.4443795382976532
train gradient:  0.11477817174277613
iteration : 11880
train acc:  0.7734375
train loss:  0.4469030499458313
train gradient:  0.1097235278919078
iteration : 11881
train acc:  0.703125
train loss:  0.5196877717971802
train gradient:  0.12243068014996734
iteration : 11882
train acc:  0.75
train loss:  0.5191271305084229
train gradient:  0.14581506893141066
iteration : 11883
train acc:  0.7421875
train loss:  0.4868007004261017
train gradient:  0.1325131901779883
iteration : 11884
train acc:  0.75
train loss:  0.49051859974861145
train gradient:  0.13063406982010972
iteration : 11885
train acc:  0.7109375
train loss:  0.5518791675567627
train gradient:  0.14105013238220565
iteration : 11886
train acc:  0.7109375
train loss:  0.49620938301086426
train gradient:  0.11225000427887309
iteration : 11887
train acc:  0.703125
train loss:  0.567566990852356
train gradient:  0.13871535798975396
iteration : 11888
train acc:  0.7734375
train loss:  0.4983612298965454
train gradient:  0.14428462948740833
iteration : 11889
train acc:  0.7734375
train loss:  0.44111377000808716
train gradient:  0.09823309201303877
iteration : 11890
train acc:  0.765625
train loss:  0.46443575620651245
train gradient:  0.11918277096028014
iteration : 11891
train acc:  0.8125
train loss:  0.452791303396225
train gradient:  0.0941136868666133
iteration : 11892
train acc:  0.8125
train loss:  0.4789203703403473
train gradient:  0.11514533508447995
iteration : 11893
train acc:  0.734375
train loss:  0.5058230757713318
train gradient:  0.1515037781716631
iteration : 11894
train acc:  0.765625
train loss:  0.46134209632873535
train gradient:  0.10972222546726375
iteration : 11895
train acc:  0.734375
train loss:  0.4984644651412964
train gradient:  0.1536977269071555
iteration : 11896
train acc:  0.78125
train loss:  0.4282877445220947
train gradient:  0.08285176060620168
iteration : 11897
train acc:  0.71875
train loss:  0.5203303098678589
train gradient:  0.13426986114683637
iteration : 11898
train acc:  0.765625
train loss:  0.4939289689064026
train gradient:  0.11059992406115716
iteration : 11899
train acc:  0.8125
train loss:  0.4484003782272339
train gradient:  0.10844101002138883
iteration : 11900
train acc:  0.7578125
train loss:  0.4954712986946106
train gradient:  0.13840981300409566
iteration : 11901
train acc:  0.7421875
train loss:  0.504369854927063
train gradient:  0.11749246503983674
iteration : 11902
train acc:  0.7578125
train loss:  0.5140284299850464
train gradient:  0.16449243127641694
iteration : 11903
train acc:  0.78125
train loss:  0.4648900628089905
train gradient:  0.11000233449076142
iteration : 11904
train acc:  0.6953125
train loss:  0.5408751964569092
train gradient:  0.14897847203675596
iteration : 11905
train acc:  0.7265625
train loss:  0.48090022802352905
train gradient:  0.14846137132920284
iteration : 11906
train acc:  0.7421875
train loss:  0.47361165285110474
train gradient:  0.15234589345790406
iteration : 11907
train acc:  0.7265625
train loss:  0.4856707453727722
train gradient:  0.11484232079433819
iteration : 11908
train acc:  0.765625
train loss:  0.5331051349639893
train gradient:  0.1451003669080681
iteration : 11909
train acc:  0.6953125
train loss:  0.5580298900604248
train gradient:  0.1363906471676401
iteration : 11910
train acc:  0.7265625
train loss:  0.5317578911781311
train gradient:  0.1581512362911845
iteration : 11911
train acc:  0.7578125
train loss:  0.49207523465156555
train gradient:  0.1277731360938244
iteration : 11912
train acc:  0.75
train loss:  0.5523699522018433
train gradient:  0.1353725994622702
iteration : 11913
train acc:  0.8203125
train loss:  0.407756507396698
train gradient:  0.13053611042076596
iteration : 11914
train acc:  0.7578125
train loss:  0.4918724596500397
train gradient:  0.11377045128603501
iteration : 11915
train acc:  0.796875
train loss:  0.39564090967178345
train gradient:  0.08914049947075885
iteration : 11916
train acc:  0.765625
train loss:  0.4685063064098358
train gradient:  0.10168811277266805
iteration : 11917
train acc:  0.7890625
train loss:  0.4513113498687744
train gradient:  0.11926763544920962
iteration : 11918
train acc:  0.75
train loss:  0.4725620150566101
train gradient:  0.1196736717067526
iteration : 11919
train acc:  0.71875
train loss:  0.5213172435760498
train gradient:  0.13328297991906124
iteration : 11920
train acc:  0.8046875
train loss:  0.41769400238990784
train gradient:  0.09627865472549729
iteration : 11921
train acc:  0.796875
train loss:  0.410168319940567
train gradient:  0.07014421414422983
iteration : 11922
train acc:  0.7734375
train loss:  0.39611220359802246
train gradient:  0.0778253524925586
iteration : 11923
train acc:  0.6875
train loss:  0.5238972902297974
train gradient:  0.17692842388967822
iteration : 11924
train acc:  0.8046875
train loss:  0.4461503326892853
train gradient:  0.11534438080951333
iteration : 11925
train acc:  0.796875
train loss:  0.47380179166793823
train gradient:  0.11189273614575672
iteration : 11926
train acc:  0.6953125
train loss:  0.5676006078720093
train gradient:  0.1745699415573156
iteration : 11927
train acc:  0.6875
train loss:  0.5274404883384705
train gradient:  0.1353734580381274
iteration : 11928
train acc:  0.65625
train loss:  0.5951530933380127
train gradient:  0.12781741959856197
iteration : 11929
train acc:  0.7265625
train loss:  0.5145530700683594
train gradient:  0.12675027870022856
iteration : 11930
train acc:  0.7265625
train loss:  0.511062741279602
train gradient:  0.14703328478415534
iteration : 11931
train acc:  0.734375
train loss:  0.4594918489456177
train gradient:  0.12979997406907046
iteration : 11932
train acc:  0.734375
train loss:  0.504791259765625
train gradient:  0.13991422673485687
iteration : 11933
train acc:  0.765625
train loss:  0.47538596391677856
train gradient:  0.12559181437047318
iteration : 11934
train acc:  0.7109375
train loss:  0.4995861053466797
train gradient:  0.12436217365747704
iteration : 11935
train acc:  0.734375
train loss:  0.5227439403533936
train gradient:  0.1159548254353487
iteration : 11936
train acc:  0.7578125
train loss:  0.46592503786087036
train gradient:  0.1221968068235439
iteration : 11937
train acc:  0.75
train loss:  0.4090158939361572
train gradient:  0.0793453485841032
iteration : 11938
train acc:  0.7578125
train loss:  0.48913609981536865
train gradient:  0.1522972562355696
iteration : 11939
train acc:  0.828125
train loss:  0.44378578662872314
train gradient:  0.0971710393617653
iteration : 11940
train acc:  0.765625
train loss:  0.4916139245033264
train gradient:  0.10992409224629594
iteration : 11941
train acc:  0.765625
train loss:  0.4419788420200348
train gradient:  0.1100522519643702
iteration : 11942
train acc:  0.6796875
train loss:  0.5722318887710571
train gradient:  0.15488193977608322
iteration : 11943
train acc:  0.78125
train loss:  0.4693765342235565
train gradient:  0.11819954852144027
iteration : 11944
train acc:  0.78125
train loss:  0.4981832504272461
train gradient:  0.15810898180650917
iteration : 11945
train acc:  0.7421875
train loss:  0.5192584991455078
train gradient:  0.17102442674134555
iteration : 11946
train acc:  0.7265625
train loss:  0.5229125022888184
train gradient:  0.13438353921375823
iteration : 11947
train acc:  0.7421875
train loss:  0.4473050534725189
train gradient:  0.1009512914411001
iteration : 11948
train acc:  0.6953125
train loss:  0.5803605318069458
train gradient:  0.1514432830411292
iteration : 11949
train acc:  0.78125
train loss:  0.430331289768219
train gradient:  0.09286478845289772
iteration : 11950
train acc:  0.6875
train loss:  0.5482558012008667
train gradient:  0.12591933945486397
iteration : 11951
train acc:  0.7421875
train loss:  0.4821659028530121
train gradient:  0.13502837879704394
iteration : 11952
train acc:  0.6875
train loss:  0.4792627692222595
train gradient:  0.10611563936799649
iteration : 11953
train acc:  0.6328125
train loss:  0.5697789788246155
train gradient:  0.14653493249718252
iteration : 11954
train acc:  0.7421875
train loss:  0.4925529956817627
train gradient:  0.10486217612072725
iteration : 11955
train acc:  0.734375
train loss:  0.5011221170425415
train gradient:  0.10576492589905206
iteration : 11956
train acc:  0.7890625
train loss:  0.44797924160957336
train gradient:  0.09536374370507116
iteration : 11957
train acc:  0.7421875
train loss:  0.4551871716976166
train gradient:  0.10482146050580939
iteration : 11958
train acc:  0.71875
train loss:  0.4721718728542328
train gradient:  0.12900687746714032
iteration : 11959
train acc:  0.8125
train loss:  0.48537111282348633
train gradient:  0.11034362017056817
iteration : 11960
train acc:  0.6953125
train loss:  0.5069579482078552
train gradient:  0.12640915228874006
iteration : 11961
train acc:  0.734375
train loss:  0.526351809501648
train gradient:  0.11618061106358717
iteration : 11962
train acc:  0.7578125
train loss:  0.4504587948322296
train gradient:  0.10926231577964604
iteration : 11963
train acc:  0.734375
train loss:  0.516426682472229
train gradient:  0.11309717434332683
iteration : 11964
train acc:  0.8125
train loss:  0.43289822340011597
train gradient:  0.10419595910368172
iteration : 11965
train acc:  0.6875
train loss:  0.5310344696044922
train gradient:  0.14943925880722536
iteration : 11966
train acc:  0.7421875
train loss:  0.5014775991439819
train gradient:  0.15811245241775063
iteration : 11967
train acc:  0.734375
train loss:  0.49641573429107666
train gradient:  0.13010305572723332
iteration : 11968
train acc:  0.7421875
train loss:  0.5100079774856567
train gradient:  0.11526730911389392
iteration : 11969
train acc:  0.8359375
train loss:  0.3964664340019226
train gradient:  0.10206104286391829
iteration : 11970
train acc:  0.78125
train loss:  0.4668944478034973
train gradient:  0.1137740681559674
iteration : 11971
train acc:  0.7578125
train loss:  0.4803863763809204
train gradient:  0.11238379933028911
iteration : 11972
train acc:  0.8125
train loss:  0.42846131324768066
train gradient:  0.0761843225308738
iteration : 11973
train acc:  0.7578125
train loss:  0.4214014410972595
train gradient:  0.08649070347086213
iteration : 11974
train acc:  0.7734375
train loss:  0.46510717272758484
train gradient:  0.10907738504237195
iteration : 11975
train acc:  0.7109375
train loss:  0.5268629789352417
train gradient:  0.12552928520252868
iteration : 11976
train acc:  0.7421875
train loss:  0.4838096499443054
train gradient:  0.0951437223628681
iteration : 11977
train acc:  0.78125
train loss:  0.4569627344608307
train gradient:  0.10210017465864078
iteration : 11978
train acc:  0.7578125
train loss:  0.4410971999168396
train gradient:  0.12014537622306001
iteration : 11979
train acc:  0.75
train loss:  0.4942542314529419
train gradient:  0.09675100462685705
iteration : 11980
train acc:  0.734375
train loss:  0.49025624990463257
train gradient:  0.13078051519433093
iteration : 11981
train acc:  0.7890625
train loss:  0.4604197144508362
train gradient:  0.12926016472670898
iteration : 11982
train acc:  0.78125
train loss:  0.44348499178886414
train gradient:  0.0971357931703864
iteration : 11983
train acc:  0.75
train loss:  0.47557228803634644
train gradient:  0.1357491471175689
iteration : 11984
train acc:  0.7421875
train loss:  0.5087097883224487
train gradient:  0.16098761789182386
iteration : 11985
train acc:  0.765625
train loss:  0.49814915657043457
train gradient:  0.12026651389916887
iteration : 11986
train acc:  0.7890625
train loss:  0.4255281984806061
train gradient:  0.1150126422026756
iteration : 11987
train acc:  0.734375
train loss:  0.5540207624435425
train gradient:  0.14560399191234402
iteration : 11988
train acc:  0.703125
train loss:  0.5419485569000244
train gradient:  0.1255414690093943
iteration : 11989
train acc:  0.671875
train loss:  0.5675171613693237
train gradient:  0.15274608249392246
iteration : 11990
train acc:  0.7578125
train loss:  0.45516589283943176
train gradient:  0.11803430140197319
iteration : 11991
train acc:  0.75
train loss:  0.459085613489151
train gradient:  0.10566302084619637
iteration : 11992
train acc:  0.765625
train loss:  0.4516342878341675
train gradient:  0.0988951401261201
iteration : 11993
train acc:  0.7578125
train loss:  0.4827885329723358
train gradient:  0.12824321995732202
iteration : 11994
train acc:  0.7109375
train loss:  0.4801122546195984
train gradient:  0.13425301560722724
iteration : 11995
train acc:  0.7421875
train loss:  0.48028564453125
train gradient:  0.11403568700456636
iteration : 11996
train acc:  0.7890625
train loss:  0.46328845620155334
train gradient:  0.12376664028982827
iteration : 11997
train acc:  0.7734375
train loss:  0.4875599145889282
train gradient:  0.1372869472276559
iteration : 11998
train acc:  0.734375
train loss:  0.4755540192127228
train gradient:  0.1002789439617485
iteration : 11999
train acc:  0.8125
train loss:  0.4490917921066284
train gradient:  0.11046835644373966
iteration : 12000
train acc:  0.7265625
train loss:  0.5139724016189575
train gradient:  0.11831306671752163
iteration : 12001
train acc:  0.7734375
train loss:  0.5374376773834229
train gradient:  0.14313452038462612
iteration : 12002
train acc:  0.703125
train loss:  0.5453622341156006
train gradient:  0.1207292003937147
iteration : 12003
train acc:  0.7421875
train loss:  0.5474985837936401
train gradient:  0.16673554820753872
iteration : 12004
train acc:  0.7890625
train loss:  0.5145286321640015
train gradient:  0.1447944787178071
iteration : 12005
train acc:  0.78125
train loss:  0.4570368230342865
train gradient:  0.0987052915925898
iteration : 12006
train acc:  0.7421875
train loss:  0.5087525844573975
train gradient:  0.14273294311507018
iteration : 12007
train acc:  0.75
train loss:  0.4707839787006378
train gradient:  0.08338922629059646
iteration : 12008
train acc:  0.7265625
train loss:  0.4568941593170166
train gradient:  0.09503025219851453
iteration : 12009
train acc:  0.734375
train loss:  0.46002277731895447
train gradient:  0.10229756122141206
iteration : 12010
train acc:  0.75
train loss:  0.5077976584434509
train gradient:  0.11520856958349998
iteration : 12011
train acc:  0.7578125
train loss:  0.4653049111366272
train gradient:  0.10713947652492256
iteration : 12012
train acc:  0.7109375
train loss:  0.5115322470664978
train gradient:  0.13103228564312008
iteration : 12013
train acc:  0.734375
train loss:  0.45711153745651245
train gradient:  0.09544464801240715
iteration : 12014
train acc:  0.765625
train loss:  0.4423726201057434
train gradient:  0.11446570834718539
iteration : 12015
train acc:  0.6875
train loss:  0.5297656059265137
train gradient:  0.1070053685350425
iteration : 12016
train acc:  0.71875
train loss:  0.45488065481185913
train gradient:  0.12851703279734972
iteration : 12017
train acc:  0.7578125
train loss:  0.467684805393219
train gradient:  0.10946142533026043
iteration : 12018
train acc:  0.7109375
train loss:  0.5186945796012878
train gradient:  0.13792283616021012
iteration : 12019
train acc:  0.8125
train loss:  0.45121192932128906
train gradient:  0.10742420718376428
iteration : 12020
train acc:  0.734375
train loss:  0.5128661394119263
train gradient:  0.15177666213588215
iteration : 12021
train acc:  0.7890625
train loss:  0.4277186989784241
train gradient:  0.09464665722826628
iteration : 12022
train acc:  0.7109375
train loss:  0.49035996198654175
train gradient:  0.1427066667855041
iteration : 12023
train acc:  0.78125
train loss:  0.5042568445205688
train gradient:  0.11679823346345478
iteration : 12024
train acc:  0.71875
train loss:  0.5221792459487915
train gradient:  0.16188158398444155
iteration : 12025
train acc:  0.78125
train loss:  0.415946364402771
train gradient:  0.08627825426318748
iteration : 12026
train acc:  0.7265625
train loss:  0.48066726326942444
train gradient:  0.15581064572263909
iteration : 12027
train acc:  0.765625
train loss:  0.49124205112457275
train gradient:  0.12043221903239724
iteration : 12028
train acc:  0.71875
train loss:  0.48764634132385254
train gradient:  0.12740465307153237
iteration : 12029
train acc:  0.7421875
train loss:  0.4916456341743469
train gradient:  0.11998408318843276
iteration : 12030
train acc:  0.6953125
train loss:  0.4772821366786957
train gradient:  0.13714470370351498
iteration : 12031
train acc:  0.78125
train loss:  0.4439049959182739
train gradient:  0.11369613357726653
iteration : 12032
train acc:  0.75
train loss:  0.5432649850845337
train gradient:  0.1805452396057633
iteration : 12033
train acc:  0.7265625
train loss:  0.5040850639343262
train gradient:  0.09794304380777837
iteration : 12034
train acc:  0.7265625
train loss:  0.48536574840545654
train gradient:  0.10654044327696027
iteration : 12035
train acc:  0.7421875
train loss:  0.4718211889266968
train gradient:  0.15008535813162585
iteration : 12036
train acc:  0.734375
train loss:  0.450035959482193
train gradient:  0.09389424596783982
iteration : 12037
train acc:  0.8046875
train loss:  0.4327554702758789
train gradient:  0.08992479627495994
iteration : 12038
train acc:  0.7421875
train loss:  0.5068745613098145
train gradient:  0.1401021009323374
iteration : 12039
train acc:  0.765625
train loss:  0.4686846137046814
train gradient:  0.13881700023651322
iteration : 12040
train acc:  0.828125
train loss:  0.42333120107650757
train gradient:  0.08338934404515806
iteration : 12041
train acc:  0.7421875
train loss:  0.4517773985862732
train gradient:  0.12748056010191583
iteration : 12042
train acc:  0.6953125
train loss:  0.5372759103775024
train gradient:  0.1511864878558477
iteration : 12043
train acc:  0.7890625
train loss:  0.45364904403686523
train gradient:  0.10398817788649088
iteration : 12044
train acc:  0.7890625
train loss:  0.46221214532852173
train gradient:  0.11607784376986172
iteration : 12045
train acc:  0.8203125
train loss:  0.4436992406845093
train gradient:  0.11001375462664446
iteration : 12046
train acc:  0.734375
train loss:  0.4931323230266571
train gradient:  0.10605396211304882
iteration : 12047
train acc:  0.7265625
train loss:  0.4630392789840698
train gradient:  0.10064710993052355
iteration : 12048
train acc:  0.7265625
train loss:  0.4999561905860901
train gradient:  0.11943850724843608
iteration : 12049
train acc:  0.71875
train loss:  0.5108119249343872
train gradient:  0.15768395633788268
iteration : 12050
train acc:  0.8203125
train loss:  0.4005544185638428
train gradient:  0.11031573056334669
iteration : 12051
train acc:  0.78125
train loss:  0.4230363965034485
train gradient:  0.12620892833089198
iteration : 12052
train acc:  0.78125
train loss:  0.46880123019218445
train gradient:  0.10172690686809244
iteration : 12053
train acc:  0.78125
train loss:  0.47377219796180725
train gradient:  0.09931777376809521
iteration : 12054
train acc:  0.6875
train loss:  0.5289692282676697
train gradient:  0.13219671638579022
iteration : 12055
train acc:  0.6640625
train loss:  0.5634210109710693
train gradient:  0.18852461350213678
iteration : 12056
train acc:  0.7734375
train loss:  0.4576795697212219
train gradient:  0.10237919542234718
iteration : 12057
train acc:  0.671875
train loss:  0.5344969034194946
train gradient:  0.14747111141409092
iteration : 12058
train acc:  0.75
train loss:  0.48880186676979065
train gradient:  0.1359948303316823
iteration : 12059
train acc:  0.7265625
train loss:  0.4575815200805664
train gradient:  0.0992376070352983
iteration : 12060
train acc:  0.75
train loss:  0.4989287853240967
train gradient:  0.12286737751484962
iteration : 12061
train acc:  0.828125
train loss:  0.42592504620552063
train gradient:  0.09470909796531986
iteration : 12062
train acc:  0.7265625
train loss:  0.491949200630188
train gradient:  0.12859352370419208
iteration : 12063
train acc:  0.703125
train loss:  0.49658793210983276
train gradient:  0.13085647785317425
iteration : 12064
train acc:  0.734375
train loss:  0.5000208616256714
train gradient:  0.10784800136829861
iteration : 12065
train acc:  0.7734375
train loss:  0.47848984599113464
train gradient:  0.1373753403508972
iteration : 12066
train acc:  0.7578125
train loss:  0.4748951494693756
train gradient:  0.1613924068638262
iteration : 12067
train acc:  0.7890625
train loss:  0.4364950358867645
train gradient:  0.10324101583083085
iteration : 12068
train acc:  0.7578125
train loss:  0.46978163719177246
train gradient:  0.09477856714548889
iteration : 12069
train acc:  0.7578125
train loss:  0.43460583686828613
train gradient:  0.08944333708633909
iteration : 12070
train acc:  0.6953125
train loss:  0.48107749223709106
train gradient:  0.11946866991896801
iteration : 12071
train acc:  0.78125
train loss:  0.4823007583618164
train gradient:  0.14468817305613302
iteration : 12072
train acc:  0.7421875
train loss:  0.5048784017562866
train gradient:  0.10334935588270133
iteration : 12073
train acc:  0.75
train loss:  0.43913477659225464
train gradient:  0.1014110703448644
iteration : 12074
train acc:  0.78125
train loss:  0.47547173500061035
train gradient:  0.1333940484542384
iteration : 12075
train acc:  0.703125
train loss:  0.5517764091491699
train gradient:  0.14618923826133776
iteration : 12076
train acc:  0.7890625
train loss:  0.5013387799263
train gradient:  0.11083151980726522
iteration : 12077
train acc:  0.7890625
train loss:  0.4378841519355774
train gradient:  0.11350738667859156
iteration : 12078
train acc:  0.7578125
train loss:  0.4685273766517639
train gradient:  0.13978469461494358
iteration : 12079
train acc:  0.7109375
train loss:  0.5645952224731445
train gradient:  0.15914010394237538
iteration : 12080
train acc:  0.7421875
train loss:  0.5059041380882263
train gradient:  0.13147074995093677
iteration : 12081
train acc:  0.7578125
train loss:  0.488810658454895
train gradient:  0.14175359083932887
iteration : 12082
train acc:  0.7265625
train loss:  0.5169931054115295
train gradient:  0.1447895335918725
iteration : 12083
train acc:  0.765625
train loss:  0.4488759934902191
train gradient:  0.0844358866050544
iteration : 12084
train acc:  0.703125
train loss:  0.5354204177856445
train gradient:  0.1646851421797908
iteration : 12085
train acc:  0.7578125
train loss:  0.48135387897491455
train gradient:  0.11633225877227864
iteration : 12086
train acc:  0.796875
train loss:  0.4798206090927124
train gradient:  0.11403656114197729
iteration : 12087
train acc:  0.8125
train loss:  0.4629153609275818
train gradient:  0.15373154382755128
iteration : 12088
train acc:  0.8203125
train loss:  0.4082196354866028
train gradient:  0.0996460979743372
iteration : 12089
train acc:  0.796875
train loss:  0.48076921701431274
train gradient:  0.10875156092280583
iteration : 12090
train acc:  0.7734375
train loss:  0.5220789313316345
train gradient:  0.19233019071046603
iteration : 12091
train acc:  0.7421875
train loss:  0.4895361065864563
train gradient:  0.12324816407640608
iteration : 12092
train acc:  0.78125
train loss:  0.4776679277420044
train gradient:  0.12085204793440336
iteration : 12093
train acc:  0.796875
train loss:  0.44591885805130005
train gradient:  0.10571487927442981
iteration : 12094
train acc:  0.8046875
train loss:  0.4482344686985016
train gradient:  0.11219125295171294
iteration : 12095
train acc:  0.7421875
train loss:  0.5260374546051025
train gradient:  0.1299605195821617
iteration : 12096
train acc:  0.7734375
train loss:  0.46767985820770264
train gradient:  0.14242128852320074
iteration : 12097
train acc:  0.7890625
train loss:  0.47387951612472534
train gradient:  0.11277544823967951
iteration : 12098
train acc:  0.765625
train loss:  0.5080264806747437
train gradient:  0.1514593120484288
iteration : 12099
train acc:  0.734375
train loss:  0.5140354037284851
train gradient:  0.12763162127290484
iteration : 12100
train acc:  0.796875
train loss:  0.43814408779144287
train gradient:  0.10591560655913257
iteration : 12101
train acc:  0.71875
train loss:  0.48428061604499817
train gradient:  0.12375485608744066
iteration : 12102
train acc:  0.8046875
train loss:  0.4498085379600525
train gradient:  0.10039373461161269
iteration : 12103
train acc:  0.765625
train loss:  0.4311443269252777
train gradient:  0.1352428691260853
iteration : 12104
train acc:  0.671875
train loss:  0.549439549446106
train gradient:  0.12951989224701882
iteration : 12105
train acc:  0.7109375
train loss:  0.6013574004173279
train gradient:  0.21071128437048314
iteration : 12106
train acc:  0.7265625
train loss:  0.49969959259033203
train gradient:  0.12699516381371503
iteration : 12107
train acc:  0.75
train loss:  0.47030720114707947
train gradient:  0.12547599057374154
iteration : 12108
train acc:  0.7890625
train loss:  0.43848252296447754
train gradient:  0.08310007057172702
iteration : 12109
train acc:  0.7734375
train loss:  0.5038037300109863
train gradient:  0.13026269454032038
iteration : 12110
train acc:  0.7734375
train loss:  0.5019716024398804
train gradient:  0.12562653685305014
iteration : 12111
train acc:  0.8046875
train loss:  0.4257877767086029
train gradient:  0.0754610657573724
iteration : 12112
train acc:  0.6640625
train loss:  0.5602117776870728
train gradient:  0.15550858900367898
iteration : 12113
train acc:  0.765625
train loss:  0.4704795777797699
train gradient:  0.11982279803383523
iteration : 12114
train acc:  0.765625
train loss:  0.4446151852607727
train gradient:  0.09414589398107648
iteration : 12115
train acc:  0.71875
train loss:  0.4710758328437805
train gradient:  0.10347671290712435
iteration : 12116
train acc:  0.703125
train loss:  0.580761194229126
train gradient:  0.181157871686855
iteration : 12117
train acc:  0.71875
train loss:  0.5488088726997375
train gradient:  0.12705725221432718
iteration : 12118
train acc:  0.7265625
train loss:  0.49344033002853394
train gradient:  0.13274439251913586
iteration : 12119
train acc:  0.7109375
train loss:  0.5710422396659851
train gradient:  0.16660628440777914
iteration : 12120
train acc:  0.7734375
train loss:  0.4600415527820587
train gradient:  0.11781812461256051
iteration : 12121
train acc:  0.6875
train loss:  0.5616533756256104
train gradient:  0.1616688630246887
iteration : 12122
train acc:  0.796875
train loss:  0.43109750747680664
train gradient:  0.0847702910935044
iteration : 12123
train acc:  0.703125
train loss:  0.5424981117248535
train gradient:  0.1486933352673953
iteration : 12124
train acc:  0.6796875
train loss:  0.643119215965271
train gradient:  0.17717212592618392
iteration : 12125
train acc:  0.7421875
train loss:  0.47092732787132263
train gradient:  0.1240304184927149
iteration : 12126
train acc:  0.734375
train loss:  0.4855636954307556
train gradient:  0.12004950318111868
iteration : 12127
train acc:  0.6953125
train loss:  0.5042933821678162
train gradient:  0.14129244988823825
iteration : 12128
train acc:  0.7421875
train loss:  0.5065361261367798
train gradient:  0.11811129725112063
iteration : 12129
train acc:  0.7265625
train loss:  0.5418305397033691
train gradient:  0.1695230330074714
iteration : 12130
train acc:  0.7421875
train loss:  0.5132259726524353
train gradient:  0.126609730756085
iteration : 12131
train acc:  0.71875
train loss:  0.5157989859580994
train gradient:  0.1472408025867174
iteration : 12132
train acc:  0.7578125
train loss:  0.49471789598464966
train gradient:  0.12656991111861518
iteration : 12133
train acc:  0.7109375
train loss:  0.49031829833984375
train gradient:  0.1059470784508478
iteration : 12134
train acc:  0.6875
train loss:  0.5187432765960693
train gradient:  0.12203119483982224
iteration : 12135
train acc:  0.734375
train loss:  0.5725966691970825
train gradient:  0.14556485594096302
iteration : 12136
train acc:  0.75
train loss:  0.4964805245399475
train gradient:  0.10441135423445062
iteration : 12137
train acc:  0.734375
train loss:  0.45609521865844727
train gradient:  0.09732942980338927
iteration : 12138
train acc:  0.7421875
train loss:  0.47034621238708496
train gradient:  0.0879810601822615
iteration : 12139
train acc:  0.71875
train loss:  0.5340018272399902
train gradient:  0.1369917828047634
iteration : 12140
train acc:  0.7109375
train loss:  0.5450937747955322
train gradient:  0.14258594701422908
iteration : 12141
train acc:  0.734375
train loss:  0.521446943283081
train gradient:  0.11776867019654592
iteration : 12142
train acc:  0.7421875
train loss:  0.49000248312950134
train gradient:  0.14405178538653823
iteration : 12143
train acc:  0.7265625
train loss:  0.47729548811912537
train gradient:  0.13097944033780487
iteration : 12144
train acc:  0.734375
train loss:  0.5346781611442566
train gradient:  0.13519671506254466
iteration : 12145
train acc:  0.75
train loss:  0.4737890362739563
train gradient:  0.11757732992784176
iteration : 12146
train acc:  0.796875
train loss:  0.45064297318458557
train gradient:  0.0849734152381843
iteration : 12147
train acc:  0.8359375
train loss:  0.38661718368530273
train gradient:  0.07513523282632553
iteration : 12148
train acc:  0.8125
train loss:  0.44556427001953125
train gradient:  0.13138402676499794
iteration : 12149
train acc:  0.7265625
train loss:  0.5088322162628174
train gradient:  0.12513541068992617
iteration : 12150
train acc:  0.75
train loss:  0.459572434425354
train gradient:  0.08748648145416517
iteration : 12151
train acc:  0.7578125
train loss:  0.49964338541030884
train gradient:  0.11604707121721905
iteration : 12152
train acc:  0.7109375
train loss:  0.5426959991455078
train gradient:  0.14236214766175637
iteration : 12153
train acc:  0.796875
train loss:  0.47794002294540405
train gradient:  0.11425740359844319
iteration : 12154
train acc:  0.734375
train loss:  0.5343114137649536
train gradient:  0.15509492311612078
iteration : 12155
train acc:  0.8203125
train loss:  0.44678065180778503
train gradient:  0.1020978914404158
iteration : 12156
train acc:  0.6953125
train loss:  0.5829513072967529
train gradient:  0.1696418146775558
iteration : 12157
train acc:  0.7890625
train loss:  0.44078707695007324
train gradient:  0.11187313916187014
iteration : 12158
train acc:  0.7265625
train loss:  0.4995882511138916
train gradient:  0.15828139933294913
iteration : 12159
train acc:  0.7890625
train loss:  0.41455405950546265
train gradient:  0.07844558378499894
iteration : 12160
train acc:  0.796875
train loss:  0.4251106381416321
train gradient:  0.08532316461861623
iteration : 12161
train acc:  0.6640625
train loss:  0.5842995047569275
train gradient:  0.14255175844349305
iteration : 12162
train acc:  0.765625
train loss:  0.4608624279499054
train gradient:  0.0721549010899329
iteration : 12163
train acc:  0.78125
train loss:  0.47057992219924927
train gradient:  0.10958553774774775
iteration : 12164
train acc:  0.75
train loss:  0.5106195211410522
train gradient:  0.13071011918054898
iteration : 12165
train acc:  0.671875
train loss:  0.5331541299819946
train gradient:  0.12263340337409608
iteration : 12166
train acc:  0.734375
train loss:  0.4743591547012329
train gradient:  0.10198363237279108
iteration : 12167
train acc:  0.7890625
train loss:  0.46191921830177307
train gradient:  0.12749071189508265
iteration : 12168
train acc:  0.7421875
train loss:  0.4804767668247223
train gradient:  0.12481490824520539
iteration : 12169
train acc:  0.8203125
train loss:  0.3946842849254608
train gradient:  0.07681933951728227
iteration : 12170
train acc:  0.734375
train loss:  0.512161374092102
train gradient:  0.14792238091720206
iteration : 12171
train acc:  0.7265625
train loss:  0.4564039409160614
train gradient:  0.10230761966714551
iteration : 12172
train acc:  0.703125
train loss:  0.48622485995292664
train gradient:  0.13444652407900848
iteration : 12173
train acc:  0.7578125
train loss:  0.4714130163192749
train gradient:  0.0785611301202556
iteration : 12174
train acc:  0.7109375
train loss:  0.5818252563476562
train gradient:  0.16909973869301845
iteration : 12175
train acc:  0.6953125
train loss:  0.5050785541534424
train gradient:  0.15433091705049384
iteration : 12176
train acc:  0.71875
train loss:  0.5292044281959534
train gradient:  0.1384098837567634
iteration : 12177
train acc:  0.7109375
train loss:  0.5081435441970825
train gradient:  0.13487259665603596
iteration : 12178
train acc:  0.7578125
train loss:  0.4568297266960144
train gradient:  0.09140054249310263
iteration : 12179
train acc:  0.765625
train loss:  0.4291169047355652
train gradient:  0.100405212876743
iteration : 12180
train acc:  0.75
train loss:  0.49172961711883545
train gradient:  0.101663558567045
iteration : 12181
train acc:  0.703125
train loss:  0.5477522015571594
train gradient:  0.1465583201632557
iteration : 12182
train acc:  0.7109375
train loss:  0.49665147066116333
train gradient:  0.10825119049000724
iteration : 12183
train acc:  0.8125
train loss:  0.44012391567230225
train gradient:  0.08628410083334362
iteration : 12184
train acc:  0.7734375
train loss:  0.47182244062423706
train gradient:  0.10588060055386035
iteration : 12185
train acc:  0.7109375
train loss:  0.491604208946228
train gradient:  0.11409313522959745
iteration : 12186
train acc:  0.71875
train loss:  0.5211579203605652
train gradient:  0.14368378629628936
iteration : 12187
train acc:  0.734375
train loss:  0.46175622940063477
train gradient:  0.14065377858678862
iteration : 12188
train acc:  0.7109375
train loss:  0.5070460438728333
train gradient:  0.1346278709400932
iteration : 12189
train acc:  0.7265625
train loss:  0.46205583214759827
train gradient:  0.11355333662090325
iteration : 12190
train acc:  0.7578125
train loss:  0.4765348732471466
train gradient:  0.12183478061607537
iteration : 12191
train acc:  0.7109375
train loss:  0.5413219928741455
train gradient:  0.1174254953433939
iteration : 12192
train acc:  0.765625
train loss:  0.45364096760749817
train gradient:  0.11102375541329282
iteration : 12193
train acc:  0.8125
train loss:  0.432682603597641
train gradient:  0.10190847082690255
iteration : 12194
train acc:  0.7578125
train loss:  0.4767976403236389
train gradient:  0.11843311486061565
iteration : 12195
train acc:  0.765625
train loss:  0.46327584981918335
train gradient:  0.11211635190890687
iteration : 12196
train acc:  0.6953125
train loss:  0.5055065155029297
train gradient:  0.15301399039175223
iteration : 12197
train acc:  0.7734375
train loss:  0.4875449538230896
train gradient:  0.09939831371039591
iteration : 12198
train acc:  0.75
train loss:  0.49741098284721375
train gradient:  0.1108662936412673
iteration : 12199
train acc:  0.7890625
train loss:  0.42609530687332153
train gradient:  0.10587397750695592
iteration : 12200
train acc:  0.7578125
train loss:  0.41902363300323486
train gradient:  0.07446336385837612
iteration : 12201
train acc:  0.7734375
train loss:  0.4645969867706299
train gradient:  0.0971781514776478
iteration : 12202
train acc:  0.734375
train loss:  0.5431303977966309
train gradient:  0.1450052172300126
iteration : 12203
train acc:  0.796875
train loss:  0.44146648049354553
train gradient:  0.10125608993925615
iteration : 12204
train acc:  0.7890625
train loss:  0.4346797466278076
train gradient:  0.11973647407524045
iteration : 12205
train acc:  0.6484375
train loss:  0.5434658527374268
train gradient:  0.12549761043447855
iteration : 12206
train acc:  0.65625
train loss:  0.540767252445221
train gradient:  0.1649985089657713
iteration : 12207
train acc:  0.671875
train loss:  0.5945872664451599
train gradient:  0.16404036924354812
iteration : 12208
train acc:  0.7578125
train loss:  0.44424593448638916
train gradient:  0.08837177201863328
iteration : 12209
train acc:  0.7421875
train loss:  0.4481929540634155
train gradient:  0.0857546421922469
iteration : 12210
train acc:  0.6953125
train loss:  0.546035885810852
train gradient:  0.158878312644868
iteration : 12211
train acc:  0.78125
train loss:  0.44460415840148926
train gradient:  0.08951628453760599
iteration : 12212
train acc:  0.7421875
train loss:  0.4789251685142517
train gradient:  0.09124020066380412
iteration : 12213
train acc:  0.7421875
train loss:  0.45290467143058777
train gradient:  0.10884497754149203
iteration : 12214
train acc:  0.8671875
train loss:  0.3631841540336609
train gradient:  0.07662787819910195
iteration : 12215
train acc:  0.7109375
train loss:  0.497602641582489
train gradient:  0.11678811421095624
iteration : 12216
train acc:  0.71875
train loss:  0.5554637312889099
train gradient:  0.16701108700575362
iteration : 12217
train acc:  0.71875
train loss:  0.5072360038757324
train gradient:  0.09883454351031247
iteration : 12218
train acc:  0.78125
train loss:  0.4312988817691803
train gradient:  0.1472958418757851
iteration : 12219
train acc:  0.7421875
train loss:  0.5295184254646301
train gradient:  0.10650957531353401
iteration : 12220
train acc:  0.7734375
train loss:  0.4333209991455078
train gradient:  0.09264324046010514
iteration : 12221
train acc:  0.7109375
train loss:  0.5526089668273926
train gradient:  0.15132480042352497
iteration : 12222
train acc:  0.8046875
train loss:  0.4372396171092987
train gradient:  0.1161956503615418
iteration : 12223
train acc:  0.71875
train loss:  0.48867565393447876
train gradient:  0.11153005121791738
iteration : 12224
train acc:  0.6796875
train loss:  0.5523303747177124
train gradient:  0.18127615228834482
iteration : 12225
train acc:  0.796875
train loss:  0.44003826379776
train gradient:  0.08987526687377718
iteration : 12226
train acc:  0.78125
train loss:  0.46448346972465515
train gradient:  0.11826850745160038
iteration : 12227
train acc:  0.796875
train loss:  0.4535161852836609
train gradient:  0.10359932243766691
iteration : 12228
train acc:  0.6796875
train loss:  0.519120454788208
train gradient:  0.14975741900912387
iteration : 12229
train acc:  0.7734375
train loss:  0.481600284576416
train gradient:  0.12598871404086964
iteration : 12230
train acc:  0.7890625
train loss:  0.4185170531272888
train gradient:  0.11333738365132527
iteration : 12231
train acc:  0.7265625
train loss:  0.46909651160240173
train gradient:  0.11990850785341295
iteration : 12232
train acc:  0.765625
train loss:  0.47991669178009033
train gradient:  0.12367697590072857
iteration : 12233
train acc:  0.734375
train loss:  0.469781756401062
train gradient:  0.0992592471802796
iteration : 12234
train acc:  0.765625
train loss:  0.4505999982357025
train gradient:  0.11577321088671634
iteration : 12235
train acc:  0.734375
train loss:  0.514421284198761
train gradient:  0.1622576512909919
iteration : 12236
train acc:  0.7421875
train loss:  0.46495139598846436
train gradient:  0.12140015492899778
iteration : 12237
train acc:  0.6796875
train loss:  0.5051552057266235
train gradient:  0.13887280150914508
iteration : 12238
train acc:  0.7890625
train loss:  0.4551045298576355
train gradient:  0.13088447118567503
iteration : 12239
train acc:  0.765625
train loss:  0.47368454933166504
train gradient:  0.12734713049086072
iteration : 12240
train acc:  0.7265625
train loss:  0.48549485206604004
train gradient:  0.1241000271396747
iteration : 12241
train acc:  0.78125
train loss:  0.4414135813713074
train gradient:  0.09042422135175508
iteration : 12242
train acc:  0.7890625
train loss:  0.444225013256073
train gradient:  0.10512327137924436
iteration : 12243
train acc:  0.7734375
train loss:  0.4891808032989502
train gradient:  0.11028115964644923
iteration : 12244
train acc:  0.6953125
train loss:  0.529255211353302
train gradient:  0.18315052216189562
iteration : 12245
train acc:  0.71875
train loss:  0.4956808090209961
train gradient:  0.13869729883991017
iteration : 12246
train acc:  0.703125
train loss:  0.5265612602233887
train gradient:  0.13293418838153015
iteration : 12247
train acc:  0.734375
train loss:  0.46167829632759094
train gradient:  0.1079974209413207
iteration : 12248
train acc:  0.7890625
train loss:  0.43120285868644714
train gradient:  0.11749720383147516
iteration : 12249
train acc:  0.7734375
train loss:  0.42121651768684387
train gradient:  0.08781315899172244
iteration : 12250
train acc:  0.7265625
train loss:  0.5524533987045288
train gradient:  0.13173381754546248
iteration : 12251
train acc:  0.7265625
train loss:  0.4832609295845032
train gradient:  0.11823819768355921
iteration : 12252
train acc:  0.75
train loss:  0.5039547681808472
train gradient:  0.1429743551260197
iteration : 12253
train acc:  0.703125
train loss:  0.5126621723175049
train gradient:  0.10929391110777172
iteration : 12254
train acc:  0.71875
train loss:  0.5753997564315796
train gradient:  0.14005552835415908
iteration : 12255
train acc:  0.75
train loss:  0.4607076644897461
train gradient:  0.10847161595468852
iteration : 12256
train acc:  0.765625
train loss:  0.529289722442627
train gradient:  0.10986853333812181
iteration : 12257
train acc:  0.671875
train loss:  0.5582579970359802
train gradient:  0.1298845546108002
iteration : 12258
train acc:  0.7421875
train loss:  0.46731919050216675
train gradient:  0.10829605046815872
iteration : 12259
train acc:  0.6640625
train loss:  0.5879921913146973
train gradient:  0.1679239140710423
iteration : 12260
train acc:  0.7109375
train loss:  0.5292609930038452
train gradient:  0.13519908309659762
iteration : 12261
train acc:  0.734375
train loss:  0.4763104319572449
train gradient:  0.1270204073746894
iteration : 12262
train acc:  0.7109375
train loss:  0.4642612636089325
train gradient:  0.11581193075199805
iteration : 12263
train acc:  0.7265625
train loss:  0.565719485282898
train gradient:  0.15114906206324827
iteration : 12264
train acc:  0.7109375
train loss:  0.5079295635223389
train gradient:  0.12426585655334335
iteration : 12265
train acc:  0.71875
train loss:  0.5097205638885498
train gradient:  0.13747239709513448
iteration : 12266
train acc:  0.765625
train loss:  0.4298385977745056
train gradient:  0.10237459134659191
iteration : 12267
train acc:  0.7421875
train loss:  0.4945642948150635
train gradient:  0.1352277781356023
iteration : 12268
train acc:  0.7265625
train loss:  0.5345782041549683
train gradient:  0.13320653930618356
iteration : 12269
train acc:  0.703125
train loss:  0.5273429155349731
train gradient:  0.13216486583046017
iteration : 12270
train acc:  0.7265625
train loss:  0.4716673791408539
train gradient:  0.10257386707892675
iteration : 12271
train acc:  0.7734375
train loss:  0.44220539927482605
train gradient:  0.1118750739063197
iteration : 12272
train acc:  0.7265625
train loss:  0.5113646984100342
train gradient:  0.10457132405432928
iteration : 12273
train acc:  0.7890625
train loss:  0.4741165041923523
train gradient:  0.11701653866809585
iteration : 12274
train acc:  0.7421875
train loss:  0.47641509771347046
train gradient:  0.15225001631065493
iteration : 12275
train acc:  0.671875
train loss:  0.625853419303894
train gradient:  0.17401052297563002
iteration : 12276
train acc:  0.7265625
train loss:  0.4967862665653229
train gradient:  0.13039772658856286
iteration : 12277
train acc:  0.7421875
train loss:  0.5021289587020874
train gradient:  0.13321354835963511
iteration : 12278
train acc:  0.7265625
train loss:  0.5158613920211792
train gradient:  0.14474565150520446
iteration : 12279
train acc:  0.7890625
train loss:  0.43036746978759766
train gradient:  0.0989708723465754
iteration : 12280
train acc:  0.765625
train loss:  0.42291486263275146
train gradient:  0.084605099824821
iteration : 12281
train acc:  0.8125
train loss:  0.44285398721694946
train gradient:  0.10690458552429999
iteration : 12282
train acc:  0.671875
train loss:  0.5327251553535461
train gradient:  0.203071712834137
iteration : 12283
train acc:  0.78125
train loss:  0.4772431254386902
train gradient:  0.14007703012966816
iteration : 12284
train acc:  0.7578125
train loss:  0.4575917422771454
train gradient:  0.12382929827720093
iteration : 12285
train acc:  0.7734375
train loss:  0.4676487445831299
train gradient:  0.09868893622391912
iteration : 12286
train acc:  0.75
train loss:  0.509591817855835
train gradient:  0.13714518251440078
iteration : 12287
train acc:  0.75
train loss:  0.4810090661048889
train gradient:  0.10784475850918304
iteration : 12288
train acc:  0.765625
train loss:  0.4560868442058563
train gradient:  0.09926542398006669
iteration : 12289
train acc:  0.765625
train loss:  0.5121067762374878
train gradient:  0.18022651170844453
iteration : 12290
train acc:  0.78125
train loss:  0.4412902593612671
train gradient:  0.09004763839464694
iteration : 12291
train acc:  0.765625
train loss:  0.47232842445373535
train gradient:  0.10213466336007035
iteration : 12292
train acc:  0.7578125
train loss:  0.4702458083629608
train gradient:  0.11275193981226192
iteration : 12293
train acc:  0.796875
train loss:  0.41993409395217896
train gradient:  0.08749275023936352
iteration : 12294
train acc:  0.7421875
train loss:  0.4734421372413635
train gradient:  0.09077767234788331
iteration : 12295
train acc:  0.7265625
train loss:  0.5033949613571167
train gradient:  0.1217883728613502
iteration : 12296
train acc:  0.671875
train loss:  0.5147749185562134
train gradient:  0.11037241868667332
iteration : 12297
train acc:  0.7421875
train loss:  0.4845503866672516
train gradient:  0.09980485425310466
iteration : 12298
train acc:  0.6953125
train loss:  0.5183241963386536
train gradient:  0.11061818455674893
iteration : 12299
train acc:  0.78125
train loss:  0.46729427576065063
train gradient:  0.10416151532764129
iteration : 12300
train acc:  0.75
train loss:  0.48445388674736023
train gradient:  0.1268166819288804
iteration : 12301
train acc:  0.703125
train loss:  0.48964959383010864
train gradient:  0.11042887663126062
iteration : 12302
train acc:  0.6953125
train loss:  0.5311352610588074
train gradient:  0.1268939518499057
iteration : 12303
train acc:  0.6953125
train loss:  0.571007251739502
train gradient:  0.1623162757007049
iteration : 12304
train acc:  0.7578125
train loss:  0.449476420879364
train gradient:  0.10441817871994194
iteration : 12305
train acc:  0.7890625
train loss:  0.42070722579956055
train gradient:  0.11008749003841982
iteration : 12306
train acc:  0.7421875
train loss:  0.48610132932662964
train gradient:  0.1582654171862795
iteration : 12307
train acc:  0.75
train loss:  0.4702233076095581
train gradient:  0.15658164987751583
iteration : 12308
train acc:  0.734375
train loss:  0.4696640372276306
train gradient:  0.09583164835762462
iteration : 12309
train acc:  0.734375
train loss:  0.4871334433555603
train gradient:  0.10539080132294026
iteration : 12310
train acc:  0.796875
train loss:  0.461968332529068
train gradient:  0.10294443608418226
iteration : 12311
train acc:  0.7578125
train loss:  0.5235915184020996
train gradient:  0.18378183559286007
iteration : 12312
train acc:  0.7265625
train loss:  0.5558675527572632
train gradient:  0.146954536537728
iteration : 12313
train acc:  0.7109375
train loss:  0.5028608441352844
train gradient:  0.11125319974393398
iteration : 12314
train acc:  0.7734375
train loss:  0.4974989891052246
train gradient:  0.14563141090895282
iteration : 12315
train acc:  0.7734375
train loss:  0.4723198413848877
train gradient:  0.13288239999376816
iteration : 12316
train acc:  0.7109375
train loss:  0.49034854769706726
train gradient:  0.10688648454494924
iteration : 12317
train acc:  0.765625
train loss:  0.4442102909088135
train gradient:  0.11219478575493176
iteration : 12318
train acc:  0.6953125
train loss:  0.5743808150291443
train gradient:  0.17446234249989911
iteration : 12319
train acc:  0.796875
train loss:  0.4004005491733551
train gradient:  0.07601666992895223
iteration : 12320
train acc:  0.734375
train loss:  0.5046594738960266
train gradient:  0.11603171540595045
iteration : 12321
train acc:  0.7265625
train loss:  0.4943954646587372
train gradient:  0.1237727143429384
iteration : 12322
train acc:  0.796875
train loss:  0.4000662565231323
train gradient:  0.06972513574411596
iteration : 12323
train acc:  0.7421875
train loss:  0.49389708042144775
train gradient:  0.12933079209288667
iteration : 12324
train acc:  0.78125
train loss:  0.49687668681144714
train gradient:  0.12163539122129578
iteration : 12325
train acc:  0.7265625
train loss:  0.5214806795120239
train gradient:  0.11194445333759748
iteration : 12326
train acc:  0.7265625
train loss:  0.5265347957611084
train gradient:  0.11286800469099732
iteration : 12327
train acc:  0.6953125
train loss:  0.5388365387916565
train gradient:  0.15820409715705197
iteration : 12328
train acc:  0.765625
train loss:  0.505020022392273
train gradient:  0.11717998494997019
iteration : 12329
train acc:  0.703125
train loss:  0.5207035541534424
train gradient:  0.1522027889081397
iteration : 12330
train acc:  0.71875
train loss:  0.5308592319488525
train gradient:  0.10959842036558319
iteration : 12331
train acc:  0.765625
train loss:  0.4460318088531494
train gradient:  0.1052021021199893
iteration : 12332
train acc:  0.7578125
train loss:  0.4638439118862152
train gradient:  0.13054246695138724
iteration : 12333
train acc:  0.7265625
train loss:  0.4927229881286621
train gradient:  0.12274326117150113
iteration : 12334
train acc:  0.6796875
train loss:  0.5418341755867004
train gradient:  0.12473168681217339
iteration : 12335
train acc:  0.71875
train loss:  0.5402601957321167
train gradient:  0.15862646417338538
iteration : 12336
train acc:  0.8046875
train loss:  0.44017577171325684
train gradient:  0.10365832295010552
iteration : 12337
train acc:  0.78125
train loss:  0.4744431972503662
train gradient:  0.13312057478108463
iteration : 12338
train acc:  0.71875
train loss:  0.4818772077560425
train gradient:  0.0951600531000376
iteration : 12339
train acc:  0.7890625
train loss:  0.49772995710372925
train gradient:  0.12500555610464875
iteration : 12340
train acc:  0.7578125
train loss:  0.4608715772628784
train gradient:  0.11398830241593859
iteration : 12341
train acc:  0.796875
train loss:  0.4844990670681
train gradient:  0.0995138485783633
iteration : 12342
train acc:  0.7109375
train loss:  0.5118833780288696
train gradient:  0.1372709282765073
iteration : 12343
train acc:  0.734375
train loss:  0.5053691864013672
train gradient:  0.12122497023334038
iteration : 12344
train acc:  0.6953125
train loss:  0.5456043481826782
train gradient:  0.13543682947359584
iteration : 12345
train acc:  0.75
train loss:  0.5656032562255859
train gradient:  0.15706078714207994
iteration : 12346
train acc:  0.8046875
train loss:  0.44652479887008667
train gradient:  0.10010853923766173
iteration : 12347
train acc:  0.7578125
train loss:  0.4351985454559326
train gradient:  0.08469095243113697
iteration : 12348
train acc:  0.7734375
train loss:  0.467430055141449
train gradient:  0.10439898533784227
iteration : 12349
train acc:  0.7578125
train loss:  0.46696737408638
train gradient:  0.08401339998877516
iteration : 12350
train acc:  0.71875
train loss:  0.5526991486549377
train gradient:  0.176324878170368
iteration : 12351
train acc:  0.703125
train loss:  0.4985444247722626
train gradient:  0.1021240511416057
iteration : 12352
train acc:  0.78125
train loss:  0.45065838098526
train gradient:  0.09471809072731238
iteration : 12353
train acc:  0.765625
train loss:  0.47872257232666016
train gradient:  0.10317087899048115
iteration : 12354
train acc:  0.7421875
train loss:  0.4667898416519165
train gradient:  0.10685842036772696
iteration : 12355
train acc:  0.7421875
train loss:  0.4937154948711395
train gradient:  0.108127379852016
iteration : 12356
train acc:  0.71875
train loss:  0.44790637493133545
train gradient:  0.09495518049820108
iteration : 12357
train acc:  0.7421875
train loss:  0.4794338345527649
train gradient:  0.1092473792471269
iteration : 12358
train acc:  0.8125
train loss:  0.41784727573394775
train gradient:  0.09127926040616542
iteration : 12359
train acc:  0.7265625
train loss:  0.4668225646018982
train gradient:  0.10565377999436334
iteration : 12360
train acc:  0.6796875
train loss:  0.5128438472747803
train gradient:  0.12335942498079892
iteration : 12361
train acc:  0.8046875
train loss:  0.46144211292266846
train gradient:  0.10346369466035592
iteration : 12362
train acc:  0.671875
train loss:  0.6259347200393677
train gradient:  0.16006421717770009
iteration : 12363
train acc:  0.7265625
train loss:  0.4795072376728058
train gradient:  0.1323861424101514
iteration : 12364
train acc:  0.71875
train loss:  0.5371391177177429
train gradient:  0.11892647489704114
iteration : 12365
train acc:  0.6796875
train loss:  0.5196633338928223
train gradient:  0.17025679667799684
iteration : 12366
train acc:  0.765625
train loss:  0.42927080392837524
train gradient:  0.09574764727854247
iteration : 12367
train acc:  0.703125
train loss:  0.5887250900268555
train gradient:  0.14888154277146665
iteration : 12368
train acc:  0.75
train loss:  0.48666876554489136
train gradient:  0.08862927142133496
iteration : 12369
train acc:  0.75
train loss:  0.48995694518089294
train gradient:  0.09393923869492483
iteration : 12370
train acc:  0.7109375
train loss:  0.5182515382766724
train gradient:  0.12752452739631265
iteration : 12371
train acc:  0.78125
train loss:  0.46438777446746826
train gradient:  0.09400218889388619
iteration : 12372
train acc:  0.75
train loss:  0.49683159589767456
train gradient:  0.12069734700808385
iteration : 12373
train acc:  0.75
train loss:  0.5450198650360107
train gradient:  0.18050772390305508
iteration : 12374
train acc:  0.796875
train loss:  0.4538654685020447
train gradient:  0.09596668945077493
iteration : 12375
train acc:  0.6953125
train loss:  0.5078416466712952
train gradient:  0.1622667892529689
iteration : 12376
train acc:  0.765625
train loss:  0.4716203510761261
train gradient:  0.11280176546926601
iteration : 12377
train acc:  0.828125
train loss:  0.3899844288825989
train gradient:  0.07584035746971267
iteration : 12378
train acc:  0.703125
train loss:  0.5113565921783447
train gradient:  0.11500798995875619
iteration : 12379
train acc:  0.7109375
train loss:  0.47631385922431946
train gradient:  0.1012752504911879
iteration : 12380
train acc:  0.7109375
train loss:  0.5532727241516113
train gradient:  0.15675039413655423
iteration : 12381
train acc:  0.71875
train loss:  0.522149920463562
train gradient:  0.12436359347883981
iteration : 12382
train acc:  0.8359375
train loss:  0.43017876148223877
train gradient:  0.10155824652228643
iteration : 12383
train acc:  0.734375
train loss:  0.48517513275146484
train gradient:  0.11826129881862968
iteration : 12384
train acc:  0.703125
train loss:  0.5170676708221436
train gradient:  0.13657901758540728
iteration : 12385
train acc:  0.796875
train loss:  0.40314602851867676
train gradient:  0.0791480686832097
iteration : 12386
train acc:  0.7265625
train loss:  0.5223082900047302
train gradient:  0.13203315048642505
iteration : 12387
train acc:  0.7578125
train loss:  0.4829767644405365
train gradient:  0.10700876527263481
iteration : 12388
train acc:  0.7265625
train loss:  0.5212268829345703
train gradient:  0.1264714107945278
iteration : 12389
train acc:  0.71875
train loss:  0.4823393225669861
train gradient:  0.0956930886028601
iteration : 12390
train acc:  0.71875
train loss:  0.52131187915802
train gradient:  0.12391088630026054
iteration : 12391
train acc:  0.7109375
train loss:  0.5233970880508423
train gradient:  0.12935107501721438
iteration : 12392
train acc:  0.7265625
train loss:  0.4796016812324524
train gradient:  0.10873458764582605
iteration : 12393
train acc:  0.71875
train loss:  0.5233949422836304
train gradient:  0.1643636557252537
iteration : 12394
train acc:  0.7578125
train loss:  0.4596560299396515
train gradient:  0.11637984642989237
iteration : 12395
train acc:  0.8046875
train loss:  0.4294354319572449
train gradient:  0.10182210363109297
iteration : 12396
train acc:  0.734375
train loss:  0.5019568800926208
train gradient:  0.11782516943097167
iteration : 12397
train acc:  0.7578125
train loss:  0.46005797386169434
train gradient:  0.09965106680149925
iteration : 12398
train acc:  0.7578125
train loss:  0.4651515781879425
train gradient:  0.09495778857071127
iteration : 12399
train acc:  0.6875
train loss:  0.532623291015625
train gradient:  0.11702258639082602
iteration : 12400
train acc:  0.7265625
train loss:  0.514417290687561
train gradient:  0.16860223336541075
iteration : 12401
train acc:  0.78125
train loss:  0.4542050361633301
train gradient:  0.11592656160321997
iteration : 12402
train acc:  0.765625
train loss:  0.46671557426452637
train gradient:  0.11493463599036001
iteration : 12403
train acc:  0.765625
train loss:  0.4744473695755005
train gradient:  0.10835439598369873
iteration : 12404
train acc:  0.734375
train loss:  0.49982374906539917
train gradient:  0.1071704748873937
iteration : 12405
train acc:  0.703125
train loss:  0.49133431911468506
train gradient:  0.12049462250698556
iteration : 12406
train acc:  0.7109375
train loss:  0.5315482020378113
train gradient:  0.17022746872542738
iteration : 12407
train acc:  0.7421875
train loss:  0.4787481427192688
train gradient:  0.12582212830563005
iteration : 12408
train acc:  0.7109375
train loss:  0.5573773384094238
train gradient:  0.1599921569637402
iteration : 12409
train acc:  0.6875
train loss:  0.5638316869735718
train gradient:  0.14988115201137847
iteration : 12410
train acc:  0.84375
train loss:  0.4343249797821045
train gradient:  0.08834963939725507
iteration : 12411
train acc:  0.7890625
train loss:  0.44462865591049194
train gradient:  0.10329740717375835
iteration : 12412
train acc:  0.765625
train loss:  0.432914137840271
train gradient:  0.09270268019767446
iteration : 12413
train acc:  0.7734375
train loss:  0.4390537142753601
train gradient:  0.09015818054317516
iteration : 12414
train acc:  0.6875
train loss:  0.5476313829421997
train gradient:  0.13022618075693504
iteration : 12415
train acc:  0.7265625
train loss:  0.46323493123054504
train gradient:  0.08975242314833123
iteration : 12416
train acc:  0.7265625
train loss:  0.4906911551952362
train gradient:  0.11406054496855751
iteration : 12417
train acc:  0.6953125
train loss:  0.5568280816078186
train gradient:  0.1356105588234605
iteration : 12418
train acc:  0.78125
train loss:  0.4362340569496155
train gradient:  0.08865286706511524
iteration : 12419
train acc:  0.7890625
train loss:  0.42767199873924255
train gradient:  0.09511331619477947
iteration : 12420
train acc:  0.75
train loss:  0.491958349943161
train gradient:  0.12683326295937544
iteration : 12421
train acc:  0.7578125
train loss:  0.48579010367393494
train gradient:  0.10648898999183234
iteration : 12422
train acc:  0.71875
train loss:  0.5133823752403259
train gradient:  0.13282351244766177
iteration : 12423
train acc:  0.7109375
train loss:  0.5075857043266296
train gradient:  0.12142290603569264
iteration : 12424
train acc:  0.6796875
train loss:  0.5761500000953674
train gradient:  0.15409475371164338
iteration : 12425
train acc:  0.78125
train loss:  0.4917968213558197
train gradient:  0.09690895520397926
iteration : 12426
train acc:  0.7578125
train loss:  0.4823552966117859
train gradient:  0.13781578674517386
iteration : 12427
train acc:  0.6953125
train loss:  0.5427321195602417
train gradient:  0.15030899430343236
iteration : 12428
train acc:  0.6875
train loss:  0.5554807186126709
train gradient:  0.17994975958013198
iteration : 12429
train acc:  0.734375
train loss:  0.4523824453353882
train gradient:  0.10109856154396256
iteration : 12430
train acc:  0.7890625
train loss:  0.4811742901802063
train gradient:  0.10216227716705438
iteration : 12431
train acc:  0.734375
train loss:  0.5312789678573608
train gradient:  0.13457587640738702
iteration : 12432
train acc:  0.7578125
train loss:  0.47172361612319946
train gradient:  0.09422413126560673
iteration : 12433
train acc:  0.7734375
train loss:  0.47659844160079956
train gradient:  0.09964074865512063
iteration : 12434
train acc:  0.75
train loss:  0.474708616733551
train gradient:  0.09396242555505056
iteration : 12435
train acc:  0.75
train loss:  0.4471103847026825
train gradient:  0.09385820590213668
iteration : 12436
train acc:  0.71875
train loss:  0.5498888492584229
train gradient:  0.16159279642728866
iteration : 12437
train acc:  0.7265625
train loss:  0.5365185737609863
train gradient:  0.1224211931533005
iteration : 12438
train acc:  0.6796875
train loss:  0.5287989377975464
train gradient:  0.12535200128595905
iteration : 12439
train acc:  0.78125
train loss:  0.4647839069366455
train gradient:  0.0828068787572381
iteration : 12440
train acc:  0.7421875
train loss:  0.5359879732131958
train gradient:  0.1434549682726743
iteration : 12441
train acc:  0.765625
train loss:  0.502356231212616
train gradient:  0.11029777570639658
iteration : 12442
train acc:  0.6796875
train loss:  0.563826322555542
train gradient:  0.13423782155608582
iteration : 12443
train acc:  0.7890625
train loss:  0.4383084774017334
train gradient:  0.1229309091117925
iteration : 12444
train acc:  0.7421875
train loss:  0.45464378595352173
train gradient:  0.10178742777499464
iteration : 12445
train acc:  0.671875
train loss:  0.5025070905685425
train gradient:  0.1059689646177794
iteration : 12446
train acc:  0.7890625
train loss:  0.43453824520111084
train gradient:  0.09351503986695171
iteration : 12447
train acc:  0.6875
train loss:  0.584192156791687
train gradient:  0.16483097921044415
iteration : 12448
train acc:  0.7421875
train loss:  0.5260997414588928
train gradient:  0.12689912116145924
iteration : 12449
train acc:  0.796875
train loss:  0.4515848159790039
train gradient:  0.08730840997732814
iteration : 12450
train acc:  0.6796875
train loss:  0.5341636538505554
train gradient:  0.13548228824588904
iteration : 12451
train acc:  0.765625
train loss:  0.5352484583854675
train gradient:  0.13487259419623618
iteration : 12452
train acc:  0.703125
train loss:  0.4991806149482727
train gradient:  0.12346701932770504
iteration : 12453
train acc:  0.734375
train loss:  0.4941651225090027
train gradient:  0.11867264315801802
iteration : 12454
train acc:  0.78125
train loss:  0.4226517379283905
train gradient:  0.09458564452251732
iteration : 12455
train acc:  0.71875
train loss:  0.49211812019348145
train gradient:  0.10619541354067066
iteration : 12456
train acc:  0.7421875
train loss:  0.5133680105209351
train gradient:  0.17061941487225235
iteration : 12457
train acc:  0.75
train loss:  0.47243183851242065
train gradient:  0.09571352268076853
iteration : 12458
train acc:  0.765625
train loss:  0.4532759487628937
train gradient:  0.10546013585771738
iteration : 12459
train acc:  0.71875
train loss:  0.4701705574989319
train gradient:  0.11321656264841166
iteration : 12460
train acc:  0.75
train loss:  0.5936560034751892
train gradient:  0.1328106834056736
iteration : 12461
train acc:  0.703125
train loss:  0.511542797088623
train gradient:  0.13900471413863053
iteration : 12462
train acc:  0.7265625
train loss:  0.494866281747818
train gradient:  0.11058954221635821
iteration : 12463
train acc:  0.75
train loss:  0.4622024893760681
train gradient:  0.0946632526778366
iteration : 12464
train acc:  0.734375
train loss:  0.45860594511032104
train gradient:  0.11349796970867986
iteration : 12465
train acc:  0.796875
train loss:  0.4405098557472229
train gradient:  0.09164683179554901
iteration : 12466
train acc:  0.7890625
train loss:  0.49698585271835327
train gradient:  0.11839970014525061
iteration : 12467
train acc:  0.7265625
train loss:  0.5046321153640747
train gradient:  0.13368435091707004
iteration : 12468
train acc:  0.7578125
train loss:  0.4579591751098633
train gradient:  0.10969429119702101
iteration : 12469
train acc:  0.84375
train loss:  0.43664783239364624
train gradient:  0.12254625287343711
iteration : 12470
train acc:  0.765625
train loss:  0.5189995765686035
train gradient:  0.12263030337095145
iteration : 12471
train acc:  0.7421875
train loss:  0.5161365270614624
train gradient:  0.14583672068446513
iteration : 12472
train acc:  0.71875
train loss:  0.49987781047821045
train gradient:  0.10071035282468875
iteration : 12473
train acc:  0.7421875
train loss:  0.47931283712387085
train gradient:  0.09465578403553
iteration : 12474
train acc:  0.6953125
train loss:  0.6600120067596436
train gradient:  0.2103951925216303
iteration : 12475
train acc:  0.84375
train loss:  0.3865739703178406
train gradient:  0.07405144107405384
iteration : 12476
train acc:  0.7890625
train loss:  0.4285859763622284
train gradient:  0.08299951561072276
iteration : 12477
train acc:  0.7109375
train loss:  0.49767884612083435
train gradient:  0.1198252821516351
iteration : 12478
train acc:  0.7578125
train loss:  0.44755762815475464
train gradient:  0.10712357387520335
iteration : 12479
train acc:  0.6796875
train loss:  0.5393273830413818
train gradient:  0.12089738611380654
iteration : 12480
train acc:  0.6875
train loss:  0.5800752639770508
train gradient:  0.1374730637063496
iteration : 12481
train acc:  0.734375
train loss:  0.48720240592956543
train gradient:  0.10261637576110576
iteration : 12482
train acc:  0.703125
train loss:  0.5108704566955566
train gradient:  0.11385391916606578
iteration : 12483
train acc:  0.75
train loss:  0.511529803276062
train gradient:  0.12492265967786416
iteration : 12484
train acc:  0.7109375
train loss:  0.5048305988311768
train gradient:  0.14067403666396744
iteration : 12485
train acc:  0.6953125
train loss:  0.5533328652381897
train gradient:  0.13791401795399985
iteration : 12486
train acc:  0.7265625
train loss:  0.47523877024650574
train gradient:  0.0941999870239397
iteration : 12487
train acc:  0.6875
train loss:  0.505143404006958
train gradient:  0.11652197680209161
iteration : 12488
train acc:  0.765625
train loss:  0.4608227014541626
train gradient:  0.07804052472022306
iteration : 12489
train acc:  0.7734375
train loss:  0.4618682265281677
train gradient:  0.08482602714015937
iteration : 12490
train acc:  0.7265625
train loss:  0.4865384101867676
train gradient:  0.1217852271290901
iteration : 12491
train acc:  0.765625
train loss:  0.4741387963294983
train gradient:  0.09808307455634345
iteration : 12492
train acc:  0.734375
train loss:  0.47148147225379944
train gradient:  0.10078336699302663
iteration : 12493
train acc:  0.7265625
train loss:  0.5161741971969604
train gradient:  0.10514399901517302
iteration : 12494
train acc:  0.6796875
train loss:  0.5649171471595764
train gradient:  0.16613766464765056
iteration : 12495
train acc:  0.765625
train loss:  0.43140506744384766
train gradient:  0.08096258041159346
iteration : 12496
train acc:  0.7890625
train loss:  0.48306241631507874
train gradient:  0.12028307416484631
iteration : 12497
train acc:  0.7578125
train loss:  0.44576239585876465
train gradient:  0.113081351875184
iteration : 12498
train acc:  0.75
train loss:  0.48761653900146484
train gradient:  0.11437476309702552
iteration : 12499
train acc:  0.7578125
train loss:  0.4697834849357605
train gradient:  0.09775255156056241
iteration : 12500
train acc:  0.734375
train loss:  0.46585795283317566
train gradient:  0.08798448599207535
iteration : 12501
train acc:  0.75
train loss:  0.49803265929222107
train gradient:  0.15186201571602115
iteration : 12502
train acc:  0.75
train loss:  0.487283319234848
train gradient:  0.10855833070104967
iteration : 12503
train acc:  0.75
train loss:  0.4314661920070648
train gradient:  0.10715814010909805
iteration : 12504
train acc:  0.7265625
train loss:  0.4995308518409729
train gradient:  0.11996861229111064
iteration : 12505
train acc:  0.734375
train loss:  0.5075499415397644
train gradient:  0.12210080501704829
iteration : 12506
train acc:  0.7890625
train loss:  0.49297457933425903
train gradient:  0.10200584426255427
iteration : 12507
train acc:  0.703125
train loss:  0.46899837255477905
train gradient:  0.10436662814508599
iteration : 12508
train acc:  0.75
train loss:  0.5023961067199707
train gradient:  0.10043380552356079
iteration : 12509
train acc:  0.78125
train loss:  0.45132142305374146
train gradient:  0.11494150889313634
iteration : 12510
train acc:  0.6875
train loss:  0.5188037157058716
train gradient:  0.10580311966354676
iteration : 12511
train acc:  0.671875
train loss:  0.5310303568840027
train gradient:  0.12151295683376642
iteration : 12512
train acc:  0.796875
train loss:  0.43424633145332336
train gradient:  0.09258004341870937
iteration : 12513
train acc:  0.7265625
train loss:  0.49085211753845215
train gradient:  0.14032671425046128
iteration : 12514
train acc:  0.765625
train loss:  0.4479903280735016
train gradient:  0.11681911798620749
iteration : 12515
train acc:  0.65625
train loss:  0.5784004926681519
train gradient:  0.1457816379937582
iteration : 12516
train acc:  0.7265625
train loss:  0.5256446599960327
train gradient:  0.13593445968640852
iteration : 12517
train acc:  0.78125
train loss:  0.4958895742893219
train gradient:  0.13912456042262628
iteration : 12518
train acc:  0.7421875
train loss:  0.4911442995071411
train gradient:  0.11302418748996862
iteration : 12519
train acc:  0.734375
train loss:  0.4776935875415802
train gradient:  0.0846622260421495
iteration : 12520
train acc:  0.7578125
train loss:  0.4683321714401245
train gradient:  0.0935279343086067
iteration : 12521
train acc:  0.703125
train loss:  0.5186848640441895
train gradient:  0.1451020935702259
iteration : 12522
train acc:  0.765625
train loss:  0.5448353886604309
train gradient:  0.1424094332542631
iteration : 12523
train acc:  0.71875
train loss:  0.4740259647369385
train gradient:  0.10891190957928572
iteration : 12524
train acc:  0.734375
train loss:  0.5264509916305542
train gradient:  0.15908147599011707
iteration : 12525
train acc:  0.7109375
train loss:  0.47947144508361816
train gradient:  0.10793024234696487
iteration : 12526
train acc:  0.765625
train loss:  0.448015421628952
train gradient:  0.08884715978047956
iteration : 12527
train acc:  0.796875
train loss:  0.4341524839401245
train gradient:  0.08484490475671341
iteration : 12528
train acc:  0.75
train loss:  0.5161889791488647
train gradient:  0.12799008773168125
iteration : 12529
train acc:  0.7109375
train loss:  0.5242464542388916
train gradient:  0.11817745049842539
iteration : 12530
train acc:  0.78125
train loss:  0.468147873878479
train gradient:  0.09504603221367534
iteration : 12531
train acc:  0.75
train loss:  0.48101478815078735
train gradient:  0.09888794781493528
iteration : 12532
train acc:  0.75
train loss:  0.4983915090560913
train gradient:  0.11996123485870679
iteration : 12533
train acc:  0.796875
train loss:  0.4118685722351074
train gradient:  0.09695262541669056
iteration : 12534
train acc:  0.8125
train loss:  0.43535125255584717
train gradient:  0.08618283444319448
iteration : 12535
train acc:  0.8046875
train loss:  0.422652006149292
train gradient:  0.06857265604657051
iteration : 12536
train acc:  0.8203125
train loss:  0.41473159193992615
train gradient:  0.10194626662256948
iteration : 12537
train acc:  0.8359375
train loss:  0.39503031969070435
train gradient:  0.08292222629564253
iteration : 12538
train acc:  0.7734375
train loss:  0.5144902467727661
train gradient:  0.15737253708195575
iteration : 12539
train acc:  0.71875
train loss:  0.513491690158844
train gradient:  0.13119709874478147
iteration : 12540
train acc:  0.765625
train loss:  0.43551868200302124
train gradient:  0.11535675031845716
iteration : 12541
train acc:  0.7109375
train loss:  0.5176037549972534
train gradient:  0.1561939686216997
iteration : 12542
train acc:  0.7890625
train loss:  0.4661853313446045
train gradient:  0.10757245470755349
iteration : 12543
train acc:  0.765625
train loss:  0.41358205676078796
train gradient:  0.08167473547601391
iteration : 12544
train acc:  0.7578125
train loss:  0.4674745202064514
train gradient:  0.09956832269196873
iteration : 12545
train acc:  0.6953125
train loss:  0.5261046886444092
train gradient:  0.13749623126605787
iteration : 12546
train acc:  0.734375
train loss:  0.4803846478462219
train gradient:  0.11258793489769008
iteration : 12547
train acc:  0.7109375
train loss:  0.4929053783416748
train gradient:  0.10169982650572208
iteration : 12548
train acc:  0.7578125
train loss:  0.4464556574821472
train gradient:  0.0930563158729422
iteration : 12549
train acc:  0.8359375
train loss:  0.4015434980392456
train gradient:  0.08032454061535889
iteration : 12550
train acc:  0.7890625
train loss:  0.4268275499343872
train gradient:  0.1012576439470253
iteration : 12551
train acc:  0.7109375
train loss:  0.5867494940757751
train gradient:  0.1936026580628358
iteration : 12552
train acc:  0.734375
train loss:  0.5255857110023499
train gradient:  0.15883916928440367
iteration : 12553
train acc:  0.71875
train loss:  0.5287719368934631
train gradient:  0.1147196197003511
iteration : 12554
train acc:  0.859375
train loss:  0.39446091651916504
train gradient:  0.0855627128444084
iteration : 12555
train acc:  0.78125
train loss:  0.4429881274700165
train gradient:  0.12498256914026037
iteration : 12556
train acc:  0.6640625
train loss:  0.5476034879684448
train gradient:  0.16310450405593724
iteration : 12557
train acc:  0.75
train loss:  0.44618678092956543
train gradient:  0.0853831815604891
iteration : 12558
train acc:  0.765625
train loss:  0.5131423473358154
train gradient:  0.12764635558498058
iteration : 12559
train acc:  0.75
train loss:  0.47922375798225403
train gradient:  0.17440177728904016
iteration : 12560
train acc:  0.796875
train loss:  0.41583964228630066
train gradient:  0.08374159313113437
iteration : 12561
train acc:  0.7578125
train loss:  0.4721594750881195
train gradient:  0.12437670823581586
iteration : 12562
train acc:  0.7734375
train loss:  0.4960962235927582
train gradient:  0.11098957421393822
iteration : 12563
train acc:  0.75
train loss:  0.46301788091659546
train gradient:  0.11584602657481625
iteration : 12564
train acc:  0.7109375
train loss:  0.49101459980010986
train gradient:  0.10286494582253727
iteration : 12565
train acc:  0.7421875
train loss:  0.4741261303424835
train gradient:  0.13118116121629597
iteration : 12566
train acc:  0.7890625
train loss:  0.4374251067638397
train gradient:  0.10574563326398675
iteration : 12567
train acc:  0.6875
train loss:  0.5494576692581177
train gradient:  0.1712240605339877
iteration : 12568
train acc:  0.7578125
train loss:  0.5006935596466064
train gradient:  0.14266898004693263
iteration : 12569
train acc:  0.796875
train loss:  0.4532477855682373
train gradient:  0.1054015900896397
iteration : 12570
train acc:  0.765625
train loss:  0.4830339252948761
train gradient:  0.12317901581089628
iteration : 12571
train acc:  0.7421875
train loss:  0.4986788034439087
train gradient:  0.11870893047282897
iteration : 12572
train acc:  0.78125
train loss:  0.42127323150634766
train gradient:  0.08493861188734252
iteration : 12573
train acc:  0.765625
train loss:  0.46552732586860657
train gradient:  0.1088847754258299
iteration : 12574
train acc:  0.703125
train loss:  0.4688539505004883
train gradient:  0.09686705491226504
iteration : 12575
train acc:  0.6953125
train loss:  0.522409200668335
train gradient:  0.12026104239080736
iteration : 12576
train acc:  0.7421875
train loss:  0.5067273378372192
train gradient:  0.12311650976095961
iteration : 12577
train acc:  0.7421875
train loss:  0.44882524013519287
train gradient:  0.10142489623505822
iteration : 12578
train acc:  0.7734375
train loss:  0.45016446709632874
train gradient:  0.11733649992558072
iteration : 12579
train acc:  0.7109375
train loss:  0.5505853891372681
train gradient:  0.1403676867312525
iteration : 12580
train acc:  0.734375
train loss:  0.4711529314517975
train gradient:  0.09572981402788812
iteration : 12581
train acc:  0.71875
train loss:  0.5338314771652222
train gradient:  0.17656643273345385
iteration : 12582
train acc:  0.7109375
train loss:  0.5154603123664856
train gradient:  0.10429908458819656
iteration : 12583
train acc:  0.71875
train loss:  0.5046045184135437
train gradient:  0.12379109697190238
iteration : 12584
train acc:  0.7734375
train loss:  0.48991137742996216
train gradient:  0.12851197015614574
iteration : 12585
train acc:  0.7578125
train loss:  0.4855567514896393
train gradient:  0.1165860783782895
iteration : 12586
train acc:  0.71875
train loss:  0.49362680315971375
train gradient:  0.13494595124141612
iteration : 12587
train acc:  0.7578125
train loss:  0.4258715510368347
train gradient:  0.09794128308623007
iteration : 12588
train acc:  0.7421875
train loss:  0.5207846164703369
train gradient:  0.12278683658497479
iteration : 12589
train acc:  0.7890625
train loss:  0.4175424575805664
train gradient:  0.08243658566814967
iteration : 12590
train acc:  0.8125
train loss:  0.4374481439590454
train gradient:  0.10175051123149813
iteration : 12591
train acc:  0.7421875
train loss:  0.5015239715576172
train gradient:  0.14938005237871999
iteration : 12592
train acc:  0.7734375
train loss:  0.4544030427932739
train gradient:  0.0957005889200334
iteration : 12593
train acc:  0.796875
train loss:  0.458543598651886
train gradient:  0.10979722995186361
iteration : 12594
train acc:  0.75
train loss:  0.4824507534503937
train gradient:  0.1280062555706506
iteration : 12595
train acc:  0.7578125
train loss:  0.5141333341598511
train gradient:  0.11755170486345193
iteration : 12596
train acc:  0.6953125
train loss:  0.5527204275131226
train gradient:  0.16747676513731502
iteration : 12597
train acc:  0.7265625
train loss:  0.5201443433761597
train gradient:  0.15630074755043627
iteration : 12598
train acc:  0.78125
train loss:  0.4829552471637726
train gradient:  0.10373976128499388
iteration : 12599
train acc:  0.78125
train loss:  0.4361730217933655
train gradient:  0.09962054667107546
iteration : 12600
train acc:  0.78125
train loss:  0.4290667474269867
train gradient:  0.10166732214714935
iteration : 12601
train acc:  0.7421875
train loss:  0.5178592205047607
train gradient:  0.0991852893796611
iteration : 12602
train acc:  0.7265625
train loss:  0.5836145877838135
train gradient:  0.21266204054901922
iteration : 12603
train acc:  0.765625
train loss:  0.5121375322341919
train gradient:  0.12827954524440233
iteration : 12604
train acc:  0.7421875
train loss:  0.5023550391197205
train gradient:  0.12058624110540807
iteration : 12605
train acc:  0.6796875
train loss:  0.515810489654541
train gradient:  0.10984922876498941
iteration : 12606
train acc:  0.6640625
train loss:  0.5515852570533752
train gradient:  0.15092385918189477
iteration : 12607
train acc:  0.703125
train loss:  0.5378251075744629
train gradient:  0.17633901663922324
iteration : 12608
train acc:  0.7421875
train loss:  0.5729477405548096
train gradient:  0.14584416819973317
iteration : 12609
train acc:  0.7421875
train loss:  0.5081498622894287
train gradient:  0.10975819016972477
iteration : 12610
train acc:  0.703125
train loss:  0.5108040571212769
train gradient:  0.17153295230027898
iteration : 12611
train acc:  0.7578125
train loss:  0.47866612672805786
train gradient:  0.12452714070146215
iteration : 12612
train acc:  0.7890625
train loss:  0.48378223180770874
train gradient:  0.13126011757787914
iteration : 12613
train acc:  0.71875
train loss:  0.5338134169578552
train gradient:  0.1465054059955584
iteration : 12614
train acc:  0.6953125
train loss:  0.5128481984138489
train gradient:  0.12921238242686742
iteration : 12615
train acc:  0.765625
train loss:  0.4622090756893158
train gradient:  0.1167750718691733
iteration : 12616
train acc:  0.6171875
train loss:  0.6798099279403687
train gradient:  0.2806066560803581
iteration : 12617
train acc:  0.7578125
train loss:  0.5108331441879272
train gradient:  0.15410595476548367
iteration : 12618
train acc:  0.765625
train loss:  0.4267413318157196
train gradient:  0.09744386868550949
iteration : 12619
train acc:  0.765625
train loss:  0.5102361440658569
train gradient:  0.11553459457762313
iteration : 12620
train acc:  0.84375
train loss:  0.3956649899482727
train gradient:  0.07391685920011619
iteration : 12621
train acc:  0.7734375
train loss:  0.4463285207748413
train gradient:  0.11144637006846161
iteration : 12622
train acc:  0.75
train loss:  0.5178175568580627
train gradient:  0.11043081379568054
iteration : 12623
train acc:  0.7890625
train loss:  0.4710794985294342
train gradient:  0.09699907691543735
iteration : 12624
train acc:  0.7578125
train loss:  0.5146570205688477
train gradient:  0.12923055214442736
iteration : 12625
train acc:  0.8046875
train loss:  0.4574086666107178
train gradient:  0.10225474546893785
iteration : 12626
train acc:  0.78125
train loss:  0.5571283102035522
train gradient:  0.17700713855896835
iteration : 12627
train acc:  0.7421875
train loss:  0.5018892884254456
train gradient:  0.15036146196465228
iteration : 12628
train acc:  0.71875
train loss:  0.5112821459770203
train gradient:  0.09473078177288137
iteration : 12629
train acc:  0.7265625
train loss:  0.5172649025917053
train gradient:  0.13470725582693938
iteration : 12630
train acc:  0.8125
train loss:  0.4069133400917053
train gradient:  0.07629316510793442
iteration : 12631
train acc:  0.7109375
train loss:  0.5256897211074829
train gradient:  0.14212884947664464
iteration : 12632
train acc:  0.7265625
train loss:  0.5820668339729309
train gradient:  0.14790247904687398
iteration : 12633
train acc:  0.8046875
train loss:  0.4525231719017029
train gradient:  0.1295259159104904
iteration : 12634
train acc:  0.75
train loss:  0.4867347478866577
train gradient:  0.12646518840248633
iteration : 12635
train acc:  0.65625
train loss:  0.5758774876594543
train gradient:  0.14372130088255008
iteration : 12636
train acc:  0.7109375
train loss:  0.512682318687439
train gradient:  0.13323360554596364
iteration : 12637
train acc:  0.78125
train loss:  0.4727669954299927
train gradient:  0.10890703300664017
iteration : 12638
train acc:  0.7734375
train loss:  0.4642651081085205
train gradient:  0.08955128300532397
iteration : 12639
train acc:  0.765625
train loss:  0.4527648091316223
train gradient:  0.09855614737902804
iteration : 12640
train acc:  0.6953125
train loss:  0.5591058731079102
train gradient:  0.1557236292066039
iteration : 12641
train acc:  0.7734375
train loss:  0.447902113199234
train gradient:  0.0955441514507315
iteration : 12642
train acc:  0.78125
train loss:  0.46672898530960083
train gradient:  0.11191594145571673
iteration : 12643
train acc:  0.7421875
train loss:  0.47962668538093567
train gradient:  0.1042556724040206
iteration : 12644
train acc:  0.7265625
train loss:  0.5408424735069275
train gradient:  0.15640392768299818
iteration : 12645
train acc:  0.703125
train loss:  0.4735225439071655
train gradient:  0.12147704156063636
iteration : 12646
train acc:  0.765625
train loss:  0.4575957655906677
train gradient:  0.1300024614420202
iteration : 12647
train acc:  0.7890625
train loss:  0.4014655351638794
train gradient:  0.08955730790146345
iteration : 12648
train acc:  0.765625
train loss:  0.5122095942497253
train gradient:  0.09334832667481288
iteration : 12649
train acc:  0.796875
train loss:  0.41060999035835266
train gradient:  0.11535356336822021
iteration : 12650
train acc:  0.6875
train loss:  0.5437808036804199
train gradient:  0.12071603234789695
iteration : 12651
train acc:  0.734375
train loss:  0.45114371180534363
train gradient:  0.11450729937144791
iteration : 12652
train acc:  0.765625
train loss:  0.46001726388931274
train gradient:  0.10416353188267173
iteration : 12653
train acc:  0.7734375
train loss:  0.4610670506954193
train gradient:  0.13803034064112313
iteration : 12654
train acc:  0.7421875
train loss:  0.5362091064453125
train gradient:  0.1485627268685354
iteration : 12655
train acc:  0.7734375
train loss:  0.45937296748161316
train gradient:  0.10721610084738807
iteration : 12656
train acc:  0.7109375
train loss:  0.5164147615432739
train gradient:  0.18766089205668973
iteration : 12657
train acc:  0.7734375
train loss:  0.43709567189216614
train gradient:  0.0836290480258191
iteration : 12658
train acc:  0.65625
train loss:  0.4963488280773163
train gradient:  0.12488327533793614
iteration : 12659
train acc:  0.7578125
train loss:  0.45765915513038635
train gradient:  0.13713858498868037
iteration : 12660
train acc:  0.78125
train loss:  0.442931592464447
train gradient:  0.1244682555512335
iteration : 12661
train acc:  0.7734375
train loss:  0.4950762093067169
train gradient:  0.15359474829792885
iteration : 12662
train acc:  0.8203125
train loss:  0.4366946220397949
train gradient:  0.09438837091772298
iteration : 12663
train acc:  0.7421875
train loss:  0.48584049940109253
train gradient:  0.10797672293181548
iteration : 12664
train acc:  0.7109375
train loss:  0.5396248698234558
train gradient:  0.12716977995427933
iteration : 12665
train acc:  0.703125
train loss:  0.5640273094177246
train gradient:  0.13986357987221795
iteration : 12666
train acc:  0.828125
train loss:  0.42760196328163147
train gradient:  0.09167125944603487
iteration : 12667
train acc:  0.71875
train loss:  0.5360963344573975
train gradient:  0.14531499594398511
iteration : 12668
train acc:  0.8125
train loss:  0.46656322479248047
train gradient:  0.10802262444525415
iteration : 12669
train acc:  0.71875
train loss:  0.5557825565338135
train gradient:  0.15126585432590053
iteration : 12670
train acc:  0.7421875
train loss:  0.5012579560279846
train gradient:  0.1565390291783373
iteration : 12671
train acc:  0.734375
train loss:  0.47759538888931274
train gradient:  0.10548249771761027
iteration : 12672
train acc:  0.703125
train loss:  0.5682106018066406
train gradient:  0.1623333821431065
iteration : 12673
train acc:  0.7890625
train loss:  0.4875829219818115
train gradient:  0.116633572082226
iteration : 12674
train acc:  0.7265625
train loss:  0.5436064600944519
train gradient:  0.14346419712944983
iteration : 12675
train acc:  0.7890625
train loss:  0.4518669843673706
train gradient:  0.10458648800254841
iteration : 12676
train acc:  0.7890625
train loss:  0.43300342559814453
train gradient:  0.09276589960474806
iteration : 12677
train acc:  0.78125
train loss:  0.5343162417411804
train gradient:  0.13949205329240505
iteration : 12678
train acc:  0.7265625
train loss:  0.5344619750976562
train gradient:  0.11754495186009276
iteration : 12679
train acc:  0.75
train loss:  0.4796261191368103
train gradient:  0.09478346455191342
iteration : 12680
train acc:  0.7890625
train loss:  0.45504453778266907
train gradient:  0.09857823897288925
iteration : 12681
train acc:  0.7734375
train loss:  0.49491843581199646
train gradient:  0.1119929308507791
iteration : 12682
train acc:  0.7109375
train loss:  0.5406938791275024
train gradient:  0.1601268485443007
iteration : 12683
train acc:  0.7421875
train loss:  0.47679612040519714
train gradient:  0.10071336157518344
iteration : 12684
train acc:  0.7421875
train loss:  0.4712771773338318
train gradient:  0.10842101542480045
iteration : 12685
train acc:  0.796875
train loss:  0.4658307135105133
train gradient:  0.09174066765658402
iteration : 12686
train acc:  0.7578125
train loss:  0.49048423767089844
train gradient:  0.12128179132791156
iteration : 12687
train acc:  0.734375
train loss:  0.4769638776779175
train gradient:  0.09443907370784871
iteration : 12688
train acc:  0.71875
train loss:  0.5481553077697754
train gradient:  0.144625521531508
iteration : 12689
train acc:  0.75
train loss:  0.45414629578590393
train gradient:  0.0982455558629412
iteration : 12690
train acc:  0.765625
train loss:  0.48130863904953003
train gradient:  0.10773553757333343
iteration : 12691
train acc:  0.796875
train loss:  0.453834593296051
train gradient:  0.09645229615521855
iteration : 12692
train acc:  0.7265625
train loss:  0.4982469379901886
train gradient:  0.12440622702331848
iteration : 12693
train acc:  0.7734375
train loss:  0.5090891122817993
train gradient:  0.15827715166722328
iteration : 12694
train acc:  0.6953125
train loss:  0.5325772166252136
train gradient:  0.14843749643730272
iteration : 12695
train acc:  0.734375
train loss:  0.455960750579834
train gradient:  0.09217452870273019
iteration : 12696
train acc:  0.796875
train loss:  0.44104239344596863
train gradient:  0.0950278644977505
iteration : 12697
train acc:  0.75
train loss:  0.4739413559436798
train gradient:  0.13501007277807395
iteration : 12698
train acc:  0.7109375
train loss:  0.5080159902572632
train gradient:  0.12563595513147974
iteration : 12699
train acc:  0.703125
train loss:  0.514224648475647
train gradient:  0.18444307223702633
iteration : 12700
train acc:  0.71875
train loss:  0.47922489047050476
train gradient:  0.11106080780656692
iteration : 12701
train acc:  0.703125
train loss:  0.5881949663162231
train gradient:  0.18082224528288632
iteration : 12702
train acc:  0.765625
train loss:  0.4483027458190918
train gradient:  0.08966310367116306
iteration : 12703
train acc:  0.7265625
train loss:  0.4529103636741638
train gradient:  0.09000742100038095
iteration : 12704
train acc:  0.78125
train loss:  0.5075923204421997
train gradient:  0.13665263319203375
iteration : 12705
train acc:  0.7265625
train loss:  0.4932577610015869
train gradient:  0.12015271166170474
iteration : 12706
train acc:  0.6875
train loss:  0.5475431084632874
train gradient:  0.12837503519812263
iteration : 12707
train acc:  0.7109375
train loss:  0.5439903140068054
train gradient:  0.12510338943590998
iteration : 12708
train acc:  0.765625
train loss:  0.472251832485199
train gradient:  0.10423399998623792
iteration : 12709
train acc:  0.796875
train loss:  0.45909324288368225
train gradient:  0.08004092840008219
iteration : 12710
train acc:  0.7109375
train loss:  0.6005769968032837
train gradient:  0.18451048512493917
iteration : 12711
train acc:  0.8125
train loss:  0.4438331723213196
train gradient:  0.1209635496269697
iteration : 12712
train acc:  0.734375
train loss:  0.47386062145233154
train gradient:  0.10338517027220895
iteration : 12713
train acc:  0.7578125
train loss:  0.4901330769062042
train gradient:  0.12274131516334523
iteration : 12714
train acc:  0.7421875
train loss:  0.49623966217041016
train gradient:  0.12849802386431017
iteration : 12715
train acc:  0.7265625
train loss:  0.5020637512207031
train gradient:  0.1113001344874425
iteration : 12716
train acc:  0.7421875
train loss:  0.48853766918182373
train gradient:  0.11046975296947255
iteration : 12717
train acc:  0.7421875
train loss:  0.49820879101753235
train gradient:  0.10954678895461199
iteration : 12718
train acc:  0.78125
train loss:  0.4312177896499634
train gradient:  0.08726760614690235
iteration : 12719
train acc:  0.7734375
train loss:  0.4387722611427307
train gradient:  0.08563786704389269
iteration : 12720
train acc:  0.71875
train loss:  0.5096540451049805
train gradient:  0.11499463530775884
iteration : 12721
train acc:  0.78125
train loss:  0.4558078348636627
train gradient:  0.09042250310575724
iteration : 12722
train acc:  0.734375
train loss:  0.5088610649108887
train gradient:  0.13037961207315946
iteration : 12723
train acc:  0.7421875
train loss:  0.4530695974826813
train gradient:  0.1201421264397744
iteration : 12724
train acc:  0.78125
train loss:  0.4628322720527649
train gradient:  0.08484483081876415
iteration : 12725
train acc:  0.7265625
train loss:  0.5083745718002319
train gradient:  0.11554842405152488
iteration : 12726
train acc:  0.671875
train loss:  0.5834411382675171
train gradient:  0.17509009889281418
iteration : 12727
train acc:  0.7265625
train loss:  0.5238192677497864
train gradient:  0.142933482424828
iteration : 12728
train acc:  0.7265625
train loss:  0.48574966192245483
train gradient:  0.10292108794198876
iteration : 12729
train acc:  0.71875
train loss:  0.5102462768554688
train gradient:  0.11978656910987348
iteration : 12730
train acc:  0.734375
train loss:  0.4823530316352844
train gradient:  0.15222700371390863
iteration : 12731
train acc:  0.7265625
train loss:  0.4999859631061554
train gradient:  0.10757812107875547
iteration : 12732
train acc:  0.7109375
train loss:  0.5801102519035339
train gradient:  0.1593545195954461
iteration : 12733
train acc:  0.765625
train loss:  0.4218509793281555
train gradient:  0.10252273895639365
iteration : 12734
train acc:  0.7265625
train loss:  0.5067983865737915
train gradient:  0.11971400748629812
iteration : 12735
train acc:  0.734375
train loss:  0.5082495212554932
train gradient:  0.12485314399472498
iteration : 12736
train acc:  0.671875
train loss:  0.5154467821121216
train gradient:  0.12232965311073667
iteration : 12737
train acc:  0.7734375
train loss:  0.4478820860385895
train gradient:  0.10733231667527031
iteration : 12738
train acc:  0.7265625
train loss:  0.5134337544441223
train gradient:  0.1219870436236101
iteration : 12739
train acc:  0.8046875
train loss:  0.4935891330242157
train gradient:  0.1270458792425626
iteration : 12740
train acc:  0.7734375
train loss:  0.44366520643234253
train gradient:  0.08772055570331554
iteration : 12741
train acc:  0.7421875
train loss:  0.45624876022338867
train gradient:  0.08244069488988613
iteration : 12742
train acc:  0.765625
train loss:  0.45781639218330383
train gradient:  0.08914153770263974
iteration : 12743
train acc:  0.7578125
train loss:  0.45714935660362244
train gradient:  0.10797633725813933
iteration : 12744
train acc:  0.765625
train loss:  0.5398741364479065
train gradient:  0.22900650897181965
iteration : 12745
train acc:  0.7265625
train loss:  0.5347566604614258
train gradient:  0.13927189748319807
iteration : 12746
train acc:  0.7734375
train loss:  0.500861406326294
train gradient:  0.1313340872444513
iteration : 12747
train acc:  0.765625
train loss:  0.43913763761520386
train gradient:  0.10782630169015872
iteration : 12748
train acc:  0.6953125
train loss:  0.52805095911026
train gradient:  0.13017234916060777
iteration : 12749
train acc:  0.7265625
train loss:  0.47088006138801575
train gradient:  0.10763126650013895
iteration : 12750
train acc:  0.7421875
train loss:  0.5019896030426025
train gradient:  0.11343056360482576
iteration : 12751
train acc:  0.765625
train loss:  0.42001646757125854
train gradient:  0.10339848182358082
iteration : 12752
train acc:  0.8046875
train loss:  0.46172255277633667
train gradient:  0.0973001721117169
iteration : 12753
train acc:  0.71875
train loss:  0.5519548654556274
train gradient:  0.17253549886843866
iteration : 12754
train acc:  0.734375
train loss:  0.5329551100730896
train gradient:  0.14400285364366686
iteration : 12755
train acc:  0.7265625
train loss:  0.5285876393318176
train gradient:  0.15572389778513213
iteration : 12756
train acc:  0.71875
train loss:  0.5306539535522461
train gradient:  0.1415818729365127
iteration : 12757
train acc:  0.71875
train loss:  0.5127065181732178
train gradient:  0.1287370047755405
iteration : 12758
train acc:  0.6953125
train loss:  0.589743971824646
train gradient:  0.16980311828772163
iteration : 12759
train acc:  0.7265625
train loss:  0.49902889132499695
train gradient:  0.10770756297355781
iteration : 12760
train acc:  0.828125
train loss:  0.41172462701797485
train gradient:  0.10083309350503954
iteration : 12761
train acc:  0.6640625
train loss:  0.5151624083518982
train gradient:  0.1265437564592864
iteration : 12762
train acc:  0.6875
train loss:  0.5676907300949097
train gradient:  0.12815260141101387
iteration : 12763
train acc:  0.8046875
train loss:  0.491364985704422
train gradient:  0.10065436112626466
iteration : 12764
train acc:  0.65625
train loss:  0.6170852184295654
train gradient:  0.20482439178194234
iteration : 12765
train acc:  0.75
train loss:  0.5321857929229736
train gradient:  0.14895046024787056
iteration : 12766
train acc:  0.703125
train loss:  0.5333613157272339
train gradient:  0.15889996318754818
iteration : 12767
train acc:  0.7265625
train loss:  0.592008113861084
train gradient:  0.1676087943177701
iteration : 12768
train acc:  0.765625
train loss:  0.45061594247817993
train gradient:  0.10043024537497289
iteration : 12769
train acc:  0.71875
train loss:  0.5103058218955994
train gradient:  0.12072184411989323
iteration : 12770
train acc:  0.7421875
train loss:  0.5195533633232117
train gradient:  0.130679468057384
iteration : 12771
train acc:  0.75
train loss:  0.4784831702709198
train gradient:  0.09796265656384966
iteration : 12772
train acc:  0.75
train loss:  0.4812447726726532
train gradient:  0.12387236001800402
iteration : 12773
train acc:  0.71875
train loss:  0.4664129912853241
train gradient:  0.1103512984223856
iteration : 12774
train acc:  0.8046875
train loss:  0.43087223172187805
train gradient:  0.09517230110506336
iteration : 12775
train acc:  0.75
train loss:  0.4727182388305664
train gradient:  0.09230572708622432
iteration : 12776
train acc:  0.78125
train loss:  0.45816078782081604
train gradient:  0.11509851160411537
iteration : 12777
train acc:  0.7109375
train loss:  0.5314944982528687
train gradient:  0.12037064206520177
iteration : 12778
train acc:  0.75
train loss:  0.4481261372566223
train gradient:  0.09904880353316504
iteration : 12779
train acc:  0.796875
train loss:  0.4380926489830017
train gradient:  0.11185889725420921
iteration : 12780
train acc:  0.765625
train loss:  0.4450107514858246
train gradient:  0.1002028120050223
iteration : 12781
train acc:  0.7578125
train loss:  0.5099151134490967
train gradient:  0.11459079404956368
iteration : 12782
train acc:  0.71875
train loss:  0.5168477892875671
train gradient:  0.13996347806389936
iteration : 12783
train acc:  0.7890625
train loss:  0.4238978624343872
train gradient:  0.07682725958808385
iteration : 12784
train acc:  0.78125
train loss:  0.4453416168689728
train gradient:  0.12258508780457607
iteration : 12785
train acc:  0.6953125
train loss:  0.5496373176574707
train gradient:  0.15402822402992383
iteration : 12786
train acc:  0.6796875
train loss:  0.5095828771591187
train gradient:  0.12517343747832327
iteration : 12787
train acc:  0.6953125
train loss:  0.5513904690742493
train gradient:  0.15364917910491338
iteration : 12788
train acc:  0.765625
train loss:  0.42223232984542847
train gradient:  0.11211179740670038
iteration : 12789
train acc:  0.8125
train loss:  0.4750409722328186
train gradient:  0.11440304312312694
iteration : 12790
train acc:  0.7109375
train loss:  0.5552321672439575
train gradient:  0.14142714341115586
iteration : 12791
train acc:  0.78125
train loss:  0.45616430044174194
train gradient:  0.09945390092612888
iteration : 12792
train acc:  0.7265625
train loss:  0.46687906980514526
train gradient:  0.12840166081752083
iteration : 12793
train acc:  0.765625
train loss:  0.4344491958618164
train gradient:  0.08523660051181786
iteration : 12794
train acc:  0.7734375
train loss:  0.47982752323150635
train gradient:  0.1153304601957404
iteration : 12795
train acc:  0.765625
train loss:  0.42533808946609497
train gradient:  0.0993584197743876
iteration : 12796
train acc:  0.8125
train loss:  0.4846700429916382
train gradient:  0.14701452186534003
iteration : 12797
train acc:  0.796875
train loss:  0.4663204550743103
train gradient:  0.1199836492011006
iteration : 12798
train acc:  0.7890625
train loss:  0.46475228667259216
train gradient:  0.12674405488471707
iteration : 12799
train acc:  0.8359375
train loss:  0.4195418655872345
train gradient:  0.10585942644407463
iteration : 12800
train acc:  0.7421875
train loss:  0.5176084041595459
train gradient:  0.12944950567829022
iteration : 12801
train acc:  0.71875
train loss:  0.4963586926460266
train gradient:  0.11556438963735209
iteration : 12802
train acc:  0.734375
train loss:  0.509341835975647
train gradient:  0.13785877331316793
iteration : 12803
train acc:  0.7421875
train loss:  0.5274474620819092
train gradient:  0.18625994269247287
iteration : 12804
train acc:  0.765625
train loss:  0.4370163381099701
train gradient:  0.08244277262671136
iteration : 12805
train acc:  0.7578125
train loss:  0.5277182459831238
train gradient:  0.12322305288771299
iteration : 12806
train acc:  0.640625
train loss:  0.5478838682174683
train gradient:  0.13201843003108413
iteration : 12807
train acc:  0.7109375
train loss:  0.5922935009002686
train gradient:  0.15944441235232926
iteration : 12808
train acc:  0.7421875
train loss:  0.5071601271629333
train gradient:  0.12057813810179431
iteration : 12809
train acc:  0.734375
train loss:  0.5095292329788208
train gradient:  0.12030431074260252
iteration : 12810
train acc:  0.765625
train loss:  0.5071460008621216
train gradient:  0.138163831750772
iteration : 12811
train acc:  0.6875
train loss:  0.5291388034820557
train gradient:  0.11089343043683433
iteration : 12812
train acc:  0.7109375
train loss:  0.5689903497695923
train gradient:  0.1581533760259946
iteration : 12813
train acc:  0.734375
train loss:  0.5258329510688782
train gradient:  0.13379890515036538
iteration : 12814
train acc:  0.6875
train loss:  0.5672442317008972
train gradient:  0.13970956094100134
iteration : 12815
train acc:  0.7578125
train loss:  0.4736552834510803
train gradient:  0.10251442863018613
iteration : 12816
train acc:  0.6953125
train loss:  0.5212810039520264
train gradient:  0.13444603164474533
iteration : 12817
train acc:  0.7578125
train loss:  0.4815642833709717
train gradient:  0.1229420075125085
iteration : 12818
train acc:  0.75
train loss:  0.4288068413734436
train gradient:  0.08911590568297917
iteration : 12819
train acc:  0.7109375
train loss:  0.4295896589756012
train gradient:  0.09967221868007939
iteration : 12820
train acc:  0.7890625
train loss:  0.43838468194007874
train gradient:  0.10563846060905507
iteration : 12821
train acc:  0.7578125
train loss:  0.518118143081665
train gradient:  0.1076333897396954
iteration : 12822
train acc:  0.7421875
train loss:  0.5124046206474304
train gradient:  0.12142096842376862
iteration : 12823
train acc:  0.734375
train loss:  0.4712201654911041
train gradient:  0.11213928082981096
iteration : 12824
train acc:  0.671875
train loss:  0.5472147464752197
train gradient:  0.15681826339276778
iteration : 12825
train acc:  0.78125
train loss:  0.4742318093776703
train gradient:  0.17571221691171152
iteration : 12826
train acc:  0.7890625
train loss:  0.4241352081298828
train gradient:  0.09131806514035182
iteration : 12827
train acc:  0.703125
train loss:  0.5404667854309082
train gradient:  0.13001771008470683
iteration : 12828
train acc:  0.78125
train loss:  0.43635839223861694
train gradient:  0.09933587653639032
iteration : 12829
train acc:  0.8046875
train loss:  0.4118131995201111
train gradient:  0.09234907291170719
iteration : 12830
train acc:  0.78125
train loss:  0.44062286615371704
train gradient:  0.09494004037575934
iteration : 12831
train acc:  0.75
train loss:  0.4614478349685669
train gradient:  0.10190485037040647
iteration : 12832
train acc:  0.7890625
train loss:  0.41933032870292664
train gradient:  0.09491342915818522
iteration : 12833
train acc:  0.734375
train loss:  0.5100430250167847
train gradient:  0.13598673711537376
iteration : 12834
train acc:  0.6875
train loss:  0.5633676052093506
train gradient:  0.13289075396089525
iteration : 12835
train acc:  0.734375
train loss:  0.49140679836273193
train gradient:  0.11031261362011131
iteration : 12836
train acc:  0.7578125
train loss:  0.4500271677970886
train gradient:  0.08806879653189348
iteration : 12837
train acc:  0.7578125
train loss:  0.48705044388771057
train gradient:  0.10452509498068023
iteration : 12838
train acc:  0.671875
train loss:  0.5850198268890381
train gradient:  0.1778186463737798
iteration : 12839
train acc:  0.7734375
train loss:  0.5055062174797058
train gradient:  0.15071215229896695
iteration : 12840
train acc:  0.765625
train loss:  0.5039730668067932
train gradient:  0.1309365087775374
iteration : 12841
train acc:  0.7734375
train loss:  0.44971922039985657
train gradient:  0.09155484123162222
iteration : 12842
train acc:  0.8125
train loss:  0.425510972738266
train gradient:  0.11476362043747057
iteration : 12843
train acc:  0.7421875
train loss:  0.5254177451133728
train gradient:  0.1463606106004624
iteration : 12844
train acc:  0.765625
train loss:  0.5241747498512268
train gradient:  0.14859339999334287
iteration : 12845
train acc:  0.7734375
train loss:  0.4408224821090698
train gradient:  0.09113320371838507
iteration : 12846
train acc:  0.7734375
train loss:  0.47154325246810913
train gradient:  0.09053449787176258
iteration : 12847
train acc:  0.796875
train loss:  0.4817236661911011
train gradient:  0.11912183202216114
iteration : 12848
train acc:  0.75
train loss:  0.44375547766685486
train gradient:  0.10755189240878288
iteration : 12849
train acc:  0.765625
train loss:  0.4500674605369568
train gradient:  0.09474817459552141
iteration : 12850
train acc:  0.703125
train loss:  0.5084958076477051
train gradient:  0.11833564868820054
iteration : 12851
train acc:  0.7578125
train loss:  0.4382551312446594
train gradient:  0.1062554861894466
iteration : 12852
train acc:  0.7890625
train loss:  0.4410286247730255
train gradient:  0.10986897337349628
iteration : 12853
train acc:  0.8125
train loss:  0.3869708180427551
train gradient:  0.08181174010795891
iteration : 12854
train acc:  0.75
train loss:  0.49396973848342896
train gradient:  0.09082857499618197
iteration : 12855
train acc:  0.75
train loss:  0.4822810888290405
train gradient:  0.11555802498524027
iteration : 12856
train acc:  0.7265625
train loss:  0.49141883850097656
train gradient:  0.10427513409907228
iteration : 12857
train acc:  0.703125
train loss:  0.5420821309089661
train gradient:  0.13494828685975252
iteration : 12858
train acc:  0.78125
train loss:  0.4695158004760742
train gradient:  0.10668667260879379
iteration : 12859
train acc:  0.7578125
train loss:  0.45834046602249146
train gradient:  0.09249841700909321
iteration : 12860
train acc:  0.7734375
train loss:  0.44069379568099976
train gradient:  0.09070097177133932
iteration : 12861
train acc:  0.7578125
train loss:  0.48169711232185364
train gradient:  0.14501477579955374
iteration : 12862
train acc:  0.6875
train loss:  0.516165018081665
train gradient:  0.16622684162512358
iteration : 12863
train acc:  0.8359375
train loss:  0.3652476370334625
train gradient:  0.1123908759003483
iteration : 12864
train acc:  0.6953125
train loss:  0.5699396729469299
train gradient:  0.1570700018288649
iteration : 12865
train acc:  0.7578125
train loss:  0.4720349609851837
train gradient:  0.10487508488266227
iteration : 12866
train acc:  0.65625
train loss:  0.5656660199165344
train gradient:  0.16184787013281193
iteration : 12867
train acc:  0.796875
train loss:  0.4478641748428345
train gradient:  0.10881858912672386
iteration : 12868
train acc:  0.71875
train loss:  0.5087758302688599
train gradient:  0.11848202172941759
iteration : 12869
train acc:  0.7734375
train loss:  0.4447806477546692
train gradient:  0.10498453159348958
iteration : 12870
train acc:  0.8046875
train loss:  0.4503439664840698
train gradient:  0.09544096520781392
iteration : 12871
train acc:  0.8203125
train loss:  0.42635536193847656
train gradient:  0.10259155057712563
iteration : 12872
train acc:  0.71875
train loss:  0.4616418480873108
train gradient:  0.10045851349712105
iteration : 12873
train acc:  0.7890625
train loss:  0.4171109199523926
train gradient:  0.08439572659515718
iteration : 12874
train acc:  0.7109375
train loss:  0.5194750428199768
train gradient:  0.12283359318207579
iteration : 12875
train acc:  0.7421875
train loss:  0.531984806060791
train gradient:  0.14586991881333564
iteration : 12876
train acc:  0.7421875
train loss:  0.468942791223526
train gradient:  0.10284649995770997
iteration : 12877
train acc:  0.78125
train loss:  0.486339807510376
train gradient:  0.10876698107182321
iteration : 12878
train acc:  0.75
train loss:  0.4573270082473755
train gradient:  0.09030833311376009
iteration : 12879
train acc:  0.7578125
train loss:  0.4657754898071289
train gradient:  0.10762638799312461
iteration : 12880
train acc:  0.7734375
train loss:  0.47046124935150146
train gradient:  0.1093423663249295
iteration : 12881
train acc:  0.75
train loss:  0.4736902415752411
train gradient:  0.13308082699660634
iteration : 12882
train acc:  0.7109375
train loss:  0.5477105975151062
train gradient:  0.15067211309004996
iteration : 12883
train acc:  0.7265625
train loss:  0.5078631639480591
train gradient:  0.13948675879293926
iteration : 12884
train acc:  0.75
train loss:  0.5254820585250854
train gradient:  0.13208594367320098
iteration : 12885
train acc:  0.703125
train loss:  0.5531406402587891
train gradient:  0.12451154042704819
iteration : 12886
train acc:  0.7734375
train loss:  0.4407496452331543
train gradient:  0.11645824098609299
iteration : 12887
train acc:  0.78125
train loss:  0.4791340231895447
train gradient:  0.10817973398033771
iteration : 12888
train acc:  0.734375
train loss:  0.4856145977973938
train gradient:  0.13070132337187107
iteration : 12889
train acc:  0.765625
train loss:  0.45154789090156555
train gradient:  0.10088110979799399
iteration : 12890
train acc:  0.765625
train loss:  0.49129217863082886
train gradient:  0.12258394159580742
iteration : 12891
train acc:  0.75
train loss:  0.49162954092025757
train gradient:  0.15278396581692322
iteration : 12892
train acc:  0.6484375
train loss:  0.6090738773345947
train gradient:  0.18551134749935952
iteration : 12893
train acc:  0.765625
train loss:  0.4870862364768982
train gradient:  0.10818122788130864
iteration : 12894
train acc:  0.7109375
train loss:  0.5293425917625427
train gradient:  0.13464815585323608
iteration : 12895
train acc:  0.7734375
train loss:  0.4449644386768341
train gradient:  0.10945220079109155
iteration : 12896
train acc:  0.75
train loss:  0.47893089056015015
train gradient:  0.1322859667573653
iteration : 12897
train acc:  0.8046875
train loss:  0.4293232262134552
train gradient:  0.09278621098509257
iteration : 12898
train acc:  0.6875
train loss:  0.5216881632804871
train gradient:  0.15047363470876135
iteration : 12899
train acc:  0.734375
train loss:  0.47643572092056274
train gradient:  0.10275879420920951
iteration : 12900
train acc:  0.7578125
train loss:  0.4639161229133606
train gradient:  0.12990706845425207
iteration : 12901
train acc:  0.7265625
train loss:  0.47274115681648254
train gradient:  0.13193749350952866
iteration : 12902
train acc:  0.7578125
train loss:  0.4612286686897278
train gradient:  0.10176370728854296
iteration : 12903
train acc:  0.7421875
train loss:  0.4685969054698944
train gradient:  0.1257861122990056
iteration : 12904
train acc:  0.7734375
train loss:  0.48969709873199463
train gradient:  0.13666173188157243
iteration : 12905
train acc:  0.7890625
train loss:  0.4251552224159241
train gradient:  0.07943763447377725
iteration : 12906
train acc:  0.765625
train loss:  0.4526239037513733
train gradient:  0.09309553863232352
iteration : 12907
train acc:  0.7265625
train loss:  0.5092138051986694
train gradient:  0.11540996128549717
iteration : 12908
train acc:  0.7109375
train loss:  0.4983706474304199
train gradient:  0.1361793098048766
iteration : 12909
train acc:  0.75
train loss:  0.48468565940856934
train gradient:  0.12609352959607067
iteration : 12910
train acc:  0.84375
train loss:  0.39856231212615967
train gradient:  0.08558926255272951
iteration : 12911
train acc:  0.703125
train loss:  0.4917452037334442
train gradient:  0.10619847636871602
iteration : 12912
train acc:  0.7265625
train loss:  0.4974014163017273
train gradient:  0.10425952738522883
iteration : 12913
train acc:  0.71875
train loss:  0.5086770057678223
train gradient:  0.13666446149743142
iteration : 12914
train acc:  0.7421875
train loss:  0.4494234621524811
train gradient:  0.07510205628017788
iteration : 12915
train acc:  0.7421875
train loss:  0.5055469870567322
train gradient:  0.13203754045605182
iteration : 12916
train acc:  0.6953125
train loss:  0.5506253838539124
train gradient:  0.13343351112525392
iteration : 12917
train acc:  0.7265625
train loss:  0.4938426613807678
train gradient:  0.1097147136866885
iteration : 12918
train acc:  0.7734375
train loss:  0.45897573232650757
train gradient:  0.11125018300348297
iteration : 12919
train acc:  0.703125
train loss:  0.5245658159255981
train gradient:  0.14459603388943415
iteration : 12920
train acc:  0.7578125
train loss:  0.4413420855998993
train gradient:  0.09686135753855996
iteration : 12921
train acc:  0.7578125
train loss:  0.46744775772094727
train gradient:  0.10932829837343513
iteration : 12922
train acc:  0.734375
train loss:  0.5276755094528198
train gradient:  0.16001787880061685
iteration : 12923
train acc:  0.7734375
train loss:  0.46434804797172546
train gradient:  0.10559072479020958
iteration : 12924
train acc:  0.7734375
train loss:  0.4659290313720703
train gradient:  0.12785900576712322
iteration : 12925
train acc:  0.7734375
train loss:  0.5050119757652283
train gradient:  0.12239214861935514
iteration : 12926
train acc:  0.8203125
train loss:  0.38641658425331116
train gradient:  0.08057121430245728
iteration : 12927
train acc:  0.796875
train loss:  0.4531603753566742
train gradient:  0.10856044518508325
iteration : 12928
train acc:  0.7421875
train loss:  0.5334855318069458
train gradient:  0.12381923822172279
iteration : 12929
train acc:  0.7578125
train loss:  0.4329442083835602
train gradient:  0.0892360419580964
iteration : 12930
train acc:  0.8125
train loss:  0.4172520041465759
train gradient:  0.10447128934906225
iteration : 12931
train acc:  0.7109375
train loss:  0.506039023399353
train gradient:  0.12561130935554132
iteration : 12932
train acc:  0.7578125
train loss:  0.4548569917678833
train gradient:  0.11301818848636196
iteration : 12933
train acc:  0.78125
train loss:  0.435889333486557
train gradient:  0.08672820446859203
iteration : 12934
train acc:  0.7578125
train loss:  0.4666813611984253
train gradient:  0.11259802192600706
iteration : 12935
train acc:  0.75
train loss:  0.49648135900497437
train gradient:  0.12306397915816596
iteration : 12936
train acc:  0.65625
train loss:  0.5317189693450928
train gradient:  0.1290197259950413
iteration : 12937
train acc:  0.7109375
train loss:  0.5820567607879639
train gradient:  0.2054379857177126
iteration : 12938
train acc:  0.75
train loss:  0.4705585837364197
train gradient:  0.12428444489018547
iteration : 12939
train acc:  0.7421875
train loss:  0.4689297676086426
train gradient:  0.11877858852337125
iteration : 12940
train acc:  0.6953125
train loss:  0.5644822120666504
train gradient:  0.14631231415932716
iteration : 12941
train acc:  0.7578125
train loss:  0.508362889289856
train gradient:  0.1380139654870734
iteration : 12942
train acc:  0.78125
train loss:  0.46748071908950806
train gradient:  0.10947600515594955
iteration : 12943
train acc:  0.71875
train loss:  0.5039901733398438
train gradient:  0.09819864996398946
iteration : 12944
train acc:  0.7109375
train loss:  0.5413321852684021
train gradient:  0.15417853036842527
iteration : 12945
train acc:  0.75
train loss:  0.4730110764503479
train gradient:  0.10387582464406153
iteration : 12946
train acc:  0.7890625
train loss:  0.5318353772163391
train gradient:  0.14181547123312027
iteration : 12947
train acc:  0.75
train loss:  0.48073750734329224
train gradient:  0.11783870025925029
iteration : 12948
train acc:  0.6875
train loss:  0.5436099171638489
train gradient:  0.15276677710530132
iteration : 12949
train acc:  0.7421875
train loss:  0.5168078541755676
train gradient:  0.13312251215137189
iteration : 12950
train acc:  0.6796875
train loss:  0.5780701637268066
train gradient:  0.1936047955616631
iteration : 12951
train acc:  0.8203125
train loss:  0.3959689736366272
train gradient:  0.08014449378065203
iteration : 12952
train acc:  0.7265625
train loss:  0.536339521408081
train gradient:  0.17262359224367707
iteration : 12953
train acc:  0.71875
train loss:  0.5304542779922485
train gradient:  0.1218610791978962
iteration : 12954
train acc:  0.78125
train loss:  0.4422323703765869
train gradient:  0.08986910553893136
iteration : 12955
train acc:  0.7890625
train loss:  0.4939344525337219
train gradient:  0.12944431338827783
iteration : 12956
train acc:  0.75
train loss:  0.495219886302948
train gradient:  0.15167252079218246
iteration : 12957
train acc:  0.7421875
train loss:  0.5575478076934814
train gradient:  0.1274517934497153
iteration : 12958
train acc:  0.734375
train loss:  0.5545265078544617
train gradient:  0.1571587768490107
iteration : 12959
train acc:  0.7734375
train loss:  0.48391294479370117
train gradient:  0.13235703178964592
iteration : 12960
train acc:  0.796875
train loss:  0.4405594766139984
train gradient:  0.10029083901905445
iteration : 12961
train acc:  0.765625
train loss:  0.5350892543792725
train gradient:  0.16344606575916715
iteration : 12962
train acc:  0.703125
train loss:  0.5097353458404541
train gradient:  0.13635616716302826
iteration : 12963
train acc:  0.8359375
train loss:  0.3717714548110962
train gradient:  0.07615362333646238
iteration : 12964
train acc:  0.703125
train loss:  0.5804975032806396
train gradient:  0.1599835396885924
iteration : 12965
train acc:  0.8046875
train loss:  0.4615050256252289
train gradient:  0.11437901685382423
iteration : 12966
train acc:  0.7265625
train loss:  0.4834096133708954
train gradient:  0.1258400378832925
iteration : 12967
train acc:  0.7265625
train loss:  0.4940740466117859
train gradient:  0.12554998747525037
iteration : 12968
train acc:  0.7734375
train loss:  0.42776286602020264
train gradient:  0.0976026748999852
iteration : 12969
train acc:  0.7421875
train loss:  0.5131632685661316
train gradient:  0.11756496139648451
iteration : 12970
train acc:  0.7890625
train loss:  0.41629558801651
train gradient:  0.10496552203269133
iteration : 12971
train acc:  0.75
train loss:  0.5692180395126343
train gradient:  0.14050996011050612
iteration : 12972
train acc:  0.8359375
train loss:  0.4864598512649536
train gradient:  0.10158030331419339
iteration : 12973
train acc:  0.75
train loss:  0.47172072529792786
train gradient:  0.11487257559967512
iteration : 12974
train acc:  0.75
train loss:  0.5382814407348633
train gradient:  0.15456471640919467
iteration : 12975
train acc:  0.7421875
train loss:  0.4553129971027374
train gradient:  0.09612042235464933
iteration : 12976
train acc:  0.71875
train loss:  0.5343412160873413
train gradient:  0.14750526977780948
iteration : 12977
train acc:  0.703125
train loss:  0.554594099521637
train gradient:  0.15135082512045228
iteration : 12978
train acc:  0.7265625
train loss:  0.4516720175743103
train gradient:  0.10039105104428499
iteration : 12979
train acc:  0.7734375
train loss:  0.46129512786865234
train gradient:  0.13390090603218108
iteration : 12980
train acc:  0.8203125
train loss:  0.4579417109489441
train gradient:  0.10091959477576039
iteration : 12981
train acc:  0.7578125
train loss:  0.504470944404602
train gradient:  0.1441714108497832
iteration : 12982
train acc:  0.6953125
train loss:  0.5283280611038208
train gradient:  0.1305477082446106
iteration : 12983
train acc:  0.796875
train loss:  0.4082566499710083
train gradient:  0.08979940357290093
iteration : 12984
train acc:  0.7734375
train loss:  0.49656105041503906
train gradient:  0.12442372022662707
iteration : 12985
train acc:  0.78125
train loss:  0.5187715291976929
train gradient:  0.13075471055671173
iteration : 12986
train acc:  0.75
train loss:  0.507516086101532
train gradient:  0.12216137855671583
iteration : 12987
train acc:  0.78125
train loss:  0.4950760006904602
train gradient:  0.09829847929805963
iteration : 12988
train acc:  0.6953125
train loss:  0.4896148443222046
train gradient:  0.12441938690029936
iteration : 12989
train acc:  0.7421875
train loss:  0.477779746055603
train gradient:  0.11271394813552602
iteration : 12990
train acc:  0.75
train loss:  0.4739668369293213
train gradient:  0.133252314891421
iteration : 12991
train acc:  0.7578125
train loss:  0.49683356285095215
train gradient:  0.11740403871570051
iteration : 12992
train acc:  0.7421875
train loss:  0.49068474769592285
train gradient:  0.09477202684261272
iteration : 12993
train acc:  0.7265625
train loss:  0.4641388952732086
train gradient:  0.10900358798353893
iteration : 12994
train acc:  0.703125
train loss:  0.5028338432312012
train gradient:  0.13126685013691136
iteration : 12995
train acc:  0.7890625
train loss:  0.4076095223426819
train gradient:  0.11101872297963862
iteration : 12996
train acc:  0.7421875
train loss:  0.5080751776695251
train gradient:  0.13644918152977492
iteration : 12997
train acc:  0.78125
train loss:  0.453646719455719
train gradient:  0.1036057311203513
iteration : 12998
train acc:  0.734375
train loss:  0.5192052125930786
train gradient:  0.11013780159870046
iteration : 12999
train acc:  0.6796875
train loss:  0.5960702300071716
train gradient:  0.1312129786251799
iteration : 13000
train acc:  0.7734375
train loss:  0.452431857585907
train gradient:  0.10303996556409747
iteration : 13001
train acc:  0.765625
train loss:  0.47100716829299927
train gradient:  0.10932376772685087
iteration : 13002
train acc:  0.7265625
train loss:  0.5397158265113831
train gradient:  0.16066586536952915
iteration : 13003
train acc:  0.6796875
train loss:  0.5162503719329834
train gradient:  0.1457821171319003
iteration : 13004
train acc:  0.75
train loss:  0.46621209383010864
train gradient:  0.10063287787652869
iteration : 13005
train acc:  0.71875
train loss:  0.4898223578929901
train gradient:  0.11081506634538185
iteration : 13006
train acc:  0.71875
train loss:  0.5329163670539856
train gradient:  0.15838051193569883
iteration : 13007
train acc:  0.6875
train loss:  0.564335823059082
train gradient:  0.1345701688515099
iteration : 13008
train acc:  0.7421875
train loss:  0.48201239109039307
train gradient:  0.1211209739706211
iteration : 13009
train acc:  0.765625
train loss:  0.4927450120449066
train gradient:  0.1067198776366532
iteration : 13010
train acc:  0.8125
train loss:  0.42589515447616577
train gradient:  0.10592628271671385
iteration : 13011
train acc:  0.7890625
train loss:  0.4242345690727234
train gradient:  0.08254832698275903
iteration : 13012
train acc:  0.7890625
train loss:  0.4398937225341797
train gradient:  0.13001559948369334
iteration : 13013
train acc:  0.75
train loss:  0.4702439308166504
train gradient:  0.09884782692365543
iteration : 13014
train acc:  0.6875
train loss:  0.5474594831466675
train gradient:  0.20512940247431122
iteration : 13015
train acc:  0.78125
train loss:  0.467492938041687
train gradient:  0.10539644356297546
iteration : 13016
train acc:  0.734375
train loss:  0.45109474658966064
train gradient:  0.09046766385104105
iteration : 13017
train acc:  0.7578125
train loss:  0.4721841514110565
train gradient:  0.10560419281581176
iteration : 13018
train acc:  0.7109375
train loss:  0.5108748078346252
train gradient:  0.12968017391069675
iteration : 13019
train acc:  0.6640625
train loss:  0.578709602355957
train gradient:  0.1331042314770996
iteration : 13020
train acc:  0.7734375
train loss:  0.4711647629737854
train gradient:  0.09532485211818262
iteration : 13021
train acc:  0.78125
train loss:  0.4591060280799866
train gradient:  0.11286882832090853
iteration : 13022
train acc:  0.7734375
train loss:  0.4691823124885559
train gradient:  0.12393196971699191
iteration : 13023
train acc:  0.7109375
train loss:  0.47072848677635193
train gradient:  0.10744646787713999
iteration : 13024
train acc:  0.6796875
train loss:  0.5981534719467163
train gradient:  0.16916949369656042
iteration : 13025
train acc:  0.7734375
train loss:  0.5055184364318848
train gradient:  0.14947090256756362
iteration : 13026
train acc:  0.7734375
train loss:  0.4266919195652008
train gradient:  0.09609170807191005
iteration : 13027
train acc:  0.7109375
train loss:  0.5183428525924683
train gradient:  0.10704685727894984
iteration : 13028
train acc:  0.765625
train loss:  0.49176692962646484
train gradient:  0.12149126855185322
iteration : 13029
train acc:  0.7578125
train loss:  0.5018965005874634
train gradient:  0.15342318503531976
iteration : 13030
train acc:  0.71875
train loss:  0.5237886309623718
train gradient:  0.11183155268232678
iteration : 13031
train acc:  0.7421875
train loss:  0.5047900080680847
train gradient:  0.1344803856111248
iteration : 13032
train acc:  0.75
train loss:  0.4724089205265045
train gradient:  0.10029916048782109
iteration : 13033
train acc:  0.65625
train loss:  0.5818353891372681
train gradient:  0.14137024122555136
iteration : 13034
train acc:  0.796875
train loss:  0.4238607883453369
train gradient:  0.09533992358361444
iteration : 13035
train acc:  0.6953125
train loss:  0.5110872983932495
train gradient:  0.13276488839472866
iteration : 13036
train acc:  0.7421875
train loss:  0.5002220273017883
train gradient:  0.10993398868662378
iteration : 13037
train acc:  0.8046875
train loss:  0.410785973072052
train gradient:  0.0790082492161182
iteration : 13038
train acc:  0.75
train loss:  0.49411049485206604
train gradient:  0.11116180429486239
iteration : 13039
train acc:  0.734375
train loss:  0.4917454123497009
train gradient:  0.13371936758299108
iteration : 13040
train acc:  0.7109375
train loss:  0.5229300260543823
train gradient:  0.1391936711064027
iteration : 13041
train acc:  0.765625
train loss:  0.45275115966796875
train gradient:  0.0892058931280538
iteration : 13042
train acc:  0.71875
train loss:  0.5422626733779907
train gradient:  0.12085265955612885
iteration : 13043
train acc:  0.7109375
train loss:  0.5203588008880615
train gradient:  0.12112559061461493
iteration : 13044
train acc:  0.7578125
train loss:  0.5391613245010376
train gradient:  0.1162655182874533
iteration : 13045
train acc:  0.7109375
train loss:  0.509856104850769
train gradient:  0.1341235727745475
iteration : 13046
train acc:  0.7265625
train loss:  0.4982374906539917
train gradient:  0.13572158747561383
iteration : 13047
train acc:  0.7265625
train loss:  0.5274248123168945
train gradient:  0.14076172442990864
iteration : 13048
train acc:  0.75
train loss:  0.44881707429885864
train gradient:  0.09605847063120809
iteration : 13049
train acc:  0.7734375
train loss:  0.45113372802734375
train gradient:  0.11176035944614865
iteration : 13050
train acc:  0.7734375
train loss:  0.5231563448905945
train gradient:  0.13701174636279997
iteration : 13051
train acc:  0.6953125
train loss:  0.4937390983104706
train gradient:  0.12391681055581871
iteration : 13052
train acc:  0.75
train loss:  0.5109330415725708
train gradient:  0.1357909114448657
iteration : 13053
train acc:  0.75
train loss:  0.46738672256469727
train gradient:  0.08205545818420976
iteration : 13054
train acc:  0.7890625
train loss:  0.4553086459636688
train gradient:  0.12375399191488042
iteration : 13055
train acc:  0.75
train loss:  0.48441803455352783
train gradient:  0.1075104142487381
iteration : 13056
train acc:  0.734375
train loss:  0.5089179277420044
train gradient:  0.09496256456549315
iteration : 13057
train acc:  0.7890625
train loss:  0.4955088496208191
train gradient:  0.16224315094889696
iteration : 13058
train acc:  0.765625
train loss:  0.5047448873519897
train gradient:  0.10830532450294916
iteration : 13059
train acc:  0.8125
train loss:  0.4013013243675232
train gradient:  0.10158374292317061
iteration : 13060
train acc:  0.6796875
train loss:  0.524253249168396
train gradient:  0.10793401490404952
iteration : 13061
train acc:  0.7421875
train loss:  0.4732500910758972
train gradient:  0.12984428668026238
iteration : 13062
train acc:  0.7109375
train loss:  0.5410345792770386
train gradient:  0.16199784338801487
iteration : 13063
train acc:  0.7578125
train loss:  0.5021008849143982
train gradient:  0.14270505467781996
iteration : 13064
train acc:  0.7734375
train loss:  0.4599096179008484
train gradient:  0.11020302129927939
iteration : 13065
train acc:  0.671875
train loss:  0.5171552300453186
train gradient:  0.13727176282401882
iteration : 13066
train acc:  0.734375
train loss:  0.48127585649490356
train gradient:  0.10390341447003248
iteration : 13067
train acc:  0.7578125
train loss:  0.5165059566497803
train gradient:  0.13924762328094964
iteration : 13068
train acc:  0.7109375
train loss:  0.5226525664329529
train gradient:  0.11656341999764948
iteration : 13069
train acc:  0.71875
train loss:  0.519019365310669
train gradient:  0.13898502127471093
iteration : 13070
train acc:  0.765625
train loss:  0.4751957058906555
train gradient:  0.10194600647143938
iteration : 13071
train acc:  0.7734375
train loss:  0.47966310381889343
train gradient:  0.12154587967207664
iteration : 13072
train acc:  0.7734375
train loss:  0.4406934678554535
train gradient:  0.09695009303291025
iteration : 13073
train acc:  0.8046875
train loss:  0.42255109548568726
train gradient:  0.08842911137679695
iteration : 13074
train acc:  0.8203125
train loss:  0.41936978697776794
train gradient:  0.10320333181026892
iteration : 13075
train acc:  0.7109375
train loss:  0.47774291038513184
train gradient:  0.13935596193197114
iteration : 13076
train acc:  0.7421875
train loss:  0.5052604675292969
train gradient:  0.12136933297228107
iteration : 13077
train acc:  0.7578125
train loss:  0.4676678776741028
train gradient:  0.11633158814209753
iteration : 13078
train acc:  0.765625
train loss:  0.41290995478630066
train gradient:  0.08269386820026942
iteration : 13079
train acc:  0.6953125
train loss:  0.5736919641494751
train gradient:  0.2060667802074473
iteration : 13080
train acc:  0.6953125
train loss:  0.5667324066162109
train gradient:  0.1218016262945822
iteration : 13081
train acc:  0.78125
train loss:  0.45226916670799255
train gradient:  0.09845739565945556
iteration : 13082
train acc:  0.6953125
train loss:  0.5639386177062988
train gradient:  0.1859244789011337
iteration : 13083
train acc:  0.7109375
train loss:  0.5096398591995239
train gradient:  0.10495218433935971
iteration : 13084
train acc:  0.6953125
train loss:  0.5162315964698792
train gradient:  0.14103481564878384
iteration : 13085
train acc:  0.7890625
train loss:  0.46330726146698
train gradient:  0.14652836986920262
iteration : 13086
train acc:  0.8046875
train loss:  0.47381293773651123
train gradient:  0.11317351046035265
iteration : 13087
train acc:  0.78125
train loss:  0.4558838903903961
train gradient:  0.09387952035765196
iteration : 13088
train acc:  0.734375
train loss:  0.45545274019241333
train gradient:  0.08459119888383652
iteration : 13089
train acc:  0.78125
train loss:  0.45150506496429443
train gradient:  0.12325743575319123
iteration : 13090
train acc:  0.7890625
train loss:  0.4769430458545685
train gradient:  0.12798725216703666
iteration : 13091
train acc:  0.7890625
train loss:  0.4352412819862366
train gradient:  0.09785515824704713
iteration : 13092
train acc:  0.6875
train loss:  0.5878299474716187
train gradient:  0.1606130268216962
iteration : 13093
train acc:  0.7578125
train loss:  0.4365808963775635
train gradient:  0.07543321156224372
iteration : 13094
train acc:  0.7578125
train loss:  0.5063899755477905
train gradient:  0.13396155689951278
iteration : 13095
train acc:  0.78125
train loss:  0.4677123725414276
train gradient:  0.10046379258979181
iteration : 13096
train acc:  0.78125
train loss:  0.4210266172885895
train gradient:  0.10007494272086981
iteration : 13097
train acc:  0.796875
train loss:  0.39873072504997253
train gradient:  0.09660244886516639
iteration : 13098
train acc:  0.7265625
train loss:  0.5157718658447266
train gradient:  0.12863287081190675
iteration : 13099
train acc:  0.765625
train loss:  0.48349079489707947
train gradient:  0.09391312792314858
iteration : 13100
train acc:  0.765625
train loss:  0.43254947662353516
train gradient:  0.11294353804282446
iteration : 13101
train acc:  0.71875
train loss:  0.4823736548423767
train gradient:  0.13024729976653043
iteration : 13102
train acc:  0.7734375
train loss:  0.5071201920509338
train gradient:  0.12138368193916742
iteration : 13103
train acc:  0.7578125
train loss:  0.4471282958984375
train gradient:  0.09117657084805338
iteration : 13104
train acc:  0.78125
train loss:  0.4554985761642456
train gradient:  0.10176285087831763
iteration : 13105
train acc:  0.8203125
train loss:  0.39155662059783936
train gradient:  0.07944560822089797
iteration : 13106
train acc:  0.7265625
train loss:  0.5232481956481934
train gradient:  0.14088439204683478
iteration : 13107
train acc:  0.71875
train loss:  0.5650782585144043
train gradient:  0.13687715981588328
iteration : 13108
train acc:  0.703125
train loss:  0.5597857236862183
train gradient:  0.17218672579446248
iteration : 13109
train acc:  0.7109375
train loss:  0.48294752836227417
train gradient:  0.09803614042267637
iteration : 13110
train acc:  0.8046875
train loss:  0.4252139627933502
train gradient:  0.10698926024150608
iteration : 13111
train acc:  0.7421875
train loss:  0.4726303815841675
train gradient:  0.11103775722579882
iteration : 13112
train acc:  0.7109375
train loss:  0.5335264205932617
train gradient:  0.1287850633601045
iteration : 13113
train acc:  0.7265625
train loss:  0.4843367040157318
train gradient:  0.13708394645439415
iteration : 13114
train acc:  0.734375
train loss:  0.4767735004425049
train gradient:  0.10188907826188091
iteration : 13115
train acc:  0.8125
train loss:  0.37939006090164185
train gradient:  0.06966084877086728
iteration : 13116
train acc:  0.6875
train loss:  0.5393813252449036
train gradient:  0.16241847050703295
iteration : 13117
train acc:  0.7109375
train loss:  0.49329468607902527
train gradient:  0.11855398958202013
iteration : 13118
train acc:  0.8203125
train loss:  0.45697668194770813
train gradient:  0.09463866659876914
iteration : 13119
train acc:  0.6875
train loss:  0.5533618927001953
train gradient:  0.15387826047713865
iteration : 13120
train acc:  0.7890625
train loss:  0.429638534784317
train gradient:  0.09594177051005211
iteration : 13121
train acc:  0.6796875
train loss:  0.5348537564277649
train gradient:  0.1160282201454432
iteration : 13122
train acc:  0.703125
train loss:  0.46631425619125366
train gradient:  0.0913961503525259
iteration : 13123
train acc:  0.7734375
train loss:  0.4677492678165436
train gradient:  0.13973180354581585
iteration : 13124
train acc:  0.78125
train loss:  0.4655129611492157
train gradient:  0.14946008816389617
iteration : 13125
train acc:  0.7578125
train loss:  0.5020864009857178
train gradient:  0.10194395104293504
iteration : 13126
train acc:  0.7578125
train loss:  0.45807838439941406
train gradient:  0.12241958320752162
iteration : 13127
train acc:  0.78125
train loss:  0.45551037788391113
train gradient:  0.08942136435109171
iteration : 13128
train acc:  0.7265625
train loss:  0.5328470468521118
train gradient:  0.16643344219018014
iteration : 13129
train acc:  0.75
train loss:  0.4885656535625458
train gradient:  0.1308152383583751
iteration : 13130
train acc:  0.7421875
train loss:  0.4885251522064209
train gradient:  0.11387719329025708
iteration : 13131
train acc:  0.75
train loss:  0.46008020639419556
train gradient:  0.11000963138497563
iteration : 13132
train acc:  0.8125
train loss:  0.4059564471244812
train gradient:  0.10573040079411773
iteration : 13133
train acc:  0.6875
train loss:  0.541581392288208
train gradient:  0.1411094148371837
iteration : 13134
train acc:  0.7734375
train loss:  0.4740966558456421
train gradient:  0.12226452347099989
iteration : 13135
train acc:  0.7421875
train loss:  0.48854079842567444
train gradient:  0.0999226408165633
iteration : 13136
train acc:  0.7109375
train loss:  0.5290971994400024
train gradient:  0.14772285535023458
iteration : 13137
train acc:  0.6875
train loss:  0.5154993534088135
train gradient:  0.1345972074278778
iteration : 13138
train acc:  0.765625
train loss:  0.4736599326133728
train gradient:  0.10417655214617855
iteration : 13139
train acc:  0.796875
train loss:  0.4354972839355469
train gradient:  0.10747783920264611
iteration : 13140
train acc:  0.71875
train loss:  0.5111358165740967
train gradient:  0.1203546513991784
iteration : 13141
train acc:  0.7421875
train loss:  0.4626014828681946
train gradient:  0.08906189374142516
iteration : 13142
train acc:  0.7890625
train loss:  0.41976821422576904
train gradient:  0.09874731477533401
iteration : 13143
train acc:  0.671875
train loss:  0.5473907589912415
train gradient:  0.15752747514033993
iteration : 13144
train acc:  0.734375
train loss:  0.481964647769928
train gradient:  0.17192453015628933
iteration : 13145
train acc:  0.703125
train loss:  0.5644968748092651
train gradient:  0.16332823892157222
iteration : 13146
train acc:  0.765625
train loss:  0.4264267385005951
train gradient:  0.10490238060157374
iteration : 13147
train acc:  0.78125
train loss:  0.39857590198516846
train gradient:  0.09729578454183171
iteration : 13148
train acc:  0.7734375
train loss:  0.49505484104156494
train gradient:  0.11799611109341557
iteration : 13149
train acc:  0.6875
train loss:  0.5105977058410645
train gradient:  0.1178487581989821
iteration : 13150
train acc:  0.7578125
train loss:  0.4887673854827881
train gradient:  0.16198742762136015
iteration : 13151
train acc:  0.7421875
train loss:  0.43877673149108887
train gradient:  0.08681042453413641
iteration : 13152
train acc:  0.7265625
train loss:  0.5074756145477295
train gradient:  0.12438105055664254
iteration : 13153
train acc:  0.7734375
train loss:  0.4700096845626831
train gradient:  0.1237590202507839
iteration : 13154
train acc:  0.8359375
train loss:  0.40874165296554565
train gradient:  0.06479764159054871
iteration : 13155
train acc:  0.796875
train loss:  0.46242910623550415
train gradient:  0.1068257725797389
iteration : 13156
train acc:  0.7421875
train loss:  0.45119237899780273
train gradient:  0.11254272532333029
iteration : 13157
train acc:  0.75
train loss:  0.4912679195404053
train gradient:  0.12403037078378694
iteration : 13158
train acc:  0.7421875
train loss:  0.454298734664917
train gradient:  0.09946705029970657
iteration : 13159
train acc:  0.734375
train loss:  0.4954766631126404
train gradient:  0.11609342657400845
iteration : 13160
train acc:  0.8046875
train loss:  0.46394509077072144
train gradient:  0.11630482407331928
iteration : 13161
train acc:  0.7109375
train loss:  0.5507190227508545
train gradient:  0.15484860298352948
iteration : 13162
train acc:  0.765625
train loss:  0.4660034775733948
train gradient:  0.10542209965601042
iteration : 13163
train acc:  0.8125
train loss:  0.41772592067718506
train gradient:  0.09597680319235703
iteration : 13164
train acc:  0.7421875
train loss:  0.5040382742881775
train gradient:  0.11861196540025919
iteration : 13165
train acc:  0.828125
train loss:  0.4089356064796448
train gradient:  0.10140974370407535
iteration : 13166
train acc:  0.7578125
train loss:  0.48040953278541565
train gradient:  0.11319317521913595
iteration : 13167
train acc:  0.8046875
train loss:  0.45716750621795654
train gradient:  0.1443589640694245
iteration : 13168
train acc:  0.765625
train loss:  0.5044677257537842
train gradient:  0.15136075311419195
iteration : 13169
train acc:  0.7578125
train loss:  0.44486474990844727
train gradient:  0.10239479273100494
iteration : 13170
train acc:  0.7734375
train loss:  0.46924903988838196
train gradient:  0.10063939078844317
iteration : 13171
train acc:  0.7578125
train loss:  0.48383671045303345
train gradient:  0.13397067350845387
iteration : 13172
train acc:  0.78125
train loss:  0.41683435440063477
train gradient:  0.06790932209914105
iteration : 13173
train acc:  0.75
train loss:  0.4999467134475708
train gradient:  0.13491738872672465
iteration : 13174
train acc:  0.734375
train loss:  0.5041301250457764
train gradient:  0.13358122166400793
iteration : 13175
train acc:  0.7265625
train loss:  0.46440571546554565
train gradient:  0.08124874013618105
iteration : 13176
train acc:  0.7265625
train loss:  0.47473663091659546
train gradient:  0.11417300968299905
iteration : 13177
train acc:  0.7265625
train loss:  0.47799476981163025
train gradient:  0.0964124742170166
iteration : 13178
train acc:  0.6953125
train loss:  0.5391358733177185
train gradient:  0.14513337412577462
iteration : 13179
train acc:  0.8125
train loss:  0.458412230014801
train gradient:  0.0938424421383561
iteration : 13180
train acc:  0.765625
train loss:  0.43500590324401855
train gradient:  0.10053767249440389
iteration : 13181
train acc:  0.71875
train loss:  0.5311352014541626
train gradient:  0.1408057572431628
iteration : 13182
train acc:  0.7890625
train loss:  0.4644031524658203
train gradient:  0.09397087906667004
iteration : 13183
train acc:  0.7421875
train loss:  0.5247558951377869
train gradient:  0.12759581935301678
iteration : 13184
train acc:  0.8046875
train loss:  0.4662947654724121
train gradient:  0.0961671057582572
iteration : 13185
train acc:  0.7265625
train loss:  0.5150431394577026
train gradient:  0.1134225242468611
iteration : 13186
train acc:  0.7265625
train loss:  0.466025173664093
train gradient:  0.10918783900156528
iteration : 13187
train acc:  0.8046875
train loss:  0.44160211086273193
train gradient:  0.12624298209590795
iteration : 13188
train acc:  0.796875
train loss:  0.48433399200439453
train gradient:  0.10430284086085927
iteration : 13189
train acc:  0.6875
train loss:  0.49378591775894165
train gradient:  0.1083557945584024
iteration : 13190
train acc:  0.7734375
train loss:  0.48562324047088623
train gradient:  0.1617666248495987
iteration : 13191
train acc:  0.7734375
train loss:  0.4340447783470154
train gradient:  0.10647989268998748
iteration : 13192
train acc:  0.6953125
train loss:  0.5860587954521179
train gradient:  0.15413266783775897
iteration : 13193
train acc:  0.703125
train loss:  0.5052983164787292
train gradient:  0.17824979996645743
iteration : 13194
train acc:  0.7421875
train loss:  0.5248806476593018
train gradient:  0.14461835424721575
iteration : 13195
train acc:  0.7109375
train loss:  0.5304483771324158
train gradient:  0.13428258974567092
iteration : 13196
train acc:  0.734375
train loss:  0.5219647288322449
train gradient:  0.15215216818181748
iteration : 13197
train acc:  0.703125
train loss:  0.5101015567779541
train gradient:  0.1344306243217312
iteration : 13198
train acc:  0.7109375
train loss:  0.5024983882904053
train gradient:  0.15546124708268133
iteration : 13199
train acc:  0.7109375
train loss:  0.5328857898712158
train gradient:  0.14218640437958305
iteration : 13200
train acc:  0.734375
train loss:  0.4542655944824219
train gradient:  0.09033260223941798
iteration : 13201
train acc:  0.7265625
train loss:  0.57221919298172
train gradient:  0.15231160570253818
iteration : 13202
train acc:  0.703125
train loss:  0.5738259553909302
train gradient:  0.19489393121672755
iteration : 13203
train acc:  0.6875
train loss:  0.5123050212860107
train gradient:  0.10968046715146272
iteration : 13204
train acc:  0.78125
train loss:  0.4386913776397705
train gradient:  0.10669424169561945
iteration : 13205
train acc:  0.703125
train loss:  0.5098093748092651
train gradient:  0.18857926077053686
iteration : 13206
train acc:  0.8046875
train loss:  0.4416109025478363
train gradient:  0.08209092053923517
iteration : 13207
train acc:  0.6953125
train loss:  0.5031331777572632
train gradient:  0.16062074557750036
iteration : 13208
train acc:  0.7265625
train loss:  0.46308064460754395
train gradient:  0.0958767403511835
iteration : 13209
train acc:  0.71875
train loss:  0.4724600911140442
train gradient:  0.10621289848255065
iteration : 13210
train acc:  0.7734375
train loss:  0.44158369302749634
train gradient:  0.0930746414542339
iteration : 13211
train acc:  0.7265625
train loss:  0.4886847138404846
train gradient:  0.10688243020176447
iteration : 13212
train acc:  0.703125
train loss:  0.5349909663200378
train gradient:  0.14588905909467115
iteration : 13213
train acc:  0.7109375
train loss:  0.5533825159072876
train gradient:  0.15084065628387883
iteration : 13214
train acc:  0.765625
train loss:  0.44823479652404785
train gradient:  0.11396168699616256
iteration : 13215
train acc:  0.734375
train loss:  0.49846774339675903
train gradient:  0.13223461318383656
iteration : 13216
train acc:  0.75
train loss:  0.5213736295700073
train gradient:  0.15260776497882655
iteration : 13217
train acc:  0.7734375
train loss:  0.4396364092826843
train gradient:  0.10204651373877137
iteration : 13218
train acc:  0.75
train loss:  0.525276780128479
train gradient:  0.15342080375442535
iteration : 13219
train acc:  0.7421875
train loss:  0.4640108346939087
train gradient:  0.1252949133528815
iteration : 13220
train acc:  0.8046875
train loss:  0.44606244564056396
train gradient:  0.09790136507122948
iteration : 13221
train acc:  0.71875
train loss:  0.4829176664352417
train gradient:  0.14041579693602219
iteration : 13222
train acc:  0.765625
train loss:  0.4373830556869507
train gradient:  0.09499577395328158
iteration : 13223
train acc:  0.75
train loss:  0.43401575088500977
train gradient:  0.10513205746446486
iteration : 13224
train acc:  0.6953125
train loss:  0.46771687269210815
train gradient:  0.11722409335657193
iteration : 13225
train acc:  0.765625
train loss:  0.4346996545791626
train gradient:  0.12261641920059095
iteration : 13226
train acc:  0.78125
train loss:  0.4288550913333893
train gradient:  0.09463337857275989
iteration : 13227
train acc:  0.7109375
train loss:  0.49361658096313477
train gradient:  0.0978549341008362
iteration : 13228
train acc:  0.78125
train loss:  0.4856258034706116
train gradient:  0.13713541420497088
iteration : 13229
train acc:  0.75
train loss:  0.4818258285522461
train gradient:  0.11812409487067609
iteration : 13230
train acc:  0.71875
train loss:  0.4994046092033386
train gradient:  0.12871283476809003
iteration : 13231
train acc:  0.765625
train loss:  0.5099191665649414
train gradient:  0.11527742496929423
iteration : 13232
train acc:  0.796875
train loss:  0.44072026014328003
train gradient:  0.11655431768091491
iteration : 13233
train acc:  0.6796875
train loss:  0.561627209186554
train gradient:  0.13784330003787426
iteration : 13234
train acc:  0.75
train loss:  0.44848352670669556
train gradient:  0.11005546858219424
iteration : 13235
train acc:  0.7734375
train loss:  0.43413352966308594
train gradient:  0.12195005814649182
iteration : 13236
train acc:  0.75
train loss:  0.5390284657478333
train gradient:  0.15679125122487514
iteration : 13237
train acc:  0.7578125
train loss:  0.44909125566482544
train gradient:  0.10955985741485917
iteration : 13238
train acc:  0.75
train loss:  0.4641483426094055
train gradient:  0.09972545984378037
iteration : 13239
train acc:  0.7578125
train loss:  0.5359002351760864
train gradient:  0.10539374130540599
iteration : 13240
train acc:  0.703125
train loss:  0.5132865905761719
train gradient:  0.11690173552576356
iteration : 13241
train acc:  0.7265625
train loss:  0.610414981842041
train gradient:  0.14417975112396558
iteration : 13242
train acc:  0.7890625
train loss:  0.4727766811847687
train gradient:  0.10516421520661673
iteration : 13243
train acc:  0.734375
train loss:  0.45120033621788025
train gradient:  0.11863880755093592
iteration : 13244
train acc:  0.734375
train loss:  0.5292773246765137
train gradient:  0.12920479591174355
iteration : 13245
train acc:  0.796875
train loss:  0.42682576179504395
train gradient:  0.10162684605998604
iteration : 13246
train acc:  0.796875
train loss:  0.4960353970527649
train gradient:  0.15563362143878284
iteration : 13247
train acc:  0.7890625
train loss:  0.41950857639312744
train gradient:  0.09309581041934842
iteration : 13248
train acc:  0.765625
train loss:  0.47185057401657104
train gradient:  0.09980246801356814
iteration : 13249
train acc:  0.7578125
train loss:  0.4538835287094116
train gradient:  0.12068389340105941
iteration : 13250
train acc:  0.765625
train loss:  0.4617604613304138
train gradient:  0.11123115657623214
iteration : 13251
train acc:  0.734375
train loss:  0.5300526022911072
train gradient:  0.14973336094505724
iteration : 13252
train acc:  0.703125
train loss:  0.49712714552879333
train gradient:  0.09857946068065576
iteration : 13253
train acc:  0.65625
train loss:  0.5790973901748657
train gradient:  0.13701043589132073
iteration : 13254
train acc:  0.78125
train loss:  0.4715328812599182
train gradient:  0.13976597953761322
iteration : 13255
train acc:  0.703125
train loss:  0.5598204135894775
train gradient:  0.1403337167315776
iteration : 13256
train acc:  0.78125
train loss:  0.4139355421066284
train gradient:  0.09949985766463126
iteration : 13257
train acc:  0.75
train loss:  0.45620447397232056
train gradient:  0.09821823633468588
iteration : 13258
train acc:  0.828125
train loss:  0.39384472370147705
train gradient:  0.0861225631738551
iteration : 13259
train acc:  0.7421875
train loss:  0.4484805464744568
train gradient:  0.1068867435911732
iteration : 13260
train acc:  0.7578125
train loss:  0.47247499227523804
train gradient:  0.11064578333954483
iteration : 13261
train acc:  0.75
train loss:  0.4798209071159363
train gradient:  0.120334911238622
iteration : 13262
train acc:  0.7421875
train loss:  0.4928668141365051
train gradient:  0.128506661944797
iteration : 13263
train acc:  0.78125
train loss:  0.4792943596839905
train gradient:  0.1055475800892123
iteration : 13264
train acc:  0.7890625
train loss:  0.41482001543045044
train gradient:  0.09347743038602516
iteration : 13265
train acc:  0.8125
train loss:  0.4658861756324768
train gradient:  0.14284288389719535
iteration : 13266
train acc:  0.7421875
train loss:  0.4803396165370941
train gradient:  0.11977008587691804
iteration : 13267
train acc:  0.78125
train loss:  0.49605774879455566
train gradient:  0.11740753960778247
iteration : 13268
train acc:  0.7734375
train loss:  0.5011190176010132
train gradient:  0.14718203973595317
iteration : 13269
train acc:  0.78125
train loss:  0.48879992961883545
train gradient:  0.1426257280008545
iteration : 13270
train acc:  0.8046875
train loss:  0.43538898229599
train gradient:  0.1134426233196538
iteration : 13271
train acc:  0.734375
train loss:  0.4661102890968323
train gradient:  0.1340057548245975
iteration : 13272
train acc:  0.71875
train loss:  0.4610828161239624
train gradient:  0.09862103571764777
iteration : 13273
train acc:  0.8046875
train loss:  0.3837733864784241
train gradient:  0.08695754808646473
iteration : 13274
train acc:  0.7109375
train loss:  0.5077604055404663
train gradient:  0.10895788207032811
iteration : 13275
train acc:  0.7421875
train loss:  0.5118532776832581
train gradient:  0.11098009438345965
iteration : 13276
train acc:  0.796875
train loss:  0.4448423981666565
train gradient:  0.11512508759273211
iteration : 13277
train acc:  0.71875
train loss:  0.5332406759262085
train gradient:  0.1568383811759565
iteration : 13278
train acc:  0.78125
train loss:  0.4795180857181549
train gradient:  0.11413351631278983
iteration : 13279
train acc:  0.6328125
train loss:  0.6249055862426758
train gradient:  0.17488416444433696
iteration : 13280
train acc:  0.7890625
train loss:  0.49193382263183594
train gradient:  0.11138610873525098
iteration : 13281
train acc:  0.7578125
train loss:  0.43444156646728516
train gradient:  0.08786652871079977
iteration : 13282
train acc:  0.7578125
train loss:  0.4449748694896698
train gradient:  0.08956179311654157
iteration : 13283
train acc:  0.7890625
train loss:  0.5125399231910706
train gradient:  0.13748314198115097
iteration : 13284
train acc:  0.7578125
train loss:  0.46150240302085876
train gradient:  0.10124371383474465
iteration : 13285
train acc:  0.7578125
train loss:  0.4665847718715668
train gradient:  0.10567405026756675
iteration : 13286
train acc:  0.71875
train loss:  0.5123313665390015
train gradient:  0.10894983286185975
iteration : 13287
train acc:  0.7734375
train loss:  0.4806950092315674
train gradient:  0.11696643134149881
iteration : 13288
train acc:  0.703125
train loss:  0.5202348232269287
train gradient:  0.11438413273647115
iteration : 13289
train acc:  0.7421875
train loss:  0.5490628480911255
train gradient:  0.15665554550682972
iteration : 13290
train acc:  0.6796875
train loss:  0.5382186770439148
train gradient:  0.1284402025468579
iteration : 13291
train acc:  0.7421875
train loss:  0.4950082004070282
train gradient:  0.12090544247426098
iteration : 13292
train acc:  0.796875
train loss:  0.44673287868499756
train gradient:  0.09205125692211837
iteration : 13293
train acc:  0.796875
train loss:  0.39682537317276
train gradient:  0.08148517319208408
iteration : 13294
train acc:  0.75
train loss:  0.4804639220237732
train gradient:  0.10570680219912466
iteration : 13295
train acc:  0.734375
train loss:  0.48688429594039917
train gradient:  0.10970487693542881
iteration : 13296
train acc:  0.7109375
train loss:  0.5413217544555664
train gradient:  0.13011995388168685
iteration : 13297
train acc:  0.7578125
train loss:  0.5170947313308716
train gradient:  0.11155097776239438
iteration : 13298
train acc:  0.7421875
train loss:  0.47134751081466675
train gradient:  0.13847464264838538
iteration : 13299
train acc:  0.71875
train loss:  0.5841842889785767
train gradient:  0.16006845208494438
iteration : 13300
train acc:  0.65625
train loss:  0.6025223731994629
train gradient:  0.25180138734918045
iteration : 13301
train acc:  0.7578125
train loss:  0.48544758558273315
train gradient:  0.14344912227681328
iteration : 13302
train acc:  0.75
train loss:  0.5297672152519226
train gradient:  0.1152606846150668
iteration : 13303
train acc:  0.7421875
train loss:  0.5362164378166199
train gradient:  0.12979953569840852
iteration : 13304
train acc:  0.71875
train loss:  0.47977781295776367
train gradient:  0.12059857695617636
iteration : 13305
train acc:  0.7265625
train loss:  0.5197538137435913
train gradient:  0.1429963917037628
iteration : 13306
train acc:  0.8125
train loss:  0.47302472591400146
train gradient:  0.1314392358978964
iteration : 13307
train acc:  0.7421875
train loss:  0.5269143581390381
train gradient:  0.12321652441449449
iteration : 13308
train acc:  0.7890625
train loss:  0.4766797423362732
train gradient:  0.12143570568596629
iteration : 13309
train acc:  0.7421875
train loss:  0.4985208511352539
train gradient:  0.11252957691502356
iteration : 13310
train acc:  0.71875
train loss:  0.5086698532104492
train gradient:  0.1498268003418977
iteration : 13311
train acc:  0.734375
train loss:  0.5134028196334839
train gradient:  0.14169377993696555
iteration : 13312
train acc:  0.78125
train loss:  0.4401106536388397
train gradient:  0.09491497906044777
iteration : 13313
train acc:  0.7421875
train loss:  0.5061429738998413
train gradient:  0.12084603396283808
iteration : 13314
train acc:  0.734375
train loss:  0.5199689865112305
train gradient:  0.13716862316228806
iteration : 13315
train acc:  0.703125
train loss:  0.5349339842796326
train gradient:  0.11696475784699001
iteration : 13316
train acc:  0.75
train loss:  0.5099834203720093
train gradient:  0.12255326086526729
iteration : 13317
train acc:  0.7265625
train loss:  0.4496879279613495
train gradient:  0.0901986716158169
iteration : 13318
train acc:  0.71875
train loss:  0.5286519527435303
train gradient:  0.13579319325265035
iteration : 13319
train acc:  0.765625
train loss:  0.44989103078842163
train gradient:  0.08831500577354953
iteration : 13320
train acc:  0.7890625
train loss:  0.4933423697948456
train gradient:  0.1346541687657853
iteration : 13321
train acc:  0.7109375
train loss:  0.4581671357154846
train gradient:  0.10948432502883744
iteration : 13322
train acc:  0.7578125
train loss:  0.49616870284080505
train gradient:  0.16762572757368457
iteration : 13323
train acc:  0.6796875
train loss:  0.568515419960022
train gradient:  0.13406841803076658
iteration : 13324
train acc:  0.765625
train loss:  0.4484618902206421
train gradient:  0.09693591085608344
iteration : 13325
train acc:  0.765625
train loss:  0.4655318260192871
train gradient:  0.09647786857135447
iteration : 13326
train acc:  0.8125
train loss:  0.4251744747161865
train gradient:  0.08638784009911628
iteration : 13327
train acc:  0.78125
train loss:  0.45979300141334534
train gradient:  0.1176920448502542
iteration : 13328
train acc:  0.71875
train loss:  0.5126252770423889
train gradient:  0.11007261469252726
iteration : 13329
train acc:  0.71875
train loss:  0.5122394561767578
train gradient:  0.11631566364418601
iteration : 13330
train acc:  0.8203125
train loss:  0.4531427025794983
train gradient:  0.13465966225777154
iteration : 13331
train acc:  0.6953125
train loss:  0.5324182510375977
train gradient:  0.11505524140061041
iteration : 13332
train acc:  0.7265625
train loss:  0.4372020661830902
train gradient:  0.08927403156693947
iteration : 13333
train acc:  0.75
train loss:  0.45789727568626404
train gradient:  0.12196382320589037
iteration : 13334
train acc:  0.7578125
train loss:  0.4766666293144226
train gradient:  0.1124067692864188
iteration : 13335
train acc:  0.7890625
train loss:  0.4182235598564148
train gradient:  0.09049508923683242
iteration : 13336
train acc:  0.7734375
train loss:  0.41168510913848877
train gradient:  0.08563425972666391
iteration : 13337
train acc:  0.7734375
train loss:  0.46464642882347107
train gradient:  0.1199437970249903
iteration : 13338
train acc:  0.796875
train loss:  0.4417709708213806
train gradient:  0.10191063416097475
iteration : 13339
train acc:  0.7734375
train loss:  0.4978797137737274
train gradient:  0.11810010678480751
iteration : 13340
train acc:  0.7265625
train loss:  0.48523885011672974
train gradient:  0.11458096868702619
iteration : 13341
train acc:  0.7421875
train loss:  0.4807378053665161
train gradient:  0.09559749570572836
iteration : 13342
train acc:  0.6875
train loss:  0.5261256694793701
train gradient:  0.12434898360774614
iteration : 13343
train acc:  0.71875
train loss:  0.5260312557220459
train gradient:  0.1640785490828603
iteration : 13344
train acc:  0.8046875
train loss:  0.44323843717575073
train gradient:  0.09702397964653642
iteration : 13345
train acc:  0.7421875
train loss:  0.48518142104148865
train gradient:  0.11514485413067081
iteration : 13346
train acc:  0.703125
train loss:  0.4973585605621338
train gradient:  0.10807112973861534
iteration : 13347
train acc:  0.7109375
train loss:  0.5725277662277222
train gradient:  0.17265263853049723
iteration : 13348
train acc:  0.671875
train loss:  0.5129505395889282
train gradient:  0.10045371743532963
iteration : 13349
train acc:  0.8359375
train loss:  0.44467416405677795
train gradient:  0.11180271938447478
iteration : 13350
train acc:  0.765625
train loss:  0.4519956111907959
train gradient:  0.10330751210200469
iteration : 13351
train acc:  0.78125
train loss:  0.4595891237258911
train gradient:  0.09965865722668331
iteration : 13352
train acc:  0.6875
train loss:  0.47735828161239624
train gradient:  0.09153599213260943
iteration : 13353
train acc:  0.7421875
train loss:  0.47419896721839905
train gradient:  0.11946683557644876
iteration : 13354
train acc:  0.6953125
train loss:  0.48783183097839355
train gradient:  0.09353778932538342
iteration : 13355
train acc:  0.765625
train loss:  0.4454677402973175
train gradient:  0.10619836520053894
iteration : 13356
train acc:  0.6875
train loss:  0.5435186624526978
train gradient:  0.1499247031563931
iteration : 13357
train acc:  0.7421875
train loss:  0.46043211221694946
train gradient:  0.09501236313140198
iteration : 13358
train acc:  0.7265625
train loss:  0.45607832074165344
train gradient:  0.11463219773763497
iteration : 13359
train acc:  0.7734375
train loss:  0.4382517337799072
train gradient:  0.09377384697938221
iteration : 13360
train acc:  0.7421875
train loss:  0.47459977865219116
train gradient:  0.13133914349044046
iteration : 13361
train acc:  0.71875
train loss:  0.4928261637687683
train gradient:  0.15367242204737122
iteration : 13362
train acc:  0.828125
train loss:  0.3840900659561157
train gradient:  0.07825759319474901
iteration : 13363
train acc:  0.7734375
train loss:  0.44511038064956665
train gradient:  0.11302833823530357
iteration : 13364
train acc:  0.7421875
train loss:  0.4667929410934448
train gradient:  0.12614833228090463
iteration : 13365
train acc:  0.734375
train loss:  0.46908196806907654
train gradient:  0.10116310704107935
iteration : 13366
train acc:  0.828125
train loss:  0.42749419808387756
train gradient:  0.09387059297592333
iteration : 13367
train acc:  0.7265625
train loss:  0.5027717351913452
train gradient:  0.12528675925137944
iteration : 13368
train acc:  0.7734375
train loss:  0.4272027611732483
train gradient:  0.08358607919644219
iteration : 13369
train acc:  0.75
train loss:  0.5173249244689941
train gradient:  0.1365598244887696
iteration : 13370
train acc:  0.84375
train loss:  0.3955557644367218
train gradient:  0.08924100536579396
iteration : 13371
train acc:  0.765625
train loss:  0.4645279049873352
train gradient:  0.1000368253591261
iteration : 13372
train acc:  0.7734375
train loss:  0.45615464448928833
train gradient:  0.11011204601519164
iteration : 13373
train acc:  0.734375
train loss:  0.472590833902359
train gradient:  0.10140147836796505
iteration : 13374
train acc:  0.65625
train loss:  0.5884347558021545
train gradient:  0.18568140927485802
iteration : 13375
train acc:  0.6953125
train loss:  0.526294469833374
train gradient:  0.13359033724244823
iteration : 13376
train acc:  0.7734375
train loss:  0.4841422438621521
train gradient:  0.1382972826712886
iteration : 13377
train acc:  0.7421875
train loss:  0.46270036697387695
train gradient:  0.1108937168426367
iteration : 13378
train acc:  0.75
train loss:  0.4870126247406006
train gradient:  0.11648821500013293
iteration : 13379
train acc:  0.8203125
train loss:  0.46084824204444885
train gradient:  0.09681756933120582
iteration : 13380
train acc:  0.7578125
train loss:  0.5031315088272095
train gradient:  0.10550745536084685
iteration : 13381
train acc:  0.734375
train loss:  0.5326206684112549
train gradient:  0.14139970246044253
iteration : 13382
train acc:  0.671875
train loss:  0.5635634660720825
train gradient:  0.18462817982373084
iteration : 13383
train acc:  0.734375
train loss:  0.5034787058830261
train gradient:  0.11371744648746573
iteration : 13384
train acc:  0.71875
train loss:  0.5480068922042847
train gradient:  0.14070780343206762
iteration : 13385
train acc:  0.71875
train loss:  0.5141156315803528
train gradient:  0.13035825521020739
iteration : 13386
train acc:  0.7109375
train loss:  0.5214911103248596
train gradient:  0.11356957317875092
iteration : 13387
train acc:  0.671875
train loss:  0.6161174774169922
train gradient:  0.22456969890755962
iteration : 13388
train acc:  0.671875
train loss:  0.5139611959457397
train gradient:  0.1631767795581054
iteration : 13389
train acc:  0.78125
train loss:  0.4415814280509949
train gradient:  0.0989185236145654
iteration : 13390
train acc:  0.7890625
train loss:  0.492156982421875
train gradient:  0.13589122717511348
iteration : 13391
train acc:  0.7109375
train loss:  0.48711884021759033
train gradient:  0.10146384559642622
iteration : 13392
train acc:  0.6953125
train loss:  0.5096859931945801
train gradient:  0.14099563612716187
iteration : 13393
train acc:  0.7421875
train loss:  0.47634655237197876
train gradient:  0.11272047200244287
iteration : 13394
train acc:  0.7109375
train loss:  0.5666689872741699
train gradient:  0.17038490062346118
iteration : 13395
train acc:  0.71875
train loss:  0.4658052623271942
train gradient:  0.10564176171324405
iteration : 13396
train acc:  0.78125
train loss:  0.448022723197937
train gradient:  0.10464741157550235
iteration : 13397
train acc:  0.7734375
train loss:  0.4398376941680908
train gradient:  0.0952777671925084
iteration : 13398
train acc:  0.7734375
train loss:  0.5004417896270752
train gradient:  0.1102929174504401
iteration : 13399
train acc:  0.7265625
train loss:  0.5654172897338867
train gradient:  0.15099322433342144
iteration : 13400
train acc:  0.796875
train loss:  0.41602957248687744
train gradient:  0.09373275568854934
iteration : 13401
train acc:  0.703125
train loss:  0.5177780985832214
train gradient:  0.12372875253571915
iteration : 13402
train acc:  0.78125
train loss:  0.46852168440818787
train gradient:  0.11575989732048862
iteration : 13403
train acc:  0.7265625
train loss:  0.4596198797225952
train gradient:  0.13043091063776863
iteration : 13404
train acc:  0.734375
train loss:  0.5033789873123169
train gradient:  0.12585548982408157
iteration : 13405
train acc:  0.7421875
train loss:  0.5150808691978455
train gradient:  0.15591993548599048
iteration : 13406
train acc:  0.7421875
train loss:  0.4394955635070801
train gradient:  0.10218804703651892
iteration : 13407
train acc:  0.7890625
train loss:  0.4246535897254944
train gradient:  0.08771082303574239
iteration : 13408
train acc:  0.7734375
train loss:  0.5316542387008667
train gradient:  0.1493926464751715
iteration : 13409
train acc:  0.765625
train loss:  0.41955140233039856
train gradient:  0.09883353421211684
iteration : 13410
train acc:  0.796875
train loss:  0.40099525451660156
train gradient:  0.07395143466755548
iteration : 13411
train acc:  0.796875
train loss:  0.4595707654953003
train gradient:  0.10022538438955608
iteration : 13412
train acc:  0.78125
train loss:  0.4439966678619385
train gradient:  0.09224692616521693
iteration : 13413
train acc:  0.7734375
train loss:  0.5178807377815247
train gradient:  0.11419622216404178
iteration : 13414
train acc:  0.7421875
train loss:  0.5080429315567017
train gradient:  0.13239022494936242
iteration : 13415
train acc:  0.71875
train loss:  0.47900086641311646
train gradient:  0.11601384394021365
iteration : 13416
train acc:  0.7734375
train loss:  0.5057416558265686
train gradient:  0.1319921235437128
iteration : 13417
train acc:  0.7265625
train loss:  0.5047795176506042
train gradient:  0.1334452062572486
iteration : 13418
train acc:  0.7265625
train loss:  0.5376443266868591
train gradient:  0.1308789517774087
iteration : 13419
train acc:  0.7578125
train loss:  0.48321837186813354
train gradient:  0.1113768470881254
iteration : 13420
train acc:  0.765625
train loss:  0.5630877017974854
train gradient:  0.19721844849974382
iteration : 13421
train acc:  0.8125
train loss:  0.43804752826690674
train gradient:  0.11087564681575025
iteration : 13422
train acc:  0.7265625
train loss:  0.4280647039413452
train gradient:  0.09767470220899582
iteration : 13423
train acc:  0.78125
train loss:  0.4384593069553375
train gradient:  0.09818635155756611
iteration : 13424
train acc:  0.734375
train loss:  0.46935099363327026
train gradient:  0.10502679370864122
iteration : 13425
train acc:  0.71875
train loss:  0.5164645910263062
train gradient:  0.13367902403079518
iteration : 13426
train acc:  0.734375
train loss:  0.46235671639442444
train gradient:  0.10224837555773345
iteration : 13427
train acc:  0.7578125
train loss:  0.42387187480926514
train gradient:  0.08304125980785794
iteration : 13428
train acc:  0.7578125
train loss:  0.5012187957763672
train gradient:  0.14523570498563187
iteration : 13429
train acc:  0.7578125
train loss:  0.4546121656894684
train gradient:  0.10579546609215902
iteration : 13430
train acc:  0.703125
train loss:  0.589000940322876
train gradient:  0.15370567798388315
iteration : 13431
train acc:  0.7734375
train loss:  0.47426846623420715
train gradient:  0.10595174731892204
iteration : 13432
train acc:  0.7421875
train loss:  0.4348687529563904
train gradient:  0.08921118415125473
iteration : 13433
train acc:  0.7734375
train loss:  0.4418710470199585
train gradient:  0.1046171681363532
iteration : 13434
train acc:  0.7421875
train loss:  0.49807798862457275
train gradient:  0.09058995676906953
iteration : 13435
train acc:  0.7421875
train loss:  0.45039933919906616
train gradient:  0.12745625736430677
iteration : 13436
train acc:  0.6953125
train loss:  0.5311709642410278
train gradient:  0.13136477200696237
iteration : 13437
train acc:  0.6953125
train loss:  0.4973357319831848
train gradient:  0.13291364368485337
iteration : 13438
train acc:  0.7421875
train loss:  0.4955870807170868
train gradient:  0.1032309891081992
iteration : 13439
train acc:  0.7578125
train loss:  0.48351994156837463
train gradient:  0.12162921958143828
iteration : 13440
train acc:  0.7421875
train loss:  0.4704751968383789
train gradient:  0.12683283771505383
iteration : 13441
train acc:  0.734375
train loss:  0.5182985663414001
train gradient:  0.10666212365148617
iteration : 13442
train acc:  0.6796875
train loss:  0.5528256893157959
train gradient:  0.19243907773261643
iteration : 13443
train acc:  0.765625
train loss:  0.46080371737480164
train gradient:  0.09945971857783417
iteration : 13444
train acc:  0.8125
train loss:  0.45110026001930237
train gradient:  0.08933138822911599
iteration : 13445
train acc:  0.7265625
train loss:  0.49318015575408936
train gradient:  0.14109043799585502
iteration : 13446
train acc:  0.796875
train loss:  0.4341449737548828
train gradient:  0.1008630601084233
iteration : 13447
train acc:  0.734375
train loss:  0.5660920143127441
train gradient:  0.14438767413054326
iteration : 13448
train acc:  0.734375
train loss:  0.5118913054466248
train gradient:  0.11694412877567148
iteration : 13449
train acc:  0.7265625
train loss:  0.5175208449363708
train gradient:  0.14175649576622934
iteration : 13450
train acc:  0.75
train loss:  0.4821830987930298
train gradient:  0.11102019298676603
iteration : 13451
train acc:  0.7578125
train loss:  0.48081856966018677
train gradient:  0.11629327000103272
iteration : 13452
train acc:  0.8046875
train loss:  0.450808048248291
train gradient:  0.08414830597021968
iteration : 13453
train acc:  0.7890625
train loss:  0.43344011902809143
train gradient:  0.08689490164941169
iteration : 13454
train acc:  0.75
train loss:  0.4987848401069641
train gradient:  0.12979874798615812
iteration : 13455
train acc:  0.6875
train loss:  0.5166191458702087
train gradient:  0.118018338795328
iteration : 13456
train acc:  0.734375
train loss:  0.5032957792282104
train gradient:  0.13157345641253307
iteration : 13457
train acc:  0.7890625
train loss:  0.4571915566921234
train gradient:  0.09239657885457561
iteration : 13458
train acc:  0.734375
train loss:  0.4913046360015869
train gradient:  0.1279563171330783
iteration : 13459
train acc:  0.6875
train loss:  0.5423078536987305
train gradient:  0.11887393229216695
iteration : 13460
train acc:  0.7578125
train loss:  0.4650842547416687
train gradient:  0.13773757705383222
iteration : 13461
train acc:  0.7578125
train loss:  0.5050074458122253
train gradient:  0.10717626648623162
iteration : 13462
train acc:  0.8125
train loss:  0.3913193941116333
train gradient:  0.09279126401675733
iteration : 13463
train acc:  0.765625
train loss:  0.4834527373313904
train gradient:  0.1086224804697664
iteration : 13464
train acc:  0.703125
train loss:  0.4977593421936035
train gradient:  0.14053006099925852
iteration : 13465
train acc:  0.7265625
train loss:  0.49383026361465454
train gradient:  0.1220504863540388
iteration : 13466
train acc:  0.7890625
train loss:  0.48668742179870605
train gradient:  0.11936677431973285
iteration : 13467
train acc:  0.734375
train loss:  0.5058493614196777
train gradient:  0.15189853003680354
iteration : 13468
train acc:  0.75
train loss:  0.45103922486305237
train gradient:  0.08232725959123005
iteration : 13469
train acc:  0.734375
train loss:  0.5112928152084351
train gradient:  0.12879225950672152
iteration : 13470
train acc:  0.7265625
train loss:  0.44605594873428345
train gradient:  0.07662042882995934
iteration : 13471
train acc:  0.71875
train loss:  0.4750017523765564
train gradient:  0.11311070034792173
iteration : 13472
train acc:  0.7890625
train loss:  0.4289194643497467
train gradient:  0.0802340064370687
iteration : 13473
train acc:  0.765625
train loss:  0.4360724687576294
train gradient:  0.08725945545872384
iteration : 13474
train acc:  0.7578125
train loss:  0.4593343734741211
train gradient:  0.08655785485973869
iteration : 13475
train acc:  0.7734375
train loss:  0.4750206172466278
train gradient:  0.0955433012436246
iteration : 13476
train acc:  0.7421875
train loss:  0.4515083432197571
train gradient:  0.08565019744519724
iteration : 13477
train acc:  0.6953125
train loss:  0.5007306337356567
train gradient:  0.11702262995723496
iteration : 13478
train acc:  0.7734375
train loss:  0.49272269010543823
train gradient:  0.14026668979620094
iteration : 13479
train acc:  0.703125
train loss:  0.5101823210716248
train gradient:  0.1127756491985652
iteration : 13480
train acc:  0.78125
train loss:  0.4293665885925293
train gradient:  0.07855217888138757
iteration : 13481
train acc:  0.7109375
train loss:  0.518120527267456
train gradient:  0.17000276541554704
iteration : 13482
train acc:  0.7578125
train loss:  0.47081661224365234
train gradient:  0.12554527643776875
iteration : 13483
train acc:  0.7265625
train loss:  0.4780314266681671
train gradient:  0.12178134870040036
iteration : 13484
train acc:  0.7265625
train loss:  0.5130326747894287
train gradient:  0.12280150861159073
iteration : 13485
train acc:  0.71875
train loss:  0.4728943109512329
train gradient:  0.09207086097485803
iteration : 13486
train acc:  0.75
train loss:  0.4480261206626892
train gradient:  0.08577943431628571
iteration : 13487
train acc:  0.7421875
train loss:  0.44374966621398926
train gradient:  0.09225975040996293
iteration : 13488
train acc:  0.7578125
train loss:  0.5522794723510742
train gradient:  0.14942436638737022
iteration : 13489
train acc:  0.7421875
train loss:  0.502091646194458
train gradient:  0.10201569683159173
iteration : 13490
train acc:  0.703125
train loss:  0.48122671246528625
train gradient:  0.10385992683074995
iteration : 13491
train acc:  0.75
train loss:  0.4880075752735138
train gradient:  0.11661234962719584
iteration : 13492
train acc:  0.8046875
train loss:  0.3995385766029358
train gradient:  0.07659162039561096
iteration : 13493
train acc:  0.7578125
train loss:  0.45109373331069946
train gradient:  0.11781710718573822
iteration : 13494
train acc:  0.6953125
train loss:  0.5067671537399292
train gradient:  0.1291611199428387
iteration : 13495
train acc:  0.75
train loss:  0.48218828439712524
train gradient:  0.12654796734718288
iteration : 13496
train acc:  0.765625
train loss:  0.4657514691352844
train gradient:  0.11515876174772606
iteration : 13497
train acc:  0.8125
train loss:  0.4269687533378601
train gradient:  0.11182826683685396
iteration : 13498
train acc:  0.7578125
train loss:  0.49479156732559204
train gradient:  0.11134336993754196
iteration : 13499
train acc:  0.7890625
train loss:  0.4339990019798279
train gradient:  0.09723610960854975
iteration : 13500
train acc:  0.734375
train loss:  0.4633834660053253
train gradient:  0.11408463228605312
iteration : 13501
train acc:  0.7109375
train loss:  0.5187633037567139
train gradient:  0.14254119995897185
iteration : 13502
train acc:  0.65625
train loss:  0.5418602228164673
train gradient:  0.16900563169643035
iteration : 13503
train acc:  0.7890625
train loss:  0.4776347279548645
train gradient:  0.18492610955257754
iteration : 13504
train acc:  0.78125
train loss:  0.4690658748149872
train gradient:  0.12112132817179697
iteration : 13505
train acc:  0.703125
train loss:  0.47251033782958984
train gradient:  0.10114930515497206
iteration : 13506
train acc:  0.734375
train loss:  0.5188484191894531
train gradient:  0.1487191164943315
iteration : 13507
train acc:  0.765625
train loss:  0.4555096924304962
train gradient:  0.12619289831929292
iteration : 13508
train acc:  0.7109375
train loss:  0.5097639560699463
train gradient:  0.15905242455126667
iteration : 13509
train acc:  0.7578125
train loss:  0.5494528412818909
train gradient:  0.1485624524438803
iteration : 13510
train acc:  0.71875
train loss:  0.4950430393218994
train gradient:  0.1548014448528205
iteration : 13511
train acc:  0.8359375
train loss:  0.44281816482543945
train gradient:  0.09457735199154567
iteration : 13512
train acc:  0.7109375
train loss:  0.5190747380256653
train gradient:  0.16667091086860564
iteration : 13513
train acc:  0.7734375
train loss:  0.44603997468948364
train gradient:  0.0951157475606335
iteration : 13514
train acc:  0.7890625
train loss:  0.45265477895736694
train gradient:  0.11018877199449043
iteration : 13515
train acc:  0.7421875
train loss:  0.49858954548835754
train gradient:  0.10310668065027052
iteration : 13516
train acc:  0.7890625
train loss:  0.42044883966445923
train gradient:  0.10311556786557949
iteration : 13517
train acc:  0.7421875
train loss:  0.4682249426841736
train gradient:  0.10668156444453834
iteration : 13518
train acc:  0.765625
train loss:  0.44418299198150635
train gradient:  0.1403312017890494
iteration : 13519
train acc:  0.8046875
train loss:  0.45661216974258423
train gradient:  0.11179317769318024
iteration : 13520
train acc:  0.828125
train loss:  0.37642228603363037
train gradient:  0.09447274609179424
iteration : 13521
train acc:  0.7578125
train loss:  0.5266051888465881
train gradient:  0.1486714581374476
iteration : 13522
train acc:  0.75
train loss:  0.4724409282207489
train gradient:  0.12236361700223672
iteration : 13523
train acc:  0.671875
train loss:  0.5572962760925293
train gradient:  0.14598985230693345
iteration : 13524
train acc:  0.7265625
train loss:  0.4950185716152191
train gradient:  0.12247992580459229
iteration : 13525
train acc:  0.796875
train loss:  0.48608866333961487
train gradient:  0.1038770468365582
iteration : 13526
train acc:  0.765625
train loss:  0.43715715408325195
train gradient:  0.11681757584803985
iteration : 13527
train acc:  0.75
train loss:  0.4910151958465576
train gradient:  0.12791123947053595
iteration : 13528
train acc:  0.7890625
train loss:  0.4005570411682129
train gradient:  0.10000686324676426
iteration : 13529
train acc:  0.6953125
train loss:  0.5720891952514648
train gradient:  0.1618731959958663
iteration : 13530
train acc:  0.734375
train loss:  0.4516398012638092
train gradient:  0.10201118859898654
iteration : 13531
train acc:  0.6953125
train loss:  0.5499407052993774
train gradient:  0.16723649577764282
iteration : 13532
train acc:  0.765625
train loss:  0.46486151218414307
train gradient:  0.12105418509368647
iteration : 13533
train acc:  0.75
train loss:  0.4834044277667999
train gradient:  0.10663756292964904
iteration : 13534
train acc:  0.796875
train loss:  0.44519785046577454
train gradient:  0.14449028556495047
iteration : 13535
train acc:  0.828125
train loss:  0.41927725076675415
train gradient:  0.0887594592207332
iteration : 13536
train acc:  0.75
train loss:  0.517576277256012
train gradient:  0.11775759017452343
iteration : 13537
train acc:  0.734375
train loss:  0.4950941503047943
train gradient:  0.12082918410626829
iteration : 13538
train acc:  0.828125
train loss:  0.4877520501613617
train gradient:  0.11122117062231476
iteration : 13539
train acc:  0.75
train loss:  0.49657708406448364
train gradient:  0.1282628181119448
iteration : 13540
train acc:  0.7734375
train loss:  0.4604324698448181
train gradient:  0.09379307597553563
iteration : 13541
train acc:  0.7578125
train loss:  0.5216193199157715
train gradient:  0.11649688554163719
iteration : 13542
train acc:  0.7265625
train loss:  0.4906175136566162
train gradient:  0.15053883925412992
iteration : 13543
train acc:  0.78125
train loss:  0.4272650480270386
train gradient:  0.09206918133147421
iteration : 13544
train acc:  0.7734375
train loss:  0.47321033477783203
train gradient:  0.11984121679390955
iteration : 13545
train acc:  0.71875
train loss:  0.48040905594825745
train gradient:  0.1119738215975255
iteration : 13546
train acc:  0.765625
train loss:  0.4262743890285492
train gradient:  0.0816189170534641
iteration : 13547
train acc:  0.7734375
train loss:  0.4376821517944336
train gradient:  0.11131013592185704
iteration : 13548
train acc:  0.75
train loss:  0.48020806908607483
train gradient:  0.1202518140948611
iteration : 13549
train acc:  0.7890625
train loss:  0.4272279739379883
train gradient:  0.09610901000715387
iteration : 13550
train acc:  0.796875
train loss:  0.4888871908187866
train gradient:  0.11824634497492001
iteration : 13551
train acc:  0.765625
train loss:  0.48231154680252075
train gradient:  0.11849233992250065
iteration : 13552
train acc:  0.75
train loss:  0.518531858921051
train gradient:  0.12627306609916117
iteration : 13553
train acc:  0.6796875
train loss:  0.5149002075195312
train gradient:  0.12847283012108351
iteration : 13554
train acc:  0.703125
train loss:  0.4756757616996765
train gradient:  0.12290340837928358
iteration : 13555
train acc:  0.7890625
train loss:  0.46230122447013855
train gradient:  0.1334364423978706
iteration : 13556
train acc:  0.734375
train loss:  0.5050008893013
train gradient:  0.1547500286471284
iteration : 13557
train acc:  0.765625
train loss:  0.46822673082351685
train gradient:  0.11400749390523801
iteration : 13558
train acc:  0.765625
train loss:  0.4783174395561218
train gradient:  0.12178661187089977
iteration : 13559
train acc:  0.796875
train loss:  0.41319936513900757
train gradient:  0.0852874960155388
iteration : 13560
train acc:  0.7578125
train loss:  0.4514331817626953
train gradient:  0.09663176416414115
iteration : 13561
train acc:  0.7578125
train loss:  0.44197702407836914
train gradient:  0.11572443795454247
iteration : 13562
train acc:  0.734375
train loss:  0.5133637189865112
train gradient:  0.13350105408119672
iteration : 13563
train acc:  0.765625
train loss:  0.46291255950927734
train gradient:  0.11165614544655939
iteration : 13564
train acc:  0.796875
train loss:  0.43858802318573
train gradient:  0.10125032752023896
iteration : 13565
train acc:  0.796875
train loss:  0.42419764399528503
train gradient:  0.11624805888074986
iteration : 13566
train acc:  0.7265625
train loss:  0.5858563780784607
train gradient:  0.20034902263052268
iteration : 13567
train acc:  0.7734375
train loss:  0.42996275424957275
train gradient:  0.11506153264170496
iteration : 13568
train acc:  0.7421875
train loss:  0.47373831272125244
train gradient:  0.12733159539411065
iteration : 13569
train acc:  0.765625
train loss:  0.5169045925140381
train gradient:  0.1227979209197817
iteration : 13570
train acc:  0.7421875
train loss:  0.5010047554969788
train gradient:  0.11150000479302444
iteration : 13571
train acc:  0.703125
train loss:  0.49485042691230774
train gradient:  0.13319086402278285
iteration : 13572
train acc:  0.734375
train loss:  0.4781865179538727
train gradient:  0.13697734237903206
iteration : 13573
train acc:  0.7265625
train loss:  0.5425127744674683
train gradient:  0.1372990639687774
iteration : 13574
train acc:  0.7421875
train loss:  0.4677838385105133
train gradient:  0.11151750040633684
iteration : 13575
train acc:  0.765625
train loss:  0.49322807788848877
train gradient:  0.10491643366767758
iteration : 13576
train acc:  0.7734375
train loss:  0.44366455078125
train gradient:  0.09893303522240829
iteration : 13577
train acc:  0.6953125
train loss:  0.5414332151412964
train gradient:  0.12826295893558676
iteration : 13578
train acc:  0.796875
train loss:  0.4706465005874634
train gradient:  0.10645667553303348
iteration : 13579
train acc:  0.734375
train loss:  0.46544724702835083
train gradient:  0.12294695628596408
iteration : 13580
train acc:  0.796875
train loss:  0.42117929458618164
train gradient:  0.09789161984177663
iteration : 13581
train acc:  0.734375
train loss:  0.48396003246307373
train gradient:  0.12227380901054362
iteration : 13582
train acc:  0.765625
train loss:  0.5187665224075317
train gradient:  0.14364264214103323
iteration : 13583
train acc:  0.6875
train loss:  0.5364802479743958
train gradient:  0.1363746777562537
iteration : 13584
train acc:  0.765625
train loss:  0.47802603244781494
train gradient:  0.09755653512135808
iteration : 13585
train acc:  0.7734375
train loss:  0.4751967787742615
train gradient:  0.11185128203054544
iteration : 13586
train acc:  0.78125
train loss:  0.4413837194442749
train gradient:  0.08788558442115492
iteration : 13587
train acc:  0.7890625
train loss:  0.4880408048629761
train gradient:  0.11559582174794122
iteration : 13588
train acc:  0.7734375
train loss:  0.42809444665908813
train gradient:  0.1542845144796962
iteration : 13589
train acc:  0.7109375
train loss:  0.5665953159332275
train gradient:  0.15772656979814026
iteration : 13590
train acc:  0.8203125
train loss:  0.37936121225357056
train gradient:  0.08306015734356446
iteration : 13591
train acc:  0.7578125
train loss:  0.4801453649997711
train gradient:  0.11005097077781985
iteration : 13592
train acc:  0.6953125
train loss:  0.5342534780502319
train gradient:  0.1765238892420513
iteration : 13593
train acc:  0.7734375
train loss:  0.43806231021881104
train gradient:  0.11625401161341839
iteration : 13594
train acc:  0.78125
train loss:  0.4755290448665619
train gradient:  0.1148991123736032
iteration : 13595
train acc:  0.765625
train loss:  0.4729040861129761
train gradient:  0.10695045043988487
iteration : 13596
train acc:  0.7734375
train loss:  0.4435921311378479
train gradient:  0.1210000150924072
iteration : 13597
train acc:  0.6796875
train loss:  0.504014790058136
train gradient:  0.10331080389387985
iteration : 13598
train acc:  0.7109375
train loss:  0.5047352313995361
train gradient:  0.13457546317620386
iteration : 13599
train acc:  0.75
train loss:  0.49500077962875366
train gradient:  0.12558493652631558
iteration : 13600
train acc:  0.7421875
train loss:  0.5001058578491211
train gradient:  0.10585177197538113
iteration : 13601
train acc:  0.7109375
train loss:  0.532049834728241
train gradient:  0.13176077870814518
iteration : 13602
train acc:  0.7421875
train loss:  0.48095864057540894
train gradient:  0.15200143561745227
iteration : 13603
train acc:  0.6953125
train loss:  0.5061360597610474
train gradient:  0.10812642787758792
iteration : 13604
train acc:  0.78125
train loss:  0.4842735528945923
train gradient:  0.10299686920401989
iteration : 13605
train acc:  0.8125
train loss:  0.4151601493358612
train gradient:  0.08818438693616434
iteration : 13606
train acc:  0.8046875
train loss:  0.43734076619148254
train gradient:  0.09172760821995711
iteration : 13607
train acc:  0.7734375
train loss:  0.45632851123809814
train gradient:  0.10045898541383011
iteration : 13608
train acc:  0.75
train loss:  0.5189586281776428
train gradient:  0.12110796376654742
iteration : 13609
train acc:  0.7109375
train loss:  0.5444701910018921
train gradient:  0.13568014989781307
iteration : 13610
train acc:  0.703125
train loss:  0.553511381149292
train gradient:  0.1612425428502504
iteration : 13611
train acc:  0.71875
train loss:  0.5367477536201477
train gradient:  0.18824654036408006
iteration : 13612
train acc:  0.6484375
train loss:  0.5479716062545776
train gradient:  0.1459591208007519
iteration : 13613
train acc:  0.765625
train loss:  0.4513077139854431
train gradient:  0.11302073929997702
iteration : 13614
train acc:  0.6640625
train loss:  0.5155903697013855
train gradient:  0.12024606035430037
iteration : 13615
train acc:  0.7734375
train loss:  0.4520602822303772
train gradient:  0.13233529476542216
iteration : 13616
train acc:  0.7421875
train loss:  0.5020647048950195
train gradient:  0.15002721810918068
iteration : 13617
train acc:  0.7890625
train loss:  0.41286027431488037
train gradient:  0.09502129216270795
iteration : 13618
train acc:  0.7421875
train loss:  0.47305572032928467
train gradient:  0.1245420131985159
iteration : 13619
train acc:  0.8046875
train loss:  0.44512873888015747
train gradient:  0.10494074837693983
iteration : 13620
train acc:  0.7109375
train loss:  0.5613433122634888
train gradient:  0.15237650115844178
iteration : 13621
train acc:  0.71875
train loss:  0.4745367467403412
train gradient:  0.110506871434572
iteration : 13622
train acc:  0.84375
train loss:  0.4231823682785034
train gradient:  0.08398584741796238
iteration : 13623
train acc:  0.7890625
train loss:  0.47756731510162354
train gradient:  0.10688474412590376
iteration : 13624
train acc:  0.734375
train loss:  0.4666229486465454
train gradient:  0.10265215278627667
iteration : 13625
train acc:  0.8359375
train loss:  0.4127182066440582
train gradient:  0.09663263279047685
iteration : 13626
train acc:  0.84375
train loss:  0.41755211353302
train gradient:  0.11217970425874171
iteration : 13627
train acc:  0.8203125
train loss:  0.4372413158416748
train gradient:  0.12826574784085845
iteration : 13628
train acc:  0.71875
train loss:  0.5137614607810974
train gradient:  0.15145486437381173
iteration : 13629
train acc:  0.7421875
train loss:  0.5059340000152588
train gradient:  0.12596335459696156
iteration : 13630
train acc:  0.7265625
train loss:  0.5099585056304932
train gradient:  0.11659118314534642
iteration : 13631
train acc:  0.7734375
train loss:  0.4892681837081909
train gradient:  0.1277468422849255
iteration : 13632
train acc:  0.6875
train loss:  0.5739051103591919
train gradient:  0.1557784246748885
iteration : 13633
train acc:  0.7890625
train loss:  0.4566229283809662
train gradient:  0.09658324853609207
iteration : 13634
train acc:  0.703125
train loss:  0.5248117446899414
train gradient:  0.1231147399948457
iteration : 13635
train acc:  0.765625
train loss:  0.4284811019897461
train gradient:  0.08892320332211542
iteration : 13636
train acc:  0.7265625
train loss:  0.504814863204956
train gradient:  0.1291671836491004
iteration : 13637
train acc:  0.75
train loss:  0.43357640504837036
train gradient:  0.0923812501210009
iteration : 13638
train acc:  0.734375
train loss:  0.5410757064819336
train gradient:  0.12789223686938717
iteration : 13639
train acc:  0.7578125
train loss:  0.5001620650291443
train gradient:  0.13039848714856844
iteration : 13640
train acc:  0.75
train loss:  0.4927729666233063
train gradient:  0.11532491956699949
iteration : 13641
train acc:  0.75
train loss:  0.5060746669769287
train gradient:  0.12356737326882504
iteration : 13642
train acc:  0.765625
train loss:  0.43536102771759033
train gradient:  0.0991387367421574
iteration : 13643
train acc:  0.78125
train loss:  0.4758364260196686
train gradient:  0.10453001251248858
iteration : 13644
train acc:  0.796875
train loss:  0.4741690158843994
train gradient:  0.12683633503856157
iteration : 13645
train acc:  0.640625
train loss:  0.5710354447364807
train gradient:  0.18341140391143596
iteration : 13646
train acc:  0.7578125
train loss:  0.46678268909454346
train gradient:  0.1149270378050865
iteration : 13647
train acc:  0.78125
train loss:  0.4828229546546936
train gradient:  0.11114456091023092
iteration : 13648
train acc:  0.7734375
train loss:  0.4538570046424866
train gradient:  0.09400052636428811
iteration : 13649
train acc:  0.7265625
train loss:  0.4516141414642334
train gradient:  0.08093785779895338
iteration : 13650
train acc:  0.7734375
train loss:  0.4675039052963257
train gradient:  0.10726091584394487
iteration : 13651
train acc:  0.8203125
train loss:  0.43709200620651245
train gradient:  0.09315740589519834
iteration : 13652
train acc:  0.6953125
train loss:  0.5368272066116333
train gradient:  0.13511902303057743
iteration : 13653
train acc:  0.734375
train loss:  0.48981744050979614
train gradient:  0.10603364937467571
iteration : 13654
train acc:  0.703125
train loss:  0.6049880981445312
train gradient:  0.16189256647861522
iteration : 13655
train acc:  0.71875
train loss:  0.5544136762619019
train gradient:  0.132308561639163
iteration : 13656
train acc:  0.7109375
train loss:  0.5060495734214783
train gradient:  0.13138431347729723
iteration : 13657
train acc:  0.7890625
train loss:  0.42190757393836975
train gradient:  0.12087093293364987
iteration : 13658
train acc:  0.8046875
train loss:  0.4507331848144531
train gradient:  0.11819596771420625
iteration : 13659
train acc:  0.7421875
train loss:  0.5234558582305908
train gradient:  0.12239609132149766
iteration : 13660
train acc:  0.6640625
train loss:  0.5894859433174133
train gradient:  0.1638369752294343
iteration : 13661
train acc:  0.78125
train loss:  0.4421919584274292
train gradient:  0.09320701121085996
iteration : 13662
train acc:  0.7265625
train loss:  0.5256694555282593
train gradient:  0.11806883352610247
iteration : 13663
train acc:  0.671875
train loss:  0.5501289367675781
train gradient:  0.15754115147352749
iteration : 13664
train acc:  0.75
train loss:  0.4826624095439911
train gradient:  0.10221880419934681
iteration : 13665
train acc:  0.7109375
train loss:  0.5002771615982056
train gradient:  0.11390587558671132
iteration : 13666
train acc:  0.6640625
train loss:  0.5854727625846863
train gradient:  0.17220304675932702
iteration : 13667
train acc:  0.7734375
train loss:  0.4873073697090149
train gradient:  0.1132741559000046
iteration : 13668
train acc:  0.7734375
train loss:  0.4589729905128479
train gradient:  0.10190769261762364
iteration : 13669
train acc:  0.6953125
train loss:  0.5007423162460327
train gradient:  0.10667830049175477
iteration : 13670
train acc:  0.78125
train loss:  0.4460418224334717
train gradient:  0.09691586950128654
iteration : 13671
train acc:  0.7265625
train loss:  0.47708702087402344
train gradient:  0.11460530417765019
iteration : 13672
train acc:  0.7421875
train loss:  0.5182434320449829
train gradient:  0.12375593021849414
iteration : 13673
train acc:  0.7578125
train loss:  0.4265126585960388
train gradient:  0.08713282210164355
iteration : 13674
train acc:  0.7421875
train loss:  0.5470444560050964
train gradient:  0.14741529560831573
iteration : 13675
train acc:  0.734375
train loss:  0.4822157323360443
train gradient:  0.12350049881463834
iteration : 13676
train acc:  0.8125
train loss:  0.4363660216331482
train gradient:  0.11893387201263973
iteration : 13677
train acc:  0.7109375
train loss:  0.5089917182922363
train gradient:  0.12481295417324821
iteration : 13678
train acc:  0.8125
train loss:  0.41722896695137024
train gradient:  0.12743116439280783
iteration : 13679
train acc:  0.7890625
train loss:  0.422826886177063
train gradient:  0.11720591631089218
iteration : 13680
train acc:  0.75
train loss:  0.46773815155029297
train gradient:  0.12701468563251267
iteration : 13681
train acc:  0.734375
train loss:  0.528384804725647
train gradient:  0.14967931543437693
iteration : 13682
train acc:  0.7109375
train loss:  0.5240585803985596
train gradient:  0.1357388751435903
iteration : 13683
train acc:  0.6953125
train loss:  0.5416278839111328
train gradient:  0.15053949335286992
iteration : 13684
train acc:  0.796875
train loss:  0.4234253168106079
train gradient:  0.07613321387492335
iteration : 13685
train acc:  0.7421875
train loss:  0.45381468534469604
train gradient:  0.10001527399787712
iteration : 13686
train acc:  0.78125
train loss:  0.38460713624954224
train gradient:  0.07197068255579125
iteration : 13687
train acc:  0.7890625
train loss:  0.48277032375335693
train gradient:  0.11204876236803883
iteration : 13688
train acc:  0.7734375
train loss:  0.468448668718338
train gradient:  0.10043576866440385
iteration : 13689
train acc:  0.7890625
train loss:  0.4264991879463196
train gradient:  0.07912984941897437
iteration : 13690
train acc:  0.7265625
train loss:  0.46803247928619385
train gradient:  0.1522711924435698
iteration : 13691
train acc:  0.71875
train loss:  0.4978792071342468
train gradient:  0.1249385798484705
iteration : 13692
train acc:  0.703125
train loss:  0.5579062700271606
train gradient:  0.18394181949928035
iteration : 13693
train acc:  0.75
train loss:  0.4813370406627655
train gradient:  0.12756814348953405
iteration : 13694
train acc:  0.7890625
train loss:  0.4521682858467102
train gradient:  0.09437069541178457
iteration : 13695
train acc:  0.6796875
train loss:  0.561447024345398
train gradient:  0.226886552796288
iteration : 13696
train acc:  0.7578125
train loss:  0.5079326629638672
train gradient:  0.11707272012884634
iteration : 13697
train acc:  0.765625
train loss:  0.4651106297969818
train gradient:  0.09560157500446985
iteration : 13698
train acc:  0.6953125
train loss:  0.5124938488006592
train gradient:  0.1355544846249303
iteration : 13699
train acc:  0.734375
train loss:  0.579058051109314
train gradient:  0.19627491918227408
iteration : 13700
train acc:  0.796875
train loss:  0.47801151871681213
train gradient:  0.10219706934549026
iteration : 13701
train acc:  0.7578125
train loss:  0.5343383550643921
train gradient:  0.13907777972271432
iteration : 13702
train acc:  0.6875
train loss:  0.5214642882347107
train gradient:  0.13017696658688813
iteration : 13703
train acc:  0.7578125
train loss:  0.46911314129829407
train gradient:  0.0944580732067566
iteration : 13704
train acc:  0.796875
train loss:  0.4814508557319641
train gradient:  0.11631805805010964
iteration : 13705
train acc:  0.7578125
train loss:  0.4721429646015167
train gradient:  0.1296027117710119
iteration : 13706
train acc:  0.7109375
train loss:  0.5187348127365112
train gradient:  0.14988821276497505
iteration : 13707
train acc:  0.7109375
train loss:  0.5708236694335938
train gradient:  0.14468030905944435
iteration : 13708
train acc:  0.734375
train loss:  0.5134127140045166
train gradient:  0.15433146609519816
iteration : 13709
train acc:  0.75
train loss:  0.5129846334457397
train gradient:  0.13284654952947755
iteration : 13710
train acc:  0.78125
train loss:  0.45300695300102234
train gradient:  0.09216234645982663
iteration : 13711
train acc:  0.8046875
train loss:  0.42497095465660095
train gradient:  0.08063278691293933
iteration : 13712
train acc:  0.765625
train loss:  0.4916045069694519
train gradient:  0.13028900774048238
iteration : 13713
train acc:  0.765625
train loss:  0.48712319135665894
train gradient:  0.09978530025886188
iteration : 13714
train acc:  0.7890625
train loss:  0.4664125144481659
train gradient:  0.1445976671969315
iteration : 13715
train acc:  0.7109375
train loss:  0.48401203751564026
train gradient:  0.12942884222539003
iteration : 13716
train acc:  0.671875
train loss:  0.5232025384902954
train gradient:  0.14104291597044347
iteration : 13717
train acc:  0.7578125
train loss:  0.41084104776382446
train gradient:  0.07709168861155696
iteration : 13718
train acc:  0.796875
train loss:  0.4836050271987915
train gradient:  0.09701974897279236
iteration : 13719
train acc:  0.7734375
train loss:  0.43548983335494995
train gradient:  0.09202199931876164
iteration : 13720
train acc:  0.71875
train loss:  0.4786660671234131
train gradient:  0.11017532571409311
iteration : 13721
train acc:  0.765625
train loss:  0.4300028681755066
train gradient:  0.10986440114299119
iteration : 13722
train acc:  0.75
train loss:  0.4774482846260071
train gradient:  0.10044357896412688
iteration : 13723
train acc:  0.7734375
train loss:  0.528390109539032
train gradient:  0.15275530011250005
iteration : 13724
train acc:  0.796875
train loss:  0.423001766204834
train gradient:  0.10719081430334049
iteration : 13725
train acc:  0.71875
train loss:  0.5528411865234375
train gradient:  0.17925442469154526
iteration : 13726
train acc:  0.6953125
train loss:  0.49077701568603516
train gradient:  0.11237107853192649
iteration : 13727
train acc:  0.734375
train loss:  0.5084944367408752
train gradient:  0.12521934795678386
iteration : 13728
train acc:  0.7265625
train loss:  0.5415282845497131
train gradient:  0.168677831768473
iteration : 13729
train acc:  0.7421875
train loss:  0.5332973599433899
train gradient:  0.12472706743238182
iteration : 13730
train acc:  0.78125
train loss:  0.45648545026779175
train gradient:  0.09345653455546321
iteration : 13731
train acc:  0.71875
train loss:  0.4899141192436218
train gradient:  0.12328183100876819
iteration : 13732
train acc:  0.6953125
train loss:  0.5385657548904419
train gradient:  0.12692348984763016
iteration : 13733
train acc:  0.7578125
train loss:  0.4787859320640564
train gradient:  0.11235933567080354
iteration : 13734
train acc:  0.7578125
train loss:  0.4677606523036957
train gradient:  0.1286638248078551
iteration : 13735
train acc:  0.71875
train loss:  0.5055407881736755
train gradient:  0.1358415054926072
iteration : 13736
train acc:  0.765625
train loss:  0.43158721923828125
train gradient:  0.0853686646091357
iteration : 13737
train acc:  0.75
train loss:  0.43273085355758667
train gradient:  0.10181308836828723
iteration : 13738
train acc:  0.6953125
train loss:  0.523486852645874
train gradient:  0.14770555366742433
iteration : 13739
train acc:  0.8046875
train loss:  0.41577571630477905
train gradient:  0.0756323471973046
iteration : 13740
train acc:  0.828125
train loss:  0.45423927903175354
train gradient:  0.13718280542556238
iteration : 13741
train acc:  0.765625
train loss:  0.43355792760849
train gradient:  0.0878901422612279
iteration : 13742
train acc:  0.8125
train loss:  0.3906385004520416
train gradient:  0.07879677652517622
iteration : 13743
train acc:  0.765625
train loss:  0.47092199325561523
train gradient:  0.13145586019450517
iteration : 13744
train acc:  0.796875
train loss:  0.4896448850631714
train gradient:  0.1499143114717712
iteration : 13745
train acc:  0.78125
train loss:  0.44848203659057617
train gradient:  0.12630956133291382
iteration : 13746
train acc:  0.7265625
train loss:  0.5022701025009155
train gradient:  0.12338216526522877
iteration : 13747
train acc:  0.7734375
train loss:  0.47251105308532715
train gradient:  0.12031294515820491
iteration : 13748
train acc:  0.828125
train loss:  0.4107229709625244
train gradient:  0.10671750399901489
iteration : 13749
train acc:  0.71875
train loss:  0.5089366436004639
train gradient:  0.11787117499500598
iteration : 13750
train acc:  0.7890625
train loss:  0.4308212995529175
train gradient:  0.09011813643074985
iteration : 13751
train acc:  0.671875
train loss:  0.5477601289749146
train gradient:  0.13634257449221962
iteration : 13752
train acc:  0.75
train loss:  0.5175538659095764
train gradient:  0.15811491176607992
iteration : 13753
train acc:  0.7109375
train loss:  0.4824924170970917
train gradient:  0.10857971769072328
iteration : 13754
train acc:  0.7421875
train loss:  0.5291330814361572
train gradient:  0.15611664129761166
iteration : 13755
train acc:  0.7109375
train loss:  0.5491325855255127
train gradient:  0.16753418874708653
iteration : 13756
train acc:  0.7109375
train loss:  0.5367580652236938
train gradient:  0.14991488962111196
iteration : 13757
train acc:  0.6484375
train loss:  0.6239769458770752
train gradient:  0.1878348637194659
iteration : 13758
train acc:  0.734375
train loss:  0.4714096188545227
train gradient:  0.10624174563181453
iteration : 13759
train acc:  0.75
train loss:  0.4787238836288452
train gradient:  0.11516395671494531
iteration : 13760
train acc:  0.75
train loss:  0.49920326471328735
train gradient:  0.12759703646763065
iteration : 13761
train acc:  0.7578125
train loss:  0.4791078567504883
train gradient:  0.12922873824942302
iteration : 13762
train acc:  0.703125
train loss:  0.5024445056915283
train gradient:  0.11455327408808733
iteration : 13763
train acc:  0.71875
train loss:  0.5665050745010376
train gradient:  0.1820340507656398
iteration : 13764
train acc:  0.8203125
train loss:  0.49495363235473633
train gradient:  0.11348098517982655
iteration : 13765
train acc:  0.703125
train loss:  0.4907931387424469
train gradient:  0.1120451837941482
iteration : 13766
train acc:  0.765625
train loss:  0.4799952805042267
train gradient:  0.12446996760267168
iteration : 13767
train acc:  0.7578125
train loss:  0.43228548765182495
train gradient:  0.10826414598191965
iteration : 13768
train acc:  0.7890625
train loss:  0.4117530584335327
train gradient:  0.0903869913265103
iteration : 13769
train acc:  0.7421875
train loss:  0.5203855633735657
train gradient:  0.14131309449366597
iteration : 13770
train acc:  0.6953125
train loss:  0.5644763708114624
train gradient:  0.1672014378498093
iteration : 13771
train acc:  0.7265625
train loss:  0.47999727725982666
train gradient:  0.15433447147539203
iteration : 13772
train acc:  0.75
train loss:  0.5057920813560486
train gradient:  0.12160276971799196
iteration : 13773
train acc:  0.75
train loss:  0.5045170187950134
train gradient:  0.13024210278986253
iteration : 13774
train acc:  0.75
train loss:  0.5074387788772583
train gradient:  0.14023605074836115
iteration : 13775
train acc:  0.8046875
train loss:  0.4642070531845093
train gradient:  0.1030473597510547
iteration : 13776
train acc:  0.7265625
train loss:  0.48157748579978943
train gradient:  0.12583483347047464
iteration : 13777
train acc:  0.703125
train loss:  0.5945320129394531
train gradient:  0.16404593836687353
iteration : 13778
train acc:  0.796875
train loss:  0.429504930973053
train gradient:  0.11587385646506308
iteration : 13779
train acc:  0.796875
train loss:  0.46117639541625977
train gradient:  0.12079202553077213
iteration : 13780
train acc:  0.7734375
train loss:  0.45784682035446167
train gradient:  0.0940516792613573
iteration : 13781
train acc:  0.71875
train loss:  0.52158522605896
train gradient:  0.10927389606166692
iteration : 13782
train acc:  0.8125
train loss:  0.46779507398605347
train gradient:  0.10486804534161391
iteration : 13783
train acc:  0.7109375
train loss:  0.5683636665344238
train gradient:  0.1445116902684686
iteration : 13784
train acc:  0.75
train loss:  0.48486456274986267
train gradient:  0.08582373901497428
iteration : 13785
train acc:  0.78125
train loss:  0.4108423888683319
train gradient:  0.09360381207058875
iteration : 13786
train acc:  0.7265625
train loss:  0.5009070634841919
train gradient:  0.1461415438604829
iteration : 13787
train acc:  0.765625
train loss:  0.43830591440200806
train gradient:  0.09283280955589818
iteration : 13788
train acc:  0.765625
train loss:  0.4521488547325134
train gradient:  0.11107344618312388
iteration : 13789
train acc:  0.7578125
train loss:  0.5386179089546204
train gradient:  0.12289602157620176
iteration : 13790
train acc:  0.7890625
train loss:  0.4783223867416382
train gradient:  0.10639761637145964
iteration : 13791
train acc:  0.75
train loss:  0.5021229386329651
train gradient:  0.10706724567001814
iteration : 13792
train acc:  0.7734375
train loss:  0.46764296293258667
train gradient:  0.12355588883026035
iteration : 13793
train acc:  0.7265625
train loss:  0.512326717376709
train gradient:  0.14146452479624327
iteration : 13794
train acc:  0.75
train loss:  0.48590758442878723
train gradient:  0.11331977326703457
iteration : 13795
train acc:  0.71875
train loss:  0.5376282930374146
train gradient:  0.1255203057169609
iteration : 13796
train acc:  0.734375
train loss:  0.4875956177711487
train gradient:  0.11989790410133973
iteration : 13797
train acc:  0.796875
train loss:  0.49157071113586426
train gradient:  0.09914497866613207
iteration : 13798
train acc:  0.8046875
train loss:  0.40610986948013306
train gradient:  0.07341293377254772
iteration : 13799
train acc:  0.671875
train loss:  0.5689520239830017
train gradient:  0.15164477363455364
iteration : 13800
train acc:  0.703125
train loss:  0.4835854768753052
train gradient:  0.1011123509045295
iteration : 13801
train acc:  0.71875
train loss:  0.49388211965560913
train gradient:  0.15248744051014207
iteration : 13802
train acc:  0.828125
train loss:  0.4033762514591217
train gradient:  0.06935851925369045
iteration : 13803
train acc:  0.75
train loss:  0.46277916431427
train gradient:  0.10608744617588242
iteration : 13804
train acc:  0.7578125
train loss:  0.5181291103363037
train gradient:  0.12585348673120947
iteration : 13805
train acc:  0.7265625
train loss:  0.46748122572898865
train gradient:  0.10438088559420855
iteration : 13806
train acc:  0.78125
train loss:  0.48294270038604736
train gradient:  0.0992994533159271
iteration : 13807
train acc:  0.7578125
train loss:  0.49493667483329773
train gradient:  0.14750834609281469
iteration : 13808
train acc:  0.6953125
train loss:  0.5299429893493652
train gradient:  0.12510666237991092
iteration : 13809
train acc:  0.71875
train loss:  0.5169757604598999
train gradient:  0.1973544392029773
iteration : 13810
train acc:  0.6953125
train loss:  0.4917641878128052
train gradient:  0.1188197096369509
iteration : 13811
train acc:  0.7421875
train loss:  0.48554158210754395
train gradient:  0.1257913289315797
iteration : 13812
train acc:  0.7578125
train loss:  0.47010746598243713
train gradient:  0.11131987121350607
iteration : 13813
train acc:  0.6796875
train loss:  0.5091261267662048
train gradient:  0.09834443460779475
iteration : 13814
train acc:  0.7421875
train loss:  0.4913691282272339
train gradient:  0.11551701174170304
iteration : 13815
train acc:  0.7890625
train loss:  0.4905075132846832
train gradient:  0.12475099111013171
iteration : 13816
train acc:  0.703125
train loss:  0.47321927547454834
train gradient:  0.10005174544500478
iteration : 13817
train acc:  0.7265625
train loss:  0.49090129137039185
train gradient:  0.11799490250886549
iteration : 13818
train acc:  0.78125
train loss:  0.449535608291626
train gradient:  0.0973710336434075
iteration : 13819
train acc:  0.7421875
train loss:  0.47065848112106323
train gradient:  0.10611988383442551
iteration : 13820
train acc:  0.640625
train loss:  0.61981201171875
train gradient:  0.1586356411548019
iteration : 13821
train acc:  0.71875
train loss:  0.4641869366168976
train gradient:  0.12460921999300978
iteration : 13822
train acc:  0.78125
train loss:  0.4404105544090271
train gradient:  0.09747423138333049
iteration : 13823
train acc:  0.75
train loss:  0.46065735816955566
train gradient:  0.11839979372732717
iteration : 13824
train acc:  0.734375
train loss:  0.5285480618476868
train gradient:  0.16375849815044247
iteration : 13825
train acc:  0.6875
train loss:  0.5554953813552856
train gradient:  0.15415183388969722
iteration : 13826
train acc:  0.703125
train loss:  0.5071805715560913
train gradient:  0.13231220736136848
iteration : 13827
train acc:  0.8046875
train loss:  0.4126058518886566
train gradient:  0.10430929547431533
iteration : 13828
train acc:  0.7109375
train loss:  0.5241008400917053
train gradient:  0.14215293903250587
iteration : 13829
train acc:  0.7890625
train loss:  0.4296426773071289
train gradient:  0.09517461300565745
iteration : 13830
train acc:  0.71875
train loss:  0.573724091053009
train gradient:  0.15133490607053415
iteration : 13831
train acc:  0.671875
train loss:  0.577937126159668
train gradient:  0.14425496632237056
iteration : 13832
train acc:  0.765625
train loss:  0.4461889863014221
train gradient:  0.11989506860259533
iteration : 13833
train acc:  0.7890625
train loss:  0.43344902992248535
train gradient:  0.09451766206067203
iteration : 13834
train acc:  0.703125
train loss:  0.4823729991912842
train gradient:  0.0991738235748719
iteration : 13835
train acc:  0.6953125
train loss:  0.4979037642478943
train gradient:  0.10204382173865942
iteration : 13836
train acc:  0.7421875
train loss:  0.4647009074687958
train gradient:  0.09786847061717092
iteration : 13837
train acc:  0.75
train loss:  0.49384093284606934
train gradient:  0.11095104985237311
iteration : 13838
train acc:  0.7265625
train loss:  0.5256169438362122
train gradient:  0.12640478144655667
iteration : 13839
train acc:  0.7734375
train loss:  0.4442339539527893
train gradient:  0.09583687209976431
iteration : 13840
train acc:  0.7265625
train loss:  0.5341308116912842
train gradient:  0.13591464547719967
iteration : 13841
train acc:  0.84375
train loss:  0.36050736904144287
train gradient:  0.07473678022777917
iteration : 13842
train acc:  0.765625
train loss:  0.46070757508277893
train gradient:  0.11443067945961047
iteration : 13843
train acc:  0.75
train loss:  0.5364241003990173
train gradient:  0.146732276368933
iteration : 13844
train acc:  0.7734375
train loss:  0.4668682813644409
train gradient:  0.09455784919121124
iteration : 13845
train acc:  0.765625
train loss:  0.47548365592956543
train gradient:  0.1006286463341458
iteration : 13846
train acc:  0.6875
train loss:  0.5239518880844116
train gradient:  0.15599303850430368
iteration : 13847
train acc:  0.6953125
train loss:  0.5007081031799316
train gradient:  0.11305030983767345
iteration : 13848
train acc:  0.796875
train loss:  0.4554600715637207
train gradient:  0.12037849814278793
iteration : 13849
train acc:  0.7734375
train loss:  0.4890913963317871
train gradient:  0.10472233223215938
iteration : 13850
train acc:  0.6953125
train loss:  0.5592149496078491
train gradient:  0.13656976759325196
iteration : 13851
train acc:  0.75
train loss:  0.493274450302124
train gradient:  0.12247855993258966
iteration : 13852
train acc:  0.703125
train loss:  0.5465216636657715
train gradient:  0.1447629784874686
iteration : 13853
train acc:  0.7890625
train loss:  0.45000800490379333
train gradient:  0.09486864210325094
iteration : 13854
train acc:  0.71875
train loss:  0.5102630257606506
train gradient:  0.14873133664818267
iteration : 13855
train acc:  0.734375
train loss:  0.5168205499649048
train gradient:  0.11905955269574045
iteration : 13856
train acc:  0.71875
train loss:  0.4675300419330597
train gradient:  0.10748679456006675
iteration : 13857
train acc:  0.7109375
train loss:  0.5447033047676086
train gradient:  0.12008199478840721
iteration : 13858
train acc:  0.7734375
train loss:  0.4695209264755249
train gradient:  0.11490064743762743
iteration : 13859
train acc:  0.78125
train loss:  0.4328473210334778
train gradient:  0.08604385259927555
iteration : 13860
train acc:  0.765625
train loss:  0.46527379751205444
train gradient:  0.10026254267923253
iteration : 13861
train acc:  0.703125
train loss:  0.5250225067138672
train gradient:  0.12314232649607135
iteration : 13862
train acc:  0.78125
train loss:  0.4617408514022827
train gradient:  0.08866592521041078
iteration : 13863
train acc:  0.796875
train loss:  0.444564551115036
train gradient:  0.09943238912662859
iteration : 13864
train acc:  0.7734375
train loss:  0.48890817165374756
train gradient:  0.15834348029527975
iteration : 13865
train acc:  0.6484375
train loss:  0.6127340197563171
train gradient:  0.16813485918235527
iteration : 13866
train acc:  0.765625
train loss:  0.4707595705986023
train gradient:  0.10395520052732467
iteration : 13867
train acc:  0.765625
train loss:  0.461544394493103
train gradient:  0.09773699655342602
iteration : 13868
train acc:  0.75
train loss:  0.5108368396759033
train gradient:  0.15400405686900492
iteration : 13869
train acc:  0.7421875
train loss:  0.4585248827934265
train gradient:  0.10290775518170107
iteration : 13870
train acc:  0.7578125
train loss:  0.4620145559310913
train gradient:  0.09537873392815108
iteration : 13871
train acc:  0.7421875
train loss:  0.5096667408943176
train gradient:  0.10762674165963484
iteration : 13872
train acc:  0.7734375
train loss:  0.49880990386009216
train gradient:  0.14365063377892212
iteration : 13873
train acc:  0.75
train loss:  0.4946681261062622
train gradient:  0.14059094339746464
iteration : 13874
train acc:  0.703125
train loss:  0.4570581912994385
train gradient:  0.09572491467681993
iteration : 13875
train acc:  0.7578125
train loss:  0.4184173345565796
train gradient:  0.08984361784405354
iteration : 13876
train acc:  0.75
train loss:  0.44571083784103394
train gradient:  0.08566183001327439
iteration : 13877
train acc:  0.8125
train loss:  0.42412418127059937
train gradient:  0.09573609931369473
iteration : 13878
train acc:  0.6953125
train loss:  0.5316616296768188
train gradient:  0.17570531356965274
iteration : 13879
train acc:  0.734375
train loss:  0.46262693405151367
train gradient:  0.12879412046411498
iteration : 13880
train acc:  0.734375
train loss:  0.4650930166244507
train gradient:  0.09666190437529051
iteration : 13881
train acc:  0.6875
train loss:  0.5471898317337036
train gradient:  0.1396683722938098
iteration : 13882
train acc:  0.765625
train loss:  0.4403154253959656
train gradient:  0.13014609361593557
iteration : 13883
train acc:  0.78125
train loss:  0.4088723659515381
train gradient:  0.09624949637267298
iteration : 13884
train acc:  0.765625
train loss:  0.46226048469543457
train gradient:  0.09014445843523643
iteration : 13885
train acc:  0.78125
train loss:  0.47512221336364746
train gradient:  0.12533017488509374
iteration : 13886
train acc:  0.734375
train loss:  0.5178866982460022
train gradient:  0.12711632596281383
iteration : 13887
train acc:  0.7109375
train loss:  0.500457763671875
train gradient:  0.11560097948190715
iteration : 13888
train acc:  0.78125
train loss:  0.45488154888153076
train gradient:  0.14550888791162891
iteration : 13889
train acc:  0.734375
train loss:  0.5312490463256836
train gradient:  0.10763200022603583
iteration : 13890
train acc:  0.7890625
train loss:  0.44266477227211
train gradient:  0.10394384116670946
iteration : 13891
train acc:  0.7890625
train loss:  0.3840329647064209
train gradient:  0.08372843937374654
iteration : 13892
train acc:  0.7109375
train loss:  0.5043651461601257
train gradient:  0.1282859975003138
iteration : 13893
train acc:  0.7421875
train loss:  0.5231293439865112
train gradient:  0.13465887941563465
iteration : 13894
train acc:  0.7421875
train loss:  0.5052248239517212
train gradient:  0.10833725156039467
iteration : 13895
train acc:  0.6875
train loss:  0.5203347206115723
train gradient:  0.13501725936942308
iteration : 13896
train acc:  0.7421875
train loss:  0.5135492086410522
train gradient:  0.11530473331547178
iteration : 13897
train acc:  0.625
train loss:  0.6317389011383057
train gradient:  0.18475406499680574
iteration : 13898
train acc:  0.7109375
train loss:  0.5509182810783386
train gradient:  0.11554044637283828
iteration : 13899
train acc:  0.7421875
train loss:  0.4871171712875366
train gradient:  0.09933435607012372
iteration : 13900
train acc:  0.65625
train loss:  0.5075443387031555
train gradient:  0.13417301326472264
iteration : 13901
train acc:  0.7578125
train loss:  0.5032086372375488
train gradient:  0.1260544297095546
iteration : 13902
train acc:  0.71875
train loss:  0.49108150601387024
train gradient:  0.10838816080402218
iteration : 13903
train acc:  0.7109375
train loss:  0.515991747379303
train gradient:  0.11793484582199873
iteration : 13904
train acc:  0.7578125
train loss:  0.4683487117290497
train gradient:  0.08779265236379082
iteration : 13905
train acc:  0.765625
train loss:  0.48924022912979126
train gradient:  0.13925032406991064
iteration : 13906
train acc:  0.7421875
train loss:  0.4612646996974945
train gradient:  0.11125524824914569
iteration : 13907
train acc:  0.796875
train loss:  0.39290279150009155
train gradient:  0.08791787921816119
iteration : 13908
train acc:  0.7421875
train loss:  0.4773100018501282
train gradient:  0.09816099945386283
iteration : 13909
train acc:  0.7890625
train loss:  0.4260159730911255
train gradient:  0.09657505464257883
iteration : 13910
train acc:  0.671875
train loss:  0.5531890392303467
train gradient:  0.1346223236572125
iteration : 13911
train acc:  0.7890625
train loss:  0.46475890278816223
train gradient:  0.09601229435249424
iteration : 13912
train acc:  0.7578125
train loss:  0.48490846157073975
train gradient:  0.09959572183492392
iteration : 13913
train acc:  0.71875
train loss:  0.567135214805603
train gradient:  0.1476098578296534
iteration : 13914
train acc:  0.7421875
train loss:  0.4776439070701599
train gradient:  0.10511245296293988
iteration : 13915
train acc:  0.7734375
train loss:  0.4751504361629486
train gradient:  0.1269598028263756
iteration : 13916
train acc:  0.7265625
train loss:  0.5046297311782837
train gradient:  0.14085155208197053
iteration : 13917
train acc:  0.7734375
train loss:  0.46904972195625305
train gradient:  0.08778942361176036
iteration : 13918
train acc:  0.7109375
train loss:  0.5219637155532837
train gradient:  0.11389213208313567
iteration : 13919
train acc:  0.796875
train loss:  0.44515281915664673
train gradient:  0.08984629815020963
iteration : 13920
train acc:  0.7578125
train loss:  0.5037119388580322
train gradient:  0.12443722950137616
iteration : 13921
train acc:  0.7578125
train loss:  0.45984137058258057
train gradient:  0.0991689010967433
iteration : 13922
train acc:  0.7734375
train loss:  0.446372926235199
train gradient:  0.1154243147560118
iteration : 13923
train acc:  0.7265625
train loss:  0.5278980731964111
train gradient:  0.13672260506769746
iteration : 13924
train acc:  0.7421875
train loss:  0.4885578453540802
train gradient:  0.11228386464852723
iteration : 13925
train acc:  0.7890625
train loss:  0.46218928694725037
train gradient:  0.0938023870548634
iteration : 13926
train acc:  0.7421875
train loss:  0.48707079887390137
train gradient:  0.11433488543590493
iteration : 13927
train acc:  0.78125
train loss:  0.4364345073699951
train gradient:  0.12191482657901859
iteration : 13928
train acc:  0.734375
train loss:  0.5118979811668396
train gradient:  0.12860344089295286
iteration : 13929
train acc:  0.7578125
train loss:  0.4838479459285736
train gradient:  0.12464519471626542
iteration : 13930
train acc:  0.7734375
train loss:  0.4353563189506531
train gradient:  0.09862038303634375
iteration : 13931
train acc:  0.78125
train loss:  0.4473946690559387
train gradient:  0.08430448089935663
iteration : 13932
train acc:  0.796875
train loss:  0.3983986973762512
train gradient:  0.06147815291788311
iteration : 13933
train acc:  0.8046875
train loss:  0.42364275455474854
train gradient:  0.08384804462590253
iteration : 13934
train acc:  0.765625
train loss:  0.4140165150165558
train gradient:  0.09109032816335959
iteration : 13935
train acc:  0.7578125
train loss:  0.47424906492233276
train gradient:  0.10844577403731544
iteration : 13936
train acc:  0.8046875
train loss:  0.4278004467487335
train gradient:  0.0928947788137066
iteration : 13937
train acc:  0.7890625
train loss:  0.45838379859924316
train gradient:  0.08430855772922484
iteration : 13938
train acc:  0.7109375
train loss:  0.4756283164024353
train gradient:  0.09769877474799514
iteration : 13939
train acc:  0.6484375
train loss:  0.6489830017089844
train gradient:  0.22243342949966263
iteration : 13940
train acc:  0.765625
train loss:  0.43437182903289795
train gradient:  0.08373276574549521
iteration : 13941
train acc:  0.8125
train loss:  0.42104583978652954
train gradient:  0.08118926517523842
iteration : 13942
train acc:  0.78125
train loss:  0.4652792811393738
train gradient:  0.1124028504017246
iteration : 13943
train acc:  0.6875
train loss:  0.5305280685424805
train gradient:  0.09949122163119846
iteration : 13944
train acc:  0.7265625
train loss:  0.46853092312812805
train gradient:  0.12520978658530343
iteration : 13945
train acc:  0.7734375
train loss:  0.46821507811546326
train gradient:  0.11789720266176412
iteration : 13946
train acc:  0.734375
train loss:  0.49408483505249023
train gradient:  0.14866312048272678
iteration : 13947
train acc:  0.71875
train loss:  0.5215529799461365
train gradient:  0.1275103015944003
iteration : 13948
train acc:  0.765625
train loss:  0.45425713062286377
train gradient:  0.10683930664628956
iteration : 13949
train acc:  0.7890625
train loss:  0.463284432888031
train gradient:  0.10994980141259056
iteration : 13950
train acc:  0.7421875
train loss:  0.5395270586013794
train gradient:  0.13094268505840945
iteration : 13951
train acc:  0.71875
train loss:  0.5207505226135254
train gradient:  0.14574972971507502
iteration : 13952
train acc:  0.734375
train loss:  0.487297385931015
train gradient:  0.10884525893724241
iteration : 13953
train acc:  0.7421875
train loss:  0.4998795986175537
train gradient:  0.11700533432730177
iteration : 13954
train acc:  0.734375
train loss:  0.45314016938209534
train gradient:  0.09861029335819908
iteration : 13955
train acc:  0.765625
train loss:  0.5057029724121094
train gradient:  0.11484992213098763
iteration : 13956
train acc:  0.7265625
train loss:  0.5213634967803955
train gradient:  0.12351936709692414
iteration : 13957
train acc:  0.6953125
train loss:  0.5151509046554565
train gradient:  0.15331765436330574
iteration : 13958
train acc:  0.8203125
train loss:  0.41390755772590637
train gradient:  0.09992426357899092
iteration : 13959
train acc:  0.7421875
train loss:  0.49781888723373413
train gradient:  0.1343127526451035
iteration : 13960
train acc:  0.6875
train loss:  0.5786948204040527
train gradient:  0.1901873638838159
iteration : 13961
train acc:  0.78125
train loss:  0.47042208909988403
train gradient:  0.09367613753792875
iteration : 13962
train acc:  0.765625
train loss:  0.4927081763744354
train gradient:  0.10600228527613177
iteration : 13963
train acc:  0.78125
train loss:  0.4716171622276306
train gradient:  0.11052464235445784
iteration : 13964
train acc:  0.7109375
train loss:  0.4935375452041626
train gradient:  0.12308595431028864
iteration : 13965
train acc:  0.75
train loss:  0.5010542869567871
train gradient:  0.12932374170196637
iteration : 13966
train acc:  0.7578125
train loss:  0.4742972254753113
train gradient:  0.10578752172899149
iteration : 13967
train acc:  0.75
train loss:  0.48721548914909363
train gradient:  0.12547522092102104
iteration : 13968
train acc:  0.8125
train loss:  0.4157099425792694
train gradient:  0.07604388474641119
iteration : 13969
train acc:  0.7109375
train loss:  0.5235903263092041
train gradient:  0.14287774122081728
iteration : 13970
train acc:  0.7109375
train loss:  0.5189183354377747
train gradient:  0.12570227428384145
iteration : 13971
train acc:  0.6875
train loss:  0.48935845494270325
train gradient:  0.1049982877320855
iteration : 13972
train acc:  0.765625
train loss:  0.4559381604194641
train gradient:  0.09433329145849319
iteration : 13973
train acc:  0.8046875
train loss:  0.4686635136604309
train gradient:  0.09016174851930062
iteration : 13974
train acc:  0.7890625
train loss:  0.46719273924827576
train gradient:  0.10976636936161276
iteration : 13975
train acc:  0.7734375
train loss:  0.5035021305084229
train gradient:  0.12301279678689171
iteration : 13976
train acc:  0.71875
train loss:  0.5820870995521545
train gradient:  0.1838545477768545
iteration : 13977
train acc:  0.7578125
train loss:  0.5046935081481934
train gradient:  0.179785181267051
iteration : 13978
train acc:  0.703125
train loss:  0.553125262260437
train gradient:  0.1619719407652828
iteration : 13979
train acc:  0.6875
train loss:  0.5283615589141846
train gradient:  0.1295564099764221
iteration : 13980
train acc:  0.796875
train loss:  0.4303692877292633
train gradient:  0.08186964725394039
iteration : 13981
train acc:  0.765625
train loss:  0.4691755771636963
train gradient:  0.11921764503395257
iteration : 13982
train acc:  0.78125
train loss:  0.40902432799339294
train gradient:  0.08444504928197923
iteration : 13983
train acc:  0.7421875
train loss:  0.46338361501693726
train gradient:  0.08998756842055167
iteration : 13984
train acc:  0.7265625
train loss:  0.4783371686935425
train gradient:  0.0970452206751815
iteration : 13985
train acc:  0.7265625
train loss:  0.46928149461746216
train gradient:  0.1343677839035321
iteration : 13986
train acc:  0.78125
train loss:  0.4519829750061035
train gradient:  0.1168536116916656
iteration : 13987
train acc:  0.828125
train loss:  0.40792980790138245
train gradient:  0.0716961803716491
iteration : 13988
train acc:  0.75
train loss:  0.45676320791244507
train gradient:  0.08739498521547347
iteration : 13989
train acc:  0.7578125
train loss:  0.4849669933319092
train gradient:  0.11392320282323246
iteration : 13990
train acc:  0.7734375
train loss:  0.4595358967781067
train gradient:  0.09326428356746441
iteration : 13991
train acc:  0.7578125
train loss:  0.5337185859680176
train gradient:  0.14702910252947554
iteration : 13992
train acc:  0.7421875
train loss:  0.44624605774879456
train gradient:  0.09641967396979163
iteration : 13993
train acc:  0.7578125
train loss:  0.4593367576599121
train gradient:  0.1222623623630812
iteration : 13994
train acc:  0.640625
train loss:  0.5821021795272827
train gradient:  0.17344258314197727
iteration : 13995
train acc:  0.7421875
train loss:  0.45023947954177856
train gradient:  0.09407404083036106
iteration : 13996
train acc:  0.8046875
train loss:  0.42525970935821533
train gradient:  0.09503189522523617
iteration : 13997
train acc:  0.6796875
train loss:  0.5531209707260132
train gradient:  0.1186724425954484
iteration : 13998
train acc:  0.7578125
train loss:  0.4232345223426819
train gradient:  0.10863879233289946
iteration : 13999
train acc:  0.7421875
train loss:  0.49606049060821533
train gradient:  0.10610805461288665
iteration : 14000
train acc:  0.78125
train loss:  0.43780750036239624
train gradient:  0.12042040249171543
iteration : 14001
train acc:  0.71875
train loss:  0.5467344522476196
train gradient:  0.13876625020515526
iteration : 14002
train acc:  0.734375
train loss:  0.5015746355056763
train gradient:  0.10935577778646162
iteration : 14003
train acc:  0.703125
train loss:  0.5504648685455322
train gradient:  0.15407887021129024
iteration : 14004
train acc:  0.734375
train loss:  0.4924114942550659
train gradient:  0.1090036885014676
iteration : 14005
train acc:  0.75
train loss:  0.49013975262641907
train gradient:  0.12117233213688555
iteration : 14006
train acc:  0.6875
train loss:  0.5401393175125122
train gradient:  0.14730446913422213
iteration : 14007
train acc:  0.765625
train loss:  0.4503471553325653
train gradient:  0.0800605416462485
iteration : 14008
train acc:  0.7734375
train loss:  0.4633401334285736
train gradient:  0.09722122348952562
iteration : 14009
train acc:  0.65625
train loss:  0.558892011642456
train gradient:  0.15131839613549125
iteration : 14010
train acc:  0.8203125
train loss:  0.4023231267929077
train gradient:  0.08553524861165193
iteration : 14011
train acc:  0.6953125
train loss:  0.5836502313613892
train gradient:  0.15473443862745562
iteration : 14012
train acc:  0.71875
train loss:  0.5124312043190002
train gradient:  0.11948950169239331
iteration : 14013
train acc:  0.71875
train loss:  0.49599114060401917
train gradient:  0.11341227466023797
iteration : 14014
train acc:  0.78125
train loss:  0.42893528938293457
train gradient:  0.10082280412490108
iteration : 14015
train acc:  0.7578125
train loss:  0.5132274627685547
train gradient:  0.15744394201582712
iteration : 14016
train acc:  0.796875
train loss:  0.42286786437034607
train gradient:  0.08525272291403385
iteration : 14017
train acc:  0.734375
train loss:  0.46303439140319824
train gradient:  0.10329009438306931
iteration : 14018
train acc:  0.765625
train loss:  0.4583801329135895
train gradient:  0.09753548310481107
iteration : 14019
train acc:  0.7578125
train loss:  0.4770735502243042
train gradient:  0.0991819218032436
iteration : 14020
train acc:  0.7109375
train loss:  0.5315439701080322
train gradient:  0.17229033558963136
iteration : 14021
train acc:  0.7578125
train loss:  0.466410756111145
train gradient:  0.13430368025997297
iteration : 14022
train acc:  0.6953125
train loss:  0.5377466678619385
train gradient:  0.17992126355693192
iteration : 14023
train acc:  0.78125
train loss:  0.44417911767959595
train gradient:  0.0986198437999195
iteration : 14024
train acc:  0.765625
train loss:  0.47448229789733887
train gradient:  0.09707436990410494
iteration : 14025
train acc:  0.7578125
train loss:  0.46371763944625854
train gradient:  0.11094660588497669
iteration : 14026
train acc:  0.796875
train loss:  0.4280434250831604
train gradient:  0.07862705496965049
iteration : 14027
train acc:  0.796875
train loss:  0.42336854338645935
train gradient:  0.08242594782852466
iteration : 14028
train acc:  0.828125
train loss:  0.4626580476760864
train gradient:  0.10700136146729672
iteration : 14029
train acc:  0.7421875
train loss:  0.4764547348022461
train gradient:  0.08142036150866479
iteration : 14030
train acc:  0.7578125
train loss:  0.4561641812324524
train gradient:  0.10082574359920167
iteration : 14031
train acc:  0.765625
train loss:  0.49987858533859253
train gradient:  0.11749436521536147
iteration : 14032
train acc:  0.734375
train loss:  0.4682440161705017
train gradient:  0.12670828352469782
iteration : 14033
train acc:  0.734375
train loss:  0.4806686043739319
train gradient:  0.09627290957362307
iteration : 14034
train acc:  0.796875
train loss:  0.5214273929595947
train gradient:  0.12868787978398738
iteration : 14035
train acc:  0.7578125
train loss:  0.5172692537307739
train gradient:  0.09964741712453527
iteration : 14036
train acc:  0.75
train loss:  0.46019184589385986
train gradient:  0.10969674677334151
iteration : 14037
train acc:  0.7578125
train loss:  0.5326426029205322
train gradient:  0.1403915685534006
iteration : 14038
train acc:  0.7109375
train loss:  0.5405292510986328
train gradient:  0.14910078189713216
iteration : 14039
train acc:  0.7890625
train loss:  0.41538506746292114
train gradient:  0.09796932269958296
iteration : 14040
train acc:  0.7890625
train loss:  0.4688967168331146
train gradient:  0.12228636395173573
iteration : 14041
train acc:  0.7890625
train loss:  0.4432094693183899
train gradient:  0.08963828421671025
iteration : 14042
train acc:  0.796875
train loss:  0.47773173451423645
train gradient:  0.12487366831575389
iteration : 14043
train acc:  0.7734375
train loss:  0.45167332887649536
train gradient:  0.10542982791235452
iteration : 14044
train acc:  0.6796875
train loss:  0.5647693872451782
train gradient:  0.12631233134597525
iteration : 14045
train acc:  0.703125
train loss:  0.5155788660049438
train gradient:  0.12270006756854933
iteration : 14046
train acc:  0.7578125
train loss:  0.4890449345111847
train gradient:  0.12627145463724287
iteration : 14047
train acc:  0.7421875
train loss:  0.4855422377586365
train gradient:  0.12919359713780842
iteration : 14048
train acc:  0.734375
train loss:  0.5280251502990723
train gradient:  0.1472014919680798
iteration : 14049
train acc:  0.7265625
train loss:  0.4945414960384369
train gradient:  0.14597631229972996
iteration : 14050
train acc:  0.78125
train loss:  0.4509025812149048
train gradient:  0.09978842498808098
iteration : 14051
train acc:  0.7109375
train loss:  0.5140165090560913
train gradient:  0.15703404443039115
iteration : 14052
train acc:  0.7578125
train loss:  0.47251972556114197
train gradient:  0.12680352443069076
iteration : 14053
train acc:  0.7421875
train loss:  0.5142388939857483
train gradient:  0.13068772617653557
iteration : 14054
train acc:  0.7578125
train loss:  0.4780236780643463
train gradient:  0.09971804758178061
iteration : 14055
train acc:  0.703125
train loss:  0.5351411700248718
train gradient:  0.14146245794057927
iteration : 14056
train acc:  0.796875
train loss:  0.45749109983444214
train gradient:  0.15529157856342762
iteration : 14057
train acc:  0.7265625
train loss:  0.5368984937667847
train gradient:  0.12927255886658193
iteration : 14058
train acc:  0.671875
train loss:  0.5962834358215332
train gradient:  0.1814375808868255
iteration : 14059
train acc:  0.7421875
train loss:  0.5104514360427856
train gradient:  0.1319631899620792
iteration : 14060
train acc:  0.71875
train loss:  0.45865297317504883
train gradient:  0.1164115548357487
iteration : 14061
train acc:  0.765625
train loss:  0.4343101680278778
train gradient:  0.11093048069517082
iteration : 14062
train acc:  0.6953125
train loss:  0.5254292488098145
train gradient:  0.14834378402754048
iteration : 14063
train acc:  0.7734375
train loss:  0.4868752360343933
train gradient:  0.10913281177420463
iteration : 14064
train acc:  0.7890625
train loss:  0.4106360375881195
train gradient:  0.11647787941963562
iteration : 14065
train acc:  0.796875
train loss:  0.4373275935649872
train gradient:  0.11613849471118322
iteration : 14066
train acc:  0.7265625
train loss:  0.4945823550224304
train gradient:  0.1173300486848331
iteration : 14067
train acc:  0.78125
train loss:  0.44858500361442566
train gradient:  0.09513379771133448
iteration : 14068
train acc:  0.7421875
train loss:  0.44111204147338867
train gradient:  0.09690304800204862
iteration : 14069
train acc:  0.7109375
train loss:  0.5146356821060181
train gradient:  0.12976804923430021
iteration : 14070
train acc:  0.7578125
train loss:  0.4959149956703186
train gradient:  0.14322015607270355
iteration : 14071
train acc:  0.71875
train loss:  0.5349434614181519
train gradient:  0.1194513453676461
iteration : 14072
train acc:  0.78125
train loss:  0.47382858395576477
train gradient:  0.1231652529520222
iteration : 14073
train acc:  0.734375
train loss:  0.5017879009246826
train gradient:  0.1591949951588042
iteration : 14074
train acc:  0.796875
train loss:  0.4435194730758667
train gradient:  0.10330277493345588
iteration : 14075
train acc:  0.7578125
train loss:  0.513044536113739
train gradient:  0.12828432959828234
iteration : 14076
train acc:  0.7421875
train loss:  0.5071194171905518
train gradient:  0.10168645669977976
iteration : 14077
train acc:  0.71875
train loss:  0.49829763174057007
train gradient:  0.10111906112775666
iteration : 14078
train acc:  0.7734375
train loss:  0.4606272578239441
train gradient:  0.08096550249494999
iteration : 14079
train acc:  0.7109375
train loss:  0.5272805690765381
train gradient:  0.12042875786628397
iteration : 14080
train acc:  0.625
train loss:  0.6372982859611511
train gradient:  0.17034657068211126
iteration : 14081
train acc:  0.765625
train loss:  0.47237497568130493
train gradient:  0.13379893314948904
iteration : 14082
train acc:  0.65625
train loss:  0.5468206405639648
train gradient:  0.1243029506362055
iteration : 14083
train acc:  0.71875
train loss:  0.4778897166252136
train gradient:  0.11633527232787699
iteration : 14084
train acc:  0.7265625
train loss:  0.5119030475616455
train gradient:  0.11450075626695522
iteration : 14085
train acc:  0.75
train loss:  0.4868224263191223
train gradient:  0.12662989151084572
iteration : 14086
train acc:  0.8046875
train loss:  0.41993194818496704
train gradient:  0.09509536085642452
iteration : 14087
train acc:  0.6796875
train loss:  0.53835129737854
train gradient:  0.1218213464384773
iteration : 14088
train acc:  0.7578125
train loss:  0.4704296290874481
train gradient:  0.11400664733044992
iteration : 14089
train acc:  0.734375
train loss:  0.5307198762893677
train gradient:  0.12489760017443582
iteration : 14090
train acc:  0.765625
train loss:  0.4192080497741699
train gradient:  0.08925510185733335
iteration : 14091
train acc:  0.734375
train loss:  0.5196464657783508
train gradient:  0.10798841315165138
iteration : 14092
train acc:  0.75
train loss:  0.4797629714012146
train gradient:  0.11244220629920014
iteration : 14093
train acc:  0.84375
train loss:  0.37741345167160034
train gradient:  0.07021996175839562
iteration : 14094
train acc:  0.703125
train loss:  0.566864013671875
train gradient:  0.17356773044426835
iteration : 14095
train acc:  0.7421875
train loss:  0.4693228602409363
train gradient:  0.10224218043858165
iteration : 14096
train acc:  0.703125
train loss:  0.5089758634567261
train gradient:  0.12554234051316587
iteration : 14097
train acc:  0.703125
train loss:  0.5332951545715332
train gradient:  0.10754030528202672
iteration : 14098
train acc:  0.7421875
train loss:  0.5044720768928528
train gradient:  0.11480660467071863
iteration : 14099
train acc:  0.7890625
train loss:  0.47850140929222107
train gradient:  0.1322914638544355
iteration : 14100
train acc:  0.796875
train loss:  0.4218142032623291
train gradient:  0.08153077755435155
iteration : 14101
train acc:  0.6953125
train loss:  0.5988284349441528
train gradient:  0.1379640093364667
iteration : 14102
train acc:  0.8125
train loss:  0.4376799762248993
train gradient:  0.0979312982467357
iteration : 14103
train acc:  0.7109375
train loss:  0.5145683884620667
train gradient:  0.13948267128488906
iteration : 14104
train acc:  0.7265625
train loss:  0.5053951740264893
train gradient:  0.13204508606642196
iteration : 14105
train acc:  0.75
train loss:  0.48341119289398193
train gradient:  0.12120135762250152
iteration : 14106
train acc:  0.75
train loss:  0.460637629032135
train gradient:  0.10282593091432499
iteration : 14107
train acc:  0.828125
train loss:  0.4635085463523865
train gradient:  0.10014604530982066
iteration : 14108
train acc:  0.71875
train loss:  0.5013184547424316
train gradient:  0.1394125961463118
iteration : 14109
train acc:  0.8046875
train loss:  0.45831817388534546
train gradient:  0.14206635650545207
iteration : 14110
train acc:  0.7890625
train loss:  0.4450194239616394
train gradient:  0.09481166618980416
iteration : 14111
train acc:  0.6796875
train loss:  0.5472075939178467
train gradient:  0.12755543988290519
iteration : 14112
train acc:  0.8203125
train loss:  0.4107082486152649
train gradient:  0.08743073130048211
iteration : 14113
train acc:  0.78125
train loss:  0.4752890467643738
train gradient:  0.09849225544544972
iteration : 14114
train acc:  0.7109375
train loss:  0.48059409856796265
train gradient:  0.08078391192908753
iteration : 14115
train acc:  0.7890625
train loss:  0.434349000453949
train gradient:  0.09923193580140091
iteration : 14116
train acc:  0.8359375
train loss:  0.4141662120819092
train gradient:  0.09306810047837348
iteration : 14117
train acc:  0.71875
train loss:  0.49105536937713623
train gradient:  0.10243622926980399
iteration : 14118
train acc:  0.734375
train loss:  0.4905266761779785
train gradient:  0.11945683638939981
iteration : 14119
train acc:  0.71875
train loss:  0.4941141903400421
train gradient:  0.11471918511438235
iteration : 14120
train acc:  0.7734375
train loss:  0.46867096424102783
train gradient:  0.12175343908156601
iteration : 14121
train acc:  0.8046875
train loss:  0.45217204093933105
train gradient:  0.09124461255467622
iteration : 14122
train acc:  0.75
train loss:  0.5153912901878357
train gradient:  0.14311242718938463
iteration : 14123
train acc:  0.765625
train loss:  0.45418888330459595
train gradient:  0.11406799012948572
iteration : 14124
train acc:  0.734375
train loss:  0.476706326007843
train gradient:  0.12797382588349654
iteration : 14125
train acc:  0.7265625
train loss:  0.4927148222923279
train gradient:  0.1348427140949807
iteration : 14126
train acc:  0.7890625
train loss:  0.4134480953216553
train gradient:  0.11375431393672297
iteration : 14127
train acc:  0.765625
train loss:  0.4922950267791748
train gradient:  0.10984010371959728
iteration : 14128
train acc:  0.734375
train loss:  0.4967125952243805
train gradient:  0.10003116427982002
iteration : 14129
train acc:  0.671875
train loss:  0.5393707752227783
train gradient:  0.15106296237183292
iteration : 14130
train acc:  0.7734375
train loss:  0.41871610283851624
train gradient:  0.09501304146857645
iteration : 14131
train acc:  0.796875
train loss:  0.42526912689208984
train gradient:  0.08816847851725784
iteration : 14132
train acc:  0.71875
train loss:  0.5149986743927002
train gradient:  0.1575695216463484
iteration : 14133
train acc:  0.7578125
train loss:  0.44622373580932617
train gradient:  0.08845060170291599
iteration : 14134
train acc:  0.8125
train loss:  0.4615415334701538
train gradient:  0.09471121869751953
iteration : 14135
train acc:  0.734375
train loss:  0.5252232551574707
train gradient:  0.12428667598946198
iteration : 14136
train acc:  0.7421875
train loss:  0.43230241537094116
train gradient:  0.1089662463070871
iteration : 14137
train acc:  0.828125
train loss:  0.4428589344024658
train gradient:  0.07809868408385012
iteration : 14138
train acc:  0.7265625
train loss:  0.4879645109176636
train gradient:  0.10784781701115558
iteration : 14139
train acc:  0.65625
train loss:  0.5293314456939697
train gradient:  0.12425172839073591
iteration : 14140
train acc:  0.7421875
train loss:  0.49805036187171936
train gradient:  0.10705804409776946
iteration : 14141
train acc:  0.75
train loss:  0.5089258551597595
train gradient:  0.13241611923458024
iteration : 14142
train acc:  0.7421875
train loss:  0.5032545328140259
train gradient:  0.12696351782704945
iteration : 14143
train acc:  0.8125
train loss:  0.3945040702819824
train gradient:  0.08036923456420711
iteration : 14144
train acc:  0.703125
train loss:  0.5281864404678345
train gradient:  0.14689736898284111
iteration : 14145
train acc:  0.7421875
train loss:  0.4724125862121582
train gradient:  0.0945978113126375
iteration : 14146
train acc:  0.7578125
train loss:  0.46454888582229614
train gradient:  0.11616049639285883
iteration : 14147
train acc:  0.6875
train loss:  0.5237623453140259
train gradient:  0.14644259669773163
iteration : 14148
train acc:  0.75
train loss:  0.4947214722633362
train gradient:  0.11045954456444496
iteration : 14149
train acc:  0.71875
train loss:  0.5653942823410034
train gradient:  0.1377799828342432
iteration : 14150
train acc:  0.8125
train loss:  0.4286012649536133
train gradient:  0.1009895294223379
iteration : 14151
train acc:  0.765625
train loss:  0.5330682396888733
train gradient:  0.1336057063286477
iteration : 14152
train acc:  0.78125
train loss:  0.49172043800354004
train gradient:  0.10319547780480877
iteration : 14153
train acc:  0.71875
train loss:  0.519576907157898
train gradient:  0.11518850474112831
iteration : 14154
train acc:  0.6640625
train loss:  0.5448697805404663
train gradient:  0.13357003326249153
iteration : 14155
train acc:  0.8046875
train loss:  0.4164942502975464
train gradient:  0.07678574284887953
iteration : 14156
train acc:  0.75
train loss:  0.5025731325149536
train gradient:  0.0940737984703097
iteration : 14157
train acc:  0.6953125
train loss:  0.4942200481891632
train gradient:  0.0989335816835741
iteration : 14158
train acc:  0.7734375
train loss:  0.43649306893348694
train gradient:  0.08239315347066546
iteration : 14159
train acc:  0.765625
train loss:  0.47877684235572815
train gradient:  0.142106209015941
iteration : 14160
train acc:  0.7421875
train loss:  0.5325794219970703
train gradient:  0.13381475114495028
iteration : 14161
train acc:  0.7890625
train loss:  0.4655002951622009
train gradient:  0.10073825322059998
iteration : 14162
train acc:  0.71875
train loss:  0.5029195547103882
train gradient:  0.11883679264490363
iteration : 14163
train acc:  0.734375
train loss:  0.5160754919052124
train gradient:  0.1387281502961721
iteration : 14164
train acc:  0.7265625
train loss:  0.4994492828845978
train gradient:  0.16692327952272207
iteration : 14165
train acc:  0.7109375
train loss:  0.5435947179794312
train gradient:  0.1516280111860311
iteration : 14166
train acc:  0.71875
train loss:  0.5268281698226929
train gradient:  0.11409881830578371
iteration : 14167
train acc:  0.78125
train loss:  0.48428159952163696
train gradient:  0.13485411879985132
iteration : 14168
train acc:  0.765625
train loss:  0.4516063332557678
train gradient:  0.11465951736319845
iteration : 14169
train acc:  0.8359375
train loss:  0.44583213329315186
train gradient:  0.1345340910890346
iteration : 14170
train acc:  0.703125
train loss:  0.6333081722259521
train gradient:  0.21760690917972025
iteration : 14171
train acc:  0.7265625
train loss:  0.5249443054199219
train gradient:  0.11496903721607342
iteration : 14172
train acc:  0.7578125
train loss:  0.4397803246974945
train gradient:  0.07644385153699906
iteration : 14173
train acc:  0.78125
train loss:  0.4457334578037262
train gradient:  0.08464459797625404
iteration : 14174
train acc:  0.7890625
train loss:  0.4208836853504181
train gradient:  0.08192585980221614
iteration : 14175
train acc:  0.703125
train loss:  0.5238991975784302
train gradient:  0.11873092294650889
iteration : 14176
train acc:  0.7890625
train loss:  0.4493362307548523
train gradient:  0.12008639289973515
iteration : 14177
train acc:  0.828125
train loss:  0.3983937203884125
train gradient:  0.07571233406361957
iteration : 14178
train acc:  0.765625
train loss:  0.506883442401886
train gradient:  0.1427378260400495
iteration : 14179
train acc:  0.7265625
train loss:  0.5235686898231506
train gradient:  0.12502832902729652
iteration : 14180
train acc:  0.7265625
train loss:  0.539414644241333
train gradient:  0.13491331854597963
iteration : 14181
train acc:  0.7265625
train loss:  0.5275220274925232
train gradient:  0.1512984133033434
iteration : 14182
train acc:  0.7890625
train loss:  0.471290647983551
train gradient:  0.12988060224495085
iteration : 14183
train acc:  0.6875
train loss:  0.5006467700004578
train gradient:  0.11423427018888954
iteration : 14184
train acc:  0.8125
train loss:  0.4377874732017517
train gradient:  0.1292999745816762
iteration : 14185
train acc:  0.828125
train loss:  0.41626274585723877
train gradient:  0.08849747090645309
iteration : 14186
train acc:  0.8333333333333334
train loss:  0.43594443798065186
train gradient:  0.6191702144426086
val acc:  0.734230808234459
val f1:  0.7642712263832735
val confusion matrix:  [[59836 38774]
 [13641 84969]]

----------------------------------------new_epoch--------------------------------------

epoch:  1
iteration : 0
train acc:  0.765625
train loss:  0.5009323954582214
train gradient:  0.11239309830114573
iteration : 1
train acc:  0.7578125
train loss:  0.5019599199295044
train gradient:  0.11570035669316814
iteration : 2
train acc:  0.703125
train loss:  0.5459434986114502
train gradient:  0.147187026933517
iteration : 3
train acc:  0.703125
train loss:  0.5021603107452393
train gradient:  0.1190192086370292
iteration : 4
train acc:  0.8203125
train loss:  0.412850558757782
train gradient:  0.07182665677899885
iteration : 5
train acc:  0.828125
train loss:  0.4294523000717163
train gradient:  0.10040440311283905
iteration : 6
train acc:  0.765625
train loss:  0.4838316738605499
train gradient:  0.12575142755118232
iteration : 7
train acc:  0.78125
train loss:  0.4853631854057312
train gradient:  0.10030839106291928
iteration : 8
train acc:  0.7265625
train loss:  0.4752177596092224
train gradient:  0.1381599486251717
iteration : 9
train acc:  0.734375
train loss:  0.4556424021720886
train gradient:  0.09906486335753177
iteration : 10
train acc:  0.796875
train loss:  0.4429442882537842
train gradient:  0.10881557973077294
iteration : 11
train acc:  0.765625
train loss:  0.5060006380081177
train gradient:  0.128581369685867
iteration : 12
train acc:  0.71875
train loss:  0.4798777103424072
train gradient:  0.10976374202161505
iteration : 13
train acc:  0.7421875
train loss:  0.460693359375
train gradient:  0.08960533082687364
iteration : 14
train acc:  0.7578125
train loss:  0.46334853768348694
train gradient:  0.08777602433984917
iteration : 15
train acc:  0.765625
train loss:  0.4446989595890045
train gradient:  0.09010342269390396
iteration : 16
train acc:  0.71875
train loss:  0.5768591165542603
train gradient:  0.12734250107223277
iteration : 17
train acc:  0.7578125
train loss:  0.46968191862106323
train gradient:  0.08860928071421643
iteration : 18
train acc:  0.6796875
train loss:  0.5698291063308716
train gradient:  0.1816215908468946
iteration : 19
train acc:  0.6484375
train loss:  0.625484049320221
train gradient:  0.22012902742040585
iteration : 20
train acc:  0.796875
train loss:  0.450530081987381
train gradient:  0.0993008330109414
iteration : 21
train acc:  0.7734375
train loss:  0.45231157541275024
train gradient:  0.12061181467909983
iteration : 22
train acc:  0.765625
train loss:  0.4293055534362793
train gradient:  0.08593810026620005
iteration : 23
train acc:  0.7578125
train loss:  0.44026535749435425
train gradient:  0.09689694502245494
iteration : 24
train acc:  0.796875
train loss:  0.4539691209793091
train gradient:  0.10533378234296059
iteration : 25
train acc:  0.796875
train loss:  0.47166964411735535
train gradient:  0.11271167110425832
iteration : 26
train acc:  0.75
train loss:  0.4539424777030945
train gradient:  0.10644168165644358
iteration : 27
train acc:  0.7421875
train loss:  0.4743949770927429
train gradient:  0.11387319767306436
iteration : 28
train acc:  0.6953125
train loss:  0.4995231628417969
train gradient:  0.11630167750762523
iteration : 29
train acc:  0.7109375
train loss:  0.47910091280937195
train gradient:  0.09755509713929017
iteration : 30
train acc:  0.78125
train loss:  0.4915560483932495
train gradient:  0.1210871657025545
iteration : 31
train acc:  0.7734375
train loss:  0.47310349345207214
train gradient:  0.1088588576132752
iteration : 32
train acc:  0.71875
train loss:  0.48477059602737427
train gradient:  0.11246305672122331
iteration : 33
train acc:  0.75
train loss:  0.4620693624019623
train gradient:  0.09706497684481288
iteration : 34
train acc:  0.7421875
train loss:  0.4962863326072693
train gradient:  0.14658182644189094
iteration : 35
train acc:  0.6953125
train loss:  0.5778488516807556
train gradient:  0.1594977170836006
iteration : 36
train acc:  0.734375
train loss:  0.4932522177696228
train gradient:  0.09827859734343529
iteration : 37
train acc:  0.796875
train loss:  0.3858863115310669
train gradient:  0.09717697273873971
iteration : 38
train acc:  0.7734375
train loss:  0.45885661244392395
train gradient:  0.11181442288809441
iteration : 39
train acc:  0.75
train loss:  0.451422780752182
train gradient:  0.09053784239780087
iteration : 40
train acc:  0.703125
train loss:  0.5438335537910461
train gradient:  0.1445575433599341
iteration : 41
train acc:  0.7265625
train loss:  0.5415771007537842
train gradient:  0.1595044940069467
iteration : 42
train acc:  0.7421875
train loss:  0.4368715286254883
train gradient:  0.09343789372481306
iteration : 43
train acc:  0.765625
train loss:  0.4231717586517334
train gradient:  0.08840773483401666
iteration : 44
train acc:  0.71875
train loss:  0.5352671146392822
train gradient:  0.1655875896805935
iteration : 45
train acc:  0.734375
train loss:  0.4689086675643921
train gradient:  0.1139542054485069
iteration : 46
train acc:  0.6875
train loss:  0.5262120366096497
train gradient:  0.13089752031915022
iteration : 47
train acc:  0.7734375
train loss:  0.4932582676410675
train gradient:  0.11991688892700107
iteration : 48
train acc:  0.734375
train loss:  0.5000823140144348
train gradient:  0.12106048096008887
iteration : 49
train acc:  0.765625
train loss:  0.5021644830703735
train gradient:  0.1208764971592277
iteration : 50
train acc:  0.765625
train loss:  0.4964291751384735
train gradient:  0.11543896546759642
iteration : 51
train acc:  0.7421875
train loss:  0.5299453735351562
train gradient:  0.1029610485425259
iteration : 52
train acc:  0.78125
train loss:  0.45837920904159546
train gradient:  0.09793544008701577
iteration : 53
train acc:  0.7734375
train loss:  0.4617016911506653
train gradient:  0.09406357920637438
iteration : 54
train acc:  0.7265625
train loss:  0.47757989168167114
train gradient:  0.11351500134897323
iteration : 55
train acc:  0.75
train loss:  0.5362645387649536
train gradient:  0.10841158032843064
iteration : 56
train acc:  0.828125
train loss:  0.40664300322532654
train gradient:  0.08119738729623159
iteration : 57
train acc:  0.78125
train loss:  0.48152828216552734
train gradient:  0.11280130559289807
iteration : 58
train acc:  0.7265625
train loss:  0.5212072730064392
train gradient:  0.10881165120258124
iteration : 59
train acc:  0.7578125
train loss:  0.4176968038082123
train gradient:  0.09606827546788142
iteration : 60
train acc:  0.734375
train loss:  0.480729341506958
train gradient:  0.12071748495716758
iteration : 61
train acc:  0.7578125
train loss:  0.5019500255584717
train gradient:  0.10394313301583812
iteration : 62
train acc:  0.75
train loss:  0.46688947081565857
train gradient:  0.11208814772524049
iteration : 63
train acc:  0.6875
train loss:  0.5846014022827148
train gradient:  0.20040458841064646
iteration : 64
train acc:  0.734375
train loss:  0.4886445999145508
train gradient:  0.13077958910736404
iteration : 65
train acc:  0.734375
train loss:  0.5041698217391968
train gradient:  0.12314936230739235
iteration : 66
train acc:  0.7109375
train loss:  0.5058833956718445
train gradient:  0.12990072092786378
iteration : 67
train acc:  0.71875
train loss:  0.49226799607276917
train gradient:  0.09665285633288967
iteration : 68
train acc:  0.734375
train loss:  0.4698143005371094
train gradient:  0.11809912402670841
iteration : 69
train acc:  0.7890625
train loss:  0.44190698862075806
train gradient:  0.09708120199921352
iteration : 70
train acc:  0.75
train loss:  0.46537452936172485
train gradient:  0.09463742039263619
iteration : 71
train acc:  0.6875
train loss:  0.5452181696891785
train gradient:  0.16418186590470252
iteration : 72
train acc:  0.7890625
train loss:  0.4369439482688904
train gradient:  0.0908662913298356
iteration : 73
train acc:  0.734375
train loss:  0.5272511839866638
train gradient:  0.14048222305215957
iteration : 74
train acc:  0.7421875
train loss:  0.4785603880882263
train gradient:  0.14149397744833186
iteration : 75
train acc:  0.7734375
train loss:  0.4749404788017273
train gradient:  0.08861365944964245
iteration : 76
train acc:  0.734375
train loss:  0.4717365801334381
train gradient:  0.09855503390447462
iteration : 77
train acc:  0.7109375
train loss:  0.5157175660133362
train gradient:  0.12379119887723765
iteration : 78
train acc:  0.7421875
train loss:  0.47497546672821045
train gradient:  0.11417984129665357
iteration : 79
train acc:  0.71875
train loss:  0.48844847083091736
train gradient:  0.1185045412488108
iteration : 80
train acc:  0.7578125
train loss:  0.47857505083084106
train gradient:  0.11766276003231164
iteration : 81
train acc:  0.796875
train loss:  0.4355270266532898
train gradient:  0.0932824478320207
iteration : 82
train acc:  0.75
train loss:  0.4660351872444153
train gradient:  0.09918296555737897
iteration : 83
train acc:  0.75
train loss:  0.4809267222881317
train gradient:  0.11771336892830431
iteration : 84
train acc:  0.703125
train loss:  0.500302791595459
train gradient:  0.1285699486146215
iteration : 85
train acc:  0.7734375
train loss:  0.4659324884414673
train gradient:  0.11963114561916835
iteration : 86
train acc:  0.75
train loss:  0.5215235948562622
train gradient:  0.11921463652136803
iteration : 87
train acc:  0.703125
train loss:  0.5700490474700928
train gradient:  0.14182567951119052
iteration : 88
train acc:  0.8046875
train loss:  0.4199313521385193
train gradient:  0.10713353014739627
iteration : 89
train acc:  0.7421875
train loss:  0.4302215576171875
train gradient:  0.08042443167248745
iteration : 90
train acc:  0.765625
train loss:  0.49530062079429626
train gradient:  0.1014034708847311
iteration : 91
train acc:  0.7265625
train loss:  0.4867752492427826
train gradient:  0.1025056135204344
iteration : 92
train acc:  0.7578125
train loss:  0.45160743594169617
train gradient:  0.09016182085252876
iteration : 93
train acc:  0.6953125
train loss:  0.5062512159347534
train gradient:  0.11218452033961934
iteration : 94
train acc:  0.8046875
train loss:  0.4374226927757263
train gradient:  0.08991912309331189
iteration : 95
train acc:  0.7890625
train loss:  0.4988611340522766
train gradient:  0.10940981873575381
iteration : 96
train acc:  0.6875
train loss:  0.5280629396438599
train gradient:  0.14594363185654063
iteration : 97
train acc:  0.7421875
train loss:  0.508017897605896
train gradient:  0.11779086095263554
iteration : 98
train acc:  0.7734375
train loss:  0.4763713777065277
train gradient:  0.11365444407209978
iteration : 99
train acc:  0.6796875
train loss:  0.5159973502159119
train gradient:  0.10708388611112644
iteration : 100
train acc:  0.796875
train loss:  0.438457727432251
train gradient:  0.11319319255971412
iteration : 101
train acc:  0.75
train loss:  0.49598175287246704
train gradient:  0.12326385642355811
iteration : 102
train acc:  0.796875
train loss:  0.422177791595459
train gradient:  0.0758199281658432
iteration : 103
train acc:  0.7890625
train loss:  0.4810972213745117
train gradient:  0.10512326031178047
iteration : 104
train acc:  0.7578125
train loss:  0.4892337918281555
train gradient:  0.11600285792617923
iteration : 105
train acc:  0.734375
train loss:  0.5395694971084595
train gradient:  0.1526611967690133
iteration : 106
train acc:  0.7109375
train loss:  0.5398428440093994
train gradient:  0.11425650574739642
iteration : 107
train acc:  0.7421875
train loss:  0.5182377099990845
train gradient:  0.13071483787473243
iteration : 108
train acc:  0.765625
train loss:  0.425719678401947
train gradient:  0.07502409439823883
iteration : 109
train acc:  0.796875
train loss:  0.4345891773700714
train gradient:  0.0846270751494947
iteration : 110
train acc:  0.75
train loss:  0.4726751148700714
train gradient:  0.12264818483625091
iteration : 111
train acc:  0.6484375
train loss:  0.5810906887054443
train gradient:  0.13932783132253262
iteration : 112
train acc:  0.671875
train loss:  0.5403226017951965
train gradient:  0.13420078287297393
iteration : 113
train acc:  0.765625
train loss:  0.4384050965309143
train gradient:  0.08950620434380709
iteration : 114
train acc:  0.75
train loss:  0.5222263932228088
train gradient:  0.12901621947461409
iteration : 115
train acc:  0.703125
train loss:  0.5259350538253784
train gradient:  0.15333342444182552
iteration : 116
train acc:  0.703125
train loss:  0.5145747065544128
train gradient:  0.11812794716499017
iteration : 117
train acc:  0.765625
train loss:  0.44321417808532715
train gradient:  0.10880887461295494
iteration : 118
train acc:  0.7578125
train loss:  0.455486923456192
train gradient:  0.099255739955451
iteration : 119
train acc:  0.71875
train loss:  0.5159356594085693
train gradient:  0.13125612409756893
iteration : 120
train acc:  0.75
train loss:  0.45868757367134094
train gradient:  0.0984850462268342
iteration : 121
train acc:  0.7734375
train loss:  0.4680231213569641
train gradient:  0.09998731298211108
iteration : 122
train acc:  0.734375
train loss:  0.47623777389526367
train gradient:  0.1184179351956727
iteration : 123
train acc:  0.7890625
train loss:  0.4776415228843689
train gradient:  0.11865103444432079
iteration : 124
train acc:  0.7578125
train loss:  0.4869029223918915
train gradient:  0.1246843384020031
iteration : 125
train acc:  0.765625
train loss:  0.5357456803321838
train gradient:  0.14605649750907546
iteration : 126
train acc:  0.8046875
train loss:  0.43441376090049744
train gradient:  0.09146847285210603
iteration : 127
train acc:  0.8046875
train loss:  0.44238603115081787
train gradient:  0.10324683031144799
iteration : 128
train acc:  0.75
train loss:  0.44126057624816895
train gradient:  0.09974163042089822
iteration : 129
train acc:  0.796875
train loss:  0.48514920473098755
train gradient:  0.12700591481464144
iteration : 130
train acc:  0.796875
train loss:  0.4297533333301544
train gradient:  0.11255576081274367
iteration : 131
train acc:  0.8046875
train loss:  0.4277763366699219
train gradient:  0.08466512210039276
iteration : 132
train acc:  0.7578125
train loss:  0.4648780822753906
train gradient:  0.10915692008844285
iteration : 133
train acc:  0.7265625
train loss:  0.5520656108856201
train gradient:  0.13013117957148018
iteration : 134
train acc:  0.796875
train loss:  0.4287762939929962
train gradient:  0.09611078904513395
iteration : 135
train acc:  0.7109375
train loss:  0.5069326162338257
train gradient:  0.14882240475040104
iteration : 136
train acc:  0.765625
train loss:  0.4638722538948059
train gradient:  0.10458934576051843
iteration : 137
train acc:  0.8125
train loss:  0.49543899297714233
train gradient:  0.155193070951201
iteration : 138
train acc:  0.7578125
train loss:  0.5155435800552368
train gradient:  0.1379917718406634
iteration : 139
train acc:  0.8125
train loss:  0.4214564561843872
train gradient:  0.0991585394173055
iteration : 140
train acc:  0.8359375
train loss:  0.43629369139671326
train gradient:  0.10407191369032612
iteration : 141
train acc:  0.7578125
train loss:  0.4669661819934845
train gradient:  0.10263443654505461
iteration : 142
train acc:  0.75
train loss:  0.5022334456443787
train gradient:  0.13664253431602624
iteration : 143
train acc:  0.7265625
train loss:  0.466464638710022
train gradient:  0.11352329815317802
iteration : 144
train acc:  0.7578125
train loss:  0.4740005135536194
train gradient:  0.11675586120514403
iteration : 145
train acc:  0.75
train loss:  0.4921514093875885
train gradient:  0.12542835793562468
iteration : 146
train acc:  0.7578125
train loss:  0.49688291549682617
train gradient:  0.13379926875121914
iteration : 147
train acc:  0.734375
train loss:  0.4554022550582886
train gradient:  0.1147585094128358
iteration : 148
train acc:  0.7109375
train loss:  0.5096358060836792
train gradient:  0.11293345750962035
iteration : 149
train acc:  0.703125
train loss:  0.533333420753479
train gradient:  0.1644152472742301
iteration : 150
train acc:  0.734375
train loss:  0.48810824751853943
train gradient:  0.15626185187727326
iteration : 151
train acc:  0.7109375
train loss:  0.48147499561309814
train gradient:  0.10571623046737436
iteration : 152
train acc:  0.7421875
train loss:  0.5062276124954224
train gradient:  0.12904482676736
iteration : 153
train acc:  0.7734375
train loss:  0.4652515649795532
train gradient:  0.1068696435785113
iteration : 154
train acc:  0.8046875
train loss:  0.40759900212287903
train gradient:  0.07625977046222779
iteration : 155
train acc:  0.75
train loss:  0.4969601631164551
train gradient:  0.15821611476070255
iteration : 156
train acc:  0.8125
train loss:  0.47649386525154114
train gradient:  0.13364635456440516
iteration : 157
train acc:  0.734375
train loss:  0.47353798151016235
train gradient:  0.1100548622989503
iteration : 158
train acc:  0.765625
train loss:  0.4386254549026489
train gradient:  0.09869481303591374
iteration : 159
train acc:  0.7890625
train loss:  0.437825083732605
train gradient:  0.0890087178206492
iteration : 160
train acc:  0.7265625
train loss:  0.44917532801628113
train gradient:  0.12365526427156376
iteration : 161
train acc:  0.734375
train loss:  0.4592881500720978
train gradient:  0.10454748142424711
iteration : 162
train acc:  0.8046875
train loss:  0.3918805718421936
train gradient:  0.07608139875833571
iteration : 163
train acc:  0.7734375
train loss:  0.4741917848587036
train gradient:  0.09836422403521902
iteration : 164
train acc:  0.75
train loss:  0.5170716643333435
train gradient:  0.12263775876203022
iteration : 165
train acc:  0.734375
train loss:  0.46719223260879517
train gradient:  0.09263259832028489
iteration : 166
train acc:  0.765625
train loss:  0.4904051423072815
train gradient:  0.11165986637193194
iteration : 167
train acc:  0.765625
train loss:  0.48831692337989807
train gradient:  0.09365542896225837
iteration : 168
train acc:  0.7734375
train loss:  0.44322818517684937
train gradient:  0.12840386804327864
iteration : 169
train acc:  0.7421875
train loss:  0.4416908323764801
train gradient:  0.10615452682827914
iteration : 170
train acc:  0.71875
train loss:  0.547907829284668
train gradient:  0.1581823867810615
iteration : 171
train acc:  0.7421875
train loss:  0.48390987515449524
train gradient:  0.1135352248866876
iteration : 172
train acc:  0.7265625
train loss:  0.4741031527519226
train gradient:  0.11787047665576299
iteration : 173
train acc:  0.7734375
train loss:  0.47278207540512085
train gradient:  0.12104959102818667
iteration : 174
train acc:  0.8125
train loss:  0.41446441411972046
train gradient:  0.07713938296162096
iteration : 175
train acc:  0.75
train loss:  0.4513615369796753
train gradient:  0.1371449263514751
iteration : 176
train acc:  0.75
train loss:  0.5072110891342163
train gradient:  0.14921569016671038
iteration : 177
train acc:  0.734375
train loss:  0.49627450108528137
train gradient:  0.11414198513266637
iteration : 178
train acc:  0.78125
train loss:  0.4542039632797241
train gradient:  0.11800932720517339
iteration : 179
train acc:  0.6953125
train loss:  0.5011194944381714
train gradient:  0.11542050307911582
iteration : 180
train acc:  0.7109375
train loss:  0.5031633973121643
train gradient:  0.16344864523456792
iteration : 181
train acc:  0.78125
train loss:  0.4484512507915497
train gradient:  0.07522375754631662
iteration : 182
train acc:  0.7421875
train loss:  0.5144840478897095
train gradient:  0.13261800780575117
iteration : 183
train acc:  0.7578125
train loss:  0.45799946784973145
train gradient:  0.10889777445799756
iteration : 184
train acc:  0.7421875
train loss:  0.4532516896724701
train gradient:  0.10211551639885495
iteration : 185
train acc:  0.703125
train loss:  0.4815290570259094
train gradient:  0.10097281254610169
iteration : 186
train acc:  0.7421875
train loss:  0.44671431183815
train gradient:  0.10406931503845404
iteration : 187
train acc:  0.75
train loss:  0.5536476373672485
train gradient:  0.13193419633677678
iteration : 188
train acc:  0.78125
train loss:  0.5038788318634033
train gradient:  0.10783592988948267
iteration : 189
train acc:  0.7734375
train loss:  0.44595882296562195
train gradient:  0.1264653680725267
iteration : 190
train acc:  0.734375
train loss:  0.5346941351890564
train gradient:  0.14822488434935546
iteration : 191
train acc:  0.75
train loss:  0.49612957239151
train gradient:  0.13242336082700418
iteration : 192
train acc:  0.734375
train loss:  0.471586674451828
train gradient:  0.09926138209506871
iteration : 193
train acc:  0.78125
train loss:  0.4661765694618225
train gradient:  0.09733418588162455
iteration : 194
train acc:  0.7421875
train loss:  0.4689919352531433
train gradient:  0.11757851723491426
iteration : 195
train acc:  0.71875
train loss:  0.5091336369514465
train gradient:  0.13032994442829837
iteration : 196
train acc:  0.75
train loss:  0.468372106552124
train gradient:  0.10135677958733154
iteration : 197
train acc:  0.71875
train loss:  0.5062874555587769
train gradient:  0.11359015344197104
iteration : 198
train acc:  0.78125
train loss:  0.4320034384727478
train gradient:  0.09775547990108405
iteration : 199
train acc:  0.6875
train loss:  0.551432192325592
train gradient:  0.13895377758826252
iteration : 200
train acc:  0.7421875
train loss:  0.5328307747840881
train gradient:  0.13291313954016548
iteration : 201
train acc:  0.828125
train loss:  0.41199666261672974
train gradient:  0.09916146872961544
iteration : 202
train acc:  0.734375
train loss:  0.5123200416564941
train gradient:  0.12878100364589695
iteration : 203
train acc:  0.7734375
train loss:  0.47575587034225464
train gradient:  0.16434719940513773
iteration : 204
train acc:  0.8046875
train loss:  0.43444985151290894
train gradient:  0.09291111349185674
iteration : 205
train acc:  0.71875
train loss:  0.5453577637672424
train gradient:  0.1375318870285377
iteration : 206
train acc:  0.7265625
train loss:  0.5144227743148804
train gradient:  0.12853917961167693
iteration : 207
train acc:  0.7734375
train loss:  0.45090150833129883
train gradient:  0.1184196463187696
iteration : 208
train acc:  0.734375
train loss:  0.5255768895149231
train gradient:  0.21365254733889935
iteration : 209
train acc:  0.734375
train loss:  0.45402124524116516
train gradient:  0.12285151037162546
iteration : 210
train acc:  0.7578125
train loss:  0.48896652460098267
train gradient:  0.13827469026849193
iteration : 211
train acc:  0.75
train loss:  0.488771915435791
train gradient:  0.09807585511450832
iteration : 212
train acc:  0.7421875
train loss:  0.49178546667099
train gradient:  0.11757171380916841
iteration : 213
train acc:  0.7265625
train loss:  0.4447082579135895
train gradient:  0.07011709074699429
iteration : 214
train acc:  0.75
train loss:  0.4661504030227661
train gradient:  0.11734923204039166
iteration : 215
train acc:  0.7734375
train loss:  0.47506237030029297
train gradient:  0.119913866600714
iteration : 216
train acc:  0.7578125
train loss:  0.4247938096523285
train gradient:  0.08789635035496914
iteration : 217
train acc:  0.75
train loss:  0.48780208826065063
train gradient:  0.10501930767433691
iteration : 218
train acc:  0.7734375
train loss:  0.47784921526908875
train gradient:  0.10260525064504217
iteration : 219
train acc:  0.796875
train loss:  0.4943590462207794
train gradient:  0.10200926853270882
iteration : 220
train acc:  0.765625
train loss:  0.45236319303512573
train gradient:  0.09177421452506473
iteration : 221
train acc:  0.7734375
train loss:  0.4796648621559143
train gradient:  0.14073502114448028
iteration : 222
train acc:  0.7421875
train loss:  0.49268555641174316
train gradient:  0.11512126742823263
iteration : 223
train acc:  0.7734375
train loss:  0.40984833240509033
train gradient:  0.07889593504219385
iteration : 224
train acc:  0.7578125
train loss:  0.5052810907363892
train gradient:  0.11505418177724613
iteration : 225
train acc:  0.7421875
train loss:  0.4332035779953003
train gradient:  0.08788686808881395
iteration : 226
train acc:  0.703125
train loss:  0.505633533000946
train gradient:  0.11480275487730172
iteration : 227
train acc:  0.703125
train loss:  0.5256868600845337
train gradient:  0.14133920788214732
iteration : 228
train acc:  0.796875
train loss:  0.4689624011516571
train gradient:  0.1089957183610644
iteration : 229
train acc:  0.734375
train loss:  0.4740108549594879
train gradient:  0.09723132930840347
iteration : 230
train acc:  0.7890625
train loss:  0.4417397379875183
train gradient:  0.09234176205883687
iteration : 231
train acc:  0.75
train loss:  0.45507150888442993
train gradient:  0.1029363425235079
iteration : 232
train acc:  0.8203125
train loss:  0.41874754428863525
train gradient:  0.09980058926849383
iteration : 233
train acc:  0.7734375
train loss:  0.48374226689338684
train gradient:  0.0982396660354538
iteration : 234
train acc:  0.7421875
train loss:  0.49980276823043823
train gradient:  0.16093044587283922
iteration : 235
train acc:  0.6953125
train loss:  0.5428099632263184
train gradient:  0.14315315150272506
iteration : 236
train acc:  0.703125
train loss:  0.4991065263748169
train gradient:  0.15191867425800176
iteration : 237
train acc:  0.765625
train loss:  0.43052685260772705
train gradient:  0.08316681938543408
iteration : 238
train acc:  0.7734375
train loss:  0.47244733572006226
train gradient:  0.10209481642710892
iteration : 239
train acc:  0.765625
train loss:  0.44877976179122925
train gradient:  0.09986881794964382
iteration : 240
train acc:  0.7109375
train loss:  0.5112617015838623
train gradient:  0.12777617113199766
iteration : 241
train acc:  0.8125
train loss:  0.45309212803840637
train gradient:  0.10748996011127644
iteration : 242
train acc:  0.8046875
train loss:  0.41708827018737793
train gradient:  0.08864269760990455
iteration : 243
train acc:  0.859375
train loss:  0.40305647253990173
train gradient:  0.09220383157636526
iteration : 244
train acc:  0.78125
train loss:  0.44790542125701904
train gradient:  0.08195004244433393
iteration : 245
train acc:  0.7265625
train loss:  0.5015517473220825
train gradient:  0.11152306296542276
iteration : 246
train acc:  0.7421875
train loss:  0.4319556653499603
train gradient:  0.09538229349124352
iteration : 247
train acc:  0.8046875
train loss:  0.4465705156326294
train gradient:  0.09300165872316957
iteration : 248
train acc:  0.7578125
train loss:  0.47080034017562866
train gradient:  0.10075080714041162
iteration : 249
train acc:  0.7578125
train loss:  0.47291576862335205
train gradient:  0.08453931747289015
iteration : 250
train acc:  0.7421875
train loss:  0.49325406551361084
train gradient:  0.1121696181477514
iteration : 251
train acc:  0.703125
train loss:  0.5837866067886353
train gradient:  0.13690157954360668
iteration : 252
train acc:  0.7734375
train loss:  0.4108738303184509
train gradient:  0.09302861666277643
iteration : 253
train acc:  0.765625
train loss:  0.4434531629085541
train gradient:  0.11048733917260158
iteration : 254
train acc:  0.796875
train loss:  0.4257551431655884
train gradient:  0.08190485622296122
iteration : 255
train acc:  0.7890625
train loss:  0.44891729950904846
train gradient:  0.10919321274925195
iteration : 256
train acc:  0.7734375
train loss:  0.46792423725128174
train gradient:  0.13306267483024123
iteration : 257
train acc:  0.7734375
train loss:  0.47506171464920044
train gradient:  0.13864759397987136
iteration : 258
train acc:  0.7265625
train loss:  0.5464497804641724
train gradient:  0.11500032573560579
iteration : 259
train acc:  0.75
train loss:  0.552801251411438
train gradient:  0.16657483032674297
iteration : 260
train acc:  0.765625
train loss:  0.5054345726966858
train gradient:  0.10245090705844016
iteration : 261
train acc:  0.7265625
train loss:  0.46264004707336426
train gradient:  0.10861589061079352
iteration : 262
train acc:  0.7734375
train loss:  0.4337840676307678
train gradient:  0.09359475781281963
iteration : 263
train acc:  0.8125
train loss:  0.44526538252830505
train gradient:  0.1044118097533186
iteration : 264
train acc:  0.7734375
train loss:  0.4698563814163208
train gradient:  0.09955411451393371
iteration : 265
train acc:  0.7578125
train loss:  0.4794776439666748
train gradient:  0.121834445748233
iteration : 266
train acc:  0.796875
train loss:  0.4426628351211548
train gradient:  0.09971136424165106
iteration : 267
train acc:  0.7265625
train loss:  0.4793955087661743
train gradient:  0.10479564433864753
iteration : 268
train acc:  0.796875
train loss:  0.433025598526001
train gradient:  0.12476738564151657
iteration : 269
train acc:  0.765625
train loss:  0.42567670345306396
train gradient:  0.08780737534814281
iteration : 270
train acc:  0.75
train loss:  0.48202234506607056
train gradient:  0.10862119761369332
iteration : 271
train acc:  0.8125
train loss:  0.4223005175590515
train gradient:  0.11242747087792325
iteration : 272
train acc:  0.7109375
train loss:  0.5242328643798828
train gradient:  0.1322504662499614
iteration : 273
train acc:  0.796875
train loss:  0.4270229935646057
train gradient:  0.10005271372303641
iteration : 274
train acc:  0.8359375
train loss:  0.3726620674133301
train gradient:  0.08602202655020694
iteration : 275
train acc:  0.703125
train loss:  0.5489436388015747
train gradient:  0.14310341220255984
iteration : 276
train acc:  0.734375
train loss:  0.45555421710014343
train gradient:  0.10135431575135338
iteration : 277
train acc:  0.703125
train loss:  0.4846481680870056
train gradient:  0.10164777963729281
iteration : 278
train acc:  0.71875
train loss:  0.5173307061195374
train gradient:  0.11713371625883202
iteration : 279
train acc:  0.7421875
train loss:  0.4543227553367615
train gradient:  0.1086495778843059
iteration : 280
train acc:  0.71875
train loss:  0.5285847783088684
train gradient:  0.12347125288597173
iteration : 281
train acc:  0.78125
train loss:  0.5020354390144348
train gradient:  0.13597938033964757
iteration : 282
train acc:  0.765625
train loss:  0.4392929971218109
train gradient:  0.09128636642181075
iteration : 283
train acc:  0.78125
train loss:  0.4789275527000427
train gradient:  0.12833824007084887
iteration : 284
train acc:  0.671875
train loss:  0.5687319040298462
train gradient:  0.1510680651280889
iteration : 285
train acc:  0.7734375
train loss:  0.47084715962409973
train gradient:  0.11758304009870316
iteration : 286
train acc:  0.7421875
train loss:  0.4914642572402954
train gradient:  0.12873530927387467
iteration : 287
train acc:  0.75
train loss:  0.4723702073097229
train gradient:  0.1142970332076046
iteration : 288
train acc:  0.765625
train loss:  0.41584646701812744
train gradient:  0.08488316734478664
iteration : 289
train acc:  0.75
train loss:  0.47275400161743164
train gradient:  0.10194629892383239
iteration : 290
train acc:  0.703125
train loss:  0.5499091148376465
train gradient:  0.14830454810298677
iteration : 291
train acc:  0.734375
train loss:  0.4652706980705261
train gradient:  0.11967584994816614
iteration : 292
train acc:  0.75
train loss:  0.4482852816581726
train gradient:  0.1101641010717666
iteration : 293
train acc:  0.7734375
train loss:  0.4725656509399414
train gradient:  0.10828143180008411
iteration : 294
train acc:  0.765625
train loss:  0.45597386360168457
train gradient:  0.09893228653861916
iteration : 295
train acc:  0.7265625
train loss:  0.5026077628135681
train gradient:  0.11100973789569672
iteration : 296
train acc:  0.7421875
train loss:  0.5014004707336426
train gradient:  0.14252215093435733
iteration : 297
train acc:  0.78125
train loss:  0.46608996391296387
train gradient:  0.11850163393649765
iteration : 298
train acc:  0.75
train loss:  0.5215900540351868
train gradient:  0.10951912438197861
iteration : 299
train acc:  0.8203125
train loss:  0.42601001262664795
train gradient:  0.09476569081012429
iteration : 300
train acc:  0.7421875
train loss:  0.5294129848480225
train gradient:  0.13921496765673855
iteration : 301
train acc:  0.7421875
train loss:  0.49230149388313293
train gradient:  0.1161655744319914
iteration : 302
train acc:  0.75
train loss:  0.46878117322921753
train gradient:  0.09172942848117094
iteration : 303
train acc:  0.7421875
train loss:  0.500691294670105
train gradient:  0.11713364748989835
iteration : 304
train acc:  0.6953125
train loss:  0.5037295818328857
train gradient:  0.09978922689222129
iteration : 305
train acc:  0.75
train loss:  0.4528998136520386
train gradient:  0.11821603841287633
iteration : 306
train acc:  0.765625
train loss:  0.5069032907485962
train gradient:  0.11799203761116446
iteration : 307
train acc:  0.71875
train loss:  0.502324104309082
train gradient:  0.11788614745823893
iteration : 308
train acc:  0.75
train loss:  0.47829046845436096
train gradient:  0.11395412380231845
iteration : 309
train acc:  0.8046875
train loss:  0.4425196051597595
train gradient:  0.1005270659535783
iteration : 310
train acc:  0.734375
train loss:  0.4974610507488251
train gradient:  0.11843746758835388
iteration : 311
train acc:  0.7734375
train loss:  0.4364413321018219
train gradient:  0.11076923381535964
iteration : 312
train acc:  0.734375
train loss:  0.4858241081237793
train gradient:  0.11892287818239855
iteration : 313
train acc:  0.734375
train loss:  0.4927731156349182
train gradient:  0.11916459516697939
iteration : 314
train acc:  0.8125
train loss:  0.4351678788661957
train gradient:  0.08904549621322749
iteration : 315
train acc:  0.7421875
train loss:  0.5154509544372559
train gradient:  0.14568370351015184
iteration : 316
train acc:  0.734375
train loss:  0.5348144769668579
train gradient:  0.12372852608170122
iteration : 317
train acc:  0.7421875
train loss:  0.4293787181377411
train gradient:  0.09332092910496767
iteration : 318
train acc:  0.7109375
train loss:  0.502945601940155
train gradient:  0.11375450810841417
iteration : 319
train acc:  0.78125
train loss:  0.42616844177246094
train gradient:  0.09180229158809859
iteration : 320
train acc:  0.78125
train loss:  0.4144183397293091
train gradient:  0.09082595816660564
iteration : 321
train acc:  0.734375
train loss:  0.4667857587337494
train gradient:  0.09740143367615221
iteration : 322
train acc:  0.78125
train loss:  0.4718751311302185
train gradient:  0.13441374452478153
iteration : 323
train acc:  0.7265625
train loss:  0.4952312409877777
train gradient:  0.1258577198097009
iteration : 324
train acc:  0.765625
train loss:  0.524834394454956
train gradient:  0.17669824527487377
iteration : 325
train acc:  0.734375
train loss:  0.5012720823287964
train gradient:  0.12316292063287092
iteration : 326
train acc:  0.7109375
train loss:  0.4958255887031555
train gradient:  0.14115181763170004
iteration : 327
train acc:  0.75
train loss:  0.47448888421058655
train gradient:  0.11040588471536686
iteration : 328
train acc:  0.765625
train loss:  0.5129823684692383
train gradient:  0.12355652848472629
iteration : 329
train acc:  0.7421875
train loss:  0.44779008626937866
train gradient:  0.1219100964592998
iteration : 330
train acc:  0.8203125
train loss:  0.44131219387054443
train gradient:  0.08945296209540178
iteration : 331
train acc:  0.703125
train loss:  0.5393810868263245
train gradient:  0.1301299147251203
iteration : 332
train acc:  0.8046875
train loss:  0.48095548152923584
train gradient:  0.11390894906950677
iteration : 333
train acc:  0.8046875
train loss:  0.3895393908023834
train gradient:  0.09604140493469301
iteration : 334
train acc:  0.796875
train loss:  0.42652058601379395
train gradient:  0.0985203705461391
iteration : 335
train acc:  0.7109375
train loss:  0.4996713697910309
train gradient:  0.12845709746650752
iteration : 336
train acc:  0.7421875
train loss:  0.5146600008010864
train gradient:  0.16034251666244106
iteration : 337
train acc:  0.7265625
train loss:  0.5032309293746948
train gradient:  0.12621238875715035
iteration : 338
train acc:  0.75
train loss:  0.5159232020378113
train gradient:  0.10852305174352753
iteration : 339
train acc:  0.734375
train loss:  0.5523364543914795
train gradient:  0.15022602330781024
iteration : 340
train acc:  0.78125
train loss:  0.4436117708683014
train gradient:  0.10810190844155378
iteration : 341
train acc:  0.7734375
train loss:  0.45240724086761475
train gradient:  0.08783564563528876
iteration : 342
train acc:  0.734375
train loss:  0.4609198570251465
train gradient:  0.10774189375992979
iteration : 343
train acc:  0.7265625
train loss:  0.5267208814620972
train gradient:  0.12230070725188505
iteration : 344
train acc:  0.71875
train loss:  0.5237283706665039
train gradient:  0.14113475624727517
iteration : 345
train acc:  0.7421875
train loss:  0.5185774564743042
train gradient:  0.12127552437408978
iteration : 346
train acc:  0.703125
train loss:  0.544333815574646
train gradient:  0.17404578215481611
iteration : 347
train acc:  0.71875
train loss:  0.4899461567401886
train gradient:  0.11853827373339641
iteration : 348
train acc:  0.796875
train loss:  0.40567469596862793
train gradient:  0.08635722658692524
iteration : 349
train acc:  0.7265625
train loss:  0.4746825098991394
train gradient:  0.10152570685258758
iteration : 350
train acc:  0.765625
train loss:  0.5137430429458618
train gradient:  0.13324764401942757
iteration : 351
train acc:  0.7421875
train loss:  0.4697762727737427
train gradient:  0.09958898441720125
iteration : 352
train acc:  0.71875
train loss:  0.4843110740184784
train gradient:  0.09166833416875746
iteration : 353
train acc:  0.6953125
train loss:  0.5874208807945251
train gradient:  0.19492981661924308
iteration : 354
train acc:  0.796875
train loss:  0.41666871309280396
train gradient:  0.10273123197633581
iteration : 355
train acc:  0.734375
train loss:  0.47948646545410156
train gradient:  0.11713814310805563
iteration : 356
train acc:  0.7734375
train loss:  0.444757342338562
train gradient:  0.11667903017005236
iteration : 357
train acc:  0.765625
train loss:  0.44642579555511475
train gradient:  0.09895841453871217
iteration : 358
train acc:  0.6953125
train loss:  0.5201549530029297
train gradient:  0.16156757494187995
iteration : 359
train acc:  0.7109375
train loss:  0.49341145157814026
train gradient:  0.12510756769050613
iteration : 360
train acc:  0.7734375
train loss:  0.4754919707775116
train gradient:  0.1048434238012421
iteration : 361
train acc:  0.6953125
train loss:  0.5681572556495667
train gradient:  0.15012232524940314
iteration : 362
train acc:  0.7421875
train loss:  0.49690788984298706
train gradient:  0.12622452274343027
iteration : 363
train acc:  0.7265625
train loss:  0.5048491358757019
train gradient:  0.11484086671434954
iteration : 364
train acc:  0.7109375
train loss:  0.5388599634170532
train gradient:  0.1349437663488012
iteration : 365
train acc:  0.7421875
train loss:  0.4882541000843048
train gradient:  0.11028893062473288
iteration : 366
train acc:  0.75
train loss:  0.4320834279060364
train gradient:  0.08624828121710007
iteration : 367
train acc:  0.7265625
train loss:  0.5161609649658203
train gradient:  0.11861454614547122
iteration : 368
train acc:  0.765625
train loss:  0.4340771436691284
train gradient:  0.10563094499704163
iteration : 369
train acc:  0.8203125
train loss:  0.41225630044937134
train gradient:  0.08401317535940411
iteration : 370
train acc:  0.8046875
train loss:  0.4501705467700958
train gradient:  0.09800186120652508
iteration : 371
train acc:  0.7734375
train loss:  0.4749324321746826
train gradient:  0.10850347588479956
iteration : 372
train acc:  0.734375
train loss:  0.5042283535003662
train gradient:  0.11119828241790028
iteration : 373
train acc:  0.703125
train loss:  0.5345730185508728
train gradient:  0.14279337171106227
iteration : 374
train acc:  0.7734375
train loss:  0.43560558557510376
train gradient:  0.10283093443883104
iteration : 375
train acc:  0.7578125
train loss:  0.4498691260814667
train gradient:  0.11245860451227718
iteration : 376
train acc:  0.703125
train loss:  0.5227400064468384
train gradient:  0.13810164737956498
iteration : 377
train acc:  0.7265625
train loss:  0.542738676071167
train gradient:  0.14174754540379952
iteration : 378
train acc:  0.7265625
train loss:  0.5004132390022278
train gradient:  0.09652187809842189
iteration : 379
train acc:  0.7421875
train loss:  0.535871684551239
train gradient:  0.13556577647951323
iteration : 380
train acc:  0.78125
train loss:  0.4935339093208313
train gradient:  0.11304528105661725
iteration : 381
train acc:  0.78125
train loss:  0.4520423412322998
train gradient:  0.09050881142446357
iteration : 382
train acc:  0.7578125
train loss:  0.4565887451171875
train gradient:  0.12023962571235279
iteration : 383
train acc:  0.7265625
train loss:  0.5173954963684082
train gradient:  0.12117592157421861
iteration : 384
train acc:  0.6796875
train loss:  0.5190175771713257
train gradient:  0.15678983980413513
iteration : 385
train acc:  0.7578125
train loss:  0.4468690752983093
train gradient:  0.10151808718561882
iteration : 386
train acc:  0.7578125
train loss:  0.4310237169265747
train gradient:  0.08920147502462188
iteration : 387
train acc:  0.6953125
train loss:  0.4685704708099365
train gradient:  0.10018975293632533
iteration : 388
train acc:  0.71875
train loss:  0.5541002750396729
train gradient:  0.15915958451024453
iteration : 389
train acc:  0.7578125
train loss:  0.4797254800796509
train gradient:  0.10104089051232445
iteration : 390
train acc:  0.8046875
train loss:  0.4473235607147217
train gradient:  0.08885485915158642
iteration : 391
train acc:  0.7734375
train loss:  0.42651185393333435
train gradient:  0.07924881825176573
iteration : 392
train acc:  0.75
train loss:  0.4574861526489258
train gradient:  0.10520523698657083
iteration : 393
train acc:  0.6953125
train loss:  0.5550554990768433
train gradient:  0.16264623978127837
iteration : 394
train acc:  0.671875
train loss:  0.6188734173774719
train gradient:  0.16409650604878356
iteration : 395
train acc:  0.71875
train loss:  0.5161982774734497
train gradient:  0.12523466068740435
iteration : 396
train acc:  0.78125
train loss:  0.4565316438674927
train gradient:  0.09793882018198379
iteration : 397
train acc:  0.703125
train loss:  0.5245476961135864
train gradient:  0.14540381613842077
iteration : 398
train acc:  0.7890625
train loss:  0.4439874291419983
train gradient:  0.09164492863143113
iteration : 399
train acc:  0.734375
train loss:  0.47402459383010864
train gradient:  0.10657354283542408
iteration : 400
train acc:  0.71875
train loss:  0.47835686802864075
train gradient:  0.10230387805375864
iteration : 401
train acc:  0.71875
train loss:  0.49133598804473877
train gradient:  0.09703739360453621
iteration : 402
train acc:  0.7578125
train loss:  0.48513349890708923
train gradient:  0.1317743290910447
iteration : 403
train acc:  0.71875
train loss:  0.5126887559890747
train gradient:  0.12034985757227026
iteration : 404
train acc:  0.7734375
train loss:  0.5141481161117554
train gradient:  0.145919277885444
iteration : 405
train acc:  0.78125
train loss:  0.4481215178966522
train gradient:  0.10797881237612296
iteration : 406
train acc:  0.6953125
train loss:  0.5310219526290894
train gradient:  0.1315636218390199
iteration : 407
train acc:  0.78125
train loss:  0.47106605768203735
train gradient:  0.14515715665731505
iteration : 408
train acc:  0.765625
train loss:  0.4732944667339325
train gradient:  0.10284586873370863
iteration : 409
train acc:  0.7265625
train loss:  0.5110212564468384
train gradient:  0.10192630210251652
iteration : 410
train acc:  0.71875
train loss:  0.4927024245262146
train gradient:  0.12325135086024111
iteration : 411
train acc:  0.75
train loss:  0.4672747850418091
train gradient:  0.10414244647128472
iteration : 412
train acc:  0.7265625
train loss:  0.5373077988624573
train gradient:  0.11426099427293991
iteration : 413
train acc:  0.734375
train loss:  0.44815924763679504
train gradient:  0.0932058556557133
iteration : 414
train acc:  0.7265625
train loss:  0.5462818741798401
train gradient:  0.1444466556306408
iteration : 415
train acc:  0.7734375
train loss:  0.4685118794441223
train gradient:  0.10816538222488423
iteration : 416
train acc:  0.7265625
train loss:  0.5095177292823792
train gradient:  0.10191428141274357
iteration : 417
train acc:  0.7578125
train loss:  0.4676370918750763
train gradient:  0.09724111408028936
iteration : 418
train acc:  0.734375
train loss:  0.5186808109283447
train gradient:  0.14190396720728943
iteration : 419
train acc:  0.7734375
train loss:  0.429504930973053
train gradient:  0.08022389465890634
iteration : 420
train acc:  0.734375
train loss:  0.49805015325546265
train gradient:  0.11882418840711466
iteration : 421
train acc:  0.8203125
train loss:  0.3927316665649414
train gradient:  0.06593486341428853
iteration : 422
train acc:  0.765625
train loss:  0.481564998626709
train gradient:  0.1018217416840406
iteration : 423
train acc:  0.703125
train loss:  0.514565646648407
train gradient:  0.11692073246869467
iteration : 424
train acc:  0.7265625
train loss:  0.5295071601867676
train gradient:  0.15323346082084952
iteration : 425
train acc:  0.6796875
train loss:  0.5512559413909912
train gradient:  0.13621977753545744
iteration : 426
train acc:  0.75
train loss:  0.46532702445983887
train gradient:  0.10728513255577844
iteration : 427
train acc:  0.8828125
train loss:  0.37135565280914307
train gradient:  0.07716807741263813
iteration : 428
train acc:  0.75
train loss:  0.4743399918079376
train gradient:  0.12294481334085265
iteration : 429
train acc:  0.796875
train loss:  0.42837026715278625
train gradient:  0.08946874184408031
iteration : 430
train acc:  0.71875
train loss:  0.4914346933364868
train gradient:  0.09885387297801777
iteration : 431
train acc:  0.8515625
train loss:  0.4051781892776489
train gradient:  0.07892166710495843
iteration : 432
train acc:  0.765625
train loss:  0.462454617023468
train gradient:  0.10344458874928858
iteration : 433
train acc:  0.7578125
train loss:  0.48806142807006836
train gradient:  0.08684852712778642
iteration : 434
train acc:  0.7578125
train loss:  0.4259839653968811
train gradient:  0.09210747666582655
iteration : 435
train acc:  0.7578125
train loss:  0.4083944857120514
train gradient:  0.07996692085200384
iteration : 436
train acc:  0.765625
train loss:  0.4514933228492737
train gradient:  0.10500526142063747
iteration : 437
train acc:  0.78125
train loss:  0.4464944303035736
train gradient:  0.0914301093556359
iteration : 438
train acc:  0.8125
train loss:  0.37413692474365234
train gradient:  0.09216972541732013
iteration : 439
train acc:  0.71875
train loss:  0.545121431350708
train gradient:  0.1355795816749364
iteration : 440
train acc:  0.703125
train loss:  0.4846911132335663
train gradient:  0.11334737466780136
iteration : 441
train acc:  0.7578125
train loss:  0.4736417531967163
train gradient:  0.12338927922162662
iteration : 442
train acc:  0.765625
train loss:  0.43248745799064636
train gradient:  0.1329031818210561
iteration : 443
train acc:  0.796875
train loss:  0.41082558035850525
train gradient:  0.10157926028393599
iteration : 444
train acc:  0.7734375
train loss:  0.4529609680175781
train gradient:  0.12012308950406858
iteration : 445
train acc:  0.703125
train loss:  0.4933013916015625
train gradient:  0.12091935074006446
iteration : 446
train acc:  0.78125
train loss:  0.44721654057502747
train gradient:  0.08439513687630826
iteration : 447
train acc:  0.7265625
train loss:  0.5485793352127075
train gradient:  0.13513465057922047
iteration : 448
train acc:  0.7421875
train loss:  0.4452233910560608
train gradient:  0.09262141193383494
iteration : 449
train acc:  0.7265625
train loss:  0.5522370934486389
train gradient:  0.15089760635617716
iteration : 450
train acc:  0.75
train loss:  0.481106162071228
train gradient:  0.11662391310663969
iteration : 451
train acc:  0.7109375
train loss:  0.504780650138855
train gradient:  0.11870360339372081
iteration : 452
train acc:  0.828125
train loss:  0.39792680740356445
train gradient:  0.0790119666086503
iteration : 453
train acc:  0.7109375
train loss:  0.5094126462936401
train gradient:  0.13735264968785038
iteration : 454
train acc:  0.8125
train loss:  0.42813000082969666
train gradient:  0.09693636500404484
iteration : 455
train acc:  0.703125
train loss:  0.5501723885536194
train gradient:  0.1434163630929749
iteration : 456
train acc:  0.7109375
train loss:  0.5300891399383545
train gradient:  0.17292899423243524
iteration : 457
train acc:  0.8203125
train loss:  0.44243013858795166
train gradient:  0.12837131690997178
iteration : 458
train acc:  0.7421875
train loss:  0.47870102524757385
train gradient:  0.09952834118920914
iteration : 459
train acc:  0.765625
train loss:  0.4432750642299652
train gradient:  0.09613399700666381
iteration : 460
train acc:  0.8125
train loss:  0.4499703645706177
train gradient:  0.11116146923100552
iteration : 461
train acc:  0.78125
train loss:  0.44697919487953186
train gradient:  0.10287529723632417
iteration : 462
train acc:  0.796875
train loss:  0.4468282163143158
train gradient:  0.09703677162979824
iteration : 463
train acc:  0.796875
train loss:  0.4462823271751404
train gradient:  0.08958881863084626
iteration : 464
train acc:  0.640625
train loss:  0.5729731917381287
train gradient:  0.14364980569521335
iteration : 465
train acc:  0.75
train loss:  0.4681050479412079
train gradient:  0.13905104124780288
iteration : 466
train acc:  0.7421875
train loss:  0.4430491626262665
train gradient:  0.11925651753831085
iteration : 467
train acc:  0.7265625
train loss:  0.5559388995170593
train gradient:  0.21074295372907054
iteration : 468
train acc:  0.7265625
train loss:  0.4836360514163971
train gradient:  0.11315313462738054
iteration : 469
train acc:  0.78125
train loss:  0.4478248655796051
train gradient:  0.0867033769163572
iteration : 470
train acc:  0.7578125
train loss:  0.453926682472229
train gradient:  0.10541018957347657
iteration : 471
train acc:  0.75
train loss:  0.5051901340484619
train gradient:  0.12797619492474918
iteration : 472
train acc:  0.75
train loss:  0.4736005663871765
train gradient:  0.0884233785294732
iteration : 473
train acc:  0.7578125
train loss:  0.451171875
train gradient:  0.10914144130650634
iteration : 474
train acc:  0.75
train loss:  0.4919176697731018
train gradient:  0.10761941918597857
iteration : 475
train acc:  0.796875
train loss:  0.4405199885368347
train gradient:  0.10146082828060915
iteration : 476
train acc:  0.765625
train loss:  0.45538705587387085
train gradient:  0.09843677935452744
iteration : 477
train acc:  0.8125
train loss:  0.4102773070335388
train gradient:  0.09801152845976754
iteration : 478
train acc:  0.7421875
train loss:  0.48643621802330017
train gradient:  0.11407212792350661
iteration : 479
train acc:  0.71875
train loss:  0.5083077549934387
train gradient:  0.14932690262319567
iteration : 480
train acc:  0.71875
train loss:  0.46187078952789307
train gradient:  0.1074395840162412
iteration : 481
train acc:  0.78125
train loss:  0.4517679810523987
train gradient:  0.10006871765004086
iteration : 482
train acc:  0.75
train loss:  0.44307753443717957
train gradient:  0.09930297360077603
iteration : 483
train acc:  0.71875
train loss:  0.5488979816436768
train gradient:  0.12995711063999857
iteration : 484
train acc:  0.75
train loss:  0.47272276878356934
train gradient:  0.11248901490218703
iteration : 485
train acc:  0.7578125
train loss:  0.4585370719432831
train gradient:  0.12528977905199984
iteration : 486
train acc:  0.6796875
train loss:  0.5612671375274658
train gradient:  0.1529005041463974
iteration : 487
train acc:  0.796875
train loss:  0.4092220664024353
train gradient:  0.08744676264335534
iteration : 488
train acc:  0.7421875
train loss:  0.459762305021286
train gradient:  0.11299064732229085
iteration : 489
train acc:  0.765625
train loss:  0.47123318910598755
train gradient:  0.11316670456496097
iteration : 490
train acc:  0.71875
train loss:  0.5374149680137634
train gradient:  0.1854637982668903
iteration : 491
train acc:  0.78125
train loss:  0.4286130666732788
train gradient:  0.12029187545285563
iteration : 492
train acc:  0.765625
train loss:  0.44512519240379333
train gradient:  0.09557469025722012
iteration : 493
train acc:  0.734375
train loss:  0.4979589581489563
train gradient:  0.11527683212129215
iteration : 494
train acc:  0.734375
train loss:  0.4862009882926941
train gradient:  0.11825493022440137
iteration : 495
train acc:  0.703125
train loss:  0.531069278717041
train gradient:  0.1384861723553696
iteration : 496
train acc:  0.703125
train loss:  0.5584087371826172
train gradient:  0.1661268938808507
iteration : 497
train acc:  0.703125
train loss:  0.5167652368545532
train gradient:  0.1344803055929046
iteration : 498
train acc:  0.7265625
train loss:  0.47132113575935364
train gradient:  0.0992457445468415
iteration : 499
train acc:  0.7890625
train loss:  0.4674007296562195
train gradient:  0.11051079524664478
iteration : 500
train acc:  0.71875
train loss:  0.4760434925556183
train gradient:  0.11787250129646726
iteration : 501
train acc:  0.7734375
train loss:  0.45748040080070496
train gradient:  0.11181385357734926
iteration : 502
train acc:  0.7109375
train loss:  0.5504641532897949
train gradient:  0.15244626273955114
iteration : 503
train acc:  0.703125
train loss:  0.5082643032073975
train gradient:  0.12430621216751415
iteration : 504
train acc:  0.75
train loss:  0.4754185676574707
train gradient:  0.15353438377888523
iteration : 505
train acc:  0.7578125
train loss:  0.4617602229118347
train gradient:  0.11519893238536844
iteration : 506
train acc:  0.75
train loss:  0.5190269947052002
train gradient:  0.1391919078227327
iteration : 507
train acc:  0.8046875
train loss:  0.3905891478061676
train gradient:  0.06832607051615855
iteration : 508
train acc:  0.7890625
train loss:  0.42238977551460266
train gradient:  0.09569223459477304
iteration : 509
train acc:  0.6796875
train loss:  0.5423756241798401
train gradient:  0.15725100229169042
iteration : 510
train acc:  0.78125
train loss:  0.4392065405845642
train gradient:  0.09768186558584029
iteration : 511
train acc:  0.78125
train loss:  0.4961295425891876
train gradient:  0.13867478067072503
iteration : 512
train acc:  0.703125
train loss:  0.5133605003356934
train gradient:  0.1190309934901605
iteration : 513
train acc:  0.75
train loss:  0.40804338455200195
train gradient:  0.07651557040979971
iteration : 514
train acc:  0.703125
train loss:  0.5547645092010498
train gradient:  0.14577005151622044
iteration : 515
train acc:  0.6875
train loss:  0.6232988238334656
train gradient:  0.16031928748914895
iteration : 516
train acc:  0.75
train loss:  0.5461071133613586
train gradient:  0.1633211579569499
iteration : 517
train acc:  0.6875
train loss:  0.5500317811965942
train gradient:  0.15177069157692355
iteration : 518
train acc:  0.6953125
train loss:  0.5076165795326233
train gradient:  0.1335363826736319
iteration : 519
train acc:  0.8125
train loss:  0.4185410737991333
train gradient:  0.09574544171395818
iteration : 520
train acc:  0.7578125
train loss:  0.45054706931114197
train gradient:  0.09384914071551115
iteration : 521
train acc:  0.796875
train loss:  0.41829901933670044
train gradient:  0.15162972061753627
iteration : 522
train acc:  0.734375
train loss:  0.48389217257499695
train gradient:  0.13565593632581513
iteration : 523
train acc:  0.796875
train loss:  0.4115241765975952
train gradient:  0.07353794332799282
iteration : 524
train acc:  0.6953125
train loss:  0.45887207984924316
train gradient:  0.08958823555307452
iteration : 525
train acc:  0.8125
train loss:  0.4781419634819031
train gradient:  0.1296906193705634
iteration : 526
train acc:  0.7578125
train loss:  0.42636311054229736
train gradient:  0.0879127447483627
iteration : 527
train acc:  0.6796875
train loss:  0.5642765164375305
train gradient:  0.1726757328706625
iteration : 528
train acc:  0.734375
train loss:  0.5133298635482788
train gradient:  0.12044320323864145
iteration : 529
train acc:  0.7578125
train loss:  0.4967587888240814
train gradient:  0.10547081402273599
iteration : 530
train acc:  0.734375
train loss:  0.5147354602813721
train gradient:  0.13330807599130606
iteration : 531
train acc:  0.8203125
train loss:  0.43450307846069336
train gradient:  0.09223033881758137
iteration : 532
train acc:  0.765625
train loss:  0.47885948419570923
train gradient:  0.11894240526972207
iteration : 533
train acc:  0.7421875
train loss:  0.48554277420043945
train gradient:  0.10023808544379524
iteration : 534
train acc:  0.765625
train loss:  0.4670775532722473
train gradient:  0.1259755444302864
iteration : 535
train acc:  0.7265625
train loss:  0.4572179913520813
train gradient:  0.10740590354345599
iteration : 536
train acc:  0.7734375
train loss:  0.4567010998725891
train gradient:  0.08524838444484391
iteration : 537
train acc:  0.7734375
train loss:  0.4483046531677246
train gradient:  0.12117571695186723
iteration : 538
train acc:  0.765625
train loss:  0.4886505901813507
train gradient:  0.1204521459664707
iteration : 539
train acc:  0.7578125
train loss:  0.4167781174182892
train gradient:  0.09442333089984122
iteration : 540
train acc:  0.703125
train loss:  0.4972264766693115
train gradient:  0.1410387710482866
iteration : 541
train acc:  0.671875
train loss:  0.5642713308334351
train gradient:  0.15005479600216598
iteration : 542
train acc:  0.7578125
train loss:  0.4453469514846802
train gradient:  0.09085493687625738
iteration : 543
train acc:  0.7890625
train loss:  0.4105609655380249
train gradient:  0.09124222065845246
iteration : 544
train acc:  0.7421875
train loss:  0.4549735486507416
train gradient:  0.11072718580284306
iteration : 545
train acc:  0.734375
train loss:  0.49852705001831055
train gradient:  0.10579087830155451
iteration : 546
train acc:  0.8046875
train loss:  0.391512393951416
train gradient:  0.08212552747500407
iteration : 547
train acc:  0.7890625
train loss:  0.43934935331344604
train gradient:  0.08144383256165791
iteration : 548
train acc:  0.71875
train loss:  0.49470627307891846
train gradient:  0.1310138661914089
iteration : 549
train acc:  0.75
train loss:  0.49512964487075806
train gradient:  0.12010546987413047
iteration : 550
train acc:  0.71875
train loss:  0.5642650723457336
train gradient:  0.14121295735364492
iteration : 551
train acc:  0.734375
train loss:  0.4657275676727295
train gradient:  0.10355585218823721
iteration : 552
train acc:  0.7421875
train loss:  0.5010064840316772
train gradient:  0.1109159329845996
iteration : 553
train acc:  0.6953125
train loss:  0.5444148778915405
train gradient:  0.13715917266160832
iteration : 554
train acc:  0.75
train loss:  0.49044203758239746
train gradient:  0.1182850186056768
iteration : 555
train acc:  0.703125
train loss:  0.5311586856842041
train gradient:  0.13993470435281063
iteration : 556
train acc:  0.7421875
train loss:  0.47431695461273193
train gradient:  0.09517310372134527
iteration : 557
train acc:  0.7265625
train loss:  0.493254154920578
train gradient:  0.12675652770690604
iteration : 558
train acc:  0.78125
train loss:  0.41171932220458984
train gradient:  0.07803975138910195
iteration : 559
train acc:  0.8046875
train loss:  0.40010011196136475
train gradient:  0.08361473630289595
iteration : 560
train acc:  0.84375
train loss:  0.3692777156829834
train gradient:  0.08477596585778303
iteration : 561
train acc:  0.734375
train loss:  0.4945557713508606
train gradient:  0.11821234517473148
iteration : 562
train acc:  0.734375
train loss:  0.5381993651390076
train gradient:  0.14950647314327292
iteration : 563
train acc:  0.7734375
train loss:  0.4667717218399048
train gradient:  0.11495067182165859
iteration : 564
train acc:  0.7421875
train loss:  0.445329874753952
train gradient:  0.08720895321079677
iteration : 565
train acc:  0.7734375
train loss:  0.4589686095714569
train gradient:  0.13582835951683914
iteration : 566
train acc:  0.7265625
train loss:  0.5387536287307739
train gradient:  0.17240787015448236
iteration : 567
train acc:  0.7578125
train loss:  0.4697292447090149
train gradient:  0.1220195696980752
iteration : 568
train acc:  0.6640625
train loss:  0.5403449535369873
train gradient:  0.14251733089012686
iteration : 569
train acc:  0.765625
train loss:  0.4645829200744629
train gradient:  0.10126149662088602
iteration : 570
train acc:  0.703125
train loss:  0.5248048305511475
train gradient:  0.13570050439704778
iteration : 571
train acc:  0.6640625
train loss:  0.585614800453186
train gradient:  0.1659431437356092
iteration : 572
train acc:  0.8046875
train loss:  0.44655144214630127
train gradient:  0.11347688569761137
iteration : 573
train acc:  0.765625
train loss:  0.40174978971481323
train gradient:  0.08451373984286009
iteration : 574
train acc:  0.7734375
train loss:  0.48039937019348145
train gradient:  0.13356822504421897
iteration : 575
train acc:  0.7578125
train loss:  0.49944180250167847
train gradient:  0.12658144274858918
iteration : 576
train acc:  0.78125
train loss:  0.4721158444881439
train gradient:  0.1228049514146506
iteration : 577
train acc:  0.765625
train loss:  0.488182932138443
train gradient:  0.10050893869141116
iteration : 578
train acc:  0.796875
train loss:  0.4629986584186554
train gradient:  0.11759199012153577
iteration : 579
train acc:  0.765625
train loss:  0.4475962519645691
train gradient:  0.12697676265772634
iteration : 580
train acc:  0.71875
train loss:  0.49035215377807617
train gradient:  0.14409297828176526
iteration : 581
train acc:  0.7578125
train loss:  0.46165239810943604
train gradient:  0.09826111183121146
iteration : 582
train acc:  0.6328125
train loss:  0.5934834480285645
train gradient:  0.17404639512595377
iteration : 583
train acc:  0.7734375
train loss:  0.4623919427394867
train gradient:  0.12876726408237887
iteration : 584
train acc:  0.6875
train loss:  0.5320732593536377
train gradient:  0.16570991067149382
iteration : 585
train acc:  0.765625
train loss:  0.4872039556503296
train gradient:  0.1163636976499953
iteration : 586
train acc:  0.6640625
train loss:  0.6143775582313538
train gradient:  0.1761659243867355
iteration : 587
train acc:  0.6953125
train loss:  0.5857594013214111
train gradient:  0.1418034755474094
iteration : 588
train acc:  0.75
train loss:  0.4562377631664276
train gradient:  0.1239235513296649
iteration : 589
train acc:  0.75
train loss:  0.46743735671043396
train gradient:  0.12398527033725161
iteration : 590
train acc:  0.78125
train loss:  0.40960320830345154
train gradient:  0.10623692781227699
iteration : 591
train acc:  0.7890625
train loss:  0.4191264808177948
train gradient:  0.09657226377780219
iteration : 592
train acc:  0.7734375
train loss:  0.46077486872673035
train gradient:  0.09218039840020464
iteration : 593
train acc:  0.7109375
train loss:  0.5598659515380859
train gradient:  0.19073765733369852
iteration : 594
train acc:  0.7734375
train loss:  0.5208836793899536
train gradient:  0.1301442866957901
iteration : 595
train acc:  0.703125
train loss:  0.5341931581497192
train gradient:  0.1437951946138834
iteration : 596
train acc:  0.7265625
train loss:  0.4698861241340637
train gradient:  0.09083144047276655
iteration : 597
train acc:  0.765625
train loss:  0.5196404457092285
train gradient:  0.1519537347547166
iteration : 598
train acc:  0.6875
train loss:  0.5752842426300049
train gradient:  0.14232855394200608
iteration : 599
train acc:  0.7578125
train loss:  0.43740314245224
train gradient:  0.10217465472698213
iteration : 600
train acc:  0.7578125
train loss:  0.4400767385959625
train gradient:  0.09661605824061038
iteration : 601
train acc:  0.7578125
train loss:  0.44978660345077515
train gradient:  0.1072768674450623
iteration : 602
train acc:  0.7578125
train loss:  0.46768879890441895
train gradient:  0.11675143519518055
iteration : 603
train acc:  0.78125
train loss:  0.4407469928264618
train gradient:  0.11947408878249008
iteration : 604
train acc:  0.75
train loss:  0.5265491604804993
train gradient:  0.12119426497592531
iteration : 605
train acc:  0.7421875
train loss:  0.4860842227935791
train gradient:  0.13974841601331273
iteration : 606
train acc:  0.71875
train loss:  0.49062591791152954
train gradient:  0.13410717213133272
iteration : 607
train acc:  0.75
train loss:  0.5140624642372131
train gradient:  0.13028281415345194
iteration : 608
train acc:  0.6953125
train loss:  0.5187515020370483
train gradient:  0.1253977630415234
iteration : 609
train acc:  0.734375
train loss:  0.47700345516204834
train gradient:  0.11206871226190136
iteration : 610
train acc:  0.7265625
train loss:  0.4972195625305176
train gradient:  0.10767435393683092
iteration : 611
train acc:  0.75
train loss:  0.4757211208343506
train gradient:  0.1015807997289575
iteration : 612
train acc:  0.671875
train loss:  0.5266364216804504
train gradient:  0.1322401742074903
iteration : 613
train acc:  0.7109375
train loss:  0.4723178744316101
train gradient:  0.09747510615736696
iteration : 614
train acc:  0.7265625
train loss:  0.5308358073234558
train gradient:  0.12008292837995976
iteration : 615
train acc:  0.8046875
train loss:  0.4816442131996155
train gradient:  0.11293839831950823
iteration : 616
train acc:  0.78125
train loss:  0.47559839487075806
train gradient:  0.10781524478552348
iteration : 617
train acc:  0.734375
train loss:  0.5503957271575928
train gradient:  0.15588093487256338
iteration : 618
train acc:  0.7421875
train loss:  0.4938367009162903
train gradient:  0.11744676345288745
iteration : 619
train acc:  0.6953125
train loss:  0.49820995330810547
train gradient:  0.12110338910847347
iteration : 620
train acc:  0.828125
train loss:  0.4245252013206482
train gradient:  0.11931012052466947
iteration : 621
train acc:  0.703125
train loss:  0.5105328559875488
train gradient:  0.18422443943636693
iteration : 622
train acc:  0.6953125
train loss:  0.5919579267501831
train gradient:  0.16885024974020568
iteration : 623
train acc:  0.828125
train loss:  0.41085368394851685
train gradient:  0.07751275885003908
iteration : 624
train acc:  0.6640625
train loss:  0.5077629089355469
train gradient:  0.11612262994873969
iteration : 625
train acc:  0.703125
train loss:  0.5088567733764648
train gradient:  0.10993031034219773
iteration : 626
train acc:  0.7578125
train loss:  0.4716678261756897
train gradient:  0.10315266558985452
iteration : 627
train acc:  0.7890625
train loss:  0.4569171369075775
train gradient:  0.11444958595683155
iteration : 628
train acc:  0.7890625
train loss:  0.44657421112060547
train gradient:  0.08907960228097948
iteration : 629
train acc:  0.7578125
train loss:  0.4585263729095459
train gradient:  0.1356065551758629
iteration : 630
train acc:  0.7890625
train loss:  0.43867406249046326
train gradient:  0.1077541005494957
iteration : 631
train acc:  0.75
train loss:  0.5009297132492065
train gradient:  0.14363802761380512
iteration : 632
train acc:  0.765625
train loss:  0.4218004643917084
train gradient:  0.09023905485917318
iteration : 633
train acc:  0.7265625
train loss:  0.48242148756980896
train gradient:  0.11724097203433269
iteration : 634
train acc:  0.7734375
train loss:  0.45401817560195923
train gradient:  0.0983985135542043
iteration : 635
train acc:  0.7734375
train loss:  0.4617166221141815
train gradient:  0.08240941926490136
iteration : 636
train acc:  0.75
train loss:  0.4716528058052063
train gradient:  0.09751174588126948
iteration : 637
train acc:  0.7890625
train loss:  0.47301629185676575
train gradient:  0.1390067437004601
iteration : 638
train acc:  0.6953125
train loss:  0.5217270851135254
train gradient:  0.13432533472029634
iteration : 639
train acc:  0.7109375
train loss:  0.4739442467689514
train gradient:  0.11792786815593093
iteration : 640
train acc:  0.765625
train loss:  0.448333740234375
train gradient:  0.09403172899423992
iteration : 641
train acc:  0.6796875
train loss:  0.5843280553817749
train gradient:  0.13579529115189054
iteration : 642
train acc:  0.6875
train loss:  0.5102710723876953
train gradient:  0.1211513623612045
iteration : 643
train acc:  0.7578125
train loss:  0.43868380784988403
train gradient:  0.09339915434306463
iteration : 644
train acc:  0.671875
train loss:  0.5950964689254761
train gradient:  0.2138323265889709
iteration : 645
train acc:  0.7421875
train loss:  0.4832220673561096
train gradient:  0.1138916133996747
iteration : 646
train acc:  0.7734375
train loss:  0.4305424988269806
train gradient:  0.09236011179024291
iteration : 647
train acc:  0.7734375
train loss:  0.43118053674697876
train gradient:  0.08545212845501696
iteration : 648
train acc:  0.71875
train loss:  0.4876018762588501
train gradient:  0.10822678204445818
iteration : 649
train acc:  0.78125
train loss:  0.4834650754928589
train gradient:  0.1278195717937844
iteration : 650
train acc:  0.703125
train loss:  0.5239392518997192
train gradient:  0.1337752981251161
iteration : 651
train acc:  0.8046875
train loss:  0.40519946813583374
train gradient:  0.11377619505733977
iteration : 652
train acc:  0.734375
train loss:  0.5288404226303101
train gradient:  0.1192541608663061
iteration : 653
train acc:  0.78125
train loss:  0.45689719915390015
train gradient:  0.10587163963982234
iteration : 654
train acc:  0.7578125
train loss:  0.44729334115982056
train gradient:  0.08810171041046873
iteration : 655
train acc:  0.7578125
train loss:  0.504054069519043
train gradient:  0.1132984060290344
iteration : 656
train acc:  0.7421875
train loss:  0.5060172080993652
train gradient:  0.1254110949877087
iteration : 657
train acc:  0.78125
train loss:  0.44125527143478394
train gradient:  0.0871761750114102
iteration : 658
train acc:  0.7265625
train loss:  0.5475864410400391
train gradient:  0.12590204744620276
iteration : 659
train acc:  0.7109375
train loss:  0.5153475999832153
train gradient:  0.1216719888356209
iteration : 660
train acc:  0.78125
train loss:  0.43179455399513245
train gradient:  0.09997917836092485
iteration : 661
train acc:  0.765625
train loss:  0.4816337525844574
train gradient:  0.1366035783378486
iteration : 662
train acc:  0.7109375
train loss:  0.4744604527950287
train gradient:  0.1307207108306176
iteration : 663
train acc:  0.7421875
train loss:  0.4976463317871094
train gradient:  0.10300272131967393
iteration : 664
train acc:  0.7265625
train loss:  0.473829448223114
train gradient:  0.14013326475155996
iteration : 665
train acc:  0.71875
train loss:  0.5312564373016357
train gradient:  0.11155341917695595
iteration : 666
train acc:  0.71875
train loss:  0.5093564987182617
train gradient:  0.11380419639008263
iteration : 667
train acc:  0.7109375
train loss:  0.501754105091095
train gradient:  0.14594029548645499
iteration : 668
train acc:  0.7421875
train loss:  0.48019886016845703
train gradient:  0.11788459981611918
iteration : 669
train acc:  0.7109375
train loss:  0.529358983039856
train gradient:  0.13856431750192577
iteration : 670
train acc:  0.734375
train loss:  0.4950884282588959
train gradient:  0.09167390825494875
iteration : 671
train acc:  0.7734375
train loss:  0.47197026014328003
train gradient:  0.14403588198311668
iteration : 672
train acc:  0.7421875
train loss:  0.4351762533187866
train gradient:  0.09138099925761616
iteration : 673
train acc:  0.734375
train loss:  0.5361865758895874
train gradient:  0.13209815566932745
iteration : 674
train acc:  0.7421875
train loss:  0.4574013948440552
train gradient:  0.10532858462811155
iteration : 675
train acc:  0.796875
train loss:  0.4397308826446533
train gradient:  0.08870101040956199
iteration : 676
train acc:  0.78125
train loss:  0.4442136883735657
train gradient:  0.09446659099365359
iteration : 677
train acc:  0.734375
train loss:  0.5315665006637573
train gradient:  0.11402260982516275
iteration : 678
train acc:  0.765625
train loss:  0.4921196699142456
train gradient:  0.11759704239095381
iteration : 679
train acc:  0.7265625
train loss:  0.4885897934436798
train gradient:  0.1369388904463471
iteration : 680
train acc:  0.7734375
train loss:  0.45803430676460266
train gradient:  0.12334857897386316
iteration : 681
train acc:  0.7265625
train loss:  0.4912117123603821
train gradient:  0.11779334531502388
iteration : 682
train acc:  0.765625
train loss:  0.5008069276809692
train gradient:  0.10572689028921367
iteration : 683
train acc:  0.765625
train loss:  0.5008570551872253
train gradient:  0.13316131676067477
iteration : 684
train acc:  0.75
train loss:  0.43330883979797363
train gradient:  0.10842576130123793
iteration : 685
train acc:  0.75
train loss:  0.46766728162765503
train gradient:  0.10746448234324854
iteration : 686
train acc:  0.7734375
train loss:  0.48860833048820496
train gradient:  0.10429499740211343
iteration : 687
train acc:  0.6796875
train loss:  0.4930056631565094
train gradient:  0.10010673615978642
iteration : 688
train acc:  0.8359375
train loss:  0.3790421187877655
train gradient:  0.07245194514527303
iteration : 689
train acc:  0.7734375
train loss:  0.44388875365257263
train gradient:  0.09756816527047046
iteration : 690
train acc:  0.7578125
train loss:  0.4865766763687134
train gradient:  0.09894397696470306
iteration : 691
train acc:  0.7578125
train loss:  0.4455184042453766
train gradient:  0.0926943484688644
iteration : 692
train acc:  0.75
train loss:  0.48186013102531433
train gradient:  0.12816479480152593
iteration : 693
train acc:  0.6875
train loss:  0.5640254020690918
train gradient:  0.15815400869427276
iteration : 694
train acc:  0.78125
train loss:  0.4677940309047699
train gradient:  0.1273444717743785
iteration : 695
train acc:  0.8125
train loss:  0.504396915435791
train gradient:  0.11647646768413274
iteration : 696
train acc:  0.7265625
train loss:  0.4616444706916809
train gradient:  0.09847082668692443
iteration : 697
train acc:  0.71875
train loss:  0.5140466690063477
train gradient:  0.12103464333208314
iteration : 698
train acc:  0.71875
train loss:  0.49970102310180664
train gradient:  0.13944666305627584
iteration : 699
train acc:  0.7265625
train loss:  0.5329772233963013
train gradient:  0.14206934205686916
iteration : 700
train acc:  0.75
train loss:  0.517830491065979
train gradient:  0.1278007020948309
iteration : 701
train acc:  0.671875
train loss:  0.5526260137557983
train gradient:  0.13682077768317863
iteration : 702
train acc:  0.796875
train loss:  0.4499826431274414
train gradient:  0.0885805968172054
iteration : 703
train acc:  0.8046875
train loss:  0.4534319043159485
train gradient:  0.10765094679903384
iteration : 704
train acc:  0.765625
train loss:  0.455187052488327
train gradient:  0.13013334077733274
iteration : 705
train acc:  0.765625
train loss:  0.4629490077495575
train gradient:  0.12278294020045961
iteration : 706
train acc:  0.7890625
train loss:  0.4235267639160156
train gradient:  0.11470352514344358
iteration : 707
train acc:  0.7421875
train loss:  0.5083293914794922
train gradient:  0.11918587615921233
iteration : 708
train acc:  0.75
train loss:  0.5214642882347107
train gradient:  0.1087609601567869
iteration : 709
train acc:  0.703125
train loss:  0.4879817068576813
train gradient:  0.12812896802614024
iteration : 710
train acc:  0.7109375
train loss:  0.5177932977676392
train gradient:  0.11289341177736871
iteration : 711
train acc:  0.71875
train loss:  0.552192211151123
train gradient:  0.1571107220877469
iteration : 712
train acc:  0.7421875
train loss:  0.4489727020263672
train gradient:  0.10225445264902454
iteration : 713
train acc:  0.734375
train loss:  0.4822250008583069
train gradient:  0.10874835974355025
iteration : 714
train acc:  0.7265625
train loss:  0.552777111530304
train gradient:  0.1436226718874766
iteration : 715
train acc:  0.796875
train loss:  0.4259973466396332
train gradient:  0.08186639065638401
iteration : 716
train acc:  0.7421875
train loss:  0.44298258423805237
train gradient:  0.09368770834721943
iteration : 717
train acc:  0.7890625
train loss:  0.4352656602859497
train gradient:  0.09110140747407253
iteration : 718
train acc:  0.7734375
train loss:  0.440155029296875
train gradient:  0.09257947523458913
iteration : 719
train acc:  0.7734375
train loss:  0.46981966495513916
train gradient:  0.14379366271337046
iteration : 720
train acc:  0.7109375
train loss:  0.49979501962661743
train gradient:  0.11036144586414363
iteration : 721
train acc:  0.78125
train loss:  0.49121493101119995
train gradient:  0.1346628888555751
iteration : 722
train acc:  0.71875
train loss:  0.5000313520431519
train gradient:  0.13755942093512327
iteration : 723
train acc:  0.75
train loss:  0.4854021370410919
train gradient:  0.11752501529793531
iteration : 724
train acc:  0.734375
train loss:  0.5127483010292053
train gradient:  0.12205732409960933
iteration : 725
train acc:  0.8125
train loss:  0.4197848439216614
train gradient:  0.09489900262913703
iteration : 726
train acc:  0.7265625
train loss:  0.4973134398460388
train gradient:  0.13495241883220402
iteration : 727
train acc:  0.734375
train loss:  0.4555070698261261
train gradient:  0.08900731148523217
iteration : 728
train acc:  0.7109375
train loss:  0.5945254564285278
train gradient:  0.16989920025955382
iteration : 729
train acc:  0.78125
train loss:  0.42127135396003723
train gradient:  0.07591943139975468
iteration : 730
train acc:  0.8046875
train loss:  0.39200007915496826
train gradient:  0.08956599643152781
iteration : 731
train acc:  0.75
train loss:  0.4862552583217621
train gradient:  0.11829476657084352
iteration : 732
train acc:  0.703125
train loss:  0.47724443674087524
train gradient:  0.08506960292854593
iteration : 733
train acc:  0.6953125
train loss:  0.521225094795227
train gradient:  0.12282048245993761
iteration : 734
train acc:  0.703125
train loss:  0.5034275054931641
train gradient:  0.11388446305169811
iteration : 735
train acc:  0.8046875
train loss:  0.3595502972602844
train gradient:  0.07278417748488543
iteration : 736
train acc:  0.765625
train loss:  0.5279353857040405
train gradient:  0.1413458441818337
iteration : 737
train acc:  0.765625
train loss:  0.5106074213981628
train gradient:  0.11595254134642498
iteration : 738
train acc:  0.7734375
train loss:  0.44855064153671265
train gradient:  0.09808741056927531
iteration : 739
train acc:  0.75
train loss:  0.4561018645763397
train gradient:  0.09452024027064908
iteration : 740
train acc:  0.7734375
train loss:  0.452503502368927
train gradient:  0.12178050347680157
iteration : 741
train acc:  0.734375
train loss:  0.48118358850479126
train gradient:  0.11240858596908102
iteration : 742
train acc:  0.7578125
train loss:  0.4745556116104126
train gradient:  0.10308264560637959
iteration : 743
train acc:  0.7265625
train loss:  0.5342962145805359
train gradient:  0.11897895099039531
iteration : 744
train acc:  0.7421875
train loss:  0.5195174217224121
train gradient:  0.13590618458964507
iteration : 745
train acc:  0.6796875
train loss:  0.5188437700271606
train gradient:  0.10168547731297137
iteration : 746
train acc:  0.7578125
train loss:  0.4707399606704712
train gradient:  0.1002653587751644
iteration : 747
train acc:  0.671875
train loss:  0.5590143203735352
train gradient:  0.1650298554673511
iteration : 748
train acc:  0.75
train loss:  0.5097639560699463
train gradient:  0.14264702763525355
iteration : 749
train acc:  0.78125
train loss:  0.4284808039665222
train gradient:  0.10912601729226115
iteration : 750
train acc:  0.78125
train loss:  0.4527968168258667
train gradient:  0.09172005823117423
iteration : 751
train acc:  0.7578125
train loss:  0.4411887526512146
train gradient:  0.08364203391815274
iteration : 752
train acc:  0.7734375
train loss:  0.49033069610595703
train gradient:  0.14396765530340083
iteration : 753
train acc:  0.8046875
train loss:  0.412479043006897
train gradient:  0.07103350545872482
iteration : 754
train acc:  0.7421875
train loss:  0.4582974910736084
train gradient:  0.09635842371283265
iteration : 755
train acc:  0.6953125
train loss:  0.5049521923065186
train gradient:  0.11002872443613805
iteration : 756
train acc:  0.7421875
train loss:  0.5096062421798706
train gradient:  0.1075486764509217
iteration : 757
train acc:  0.7265625
train loss:  0.5414085388183594
train gradient:  0.13212189294182966
iteration : 758
train acc:  0.7421875
train loss:  0.5202841758728027
train gradient:  0.12965881334178914
iteration : 759
train acc:  0.6953125
train loss:  0.49191954731941223
train gradient:  0.12189554952306615
iteration : 760
train acc:  0.734375
train loss:  0.4785543978214264
train gradient:  0.11336305885213505
iteration : 761
train acc:  0.7265625
train loss:  0.4684262275695801
train gradient:  0.12715473409662945
iteration : 762
train acc:  0.75
train loss:  0.45474183559417725
train gradient:  0.10599419055762123
iteration : 763
train acc:  0.75
train loss:  0.5140302181243896
train gradient:  0.14562010305489442
iteration : 764
train acc:  0.734375
train loss:  0.5372498035430908
train gradient:  0.1401764149881299
iteration : 765
train acc:  0.734375
train loss:  0.46964114904403687
train gradient:  0.10958038306909805
iteration : 766
train acc:  0.7578125
train loss:  0.46557462215423584
train gradient:  0.10625460324115267
iteration : 767
train acc:  0.7421875
train loss:  0.5215675830841064
train gradient:  0.1528438511872211
iteration : 768
train acc:  0.734375
train loss:  0.46945920586586
train gradient:  0.10903447127769073
iteration : 769
train acc:  0.71875
train loss:  0.5868750214576721
train gradient:  0.15696519988090113
iteration : 770
train acc:  0.7578125
train loss:  0.4686671197414398
train gradient:  0.09945720845753282
iteration : 771
train acc:  0.765625
train loss:  0.4609982967376709
train gradient:  0.10202959873626238
iteration : 772
train acc:  0.765625
train loss:  0.4586401879787445
train gradient:  0.11595305300692375
iteration : 773
train acc:  0.78125
train loss:  0.46178585290908813
train gradient:  0.09465873142082945
iteration : 774
train acc:  0.7734375
train loss:  0.4332107901573181
train gradient:  0.08898586504057863
iteration : 775
train acc:  0.78125
train loss:  0.49081292748451233
train gradient:  0.11082330583230214
iteration : 776
train acc:  0.71875
train loss:  0.4984162151813507
train gradient:  0.10604865880754068
iteration : 777
train acc:  0.71875
train loss:  0.48854365944862366
train gradient:  0.0930318071992523
iteration : 778
train acc:  0.71875
train loss:  0.5167869329452515
train gradient:  0.11556155453336864
