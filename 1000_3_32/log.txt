program start:
num_rounds= 3
node_emb_dim= 32

----------------------------------------new_epoch--------------------------------------

epoch:  0
iteration : 0
train acc:  0.4296875
train loss:  0.7320830225944519
train gradient:  4.2205874728652795
iteration : 1
train acc:  0.53125
train loss:  0.695777952671051
train gradient:  2.488059053085516
iteration : 2
train acc:  0.578125
train loss:  0.6981188058853149
train gradient:  4.52350192183381
iteration : 3
train acc:  0.53125
train loss:  0.7088832855224609
train gradient:  3.5529529569192015
iteration : 4
train acc:  0.5390625
train loss:  0.687792181968689
train gradient:  1.349223376054086
iteration : 5
train acc:  0.5078125
train loss:  0.6895148754119873
train gradient:  1.4171023849058064
iteration : 6
train acc:  0.5859375
train loss:  0.6676034927368164
train gradient:  0.7553312763602789
iteration : 7
train acc:  0.6015625
train loss:  0.7060253620147705
train gradient:  3.0556096168424656
iteration : 8
train acc:  0.4765625
train loss:  0.7058475017547607
train gradient:  0.7114906763444806
iteration : 9
train acc:  0.484375
train loss:  0.7099565267562866
train gradient:  1.08714828486599
iteration : 10
train acc:  0.625
train loss:  0.6462889909744263
train gradient:  0.7428820276665821
iteration : 11
train acc:  0.53125
train loss:  0.6824131011962891
train gradient:  0.7682822843706458
iteration : 12
train acc:  0.453125
train loss:  0.7002736330032349
train gradient:  0.7590114362588976
iteration : 13
train acc:  0.515625
train loss:  0.6893609166145325
train gradient:  0.8964126782787607
iteration : 14
train acc:  0.5703125
train loss:  0.6589243412017822
train gradient:  0.47221181873902235
iteration : 15
train acc:  0.6015625
train loss:  0.699241578578949
train gradient:  1.8985618704202787
iteration : 16
train acc:  0.546875
train loss:  0.6675680875778198
train gradient:  0.30039359970940155
iteration : 17
train acc:  0.515625
train loss:  0.6965739130973816
train gradient:  0.3699105467135132
iteration : 18
train acc:  0.5390625
train loss:  0.7210782170295715
train gradient:  0.7072163803891675
iteration : 19
train acc:  0.5
train loss:  0.713485836982727
train gradient:  1.2508998090762515
iteration : 20
train acc:  0.5859375
train loss:  0.6589720249176025
train gradient:  0.9823554642285836
iteration : 21
train acc:  0.546875
train loss:  0.6882332563400269
train gradient:  1.8898780771286159
iteration : 22
train acc:  0.59375
train loss:  0.7100328207015991
train gradient:  1.0686560144615036
iteration : 23
train acc:  0.5859375
train loss:  0.6629483699798584
train gradient:  0.38667622753813663
iteration : 24
train acc:  0.5859375
train loss:  0.6870450973510742
train gradient:  0.8734038445035325
iteration : 25
train acc:  0.6015625
train loss:  0.654678463935852
train gradient:  0.3453454153433945
iteration : 26
train acc:  0.5703125
train loss:  0.6527436375617981
train gradient:  0.34510137422407455
iteration : 27
train acc:  0.59375
train loss:  0.6786214709281921
train gradient:  0.407805512548722
iteration : 28
train acc:  0.546875
train loss:  0.6736522912979126
train gradient:  0.24062730486892156
iteration : 29
train acc:  0.5703125
train loss:  0.6435931921005249
train gradient:  0.2391007539292874
iteration : 30
train acc:  0.625
train loss:  0.6528153419494629
train gradient:  0.24782567410046627
iteration : 31
train acc:  0.6015625
train loss:  0.6787777543067932
train gradient:  0.5938043830833748
iteration : 32
train acc:  0.4921875
train loss:  0.6982346773147583
train gradient:  0.8855435771366964
iteration : 33
train acc:  0.5390625
train loss:  0.6820890307426453
train gradient:  1.0222342287880701
iteration : 34
train acc:  0.484375
train loss:  0.7262219190597534
train gradient:  1.125766911676379
iteration : 35
train acc:  0.578125
train loss:  0.6920820474624634
train gradient:  0.5785354513769996
iteration : 36
train acc:  0.5625
train loss:  0.6941806077957153
train gradient:  0.49831387606263966
iteration : 37
train acc:  0.578125
train loss:  0.6747243404388428
train gradient:  0.7128986255680372
iteration : 38
train acc:  0.578125
train loss:  0.6982555389404297
train gradient:  1.0659784483047035
iteration : 39
train acc:  0.609375
train loss:  0.6829944849014282
train gradient:  0.9461291846317065
iteration : 40
train acc:  0.5390625
train loss:  0.6637864112854004
train gradient:  0.29191419578876787
iteration : 41
train acc:  0.609375
train loss:  0.6628806591033936
train gradient:  0.8593934916650241
iteration : 42
train acc:  0.515625
train loss:  0.7005445957183838
train gradient:  0.5597601975393456
iteration : 43
train acc:  0.578125
train loss:  0.6839121580123901
train gradient:  0.7566300636368519
iteration : 44
train acc:  0.6328125
train loss:  0.683495044708252
train gradient:  0.55859303496713
iteration : 45
train acc:  0.578125
train loss:  0.6647237539291382
train gradient:  0.24758462268843273
iteration : 46
train acc:  0.546875
train loss:  0.6596859693527222
train gradient:  0.33389217540136307
iteration : 47
train acc:  0.609375
train loss:  0.6778048276901245
train gradient:  0.426901163279009
iteration : 48
train acc:  0.5859375
train loss:  0.6850844621658325
train gradient:  0.46697199840316994
iteration : 49
train acc:  0.5546875
train loss:  0.6792334914207458
train gradient:  0.5047498627161897
iteration : 50
train acc:  0.59375
train loss:  0.6830095648765564
train gradient:  0.7208775725283773
iteration : 51
train acc:  0.5234375
train loss:  0.7104780077934265
train gradient:  0.5763089757882145
iteration : 52
train acc:  0.6171875
train loss:  0.6470710039138794
train gradient:  0.5566044598306374
iteration : 53
train acc:  0.6484375
train loss:  0.6814438104629517
train gradient:  0.5122757553730797
iteration : 54
train acc:  0.671875
train loss:  0.6246668100357056
train gradient:  0.6639763696166261
iteration : 55
train acc:  0.59375
train loss:  0.6683633327484131
train gradient:  0.2877313771991808
iteration : 56
train acc:  0.625
train loss:  0.6309055089950562
train gradient:  0.27225701226824617
iteration : 57
train acc:  0.546875
train loss:  0.6684530973434448
train gradient:  0.6651564788246732
iteration : 58
train acc:  0.640625
train loss:  0.6673334836959839
train gradient:  0.41527114212733296
iteration : 59
train acc:  0.59375
train loss:  0.6743625402450562
train gradient:  0.7575305632143121
iteration : 60
train acc:  0.6015625
train loss:  0.6727033853530884
train gradient:  0.341193157001392
iteration : 61
train acc:  0.6640625
train loss:  0.6145484447479248
train gradient:  0.5069980559138798
iteration : 62
train acc:  0.6640625
train loss:  0.6428651809692383
train gradient:  0.7084012040901906
iteration : 63
train acc:  0.6875
train loss:  0.6358817219734192
train gradient:  0.5938035220196244
iteration : 64
train acc:  0.6484375
train loss:  0.6364460587501526
train gradient:  0.2726998046957055
iteration : 65
train acc:  0.6484375
train loss:  0.6275109052658081
train gradient:  0.5798742498228877
iteration : 66
train acc:  0.6328125
train loss:  0.6535519361495972
train gradient:  0.7486232658705021
iteration : 67
train acc:  0.5859375
train loss:  0.666969358921051
train gradient:  0.39241159784268675
iteration : 68
train acc:  0.5390625
train loss:  0.6769462823867798
train gradient:  0.4900502827775984
iteration : 69
train acc:  0.65625
train loss:  0.6303527355194092
train gradient:  0.679808228626682
iteration : 70
train acc:  0.671875
train loss:  0.6199971437454224
train gradient:  0.7901549479070397
iteration : 71
train acc:  0.609375
train loss:  0.6340034008026123
train gradient:  0.42315001219061016
iteration : 72
train acc:  0.59375
train loss:  0.6688088178634644
train gradient:  0.5309259144321523
iteration : 73
train acc:  0.578125
train loss:  0.6464532613754272
train gradient:  0.4501460221765871
iteration : 74
train acc:  0.6484375
train loss:  0.6336368918418884
train gradient:  0.3240844265933962
iteration : 75
train acc:  0.6328125
train loss:  0.6385520696640015
train gradient:  0.23010356418639655
iteration : 76
train acc:  0.671875
train loss:  0.6629531383514404
train gradient:  0.7755390446156041
iteration : 77
train acc:  0.5703125
train loss:  0.689302921295166
train gradient:  0.579630572412251
iteration : 78
train acc:  0.546875
train loss:  0.7266562581062317
train gradient:  1.3221062609150143
iteration : 79
train acc:  0.546875
train loss:  0.6810962557792664
train gradient:  0.37043463463254866
iteration : 80
train acc:  0.6328125
train loss:  0.6375042200088501
train gradient:  0.23030816408925087
iteration : 81
train acc:  0.578125
train loss:  0.645953893661499
train gradient:  0.36459898967142185
iteration : 82
train acc:  0.609375
train loss:  0.6232957243919373
train gradient:  0.3913001169249092
iteration : 83
train acc:  0.7421875
train loss:  0.5981242656707764
train gradient:  0.2663565483015395
iteration : 84
train acc:  0.5703125
train loss:  0.6361596584320068
train gradient:  0.5890258089688665
iteration : 85
train acc:  0.6953125
train loss:  0.6171908378601074
train gradient:  0.5137307790133967
iteration : 86
train acc:  0.5546875
train loss:  0.6531869173049927
train gradient:  0.5127162935000414
iteration : 87
train acc:  0.6015625
train loss:  0.6663204431533813
train gradient:  0.4607035615869425
iteration : 88
train acc:  0.6171875
train loss:  0.6569183468818665
train gradient:  0.8138333319076363
iteration : 89
train acc:  0.625
train loss:  0.6324219703674316
train gradient:  0.37275688530078943
iteration : 90
train acc:  0.6484375
train loss:  0.6277838349342346
train gradient:  0.20934388608021032
iteration : 91
train acc:  0.6171875
train loss:  0.6365351676940918
train gradient:  0.24109855687671178
iteration : 92
train acc:  0.609375
train loss:  0.633030354976654
train gradient:  0.26395552616458995
iteration : 93
train acc:  0.625
train loss:  0.6446714997291565
train gradient:  0.32965403950986166
iteration : 94
train acc:  0.65625
train loss:  0.6434674263000488
train gradient:  0.4031734580266208
iteration : 95
train acc:  0.5703125
train loss:  0.6843240261077881
train gradient:  0.7635899448984055
iteration : 96
train acc:  0.703125
train loss:  0.6060130596160889
train gradient:  0.3164381533431655
iteration : 97
train acc:  0.5859375
train loss:  0.6452152729034424
train gradient:  0.6673162137838003
iteration : 98
train acc:  0.625
train loss:  0.6431657075881958
train gradient:  0.8030212010244864
iteration : 99
train acc:  0.6953125
train loss:  0.6137583255767822
train gradient:  0.34918366209138796
iteration : 100
train acc:  0.6328125
train loss:  0.6254168748855591
train gradient:  0.2838464065890815
iteration : 101
train acc:  0.6015625
train loss:  0.6476835012435913
train gradient:  0.4083616678560487
iteration : 102
train acc:  0.6640625
train loss:  0.6173794269561768
train gradient:  0.34587533663168385
iteration : 103
train acc:  0.6640625
train loss:  0.6435633301734924
train gradient:  0.6656465038203396
iteration : 104
train acc:  0.671875
train loss:  0.6046602725982666
train gradient:  0.47618221051972065
iteration : 105
train acc:  0.6640625
train loss:  0.6089133024215698
train gradient:  0.19916128944213807
iteration : 106
train acc:  0.6328125
train loss:  0.620499849319458
train gradient:  0.36464191655339273
iteration : 107
train acc:  0.6484375
train loss:  0.622125506401062
train gradient:  0.3953794428103615
iteration : 108
train acc:  0.640625
train loss:  0.6669092774391174
train gradient:  0.5117804832188022
iteration : 109
train acc:  0.6328125
train loss:  0.6407554745674133
train gradient:  0.3108410231802966
iteration : 110
train acc:  0.71875
train loss:  0.585882842540741
train gradient:  0.24151686317233298
iteration : 111
train acc:  0.671875
train loss:  0.6094145774841309
train gradient:  0.4187359890256333
iteration : 112
train acc:  0.609375
train loss:  0.6623615026473999
train gradient:  0.5001606467065174
iteration : 113
train acc:  0.6640625
train loss:  0.6245434284210205
train gradient:  0.3432906143369361
iteration : 114
train acc:  0.6484375
train loss:  0.6474905014038086
train gradient:  0.4507855952429947
iteration : 115
train acc:  0.6171875
train loss:  0.6402237415313721
train gradient:  0.3744807853365396
iteration : 116
train acc:  0.6171875
train loss:  0.6664624214172363
train gradient:  0.6359320546907846
iteration : 117
train acc:  0.75
train loss:  0.5934419631958008
train gradient:  0.6368485690060424
iteration : 118
train acc:  0.6875
train loss:  0.6048119068145752
train gradient:  0.6386335004801572
iteration : 119
train acc:  0.640625
train loss:  0.5999389886856079
train gradient:  0.42490692673418734
iteration : 120
train acc:  0.625
train loss:  0.6543123722076416
train gradient:  0.5714868965886901
iteration : 121
train acc:  0.6796875
train loss:  0.5974284410476685
train gradient:  0.35476295993054846
iteration : 122
train acc:  0.609375
train loss:  0.6274168491363525
train gradient:  1.264718765382539
iteration : 123
train acc:  0.609375
train loss:  0.6316995620727539
train gradient:  0.5317332204921799
iteration : 124
train acc:  0.6328125
train loss:  0.6707035303115845
train gradient:  0.696846390883662
iteration : 125
train acc:  0.6484375
train loss:  0.5877114534378052
train gradient:  0.22363995240901055
iteration : 126
train acc:  0.6171875
train loss:  0.6486035585403442
train gradient:  0.5737655873994792
iteration : 127
train acc:  0.5546875
train loss:  0.7197001576423645
train gradient:  2.8896456678047437
iteration : 128
train acc:  0.5625
train loss:  0.6593379974365234
train gradient:  0.5993558472191034
iteration : 129
train acc:  0.6640625
train loss:  0.6234078407287598
train gradient:  0.47919059734637254
iteration : 130
train acc:  0.5625
train loss:  0.6882121562957764
train gradient:  0.5599958641043843
iteration : 131
train acc:  0.6171875
train loss:  0.6261153817176819
train gradient:  0.9074563570000431
iteration : 132
train acc:  0.6015625
train loss:  0.646180272102356
train gradient:  0.883204620554453
iteration : 133
train acc:  0.5859375
train loss:  0.6317740678787231
train gradient:  0.5185767817079066
iteration : 134
train acc:  0.6796875
train loss:  0.6220821142196655
train gradient:  0.5024139839501255
iteration : 135
train acc:  0.6328125
train loss:  0.6339117288589478
train gradient:  0.584202738217707
iteration : 136
train acc:  0.6484375
train loss:  0.6269254684448242
train gradient:  0.33625369344600875
iteration : 137
train acc:  0.609375
train loss:  0.6387811899185181
train gradient:  0.6103337819088898
iteration : 138
train acc:  0.671875
train loss:  0.5834188461303711
train gradient:  0.4625565557419972
iteration : 139
train acc:  0.625
train loss:  0.6538372039794922
train gradient:  0.6032531802437099
iteration : 140
train acc:  0.640625
train loss:  0.608963131904602
train gradient:  0.3804990455161697
iteration : 141
train acc:  0.6484375
train loss:  0.6275209784507751
train gradient:  0.4704692022893108
iteration : 142
train acc:  0.703125
train loss:  0.5844337940216064
train gradient:  0.5623298596770803
iteration : 143
train acc:  0.625
train loss:  0.6027311682701111
train gradient:  0.49287328268994324
iteration : 144
train acc:  0.6171875
train loss:  0.6312166452407837
train gradient:  0.2279627236383258
iteration : 145
train acc:  0.640625
train loss:  0.6657235622406006
train gradient:  0.5975547082733037
iteration : 146
train acc:  0.703125
train loss:  0.5645999908447266
train gradient:  0.33674748366685187
iteration : 147
train acc:  0.640625
train loss:  0.6221279501914978
train gradient:  0.469392912765997
iteration : 148
train acc:  0.703125
train loss:  0.5919463634490967
train gradient:  0.30521376018257607
iteration : 149
train acc:  0.640625
train loss:  0.594158947467804
train gradient:  0.36068128812728845
iteration : 150
train acc:  0.6640625
train loss:  0.5846090912818909
train gradient:  0.5169714444218804
iteration : 151
train acc:  0.6171875
train loss:  0.6797206401824951
train gradient:  1.0971237865365882
iteration : 152
train acc:  0.609375
train loss:  0.6266911029815674
train gradient:  0.3169740598912512
iteration : 153
train acc:  0.625
train loss:  0.6297829747200012
train gradient:  0.3336215187943188
iteration : 154
train acc:  0.6875
train loss:  0.6122444868087769
train gradient:  0.5253764909888688
iteration : 155
train acc:  0.75
train loss:  0.5637576580047607
train gradient:  0.2607349452344657
iteration : 156
train acc:  0.578125
train loss:  0.6357094049453735
train gradient:  0.3879597166906514
iteration : 157
train acc:  0.640625
train loss:  0.5922995805740356
train gradient:  0.30877810361193253
iteration : 158
train acc:  0.578125
train loss:  0.6814554929733276
train gradient:  0.5423595587091805
iteration : 159
train acc:  0.671875
train loss:  0.5862023830413818
train gradient:  0.2638702029160781
iteration : 160
train acc:  0.6484375
train loss:  0.6368148922920227
train gradient:  0.4721325834179752
iteration : 161
train acc:  0.7265625
train loss:  0.619114875793457
train gradient:  0.6965639563152448
iteration : 162
train acc:  0.640625
train loss:  0.5903186202049255
train gradient:  0.31241296961044457
iteration : 163
train acc:  0.6796875
train loss:  0.5915780663490295
train gradient:  0.480007366356366
iteration : 164
train acc:  0.6171875
train loss:  0.6261560320854187
train gradient:  0.35676611217758064
iteration : 165
train acc:  0.6171875
train loss:  0.6264718174934387
train gradient:  0.4008468839255105
iteration : 166
train acc:  0.65625
train loss:  0.6106185913085938
train gradient:  0.34271036877131666
iteration : 167
train acc:  0.6953125
train loss:  0.5916033983230591
train gradient:  0.4504534253717673
iteration : 168
train acc:  0.6171875
train loss:  0.6355769634246826
train gradient:  0.499355074099569
iteration : 169
train acc:  0.640625
train loss:  0.5977615714073181
train gradient:  0.3961686479418527
iteration : 170
train acc:  0.625
train loss:  0.6552375555038452
train gradient:  0.4394514479524285
iteration : 171
train acc:  0.6875
train loss:  0.5989730954170227
train gradient:  0.3650190521451466
iteration : 172
train acc:  0.65625
train loss:  0.5886147022247314
train gradient:  0.27871024394511507
iteration : 173
train acc:  0.625
train loss:  0.6084554195404053
train gradient:  0.5614847036871654
iteration : 174
train acc:  0.6953125
train loss:  0.5909701585769653
train gradient:  0.5954531674950473
iteration : 175
train acc:  0.7109375
train loss:  0.5938457250595093
train gradient:  0.21433929380074163
iteration : 176
train acc:  0.65625
train loss:  0.6141947507858276
train gradient:  0.33486606069654495
iteration : 177
train acc:  0.6484375
train loss:  0.6362563967704773
train gradient:  0.5675082460990026
iteration : 178
train acc:  0.671875
train loss:  0.5777506828308105
train gradient:  0.27095222356405874
iteration : 179
train acc:  0.6015625
train loss:  0.6238266229629517
train gradient:  0.2759836356899015
iteration : 180
train acc:  0.609375
train loss:  0.6414427757263184
train gradient:  0.3851864565182955
iteration : 181
train acc:  0.578125
train loss:  0.6105396747589111
train gradient:  0.44912137168511573
iteration : 182
train acc:  0.7265625
train loss:  0.572455644607544
train gradient:  0.6237353956269487
iteration : 183
train acc:  0.671875
train loss:  0.6113845109939575
train gradient:  0.3222570098540433
iteration : 184
train acc:  0.6796875
train loss:  0.5685961842536926
train gradient:  0.5033521805872774
iteration : 185
train acc:  0.640625
train loss:  0.6469525694847107
train gradient:  0.4402064872867266
iteration : 186
train acc:  0.6171875
train loss:  0.6847946643829346
train gradient:  0.8830741235119028
iteration : 187
train acc:  0.640625
train loss:  0.6350321173667908
train gradient:  0.7801957345349754
iteration : 188
train acc:  0.671875
train loss:  0.5810483694076538
train gradient:  0.3335943976935783
iteration : 189
train acc:  0.6640625
train loss:  0.5944217443466187
train gradient:  0.7092335983323906
iteration : 190
train acc:  0.703125
train loss:  0.5647181868553162
train gradient:  0.40411389961629207
iteration : 191
train acc:  0.59375
train loss:  0.6492455005645752
train gradient:  0.7563230735093377
iteration : 192
train acc:  0.6875
train loss:  0.591283917427063
train gradient:  0.39102909362945404
iteration : 193
train acc:  0.6640625
train loss:  0.612973690032959
train gradient:  0.3470096681967785
iteration : 194
train acc:  0.640625
train loss:  0.6047168970108032
train gradient:  0.4545428868206768
iteration : 195
train acc:  0.7734375
train loss:  0.5253820419311523
train gradient:  0.5204357131039433
iteration : 196
train acc:  0.625
train loss:  0.6231648921966553
train gradient:  0.46236128672917937
iteration : 197
train acc:  0.6875
train loss:  0.5821034908294678
train gradient:  0.37877968176669485
iteration : 198
train acc:  0.6796875
train loss:  0.5836933851242065
train gradient:  0.6259606016689543
iteration : 199
train acc:  0.6953125
train loss:  0.5782163739204407
train gradient:  0.5409194343612975
iteration : 200
train acc:  0.703125
train loss:  0.5480473041534424
train gradient:  0.5657967549672454
iteration : 201
train acc:  0.6328125
train loss:  0.5878080129623413
train gradient:  0.5272043939792161
iteration : 202
train acc:  0.71875
train loss:  0.6215758323669434
train gradient:  0.8576531035081086
iteration : 203
train acc:  0.6875
train loss:  0.6034224033355713
train gradient:  0.34338059997226594
iteration : 204
train acc:  0.6875
train loss:  0.5679709911346436
train gradient:  0.3743175422197028
iteration : 205
train acc:  0.6484375
train loss:  0.639503002166748
train gradient:  0.7638366299521061
iteration : 206
train acc:  0.671875
train loss:  0.6369943022727966
train gradient:  0.692417519142358
iteration : 207
train acc:  0.7109375
train loss:  0.5404919981956482
train gradient:  0.38708306514790897
iteration : 208
train acc:  0.6015625
train loss:  0.6345846652984619
train gradient:  0.33270872147144975
iteration : 209
train acc:  0.6640625
train loss:  0.5955601334571838
train gradient:  0.32863186458582017
iteration : 210
train acc:  0.6640625
train loss:  0.6026663780212402
train gradient:  0.7182919824125852
iteration : 211
train acc:  0.7578125
train loss:  0.5196240544319153
train gradient:  0.30691529683583074
iteration : 212
train acc:  0.6796875
train loss:  0.6108242869377136
train gradient:  0.8448057972246956
iteration : 213
train acc:  0.5859375
train loss:  0.6663599014282227
train gradient:  0.3858317922924951
iteration : 214
train acc:  0.734375
train loss:  0.5926944017410278
train gradient:  0.6754066272954193
iteration : 215
train acc:  0.703125
train loss:  0.5864460468292236
train gradient:  0.5010156853011816
iteration : 216
train acc:  0.6875
train loss:  0.5764645338058472
train gradient:  0.2661811548379281
iteration : 217
train acc:  0.6953125
train loss:  0.6120617389678955
train gradient:  0.6552388078782556
iteration : 218
train acc:  0.7265625
train loss:  0.5942386388778687
train gradient:  0.554642465838613
iteration : 219
train acc:  0.640625
train loss:  0.6313745975494385
train gradient:  0.5974486694449381
iteration : 220
train acc:  0.6796875
train loss:  0.6084070205688477
train gradient:  0.3995848002600683
iteration : 221
train acc:  0.6875
train loss:  0.5906586647033691
train gradient:  0.4668286827937936
iteration : 222
train acc:  0.671875
train loss:  0.5866758227348328
train gradient:  0.490373787772322
iteration : 223
train acc:  0.6875
train loss:  0.6189842820167542
train gradient:  0.5020791683198684
iteration : 224
train acc:  0.7109375
train loss:  0.5804438591003418
train gradient:  1.291367239398162
iteration : 225
train acc:  0.6171875
train loss:  0.616977334022522
train gradient:  0.5712743287562569
iteration : 226
train acc:  0.71875
train loss:  0.5813902616500854
train gradient:  0.44109762946721276
iteration : 227
train acc:  0.65625
train loss:  0.602846086025238
train gradient:  0.44637504023074476
iteration : 228
train acc:  0.65625
train loss:  0.563697874546051
train gradient:  0.25190936824445853
iteration : 229
train acc:  0.765625
train loss:  0.532479465007782
train gradient:  0.6579705144551444
iteration : 230
train acc:  0.609375
train loss:  0.6155969500541687
train gradient:  0.6400569136143959
iteration : 231
train acc:  0.71875
train loss:  0.5610388517379761
train gradient:  0.4300230043381266
iteration : 232
train acc:  0.6328125
train loss:  0.6451351642608643
train gradient:  0.7190578110519681
iteration : 233
train acc:  0.6875
train loss:  0.5893573760986328
train gradient:  0.6237566934730566
iteration : 234
train acc:  0.6875
train loss:  0.5726419687271118
train gradient:  0.31251590569587284
iteration : 235
train acc:  0.671875
train loss:  0.5775653123855591
train gradient:  0.6677733385021607
iteration : 236
train acc:  0.6875
train loss:  0.6025747060775757
train gradient:  0.5880909519157186
iteration : 237
train acc:  0.7421875
train loss:  0.5347040891647339
train gradient:  0.36223019002740137
iteration : 238
train acc:  0.65625
train loss:  0.6182495355606079
train gradient:  0.5698005737318272
iteration : 239
train acc:  0.671875
train loss:  0.6020644903182983
train gradient:  0.4102277407280391
iteration : 240
train acc:  0.7109375
train loss:  0.5997991561889648
train gradient:  0.6395084983564555
iteration : 241
train acc:  0.6328125
train loss:  0.6609794497489929
train gradient:  0.7775209102306996
iteration : 242
train acc:  0.765625
train loss:  0.5199936628341675
train gradient:  0.4959394298631276
iteration : 243
train acc:  0.734375
train loss:  0.5504612922668457
train gradient:  0.4351801242983863
iteration : 244
train acc:  0.65625
train loss:  0.6009790897369385
train gradient:  0.5564925128462277
iteration : 245
train acc:  0.71875
train loss:  0.5478683710098267
train gradient:  0.47793503422693007
iteration : 246
train acc:  0.6171875
train loss:  0.6358942985534668
train gradient:  1.1070207096515008
iteration : 247
train acc:  0.65625
train loss:  0.5930699110031128
train gradient:  0.6288417557894315
iteration : 248
train acc:  0.6953125
train loss:  0.6059044599533081
train gradient:  0.963403676975553
iteration : 249
train acc:  0.703125
train loss:  0.5390119552612305
train gradient:  0.5319945649889172
iteration : 250
train acc:  0.609375
train loss:  0.5829296112060547
train gradient:  0.4165685508837064
iteration : 251
train acc:  0.71875
train loss:  0.5622484087944031
train gradient:  0.3888165482764572
iteration : 252
train acc:  0.6796875
train loss:  0.5878311395645142
train gradient:  0.7445667071562578
iteration : 253
train acc:  0.7421875
train loss:  0.5667722225189209
train gradient:  0.48745801374391295
iteration : 254
train acc:  0.703125
train loss:  0.5908510088920593
train gradient:  0.4959786640195404
iteration : 255
train acc:  0.625
train loss:  0.6525533199310303
train gradient:  0.6218340036559413
iteration : 256
train acc:  0.6796875
train loss:  0.5910177230834961
train gradient:  0.5525894656971615
iteration : 257
train acc:  0.6953125
train loss:  0.5507210493087769
train gradient:  0.49323786357618676
iteration : 258
train acc:  0.7109375
train loss:  0.5517048835754395
train gradient:  0.4888605708097011
iteration : 259
train acc:  0.703125
train loss:  0.5407794713973999
train gradient:  0.4211909762349933
iteration : 260
train acc:  0.6328125
train loss:  0.6273764371871948
train gradient:  0.48576384253320126
iteration : 261
train acc:  0.6015625
train loss:  0.6449605226516724
train gradient:  0.6269430456952021
iteration : 262
train acc:  0.6171875
train loss:  0.6595162153244019
train gradient:  0.6384308928963371
iteration : 263
train acc:  0.7109375
train loss:  0.5462229251861572
train gradient:  0.46089724809739946
iteration : 264
train acc:  0.640625
train loss:  0.5827891826629639
train gradient:  0.4888676316365194
iteration : 265
train acc:  0.6796875
train loss:  0.6043034195899963
train gradient:  0.4857701831149936
iteration : 266
train acc:  0.6796875
train loss:  0.5996230840682983
train gradient:  0.33526865920198934
iteration : 267
train acc:  0.65625
train loss:  0.5715376138687134
train gradient:  0.49979513089788646
iteration : 268
train acc:  0.71875
train loss:  0.557501494884491
train gradient:  0.48424749082395074
iteration : 269
train acc:  0.671875
train loss:  0.5593553781509399
train gradient:  0.5199120522378868
iteration : 270
train acc:  0.703125
train loss:  0.5408391952514648
train gradient:  0.3886903015649504
iteration : 271
train acc:  0.6953125
train loss:  0.554896354675293
train gradient:  0.500246144568684
iteration : 272
train acc:  0.7265625
train loss:  0.5483641624450684
train gradient:  0.3670071455016801
iteration : 273
train acc:  0.71875
train loss:  0.5755002498626709
train gradient:  0.8567819801330687
iteration : 274
train acc:  0.6953125
train loss:  0.6058368682861328
train gradient:  0.6440585178728859
iteration : 275
train acc:  0.7734375
train loss:  0.5248241424560547
train gradient:  0.504350313316966
iteration : 276
train acc:  0.75
train loss:  0.5592302083969116
train gradient:  0.6399337334950659
iteration : 277
train acc:  0.6484375
train loss:  0.6761934757232666
train gradient:  0.7431709372646125
iteration : 278
train acc:  0.734375
train loss:  0.5726372003555298
train gradient:  0.5982446210406096
iteration : 279
train acc:  0.6484375
train loss:  0.5718127489089966
train gradient:  0.8365179583582333
iteration : 280
train acc:  0.65625
train loss:  0.5953615307807922
train gradient:  0.608202730603886
iteration : 281
train acc:  0.7578125
train loss:  0.5351691246032715
train gradient:  0.49384990343530194
iteration : 282
train acc:  0.6875
train loss:  0.5966207981109619
train gradient:  0.8180322579104022
iteration : 283
train acc:  0.7265625
train loss:  0.5528460741043091
train gradient:  0.5002328447770974
iteration : 284
train acc:  0.578125
train loss:  0.6738601922988892
train gradient:  0.7753950973644788
iteration : 285
train acc:  0.7265625
train loss:  0.5358646512031555
train gradient:  0.3958032442412869
iteration : 286
train acc:  0.671875
train loss:  0.6192349791526794
train gradient:  0.6891936491248503
iteration : 287
train acc:  0.65625
train loss:  0.5864995121955872
train gradient:  0.5191635500051759
iteration : 288
train acc:  0.75
train loss:  0.5822510719299316
train gradient:  0.5710121984483938
iteration : 289
train acc:  0.7109375
train loss:  0.5766651630401611
train gradient:  0.4099966225537675
iteration : 290
train acc:  0.671875
train loss:  0.6018065810203552
train gradient:  0.5430971644054983
iteration : 291
train acc:  0.75
train loss:  0.5103492736816406
train gradient:  0.44261998121197804
iteration : 292
train acc:  0.71875
train loss:  0.5182821154594421
train gradient:  0.4237183717889385
iteration : 293
train acc:  0.6484375
train loss:  0.5657484531402588
train gradient:  0.648258572670344
iteration : 294
train acc:  0.65625
train loss:  0.641822099685669
train gradient:  0.7345547805875228
iteration : 295
train acc:  0.7421875
train loss:  0.512823224067688
train gradient:  0.3544888016048358
iteration : 296
train acc:  0.71875
train loss:  0.6055161952972412
train gradient:  0.6651143064576914
iteration : 297
train acc:  0.6953125
train loss:  0.5911074876785278
train gradient:  0.47456500687812414
iteration : 298
train acc:  0.7265625
train loss:  0.5284574031829834
train gradient:  0.4222402478945601
iteration : 299
train acc:  0.671875
train loss:  0.5944921970367432
train gradient:  0.6543666652097428
iteration : 300
train acc:  0.65625
train loss:  0.5829366445541382
train gradient:  0.6999948020113994
iteration : 301
train acc:  0.6953125
train loss:  0.5328454971313477
train gradient:  0.5165082678277165
iteration : 302
train acc:  0.6796875
train loss:  0.616686224937439
train gradient:  0.6838132269630912
iteration : 303
train acc:  0.703125
train loss:  0.6019008755683899
train gradient:  0.48146371665484344
iteration : 304
train acc:  0.71875
train loss:  0.5600817799568176
train gradient:  0.43577130246156137
iteration : 305
train acc:  0.6796875
train loss:  0.5741453170776367
train gradient:  0.3643381138507631
iteration : 306
train acc:  0.6640625
train loss:  0.6022309064865112
train gradient:  0.8376839652466702
iteration : 307
train acc:  0.7734375
train loss:  0.5038245320320129
train gradient:  0.47011482786925807
iteration : 308
train acc:  0.6640625
train loss:  0.5952378511428833
train gradient:  0.515275893607149
iteration : 309
train acc:  0.71875
train loss:  0.5222651958465576
train gradient:  0.3393960164403373
iteration : 310
train acc:  0.796875
train loss:  0.4918093979358673
train gradient:  0.43061689643437795
iteration : 311
train acc:  0.7109375
train loss:  0.6044405102729797
train gradient:  0.3918038032183132
iteration : 312
train acc:  0.7421875
train loss:  0.5212910771369934
train gradient:  0.4242708294758551
iteration : 313
train acc:  0.796875
train loss:  0.49668365716934204
train gradient:  0.3325840774737594
iteration : 314
train acc:  0.6484375
train loss:  0.598724901676178
train gradient:  0.44764031421308587
iteration : 315
train acc:  0.6171875
train loss:  0.6290686726570129
train gradient:  0.605099034383054
iteration : 316
train acc:  0.6953125
train loss:  0.5606446266174316
train gradient:  0.4987363418906355
iteration : 317
train acc:  0.6875
train loss:  0.5894129872322083
train gradient:  0.4323519064862851
iteration : 318
train acc:  0.71875
train loss:  0.5582196712493896
train gradient:  0.30749718465225745
iteration : 319
train acc:  0.71875
train loss:  0.543768048286438
train gradient:  0.36999454424587075
iteration : 320
train acc:  0.7265625
train loss:  0.5154402852058411
train gradient:  0.3402783368806816
iteration : 321
train acc:  0.75
train loss:  0.5645128488540649
train gradient:  0.500739571919109
iteration : 322
train acc:  0.7265625
train loss:  0.5695987343788147
train gradient:  0.5489274310871644
iteration : 323
train acc:  0.703125
train loss:  0.563574492931366
train gradient:  0.447120954890303
iteration : 324
train acc:  0.7265625
train loss:  0.5394082069396973
train gradient:  0.5973464256939254
iteration : 325
train acc:  0.7421875
train loss:  0.5601868629455566
train gradient:  0.2692086786564916
iteration : 326
train acc:  0.6328125
train loss:  0.6250198483467102
train gradient:  0.6724572019772661
iteration : 327
train acc:  0.671875
train loss:  0.567193865776062
train gradient:  0.46213439591469163
iteration : 328
train acc:  0.6796875
train loss:  0.6096742153167725
train gradient:  0.7518542370571283
iteration : 329
train acc:  0.7578125
train loss:  0.5197268724441528
train gradient:  0.4252515734752703
iteration : 330
train acc:  0.6796875
train loss:  0.5715551376342773
train gradient:  0.5027443283317656
iteration : 331
train acc:  0.734375
train loss:  0.5260427594184875
train gradient:  0.3128544826865144
iteration : 332
train acc:  0.7109375
train loss:  0.5581281781196594
train gradient:  0.7206517993010851
iteration : 333
train acc:  0.7109375
train loss:  0.5680480003356934
train gradient:  0.7370893039124444
iteration : 334
train acc:  0.6875
train loss:  0.5683578848838806
train gradient:  0.49572585984303774
iteration : 335
train acc:  0.703125
train loss:  0.5479156970977783
train gradient:  0.3002845949419594
iteration : 336
train acc:  0.6953125
train loss:  0.5393984317779541
train gradient:  0.3572388610894229
iteration : 337
train acc:  0.609375
train loss:  0.6200617551803589
train gradient:  0.818266846095408
iteration : 338
train acc:  0.6953125
train loss:  0.5820855498313904
train gradient:  1.0405186459050058
iteration : 339
train acc:  0.7265625
train loss:  0.5785699486732483
train gradient:  0.2769641985195894
iteration : 340
train acc:  0.7109375
train loss:  0.5373836755752563
train gradient:  0.5754871526278613
iteration : 341
train acc:  0.6875
train loss:  0.5778912901878357
train gradient:  1.004085592672726
iteration : 342
train acc:  0.65625
train loss:  0.5748775601387024
train gradient:  0.4516449843282504
iteration : 343
train acc:  0.734375
train loss:  0.5496165752410889
train gradient:  0.40163840609225887
iteration : 344
train acc:  0.7265625
train loss:  0.5418753027915955
train gradient:  0.3845748439480979
iteration : 345
train acc:  0.65625
train loss:  0.6099882125854492
train gradient:  0.9513833651953445
iteration : 346
train acc:  0.65625
train loss:  0.6048222780227661
train gradient:  0.4979540646490625
iteration : 347
train acc:  0.703125
train loss:  0.5151625871658325
train gradient:  0.5255072673123651
iteration : 348
train acc:  0.65625
train loss:  0.555245578289032
train gradient:  0.4402147915665084
iteration : 349
train acc:  0.734375
train loss:  0.5348286628723145
train gradient:  0.5941458538572628
iteration : 350
train acc:  0.7734375
train loss:  0.52662193775177
train gradient:  0.6217392882513142
iteration : 351
train acc:  0.6875
train loss:  0.5528188943862915
train gradient:  0.3900668670790222
iteration : 352
train acc:  0.7109375
train loss:  0.5548496246337891
train gradient:  0.5318609029580172
iteration : 353
train acc:  0.6796875
train loss:  0.5717707872390747
train gradient:  0.3206362966837126
iteration : 354
train acc:  0.7265625
train loss:  0.5470789670944214
train gradient:  0.5258186870018978
iteration : 355
train acc:  0.671875
train loss:  0.5451191067695618
train gradient:  0.37554273763395374
iteration : 356
train acc:  0.7578125
train loss:  0.508042573928833
train gradient:  0.7103137979555952
iteration : 357
train acc:  0.734375
train loss:  0.5237891674041748
train gradient:  0.5264604201586076
iteration : 358
train acc:  0.703125
train loss:  0.544029951095581
train gradient:  0.6243746105069273
iteration : 359
train acc:  0.7265625
train loss:  0.5317050814628601
train gradient:  0.6496033971269289
iteration : 360
train acc:  0.71875
train loss:  0.5416417121887207
train gradient:  0.4557236733837539
iteration : 361
train acc:  0.765625
train loss:  0.5169810056686401
train gradient:  0.4332667045726005
iteration : 362
train acc:  0.6875
train loss:  0.5729517936706543
train gradient:  0.6912881534277825
iteration : 363
train acc:  0.71875
train loss:  0.5697386264801025
train gradient:  0.6021245213851738
iteration : 364
train acc:  0.65625
train loss:  0.5790834426879883
train gradient:  0.7686143397298314
iteration : 365
train acc:  0.7109375
train loss:  0.5999250411987305
train gradient:  0.9509472097238352
iteration : 366
train acc:  0.671875
train loss:  0.6181313991546631
train gradient:  0.6096236980236207
iteration : 367
train acc:  0.71875
train loss:  0.5481921434402466
train gradient:  1.0477582264963807
iteration : 368
train acc:  0.7109375
train loss:  0.5548874139785767
train gradient:  0.6213964577033182
iteration : 369
train acc:  0.640625
train loss:  0.6214107275009155
train gradient:  0.7649316896361388
iteration : 370
train acc:  0.7421875
train loss:  0.49503982067108154
train gradient:  0.5482704321372434
iteration : 371
train acc:  0.75
train loss:  0.5643677711486816
train gradient:  0.5734368433471508
iteration : 372
train acc:  0.6796875
train loss:  0.6192378401756287
train gradient:  0.7334783842101644
iteration : 373
train acc:  0.6875
train loss:  0.5383575558662415
train gradient:  0.768532916532466
iteration : 374
train acc:  0.6796875
train loss:  0.5815000534057617
train gradient:  0.5887592664544645
iteration : 375
train acc:  0.6484375
train loss:  0.5893497467041016
train gradient:  0.7265207846332628
iteration : 376
train acc:  0.703125
train loss:  0.5688214898109436
train gradient:  0.5473366026929762
iteration : 377
train acc:  0.6953125
train loss:  0.5581743717193604
train gradient:  0.7304156328059076
iteration : 378
train acc:  0.6953125
train loss:  0.5681940913200378
train gradient:  0.43645633227801706
iteration : 379
train acc:  0.65625
train loss:  0.5927667617797852
train gradient:  0.8388816315261296
iteration : 380
train acc:  0.6953125
train loss:  0.5206046104431152
train gradient:  0.3112512340393131
iteration : 381
train acc:  0.75
train loss:  0.5369997024536133
train gradient:  0.568342665278103
iteration : 382
train acc:  0.75
train loss:  0.5455235242843628
train gradient:  0.5460094682731621
iteration : 383
train acc:  0.65625
train loss:  0.5837175250053406
train gradient:  0.5797670354331357
iteration : 384
train acc:  0.7265625
train loss:  0.5625935792922974
train gradient:  0.2859001330703943
iteration : 385
train acc:  0.7421875
train loss:  0.5125188827514648
train gradient:  0.36741934263908155
iteration : 386
train acc:  0.7265625
train loss:  0.5249704122543335
train gradient:  0.5140542414395426
iteration : 387
train acc:  0.6796875
train loss:  0.5856927633285522
train gradient:  0.6446809636867156
iteration : 388
train acc:  0.703125
train loss:  0.5437817573547363
train gradient:  0.6645503556816035
iteration : 389
train acc:  0.703125
train loss:  0.5509989261627197
train gradient:  0.3730737505389651
iteration : 390
train acc:  0.6796875
train loss:  0.6205328702926636
train gradient:  0.6187110320668364
iteration : 391
train acc:  0.703125
train loss:  0.5210610628128052
train gradient:  0.39563787457724214
iteration : 392
train acc:  0.71875
train loss:  0.5442404747009277
train gradient:  0.8582008040327187
iteration : 393
train acc:  0.7734375
train loss:  0.49441108107566833
train gradient:  0.36705364825879466
iteration : 394
train acc:  0.6953125
train loss:  0.5784156322479248
train gradient:  0.6244229310005307
iteration : 395
train acc:  0.7109375
train loss:  0.5635231733322144
train gradient:  0.5748087722841543
iteration : 396
train acc:  0.6796875
train loss:  0.6060323715209961
train gradient:  0.5812201026534912
iteration : 397
train acc:  0.7578125
train loss:  0.5856029391288757
train gradient:  0.5589989811779268
iteration : 398
train acc:  0.6640625
train loss:  0.6264373064041138
train gradient:  0.7776484483529915
iteration : 399
train acc:  0.703125
train loss:  0.5519065856933594
train gradient:  0.5806056850274443
iteration : 400
train acc:  0.71875
train loss:  0.5033858418464661
train gradient:  0.6092973087113495
iteration : 401
train acc:  0.640625
train loss:  0.6470475792884827
train gradient:  0.6595390210844166
iteration : 402
train acc:  0.703125
train loss:  0.5537666082382202
train gradient:  0.44255200256640703
iteration : 403
train acc:  0.765625
train loss:  0.5215019583702087
train gradient:  0.5658658349758985
iteration : 404
train acc:  0.6875
train loss:  0.601039707660675
train gradient:  0.5048265700967487
iteration : 405
train acc:  0.6953125
train loss:  0.5916101932525635
train gradient:  0.4873055212540817
iteration : 406
train acc:  0.6640625
train loss:  0.5456727147102356
train gradient:  0.47589265297922384
iteration : 407
train acc:  0.6953125
train loss:  0.5552089214324951
train gradient:  0.38900231871715146
iteration : 408
train acc:  0.7890625
train loss:  0.5086817145347595
train gradient:  0.38107860055489323
iteration : 409
train acc:  0.7265625
train loss:  0.5360458493232727
train gradient:  0.4083880903795239
iteration : 410
train acc:  0.703125
train loss:  0.5415494441986084
train gradient:  0.4049250691853156
iteration : 411
train acc:  0.734375
train loss:  0.5347474813461304
train gradient:  0.3906610775377073
iteration : 412
train acc:  0.6640625
train loss:  0.5734774470329285
train gradient:  0.3951986445758411
iteration : 413
train acc:  0.703125
train loss:  0.5517433285713196
train gradient:  0.38178831364882454
iteration : 414
train acc:  0.6328125
train loss:  0.6355429887771606
train gradient:  0.526923085624341
iteration : 415
train acc:  0.7421875
train loss:  0.5067816972732544
train gradient:  0.3629025158736261
iteration : 416
train acc:  0.703125
train loss:  0.5398633480072021
train gradient:  0.30978063930305916
iteration : 417
train acc:  0.6953125
train loss:  0.5931105613708496
train gradient:  0.5217856883043948
iteration : 418
train acc:  0.75
train loss:  0.49392008781433105
train gradient:  0.2792045309533469
iteration : 419
train acc:  0.71875
train loss:  0.5539872646331787
train gradient:  0.33920472793088374
iteration : 420
train acc:  0.6953125
train loss:  0.6112861037254333
train gradient:  0.7620227845961511
iteration : 421
train acc:  0.71875
train loss:  0.5549837946891785
train gradient:  0.37224236322803766
iteration : 422
train acc:  0.6484375
train loss:  0.5973218679428101
train gradient:  0.7361770110905083
iteration : 423
train acc:  0.6796875
train loss:  0.6357507109642029
train gradient:  0.5978433020380417
iteration : 424
train acc:  0.75
train loss:  0.4810018539428711
train gradient:  0.3825741283139629
iteration : 425
train acc:  0.703125
train loss:  0.5413122177124023
train gradient:  0.3898113474363817
iteration : 426
train acc:  0.75
train loss:  0.5531320571899414
train gradient:  0.43775123770255475
iteration : 427
train acc:  0.6953125
train loss:  0.5260343551635742
train gradient:  0.4758285173947797
iteration : 428
train acc:  0.671875
train loss:  0.6508308053016663
train gradient:  0.7090781132458073
iteration : 429
train acc:  0.7890625
train loss:  0.5126280188560486
train gradient:  0.5433101515295462
iteration : 430
train acc:  0.7578125
train loss:  0.5449392795562744
train gradient:  0.5987314247771187
iteration : 431
train acc:  0.7421875
train loss:  0.5581820607185364
train gradient:  0.6295350554623486
iteration : 432
train acc:  0.7109375
train loss:  0.5454769134521484
train gradient:  0.5591485021044976
iteration : 433
train acc:  0.6796875
train loss:  0.5334869623184204
train gradient:  0.4519787845459363
iteration : 434
train acc:  0.734375
train loss:  0.5121608972549438
train gradient:  0.4705696541850709
iteration : 435
train acc:  0.75
train loss:  0.5146180987358093
train gradient:  0.6892517680520792
iteration : 436
train acc:  0.7734375
train loss:  0.4832751452922821
train gradient:  0.46401470993383126
iteration : 437
train acc:  0.7578125
train loss:  0.48535534739494324
train gradient:  0.38436921171855376
iteration : 438
train acc:  0.6875
train loss:  0.5928460359573364
train gradient:  0.9526610688114251
iteration : 439
train acc:  0.703125
train loss:  0.5880706310272217
train gradient:  0.6936566027877356
iteration : 440
train acc:  0.7109375
train loss:  0.5304874181747437
train gradient:  0.42388650991016735
iteration : 441
train acc:  0.734375
train loss:  0.5522944927215576
train gradient:  0.5940966266860991
iteration : 442
train acc:  0.6953125
train loss:  0.531269907951355
train gradient:  0.5878466861773846
iteration : 443
train acc:  0.6875
train loss:  0.5782897472381592
train gradient:  0.7656751253941679
iteration : 444
train acc:  0.734375
train loss:  0.5489498972892761
train gradient:  0.5097864400984824
iteration : 445
train acc:  0.6796875
train loss:  0.5816221833229065
train gradient:  0.6213113829656132
iteration : 446
train acc:  0.6484375
train loss:  0.6454200148582458
train gradient:  0.724725461281681
iteration : 447
train acc:  0.65625
train loss:  0.5559706091880798
train gradient:  0.5860084451870313
iteration : 448
train acc:  0.703125
train loss:  0.5529323220252991
train gradient:  0.4404756843913934
iteration : 449
train acc:  0.8125
train loss:  0.5020390748977661
train gradient:  0.48162231299423125
iteration : 450
train acc:  0.65625
train loss:  0.5970721244812012
train gradient:  0.5818213209551903
iteration : 451
train acc:  0.75
train loss:  0.5530514121055603
train gradient:  0.6835401727775521
iteration : 452
train acc:  0.7109375
train loss:  0.5456461906433105
train gradient:  0.6794065900471289
iteration : 453
train acc:  0.71875
train loss:  0.5633352994918823
train gradient:  0.5555315168914889
iteration : 454
train acc:  0.6796875
train loss:  0.5539863705635071
train gradient:  0.4521336757426497
iteration : 455
train acc:  0.7265625
train loss:  0.5492765307426453
train gradient:  0.5865037597797229
iteration : 456
train acc:  0.6953125
train loss:  0.5716032981872559
train gradient:  0.6006687173028292
iteration : 457
train acc:  0.8046875
train loss:  0.49688720703125
train gradient:  0.4151713574595656
iteration : 458
train acc:  0.7578125
train loss:  0.5589596033096313
train gradient:  0.5278175170662889
iteration : 459
train acc:  0.71875
train loss:  0.5367860794067383
train gradient:  0.47445180376518653
iteration : 460
train acc:  0.6796875
train loss:  0.5677525997161865
train gradient:  0.6606831420821166
iteration : 461
train acc:  0.703125
train loss:  0.5416374206542969
train gradient:  0.5150340606228159
iteration : 462
train acc:  0.71875
train loss:  0.5345010161399841
train gradient:  0.5521160912843941
iteration : 463
train acc:  0.6328125
train loss:  0.5993201732635498
train gradient:  0.7011490938867098
iteration : 464
train acc:  0.703125
train loss:  0.5931035876274109
train gradient:  0.6433653407643893
iteration : 465
train acc:  0.796875
train loss:  0.5368517637252808
train gradient:  0.7289719756933823
iteration : 466
train acc:  0.6796875
train loss:  0.5962265729904175
train gradient:  0.7311340043278878
iteration : 467
train acc:  0.78125
train loss:  0.4789372384548187
train gradient:  0.4850444065869537
iteration : 468
train acc:  0.6953125
train loss:  0.5602782964706421
train gradient:  0.43727908478660743
iteration : 469
train acc:  0.6875
train loss:  0.5804015398025513
train gradient:  0.780871381550013
iteration : 470
train acc:  0.7578125
train loss:  0.5369658470153809
train gradient:  0.6038351597654528
iteration : 471
train acc:  0.734375
train loss:  0.5116069316864014
train gradient:  0.5168578563318476
iteration : 472
train acc:  0.6640625
train loss:  0.6002609729766846
train gradient:  0.8828172645448658
iteration : 473
train acc:  0.828125
train loss:  0.45595288276672363
train gradient:  0.40543548123076795
iteration : 474
train acc:  0.75
train loss:  0.5201263427734375
train gradient:  0.49119345758272376
iteration : 475
train acc:  0.75
train loss:  0.506112813949585
train gradient:  0.4591208162938986
iteration : 476
train acc:  0.7109375
train loss:  0.6158338785171509
train gradient:  0.6525397609385736
iteration : 477
train acc:  0.75
train loss:  0.5080662369728088
train gradient:  0.408412117159018
iteration : 478
train acc:  0.7109375
train loss:  0.5488962531089783
train gradient:  0.4855871601350349
iteration : 479
train acc:  0.703125
train loss:  0.5399250388145447
train gradient:  0.4246574158786199
iteration : 480
train acc:  0.71875
train loss:  0.5338548421859741
train gradient:  0.5509503955452386
iteration : 481
train acc:  0.640625
train loss:  0.6245104670524597
train gradient:  0.562535884026877
iteration : 482
train acc:  0.71875
train loss:  0.5384886264801025
train gradient:  0.5033382934297457
iteration : 483
train acc:  0.765625
train loss:  0.4937259554862976
train gradient:  0.44970779612175227
iteration : 484
train acc:  0.7109375
train loss:  0.5415846109390259
train gradient:  0.515575081119453
iteration : 485
train acc:  0.7578125
train loss:  0.4982447922229767
train gradient:  0.4304047644555332
iteration : 486
train acc:  0.671875
train loss:  0.5567607879638672
train gradient:  0.5522027817183413
iteration : 487
train acc:  0.796875
train loss:  0.4956166744232178
train gradient:  0.5691884375596359
iteration : 488
train acc:  0.796875
train loss:  0.48540952801704407
train gradient:  0.6572921366838378
iteration : 489
train acc:  0.765625
train loss:  0.5174547433853149
train gradient:  0.5556926305120569
iteration : 490
train acc:  0.765625
train loss:  0.5038354396820068
train gradient:  0.3719963389405013
iteration : 491
train acc:  0.765625
train loss:  0.4974808692932129
train gradient:  0.37827439451233696
iteration : 492
train acc:  0.734375
train loss:  0.5176777839660645
train gradient:  0.47169375083809484
iteration : 493
train acc:  0.7421875
train loss:  0.5351789593696594
train gradient:  0.610010822355477
iteration : 494
train acc:  0.6953125
train loss:  0.5437158942222595
train gradient:  0.5047866055389184
iteration : 495
train acc:  0.796875
train loss:  0.4921894967556
train gradient:  0.43541031511852313
iteration : 496
train acc:  0.75
train loss:  0.5108643770217896
train gradient:  0.5708576534090535
iteration : 497
train acc:  0.7734375
train loss:  0.45676156878471375
train gradient:  0.2955951742064455
iteration : 498
train acc:  0.7421875
train loss:  0.5156146287918091
train gradient:  0.4574697113778939
iteration : 499
train acc:  0.671875
train loss:  0.6171591281890869
train gradient:  0.6659434661987249
iteration : 500
train acc:  0.7734375
train loss:  0.45526519417762756
train gradient:  0.33030611466727267
iteration : 501
train acc:  0.671875
train loss:  0.5914386510848999
train gradient:  0.7922455299505994
iteration : 502
train acc:  0.734375
train loss:  0.5708286166191101
train gradient:  0.7512380971129324
iteration : 503
train acc:  0.7734375
train loss:  0.47970303893089294
train gradient:  0.44893986430318056
iteration : 504
train acc:  0.6796875
train loss:  0.576778769493103
train gradient:  0.7350684666973569
iteration : 505
train acc:  0.671875
train loss:  0.5608638525009155
train gradient:  1.019268095670288
iteration : 506
train acc:  0.7109375
train loss:  0.5919922590255737
train gradient:  0.6409611617492754
iteration : 507
train acc:  0.78125
train loss:  0.5306382179260254
train gradient:  0.4091274046925364
iteration : 508
train acc:  0.765625
train loss:  0.4593174457550049
train gradient:  0.42778664394957566
iteration : 509
train acc:  0.7265625
train loss:  0.5112714767456055
train gradient:  0.5798710648885081
iteration : 510
train acc:  0.7578125
train loss:  0.47782573103904724
train gradient:  0.4044389066809864
iteration : 511
train acc:  0.859375
train loss:  0.40880319476127625
train gradient:  0.3786473504934009
iteration : 512
train acc:  0.8046875
train loss:  0.49909061193466187
train gradient:  0.562525249740339
iteration : 513
train acc:  0.7890625
train loss:  0.4843384027481079
train gradient:  0.47163913814242653
iteration : 514
train acc:  0.75
train loss:  0.4592047333717346
train gradient:  0.4681636506062178
iteration : 515
train acc:  0.71875
train loss:  0.5190595388412476
train gradient:  0.5280957852405548
iteration : 516
train acc:  0.703125
train loss:  0.5466197729110718
train gradient:  0.6640407920560012
iteration : 517
train acc:  0.8046875
train loss:  0.4840777516365051
train gradient:  0.6988562884791536
iteration : 518
train acc:  0.6953125
train loss:  0.5584403872489929
train gradient:  0.5639385607375085
iteration : 519
train acc:  0.7109375
train loss:  0.5722719430923462
train gradient:  0.6200434802576937
iteration : 520
train acc:  0.7734375
train loss:  0.4601118564605713
train gradient:  0.4531197958643486
iteration : 521
train acc:  0.78125
train loss:  0.47254884243011475
train gradient:  0.5217270771452933
iteration : 522
train acc:  0.7734375
train loss:  0.5087088346481323
train gradient:  0.5289463121263778
iteration : 523
train acc:  0.7265625
train loss:  0.48988693952560425
train gradient:  0.46656749707244183
iteration : 524
train acc:  0.7265625
train loss:  0.5118756294250488
train gradient:  0.5433820575687752
iteration : 525
train acc:  0.7109375
train loss:  0.5830651521682739
train gradient:  0.5793918736760825
iteration : 526
train acc:  0.7734375
train loss:  0.48249948024749756
train gradient:  0.48511565622407277
iteration : 527
train acc:  0.765625
train loss:  0.48234495520591736
train gradient:  0.6852269387714808
iteration : 528
train acc:  0.7734375
train loss:  0.4663745164871216
train gradient:  0.4533401456801739
iteration : 529
train acc:  0.7265625
train loss:  0.5487081408500671
train gradient:  0.5359869492951589
iteration : 530
train acc:  0.7578125
train loss:  0.4847024083137512
train gradient:  0.4486726828467936
iteration : 531
train acc:  0.6875
train loss:  0.5670387148857117
train gradient:  1.3576593563179002
iteration : 532
train acc:  0.765625
train loss:  0.5389293432235718
train gradient:  0.5747707668403446
iteration : 533
train acc:  0.7578125
train loss:  0.525545597076416
train gradient:  0.669364770959977
iteration : 534
train acc:  0.75
train loss:  0.4725492596626282
train gradient:  0.7233388994476901
iteration : 535
train acc:  0.7890625
train loss:  0.47326985001564026
train gradient:  0.48040857256479735
iteration : 536
train acc:  0.8046875
train loss:  0.4444830119609833
train gradient:  0.5693552175063435
iteration : 537
train acc:  0.734375
train loss:  0.5007526278495789
train gradient:  0.4265938847270947
iteration : 538
train acc:  0.6796875
train loss:  0.5761555433273315
train gradient:  0.7013038964779711
iteration : 539
train acc:  0.7265625
train loss:  0.4934053421020508
train gradient:  0.49966366059782824
iteration : 540
train acc:  0.7578125
train loss:  0.4766960144042969
train gradient:  0.5208338618157908
iteration : 541
train acc:  0.7578125
train loss:  0.5059466361999512
train gradient:  0.5644184193694055
iteration : 542
train acc:  0.7578125
train loss:  0.45852917432785034
train gradient:  0.5118083124911499
iteration : 543
train acc:  0.7890625
train loss:  0.448378324508667
train gradient:  0.4348902551872038
iteration : 544
train acc:  0.7734375
train loss:  0.4640585482120514
train gradient:  0.4741887441261201
iteration : 545
train acc:  0.7421875
train loss:  0.5342000722885132
train gradient:  0.679603441493329
iteration : 546
train acc:  0.8046875
train loss:  0.44674474000930786
train gradient:  0.46642811531142264
iteration : 547
train acc:  0.734375
train loss:  0.5007033944129944
train gradient:  0.4784038629593794
iteration : 548
train acc:  0.75
train loss:  0.48826485872268677
train gradient:  0.49921346642324854
iteration : 549
train acc:  0.765625
train loss:  0.4686763286590576
train gradient:  0.6499131080598037
iteration : 550
train acc:  0.75
train loss:  0.5046035051345825
train gradient:  0.5035331475673219
iteration : 551
train acc:  0.7890625
train loss:  0.44351208209991455
train gradient:  0.4002081210702509
iteration : 552
train acc:  0.765625
train loss:  0.4549132287502289
train gradient:  0.6057382003381254
iteration : 553
train acc:  0.734375
train loss:  0.49426913261413574
train gradient:  0.5437715171998755
iteration : 554
train acc:  0.703125
train loss:  0.506980836391449
train gradient:  0.6171878572441682
iteration : 555
train acc:  0.78125
train loss:  0.472541868686676
train gradient:  0.34224928884472267
iteration : 556
train acc:  0.7265625
train loss:  0.5308548212051392
train gradient:  0.49964994050643247
iteration : 557
train acc:  0.796875
train loss:  0.4398164749145508
train gradient:  0.5229464158026359
iteration : 558
train acc:  0.75
train loss:  0.5108209848403931
train gradient:  0.6296561998602473
iteration : 559
train acc:  0.7109375
train loss:  0.5925939083099365
train gradient:  0.8897479300607893
iteration : 560
train acc:  0.7421875
train loss:  0.4866909980773926
train gradient:  0.6839922427952648
iteration : 561
train acc:  0.7578125
train loss:  0.45937204360961914
train gradient:  0.5521983175705987
iteration : 562
train acc:  0.7421875
train loss:  0.5363888740539551
train gradient:  0.585828739967651
iteration : 563
train acc:  0.7265625
train loss:  0.48214012384414673
train gradient:  0.6799637843943187
iteration : 564
train acc:  0.75
train loss:  0.49222874641418457
train gradient:  0.5754432553136171
iteration : 565
train acc:  0.734375
train loss:  0.5075232982635498
train gradient:  0.6898985654385873
iteration : 566
train acc:  0.7578125
train loss:  0.5257390737533569
train gradient:  0.7889018690183125
iteration : 567
train acc:  0.71875
train loss:  0.6255499720573425
train gradient:  1.0102100909032936
iteration : 568
train acc:  0.71875
train loss:  0.5203195810317993
train gradient:  0.7862019952802469
iteration : 569
train acc:  0.75
train loss:  0.5198851823806763
train gradient:  0.974614600016725
iteration : 570
train acc:  0.765625
train loss:  0.5203849077224731
train gradient:  0.7823299364130198
iteration : 571
train acc:  0.7890625
train loss:  0.4700775742530823
train gradient:  0.6383449628144428
iteration : 572
train acc:  0.7578125
train loss:  0.5397011041641235
train gradient:  0.8921914152406044
iteration : 573
train acc:  0.703125
train loss:  0.5913772583007812
train gradient:  0.6357113153009786
iteration : 574
train acc:  0.671875
train loss:  0.581396758556366
train gradient:  1.059116348630265
iteration : 575
train acc:  0.75
train loss:  0.505078911781311
train gradient:  0.5470306046813955
iteration : 576
train acc:  0.78125
train loss:  0.4751700162887573
train gradient:  0.8724154288042131
iteration : 577
train acc:  0.7421875
train loss:  0.5147435069084167
train gradient:  0.48780459849301183
iteration : 578
train acc:  0.6796875
train loss:  0.5882238149642944
train gradient:  0.8248716982221291
iteration : 579
train acc:  0.7109375
train loss:  0.5125260353088379
train gradient:  0.6414006283969192
iteration : 580
train acc:  0.765625
train loss:  0.43836942315101624
train gradient:  0.5984801767445608
iteration : 581
train acc:  0.7734375
train loss:  0.45883405208587646
train gradient:  0.45337242145387946
iteration : 582
train acc:  0.734375
train loss:  0.48847219347953796
train gradient:  0.47163448383053347
iteration : 583
train acc:  0.8125
train loss:  0.4713796079158783
train gradient:  0.49112012131672905
iteration : 584
train acc:  0.7421875
train loss:  0.5494759678840637
train gradient:  0.5245874118510465
iteration : 585
train acc:  0.7265625
train loss:  0.5271835327148438
train gradient:  0.4676650237868289
iteration : 586
train acc:  0.7109375
train loss:  0.5136815905570984
train gradient:  0.5412804114923384
iteration : 587
train acc:  0.7578125
train loss:  0.47371208667755127
train gradient:  0.4619197106807344
iteration : 588
train acc:  0.703125
train loss:  0.5830798149108887
train gradient:  0.7728367112805681
iteration : 589
train acc:  0.78125
train loss:  0.45261478424072266
train gradient:  0.4240605252822688
iteration : 590
train acc:  0.765625
train loss:  0.467645525932312
train gradient:  0.39667810896833017
iteration : 591
train acc:  0.703125
train loss:  0.501428484916687
train gradient:  0.6165893861321633
iteration : 592
train acc:  0.734375
train loss:  0.5221836566925049
train gradient:  0.5793178976708334
iteration : 593
train acc:  0.7890625
train loss:  0.5100874900817871
train gradient:  0.5481862416966689
iteration : 594
train acc:  0.734375
train loss:  0.5108436346054077
train gradient:  0.6400470499854025
iteration : 595
train acc:  0.71875
train loss:  0.5444775819778442
train gradient:  0.755228663615197
iteration : 596
train acc:  0.75
train loss:  0.5198237895965576
train gradient:  0.7018278518595702
iteration : 597
train acc:  0.7421875
train loss:  0.5047045350074768
train gradient:  0.47819488592088244
iteration : 598
train acc:  0.7421875
train loss:  0.5042852759361267
train gradient:  0.5926210561549745
iteration : 599
train acc:  0.7421875
train loss:  0.5147228240966797
train gradient:  0.49527655146367433
iteration : 600
train acc:  0.7109375
train loss:  0.532710075378418
train gradient:  0.7110485568957412
iteration : 601
train acc:  0.703125
train loss:  0.5614492893218994
train gradient:  0.7554104506402821
iteration : 602
train acc:  0.78125
train loss:  0.46022140979766846
train gradient:  0.4098277660270203
iteration : 603
train acc:  0.7109375
train loss:  0.5908153057098389
train gradient:  0.6634738713521436
iteration : 604
train acc:  0.6953125
train loss:  0.5481730699539185
train gradient:  0.5425536279766682
iteration : 605
train acc:  0.671875
train loss:  0.6382824182510376
train gradient:  2.1867474068355963
iteration : 606
train acc:  0.75
train loss:  0.5308890342712402
train gradient:  0.8731665521200539
iteration : 607
train acc:  0.6796875
train loss:  0.5673698782920837
train gradient:  0.5360565691987809
iteration : 608
train acc:  0.7265625
train loss:  0.541587233543396
train gradient:  0.7728020560138816
iteration : 609
train acc:  0.7109375
train loss:  0.5854266881942749
train gradient:  0.7059335667122848
iteration : 610
train acc:  0.765625
train loss:  0.5984942317008972
train gradient:  0.91609581193469
iteration : 611
train acc:  0.734375
train loss:  0.555094838142395
train gradient:  0.5492137233646339
iteration : 612
train acc:  0.703125
train loss:  0.5635079741477966
train gradient:  0.5736908472245204
iteration : 613
train acc:  0.71875
train loss:  0.5437802076339722
train gradient:  0.6046789433573552
iteration : 614
train acc:  0.78125
train loss:  0.4900630712509155
train gradient:  0.5497511971794429
iteration : 615
train acc:  0.71875
train loss:  0.571330189704895
train gradient:  0.5287774042628391
iteration : 616
train acc:  0.75
train loss:  0.5061279535293579
train gradient:  0.5649319146041574
iteration : 617
train acc:  0.71875
train loss:  0.5554840564727783
train gradient:  0.712171502203399
iteration : 618
train acc:  0.734375
train loss:  0.45789164304733276
train gradient:  0.4654072834443695
iteration : 619
train acc:  0.734375
train loss:  0.49934834241867065
train gradient:  0.42808987430043666
iteration : 620
train acc:  0.8203125
train loss:  0.42674684524536133
train gradient:  0.37963874240089734
iteration : 621
train acc:  0.78125
train loss:  0.5031443238258362
train gradient:  0.4177709217281398
iteration : 622
train acc:  0.8046875
train loss:  0.5070335865020752
train gradient:  0.6259713811887813
iteration : 623
train acc:  0.7578125
train loss:  0.46651411056518555
train gradient:  0.42883459472914875
iteration : 624
train acc:  0.78125
train loss:  0.4530773162841797
train gradient:  0.3125224225085736
iteration : 625
train acc:  0.7265625
train loss:  0.5168566703796387
train gradient:  0.42167556630283826
iteration : 626
train acc:  0.75
train loss:  0.5230216979980469
train gradient:  0.44118369918119876
iteration : 627
train acc:  0.703125
train loss:  0.5505166053771973
train gradient:  0.4290060048914083
iteration : 628
train acc:  0.78125
train loss:  0.4608202874660492
train gradient:  0.3954745140019403
iteration : 629
train acc:  0.75
train loss:  0.4849117398262024
train gradient:  0.4536748944959729
iteration : 630
train acc:  0.8125
train loss:  0.46089133620262146
train gradient:  0.381773563995217
iteration : 631
train acc:  0.734375
train loss:  0.5436254143714905
train gradient:  0.5315216798226
iteration : 632
train acc:  0.71875
train loss:  0.573439359664917
train gradient:  0.6504129263238935
iteration : 633
train acc:  0.7890625
train loss:  0.49089255928993225
train gradient:  0.40929756202223877
iteration : 634
train acc:  0.796875
train loss:  0.48959586024284363
train gradient:  0.45688236458795145
iteration : 635
train acc:  0.78125
train loss:  0.4858788847923279
train gradient:  0.5279579200285682
iteration : 636
train acc:  0.7890625
train loss:  0.4257202744483948
train gradient:  0.4310352534883734
iteration : 637
train acc:  0.7890625
train loss:  0.42029592394828796
train gradient:  0.26680162354413084
iteration : 638
train acc:  0.7265625
train loss:  0.48190680146217346
train gradient:  0.5653253771190642
iteration : 639
train acc:  0.7265625
train loss:  0.5080862045288086
train gradient:  0.5407882752911775
iteration : 640
train acc:  0.8203125
train loss:  0.4167356491088867
train gradient:  0.4143282413706002
iteration : 641
train acc:  0.6796875
train loss:  0.6009509563446045
train gradient:  0.7927253719083903
iteration : 642
train acc:  0.7890625
train loss:  0.4657582640647888
train gradient:  0.5038984291475379
iteration : 643
train acc:  0.734375
train loss:  0.46427977085113525
train gradient:  0.5547381987702663
iteration : 644
train acc:  0.734375
train loss:  0.5069288611412048
train gradient:  0.5488102712287638
iteration : 645
train acc:  0.7734375
train loss:  0.5086351633071899
train gradient:  0.6213210220440006
iteration : 646
train acc:  0.6875
train loss:  0.5457682609558105
train gradient:  0.7305653351472943
iteration : 647
train acc:  0.78125
train loss:  0.505734384059906
train gradient:  0.5870329317815368
iteration : 648
train acc:  0.8203125
train loss:  0.45010051131248474
train gradient:  0.49269960813432323
iteration : 649
train acc:  0.7578125
train loss:  0.4752553105354309
train gradient:  0.4766494396239904
iteration : 650
train acc:  0.765625
train loss:  0.44491589069366455
train gradient:  0.45149469880921833
iteration : 651
train acc:  0.6796875
train loss:  0.5478237867355347
train gradient:  0.6838222410126655
iteration : 652
train acc:  0.7734375
train loss:  0.45637065172195435
train gradient:  0.46728012068805663
iteration : 653
train acc:  0.7578125
train loss:  0.4857422113418579
train gradient:  0.5184643491501869
iteration : 654
train acc:  0.8125
train loss:  0.45401647686958313
train gradient:  0.4273791522709451
iteration : 655
train acc:  0.78125
train loss:  0.4855049252510071
train gradient:  0.5135294997635153
iteration : 656
train acc:  0.7421875
train loss:  0.5409471988677979
train gradient:  0.6876098146202099
iteration : 657
train acc:  0.7109375
train loss:  0.48799917101860046
train gradient:  0.48075807998713077
iteration : 658
train acc:  0.7734375
train loss:  0.49560546875
train gradient:  0.537137466335532
iteration : 659
train acc:  0.796875
train loss:  0.4568523168563843
train gradient:  0.45245066823729313
iteration : 660
train acc:  0.71875
train loss:  0.5895870923995972
train gradient:  0.7166736507968146
iteration : 661
train acc:  0.828125
train loss:  0.43835723400115967
train gradient:  0.531459017624418
iteration : 662
train acc:  0.765625
train loss:  0.4248203635215759
train gradient:  0.45757016701696246
iteration : 663
train acc:  0.796875
train loss:  0.5077865123748779
train gradient:  0.6393680184512001
iteration : 664
train acc:  0.7109375
train loss:  0.5207815170288086
train gradient:  0.6449310028349549
iteration : 665
train acc:  0.765625
train loss:  0.4535207152366638
train gradient:  0.5280172016157374
iteration : 666
train acc:  0.78125
train loss:  0.47882118821144104
train gradient:  0.43044537524033555
iteration : 667
train acc:  0.78125
train loss:  0.446266233921051
train gradient:  0.4806956007540628
iteration : 668
train acc:  0.84375
train loss:  0.4143184721469879
train gradient:  0.3641374532792613
iteration : 669
train acc:  0.78125
train loss:  0.4821891486644745
train gradient:  0.5619181954555348
iteration : 670
train acc:  0.7734375
train loss:  0.49556052684783936
train gradient:  0.5456838671377641
iteration : 671
train acc:  0.7265625
train loss:  0.5071346163749695
train gradient:  0.5347070376588879
iteration : 672
train acc:  0.6953125
train loss:  0.5594279766082764
train gradient:  0.7482224063845365
iteration : 673
train acc:  0.7421875
train loss:  0.4857149124145508
train gradient:  0.421417612163795
iteration : 674
train acc:  0.765625
train loss:  0.48475879430770874
train gradient:  0.6630024073990256
iteration : 675
train acc:  0.7421875
train loss:  0.5343241691589355
train gradient:  0.8001513113539243
iteration : 676
train acc:  0.7578125
train loss:  0.48071810603141785
train gradient:  0.5425626268441884
iteration : 677
train acc:  0.7734375
train loss:  0.4499596357345581
train gradient:  0.4585426061870596
iteration : 678
train acc:  0.75
train loss:  0.5060933828353882
train gradient:  0.6665903095657446
iteration : 679
train acc:  0.8046875
train loss:  0.45920330286026
train gradient:  0.4299725597702678
iteration : 680
train acc:  0.734375
train loss:  0.5059036612510681
train gradient:  0.43342286622266407
iteration : 681
train acc:  0.7265625
train loss:  0.5436756610870361
train gradient:  0.7197001671869294
iteration : 682
train acc:  0.7578125
train loss:  0.470257431268692
train gradient:  0.4424499260513228
iteration : 683
train acc:  0.6875
train loss:  0.6247053742408752
train gradient:  0.8112570547664533
iteration : 684
train acc:  0.71875
train loss:  0.5357645750045776
train gradient:  0.5710864471618495
iteration : 685
train acc:  0.71875
train loss:  0.5264153480529785
train gradient:  0.6265302112826019
iteration : 686
train acc:  0.734375
train loss:  0.5263539552688599
train gradient:  0.7351506610561753
iteration : 687
train acc:  0.75
train loss:  0.4966811537742615
train gradient:  0.5254245446166463
iteration : 688
train acc:  0.796875
train loss:  0.433846116065979
train gradient:  0.35293194655855753
iteration : 689
train acc:  0.7734375
train loss:  0.45766526460647583
train gradient:  0.4876345746117457
iteration : 690
train acc:  0.75
train loss:  0.4872002601623535
train gradient:  0.44267715703517285
iteration : 691
train acc:  0.8046875
train loss:  0.43126627802848816
train gradient:  0.3261095929821927
iteration : 692
train acc:  0.7265625
train loss:  0.5088493227958679
train gradient:  0.6799398584884042
iteration : 693
train acc:  0.75
train loss:  0.4908878803253174
train gradient:  0.47104650300759804
iteration : 694
train acc:  0.734375
train loss:  0.5483984351158142
train gradient:  0.7536745869799122
iteration : 695
train acc:  0.765625
train loss:  0.4581645727157593
train gradient:  0.3559353380989
iteration : 696
train acc:  0.765625
train loss:  0.49370476603507996
train gradient:  0.47081127748833124
iteration : 697
train acc:  0.7578125
train loss:  0.4901021122932434
train gradient:  0.5575263452512889
iteration : 698
train acc:  0.7890625
train loss:  0.5005860924720764
train gradient:  0.8440555598403147
iteration : 699
train acc:  0.765625
train loss:  0.4800952970981598
train gradient:  0.5246273907531701
iteration : 700
train acc:  0.7578125
train loss:  0.4558348059654236
train gradient:  0.4145756168049314
iteration : 701
train acc:  0.703125
train loss:  0.5094701647758484
train gradient:  0.5472381248686082
iteration : 702
train acc:  0.78125
train loss:  0.47821903228759766
train gradient:  0.5057761919619685
iteration : 703
train acc:  0.7421875
train loss:  0.5605247020721436
train gradient:  0.6683258205358218
iteration : 704
train acc:  0.7578125
train loss:  0.48643139004707336
train gradient:  0.5372286568749535
iteration : 705
train acc:  0.7109375
train loss:  0.5655156373977661
train gradient:  0.8368376763059434
iteration : 706
train acc:  0.7421875
train loss:  0.5424791574478149
train gradient:  0.7469189959242044
iteration : 707
train acc:  0.6796875
train loss:  0.565591037273407
train gradient:  0.6873441794315917
iteration : 708
train acc:  0.8046875
train loss:  0.47220009565353394
train gradient:  0.601054162635169
iteration : 709
train acc:  0.7890625
train loss:  0.43435195088386536
train gradient:  0.48045976283666164
iteration : 710
train acc:  0.765625
train loss:  0.5061404705047607
train gradient:  0.3974971385375308
iteration : 711
train acc:  0.7265625
train loss:  0.5292174816131592
train gradient:  0.6108770829513991
iteration : 712
train acc:  0.7734375
train loss:  0.47927653789520264
train gradient:  0.38010801578640585
iteration : 713
train acc:  0.6953125
train loss:  0.6035573482513428
train gradient:  0.742226854272916
iteration : 714
train acc:  0.765625
train loss:  0.5231364369392395
train gradient:  0.4445609750516258
iteration : 715
train acc:  0.71875
train loss:  0.5462391376495361
train gradient:  0.6684441421404735
iteration : 716
train acc:  0.765625
train loss:  0.4985451102256775
train gradient:  0.782922353829785
iteration : 717
train acc:  0.796875
train loss:  0.4654970169067383
train gradient:  0.4098171440560381
iteration : 718
train acc:  0.765625
train loss:  0.45825818181037903
train gradient:  0.4867413581703388
iteration : 719
train acc:  0.828125
train loss:  0.41176068782806396
train gradient:  0.30211874477967915
iteration : 720
train acc:  0.7578125
train loss:  0.4954480528831482
train gradient:  0.3534332184623212
iteration : 721
train acc:  0.8046875
train loss:  0.4554009437561035
train gradient:  0.46083060478084487
iteration : 722
train acc:  0.765625
train loss:  0.5401516556739807
train gradient:  0.5868405289756538
iteration : 723
train acc:  0.734375
train loss:  0.5512369871139526
train gradient:  0.3880436578338772
iteration : 724
train acc:  0.7421875
train loss:  0.5262488722801208
train gradient:  0.48898004217357516
iteration : 725
train acc:  0.7890625
train loss:  0.4703906178474426
train gradient:  0.3264502862730122
iteration : 726
train acc:  0.734375
train loss:  0.4966241419315338
train gradient:  0.5132918594439724
iteration : 727
train acc:  0.703125
train loss:  0.5770912170410156
train gradient:  0.7546023405354206
iteration : 728
train acc:  0.7578125
train loss:  0.4981401264667511
train gradient:  0.5159858864468649
iteration : 729
train acc:  0.7890625
train loss:  0.4644976258277893
train gradient:  0.49340625859884635
iteration : 730
train acc:  0.75
train loss:  0.483894944190979
train gradient:  0.3504764416696599
iteration : 731
train acc:  0.7578125
train loss:  0.49812471866607666
train gradient:  0.47900141578791916
iteration : 732
train acc:  0.7578125
train loss:  0.5262141823768616
train gradient:  0.41963829916570033
iteration : 733
train acc:  0.7421875
train loss:  0.4709213376045227
train gradient:  0.4751695077080914
iteration : 734
train acc:  0.7421875
train loss:  0.4546440839767456
train gradient:  0.5165229430545453
iteration : 735
train acc:  0.78125
train loss:  0.493039071559906
train gradient:  0.4086800005640598
iteration : 736
train acc:  0.7890625
train loss:  0.494375616312027
train gradient:  0.6173870428862633
iteration : 737
train acc:  0.7578125
train loss:  0.5209991335868835
train gradient:  0.5515998681192429
iteration : 738
train acc:  0.8125
train loss:  0.4066810607910156
train gradient:  0.5492046425971094
iteration : 739
train acc:  0.8203125
train loss:  0.43241721391677856
train gradient:  0.429193029677007
iteration : 740
train acc:  0.7265625
train loss:  0.49398526549339294
train gradient:  0.5381100893321479
iteration : 741
train acc:  0.71875
train loss:  0.5103211402893066
train gradient:  0.541917389025321
iteration : 742
train acc:  0.7578125
train loss:  0.5089447498321533
train gradient:  0.5964710930169718
iteration : 743
train acc:  0.78125
train loss:  0.492108017206192
train gradient:  0.3724127449548446
iteration : 744
train acc:  0.7890625
train loss:  0.4716823399066925
train gradient:  0.4771051549737037
iteration : 745
train acc:  0.75
train loss:  0.5287103056907654
train gradient:  0.5842674126940117
iteration : 746
train acc:  0.75
train loss:  0.48542875051498413
train gradient:  0.5881251454957684
iteration : 747
train acc:  0.75
train loss:  0.5359860062599182
train gradient:  0.6060041199584347
iteration : 748
train acc:  0.796875
train loss:  0.42643389105796814
train gradient:  0.4118614122473007
iteration : 749
train acc:  0.7109375
train loss:  0.5874205827713013
train gradient:  0.6916594443835461
iteration : 750
train acc:  0.8203125
train loss:  0.44513994455337524
train gradient:  0.4812610125214103
iteration : 751
train acc:  0.7421875
train loss:  0.4790163040161133
train gradient:  0.502461673765623
iteration : 752
train acc:  0.8125
train loss:  0.43817171454429626
train gradient:  0.4837439722881676
iteration : 753
train acc:  0.8359375
train loss:  0.40461060404777527
train gradient:  0.36116835397194774
iteration : 754
train acc:  0.75
train loss:  0.4761838912963867
train gradient:  0.38609044773448875
iteration : 755
train acc:  0.796875
train loss:  0.41652995347976685
train gradient:  0.3638108640898199
iteration : 756
train acc:  0.8125
train loss:  0.475860595703125
train gradient:  0.4713893292751225
iteration : 757
train acc:  0.75
train loss:  0.4730945825576782
train gradient:  0.5222815490073986
iteration : 758
train acc:  0.7265625
train loss:  0.5310215950012207
train gradient:  0.6245844151430571
iteration : 759
train acc:  0.78125
train loss:  0.495422899723053
train gradient:  0.4624531641536031
iteration : 760
train acc:  0.7265625
train loss:  0.4836147129535675
train gradient:  0.4789524730003832
iteration : 761
train acc:  0.71875
train loss:  0.5429016947746277
train gradient:  0.5488007345461898
iteration : 762
train acc:  0.78125
train loss:  0.5006792545318604
train gradient:  0.45708139019927907
iteration : 763
train acc:  0.71875
train loss:  0.5420514941215515
train gradient:  0.7378633658302611
iteration : 764
train acc:  0.6953125
train loss:  0.5357300043106079
train gradient:  0.8527910614812737
iteration : 765
train acc:  0.7578125
train loss:  0.47718822956085205
train gradient:  0.543373830202339
iteration : 766
train acc:  0.8671875
train loss:  0.38756880164146423
train gradient:  0.45773461942461874
iteration : 767
train acc:  0.796875
train loss:  0.4160591959953308
train gradient:  0.3436473509376774
iteration : 768
train acc:  0.7734375
train loss:  0.43575090169906616
train gradient:  0.5085425924693865
iteration : 769
train acc:  0.7578125
train loss:  0.49592655897140503
train gradient:  0.5273972458909235
iteration : 770
train acc:  0.7421875
train loss:  0.5096659660339355
train gradient:  0.6261371672877037
iteration : 771
train acc:  0.7734375
train loss:  0.4516725540161133
train gradient:  0.4803875339986453
iteration : 772
train acc:  0.78125
train loss:  0.4343165159225464
train gradient:  0.4700263169004745
iteration : 773
train acc:  0.71875
train loss:  0.5889295339584351
train gradient:  0.8080444862533068
iteration : 774
train acc:  0.7578125
train loss:  0.5209877490997314
train gradient:  0.5523539223757814
iteration : 775
train acc:  0.8203125
train loss:  0.4106864333152771
train gradient:  0.3338544625417737
iteration : 776
train acc:  0.7734375
train loss:  0.45592811703681946
train gradient:  0.4665833733929362
iteration : 777
train acc:  0.7890625
train loss:  0.4731582701206207
train gradient:  0.48183478922080775
iteration : 778
train acc:  0.7265625
train loss:  0.5287958979606628
train gradient:  0.552036725074468
iteration : 779
train acc:  0.7109375
train loss:  0.48875027894973755
train gradient:  0.6148447390928069
iteration : 780
train acc:  0.8046875
train loss:  0.4329115152359009
train gradient:  0.4030333847759576
iteration : 781
train acc:  0.765625
train loss:  0.44256526231765747
train gradient:  0.45820508578694785
iteration : 782
train acc:  0.765625
train loss:  0.48446211218833923
train gradient:  0.46475837276815657
iteration : 783
train acc:  0.7890625
train loss:  0.45551589131355286
train gradient:  0.43860203361142325
iteration : 784
train acc:  0.828125
train loss:  0.42638611793518066
train gradient:  0.5355864651071358
iteration : 785
train acc:  0.7890625
train loss:  0.4197664260864258
train gradient:  0.3961226990375901
iteration : 786
train acc:  0.8046875
train loss:  0.43418997526168823
train gradient:  0.43927814088253403
iteration : 787
train acc:  0.7734375
train loss:  0.489784300327301
train gradient:  0.6150726368828352
iteration : 788
train acc:  0.78125
train loss:  0.4520578980445862
train gradient:  0.4492559063321547
iteration : 789
train acc:  0.703125
train loss:  0.5369476079940796
train gradient:  0.6516928334378855
iteration : 790
train acc:  0.71875
train loss:  0.5237897038459778
train gradient:  0.7813667265971246
iteration : 791
train acc:  0.765625
train loss:  0.45330217480659485
train gradient:  0.6713218927900398
iteration : 792
train acc:  0.7734375
train loss:  0.47817158699035645
train gradient:  0.5906060248671802
iteration : 793
train acc:  0.7578125
train loss:  0.49668747186660767
train gradient:  0.7758482450428867
iteration : 794
train acc:  0.8515625
train loss:  0.40002983808517456
train gradient:  0.41431187941282827
iteration : 795
train acc:  0.78125
train loss:  0.4434643089771271
train gradient:  0.530634333244553
iteration : 796
train acc:  0.8046875
train loss:  0.44739246368408203
train gradient:  0.7045325522206284
iteration : 797
train acc:  0.75
train loss:  0.4964272379875183
train gradient:  0.5692443526317841
iteration : 798
train acc:  0.78125
train loss:  0.4754926264286041
train gradient:  0.4395031374038581
iteration : 799
train acc:  0.8359375
train loss:  0.38780057430267334
train gradient:  0.3894781773757424
iteration : 800
train acc:  0.71875
train loss:  0.5091603994369507
train gradient:  0.7491179482591876
iteration : 801
train acc:  0.734375
train loss:  0.5882546901702881
train gradient:  0.760630649103009
iteration : 802
train acc:  0.7578125
train loss:  0.4881259799003601
train gradient:  0.5005750768595819
iteration : 803
train acc:  0.7578125
train loss:  0.4500833749771118
train gradient:  0.6056810807601563
iteration : 804
train acc:  0.8125
train loss:  0.43435361981391907
train gradient:  0.5133594382625122
iteration : 805
train acc:  0.8203125
train loss:  0.3944132924079895
train gradient:  0.3821480361018506
iteration : 806
train acc:  0.7890625
train loss:  0.4908536374568939
train gradient:  0.6620621232566904
iteration : 807
train acc:  0.7109375
train loss:  0.5607494115829468
train gradient:  0.7221744345784503
iteration : 808
train acc:  0.7421875
train loss:  0.5541926622390747
train gradient:  0.6750140353935742
iteration : 809
train acc:  0.7265625
train loss:  0.5394716262817383
train gradient:  0.5925346832436449
iteration : 810
train acc:  0.828125
train loss:  0.4294205904006958
train gradient:  0.6403689826489157
iteration : 811
train acc:  0.734375
train loss:  0.5529910922050476
train gradient:  0.6294748880912022
iteration : 812
train acc:  0.78125
train loss:  0.474597692489624
train gradient:  0.48742772894306136
iteration : 813
train acc:  0.8125
train loss:  0.44627031683921814
train gradient:  0.5483844054361979
iteration : 814
train acc:  0.78125
train loss:  0.4712905287742615
train gradient:  0.5571422233067216
iteration : 815
train acc:  0.8125
train loss:  0.43435895442962646
train gradient:  0.5184658387099581
iteration : 816
train acc:  0.796875
train loss:  0.42152127623558044
train gradient:  0.44610163725292923
iteration : 817
train acc:  0.7734375
train loss:  0.4333292245864868
train gradient:  0.49513456032892106
iteration : 818
train acc:  0.8125
train loss:  0.42092734575271606
train gradient:  0.40732875674567787
iteration : 819
train acc:  0.8125
train loss:  0.46130841970443726
train gradient:  0.4016733102996887
iteration : 820
train acc:  0.7578125
train loss:  0.4965958893299103
train gradient:  0.5618145134059707
iteration : 821
train acc:  0.8203125
train loss:  0.43421900272369385
train gradient:  0.4095795820174727
iteration : 822
train acc:  0.765625
train loss:  0.512448787689209
train gradient:  0.7351086174512269
iteration : 823
train acc:  0.796875
train loss:  0.505549967288971
train gradient:  0.724900045170122
iteration : 824
train acc:  0.796875
train loss:  0.47448039054870605
train gradient:  0.5285768376306036
iteration : 825
train acc:  0.7734375
train loss:  0.5129289031028748
train gradient:  0.5202699052073783
iteration : 826
train acc:  0.71875
train loss:  0.5629012584686279
train gradient:  0.5983550507512763
iteration : 827
train acc:  0.8046875
train loss:  0.44668304920196533
train gradient:  0.37705270245956635
iteration : 828
train acc:  0.71875
train loss:  0.5384882092475891
train gradient:  0.6772174806879895
iteration : 829
train acc:  0.75
train loss:  0.5537086725234985
train gradient:  0.7377021414075403
iteration : 830
train acc:  0.765625
train loss:  0.48667407035827637
train gradient:  0.4691021177864283
iteration : 831
train acc:  0.796875
train loss:  0.4281862676143646
train gradient:  0.35599740885405373
iteration : 832
train acc:  0.765625
train loss:  0.49311667680740356
train gradient:  0.509969681060378
iteration : 833
train acc:  0.7578125
train loss:  0.4622812271118164
train gradient:  0.6356902614484594
iteration : 834
train acc:  0.75
train loss:  0.5129231810569763
train gradient:  0.6967658993770092
iteration : 835
train acc:  0.8046875
train loss:  0.48219582438468933
train gradient:  0.4747513987116182
iteration : 836
train acc:  0.8203125
train loss:  0.4319949746131897
train gradient:  0.5925114944083434
iteration : 837
train acc:  0.734375
train loss:  0.545847475528717
train gradient:  0.696729554499681
iteration : 838
train acc:  0.734375
train loss:  0.4970014691352844
train gradient:  0.6100865433687281
iteration : 839
train acc:  0.765625
train loss:  0.5150766372680664
train gradient:  0.6201570953896397
iteration : 840
train acc:  0.796875
train loss:  0.45375746488571167
train gradient:  0.43525453836716965
iteration : 841
train acc:  0.7734375
train loss:  0.5026769638061523
train gradient:  0.7586382383312327
iteration : 842
train acc:  0.796875
train loss:  0.43533194065093994
train gradient:  0.5954106726605265
iteration : 843
train acc:  0.734375
train loss:  0.5059244632720947
train gradient:  0.5883456193044154
iteration : 844
train acc:  0.8046875
train loss:  0.43017417192459106
train gradient:  0.46139617790292603
iteration : 845
train acc:  0.8125
train loss:  0.45699161291122437
train gradient:  0.7442209018759636
iteration : 846
train acc:  0.7734375
train loss:  0.48232102394104004
train gradient:  0.5703614757400358
iteration : 847
train acc:  0.7734375
train loss:  0.44136959314346313
train gradient:  0.4774937578477126
iteration : 848
train acc:  0.7890625
train loss:  0.46466609835624695
train gradient:  0.6004436298048086
iteration : 849
train acc:  0.7578125
train loss:  0.47749191522598267
train gradient:  0.864341506026914
iteration : 850
train acc:  0.78125
train loss:  0.4406895637512207
train gradient:  0.36332808400289907
iteration : 851
train acc:  0.7578125
train loss:  0.4344235062599182
train gradient:  0.4518089691743622
iteration : 852
train acc:  0.796875
train loss:  0.4767966866493225
train gradient:  0.5148544237535099
iteration : 853
train acc:  0.75
train loss:  0.4815937876701355
train gradient:  0.4551450167592957
iteration : 854
train acc:  0.7421875
train loss:  0.5044555068016052
train gradient:  0.7546222328084574
iteration : 855
train acc:  0.765625
train loss:  0.5171585083007812
train gradient:  0.6870910753641055
iteration : 856
train acc:  0.734375
train loss:  0.5048739314079285
train gradient:  0.5758982084485683
iteration : 857
train acc:  0.796875
train loss:  0.47888392210006714
train gradient:  0.691547828328068
iteration : 858
train acc:  0.8203125
train loss:  0.42226123809814453
train gradient:  0.5140372733541432
iteration : 859
train acc:  0.765625
train loss:  0.524551510810852
train gradient:  0.6329275448077784
iteration : 860
train acc:  0.765625
train loss:  0.4766679108142853
train gradient:  0.48048494205501985
iteration : 861
train acc:  0.7109375
train loss:  0.5110093355178833
train gradient:  0.6285741654130373
iteration : 862
train acc:  0.7890625
train loss:  0.4992581009864807
train gradient:  0.6139963402618028
iteration : 863
train acc:  0.8125
train loss:  0.48481231927871704
train gradient:  0.3872011070302774
iteration : 864
train acc:  0.71875
train loss:  0.5218758583068848
train gradient:  0.7297004055417291
iteration : 865
train acc:  0.7734375
train loss:  0.4356403946876526
train gradient:  0.40239753365668507
iteration : 866
train acc:  0.7734375
train loss:  0.42977631092071533
train gradient:  0.4172528464484893
iteration : 867
train acc:  0.734375
train loss:  0.4851455092430115
train gradient:  0.4576135919478414
iteration : 868
train acc:  0.765625
train loss:  0.49814435839653015
train gradient:  0.3597007494265151
iteration : 869
train acc:  0.7265625
train loss:  0.5594218969345093
train gradient:  0.6186608119137835
iteration : 870
train acc:  0.7578125
train loss:  0.46775388717651367
train gradient:  0.42034727685157885
iteration : 871
train acc:  0.796875
train loss:  0.4279938340187073
train gradient:  0.3723926963223474
iteration : 872
train acc:  0.7578125
train loss:  0.46593421697616577
train gradient:  0.4719866324762937
iteration : 873
train acc:  0.7421875
train loss:  0.5196606516838074
train gradient:  0.7215210089962424
iteration : 874
train acc:  0.75
train loss:  0.5234223008155823
train gradient:  0.5607483123892771
iteration : 875
train acc:  0.6953125
train loss:  0.6077706813812256
train gradient:  0.8609221020416238
iteration : 876
train acc:  0.78125
train loss:  0.448773056268692
train gradient:  0.5630563921125005
iteration : 877
train acc:  0.8046875
train loss:  0.4376752972602844
train gradient:  0.5841415968855612
iteration : 878
train acc:  0.8203125
train loss:  0.4457685053348541
train gradient:  0.41458058796510244
iteration : 879
train acc:  0.78125
train loss:  0.4779859483242035
train gradient:  0.40530780229523006
iteration : 880
train acc:  0.7421875
train loss:  0.5034192800521851
train gradient:  0.4557218812606233
iteration : 881
train acc:  0.7890625
train loss:  0.49236518144607544
train gradient:  0.46889023134098623
iteration : 882
train acc:  0.7890625
train loss:  0.46693500876426697
train gradient:  0.5112398217675036
iteration : 883
train acc:  0.84375
train loss:  0.3973698019981384
train gradient:  0.426295555846942
iteration : 884
train acc:  0.84375
train loss:  0.4264473021030426
train gradient:  0.6609377536386527
iteration : 885
train acc:  0.7734375
train loss:  0.4202764332294464
train gradient:  0.37011885341609635
iteration : 886
train acc:  0.7734375
train loss:  0.47002846002578735
train gradient:  0.4878621889104012
iteration : 887
train acc:  0.7578125
train loss:  0.4539494514465332
train gradient:  0.6856991069317719
iteration : 888
train acc:  0.796875
train loss:  0.43134817481040955
train gradient:  0.39381924060507734
iteration : 889
train acc:  0.765625
train loss:  0.48269516229629517
train gradient:  0.5312225232420165
iteration : 890
train acc:  0.78125
train loss:  0.4842182993888855
train gradient:  0.5834275436995939
iteration : 891
train acc:  0.7890625
train loss:  0.4268138110637665
train gradient:  0.6274761238746678
iteration : 892
train acc:  0.7734375
train loss:  0.42327919602394104
train gradient:  0.4620227127195879
iteration : 893
train acc:  0.8203125
train loss:  0.4138314127922058
train gradient:  0.5152899274396152
iteration : 894
train acc:  0.7578125
train loss:  0.4828260838985443
train gradient:  0.5542011048475693
iteration : 895
train acc:  0.7421875
train loss:  0.4953807294368744
train gradient:  0.7155989359332907
iteration : 896
train acc:  0.7578125
train loss:  0.5228421092033386
train gradient:  0.7562116686894451
iteration : 897
train acc:  0.734375
train loss:  0.5972139835357666
train gradient:  1.1241555463255173
iteration : 898
train acc:  0.7890625
train loss:  0.416185200214386
train gradient:  0.41776372779550036
iteration : 899
train acc:  0.7265625
train loss:  0.5813995599746704
train gradient:  0.9101601822138052
iteration : 900
train acc:  0.84375
train loss:  0.36556699872016907
train gradient:  0.33085479972592324
iteration : 901
train acc:  0.7578125
train loss:  0.45171302556991577
train gradient:  0.46586052966305697
iteration : 902
train acc:  0.75
train loss:  0.48578736186027527
train gradient:  0.49153975775448866
iteration : 903
train acc:  0.75
train loss:  0.4975430965423584
train gradient:  0.62095660860537
iteration : 904
train acc:  0.7578125
train loss:  0.4576491713523865
train gradient:  0.6067151110965794
iteration : 905
train acc:  0.734375
train loss:  0.4989066421985626
train gradient:  0.6070420873644604
iteration : 906
train acc:  0.7890625
train loss:  0.49802184104919434
train gradient:  0.45915926891975856
iteration : 907
train acc:  0.8125
train loss:  0.42457306385040283
train gradient:  1.1167564399379841
iteration : 908
train acc:  0.7734375
train loss:  0.4666980504989624
train gradient:  0.5759842192176212
iteration : 909
train acc:  0.78125
train loss:  0.48605990409851074
train gradient:  0.5930370021241185
iteration : 910
train acc:  0.7734375
train loss:  0.49787193536758423
train gradient:  0.5742012822650971
iteration : 911
train acc:  0.7578125
train loss:  0.510115921497345
train gradient:  0.5322783853298632
iteration : 912
train acc:  0.7421875
train loss:  0.5011376142501831
train gradient:  0.5719625223220375
iteration : 913
train acc:  0.7578125
train loss:  0.49138692021369934
train gradient:  0.5812752581375155
iteration : 914
train acc:  0.7421875
train loss:  0.4613402783870697
train gradient:  0.40096137362893997
iteration : 915
train acc:  0.78125
train loss:  0.43682438135147095
train gradient:  0.5357134683564542
iteration : 916
train acc:  0.6953125
train loss:  0.5796991586685181
train gradient:  0.6797237778225461
iteration : 917
train acc:  0.7890625
train loss:  0.4354793429374695
train gradient:  0.395575723698563
iteration : 918
train acc:  0.703125
train loss:  0.6037254929542542
train gradient:  0.8149664042051818
iteration : 919
train acc:  0.78125
train loss:  0.4811033010482788
train gradient:  0.4429384603827886
iteration : 920
train acc:  0.7734375
train loss:  0.537155032157898
train gradient:  0.5372666001800579
iteration : 921
train acc:  0.8046875
train loss:  0.4076148569583893
train gradient:  0.3284030249454817
iteration : 922
train acc:  0.7578125
train loss:  0.577285647392273
train gradient:  0.6071855463524934
iteration : 923
train acc:  0.7265625
train loss:  0.5329148769378662
train gradient:  0.6087833081589915
iteration : 924
train acc:  0.7890625
train loss:  0.49420595169067383
train gradient:  0.36912944815525583
iteration : 925
train acc:  0.7421875
train loss:  0.5284870266914368
train gradient:  0.49462794853672365
iteration : 926
train acc:  0.78125
train loss:  0.4297768473625183
train gradient:  0.4043639591173482
iteration : 927
train acc:  0.7890625
train loss:  0.4536697566509247
train gradient:  0.4273433521564279
iteration : 928
train acc:  0.78125
train loss:  0.44762858748435974
train gradient:  0.40209822112659943
iteration : 929
train acc:  0.8203125
train loss:  0.4370552897453308
train gradient:  0.3275240485816268
iteration : 930
train acc:  0.7421875
train loss:  0.45860379934310913
train gradient:  0.39911850488507783
iteration : 931
train acc:  0.78125
train loss:  0.49957582354545593
train gradient:  0.5921747791081613
iteration : 932
train acc:  0.6796875
train loss:  0.5511553287506104
train gradient:  0.6206081869526263
iteration : 933
train acc:  0.7734375
train loss:  0.41044384241104126
train gradient:  0.45727825250834353
iteration : 934
train acc:  0.7890625
train loss:  0.48798897862434387
train gradient:  0.46275896215054646
iteration : 935
train acc:  0.78125
train loss:  0.4435192346572876
train gradient:  0.3379996311952048
iteration : 936
train acc:  0.7734375
train loss:  0.47169309854507446
train gradient:  0.4351817585431949
iteration : 937
train acc:  0.8359375
train loss:  0.4076237976551056
train gradient:  0.5165222068657256
iteration : 938
train acc:  0.765625
train loss:  0.5128039121627808
train gradient:  0.45470967832233844
iteration : 939
train acc:  0.796875
train loss:  0.47464656829833984
train gradient:  0.44843888151842304
iteration : 940
train acc:  0.8046875
train loss:  0.43532639741897583
train gradient:  0.38490895617430554
iteration : 941
train acc:  0.765625
train loss:  0.4812806248664856
train gradient:  0.6119021739589521
iteration : 942
train acc:  0.7890625
train loss:  0.42209550738334656
train gradient:  0.44453563509120797
iteration : 943
train acc:  0.8203125
train loss:  0.3958749771118164
train gradient:  0.34313921710650225
iteration : 944
train acc:  0.765625
train loss:  0.4877445101737976
train gradient:  0.6874335580545943
iteration : 945
train acc:  0.7734375
train loss:  0.4776683449745178
train gradient:  0.517185360295753
iteration : 946
train acc:  0.8359375
train loss:  0.4168434143066406
train gradient:  0.4058924748023597
iteration : 947
train acc:  0.7734375
train loss:  0.43941202759742737
train gradient:  0.5791265945071116
iteration : 948
train acc:  0.7421875
train loss:  0.4845171868801117
train gradient:  0.6188647613108469
iteration : 949
train acc:  0.7734375
train loss:  0.48563045263290405
train gradient:  0.5648792067683186
iteration : 950
train acc:  0.734375
train loss:  0.49112486839294434
train gradient:  0.5410279261300146
iteration : 951
train acc:  0.8125
train loss:  0.4312651753425598
train gradient:  0.49173289778313883
iteration : 952
train acc:  0.78125
train loss:  0.48060938715934753
train gradient:  0.46747691251819645
iteration : 953
train acc:  0.8125
train loss:  0.44494566321372986
train gradient:  0.4398881783242461
iteration : 954
train acc:  0.734375
train loss:  0.4716262221336365
train gradient:  0.4246612636556677
iteration : 955
train acc:  0.71875
train loss:  0.5187603831291199
train gradient:  0.696264167017867
iteration : 956
train acc:  0.8203125
train loss:  0.4016762375831604
train gradient:  0.348414177593995
iteration : 957
train acc:  0.703125
train loss:  0.5920773148536682
train gradient:  0.6962575736247989
iteration : 958
train acc:  0.78125
train loss:  0.48190611600875854
train gradient:  0.6007253195107591
iteration : 959
train acc:  0.7578125
train loss:  0.48828619718551636
train gradient:  0.285917166714349
iteration : 960
train acc:  0.84375
train loss:  0.39896732568740845
train gradient:  0.37243865054673725
iteration : 961
train acc:  0.7734375
train loss:  0.4334281384944916
train gradient:  0.5086932621787668
iteration : 962
train acc:  0.765625
train loss:  0.4733259081840515
train gradient:  0.34157899201678477
iteration : 963
train acc:  0.7578125
train loss:  0.4741517901420593
train gradient:  0.5722502927617829
iteration : 964
train acc:  0.7265625
train loss:  0.5230220556259155
train gradient:  0.5333383178440462
iteration : 965
train acc:  0.765625
train loss:  0.4957786500453949
train gradient:  0.44774842042182245
iteration : 966
train acc:  0.8046875
train loss:  0.4005313515663147
train gradient:  0.34684993601043457
iteration : 967
train acc:  0.703125
train loss:  0.5378462076187134
train gradient:  0.5380307948144765
iteration : 968
train acc:  0.7734375
train loss:  0.49892300367355347
train gradient:  0.42697712005807875
iteration : 969
train acc:  0.7734375
train loss:  0.4769328832626343
train gradient:  0.4509343121442062
iteration : 970
train acc:  0.8203125
train loss:  0.4060291051864624
train gradient:  0.45845075666952784
iteration : 971
train acc:  0.8046875
train loss:  0.43991512060165405
train gradient:  0.49191166509380596
iteration : 972
train acc:  0.8125
train loss:  0.4098585844039917
train gradient:  0.2544531933489716
iteration : 973
train acc:  0.796875
train loss:  0.4078068137168884
train gradient:  0.4357169623956821
iteration : 974
train acc:  0.78125
train loss:  0.4234272837638855
train gradient:  0.3296811965377928
iteration : 975
train acc:  0.8203125
train loss:  0.40848639607429504
train gradient:  0.41761163696327375
iteration : 976
train acc:  0.7578125
train loss:  0.4512459635734558
train gradient:  0.5431439883064668
iteration : 977
train acc:  0.7109375
train loss:  0.5150683522224426
train gradient:  0.5723118601727885
iteration : 978
train acc:  0.765625
train loss:  0.4557313621044159
train gradient:  0.4084524442688101
iteration : 979
train acc:  0.734375
train loss:  0.49241164326667786
train gradient:  0.5118114286597699
iteration : 980
train acc:  0.7734375
train loss:  0.4599023461341858
train gradient:  0.5052365686488003
iteration : 981
train acc:  0.796875
train loss:  0.4532703757286072
train gradient:  0.4657745165507136
iteration : 982
train acc:  0.8203125
train loss:  0.416179895401001
train gradient:  0.558415379323313
iteration : 983
train acc:  0.8203125
train loss:  0.4163053333759308
train gradient:  0.40496990531708216
iteration : 984
train acc:  0.734375
train loss:  0.5308210849761963
train gradient:  0.8311213411327549
iteration : 985
train acc:  0.796875
train loss:  0.4286200702190399
train gradient:  0.638026932894341
iteration : 986
train acc:  0.7265625
train loss:  0.5324424505233765
train gradient:  0.6790346677582363
iteration : 987
train acc:  0.7578125
train loss:  0.47278690338134766
train gradient:  0.6806840755539241
iteration : 988
train acc:  0.78125
train loss:  0.4084291160106659
train gradient:  0.5083206446208461
iteration : 989
train acc:  0.71875
train loss:  0.5117037296295166
train gradient:  0.8651677791303222
iteration : 990
train acc:  0.7734375
train loss:  0.46279996633529663
train gradient:  0.6191785294982782
iteration : 991
train acc:  0.7734375
train loss:  0.452333003282547
train gradient:  0.5737356925537338
iteration : 992
train acc:  0.7890625
train loss:  0.4256017506122589
train gradient:  0.579225175702649
iteration : 993
train acc:  0.78125
train loss:  0.45010364055633545
train gradient:  0.48751295925776567
iteration : 994
train acc:  0.7890625
train loss:  0.4691621959209442
train gradient:  0.598164784630328
iteration : 995
train acc:  0.7421875
train loss:  0.44671088457107544
train gradient:  0.5218129999223663
iteration : 996
train acc:  0.8359375
train loss:  0.39277517795562744
train gradient:  0.42797360487994235
iteration : 997
train acc:  0.828125
train loss:  0.41151300072669983
train gradient:  0.6608369212370047
iteration : 998
train acc:  0.734375
train loss:  0.5118518471717834
train gradient:  0.5466023072119965
iteration : 999
train acc:  0.796875
train loss:  0.45391589403152466
train gradient:  0.5975084005887721
iteration : 1000
train acc:  0.796875
train loss:  0.43210482597351074
train gradient:  0.5029996830203735
iteration : 1001
train acc:  0.8046875
train loss:  0.40722423791885376
train gradient:  0.3626255983974538
iteration : 1002
train acc:  0.7578125
train loss:  0.44685328006744385
train gradient:  0.5790771718634842
iteration : 1003
train acc:  0.7265625
train loss:  0.5249171853065491
train gradient:  0.7281426990212323
iteration : 1004
train acc:  0.7890625
train loss:  0.4387986958026886
train gradient:  0.4544037026759052
iteration : 1005
train acc:  0.8046875
train loss:  0.4464031159877777
train gradient:  0.42149610432713647
iteration : 1006
train acc:  0.7421875
train loss:  0.5004367232322693
train gradient:  0.5915807471593224
iteration : 1007
train acc:  0.8828125
train loss:  0.3424208164215088
train gradient:  0.3766365843991933
iteration : 1008
train acc:  0.8203125
train loss:  0.4225618839263916
train gradient:  0.49110069498395764
iteration : 1009
train acc:  0.796875
train loss:  0.4464043080806732
train gradient:  0.4698721361914116
iteration : 1010
train acc:  0.765625
train loss:  0.49106499552726746
train gradient:  0.619860759945936
iteration : 1011
train acc:  0.7734375
train loss:  0.44635602831840515
train gradient:  0.5435742271983109
iteration : 1012
train acc:  0.8359375
train loss:  0.39964842796325684
train gradient:  0.5498892358277376
iteration : 1013
train acc:  0.7734375
train loss:  0.4451408386230469
train gradient:  0.600521817865566
iteration : 1014
train acc:  0.8125
train loss:  0.4999004006385803
train gradient:  0.7328145614889645
iteration : 1015
train acc:  0.78125
train loss:  0.4120379090309143
train gradient:  0.4529119149936965
iteration : 1016
train acc:  0.8203125
train loss:  0.40502411127090454
train gradient:  0.415191686118436
iteration : 1017
train acc:  0.8203125
train loss:  0.4376789927482605
train gradient:  0.7490141640885233
iteration : 1018
train acc:  0.7578125
train loss:  0.4176337718963623
train gradient:  0.5418093097232292
iteration : 1019
train acc:  0.7421875
train loss:  0.487590491771698
train gradient:  0.7548227368806364
iteration : 1020
train acc:  0.7734375
train loss:  0.42189809679985046
train gradient:  0.5164971883063006
iteration : 1021
train acc:  0.7421875
train loss:  0.4919779896736145
train gradient:  0.6276750393394259
iteration : 1022
train acc:  0.8046875
train loss:  0.47095808386802673
train gradient:  0.6834389998604605
iteration : 1023
train acc:  0.796875
train loss:  0.402170866727829
train gradient:  0.5085742076830996
iteration : 1024
train acc:  0.6875
train loss:  0.5378742218017578
train gradient:  0.8357625362607932
iteration : 1025
train acc:  0.78125
train loss:  0.43893569707870483
train gradient:  0.613137622028666
iteration : 1026
train acc:  0.859375
train loss:  0.4211442172527313
train gradient:  0.4241891731567268
iteration : 1027
train acc:  0.859375
train loss:  0.3717529773712158
train gradient:  0.35814411853182404
iteration : 1028
train acc:  0.7890625
train loss:  0.47371262311935425
train gradient:  0.7197985193553084
iteration : 1029
train acc:  0.7578125
train loss:  0.4978039264678955
train gradient:  0.7980935289383345
iteration : 1030
train acc:  0.8125
train loss:  0.39916008710861206
train gradient:  0.4622687213721097
iteration : 1031
train acc:  0.796875
train loss:  0.4418816566467285
train gradient:  0.722138346177629
iteration : 1032
train acc:  0.78125
train loss:  0.43357837200164795
train gradient:  0.559177041045626
iteration : 1033
train acc:  0.84375
train loss:  0.3772541880607605
train gradient:  0.4707907642086083
iteration : 1034
train acc:  0.8671875
train loss:  0.3713451623916626
train gradient:  0.4735400296177011
iteration : 1035
train acc:  0.8203125
train loss:  0.40233245491981506
train gradient:  0.3961613684792863
iteration : 1036
train acc:  0.734375
train loss:  0.5487105846405029
train gradient:  0.8155863398987964
iteration : 1037
train acc:  0.7734375
train loss:  0.4660600423812866
train gradient:  0.4333899806124508
iteration : 1038
train acc:  0.78125
train loss:  0.4638395607471466
train gradient:  0.6114481477958178
iteration : 1039
train acc:  0.78125
train loss:  0.42077115178108215
train gradient:  0.48948725237664376
iteration : 1040
train acc:  0.8046875
train loss:  0.45398467779159546
train gradient:  0.567767307705494
iteration : 1041
train acc:  0.8359375
train loss:  0.37474387884140015
train gradient:  0.42781108162730197
iteration : 1042
train acc:  0.734375
train loss:  0.5116336345672607
train gradient:  0.549228848377641
iteration : 1043
train acc:  0.765625
train loss:  0.48058491945266724
train gradient:  0.5100565352454617
iteration : 1044
train acc:  0.7890625
train loss:  0.47012513875961304
train gradient:  0.8891530818815429
iteration : 1045
train acc:  0.8046875
train loss:  0.4049643874168396
train gradient:  0.652191342548242
iteration : 1046
train acc:  0.734375
train loss:  0.453254759311676
train gradient:  0.5027663609833741
iteration : 1047
train acc:  0.7734375
train loss:  0.4447207450866699
train gradient:  0.7972289751991858
iteration : 1048
train acc:  0.7890625
train loss:  0.4478025436401367
train gradient:  1.4788696577309954
iteration : 1049
train acc:  0.859375
train loss:  0.3547137975692749
train gradient:  0.399328803029443
iteration : 1050
train acc:  0.7421875
train loss:  0.5503143072128296
train gradient:  0.8124305065385926
iteration : 1051
train acc:  0.8515625
train loss:  0.3720170855522156
train gradient:  0.38385432984965606
iteration : 1052
train acc:  0.7890625
train loss:  0.4495561420917511
train gradient:  0.6066890898819708
iteration : 1053
train acc:  0.6875
train loss:  0.5332685708999634
train gradient:  0.6683932170459226
iteration : 1054
train acc:  0.796875
train loss:  0.4325754642486572
train gradient:  0.7048639195427816
iteration : 1055
train acc:  0.8515625
train loss:  0.4181320071220398
train gradient:  0.6335511768929691
iteration : 1056
train acc:  0.7734375
train loss:  0.4538308084011078
train gradient:  0.5090494871708771
iteration : 1057
train acc:  0.796875
train loss:  0.4162815511226654
train gradient:  0.4982213078225637
iteration : 1058
train acc:  0.796875
train loss:  0.4284791052341461
train gradient:  0.6282635980027795
iteration : 1059
train acc:  0.7890625
train loss:  0.436842143535614
train gradient:  0.4918316655978985
iteration : 1060
train acc:  0.7734375
train loss:  0.4616442322731018
train gradient:  0.5414585631757851
iteration : 1061
train acc:  0.78125
train loss:  0.44288039207458496
train gradient:  0.6582459170669287
iteration : 1062
train acc:  0.75
train loss:  0.48040419816970825
train gradient:  0.6083763631193677
iteration : 1063
train acc:  0.75
train loss:  0.4768885672092438
train gradient:  0.548920514674326
iteration : 1064
train acc:  0.7421875
train loss:  0.5252600908279419
train gradient:  0.6081539000993637
iteration : 1065
train acc:  0.796875
train loss:  0.438554584980011
train gradient:  0.49490151616186784
iteration : 1066
train acc:  0.8203125
train loss:  0.4287129044532776
train gradient:  0.6833936814579465
iteration : 1067
train acc:  0.8203125
train loss:  0.37642258405685425
train gradient:  0.5013921932490866
iteration : 1068
train acc:  0.7890625
train loss:  0.4638696014881134
train gradient:  0.5136621861950517
iteration : 1069
train acc:  0.7734375
train loss:  0.4398682713508606
train gradient:  0.35931922370206015
iteration : 1070
train acc:  0.7421875
train loss:  0.47403451800346375
train gradient:  0.6333922971869164
iteration : 1071
train acc:  0.8359375
train loss:  0.39484336972236633
train gradient:  0.43707368743040836
iteration : 1072
train acc:  0.796875
train loss:  0.44682174921035767
train gradient:  0.5287456688637004
iteration : 1073
train acc:  0.84375
train loss:  0.4640447199344635
train gradient:  0.7451249830930042
iteration : 1074
train acc:  0.8046875
train loss:  0.38362544775009155
train gradient:  0.5094150292322615
iteration : 1075
train acc:  0.796875
train loss:  0.41669732332229614
train gradient:  0.5409911183207841
iteration : 1076
train acc:  0.734375
train loss:  0.526814341545105
train gradient:  0.6204402445624442
iteration : 1077
train acc:  0.765625
train loss:  0.4501630663871765
train gradient:  0.5854978734134381
iteration : 1078
train acc:  0.828125
train loss:  0.4252004027366638
train gradient:  0.5644820309577386
iteration : 1079
train acc:  0.734375
train loss:  0.5056198835372925
train gradient:  0.6507825134324817
iteration : 1080
train acc:  0.828125
train loss:  0.4068944454193115
train gradient:  0.37350837025085504
iteration : 1081
train acc:  0.7578125
train loss:  0.4786568284034729
train gradient:  0.5297099654776185
iteration : 1082
train acc:  0.8203125
train loss:  0.3583373725414276
train gradient:  0.4859035126956229
iteration : 1083
train acc:  0.828125
train loss:  0.4131240248680115
train gradient:  0.46051963825554243
iteration : 1084
train acc:  0.8203125
train loss:  0.42009875178337097
train gradient:  0.4396779006181622
iteration : 1085
train acc:  0.8515625
train loss:  0.45073145627975464
train gradient:  0.4125418095137553
iteration : 1086
train acc:  0.8046875
train loss:  0.4389365613460541
train gradient:  0.48263061414941655
iteration : 1087
train acc:  0.8125
train loss:  0.4190157353878021
train gradient:  0.4721706703000104
iteration : 1088
train acc:  0.765625
train loss:  0.4309210181236267
train gradient:  0.4964991162207063
iteration : 1089
train acc:  0.78125
train loss:  0.4177142381668091
train gradient:  0.5861063756927095
iteration : 1090
train acc:  0.828125
train loss:  0.40642040967941284
train gradient:  0.5141504960534373
iteration : 1091
train acc:  0.7578125
train loss:  0.5229412317276001
train gradient:  0.7232755870907175
iteration : 1092
train acc:  0.7734375
train loss:  0.4739646911621094
train gradient:  0.6525510494623016
iteration : 1093
train acc:  0.765625
train loss:  0.44909924268722534
train gradient:  0.7210822414361131
iteration : 1094
train acc:  0.75
train loss:  0.4584619998931885
train gradient:  0.4952616419599502
iteration : 1095
train acc:  0.796875
train loss:  0.3964223861694336
train gradient:  0.4480525256658897
iteration : 1096
train acc:  0.8046875
train loss:  0.41778960824012756
train gradient:  0.42176308768516857
iteration : 1097
train acc:  0.7578125
train loss:  0.46231335401535034
train gradient:  0.6019873507256233
iteration : 1098
train acc:  0.8125
train loss:  0.4325437545776367
train gradient:  0.46238968165481886
iteration : 1099
train acc:  0.8046875
train loss:  0.4100368022918701
train gradient:  0.44150169177909054
iteration : 1100
train acc:  0.796875
train loss:  0.4806043207645416
train gradient:  0.68264159330191
iteration : 1101
train acc:  0.71875
train loss:  0.5425223708152771
train gradient:  0.8269379886364138
iteration : 1102
train acc:  0.7734375
train loss:  0.44686681032180786
train gradient:  0.5633510134495889
iteration : 1103
train acc:  0.78125
train loss:  0.48478883504867554
train gradient:  0.6633756317847269
iteration : 1104
train acc:  0.859375
train loss:  0.3840702176094055
train gradient:  0.5231963058638737
iteration : 1105
train acc:  0.796875
train loss:  0.42100292444229126
train gradient:  0.5321016352803065
iteration : 1106
train acc:  0.75
train loss:  0.5217965841293335
train gradient:  0.729779365221058
iteration : 1107
train acc:  0.75
train loss:  0.4792870581150055
train gradient:  0.5745364749996152
iteration : 1108
train acc:  0.796875
train loss:  0.3878859281539917
train gradient:  0.4896544887869571
iteration : 1109
train acc:  0.7578125
train loss:  0.501610279083252
train gradient:  0.5732121485753745
iteration : 1110
train acc:  0.8515625
train loss:  0.3620954751968384
train gradient:  0.507729944652741
iteration : 1111
train acc:  0.8046875
train loss:  0.45748502016067505
train gradient:  0.5710440819717042
iteration : 1112
train acc:  0.7734375
train loss:  0.44618743658065796
train gradient:  0.531133642971708
iteration : 1113
train acc:  0.8046875
train loss:  0.42418932914733887
train gradient:  0.4404832848706761
iteration : 1114
train acc:  0.78125
train loss:  0.49189865589141846
train gradient:  0.688985702088905
iteration : 1115
train acc:  0.75
train loss:  0.5066319704055786
train gradient:  0.6226321833417051
iteration : 1116
train acc:  0.8359375
train loss:  0.38116687536239624
train gradient:  0.46709037244210627
iteration : 1117
train acc:  0.8125
train loss:  0.4420340061187744
train gradient:  0.5788483373310103
iteration : 1118
train acc:  0.765625
train loss:  0.5009703040122986
train gradient:  0.6680760565134634
iteration : 1119
train acc:  0.7578125
train loss:  0.4651299715042114
train gradient:  0.6675147973653455
iteration : 1120
train acc:  0.8046875
train loss:  0.4850959777832031
train gradient:  0.519364707095484
iteration : 1121
train acc:  0.8125
train loss:  0.44139763712882996
train gradient:  0.47275788774384847
iteration : 1122
train acc:  0.8125
train loss:  0.41869616508483887
train gradient:  0.42377484863027925
iteration : 1123
train acc:  0.8046875
train loss:  0.4371170401573181
train gradient:  0.7566005732619427
iteration : 1124
train acc:  0.765625
train loss:  0.4890669584274292
train gradient:  0.7751886170579618
iteration : 1125
train acc:  0.8125
train loss:  0.4132528603076935
train gradient:  0.7171617014708949
iteration : 1126
train acc:  0.8046875
train loss:  0.396677166223526
train gradient:  0.44131228645288484
iteration : 1127
train acc:  0.8125
train loss:  0.4065786600112915
train gradient:  0.4486960867416762
iteration : 1128
train acc:  0.8203125
train loss:  0.37223416566848755
train gradient:  0.40675696607125694
iteration : 1129
train acc:  0.8515625
train loss:  0.3887522220611572
train gradient:  0.6436439658697697
iteration : 1130
train acc:  0.875
train loss:  0.3947564363479614
train gradient:  0.5707243381784073
iteration : 1131
train acc:  0.734375
train loss:  0.4908258616924286
train gradient:  0.6055005861480475
iteration : 1132
train acc:  0.78125
train loss:  0.4459673762321472
train gradient:  0.48374438952943
iteration : 1133
train acc:  0.890625
train loss:  0.3341899514198303
train gradient:  0.3275508771800384
iteration : 1134
train acc:  0.859375
train loss:  0.3887675106525421
train gradient:  0.4533792255187698
iteration : 1135
train acc:  0.7890625
train loss:  0.3949481248855591
train gradient:  0.4406274619218561
iteration : 1136
train acc:  0.8046875
train loss:  0.4098241925239563
train gradient:  0.5284847880341927
iteration : 1137
train acc:  0.8046875
train loss:  0.4565664231777191
train gradient:  0.586622852308746
iteration : 1138
train acc:  0.7890625
train loss:  0.43050408363342285
train gradient:  0.47641944547677295
iteration : 1139
train acc:  0.7890625
train loss:  0.43594685196876526
train gradient:  0.5570754448916482
iteration : 1140
train acc:  0.7265625
train loss:  0.508579432964325
train gradient:  0.6584075858120282
iteration : 1141
train acc:  0.7578125
train loss:  0.45328766107559204
train gradient:  0.6117926393067727
iteration : 1142
train acc:  0.78125
train loss:  0.42793726921081543
train gradient:  0.4645247011907601
iteration : 1143
train acc:  0.78125
train loss:  0.43078577518463135
train gradient:  0.759201662237959
iteration : 1144
train acc:  0.84375
train loss:  0.4014940857887268
train gradient:  0.7278202117640207
iteration : 1145
train acc:  0.7421875
train loss:  0.4446668326854706
train gradient:  0.7014066973544413
iteration : 1146
train acc:  0.84375
train loss:  0.41274160146713257
train gradient:  0.47255161265849177
iteration : 1147
train acc:  0.8125
train loss:  0.389155775308609
train gradient:  0.44816831337460167
iteration : 1148
train acc:  0.8359375
train loss:  0.38248223066329956
train gradient:  0.4574440641127397
iteration : 1149
train acc:  0.765625
train loss:  0.48311102390289307
train gradient:  0.6554130500079342
iteration : 1150
train acc:  0.78125
train loss:  0.42901068925857544
train gradient:  0.49735738665231316
iteration : 1151
train acc:  0.8515625
train loss:  0.3540627360343933
train gradient:  0.36782199454905756
iteration : 1152
train acc:  0.765625
train loss:  0.47837114334106445
train gradient:  0.8431872688921545
iteration : 1153
train acc:  0.8203125
train loss:  0.4435080885887146
train gradient:  0.7154366201460849
iteration : 1154
train acc:  0.7890625
train loss:  0.4657418429851532
train gradient:  0.6522928184092139
iteration : 1155
train acc:  0.84375
train loss:  0.3817065954208374
train gradient:  0.48981307423991677
iteration : 1156
train acc:  0.7421875
train loss:  0.5048184394836426
train gradient:  0.655991338339196
iteration : 1157
train acc:  0.734375
train loss:  0.5306639671325684
train gradient:  0.7572577052366605
iteration : 1158
train acc:  0.7734375
train loss:  0.489995539188385
train gradient:  0.6383269628518604
iteration : 1159
train acc:  0.8046875
train loss:  0.46386468410491943
train gradient:  0.6410766118021367
iteration : 1160
train acc:  0.796875
train loss:  0.4202784299850464
train gradient:  0.4467404756534338
iteration : 1161
train acc:  0.84375
train loss:  0.3584129512310028
train gradient:  0.2964640042280627
iteration : 1162
train acc:  0.78125
train loss:  0.45385652780532837
train gradient:  0.6315109793692995
iteration : 1163
train acc:  0.7265625
train loss:  0.4930822551250458
train gradient:  0.5656613839048521
iteration : 1164
train acc:  0.828125
train loss:  0.4343368411064148
train gradient:  0.6279801560296983
iteration : 1165
train acc:  0.828125
train loss:  0.39605751633644104
train gradient:  0.3946538237162926
iteration : 1166
train acc:  0.75
train loss:  0.48269784450531006
train gradient:  0.600362961216687
iteration : 1167
train acc:  0.8125
train loss:  0.44203633069992065
train gradient:  0.5151674516534592
iteration : 1168
train acc:  0.734375
train loss:  0.5311003923416138
train gradient:  0.5555422218869102
iteration : 1169
train acc:  0.7890625
train loss:  0.4589502215385437
train gradient:  0.4881811661704118
iteration : 1170
train acc:  0.8125
train loss:  0.41692036390304565
train gradient:  0.4689230680112287
iteration : 1171
train acc:  0.75
train loss:  0.4931716322898865
train gradient:  0.685791780536505
iteration : 1172
train acc:  0.84375
train loss:  0.3576515316963196
train gradient:  0.36945446578714125
iteration : 1173
train acc:  0.75
train loss:  0.46737077832221985
train gradient:  0.5064661069403076
iteration : 1174
train acc:  0.8515625
train loss:  0.3716993033885956
train gradient:  0.5728825064677868
iteration : 1175
train acc:  0.859375
train loss:  0.35902902483940125
train gradient:  0.3418785828080264
iteration : 1176
train acc:  0.8125
train loss:  0.4175207018852234
train gradient:  0.640103944829975
iteration : 1177
train acc:  0.765625
train loss:  0.4519582986831665
train gradient:  0.6385450834913919
iteration : 1178
train acc:  0.7734375
train loss:  0.502949059009552
train gradient:  0.5968241331637584
iteration : 1179
train acc:  0.828125
train loss:  0.46526867151260376
train gradient:  0.6792685450542962
iteration : 1180
train acc:  0.8046875
train loss:  0.4182776212692261
train gradient:  0.45482668031685963
iteration : 1181
train acc:  0.8125
train loss:  0.3986395001411438
train gradient:  0.39783113250694696
iteration : 1182
train acc:  0.78125
train loss:  0.43197616934776306
train gradient:  0.6578268804229206
iteration : 1183
train acc:  0.8359375
train loss:  0.4440522789955139
train gradient:  0.5257170692439213
iteration : 1184
train acc:  0.78125
train loss:  0.4187908172607422
train gradient:  0.4873040178068931
iteration : 1185
train acc:  0.828125
train loss:  0.40170979499816895
train gradient:  0.48125913543368626
iteration : 1186
train acc:  0.828125
train loss:  0.40234631299972534
train gradient:  0.599667516438384
iteration : 1187
train acc:  0.8046875
train loss:  0.43137454986572266
train gradient:  0.5766873529748875
iteration : 1188
train acc:  0.7734375
train loss:  0.5040634870529175
train gradient:  0.8633991210858792
iteration : 1189
train acc:  0.8046875
train loss:  0.38696199655532837
train gradient:  0.43719284771515426
iteration : 1190
train acc:  0.75
train loss:  0.5180176496505737
train gradient:  0.6636036404954087
iteration : 1191
train acc:  0.796875
train loss:  0.44964128732681274
train gradient:  0.607536721859047
iteration : 1192
train acc:  0.7890625
train loss:  0.4304220676422119
train gradient:  0.48164581783979343
iteration : 1193
train acc:  0.7890625
train loss:  0.5008276700973511
train gradient:  0.6500304335288498
iteration : 1194
train acc:  0.796875
train loss:  0.42977842688560486
train gradient:  0.40553208749656466
iteration : 1195
train acc:  0.8046875
train loss:  0.393271803855896
train gradient:  0.613987773310394
iteration : 1196
train acc:  0.8125
train loss:  0.4188462793827057
train gradient:  0.4842306884842254
iteration : 1197
train acc:  0.7890625
train loss:  0.41578423976898193
train gradient:  0.540321023497875
iteration : 1198
train acc:  0.8203125
train loss:  0.44717133045196533
train gradient:  0.5467459042318648
iteration : 1199
train acc:  0.765625
train loss:  0.4710787236690521
train gradient:  0.8972797752283871
iteration : 1200
train acc:  0.828125
train loss:  0.39415621757507324
train gradient:  0.5175066839381702
iteration : 1201
train acc:  0.8046875
train loss:  0.42241033911705017
train gradient:  0.6256511115925613
iteration : 1202
train acc:  0.8515625
train loss:  0.35621631145477295
train gradient:  0.44549567201874213
iteration : 1203
train acc:  0.7734375
train loss:  0.5234670042991638
train gradient:  2.313635282609302
iteration : 1204
train acc:  0.8203125
train loss:  0.3934538960456848
train gradient:  0.5064838778786249
iteration : 1205
train acc:  0.6953125
train loss:  0.5324819087982178
train gradient:  0.9991856479461902
iteration : 1206
train acc:  0.765625
train loss:  0.43645453453063965
train gradient:  0.5628744531798426
iteration : 1207
train acc:  0.828125
train loss:  0.43135586380958557
train gradient:  0.42775867071823453
iteration : 1208
train acc:  0.7109375
train loss:  0.5531189441680908
train gradient:  0.6153779865177113
iteration : 1209
train acc:  0.796875
train loss:  0.4003204107284546
train gradient:  0.35026572334054634
iteration : 1210
train acc:  0.734375
train loss:  0.490184485912323
train gradient:  0.7379489473826848
iteration : 1211
train acc:  0.84375
train loss:  0.3779400885105133
train gradient:  0.47991062072132884
iteration : 1212
train acc:  0.796875
train loss:  0.4602140188217163
train gradient:  0.5162969141792275
iteration : 1213
train acc:  0.8046875
train loss:  0.44594234228134155
train gradient:  0.8372721091461617
iteration : 1214
train acc:  0.8046875
train loss:  0.43906164169311523
train gradient:  0.42132356974245533
iteration : 1215
train acc:  0.828125
train loss:  0.39155858755111694
train gradient:  0.5617716315549722
iteration : 1216
train acc:  0.8203125
train loss:  0.4028351902961731
train gradient:  0.4487614521232376
iteration : 1217
train acc:  0.7578125
train loss:  0.4708775281906128
train gradient:  0.4979352532566926
iteration : 1218
train acc:  0.75
train loss:  0.4654744267463684
train gradient:  0.7885457210101543
iteration : 1219
train acc:  0.7734375
train loss:  0.4701891839504242
train gradient:  0.6352921794160913
iteration : 1220
train acc:  0.8125
train loss:  0.37835511565208435
train gradient:  0.43773140505942204
iteration : 1221
train acc:  0.765625
train loss:  0.497367799282074
train gradient:  0.6098466931099819
iteration : 1222
train acc:  0.765625
train loss:  0.48781919479370117
train gradient:  0.5723072061569825
iteration : 1223
train acc:  0.7890625
train loss:  0.4028699994087219
train gradient:  0.4205820357304229
iteration : 1224
train acc:  0.8046875
train loss:  0.3950902223587036
train gradient:  0.3933337715353223
iteration : 1225
train acc:  0.8046875
train loss:  0.42823392152786255
train gradient:  0.5266600258694264
iteration : 1226
train acc:  0.7890625
train loss:  0.4435763359069824
train gradient:  0.44511538998994543
iteration : 1227
train acc:  0.8125
train loss:  0.4023772180080414
train gradient:  0.482376374788899
iteration : 1228
train acc:  0.8203125
train loss:  0.4048117995262146
train gradient:  0.4036736793388408
iteration : 1229
train acc:  0.8046875
train loss:  0.43266624212265015
train gradient:  0.5127396475365923
iteration : 1230
train acc:  0.8046875
train loss:  0.4490143656730652
train gradient:  0.9870529613783477
iteration : 1231
train acc:  0.7421875
train loss:  0.5069360733032227
train gradient:  0.6496611448640507
iteration : 1232
train acc:  0.8671875
train loss:  0.34886306524276733
train gradient:  0.41932897548338455
iteration : 1233
train acc:  0.8046875
train loss:  0.3994234800338745
train gradient:  0.3679390182764406
iteration : 1234
train acc:  0.828125
train loss:  0.3885875940322876
train gradient:  0.38507458015111007
iteration : 1235
train acc:  0.7734375
train loss:  0.4935136139392853
train gradient:  0.6519303209508853
iteration : 1236
train acc:  0.8046875
train loss:  0.4208221435546875
train gradient:  0.5678552868380967
iteration : 1237
train acc:  0.8359375
train loss:  0.39859268069267273
train gradient:  0.49658724410212585
iteration : 1238
train acc:  0.8359375
train loss:  0.3664513826370239
train gradient:  0.44546208808558985
iteration : 1239
train acc:  0.7421875
train loss:  0.5455726385116577
train gradient:  0.7382613734534476
iteration : 1240
train acc:  0.75
train loss:  0.46180617809295654
train gradient:  0.5714485200715205
iteration : 1241
train acc:  0.7890625
train loss:  0.43242043256759644
train gradient:  0.5868233627132117
iteration : 1242
train acc:  0.828125
train loss:  0.3808220624923706
train gradient:  0.40036674143892037
iteration : 1243
train acc:  0.7890625
train loss:  0.44345441460609436
train gradient:  0.5665388289703939
iteration : 1244
train acc:  0.7890625
train loss:  0.4364393949508667
train gradient:  0.4875031049688091
iteration : 1245
train acc:  0.765625
train loss:  0.5182292461395264
train gradient:  0.7179255774585722
iteration : 1246
train acc:  0.765625
train loss:  0.4188207983970642
train gradient:  0.5037105483910014
iteration : 1247
train acc:  0.8203125
train loss:  0.4278022050857544
train gradient:  0.45557068133109735
iteration : 1248
train acc:  0.75
train loss:  0.4803522825241089
train gradient:  0.4705235115302197
iteration : 1249
train acc:  0.7578125
train loss:  0.4726729989051819
train gradient:  0.4805649028445616
iteration : 1250
train acc:  0.8046875
train loss:  0.46745359897613525
train gradient:  0.5162283434874932
iteration : 1251
train acc:  0.8046875
train loss:  0.4386729598045349
train gradient:  0.5019126841201351
iteration : 1252
train acc:  0.7421875
train loss:  0.4636589586734772
train gradient:  0.4285380509799252
iteration : 1253
train acc:  0.78125
train loss:  0.4680967628955841
train gradient:  0.39247881327648104
iteration : 1254
train acc:  0.8515625
train loss:  0.36367523670196533
train gradient:  0.3027241732990628
iteration : 1255
train acc:  0.8671875
train loss:  0.3165479302406311
train gradient:  0.2936363152029255
iteration : 1256
train acc:  0.84375
train loss:  0.44019049406051636
train gradient:  0.4584864851977964
iteration : 1257
train acc:  0.7734375
train loss:  0.45085591077804565
train gradient:  0.5169915420510631
iteration : 1258
train acc:  0.84375
train loss:  0.35800838470458984
train gradient:  0.29745026015310266
iteration : 1259
train acc:  0.75
train loss:  0.4835042953491211
train gradient:  0.5062681461865226
iteration : 1260
train acc:  0.7890625
train loss:  0.42655983567237854
train gradient:  0.5126174532796689
iteration : 1261
train acc:  0.8125
train loss:  0.4094012379646301
train gradient:  0.5800141537501634
iteration : 1262
train acc:  0.734375
train loss:  0.4855538010597229
train gradient:  0.7611415495451257
iteration : 1263
train acc:  0.8046875
train loss:  0.37505844235420227
train gradient:  0.3235002102609385
iteration : 1264
train acc:  0.75
train loss:  0.5128417611122131
train gradient:  0.6006313017244366
iteration : 1265
train acc:  0.7890625
train loss:  0.4303858280181885
train gradient:  0.6346276206252089
iteration : 1266
train acc:  0.8828125
train loss:  0.3294309973716736
train gradient:  0.30310045532542523
iteration : 1267
train acc:  0.796875
train loss:  0.36946964263916016
train gradient:  0.35072979843288377
iteration : 1268
train acc:  0.765625
train loss:  0.4459584653377533
train gradient:  0.5190485777162749
iteration : 1269
train acc:  0.75
train loss:  0.5145339369773865
train gradient:  0.7379923441208809
iteration : 1270
train acc:  0.90625
train loss:  0.3243877589702606
train gradient:  0.3214379731392627
iteration : 1271
train acc:  0.8125
train loss:  0.3917364776134491
train gradient:  0.42024553755055505
iteration : 1272
train acc:  0.8359375
train loss:  0.3786467909812927
train gradient:  0.5249378135247321
iteration : 1273
train acc:  0.8203125
train loss:  0.37649622559547424
train gradient:  0.4704526949541221
iteration : 1274
train acc:  0.7890625
train loss:  0.4941386580467224
train gradient:  0.7749621996396071
iteration : 1275
train acc:  0.8203125
train loss:  0.43074387311935425
train gradient:  0.5311124567916468
iteration : 1276
train acc:  0.8515625
train loss:  0.3832811713218689
train gradient:  0.5914486975502699
iteration : 1277
train acc:  0.765625
train loss:  0.46904456615448
train gradient:  0.600644844305623
iteration : 1278
train acc:  0.8125
train loss:  0.4510419964790344
train gradient:  0.49219863483389475
iteration : 1279
train acc:  0.8359375
train loss:  0.407774955034256
train gradient:  0.4504534234325917
iteration : 1280
train acc:  0.8046875
train loss:  0.4453895390033722
train gradient:  0.4263865873690095
iteration : 1281
train acc:  0.8359375
train loss:  0.35419994592666626
train gradient:  0.46094654256875545
iteration : 1282
train acc:  0.8203125
train loss:  0.3652488589286804
train gradient:  0.41859817477001704
iteration : 1283
train acc:  0.765625
train loss:  0.44220253825187683
train gradient:  0.5100760875560775
iteration : 1284
train acc:  0.8046875
train loss:  0.44689488410949707
train gradient:  0.49733672329516726
iteration : 1285
train acc:  0.7890625
train loss:  0.4089762568473816
train gradient:  0.5582369507015253
iteration : 1286
train acc:  0.8046875
train loss:  0.45355671644210815
train gradient:  0.6772721012083625
iteration : 1287
train acc:  0.7890625
train loss:  0.4122248888015747
train gradient:  0.46615147343985625
iteration : 1288
train acc:  0.7578125
train loss:  0.5360268354415894
train gradient:  0.9604241021936553
iteration : 1289
train acc:  0.7734375
train loss:  0.4864976704120636
train gradient:  0.7254307652907033
iteration : 1290
train acc:  0.796875
train loss:  0.43019700050354004
train gradient:  0.38383097537662075
iteration : 1291
train acc:  0.8203125
train loss:  0.37716126441955566
train gradient:  0.46765047462990966
iteration : 1292
train acc:  0.8125
train loss:  0.4556172490119934
train gradient:  0.4447434418366066
iteration : 1293
train acc:  0.7421875
train loss:  0.453557550907135
train gradient:  0.6660364541819263
iteration : 1294
train acc:  0.8203125
train loss:  0.4191488027572632
train gradient:  0.4850671424740133
iteration : 1295
train acc:  0.7734375
train loss:  0.44231194257736206
train gradient:  0.5008557248109313
iteration : 1296
train acc:  0.84375
train loss:  0.33608174324035645
train gradient:  0.25408161025520887
iteration : 1297
train acc:  0.796875
train loss:  0.5156834721565247
train gradient:  0.7550215075398874
iteration : 1298
train acc:  0.875
train loss:  0.3777429759502411
train gradient:  0.4250061282982088
iteration : 1299
train acc:  0.765625
train loss:  0.41567134857177734
train gradient:  0.5623340547476334
iteration : 1300
train acc:  0.75
train loss:  0.4497492015361786
train gradient:  0.5508606389553596
iteration : 1301
train acc:  0.796875
train loss:  0.433954119682312
train gradient:  0.4442178541187699
iteration : 1302
train acc:  0.828125
train loss:  0.3805430829524994
train gradient:  0.4263622991042495
iteration : 1303
train acc:  0.796875
train loss:  0.40615853667259216
train gradient:  0.4087238006919576
iteration : 1304
train acc:  0.7734375
train loss:  0.41113710403442383
train gradient:  0.4029926388402931
iteration : 1305
train acc:  0.7890625
train loss:  0.5245747566223145
train gradient:  0.7736453032129327
iteration : 1306
train acc:  0.7890625
train loss:  0.4072644114494324
train gradient:  0.6368998889375748
iteration : 1307
train acc:  0.796875
train loss:  0.4328121542930603
train gradient:  0.3632181511786648
iteration : 1308
train acc:  0.875
train loss:  0.33593153953552246
train gradient:  0.35538420670574905
iteration : 1309
train acc:  0.7890625
train loss:  0.4771065413951874
train gradient:  0.7607392155304764
iteration : 1310
train acc:  0.8203125
train loss:  0.37430089712142944
train gradient:  0.4368376265621293
iteration : 1311
train acc:  0.796875
train loss:  0.4472351670265198
train gradient:  0.7129529247581047
iteration : 1312
train acc:  0.75
train loss:  0.5162379145622253
train gradient:  0.8795232377836983
iteration : 1313
train acc:  0.78125
train loss:  0.4417284429073334
train gradient:  0.4836321338791826
iteration : 1314
train acc:  0.828125
train loss:  0.4444030821323395
train gradient:  0.5392064168267726
iteration : 1315
train acc:  0.8359375
train loss:  0.3610374331474304
train gradient:  0.5923525825683765
iteration : 1316
train acc:  0.8125
train loss:  0.36638545989990234
train gradient:  0.47655051857440217
iteration : 1317
train acc:  0.8046875
train loss:  0.36428335309028625
train gradient:  0.4480088318482825
iteration : 1318
train acc:  0.796875
train loss:  0.4148441553115845
train gradient:  0.4605326694652252
iteration : 1319
train acc:  0.7890625
train loss:  0.46551039814949036
train gradient:  0.5801666937501035
iteration : 1320
train acc:  0.78125
train loss:  0.45476341247558594
train gradient:  0.5858731400561679
iteration : 1321
train acc:  0.8046875
train loss:  0.46734383702278137
train gradient:  0.7346322236156941
iteration : 1322
train acc:  0.7890625
train loss:  0.42326152324676514
train gradient:  0.4965451864815729
iteration : 1323
train acc:  0.828125
train loss:  0.4421883821487427
train gradient:  0.5570421157647822
iteration : 1324
train acc:  0.8203125
train loss:  0.45081305503845215
train gradient:  0.5512312732234228
iteration : 1325
train acc:  0.828125
train loss:  0.381052702665329
train gradient:  0.3429479490235523
iteration : 1326
train acc:  0.8046875
train loss:  0.4345027506351471
train gradient:  0.6143158218007578
iteration : 1327
train acc:  0.859375
train loss:  0.33208975195884705
train gradient:  0.49788613989843256
iteration : 1328
train acc:  0.8515625
train loss:  0.3667060136795044
train gradient:  0.40675614782190783
iteration : 1329
train acc:  0.765625
train loss:  0.43421947956085205
train gradient:  0.6326441816029093
iteration : 1330
train acc:  0.75
train loss:  0.4862792193889618
train gradient:  0.6361505455528758
iteration : 1331
train acc:  0.765625
train loss:  0.45631325244903564
train gradient:  0.8414133467266544
iteration : 1332
train acc:  0.75
train loss:  0.5105273723602295
train gradient:  0.6395608807905188
iteration : 1333
train acc:  0.796875
train loss:  0.45014625787734985
train gradient:  0.576998974813106
iteration : 1334
train acc:  0.828125
train loss:  0.45302629470825195
train gradient:  0.5882896568161998
iteration : 1335
train acc:  0.7890625
train loss:  0.4114495515823364
train gradient:  0.4108683764023238
iteration : 1336
train acc:  0.8203125
train loss:  0.42381957173347473
train gradient:  0.5480447874272565
iteration : 1337
train acc:  0.8046875
train loss:  0.4552856981754303
train gradient:  0.5697602108756608
iteration : 1338
train acc:  0.8125
train loss:  0.3897477090358734
train gradient:  0.42084417746697333
iteration : 1339
train acc:  0.8125
train loss:  0.3895508646965027
train gradient:  0.3540234470111574
iteration : 1340
train acc:  0.7734375
train loss:  0.4374481439590454
train gradient:  0.48906088322663294
iteration : 1341
train acc:  0.75
train loss:  0.5231658816337585
train gradient:  0.9501294908011487
iteration : 1342
train acc:  0.765625
train loss:  0.47423702478408813
train gradient:  0.5376019122897937
iteration : 1343
train acc:  0.7578125
train loss:  0.47052040696144104
train gradient:  0.5868718676009277
iteration : 1344
train acc:  0.7421875
train loss:  0.5356072187423706
train gradient:  0.8046099061239403
iteration : 1345
train acc:  0.71875
train loss:  0.5402405261993408
train gradient:  0.6479492002099869
iteration : 1346
train acc:  0.828125
train loss:  0.400053471326828
train gradient:  0.4299345785041019
iteration : 1347
train acc:  0.765625
train loss:  0.4983639717102051
train gradient:  0.6673855596615489
iteration : 1348
train acc:  0.78125
train loss:  0.42945805191993713
train gradient:  0.5150918085206625
iteration : 1349
train acc:  0.828125
train loss:  0.4320860505104065
train gradient:  0.5038799778776946
iteration : 1350
train acc:  0.7734375
train loss:  0.507152259349823
train gradient:  0.5692348213191091
iteration : 1351
train acc:  0.828125
train loss:  0.35969340801239014
train gradient:  0.3634328712343029
iteration : 1352
train acc:  0.8125
train loss:  0.4582621157169342
train gradient:  0.48395305082770207
iteration : 1353
train acc:  0.8125
train loss:  0.4060341715812683
train gradient:  0.6227443612791409
iteration : 1354
train acc:  0.78125
train loss:  0.45967140793800354
train gradient:  0.4527795422419587
iteration : 1355
train acc:  0.8359375
train loss:  0.4367801547050476
train gradient:  0.5113218899039229
iteration : 1356
train acc:  0.8125
train loss:  0.4008675515651703
train gradient:  0.3917206935471252
iteration : 1357
train acc:  0.78125
train loss:  0.44408637285232544
train gradient:  0.4329917613578844
iteration : 1358
train acc:  0.765625
train loss:  0.46066349744796753
train gradient:  0.5240539268199379
iteration : 1359
train acc:  0.84375
train loss:  0.3374728262424469
train gradient:  0.31554076655500213
iteration : 1360
train acc:  0.828125
train loss:  0.38850274682044983
train gradient:  0.3349345459303029
iteration : 1361
train acc:  0.75
train loss:  0.4663137197494507
train gradient:  0.5135447254124115
iteration : 1362
train acc:  0.7890625
train loss:  0.4043951630592346
train gradient:  0.45359470759774334
iteration : 1363
train acc:  0.8125
train loss:  0.38696837425231934
train gradient:  0.30838967519812654
iteration : 1364
train acc:  0.734375
train loss:  0.5184563398361206
train gradient:  0.5378894860580162
iteration : 1365
train acc:  0.84375
train loss:  0.44052451848983765
train gradient:  0.4466756006418193
iteration : 1366
train acc:  0.7734375
train loss:  0.45863208174705505
train gradient:  0.45615430766052484
iteration : 1367
train acc:  0.7890625
train loss:  0.4315086603164673
train gradient:  0.5000582331574284
iteration : 1368
train acc:  0.8515625
train loss:  0.37371641397476196
train gradient:  0.4509016704848485
iteration : 1369
train acc:  0.8125
train loss:  0.40405020117759705
train gradient:  0.3750919493534755
iteration : 1370
train acc:  0.8359375
train loss:  0.399046391248703
train gradient:  0.46503303963575526
iteration : 1371
train acc:  0.8046875
train loss:  0.3987700343132019
train gradient:  0.4000891344738663
iteration : 1372
train acc:  0.8046875
train loss:  0.485959529876709
train gradient:  0.6018797211719604
iteration : 1373
train acc:  0.7421875
train loss:  0.5009690523147583
train gradient:  0.7683896902814469
iteration : 1374
train acc:  0.859375
train loss:  0.36278069019317627
train gradient:  0.40225660647270856
iteration : 1375
train acc:  0.7265625
train loss:  0.5112435817718506
train gradient:  0.466153275274382
iteration : 1376
train acc:  0.765625
train loss:  0.47151559591293335
train gradient:  0.4635585789377347
iteration : 1377
train acc:  0.8203125
train loss:  0.3704273998737335
train gradient:  0.3739818835790145
iteration : 1378
train acc:  0.7578125
train loss:  0.49871525168418884
train gradient:  0.654100928904743
iteration : 1379
train acc:  0.7734375
train loss:  0.4593483805656433
train gradient:  0.5349825799487533
iteration : 1380
train acc:  0.7265625
train loss:  0.46309030055999756
train gradient:  0.4942927982013785
iteration : 1381
train acc:  0.8203125
train loss:  0.4086289405822754
train gradient:  0.46621514108910334
iteration : 1382
train acc:  0.828125
train loss:  0.4560142159461975
train gradient:  0.774525507933851
iteration : 1383
train acc:  0.765625
train loss:  0.47406888008117676
train gradient:  0.5443111449073422
iteration : 1384
train acc:  0.78125
train loss:  0.49459078907966614
train gradient:  0.5968234737874636
iteration : 1385
train acc:  0.7734375
train loss:  0.44020509719848633
train gradient:  0.49349304245758596
iteration : 1386
train acc:  0.7578125
train loss:  0.46078795194625854
train gradient:  0.4808984585527773
iteration : 1387
train acc:  0.7578125
train loss:  0.4741351306438446
train gradient:  0.5558760959523332
iteration : 1388
train acc:  0.7734375
train loss:  0.4611605405807495
train gradient:  0.5112069883579593
iteration : 1389
train acc:  0.8359375
train loss:  0.4264620840549469
train gradient:  0.5124297783632499
iteration : 1390
train acc:  0.765625
train loss:  0.4719076156616211
train gradient:  0.46805598952398925
iteration : 1391
train acc:  0.75
train loss:  0.5092769861221313
train gradient:  0.4651472505313725
iteration : 1392
train acc:  0.78125
train loss:  0.43157413601875305
train gradient:  0.4836413708158813
iteration : 1393
train acc:  0.8046875
train loss:  0.41433632373809814
train gradient:  0.3723005358820849
iteration : 1394
train acc:  0.8046875
train loss:  0.3846569061279297
train gradient:  0.4559333127165684
iteration : 1395
train acc:  0.8046875
train loss:  0.3887050747871399
train gradient:  0.39732747914261546
iteration : 1396
train acc:  0.7734375
train loss:  0.4463489055633545
train gradient:  0.47961701488336117
iteration : 1397
train acc:  0.84375
train loss:  0.36249667406082153
train gradient:  0.33914104786284405
iteration : 1398
train acc:  0.8046875
train loss:  0.43786248564720154
train gradient:  0.48161852750042883
iteration : 1399
train acc:  0.75
train loss:  0.4694843292236328
train gradient:  0.4419835732656088
iteration : 1400
train acc:  0.8515625
train loss:  0.39550358057022095
train gradient:  0.4432413366153805
iteration : 1401
train acc:  0.765625
train loss:  0.43202826380729675
train gradient:  0.4546269939332355
iteration : 1402
train acc:  0.796875
train loss:  0.42159175872802734
train gradient:  0.5037740471536424
iteration : 1403
train acc:  0.7734375
train loss:  0.467521607875824
train gradient:  0.47417458460296263
iteration : 1404
train acc:  0.796875
train loss:  0.45073288679122925
train gradient:  0.48469811160840076
iteration : 1405
train acc:  0.8203125
train loss:  0.3762783706188202
train gradient:  0.3458148481004398
iteration : 1406
train acc:  0.8203125
train loss:  0.3834630846977234
train gradient:  0.5400176733571644
iteration : 1407
train acc:  0.78125
train loss:  0.43327784538269043
train gradient:  0.4335173379252688
iteration : 1408
train acc:  0.78125
train loss:  0.4793850779533386
train gradient:  0.48227598462965005
iteration : 1409
train acc:  0.8203125
train loss:  0.33345305919647217
train gradient:  0.2927577802341233
iteration : 1410
train acc:  0.796875
train loss:  0.4372767210006714
train gradient:  0.6296555396843861
iteration : 1411
train acc:  0.84375
train loss:  0.4048842191696167
train gradient:  0.39654726883747937
iteration : 1412
train acc:  0.78125
train loss:  0.45838940143585205
train gradient:  0.4055088520161911
iteration : 1413
train acc:  0.75
train loss:  0.4995104670524597
train gradient:  0.6349209288658112
iteration : 1414
train acc:  0.796875
train loss:  0.45848649740219116
train gradient:  0.38580628904493686
iteration : 1415
train acc:  0.7734375
train loss:  0.413169264793396
train gradient:  0.3975202627489476
iteration : 1416
train acc:  0.78125
train loss:  0.42280149459838867
train gradient:  0.44551024297012587
iteration : 1417
train acc:  0.8046875
train loss:  0.41931602358818054
train gradient:  0.4491449040668677
iteration : 1418
train acc:  0.828125
train loss:  0.3729243874549866
train gradient:  0.3796939746013304
iteration : 1419
train acc:  0.828125
train loss:  0.41065269708633423
train gradient:  0.3668753128845611
iteration : 1420
train acc:  0.828125
train loss:  0.3424132466316223
train gradient:  0.31045526421257674
iteration : 1421
train acc:  0.8125
train loss:  0.3864845633506775
train gradient:  0.38147299286328773
iteration : 1422
train acc:  0.7734375
train loss:  0.49898141622543335
train gradient:  0.6297747523104205
iteration : 1423
train acc:  0.796875
train loss:  0.40339237451553345
train gradient:  0.3268471525532376
iteration : 1424
train acc:  0.7734375
train loss:  0.5177770256996155
train gradient:  0.6817826669948188
iteration : 1425
train acc:  0.875
train loss:  0.3561452031135559
train gradient:  0.386142502225343
iteration : 1426
train acc:  0.8359375
train loss:  0.3794492483139038
train gradient:  0.31161512109456607
iteration : 1427
train acc:  0.8046875
train loss:  0.42024075984954834
train gradient:  0.3437349803850893
iteration : 1428
train acc:  0.8203125
train loss:  0.4069414734840393
train gradient:  0.614398858711962
iteration : 1429
train acc:  0.796875
train loss:  0.46006327867507935
train gradient:  0.5331921184337521
iteration : 1430
train acc:  0.8515625
train loss:  0.3710750937461853
train gradient:  0.4713267880069591
iteration : 1431
train acc:  0.734375
train loss:  0.47461116313934326
train gradient:  0.5772162823321818
iteration : 1432
train acc:  0.765625
train loss:  0.4519888460636139
train gradient:  0.43930932192972344
iteration : 1433
train acc:  0.859375
train loss:  0.35484778881073
train gradient:  0.3262651436574692
iteration : 1434
train acc:  0.7890625
train loss:  0.4834407567977905
train gradient:  0.6455639498126424
iteration : 1435
train acc:  0.75
train loss:  0.5068941116333008
train gradient:  0.7005029635653973
iteration : 1436
train acc:  0.7578125
train loss:  0.4529334604740143
train gradient:  0.4587695436519584
iteration : 1437
train acc:  0.765625
train loss:  0.46820634603500366
train gradient:  0.6606758984863808
iteration : 1438
train acc:  0.8359375
train loss:  0.3591817617416382
train gradient:  0.3348415908184684
iteration : 1439
train acc:  0.8203125
train loss:  0.3505924642086029
train gradient:  0.3478820050911071
iteration : 1440
train acc:  0.8359375
train loss:  0.3959563374519348
train gradient:  0.48557089878361975
iteration : 1441
train acc:  0.7890625
train loss:  0.3959704637527466
train gradient:  0.3006557785575879
iteration : 1442
train acc:  0.78125
train loss:  0.42142075300216675
train gradient:  0.3919008904838509
iteration : 1443
train acc:  0.8046875
train loss:  0.4965273141860962
train gradient:  0.6033155663382781
iteration : 1444
train acc:  0.8359375
train loss:  0.33068180084228516
train gradient:  0.314838151801529
iteration : 1445
train acc:  0.765625
train loss:  0.43328362703323364
train gradient:  0.4769001645562374
iteration : 1446
train acc:  0.7890625
train loss:  0.41734686493873596
train gradient:  0.44410714516031313
iteration : 1447
train acc:  0.8125
train loss:  0.43713897466659546
train gradient:  0.5700467411442761
iteration : 1448
train acc:  0.8046875
train loss:  0.45135417580604553
train gradient:  0.5012298913615246
iteration : 1449
train acc:  0.8203125
train loss:  0.40305596590042114
train gradient:  0.44323047054163245
iteration : 1450
train acc:  0.765625
train loss:  0.5044957399368286
train gradient:  0.8032737074468936
iteration : 1451
train acc:  0.8515625
train loss:  0.4283819794654846
train gradient:  0.5965845587763612
iteration : 1452
train acc:  0.765625
train loss:  0.4398849606513977
train gradient:  0.45831058795974605
iteration : 1453
train acc:  0.828125
train loss:  0.43081241846084595
train gradient:  0.4459429219201458
iteration : 1454
train acc:  0.828125
train loss:  0.4531300663948059
train gradient:  0.5807894535812779
iteration : 1455
train acc:  0.8125
train loss:  0.45022523403167725
train gradient:  0.6175327703102111
iteration : 1456
train acc:  0.7734375
train loss:  0.490246057510376
train gradient:  0.5964058021485434
iteration : 1457
train acc:  0.84375
train loss:  0.3694518208503723
train gradient:  0.4347021307668305
iteration : 1458
train acc:  0.8046875
train loss:  0.4573342800140381
train gradient:  0.5152211806447293
iteration : 1459
train acc:  0.7578125
train loss:  0.5845996737480164
train gradient:  0.8628622816564403
iteration : 1460
train acc:  0.8359375
train loss:  0.4122929573059082
train gradient:  0.4893522849487009
iteration : 1461
train acc:  0.7578125
train loss:  0.44724294543266296
train gradient:  0.623858521887428
iteration : 1462
train acc:  0.8671875
train loss:  0.36598536372184753
train gradient:  0.30689184727334917
iteration : 1463
train acc:  0.765625
train loss:  0.5155370235443115
train gradient:  0.706940542610214
iteration : 1464
train acc:  0.7890625
train loss:  0.40793508291244507
train gradient:  0.3730254780421947
iteration : 1465
train acc:  0.796875
train loss:  0.45929867029190063
train gradient:  0.559924530046003
iteration : 1466
train acc:  0.765625
train loss:  0.46166175603866577
train gradient:  0.40349857660238814
iteration : 1467
train acc:  0.7421875
train loss:  0.45820164680480957
train gradient:  0.5295659251567146
iteration : 1468
train acc:  0.859375
train loss:  0.39522361755371094
train gradient:  0.4027882511135625
iteration : 1469
train acc:  0.796875
train loss:  0.4063829183578491
train gradient:  0.2999678597713472
iteration : 1470
train acc:  0.7890625
train loss:  0.4595777094364166
train gradient:  0.5173811668112683
iteration : 1471
train acc:  0.796875
train loss:  0.41595372557640076
train gradient:  0.37783334815814973
iteration : 1472
train acc:  0.8125
train loss:  0.42464154958724976
train gradient:  0.5084842177200576
iteration : 1473
train acc:  0.8828125
train loss:  0.3541242480278015
train gradient:  0.2758300252636062
iteration : 1474
train acc:  0.8046875
train loss:  0.42695051431655884
train gradient:  0.3900486272261571
iteration : 1475
train acc:  0.7890625
train loss:  0.48361775279045105
train gradient:  0.4693988505817176
iteration : 1476
train acc:  0.828125
train loss:  0.36602461338043213
train gradient:  0.4258062484158081
iteration : 1477
train acc:  0.7734375
train loss:  0.46120190620422363
train gradient:  0.474099256680109
iteration : 1478
train acc:  0.8125
train loss:  0.40829527378082275
train gradient:  0.40350825793091416
iteration : 1479
train acc:  0.7578125
train loss:  0.43579739332199097
train gradient:  0.38479497690794956
iteration : 1480
train acc:  0.796875
train loss:  0.42386382818222046
train gradient:  0.33501357410614885
iteration : 1481
train acc:  0.828125
train loss:  0.43723493814468384
train gradient:  0.3859996092852132
iteration : 1482
train acc:  0.7421875
train loss:  0.519096851348877
train gradient:  0.8401898628245392
iteration : 1483
train acc:  0.8203125
train loss:  0.37214452028274536
train gradient:  0.37291170293058246
iteration : 1484
train acc:  0.8125
train loss:  0.36142051219940186
train gradient:  0.43848142981016974
iteration : 1485
train acc:  0.84375
train loss:  0.37901735305786133
train gradient:  0.32817167135468134
iteration : 1486
train acc:  0.8671875
train loss:  0.3024158477783203
train gradient:  0.23769995268739694
iteration : 1487
train acc:  0.78125
train loss:  0.4827248454093933
train gradient:  0.47998452863635266
iteration : 1488
train acc:  0.7890625
train loss:  0.40493011474609375
train gradient:  0.48694326327471427
iteration : 1489
train acc:  0.8046875
train loss:  0.4690876603126526
train gradient:  0.421237769065085
iteration : 1490
train acc:  0.7734375
train loss:  0.40128862857818604
train gradient:  0.339198452592515
iteration : 1491
train acc:  0.7734375
train loss:  0.4355723559856415
train gradient:  0.40937059553597316
iteration : 1492
train acc:  0.8125
train loss:  0.4338483214378357
train gradient:  0.5081996428987565
iteration : 1493
train acc:  0.75
train loss:  0.4344269037246704
train gradient:  0.4042500747542042
iteration : 1494
train acc:  0.8515625
train loss:  0.3905978202819824
train gradient:  0.47369752049195046
iteration : 1495
train acc:  0.7734375
train loss:  0.41600126028060913
train gradient:  0.4769809389319226
iteration : 1496
train acc:  0.7734375
train loss:  0.44477957487106323
train gradient:  0.5622595801849262
iteration : 1497
train acc:  0.796875
train loss:  0.48804256319999695
train gradient:  0.806402956828546
iteration : 1498
train acc:  0.8125
train loss:  0.44070494174957275
train gradient:  0.5041649569545219
iteration : 1499
train acc:  0.75
train loss:  0.5202550888061523
train gradient:  0.5576569780169347
iteration : 1500
train acc:  0.8125
train loss:  0.40373289585113525
train gradient:  0.47304206825236295
iteration : 1501
train acc:  0.796875
train loss:  0.43789124488830566
train gradient:  0.40053917664204886
iteration : 1502
train acc:  0.890625
train loss:  0.2831762731075287
train gradient:  0.2601627597154334
iteration : 1503
train acc:  0.828125
train loss:  0.3898541331291199
train gradient:  0.4430866371930127
iteration : 1504
train acc:  0.796875
train loss:  0.397882342338562
train gradient:  0.3751199407628558
iteration : 1505
train acc:  0.78125
train loss:  0.4845370650291443
train gradient:  0.6240133785241071
iteration : 1506
train acc:  0.7890625
train loss:  0.45099836587905884
train gradient:  0.46571716451107614
iteration : 1507
train acc:  0.828125
train loss:  0.4269369840621948
train gradient:  0.46147460959875897
iteration : 1508
train acc:  0.8203125
train loss:  0.4216252565383911
train gradient:  0.41307988055878
iteration : 1509
train acc:  0.8203125
train loss:  0.4142456650733948
train gradient:  0.5990957356206204
iteration : 1510
train acc:  0.859375
train loss:  0.34697848558425903
train gradient:  0.6156348225793713
iteration : 1511
train acc:  0.8203125
train loss:  0.34950366616249084
train gradient:  0.3601306663261899
iteration : 1512
train acc:  0.7734375
train loss:  0.430730402469635
train gradient:  0.45478048022932754
iteration : 1513
train acc:  0.796875
train loss:  0.4684697389602661
train gradient:  0.5481157659960308
iteration : 1514
train acc:  0.8125
train loss:  0.4304655194282532
train gradient:  0.4514301671491807
iteration : 1515
train acc:  0.828125
train loss:  0.371026873588562
train gradient:  0.3146322530689097
iteration : 1516
train acc:  0.8125
train loss:  0.39454638957977295
train gradient:  0.41182217317263775
iteration : 1517
train acc:  0.796875
train loss:  0.3965998888015747
train gradient:  0.4141385523739086
iteration : 1518
train acc:  0.84375
train loss:  0.3392795920372009
train gradient:  0.35270288294204805
iteration : 1519
train acc:  0.7265625
train loss:  0.5105587840080261
train gradient:  0.7662281155037924
iteration : 1520
train acc:  0.8515625
train loss:  0.4073716998100281
train gradient:  0.4979661450122883
iteration : 1521
train acc:  0.796875
train loss:  0.43960216641426086
train gradient:  0.604600979687704
iteration : 1522
train acc:  0.8359375
train loss:  0.4233076572418213
train gradient:  0.5184831536607919
iteration : 1523
train acc:  0.7421875
train loss:  0.4913440942764282
train gradient:  0.8498812633629442
iteration : 1524
train acc:  0.78125
train loss:  0.42705199122428894
train gradient:  0.49996288210644474
iteration : 1525
train acc:  0.78125
train loss:  0.4261687994003296
train gradient:  0.4975842816128265
iteration : 1526
train acc:  0.8046875
train loss:  0.4528965651988983
train gradient:  0.6663228504771972
iteration : 1527
train acc:  0.8515625
train loss:  0.3403368592262268
train gradient:  0.4525637461553331
iteration : 1528
train acc:  0.78125
train loss:  0.4197521209716797
train gradient:  0.48900398444726745
iteration : 1529
train acc:  0.8203125
train loss:  0.40405553579330444
train gradient:  0.5006249123465197
iteration : 1530
train acc:  0.7890625
train loss:  0.3830528259277344
train gradient:  0.3732456464278267
iteration : 1531
train acc:  0.8125
train loss:  0.42645537853240967
train gradient:  0.4775995428788417
iteration : 1532
train acc:  0.8359375
train loss:  0.35377898812294006
train gradient:  0.4474578168749029
iteration : 1533
train acc:  0.7890625
train loss:  0.4408203959465027
train gradient:  0.6433020316077132
iteration : 1534
train acc:  0.7421875
train loss:  0.5177529454231262
train gradient:  0.7883817398992234
iteration : 1535
train acc:  0.8125
train loss:  0.3780309855937958
train gradient:  0.43482205509942345
iteration : 1536
train acc:  0.8203125
train loss:  0.4238334000110626
train gradient:  0.41095200326193654
iteration : 1537
train acc:  0.7578125
train loss:  0.4820590019226074
train gradient:  0.6280287458205251
iteration : 1538
train acc:  0.796875
train loss:  0.4179758131504059
train gradient:  0.5481773759716634
iteration : 1539
train acc:  0.859375
train loss:  0.3459699749946594
train gradient:  0.37597197470512167
iteration : 1540
train acc:  0.8515625
train loss:  0.3599933981895447
train gradient:  0.34207278402243774
iteration : 1541
train acc:  0.8125
train loss:  0.4686940908432007
train gradient:  0.6049001394519086
iteration : 1542
train acc:  0.796875
train loss:  0.4344126582145691
train gradient:  0.4556218331563236
iteration : 1543
train acc:  0.7734375
train loss:  0.4654995799064636
train gradient:  0.7702351901024707
iteration : 1544
train acc:  0.828125
train loss:  0.4003690481185913
train gradient:  0.5186269282345726
iteration : 1545
train acc:  0.796875
train loss:  0.3845933973789215
train gradient:  0.45886779306898606
iteration : 1546
train acc:  0.828125
train loss:  0.39361509680747986
train gradient:  0.5713983114840402
iteration : 1547
train acc:  0.7890625
train loss:  0.4640711545944214
train gradient:  0.6873179742013645
iteration : 1548
train acc:  0.734375
train loss:  0.5159337520599365
train gradient:  0.7662871228091673
iteration : 1549
train acc:  0.765625
train loss:  0.4600825905799866
train gradient:  0.5449267242585177
iteration : 1550
train acc:  0.8984375
train loss:  0.3222655951976776
train gradient:  0.30091548063570767
iteration : 1551
train acc:  0.8203125
train loss:  0.3616463840007782
train gradient:  0.48766389839892427
iteration : 1552
train acc:  0.8515625
train loss:  0.3635711967945099
train gradient:  0.42804682926147725
iteration : 1553
train acc:  0.8125
train loss:  0.4283409118652344
train gradient:  0.44675472374351954
iteration : 1554
train acc:  0.71875
train loss:  0.4883936941623688
train gradient:  0.638737545122416
iteration : 1555
train acc:  0.7421875
train loss:  0.5239467620849609
train gradient:  0.7318798846751702
iteration : 1556
train acc:  0.8046875
train loss:  0.4580575227737427
train gradient:  0.4809040399675225
iteration : 1557
train acc:  0.8671875
train loss:  0.3882630169391632
train gradient:  0.47930320323628295
iteration : 1558
train acc:  0.8359375
train loss:  0.37346941232681274
train gradient:  0.35811676689032446
iteration : 1559
train acc:  0.765625
train loss:  0.4376991391181946
train gradient:  0.5472550769465269
iteration : 1560
train acc:  0.890625
train loss:  0.3621346950531006
train gradient:  0.7093466933945319
iteration : 1561
train acc:  0.78125
train loss:  0.4429270029067993
train gradient:  0.45423643710410433
iteration : 1562
train acc:  0.796875
train loss:  0.5147923827171326
train gradient:  0.9049492978559175
iteration : 1563
train acc:  0.8046875
train loss:  0.4023730158805847
train gradient:  0.6615759307363452
iteration : 1564
train acc:  0.7578125
train loss:  0.4638194143772125
train gradient:  0.5578425276141638
iteration : 1565
train acc:  0.7890625
train loss:  0.4291198253631592
train gradient:  0.45036853685588785
iteration : 1566
train acc:  0.8359375
train loss:  0.31713083386421204
train gradient:  0.3297364428602068
iteration : 1567
train acc:  0.8515625
train loss:  0.39581120014190674
train gradient:  0.3538063217290117
iteration : 1568
train acc:  0.8203125
train loss:  0.3797098696231842
train gradient:  0.4923602004121195
iteration : 1569
train acc:  0.8828125
train loss:  0.3262574076652527
train gradient:  0.36166047407974256
iteration : 1570
train acc:  0.828125
train loss:  0.4216293692588806
train gradient:  0.5775546242190546
iteration : 1571
train acc:  0.8203125
train loss:  0.40357571840286255
train gradient:  0.5057552493151565
iteration : 1572
train acc:  0.8046875
train loss:  0.3801113963127136
train gradient:  0.5325577139804406
iteration : 1573
train acc:  0.8359375
train loss:  0.3709544539451599
train gradient:  0.5347293678824742
iteration : 1574
train acc:  0.8515625
train loss:  0.36752939224243164
train gradient:  0.675351942428357
iteration : 1575
train acc:  0.8046875
train loss:  0.43159037828445435
train gradient:  0.6203959959901901
iteration : 1576
train acc:  0.8203125
train loss:  0.38518446683883667
train gradient:  0.4299372158859985
iteration : 1577
train acc:  0.828125
train loss:  0.36384162306785583
train gradient:  0.36238041064373383
iteration : 1578
train acc:  0.8046875
train loss:  0.3884040117263794
train gradient:  0.41362712776790556
iteration : 1579
train acc:  0.8203125
train loss:  0.3862032890319824
train gradient:  0.4319352398392674
iteration : 1580
train acc:  0.84375
train loss:  0.3360636830329895
train gradient:  0.4224894949453375
iteration : 1581
train acc:  0.890625
train loss:  0.3192543387413025
train gradient:  0.3413995995255777
iteration : 1582
train acc:  0.8046875
train loss:  0.38887637853622437
train gradient:  0.34048393743591415
iteration : 1583
train acc:  0.796875
train loss:  0.41767480969429016
train gradient:  0.4057462382059093
iteration : 1584
train acc:  0.7734375
train loss:  0.46810048818588257
train gradient:  0.6432527764567575
iteration : 1585
train acc:  0.78125
train loss:  0.4969397187232971
train gradient:  0.7553368115208957
iteration : 1586
train acc:  0.84375
train loss:  0.4199918210506439
train gradient:  0.6458281965415036
iteration : 1587
train acc:  0.8359375
train loss:  0.34220442175865173
train gradient:  0.32594136229253196
iteration : 1588
train acc:  0.8671875
train loss:  0.320803165435791
train gradient:  0.382645301825262
iteration : 1589
train acc:  0.8125
train loss:  0.3699025511741638
train gradient:  0.35454257091337316
iteration : 1590
train acc:  0.8046875
train loss:  0.4433138966560364
train gradient:  0.7470827402229897
iteration : 1591
train acc:  0.78125
train loss:  0.4976387619972229
train gradient:  0.5373470073599542
iteration : 1592
train acc:  0.8671875
train loss:  0.3084927499294281
train gradient:  0.2710667052385441
iteration : 1593
train acc:  0.8203125
train loss:  0.4349249005317688
train gradient:  0.5716206580795322
iteration : 1594
train acc:  0.8125
train loss:  0.3870612382888794
train gradient:  0.3778776979070952
iteration : 1595
train acc:  0.796875
train loss:  0.4321408271789551
train gradient:  0.4855371890523313
iteration : 1596
train acc:  0.8671875
train loss:  0.3208314776420593
train gradient:  0.39962230773740737
iteration : 1597
train acc:  0.8046875
train loss:  0.43256062269210815
train gradient:  0.7270461353064719
iteration : 1598
train acc:  0.6796875
train loss:  0.5285446643829346
train gradient:  0.64697427831981
iteration : 1599
train acc:  0.7890625
train loss:  0.3777129650115967
train gradient:  0.47067759297430395
iteration : 1600
train acc:  0.8046875
train loss:  0.3973239064216614
train gradient:  0.41839583149084864
iteration : 1601
train acc:  0.8046875
train loss:  0.4178368151187897
train gradient:  0.45645312199427635
iteration : 1602
train acc:  0.84375
train loss:  0.4218994975090027
train gradient:  0.5041222407831889
iteration : 1603
train acc:  0.8359375
train loss:  0.3602418303489685
train gradient:  0.3155301905370659
iteration : 1604
train acc:  0.8046875
train loss:  0.3742416501045227
train gradient:  0.4482873899713097
iteration : 1605
train acc:  0.8125
train loss:  0.4042738676071167
train gradient:  0.34254562812948347
iteration : 1606
train acc:  0.8203125
train loss:  0.37015199661254883
train gradient:  0.3623269387688461
iteration : 1607
train acc:  0.78125
train loss:  0.41530781984329224
train gradient:  0.3456246593905567
iteration : 1608
train acc:  0.8125
train loss:  0.4029143452644348
train gradient:  0.6600829948681052
iteration : 1609
train acc:  0.765625
train loss:  0.48017793893814087
train gradient:  0.5698801648347597
iteration : 1610
train acc:  0.8046875
train loss:  0.3819811940193176
train gradient:  0.6026759262190573
iteration : 1611
train acc:  0.8359375
train loss:  0.3655655086040497
train gradient:  0.32466688437167673
iteration : 1612
train acc:  0.8125
train loss:  0.3577845096588135
train gradient:  0.3975895071499234
iteration : 1613
train acc:  0.7578125
train loss:  0.527483344078064
train gradient:  0.8780408329731478
iteration : 1614
train acc:  0.8515625
train loss:  0.396298348903656
train gradient:  0.3984601197192851
iteration : 1615
train acc:  0.8359375
train loss:  0.39526933431625366
train gradient:  0.5319673812050396
iteration : 1616
train acc:  0.765625
train loss:  0.48833397030830383
train gradient:  0.5583574664022843
iteration : 1617
train acc:  0.7734375
train loss:  0.494601309299469
train gradient:  0.6586379534669937
iteration : 1618
train acc:  0.875
train loss:  0.3186684548854828
train gradient:  0.23310711727275132
iteration : 1619
train acc:  0.8046875
train loss:  0.4787914454936981
train gradient:  0.5619710225052471
iteration : 1620
train acc:  0.78125
train loss:  0.46605953574180603
train gradient:  0.5177760230452747
iteration : 1621
train acc:  0.8359375
train loss:  0.4066046476364136
train gradient:  0.48798133080709166
iteration : 1622
train acc:  0.828125
train loss:  0.40004444122314453
train gradient:  0.4318888803366689
iteration : 1623
train acc:  0.7734375
train loss:  0.48080503940582275
train gradient:  0.7649328706948189
iteration : 1624
train acc:  0.8671875
train loss:  0.33990591764450073
train gradient:  0.33238599400906194
iteration : 1625
train acc:  0.796875
train loss:  0.4195924997329712
train gradient:  0.3367248009760357
iteration : 1626
train acc:  0.7890625
train loss:  0.4142245948314667
train gradient:  0.47427114226639555
iteration : 1627
train acc:  0.8515625
train loss:  0.40734249353408813
train gradient:  0.40569936809823515
iteration : 1628
train acc:  0.78125
train loss:  0.49009349942207336
train gradient:  0.5545618013315241
iteration : 1629
train acc:  0.765625
train loss:  0.4426475763320923
train gradient:  0.3591968632783998
iteration : 1630
train acc:  0.8359375
train loss:  0.42101719975471497
train gradient:  0.4043334582191972
iteration : 1631
train acc:  0.8125
train loss:  0.3860095739364624
train gradient:  0.3697031440802587
iteration : 1632
train acc:  0.84375
train loss:  0.3581785261631012
train gradient:  0.4262563693987307
iteration : 1633
train acc:  0.84375
train loss:  0.3810489773750305
train gradient:  0.4022288553362662
iteration : 1634
train acc:  0.796875
train loss:  0.470784068107605
train gradient:  0.3879637145432312
iteration : 1635
train acc:  0.8203125
train loss:  0.3471013903617859
train gradient:  0.2667299054513281
iteration : 1636
train acc:  0.75
train loss:  0.4628070294857025
train gradient:  0.5223779547585043
iteration : 1637
train acc:  0.828125
train loss:  0.43752413988113403
train gradient:  0.45470977768088816
iteration : 1638
train acc:  0.8515625
train loss:  0.36271071434020996
train gradient:  0.37524506866707624
iteration : 1639
train acc:  0.75
train loss:  0.4193262457847595
train gradient:  0.3640336677065444
iteration : 1640
train acc:  0.7578125
train loss:  0.4629095792770386
train gradient:  0.6919994411584989
iteration : 1641
train acc:  0.75
train loss:  0.5159847736358643
train gradient:  0.7792727267154668
iteration : 1642
train acc:  0.8046875
train loss:  0.4576486647129059
train gradient:  0.5467694609127933
iteration : 1643
train acc:  0.828125
train loss:  0.373444139957428
train gradient:  0.40727445387568967
iteration : 1644
train acc:  0.8046875
train loss:  0.40474796295166016
train gradient:  0.38802024365694265
iteration : 1645
train acc:  0.8046875
train loss:  0.4808887839317322
train gradient:  0.5807943544701485
iteration : 1646
train acc:  0.8125
train loss:  0.40964066982269287
train gradient:  0.40787075826555513
iteration : 1647
train acc:  0.828125
train loss:  0.43123987317085266
train gradient:  0.5356578622162578
iteration : 1648
train acc:  0.796875
train loss:  0.4052412509918213
train gradient:  0.4222267643365294
iteration : 1649
train acc:  0.8359375
train loss:  0.4438575804233551
train gradient:  0.4729147029335086
iteration : 1650
train acc:  0.7890625
train loss:  0.4433205723762512
train gradient:  0.5727054601288443
iteration : 1651
train acc:  0.796875
train loss:  0.4344509243965149
train gradient:  0.4484465487163812
iteration : 1652
train acc:  0.8359375
train loss:  0.37697333097457886
train gradient:  0.42779304024225334
iteration : 1653
train acc:  0.8359375
train loss:  0.3522132337093353
train gradient:  0.2740880726512585
iteration : 1654
train acc:  0.7265625
train loss:  0.4896186292171478
train gradient:  0.5967130522357994
iteration : 1655
train acc:  0.7734375
train loss:  0.45539480447769165
train gradient:  0.5056335906060163
iteration : 1656
train acc:  0.8125
train loss:  0.4040503203868866
train gradient:  0.40204614323121746
iteration : 1657
train acc:  0.84375
train loss:  0.3848941922187805
train gradient:  0.40296478137307673
iteration : 1658
train acc:  0.8671875
train loss:  0.34079188108444214
train gradient:  0.3266208505100781
iteration : 1659
train acc:  0.828125
train loss:  0.3754866123199463
train gradient:  0.41057220171692727
iteration : 1660
train acc:  0.75
train loss:  0.4741067886352539
train gradient:  0.5042117481706186
iteration : 1661
train acc:  0.7890625
train loss:  0.5006729364395142
train gradient:  0.7656104753375335
iteration : 1662
train acc:  0.78125
train loss:  0.4380728304386139
train gradient:  0.4801364318807386
iteration : 1663
train acc:  0.8359375
train loss:  0.3683484196662903
train gradient:  0.317054528221993
iteration : 1664
train acc:  0.84375
train loss:  0.37980344891548157
train gradient:  0.33485516760977413
iteration : 1665
train acc:  0.8203125
train loss:  0.37887513637542725
train gradient:  0.3098062222765124
iteration : 1666
train acc:  0.8359375
train loss:  0.36322879791259766
train gradient:  0.34678611640247076
iteration : 1667
train acc:  0.8203125
train loss:  0.40239912271499634
train gradient:  0.45238178480946956
iteration : 1668
train acc:  0.75
train loss:  0.5142563581466675
train gradient:  0.6805789809052161
iteration : 1669
train acc:  0.796875
train loss:  0.41132640838623047
train gradient:  0.91830849551947
iteration : 1670
train acc:  0.734375
train loss:  0.479735791683197
train gradient:  0.46431379731183
iteration : 1671
train acc:  0.8046875
train loss:  0.34455305337905884
train gradient:  0.30463019730922725
iteration : 1672
train acc:  0.84375
train loss:  0.37595558166503906
train gradient:  0.35444819957693086
iteration : 1673
train acc:  0.84375
train loss:  0.37240904569625854
train gradient:  0.33654224003384875
iteration : 1674
train acc:  0.7890625
train loss:  0.4575478136539459
train gradient:  0.6309473021819462
iteration : 1675
train acc:  0.8359375
train loss:  0.40855416655540466
train gradient:  0.34036424560807044
iteration : 1676
train acc:  0.78125
train loss:  0.45890650153160095
train gradient:  0.46581278533855563
iteration : 1677
train acc:  0.7890625
train loss:  0.3969098925590515
train gradient:  0.3160901424940444
iteration : 1678
train acc:  0.828125
train loss:  0.3969908356666565
train gradient:  0.3931523628754214
iteration : 1679
train acc:  0.8203125
train loss:  0.4032089114189148
train gradient:  0.4593004517802846
iteration : 1680
train acc:  0.84375
train loss:  0.329208105802536
train gradient:  0.4556713130709322
iteration : 1681
train acc:  0.8046875
train loss:  0.40048736333847046
train gradient:  0.4689329753117268
iteration : 1682
train acc:  0.7890625
train loss:  0.4084446430206299
train gradient:  0.3809051391124177
iteration : 1683
train acc:  0.78125
train loss:  0.406819224357605
train gradient:  0.47757686808788635
iteration : 1684
train acc:  0.8203125
train loss:  0.42092669010162354
train gradient:  0.4225449391459029
iteration : 1685
train acc:  0.7890625
train loss:  0.4770798981189728
train gradient:  0.5898194241495851
iteration : 1686
train acc:  0.8515625
train loss:  0.3511454463005066
train gradient:  0.3371450003585852
iteration : 1687
train acc:  0.7890625
train loss:  0.45445457100868225
train gradient:  0.5572255102597599
iteration : 1688
train acc:  0.8046875
train loss:  0.4023396372795105
train gradient:  0.3324161925585438
iteration : 1689
train acc:  0.8046875
train loss:  0.38780850172042847
train gradient:  0.3705538459954286
iteration : 1690
train acc:  0.8046875
train loss:  0.4189131259918213
train gradient:  0.4648904674809879
iteration : 1691
train acc:  0.8671875
train loss:  0.3487706780433655
train gradient:  0.34583043090878
iteration : 1692
train acc:  0.8125
train loss:  0.38859671354293823
train gradient:  0.7495304795020832
iteration : 1693
train acc:  0.8359375
train loss:  0.40734773874282837
train gradient:  0.48758684859422236
iteration : 1694
train acc:  0.8671875
train loss:  0.37646690011024475
train gradient:  0.43006061513232613
iteration : 1695
train acc:  0.8359375
train loss:  0.36368125677108765
train gradient:  0.37251739440657256
iteration : 1696
train acc:  0.8125
train loss:  0.4043174386024475
train gradient:  0.44809608132444784
iteration : 1697
train acc:  0.8359375
train loss:  0.40190958976745605
train gradient:  0.47545219689277124
iteration : 1698
train acc:  0.8125
train loss:  0.3583906590938568
train gradient:  0.44044585118303775
iteration : 1699
train acc:  0.8046875
train loss:  0.42118704319000244
train gradient:  0.5123494840729103
iteration : 1700
train acc:  0.84375
train loss:  0.3602474629878998
train gradient:  0.36440659292194316
iteration : 1701
train acc:  0.828125
train loss:  0.38752275705337524
train gradient:  0.4792417579519137
iteration : 1702
train acc:  0.7890625
train loss:  0.4149489402770996
train gradient:  0.4880369647905139
iteration : 1703
train acc:  0.7578125
train loss:  0.4997882544994354
train gradient:  0.5470080975470094
iteration : 1704
train acc:  0.7890625
train loss:  0.3999500870704651
train gradient:  0.5014952797990693
iteration : 1705
train acc:  0.8359375
train loss:  0.38141384720802307
train gradient:  0.5082280028778269
iteration : 1706
train acc:  0.8125
train loss:  0.389312207698822
train gradient:  0.3763232518813706
iteration : 1707
train acc:  0.8515625
train loss:  0.4092586636543274
train gradient:  0.5340373883842009
iteration : 1708
train acc:  0.875
train loss:  0.335795521736145
train gradient:  0.2656546819451199
iteration : 1709
train acc:  0.796875
train loss:  0.4349762201309204
train gradient:  0.5446896970076125
iteration : 1710
train acc:  0.8515625
train loss:  0.38019829988479614
train gradient:  0.41419891484438587
iteration : 1711
train acc:  0.84375
train loss:  0.4342082142829895
train gradient:  0.544243754726645
iteration : 1712
train acc:  0.7734375
train loss:  0.4825889468193054
train gradient:  0.6930817749678042
iteration : 1713
train acc:  0.875
train loss:  0.3813368082046509
train gradient:  0.4703371040313611
iteration : 1714
train acc:  0.8125
train loss:  0.377797394990921
train gradient:  0.4511262499764665
iteration : 1715
train acc:  0.8359375
train loss:  0.40696579217910767
train gradient:  0.5130515987488459
iteration : 1716
train acc:  0.8359375
train loss:  0.4171558618545532
train gradient:  0.46884164110084875
iteration : 1717
train acc:  0.8515625
train loss:  0.39088448882102966
train gradient:  0.32708288970005883
iteration : 1718
train acc:  0.7734375
train loss:  0.47948527336120605
train gradient:  0.6075307531036169
iteration : 1719
train acc:  0.7890625
train loss:  0.4483538866043091
train gradient:  0.4871619982531867
iteration : 1720
train acc:  0.7890625
train loss:  0.4154326915740967
train gradient:  0.4096658927170789
iteration : 1721
train acc:  0.8046875
train loss:  0.4309869110584259
train gradient:  0.38512043782492955
iteration : 1722
train acc:  0.7734375
train loss:  0.4026566743850708
train gradient:  0.5203257916126539
iteration : 1723
train acc:  0.796875
train loss:  0.4497544765472412
train gradient:  0.5207679254474977
iteration : 1724
train acc:  0.875
train loss:  0.35357391834259033
train gradient:  0.3169971694799829
iteration : 1725
train acc:  0.75
train loss:  0.46295541524887085
train gradient:  0.615268927118361
iteration : 1726
train acc:  0.78125
train loss:  0.41139131784439087
train gradient:  0.4827958062955595
iteration : 1727
train acc:  0.828125
train loss:  0.3752651810646057
train gradient:  0.34577642327014035
iteration : 1728
train acc:  0.7578125
train loss:  0.4997774362564087
train gradient:  0.731548097868532
iteration : 1729
train acc:  0.78125
train loss:  0.5279778242111206
train gradient:  0.6673237041167965
iteration : 1730
train acc:  0.75
train loss:  0.4353890120983124
train gradient:  0.6246788240325772
iteration : 1731
train acc:  0.8046875
train loss:  0.4125051498413086
train gradient:  0.5347912292784628
iteration : 1732
train acc:  0.859375
train loss:  0.3404048979282379
train gradient:  0.24210162907319774
iteration : 1733
train acc:  0.7421875
train loss:  0.4966244697570801
train gradient:  0.7913820644995123
iteration : 1734
train acc:  0.8125
train loss:  0.3495417833328247
train gradient:  0.33177233430444075
iteration : 1735
train acc:  0.875
train loss:  0.4420773386955261
train gradient:  0.7215488966051565
iteration : 1736
train acc:  0.78125
train loss:  0.49167871475219727
train gradient:  0.6387423504562659
iteration : 1737
train acc:  0.8359375
train loss:  0.3777006268501282
train gradient:  0.43077559866474946
iteration : 1738
train acc:  0.859375
train loss:  0.3342306315898895
train gradient:  0.25609281370061376
iteration : 1739
train acc:  0.796875
train loss:  0.3938605785369873
train gradient:  0.350868705741498
iteration : 1740
train acc:  0.8359375
train loss:  0.36333930492401123
train gradient:  0.3328889734413029
iteration : 1741
train acc:  0.875
train loss:  0.34543007612228394
train gradient:  0.4519031197219747
iteration : 1742
train acc:  0.8125
train loss:  0.38530272245407104
train gradient:  0.35778504824587354
iteration : 1743
train acc:  0.8046875
train loss:  0.3767834007740021
train gradient:  0.3752516939308728
iteration : 1744
train acc:  0.8359375
train loss:  0.3384072780609131
train gradient:  0.3135919135647762
iteration : 1745
train acc:  0.875
train loss:  0.32001954317092896
train gradient:  0.2436663052483261
iteration : 1746
train acc:  0.84375
train loss:  0.358249694108963
train gradient:  0.4083889475869137
iteration : 1747
train acc:  0.796875
train loss:  0.49305570125579834
train gradient:  0.598171776848952
iteration : 1748
train acc:  0.8515625
train loss:  0.3856673240661621
train gradient:  0.4037691498903955
iteration : 1749
train acc:  0.7890625
train loss:  0.41588178277015686
train gradient:  0.4623199164657473
iteration : 1750
train acc:  0.796875
train loss:  0.4177422523498535
train gradient:  0.3853994673350137
iteration : 1751
train acc:  0.7890625
train loss:  0.4073367416858673
train gradient:  0.36169602786420374
iteration : 1752
train acc:  0.828125
train loss:  0.3807756304740906
train gradient:  0.25577324738484597
iteration : 1753
train acc:  0.84375
train loss:  0.3414763808250427
train gradient:  0.34006850110139997
iteration : 1754
train acc:  0.7734375
train loss:  0.4143153429031372
train gradient:  0.3758731362644071
iteration : 1755
train acc:  0.75
train loss:  0.5006767511367798
train gradient:  0.7595413740252983
iteration : 1756
train acc:  0.8125
train loss:  0.37236708402633667
train gradient:  0.3855302211466432
iteration : 1757
train acc:  0.7734375
train loss:  0.4235934019088745
train gradient:  0.4072996723149924
iteration : 1758
train acc:  0.8046875
train loss:  0.4218672513961792
train gradient:  0.3827214335106205
iteration : 1759
train acc:  0.765625
train loss:  0.4660996198654175
train gradient:  0.4964401244569428
iteration : 1760
train acc:  0.796875
train loss:  0.3692740201950073
train gradient:  0.39701825815183045
iteration : 1761
train acc:  0.8203125
train loss:  0.4236270785331726
train gradient:  0.4123797903689801
iteration : 1762
train acc:  0.8125
train loss:  0.4193557798862457
train gradient:  0.4911344517688947
iteration : 1763
train acc:  0.890625
train loss:  0.332475483417511
train gradient:  0.2959810668882333
iteration : 1764
train acc:  0.8046875
train loss:  0.44763612747192383
train gradient:  0.5090377650291983
iteration : 1765
train acc:  0.75
train loss:  0.43532413244247437
train gradient:  0.5347599909889862
iteration : 1766
train acc:  0.8046875
train loss:  0.38363945484161377
train gradient:  0.353402391884977
iteration : 1767
train acc:  0.859375
train loss:  0.3651493787765503
train gradient:  0.3456908990565043
iteration : 1768
train acc:  0.796875
train loss:  0.48366498947143555
train gradient:  0.5034530664081298
iteration : 1769
train acc:  0.828125
train loss:  0.39253532886505127
train gradient:  0.43775202400218727
iteration : 1770
train acc:  0.796875
train loss:  0.5017127394676208
train gradient:  0.6191547938115483
iteration : 1771
train acc:  0.78125
train loss:  0.45901572704315186
train gradient:  0.4350471228913359
iteration : 1772
train acc:  0.8359375
train loss:  0.42781496047973633
train gradient:  0.442068625746315
iteration : 1773
train acc:  0.8515625
train loss:  0.36211955547332764
train gradient:  0.6127795689998025
iteration : 1774
train acc:  0.8125
train loss:  0.4051249027252197
train gradient:  0.3409989069214965
iteration : 1775
train acc:  0.828125
train loss:  0.3688613176345825
train gradient:  0.34081384499791323
iteration : 1776
train acc:  0.875
train loss:  0.3494766056537628
train gradient:  0.4364348205940837
iteration : 1777
train acc:  0.8671875
train loss:  0.3789684772491455
train gradient:  0.40255926785414586
iteration : 1778
train acc:  0.703125
train loss:  0.5313551425933838
train gradient:  0.9555911708471434
iteration : 1779
train acc:  0.8046875
train loss:  0.4162316918373108
train gradient:  0.7323371500161125
iteration : 1780
train acc:  0.828125
train loss:  0.3785754144191742
train gradient:  0.4463058454735602
iteration : 1781
train acc:  0.78125
train loss:  0.4134436547756195
train gradient:  0.5336146292210067
iteration : 1782
train acc:  0.8046875
train loss:  0.4168289601802826
train gradient:  0.5340508880069567
iteration : 1783
train acc:  0.8671875
train loss:  0.3466072976589203
train gradient:  0.3574789800783142
iteration : 1784
train acc:  0.7578125
train loss:  0.47564369440078735
train gradient:  0.6439057285918558
iteration : 1785
train acc:  0.8203125
train loss:  0.4023953676223755
train gradient:  0.4016793274608172
iteration : 1786
train acc:  0.8359375
train loss:  0.4174219071865082
train gradient:  0.3001660331881425
iteration : 1787
train acc:  0.84375
train loss:  0.4102478623390198
train gradient:  1.169194514623987
iteration : 1788
train acc:  0.8359375
train loss:  0.43678027391433716
train gradient:  0.4365457068085985
iteration : 1789
train acc:  0.8359375
train loss:  0.36559855937957764
train gradient:  0.47268457461471736
iteration : 1790
train acc:  0.7734375
train loss:  0.44340765476226807
train gradient:  0.5444301551639114
iteration : 1791
train acc:  0.796875
train loss:  0.38542795181274414
train gradient:  0.2736820555672078
iteration : 1792
train acc:  0.7890625
train loss:  0.4538534879684448
train gradient:  0.5188387995077837
iteration : 1793
train acc:  0.8671875
train loss:  0.349165141582489
train gradient:  0.293972915957791
iteration : 1794
train acc:  0.8046875
train loss:  0.39484113454818726
train gradient:  0.36445720989430314
iteration : 1795
train acc:  0.8046875
train loss:  0.4561208188533783
train gradient:  0.4424768604380647
iteration : 1796
train acc:  0.796875
train loss:  0.41066139936447144
train gradient:  0.37190736374597533
iteration : 1797
train acc:  0.7890625
train loss:  0.43681880831718445
train gradient:  0.4545137592602016
iteration : 1798
train acc:  0.796875
train loss:  0.41928189992904663
train gradient:  0.4496512890218767
iteration : 1799
train acc:  0.8046875
train loss:  0.41429975628852844
train gradient:  0.286447283739489
iteration : 1800
train acc:  0.8515625
train loss:  0.3865116238594055
train gradient:  0.2624313688844647
iteration : 1801
train acc:  0.78125
train loss:  0.4487726390361786
train gradient:  0.418783807697676
iteration : 1802
train acc:  0.796875
train loss:  0.41889023780822754
train gradient:  0.3888053303226292
iteration : 1803
train acc:  0.7890625
train loss:  0.4177665114402771
train gradient:  0.3439574710493076
iteration : 1804
train acc:  0.828125
train loss:  0.4383822977542877
train gradient:  0.5144431794819856
iteration : 1805
train acc:  0.78125
train loss:  0.39944157004356384
train gradient:  0.35492304069904324
iteration : 1806
train acc:  0.7421875
train loss:  0.5326734185218811
train gradient:  0.5922596221718294
iteration : 1807
train acc:  0.8125
train loss:  0.4192633628845215
train gradient:  0.3070404849216023
iteration : 1808
train acc:  0.8359375
train loss:  0.3798828125
train gradient:  0.30421504449929165
iteration : 1809
train acc:  0.734375
train loss:  0.5412672758102417
train gradient:  0.5564888220939426
iteration : 1810
train acc:  0.8515625
train loss:  0.3713149428367615
train gradient:  0.259009306476354
iteration : 1811
train acc:  0.8046875
train loss:  0.40835779905319214
train gradient:  0.36472904393866024
iteration : 1812
train acc:  0.828125
train loss:  0.43330302834510803
train gradient:  0.5564308820560546
iteration : 1813
train acc:  0.828125
train loss:  0.3739088177680969
train gradient:  0.2798219137074295
iteration : 1814
train acc:  0.84375
train loss:  0.3707224130630493
train gradient:  0.38511712716854934
iteration : 1815
train acc:  0.7578125
train loss:  0.4554084837436676
train gradient:  0.347135669409453
iteration : 1816
train acc:  0.8203125
train loss:  0.3975368142127991
train gradient:  0.4791984534990387
iteration : 1817
train acc:  0.8515625
train loss:  0.3897177577018738
train gradient:  0.267700425690032
iteration : 1818
train acc:  0.7734375
train loss:  0.3955424427986145
train gradient:  0.37155400828290447
iteration : 1819
train acc:  0.875
train loss:  0.32204365730285645
train gradient:  0.2607339295868011
iteration : 1820
train acc:  0.8203125
train loss:  0.381954550743103
train gradient:  0.2937352539590764
iteration : 1821
train acc:  0.796875
train loss:  0.4049895107746124
train gradient:  0.40279706151154665
iteration : 1822
train acc:  0.890625
train loss:  0.3059038519859314
train gradient:  0.3380839049437167
iteration : 1823
train acc:  0.8828125
train loss:  0.31756120920181274
train gradient:  0.22215347272630867
iteration : 1824
train acc:  0.765625
train loss:  0.4837953448295593
train gradient:  0.4442325227402104
iteration : 1825
train acc:  0.7734375
train loss:  0.414010226726532
train gradient:  0.40930251776226967
iteration : 1826
train acc:  0.796875
train loss:  0.4229036867618561
train gradient:  0.39779285041729284
iteration : 1827
train acc:  0.828125
train loss:  0.4102681875228882
train gradient:  0.3502721370880776
iteration : 1828
train acc:  0.8125
train loss:  0.38829195499420166
train gradient:  0.3562011360064628
iteration : 1829
train acc:  0.828125
train loss:  0.3633577823638916
train gradient:  0.36141796256335035
iteration : 1830
train acc:  0.8125
train loss:  0.37661051750183105
train gradient:  0.404162847062136
iteration : 1831
train acc:  0.796875
train loss:  0.4579203724861145
train gradient:  0.4240774228555838
iteration : 1832
train acc:  0.859375
train loss:  0.38235026597976685
train gradient:  0.3548624134703598
iteration : 1833
train acc:  0.8984375
train loss:  0.3179044723510742
train gradient:  0.24960856695169154
iteration : 1834
train acc:  0.8515625
train loss:  0.3757926821708679
train gradient:  0.34342135609772095
iteration : 1835
train acc:  0.78125
train loss:  0.4130888879299164
train gradient:  0.4084848656148631
iteration : 1836
train acc:  0.8046875
train loss:  0.38182881474494934
train gradient:  0.3096903459702752
iteration : 1837
train acc:  0.828125
train loss:  0.34679555892944336
train gradient:  0.2909751437775204
iteration : 1838
train acc:  0.859375
train loss:  0.35196182131767273
train gradient:  0.3879554416617663
iteration : 1839
train acc:  0.828125
train loss:  0.4459187984466553
train gradient:  0.45472736530131513
iteration : 1840
train acc:  0.84375
train loss:  0.41628551483154297
train gradient:  0.5259934244480344
iteration : 1841
train acc:  0.7734375
train loss:  0.47942212224006653
train gradient:  0.5209206244579013
iteration : 1842
train acc:  0.8125
train loss:  0.4111447334289551
train gradient:  0.3887403758118693
iteration : 1843
train acc:  0.828125
train loss:  0.39918869733810425
train gradient:  0.37834838567177037
iteration : 1844
train acc:  0.84375
train loss:  0.3488708436489105
train gradient:  0.28859260070146203
iteration : 1845
train acc:  0.8203125
train loss:  0.45038315653800964
train gradient:  0.3214992749242904
iteration : 1846
train acc:  0.828125
train loss:  0.4457656443119049
train gradient:  0.5003942365704361
iteration : 1847
train acc:  0.7578125
train loss:  0.40810680389404297
train gradient:  0.5234597694954936
iteration : 1848
train acc:  0.78125
train loss:  0.46766597032546997
train gradient:  0.5006059140035989
iteration : 1849
train acc:  0.8125
train loss:  0.4340541958808899
train gradient:  0.4332900100668295
iteration : 1850
train acc:  0.765625
train loss:  0.4721493721008301
train gradient:  0.5550723919180918
iteration : 1851
train acc:  0.8046875
train loss:  0.39405718445777893
train gradient:  0.36993901037782656
iteration : 1852
train acc:  0.828125
train loss:  0.4018377959728241
train gradient:  0.4779618595792897
iteration : 1853
train acc:  0.796875
train loss:  0.425235390663147
train gradient:  0.44544845920115533
iteration : 1854
train acc:  0.8203125
train loss:  0.435289204120636
train gradient:  0.5845504500544506
iteration : 1855
train acc:  0.8125
train loss:  0.4583088457584381
train gradient:  0.521684569981171
iteration : 1856
train acc:  0.8203125
train loss:  0.3686991333961487
train gradient:  0.3949181370760303
iteration : 1857
train acc:  0.7890625
train loss:  0.4337734580039978
train gradient:  0.5390462845902165
iteration : 1858
train acc:  0.828125
train loss:  0.39719855785369873
train gradient:  0.3943137731718517
iteration : 1859
train acc:  0.7890625
train loss:  0.47115230560302734
train gradient:  0.6513346628949142
iteration : 1860
train acc:  0.8046875
train loss:  0.4180909991264343
train gradient:  0.49591544372075597
iteration : 1861
train acc:  0.7734375
train loss:  0.43787747621536255
train gradient:  0.470897720792304
iteration : 1862
train acc:  0.796875
train loss:  0.42326414585113525
train gradient:  0.3661339415041198
iteration : 1863
train acc:  0.796875
train loss:  0.4758864641189575
train gradient:  0.4018132920768607
iteration : 1864
train acc:  0.8203125
train loss:  0.4056081175804138
train gradient:  0.36279777095689264
iteration : 1865
train acc:  0.796875
train loss:  0.43809473514556885
train gradient:  0.39810862928291085
iteration : 1866
train acc:  0.8203125
train loss:  0.4031022787094116
train gradient:  0.42360063781149826
iteration : 1867
train acc:  0.796875
train loss:  0.4516376256942749
train gradient:  0.41951209562801206
iteration : 1868
train acc:  0.8671875
train loss:  0.33989083766937256
train gradient:  0.27190957205509864
iteration : 1869
train acc:  0.7890625
train loss:  0.39331233501434326
train gradient:  0.3461768155606204
iteration : 1870
train acc:  0.8671875
train loss:  0.3652459979057312
train gradient:  0.42770924241610264
iteration : 1871
train acc:  0.859375
train loss:  0.3839392364025116
train gradient:  0.5147916405035893
iteration : 1872
train acc:  0.7890625
train loss:  0.47185370326042175
train gradient:  0.44624858778192383
iteration : 1873
train acc:  0.8046875
train loss:  0.42704248428344727
train gradient:  0.34134654310591206
iteration : 1874
train acc:  0.84375
train loss:  0.3608810007572174
train gradient:  0.3703676951397554
iteration : 1875
train acc:  0.7890625
train loss:  0.42237532138824463
train gradient:  0.4679523981558261
iteration : 1876
train acc:  0.7890625
train loss:  0.4357517957687378
train gradient:  0.3876144834207235
iteration : 1877
train acc:  0.84375
train loss:  0.37004998326301575
train gradient:  0.4412337983745245
iteration : 1878
train acc:  0.7890625
train loss:  0.4435933530330658
train gradient:  0.4636774984159867
iteration : 1879
train acc:  0.8359375
train loss:  0.3702796697616577
train gradient:  0.34058840811730334
iteration : 1880
train acc:  0.828125
train loss:  0.37988078594207764
train gradient:  0.4049861385210883
iteration : 1881
train acc:  0.828125
train loss:  0.3548206090927124
train gradient:  0.36422409925359217
iteration : 1882
train acc:  0.8046875
train loss:  0.3723578453063965
train gradient:  0.39636816509158634
iteration : 1883
train acc:  0.828125
train loss:  0.40271884202957153
train gradient:  0.24088884833935964
iteration : 1884
train acc:  0.828125
train loss:  0.3960148096084595
train gradient:  0.4801024436681198
iteration : 1885
train acc:  0.8203125
train loss:  0.4110439419746399
train gradient:  0.37944551086013534
iteration : 1886
train acc:  0.859375
train loss:  0.3727726936340332
train gradient:  0.36768287042499537
iteration : 1887
train acc:  0.84375
train loss:  0.35459229350090027
train gradient:  0.3558532109936214
iteration : 1888
train acc:  0.8046875
train loss:  0.36182326078414917
train gradient:  0.4005036283028215
iteration : 1889
train acc:  0.8203125
train loss:  0.36682263016700745
train gradient:  0.3526904808785278
iteration : 1890
train acc:  0.8359375
train loss:  0.4013237953186035
train gradient:  0.4223480429426034
iteration : 1891
train acc:  0.8203125
train loss:  0.3610442280769348
train gradient:  0.3399510790878966
iteration : 1892
train acc:  0.859375
train loss:  0.34583818912506104
train gradient:  0.2894064933986993
iteration : 1893
train acc:  0.8046875
train loss:  0.4068717062473297
train gradient:  0.3777272912934036
iteration : 1894
train acc:  0.8046875
train loss:  0.39813435077667236
train gradient:  0.4533890362095787
iteration : 1895
train acc:  0.8125
train loss:  0.3558599352836609
train gradient:  0.4516305924965646
iteration : 1896
train acc:  0.75
train loss:  0.4426245093345642
train gradient:  0.44039934658180196
iteration : 1897
train acc:  0.78125
train loss:  0.44317010045051575
train gradient:  0.6042271197329876
iteration : 1898
train acc:  0.7890625
train loss:  0.41013219952583313
train gradient:  0.495692213834639
iteration : 1899
train acc:  0.8359375
train loss:  0.3695436716079712
train gradient:  0.31018727117993516
iteration : 1900
train acc:  0.8046875
train loss:  0.3686371445655823
train gradient:  0.32624058899174435
iteration : 1901
train acc:  0.828125
train loss:  0.43825387954711914
train gradient:  0.47371875774842237
iteration : 1902
train acc:  0.796875
train loss:  0.40607964992523193
train gradient:  0.4926561454718408
iteration : 1903
train acc:  0.84375
train loss:  0.3986146152019501
train gradient:  0.3820435570463626
iteration : 1904
train acc:  0.828125
train loss:  0.41531774401664734
train gradient:  0.35391476176017783
iteration : 1905
train acc:  0.8125
train loss:  0.43058228492736816
train gradient:  0.3248344618431744
iteration : 1906
train acc:  0.8046875
train loss:  0.4750980734825134
train gradient:  0.729875796514239
iteration : 1907
train acc:  0.84375
train loss:  0.346439003944397
train gradient:  0.3273958526656722
iteration : 1908
train acc:  0.7734375
train loss:  0.465075820684433
train gradient:  0.6548556805251793
iteration : 1909
train acc:  0.8046875
train loss:  0.514872670173645
train gradient:  0.6172671140098305
iteration : 1910
train acc:  0.8046875
train loss:  0.34750044345855713
train gradient:  0.4069449611060041
iteration : 1911
train acc:  0.8515625
train loss:  0.3348051905632019
train gradient:  0.3034424743056026
iteration : 1912
train acc:  0.78125
train loss:  0.43129652738571167
train gradient:  0.47577342628766045
iteration : 1913
train acc:  0.8046875
train loss:  0.4181476831436157
train gradient:  0.461907673663061
iteration : 1914
train acc:  0.7890625
train loss:  0.3826032876968384
train gradient:  0.5283125403913322
iteration : 1915
train acc:  0.8203125
train loss:  0.4098101258277893
train gradient:  0.6763181379593399
iteration : 1916
train acc:  0.84375
train loss:  0.3879185914993286
train gradient:  0.3795627241662713
iteration : 1917
train acc:  0.859375
train loss:  0.3593621253967285
train gradient:  0.38081784450059925
iteration : 1918
train acc:  0.8125
train loss:  0.42484062910079956
train gradient:  0.478948882638784
iteration : 1919
train acc:  0.796875
train loss:  0.44165945053100586
train gradient:  0.6353320126157087
iteration : 1920
train acc:  0.8125
train loss:  0.40530669689178467
train gradient:  0.34475736345895974
iteration : 1921
train acc:  0.828125
train loss:  0.37289783358573914
train gradient:  0.27915833253286676
iteration : 1922
train acc:  0.796875
train loss:  0.4387972950935364
train gradient:  0.5299430020622489
iteration : 1923
train acc:  0.8046875
train loss:  0.456277996301651
train gradient:  0.5737736597795949
iteration : 1924
train acc:  0.734375
train loss:  0.5111459493637085
train gradient:  0.6785722395514189
iteration : 1925
train acc:  0.8125
train loss:  0.4287545680999756
train gradient:  0.38896271073649463
iteration : 1926
train acc:  0.8203125
train loss:  0.37508344650268555
train gradient:  0.41498018689318683
iteration : 1927
train acc:  0.78125
train loss:  0.4186021685600281
train gradient:  0.45381818805449303
iteration : 1928
train acc:  0.84375
train loss:  0.40113645792007446
train gradient:  0.4157918477077211
iteration : 1929
train acc:  0.90625
train loss:  0.30120569467544556
train gradient:  0.35139579601156684
iteration : 1930
train acc:  0.8046875
train loss:  0.37405383586883545
train gradient:  0.298807232228791
iteration : 1931
train acc:  0.8125
train loss:  0.39910024404525757
train gradient:  0.38760784531297005
iteration : 1932
train acc:  0.7890625
train loss:  0.42024391889572144
train gradient:  0.42623587848837907
iteration : 1933
train acc:  0.8359375
train loss:  0.3851810097694397
train gradient:  0.3804174975289541
iteration : 1934
train acc:  0.7890625
train loss:  0.4531988203525543
train gradient:  0.6216473445539121
iteration : 1935
train acc:  0.859375
train loss:  0.3483264446258545
train gradient:  0.3085976330674479
iteration : 1936
train acc:  0.8203125
train loss:  0.363265722990036
train gradient:  0.4106839802747954
iteration : 1937
train acc:  0.84375
train loss:  0.37027615308761597
train gradient:  0.3119153483798555
iteration : 1938
train acc:  0.8203125
train loss:  0.36491361260414124
train gradient:  0.24589816179075927
iteration : 1939
train acc:  0.7890625
train loss:  0.4178771674633026
train gradient:  0.33177905157126286
iteration : 1940
train acc:  0.8359375
train loss:  0.40733325481414795
train gradient:  0.3607068767582562
iteration : 1941
train acc:  0.8046875
train loss:  0.37114274501800537
train gradient:  0.41881843061043034
iteration : 1942
train acc:  0.828125
train loss:  0.3581855893135071
train gradient:  0.3376815578861297
iteration : 1943
train acc:  0.8125
train loss:  0.40811946988105774
train gradient:  0.42674059548611604
iteration : 1944
train acc:  0.78125
train loss:  0.4427404999732971
train gradient:  0.6144438553099565
iteration : 1945
train acc:  0.8359375
train loss:  0.3878348767757416
train gradient:  0.45119159379703055
iteration : 1946
train acc:  0.7578125
train loss:  0.5091158151626587
train gradient:  0.41259532022203743
iteration : 1947
train acc:  0.8125
train loss:  0.38370683789253235
train gradient:  0.3533644257708577
iteration : 1948
train acc:  0.796875
train loss:  0.40771955251693726
train gradient:  0.32943299569436485
iteration : 1949
train acc:  0.7890625
train loss:  0.40636956691741943
train gradient:  0.40804496411921076
iteration : 1950
train acc:  0.84375
train loss:  0.35194408893585205
train gradient:  0.37432215795821827
iteration : 1951
train acc:  0.875
train loss:  0.3268647789955139
train gradient:  0.34008478343574783
iteration : 1952
train acc:  0.8515625
train loss:  0.34631648659706116
train gradient:  0.3620351695740502
iteration : 1953
train acc:  0.8046875
train loss:  0.43495824933052063
train gradient:  0.35538755555534285
iteration : 1954
train acc:  0.78125
train loss:  0.4910256266593933
train gradient:  0.49462632598114886
iteration : 1955
train acc:  0.828125
train loss:  0.3469175696372986
train gradient:  0.2830821448369474
iteration : 1956
train acc:  0.84375
train loss:  0.4244464933872223
train gradient:  0.45995120109772986
iteration : 1957
train acc:  0.8203125
train loss:  0.3844430446624756
train gradient:  0.31439041521152605
iteration : 1958
train acc:  0.796875
train loss:  0.39899203181266785
train gradient:  0.36853239264582904
iteration : 1959
train acc:  0.765625
train loss:  0.4474293291568756
train gradient:  0.4242554900704994
iteration : 1960
train acc:  0.8359375
train loss:  0.35654228925704956
train gradient:  0.46021806776314556
iteration : 1961
train acc:  0.8046875
train loss:  0.38592076301574707
train gradient:  0.35701986777002176
iteration : 1962
train acc:  0.8515625
train loss:  0.3382894992828369
train gradient:  0.37264125833590067
iteration : 1963
train acc:  0.8671875
train loss:  0.3912607431411743
train gradient:  0.4730274315727089
iteration : 1964
train acc:  0.8359375
train loss:  0.3676965534687042
train gradient:  0.3969878178681848
iteration : 1965
train acc:  0.828125
train loss:  0.41837871074676514
train gradient:  0.4948823306561284
iteration : 1966
train acc:  0.859375
train loss:  0.37135475873947144
train gradient:  0.3454304636255925
iteration : 1967
train acc:  0.75
train loss:  0.4696962833404541
train gradient:  0.4810119945858656
iteration : 1968
train acc:  0.7890625
train loss:  0.3923020362854004
train gradient:  0.4012066692847235
iteration : 1969
train acc:  0.734375
train loss:  0.49480387568473816
train gradient:  0.7380147863968529
iteration : 1970
train acc:  0.8359375
train loss:  0.37534356117248535
train gradient:  0.4132302470521212
iteration : 1971
train acc:  0.8125
train loss:  0.45304423570632935
train gradient:  0.5086694946414961
iteration : 1972
train acc:  0.78125
train loss:  0.4223378002643585
train gradient:  0.6526562338374393
iteration : 1973
train acc:  0.84375
train loss:  0.3471997082233429
train gradient:  0.39342144410980096
iteration : 1974
train acc:  0.875
train loss:  0.2796050012111664
train gradient:  0.22255178515753904
iteration : 1975
train acc:  0.8203125
train loss:  0.41882771253585815
train gradient:  0.4646692049055901
iteration : 1976
train acc:  0.8359375
train loss:  0.3925582468509674
train gradient:  0.5068662525922194
iteration : 1977
train acc:  0.8671875
train loss:  0.34210100769996643
train gradient:  0.3757372557844825
iteration : 1978
train acc:  0.8359375
train loss:  0.3523215055465698
train gradient:  0.2689865896858759
iteration : 1979
train acc:  0.8203125
train loss:  0.41158628463745117
train gradient:  0.48227544282923773
iteration : 1980
train acc:  0.796875
train loss:  0.4648124575614929
train gradient:  0.5546303986562315
iteration : 1981
train acc:  0.78125
train loss:  0.41807323694229126
train gradient:  0.4835133138479737
iteration : 1982
train acc:  0.7890625
train loss:  0.494317889213562
train gradient:  0.5466548325263126
iteration : 1983
train acc:  0.75
train loss:  0.4695357084274292
train gradient:  0.7859352407433765
iteration : 1984
train acc:  0.84375
train loss:  0.3895857334136963
train gradient:  0.3637407992660292
iteration : 1985
train acc:  0.828125
train loss:  0.39555054903030396
train gradient:  0.33757080506794734
iteration : 1986
train acc:  0.8671875
train loss:  0.3295019865036011
train gradient:  0.31942615309338385
iteration : 1987
train acc:  0.8046875
train loss:  0.40846043825149536
train gradient:  0.32735776645080716
iteration : 1988
train acc:  0.859375
train loss:  0.37917476892471313
train gradient:  0.28927703883376416
iteration : 1989
train acc:  0.8359375
train loss:  0.3554803431034088
train gradient:  0.34358850853365397
iteration : 1990
train acc:  0.859375
train loss:  0.32755047082901
train gradient:  0.2196545564923943
iteration : 1991
train acc:  0.8125
train loss:  0.37525737285614014
train gradient:  0.3238083530780486
iteration : 1992
train acc:  0.7578125
train loss:  0.47577840089797974
train gradient:  0.6317647259087951
iteration : 1993
train acc:  0.8515625
train loss:  0.36082762479782104
train gradient:  0.38415619560365183
iteration : 1994
train acc:  0.828125
train loss:  0.37880194187164307
train gradient:  0.32111833159433495
iteration : 1995
train acc:  0.78125
train loss:  0.3886595368385315
train gradient:  0.4591075464359761
iteration : 1996
train acc:  0.796875
train loss:  0.36779657006263733
train gradient:  0.4720199439101062
iteration : 1997
train acc:  0.78125
train loss:  0.45267701148986816
train gradient:  0.46403292778526506
iteration : 1998
train acc:  0.78125
train loss:  0.4856142997741699
train gradient:  0.602843433571056
iteration : 1999
train acc:  0.765625
train loss:  0.5036908984184265
train gradient:  0.5231737482907762
iteration : 2000
train acc:  0.765625
train loss:  0.41782712936401367
train gradient:  0.4436860732430792
iteration : 2001
train acc:  0.78125
train loss:  0.4096667170524597
train gradient:  0.4010471821433597
iteration : 2002
train acc:  0.8125
train loss:  0.3997498154640198
train gradient:  0.36674735717346424
iteration : 2003
train acc:  0.7890625
train loss:  0.42957887053489685
train gradient:  0.42655883121891913
iteration : 2004
train acc:  0.828125
train loss:  0.4834659695625305
train gradient:  0.41091577576479715
iteration : 2005
train acc:  0.765625
train loss:  0.4086233973503113
train gradient:  0.3552184328008033
iteration : 2006
train acc:  0.8359375
train loss:  0.3886309266090393
train gradient:  0.2971473905289318
iteration : 2007
train acc:  0.78125
train loss:  0.42809510231018066
train gradient:  0.35315446701169695
iteration : 2008
train acc:  0.8125
train loss:  0.3973722755908966
train gradient:  0.3102295731051168
iteration : 2009
train acc:  0.8125
train loss:  0.42415040731430054
train gradient:  0.4624520315118835
iteration : 2010
train acc:  0.8203125
train loss:  0.3851199746131897
train gradient:  0.29849110289661557
iteration : 2011
train acc:  0.8359375
train loss:  0.40633711218833923
train gradient:  0.29858253305601956
iteration : 2012
train acc:  0.828125
train loss:  0.38741782307624817
train gradient:  0.3982790521430557
iteration : 2013
train acc:  0.828125
train loss:  0.34130561351776123
train gradient:  0.26268486065312785
iteration : 2014
train acc:  0.859375
train loss:  0.38175705075263977
train gradient:  0.3549892069490794
iteration : 2015
train acc:  0.7578125
train loss:  0.48014312982559204
train gradient:  0.6114143851981699
iteration : 2016
train acc:  0.796875
train loss:  0.4461282789707184
train gradient:  0.8721954071853725
iteration : 2017
train acc:  0.7734375
train loss:  0.44305112957954407
train gradient:  0.4397787495696746
iteration : 2018
train acc:  0.8203125
train loss:  0.4046021103858948
train gradient:  0.3719314688736007
iteration : 2019
train acc:  0.796875
train loss:  0.3976901173591614
train gradient:  0.43018578972510124
iteration : 2020
train acc:  0.8203125
train loss:  0.4330575168132782
train gradient:  0.4694159857505466
iteration : 2021
train acc:  0.78125
train loss:  0.40189841389656067
train gradient:  0.27913208729826144
iteration : 2022
train acc:  0.8515625
train loss:  0.3923935294151306
train gradient:  0.42218263125978056
iteration : 2023
train acc:  0.78125
train loss:  0.470653235912323
train gradient:  0.49011593290088323
iteration : 2024
train acc:  0.7421875
train loss:  0.5019265413284302
train gradient:  0.5009357342304543
iteration : 2025
train acc:  0.78125
train loss:  0.4306320548057556
train gradient:  0.49577335824455326
iteration : 2026
train acc:  0.765625
train loss:  0.45606788992881775
train gradient:  0.48859286618127723
iteration : 2027
train acc:  0.78125
train loss:  0.3858078420162201
train gradient:  0.3174840349689015
iteration : 2028
train acc:  0.84375
train loss:  0.354035347700119
train gradient:  0.41758945694396393
iteration : 2029
train acc:  0.84375
train loss:  0.3203110694885254
train gradient:  0.2910471032470688
iteration : 2030
train acc:  0.8359375
train loss:  0.4075247347354889
train gradient:  0.2952284107548627
iteration : 2031
train acc:  0.8046875
train loss:  0.4049047529697418
train gradient:  0.3520830684783328
iteration : 2032
train acc:  0.796875
train loss:  0.3957195281982422
train gradient:  0.3386723386388802
iteration : 2033
train acc:  0.859375
train loss:  0.32541269063949585
train gradient:  0.2719352502907713
iteration : 2034
train acc:  0.8671875
train loss:  0.31882765889167786
train gradient:  0.24406222494668522
iteration : 2035
train acc:  0.78125
train loss:  0.37216031551361084
train gradient:  0.28512607943917145
iteration : 2036
train acc:  0.75
train loss:  0.4492951035499573
train gradient:  0.4765409418360569
iteration : 2037
train acc:  0.796875
train loss:  0.4378684163093567
train gradient:  0.3414841926070628
iteration : 2038
train acc:  0.8359375
train loss:  0.3681933879852295
train gradient:  0.3955712755595282
iteration : 2039
train acc:  0.796875
train loss:  0.3914802372455597
train gradient:  0.3442982220700755
iteration : 2040
train acc:  0.78125
train loss:  0.43357840180397034
train gradient:  0.378055265351868
iteration : 2041
train acc:  0.7890625
train loss:  0.36013197898864746
train gradient:  0.2885763295271646
iteration : 2042
train acc:  0.8515625
train loss:  0.3470003008842468
train gradient:  0.2936757394350379
iteration : 2043
train acc:  0.828125
train loss:  0.40629422664642334
train gradient:  0.3467741938122743
iteration : 2044
train acc:  0.7734375
train loss:  0.5156762599945068
train gradient:  0.5138863764577486
iteration : 2045
train acc:  0.8515625
train loss:  0.31001532077789307
train gradient:  0.22417168663603312
iteration : 2046
train acc:  0.828125
train loss:  0.4199841618537903
train gradient:  0.2764017264859009
iteration : 2047
train acc:  0.8125
train loss:  0.4064554274082184
train gradient:  0.3770878216318128
iteration : 2048
train acc:  0.8125
train loss:  0.4491804242134094
train gradient:  0.4144312411346106
iteration : 2049
train acc:  0.796875
train loss:  0.40571480989456177
train gradient:  0.38201958392215485
iteration : 2050
train acc:  0.796875
train loss:  0.36280345916748047
train gradient:  0.31968211041808503
iteration : 2051
train acc:  0.8203125
train loss:  0.39086803793907166
train gradient:  0.37275930859284917
iteration : 2052
train acc:  0.8203125
train loss:  0.38355720043182373
train gradient:  0.344510829378627
iteration : 2053
train acc:  0.84375
train loss:  0.3429115414619446
train gradient:  0.28939222142151055
iteration : 2054
train acc:  0.8359375
train loss:  0.37419748306274414
train gradient:  0.33197236621078857
iteration : 2055
train acc:  0.7890625
train loss:  0.438895046710968
train gradient:  0.42303740849729116
iteration : 2056
train acc:  0.8203125
train loss:  0.3828488886356354
train gradient:  0.35724949662257743
iteration : 2057
train acc:  0.8203125
train loss:  0.4051809012889862
train gradient:  0.5503103098358452
iteration : 2058
train acc:  0.796875
train loss:  0.43121665716171265
train gradient:  0.4211929482792572
iteration : 2059
train acc:  0.8125
train loss:  0.38323330879211426
train gradient:  0.4899776139511259
iteration : 2060
train acc:  0.84375
train loss:  0.3431411385536194
train gradient:  0.3145965071563862
iteration : 2061
train acc:  0.828125
train loss:  0.36151570081710815
train gradient:  0.2938251851467932
iteration : 2062
train acc:  0.84375
train loss:  0.33241868019104004
train gradient:  0.32640216351956475
iteration : 2063
train acc:  0.8359375
train loss:  0.3785039782524109
train gradient:  0.338263512578406
iteration : 2064
train acc:  0.78125
train loss:  0.4915262460708618
train gradient:  0.520368514283094
iteration : 2065
train acc:  0.7890625
train loss:  0.4909219741821289
train gradient:  0.4926052860359567
iteration : 2066
train acc:  0.828125
train loss:  0.3525461256504059
train gradient:  0.28196394531473956
iteration : 2067
train acc:  0.8046875
train loss:  0.37776029109954834
train gradient:  0.39784832085778166
iteration : 2068
train acc:  0.78125
train loss:  0.4329414665699005
train gradient:  0.40747416527682895
iteration : 2069
train acc:  0.8515625
train loss:  0.38210827112197876
train gradient:  0.3986339632921654
iteration : 2070
train acc:  0.8828125
train loss:  0.32671046257019043
train gradient:  0.24572561624836303
iteration : 2071
train acc:  0.84375
train loss:  0.40933892130851746
train gradient:  0.32363409969429713
iteration : 2072
train acc:  0.8125
train loss:  0.4307275116443634
train gradient:  0.4804169100929438
iteration : 2073
train acc:  0.8359375
train loss:  0.3882768750190735
train gradient:  0.4079583819187996
iteration : 2074
train acc:  0.8203125
train loss:  0.43753400444984436
train gradient:  0.3935463916081682
iteration : 2075
train acc:  0.8359375
train loss:  0.37151533365249634
train gradient:  0.35095094845214525
iteration : 2076
train acc:  0.8125
train loss:  0.43250179290771484
train gradient:  0.43940670496995193
iteration : 2077
train acc:  0.890625
train loss:  0.310493141412735
train gradient:  0.24988005406574879
iteration : 2078
train acc:  0.84375
train loss:  0.3719364404678345
train gradient:  0.3613325087150862
iteration : 2079
train acc:  0.8671875
train loss:  0.36237141489982605
train gradient:  0.3010877456807856
iteration : 2080
train acc:  0.8125
train loss:  0.4580598473548889
train gradient:  0.5343302380028818
iteration : 2081
train acc:  0.8359375
train loss:  0.4026705026626587
train gradient:  0.33612955818088347
iteration : 2082
train acc:  0.828125
train loss:  0.3640633821487427
train gradient:  0.4190958635885861
iteration : 2083
train acc:  0.828125
train loss:  0.3682180643081665
train gradient:  0.3589937716348707
iteration : 2084
train acc:  0.84375
train loss:  0.3281790614128113
train gradient:  0.35917868675275527
iteration : 2085
train acc:  0.828125
train loss:  0.3710606098175049
train gradient:  0.455044859921011
iteration : 2086
train acc:  0.828125
train loss:  0.4306958317756653
train gradient:  0.4454697204523359
iteration : 2087
train acc:  0.8515625
train loss:  0.4030286371707916
train gradient:  0.35416638610589435
iteration : 2088
train acc:  0.84375
train loss:  0.39871612191200256
train gradient:  0.5144107380342482
iteration : 2089
train acc:  0.7734375
train loss:  0.435457319021225
train gradient:  0.4896125517787307
iteration : 2090
train acc:  0.8046875
train loss:  0.4419333338737488
train gradient:  0.47079433954314354
iteration : 2091
train acc:  0.828125
train loss:  0.38748031854629517
train gradient:  0.34481263047053085
iteration : 2092
train acc:  0.8046875
train loss:  0.44553282856941223
train gradient:  0.48749311571805065
iteration : 2093
train acc:  0.8515625
train loss:  0.3263580799102783
train gradient:  0.3605323930082484
iteration : 2094
train acc:  0.8125
train loss:  0.42824843525886536
train gradient:  0.3765884466289164
iteration : 2095
train acc:  0.8125
train loss:  0.37425851821899414
train gradient:  0.403058455210449
iteration : 2096
train acc:  0.890625
train loss:  0.28052109479904175
train gradient:  0.187396287520558
iteration : 2097
train acc:  0.8515625
train loss:  0.3245766758918762
train gradient:  0.4625084803061736
iteration : 2098
train acc:  0.84375
train loss:  0.42027732729911804
train gradient:  0.37994204612838695
iteration : 2099
train acc:  0.84375
train loss:  0.42637622356414795
train gradient:  0.4501089478948656
iteration : 2100
train acc:  0.8203125
train loss:  0.40916118025779724
train gradient:  0.5031641628562803
iteration : 2101
train acc:  0.828125
train loss:  0.38331443071365356
train gradient:  0.3606349855382004
iteration : 2102
train acc:  0.8046875
train loss:  0.4417649805545807
train gradient:  0.5963445538521677
iteration : 2103
train acc:  0.8359375
train loss:  0.36812806129455566
train gradient:  0.36886880010646456
iteration : 2104
train acc:  0.7734375
train loss:  0.43910884857177734
train gradient:  0.7353713572598373
iteration : 2105
train acc:  0.78125
train loss:  0.45855456590652466
train gradient:  0.5124422482742437
iteration : 2106
train acc:  0.7734375
train loss:  0.45458757877349854
train gradient:  0.4803867265339588
iteration : 2107
train acc:  0.8203125
train loss:  0.3546302318572998
train gradient:  0.35928149531355486
iteration : 2108
train acc:  0.8125
train loss:  0.44412657618522644
train gradient:  0.5120655716673029
iteration : 2109
train acc:  0.828125
train loss:  0.4354935884475708
train gradient:  0.45386649676149005
iteration : 2110
train acc:  0.796875
train loss:  0.4154490828514099
train gradient:  0.40937267578180603
iteration : 2111
train acc:  0.75
train loss:  0.5200690031051636
train gradient:  0.556852952433796
iteration : 2112
train acc:  0.8359375
train loss:  0.38041621446609497
train gradient:  0.37470514470126537
iteration : 2113
train acc:  0.8203125
train loss:  0.339653342962265
train gradient:  0.29972050912109993
iteration : 2114
train acc:  0.7734375
train loss:  0.39466267824172974
train gradient:  0.4478370477465164
iteration : 2115
train acc:  0.828125
train loss:  0.35092729330062866
train gradient:  0.2806986354630002
iteration : 2116
train acc:  0.8359375
train loss:  0.36388498544692993
train gradient:  0.2802943505637931
iteration : 2117
train acc:  0.8125
train loss:  0.43261587619781494
train gradient:  0.43747510904831133
iteration : 2118
train acc:  0.8515625
train loss:  0.4175542891025543
train gradient:  0.3720841718092774
iteration : 2119
train acc:  0.8125
train loss:  0.4077569842338562
train gradient:  0.4548460755377347
iteration : 2120
train acc:  0.84375
train loss:  0.36259138584136963
train gradient:  0.3371801676470518
iteration : 2121
train acc:  0.8046875
train loss:  0.4589531421661377
train gradient:  0.6223251332035242
iteration : 2122
train acc:  0.8203125
train loss:  0.419151246547699
train gradient:  0.5074627303081358
iteration : 2123
train acc:  0.8125
train loss:  0.3578440845012665
train gradient:  0.45992483994819827
iteration : 2124
train acc:  0.84375
train loss:  0.3893339931964874
train gradient:  0.344073794890096
iteration : 2125
train acc:  0.8203125
train loss:  0.42480793595314026
train gradient:  0.44737939920482317
iteration : 2126
train acc:  0.78125
train loss:  0.44850969314575195
train gradient:  0.4971652605254907
iteration : 2127
train acc:  0.7890625
train loss:  0.3744552731513977
train gradient:  0.32445735270940185
iteration : 2128
train acc:  0.8359375
train loss:  0.35025882720947266
train gradient:  0.22859056670751743
iteration : 2129
train acc:  0.8125
train loss:  0.39229801297187805
train gradient:  0.3911017595553886
iteration : 2130
train acc:  0.859375
train loss:  0.32751694321632385
train gradient:  0.2123978294247889
iteration : 2131
train acc:  0.84375
train loss:  0.36339807510375977
train gradient:  0.2660979408731121
iteration : 2132
train acc:  0.90625
train loss:  0.23640738427639008
train gradient:  0.15727011462421175
iteration : 2133
train acc:  0.859375
train loss:  0.3660799562931061
train gradient:  0.31612827906018615
iteration : 2134
train acc:  0.84375
train loss:  0.3796263635158539
train gradient:  0.4750861163308467
iteration : 2135
train acc:  0.8671875
train loss:  0.31668829917907715
train gradient:  0.24549165821581978
iteration : 2136
train acc:  0.828125
train loss:  0.3226845860481262
train gradient:  0.24640955599514713
iteration : 2137
train acc:  0.84375
train loss:  0.35938119888305664
train gradient:  0.40437190223523706
iteration : 2138
train acc:  0.7734375
train loss:  0.481981098651886
train gradient:  0.504243062206593
iteration : 2139
train acc:  0.765625
train loss:  0.4829395115375519
train gradient:  0.4962921868910106
iteration : 2140
train acc:  0.7890625
train loss:  0.3986085057258606
train gradient:  0.3974684492518569
iteration : 2141
train acc:  0.828125
train loss:  0.3927961587905884
train gradient:  0.3983468146170472
iteration : 2142
train acc:  0.828125
train loss:  0.429587185382843
train gradient:  0.3084783676081543
iteration : 2143
train acc:  0.796875
train loss:  0.4414616525173187
train gradient:  0.4216941030529042
iteration : 2144
train acc:  0.8046875
train loss:  0.42968785762786865
train gradient:  0.4595283927827347
iteration : 2145
train acc:  0.796875
train loss:  0.4110206663608551
train gradient:  0.37257361234700126
iteration : 2146
train acc:  0.8359375
train loss:  0.315041184425354
train gradient:  0.30979879136067157
iteration : 2147
train acc:  0.7578125
train loss:  0.49436816573143005
train gradient:  0.5637912718981063
iteration : 2148
train acc:  0.8828125
train loss:  0.30935701727867126
train gradient:  0.2739037887020155
iteration : 2149
train acc:  0.7890625
train loss:  0.40227210521698
train gradient:  0.4594108200947789
iteration : 2150
train acc:  0.84375
train loss:  0.3433389365673065
train gradient:  0.4492037888689762
iteration : 2151
train acc:  0.828125
train loss:  0.35401973128318787
train gradient:  0.3784350534605417
iteration : 2152
train acc:  0.8671875
train loss:  0.3074256181716919
train gradient:  0.26115789082860774
iteration : 2153
train acc:  0.8828125
train loss:  0.2865646779537201
train gradient:  0.23084778855392446
iteration : 2154
train acc:  0.84375
train loss:  0.3803567886352539
train gradient:  0.27038301738157045
iteration : 2155
train acc:  0.84375
train loss:  0.3889651596546173
train gradient:  0.38151025729124916
iteration : 2156
train acc:  0.7890625
train loss:  0.4215085506439209
train gradient:  0.38058687441194183
iteration : 2157
train acc:  0.8515625
train loss:  0.3539447784423828
train gradient:  0.28180978975547766
iteration : 2158
train acc:  0.890625
train loss:  0.33679670095443726
train gradient:  0.3572230305191658
iteration : 2159
train acc:  0.859375
train loss:  0.3638618588447571
train gradient:  0.5010118040960556
iteration : 2160
train acc:  0.84375
train loss:  0.3608214855194092
train gradient:  0.3738632417494239
iteration : 2161
train acc:  0.8828125
train loss:  0.3483627438545227
train gradient:  0.22015519517731305
iteration : 2162
train acc:  0.8515625
train loss:  0.38068270683288574
train gradient:  0.4178329934700855
iteration : 2163
train acc:  0.796875
train loss:  0.39194005727767944
train gradient:  0.35975753250497317
iteration : 2164
train acc:  0.828125
train loss:  0.3649083971977234
train gradient:  0.4078384740270866
iteration : 2165
train acc:  0.7734375
train loss:  0.508210301399231
train gradient:  0.7876459448762455
iteration : 2166
train acc:  0.7890625
train loss:  0.41509562730789185
train gradient:  0.4275553035089663
iteration : 2167
train acc:  0.8203125
train loss:  0.3747003674507141
train gradient:  0.3994662527379571
iteration : 2168
train acc:  0.84375
train loss:  0.3224133849143982
train gradient:  0.259431060298253
iteration : 2169
train acc:  0.8359375
train loss:  0.44305312633514404
train gradient:  0.5933573474097493
iteration : 2170
train acc:  0.8515625
train loss:  0.37270352244377136
train gradient:  0.35194647416620706
iteration : 2171
train acc:  0.8515625
train loss:  0.3523079454898834
train gradient:  0.6061350857016858
iteration : 2172
train acc:  0.84375
train loss:  0.37102529406547546
train gradient:  0.3930792892192163
iteration : 2173
train acc:  0.796875
train loss:  0.41189485788345337
train gradient:  0.5819208072051765
iteration : 2174
train acc:  0.796875
train loss:  0.37634021043777466
train gradient:  0.5591507642787772
iteration : 2175
train acc:  0.8125
train loss:  0.39452624320983887
train gradient:  0.36484864587111604
iteration : 2176
train acc:  0.8046875
train loss:  0.3975762128829956
train gradient:  0.49957579427778925
iteration : 2177
train acc:  0.8515625
train loss:  0.3653762936592102
train gradient:  0.43800131331410574
iteration : 2178
train acc:  0.8359375
train loss:  0.38126707077026367
train gradient:  0.4249356612767241
iteration : 2179
train acc:  0.8203125
train loss:  0.41463765501976013
train gradient:  0.573728953611981
iteration : 2180
train acc:  0.8203125
train loss:  0.3590809106826782
train gradient:  0.299684503880389
iteration : 2181
train acc:  0.8359375
train loss:  0.4042731523513794
train gradient:  0.45434785541432426
iteration : 2182
train acc:  0.796875
train loss:  0.4738919138908386
train gradient:  0.5373558133648696
iteration : 2183
train acc:  0.8046875
train loss:  0.4976364076137543
train gradient:  0.5813249128412507
iteration : 2184
train acc:  0.8515625
train loss:  0.3706493675708771
train gradient:  0.40137997763363714
iteration : 2185
train acc:  0.84375
train loss:  0.3905244767665863
train gradient:  0.39934787924232157
iteration : 2186
train acc:  0.84375
train loss:  0.36582303047180176
train gradient:  0.34465921207533035
iteration : 2187
train acc:  0.7890625
train loss:  0.4777969717979431
train gradient:  0.5822506179591594
iteration : 2188
train acc:  0.8046875
train loss:  0.439425528049469
train gradient:  0.4449599801346146
iteration : 2189
train acc:  0.8515625
train loss:  0.3403249680995941
train gradient:  0.4385340723478842
iteration : 2190
train acc:  0.796875
train loss:  0.42611047625541687
train gradient:  0.4039743757050273
iteration : 2191
train acc:  0.796875
train loss:  0.4174600839614868
train gradient:  0.545381034570631
iteration : 2192
train acc:  0.765625
train loss:  0.5195661783218384
train gradient:  0.6357567382100862
iteration : 2193
train acc:  0.7109375
train loss:  0.5436078906059265
train gradient:  0.7672939966652168
iteration : 2194
train acc:  0.828125
train loss:  0.42175963521003723
train gradient:  0.4007918151619005
iteration : 2195
train acc:  0.828125
train loss:  0.36252331733703613
train gradient:  0.3473152435283936
iteration : 2196
train acc:  0.8671875
train loss:  0.36514759063720703
train gradient:  0.24725612763099442
iteration : 2197
train acc:  0.8359375
train loss:  0.3729436993598938
train gradient:  0.26862816273868123
iteration : 2198
train acc:  0.8515625
train loss:  0.35654059052467346
train gradient:  0.38750241503182825
iteration : 2199
train acc:  0.7578125
train loss:  0.461783230304718
train gradient:  0.394551254940131
iteration : 2200
train acc:  0.7265625
train loss:  0.4851476848125458
train gradient:  0.4333775845242057
iteration : 2201
train acc:  0.796875
train loss:  0.4124828577041626
train gradient:  0.3619598749030976
iteration : 2202
train acc:  0.8359375
train loss:  0.3460886478424072
train gradient:  0.31185361117221455
iteration : 2203
train acc:  0.90625
train loss:  0.29974091053009033
train gradient:  0.269533099279689
iteration : 2204
train acc:  0.8046875
train loss:  0.4023318290710449
train gradient:  0.34074274348048234
iteration : 2205
train acc:  0.8203125
train loss:  0.35723036527633667
train gradient:  0.2840369862926736
iteration : 2206
train acc:  0.8359375
train loss:  0.3581555485725403
train gradient:  0.3635223148212769
iteration : 2207
train acc:  0.7734375
train loss:  0.4538877010345459
train gradient:  0.39819288596137853
iteration : 2208
train acc:  0.78125
train loss:  0.41824087500572205
train gradient:  0.3532285364930673
iteration : 2209
train acc:  0.8515625
train loss:  0.3237244188785553
train gradient:  0.24760053790460773
iteration : 2210
train acc:  0.78125
train loss:  0.37847593426704407
train gradient:  0.34040290529985623
iteration : 2211
train acc:  0.8203125
train loss:  0.3931581377983093
train gradient:  0.34736262717705035
iteration : 2212
train acc:  0.8203125
train loss:  0.37476271390914917
train gradient:  0.3900600561834985
iteration : 2213
train acc:  0.7890625
train loss:  0.37445586919784546
train gradient:  0.3220114960848438
iteration : 2214
train acc:  0.8359375
train loss:  0.3484317362308502
train gradient:  0.2630685210742416
iteration : 2215
train acc:  0.8515625
train loss:  0.3857043385505676
train gradient:  0.27369417755025827
iteration : 2216
train acc:  0.8125
train loss:  0.3921230435371399
train gradient:  0.3199726094601565
iteration : 2217
train acc:  0.75
train loss:  0.45736077427864075
train gradient:  0.3967177587880747
iteration : 2218
train acc:  0.8203125
train loss:  0.39675402641296387
train gradient:  0.29373880288109977
iteration : 2219
train acc:  0.765625
train loss:  0.4738159775733948
train gradient:  11.346719276459021
iteration : 2220
train acc:  0.84375
train loss:  0.3427119553089142
train gradient:  0.36012841128090484
iteration : 2221
train acc:  0.8203125
train loss:  0.37848711013793945
train gradient:  0.50938227995318
iteration : 2222
train acc:  0.828125
train loss:  0.4250836968421936
train gradient:  0.3991228639010578
iteration : 2223
train acc:  0.8515625
train loss:  0.35035568475723267
train gradient:  0.34666222959896986
iteration : 2224
train acc:  0.765625
train loss:  0.39667829871177673
train gradient:  0.5422506000139296
iteration : 2225
train acc:  0.7890625
train loss:  0.38895487785339355
train gradient:  0.5403603824705134
iteration : 2226
train acc:  0.8203125
train loss:  0.45409783720970154
train gradient:  0.541730984171291
iteration : 2227
train acc:  0.8203125
train loss:  0.3613920211791992
train gradient:  0.4447044247821243
iteration : 2228
train acc:  0.78125
train loss:  0.4406328797340393
train gradient:  0.45024568809283755
iteration : 2229
train acc:  0.8125
train loss:  0.38082751631736755
train gradient:  0.5425747930370393
iteration : 2230
train acc:  0.875
train loss:  0.29900527000427246
train gradient:  0.3345754613063712
iteration : 2231
train acc:  0.8046875
train loss:  0.42464032769203186
train gradient:  0.5100970365832858
iteration : 2232
train acc:  0.8046875
train loss:  0.37908387184143066
train gradient:  0.3631233706992653
iteration : 2233
train acc:  0.890625
train loss:  0.33187514543533325
train gradient:  0.25741098336709545
iteration : 2234
train acc:  0.8359375
train loss:  0.3762369155883789
train gradient:  0.517357919425161
iteration : 2235
train acc:  0.796875
train loss:  0.4080190062522888
train gradient:  0.4107568499794022
iteration : 2236
train acc:  0.7890625
train loss:  0.4362594783306122
train gradient:  0.39378452178321033
iteration : 2237
train acc:  0.8125
train loss:  0.37613406777381897
train gradient:  0.39164993449736607
iteration : 2238
train acc:  0.84375
train loss:  0.35336124897003174
train gradient:  0.47896696853740506
iteration : 2239
train acc:  0.8515625
train loss:  0.34034663438796997
train gradient:  0.31550583529126536
iteration : 2240
train acc:  0.7578125
train loss:  0.4690127372741699
train gradient:  0.4545903072305875
iteration : 2241
train acc:  0.78125
train loss:  0.47909292578697205
train gradient:  0.4069599941363183
iteration : 2242
train acc:  0.796875
train loss:  0.3692223131656647
train gradient:  0.24093272221638112
iteration : 2243
train acc:  0.8828125
train loss:  0.31176891922950745
train gradient:  0.22474441884668944
iteration : 2244
train acc:  0.8515625
train loss:  0.39711740612983704
train gradient:  0.3379335650630985
iteration : 2245
train acc:  0.8671875
train loss:  0.32280656695365906
train gradient:  0.2185750439287919
iteration : 2246
train acc:  0.7890625
train loss:  0.4840192198753357
train gradient:  0.4700993323434164
iteration : 2247
train acc:  0.765625
train loss:  0.4621484875679016
train gradient:  0.46554020620173786
iteration : 2248
train acc:  0.7734375
train loss:  0.43972575664520264
train gradient:  0.5012838876938853
iteration : 2249
train acc:  0.8671875
train loss:  0.3290674090385437
train gradient:  0.3122091898012461
iteration : 2250
train acc:  0.8125
train loss:  0.38970473408699036
train gradient:  0.403652652828515
iteration : 2251
train acc:  0.75
train loss:  0.49966204166412354
train gradient:  0.5893017681877779
iteration : 2252
train acc:  0.8046875
train loss:  0.3955489695072174
train gradient:  0.4220925930571719
iteration : 2253
train acc:  0.7578125
train loss:  0.49933892488479614
train gradient:  0.5183661522190742
iteration : 2254
train acc:  0.796875
train loss:  0.40012818574905396
train gradient:  0.38334578066734204
iteration : 2255
train acc:  0.8203125
train loss:  0.41737258434295654
train gradient:  0.42665013167469806
iteration : 2256
train acc:  0.8515625
train loss:  0.3515280485153198
train gradient:  0.2526443905134341
iteration : 2257
train acc:  0.8359375
train loss:  0.3519565165042877
train gradient:  0.41005899468473794
iteration : 2258
train acc:  0.7890625
train loss:  0.4279545247554779
train gradient:  0.5642456496116701
iteration : 2259
train acc:  0.7890625
train loss:  0.3861923813819885
train gradient:  0.3120549208316273
iteration : 2260
train acc:  0.8515625
train loss:  0.3449571132659912
train gradient:  0.3012528897749431
iteration : 2261
train acc:  0.8203125
train loss:  0.3684931993484497
train gradient:  0.34200567510479596
iteration : 2262
train acc:  0.828125
train loss:  0.38840699195861816
train gradient:  0.44268374195959187
iteration : 2263
train acc:  0.8359375
train loss:  0.3805761933326721
train gradient:  0.25404426126196644
iteration : 2264
train acc:  0.84375
train loss:  0.38353174924850464
train gradient:  0.3544561338030959
iteration : 2265
train acc:  0.796875
train loss:  0.4182746410369873
train gradient:  0.3883538828858106
iteration : 2266
train acc:  0.8203125
train loss:  0.3874492645263672
train gradient:  0.3740159165899061
iteration : 2267
train acc:  0.7734375
train loss:  0.45090699195861816
train gradient:  0.406744923079529
iteration : 2268
train acc:  0.875
train loss:  0.3814661502838135
train gradient:  0.43799486196851334
iteration : 2269
train acc:  0.859375
train loss:  0.339590847492218
train gradient:  0.41034889785214496
iteration : 2270
train acc:  0.8046875
train loss:  0.3845757842063904
train gradient:  0.5279137541423742
iteration : 2271
train acc:  0.78125
train loss:  0.40338265895843506
train gradient:  0.3817854368164171
iteration : 2272
train acc:  0.8359375
train loss:  0.39288294315338135
train gradient:  0.3722345346858637
iteration : 2273
train acc:  0.796875
train loss:  0.4097106158733368
train gradient:  0.47923059364579346
iteration : 2274
train acc:  0.828125
train loss:  0.40394431352615356
train gradient:  0.41715789975842416
iteration : 2275
train acc:  0.875
train loss:  0.29284822940826416
train gradient:  0.3408099302541686
iteration : 2276
train acc:  0.84375
train loss:  0.3557169437408447
train gradient:  0.3175422923192403
iteration : 2277
train acc:  0.8203125
train loss:  0.4070781469345093
train gradient:  0.36915853013547373
iteration : 2278
train acc:  0.875
train loss:  0.28617507219314575
train gradient:  0.3354846078084025
iteration : 2279
train acc:  0.8125
train loss:  0.45249778032302856
train gradient:  0.47610556648198765
iteration : 2280
train acc:  0.8515625
train loss:  0.3684987425804138
train gradient:  0.3952798572349151
iteration : 2281
train acc:  0.84375
train loss:  0.37271684408187866
train gradient:  0.441414668164144
iteration : 2282
train acc:  0.859375
train loss:  0.33860182762145996
train gradient:  0.4118412520326267
iteration : 2283
train acc:  0.8125
train loss:  0.3487768769264221
train gradient:  0.38584169947206187
iteration : 2284
train acc:  0.8046875
train loss:  0.42494475841522217
train gradient:  0.5108744625299367
iteration : 2285
train acc:  0.8515625
train loss:  0.35017311573028564
train gradient:  0.30291048697126
iteration : 2286
train acc:  0.859375
train loss:  0.3334848880767822
train gradient:  0.3454215442696618
iteration : 2287
train acc:  0.828125
train loss:  0.41161417961120605
train gradient:  0.45861180309488553
iteration : 2288
train acc:  0.796875
train loss:  0.40956103801727295
train gradient:  0.48574424697414587
iteration : 2289
train acc:  0.7734375
train loss:  0.4279235005378723
train gradient:  0.5141082430719424
iteration : 2290
train acc:  0.7578125
train loss:  0.4491599202156067
train gradient:  0.48335232589275445
iteration : 2291
train acc:  0.8203125
train loss:  0.37535804510116577
train gradient:  0.42863905122828305
iteration : 2292
train acc:  0.8515625
train loss:  0.3645445704460144
train gradient:  0.32477821636337684
iteration : 2293
train acc:  0.8671875
train loss:  0.3336699306964874
train gradient:  0.32191586657591165
iteration : 2294
train acc:  0.8203125
train loss:  0.4382284879684448
train gradient:  0.42418233925971305
iteration : 2295
train acc:  0.859375
train loss:  0.3693804442882538
train gradient:  0.32632812366964853
iteration : 2296
train acc:  0.8046875
train loss:  0.3562299609184265
train gradient:  0.2931875678691036
iteration : 2297
train acc:  0.8671875
train loss:  0.31799086928367615
train gradient:  0.3573272134030612
iteration : 2298
train acc:  0.78125
train loss:  0.4288174510002136
train gradient:  0.6248072669012488
iteration : 2299
train acc:  0.8671875
train loss:  0.3020249009132385
train gradient:  0.3022408871538792
iteration : 2300
train acc:  0.8125
train loss:  0.3596152067184448
train gradient:  0.38222501222084193
iteration : 2301
train acc:  0.84375
train loss:  0.3961160182952881
train gradient:  0.4663881611822531
iteration : 2302
train acc:  0.8359375
train loss:  0.3812144994735718
train gradient:  0.4024453354932915
iteration : 2303
train acc:  0.828125
train loss:  0.4601413607597351
train gradient:  0.5261206326647441
iteration : 2304
train acc:  0.8828125
train loss:  0.32328206300735474
train gradient:  0.2945638849633685
iteration : 2305
train acc:  0.84375
train loss:  0.3393365740776062
train gradient:  0.3636034370099139
iteration : 2306
train acc:  0.8046875
train loss:  0.4011162519454956
train gradient:  0.5586346313868901
iteration : 2307
train acc:  0.828125
train loss:  0.40600597858428955
train gradient:  0.3948678150405646
iteration : 2308
train acc:  0.8828125
train loss:  0.3190210461616516
train gradient:  0.39535323155323193
iteration : 2309
train acc:  0.765625
train loss:  0.4809926748275757
train gradient:  0.5252529597373974
iteration : 2310
train acc:  0.8203125
train loss:  0.3619190454483032
train gradient:  0.4570451476150797
iteration : 2311
train acc:  0.84375
train loss:  0.355691134929657
train gradient:  0.3572417347867646
iteration : 2312
train acc:  0.8046875
train loss:  0.42756372690200806
train gradient:  0.4796309256328176
iteration : 2313
train acc:  0.8515625
train loss:  0.37247517704963684
train gradient:  0.36493566573033387
iteration : 2314
train acc:  0.8515625
train loss:  0.43967297673225403
train gradient:  0.6680849616942202
iteration : 2315
train acc:  0.84375
train loss:  0.35038864612579346
train gradient:  0.35686462177564504
iteration : 2316
train acc:  0.84375
train loss:  0.3269054591655731
train gradient:  0.3635665335667721
iteration : 2317
train acc:  0.828125
train loss:  0.38497868180274963
train gradient:  0.3717582544597925
iteration : 2318
train acc:  0.8203125
train loss:  0.42448484897613525
train gradient:  0.4235859551237059
iteration : 2319
train acc:  0.8515625
train loss:  0.3376839756965637
train gradient:  0.553153028237698
iteration : 2320
train acc:  0.796875
train loss:  0.44116777181625366
train gradient:  0.6240703115135058
iteration : 2321
train acc:  0.8515625
train loss:  0.3614298105239868
train gradient:  0.44300043520323124
iteration : 2322
train acc:  0.796875
train loss:  0.38676023483276367
train gradient:  0.3622873953777144
iteration : 2323
train acc:  0.859375
train loss:  0.37616485357284546
train gradient:  0.3368899300447727
iteration : 2324
train acc:  0.8515625
train loss:  0.4078289866447449
train gradient:  0.33513937292997115
iteration : 2325
train acc:  0.8359375
train loss:  0.3638225793838501
train gradient:  0.32025921268082985
iteration : 2326
train acc:  0.828125
train loss:  0.3631049394607544
train gradient:  0.6694888382476787
iteration : 2327
train acc:  0.796875
train loss:  0.4580395221710205
train gradient:  0.4444383519587449
iteration : 2328
train acc:  0.8359375
train loss:  0.3764384090900421
train gradient:  0.3226289601692472
iteration : 2329
train acc:  0.875
train loss:  0.32610830664634705
train gradient:  0.29832953624757924
iteration : 2330
train acc:  0.8671875
train loss:  0.3181721568107605
train gradient:  0.2651448450990987
iteration : 2331
train acc:  0.75
train loss:  0.5025820136070251
train gradient:  0.5796355628613239
iteration : 2332
train acc:  0.8046875
train loss:  0.3468688726425171
train gradient:  0.34466993893665315
iteration : 2333
train acc:  0.828125
train loss:  0.3105927109718323
train gradient:  0.4155323549249425
iteration : 2334
train acc:  0.765625
train loss:  0.44664233922958374
train gradient:  0.5099941535663917
iteration : 2335
train acc:  0.8125
train loss:  0.39436018466949463
train gradient:  0.39706248418613427
iteration : 2336
train acc:  0.8359375
train loss:  0.35801631212234497
train gradient:  0.4047872384916162
iteration : 2337
train acc:  0.8515625
train loss:  0.37885063886642456
train gradient:  0.43546384329607385
iteration : 2338
train acc:  0.7578125
train loss:  0.42506805062294006
train gradient:  0.5298156689862901
iteration : 2339
train acc:  0.84375
train loss:  0.3801347315311432
train gradient:  0.3627516808484319
iteration : 2340
train acc:  0.7890625
train loss:  0.4085398316383362
train gradient:  0.49665544323899585
iteration : 2341
train acc:  0.796875
train loss:  0.4030429720878601
train gradient:  0.347036053519011
iteration : 2342
train acc:  0.828125
train loss:  0.43152421712875366
train gradient:  0.5151329247553785
iteration : 2343
train acc:  0.796875
train loss:  0.45045578479766846
train gradient:  0.5202137315678355
iteration : 2344
train acc:  0.8203125
train loss:  0.37450820207595825
train gradient:  0.31173225976300595
iteration : 2345
train acc:  0.8046875
train loss:  0.3936721980571747
train gradient:  0.5180610777346066
iteration : 2346
train acc:  0.8671875
train loss:  0.32499825954437256
train gradient:  0.22699365146422815
iteration : 2347
train acc:  0.8359375
train loss:  0.47088563442230225
train gradient:  0.5808449915443286
iteration : 2348
train acc:  0.8359375
train loss:  0.39930158853530884
train gradient:  0.3834841697484904
iteration : 2349
train acc:  0.765625
train loss:  0.4033993184566498
train gradient:  0.5079522508670176
iteration : 2350
train acc:  0.8359375
train loss:  0.4257357120513916
train gradient:  0.43098642821237426
iteration : 2351
train acc:  0.8359375
train loss:  0.34625303745269775
train gradient:  0.3269698941523572
iteration : 2352
train acc:  0.8203125
train loss:  0.3944851756095886
train gradient:  0.42348418258874093
iteration : 2353
train acc:  0.8359375
train loss:  0.3949381709098816
train gradient:  0.3581265598325123
iteration : 2354
train acc:  0.7421875
train loss:  0.5500836372375488
train gradient:  0.5192228799781906
iteration : 2355
train acc:  0.8515625
train loss:  0.3831448256969452
train gradient:  0.36334078945613846
iteration : 2356
train acc:  0.8359375
train loss:  0.39053136110305786
train gradient:  0.2820188591463052
iteration : 2357
train acc:  0.796875
train loss:  0.45814692974090576
train gradient:  0.5379872654773525
iteration : 2358
train acc:  0.8359375
train loss:  0.3682609498500824
train gradient:  0.36410964011400193
iteration : 2359
train acc:  0.75
train loss:  0.4578597843647003
train gradient:  0.5984600117166206
iteration : 2360
train acc:  0.8203125
train loss:  0.42043977975845337
train gradient:  0.4973950221305319
iteration : 2361
train acc:  0.796875
train loss:  0.4066247344017029
train gradient:  0.44646260422433537
iteration : 2362
train acc:  0.828125
train loss:  0.3526015877723694
train gradient:  0.42191811844952615
iteration : 2363
train acc:  0.8828125
train loss:  0.35098397731781006
train gradient:  0.238637515854968
iteration : 2364
train acc:  0.7890625
train loss:  0.3808882534503937
train gradient:  0.3951346182482651
iteration : 2365
train acc:  0.890625
train loss:  0.27772635221481323
train gradient:  0.23053285975301935
iteration : 2366
train acc:  0.8046875
train loss:  0.40834498405456543
train gradient:  0.42496484687030356
iteration : 2367
train acc:  0.828125
train loss:  0.3893614411354065
train gradient:  0.32350401582803706
iteration : 2368
train acc:  0.8671875
train loss:  0.37030455470085144
train gradient:  0.4796203746584876
iteration : 2369
train acc:  0.875
train loss:  0.3253669738769531
train gradient:  0.44573484766216903
iteration : 2370
train acc:  0.7578125
train loss:  0.4793955683708191
train gradient:  0.5010307921020831
iteration : 2371
train acc:  0.7734375
train loss:  0.47684359550476074
train gradient:  0.5137177211298708
iteration : 2372
train acc:  0.8671875
train loss:  0.3850988447666168
train gradient:  0.3182940716351962
iteration : 2373
train acc:  0.8671875
train loss:  0.36071261763572693
train gradient:  0.3012373530648387
iteration : 2374
train acc:  0.8359375
train loss:  0.3682917654514313
train gradient:  0.2755531141808891
iteration : 2375
train acc:  0.828125
train loss:  0.443730890750885
train gradient:  0.3913481812275386
iteration : 2376
train acc:  0.8359375
train loss:  0.32087814807891846
train gradient:  0.3744765994663102
iteration : 2377
train acc:  0.8515625
train loss:  0.31567978858947754
train gradient:  0.2843120085544182
iteration : 2378
train acc:  0.84375
train loss:  0.3414427936077118
train gradient:  0.2508227377749676
iteration : 2379
train acc:  0.7421875
train loss:  0.4617811143398285
train gradient:  0.43044081268013273
iteration : 2380
train acc:  0.765625
train loss:  0.4273093342781067
train gradient:  0.5506672955208325
iteration : 2381
train acc:  0.7578125
train loss:  0.47679832577705383
train gradient:  0.40228664895127786
iteration : 2382
train acc:  0.875
train loss:  0.37363916635513306
train gradient:  0.29629351835659745
iteration : 2383
train acc:  0.828125
train loss:  0.4310792088508606
train gradient:  0.3683458299202403
iteration : 2384
train acc:  0.8359375
train loss:  0.34628111124038696
train gradient:  0.36319079016181915
iteration : 2385
train acc:  0.8203125
train loss:  0.36763638257980347
train gradient:  0.29551975242387224
iteration : 2386
train acc:  0.796875
train loss:  0.41001778841018677
train gradient:  0.3284740860109001
iteration : 2387
train acc:  0.84375
train loss:  0.40184152126312256
train gradient:  0.351328008796853
iteration : 2388
train acc:  0.8125
train loss:  0.3822147846221924
train gradient:  0.29966990616275235
iteration : 2389
train acc:  0.8828125
train loss:  0.30311280488967896
train gradient:  0.2164451220382522
iteration : 2390
train acc:  0.8046875
train loss:  0.35048550367355347
train gradient:  0.2664122632907309
iteration : 2391
train acc:  0.8046875
train loss:  0.391420841217041
train gradient:  0.3461924849913873
iteration : 2392
train acc:  0.8125
train loss:  0.3928571939468384
train gradient:  0.3292132407406795
iteration : 2393
train acc:  0.6953125
train loss:  0.6151007413864136
train gradient:  0.9618006631095372
iteration : 2394
train acc:  0.8203125
train loss:  0.42412886023521423
train gradient:  0.39010962744651045
iteration : 2395
train acc:  0.7890625
train loss:  0.40171632170677185
train gradient:  0.3226109062432539
iteration : 2396
train acc:  0.8359375
train loss:  0.34055179357528687
train gradient:  0.2515308096452951
iteration : 2397
train acc:  0.78125
train loss:  0.48045527935028076
train gradient:  0.501374124484101
iteration : 2398
train acc:  0.84375
train loss:  0.3563135266304016
train gradient:  0.3011023506971988
iteration : 2399
train acc:  0.8125
train loss:  0.3891206383705139
train gradient:  0.35651149251795566
iteration : 2400
train acc:  0.828125
train loss:  0.3486518859863281
train gradient:  0.31375735151197476
iteration : 2401
train acc:  0.828125
train loss:  0.3833540081977844
train gradient:  0.4155533492185232
iteration : 2402
train acc:  0.796875
train loss:  0.36997169256210327
train gradient:  0.6111407360449315
iteration : 2403
train acc:  0.8046875
train loss:  0.4793089032173157
train gradient:  0.5198111892041777
iteration : 2404
train acc:  0.7890625
train loss:  0.43136119842529297
train gradient:  0.5095784304109436
iteration : 2405
train acc:  0.828125
train loss:  0.3433248996734619
train gradient:  0.2617945603918584
iteration : 2406
train acc:  0.8359375
train loss:  0.4198310077190399
train gradient:  0.5746734604216382
iteration : 2407
train acc:  0.78125
train loss:  0.39564788341522217
train gradient:  0.33401045014391606
iteration : 2408
train acc:  0.828125
train loss:  0.37098485231399536
train gradient:  0.2883719067610978
iteration : 2409
train acc:  0.8046875
train loss:  0.39038076996803284
train gradient:  0.42099005704739006
iteration : 2410
train acc:  0.84375
train loss:  0.30592262744903564
train gradient:  0.21968684888991585
iteration : 2411
train acc:  0.828125
train loss:  0.3641008138656616
train gradient:  0.32797879649544937
iteration : 2412
train acc:  0.8125
train loss:  0.390641987323761
train gradient:  0.4750085748265071
iteration : 2413
train acc:  0.828125
train loss:  0.4080636501312256
train gradient:  0.43821358485349643
iteration : 2414
train acc:  0.84375
train loss:  0.32016879320144653
train gradient:  0.24744358615274054
iteration : 2415
train acc:  0.859375
train loss:  0.36128222942352295
train gradient:  0.36024932853758906
iteration : 2416
train acc:  0.828125
train loss:  0.34181737899780273
train gradient:  0.31608543035679776
iteration : 2417
train acc:  0.875
train loss:  0.33449769020080566
train gradient:  0.2935795055949212
iteration : 2418
train acc:  0.765625
train loss:  0.40278884768486023
train gradient:  0.3083287152001966
iteration : 2419
train acc:  0.9296875
train loss:  0.2455255389213562
train gradient:  0.2170315664773161
iteration : 2420
train acc:  0.84375
train loss:  0.36073392629623413
train gradient:  0.3290797160209767
iteration : 2421
train acc:  0.8359375
train loss:  0.42604684829711914
train gradient:  0.4157004779556679
iteration : 2422
train acc:  0.859375
train loss:  0.3696420192718506
train gradient:  0.41678483968414176
iteration : 2423
train acc:  0.84375
train loss:  0.3926464319229126
train gradient:  0.35805440266454847
iteration : 2424
train acc:  0.8203125
train loss:  0.38856232166290283
train gradient:  0.31926273470553346
iteration : 2425
train acc:  0.828125
train loss:  0.45059746503829956
train gradient:  0.5502411255478784
iteration : 2426
train acc:  0.8359375
train loss:  0.3857617974281311
train gradient:  0.3439710468930687
iteration : 2427
train acc:  0.859375
train loss:  0.33932238817214966
train gradient:  0.2852383768806995
iteration : 2428
train acc:  0.8203125
train loss:  0.41149312257766724
train gradient:  0.4514398963910601
iteration : 2429
train acc:  0.8125
train loss:  0.40134793519973755
train gradient:  0.4341168023253064
iteration : 2430
train acc:  0.7578125
train loss:  0.5583682060241699
train gradient:  0.9256953050966963
iteration : 2431
train acc:  0.7421875
train loss:  0.4484911561012268
train gradient:  0.46553375859111285
iteration : 2432
train acc:  0.8046875
train loss:  0.40651506185531616
train gradient:  0.39204864567569764
iteration : 2433
train acc:  0.8046875
train loss:  0.39499449729919434
train gradient:  0.31099964741128455
iteration : 2434
train acc:  0.8515625
train loss:  0.4301736354827881
train gradient:  0.39141231585441194
iteration : 2435
train acc:  0.8046875
train loss:  0.4897289574146271
train gradient:  0.4038744517795565
iteration : 2436
train acc:  0.8125
train loss:  0.3967607617378235
train gradient:  0.36754652165599033
iteration : 2437
train acc:  0.828125
train loss:  0.3728269636631012
train gradient:  0.34530500855280666
iteration : 2438
train acc:  0.828125
train loss:  0.36750850081443787
train gradient:  0.3851485808852771
iteration : 2439
train acc:  0.765625
train loss:  0.4508008062839508
train gradient:  0.47164899708911273
iteration : 2440
train acc:  0.8359375
train loss:  0.36189672350883484
train gradient:  0.2672030440489705
iteration : 2441
train acc:  0.8203125
train loss:  0.38661861419677734
train gradient:  0.23375363710128844
iteration : 2442
train acc:  0.859375
train loss:  0.3798733353614807
train gradient:  0.3186240616645502
iteration : 2443
train acc:  0.828125
train loss:  0.32410508394241333
train gradient:  0.25119266376363253
iteration : 2444
train acc:  0.7734375
train loss:  0.455916166305542
train gradient:  0.48349719116070344
iteration : 2445
train acc:  0.8359375
train loss:  0.3570936322212219
train gradient:  0.27065813076169415
iteration : 2446
train acc:  0.8125
train loss:  0.41953831911087036
train gradient:  0.3701268040541941
iteration : 2447
train acc:  0.84375
train loss:  0.3696432113647461
train gradient:  0.365500881569566
iteration : 2448
train acc:  0.8125
train loss:  0.39787766337394714
train gradient:  0.41934722899407234
iteration : 2449
train acc:  0.84375
train loss:  0.3934611976146698
train gradient:  0.3884499725303995
iteration : 2450
train acc:  0.7890625
train loss:  0.4579892158508301
train gradient:  0.48885353504133183
iteration : 2451
train acc:  0.859375
train loss:  0.351316899061203
train gradient:  0.30196023070938277
iteration : 2452
train acc:  0.8359375
train loss:  0.364737331867218
train gradient:  0.3390578110262405
iteration : 2453
train acc:  0.84375
train loss:  0.3590894639492035
train gradient:  0.33290465432573635
iteration : 2454
train acc:  0.828125
train loss:  0.3767373561859131
train gradient:  0.31448376959431057
iteration : 2455
train acc:  0.875
train loss:  0.3510313630104065
train gradient:  0.3353175787099663
iteration : 2456
train acc:  0.8125
train loss:  0.42163729667663574
train gradient:  0.3852078591790103
iteration : 2457
train acc:  0.859375
train loss:  0.3634369969367981
train gradient:  0.3190829328272214
iteration : 2458
train acc:  0.875
train loss:  0.3555242717266083
train gradient:  0.41788705622096095
iteration : 2459
train acc:  0.828125
train loss:  0.3424259424209595
train gradient:  0.25299013625115696
iteration : 2460
train acc:  0.8515625
train loss:  0.3441067337989807
train gradient:  0.3299011107651918
iteration : 2461
train acc:  0.828125
train loss:  0.44149601459503174
train gradient:  0.4009212947661861
iteration : 2462
train acc:  0.84375
train loss:  0.3788353204727173
train gradient:  0.3235899390125773
iteration : 2463
train acc:  0.8203125
train loss:  0.38179337978363037
train gradient:  0.422545080518808
iteration : 2464
train acc:  0.8203125
train loss:  0.43133804202079773
train gradient:  0.43222539873057925
iteration : 2465
train acc:  0.796875
train loss:  0.4063931405544281
train gradient:  0.4078394525784711
iteration : 2466
train acc:  0.859375
train loss:  0.32886648178100586
train gradient:  0.24599141999272275
iteration : 2467
train acc:  0.7890625
train loss:  0.44087424874305725
train gradient:  0.3726662696522576
iteration : 2468
train acc:  0.8046875
train loss:  0.4421534538269043
train gradient:  0.37336419028351164
iteration : 2469
train acc:  0.8203125
train loss:  0.37612926959991455
train gradient:  0.3587221426362275
iteration : 2470
train acc:  0.78125
train loss:  0.40822848677635193
train gradient:  0.4614821516812478
iteration : 2471
train acc:  0.796875
train loss:  0.42472800612449646
train gradient:  0.34919281128607993
iteration : 2472
train acc:  0.84375
train loss:  0.34274861216545105
train gradient:  0.3142784503217967
iteration : 2473
train acc:  0.8203125
train loss:  0.4079053997993469
train gradient:  0.5163597834694411
iteration : 2474
train acc:  0.796875
train loss:  0.43426674604415894
train gradient:  0.41741148383561794
iteration : 2475
train acc:  0.828125
train loss:  0.38780131936073303
train gradient:  0.3906432061976301
iteration : 2476
train acc:  0.8359375
train loss:  0.367724746465683
train gradient:  0.46708535098667475
iteration : 2477
train acc:  0.828125
train loss:  0.34381479024887085
train gradient:  0.270480961102479
iteration : 2478
train acc:  0.828125
train loss:  0.4232672154903412
train gradient:  0.4869679812772301
iteration : 2479
train acc:  0.859375
train loss:  0.31170716881752014
train gradient:  0.27942702450399964
iteration : 2480
train acc:  0.875
train loss:  0.3473805785179138
train gradient:  0.32836087877127423
iteration : 2481
train acc:  0.765625
train loss:  0.4394587278366089
train gradient:  0.5078362160815655
iteration : 2482
train acc:  0.84375
train loss:  0.3779665231704712
train gradient:  0.3146045064231382
iteration : 2483
train acc:  0.8671875
train loss:  0.3614262342453003
train gradient:  0.3749995209249126
iteration : 2484
train acc:  0.8203125
train loss:  0.40390467643737793
train gradient:  0.3256431400759826
iteration : 2485
train acc:  0.8203125
train loss:  0.3589506447315216
train gradient:  0.3221880917368918
iteration : 2486
train acc:  0.84375
train loss:  0.35103505849838257
train gradient:  0.20618503051638057
iteration : 2487
train acc:  0.8359375
train loss:  0.3428998291492462
train gradient:  0.33954413042383375
iteration : 2488
train acc:  0.8203125
train loss:  0.3922538161277771
train gradient:  0.3124854594913643
iteration : 2489
train acc:  0.8203125
train loss:  0.3974820673465729
train gradient:  0.32607593647766114
iteration : 2490
train acc:  0.828125
train loss:  0.3457332253456116
train gradient:  0.31736469503034764
iteration : 2491
train acc:  0.859375
train loss:  0.3605683445930481
train gradient:  0.291435799602934
iteration : 2492
train acc:  0.8046875
train loss:  0.4431334137916565
train gradient:  0.35443156320235625
iteration : 2493
train acc:  0.84375
train loss:  0.32974353432655334
train gradient:  0.32696086209125375
iteration : 2494
train acc:  0.8203125
train loss:  0.4117104709148407
train gradient:  0.41980101445909873
iteration : 2495
train acc:  0.8046875
train loss:  0.38398751616477966
train gradient:  0.3935715189767407
iteration : 2496
train acc:  0.8203125
train loss:  0.3681824505329132
train gradient:  0.36331306962342025
iteration : 2497
train acc:  0.890625
train loss:  0.28251853585243225
train gradient:  0.24312926752961567
iteration : 2498
train acc:  0.8515625
train loss:  0.4281432032585144
train gradient:  0.3672872008492266
iteration : 2499
train acc:  0.75
train loss:  0.5404231548309326
train gradient:  0.5257837023337941
iteration : 2500
train acc:  0.8203125
train loss:  0.38076120615005493
train gradient:  0.33890419502571645
iteration : 2501
train acc:  0.859375
train loss:  0.31909483671188354
train gradient:  0.21738610571109576
iteration : 2502
train acc:  0.8515625
train loss:  0.413394033908844
train gradient:  0.6209594121483537
iteration : 2503
train acc:  0.7890625
train loss:  0.43239811062812805
train gradient:  0.3821378744442554
iteration : 2504
train acc:  0.7421875
train loss:  0.5412719249725342
train gradient:  0.7300209353985478
iteration : 2505
train acc:  0.765625
train loss:  0.4945831000804901
train gradient:  0.4695554489418136
iteration : 2506
train acc:  0.8359375
train loss:  0.37033259868621826
train gradient:  0.36689512250109496
iteration : 2507
train acc:  0.8046875
train loss:  0.39885634183883667
train gradient:  0.40889248372544745
iteration : 2508
train acc:  0.8046875
train loss:  0.4027252495288849
train gradient:  0.539593007370406
iteration : 2509
train acc:  0.8671875
train loss:  0.3430265188217163
train gradient:  0.28765331985080317
iteration : 2510
train acc:  0.7734375
train loss:  0.40878725051879883
train gradient:  0.34711994426473586
iteration : 2511
train acc:  0.8828125
train loss:  0.3204931914806366
train gradient:  0.3114145187916361
iteration : 2512
train acc:  0.8671875
train loss:  0.3384026288986206
train gradient:  0.3272320138146968
iteration : 2513
train acc:  0.8671875
train loss:  0.32379817962646484
train gradient:  0.3377275834108436
iteration : 2514
train acc:  0.8671875
train loss:  0.3078586459159851
train gradient:  0.2413336594398902
iteration : 2515
train acc:  0.828125
train loss:  0.33932918310165405
train gradient:  0.36276853874736137
iteration : 2516
train acc:  0.84375
train loss:  0.4667835533618927
train gradient:  0.3716633991643488
iteration : 2517
train acc:  0.8203125
train loss:  0.3985283374786377
train gradient:  0.2555412710103078
iteration : 2518
train acc:  0.8359375
train loss:  0.3311508297920227
train gradient:  0.2387912905822624
iteration : 2519
train acc:  0.8046875
train loss:  0.39798110723495483
train gradient:  0.34859629226490874
iteration : 2520
train acc:  0.828125
train loss:  0.4171634912490845
train gradient:  0.4081090428341268
iteration : 2521
train acc:  0.8203125
train loss:  0.38885748386383057
train gradient:  0.32703020299726726
iteration : 2522
train acc:  0.828125
train loss:  0.3509082794189453
train gradient:  0.348234798694498
iteration : 2523
train acc:  0.84375
train loss:  0.37739184498786926
train gradient:  0.2262988343355733
iteration : 2524
train acc:  0.8203125
train loss:  0.3730407953262329
train gradient:  0.22932514280818284
iteration : 2525
train acc:  0.796875
train loss:  0.42520397901535034
train gradient:  0.45466342926400344
iteration : 2526
train acc:  0.875
train loss:  0.30695927143096924
train gradient:  0.25776005233017085
iteration : 2527
train acc:  0.84375
train loss:  0.39566347002983093
train gradient:  0.3371803552765707
iteration : 2528
train acc:  0.8359375
train loss:  0.3724643886089325
train gradient:  0.30615125229688817
iteration : 2529
train acc:  0.84375
train loss:  0.34347090125083923
train gradient:  0.28597982909606545
iteration : 2530
train acc:  0.859375
train loss:  0.3596843183040619
train gradient:  0.2858842254254963
iteration : 2531
train acc:  0.890625
train loss:  0.3309769034385681
train gradient:  0.31377169798356314
iteration : 2532
train acc:  0.84375
train loss:  0.3671034872531891
train gradient:  0.26253599650305015
iteration : 2533
train acc:  0.84375
train loss:  0.31562116742134094
train gradient:  0.2952294513126639
iteration : 2534
train acc:  0.7890625
train loss:  0.4518857002258301
train gradient:  0.5090926760066378
iteration : 2535
train acc:  0.796875
train loss:  0.412996768951416
train gradient:  0.34108750912946606
iteration : 2536
train acc:  0.828125
train loss:  0.32780587673187256
train gradient:  0.28145801223130085
iteration : 2537
train acc:  0.7890625
train loss:  0.4178885221481323
train gradient:  0.5002768729796405
iteration : 2538
train acc:  0.8046875
train loss:  0.42463064193725586
train gradient:  0.38542177812058726
iteration : 2539
train acc:  0.8046875
train loss:  0.3636513352394104
train gradient:  0.3913367592176685
iteration : 2540
train acc:  0.828125
train loss:  0.3752000331878662
train gradient:  0.484653055948286
iteration : 2541
train acc:  0.78125
train loss:  0.4676574766635895
train gradient:  0.41881745485058125
iteration : 2542
train acc:  0.8046875
train loss:  0.41130369901657104
train gradient:  0.4288886233827581
iteration : 2543
train acc:  0.765625
train loss:  0.4398362636566162
train gradient:  0.4180869627360514
iteration : 2544
train acc:  0.8203125
train loss:  0.38378554582595825
train gradient:  0.4067039689287611
iteration : 2545
train acc:  0.828125
train loss:  0.40920332074165344
train gradient:  0.433049963875958
iteration : 2546
train acc:  0.8359375
train loss:  0.35854458808898926
train gradient:  0.40643927810236263
iteration : 2547
train acc:  0.8203125
train loss:  0.45128515362739563
train gradient:  0.4437626682600597
iteration : 2548
train acc:  0.8515625
train loss:  0.32920485734939575
train gradient:  0.24667312785332543
iteration : 2549
train acc:  0.796875
train loss:  0.3798235058784485
train gradient:  0.448596591577105
iteration : 2550
train acc:  0.8203125
train loss:  0.37625032663345337
train gradient:  0.3434594385124714
iteration : 2551
train acc:  0.8203125
train loss:  0.39341259002685547
train gradient:  0.3215180902923661
iteration : 2552
train acc:  0.8125
train loss:  0.41711971163749695
train gradient:  0.38413345550437694
iteration : 2553
train acc:  0.796875
train loss:  0.39832746982574463
train gradient:  0.4817813856514561
iteration : 2554
train acc:  0.828125
train loss:  0.3919828534126282
train gradient:  0.3323862276273605
iteration : 2555
train acc:  0.8046875
train loss:  0.3992871046066284
train gradient:  0.3869463767535136
iteration : 2556
train acc:  0.796875
train loss:  0.49776411056518555
train gradient:  0.501673848531882
iteration : 2557
train acc:  0.875
train loss:  0.3044183552265167
train gradient:  0.25686982523557
iteration : 2558
train acc:  0.84375
train loss:  0.3482581377029419
train gradient:  0.2261933050955548
iteration : 2559
train acc:  0.7890625
train loss:  0.37236157059669495
train gradient:  0.3608739542845158
iteration : 2560
train acc:  0.828125
train loss:  0.4006716310977936
train gradient:  0.38420732268800317
iteration : 2561
train acc:  0.859375
train loss:  0.3602967858314514
train gradient:  0.2688306940957777
iteration : 2562
train acc:  0.796875
train loss:  0.40429916977882385
train gradient:  0.33031307278544536
iteration : 2563
train acc:  0.8125
train loss:  0.43257004022598267
train gradient:  0.5109748242464804
iteration : 2564
train acc:  0.8203125
train loss:  0.40542030334472656
train gradient:  0.3075021579511845
iteration : 2565
train acc:  0.8515625
train loss:  0.30972498655319214
train gradient:  0.22991905090195403
iteration : 2566
train acc:  0.859375
train loss:  0.3226741850376129
train gradient:  0.2068753136051797
iteration : 2567
train acc:  0.828125
train loss:  0.36218321323394775
train gradient:  0.3182536239284257
iteration : 2568
train acc:  0.8203125
train loss:  0.3808087408542633
train gradient:  0.2577264860681607
iteration : 2569
train acc:  0.8359375
train loss:  0.41017287969589233
train gradient:  0.42952855825969966
iteration : 2570
train acc:  0.8359375
train loss:  0.3796462416648865
train gradient:  0.32549966045482503
iteration : 2571
train acc:  0.796875
train loss:  0.40753650665283203
train gradient:  0.3982235535009618
iteration : 2572
train acc:  0.7890625
train loss:  0.48344361782073975
train gradient:  0.5164001634115091
iteration : 2573
train acc:  0.875
train loss:  0.3131106495857239
train gradient:  0.24773917760007552
iteration : 2574
train acc:  0.8671875
train loss:  0.348503053188324
train gradient:  0.37178041140460305
iteration : 2575
train acc:  0.84375
train loss:  0.35855644941329956
train gradient:  0.3446957579674322
iteration : 2576
train acc:  0.8828125
train loss:  0.33374637365341187
train gradient:  0.2528672623450915
iteration : 2577
train acc:  0.84375
train loss:  0.35539788007736206
train gradient:  0.2935139764967661
iteration : 2578
train acc:  0.8515625
train loss:  0.3672971725463867
train gradient:  0.411123943213685
iteration : 2579
train acc:  0.84375
train loss:  0.34407126903533936
train gradient:  0.26976892646009126
iteration : 2580
train acc:  0.8125
train loss:  0.43117284774780273
train gradient:  0.40128395629222513
iteration : 2581
train acc:  0.8515625
train loss:  0.3313582241535187
train gradient:  0.31456947152039244
iteration : 2582
train acc:  0.765625
train loss:  0.44993460178375244
train gradient:  0.41960748073851123
iteration : 2583
train acc:  0.78125
train loss:  0.4692157804965973
train gradient:  0.6265806286850379
iteration : 2584
train acc:  0.859375
train loss:  0.3527272939682007
train gradient:  0.33236593602580355
iteration : 2585
train acc:  0.78125
train loss:  0.4432242214679718
train gradient:  0.38943200844761316
iteration : 2586
train acc:  0.8125
train loss:  0.4792681336402893
train gradient:  0.4572734356893664
iteration : 2587
train acc:  0.8125
train loss:  0.36913996934890747
train gradient:  0.3248259800318028
iteration : 2588
train acc:  0.796875
train loss:  0.3915083408355713
train gradient:  0.2948749468855148
iteration : 2589
train acc:  0.8125
train loss:  0.4232710301876068
train gradient:  0.34364452816823526
iteration : 2590
train acc:  0.84375
train loss:  0.3298236131668091
train gradient:  0.29179891354782655
iteration : 2591
train acc:  0.8203125
train loss:  0.39534997940063477
train gradient:  0.4792420788772626
iteration : 2592
train acc:  0.8671875
train loss:  0.3494829833507538
train gradient:  0.3424563323413261
iteration : 2593
train acc:  0.8359375
train loss:  0.3265438675880432
train gradient:  0.26267377215602244
iteration : 2594
train acc:  0.8203125
train loss:  0.434620201587677
train gradient:  0.3463386833499118
iteration : 2595
train acc:  0.8046875
train loss:  0.391517698764801
train gradient:  0.35369176346121034
iteration : 2596
train acc:  0.8203125
train loss:  0.3934638798236847
train gradient:  0.2708884526416877
iteration : 2597
train acc:  0.828125
train loss:  0.3631255030632019
train gradient:  0.42074051181473027
iteration : 2598
train acc:  0.828125
train loss:  0.41061392426490784
train gradient:  0.27528775470506756
iteration : 2599
train acc:  0.90625
train loss:  0.30055534839630127
train gradient:  0.28477173599007366
iteration : 2600
train acc:  0.875
train loss:  0.3873342275619507
train gradient:  0.6618613269581695
iteration : 2601
train acc:  0.8515625
train loss:  0.3821675777435303
train gradient:  0.31156799605118835
iteration : 2602
train acc:  0.7890625
train loss:  0.4070107340812683
train gradient:  0.3770183344318176
iteration : 2603
train acc:  0.8203125
train loss:  0.35745561122894287
train gradient:  0.2969321627177556
iteration : 2604
train acc:  0.765625
train loss:  0.49675077199935913
train gradient:  0.5644717747120811
iteration : 2605
train acc:  0.7890625
train loss:  0.48940086364746094
train gradient:  0.49221285159576883
iteration : 2606
train acc:  0.8515625
train loss:  0.43136104941368103
train gradient:  0.3805981788114117
iteration : 2607
train acc:  0.8046875
train loss:  0.36533641815185547
train gradient:  0.33134211506460126
iteration : 2608
train acc:  0.84375
train loss:  0.40262842178344727
train gradient:  0.3806278285910445
iteration : 2609
train acc:  0.8828125
train loss:  0.30851003527641296
train gradient:  0.23880289689006226
iteration : 2610
train acc:  0.8671875
train loss:  0.3179260194301605
train gradient:  0.31478829295189975
iteration : 2611
train acc:  0.796875
train loss:  0.4389767050743103
train gradient:  0.3155435025123234
iteration : 2612
train acc:  0.796875
train loss:  0.4063774347305298
train gradient:  0.3468149176035508
iteration : 2613
train acc:  0.8515625
train loss:  0.33651602268218994
train gradient:  0.2661229171683037
iteration : 2614
train acc:  0.8203125
train loss:  0.40736934542655945
train gradient:  0.37165361068961633
iteration : 2615
train acc:  0.84375
train loss:  0.3105155825614929
train gradient:  0.32799427487475374
iteration : 2616
train acc:  0.8515625
train loss:  0.3659641742706299
train gradient:  0.2621288067227841
iteration : 2617
train acc:  0.8359375
train loss:  0.4181312918663025
train gradient:  0.3708741720918463
iteration : 2618
train acc:  0.8046875
train loss:  0.3732459545135498
train gradient:  0.2980579026346157
iteration : 2619
train acc:  0.8203125
train loss:  0.3795168995857239
train gradient:  0.3414558594975237
iteration : 2620
train acc:  0.78125
train loss:  0.40879085659980774
train gradient:  0.3885467654764049
iteration : 2621
train acc:  0.8515625
train loss:  0.3553057909011841
train gradient:  0.31069406677645695
iteration : 2622
train acc:  0.765625
train loss:  0.4458127021789551
train gradient:  0.44170851752016427
iteration : 2623
train acc:  0.8359375
train loss:  0.3230462670326233
train gradient:  0.24325492354624195
iteration : 2624
train acc:  0.8359375
train loss:  0.40452468395233154
train gradient:  0.35379203496279765
iteration : 2625
train acc:  0.8515625
train loss:  0.38231319189071655
train gradient:  0.28190125133197813
iteration : 2626
train acc:  0.8046875
train loss:  0.3879530727863312
train gradient:  0.3349668823897995
iteration : 2627
train acc:  0.78125
train loss:  0.43847882747650146
train gradient:  0.3432667029790752
iteration : 2628
train acc:  0.796875
train loss:  0.4364813566207886
train gradient:  0.5014639719770312
iteration : 2629
train acc:  0.8203125
train loss:  0.3608771562576294
train gradient:  0.2514797324149037
iteration : 2630
train acc:  0.8203125
train loss:  0.34214574098587036
train gradient:  0.2172460774844313
iteration : 2631
train acc:  0.78125
train loss:  0.4127018451690674
train gradient:  0.3101697266052163
iteration : 2632
train acc:  0.8125
train loss:  0.4606861472129822
train gradient:  0.406111508959898
iteration : 2633
train acc:  0.828125
train loss:  0.4283187985420227
train gradient:  0.32523594131426903
iteration : 2634
train acc:  0.796875
train loss:  0.46409010887145996
train gradient:  0.45558258976219135
iteration : 2635
train acc:  0.8046875
train loss:  0.41126206517219543
train gradient:  0.30393904036177727
iteration : 2636
train acc:  0.78125
train loss:  0.4352174997329712
train gradient:  0.4566551551996558
iteration : 2637
train acc:  0.75
train loss:  0.49564963579177856
train gradient:  0.452163414474312
iteration : 2638
train acc:  0.828125
train loss:  0.3702598214149475
train gradient:  0.27681222243091225
iteration : 2639
train acc:  0.890625
train loss:  0.3530113697052002
train gradient:  0.36485010744012303
iteration : 2640
train acc:  0.8359375
train loss:  0.348062127828598
train gradient:  0.2336047404320658
iteration : 2641
train acc:  0.7890625
train loss:  0.3892911374568939
train gradient:  0.3209558057201975
iteration : 2642
train acc:  0.828125
train loss:  0.3745238184928894
train gradient:  0.24413101987328767
iteration : 2643
train acc:  0.75
train loss:  0.47577178478240967
train gradient:  0.4660418140798308
iteration : 2644
train acc:  0.796875
train loss:  0.46695882081985474
train gradient:  0.4669220015929916
iteration : 2645
train acc:  0.875
train loss:  0.31081482768058777
train gradient:  0.23521084431797726
iteration : 2646
train acc:  0.8828125
train loss:  0.34428948163986206
train gradient:  0.3172285138258593
iteration : 2647
train acc:  0.8515625
train loss:  0.31576859951019287
train gradient:  0.27538377119844054
iteration : 2648
train acc:  0.828125
train loss:  0.35248690843582153
train gradient:  0.2241066682321846
iteration : 2649
train acc:  0.828125
train loss:  0.4137396812438965
train gradient:  0.2938956111205333
iteration : 2650
train acc:  0.8046875
train loss:  0.4374977946281433
train gradient:  0.3332488493344908
iteration : 2651
train acc:  0.8203125
train loss:  0.32027971744537354
train gradient:  0.2195136672743767
iteration : 2652
train acc:  0.8671875
train loss:  0.3259620666503906
train gradient:  0.33005043721868993
iteration : 2653
train acc:  0.8828125
train loss:  0.2924906611442566
train gradient:  0.2071813180050849
iteration : 2654
train acc:  0.8515625
train loss:  0.3271729350090027
train gradient:  0.24652384741173464
iteration : 2655
train acc:  0.78125
train loss:  0.4439428150653839
train gradient:  0.45689136699246186
iteration : 2656
train acc:  0.8359375
train loss:  0.3926970064640045
train gradient:  0.28415548493173703
iteration : 2657
train acc:  0.8359375
train loss:  0.333954781293869
train gradient:  0.34395804961586285
iteration : 2658
train acc:  0.84375
train loss:  0.3593193292617798
train gradient:  0.3019437343504891
iteration : 2659
train acc:  0.796875
train loss:  0.4220195412635803
train gradient:  0.44209373228000987
iteration : 2660
train acc:  0.8203125
train loss:  0.3725689649581909
train gradient:  0.3323170731443676
iteration : 2661
train acc:  0.8125
train loss:  0.4312201738357544
train gradient:  0.3956722064130037
iteration : 2662
train acc:  0.8125
train loss:  0.4089006781578064
train gradient:  0.3834734251498509
iteration : 2663
train acc:  0.8046875
train loss:  0.3706345558166504
train gradient:  0.32665213837057755
iteration : 2664
train acc:  0.828125
train loss:  0.3789561986923218
train gradient:  0.27817902731872646
iteration : 2665
train acc:  0.859375
train loss:  0.3321229815483093
train gradient:  0.3234562455020389
iteration : 2666
train acc:  0.8046875
train loss:  0.3997728228569031
train gradient:  0.4238405167356763
iteration : 2667
train acc:  0.8671875
train loss:  0.33847275376319885
train gradient:  0.28863749681569784
iteration : 2668
train acc:  0.8671875
train loss:  0.3222047686576843
train gradient:  0.2505628533153977
iteration : 2669
train acc:  0.828125
train loss:  0.42592760920524597
train gradient:  0.4361128842692387
iteration : 2670
train acc:  0.78125
train loss:  0.45526066422462463
train gradient:  0.354157632393304
iteration : 2671
train acc:  0.8515625
train loss:  0.409927636384964
train gradient:  0.4181413146118594
iteration : 2672
train acc:  0.796875
train loss:  0.42307791113853455
train gradient:  0.3012114653330234
iteration : 2673
train acc:  0.7890625
train loss:  0.41720378398895264
train gradient:  0.3429127161132443
iteration : 2674
train acc:  0.765625
train loss:  0.44452759623527527
train gradient:  0.3688011422346897
iteration : 2675
train acc:  0.8671875
train loss:  0.32823607325553894
train gradient:  0.21421489687683498
iteration : 2676
train acc:  0.7890625
train loss:  0.40707552433013916
train gradient:  0.3277293151918708
iteration : 2677
train acc:  0.7890625
train loss:  0.3965846002101898
train gradient:  0.2727977132214844
iteration : 2678
train acc:  0.828125
train loss:  0.38468658924102783
train gradient:  0.2423878086119202
iteration : 2679
train acc:  0.8203125
train loss:  0.39228957891464233
train gradient:  0.3773656632000058
iteration : 2680
train acc:  0.8359375
train loss:  0.339125394821167
train gradient:  0.1972970974785993
iteration : 2681
train acc:  0.78125
train loss:  0.44530928134918213
train gradient:  0.5072904175924906
iteration : 2682
train acc:  0.8203125
train loss:  0.36542579531669617
train gradient:  0.20739724456147585
iteration : 2683
train acc:  0.8125
train loss:  0.3903132379055023
train gradient:  0.36431163721146365
iteration : 2684
train acc:  0.8203125
train loss:  0.4126264452934265
train gradient:  0.3106310501243751
iteration : 2685
train acc:  0.8515625
train loss:  0.40011847019195557
train gradient:  0.349703670181385
iteration : 2686
train acc:  0.8359375
train loss:  0.32342568039894104
train gradient:  0.20322642944388458
iteration : 2687
train acc:  0.8203125
train loss:  0.39421072602272034
train gradient:  0.3433162442068316
iteration : 2688
train acc:  0.796875
train loss:  0.4178178906440735
train gradient:  0.5833310673598984
iteration : 2689
train acc:  0.8828125
train loss:  0.3251892924308777
train gradient:  0.3038693213281096
iteration : 2690
train acc:  0.8203125
train loss:  0.40599238872528076
train gradient:  0.39573588153876726
iteration : 2691
train acc:  0.75
train loss:  0.44877874851226807
train gradient:  0.5387948395966896
iteration : 2692
train acc:  0.828125
train loss:  0.40467628836631775
train gradient:  0.35320046344048256
iteration : 2693
train acc:  0.84375
train loss:  0.39075717329978943
train gradient:  0.26174066534845003
iteration : 2694
train acc:  0.84375
train loss:  0.38808056712150574
train gradient:  0.29522428944720464
iteration : 2695
train acc:  0.8203125
train loss:  0.4102410078048706
train gradient:  0.4658772262739314
iteration : 2696
train acc:  0.8046875
train loss:  0.4106729030609131
train gradient:  0.3777269292996108
iteration : 2697
train acc:  0.8515625
train loss:  0.3676934838294983
train gradient:  0.3581850071301974
iteration : 2698
train acc:  0.8046875
train loss:  0.39975428581237793
train gradient:  0.47035074615409106
iteration : 2699
train acc:  0.765625
train loss:  0.4134587049484253
train gradient:  0.41101254125481285
iteration : 2700
train acc:  0.828125
train loss:  0.3746986985206604
train gradient:  0.37631431862548304
iteration : 2701
train acc:  0.8359375
train loss:  0.35026857256889343
train gradient:  0.3707893362488513
iteration : 2702
train acc:  0.765625
train loss:  0.4987431466579437
train gradient:  0.6097601219440716
iteration : 2703
train acc:  0.8515625
train loss:  0.36091792583465576
train gradient:  0.3413939808211394
iteration : 2704
train acc:  0.828125
train loss:  0.39307618141174316
train gradient:  0.31351404003882855
iteration : 2705
train acc:  0.765625
train loss:  0.4686804711818695
train gradient:  0.4293952206524527
iteration : 2706
train acc:  0.828125
train loss:  0.3834865689277649
train gradient:  0.30953830573601454
iteration : 2707
train acc:  0.8515625
train loss:  0.3589758276939392
train gradient:  0.3242110615771358
iteration : 2708
train acc:  0.7890625
train loss:  0.38101083040237427
train gradient:  0.39508865730680637
iteration : 2709
train acc:  0.796875
train loss:  0.3980209529399872
train gradient:  0.3934551718368133
iteration : 2710
train acc:  0.8046875
train loss:  0.44557082653045654
train gradient:  0.39567089270060996
iteration : 2711
train acc:  0.8359375
train loss:  0.3972499966621399
train gradient:  0.35290609199449013
iteration : 2712
train acc:  0.828125
train loss:  0.4193360507488251
train gradient:  0.3303391574766565
iteration : 2713
train acc:  0.8125
train loss:  0.37985876202583313
train gradient:  0.3112395480628535
iteration : 2714
train acc:  0.8125
train loss:  0.409196674823761
train gradient:  0.41136255326672205
iteration : 2715
train acc:  0.859375
train loss:  0.3266361951828003
train gradient:  0.2176772726753563
iteration : 2716
train acc:  0.875
train loss:  0.3620704114437103
train gradient:  0.30036291574523716
iteration : 2717
train acc:  0.8359375
train loss:  0.34811896085739136
train gradient:  0.28653836915031383
iteration : 2718
train acc:  0.859375
train loss:  0.3089562654495239
train gradient:  0.25375222008435605
iteration : 2719
train acc:  0.7421875
train loss:  0.5623288154602051
train gradient:  0.6896591736861855
iteration : 2720
train acc:  0.7421875
train loss:  0.4906997084617615
train gradient:  0.4406898371773729
iteration : 2721
train acc:  0.8046875
train loss:  0.44342583417892456
train gradient:  0.4596935406536956
iteration : 2722
train acc:  0.875
train loss:  0.31486573815345764
train gradient:  0.282395394930087
iteration : 2723
train acc:  0.8359375
train loss:  0.38282451033592224
train gradient:  0.29100795307593735
iteration : 2724
train acc:  0.859375
train loss:  0.3428311347961426
train gradient:  0.2815579953780065
iteration : 2725
train acc:  0.84375
train loss:  0.35708868503570557
train gradient:  0.24957162945107375
iteration : 2726
train acc:  0.8671875
train loss:  0.3288183808326721
train gradient:  0.28838350781241023
iteration : 2727
train acc:  0.828125
train loss:  0.37930363416671753
train gradient:  0.29216161688541875
iteration : 2728
train acc:  0.828125
train loss:  0.4049471914768219
train gradient:  0.2877210725916998
iteration : 2729
train acc:  0.859375
train loss:  0.3441771864891052
train gradient:  0.2311585362756428
iteration : 2730
train acc:  0.78125
train loss:  0.43843626976013184
train gradient:  0.5230378202257595
iteration : 2731
train acc:  0.8203125
train loss:  0.36303064227104187
train gradient:  0.27785537432482854
iteration : 2732
train acc:  0.8046875
train loss:  0.3631174564361572
train gradient:  0.2964583062694059
iteration : 2733
train acc:  0.7890625
train loss:  0.44072383642196655
train gradient:  0.3550594290909401
iteration : 2734
train acc:  0.8515625
train loss:  0.36600786447525024
train gradient:  0.39240682150208817
iteration : 2735
train acc:  0.8515625
train loss:  0.3467666506767273
train gradient:  0.5663022291864163
iteration : 2736
train acc:  0.8046875
train loss:  0.4288080930709839
train gradient:  0.585868423960844
iteration : 2737
train acc:  0.796875
train loss:  0.41044723987579346
train gradient:  0.3421549129669594
iteration : 2738
train acc:  0.8203125
train loss:  0.4278072118759155
train gradient:  0.4765394098363632
iteration : 2739
train acc:  0.8828125
train loss:  0.3417453467845917
train gradient:  0.32166403436854446
iteration : 2740
train acc:  0.8515625
train loss:  0.41203218698501587
train gradient:  0.4125143836803276
iteration : 2741
train acc:  0.875
train loss:  0.36435532569885254
train gradient:  0.2568052566223786
iteration : 2742
train acc:  0.859375
train loss:  0.32975584268569946
train gradient:  0.15935172844901552
iteration : 2743
train acc:  0.796875
train loss:  0.39656639099121094
train gradient:  0.4485018773071835
iteration : 2744
train acc:  0.828125
train loss:  0.3751005530357361
train gradient:  0.34647386438489397
iteration : 2745
train acc:  0.8359375
train loss:  0.3338918685913086
train gradient:  0.24905821128761177
iteration : 2746
train acc:  0.8203125
train loss:  0.39523404836654663
train gradient:  0.33733114092088634
iteration : 2747
train acc:  0.8203125
train loss:  0.39217859506607056
train gradient:  0.3798853079652163
iteration : 2748
train acc:  0.859375
train loss:  0.3299219012260437
train gradient:  0.2828418480982151
iteration : 2749
train acc:  0.828125
train loss:  0.3885084092617035
train gradient:  0.332426977867898
iteration : 2750
train acc:  0.8046875
train loss:  0.3578779101371765
train gradient:  0.23783354041173016
iteration : 2751
train acc:  0.828125
train loss:  0.38745975494384766
train gradient:  0.33207035632398496
iteration : 2752
train acc:  0.84375
train loss:  0.38780277967453003
train gradient:  0.3528734407555703
iteration : 2753
train acc:  0.8359375
train loss:  0.3959408402442932
train gradient:  0.28359291346968385
iteration : 2754
train acc:  0.8359375
train loss:  0.3970869183540344
train gradient:  0.4025032872581112
iteration : 2755
train acc:  0.890625
train loss:  0.3191152811050415
train gradient:  0.24879276698308203
iteration : 2756
train acc:  0.8515625
train loss:  0.34443724155426025
train gradient:  0.23639064089097225
iteration : 2757
train acc:  0.8671875
train loss:  0.3952460289001465
train gradient:  0.289388383151791
iteration : 2758
train acc:  0.78125
train loss:  0.4207477569580078
train gradient:  0.41061385496962416
iteration : 2759
train acc:  0.7734375
train loss:  0.5019338130950928
train gradient:  0.5225351289625814
iteration : 2760
train acc:  0.875
train loss:  0.373807817697525
train gradient:  0.44743990657482907
iteration : 2761
train acc:  0.828125
train loss:  0.40392357110977173
train gradient:  0.27264386266472784
iteration : 2762
train acc:  0.84375
train loss:  0.3413034677505493
train gradient:  0.38536541042238576
iteration : 2763
train acc:  0.8203125
train loss:  0.41330233216285706
train gradient:  0.3158299799442559
iteration : 2764
train acc:  0.8515625
train loss:  0.3437638282775879
train gradient:  0.3708442705916563
iteration : 2765
train acc:  0.828125
train loss:  0.37718820571899414
train gradient:  0.5714305960171757
iteration : 2766
train acc:  0.859375
train loss:  0.38713252544403076
train gradient:  0.3172636470083564
iteration : 2767
train acc:  0.8515625
train loss:  0.3308326005935669
train gradient:  0.37727264238552216
iteration : 2768
train acc:  0.84375
train loss:  0.44617581367492676
train gradient:  0.4275647074947539
iteration : 2769
train acc:  0.84375
train loss:  0.38252565264701843
train gradient:  0.2839957501078736
iteration : 2770
train acc:  0.84375
train loss:  0.39017051458358765
train gradient:  0.3915052371875512
iteration : 2771
train acc:  0.8984375
train loss:  0.24828718602657318
train gradient:  0.33196344969646674
iteration : 2772
train acc:  0.8671875
train loss:  0.3557544946670532
train gradient:  0.238821955572819
iteration : 2773
train acc:  0.859375
train loss:  0.3640401363372803
train gradient:  0.2135468473319011
iteration : 2774
train acc:  0.84375
train loss:  0.37998348474502563
train gradient:  0.31077550652166397
iteration : 2775
train acc:  0.84375
train loss:  0.3886975646018982
train gradient:  0.4519381430085351
iteration : 2776
train acc:  0.859375
train loss:  0.2887038588523865
train gradient:  0.20874981789063693
iteration : 2777
train acc:  0.875
train loss:  0.3394795060157776
train gradient:  0.35924246381608155
iteration : 2778
train acc:  0.8125
train loss:  0.3750463128089905
train gradient:  0.33378701464168203
iteration : 2779
train acc:  0.8046875
train loss:  0.36430227756500244
train gradient:  0.28144952873867324
iteration : 2780
train acc:  0.8125
train loss:  0.4002845883369446
train gradient:  0.6383889256166057
iteration : 2781
train acc:  0.8125
train loss:  0.4182986915111542
train gradient:  0.43083314424975666
iteration : 2782
train acc:  0.875
train loss:  0.4109152555465698
train gradient:  0.35385964038035117
iteration : 2783
train acc:  0.859375
train loss:  0.35120993852615356
train gradient:  0.3004634058295472
iteration : 2784
train acc:  0.78125
train loss:  0.3994804322719574
train gradient:  0.36995912368493505
iteration : 2785
train acc:  0.7734375
train loss:  0.4495173692703247
train gradient:  0.4775779583452122
iteration : 2786
train acc:  0.7734375
train loss:  0.4722263216972351
train gradient:  0.5412658090577636
iteration : 2787
train acc:  0.765625
train loss:  0.4625341296195984
train gradient:  0.499214926884269
iteration : 2788
train acc:  0.7890625
train loss:  0.4786987900733948
train gradient:  0.3650224586191459
iteration : 2789
train acc:  0.84375
train loss:  0.3647187650203705
train gradient:  0.33778890042594323
iteration : 2790
train acc:  0.765625
train loss:  0.4602779746055603
train gradient:  0.4941201588896339
iteration : 2791
train acc:  0.8046875
train loss:  0.39655500650405884
train gradient:  0.46090168521569286
iteration : 2792
train acc:  0.8125
train loss:  0.40872856974601746
train gradient:  0.347101415367005
iteration : 2793
train acc:  0.84375
train loss:  0.32209134101867676
train gradient:  0.23810847241118596
iteration : 2794
train acc:  0.8046875
train loss:  0.4085541367530823
train gradient:  0.2729059616803225
iteration : 2795
train acc:  0.7890625
train loss:  0.44409728050231934
train gradient:  0.5868152061508771
iteration : 2796
train acc:  0.8359375
train loss:  0.3829607367515564
train gradient:  0.3935589251018465
iteration : 2797
train acc:  0.859375
train loss:  0.3390926718711853
train gradient:  0.27830400231726077
iteration : 2798
train acc:  0.8359375
train loss:  0.40020060539245605
train gradient:  0.3136815809269369
iteration : 2799
train acc:  0.8359375
train loss:  0.35731202363967896
train gradient:  0.3422385058499351
iteration : 2800
train acc:  0.8046875
train loss:  0.4098758101463318
train gradient:  0.34171865853852573
iteration : 2801
train acc:  0.8828125
train loss:  0.3240273892879486
train gradient:  0.19666444946802508
iteration : 2802
train acc:  0.8359375
train loss:  0.432028591632843
train gradient:  0.3357712633776163
iteration : 2803
train acc:  0.7890625
train loss:  0.4080254137516022
train gradient:  0.3348433648717467
iteration : 2804
train acc:  0.8828125
train loss:  0.27579793334007263
train gradient:  0.20188070750463769
iteration : 2805
train acc:  0.7734375
train loss:  0.4743371903896332
train gradient:  0.4878365826075117
iteration : 2806
train acc:  0.8515625
train loss:  0.32858580350875854
train gradient:  0.236292267527503
iteration : 2807
train acc:  0.8671875
train loss:  0.36732903122901917
train gradient:  0.23199243550016513
iteration : 2808
train acc:  0.828125
train loss:  0.3736705183982849
train gradient:  0.2944112666303795
iteration : 2809
train acc:  0.8515625
train loss:  0.3623655438423157
train gradient:  0.2472184039079656
iteration : 2810
train acc:  0.890625
train loss:  0.3319956064224243
train gradient:  0.32414523356927816
iteration : 2811
train acc:  0.828125
train loss:  0.3902713358402252
train gradient:  0.3983811744729521
iteration : 2812
train acc:  0.734375
train loss:  0.532381534576416
train gradient:  0.4766337058733608
iteration : 2813
train acc:  0.8515625
train loss:  0.3868103325366974
train gradient:  0.3144874204599802
iteration : 2814
train acc:  0.8125
train loss:  0.3931654095649719
train gradient:  0.28627240215591887
iteration : 2815
train acc:  0.796875
train loss:  0.40716809034347534
train gradient:  0.3132034404978842
iteration : 2816
train acc:  0.7734375
train loss:  0.49439874291419983
train gradient:  0.5748006745926358
iteration : 2817
train acc:  0.84375
train loss:  0.340920627117157
train gradient:  0.2566870396116783
iteration : 2818
train acc:  0.796875
train loss:  0.42818260192871094
train gradient:  0.34886599921078487
iteration : 2819
train acc:  0.8828125
train loss:  0.30238354206085205
train gradient:  0.21901965899444428
iteration : 2820
train acc:  0.8203125
train loss:  0.39115995168685913
train gradient:  0.3598828428091522
iteration : 2821
train acc:  0.859375
train loss:  0.31650376319885254
train gradient:  0.2707932728963901
iteration : 2822
train acc:  0.7890625
train loss:  0.43084651231765747
train gradient:  0.39361880040684005
iteration : 2823
train acc:  0.859375
train loss:  0.31644725799560547
train gradient:  0.2401287977497401
iteration : 2824
train acc:  0.828125
train loss:  0.3217031955718994
train gradient:  0.2539706610993831
iteration : 2825
train acc:  0.84375
train loss:  0.3572764992713928
train gradient:  0.3531143026748951
iteration : 2826
train acc:  0.8046875
train loss:  0.39430907368659973
train gradient:  0.33794340071643625
iteration : 2827
train acc:  0.8515625
train loss:  0.33037424087524414
train gradient:  0.27729925406529393
iteration : 2828
train acc:  0.8203125
train loss:  0.36799466609954834
train gradient:  0.334331273432504
iteration : 2829
train acc:  0.8984375
train loss:  0.28005319833755493
train gradient:  0.21438052054106224
iteration : 2830
train acc:  0.84375
train loss:  0.35689735412597656
train gradient:  0.2521597520031489
iteration : 2831
train acc:  0.7890625
train loss:  0.4047118127346039
train gradient:  0.41373980275165007
iteration : 2832
train acc:  0.8671875
train loss:  0.337750643491745
train gradient:  0.25579977515812635
iteration : 2833
train acc:  0.8046875
train loss:  0.3927220106124878
train gradient:  0.4860681885303933
iteration : 2834
train acc:  0.828125
train loss:  0.40596672892570496
train gradient:  0.3771433604481238
iteration : 2835
train acc:  0.8046875
train loss:  0.44119977951049805
train gradient:  0.4910932345000325
iteration : 2836
train acc:  0.8125
train loss:  0.4490724802017212
train gradient:  0.3488627823517758
iteration : 2837
train acc:  0.84375
train loss:  0.4127630591392517
train gradient:  0.3397248037347765
iteration : 2838
train acc:  0.7578125
train loss:  0.4345155954360962
train gradient:  0.5940992225834996
iteration : 2839
train acc:  0.8203125
train loss:  0.39551860094070435
train gradient:  0.3735186726553895
iteration : 2840
train acc:  0.875
train loss:  0.3502548336982727
train gradient:  0.4566929170539789
iteration : 2841
train acc:  0.828125
train loss:  0.38466158509254456
train gradient:  0.2730265825846391
iteration : 2842
train acc:  0.7734375
train loss:  0.4453609585762024
train gradient:  0.3641594883717757
iteration : 2843
train acc:  0.8125
train loss:  0.4876013398170471
train gradient:  0.5340354365423534
iteration : 2844
train acc:  0.8515625
train loss:  0.3471975326538086
train gradient:  0.2804369989034227
iteration : 2845
train acc:  0.84375
train loss:  0.35589516162872314
train gradient:  0.3749788381552691
iteration : 2846
train acc:  0.859375
train loss:  0.3459888696670532
train gradient:  0.34286049582013656
iteration : 2847
train acc:  0.8125
train loss:  0.4305180311203003
train gradient:  0.45249547252628736
iteration : 2848
train acc:  0.8203125
train loss:  0.37080663442611694
train gradient:  0.33434604004019536
iteration : 2849
train acc:  0.8125
train loss:  0.43004295229911804
train gradient:  0.35137293810783315
iteration : 2850
train acc:  0.7734375
train loss:  0.48587656021118164
train gradient:  0.4641048399657647
iteration : 2851
train acc:  0.8046875
train loss:  0.39092880487442017
train gradient:  0.3359904995017087
iteration : 2852
train acc:  0.84375
train loss:  0.3403554856777191
train gradient:  0.28168003188658103
iteration : 2853
train acc:  0.8828125
train loss:  0.317844033241272
train gradient:  0.2395469588301585
iteration : 2854
train acc:  0.8125
train loss:  0.42621517181396484
train gradient:  0.459043377018252
iteration : 2855
train acc:  0.859375
train loss:  0.36281847953796387
train gradient:  0.23118280055639387
iteration : 2856
train acc:  0.8125
train loss:  0.5158913731575012
train gradient:  0.6497386792782971
iteration : 2857
train acc:  0.8359375
train loss:  0.4092266857624054
train gradient:  0.4218903396610269
iteration : 2858
train acc:  0.921875
train loss:  0.24732089042663574
train gradient:  0.21373129585039508
iteration : 2859
train acc:  0.7421875
train loss:  0.4686722755432129
train gradient:  0.4335487248691596
iteration : 2860
train acc:  0.859375
train loss:  0.35492485761642456
train gradient:  0.27645216938857514
iteration : 2861
train acc:  0.8515625
train loss:  0.32179686427116394
train gradient:  0.25091450933781173
iteration : 2862
train acc:  0.90625
train loss:  0.2670047879219055
train gradient:  0.27036582299521483
iteration : 2863
train acc:  0.78125
train loss:  0.46431758999824524
train gradient:  0.37549214199859215
iteration : 2864
train acc:  0.7734375
train loss:  0.44135987758636475
train gradient:  0.3890124700200681
iteration : 2865
train acc:  0.859375
train loss:  0.3517058491706848
train gradient:  0.5595293512678099
iteration : 2866
train acc:  0.828125
train loss:  0.3531678318977356
train gradient:  0.2747234631542918
iteration : 2867
train acc:  0.796875
train loss:  0.4395219087600708
train gradient:  0.3654326056141143
iteration : 2868
train acc:  0.7890625
train loss:  0.4613252282142639
train gradient:  0.5014222200904447
iteration : 2869
train acc:  0.8828125
train loss:  0.2856970727443695
train gradient:  0.2836768669805132
iteration : 2870
train acc:  0.8671875
train loss:  0.3340296745300293
train gradient:  0.20928631246047752
iteration : 2871
train acc:  0.8359375
train loss:  0.3530702590942383
train gradient:  0.3318230330128418
iteration : 2872
train acc:  0.8671875
train loss:  0.3010079860687256
train gradient:  0.269202857829289
iteration : 2873
train acc:  0.796875
train loss:  0.4332362115383148
train gradient:  0.33028899320933974
iteration : 2874
train acc:  0.8671875
train loss:  0.33548223972320557
train gradient:  0.27794413191688877
iteration : 2875
train acc:  0.828125
train loss:  0.3484048843383789
train gradient:  0.40945446254567935
iteration : 2876
train acc:  0.75
train loss:  0.45998573303222656
train gradient:  0.5143435789080135
iteration : 2877
train acc:  0.859375
train loss:  0.36142879724502563
train gradient:  0.28670701936690335
iteration : 2878
train acc:  0.796875
train loss:  0.4147460162639618
train gradient:  0.35160597057330994
iteration : 2879
train acc:  0.8046875
train loss:  0.4338224232196808
train gradient:  0.42581724236300544
iteration : 2880
train acc:  0.8671875
train loss:  0.33568763732910156
train gradient:  0.30708454331851914
iteration : 2881
train acc:  0.84375
train loss:  0.3553570806980133
train gradient:  0.35565245468623713
iteration : 2882
train acc:  0.8828125
train loss:  0.3136332929134369
train gradient:  0.2137769567318119
iteration : 2883
train acc:  0.8203125
train loss:  0.3487035632133484
train gradient:  0.3429525953835969
iteration : 2884
train acc:  0.875
train loss:  0.35080716013908386
train gradient:  0.3850823502638512
iteration : 2885
train acc:  0.859375
train loss:  0.3341783881187439
train gradient:  0.2648753677408144
iteration : 2886
train acc:  0.84375
train loss:  0.343205988407135
train gradient:  0.3329174598237184
iteration : 2887
train acc:  0.8515625
train loss:  0.36781543493270874
train gradient:  0.35952353276738735
iteration : 2888
train acc:  0.8515625
train loss:  0.3509845733642578
train gradient:  0.3540956130593982
iteration : 2889
train acc:  0.7734375
train loss:  0.4637051522731781
train gradient:  0.433985038110438
iteration : 2890
train acc:  0.8203125
train loss:  0.42348694801330566
train gradient:  0.46432244141598594
iteration : 2891
train acc:  0.859375
train loss:  0.35333314538002014
train gradient:  0.2317339977623088
iteration : 2892
train acc:  0.8125
train loss:  0.42618095874786377
train gradient:  0.3411213524738246
iteration : 2893
train acc:  0.8125
train loss:  0.4987031817436218
train gradient:  0.6542417418127213
iteration : 2894
train acc:  0.8671875
train loss:  0.39013075828552246
train gradient:  0.4564216338795771
iteration : 2895
train acc:  0.7890625
train loss:  0.4099171459674835
train gradient:  0.4484475370467919
iteration : 2896
train acc:  0.84375
train loss:  0.37322288751602173
train gradient:  0.41218096360284406
iteration : 2897
train acc:  0.8046875
train loss:  0.44159889221191406
train gradient:  0.4854152869382521
iteration : 2898
train acc:  0.84375
train loss:  0.3690626621246338
train gradient:  0.34563687650817193
iteration : 2899
train acc:  0.890625
train loss:  0.3227827548980713
train gradient:  0.29762994135208726
iteration : 2900
train acc:  0.8984375
train loss:  0.2958425283432007
train gradient:  0.3144485502714465
iteration : 2901
train acc:  0.7890625
train loss:  0.43998250365257263
train gradient:  0.5453755025029657
iteration : 2902
train acc:  0.8515625
train loss:  0.38264644145965576
train gradient:  0.37683453913492326
iteration : 2903
train acc:  0.8046875
train loss:  0.45968273282051086
train gradient:  0.47009576741605796
iteration : 2904
train acc:  0.8046875
train loss:  0.4319397211074829
train gradient:  0.4620369726808766
iteration : 2905
train acc:  0.84375
train loss:  0.3349493741989136
train gradient:  0.3785141180420115
iteration : 2906
train acc:  0.8125
train loss:  0.3893018960952759
train gradient:  0.26973594173008986
iteration : 2907
train acc:  0.8359375
train loss:  0.3917771577835083
train gradient:  0.3519565334593373
iteration : 2908
train acc:  0.84375
train loss:  0.3545747399330139
train gradient:  0.3112971203561711
iteration : 2909
train acc:  0.8125
train loss:  0.4004051089286804
train gradient:  0.5645084102377532
iteration : 2910
train acc:  0.8125
train loss:  0.4279641807079315
train gradient:  0.4098319642172138
iteration : 2911
train acc:  0.828125
train loss:  0.37779808044433594
train gradient:  0.28876628677505745
iteration : 2912
train acc:  0.828125
train loss:  0.3890407383441925
train gradient:  0.27852012642855367
iteration : 2913
train acc:  0.890625
train loss:  0.31580275297164917
train gradient:  0.20248614152737796
iteration : 2914
train acc:  0.84375
train loss:  0.37626713514328003
train gradient:  0.21868856911318657
iteration : 2915
train acc:  0.8671875
train loss:  0.3271259069442749
train gradient:  0.19989218799858247
iteration : 2916
train acc:  0.8125
train loss:  0.4354265332221985
train gradient:  0.363444452516331
iteration : 2917
train acc:  0.8984375
train loss:  0.2798802852630615
train gradient:  0.18330736569919076
iteration : 2918
train acc:  0.8046875
train loss:  0.4283527135848999
train gradient:  0.3273771019120553
iteration : 2919
train acc:  0.8359375
train loss:  0.4486194849014282
train gradient:  0.4296403572238831
iteration : 2920
train acc:  0.8203125
train loss:  0.38957053422927856
train gradient:  0.23081830739486817
iteration : 2921
train acc:  0.875
train loss:  0.3126336634159088
train gradient:  0.27800043421615234
iteration : 2922
train acc:  0.78125
train loss:  0.43896302580833435
train gradient:  0.40005718332227114
iteration : 2923
train acc:  0.7890625
train loss:  0.42133358120918274
train gradient:  0.3807726346789501
iteration : 2924
train acc:  0.859375
train loss:  0.30449050664901733
train gradient:  0.2347403572201354
iteration : 2925
train acc:  0.8671875
train loss:  0.3690152168273926
train gradient:  0.2510253367430041
iteration : 2926
train acc:  0.78125
train loss:  0.48563897609710693
train gradient:  0.5097119143012252
iteration : 2927
train acc:  0.8125
train loss:  0.41681548953056335
train gradient:  0.35328791416424593
iteration : 2928
train acc:  0.8515625
train loss:  0.3137359321117401
train gradient:  0.30717760066517424
iteration : 2929
train acc:  0.875
train loss:  0.2939848303794861
train gradient:  0.22675461065116026
iteration : 2930
train acc:  0.8046875
train loss:  0.4220919907093048
train gradient:  0.5155955323921817
iteration : 2931
train acc:  0.8671875
train loss:  0.3079557716846466
train gradient:  0.20098569574860412
iteration : 2932
train acc:  0.7890625
train loss:  0.47073477506637573
train gradient:  0.5306018981036759
iteration : 2933
train acc:  0.859375
train loss:  0.29953521490097046
train gradient:  0.2843228457658912
iteration : 2934
train acc:  0.890625
train loss:  0.2879149317741394
train gradient:  0.19190648666588786
iteration : 2935
train acc:  0.796875
train loss:  0.3466153144836426
train gradient:  0.32009717339625404
iteration : 2936
train acc:  0.828125
train loss:  0.4157963991165161
train gradient:  0.4657059382732837
iteration : 2937
train acc:  0.84375
train loss:  0.40469884872436523
train gradient:  0.2771859124633443
iteration : 2938
train acc:  0.8515625
train loss:  0.30079880356788635
train gradient:  0.2450114886903134
iteration : 2939
train acc:  0.875
train loss:  0.2892495393753052
train gradient:  0.17881563950468987
iteration : 2940
train acc:  0.8046875
train loss:  0.427666574716568
train gradient:  0.5127175309208518
iteration : 2941
train acc:  0.765625
train loss:  0.43555396795272827
train gradient:  0.46818959373077645
iteration : 2942
train acc:  0.8359375
train loss:  0.35616981983184814
train gradient:  0.4139061367336341
iteration : 2943
train acc:  0.828125
train loss:  0.37528109550476074
train gradient:  0.3404838899911912
iteration : 2944
train acc:  0.8671875
train loss:  0.3481487035751343
train gradient:  0.27900417818327156
iteration : 2945
train acc:  0.796875
train loss:  0.36969056725502014
train gradient:  0.3552383290125043
iteration : 2946
train acc:  0.8671875
train loss:  0.314717561006546
train gradient:  0.22339983198352387
iteration : 2947
train acc:  0.7890625
train loss:  0.47704967856407166
train gradient:  0.5951237104976174
iteration : 2948
train acc:  0.8671875
train loss:  0.34723812341690063
train gradient:  0.2617857623363561
iteration : 2949
train acc:  0.8359375
train loss:  0.4255533814430237
train gradient:  0.39106944754429673
iteration : 2950
train acc:  0.8203125
train loss:  0.3651023507118225
train gradient:  0.32858510779806627
iteration : 2951
train acc:  0.8046875
train loss:  0.4840729236602783
train gradient:  0.6400398290976368
iteration : 2952
train acc:  0.8359375
train loss:  0.336883008480072
train gradient:  0.27682961714896387
iteration : 2953
train acc:  0.8125
train loss:  0.3772490620613098
train gradient:  0.3960005851392046
iteration : 2954
train acc:  0.875
train loss:  0.3446881175041199
train gradient:  0.284469870699954
iteration : 2955
train acc:  0.765625
train loss:  0.512719452381134
train gradient:  0.5133816442451797
iteration : 2956
train acc:  0.828125
train loss:  0.3262792229652405
train gradient:  0.28481603021608043
iteration : 2957
train acc:  0.8046875
train loss:  0.4247151017189026
train gradient:  0.4503745254868103
iteration : 2958
train acc:  0.84375
train loss:  0.39996397495269775
train gradient:  0.3588266238916007
iteration : 2959
train acc:  0.859375
train loss:  0.303159236907959
train gradient:  0.20837737452252061
iteration : 2960
train acc:  0.890625
train loss:  0.3390007019042969
train gradient:  0.24043935664427674
iteration : 2961
train acc:  0.8203125
train loss:  0.4073137044906616
train gradient:  0.31548068642620525
iteration : 2962
train acc:  0.828125
train loss:  0.3832951784133911
train gradient:  0.279105408263486
iteration : 2963
train acc:  0.8203125
train loss:  0.36874645948410034
train gradient:  0.31441199475044485
iteration : 2964
train acc:  0.796875
train loss:  0.43759140372276306
train gradient:  0.45515159625741153
iteration : 2965
train acc:  0.8359375
train loss:  0.40276169776916504
train gradient:  0.3901821062293456
iteration : 2966
train acc:  0.8359375
train loss:  0.3483485281467438
train gradient:  0.2698005824855387
iteration : 2967
train acc:  0.84375
train loss:  0.33707913756370544
train gradient:  0.3390221471744826
iteration : 2968
train acc:  0.78125
train loss:  0.4913997948169708
train gradient:  0.418186185277939
iteration : 2969
train acc:  0.8203125
train loss:  0.39759618043899536
train gradient:  0.3311204903465192
iteration : 2970
train acc:  0.8359375
train loss:  0.3664025068283081
train gradient:  0.23987259861109134
iteration : 2971
train acc:  0.8671875
train loss:  0.3169662356376648
train gradient:  0.1958599008413905
iteration : 2972
train acc:  0.859375
train loss:  0.3317878842353821
train gradient:  0.32321808958925013
iteration : 2973
train acc:  0.8515625
train loss:  0.3811958432197571
train gradient:  0.36746416562165557
iteration : 2974
train acc:  0.8125
train loss:  0.3521188795566559
train gradient:  0.2428212399735747
iteration : 2975
train acc:  0.7734375
train loss:  0.4282799959182739
train gradient:  0.3662107117946415
iteration : 2976
train acc:  0.828125
train loss:  0.39015674591064453
train gradient:  0.3851434327674479
iteration : 2977
train acc:  0.8984375
train loss:  0.30775976181030273
train gradient:  0.24523622788871954
iteration : 2978
train acc:  0.8828125
train loss:  0.30799493193626404
train gradient:  0.2128137817299569
iteration : 2979
train acc:  0.796875
train loss:  0.42868921160697937
train gradient:  0.33650451052756913
iteration : 2980
train acc:  0.8359375
train loss:  0.3668786287307739
train gradient:  0.2239102768252288
iteration : 2981
train acc:  0.7890625
train loss:  0.48671725392341614
train gradient:  0.4789427327525835
iteration : 2982
train acc:  0.8203125
train loss:  0.38961976766586304
train gradient:  0.3118831791924714
iteration : 2983
train acc:  0.734375
train loss:  0.509834885597229
train gradient:  0.6772342963370096
iteration : 2984
train acc:  0.765625
train loss:  0.44492003321647644
train gradient:  0.3867410718062014
iteration : 2985
train acc:  0.8046875
train loss:  0.48247581720352173
train gradient:  0.5567440238663512
iteration : 2986
train acc:  0.890625
train loss:  0.31144219636917114
train gradient:  0.2721470304523616
iteration : 2987
train acc:  0.765625
train loss:  0.46210289001464844
train gradient:  0.4068469650665109
iteration : 2988
train acc:  0.84375
train loss:  0.3671645522117615
train gradient:  0.30887009744567895
iteration : 2989
train acc:  0.8359375
train loss:  0.3787660002708435
train gradient:  0.382708149229124
iteration : 2990
train acc:  0.828125
train loss:  0.42736178636550903
train gradient:  0.34537068177786134
iteration : 2991
train acc:  0.8203125
train loss:  0.4294942617416382
train gradient:  0.46589400017924365
iteration : 2992
train acc:  0.8359375
train loss:  0.3910827934741974
train gradient:  0.5291750831083389
iteration : 2993
train acc:  0.8671875
train loss:  0.37956351041793823
train gradient:  0.2566587855237668
iteration : 2994
train acc:  0.7890625
train loss:  0.42473703622817993
train gradient:  0.3172308056715744
iteration : 2995
train acc:  0.8515625
train loss:  0.3710370659828186
train gradient:  0.2425448847291879
iteration : 2996
train acc:  0.8671875
train loss:  0.32197511196136475
train gradient:  0.20665844431847197
iteration : 2997
train acc:  0.828125
train loss:  0.42230528593063354
train gradient:  0.34231567765393833
iteration : 2998
train acc:  0.8671875
train loss:  0.3356078565120697
train gradient:  0.33007407604516364
iteration : 2999
train acc:  0.859375
train loss:  0.3350534439086914
train gradient:  0.21444299547729664
iteration : 3000
train acc:  0.8203125
train loss:  0.3868092894554138
train gradient:  0.2638022584665608
iteration : 3001
train acc:  0.8359375
train loss:  0.36475586891174316
train gradient:  0.27685605402181473
iteration : 3002
train acc:  0.8203125
train loss:  0.3928011953830719
train gradient:  0.3003372742123319
iteration : 3003
train acc:  0.875
train loss:  0.3551901578903198
train gradient:  0.2555987952253019
iteration : 3004
train acc:  0.8125
train loss:  0.4426836669445038
train gradient:  0.3136610048073526
iteration : 3005
train acc:  0.734375
train loss:  0.46204936504364014
train gradient:  0.33460051217772446
iteration : 3006
train acc:  0.7890625
train loss:  0.40550151467323303
train gradient:  0.2993124808356909
iteration : 3007
train acc:  0.84375
train loss:  0.3543878495693207
train gradient:  0.2887140125524636
iteration : 3008
train acc:  0.8125
train loss:  0.43150103092193604
train gradient:  0.3789828487521904
iteration : 3009
train acc:  0.8671875
train loss:  0.3306005597114563
train gradient:  0.19881943121036388
iteration : 3010
train acc:  0.8203125
train loss:  0.41423171758651733
train gradient:  0.505772007261245
iteration : 3011
train acc:  0.75
train loss:  0.4643474817276001
train gradient:  0.3397743054487455
iteration : 3012
train acc:  0.8515625
train loss:  0.38390254974365234
train gradient:  0.2301088624825788
iteration : 3013
train acc:  0.8359375
train loss:  0.42552506923675537
train gradient:  0.39172713161952855
iteration : 3014
train acc:  0.8515625
train loss:  0.30839893221855164
train gradient:  0.16431678610472364
iteration : 3015
train acc:  0.828125
train loss:  0.34722840785980225
train gradient:  0.38710806267788594
iteration : 3016
train acc:  0.796875
train loss:  0.4438779652118683
train gradient:  0.3558312125109838
iteration : 3017
train acc:  0.8046875
train loss:  0.40349283814430237
train gradient:  0.4273615469310926
iteration : 3018
train acc:  0.796875
train loss:  0.4374084174633026
train gradient:  0.37579045958464463
iteration : 3019
train acc:  0.828125
train loss:  0.4384487271308899
train gradient:  0.44734793782054033
iteration : 3020
train acc:  0.7890625
train loss:  0.4242454767227173
train gradient:  0.3214210289492274
iteration : 3021
train acc:  0.7890625
train loss:  0.4032762050628662
train gradient:  0.33408890666069097
iteration : 3022
train acc:  0.8671875
train loss:  0.32752108573913574
train gradient:  0.24287371197629554
iteration : 3023
train acc:  0.8203125
train loss:  0.37942779064178467
train gradient:  0.30582574304552623
iteration : 3024
train acc:  0.8359375
train loss:  0.3814680576324463
train gradient:  0.24715257124368822
iteration : 3025
train acc:  0.84375
train loss:  0.38081586360931396
train gradient:  0.333843420851556
iteration : 3026
train acc:  0.875
train loss:  0.34560561180114746
train gradient:  0.35797399757471304
iteration : 3027
train acc:  0.890625
train loss:  0.30092713236808777
train gradient:  0.22756514843648018
iteration : 3028
train acc:  0.859375
train loss:  0.34067919850349426
train gradient:  0.2911536338584431
iteration : 3029
train acc:  0.8671875
train loss:  0.32205918431282043
train gradient:  0.2250018090495317
iteration : 3030
train acc:  0.8046875
train loss:  0.4339457154273987
train gradient:  0.3324410288746469
iteration : 3031
train acc:  0.859375
train loss:  0.35112541913986206
train gradient:  0.4451078746974862
iteration : 3032
train acc:  0.8203125
train loss:  0.39142540097236633
train gradient:  0.4481851575766852
iteration : 3033
train acc:  0.84375
train loss:  0.3530295491218567
train gradient:  0.25491449739313066
iteration : 3034
train acc:  0.796875
train loss:  0.36327674984931946
train gradient:  0.3024218306621094
iteration : 3035
train acc:  0.890625
train loss:  0.2682112753391266
train gradient:  0.2121698529484385
iteration : 3036
train acc:  0.8203125
train loss:  0.4359320402145386
train gradient:  0.41901257046296614
iteration : 3037
train acc:  0.796875
train loss:  0.38753318786621094
train gradient:  0.3033572659470255
iteration : 3038
train acc:  0.8359375
train loss:  0.32187268137931824
train gradient:  0.21610356383871554
iteration : 3039
train acc:  0.8125
train loss:  0.40857675671577454
train gradient:  0.40418654884321953
iteration : 3040
train acc:  0.828125
train loss:  0.36125069856643677
train gradient:  0.2526200177912014
iteration : 3041
train acc:  0.828125
train loss:  0.38234615325927734
train gradient:  0.29562194702710004
iteration : 3042
train acc:  0.8046875
train loss:  0.3622244596481323
train gradient:  0.23900507315606334
iteration : 3043
train acc:  0.8515625
train loss:  0.35660821199417114
train gradient:  0.3847722911705106
iteration : 3044
train acc:  0.875
train loss:  0.31950175762176514
train gradient:  0.24394691910013067
iteration : 3045
train acc:  0.8203125
train loss:  0.4127940535545349
train gradient:  0.3088901981029994
iteration : 3046
train acc:  0.828125
train loss:  0.36033546924591064
train gradient:  0.27718837393474133
iteration : 3047
train acc:  0.859375
train loss:  0.31250205636024475
train gradient:  0.2682334181210416
iteration : 3048
train acc:  0.8203125
train loss:  0.3780018389225006
train gradient:  0.39545974543647733
iteration : 3049
train acc:  0.796875
train loss:  0.3667665719985962
train gradient:  0.27498791296363523
iteration : 3050
train acc:  0.78125
train loss:  0.37314122915267944
train gradient:  0.35368156187098493
iteration : 3051
train acc:  0.8515625
train loss:  0.32998335361480713
train gradient:  0.2881652973167323
iteration : 3052
train acc:  0.8203125
train loss:  0.33636316657066345
train gradient:  0.3327591358957705
iteration : 3053
train acc:  0.8515625
train loss:  0.32239529490470886
train gradient:  0.2777294658512787
iteration : 3054
train acc:  0.828125
train loss:  0.4370657801628113
train gradient:  0.38411382154687057
iteration : 3055
train acc:  0.828125
train loss:  0.34609025716781616
train gradient:  0.2662358690458609
iteration : 3056
train acc:  0.84375
train loss:  0.3694937825202942
train gradient:  0.292977076846554
iteration : 3057
train acc:  0.765625
train loss:  0.43253278732299805
train gradient:  0.4454068362199134
iteration : 3058
train acc:  0.84375
train loss:  0.3632916808128357
train gradient:  0.30465179372699636
iteration : 3059
train acc:  0.8828125
train loss:  0.3221200108528137
train gradient:  0.24232740955371673
iteration : 3060
train acc:  0.8515625
train loss:  0.3886409103870392
train gradient:  0.27107833539845055
iteration : 3061
train acc:  0.828125
train loss:  0.3320060074329376
train gradient:  0.3075195932604235
iteration : 3062
train acc:  0.828125
train loss:  0.3522648811340332
train gradient:  0.3288268663103906
iteration : 3063
train acc:  0.8125
train loss:  0.36774125695228577
train gradient:  0.3420545767574472
iteration : 3064
train acc:  0.84375
train loss:  0.3710818290710449
train gradient:  0.40902352363640954
iteration : 3065
train acc:  0.84375
train loss:  0.35966935753822327
train gradient:  0.3285122687658099
iteration : 3066
train acc:  0.8515625
train loss:  0.3558666706085205
train gradient:  0.2538860964670979
iteration : 3067
train acc:  0.8828125
train loss:  0.3263005018234253
train gradient:  0.24393984641101274
iteration : 3068
train acc:  0.8671875
train loss:  0.36591485142707825
train gradient:  0.27071767926696655
iteration : 3069
train acc:  0.8671875
train loss:  0.3743113577365875
train gradient:  0.4517489243530384
iteration : 3070
train acc:  0.8515625
train loss:  0.3493605852127075
train gradient:  0.27748845866293365
iteration : 3071
train acc:  0.7890625
train loss:  0.4954398274421692
train gradient:  0.5423362898460586
iteration : 3072
train acc:  0.859375
train loss:  0.3227941393852234
train gradient:  0.2060218778899709
iteration : 3073
train acc:  0.8359375
train loss:  0.359402060508728
train gradient:  0.47793920922440997
iteration : 3074
train acc:  0.8515625
train loss:  0.35336732864379883
train gradient:  0.2273741095855172
iteration : 3075
train acc:  0.84375
train loss:  0.3472845256328583
train gradient:  0.34479914193452965
iteration : 3076
train acc:  0.796875
train loss:  0.4230756163597107
train gradient:  0.3877576832765104
iteration : 3077
train acc:  0.8515625
train loss:  0.34179285168647766
train gradient:  0.38228319530427773
iteration : 3078
train acc:  0.8984375
train loss:  0.32343003153800964
train gradient:  0.2552054644179868
iteration : 3079
train acc:  0.8203125
train loss:  0.3948855400085449
train gradient:  0.41461273059130066
iteration : 3080
train acc:  0.7734375
train loss:  0.43979620933532715
train gradient:  0.7812526593616469
iteration : 3081
train acc:  0.890625
train loss:  0.28853294253349304
train gradient:  0.2124536732154054
iteration : 3082
train acc:  0.8046875
train loss:  0.44377851486206055
train gradient:  0.3756720327771047
iteration : 3083
train acc:  0.7890625
train loss:  0.382266640663147
train gradient:  0.3054485944291819
iteration : 3084
train acc:  0.765625
train loss:  0.4646161198616028
train gradient:  0.36811822996542837
iteration : 3085
train acc:  0.8828125
train loss:  0.3192104697227478
train gradient:  0.29150587331268163
iteration : 3086
train acc:  0.7890625
train loss:  0.47698649764060974
train gradient:  0.4997631534741983
iteration : 3087
train acc:  0.8203125
train loss:  0.3440898656845093
train gradient:  0.2711153942067073
iteration : 3088
train acc:  0.84375
train loss:  0.3450339734554291
train gradient:  0.29569465839188874
iteration : 3089
train acc:  0.8515625
train loss:  0.33764365315437317
train gradient:  0.2784792901284592
iteration : 3090
train acc:  0.7890625
train loss:  0.44448208808898926
train gradient:  0.5612945057316325
iteration : 3091
train acc:  0.828125
train loss:  0.43012261390686035
train gradient:  0.3782133614905208
iteration : 3092
train acc:  0.8515625
train loss:  0.3672025501728058
train gradient:  0.33254239510898886
iteration : 3093
train acc:  0.8203125
train loss:  0.4038587808609009
train gradient:  0.36479065057835963
iteration : 3094
train acc:  0.8515625
train loss:  0.30883699655532837
train gradient:  0.23791088797353221
iteration : 3095
train acc:  0.8125
train loss:  0.4107058644294739
train gradient:  0.4732499167858076
iteration : 3096
train acc:  0.859375
train loss:  0.354434609413147
train gradient:  0.28662616357292264
iteration : 3097
train acc:  0.84375
train loss:  0.348196417093277
train gradient:  0.3273302489263374
iteration : 3098
train acc:  0.7890625
train loss:  0.4732900857925415
train gradient:  0.37869548946043013
iteration : 3099
train acc:  0.8359375
train loss:  0.4043992757797241
train gradient:  0.4828491413293111
iteration : 3100
train acc:  0.8359375
train loss:  0.37223517894744873
train gradient:  0.340641712096866
iteration : 3101
train acc:  0.8203125
train loss:  0.34759312868118286
train gradient:  0.2276013841895941
iteration : 3102
train acc:  0.875
train loss:  0.3262282609939575
train gradient:  0.1836701005086943
iteration : 3103
train acc:  0.8203125
train loss:  0.4113950729370117
train gradient:  0.30095914041961264
iteration : 3104
train acc:  0.7890625
train loss:  0.41448286175727844
train gradient:  0.3487328944841524
iteration : 3105
train acc:  0.796875
train loss:  0.46450942754745483
train gradient:  0.8760814837834502
iteration : 3106
train acc:  0.8515625
train loss:  0.3897102177143097
train gradient:  0.2755525745352221
iteration : 3107
train acc:  0.859375
train loss:  0.3162631094455719
train gradient:  0.2372845941184384
iteration : 3108
train acc:  0.78125
train loss:  0.43808671832084656
train gradient:  0.31583338769654323
iteration : 3109
train acc:  0.859375
train loss:  0.3548905551433563
train gradient:  0.32075476428561295
iteration : 3110
train acc:  0.8046875
train loss:  0.4688992500305176
train gradient:  0.5325992700882598
iteration : 3111
train acc:  0.8125
train loss:  0.39291268587112427
train gradient:  0.30787087297855525
iteration : 3112
train acc:  0.859375
train loss:  0.3406097888946533
train gradient:  0.2935533571163746
iteration : 3113
train acc:  0.828125
train loss:  0.3792295455932617
train gradient:  0.2533514256079297
iteration : 3114
train acc:  0.8046875
train loss:  0.4028143882751465
train gradient:  0.27412438514970433
iteration : 3115
train acc:  0.765625
train loss:  0.4437357187271118
train gradient:  0.3614046148141037
iteration : 3116
train acc:  0.859375
train loss:  0.3579939007759094
train gradient:  0.274956357519457
iteration : 3117
train acc:  0.8125
train loss:  0.39317360520362854
train gradient:  0.3149391138729968
iteration : 3118
train acc:  0.828125
train loss:  0.3491545021533966
train gradient:  0.25491093041387597
iteration : 3119
train acc:  0.859375
train loss:  0.3605436682701111
train gradient:  0.2912677992466012
iteration : 3120
train acc:  0.8125
train loss:  0.3935941755771637
train gradient:  0.3681250868622694
iteration : 3121
train acc:  0.8671875
train loss:  0.3039952516555786
train gradient:  0.22077697624550596
iteration : 3122
train acc:  0.7890625
train loss:  0.4402630925178528
train gradient:  0.4341337931244176
iteration : 3123
train acc:  0.8515625
train loss:  0.37825828790664673
train gradient:  0.3169317710699997
iteration : 3124
train acc:  0.84375
train loss:  0.3702130615711212
train gradient:  0.36734251766819226
iteration : 3125
train acc:  0.84375
train loss:  0.34359192848205566
train gradient:  0.3646337693537472
iteration : 3126
train acc:  0.7734375
train loss:  0.44778546690940857
train gradient:  0.43343965072223073
iteration : 3127
train acc:  0.7890625
train loss:  0.4515441656112671
train gradient:  0.36389882651243616
iteration : 3128
train acc:  0.859375
train loss:  0.3173990845680237
train gradient:  0.26599125994391004
iteration : 3129
train acc:  0.8046875
train loss:  0.39637959003448486
train gradient:  0.36937971765569566
iteration : 3130
train acc:  0.828125
train loss:  0.3826506733894348
train gradient:  0.3299048530647728
iteration : 3131
train acc:  0.8671875
train loss:  0.3849921226501465
train gradient:  0.4038217659674069
iteration : 3132
train acc:  0.8203125
train loss:  0.37256306409835815
train gradient:  0.31478047853547075
iteration : 3133
train acc:  0.8046875
train loss:  0.4633924961090088
train gradient:  0.4201883563799499
iteration : 3134
train acc:  0.796875
train loss:  0.42607179284095764
train gradient:  0.40186059593868284
iteration : 3135
train acc:  0.8828125
train loss:  0.2911564111709595
train gradient:  0.3035439560818773
iteration : 3136
train acc:  0.8359375
train loss:  0.3660132586956024
train gradient:  0.22017501345323914
iteration : 3137
train acc:  0.84375
train loss:  0.4089707136154175
train gradient:  0.3395956131487321
iteration : 3138
train acc:  0.796875
train loss:  0.42856770753860474
train gradient:  0.36189473253008364
iteration : 3139
train acc:  0.78125
train loss:  0.44560903310775757
train gradient:  0.3217483905153395
iteration : 3140
train acc:  0.84375
train loss:  0.36457934975624084
train gradient:  0.2438098348161422
iteration : 3141
train acc:  0.890625
train loss:  0.29945844411849976
train gradient:  0.2934899209879895
iteration : 3142
train acc:  0.84375
train loss:  0.3077622056007385
train gradient:  0.22209262554625547
iteration : 3143
train acc:  0.8671875
train loss:  0.32599520683288574
train gradient:  0.3151412814216377
iteration : 3144
train acc:  0.8125
train loss:  0.36681485176086426
train gradient:  0.286447980673249
iteration : 3145
train acc:  0.8203125
train loss:  0.40481793880462646
train gradient:  0.284548412024173
iteration : 3146
train acc:  0.78125
train loss:  0.3943975567817688
train gradient:  0.2964289115342975
iteration : 3147
train acc:  0.8359375
train loss:  0.354451060295105
train gradient:  0.28643176881980226
iteration : 3148
train acc:  0.84375
train loss:  0.31549057364463806
train gradient:  0.20333750221474886
iteration : 3149
train acc:  0.796875
train loss:  0.4244270324707031
train gradient:  0.3522655843398228
iteration : 3150
train acc:  0.8125
train loss:  0.4155336022377014
train gradient:  0.4137515120793859
iteration : 3151
train acc:  0.8203125
train loss:  0.3381774127483368
train gradient:  0.29841292893586424
iteration : 3152
train acc:  0.7890625
train loss:  0.39815354347229004
train gradient:  0.3746949795280673
iteration : 3153
train acc:  0.8046875
train loss:  0.3678710460662842
train gradient:  0.29867850616359065
iteration : 3154
train acc:  0.8125
train loss:  0.4083239734172821
train gradient:  0.3005118753036478
iteration : 3155
train acc:  0.859375
train loss:  0.32687973976135254
train gradient:  0.24344971141450575
iteration : 3156
train acc:  0.7890625
train loss:  0.3598482608795166
train gradient:  0.2632636135423899
iteration : 3157
train acc:  0.890625
train loss:  0.2802007794380188
train gradient:  0.2325269409012658
iteration : 3158
train acc:  0.8515625
train loss:  0.35794103145599365
train gradient:  0.2677431814999984
iteration : 3159
train acc:  0.859375
train loss:  0.3728998601436615
train gradient:  0.32008164636937225
iteration : 3160
train acc:  0.859375
train loss:  0.3531339764595032
train gradient:  0.29000242614551464
iteration : 3161
train acc:  0.796875
train loss:  0.4356846809387207
train gradient:  0.4303334378060775
iteration : 3162
train acc:  0.796875
train loss:  0.45148006081581116
train gradient:  0.3829727387985811
iteration : 3163
train acc:  0.8046875
train loss:  0.37332165241241455
train gradient:  0.30373872861153534
iteration : 3164
train acc:  0.8359375
train loss:  0.3895476758480072
train gradient:  0.3282787454088599
iteration : 3165
train acc:  0.7734375
train loss:  0.407859206199646
train gradient:  0.32734297671504736
iteration : 3166
train acc:  0.796875
train loss:  0.46304798126220703
train gradient:  0.27895691959795726
iteration : 3167
train acc:  0.78125
train loss:  0.4395134449005127
train gradient:  0.35050795413615565
iteration : 3168
train acc:  0.8671875
train loss:  0.3375750184059143
train gradient:  0.25435405521043003
iteration : 3169
train acc:  0.84375
train loss:  0.37994667887687683
train gradient:  0.3190040314200268
iteration : 3170
train acc:  0.8984375
train loss:  0.27514326572418213
train gradient:  0.19704373425928223
iteration : 3171
train acc:  0.921875
train loss:  0.2609589695930481
train gradient:  0.20363403228045818
iteration : 3172
train acc:  0.8125
train loss:  0.3882782459259033
train gradient:  0.39268044302700755
iteration : 3173
train acc:  0.796875
train loss:  0.40148335695266724
train gradient:  0.31641029263698306
iteration : 3174
train acc:  0.828125
train loss:  0.3422245681285858
train gradient:  0.21853501167864306
iteration : 3175
train acc:  0.78125
train loss:  0.462895005941391
train gradient:  0.3736904806705425
iteration : 3176
train acc:  0.890625
train loss:  0.3366546928882599
train gradient:  0.2584709115782454
iteration : 3177
train acc:  0.859375
train loss:  0.33028554916381836
train gradient:  0.3292766767208718
iteration : 3178
train acc:  0.8671875
train loss:  0.3777349591255188
train gradient:  0.40678395865467865
iteration : 3179
train acc:  0.828125
train loss:  0.37694552540779114
train gradient:  0.3228198957387236
iteration : 3180
train acc:  0.8515625
train loss:  0.38111957907676697
train gradient:  0.2927967295908371
iteration : 3181
train acc:  0.8671875
train loss:  0.30083584785461426
train gradient:  0.23675142172171407
iteration : 3182
train acc:  0.7578125
train loss:  0.4850776791572571
train gradient:  0.5659091149568514
iteration : 3183
train acc:  0.859375
train loss:  0.30012810230255127
train gradient:  0.16811686195840161
iteration : 3184
train acc:  0.765625
train loss:  0.42290449142456055
train gradient:  0.4971161714086184
iteration : 3185
train acc:  0.8046875
train loss:  0.37351831793785095
train gradient:  0.33387253812427486
iteration : 3186
train acc:  0.796875
train loss:  0.41386014223098755
train gradient:  0.35249257662305045
iteration : 3187
train acc:  0.84375
train loss:  0.33738547563552856
train gradient:  0.31732540016329963
iteration : 3188
train acc:  0.859375
train loss:  0.3238674998283386
train gradient:  0.27784960192341646
iteration : 3189
train acc:  0.8515625
train loss:  0.34995734691619873
train gradient:  0.2915337864461498
iteration : 3190
train acc:  0.8515625
train loss:  0.3411267399787903
train gradient:  0.3229479043043482
iteration : 3191
train acc:  0.7890625
train loss:  0.44394516944885254
train gradient:  0.3477227164945903
iteration : 3192
train acc:  0.796875
train loss:  0.4217034578323364
train gradient:  0.386588428241493
iteration : 3193
train acc:  0.8515625
train loss:  0.4145248234272003
train gradient:  0.347863262257299
iteration : 3194
train acc:  0.78125
train loss:  0.4137711524963379
train gradient:  0.4230044962483516
iteration : 3195
train acc:  0.875
train loss:  0.3147345781326294
train gradient:  0.21238575884319422
iteration : 3196
train acc:  0.78125
train loss:  0.45461517572402954
train gradient:  0.3762095600384845
iteration : 3197
train acc:  0.7890625
train loss:  0.4728468358516693
train gradient:  0.4566915339537524
iteration : 3198
train acc:  0.8203125
train loss:  0.36752861738204956
train gradient:  0.33299740691492363
iteration : 3199
train acc:  0.875
train loss:  0.3278160095214844
train gradient:  0.2723486702420138
iteration : 3200
train acc:  0.7890625
train loss:  0.4412183165550232
train gradient:  0.35044613427077015
iteration : 3201
train acc:  0.8046875
train loss:  0.4437495768070221
train gradient:  0.5109864723753554
iteration : 3202
train acc:  0.8828125
train loss:  0.3214980661869049
train gradient:  0.2919400170283055
iteration : 3203
train acc:  0.828125
train loss:  0.4406747817993164
train gradient:  0.3881310624685883
iteration : 3204
train acc:  0.8125
train loss:  0.3978133499622345
train gradient:  0.2399461666391217
iteration : 3205
train acc:  0.8671875
train loss:  0.3368222713470459
train gradient:  0.23794359936130083
iteration : 3206
train acc:  0.8359375
train loss:  0.36750391125679016
train gradient:  0.32545377386578356
iteration : 3207
train acc:  0.84375
train loss:  0.36867016553878784
train gradient:  0.26167749405142615
iteration : 3208
train acc:  0.8125
train loss:  0.4114866256713867
train gradient:  0.3819511958696256
iteration : 3209
train acc:  0.859375
train loss:  0.3970208764076233
train gradient:  0.23459080089015574
iteration : 3210
train acc:  0.90625
train loss:  0.3172033429145813
train gradient:  0.323011999780188
iteration : 3211
train acc:  0.8203125
train loss:  0.37493884563446045
train gradient:  0.5268963827101525
iteration : 3212
train acc:  0.828125
train loss:  0.37345385551452637
train gradient:  0.372163528340676
iteration : 3213
train acc:  0.828125
train loss:  0.38711482286453247
train gradient:  0.27782548867787227
iteration : 3214
train acc:  0.8046875
train loss:  0.3578563630580902
train gradient:  0.3187116503130974
iteration : 3215
train acc:  0.8671875
train loss:  0.3455207943916321
train gradient:  0.23637356366000462
iteration : 3216
train acc:  0.859375
train loss:  0.3325587511062622
train gradient:  0.26747969149577827
iteration : 3217
train acc:  0.8515625
train loss:  0.33697590231895447
train gradient:  0.26528621713019296
iteration : 3218
train acc:  0.859375
train loss:  0.30640819668769836
train gradient:  0.16036490561772487
iteration : 3219
train acc:  0.84375
train loss:  0.3533448576927185
train gradient:  0.47789364584079713
iteration : 3220
train acc:  0.7734375
train loss:  0.41218340396881104
train gradient:  0.38202820908651586
iteration : 3221
train acc:  0.8125
train loss:  0.3869727849960327
train gradient:  0.35242704292789606
iteration : 3222
train acc:  0.8203125
train loss:  0.40521371364593506
train gradient:  0.31389090619940946
iteration : 3223
train acc:  0.8828125
train loss:  0.3252624571323395
train gradient:  0.34610084567978927
iteration : 3224
train acc:  0.8203125
train loss:  0.4696909785270691
train gradient:  0.39345339333003554
iteration : 3225
train acc:  0.8359375
train loss:  0.3568326234817505
train gradient:  0.3052952416595488
iteration : 3226
train acc:  0.859375
train loss:  0.42152491211891174
train gradient:  0.373918170177024
iteration : 3227
train acc:  0.875
train loss:  0.3072145879268646
train gradient:  0.19080593134827287
iteration : 3228
train acc:  0.84375
train loss:  0.3218689560890198
train gradient:  0.23295240151923807
iteration : 3229
train acc:  0.78125
train loss:  0.44664865732192993
train gradient:  0.46841425968222866
iteration : 3230
train acc:  0.8046875
train loss:  0.4319259524345398
train gradient:  0.3721391895816512
iteration : 3231
train acc:  0.84375
train loss:  0.3331327438354492
train gradient:  0.16550534402607184
iteration : 3232
train acc:  0.7890625
train loss:  0.47929647564888
train gradient:  0.40292552348411764
iteration : 3233
train acc:  0.859375
train loss:  0.33483803272247314
train gradient:  0.25383956990580403
iteration : 3234
train acc:  0.875
train loss:  0.3267427682876587
train gradient:  0.19275939605419576
iteration : 3235
train acc:  0.8125
train loss:  0.458666056394577
train gradient:  0.42169409308415007
iteration : 3236
train acc:  0.859375
train loss:  0.399925172328949
train gradient:  0.32445608543852394
iteration : 3237
train acc:  0.8515625
train loss:  0.3972380757331848
train gradient:  0.34633644696524707
iteration : 3238
train acc:  0.90625
train loss:  0.30491799116134644
train gradient:  0.2629033556101962
iteration : 3239
train acc:  0.859375
train loss:  0.30258315801620483
train gradient:  0.28939137572876145
iteration : 3240
train acc:  0.7734375
train loss:  0.4475306272506714
train gradient:  0.3021004000185544
iteration : 3241
train acc:  0.8828125
train loss:  0.3540002703666687
train gradient:  0.24029028948599573
iteration : 3242
train acc:  0.8125
train loss:  0.3871256113052368
train gradient:  0.4163777163611626
iteration : 3243
train acc:  0.859375
train loss:  0.3728930354118347
train gradient:  0.29953157790256596
iteration : 3244
train acc:  0.8359375
train loss:  0.3482723534107208
train gradient:  0.2096740534872921
iteration : 3245
train acc:  0.8984375
train loss:  0.32399940490722656
train gradient:  0.18804636611485448
iteration : 3246
train acc:  0.84375
train loss:  0.3889411687850952
train gradient:  0.4645053976422735
iteration : 3247
train acc:  0.875
train loss:  0.2918897867202759
train gradient:  0.17859260511041872
iteration : 3248
train acc:  0.859375
train loss:  0.3160969316959381
train gradient:  0.2993084278430358
iteration : 3249
train acc:  0.8046875
train loss:  0.40751034021377563
train gradient:  0.3569706495686597
iteration : 3250
train acc:  0.765625
train loss:  0.5182905197143555
train gradient:  0.5071895874694505
iteration : 3251
train acc:  0.828125
train loss:  0.36743107438087463
train gradient:  0.2419129028837471
iteration : 3252
train acc:  0.7890625
train loss:  0.4932587444782257
train gradient:  0.4217550292058749
iteration : 3253
train acc:  0.8359375
train loss:  0.3513895273208618
train gradient:  0.500787851531405
iteration : 3254
train acc:  0.796875
train loss:  0.36151379346847534
train gradient:  0.33569818084080133
iteration : 3255
train acc:  0.7890625
train loss:  0.4265747666358948
train gradient:  0.3081762803512514
iteration : 3256
train acc:  0.8203125
train loss:  0.3742746114730835
train gradient:  0.2486057689961824
iteration : 3257
train acc:  0.859375
train loss:  0.3288312554359436
train gradient:  0.2712939257245712
iteration : 3258
train acc:  0.796875
train loss:  0.4096038341522217
train gradient:  0.38998133190667633
iteration : 3259
train acc:  0.8125
train loss:  0.34070491790771484
train gradient:  0.20898875240344347
iteration : 3260
train acc:  0.8125
train loss:  0.43109777569770813
train gradient:  0.3651130279564738
iteration : 3261
train acc:  0.8515625
train loss:  0.34677040576934814
train gradient:  0.32438290553466687
iteration : 3262
train acc:  0.8515625
train loss:  0.36329197883605957
train gradient:  0.37062362912944574
iteration : 3263
train acc:  0.8203125
train loss:  0.39434319734573364
train gradient:  0.36598772710528965
iteration : 3264
train acc:  0.828125
train loss:  0.4312199354171753
train gradient:  0.4410668998818612
iteration : 3265
train acc:  0.8359375
train loss:  0.3144998848438263
train gradient:  0.2116625612350171
iteration : 3266
train acc:  0.7890625
train loss:  0.4258720576763153
train gradient:  0.3826080841653683
iteration : 3267
train acc:  0.796875
train loss:  0.4325311779975891
train gradient:  0.3737748966117246
iteration : 3268
train acc:  0.8203125
train loss:  0.37012946605682373
train gradient:  0.26044785684460936
iteration : 3269
train acc:  0.859375
train loss:  0.37932199239730835
train gradient:  0.3492813445797519
iteration : 3270
train acc:  0.8203125
train loss:  0.3896074593067169
train gradient:  0.2240346965418722
iteration : 3271
train acc:  0.8671875
train loss:  0.3057142198085785
train gradient:  0.22669975724773914
iteration : 3272
train acc:  0.859375
train loss:  0.32606154680252075
train gradient:  0.3191940246143906
iteration : 3273
train acc:  0.7578125
train loss:  0.4769129157066345
train gradient:  0.39922700148495466
iteration : 3274
train acc:  0.828125
train loss:  0.4202054738998413
train gradient:  0.35885618411832987
iteration : 3275
train acc:  0.8203125
train loss:  0.3660913109779358
train gradient:  0.3109208773025965
iteration : 3276
train acc:  0.8984375
train loss:  0.3828163146972656
train gradient:  0.3633734797015847
iteration : 3277
train acc:  0.84375
train loss:  0.3204989433288574
train gradient:  0.3087457498760613
iteration : 3278
train acc:  0.8671875
train loss:  0.30478203296661377
train gradient:  0.1691482352509096
iteration : 3279
train acc:  0.828125
train loss:  0.4504525661468506
train gradient:  0.3508611389349466
iteration : 3280
train acc:  0.8046875
train loss:  0.4103775918483734
train gradient:  0.3358141105684807
iteration : 3281
train acc:  0.875
train loss:  0.3070862889289856
train gradient:  0.22377498290205874
iteration : 3282
train acc:  0.7890625
train loss:  0.48312079906463623
train gradient:  0.541593736673394
iteration : 3283
train acc:  0.84375
train loss:  0.40860676765441895
train gradient:  0.36939029827567393
iteration : 3284
train acc:  0.8359375
train loss:  0.37795811891555786
train gradient:  0.2689714536840689
iteration : 3285
train acc:  0.8359375
train loss:  0.335819810628891
train gradient:  0.2635347953296161
iteration : 3286
train acc:  0.828125
train loss:  0.3431122899055481
train gradient:  0.3312920587182828
iteration : 3287
train acc:  0.8984375
train loss:  0.2795315980911255
train gradient:  0.16577687774514832
iteration : 3288
train acc:  0.8203125
train loss:  0.4198434054851532
train gradient:  0.3603172703344158
iteration : 3289
train acc:  0.84375
train loss:  0.3813781142234802
train gradient:  0.30925617212048434
iteration : 3290
train acc:  0.796875
train loss:  0.42000555992126465
train gradient:  0.3237324695283056
iteration : 3291
train acc:  0.84375
train loss:  0.36815187335014343
train gradient:  0.287412002232162
iteration : 3292
train acc:  0.828125
train loss:  0.3634614944458008
train gradient:  0.24764375907399697
iteration : 3293
train acc:  0.875
train loss:  0.30240678787231445
train gradient:  0.2864360681296911
iteration : 3294
train acc:  0.78125
train loss:  0.4146888852119446
train gradient:  0.4932063504849546
iteration : 3295
train acc:  0.890625
train loss:  0.3149130940437317
train gradient:  0.2602535569297935
iteration : 3296
train acc:  0.8203125
train loss:  0.39614206552505493
train gradient:  0.38655090204042036
iteration : 3297
train acc:  0.8359375
train loss:  0.35880714654922485
train gradient:  0.2015540481941324
iteration : 3298
train acc:  0.8125
train loss:  0.43129828572273254
train gradient:  0.4275983783472702
iteration : 3299
train acc:  0.8203125
train loss:  0.35076868534088135
train gradient:  0.3346393132429631
iteration : 3300
train acc:  0.875
train loss:  0.3106917142868042
train gradient:  0.28946908028587465
iteration : 3301
train acc:  0.8671875
train loss:  0.3281904458999634
train gradient:  0.2784598824051575
iteration : 3302
train acc:  0.8515625
train loss:  0.3581496775150299
train gradient:  0.3122837739839539
iteration : 3303
train acc:  0.84375
train loss:  0.35036739706993103
train gradient:  0.2841980184038875
iteration : 3304
train acc:  0.859375
train loss:  0.3411577641963959
train gradient:  0.2655702925960129
iteration : 3305
train acc:  0.7890625
train loss:  0.44054698944091797
train gradient:  0.46451634094308464
iteration : 3306
train acc:  0.828125
train loss:  0.36571353673934937
train gradient:  0.2786784088292794
iteration : 3307
train acc:  0.8515625
train loss:  0.3480226993560791
train gradient:  0.28713991349352563
iteration : 3308
train acc:  0.84375
train loss:  0.31543827056884766
train gradient:  0.22471587590116995
iteration : 3309
train acc:  0.7890625
train loss:  0.4063979387283325
train gradient:  0.2948339216466793
iteration : 3310
train acc:  0.8203125
train loss:  0.3471418619155884
train gradient:  0.2335033158558284
iteration : 3311
train acc:  0.8125
train loss:  0.3721439242362976
train gradient:  0.3813922944873976
iteration : 3312
train acc:  0.828125
train loss:  0.3467230796813965
train gradient:  0.4222297733181363
iteration : 3313
train acc:  0.8671875
train loss:  0.35428035259246826
train gradient:  0.28334190733993897
iteration : 3314
train acc:  0.7578125
train loss:  0.42830920219421387
train gradient:  0.3460299634920499
iteration : 3315
train acc:  0.8359375
train loss:  0.39339742064476013
train gradient:  0.34082036325123016
iteration : 3316
train acc:  0.859375
train loss:  0.3315597474575043
train gradient:  0.25259476490150073
iteration : 3317
train acc:  0.8515625
train loss:  0.3852975368499756
train gradient:  0.4260187845506794
iteration : 3318
train acc:  0.8671875
train loss:  0.34720978140830994
train gradient:  0.3323239151259207
iteration : 3319
train acc:  0.796875
train loss:  0.44130727648735046
train gradient:  0.48399689604877005
iteration : 3320
train acc:  0.8203125
train loss:  0.3856824040412903
train gradient:  0.37296698894079955
iteration : 3321
train acc:  0.8671875
train loss:  0.3181612491607666
train gradient:  0.3272704901849114
iteration : 3322
train acc:  0.8125
train loss:  0.4096167981624603
train gradient:  0.4000067528065355
iteration : 3323
train acc:  0.7890625
train loss:  0.4698673486709595
train gradient:  0.4069896840295044
iteration : 3324
train acc:  0.7890625
train loss:  0.44687700271606445
train gradient:  0.45430171873071085
iteration : 3325
train acc:  0.859375
train loss:  0.32843801379203796
train gradient:  0.27218777220739326
iteration : 3326
train acc:  0.8671875
train loss:  0.3973916172981262
train gradient:  0.3500749856044193
iteration : 3327
train acc:  0.8046875
train loss:  0.3980059027671814
train gradient:  0.34375820794051376
iteration : 3328
train acc:  0.8125
train loss:  0.41821783781051636
train gradient:  0.4158152565480487
iteration : 3329
train acc:  0.8203125
train loss:  0.37470316886901855
train gradient:  0.28043840319999275
iteration : 3330
train acc:  0.8671875
train loss:  0.28988951444625854
train gradient:  0.27356148552361165
iteration : 3331
train acc:  0.796875
train loss:  0.4563731551170349
train gradient:  0.47551471333985534
iteration : 3332
train acc:  0.8203125
train loss:  0.3854096531867981
train gradient:  0.29622571712547413
iteration : 3333
train acc:  0.859375
train loss:  0.3318544030189514
train gradient:  0.2386962908818443
iteration : 3334
train acc:  0.875
train loss:  0.30646079778671265
train gradient:  0.24478200185813792
iteration : 3335
train acc:  0.7890625
train loss:  0.39150840044021606
train gradient:  0.2644934588731229
iteration : 3336
train acc:  0.84375
train loss:  0.35428759455680847
train gradient:  0.28325352190218445
iteration : 3337
train acc:  0.8359375
train loss:  0.39640432596206665
train gradient:  0.25165521744785324
iteration : 3338
train acc:  0.890625
train loss:  0.33808380365371704
train gradient:  0.33450414862411953
iteration : 3339
train acc:  0.8203125
train loss:  0.3884269893169403
train gradient:  0.32570037006152874
iteration : 3340
train acc:  0.8203125
train loss:  0.3893926739692688
train gradient:  0.36020551608418333
iteration : 3341
train acc:  0.859375
train loss:  0.332249253988266
train gradient:  0.2665549129062676
iteration : 3342
train acc:  0.8828125
train loss:  0.31187838315963745
train gradient:  0.3017553274343134
iteration : 3343
train acc:  0.8125
train loss:  0.41326287388801575
train gradient:  0.34543539220841446
iteration : 3344
train acc:  0.8203125
train loss:  0.39513418078422546
train gradient:  0.4225047093980946
iteration : 3345
train acc:  0.859375
train loss:  0.3160286843776703
train gradient:  0.26674493342219635
iteration : 3346
train acc:  0.8046875
train loss:  0.43706774711608887
train gradient:  0.52260748437681
iteration : 3347
train acc:  0.84375
train loss:  0.33572232723236084
train gradient:  0.24306846599071608
iteration : 3348
train acc:  0.8125
train loss:  0.36831510066986084
train gradient:  0.2832892446645064
iteration : 3349
train acc:  0.8515625
train loss:  0.3877581059932709
train gradient:  0.36824845668079464
iteration : 3350
train acc:  0.859375
train loss:  0.34538817405700684
train gradient:  0.18835521550354575
iteration : 3351
train acc:  0.8046875
train loss:  0.40041208267211914
train gradient:  0.32585357325563163
iteration : 3352
train acc:  0.8671875
train loss:  0.34221237897872925
train gradient:  0.34489791858954577
iteration : 3353
train acc:  0.828125
train loss:  0.3662090301513672
train gradient:  0.41631809137936104
iteration : 3354
train acc:  0.8515625
train loss:  0.3622344732284546
train gradient:  0.4679889280805907
iteration : 3355
train acc:  0.8359375
train loss:  0.3654331564903259
train gradient:  0.3620605078582599
iteration : 3356
train acc:  0.8046875
train loss:  0.43147000670433044
train gradient:  0.4113103653644591
iteration : 3357
train acc:  0.8203125
train loss:  0.43109235167503357
train gradient:  0.3921194444004632
iteration : 3358
train acc:  0.8359375
train loss:  0.3969612121582031
train gradient:  0.3437567392494205
iteration : 3359
train acc:  0.859375
train loss:  0.3794928789138794
train gradient:  0.38846184939023387
iteration : 3360
train acc:  0.78125
train loss:  0.4386536478996277
train gradient:  0.38165025906734434
iteration : 3361
train acc:  0.84375
train loss:  0.3626042604446411
train gradient:  0.34186013173026175
iteration : 3362
train acc:  0.8671875
train loss:  0.3431253433227539
train gradient:  0.2856849124633658
iteration : 3363
train acc:  0.828125
train loss:  0.37154701352119446
train gradient:  0.33688184781349645
iteration : 3364
train acc:  0.8203125
train loss:  0.35336488485336304
train gradient:  0.42197161519230003
iteration : 3365
train acc:  0.828125
train loss:  0.4263494908809662
train gradient:  0.43085603341324347
iteration : 3366
train acc:  0.8125
train loss:  0.37673890590667725
train gradient:  0.2518391744891771
iteration : 3367
train acc:  0.8203125
train loss:  0.4428025484085083
train gradient:  0.3977702673944686
iteration : 3368
train acc:  0.7734375
train loss:  0.4517039656639099
train gradient:  0.4573237660165318
iteration : 3369
train acc:  0.828125
train loss:  0.3925231993198395
train gradient:  0.43340360789712623
iteration : 3370
train acc:  0.8125
train loss:  0.3379955291748047
train gradient:  0.219970034606236
iteration : 3371
train acc:  0.8125
train loss:  0.46357059478759766
train gradient:  0.5549721842996782
iteration : 3372
train acc:  0.9140625
train loss:  0.27183955907821655
train gradient:  0.19942153545554747
iteration : 3373
train acc:  0.8203125
train loss:  0.37778085470199585
train gradient:  0.33966183657760507
iteration : 3374
train acc:  0.8203125
train loss:  0.39379531145095825
train gradient:  0.3832465167715176
iteration : 3375
train acc:  0.8671875
train loss:  0.326957106590271
train gradient:  0.1781873565567602
iteration : 3376
train acc:  0.8203125
train loss:  0.3378186821937561
train gradient:  0.26743464368639264
iteration : 3377
train acc:  0.875
train loss:  0.30470895767211914
train gradient:  0.17185671315235393
iteration : 3378
train acc:  0.84375
train loss:  0.3172914981842041
train gradient:  0.21819859570991998
iteration : 3379
train acc:  0.84375
train loss:  0.32983386516571045
train gradient:  0.25577207964822585
iteration : 3380
train acc:  0.7578125
train loss:  0.5085189342498779
train gradient:  0.5799053029664372
iteration : 3381
train acc:  0.8359375
train loss:  0.3702857494354248
train gradient:  0.25880360465214436
iteration : 3382
train acc:  0.8125
train loss:  0.43762004375457764
train gradient:  0.43495798930451535
iteration : 3383
train acc:  0.859375
train loss:  0.29991763830184937
train gradient:  0.26730647119506773
iteration : 3384
train acc:  0.8359375
train loss:  0.34651896357536316
train gradient:  0.20955570420990705
iteration : 3385
train acc:  0.796875
train loss:  0.3743075728416443
train gradient:  0.4258113827543666
iteration : 3386
train acc:  0.7890625
train loss:  0.42148998379707336
train gradient:  0.4132251803167177
iteration : 3387
train acc:  0.8046875
train loss:  0.44406363368034363
train gradient:  0.33429407638306313
iteration : 3388
train acc:  0.8046875
train loss:  0.43144434690475464
train gradient:  0.37956141358835604
iteration : 3389
train acc:  0.828125
train loss:  0.39652177691459656
train gradient:  0.30458902268081994
iteration : 3390
train acc:  0.828125
train loss:  0.43217289447784424
train gradient:  0.39763298110615886
iteration : 3391
train acc:  0.8046875
train loss:  0.43020832538604736
train gradient:  0.6656164905852918
iteration : 3392
train acc:  0.8046875
train loss:  0.38191914558410645
train gradient:  0.43506211892593954
iteration : 3393
train acc:  0.828125
train loss:  0.3800349831581116
train gradient:  0.3049763739782295
iteration : 3394
train acc:  0.90625
train loss:  0.2780282497406006
train gradient:  0.15751309919957635
iteration : 3395
train acc:  0.8125
train loss:  0.4144970178604126
train gradient:  0.33073892649218745
iteration : 3396
train acc:  0.828125
train loss:  0.3983549177646637
train gradient:  0.46744912466581345
iteration : 3397
train acc:  0.875
train loss:  0.3025669455528259
train gradient:  0.16606374245315136
iteration : 3398
train acc:  0.8359375
train loss:  0.4035378098487854
train gradient:  0.3107480409050726
iteration : 3399
train acc:  0.859375
train loss:  0.33936065435409546
train gradient:  0.2673636212102821
iteration : 3400
train acc:  0.8671875
train loss:  0.3440660834312439
train gradient:  0.25415590842839864
iteration : 3401
train acc:  0.8125
train loss:  0.3570747673511505
train gradient:  0.28289734997430926
iteration : 3402
train acc:  0.828125
train loss:  0.38403502106666565
train gradient:  0.3634717149009696
iteration : 3403
train acc:  0.796875
train loss:  0.39366084337234497
train gradient:  0.2532937369095906
iteration : 3404
train acc:  0.890625
train loss:  0.35261598229408264
train gradient:  0.2472498572053398
iteration : 3405
train acc:  0.8359375
train loss:  0.44320419430732727
train gradient:  0.4202127774382565
iteration : 3406
train acc:  0.859375
train loss:  0.32735589146614075
train gradient:  0.2468776804264195
iteration : 3407
train acc:  0.8203125
train loss:  0.35321947932243347
train gradient:  0.2678567372039075
iteration : 3408
train acc:  0.78125
train loss:  0.4362829923629761
train gradient:  0.3603115008771027
iteration : 3409
train acc:  0.7890625
train loss:  0.4136778712272644
train gradient:  0.3463405856308388
iteration : 3410
train acc:  0.7890625
train loss:  0.42034220695495605
train gradient:  0.38784053395981377
iteration : 3411
train acc:  0.8828125
train loss:  0.2841150462627411
train gradient:  0.20128439399443243
iteration : 3412
train acc:  0.8203125
train loss:  0.37798821926116943
train gradient:  0.2805329886361849
iteration : 3413
train acc:  0.8359375
train loss:  0.4030323326587677
train gradient:  0.23180020014189356
iteration : 3414
train acc:  0.859375
train loss:  0.33691859245300293
train gradient:  0.3315369752164856
iteration : 3415
train acc:  0.84375
train loss:  0.36752498149871826
train gradient:  0.3037021692798259
iteration : 3416
train acc:  0.7890625
train loss:  0.4258309006690979
train gradient:  0.546280595235327
iteration : 3417
train acc:  0.8046875
train loss:  0.3544652462005615
train gradient:  0.257082298503792
iteration : 3418
train acc:  0.7890625
train loss:  0.39902544021606445
train gradient:  0.3586494113343683
iteration : 3419
train acc:  0.796875
train loss:  0.4218232035636902
train gradient:  0.3641980364083095
iteration : 3420
train acc:  0.8515625
train loss:  0.3745432198047638
train gradient:  0.34148149625212915
iteration : 3421
train acc:  0.8515625
train loss:  0.36767005920410156
train gradient:  0.2735952467229439
iteration : 3422
train acc:  0.7734375
train loss:  0.4299180209636688
train gradient:  0.29873344594203294
iteration : 3423
train acc:  0.765625
train loss:  0.47497084736824036
train gradient:  0.39594630167187866
iteration : 3424
train acc:  0.8046875
train loss:  0.3643299639225006
train gradient:  0.27916194920886844
iteration : 3425
train acc:  0.8046875
train loss:  0.4251413941383362
train gradient:  0.3680660030499288
iteration : 3426
train acc:  0.8515625
train loss:  0.3450022339820862
train gradient:  0.21561594722743346
iteration : 3427
train acc:  0.875
train loss:  0.34408506751060486
train gradient:  0.29143712802524596
iteration : 3428
train acc:  0.8046875
train loss:  0.3572691082954407
train gradient:  0.2271264706579054
iteration : 3429
train acc:  0.84375
train loss:  0.30584755539894104
train gradient:  0.25343508764795825
iteration : 3430
train acc:  0.8828125
train loss:  0.32014191150665283
train gradient:  0.19077547800716244
iteration : 3431
train acc:  0.8828125
train loss:  0.29877573251724243
train gradient:  0.21118978605461564
iteration : 3432
train acc:  0.9140625
train loss:  0.29501256346702576
train gradient:  0.21695475345982235
iteration : 3433
train acc:  0.7734375
train loss:  0.41922318935394287
train gradient:  0.4314614506305443
iteration : 3434
train acc:  0.875
train loss:  0.32566916942596436
train gradient:  0.22870667402877554
iteration : 3435
train acc:  0.84375
train loss:  0.41983628273010254
train gradient:  0.4726778039000742
iteration : 3436
train acc:  0.84375
train loss:  0.3514993190765381
train gradient:  0.25775249563644753
iteration : 3437
train acc:  0.8359375
train loss:  0.3501819670200348
train gradient:  0.2705509924275778
iteration : 3438
train acc:  0.8359375
train loss:  0.3406657874584198
train gradient:  0.29557777812818725
iteration : 3439
train acc:  0.8671875
train loss:  0.29874905943870544
train gradient:  0.1801153586815551
iteration : 3440
train acc:  0.8984375
train loss:  0.27811741828918457
train gradient:  0.2285987941498247
iteration : 3441
train acc:  0.8359375
train loss:  0.3803786635398865
train gradient:  0.2884021264726945
iteration : 3442
train acc:  0.84375
train loss:  0.3368648886680603
train gradient:  0.23298995656707427
iteration : 3443
train acc:  0.8515625
train loss:  0.34199216961860657
train gradient:  0.27485397269909
iteration : 3444
train acc:  0.828125
train loss:  0.45971813797950745
train gradient:  0.2917227018708176
iteration : 3445
train acc:  0.8359375
train loss:  0.37517207860946655
train gradient:  0.36590464427251707
iteration : 3446
train acc:  0.8515625
train loss:  0.3311265707015991
train gradient:  0.47356441430833107
iteration : 3447
train acc:  0.859375
train loss:  0.32548993825912476
train gradient:  0.19067313715890605
iteration : 3448
train acc:  0.8203125
train loss:  0.3861881196498871
train gradient:  0.2768128568705702
iteration : 3449
train acc:  0.859375
train loss:  0.3517721891403198
train gradient:  0.2605478626939882
iteration : 3450
train acc:  0.8203125
train loss:  0.3910796642303467
train gradient:  0.2572507979915683
iteration : 3451
train acc:  0.8046875
train loss:  0.42038241028785706
train gradient:  0.39154206172621225
iteration : 3452
train acc:  0.8125
train loss:  0.4046517014503479
train gradient:  0.2726546787464667
iteration : 3453
train acc:  0.8359375
train loss:  0.32161587476730347
train gradient:  0.20331326250306098
iteration : 3454
train acc:  0.859375
train loss:  0.31362009048461914
train gradient:  0.19515976782437533
iteration : 3455
train acc:  0.890625
train loss:  0.3439621329307556
train gradient:  0.20840761234810617
iteration : 3456
train acc:  0.859375
train loss:  0.3343201279640198
train gradient:  0.2656469104812072
iteration : 3457
train acc:  0.7734375
train loss:  0.5009236335754395
train gradient:  0.4252133367895069
iteration : 3458
train acc:  0.8671875
train loss:  0.33563295006752014
train gradient:  0.244473771271006
iteration : 3459
train acc:  0.8515625
train loss:  0.35652607679367065
train gradient:  0.17008879961646256
iteration : 3460
train acc:  0.7734375
train loss:  0.4711301922798157
train gradient:  0.3533308385536391
iteration : 3461
train acc:  0.890625
train loss:  0.2901760935783386
train gradient:  0.14051715450591862
iteration : 3462
train acc:  0.8046875
train loss:  0.44179844856262207
train gradient:  0.3472624481307048
iteration : 3463
train acc:  0.8671875
train loss:  0.27718091011047363
train gradient:  0.1934252390441165
iteration : 3464
train acc:  0.78125
train loss:  0.455098956823349
train gradient:  0.4181409474034595
iteration : 3465
train acc:  0.8125
train loss:  0.376854807138443
train gradient:  0.33919028806732254
iteration : 3466
train acc:  0.8515625
train loss:  0.3127344846725464
train gradient:  0.2745478109498271
iteration : 3467
train acc:  0.84375
train loss:  0.35600316524505615
train gradient:  0.23761790698332902
iteration : 3468
train acc:  0.875
train loss:  0.29153943061828613
train gradient:  0.20793324503211835
iteration : 3469
train acc:  0.8828125
train loss:  0.32688748836517334
train gradient:  0.31497688065518076
iteration : 3470
train acc:  0.8046875
train loss:  0.36919718980789185
train gradient:  0.27035105801398623
iteration : 3471
train acc:  0.84375
train loss:  0.35486680269241333
train gradient:  0.22456699402681024
iteration : 3472
train acc:  0.8359375
train loss:  0.4135698676109314
train gradient:  0.41820571084115304
iteration : 3473
train acc:  0.8203125
train loss:  0.4086517095565796
train gradient:  0.3078769507014614
iteration : 3474
train acc:  0.828125
train loss:  0.3540826737880707
train gradient:  0.226883385352943
iteration : 3475
train acc:  0.828125
train loss:  0.4512181878089905
train gradient:  0.3901217299867132
iteration : 3476
train acc:  0.796875
train loss:  0.39685124158859253
train gradient:  0.3061692702128754
iteration : 3477
train acc:  0.765625
train loss:  0.47832441329956055
train gradient:  0.4378452308018718
iteration : 3478
train acc:  0.8125
train loss:  0.40326741337776184
train gradient:  0.4374144026130414
iteration : 3479
train acc:  0.84375
train loss:  0.35259440541267395
train gradient:  0.3775381144677218
iteration : 3480
train acc:  0.859375
train loss:  0.35766375064849854
train gradient:  0.3636272187831748
iteration : 3481
train acc:  0.8046875
train loss:  0.4417774975299835
train gradient:  0.3487886238965645
iteration : 3482
train acc:  0.796875
train loss:  0.468224436044693
train gradient:  0.5190627482206595
iteration : 3483
train acc:  0.875
train loss:  0.296304315328598
train gradient:  0.1955685423516625
iteration : 3484
train acc:  0.8359375
train loss:  0.42529574036598206
train gradient:  0.40262043951194404
iteration : 3485
train acc:  0.765625
train loss:  0.5245038866996765
train gradient:  0.4521094748016654
iteration : 3486
train acc:  0.859375
train loss:  0.3406570851802826
train gradient:  0.2550568943385716
iteration : 3487
train acc:  0.890625
train loss:  0.29524052143096924
train gradient:  0.23625000371448446
iteration : 3488
train acc:  0.8203125
train loss:  0.4109060764312744
train gradient:  0.3336801943628029
iteration : 3489
train acc:  0.8125
train loss:  0.42599040269851685
train gradient:  0.30561802888430634
iteration : 3490
train acc:  0.8359375
train loss:  0.4409274458885193
train gradient:  0.4038021237629183
iteration : 3491
train acc:  0.828125
train loss:  0.3936089873313904
train gradient:  0.34624419222591246
iteration : 3492
train acc:  0.7890625
train loss:  0.4679955840110779
train gradient:  0.3891768324848003
iteration : 3493
train acc:  0.8203125
train loss:  0.3974933624267578
train gradient:  0.2827194257345815
iteration : 3494
train acc:  0.8203125
train loss:  0.37569060921669006
train gradient:  0.3450900782687099
iteration : 3495
train acc:  0.8125
train loss:  0.37674611806869507
train gradient:  0.30230946472474013
iteration : 3496
train acc:  0.8359375
train loss:  0.4321047067642212
train gradient:  0.4224353697220175
iteration : 3497
train acc:  0.7890625
train loss:  0.4128677248954773
train gradient:  0.3300255993039755
iteration : 3498
train acc:  0.84375
train loss:  0.364142507314682
train gradient:  0.27089891857987236
iteration : 3499
train acc:  0.8046875
train loss:  0.47056615352630615
train gradient:  0.41655547471869586
iteration : 3500
train acc:  0.84375
train loss:  0.3126141428947449
train gradient:  0.2185341207751016
iteration : 3501
train acc:  0.84375
train loss:  0.36074212193489075
train gradient:  0.29668506297831576
iteration : 3502
train acc:  0.84375
train loss:  0.3522276282310486
train gradient:  0.27102603107730716
iteration : 3503
train acc:  0.8515625
train loss:  0.32006630301475525
train gradient:  0.26328018403576703
iteration : 3504
train acc:  0.890625
train loss:  0.28996706008911133
train gradient:  0.20440075615388514
iteration : 3505
train acc:  0.8203125
train loss:  0.4270537495613098
train gradient:  0.377381497475893
iteration : 3506
train acc:  0.78125
train loss:  0.44229358434677124
train gradient:  0.25683146210499186
iteration : 3507
train acc:  0.828125
train loss:  0.4122548997402191
train gradient:  0.39936592424439754
iteration : 3508
train acc:  0.890625
train loss:  0.31020912528038025
train gradient:  0.2652347290162801
iteration : 3509
train acc:  0.8046875
train loss:  0.4048922061920166
train gradient:  0.3089404570960453
iteration : 3510
train acc:  0.828125
train loss:  0.37924331426620483
train gradient:  0.25830316558387945
iteration : 3511
train acc:  0.8125
train loss:  0.38747090101242065
train gradient:  0.2815448347588409
iteration : 3512
train acc:  0.84375
train loss:  0.38901057839393616
train gradient:  0.33190367875778926
iteration : 3513
train acc:  0.828125
train loss:  0.4111456274986267
train gradient:  0.24771139860987024
iteration : 3514
train acc:  0.859375
train loss:  0.3662630021572113
train gradient:  0.21930196884433278
iteration : 3515
train acc:  0.8359375
train loss:  0.37577128410339355
train gradient:  0.36100524824017716
iteration : 3516
train acc:  0.8828125
train loss:  0.3154749870300293
train gradient:  0.16470097658672334
iteration : 3517
train acc:  0.8046875
train loss:  0.42927658557891846
train gradient:  0.3580342564718823
iteration : 3518
train acc:  0.8671875
train loss:  0.3540474772453308
train gradient:  0.33444921632010394
iteration : 3519
train acc:  0.8046875
train loss:  0.36960887908935547
train gradient:  0.2523886101821553
iteration : 3520
train acc:  0.8046875
train loss:  0.41641896963119507
train gradient:  0.3235896201259726
iteration : 3521
train acc:  0.8828125
train loss:  0.3356545567512512
train gradient:  0.24609251057726003
iteration : 3522
train acc:  0.828125
train loss:  0.4109141528606415
train gradient:  0.43237763933941614
iteration : 3523
train acc:  0.8046875
train loss:  0.37695419788360596
train gradient:  0.25191660075257966
iteration : 3524
train acc:  0.8203125
train loss:  0.3932095766067505
train gradient:  0.3795277048224191
iteration : 3525
train acc:  0.8515625
train loss:  0.3207498788833618
train gradient:  0.3714530535897877
iteration : 3526
train acc:  0.8125
train loss:  0.4313274025917053
train gradient:  0.36421013589949697
iteration : 3527
train acc:  0.8125
train loss:  0.37512779235839844
train gradient:  0.3816355658371713
iteration : 3528
train acc:  0.8203125
train loss:  0.3917441964149475
train gradient:  0.24735747289147092
iteration : 3529
train acc:  0.8515625
train loss:  0.32922881841659546
train gradient:  0.1817105267548667
iteration : 3530
train acc:  0.8359375
train loss:  0.38071000576019287
train gradient:  0.2513308796905429
iteration : 3531
train acc:  0.875
train loss:  0.30049675703048706
train gradient:  0.2512631276028693
iteration : 3532
train acc:  0.8515625
train loss:  0.36429813504219055
train gradient:  0.2509886521186813
iteration : 3533
train acc:  0.84375
train loss:  0.3510226011276245
train gradient:  0.2669376272852296
iteration : 3534
train acc:  0.84375
train loss:  0.32750120759010315
train gradient:  0.27462053562979
iteration : 3535
train acc:  0.84375
train loss:  0.33805394172668457
train gradient:  0.4179565228006953
iteration : 3536
train acc:  0.765625
train loss:  0.4939112961292267
train gradient:  0.4771458162799808
iteration : 3537
train acc:  0.8203125
train loss:  0.41905677318573
train gradient:  0.3790311106205176
iteration : 3538
train acc:  0.8515625
train loss:  0.31579291820526123
train gradient:  0.2559427858605309
iteration : 3539
train acc:  0.84375
train loss:  0.3344699740409851
train gradient:  0.23435272998734696
iteration : 3540
train acc:  0.8671875
train loss:  0.30756449699401855
train gradient:  0.17050462419221238
iteration : 3541
train acc:  0.7734375
train loss:  0.44446811079978943
train gradient:  0.41612123691202935
iteration : 3542
train acc:  0.8828125
train loss:  0.3276624381542206
train gradient:  0.2918350939828103
iteration : 3543
train acc:  0.8125
train loss:  0.4374273717403412
train gradient:  0.39231875068592553
iteration : 3544
train acc:  0.8203125
train loss:  0.3778732120990753
train gradient:  0.2987384843899692
iteration : 3545
train acc:  0.8046875
train loss:  0.5054004788398743
train gradient:  0.4118310678270489
iteration : 3546
train acc:  0.8515625
train loss:  0.3102341294288635
train gradient:  0.23594403763395277
iteration : 3547
train acc:  0.8359375
train loss:  0.3312838077545166
train gradient:  0.17145344827735787
iteration : 3548
train acc:  0.78125
train loss:  0.4619518518447876
train gradient:  0.3904206895715082
iteration : 3549
train acc:  0.8359375
train loss:  0.3808700442314148
train gradient:  0.3649438556542465
iteration : 3550
train acc:  0.7890625
train loss:  0.39443475008010864
train gradient:  0.3664035546913358
iteration : 3551
train acc:  0.875
train loss:  0.3322140574455261
train gradient:  0.23472746952219084
iteration : 3552
train acc:  0.828125
train loss:  0.3715391159057617
train gradient:  0.25990055334846324
iteration : 3553
train acc:  0.8203125
train loss:  0.37988415360450745
train gradient:  0.3227028440918665
iteration : 3554
train acc:  0.8984375
train loss:  0.2651755213737488
train gradient:  0.2257867533155218
iteration : 3555
train acc:  0.8359375
train loss:  0.34138011932373047
train gradient:  0.26576053629055635
iteration : 3556
train acc:  0.8671875
train loss:  0.3363386392593384
train gradient:  0.3463037566333229
iteration : 3557
train acc:  0.7890625
train loss:  0.4496520161628723
train gradient:  0.39787711157249034
iteration : 3558
train acc:  0.8515625
train loss:  0.2999698519706726
train gradient:  0.18363933309765307
iteration : 3559
train acc:  0.84375
train loss:  0.3599078059196472
train gradient:  0.4186994114268705
iteration : 3560
train acc:  0.8671875
train loss:  0.3343092203140259
train gradient:  0.19291669388170365
iteration : 3561
train acc:  0.8828125
train loss:  0.4183817207813263
train gradient:  0.47138605711545356
iteration : 3562
train acc:  0.828125
train loss:  0.4195990264415741
train gradient:  0.37200590078514284
iteration : 3563
train acc:  0.875
train loss:  0.31136515736579895
train gradient:  0.21985928524020384
iteration : 3564
train acc:  0.859375
train loss:  0.37901753187179565
train gradient:  0.24352065750259783
iteration : 3565
train acc:  0.90625
train loss:  0.25639694929122925
train gradient:  0.1419221604379878
iteration : 3566
train acc:  0.875
train loss:  0.3099719285964966
train gradient:  0.2634400085198008
iteration : 3567
train acc:  0.796875
train loss:  0.4024754762649536
train gradient:  0.30474245058261923
iteration : 3568
train acc:  0.8203125
train loss:  0.4219425320625305
train gradient:  0.4034231286683434
iteration : 3569
train acc:  0.8359375
train loss:  0.4057358503341675
train gradient:  0.3489376428640328
iteration : 3570
train acc:  0.84375
train loss:  0.33161473274230957
train gradient:  0.22602457168570922
iteration : 3571
train acc:  0.828125
train loss:  0.40231508016586304
train gradient:  0.3100535039238575
iteration : 3572
train acc:  0.8359375
train loss:  0.3781951665878296
train gradient:  0.38506996320074954
iteration : 3573
train acc:  0.8203125
train loss:  0.3607891798019409
train gradient:  0.36601644201564787
iteration : 3574
train acc:  0.8125
train loss:  0.46736329793930054
train gradient:  0.49231476289307147
iteration : 3575
train acc:  0.8515625
train loss:  0.3856108486652374
train gradient:  0.505415067385961
iteration : 3576
train acc:  0.828125
train loss:  0.3977601230144501
train gradient:  0.33991141077505865
iteration : 3577
train acc:  0.859375
train loss:  0.2928190529346466
train gradient:  0.2406125623074452
iteration : 3578
train acc:  0.859375
train loss:  0.3576164245605469
train gradient:  0.2500009341375455
iteration : 3579
train acc:  0.875
train loss:  0.2936338782310486
train gradient:  0.2927377764035724
iteration : 3580
train acc:  0.7890625
train loss:  0.406480073928833
train gradient:  0.34390014572892724
iteration : 3581
train acc:  0.8828125
train loss:  0.3156365156173706
train gradient:  0.25052316548571335
iteration : 3582
train acc:  0.859375
train loss:  0.35067179799079895
train gradient:  0.3061780973839732
iteration : 3583
train acc:  0.859375
train loss:  0.36205893754959106
train gradient:  0.22043642039539996
iteration : 3584
train acc:  0.859375
train loss:  0.29392218589782715
train gradient:  0.20536569893450277
iteration : 3585
train acc:  0.84375
train loss:  0.4057996869087219
train gradient:  0.34283968561952444
iteration : 3586
train acc:  0.84375
train loss:  0.3966541588306427
train gradient:  0.5501251320645208
iteration : 3587
train acc:  0.890625
train loss:  0.3013812303543091
train gradient:  0.2679845315544802
iteration : 3588
train acc:  0.859375
train loss:  0.35328081250190735
train gradient:  0.292918091176633
iteration : 3589
train acc:  0.84375
train loss:  0.4084479808807373
train gradient:  0.3484745490840538
iteration : 3590
train acc:  0.8203125
train loss:  0.3613106608390808
train gradient:  0.3334733405533053
iteration : 3591
train acc:  0.8125
train loss:  0.3644776940345764
train gradient:  0.252778507329887
iteration : 3592
train acc:  0.8046875
train loss:  0.40196165442466736
train gradient:  0.342033312426602
iteration : 3593
train acc:  0.8125
train loss:  0.360795259475708
train gradient:  0.3758947145524858
iteration : 3594
train acc:  0.8125
train loss:  0.41970115900039673
train gradient:  0.3248371386403135
iteration : 3595
train acc:  0.828125
train loss:  0.37643134593963623
train gradient:  0.3562377181551345
iteration : 3596
train acc:  0.7578125
train loss:  0.4772867262363434
train gradient:  0.4737153303805288
iteration : 3597
train acc:  0.765625
train loss:  0.44421496987342834
train gradient:  0.42348015716683823
iteration : 3598
train acc:  0.8203125
train loss:  0.4268563687801361
train gradient:  0.45981409167333226
iteration : 3599
train acc:  0.828125
train loss:  0.35813039541244507
train gradient:  0.21677390060765728
iteration : 3600
train acc:  0.8125
train loss:  0.424763023853302
train gradient:  0.4803732020098292
iteration : 3601
train acc:  0.8984375
train loss:  0.323122501373291
train gradient:  0.2281414754110635
iteration : 3602
train acc:  0.7734375
train loss:  0.4252004623413086
train gradient:  0.3875427977233662
iteration : 3603
train acc:  0.828125
train loss:  0.37534213066101074
train gradient:  0.27324446031207905
iteration : 3604
train acc:  0.8125
train loss:  0.37256038188934326
train gradient:  0.3132919549399094
iteration : 3605
train acc:  0.8359375
train loss:  0.3818630874156952
train gradient:  0.2537655495803056
iteration : 3606
train acc:  0.8359375
train loss:  0.3938630223274231
train gradient:  0.3210680909771985
iteration : 3607
train acc:  0.7734375
train loss:  0.49078214168548584
train gradient:  0.424062679545166
iteration : 3608
train acc:  0.8359375
train loss:  0.3739425241947174
train gradient:  0.23119140559762258
iteration : 3609
train acc:  0.7890625
train loss:  0.41504207253456116
train gradient:  0.3672611476783597
iteration : 3610
train acc:  0.8359375
train loss:  0.4040853679180145
train gradient:  0.2950826880546628
iteration : 3611
train acc:  0.796875
train loss:  0.39479005336761475
train gradient:  0.3289294042101456
iteration : 3612
train acc:  0.8671875
train loss:  0.32991456985473633
train gradient:  0.27360328769015707
iteration : 3613
train acc:  0.7890625
train loss:  0.41586607694625854
train gradient:  0.37665154772703885
iteration : 3614
train acc:  0.859375
train loss:  0.34671029448509216
train gradient:  0.5103275500948588
iteration : 3615
train acc:  0.8359375
train loss:  0.35499417781829834
train gradient:  0.2447404186872355
iteration : 3616
train acc:  0.9140625
train loss:  0.261271595954895
train gradient:  0.17076665969635152
iteration : 3617
train acc:  0.859375
train loss:  0.3317870795726776
train gradient:  0.2121858155860104
iteration : 3618
train acc:  0.875
train loss:  0.3386589288711548
train gradient:  0.21004815138214167
iteration : 3619
train acc:  0.8515625
train loss:  0.395263135433197
train gradient:  0.2888036102101153
iteration : 3620
train acc:  0.890625
train loss:  0.29863154888153076
train gradient:  0.2264653637094678
iteration : 3621
train acc:  0.8203125
train loss:  0.3666864037513733
train gradient:  0.22011942530637799
iteration : 3622
train acc:  0.796875
train loss:  0.3608204126358032
train gradient:  0.2560695531950744
iteration : 3623
train acc:  0.796875
train loss:  0.38616669178009033
train gradient:  0.3534151355528147
iteration : 3624
train acc:  0.84375
train loss:  0.3852076530456543
train gradient:  0.27986969614079926
iteration : 3625
train acc:  0.8125
train loss:  0.3446810841560364
train gradient:  0.33344685774433075
iteration : 3626
train acc:  0.859375
train loss:  0.3060758411884308
train gradient:  0.18028586398605265
iteration : 3627
train acc:  0.8359375
train loss:  0.33102184534072876
train gradient:  0.32613058606224976
iteration : 3628
train acc:  0.828125
train loss:  0.35692504048347473
train gradient:  0.24313717951119923
iteration : 3629
train acc:  0.8515625
train loss:  0.35613587498664856
train gradient:  0.21120250909550425
iteration : 3630
train acc:  0.90625
train loss:  0.3209593892097473
train gradient:  0.1980877111109171
iteration : 3631
train acc:  0.8359375
train loss:  0.4117623269557953
train gradient:  0.3712639531452579
iteration : 3632
train acc:  0.8515625
train loss:  0.31171587109565735
train gradient:  0.2777563657579471
iteration : 3633
train acc:  0.84375
train loss:  0.33946579694747925
train gradient:  0.20884086005982924
iteration : 3634
train acc:  0.8515625
train loss:  0.34221506118774414
train gradient:  0.21382647519640668
iteration : 3635
train acc:  0.828125
train loss:  0.37578168511390686
train gradient:  0.30335182571347363
iteration : 3636
train acc:  0.7734375
train loss:  0.4602738618850708
train gradient:  0.40170769020319563
iteration : 3637
train acc:  0.875
train loss:  0.3385655879974365
train gradient:  0.37061909241871854
iteration : 3638
train acc:  0.8515625
train loss:  0.34725457429885864
train gradient:  0.20313132773219933
iteration : 3639
train acc:  0.7109375
train loss:  0.5194754600524902
train gradient:  0.6536172760634459
iteration : 3640
train acc:  0.8125
train loss:  0.3935622572898865
train gradient:  0.4020157066402007
iteration : 3641
train acc:  0.875
train loss:  0.27324730157852173
train gradient:  0.1813844545352186
iteration : 3642
train acc:  0.8828125
train loss:  0.2893493175506592
train gradient:  0.18196020959978595
iteration : 3643
train acc:  0.8515625
train loss:  0.3327517509460449
train gradient:  0.28757428834372145
iteration : 3644
train acc:  0.8828125
train loss:  0.31585419178009033
train gradient:  0.22404500154387827
iteration : 3645
train acc:  0.8984375
train loss:  0.29688525199890137
train gradient:  0.19511614816175238
iteration : 3646
train acc:  0.765625
train loss:  0.4993056654930115
train gradient:  0.61100735916776
iteration : 3647
train acc:  0.8046875
train loss:  0.4491459131240845
train gradient:  0.38347014001024404
iteration : 3648
train acc:  0.828125
train loss:  0.31292077898979187
train gradient:  0.2632298067867738
iteration : 3649
train acc:  0.859375
train loss:  0.3475380539894104
train gradient:  0.2200999271651905
iteration : 3650
train acc:  0.765625
train loss:  0.3887460231781006
train gradient:  0.32634986070459365
iteration : 3651
train acc:  0.8359375
train loss:  0.387667179107666
train gradient:  0.2803121162709246
iteration : 3652
train acc:  0.84375
train loss:  0.32660973072052
train gradient:  0.26624264588459073
iteration : 3653
train acc:  0.84375
train loss:  0.3212224841117859
train gradient:  0.19184181213126628
iteration : 3654
train acc:  0.796875
train loss:  0.402026504278183
train gradient:  0.43229949766411535
iteration : 3655
train acc:  0.828125
train loss:  0.3518750071525574
train gradient:  0.2171597683638657
iteration : 3656
train acc:  0.9296875
train loss:  0.24525557458400726
train gradient:  0.16210799694356354
iteration : 3657
train acc:  0.8515625
train loss:  0.3318241238594055
train gradient:  0.2719306342026893
iteration : 3658
train acc:  0.8359375
train loss:  0.4012684226036072
train gradient:  0.332273608064943
iteration : 3659
train acc:  0.84375
train loss:  0.3596135377883911
train gradient:  0.29583319074105024
iteration : 3660
train acc:  0.78125
train loss:  0.4482535123825073
train gradient:  0.4334283567623976
iteration : 3661
train acc:  0.8203125
train loss:  0.3811153769493103
train gradient:  0.2673736819626795
iteration : 3662
train acc:  0.828125
train loss:  0.38915109634399414
train gradient:  0.41105990696142036
iteration : 3663
train acc:  0.8125
train loss:  0.3758205473423004
train gradient:  0.32026758644213477
iteration : 3664
train acc:  0.859375
train loss:  0.36077940464019775
train gradient:  0.23144684543869015
iteration : 3665
train acc:  0.7578125
train loss:  0.5386150479316711
train gradient:  0.5177109773312698
iteration : 3666
train acc:  0.78125
train loss:  0.4018409550189972
train gradient:  0.35160263060114505
iteration : 3667
train acc:  0.8359375
train loss:  0.39404356479644775
train gradient:  0.283409866459674
iteration : 3668
train acc:  0.8203125
train loss:  0.43005454540252686
train gradient:  0.33452580497341033
iteration : 3669
train acc:  0.8125
train loss:  0.34762880206108093
train gradient:  0.26209670595447443
iteration : 3670
train acc:  0.859375
train loss:  0.38719671964645386
train gradient:  0.3045793946038811
iteration : 3671
train acc:  0.828125
train loss:  0.4021778106689453
train gradient:  0.2907455514011341
iteration : 3672
train acc:  0.796875
train loss:  0.39674630761146545
train gradient:  0.2841107456181601
iteration : 3673
train acc:  0.8203125
train loss:  0.35204094648361206
train gradient:  0.2812906746169454
iteration : 3674
train acc:  0.859375
train loss:  0.29266780614852905
train gradient:  0.22752957495503262
iteration : 3675
train acc:  0.8046875
train loss:  0.38915079832077026
train gradient:  0.27129055931882357
iteration : 3676
train acc:  0.828125
train loss:  0.4208711087703705
train gradient:  0.33552959050472236
iteration : 3677
train acc:  0.8359375
train loss:  0.3731555938720703
train gradient:  0.4997437290209366
iteration : 3678
train acc:  0.84375
train loss:  0.3312758505344391
train gradient:  0.2232248692230427
iteration : 3679
train acc:  0.8359375
train loss:  0.38485437631607056
train gradient:  0.23560607771090664
iteration : 3680
train acc:  0.8359375
train loss:  0.36711716651916504
train gradient:  0.2552776559133946
iteration : 3681
train acc:  0.7734375
train loss:  0.5293540358543396
train gradient:  0.446164313887413
iteration : 3682
train acc:  0.8203125
train loss:  0.3573763370513916
train gradient:  0.30027598154969487
iteration : 3683
train acc:  0.859375
train loss:  0.3495032787322998
train gradient:  0.22283951675371128
iteration : 3684
train acc:  0.859375
train loss:  0.331659197807312
train gradient:  0.2168131107171043
iteration : 3685
train acc:  0.859375
train loss:  0.3441528379917145
train gradient:  0.36267415864653496
iteration : 3686
train acc:  0.890625
train loss:  0.2678852081298828
train gradient:  0.15880884577658552
iteration : 3687
train acc:  0.828125
train loss:  0.36981308460235596
train gradient:  0.24847238168662772
iteration : 3688
train acc:  0.8984375
train loss:  0.2935939431190491
train gradient:  0.18854734540272322
iteration : 3689
train acc:  0.8046875
train loss:  0.4066718816757202
train gradient:  0.2963547492550124
iteration : 3690
train acc:  0.84375
train loss:  0.3138083517551422
train gradient:  0.2517105045255238
iteration : 3691
train acc:  0.796875
train loss:  0.39515501260757446
train gradient:  0.27162809162441687
iteration : 3692
train acc:  0.828125
train loss:  0.3750002682209015
train gradient:  0.2293712670648862
iteration : 3693
train acc:  0.8359375
train loss:  0.36584973335266113
train gradient:  0.24569227821119335
iteration : 3694
train acc:  0.8203125
train loss:  0.4102904796600342
train gradient:  0.2938003869140123
iteration : 3695
train acc:  0.859375
train loss:  0.3348627984523773
train gradient:  0.20643146230870563
iteration : 3696
train acc:  0.875
train loss:  0.32955342531204224
train gradient:  0.16583752460091794
iteration : 3697
train acc:  0.8359375
train loss:  0.3492457866668701
train gradient:  0.24968282403930941
iteration : 3698
train acc:  0.8203125
train loss:  0.357460081577301
train gradient:  0.2372405253222265
iteration : 3699
train acc:  0.8125
train loss:  0.39546871185302734
train gradient:  0.4520546826789079
iteration : 3700
train acc:  0.8125
train loss:  0.3637690544128418
train gradient:  0.25096990888845616
iteration : 3701
train acc:  0.9140625
train loss:  0.2780074179172516
train gradient:  0.24461097597895548
iteration : 3702
train acc:  0.8046875
train loss:  0.4697953462600708
train gradient:  0.5228553316663037
iteration : 3703
train acc:  0.859375
train loss:  0.29831886291503906
train gradient:  0.17005493691787704
iteration : 3704
train acc:  0.8515625
train loss:  0.35170215368270874
train gradient:  0.24674309235288155
iteration : 3705
train acc:  0.8359375
train loss:  0.3511396050453186
train gradient:  0.21764221639049744
iteration : 3706
train acc:  0.8515625
train loss:  0.3501509428024292
train gradient:  0.198666210050421
iteration : 3707
train acc:  0.7421875
train loss:  0.5081900358200073
train gradient:  0.5841950297807512
iteration : 3708
train acc:  0.8671875
train loss:  0.31615620851516724
train gradient:  0.18319077157954317
iteration : 3709
train acc:  0.765625
train loss:  0.49133893847465515
train gradient:  0.4227921515487984
iteration : 3710
train acc:  0.828125
train loss:  0.398586630821228
train gradient:  0.279941029865373
iteration : 3711
train acc:  0.8125
train loss:  0.40563714504241943
train gradient:  0.27704619932455893
iteration : 3712
train acc:  0.90625
train loss:  0.2879772186279297
train gradient:  0.21056424570406568
iteration : 3713
train acc:  0.8203125
train loss:  0.3855699896812439
train gradient:  0.4616613880701016
iteration : 3714
train acc:  0.828125
train loss:  0.37518471479415894
train gradient:  0.3028065404059158
iteration : 3715
train acc:  0.7734375
train loss:  0.46717506647109985
train gradient:  0.4736708536068197
iteration : 3716
train acc:  0.8515625
train loss:  0.34398186206817627
train gradient:  0.25491080290495477
iteration : 3717
train acc:  0.8359375
train loss:  0.368127703666687
train gradient:  0.306742652674814
iteration : 3718
train acc:  0.828125
train loss:  0.36872395873069763
train gradient:  0.29913327513856264
iteration : 3719
train acc:  0.8125
train loss:  0.420307993888855
train gradient:  0.37128273982490606
iteration : 3720
train acc:  0.7265625
train loss:  0.5103972554206848
train gradient:  0.40648860556773364
iteration : 3721
train acc:  0.8984375
train loss:  0.32841479778289795
train gradient:  0.2080054276149329
iteration : 3722
train acc:  0.8125
train loss:  0.399417519569397
train gradient:  0.33309595091111827
iteration : 3723
train acc:  0.8984375
train loss:  0.29820817708969116
train gradient:  0.23131961831878384
iteration : 3724
train acc:  0.8828125
train loss:  0.34944653511047363
train gradient:  0.2736177499156126
iteration : 3725
train acc:  0.8359375
train loss:  0.33678650856018066
train gradient:  0.19903306652298006
iteration : 3726
train acc:  0.828125
train loss:  0.3889199495315552
train gradient:  0.22789230780879438
iteration : 3727
train acc:  0.828125
train loss:  0.3385394215583801
train gradient:  0.19575319265368143
iteration : 3728
train acc:  0.84375
train loss:  0.3361262083053589
train gradient:  0.3251047774165035
iteration : 3729
train acc:  0.875
train loss:  0.3420940637588501
train gradient:  0.29569090959665173
iteration : 3730
train acc:  0.90625
train loss:  0.3096427321434021
train gradient:  0.1465035132787994
iteration : 3731
train acc:  0.78125
train loss:  0.3973243832588196
train gradient:  0.3103879739762963
iteration : 3732
train acc:  0.7890625
train loss:  0.43160176277160645
train gradient:  0.3165041968730784
iteration : 3733
train acc:  0.8984375
train loss:  0.27307209372520447
train gradient:  0.2108736732162605
iteration : 3734
train acc:  0.7890625
train loss:  0.38452982902526855
train gradient:  0.1884601100053938
iteration : 3735
train acc:  0.7890625
train loss:  0.4349460303783417
train gradient:  0.4494531869330755
iteration : 3736
train acc:  0.8359375
train loss:  0.351709246635437
train gradient:  0.3068049282163845
iteration : 3737
train acc:  0.859375
train loss:  0.34667566418647766
train gradient:  0.2768043685044153
iteration : 3738
train acc:  0.8046875
train loss:  0.4023844003677368
train gradient:  0.3155162594697608
iteration : 3739
train acc:  0.7890625
train loss:  0.3920677900314331
train gradient:  0.2570669088607596
iteration : 3740
train acc:  0.8125
train loss:  0.40286603569984436
train gradient:  0.5686930164018509
iteration : 3741
train acc:  0.8515625
train loss:  0.334642231464386
train gradient:  0.2229186082809782
iteration : 3742
train acc:  0.8828125
train loss:  0.3115001916885376
train gradient:  0.3051860224258933
iteration : 3743
train acc:  0.84375
train loss:  0.4000359773635864
train gradient:  0.2871965626958046
iteration : 3744
train acc:  0.8359375
train loss:  0.3820819556713104
train gradient:  0.20593739701642644
iteration : 3745
train acc:  0.8671875
train loss:  0.3798038959503174
train gradient:  0.2850341261330455
iteration : 3746
train acc:  0.8359375
train loss:  0.34177273511886597
train gradient:  0.2545876246520974
iteration : 3747
train acc:  0.8359375
train loss:  0.36698800325393677
train gradient:  0.3247047414547715
iteration : 3748
train acc:  0.8515625
train loss:  0.3361949622631073
train gradient:  0.228179314161221
iteration : 3749
train acc:  0.8671875
train loss:  0.36218562722206116
train gradient:  0.2772934231464913
iteration : 3750
train acc:  0.8671875
train loss:  0.34784162044525146
train gradient:  0.480585680032003
iteration : 3751
train acc:  0.8515625
train loss:  0.36114585399627686
train gradient:  0.36947778247268026
iteration : 3752
train acc:  0.8828125
train loss:  0.3033444583415985
train gradient:  0.18088637369636068
iteration : 3753
train acc:  0.859375
train loss:  0.324047327041626
train gradient:  0.2479781048822573
iteration : 3754
train acc:  0.84375
train loss:  0.35557249188423157
train gradient:  0.34770722358175327
iteration : 3755
train acc:  0.8828125
train loss:  0.32934343814849854
train gradient:  0.2663272082589708
iteration : 3756
train acc:  0.890625
train loss:  0.28397536277770996
train gradient:  0.19605894242981542
iteration : 3757
train acc:  0.8203125
train loss:  0.4201391339302063
train gradient:  0.31171975710677063
iteration : 3758
train acc:  0.8046875
train loss:  0.385129451751709
train gradient:  0.382961819727226
iteration : 3759
train acc:  0.8828125
train loss:  0.31492555141448975
train gradient:  0.28049222418845354
iteration : 3760
train acc:  0.8203125
train loss:  0.35079747438430786
train gradient:  0.3412771994950505
iteration : 3761
train acc:  0.875
train loss:  0.33522605895996094
train gradient:  0.2644456711148571
iteration : 3762
train acc:  0.8046875
train loss:  0.40306326746940613
train gradient:  0.28108972013387146
iteration : 3763
train acc:  0.8359375
train loss:  0.3294302225112915
train gradient:  0.4354917564428484
iteration : 3764
train acc:  0.8203125
train loss:  0.49073413014411926
train gradient:  0.6331772619220635
iteration : 3765
train acc:  0.8828125
train loss:  0.32508033514022827
train gradient:  0.28075146609213103
iteration : 3766
train acc:  0.8359375
train loss:  0.39654991030693054
train gradient:  0.3677618188637914
iteration : 3767
train acc:  0.828125
train loss:  0.43348202109336853
train gradient:  0.3300727344538938
iteration : 3768
train acc:  0.875
train loss:  0.27168402075767517
train gradient:  0.18415116319988054
iteration : 3769
train acc:  0.8046875
train loss:  0.4064634442329407
train gradient:  0.32443205065579006
iteration : 3770
train acc:  0.8203125
train loss:  0.34598663449287415
train gradient:  0.2719118961144979
iteration : 3771
train acc:  0.8125
train loss:  0.3991394340991974
train gradient:  0.33255927128840246
iteration : 3772
train acc:  0.8203125
train loss:  0.3532572388648987
train gradient:  0.29579699668594023
iteration : 3773
train acc:  0.859375
train loss:  0.3602589964866638
train gradient:  0.27856425960523934
iteration : 3774
train acc:  0.8515625
train loss:  0.34298187494277954
train gradient:  0.25856355785105134
iteration : 3775
train acc:  0.8046875
train loss:  0.42316049337387085
train gradient:  0.33647246591416063
iteration : 3776
train acc:  0.8203125
train loss:  0.32951605319976807
train gradient:  0.3670777236533061
iteration : 3777
train acc:  0.8359375
train loss:  0.32205259799957275
train gradient:  0.2910921849965679
iteration : 3778
train acc:  0.7890625
train loss:  0.3699798583984375
train gradient:  0.43267295936816164
iteration : 3779
train acc:  0.8671875
train loss:  0.29994913935661316
train gradient:  0.22691709280215816
iteration : 3780
train acc:  0.7734375
train loss:  0.42588579654693604
train gradient:  0.285716877256774
iteration : 3781
train acc:  0.8046875
train loss:  0.4271307587623596
train gradient:  0.5452323586461301
iteration : 3782
train acc:  0.8671875
train loss:  0.3203331232070923
train gradient:  0.2421534325490945
iteration : 3783
train acc:  0.859375
train loss:  0.3466716706752777
train gradient:  0.3402605169797732
iteration : 3784
train acc:  0.8515625
train loss:  0.3682093620300293
train gradient:  0.22896376514778544
iteration : 3785
train acc:  0.859375
train loss:  0.34035253524780273
train gradient:  0.3037945167705924
iteration : 3786
train acc:  0.875
train loss:  0.3108380436897278
train gradient:  0.248278982042736
iteration : 3787
train acc:  0.890625
train loss:  0.3283199667930603
train gradient:  0.22046053968300297
iteration : 3788
train acc:  0.8515625
train loss:  0.33998069167137146
train gradient:  0.24671435296690727
iteration : 3789
train acc:  0.7890625
train loss:  0.4784165024757385
train gradient:  0.37310357628989327
iteration : 3790
train acc:  0.828125
train loss:  0.34603065252304077
train gradient:  0.3542714470468223
iteration : 3791
train acc:  0.890625
train loss:  0.3310060501098633
train gradient:  0.26863774215781644
iteration : 3792
train acc:  0.8046875
train loss:  0.38121503591537476
train gradient:  0.28971704937541665
iteration : 3793
train acc:  0.796875
train loss:  0.4291267395019531
train gradient:  0.3328371697053183
iteration : 3794
train acc:  0.8515625
train loss:  0.35521456599235535
train gradient:  0.26269969099009294
iteration : 3795
train acc:  0.796875
train loss:  0.3825419545173645
train gradient:  0.25213292841838764
iteration : 3796
train acc:  0.8125
train loss:  0.4050569534301758
train gradient:  0.2720743073786032
iteration : 3797
train acc:  0.84375
train loss:  0.3308795094490051
train gradient:  0.263053745341483
iteration : 3798
train acc:  0.8203125
train loss:  0.33970314264297485
train gradient:  0.3577918819791086
iteration : 3799
train acc:  0.859375
train loss:  0.3793097138404846
train gradient:  0.29598055318768174
iteration : 3800
train acc:  0.8046875
train loss:  0.350073903799057
train gradient:  0.451942170318584
iteration : 3801
train acc:  0.859375
train loss:  0.32012489438056946
train gradient:  0.2029765497312066
iteration : 3802
train acc:  0.8359375
train loss:  0.37893521785736084
train gradient:  0.24935649485611372
iteration : 3803
train acc:  0.9140625
train loss:  0.25988292694091797
train gradient:  0.15509028930959878
iteration : 3804
train acc:  0.859375
train loss:  0.3442986309528351
train gradient:  0.26382018675690644
iteration : 3805
train acc:  0.8203125
train loss:  0.3846259117126465
train gradient:  0.32770600652893495
iteration : 3806
train acc:  0.8203125
train loss:  0.3648117184638977
train gradient:  0.27760494279910064
iteration : 3807
train acc:  0.859375
train loss:  0.3458143472671509
train gradient:  0.2807228422117016
iteration : 3808
train acc:  0.859375
train loss:  0.32004696130752563
train gradient:  0.1976637821588555
iteration : 3809
train acc:  0.828125
train loss:  0.3302258253097534
train gradient:  0.3522278815921776
iteration : 3810
train acc:  0.828125
train loss:  0.3532842993736267
train gradient:  0.4036314743459334
iteration : 3811
train acc:  0.8125
train loss:  0.423531711101532
train gradient:  0.3510504279890263
iteration : 3812
train acc:  0.8984375
train loss:  0.2960061728954315
train gradient:  0.1922418908944862
iteration : 3813
train acc:  0.828125
train loss:  0.35091835260391235
train gradient:  0.34232220278935815
iteration : 3814
train acc:  0.890625
train loss:  0.32470762729644775
train gradient:  0.3891578288098432
iteration : 3815
train acc:  0.8671875
train loss:  0.294324666261673
train gradient:  0.27492928712829046
iteration : 3816
train acc:  0.8359375
train loss:  0.3381502628326416
train gradient:  0.2342833227666617
iteration : 3817
train acc:  0.8203125
train loss:  0.3196942210197449
train gradient:  0.21803745721947004
iteration : 3818
train acc:  0.8359375
train loss:  0.3614298701286316
train gradient:  0.3607172428454057
iteration : 3819
train acc:  0.84375
train loss:  0.3475799560546875
train gradient:  0.3162480039744689
iteration : 3820
train acc:  0.84375
train loss:  0.3779703676700592
train gradient:  0.36327959587445496
iteration : 3821
train acc:  0.859375
train loss:  0.3397480249404907
train gradient:  0.26236224012899256
iteration : 3822
train acc:  0.8125
train loss:  0.3917009234428406
train gradient:  0.39966320639970004
iteration : 3823
train acc:  0.8203125
train loss:  0.4016267657279968
train gradient:  0.3742193712183056
iteration : 3824
train acc:  0.890625
train loss:  0.28968214988708496
train gradient:  0.20548294986483148
iteration : 3825
train acc:  0.8359375
train loss:  0.3123546838760376
train gradient:  0.23150232113860683
iteration : 3826
train acc:  0.8828125
train loss:  0.31810083985328674
train gradient:  0.26020272253280996
iteration : 3827
train acc:  0.8828125
train loss:  0.2788180708885193
train gradient:  0.2183613404531622
iteration : 3828
train acc:  0.8125
train loss:  0.362155944108963
train gradient:  0.3408515107948935
iteration : 3829
train acc:  0.828125
train loss:  0.35928064584732056
train gradient:  0.32808437786120936
iteration : 3830
train acc:  0.796875
train loss:  0.40746647119522095
train gradient:  0.5375506721875563
iteration : 3831
train acc:  0.8359375
train loss:  0.32847529649734497
train gradient:  0.2906118046881999
iteration : 3832
train acc:  0.828125
train loss:  0.36761438846588135
train gradient:  0.3726489746627517
iteration : 3833
train acc:  0.828125
train loss:  0.4131229817867279
train gradient:  0.35953730997722416
iteration : 3834
train acc:  0.8671875
train loss:  0.31218165159225464
train gradient:  0.2913251183382801
iteration : 3835
train acc:  0.8046875
train loss:  0.4233899712562561
train gradient:  0.3244196128855497
iteration : 3836
train acc:  0.8046875
train loss:  0.4389340281486511
train gradient:  0.4812366028393736
iteration : 3837
train acc:  0.8515625
train loss:  0.3236273527145386
train gradient:  0.3004255704226146
iteration : 3838
train acc:  0.828125
train loss:  0.3661861717700958
train gradient:  0.35185800768101266
iteration : 3839
train acc:  0.8515625
train loss:  0.3405202627182007
train gradient:  0.2753794159973066
iteration : 3840
train acc:  0.8671875
train loss:  0.3120044767856598
train gradient:  0.2547757464896775
iteration : 3841
train acc:  0.84375
train loss:  0.40058082342147827
train gradient:  0.4447152862838168
iteration : 3842
train acc:  0.8203125
train loss:  0.41143178939819336
train gradient:  0.42552150624031426
iteration : 3843
train acc:  0.828125
train loss:  0.3890448808670044
train gradient:  0.33859732498269074
iteration : 3844
train acc:  0.8359375
train loss:  0.3888170123100281
train gradient:  0.3891936231613468
iteration : 3845
train acc:  0.8125
train loss:  0.42296886444091797
train gradient:  0.4872719429848719
iteration : 3846
train acc:  0.828125
train loss:  0.36314594745635986
train gradient:  0.33656506894117283
iteration : 3847
train acc:  0.796875
train loss:  0.42418912053108215
train gradient:  0.4995490327634734
iteration : 3848
train acc:  0.8984375
train loss:  0.32048022747039795
train gradient:  0.23327110327878564
iteration : 3849
train acc:  0.8984375
train loss:  0.32065463066101074
train gradient:  0.29741506864375566
iteration : 3850
train acc:  0.796875
train loss:  0.4581514596939087
train gradient:  0.6071899626894516
iteration : 3851
train acc:  0.8515625
train loss:  0.4666871726512909
train gradient:  0.4806601192619364
iteration : 3852
train acc:  0.859375
train loss:  0.34985291957855225
train gradient:  0.25395956457106084
iteration : 3853
train acc:  0.875
train loss:  0.2779384255409241
train gradient:  0.19006106614501178
iteration : 3854
train acc:  0.796875
train loss:  0.37278470396995544
train gradient:  0.30064295682954745
iteration : 3855
train acc:  0.7890625
train loss:  0.4501633644104004
train gradient:  0.439261742856043
iteration : 3856
train acc:  0.8671875
train loss:  0.35909175872802734
train gradient:  0.2619940616285205
iteration : 3857
train acc:  0.84375
train loss:  0.30255022644996643
train gradient:  0.18542969021297195
iteration : 3858
train acc:  0.8203125
train loss:  0.36966806650161743
train gradient:  0.23555042187603129
iteration : 3859
train acc:  0.890625
train loss:  0.2686021625995636
train gradient:  0.24938558404757688
iteration : 3860
train acc:  0.8828125
train loss:  0.32253092527389526
train gradient:  0.21716910422804092
iteration : 3861
train acc:  0.8125
train loss:  0.3870992958545685
train gradient:  0.4225153072967785
iteration : 3862
train acc:  0.890625
train loss:  0.3163876533508301
train gradient:  0.23905137793922032
iteration : 3863
train acc:  0.7890625
train loss:  0.389204740524292
train gradient:  0.2845759418244092
iteration : 3864
train acc:  0.84375
train loss:  0.37606436014175415
train gradient:  0.38179874615874554
iteration : 3865
train acc:  0.7734375
train loss:  0.4680667817592621
train gradient:  0.5782805390263173
iteration : 3866
train acc:  0.8359375
train loss:  0.3443858027458191
train gradient:  0.24305262460346744
iteration : 3867
train acc:  0.8515625
train loss:  0.34861788153648376
train gradient:  0.2808402681882337
iteration : 3868
train acc:  0.84375
train loss:  0.33142390847206116
train gradient:  0.23897528916515048
iteration : 3869
train acc:  0.8359375
train loss:  0.3223181962966919
train gradient:  0.2136479934686319
iteration : 3870
train acc:  0.734375
train loss:  0.45810678601264954
train gradient:  0.44540505787358886
iteration : 3871
train acc:  0.84375
train loss:  0.3613065481185913
train gradient:  0.2780314417269333
iteration : 3872
train acc:  0.8359375
train loss:  0.3800758421421051
train gradient:  0.2557258085480951
iteration : 3873
train acc:  0.859375
train loss:  0.4066348075866699
train gradient:  0.40194586305619806
iteration : 3874
train acc:  0.796875
train loss:  0.39941978454589844
train gradient:  0.42267038076988916
iteration : 3875
train acc:  0.8046875
train loss:  0.4405134320259094
train gradient:  0.6001830414923823
iteration : 3876
train acc:  0.8203125
train loss:  0.42042413353919983
train gradient:  0.33924136466585325
iteration : 3877
train acc:  0.8125
train loss:  0.40186503529548645
train gradient:  0.4408446158798347
iteration : 3878
train acc:  0.8359375
train loss:  0.36893898248672485
train gradient:  0.3393212892805299
iteration : 3879
train acc:  0.8671875
train loss:  0.3266509771347046
train gradient:  0.17108663336125246
iteration : 3880
train acc:  0.84375
train loss:  0.35899272561073303
train gradient:  0.2108857608470884
iteration : 3881
train acc:  0.859375
train loss:  0.42367005348205566
train gradient:  0.3892422120202646
iteration : 3882
train acc:  0.8984375
train loss:  0.27818095684051514
train gradient:  0.1775978551527258
iteration : 3883
train acc:  0.8125
train loss:  0.4231821596622467
train gradient:  0.30480137025920684
iteration : 3884
train acc:  0.859375
train loss:  0.29031902551651
train gradient:  0.3661145341822221
iteration : 3885
train acc:  0.84375
train loss:  0.34736594557762146
train gradient:  0.30262682768126176
iteration : 3886
train acc:  0.8359375
train loss:  0.3270466923713684
train gradient:  0.24884349306046713
iteration : 3887
train acc:  0.8203125
train loss:  0.35449790954589844
train gradient:  0.2221339929493638
iteration : 3888
train acc:  0.8359375
train loss:  0.36989763379096985
train gradient:  0.2825759471914846
iteration : 3889
train acc:  0.84375
train loss:  0.4139254689216614
train gradient:  0.27774937368322117
iteration : 3890
train acc:  0.8671875
train loss:  0.31110528111457825
train gradient:  0.13154921184428334
iteration : 3891
train acc:  0.8203125
train loss:  0.43499183654785156
train gradient:  0.2971760154239879
iteration : 3892
train acc:  0.8359375
train loss:  0.32625991106033325
train gradient:  0.25426296553672745
iteration : 3893
train acc:  0.84375
train loss:  0.3633105158805847
train gradient:  0.27006239184168995
iteration : 3894
train acc:  0.8828125
train loss:  0.3360147476196289
train gradient:  0.26141370043251466
iteration : 3895
train acc:  0.796875
train loss:  0.43543195724487305
train gradient:  0.4223973220850397
iteration : 3896
train acc:  0.859375
train loss:  0.3353935778141022
train gradient:  0.30840009377489536
iteration : 3897
train acc:  0.8125
train loss:  0.3986594080924988
train gradient:  0.2843901069163953
iteration : 3898
train acc:  0.8203125
train loss:  0.4294681251049042
train gradient:  0.4903922729389231
iteration : 3899
train acc:  0.875
train loss:  0.3283841609954834
train gradient:  0.18129543443627621
iteration : 3900
train acc:  0.8203125
train loss:  0.4334050416946411
train gradient:  0.3797513979049979
iteration : 3901
train acc:  0.890625
train loss:  0.2973858714103699
train gradient:  0.27166991901898707
iteration : 3902
train acc:  0.8515625
train loss:  0.3439701199531555
train gradient:  0.2169038613951743
iteration : 3903
train acc:  0.8359375
train loss:  0.3592628240585327
train gradient:  0.3020050100919841
iteration : 3904
train acc:  0.828125
train loss:  0.32274454832077026
train gradient:  0.22717579990430325
iteration : 3905
train acc:  0.84375
train loss:  0.40013715624809265
train gradient:  0.48772325918458415
iteration : 3906
train acc:  0.7890625
train loss:  0.431424617767334
train gradient:  0.5759959589937808
iteration : 3907
train acc:  0.8203125
train loss:  0.40036362409591675
train gradient:  0.31305784055512315
iteration : 3908
train acc:  0.84375
train loss:  0.39758363366127014
train gradient:  0.3483559979142652
iteration : 3909
train acc:  0.8515625
train loss:  0.33622461557388306
train gradient:  0.2519510198586815
iteration : 3910
train acc:  0.7890625
train loss:  0.42979902029037476
train gradient:  0.3592658684822799
iteration : 3911
train acc:  0.8515625
train loss:  0.3484008312225342
train gradient:  0.21812955793064154
iteration : 3912
train acc:  0.8828125
train loss:  0.30982595682144165
train gradient:  0.2535100969741923
iteration : 3913
train acc:  0.859375
train loss:  0.299665242433548
train gradient:  0.1478313576739399
iteration : 3914
train acc:  0.84375
train loss:  0.35207492113113403
train gradient:  0.23116393716027753
iteration : 3915
train acc:  0.8359375
train loss:  0.42259424924850464
train gradient:  0.2555225045273163
iteration : 3916
train acc:  0.859375
train loss:  0.3335569500923157
train gradient:  0.25173788106794315
iteration : 3917
train acc:  0.8515625
train loss:  0.36249640583992004
train gradient:  0.3675275225143834
iteration : 3918
train acc:  0.84375
train loss:  0.3174412250518799
train gradient:  0.214889857642134
iteration : 3919
train acc:  0.828125
train loss:  0.3252675533294678
train gradient:  0.2042004465163922
iteration : 3920
train acc:  0.859375
train loss:  0.3409944474697113
train gradient:  0.24341416871175653
iteration : 3921
train acc:  0.828125
train loss:  0.396170437335968
train gradient:  0.3293399933762721
iteration : 3922
train acc:  0.84375
train loss:  0.4053497314453125
train gradient:  0.24255625521981608
iteration : 3923
train acc:  0.796875
train loss:  0.3717687427997589
train gradient:  0.28549901015902407
iteration : 3924
train acc:  0.8125
train loss:  0.40331727266311646
train gradient:  0.38650470865968983
iteration : 3925
train acc:  0.8203125
train loss:  0.3961750268936157
train gradient:  0.31045176741644176
iteration : 3926
train acc:  0.8671875
train loss:  0.32279685139656067
train gradient:  0.22871331200501968
iteration : 3927
train acc:  0.84375
train loss:  0.33921951055526733
train gradient:  0.20316877729634464
iteration : 3928
train acc:  0.8203125
train loss:  0.4007544219493866
train gradient:  0.3596005314336082
iteration : 3929
train acc:  0.828125
train loss:  0.33531975746154785
train gradient:  0.21424390159536946
iteration : 3930
train acc:  0.8125
train loss:  0.4249401092529297
train gradient:  0.3237405775508589
iteration : 3931
train acc:  0.828125
train loss:  0.3321581482887268
train gradient:  0.24840811007806685
iteration : 3932
train acc:  0.828125
train loss:  0.35654324293136597
train gradient:  0.2796717167173322
iteration : 3933
train acc:  0.8203125
train loss:  0.37708210945129395
train gradient:  0.39647567781170656
iteration : 3934
train acc:  0.828125
train loss:  0.3168516755104065
train gradient:  0.2755972399883969
iteration : 3935
train acc:  0.8359375
train loss:  0.34400641918182373
train gradient:  0.26039175248605567
iteration : 3936
train acc:  0.8046875
train loss:  0.37632542848587036
train gradient:  0.251862751814271
iteration : 3937
train acc:  0.890625
train loss:  0.31798914074897766
train gradient:  0.2264632684980624
iteration : 3938
train acc:  0.890625
train loss:  0.25313621759414673
train gradient:  0.185400120228732
iteration : 3939
train acc:  0.859375
train loss:  0.34474167227745056
train gradient:  0.26251055351020236
iteration : 3940
train acc:  0.84375
train loss:  0.31979626417160034
train gradient:  0.23756928483484968
iteration : 3941
train acc:  0.8984375
train loss:  0.3415185213088989
train gradient:  0.21785731327249455
iteration : 3942
train acc:  0.8671875
train loss:  0.33085671067237854
train gradient:  0.2473144566870412
iteration : 3943
train acc:  0.828125
train loss:  0.37601447105407715
train gradient:  0.2307174451516068
iteration : 3944
train acc:  0.7890625
train loss:  0.4380536377429962
train gradient:  0.5578347209767254
iteration : 3945
train acc:  0.7890625
train loss:  0.48873552680015564
train gradient:  0.483307921088669
iteration : 3946
train acc:  0.8125
train loss:  0.36356377601623535
train gradient:  0.24705542462777041
iteration : 3947
train acc:  0.8515625
train loss:  0.32731348276138306
train gradient:  0.20181039341615642
iteration : 3948
train acc:  0.890625
train loss:  0.31474876403808594
train gradient:  0.24022190039668845
iteration : 3949
train acc:  0.859375
train loss:  0.3222951292991638
train gradient:  0.23610981778742485
iteration : 3950
train acc:  0.828125
train loss:  0.36334285140037537
train gradient:  0.27980992941120997
iteration : 3951
train acc:  0.8671875
train loss:  0.31509193778038025
train gradient:  0.19151374676997604
iteration : 3952
train acc:  0.8046875
train loss:  0.41689640283584595
train gradient:  0.3362327387350078
iteration : 3953
train acc:  0.8125
train loss:  0.413200318813324
train gradient:  0.3946228908123636
iteration : 3954
train acc:  0.8359375
train loss:  0.3667304813861847
train gradient:  0.22691638294676314
iteration : 3955
train acc:  0.7890625
train loss:  0.4628788232803345
train gradient:  0.3889496881995749
iteration : 3956
train acc:  0.8359375
train loss:  0.37561190128326416
train gradient:  0.3136789711542633
iteration : 3957
train acc:  0.796875
train loss:  0.4012971520423889
train gradient:  0.3144610771381468
iteration : 3958
train acc:  0.8046875
train loss:  0.41357022523880005
train gradient:  0.3584275022142108
iteration : 3959
train acc:  0.8359375
train loss:  0.3734349012374878
train gradient:  0.2690529493713187
iteration : 3960
train acc:  0.828125
train loss:  0.4146875739097595
train gradient:  0.3433861306959532
iteration : 3961
train acc:  0.8046875
train loss:  0.45594272017478943
train gradient:  0.3881631735485366
iteration : 3962
train acc:  0.8359375
train loss:  0.38372719287872314
train gradient:  0.38831292925543953
iteration : 3963
train acc:  0.828125
train loss:  0.40417343378067017
train gradient:  0.25257832262606184
iteration : 3964
train acc:  0.8515625
train loss:  0.34835052490234375
train gradient:  0.20085054263197782
iteration : 3965
train acc:  0.8046875
train loss:  0.3747471272945404
train gradient:  0.2836499286719042
iteration : 3966
train acc:  0.8046875
train loss:  0.39470577239990234
train gradient:  0.32681388100682557
iteration : 3967
train acc:  0.8671875
train loss:  0.2894341051578522
train gradient:  0.24776438182674446
iteration : 3968
train acc:  0.828125
train loss:  0.4420451521873474
train gradient:  0.25836518285597565
iteration : 3969
train acc:  0.828125
train loss:  0.3577364981174469
train gradient:  0.2703400240931638
iteration : 3970
train acc:  0.84375
train loss:  0.33917707204818726
train gradient:  0.24986780669704434
iteration : 3971
train acc:  0.8984375
train loss:  0.29365718364715576
train gradient:  0.2465283717796107
iteration : 3972
train acc:  0.84375
train loss:  0.33734995126724243
train gradient:  0.2330795652766684
iteration : 3973
train acc:  0.875
train loss:  0.2911607623100281
train gradient:  0.177033284816002
iteration : 3974
train acc:  0.8515625
train loss:  0.36291471123695374
train gradient:  0.2853066599614341
iteration : 3975
train acc:  0.8515625
train loss:  0.32898133993148804
train gradient:  0.3153218865665557
iteration : 3976
train acc:  0.859375
train loss:  0.3397897481918335
train gradient:  0.19836421257772013
iteration : 3977
train acc:  0.84375
train loss:  0.36364126205444336
train gradient:  0.3177931370388874
iteration : 3978
train acc:  0.8671875
train loss:  0.3067294955253601
train gradient:  0.24568404676931788
iteration : 3979
train acc:  0.8515625
train loss:  0.37859421968460083
train gradient:  0.20654059908816463
iteration : 3980
train acc:  0.8203125
train loss:  0.40565726161003113
train gradient:  0.42449416205452406
iteration : 3981
train acc:  0.859375
train loss:  0.366493284702301
train gradient:  0.3071854411664405
iteration : 3982
train acc:  0.8984375
train loss:  0.3356556296348572
train gradient:  0.2866436920087498
iteration : 3983
train acc:  0.84375
train loss:  0.3744354248046875
train gradient:  0.3170737486665539
iteration : 3984
train acc:  0.8046875
train loss:  0.3450586199760437
train gradient:  0.18786467892964048
iteration : 3985
train acc:  0.8515625
train loss:  0.3333394229412079
train gradient:  0.24478511608657952
iteration : 3986
train acc:  0.859375
train loss:  0.30795547366142273
train gradient:  0.18556193676549132
iteration : 3987
train acc:  0.78125
train loss:  0.41107088327407837
train gradient:  0.34345380898831807
iteration : 3988
train acc:  0.890625
train loss:  0.32171571254730225
train gradient:  0.23723318332894994
iteration : 3989
train acc:  0.8671875
train loss:  0.31932130455970764
train gradient:  0.2762045918959583
iteration : 3990
train acc:  0.890625
train loss:  0.3016984760761261
train gradient:  0.3872400665512971
iteration : 3991
train acc:  0.8359375
train loss:  0.35264718532562256
train gradient:  0.26520831050682103
iteration : 3992
train acc:  0.8359375
train loss:  0.3597443103790283
train gradient:  0.29573990892744234
iteration : 3993
train acc:  0.828125
train loss:  0.3642541468143463
train gradient:  0.43719990586001667
iteration : 3994
train acc:  0.8046875
train loss:  0.38404518365859985
train gradient:  0.5405314081943582
iteration : 3995
train acc:  0.8046875
train loss:  0.4239180088043213
train gradient:  0.3613931653934001
iteration : 3996
train acc:  0.8203125
train loss:  0.35568398237228394
train gradient:  0.2429274048944403
iteration : 3997
train acc:  0.8515625
train loss:  0.33895623683929443
train gradient:  0.370213043010655
iteration : 3998
train acc:  0.8125
train loss:  0.41578996181488037
train gradient:  0.4594273182069027
iteration : 3999
train acc:  0.828125
train loss:  0.37587952613830566
train gradient:  0.21780734697451232
iteration : 4000
train acc:  0.796875
train loss:  0.3988792300224304
train gradient:  0.2873773350049102
iteration : 4001
train acc:  0.8359375
train loss:  0.419524222612381
train gradient:  0.3178780164472884
iteration : 4002
train acc:  0.8515625
train loss:  0.3383212089538574
train gradient:  0.29276144576552315
iteration : 4003
train acc:  0.7890625
train loss:  0.46854516863822937
train gradient:  0.44140963782090886
iteration : 4004
train acc:  0.875
train loss:  0.3079276978969574
train gradient:  0.15800961235208036
iteration : 4005
train acc:  0.8515625
train loss:  0.34223952889442444
train gradient:  0.23297402696227332
iteration : 4006
train acc:  0.8515625
train loss:  0.354573518037796
train gradient:  0.3133933309515816
iteration : 4007
train acc:  0.875
train loss:  0.3494186997413635
train gradient:  0.25785222618860054
iteration : 4008
train acc:  0.875
train loss:  0.3261971175670624
train gradient:  0.2126922115057888
iteration : 4009
train acc:  0.875
train loss:  0.3252115845680237
train gradient:  0.2496999988036297
iteration : 4010
train acc:  0.859375
train loss:  0.3660086393356323
train gradient:  0.34499279406083716
iteration : 4011
train acc:  0.8671875
train loss:  0.3707343339920044
train gradient:  0.32096276109930993
iteration : 4012
train acc:  0.8359375
train loss:  0.33915823698043823
train gradient:  0.23694127911863522
iteration : 4013
train acc:  0.84375
train loss:  0.32946473360061646
train gradient:  0.23987182167140436
iteration : 4014
train acc:  0.875
train loss:  0.2877218723297119
train gradient:  0.18589920975254087
iteration : 4015
train acc:  0.90625
train loss:  0.2796362042427063
train gradient:  0.16243538061620333
iteration : 4016
train acc:  0.859375
train loss:  0.38891422748565674
train gradient:  0.28251052008397093
iteration : 4017
train acc:  0.796875
train loss:  0.4104476273059845
train gradient:  0.39347087989404816
iteration : 4018
train acc:  0.8359375
train loss:  0.36872950196266174
train gradient:  0.25921696201903033
iteration : 4019
train acc:  0.921875
train loss:  0.2582598328590393
train gradient:  0.213540064910122
iteration : 4020
train acc:  0.890625
train loss:  0.3094407916069031
train gradient:  0.2654062473476419
iteration : 4021
train acc:  0.8203125
train loss:  0.396175742149353
train gradient:  0.36039104102871533
iteration : 4022
train acc:  0.828125
train loss:  0.3698742389678955
train gradient:  0.35849205676170415
iteration : 4023
train acc:  0.828125
train loss:  0.43091535568237305
train gradient:  0.2724436926338626
iteration : 4024
train acc:  0.828125
train loss:  0.34931331872940063
train gradient:  0.2364263414402149
iteration : 4025
train acc:  0.7890625
train loss:  0.45566385984420776
train gradient:  0.337604902408666
iteration : 4026
train acc:  0.828125
train loss:  0.3791859447956085
train gradient:  0.26474617089508873
iteration : 4027
train acc:  0.8359375
train loss:  0.3930402398109436
train gradient:  0.29039173361339826
iteration : 4028
train acc:  0.8203125
train loss:  0.355355441570282
train gradient:  0.23916853827177098
iteration : 4029
train acc:  0.8671875
train loss:  0.27989357709884644
train gradient:  0.1602651665844451
iteration : 4030
train acc:  0.8125
train loss:  0.4207451343536377
train gradient:  0.45993455743273803
iteration : 4031
train acc:  0.8828125
train loss:  0.30820387601852417
train gradient:  0.25229442054954637
iteration : 4032
train acc:  0.84375
train loss:  0.3523445129394531
train gradient:  0.322950007299008
iteration : 4033
train acc:  0.8125
train loss:  0.4074329137802124
train gradient:  0.3303066723736417
iteration : 4034
train acc:  0.7890625
train loss:  0.45450741052627563
train gradient:  0.34913248283944326
iteration : 4035
train acc:  0.875
train loss:  0.3367401957511902
train gradient:  0.20376398190521117
iteration : 4036
train acc:  0.8359375
train loss:  0.3758450746536255
train gradient:  0.32742440206640777
iteration : 4037
train acc:  0.8125
train loss:  0.3887293338775635
train gradient:  0.22323172647717207
iteration : 4038
train acc:  0.859375
train loss:  0.30809521675109863
train gradient:  0.19461442798189743
iteration : 4039
train acc:  0.8515625
train loss:  0.35488834977149963
train gradient:  0.3307797743046131
iteration : 4040
train acc:  0.84375
train loss:  0.331784725189209
train gradient:  0.21543322041620855
iteration : 4041
train acc:  0.84375
train loss:  0.35881054401397705
train gradient:  0.22070396781855156
iteration : 4042
train acc:  0.859375
train loss:  0.3273979127407074
train gradient:  0.25648918229545226
iteration : 4043
train acc:  0.8203125
train loss:  0.3638627827167511
train gradient:  0.3268852448230477
iteration : 4044
train acc:  0.75
train loss:  0.48954707384109497
train gradient:  0.45051999756786293
iteration : 4045
train acc:  0.8359375
train loss:  0.31988251209259033
train gradient:  0.19972877717011048
iteration : 4046
train acc:  0.8203125
train loss:  0.36525753140449524
train gradient:  0.28057442486783263
iteration : 4047
train acc:  0.8203125
train loss:  0.3776914179325104
train gradient:  0.24953951034008867
iteration : 4048
train acc:  0.8515625
train loss:  0.2882806062698364
train gradient:  0.24968703092851222
iteration : 4049
train acc:  0.8515625
train loss:  0.2855697274208069
train gradient:  0.1492871078228361
iteration : 4050
train acc:  0.921875
train loss:  0.2597123384475708
train gradient:  0.22557777344915814
iteration : 4051
train acc:  0.8359375
train loss:  0.4181556701660156
train gradient:  0.29592199253843643
iteration : 4052
train acc:  0.796875
train loss:  0.4265381097793579
train gradient:  0.39308158433807133
iteration : 4053
train acc:  0.7578125
train loss:  0.48047125339508057
train gradient:  0.4253831307678626
iteration : 4054
train acc:  0.8046875
train loss:  0.4141761064529419
train gradient:  0.4821239136795132
iteration : 4055
train acc:  0.859375
train loss:  0.35206183791160583
train gradient:  0.29133041164392054
iteration : 4056
train acc:  0.8046875
train loss:  0.3636215329170227
train gradient:  0.2790236113063228
iteration : 4057
train acc:  0.8515625
train loss:  0.33744627237319946
train gradient:  0.30926759598961584
iteration : 4058
train acc:  0.8203125
train loss:  0.38917386531829834
train gradient:  0.29950915668787215
iteration : 4059
train acc:  0.8046875
train loss:  0.39429837465286255
train gradient:  0.29507528883391354
iteration : 4060
train acc:  0.8515625
train loss:  0.3407191038131714
train gradient:  0.3386873685269654
iteration : 4061
train acc:  0.828125
train loss:  0.3881163001060486
train gradient:  0.412607735129487
iteration : 4062
train acc:  0.859375
train loss:  0.3124517798423767
train gradient:  0.24009690218012442
iteration : 4063
train acc:  0.84375
train loss:  0.29922860860824585
train gradient:  0.30633492747853613
iteration : 4064
train acc:  0.7890625
train loss:  0.39259815216064453
train gradient:  0.333957460231196
iteration : 4065
train acc:  0.828125
train loss:  0.3393721878528595
train gradient:  0.5678787343135092
iteration : 4066
train acc:  0.8671875
train loss:  0.34715208411216736
train gradient:  0.2299708127331696
iteration : 4067
train acc:  0.8125
train loss:  0.44797876477241516
train gradient:  0.3546010369049046
iteration : 4068
train acc:  0.7890625
train loss:  0.412947416305542
train gradient:  0.2744337984709466
iteration : 4069
train acc:  0.8046875
train loss:  0.3899402320384979
train gradient:  0.32261049499156375
iteration : 4070
train acc:  0.8125
train loss:  0.42915797233581543
train gradient:  0.37719702922955156
iteration : 4071
train acc:  0.8984375
train loss:  0.30479907989501953
train gradient:  0.2277049551193305
iteration : 4072
train acc:  0.8203125
train loss:  0.37520766258239746
train gradient:  0.31568444863360406
iteration : 4073
train acc:  0.890625
train loss:  0.3113325238227844
train gradient:  0.202406476745521
iteration : 4074
train acc:  0.90625
train loss:  0.34983402490615845
train gradient:  0.26702385651284133
iteration : 4075
train acc:  0.8515625
train loss:  0.3091910779476166
train gradient:  0.2619130006555411
iteration : 4076
train acc:  0.84375
train loss:  0.365570068359375
train gradient:  0.21316937522790153
iteration : 4077
train acc:  0.828125
train loss:  0.3738325238227844
train gradient:  0.3366550959758161
iteration : 4078
train acc:  0.8828125
train loss:  0.29175394773483276
train gradient:  0.16648267602881497
iteration : 4079
train acc:  0.859375
train loss:  0.3069344758987427
train gradient:  0.18779799570191383
iteration : 4080
train acc:  0.8359375
train loss:  0.378107488155365
train gradient:  0.44562317667227863
iteration : 4081
train acc:  0.859375
train loss:  0.33996522426605225
train gradient:  0.2254104872808324
iteration : 4082
train acc:  0.8828125
train loss:  0.32825028896331787
train gradient:  0.26120106944799376
iteration : 4083
train acc:  0.8671875
train loss:  0.3235056400299072
train gradient:  0.20364723905762822
iteration : 4084
train acc:  0.8046875
train loss:  0.3788180351257324
train gradient:  0.3957082019553407
iteration : 4085
train acc:  0.8984375
train loss:  0.28474611043930054
train gradient:  0.18523613877302672
iteration : 4086
train acc:  0.8359375
train loss:  0.33050623536109924
train gradient:  0.2265804051452936
iteration : 4087
train acc:  0.8671875
train loss:  0.34679803252220154
train gradient:  0.23525455471154275
iteration : 4088
train acc:  0.8671875
train loss:  0.3380562365055084
train gradient:  0.3022883187839771
iteration : 4089
train acc:  0.7890625
train loss:  0.42167162895202637
train gradient:  0.416683497143876
iteration : 4090
train acc:  0.8359375
train loss:  0.3004242181777954
train gradient:  0.24928814940671207
iteration : 4091
train acc:  0.8671875
train loss:  0.3331548273563385
train gradient:  0.24713263609970887
iteration : 4092
train acc:  0.859375
train loss:  0.3618834912776947
train gradient:  0.27657990600659504
iteration : 4093
train acc:  0.890625
train loss:  0.25783154368400574
train gradient:  0.2552061351469782
iteration : 4094
train acc:  0.828125
train loss:  0.38249072432518005
train gradient:  0.23993490105777765
iteration : 4095
train acc:  0.84375
train loss:  0.38553205132484436
train gradient:  0.33516244882424295
iteration : 4096
train acc:  0.84375
train loss:  0.3664802014827728
train gradient:  0.3515772782238568
iteration : 4097
train acc:  0.7890625
train loss:  0.42463263869285583
train gradient:  0.37509052518380326
iteration : 4098
train acc:  0.8515625
train loss:  0.3518696427345276
train gradient:  0.4388667353586922
iteration : 4099
train acc:  0.8203125
train loss:  0.36197128891944885
train gradient:  0.3443828377074014
iteration : 4100
train acc:  0.890625
train loss:  0.27592989802360535
train gradient:  0.1915532499252875
iteration : 4101
train acc:  0.8125
train loss:  0.3973442614078522
train gradient:  0.31859304937051575
iteration : 4102
train acc:  0.84375
train loss:  0.3836228549480438
train gradient:  0.2882360746288399
iteration : 4103
train acc:  0.859375
train loss:  0.3917345404624939
train gradient:  0.34295228931661936
iteration : 4104
train acc:  0.8125
train loss:  0.40264296531677246
train gradient:  0.29815428534136346
iteration : 4105
train acc:  0.7890625
train loss:  0.46212834119796753
train gradient:  0.37921554256083234
iteration : 4106
train acc:  0.78125
train loss:  0.40535464882850647
train gradient:  0.3222333763724353
iteration : 4107
train acc:  0.8125
train loss:  0.36983001232147217
train gradient:  0.26934691282097817
iteration : 4108
train acc:  0.8515625
train loss:  0.35317832231521606
train gradient:  0.2389561293614224
iteration : 4109
train acc:  0.796875
train loss:  0.4364241361618042
train gradient:  0.4508826836351617
iteration : 4110
train acc:  0.84375
train loss:  0.38386771082878113
train gradient:  0.462734394725927
iteration : 4111
train acc:  0.8125
train loss:  0.41594284772872925
train gradient:  0.3485951692508657
iteration : 4112
train acc:  0.8359375
train loss:  0.38070160150527954
train gradient:  0.31888743773071243
iteration : 4113
train acc:  0.8515625
train loss:  0.35039302706718445
train gradient:  0.31848330025751753
iteration : 4114
train acc:  0.859375
train loss:  0.31572505831718445
train gradient:  0.2493224912208459
iteration : 4115
train acc:  0.8828125
train loss:  0.2742003798484802
train gradient:  0.19786090500237002
iteration : 4116
train acc:  0.8515625
train loss:  0.3495252728462219
train gradient:  0.3718268591578055
iteration : 4117
train acc:  0.8671875
train loss:  0.35358062386512756
train gradient:  0.239165370142487
iteration : 4118
train acc:  0.84375
train loss:  0.37690621614456177
train gradient:  0.3183630678579232
iteration : 4119
train acc:  0.84375
train loss:  0.3378894031047821
train gradient:  0.31726202755932026
iteration : 4120
train acc:  0.8046875
train loss:  0.43234318494796753
train gradient:  0.44752737326504916
iteration : 4121
train acc:  0.859375
train loss:  0.3219068646430969
train gradient:  0.2712048952439672
iteration : 4122
train acc:  0.8203125
train loss:  0.4045817255973816
train gradient:  0.3419176224778988
iteration : 4123
train acc:  0.8828125
train loss:  0.29979097843170166
train gradient:  0.21315859434621734
iteration : 4124
train acc:  0.8125
train loss:  0.4056471884250641
train gradient:  0.4253846014453059
iteration : 4125
train acc:  0.8046875
train loss:  0.36024320125579834
train gradient:  0.4288939695330609
iteration : 4126
train acc:  0.8515625
train loss:  0.3490971326828003
train gradient:  0.24783764939343766
iteration : 4127
train acc:  0.7890625
train loss:  0.3895423710346222
train gradient:  0.42585392131902355
iteration : 4128
train acc:  0.8046875
train loss:  0.382651150226593
train gradient:  0.2893918898386178
iteration : 4129
train acc:  0.8671875
train loss:  0.29376929998397827
train gradient:  0.2152489324382129
iteration : 4130
train acc:  0.8515625
train loss:  0.3763082027435303
train gradient:  0.24935830493568595
iteration : 4131
train acc:  0.8125
train loss:  0.49415916204452515
train gradient:  0.4116195364800107
iteration : 4132
train acc:  0.8203125
train loss:  0.3370988667011261
train gradient:  0.24322860170240873
iteration : 4133
train acc:  0.7890625
train loss:  0.4356527626514435
train gradient:  0.4080826899359492
iteration : 4134
train acc:  0.8203125
train loss:  0.44583672285079956
train gradient:  0.6120774884738195
iteration : 4135
train acc:  0.875
train loss:  0.3253202438354492
train gradient:  0.22476944211066063
iteration : 4136
train acc:  0.8671875
train loss:  0.33174169063568115
train gradient:  0.31142148842567086
iteration : 4137
train acc:  0.859375
train loss:  0.3269430994987488
train gradient:  0.2559196116280168
iteration : 4138
train acc:  0.859375
train loss:  0.3213960528373718
train gradient:  0.20033497476590273
iteration : 4139
train acc:  0.828125
train loss:  0.36686640977859497
train gradient:  0.2807988025024801
iteration : 4140
train acc:  0.8984375
train loss:  0.31220346689224243
train gradient:  0.27888381833878184
iteration : 4141
train acc:  0.7734375
train loss:  0.44550591707229614
train gradient:  0.3809994620195948
iteration : 4142
train acc:  0.8515625
train loss:  0.3459257483482361
train gradient:  0.27924854800148036
iteration : 4143
train acc:  0.84375
train loss:  0.4113759696483612
train gradient:  0.34772997462408345
iteration : 4144
train acc:  0.8515625
train loss:  0.35185450315475464
train gradient:  0.3233709459560678
iteration : 4145
train acc:  0.8359375
train loss:  0.4749550223350525
train gradient:  0.43084960911254094
iteration : 4146
train acc:  0.84375
train loss:  0.3477987051010132
train gradient:  0.21647197263822904
iteration : 4147
train acc:  0.8515625
train loss:  0.3365967869758606
train gradient:  0.3503968020479244
iteration : 4148
train acc:  0.8046875
train loss:  0.3898211717605591
train gradient:  0.27347854289107243
iteration : 4149
train acc:  0.796875
train loss:  0.3947027921676636
train gradient:  0.44137447201475927
iteration : 4150
train acc:  0.890625
train loss:  0.28643590211868286
train gradient:  0.21837644909493095
iteration : 4151
train acc:  0.8671875
train loss:  0.35805651545524597
train gradient:  0.2729802408033427
iteration : 4152
train acc:  0.8046875
train loss:  0.3938853144645691
train gradient:  0.278605905724055
iteration : 4153
train acc:  0.890625
train loss:  0.3014677166938782
train gradient:  0.2013564797667926
iteration : 4154
train acc:  0.84375
train loss:  0.3546956181526184
train gradient:  0.21613242840409622
iteration : 4155
train acc:  0.828125
train loss:  0.3617287874221802
train gradient:  0.20712109879669707
iteration : 4156
train acc:  0.8515625
train loss:  0.3401523232460022
train gradient:  0.29034964435824
iteration : 4157
train acc:  0.8515625
train loss:  0.3214573264122009
train gradient:  0.29350597574576565
iteration : 4158
train acc:  0.8359375
train loss:  0.3521909713745117
train gradient:  0.32582759699834957
iteration : 4159
train acc:  0.8203125
train loss:  0.4671061635017395
train gradient:  0.42309370321510914
iteration : 4160
train acc:  0.828125
train loss:  0.37285521626472473
train gradient:  0.25557657851005966
iteration : 4161
train acc:  0.8125
train loss:  0.40167972445487976
train gradient:  0.2982789146754656
iteration : 4162
train acc:  0.859375
train loss:  0.33246687054634094
train gradient:  0.23620689559120472
iteration : 4163
train acc:  0.8203125
train loss:  0.3772810697555542
train gradient:  0.3395576082925854
iteration : 4164
train acc:  0.828125
train loss:  0.34630098938941956
train gradient:  0.2829329775531076
iteration : 4165
train acc:  0.8125
train loss:  0.3923521339893341
train gradient:  0.2514415163600614
iteration : 4166
train acc:  0.78125
train loss:  0.4581989645957947
train gradient:  0.46050851559252814
iteration : 4167
train acc:  0.8125
train loss:  0.3448196351528168
train gradient:  0.20987952591405534
iteration : 4168
train acc:  0.8046875
train loss:  0.43179231882095337
train gradient:  0.3132180842018573
iteration : 4169
train acc:  0.8125
train loss:  0.48372530937194824
train gradient:  0.5450590655581941
iteration : 4170
train acc:  0.8125
train loss:  0.3777605891227722
train gradient:  0.21850967674464591
iteration : 4171
train acc:  0.828125
train loss:  0.36448752880096436
train gradient:  0.26733665197217454
iteration : 4172
train acc:  0.859375
train loss:  0.36619147658348083
train gradient:  0.3677443459327493
iteration : 4173
train acc:  0.8046875
train loss:  0.39776474237442017
train gradient:  0.28173473859448117
iteration : 4174
train acc:  0.828125
train loss:  0.39086979627609253
train gradient:  0.2948021566297819
iteration : 4175
train acc:  0.8828125
train loss:  0.2626456618309021
train gradient:  0.21235210843660632
iteration : 4176
train acc:  0.78125
train loss:  0.43446484208106995
train gradient:  0.3004489119149186
iteration : 4177
train acc:  0.84375
train loss:  0.3357623815536499
train gradient:  0.22454332921226436
iteration : 4178
train acc:  0.8828125
train loss:  0.2776746451854706
train gradient:  0.1679822789477542
iteration : 4179
train acc:  0.8359375
train loss:  0.42748963832855225
train gradient:  0.34156441524000003
iteration : 4180
train acc:  0.84375
train loss:  0.38554251194000244
train gradient:  0.28695547875139316
iteration : 4181
train acc:  0.8203125
train loss:  0.4106420874595642
train gradient:  0.2897192955130481
iteration : 4182
train acc:  0.8203125
train loss:  0.34620705246925354
train gradient:  0.1953769503280651
iteration : 4183
train acc:  0.796875
train loss:  0.44964599609375
train gradient:  0.26126081381055105
iteration : 4184
train acc:  0.828125
train loss:  0.3828034996986389
train gradient:  0.4376360317470208
iteration : 4185
train acc:  0.890625
train loss:  0.3355313539505005
train gradient:  0.21672766681887146
iteration : 4186
train acc:  0.8515625
train loss:  0.34986552596092224
train gradient:  0.2534936957158546
iteration : 4187
train acc:  0.78125
train loss:  0.4441078305244446
train gradient:  0.3144671908038407
iteration : 4188
train acc:  0.8828125
train loss:  0.38605421781539917
train gradient:  0.3204049722537946
iteration : 4189
train acc:  0.796875
train loss:  0.4506797790527344
train gradient:  0.28167772007755004
iteration : 4190
train acc:  0.8515625
train loss:  0.3271765410900116
train gradient:  0.255367285394281
iteration : 4191
train acc:  0.8359375
train loss:  0.4086136519908905
train gradient:  0.24239362414462734
iteration : 4192
train acc:  0.8125
train loss:  0.45850956439971924
train gradient:  0.3344617242629993
iteration : 4193
train acc:  0.84375
train loss:  0.38252660632133484
train gradient:  0.37046853065162216
iteration : 4194
train acc:  0.84375
train loss:  0.37520933151245117
train gradient:  0.2653329886409443
iteration : 4195
train acc:  0.8671875
train loss:  0.36940065026283264
train gradient:  0.21914770371293904
iteration : 4196
train acc:  0.8046875
train loss:  0.44765806198120117
train gradient:  0.3620524675659705
iteration : 4197
train acc:  0.875
train loss:  0.30503442883491516
train gradient:  0.17024317501327813
iteration : 4198
train acc:  0.8125
train loss:  0.36634063720703125
train gradient:  0.28084196450570015
iteration : 4199
train acc:  0.8125
train loss:  0.40007513761520386
train gradient:  0.2662023745945877
iteration : 4200
train acc:  0.875
train loss:  0.3127055764198303
train gradient:  0.19155175839149097
iteration : 4201
train acc:  0.8203125
train loss:  0.38912931084632874
train gradient:  0.5238128579115917
iteration : 4202
train acc:  0.8046875
train loss:  0.37003153562545776
train gradient:  0.3061398168929889
iteration : 4203
train acc:  0.8515625
train loss:  0.3453945219516754
train gradient:  0.2745831971903976
iteration : 4204
train acc:  0.8125
train loss:  0.38663437962532043
train gradient:  0.3751167808452117
iteration : 4205
train acc:  0.90625
train loss:  0.30478909611701965
train gradient:  0.2797682661211396
iteration : 4206
train acc:  0.921875
train loss:  0.21828927099704742
train gradient:  0.135642378386729
iteration : 4207
train acc:  0.890625
train loss:  0.30854082107543945
train gradient:  0.2607036441959964
iteration : 4208
train acc:  0.875
train loss:  0.3184189200401306
train gradient:  0.21731990005708354
iteration : 4209
train acc:  0.84375
train loss:  0.3649221956729889
train gradient:  0.2467575943456865
iteration : 4210
train acc:  0.78125
train loss:  0.43426987528800964
train gradient:  0.3832026651554602
iteration : 4211
train acc:  0.84375
train loss:  0.3663960099220276
train gradient:  0.18922813362007296
iteration : 4212
train acc:  0.890625
train loss:  0.3048548102378845
train gradient:  0.17133030263467716
iteration : 4213
train acc:  0.8671875
train loss:  0.355256587266922
train gradient:  0.26910727717377464
iteration : 4214
train acc:  0.75
train loss:  0.4694732129573822
train gradient:  0.4819322391084153
iteration : 4215
train acc:  0.828125
train loss:  0.3629012107849121
train gradient:  0.30510306216863
iteration : 4216
train acc:  0.8515625
train loss:  0.3495556116104126
train gradient:  0.2761430513812064
iteration : 4217
train acc:  0.8671875
train loss:  0.3506452441215515
train gradient:  0.3618066414676269
iteration : 4218
train acc:  0.875
train loss:  0.3513082265853882
train gradient:  0.19577624491773107
iteration : 4219
train acc:  0.8203125
train loss:  0.4085826277732849
train gradient:  0.3850873379639102
iteration : 4220
train acc:  0.8515625
train loss:  0.3672536611557007
train gradient:  0.32415411866477584
iteration : 4221
train acc:  0.8828125
train loss:  0.3270145058631897
train gradient:  0.31189762313989283
iteration : 4222
train acc:  0.8359375
train loss:  0.3917965292930603
train gradient:  0.2808316120992799
iteration : 4223
train acc:  0.796875
train loss:  0.44790297746658325
train gradient:  0.33409265012169087
iteration : 4224
train acc:  0.859375
train loss:  0.28883394598960876
train gradient:  0.23466829889775231
iteration : 4225
train acc:  0.84375
train loss:  0.3214181363582611
train gradient:  0.22855841401821128
iteration : 4226
train acc:  0.8984375
train loss:  0.2775561809539795
train gradient:  0.1871206839898244
iteration : 4227
train acc:  0.8359375
train loss:  0.34631437063217163
train gradient:  0.21091417418276218
iteration : 4228
train acc:  0.8671875
train loss:  0.3278821110725403
train gradient:  0.23183005648534943
iteration : 4229
train acc:  0.859375
train loss:  0.28306853771209717
train gradient:  0.2280210091096836
iteration : 4230
train acc:  0.796875
train loss:  0.44580739736557007
train gradient:  0.3639220347845848
iteration : 4231
train acc:  0.8515625
train loss:  0.35211482644081116
train gradient:  0.31799590792182636
iteration : 4232
train acc:  0.8671875
train loss:  0.3065183162689209
train gradient:  0.24021536238756902
iteration : 4233
train acc:  0.875
train loss:  0.2866861820220947
train gradient:  0.22566293138841748
iteration : 4234
train acc:  0.875
train loss:  0.4269186556339264
train gradient:  0.4255023975827645
iteration : 4235
train acc:  0.8359375
train loss:  0.34172141551971436
train gradient:  0.2322002364614337
iteration : 4236
train acc:  0.859375
train loss:  0.3126988112926483
train gradient:  0.20588087297689836
iteration : 4237
train acc:  0.8203125
train loss:  0.4152778089046478
train gradient:  0.32118465950243996
iteration : 4238
train acc:  0.8515625
train loss:  0.36064213514328003
train gradient:  0.2540118893503143
iteration : 4239
train acc:  0.8828125
train loss:  0.3058924674987793
train gradient:  0.23837692919289666
iteration : 4240
train acc:  0.8203125
train loss:  0.4311314821243286
train gradient:  0.446821284103969
iteration : 4241
train acc:  0.8515625
train loss:  0.36314404010772705
train gradient:  0.27692249492215243
iteration : 4242
train acc:  0.859375
train loss:  0.41098538041114807
train gradient:  0.332553803648594
iteration : 4243
train acc:  0.796875
train loss:  0.41894859075546265
train gradient:  0.30759583320347045
iteration : 4244
train acc:  0.84375
train loss:  0.2786791920661926
train gradient:  0.2121875800473922
iteration : 4245
train acc:  0.8359375
train loss:  0.3269519805908203
train gradient:  0.24418876148256824
iteration : 4246
train acc:  0.859375
train loss:  0.3344346880912781
train gradient:  0.21155191524333278
iteration : 4247
train acc:  0.828125
train loss:  0.329941987991333
train gradient:  0.28174495015590895
iteration : 4248
train acc:  0.8125
train loss:  0.40997326374053955
train gradient:  0.32454787962359793
iteration : 4249
train acc:  0.8671875
train loss:  0.34031349420547485
train gradient:  0.1832729589524577
iteration : 4250
train acc:  0.78125
train loss:  0.4458261728286743
train gradient:  0.44849266417515016
iteration : 4251
train acc:  0.8828125
train loss:  0.2821793258190155
train gradient:  0.21761602165118388
iteration : 4252
train acc:  0.7734375
train loss:  0.4354557991027832
train gradient:  0.4278398849985704
iteration : 4253
train acc:  0.84375
train loss:  0.32810938358306885
train gradient:  0.22911084337713092
iteration : 4254
train acc:  0.8671875
train loss:  0.3396466374397278
train gradient:  0.19354595454968154
iteration : 4255
train acc:  0.8671875
train loss:  0.31738758087158203
train gradient:  0.23177637543654084
iteration : 4256
train acc:  0.796875
train loss:  0.43717628717422485
train gradient:  0.31165088554922776
iteration : 4257
train acc:  0.859375
train loss:  0.39645224809646606
train gradient:  0.2937389558510124
iteration : 4258
train acc:  0.8671875
train loss:  0.3057706952095032
train gradient:  0.21289105120201998
iteration : 4259
train acc:  0.8359375
train loss:  0.36878156661987305
train gradient:  0.320673094177174
iteration : 4260
train acc:  0.828125
train loss:  0.42174893617630005
train gradient:  0.3085582427472599
iteration : 4261
train acc:  0.8984375
train loss:  0.2846719026565552
train gradient:  0.19327466216392336
iteration : 4262
train acc:  0.890625
train loss:  0.2783181667327881
train gradient:  0.2009655353858437
iteration : 4263
train acc:  0.8359375
train loss:  0.3785579800605774
train gradient:  0.3223688304222517
iteration : 4264
train acc:  0.8203125
train loss:  0.43418651819229126
train gradient:  0.42302203345311523
iteration : 4265
train acc:  0.9140625
train loss:  0.3169231414794922
train gradient:  0.2502447789393239
iteration : 4266
train acc:  0.84375
train loss:  0.4166018068790436
train gradient:  0.33704048741650294
iteration : 4267
train acc:  0.8125
train loss:  0.44430673122406006
train gradient:  0.4587124162251464
iteration : 4268
train acc:  0.8671875
train loss:  0.33143723011016846
train gradient:  0.2496831418776994
iteration : 4269
train acc:  0.7578125
train loss:  0.43780210614204407
train gradient:  0.38829723150917106
iteration : 4270
train acc:  0.84375
train loss:  0.38076603412628174
train gradient:  0.2696978106742831
iteration : 4271
train acc:  0.890625
train loss:  0.26526200771331787
train gradient:  0.13315511616772618
iteration : 4272
train acc:  0.7890625
train loss:  0.4324694275856018
train gradient:  0.39071029808403424
iteration : 4273
train acc:  0.84375
train loss:  0.31008562445640564
train gradient:  0.1847094061222238
iteration : 4274
train acc:  0.8828125
train loss:  0.2709904909133911
train gradient:  0.1939696274776344
iteration : 4275
train acc:  0.8203125
train loss:  0.3807303011417389
train gradient:  0.3226088156658455
iteration : 4276
train acc:  0.8828125
train loss:  0.32530829310417175
train gradient:  0.21102977576379073
iteration : 4277
train acc:  0.8203125
train loss:  0.43962740898132324
train gradient:  0.7740960461247666
iteration : 4278
train acc:  0.8515625
train loss:  0.30977219343185425
train gradient:  0.2880183783291578
iteration : 4279
train acc:  0.8046875
train loss:  0.5199410319328308
train gradient:  0.41227123582874053
iteration : 4280
train acc:  0.828125
train loss:  0.33985045552253723
train gradient:  0.3456089733389869
iteration : 4281
train acc:  0.8671875
train loss:  0.34891974925994873
train gradient:  0.2159447451262255
iteration : 4282
train acc:  0.859375
train loss:  0.3124111294746399
train gradient:  0.19369040794962053
iteration : 4283
train acc:  0.78125
train loss:  0.41242289543151855
train gradient:  0.4709186319419717
iteration : 4284
train acc:  0.828125
train loss:  0.40792739391326904
train gradient:  0.2927849102243165
iteration : 4285
train acc:  0.84375
train loss:  0.34060895442962646
train gradient:  0.17468338064404554
iteration : 4286
train acc:  0.8828125
train loss:  0.32583677768707275
train gradient:  0.1602454048526
iteration : 4287
train acc:  0.8515625
train loss:  0.2956111431121826
train gradient:  0.2176955397418457
iteration : 4288
train acc:  0.8671875
train loss:  0.3620396852493286
train gradient:  0.3433928015301266
iteration : 4289
train acc:  0.84375
train loss:  0.30055540800094604
train gradient:  0.28953940094818226
iteration : 4290
train acc:  0.7890625
train loss:  0.44705575704574585
train gradient:  0.4174447704015233
iteration : 4291
train acc:  0.8828125
train loss:  0.31743040680885315
train gradient:  0.3291015547605664
iteration : 4292
train acc:  0.8515625
train loss:  0.3398903012275696
train gradient:  0.2564138738479914
iteration : 4293
train acc:  0.8359375
train loss:  0.3500317335128784
train gradient:  0.25165322695774567
iteration : 4294
train acc:  0.828125
train loss:  0.30208030343055725
train gradient:  0.2141238567794792
iteration : 4295
train acc:  0.8515625
train loss:  0.3453875780105591
train gradient:  0.20752165766760078
iteration : 4296
train acc:  0.875
train loss:  0.33083224296569824
train gradient:  0.19482890823787805
iteration : 4297
train acc:  0.796875
train loss:  0.4173114597797394
train gradient:  0.35627804832307225
iteration : 4298
train acc:  0.828125
train loss:  0.3483498692512512
train gradient:  0.3337397656658296
iteration : 4299
train acc:  0.828125
train loss:  0.35268664360046387
train gradient:  0.3079930259092732
iteration : 4300
train acc:  0.859375
train loss:  0.2880564332008362
train gradient:  0.18098551425808102
iteration : 4301
train acc:  0.8046875
train loss:  0.4030212461948395
train gradient:  0.28058002391522546
iteration : 4302
train acc:  0.8125
train loss:  0.4962400197982788
train gradient:  0.46026928940812384
iteration : 4303
train acc:  0.890625
train loss:  0.32192230224609375
train gradient:  0.2408732181202857
iteration : 4304
train acc:  0.8515625
train loss:  0.33003950119018555
train gradient:  0.2267547104034894
iteration : 4305
train acc:  0.8671875
train loss:  0.3688202500343323
train gradient:  0.2627136375722437
iteration : 4306
train acc:  0.8359375
train loss:  0.4035608768463135
train gradient:  0.31136845465912216
iteration : 4307
train acc:  0.8203125
train loss:  0.3817909359931946
train gradient:  0.23525934592122288
iteration : 4308
train acc:  0.8125
train loss:  0.5065446496009827
train gradient:  0.40187295009171514
iteration : 4309
train acc:  0.796875
train loss:  0.4448995590209961
train gradient:  0.5803500156143574
iteration : 4310
train acc:  0.859375
train loss:  0.2966587543487549
train gradient:  0.17138121829837893
iteration : 4311
train acc:  0.8125
train loss:  0.3629516065120697
train gradient:  0.23529789048271305
iteration : 4312
train acc:  0.890625
train loss:  0.3156903088092804
train gradient:  0.2490714638296651
iteration : 4313
train acc:  0.8359375
train loss:  0.38874173164367676
train gradient:  0.26657358122370456
iteration : 4314
train acc:  0.8359375
train loss:  0.34163573384284973
train gradient:  0.21172936079102486
iteration : 4315
train acc:  0.828125
train loss:  0.3614755868911743
train gradient:  0.24099120772731408
iteration : 4316
train acc:  0.8515625
train loss:  0.3680104613304138
train gradient:  0.2782643578552326
iteration : 4317
train acc:  0.8359375
train loss:  0.35465604066848755
train gradient:  0.2167730889597201
iteration : 4318
train acc:  0.875
train loss:  0.33356934785842896
train gradient:  0.17370668421014307
iteration : 4319
train acc:  0.8828125
train loss:  0.30263498425483704
train gradient:  0.16928699561263055
iteration : 4320
train acc:  0.796875
train loss:  0.40882548689842224
train gradient:  0.2874799071455542
iteration : 4321
train acc:  0.875
train loss:  0.29798179864883423
train gradient:  0.19324647918642182
iteration : 4322
train acc:  0.796875
train loss:  0.42479467391967773
train gradient:  0.2176994682180876
iteration : 4323
train acc:  0.90625
train loss:  0.29842472076416016
train gradient:  0.3014446649508396
iteration : 4324
train acc:  0.8359375
train loss:  0.41678470373153687
train gradient:  0.2852813269380438
iteration : 4325
train acc:  0.828125
train loss:  0.3953689932823181
train gradient:  0.3365349975776533
iteration : 4326
train acc:  0.8828125
train loss:  0.2961714267730713
train gradient:  0.1990947671712405
iteration : 4327
train acc:  0.8515625
train loss:  0.4154551029205322
train gradient:  0.2796096004549163
iteration : 4328
train acc:  0.8671875
train loss:  0.3071601390838623
train gradient:  0.17484975656455337
iteration : 4329
train acc:  0.859375
train loss:  0.35975003242492676
train gradient:  0.4776085497080697
iteration : 4330
train acc:  0.828125
train loss:  0.375623494386673
train gradient:  0.277412370598813
iteration : 4331
train acc:  0.84375
train loss:  0.36035624146461487
train gradient:  0.18454621689304987
iteration : 4332
train acc:  0.859375
train loss:  0.3425208628177643
train gradient:  0.28993725331514864
iteration : 4333
train acc:  0.84375
train loss:  0.35081860423088074
train gradient:  0.22365520873714512
iteration : 4334
train acc:  0.859375
train loss:  0.3389964997768402
train gradient:  0.20244895521731646
iteration : 4335
train acc:  0.8359375
train loss:  0.34422075748443604
train gradient:  0.23057487736013185
iteration : 4336
train acc:  0.84375
train loss:  0.38805609941482544
train gradient:  0.30497926964501954
iteration : 4337
train acc:  0.8125
train loss:  0.407507061958313
train gradient:  0.2442078707637166
iteration : 4338
train acc:  0.8828125
train loss:  0.3163202404975891
train gradient:  0.17757957725129372
iteration : 4339
train acc:  0.8984375
train loss:  0.30625370144844055
train gradient:  0.17666702955477656
iteration : 4340
train acc:  0.8671875
train loss:  0.3730276823043823
train gradient:  0.2518203533002603
iteration : 4341
train acc:  0.84375
train loss:  0.34711408615112305
train gradient:  0.2155275642595328
iteration : 4342
train acc:  0.8828125
train loss:  0.2896018624305725
train gradient:  0.15795613853695994
iteration : 4343
train acc:  0.7890625
train loss:  0.3895963430404663
train gradient:  0.3263848488879812
iteration : 4344
train acc:  0.859375
train loss:  0.3363742530345917
train gradient:  0.29190084958335655
iteration : 4345
train acc:  0.8515625
train loss:  0.34506452083587646
train gradient:  0.1842482832548341
iteration : 4346
train acc:  0.8203125
train loss:  0.3830304741859436
train gradient:  0.27796939181102515
iteration : 4347
train acc:  0.8359375
train loss:  0.3481890857219696
train gradient:  0.20859823376674957
iteration : 4348
train acc:  0.8515625
train loss:  0.38425159454345703
train gradient:  0.2509224157533353
iteration : 4349
train acc:  0.796875
train loss:  0.40642407536506653
train gradient:  0.25460068514817297
iteration : 4350
train acc:  0.8203125
train loss:  0.35395747423171997
train gradient:  0.20425856727820582
iteration : 4351
train acc:  0.84375
train loss:  0.4033474326133728
train gradient:  0.2639827327731088
iteration : 4352
train acc:  0.84375
train loss:  0.404682457447052
train gradient:  0.4289430413362159
iteration : 4353
train acc:  0.8515625
train loss:  0.3122221827507019
train gradient:  0.17045513158902653
iteration : 4354
train acc:  0.859375
train loss:  0.29912346601486206
train gradient:  0.1688421549719894
iteration : 4355
train acc:  0.875
train loss:  0.3010689616203308
train gradient:  0.2495805266875057
iteration : 4356
train acc:  0.890625
train loss:  0.31019383668899536
train gradient:  0.20717456104172977
iteration : 4357
train acc:  0.8125
train loss:  0.37285172939300537
train gradient:  0.22344857436110693
iteration : 4358
train acc:  0.8984375
train loss:  0.29302680492401123
train gradient:  0.20061390011399996
iteration : 4359
train acc:  0.9375
train loss:  0.2552966773509979
train gradient:  0.16489784797591128
iteration : 4360
train acc:  0.875
train loss:  0.27366721630096436
train gradient:  0.2221800528579247
iteration : 4361
train acc:  0.8203125
train loss:  0.4441874027252197
train gradient:  0.5331380270238812
iteration : 4362
train acc:  0.8359375
train loss:  0.4130439758300781
train gradient:  0.24274181669632872
iteration : 4363
train acc:  0.8125
train loss:  0.3968679904937744
train gradient:  0.3296158617913867
iteration : 4364
train acc:  0.859375
train loss:  0.3134765326976776
train gradient:  0.20128620369750855
iteration : 4365
train acc:  0.8515625
train loss:  0.30781078338623047
train gradient:  0.33761846327045214
iteration : 4366
train acc:  0.7890625
train loss:  0.4467592239379883
train gradient:  0.4082125692145213
iteration : 4367
train acc:  0.8125
train loss:  0.3520592451095581
train gradient:  0.30002799021302773
iteration : 4368
train acc:  0.890625
train loss:  0.29406842589378357
train gradient:  0.25330164700108476
iteration : 4369
train acc:  0.859375
train loss:  0.3205532729625702
train gradient:  0.2749660586789907
iteration : 4370
train acc:  0.8984375
train loss:  0.2805722951889038
train gradient:  0.22732054301586999
iteration : 4371
train acc:  0.8671875
train loss:  0.3229679465293884
train gradient:  0.3898190856279257
iteration : 4372
train acc:  0.8984375
train loss:  0.2836202383041382
train gradient:  0.2182315084848832
iteration : 4373
train acc:  0.828125
train loss:  0.34739238023757935
train gradient:  0.41353707041689086
iteration : 4374
train acc:  0.875
train loss:  0.29385143518447876
train gradient:  0.3789733789011693
iteration : 4375
train acc:  0.8203125
train loss:  0.41823333501815796
train gradient:  0.2677933477701875
iteration : 4376
train acc:  0.84375
train loss:  0.3282383382320404
train gradient:  0.19626312949826175
iteration : 4377
train acc:  0.8984375
train loss:  0.28506800532341003
train gradient:  0.2015357129684146
iteration : 4378
train acc:  0.859375
train loss:  0.2963564991950989
train gradient:  0.238835063825905
iteration : 4379
train acc:  0.8203125
train loss:  0.37861090898513794
train gradient:  0.26987385759641813
iteration : 4380
train acc:  0.8359375
train loss:  0.4079972803592682
train gradient:  0.2950960016046989
iteration : 4381
train acc:  0.78125
train loss:  0.4080686569213867
train gradient:  0.33547331591709406
iteration : 4382
train acc:  0.84375
train loss:  0.3518716096878052
train gradient:  0.33318749149811416
iteration : 4383
train acc:  0.8359375
train loss:  0.36454665660858154
train gradient:  0.283875632972886
iteration : 4384
train acc:  0.84375
train loss:  0.3133302927017212
train gradient:  0.27459498791117265
iteration : 4385
train acc:  0.8515625
train loss:  0.3239518404006958
train gradient:  0.19472273306242482
iteration : 4386
train acc:  0.8515625
train loss:  0.3241506814956665
train gradient:  0.23622603159192979
iteration : 4387
train acc:  0.796875
train loss:  0.35091134905815125
train gradient:  0.3334375652586304
iteration : 4388
train acc:  0.875
train loss:  0.2738361060619354
train gradient:  0.2709071796161937
iteration : 4389
train acc:  0.890625
train loss:  0.24657554924488068
train gradient:  0.1412827456308368
iteration : 4390
train acc:  0.8515625
train loss:  0.3988904654979706
train gradient:  0.2880162843410439
iteration : 4391
train acc:  0.828125
train loss:  0.4107655882835388
train gradient:  0.34601409115477505
iteration : 4392
train acc:  0.859375
train loss:  0.3013102114200592
train gradient:  0.23625526524515525
iteration : 4393
train acc:  0.84375
train loss:  0.38209086656570435
train gradient:  0.2704033166545493
iteration : 4394
train acc:  0.859375
train loss:  0.31887614727020264
train gradient:  0.21667229055675943
iteration : 4395
train acc:  0.84375
train loss:  0.3995066285133362
train gradient:  0.3311519932763069
iteration : 4396
train acc:  0.8203125
train loss:  0.39619600772857666
train gradient:  0.2820645038541001
iteration : 4397
train acc:  0.8515625
train loss:  0.35655391216278076
train gradient:  0.30061255537702447
iteration : 4398
train acc:  0.8203125
train loss:  0.4058680534362793
train gradient:  0.3794692128115261
iteration : 4399
train acc:  0.8515625
train loss:  0.3529633581638336
train gradient:  0.36467224231460266
iteration : 4400
train acc:  0.8828125
train loss:  0.276037335395813
train gradient:  0.1794941737224972
iteration : 4401
train acc:  0.875
train loss:  0.32733890414237976
train gradient:  0.23429785830620414
iteration : 4402
train acc:  0.859375
train loss:  0.3650873899459839
train gradient:  0.3168046162612833
iteration : 4403
train acc:  0.828125
train loss:  0.34381526708602905
train gradient:  0.24412764135656367
iteration : 4404
train acc:  0.8203125
train loss:  0.3756256699562073
train gradient:  0.31145226850062396
iteration : 4405
train acc:  0.796875
train loss:  0.4212573170661926
train gradient:  0.38835130395197426
iteration : 4406
train acc:  0.8515625
train loss:  0.3867628276348114
train gradient:  0.30491198111369655
iteration : 4407
train acc:  0.78125
train loss:  0.4012952744960785
train gradient:  0.28978023535091174
iteration : 4408
train acc:  0.8515625
train loss:  0.35240447521209717
train gradient:  0.25328146019667147
iteration : 4409
train acc:  0.828125
train loss:  0.40657854080200195
train gradient:  0.3378934254684758
iteration : 4410
train acc:  0.84375
train loss:  0.32501494884490967
train gradient:  0.23054720970518092
iteration : 4411
train acc:  0.875
train loss:  0.3459208607673645
train gradient:  0.23224982115977155
iteration : 4412
train acc:  0.859375
train loss:  0.31084078550338745
train gradient:  0.2641043507680993
iteration : 4413
train acc:  0.859375
train loss:  0.295162171125412
train gradient:  0.19360457220674632
iteration : 4414
train acc:  0.8203125
train loss:  0.3622816205024719
train gradient:  0.37319937851045126
iteration : 4415
train acc:  0.84375
train loss:  0.31211233139038086
train gradient:  0.16975523386471086
iteration : 4416
train acc:  0.875
train loss:  0.28654682636260986
train gradient:  0.22979872806370819
iteration : 4417
train acc:  0.8515625
train loss:  0.35813939571380615
train gradient:  0.28188937544247517
iteration : 4418
train acc:  0.8203125
train loss:  0.3230997920036316
train gradient:  0.21604725608797123
iteration : 4419
train acc:  0.8203125
train loss:  0.3412575125694275
train gradient:  0.3219829324156689
iteration : 4420
train acc:  0.7890625
train loss:  0.4089462161064148
train gradient:  0.37662897770373566
iteration : 4421
train acc:  0.796875
train loss:  0.3968823552131653
train gradient:  0.34773670692663994
iteration : 4422
train acc:  0.84375
train loss:  0.3233898878097534
train gradient:  0.25301193312828785
iteration : 4423
train acc:  0.84375
train loss:  0.32471585273742676
train gradient:  0.2957130725429587
iteration : 4424
train acc:  0.8515625
train loss:  0.35131725668907166
train gradient:  0.2709148304571434
iteration : 4425
train acc:  0.796875
train loss:  0.4294033646583557
train gradient:  0.3087306335021925
iteration : 4426
train acc:  0.828125
train loss:  0.36847037076950073
train gradient:  0.2592892841944389
iteration : 4427
train acc:  0.8671875
train loss:  0.41507992148399353
train gradient:  0.31793370259720427
iteration : 4428
train acc:  0.828125
train loss:  0.42876917123794556
train gradient:  0.3372530137153883
iteration : 4429
train acc:  0.8828125
train loss:  0.3241068124771118
train gradient:  0.18944417019295817
iteration : 4430
train acc:  0.9140625
train loss:  0.2504793405532837
train gradient:  0.19908579902170748
iteration : 4431
train acc:  0.859375
train loss:  0.33530867099761963
train gradient:  0.21591441670260877
iteration : 4432
train acc:  0.7734375
train loss:  0.5851382613182068
train gradient:  0.6791969278157278
iteration : 4433
train acc:  0.8359375
train loss:  0.313315212726593
train gradient:  0.27288737948552794
iteration : 4434
train acc:  0.890625
train loss:  0.3131292760372162
train gradient:  0.2220302557675775
iteration : 4435
train acc:  0.828125
train loss:  0.40171611309051514
train gradient:  0.4141592153682658
iteration : 4436
train acc:  0.8125
train loss:  0.4180818498134613
train gradient:  0.3871166158026552
iteration : 4437
train acc:  0.8203125
train loss:  0.33888381719589233
train gradient:  0.21847635416038186
iteration : 4438
train acc:  0.8671875
train loss:  0.3560902178287506
train gradient:  0.27542447791711544
iteration : 4439
train acc:  0.796875
train loss:  0.4124952554702759
train gradient:  0.31718651371019846
iteration : 4440
train acc:  0.84375
train loss:  0.4157683253288269
train gradient:  0.3188533950109133
iteration : 4441
train acc:  0.828125
train loss:  0.38881367444992065
train gradient:  0.25969551780433536
iteration : 4442
train acc:  0.8203125
train loss:  0.35803669691085815
train gradient:  0.23913167033164057
iteration : 4443
train acc:  0.8515625
train loss:  0.3179580569267273
train gradient:  0.22158189253939786
iteration : 4444
train acc:  0.796875
train loss:  0.46159058809280396
train gradient:  0.36242463112201484
iteration : 4445
train acc:  0.8359375
train loss:  0.3594473600387573
train gradient:  0.36365688058353235
iteration : 4446
train acc:  0.8359375
train loss:  0.349977970123291
train gradient:  0.27140818165701386
iteration : 4447
train acc:  0.828125
train loss:  0.355013906955719
train gradient:  0.25307739759071424
iteration : 4448
train acc:  0.875
train loss:  0.2929157018661499
train gradient:  0.18493750570305814
iteration : 4449
train acc:  0.828125
train loss:  0.3844907879829407
train gradient:  0.2977857758032307
iteration : 4450
train acc:  0.8046875
train loss:  0.42201876640319824
train gradient:  0.33582666970949543
iteration : 4451
train acc:  0.84375
train loss:  0.34563344717025757
train gradient:  0.17567686701789248
iteration : 4452
train acc:  0.7890625
train loss:  0.411268949508667
train gradient:  0.4227221113288175
iteration : 4453
train acc:  0.8671875
train loss:  0.3718909025192261
train gradient:  0.30043882901811086
iteration : 4454
train acc:  0.8671875
train loss:  0.32384344935417175
train gradient:  0.2481206901715539
iteration : 4455
train acc:  0.8125
train loss:  0.35089918971061707
train gradient:  0.2664079537756291
iteration : 4456
train acc:  0.84375
train loss:  0.4132031798362732
train gradient:  0.38110624464197956
iteration : 4457
train acc:  0.828125
train loss:  0.34310945868492126
train gradient:  0.23370697050919187
iteration : 4458
train acc:  0.7578125
train loss:  0.5116686224937439
train gradient:  0.564484418300012
iteration : 4459
train acc:  0.890625
train loss:  0.29615670442581177
train gradient:  0.160322425538667
iteration : 4460
train acc:  0.828125
train loss:  0.3279626667499542
train gradient:  0.27403528959383366
iteration : 4461
train acc:  0.84375
train loss:  0.3628087043762207
train gradient:  0.3311531595118823
iteration : 4462
train acc:  0.796875
train loss:  0.3873787820339203
train gradient:  0.2690532741575519
iteration : 4463
train acc:  0.8046875
train loss:  0.45629823207855225
train gradient:  0.511834566692484
iteration : 4464
train acc:  0.8984375
train loss:  0.2819315791130066
train gradient:  0.18956956088218832
iteration : 4465
train acc:  0.859375
train loss:  0.3411716818809509
train gradient:  0.37855362963292793
iteration : 4466
train acc:  0.859375
train loss:  0.32788360118865967
train gradient:  0.2199641342162355
iteration : 4467
train acc:  0.84375
train loss:  0.3362996578216553
train gradient:  0.24861081540332072
iteration : 4468
train acc:  0.84375
train loss:  0.4229109287261963
train gradient:  0.402882549477658
iteration : 4469
train acc:  0.8359375
train loss:  0.36482781171798706
train gradient:  0.2172832532550228
iteration : 4470
train acc:  0.8515625
train loss:  0.33744463324546814
train gradient:  0.2406066468617546
iteration : 4471
train acc:  0.859375
train loss:  0.34330689907073975
train gradient:  0.3513181032052708
iteration : 4472
train acc:  0.8203125
train loss:  0.43266069889068604
train gradient:  0.2706249956682437
iteration : 4473
train acc:  0.84375
train loss:  0.3880402445793152
train gradient:  0.3334438604258524
iteration : 4474
train acc:  0.859375
train loss:  0.34783098101615906
train gradient:  0.22669329973297464
iteration : 4475
train acc:  0.8515625
train loss:  0.3604480028152466
train gradient:  0.28116901642092107
iteration : 4476
train acc:  0.8125
train loss:  0.4887412190437317
train gradient:  0.36555402116889624
iteration : 4477
train acc:  0.875
train loss:  0.32161837816238403
train gradient:  0.2687617813574328
iteration : 4478
train acc:  0.8359375
train loss:  0.3407575786113739
train gradient:  0.20797084013461764
iteration : 4479
train acc:  0.8359375
train loss:  0.40657657384872437
train gradient:  0.3897381080930298
iteration : 4480
train acc:  0.84375
train loss:  0.35107704997062683
train gradient:  0.21962671871450862
iteration : 4481
train acc:  0.8046875
train loss:  0.39990103244781494
train gradient:  0.3558033211231496
iteration : 4482
train acc:  0.8046875
train loss:  0.40113815665245056
train gradient:  0.28312222511684093
iteration : 4483
train acc:  0.8671875
train loss:  0.31786808371543884
train gradient:  0.22589604916645037
iteration : 4484
train acc:  0.828125
train loss:  0.3489496111869812
train gradient:  0.21677965174427583
iteration : 4485
train acc:  0.828125
train loss:  0.3417133092880249
train gradient:  0.23966131320594752
iteration : 4486
train acc:  0.8046875
train loss:  0.4027078151702881
train gradient:  0.3521396901159438
iteration : 4487
train acc:  0.8984375
train loss:  0.28510475158691406
train gradient:  0.15325422749567053
iteration : 4488
train acc:  0.8046875
train loss:  0.3606039881706238
train gradient:  0.2628248795327891
iteration : 4489
train acc:  0.828125
train loss:  0.3451380729675293
train gradient:  0.21416529188702074
iteration : 4490
train acc:  0.859375
train loss:  0.33795875310897827
train gradient:  0.21006785984971704
iteration : 4491
train acc:  0.828125
train loss:  0.37058010697364807
train gradient:  0.2912390089575459
iteration : 4492
train acc:  0.796875
train loss:  0.43016934394836426
train gradient:  0.3759642528608423
iteration : 4493
train acc:  0.8203125
train loss:  0.4161435663700104
train gradient:  0.323993412379634
iteration : 4494
train acc:  0.875
train loss:  0.3110412061214447
train gradient:  0.21024203361811133
iteration : 4495
train acc:  0.8125
train loss:  0.3624456524848938
train gradient:  0.39252843117277164
iteration : 4496
train acc:  0.8671875
train loss:  0.3299882709980011
train gradient:  0.20741712905330567
iteration : 4497
train acc:  0.8515625
train loss:  0.35811710357666016
train gradient:  0.2557025055042103
iteration : 4498
train acc:  0.8515625
train loss:  0.3565717339515686
train gradient:  0.255482437825581
iteration : 4499
train acc:  0.8671875
train loss:  0.37778180837631226
train gradient:  0.6762813162355551
iteration : 4500
train acc:  0.7890625
train loss:  0.40112215280532837
train gradient:  0.5124231465786235
iteration : 4501
train acc:  0.828125
train loss:  0.3479883670806885
train gradient:  0.29905500158027176
iteration : 4502
train acc:  0.84375
train loss:  0.339560866355896
train gradient:  0.231421564041784
iteration : 4503
train acc:  0.8984375
train loss:  0.29355430603027344
train gradient:  0.24385228953767601
iteration : 4504
train acc:  0.8828125
train loss:  0.35058170557022095
train gradient:  0.28467171950962367
iteration : 4505
train acc:  0.875
train loss:  0.2800966203212738
train gradient:  0.17480384575337343
iteration : 4506
train acc:  0.8515625
train loss:  0.39493727684020996
train gradient:  0.317247193493668
iteration : 4507
train acc:  0.7890625
train loss:  0.46359628438949585
train gradient:  0.391649744730384
iteration : 4508
train acc:  0.875
train loss:  0.30746695399284363
train gradient:  0.20066332990277508
iteration : 4509
train acc:  0.8515625
train loss:  0.3728598356246948
train gradient:  0.29264096954092284
iteration : 4510
train acc:  0.84375
train loss:  0.40638676285743713
train gradient:  0.2866230790212827
iteration : 4511
train acc:  0.8046875
train loss:  0.44555777311325073
train gradient:  0.3523275931555819
iteration : 4512
train acc:  0.890625
train loss:  0.3015868663787842
train gradient:  0.20602662245275233
iteration : 4513
train acc:  0.8515625
train loss:  0.35561928153038025
train gradient:  0.22140133702870365
iteration : 4514
train acc:  0.8671875
train loss:  0.378528356552124
train gradient:  0.3871433336076664
iteration : 4515
train acc:  0.796875
train loss:  0.5016705989837646
train gradient:  0.4247000239229667
iteration : 4516
train acc:  0.828125
train loss:  0.37558865547180176
train gradient:  0.2719262523758605
iteration : 4517
train acc:  0.8359375
train loss:  0.4244422912597656
train gradient:  0.33918786156769176
iteration : 4518
train acc:  0.8515625
train loss:  0.3745206892490387
train gradient:  0.2569431094939376
iteration : 4519
train acc:  0.84375
train loss:  0.32233962416648865
train gradient:  0.23935064043539572
iteration : 4520
train acc:  0.8515625
train loss:  0.3418542146682739
train gradient:  0.21754784197647767
iteration : 4521
train acc:  0.8828125
train loss:  0.30972474813461304
train gradient:  0.2948985366973223
iteration : 4522
train acc:  0.8046875
train loss:  0.38219916820526123
train gradient:  0.37018583415323975
iteration : 4523
train acc:  0.8125
train loss:  0.38492608070373535
train gradient:  0.38672779109883265
iteration : 4524
train acc:  0.828125
train loss:  0.34844720363616943
train gradient:  0.35406372258095986
iteration : 4525
train acc:  0.78125
train loss:  0.3957812786102295
train gradient:  0.32645294372640177
iteration : 4526
train acc:  0.828125
train loss:  0.4031761884689331
train gradient:  0.24313076462675923
iteration : 4527
train acc:  0.796875
train loss:  0.406799852848053
train gradient:  0.42949225411314995
iteration : 4528
train acc:  0.8125
train loss:  0.41988420486450195
train gradient:  0.31925460057655747
iteration : 4529
train acc:  0.8359375
train loss:  0.3787788450717926
train gradient:  0.3184599312521232
iteration : 4530
train acc:  0.8125
train loss:  0.4309806823730469
train gradient:  0.32901787873175714
iteration : 4531
train acc:  0.859375
train loss:  0.32293272018432617
train gradient:  0.22354354901168527
iteration : 4532
train acc:  0.859375
train loss:  0.3314708471298218
train gradient:  0.2511674239327048
iteration : 4533
train acc:  0.8984375
train loss:  0.26838651299476624
train gradient:  0.11977577404436486
iteration : 4534
train acc:  0.875
train loss:  0.30944114923477173
train gradient:  0.17255390570134432
iteration : 4535
train acc:  0.765625
train loss:  0.4600362777709961
train gradient:  0.4711017014102855
iteration : 4536
train acc:  0.84375
train loss:  0.4150497019290924
train gradient:  0.30384407843798833
iteration : 4537
train acc:  0.8359375
train loss:  0.39305102825164795
train gradient:  0.2930232881665905
iteration : 4538
train acc:  0.875
train loss:  0.31045597791671753
train gradient:  0.20284919656969413
iteration : 4539
train acc:  0.859375
train loss:  0.3580353856086731
train gradient:  0.19161518743158754
iteration : 4540
train acc:  0.828125
train loss:  0.3620825409889221
train gradient:  0.2308787398299152
iteration : 4541
train acc:  0.8515625
train loss:  0.3488328456878662
train gradient:  0.23597700912611602
iteration : 4542
train acc:  0.84375
train loss:  0.3600236177444458
train gradient:  0.2857163542023029
iteration : 4543
train acc:  0.84375
train loss:  0.3526102900505066
train gradient:  0.27682528922864214
iteration : 4544
train acc:  0.828125
train loss:  0.3869825601577759
train gradient:  0.2727800401698615
iteration : 4545
train acc:  0.8515625
train loss:  0.35874003171920776
train gradient:  0.2594303936553366
iteration : 4546
train acc:  0.859375
train loss:  0.32500553131103516
train gradient:  0.201449602368125
iteration : 4547
train acc:  0.890625
train loss:  0.3150597810745239
train gradient:  0.19297880257706612
iteration : 4548
train acc:  0.8359375
train loss:  0.41490164399147034
train gradient:  0.28870849373852014
iteration : 4549
train acc:  0.828125
train loss:  0.3792060613632202
train gradient:  0.22284421316632727
iteration : 4550
train acc:  0.90625
train loss:  0.28172725439071655
train gradient:  0.1627508801155407
iteration : 4551
train acc:  0.8828125
train loss:  0.2950126528739929
train gradient:  0.22481347394965684
iteration : 4552
train acc:  0.828125
train loss:  0.3516755998134613
train gradient:  0.26079563045365983
iteration : 4553
train acc:  0.8046875
train loss:  0.40726438164711
train gradient:  0.32484883284137206
iteration : 4554
train acc:  0.8046875
train loss:  0.36117756366729736
train gradient:  0.20824704430982538
iteration : 4555
train acc:  0.8515625
train loss:  0.4060521125793457
train gradient:  0.33134767824336797
iteration : 4556
train acc:  0.90625
train loss:  0.2498161494731903
train gradient:  0.13127833096275018
iteration : 4557
train acc:  0.8359375
train loss:  0.3606411814689636
train gradient:  0.24157506967345274
iteration : 4558
train acc:  0.84375
train loss:  0.3363102376461029
train gradient:  0.2070469357262002
iteration : 4559
train acc:  0.8359375
train loss:  0.385883092880249
train gradient:  0.24619977580439903
iteration : 4560
train acc:  0.84375
train loss:  0.3759132921695709
train gradient:  0.30781558714836244
iteration : 4561
train acc:  0.8203125
train loss:  0.37729018926620483
train gradient:  0.2522975574255847
iteration : 4562
train acc:  0.875
train loss:  0.32786431908607483
train gradient:  0.34175023850314873
iteration : 4563
train acc:  0.859375
train loss:  0.3051247000694275
train gradient:  0.32794810953720627
iteration : 4564
train acc:  0.8203125
train loss:  0.4066263735294342
train gradient:  0.2803692345491832
iteration : 4565
train acc:  0.84375
train loss:  0.3968915045261383
train gradient:  0.3211218085953165
iteration : 4566
train acc:  0.859375
train loss:  0.3337406516075134
train gradient:  0.27941293178360743
iteration : 4567
train acc:  0.8828125
train loss:  0.320930540561676
train gradient:  0.20997581988338657
iteration : 4568
train acc:  0.875
train loss:  0.3002680540084839
train gradient:  0.23671608553867524
iteration : 4569
train acc:  0.8203125
train loss:  0.36940473318099976
train gradient:  0.3255370830306449
iteration : 4570
train acc:  0.8984375
train loss:  0.2834445834159851
train gradient:  0.21841335028711326
iteration : 4571
train acc:  0.828125
train loss:  0.3414148986339569
train gradient:  0.298570895091369
iteration : 4572
train acc:  0.8359375
train loss:  0.31686630845069885
train gradient:  0.1892063906516136
iteration : 4573
train acc:  0.875
train loss:  0.25809744000434875
train gradient:  0.17615011655662993
iteration : 4574
train acc:  0.828125
train loss:  0.34858739376068115
train gradient:  0.22999179197651362
iteration : 4575
train acc:  0.859375
train loss:  0.38121747970581055
train gradient:  0.4199006220343236
iteration : 4576
train acc:  0.8515625
train loss:  0.310271292924881
train gradient:  0.2530841695588357
iteration : 4577
train acc:  0.828125
train loss:  0.3669683635234833
train gradient:  0.25828421318632916
iteration : 4578
train acc:  0.7890625
train loss:  0.4523710608482361
train gradient:  0.3014702796308263
iteration : 4579
train acc:  0.796875
train loss:  0.44057178497314453
train gradient:  0.3315307192938422
iteration : 4580
train acc:  0.8828125
train loss:  0.2825448513031006
train gradient:  0.2588004520748733
iteration : 4581
train acc:  0.859375
train loss:  0.3465908467769623
train gradient:  0.3319835742593012
iteration : 4582
train acc:  0.84375
train loss:  0.36961621046066284
train gradient:  0.30173946693128334
iteration : 4583
train acc:  0.859375
train loss:  0.3733106851577759
train gradient:  0.25023774319688763
iteration : 4584
train acc:  0.859375
train loss:  0.3340323567390442
train gradient:  0.17620353355423607
iteration : 4585
train acc:  0.8046875
train loss:  0.4500613808631897
train gradient:  0.40808635512217484
iteration : 4586
train acc:  0.7890625
train loss:  0.43633949756622314
train gradient:  0.2952008374346957
iteration : 4587
train acc:  0.875
train loss:  0.3369019031524658
train gradient:  0.20313517670734843
iteration : 4588
train acc:  0.7890625
train loss:  0.44731104373931885
train gradient:  0.42290882662975327
iteration : 4589
train acc:  0.8515625
train loss:  0.32803526520729065
train gradient:  0.203577091122452
iteration : 4590
train acc:  0.859375
train loss:  0.33120113611221313
train gradient:  0.29242343995936576
iteration : 4591
train acc:  0.8125
train loss:  0.3945055305957794
train gradient:  0.2341692697895581
iteration : 4592
train acc:  0.796875
train loss:  0.4373093545436859
train gradient:  0.3470319914231064
iteration : 4593
train acc:  0.859375
train loss:  0.3424920439720154
train gradient:  0.20488050266982183
iteration : 4594
train acc:  0.7734375
train loss:  0.5285545587539673
train gradient:  0.6247274842587169
iteration : 4595
train acc:  0.828125
train loss:  0.40201377868652344
train gradient:  0.29966227136323814
iteration : 4596
train acc:  0.859375
train loss:  0.3463345170021057
train gradient:  0.3949968611153104
iteration : 4597
train acc:  0.8515625
train loss:  0.34665894508361816
train gradient:  0.19358185379309517
iteration : 4598
train acc:  0.828125
train loss:  0.39344000816345215
train gradient:  0.31487277224089816
iteration : 4599
train acc:  0.8203125
train loss:  0.4267667829990387
train gradient:  0.3341759124315139
iteration : 4600
train acc:  0.8359375
train loss:  0.37384456396102905
train gradient:  0.22568231244533785
iteration : 4601
train acc:  0.8671875
train loss:  0.32040935754776
train gradient:  0.21764136807906675
iteration : 4602
train acc:  0.8046875
train loss:  0.3941773772239685
train gradient:  0.2350042089129166
iteration : 4603
train acc:  0.875
train loss:  0.2727477550506592
train gradient:  0.130970775069591
iteration : 4604
train acc:  0.84375
train loss:  0.35909444093704224
train gradient:  0.2813363416843103
iteration : 4605
train acc:  0.7890625
train loss:  0.37723755836486816
train gradient:  0.30274565986369495
iteration : 4606
train acc:  0.8515625
train loss:  0.3292950987815857
train gradient:  0.27173544141708494
iteration : 4607
train acc:  0.875
train loss:  0.3447721004486084
train gradient:  0.19279480831674406
iteration : 4608
train acc:  0.8359375
train loss:  0.3785213232040405
train gradient:  0.30188150137787084
iteration : 4609
train acc:  0.859375
train loss:  0.3199981451034546
train gradient:  0.20877602109556437
iteration : 4610
train acc:  0.90625
train loss:  0.30674439668655396
train gradient:  0.20982215572082352
iteration : 4611
train acc:  0.828125
train loss:  0.3790813088417053
train gradient:  0.2926443782992238
iteration : 4612
train acc:  0.8203125
train loss:  0.3694300055503845
train gradient:  0.2495327968168229
iteration : 4613
train acc:  0.859375
train loss:  0.36238527297973633
train gradient:  0.30237692217000406
iteration : 4614
train acc:  0.828125
train loss:  0.435316801071167
train gradient:  0.28884959317469444
iteration : 4615
train acc:  0.8125
train loss:  0.37969234585762024
train gradient:  0.24247659664723814
iteration : 4616
train acc:  0.84375
train loss:  0.32186323404312134
train gradient:  0.22824406018983803
iteration : 4617
train acc:  0.8125
train loss:  0.3990783095359802
train gradient:  0.32320302907196374
iteration : 4618
train acc:  0.859375
train loss:  0.33755922317504883
train gradient:  0.22847721017216172
iteration : 4619
train acc:  0.8671875
train loss:  0.30104294419288635
train gradient:  0.15899053171206834
iteration : 4620
train acc:  0.8671875
train loss:  0.32133960723876953
train gradient:  0.2240707166226556
iteration : 4621
train acc:  0.7734375
train loss:  0.47847840189933777
train gradient:  0.3450792780365618
iteration : 4622
train acc:  0.8125
train loss:  0.38148343563079834
train gradient:  0.3095840508673073
iteration : 4623
train acc:  0.796875
train loss:  0.43050241470336914
train gradient:  0.2999841462788231
iteration : 4624
train acc:  0.8671875
train loss:  0.298648864030838
train gradient:  0.1689766820924613
iteration : 4625
train acc:  0.875
train loss:  0.3237641155719757
train gradient:  0.2345146834793594
iteration : 4626
train acc:  0.875
train loss:  0.2866411805152893
train gradient:  0.2541569499880082
iteration : 4627
train acc:  0.8359375
train loss:  0.3496582508087158
train gradient:  0.284421342126338
iteration : 4628
train acc:  0.8125
train loss:  0.3734840452671051
train gradient:  0.22011369134093933
iteration : 4629
train acc:  0.8125
train loss:  0.35260698199272156
train gradient:  0.22345270933403294
iteration : 4630
train acc:  0.84375
train loss:  0.36288756132125854
train gradient:  0.27881416069945864
iteration : 4631
train acc:  0.8125
train loss:  0.3920556902885437
train gradient:  0.32751478053962146
iteration : 4632
train acc:  0.875
train loss:  0.3197043538093567
train gradient:  0.18399296402400947
iteration : 4633
train acc:  0.90625
train loss:  0.26184993982315063
train gradient:  0.17345397694333117
iteration : 4634
train acc:  0.8515625
train loss:  0.3474682867527008
train gradient:  0.19086693856109135
iteration : 4635
train acc:  0.8828125
train loss:  0.3161858320236206
train gradient:  0.20357994769608734
iteration : 4636
train acc:  0.8125
train loss:  0.43845033645629883
train gradient:  0.31415134634609715
iteration : 4637
train acc:  0.8125
train loss:  0.37729814648628235
train gradient:  0.22250544043720807
iteration : 4638
train acc:  0.875
train loss:  0.3109247088432312
train gradient:  0.1578589486218698
iteration : 4639
train acc:  0.8046875
train loss:  0.3684544265270233
train gradient:  0.3092886513248149
iteration : 4640
train acc:  0.875
train loss:  0.30960071086883545
train gradient:  0.15922940252946752
iteration : 4641
train acc:  0.8125
train loss:  0.3954692482948303
train gradient:  0.2815124370720861
iteration : 4642
train acc:  0.84375
train loss:  0.3409586548805237
train gradient:  0.20745230602011494
iteration : 4643
train acc:  0.8671875
train loss:  0.29786115884780884
train gradient:  0.15974720002183612
iteration : 4644
train acc:  0.8828125
train loss:  0.3068710267543793
train gradient:  0.2141899393094075
iteration : 4645
train acc:  0.8515625
train loss:  0.2992185950279236
train gradient:  0.18130000900667717
iteration : 4646
train acc:  0.875
train loss:  0.3417549133300781
train gradient:  0.2666983525736066
iteration : 4647
train acc:  0.78125
train loss:  0.4432571530342102
train gradient:  0.42074411927017147
iteration : 4648
train acc:  0.8203125
train loss:  0.37745463848114014
train gradient:  0.2959249078865148
iteration : 4649
train acc:  0.8515625
train loss:  0.3027023673057556
train gradient:  0.18501941667457936
iteration : 4650
train acc:  0.796875
train loss:  0.414348304271698
train gradient:  0.3936962810882426
iteration : 4651
train acc:  0.8671875
train loss:  0.311848908662796
train gradient:  0.3020469120809005
iteration : 4652
train acc:  0.8671875
train loss:  0.3170887529850006
train gradient:  0.20090769959391172
iteration : 4653
train acc:  0.8359375
train loss:  0.3350221514701843
train gradient:  0.3665649061082711
iteration : 4654
train acc:  0.875
train loss:  0.39077502489089966
train gradient:  0.2804756187757162
iteration : 4655
train acc:  0.8359375
train loss:  0.3371019959449768
train gradient:  0.25647121074292034
iteration : 4656
train acc:  0.84375
train loss:  0.3171039819717407
train gradient:  0.21078222463214635
iteration : 4657
train acc:  0.8125
train loss:  0.43295788764953613
train gradient:  0.34195135928097936
iteration : 4658
train acc:  0.875
train loss:  0.3037285804748535
train gradient:  0.2145330956467621
iteration : 4659
train acc:  0.8359375
train loss:  0.43396857380867004
train gradient:  0.473422791653504
iteration : 4660
train acc:  0.8203125
train loss:  0.33001625537872314
train gradient:  0.2417766296155917
iteration : 4661
train acc:  0.84375
train loss:  0.30808305740356445
train gradient:  0.2058074522798221
iteration : 4662
train acc:  0.828125
train loss:  0.3259127140045166
train gradient:  0.2700286286084677
iteration : 4663
train acc:  0.890625
train loss:  0.2767072319984436
train gradient:  0.1357465316003391
iteration : 4664
train acc:  0.8359375
train loss:  0.3542539179325104
train gradient:  0.31157021173052446
iteration : 4665
train acc:  0.84375
train loss:  0.30966341495513916
train gradient:  0.22048764357173206
iteration : 4666
train acc:  0.8203125
train loss:  0.34765371680259705
train gradient:  0.23105917086372968
iteration : 4667
train acc:  0.828125
train loss:  0.3608829379081726
train gradient:  0.2667443672256548
iteration : 4668
train acc:  0.84375
train loss:  0.3606935739517212
train gradient:  0.25946980436573
iteration : 4669
train acc:  0.8125
train loss:  0.39371606707572937
train gradient:  0.2837495944593712
iteration : 4670
train acc:  0.828125
train loss:  0.45684489607810974
train gradient:  0.4327721593205462
iteration : 4671
train acc:  0.8515625
train loss:  0.38023272156715393
train gradient:  0.3081637847987283
iteration : 4672
train acc:  0.8125
train loss:  0.35763490200042725
train gradient:  0.2717357248529676
iteration : 4673
train acc:  0.8671875
train loss:  0.2919207215309143
train gradient:  0.22383116525794122
iteration : 4674
train acc:  0.859375
train loss:  0.32628434896469116
train gradient:  0.22156656654709422
iteration : 4675
train acc:  0.84375
train loss:  0.28058719635009766
train gradient:  0.2392498694823544
iteration : 4676
train acc:  0.828125
train loss:  0.38040584325790405
train gradient:  0.25592444618385324
iteration : 4677
train acc:  0.8515625
train loss:  0.31869056820869446
train gradient:  0.2421311294089178
iteration : 4678
train acc:  0.875
train loss:  0.3389776349067688
train gradient:  0.16681881773560797
iteration : 4679
train acc:  0.9140625
train loss:  0.2994146943092346
train gradient:  0.3193955867659737
iteration : 4680
train acc:  0.8125
train loss:  0.41325679421424866
train gradient:  0.3468814176387291
iteration : 4681
train acc:  0.8359375
train loss:  0.3762189447879791
train gradient:  0.25364162663180695
iteration : 4682
train acc:  0.8359375
train loss:  0.34844160079956055
train gradient:  0.3829804600374925
iteration : 4683
train acc:  0.8125
train loss:  0.3755560517311096
train gradient:  0.28829093142209605
iteration : 4684
train acc:  0.7578125
train loss:  0.49949172139167786
train gradient:  0.5233902358037226
iteration : 4685
train acc:  0.890625
train loss:  0.32150426506996155
train gradient:  0.1914890137111348
iteration : 4686
train acc:  0.8203125
train loss:  0.37720370292663574
train gradient:  0.35163418402140373
iteration : 4687
train acc:  0.8671875
train loss:  0.2792525589466095
train gradient:  0.1746598319943894
iteration : 4688
train acc:  0.84375
train loss:  0.3415827751159668
train gradient:  0.2504955654433369
iteration : 4689
train acc:  0.8984375
train loss:  0.301888108253479
train gradient:  0.2945355591170757
iteration : 4690
train acc:  0.8359375
train loss:  0.4067690968513489
train gradient:  0.26914791614341504
iteration : 4691
train acc:  0.8359375
train loss:  0.37673497200012207
train gradient:  0.29968864384699184
iteration : 4692
train acc:  0.84375
train loss:  0.35659781098365784
train gradient:  0.2981205622054277
iteration : 4693
train acc:  0.84375
train loss:  0.35734695196151733
train gradient:  0.44738078219521094
iteration : 4694
train acc:  0.875
train loss:  0.3143129348754883
train gradient:  0.2281108055085699
iteration : 4695
train acc:  0.84375
train loss:  0.367817759513855
train gradient:  0.32617228215905847
iteration : 4696
train acc:  0.8359375
train loss:  0.3490297794342041
train gradient:  0.3783985917126408
iteration : 4697
train acc:  0.796875
train loss:  0.5361956357955933
train gradient:  0.6733004682715198
iteration : 4698
train acc:  0.7890625
train loss:  0.37080979347229004
train gradient:  0.2720785185433687
iteration : 4699
train acc:  0.8359375
train loss:  0.3500635027885437
train gradient:  0.2605343087781075
iteration : 4700
train acc:  0.7578125
train loss:  0.535818874835968
train gradient:  0.4330767588003599
iteration : 4701
train acc:  0.8984375
train loss:  0.287837415933609
train gradient:  0.22615056714252135
iteration : 4702
train acc:  0.84375
train loss:  0.31138259172439575
train gradient:  0.3329008746508759
iteration : 4703
train acc:  0.796875
train loss:  0.46100732684135437
train gradient:  0.509271454251523
iteration : 4704
train acc:  0.859375
train loss:  0.347780704498291
train gradient:  0.28338925349955085
iteration : 4705
train acc:  0.8046875
train loss:  0.4358517825603485
train gradient:  0.3564935876189447
iteration : 4706
train acc:  0.8515625
train loss:  0.37925195693969727
train gradient:  0.28072505865018127
iteration : 4707
train acc:  0.8671875
train loss:  0.40164339542388916
train gradient:  0.41304576205406207
iteration : 4708
train acc:  0.84375
train loss:  0.30601373314857483
train gradient:  0.24831361617810266
iteration : 4709
train acc:  0.8515625
train loss:  0.3577684164047241
train gradient:  0.3061003846521579
iteration : 4710
train acc:  0.7890625
train loss:  0.39893487095832825
train gradient:  0.27052206077208496
iteration : 4711
train acc:  0.8671875
train loss:  0.3058598041534424
train gradient:  0.22970597846132768
iteration : 4712
train acc:  0.875
train loss:  0.316306471824646
train gradient:  0.25695634553690383
iteration : 4713
train acc:  0.84375
train loss:  0.4080265760421753
train gradient:  0.340027314341945
iteration : 4714
train acc:  0.8359375
train loss:  0.371895432472229
train gradient:  0.23917785293263433
iteration : 4715
train acc:  0.8203125
train loss:  0.3457464575767517
train gradient:  0.20047522782866284
iteration : 4716
train acc:  0.828125
train loss:  0.40954411029815674
train gradient:  0.29204135388704805
iteration : 4717
train acc:  0.90625
train loss:  0.2853671610355377
train gradient:  0.1952907799569003
iteration : 4718
train acc:  0.859375
train loss:  0.32584506273269653
train gradient:  0.25215204696393123
iteration : 4719
train acc:  0.84375
train loss:  0.33771124482154846
train gradient:  0.23265078457060215
iteration : 4720
train acc:  0.7890625
train loss:  0.4344578683376312
train gradient:  0.4069449247906692
iteration : 4721
train acc:  0.859375
train loss:  0.3623965084552765
train gradient:  0.26758691772088283
iteration : 4722
train acc:  0.859375
train loss:  0.34256410598754883
train gradient:  0.21465683123970614
iteration : 4723
train acc:  0.8515625
train loss:  0.41940975189208984
train gradient:  0.31954781745762395
iteration : 4724
train acc:  0.8125
train loss:  0.43281668424606323
train gradient:  0.3033097085238518
iteration : 4725
train acc:  0.8515625
train loss:  0.3391854166984558
train gradient:  0.16660586545666173
iteration : 4726
train acc:  0.84375
train loss:  0.354716420173645
train gradient:  0.2507862424611655
iteration : 4727
train acc:  0.84375
train loss:  0.33095884323120117
train gradient:  0.2013643278449025
iteration : 4728
train acc:  0.8359375
train loss:  0.367615282535553
train gradient:  0.2816504817315267
iteration : 4729
train acc:  0.859375
train loss:  0.3043558597564697
train gradient:  0.2007851726394698
iteration : 4730
train acc:  0.8359375
train loss:  0.317412406206131
train gradient:  0.2320620031497675
iteration : 4731
train acc:  0.8125
train loss:  0.3735884428024292
train gradient:  0.22615272416570112
iteration : 4732
train acc:  0.8203125
train loss:  0.37921586632728577
train gradient:  0.2574204262840625
iteration : 4733
train acc:  0.8359375
train loss:  0.3441803753376007
train gradient:  0.18560809932398817
iteration : 4734
train acc:  0.8359375
train loss:  0.3591744601726532
train gradient:  0.2633665899554405
iteration : 4735
train acc:  0.8203125
train loss:  0.36817601323127747
train gradient:  0.29883572300187355
iteration : 4736
train acc:  0.8515625
train loss:  0.3122293949127197
train gradient:  0.16305308449319567
iteration : 4737
train acc:  0.8203125
train loss:  0.3815389573574066
train gradient:  0.195158809958772
iteration : 4738
train acc:  0.828125
train loss:  0.3922286331653595
train gradient:  0.3431519448398145
iteration : 4739
train acc:  0.828125
train loss:  0.3472902774810791
train gradient:  0.24587299562671439
iteration : 4740
train acc:  0.84375
train loss:  0.3077712059020996
train gradient:  0.20501862970089302
iteration : 4741
train acc:  0.8359375
train loss:  0.31759774684906006
train gradient:  0.17348531501494738
iteration : 4742
train acc:  0.7890625
train loss:  0.47966867685317993
train gradient:  0.43318976015185146
iteration : 4743
train acc:  0.8359375
train loss:  0.4007457494735718
train gradient:  0.27338362849523623
iteration : 4744
train acc:  0.796875
train loss:  0.3842475414276123
train gradient:  0.22206800909674465
iteration : 4745
train acc:  0.796875
train loss:  0.39797723293304443
train gradient:  0.2676290258511945
iteration : 4746
train acc:  0.875
train loss:  0.30856063961982727
train gradient:  0.15250384116715773
iteration : 4747
train acc:  0.90625
train loss:  0.2566703259944916
train gradient:  0.17497961475150767
iteration : 4748
train acc:  0.8671875
train loss:  0.34762972593307495
train gradient:  0.19765144661800588
iteration : 4749
train acc:  0.8046875
train loss:  0.44419634342193604
train gradient:  0.434308609141931
iteration : 4750
train acc:  0.8046875
train loss:  0.3880576491355896
train gradient:  0.31718536671210046
iteration : 4751
train acc:  0.8515625
train loss:  0.3209066390991211
train gradient:  0.2509952117136859
iteration : 4752
train acc:  0.796875
train loss:  0.4212934374809265
train gradient:  0.30870888242869055
iteration : 4753
train acc:  0.8359375
train loss:  0.37640607357025146
train gradient:  1.0393577238558256
iteration : 4754
train acc:  0.8671875
train loss:  0.34377336502075195
train gradient:  0.17790163259208971
iteration : 4755
train acc:  0.90625
train loss:  0.2628950774669647
train gradient:  0.15190898006864423
iteration : 4756
train acc:  0.859375
train loss:  0.351063072681427
train gradient:  0.25869162699068154
iteration : 4757
train acc:  0.828125
train loss:  0.38152584433555603
train gradient:  0.27021852650566464
iteration : 4758
train acc:  0.84375
train loss:  0.357208251953125
train gradient:  0.22448343469903792
iteration : 4759
train acc:  0.8203125
train loss:  0.37608230113983154
train gradient:  0.2103020878801305
iteration : 4760
train acc:  0.8203125
train loss:  0.4140605330467224
train gradient:  0.3224390154902345
iteration : 4761
train acc:  0.8046875
train loss:  0.4074496030807495
train gradient:  0.2301818063610968
iteration : 4762
train acc:  0.796875
train loss:  0.3826596140861511
train gradient:  0.23960774902519533
iteration : 4763
train acc:  0.875
train loss:  0.2996545433998108
train gradient:  0.16971035791853417
iteration : 4764
train acc:  0.8828125
train loss:  0.3345094919204712
train gradient:  0.19348211554629718
iteration : 4765
train acc:  0.8359375
train loss:  0.37128373980522156
train gradient:  0.219318504028642
iteration : 4766
train acc:  0.7734375
train loss:  0.40467965602874756
train gradient:  0.29204877156818676
iteration : 4767
train acc:  0.8515625
train loss:  0.3491363227367401
train gradient:  0.3196257454174401
iteration : 4768
train acc:  0.90625
train loss:  0.2456480711698532
train gradient:  0.14769379681282274
iteration : 4769
train acc:  0.8359375
train loss:  0.3590133786201477
train gradient:  0.21832888901076858
iteration : 4770
train acc:  0.8828125
train loss:  0.3034910559654236
train gradient:  0.18500201406752803
iteration : 4771
train acc:  0.84375
train loss:  0.35903578996658325
train gradient:  0.316349827980752
iteration : 4772
train acc:  0.8984375
train loss:  0.2605574131011963
train gradient:  0.13799514071191182
iteration : 4773
train acc:  0.90625
train loss:  0.25314587354660034
train gradient:  0.11002498553882155
iteration : 4774
train acc:  0.84375
train loss:  0.4255302846431732
train gradient:  0.4498262476871404
iteration : 4775
train acc:  0.8828125
train loss:  0.32686030864715576
train gradient:  0.25532008766059794
iteration : 4776
train acc:  0.8046875
train loss:  0.3923780918121338
train gradient:  0.24844374216949242
iteration : 4777
train acc:  0.8515625
train loss:  0.3553888201713562
train gradient:  0.27845646393376333
iteration : 4778
train acc:  0.8515625
train loss:  0.36166316270828247
train gradient:  0.2520393613183018
iteration : 4779
train acc:  0.8125
train loss:  0.4066639542579651
train gradient:  0.2808774304998598
iteration : 4780
train acc:  0.78125
train loss:  0.4369787871837616
train gradient:  0.3377249704680692
iteration : 4781
train acc:  0.796875
train loss:  0.4092857241630554
train gradient:  0.25962536741112496
iteration : 4782
train acc:  0.8515625
train loss:  0.3329399824142456
train gradient:  0.21667778582901365
iteration : 4783
train acc:  0.875
train loss:  0.28770726919174194
train gradient:  0.2210976976155381
iteration : 4784
train acc:  0.828125
train loss:  0.3282688856124878
train gradient:  0.19838404214114053
iteration : 4785
train acc:  0.8359375
train loss:  0.37006890773773193
train gradient:  0.2391645141588408
iteration : 4786
train acc:  0.8359375
train loss:  0.3573437035083771
train gradient:  0.2930257658239545
iteration : 4787
train acc:  0.8203125
train loss:  0.39378654956817627
train gradient:  0.30385145332206986
iteration : 4788
train acc:  0.84375
train loss:  0.394738107919693
train gradient:  0.26160671885860864
iteration : 4789
train acc:  0.828125
train loss:  0.38746902346611023
train gradient:  0.3040181893934584
iteration : 4790
train acc:  0.8515625
train loss:  0.36292558908462524
train gradient:  0.24345437247006713
iteration : 4791
train acc:  0.8671875
train loss:  0.3156282603740692
train gradient:  0.24699141480841408
iteration : 4792
train acc:  0.7421875
train loss:  0.5107953548431396
train gradient:  0.4955631951427616
iteration : 4793
train acc:  0.8359375
train loss:  0.3537290096282959
train gradient:  0.2844236302938922
iteration : 4794
train acc:  0.8203125
train loss:  0.3746000826358795
train gradient:  0.38363281639512065
iteration : 4795
train acc:  0.8046875
train loss:  0.38077765703201294
train gradient:  0.2856241255551319
iteration : 4796
train acc:  0.8203125
train loss:  0.3980102837085724
train gradient:  0.3455825729591726
iteration : 4797
train acc:  0.8125
train loss:  0.3795154094696045
train gradient:  0.22581904954304693
iteration : 4798
train acc:  0.8828125
train loss:  0.30108100175857544
train gradient:  0.29615940165288696
iteration : 4799
train acc:  0.796875
train loss:  0.46222054958343506
train gradient:  0.3046471526963175
iteration : 4800
train acc:  0.875
train loss:  0.3121807873249054
train gradient:  0.21995957989831952
iteration : 4801
train acc:  0.8359375
train loss:  0.3452832102775574
train gradient:  0.32221793220080686
iteration : 4802
train acc:  0.8671875
train loss:  0.34742796421051025
train gradient:  0.18289657891475228
iteration : 4803
train acc:  0.8046875
train loss:  0.339063823223114
train gradient:  0.1905826898984055
iteration : 4804
train acc:  0.796875
train loss:  0.39997345209121704
train gradient:  0.3031067223734743
iteration : 4805
train acc:  0.78125
train loss:  0.4125269055366516
train gradient:  0.3760831217433474
iteration : 4806
train acc:  0.875
train loss:  0.3329266309738159
train gradient:  0.18331782261680832
iteration : 4807
train acc:  0.8359375
train loss:  0.3562389016151428
train gradient:  0.20879070584133672
iteration : 4808
train acc:  0.8046875
train loss:  0.3678171932697296
train gradient:  0.2263579758615883
iteration : 4809
train acc:  0.8359375
train loss:  0.297529399394989
train gradient:  0.17792253646797404
iteration : 4810
train acc:  0.8671875
train loss:  0.34198129177093506
train gradient:  0.20596973992894813
iteration : 4811
train acc:  0.8671875
train loss:  0.34791314601898193
train gradient:  0.24644275951223638
iteration : 4812
train acc:  0.8203125
train loss:  0.4015159606933594
train gradient:  0.29687270089761786
iteration : 4813
train acc:  0.8359375
train loss:  0.4169692099094391
train gradient:  0.3449005330584929
iteration : 4814
train acc:  0.7890625
train loss:  0.3807521164417267
train gradient:  0.30807495733468104
iteration : 4815
train acc:  0.828125
train loss:  0.37806084752082825
train gradient:  0.276249069501851
iteration : 4816
train acc:  0.828125
train loss:  0.3225080072879791
train gradient:  0.2024602901340869
iteration : 4817
train acc:  0.859375
train loss:  0.359014630317688
train gradient:  0.17012576489560255
iteration : 4818
train acc:  0.8125
train loss:  0.3990863561630249
train gradient:  0.2952160674536459
iteration : 4819
train acc:  0.8671875
train loss:  0.3305617570877075
train gradient:  0.19834988533506576
iteration : 4820
train acc:  0.8515625
train loss:  0.33965083956718445
train gradient:  0.1737740076914865
iteration : 4821
train acc:  0.8515625
train loss:  0.32511645555496216
train gradient:  0.20089392079078283
iteration : 4822
train acc:  0.796875
train loss:  0.3706319332122803
train gradient:  0.3023946143353994
iteration : 4823
train acc:  0.828125
train loss:  0.35745540261268616
train gradient:  0.17607593300701474
iteration : 4824
train acc:  0.859375
train loss:  0.34786051511764526
train gradient:  0.1873221311310379
iteration : 4825
train acc:  0.8359375
train loss:  0.40951278805732727
train gradient:  0.4655285637332427
iteration : 4826
train acc:  0.8359375
train loss:  0.40588611364364624
train gradient:  0.21118984989840983
iteration : 4827
train acc:  0.8671875
train loss:  0.29904770851135254
train gradient:  0.13255933432549222
iteration : 4828
train acc:  0.828125
train loss:  0.35312020778656006
train gradient:  0.263826996454985
iteration : 4829
train acc:  0.828125
train loss:  0.35082143545150757
train gradient:  0.2068937105044185
iteration : 4830
train acc:  0.8515625
train loss:  0.31319862604141235
train gradient:  0.20324854477667062
iteration : 4831
train acc:  0.859375
train loss:  0.3678079843521118
train gradient:  0.20479672134024612
iteration : 4832
train acc:  0.8359375
train loss:  0.3875342011451721
train gradient:  0.2794852077014807
iteration : 4833
train acc:  0.8125
train loss:  0.4262154698371887
train gradient:  0.35135729555316336
iteration : 4834
train acc:  0.859375
train loss:  0.2729071080684662
train gradient:  0.1465387290424822
iteration : 4835
train acc:  0.859375
train loss:  0.3320245146751404
train gradient:  0.21034465456071538
iteration : 4836
train acc:  0.8671875
train loss:  0.32281923294067383
train gradient:  0.22878494675093278
iteration : 4837
train acc:  0.859375
train loss:  0.3349613845348358
train gradient:  0.327899762750894
iteration : 4838
train acc:  0.859375
train loss:  0.3361443877220154
train gradient:  0.3065065839159054
iteration : 4839
train acc:  0.8515625
train loss:  0.34371519088745117
train gradient:  0.26475716369952773
iteration : 4840
train acc:  0.8515625
train loss:  0.35112231969833374
train gradient:  0.2035783228178242
iteration : 4841
train acc:  0.84375
train loss:  0.4101787209510803
train gradient:  0.2879249083962704
iteration : 4842
train acc:  0.84375
train loss:  0.35603052377700806
train gradient:  0.21997980670068734
iteration : 4843
train acc:  0.828125
train loss:  0.3637661933898926
train gradient:  0.18815097815548062
iteration : 4844
train acc:  0.8203125
train loss:  0.4188827872276306
train gradient:  0.27171361172629616
iteration : 4845
train acc:  0.8515625
train loss:  0.3338475227355957
train gradient:  0.19912250796564623
iteration : 4846
train acc:  0.828125
train loss:  0.3943132758140564
train gradient:  0.25352992298899985
iteration : 4847
train acc:  0.8671875
train loss:  0.3760876953601837
train gradient:  0.28433250612636757
iteration : 4848
train acc:  0.765625
train loss:  0.4132935702800751
train gradient:  0.34244577630349515
iteration : 4849
train acc:  0.7890625
train loss:  0.46229079365730286
train gradient:  0.45171891175429457
iteration : 4850
train acc:  0.859375
train loss:  0.2947169542312622
train gradient:  0.16900839179738364
iteration : 4851
train acc:  0.8359375
train loss:  0.363323450088501
train gradient:  0.267604035217859
iteration : 4852
train acc:  0.8203125
train loss:  0.40364986658096313
train gradient:  0.25177737626569996
iteration : 4853
train acc:  0.84375
train loss:  0.35209137201309204
train gradient:  0.24604501802312695
iteration : 4854
train acc:  0.828125
train loss:  0.4688194990158081
train gradient:  0.35259596718739866
iteration : 4855
train acc:  0.890625
train loss:  0.24528111517429352
train gradient:  0.14732180268993975
iteration : 4856
train acc:  0.8046875
train loss:  0.4162542521953583
train gradient:  0.30705104785915777
iteration : 4857
train acc:  0.8515625
train loss:  0.32176458835601807
train gradient:  0.22821030322215488
iteration : 4858
train acc:  0.8515625
train loss:  0.3539609909057617
train gradient:  0.24184468105596257
iteration : 4859
train acc:  0.8515625
train loss:  0.3504253625869751
train gradient:  0.1934131561892365
iteration : 4860
train acc:  0.8125
train loss:  0.3656727075576782
train gradient:  0.3120382020549421
iteration : 4861
train acc:  0.8203125
train loss:  0.38046371936798096
train gradient:  0.19645481820600078
iteration : 4862
train acc:  0.8203125
train loss:  0.3868159353733063
train gradient:  0.2877547499405309
iteration : 4863
train acc:  0.8359375
train loss:  0.3848351836204529
train gradient:  0.18591390631849913
iteration : 4864
train acc:  0.8046875
train loss:  0.41523855924606323
train gradient:  0.28038409604716535
iteration : 4865
train acc:  0.875
train loss:  0.33135998249053955
train gradient:  0.20909412676212732
iteration : 4866
train acc:  0.8828125
train loss:  0.29555606842041016
train gradient:  0.1484248027766531
iteration : 4867
train acc:  0.8046875
train loss:  0.3745267987251282
train gradient:  0.24029113461972706
iteration : 4868
train acc:  0.8828125
train loss:  0.3301171064376831
train gradient:  0.16420528397706954
iteration : 4869
train acc:  0.8046875
train loss:  0.4343854486942291
train gradient:  0.37012217329048386
iteration : 4870
train acc:  0.8828125
train loss:  0.2857394814491272
train gradient:  0.17273066902062983
iteration : 4871
train acc:  0.859375
train loss:  0.2635723650455475
train gradient:  0.16473053717994368
iteration : 4872
train acc:  0.8203125
train loss:  0.3624146580696106
train gradient:  0.2790672294154725
iteration : 4873
train acc:  0.7734375
train loss:  0.3856624364852905
train gradient:  0.3563716074473221
iteration : 4874
train acc:  0.8046875
train loss:  0.36754345893859863
train gradient:  0.21328149409340783
iteration : 4875
train acc:  0.796875
train loss:  0.4191019535064697
train gradient:  0.35413634982530723
iteration : 4876
train acc:  0.7890625
train loss:  0.3501870036125183
train gradient:  0.25406940775324477
iteration : 4877
train acc:  0.8359375
train loss:  0.4233461618423462
train gradient:  0.23702392137325923
iteration : 4878
train acc:  0.84375
train loss:  0.31345272064208984
train gradient:  0.227773135750451
iteration : 4879
train acc:  0.8359375
train loss:  0.3494393825531006
train gradient:  0.222666766099124
iteration : 4880
train acc:  0.8515625
train loss:  0.329306423664093
train gradient:  0.1987672873530467
iteration : 4881
train acc:  0.8359375
train loss:  0.3003659248352051
train gradient:  0.17642685269295222
iteration : 4882
train acc:  0.84375
train loss:  0.3534402847290039
train gradient:  0.25649060025789117
iteration : 4883
train acc:  0.859375
train loss:  0.3019147217273712
train gradient:  0.19038163462202817
iteration : 4884
train acc:  0.8359375
train loss:  0.3220241665840149
train gradient:  0.2343443163335731
iteration : 4885
train acc:  0.890625
train loss:  0.28502386808395386
train gradient:  0.15186394623233668
iteration : 4886
train acc:  0.8046875
train loss:  0.3946041464805603
train gradient:  0.23209852142767512
iteration : 4887
train acc:  0.828125
train loss:  0.3708001673221588
train gradient:  0.2917109539518125
iteration : 4888
train acc:  0.828125
train loss:  0.3593018054962158
train gradient:  0.25404772841241546
iteration : 4889
train acc:  0.8046875
train loss:  0.40685737133026123
train gradient:  0.24174569263801085
iteration : 4890
train acc:  0.8125
train loss:  0.3865097165107727
train gradient:  0.4252740889046016
iteration : 4891
train acc:  0.84375
train loss:  0.37886354327201843
train gradient:  0.3291393848422153
iteration : 4892
train acc:  0.8828125
train loss:  0.3362618088722229
train gradient:  0.24870204533399687
iteration : 4893
train acc:  0.8515625
train loss:  0.3392711579799652
train gradient:  0.2694606987238235
iteration : 4894
train acc:  0.75
train loss:  0.5517340898513794
train gradient:  0.5262372701861944
iteration : 4895
train acc:  0.8671875
train loss:  0.30541926622390747
train gradient:  0.17195836593471464
iteration : 4896
train acc:  0.8515625
train loss:  0.3661801218986511
train gradient:  0.3938187106889617
iteration : 4897
train acc:  0.859375
train loss:  0.32674553990364075
train gradient:  0.20131483601193936
iteration : 4898
train acc:  0.8671875
train loss:  0.34626975655555725
train gradient:  0.2771360652999775
iteration : 4899
train acc:  0.75
train loss:  0.47792279720306396
train gradient:  0.29441907091253655
iteration : 4900
train acc:  0.8515625
train loss:  0.3250628113746643
train gradient:  0.19881892705278298
iteration : 4901
train acc:  0.828125
train loss:  0.3849029541015625
train gradient:  0.3031742404702614
iteration : 4902
train acc:  0.828125
train loss:  0.43001657724380493
train gradient:  0.4032280993835881
iteration : 4903
train acc:  0.8515625
train loss:  0.32765042781829834
train gradient:  0.21794244407207092
iteration : 4904
train acc:  0.8828125
train loss:  0.3380511403083801
train gradient:  0.19322163464093192
iteration : 4905
train acc:  0.7734375
train loss:  0.441151887178421
train gradient:  0.30824418987619406
iteration : 4906
train acc:  0.8984375
train loss:  0.2893020808696747
train gradient:  0.18868283106462264
iteration : 4907
train acc:  0.90625
train loss:  0.27483755350112915
train gradient:  0.14229930090648887
iteration : 4908
train acc:  0.828125
train loss:  0.37720322608947754
train gradient:  0.25636436713412974
iteration : 4909
train acc:  0.796875
train loss:  0.3817102909088135
train gradient:  0.31050673320133404
iteration : 4910
train acc:  0.8828125
train loss:  0.2749401926994324
train gradient:  0.1290707670148062
iteration : 4911
train acc:  0.7890625
train loss:  0.41549786925315857
train gradient:  0.34649232474840586
iteration : 4912
train acc:  0.828125
train loss:  0.37089359760284424
train gradient:  0.29458039922047086
iteration : 4913
train acc:  0.765625
train loss:  0.4320736825466156
train gradient:  0.33551554768155417
iteration : 4914
train acc:  0.8515625
train loss:  0.31948965787887573
train gradient:  0.23091754661623765
iteration : 4915
train acc:  0.8515625
train loss:  0.3018626868724823
train gradient:  0.1605669647460253
iteration : 4916
train acc:  0.9140625
train loss:  0.26933547854423523
train gradient:  0.14357164890334867
iteration : 4917
train acc:  0.7890625
train loss:  0.4294881820678711
train gradient:  0.3265076385885906
iteration : 4918
train acc:  0.78125
train loss:  0.4465850591659546
train gradient:  0.3695972003996795
iteration : 4919
train acc:  0.8828125
train loss:  0.29444625973701477
train gradient:  0.1572790713091672
iteration : 4920
train acc:  0.84375
train loss:  0.3707127869129181
train gradient:  0.25145550450742393
iteration : 4921
train acc:  0.828125
train loss:  0.41099095344543457
train gradient:  0.4593502250360901
iteration : 4922
train acc:  0.828125
train loss:  0.3885667026042938
train gradient:  0.3904705756491958
iteration : 4923
train acc:  0.890625
train loss:  0.28394830226898193
train gradient:  0.20617328374152155
iteration : 4924
train acc:  0.84375
train loss:  0.35479408502578735
train gradient:  0.1912279398301497
iteration : 4925
train acc:  0.8515625
train loss:  0.3778199553489685
train gradient:  0.23254988577007746
iteration : 4926
train acc:  0.828125
train loss:  0.4247937798500061
train gradient:  0.29310859717633325
iteration : 4927
train acc:  0.859375
train loss:  0.3023868203163147
train gradient:  0.2477239349152736
iteration : 4928
train acc:  0.875
train loss:  0.34234142303466797
train gradient:  0.23522014758505858
iteration : 4929
train acc:  0.8359375
train loss:  0.33803263306617737
train gradient:  0.3578949386847979
iteration : 4930
train acc:  0.8515625
train loss:  0.33352527022361755
train gradient:  0.2190444942062101
iteration : 4931
train acc:  0.8046875
train loss:  0.41640233993530273
train gradient:  0.32872959236883736
iteration : 4932
train acc:  0.859375
train loss:  0.34889522194862366
train gradient:  0.1905664216209482
iteration : 4933
train acc:  0.84375
train loss:  0.35387229919433594
train gradient:  0.3384589920713907
iteration : 4934
train acc:  0.875
train loss:  0.34551048278808594
train gradient:  0.2110366674230708
iteration : 4935
train acc:  0.8671875
train loss:  0.3036917746067047
train gradient:  0.18773813712342058
iteration : 4936
train acc:  0.84375
train loss:  0.37922203540802
train gradient:  0.38511580679283913
iteration : 4937
train acc:  0.8828125
train loss:  0.3116512894630432
train gradient:  0.22027132402797028
iteration : 4938
train acc:  0.78125
train loss:  0.44911015033721924
train gradient:  0.3532570720036118
iteration : 4939
train acc:  0.8203125
train loss:  0.38035082817077637
train gradient:  0.2037681788503489
iteration : 4940
train acc:  0.84375
train loss:  0.3485381007194519
train gradient:  0.23660382389756476
iteration : 4941
train acc:  0.8359375
train loss:  0.3528944253921509
train gradient:  0.24914602244330625
iteration : 4942
train acc:  0.828125
train loss:  0.3650616407394409
train gradient:  0.23172731401971555
iteration : 4943
train acc:  0.8671875
train loss:  0.30378401279449463
train gradient:  0.15641010273269684
iteration : 4944
train acc:  0.890625
train loss:  0.3449791669845581
train gradient:  0.25289446309566166
iteration : 4945
train acc:  0.796875
train loss:  0.3941100835800171
train gradient:  0.2655405785154331
iteration : 4946
train acc:  0.8671875
train loss:  0.3727593421936035
train gradient:  0.293476554504861
iteration : 4947
train acc:  0.8515625
train loss:  0.33843743801116943
train gradient:  0.24876054911986822
iteration : 4948
train acc:  0.8203125
train loss:  0.35001030564308167
train gradient:  0.21925460539558622
iteration : 4949
train acc:  0.84375
train loss:  0.33140045404434204
train gradient:  0.24846050615279577
iteration : 4950
train acc:  0.859375
train loss:  0.2836173474788666
train gradient:  0.16609603300405873
iteration : 4951
train acc:  0.828125
train loss:  0.2994651198387146
train gradient:  0.2455517047835065
iteration : 4952
train acc:  0.84375
train loss:  0.36224162578582764
train gradient:  0.23105277896530396
iteration : 4953
train acc:  0.90625
train loss:  0.2323581874370575
train gradient:  0.12771921864419705
iteration : 4954
train acc:  0.8828125
train loss:  0.26286423206329346
train gradient:  0.21645433399771452
iteration : 4955
train acc:  0.8125
train loss:  0.38404586911201477
train gradient:  0.3364650539194497
iteration : 4956
train acc:  0.8359375
train loss:  0.3593340516090393
train gradient:  0.2913427123538683
iteration : 4957
train acc:  0.9140625
train loss:  0.28862860798835754
train gradient:  0.173229827202068
iteration : 4958
train acc:  0.84375
train loss:  0.3612819314002991
train gradient:  0.19177525181173743
iteration : 4959
train acc:  0.859375
train loss:  0.31105026602745056
train gradient:  0.17667025919694415
iteration : 4960
train acc:  0.8125
train loss:  0.369792103767395
train gradient:  0.22324790202847083
iteration : 4961
train acc:  0.8125
train loss:  0.37678876519203186
train gradient:  0.3091922128956344
iteration : 4962
train acc:  0.8359375
train loss:  0.3634476661682129
train gradient:  0.21860979577716363
iteration : 4963
train acc:  0.8828125
train loss:  0.2906869053840637
train gradient:  0.16036968138152863
iteration : 4964
train acc:  0.828125
train loss:  0.3673994541168213
train gradient:  0.3160118143882144
iteration : 4965
train acc:  0.875
train loss:  0.30406618118286133
train gradient:  0.17996760391430547
iteration : 4966
train acc:  0.8359375
train loss:  0.34584924578666687
train gradient:  0.25435893672682475
iteration : 4967
train acc:  0.796875
train loss:  0.47746989130973816
train gradient:  0.3694057463111048
iteration : 4968
train acc:  0.8828125
train loss:  0.30170106887817383
train gradient:  0.15081374664249736
iteration : 4969
train acc:  0.84375
train loss:  0.3379656672477722
train gradient:  0.29372142979075344
iteration : 4970
train acc:  0.859375
train loss:  0.36006462574005127
train gradient:  0.24910147707957303
iteration : 4971
train acc:  0.875
train loss:  0.29927700757980347
train gradient:  0.18335002501182007
iteration : 4972
train acc:  0.859375
train loss:  0.2989444136619568
train gradient:  0.18504178420257153
iteration : 4973
train acc:  0.8046875
train loss:  0.4269247055053711
train gradient:  0.27778250803521676
iteration : 4974
train acc:  0.8359375
train loss:  0.35305458307266235
train gradient:  0.2303084568538146
iteration : 4975
train acc:  0.7890625
train loss:  0.40024977922439575
train gradient:  0.3062847502988045
iteration : 4976
train acc:  0.828125
train loss:  0.4317496418952942
train gradient:  0.37509470814791834
iteration : 4977
train acc:  0.875
train loss:  0.28620192408561707
train gradient:  0.19531431741193273
iteration : 4978
train acc:  0.828125
train loss:  0.4011021852493286
train gradient:  0.3236408427476241
iteration : 4979
train acc:  0.84375
train loss:  0.3252783715724945
train gradient:  0.24542622641198886
iteration : 4980
train acc:  0.8671875
train loss:  0.3504787087440491
train gradient:  0.2588233019438001
iteration : 4981
train acc:  0.765625
train loss:  0.5306578874588013
train gradient:  0.47258489137821746
iteration : 4982
train acc:  0.84375
train loss:  0.36447328329086304
train gradient:  0.26050752166453617
iteration : 4983
train acc:  0.828125
train loss:  0.3253648281097412
train gradient:  0.21423392967122581
iteration : 4984
train acc:  0.8359375
train loss:  0.42919373512268066
train gradient:  0.2606069729535027
iteration : 4985
train acc:  0.859375
train loss:  0.38872379064559937
train gradient:  0.20837990824910088
iteration : 4986
train acc:  0.8125
train loss:  0.44553178548812866
train gradient:  0.328114528865935
iteration : 4987
train acc:  0.84375
train loss:  0.311554878950119
train gradient:  0.29653563487215395
iteration : 4988
train acc:  0.875
train loss:  0.3144074082374573
train gradient:  0.21936355194937618
iteration : 4989
train acc:  0.765625
train loss:  0.4521900415420532
train gradient:  0.44732306748900347
iteration : 4990
train acc:  0.8359375
train loss:  0.3780238926410675
train gradient:  0.29651553080258936
iteration : 4991
train acc:  0.8359375
train loss:  0.40023255348205566
train gradient:  0.33123150965449205
iteration : 4992
train acc:  0.8125
train loss:  0.4027133285999298
train gradient:  0.27350987614659356
iteration : 4993
train acc:  0.84375
train loss:  0.41275644302368164
train gradient:  0.3254344560210385
iteration : 4994
train acc:  0.8125
train loss:  0.422252357006073
train gradient:  0.32494650935181923
iteration : 4995
train acc:  0.8671875
train loss:  0.3477398455142975
train gradient:  0.19256790388355535
iteration : 4996
train acc:  0.8515625
train loss:  0.37313562631607056
train gradient:  0.23788068313374877
iteration : 4997
train acc:  0.8046875
train loss:  0.4211145043373108
train gradient:  0.3404106795528045
iteration : 4998
train acc:  0.875
train loss:  0.37388378381729126
train gradient:  0.19777890494912415
iteration : 4999
train acc:  0.875
train loss:  0.30289772152900696
train gradient:  0.20002178684617256
iteration : 5000
train acc:  0.7734375
train loss:  0.42665985226631165
train gradient:  0.26776893806176594
iteration : 5001
train acc:  0.8203125
train loss:  0.35890835523605347
train gradient:  0.2227551931934481
iteration : 5002
train acc:  0.8984375
train loss:  0.23273123800754547
train gradient:  0.1339908154785851
iteration : 5003
train acc:  0.7734375
train loss:  0.41985902190208435
train gradient:  0.3543495361208182
iteration : 5004
train acc:  0.84375
train loss:  0.33243751525878906
train gradient:  0.22587411627518356
iteration : 5005
train acc:  0.8125
train loss:  0.4170030355453491
train gradient:  0.27230689615285397
iteration : 5006
train acc:  0.796875
train loss:  0.4071579575538635
train gradient:  0.434799890707035
iteration : 5007
train acc:  0.828125
train loss:  0.3835195302963257
train gradient:  0.45122915944405423
iteration : 5008
train acc:  0.8359375
train loss:  0.41614535450935364
train gradient:  0.2829796454785225
iteration : 5009
train acc:  0.8515625
train loss:  0.3668769299983978
train gradient:  0.3433655375297171
iteration : 5010
train acc:  0.859375
train loss:  0.3284912705421448
train gradient:  0.20633597584770708
iteration : 5011
train acc:  0.875
train loss:  0.2970046401023865
train gradient:  0.14069395037031668
iteration : 5012
train acc:  0.859375
train loss:  0.4293241500854492
train gradient:  0.30669110660036913
iteration : 5013
train acc:  0.8125
train loss:  0.45315030217170715
train gradient:  0.37861326765828945
iteration : 5014
train acc:  0.8671875
train loss:  0.32257646322250366
train gradient:  0.14412006645237846
iteration : 5015
train acc:  0.828125
train loss:  0.34907352924346924
train gradient:  0.22569042234443099
iteration : 5016
train acc:  0.828125
train loss:  0.3508456349372864
train gradient:  0.17427167221096435
iteration : 5017
train acc:  0.84375
train loss:  0.32795780897140503
train gradient:  0.20122864130879506
iteration : 5018
train acc:  0.84375
train loss:  0.36860397458076477
train gradient:  0.2353735821778003
iteration : 5019
train acc:  0.828125
train loss:  0.37899285554885864
train gradient:  0.2451530831382662
iteration : 5020
train acc:  0.90625
train loss:  0.26620200276374817
train gradient:  0.11737178926659748
iteration : 5021
train acc:  0.84375
train loss:  0.4112343490123749
train gradient:  0.28018520587661966
iteration : 5022
train acc:  0.84375
train loss:  0.3883751630783081
train gradient:  0.2194456040208772
iteration : 5023
train acc:  0.8359375
train loss:  0.32613030076026917
train gradient:  0.21057204692048098
iteration : 5024
train acc:  0.8671875
train loss:  0.3247579336166382
train gradient:  0.21626832242918337
iteration : 5025
train acc:  0.8515625
train loss:  0.3553811013698578
train gradient:  0.20137765492121731
iteration : 5026
train acc:  0.8359375
train loss:  0.3584733307361603
train gradient:  0.2888740704890182
iteration : 5027
train acc:  0.8828125
train loss:  0.28867006301879883
train gradient:  0.14026326159907704
iteration : 5028
train acc:  0.8515625
train loss:  0.3376453220844269
train gradient:  0.27031955043928946
iteration : 5029
train acc:  0.8984375
train loss:  0.25234168767929077
train gradient:  0.14031545156691583
iteration : 5030
train acc:  0.8359375
train loss:  0.36967507004737854
train gradient:  0.29863361246868503
iteration : 5031
train acc:  0.8671875
train loss:  0.3158358037471771
train gradient:  0.1735994919103399
iteration : 5032
train acc:  0.8359375
train loss:  0.36021095514297485
train gradient:  0.22335303123590577
iteration : 5033
train acc:  0.8671875
train loss:  0.3435613512992859
train gradient:  0.1614501294684786
iteration : 5034
train acc:  0.828125
train loss:  0.3815668821334839
train gradient:  0.2548708232336029
iteration : 5035
train acc:  0.875
train loss:  0.36402732133865356
train gradient:  0.2043957414894348
iteration : 5036
train acc:  0.8046875
train loss:  0.37443095445632935
train gradient:  0.28488791102687044
iteration : 5037
train acc:  0.7890625
train loss:  0.36607906222343445
train gradient:  0.21838112407379467
iteration : 5038
train acc:  0.828125
train loss:  0.37471306324005127
train gradient:  0.25512542158663293
iteration : 5039
train acc:  0.7734375
train loss:  0.5309298634529114
train gradient:  0.4503406586202243
iteration : 5040
train acc:  0.8671875
train loss:  0.3144213557243347
train gradient:  0.22609026874796168
iteration : 5041
train acc:  0.8203125
train loss:  0.43345576524734497
train gradient:  0.2987988918350363
iteration : 5042
train acc:  0.8671875
train loss:  0.35757988691329956
train gradient:  0.20230760731075892
iteration : 5043
train acc:  0.859375
train loss:  0.3290643095970154
train gradient:  0.21082080656412333
iteration : 5044
train acc:  0.765625
train loss:  0.3771522641181946
train gradient:  0.2483998444746314
iteration : 5045
train acc:  0.84375
train loss:  0.3450901508331299
train gradient:  0.2529427898051454
iteration : 5046
train acc:  0.8828125
train loss:  0.31619542837142944
train gradient:  0.1733022527426714
iteration : 5047
train acc:  0.8515625
train loss:  0.411945641040802
train gradient:  0.3599128768491559
iteration : 5048
train acc:  0.828125
train loss:  0.340999037027359
train gradient:  0.21229573863045798
iteration : 5049
train acc:  0.8671875
train loss:  0.27155548334121704
train gradient:  0.18325198990540914
iteration : 5050
train acc:  0.8671875
train loss:  0.3648143410682678
train gradient:  0.29659450395647846
iteration : 5051
train acc:  0.8046875
train loss:  0.34790554642677307
train gradient:  0.28912978004119133
iteration : 5052
train acc:  0.7890625
train loss:  0.4393869638442993
train gradient:  0.2972257261170318
iteration : 5053
train acc:  0.8515625
train loss:  0.3542167842388153
train gradient:  0.2735661225770989
iteration : 5054
train acc:  0.8359375
train loss:  0.3514936566352844
train gradient:  0.20402109211879557
iteration : 5055
train acc:  0.7890625
train loss:  0.37956923246383667
train gradient:  0.22829042066284805
iteration : 5056
train acc:  0.8046875
train loss:  0.38863107562065125
train gradient:  0.3282678093891601
iteration : 5057
train acc:  0.8671875
train loss:  0.30706602334976196
train gradient:  0.21256516689825924
iteration : 5058
train acc:  0.859375
train loss:  0.36945411562919617
train gradient:  0.28610317974055344
iteration : 5059
train acc:  0.8515625
train loss:  0.3826352059841156
train gradient:  0.22415851954061672
iteration : 5060
train acc:  0.8671875
train loss:  0.29891330003738403
train gradient:  0.23820314549996469
iteration : 5061
train acc:  0.7890625
train loss:  0.4100850820541382
train gradient:  0.2910280159492338
iteration : 5062
train acc:  0.84375
train loss:  0.40242305397987366
train gradient:  0.2238670578771726
iteration : 5063
train acc:  0.859375
train loss:  0.3456364572048187
train gradient:  0.25808135006129335
iteration : 5064
train acc:  0.8671875
train loss:  0.305614709854126
train gradient:  0.22304700246983275
iteration : 5065
train acc:  0.8203125
train loss:  0.3560636043548584
train gradient:  0.2773666602208418
iteration : 5066
train acc:  0.828125
train loss:  0.37657028436660767
train gradient:  0.2413848565646725
iteration : 5067
train acc:  0.8671875
train loss:  0.3164435923099518
train gradient:  0.24757390854166988
iteration : 5068
train acc:  0.828125
train loss:  0.35457736253738403
train gradient:  0.29199797071465533
iteration : 5069
train acc:  0.890625
train loss:  0.2605535387992859
train gradient:  0.17142029526265046
iteration : 5070
train acc:  0.8671875
train loss:  0.2919310927391052
train gradient:  0.189255144955581
iteration : 5071
train acc:  0.828125
train loss:  0.38324326276779175
train gradient:  0.295470116650528
iteration : 5072
train acc:  0.8515625
train loss:  0.33151474595069885
train gradient:  0.1915867401239136
iteration : 5073
train acc:  0.8515625
train loss:  0.401834636926651
train gradient:  0.2803264599884231
iteration : 5074
train acc:  0.875
train loss:  0.33313900232315063
train gradient:  0.18173114983827948
iteration : 5075
train acc:  0.875
train loss:  0.31075483560562134
train gradient:  0.18881308655411683
iteration : 5076
train acc:  0.8359375
train loss:  0.3450022339820862
train gradient:  0.22730596171060297
iteration : 5077
train acc:  0.875
train loss:  0.32233893871307373
train gradient:  0.15455594965569608
iteration : 5078
train acc:  0.78125
train loss:  0.36421483755111694
train gradient:  0.2732694914112957
iteration : 5079
train acc:  0.84375
train loss:  0.3549083471298218
train gradient:  0.22729120195275956
iteration : 5080
train acc:  0.828125
train loss:  0.379573792219162
train gradient:  0.38468034041545085
iteration : 5081
train acc:  0.8203125
train loss:  0.42026445269584656
train gradient:  0.4053368312086793
iteration : 5082
train acc:  0.796875
train loss:  0.41760504245758057
train gradient:  0.31408077124187905
iteration : 5083
train acc:  0.859375
train loss:  0.3586711287498474
train gradient:  0.2280425234409386
iteration : 5084
train acc:  0.84375
train loss:  0.3981609344482422
train gradient:  0.2691323954107026
iteration : 5085
train acc:  0.8125
train loss:  0.3919956088066101
train gradient:  0.2981631951740939
iteration : 5086
train acc:  0.84375
train loss:  0.3075626492500305
train gradient:  0.1326871831613317
iteration : 5087
train acc:  0.8828125
train loss:  0.31504201889038086
train gradient:  0.15448485689251468
iteration : 5088
train acc:  0.796875
train loss:  0.4426302909851074
train gradient:  0.314393228270416
iteration : 5089
train acc:  0.8359375
train loss:  0.2974795699119568
train gradient:  0.22376776464449238
iteration : 5090
train acc:  0.8203125
train loss:  0.336216002702713
train gradient:  0.1679705084763597
iteration : 5091
train acc:  0.8515625
train loss:  0.3444182872772217
train gradient:  0.2828913256656833
iteration : 5092
train acc:  0.859375
train loss:  0.33227139711380005
train gradient:  0.24791051915746048
iteration : 5093
train acc:  0.8984375
train loss:  0.32223138213157654
train gradient:  0.2511727519244948
iteration : 5094
train acc:  0.796875
train loss:  0.44128894805908203
train gradient:  0.37376558083149747
iteration : 5095
train acc:  0.875
train loss:  0.3255481421947479
train gradient:  0.18745458764914608
iteration : 5096
train acc:  0.8515625
train loss:  0.33062922954559326
train gradient:  0.2068794550716886
iteration : 5097
train acc:  0.8203125
train loss:  0.40157216787338257
train gradient:  0.3445938751800872
iteration : 5098
train acc:  0.8671875
train loss:  0.33579379320144653
train gradient:  0.18841384595124885
iteration : 5099
train acc:  0.875
train loss:  0.32961755990982056
train gradient:  0.23421214572635116
iteration : 5100
train acc:  0.8359375
train loss:  0.3480403423309326
train gradient:  0.23542880311577624
iteration : 5101
train acc:  0.8671875
train loss:  0.354684054851532
train gradient:  0.27005287390822685
iteration : 5102
train acc:  0.84375
train loss:  0.40527185797691345
train gradient:  0.24928206857080515
iteration : 5103
train acc:  0.84375
train loss:  0.3167138695716858
train gradient:  0.16388970736734584
iteration : 5104
train acc:  0.8515625
train loss:  0.32717597484588623
train gradient:  0.23402063932772377
iteration : 5105
train acc:  0.8359375
train loss:  0.3622487783432007
train gradient:  0.2349410588740746
iteration : 5106
train acc:  0.7734375
train loss:  0.43768274784088135
train gradient:  0.2584053650153882
iteration : 5107
train acc:  0.859375
train loss:  0.3409734070301056
train gradient:  0.24090102270829458
iteration : 5108
train acc:  0.859375
train loss:  0.3121979236602783
train gradient:  0.2091022511866393
iteration : 5109
train acc:  0.8515625
train loss:  0.3412005305290222
train gradient:  0.310929250208607
iteration : 5110
train acc:  0.8125
train loss:  0.3860245943069458
train gradient:  0.2844245767246033
iteration : 5111
train acc:  0.890625
train loss:  0.33258146047592163
train gradient:  0.24473481070783717
iteration : 5112
train acc:  0.8828125
train loss:  0.2787221074104309
train gradient:  0.16850358230415718
iteration : 5113
train acc:  0.859375
train loss:  0.2975153923034668
train gradient:  0.16024332401073638
iteration : 5114
train acc:  0.8515625
train loss:  0.3433055579662323
train gradient:  0.24896523536174042
iteration : 5115
train acc:  0.8671875
train loss:  0.32480084896087646
train gradient:  0.18085531514762598
iteration : 5116
train acc:  0.8984375
train loss:  0.26769065856933594
train gradient:  0.1764384239920863
iteration : 5117
train acc:  0.84375
train loss:  0.3375650942325592
train gradient:  0.2936761517076447
iteration : 5118
train acc:  0.828125
train loss:  0.40937525033950806
train gradient:  0.39932479090486894
iteration : 5119
train acc:  0.8046875
train loss:  0.3477874994277954
train gradient:  0.27214451967384096
iteration : 5120
train acc:  0.8515625
train loss:  0.36364680528640747
train gradient:  0.4566105339550476
iteration : 5121
train acc:  0.8515625
train loss:  0.32888859510421753
train gradient:  0.2823474399285263
iteration : 5122
train acc:  0.796875
train loss:  0.4713134765625
train gradient:  0.43870912305165194
iteration : 5123
train acc:  0.859375
train loss:  0.324131041765213
train gradient:  0.23766311397043882
iteration : 5124
train acc:  0.8515625
train loss:  0.3149639070034027
train gradient:  0.201957937848235
iteration : 5125
train acc:  0.8203125
train loss:  0.38375771045684814
train gradient:  0.3337002376658466
iteration : 5126
train acc:  0.828125
train loss:  0.3440677523612976
train gradient:  0.25181185567015285
iteration : 5127
train acc:  0.8125
train loss:  0.3926788568496704
train gradient:  0.2673716726265176
iteration : 5128
train acc:  0.78125
train loss:  0.4540049433708191
train gradient:  0.4833743648300606
iteration : 5129
train acc:  0.8125
train loss:  0.37552541494369507
train gradient:  0.2734777044002458
iteration : 5130
train acc:  0.8203125
train loss:  0.33015626668930054
train gradient:  0.31964528313935
iteration : 5131
train acc:  0.828125
train loss:  0.3177042007446289
train gradient:  0.24189915457605116
iteration : 5132
train acc:  0.8671875
train loss:  0.36350828409194946
train gradient:  0.2534090771671187
iteration : 5133
train acc:  0.8828125
train loss:  0.33004599809646606
train gradient:  0.19568881950341158
iteration : 5134
train acc:  0.8515625
train loss:  0.31797298789024353
train gradient:  0.31870430977921094
iteration : 5135
train acc:  0.828125
train loss:  0.4327959716320038
train gradient:  0.3398861071626474
iteration : 5136
train acc:  0.859375
train loss:  0.3581375181674957
train gradient:  0.24885914128589293
iteration : 5137
train acc:  0.84375
train loss:  0.4108591675758362
train gradient:  0.33336654881059335
iteration : 5138
train acc:  0.8203125
train loss:  0.3636564016342163
train gradient:  0.36388824789304514
iteration : 5139
train acc:  0.7890625
train loss:  0.4872763156890869
train gradient:  0.5004162453917698
iteration : 5140
train acc:  0.8125
train loss:  0.41266995668411255
train gradient:  0.36917661612555014
iteration : 5141
train acc:  0.8515625
train loss:  0.3911309540271759
train gradient:  0.29946576757029425
iteration : 5142
train acc:  0.8125
train loss:  0.37998878955841064
train gradient:  0.22809557609065662
iteration : 5143
train acc:  0.859375
train loss:  0.3811887204647064
train gradient:  0.27961527477881565
iteration : 5144
train acc:  0.7890625
train loss:  0.40658295154571533
train gradient:  0.298899900492317
iteration : 5145
train acc:  0.8984375
train loss:  0.2846074402332306
train gradient:  0.20987130725649933
iteration : 5146
train acc:  0.875
train loss:  0.31589657068252563
train gradient:  0.15746935157760747
iteration : 5147
train acc:  0.78125
train loss:  0.40781834721565247
train gradient:  0.46466420749052195
iteration : 5148
train acc:  0.8828125
train loss:  0.29433515667915344
train gradient:  0.15399288941734773
iteration : 5149
train acc:  0.8046875
train loss:  0.42915642261505127
train gradient:  0.3567689980569879
iteration : 5150
train acc:  0.875
train loss:  0.2902563810348511
train gradient:  0.17203904949194132
iteration : 5151
train acc:  0.8046875
train loss:  0.4515744149684906
train gradient:  0.34042714992660633
iteration : 5152
train acc:  0.8515625
train loss:  0.32859110832214355
train gradient:  0.2484845783590015
iteration : 5153
train acc:  0.8984375
train loss:  0.30142441391944885
train gradient:  0.1374627873482507
iteration : 5154
train acc:  0.8359375
train loss:  0.3828071653842926
train gradient:  0.2999778498022491
iteration : 5155
train acc:  0.7578125
train loss:  0.44957125186920166
train gradient:  0.31171115763654533
iteration : 5156
train acc:  0.8359375
train loss:  0.38018798828125
train gradient:  0.232186289271327
iteration : 5157
train acc:  0.8046875
train loss:  0.4131067991256714
train gradient:  0.27717750011625686
iteration : 5158
train acc:  0.859375
train loss:  0.3247910737991333
train gradient:  0.16595259554288067
iteration : 5159
train acc:  0.875
train loss:  0.33160752058029175
train gradient:  0.15806161448301267
iteration : 5160
train acc:  0.8984375
train loss:  0.24874632060527802
train gradient:  0.11996541606102265
iteration : 5161
train acc:  0.8359375
train loss:  0.31371548771858215
train gradient:  0.1815305157450193
iteration : 5162
train acc:  0.875
train loss:  0.34308764338493347
train gradient:  0.21018519831320762
iteration : 5163
train acc:  0.890625
train loss:  0.2655789256095886
train gradient:  0.1576024313911156
iteration : 5164
train acc:  0.828125
train loss:  0.3486359715461731
train gradient:  0.21989035197947032
iteration : 5165
train acc:  0.8125
train loss:  0.3134939670562744
train gradient:  0.17709693409555347
iteration : 5166
train acc:  0.8515625
train loss:  0.30180519819259644
train gradient:  0.2466795532751933
iteration : 5167
train acc:  0.859375
train loss:  0.350702166557312
train gradient:  0.21037359632338565
iteration : 5168
train acc:  0.8671875
train loss:  0.31712520122528076
train gradient:  0.19873853952618442
iteration : 5169
train acc:  0.859375
train loss:  0.3792281746864319
train gradient:  0.255495021038034
iteration : 5170
train acc:  0.8515625
train loss:  0.3081267476081848
train gradient:  0.15035967149724697
iteration : 5171
train acc:  0.8828125
train loss:  0.28428030014038086
train gradient:  0.17276462211060956
iteration : 5172
train acc:  0.828125
train loss:  0.3654448986053467
train gradient:  0.1815464340413804
iteration : 5173
train acc:  0.796875
train loss:  0.39909982681274414
train gradient:  0.26609406662149865
iteration : 5174
train acc:  0.875
train loss:  0.3281385898590088
train gradient:  0.2609114287410935
iteration : 5175
train acc:  0.8671875
train loss:  0.34852248430252075
train gradient:  0.2529771828716714
iteration : 5176
train acc:  0.859375
train loss:  0.3219894766807556
train gradient:  0.266411621758723
iteration : 5177
train acc:  0.8203125
train loss:  0.36383140087127686
train gradient:  0.3961944427014042
iteration : 5178
train acc:  0.84375
train loss:  0.332904189825058
train gradient:  0.2589393453534074
iteration : 5179
train acc:  0.8671875
train loss:  0.29707375168800354
train gradient:  0.15727519858141192
iteration : 5180
train acc:  0.8046875
train loss:  0.40359053015708923
train gradient:  0.3879737364208777
iteration : 5181
train acc:  0.796875
train loss:  0.4099332094192505
train gradient:  0.27372267385293436
iteration : 5182
train acc:  0.8203125
train loss:  0.34933239221572876
train gradient:  0.2964194107801418
iteration : 5183
train acc:  0.828125
train loss:  0.3818473219871521
train gradient:  0.25128392793275656
iteration : 5184
train acc:  0.9140625
train loss:  0.26672112941741943
train gradient:  0.20043772927722592
iteration : 5185
train acc:  0.8671875
train loss:  0.31519222259521484
train gradient:  0.20453566342202617
iteration : 5186
train acc:  0.859375
train loss:  0.35798901319503784
train gradient:  0.25647462188275494
iteration : 5187
train acc:  0.84375
train loss:  0.344078928232193
train gradient:  0.27861572952583674
iteration : 5188
train acc:  0.859375
train loss:  0.3014981746673584
train gradient:  0.16678045609660658
iteration : 5189
train acc:  0.8515625
train loss:  0.33126091957092285
train gradient:  0.1847299203008109
iteration : 5190
train acc:  0.8125
train loss:  0.3994094133377075
train gradient:  0.2225229843048492
iteration : 5191
train acc:  0.7890625
train loss:  0.3767361044883728
train gradient:  0.43391438689318285
iteration : 5192
train acc:  0.90625
train loss:  0.24209818243980408
train gradient:  0.147786771595579
iteration : 5193
train acc:  0.828125
train loss:  0.3956161141395569
train gradient:  0.28579714235044396
iteration : 5194
train acc:  0.8515625
train loss:  0.34881290793418884
train gradient:  0.21396508147548599
iteration : 5195
train acc:  0.84375
train loss:  0.36039817333221436
train gradient:  0.19131822991004574
iteration : 5196
train acc:  0.90625
train loss:  0.26685699820518494
train gradient:  0.18745977947331727
iteration : 5197
train acc:  0.828125
train loss:  0.3773441016674042
train gradient:  0.23174149673188799
iteration : 5198
train acc:  0.8671875
train loss:  0.3432393968105316
train gradient:  0.2836939396436496
iteration : 5199
train acc:  0.828125
train loss:  0.31264662742614746
train gradient:  0.1601383377261939
iteration : 5200
train acc:  0.8828125
train loss:  0.2871442437171936
train gradient:  0.20302522300427261
iteration : 5201
train acc:  0.8125
train loss:  0.36711299419403076
train gradient:  0.41865991944909214
iteration : 5202
train acc:  0.859375
train loss:  0.31494516134262085
train gradient:  0.24632039461099534
iteration : 5203
train acc:  0.8671875
train loss:  0.2872035801410675
train gradient:  0.16805579337637466
iteration : 5204
train acc:  0.890625
train loss:  0.2897498607635498
train gradient:  0.21856340121202705
iteration : 5205
train acc:  0.8203125
train loss:  0.3830562233924866
train gradient:  0.1937455785069399
iteration : 5206
train acc:  0.875
train loss:  0.34060138463974
train gradient:  0.22769333441354625
iteration : 5207
train acc:  0.8359375
train loss:  0.36896228790283203
train gradient:  0.2264396970042041
iteration : 5208
train acc:  0.8046875
train loss:  0.4114367365837097
train gradient:  0.40842681742879516
iteration : 5209
train acc:  0.84375
train loss:  0.3613194227218628
train gradient:  0.2152823487123106
iteration : 5210
train acc:  0.8359375
train loss:  0.3489595055580139
train gradient:  0.25805947525109657
iteration : 5211
train acc:  0.859375
train loss:  0.3509981632232666
train gradient:  0.23119722084579342
iteration : 5212
train acc:  0.8125
train loss:  0.3711698353290558
train gradient:  0.26825568381362186
iteration : 5213
train acc:  0.8046875
train loss:  0.4781985878944397
train gradient:  0.5342989919835781
iteration : 5214
train acc:  0.84375
train loss:  0.36587798595428467
train gradient:  0.3308272577746375
iteration : 5215
train acc:  0.8515625
train loss:  0.2781515121459961
train gradient:  0.11499736376405242
iteration : 5216
train acc:  0.875
train loss:  0.30921605229377747
train gradient:  0.16482252921995444
iteration : 5217
train acc:  0.8046875
train loss:  0.47325044870376587
train gradient:  0.46321904784973217
iteration : 5218
train acc:  0.8671875
train loss:  0.28683820366859436
train gradient:  0.16992760646535932
iteration : 5219
train acc:  0.8671875
train loss:  0.2926443815231323
train gradient:  0.13999209461131673
iteration : 5220
train acc:  0.84375
train loss:  0.3813684284687042
train gradient:  0.1983418004014042
iteration : 5221
train acc:  0.8515625
train loss:  0.31111007928848267
train gradient:  0.18811789335597992
iteration : 5222
train acc:  0.8359375
train loss:  0.37066465616226196
train gradient:  0.265590880293544
iteration : 5223
train acc:  0.7890625
train loss:  0.4899601340293884
train gradient:  0.40426596450023494
iteration : 5224
train acc:  0.8046875
train loss:  0.48520544171333313
train gradient:  0.4362881861945417
iteration : 5225
train acc:  0.8359375
train loss:  0.39881086349487305
train gradient:  0.2809291073388804
iteration : 5226
train acc:  0.84375
train loss:  0.31843113899230957
train gradient:  0.21420641205155702
iteration : 5227
train acc:  0.8984375
train loss:  0.3057922124862671
train gradient:  0.17226251642911353
iteration : 5228
train acc:  0.8203125
train loss:  0.35975784063339233
train gradient:  0.2556718990699743
iteration : 5229
train acc:  0.84375
train loss:  0.38604071736335754
train gradient:  0.24501675505476397
iteration : 5230
train acc:  0.8125
train loss:  0.3958013653755188
train gradient:  0.23192706659591372
iteration : 5231
train acc:  0.890625
train loss:  0.2898198366165161
train gradient:  0.1760010873551241
iteration : 5232
train acc:  0.875
train loss:  0.30462032556533813
train gradient:  0.2089666039936638
iteration : 5233
train acc:  0.8359375
train loss:  0.4119509756565094
train gradient:  0.3016607739481168
iteration : 5234
train acc:  0.8125
train loss:  0.36510059237480164
train gradient:  0.22592902670301834
iteration : 5235
train acc:  0.8125
train loss:  0.4187473952770233
train gradient:  0.30342326602062364
iteration : 5236
train acc:  0.8125
train loss:  0.4287741482257843
train gradient:  0.30972487004694077
iteration : 5237
train acc:  0.7734375
train loss:  0.44420719146728516
train gradient:  0.3224847304678386
iteration : 5238
train acc:  0.8203125
train loss:  0.36783531308174133
train gradient:  0.17910372335252359
iteration : 5239
train acc:  0.8828125
train loss:  0.2973284125328064
train gradient:  0.16496534518579817
iteration : 5240
train acc:  0.8671875
train loss:  0.36201348900794983
train gradient:  0.23148565969003368
iteration : 5241
train acc:  0.8046875
train loss:  0.3370327949523926
train gradient:  0.17197837530198148
iteration : 5242
train acc:  0.8359375
train loss:  0.3330681324005127
train gradient:  0.15611079971386654
iteration : 5243
train acc:  0.8359375
train loss:  0.3531316816806793
train gradient:  0.20149986985431056
iteration : 5244
train acc:  0.828125
train loss:  0.3866708278656006
train gradient:  0.2741646964153946
iteration : 5245
train acc:  0.828125
train loss:  0.37222784757614136
train gradient:  0.21158202242395666
iteration : 5246
train acc:  0.8359375
train loss:  0.4338568150997162
train gradient:  0.31146814744592183
iteration : 5247
train acc:  0.8203125
train loss:  0.3686218857765198
train gradient:  0.2536823384663988
iteration : 5248
train acc:  0.8203125
train loss:  0.39803025126457214
train gradient:  0.28080628301844285
iteration : 5249
train acc:  0.859375
train loss:  0.3828369081020355
train gradient:  0.26515199974126713
iteration : 5250
train acc:  0.828125
train loss:  0.3927377164363861
train gradient:  0.23094776570163444
iteration : 5251
train acc:  0.859375
train loss:  0.3190617561340332
train gradient:  0.1861898986627687
iteration : 5252
train acc:  0.8125
train loss:  0.34999579191207886
train gradient:  0.3467655169518105
iteration : 5253
train acc:  0.8828125
train loss:  0.3382478654384613
train gradient:  0.20367017990821856
iteration : 5254
train acc:  0.828125
train loss:  0.4168112874031067
train gradient:  0.25461043888165985
iteration : 5255
train acc:  0.8515625
train loss:  0.32016921043395996
train gradient:  0.15453086202827845
iteration : 5256
train acc:  0.84375
train loss:  0.3573337495326996
train gradient:  0.19935666192152007
iteration : 5257
train acc:  0.84375
train loss:  0.37824827432632446
train gradient:  0.2797003399074752
iteration : 5258
train acc:  0.8828125
train loss:  0.3503100872039795
train gradient:  0.24144668260573765
iteration : 5259
train acc:  0.90625
train loss:  0.28467902541160583
train gradient:  0.12768045839731093
iteration : 5260
train acc:  0.875
train loss:  0.3584441542625427
train gradient:  0.3484765957621499
iteration : 5261
train acc:  0.921875
train loss:  0.27885735034942627
train gradient:  0.1776425412226708
iteration : 5262
train acc:  0.875
train loss:  0.38592684268951416
train gradient:  0.21519540908868082
iteration : 5263
train acc:  0.796875
train loss:  0.4480298161506653
train gradient:  0.4111304279447864
iteration : 5264
train acc:  0.859375
train loss:  0.32980722188949585
train gradient:  0.2643917806944643
iteration : 5265
train acc:  0.8671875
train loss:  0.3234952688217163
train gradient:  0.2567846318060219
iteration : 5266
train acc:  0.8515625
train loss:  0.35725921392440796
train gradient:  0.2107826378978508
iteration : 5267
train acc:  0.8671875
train loss:  0.35407793521881104
train gradient:  0.20280064913391038
iteration : 5268
train acc:  0.8203125
train loss:  0.36380690336227417
train gradient:  0.22539204513916267
iteration : 5269
train acc:  0.8203125
train loss:  0.3773120641708374
train gradient:  0.21261235885915852
iteration : 5270
train acc:  0.875
train loss:  0.27278760075569153
train gradient:  0.17002824657837826
iteration : 5271
train acc:  0.859375
train loss:  0.3624495267868042
train gradient:  0.1972503425442202
iteration : 5272
train acc:  0.8515625
train loss:  0.300778865814209
train gradient:  0.16695002886736707
iteration : 5273
train acc:  0.859375
train loss:  0.2964506447315216
train gradient:  0.13521946973346566
iteration : 5274
train acc:  0.875
train loss:  0.36887961626052856
train gradient:  0.2618691240501315
iteration : 5275
train acc:  0.8046875
train loss:  0.32211410999298096
train gradient:  0.20982339032133257
iteration : 5276
train acc:  0.8359375
train loss:  0.29517149925231934
train gradient:  0.21791102754752112
iteration : 5277
train acc:  0.8515625
train loss:  0.30117231607437134
train gradient:  0.22386044120122353
iteration : 5278
train acc:  0.8984375
train loss:  0.3082989752292633
train gradient:  0.1870736189856819
iteration : 5279
train acc:  0.828125
train loss:  0.3486284613609314
train gradient:  0.22923308967238648
iteration : 5280
train acc:  0.8046875
train loss:  0.3449336588382721
train gradient:  0.2733548048342942
iteration : 5281
train acc:  0.84375
train loss:  0.34723806381225586
train gradient:  0.19023333116609686
iteration : 5282
train acc:  0.8828125
train loss:  0.3395746052265167
train gradient:  0.2204102591331227
iteration : 5283
train acc:  0.9140625
train loss:  0.27861711382865906
train gradient:  0.1834676046049446
iteration : 5284
train acc:  0.84375
train loss:  0.3447136878967285
train gradient:  0.22336102709281896
iteration : 5285
train acc:  0.8125
train loss:  0.42712700366973877
train gradient:  0.3008031950086759
iteration : 5286
train acc:  0.8125
train loss:  0.3892293870449066
train gradient:  0.2433917371711418
iteration : 5287
train acc:  0.8203125
train loss:  0.5448992252349854
train gradient:  0.43833834015273326
iteration : 5288
train acc:  0.8828125
train loss:  0.2755691409111023
train gradient:  0.17997881220756673
iteration : 5289
train acc:  0.875
train loss:  0.30794426798820496
train gradient:  0.20365248839149702
iteration : 5290
train acc:  0.8515625
train loss:  0.3485566973686218
train gradient:  0.2052795995336572
iteration : 5291
train acc:  0.859375
train loss:  0.30952513217926025
train gradient:  0.18929550378482843
iteration : 5292
train acc:  0.8359375
train loss:  0.37616586685180664
train gradient:  0.23477601823067462
iteration : 5293
train acc:  0.8203125
train loss:  0.3669005036354065
train gradient:  0.21372088589040367
iteration : 5294
train acc:  0.796875
train loss:  0.36678943037986755
train gradient:  0.2878613127689664
iteration : 5295
train acc:  0.8359375
train loss:  0.39355140924453735
train gradient:  0.3046751210698028
iteration : 5296
train acc:  0.8125
train loss:  0.437722384929657
train gradient:  0.34815025914128633
iteration : 5297
train acc:  0.8828125
train loss:  0.35093748569488525
train gradient:  0.36318066227794815
iteration : 5298
train acc:  0.8125
train loss:  0.41499364376068115
train gradient:  0.4045108001676267
iteration : 5299
train acc:  0.8515625
train loss:  0.3694700598716736
train gradient:  0.26336668553538867
iteration : 5300
train acc:  0.8671875
train loss:  0.3664948642253876
train gradient:  0.24958354304655198
iteration : 5301
train acc:  0.8671875
train loss:  0.3573424518108368
train gradient:  0.39702871064707623
iteration : 5302
train acc:  0.71875
train loss:  0.5075279474258423
train gradient:  0.4449161043741296
iteration : 5303
train acc:  0.8203125
train loss:  0.4061565399169922
train gradient:  0.3209576156006619
iteration : 5304
train acc:  0.8515625
train loss:  0.39562278985977173
train gradient:  0.2916020005737117
iteration : 5305
train acc:  0.8125
train loss:  0.4153910279273987
train gradient:  0.2858656818355564
iteration : 5306
train acc:  0.8515625
train loss:  0.3421288728713989
train gradient:  0.1940778744842138
iteration : 5307
train acc:  0.828125
train loss:  0.34724098443984985
train gradient:  0.18226776157097213
iteration : 5308
train acc:  0.8515625
train loss:  0.35909610986709595
train gradient:  0.23728371378531146
iteration : 5309
train acc:  0.8046875
train loss:  0.38640519976615906
train gradient:  0.2358047237378808
iteration : 5310
train acc:  0.7890625
train loss:  0.40311938524246216
train gradient:  0.2395850449045469
iteration : 5311
train acc:  0.8515625
train loss:  0.36834025382995605
train gradient:  0.2907316904920689
iteration : 5312
train acc:  0.828125
train loss:  0.2988077402114868
train gradient:  0.2203765415448951
iteration : 5313
train acc:  0.8203125
train loss:  0.4226821959018707
train gradient:  0.28567392289018323
iteration : 5314
train acc:  0.8203125
train loss:  0.3646722435951233
train gradient:  0.27493078127037324
iteration : 5315
train acc:  0.8828125
train loss:  0.2788636088371277
train gradient:  0.20352833757637895
iteration : 5316
train acc:  0.8828125
train loss:  0.2990265488624573
train gradient:  0.15380212556630013
iteration : 5317
train acc:  0.890625
train loss:  0.27069464325904846
train gradient:  0.19613960061908373
iteration : 5318
train acc:  0.8515625
train loss:  0.31778302788734436
train gradient:  0.17628371297336531
iteration : 5319
train acc:  0.921875
train loss:  0.24299058318138123
train gradient:  0.15820096980795623
iteration : 5320
train acc:  0.8125
train loss:  0.3534178137779236
train gradient:  0.22622562324560352
iteration : 5321
train acc:  0.859375
train loss:  0.28121691942214966
train gradient:  0.16467645089723829
iteration : 5322
train acc:  0.765625
train loss:  0.39316874742507935
train gradient:  0.3806632312235649
iteration : 5323
train acc:  0.875
train loss:  0.31559693813323975
train gradient:  0.15143233245615256
iteration : 5324
train acc:  0.8359375
train loss:  0.3884598910808563
train gradient:  0.2929442495356763
iteration : 5325
train acc:  0.8828125
train loss:  0.30845654010772705
train gradient:  0.16780636508720387
iteration : 5326
train acc:  0.796875
train loss:  0.40629664063453674
train gradient:  0.36997866843029276
iteration : 5327
train acc:  0.8828125
train loss:  0.3554409146308899
train gradient:  0.17988881283570912
iteration : 5328
train acc:  0.8046875
train loss:  0.38288044929504395
train gradient:  0.3286755991231355
iteration : 5329
train acc:  0.859375
train loss:  0.34603071212768555
train gradient:  0.2725499461584237
iteration : 5330
train acc:  0.8203125
train loss:  0.38336288928985596
train gradient:  0.30541557635983446
iteration : 5331
train acc:  0.8125
train loss:  0.4662327468395233
train gradient:  0.4618151061829942
iteration : 5332
train acc:  0.84375
train loss:  0.3756476640701294
train gradient:  0.2760213748489944
iteration : 5333
train acc:  0.8125
train loss:  0.35629358887672424
train gradient:  0.3000596191252504
iteration : 5334
train acc:  0.8828125
train loss:  0.2958333194255829
train gradient:  0.3019756258349133
iteration : 5335
train acc:  0.8671875
train loss:  0.34044116735458374
train gradient:  0.2454834894059011
iteration : 5336
train acc:  0.859375
train loss:  0.3837270736694336
train gradient:  0.25950005413452804
iteration : 5337
train acc:  0.8125
train loss:  0.3922654390335083
train gradient:  0.3656235244988917
iteration : 5338
train acc:  0.859375
train loss:  0.2956397831439972
train gradient:  0.2251361607090822
iteration : 5339
train acc:  0.828125
train loss:  0.3567968010902405
train gradient:  0.27459524108485467
iteration : 5340
train acc:  0.828125
train loss:  0.3774717450141907
train gradient:  0.22949891467957706
iteration : 5341
train acc:  0.8046875
train loss:  0.41117265820503235
train gradient:  0.24201040465871268
iteration : 5342
train acc:  0.875
train loss:  0.2798352539539337
train gradient:  0.15852613265723073
iteration : 5343
train acc:  0.7734375
train loss:  0.43585023283958435
train gradient:  0.3231381687944252
iteration : 5344
train acc:  0.7890625
train loss:  0.4068288803100586
train gradient:  0.29448455118103967
iteration : 5345
train acc:  0.8125
train loss:  0.39333730936050415
train gradient:  0.3872459065389098
iteration : 5346
train acc:  0.890625
train loss:  0.3293704390525818
train gradient:  0.19913310204486367
iteration : 5347
train acc:  0.8671875
train loss:  0.29697468876838684
train gradient:  0.16316448537858627
iteration : 5348
train acc:  0.828125
train loss:  0.3373054265975952
train gradient:  0.20921419809103534
iteration : 5349
train acc:  0.8203125
train loss:  0.4191124439239502
train gradient:  0.24801690199822402
iteration : 5350
train acc:  0.90625
train loss:  0.24890238046646118
train gradient:  0.1262331235235604
iteration : 5351
train acc:  0.8125
train loss:  0.41617879271507263
train gradient:  0.2994262797741081
iteration : 5352
train acc:  0.8203125
train loss:  0.3585343062877655
train gradient:  0.2824995244792745
iteration : 5353
train acc:  0.8359375
train loss:  0.3405245244503021
train gradient:  0.1990302299187442
iteration : 5354
train acc:  0.875
train loss:  0.2822549343109131
train gradient:  0.13128223870878708
iteration : 5355
train acc:  0.890625
train loss:  0.29805803298950195
train gradient:  0.17412058472518244
iteration : 5356
train acc:  0.859375
train loss:  0.3463324308395386
train gradient:  0.26944240489414567
iteration : 5357
train acc:  0.8671875
train loss:  0.3253328502178192
train gradient:  0.2344695881103036
iteration : 5358
train acc:  0.875
train loss:  0.31317681074142456
train gradient:  0.20108531462504278
iteration : 5359
train acc:  0.84375
train loss:  0.3906005620956421
train gradient:  0.29101439590333467
iteration : 5360
train acc:  0.765625
train loss:  0.45386940240859985
train gradient:  0.34949618867117715
iteration : 5361
train acc:  0.8359375
train loss:  0.35817015171051025
train gradient:  0.19970128862877418
iteration : 5362
train acc:  0.8828125
train loss:  0.30944687128067017
train gradient:  0.2080948678986012
iteration : 5363
train acc:  0.921875
train loss:  0.25993800163269043
train gradient:  0.14252229109180548
iteration : 5364
train acc:  0.859375
train loss:  0.35568928718566895
train gradient:  0.2675118883556279
iteration : 5365
train acc:  0.8359375
train loss:  0.38693076372146606
train gradient:  0.3157900566678801
iteration : 5366
train acc:  0.8671875
train loss:  0.38469645380973816
train gradient:  0.21041430717626242
iteration : 5367
train acc:  0.9140625
train loss:  0.2598068118095398
train gradient:  0.19069364490793872
iteration : 5368
train acc:  0.8515625
train loss:  0.34308290481567383
train gradient:  0.20239641338648628
iteration : 5369
train acc:  0.8046875
train loss:  0.4106847047805786
train gradient:  0.3137032065280459
iteration : 5370
train acc:  0.8515625
train loss:  0.3355337381362915
train gradient:  0.20368719052170065
iteration : 5371
train acc:  0.8359375
train loss:  0.40290582180023193
train gradient:  0.3702286422409869
iteration : 5372
train acc:  0.8203125
train loss:  0.36155542731285095
train gradient:  0.2501754652500959
iteration : 5373
train acc:  0.828125
train loss:  0.3626633286476135
train gradient:  0.3094657429360276
iteration : 5374
train acc:  0.8203125
train loss:  0.34374356269836426
train gradient:  0.23963722979046106
iteration : 5375
train acc:  0.84375
train loss:  0.328380286693573
train gradient:  0.43103005015296686
iteration : 5376
train acc:  0.921875
train loss:  0.24719959497451782
train gradient:  0.16300411426249556
iteration : 5377
train acc:  0.796875
train loss:  0.3684341013431549
train gradient:  0.2737410926723586
iteration : 5378
train acc:  0.765625
train loss:  0.4739556908607483
train gradient:  0.3891485937079688
iteration : 5379
train acc:  0.8203125
train loss:  0.36900442838668823
train gradient:  0.2570842977623315
iteration : 5380
train acc:  0.875
train loss:  0.30794084072113037
train gradient:  0.22745674358552936
iteration : 5381
train acc:  0.8203125
train loss:  0.363311767578125
train gradient:  0.3604643698391109
iteration : 5382
train acc:  0.859375
train loss:  0.32663628458976746
train gradient:  0.2033386666275189
iteration : 5383
train acc:  0.828125
train loss:  0.3795870542526245
train gradient:  0.29056379265447074
iteration : 5384
train acc:  0.8359375
train loss:  0.4019363522529602
train gradient:  0.22801133693992426
iteration : 5385
train acc:  0.78125
train loss:  0.3828495740890503
train gradient:  0.281665730541884
iteration : 5386
train acc:  0.8671875
train loss:  0.3244248032569885
train gradient:  0.15222859721290902
iteration : 5387
train acc:  0.84375
train loss:  0.32802289724349976
train gradient:  0.22157063935051907
iteration : 5388
train acc:  0.8125
train loss:  0.3371238112449646
train gradient:  0.1945477775390364
iteration : 5389
train acc:  0.875
train loss:  0.31097206473350525
train gradient:  0.18772483418872
iteration : 5390
train acc:  0.84375
train loss:  0.3215954899787903
train gradient:  0.17569134281448645
iteration : 5391
train acc:  0.84375
train loss:  0.34386375546455383
train gradient:  0.1816715286227838
iteration : 5392
train acc:  0.875
train loss:  0.2861926853656769
train gradient:  0.13498623280830832
iteration : 5393
train acc:  0.8515625
train loss:  0.3659144639968872
train gradient:  0.19169009388631247
iteration : 5394
train acc:  0.828125
train loss:  0.376245379447937
train gradient:  0.2864855250884812
iteration : 5395
train acc:  0.828125
train loss:  0.35560768842697144
train gradient:  0.25350406457418523
iteration : 5396
train acc:  0.859375
train loss:  0.36530089378356934
train gradient:  0.2568834130877686
iteration : 5397
train acc:  0.8671875
train loss:  0.3084760308265686
train gradient:  0.1528203730405064
iteration : 5398
train acc:  0.859375
train loss:  0.3198355436325073
train gradient:  0.13816547357548742
iteration : 5399
train acc:  0.921875
train loss:  0.25233882665634155
train gradient:  0.15602818883604316
iteration : 5400
train acc:  0.8671875
train loss:  0.3232246935367584
train gradient:  0.1469393977259163
iteration : 5401
train acc:  0.875
train loss:  0.33552125096321106
train gradient:  0.23454775062617536
iteration : 5402
train acc:  0.84375
train loss:  0.3461146652698517
train gradient:  0.22330062630770733
iteration : 5403
train acc:  0.828125
train loss:  0.32614824175834656
train gradient:  0.22619053013521137
iteration : 5404
train acc:  0.8671875
train loss:  0.38846802711486816
train gradient:  0.20741691943576238
iteration : 5405
train acc:  0.84375
train loss:  0.33086809515953064
train gradient:  0.20639829580736224
iteration : 5406
train acc:  0.8203125
train loss:  0.43372297286987305
train gradient:  0.4646219568569911
iteration : 5407
train acc:  0.828125
train loss:  0.3048304319381714
train gradient:  0.16973694180360552
iteration : 5408
train acc:  0.875
train loss:  0.2694866955280304
train gradient:  0.11217089658977057
iteration : 5409
train acc:  0.875
train loss:  0.31143397092819214
train gradient:  0.1662231834000583
iteration : 5410
train acc:  0.8828125
train loss:  0.2614549398422241
train gradient:  0.1733314976231028
iteration : 5411
train acc:  0.7578125
train loss:  0.5125570297241211
train gradient:  0.4252833857506421
iteration : 5412
train acc:  0.8359375
train loss:  0.3184208571910858
train gradient:  0.23744222816767713
iteration : 5413
train acc:  0.8046875
train loss:  0.36829161643981934
train gradient:  0.2406920923598342
iteration : 5414
train acc:  0.84375
train loss:  0.3667040467262268
train gradient:  0.2768025016118369
iteration : 5415
train acc:  0.828125
train loss:  0.37243950366973877
train gradient:  0.27681306314894866
iteration : 5416
train acc:  0.7890625
train loss:  0.39931806921958923
train gradient:  0.27089174569802776
iteration : 5417
train acc:  0.8515625
train loss:  0.35194480419158936
train gradient:  0.20535457707887475
iteration : 5418
train acc:  0.78125
train loss:  0.5026536583900452
train gradient:  0.4466842165247487
iteration : 5419
train acc:  0.890625
train loss:  0.3390187919139862
train gradient:  0.17108080655284402
iteration : 5420
train acc:  0.8671875
train loss:  0.30324843525886536
train gradient:  0.17238498287737913
iteration : 5421
train acc:  0.8203125
train loss:  0.3615937829017639
train gradient:  0.27395673148309957
iteration : 5422
train acc:  0.875
train loss:  0.28086769580841064
train gradient:  0.17478074830659937
iteration : 5423
train acc:  0.8515625
train loss:  0.34632307291030884
train gradient:  0.18964078534660145
iteration : 5424
train acc:  0.8203125
train loss:  0.3899381756782532
train gradient:  0.27744239567936213
iteration : 5425
train acc:  0.8671875
train loss:  0.299751877784729
train gradient:  0.19258880741790313
iteration : 5426
train acc:  0.8515625
train loss:  0.32195672392845154
train gradient:  0.2867166016115767
iteration : 5427
train acc:  0.875
train loss:  0.2649967074394226
train gradient:  0.13025416978106863
iteration : 5428
train acc:  0.875
train loss:  0.3237752616405487
train gradient:  0.2071363582024796
iteration : 5429
train acc:  0.84375
train loss:  0.333335280418396
train gradient:  0.1927312989723995
iteration : 5430
train acc:  0.859375
train loss:  0.3284268379211426
train gradient:  0.1877720963089975
iteration : 5431
train acc:  0.84375
train loss:  0.3401891589164734
train gradient:  0.21875191439114283
iteration : 5432
train acc:  0.8359375
train loss:  0.3592877686023712
train gradient:  0.22425167554164152
iteration : 5433
train acc:  0.890625
train loss:  0.3428575396537781
train gradient:  0.16705242729595432
iteration : 5434
train acc:  0.8515625
train loss:  0.33407002687454224
train gradient:  0.20563442190124837
iteration : 5435
train acc:  0.84375
train loss:  0.35759437084198
train gradient:  0.21677216691448992
iteration : 5436
train acc:  0.78125
train loss:  0.46020740270614624
train gradient:  0.2362471136811288
iteration : 5437
train acc:  0.8828125
train loss:  0.29302480816841125
train gradient:  0.2285198510446595
iteration : 5438
train acc:  0.84375
train loss:  0.32950401306152344
train gradient:  0.1809821674941795
iteration : 5439
train acc:  0.890625
train loss:  0.2685890197753906
train gradient:  0.232017865835861
iteration : 5440
train acc:  0.8203125
train loss:  0.33338791131973267
train gradient:  0.2943412806715474
iteration : 5441
train acc:  0.8515625
train loss:  0.3384265899658203
train gradient:  0.23953505259896937
iteration : 5442
train acc:  0.8046875
train loss:  0.452808141708374
train gradient:  0.41564773121362825
iteration : 5443
train acc:  0.8203125
train loss:  0.35996121168136597
train gradient:  0.1923329819821522
iteration : 5444
train acc:  0.8125
train loss:  0.4177858233451843
train gradient:  0.280009062185568
iteration : 5445
train acc:  0.8046875
train loss:  0.3865508437156677
train gradient:  0.23607937132018425
iteration : 5446
train acc:  0.859375
train loss:  0.3582131862640381
train gradient:  0.21064885504366648
iteration : 5447
train acc:  0.78125
train loss:  0.44160765409469604
train gradient:  0.44331167325124265
iteration : 5448
train acc:  0.84375
train loss:  0.37058091163635254
train gradient:  0.27050445448588734
iteration : 5449
train acc:  0.8984375
train loss:  0.31672319769859314
train gradient:  0.2905590978085578
iteration : 5450
train acc:  0.8125
train loss:  0.4304560422897339
train gradient:  0.29059990256584406
iteration : 5451
train acc:  0.84375
train loss:  0.3400525152683258
train gradient:  0.20137990523173227
iteration : 5452
train acc:  0.8515625
train loss:  0.29177549481391907
train gradient:  0.17108561847779286
iteration : 5453
train acc:  0.8515625
train loss:  0.31309226155281067
train gradient:  0.21856893280260653
iteration : 5454
train acc:  0.828125
train loss:  0.35752081871032715
train gradient:  0.19921552978870427
iteration : 5455
train acc:  0.8515625
train loss:  0.34703972935676575
train gradient:  0.22338215441019077
iteration : 5456
train acc:  0.8671875
train loss:  0.3055954575538635
train gradient:  0.18094730595835137
iteration : 5457
train acc:  0.8359375
train loss:  0.3216439485549927
train gradient:  0.18822151777694673
iteration : 5458
train acc:  0.8359375
train loss:  0.3664894700050354
train gradient:  0.265965040191884
iteration : 5459
train acc:  0.8046875
train loss:  0.4021109342575073
train gradient:  0.2790327115097834
iteration : 5460
train acc:  0.8203125
train loss:  0.42291486263275146
train gradient:  0.42742254595423373
iteration : 5461
train acc:  0.8671875
train loss:  0.294341504573822
train gradient:  0.1780846480657564
iteration : 5462
train acc:  0.8515625
train loss:  0.30732524394989014
train gradient:  0.1866795542313678
iteration : 5463
train acc:  0.78125
train loss:  0.4693848490715027
train gradient:  0.4572483434520206
iteration : 5464
train acc:  0.796875
train loss:  0.4085302948951721
train gradient:  0.276096662897915
iteration : 5465
train acc:  0.8515625
train loss:  0.34880220890045166
train gradient:  0.20795738497021118
iteration : 5466
train acc:  0.8359375
train loss:  0.3369610607624054
train gradient:  0.2595060060874732
iteration : 5467
train acc:  0.8828125
train loss:  0.28808480501174927
train gradient:  0.2663020305655295
iteration : 5468
train acc:  0.796875
train loss:  0.3976583182811737
train gradient:  0.27041451727888444
iteration : 5469
train acc:  0.8515625
train loss:  0.3615407347679138
train gradient:  0.20474850327633906
iteration : 5470
train acc:  0.8515625
train loss:  0.3152764141559601
train gradient:  0.16878651197038166
iteration : 5471
train acc:  0.8984375
train loss:  0.32014715671539307
train gradient:  0.33662520868523205
iteration : 5472
train acc:  0.828125
train loss:  0.3880164921283722
train gradient:  0.25378079595074804
iteration : 5473
train acc:  0.8359375
train loss:  0.3490840196609497
train gradient:  0.190206541626194
iteration : 5474
train acc:  0.8359375
train loss:  0.36136603355407715
train gradient:  0.23356064587436282
iteration : 5475
train acc:  0.8828125
train loss:  0.31240737438201904
train gradient:  0.1460047346646205
iteration : 5476
train acc:  0.875
train loss:  0.3543348014354706
train gradient:  0.24769340542991092
iteration : 5477
train acc:  0.8515625
train loss:  0.3513215482234955
train gradient:  0.16109323714505097
iteration : 5478
train acc:  0.8515625
train loss:  0.3524842858314514
train gradient:  0.24458432667534744
iteration : 5479
train acc:  0.8515625
train loss:  0.3406696319580078
train gradient:  0.17489399253859858
iteration : 5480
train acc:  0.859375
train loss:  0.353606641292572
train gradient:  0.20659173036749098
iteration : 5481
train acc:  0.84375
train loss:  0.32987797260284424
train gradient:  0.22349625585835192
iteration : 5482
train acc:  0.7890625
train loss:  0.49950647354125977
train gradient:  0.3072155866998376
iteration : 5483
train acc:  0.8984375
train loss:  0.2760073244571686
train gradient:  0.15809515388172812
iteration : 5484
train acc:  0.8125
train loss:  0.44509434700012207
train gradient:  0.3140749508341183
iteration : 5485
train acc:  0.8515625
train loss:  0.3691887855529785
train gradient:  0.2145298840436812
iteration : 5486
train acc:  0.890625
train loss:  0.2799457311630249
train gradient:  0.13643554757803933
iteration : 5487
train acc:  0.828125
train loss:  0.34365153312683105
train gradient:  0.22501790736378083
iteration : 5488
train acc:  0.8125
train loss:  0.38030076026916504
train gradient:  0.24433078411554082
iteration : 5489
train acc:  0.8125
train loss:  0.35222941637039185
train gradient:  0.2296179162130224
iteration : 5490
train acc:  0.84375
train loss:  0.34864938259124756
train gradient:  0.19592385946458918
iteration : 5491
train acc:  0.8515625
train loss:  0.3181759715080261
train gradient:  0.2754169256372109
iteration : 5492
train acc:  0.8515625
train loss:  0.34852638840675354
train gradient:  0.27556944027582747
iteration : 5493
train acc:  0.859375
train loss:  0.3370504379272461
train gradient:  0.1899195372935024
iteration : 5494
train acc:  0.8515625
train loss:  0.37271422147750854
train gradient:  0.2934690911069918
iteration : 5495
train acc:  0.875
train loss:  0.32564955949783325
train gradient:  0.12427338674430699
iteration : 5496
train acc:  0.859375
train loss:  0.3095172643661499
train gradient:  0.16692702875682128
iteration : 5497
train acc:  0.8203125
train loss:  0.39144545793533325
train gradient:  0.29739441192030863
iteration : 5498
train acc:  0.8671875
train loss:  0.29421466588974
train gradient:  0.1553124036088848
iteration : 5499
train acc:  0.84375
train loss:  0.30596065521240234
train gradient:  0.19414860146045332
iteration : 5500
train acc:  0.859375
train loss:  0.30899038910865784
train gradient:  0.23626988827358986
iteration : 5501
train acc:  0.7734375
train loss:  0.48006534576416016
train gradient:  0.36943019317441445
iteration : 5502
train acc:  0.8671875
train loss:  0.3404379189014435
train gradient:  0.16649829574626873
iteration : 5503
train acc:  0.859375
train loss:  0.27921730279922485
train gradient:  0.22218129869848546
iteration : 5504
train acc:  0.8203125
train loss:  0.36567723751068115
train gradient:  0.22042686125137478
iteration : 5505
train acc:  0.828125
train loss:  0.34271836280822754
train gradient:  0.23150762077759224
iteration : 5506
train acc:  0.828125
train loss:  0.3410825729370117
train gradient:  0.20512838056815297
iteration : 5507
train acc:  0.8671875
train loss:  0.4168447256088257
train gradient:  0.2603958273194364
iteration : 5508
train acc:  0.8203125
train loss:  0.3963385224342346
train gradient:  0.26297550836800726
iteration : 5509
train acc:  0.828125
train loss:  0.36994069814682007
train gradient:  0.20786057587474205
iteration : 5510
train acc:  0.8125
train loss:  0.3755672574043274
train gradient:  0.21101400184830904
iteration : 5511
train acc:  0.859375
train loss:  0.30781835317611694
train gradient:  0.2543070104400712
iteration : 5512
train acc:  0.828125
train loss:  0.3568703532218933
train gradient:  0.2623633150085854
iteration : 5513
train acc:  0.859375
train loss:  0.3261234164237976
train gradient:  0.2764338580268899
iteration : 5514
train acc:  0.859375
train loss:  0.2904478907585144
train gradient:  0.22858958844586513
iteration : 5515
train acc:  0.8203125
train loss:  0.34629204869270325
train gradient:  0.2015847352759288
iteration : 5516
train acc:  0.828125
train loss:  0.36374223232269287
train gradient:  0.2356143521689737
iteration : 5517
train acc:  0.84375
train loss:  0.3591074049472809
train gradient:  0.22551150238195924
iteration : 5518
train acc:  0.796875
train loss:  0.3785715699195862
train gradient:  0.2460001284821418
iteration : 5519
train acc:  0.84375
train loss:  0.36640602350234985
train gradient:  0.2190766489828441
iteration : 5520
train acc:  0.859375
train loss:  0.41217678785324097
train gradient:  0.2960475542544461
iteration : 5521
train acc:  0.8828125
train loss:  0.29676270484924316
train gradient:  0.20107406586727666
iteration : 5522
train acc:  0.8515625
train loss:  0.3428612947463989
train gradient:  0.2001384285837678
iteration : 5523
train acc:  0.859375
train loss:  0.31662115454673767
train gradient:  0.23410776126450622
iteration : 5524
train acc:  0.84375
train loss:  0.34863704442977905
train gradient:  0.21140977914044862
iteration : 5525
train acc:  0.8515625
train loss:  0.4226825535297394
train gradient:  0.2696984434320774
iteration : 5526
train acc:  0.890625
train loss:  0.2631334662437439
train gradient:  0.15840477870020428
iteration : 5527
train acc:  0.8828125
train loss:  0.3146604299545288
train gradient:  0.18951450409440487
iteration : 5528
train acc:  0.8984375
train loss:  0.27905431389808655
train gradient:  0.16174481300634233
iteration : 5529
train acc:  0.8515625
train loss:  0.34467923641204834
train gradient:  0.24099071251106782
iteration : 5530
train acc:  0.8671875
train loss:  0.29800814390182495
train gradient:  0.1473251585361497
iteration : 5531
train acc:  0.828125
train loss:  0.39487922191619873
train gradient:  0.3221964523222879
iteration : 5532
train acc:  0.8671875
train loss:  0.33850815892219543
train gradient:  0.19754985432709743
iteration : 5533
train acc:  0.90625
train loss:  0.24717777967453003
train gradient:  0.13649503357455595
iteration : 5534
train acc:  0.796875
train loss:  0.3833308219909668
train gradient:  0.20301678934134718
iteration : 5535
train acc:  0.8203125
train loss:  0.39313098788261414
train gradient:  0.26173863157811333
iteration : 5536
train acc:  0.8515625
train loss:  0.34100937843322754
train gradient:  0.16535658826730482
iteration : 5537
train acc:  0.796875
train loss:  0.43857264518737793
train gradient:  0.37613145305292994
iteration : 5538
train acc:  0.8359375
train loss:  0.31856489181518555
train gradient:  0.21811093554748073
iteration : 5539
train acc:  0.84375
train loss:  0.3457527756690979
train gradient:  0.18110483054862733
iteration : 5540
train acc:  0.859375
train loss:  0.29754871129989624
train gradient:  0.133956168499218
iteration : 5541
train acc:  0.8203125
train loss:  0.38838252425193787
train gradient:  0.18696778153157317
iteration : 5542
train acc:  0.875
train loss:  0.32984083890914917
train gradient:  0.15382079042102342
iteration : 5543
train acc:  0.8125
train loss:  0.4106464385986328
train gradient:  0.3228182744163763
iteration : 5544
train acc:  0.765625
train loss:  0.48373526334762573
train gradient:  0.31915995694816385
iteration : 5545
train acc:  0.921875
train loss:  0.2671295702457428
train gradient:  0.20014757704550865
iteration : 5546
train acc:  0.8828125
train loss:  0.3340764045715332
train gradient:  0.20023894838157952
iteration : 5547
train acc:  0.8515625
train loss:  0.3171888589859009
train gradient:  0.20732635404786012
iteration : 5548
train acc:  0.921875
train loss:  0.2590304911136627
train gradient:  0.1647874700890637
iteration : 5549
train acc:  0.78125
train loss:  0.3725906014442444
train gradient:  0.24602202704803072
iteration : 5550
train acc:  0.828125
train loss:  0.3332159221172333
train gradient:  0.20613677314565426
iteration : 5551
train acc:  0.890625
train loss:  0.31062451004981995
train gradient:  0.23111731755243342
iteration : 5552
train acc:  0.8828125
train loss:  0.2870863080024719
train gradient:  0.20845849438157601
iteration : 5553
train acc:  0.84375
train loss:  0.4336458146572113
train gradient:  0.3215565586847665
iteration : 5554
train acc:  0.796875
train loss:  0.4820893108844757
train gradient:  0.311228634074128
iteration : 5555
train acc:  0.8046875
train loss:  0.3986400067806244
train gradient:  0.21662784168946558
iteration : 5556
train acc:  0.875
train loss:  0.32687050104141235
train gradient:  0.18102520431000255
iteration : 5557
train acc:  0.8515625
train loss:  0.33021241426467896
train gradient:  0.22288011975001906
iteration : 5558
train acc:  0.8359375
train loss:  0.3567492663860321
train gradient:  0.24118195315596042
iteration : 5559
train acc:  0.84375
train loss:  0.31280890107154846
train gradient:  0.15407162618771897
iteration : 5560
train acc:  0.828125
train loss:  0.41989296674728394
train gradient:  0.23709464384629597
iteration : 5561
train acc:  0.828125
train loss:  0.36706778407096863
train gradient:  0.2537967204757741
iteration : 5562
train acc:  0.8671875
train loss:  0.32116246223449707
train gradient:  0.15823806542606048
iteration : 5563
train acc:  0.8671875
train loss:  0.3916979730129242
train gradient:  0.15670284945603608
iteration : 5564
train acc:  0.796875
train loss:  0.4088563323020935
train gradient:  0.33923142349294666
iteration : 5565
train acc:  0.8515625
train loss:  0.3386426270008087
train gradient:  0.1731083823957853
iteration : 5566
train acc:  0.859375
train loss:  0.2895505130290985
train gradient:  0.17194734962951003
iteration : 5567
train acc:  0.8671875
train loss:  0.34331637620925903
train gradient:  0.18852131620984394
iteration : 5568
train acc:  0.859375
train loss:  0.30134761333465576
train gradient:  0.3871423429973628
iteration : 5569
train acc:  0.8671875
train loss:  0.30623120069503784
train gradient:  0.21043673827329723
iteration : 5570
train acc:  0.7890625
train loss:  0.4601283073425293
train gradient:  0.4312889032073539
iteration : 5571
train acc:  0.828125
train loss:  0.3495495319366455
train gradient:  0.1703099771669654
iteration : 5572
train acc:  0.84375
train loss:  0.3180306553840637
train gradient:  0.22695673930553872
iteration : 5573
train acc:  0.8359375
train loss:  0.3417484164237976
train gradient:  0.19569979693213113
iteration : 5574
train acc:  0.859375
train loss:  0.37123435735702515
train gradient:  0.20621822761247438
iteration : 5575
train acc:  0.859375
train loss:  0.34806397557258606
train gradient:  0.15702543328355303
iteration : 5576
train acc:  0.84375
train loss:  0.32461097836494446
train gradient:  0.17154351626983233
iteration : 5577
train acc:  0.8828125
train loss:  0.29647260904312134
train gradient:  0.15239751136072893
iteration : 5578
train acc:  0.890625
train loss:  0.2516314387321472
train gradient:  0.11919490293566952
iteration : 5579
train acc:  0.8203125
train loss:  0.3460142910480499
train gradient:  0.19715357772967482
iteration : 5580
train acc:  0.828125
train loss:  0.40887168049812317
train gradient:  0.30434001358097296
iteration : 5581
train acc:  0.84375
train loss:  0.3377419114112854
train gradient:  0.16202098503011247
iteration : 5582
train acc:  0.84375
train loss:  0.3715088963508606
train gradient:  0.2389296598832488
iteration : 5583
train acc:  0.8515625
train loss:  0.32309192419052124
train gradient:  0.17113462187004547
iteration : 5584
train acc:  0.8359375
train loss:  0.3637465834617615
train gradient:  0.20992002710574853
iteration : 5585
train acc:  0.796875
train loss:  0.4465080797672272
train gradient:  0.3111189942808701
iteration : 5586
train acc:  0.8828125
train loss:  0.2847241461277008
train gradient:  0.19109335602927183
iteration : 5587
train acc:  0.84375
train loss:  0.31047216057777405
train gradient:  0.21941756235840176
iteration : 5588
train acc:  0.84375
train loss:  0.30140411853790283
train gradient:  0.16807675627249846
iteration : 5589
train acc:  0.8359375
train loss:  0.3173447251319885
train gradient:  0.14617676293093976
iteration : 5590
train acc:  0.8671875
train loss:  0.33091551065444946
train gradient:  0.21465829302988923
iteration : 5591
train acc:  0.859375
train loss:  0.28679558634757996
train gradient:  0.14818338265866834
iteration : 5592
train acc:  0.796875
train loss:  0.3619219660758972
train gradient:  0.1805027805960088
iteration : 5593
train acc:  0.8203125
train loss:  0.35009151697158813
train gradient:  0.3250605454983056
iteration : 5594
train acc:  0.828125
train loss:  0.321237176656723
train gradient:  0.18099628721360195
iteration : 5595
train acc:  0.8515625
train loss:  0.36087289452552795
train gradient:  0.17298536454799462
iteration : 5596
train acc:  0.78125
train loss:  0.4063163995742798
train gradient:  0.2783744110471259
iteration : 5597
train acc:  0.84375
train loss:  0.354290246963501
train gradient:  0.21063517775352175
iteration : 5598
train acc:  0.8203125
train loss:  0.4108830988407135
train gradient:  0.37093902191469424
iteration : 5599
train acc:  0.8125
train loss:  0.42783012986183167
train gradient:  0.33886478582047186
iteration : 5600
train acc:  0.8828125
train loss:  0.31658172607421875
train gradient:  0.19035008193071906
iteration : 5601
train acc:  0.8515625
train loss:  0.34654301404953003
train gradient:  0.2322552059811192
iteration : 5602
train acc:  0.8203125
train loss:  0.42972075939178467
train gradient:  0.2191875049197587
iteration : 5603
train acc:  0.828125
train loss:  0.39477241039276123
train gradient:  0.29697358441425176
iteration : 5604
train acc:  0.8359375
train loss:  0.3624933362007141
train gradient:  0.1853451733753631
iteration : 5605
train acc:  0.8515625
train loss:  0.3534781038761139
train gradient:  0.21497596722136766
iteration : 5606
train acc:  0.796875
train loss:  0.41369277238845825
train gradient:  0.3548159447544604
iteration : 5607
train acc:  0.8515625
train loss:  0.33925551176071167
train gradient:  0.24234086507546324
iteration : 5608
train acc:  0.8359375
train loss:  0.39544937014579773
train gradient:  0.2440398617522308
iteration : 5609
train acc:  0.8984375
train loss:  0.2648603320121765
train gradient:  0.14135655861079924
iteration : 5610
train acc:  0.828125
train loss:  0.36489763855934143
train gradient:  0.2662815181015841
iteration : 5611
train acc:  0.7734375
train loss:  0.46836039423942566
train gradient:  0.3496583622219583
iteration : 5612
train acc:  0.875
train loss:  0.3286556303501129
train gradient:  0.1678824409500895
iteration : 5613
train acc:  0.875
train loss:  0.29322636127471924
train gradient:  0.21362532604213988
iteration : 5614
train acc:  0.8828125
train loss:  0.2858743369579315
train gradient:  0.2275631713882221
iteration : 5615
train acc:  0.8515625
train loss:  0.37000179290771484
train gradient:  0.23196882574378277
iteration : 5616
train acc:  0.8515625
train loss:  0.3764503598213196
train gradient:  0.21232582805847913
iteration : 5617
train acc:  0.90625
train loss:  0.27218878269195557
train gradient:  0.1632670781423329
iteration : 5618
train acc:  0.8046875
train loss:  0.38596320152282715
train gradient:  0.18407482723581092
iteration : 5619
train acc:  0.90625
train loss:  0.30547189712524414
train gradient:  0.1883560238840547
iteration : 5620
train acc:  0.828125
train loss:  0.3571017384529114
train gradient:  0.17557137896157377
iteration : 5621
train acc:  0.78125
train loss:  0.35600993037223816
train gradient:  0.23188724548187756
iteration : 5622
train acc:  0.8046875
train loss:  0.4018212556838989
train gradient:  0.29147874368477406
iteration : 5623
train acc:  0.8046875
train loss:  0.32777145504951477
train gradient:  0.1720806229044202
iteration : 5624
train acc:  0.8671875
train loss:  0.321028470993042
train gradient:  0.2486032905710699
iteration : 5625
train acc:  0.8203125
train loss:  0.37848713994026184
train gradient:  0.21418331098569926
iteration : 5626
train acc:  0.8359375
train loss:  0.3952261209487915
train gradient:  0.2780726086367642
iteration : 5627
train acc:  0.875
train loss:  0.345213919878006
train gradient:  0.20604846421954998
iteration : 5628
train acc:  0.875
train loss:  0.2860656976699829
train gradient:  0.1807634681763089
iteration : 5629
train acc:  0.859375
train loss:  0.3161337971687317
train gradient:  0.14778248519803647
iteration : 5630
train acc:  0.84375
train loss:  0.35341137647628784
train gradient:  0.18306635252633757
iteration : 5631
train acc:  0.84375
train loss:  0.37462037801742554
train gradient:  0.20105917964979614
iteration : 5632
train acc:  0.890625
train loss:  0.34006017446517944
train gradient:  0.14196418212756448
iteration : 5633
train acc:  0.8828125
train loss:  0.2828963100910187
train gradient:  0.25411733194905817
iteration : 5634
train acc:  0.859375
train loss:  0.306748628616333
train gradient:  0.16790474653952875
iteration : 5635
train acc:  0.8203125
train loss:  0.36000049114227295
train gradient:  0.28769088294383693
iteration : 5636
train acc:  0.8203125
train loss:  0.39958903193473816
train gradient:  0.32484605435178193
iteration : 5637
train acc:  0.8984375
train loss:  0.27759498357772827
train gradient:  0.16151669170045685
iteration : 5638
train acc:  0.859375
train loss:  0.3437526226043701
train gradient:  0.20900154619657674
iteration : 5639
train acc:  0.8828125
train loss:  0.2904852628707886
train gradient:  0.1763353522748197
iteration : 5640
train acc:  0.84375
train loss:  0.3293703496456146
train gradient:  0.2648905190645814
iteration : 5641
train acc:  0.875
train loss:  0.3622834086418152
train gradient:  0.3179660527469268
iteration : 5642
train acc:  0.828125
train loss:  0.413433313369751
train gradient:  0.28603983046097053
iteration : 5643
train acc:  0.859375
train loss:  0.30360811948776245
train gradient:  0.1850398884210558
iteration : 5644
train acc:  0.8046875
train loss:  0.4838370084762573
train gradient:  0.4688374906547118
iteration : 5645
train acc:  0.8515625
train loss:  0.3169577717781067
train gradient:  0.2470876277256945
iteration : 5646
train acc:  0.8671875
train loss:  0.3081616759300232
train gradient:  0.16690810874633732
iteration : 5647
train acc:  0.8359375
train loss:  0.3849009573459625
train gradient:  0.2973281897460702
iteration : 5648
train acc:  0.8046875
train loss:  0.4152448773384094
train gradient:  0.3208481558089118
iteration : 5649
train acc:  0.875
train loss:  0.3100600838661194
train gradient:  0.19882425258052472
iteration : 5650
train acc:  0.8828125
train loss:  0.33878418803215027
train gradient:  0.1909332158211706
iteration : 5651
train acc:  0.90625
train loss:  0.25352394580841064
train gradient:  0.11730022617956005
iteration : 5652
train acc:  0.8828125
train loss:  0.2937730550765991
train gradient:  0.16738791543255468
iteration : 5653
train acc:  0.8828125
train loss:  0.34870433807373047
train gradient:  0.2401347931879041
iteration : 5654
train acc:  0.828125
train loss:  0.3453616499900818
train gradient:  0.1805518143981602
iteration : 5655
train acc:  0.8203125
train loss:  0.4156085252761841
train gradient:  0.2646686310057977
iteration : 5656
train acc:  0.796875
train loss:  0.39085543155670166
train gradient:  0.2837262302778976
iteration : 5657
train acc:  0.8515625
train loss:  0.3400629460811615
train gradient:  0.25269012026266924
iteration : 5658
train acc:  0.8125
train loss:  0.4110567569732666
train gradient:  0.25370060221349866
iteration : 5659
train acc:  0.828125
train loss:  0.3272797465324402
train gradient:  0.22234940882539203
iteration : 5660
train acc:  0.828125
train loss:  0.39567285776138306
train gradient:  0.2501099636202355
iteration : 5661
train acc:  0.84375
train loss:  0.37274569272994995
train gradient:  0.23722648227850301
iteration : 5662
train acc:  0.8125
train loss:  0.4050983488559723
train gradient:  0.3194369753651273
iteration : 5663
train acc:  0.8359375
train loss:  0.3781477212905884
train gradient:  0.22828701291636916
iteration : 5664
train acc:  0.7890625
train loss:  0.45295584201812744
train gradient:  0.2761036578452305
iteration : 5665
train acc:  0.8359375
train loss:  0.3559209108352661
train gradient:  0.22169053270763914
iteration : 5666
train acc:  0.828125
train loss:  0.37936311960220337
train gradient:  0.22561957696427298
iteration : 5667
train acc:  0.8671875
train loss:  0.32163113355636597
train gradient:  0.17420662163861106
iteration : 5668
train acc:  0.8984375
train loss:  0.2879253327846527
train gradient:  0.14188112345029658
iteration : 5669
train acc:  0.8046875
train loss:  0.40998923778533936
train gradient:  0.23259810951836368
iteration : 5670
train acc:  0.875
train loss:  0.3705291450023651
train gradient:  0.22092112232953987
iteration : 5671
train acc:  0.8125
train loss:  0.4145691990852356
train gradient:  0.3060417628123196
iteration : 5672
train acc:  0.8359375
train loss:  0.4432811141014099
train gradient:  0.32701506197077834
iteration : 5673
train acc:  0.8671875
train loss:  0.3098636269569397
train gradient:  0.11031348952642325
iteration : 5674
train acc:  0.828125
train loss:  0.33769476413726807
train gradient:  0.17024761031815555
iteration : 5675
train acc:  0.8203125
train loss:  0.3567028343677521
train gradient:  0.3324233140708397
iteration : 5676
train acc:  0.8515625
train loss:  0.3254711627960205
train gradient:  0.15609875733981482
iteration : 5677
train acc:  0.84375
train loss:  0.31158947944641113
train gradient:  0.18841979447602153
iteration : 5678
train acc:  0.84375
train loss:  0.3615691661834717
train gradient:  0.22135431581111836
iteration : 5679
train acc:  0.84375
train loss:  0.36169061064720154
train gradient:  0.2983712463952107
iteration : 5680
train acc:  0.8828125
train loss:  0.30569976568222046
train gradient:  0.18775647390447453
iteration : 5681
train acc:  0.8515625
train loss:  0.3208370804786682
train gradient:  0.19319962978713062
iteration : 5682
train acc:  0.8671875
train loss:  0.31556758284568787
train gradient:  0.1511578371509111
iteration : 5683
train acc:  0.7890625
train loss:  0.39349204301834106
train gradient:  0.2587755010040333
iteration : 5684
train acc:  0.84375
train loss:  0.3549202084541321
train gradient:  0.14815011300259617
iteration : 5685
train acc:  0.8984375
train loss:  0.353006511926651
train gradient:  0.16816934760633026
iteration : 5686
train acc:  0.8203125
train loss:  0.4152567684650421
train gradient:  0.3552346004061417
iteration : 5687
train acc:  0.796875
train loss:  0.4808368980884552
train gradient:  0.38285717681244846
iteration : 5688
train acc:  0.8359375
train loss:  0.34084057807922363
train gradient:  0.21600537769793104
iteration : 5689
train acc:  0.8046875
train loss:  0.34979182481765747
train gradient:  0.18464476049210965
iteration : 5690
train acc:  0.796875
train loss:  0.45277681946754456
train gradient:  0.342809976826884
iteration : 5691
train acc:  0.8515625
train loss:  0.3233215808868408
train gradient:  0.18908503970426102
iteration : 5692
train acc:  0.859375
train loss:  0.32357242703437805
train gradient:  0.18951920293396188
iteration : 5693
train acc:  0.8515625
train loss:  0.36557066440582275
train gradient:  0.21791271541006593
iteration : 5694
train acc:  0.8828125
train loss:  0.2913563847541809
train gradient:  0.13116698595783755
iteration : 5695
train acc:  0.8515625
train loss:  0.34215953946113586
train gradient:  0.2512194445154625
iteration : 5696
train acc:  0.859375
train loss:  0.36371076107025146
train gradient:  0.19546305951367596
iteration : 5697
train acc:  0.7734375
train loss:  0.37289655208587646
train gradient:  0.24388045402972452
iteration : 5698
train acc:  0.796875
train loss:  0.394280344247818
train gradient:  0.24027805804545635
iteration : 5699
train acc:  0.8359375
train loss:  0.3519616723060608
train gradient:  0.2682674066783966
iteration : 5700
train acc:  0.859375
train loss:  0.373928427696228
train gradient:  0.2818676441171917
iteration : 5701
train acc:  0.796875
train loss:  0.4089588522911072
train gradient:  0.31291253540482666
iteration : 5702
train acc:  0.8671875
train loss:  0.3053981065750122
train gradient:  0.12905977086433973
iteration : 5703
train acc:  0.8515625
train loss:  0.3556048274040222
train gradient:  0.28914482694654775
iteration : 5704
train acc:  0.8203125
train loss:  0.3648659586906433
train gradient:  0.3198213734323486
iteration : 5705
train acc:  0.8671875
train loss:  0.36490702629089355
train gradient:  0.2276451086710155
iteration : 5706
train acc:  0.8515625
train loss:  0.31170904636383057
train gradient:  0.16064418272025693
iteration : 5707
train acc:  0.8359375
train loss:  0.3364717662334442
train gradient:  0.19645289666325594
iteration : 5708
train acc:  0.8828125
train loss:  0.2761183977127075
train gradient:  0.13693658261617775
iteration : 5709
train acc:  0.8515625
train loss:  0.3956998586654663
train gradient:  0.23319409982260714
iteration : 5710
train acc:  0.8203125
train loss:  0.39698129892349243
train gradient:  0.49835733229843865
iteration : 5711
train acc:  0.8125
train loss:  0.40862125158309937
train gradient:  0.33103581502007373
iteration : 5712
train acc:  0.859375
train loss:  0.36988911032676697
train gradient:  0.23678012229494383
iteration : 5713
train acc:  0.84375
train loss:  0.30607640743255615
train gradient:  0.13861356565342658
iteration : 5714
train acc:  0.859375
train loss:  0.32772111892700195
train gradient:  0.1927043804295584
iteration : 5715
train acc:  0.84375
train loss:  0.32634472846984863
train gradient:  0.17184197853676908
iteration : 5716
train acc:  0.8515625
train loss:  0.3116421103477478
train gradient:  0.24721782398081393
iteration : 5717
train acc:  0.859375
train loss:  0.32185566425323486
train gradient:  0.18433225418970892
iteration : 5718
train acc:  0.9140625
train loss:  0.2364046275615692
train gradient:  0.12142779414098738
iteration : 5719
train acc:  0.8515625
train loss:  0.3371548056602478
train gradient:  0.186285222433787
iteration : 5720
train acc:  0.7734375
train loss:  0.4303135871887207
train gradient:  0.3287222418925185
iteration : 5721
train acc:  0.859375
train loss:  0.3031792938709259
train gradient:  0.19435915950554433
iteration : 5722
train acc:  0.8046875
train loss:  0.3831404447555542
train gradient:  0.27864969582007726
iteration : 5723
train acc:  0.8671875
train loss:  0.30631113052368164
train gradient:  0.20169219901586344
iteration : 5724
train acc:  0.859375
train loss:  0.37314948439598083
train gradient:  0.1949296795522591
iteration : 5725
train acc:  0.875
train loss:  0.2976912260055542
train gradient:  0.19601895202095793
iteration : 5726
train acc:  0.8203125
train loss:  0.41831740736961365
train gradient:  0.3198176121461481
iteration : 5727
train acc:  0.8828125
train loss:  0.3265594244003296
train gradient:  0.1734331944424246
iteration : 5728
train acc:  0.890625
train loss:  0.31330209970474243
train gradient:  0.13957016165016942
iteration : 5729
train acc:  0.84375
train loss:  0.3397509455680847
train gradient:  0.22311427282788482
iteration : 5730
train acc:  0.78125
train loss:  0.38330569863319397
train gradient:  0.21283799755154037
iteration : 5731
train acc:  0.875
train loss:  0.40145474672317505
train gradient:  0.19730365628743873
iteration : 5732
train acc:  0.859375
train loss:  0.36146196722984314
train gradient:  0.16539500550677688
iteration : 5733
train acc:  0.828125
train loss:  0.4337555766105652
train gradient:  0.3685407970685044
iteration : 5734
train acc:  0.828125
train loss:  0.3557360768318176
train gradient:  0.14527005955871936
iteration : 5735
train acc:  0.84375
train loss:  0.39042744040489197
train gradient:  0.2752284185471995
iteration : 5736
train acc:  0.8515625
train loss:  0.3201138377189636
train gradient:  0.18364895842978085
iteration : 5737
train acc:  0.8671875
train loss:  0.3346775770187378
train gradient:  0.16269155465272792
iteration : 5738
train acc:  0.8515625
train loss:  0.30790382623672485
train gradient:  0.15715192820143015
iteration : 5739
train acc:  0.8046875
train loss:  0.4340194761753082
train gradient:  0.31230237704217745
iteration : 5740
train acc:  0.8203125
train loss:  0.3846476972103119
train gradient:  0.2264606828871724
iteration : 5741
train acc:  0.875
train loss:  0.3529699444770813
train gradient:  0.19454015846435882
iteration : 5742
train acc:  0.8203125
train loss:  0.4423002600669861
train gradient:  0.2646806344001212
iteration : 5743
train acc:  0.90625
train loss:  0.2757833003997803
train gradient:  0.13299730012742622
iteration : 5744
train acc:  0.796875
train loss:  0.4074052572250366
train gradient:  0.3115201635107984
iteration : 5745
train acc:  0.90625
train loss:  0.32802993059158325
train gradient:  0.19443684224094598
iteration : 5746
train acc:  0.859375
train loss:  0.36685431003570557
train gradient:  0.21163431865381097
iteration : 5747
train acc:  0.8515625
train loss:  0.2878391146659851
train gradient:  0.15652714573141946
iteration : 5748
train acc:  0.8125
train loss:  0.4264312982559204
train gradient:  0.26514244174590174
iteration : 5749
train acc:  0.8125
train loss:  0.3574982285499573
train gradient:  0.2299815936240149
iteration : 5750
train acc:  0.859375
train loss:  0.3253135681152344
train gradient:  0.19299967276881644
iteration : 5751
train acc:  0.828125
train loss:  0.32328271865844727
train gradient:  0.21366563892567178
iteration : 5752
train acc:  0.8046875
train loss:  0.41529279947280884
train gradient:  0.3825516904804638
iteration : 5753
train acc:  0.859375
train loss:  0.29737427830696106
train gradient:  0.16628179151822506
iteration : 5754
train acc:  0.8671875
train loss:  0.3136824369430542
train gradient:  0.16075705115414451
iteration : 5755
train acc:  0.859375
train loss:  0.3456261456012726
train gradient:  0.20769440595636612
iteration : 5756
train acc:  0.8671875
train loss:  0.39031994342803955
train gradient:  0.30790350693003654
iteration : 5757
train acc:  0.859375
train loss:  0.36047691106796265
train gradient:  0.2647768880097671
iteration : 5758
train acc:  0.84375
train loss:  0.35522472858428955
train gradient:  0.19269240991031086
iteration : 5759
train acc:  0.875
train loss:  0.32939380407333374
train gradient:  0.15989826895777354
iteration : 5760
train acc:  0.859375
train loss:  0.3610033094882965
train gradient:  0.16562104032564207
iteration : 5761
train acc:  0.875
train loss:  0.3511355519294739
train gradient:  0.19343378769607314
iteration : 5762
train acc:  0.8203125
train loss:  0.36510545015335083
train gradient:  0.23492909308083756
iteration : 5763
train acc:  0.8515625
train loss:  0.31499531865119934
train gradient:  0.165704692757441
iteration : 5764
train acc:  0.8125
train loss:  0.3995380997657776
train gradient:  0.3073719518850685
iteration : 5765
train acc:  0.875
train loss:  0.32035863399505615
train gradient:  0.14016121270479814
iteration : 5766
train acc:  0.859375
train loss:  0.3609454929828644
train gradient:  0.3473409682058465
iteration : 5767
train acc:  0.796875
train loss:  0.3921043574810028
train gradient:  0.2129955724314324
iteration : 5768
train acc:  0.7890625
train loss:  0.45595839619636536
train gradient:  0.33267283776482387
iteration : 5769
train acc:  0.84375
train loss:  0.3829989433288574
train gradient:  0.2100821547886986
iteration : 5770
train acc:  0.875
train loss:  0.2883954644203186
train gradient:  0.16200765443069431
iteration : 5771
train acc:  0.8671875
train loss:  0.3442087173461914
train gradient:  0.21712476048630527
iteration : 5772
train acc:  0.859375
train loss:  0.32111436128616333
train gradient:  0.2012411241680965
iteration : 5773
train acc:  0.9140625
train loss:  0.3002598285675049
train gradient:  0.1786914747437598
iteration : 5774
train acc:  0.890625
train loss:  0.2770734429359436
train gradient:  0.13106757886214943
iteration : 5775
train acc:  0.8515625
train loss:  0.35523471236228943
train gradient:  0.28284949147622696
iteration : 5776
train acc:  0.8359375
train loss:  0.35907548666000366
train gradient:  0.20747193647031115
iteration : 5777
train acc:  0.890625
train loss:  0.2828007936477661
train gradient:  0.21438870881898114
iteration : 5778
train acc:  0.8515625
train loss:  0.35746902227401733
train gradient:  0.24753186202641314
iteration : 5779
train acc:  0.84375
train loss:  0.39852702617645264
train gradient:  0.393560639014416
iteration : 5780
train acc:  0.890625
train loss:  0.2711528241634369
train gradient:  0.1422677073951664
iteration : 5781
train acc:  0.8203125
train loss:  0.4260792136192322
train gradient:  0.2388289251998682
iteration : 5782
train acc:  0.8515625
train loss:  0.37053483724594116
train gradient:  0.3149418680212075
iteration : 5783
train acc:  0.84375
train loss:  0.31818193197250366
train gradient:  0.1498164975985428
iteration : 5784
train acc:  0.828125
train loss:  0.33412063121795654
train gradient:  0.24933911911096152
iteration : 5785
train acc:  0.8828125
train loss:  0.3193078637123108
train gradient:  0.18050515429319036
iteration : 5786
train acc:  0.84375
train loss:  0.3981354534626007
train gradient:  0.20954952727698906
iteration : 5787
train acc:  0.859375
train loss:  0.30818474292755127
train gradient:  0.16601439972756787
iteration : 5788
train acc:  0.859375
train loss:  0.37456193566322327
train gradient:  0.23478974946047831
iteration : 5789
train acc:  0.8984375
train loss:  0.28673994541168213
train gradient:  0.13789792321413988
iteration : 5790
train acc:  0.84375
train loss:  0.39162909984588623
train gradient:  0.1824564547917192
iteration : 5791
train acc:  0.78125
train loss:  0.4050620198249817
train gradient:  0.2611779167501617
iteration : 5792
train acc:  0.8515625
train loss:  0.31497055292129517
train gradient:  0.1422830263096881
iteration : 5793
train acc:  0.8046875
train loss:  0.39588603377342224
train gradient:  0.2783048575754204
iteration : 5794
train acc:  0.859375
train loss:  0.30508455634117126
train gradient:  0.36639369251182186
iteration : 5795
train acc:  0.8359375
train loss:  0.3626193404197693
train gradient:  0.2199118935553752
iteration : 5796
train acc:  0.890625
train loss:  0.26194873452186584
train gradient:  0.14232392136931757
iteration : 5797
train acc:  0.8515625
train loss:  0.3215194344520569
train gradient:  0.2373390584261305
iteration : 5798
train acc:  0.8125
train loss:  0.4026419520378113
train gradient:  0.3947308064440055
iteration : 5799
train acc:  0.8359375
train loss:  0.4123994708061218
train gradient:  0.32857285051543805
iteration : 5800
train acc:  0.8671875
train loss:  0.3062196373939514
train gradient:  0.20735229786119896
iteration : 5801
train acc:  0.8125
train loss:  0.37449902296066284
train gradient:  0.2531959881568663
iteration : 5802
train acc:  0.8203125
train loss:  0.4134593904018402
train gradient:  0.2737597634260961
iteration : 5803
train acc:  0.828125
train loss:  0.41294169425964355
train gradient:  0.3562323280258222
iteration : 5804
train acc:  0.8203125
train loss:  0.3607775568962097
train gradient:  0.2466559489035699
iteration : 5805
train acc:  0.8671875
train loss:  0.29768335819244385
train gradient:  0.19794648955220626
iteration : 5806
train acc:  0.8984375
train loss:  0.2897660732269287
train gradient:  0.23196923286421553
iteration : 5807
train acc:  0.8359375
train loss:  0.33740776777267456
train gradient:  0.22334815228309352
iteration : 5808
train acc:  0.7890625
train loss:  0.40026479959487915
train gradient:  0.2606972687037073
iteration : 5809
train acc:  0.8515625
train loss:  0.3123990297317505
train gradient:  0.14544427091033832
iteration : 5810
train acc:  0.8359375
train loss:  0.3570169508457184
train gradient:  0.21457363715349081
iteration : 5811
train acc:  0.8515625
train loss:  0.3053987920284271
train gradient:  0.16889046632491894
iteration : 5812
train acc:  0.75
train loss:  0.5690773725509644
train gradient:  0.3915963667408681
iteration : 5813
train acc:  0.84375
train loss:  0.38044264912605286
train gradient:  0.19895695139922717
iteration : 5814
train acc:  0.8515625
train loss:  0.337283730506897
train gradient:  0.21356861272986316
iteration : 5815
train acc:  0.8984375
train loss:  0.2693564295768738
train gradient:  0.1399265337754366
iteration : 5816
train acc:  0.8515625
train loss:  0.3396609425544739
train gradient:  0.21635723376966343
iteration : 5817
train acc:  0.8515625
train loss:  0.3251308798789978
train gradient:  0.22709950676868584
iteration : 5818
train acc:  0.875
train loss:  0.30651789903640747
train gradient:  0.1534025696741951
iteration : 5819
train acc:  0.859375
train loss:  0.35833093523979187
train gradient:  0.19465977117517785
iteration : 5820
train acc:  0.84375
train loss:  0.395724356174469
train gradient:  0.29034109247338213
iteration : 5821
train acc:  0.84375
train loss:  0.3340393304824829
train gradient:  0.19953036801830723
iteration : 5822
train acc:  0.8515625
train loss:  0.3712529242038727
train gradient:  0.21278745069483634
iteration : 5823
train acc:  0.875
train loss:  0.29967671632766724
train gradient:  0.13431199524424697
iteration : 5824
train acc:  0.8515625
train loss:  0.33168554306030273
train gradient:  0.1526143767587501
iteration : 5825
train acc:  0.8828125
train loss:  0.2867025136947632
train gradient:  0.1752251011715985
iteration : 5826
train acc:  0.8515625
train loss:  0.37569254636764526
train gradient:  0.23526932207998258
iteration : 5827
train acc:  0.8984375
train loss:  0.2756967544555664
train gradient:  0.18199057929737827
iteration : 5828
train acc:  0.8125
train loss:  0.32960689067840576
train gradient:  0.24934697330113426
iteration : 5829
train acc:  0.828125
train loss:  0.3839922547340393
train gradient:  0.17447596674716173
iteration : 5830
train acc:  0.8984375
train loss:  0.28564131259918213
train gradient:  0.14746645992239724
iteration : 5831
train acc:  0.8203125
train loss:  0.3747994005680084
train gradient:  0.22161214092247775
iteration : 5832
train acc:  0.8984375
train loss:  0.25599923729896545
train gradient:  0.10288192489601475
iteration : 5833
train acc:  0.8125
train loss:  0.38028788566589355
train gradient:  0.25126413257072666
iteration : 5834
train acc:  0.8515625
train loss:  0.38958293199539185
train gradient:  0.27251284655420444
iteration : 5835
train acc:  0.828125
train loss:  0.3539144992828369
train gradient:  0.26446752579453237
iteration : 5836
train acc:  0.8359375
train loss:  0.35275429487228394
train gradient:  0.26236137396400144
iteration : 5837
train acc:  0.8515625
train loss:  0.30719229578971863
train gradient:  0.14057687152098586
iteration : 5838
train acc:  0.8828125
train loss:  0.27717405557632446
train gradient:  0.12858195250279914
iteration : 5839
train acc:  0.859375
train loss:  0.3473677635192871
train gradient:  0.22545397128956246
iteration : 5840
train acc:  0.84375
train loss:  0.3379790186882019
train gradient:  0.17265669443389273
iteration : 5841
train acc:  0.859375
train loss:  0.3327232897281647
train gradient:  0.17877178045275183
iteration : 5842
train acc:  0.8046875
train loss:  0.3775596618652344
train gradient:  0.22035420932980188
iteration : 5843
train acc:  0.8203125
train loss:  0.3785993456840515
train gradient:  0.200053401532791
iteration : 5844
train acc:  0.859375
train loss:  0.3365976810455322
train gradient:  0.20109057276265868
iteration : 5845
train acc:  0.8515625
train loss:  0.38095614314079285
train gradient:  0.1915066360429871
iteration : 5846
train acc:  0.8671875
train loss:  0.3174118101596832
train gradient:  0.18263719941911838
iteration : 5847
train acc:  0.8046875
train loss:  0.3892301023006439
train gradient:  0.42674140913565467
iteration : 5848
train acc:  0.8203125
train loss:  0.44201552867889404
train gradient:  0.3866581494901423
iteration : 5849
train acc:  0.859375
train loss:  0.3486182987689972
train gradient:  0.28703221645550525
iteration : 5850
train acc:  0.8359375
train loss:  0.4345433712005615
train gradient:  0.3435007431425657
iteration : 5851
train acc:  0.796875
train loss:  0.41335800290107727
train gradient:  0.2140898644465719
iteration : 5852
train acc:  0.8125
train loss:  0.4149610102176666
train gradient:  0.308705546266735
iteration : 5853
train acc:  0.7890625
train loss:  0.4320727586746216
train gradient:  0.25247124783446556
iteration : 5854
train acc:  0.875
train loss:  0.2812685966491699
train gradient:  0.1944179279914305
iteration : 5855
train acc:  0.8359375
train loss:  0.3670393228530884
train gradient:  0.2441238084265587
iteration : 5856
train acc:  0.84375
train loss:  0.36779284477233887
train gradient:  0.195484112153049
iteration : 5857
train acc:  0.859375
train loss:  0.33639150857925415
train gradient:  0.19155648083904175
iteration : 5858
train acc:  0.875
train loss:  0.34971678256988525
train gradient:  0.1721614053443598
iteration : 5859
train acc:  0.8515625
train loss:  0.33276116847991943
train gradient:  0.1827303901727379
iteration : 5860
train acc:  0.828125
train loss:  0.35595807433128357
train gradient:  0.21423477629055498
iteration : 5861
train acc:  0.8359375
train loss:  0.36062565445899963
train gradient:  0.21087307598923563
iteration : 5862
train acc:  0.90625
train loss:  0.26230478286743164
train gradient:  0.11810377726525978
iteration : 5863
train acc:  0.828125
train loss:  0.39106202125549316
train gradient:  0.23546277709349764
iteration : 5864
train acc:  0.828125
train loss:  0.41817140579223633
train gradient:  0.3390554451243162
iteration : 5865
train acc:  0.7890625
train loss:  0.4495987892150879
train gradient:  0.3320415325187425
iteration : 5866
train acc:  0.8984375
train loss:  0.30952364206314087
train gradient:  0.2639400670364881
iteration : 5867
train acc:  0.84375
train loss:  0.361122727394104
train gradient:  0.2799809271642343
iteration : 5868
train acc:  0.8515625
train loss:  0.39604681730270386
train gradient:  0.29203500817217803
iteration : 5869
train acc:  0.8515625
train loss:  0.34061920642852783
train gradient:  0.17581537902481742
iteration : 5870
train acc:  0.8984375
train loss:  0.30484122037887573
train gradient:  0.1790909074970902
iteration : 5871
train acc:  0.828125
train loss:  0.3822551369667053
train gradient:  0.20882401297781814
iteration : 5872
train acc:  0.8046875
train loss:  0.368458092212677
train gradient:  0.25593958917065196
iteration : 5873
train acc:  0.7734375
train loss:  0.38892775774002075
train gradient:  0.18661015717274015
iteration : 5874
train acc:  0.890625
train loss:  0.3220115303993225
train gradient:  0.18045465331942934
iteration : 5875
train acc:  0.8515625
train loss:  0.36778387427330017
train gradient:  0.22276432457154632
iteration : 5876
train acc:  0.890625
train loss:  0.2787114977836609
train gradient:  0.1628192258354279
iteration : 5877
train acc:  0.8515625
train loss:  0.4401949346065521
train gradient:  0.1999781643644848
iteration : 5878
train acc:  0.8359375
train loss:  0.3851010799407959
train gradient:  0.20369320683757489
iteration : 5879
train acc:  0.8125
train loss:  0.45527711510658264
train gradient:  0.3061663964052308
iteration : 5880
train acc:  0.84375
train loss:  0.3264545798301697
train gradient:  0.22945373830621196
iteration : 5881
train acc:  0.890625
train loss:  0.3087713122367859
train gradient:  0.15544255765693898
iteration : 5882
train acc:  0.859375
train loss:  0.2918912470340729
train gradient:  0.14946691678784818
iteration : 5883
train acc:  0.84375
train loss:  0.35211679339408875
train gradient:  0.20997971680243077
iteration : 5884
train acc:  0.8828125
train loss:  0.3173772096633911
train gradient:  0.18785861664032943
iteration : 5885
train acc:  0.8203125
train loss:  0.3887530565261841
train gradient:  0.2254521907498241
iteration : 5886
train acc:  0.8359375
train loss:  0.33299195766448975
train gradient:  0.2291896563663118
iteration : 5887
train acc:  0.78125
train loss:  0.4627339243888855
train gradient:  0.2910696436397212
iteration : 5888
train acc:  0.8359375
train loss:  0.35995981097221375
train gradient:  0.31892864205171284
iteration : 5889
train acc:  0.8515625
train loss:  0.34331393241882324
train gradient:  0.22325609933053833
iteration : 5890
train acc:  0.84375
train loss:  0.317745566368103
train gradient:  0.21017002431919118
iteration : 5891
train acc:  0.859375
train loss:  0.3091889023780823
train gradient:  0.14255943055078601
iteration : 5892
train acc:  0.8671875
train loss:  0.33719462156295776
train gradient:  0.20867248166478325
iteration : 5893
train acc:  0.8125
train loss:  0.4364365339279175
train gradient:  0.29024187752448266
iteration : 5894
train acc:  0.859375
train loss:  0.3325483798980713
train gradient:  0.19211931059537407
iteration : 5895
train acc:  0.7890625
train loss:  0.4776257872581482
train gradient:  0.4807598276326429
iteration : 5896
train acc:  0.890625
train loss:  0.27820929884910583
train gradient:  0.13582949871172517
iteration : 5897
train acc:  0.8046875
train loss:  0.4215748906135559
train gradient:  0.30616207549066865
iteration : 5898
train acc:  0.8359375
train loss:  0.34967249631881714
train gradient:  0.20951882885184464
iteration : 5899
train acc:  0.890625
train loss:  0.3021581470966339
train gradient:  0.15474640084797547
iteration : 5900
train acc:  0.84375
train loss:  0.34372639656066895
train gradient:  0.2245727734942028
iteration : 5901
train acc:  0.84375
train loss:  0.3321755528450012
train gradient:  0.16739250852426268
iteration : 5902
train acc:  0.8046875
train loss:  0.40469080209732056
train gradient:  0.2612691405551301
iteration : 5903
train acc:  0.875
train loss:  0.31529638171195984
train gradient:  0.14513543572913457
iteration : 5904
train acc:  0.8515625
train loss:  0.33702945709228516
train gradient:  0.28918351510164164
iteration : 5905
train acc:  0.8359375
train loss:  0.38867074251174927
train gradient:  0.2347216236240518
iteration : 5906
train acc:  0.8671875
train loss:  0.30483779311180115
train gradient:  0.207755420153439
iteration : 5907
train acc:  0.796875
train loss:  0.4151744246482849
train gradient:  0.23345928130785482
iteration : 5908
train acc:  0.859375
train loss:  0.315690279006958
train gradient:  0.15563017430767145
iteration : 5909
train acc:  0.796875
train loss:  0.38534286618232727
train gradient:  0.19944971957957386
iteration : 5910
train acc:  0.84375
train loss:  0.3498884439468384
train gradient:  0.20032220421308994
iteration : 5911
train acc:  0.859375
train loss:  0.2731502652168274
train gradient:  0.14748779497853554
iteration : 5912
train acc:  0.8828125
train loss:  0.2738289535045624
train gradient:  0.15675582182229644
iteration : 5913
train acc:  0.8359375
train loss:  0.3644202649593353
train gradient:  0.19700859512059918
iteration : 5914
train acc:  0.8125
train loss:  0.3807354271411896
train gradient:  0.25280522800635
iteration : 5915
train acc:  0.84375
train loss:  0.38619858026504517
train gradient:  0.2784860080551522
iteration : 5916
train acc:  0.8515625
train loss:  0.2878248691558838
train gradient:  0.15386439970760163
iteration : 5917
train acc:  0.7734375
train loss:  0.47638893127441406
train gradient:  0.33197164962347575
iteration : 5918
train acc:  0.8359375
train loss:  0.39571860432624817
train gradient:  0.29367172749139714
iteration : 5919
train acc:  0.828125
train loss:  0.32259833812713623
train gradient:  0.30436508571678134
iteration : 5920
train acc:  0.828125
train loss:  0.40628618001937866
train gradient:  0.19995607248314778
iteration : 5921
train acc:  0.8515625
train loss:  0.35947394371032715
train gradient:  0.2104292400177633
iteration : 5922
train acc:  0.8671875
train loss:  0.36326128244400024
train gradient:  0.23686990482750497
iteration : 5923
train acc:  0.90625
train loss:  0.24661797285079956
train gradient:  0.11660376086525144
iteration : 5924
train acc:  0.8515625
train loss:  0.35906219482421875
train gradient:  0.2248315490703487
iteration : 5925
train acc:  0.8828125
train loss:  0.26279860734939575
train gradient:  0.13658405506477483
iteration : 5926
train acc:  0.8828125
train loss:  0.34283754229545593
train gradient:  0.18223618980007572
iteration : 5927
train acc:  0.9296875
train loss:  0.26218709349632263
train gradient:  0.16701371140019805
iteration : 5928
train acc:  0.859375
train loss:  0.34584319591522217
train gradient:  0.1849196859441964
iteration : 5929
train acc:  0.8515625
train loss:  0.3576214909553528
train gradient:  0.22120911372898894
iteration : 5930
train acc:  0.8515625
train loss:  0.3433939218521118
train gradient:  0.2348530297342103
iteration : 5931
train acc:  0.84375
train loss:  0.391524076461792
train gradient:  0.21769131818234422
iteration : 5932
train acc:  0.90625
train loss:  0.28398919105529785
train gradient:  0.18068378322646678
iteration : 5933
train acc:  0.8984375
train loss:  0.28572162985801697
train gradient:  0.13257892187367729
iteration : 5934
train acc:  0.8359375
train loss:  0.3665100932121277
train gradient:  0.46161463209624065
iteration : 5935
train acc:  0.8125
train loss:  0.4229904115200043
train gradient:  0.35054073483680465
iteration : 5936
train acc:  0.8984375
train loss:  0.2815953493118286
train gradient:  0.22972825284065468
iteration : 5937
train acc:  0.8671875
train loss:  0.3545875549316406
train gradient:  0.27420317314183057
iteration : 5938
train acc:  0.828125
train loss:  0.3477976620197296
train gradient:  0.24707365565828976
iteration : 5939
train acc:  0.8359375
train loss:  0.3444064259529114
train gradient:  0.1883076783614323
iteration : 5940
train acc:  0.90625
train loss:  0.31380897760391235
train gradient:  0.16072463318604996
iteration : 5941
train acc:  0.8203125
train loss:  0.3748035430908203
train gradient:  0.2265143002070095
iteration : 5942
train acc:  0.8828125
train loss:  0.33568626642227173
train gradient:  0.23197041016947045
iteration : 5943
train acc:  0.8671875
train loss:  0.3146468698978424
train gradient:  0.14699657599844027
iteration : 5944
train acc:  0.84375
train loss:  0.32742857933044434
train gradient:  0.20185813333126357
iteration : 5945
train acc:  0.8671875
train loss:  0.31890416145324707
train gradient:  0.1805802103712232
iteration : 5946
train acc:  0.8515625
train loss:  0.33061569929122925
train gradient:  0.24171258946002672
iteration : 5947
train acc:  0.90625
train loss:  0.3022797703742981
train gradient:  0.2013723315948908
iteration : 5948
train acc:  0.8515625
train loss:  0.3126347064971924
train gradient:  0.15694644344015252
iteration : 5949
train acc:  0.8828125
train loss:  0.2622953951358795
train gradient:  0.164223995551385
iteration : 5950
train acc:  0.8046875
train loss:  0.3870055079460144
train gradient:  0.3700107038622405
iteration : 5951
train acc:  0.8359375
train loss:  0.39260220527648926
train gradient:  0.24603181515054065
iteration : 5952
train acc:  0.796875
train loss:  0.4530355632305145
train gradient:  0.32254296146136546
iteration : 5953
train acc:  0.8125
train loss:  0.41328173875808716
train gradient:  0.35422032582662893
iteration : 5954
train acc:  0.890625
train loss:  0.310188353061676
train gradient:  0.1579547665491543
iteration : 5955
train acc:  0.78125
train loss:  0.43617936968803406
train gradient:  0.3433536732524591
iteration : 5956
train acc:  0.90625
train loss:  0.2714335024356842
train gradient:  0.1328425694794218
iteration : 5957
train acc:  0.796875
train loss:  0.4079250991344452
train gradient:  0.367247071798422
iteration : 5958
train acc:  0.8828125
train loss:  0.30811867117881775
train gradient:  0.1248401361026847
iteration : 5959
train acc:  0.8671875
train loss:  0.28854483366012573
train gradient:  0.1307825454540355
iteration : 5960
train acc:  0.8125
train loss:  0.3737974166870117
train gradient:  0.24562282343651476
iteration : 5961
train acc:  0.828125
train loss:  0.35923516750335693
train gradient:  0.20650892363874745
iteration : 5962
train acc:  0.84375
train loss:  0.3805694580078125
train gradient:  0.27678901735793277
iteration : 5963
train acc:  0.8125
train loss:  0.439686119556427
train gradient:  0.49570154321918636
iteration : 5964
train acc:  0.8671875
train loss:  0.3205868601799011
train gradient:  0.19268613285551273
iteration : 5965
train acc:  0.8359375
train loss:  0.3132583796977997
train gradient:  0.19124357519890767
iteration : 5966
train acc:  0.8984375
train loss:  0.2749759554862976
train gradient:  0.23270237646860767
iteration : 5967
train acc:  0.890625
train loss:  0.32739612460136414
train gradient:  0.21397523415960862
iteration : 5968
train acc:  0.8984375
train loss:  0.31648099422454834
train gradient:  0.15136164778528693
iteration : 5969
train acc:  0.84375
train loss:  0.3526924252510071
train gradient:  0.19333300395106573
iteration : 5970
train acc:  0.84375
train loss:  0.35447901487350464
train gradient:  0.3057953606762021
iteration : 5971
train acc:  0.9140625
train loss:  0.27012038230895996
train gradient:  0.15811432603621384
iteration : 5972
train acc:  0.84375
train loss:  0.3158426284790039
train gradient:  0.1513870654761445
iteration : 5973
train acc:  0.875
train loss:  0.28420984745025635
train gradient:  0.18035522590759404
iteration : 5974
train acc:  0.828125
train loss:  0.34883591532707214
train gradient:  0.19757041318721053
iteration : 5975
train acc:  0.8515625
train loss:  0.3737940192222595
train gradient:  0.2623259240160005
iteration : 5976
train acc:  0.8359375
train loss:  0.37568342685699463
train gradient:  0.2002685319452498
iteration : 5977
train acc:  0.8203125
train loss:  0.35074666142463684
train gradient:  0.26404589074699414
iteration : 5978
train acc:  0.859375
train loss:  0.3508951961994171
train gradient:  0.20954206713988863
iteration : 5979
train acc:  0.859375
train loss:  0.30817142128944397
train gradient:  0.1595541561442298
iteration : 5980
train acc:  0.828125
train loss:  0.3644524812698364
train gradient:  0.25361501163087125
iteration : 5981
train acc:  0.8203125
train loss:  0.3651496171951294
train gradient:  0.2288471504377929
iteration : 5982
train acc:  0.890625
train loss:  0.2818913459777832
train gradient:  0.1352445095988613
iteration : 5983
train acc:  0.8828125
train loss:  0.3184615671634674
train gradient:  0.1347395740502418
iteration : 5984
train acc:  0.8203125
train loss:  0.3243453800678253
train gradient:  0.17968245627065146
iteration : 5985
train acc:  0.8671875
train loss:  0.3224906027317047
train gradient:  0.22246181714022104
iteration : 5986
train acc:  0.84375
train loss:  0.3563731014728546
train gradient:  0.20580148081793043
iteration : 5987
train acc:  0.859375
train loss:  0.3120049834251404
train gradient:  0.43517953663082487
iteration : 5988
train acc:  0.8046875
train loss:  0.3742016553878784
train gradient:  0.2357696176695001
iteration : 5989
train acc:  0.828125
train loss:  0.35092729330062866
train gradient:  0.27757067687502324
iteration : 5990
train acc:  0.90625
train loss:  0.2688571810722351
train gradient:  0.15732996166045948
iteration : 5991
train acc:  0.8671875
train loss:  0.29564863443374634
train gradient:  0.2235568303709307
iteration : 5992
train acc:  0.8203125
train loss:  0.42790186405181885
train gradient:  0.33642073293247404
iteration : 5993
train acc:  0.8046875
train loss:  0.40001124143600464
train gradient:  0.351153856368038
iteration : 5994
train acc:  0.859375
train loss:  0.3236471712589264
train gradient:  0.27656166496575324
iteration : 5995
train acc:  0.8515625
train loss:  0.3011878430843353
train gradient:  0.16202296172055336
iteration : 5996
train acc:  0.8671875
train loss:  0.35021713376045227
train gradient:  0.24492793429943716
iteration : 5997
train acc:  0.859375
train loss:  0.3323637843132019
train gradient:  0.20584220907766132
iteration : 5998
train acc:  0.8515625
train loss:  0.34670037031173706
train gradient:  0.19988083156976888
iteration : 5999
train acc:  0.8359375
train loss:  0.342637300491333
train gradient:  0.23805162319212697
iteration : 6000
train acc:  0.8203125
train loss:  0.3895430564880371
train gradient:  0.2771639735559357
iteration : 6001
train acc:  0.8828125
train loss:  0.2946094572544098
train gradient:  0.4123170574522621
iteration : 6002
train acc:  0.90625
train loss:  0.29248666763305664
train gradient:  0.12103429486557536
iteration : 6003
train acc:  0.8828125
train loss:  0.2592306137084961
train gradient:  0.17245599004148318
iteration : 6004
train acc:  0.9140625
train loss:  0.33793506026268005
train gradient:  0.1776946547150892
iteration : 6005
train acc:  0.8515625
train loss:  0.31280046701431274
train gradient:  0.2621435729749439
iteration : 6006
train acc:  0.859375
train loss:  0.30107954144477844
train gradient:  0.3296244626116401
iteration : 6007
train acc:  0.8984375
train loss:  0.26244258880615234
train gradient:  0.1188755751748029
iteration : 6008
train acc:  0.828125
train loss:  0.37456023693084717
train gradient:  0.23357668783762925
iteration : 6009
train acc:  0.859375
train loss:  0.35229575634002686
train gradient:  0.22982618846246416
iteration : 6010
train acc:  0.8515625
train loss:  0.3130899667739868
train gradient:  0.1802763455011962
iteration : 6011
train acc:  0.8515625
train loss:  0.38684388995170593
train gradient:  0.183389728439182
iteration : 6012
train acc:  0.8515625
train loss:  0.339383989572525
train gradient:  0.2792663307746176
iteration : 6013
train acc:  0.8359375
train loss:  0.3409479558467865
train gradient:  0.2748759487258476
iteration : 6014
train acc:  0.8515625
train loss:  0.2956799268722534
train gradient:  0.14439949798895868
iteration : 6015
train acc:  0.90625
train loss:  0.24947574734687805
train gradient:  0.12401077804573597
iteration : 6016
train acc:  0.859375
train loss:  0.33060944080352783
train gradient:  0.18964881303327508
iteration : 6017
train acc:  0.8671875
train loss:  0.4248659014701843
train gradient:  0.2892488869954268
iteration : 6018
train acc:  0.7890625
train loss:  0.4646682143211365
train gradient:  0.4468488183848947
iteration : 6019
train acc:  0.828125
train loss:  0.3673296272754669
train gradient:  0.2780814403614283
iteration : 6020
train acc:  0.796875
train loss:  0.44129741191864014
train gradient:  0.32662044870771634
iteration : 6021
train acc:  0.8984375
train loss:  0.2979065179824829
train gradient:  0.12413262115085377
iteration : 6022
train acc:  0.8671875
train loss:  0.35026055574417114
train gradient:  0.26900962610244655
iteration : 6023
train acc:  0.8359375
train loss:  0.38228335976600647
train gradient:  0.2157902776131141
iteration : 6024
train acc:  0.8671875
train loss:  0.33626413345336914
train gradient:  0.18818538161019033
iteration : 6025
train acc:  0.8515625
train loss:  0.32449227571487427
train gradient:  0.15318961252202518
iteration : 6026
train acc:  0.84375
train loss:  0.346199631690979
train gradient:  0.21290422030788353
iteration : 6027
train acc:  0.796875
train loss:  0.3617591857910156
train gradient:  0.2299758690403344
iteration : 6028
train acc:  0.84375
train loss:  0.28524482250213623
train gradient:  0.2871268180599833
iteration : 6029
train acc:  0.8671875
train loss:  0.3466719388961792
train gradient:  0.19745687828106745
iteration : 6030
train acc:  0.8984375
train loss:  0.31408488750457764
train gradient:  0.1669493689555872
iteration : 6031
train acc:  0.78125
train loss:  0.42543086409568787
train gradient:  0.34087181247180726
iteration : 6032
train acc:  0.8359375
train loss:  0.34482622146606445
train gradient:  0.17786275820147257
iteration : 6033
train acc:  0.8828125
train loss:  0.2980976104736328
train gradient:  0.1183098914974183
iteration : 6034
train acc:  0.796875
train loss:  0.4582885801792145
train gradient:  0.38095761364583447
iteration : 6035
train acc:  0.875
train loss:  0.2725510597229004
train gradient:  0.17474352401173637
iteration : 6036
train acc:  0.8828125
train loss:  0.2551204562187195
train gradient:  0.1616134350651689
iteration : 6037
train acc:  0.875
train loss:  0.3525843918323517
train gradient:  0.21819329843698856
iteration : 6038
train acc:  0.84375
train loss:  0.40183568000793457
train gradient:  0.2428579506836846
iteration : 6039
train acc:  0.8359375
train loss:  0.3973711133003235
train gradient:  0.24985121409917038
iteration : 6040
train acc:  0.8125
train loss:  0.4005645215511322
train gradient:  0.36350539261363995
iteration : 6041
train acc:  0.875
train loss:  0.26440635323524475
train gradient:  0.208419403105434
iteration : 6042
train acc:  0.8515625
train loss:  0.2879709303379059
train gradient:  0.16183449633052804
iteration : 6043
train acc:  0.8359375
train loss:  0.35561683773994446
train gradient:  0.22710301290980267
iteration : 6044
train acc:  0.78125
train loss:  0.43804165720939636
train gradient:  0.2766065061534547
iteration : 6045
train acc:  0.875
train loss:  0.341275155544281
train gradient:  0.2181946306813497
iteration : 6046
train acc:  0.828125
train loss:  0.35832399129867554
train gradient:  0.17312564352228493
iteration : 6047
train acc:  0.828125
train loss:  0.427584707736969
train gradient:  0.2773658156895498
iteration : 6048
train acc:  0.9140625
train loss:  0.23127496242523193
train gradient:  0.14832353884928934
iteration : 6049
train acc:  0.875
train loss:  0.2891453504562378
train gradient:  0.1959279986623612
iteration : 6050
train acc:  0.8515625
train loss:  0.30502843856811523
train gradient:  0.21146081956456425
iteration : 6051
train acc:  0.828125
train loss:  0.35307005047798157
train gradient:  0.28287090659678454
iteration : 6052
train acc:  0.8515625
train loss:  0.3396397829055786
train gradient:  0.31081909126238694
iteration : 6053
train acc:  0.8515625
train loss:  0.28907501697540283
train gradient:  0.17591072829139875
iteration : 6054
train acc:  0.8671875
train loss:  0.30333226919174194
train gradient:  0.14291648365565923
iteration : 6055
train acc:  0.875
train loss:  0.2995385229587555
train gradient:  0.15267410881278248
iteration : 6056
train acc:  0.859375
train loss:  0.40533286333084106
train gradient:  0.2839608286056009
iteration : 6057
train acc:  0.828125
train loss:  0.3967982530593872
train gradient:  0.26735663373801016
iteration : 6058
train acc:  0.8828125
train loss:  0.2980375587940216
train gradient:  0.23737714109783242
iteration : 6059
train acc:  0.8046875
train loss:  0.4680136740207672
train gradient:  0.37174282868025155
iteration : 6060
train acc:  0.8671875
train loss:  0.3254106938838959
train gradient:  0.1619037380550828
iteration : 6061
train acc:  0.8203125
train loss:  0.4078838527202606
train gradient:  0.24207112965489042
iteration : 6062
train acc:  0.8125
train loss:  0.34972989559173584
train gradient:  0.3007541133266323
iteration : 6063
train acc:  0.8671875
train loss:  0.34117817878723145
train gradient:  0.18520036079929186
iteration : 6064
train acc:  0.84375
train loss:  0.33231431245803833
train gradient:  0.1979816963464631
iteration : 6065
train acc:  0.8125
train loss:  0.35935327410697937
train gradient:  0.24310395985966682
iteration : 6066
train acc:  0.8984375
train loss:  0.27545201778411865
train gradient:  0.13568448389717414
iteration : 6067
train acc:  0.8515625
train loss:  0.37663865089416504
train gradient:  0.2369468372782102
iteration : 6068
train acc:  0.84375
train loss:  0.35774075984954834
train gradient:  0.21822304512932267
iteration : 6069
train acc:  0.8359375
train loss:  0.35087114572525024
train gradient:  0.1697482329466606
iteration : 6070
train acc:  0.8359375
train loss:  0.40117958188056946
train gradient:  0.2758142593935406
iteration : 6071
train acc:  0.84375
train loss:  0.31729060411453247
train gradient:  0.2235377431692358
iteration : 6072
train acc:  0.8828125
train loss:  0.336273729801178
train gradient:  0.18573626178699482
iteration : 6073
train acc:  0.8359375
train loss:  0.36291956901550293
train gradient:  0.2283796867480165
iteration : 6074
train acc:  0.8515625
train loss:  0.337103933095932
train gradient:  0.19776174864041554
iteration : 6075
train acc:  0.859375
train loss:  0.3721672296524048
train gradient:  0.2712220543942183
iteration : 6076
train acc:  0.8046875
train loss:  0.3964707553386688
train gradient:  0.3295950782173954
iteration : 6077
train acc:  0.859375
train loss:  0.2849480211734772
train gradient:  0.12038005361546916
iteration : 6078
train acc:  0.8359375
train loss:  0.4156491756439209
train gradient:  0.24785437097526586
iteration : 6079
train acc:  0.8359375
train loss:  0.33550745248794556
train gradient:  0.23081749516101632
iteration : 6080
train acc:  0.859375
train loss:  0.33000534772872925
train gradient:  0.15325922764290711
iteration : 6081
train acc:  0.828125
train loss:  0.4532598853111267
train gradient:  0.3238271202783913
iteration : 6082
train acc:  0.8671875
train loss:  0.3178451657295227
train gradient:  0.21664913605462355
iteration : 6083
train acc:  0.8828125
train loss:  0.24818208813667297
train gradient:  0.21019104859823168
iteration : 6084
train acc:  0.8359375
train loss:  0.3757663667201996
train gradient:  0.21308375988511408
iteration : 6085
train acc:  0.8359375
train loss:  0.36415016651153564
train gradient:  0.1860053453092352
iteration : 6086
train acc:  0.8515625
train loss:  0.36152321100234985
train gradient:  0.19657824918870911
iteration : 6087
train acc:  0.8359375
train loss:  0.3549765944480896
train gradient:  0.16656927338013167
iteration : 6088
train acc:  0.890625
train loss:  0.2611148953437805
train gradient:  0.14527707076916554
iteration : 6089
train acc:  0.8671875
train loss:  0.310363233089447
train gradient:  0.3486506408892204
iteration : 6090
train acc:  0.8515625
train loss:  0.34318995475769043
train gradient:  0.20939676299753768
iteration : 6091
train acc:  0.921875
train loss:  0.2624841332435608
train gradient:  0.15359263913155177
iteration : 6092
train acc:  0.84375
train loss:  0.315929651260376
train gradient:  0.16520486333641404
iteration : 6093
train acc:  0.765625
train loss:  0.504382848739624
train gradient:  0.455765055260531
iteration : 6094
train acc:  0.8359375
train loss:  0.3030652701854706
train gradient:  0.2067004976443412
iteration : 6095
train acc:  0.859375
train loss:  0.32266706228256226
train gradient:  0.24029020624163286
iteration : 6096
train acc:  0.859375
train loss:  0.31061074137687683
train gradient:  0.21040264227278505
iteration : 6097
train acc:  0.8125
train loss:  0.413819819688797
train gradient:  0.24713698085922828
iteration : 6098
train acc:  0.796875
train loss:  0.3746913969516754
train gradient:  0.2198478361562399
iteration : 6099
train acc:  0.828125
train loss:  0.32442694902420044
train gradient:  0.322391754459897
iteration : 6100
train acc:  0.828125
train loss:  0.37352102994918823
train gradient:  0.19550100007974602
iteration : 6101
train acc:  0.859375
train loss:  0.4139922857284546
train gradient:  0.33707789027722435
iteration : 6102
train acc:  0.828125
train loss:  0.3609928786754608
train gradient:  0.24398052517845348
iteration : 6103
train acc:  0.8828125
train loss:  0.3080523610115051
train gradient:  0.2757930272435358
iteration : 6104
train acc:  0.8125
train loss:  0.3582633435726166
train gradient:  0.1901343876050377
iteration : 6105
train acc:  0.9375
train loss:  0.2425474226474762
train gradient:  0.16577515209314586
iteration : 6106
train acc:  0.8125
train loss:  0.3595576584339142
train gradient:  0.20610650122296817
iteration : 6107
train acc:  0.890625
train loss:  0.2830352783203125
train gradient:  0.20239507177929375
iteration : 6108
train acc:  0.9375
train loss:  0.2586461901664734
train gradient:  0.1542796329652266
iteration : 6109
train acc:  0.8671875
train loss:  0.28497469425201416
train gradient:  0.17902794122769827
iteration : 6110
train acc:  0.8359375
train loss:  0.3492959141731262
train gradient:  0.2076867778537457
iteration : 6111
train acc:  0.84375
train loss:  0.36774253845214844
train gradient:  0.26302627950950974
iteration : 6112
train acc:  0.8046875
train loss:  0.4421294629573822
train gradient:  0.3408981357813222
iteration : 6113
train acc:  0.8671875
train loss:  0.3052462935447693
train gradient:  0.15499782979144056
iteration : 6114
train acc:  0.8671875
train loss:  0.28461557626724243
train gradient:  0.16873248589774398
iteration : 6115
train acc:  0.859375
train loss:  0.31427618861198425
train gradient:  0.2158423105620345
iteration : 6116
train acc:  0.8828125
train loss:  0.36204132437705994
train gradient:  0.27611300172948755
iteration : 6117
train acc:  0.8984375
train loss:  0.27624109387397766
train gradient:  0.17082887783886314
iteration : 6118
train acc:  0.8515625
train loss:  0.3282160758972168
train gradient:  0.24633256940105933
iteration : 6119
train acc:  0.828125
train loss:  0.3568040132522583
train gradient:  0.3492247696491451
iteration : 6120
train acc:  0.8359375
train loss:  0.3418009281158447
train gradient:  0.22526687063573678
iteration : 6121
train acc:  0.8671875
train loss:  0.3516264259815216
train gradient:  0.28631733817262717
iteration : 6122
train acc:  0.8359375
train loss:  0.3248339891433716
train gradient:  0.2430838831249965
iteration : 6123
train acc:  0.8046875
train loss:  0.4010476768016815
train gradient:  0.33263313624611834
iteration : 6124
train acc:  0.859375
train loss:  0.31087854504585266
train gradient:  0.3322668249904486
iteration : 6125
train acc:  0.84375
train loss:  0.3619624376296997
train gradient:  0.18407486446392876
iteration : 6126
train acc:  0.859375
train loss:  0.3275361955165863
train gradient:  0.21578940250371192
iteration : 6127
train acc:  0.859375
train loss:  0.3073708415031433
train gradient:  0.18102933988351055
iteration : 6128
train acc:  0.8359375
train loss:  0.34301093220710754
train gradient:  0.2932362011142153
iteration : 6129
train acc:  0.8515625
train loss:  0.32641899585723877
train gradient:  0.18877697138836053
iteration : 6130
train acc:  0.828125
train loss:  0.38418930768966675
train gradient:  0.3008614851386121
iteration : 6131
train acc:  0.8359375
train loss:  0.36767297983169556
train gradient:  0.23261341635463315
iteration : 6132
train acc:  0.8203125
train loss:  0.4302431046962738
train gradient:  0.3470347190349843
iteration : 6133
train acc:  0.875
train loss:  0.31840813159942627
train gradient:  0.21449690161546991
iteration : 6134
train acc:  0.890625
train loss:  0.2891448140144348
train gradient:  0.1978275343523483
iteration : 6135
train acc:  0.8828125
train loss:  0.34275200963020325
train gradient:  0.22820818187157277
iteration : 6136
train acc:  0.8203125
train loss:  0.42142534255981445
train gradient:  0.4509075191644615
iteration : 6137
train acc:  0.8046875
train loss:  0.38163095712661743
train gradient:  0.22923294767132338
iteration : 6138
train acc:  0.875
train loss:  0.329941064119339
train gradient:  0.21713562755051202
iteration : 6139
train acc:  0.859375
train loss:  0.31790751218795776
train gradient:  0.20888511967752868
iteration : 6140
train acc:  0.8828125
train loss:  0.28420788049697876
train gradient:  0.12811670639932396
iteration : 6141
train acc:  0.84375
train loss:  0.3441472053527832
train gradient:  0.16198579170568456
iteration : 6142
train acc:  0.796875
train loss:  0.43472209572792053
train gradient:  0.2841050966380144
iteration : 6143
train acc:  0.859375
train loss:  0.318787157535553
train gradient:  0.1782455660099167
iteration : 6144
train acc:  0.828125
train loss:  0.3357894718647003
train gradient:  0.24250760250910558
iteration : 6145
train acc:  0.8828125
train loss:  0.34087735414505005
train gradient:  0.17976624971686614
iteration : 6146
train acc:  0.84375
train loss:  0.37477973103523254
train gradient:  0.25551634164786813
iteration : 6147
train acc:  0.84375
train loss:  0.3680003881454468
train gradient:  0.2218875962532264
iteration : 6148
train acc:  0.84375
train loss:  0.3774133324623108
train gradient:  0.22304591514293975
iteration : 6149
train acc:  0.78125
train loss:  0.4117196202278137
train gradient:  0.3432189670007096
iteration : 6150
train acc:  0.765625
train loss:  0.4529035687446594
train gradient:  0.3238853072691633
iteration : 6151
train acc:  0.796875
train loss:  0.4527176320552826
train gradient:  0.343992105841467
iteration : 6152
train acc:  0.8671875
train loss:  0.28762558102607727
train gradient:  0.20862299278103658
iteration : 6153
train acc:  0.8125
train loss:  0.3864046335220337
train gradient:  0.2353648604277661
iteration : 6154
train acc:  0.84375
train loss:  0.3581874966621399
train gradient:  0.36173325106742926
iteration : 6155
train acc:  0.8984375
train loss:  0.341854065656662
train gradient:  0.22581621719447473
iteration : 6156
train acc:  0.8671875
train loss:  0.31046345829963684
train gradient:  0.1917631500814404
iteration : 6157
train acc:  0.875
train loss:  0.3051563501358032
train gradient:  0.1746732016463627
iteration : 6158
train acc:  0.875
train loss:  0.3221350312232971
train gradient:  0.20057698421368586
iteration : 6159
train acc:  0.84375
train loss:  0.3888411223888397
train gradient:  0.22970061634873903
iteration : 6160
train acc:  0.9296875
train loss:  0.25589361786842346
train gradient:  0.16043341977539027
iteration : 6161
train acc:  0.8671875
train loss:  0.3856188952922821
train gradient:  0.19574713103223532
iteration : 6162
train acc:  0.7734375
train loss:  0.41079509258270264
train gradient:  0.2662489886464469
iteration : 6163
train acc:  0.8359375
train loss:  0.3847312331199646
train gradient:  0.23089277244836331
iteration : 6164
train acc:  0.8671875
train loss:  0.32714784145355225
train gradient:  0.17755838231539348
iteration : 6165
train acc:  0.859375
train loss:  0.3821544051170349
train gradient:  0.44999620987467587
iteration : 6166
train acc:  0.859375
train loss:  0.34164905548095703
train gradient:  0.2101279007239304
iteration : 6167
train acc:  0.8359375
train loss:  0.35303473472595215
train gradient:  0.24695369128065386
iteration : 6168
train acc:  0.8359375
train loss:  0.3569399118423462
train gradient:  0.3580016917700197
iteration : 6169
train acc:  0.890625
train loss:  0.26523709297180176
train gradient:  0.18255019937225078
iteration : 6170
train acc:  0.8125
train loss:  0.4326079785823822
train gradient:  0.397815409226165
iteration : 6171
train acc:  0.8046875
train loss:  0.343727171421051
train gradient:  0.20632799471498203
iteration : 6172
train acc:  0.8984375
train loss:  0.27564743161201477
train gradient:  0.1504718828243195
iteration : 6173
train acc:  0.8046875
train loss:  0.38122549653053284
train gradient:  0.21739573994290823
iteration : 6174
train acc:  0.84375
train loss:  0.36566078662872314
train gradient:  0.30528399014008767
iteration : 6175
train acc:  0.875
train loss:  0.3013025224208832
train gradient:  0.15549791533925159
iteration : 6176
train acc:  0.875
train loss:  0.3313206434249878
train gradient:  0.2132503306387499
iteration : 6177
train acc:  0.8671875
train loss:  0.34238505363464355
train gradient:  0.13817843030746027
iteration : 6178
train acc:  0.8203125
train loss:  0.34460216760635376
train gradient:  0.1692982037116928
iteration : 6179
train acc:  0.875
train loss:  0.3828262388706207
train gradient:  0.3661752883517583
iteration : 6180
train acc:  0.8125
train loss:  0.3785327672958374
train gradient:  0.23022851563644398
iteration : 6181
train acc:  0.8671875
train loss:  0.34651732444763184
train gradient:  0.18681943850901772
iteration : 6182
train acc:  0.875
train loss:  0.26763927936553955
train gradient:  0.16459341648119158
iteration : 6183
train acc:  0.8671875
train loss:  0.30793261528015137
train gradient:  0.17937166617656927
iteration : 6184
train acc:  0.875
train loss:  0.3010315001010895
train gradient:  0.16699205094610187
iteration : 6185
train acc:  0.8515625
train loss:  0.30807822942733765
train gradient:  0.19555092665710513
iteration : 6186
train acc:  0.8203125
train loss:  0.36618614196777344
train gradient:  0.2351931292679517
iteration : 6187
train acc:  0.8359375
train loss:  0.38667914271354675
train gradient:  0.33645482991669823
iteration : 6188
train acc:  0.828125
train loss:  0.39409422874450684
train gradient:  0.2175431549164494
iteration : 6189
train acc:  0.8125
train loss:  0.31223052740097046
train gradient:  0.2081641564234532
iteration : 6190
train acc:  0.84375
train loss:  0.3385196924209595
train gradient:  0.19107529437179366
iteration : 6191
train acc:  0.8046875
train loss:  0.39573317766189575
train gradient:  0.19144581880435177
iteration : 6192
train acc:  0.8046875
train loss:  0.3284522294998169
train gradient:  0.14604781100267045
iteration : 6193
train acc:  0.84375
train loss:  0.3253953456878662
train gradient:  0.20932704832675575
iteration : 6194
train acc:  0.90625
train loss:  0.2875671982765198
train gradient:  0.2723460152078078
iteration : 6195
train acc:  0.8046875
train loss:  0.438039630651474
train gradient:  0.2628491221830874
iteration : 6196
train acc:  0.8203125
train loss:  0.37518179416656494
train gradient:  0.3934510964046495
iteration : 6197
train acc:  0.890625
train loss:  0.2682844400405884
train gradient:  0.1484175103854847
iteration : 6198
train acc:  0.8203125
train loss:  0.36727502942085266
train gradient:  0.19215709468296696
iteration : 6199
train acc:  0.8984375
train loss:  0.265007883310318
train gradient:  0.14193781172320818
iteration : 6200
train acc:  0.90625
train loss:  0.23339179158210754
train gradient:  0.1167821924254428
iteration : 6201
train acc:  0.8203125
train loss:  0.5073903799057007
train gradient:  0.3405761295208846
iteration : 6202
train acc:  0.8515625
train loss:  0.3764652609825134
train gradient:  0.17014887078029714
iteration : 6203
train acc:  0.8125
train loss:  0.39518505334854126
train gradient:  0.23383275027291792
iteration : 6204
train acc:  0.8203125
train loss:  0.3419320285320282
train gradient:  0.20949218869070163
iteration : 6205
train acc:  0.8515625
train loss:  0.3301972448825836
train gradient:  0.2320621178712009
iteration : 6206
train acc:  0.84375
train loss:  0.39576607942581177
train gradient:  0.26231345676820994
iteration : 6207
train acc:  0.8984375
train loss:  0.29569077491760254
train gradient:  0.2765020470841451
iteration : 6208
train acc:  0.859375
train loss:  0.3302028775215149
train gradient:  0.4221440017771186
iteration : 6209
train acc:  0.84375
train loss:  0.3824998438358307
train gradient:  0.2603108548196181
iteration : 6210
train acc:  0.859375
train loss:  0.29531434178352356
train gradient:  0.1768310987442374
iteration : 6211
train acc:  0.7734375
train loss:  0.42539140582084656
train gradient:  0.3263698078336844
iteration : 6212
train acc:  0.828125
train loss:  0.3984401226043701
train gradient:  0.28050128165397625
iteration : 6213
train acc:  0.8671875
train loss:  0.30574044585227966
train gradient:  0.14346480984667767
iteration : 6214
train acc:  0.8203125
train loss:  0.36362528800964355
train gradient:  0.25205791681656975
iteration : 6215
train acc:  0.8515625
train loss:  0.3108685612678528
train gradient:  0.15853855547877221
iteration : 6216
train acc:  0.828125
train loss:  0.4336012601852417
train gradient:  0.24772043162871055
iteration : 6217
train acc:  0.8359375
train loss:  0.35659000277519226
train gradient:  0.23696697258695873
iteration : 6218
train acc:  0.9296875
train loss:  0.2744176387786865
train gradient:  0.12724620638653303
iteration : 6219
train acc:  0.875
train loss:  0.3188949525356293
train gradient:  0.1650411502769631
iteration : 6220
train acc:  0.8046875
train loss:  0.3648080825805664
train gradient:  0.2193481431393194
iteration : 6221
train acc:  0.859375
train loss:  0.3772357106208801
train gradient:  0.2431020571549593
iteration : 6222
train acc:  0.8203125
train loss:  0.4266107380390167
train gradient:  0.2557825265960783
iteration : 6223
train acc:  0.8125
train loss:  0.47280949354171753
train gradient:  0.5929261611077706
iteration : 6224
train acc:  0.8203125
train loss:  0.36620450019836426
train gradient:  0.2219888545252991
iteration : 6225
train acc:  0.8359375
train loss:  0.3157687187194824
train gradient:  0.1949207622057399
iteration : 6226
train acc:  0.859375
train loss:  0.3172720968723297
train gradient:  0.13272646206872668
iteration : 6227
train acc:  0.875
train loss:  0.3199945092201233
train gradient:  0.17760997045739618
iteration : 6228
train acc:  0.859375
train loss:  0.30695870518684387
train gradient:  0.1518467572510095
iteration : 6229
train acc:  0.8671875
train loss:  0.3401866555213928
train gradient:  0.2612754747750613
iteration : 6230
train acc:  0.8046875
train loss:  0.36266329884529114
train gradient:  0.20673612853956477
iteration : 6231
train acc:  0.8359375
train loss:  0.33445078134536743
train gradient:  0.17413725728460777
iteration : 6232
train acc:  0.8671875
train loss:  0.3392190933227539
train gradient:  0.2089444769744182
iteration : 6233
train acc:  0.8828125
train loss:  0.28649771213531494
train gradient:  0.15799416745534628
iteration : 6234
train acc:  0.859375
train loss:  0.3190980553627014
train gradient:  0.1951592445280631
iteration : 6235
train acc:  0.8984375
train loss:  0.32152798771858215
train gradient:  0.22344837505740295
iteration : 6236
train acc:  0.8671875
train loss:  0.3387841582298279
train gradient:  0.22614498076993683
iteration : 6237
train acc:  0.859375
train loss:  0.34265944361686707
train gradient:  0.2300443557504184
iteration : 6238
train acc:  0.8359375
train loss:  0.37085580825805664
train gradient:  0.2564690610701715
iteration : 6239
train acc:  0.8828125
train loss:  0.28506648540496826
train gradient:  0.1540149096448652
iteration : 6240
train acc:  0.8984375
train loss:  0.2690126299858093
train gradient:  0.20999986113116836
iteration : 6241
train acc:  0.8203125
train loss:  0.4264140725135803
train gradient:  0.3520069147973968
iteration : 6242
train acc:  0.828125
train loss:  0.3783411681652069
train gradient:  0.24761059915054318
iteration : 6243
train acc:  0.859375
train loss:  0.3404185175895691
train gradient:  0.18780466324394246
iteration : 6244
train acc:  0.8359375
train loss:  0.39754295349121094
train gradient:  0.2631171411556032
iteration : 6245
train acc:  0.8203125
train loss:  0.459389328956604
train gradient:  0.24083925626836084
iteration : 6246
train acc:  0.8359375
train loss:  0.39062148332595825
train gradient:  0.31289777070820957
iteration : 6247
train acc:  0.8515625
train loss:  0.3117351233959198
train gradient:  0.1669007440758561
iteration : 6248
train acc:  0.8515625
train loss:  0.3094055950641632
train gradient:  0.15811046321780275
iteration : 6249
train acc:  0.8515625
train loss:  0.37163037061691284
train gradient:  0.2900715445025145
iteration : 6250
train acc:  0.8125
train loss:  0.3516315221786499
train gradient:  0.2470042557188241
iteration : 6251
train acc:  0.8515625
train loss:  0.3235663175582886
train gradient:  0.2263638267767175
iteration : 6252
train acc:  0.796875
train loss:  0.418083131313324
train gradient:  0.2897788214771506
iteration : 6253
train acc:  0.890625
train loss:  0.2722901403903961
train gradient:  0.13097708334684252
iteration : 6254
train acc:  0.859375
train loss:  0.29570770263671875
train gradient:  0.2017041833420007
iteration : 6255
train acc:  0.890625
train loss:  0.2724648714065552
train gradient:  0.16905746558580753
iteration : 6256
train acc:  0.859375
train loss:  0.3521580696105957
train gradient:  0.21046606681187854
iteration : 6257
train acc:  0.8359375
train loss:  0.3369242548942566
train gradient:  0.2251504671222449
iteration : 6258
train acc:  0.859375
train loss:  0.33818620443344116
train gradient:  0.17564660124585385
iteration : 6259
train acc:  0.8203125
train loss:  0.39341944456100464
train gradient:  0.39622586230884016
iteration : 6260
train acc:  0.875
train loss:  0.2667151093482971
train gradient:  0.15572576025797252
iteration : 6261
train acc:  0.84375
train loss:  0.37705519795417786
train gradient:  0.20042829717370525
iteration : 6262
train acc:  0.8046875
train loss:  0.3797205686569214
train gradient:  0.2165351899103548
iteration : 6263
train acc:  0.84375
train loss:  0.309353768825531
train gradient:  0.221575969741609
iteration : 6264
train acc:  0.78125
train loss:  0.4195854961872101
train gradient:  0.36732178945325933
iteration : 6265
train acc:  0.84375
train loss:  0.3247756361961365
train gradient:  0.21967671886342066
iteration : 6266
train acc:  0.8203125
train loss:  0.34602266550064087
train gradient:  0.17561821399525634
iteration : 6267
train acc:  0.875
train loss:  0.30056890845298767
train gradient:  0.13210028779616978
iteration : 6268
train acc:  0.859375
train loss:  0.3079604506492615
train gradient:  0.12752502967575874
iteration : 6269
train acc:  0.875
train loss:  0.3271876573562622
train gradient:  0.2727624678378276
iteration : 6270
train acc:  0.78125
train loss:  0.46382105350494385
train gradient:  0.31618720724087057
iteration : 6271
train acc:  0.859375
train loss:  0.3359845280647278
train gradient:  0.1838232726631396
iteration : 6272
train acc:  0.8671875
train loss:  0.2770901322364807
train gradient:  0.2404387653555083
iteration : 6273
train acc:  0.7734375
train loss:  0.43060845136642456
train gradient:  0.24160474447793995
iteration : 6274
train acc:  0.890625
train loss:  0.32924455404281616
train gradient:  0.1855430685037079
iteration : 6275
train acc:  0.796875
train loss:  0.39536523818969727
train gradient:  0.2253227300054677
iteration : 6276
train acc:  0.765625
train loss:  0.47410887479782104
train gradient:  0.4405885622474312
iteration : 6277
train acc:  0.8359375
train loss:  0.34223997592926025
train gradient:  0.16935487802174917
iteration : 6278
train acc:  0.8828125
train loss:  0.32973331212997437
train gradient:  0.1618657355759221
iteration : 6279
train acc:  0.828125
train loss:  0.32525867223739624
train gradient:  0.13890528807677183
iteration : 6280
train acc:  0.796875
train loss:  0.45927947759628296
train gradient:  0.40986709863286025
iteration : 6281
train acc:  0.8359375
train loss:  0.3371553421020508
train gradient:  0.13545069737006832
iteration : 6282
train acc:  0.84375
train loss:  0.42220187187194824
train gradient:  0.35622012596483194
iteration : 6283
train acc:  0.84375
train loss:  0.3287760019302368
train gradient:  0.21436018626421816
iteration : 6284
train acc:  0.8984375
train loss:  0.2841683626174927
train gradient:  0.1537514935157684
iteration : 6285
train acc:  0.828125
train loss:  0.39470887184143066
train gradient:  0.314746445526546
iteration : 6286
train acc:  0.796875
train loss:  0.44446760416030884
train gradient:  0.4552115901750118
iteration : 6287
train acc:  0.859375
train loss:  0.27200883626937866
train gradient:  0.13598110137394862
iteration : 6288
train acc:  0.859375
train loss:  0.37033361196517944
train gradient:  0.2959739621517147
iteration : 6289
train acc:  0.84375
train loss:  0.3296520709991455
train gradient:  0.25009521739893376
iteration : 6290
train acc:  0.8046875
train loss:  0.3848053216934204
train gradient:  0.25733623704684794
iteration : 6291
train acc:  0.8125
train loss:  0.354461133480072
train gradient:  0.2280448640178076
iteration : 6292
train acc:  0.8046875
train loss:  0.33323657512664795
train gradient:  0.14913889280589848
iteration : 6293
train acc:  0.8125
train loss:  0.3919193148612976
train gradient:  0.23439532705968233
iteration : 6294
train acc:  0.828125
train loss:  0.3594353199005127
train gradient:  0.2039679755245043
iteration : 6295
train acc:  0.890625
train loss:  0.286342978477478
train gradient:  0.18395845699098928
iteration : 6296
train acc:  0.8515625
train loss:  0.3092917799949646
train gradient:  0.15542855696669616
iteration : 6297
train acc:  0.890625
train loss:  0.27738940715789795
train gradient:  0.20827137123338713
iteration : 6298
train acc:  0.84375
train loss:  0.382582426071167
train gradient:  0.2353523460226995
iteration : 6299
train acc:  0.84375
train loss:  0.3296247124671936
train gradient:  0.17885461175476142
iteration : 6300
train acc:  0.8125
train loss:  0.38333451747894287
train gradient:  0.2071670172747164
iteration : 6301
train acc:  0.8671875
train loss:  0.3482095003128052
train gradient:  0.22553544117628682
iteration : 6302
train acc:  0.859375
train loss:  0.31117522716522217
train gradient:  0.14673575338756864
iteration : 6303
train acc:  0.8203125
train loss:  0.3947063088417053
train gradient:  0.2591459387287089
iteration : 6304
train acc:  0.8046875
train loss:  0.45241135358810425
train gradient:  0.33644384445129993
iteration : 6305
train acc:  0.8203125
train loss:  0.3418351411819458
train gradient:  0.2407062973292856
iteration : 6306
train acc:  0.8984375
train loss:  0.2827589809894562
train gradient:  0.15788070460337478
iteration : 6307
train acc:  0.8359375
train loss:  0.3791595697402954
train gradient:  0.21380274121298817
iteration : 6308
train acc:  0.8515625
train loss:  0.39024847745895386
train gradient:  0.20280865711786825
iteration : 6309
train acc:  0.859375
train loss:  0.3542065918445587
train gradient:  0.2226104523968121
iteration : 6310
train acc:  0.8046875
train loss:  0.3866117000579834
train gradient:  0.22378686368281
iteration : 6311
train acc:  0.8125
train loss:  0.3420500159263611
train gradient:  0.23152404066397003
iteration : 6312
train acc:  0.875
train loss:  0.3267122805118561
train gradient:  0.3005413670033928
iteration : 6313
train acc:  0.84375
train loss:  0.3298676013946533
train gradient:  0.15921123498319875
iteration : 6314
train acc:  0.8828125
train loss:  0.372586190700531
train gradient:  0.1457576342991747
iteration : 6315
train acc:  0.8671875
train loss:  0.28808194398880005
train gradient:  0.19160753014422363
iteration : 6316
train acc:  0.84375
train loss:  0.32854926586151123
train gradient:  0.17905540077846074
iteration : 6317
train acc:  0.7578125
train loss:  0.5246976613998413
train gradient:  0.3785989177051563
iteration : 6318
train acc:  0.8828125
train loss:  0.3118261694908142
train gradient:  0.6572514014414476
iteration : 6319
train acc:  0.8046875
train loss:  0.36173945665359497
train gradient:  0.2542957159137427
iteration : 6320
train acc:  0.875
train loss:  0.3573801517486572
train gradient:  0.19187905607514846
iteration : 6321
train acc:  0.7890625
train loss:  0.445510596036911
train gradient:  0.4556653619174356
iteration : 6322
train acc:  0.9140625
train loss:  0.2702597975730896
train gradient:  0.12735052954638032
iteration : 6323
train acc:  0.8515625
train loss:  0.34747177362442017
train gradient:  0.22666187698215776
iteration : 6324
train acc:  0.8125
train loss:  0.45509791374206543
train gradient:  0.32192916043236985
iteration : 6325
train acc:  0.875
train loss:  0.29933738708496094
train gradient:  0.15613042303519553
iteration : 6326
train acc:  0.875
train loss:  0.3175007700920105
train gradient:  0.1573987348820533
iteration : 6327
train acc:  0.8359375
train loss:  0.33851006627082825
train gradient:  0.1977273137446155
iteration : 6328
train acc:  0.84375
train loss:  0.316784143447876
train gradient:  0.21319831513526977
iteration : 6329
train acc:  0.875
train loss:  0.27634382247924805
train gradient:  0.1878608434479874
iteration : 6330
train acc:  0.84375
train loss:  0.3476138710975647
train gradient:  0.17344254628863803
iteration : 6331
train acc:  0.8515625
train loss:  0.353568434715271
train gradient:  0.2183360356097927
iteration : 6332
train acc:  0.8828125
train loss:  0.33236873149871826
train gradient:  0.17043926176681679
iteration : 6333
train acc:  0.8515625
train loss:  0.32780569791793823
train gradient:  0.20852981543541987
iteration : 6334
train acc:  0.8203125
train loss:  0.3649331331253052
train gradient:  0.2316236570903193
iteration : 6335
train acc:  0.8203125
train loss:  0.3752691149711609
train gradient:  0.24782883237773723
iteration : 6336
train acc:  0.859375
train loss:  0.3266286849975586
train gradient:  0.16669190962089478
iteration : 6337
train acc:  0.890625
train loss:  0.26130133867263794
train gradient:  0.153536643729363
iteration : 6338
train acc:  0.890625
train loss:  0.29589468240737915
train gradient:  0.16507987614123026
iteration : 6339
train acc:  0.890625
train loss:  0.3024637699127197
train gradient:  0.1436253596334312
iteration : 6340
train acc:  0.8671875
train loss:  0.30979448556900024
train gradient:  0.21086023512159022
iteration : 6341
train acc:  0.8359375
train loss:  0.34261536598205566
train gradient:  0.2065352488432448
iteration : 6342
train acc:  0.828125
train loss:  0.31198492646217346
train gradient:  0.2321773343762143
iteration : 6343
train acc:  0.8984375
train loss:  0.23723837733268738
train gradient:  0.16471835201002966
iteration : 6344
train acc:  0.78125
train loss:  0.4370782673358917
train gradient:  0.47630819555379506
iteration : 6345
train acc:  0.828125
train loss:  0.3355284333229065
train gradient:  0.2649472297845955
iteration : 6346
train acc:  0.84375
train loss:  0.3346102237701416
train gradient:  0.1823631061479833
iteration : 6347
train acc:  0.8671875
train loss:  0.30266547203063965
train gradient:  0.19778817145143074
iteration : 6348
train acc:  0.796875
train loss:  0.41250115633010864
train gradient:  0.42524707875873335
iteration : 6349
train acc:  0.8203125
train loss:  0.3895781636238098
train gradient:  0.28493661659633607
iteration : 6350
train acc:  0.8984375
train loss:  0.2617376446723938
train gradient:  0.17085983727259046
iteration : 6351
train acc:  0.8671875
train loss:  0.3205507695674896
train gradient:  0.21393937063989565
iteration : 6352
train acc:  0.8203125
train loss:  0.39195436239242554
train gradient:  0.22308388319321792
iteration : 6353
train acc:  0.7890625
train loss:  0.41394299268722534
train gradient:  0.31324491427733325
iteration : 6354
train acc:  0.796875
train loss:  0.4101570248603821
train gradient:  0.31179065009104406
iteration : 6355
train acc:  0.8828125
train loss:  0.27846187353134155
train gradient:  0.17820618222614476
iteration : 6356
train acc:  0.8984375
train loss:  0.27512943744659424
train gradient:  0.21701220163321666
iteration : 6357
train acc:  0.8828125
train loss:  0.2990284562110901
train gradient:  0.1698708723501479
iteration : 6358
train acc:  0.796875
train loss:  0.3811422288417816
train gradient:  0.23340250829463816
iteration : 6359
train acc:  0.828125
train loss:  0.38267284631729126
train gradient:  0.264447208044038
iteration : 6360
train acc:  0.9140625
train loss:  0.27638837695121765
train gradient:  0.12152558590640658
iteration : 6361
train acc:  0.875
train loss:  0.33493557572364807
train gradient:  0.21876402130660313
iteration : 6362
train acc:  0.875
train loss:  0.3342357575893402
train gradient:  0.21018749044277993
iteration : 6363
train acc:  0.828125
train loss:  0.3665035367012024
train gradient:  0.31081197675403854
iteration : 6364
train acc:  0.8125
train loss:  0.34310024976730347
train gradient:  0.19310181837182167
iteration : 6365
train acc:  0.8671875
train loss:  0.29354578256607056
train gradient:  0.13435970804227537
iteration : 6366
train acc:  0.875
train loss:  0.3186480700969696
train gradient:  0.2737938162840085
iteration : 6367
train acc:  0.8359375
train loss:  0.37302908301353455
train gradient:  0.33912853775048063
iteration : 6368
train acc:  0.8125
train loss:  0.44230180978775024
train gradient:  0.4568180098092329
iteration : 6369
train acc:  0.8359375
train loss:  0.31999659538269043
train gradient:  0.20380185331355766
iteration : 6370
train acc:  0.8515625
train loss:  0.30416440963745117
train gradient:  0.28192466047762255
iteration : 6371
train acc:  0.8359375
train loss:  0.3380858600139618
train gradient:  0.21112312744660316
iteration : 6372
train acc:  0.8046875
train loss:  0.4378088116645813
train gradient:  0.4235093908971176
iteration : 6373
train acc:  0.8671875
train loss:  0.2593350410461426
train gradient:  0.163926063299976
iteration : 6374
train acc:  0.8515625
train loss:  0.3606266379356384
train gradient:  0.2023663812569841
iteration : 6375
train acc:  0.8515625
train loss:  0.3333190083503723
train gradient:  0.20045538504681598
iteration : 6376
train acc:  0.8671875
train loss:  0.27695995569229126
train gradient:  0.22876999908389173
iteration : 6377
train acc:  0.84375
train loss:  0.36749714612960815
train gradient:  0.2755500235132118
iteration : 6378
train acc:  0.859375
train loss:  0.28610098361968994
train gradient:  0.22419905952547337
iteration : 6379
train acc:  0.921875
train loss:  0.24988128244876862
train gradient:  0.12122816106977578
iteration : 6380
train acc:  0.8515625
train loss:  0.30328482389450073
train gradient:  0.1872687881407447
iteration : 6381
train acc:  0.859375
train loss:  0.3179756999015808
train gradient:  0.2147142259158571
iteration : 6382
train acc:  0.8515625
train loss:  0.3996506929397583
train gradient:  0.29975434526856537
iteration : 6383
train acc:  0.8046875
train loss:  0.3820159137248993
train gradient:  0.26815323694621973
iteration : 6384
train acc:  0.8828125
train loss:  0.2943044900894165
train gradient:  0.18104329373524627
iteration : 6385
train acc:  0.8359375
train loss:  0.3751274645328522
train gradient:  0.3866463205084577
iteration : 6386
train acc:  0.8203125
train loss:  0.3693440854549408
train gradient:  0.3340090524539433
iteration : 6387
train acc:  0.8828125
train loss:  0.31829434633255005
train gradient:  0.21286232814744743
iteration : 6388
train acc:  0.8828125
train loss:  0.3788692355155945
train gradient:  0.2503054732976386
iteration : 6389
train acc:  0.875
train loss:  0.30622756481170654
train gradient:  0.165184711455453
iteration : 6390
train acc:  0.859375
train loss:  0.3487195372581482
train gradient:  0.21591646437263928
iteration : 6391
train acc:  0.8828125
train loss:  0.4007105827331543
train gradient:  0.22626018211866647
iteration : 6392
train acc:  0.8359375
train loss:  0.3625430762767792
train gradient:  0.2497179023703887
iteration : 6393
train acc:  0.84375
train loss:  0.37998881936073303
train gradient:  0.22874180705391842
iteration : 6394
train acc:  0.8046875
train loss:  0.40173324942588806
train gradient:  0.34542604390541437
iteration : 6395
train acc:  0.828125
train loss:  0.33664894104003906
train gradient:  0.32076930494894307
iteration : 6396
train acc:  0.8671875
train loss:  0.2882853150367737
train gradient:  0.18031160735111262
iteration : 6397
train acc:  0.84375
train loss:  0.38256746530532837
train gradient:  0.30084240071702534
iteration : 6398
train acc:  0.8828125
train loss:  0.29649949073791504
train gradient:  0.1747973688970674
iteration : 6399
train acc:  0.8984375
train loss:  0.2871084213256836
train gradient:  0.16298045465870015
iteration : 6400
train acc:  0.859375
train loss:  0.33836430311203003
train gradient:  0.3098538081031287
iteration : 6401
train acc:  0.7890625
train loss:  0.4331050217151642
train gradient:  0.30169809299853
iteration : 6402
train acc:  0.8359375
train loss:  0.32477888464927673
train gradient:  0.21382763460687723
iteration : 6403
train acc:  0.8203125
train loss:  0.32496702671051025
train gradient:  0.2526684474417919
iteration : 6404
train acc:  0.7890625
train loss:  0.4170779585838318
train gradient:  0.32316529084116635
iteration : 6405
train acc:  0.8515625
train loss:  0.34608060121536255
train gradient:  0.2554165617892898
iteration : 6406
train acc:  0.765625
train loss:  0.4061524271965027
train gradient:  0.31006223291457385
iteration : 6407
train acc:  0.84375
train loss:  0.3334931433200836
train gradient:  0.28922756350124257
iteration : 6408
train acc:  0.84375
train loss:  0.3081567585468292
train gradient:  0.16824426016422298
iteration : 6409
train acc:  0.828125
train loss:  0.41396480798721313
train gradient:  0.31057817644530356
iteration : 6410
train acc:  0.796875
train loss:  0.43689286708831787
train gradient:  0.2829895761684852
iteration : 6411
train acc:  0.8671875
train loss:  0.3530563712120056
train gradient:  0.2537473218705219
iteration : 6412
train acc:  0.828125
train loss:  0.3797004818916321
train gradient:  0.2972572883668674
iteration : 6413
train acc:  0.7890625
train loss:  0.4178507626056671
train gradient:  0.2199290195837769
iteration : 6414
train acc:  0.796875
train loss:  0.40307024121284485
train gradient:  0.2893780749897027
iteration : 6415
train acc:  0.8359375
train loss:  0.33095914125442505
train gradient:  0.18065586404880646
iteration : 6416
train acc:  0.796875
train loss:  0.4265843629837036
train gradient:  0.2797832536145487
iteration : 6417
train acc:  0.8515625
train loss:  0.3503980040550232
train gradient:  0.24785145744135922
iteration : 6418
train acc:  0.84375
train loss:  0.34019893407821655
train gradient:  0.17954163840408993
iteration : 6419
train acc:  0.890625
train loss:  0.2611379027366638
train gradient:  0.18999777244507443
iteration : 6420
train acc:  0.84375
train loss:  0.36246198415756226
train gradient:  0.24538216808864566
iteration : 6421
train acc:  0.7578125
train loss:  0.4244794249534607
train gradient:  0.2813413385524314
iteration : 6422
train acc:  0.875
train loss:  0.32431936264038086
train gradient:  0.21454650953462062
iteration : 6423
train acc:  0.84375
train loss:  0.36439990997314453
train gradient:  0.19930704557834195
iteration : 6424
train acc:  0.8359375
train loss:  0.3324059844017029
train gradient:  0.2999567084553744
iteration : 6425
train acc:  0.8828125
train loss:  0.27009013295173645
train gradient:  0.12991443102025835
iteration : 6426
train acc:  0.8359375
train loss:  0.3938751518726349
train gradient:  0.22450126782929397
iteration : 6427
train acc:  0.875
train loss:  0.3195421099662781
train gradient:  0.19579609279330212
iteration : 6428
train acc:  0.8125
train loss:  0.42777037620544434
train gradient:  0.2256785363847425
iteration : 6429
train acc:  0.875
train loss:  0.2895468771457672
train gradient:  0.1328664364928318
iteration : 6430
train acc:  0.8984375
train loss:  0.28672003746032715
train gradient:  0.14823803220211546
iteration : 6431
train acc:  0.8828125
train loss:  0.2618051767349243
train gradient:  0.0988954891389488
iteration : 6432
train acc:  0.8359375
train loss:  0.3225247263908386
train gradient:  0.26011164690515265
iteration : 6433
train acc:  0.8671875
train loss:  0.3252543807029724
train gradient:  0.15937012332717104
iteration : 6434
train acc:  0.875
train loss:  0.3370629549026489
train gradient:  0.1810175693501046
iteration : 6435
train acc:  0.78125
train loss:  0.43592369556427
train gradient:  0.30235581896793834
iteration : 6436
train acc:  0.8984375
train loss:  0.26814499497413635
train gradient:  0.1468968451168289
iteration : 6437
train acc:  0.8046875
train loss:  0.3780832886695862
train gradient:  0.2752368055310118
iteration : 6438
train acc:  0.828125
train loss:  0.3315083682537079
train gradient:  0.1703173064099695
iteration : 6439
train acc:  0.8359375
train loss:  0.3632448613643646
train gradient:  0.20846420403789334
iteration : 6440
train acc:  0.8203125
train loss:  0.3463359475135803
train gradient:  0.28083769105270245
iteration : 6441
train acc:  0.828125
train loss:  0.34517142176628113
train gradient:  0.16524584636410897
iteration : 6442
train acc:  0.8671875
train loss:  0.3721083998680115
train gradient:  0.1821199319991595
iteration : 6443
train acc:  0.8359375
train loss:  0.33096134662628174
train gradient:  0.1787534788476457
iteration : 6444
train acc:  0.8515625
train loss:  0.3437480926513672
train gradient:  0.15834585123743844
iteration : 6445
train acc:  0.8359375
train loss:  0.3569818437099457
train gradient:  0.22627947994826153
iteration : 6446
train acc:  0.765625
train loss:  0.4395780563354492
train gradient:  0.3692655220668876
iteration : 6447
train acc:  0.828125
train loss:  0.40799978375434875
train gradient:  0.2663919788343813
iteration : 6448
train acc:  0.8359375
train loss:  0.3515562415122986
train gradient:  0.18483220042471285
iteration : 6449
train acc:  0.78125
train loss:  0.4425126016139984
train gradient:  0.28561911670190143
iteration : 6450
train acc:  0.8515625
train loss:  0.34611445665359497
train gradient:  0.16426714536636638
iteration : 6451
train acc:  0.8671875
train loss:  0.29011571407318115
train gradient:  0.12654566840392123
iteration : 6452
train acc:  0.84375
train loss:  0.3304174542427063
train gradient:  0.2086027848006024
iteration : 6453
train acc:  0.8203125
train loss:  0.35786673426628113
train gradient:  0.17660321057504583
iteration : 6454
train acc:  0.828125
train loss:  0.3598094582557678
train gradient:  0.23580340748119308
iteration : 6455
train acc:  0.8671875
train loss:  0.31250861287117004
train gradient:  0.16248581541387397
iteration : 6456
train acc:  0.8203125
train loss:  0.33090031147003174
train gradient:  0.14722849980122013
iteration : 6457
train acc:  0.859375
train loss:  0.3293677568435669
train gradient:  0.15750404720679842
iteration : 6458
train acc:  0.8828125
train loss:  0.29163801670074463
train gradient:  0.13506593503774555
iteration : 6459
train acc:  0.8359375
train loss:  0.32379022240638733
train gradient:  0.18877504024088676
iteration : 6460
train acc:  0.8515625
train loss:  0.3024612069129944
train gradient:  0.1312602666081616
iteration : 6461
train acc:  0.7734375
train loss:  0.43564218282699585
train gradient:  0.32012294130742247
iteration : 6462
train acc:  0.875
train loss:  0.34403538703918457
train gradient:  0.16227673325515435
iteration : 6463
train acc:  0.8359375
train loss:  0.3554050326347351
train gradient:  0.20201595170016795
iteration : 6464
train acc:  0.8515625
train loss:  0.3578239381313324
train gradient:  0.35529430152560687
iteration : 6465
train acc:  0.8203125
train loss:  0.40608906745910645
train gradient:  0.3089456972342785
iteration : 6466
train acc:  0.8671875
train loss:  0.31873559951782227
train gradient:  0.14222709310613466
iteration : 6467
train acc:  0.859375
train loss:  0.37327730655670166
train gradient:  0.22450561319300025
iteration : 6468
train acc:  0.828125
train loss:  0.3331040143966675
train gradient:  0.2792857988867633
iteration : 6469
train acc:  0.8359375
train loss:  0.3289182186126709
train gradient:  0.19505298489364287
iteration : 6470
train acc:  0.8671875
train loss:  0.33742576837539673
train gradient:  0.18261710941102305
iteration : 6471
train acc:  0.890625
train loss:  0.30016106367111206
train gradient:  0.15479370354843947
iteration : 6472
train acc:  0.8828125
train loss:  0.27576905488967896
train gradient:  0.1431892671339149
iteration : 6473
train acc:  0.890625
train loss:  0.2980518639087677
train gradient:  0.15012201325578783
iteration : 6474
train acc:  0.8359375
train loss:  0.377775102853775
train gradient:  0.21272012889181963
iteration : 6475
train acc:  0.828125
train loss:  0.4216369688510895
train gradient:  0.27939474610675363
iteration : 6476
train acc:  0.8515625
train loss:  0.33017677068710327
train gradient:  0.18750880166724496
iteration : 6477
train acc:  0.890625
train loss:  0.31296274065971375
train gradient:  0.15752206880945796
iteration : 6478
train acc:  0.9453125
train loss:  0.2552996873855591
train gradient:  0.12196128974341786
iteration : 6479
train acc:  0.859375
train loss:  0.3409878611564636
train gradient:  0.19325627936975637
iteration : 6480
train acc:  0.828125
train loss:  0.35581979155540466
train gradient:  0.20748059462179225
iteration : 6481
train acc:  0.8359375
train loss:  0.4253920018672943
train gradient:  0.2593495026981793
iteration : 6482
train acc:  0.8671875
train loss:  0.30138465762138367
train gradient:  0.13400932997386736
iteration : 6483
train acc:  0.8359375
train loss:  0.3582271933555603
train gradient:  0.2030754008349687
iteration : 6484
train acc:  0.8671875
train loss:  0.331129789352417
train gradient:  0.1715212277074464
iteration : 6485
train acc:  0.8203125
train loss:  0.3960093557834625
train gradient:  0.26089938328763407
iteration : 6486
train acc:  0.8828125
train loss:  0.30827954411506653
train gradient:  0.133063787531512
iteration : 6487
train acc:  0.84375
train loss:  0.37791991233825684
train gradient:  0.30559132302555003
iteration : 6488
train acc:  0.890625
train loss:  0.2916940748691559
train gradient:  0.1473246357546564
iteration : 6489
train acc:  0.8515625
train loss:  0.3191758990287781
train gradient:  0.1751423495375846
iteration : 6490
train acc:  0.8125
train loss:  0.39848166704177856
train gradient:  0.23043583312730767
iteration : 6491
train acc:  0.8671875
train loss:  0.33930036425590515
train gradient:  0.20408957162213723
iteration : 6492
train acc:  0.890625
train loss:  0.27209287881851196
train gradient:  0.13402409095156448
iteration : 6493
train acc:  0.828125
train loss:  0.37925106287002563
train gradient:  0.2709958692114027
iteration : 6494
train acc:  0.890625
train loss:  0.2505131959915161
train gradient:  0.1654750172178157
iteration : 6495
train acc:  0.84375
train loss:  0.38649237155914307
train gradient:  0.200019526061031
iteration : 6496
train acc:  0.859375
train loss:  0.31429052352905273
train gradient:  0.18864519017755477
iteration : 6497
train acc:  0.8671875
train loss:  0.3816661536693573
train gradient:  0.19645439271854265
iteration : 6498
train acc:  0.8671875
train loss:  0.3706440031528473
train gradient:  0.22478706368784793
iteration : 6499
train acc:  0.859375
train loss:  0.2993752658367157
train gradient:  0.1714417750460379
iteration : 6500
train acc:  0.8828125
train loss:  0.2841896116733551
train gradient:  0.16229318260818715
iteration : 6501
train acc:  0.84375
train loss:  0.32137221097946167
train gradient:  0.1792861645916044
iteration : 6502
train acc:  0.765625
train loss:  0.4237365126609802
train gradient:  0.29072117724069246
iteration : 6503
train acc:  0.8828125
train loss:  0.37864571809768677
train gradient:  0.23051842437771178
iteration : 6504
train acc:  0.8671875
train loss:  0.3188461661338806
train gradient:  0.1757640906621535
iteration : 6505
train acc:  0.734375
train loss:  0.495394229888916
train gradient:  0.3321238622470079
iteration : 6506
train acc:  0.8203125
train loss:  0.3568606376647949
train gradient:  0.19128502296529404
iteration : 6507
train acc:  0.828125
train loss:  0.3704449534416199
train gradient:  0.22180592058697995
iteration : 6508
train acc:  0.875
train loss:  0.3200426399707794
train gradient:  0.1644878699753967
iteration : 6509
train acc:  0.8203125
train loss:  0.3655296564102173
train gradient:  0.23644828919173838
iteration : 6510
train acc:  0.8671875
train loss:  0.3199525773525238
train gradient:  0.22186029842653257
iteration : 6511
train acc:  0.890625
train loss:  0.2927738428115845
train gradient:  0.15147079405631209
iteration : 6512
train acc:  0.828125
train loss:  0.39599862694740295
train gradient:  0.22485454382493253
iteration : 6513
train acc:  0.859375
train loss:  0.32902029156684875
train gradient:  0.21574597804591714
iteration : 6514
train acc:  0.8828125
train loss:  0.28738078474998474
train gradient:  0.1093098293600505
iteration : 6515
train acc:  0.8515625
train loss:  0.33510303497314453
train gradient:  0.16419761755715245
iteration : 6516
train acc:  0.8359375
train loss:  0.3771732747554779
train gradient:  0.2320803390333779
iteration : 6517
train acc:  0.78125
train loss:  0.40910446643829346
train gradient:  0.27296458365893495
iteration : 6518
train acc:  0.8515625
train loss:  0.33045727014541626
train gradient:  0.20278939616194525
iteration : 6519
train acc:  0.921875
train loss:  0.27091652154922485
train gradient:  0.1303215510704671
iteration : 6520
train acc:  0.8515625
train loss:  0.3762574791908264
train gradient:  0.2508084001850711
iteration : 6521
train acc:  0.859375
train loss:  0.3572557270526886
train gradient:  0.22043250604724285
iteration : 6522
train acc:  0.84375
train loss:  0.35439932346343994
train gradient:  0.19562574763380325
iteration : 6523
train acc:  0.84375
train loss:  0.3257623612880707
train gradient:  0.18583363436699846
iteration : 6524
train acc:  0.890625
train loss:  0.2680540382862091
train gradient:  0.14732385591316782
iteration : 6525
train acc:  0.8125
train loss:  0.35136374831199646
train gradient:  0.17211228100538942
iteration : 6526
train acc:  0.890625
train loss:  0.33121591806411743
train gradient:  0.17097060943061337
iteration : 6527
train acc:  0.8203125
train loss:  0.3980405926704407
train gradient:  0.3187248339163914
iteration : 6528
train acc:  0.8046875
train loss:  0.36555057764053345
train gradient:  0.232106812351014
iteration : 6529
train acc:  0.8671875
train loss:  0.23614002764225006
train gradient:  0.11686183182987035
iteration : 6530
train acc:  0.8203125
train loss:  0.3620753288269043
train gradient:  0.3195580708009575
iteration : 6531
train acc:  0.828125
train loss:  0.38284093141555786
train gradient:  0.27170670124419455
iteration : 6532
train acc:  0.8515625
train loss:  0.356717050075531
train gradient:  0.293517292079634
iteration : 6533
train acc:  0.84375
train loss:  0.34658241271972656
train gradient:  0.17093972417080028
iteration : 6534
train acc:  0.8359375
train loss:  0.3394116163253784
train gradient:  0.19327325405932597
iteration : 6535
train acc:  0.84375
train loss:  0.3591573238372803
train gradient:  0.19233261302931676
iteration : 6536
train acc:  0.8125
train loss:  0.44137394428253174
train gradient:  0.27490875532703296
iteration : 6537
train acc:  0.8515625
train loss:  0.3570979833602905
train gradient:  0.27936020401786105
iteration : 6538
train acc:  0.8671875
train loss:  0.3347015976905823
train gradient:  0.18704697537129217
iteration : 6539
train acc:  0.8359375
train loss:  0.4053494334220886
train gradient:  0.28485837574343287
iteration : 6540
train acc:  0.8828125
train loss:  0.2971544563770294
train gradient:  0.1481889082115155
iteration : 6541
train acc:  0.8125
train loss:  0.3749646544456482
train gradient:  0.1998822506415432
iteration : 6542
train acc:  0.8984375
train loss:  0.2793022096157074
train gradient:  0.14902004458787288
iteration : 6543
train acc:  0.7734375
train loss:  0.4698309302330017
train gradient:  0.6493191273930613
iteration : 6544
train acc:  0.8359375
train loss:  0.3389872610569
train gradient:  0.2198342676400335
iteration : 6545
train acc:  0.84375
train loss:  0.333385705947876
train gradient:  0.22342337681795013
iteration : 6546
train acc:  0.875
train loss:  0.2479172945022583
train gradient:  0.142341015422548
iteration : 6547
train acc:  0.8984375
train loss:  0.2539105713367462
train gradient:  0.1122840702486492
iteration : 6548
train acc:  0.859375
train loss:  0.312745600938797
train gradient:  0.19446258174145067
iteration : 6549
train acc:  0.8828125
train loss:  0.28852513432502747
train gradient:  0.09516237071863308
iteration : 6550
train acc:  0.859375
train loss:  0.3217581808567047
train gradient:  0.22477812937148903
iteration : 6551
train acc:  0.8359375
train loss:  0.3529760241508484
train gradient:  0.17981501975915415
iteration : 6552
train acc:  0.8828125
train loss:  0.2994592487812042
train gradient:  0.16011857228557308
iteration : 6553
train acc:  0.8984375
train loss:  0.2551453709602356
train gradient:  0.14032631546773622
iteration : 6554
train acc:  0.8515625
train loss:  0.3252228796482086
train gradient:  0.18749213351520694
iteration : 6555
train acc:  0.8203125
train loss:  0.4083874225616455
train gradient:  0.27434236596412204
iteration : 6556
train acc:  0.8671875
train loss:  0.23705630004405975
train gradient:  0.1401099132366651
iteration : 6557
train acc:  0.890625
train loss:  0.30279597640037537
train gradient:  0.1713152938582606
iteration : 6558
train acc:  0.8515625
train loss:  0.2986430525779724
train gradient:  0.17371950465739164
iteration : 6559
train acc:  0.859375
train loss:  0.3546110987663269
train gradient:  0.20975108699841988
iteration : 6560
train acc:  0.8515625
train loss:  0.3643611669540405
train gradient:  0.24058284855644363
iteration : 6561
train acc:  0.8515625
train loss:  0.3227250576019287
train gradient:  0.19073735317943474
iteration : 6562
train acc:  0.84375
train loss:  0.2830861210823059
train gradient:  0.20243682890575376
iteration : 6563
train acc:  0.859375
train loss:  0.33543074131011963
train gradient:  0.13992908029843637
iteration : 6564
train acc:  0.8984375
train loss:  0.2570543885231018
train gradient:  0.1220279797984969
iteration : 6565
train acc:  0.8125
train loss:  0.4088667035102844
train gradient:  0.19587529725022776
iteration : 6566
train acc:  0.8671875
train loss:  0.3244515061378479
train gradient:  0.21595572021168385
iteration : 6567
train acc:  0.7890625
train loss:  0.40126192569732666
train gradient:  0.29034208813495826
iteration : 6568
train acc:  0.8359375
train loss:  0.327631413936615
train gradient:  0.17688562181177342
iteration : 6569
train acc:  0.8359375
train loss:  0.37420523166656494
train gradient:  0.23727993609501138
iteration : 6570
train acc:  0.84375
train loss:  0.3989144265651703
train gradient:  0.2549605450548148
iteration : 6571
train acc:  0.8125
train loss:  0.3891375958919525
train gradient:  0.26655976187554054
iteration : 6572
train acc:  0.8359375
train loss:  0.3274284601211548
train gradient:  0.25489443419533897
iteration : 6573
train acc:  0.875
train loss:  0.3267573416233063
train gradient:  0.21098374404330172
iteration : 6574
train acc:  0.8359375
train loss:  0.35963380336761475
train gradient:  0.20654791124737285
iteration : 6575
train acc:  0.84375
train loss:  0.3358534574508667
train gradient:  0.21611359760183324
iteration : 6576
train acc:  0.875
train loss:  0.2974880039691925
train gradient:  0.14670725465821854
iteration : 6577
train acc:  0.828125
train loss:  0.3851444721221924
train gradient:  0.2550672698772675
iteration : 6578
train acc:  0.765625
train loss:  0.4520481526851654
train gradient:  0.3297908588757273
iteration : 6579
train acc:  0.828125
train loss:  0.40080171823501587
train gradient:  0.21358831425638047
iteration : 6580
train acc:  0.8203125
train loss:  0.43676769733428955
train gradient:  0.34798036801824456
iteration : 6581
train acc:  0.875
train loss:  0.3538625240325928
train gradient:  0.21407574463725704
iteration : 6582
train acc:  0.859375
train loss:  0.33838894963264465
train gradient:  0.20562869932237243
iteration : 6583
train acc:  0.8671875
train loss:  0.31445613503456116
train gradient:  0.2176969214448592
iteration : 6584
train acc:  0.84375
train loss:  0.30595558881759644
train gradient:  0.13503042006287172
iteration : 6585
train acc:  0.84375
train loss:  0.42800474166870117
train gradient:  0.23697003693200888
iteration : 6586
train acc:  0.859375
train loss:  0.3217940032482147
train gradient:  0.21517080902172875
iteration : 6587
train acc:  0.8203125
train loss:  0.42908135056495667
train gradient:  0.3569700814296563
iteration : 6588
train acc:  0.8203125
train loss:  0.3716462552547455
train gradient:  0.2624101748811859
iteration : 6589
train acc:  0.8359375
train loss:  0.33051303029060364
train gradient:  0.1660479163787998
iteration : 6590
train acc:  0.859375
train loss:  0.32505735754966736
train gradient:  0.2470660019743713
iteration : 6591
train acc:  0.8515625
train loss:  0.36334067583084106
train gradient:  0.2003358682107425
iteration : 6592
train acc:  0.8671875
train loss:  0.3584649860858917
train gradient:  0.22206903420436297
iteration : 6593
train acc:  0.75
train loss:  0.49384066462516785
train gradient:  0.4429053244356076
iteration : 6594
train acc:  0.890625
train loss:  0.2786089777946472
train gradient:  0.177387461564327
iteration : 6595
train acc:  0.8359375
train loss:  0.3239383101463318
train gradient:  0.16418144450116995
iteration : 6596
train acc:  0.828125
train loss:  0.414612740278244
train gradient:  0.2204072653977816
iteration : 6597
train acc:  0.828125
train loss:  0.31571295857429504
train gradient:  0.2212588466111029
iteration : 6598
train acc:  0.8515625
train loss:  0.32548239827156067
train gradient:  0.18564235388149156
iteration : 6599
train acc:  0.8671875
train loss:  0.3855404853820801
train gradient:  0.20113707953515592
iteration : 6600
train acc:  0.921875
train loss:  0.280579537153244
train gradient:  0.17366111600054976
iteration : 6601
train acc:  0.828125
train loss:  0.3572508692741394
train gradient:  0.20243483302921533
iteration : 6602
train acc:  0.8671875
train loss:  0.34063518047332764
train gradient:  0.2030362746673321
iteration : 6603
train acc:  0.921875
train loss:  0.247487872838974
train gradient:  0.13114417573592785
iteration : 6604
train acc:  0.8046875
train loss:  0.41022416949272156
train gradient:  0.3000626518151477
iteration : 6605
train acc:  0.8046875
train loss:  0.36298370361328125
train gradient:  0.18051196847488307
iteration : 6606
train acc:  0.8671875
train loss:  0.34632962942123413
train gradient:  0.34406680541920326
iteration : 6607
train acc:  0.84375
train loss:  0.3358972668647766
train gradient:  0.2199193575417664
iteration : 6608
train acc:  0.9140625
train loss:  0.31597018241882324
train gradient:  0.13807462181461994
iteration : 6609
train acc:  0.8515625
train loss:  0.3727973699569702
train gradient:  0.24357000294481418
iteration : 6610
train acc:  0.8671875
train loss:  0.2930956482887268
train gradient:  0.15536595040159384
iteration : 6611
train acc:  0.859375
train loss:  0.28694838285446167
train gradient:  0.1575310963921
iteration : 6612
train acc:  0.8515625
train loss:  0.32966387271881104
train gradient:  0.19060191299824678
iteration : 6613
train acc:  0.8125
train loss:  0.45538321137428284
train gradient:  0.2830687111500937
iteration : 6614
train acc:  0.8515625
train loss:  0.33718380331993103
train gradient:  0.23620865363193058
iteration : 6615
train acc:  0.84375
train loss:  0.3189486265182495
train gradient:  0.1697894133032933
iteration : 6616
train acc:  0.8359375
train loss:  0.31748026609420776
train gradient:  0.14636404963783506
iteration : 6617
train acc:  0.859375
train loss:  0.3002873659133911
train gradient:  0.14452957407679443
iteration : 6618
train acc:  0.9140625
train loss:  0.27332937717437744
train gradient:  0.2568733938503331
iteration : 6619
train acc:  0.8359375
train loss:  0.3695322871208191
train gradient:  0.30597512059500553
iteration : 6620
train acc:  0.90625
train loss:  0.2884056568145752
train gradient:  0.22647628121844315
iteration : 6621
train acc:  0.8671875
train loss:  0.2849947512149811
train gradient:  0.17545427432024185
iteration : 6622
train acc:  0.8046875
train loss:  0.4195590615272522
train gradient:  0.3360174032121065
iteration : 6623
train acc:  0.828125
train loss:  0.40500879287719727
train gradient:  0.263172682975526
iteration : 6624
train acc:  0.8359375
train loss:  0.3512744903564453
train gradient:  0.22859807229611398
iteration : 6625
train acc:  0.875
train loss:  0.3579252362251282
train gradient:  0.20012114071528353
iteration : 6626
train acc:  0.828125
train loss:  0.33104994893074036
train gradient:  0.15800510031775705
iteration : 6627
train acc:  0.859375
train loss:  0.2670638859272003
train gradient:  0.1786628137637894
iteration : 6628
train acc:  0.8203125
train loss:  0.3293895721435547
train gradient:  0.2016902422254753
iteration : 6629
train acc:  0.8515625
train loss:  0.30510765314102173
train gradient:  0.1781943745077092
iteration : 6630
train acc:  0.828125
train loss:  0.36180752515792847
train gradient:  0.2199153026741047
iteration : 6631
train acc:  0.828125
train loss:  0.3683188557624817
train gradient:  0.2961485639077343
iteration : 6632
train acc:  0.8671875
train loss:  0.31622248888015747
train gradient:  0.18001913778750345
iteration : 6633
train acc:  0.890625
train loss:  0.24961735308170319
train gradient:  0.13796846077482827
iteration : 6634
train acc:  0.8828125
train loss:  0.2833826243877411
train gradient:  0.1286226202071306
iteration : 6635
train acc:  0.8046875
train loss:  0.4399257302284241
train gradient:  0.3609889424190172
iteration : 6636
train acc:  0.8984375
train loss:  0.3263916075229645
train gradient:  0.19208412602352073
iteration : 6637
train acc:  0.8671875
train loss:  0.28894203901290894
train gradient:  0.21259465166037944
iteration : 6638
train acc:  0.8671875
train loss:  0.3195870816707611
train gradient:  0.22971152244735127
iteration : 6639
train acc:  0.875
train loss:  0.3093474805355072
train gradient:  0.17905944202853627
iteration : 6640
train acc:  0.8515625
train loss:  0.330025315284729
train gradient:  0.1551615508818272
iteration : 6641
train acc:  0.796875
train loss:  0.3690357208251953
train gradient:  0.2827440930877827
iteration : 6642
train acc:  0.8359375
train loss:  0.3282480239868164
train gradient:  0.1954367335540643
iteration : 6643
train acc:  0.8515625
train loss:  0.31856101751327515
train gradient:  0.14721205492491915
iteration : 6644
train acc:  0.8203125
train loss:  0.39512011408805847
train gradient:  0.25379876409064517
iteration : 6645
train acc:  0.890625
train loss:  0.28033319115638733
train gradient:  0.1649427492088521
iteration : 6646
train acc:  0.84375
train loss:  0.36185982823371887
train gradient:  0.30296691573515055
iteration : 6647
train acc:  0.8515625
train loss:  0.2937270402908325
train gradient:  0.16017481954773613
iteration : 6648
train acc:  0.828125
train loss:  0.36128491163253784
train gradient:  0.24617062368343373
iteration : 6649
train acc:  0.828125
train loss:  0.36307644844055176
train gradient:  0.2690768828809522
iteration : 6650
train acc:  0.8515625
train loss:  0.3603588342666626
train gradient:  0.20891946953597962
iteration : 6651
train acc:  0.9140625
train loss:  0.25891053676605225
train gradient:  0.19388669985540566
iteration : 6652
train acc:  0.859375
train loss:  0.3333643674850464
train gradient:  0.2273489483524781
iteration : 6653
train acc:  0.8671875
train loss:  0.2827311158180237
train gradient:  0.16232529514942162
iteration : 6654
train acc:  0.875
train loss:  0.2878727912902832
train gradient:  0.12983908248420467
iteration : 6655
train acc:  0.8203125
train loss:  0.3998293876647949
train gradient:  0.19604312174110874
iteration : 6656
train acc:  0.84375
train loss:  0.3659420311450958
train gradient:  0.25896996096603153
iteration : 6657
train acc:  0.8828125
train loss:  0.35578060150146484
train gradient:  0.23335535269315505
iteration : 6658
train acc:  0.859375
train loss:  0.3580258786678314
train gradient:  0.257274662127241
iteration : 6659
train acc:  0.8671875
train loss:  0.34218674898147583
train gradient:  0.19491217645323775
iteration : 6660
train acc:  0.8203125
train loss:  0.36820557713508606
train gradient:  0.22363648646251716
iteration : 6661
train acc:  0.8671875
train loss:  0.2663191258907318
train gradient:  0.12844472330027903
iteration : 6662
train acc:  0.8515625
train loss:  0.3018314242362976
train gradient:  0.19980458958281788
iteration : 6663
train acc:  0.890625
train loss:  0.31118887662887573
train gradient:  0.18833503804733598
iteration : 6664
train acc:  0.8515625
train loss:  0.31759077310562134
train gradient:  0.20025926562930374
iteration : 6665
train acc:  0.84375
train loss:  0.34457412362098694
train gradient:  0.1888887044305174
iteration : 6666
train acc:  0.859375
train loss:  0.32336142659187317
train gradient:  0.21687253671394413
iteration : 6667
train acc:  0.84375
train loss:  0.4277474880218506
train gradient:  0.2937980961480234
iteration : 6668
train acc:  0.8125
train loss:  0.399650514125824
train gradient:  0.2984627268716278
iteration : 6669
train acc:  0.8046875
train loss:  0.40541133284568787
train gradient:  0.3503316845947358
iteration : 6670
train acc:  0.9140625
train loss:  0.22263365983963013
train gradient:  0.11314891199837188
iteration : 6671
train acc:  0.90625
train loss:  0.23954598605632782
train gradient:  0.1338687654016914
iteration : 6672
train acc:  0.8515625
train loss:  0.328779935836792
train gradient:  0.2102841928957945
iteration : 6673
train acc:  0.8359375
train loss:  0.34678351879119873
train gradient:  0.2059677026281655
iteration : 6674
train acc:  0.8671875
train loss:  0.31472885608673096
train gradient:  0.20815989574709814
iteration : 6675
train acc:  0.84375
train loss:  0.3427053987979889
train gradient:  0.2031568690186079
iteration : 6676
train acc:  0.84375
train loss:  0.354343056678772
train gradient:  0.26533146631327853
iteration : 6677
train acc:  0.8828125
train loss:  0.3385258913040161
train gradient:  0.1466758733470794
iteration : 6678
train acc:  0.8671875
train loss:  0.3238397538661957
train gradient:  0.23287141477372908
iteration : 6679
train acc:  0.796875
train loss:  0.34245699644088745
train gradient:  0.1778230720984476
iteration : 6680
train acc:  0.875
train loss:  0.283926784992218
train gradient:  0.15507506343565586
iteration : 6681
train acc:  0.8515625
train loss:  0.3553670346736908
train gradient:  0.31207470463695086
iteration : 6682
train acc:  0.8515625
train loss:  0.29784923791885376
train gradient:  0.14733875367435695
iteration : 6683
train acc:  0.828125
train loss:  0.34175437688827515
train gradient:  0.3175591628085113
iteration : 6684
train acc:  0.875
train loss:  0.24298709630966187
train gradient:  0.11155056807292006
iteration : 6685
train acc:  0.8046875
train loss:  0.45038479566574097
train gradient:  0.37613657265077793
iteration : 6686
train acc:  0.8359375
train loss:  0.37534475326538086
train gradient:  0.2783015383891693
iteration : 6687
train acc:  0.890625
train loss:  0.2779385447502136
train gradient:  0.1900276403525482
iteration : 6688
train acc:  0.8515625
train loss:  0.3369447588920593
train gradient:  0.22775063721971234
iteration : 6689
train acc:  0.796875
train loss:  0.370442271232605
train gradient:  0.2494547108161885
iteration : 6690
train acc:  0.8984375
train loss:  0.2677219808101654
train gradient:  0.18738406962600324
iteration : 6691
train acc:  0.8125
train loss:  0.43466436862945557
train gradient:  0.24673571398394817
iteration : 6692
train acc:  0.8515625
train loss:  0.3466925024986267
train gradient:  0.2861343413624476
iteration : 6693
train acc:  0.859375
train loss:  0.330766499042511
train gradient:  0.194217291466484
iteration : 6694
train acc:  0.859375
train loss:  0.31007757782936096
train gradient:  0.17360314968736068
iteration : 6695
train acc:  0.8515625
train loss:  0.3238118886947632
train gradient:  0.21955366087230366
iteration : 6696
train acc:  0.7890625
train loss:  0.44269007444381714
train gradient:  0.353746989008371
iteration : 6697
train acc:  0.7890625
train loss:  0.45764100551605225
train gradient:  0.376652612791169
iteration : 6698
train acc:  0.8359375
train loss:  0.3389284312725067
train gradient:  0.19930592528913713
iteration : 6699
train acc:  0.859375
train loss:  0.3107268214225769
train gradient:  0.25374755257397313
iteration : 6700
train acc:  0.8046875
train loss:  0.4090843200683594
train gradient:  0.2780845611946997
iteration : 6701
train acc:  0.875
train loss:  0.3266872763633728
train gradient:  0.19322789684541947
iteration : 6702
train acc:  0.8203125
train loss:  0.3721997141838074
train gradient:  0.23073222112844874
iteration : 6703
train acc:  0.796875
train loss:  0.3875043988227844
train gradient:  0.23263359358793936
iteration : 6704
train acc:  0.7734375
train loss:  0.4759295582771301
train gradient:  0.4009178610021735
iteration : 6705
train acc:  0.84375
train loss:  0.374369740486145
train gradient:  0.2236932476527455
iteration : 6706
train acc:  0.875
train loss:  0.2923769950866699
train gradient:  0.1504867919316502
iteration : 6707
train acc:  0.8125
train loss:  0.3904256224632263
train gradient:  0.23764902747107133
iteration : 6708
train acc:  0.78125
train loss:  0.44541993737220764
train gradient:  0.33711822355663806
iteration : 6709
train acc:  0.90625
train loss:  0.29225754737854004
train gradient:  0.14709450003113705
iteration : 6710
train acc:  0.828125
train loss:  0.38510245084762573
train gradient:  0.1849285708104657
iteration : 6711
train acc:  0.875
train loss:  0.3036174178123474
train gradient:  0.14651323588742549
iteration : 6712
train acc:  0.828125
train loss:  0.35408878326416016
train gradient:  0.18363737427381127
iteration : 6713
train acc:  0.8515625
train loss:  0.3094390630722046
train gradient:  0.16645421289175233
iteration : 6714
train acc:  0.875
train loss:  0.29402458667755127
train gradient:  0.12027995495588831
iteration : 6715
train acc:  0.921875
train loss:  0.24003100395202637
train gradient:  0.09965143776589254
iteration : 6716
train acc:  0.8515625
train loss:  0.3825368285179138
train gradient:  0.22983700799599371
iteration : 6717
train acc:  0.8828125
train loss:  0.29832786321640015
train gradient:  0.11289090786106387
iteration : 6718
train acc:  0.8515625
train loss:  0.34655892848968506
train gradient:  0.18662276086024665
iteration : 6719
train acc:  0.8984375
train loss:  0.28392088413238525
train gradient:  0.17475786588949854
iteration : 6720
train acc:  0.8671875
train loss:  0.3352883458137512
train gradient:  0.1274666223754239
iteration : 6721
train acc:  0.8359375
train loss:  0.39898681640625
train gradient:  0.2414583384965769
iteration : 6722
train acc:  0.84375
train loss:  0.35156726837158203
train gradient:  0.23843835119253867
iteration : 6723
train acc:  0.8671875
train loss:  0.33464720845222473
train gradient:  0.22077419860834246
iteration : 6724
train acc:  0.828125
train loss:  0.3700620234012604
train gradient:  0.22949205506986387
iteration : 6725
train acc:  0.8984375
train loss:  0.25597214698791504
train gradient:  0.1874416823700139
iteration : 6726
train acc:  0.875
train loss:  0.3242610692977905
train gradient:  0.35952849551513755
iteration : 6727
train acc:  0.8359375
train loss:  0.4231169819831848
train gradient:  0.23682333567607328
iteration : 6728
train acc:  0.828125
train loss:  0.41331255435943604
train gradient:  0.2265790618399814
iteration : 6729
train acc:  0.8671875
train loss:  0.29987797141075134
train gradient:  0.34610770412391945
iteration : 6730
train acc:  0.859375
train loss:  0.3185145854949951
train gradient:  0.1619374985763057
iteration : 6731
train acc:  0.875
train loss:  0.2661917805671692
train gradient:  0.1233389089064465
iteration : 6732
train acc:  0.8671875
train loss:  0.4016686677932739
train gradient:  0.2224133896678453
iteration : 6733
train acc:  0.8359375
train loss:  0.40369802713394165
train gradient:  0.2260000379468738
iteration : 6734
train acc:  0.796875
train loss:  0.4307042956352234
train gradient:  0.23858328010884933
iteration : 6735
train acc:  0.890625
train loss:  0.2804450988769531
train gradient:  0.14617438573258207
iteration : 6736
train acc:  0.859375
train loss:  0.3346344530582428
train gradient:  0.18127310437736927
iteration : 6737
train acc:  0.84375
train loss:  0.3050256371498108
train gradient:  0.22690031732666296
iteration : 6738
train acc:  0.8984375
train loss:  0.26302993297576904
train gradient:  0.12371957719006535
iteration : 6739
train acc:  0.8125
train loss:  0.33891719579696655
train gradient:  0.26362303901864065
iteration : 6740
train acc:  0.8671875
train loss:  0.3162938058376312
train gradient:  0.1750190745486418
iteration : 6741
train acc:  0.875
train loss:  0.3198126554489136
train gradient:  0.15090883795156224
iteration : 6742
train acc:  0.84375
train loss:  0.32548949122428894
train gradient:  0.26458523910104853
iteration : 6743
train acc:  0.875
train loss:  0.32286059856414795
train gradient:  0.15751457530763743
iteration : 6744
train acc:  0.875
train loss:  0.2745926082134247
train gradient:  0.11806335715060264
iteration : 6745
train acc:  0.8359375
train loss:  0.2955958843231201
train gradient:  0.17581805757818667
iteration : 6746
train acc:  0.8125
train loss:  0.37797045707702637
train gradient:  0.211410109574278
iteration : 6747
train acc:  0.78125
train loss:  0.46216678619384766
train gradient:  0.34275358679804596
iteration : 6748
train acc:  0.796875
train loss:  0.37071073055267334
train gradient:  0.2019445159765944
iteration : 6749
train acc:  0.8515625
train loss:  0.3250393569469452
train gradient:  0.23091750906310576
iteration : 6750
train acc:  0.8828125
train loss:  0.2525638937950134
train gradient:  0.12680835409274718
iteration : 6751
train acc:  0.875
train loss:  0.3176285922527313
train gradient:  0.17465421624748395
iteration : 6752
train acc:  0.90625
train loss:  0.3417847156524658
train gradient:  0.26735151088182957
iteration : 6753
train acc:  0.84375
train loss:  0.41521239280700684
train gradient:  0.2847771077673262
iteration : 6754
train acc:  0.84375
train loss:  0.3509990870952606
train gradient:  0.16623119710266637
iteration : 6755
train acc:  0.890625
train loss:  0.2654019296169281
train gradient:  0.1410379904671409
iteration : 6756
train acc:  0.7734375
train loss:  0.44505539536476135
train gradient:  0.3251942403827977
iteration : 6757
train acc:  0.859375
train loss:  0.3371211290359497
train gradient:  0.2439802658776783
iteration : 6758
train acc:  0.8984375
train loss:  0.2447662651538849
train gradient:  0.2249950512702102
iteration : 6759
train acc:  0.8203125
train loss:  0.37531155347824097
train gradient:  0.19139497548601675
iteration : 6760
train acc:  0.84375
train loss:  0.3507804870605469
train gradient:  0.23190605795660135
iteration : 6761
train acc:  0.859375
train loss:  0.29086124897003174
train gradient:  0.1608308192397667
iteration : 6762
train acc:  0.8828125
train loss:  0.2631129324436188
train gradient:  0.1315684287614712
iteration : 6763
train acc:  0.875
train loss:  0.3011423945426941
train gradient:  0.16748052798676716
iteration : 6764
train acc:  0.84375
train loss:  0.35709118843078613
train gradient:  0.20211124564398525
iteration : 6765
train acc:  0.859375
train loss:  0.3299137353897095
train gradient:  0.21726298154229912
iteration : 6766
train acc:  0.8984375
train loss:  0.27661287784576416
train gradient:  0.21482673017464032
iteration : 6767
train acc:  0.8515625
train loss:  0.3259677290916443
train gradient:  0.19429988665944634
iteration : 6768
train acc:  0.8515625
train loss:  0.2963990867137909
train gradient:  0.1699460613898639
iteration : 6769
train acc:  0.875
train loss:  0.28203505277633667
train gradient:  0.15788058813505745
iteration : 6770
train acc:  0.859375
train loss:  0.29622846841812134
train gradient:  0.26714624076247234
iteration : 6771
train acc:  0.8203125
train loss:  0.3769702911376953
train gradient:  0.21036980862200375
iteration : 6772
train acc:  0.84375
train loss:  0.35060155391693115
train gradient:  0.21409537938714873
iteration : 6773
train acc:  0.8984375
train loss:  0.2953505218029022
train gradient:  0.2901606067331458
iteration : 6774
train acc:  0.875
train loss:  0.3034157156944275
train gradient:  0.15654600532328497
iteration : 6775
train acc:  0.875
train loss:  0.28717613220214844
train gradient:  0.14359806001665015
iteration : 6776
train acc:  0.8671875
train loss:  0.25371792912483215
train gradient:  0.15063960270682658
iteration : 6777
train acc:  0.859375
train loss:  0.28389790654182434
train gradient:  0.20737059213993803
iteration : 6778
train acc:  0.8515625
train loss:  0.3496207594871521
train gradient:  0.1849341206390541
iteration : 6779
train acc:  0.8203125
train loss:  0.40262895822525024
train gradient:  0.18784778156610366
iteration : 6780
train acc:  0.8828125
train loss:  0.2841850221157074
train gradient:  0.15126911068159102
iteration : 6781
train acc:  0.875
train loss:  0.3469907343387604
train gradient:  0.2969601014588979
iteration : 6782
train acc:  0.890625
train loss:  0.2522328197956085
train gradient:  0.13426755290571124
iteration : 6783
train acc:  0.828125
train loss:  0.41737133264541626
train gradient:  0.2487438733937123
iteration : 6784
train acc:  0.8671875
train loss:  0.3277233839035034
train gradient:  0.2623724379375501
iteration : 6785
train acc:  0.8359375
train loss:  0.33754438161849976
train gradient:  0.28521806403788935
iteration : 6786
train acc:  0.828125
train loss:  0.3368857800960541
train gradient:  0.16854764090428753
iteration : 6787
train acc:  0.8515625
train loss:  0.35115253925323486
train gradient:  0.3747149371940915
iteration : 6788
train acc:  0.90625
train loss:  0.2768150568008423
train gradient:  0.20910245129577304
iteration : 6789
train acc:  0.84375
train loss:  0.4184681177139282
train gradient:  0.3910332700523177
iteration : 6790
train acc:  0.8125
train loss:  0.3939197361469269
train gradient:  0.25288186392652007
iteration : 6791
train acc:  0.84375
train loss:  0.332913875579834
train gradient:  0.37241643411882797
iteration : 6792
train acc:  0.859375
train loss:  0.39990776777267456
train gradient:  0.24877416824600468
iteration : 6793
train acc:  0.8203125
train loss:  0.3408823013305664
train gradient:  0.23052231282386354
iteration : 6794
train acc:  0.8359375
train loss:  0.34906700253486633
train gradient:  0.22365717194311702
iteration : 6795
train acc:  0.8515625
train loss:  0.30401256680488586
train gradient:  0.17283617332545725
iteration : 6796
train acc:  0.8671875
train loss:  0.3282972276210785
train gradient:  0.2035461239591877
iteration : 6797
train acc:  0.859375
train loss:  0.3127443492412567
train gradient:  0.16188139062747728
iteration : 6798
train acc:  0.8359375
train loss:  0.3459441065788269
train gradient:  0.19867056295670754
iteration : 6799
train acc:  0.796875
train loss:  0.4456743896007538
train gradient:  0.40313933115600825
iteration : 6800
train acc:  0.828125
train loss:  0.4416346549987793
train gradient:  0.2967345590898906
iteration : 6801
train acc:  0.828125
train loss:  0.41261738538742065
train gradient:  0.24458027179689917
iteration : 6802
train acc:  0.8515625
train loss:  0.30948370695114136
train gradient:  0.1968175148172715
iteration : 6803
train acc:  0.828125
train loss:  0.3683444857597351
train gradient:  0.20490374742860978
iteration : 6804
train acc:  0.8359375
train loss:  0.41069296002388
train gradient:  0.23727845958862265
iteration : 6805
train acc:  0.859375
train loss:  0.37066763639450073
train gradient:  0.22269666799922305
iteration : 6806
train acc:  0.796875
train loss:  0.43461689352989197
train gradient:  0.28786877739453476
iteration : 6807
train acc:  0.8359375
train loss:  0.35630863904953003
train gradient:  0.16327315075951782
iteration : 6808
train acc:  0.8515625
train loss:  0.3990432918071747
train gradient:  0.19131629808675402
iteration : 6809
train acc:  0.8203125
train loss:  0.4182705283164978
train gradient:  0.29205741659291795
iteration : 6810
train acc:  0.7890625
train loss:  0.3560273051261902
train gradient:  0.269126604463867
iteration : 6811
train acc:  0.8515625
train loss:  0.30097198486328125
train gradient:  0.16510530399434323
iteration : 6812
train acc:  0.8828125
train loss:  0.28917762637138367
train gradient:  0.16319504324977968
iteration : 6813
train acc:  0.8046875
train loss:  0.423500120639801
train gradient:  0.2255450473859245
iteration : 6814
train acc:  0.8515625
train loss:  0.37946903705596924
train gradient:  0.17389160306086346
iteration : 6815
train acc:  0.7578125
train loss:  0.39760687947273254
train gradient:  0.26250031674545193
iteration : 6816
train acc:  0.796875
train loss:  0.40373218059539795
train gradient:  0.27570608460371676
iteration : 6817
train acc:  0.859375
train loss:  0.3797816038131714
train gradient:  0.23565878917223432
iteration : 6818
train acc:  0.84375
train loss:  0.38165679574012756
train gradient:  0.24120860037403624
iteration : 6819
train acc:  0.90625
train loss:  0.25190216302871704
train gradient:  0.12906153467439557
iteration : 6820
train acc:  0.84375
train loss:  0.3761886954307556
train gradient:  0.22566217990237525
iteration : 6821
train acc:  0.8203125
train loss:  0.3757673501968384
train gradient:  0.2273525647266013
iteration : 6822
train acc:  0.859375
train loss:  0.32801783084869385
train gradient:  0.1261850975763711
iteration : 6823
train acc:  0.828125
train loss:  0.40234988927841187
train gradient:  0.3892881376563021
iteration : 6824
train acc:  0.890625
train loss:  0.2873085141181946
train gradient:  0.16328165015413942
iteration : 6825
train acc:  0.875
train loss:  0.29298585653305054
train gradient:  0.1259022185688884
iteration : 6826
train acc:  0.890625
train loss:  0.30317825078964233
train gradient:  0.20604263962948702
iteration : 6827
train acc:  0.7890625
train loss:  0.4194555878639221
train gradient:  0.3022528467534806
iteration : 6828
train acc:  0.8125
train loss:  0.4318296015262604
train gradient:  0.2557328046205544
iteration : 6829
train acc:  0.78125
train loss:  0.4675346612930298
train gradient:  0.3069134895097098
iteration : 6830
train acc:  0.828125
train loss:  0.3766196370124817
train gradient:  0.29857278528926495
iteration : 6831
train acc:  0.8515625
train loss:  0.3135435879230499
train gradient:  0.14884949399473751
iteration : 6832
train acc:  0.8359375
train loss:  0.3061857223510742
train gradient:  0.13591978701555463
iteration : 6833
train acc:  0.7890625
train loss:  0.43161964416503906
train gradient:  0.17230785548927358
iteration : 6834
train acc:  0.859375
train loss:  0.41501671075820923
train gradient:  0.20940047181340382
iteration : 6835
train acc:  0.8359375
train loss:  0.3048941493034363
train gradient:  0.1251169737372375
iteration : 6836
train acc:  0.8203125
train loss:  0.40161463618278503
train gradient:  0.2350391483472759
iteration : 6837
train acc:  0.90625
train loss:  0.2713150382041931
train gradient:  0.13665283199317818
iteration : 6838
train acc:  0.8359375
train loss:  0.3459598422050476
train gradient:  0.1538378629796872
iteration : 6839
train acc:  0.796875
train loss:  0.3581894636154175
train gradient:  0.16665128808671495
iteration : 6840
train acc:  0.8359375
train loss:  0.317518949508667
train gradient:  0.24759905973176516
iteration : 6841
train acc:  0.828125
train loss:  0.3881210684776306
train gradient:  0.24169714151871108
iteration : 6842
train acc:  0.90625
train loss:  0.266230046749115
train gradient:  0.12421812639683533
iteration : 6843
train acc:  0.875
train loss:  0.3318140208721161
train gradient:  0.23411435596826866
iteration : 6844
train acc:  0.8359375
train loss:  0.35499972105026245
train gradient:  0.1733963811403076
iteration : 6845
train acc:  0.8515625
train loss:  0.29446983337402344
train gradient:  0.11640096174471497
iteration : 6846
train acc:  0.8828125
train loss:  0.2850862145423889
train gradient:  0.14728915949353238
iteration : 6847
train acc:  0.8828125
train loss:  0.28930363059043884
train gradient:  0.127632469747701
iteration : 6848
train acc:  0.875
train loss:  0.32167088985443115
train gradient:  0.16620207241673876
iteration : 6849
train acc:  0.84375
train loss:  0.32443708181381226
train gradient:  0.18233104245577633
iteration : 6850
train acc:  0.828125
train loss:  0.3761320114135742
train gradient:  0.19190397425434705
iteration : 6851
train acc:  0.796875
train loss:  0.4058036804199219
train gradient:  0.22444007569876212
iteration : 6852
train acc:  0.859375
train loss:  0.35484158992767334
train gradient:  0.2567235750270013
iteration : 6853
train acc:  0.8515625
train loss:  0.3577389121055603
train gradient:  0.16255627576389806
iteration : 6854
train acc:  0.8359375
train loss:  0.3573943078517914
train gradient:  0.1831688188633539
iteration : 6855
train acc:  0.8046875
train loss:  0.4286521077156067
train gradient:  0.3230876503895125
iteration : 6856
train acc:  0.8671875
train loss:  0.32886698842048645
train gradient:  0.2432618246267849
iteration : 6857
train acc:  0.8828125
train loss:  0.30400964617729187
train gradient:  0.1520836862886665
iteration : 6858
train acc:  0.90625
train loss:  0.278688907623291
train gradient:  0.15795244538819175
iteration : 6859
train acc:  0.90625
train loss:  0.3069303035736084
train gradient:  0.1457240378454781
iteration : 6860
train acc:  0.8125
train loss:  0.3652082085609436
train gradient:  0.1925331109858024
iteration : 6861
train acc:  0.8515625
train loss:  0.3249291777610779
train gradient:  0.20697605577965125
iteration : 6862
train acc:  0.8203125
train loss:  0.3604525327682495
train gradient:  0.22543528107354266
iteration : 6863
train acc:  0.828125
train loss:  0.3849477767944336
train gradient:  0.2090715405622416
iteration : 6864
train acc:  0.7890625
train loss:  0.35428497195243835
train gradient:  0.18124890555720882
iteration : 6865
train acc:  0.78125
train loss:  0.38177281618118286
train gradient:  0.17338360963076804
iteration : 6866
train acc:  0.875
train loss:  0.3408198356628418
train gradient:  0.15256576365806607
iteration : 6867
train acc:  0.859375
train loss:  0.354741632938385
train gradient:  0.20272941219514
iteration : 6868
train acc:  0.875
train loss:  0.310665100812912
train gradient:  0.1621264744479047
iteration : 6869
train acc:  0.859375
train loss:  0.3293420672416687
train gradient:  0.21531834608012274
iteration : 6870
train acc:  0.84375
train loss:  0.31269729137420654
train gradient:  0.17203850443396684
iteration : 6871
train acc:  0.84375
train loss:  0.29641690850257874
train gradient:  0.13291575974130518
iteration : 6872
train acc:  0.8984375
train loss:  0.2730266749858856
train gradient:  0.1437517069339198
iteration : 6873
train acc:  0.8671875
train loss:  0.34015047550201416
train gradient:  0.22086639999483254
iteration : 6874
train acc:  0.8828125
train loss:  0.32508817315101624
train gradient:  0.2128517546577055
iteration : 6875
train acc:  0.859375
train loss:  0.2991873621940613
train gradient:  0.24766440389599498
iteration : 6876
train acc:  0.828125
train loss:  0.39152392745018005
train gradient:  0.35527563616052427
iteration : 6877
train acc:  0.8671875
train loss:  0.286405473947525
train gradient:  0.17009024291983477
iteration : 6878
train acc:  0.84375
train loss:  0.40854158997535706
train gradient:  0.2657182890662112
iteration : 6879
train acc:  0.8984375
train loss:  0.2851119339466095
train gradient:  0.20537329455008607
iteration : 6880
train acc:  0.7890625
train loss:  0.38901442289352417
train gradient:  0.2490570836928379
iteration : 6881
train acc:  0.8671875
train loss:  0.3373176157474518
train gradient:  0.14646303091414678
iteration : 6882
train acc:  0.9140625
train loss:  0.28642600774765015
train gradient:  0.1657157281323583
iteration : 6883
train acc:  0.84375
train loss:  0.3708767890930176
train gradient:  0.3042178586876509
iteration : 6884
train acc:  0.8515625
train loss:  0.36153745651245117
train gradient:  0.2722825356961958
iteration : 6885
train acc:  0.8125
train loss:  0.3552626669406891
train gradient:  0.26117643315256656
iteration : 6886
train acc:  0.8125
train loss:  0.3954649567604065
train gradient:  0.3543448990276
iteration : 6887
train acc:  0.890625
train loss:  0.28491097688674927
train gradient:  0.1734129178991735
iteration : 6888
train acc:  0.8515625
train loss:  0.3784599304199219
train gradient:  0.19577179548432266
iteration : 6889
train acc:  0.8515625
train loss:  0.34584155678749084
train gradient:  0.17437718951961098
iteration : 6890
train acc:  0.84375
train loss:  0.3372255563735962
train gradient:  0.22890558034037503
iteration : 6891
train acc:  0.8828125
train loss:  0.2778984308242798
train gradient:  0.14635365714634385
iteration : 6892
train acc:  0.8515625
train loss:  0.33497095108032227
train gradient:  0.14942220002804435
iteration : 6893
train acc:  0.84375
train loss:  0.33496880531311035
train gradient:  0.1770725049658052
iteration : 6894
train acc:  0.828125
train loss:  0.3882492780685425
train gradient:  0.2386237758989037
iteration : 6895
train acc:  0.8359375
train loss:  0.3723217248916626
train gradient:  0.20239986591253706
iteration : 6896
train acc:  0.84375
train loss:  0.2946557402610779
train gradient:  0.1394547638800518
iteration : 6897
train acc:  0.828125
train loss:  0.36234450340270996
train gradient:  0.2598703180457257
iteration : 6898
train acc:  0.8515625
train loss:  0.40042442083358765
train gradient:  0.25071187256623656
iteration : 6899
train acc:  0.8359375
train loss:  0.34556448459625244
train gradient:  0.2905335338132037
iteration : 6900
train acc:  0.828125
train loss:  0.357327938079834
train gradient:  0.20093239518928013
iteration : 6901
train acc:  0.8984375
train loss:  0.33149734139442444
train gradient:  0.1970688166889243
iteration : 6902
train acc:  0.90625
train loss:  0.2604961395263672
train gradient:  0.11838504011038932
iteration : 6903
train acc:  0.8984375
train loss:  0.25542452931404114
train gradient:  0.11161896777023382
iteration : 6904
train acc:  0.890625
train loss:  0.32028496265411377
train gradient:  0.20544066162610491
iteration : 6905
train acc:  0.828125
train loss:  0.38013002276420593
train gradient:  0.22620144790520627
iteration : 6906
train acc:  0.8046875
train loss:  0.3289887309074402
train gradient:  0.2247396051922456
iteration : 6907
train acc:  0.8671875
train loss:  0.3337692618370056
train gradient:  0.20104839820004233
iteration : 6908
train acc:  0.8203125
train loss:  0.3824292719364166
train gradient:  0.25166472785155897
iteration : 6909
train acc:  0.828125
train loss:  0.3664696514606476
train gradient:  0.2176380296449153
iteration : 6910
train acc:  0.8203125
train loss:  0.3818553686141968
train gradient:  0.40306791187163354
iteration : 6911
train acc:  0.8828125
train loss:  0.34398016333580017
train gradient:  0.22236763822545424
iteration : 6912
train acc:  0.84375
train loss:  0.3285175561904907
train gradient:  0.17729167893634157
iteration : 6913
train acc:  0.8046875
train loss:  0.3742688298225403
train gradient:  0.34748120489373674
iteration : 6914
train acc:  0.84375
train loss:  0.3627244830131531
train gradient:  0.31251708383131704
iteration : 6915
train acc:  0.8671875
train loss:  0.2967793345451355
train gradient:  0.17153896265583374
iteration : 6916
train acc:  0.8671875
train loss:  0.3486449420452118
train gradient:  0.18955076995686249
iteration : 6917
train acc:  0.8359375
train loss:  0.3342835009098053
train gradient:  0.2002489922603065
iteration : 6918
train acc:  0.859375
train loss:  0.2940570116043091
train gradient:  0.13037178123264914
iteration : 6919
train acc:  0.828125
train loss:  0.38901281356811523
train gradient:  0.2671159055683033
iteration : 6920
train acc:  0.859375
train loss:  0.3410039246082306
train gradient:  0.2256523493631175
iteration : 6921
train acc:  0.875
train loss:  0.2737933099269867
train gradient:  0.1658324046730148
iteration : 6922
train acc:  0.90625
train loss:  0.26970306038856506
train gradient:  0.12017660347289613
iteration : 6923
train acc:  0.9140625
train loss:  0.24977891147136688
train gradient:  0.12707985346467515
iteration : 6924
train acc:  0.78125
train loss:  0.370366632938385
train gradient:  0.27464070734002133
iteration : 6925
train acc:  0.8515625
train loss:  0.39161157608032227
train gradient:  0.2062986101939928
iteration : 6926
train acc:  0.890625
train loss:  0.2698753774166107
train gradient:  0.1632110263022228
iteration : 6927
train acc:  0.84375
train loss:  0.35698583722114563
train gradient:  0.20428119036428916
iteration : 6928
train acc:  0.8828125
train loss:  0.29910579323768616
train gradient:  0.1278428287478117
iteration : 6929
train acc:  0.8203125
train loss:  0.3877823054790497
train gradient:  0.2202773013469633
iteration : 6930
train acc:  0.859375
train loss:  0.3778492510318756
train gradient:  0.23498405351866142
iteration : 6931
train acc:  0.8828125
train loss:  0.29784053564071655
train gradient:  0.16570079123213347
iteration : 6932
train acc:  0.890625
train loss:  0.29734382033348083
train gradient:  0.16812345030717588
iteration : 6933
train acc:  0.8125
train loss:  0.3803294003009796
train gradient:  0.21836853049433214
iteration : 6934
train acc:  0.8359375
train loss:  0.38110053539276123
train gradient:  0.37419169117997925
iteration : 6935
train acc:  0.828125
train loss:  0.3412068784236908
train gradient:  0.21804322589813518
iteration : 6936
train acc:  0.859375
train loss:  0.33034586906433105
train gradient:  0.1701159953492456
iteration : 6937
train acc:  0.84375
train loss:  0.42179080843925476
train gradient:  0.24573965665967243
iteration : 6938
train acc:  0.8984375
train loss:  0.3024735450744629
train gradient:  0.2377834843353077
iteration : 6939
train acc:  0.8828125
train loss:  0.2620757818222046
train gradient:  0.11262888662713412
iteration : 6940
train acc:  0.8125
train loss:  0.38852107524871826
train gradient:  0.21081905808472706
iteration : 6941
train acc:  0.8671875
train loss:  0.3311167359352112
train gradient:  0.20770092560042266
iteration : 6942
train acc:  0.8359375
train loss:  0.414556622505188
train gradient:  0.2538725915216327
iteration : 6943
train acc:  0.8671875
train loss:  0.35110461711883545
train gradient:  0.24000168626568463
iteration : 6944
train acc:  0.875
train loss:  0.3478742837905884
train gradient:  0.14435020721650338
iteration : 6945
train acc:  0.796875
train loss:  0.4129575490951538
train gradient:  0.32849786614684784
iteration : 6946
train acc:  0.84375
train loss:  0.3255334496498108
train gradient:  0.1724198203411677
iteration : 6947
train acc:  0.84375
train loss:  0.36334070563316345
train gradient:  0.21255943737416702
iteration : 6948
train acc:  0.8671875
train loss:  0.31620216369628906
train gradient:  0.17044787797613586
iteration : 6949
train acc:  0.890625
train loss:  0.28343039751052856
train gradient:  0.13101811880814956
iteration : 6950
train acc:  0.875
train loss:  0.2741011381149292
train gradient:  0.1592831170984826
iteration : 6951
train acc:  0.8515625
train loss:  0.3268381357192993
train gradient:  0.18571929714512614
iteration : 6952
train acc:  0.84375
train loss:  0.43076419830322266
train gradient:  0.26076143947410924
iteration : 6953
train acc:  0.828125
train loss:  0.3504940867424011
train gradient:  0.28274154325371426
iteration : 6954
train acc:  0.890625
train loss:  0.2857820689678192
train gradient:  0.17520022944913838
iteration : 6955
train acc:  0.859375
train loss:  0.3097837567329407
train gradient:  0.16548948693338122
iteration : 6956
train acc:  0.7578125
train loss:  0.447416216135025
train gradient:  0.29496344187513784
iteration : 6957
train acc:  0.8515625
train loss:  0.33450978994369507
train gradient:  0.1972095522935987
iteration : 6958
train acc:  0.8359375
train loss:  0.3466830253601074
train gradient:  0.21536656731448117
iteration : 6959
train acc:  0.8359375
train loss:  0.35776805877685547
train gradient:  0.24701686787298915
iteration : 6960
train acc:  0.8515625
train loss:  0.3373887538909912
train gradient:  0.23165314107537693
iteration : 6961
train acc:  0.84375
train loss:  0.3700120449066162
train gradient:  0.2813074984132131
iteration : 6962
train acc:  0.8671875
train loss:  0.28740084171295166
train gradient:  0.1690304744176442
iteration : 6963
train acc:  0.8203125
train loss:  0.40017759799957275
train gradient:  0.32744979360933657
iteration : 6964
train acc:  0.859375
train loss:  0.33645182847976685
train gradient:  0.16005318530630397
iteration : 6965
train acc:  0.8671875
train loss:  0.3065335750579834
train gradient:  0.23624278464543969
iteration : 6966
train acc:  0.859375
train loss:  0.35688069462776184
train gradient:  0.18446775629808537
iteration : 6967
train acc:  0.84375
train loss:  0.31552112102508545
train gradient:  0.12897008526118264
iteration : 6968
train acc:  0.796875
train loss:  0.3607693314552307
train gradient:  0.20200403567504024
iteration : 6969
train acc:  0.8671875
train loss:  0.26826906204223633
train gradient:  0.15524736309616988
iteration : 6970
train acc:  0.8515625
train loss:  0.34727954864501953
train gradient:  0.18089608228319304
iteration : 6971
train acc:  0.7890625
train loss:  0.45813119411468506
train gradient:  0.3807307756068747
iteration : 6972
train acc:  0.8515625
train loss:  0.3400944471359253
train gradient:  0.14493497873804712
iteration : 6973
train acc:  0.8671875
train loss:  0.3284008204936981
train gradient:  0.19421946731398182
iteration : 6974
train acc:  0.8671875
train loss:  0.2947142720222473
train gradient:  0.1692612117974497
iteration : 6975
train acc:  0.8125
train loss:  0.3946937918663025
train gradient:  0.2317993939294144
iteration : 6976
train acc:  0.8046875
train loss:  0.4080467224121094
train gradient:  0.23239039577491732
iteration : 6977
train acc:  0.8671875
train loss:  0.30474185943603516
train gradient:  0.15019562068414866
iteration : 6978
train acc:  0.828125
train loss:  0.3797016143798828
train gradient:  0.2831519662406207
iteration : 6979
train acc:  0.8984375
train loss:  0.26159340143203735
train gradient:  0.1791418763085845
iteration : 6980
train acc:  0.890625
train loss:  0.26800084114074707
train gradient:  0.17537300976813613
iteration : 6981
train acc:  0.8515625
train loss:  0.351999968290329
train gradient:  0.21076134247161293
iteration : 6982
train acc:  0.828125
train loss:  0.4108029007911682
train gradient:  0.3560764476242487
iteration : 6983
train acc:  0.875
train loss:  0.30011090636253357
train gradient:  0.17003394362236207
iteration : 6984
train acc:  0.8515625
train loss:  0.32096874713897705
train gradient:  0.19205157368554868
iteration : 6985
train acc:  0.8515625
train loss:  0.3376876711845398
train gradient:  0.1448671310394244
iteration : 6986
train acc:  0.8359375
train loss:  0.3158649206161499
train gradient:  0.1662387765475611
iteration : 6987
train acc:  0.890625
train loss:  0.32040297985076904
train gradient:  0.13474175227450325
iteration : 6988
train acc:  0.8125
train loss:  0.41151684522628784
train gradient:  0.2773413682996093
iteration : 6989
train acc:  0.8515625
train loss:  0.33811187744140625
train gradient:  0.13569732999839218
iteration : 6990
train acc:  0.8125
train loss:  0.4290637671947479
train gradient:  0.24880498885355667
iteration : 6991
train acc:  0.8671875
train loss:  0.3812296688556671
train gradient:  0.16378905401650803
iteration : 6992
train acc:  0.8671875
train loss:  0.3088153004646301
train gradient:  0.16022008088470954
iteration : 6993
train acc:  0.8828125
train loss:  0.3239882290363312
train gradient:  0.18216571695701333
iteration : 6994
train acc:  0.8828125
train loss:  0.3420715928077698
train gradient:  0.2439905830789894
iteration : 6995
train acc:  0.8984375
train loss:  0.2703600525856018
train gradient:  0.12646543920467435
iteration : 6996
train acc:  0.8203125
train loss:  0.38183313608169556
train gradient:  0.19872579007718677
iteration : 6997
train acc:  0.859375
train loss:  0.32248562574386597
train gradient:  0.1770087480580168
iteration : 6998
train acc:  0.9140625
train loss:  0.2824598550796509
train gradient:  0.1391449487293845
iteration : 6999
train acc:  0.890625
train loss:  0.28800511360168457
train gradient:  0.16360892395798404
iteration : 7000
train acc:  0.84375
train loss:  0.34860074520111084
train gradient:  0.1853296961885752
iteration : 7001
train acc:  0.859375
train loss:  0.33612391352653503
train gradient:  0.1642047467934046
iteration : 7002
train acc:  0.7890625
train loss:  0.4295859634876251
train gradient:  0.24961674080535196
iteration : 7003
train acc:  0.859375
train loss:  0.3425978422164917
train gradient:  0.19895556141964432
iteration : 7004
train acc:  0.9375
train loss:  0.250197172164917
train gradient:  0.15160238618047497
iteration : 7005
train acc:  0.84375
train loss:  0.35807204246520996
train gradient:  0.17010913988688303
iteration : 7006
train acc:  0.8984375
train loss:  0.29565948247909546
train gradient:  0.1746103880443926
iteration : 7007
train acc:  0.8203125
train loss:  0.3853129744529724
train gradient:  0.22033192151557884
iteration : 7008
train acc:  0.8125
train loss:  0.3792163133621216
train gradient:  0.22240357607529515
iteration : 7009
train acc:  0.8515625
train loss:  0.32495972514152527
train gradient:  0.1741221953381005
iteration : 7010
train acc:  0.8359375
train loss:  0.3321753144264221
train gradient:  0.18883914047112876
iteration : 7011
train acc:  0.8671875
train loss:  0.27363723516464233
train gradient:  0.17965792389357388
iteration : 7012
train acc:  0.859375
train loss:  0.34364598989486694
train gradient:  0.20959358266362318
iteration : 7013
train acc:  0.8359375
train loss:  0.35891979932785034
train gradient:  0.2396991475461603
iteration : 7014
train acc:  0.9140625
train loss:  0.2547442317008972
train gradient:  0.11425364942218233
iteration : 7015
train acc:  0.8125
train loss:  0.4122283458709717
train gradient:  0.19835865742606767
iteration : 7016
train acc:  0.84375
train loss:  0.38045534491539
train gradient:  0.18340012584200632
iteration : 7017
train acc:  0.8828125
train loss:  0.2674578130245209
train gradient:  0.14978658156291583
iteration : 7018
train acc:  0.890625
train loss:  0.30726712942123413
train gradient:  0.17896775678694588
iteration : 7019
train acc:  0.8359375
train loss:  0.334139347076416
train gradient:  0.23204310540482326
iteration : 7020
train acc:  0.8359375
train loss:  0.3895699977874756
train gradient:  0.24325000829730048
iteration : 7021
train acc:  0.8359375
train loss:  0.3446766436100006
train gradient:  0.14618167897905526
iteration : 7022
train acc:  0.8203125
train loss:  0.3339005708694458
train gradient:  0.16783108477236497
iteration : 7023
train acc:  0.78125
train loss:  0.37821704149246216
train gradient:  0.2422660468251761
iteration : 7024
train acc:  0.84375
train loss:  0.3078172504901886
train gradient:  0.214266438482849
iteration : 7025
train acc:  0.8828125
train loss:  0.2884141802787781
train gradient:  0.20328729604072177
iteration : 7026
train acc:  0.8671875
train loss:  0.2689609229564667
train gradient:  0.1689272882456121
iteration : 7027
train acc:  0.7890625
train loss:  0.3903449773788452
train gradient:  0.21248347505354392
iteration : 7028
train acc:  0.890625
train loss:  0.2866802215576172
train gradient:  0.14550020339417372
iteration : 7029
train acc:  0.828125
train loss:  0.3333899974822998
train gradient:  0.2097670847423896
iteration : 7030
train acc:  0.9140625
train loss:  0.2559279501438141
train gradient:  0.15273904089070262
iteration : 7031
train acc:  0.8671875
train loss:  0.3158309757709503
train gradient:  0.17658248368093332
iteration : 7032
train acc:  0.8515625
train loss:  0.36335664987564087
train gradient:  0.19606295155806253
iteration : 7033
train acc:  0.890625
train loss:  0.24938805401325226
train gradient:  0.17614329662806422
iteration : 7034
train acc:  0.8359375
train loss:  0.3250795006752014
train gradient:  0.18675797432011057
iteration : 7035
train acc:  0.859375
train loss:  0.3263113796710968
train gradient:  0.22444941555119696
iteration : 7036
train acc:  0.8359375
train loss:  0.35983240604400635
train gradient:  0.2297724845753839
iteration : 7037
train acc:  0.8203125
train loss:  0.41176632046699524
train gradient:  0.2834376257240124
iteration : 7038
train acc:  0.78125
train loss:  0.41718414425849915
train gradient:  0.28403187113782974
iteration : 7039
train acc:  0.859375
train loss:  0.3133663833141327
train gradient:  0.24218520603028848
iteration : 7040
train acc:  0.8515625
train loss:  0.3320198059082031
train gradient:  0.1836457523839165
iteration : 7041
train acc:  0.8671875
train loss:  0.33876678347587585
train gradient:  0.2304473177259711
iteration : 7042
train acc:  0.7734375
train loss:  0.468010812997818
train gradient:  0.40224725590189064
iteration : 7043
train acc:  0.890625
train loss:  0.32088813185691833
train gradient:  0.28980200333798145
iteration : 7044
train acc:  0.8515625
train loss:  0.3143465518951416
train gradient:  0.19427655514520342
iteration : 7045
train acc:  0.875
train loss:  0.3140808343887329
train gradient:  0.18652058840021896
iteration : 7046
train acc:  0.8203125
train loss:  0.3570156693458557
train gradient:  0.17508706469293495
iteration : 7047
train acc:  0.8125
train loss:  0.37920111417770386
train gradient:  0.30550777975192134
iteration : 7048
train acc:  0.8671875
train loss:  0.3114975392818451
train gradient:  0.1815799211412113
iteration : 7049
train acc:  0.8515625
train loss:  0.30764663219451904
train gradient:  0.24185500198696241
iteration : 7050
train acc:  0.875
train loss:  0.26707524061203003
train gradient:  0.1377987152889546
iteration : 7051
train acc:  0.84375
train loss:  0.3918563723564148
train gradient:  0.2518004322137624
iteration : 7052
train acc:  0.8828125
train loss:  0.31741833686828613
train gradient:  0.10877219121821699
iteration : 7053
train acc:  0.8671875
train loss:  0.3110859990119934
train gradient:  0.14755255395074757
iteration : 7054
train acc:  0.8203125
train loss:  0.4093617796897888
train gradient:  0.29996047267005405
iteration : 7055
train acc:  0.890625
train loss:  0.3168666362762451
train gradient:  0.1992167740484877
iteration : 7056
train acc:  0.8515625
train loss:  0.2847272753715515
train gradient:  0.17897003839090783
iteration : 7057
train acc:  0.8515625
train loss:  0.3917768597602844
train gradient:  0.2621972431374587
iteration : 7058
train acc:  0.8515625
train loss:  0.3289107084274292
train gradient:  0.20058935672847034
iteration : 7059
train acc:  0.84375
train loss:  0.3043450713157654
train gradient:  0.2127581239747832
iteration : 7060
train acc:  0.875
train loss:  0.30137866735458374
train gradient:  0.1693067310400538
iteration : 7061
train acc:  0.8671875
train loss:  0.3138364553451538
train gradient:  0.1525105918571671
iteration : 7062
train acc:  0.8828125
train loss:  0.32197335362434387
train gradient:  0.16792765964786274
iteration : 7063
train acc:  0.8515625
train loss:  0.2815709710121155
train gradient:  0.23151441373740073
iteration : 7064
train acc:  0.8125
train loss:  0.340536892414093
train gradient:  0.18876575554710168
iteration : 7065
train acc:  0.765625
train loss:  0.4520098865032196
train gradient:  0.297269441108751
iteration : 7066
train acc:  0.84375
train loss:  0.3275872468948364
train gradient:  0.19307507153454434
iteration : 7067
train acc:  0.8515625
train loss:  0.3669360280036926
train gradient:  0.20084679301581473
iteration : 7068
train acc:  0.8125
train loss:  0.368604451417923
train gradient:  0.2956738869578692
iteration : 7069
train acc:  0.8828125
train loss:  0.28226593136787415
train gradient:  0.17995786310423864
iteration : 7070
train acc:  0.8359375
train loss:  0.374785840511322
train gradient:  0.16964555089248506
iteration : 7071
train acc:  0.859375
train loss:  0.3553207516670227
train gradient:  0.1936688500572404
iteration : 7072
train acc:  0.78125
train loss:  0.46761375665664673
train gradient:  0.31738472026952147
iteration : 7073
train acc:  0.90625
train loss:  0.2711549401283264
train gradient:  0.1122907448421355
iteration : 7074
train acc:  0.90625
train loss:  0.29039597511291504
train gradient:  0.1454115394880696
iteration : 7075
train acc:  0.890625
train loss:  0.3054552972316742
train gradient:  0.21980593986203634
iteration : 7076
train acc:  0.8828125
train loss:  0.2904159128665924
train gradient:  0.20145445572514298
iteration : 7077
train acc:  0.8515625
train loss:  0.3697429895401001
train gradient:  0.3109105817290845
iteration : 7078
train acc:  0.875
train loss:  0.3449706733226776
train gradient:  0.29845607953315545
iteration : 7079
train acc:  0.8125
train loss:  0.47379040718078613
train gradient:  0.34340426277943664
iteration : 7080
train acc:  0.8984375
train loss:  0.24410253763198853
train gradient:  0.13249051631280473
iteration : 7081
train acc:  0.8828125
train loss:  0.29980719089508057
train gradient:  0.1564573425996102
iteration : 7082
train acc:  0.8125
train loss:  0.3577622175216675
train gradient:  0.14042635175035467
iteration : 7083
train acc:  0.859375
train loss:  0.2942495346069336
train gradient:  0.2085994003461062
iteration : 7084
train acc:  0.8046875
train loss:  0.3695056736469269
train gradient:  0.18530415016778862
iteration : 7085
train acc:  0.875
train loss:  0.2926974594593048
train gradient:  0.12522275329963886
iteration : 7086
train acc:  0.8984375
train loss:  0.27871978282928467
train gradient:  0.16899661352713133
iteration : 7087
train acc:  0.859375
train loss:  0.34518224000930786
train gradient:  0.15728055705719654
iteration : 7088
train acc:  0.84375
train loss:  0.375309556722641
train gradient:  0.260594492932504
iteration : 7089
train acc:  0.8359375
train loss:  0.37269341945648193
train gradient:  0.24882576210424281
iteration : 7090
train acc:  0.859375
train loss:  0.3441895544528961
train gradient:  0.2239812071400303
iteration : 7091
train acc:  0.875
train loss:  0.3381691575050354
train gradient:  0.24168311568943376
iteration : 7092
train acc:  0.8046875
train loss:  0.4101625680923462
train gradient:  0.37306797155627724
iteration : 7093
train acc:  0.8515625
train loss:  0.36695966124534607
train gradient:  0.20451077353829575
iteration : 7094
train acc:  0.890625
train loss:  0.2781882882118225
train gradient:  0.15195501470508999
iteration : 7095
train acc:  0.90625
train loss:  0.3255479335784912
train gradient:  0.1631337916335803
iteration : 7096
train acc:  0.8046875
train loss:  0.47315317392349243
train gradient:  0.28667482758571544
iteration : 7097
train acc:  0.890625
train loss:  0.30317163467407227
train gradient:  0.17274110688923336
iteration : 7098
train acc:  0.875
train loss:  0.33325111865997314
train gradient:  0.15194575185395903
iteration : 7099
train acc:  0.8125
train loss:  0.3993509113788605
train gradient:  0.31253629939489913
iteration : 7100
train acc:  0.8359375
train loss:  0.3327973484992981
train gradient:  0.17156675249490538
iteration : 7101
train acc:  0.875
train loss:  0.29724615812301636
train gradient:  0.12814992844833864
iteration : 7102
train acc:  0.8671875
train loss:  0.3298689126968384
train gradient:  0.1700879840628125
iteration : 7103
train acc:  0.84375
train loss:  0.30521345138549805
train gradient:  0.19987635451247393
iteration : 7104
train acc:  0.859375
train loss:  0.3314617872238159
train gradient:  0.24800557580218868
iteration : 7105
train acc:  0.8125
train loss:  0.39057183265686035
train gradient:  0.21529075111469098
iteration : 7106
train acc:  0.828125
train loss:  0.37357068061828613
train gradient:  0.24325285262157223
iteration : 7107
train acc:  0.875
train loss:  0.34461289644241333
train gradient:  0.22239975375351567
iteration : 7108
train acc:  0.90625
train loss:  0.2810817360877991
train gradient:  0.2255430441583438
iteration : 7109
train acc:  0.8359375
train loss:  0.3282056152820587
train gradient:  0.13511827116024971
iteration : 7110
train acc:  0.8515625
train loss:  0.37250956892967224
train gradient:  0.19598174312598068
iteration : 7111
train acc:  0.765625
train loss:  0.4095931649208069
train gradient:  0.26165028733566875
iteration : 7112
train acc:  0.8046875
train loss:  0.4914740324020386
train gradient:  0.39190781871164565
iteration : 7113
train acc:  0.84375
train loss:  0.40204063057899475
train gradient:  0.2642526855233485
iteration : 7114
train acc:  0.8203125
train loss:  0.365130752325058
train gradient:  0.15799698365051684
iteration : 7115
train acc:  0.8515625
train loss:  0.29289710521698
train gradient:  0.12824503664597722
iteration : 7116
train acc:  0.828125
train loss:  0.3488498628139496
train gradient:  0.26503988128753175
iteration : 7117
train acc:  0.8203125
train loss:  0.3680208921432495
train gradient:  0.21761594988940042
iteration : 7118
train acc:  0.8671875
train loss:  0.32860463857650757
train gradient:  0.14490009633598538
iteration : 7119
train acc:  0.796875
train loss:  0.3929111957550049
train gradient:  0.2144707509378132
iteration : 7120
train acc:  0.8671875
train loss:  0.289486825466156
train gradient:  0.1345059719985166
iteration : 7121
train acc:  0.8828125
train loss:  0.3040273189544678
train gradient:  0.21606809278864667
iteration : 7122
train acc:  0.8671875
train loss:  0.3489964008331299
train gradient:  0.16033508742650648
iteration : 7123
train acc:  0.828125
train loss:  0.31110602617263794
train gradient:  0.22821851450203556
iteration : 7124
train acc:  0.8828125
train loss:  0.29439932107925415
train gradient:  0.13479280428691529
iteration : 7125
train acc:  0.8984375
train loss:  0.30126431584358215
train gradient:  0.16232622863766627
iteration : 7126
train acc:  0.828125
train loss:  0.35411545634269714
train gradient:  0.17707747538182067
iteration : 7127
train acc:  0.8671875
train loss:  0.3078762888908386
train gradient:  0.2517389190951579
iteration : 7128
train acc:  0.8203125
train loss:  0.35865920782089233
train gradient:  0.23576488167047538
iteration : 7129
train acc:  0.9140625
train loss:  0.26973649859428406
train gradient:  0.15380366079112093
iteration : 7130
train acc:  0.7890625
train loss:  0.37062495946884155
train gradient:  0.2893366350746923
iteration : 7131
train acc:  0.8046875
train loss:  0.39016008377075195
train gradient:  0.22470314754281445
iteration : 7132
train acc:  0.859375
train loss:  0.30726122856140137
train gradient:  0.12793522402142737
iteration : 7133
train acc:  0.8515625
train loss:  0.343257874250412
train gradient:  0.29435700039022444
iteration : 7134
train acc:  0.796875
train loss:  0.40800920128822327
train gradient:  0.2745688483861706
iteration : 7135
train acc:  0.8359375
train loss:  0.3172794282436371
train gradient:  0.1734211619808932
iteration : 7136
train acc:  0.828125
train loss:  0.39587879180908203
train gradient:  0.2909162095972232
iteration : 7137
train acc:  0.8828125
train loss:  0.2907472848892212
train gradient:  0.149581541220367
iteration : 7138
train acc:  0.8671875
train loss:  0.33792445063591003
train gradient:  0.283635529695945
iteration : 7139
train acc:  0.796875
train loss:  0.4450240731239319
train gradient:  0.32325598712736836
iteration : 7140
train acc:  0.84375
train loss:  0.3347272276878357
train gradient:  0.1642421196056882
iteration : 7141
train acc:  0.9140625
train loss:  0.27839595079421997
train gradient:  0.11577927582587443
iteration : 7142
train acc:  0.8515625
train loss:  0.38247084617614746
train gradient:  0.25314775638123144
iteration : 7143
train acc:  0.84375
train loss:  0.37621086835861206
train gradient:  0.21936414268207716
iteration : 7144
train acc:  0.875
train loss:  0.2909952402114868
train gradient:  0.17861388388803798
iteration : 7145
train acc:  0.7890625
train loss:  0.44625145196914673
train gradient:  0.3710123457320472
iteration : 7146
train acc:  0.8671875
train loss:  0.30252373218536377
train gradient:  0.18503393810289653
iteration : 7147
train acc:  0.8671875
train loss:  0.3045978546142578
train gradient:  0.14844603633731024
iteration : 7148
train acc:  0.84375
train loss:  0.3721274435520172
train gradient:  0.1924500340764867
iteration : 7149
train acc:  0.828125
train loss:  0.3789407014846802
train gradient:  0.17674184690187228
iteration : 7150
train acc:  0.875
train loss:  0.2660752236843109
train gradient:  0.1334387239576408
iteration : 7151
train acc:  0.8046875
train loss:  0.38482314348220825
train gradient:  0.22536065097287808
iteration : 7152
train acc:  0.8515625
train loss:  0.3351778984069824
train gradient:  0.18440524176423567
iteration : 7153
train acc:  0.90625
train loss:  0.30290472507476807
train gradient:  0.1822890099273236
iteration : 7154
train acc:  0.875
train loss:  0.2706546187400818
train gradient:  0.18041483843313194
iteration : 7155
train acc:  0.8515625
train loss:  0.3550265431404114
train gradient:  0.2405206537679214
iteration : 7156
train acc:  0.8359375
train loss:  0.37767571210861206
train gradient:  0.2772852354725358
iteration : 7157
train acc:  0.8125
train loss:  0.44494372606277466
train gradient:  0.29342143565335294
iteration : 7158
train acc:  0.8125
train loss:  0.4340795874595642
train gradient:  0.30383527293249396
iteration : 7159
train acc:  0.859375
train loss:  0.3899645209312439
train gradient:  0.21105180461725104
iteration : 7160
train acc:  0.875
train loss:  0.3227580189704895
train gradient:  0.2155673007995959
iteration : 7161
train acc:  0.84375
train loss:  0.37790602445602417
train gradient:  0.1828536843093959
iteration : 7162
train acc:  0.8515625
train loss:  0.35004252195358276
train gradient:  0.1565759022658808
iteration : 7163
train acc:  0.828125
train loss:  0.3428756594657898
train gradient:  0.3647097617452326
iteration : 7164
train acc:  0.8359375
train loss:  0.3682289719581604
train gradient:  0.20878608369490054
iteration : 7165
train acc:  0.859375
train loss:  0.2715527415275574
train gradient:  0.11373321492397104
iteration : 7166
train acc:  0.8359375
train loss:  0.38067078590393066
train gradient:  0.26033672337668146
iteration : 7167
train acc:  0.8671875
train loss:  0.3477581739425659
train gradient:  0.17054884505965903
iteration : 7168
train acc:  0.859375
train loss:  0.3883233666419983
train gradient:  0.217633565937916
iteration : 7169
train acc:  0.796875
train loss:  0.44092005491256714
train gradient:  0.2863561743384945
iteration : 7170
train acc:  0.828125
train loss:  0.3570457100868225
train gradient:  0.17028693569531989
iteration : 7171
train acc:  0.8828125
train loss:  0.2824850380420685
train gradient:  0.22932212201285115
iteration : 7172
train acc:  0.890625
train loss:  0.3365408778190613
train gradient:  0.14687739883694678
iteration : 7173
train acc:  0.859375
train loss:  0.3572111129760742
train gradient:  0.18948052255407133
iteration : 7174
train acc:  0.875
train loss:  0.3173569142818451
train gradient:  0.21140156949609562
iteration : 7175
train acc:  0.8984375
train loss:  0.26973792910575867
train gradient:  0.11306822165515701
iteration : 7176
train acc:  0.8671875
train loss:  0.2949000597000122
train gradient:  0.13995229637394613
iteration : 7177
train acc:  0.8671875
train loss:  0.2869681119918823
train gradient:  0.1622708736803783
iteration : 7178
train acc:  0.828125
train loss:  0.35617518424987793
train gradient:  0.14391139279334525
iteration : 7179
train acc:  0.875
train loss:  0.319254994392395
train gradient:  0.1489918052421969
iteration : 7180
train acc:  0.90625
train loss:  0.26849550008773804
train gradient:  0.13009092062166044
iteration : 7181
train acc:  0.8359375
train loss:  0.4135630428791046
train gradient:  0.2697868075928096
iteration : 7182
train acc:  0.796875
train loss:  0.3683127164840698
train gradient:  0.23322074985770982
iteration : 7183
train acc:  0.8671875
train loss:  0.3828180432319641
train gradient:  0.2471532190002233
iteration : 7184
train acc:  0.8125
train loss:  0.36701303720474243
train gradient:  0.30679674895571624
iteration : 7185
train acc:  0.859375
train loss:  0.3332763910293579
train gradient:  0.22008773216494076
iteration : 7186
train acc:  0.828125
train loss:  0.4187532365322113
train gradient:  0.33306513553903766
iteration : 7187
train acc:  0.859375
train loss:  0.291557252407074
train gradient:  0.14145664345887154
iteration : 7188
train acc:  0.8515625
train loss:  0.30502307415008545
train gradient:  0.15820491809321446
iteration : 7189
train acc:  0.828125
train loss:  0.37820881605148315
train gradient:  0.1644242854337813
iteration : 7190
train acc:  0.8046875
train loss:  0.4206708073616028
train gradient:  0.23206058280295982
iteration : 7191
train acc:  0.7890625
train loss:  0.4103546440601349
train gradient:  0.2285383965896649
iteration : 7192
train acc:  0.8828125
train loss:  0.2715759873390198
train gradient:  0.12463963647821436
iteration : 7193
train acc:  0.8515625
train loss:  0.3513377010822296
train gradient:  0.17077760069047554
iteration : 7194
train acc:  0.8671875
train loss:  0.32335078716278076
train gradient:  0.17355171818330895
iteration : 7195
train acc:  0.890625
train loss:  0.30441272258758545
train gradient:  0.1907124593578942
iteration : 7196
train acc:  0.8671875
train loss:  0.31578993797302246
train gradient:  0.1600773342613756
iteration : 7197
train acc:  0.859375
train loss:  0.33978885412216187
train gradient:  0.19158761929050644
iteration : 7198
train acc:  0.8515625
train loss:  0.36938101053237915
train gradient:  0.24266490457594414
iteration : 7199
train acc:  0.8125
train loss:  0.4336104989051819
train gradient:  0.28557802327753945
iteration : 7200
train acc:  0.84375
train loss:  0.3319774270057678
train gradient:  0.17429312303248717
iteration : 7201
train acc:  0.8671875
train loss:  0.38254088163375854
train gradient:  0.2777980602552589
iteration : 7202
train acc:  0.90625
train loss:  0.29469728469848633
train gradient:  0.15759706581481464
iteration : 7203
train acc:  0.828125
train loss:  0.41163307428359985
train gradient:  0.35146319545610577
iteration : 7204
train acc:  0.90625
train loss:  0.23414722084999084
train gradient:  0.09199650192253385
iteration : 7205
train acc:  0.84375
train loss:  0.3611251711845398
train gradient:  0.32653966245649557
iteration : 7206
train acc:  0.84375
train loss:  0.3253422975540161
train gradient:  0.24156696349603943
iteration : 7207
train acc:  0.8125
train loss:  0.3413812816143036
train gradient:  0.21550243191369858
iteration : 7208
train acc:  0.890625
train loss:  0.31123656034469604
train gradient:  0.20109193186310817
iteration : 7209
train acc:  0.8125
train loss:  0.3515777587890625
train gradient:  0.23688383759437454
iteration : 7210
train acc:  0.8671875
train loss:  0.2809092402458191
train gradient:  0.10459514834873153
iteration : 7211
train acc:  0.8046875
train loss:  0.40008771419525146
train gradient:  0.24525090159184676
iteration : 7212
train acc:  0.8671875
train loss:  0.35408636927604675
train gradient:  0.31429376781133345
iteration : 7213
train acc:  0.8984375
train loss:  0.2940354347229004
train gradient:  0.17967729688965178
iteration : 7214
train acc:  0.890625
train loss:  0.33475321531295776
train gradient:  0.15935565607872737
iteration : 7215
train acc:  0.8671875
train loss:  0.27564987540245056
train gradient:  0.11211759736536044
iteration : 7216
train acc:  0.796875
train loss:  0.38468167185783386
train gradient:  0.17468476140914863
iteration : 7217
train acc:  0.8515625
train loss:  0.41342848539352417
train gradient:  0.3304775146935647
iteration : 7218
train acc:  0.8671875
train loss:  0.31959834694862366
train gradient:  0.17268769432994494
iteration : 7219
train acc:  0.828125
train loss:  0.38312065601348877
train gradient:  0.22956348489943815
iteration : 7220
train acc:  0.90625
train loss:  0.3219362497329712
train gradient:  0.18290709447588918
iteration : 7221
train acc:  0.8515625
train loss:  0.3761201798915863
train gradient:  0.22443558296509347
iteration : 7222
train acc:  0.859375
train loss:  0.3548683822154999
train gradient:  0.3212543560622899
iteration : 7223
train acc:  0.8828125
train loss:  0.3174029290676117
train gradient:  0.1836390503504995
iteration : 7224
train acc:  0.796875
train loss:  0.41763949394226074
train gradient:  0.20448117978247046
iteration : 7225
train acc:  0.8828125
train loss:  0.29700779914855957
train gradient:  0.15765777633641434
iteration : 7226
train acc:  0.84375
train loss:  0.3219628930091858
train gradient:  0.15687921427882992
iteration : 7227
train acc:  0.8125
train loss:  0.3970644474029541
train gradient:  0.2573492830101765
iteration : 7228
train acc:  0.84375
train loss:  0.379679799079895
train gradient:  0.2741605673574479
iteration : 7229
train acc:  0.859375
train loss:  0.3166239857673645
train gradient:  0.17775727626617321
iteration : 7230
train acc:  0.875
train loss:  0.30267155170440674
train gradient:  0.1559470247127874
iteration : 7231
train acc:  0.8828125
train loss:  0.2829456031322479
train gradient:  0.1539537260421054
iteration : 7232
train acc:  0.875
train loss:  0.3313775658607483
train gradient:  0.2830915039267462
iteration : 7233
train acc:  0.796875
train loss:  0.4468907117843628
train gradient:  0.32216386176640743
iteration : 7234
train acc:  0.8359375
train loss:  0.40275517106056213
train gradient:  0.20671483802958718
iteration : 7235
train acc:  0.78125
train loss:  0.4471516013145447
train gradient:  0.27844616825530094
iteration : 7236
train acc:  0.875
train loss:  0.3146284818649292
train gradient:  0.15708727077227616
iteration : 7237
train acc:  0.8203125
train loss:  0.3753703534603119
train gradient:  0.2031496564591322
iteration : 7238
train acc:  0.84375
train loss:  0.35383033752441406
train gradient:  0.2304817111405323
iteration : 7239
train acc:  0.8515625
train loss:  0.3061710596084595
train gradient:  0.12566463576344392
iteration : 7240
train acc:  0.8828125
train loss:  0.26430660486221313
train gradient:  0.13486510242601962
iteration : 7241
train acc:  0.84375
train loss:  0.31635400652885437
train gradient:  0.1745880247692603
iteration : 7242
train acc:  0.859375
train loss:  0.3218957185745239
train gradient:  0.22526392473732587
iteration : 7243
train acc:  0.8984375
train loss:  0.29526621103286743
train gradient:  0.14376715135632062
iteration : 7244
train acc:  0.78125
train loss:  0.3732362687587738
train gradient:  0.19112635181867033
iteration : 7245
train acc:  0.828125
train loss:  0.3881969451904297
train gradient:  0.29928539733149395
iteration : 7246
train acc:  0.9140625
train loss:  0.259467750787735
train gradient:  0.13076510657577467
iteration : 7247
train acc:  0.9375
train loss:  0.2373344749212265
train gradient:  0.12504931760296611
iteration : 7248
train acc:  0.8828125
train loss:  0.2914966940879822
train gradient:  0.12773007334038453
iteration : 7249
train acc:  0.859375
train loss:  0.33054226636886597
train gradient:  0.1653670008391163
iteration : 7250
train acc:  0.8125
train loss:  0.4133296310901642
train gradient:  0.31165780987907654
iteration : 7251
train acc:  0.8671875
train loss:  0.338356077671051
train gradient:  0.17588984307118607
iteration : 7252
train acc:  0.828125
train loss:  0.36144208908081055
train gradient:  0.23280247395133702
iteration : 7253
train acc:  0.8203125
train loss:  0.35576263070106506
train gradient:  0.1918387046345274
iteration : 7254
train acc:  0.828125
train loss:  0.3985941708087921
train gradient:  0.17754352292205053
iteration : 7255
train acc:  0.90625
train loss:  0.2853256165981293
train gradient:  0.1658406841118913
iteration : 7256
train acc:  0.7890625
train loss:  0.36599496006965637
train gradient:  0.24661162812337062
iteration : 7257
train acc:  0.8515625
train loss:  0.33767688274383545
train gradient:  0.2856979621447021
iteration : 7258
train acc:  0.8125
train loss:  0.4055078625679016
train gradient:  0.1676240714558245
iteration : 7259
train acc:  0.875
train loss:  0.25499865412712097
train gradient:  0.12867325350296444
iteration : 7260
train acc:  0.8359375
train loss:  0.3558693528175354
train gradient:  0.26763719075129744
iteration : 7261
train acc:  0.8125
train loss:  0.4061858057975769
train gradient:  0.21156291078392242
iteration : 7262
train acc:  0.890625
train loss:  0.2419397532939911
train gradient:  0.09942154839730734
iteration : 7263
train acc:  0.8671875
train loss:  0.31523317098617554
train gradient:  0.17212653623064167
iteration : 7264
train acc:  0.8828125
train loss:  0.2922908067703247
train gradient:  0.18818911156212453
iteration : 7265
train acc:  0.859375
train loss:  0.3398602604866028
train gradient:  0.2676766843583621
iteration : 7266
train acc:  0.84375
train loss:  0.33532238006591797
train gradient:  0.1750277281277911
iteration : 7267
train acc:  0.875
train loss:  0.36119508743286133
train gradient:  0.18454789996243084
iteration : 7268
train acc:  0.8515625
train loss:  0.43103575706481934
train gradient:  0.24961108743240862
iteration : 7269
train acc:  0.8359375
train loss:  0.35754698514938354
train gradient:  0.20299587131985591
iteration : 7270
train acc:  0.828125
train loss:  0.4022451937198639
train gradient:  0.193078620518653
iteration : 7271
train acc:  0.84375
train loss:  0.34673893451690674
train gradient:  0.19888863293328984
iteration : 7272
train acc:  0.890625
train loss:  0.30327165126800537
train gradient:  0.18358949126872937
iteration : 7273
train acc:  0.875
train loss:  0.28429070115089417
train gradient:  0.15460346318395066
iteration : 7274
train acc:  0.8515625
train loss:  0.31205394864082336
train gradient:  0.1878051999132176
iteration : 7275
train acc:  0.828125
train loss:  0.36735379695892334
train gradient:  0.2337787179953425
iteration : 7276
train acc:  0.890625
train loss:  0.2904990017414093
train gradient:  0.1588924597611614
iteration : 7277
train acc:  0.8125
train loss:  0.4044002890586853
train gradient:  0.22214243329001215
iteration : 7278
train acc:  0.8203125
train loss:  0.39289960265159607
train gradient:  0.24472539460896725
iteration : 7279
train acc:  0.8125
train loss:  0.39231500029563904
train gradient:  0.24939582579372852
iteration : 7280
train acc:  0.8828125
train loss:  0.3155081570148468
train gradient:  0.1652259028071833
iteration : 7281
train acc:  0.7890625
train loss:  0.4943610429763794
train gradient:  0.4047828158917174
iteration : 7282
train acc:  0.84375
train loss:  0.3224904239177704
train gradient:  0.19387149552980493
iteration : 7283
train acc:  0.8515625
train loss:  0.3709036707878113
train gradient:  0.24029787177735806
iteration : 7284
train acc:  0.828125
train loss:  0.35722118616104126
train gradient:  0.23201946362665535
iteration : 7285
train acc:  0.8515625
train loss:  0.3134212791919708
train gradient:  0.1649039471303008
iteration : 7286
train acc:  0.8046875
train loss:  0.36109909415245056
train gradient:  0.21859769686055391
iteration : 7287
train acc:  0.890625
train loss:  0.35157155990600586
train gradient:  0.16568873883735205
iteration : 7288
train acc:  0.84375
train loss:  0.3140651285648346
train gradient:  0.22548667918379783
iteration : 7289
train acc:  0.84375
train loss:  0.2785666584968567
train gradient:  0.19944515980603253
iteration : 7290
train acc:  0.8046875
train loss:  0.44069796800613403
train gradient:  0.3310437707022153
iteration : 7291
train acc:  0.875
train loss:  0.3537527918815613
train gradient:  0.18950331577206408
iteration : 7292
train acc:  0.8984375
train loss:  0.2514229714870453
train gradient:  0.156926314631944
iteration : 7293
train acc:  0.84375
train loss:  0.3263794183731079
train gradient:  0.19920235414609516
iteration : 7294
train acc:  0.8046875
train loss:  0.3831797242164612
train gradient:  0.33886668932528163
iteration : 7295
train acc:  0.828125
train loss:  0.3704582452774048
train gradient:  0.2721329851020961
iteration : 7296
train acc:  0.84375
train loss:  0.33967334032058716
train gradient:  0.17555894824491253
iteration : 7297
train acc:  0.8515625
train loss:  0.32237571477890015
train gradient:  0.23584009668811623
iteration : 7298
train acc:  0.828125
train loss:  0.36648622155189514
train gradient:  0.19488401719177792
iteration : 7299
train acc:  0.8828125
train loss:  0.31432783603668213
train gradient:  0.18858891534380628
iteration : 7300
train acc:  0.8359375
train loss:  0.3343704044818878
train gradient:  0.20995186240496266
iteration : 7301
train acc:  0.8359375
train loss:  0.33677294850349426
train gradient:  0.21948055721933024
iteration : 7302
train acc:  0.875
train loss:  0.2918630838394165
train gradient:  0.19155045410934357
iteration : 7303
train acc:  0.75
train loss:  0.4399421811103821
train gradient:  0.3293253835583456
iteration : 7304
train acc:  0.796875
train loss:  0.387294739484787
train gradient:  0.23103280183233169
iteration : 7305
train acc:  0.8984375
train loss:  0.29775863885879517
train gradient:  0.15754027988368943
iteration : 7306
train acc:  0.8515625
train loss:  0.35943078994750977
train gradient:  0.2985122363573852
iteration : 7307
train acc:  0.8515625
train loss:  0.28907692432403564
train gradient:  0.17748745184711068
iteration : 7308
train acc:  0.7890625
train loss:  0.4055712819099426
train gradient:  0.2628191254909551
iteration : 7309
train acc:  0.8203125
train loss:  0.38758033514022827
train gradient:  0.2903759445211108
iteration : 7310
train acc:  0.8671875
train loss:  0.3117581009864807
train gradient:  0.1852087159005706
iteration : 7311
train acc:  0.8203125
train loss:  0.3430555760860443
train gradient:  0.14662701628353725
iteration : 7312
train acc:  0.890625
train loss:  0.2774200141429901
train gradient:  0.12945704508897188
iteration : 7313
train acc:  0.8359375
train loss:  0.29989001154899597
train gradient:  0.13044661673152022
iteration : 7314
train acc:  0.8828125
train loss:  0.26458078622817993
train gradient:  0.12040507467196088
iteration : 7315
train acc:  0.8359375
train loss:  0.3731118440628052
train gradient:  0.2226820261987812
iteration : 7316
train acc:  0.8125
train loss:  0.35573911666870117
train gradient:  0.17955645890664107
iteration : 7317
train acc:  0.8671875
train loss:  0.3967084288597107
train gradient:  0.2313753997728309
iteration : 7318
train acc:  0.828125
train loss:  0.3208875060081482
train gradient:  0.18078459280388265
iteration : 7319
train acc:  0.875
train loss:  0.30467092990875244
train gradient:  0.14210058374210643
iteration : 7320
train acc:  0.9140625
train loss:  0.24725285172462463
train gradient:  0.11868661620329965
iteration : 7321
train acc:  0.8359375
train loss:  0.33599531650543213
train gradient:  0.2634655025298498
iteration : 7322
train acc:  0.859375
train loss:  0.36629605293273926
train gradient:  0.23361466192287184
iteration : 7323
train acc:  0.84375
train loss:  0.38058435916900635
train gradient:  0.22349782492704806
iteration : 7324
train acc:  0.84375
train loss:  0.3664814233779907
train gradient:  0.2558034617244954
iteration : 7325
train acc:  0.875
train loss:  0.3235444128513336
train gradient:  0.2081762504823446
iteration : 7326
train acc:  0.8828125
train loss:  0.32259219884872437
train gradient:  0.29259858313813697
iteration : 7327
train acc:  0.890625
train loss:  0.28731051087379456
train gradient:  0.12508076406655436
iteration : 7328
train acc:  0.8359375
train loss:  0.3400116264820099
train gradient:  0.18928008230823903
iteration : 7329
train acc:  0.8671875
train loss:  0.3329501748085022
train gradient:  0.16224894274749332
iteration : 7330
train acc:  0.84375
train loss:  0.35249143838882446
train gradient:  0.2546394805000326
iteration : 7331
train acc:  0.875
train loss:  0.35953596234321594
train gradient:  0.18127523159092396
iteration : 7332
train acc:  0.890625
train loss:  0.26475048065185547
train gradient:  0.11350139165534236
iteration : 7333
train acc:  0.8671875
train loss:  0.298634797334671
train gradient:  0.16855323225206587
iteration : 7334
train acc:  0.859375
train loss:  0.3140071630477905
train gradient:  0.22784182771969108
iteration : 7335
train acc:  0.8046875
train loss:  0.38137829303741455
train gradient:  0.19244401901822641
iteration : 7336
train acc:  0.875
train loss:  0.2740171551704407
train gradient:  0.12868100576550323
iteration : 7337
train acc:  0.8828125
train loss:  0.3036264479160309
train gradient:  0.12294287228452604
iteration : 7338
train acc:  0.859375
train loss:  0.29274874925613403
train gradient:  0.15078424170492571
iteration : 7339
train acc:  0.8125
train loss:  0.37910687923431396
train gradient:  0.24058881788808092
iteration : 7340
train acc:  0.8125
train loss:  0.42480960488319397
train gradient:  0.3064884692892295
iteration : 7341
train acc:  0.8828125
train loss:  0.2770746648311615
train gradient:  0.20675925488486327
iteration : 7342
train acc:  0.859375
train loss:  0.3589676320552826
train gradient:  0.23557803219353218
iteration : 7343
train acc:  0.8515625
train loss:  0.3047582507133484
train gradient:  0.15377728387577289
iteration : 7344
train acc:  0.875
train loss:  0.2649519443511963
train gradient:  0.14761969138839032
iteration : 7345
train acc:  0.859375
train loss:  0.3304009437561035
train gradient:  0.1904638706727442
iteration : 7346
train acc:  0.890625
train loss:  0.27296990156173706
train gradient:  0.16943689110304205
iteration : 7347
train acc:  0.796875
train loss:  0.4361056089401245
train gradient:  0.23866951408743958
iteration : 7348
train acc:  0.921875
train loss:  0.2599579989910126
train gradient:  0.14034947056946345
iteration : 7349
train acc:  0.8515625
train loss:  0.3836049437522888
train gradient:  0.25162092965608523
iteration : 7350
train acc:  0.8203125
train loss:  0.4075978398323059
train gradient:  0.2860948752676401
iteration : 7351
train acc:  0.859375
train loss:  0.3031611740589142
train gradient:  0.22263829440601035
iteration : 7352
train acc:  0.859375
train loss:  0.32370680570602417
train gradient:  0.25149363975632494
iteration : 7353
train acc:  0.859375
train loss:  0.37501585483551025
train gradient:  0.29744288916025907
iteration : 7354
train acc:  0.84375
train loss:  0.3553016185760498
train gradient:  0.24153270911536284
iteration : 7355
train acc:  0.890625
train loss:  0.26575177907943726
train gradient:  0.1388175522212628
iteration : 7356
train acc:  0.828125
train loss:  0.3348318040370941
train gradient:  0.19199530389007335
iteration : 7357
train acc:  0.7890625
train loss:  0.4240221381187439
train gradient:  0.25423336319839346
iteration : 7358
train acc:  0.8125
train loss:  0.41505929827690125
train gradient:  0.30999587969121234
iteration : 7359
train acc:  0.7890625
train loss:  0.3649347424507141
train gradient:  0.2073808881361872
iteration : 7360
train acc:  0.8359375
train loss:  0.42709869146347046
train gradient:  0.2722187233190857
iteration : 7361
train acc:  0.8359375
train loss:  0.3122522830963135
train gradient:  0.13930992627059202
iteration : 7362
train acc:  0.8828125
train loss:  0.2827190160751343
train gradient:  0.13260394782237436
iteration : 7363
train acc:  0.8671875
train loss:  0.32513728737831116
train gradient:  0.2085092445260106
iteration : 7364
train acc:  0.828125
train loss:  0.3810821771621704
train gradient:  0.30952919954092656
iteration : 7365
train acc:  0.8203125
train loss:  0.345863938331604
train gradient:  0.2054521553425022
iteration : 7366
train acc:  0.8203125
train loss:  0.39625051617622375
train gradient:  0.3270675328886221
iteration : 7367
train acc:  0.8125
train loss:  0.3505992293357849
train gradient:  0.15749232137786462
iteration : 7368
train acc:  0.8359375
train loss:  0.3509589731693268
train gradient:  0.24059070442138852
iteration : 7369
train acc:  0.8984375
train loss:  0.28917691111564636
train gradient:  0.15307450681551615
iteration : 7370
train acc:  0.8671875
train loss:  0.3237341046333313
train gradient:  0.1376535786104345
iteration : 7371
train acc:  0.8046875
train loss:  0.3963010907173157
train gradient:  0.30272839499643656
iteration : 7372
train acc:  0.796875
train loss:  0.43460094928741455
train gradient:  0.40825627172849455
iteration : 7373
train acc:  0.84375
train loss:  0.34826457500457764
train gradient:  0.2088881155837548
iteration : 7374
train acc:  0.859375
train loss:  0.3229283392429352
train gradient:  0.29314782152689944
iteration : 7375
train acc:  0.828125
train loss:  0.3411983251571655
train gradient:  0.24771532166579874
iteration : 7376
train acc:  0.8203125
train loss:  0.34659919142723083
train gradient:  0.2318211210777254
iteration : 7377
train acc:  0.796875
train loss:  0.3826606869697571
train gradient:  0.24372795753376117
iteration : 7378
train acc:  0.8046875
train loss:  0.3746833801269531
train gradient:  0.19998470484942882
iteration : 7379
train acc:  0.8828125
train loss:  0.33294206857681274
train gradient:  0.20222111948634924
iteration : 7380
train acc:  0.84375
train loss:  0.3489627242088318
train gradient:  0.17652345213022347
iteration : 7381
train acc:  0.8046875
train loss:  0.34159785509109497
train gradient:  0.2326482991756968
iteration : 7382
train acc:  0.828125
train loss:  0.384448379278183
train gradient:  0.2264517912894753
iteration : 7383
train acc:  0.8359375
train loss:  0.3858765959739685
train gradient:  0.3903392465289281
iteration : 7384
train acc:  0.84375
train loss:  0.36087435483932495
train gradient:  0.21220486860455118
iteration : 7385
train acc:  0.859375
train loss:  0.2959493100643158
train gradient:  0.13722293510793032
iteration : 7386
train acc:  0.8515625
train loss:  0.32658323645591736
train gradient:  0.13794736872665303
iteration : 7387
train acc:  0.8203125
train loss:  0.4067496657371521
train gradient:  0.24112818478224074
iteration : 7388
train acc:  0.890625
train loss:  0.3260347247123718
train gradient:  0.15757406884623304
iteration : 7389
train acc:  0.859375
train loss:  0.3239907920360565
train gradient:  0.2248533458221157
iteration : 7390
train acc:  0.8828125
train loss:  0.29483214020729065
train gradient:  0.13372709830340943
iteration : 7391
train acc:  0.859375
train loss:  0.3791467547416687
train gradient:  0.20333517178217497
iteration : 7392
train acc:  0.828125
train loss:  0.3805760145187378
train gradient:  0.23601979158566808
iteration : 7393
train acc:  0.8671875
train loss:  0.3155168890953064
train gradient:  0.1400504572146012
iteration : 7394
train acc:  0.828125
train loss:  0.35612285137176514
train gradient:  0.22272172198917684
iteration : 7395
train acc:  0.84375
train loss:  0.36473023891448975
train gradient:  0.17007900663313397
iteration : 7396
train acc:  0.8671875
train loss:  0.2993326187133789
train gradient:  0.12264272568773531
iteration : 7397
train acc:  0.8828125
train loss:  0.30034294724464417
train gradient:  0.1424966499025043
iteration : 7398
train acc:  0.8125
train loss:  0.40618371963500977
train gradient:  0.23867822024102353
iteration : 7399
train acc:  0.8515625
train loss:  0.34847521781921387
train gradient:  0.1526790456311516
iteration : 7400
train acc:  0.859375
train loss:  0.3430297076702118
train gradient:  0.221395958382443
iteration : 7401
train acc:  0.859375
train loss:  0.3423261344432831
train gradient:  0.1886716308720081
iteration : 7402
train acc:  0.84375
train loss:  0.3328603506088257
train gradient:  0.1705346115003964
iteration : 7403
train acc:  0.8671875
train loss:  0.2961375117301941
train gradient:  0.1277131936983153
iteration : 7404
train acc:  0.84375
train loss:  0.31814253330230713
train gradient:  0.22083425934310963
iteration : 7405
train acc:  0.90625
train loss:  0.30261528491973877
train gradient:  0.15225913413911268
iteration : 7406
train acc:  0.875
train loss:  0.29243552684783936
train gradient:  0.14098031470808792
iteration : 7407
train acc:  0.890625
train loss:  0.31439971923828125
train gradient:  0.12373028935037907
iteration : 7408
train acc:  0.859375
train loss:  0.33641570806503296
train gradient:  0.17884603541470695
iteration : 7409
train acc:  0.859375
train loss:  0.2948092222213745
train gradient:  0.11692364061333105
iteration : 7410
train acc:  0.8125
train loss:  0.42780399322509766
train gradient:  0.31500306682674606
iteration : 7411
train acc:  0.84375
train loss:  0.4052134156227112
train gradient:  0.24560110370982124
iteration : 7412
train acc:  0.8671875
train loss:  0.33669513463974
train gradient:  0.17118669293829392
iteration : 7413
train acc:  0.84375
train loss:  0.37117040157318115
train gradient:  0.19709730095022993
iteration : 7414
train acc:  0.875
train loss:  0.34494295716285706
train gradient:  0.18202837436373892
iteration : 7415
train acc:  0.828125
train loss:  0.3518678545951843
train gradient:  0.15739502671997868
iteration : 7416
train acc:  0.859375
train loss:  0.34936702251434326
train gradient:  0.1682829165265053
iteration : 7417
train acc:  0.8359375
train loss:  0.3825967013835907
train gradient:  0.18137631557489406
iteration : 7418
train acc:  0.8203125
train loss:  0.34617650508880615
train gradient:  0.16109639534334003
iteration : 7419
train acc:  0.859375
train loss:  0.3234267830848694
train gradient:  0.24778068228577943
iteration : 7420
train acc:  0.8203125
train loss:  0.3697620928287506
train gradient:  0.2733538184864687
iteration : 7421
train acc:  0.8828125
train loss:  0.29258355498313904
train gradient:  0.15208704170084575
iteration : 7422
train acc:  0.859375
train loss:  0.35261306166648865
train gradient:  0.1655580389874624
iteration : 7423
train acc:  0.828125
train loss:  0.3787534832954407
train gradient:  0.14290692617025436
iteration : 7424
train acc:  0.8515625
train loss:  0.34396618604660034
train gradient:  0.25835551013506325
iteration : 7425
train acc:  0.875
train loss:  0.29471462965011597
train gradient:  0.1748984574944118
iteration : 7426
train acc:  0.859375
train loss:  0.3570401072502136
train gradient:  0.1839960807458856
iteration : 7427
train acc:  0.8125
train loss:  0.3421032428741455
train gradient:  0.2422235089923816
iteration : 7428
train acc:  0.8984375
train loss:  0.3567538261413574
train gradient:  0.23109315086858395
iteration : 7429
train acc:  0.8203125
train loss:  0.43823981285095215
train gradient:  0.4967554179803709
iteration : 7430
train acc:  0.8359375
train loss:  0.3078087568283081
train gradient:  0.1947258412568231
iteration : 7431
train acc:  0.875
train loss:  0.3145623803138733
train gradient:  0.1349294262870655
iteration : 7432
train acc:  0.8515625
train loss:  0.3225714862346649
train gradient:  0.16740957903692916
iteration : 7433
train acc:  0.8515625
train loss:  0.33594489097595215
train gradient:  0.3548181918102636
iteration : 7434
train acc:  0.8125
train loss:  0.32486969232559204
train gradient:  0.1644320069151251
iteration : 7435
train acc:  0.8359375
train loss:  0.34678539633750916
train gradient:  0.15748534142555354
iteration : 7436
train acc:  0.859375
train loss:  0.3132728040218353
train gradient:  0.19238161349737898
iteration : 7437
train acc:  0.8984375
train loss:  0.27141427993774414
train gradient:  0.11993556272697646
iteration : 7438
train acc:  0.8203125
train loss:  0.347995400428772
train gradient:  0.14735136972125631
iteration : 7439
train acc:  0.8828125
train loss:  0.31303030252456665
train gradient:  0.1479175887350541
iteration : 7440
train acc:  0.78125
train loss:  0.4211116433143616
train gradient:  0.31177058420695036
iteration : 7441
train acc:  0.828125
train loss:  0.3938632011413574
train gradient:  0.4268220975098696
iteration : 7442
train acc:  0.8671875
train loss:  0.35234278440475464
train gradient:  0.2203384391567492
iteration : 7443
train acc:  0.859375
train loss:  0.3423066735267639
train gradient:  0.22559860704831633
iteration : 7444
train acc:  0.8125
train loss:  0.4434822201728821
train gradient:  0.3007176545158908
iteration : 7445
train acc:  0.8515625
train loss:  0.35313156247138977
train gradient:  0.15772623079992829
iteration : 7446
train acc:  0.8671875
train loss:  0.3222688138484955
train gradient:  0.19692583173998288
iteration : 7447
train acc:  0.8671875
train loss:  0.29880136251449585
train gradient:  0.14462252531167313
iteration : 7448
train acc:  0.8515625
train loss:  0.33519744873046875
train gradient:  0.20648719574332258
iteration : 7449
train acc:  0.8828125
train loss:  0.32865628600120544
train gradient:  0.21128955346498562
iteration : 7450
train acc:  0.8203125
train loss:  0.45490145683288574
train gradient:  0.3649762572402255
iteration : 7451
train acc:  0.8671875
train loss:  0.3162124752998352
train gradient:  0.20121976480598813
iteration : 7452
train acc:  0.796875
train loss:  0.3736422657966614
train gradient:  0.21943843713223876
iteration : 7453
train acc:  0.8828125
train loss:  0.3849082291126251
train gradient:  0.20726780337008704
iteration : 7454
train acc:  0.84375
train loss:  0.29527556896209717
train gradient:  0.2112117200014705
iteration : 7455
train acc:  0.8515625
train loss:  0.3842260241508484
train gradient:  0.19500138598465455
iteration : 7456
train acc:  0.875
train loss:  0.3258238136768341
train gradient:  0.14410024339501007
iteration : 7457
train acc:  0.8359375
train loss:  0.37116941809654236
train gradient:  0.16214494373522076
iteration : 7458
train acc:  0.859375
train loss:  0.30946022272109985
train gradient:  0.25570803597128167
iteration : 7459
train acc:  0.8203125
train loss:  0.3709997534751892
train gradient:  0.26326290540158565
iteration : 7460
train acc:  0.8828125
train loss:  0.25572407245635986
train gradient:  0.11252920109644576
iteration : 7461
train acc:  0.8828125
train loss:  0.2846653163433075
train gradient:  0.1706493977993724
iteration : 7462
train acc:  0.796875
train loss:  0.42374151945114136
train gradient:  0.24891644618579872
iteration : 7463
train acc:  0.84375
train loss:  0.3752472400665283
train gradient:  0.23503331650151543
iteration : 7464
train acc:  0.8046875
train loss:  0.3758623003959656
train gradient:  0.20073553349639967
iteration : 7465
train acc:  0.875
train loss:  0.30067986249923706
train gradient:  0.19624154590776022
iteration : 7466
train acc:  0.8359375
train loss:  0.37197184562683105
train gradient:  0.1624054846054308
iteration : 7467
train acc:  0.8046875
train loss:  0.3890872597694397
train gradient:  0.19870297640796075
iteration : 7468
train acc:  0.875
train loss:  0.343328058719635
train gradient:  0.18300926367062237
iteration : 7469
train acc:  0.8671875
train loss:  0.2964808940887451
train gradient:  0.12023037724109135
iteration : 7470
train acc:  0.8359375
train loss:  0.33149924874305725
train gradient:  0.16639975783252364
iteration : 7471
train acc:  0.796875
train loss:  0.41102486848831177
train gradient:  0.2534447227699116
iteration : 7472
train acc:  0.8125
train loss:  0.3965902328491211
train gradient:  0.3488088578528191
iteration : 7473
train acc:  0.859375
train loss:  0.32239067554473877
train gradient:  0.18608255040108945
iteration : 7474
train acc:  0.859375
train loss:  0.3354037404060364
train gradient:  0.24396300485700842
iteration : 7475
train acc:  0.8671875
train loss:  0.31950074434280396
train gradient:  0.1499107873976377
iteration : 7476
train acc:  0.8359375
train loss:  0.31168344616889954
train gradient:  0.3195079416533793
iteration : 7477
train acc:  0.828125
train loss:  0.38575834035873413
train gradient:  0.18126025990194777
iteration : 7478
train acc:  0.8515625
train loss:  0.39616304636001587
train gradient:  0.17297726752948905
iteration : 7479
train acc:  0.84375
train loss:  0.40479665994644165
train gradient:  0.24417605960558406
iteration : 7480
train acc:  0.8984375
train loss:  0.2568477988243103
train gradient:  0.1567516911532284
iteration : 7481
train acc:  0.8984375
train loss:  0.272000789642334
train gradient:  0.12662848034575094
iteration : 7482
train acc:  0.8671875
train loss:  0.33374014496803284
train gradient:  0.1422251175615986
iteration : 7483
train acc:  0.875
train loss:  0.31629276275634766
train gradient:  0.14409779199446038
iteration : 7484
train acc:  0.8671875
train loss:  0.3158611059188843
train gradient:  0.12366926857303785
iteration : 7485
train acc:  0.9140625
train loss:  0.2718672454357147
train gradient:  0.18944981722255053
iteration : 7486
train acc:  0.859375
train loss:  0.3020843267440796
train gradient:  0.1268098367031371
iteration : 7487
train acc:  0.90625
train loss:  0.30080604553222656
train gradient:  0.1237868620294701
iteration : 7488
train acc:  0.8671875
train loss:  0.29443618655204773
train gradient:  0.11504560137113216
iteration : 7489
train acc:  0.84375
train loss:  0.3460582494735718
train gradient:  0.32688155594641716
iteration : 7490
train acc:  0.8828125
train loss:  0.34984421730041504
train gradient:  0.15122182243254867
iteration : 7491
train acc:  0.8046875
train loss:  0.3768880367279053
train gradient:  0.23933500303243266
iteration : 7492
train acc:  0.828125
train loss:  0.35390931367874146
train gradient:  0.16831478666016061
iteration : 7493
train acc:  0.890625
train loss:  0.30161309242248535
train gradient:  0.1894281736281335
iteration : 7494
train acc:  0.8828125
train loss:  0.3742995262145996
train gradient:  0.2377467178857705
iteration : 7495
train acc:  0.75
train loss:  0.4703309237957001
train gradient:  0.29848820543893906
iteration : 7496
train acc:  0.8046875
train loss:  0.36424946784973145
train gradient:  0.2230926094909583
iteration : 7497
train acc:  0.828125
train loss:  0.4227563440799713
train gradient:  0.23563156638001484
iteration : 7498
train acc:  0.8125
train loss:  0.40853506326675415
train gradient:  0.3257888518660636
iteration : 7499
train acc:  0.8046875
train loss:  0.38478201627731323
train gradient:  0.26966411603041995
iteration : 7500
train acc:  0.8828125
train loss:  0.28172773122787476
train gradient:  0.11633372940625257
iteration : 7501
train acc:  0.8671875
train loss:  0.28261274099349976
train gradient:  0.12208212811628016
iteration : 7502
train acc:  0.875
train loss:  0.29645901918411255
train gradient:  0.13035052535750896
iteration : 7503
train acc:  0.9140625
train loss:  0.22447791695594788
train gradient:  0.09878670065532968
iteration : 7504
train acc:  0.8515625
train loss:  0.3704643249511719
train gradient:  0.2321507084584591
iteration : 7505
train acc:  0.84375
train loss:  0.36359038949012756
train gradient:  0.3374114370423258
iteration : 7506
train acc:  0.859375
train loss:  0.28718820214271545
train gradient:  0.17937555651036552
iteration : 7507
train acc:  0.8984375
train loss:  0.2645682692527771
train gradient:  0.11288983155651178
iteration : 7508
train acc:  0.890625
train loss:  0.28893935680389404
train gradient:  0.12235299226811139
iteration : 7509
train acc:  0.84375
train loss:  0.343436062335968
train gradient:  0.21154661822202125
iteration : 7510
train acc:  0.828125
train loss:  0.319027841091156
train gradient:  0.25150236668888815
iteration : 7511
train acc:  0.796875
train loss:  0.43159472942352295
train gradient:  0.27427151566309116
iteration : 7512
train acc:  0.8203125
train loss:  0.3856807351112366
train gradient:  0.2463555494741387
iteration : 7513
train acc:  0.8359375
train loss:  0.3158607482910156
train gradient:  0.16011139668705762
iteration : 7514
train acc:  0.859375
train loss:  0.27079278230667114
train gradient:  0.14528324873871495
iteration : 7515
train acc:  0.875
train loss:  0.2406623363494873
train gradient:  0.12465079982899094
iteration : 7516
train acc:  0.8671875
train loss:  0.28020212054252625
train gradient:  0.17017391782358285
iteration : 7517
train acc:  0.84375
train loss:  0.32495665550231934
train gradient:  0.12519273281086307
iteration : 7518
train acc:  0.84375
train loss:  0.35736000537872314
train gradient:  0.19091127425264465
iteration : 7519
train acc:  0.8671875
train loss:  0.296688437461853
train gradient:  0.14940591815089993
iteration : 7520
train acc:  0.8671875
train loss:  0.32950642704963684
train gradient:  0.16976947432347333
iteration : 7521
train acc:  0.859375
train loss:  0.38615846633911133
train gradient:  0.24530865095802562
iteration : 7522
train acc:  0.828125
train loss:  0.3687933683395386
train gradient:  0.25400828528424857
iteration : 7523
train acc:  0.8671875
train loss:  0.3272416889667511
train gradient:  0.15446707643443197
iteration : 7524
train acc:  0.8671875
train loss:  0.354573130607605
train gradient:  0.25018927731987006
iteration : 7525
train acc:  0.8515625
train loss:  0.33499422669410706
train gradient:  0.15967247501080084
iteration : 7526
train acc:  0.859375
train loss:  0.32087433338165283
train gradient:  0.19607622125910967
iteration : 7527
train acc:  0.859375
train loss:  0.29306530952453613
train gradient:  0.18609314467013374
iteration : 7528
train acc:  0.859375
train loss:  0.3444114923477173
train gradient:  0.18260007618656
iteration : 7529
train acc:  0.8671875
train loss:  0.3264598250389099
train gradient:  0.21677970659864562
iteration : 7530
train acc:  0.875
train loss:  0.2983173727989197
train gradient:  0.20389755415391403
iteration : 7531
train acc:  0.90625
train loss:  0.2835254669189453
train gradient:  0.14219036168502885
iteration : 7532
train acc:  0.828125
train loss:  0.3692755103111267
train gradient:  0.2073207172348095
iteration : 7533
train acc:  0.8515625
train loss:  0.3657888174057007
train gradient:  0.14258395370396204
iteration : 7534
train acc:  0.8515625
train loss:  0.3179272413253784
train gradient:  0.23035936976921365
iteration : 7535
train acc:  0.8046875
train loss:  0.47103768587112427
train gradient:  0.3570282904734455
iteration : 7536
train acc:  0.7734375
train loss:  0.4940921664237976
train gradient:  0.3569125276900361
iteration : 7537
train acc:  0.859375
train loss:  0.28674137592315674
train gradient:  0.16284370441206536
iteration : 7538
train acc:  0.8203125
train loss:  0.3437731862068176
train gradient:  0.14862574889137375
iteration : 7539
train acc:  0.8984375
train loss:  0.28331631422042847
train gradient:  0.1858026286543759
iteration : 7540
train acc:  0.8828125
train loss:  0.3140447735786438
train gradient:  0.18415708912117953
iteration : 7541
train acc:  0.8203125
train loss:  0.3610001802444458
train gradient:  0.23686774300124447
iteration : 7542
train acc:  0.8125
train loss:  0.33587977290153503
train gradient:  0.14666036581433883
iteration : 7543
train acc:  0.890625
train loss:  0.3442889451980591
train gradient:  0.14669775933266524
iteration : 7544
train acc:  0.7890625
train loss:  0.46802791953086853
train gradient:  0.4333619797740497
iteration : 7545
train acc:  0.8125
train loss:  0.4192918837070465
train gradient:  0.315593619864149
iteration : 7546
train acc:  0.8203125
train loss:  0.3750160336494446
train gradient:  0.2972376110570022
iteration : 7547
train acc:  0.8671875
train loss:  0.30956223607063293
train gradient:  0.15130464371037777
iteration : 7548
train acc:  0.8203125
train loss:  0.35575586557388306
train gradient:  0.1982776821673743
iteration : 7549
train acc:  0.8359375
train loss:  0.3884996175765991
train gradient:  0.22044815235817009
iteration : 7550
train acc:  0.90625
train loss:  0.2746245265007019
train gradient:  0.14767032959017934
iteration : 7551
train acc:  0.8515625
train loss:  0.3012372851371765
train gradient:  0.14659384672021047
iteration : 7552
train acc:  0.8125
train loss:  0.37217003107070923
train gradient:  0.31740620477137277
iteration : 7553
train acc:  0.8125
train loss:  0.39587530493736267
train gradient:  0.29784040191699784
iteration : 7554
train acc:  0.875
train loss:  0.33775126934051514
train gradient:  0.17624279667518603
iteration : 7555
train acc:  0.84375
train loss:  0.3381742537021637
train gradient:  0.1771964270669354
iteration : 7556
train acc:  0.796875
train loss:  0.4032136797904968
train gradient:  0.2985757223246945
iteration : 7557
train acc:  0.8125
train loss:  0.35856562852859497
train gradient:  0.21252749347473815
iteration : 7558
train acc:  0.8515625
train loss:  0.3910461664199829
train gradient:  0.35065291302077434
iteration : 7559
train acc:  0.8203125
train loss:  0.35569944977760315
train gradient:  0.22039080277900364
iteration : 7560
train acc:  0.8515625
train loss:  0.37946149706840515
train gradient:  0.23717659013452694
iteration : 7561
train acc:  0.8671875
train loss:  0.3609403371810913
train gradient:  0.17086469222955103
iteration : 7562
train acc:  0.8203125
train loss:  0.33439821004867554
train gradient:  0.18483841282349128
iteration : 7563
train acc:  0.8671875
train loss:  0.3084530234336853
train gradient:  0.19310145637624418
iteration : 7564
train acc:  0.84375
train loss:  0.3453378975391388
train gradient:  0.2191780681271887
iteration : 7565
train acc:  0.8046875
train loss:  0.4167276620864868
train gradient:  0.2885636622890018
iteration : 7566
train acc:  0.78125
train loss:  0.4176141619682312
train gradient:  0.2096032968338838
iteration : 7567
train acc:  0.8125
train loss:  0.37874817848205566
train gradient:  0.20248514119879307
iteration : 7568
train acc:  0.84375
train loss:  0.29024770855903625
train gradient:  0.17091648254401284
iteration : 7569
train acc:  0.828125
train loss:  0.3744208812713623
train gradient:  0.17206846234978473
iteration : 7570
train acc:  0.8203125
train loss:  0.3415204882621765
train gradient:  0.24743120667817314
iteration : 7571
train acc:  0.859375
train loss:  0.3218674063682556
train gradient:  0.14947325090576014
iteration : 7572
train acc:  0.8359375
train loss:  0.3470069169998169
train gradient:  0.23486972576650178
iteration : 7573
train acc:  0.7890625
train loss:  0.4551461935043335
train gradient:  0.4357536852910661
iteration : 7574
train acc:  0.8515625
train loss:  0.339720755815506
train gradient:  0.22410523011436112
iteration : 7575
train acc:  0.8828125
train loss:  0.2731253206729889
train gradient:  0.10716224059916023
iteration : 7576
train acc:  0.8828125
train loss:  0.34483569860458374
train gradient:  0.1992040301971032
iteration : 7577
train acc:  0.7578125
train loss:  0.4374356269836426
train gradient:  0.24613257009630907
iteration : 7578
train acc:  0.859375
train loss:  0.3394418954849243
train gradient:  0.11466915702060765
iteration : 7579
train acc:  0.859375
train loss:  0.31573837995529175
train gradient:  0.15682986070635158
iteration : 7580
train acc:  0.8515625
train loss:  0.30806607007980347
train gradient:  0.14751345933101773
iteration : 7581
train acc:  0.828125
train loss:  0.380728542804718
train gradient:  0.21259728883948814
iteration : 7582
train acc:  0.8515625
train loss:  0.38482505083084106
train gradient:  0.24777540934508324
iteration : 7583
train acc:  0.8203125
train loss:  0.37864094972610474
train gradient:  0.2502904465261743
iteration : 7584
train acc:  0.8515625
train loss:  0.3393296003341675
train gradient:  0.15234433156340432
iteration : 7585
train acc:  0.796875
train loss:  0.3944050669670105
train gradient:  0.1658140383309355
iteration : 7586
train acc:  0.90625
train loss:  0.2819535732269287
train gradient:  0.16905891006525495
iteration : 7587
train acc:  0.8359375
train loss:  0.31223151087760925
train gradient:  0.16197959536935533
iteration : 7588
train acc:  0.84375
train loss:  0.3492647409439087
train gradient:  0.1979524725677999
iteration : 7589
train acc:  0.828125
train loss:  0.38514506816864014
train gradient:  0.16814818835358322
iteration : 7590
train acc:  0.8125
train loss:  0.4342648983001709
train gradient:  0.5117589370302686
iteration : 7591
train acc:  0.84375
train loss:  0.37388187646865845
train gradient:  0.1949989596204914
iteration : 7592
train acc:  0.8125
train loss:  0.3760911822319031
train gradient:  0.20016541834511148
iteration : 7593
train acc:  0.8671875
train loss:  0.2746397852897644
train gradient:  0.15541916918530452
iteration : 7594
train acc:  0.8984375
train loss:  0.27847176790237427
train gradient:  0.09043597655256298
iteration : 7595
train acc:  0.8828125
train loss:  0.3129391074180603
train gradient:  0.17276818453747175
iteration : 7596
train acc:  0.8203125
train loss:  0.3555641770362854
train gradient:  0.1838054695356539
iteration : 7597
train acc:  0.8515625
train loss:  0.35801857709884644
train gradient:  0.19421685449368925
iteration : 7598
train acc:  0.8671875
train loss:  0.3130785822868347
train gradient:  0.12012056957405505
iteration : 7599
train acc:  0.828125
train loss:  0.3428824543952942
train gradient:  0.17978079136821068
iteration : 7600
train acc:  0.8671875
train loss:  0.31534528732299805
train gradient:  0.1337708217193953
iteration : 7601
train acc:  0.8203125
train loss:  0.33225464820861816
train gradient:  0.18855171039523466
iteration : 7602
train acc:  0.828125
train loss:  0.3627859950065613
train gradient:  0.2876577658496438
iteration : 7603
train acc:  0.84375
train loss:  0.33898794651031494
train gradient:  0.15429255541202963
iteration : 7604
train acc:  0.859375
train loss:  0.33374547958374023
train gradient:  0.19855064831986857
iteration : 7605
train acc:  0.859375
train loss:  0.31428050994873047
train gradient:  0.16567156602758618
iteration : 7606
train acc:  0.859375
train loss:  0.3060425817966461
train gradient:  0.22792043171347248
iteration : 7607
train acc:  0.875
train loss:  0.3281123638153076
train gradient:  0.20694824040663612
iteration : 7608
train acc:  0.8671875
train loss:  0.30713146924972534
train gradient:  0.15199318952993252
iteration : 7609
train acc:  0.8515625
train loss:  0.42656421661376953
train gradient:  0.23065750974618504
iteration : 7610
train acc:  0.859375
train loss:  0.3283020257949829
train gradient:  0.15340336012018968
iteration : 7611
train acc:  0.90625
train loss:  0.2924496829509735
train gradient:  0.24994579777164444
iteration : 7612
train acc:  0.8203125
train loss:  0.36197659373283386
train gradient:  0.21069547451145618
iteration : 7613
train acc:  0.8359375
train loss:  0.3721834421157837
train gradient:  0.29664437729669707
iteration : 7614
train acc:  0.8515625
train loss:  0.2800692319869995
train gradient:  0.15736741866243525
iteration : 7615
train acc:  0.8828125
train loss:  0.27427443861961365
train gradient:  0.14440226971667575
iteration : 7616
train acc:  0.90625
train loss:  0.28407996892929077
train gradient:  0.2968951100572564
iteration : 7617
train acc:  0.859375
train loss:  0.37085410952568054
train gradient:  0.2719286873740089
iteration : 7618
train acc:  0.8515625
train loss:  0.27747270464897156
train gradient:  0.1458096303054829
iteration : 7619
train acc:  0.8671875
train loss:  0.29858464002609253
train gradient:  0.14757126899397285
iteration : 7620
train acc:  0.8515625
train loss:  0.32354113459587097
train gradient:  0.20949734966192685
iteration : 7621
train acc:  0.8671875
train loss:  0.2884387969970703
train gradient:  0.14586360663729797
iteration : 7622
train acc:  0.859375
train loss:  0.3014184832572937
train gradient:  0.12499676332966811
iteration : 7623
train acc:  0.8828125
train loss:  0.30367550253868103
train gradient:  0.16877998119385204
iteration : 7624
train acc:  0.8828125
train loss:  0.29864874482154846
train gradient:  0.15755371316553263
iteration : 7625
train acc:  0.875
train loss:  0.33073753118515015
train gradient:  0.15403421499695097
iteration : 7626
train acc:  0.859375
train loss:  0.3036884367465973
train gradient:  0.18402199296629518
iteration : 7627
train acc:  0.84375
train loss:  0.3561590313911438
train gradient:  0.16229705348682794
iteration : 7628
train acc:  0.8359375
train loss:  0.340322345495224
train gradient:  0.2027247905151664
iteration : 7629
train acc:  0.890625
train loss:  0.2722444236278534
train gradient:  0.17531347150719762
iteration : 7630
train acc:  0.8203125
train loss:  0.3881804347038269
train gradient:  0.2707498237645663
iteration : 7631
train acc:  0.8046875
train loss:  0.3794898986816406
train gradient:  0.257847365136851
iteration : 7632
train acc:  0.8125
train loss:  0.30807510018348694
train gradient:  0.14605123937163744
iteration : 7633
train acc:  0.8203125
train loss:  0.346622109413147
train gradient:  0.20808659867102028
iteration : 7634
train acc:  0.84375
train loss:  0.389060378074646
train gradient:  0.2367710299735017
iteration : 7635
train acc:  0.875
train loss:  0.3542555272579193
train gradient:  0.2509843672819137
iteration : 7636
train acc:  0.859375
train loss:  0.3375934660434723
train gradient:  0.37704751290323024
iteration : 7637
train acc:  0.8984375
train loss:  0.2725223898887634
train gradient:  0.22461206245969126
iteration : 7638
train acc:  0.859375
train loss:  0.2832104563713074
train gradient:  0.12157208299444872
iteration : 7639
train acc:  0.8203125
train loss:  0.3585401773452759
train gradient:  0.2759587551494653
iteration : 7640
train acc:  0.828125
train loss:  0.3453488051891327
train gradient:  0.2872207313413036
iteration : 7641
train acc:  0.8203125
train loss:  0.35291942954063416
train gradient:  0.21565008177012027
iteration : 7642
train acc:  0.8671875
train loss:  0.32376718521118164
train gradient:  0.16835936965523907
iteration : 7643
train acc:  0.8359375
train loss:  0.3059738874435425
train gradient:  0.14733631382462542
iteration : 7644
train acc:  0.8671875
train loss:  0.3370653986930847
train gradient:  0.15076928537302556
iteration : 7645
train acc:  0.8671875
train loss:  0.342809796333313
train gradient:  0.20554987472209285
iteration : 7646
train acc:  0.890625
train loss:  0.2989192008972168
train gradient:  0.13649907961057012
iteration : 7647
train acc:  0.875
train loss:  0.30841201543807983
train gradient:  0.23370488866611525
iteration : 7648
train acc:  0.8359375
train loss:  0.38994118571281433
train gradient:  0.3624721789225477
iteration : 7649
train acc:  0.8203125
train loss:  0.35120874643325806
train gradient:  0.19764379700940715
iteration : 7650
train acc:  0.8671875
train loss:  0.2874373197555542
train gradient:  0.27466231951273473
iteration : 7651
train acc:  0.8984375
train loss:  0.2602922320365906
train gradient:  0.11959893163003488
iteration : 7652
train acc:  0.8671875
train loss:  0.32518240809440613
train gradient:  0.17110542820520508
iteration : 7653
train acc:  0.859375
train loss:  0.3213472068309784
train gradient:  0.1490725970309147
iteration : 7654
train acc:  0.8515625
train loss:  0.2967231571674347
train gradient:  0.18558883976993176
iteration : 7655
train acc:  0.828125
train loss:  0.3701249957084656
train gradient:  0.21572867362321863
iteration : 7656
train acc:  0.8671875
train loss:  0.2849409580230713
train gradient:  0.12286045046084638
iteration : 7657
train acc:  0.8359375
train loss:  0.33472591638565063
train gradient:  0.17194871005223766
iteration : 7658
train acc:  0.84375
train loss:  0.3703184127807617
train gradient:  0.23258709444941272
iteration : 7659
train acc:  0.8125
train loss:  0.3715600371360779
train gradient:  0.23583825673596878
iteration : 7660
train acc:  0.859375
train loss:  0.2929050922393799
train gradient:  0.22509410373196462
iteration : 7661
train acc:  0.8359375
train loss:  0.3670106530189514
train gradient:  0.22459391328728553
iteration : 7662
train acc:  0.8984375
train loss:  0.24622227251529694
train gradient:  0.14380752051889212
iteration : 7663
train acc:  0.875
train loss:  0.32992023229599
train gradient:  0.28108833883000084
iteration : 7664
train acc:  0.828125
train loss:  0.3180256187915802
train gradient:  0.201665764013843
iteration : 7665
train acc:  0.8515625
train loss:  0.3345317840576172
train gradient:  0.17660737905039525
iteration : 7666
train acc:  0.8515625
train loss:  0.3856842815876007
train gradient:  0.24722788526255868
iteration : 7667
train acc:  0.8828125
train loss:  0.28166601061820984
train gradient:  0.12306492992052995
iteration : 7668
train acc:  0.8515625
train loss:  0.35711801052093506
train gradient:  0.24165350249962164
iteration : 7669
train acc:  0.828125
train loss:  0.37935322523117065
train gradient:  0.21332815444821507
iteration : 7670
train acc:  0.84375
train loss:  0.40241241455078125
train gradient:  0.23679680777396217
iteration : 7671
train acc:  0.859375
train loss:  0.2980553209781647
train gradient:  0.29038280148893597
iteration : 7672
train acc:  0.8125
train loss:  0.3659206032752991
train gradient:  0.30833221077295087
iteration : 7673
train acc:  0.8515625
train loss:  0.35646218061447144
train gradient:  0.26941250683244256
iteration : 7674
train acc:  0.890625
train loss:  0.2616880536079407
train gradient:  0.1783662999834033
iteration : 7675
train acc:  0.8671875
train loss:  0.33256638050079346
train gradient:  0.18094123747729302
iteration : 7676
train acc:  0.8359375
train loss:  0.4234144687652588
train gradient:  0.2477887513267099
iteration : 7677
train acc:  0.8125
train loss:  0.4083593785762787
train gradient:  0.2767777931748513
iteration : 7678
train acc:  0.8359375
train loss:  0.4184890389442444
train gradient:  0.2489760174287011
iteration : 7679
train acc:  0.8984375
train loss:  0.2894940972328186
train gradient:  0.15051836628822549
iteration : 7680
train acc:  0.8359375
train loss:  0.35894063115119934
train gradient:  0.24401846191423543
iteration : 7681
train acc:  0.828125
train loss:  0.32346296310424805
train gradient:  0.1227707837690179
iteration : 7682
train acc:  0.828125
train loss:  0.3732883930206299
train gradient:  0.28114812364747793
iteration : 7683
train acc:  0.8203125
train loss:  0.3627411723136902
train gradient:  0.23160351372533183
iteration : 7684
train acc:  0.8515625
train loss:  0.41511017084121704
train gradient:  0.26679348132412173
iteration : 7685
train acc:  0.890625
train loss:  0.2601534128189087
train gradient:  0.14497644398923076
iteration : 7686
train acc:  0.796875
train loss:  0.37955912947654724
train gradient:  0.27153214902899897
iteration : 7687
train acc:  0.828125
train loss:  0.3745601177215576
train gradient:  0.30958959047972723
iteration : 7688
train acc:  0.859375
train loss:  0.3602025508880615
train gradient:  0.15958099354017463
iteration : 7689
train acc:  0.8125
train loss:  0.3306061029434204
train gradient:  0.22967642132545332
iteration : 7690
train acc:  0.859375
train loss:  0.2927878499031067
train gradient:  0.1537740460369032
iteration : 7691
train acc:  0.90625
train loss:  0.2711232304573059
train gradient:  0.1239861455842665
iteration : 7692
train acc:  0.828125
train loss:  0.3518218696117401
train gradient:  0.27214804404992016
iteration : 7693
train acc:  0.8515625
train loss:  0.3028877079486847
train gradient:  0.20910360197568517
iteration : 7694
train acc:  0.8359375
train loss:  0.32264161109924316
train gradient:  0.17287401339662434
iteration : 7695
train acc:  0.90625
train loss:  0.24087172746658325
train gradient:  0.10685874743782123
iteration : 7696
train acc:  0.8828125
train loss:  0.29379701614379883
train gradient:  0.20655663453645734
iteration : 7697
train acc:  0.828125
train loss:  0.3475446105003357
train gradient:  0.20872259695067302
iteration : 7698
train acc:  0.828125
train loss:  0.351776123046875
train gradient:  0.22851076412077242
iteration : 7699
train acc:  0.84375
train loss:  0.3854184150695801
train gradient:  0.2656762059748834
iteration : 7700
train acc:  0.8125
train loss:  0.36384057998657227
train gradient:  0.26694582081780893
iteration : 7701
train acc:  0.84375
train loss:  0.33230042457580566
train gradient:  0.14687515620081493
iteration : 7702
train acc:  0.859375
train loss:  0.2517625391483307
train gradient:  0.12843901915188366
iteration : 7703
train acc:  0.890625
train loss:  0.25458526611328125
train gradient:  0.16138296132045396
iteration : 7704
train acc:  0.8359375
train loss:  0.34196144342422485
train gradient:  0.17568623722452392
iteration : 7705
train acc:  0.875
train loss:  0.3145383894443512
train gradient:  0.20357267331415874
iteration : 7706
train acc:  0.8671875
train loss:  0.2972661256790161
train gradient:  0.1348267907372941
iteration : 7707
train acc:  0.8515625
train loss:  0.28018420934677124
train gradient:  0.15857064975879892
iteration : 7708
train acc:  0.8046875
train loss:  0.36648350954055786
train gradient:  0.2934338698707707
iteration : 7709
train acc:  0.84375
train loss:  0.40745803713798523
train gradient:  0.30762856883843626
iteration : 7710
train acc:  0.8828125
train loss:  0.2735133171081543
train gradient:  0.1649341807024135
iteration : 7711
train acc:  0.8203125
train loss:  0.40768367052078247
train gradient:  0.30040338185772814
iteration : 7712
train acc:  0.859375
train loss:  0.36297816038131714
train gradient:  0.2541650627336516
iteration : 7713
train acc:  0.921875
train loss:  0.2665342688560486
train gradient:  0.163098853736298
iteration : 7714
train acc:  0.8046875
train loss:  0.3444041609764099
train gradient:  0.18879941598965955
iteration : 7715
train acc:  0.8359375
train loss:  0.316849946975708
train gradient:  0.23426495733764258
iteration : 7716
train acc:  0.8359375
train loss:  0.34970664978027344
train gradient:  0.23274722404163894
iteration : 7717
train acc:  0.8828125
train loss:  0.285151869058609
train gradient:  0.16100990121776956
iteration : 7718
train acc:  0.9375
train loss:  0.2646038234233856
train gradient:  0.1968856356105096
iteration : 7719
train acc:  0.859375
train loss:  0.32561737298965454
train gradient:  0.19594094833757125
iteration : 7720
train acc:  0.8203125
train loss:  0.37656524777412415
train gradient:  0.28053535246373096
iteration : 7721
train acc:  0.875
train loss:  0.3251878619194031
train gradient:  0.1968294220239119
iteration : 7722
train acc:  0.875
train loss:  0.2673024535179138
train gradient:  0.15801176198241923
iteration : 7723
train acc:  0.84375
train loss:  0.33318275213241577
train gradient:  0.1550710533150476
iteration : 7724
train acc:  0.84375
train loss:  0.32898879051208496
train gradient:  0.1808276231288048
iteration : 7725
train acc:  0.8984375
train loss:  0.2749268412590027
train gradient:  0.18079292153767465
iteration : 7726
train acc:  0.828125
train loss:  0.33981233835220337
train gradient:  0.1870738040073241
iteration : 7727
train acc:  0.8515625
train loss:  0.31738001108169556
train gradient:  0.22685248383270243
iteration : 7728
train acc:  0.890625
train loss:  0.24816639721393585
train gradient:  0.10721409312297187
iteration : 7729
train acc:  0.8046875
train loss:  0.38345542550086975
train gradient:  0.28194062016642457
iteration : 7730
train acc:  0.84375
train loss:  0.2680928111076355
train gradient:  0.19122474042448268
iteration : 7731
train acc:  0.8828125
train loss:  0.32572680711746216
train gradient:  0.1641880071230812
iteration : 7732
train acc:  0.8359375
train loss:  0.3384774923324585
train gradient:  0.18206178901204903
iteration : 7733
train acc:  0.8671875
train loss:  0.3390709161758423
train gradient:  0.17813344453727586
iteration : 7734
train acc:  0.84375
train loss:  0.3630337715148926
train gradient:  0.24456128782379744
iteration : 7735
train acc:  0.84375
train loss:  0.33787620067596436
train gradient:  0.2035418551516243
iteration : 7736
train acc:  0.84375
train loss:  0.32467955350875854
train gradient:  0.16148843601174726
iteration : 7737
train acc:  0.8515625
train loss:  0.30739784240722656
train gradient:  0.12924520485526525
iteration : 7738
train acc:  0.828125
train loss:  0.3148079514503479
train gradient:  0.2532104988166438
iteration : 7739
train acc:  0.8359375
train loss:  0.3248724937438965
train gradient:  0.18056337180963172
iteration : 7740
train acc:  0.90625
train loss:  0.33725813031196594
train gradient:  0.2030718648544474
iteration : 7741
train acc:  0.859375
train loss:  0.3389955163002014
train gradient:  0.19125808613025908
iteration : 7742
train acc:  0.8671875
train loss:  0.2945680022239685
train gradient:  0.14592828791213697
iteration : 7743
train acc:  0.8125
train loss:  0.45208457112312317
train gradient:  0.25644892104291916
iteration : 7744
train acc:  0.84375
train loss:  0.3016965985298157
train gradient:  0.16739111921433533
iteration : 7745
train acc:  0.8515625
train loss:  0.34808242321014404
train gradient:  0.21522458681752743
iteration : 7746
train acc:  0.875
train loss:  0.26478713750839233
train gradient:  0.15107965495445375
iteration : 7747
train acc:  0.8515625
train loss:  0.34616416692733765
train gradient:  0.23672356289499918
iteration : 7748
train acc:  0.84375
train loss:  0.336385577917099
train gradient:  0.2727304253637592
iteration : 7749
train acc:  0.8515625
train loss:  0.3156247138977051
train gradient:  0.1966130019378612
iteration : 7750
train acc:  0.90625
train loss:  0.23479825258255005
train gradient:  0.1170337835357619
iteration : 7751
train acc:  0.765625
train loss:  0.5015701055526733
train gradient:  0.3680797677123054
iteration : 7752
train acc:  0.890625
train loss:  0.3635898232460022
train gradient:  0.22731120356785406
iteration : 7753
train acc:  0.84375
train loss:  0.3107059597969055
train gradient:  0.1588249039120202
iteration : 7754
train acc:  0.859375
train loss:  0.3186424672603607
train gradient:  0.18568874027802884
iteration : 7755
train acc:  0.8828125
train loss:  0.33104342222213745
train gradient:  0.24295780840999634
iteration : 7756
train acc:  0.8359375
train loss:  0.34371545910835266
train gradient:  0.2135241898636581
iteration : 7757
train acc:  0.8828125
train loss:  0.28613123297691345
train gradient:  0.23748895470618359
iteration : 7758
train acc:  0.8203125
train loss:  0.3496623635292053
train gradient:  0.1411468918193093
iteration : 7759
train acc:  0.8828125
train loss:  0.31779560446739197
train gradient:  0.1975040666303191
iteration : 7760
train acc:  0.875
train loss:  0.2796851396560669
train gradient:  0.1434469219743567
iteration : 7761
train acc:  0.8515625
train loss:  0.31152647733688354
train gradient:  0.19907055636901033
iteration : 7762
train acc:  0.8671875
train loss:  0.3598300814628601
train gradient:  0.21901252831985593
iteration : 7763
train acc:  0.8359375
train loss:  0.3947076201438904
train gradient:  0.2481790041038985
iteration : 7764
train acc:  0.796875
train loss:  0.46214622259140015
train gradient:  0.2716048055396021
iteration : 7765
train acc:  0.890625
train loss:  0.34220460057258606
train gradient:  0.16435334687522768
iteration : 7766
train acc:  0.875
train loss:  0.2986809015274048
train gradient:  0.14624518759049843
iteration : 7767
train acc:  0.875
train loss:  0.3293129801750183
train gradient:  0.1699325507789201
iteration : 7768
train acc:  0.890625
train loss:  0.2819538712501526
train gradient:  0.18476560104871642
iteration : 7769
train acc:  0.8984375
train loss:  0.2778948247432709
train gradient:  0.10764093352429456
iteration : 7770
train acc:  0.8203125
train loss:  0.4006134569644928
train gradient:  0.22603533802852424
iteration : 7771
train acc:  0.84375
train loss:  0.3397872745990753
train gradient:  0.20711759448482253
iteration : 7772
train acc:  0.859375
train loss:  0.34209585189819336
train gradient:  0.20744836357326168
iteration : 7773
train acc:  0.84375
train loss:  0.3101603090763092
train gradient:  0.11890101895512209
iteration : 7774
train acc:  0.90625
train loss:  0.2971944808959961
train gradient:  0.30001443565025443
iteration : 7775
train acc:  0.8515625
train loss:  0.3545835018157959
train gradient:  0.21018980304696752
iteration : 7776
train acc:  0.875
train loss:  0.28132450580596924
train gradient:  0.13637261599585018
iteration : 7777
train acc:  0.828125
train loss:  0.43294888734817505
train gradient:  0.29188232190472385
iteration : 7778
train acc:  0.8671875
train loss:  0.34408462047576904
train gradient:  0.17651956439959005
iteration : 7779
train acc:  0.859375
train loss:  0.36143648624420166
train gradient:  0.23600790792133608
iteration : 7780
train acc:  0.8515625
train loss:  0.31947851181030273
train gradient:  0.2366709530453866
iteration : 7781
train acc:  0.8515625
train loss:  0.33863574266433716
train gradient:  0.20294119445721945
iteration : 7782
train acc:  0.890625
train loss:  0.2746211290359497
train gradient:  0.39957160041947876
iteration : 7783
train acc:  0.828125
train loss:  0.3601081073284149
train gradient:  0.2289760516717086
iteration : 7784
train acc:  0.8828125
train loss:  0.3316650092601776
train gradient:  0.19768804773941273
iteration : 7785
train acc:  0.859375
train loss:  0.2819533050060272
train gradient:  0.14249219202210767
iteration : 7786
train acc:  0.8828125
train loss:  0.3032008111476898
train gradient:  0.18691475729379364
iteration : 7787
train acc:  0.8125
train loss:  0.36455538868904114
train gradient:  0.1983793516246711
iteration : 7788
train acc:  0.828125
train loss:  0.3525776267051697
train gradient:  0.19987575152230874
iteration : 7789
train acc:  0.859375
train loss:  0.3352225422859192
train gradient:  0.2076235870123297
iteration : 7790
train acc:  0.9140625
train loss:  0.22729268670082092
train gradient:  0.1484844864187383
iteration : 7791
train acc:  0.8359375
train loss:  0.31890636682510376
train gradient:  0.13969157634508667
iteration : 7792
train acc:  0.8046875
train loss:  0.37868785858154297
train gradient:  0.26423009176302237
iteration : 7793
train acc:  0.8359375
train loss:  0.3298300504684448
train gradient:  0.15492876562134408
iteration : 7794
train acc:  0.8515625
train loss:  0.3062421679496765
train gradient:  0.12108098542548533
iteration : 7795
train acc:  0.8203125
train loss:  0.3568155765533447
train gradient:  0.15291606163256508
iteration : 7796
train acc:  0.8984375
train loss:  0.2243901491165161
train gradient:  0.08124149532845527
iteration : 7797
train acc:  0.8671875
train loss:  0.3548167645931244
train gradient:  0.6219639145352895
iteration : 7798
train acc:  0.8828125
train loss:  0.28211480379104614
train gradient:  0.17289756463699962
iteration : 7799
train acc:  0.84375
train loss:  0.33266735076904297
train gradient:  0.14094918513106952
iteration : 7800
train acc:  0.8984375
train loss:  0.29822686314582825
train gradient:  0.1231005393434642
iteration : 7801
train acc:  0.8828125
train loss:  0.2939333915710449
train gradient:  0.15681924506809433
iteration : 7802
train acc:  0.8828125
train loss:  0.29281148314476013
train gradient:  0.17251894024149073
iteration : 7803
train acc:  0.8671875
train loss:  0.3110329210758209
train gradient:  0.16497588882663458
iteration : 7804
train acc:  0.8671875
train loss:  0.3100089430809021
train gradient:  0.14619938413348854
iteration : 7805
train acc:  0.8671875
train loss:  0.29391396045684814
train gradient:  0.19531036620373915
iteration : 7806
train acc:  0.828125
train loss:  0.37391233444213867
train gradient:  0.2213360127645507
iteration : 7807
train acc:  0.8515625
train loss:  0.3276981711387634
train gradient:  0.19479854642653505
iteration : 7808
train acc:  0.8203125
train loss:  0.39585596323013306
train gradient:  0.284767348988869
iteration : 7809
train acc:  0.875
train loss:  0.3366551399230957
train gradient:  0.204877017504396
iteration : 7810
train acc:  0.859375
train loss:  0.3134171962738037
train gradient:  0.151100236224247
iteration : 7811
train acc:  0.8515625
train loss:  0.3325803279876709
train gradient:  0.16043286409631985
iteration : 7812
train acc:  0.8671875
train loss:  0.30711668729782104
train gradient:  0.1971102639186515
iteration : 7813
train acc:  0.84375
train loss:  0.31172680854797363
train gradient:  0.1919775326706037
iteration : 7814
train acc:  0.8671875
train loss:  0.38588106632232666
train gradient:  0.3114890404654233
iteration : 7815
train acc:  0.875
train loss:  0.3374716639518738
train gradient:  0.16135359170604127
iteration : 7816
train acc:  0.8671875
train loss:  0.33078625798225403
train gradient:  0.2065343148059488
iteration : 7817
train acc:  0.8359375
train loss:  0.36261963844299316
train gradient:  0.1840809798503857
iteration : 7818
train acc:  0.8359375
train loss:  0.32456234097480774
train gradient:  0.1847519606818556
iteration : 7819
train acc:  0.8359375
train loss:  0.383284330368042
train gradient:  0.2653999632023798
iteration : 7820
train acc:  0.890625
train loss:  0.30497193336486816
train gradient:  0.16463280287478924
iteration : 7821
train acc:  0.8203125
train loss:  0.4902433753013611
train gradient:  0.2618070905389457
iteration : 7822
train acc:  0.859375
train loss:  0.3394123911857605
train gradient:  0.21519480006904784
iteration : 7823
train acc:  0.84375
train loss:  0.3130382299423218
train gradient:  0.16275620965494236
iteration : 7824
train acc:  0.8671875
train loss:  0.27973705530166626
train gradient:  0.13331637750558079
iteration : 7825
train acc:  0.7890625
train loss:  0.45534756779670715
train gradient:  0.45129293965687384
iteration : 7826
train acc:  0.8515625
train loss:  0.34306037425994873
train gradient:  0.252544516123785
iteration : 7827
train acc:  0.8984375
train loss:  0.28102347254753113
train gradient:  0.09432594451778221
iteration : 7828
train acc:  0.859375
train loss:  0.2696084678173065
train gradient:  0.13290314581491178
iteration : 7829
train acc:  0.8046875
train loss:  0.4012925922870636
train gradient:  0.22039554898514993
iteration : 7830
train acc:  0.8515625
train loss:  0.36009037494659424
train gradient:  0.36386203356391095
iteration : 7831
train acc:  0.90625
train loss:  0.25148388743400574
train gradient:  0.09392026579144908
iteration : 7832
train acc:  0.8046875
train loss:  0.45835158228874207
train gradient:  0.4330688081958281
iteration : 7833
train acc:  0.84375
train loss:  0.3643855154514313
train gradient:  0.2706858367743034
iteration : 7834
train acc:  0.890625
train loss:  0.3058355450630188
train gradient:  0.1727126935022966
iteration : 7835
train acc:  0.84375
train loss:  0.35583221912384033
train gradient:  0.13968841152695843
iteration : 7836
train acc:  0.828125
train loss:  0.3903733491897583
train gradient:  0.22069300839087636
iteration : 7837
train acc:  0.84375
train loss:  0.3648754954338074
train gradient:  0.20447845850796637
iteration : 7838
train acc:  0.8828125
train loss:  0.2890987992286682
train gradient:  0.1514678351881898
iteration : 7839
train acc:  0.8515625
train loss:  0.28150060772895813
train gradient:  0.1694333166681731
iteration : 7840
train acc:  0.859375
train loss:  0.3039917051792145
train gradient:  0.14412612852688758
iteration : 7841
train acc:  0.8125
train loss:  0.35382401943206787
train gradient:  0.21548496685256818
iteration : 7842
train acc:  0.8125
train loss:  0.4479263424873352
train gradient:  0.3002191022633834
iteration : 7843
train acc:  0.8203125
train loss:  0.3507748246192932
train gradient:  0.1934935299767337
iteration : 7844
train acc:  0.828125
train loss:  0.3761667311191559
train gradient:  0.18302775524374534
iteration : 7845
train acc:  0.8671875
train loss:  0.34106409549713135
train gradient:  0.16328209971063232
iteration : 7846
train acc:  0.859375
train loss:  0.27216991782188416
train gradient:  0.16387547491976637
iteration : 7847
train acc:  0.796875
train loss:  0.40214771032333374
train gradient:  0.27292000962663165
iteration : 7848
train acc:  0.859375
train loss:  0.363364577293396
train gradient:  0.23544634461435734
iteration : 7849
train acc:  0.84375
train loss:  0.327104777097702
train gradient:  0.1939403153330899
iteration : 7850
train acc:  0.8359375
train loss:  0.323385089635849
train gradient:  0.19879306828429055
iteration : 7851
train acc:  0.8359375
train loss:  0.4117951989173889
train gradient:  0.19256166383773135
iteration : 7852
train acc:  0.8671875
train loss:  0.2671825885772705
train gradient:  0.14784448099205716
iteration : 7853
train acc:  0.84375
train loss:  0.4456101655960083
train gradient:  0.24331829063789986
iteration : 7854
train acc:  0.828125
train loss:  0.3401302099227905
train gradient:  0.17540531566083437
iteration : 7855
train acc:  0.828125
train loss:  0.4030834436416626
train gradient:  0.353981366350508
iteration : 7856
train acc:  0.8671875
train loss:  0.2970496416091919
train gradient:  0.1360406008336347
iteration : 7857
train acc:  0.890625
train loss:  0.2832803726196289
train gradient:  0.10566179293684264
iteration : 7858
train acc:  0.8671875
train loss:  0.31832486391067505
train gradient:  0.123484509762495
iteration : 7859
train acc:  0.8671875
train loss:  0.3244253993034363
train gradient:  0.17045054764566545
iteration : 7860
train acc:  0.84375
train loss:  0.368783175945282
train gradient:  0.33965157270482554
iteration : 7861
train acc:  0.796875
train loss:  0.38195401430130005
train gradient:  0.189361568944841
iteration : 7862
train acc:  0.8515625
train loss:  0.3809349536895752
train gradient:  0.17977336742367608
iteration : 7863
train acc:  0.890625
train loss:  0.29793471097946167
train gradient:  0.1397914217862088
iteration : 7864
train acc:  0.8203125
train loss:  0.3724462389945984
train gradient:  0.26199573927299535
iteration : 7865
train acc:  0.828125
train loss:  0.3464415967464447
train gradient:  0.17774648642787147
iteration : 7866
train acc:  0.8671875
train loss:  0.35439425706863403
train gradient:  0.22408214930972264
iteration : 7867
train acc:  0.7890625
train loss:  0.41174960136413574
train gradient:  0.26323309709150855
iteration : 7868
train acc:  0.8046875
train loss:  0.4077349305152893
train gradient:  0.1971250866085538
iteration : 7869
train acc:  0.8671875
train loss:  0.329903781414032
train gradient:  0.16132954463171228
iteration : 7870
train acc:  0.84375
train loss:  0.32784658670425415
train gradient:  0.2040879925736056
iteration : 7871
train acc:  0.828125
train loss:  0.3649476170539856
train gradient:  0.19910327178983794
iteration : 7872
train acc:  0.8203125
train loss:  0.3670381009578705
train gradient:  0.1807604432365487
iteration : 7873
train acc:  0.8203125
train loss:  0.4096694588661194
train gradient:  0.201972069742484
iteration : 7874
train acc:  0.84375
train loss:  0.33855676651000977
train gradient:  0.1565607817853159
iteration : 7875
train acc:  0.8203125
train loss:  0.3451862931251526
train gradient:  0.16954777487126463
iteration : 7876
train acc:  0.8515625
train loss:  0.37287089228630066
train gradient:  0.17598629696874885
iteration : 7877
train acc:  0.859375
train loss:  0.32286831736564636
train gradient:  0.17437119457375108
iteration : 7878
train acc:  0.90625
train loss:  0.24854512512683868
train gradient:  0.10233640962802142
iteration : 7879
train acc:  0.8984375
train loss:  0.27506232261657715
train gradient:  0.15094911666523636
iteration : 7880
train acc:  0.921875
train loss:  0.25220391154289246
train gradient:  0.08992291429436133
iteration : 7881
train acc:  0.8515625
train loss:  0.35863691568374634
train gradient:  0.1848577892130394
iteration : 7882
train acc:  0.8125
train loss:  0.42985886335372925
train gradient:  0.3225461180628752
iteration : 7883
train acc:  0.890625
train loss:  0.28981366753578186
train gradient:  0.11971327182404182
iteration : 7884
train acc:  0.875
train loss:  0.34352219104766846
train gradient:  0.19465562303758216
iteration : 7885
train acc:  0.8515625
train loss:  0.35848987102508545
train gradient:  0.1608083273020648
iteration : 7886
train acc:  0.890625
train loss:  0.2772727310657501
train gradient:  0.10086267277379614
iteration : 7887
train acc:  0.8359375
train loss:  0.391243040561676
train gradient:  0.24644711741481504
iteration : 7888
train acc:  0.8515625
train loss:  0.31395840644836426
train gradient:  0.1558688513759564
iteration : 7889
train acc:  0.828125
train loss:  0.36976996064186096
train gradient:  0.21575642422609292
iteration : 7890
train acc:  0.875
train loss:  0.3063895106315613
train gradient:  0.13634992309404592
iteration : 7891
train acc:  0.8359375
train loss:  0.35272741317749023
train gradient:  0.18027419250251261
iteration : 7892
train acc:  0.8203125
train loss:  0.37934988737106323
train gradient:  0.19080307574288147
iteration : 7893
train acc:  0.890625
train loss:  0.2897973656654358
train gradient:  0.11898240450234436
iteration : 7894
train acc:  0.8046875
train loss:  0.3363955616950989
train gradient:  0.19060875688440881
iteration : 7895
train acc:  0.875
train loss:  0.2807014584541321
train gradient:  0.15165432535908502
iteration : 7896
train acc:  0.859375
train loss:  0.41224825382232666
train gradient:  0.21671947693253296
iteration : 7897
train acc:  0.8515625
train loss:  0.367125928401947
train gradient:  0.17319499534342908
iteration : 7898
train acc:  0.8515625
train loss:  0.3971889615058899
train gradient:  0.180104271331151
iteration : 7899
train acc:  0.875
train loss:  0.31852760910987854
train gradient:  0.1258691201014781
iteration : 7900
train acc:  0.8046875
train loss:  0.42910856008529663
train gradient:  0.39941091274842094
iteration : 7901
train acc:  0.8125
train loss:  0.33914825320243835
train gradient:  0.14785963838344165
iteration : 7902
train acc:  0.8671875
train loss:  0.2965017259120941
train gradient:  0.1793548691309302
iteration : 7903
train acc:  0.890625
train loss:  0.26066669821739197
train gradient:  0.15423120761747933
iteration : 7904
train acc:  0.8359375
train loss:  0.3515447974205017
train gradient:  0.2249609882424885
iteration : 7905
train acc:  0.8125
train loss:  0.3863154947757721
train gradient:  0.24923393948279288
iteration : 7906
train acc:  0.8046875
train loss:  0.3837329149246216
train gradient:  0.2178838946532426
iteration : 7907
train acc:  0.8515625
train loss:  0.3292456567287445
train gradient:  0.2355928651815286
iteration : 7908
train acc:  0.8515625
train loss:  0.3428260087966919
train gradient:  0.16925226574563754
iteration : 7909
train acc:  0.8359375
train loss:  0.3780803978443146
train gradient:  0.18307154187287583
iteration : 7910
train acc:  0.8671875
train loss:  0.3374408781528473
train gradient:  0.1349177154293335
iteration : 7911
train acc:  0.859375
train loss:  0.3608052730560303
train gradient:  0.2246866147491518
iteration : 7912
train acc:  0.8125
train loss:  0.34813374280929565
train gradient:  0.16642263755906092
iteration : 7913
train acc:  0.828125
train loss:  0.36169296503067017
train gradient:  0.17013103472214175
iteration : 7914
train acc:  0.8359375
train loss:  0.35414788126945496
train gradient:  0.2432694975391907
iteration : 7915
train acc:  0.8828125
train loss:  0.32235753536224365
train gradient:  0.14764171372993842
iteration : 7916
train acc:  0.8046875
train loss:  0.4249902665615082
train gradient:  0.17624018865029012
iteration : 7917
train acc:  0.875
train loss:  0.2906026840209961
train gradient:  0.18813331366094732
iteration : 7918
train acc:  0.8515625
train loss:  0.3039834499359131
train gradient:  0.17138042276780224
iteration : 7919
train acc:  0.890625
train loss:  0.28067898750305176
train gradient:  0.1061485979122195
iteration : 7920
train acc:  0.859375
train loss:  0.33312487602233887
train gradient:  0.1719722048842776
iteration : 7921
train acc:  0.8515625
train loss:  0.35110151767730713
train gradient:  0.3689581882500692
iteration : 7922
train acc:  0.7890625
train loss:  0.374668687582016
train gradient:  0.2351596335718175
iteration : 7923
train acc:  0.7890625
train loss:  0.4365147352218628
train gradient:  0.2652817251297114
iteration : 7924
train acc:  0.890625
train loss:  0.3461533188819885
train gradient:  0.22489723635163245
iteration : 7925
train acc:  0.8203125
train loss:  0.4047587215900421
train gradient:  0.21605892225030662
iteration : 7926
train acc:  0.84375
train loss:  0.33799973130226135
train gradient:  0.14973315864179715
iteration : 7927
train acc:  0.875
train loss:  0.30767324566841125
train gradient:  0.15226763501020352
iteration : 7928
train acc:  0.890625
train loss:  0.2737886607646942
train gradient:  0.11262549887578308
iteration : 7929
train acc:  0.859375
train loss:  0.31407082080841064
train gradient:  0.1690940680818953
iteration : 7930
train acc:  0.859375
train loss:  0.3231625556945801
train gradient:  0.18301562532078458
iteration : 7931
train acc:  0.875
train loss:  0.2895413339138031
train gradient:  0.11312993874716284
iteration : 7932
train acc:  0.78125
train loss:  0.4306723475456238
train gradient:  0.2761115754114006
iteration : 7933
train acc:  0.828125
train loss:  0.3562111258506775
train gradient:  0.15997315862667127
iteration : 7934
train acc:  0.875
train loss:  0.333276629447937
train gradient:  0.161311347252933
iteration : 7935
train acc:  0.890625
train loss:  0.2832639217376709
train gradient:  0.1682087350234454
iteration : 7936
train acc:  0.8515625
train loss:  0.3661394715309143
train gradient:  0.21515485836435178
iteration : 7937
train acc:  0.8515625
train loss:  0.3066500723361969
train gradient:  0.13961571309287066
iteration : 7938
train acc:  0.8828125
train loss:  0.3257456123828888
train gradient:  0.14649545807104042
iteration : 7939
train acc:  0.8359375
train loss:  0.38632500171661377
train gradient:  0.29360253355407345
iteration : 7940
train acc:  0.859375
train loss:  0.35856062173843384
train gradient:  0.20971849951982388
iteration : 7941
train acc:  0.8046875
train loss:  0.3522154688835144
train gradient:  0.15860943314009474
iteration : 7942
train acc:  0.8359375
train loss:  0.3550715148448944
train gradient:  0.24404028481619194
iteration : 7943
train acc:  0.84375
train loss:  0.3441786766052246
train gradient:  0.18121362351765757
iteration : 7944
train acc:  0.890625
train loss:  0.2823300063610077
train gradient:  0.10935420672632845
iteration : 7945
train acc:  0.7890625
train loss:  0.42328163981437683
train gradient:  0.24247261426148098
iteration : 7946
train acc:  0.8125
train loss:  0.40288665890693665
train gradient:  0.263213820203909
iteration : 7947
train acc:  0.8828125
train loss:  0.3193909525871277
train gradient:  0.1424505738900689
iteration : 7948
train acc:  0.859375
train loss:  0.34531837701797485
train gradient:  0.16307938670021221
iteration : 7949
train acc:  0.875
train loss:  0.3222244083881378
train gradient:  0.15273656470684144
iteration : 7950
train acc:  0.8359375
train loss:  0.44564205408096313
train gradient:  0.1951264969893362
iteration : 7951
train acc:  0.8203125
train loss:  0.35900580883026123
train gradient:  0.21163996010133784
iteration : 7952
train acc:  0.8671875
train loss:  0.30938130617141724
train gradient:  0.13487834128316958
iteration : 7953
train acc:  0.8984375
train loss:  0.3144940733909607
train gradient:  0.15376094221520706
iteration : 7954
train acc:  0.8671875
train loss:  0.2726442217826843
train gradient:  0.20286279414678926
iteration : 7955
train acc:  0.8515625
train loss:  0.3842317461967468
train gradient:  0.21017157419416904
iteration : 7956
train acc:  0.90625
train loss:  0.3081086575984955
train gradient:  0.27321692185229396
iteration : 7957
train acc:  0.8125
train loss:  0.38740628957748413
train gradient:  0.22951874259009686
iteration : 7958
train acc:  0.859375
train loss:  0.3303016424179077
train gradient:  0.15905358972553413
iteration : 7959
train acc:  0.8671875
train loss:  0.34967827796936035
train gradient:  0.23963119698623023
iteration : 7960
train acc:  0.8359375
train loss:  0.32906025648117065
train gradient:  0.15597189605908424
iteration : 7961
train acc:  0.859375
train loss:  0.3752098083496094
train gradient:  0.2897472205561909
iteration : 7962
train acc:  0.875
train loss:  0.30763953924179077
train gradient:  0.3125935614747127
iteration : 7963
train acc:  0.890625
train loss:  0.26939716935157776
train gradient:  0.14135695916658764
iteration : 7964
train acc:  0.84375
train loss:  0.37451547384262085
train gradient:  0.27595269122263605
iteration : 7965
train acc:  0.890625
train loss:  0.28067871928215027
train gradient:  0.16591970526950245
iteration : 7966
train acc:  0.828125
train loss:  0.3765064477920532
train gradient:  0.23032043297292826
iteration : 7967
train acc:  0.8828125
train loss:  0.353182315826416
train gradient:  0.2747877987152896
iteration : 7968
train acc:  0.875
train loss:  0.2999093532562256
train gradient:  0.1536683346850263
iteration : 7969
train acc:  0.8359375
train loss:  0.34692203998565674
train gradient:  0.2107989001887146
iteration : 7970
train acc:  0.8046875
train loss:  0.4122617542743683
train gradient:  0.4367035768121492
iteration : 7971
train acc:  0.84375
train loss:  0.37037089467048645
train gradient:  0.2039488340647358
iteration : 7972
train acc:  0.875
train loss:  0.2956903576850891
train gradient:  0.13226566881382212
iteration : 7973
train acc:  0.859375
train loss:  0.3340725898742676
train gradient:  0.17180570016233107
iteration : 7974
train acc:  0.8671875
train loss:  0.3169158101081848
train gradient:  0.1489467643637522
iteration : 7975
train acc:  0.875
train loss:  0.33321964740753174
train gradient:  0.20777738128706502
iteration : 7976
train acc:  0.8828125
train loss:  0.33753153681755066
train gradient:  0.15660521162365149
iteration : 7977
train acc:  0.8203125
train loss:  0.4587995409965515
train gradient:  0.4374196651782601
iteration : 7978
train acc:  0.8515625
train loss:  0.33505553007125854
train gradient:  0.14973406113487958
iteration : 7979
train acc:  0.8828125
train loss:  0.3077854514122009
train gradient:  0.16222928110359147
iteration : 7980
train acc:  0.8671875
train loss:  0.321003258228302
train gradient:  0.1797078587196079
iteration : 7981
train acc:  0.875
train loss:  0.30781570076942444
train gradient:  0.1543028773043672
iteration : 7982
train acc:  0.8359375
train loss:  0.3657500147819519
train gradient:  0.22386112214261633
iteration : 7983
train acc:  0.8046875
train loss:  0.3911839425563812
train gradient:  0.21382049894240227
iteration : 7984
train acc:  0.90625
train loss:  0.24848143756389618
train gradient:  0.13692635097594902
iteration : 7985
train acc:  0.8359375
train loss:  0.37489908933639526
train gradient:  0.1998186803532789
iteration : 7986
train acc:  0.84375
train loss:  0.2826438248157501
train gradient:  0.1311534596195766
iteration : 7987
train acc:  0.8515625
train loss:  0.3419789671897888
train gradient:  0.20960057310098856
iteration : 7988
train acc:  0.8359375
train loss:  0.3439415991306305
train gradient:  0.21698369118239452
iteration : 7989
train acc:  0.8359375
train loss:  0.33132320642471313
train gradient:  0.1604996043829131
iteration : 7990
train acc:  0.875
train loss:  0.3208627700805664
train gradient:  0.2264633772942471
iteration : 7991
train acc:  0.84375
train loss:  0.37890884280204773
train gradient:  0.18025200607395195
iteration : 7992
train acc:  0.875
train loss:  0.33352863788604736
train gradient:  0.19116171630240822
iteration : 7993
train acc:  0.8203125
train loss:  0.3627896308898926
train gradient:  0.3068306749311433
iteration : 7994
train acc:  0.859375
train loss:  0.273803174495697
train gradient:  0.1326476728209816
iteration : 7995
train acc:  0.859375
train loss:  0.2717214822769165
train gradient:  0.1490255553054483
iteration : 7996
train acc:  0.828125
train loss:  0.3254251182079315
train gradient:  0.16359195754225098
iteration : 7997
train acc:  0.859375
train loss:  0.3250634968280792
train gradient:  0.2693977461456197
iteration : 7998
train acc:  0.859375
train loss:  0.3430476188659668
train gradient:  0.17648244872032662
iteration : 7999
train acc:  0.8515625
train loss:  0.3226016163825989
train gradient:  0.2146108989567535
iteration : 8000
train acc:  0.8828125
train loss:  0.29855096340179443
train gradient:  0.15053846787792108
iteration : 8001
train acc:  0.875
train loss:  0.3045300245285034
train gradient:  0.15678194877909993
iteration : 8002
train acc:  0.8359375
train loss:  0.4248650074005127
train gradient:  0.3004519072307641
iteration : 8003
train acc:  0.8046875
train loss:  0.3668668866157532
train gradient:  0.19503010032067886
iteration : 8004
train acc:  0.8203125
train loss:  0.35339295864105225
train gradient:  0.16456829769701484
iteration : 8005
train acc:  0.8828125
train loss:  0.31824061274528503
train gradient:  0.1344641644243526
iteration : 8006
train acc:  0.828125
train loss:  0.3970387279987335
train gradient:  0.27292543038447614
iteration : 8007
train acc:  0.8671875
train loss:  0.29585689306259155
train gradient:  0.20119190273034582
iteration : 8008
train acc:  0.8046875
train loss:  0.3909400701522827
train gradient:  0.30858873221087874
iteration : 8009
train acc:  0.828125
train loss:  0.3546728491783142
train gradient:  0.23481985100737562
iteration : 8010
train acc:  0.8828125
train loss:  0.30883967876434326
train gradient:  0.13404745422770764
iteration : 8011
train acc:  0.78125
train loss:  0.46264564990997314
train gradient:  0.3508532224553542
iteration : 8012
train acc:  0.859375
train loss:  0.35640403628349304
train gradient:  0.244640056486763
iteration : 8013
train acc:  0.8125
train loss:  0.3657010793685913
train gradient:  0.21997475695670304
iteration : 8014
train acc:  0.84375
train loss:  0.35259175300598145
train gradient:  0.22574656990041303
iteration : 8015
train acc:  0.84375
train loss:  0.30638349056243896
train gradient:  0.13917981770535803
iteration : 8016
train acc:  0.8515625
train loss:  0.3522402346134186
train gradient:  0.21787469987053593
iteration : 8017
train acc:  0.859375
train loss:  0.3526489734649658
train gradient:  0.18858450024584306
iteration : 8018
train acc:  0.921875
train loss:  0.2373896986246109
train gradient:  0.11396416909676287
iteration : 8019
train acc:  0.890625
train loss:  0.26274043321609497
train gradient:  0.13866545298199678
iteration : 8020
train acc:  0.828125
train loss:  0.37736594676971436
train gradient:  0.25052236870766326
iteration : 8021
train acc:  0.84375
train loss:  0.3626079559326172
train gradient:  0.18057062063942905
iteration : 8022
train acc:  0.859375
train loss:  0.3252399265766144
train gradient:  0.23034998160479392
iteration : 8023
train acc:  0.8046875
train loss:  0.36983731389045715
train gradient:  0.231031191963273
iteration : 8024
train acc:  0.9453125
train loss:  0.25845813751220703
train gradient:  0.08602535014363427
iteration : 8025
train acc:  0.8984375
train loss:  0.23200739920139313
train gradient:  0.0931898143123794
iteration : 8026
train acc:  0.8125
train loss:  0.4217645525932312
train gradient:  0.34893542684947615
iteration : 8027
train acc:  0.8515625
train loss:  0.36949068307876587
train gradient:  0.24270836295056114
iteration : 8028
train acc:  0.890625
train loss:  0.2941703200340271
train gradient:  0.14505923631963843
iteration : 8029
train acc:  0.875
train loss:  0.2908147871494293
train gradient:  0.11894489151172533
iteration : 8030
train acc:  0.7890625
train loss:  0.43759238719940186
train gradient:  0.29958585150397876
iteration : 8031
train acc:  0.8671875
train loss:  0.3086000680923462
train gradient:  0.1898133967397762
iteration : 8032
train acc:  0.859375
train loss:  0.3745204210281372
train gradient:  0.1939889216586539
iteration : 8033
train acc:  0.84375
train loss:  0.3945241868495941
train gradient:  0.20492634085133282
iteration : 8034
train acc:  0.8125
train loss:  0.39435237646102905
train gradient:  0.1999370978937406
iteration : 8035
train acc:  0.8671875
train loss:  0.31580162048339844
train gradient:  0.15812591084881394
iteration : 8036
train acc:  0.8671875
train loss:  0.2865867614746094
train gradient:  0.21954365438435736
iteration : 8037
train acc:  0.828125
train loss:  0.33620572090148926
train gradient:  0.16184663936186056
iteration : 8038
train acc:  0.828125
train loss:  0.3483588397502899
train gradient:  0.20316930858464427
iteration : 8039
train acc:  0.8515625
train loss:  0.3557189106941223
train gradient:  0.20619771352193897
iteration : 8040
train acc:  0.8046875
train loss:  0.4713757634162903
train gradient:  0.5466816527521012
iteration : 8041
train acc:  0.8203125
train loss:  0.3919331729412079
train gradient:  0.18421746260452238
iteration : 8042
train acc:  0.796875
train loss:  0.41644418239593506
train gradient:  0.25589047625741657
iteration : 8043
train acc:  0.875
train loss:  0.2983677089214325
train gradient:  0.1362314116527217
iteration : 8044
train acc:  0.84375
train loss:  0.44224655628204346
train gradient:  0.23070791922259967
iteration : 8045
train acc:  0.8203125
train loss:  0.38185834884643555
train gradient:  0.18028410010472323
iteration : 8046
train acc:  0.90625
train loss:  0.27074629068374634
train gradient:  0.14737273395865663
iteration : 8047
train acc:  0.875
train loss:  0.3452470004558563
train gradient:  0.14479389676497625
iteration : 8048
train acc:  0.828125
train loss:  0.3507661819458008
train gradient:  0.19345750717575227
iteration : 8049
train acc:  0.828125
train loss:  0.4041668772697449
train gradient:  0.22494236377299598
iteration : 8050
train acc:  0.8359375
train loss:  0.3726264238357544
train gradient:  0.2573155586878203
iteration : 8051
train acc:  0.8671875
train loss:  0.2597738206386566
train gradient:  0.11048987027710323
iteration : 8052
train acc:  0.859375
train loss:  0.3206580579280853
train gradient:  0.1483275780966667
iteration : 8053
train acc:  0.84375
train loss:  0.3250075578689575
train gradient:  0.19002065182417002
iteration : 8054
train acc:  0.8359375
train loss:  0.39345216751098633
train gradient:  0.2684789005286694
iteration : 8055
train acc:  0.8515625
train loss:  0.2713812589645386
train gradient:  0.14405482070414544
iteration : 8056
train acc:  0.890625
train loss:  0.30387377738952637
train gradient:  0.1598036779437473
iteration : 8057
train acc:  0.796875
train loss:  0.3660653829574585
train gradient:  0.18162857334180033
iteration : 8058
train acc:  0.796875
train loss:  0.42499440908432007
train gradient:  0.23618398622636155
iteration : 8059
train acc:  0.875
train loss:  0.3119925260543823
train gradient:  0.12491513655988874
iteration : 8060
train acc:  0.8359375
train loss:  0.37847405672073364
train gradient:  0.19266426673148565
iteration : 8061
train acc:  0.84375
train loss:  0.38360458612442017
train gradient:  0.23819422946178528
iteration : 8062
train acc:  0.875
train loss:  0.28785231709480286
train gradient:  0.19710593581242605
iteration : 8063
train acc:  0.921875
train loss:  0.26584091782569885
train gradient:  0.10741525369871122
iteration : 8064
train acc:  0.859375
train loss:  0.32029587030410767
train gradient:  0.18250812621626322
iteration : 8065
train acc:  0.890625
train loss:  0.31702858209609985
train gradient:  0.18538519706740508
iteration : 8066
train acc:  0.8203125
train loss:  0.39330822229385376
train gradient:  0.17557678555152484
iteration : 8067
train acc:  0.84375
train loss:  0.34377142786979675
train gradient:  0.24466296555687217
iteration : 8068
train acc:  0.8515625
train loss:  0.31758537888526917
train gradient:  0.14627611273851693
iteration : 8069
train acc:  0.8359375
train loss:  0.36021688580513
train gradient:  0.1722901625330913
iteration : 8070
train acc:  0.8046875
train loss:  0.41786131262779236
train gradient:  0.21447930064365445
iteration : 8071
train acc:  0.828125
train loss:  0.33998122811317444
train gradient:  0.14568524792518994
iteration : 8072
train acc:  0.875
train loss:  0.3014770448207855
train gradient:  0.23740898512109565
iteration : 8073
train acc:  0.84375
train loss:  0.33808064460754395
train gradient:  0.22427840312852293
iteration : 8074
train acc:  0.859375
train loss:  0.3380938172340393
train gradient:  0.21157691651327573
iteration : 8075
train acc:  0.8125
train loss:  0.4176185131072998
train gradient:  0.246033437930243
iteration : 8076
train acc:  0.8515625
train loss:  0.3189534544944763
train gradient:  0.15758873971532056
iteration : 8077
train acc:  0.8203125
train loss:  0.36966994404792786
train gradient:  0.258037103461936
iteration : 8078
train acc:  0.8359375
train loss:  0.3445129990577698
train gradient:  0.16802644188526672
iteration : 8079
train acc:  0.8671875
train loss:  0.27594152092933655
train gradient:  0.10960347723013332
iteration : 8080
train acc:  0.8671875
train loss:  0.2951675355434418
train gradient:  0.10841827760057182
iteration : 8081
train acc:  0.8515625
train loss:  0.2929447293281555
train gradient:  0.14959453008467732
iteration : 8082
train acc:  0.8359375
train loss:  0.35566988587379456
train gradient:  0.23896790261117345
iteration : 8083
train acc:  0.8671875
train loss:  0.37268775701522827
train gradient:  0.17637229040381658
iteration : 8084
train acc:  0.7890625
train loss:  0.36591142416000366
train gradient:  0.1966011669955901
iteration : 8085
train acc:  0.8203125
train loss:  0.3731577396392822
train gradient:  0.15200426769314784
iteration : 8086
train acc:  0.8671875
train loss:  0.3123703598976135
train gradient:  0.1551691350887096
iteration : 8087
train acc:  0.828125
train loss:  0.4055444002151489
train gradient:  0.2768653034416288
iteration : 8088
train acc:  0.875
train loss:  0.2998744249343872
train gradient:  0.14431118314006378
iteration : 8089
train acc:  0.84375
train loss:  0.3419390916824341
train gradient:  0.18852697514781758
iteration : 8090
train acc:  0.8671875
train loss:  0.30774402618408203
train gradient:  0.1010260529021066
iteration : 8091
train acc:  0.875
train loss:  0.2841871976852417
train gradient:  0.13136494808473834
iteration : 8092
train acc:  0.875
train loss:  0.270439088344574
train gradient:  0.15982882908586407
iteration : 8093
train acc:  0.890625
train loss:  0.27534300088882446
train gradient:  0.15770980498584608
iteration : 8094
train acc:  0.921875
train loss:  0.284321129322052
train gradient:  0.16417475494195743
iteration : 8095
train acc:  0.8515625
train loss:  0.35783979296684265
train gradient:  0.2082668245402458
iteration : 8096
train acc:  0.828125
train loss:  0.3631784915924072
train gradient:  0.16414530576222008
iteration : 8097
train acc:  0.890625
train loss:  0.23449042439460754
train gradient:  0.09512563223177722
iteration : 8098
train acc:  0.828125
train loss:  0.37624892592430115
train gradient:  0.22089534876129951
iteration : 8099
train acc:  0.84375
train loss:  0.32519203424453735
train gradient:  0.1457127800466334
iteration : 8100
train acc:  0.8828125
train loss:  0.3137611746788025
train gradient:  0.14723554257632357
iteration : 8101
train acc:  0.890625
train loss:  0.3137580454349518
train gradient:  0.17773582127715837
iteration : 8102
train acc:  0.8359375
train loss:  0.38836026191711426
train gradient:  0.252215999145034
iteration : 8103
train acc:  0.890625
train loss:  0.26027706265449524
train gradient:  0.08832991502690236
iteration : 8104
train acc:  0.875
train loss:  0.30009716749191284
train gradient:  0.16156481973581593
iteration : 8105
train acc:  0.8359375
train loss:  0.3866041898727417
train gradient:  0.18185725521502405
iteration : 8106
train acc:  0.8828125
train loss:  0.28370094299316406
train gradient:  0.12069859663919892
iteration : 8107
train acc:  0.8828125
train loss:  0.3026129901409149
train gradient:  0.1478849106628364
iteration : 8108
train acc:  0.890625
train loss:  0.36606690287590027
train gradient:  0.16117557761996626
iteration : 8109
train acc:  0.78125
train loss:  0.4615582227706909
train gradient:  0.29023464315508657
iteration : 8110
train acc:  0.8671875
train loss:  0.34102433919906616
train gradient:  0.20174067049746852
iteration : 8111
train acc:  0.84375
train loss:  0.35529112815856934
train gradient:  0.19045745432940614
iteration : 8112
train acc:  0.84375
train loss:  0.3089178502559662
train gradient:  0.1422201336469881
iteration : 8113
train acc:  0.859375
train loss:  0.28037193417549133
train gradient:  0.1778565672115327
iteration : 8114
train acc:  0.875
train loss:  0.29632583260536194
train gradient:  0.15859965038835974
iteration : 8115
train acc:  0.8828125
train loss:  0.2883327603340149
train gradient:  0.12041182363127809
iteration : 8116
train acc:  0.84375
train loss:  0.34983712434768677
train gradient:  0.17782034187310644
iteration : 8117
train acc:  0.8203125
train loss:  0.34793519973754883
train gradient:  0.16859500568070032
iteration : 8118
train acc:  0.859375
train loss:  0.37653136253356934
train gradient:  0.26336358639272056
iteration : 8119
train acc:  0.8125
train loss:  0.3730003833770752
train gradient:  0.26248068201623603
iteration : 8120
train acc:  0.7890625
train loss:  0.40149861574172974
train gradient:  0.27159757620554226
iteration : 8121
train acc:  0.875
train loss:  0.32382920384407043
train gradient:  0.14527884501976157
iteration : 8122
train acc:  0.8203125
train loss:  0.378129243850708
train gradient:  0.15228848679476925
iteration : 8123
train acc:  0.8125
train loss:  0.36824196577072144
train gradient:  0.2344562143922489
iteration : 8124
train acc:  0.828125
train loss:  0.3817686438560486
train gradient:  0.2352073001073438
iteration : 8125
train acc:  0.828125
train loss:  0.3655266761779785
train gradient:  0.176208709884627
iteration : 8126
train acc:  0.8203125
train loss:  0.321157842874527
train gradient:  0.1607072230359795
iteration : 8127
train acc:  0.875
train loss:  0.30115675926208496
train gradient:  0.15340978274801886
iteration : 8128
train acc:  0.8515625
train loss:  0.36059701442718506
train gradient:  0.23933294196432392
iteration : 8129
train acc:  0.7890625
train loss:  0.42188936471939087
train gradient:  0.2657844532924693
iteration : 8130
train acc:  0.8515625
train loss:  0.3602842688560486
train gradient:  0.16759274074278924
iteration : 8131
train acc:  0.8515625
train loss:  0.3943501114845276
train gradient:  0.21231718058274818
iteration : 8132
train acc:  0.8359375
train loss:  0.3392118215560913
train gradient:  0.2341412033507885
iteration : 8133
train acc:  0.890625
train loss:  0.33594468235969543
train gradient:  0.17647991963370002
iteration : 8134
train acc:  0.8671875
train loss:  0.2980150580406189
train gradient:  0.20085209284061734
iteration : 8135
train acc:  0.765625
train loss:  0.4170638918876648
train gradient:  0.2946815373363783
iteration : 8136
train acc:  0.8359375
train loss:  0.3671315908432007
train gradient:  0.23699715915989591
iteration : 8137
train acc:  0.8671875
train loss:  0.28292596340179443
train gradient:  0.13469155089472512
iteration : 8138
train acc:  0.7890625
train loss:  0.41480183601379395
train gradient:  0.27485681825528746
iteration : 8139
train acc:  0.8671875
train loss:  0.30695998668670654
train gradient:  0.15112222989087187
iteration : 8140
train acc:  0.7890625
train loss:  0.36368459463119507
train gradient:  0.1520200483181266
iteration : 8141
train acc:  0.8125
train loss:  0.3785533308982849
train gradient:  0.24014078888915963
iteration : 8142
train acc:  0.859375
train loss:  0.2526835501194
train gradient:  0.13148284559997117
iteration : 8143
train acc:  0.859375
train loss:  0.3292028605937958
train gradient:  0.14585555749501594
iteration : 8144
train acc:  0.8828125
train loss:  0.283854603767395
train gradient:  0.11416310217682828
iteration : 8145
train acc:  0.84375
train loss:  0.3823651671409607
train gradient:  0.22527787831988694
iteration : 8146
train acc:  0.84375
train loss:  0.37677890062332153
train gradient:  0.22141070341546695
iteration : 8147
train acc:  0.890625
train loss:  0.26187241077423096
train gradient:  0.14485248215289553
iteration : 8148
train acc:  0.8515625
train loss:  0.33521178364753723
train gradient:  0.20375317407243276
iteration : 8149
train acc:  0.8671875
train loss:  0.29598838090896606
train gradient:  0.1597639708960134
iteration : 8150
train acc:  0.8828125
train loss:  0.2883032262325287
train gradient:  0.11616536412315943
iteration : 8151
train acc:  0.8359375
train loss:  0.3366020917892456
train gradient:  0.17833246923916535
iteration : 8152
train acc:  0.796875
train loss:  0.34870007634162903
train gradient:  0.19181643077604513
iteration : 8153
train acc:  0.84375
train loss:  0.3360654413700104
train gradient:  0.253036031978054
iteration : 8154
train acc:  0.828125
train loss:  0.39292019605636597
train gradient:  0.1973128884601798
iteration : 8155
train acc:  0.921875
train loss:  0.24297519028186798
train gradient:  0.12074162864826989
iteration : 8156
train acc:  0.8671875
train loss:  0.31708234548568726
train gradient:  0.1479680789826957
iteration : 8157
train acc:  0.8515625
train loss:  0.3580366373062134
train gradient:  0.187565654588827
iteration : 8158
train acc:  0.859375
train loss:  0.4147702753543854
train gradient:  0.19464633659448743
iteration : 8159
train acc:  0.796875
train loss:  0.40941959619522095
train gradient:  0.28425154437787603
iteration : 8160
train acc:  0.8359375
train loss:  0.34212809801101685
train gradient:  0.2427545039796755
iteration : 8161
train acc:  0.828125
train loss:  0.3513098955154419
train gradient:  0.15581038589547966
iteration : 8162
train acc:  0.8984375
train loss:  0.24479985237121582
train gradient:  0.0971516711160378
iteration : 8163
train acc:  0.875
train loss:  0.27635058760643005
train gradient:  0.10448866200438181
iteration : 8164
train acc:  0.796875
train loss:  0.38444983959198
train gradient:  0.2053395516055874
iteration : 8165
train acc:  0.8828125
train loss:  0.29063159227371216
train gradient:  0.09792498782583982
iteration : 8166
train acc:  0.8671875
train loss:  0.3427155017852783
train gradient:  0.16199384024406022
iteration : 8167
train acc:  0.8671875
train loss:  0.3021059036254883
train gradient:  0.1495005297031387
iteration : 8168
train acc:  0.8359375
train loss:  0.39202409982681274
train gradient:  0.20225038039498372
iteration : 8169
train acc:  0.828125
train loss:  0.43599241971969604
train gradient:  0.270540757495464
iteration : 8170
train acc:  0.875
train loss:  0.2926073968410492
train gradient:  0.21733925658099648
iteration : 8171
train acc:  0.796875
train loss:  0.420055091381073
train gradient:  0.24021447285581765
iteration : 8172
train acc:  0.8359375
train loss:  0.3772733211517334
train gradient:  0.1674799603806728
iteration : 8173
train acc:  0.84375
train loss:  0.29861435294151306
train gradient:  0.16530972082407336
iteration : 8174
train acc:  0.8828125
train loss:  0.27208632230758667
train gradient:  0.10552684284712566
iteration : 8175
train acc:  0.8359375
train loss:  0.347148060798645
train gradient:  0.13916177242749334
iteration : 8176
train acc:  0.8828125
train loss:  0.30467140674591064
train gradient:  0.15381332985626456
iteration : 8177
train acc:  0.8359375
train loss:  0.3906623423099518
train gradient:  0.18642094638549078
iteration : 8178
train acc:  0.859375
train loss:  0.3452880382537842
train gradient:  0.15200221037505937
iteration : 8179
train acc:  0.8046875
train loss:  0.43731188774108887
train gradient:  0.1869643802157158
iteration : 8180
train acc:  0.8359375
train loss:  0.337077796459198
train gradient:  0.21204936629989235
iteration : 8181
train acc:  0.84375
train loss:  0.31157389283180237
train gradient:  0.13322897509288006
iteration : 8182
train acc:  0.890625
train loss:  0.2760354280471802
train gradient:  0.11188197636479995
iteration : 8183
train acc:  0.7578125
train loss:  0.5019078254699707
train gradient:  0.4006062439766871
iteration : 8184
train acc:  0.8125
train loss:  0.3667273223400116
train gradient:  0.2551062911238935
iteration : 8185
train acc:  0.828125
train loss:  0.3885338008403778
train gradient:  0.2333171112101552
iteration : 8186
train acc:  0.8359375
train loss:  0.3627983331680298
train gradient:  0.2750501551870601
iteration : 8187
train acc:  0.8125
train loss:  0.3447631597518921
train gradient:  0.13778657742999847
iteration : 8188
train acc:  0.8671875
train loss:  0.4109765887260437
train gradient:  0.2536187964257656
iteration : 8189
train acc:  0.84375
train loss:  0.3052182197570801
train gradient:  0.12113505042686132
iteration : 8190
train acc:  0.859375
train loss:  0.33921942114830017
train gradient:  0.15093661851776569
iteration : 8191
train acc:  0.859375
train loss:  0.3353448212146759
train gradient:  0.1973544863499991
iteration : 8192
train acc:  0.8359375
train loss:  0.3237839341163635
train gradient:  0.18061583745348775
iteration : 8193
train acc:  0.8671875
train loss:  0.33684325218200684
train gradient:  0.1573213088590939
iteration : 8194
train acc:  0.859375
train loss:  0.3327392339706421
train gradient:  0.22290287878538292
iteration : 8195
train acc:  0.859375
train loss:  0.31748685240745544
train gradient:  0.15951625409992842
iteration : 8196
train acc:  0.84375
train loss:  0.35524219274520874
train gradient:  0.25917942325151033
iteration : 8197
train acc:  0.859375
train loss:  0.33527928590774536
train gradient:  0.18507326124621995
iteration : 8198
train acc:  0.8046875
train loss:  0.3730663061141968
train gradient:  0.1952299791987635
iteration : 8199
train acc:  0.84375
train loss:  0.40136128664016724
train gradient:  0.19466725910856303
iteration : 8200
train acc:  0.8359375
train loss:  0.35486671328544617
train gradient:  0.1790237237649405
iteration : 8201
train acc:  0.8359375
train loss:  0.3487241864204407
train gradient:  0.1693670221204982
iteration : 8202
train acc:  0.859375
train loss:  0.3159014582633972
train gradient:  0.17760712151155386
iteration : 8203
train acc:  0.84375
train loss:  0.32879847288131714
train gradient:  0.18968530818665946
iteration : 8204
train acc:  0.8828125
train loss:  0.29640454053878784
train gradient:  0.16083784401407522
iteration : 8205
train acc:  0.90625
train loss:  0.25169825553894043
train gradient:  0.10206185359042574
iteration : 8206
train acc:  0.8671875
train loss:  0.35436350107192993
train gradient:  0.2214491252214107
iteration : 8207
train acc:  0.8671875
train loss:  0.3469911813735962
train gradient:  0.17692299805241685
iteration : 8208
train acc:  0.875
train loss:  0.2986474931240082
train gradient:  0.18067328384010786
iteration : 8209
train acc:  0.859375
train loss:  0.32719409465789795
train gradient:  0.18766275127409401
iteration : 8210
train acc:  0.828125
train loss:  0.38080650568008423
train gradient:  0.17135229662744778
iteration : 8211
train acc:  0.84375
train loss:  0.3605915904045105
train gradient:  0.183029970104545
iteration : 8212
train acc:  0.8359375
train loss:  0.35498422384262085
train gradient:  0.2420107476525205
iteration : 8213
train acc:  0.8671875
train loss:  0.30825239419937134
train gradient:  0.09992929952151879
iteration : 8214
train acc:  0.890625
train loss:  0.24781003594398499
train gradient:  0.09697343469368742
iteration : 8215
train acc:  0.875
train loss:  0.342092901468277
train gradient:  0.12430104558978879
iteration : 8216
train acc:  0.8359375
train loss:  0.357205867767334
train gradient:  0.19459581245589308
iteration : 8217
train acc:  0.890625
train loss:  0.3095977008342743
train gradient:  0.14409953502481454
iteration : 8218
train acc:  0.84375
train loss:  0.35404229164123535
train gradient:  0.22879641143247692
iteration : 8219
train acc:  0.875
train loss:  0.30271607637405396
train gradient:  0.1611234891967251
iteration : 8220
train acc:  0.859375
train loss:  0.3486095666885376
train gradient:  0.18417857271958693
iteration : 8221
train acc:  0.84375
train loss:  0.3727847933769226
train gradient:  0.2264549567789031
iteration : 8222
train acc:  0.8828125
train loss:  0.31048089265823364
train gradient:  0.255050024316909
iteration : 8223
train acc:  0.8671875
train loss:  0.2890241742134094
train gradient:  0.12221102830786222
iteration : 8224
train acc:  0.875
train loss:  0.32513782382011414
train gradient:  0.14314821897236243
iteration : 8225
train acc:  0.828125
train loss:  0.3698744475841522
train gradient:  0.18917823887424184
iteration : 8226
train acc:  0.8359375
train loss:  0.3478870689868927
train gradient:  0.1562747746120962
iteration : 8227
train acc:  0.8359375
train loss:  0.3215917944908142
train gradient:  0.2020296677307853
iteration : 8228
train acc:  0.8828125
train loss:  0.2604040503501892
train gradient:  0.12654464268241777
iteration : 8229
train acc:  0.921875
train loss:  0.24026313424110413
train gradient:  0.11229403250556928
iteration : 8230
train acc:  0.8125
train loss:  0.36665016412734985
train gradient:  0.20247494492703833
iteration : 8231
train acc:  0.84375
train loss:  0.3293721079826355
train gradient:  0.16639608034386155
iteration : 8232
train acc:  0.8671875
train loss:  0.34214550256729126
train gradient:  0.19082439734863577
iteration : 8233
train acc:  0.84375
train loss:  0.29462021589279175
train gradient:  0.11852620051554509
iteration : 8234
train acc:  0.7890625
train loss:  0.4213317632675171
train gradient:  0.27102969935702304
iteration : 8235
train acc:  0.8203125
train loss:  0.35880863666534424
train gradient:  0.1341683668238211
iteration : 8236
train acc:  0.859375
train loss:  0.2946701645851135
train gradient:  0.15958394878633597
iteration : 8237
train acc:  0.796875
train loss:  0.46784940361976624
train gradient:  0.32579813244228584
iteration : 8238
train acc:  0.8828125
train loss:  0.28685274720191956
train gradient:  0.1396288978998776
iteration : 8239
train acc:  0.8671875
train loss:  0.31341618299484253
train gradient:  0.13789858710435343
iteration : 8240
train acc:  0.796875
train loss:  0.40656769275665283
train gradient:  0.222441509945043
iteration : 8241
train acc:  0.875
train loss:  0.32709306478500366
train gradient:  0.16440721741862008
iteration : 8242
train acc:  0.859375
train loss:  0.30311498045921326
train gradient:  0.18125841229923728
iteration : 8243
train acc:  0.875
train loss:  0.2800155282020569
train gradient:  0.18401978152988657
iteration : 8244
train acc:  0.828125
train loss:  0.3570440411567688
train gradient:  0.23468859222023342
iteration : 8245
train acc:  0.8984375
train loss:  0.302858829498291
train gradient:  0.1333567225793275
iteration : 8246
train acc:  0.84375
train loss:  0.35620319843292236
train gradient:  0.23124692713313005
iteration : 8247
train acc:  0.8984375
train loss:  0.28686612844467163
train gradient:  0.12926034104962825
iteration : 8248
train acc:  0.875
train loss:  0.3098048269748688
train gradient:  0.1560296711139591
iteration : 8249
train acc:  0.859375
train loss:  0.34345242381095886
train gradient:  0.1373893839601314
iteration : 8250
train acc:  0.8046875
train loss:  0.36023926734924316
train gradient:  0.22994133871650219
iteration : 8251
train acc:  0.828125
train loss:  0.357103168964386
train gradient:  0.1903871803273163
iteration : 8252
train acc:  0.875
train loss:  0.329356849193573
train gradient:  0.2092087117617951
iteration : 8253
train acc:  0.828125
train loss:  0.35963666439056396
train gradient:  0.17950917444284986
iteration : 8254
train acc:  0.8359375
train loss:  0.38232260942459106
train gradient:  0.18318792176125578
iteration : 8255
train acc:  0.8125
train loss:  0.3717404007911682
train gradient:  0.21763612721426945
iteration : 8256
train acc:  0.8828125
train loss:  0.31182920932769775
train gradient:  0.22868951587368427
iteration : 8257
train acc:  0.828125
train loss:  0.36630910634994507
train gradient:  0.18280585403876504
iteration : 8258
train acc:  0.8515625
train loss:  0.32073867321014404
train gradient:  0.17358543892565692
iteration : 8259
train acc:  0.859375
train loss:  0.32482844591140747
train gradient:  0.17625490709463681
iteration : 8260
train acc:  0.828125
train loss:  0.3198970556259155
train gradient:  0.12360792886020411
iteration : 8261
train acc:  0.859375
train loss:  0.3531840443611145
train gradient:  0.18183402032321505
iteration : 8262
train acc:  0.8828125
train loss:  0.3291897773742676
train gradient:  0.13376211076196576
iteration : 8263
train acc:  0.7890625
train loss:  0.3661344051361084
train gradient:  0.22354295791408332
iteration : 8264
train acc:  0.9140625
train loss:  0.22005563974380493
train gradient:  0.0864611544771433
iteration : 8265
train acc:  0.90625
train loss:  0.26837727427482605
train gradient:  0.11016739029447592
iteration : 8266
train acc:  0.875
train loss:  0.2769955098628998
train gradient:  0.1418084409362477
iteration : 8267
train acc:  0.8671875
train loss:  0.30999329686164856
train gradient:  0.1676022460522761
iteration : 8268
train acc:  0.8515625
train loss:  0.32194966077804565
train gradient:  0.12545855802036668
iteration : 8269
train acc:  0.8125
train loss:  0.41377121210098267
train gradient:  0.27796420745607514
iteration : 8270
train acc:  0.796875
train loss:  0.4129146933555603
train gradient:  0.21546940281692345
iteration : 8271
train acc:  0.859375
train loss:  0.29686275124549866
train gradient:  0.17953729300148866
iteration : 8272
train acc:  0.8515625
train loss:  0.359142541885376
train gradient:  0.23380802121516459
iteration : 8273
train acc:  0.84375
train loss:  0.4037455916404724
train gradient:  0.2327551668118502
iteration : 8274
train acc:  0.8671875
train loss:  0.3108862638473511
train gradient:  0.13717963602681132
iteration : 8275
train acc:  0.8671875
train loss:  0.3470713496208191
train gradient:  0.16497271542364966
iteration : 8276
train acc:  0.84375
train loss:  0.3556528687477112
train gradient:  0.23816171229437283
iteration : 8277
train acc:  0.890625
train loss:  0.2876615524291992
train gradient:  0.2644238256741285
iteration : 8278
train acc:  0.890625
train loss:  0.32368558645248413
train gradient:  0.1542065475796176
iteration : 8279
train acc:  0.84375
train loss:  0.3552068769931793
train gradient:  0.3026602282234681
iteration : 8280
train acc:  0.8046875
train loss:  0.3916895091533661
train gradient:  0.209965105092355
iteration : 8281
train acc:  0.84375
train loss:  0.3267028331756592
train gradient:  0.18098336344002058
iteration : 8282
train acc:  0.890625
train loss:  0.28145918250083923
train gradient:  0.19073407107168258
iteration : 8283
train acc:  0.8671875
train loss:  0.33531269431114197
train gradient:  0.16124755056009338
iteration : 8284
train acc:  0.828125
train loss:  0.3466719686985016
train gradient:  0.15652986358981794
iteration : 8285
train acc:  0.828125
train loss:  0.3487279713153839
train gradient:  0.2224451307130379
iteration : 8286
train acc:  0.8828125
train loss:  0.3066398501396179
train gradient:  0.11166520541532207
iteration : 8287
train acc:  0.875
train loss:  0.3375198245048523
train gradient:  0.16333664521348976
iteration : 8288
train acc:  0.8046875
train loss:  0.4535004794597626
train gradient:  0.2412480888854397
iteration : 8289
train acc:  0.8515625
train loss:  0.26797470450401306
train gradient:  0.12199112702716788
iteration : 8290
train acc:  0.859375
train loss:  0.28751081228256226
train gradient:  0.15427413836496
iteration : 8291
train acc:  0.859375
train loss:  0.32576096057891846
train gradient:  0.1847235861105409
iteration : 8292
train acc:  0.859375
train loss:  0.3565274775028229
train gradient:  0.20463031274861332
iteration : 8293
train acc:  0.875
train loss:  0.3606085181236267
train gradient:  0.24162314173425847
iteration : 8294
train acc:  0.8515625
train loss:  0.3377140164375305
train gradient:  0.16739219425231278
iteration : 8295
train acc:  0.859375
train loss:  0.30504080653190613
train gradient:  0.17919096382282124
iteration : 8296
train acc:  0.8515625
train loss:  0.29764705896377563
train gradient:  0.16071357862866753
iteration : 8297
train acc:  0.8203125
train loss:  0.33058759570121765
train gradient:  0.1487560154152173
iteration : 8298
train acc:  0.859375
train loss:  0.3163253664970398
train gradient:  0.1676510503485146
iteration : 8299
train acc:  0.8671875
train loss:  0.3569994568824768
train gradient:  0.22209088399204469
iteration : 8300
train acc:  0.8671875
train loss:  0.302249014377594
train gradient:  0.15493794970024144
iteration : 8301
train acc:  0.875
train loss:  0.2755352258682251
train gradient:  0.12870803980401618
iteration : 8302
train acc:  0.828125
train loss:  0.4131776690483093
train gradient:  0.2381599626398002
iteration : 8303
train acc:  0.8984375
train loss:  0.2888999879360199
train gradient:  0.12573054739628808
iteration : 8304
train acc:  0.8671875
train loss:  0.3075158894062042
train gradient:  0.1875615393603951
iteration : 8305
train acc:  0.84375
train loss:  0.3190760910511017
train gradient:  0.15992340904099034
iteration : 8306
train acc:  0.796875
train loss:  0.4298816919326782
train gradient:  0.3174844668481527
iteration : 8307
train acc:  0.8203125
train loss:  0.354118287563324
train gradient:  0.18667683834018506
iteration : 8308
train acc:  0.8671875
train loss:  0.4235239028930664
train gradient:  0.2520212605851019
iteration : 8309
train acc:  0.8828125
train loss:  0.2850659489631653
train gradient:  0.1355080373373702
iteration : 8310
train acc:  0.8515625
train loss:  0.2791158854961395
train gradient:  0.13237705223936935
iteration : 8311
train acc:  0.875
train loss:  0.31077784299850464
train gradient:  0.20475242952986358
iteration : 8312
train acc:  0.8515625
train loss:  0.36364132165908813
train gradient:  0.2090942056582504
iteration : 8313
train acc:  0.796875
train loss:  0.37965893745422363
train gradient:  0.24593666005349535
iteration : 8314
train acc:  0.84375
train loss:  0.29737797379493713
train gradient:  0.15068411771993168
iteration : 8315
train acc:  0.8125
train loss:  0.3867627680301666
train gradient:  0.2811799733001857
iteration : 8316
train acc:  0.8359375
train loss:  0.39514923095703125
train gradient:  0.19064413991492238
iteration : 8317
train acc:  0.8359375
train loss:  0.3503910303115845
train gradient:  0.160049643402642
iteration : 8318
train acc:  0.8125
train loss:  0.4645668566226959
train gradient:  0.35434417852820604
iteration : 8319
train acc:  0.8828125
train loss:  0.26422131061553955
train gradient:  0.10864819658703966
iteration : 8320
train acc:  0.8671875
train loss:  0.418560653924942
train gradient:  0.24664064649271197
iteration : 8321
train acc:  0.8359375
train loss:  0.3163869082927704
train gradient:  0.1756799373588539
iteration : 8322
train acc:  0.8515625
train loss:  0.32708680629730225
train gradient:  0.1434055431315296
iteration : 8323
train acc:  0.890625
train loss:  0.37188899517059326
train gradient:  0.17235082971365062
iteration : 8324
train acc:  0.8515625
train loss:  0.3464263677597046
train gradient:  0.20484757217358718
iteration : 8325
train acc:  0.90625
train loss:  0.2991469204425812
train gradient:  0.1537613826735052
iteration : 8326
train acc:  0.765625
train loss:  0.4262734651565552
train gradient:  0.34204627199837273
iteration : 8327
train acc:  0.8515625
train loss:  0.39157968759536743
train gradient:  0.29932780039468354
iteration : 8328
train acc:  0.8671875
train loss:  0.32579755783081055
train gradient:  0.1506074392468986
iteration : 8329
train acc:  0.828125
train loss:  0.39020681381225586
train gradient:  0.23834856357480727
iteration : 8330
train acc:  0.8203125
train loss:  0.32552987337112427
train gradient:  0.12883073274734713
iteration : 8331
train acc:  0.9140625
train loss:  0.28166863322257996
train gradient:  0.08682206204000999
iteration : 8332
train acc:  0.8203125
train loss:  0.3973633050918579
train gradient:  0.20480045840199035
iteration : 8333
train acc:  0.875
train loss:  0.2905580699443817
train gradient:  0.1963535183642808
iteration : 8334
train acc:  0.8359375
train loss:  0.3402490019798279
train gradient:  0.1812035762301029
iteration : 8335
train acc:  0.875
train loss:  0.3249284029006958
train gradient:  0.17064346834149355
iteration : 8336
train acc:  0.8359375
train loss:  0.3614788353443146
train gradient:  0.18405141709558737
iteration : 8337
train acc:  0.8203125
train loss:  0.32503896951675415
train gradient:  0.2281267703529069
iteration : 8338
train acc:  0.8359375
train loss:  0.4155234694480896
train gradient:  0.21095503463761595
iteration : 8339
train acc:  0.8671875
train loss:  0.3724523186683655
train gradient:  0.2543164476171916
iteration : 8340
train acc:  0.8828125
train loss:  0.2653888463973999
train gradient:  0.15575790198790535
iteration : 8341
train acc:  0.84375
train loss:  0.4329146146774292
train gradient:  0.26876822033546044
iteration : 8342
train acc:  0.8984375
train loss:  0.24950262904167175
train gradient:  0.11034456351917381
iteration : 8343
train acc:  0.859375
train loss:  0.35501766204833984
train gradient:  0.1454845855384172
iteration : 8344
train acc:  0.8828125
train loss:  0.2781175374984741
train gradient:  0.12598740691288798
iteration : 8345
train acc:  0.8359375
train loss:  0.3960883617401123
train gradient:  0.25546903801157805
iteration : 8346
train acc:  0.875
train loss:  0.3340572118759155
train gradient:  0.14201483949662724
iteration : 8347
train acc:  0.8203125
train loss:  0.32994261384010315
train gradient:  0.15002156991882248
iteration : 8348
train acc:  0.8828125
train loss:  0.28664126992225647
train gradient:  0.1353649465027199
iteration : 8349
train acc:  0.859375
train loss:  0.32648003101348877
train gradient:  0.14377601061165424
iteration : 8350
train acc:  0.828125
train loss:  0.4148079752922058
train gradient:  0.2341636338696523
iteration : 8351
train acc:  0.8671875
train loss:  0.2887435555458069
train gradient:  0.11851766558154912
iteration : 8352
train acc:  0.8671875
train loss:  0.36006155610084534
train gradient:  0.2851275250236767
iteration : 8353
train acc:  0.8203125
train loss:  0.40928006172180176
train gradient:  0.23169536694787427
iteration : 8354
train acc:  0.8828125
train loss:  0.2816922068595886
train gradient:  0.10754024513909427
iteration : 8355
train acc:  0.8359375
train loss:  0.33860474824905396
train gradient:  0.1658672246949215
iteration : 8356
train acc:  0.859375
train loss:  0.35938987135887146
train gradient:  0.28623145124746796
iteration : 8357
train acc:  0.84375
train loss:  0.3420383036136627
train gradient:  0.21480274306039676
iteration : 8358
train acc:  0.8828125
train loss:  0.3202034831047058
train gradient:  0.14081627579492512
iteration : 8359
train acc:  0.78125
train loss:  0.3734600841999054
train gradient:  0.1761983609208748
iteration : 8360
train acc:  0.828125
train loss:  0.3793490529060364
train gradient:  0.188647272511684
iteration : 8361
train acc:  0.8671875
train loss:  0.302905797958374
train gradient:  0.1267288797556958
iteration : 8362
train acc:  0.84375
train loss:  0.3231147229671478
train gradient:  0.17393098272527419
iteration : 8363
train acc:  0.8671875
train loss:  0.2892746329307556
train gradient:  0.3057555815813634
iteration : 8364
train acc:  0.7734375
train loss:  0.40179234743118286
train gradient:  0.17370849137013072
iteration : 8365
train acc:  0.90625
train loss:  0.23961612582206726
train gradient:  0.13235486575539296
iteration : 8366
train acc:  0.7890625
train loss:  0.4254336357116699
train gradient:  0.19712698063793044
iteration : 8367
train acc:  0.921875
train loss:  0.24132221937179565
train gradient:  0.11260527544738168
iteration : 8368
train acc:  0.828125
train loss:  0.360917866230011
train gradient:  0.21949530814615548
iteration : 8369
train acc:  0.890625
train loss:  0.2943321466445923
train gradient:  0.11755572753791496
iteration : 8370
train acc:  0.8671875
train loss:  0.28606468439102173
train gradient:  0.14279890380545532
iteration : 8371
train acc:  0.8984375
train loss:  0.3398841917514801
train gradient:  0.16471284951678705
iteration : 8372
train acc:  0.8828125
train loss:  0.26568400859832764
train gradient:  0.16225916499087256
iteration : 8373
train acc:  0.84375
train loss:  0.41853001713752747
train gradient:  0.22402706931303124
iteration : 8374
train acc:  0.90625
train loss:  0.25016382336616516
train gradient:  0.12545785833897877
iteration : 8375
train acc:  0.8203125
train loss:  0.5224823951721191
train gradient:  0.3637781601562871
iteration : 8376
train acc:  0.859375
train loss:  0.2731620669364929
train gradient:  0.11423977899966203
iteration : 8377
train acc:  0.8671875
train loss:  0.3421352803707123
train gradient:  0.15268476866368846
iteration : 8378
train acc:  0.8515625
train loss:  0.30954089760780334
train gradient:  0.19649232525150134
iteration : 8379
train acc:  0.8515625
train loss:  0.3568381369113922
train gradient:  0.14848780152118882
iteration : 8380
train acc:  0.859375
train loss:  0.3488065004348755
train gradient:  0.28808947048733385
iteration : 8381
train acc:  0.84375
train loss:  0.3375919461250305
train gradient:  0.1294197057542331
iteration : 8382
train acc:  0.828125
train loss:  0.36283841729164124
train gradient:  0.18288830467775974
iteration : 8383
train acc:  0.890625
train loss:  0.280616819858551
train gradient:  0.16099307503192173
iteration : 8384
train acc:  0.875
train loss:  0.28128838539123535
train gradient:  0.12732375456122508
iteration : 8385
train acc:  0.8828125
train loss:  0.2597857117652893
train gradient:  0.21624633430452367
iteration : 8386
train acc:  0.8359375
train loss:  0.3787651062011719
train gradient:  0.16670389692396595
iteration : 8387
train acc:  0.8046875
train loss:  0.3647017180919647
train gradient:  0.2601825514466839
iteration : 8388
train acc:  0.8203125
train loss:  0.40413638949394226
train gradient:  0.28276953852570896
iteration : 8389
train acc:  0.8359375
train loss:  0.32685258984565735
train gradient:  0.16578629392233157
iteration : 8390
train acc:  0.84375
train loss:  0.36437466740608215
train gradient:  0.22883948994041958
iteration : 8391
train acc:  0.9140625
train loss:  0.21707966923713684
train gradient:  0.1195018951387149
iteration : 8392
train acc:  0.7578125
train loss:  0.4185791313648224
train gradient:  0.33917526453544494
iteration : 8393
train acc:  0.8984375
train loss:  0.29507535696029663
train gradient:  0.12192579242728305
iteration : 8394
train acc:  0.8359375
train loss:  0.36047840118408203
train gradient:  0.15917448069605594
iteration : 8395
train acc:  0.8203125
train loss:  0.38223183155059814
train gradient:  0.2735509231705851
iteration : 8396
train acc:  0.8671875
train loss:  0.3211915194988251
train gradient:  0.165870230295509
iteration : 8397
train acc:  0.84375
train loss:  0.3519355356693268
train gradient:  0.19602919709122543
iteration : 8398
train acc:  0.8671875
train loss:  0.3495211601257324
train gradient:  0.1711451532544449
iteration : 8399
train acc:  0.7890625
train loss:  0.38225114345550537
train gradient:  0.24752321362214896
iteration : 8400
train acc:  0.8828125
train loss:  0.26467955112457275
train gradient:  0.1608865597074432
iteration : 8401
train acc:  0.796875
train loss:  0.41246166825294495
train gradient:  0.30462635833381374
iteration : 8402
train acc:  0.8671875
train loss:  0.29505079984664917
train gradient:  0.1398161714152769
iteration : 8403
train acc:  0.8984375
train loss:  0.246392622590065
train gradient:  0.11223522708809135
iteration : 8404
train acc:  0.8671875
train loss:  0.31986454129219055
train gradient:  0.20115436623350968
iteration : 8405
train acc:  0.859375
train loss:  0.3244902491569519
train gradient:  0.16943366506537394
iteration : 8406
train acc:  0.8203125
train loss:  0.43212276697158813
train gradient:  0.27563558040409564
iteration : 8407
train acc:  0.8515625
train loss:  0.36259645223617554
train gradient:  0.2127233883567275
iteration : 8408
train acc:  0.875
train loss:  0.31118565797805786
train gradient:  0.15417124113374608
iteration : 8409
train acc:  0.8046875
train loss:  0.37237054109573364
train gradient:  0.2459567511114027
iteration : 8410
train acc:  0.8828125
train loss:  0.27213236689567566
train gradient:  0.12498514576974905
iteration : 8411
train acc:  0.890625
train loss:  0.3398318588733673
train gradient:  0.1756321703875886
iteration : 8412
train acc:  0.8515625
train loss:  0.33695104718208313
train gradient:  0.269675933711827
iteration : 8413
train acc:  0.8203125
train loss:  0.3493039608001709
train gradient:  0.15141743532633242
iteration : 8414
train acc:  0.7890625
train loss:  0.395414263010025
train gradient:  0.24328341211251664
iteration : 8415
train acc:  0.8828125
train loss:  0.28472205996513367
train gradient:  0.15694200606123565
iteration : 8416
train acc:  0.8671875
train loss:  0.3175272047519684
train gradient:  0.16789342580694272
iteration : 8417
train acc:  0.828125
train loss:  0.3036006987094879
train gradient:  0.1218275682445882
iteration : 8418
train acc:  0.796875
train loss:  0.37253299355506897
train gradient:  0.17045712639555385
iteration : 8419
train acc:  0.8046875
train loss:  0.43585190176963806
train gradient:  0.29774534874913205
iteration : 8420
train acc:  0.875
train loss:  0.3841423988342285
train gradient:  0.22521653292598243
iteration : 8421
train acc:  0.828125
train loss:  0.4170193672180176
train gradient:  0.26606497871156004
iteration : 8422
train acc:  0.84375
train loss:  0.3420208692550659
train gradient:  0.24577290372190652
iteration : 8423
train acc:  0.828125
train loss:  0.35353922843933105
train gradient:  0.2363377028114791
iteration : 8424
train acc:  0.875
train loss:  0.28934699296951294
train gradient:  0.1331052665721713
iteration : 8425
train acc:  0.859375
train loss:  0.3445618450641632
train gradient:  0.15977343499415717
iteration : 8426
train acc:  0.8515625
train loss:  0.3593869209289551
train gradient:  0.20158332109329635
iteration : 8427
train acc:  0.8828125
train loss:  0.2689661383628845
train gradient:  0.11047549144166553
iteration : 8428
train acc:  0.8515625
train loss:  0.3554721474647522
train gradient:  0.19426646511804085
iteration : 8429
train acc:  0.8984375
train loss:  0.25686290860176086
train gradient:  0.09030352410724544
iteration : 8430
train acc:  0.890625
train loss:  0.25413280725479126
train gradient:  0.11238701412910457
iteration : 8431
train acc:  0.890625
train loss:  0.36135441064834595
train gradient:  0.17052198984831157
iteration : 8432
train acc:  0.796875
train loss:  0.40126582980155945
train gradient:  0.33836608699516496
iteration : 8433
train acc:  0.875
train loss:  0.30916523933410645
train gradient:  0.19517500888750322
iteration : 8434
train acc:  0.8984375
train loss:  0.2941426634788513
train gradient:  0.129937549343978
iteration : 8435
train acc:  0.8984375
train loss:  0.29804158210754395
train gradient:  0.14669050176567627
iteration : 8436
train acc:  0.8828125
train loss:  0.3813965320587158
train gradient:  0.21030032837946772
iteration : 8437
train acc:  0.859375
train loss:  0.27692192792892456
train gradient:  0.12197509150444777
iteration : 8438
train acc:  0.8984375
train loss:  0.28696325421333313
train gradient:  0.10197014333913229
iteration : 8439
train acc:  0.8359375
train loss:  0.36641567945480347
train gradient:  0.1561574789847031
iteration : 8440
train acc:  0.8515625
train loss:  0.3318544030189514
train gradient:  0.15172706100262925
iteration : 8441
train acc:  0.796875
train loss:  0.44872817397117615
train gradient:  0.2892494399905235
iteration : 8442
train acc:  0.8515625
train loss:  0.4137076437473297
train gradient:  0.1939855480410638
iteration : 8443
train acc:  0.8359375
train loss:  0.40261751413345337
train gradient:  0.3287458008455238
iteration : 8444
train acc:  0.8125
train loss:  0.376416951417923
train gradient:  0.20401765576709627
iteration : 8445
train acc:  0.875
train loss:  0.3027905821800232
train gradient:  0.14222337056319115
iteration : 8446
train acc:  0.7890625
train loss:  0.497389018535614
train gradient:  0.28922578045759983
iteration : 8447
train acc:  0.8671875
train loss:  0.3267456293106079
train gradient:  0.19511319075069716
iteration : 8448
train acc:  0.8359375
train loss:  0.38828951120376587
train gradient:  0.2398915140879199
iteration : 8449
train acc:  0.859375
train loss:  0.3463636636734009
train gradient:  0.21132406194387574
iteration : 8450
train acc:  0.8671875
train loss:  0.30115944147109985
train gradient:  0.11288853525274156
iteration : 8451
train acc:  0.859375
train loss:  0.2932699918746948
train gradient:  0.14801855422812726
iteration : 8452
train acc:  0.828125
train loss:  0.36224326491355896
train gradient:  0.17409001493794593
iteration : 8453
train acc:  0.875
train loss:  0.2957761883735657
train gradient:  0.15347750878143712
iteration : 8454
train acc:  0.875
train loss:  0.27500808238983154
train gradient:  0.11486495021124897
iteration : 8455
train acc:  0.8359375
train loss:  0.3541644215583801
train gradient:  0.15062819620505086
iteration : 8456
train acc:  0.8203125
train loss:  0.36345145106315613
train gradient:  0.16079490191438764
iteration : 8457
train acc:  0.8203125
train loss:  0.40411829948425293
train gradient:  0.1777143385078593
iteration : 8458
train acc:  0.8515625
train loss:  0.3168991506099701
train gradient:  0.1727546597604282
iteration : 8459
train acc:  0.8359375
train loss:  0.3115118145942688
train gradient:  0.12152958542597166
iteration : 8460
train acc:  0.8203125
train loss:  0.3605198562145233
train gradient:  0.1492529554952794
iteration : 8461
train acc:  0.890625
train loss:  0.2630999982357025
train gradient:  0.09981513488441224
iteration : 8462
train acc:  0.8515625
train loss:  0.31653106212615967
train gradient:  0.13337044725516495
iteration : 8463
train acc:  0.8828125
train loss:  0.3377302289009094
train gradient:  0.22235616724363316
iteration : 8464
train acc:  0.8671875
train loss:  0.3135942220687866
train gradient:  0.124893645751767
iteration : 8465
train acc:  0.8203125
train loss:  0.3400028347969055
train gradient:  0.1593028928959684
iteration : 8466
train acc:  0.875
train loss:  0.3086469769477844
train gradient:  0.17594449870110576
iteration : 8467
train acc:  0.8828125
train loss:  0.2832465171813965
train gradient:  0.16879391502246
iteration : 8468
train acc:  0.8671875
train loss:  0.3339115381240845
train gradient:  0.18032264142817206
iteration : 8469
train acc:  0.859375
train loss:  0.34282535314559937
train gradient:  0.18296269496775666
iteration : 8470
train acc:  0.875
train loss:  0.3069537580013275
train gradient:  0.14117709033137682
iteration : 8471
train acc:  0.8125
train loss:  0.4249965250492096
train gradient:  0.2548314885399783
iteration : 8472
train acc:  0.84375
train loss:  0.3112024664878845
train gradient:  0.15122825993011602
iteration : 8473
train acc:  0.8828125
train loss:  0.2857389748096466
train gradient:  0.11391225913061467
iteration : 8474
train acc:  0.8046875
train loss:  0.3777279853820801
train gradient:  0.1576729192535023
iteration : 8475
train acc:  0.8515625
train loss:  0.2931855022907257
train gradient:  0.15161957860035552
iteration : 8476
train acc:  0.9296875
train loss:  0.2495596706867218
train gradient:  0.10351202498778833
iteration : 8477
train acc:  0.8984375
train loss:  0.29222744703292847
train gradient:  0.1422733592811707
iteration : 8478
train acc:  0.8515625
train loss:  0.30904895067214966
train gradient:  0.16845931326480446
iteration : 8479
train acc:  0.828125
train loss:  0.4448535144329071
train gradient:  0.2609959531661243
iteration : 8480
train acc:  0.875
train loss:  0.27254438400268555
train gradient:  0.1179218816749293
iteration : 8481
train acc:  0.84375
train loss:  0.3359070420265198
train gradient:  0.1716330609079819
iteration : 8482
train acc:  0.8046875
train loss:  0.4758787155151367
train gradient:  0.2934350792827136
iteration : 8483
train acc:  0.859375
train loss:  0.31414157152175903
train gradient:  0.2644993292264255
iteration : 8484
train acc:  0.828125
train loss:  0.36012157797813416
train gradient:  0.1905461162638046
iteration : 8485
train acc:  0.8515625
train loss:  0.35118362307548523
train gradient:  0.17564750308985055
iteration : 8486
train acc:  0.8515625
train loss:  0.30863603949546814
train gradient:  0.1515008981153424
iteration : 8487
train acc:  0.8515625
train loss:  0.3815644383430481
train gradient:  0.18652587762393877
iteration : 8488
train acc:  0.859375
train loss:  0.29634642601013184
train gradient:  0.15112703940666017
iteration : 8489
train acc:  0.875
train loss:  0.3133818507194519
train gradient:  0.20024639817100037
iteration : 8490
train acc:  0.8984375
train loss:  0.33593660593032837
train gradient:  0.15207760940129317
iteration : 8491
train acc:  0.828125
train loss:  0.30401402711868286
train gradient:  0.11663578164101858
iteration : 8492
train acc:  0.8671875
train loss:  0.3162710964679718
train gradient:  0.15048372427168485
iteration : 8493
train acc:  0.90625
train loss:  0.28734463453292847
train gradient:  0.07988232075216675
iteration : 8494
train acc:  0.8828125
train loss:  0.3322237730026245
train gradient:  0.21289014530398984
iteration : 8495
train acc:  0.796875
train loss:  0.43969714641571045
train gradient:  0.24873492859668467
iteration : 8496
train acc:  0.859375
train loss:  0.3042769432067871
train gradient:  0.13116508608613622
iteration : 8497
train acc:  0.8828125
train loss:  0.305754691362381
train gradient:  0.12158707974676014
iteration : 8498
train acc:  0.9140625
train loss:  0.2124675065279007
train gradient:  0.07832407571198986
iteration : 8499
train acc:  0.890625
train loss:  0.25336751341819763
train gradient:  0.11612544887277368
iteration : 8500
train acc:  0.8671875
train loss:  0.3373139500617981
train gradient:  0.1925512923567132
iteration : 8501
train acc:  0.828125
train loss:  0.34584149718284607
train gradient:  0.1479840901956387
iteration : 8502
train acc:  0.890625
train loss:  0.27861708402633667
train gradient:  0.12629221704923035
iteration : 8503
train acc:  0.9140625
train loss:  0.2531749904155731
train gradient:  0.11486249257924892
iteration : 8504
train acc:  0.828125
train loss:  0.37887781858444214
train gradient:  0.18560583600274935
iteration : 8505
train acc:  0.8671875
train loss:  0.3871878683567047
train gradient:  0.25451515545370795
iteration : 8506
train acc:  0.890625
train loss:  0.25993555784225464
train gradient:  0.11671189081216755
iteration : 8507
train acc:  0.8515625
train loss:  0.33225300908088684
train gradient:  0.18346487372527887
iteration : 8508
train acc:  0.8359375
train loss:  0.38968706130981445
train gradient:  0.1610607166609061
iteration : 8509
train acc:  0.8515625
train loss:  0.3473227024078369
train gradient:  0.17169745865202704
iteration : 8510
train acc:  0.875
train loss:  0.29622238874435425
train gradient:  0.16505656540969163
iteration : 8511
train acc:  0.859375
train loss:  0.36049121618270874
train gradient:  0.154035723140915
iteration : 8512
train acc:  0.84375
train loss:  0.3920658528804779
train gradient:  0.21878606489331742
iteration : 8513
train acc:  0.8828125
train loss:  0.338903546333313
train gradient:  0.1314458316665474
iteration : 8514
train acc:  0.8828125
train loss:  0.2997264862060547
train gradient:  0.1411909707291265
iteration : 8515
train acc:  0.8984375
train loss:  0.2705153226852417
train gradient:  0.10563543710954573
iteration : 8516
train acc:  0.8828125
train loss:  0.26471832394599915
train gradient:  0.13046603732685644
iteration : 8517
train acc:  0.859375
train loss:  0.29182928800582886
train gradient:  0.09104562742077925
iteration : 8518
train acc:  0.8203125
train loss:  0.4308587610721588
train gradient:  0.24401262618419017
iteration : 8519
train acc:  0.890625
train loss:  0.30655449628829956
train gradient:  0.12652218014105404
iteration : 8520
train acc:  0.859375
train loss:  0.3037121295928955
train gradient:  0.1353630113354781
iteration : 8521
train acc:  0.8828125
train loss:  0.34471455216407776
train gradient:  0.17481001144093544
iteration : 8522
train acc:  0.8671875
train loss:  0.27189722657203674
train gradient:  0.11376647387192411
iteration : 8523
train acc:  0.8359375
train loss:  0.3736252188682556
train gradient:  0.18695799507263036
iteration : 8524
train acc:  0.875
train loss:  0.2827794849872589
train gradient:  0.12586350339805025
iteration : 8525
train acc:  0.8046875
train loss:  0.3858996033668518
train gradient:  0.17900844879268576
iteration : 8526
train acc:  0.875
train loss:  0.38905271887779236
train gradient:  0.1977001194122671
iteration : 8527
train acc:  0.8984375
train loss:  0.3075641393661499
train gradient:  0.2451377800828437
iteration : 8528
train acc:  0.8984375
train loss:  0.2848728895187378
train gradient:  0.09619891802013159
iteration : 8529
train acc:  0.84375
train loss:  0.39010608196258545
train gradient:  0.1564021710132753
iteration : 8530
train acc:  0.84375
train loss:  0.36168116331100464
train gradient:  0.20846619632209606
iteration : 8531
train acc:  0.796875
train loss:  0.44294625520706177
train gradient:  0.2552165932016961
iteration : 8532
train acc:  0.8515625
train loss:  0.4124900698661804
train gradient:  0.2019156429891954
iteration : 8533
train acc:  0.8515625
train loss:  0.3520837426185608
train gradient:  0.2131742832875722
iteration : 8534
train acc:  0.828125
train loss:  0.3434307873249054
train gradient:  0.18304918069346782
iteration : 8535
train acc:  0.859375
train loss:  0.371443510055542
train gradient:  0.1819572542991861
iteration : 8536
train acc:  0.8203125
train loss:  0.3906092047691345
train gradient:  0.1954418408038034
iteration : 8537
train acc:  0.8125
train loss:  0.3565331697463989
train gradient:  0.15638537038260064
iteration : 8538
train acc:  0.9296875
train loss:  0.2746114134788513
train gradient:  0.1256483094188054
iteration : 8539
train acc:  0.859375
train loss:  0.299837589263916
train gradient:  0.14695030018673583
iteration : 8540
train acc:  0.8203125
train loss:  0.41193342208862305
train gradient:  0.18017433459640714
iteration : 8541
train acc:  0.7890625
train loss:  0.4150553345680237
train gradient:  0.25509384736675605
iteration : 8542
train acc:  0.8125
train loss:  0.35336175560951233
train gradient:  0.23802679089149129
iteration : 8543
train acc:  0.84375
train loss:  0.3590785562992096
train gradient:  0.1484310554660193
iteration : 8544
train acc:  0.8359375
train loss:  0.37161731719970703
train gradient:  0.26931100179781026
iteration : 8545
train acc:  0.875
train loss:  0.37177884578704834
train gradient:  3.5725342198049836
iteration : 8546
train acc:  0.8984375
train loss:  0.2745625972747803
train gradient:  0.08985138426818365
iteration : 8547
train acc:  0.8515625
train loss:  0.2912774980068207
train gradient:  0.1255239896458594
iteration : 8548
train acc:  0.828125
train loss:  0.32657650113105774
train gradient:  0.13566209681442315
iteration : 8549
train acc:  0.859375
train loss:  0.3048449158668518
train gradient:  0.14211568424540477
iteration : 8550
train acc:  0.9140625
train loss:  0.277333527803421
train gradient:  0.14195112545831004
iteration : 8551
train acc:  0.8359375
train loss:  0.35887467861175537
train gradient:  0.18963163387136067
iteration : 8552
train acc:  0.859375
train loss:  0.3676654100418091
train gradient:  0.14190554835463143
iteration : 8553
train acc:  0.8125
train loss:  0.4526069164276123
train gradient:  0.2887535741124404
iteration : 8554
train acc:  0.8203125
train loss:  0.37746429443359375
train gradient:  0.19345969194581486
iteration : 8555
train acc:  0.890625
train loss:  0.27281391620635986
train gradient:  0.1337873008696075
iteration : 8556
train acc:  0.859375
train loss:  0.3651365637779236
train gradient:  0.23719093107761807
iteration : 8557
train acc:  0.859375
train loss:  0.33489376306533813
train gradient:  0.1839656345353402
iteration : 8558
train acc:  0.8671875
train loss:  0.2944023907184601
train gradient:  0.12685427619768191
iteration : 8559
train acc:  0.84375
train loss:  0.3601527512073517
train gradient:  0.1828184044528594
iteration : 8560
train acc:  0.84375
train loss:  0.3933122158050537
train gradient:  0.18551670382449256
iteration : 8561
train acc:  0.921875
train loss:  0.2163277268409729
train gradient:  0.1060961195830396
iteration : 8562
train acc:  0.8359375
train loss:  0.3204460144042969
train gradient:  0.16154001798631037
iteration : 8563
train acc:  0.8359375
train loss:  0.31794989109039307
train gradient:  0.17251842020542008
iteration : 8564
train acc:  0.859375
train loss:  0.28882473707199097
train gradient:  0.19666649784311147
iteration : 8565
train acc:  0.875
train loss:  0.3193032741546631
train gradient:  0.1401419597002506
iteration : 8566
train acc:  0.8671875
train loss:  0.34997013211250305
train gradient:  0.186430497118153
iteration : 8567
train acc:  0.78125
train loss:  0.44296932220458984
train gradient:  0.2565169605855048
iteration : 8568
train acc:  0.8359375
train loss:  0.3700982928276062
train gradient:  0.17316291830040242
iteration : 8569
train acc:  0.828125
train loss:  0.4161255955696106
train gradient:  0.18222621537864572
iteration : 8570
train acc:  0.859375
train loss:  0.3005003333091736
train gradient:  0.10432863959699114
iteration : 8571
train acc:  0.8046875
train loss:  0.46319830417633057
train gradient:  0.32961190197259804
iteration : 8572
train acc:  0.8515625
train loss:  0.33819863200187683
train gradient:  0.16481527798544504
iteration : 8573
train acc:  0.8359375
train loss:  0.34002792835235596
train gradient:  0.16134869070109317
iteration : 8574
train acc:  0.859375
train loss:  0.37351781129837036
train gradient:  0.21967209456302578
iteration : 8575
train acc:  0.828125
train loss:  0.3661515712738037
train gradient:  0.21879119251186793
iteration : 8576
train acc:  0.8671875
train loss:  0.3041711747646332
train gradient:  0.17284672943645624
iteration : 8577
train acc:  0.8125
train loss:  0.33374807238578796
train gradient:  0.1319594526550359
iteration : 8578
train acc:  0.8828125
train loss:  0.34326374530792236
train gradient:  0.18778511851100538
iteration : 8579
train acc:  0.8828125
train loss:  0.33903926610946655
train gradient:  0.12705019983527943
iteration : 8580
train acc:  0.8828125
train loss:  0.33329716324806213
train gradient:  0.24159252983002316
iteration : 8581
train acc:  0.8359375
train loss:  0.33667635917663574
train gradient:  0.15923666197226585
iteration : 8582
train acc:  0.8671875
train loss:  0.363975465297699
train gradient:  0.19868671219812287
iteration : 8583
train acc:  0.8359375
train loss:  0.3874450922012329
train gradient:  0.2272487277720165
iteration : 8584
train acc:  0.828125
train loss:  0.32946422696113586
train gradient:  0.1522080111957314
iteration : 8585
train acc:  0.8125
train loss:  0.35019031167030334
train gradient:  0.19604824025633721
iteration : 8586
train acc:  0.84375
train loss:  0.32340097427368164
train gradient:  0.17438686302646075
iteration : 8587
train acc:  0.8671875
train loss:  0.2684534192085266
train gradient:  0.1440571524739056
iteration : 8588
train acc:  0.890625
train loss:  0.2561887502670288
train gradient:  0.06709268395131557
iteration : 8589
train acc:  0.8203125
train loss:  0.35519394278526306
train gradient:  0.2169477415429233
iteration : 8590
train acc:  0.859375
train loss:  0.33088141679763794
train gradient:  0.1569450092782305
iteration : 8591
train acc:  0.7578125
train loss:  0.4796542525291443
train gradient:  0.29101237751619563
iteration : 8592
train acc:  0.8671875
train loss:  0.2852112650871277
train gradient:  0.14795734402011484
iteration : 8593
train acc:  0.828125
train loss:  0.35085436701774597
train gradient:  0.12863985754057228
iteration : 8594
train acc:  0.859375
train loss:  0.3246757388114929
train gradient:  0.14717112824690476
iteration : 8595
train acc:  0.8046875
train loss:  0.3790067434310913
train gradient:  0.2131646642379887
iteration : 8596
train acc:  0.8515625
train loss:  0.313613623380661
train gradient:  0.16508447517215813
iteration : 8597
train acc:  0.8515625
train loss:  0.3402904272079468
train gradient:  0.208649021191131
iteration : 8598
train acc:  0.859375
train loss:  0.2850359082221985
train gradient:  0.1071617373225861
iteration : 8599
train acc:  0.890625
train loss:  0.26130595803260803
train gradient:  0.09919274660942176
iteration : 8600
train acc:  0.875
train loss:  0.2725400924682617
train gradient:  0.11727810669876619
iteration : 8601
train acc:  0.8515625
train loss:  0.33524835109710693
train gradient:  0.1461575914204552
iteration : 8602
train acc:  0.890625
train loss:  0.3172166645526886
train gradient:  0.1353959518597543
iteration : 8603
train acc:  0.8828125
train loss:  0.3512090742588043
train gradient:  0.17094513837075123
iteration : 8604
train acc:  0.8515625
train loss:  0.389395534992218
train gradient:  0.1710793358345481
iteration : 8605
train acc:  0.8515625
train loss:  0.3737373948097229
train gradient:  0.27132936140694985
iteration : 8606
train acc:  0.8359375
train loss:  0.4161081314086914
train gradient:  0.19619576620103965
iteration : 8607
train acc:  0.859375
train loss:  0.3408774733543396
train gradient:  0.1731109544767527
iteration : 8608
train acc:  0.875
train loss:  0.3130991458892822
train gradient:  0.16103577239169925
iteration : 8609
train acc:  0.8046875
train loss:  0.38588419556617737
train gradient:  0.17112234880473148
iteration : 8610
train acc:  0.8671875
train loss:  0.3308066129684448
train gradient:  0.1351960672009909
iteration : 8611
train acc:  0.828125
train loss:  0.3563454747200012
train gradient:  0.18702660991056796
iteration : 8612
train acc:  0.828125
train loss:  0.3508390188217163
train gradient:  0.20549496368442968
iteration : 8613
train acc:  0.8984375
train loss:  0.24925929307937622
train gradient:  0.11583501367138503
iteration : 8614
train acc:  0.8359375
train loss:  0.3534191846847534
train gradient:  0.1887088792203811
iteration : 8615
train acc:  0.8828125
train loss:  0.29683375358581543
train gradient:  0.184628759890615
iteration : 8616
train acc:  0.828125
train loss:  0.3429960012435913
train gradient:  0.13958178190220272
iteration : 8617
train acc:  0.859375
train loss:  0.30885154008865356
train gradient:  0.16506095826427458
iteration : 8618
train acc:  0.875
train loss:  0.2959650158882141
train gradient:  0.1363230895012118
iteration : 8619
train acc:  0.8671875
train loss:  0.31419914960861206
train gradient:  0.12251451042212835
iteration : 8620
train acc:  0.9375
train loss:  0.20785194635391235
train gradient:  0.09743151650037908
iteration : 8621
train acc:  0.9296875
train loss:  0.23509500920772552
train gradient:  0.07267177724626693
iteration : 8622
train acc:  0.8515625
train loss:  0.32529765367507935
train gradient:  0.14937266948271247
iteration : 8623
train acc:  0.84375
train loss:  0.40739792585372925
train gradient:  0.23630504733405036
iteration : 8624
train acc:  0.9296875
train loss:  0.23476579785346985
train gradient:  0.07678166592262019
iteration : 8625
train acc:  0.828125
train loss:  0.3550764322280884
train gradient:  0.28463645629356993
iteration : 8626
train acc:  0.875
train loss:  0.3065906763076782
train gradient:  0.16170352681306552
iteration : 8627
train acc:  0.8828125
train loss:  0.3021281957626343
train gradient:  0.12684444179214913
iteration : 8628
train acc:  0.84375
train loss:  0.37569108605384827
train gradient:  0.18144062711957126
iteration : 8629
train acc:  0.796875
train loss:  0.43126681447029114
train gradient:  0.30238754466270507
iteration : 8630
train acc:  0.8125
train loss:  0.405807763338089
train gradient:  0.28779191159750983
iteration : 8631
train acc:  0.890625
train loss:  0.31909534335136414
train gradient:  0.14051633389113083
iteration : 8632
train acc:  0.828125
train loss:  0.4250088036060333
train gradient:  0.22291018174381272
iteration : 8633
train acc:  0.90625
train loss:  0.26646557450294495
train gradient:  0.14431597544602665
iteration : 8634
train acc:  0.8125
train loss:  0.41611960530281067
train gradient:  0.3989543657072464
iteration : 8635
train acc:  0.8359375
train loss:  0.3594817519187927
train gradient:  0.15354401977083776
iteration : 8636
train acc:  0.8828125
train loss:  0.30544912815093994
train gradient:  0.11529370498064143
iteration : 8637
train acc:  0.8359375
train loss:  0.305923193693161
train gradient:  0.12924688985723626
iteration : 8638
train acc:  0.8828125
train loss:  0.2749694287776947
train gradient:  0.14074131157322756
iteration : 8639
train acc:  0.8828125
train loss:  0.2754373550415039
train gradient:  0.09283447345061828
iteration : 8640
train acc:  0.8125
train loss:  0.4021884799003601
train gradient:  0.2721305346987684
iteration : 8641
train acc:  0.828125
train loss:  0.3195984363555908
train gradient:  0.14062554199108854
iteration : 8642
train acc:  0.84375
train loss:  0.3665879964828491
train gradient:  0.185891053116788
iteration : 8643
train acc:  0.84375
train loss:  0.3719051480293274
train gradient:  0.16442287731021846
iteration : 8644
train acc:  0.921875
train loss:  0.23705199360847473
train gradient:  0.10160915776120988
iteration : 8645
train acc:  0.7734375
train loss:  0.45495378971099854
train gradient:  0.2812217309197437
iteration : 8646
train acc:  0.8515625
train loss:  0.3772278130054474
train gradient:  0.2566307055742532
iteration : 8647
train acc:  0.8203125
train loss:  0.3644563555717468
train gradient:  0.26530146553126976
iteration : 8648
train acc:  0.8671875
train loss:  0.2808729410171509
train gradient:  0.12175988306806151
iteration : 8649
train acc:  0.90625
train loss:  0.2915263772010803
train gradient:  0.14389478412534096
iteration : 8650
train acc:  0.8828125
train loss:  0.23656922578811646
train gradient:  0.1178196910764324
iteration : 8651
train acc:  0.84375
train loss:  0.3119061589241028
train gradient:  0.129544423895818
iteration : 8652
train acc:  0.9140625
train loss:  0.281803697347641
train gradient:  0.1753196964319611
iteration : 8653
train acc:  0.859375
train loss:  0.304999440908432
train gradient:  0.25898736847671633
iteration : 8654
train acc:  0.8203125
train loss:  0.3393997848033905
train gradient:  0.19719725489553808
iteration : 8655
train acc:  0.9375
train loss:  0.2499592900276184
train gradient:  0.14434351878919804
iteration : 8656
train acc:  0.8984375
train loss:  0.25410908460617065
train gradient:  0.11246574186489962
iteration : 8657
train acc:  0.8671875
train loss:  0.32769569754600525
train gradient:  0.12354171415617404
iteration : 8658
train acc:  0.84375
train loss:  0.3566843867301941
train gradient:  0.22834901607687194
iteration : 8659
train acc:  0.8515625
train loss:  0.38273292779922485
train gradient:  0.23711899604500625
iteration : 8660
train acc:  0.8984375
train loss:  0.29164132475852966
train gradient:  0.11767520195670245
iteration : 8661
train acc:  0.8515625
train loss:  0.33276528120040894
train gradient:  0.16578544273001244
iteration : 8662
train acc:  0.8984375
train loss:  0.2818129062652588
train gradient:  0.11545639384463528
iteration : 8663
train acc:  0.8671875
train loss:  0.32462263107299805
train gradient:  0.260536748103989
iteration : 8664
train acc:  0.8828125
train loss:  0.26134172081947327
train gradient:  0.13278007965542987
iteration : 8665
train acc:  0.8671875
train loss:  0.3528156280517578
train gradient:  0.16243260571636697
iteration : 8666
train acc:  0.859375
train loss:  0.30161795020103455
train gradient:  0.15274328348498026
iteration : 8667
train acc:  0.84375
train loss:  0.27856922149658203
train gradient:  0.16271887767880644
iteration : 8668
train acc:  0.8125
train loss:  0.39145177602767944
train gradient:  0.25761772307183045
iteration : 8669
train acc:  0.828125
train loss:  0.35462790727615356
train gradient:  0.18110747130821092
iteration : 8670
train acc:  0.8125
train loss:  0.29663851857185364
train gradient:  0.1539082002135115
iteration : 8671
train acc:  0.84375
train loss:  0.3174794912338257
train gradient:  0.1497115226970368
iteration : 8672
train acc:  0.84375
train loss:  0.3268768787384033
train gradient:  0.19899823409791545
iteration : 8673
train acc:  0.8359375
train loss:  0.37080127000808716
train gradient:  0.2433228245357062
iteration : 8674
train acc:  0.78125
train loss:  0.4897652864456177
train gradient:  0.3890411791062144
iteration : 8675
train acc:  0.7890625
train loss:  0.4226849675178528
train gradient:  0.18715677653868695
iteration : 8676
train acc:  0.8515625
train loss:  0.3030695617198944
train gradient:  0.19733445894043838
iteration : 8677
train acc:  0.8359375
train loss:  0.29009270668029785
train gradient:  0.13625350339224412
iteration : 8678
train acc:  0.8359375
train loss:  0.4248107075691223
train gradient:  0.27782729280313473
iteration : 8679
train acc:  0.828125
train loss:  0.29623356461524963
train gradient:  0.15092686162477212
iteration : 8680
train acc:  0.8359375
train loss:  0.3307751715183258
train gradient:  0.16959336548379766
iteration : 8681
train acc:  0.8984375
train loss:  0.3085533380508423
train gradient:  0.19718029831104922
iteration : 8682
train acc:  0.8359375
train loss:  0.3510385751724243
train gradient:  0.18853092248587044
iteration : 8683
train acc:  0.90625
train loss:  0.2248513400554657
train gradient:  0.10751466968504297
iteration : 8684
train acc:  0.890625
train loss:  0.2403043508529663
train gradient:  0.12313532564346535
iteration : 8685
train acc:  0.8203125
train loss:  0.43776965141296387
train gradient:  0.2577598899757848
iteration : 8686
train acc:  0.8515625
train loss:  0.30049318075180054
train gradient:  0.15510849981551936
iteration : 8687
train acc:  0.890625
train loss:  0.2653740644454956
train gradient:  0.19344082099758314
iteration : 8688
train acc:  0.8671875
train loss:  0.33646008372306824
train gradient:  0.20153902686734684
iteration : 8689
train acc:  0.84375
train loss:  0.35816502571105957
train gradient:  0.19592614142129422
iteration : 8690
train acc:  0.828125
train loss:  0.3845120072364807
train gradient:  0.19583196460524535
iteration : 8691
train acc:  0.859375
train loss:  0.34475719928741455
train gradient:  0.193213898415182
iteration : 8692
train acc:  0.8046875
train loss:  0.3865031599998474
train gradient:  0.23759216304755143
iteration : 8693
train acc:  0.84375
train loss:  0.3364256024360657
train gradient:  0.17236175585212576
iteration : 8694
train acc:  0.8125
train loss:  0.3607354760169983
train gradient:  0.15639353830621938
iteration : 8695
train acc:  0.8359375
train loss:  0.3617814779281616
train gradient:  0.19015843763158688
iteration : 8696
train acc:  0.8125
train loss:  0.4439459443092346
train gradient:  0.3991523887895734
iteration : 8697
train acc:  0.8359375
train loss:  0.32579827308654785
train gradient:  0.12843998579362648
iteration : 8698
train acc:  0.8671875
train loss:  0.33102643489837646
train gradient:  0.2828336913459307
iteration : 8699
train acc:  0.84375
train loss:  0.3472507894039154
train gradient:  0.24432257275201386
iteration : 8700
train acc:  0.8125
train loss:  0.3925081491470337
train gradient:  0.22632406885134232
iteration : 8701
train acc:  0.84375
train loss:  0.3612029552459717
train gradient:  0.2074689443161502
iteration : 8702
train acc:  0.859375
train loss:  0.3806421160697937
train gradient:  0.23625617168306023
iteration : 8703
train acc:  0.8046875
train loss:  0.4293282926082611
train gradient:  0.32798177641211745
iteration : 8704
train acc:  0.8515625
train loss:  0.3936561346054077
train gradient:  0.23266786301589332
iteration : 8705
train acc:  0.796875
train loss:  0.42154091596603394
train gradient:  0.2861183181695408
iteration : 8706
train acc:  0.78125
train loss:  0.4118705689907074
train gradient:  0.2557554028484859
iteration : 8707
train acc:  0.8671875
train loss:  0.27310317754745483
train gradient:  0.12670351692378706
iteration : 8708
train acc:  0.8828125
train loss:  0.3083134591579437
train gradient:  0.15082962677601378
iteration : 8709
train acc:  0.8671875
train loss:  0.29439061880111694
train gradient:  0.12910433853047296
iteration : 8710
train acc:  0.84375
train loss:  0.34138450026512146
train gradient:  0.18732457870433755
iteration : 8711
train acc:  0.890625
train loss:  0.2769082188606262
train gradient:  0.16172929326574928
iteration : 8712
train acc:  0.8984375
train loss:  0.2684876322746277
train gradient:  0.11722506953873277
iteration : 8713
train acc:  0.890625
train loss:  0.2679370045661926
train gradient:  0.14027339711242418
iteration : 8714
train acc:  0.8828125
train loss:  0.34029483795166016
train gradient:  0.19996619798935106
iteration : 8715
train acc:  0.90625
train loss:  0.2532102167606354
train gradient:  0.15339502647333225
iteration : 8716
train acc:  0.828125
train loss:  0.4098643660545349
train gradient:  0.23054893084187295
iteration : 8717
train acc:  0.875
train loss:  0.33129259943962097
train gradient:  0.17621873024557494
iteration : 8718
train acc:  0.8671875
train loss:  0.33479931950569153
train gradient:  0.24147829543773586
iteration : 8719
train acc:  0.8046875
train loss:  0.42352211475372314
train gradient:  0.29531486707939203
iteration : 8720
train acc:  0.8359375
train loss:  0.3826574981212616
train gradient:  0.18750704594799922
iteration : 8721
train acc:  0.84375
train loss:  0.35954374074935913
train gradient:  0.13354421452094986
iteration : 8722
train acc:  0.8359375
train loss:  0.35075390338897705
train gradient:  0.2079040188972257
iteration : 8723
train acc:  0.828125
train loss:  0.3722795248031616
train gradient:  0.159634660205862
iteration : 8724
train acc:  0.8828125
train loss:  0.31962502002716064
train gradient:  0.1809879051315895
iteration : 8725
train acc:  0.8359375
train loss:  0.3412904143333435
train gradient:  0.208006274938673
iteration : 8726
train acc:  0.8359375
train loss:  0.3154166042804718
train gradient:  0.1512507933257743
iteration : 8727
train acc:  0.8203125
train loss:  0.3760560154914856
train gradient:  0.26094597719063095
iteration : 8728
train acc:  0.8671875
train loss:  0.3298754096031189
train gradient:  0.12583160589737152
iteration : 8729
train acc:  0.796875
train loss:  0.4536534547805786
train gradient:  0.19898694871179734
iteration : 8730
train acc:  0.8359375
train loss:  0.38353002071380615
train gradient:  0.15978690703483453
iteration : 8731
train acc:  0.8125
train loss:  0.3741220533847809
train gradient:  0.2024960482550862
iteration : 8732
train acc:  0.90625
train loss:  0.21864891052246094
train gradient:  0.11329555443275671
iteration : 8733
train acc:  0.8984375
train loss:  0.2729056775569916
train gradient:  0.14102348149005023
iteration : 8734
train acc:  0.8671875
train loss:  0.3540852665901184
train gradient:  0.13707552321375258
iteration : 8735
train acc:  0.8203125
train loss:  0.4052237570285797
train gradient:  0.1853951284599066
iteration : 8736
train acc:  0.8515625
train loss:  0.29287803173065186
train gradient:  0.09917134952409334
iteration : 8737
train acc:  0.8203125
train loss:  0.3904625475406647
train gradient:  0.2234678077549886
iteration : 8738
train acc:  0.796875
train loss:  0.4503427743911743
train gradient:  0.23552456041409248
iteration : 8739
train acc:  0.921875
train loss:  0.25456807017326355
train gradient:  0.1659870952952197
iteration : 8740
train acc:  0.875
train loss:  0.30962562561035156
train gradient:  0.15343351026023055
iteration : 8741
train acc:  0.84375
train loss:  0.39977598190307617
train gradient:  0.27483625636545117
iteration : 8742
train acc:  0.84375
train loss:  0.3329806327819824
train gradient:  0.2038223450705754
iteration : 8743
train acc:  0.859375
train loss:  0.3137429356575012
train gradient:  0.11769280650174288
iteration : 8744
train acc:  0.828125
train loss:  0.3625577688217163
train gradient:  0.37041743146686124
iteration : 8745
train acc:  0.8515625
train loss:  0.31283143162727356
train gradient:  0.22673115264021682
iteration : 8746
train acc:  0.875
train loss:  0.3310059607028961
train gradient:  0.14221012588429482
iteration : 8747
train acc:  0.875
train loss:  0.2838056981563568
train gradient:  0.14471004262920084
iteration : 8748
train acc:  0.84375
train loss:  0.3594970703125
train gradient:  0.2142679611059669
iteration : 8749
train acc:  0.875
train loss:  0.31971055269241333
train gradient:  0.1717746407656151
iteration : 8750
train acc:  0.859375
train loss:  0.3731970191001892
train gradient:  0.18591370259914142
iteration : 8751
train acc:  0.9140625
train loss:  0.22223666310310364
train gradient:  0.10663097078496066
iteration : 8752
train acc:  0.84375
train loss:  0.37664464116096497
train gradient:  0.22267751127687369
iteration : 8753
train acc:  0.8125
train loss:  0.4042542576789856
train gradient:  0.18152871916778665
iteration : 8754
train acc:  0.8203125
train loss:  0.35329920053482056
train gradient:  0.15511995180271249
iteration : 8755
train acc:  0.84375
train loss:  0.3842222988605499
train gradient:  0.1835465720003643
iteration : 8756
train acc:  0.84375
train loss:  0.3296435475349426
train gradient:  0.13894699346278183
iteration : 8757
train acc:  0.8984375
train loss:  0.26624342799186707
train gradient:  0.14584091184295594
iteration : 8758
train acc:  0.859375
train loss:  0.3108617961406708
train gradient:  0.23642804276325496
iteration : 8759
train acc:  0.84375
train loss:  0.3488144874572754
train gradient:  0.21369464540625194
iteration : 8760
train acc:  0.8984375
train loss:  0.2796478867530823
train gradient:  0.10240984007637101
iteration : 8761
train acc:  0.8515625
train loss:  0.34108859300613403
train gradient:  0.18026473525369954
iteration : 8762
train acc:  0.8515625
train loss:  0.3344195783138275
train gradient:  0.11646194297114829
iteration : 8763
train acc:  0.8671875
train loss:  0.30499953031539917
train gradient:  0.12318627312172623
iteration : 8764
train acc:  0.890625
train loss:  0.28065061569213867
train gradient:  0.10725974540307347
iteration : 8765
train acc:  0.8203125
train loss:  0.38953715562820435
train gradient:  0.2384906877989647
iteration : 8766
train acc:  0.8203125
train loss:  0.43645358085632324
train gradient:  0.26267347241494804
iteration : 8767
train acc:  0.84375
train loss:  0.3535136878490448
train gradient:  0.17507644837905723
iteration : 8768
train acc:  0.8515625
train loss:  0.3460971713066101
train gradient:  0.14071701115341656
iteration : 8769
train acc:  0.8046875
train loss:  0.4607490301132202
train gradient:  0.23939855919171943
iteration : 8770
train acc:  0.8828125
train loss:  0.3745107650756836
train gradient:  0.18469117106502803
iteration : 8771
train acc:  0.8515625
train loss:  0.3131541609764099
train gradient:  0.1315501214777179
iteration : 8772
train acc:  0.8203125
train loss:  0.33864325284957886
train gradient:  0.14789006338989122
iteration : 8773
train acc:  0.859375
train loss:  0.3263237476348877
train gradient:  0.17079696528310062
iteration : 8774
train acc:  0.828125
train loss:  0.3399307131767273
train gradient:  0.16753207298003864
iteration : 8775
train acc:  0.8671875
train loss:  0.2818630337715149
train gradient:  0.09767656449537611
iteration : 8776
train acc:  0.8203125
train loss:  0.3865527808666229
train gradient:  0.14375382138178555
iteration : 8777
train acc:  0.8671875
train loss:  0.32248273491859436
train gradient:  0.17019593897561702
iteration : 8778
train acc:  0.84375
train loss:  0.3567708134651184
train gradient:  0.20601094467533596
iteration : 8779
train acc:  0.8828125
train loss:  0.3023076355457306
train gradient:  0.19880793023988474
iteration : 8780
train acc:  0.890625
train loss:  0.3085177540779114
train gradient:  0.13372018266251307
iteration : 8781
train acc:  0.8984375
train loss:  0.29571354389190674
train gradient:  0.12431653705105314
iteration : 8782
train acc:  0.875
train loss:  0.24939508736133575
train gradient:  0.10009738624884419
iteration : 8783
train acc:  0.84375
train loss:  0.33474063873291016
train gradient:  0.26934402026369897
iteration : 8784
train acc:  0.828125
train loss:  0.3981541097164154
train gradient:  0.16733120226309475
iteration : 8785
train acc:  0.859375
train loss:  0.3456264138221741
train gradient:  0.16837939204457608
iteration : 8786
train acc:  0.8046875
train loss:  0.4236190915107727
train gradient:  0.20310606588024754
iteration : 8787
train acc:  0.8671875
train loss:  0.31687337160110474
train gradient:  0.16151870131173734
iteration : 8788
train acc:  0.875
train loss:  0.3017493486404419
train gradient:  0.1960022636214382
iteration : 8789
train acc:  0.859375
train loss:  0.3217869997024536
train gradient:  0.17105377472358968
iteration : 8790
train acc:  0.8125
train loss:  0.38658469915390015
train gradient:  0.2443476846137695
iteration : 8791
train acc:  0.859375
train loss:  0.3042992651462555
train gradient:  0.13651257497830555
iteration : 8792
train acc:  0.875
train loss:  0.3027450442314148
train gradient:  0.17182647202201753
iteration : 8793
train acc:  0.890625
train loss:  0.31430232524871826
train gradient:  0.13008550717211279
iteration : 8794
train acc:  0.8515625
train loss:  0.34445005655288696
train gradient:  0.17027636869358098
iteration : 8795
train acc:  0.8046875
train loss:  0.4376082420349121
train gradient:  0.3289046431264506
iteration : 8796
train acc:  0.9140625
train loss:  0.2775987982749939
train gradient:  0.11092269725391275
iteration : 8797
train acc:  0.828125
train loss:  0.3503142297267914
train gradient:  0.23230146802398055
iteration : 8798
train acc:  0.875
train loss:  0.31583350896835327
train gradient:  0.1960557314642627
iteration : 8799
train acc:  0.8515625
train loss:  0.3306450843811035
train gradient:  0.17178488097681127
iteration : 8800
train acc:  0.90625
train loss:  0.26680636405944824
train gradient:  0.13651655079545816
iteration : 8801
train acc:  0.84375
train loss:  0.3016121983528137
train gradient:  0.18081283067574044
iteration : 8802
train acc:  0.828125
train loss:  0.3714779019355774
train gradient:  0.1521235612480064
iteration : 8803
train acc:  0.84375
train loss:  0.3259847164154053
train gradient:  0.17455447253961262
iteration : 8804
train acc:  0.8125
train loss:  0.40490150451660156
train gradient:  0.25904094495265667
iteration : 8805
train acc:  0.875
train loss:  0.31571680307388306
train gradient:  0.12773374029492254
iteration : 8806
train acc:  0.828125
train loss:  0.4197482764720917
train gradient:  0.20558315739553512
iteration : 8807
train acc:  0.890625
train loss:  0.293218195438385
train gradient:  0.12885815903472936
iteration : 8808
train acc:  0.828125
train loss:  0.3359701633453369
train gradient:  0.12090272138170258
iteration : 8809
train acc:  0.8359375
train loss:  0.3598169684410095
train gradient:  0.1806750561430149
iteration : 8810
train acc:  0.859375
train loss:  0.33411288261413574
train gradient:  0.15797081787276082
iteration : 8811
train acc:  0.859375
train loss:  0.32224544882774353
train gradient:  0.13222769213554908
iteration : 8812
train acc:  0.8828125
train loss:  0.3443097770214081
train gradient:  0.1814219930656447
iteration : 8813
train acc:  0.8359375
train loss:  0.3462373614311218
train gradient:  0.18998746651024906
iteration : 8814
train acc:  0.8984375
train loss:  0.2915131151676178
train gradient:  0.1412532822923342
iteration : 8815
train acc:  0.828125
train loss:  0.38401299715042114
train gradient:  0.2075416938585678
iteration : 8816
train acc:  0.8828125
train loss:  0.2666460871696472
train gradient:  0.08591249740450643
iteration : 8817
train acc:  0.7734375
train loss:  0.4630064368247986
train gradient:  0.2987490131855522
iteration : 8818
train acc:  0.8828125
train loss:  0.3269168734550476
train gradient:  0.17856757302809234
iteration : 8819
train acc:  0.765625
train loss:  0.40756911039352417
train gradient:  0.23258813887501883
iteration : 8820
train acc:  0.8359375
train loss:  0.34318023920059204
train gradient:  0.13516340746475924
iteration : 8821
train acc:  0.8828125
train loss:  0.26003775000572205
train gradient:  0.10248631435229875
iteration : 8822
train acc:  0.875
train loss:  0.2903215289115906
train gradient:  0.12291689519152037
iteration : 8823
train acc:  0.8984375
train loss:  0.28376126289367676
train gradient:  0.12395587187666546
iteration : 8824
train acc:  0.890625
train loss:  0.27228987216949463
train gradient:  0.15486435290005773
iteration : 8825
train acc:  0.859375
train loss:  0.30800262093544006
train gradient:  0.14228338772985882
iteration : 8826
train acc:  0.8984375
train loss:  0.2518298625946045
train gradient:  0.20546035613371239
iteration : 8827
train acc:  0.8515625
train loss:  0.3234849274158478
train gradient:  0.20616964104592447
iteration : 8828
train acc:  0.8671875
train loss:  0.3564773201942444
train gradient:  0.155188119901289
iteration : 8829
train acc:  0.8046875
train loss:  0.43943625688552856
train gradient:  0.22536753782492192
iteration : 8830
train acc:  0.90625
train loss:  0.2780835032463074
train gradient:  0.09128235116965931
iteration : 8831
train acc:  0.8671875
train loss:  0.35288703441619873
train gradient:  0.17176318096352228
iteration : 8832
train acc:  0.859375
train loss:  0.329662024974823
train gradient:  0.16115216934217771
iteration : 8833
train acc:  0.8671875
train loss:  0.35561490058898926
train gradient:  0.22360281256498282
iteration : 8834
train acc:  0.8984375
train loss:  0.2667044997215271
train gradient:  0.0952284995808064
iteration : 8835
train acc:  0.8515625
train loss:  0.390682578086853
train gradient:  0.23655782797195155
iteration : 8836
train acc:  0.8671875
train loss:  0.31644535064697266
train gradient:  0.2348170721874575
iteration : 8837
train acc:  0.828125
train loss:  0.39855799078941345
train gradient:  0.30550238774886074
iteration : 8838
train acc:  0.859375
train loss:  0.35335487127304077
train gradient:  0.19087815645757208
iteration : 8839
train acc:  0.890625
train loss:  0.29798245429992676
train gradient:  0.15238112447225383
iteration : 8840
train acc:  0.8515625
train loss:  0.32869648933410645
train gradient:  0.15598917455597355
iteration : 8841
train acc:  0.875
train loss:  0.32182902097702026
train gradient:  0.1532152671410304
iteration : 8842
train acc:  0.8125
train loss:  0.41106587648391724
train gradient:  0.20317685192516946
iteration : 8843
train acc:  0.875
train loss:  0.34225213527679443
train gradient:  0.1445539557379045
iteration : 8844
train acc:  0.859375
train loss:  0.3192010521888733
train gradient:  0.24995850784831514
iteration : 8845
train acc:  0.890625
train loss:  0.27680984139442444
train gradient:  0.13668033981905653
iteration : 8846
train acc:  0.875
train loss:  0.3066529631614685
train gradient:  0.14374040014663697
iteration : 8847
train acc:  0.8515625
train loss:  0.3789669871330261
train gradient:  0.2916151110733298
iteration : 8848
train acc:  0.8125
train loss:  0.45662105083465576
train gradient:  0.2656270950828713
iteration : 8849
train acc:  0.859375
train loss:  0.3649347424507141
train gradient:  0.23164612450562103
iteration : 8850
train acc:  0.859375
train loss:  0.37192463874816895
train gradient:  0.17800516072223438
iteration : 8851
train acc:  0.828125
train loss:  0.4063904881477356
train gradient:  0.18468918721434452
iteration : 8852
train acc:  0.90625
train loss:  0.28320711851119995
train gradient:  0.1301120775811447
iteration : 8853
train acc:  0.8828125
train loss:  0.3074573874473572
train gradient:  0.14173347063498343
iteration : 8854
train acc:  0.7890625
train loss:  0.381999671459198
train gradient:  0.16524316589878968
iteration : 8855
train acc:  0.9140625
train loss:  0.21870651841163635
train gradient:  0.11392490722061174
iteration : 8856
train acc:  0.90625
train loss:  0.2616293430328369
train gradient:  0.11120813194725969
iteration : 8857
train acc:  0.875
train loss:  0.2733575701713562
train gradient:  0.1522736608322836
iteration : 8858
train acc:  0.8203125
train loss:  0.35004913806915283
train gradient:  0.16942038835544687
iteration : 8859
train acc:  0.859375
train loss:  0.30995404720306396
train gradient:  0.1334043192764225
iteration : 8860
train acc:  0.859375
train loss:  0.32057031989097595
train gradient:  0.15752754081617315
iteration : 8861
train acc:  0.859375
train loss:  0.3069586455821991
train gradient:  0.12689259722232582
iteration : 8862
train acc:  0.828125
train loss:  0.3503023386001587
train gradient:  0.2057084201501625
iteration : 8863
train acc:  0.8359375
train loss:  0.3812217712402344
train gradient:  0.2632679082111943
iteration : 8864
train acc:  0.890625
train loss:  0.3209793269634247
train gradient:  0.20012684952787832
iteration : 8865
train acc:  0.8671875
train loss:  0.3423195481300354
train gradient:  0.21108984506223838
iteration : 8866
train acc:  0.921875
train loss:  0.22436699271202087
train gradient:  0.09132642871815115
iteration : 8867
train acc:  0.84375
train loss:  0.3031829595565796
train gradient:  0.14118053910796316
iteration : 8868
train acc:  0.828125
train loss:  0.42453843355178833
train gradient:  0.21124002877277717
iteration : 8869
train acc:  0.828125
train loss:  0.3118691146373749
train gradient:  0.1483495193598001
iteration : 8870
train acc:  0.8125
train loss:  0.39631780982017517
train gradient:  0.22493241574864048
iteration : 8871
train acc:  0.8203125
train loss:  0.34018459916114807
train gradient:  0.1536582320595452
iteration : 8872
train acc:  0.890625
train loss:  0.2640107572078705
train gradient:  0.1444324896672956
iteration : 8873
train acc:  0.890625
train loss:  0.2697601318359375
train gradient:  0.10172308599716273
iteration : 8874
train acc:  0.8359375
train loss:  0.34374409914016724
train gradient:  0.2460370461124602
iteration : 8875
train acc:  0.8984375
train loss:  0.33011990785598755
train gradient:  0.14796557042682928
iteration : 8876
train acc:  0.84375
train loss:  0.3482949733734131
train gradient:  0.18901162745042813
iteration : 8877
train acc:  0.8671875
train loss:  0.31057047843933105
train gradient:  0.14433238942960488
iteration : 8878
train acc:  0.828125
train loss:  0.3494398891925812
train gradient:  0.22952548644559062
iteration : 8879
train acc:  0.8828125
train loss:  0.28293684124946594
train gradient:  0.11471525086441638
iteration : 8880
train acc:  0.8359375
train loss:  0.3682626485824585
train gradient:  0.18758868538139917
iteration : 8881
train acc:  0.8671875
train loss:  0.3345392942428589
train gradient:  0.17741223944562842
iteration : 8882
train acc:  0.859375
train loss:  0.3119761049747467
train gradient:  0.11201406632269052
iteration : 8883
train acc:  0.8515625
train loss:  0.34895920753479004
train gradient:  0.1480285867460555
iteration : 8884
train acc:  0.8359375
train loss:  0.38347622752189636
train gradient:  0.1881925039830848
iteration : 8885
train acc:  0.9140625
train loss:  0.28641316294670105
train gradient:  0.1255468748380047
iteration : 8886
train acc:  0.8046875
train loss:  0.40053683519363403
train gradient:  0.21165037940178277
iteration : 8887
train acc:  0.8359375
train loss:  0.37464436888694763
train gradient:  0.1743626614234594
iteration : 8888
train acc:  0.8359375
train loss:  0.41234850883483887
train gradient:  0.2197192391504805
iteration : 8889
train acc:  0.8828125
train loss:  0.278913289308548
train gradient:  0.10880866066622703
iteration : 8890
train acc:  0.859375
train loss:  0.3207551836967468
train gradient:  0.15792820599785368
iteration : 8891
train acc:  0.8359375
train loss:  0.390092670917511
train gradient:  0.16987489000575376
iteration : 8892
train acc:  0.875
train loss:  0.32512617111206055
train gradient:  0.15318861705125728
iteration : 8893
train acc:  0.8515625
train loss:  0.28807878494262695
train gradient:  0.16575605161335738
iteration : 8894
train acc:  0.8125
train loss:  0.4045156240463257
train gradient:  0.21520271006494474
iteration : 8895
train acc:  0.8515625
train loss:  0.35917335748672485
train gradient:  0.2189170037345356
iteration : 8896
train acc:  0.8984375
train loss:  0.2580428421497345
train gradient:  0.09965731157391833
iteration : 8897
train acc:  0.8671875
train loss:  0.30507874488830566
train gradient:  0.11464437320600397
iteration : 8898
train acc:  0.8828125
train loss:  0.3092532157897949
train gradient:  0.12087946656490921
iteration : 8899
train acc:  0.875
train loss:  0.2897305190563202
train gradient:  0.16864171971801611
iteration : 8900
train acc:  0.8125
train loss:  0.38246744871139526
train gradient:  0.1694408692361281
iteration : 8901
train acc:  0.8671875
train loss:  0.2813575565814972
train gradient:  0.12263656934059813
iteration : 8902
train acc:  0.8828125
train loss:  0.29294025897979736
train gradient:  0.14526025318161753
iteration : 8903
train acc:  0.890625
train loss:  0.27320635318756104
train gradient:  0.1013185937398365
iteration : 8904
train acc:  0.828125
train loss:  0.32499510049819946
train gradient:  0.14785347725502915
iteration : 8905
train acc:  0.859375
train loss:  0.30899524688720703
train gradient:  0.17655968893241894
iteration : 8906
train acc:  0.875
train loss:  0.2661810517311096
train gradient:  0.11772095440543742
iteration : 8907
train acc:  0.8671875
train loss:  0.3354194760322571
train gradient:  0.18258215088224117
iteration : 8908
train acc:  0.875
train loss:  0.300998330116272
train gradient:  0.12778037905653916
iteration : 8909
train acc:  0.8828125
train loss:  0.27534234523773193
train gradient:  0.11737138149378126
iteration : 8910
train acc:  0.84375
train loss:  0.340739905834198
train gradient:  0.16929269939086866
iteration : 8911
train acc:  0.8515625
train loss:  0.3638763129711151
train gradient:  0.15855605515205856
iteration : 8912
train acc:  0.8515625
train loss:  0.33252018690109253
train gradient:  0.1927014729519187
iteration : 8913
train acc:  0.8359375
train loss:  0.3350740373134613
train gradient:  0.16095789045188733
iteration : 8914
train acc:  0.859375
train loss:  0.3150274157524109
train gradient:  0.18022436810095033
iteration : 8915
train acc:  0.8671875
train loss:  0.2713121175765991
train gradient:  0.1883085458873932
iteration : 8916
train acc:  0.921875
train loss:  0.21308788657188416
train gradient:  0.1436249933863689
iteration : 8917
train acc:  0.8203125
train loss:  0.39404040575027466
train gradient:  0.2407231775872382
iteration : 8918
train acc:  0.828125
train loss:  0.4022749662399292
train gradient:  0.23031375995333697
iteration : 8919
train acc:  0.921875
train loss:  0.2340514063835144
train gradient:  0.15784845615043247
iteration : 8920
train acc:  0.8671875
train loss:  0.35176366567611694
train gradient:  0.2208857943543327
iteration : 8921
train acc:  0.8515625
train loss:  0.32004088163375854
train gradient:  0.19756916806108724
iteration : 8922
train acc:  0.90625
train loss:  0.29887792468070984
train gradient:  0.15323705619337225
iteration : 8923
train acc:  0.8125
train loss:  0.37390372157096863
train gradient:  0.26280395385080935
iteration : 8924
train acc:  0.875
train loss:  0.2642372250556946
train gradient:  0.11949454938134059
iteration : 8925
train acc:  0.8359375
train loss:  0.3399450182914734
train gradient:  0.17106661877445695
iteration : 8926
train acc:  0.8515625
train loss:  0.36010342836380005
train gradient:  0.19416082749041808
iteration : 8927
train acc:  0.875
train loss:  0.34130311012268066
train gradient:  0.27604898249164483
iteration : 8928
train acc:  0.796875
train loss:  0.44064566493034363
train gradient:  0.30813943514391057
iteration : 8929
train acc:  0.859375
train loss:  0.30838489532470703
train gradient:  0.18352558895401516
iteration : 8930
train acc:  0.859375
train loss:  0.3207671046257019
train gradient:  0.1762273476263922
iteration : 8931
train acc:  0.875
train loss:  0.32085663080215454
train gradient:  0.11589600707479472
iteration : 8932
train acc:  0.8046875
train loss:  0.4214994013309479
train gradient:  0.33232955372214124
iteration : 8933
train acc:  0.8671875
train loss:  0.2862657904624939
train gradient:  0.12766077191796685
iteration : 8934
train acc:  0.8203125
train loss:  0.37057989835739136
train gradient:  0.20580163561101106
iteration : 8935
train acc:  0.828125
train loss:  0.387149453163147
train gradient:  0.2611786253841052
iteration : 8936
train acc:  0.8515625
train loss:  0.33395180106163025
train gradient:  0.21321448118353076
iteration : 8937
train acc:  0.8359375
train loss:  0.360703706741333
train gradient:  0.16948507174867122
iteration : 8938
train acc:  0.8359375
train loss:  0.36734917759895325
train gradient:  0.2096850602070211
iteration : 8939
train acc:  0.8984375
train loss:  0.26104676723480225
train gradient:  0.08994795510933777
iteration : 8940
train acc:  0.8125
train loss:  0.3838348984718323
train gradient:  0.20887668848214852
iteration : 8941
train acc:  0.875
train loss:  0.3466387689113617
train gradient:  0.16639788878474931
iteration : 8942
train acc:  0.859375
train loss:  0.3339563012123108
train gradient:  0.1260032832891667
iteration : 8943
train acc:  0.875
train loss:  0.3034079372882843
train gradient:  0.1180933512810558
iteration : 8944
train acc:  0.8515625
train loss:  0.3329799175262451
train gradient:  0.13738257477900895
iteration : 8945
train acc:  0.8359375
train loss:  0.37972918152809143
train gradient:  0.22010999488980998
iteration : 8946
train acc:  0.859375
train loss:  0.29860153794288635
train gradient:  0.14907610024655993
iteration : 8947
train acc:  0.84375
train loss:  0.33178481459617615
train gradient:  0.17022579200114704
iteration : 8948
train acc:  0.828125
train loss:  0.4525966942310333
train gradient:  0.2654572136063598
iteration : 8949
train acc:  0.8671875
train loss:  0.33289414644241333
train gradient:  0.17262050666032472
iteration : 8950
train acc:  0.8671875
train loss:  0.29974138736724854
train gradient:  0.11783202183600512
iteration : 8951
train acc:  0.890625
train loss:  0.2614077925682068
train gradient:  0.1906449525780491
iteration : 8952
train acc:  0.859375
train loss:  0.32655906677246094
train gradient:  0.18753539508195705
iteration : 8953
train acc:  0.828125
train loss:  0.34191930294036865
train gradient:  0.1780351441593952
iteration : 8954
train acc:  0.875
train loss:  0.30956023931503296
train gradient:  0.18609185161191263
iteration : 8955
train acc:  0.84375
train loss:  0.3251984715461731
train gradient:  0.2641177996497498
iteration : 8956
train acc:  0.859375
train loss:  0.3429968059062958
train gradient:  0.194950818514173
iteration : 8957
train acc:  0.8671875
train loss:  0.3778725862503052
train gradient:  0.17428473549849444
iteration : 8958
train acc:  0.859375
train loss:  0.3000802993774414
train gradient:  0.160591953283096
iteration : 8959
train acc:  0.8046875
train loss:  0.41678866744041443
train gradient:  0.20262484436734315
iteration : 8960
train acc:  0.859375
train loss:  0.3193654417991638
train gradient:  0.18457343359298103
iteration : 8961
train acc:  0.875
train loss:  0.29292207956314087
train gradient:  0.11205532992232943
iteration : 8962
train acc:  0.78125
train loss:  0.4443819522857666
train gradient:  0.24447483582576282
iteration : 8963
train acc:  0.8125
train loss:  0.37675559520721436
train gradient:  0.23447143679485377
iteration : 8964
train acc:  0.859375
train loss:  0.3370811939239502
train gradient:  0.13182864588578871
iteration : 8965
train acc:  0.8359375
train loss:  0.3387121260166168
train gradient:  0.22872846461351148
iteration : 8966
train acc:  0.8828125
train loss:  0.31080520153045654
train gradient:  0.16253492251113297
iteration : 8967
train acc:  0.78125
train loss:  0.4825861155986786
train gradient:  0.2546230087367251
iteration : 8968
train acc:  0.8359375
train loss:  0.34393858909606934
train gradient:  0.17050955868541676
iteration : 8969
train acc:  0.8515625
train loss:  0.3263922333717346
train gradient:  0.13341484942383808
iteration : 8970
train acc:  0.828125
train loss:  0.37309566140174866
train gradient:  0.15206169186260632
iteration : 8971
train acc:  0.8828125
train loss:  0.25029775500297546
train gradient:  0.09381489646617366
iteration : 8972
train acc:  0.7890625
train loss:  0.43314871191978455
train gradient:  0.17860879593624884
iteration : 8973
train acc:  0.8515625
train loss:  0.30637848377227783
train gradient:  0.17564291942934676
iteration : 8974
train acc:  0.8359375
train loss:  0.4056912064552307
train gradient:  0.21285111358685732
iteration : 8975
train acc:  0.8984375
train loss:  0.2628802955150604
train gradient:  0.10147699465511502
iteration : 8976
train acc:  0.8515625
train loss:  0.32972630858421326
train gradient:  0.12666905435857875
iteration : 8977
train acc:  0.8671875
train loss:  0.3024234175682068
train gradient:  0.1042100044837277
iteration : 8978
train acc:  0.8359375
train loss:  0.29991430044174194
train gradient:  0.18735938320587942
iteration : 8979
train acc:  0.84375
train loss:  0.34601008892059326
train gradient:  0.16466036906231168
iteration : 8980
train acc:  0.8515625
train loss:  0.3166539967060089
train gradient:  0.1722912548792242
iteration : 8981
train acc:  0.890625
train loss:  0.28483113646507263
train gradient:  0.12499077270959501
iteration : 8982
train acc:  0.8046875
train loss:  0.36925363540649414
train gradient:  0.18865979686001888
iteration : 8983
train acc:  0.875
train loss:  0.3079923987388611
train gradient:  0.1888602565085371
iteration : 8984
train acc:  0.8359375
train loss:  0.34919777512550354
train gradient:  0.15176188886943234
iteration : 8985
train acc:  0.9140625
train loss:  0.23956651985645294
train gradient:  0.08875135582260332
iteration : 8986
train acc:  0.8984375
train loss:  0.23975911736488342
train gradient:  0.08266648866775314
iteration : 8987
train acc:  0.890625
train loss:  0.29812195897102356
train gradient:  0.12797154202178693
iteration : 8988
train acc:  0.8984375
train loss:  0.3390480577945709
train gradient:  0.19401082164936817
iteration : 8989
train acc:  0.9140625
train loss:  0.285567045211792
train gradient:  0.1577378153821347
iteration : 8990
train acc:  0.90625
train loss:  0.261107474565506
train gradient:  0.1308037010634504
iteration : 8991
train acc:  0.84375
train loss:  0.3357524275779724
train gradient:  0.16722023519083615
iteration : 8992
train acc:  0.8671875
train loss:  0.3066394031047821
train gradient:  0.1683802957911134
iteration : 8993
train acc:  0.84375
train loss:  0.3590908646583557
train gradient:  0.19840509210785087
iteration : 8994
train acc:  0.8984375
train loss:  0.3091024160385132
train gradient:  0.1026134158801712
iteration : 8995
train acc:  0.8828125
train loss:  0.28728407621383667
train gradient:  0.13690226032749964
iteration : 8996
train acc:  0.8671875
train loss:  0.34352627396583557
train gradient:  0.2203367369545968
iteration : 8997
train acc:  0.859375
train loss:  0.31135934591293335
train gradient:  0.1504023858025506
iteration : 8998
train acc:  0.8515625
train loss:  0.2841648459434509
train gradient:  0.11886636029143942
iteration : 8999
train acc:  0.8515625
train loss:  0.3322824239730835
train gradient:  0.18280818710077956
iteration : 9000
train acc:  0.8046875
train loss:  0.44859808683395386
train gradient:  0.34172760086825893
iteration : 9001
train acc:  0.8359375
train loss:  0.31595855951309204
train gradient:  0.1041733788590353
iteration : 9002
train acc:  0.8828125
train loss:  0.29518961906433105
train gradient:  0.15740192901448233
iteration : 9003
train acc:  0.8203125
train loss:  0.3765324056148529
train gradient:  0.20668978061050686
iteration : 9004
train acc:  0.796875
train loss:  0.3822634518146515
train gradient:  0.1773155990237382
iteration : 9005
train acc:  0.875
train loss:  0.2954823970794678
train gradient:  0.13059016471855017
iteration : 9006
train acc:  0.8984375
train loss:  0.240727037191391
train gradient:  0.08421763914364247
iteration : 9007
train acc:  0.828125
train loss:  0.3611626625061035
train gradient:  0.17563254971050524
iteration : 9008
train acc:  0.9140625
train loss:  0.24915675818920135
train gradient:  0.09702200215758137
iteration : 9009
train acc:  0.8515625
train loss:  0.28687331080436707
train gradient:  0.1333668989884273
iteration : 9010
train acc:  0.84375
train loss:  0.3835916817188263
train gradient:  0.18274021869913998
iteration : 9011
train acc:  0.8828125
train loss:  0.2765042185783386
train gradient:  0.13927064064782657
iteration : 9012
train acc:  0.84375
train loss:  0.3301151692867279
train gradient:  0.16798598950767382
iteration : 9013
train acc:  0.828125
train loss:  0.37118178606033325
train gradient:  0.18316190223630627
iteration : 9014
train acc:  0.890625
train loss:  0.2780403196811676
train gradient:  0.1269887862471702
iteration : 9015
train acc:  0.90625
train loss:  0.2705715298652649
train gradient:  0.14051749173645048
iteration : 9016
train acc:  0.84375
train loss:  0.4091407060623169
train gradient:  0.2440266234789324
iteration : 9017
train acc:  0.8828125
train loss:  0.289520263671875
train gradient:  0.12905329110654185
iteration : 9018
train acc:  0.828125
train loss:  0.34960198402404785
train gradient:  0.2190782007550293
iteration : 9019
train acc:  0.8671875
train loss:  0.2924881875514984
train gradient:  0.14497473172195496
iteration : 9020
train acc:  0.859375
train loss:  0.3477098345756531
train gradient:  0.16884881534050522
iteration : 9021
train acc:  0.8203125
train loss:  0.33566156029701233
train gradient:  0.13550531595588547
iteration : 9022
train acc:  0.8671875
train loss:  0.2820281982421875
train gradient:  0.08493273601045909
iteration : 9023
train acc:  0.9140625
train loss:  0.2836955189704895
train gradient:  0.11882868569279899
iteration : 9024
train acc:  0.921875
train loss:  0.21924343705177307
train gradient:  0.07354792135726924
iteration : 9025
train acc:  0.859375
train loss:  0.36821338534355164
train gradient:  0.18675382973165353
iteration : 9026
train acc:  0.8203125
train loss:  0.3856254816055298
train gradient:  0.26032547889432084
iteration : 9027
train acc:  0.8828125
train loss:  0.29311102628707886
train gradient:  0.14697353303181826
iteration : 9028
train acc:  0.8515625
train loss:  0.3473855257034302
train gradient:  0.22452825840953458
iteration : 9029
train acc:  0.8046875
train loss:  0.40812233090400696
train gradient:  0.36608985138708894
iteration : 9030
train acc:  0.8359375
train loss:  0.37462130188941956
train gradient:  0.3196382427668699
iteration : 9031
train acc:  0.875
train loss:  0.3312077522277832
train gradient:  0.14736204485290905
iteration : 9032
train acc:  0.875
train loss:  0.29215100407600403
train gradient:  0.17014647810724515
iteration : 9033
train acc:  0.84375
train loss:  0.3626219630241394
train gradient:  0.19035712450445247
iteration : 9034
train acc:  0.875
train loss:  0.3372088670730591
train gradient:  0.16021622449366435
iteration : 9035
train acc:  0.84375
train loss:  0.311176061630249
train gradient:  0.11382668105961938
iteration : 9036
train acc:  0.890625
train loss:  0.2973720133304596
train gradient:  0.15639653652513413
iteration : 9037
train acc:  0.84375
train loss:  0.3521011769771576
train gradient:  0.17704814458955404
iteration : 9038
train acc:  0.890625
train loss:  0.31261181831359863
train gradient:  0.14475332412029296
iteration : 9039
train acc:  0.8828125
train loss:  0.3202376961708069
train gradient:  0.1673578882260724
iteration : 9040
train acc:  0.875
train loss:  0.37109893560409546
train gradient:  0.21903532191660946
iteration : 9041
train acc:  0.8125
train loss:  0.3886524736881256
train gradient:  0.17026245164965703
iteration : 9042
train acc:  0.8203125
train loss:  0.40855467319488525
train gradient:  0.22335269219918868
iteration : 9043
train acc:  0.890625
train loss:  0.30973559617996216
train gradient:  0.18830566188032719
iteration : 9044
train acc:  0.8828125
train loss:  0.333116352558136
train gradient:  0.18100584501163963
iteration : 9045
train acc:  0.90625
train loss:  0.2856972813606262
train gradient:  0.12732913899128745
iteration : 9046
train acc:  0.8359375
train loss:  0.3920697867870331
train gradient:  0.19381487621580681
iteration : 9047
train acc:  0.8359375
train loss:  0.37396106123924255
train gradient:  0.181119270493799
iteration : 9048
train acc:  0.84375
train loss:  0.3694125711917877
train gradient:  0.19535820091166728
iteration : 9049
train acc:  0.890625
train loss:  0.31333476305007935
train gradient:  0.22669096706754155
iteration : 9050
train acc:  0.8671875
train loss:  0.30577853322029114
train gradient:  0.184949123458503
iteration : 9051
train acc:  0.890625
train loss:  0.2872331440448761
train gradient:  0.12389679134567423
iteration : 9052
train acc:  0.84375
train loss:  0.40006184577941895
train gradient:  0.2587666421355134
iteration : 9053
train acc:  0.8984375
train loss:  0.2583865225315094
train gradient:  0.10668080734155613
iteration : 9054
train acc:  0.875
train loss:  0.3173394799232483
train gradient:  0.12803114502686042
iteration : 9055
train acc:  0.8359375
train loss:  0.32841986417770386
train gradient:  0.14697566517876898
iteration : 9056
train acc:  0.890625
train loss:  0.2789232134819031
train gradient:  0.10105024667337638
iteration : 9057
train acc:  0.875
train loss:  0.32640546560287476
train gradient:  0.13126631923846832
iteration : 9058
train acc:  0.8828125
train loss:  0.2910211682319641
train gradient:  0.19545069049399258
iteration : 9059
train acc:  0.8359375
train loss:  0.37015029788017273
train gradient:  0.26390626906693415
iteration : 9060
train acc:  0.8515625
train loss:  0.32914263010025024
train gradient:  0.13641955230535022
iteration : 9061
train acc:  0.8671875
train loss:  0.2940410077571869
train gradient:  0.14518905706830687
iteration : 9062
train acc:  0.8359375
train loss:  0.3403787612915039
train gradient:  0.168705898698324
iteration : 9063
train acc:  0.9140625
train loss:  0.3115924596786499
train gradient:  0.1845075919483891
iteration : 9064
train acc:  0.828125
train loss:  0.3344053030014038
train gradient:  0.23994889261807695
iteration : 9065
train acc:  0.8203125
train loss:  0.33148473501205444
train gradient:  0.13420802343265636
iteration : 9066
train acc:  0.84375
train loss:  0.38195592164993286
train gradient:  0.2569397962630791
iteration : 9067
train acc:  0.859375
train loss:  0.34504592418670654
train gradient:  0.15694939834794264
iteration : 9068
train acc:  0.8828125
train loss:  0.298415869474411
train gradient:  0.1764280754047627
iteration : 9069
train acc:  0.828125
train loss:  0.34906962513923645
train gradient:  0.16367965387967115
iteration : 9070
train acc:  0.828125
train loss:  0.41379186511039734
train gradient:  0.20284222300144789
iteration : 9071
train acc:  0.8671875
train loss:  0.2934926152229309
train gradient:  0.17157262182925792
iteration : 9072
train acc:  0.8671875
train loss:  0.2851112484931946
train gradient:  0.12226425751381854
iteration : 9073
train acc:  0.859375
train loss:  0.30467039346694946
train gradient:  0.2852174271330263
iteration : 9074
train acc:  0.8203125
train loss:  0.39070090651512146
train gradient:  0.2034690486380232
iteration : 9075
train acc:  0.84375
train loss:  0.4116036593914032
train gradient:  0.24263088395546803
iteration : 9076
train acc:  0.8203125
train loss:  0.4285637140274048
train gradient:  0.22887593840110354
iteration : 9077
train acc:  0.875
train loss:  0.33699721097946167
train gradient:  0.15226026581416385
iteration : 9078
train acc:  0.8203125
train loss:  0.39691534638404846
train gradient:  0.21351703345944809
iteration : 9079
train acc:  0.859375
train loss:  0.4180528521537781
train gradient:  0.21429377822067536
iteration : 9080
train acc:  0.828125
train loss:  0.3252563774585724
train gradient:  0.24074214606059283
iteration : 9081
train acc:  0.859375
train loss:  0.39450645446777344
train gradient:  0.28301233941133386
iteration : 9082
train acc:  0.8515625
train loss:  0.3672177791595459
train gradient:  0.14348615791866287
iteration : 9083
train acc:  0.8515625
train loss:  0.3574231266975403
train gradient:  0.28732136628606897
iteration : 9084
train acc:  0.890625
train loss:  0.278942346572876
train gradient:  0.11614883332597219
iteration : 9085
train acc:  0.875
train loss:  0.2881602644920349
train gradient:  0.12592039967311913
iteration : 9086
train acc:  0.90625
train loss:  0.2428789734840393
train gradient:  0.09266596822327994
iteration : 9087
train acc:  0.859375
train loss:  0.31627461314201355
train gradient:  0.15536798647431724
iteration : 9088
train acc:  0.8828125
train loss:  0.2941836416721344
train gradient:  0.13078504925083514
iteration : 9089
train acc:  0.84375
train loss:  0.2863326668739319
train gradient:  0.13228718524720265
iteration : 9090
train acc:  0.8515625
train loss:  0.31254416704177856
train gradient:  0.16701459372649646
iteration : 9091
train acc:  0.8203125
train loss:  0.31562697887420654
train gradient:  0.14243224687669945
iteration : 9092
train acc:  0.8828125
train loss:  0.2937748432159424
train gradient:  0.16442903693848943
iteration : 9093
train acc:  0.859375
train loss:  0.3026357889175415
train gradient:  0.16518064209175104
iteration : 9094
train acc:  0.8125
train loss:  0.379267156124115
train gradient:  0.2043251408363948
iteration : 9095
train acc:  0.8359375
train loss:  0.357873797416687
train gradient:  0.180228972314862
iteration : 9096
train acc:  0.875
train loss:  0.3269100785255432
train gradient:  0.17798212739272506
iteration : 9097
train acc:  0.90625
train loss:  0.2643531560897827
train gradient:  0.11921006419217943
iteration : 9098
train acc:  0.7890625
train loss:  0.461645245552063
train gradient:  0.2679569577605679
iteration : 9099
train acc:  0.828125
train loss:  0.34407126903533936
train gradient:  0.2174058504077263
iteration : 9100
train acc:  0.859375
train loss:  0.3263969421386719
train gradient:  0.16480833700913944
iteration : 9101
train acc:  0.859375
train loss:  0.2993963956832886
train gradient:  0.13300436228540796
iteration : 9102
train acc:  0.8671875
train loss:  0.3118537366390228
train gradient:  0.1668845141454339
iteration : 9103
train acc:  0.8359375
train loss:  0.3947659134864807
train gradient:  0.2555905313837196
iteration : 9104
train acc:  0.84375
train loss:  0.2846565842628479
train gradient:  0.1260773848881831
iteration : 9105
train acc:  0.8359375
train loss:  0.34206920862197876
train gradient:  0.12878130185696962
iteration : 9106
train acc:  0.890625
train loss:  0.3369332551956177
train gradient:  0.15618961269724616
iteration : 9107
train acc:  0.8828125
train loss:  0.261282354593277
train gradient:  0.12972212481399903
iteration : 9108
train acc:  0.8984375
train loss:  0.3098183870315552
train gradient:  0.11948488662759672
iteration : 9109
train acc:  0.8125
train loss:  0.43122854828834534
train gradient:  0.23870481738409824
iteration : 9110
train acc:  0.859375
train loss:  0.29444620013237
train gradient:  0.16753933917917996
iteration : 9111
train acc:  0.8671875
train loss:  0.3112644553184509
train gradient:  0.1254118459918094
iteration : 9112
train acc:  0.8359375
train loss:  0.32929953932762146
train gradient:  0.16411275517361226
iteration : 9113
train acc:  0.8984375
train loss:  0.2629244923591614
train gradient:  0.15689303735849883
iteration : 9114
train acc:  0.8515625
train loss:  0.40361639857292175
train gradient:  0.23178036826462275
iteration : 9115
train acc:  0.875
train loss:  0.33928176760673523
train gradient:  0.13829845051209283
iteration : 9116
train acc:  0.8828125
train loss:  0.2800065875053406
train gradient:  0.1074819183935345
iteration : 9117
train acc:  0.90625
train loss:  0.245542973279953
train gradient:  0.15673391348728466
iteration : 9118
train acc:  0.8828125
train loss:  0.27957576513290405
train gradient:  0.1623870368077035
iteration : 9119
train acc:  0.8671875
train loss:  0.32232168316841125
train gradient:  0.17166179295006484
iteration : 9120
train acc:  0.8671875
train loss:  0.29832664132118225
train gradient:  0.1282582867629528
iteration : 9121
train acc:  0.8671875
train loss:  0.33940058946609497
train gradient:  0.14455759650693747
iteration : 9122
train acc:  0.84375
train loss:  0.3857860267162323
train gradient:  0.38237123929755434
iteration : 9123
train acc:  0.859375
train loss:  0.3134419322013855
train gradient:  0.1455200706836282
iteration : 9124
train acc:  0.8203125
train loss:  0.37376612424850464
train gradient:  0.23301330707045292
iteration : 9125
train acc:  0.8359375
train loss:  0.380426287651062
train gradient:  0.22236796227185168
iteration : 9126
train acc:  0.828125
train loss:  0.359783411026001
train gradient:  0.18925701909887216
iteration : 9127
train acc:  0.8984375
train loss:  0.3539077937602997
train gradient:  0.20535753673914767
iteration : 9128
train acc:  0.859375
train loss:  0.35592764616012573
train gradient:  0.18467027889428128
iteration : 9129
train acc:  0.8515625
train loss:  0.32295072078704834
train gradient:  0.20101590056186203
iteration : 9130
train acc:  0.8203125
train loss:  0.3566046357154846
train gradient:  0.2027359287231554
iteration : 9131
train acc:  0.8515625
train loss:  0.3049588203430176
train gradient:  0.13250802796050126
iteration : 9132
train acc:  0.8671875
train loss:  0.33439570665359497
train gradient:  0.1825175884001381
iteration : 9133
train acc:  0.890625
train loss:  0.32840317487716675
train gradient:  0.17565970500775743
iteration : 9134
train acc:  0.9140625
train loss:  0.26992782950401306
train gradient:  0.19892233164421888
iteration : 9135
train acc:  0.8828125
train loss:  0.2696552276611328
train gradient:  0.10979692728923611
iteration : 9136
train acc:  0.8671875
train loss:  0.2670370936393738
train gradient:  0.11146041847645646
iteration : 9137
train acc:  0.875
train loss:  0.30356693267822266
train gradient:  0.11276829177578196
iteration : 9138
train acc:  0.8515625
train loss:  0.35001060366630554
train gradient:  0.19704585578695588
iteration : 9139
train acc:  0.8984375
train loss:  0.26414361596107483
train gradient:  0.12622153050797136
iteration : 9140
train acc:  0.859375
train loss:  0.34557217359542847
train gradient:  0.19622793960673304
iteration : 9141
train acc:  0.828125
train loss:  0.3607425093650818
train gradient:  0.1754159903372871
iteration : 9142
train acc:  0.8203125
train loss:  0.3646389842033386
train gradient:  0.28273344675650863
iteration : 9143
train acc:  0.84375
train loss:  0.34514153003692627
train gradient:  0.25989972791342036
iteration : 9144
train acc:  0.875
train loss:  0.2953675389289856
train gradient:  0.10843194916345854
iteration : 9145
train acc:  0.8671875
train loss:  0.2976400852203369
train gradient:  0.12222594224611662
iteration : 9146
train acc:  0.8359375
train loss:  0.42092010378837585
train gradient:  0.2319005694756182
iteration : 9147
train acc:  0.875
train loss:  0.33299684524536133
train gradient:  0.21523819624067042
iteration : 9148
train acc:  0.7890625
train loss:  0.4335671067237854
train gradient:  0.25215874778201586
iteration : 9149
train acc:  0.859375
train loss:  0.3196355402469635
train gradient:  0.24515940261636626
iteration : 9150
train acc:  0.8203125
train loss:  0.37142378091812134
train gradient:  0.23052683827300538
iteration : 9151
train acc:  0.8984375
train loss:  0.2641373574733734
train gradient:  0.11237045874394905
iteration : 9152
train acc:  0.828125
train loss:  0.4006074070930481
train gradient:  0.24756523126680563
iteration : 9153
train acc:  0.84375
train loss:  0.36736130714416504
train gradient:  0.3000455480523391
iteration : 9154
train acc:  0.8359375
train loss:  0.3320996165275574
train gradient:  0.189515942030361
iteration : 9155
train acc:  0.8984375
train loss:  0.30420663952827454
train gradient:  0.17319526290692983
iteration : 9156
train acc:  0.8515625
train loss:  0.37236881256103516
train gradient:  0.23676497572607896
iteration : 9157
train acc:  0.828125
train loss:  0.3077865242958069
train gradient:  0.1288299521654126
iteration : 9158
train acc:  0.8359375
train loss:  0.406817764043808
train gradient:  0.21528054299225857
iteration : 9159
train acc:  0.84375
train loss:  0.36815720796585083
train gradient:  0.15182509589083976
iteration : 9160
train acc:  0.84375
train loss:  0.4160601496696472
train gradient:  0.22077440570709023
iteration : 9161
train acc:  0.8671875
train loss:  0.2996307611465454
train gradient:  0.1479228489139524
iteration : 9162
train acc:  0.859375
train loss:  0.3574873208999634
train gradient:  0.11758831578911907
iteration : 9163
train acc:  0.9140625
train loss:  0.23938460648059845
train gradient:  0.10855542325935706
iteration : 9164
train acc:  0.8671875
train loss:  0.31905844807624817
train gradient:  0.14822076165249687
iteration : 9165
train acc:  0.8671875
train loss:  0.3408663272857666
train gradient:  0.17412945965991528
iteration : 9166
train acc:  0.8828125
train loss:  0.27938252687454224
train gradient:  0.13614184028909865
iteration : 9167
train acc:  0.8828125
train loss:  0.24007950723171234
train gradient:  0.09288273318504772
iteration : 9168
train acc:  0.8359375
train loss:  0.3402232527732849
train gradient:  0.15348133913540302
iteration : 9169
train acc:  0.8671875
train loss:  0.39190638065338135
train gradient:  0.35079115120336113
iteration : 9170
train acc:  0.8671875
train loss:  0.29864931106567383
train gradient:  0.1609916041874643
iteration : 9171
train acc:  0.8671875
train loss:  0.2576614022254944
train gradient:  0.10083844368108473
iteration : 9172
train acc:  0.828125
train loss:  0.32421672344207764
train gradient:  0.16275185376809592
iteration : 9173
train acc:  0.859375
train loss:  0.3155820667743683
train gradient:  0.17230130332496502
iteration : 9174
train acc:  0.828125
train loss:  0.37779784202575684
train gradient:  0.1590345122042522
iteration : 9175
train acc:  0.875
train loss:  0.3041630983352661
train gradient:  0.2398397248376311
iteration : 9176
train acc:  0.828125
train loss:  0.44977134466171265
train gradient:  0.27145328123730617
iteration : 9177
train acc:  0.8125
train loss:  0.4262670874595642
train gradient:  0.32415584554311694
iteration : 9178
train acc:  0.8359375
train loss:  0.4324977397918701
train gradient:  0.2140747568230303
iteration : 9179
train acc:  0.8125
train loss:  0.46704530715942383
train gradient:  0.3144304911902348
iteration : 9180
train acc:  0.8359375
train loss:  0.35500842332839966
train gradient:  0.19079986510344443
iteration : 9181
train acc:  0.8671875
train loss:  0.28020283579826355
train gradient:  0.10776418521167676
iteration : 9182
train acc:  0.8515625
train loss:  0.33597517013549805
train gradient:  0.20111470311790203
iteration : 9183
train acc:  0.796875
train loss:  0.40995973348617554
train gradient:  0.2613578707719164
iteration : 9184
train acc:  0.8671875
train loss:  0.28731468319892883
train gradient:  0.23173293155882335
iteration : 9185
train acc:  0.84375
train loss:  0.3481842875480652
train gradient:  0.19999941138870903
iteration : 9186
train acc:  0.890625
train loss:  0.27478981018066406
train gradient:  0.11925496352602466
iteration : 9187
train acc:  0.8359375
train loss:  0.40703630447387695
train gradient:  0.2257305798860355
iteration : 9188
train acc:  0.828125
train loss:  0.3660739064216614
train gradient:  0.24103249751322484
iteration : 9189
train acc:  0.8359375
train loss:  0.3654406666755676
train gradient:  0.23187146873705566
iteration : 9190
train acc:  0.859375
train loss:  0.3309594988822937
train gradient:  0.1909007893690834
iteration : 9191
train acc:  0.8203125
train loss:  0.3703790307044983
train gradient:  0.17483597466739045
iteration : 9192
train acc:  0.8125
train loss:  0.3911665678024292
train gradient:  0.19218600960062943
iteration : 9193
train acc:  0.8046875
train loss:  0.39334312081336975
train gradient:  0.2529619589825787
iteration : 9194
train acc:  0.890625
train loss:  0.26195043325424194
train gradient:  0.08739480044963586
iteration : 9195
train acc:  0.8671875
train loss:  0.3010181784629822
train gradient:  0.1027707397921215
iteration : 9196
train acc:  0.8671875
train loss:  0.37050122022628784
train gradient:  0.1712466314439596
iteration : 9197
train acc:  0.8828125
train loss:  0.3482232391834259
train gradient:  0.275371563900497
iteration : 9198
train acc:  0.84375
train loss:  0.3101831376552582
train gradient:  0.19015013562283561
iteration : 9199
train acc:  0.90625
train loss:  0.29076048731803894
train gradient:  0.28321379659688006
iteration : 9200
train acc:  0.8828125
train loss:  0.2816655933856964
train gradient:  0.16053233895994656
iteration : 9201
train acc:  0.8828125
train loss:  0.3060973286628723
train gradient:  0.17397440429982455
iteration : 9202
train acc:  0.8203125
train loss:  0.3422035574913025
train gradient:  0.148330278490868
iteration : 9203
train acc:  0.890625
train loss:  0.28162825107574463
train gradient:  0.10066258053150562
iteration : 9204
train acc:  0.8359375
train loss:  0.3247670531272888
train gradient:  0.141108718801151
iteration : 9205
train acc:  0.90625
train loss:  0.2356417179107666
train gradient:  0.09129833835198024
iteration : 9206
train acc:  0.8203125
train loss:  0.3728080987930298
train gradient:  0.20991079166612098
iteration : 9207
train acc:  0.875
train loss:  0.2884841859340668
train gradient:  0.18325984419135785
iteration : 9208
train acc:  0.9140625
train loss:  0.24720579385757446
train gradient:  0.10938127140026983
iteration : 9209
train acc:  0.8359375
train loss:  0.3720585107803345
train gradient:  0.22492525595611806
iteration : 9210
train acc:  0.8515625
train loss:  0.33652520179748535
train gradient:  0.204733417998641
iteration : 9211
train acc:  0.890625
train loss:  0.3650595545768738
train gradient:  0.2026514166141331
iteration : 9212
train acc:  0.8828125
train loss:  0.3258441090583801
train gradient:  0.18513365359773373
iteration : 9213
train acc:  0.8515625
train loss:  0.3485899567604065
train gradient:  0.19029608417120453
iteration : 9214
train acc:  0.859375
train loss:  0.37432998418807983
train gradient:  0.16369233280677156
iteration : 9215
train acc:  0.84375
train loss:  0.34374547004699707
train gradient:  0.1670724377067892
iteration : 9216
train acc:  0.90625
train loss:  0.27490872144699097
train gradient:  0.13503109632378196
iteration : 9217
train acc:  0.8828125
train loss:  0.38233473896980286
train gradient:  0.1764842117357771
iteration : 9218
train acc:  0.875
train loss:  0.29224908351898193
train gradient:  0.13820323781342014
iteration : 9219
train acc:  0.84375
train loss:  0.34213852882385254
train gradient:  0.15455676582163144
iteration : 9220
train acc:  0.796875
train loss:  0.4169746935367584
train gradient:  0.19025275146905363
iteration : 9221
train acc:  0.8203125
train loss:  0.3517713248729706
train gradient:  0.17771316790130862
iteration : 9222
train acc:  0.8203125
train loss:  0.41712987422943115
train gradient:  0.28761127961736105
iteration : 9223
train acc:  0.84375
train loss:  0.30585527420043945
train gradient:  0.11551103106105264
iteration : 9224
train acc:  0.8359375
train loss:  0.3067373037338257
train gradient:  0.1681009328294609
iteration : 9225
train acc:  0.8125
train loss:  0.43301111459732056
train gradient:  0.29078085905034434
iteration : 9226
train acc:  0.859375
train loss:  0.3409996032714844
train gradient:  0.23191158296812747
iteration : 9227
train acc:  0.78125
train loss:  0.4951706826686859
train gradient:  0.29714577216933924
iteration : 9228
train acc:  0.8125
train loss:  0.382083535194397
train gradient:  0.17671437231368747
iteration : 9229
train acc:  0.8203125
train loss:  0.3981943130493164
train gradient:  0.23515258261927763
iteration : 9230
train acc:  0.8984375
train loss:  0.25421738624572754
train gradient:  0.14653231942324266
iteration : 9231
train acc:  0.875
train loss:  0.33888328075408936
train gradient:  0.20240178764407663
iteration : 9232
train acc:  0.859375
train loss:  0.3142688274383545
train gradient:  0.16211606074610563
iteration : 9233
train acc:  0.8984375
train loss:  0.3182530999183655
train gradient:  0.19874690340665568
iteration : 9234
train acc:  0.8515625
train loss:  0.3401346206665039
train gradient:  0.16161288729089535
iteration : 9235
train acc:  0.8828125
train loss:  0.30115893483161926
train gradient:  0.1672926263854909
iteration : 9236
train acc:  0.890625
train loss:  0.27886271476745605
train gradient:  0.12316258472664501
iteration : 9237
train acc:  0.8828125
train loss:  0.310223251581192
train gradient:  0.19623535991276753
iteration : 9238
train acc:  0.8515625
train loss:  0.3454601764678955
train gradient:  0.17791095538290508
iteration : 9239
train acc:  0.8671875
train loss:  0.30683979392051697
train gradient:  0.14426642496610986
iteration : 9240
train acc:  0.8046875
train loss:  0.40088850259780884
train gradient:  0.21550931009847085
iteration : 9241
train acc:  0.8515625
train loss:  0.3602147102355957
train gradient:  0.21361885579617287
iteration : 9242
train acc:  0.9140625
train loss:  0.2405918538570404
train gradient:  0.16487869031741237
iteration : 9243
train acc:  0.8046875
train loss:  0.3731822669506073
train gradient:  0.18597022531160945
iteration : 9244
train acc:  0.859375
train loss:  0.3203482925891876
train gradient:  0.13845198302118358
iteration : 9245
train acc:  0.8359375
train loss:  0.37404510378837585
train gradient:  0.18865395333078627
iteration : 9246
train acc:  0.8515625
train loss:  0.41518816351890564
train gradient:  0.2218062359715906
iteration : 9247
train acc:  0.7890625
train loss:  0.4177141785621643
train gradient:  0.2492750317747562
iteration : 9248
train acc:  0.921875
train loss:  0.2325432002544403
train gradient:  0.15344244054914968
iteration : 9249
train acc:  0.8828125
train loss:  0.3115249276161194
train gradient:  0.1692401936281786
iteration : 9250
train acc:  0.8125
train loss:  0.38983863592147827
train gradient:  0.2504439767171824
iteration : 9251
train acc:  0.9140625
train loss:  0.3009118437767029
train gradient:  0.17781695397973207
iteration : 9252
train acc:  0.875
train loss:  0.3495323956012726
train gradient:  0.1777785775312848
iteration : 9253
train acc:  0.859375
train loss:  0.35402148962020874
train gradient:  0.14671912346810445
iteration : 9254
train acc:  0.8359375
train loss:  0.33482640981674194
train gradient:  0.14013931723065276
iteration : 9255
train acc:  0.84375
train loss:  0.3129960298538208
train gradient:  0.18691759129964702
iteration : 9256
train acc:  0.8671875
train loss:  0.2942669689655304
train gradient:  0.16255861824132023
iteration : 9257
train acc:  0.8359375
train loss:  0.3042505979537964
train gradient:  0.1284140664026746
iteration : 9258
train acc:  0.8671875
train loss:  0.3083522617816925
train gradient:  0.13464732731365192
iteration : 9259
train acc:  0.8984375
train loss:  0.29402095079421997
train gradient:  0.1199949909409238
iteration : 9260
train acc:  0.828125
train loss:  0.4296374022960663
train gradient:  0.2563201793450864
iteration : 9261
train acc:  0.8125
train loss:  0.4536736011505127
train gradient:  0.21821726633794808
iteration : 9262
train acc:  0.828125
train loss:  0.4453866183757782
train gradient:  0.26529453941466785
iteration : 9263
train acc:  0.890625
train loss:  0.27414146065711975
train gradient:  0.13969429110407786
iteration : 9264
train acc:  0.859375
train loss:  0.30968213081359863
train gradient:  0.12766465233665114
iteration : 9265
train acc:  0.8203125
train loss:  0.36171409487724304
train gradient:  0.1886073861170176
iteration : 9266
train acc:  0.828125
train loss:  0.3450015187263489
train gradient:  0.1562503586208424
iteration : 9267
train acc:  0.84375
train loss:  0.33700090646743774
train gradient:  0.1464228932373426
iteration : 9268
train acc:  0.8984375
train loss:  0.29680532217025757
train gradient:  0.19188339229615956
iteration : 9269
train acc:  0.8515625
train loss:  0.3541308343410492
train gradient:  0.14403384089677423
iteration : 9270
train acc:  0.859375
train loss:  0.3497234284877777
train gradient:  0.23997885413306785
iteration : 9271
train acc:  0.90625
train loss:  0.27226120233535767
train gradient:  0.1388602481180815
iteration : 9272
train acc:  0.890625
train loss:  0.3185306191444397
train gradient:  0.20843672812617076
iteration : 9273
train acc:  0.859375
train loss:  0.33130326867103577
train gradient:  0.19411098471474691
iteration : 9274
train acc:  0.875
train loss:  0.3411228060722351
train gradient:  0.16469956653948897
iteration : 9275
train acc:  0.8828125
train loss:  0.28476837277412415
train gradient:  0.1111330100526901
iteration : 9276
train acc:  0.8515625
train loss:  0.3839298486709595
train gradient:  0.1728857946879862
iteration : 9277
train acc:  0.859375
train loss:  0.3301546275615692
train gradient:  0.12465260360694211
iteration : 9278
train acc:  0.84375
train loss:  0.3815991282463074
train gradient:  0.17736696104776925
iteration : 9279
train acc:  0.859375
train loss:  0.30187883973121643
train gradient:  0.13162169232804216
iteration : 9280
train acc:  0.8359375
train loss:  0.3694124221801758
train gradient:  0.20099315043412208
iteration : 9281
train acc:  0.78125
train loss:  0.43649375438690186
train gradient:  0.39914091900792753
iteration : 9282
train acc:  0.890625
train loss:  0.2930692732334137
train gradient:  0.1759350476385184
iteration : 9283
train acc:  0.859375
train loss:  0.36264553666114807
train gradient:  0.1834641893758805
iteration : 9284
train acc:  0.890625
train loss:  0.2549881339073181
train gradient:  0.10619244447259547
iteration : 9285
train acc:  0.8046875
train loss:  0.44793975353240967
train gradient:  0.28888898618037506
iteration : 9286
train acc:  0.8515625
train loss:  0.3270364999771118
train gradient:  0.12961364723674101
iteration : 9287
train acc:  0.8359375
train loss:  0.37159860134124756
train gradient:  0.16794269239903364
iteration : 9288
train acc:  0.828125
train loss:  0.3885505795478821
train gradient:  0.18276884853218775
iteration : 9289
train acc:  0.8203125
train loss:  0.3737335801124573
train gradient:  0.2553988726183002
iteration : 9290
train acc:  0.84375
train loss:  0.3271789252758026
train gradient:  0.2058952883734486
iteration : 9291
train acc:  0.9140625
train loss:  0.28372639417648315
train gradient:  0.1395948898811945
iteration : 9292
train acc:  0.859375
train loss:  0.36072197556495667
train gradient:  0.2293134653827248
iteration : 9293
train acc:  0.8671875
train loss:  0.37300896644592285
train gradient:  0.15412767845689046
iteration : 9294
train acc:  0.8828125
train loss:  0.30452805757522583
train gradient:  0.18330523738650079
iteration : 9295
train acc:  0.8515625
train loss:  0.3712882697582245
train gradient:  0.16867160549301313
iteration : 9296
train acc:  0.8671875
train loss:  0.363491415977478
train gradient:  0.1995596214968665
iteration : 9297
train acc:  0.921875
train loss:  0.2501087188720703
train gradient:  0.12783675364732927
iteration : 9298
train acc:  0.9296875
train loss:  0.2436743974685669
train gradient:  0.12024724720850427
iteration : 9299
train acc:  0.859375
train loss:  0.3328162133693695
train gradient:  0.15841172851612198
iteration : 9300
train acc:  0.7578125
train loss:  0.47580844163894653
train gradient:  0.3183603472383352
iteration : 9301
train acc:  0.8359375
train loss:  0.38722556829452515
train gradient:  0.19511145825909487
iteration : 9302
train acc:  0.875
train loss:  0.2967127561569214
train gradient:  0.1331422438465868
iteration : 9303
train acc:  0.8203125
train loss:  0.43045032024383545
train gradient:  0.2467812838179879
iteration : 9304
train acc:  0.859375
train loss:  0.3242471218109131
train gradient:  0.23147067911938418
iteration : 9305
train acc:  0.8359375
train loss:  0.3803569972515106
train gradient:  0.2583409656034369
iteration : 9306
train acc:  0.9140625
train loss:  0.30583757162094116
train gradient:  0.12388378343794167
iteration : 9307
train acc:  0.7734375
train loss:  0.4401136040687561
train gradient:  0.2893636330326107
iteration : 9308
train acc:  0.9296875
train loss:  0.2412804663181305
train gradient:  0.17672806442072075
iteration : 9309
train acc:  0.90625
train loss:  0.2764677107334137
train gradient:  0.11287821279420869
iteration : 9310
train acc:  0.8515625
train loss:  0.32220232486724854
train gradient:  0.14932947513946873
iteration : 9311
train acc:  0.8515625
train loss:  0.30101239681243896
train gradient:  0.1290526817103116
iteration : 9312
train acc:  0.8671875
train loss:  0.30541279911994934
train gradient:  0.1309247894706742
iteration : 9313
train acc:  0.8828125
train loss:  0.31052514910697937
train gradient:  0.1153167506520584
iteration : 9314
train acc:  0.875
train loss:  0.3503057360649109
train gradient:  0.1285839528350609
iteration : 9315
train acc:  0.8515625
train loss:  0.3485749363899231
train gradient:  0.14577828558729755
iteration : 9316
train acc:  0.8203125
train loss:  0.43411242961883545
train gradient:  0.20728661343681298
iteration : 9317
train acc:  0.8515625
train loss:  0.37823110818862915
train gradient:  0.18681175709691583
iteration : 9318
train acc:  0.890625
train loss:  0.24858489632606506
train gradient:  0.10690614001009001
iteration : 9319
train acc:  0.8515625
train loss:  0.34262576699256897
train gradient:  0.15017591535865227
iteration : 9320
train acc:  0.8671875
train loss:  0.32422879338264465
train gradient:  0.1638121957074735
iteration : 9321
train acc:  0.8671875
train loss:  0.3342297673225403
train gradient:  0.29183485036959683
iteration : 9322
train acc:  0.8515625
train loss:  0.32304200530052185
train gradient:  0.18799588739908535
iteration : 9323
train acc:  0.8828125
train loss:  0.2616167664527893
train gradient:  0.2604878858527695
iteration : 9324
train acc:  0.8203125
train loss:  0.3298690915107727
train gradient:  0.15005414786776294
iteration : 9325
train acc:  0.84375
train loss:  0.3652302622795105
train gradient:  0.1831881302489553
iteration : 9326
train acc:  0.859375
train loss:  0.3164016008377075
train gradient:  0.12196987163480973
iteration : 9327
train acc:  0.859375
train loss:  0.34308895468711853
train gradient:  0.24943325814008557
iteration : 9328
train acc:  0.875
train loss:  0.29078540205955505
train gradient:  0.12388637023094029
iteration : 9329
train acc:  0.90625
train loss:  0.22941207885742188
train gradient:  0.11596009783760083
iteration : 9330
train acc:  0.84375
train loss:  0.29658305644989014
train gradient:  0.1419887473280698
iteration : 9331
train acc:  0.84375
train loss:  0.3510761260986328
train gradient:  0.18042412754820042
iteration : 9332
train acc:  0.8203125
train loss:  0.37559205293655396
train gradient:  0.2505220342554846
iteration : 9333
train acc:  0.8671875
train loss:  0.34891921281814575
train gradient:  0.21336501470721364
iteration : 9334
train acc:  0.84375
train loss:  0.35009151697158813
train gradient:  0.1847558840672508
iteration : 9335
train acc:  0.859375
train loss:  0.36114978790283203
train gradient:  0.21704735667320582
iteration : 9336
train acc:  0.859375
train loss:  0.33012261986732483
train gradient:  0.22381008473410974
iteration : 9337
train acc:  0.859375
train loss:  0.28563523292541504
train gradient:  0.1438433957883536
iteration : 9338
train acc:  0.8828125
train loss:  0.28661495447158813
train gradient:  0.12998045454581345
iteration : 9339
train acc:  0.8828125
train loss:  0.2854953408241272
train gradient:  0.1937310995514885
iteration : 9340
train acc:  0.84375
train loss:  0.2930266857147217
train gradient:  0.17376252564169897
iteration : 9341
train acc:  0.8984375
train loss:  0.25050485134124756
train gradient:  0.1927546273950752
iteration : 9342
train acc:  0.8359375
train loss:  0.3125094771385193
train gradient:  0.2068912856017434
iteration : 9343
train acc:  0.8203125
train loss:  0.4091857075691223
train gradient:  0.25703592133697944
iteration : 9344
train acc:  0.8984375
train loss:  0.27877941727638245
train gradient:  0.13037861029549422
iteration : 9345
train acc:  0.8125
train loss:  0.3576609194278717
train gradient:  0.2215273143755826
iteration : 9346
train acc:  0.84375
train loss:  0.4585302472114563
train gradient:  0.27703088510310253
iteration : 9347
train acc:  0.8515625
train loss:  0.3470523953437805
train gradient:  0.22499697679813235
iteration : 9348
train acc:  0.828125
train loss:  0.34138596057891846
train gradient:  0.16499160396563495
iteration : 9349
train acc:  0.8828125
train loss:  0.3112160563468933
train gradient:  0.13719747829045703
iteration : 9350
train acc:  0.8828125
train loss:  0.27778297662734985
train gradient:  0.1543215286680925
iteration : 9351
train acc:  0.8984375
train loss:  0.33996349573135376
train gradient:  0.15950689328340087
iteration : 9352
train acc:  0.8828125
train loss:  0.31744861602783203
train gradient:  0.14884738057620214
iteration : 9353
train acc:  0.90625
train loss:  0.2516241669654846
train gradient:  0.11992988050896346
iteration : 9354
train acc:  0.828125
train loss:  0.372164785861969
train gradient:  0.19894660376826243
iteration : 9355
train acc:  0.890625
train loss:  0.30866074562072754
train gradient:  0.1411162989075265
iteration : 9356
train acc:  0.828125
train loss:  0.40853673219680786
train gradient:  0.2959166821730888
iteration : 9357
train acc:  0.796875
train loss:  0.40039607882499695
train gradient:  0.17309230758363597
iteration : 9358
train acc:  0.8359375
train loss:  0.35412928462028503
train gradient:  0.20751816526608163
iteration : 9359
train acc:  0.828125
train loss:  0.38707876205444336
train gradient:  0.237604452906653
iteration : 9360
train acc:  0.8359375
train loss:  0.3348183333873749
train gradient:  0.1474620103293
iteration : 9361
train acc:  0.8359375
train loss:  0.3557351529598236
train gradient:  0.19764007732450778
iteration : 9362
train acc:  0.875
train loss:  0.3267377018928528
train gradient:  0.2692335155982681
iteration : 9363
train acc:  0.8125
train loss:  0.41666436195373535
train gradient:  0.2693886242198538
iteration : 9364
train acc:  0.8515625
train loss:  0.333789587020874
train gradient:  0.12813369603080574
iteration : 9365
train acc:  0.875
train loss:  0.359196275472641
train gradient:  0.16856360506643175
iteration : 9366
train acc:  0.8671875
train loss:  0.3230848014354706
train gradient:  0.12166453993523953
iteration : 9367
train acc:  0.8515625
train loss:  0.3484604060649872
train gradient:  0.1959954463630091
iteration : 9368
train acc:  0.8671875
train loss:  0.27395856380462646
train gradient:  0.15162117386837498
iteration : 9369
train acc:  0.828125
train loss:  0.44114023447036743
train gradient:  0.25650431835459075
iteration : 9370
train acc:  0.8671875
train loss:  0.3067615330219269
train gradient:  0.2420385244203035
iteration : 9371
train acc:  0.8828125
train loss:  0.31813013553619385
train gradient:  0.17520079970730434
iteration : 9372
train acc:  0.8359375
train loss:  0.45255810022354126
train gradient:  0.32277005419855803
iteration : 9373
train acc:  0.890625
train loss:  0.28629356622695923
train gradient:  0.2032430777656643
iteration : 9374
train acc:  0.859375
train loss:  0.33317291736602783
train gradient:  0.1365957851465706
iteration : 9375
train acc:  0.828125
train loss:  0.3639930784702301
train gradient:  0.34127029770169703
iteration : 9376
train acc:  0.8203125
train loss:  0.4089929759502411
train gradient:  0.4178454063599771
iteration : 9377
train acc:  0.8515625
train loss:  0.30433228611946106
train gradient:  0.1573617048311186
iteration : 9378
train acc:  0.84375
train loss:  0.3382894992828369
train gradient:  0.14163986763909664
iteration : 9379
train acc:  0.7890625
train loss:  0.45027875900268555
train gradient:  0.25611474221505687
iteration : 9380
train acc:  0.8125
train loss:  0.4348871409893036
train gradient:  0.3171904788040718
iteration : 9381
train acc:  0.875
train loss:  0.33266162872314453
train gradient:  0.17186915128086358
iteration : 9382
train acc:  0.890625
train loss:  0.30998343229293823
train gradient:  0.14686689533636774
iteration : 9383
train acc:  0.8671875
train loss:  0.3048984110355377
train gradient:  0.13238425073738003
iteration : 9384
train acc:  0.8515625
train loss:  0.3077302575111389
train gradient:  0.09786366136880363
iteration : 9385
train acc:  0.8828125
train loss:  0.28999489545822144
train gradient:  0.13411875032184822
iteration : 9386
train acc:  0.859375
train loss:  0.3139306604862213
train gradient:  0.16038451185153105
iteration : 9387
train acc:  0.859375
train loss:  0.2791980803012848
train gradient:  0.13680234069742297
iteration : 9388
train acc:  0.796875
train loss:  0.3893519937992096
train gradient:  0.18897295732982
iteration : 9389
train acc:  0.8515625
train loss:  0.34318336844444275
train gradient:  0.14165594377172383
iteration : 9390
train acc:  0.8515625
train loss:  0.2984233796596527
train gradient:  0.11204508331273465
iteration : 9391
train acc:  0.8828125
train loss:  0.3127257823944092
train gradient:  0.140992885873287
iteration : 9392
train acc:  0.828125
train loss:  0.37595224380493164
train gradient:  0.2717272151064486
iteration : 9393
train acc:  0.8828125
train loss:  0.31745976209640503
train gradient:  0.15699052216018283
iteration : 9394
train acc:  0.796875
train loss:  0.398182213306427
train gradient:  0.23081742059166754
iteration : 9395
train acc:  0.875
train loss:  0.2973918318748474
train gradient:  0.12595181095211144
iteration : 9396
train acc:  0.8828125
train loss:  0.28740423917770386
train gradient:  0.10782580140979146
iteration : 9397
train acc:  0.8828125
train loss:  0.27307620644569397
train gradient:  0.10950243321108467
iteration : 9398
train acc:  0.828125
train loss:  0.3635462522506714
train gradient:  0.16224780622455492
iteration : 9399
train acc:  0.828125
train loss:  0.3735690116882324
train gradient:  0.1806942916151815
iteration : 9400
train acc:  0.796875
train loss:  0.3810318112373352
train gradient:  0.19737357912511722
iteration : 9401
train acc:  0.8828125
train loss:  0.27857881784439087
train gradient:  0.13408728875699027
iteration : 9402
train acc:  0.90625
train loss:  0.25825726985931396
train gradient:  0.12417574813936995
iteration : 9403
train acc:  0.84375
train loss:  0.312751829624176
train gradient:  0.12255884704185019
iteration : 9404
train acc:  0.828125
train loss:  0.35809656977653503
train gradient:  0.16366157883652122
iteration : 9405
train acc:  0.84375
train loss:  0.34810394048690796
train gradient:  0.16050578002755478
iteration : 9406
train acc:  0.859375
train loss:  0.3290141224861145
train gradient:  0.18373184957439087
iteration : 9407
train acc:  0.8671875
train loss:  0.3160865902900696
train gradient:  0.29336096524040023
iteration : 9408
train acc:  0.84375
train loss:  0.3444916605949402
train gradient:  0.19456691408653615
iteration : 9409
train acc:  0.890625
train loss:  0.26115065813064575
train gradient:  0.1465759168435203
iteration : 9410
train acc:  0.8828125
train loss:  0.3364093005657196
train gradient:  0.1479710742957011
iteration : 9411
train acc:  0.8125
train loss:  0.3362925946712494
train gradient:  0.16560768459117955
iteration : 9412
train acc:  0.859375
train loss:  0.34963589906692505
train gradient:  0.14122518379867127
iteration : 9413
train acc:  0.890625
train loss:  0.2669028043746948
train gradient:  0.1112824546808086
iteration : 9414
train acc:  0.8671875
train loss:  0.28976747393608093
train gradient:  0.12757103396476305
iteration : 9415
train acc:  0.875
train loss:  0.28871041536331177
train gradient:  0.1704948662518905
iteration : 9416
train acc:  0.8515625
train loss:  0.3343547284603119
train gradient:  0.19900060027138447
iteration : 9417
train acc:  0.8359375
train loss:  0.3591093420982361
train gradient:  0.17299157065673873
iteration : 9418
train acc:  0.890625
train loss:  0.2640650272369385
train gradient:  0.12775642446770472
iteration : 9419
train acc:  0.796875
train loss:  0.4325815439224243
train gradient:  0.29819443697538295
iteration : 9420
train acc:  0.8671875
train loss:  0.29886871576309204
train gradient:  0.16774409734160323
iteration : 9421
train acc:  0.8671875
train loss:  0.32811909914016724
train gradient:  0.20535727515117058
iteration : 9422
train acc:  0.8828125
train loss:  0.3052866458892822
train gradient:  0.1427266955291297
iteration : 9423
train acc:  0.890625
train loss:  0.2719790041446686
train gradient:  0.15305425817079255
iteration : 9424
train acc:  0.953125
train loss:  0.1781146377325058
train gradient:  0.08594252215658128
iteration : 9425
train acc:  0.875
train loss:  0.25891590118408203
train gradient:  0.13077331595970737
iteration : 9426
train acc:  0.8828125
train loss:  0.26575061678886414
train gradient:  0.1201269865389
iteration : 9427
train acc:  0.84375
train loss:  0.32863208651542664
train gradient:  0.17176810457482808
iteration : 9428
train acc:  0.84375
train loss:  0.3907984495162964
train gradient:  0.20845135328788952
iteration : 9429
train acc:  0.8671875
train loss:  0.27328747510910034
train gradient:  0.1157039539674107
iteration : 9430
train acc:  0.8125
train loss:  0.3563542366027832
train gradient:  0.18753011205766146
iteration : 9431
train acc:  0.8125
train loss:  0.3350864350795746
train gradient:  0.24616734467272305
iteration : 9432
train acc:  0.875
train loss:  0.30477869510650635
train gradient:  0.15049661115288243
iteration : 9433
train acc:  0.8984375
train loss:  0.2252965122461319
train gradient:  0.08403025926061944
iteration : 9434
train acc:  0.8515625
train loss:  0.31325215101242065
train gradient:  0.14480712913633853
iteration : 9435
train acc:  0.8671875
train loss:  0.3086393475532532
train gradient:  0.12675048228537364
iteration : 9436
train acc:  0.859375
train loss:  0.37863844633102417
train gradient:  0.21579696728125464
iteration : 9437
train acc:  0.875
train loss:  0.27048367261886597
train gradient:  0.15993859823810003
iteration : 9438
train acc:  0.859375
train loss:  0.3117437958717346
train gradient:  0.16899598638210772
iteration : 9439
train acc:  0.8671875
train loss:  0.2745963931083679
train gradient:  0.11769863705274002
iteration : 9440
train acc:  0.859375
train loss:  0.31387996673583984
train gradient:  0.20925836699107944
iteration : 9441
train acc:  0.84375
train loss:  0.3552514910697937
train gradient:  0.26072801650107136
iteration : 9442
train acc:  0.8515625
train loss:  0.3343600332736969
train gradient:  0.18913558813990292
iteration : 9443
train acc:  0.8125
train loss:  0.406207799911499
train gradient:  0.33978351498817444
iteration : 9444
train acc:  0.8671875
train loss:  0.3244866728782654
train gradient:  0.13984308511091478
iteration : 9445
train acc:  0.8515625
train loss:  0.365919828414917
train gradient:  0.1991180660731528
iteration : 9446
train acc:  0.828125
train loss:  0.4078952372074127
train gradient:  0.21071555136970893
iteration : 9447
train acc:  0.8359375
train loss:  0.3460422456264496
train gradient:  0.14762899555511638
iteration : 9448
train acc:  0.875
train loss:  0.28503701090812683
train gradient:  0.09591165068873013
iteration : 9449
train acc:  0.8359375
train loss:  0.34527483582496643
train gradient:  0.17283008214630197
iteration : 9450
train acc:  0.859375
train loss:  0.3138725459575653
train gradient:  0.1541384233957962
iteration : 9451
train acc:  0.8125
train loss:  0.4125678539276123
train gradient:  0.22942036305701147
iteration : 9452
train acc:  0.8828125
train loss:  0.3013598620891571
train gradient:  0.11747830795482549
iteration : 9453
train acc:  0.8671875
train loss:  0.2964956760406494
train gradient:  0.1584329109005102
iteration : 9454
train acc:  0.84375
train loss:  0.3106907606124878
train gradient:  0.10976936551052376
iteration : 9455
train acc:  0.859375
train loss:  0.3605373501777649
train gradient:  0.2198924438729619
iteration : 9456
train acc:  0.8515625
train loss:  0.34053343534469604
train gradient:  0.2236039974077071
iteration : 9457
train acc:  0.8203125
train loss:  0.3904997408390045
train gradient:  0.19127677000215507
iteration : 9458
train acc:  0.890625
train loss:  0.23745085299015045
train gradient:  0.10748209463357111
iteration : 9459
train acc:  0.8828125
train loss:  0.31790417432785034
train gradient:  0.22230233870842997
iteration : 9460
train acc:  0.8671875
train loss:  0.33921879529953003
train gradient:  0.1608195023190746
iteration : 9461
train acc:  0.8359375
train loss:  0.3467196226119995
train gradient:  0.2487748849666461
iteration : 9462
train acc:  0.8671875
train loss:  0.27111223340034485
train gradient:  0.1861597071553964
iteration : 9463
train acc:  0.8984375
train loss:  0.28480467200279236
train gradient:  0.16525694317198397
iteration : 9464
train acc:  0.8671875
train loss:  0.3300081491470337
train gradient:  0.19170236540202337
iteration : 9465
train acc:  0.8125
train loss:  0.4464045763015747
train gradient:  0.2915042372585631
iteration : 9466
train acc:  0.8203125
train loss:  0.3082970976829529
train gradient:  0.1221238084441349
iteration : 9467
train acc:  0.8984375
train loss:  0.31292328238487244
train gradient:  0.13851053340875813
iteration : 9468
train acc:  0.828125
train loss:  0.3135915696620941
train gradient:  0.13799134756841838
iteration : 9469
train acc:  0.890625
train loss:  0.32511061429977417
train gradient:  0.14409854754356918
iteration : 9470
train acc:  0.8671875
train loss:  0.2936900854110718
train gradient:  0.1273874732015106
iteration : 9471
train acc:  0.859375
train loss:  0.3814818263053894
train gradient:  0.2387744333703723
iteration : 9472
train acc:  0.890625
train loss:  0.3000917434692383
train gradient:  0.17592822627176724
iteration : 9473
train acc:  0.7890625
train loss:  0.4131762385368347
train gradient:  0.2549793890578471
iteration : 9474
train acc:  0.8359375
train loss:  0.33572614192962646
train gradient:  0.18746629673407694
iteration : 9475
train acc:  0.8828125
train loss:  0.3226885199546814
train gradient:  0.1571807149255824
iteration : 9476
train acc:  0.84375
train loss:  0.38929659128189087
train gradient:  0.18060618746136492
iteration : 9477
train acc:  0.859375
train loss:  0.2773464620113373
train gradient:  0.1331087209838085
iteration : 9478
train acc:  0.8359375
train loss:  0.3910571336746216
train gradient:  0.4075181724055556
iteration : 9479
train acc:  0.859375
train loss:  0.2955387830734253
train gradient:  0.14463857849568057
iteration : 9480
train acc:  0.8359375
train loss:  0.3805435597896576
train gradient:  0.16361660271964965
iteration : 9481
train acc:  0.859375
train loss:  0.33263644576072693
train gradient:  0.2029993690438559
iteration : 9482
train acc:  0.84375
train loss:  0.3209019899368286
train gradient:  0.14775362458017016
iteration : 9483
train acc:  0.8359375
train loss:  0.4370843470096588
train gradient:  0.23980941260615557
iteration : 9484
train acc:  0.8359375
train loss:  0.3656437397003174
train gradient:  0.21377564895803985
iteration : 9485
train acc:  0.796875
train loss:  0.37966668605804443
train gradient:  0.2252405284894512
iteration : 9486
train acc:  0.8671875
train loss:  0.2726447880268097
train gradient:  0.09312786142980926
iteration : 9487
train acc:  0.7734375
train loss:  0.4618891477584839
train gradient:  0.2835103544272337
iteration : 9488
train acc:  0.9140625
train loss:  0.2829893231391907
train gradient:  0.1168176536755589
iteration : 9489
train acc:  0.859375
train loss:  0.3119671940803528
train gradient:  0.18335161741850145
iteration : 9490
train acc:  0.859375
train loss:  0.30010971426963806
train gradient:  0.13862418189583275
iteration : 9491
train acc:  0.8671875
train loss:  0.34886032342910767
train gradient:  0.1450149880028121
iteration : 9492
train acc:  0.8203125
train loss:  0.35832083225250244
train gradient:  0.1702115257903144
iteration : 9493
train acc:  0.8515625
train loss:  0.32438522577285767
train gradient:  0.17581919443015376
iteration : 9494
train acc:  0.8671875
train loss:  0.28843915462493896
train gradient:  0.14227893786695842
iteration : 9495
train acc:  0.859375
train loss:  0.3815334737300873
train gradient:  0.1876126651088937
iteration : 9496
train acc:  0.7890625
train loss:  0.39531588554382324
train gradient:  0.22342511818926902
iteration : 9497
train acc:  0.8515625
train loss:  0.3398555517196655
train gradient:  0.2756734232999179
iteration : 9498
train acc:  0.8828125
train loss:  0.29068535566329956
train gradient:  0.1302690655098414
iteration : 9499
train acc:  0.8671875
train loss:  0.28574317693710327
train gradient:  0.16021018543184568
iteration : 9500
train acc:  0.828125
train loss:  0.39215219020843506
train gradient:  0.18648424530822222
iteration : 9501
train acc:  0.8828125
train loss:  0.3191855847835541
train gradient:  0.17588858516322436
iteration : 9502
train acc:  0.8125
train loss:  0.38706856966018677
train gradient:  0.1894576149160374
iteration : 9503
train acc:  0.7734375
train loss:  0.48570263385772705
train gradient:  0.3608232710038314
iteration : 9504
train acc:  0.84375
train loss:  0.3334391713142395
train gradient:  0.1954303353478737
iteration : 9505
train acc:  0.8671875
train loss:  0.28904157876968384
train gradient:  0.18073604937643795
iteration : 9506
train acc:  0.8203125
train loss:  0.3778843581676483
train gradient:  0.2446179028996005
iteration : 9507
train acc:  0.8203125
train loss:  0.39284583926200867
train gradient:  0.2214941428162952
iteration : 9508
train acc:  0.8671875
train loss:  0.31741195917129517
train gradient:  0.19392280820618424
iteration : 9509
train acc:  0.859375
train loss:  0.30188286304473877
train gradient:  0.2304590730551913
iteration : 9510
train acc:  0.8046875
train loss:  0.42445048689842224
train gradient:  0.2937527952853206
iteration : 9511
train acc:  0.8828125
train loss:  0.2746134400367737
train gradient:  0.1689243382574364
iteration : 9512
train acc:  0.90625
train loss:  0.24385163187980652
train gradient:  0.10990274063978678
iteration : 9513
train acc:  0.8828125
train loss:  0.2733190059661865
train gradient:  0.1081692655004902
iteration : 9514
train acc:  0.9140625
train loss:  0.2705128788948059
train gradient:  0.15058765343389174
iteration : 9515
train acc:  0.8828125
train loss:  0.310372531414032
train gradient:  0.12566986447333928
iteration : 9516
train acc:  0.8203125
train loss:  0.32166630029678345
train gradient:  0.200864843132119
iteration : 9517
train acc:  0.8671875
train loss:  0.32732829451560974
train gradient:  0.1362568395572027
iteration : 9518
train acc:  0.8828125
train loss:  0.27206099033355713
train gradient:  0.1324856316714051
iteration : 9519
train acc:  0.84375
train loss:  0.3902224898338318
train gradient:  0.22905012930362736
iteration : 9520
train acc:  0.90625
train loss:  0.249795600771904
train gradient:  0.17671043208631843
iteration : 9521
train acc:  0.84375
train loss:  0.3057032823562622
train gradient:  0.16623862774645448
iteration : 9522
train acc:  0.8203125
train loss:  0.3483927845954895
train gradient:  0.2091768988132353
iteration : 9523
train acc:  0.8515625
train loss:  0.3687576353549957
train gradient:  0.2154374896698898
iteration : 9524
train acc:  0.90625
train loss:  0.299466073513031
train gradient:  0.16846396916180909
iteration : 9525
train acc:  0.8828125
train loss:  0.31665006279945374
train gradient:  0.32624369802985403
iteration : 9526
train acc:  0.8046875
train loss:  0.34395962953567505
train gradient:  0.22201608508032983
iteration : 9527
train acc:  0.859375
train loss:  0.34671831130981445
train gradient:  0.20686696628528134
iteration : 9528
train acc:  0.8671875
train loss:  0.29976409673690796
train gradient:  0.19788680111639045
iteration : 9529
train acc:  0.859375
train loss:  0.31265944242477417
train gradient:  0.23162896916309705
iteration : 9530
train acc:  0.8125
train loss:  0.33535605669021606
train gradient:  0.22819010150191538
iteration : 9531
train acc:  0.8671875
train loss:  0.3026828169822693
train gradient:  0.14180196638762008
iteration : 9532
train acc:  0.84375
train loss:  0.3662221431732178
train gradient:  0.21357019118565512
iteration : 9533
train acc:  0.890625
train loss:  0.27361053228378296
train gradient:  0.14788664551889133
iteration : 9534
train acc:  0.875
train loss:  0.2955244183540344
train gradient:  0.13408823592339686
iteration : 9535
train acc:  0.9296875
train loss:  0.24242918193340302
train gradient:  0.10115317873674444
iteration : 9536
train acc:  0.8671875
train loss:  0.2913743555545807
train gradient:  0.14948289883638466
iteration : 9537
train acc:  0.828125
train loss:  0.3765295445919037
train gradient:  0.238600745062379
iteration : 9538
train acc:  0.8984375
train loss:  0.28947651386260986
train gradient:  0.13847216593126213
iteration : 9539
train acc:  0.859375
train loss:  0.31888917088508606
train gradient:  0.16244855082894244
iteration : 9540
train acc:  0.8984375
train loss:  0.2499276101589203
train gradient:  0.10097448737453725
iteration : 9541
train acc:  0.8046875
train loss:  0.36316126585006714
train gradient:  0.27402167888958345
iteration : 9542
train acc:  0.84375
train loss:  0.36418652534484863
train gradient:  0.2712057313329434
iteration : 9543
train acc:  0.8046875
train loss:  0.40399861335754395
train gradient:  0.22466047323216082
iteration : 9544
train acc:  0.84375
train loss:  0.35038548707962036
train gradient:  0.17664596210321598
iteration : 9545
train acc:  0.8515625
train loss:  0.3980060815811157
train gradient:  0.2682517439356987
iteration : 9546
train acc:  0.828125
train loss:  0.33145368099212646
train gradient:  0.16385509280206706
iteration : 9547
train acc:  0.875
train loss:  0.2967396676540375
train gradient:  0.17356863661871397
iteration : 9548
train acc:  0.8828125
train loss:  0.2909671664237976
train gradient:  0.14108248694015169
iteration : 9549
train acc:  0.8671875
train loss:  0.2899136245250702
train gradient:  0.13330945296521773
iteration : 9550
train acc:  0.890625
train loss:  0.2981542944908142
train gradient:  0.12393053943822702
iteration : 9551
train acc:  0.84375
train loss:  0.36596977710723877
train gradient:  0.3200976526595374
iteration : 9552
train acc:  0.8828125
train loss:  0.2512953281402588
train gradient:  0.14093351978855906
iteration : 9553
train acc:  0.921875
train loss:  0.26158735156059265
train gradient:  0.13783543429573172
iteration : 9554
train acc:  0.8359375
train loss:  0.3952985405921936
train gradient:  0.19183128606418265
iteration : 9555
train acc:  0.8671875
train loss:  0.29753512144088745
train gradient:  0.12341080252494108
iteration : 9556
train acc:  0.859375
train loss:  0.2860042452812195
train gradient:  0.14863897403453713
iteration : 9557
train acc:  0.9140625
train loss:  0.27700352668762207
train gradient:  0.19208356697596823
iteration : 9558
train acc:  0.8984375
train loss:  0.2798706889152527
train gradient:  0.28153345867630486
iteration : 9559
train acc:  0.8828125
train loss:  0.2551516890525818
train gradient:  0.15757882445938393
iteration : 9560
train acc:  0.8203125
train loss:  0.3915395736694336
train gradient:  0.22803145073118994
iteration : 9561
train acc:  0.875
train loss:  0.29103919863700867
train gradient:  0.5994426447761609
iteration : 9562
train acc:  0.859375
train loss:  0.33350926637649536
train gradient:  0.25188007838388354
iteration : 9563
train acc:  0.859375
train loss:  0.3496641516685486
train gradient:  0.21789038053402768
iteration : 9564
train acc:  0.8671875
train loss:  0.33614712953567505
train gradient:  0.16755048950159135
iteration : 9565
train acc:  0.8125
train loss:  0.42630884051322937
train gradient:  0.2856663862033599
iteration : 9566
train acc:  0.8984375
train loss:  0.2772032916545868
train gradient:  0.14249471890355747
iteration : 9567
train acc:  0.8359375
train loss:  0.3663349747657776
train gradient:  0.33242673864270644
iteration : 9568
train acc:  0.921875
train loss:  0.24255333840847015
train gradient:  0.109401940234153
iteration : 9569
train acc:  0.859375
train loss:  0.3343983292579651
train gradient:  0.2032728851853907
iteration : 9570
train acc:  0.765625
train loss:  0.4584893584251404
train gradient:  0.35662791343322015
iteration : 9571
train acc:  0.8828125
train loss:  0.3210769593715668
train gradient:  0.16430304701818366
iteration : 9572
train acc:  0.8359375
train loss:  0.36240172386169434
train gradient:  0.21307600807235227
iteration : 9573
train acc:  0.875
train loss:  0.28979992866516113
train gradient:  0.12339905563661718
iteration : 9574
train acc:  0.828125
train loss:  0.3013628125190735
train gradient:  0.15683173511284892
iteration : 9575
train acc:  0.84375
train loss:  0.3087497353553772
train gradient:  0.13391765153643936
iteration : 9576
train acc:  0.84375
train loss:  0.2723764181137085
train gradient:  0.10842045258289185
iteration : 9577
train acc:  0.875
train loss:  0.3310322165489197
train gradient:  0.19891128991741475
iteration : 9578
train acc:  0.8203125
train loss:  0.34525632858276367
train gradient:  0.20644800070659547
iteration : 9579
train acc:  0.8671875
train loss:  0.3022459149360657
train gradient:  0.1619048612681643
iteration : 9580
train acc:  0.8203125
train loss:  0.3990709185600281
train gradient:  0.4196266050208295
iteration : 9581
train acc:  0.828125
train loss:  0.3520185947418213
train gradient:  0.21807466194959974
iteration : 9582
train acc:  0.8984375
train loss:  0.27746281027793884
train gradient:  0.1634402021081913
iteration : 9583
train acc:  0.8359375
train loss:  0.34920352697372437
train gradient:  0.28950083822679595
iteration : 9584
train acc:  0.8203125
train loss:  0.381065309047699
train gradient:  0.20303558010862036
iteration : 9585
train acc:  0.90625
train loss:  0.26227840781211853
train gradient:  0.1821827015341601
iteration : 9586
train acc:  0.8359375
train loss:  0.3390330374240875
train gradient:  0.16606562879262735
iteration : 9587
train acc:  0.8515625
train loss:  0.3762935400009155
train gradient:  0.2213225520562343
iteration : 9588
train acc:  0.8359375
train loss:  0.37601715326309204
train gradient:  0.3018377094592749
iteration : 9589
train acc:  0.84375
train loss:  0.3872610330581665
train gradient:  0.22825049053874125
iteration : 9590
train acc:  0.859375
train loss:  0.2969808578491211
train gradient:  0.227843398004691
iteration : 9591
train acc:  0.8515625
train loss:  0.3042161464691162
train gradient:  0.17989608517428027
iteration : 9592
train acc:  0.8828125
train loss:  0.27536147832870483
train gradient:  0.11163471615873753
iteration : 9593
train acc:  0.8671875
train loss:  0.31204646825790405
train gradient:  0.12186188948841414
iteration : 9594
train acc:  0.8828125
train loss:  0.27409568428993225
train gradient:  0.24760952643133743
iteration : 9595
train acc:  0.890625
train loss:  0.30358195304870605
train gradient:  0.1384861707451429
iteration : 9596
train acc:  0.8203125
train loss:  0.5258827209472656
train gradient:  0.39281452535433686
iteration : 9597
train acc:  0.8828125
train loss:  0.2470645010471344
train gradient:  0.13401742038308012
iteration : 9598
train acc:  0.8515625
train loss:  0.32398098707199097
train gradient:  0.19021426106728423
iteration : 9599
train acc:  0.90625
train loss:  0.25404050946235657
train gradient:  0.15084561911515734
iteration : 9600
train acc:  0.796875
train loss:  0.3833314776420593
train gradient:  0.3116099093503369
iteration : 9601
train acc:  0.8125
train loss:  0.40577787160873413
train gradient:  0.2809971418247311
iteration : 9602
train acc:  0.8359375
train loss:  0.348916620016098
train gradient:  0.18728729305185524
iteration : 9603
train acc:  0.8515625
train loss:  0.3110364079475403
train gradient:  0.1265602818767702
iteration : 9604
train acc:  0.8125
train loss:  0.4232542812824249
train gradient:  0.24371385999628797
iteration : 9605
train acc:  0.875
train loss:  0.2716478109359741
train gradient:  0.12850066717938802
iteration : 9606
train acc:  0.84375
train loss:  0.3318719267845154
train gradient:  0.20478828241037647
iteration : 9607
train acc:  0.8515625
train loss:  0.3615474998950958
train gradient:  0.2174893286327337
iteration : 9608
train acc:  0.859375
train loss:  0.3069547116756439
train gradient:  0.1728427777806048
iteration : 9609
train acc:  0.890625
train loss:  0.2696892023086548
train gradient:  0.12308864690317842
iteration : 9610
train acc:  0.8046875
train loss:  0.464325487613678
train gradient:  0.24340987610132137
iteration : 9611
train acc:  0.8671875
train loss:  0.36579447984695435
train gradient:  0.22863679250172408
iteration : 9612
train acc:  0.8515625
train loss:  0.3024733066558838
train gradient:  0.18350886104941355
iteration : 9613
train acc:  0.859375
train loss:  0.33244889974594116
train gradient:  0.1579648127688135
iteration : 9614
train acc:  0.8671875
train loss:  0.28846415877342224
train gradient:  0.1565138953316955
iteration : 9615
train acc:  0.796875
train loss:  0.39128440618515015
train gradient:  0.21085505854920517
iteration : 9616
train acc:  0.8828125
train loss:  0.3048108220100403
train gradient:  0.1503167577464114
iteration : 9617
train acc:  0.8671875
train loss:  0.37561577558517456
train gradient:  0.20560542359726422
iteration : 9618
train acc:  0.8984375
train loss:  0.3035465478897095
train gradient:  0.16770715867900032
iteration : 9619
train acc:  0.84375
train loss:  0.29770052433013916
train gradient:  0.1352200503437099
iteration : 9620
train acc:  0.8515625
train loss:  0.31363850831985474
train gradient:  0.200738968891281
iteration : 9621
train acc:  0.875
train loss:  0.28492578864097595
train gradient:  0.147890918678057
iteration : 9622
train acc:  0.84375
train loss:  0.35753872990608215
train gradient:  0.19365112222086905
iteration : 9623
train acc:  0.8984375
train loss:  0.260789692401886
train gradient:  0.13053203073292247
iteration : 9624
train acc:  0.859375
train loss:  0.29620277881622314
train gradient:  0.13428898116240473
iteration : 9625
train acc:  0.8359375
train loss:  0.40027156472206116
train gradient:  0.19496732300850317
iteration : 9626
train acc:  0.8046875
train loss:  0.360535591840744
train gradient:  0.2231990080550767
iteration : 9627
train acc:  0.8828125
train loss:  0.26428765058517456
train gradient:  0.13328676385566118
iteration : 9628
train acc:  0.84375
train loss:  0.3812008500099182
train gradient:  0.21667557771275253
iteration : 9629
train acc:  0.8125
train loss:  0.3648727238178253
train gradient:  0.18194900977886616
iteration : 9630
train acc:  0.859375
train loss:  0.28201285004615784
train gradient:  0.1461278949804939
iteration : 9631
train acc:  0.890625
train loss:  0.25771594047546387
train gradient:  0.13737060125865141
iteration : 9632
train acc:  0.796875
train loss:  0.45526283979415894
train gradient:  0.2667505972469689
iteration : 9633
train acc:  0.8515625
train loss:  0.3091376721858978
train gradient:  0.18428720928803602
iteration : 9634
train acc:  0.84375
train loss:  0.35915952920913696
train gradient:  0.21348611547601154
iteration : 9635
train acc:  0.828125
train loss:  0.380531907081604
train gradient:  0.22206854014910526
iteration : 9636
train acc:  0.8515625
train loss:  0.34333088994026184
train gradient:  0.1710595554542142
iteration : 9637
train acc:  0.8359375
train loss:  0.32990413904190063
train gradient:  0.13725177331145264
iteration : 9638
train acc:  0.875
train loss:  0.318545401096344
train gradient:  0.18052409971983302
iteration : 9639
train acc:  0.890625
train loss:  0.2879430055618286
train gradient:  0.11296678135577204
iteration : 9640
train acc:  0.84375
train loss:  0.3475185036659241
train gradient:  0.1143617436299488
iteration : 9641
train acc:  0.921875
train loss:  0.23301605880260468
train gradient:  0.12744763512103177
iteration : 9642
train acc:  0.875
train loss:  0.354467511177063
train gradient:  0.14964543176178274
iteration : 9643
train acc:  0.921875
train loss:  0.27908945083618164
train gradient:  0.15814799667986923
iteration : 9644
train acc:  0.8671875
train loss:  0.30700406432151794
train gradient:  0.17365797950150746
iteration : 9645
train acc:  0.8671875
train loss:  0.3587753176689148
train gradient:  0.19087618238011517
iteration : 9646
train acc:  0.890625
train loss:  0.24763646721839905
train gradient:  0.1360388299753125
iteration : 9647
train acc:  0.8828125
train loss:  0.31565138697624207
train gradient:  0.1545701659968759
iteration : 9648
train acc:  0.8515625
train loss:  0.3720824122428894
train gradient:  0.18184643106505916
iteration : 9649
train acc:  0.84375
train loss:  0.33097389340400696
train gradient:  0.29188971300730204
iteration : 9650
train acc:  0.8125
train loss:  0.43476036190986633
train gradient:  0.2625872714470344
iteration : 9651
train acc:  0.8984375
train loss:  0.284446656703949
train gradient:  0.1625293904973425
iteration : 9652
train acc:  0.8359375
train loss:  0.3449057340621948
train gradient:  0.17939733030346836
iteration : 9653
train acc:  0.84375
train loss:  0.36079975962638855
train gradient:  0.23707705407710267
iteration : 9654
train acc:  0.8984375
train loss:  0.2753157317638397
train gradient:  0.13747900039291505
iteration : 9655
train acc:  0.859375
train loss:  0.32993918657302856
train gradient:  0.19253517511909185
iteration : 9656
train acc:  0.8671875
train loss:  0.2989782691001892
train gradient:  0.11260427631534026
iteration : 9657
train acc:  0.8203125
train loss:  0.3355379104614258
train gradient:  0.24279730463047441
iteration : 9658
train acc:  0.859375
train loss:  0.32514792680740356
train gradient:  0.19823703939707338
iteration : 9659
train acc:  0.84375
train loss:  0.3428369462490082
train gradient:  0.19412549604920254
iteration : 9660
train acc:  0.8359375
train loss:  0.39865657687187195
train gradient:  0.23688866576606415
iteration : 9661
train acc:  0.8203125
train loss:  0.34927624464035034
train gradient:  0.2746990613760707
iteration : 9662
train acc:  0.8671875
train loss:  0.2829146981239319
train gradient:  0.17167875788751386
iteration : 9663
train acc:  0.8359375
train loss:  0.3140527009963989
train gradient:  0.2545702979350505
iteration : 9664
train acc:  0.890625
train loss:  0.2915734052658081
train gradient:  0.15898784226958446
iteration : 9665
train acc:  0.8671875
train loss:  0.3352005183696747
train gradient:  0.19388752235163936
iteration : 9666
train acc:  0.8828125
train loss:  0.31407374143600464
train gradient:  0.15600327472419084
iteration : 9667
train acc:  0.875
train loss:  0.31663164496421814
train gradient:  0.18522058342706316
iteration : 9668
train acc:  0.90625
train loss:  0.2539634704589844
train gradient:  0.11705422759395612
iteration : 9669
train acc:  0.890625
train loss:  0.3358096480369568
train gradient:  0.15486073115813798
iteration : 9670
train acc:  0.8203125
train loss:  0.31286418437957764
train gradient:  0.15765716507089686
iteration : 9671
train acc:  0.7890625
train loss:  0.3812512755393982
train gradient:  0.260421359334811
iteration : 9672
train acc:  0.8125
train loss:  0.36410462856292725
train gradient:  0.16153850070029502
iteration : 9673
train acc:  0.9140625
train loss:  0.25547438859939575
train gradient:  0.10700156607031795
iteration : 9674
train acc:  0.8359375
train loss:  0.4185178279876709
train gradient:  0.2595509648952501
iteration : 9675
train acc:  0.84375
train loss:  0.34232380986213684
train gradient:  0.20189073720781453
iteration : 9676
train acc:  0.8828125
train loss:  0.33958137035369873
train gradient:  0.18331214485250252
iteration : 9677
train acc:  0.7734375
train loss:  0.3744012117385864
train gradient:  0.25627162179702856
iteration : 9678
train acc:  0.8515625
train loss:  0.2934313714504242
train gradient:  0.14594623789402728
iteration : 9679
train acc:  0.84375
train loss:  0.31448209285736084
train gradient:  0.1987581604039259
iteration : 9680
train acc:  0.8671875
train loss:  0.2892146110534668
train gradient:  0.16538521961628772
iteration : 9681
train acc:  0.890625
train loss:  0.2850775122642517
train gradient:  0.17599041725616998
iteration : 9682
train acc:  0.859375
train loss:  0.3398873209953308
train gradient:  0.17190713629443405
iteration : 9683
train acc:  0.8671875
train loss:  0.3005017638206482
train gradient:  0.14681163204289163
iteration : 9684
train acc:  0.90625
train loss:  0.2686472535133362
train gradient:  0.11912264920643771
iteration : 9685
train acc:  0.8515625
train loss:  0.3623506426811218
train gradient:  0.16838388400321827
iteration : 9686
train acc:  0.8828125
train loss:  0.27273422479629517
train gradient:  0.11702088770492555
iteration : 9687
train acc:  0.875
train loss:  0.3332936763763428
train gradient:  0.15773238460762226
iteration : 9688
train acc:  0.8515625
train loss:  0.3546864688396454
train gradient:  0.27414003547253457
iteration : 9689
train acc:  0.890625
train loss:  0.24461397528648376
train gradient:  0.13801466484727576
iteration : 9690
train acc:  0.8046875
train loss:  0.375542551279068
train gradient:  0.22321450190795844
iteration : 9691
train acc:  0.859375
train loss:  0.39310121536254883
train gradient:  0.3312987419387534
iteration : 9692
train acc:  0.875
train loss:  0.2954666316509247
train gradient:  0.12714595701013784
iteration : 9693
train acc:  0.9140625
train loss:  0.25887662172317505
train gradient:  0.1077681778664872
iteration : 9694
train acc:  0.890625
train loss:  0.277758926153183
train gradient:  0.11963190789571503
iteration : 9695
train acc:  0.796875
train loss:  0.4435058832168579
train gradient:  0.3266923754134859
iteration : 9696
train acc:  0.9375
train loss:  0.2183164656162262
train gradient:  0.1033578793208571
iteration : 9697
train acc:  0.859375
train loss:  0.3310178816318512
train gradient:  0.24046122468880532
iteration : 9698
train acc:  0.8203125
train loss:  0.35314181447029114
train gradient:  0.2649248938032934
iteration : 9699
train acc:  0.8515625
train loss:  0.29575371742248535
train gradient:  0.1663219194386196
iteration : 9700
train acc:  0.8671875
train loss:  0.31187331676483154
train gradient:  0.16218635211479437
iteration : 9701
train acc:  0.8359375
train loss:  0.34589511156082153
train gradient:  0.1939631046114408
iteration : 9702
train acc:  0.90625
train loss:  0.28069421648979187
train gradient:  0.14467989115893565
iteration : 9703
train acc:  0.8515625
train loss:  0.32773905992507935
train gradient:  0.15496682027428077
iteration : 9704
train acc:  0.8671875
train loss:  0.2821120619773865
train gradient:  0.16194493474527233
iteration : 9705
train acc:  0.8671875
train loss:  0.2956172823905945
train gradient:  0.18055570486991257
iteration : 9706
train acc:  0.828125
train loss:  0.3504113256931305
train gradient:  0.18241839131879156
iteration : 9707
train acc:  0.8359375
train loss:  0.4184684753417969
train gradient:  0.30755334065874673
iteration : 9708
train acc:  0.875
train loss:  0.2823622226715088
train gradient:  0.18422957659332337
iteration : 9709
train acc:  0.7890625
train loss:  0.46329063177108765
train gradient:  0.2531032204128649
iteration : 9710
train acc:  0.8671875
train loss:  0.2774621546268463
train gradient:  0.17233265651804863
iteration : 9711
train acc:  0.8203125
train loss:  0.4134256839752197
train gradient:  0.264752022969839
iteration : 9712
train acc:  0.8515625
train loss:  0.34144502878189087
train gradient:  0.24119020941392066
iteration : 9713
train acc:  0.84375
train loss:  0.2858189344406128
train gradient:  0.15650709338819443
iteration : 9714
train acc:  0.8125
train loss:  0.3674381673336029
train gradient:  0.25203616125869394
iteration : 9715
train acc:  0.8046875
train loss:  0.4380231499671936
train gradient:  0.31135801571285454
iteration : 9716
train acc:  0.859375
train loss:  0.2738867402076721
train gradient:  0.14559038960637083
iteration : 9717
train acc:  0.8359375
train loss:  0.37803399562835693
train gradient:  0.219906041274466
iteration : 9718
train acc:  0.8671875
train loss:  0.27166634798049927
train gradient:  0.12420561093916939
iteration : 9719
train acc:  0.875
train loss:  0.3112221360206604
train gradient:  0.16490346858656316
iteration : 9720
train acc:  0.84375
train loss:  0.3061600923538208
train gradient:  0.17722266598440667
iteration : 9721
train acc:  0.8984375
train loss:  0.25306594371795654
train gradient:  0.11790990031641377
iteration : 9722
train acc:  0.8515625
train loss:  0.3308212459087372
train gradient:  0.21058936270492468
iteration : 9723
train acc:  0.8515625
train loss:  0.3290892541408539
train gradient:  0.19323012243573623
iteration : 9724
train acc:  0.8359375
train loss:  0.34742382168769836
train gradient:  0.19122506791214733
iteration : 9725
train acc:  0.8515625
train loss:  0.3577416241168976
train gradient:  0.20233798085064267
iteration : 9726
train acc:  0.84375
train loss:  0.3517735004425049
train gradient:  0.2530544297388263
iteration : 9727
train acc:  0.8515625
train loss:  0.34987014532089233
train gradient:  0.30896175424261557
iteration : 9728
train acc:  0.8203125
train loss:  0.34586775302886963
train gradient:  0.16600192042136525
iteration : 9729
train acc:  0.8359375
train loss:  0.3558330237865448
train gradient:  0.17701743548097343
iteration : 9730
train acc:  0.8671875
train loss:  0.3243499994277954
train gradient:  0.1870778653917266
iteration : 9731
train acc:  0.859375
train loss:  0.2848612070083618
train gradient:  0.119612532624551
iteration : 9732
train acc:  0.9140625
train loss:  0.2854801118373871
train gradient:  0.10876706798224978
iteration : 9733
train acc:  0.875
train loss:  0.2530987858772278
train gradient:  0.12042438029390903
iteration : 9734
train acc:  0.84375
train loss:  0.35707223415374756
train gradient:  0.15415438220409555
iteration : 9735
train acc:  0.8671875
train loss:  0.2998908460140228
train gradient:  0.14891236845861228
iteration : 9736
train acc:  0.828125
train loss:  0.36334919929504395
train gradient:  0.1993073101880195
iteration : 9737
train acc:  0.84375
train loss:  0.3368178904056549
train gradient:  0.16489481470377726
iteration : 9738
train acc:  0.84375
train loss:  0.2973499298095703
train gradient:  0.2095240836591759
iteration : 9739
train acc:  0.8515625
train loss:  0.3274809718132019
train gradient:  0.19136860605353015
iteration : 9740
train acc:  0.8203125
train loss:  0.3458186388015747
train gradient:  0.17477645874041497
iteration : 9741
train acc:  0.890625
train loss:  0.27920180559158325
train gradient:  0.10910057173418934
iteration : 9742
train acc:  0.875
train loss:  0.3113933205604553
train gradient:  0.18407496220689906
iteration : 9743
train acc:  0.859375
train loss:  0.3335636258125305
train gradient:  0.19212209729782054
iteration : 9744
train acc:  0.8828125
train loss:  0.2541992664337158
train gradient:  0.10382246705988758
iteration : 9745
train acc:  0.8046875
train loss:  0.35266444087028503
train gradient:  0.24230153436652058
iteration : 9746
train acc:  0.8046875
train loss:  0.3891502916812897
train gradient:  0.23011458707483648
iteration : 9747
train acc:  0.8046875
train loss:  0.3860301375389099
train gradient:  0.210260817123184
iteration : 9748
train acc:  0.8203125
train loss:  0.3509768843650818
train gradient:  0.17763718204242004
iteration : 9749
train acc:  0.8671875
train loss:  0.3186562657356262
train gradient:  0.1672385826714055
iteration : 9750
train acc:  0.875
train loss:  0.33375710248947144
train gradient:  0.1611799304941338
iteration : 9751
train acc:  0.8125
train loss:  0.33325791358947754
train gradient:  0.173396510981521
iteration : 9752
train acc:  0.890625
train loss:  0.27907872200012207
train gradient:  0.11205975551537536
iteration : 9753
train acc:  0.921875
train loss:  0.24130122363567352
train gradient:  0.10971768503760193
iteration : 9754
train acc:  0.8515625
train loss:  0.31683287024497986
train gradient:  0.15751338634968914
iteration : 9755
train acc:  0.8359375
train loss:  0.3881838917732239
train gradient:  0.23511540867994496
iteration : 9756
train acc:  0.828125
train loss:  0.38500142097473145
train gradient:  0.2221328568801324
iteration : 9757
train acc:  0.84375
train loss:  0.3463480472564697
train gradient:  0.26994743768539536
iteration : 9758
train acc:  0.859375
train loss:  0.3717195391654968
train gradient:  0.23886981456971063
iteration : 9759
train acc:  0.8203125
train loss:  0.3408493101596832
train gradient:  0.17848995744308385
iteration : 9760
train acc:  0.8671875
train loss:  0.25605350732803345
train gradient:  0.15480033302327068
iteration : 9761
train acc:  0.8828125
train loss:  0.2870306670665741
train gradient:  0.1308141050549604
iteration : 9762
train acc:  0.8984375
train loss:  0.31270110607147217
train gradient:  0.1504602235472916
iteration : 9763
train acc:  0.8359375
train loss:  0.3623303174972534
train gradient:  0.24213083750502734
iteration : 9764
train acc:  0.8671875
train loss:  0.3633221387863159
train gradient:  0.1950195730190215
iteration : 9765
train acc:  0.859375
train loss:  0.2980901598930359
train gradient:  0.1412749378347088
iteration : 9766
train acc:  0.8515625
train loss:  0.30437108874320984
train gradient:  0.3087810572498149
iteration : 9767
train acc:  0.84375
train loss:  0.3209625482559204
train gradient:  0.15854799636385056
iteration : 9768
train acc:  0.8359375
train loss:  0.35282254219055176
train gradient:  0.15714135526977535
iteration : 9769
train acc:  0.8125
train loss:  0.4258950352668762
train gradient:  0.27615757540958896
iteration : 9770
train acc:  0.8515625
train loss:  0.3052900731563568
train gradient:  0.14949472113786616
iteration : 9771
train acc:  0.8515625
train loss:  0.3016475439071655
train gradient:  0.15131993102482072
iteration : 9772
train acc:  0.875
train loss:  0.3601034879684448
train gradient:  0.18795813658297622
iteration : 9773
train acc:  0.828125
train loss:  0.31124192476272583
train gradient:  0.1359045827142068
iteration : 9774
train acc:  0.8671875
train loss:  0.29510340094566345
train gradient:  0.11497243864168033
iteration : 9775
train acc:  0.8515625
train loss:  0.33023178577423096
train gradient:  0.20424192093679444
iteration : 9776
train acc:  0.875
train loss:  0.31618553400039673
train gradient:  0.15788476549092068
iteration : 9777
train acc:  0.8828125
train loss:  0.32869112491607666
train gradient:  0.17561900803936947
iteration : 9778
train acc:  0.7734375
train loss:  0.40156295895576477
train gradient:  0.23111922897452208
iteration : 9779
train acc:  0.828125
train loss:  0.3403918147087097
train gradient:  0.1965162893880997
iteration : 9780
train acc:  0.84375
train loss:  0.31728294491767883
train gradient:  0.15183551608772602
iteration : 9781
train acc:  0.8515625
train loss:  0.3298814296722412
train gradient:  0.1412368952136569
iteration : 9782
train acc:  0.8359375
train loss:  0.326221764087677
train gradient:  0.14432559767221884
iteration : 9783
train acc:  0.84375
train loss:  0.3306981921195984
train gradient:  0.1802867575392102
iteration : 9784
train acc:  0.8671875
train loss:  0.2799408435821533
train gradient:  0.12362330893677002
iteration : 9785
train acc:  0.8828125
train loss:  0.3112703561782837
train gradient:  0.17705409058927363
iteration : 9786
train acc:  0.890625
train loss:  0.303907573223114
train gradient:  0.18353012941731084
iteration : 9787
train acc:  0.8125
train loss:  0.40451082587242126
train gradient:  0.2268208809381569
iteration : 9788
train acc:  0.8515625
train loss:  0.44752949476242065
train gradient:  0.2649909965826614
iteration : 9789
train acc:  0.90625
train loss:  0.26667460799217224
train gradient:  0.18695193314998548
iteration : 9790
train acc:  0.84375
train loss:  0.35741740465164185
train gradient:  0.17339805157146143
iteration : 9791
train acc:  0.8671875
train loss:  0.3201327919960022
train gradient:  0.22839366245460552
iteration : 9792
train acc:  0.828125
train loss:  0.34559768438339233
train gradient:  0.18857169799662493
iteration : 9793
train acc:  0.8203125
train loss:  0.3685484528541565
train gradient:  0.29993294179574237
iteration : 9794
train acc:  0.890625
train loss:  0.27617737650871277
train gradient:  0.15142296501678731
iteration : 9795
train acc:  0.84375
train loss:  0.25595688819885254
train gradient:  0.11485865983257762
iteration : 9796
train acc:  0.8515625
train loss:  0.3400315046310425
train gradient:  0.1725721423187947
iteration : 9797
train acc:  0.8125
train loss:  0.3589479327201843
train gradient:  0.15969518968315233
iteration : 9798
train acc:  0.8203125
train loss:  0.43491747975349426
train gradient:  0.28941571721653336
iteration : 9799
train acc:  0.8203125
train loss:  0.37255626916885376
train gradient:  0.18920691235054343
iteration : 9800
train acc:  0.8515625
train loss:  0.3117506504058838
train gradient:  0.24178400286982843
iteration : 9801
train acc:  0.8203125
train loss:  0.39153075218200684
train gradient:  0.171481915721648
iteration : 9802
train acc:  0.8984375
train loss:  0.255869597196579
train gradient:  0.10659294590586535
iteration : 9803
train acc:  0.7890625
train loss:  0.3916327953338623
train gradient:  0.2191479229768952
iteration : 9804
train acc:  0.8515625
train loss:  0.3008023798465729
train gradient:  0.14414631535252065
iteration : 9805
train acc:  0.84375
train loss:  0.32791152596473694
train gradient:  0.238799431788188
iteration : 9806
train acc:  0.859375
train loss:  0.3412833511829376
train gradient:  0.169993850146908
iteration : 9807
train acc:  0.8515625
train loss:  0.30799680948257446
train gradient:  0.14416672665978703
iteration : 9808
train acc:  0.859375
train loss:  0.3039451837539673
train gradient:  0.16775765274350507
iteration : 9809
train acc:  0.8515625
train loss:  0.4096807837486267
train gradient:  0.20893103070232089
iteration : 9810
train acc:  0.828125
train loss:  0.35235080122947693
train gradient:  0.17193142378028436
iteration : 9811
train acc:  0.90625
train loss:  0.26966261863708496
train gradient:  0.09590026211389191
iteration : 9812
train acc:  0.859375
train loss:  0.26491695642471313
train gradient:  0.10999362616401548
iteration : 9813
train acc:  0.90625
train loss:  0.23695166409015656
train gradient:  0.09505255101506405
iteration : 9814
train acc:  0.875
train loss:  0.27217811346054077
train gradient:  0.10776274175265714
iteration : 9815
train acc:  0.8515625
train loss:  0.32827281951904297
train gradient:  0.21137389417102928
iteration : 9816
train acc:  0.828125
train loss:  0.34374743700027466
train gradient:  0.2239435991326374
iteration : 9817
train acc:  0.8359375
train loss:  0.33870741724967957
train gradient:  0.1881820297300637
iteration : 9818
train acc:  0.8828125
train loss:  0.26626425981521606
train gradient:  0.14156761101456478
iteration : 9819
train acc:  0.859375
train loss:  0.3284148573875427
train gradient:  0.1448116884746981
iteration : 9820
train acc:  0.84375
train loss:  0.2983459234237671
train gradient:  0.1120655508138033
iteration : 9821
train acc:  0.8359375
train loss:  0.35634908080101013
train gradient:  0.16686201582402777
iteration : 9822
train acc:  0.859375
train loss:  0.3266581892967224
train gradient:  0.1592631843773551
iteration : 9823
train acc:  0.8828125
train loss:  0.29171985387802124
train gradient:  0.12500174369936734
iteration : 9824
train acc:  0.859375
train loss:  0.33279532194137573
train gradient:  0.21603049039490296
iteration : 9825
train acc:  0.8515625
train loss:  0.28804874420166016
train gradient:  0.16399504687416172
iteration : 9826
train acc:  0.8671875
train loss:  0.3176119327545166
train gradient:  0.1316429802354328
iteration : 9827
train acc:  0.84375
train loss:  0.36222416162490845
train gradient:  0.33416174280291333
iteration : 9828
train acc:  0.8828125
train loss:  0.2771803140640259
train gradient:  0.09269826779177606
iteration : 9829
train acc:  0.8671875
train loss:  0.3772747218608856
train gradient:  0.22571695389469099
iteration : 9830
train acc:  0.90625
train loss:  0.3139110207557678
train gradient:  0.18005486342360785
iteration : 9831
train acc:  0.8125
train loss:  0.40662166476249695
train gradient:  0.26007664979167744
iteration : 9832
train acc:  0.875
train loss:  0.32607221603393555
train gradient:  0.1550143992351996
iteration : 9833
train acc:  0.859375
train loss:  0.31040072441101074
train gradient:  0.12313846489402383
iteration : 9834
train acc:  0.8671875
train loss:  0.2985001802444458
train gradient:  0.14958035480469045
iteration : 9835
train acc:  0.8046875
train loss:  0.42147573828697205
train gradient:  0.20479134767939983
iteration : 9836
train acc:  0.8359375
train loss:  0.33605706691741943
train gradient:  0.18470316861570296
iteration : 9837
train acc:  0.8203125
train loss:  0.37982046604156494
train gradient:  0.24479351949743366
iteration : 9838
train acc:  0.875
train loss:  0.33236825466156006
train gradient:  0.192351162435157
iteration : 9839
train acc:  0.8671875
train loss:  0.3186999559402466
train gradient:  0.1640144093820069
iteration : 9840
train acc:  0.859375
train loss:  0.3088846802711487
train gradient:  0.15067435029956067
iteration : 9841
train acc:  0.875
train loss:  0.2915557622909546
train gradient:  0.14450787563591455
iteration : 9842
train acc:  0.84375
train loss:  0.30892571806907654
train gradient:  0.23217920133907638
iteration : 9843
train acc:  0.8203125
train loss:  0.3838821053504944
train gradient:  0.20140786510155717
iteration : 9844
train acc:  0.8046875
train loss:  0.3636741638183594
train gradient:  0.21844828125358168
iteration : 9845
train acc:  0.859375
train loss:  0.30787381529808044
train gradient:  0.10814887092351738
iteration : 9846
train acc:  0.859375
train loss:  0.31655940413475037
train gradient:  0.13214802368288384
iteration : 9847
train acc:  0.8359375
train loss:  0.41144847869873047
train gradient:  0.18470428488065238
iteration : 9848
train acc:  0.859375
train loss:  0.30246663093566895
train gradient:  0.13549800906978118
iteration : 9849
train acc:  0.8671875
train loss:  0.29256996512413025
train gradient:  0.1330002568500253
iteration : 9850
train acc:  0.8359375
train loss:  0.36227357387542725
train gradient:  0.21115835680638637
iteration : 9851
train acc:  0.859375
train loss:  0.322978138923645
train gradient:  0.1705802221803641
iteration : 9852
train acc:  0.84375
train loss:  0.3459128439426422
train gradient:  0.18121362514593295
iteration : 9853
train acc:  0.828125
train loss:  0.3508337736129761
train gradient:  0.1970183121386319
iteration : 9854
train acc:  0.8671875
train loss:  0.3078141212463379
train gradient:  0.18177397222382657
iteration : 9855
train acc:  0.859375
train loss:  0.3301754295825958
train gradient:  0.1392519265417999
iteration : 9856
train acc:  0.90625
train loss:  0.2814878225326538
train gradient:  0.13610228853342432
iteration : 9857
train acc:  0.8203125
train loss:  0.3564114570617676
train gradient:  0.26341782250610957
iteration : 9858
train acc:  0.84375
train loss:  0.3067224621772766
train gradient:  0.15357147244616004
iteration : 9859
train acc:  0.859375
train loss:  0.3643608093261719
train gradient:  0.17518381330680743
iteration : 9860
train acc:  0.859375
train loss:  0.326342910528183
train gradient:  0.09826411743508648
iteration : 9861
train acc:  0.8359375
train loss:  0.33210065960884094
train gradient:  0.16349780112549228
iteration : 9862
train acc:  0.8359375
train loss:  0.3158944845199585
train gradient:  0.14299427432084433
iteration : 9863
train acc:  0.84375
train loss:  0.34633180499076843
train gradient:  0.23537575038710906
iteration : 9864
train acc:  0.8359375
train loss:  0.3453432321548462
train gradient:  0.15696991052455267
iteration : 9865
train acc:  0.84375
train loss:  0.32525861263275146
train gradient:  0.16431538061278464
iteration : 9866
train acc:  0.859375
train loss:  0.3648536205291748
train gradient:  0.16231005509446272
iteration : 9867
train acc:  0.90625
train loss:  0.262137234210968
train gradient:  0.13429170376056881
iteration : 9868
train acc:  0.8671875
train loss:  0.357013463973999
train gradient:  0.22316615950753538
iteration : 9869
train acc:  0.8828125
train loss:  0.28809553384780884
train gradient:  0.09149388766385229
iteration : 9870
train acc:  0.8671875
train loss:  0.32303622364997864
train gradient:  0.153699425432767
iteration : 9871
train acc:  0.8046875
train loss:  0.3968466818332672
train gradient:  0.26590741491014513
iteration : 9872
train acc:  0.8671875
train loss:  0.3991585969924927
train gradient:  0.20615341334261494
iteration : 9873
train acc:  0.8671875
train loss:  0.29590314626693726
train gradient:  0.09349976629807097
iteration : 9874
train acc:  0.8359375
train loss:  0.37241217494010925
train gradient:  0.37043733612242064
iteration : 9875
train acc:  0.8046875
train loss:  0.4259629547595978
train gradient:  0.20540186132775598
iteration : 9876
train acc:  0.8359375
train loss:  0.3174361288547516
train gradient:  0.14932430106669026
iteration : 9877
train acc:  0.875
train loss:  0.2894620895385742
train gradient:  0.14428123131392442
iteration : 9878
train acc:  0.8671875
train loss:  0.3148820400238037
train gradient:  0.11709752294698599
iteration : 9879
train acc:  0.828125
train loss:  0.3372783064842224
train gradient:  0.16801062390453053
iteration : 9880
train acc:  0.8671875
train loss:  0.2896682918071747
train gradient:  0.12470552553812356
iteration : 9881
train acc:  0.8828125
train loss:  0.266589879989624
train gradient:  0.17736270300524337
iteration : 9882
train acc:  0.8046875
train loss:  0.41888630390167236
train gradient:  0.22359835394735084
iteration : 9883
train acc:  0.8671875
train loss:  0.3186562657356262
train gradient:  0.18489066958699907
iteration : 9884
train acc:  0.859375
train loss:  0.3087497353553772
train gradient:  0.14713620515013384
iteration : 9885
train acc:  0.8515625
train loss:  0.3153843283653259
train gradient:  0.14033212234254275
iteration : 9886
train acc:  0.8515625
train loss:  0.31830012798309326
train gradient:  0.13337554661069723
iteration : 9887
train acc:  0.859375
train loss:  0.3164662718772888
train gradient:  0.1256386321632477
iteration : 9888
train acc:  0.9140625
train loss:  0.3194151520729065
train gradient:  0.29525119603794187
iteration : 9889
train acc:  0.8203125
train loss:  0.3722637891769409
train gradient:  0.1849783110955185
iteration : 9890
train acc:  0.8203125
train loss:  0.35574406385421753
train gradient:  0.2186817202990255
iteration : 9891
train acc:  0.8125
train loss:  0.4556177258491516
train gradient:  0.24844387881397983
iteration : 9892
train acc:  0.8828125
train loss:  0.36760789155960083
train gradient:  0.15547709000526133
iteration : 9893
train acc:  0.875
train loss:  0.32895705103874207
train gradient:  0.14785382031212543
iteration : 9894
train acc:  0.8203125
train loss:  0.33230510354042053
train gradient:  0.21138206210447388
iteration : 9895
train acc:  0.859375
train loss:  0.37935370206832886
train gradient:  0.20508501234257778
iteration : 9896
train acc:  0.890625
train loss:  0.25815728306770325
train gradient:  0.11791665230191745
iteration : 9897
train acc:  0.875
train loss:  0.31801077723503113
train gradient:  0.13035257637990447
iteration : 9898
train acc:  0.8359375
train loss:  0.37080085277557373
train gradient:  0.17152368832526138
iteration : 9899
train acc:  0.8359375
train loss:  0.3830474317073822
train gradient:  0.17326108326215856
iteration : 9900
train acc:  0.8359375
train loss:  0.30744484066963196
train gradient:  0.182406292370011
iteration : 9901
train acc:  0.875
train loss:  0.3031077980995178
train gradient:  0.14550852427008848
iteration : 9902
train acc:  0.8359375
train loss:  0.3391600251197815
train gradient:  0.16985771654390852
iteration : 9903
train acc:  0.875
train loss:  0.3025687038898468
train gradient:  0.152264147829897
iteration : 9904
train acc:  0.8828125
train loss:  0.3059670329093933
train gradient:  0.12449496323636758
iteration : 9905
train acc:  0.8515625
train loss:  0.3554861843585968
train gradient:  0.17222678971738903
iteration : 9906
train acc:  0.875
train loss:  0.3068948984146118
train gradient:  0.1268099246174496
iteration : 9907
train acc:  0.90625
train loss:  0.26824989914894104
train gradient:  0.12570605478876457
iteration : 9908
train acc:  0.8671875
train loss:  0.2936493158340454
train gradient:  0.14324954547608515
iteration : 9909
train acc:  0.890625
train loss:  0.257203608751297
train gradient:  0.1445138668419532
iteration : 9910
train acc:  0.890625
train loss:  0.31397414207458496
train gradient:  0.11902528248880048
iteration : 9911
train acc:  0.859375
train loss:  0.33911070227622986
train gradient:  0.21979064636735346
iteration : 9912
train acc:  0.90625
train loss:  0.23594577610492706
train gradient:  0.12372463170814205
iteration : 9913
train acc:  0.875
train loss:  0.3422483205795288
train gradient:  0.14338912905242712
iteration : 9914
train acc:  0.796875
train loss:  0.4370702803134918
train gradient:  0.3221748506206412
iteration : 9915
train acc:  0.8671875
train loss:  0.3388526737689972
train gradient:  0.18279501387802594
iteration : 9916
train acc:  0.84375
train loss:  0.4355300962924957
train gradient:  0.3579697153268134
iteration : 9917
train acc:  0.859375
train loss:  0.31565651297569275
train gradient:  0.12513879346156428
iteration : 9918
train acc:  0.8984375
train loss:  0.2785506248474121
train gradient:  0.12727031749296963
iteration : 9919
train acc:  0.8984375
train loss:  0.3086300790309906
train gradient:  0.15996601797345728
iteration : 9920
train acc:  0.875
train loss:  0.28876402974128723
train gradient:  0.10855420772704315
iteration : 9921
train acc:  0.8515625
train loss:  0.4300689995288849
train gradient:  0.33533247336569816
iteration : 9922
train acc:  0.84375
train loss:  0.3491160273551941
train gradient:  0.18843290855987588
iteration : 9923
train acc:  0.8984375
train loss:  0.31077656149864197
train gradient:  0.1176463930680639
iteration : 9924
train acc:  0.875
train loss:  0.30195629596710205
train gradient:  0.13849913527447444
iteration : 9925
train acc:  0.8515625
train loss:  0.3333921730518341
train gradient:  0.18368188635916788
iteration : 9926
train acc:  0.828125
train loss:  0.38489314913749695
train gradient:  0.23552557951520242
iteration : 9927
train acc:  0.796875
train loss:  0.465781033039093
train gradient:  0.28975515263347795
iteration : 9928
train acc:  0.859375
train loss:  0.3164125084877014
train gradient:  0.15639978334948548
iteration : 9929
train acc:  0.828125
train loss:  0.3571430444717407
train gradient:  0.18759991614042615
iteration : 9930
train acc:  0.8203125
train loss:  0.3641480505466461
train gradient:  0.26330477021715504
iteration : 9931
train acc:  0.8359375
train loss:  0.40766263008117676
train gradient:  0.23237590206194048
iteration : 9932
train acc:  0.8984375
train loss:  0.256306529045105
train gradient:  0.12408309877743563
iteration : 9933
train acc:  0.8125
train loss:  0.35237473249435425
train gradient:  0.1846679674291674
iteration : 9934
train acc:  0.875
train loss:  0.3140169680118561
train gradient:  0.16697609345475084
iteration : 9935
train acc:  0.8359375
train loss:  0.3770917057991028
train gradient:  0.25414498587512935
iteration : 9936
train acc:  0.78125
train loss:  0.42640221118927
train gradient:  0.3256062140981678
iteration : 9937
train acc:  0.890625
train loss:  0.2729184925556183
train gradient:  0.11644877920495175
iteration : 9938
train acc:  0.8828125
train loss:  0.2988237738609314
train gradient:  0.1425350379577257
iteration : 9939
train acc:  0.828125
train loss:  0.486131876707077
train gradient:  0.2458369559358672
iteration : 9940
train acc:  0.796875
train loss:  0.40313389897346497
train gradient:  0.19944626699109083
iteration : 9941
train acc:  0.828125
train loss:  0.3378903865814209
train gradient:  0.16116454868717459
iteration : 9942
train acc:  0.8515625
train loss:  0.34996575117111206
train gradient:  0.1415708441634964
iteration : 9943
train acc:  0.8515625
train loss:  0.339280366897583
train gradient:  0.18501114732678361
iteration : 9944
train acc:  0.8125
train loss:  0.4068908095359802
train gradient:  0.2614645126244117
iteration : 9945
train acc:  0.8828125
train loss:  0.31470561027526855
train gradient:  0.14549142609999408
iteration : 9946
train acc:  0.8984375
train loss:  0.3250678777694702
train gradient:  0.15399902123221776
iteration : 9947
train acc:  0.8515625
train loss:  0.3916597068309784
train gradient:  0.22687899602803405
iteration : 9948
train acc:  0.796875
train loss:  0.3556540310382843
train gradient:  0.16762401434246565
iteration : 9949
train acc:  0.859375
train loss:  0.3549960255622864
train gradient:  0.2087366314659726
iteration : 9950
train acc:  0.84375
train loss:  0.3663109540939331
train gradient:  0.15560895124558766
iteration : 9951
train acc:  0.859375
train loss:  0.3027425706386566
train gradient:  0.19365195167198526
iteration : 9952
train acc:  0.8125
train loss:  0.38097208738327026
train gradient:  0.20314774679402942
iteration : 9953
train acc:  0.8828125
train loss:  0.32177555561065674
train gradient:  0.2083643020325322
iteration : 9954
train acc:  0.8828125
train loss:  0.23276785016059875
train gradient:  0.09110927612864134
iteration : 9955
train acc:  0.8671875
train loss:  0.2923329174518585
train gradient:  0.12821115790678803
iteration : 9956
train acc:  0.8828125
train loss:  0.2725536525249481
train gradient:  0.1242207498968903
iteration : 9957
train acc:  0.859375
train loss:  0.34500542283058167
train gradient:  0.154020848458361
iteration : 9958
train acc:  0.875
train loss:  0.2922188639640808
train gradient:  0.12579175601021703
iteration : 9959
train acc:  0.859375
train loss:  0.30369168519973755
train gradient:  0.14579159838094166
iteration : 9960
train acc:  0.8828125
train loss:  0.25143375992774963
train gradient:  0.09940095645020416
iteration : 9961
train acc:  0.859375
train loss:  0.3017210066318512
train gradient:  0.1659434430933931
iteration : 9962
train acc:  0.859375
train loss:  0.35588154196739197
train gradient:  0.15674630459728248
iteration : 9963
train acc:  0.8828125
train loss:  0.2872580885887146
train gradient:  0.11240538273301355
iteration : 9964
train acc:  0.859375
train loss:  0.3079783320426941
train gradient:  0.14173589658963692
iteration : 9965
train acc:  0.875
train loss:  0.26214340329170227
train gradient:  0.10095800490641331
iteration : 9966
train acc:  0.890625
train loss:  0.25873810052871704
train gradient:  0.11800731790636493
iteration : 9967
train acc:  0.890625
train loss:  0.2807837128639221
train gradient:  0.1437303798843783
iteration : 9968
train acc:  0.875
train loss:  0.3127852976322174
train gradient:  0.14582129021757184
iteration : 9969
train acc:  0.8828125
train loss:  0.24920226633548737
train gradient:  0.16703043104947835
iteration : 9970
train acc:  0.859375
train loss:  0.2742844820022583
train gradient:  0.13702585826926006
iteration : 9971
train acc:  0.8828125
train loss:  0.2957904040813446
train gradient:  0.1292506898997938
iteration : 9972
train acc:  0.8203125
train loss:  0.31225937604904175
train gradient:  0.15142633471869507
iteration : 9973
train acc:  0.8203125
train loss:  0.3453235924243927
train gradient:  0.19402988698049214
iteration : 9974
train acc:  0.8515625
train loss:  0.3569633960723877
train gradient:  0.20844826389148102
iteration : 9975
train acc:  0.8828125
train loss:  0.30196452140808105
train gradient:  0.17882873640586122
iteration : 9976
train acc:  0.84375
train loss:  0.3058256506919861
train gradient:  0.12676778975226805
iteration : 9977
train acc:  0.8671875
train loss:  0.311750590801239
train gradient:  0.15677525095992323
iteration : 9978
train acc:  0.8671875
train loss:  0.2705621123313904
train gradient:  0.12391727627839506
iteration : 9979
train acc:  0.828125
train loss:  0.3522588908672333
train gradient:  0.1889396716938699
iteration : 9980
train acc:  0.890625
train loss:  0.28162190318107605
train gradient:  0.17827812368602766
iteration : 9981
train acc:  0.8515625
train loss:  0.34227073192596436
train gradient:  0.18680353426533367
iteration : 9982
train acc:  0.8359375
train loss:  0.3103659749031067
train gradient:  0.29075308706277575
iteration : 9983
train acc:  0.8125
train loss:  0.3507756292819977
train gradient:  0.17383999997258778
iteration : 9984
train acc:  0.859375
train loss:  0.3309944272041321
train gradient:  0.2152524116446587
iteration : 9985
train acc:  0.890625
train loss:  0.2828965485095978
train gradient:  0.13646533413844397
iteration : 9986
train acc:  0.8671875
train loss:  0.25474679470062256
train gradient:  0.09796409413106168
iteration : 9987
train acc:  0.875
train loss:  0.3030620813369751
train gradient:  0.1143709683014285
iteration : 9988
train acc:  0.875
train loss:  0.2943798899650574
train gradient:  0.1517974369599199
iteration : 9989
train acc:  0.84375
train loss:  0.3369656205177307
train gradient:  0.14163812883617422
iteration : 9990
train acc:  0.890625
train loss:  0.2865981459617615
train gradient:  0.17278208382136598
iteration : 9991
train acc:  0.8203125
train loss:  0.3672786355018616
train gradient:  0.3141480661094678
iteration : 9992
train acc:  0.921875
train loss:  0.22124305367469788
train gradient:  0.10494779147203696
iteration : 9993
train acc:  0.8828125
train loss:  0.3020528554916382
train gradient:  0.13235956945369193
iteration : 9994
train acc:  0.8515625
train loss:  0.3412230610847473
train gradient:  0.18058905314056517
iteration : 9995
train acc:  0.8203125
train loss:  0.3989962339401245
train gradient:  0.2907946550051902
iteration : 9996
train acc:  0.828125
train loss:  0.36676260828971863
train gradient:  0.18401394223806816
iteration : 9997
train acc:  0.8671875
train loss:  0.3397844135761261
train gradient:  0.19211128033342603
iteration : 9998
train acc:  0.8671875
train loss:  0.3062220811843872
train gradient:  0.1371365471930589
iteration : 9999
train acc:  0.8671875
train loss:  0.37867534160614014
train gradient:  0.230293167861095
iteration : 10000
train acc:  0.8359375
train loss:  0.3247707188129425
train gradient:  0.14706408870532126
iteration : 10001
train acc:  0.7734375
train loss:  0.5012259483337402
train gradient:  0.36156796508045846
iteration : 10002
train acc:  0.8046875
train loss:  0.439869225025177
train gradient:  0.27128553446942755
iteration : 10003
train acc:  0.828125
train loss:  0.3995257616043091
train gradient:  0.30868312214451243
iteration : 10004
train acc:  0.8125
train loss:  0.37560832500457764
train gradient:  0.15039328804092947
iteration : 10005
train acc:  0.875
train loss:  0.2909596264362335
train gradient:  0.10149791169521265
iteration : 10006
train acc:  0.8828125
train loss:  0.2709631323814392
train gradient:  0.11001507779204896
iteration : 10007
train acc:  0.8671875
train loss:  0.32824796438217163
train gradient:  0.20084274647178885
iteration : 10008
train acc:  0.8671875
train loss:  0.36299562454223633
train gradient:  0.1525101288314234
iteration : 10009
train acc:  0.890625
train loss:  0.2938186228275299
train gradient:  0.15767865099877038
iteration : 10010
train acc:  0.84375
train loss:  0.3118631839752197
train gradient:  0.1252733313923371
iteration : 10011
train acc:  0.859375
train loss:  0.34993505477905273
train gradient:  0.18089155281428904
iteration : 10012
train acc:  0.8515625
train loss:  0.28326621651649475
train gradient:  0.11765356036057494
iteration : 10013
train acc:  0.8359375
train loss:  0.35566410422325134
train gradient:  0.15978654738074932
iteration : 10014
train acc:  0.8359375
train loss:  0.3310142159461975
train gradient:  0.1450113539928502
iteration : 10015
train acc:  0.84375
train loss:  0.32473164796829224
train gradient:  0.16669225300853796
iteration : 10016
train acc:  0.796875
train loss:  0.3517361879348755
train gradient:  0.16260437347807968
iteration : 10017
train acc:  0.8671875
train loss:  0.276431143283844
train gradient:  0.09419590422612575
iteration : 10018
train acc:  0.828125
train loss:  0.3480599820613861
train gradient:  0.18678123513430805
iteration : 10019
train acc:  0.8359375
train loss:  0.36876124143600464
train gradient:  0.2296407356856665
iteration : 10020
train acc:  0.921875
train loss:  0.2507992386817932
train gradient:  0.11444739731891708
iteration : 10021
train acc:  0.8828125
train loss:  0.2978018522262573
train gradient:  0.1452002161206633
iteration : 10022
train acc:  0.8984375
train loss:  0.27070415019989014
train gradient:  0.1299268601681972
iteration : 10023
train acc:  0.8984375
train loss:  0.24779048562049866
train gradient:  0.13276175360852985
iteration : 10024
train acc:  0.8828125
train loss:  0.25524625182151794
train gradient:  0.12087795639657296
iteration : 10025
train acc:  0.859375
train loss:  0.35788866877555847
train gradient:  0.17412773141925464
iteration : 10026
train acc:  0.8671875
train loss:  0.3625468909740448
train gradient:  0.20862341700368126
iteration : 10027
train acc:  0.875
train loss:  0.35382527112960815
train gradient:  0.20147846881300888
iteration : 10028
train acc:  0.890625
train loss:  0.3284947872161865
train gradient:  0.12894398791034994
iteration : 10029
train acc:  0.8515625
train loss:  0.2989753782749176
train gradient:  0.14428745896192624
iteration : 10030
train acc:  0.84375
train loss:  0.2965630292892456
train gradient:  0.14374228271889267
iteration : 10031
train acc:  0.8359375
train loss:  0.3548726737499237
train gradient:  0.3152635062284427
iteration : 10032
train acc:  0.8515625
train loss:  0.34593522548675537
train gradient:  0.13803473162771512
iteration : 10033
train acc:  0.8671875
train loss:  0.3266810476779938
train gradient:  0.16898730686127258
iteration : 10034
train acc:  0.84375
train loss:  0.2963327467441559
train gradient:  0.15637710848140907
iteration : 10035
train acc:  0.8515625
train loss:  0.333583801984787
train gradient:  0.1346856295141796
iteration : 10036
train acc:  0.8828125
train loss:  0.2962573766708374
train gradient:  0.11753115002165063
iteration : 10037
train acc:  0.8203125
train loss:  0.3830406367778778
train gradient:  0.3192116949260928
iteration : 10038
train acc:  0.859375
train loss:  0.3152647018432617
train gradient:  0.20649610943081076
iteration : 10039
train acc:  0.8671875
train loss:  0.3221019506454468
train gradient:  0.18677652927374222
iteration : 10040
train acc:  0.859375
train loss:  0.27683109045028687
train gradient:  0.17701460679430894
iteration : 10041
train acc:  0.875
train loss:  0.31398656964302063
train gradient:  0.18037041143819388
iteration : 10042
train acc:  0.84375
train loss:  0.2905735373497009
train gradient:  0.11237902869038217
iteration : 10043
train acc:  0.859375
train loss:  0.3380053639411926
train gradient:  0.1860016058086262
iteration : 10044
train acc:  0.890625
train loss:  0.3124690055847168
train gradient:  0.1481594259372464
iteration : 10045
train acc:  0.8359375
train loss:  0.3501269221305847
train gradient:  0.19042322573933423
iteration : 10046
train acc:  0.859375
train loss:  0.34511664509773254
train gradient:  0.21877777665607862
iteration : 10047
train acc:  0.84375
train loss:  0.3405916392803192
train gradient:  0.1800587042563986
iteration : 10048
train acc:  0.8828125
train loss:  0.3087531626224518
train gradient:  0.11319095782797772
iteration : 10049
train acc:  0.828125
train loss:  0.3925032615661621
train gradient:  0.2940642453163155
iteration : 10050
train acc:  0.890625
train loss:  0.3016452193260193
train gradient:  0.09403093114293637
iteration : 10051
train acc:  0.828125
train loss:  0.4086924195289612
train gradient:  0.234524757566297
iteration : 10052
train acc:  0.8671875
train loss:  0.3755543828010559
train gradient:  0.16766744232709244
iteration : 10053
train acc:  0.9140625
train loss:  0.2705991566181183
train gradient:  0.15644768670642173
iteration : 10054
train acc:  0.8671875
train loss:  0.37980926036834717
train gradient:  0.17713194297054896
iteration : 10055
train acc:  0.875
train loss:  0.26946479082107544
train gradient:  0.08678562761608297
iteration : 10056
train acc:  0.828125
train loss:  0.4126083254814148
train gradient:  0.18011428362176263
iteration : 10057
train acc:  0.90625
train loss:  0.2544604539871216
train gradient:  0.10074565954964701
iteration : 10058
train acc:  0.84375
train loss:  0.4071691036224365
train gradient:  0.24127839060091688
iteration : 10059
train acc:  0.859375
train loss:  0.3061031699180603
train gradient:  0.1553443758713498
iteration : 10060
train acc:  0.8984375
train loss:  0.29627397656440735
train gradient:  0.13113441898667536
iteration : 10061
train acc:  0.84375
train loss:  0.3400319814682007
train gradient:  0.11799796593358405
iteration : 10062
train acc:  0.859375
train loss:  0.32469892501831055
train gradient:  0.1294376574633786
iteration : 10063
train acc:  0.8671875
train loss:  0.3335410952568054
train gradient:  0.12560610468357078
iteration : 10064
train acc:  0.8828125
train loss:  0.286683589220047
train gradient:  0.13461634270722
iteration : 10065
train acc:  0.8359375
train loss:  0.3524430990219116
train gradient:  0.18773572787433762
iteration : 10066
train acc:  0.84375
train loss:  0.4305444061756134
train gradient:  0.3005981770328797
iteration : 10067
train acc:  0.8046875
train loss:  0.3856792747974396
train gradient:  0.21881182135177435
iteration : 10068
train acc:  0.796875
train loss:  0.40896642208099365
train gradient:  0.20530232811507793
iteration : 10069
train acc:  0.8359375
train loss:  0.34213387966156006
train gradient:  0.17183538254398834
iteration : 10070
train acc:  0.875
train loss:  0.34889474511146545
train gradient:  0.0999692902528516
iteration : 10071
train acc:  0.8828125
train loss:  0.32454386353492737
train gradient:  0.15525696967240332
iteration : 10072
train acc:  0.8359375
train loss:  0.35759592056274414
train gradient:  0.151866416553258
iteration : 10073
train acc:  0.875
train loss:  0.3091987371444702
train gradient:  0.1837635847155444
iteration : 10074
train acc:  0.8671875
train loss:  0.33676865696907043
train gradient:  0.1981595621433838
iteration : 10075
train acc:  0.90625
train loss:  0.2535251975059509
train gradient:  0.1440714169584747
iteration : 10076
train acc:  0.875
train loss:  0.3305705189704895
train gradient:  0.15135884318902124
iteration : 10077
train acc:  0.859375
train loss:  0.3326471745967865
train gradient:  0.19310329920468267
iteration : 10078
train acc:  0.859375
train loss:  0.2908408045768738
train gradient:  0.09409999443864091
iteration : 10079
train acc:  0.8359375
train loss:  0.3444468080997467
train gradient:  0.17831232606066716
iteration : 10080
train acc:  0.8828125
train loss:  0.270463228225708
train gradient:  0.13949278166853507
iteration : 10081
train acc:  0.8203125
train loss:  0.48816123604774475
train gradient:  0.3141116234623426
iteration : 10082
train acc:  0.8828125
train loss:  0.30180349946022034
train gradient:  0.20366399502507734
iteration : 10083
train acc:  0.828125
train loss:  0.40672212839126587
train gradient:  0.2207503903899491
iteration : 10084
train acc:  0.84375
train loss:  0.3623719811439514
train gradient:  0.21316584418057327
iteration : 10085
train acc:  0.8671875
train loss:  0.31957894563674927
train gradient:  0.13846021050461532
iteration : 10086
train acc:  0.828125
train loss:  0.3381662666797638
train gradient:  0.18857245112534432
iteration : 10087
train acc:  0.84375
train loss:  0.35079920291900635
train gradient:  0.1955436822632809
iteration : 10088
train acc:  0.8984375
train loss:  0.2924984395503998
train gradient:  0.15865917809655763
iteration : 10089
train acc:  0.828125
train loss:  0.34153804183006287
train gradient:  0.1761739441471984
iteration : 10090
train acc:  0.8671875
train loss:  0.3323330879211426
train gradient:  0.40253835594077086
iteration : 10091
train acc:  0.8828125
train loss:  0.29648682475090027
train gradient:  0.12053553728149173
iteration : 10092
train acc:  0.84375
train loss:  0.430182546377182
train gradient:  0.21483694173116102
iteration : 10093
train acc:  0.875
train loss:  0.30072563886642456
train gradient:  0.1309848553975785
iteration : 10094
train acc:  0.875
train loss:  0.33266714215278625
train gradient:  0.1797509710817134
iteration : 10095
train acc:  0.8984375
train loss:  0.27420371770858765
train gradient:  0.12672762541227103
iteration : 10096
train acc:  0.890625
train loss:  0.24170061945915222
train gradient:  0.1415974153046558
iteration : 10097
train acc:  0.8125
train loss:  0.3450215458869934
train gradient:  0.20423140878897955
iteration : 10098
train acc:  0.8984375
train loss:  0.2690771818161011
train gradient:  0.11274000184742505
iteration : 10099
train acc:  0.921875
train loss:  0.263543963432312
train gradient:  0.12950428827402416
iteration : 10100
train acc:  0.8671875
train loss:  0.330253928899765
train gradient:  0.19686801367432055
iteration : 10101
train acc:  0.8515625
train loss:  0.3042725920677185
train gradient:  0.250567863845028
iteration : 10102
train acc:  0.90625
train loss:  0.2656441032886505
train gradient:  0.15656242382473984
iteration : 10103
train acc:  0.859375
train loss:  0.31187891960144043
train gradient:  0.141657166774049
iteration : 10104
train acc:  0.8828125
train loss:  0.285688579082489
train gradient:  0.1238659528945696
iteration : 10105
train acc:  0.8359375
train loss:  0.3926551938056946
train gradient:  0.18591747473258136
iteration : 10106
train acc:  0.8359375
train loss:  0.4243569076061249
train gradient:  0.3277850951163621
iteration : 10107
train acc:  0.890625
train loss:  0.2793647050857544
train gradient:  0.14593038904060657
iteration : 10108
train acc:  0.828125
train loss:  0.3739780783653259
train gradient:  0.15875493735858698
iteration : 10109
train acc:  0.875
train loss:  0.28636854887008667
train gradient:  0.21917826861085227
iteration : 10110
train acc:  0.8828125
train loss:  0.32508549094200134
train gradient:  0.20709255110543534
iteration : 10111
train acc:  0.890625
train loss:  0.2934606075286865
train gradient:  0.1781332643116937
iteration : 10112
train acc:  0.859375
train loss:  0.4001905620098114
train gradient:  0.2458131579819541
iteration : 10113
train acc:  0.890625
train loss:  0.2961815893650055
train gradient:  0.1168347497576274
iteration : 10114
train acc:  0.84375
train loss:  0.32740578055381775
train gradient:  0.19636705716137878
iteration : 10115
train acc:  0.7734375
train loss:  0.4078018069267273
train gradient:  0.24670644681686543
iteration : 10116
train acc:  0.828125
train loss:  0.38666021823883057
train gradient:  0.284891294840097
iteration : 10117
train acc:  0.8828125
train loss:  0.28161996603012085
train gradient:  0.15793027097254378
iteration : 10118
train acc:  0.84375
train loss:  0.4050222635269165
train gradient:  0.21037832076766183
iteration : 10119
train acc:  0.8203125
train loss:  0.3793032765388489
train gradient:  0.19515910258607017
iteration : 10120
train acc:  0.8125
train loss:  0.3532446622848511
train gradient:  0.2169820753589563
iteration : 10121
train acc:  0.875
train loss:  0.28492307662963867
train gradient:  0.12448494302379401
iteration : 10122
train acc:  0.8359375
train loss:  0.3396749198436737
train gradient:  0.1855175795961194
iteration : 10123
train acc:  0.8125
train loss:  0.3802999258041382
train gradient:  0.2557648530694096
iteration : 10124
train acc:  0.859375
train loss:  0.2970685660839081
train gradient:  0.16680405832950035
iteration : 10125
train acc:  0.875
train loss:  0.25561079382896423
train gradient:  0.0947731040508144
iteration : 10126
train acc:  0.875
train loss:  0.25100183486938477
train gradient:  0.17942342289116306
iteration : 10127
train acc:  0.828125
train loss:  0.36375436186790466
train gradient:  0.22568993322337783
iteration : 10128
train acc:  0.875
train loss:  0.30742424726486206
train gradient:  0.14185230650681513
iteration : 10129
train acc:  0.7734375
train loss:  0.4522678256034851
train gradient:  0.3256386972256178
iteration : 10130
train acc:  0.828125
train loss:  0.33293694257736206
train gradient:  0.2546592634480583
iteration : 10131
train acc:  0.859375
train loss:  0.3130268156528473
train gradient:  0.25058117935424734
iteration : 10132
train acc:  0.90625
train loss:  0.26323699951171875
train gradient:  0.17113673309908642
iteration : 10133
train acc:  0.8203125
train loss:  0.36150962114334106
train gradient:  0.20840247001615889
iteration : 10134
train acc:  0.890625
train loss:  0.30031150579452515
train gradient:  0.17699853926314868
iteration : 10135
train acc:  0.875
train loss:  0.27514541149139404
train gradient:  0.1075814514189804
iteration : 10136
train acc:  0.828125
train loss:  0.3697406053543091
train gradient:  0.17716642151875578
iteration : 10137
train acc:  0.8359375
train loss:  0.337607204914093
train gradient:  0.15785604055164848
iteration : 10138
train acc:  0.890625
train loss:  0.32595115900039673
train gradient:  0.15909452499302518
iteration : 10139
train acc:  0.8515625
train loss:  0.3440452218055725
train gradient:  0.1659496689823247
iteration : 10140
train acc:  0.8671875
train loss:  0.3203454613685608
train gradient:  0.12446598758061185
iteration : 10141
train acc:  0.8671875
train loss:  0.3246549367904663
train gradient:  0.16112602052317085
iteration : 10142
train acc:  0.8515625
train loss:  0.3357093334197998
train gradient:  0.19622133599518543
iteration : 10143
train acc:  0.8984375
train loss:  0.26150354743003845
train gradient:  0.14128430268692993
iteration : 10144
train acc:  0.859375
train loss:  0.31067174673080444
train gradient:  0.17076065885076952
iteration : 10145
train acc:  0.84375
train loss:  0.30246737599372864
train gradient:  0.14861403084281208
iteration : 10146
train acc:  0.8671875
train loss:  0.2908403277397156
train gradient:  0.16644331781921715
iteration : 10147
train acc:  0.8828125
train loss:  0.27083268761634827
train gradient:  0.1344361703905039
iteration : 10148
train acc:  0.8984375
train loss:  0.24406109750270844
train gradient:  0.13769105530462522
iteration : 10149
train acc:  0.8828125
train loss:  0.28830161690711975
train gradient:  0.12647625858351003
iteration : 10150
train acc:  0.8203125
train loss:  0.3584520220756531
train gradient:  0.21400298991144598
iteration : 10151
train acc:  0.8671875
train loss:  0.28118693828582764
train gradient:  0.15451242658270709
iteration : 10152
train acc:  0.8515625
train loss:  0.2824322581291199
train gradient:  0.16155475631347854
iteration : 10153
train acc:  0.84375
train loss:  0.3262510299682617
train gradient:  0.16846939643300302
iteration : 10154
train acc:  0.859375
train loss:  0.37713900208473206
train gradient:  0.24632184051288109
iteration : 10155
train acc:  0.8671875
train loss:  0.3603430986404419
train gradient:  0.19910512031402658
iteration : 10156
train acc:  0.8828125
train loss:  0.38074034452438354
train gradient:  0.24863288777195724
iteration : 10157
train acc:  0.828125
train loss:  0.3551143407821655
train gradient:  0.149363047867741
iteration : 10158
train acc:  0.8046875
train loss:  0.3616335988044739
train gradient:  0.24456546124011525
iteration : 10159
train acc:  0.890625
train loss:  0.22707822918891907
train gradient:  0.10041907386490245
iteration : 10160
train acc:  0.828125
train loss:  0.4004371762275696
train gradient:  0.21850166562755363
iteration : 10161
train acc:  0.8125
train loss:  0.4724462926387787
train gradient:  0.3098875445378014
iteration : 10162
train acc:  0.8671875
train loss:  0.3470500707626343
train gradient:  0.1691901660587761
iteration : 10163
train acc:  0.828125
train loss:  0.3797118067741394
train gradient:  0.23866966212443025
iteration : 10164
train acc:  0.8671875
train loss:  0.332919180393219
train gradient:  0.13160653098636305
iteration : 10165
train acc:  0.8359375
train loss:  0.36171793937683105
train gradient:  0.25483805926253345
iteration : 10166
train acc:  0.7890625
train loss:  0.38194891810417175
train gradient:  0.2321151801426452
iteration : 10167
train acc:  0.8515625
train loss:  0.30631357431411743
train gradient:  0.21388490247679637
iteration : 10168
train acc:  0.90625
train loss:  0.24475733935832977
train gradient:  0.11375913214519928
iteration : 10169
train acc:  0.890625
train loss:  0.3131062984466553
train gradient:  0.19457524201560045
iteration : 10170
train acc:  0.8828125
train loss:  0.25934961438179016
train gradient:  0.12667056901794801
iteration : 10171
train acc:  0.8125
train loss:  0.47717174887657166
train gradient:  0.31969363897396386
iteration : 10172
train acc:  0.875
train loss:  0.31422385573387146
train gradient:  0.1525467639463357
iteration : 10173
train acc:  0.8984375
train loss:  0.3016388416290283
train gradient:  0.1773670502040668
iteration : 10174
train acc:  0.84375
train loss:  0.3473517894744873
train gradient:  0.20466737883972685
iteration : 10175
train acc:  0.890625
train loss:  0.28607556223869324
train gradient:  0.12936716120210412
iteration : 10176
train acc:  0.8203125
train loss:  0.38506460189819336
train gradient:  0.191168688160632
iteration : 10177
train acc:  0.859375
train loss:  0.31832408905029297
train gradient:  0.15602049143836552
iteration : 10178
train acc:  0.8515625
train loss:  0.3364526033401489
train gradient:  0.21978705211687208
iteration : 10179
train acc:  0.8515625
train loss:  0.3107106387615204
train gradient:  0.1329292102172929
iteration : 10180
train acc:  0.8828125
train loss:  0.2426394820213318
train gradient:  0.09155409875639862
iteration : 10181
train acc:  0.875
train loss:  0.3360229432582855
train gradient:  0.2423631057512099
iteration : 10182
train acc:  0.8984375
train loss:  0.2885139584541321
train gradient:  0.1769639391848371
iteration : 10183
train acc:  0.8203125
train loss:  0.39011120796203613
train gradient:  0.24790557432449295
iteration : 10184
train acc:  0.8515625
train loss:  0.3244626522064209
train gradient:  0.16905271135416222
iteration : 10185
train acc:  0.9140625
train loss:  0.2872483432292938
train gradient:  0.1390223876954293
iteration : 10186
train acc:  0.8984375
train loss:  0.28231877088546753
train gradient:  0.12884758777302824
iteration : 10187
train acc:  0.859375
train loss:  0.33515653014183044
train gradient:  0.12248018978416386
iteration : 10188
train acc:  0.78125
train loss:  0.41465702652931213
train gradient:  0.2550759167867015
iteration : 10189
train acc:  0.8203125
train loss:  0.37720704078674316
train gradient:  0.22176949808060747
iteration : 10190
train acc:  0.875
train loss:  0.30175116658210754
train gradient:  0.14980663588919088
iteration : 10191
train acc:  0.8359375
train loss:  0.35795557498931885
train gradient:  0.20293743012927073
iteration : 10192
train acc:  0.875
train loss:  0.29022759199142456
train gradient:  0.14211407096128087
iteration : 10193
train acc:  0.8828125
train loss:  0.28667151927948
train gradient:  0.11953002530262803
iteration : 10194
train acc:  0.8984375
train loss:  0.2873675227165222
train gradient:  0.14992870966150484
iteration : 10195
train acc:  0.8671875
train loss:  0.2746563255786896
train gradient:  0.10777565791682553
iteration : 10196
train acc:  0.859375
train loss:  0.3760322034358978
train gradient:  0.17755422436045948
iteration : 10197
train acc:  0.8515625
train loss:  0.34428316354751587
train gradient:  0.13898281030582185
iteration : 10198
train acc:  0.8359375
train loss:  0.33050885796546936
train gradient:  0.17495220359157604
iteration : 10199
train acc:  0.8203125
train loss:  0.35216695070266724
train gradient:  0.1667844621302517
iteration : 10200
train acc:  0.8359375
train loss:  0.33199572563171387
train gradient:  0.20519906089045367
iteration : 10201
train acc:  0.8359375
train loss:  0.32602590322494507
train gradient:  0.16084692569718922
iteration : 10202
train acc:  0.921875
train loss:  0.22982266545295715
train gradient:  0.09458766631088604
iteration : 10203
train acc:  0.8671875
train loss:  0.25446075201034546
train gradient:  0.20352403894892362
iteration : 10204
train acc:  0.828125
train loss:  0.36548784375190735
train gradient:  0.16086393967361878
iteration : 10205
train acc:  0.8828125
train loss:  0.27633535861968994
train gradient:  0.13420736962531357
iteration : 10206
train acc:  0.8984375
train loss:  0.2878536283969879
train gradient:  0.11846363402016567
iteration : 10207
train acc:  0.8046875
train loss:  0.40863656997680664
train gradient:  0.28830335808253343
iteration : 10208
train acc:  0.8359375
train loss:  0.35503721237182617
train gradient:  0.14788437621020406
iteration : 10209
train acc:  0.8359375
train loss:  0.3925762176513672
train gradient:  0.2577528991800604
iteration : 10210
train acc:  0.8984375
train loss:  0.2785879075527191
train gradient:  0.13046143882883726
iteration : 10211
train acc:  0.8984375
train loss:  0.24289363622665405
train gradient:  0.08958913750873897
iteration : 10212
train acc:  0.890625
train loss:  0.29083651304244995
train gradient:  0.11995985119792625
iteration : 10213
train acc:  0.8203125
train loss:  0.3481007218360901
train gradient:  0.2456794997399286
iteration : 10214
train acc:  0.90625
train loss:  0.2622939348220825
train gradient:  0.13809935207519275
iteration : 10215
train acc:  0.859375
train loss:  0.32550016045570374
train gradient:  0.15433428516432338
iteration : 10216
train acc:  0.84375
train loss:  0.4039587080478668
train gradient:  0.2722569948794474
iteration : 10217
train acc:  0.8984375
train loss:  0.26550227403640747
train gradient:  0.13162611103254554
iteration : 10218
train acc:  0.8671875
train loss:  0.324414998292923
train gradient:  0.23000352561859025
iteration : 10219
train acc:  0.859375
train loss:  0.36556655168533325
train gradient:  0.2041460968458776
iteration : 10220
train acc:  0.890625
train loss:  0.27852725982666016
train gradient:  0.1303138931653376
iteration : 10221
train acc:  0.8671875
train loss:  0.31474608182907104
train gradient:  0.14868826810830013
iteration : 10222
train acc:  0.9375
train loss:  0.19947686791419983
train gradient:  0.08051188743533715
iteration : 10223
train acc:  0.84375
train loss:  0.29606080055236816
train gradient:  0.13955114008376093
iteration : 10224
train acc:  0.875
train loss:  0.29962581396102905
train gradient:  0.14622317638161392
iteration : 10225
train acc:  0.796875
train loss:  0.3447272777557373
train gradient:  0.1853510939639298
iteration : 10226
train acc:  0.84375
train loss:  0.38289809226989746
train gradient:  0.24293337459235578
iteration : 10227
train acc:  0.8828125
train loss:  0.2925221621990204
train gradient:  0.20374343016169322
iteration : 10228
train acc:  0.890625
train loss:  0.30686628818511963
train gradient:  0.17672621079443132
iteration : 10229
train acc:  0.8671875
train loss:  0.29008445143699646
train gradient:  0.11731354967886838
iteration : 10230
train acc:  0.8515625
train loss:  0.3662067949771881
train gradient:  0.3141046244849513
iteration : 10231
train acc:  0.8671875
train loss:  0.3103814423084259
train gradient:  0.13938627470458137
iteration : 10232
train acc:  0.828125
train loss:  0.38246646523475647
train gradient:  0.2029659201873018
iteration : 10233
train acc:  0.8984375
train loss:  0.28233474493026733
train gradient:  0.12574026473584246
iteration : 10234
train acc:  0.84375
train loss:  0.37615376710891724
train gradient:  0.19290242002384206
iteration : 10235
train acc:  0.8828125
train loss:  0.29471689462661743
train gradient:  0.1456462809655222
iteration : 10236
train acc:  0.859375
train loss:  0.3265600800514221
train gradient:  0.18756507664406424
iteration : 10237
train acc:  0.828125
train loss:  0.36979877948760986
train gradient:  0.19349198324625613
iteration : 10238
train acc:  0.8984375
train loss:  0.25465747714042664
train gradient:  0.09162474810001722
iteration : 10239
train acc:  0.8515625
train loss:  0.2718271017074585
train gradient:  0.09117626742268191
iteration : 10240
train acc:  0.8203125
train loss:  0.3872110843658447
train gradient:  0.3008326759799843
iteration : 10241
train acc:  0.859375
train loss:  0.32289737462997437
train gradient:  0.15369726291225425
iteration : 10242
train acc:  0.8515625
train loss:  0.3484063744544983
train gradient:  0.16076578758949756
iteration : 10243
train acc:  0.8671875
train loss:  0.3609749674797058
train gradient:  0.17923085771714642
iteration : 10244
train acc:  0.859375
train loss:  0.32254093885421753
train gradient:  0.1299430004609845
iteration : 10245
train acc:  0.84375
train loss:  0.3909076154232025
train gradient:  0.21562870205674128
iteration : 10246
train acc:  0.859375
train loss:  0.29520004987716675
train gradient:  0.13186590750967425
iteration : 10247
train acc:  0.8984375
train loss:  0.25844839215278625
train gradient:  0.14204277461970488
iteration : 10248
train acc:  0.8359375
train loss:  0.378210186958313
train gradient:  0.22350634424965993
iteration : 10249
train acc:  0.828125
train loss:  0.3912712335586548
train gradient:  0.19881944815019265
iteration : 10250
train acc:  0.828125
train loss:  0.35519981384277344
train gradient:  0.24565370899617875
iteration : 10251
train acc:  0.8828125
train loss:  0.2600095570087433
train gradient:  0.11904826981289181
iteration : 10252
train acc:  0.8671875
train loss:  0.3048030734062195
train gradient:  0.12113919225786365
iteration : 10253
train acc:  0.8515625
train loss:  0.34490135312080383
train gradient:  0.2807570669884304
iteration : 10254
train acc:  0.8359375
train loss:  0.3882814943790436
train gradient:  0.3277109198880478
iteration : 10255
train acc:  0.8671875
train loss:  0.3369668126106262
train gradient:  0.15669363338348039
iteration : 10256
train acc:  0.796875
train loss:  0.44001924991607666
train gradient:  0.23201076892180383
iteration : 10257
train acc:  0.8046875
train loss:  0.3631431460380554
train gradient:  0.22983293523125264
iteration : 10258
train acc:  0.8359375
train loss:  0.30954509973526
train gradient:  0.1321637495391596
iteration : 10259
train acc:  0.828125
train loss:  0.37534427642822266
train gradient:  0.18710164463445392
iteration : 10260
train acc:  0.875
train loss:  0.2692856788635254
train gradient:  0.1360343262731333
iteration : 10261
train acc:  0.8515625
train loss:  0.3134724795818329
train gradient:  0.14333291988151825
iteration : 10262
train acc:  0.8828125
train loss:  0.277465283870697
train gradient:  0.1232832266261134
iteration : 10263
train acc:  0.8515625
train loss:  0.36065176129341125
train gradient:  0.16365009792321053
iteration : 10264
train acc:  0.8828125
train loss:  0.3322623372077942
train gradient:  0.19110372683002685
iteration : 10265
train acc:  0.8515625
train loss:  0.3505461812019348
train gradient:  0.1843881521888986
iteration : 10266
train acc:  0.875
train loss:  0.29833394289016724
train gradient:  0.1440288711151992
iteration : 10267
train acc:  0.84375
train loss:  0.3868943750858307
train gradient:  0.20054152991603075
iteration : 10268
train acc:  0.8515625
train loss:  0.31075823307037354
train gradient:  0.16774818708812203
iteration : 10269
train acc:  0.8828125
train loss:  0.3323326110839844
train gradient:  0.18930531684560425
iteration : 10270
train acc:  0.859375
train loss:  0.2848173677921295
train gradient:  0.14332056769066798
iteration : 10271
train acc:  0.8828125
train loss:  0.29383760690689087
train gradient:  0.17918382945355332
iteration : 10272
train acc:  0.8359375
train loss:  0.44738322496414185
train gradient:  0.2612600751687961
iteration : 10273
train acc:  0.8828125
train loss:  0.3018600344657898
train gradient:  0.09563971361010802
iteration : 10274
train acc:  0.8359375
train loss:  0.3216160237789154
train gradient:  0.1992116163428857
iteration : 10275
train acc:  0.8359375
train loss:  0.3373022675514221
train gradient:  0.16770911478396744
iteration : 10276
train acc:  0.859375
train loss:  0.2592090666294098
train gradient:  0.16797915066593025
iteration : 10277
train acc:  0.8515625
train loss:  0.29001080989837646
train gradient:  0.1455827341864918
iteration : 10278
train acc:  0.8984375
train loss:  0.23371672630310059
train gradient:  0.09573507824426496
iteration : 10279
train acc:  0.8984375
train loss:  0.2214779108762741
train gradient:  0.06327334615886565
iteration : 10280
train acc:  0.8984375
train loss:  0.3106709122657776
train gradient:  0.1426582929408775
iteration : 10281
train acc:  0.859375
train loss:  0.3545680046081543
train gradient:  0.1873048956106163
iteration : 10282
train acc:  0.859375
train loss:  0.26957619190216064
train gradient:  0.13728124567039107
iteration : 10283
train acc:  0.8203125
train loss:  0.32280147075653076
train gradient:  0.14799970445111182
iteration : 10284
train acc:  0.90625
train loss:  0.2995079755783081
train gradient:  0.14735458552052308
iteration : 10285
train acc:  0.9140625
train loss:  0.2510072588920593
train gradient:  0.11147089488794448
iteration : 10286
train acc:  0.890625
train loss:  0.24043357372283936
train gradient:  0.0928514797686386
iteration : 10287
train acc:  0.8359375
train loss:  0.3532227575778961
train gradient:  0.17241226779880767
iteration : 10288
train acc:  0.90625
train loss:  0.2682322859764099
train gradient:  0.11817652859231653
iteration : 10289
train acc:  0.8046875
train loss:  0.38622409105300903
train gradient:  0.22705269580136847
iteration : 10290
train acc:  0.890625
train loss:  0.28078144788742065
train gradient:  0.144460382592767
iteration : 10291
train acc:  0.875
train loss:  0.27910375595092773
train gradient:  0.13844937947839692
iteration : 10292
train acc:  0.875
train loss:  0.3162335157394409
train gradient:  0.11736631245671635
iteration : 10293
train acc:  0.890625
train loss:  0.3343541622161865
train gradient:  0.16160349745027075
iteration : 10294
train acc:  0.859375
train loss:  0.2834705710411072
train gradient:  0.11110244641203322
iteration : 10295
train acc:  0.8203125
train loss:  0.4036470949649811
train gradient:  0.24058372255946175
iteration : 10296
train acc:  0.875
train loss:  0.25627148151397705
train gradient:  0.13526662810397938
iteration : 10297
train acc:  0.8125
train loss:  0.3367394208908081
train gradient:  0.2238647156984302
iteration : 10298
train acc:  0.8828125
train loss:  0.28154492378234863
train gradient:  0.119927583202969
iteration : 10299
train acc:  0.8046875
train loss:  0.439159631729126
train gradient:  0.2735372863020134
iteration : 10300
train acc:  0.8984375
train loss:  0.3016565144062042
train gradient:  0.11546409072405789
iteration : 10301
train acc:  0.8828125
train loss:  0.28541436791419983
train gradient:  0.13575789208977068
iteration : 10302
train acc:  0.796875
train loss:  0.5015724301338196
train gradient:  0.3202838661748351
iteration : 10303
train acc:  0.90625
train loss:  0.22093380987644196
train gradient:  0.10739863938423644
iteration : 10304
train acc:  0.9140625
train loss:  0.2903516888618469
train gradient:  0.15160840734922762
iteration : 10305
train acc:  0.8671875
train loss:  0.3270062804222107
train gradient:  0.20882469083713728
iteration : 10306
train acc:  0.859375
train loss:  0.38014063239097595
train gradient:  0.19059826245838735
iteration : 10307
train acc:  0.8984375
train loss:  0.29370391368865967
train gradient:  0.14843128243024195
iteration : 10308
train acc:  0.84375
train loss:  0.30398470163345337
train gradient:  0.1884284040309557
iteration : 10309
train acc:  0.890625
train loss:  0.2775605022907257
train gradient:  0.21891218117196687
iteration : 10310
train acc:  0.859375
train loss:  0.32048147916793823
train gradient:  0.13659841937054296
iteration : 10311
train acc:  0.7890625
train loss:  0.45776882767677307
train gradient:  0.34818909827908323
iteration : 10312
train acc:  0.8671875
train loss:  0.31478816270828247
train gradient:  0.21543482623502389
iteration : 10313
train acc:  0.8515625
train loss:  0.30007025599479675
train gradient:  0.12947382262846205
iteration : 10314
train acc:  0.875
train loss:  0.27219462394714355
train gradient:  0.15263550552280863
iteration : 10315
train acc:  0.875
train loss:  0.29303884506225586
train gradient:  0.12424143496310769
iteration : 10316
train acc:  0.8203125
train loss:  0.35396918654441833
train gradient:  0.23802889418205
iteration : 10317
train acc:  0.890625
train loss:  0.24271097779273987
train gradient:  0.1186964682182065
iteration : 10318
train acc:  0.8671875
train loss:  0.3407455086708069
train gradient:  0.196404829661799
iteration : 10319
train acc:  0.875
train loss:  0.28523701429367065
train gradient:  0.12175251905591498
iteration : 10320
train acc:  0.7734375
train loss:  0.4509603679180145
train gradient:  0.36877480434190446
iteration : 10321
train acc:  0.84375
train loss:  0.3593001961708069
train gradient:  0.17162030744497153
iteration : 10322
train acc:  0.875
train loss:  0.26288506388664246
train gradient:  0.11778364440426936
iteration : 10323
train acc:  0.859375
train loss:  0.2950008809566498
train gradient:  0.11651793735912923
iteration : 10324
train acc:  0.8828125
train loss:  0.3289029598236084
train gradient:  0.1900945763506673
iteration : 10325
train acc:  0.8359375
train loss:  0.349597305059433
train gradient:  0.16065441165283573
iteration : 10326
train acc:  0.84375
train loss:  0.3078891932964325
train gradient:  0.14704273045035576
iteration : 10327
train acc:  0.859375
train loss:  0.34061986207962036
train gradient:  0.14632023248696918
iteration : 10328
train acc:  0.8671875
train loss:  0.31928789615631104
train gradient:  0.13743637997066627
iteration : 10329
train acc:  0.8125
train loss:  0.4121529459953308
train gradient:  0.2669209174341011
iteration : 10330
train acc:  0.8828125
train loss:  0.27484458684921265
train gradient:  0.1306902676869613
iteration : 10331
train acc:  0.7265625
train loss:  0.4784775376319885
train gradient:  0.3320157962714585
iteration : 10332
train acc:  0.890625
train loss:  0.305767297744751
train gradient:  0.22691621129822556
iteration : 10333
train acc:  0.8984375
train loss:  0.2806190252304077
train gradient:  0.10520548624762645
iteration : 10334
train acc:  0.84375
train loss:  0.4254816770553589
train gradient:  0.25350971123908006
iteration : 10335
train acc:  0.90625
train loss:  0.2423776388168335
train gradient:  0.09921564992049312
iteration : 10336
train acc:  0.8359375
train loss:  0.3133934438228607
train gradient:  0.18719859636885966
iteration : 10337
train acc:  0.8203125
train loss:  0.4030982255935669
train gradient:  0.2798855797369711
iteration : 10338
train acc:  0.90625
train loss:  0.24056552350521088
train gradient:  0.1116735195817438
iteration : 10339
train acc:  0.8671875
train loss:  0.3044634461402893
train gradient:  0.19286057707624704
iteration : 10340
train acc:  0.8984375
train loss:  0.2720564007759094
train gradient:  0.22086215937220469
iteration : 10341
train acc:  0.8671875
train loss:  0.3143217861652374
train gradient:  0.14570215621320076
iteration : 10342
train acc:  0.8203125
train loss:  0.35851943492889404
train gradient:  0.2682687939575981
iteration : 10343
train acc:  0.828125
train loss:  0.5271193981170654
train gradient:  0.3633177330741645
iteration : 10344
train acc:  0.90625
train loss:  0.2565130293369293
train gradient:  0.12532126165677115
iteration : 10345
train acc:  0.875
train loss:  0.325346976518631
train gradient:  0.17847497008207702
iteration : 10346
train acc:  0.8828125
train loss:  0.28776460886001587
train gradient:  0.1211194612950281
iteration : 10347
train acc:  0.859375
train loss:  0.2975306808948517
train gradient:  0.1230291483800591
iteration : 10348
train acc:  0.84375
train loss:  0.3193283975124359
train gradient:  0.15833087290161815
iteration : 10349
train acc:  0.8828125
train loss:  0.2837672233581543
train gradient:  0.12674220385154875
iteration : 10350
train acc:  0.859375
train loss:  0.32705235481262207
train gradient:  0.36175866174049937
iteration : 10351
train acc:  0.8515625
train loss:  0.3650226593017578
train gradient:  0.20718995079201633
iteration : 10352
train acc:  0.8203125
train loss:  0.31754863262176514
train gradient:  0.14245209913250537
iteration : 10353
train acc:  0.84375
train loss:  0.3196815252304077
train gradient:  0.13706063512532915
iteration : 10354
train acc:  0.875
train loss:  0.27153104543685913
train gradient:  0.09986787343263623
iteration : 10355
train acc:  0.875
train loss:  0.3837488293647766
train gradient:  0.22026520392470345
iteration : 10356
train acc:  0.8828125
train loss:  0.33975058794021606
train gradient:  0.22831734621652
iteration : 10357
train acc:  0.84375
train loss:  0.36551666259765625
train gradient:  0.20576646587014158
iteration : 10358
train acc:  0.8828125
train loss:  0.3206270933151245
train gradient:  0.12917443153356112
iteration : 10359
train acc:  0.828125
train loss:  0.37250447273254395
train gradient:  0.2083918567731569
iteration : 10360
train acc:  0.890625
train loss:  0.2603777348995209
train gradient:  0.10539053640496207
iteration : 10361
train acc:  0.875
train loss:  0.2992278039455414
train gradient:  0.13177229719735256
iteration : 10362
train acc:  0.8671875
train loss:  0.28765782713890076
train gradient:  0.10552484071984249
iteration : 10363
train acc:  0.8203125
train loss:  0.3546770215034485
train gradient:  0.20720820680318508
iteration : 10364
train acc:  0.8828125
train loss:  0.28249189257621765
train gradient:  0.12952741750105884
iteration : 10365
train acc:  0.921875
train loss:  0.26904529333114624
train gradient:  0.11965964357944436
iteration : 10366
train acc:  0.859375
train loss:  0.298839271068573
train gradient:  0.14648601254813642
iteration : 10367
train acc:  0.828125
train loss:  0.3645715117454529
train gradient:  0.2611829356099881
iteration : 10368
train acc:  0.8515625
train loss:  0.32833969593048096
train gradient:  0.20141183455231854
iteration : 10369
train acc:  0.8828125
train loss:  0.29931414127349854
train gradient:  0.12275977266518855
iteration : 10370
train acc:  0.8984375
train loss:  0.24521660804748535
train gradient:  0.12525957809888164
iteration : 10371
train acc:  0.8203125
train loss:  0.3937123417854309
train gradient:  0.22625937392567025
iteration : 10372
train acc:  0.828125
train loss:  0.39293015003204346
train gradient:  0.21014023864802758
iteration : 10373
train acc:  0.8515625
train loss:  0.31479611992836
train gradient:  0.16665253209917003
iteration : 10374
train acc:  0.828125
train loss:  0.41858452558517456
train gradient:  0.2178458124444163
iteration : 10375
train acc:  0.8515625
train loss:  0.30997028946876526
train gradient:  0.18121823497138073
iteration : 10376
train acc:  0.8828125
train loss:  0.3102688491344452
train gradient:  0.17529899290735107
iteration : 10377
train acc:  0.8671875
train loss:  0.38080817461013794
train gradient:  0.1966349652452391
iteration : 10378
train acc:  0.8671875
train loss:  0.3305667042732239
train gradient:  0.1452151235736157
iteration : 10379
train acc:  0.8828125
train loss:  0.30796682834625244
train gradient:  0.13581943229652607
iteration : 10380
train acc:  0.8828125
train loss:  0.2892680764198303
train gradient:  0.15496962829376024
iteration : 10381
train acc:  0.890625
train loss:  0.275485098361969
train gradient:  0.16180566504618607
iteration : 10382
train acc:  0.84375
train loss:  0.3175310790538788
train gradient:  0.2530240285065748
iteration : 10383
train acc:  0.84375
train loss:  0.34788602590560913
train gradient:  0.21848429549204765
iteration : 10384
train acc:  0.90625
train loss:  0.2233632206916809
train gradient:  0.09832191872711925
iteration : 10385
train acc:  0.8515625
train loss:  0.3118484616279602
train gradient:  0.13056724000689535
iteration : 10386
train acc:  0.8828125
train loss:  0.30528390407562256
train gradient:  0.14481616831521843
iteration : 10387
train acc:  0.8125
train loss:  0.36011987924575806
train gradient:  0.21592394375958918
iteration : 10388
train acc:  0.8125
train loss:  0.3869175314903259
train gradient:  0.21232306058359485
iteration : 10389
train acc:  0.8359375
train loss:  0.3791980445384979
train gradient:  0.22918017832316676
iteration : 10390
train acc:  0.8984375
train loss:  0.3346859812736511
train gradient:  0.12367389322270486
iteration : 10391
train acc:  0.84375
train loss:  0.3064830005168915
train gradient:  0.1755052865316489
iteration : 10392
train acc:  0.8828125
train loss:  0.3404238820075989
train gradient:  0.19557065856208386
iteration : 10393
train acc:  0.7890625
train loss:  0.4387887716293335
train gradient:  0.21940762097779137
iteration : 10394
train acc:  0.859375
train loss:  0.31374096870422363
train gradient:  0.219245621975648
iteration : 10395
train acc:  0.8515625
train loss:  0.31908220052719116
train gradient:  0.1709368248304381
iteration : 10396
train acc:  0.90625
train loss:  0.2771693766117096
train gradient:  0.17242891458061468
iteration : 10397
train acc:  0.875
train loss:  0.3109072744846344
train gradient:  0.13989405099951763
iteration : 10398
train acc:  0.8359375
train loss:  0.343722403049469
train gradient:  0.20438415434660484
iteration : 10399
train acc:  0.8984375
train loss:  0.2819981276988983
train gradient:  0.11710815502767213
iteration : 10400
train acc:  0.875
train loss:  0.2870537042617798
train gradient:  0.11932577250285563
iteration : 10401
train acc:  0.8046875
train loss:  0.38261884450912476
train gradient:  0.18282790126517048
iteration : 10402
train acc:  0.8359375
train loss:  0.32580769062042236
train gradient:  0.16707519780521615
iteration : 10403
train acc:  0.8359375
train loss:  0.36482322216033936
train gradient:  0.20142782460800018
iteration : 10404
train acc:  0.8671875
train loss:  0.35220465064048767
train gradient:  0.21868309428722305
iteration : 10405
train acc:  0.859375
train loss:  0.31468653678894043
train gradient:  0.14123455695370776
iteration : 10406
train acc:  0.875
train loss:  0.24779069423675537
train gradient:  0.12127489843674644
iteration : 10407
train acc:  0.828125
train loss:  0.3639470338821411
train gradient:  0.174099323483144
iteration : 10408
train acc:  0.875
train loss:  0.2764511704444885
train gradient:  0.1046512958716438
iteration : 10409
train acc:  0.84375
train loss:  0.3664719760417938
train gradient:  0.1660565964368696
iteration : 10410
train acc:  0.875
train loss:  0.2810214161872864
train gradient:  0.1239167045731392
iteration : 10411
train acc:  0.8203125
train loss:  0.3818430006504059
train gradient:  0.24833616902223898
iteration : 10412
train acc:  0.8203125
train loss:  0.4184983968734741
train gradient:  0.2180181195704202
iteration : 10413
train acc:  0.84375
train loss:  0.36113879084587097
train gradient:  0.21093435623941276
iteration : 10414
train acc:  0.8203125
train loss:  0.3666709065437317
train gradient:  0.17087634817334735
iteration : 10415
train acc:  0.8984375
train loss:  0.2924671769142151
train gradient:  0.1645223323733689
iteration : 10416
train acc:  0.8828125
train loss:  0.28418585658073425
train gradient:  0.1295646915737259
iteration : 10417
train acc:  0.859375
train loss:  0.36315298080444336
train gradient:  0.16874937213404373
iteration : 10418
train acc:  0.859375
train loss:  0.29651087522506714
train gradient:  0.11455708734421566
iteration : 10419
train acc:  0.890625
train loss:  0.27154541015625
train gradient:  0.13147233873970118
iteration : 10420
train acc:  0.8515625
train loss:  0.3054744601249695
train gradient:  0.14496152882702995
iteration : 10421
train acc:  0.8984375
train loss:  0.3105319142341614
train gradient:  0.14631591999404198
iteration : 10422
train acc:  0.828125
train loss:  0.3656112253665924
train gradient:  0.1930924163785612
iteration : 10423
train acc:  0.8359375
train loss:  0.4360867738723755
train gradient:  0.2275878730355626
iteration : 10424
train acc:  0.859375
train loss:  0.28316086530685425
train gradient:  0.1126147611583242
iteration : 10425
train acc:  0.90625
train loss:  0.25404036045074463
train gradient:  0.11887109553761432
iteration : 10426
train acc:  0.8671875
train loss:  0.2668668031692505
train gradient:  0.13491967496003815
iteration : 10427
train acc:  0.78125
train loss:  0.45475509762763977
train gradient:  0.29191143084103266
iteration : 10428
train acc:  0.828125
train loss:  0.35760414600372314
train gradient:  0.17569920969833613
iteration : 10429
train acc:  0.859375
train loss:  0.35677361488342285
train gradient:  0.16829427493105154
iteration : 10430
train acc:  0.8203125
train loss:  0.39214015007019043
train gradient:  0.26482092110874605
iteration : 10431
train acc:  0.8046875
train loss:  0.43798139691352844
train gradient:  0.22575020431580323
iteration : 10432
train acc:  0.84375
train loss:  0.38954007625579834
train gradient:  0.17503235463891098
iteration : 10433
train acc:  0.8515625
train loss:  0.3385400176048279
train gradient:  0.12465015963734694
iteration : 10434
train acc:  0.8203125
train loss:  0.39380955696105957
train gradient:  0.20891823876058052
iteration : 10435
train acc:  0.84375
train loss:  0.33038827776908875
train gradient:  0.15060359197753784
iteration : 10436
train acc:  0.84375
train loss:  0.3007565140724182
train gradient:  0.12944189590076116
iteration : 10437
train acc:  0.8984375
train loss:  0.2747519612312317
train gradient:  0.1447024625377646
iteration : 10438
train acc:  0.8125
train loss:  0.3857761025428772
train gradient:  0.182965566037032
iteration : 10439
train acc:  0.84375
train loss:  0.35048139095306396
train gradient:  0.1337159277602045
iteration : 10440
train acc:  0.8671875
train loss:  0.29987531900405884
train gradient:  0.1809617748428885
iteration : 10441
train acc:  0.890625
train loss:  0.3561553955078125
train gradient:  0.1687220918929255
iteration : 10442
train acc:  0.84375
train loss:  0.3963932991027832
train gradient:  0.15723458736533047
iteration : 10443
train acc:  0.8515625
train loss:  0.35596412420272827
train gradient:  0.14083192095810032
iteration : 10444
train acc:  0.828125
train loss:  0.3705839216709137
train gradient:  0.15568585239415916
iteration : 10445
train acc:  0.8671875
train loss:  0.34445422887802124
train gradient:  0.21110716291388593
iteration : 10446
train acc:  0.84375
train loss:  0.34848418831825256
train gradient:  0.12210075277701792
iteration : 10447
train acc:  0.8359375
train loss:  0.35322821140289307
train gradient:  0.16774584763343914
iteration : 10448
train acc:  0.8046875
train loss:  0.35471802949905396
train gradient:  0.20779954679442067
iteration : 10449
train acc:  0.8984375
train loss:  0.3014627695083618
train gradient:  0.1320420571732288
iteration : 10450
train acc:  0.8828125
train loss:  0.2794182300567627
train gradient:  0.18842600239071478
iteration : 10451
train acc:  0.8515625
train loss:  0.3635327219963074
train gradient:  0.1711898917305747
iteration : 10452
train acc:  0.84375
train loss:  0.2970403730869293
train gradient:  0.10273484112464519
iteration : 10453
train acc:  0.921875
train loss:  0.2555316090583801
train gradient:  0.1263263297074344
iteration : 10454
train acc:  0.890625
train loss:  0.2825586199760437
train gradient:  0.13899617044961804
iteration : 10455
train acc:  0.8515625
train loss:  0.36354508996009827
train gradient:  0.17040560652448372
iteration : 10456
train acc:  0.828125
train loss:  0.3831630349159241
train gradient:  0.17932806501528709
iteration : 10457
train acc:  0.859375
train loss:  0.3222416043281555
train gradient:  0.12433805023419553
iteration : 10458
train acc:  0.8828125
train loss:  0.3585928976535797
train gradient:  0.3313694651448074
iteration : 10459
train acc:  0.8359375
train loss:  0.32377320528030396
train gradient:  0.17572337668323465
iteration : 10460
train acc:  0.8359375
train loss:  0.3600221276283264
train gradient:  0.1893220371004497
iteration : 10461
train acc:  0.8046875
train loss:  0.4111809730529785
train gradient:  0.20560796256461975
iteration : 10462
train acc:  0.875
train loss:  0.30568012595176697
train gradient:  0.18082021866762163
iteration : 10463
train acc:  0.828125
train loss:  0.3429485261440277
train gradient:  0.17217889107338918
iteration : 10464
train acc:  0.8359375
train loss:  0.395100474357605
train gradient:  0.15312667745295627
iteration : 10465
train acc:  0.90625
train loss:  0.23838773369789124
train gradient:  0.1747481528535105
iteration : 10466
train acc:  0.8203125
train loss:  0.36316555738449097
train gradient:  0.1865269921097062
iteration : 10467
train acc:  0.875
train loss:  0.26960083842277527
train gradient:  0.11182041819566169
iteration : 10468
train acc:  0.8515625
train loss:  0.3074098825454712
train gradient:  0.18820440966117927
iteration : 10469
train acc:  0.859375
train loss:  0.29494917392730713
train gradient:  0.1466976001924553
iteration : 10470
train acc:  0.8515625
train loss:  0.32078471779823303
train gradient:  0.15875696190522384
iteration : 10471
train acc:  0.8203125
train loss:  0.4017826318740845
train gradient:  0.19984310994613783
iteration : 10472
train acc:  0.875
train loss:  0.3050360083580017
train gradient:  0.10230880377834335
iteration : 10473
train acc:  0.8828125
train loss:  0.29935383796691895
train gradient:  0.14769533644467367
iteration : 10474
train acc:  0.8828125
train loss:  0.2812432646751404
train gradient:  0.1256461276899576
iteration : 10475
train acc:  0.8671875
train loss:  0.3344581425189972
train gradient:  0.13795015144649547
iteration : 10476
train acc:  0.8515625
train loss:  0.36764246225357056
train gradient:  0.2288250838129606
iteration : 10477
train acc:  0.859375
train loss:  0.2907346487045288
train gradient:  0.13874720122201006
iteration : 10478
train acc:  0.765625
train loss:  0.457072377204895
train gradient:  0.23565420184277291
iteration : 10479
train acc:  0.8359375
train loss:  0.3452306389808655
train gradient:  0.14324394505619836
iteration : 10480
train acc:  0.8203125
train loss:  0.2941988408565521
train gradient:  0.09955332698051066
iteration : 10481
train acc:  0.9375
train loss:  0.230796679854393
train gradient:  0.07477526973177534
iteration : 10482
train acc:  0.8984375
train loss:  0.2967604994773865
train gradient:  0.11885754296712586
iteration : 10483
train acc:  0.84375
train loss:  0.388979971408844
train gradient:  0.28307478430246225
iteration : 10484
train acc:  0.890625
train loss:  0.25378304719924927
train gradient:  0.12161312060880344
iteration : 10485
train acc:  0.90625
train loss:  0.2591208219528198
train gradient:  0.12185503070150455
iteration : 10486
train acc:  0.8203125
train loss:  0.38554736971855164
train gradient:  0.26841020326642784
iteration : 10487
train acc:  0.890625
train loss:  0.2972535490989685
train gradient:  0.11564484923257838
iteration : 10488
train acc:  0.8515625
train loss:  0.3579649329185486
train gradient:  0.15651420232716393
iteration : 10489
train acc:  0.859375
train loss:  0.328921914100647
train gradient:  0.13045347847352673
iteration : 10490
train acc:  0.828125
train loss:  0.3284290134906769
train gradient:  0.14392275959835232
iteration : 10491
train acc:  0.8984375
train loss:  0.23798751831054688
train gradient:  0.14316997363643202
iteration : 10492
train acc:  0.8046875
train loss:  0.374426007270813
train gradient:  0.27674824880491455
iteration : 10493
train acc:  0.8984375
train loss:  0.2472647875547409
train gradient:  0.11886169873750997
iteration : 10494
train acc:  0.78125
train loss:  0.4669055640697479
train gradient:  0.6132626288761638
iteration : 10495
train acc:  0.859375
train loss:  0.3058600127696991
train gradient:  0.14886304898379682
iteration : 10496
train acc:  0.890625
train loss:  0.27244794368743896
train gradient:  0.10821969989145985
iteration : 10497
train acc:  0.859375
train loss:  0.2982984185218811
train gradient:  0.17393255184216794
iteration : 10498
train acc:  0.8828125
train loss:  0.2911749482154846
train gradient:  0.11873943114044995
iteration : 10499
train acc:  0.8125
train loss:  0.33174794912338257
train gradient:  0.23913378357510223
iteration : 10500
train acc:  0.8359375
train loss:  0.32835638523101807
train gradient:  0.13500127491023403
iteration : 10501
train acc:  0.875
train loss:  0.3192674517631531
train gradient:  0.13098743885771705
iteration : 10502
train acc:  0.828125
train loss:  0.349027156829834
train gradient:  0.18358858130500139
iteration : 10503
train acc:  0.8203125
train loss:  0.3736625015735626
train gradient:  0.15904770758786058
iteration : 10504
train acc:  0.859375
train loss:  0.32321304082870483
train gradient:  0.18788234977744217
iteration : 10505
train acc:  0.8515625
train loss:  0.3481576442718506
train gradient:  0.19626421386935822
iteration : 10506
train acc:  0.8828125
train loss:  0.32251667976379395
train gradient:  0.11646327757349129
iteration : 10507
train acc:  0.8515625
train loss:  0.382901132106781
train gradient:  0.19671079239087264
iteration : 10508
train acc:  0.90625
train loss:  0.2643771767616272
train gradient:  0.09517261571121906
iteration : 10509
train acc:  0.875
train loss:  0.2949984073638916
train gradient:  0.1010079527251972
iteration : 10510
train acc:  0.8984375
train loss:  0.26192232966423035
train gradient:  0.11471233218332415
iteration : 10511
train acc:  0.859375
train loss:  0.35156166553497314
train gradient:  0.14693824162771899
iteration : 10512
train acc:  0.859375
train loss:  0.26704901456832886
train gradient:  0.09614410993070532
iteration : 10513
train acc:  0.8359375
train loss:  0.3127061724662781
train gradient:  0.14427689444150976
iteration : 10514
train acc:  0.8984375
train loss:  0.29376477003097534
train gradient:  0.1916977338150237
iteration : 10515
train acc:  0.796875
train loss:  0.35293641686439514
train gradient:  0.16542044743326562
iteration : 10516
train acc:  0.8359375
train loss:  0.34006789326667786
train gradient:  0.15559834831672198
iteration : 10517
train acc:  0.8984375
train loss:  0.2567708194255829
train gradient:  0.10118038614977816
iteration : 10518
train acc:  0.9140625
train loss:  0.27573150396347046
train gradient:  0.13483923332576456
iteration : 10519
train acc:  0.8515625
train loss:  0.4196781516075134
train gradient:  0.3754987678556549
iteration : 10520
train acc:  0.890625
train loss:  0.30956390500068665
train gradient:  0.19941756340133443
iteration : 10521
train acc:  0.8984375
train loss:  0.24280107021331787
train gradient:  0.10558102068441962
iteration : 10522
train acc:  0.84375
train loss:  0.37429678440093994
train gradient:  0.18988165434773158
iteration : 10523
train acc:  0.84375
train loss:  0.2869206964969635
train gradient:  0.14043163603810382
iteration : 10524
train acc:  0.8125
train loss:  0.38981539011001587
train gradient:  0.21041297974022416
iteration : 10525
train acc:  0.90625
train loss:  0.25458240509033203
train gradient:  0.10815717571090215
iteration : 10526
train acc:  0.8984375
train loss:  0.2677043080329895
train gradient:  0.11034075797161659
iteration : 10527
train acc:  0.875
train loss:  0.3135320544242859
train gradient:  0.19208065379396028
iteration : 10528
train acc:  0.90625
train loss:  0.2548581063747406
train gradient:  0.12710317869296045
iteration : 10529
train acc:  0.8125
train loss:  0.40561914443969727
train gradient:  0.23729386405662156
iteration : 10530
train acc:  0.9140625
train loss:  0.2766072452068329
train gradient:  0.09485758975452077
iteration : 10531
train acc:  0.859375
train loss:  0.3625756502151489
train gradient:  0.17639847531305058
iteration : 10532
train acc:  0.859375
train loss:  0.31724101305007935
train gradient:  0.19404492352801386
iteration : 10533
train acc:  0.859375
train loss:  0.3143649101257324
train gradient:  0.18402273494106272
iteration : 10534
train acc:  0.859375
train loss:  0.32190996408462524
train gradient:  0.1928923959043355
iteration : 10535
train acc:  0.8671875
train loss:  0.3294215798377991
train gradient:  0.16703919340426449
iteration : 10536
train acc:  0.859375
train loss:  0.4103546738624573
train gradient:  0.22779202106260427
iteration : 10537
train acc:  0.8828125
train loss:  0.2834545075893402
train gradient:  0.1623439836820375
iteration : 10538
train acc:  0.8828125
train loss:  0.3183046579360962
train gradient:  0.11590380331569872
iteration : 10539
train acc:  0.84375
train loss:  0.3425031304359436
train gradient:  0.14461937938389058
iteration : 10540
train acc:  0.890625
train loss:  0.23989981412887573
train gradient:  0.08473250259561987
iteration : 10541
train acc:  0.8671875
train loss:  0.3795810043811798
train gradient:  0.26371199019126246
iteration : 10542
train acc:  0.8984375
train loss:  0.29642757773399353
train gradient:  0.12199906128918243
iteration : 10543
train acc:  0.8515625
train loss:  0.27272120118141174
train gradient:  0.18594153747863607
iteration : 10544
train acc:  0.890625
train loss:  0.2800312042236328
train gradient:  0.1543835962870469
iteration : 10545
train acc:  0.8125
train loss:  0.41386228799819946
train gradient:  0.2149226279426757
iteration : 10546
train acc:  0.84375
train loss:  0.31866931915283203
train gradient:  0.13574046628188677
iteration : 10547
train acc:  0.8671875
train loss:  0.2972337603569031
train gradient:  0.20827991671894175
iteration : 10548
train acc:  0.8984375
train loss:  0.22512604296207428
train gradient:  0.0973528240630193
iteration : 10549
train acc:  0.8828125
train loss:  0.3289233148097992
train gradient:  0.14201806677471784
iteration : 10550
train acc:  0.890625
train loss:  0.31559237837791443
train gradient:  0.16197567242163557
iteration : 10551
train acc:  0.828125
train loss:  0.3564947247505188
train gradient:  0.19363347665501496
iteration : 10552
train acc:  0.8125
train loss:  0.47584739327430725
train gradient:  0.44589498658008875
iteration : 10553
train acc:  0.8515625
train loss:  0.39787179231643677
train gradient:  0.4870269806153919
iteration : 10554
train acc:  0.8828125
train loss:  0.27903252840042114
train gradient:  0.11908305822700702
iteration : 10555
train acc:  0.875
train loss:  0.26159894466400146
train gradient:  0.1186681232812227
iteration : 10556
train acc:  0.921875
train loss:  0.25461339950561523
train gradient:  0.16265086497122844
iteration : 10557
train acc:  0.8515625
train loss:  0.3633694648742676
train gradient:  0.2406824840083298
iteration : 10558
train acc:  0.84375
train loss:  0.34705284237861633
train gradient:  0.171505186632081
iteration : 10559
train acc:  0.8984375
train loss:  0.2575056850910187
train gradient:  0.12119207079524064
iteration : 10560
train acc:  0.8515625
train loss:  0.3308255970478058
train gradient:  0.15422661951381408
iteration : 10561
train acc:  0.828125
train loss:  0.3716544508934021
train gradient:  0.22515673601901573
iteration : 10562
train acc:  0.921875
train loss:  0.27370601892471313
train gradient:  0.23707279247537583
iteration : 10563
train acc:  0.859375
train loss:  0.2979273498058319
train gradient:  0.1750215756943598
iteration : 10564
train acc:  0.8359375
train loss:  0.33429306745529175
train gradient:  0.22148943988404615
iteration : 10565
train acc:  0.8671875
train loss:  0.31662386655807495
train gradient:  0.12238758314239717
iteration : 10566
train acc:  0.859375
train loss:  0.3641270697116852
train gradient:  0.20646646705638444
iteration : 10567
train acc:  0.8359375
train loss:  0.3288777470588684
train gradient:  0.26097033083932086
iteration : 10568
train acc:  0.8671875
train loss:  0.2774682641029358
train gradient:  0.13829844987280082
iteration : 10569
train acc:  0.875
train loss:  0.3019042909145355
train gradient:  0.1400599813995187
iteration : 10570
train acc:  0.828125
train loss:  0.40964847803115845
train gradient:  0.19632636170979903
iteration : 10571
train acc:  0.859375
train loss:  0.2993166744709015
train gradient:  0.15909389938432703
iteration : 10572
train acc:  0.875
train loss:  0.3075994849205017
train gradient:  0.18597306970675803
iteration : 10573
train acc:  0.890625
train loss:  0.3171493411064148
train gradient:  0.24478404354279693
iteration : 10574
train acc:  0.8671875
train loss:  0.33002445101737976
train gradient:  0.16643435820716662
iteration : 10575
train acc:  0.8671875
train loss:  0.27734872698783875
train gradient:  0.1796527611106837
iteration : 10576
train acc:  0.8359375
train loss:  0.34320420026779175
train gradient:  0.20677897023937708
iteration : 10577
train acc:  0.9140625
train loss:  0.27307283878326416
train gradient:  0.11543798370396015
iteration : 10578
train acc:  0.8515625
train loss:  0.31067901849746704
train gradient:  0.17249670431132008
iteration : 10579
train acc:  0.8515625
train loss:  0.3116164803504944
train gradient:  0.12722314929761663
iteration : 10580
train acc:  0.8359375
train loss:  0.3431066870689392
train gradient:  0.17145171951376895
iteration : 10581
train acc:  0.8125
train loss:  0.39440035820007324
train gradient:  0.20467228461868742
iteration : 10582
train acc:  0.890625
train loss:  0.3119443655014038
train gradient:  0.11919353769089659
iteration : 10583
train acc:  0.875
train loss:  0.33627045154571533
train gradient:  0.1745317556217719
iteration : 10584
train acc:  0.8984375
train loss:  0.24628421664237976
train gradient:  0.11870353601539033
iteration : 10585
train acc:  0.8671875
train loss:  0.36057382822036743
train gradient:  0.27095450776271746
iteration : 10586
train acc:  0.8359375
train loss:  0.33075839281082153
train gradient:  0.1372511811285747
iteration : 10587
train acc:  0.8359375
train loss:  0.3785626292228699
train gradient:  0.19559335054373472
iteration : 10588
train acc:  0.8828125
train loss:  0.2955787479877472
train gradient:  0.17933256502668588
iteration : 10589
train acc:  0.8671875
train loss:  0.335721492767334
train gradient:  0.2087771005041546
iteration : 10590
train acc:  0.828125
train loss:  0.38730108737945557
train gradient:  0.18815652563669724
iteration : 10591
train acc:  0.828125
train loss:  0.352780282497406
train gradient:  0.18340649486275212
iteration : 10592
train acc:  0.84375
train loss:  0.46648240089416504
train gradient:  0.3165148759645021
iteration : 10593
train acc:  0.8203125
train loss:  0.4004002809524536
train gradient:  0.2680674701468268
iteration : 10594
train acc:  0.875
train loss:  0.2736761271953583
train gradient:  0.11172200274639911
iteration : 10595
train acc:  0.8515625
train loss:  0.2942689061164856
train gradient:  0.12864637173342375
iteration : 10596
train acc:  0.828125
train loss:  0.3474947214126587
train gradient:  0.24445821897006084
iteration : 10597
train acc:  0.796875
train loss:  0.36164334416389465
train gradient:  0.18816750730499743
iteration : 10598
train acc:  0.84375
train loss:  0.36178427934646606
train gradient:  0.15558573562090472
iteration : 10599
train acc:  0.8046875
train loss:  0.4079686403274536
train gradient:  0.2783483867927684
iteration : 10600
train acc:  0.9140625
train loss:  0.288115918636322
train gradient:  0.17368252793139216
iteration : 10601
train acc:  0.8671875
train loss:  0.3541819751262665
train gradient:  0.1912851847486485
iteration : 10602
train acc:  0.8515625
train loss:  0.3436047434806824
train gradient:  0.19289510236268329
iteration : 10603
train acc:  0.84375
train loss:  0.39182138442993164
train gradient:  0.1806543880640028
iteration : 10604
train acc:  0.90625
train loss:  0.2847045660018921
train gradient:  0.15937626465778026
iteration : 10605
train acc:  0.890625
train loss:  0.25187253952026367
train gradient:  0.09808565553849048
iteration : 10606
train acc:  0.8671875
train loss:  0.3174050748348236
train gradient:  0.15280260619200575
iteration : 10607
train acc:  0.8984375
train loss:  0.25813615322113037
train gradient:  0.13652125059568992
iteration : 10608
train acc:  0.859375
train loss:  0.29870903491973877
train gradient:  0.11091520795442053
iteration : 10609
train acc:  0.890625
train loss:  0.2534196674823761
train gradient:  0.1330953792184013
iteration : 10610
train acc:  0.8515625
train loss:  0.31493061780929565
train gradient:  0.18328816817681293
iteration : 10611
train acc:  0.8203125
train loss:  0.45317232608795166
train gradient:  0.3236120594801326
iteration : 10612
train acc:  0.8828125
train loss:  0.2514645755290985
train gradient:  0.08619751338994679
iteration : 10613
train acc:  0.84375
train loss:  0.3570288121700287
train gradient:  0.20575466720423963
iteration : 10614
train acc:  0.8984375
train loss:  0.25120434165000916
train gradient:  0.11543300217548003
iteration : 10615
train acc:  0.90625
train loss:  0.2782841622829437
train gradient:  0.14992870178899534
iteration : 10616
train acc:  0.84375
train loss:  0.39026185870170593
train gradient:  0.20536664274186658
iteration : 10617
train acc:  0.8828125
train loss:  0.2674979567527771
train gradient:  0.09171945867112834
iteration : 10618
train acc:  0.8515625
train loss:  0.3193412721157074
train gradient:  0.349485766371942
iteration : 10619
train acc:  0.8515625
train loss:  0.2966744601726532
train gradient:  0.19520823161805134
iteration : 10620
train acc:  0.84375
train loss:  0.31432241201400757
train gradient:  0.2010923996100116
iteration : 10621
train acc:  0.8125
train loss:  0.4038814306259155
train gradient:  0.2815253512988569
iteration : 10622
train acc:  0.8359375
train loss:  0.35443127155303955
train gradient:  0.17600501938264554
iteration : 10623
train acc:  0.890625
train loss:  0.26089179515838623
train gradient:  0.1108412618829568
iteration : 10624
train acc:  0.828125
train loss:  0.34691083431243896
train gradient:  0.14967936551609584
iteration : 10625
train acc:  0.859375
train loss:  0.2837804853916168
train gradient:  0.10260623849610986
iteration : 10626
train acc:  0.890625
train loss:  0.27084553241729736
train gradient:  0.12869700156663072
iteration : 10627
train acc:  0.8984375
train loss:  0.3377351462841034
train gradient:  0.18016217043945715
iteration : 10628
train acc:  0.9296875
train loss:  0.2517126202583313
train gradient:  0.12509016891753225
iteration : 10629
train acc:  0.8359375
train loss:  0.3562663197517395
train gradient:  0.16427186318124487
iteration : 10630
train acc:  0.8671875
train loss:  0.2937772572040558
train gradient:  0.128616995404831
iteration : 10631
train acc:  0.8671875
train loss:  0.3357601463794708
train gradient:  0.14907195501636775
iteration : 10632
train acc:  0.8515625
train loss:  0.39035564661026
train gradient:  0.18092797845053843
iteration : 10633
train acc:  0.9140625
train loss:  0.275937557220459
train gradient:  0.13687022742693494
iteration : 10634
train acc:  0.796875
train loss:  0.482452929019928
train gradient:  0.2664065405856363
iteration : 10635
train acc:  0.8203125
train loss:  0.3599865734577179
train gradient:  0.24323729348890827
iteration : 10636
train acc:  0.875
train loss:  0.37520503997802734
train gradient:  0.20539107649210003
iteration : 10637
train acc:  0.8984375
train loss:  0.3141254186630249
train gradient:  0.11729411079041381
iteration : 10638
train acc:  0.875
train loss:  0.31215420365333557
train gradient:  0.14549601186225178
iteration : 10639
train acc:  0.8203125
train loss:  0.3441118597984314
train gradient:  0.14913322854168157
iteration : 10640
train acc:  0.8828125
train loss:  0.34222185611724854
train gradient:  0.1587170832971248
iteration : 10641
train acc:  0.8359375
train loss:  0.34469708800315857
train gradient:  0.17674583946334466
iteration : 10642
train acc:  0.84375
train loss:  0.348930299282074
train gradient:  0.15029057519373298
iteration : 10643
train acc:  0.8359375
train loss:  0.35167157649993896
train gradient:  0.18968276274444104
iteration : 10644
train acc:  0.875
train loss:  0.31125593185424805
train gradient:  0.1272709347938899
iteration : 10645
train acc:  0.8984375
train loss:  0.28744959831237793
train gradient:  0.14998960579599835
iteration : 10646
train acc:  0.890625
train loss:  0.2602635622024536
train gradient:  0.12966727913497575
iteration : 10647
train acc:  0.859375
train loss:  0.27432993054389954
train gradient:  0.13223399169740865
iteration : 10648
train acc:  0.8203125
train loss:  0.34941184520721436
train gradient:  0.1776650985269539
iteration : 10649
train acc:  0.8828125
train loss:  0.2796344757080078
train gradient:  0.15075742165512915
iteration : 10650
train acc:  0.8359375
train loss:  0.33900171518325806
train gradient:  0.16544100096597872
iteration : 10651
train acc:  0.875
train loss:  0.3011503219604492
train gradient:  0.18214924116501108
iteration : 10652
train acc:  0.90625
train loss:  0.21580663323402405
train gradient:  0.08014634248590588
iteration : 10653
train acc:  0.8515625
train loss:  0.3197818994522095
train gradient:  0.20463310752900699
iteration : 10654
train acc:  0.8359375
train loss:  0.346437931060791
train gradient:  0.2656783152600455
iteration : 10655
train acc:  0.796875
train loss:  0.38643765449523926
train gradient:  0.2015559594069966
iteration : 10656
train acc:  0.890625
train loss:  0.28867316246032715
train gradient:  0.1094914528727288
iteration : 10657
train acc:  0.8515625
train loss:  0.3779715597629547
train gradient:  0.22767418032387204
iteration : 10658
train acc:  0.8671875
train loss:  0.3449126183986664
train gradient:  0.13881139383398772
iteration : 10659
train acc:  0.890625
train loss:  0.31178557872772217
train gradient:  0.12623604318842002
iteration : 10660
train acc:  0.859375
train loss:  0.3433436155319214
train gradient:  0.20997728960443554
iteration : 10661
train acc:  0.8515625
train loss:  0.36904650926589966
train gradient:  0.21923898446492457
iteration : 10662
train acc:  0.8515625
train loss:  0.36458849906921387
train gradient:  0.22349992073187752
iteration : 10663
train acc:  0.84375
train loss:  0.3550710678100586
train gradient:  0.26096572899775894
iteration : 10664
train acc:  0.8515625
train loss:  0.3919750154018402
train gradient:  0.22199195966018465
iteration : 10665
train acc:  0.8984375
train loss:  0.30006593465805054
train gradient:  0.13248250957601265
iteration : 10666
train acc:  0.890625
train loss:  0.3051152229309082
train gradient:  0.20240273325704405
iteration : 10667
train acc:  0.8359375
train loss:  0.4089202582836151
train gradient:  0.21517419006346866
iteration : 10668
train acc:  0.8203125
train loss:  0.47252145409584045
train gradient:  0.23927530721099582
iteration : 10669
train acc:  0.8359375
train loss:  0.32775038480758667
train gradient:  0.19608262613862004
iteration : 10670
train acc:  0.859375
train loss:  0.33218055963516235
train gradient:  0.2640672376118424
iteration : 10671
train acc:  0.8671875
train loss:  0.2994347810745239
train gradient:  0.1359238000184933
iteration : 10672
train acc:  0.875
train loss:  0.25230294466018677
train gradient:  0.11148152714830735
iteration : 10673
train acc:  0.8984375
train loss:  0.2841629981994629
train gradient:  0.1494877005797355
iteration : 10674
train acc:  0.84375
train loss:  0.34049656987190247
train gradient:  0.14920102803871865
iteration : 10675
train acc:  0.828125
train loss:  0.3519326448440552
train gradient:  0.1456860021913366
iteration : 10676
train acc:  0.8515625
train loss:  0.392654687166214
train gradient:  0.22144535022888662
iteration : 10677
train acc:  0.828125
train loss:  0.32816389203071594
train gradient:  0.16286133704319963
iteration : 10678
train acc:  0.8515625
train loss:  0.37568750977516174
train gradient:  0.16915071670379936
iteration : 10679
train acc:  0.90625
train loss:  0.24586158990859985
train gradient:  0.09370517427322224
iteration : 10680
train acc:  0.859375
train loss:  0.3333168625831604
train gradient:  0.36338811497651874
iteration : 10681
train acc:  0.890625
train loss:  0.2727254331111908
train gradient:  0.09116673444439843
iteration : 10682
train acc:  0.8515625
train loss:  0.3110687732696533
train gradient:  0.16531631431065746
iteration : 10683
train acc:  0.8359375
train loss:  0.4049229025840759
train gradient:  0.332956610826993
iteration : 10684
train acc:  0.84375
train loss:  0.32777440547943115
train gradient:  0.16903141946967717
iteration : 10685
train acc:  0.8359375
train loss:  0.3531355559825897
train gradient:  0.29710987475919504
iteration : 10686
train acc:  0.8671875
train loss:  0.2794283926486969
train gradient:  0.13180085061532798
iteration : 10687
train acc:  0.90625
train loss:  0.2706470489501953
train gradient:  0.13236215420657782
iteration : 10688
train acc:  0.8359375
train loss:  0.3285372853279114
train gradient:  0.1469739772206109
iteration : 10689
train acc:  0.828125
train loss:  0.3829197883605957
train gradient:  0.21699957439942263
iteration : 10690
train acc:  0.890625
train loss:  0.2703281044960022
train gradient:  0.11052332799925978
iteration : 10691
train acc:  0.8515625
train loss:  0.3453068137168884
train gradient:  0.1608631636670403
iteration : 10692
train acc:  0.8515625
train loss:  0.31671756505966187
train gradient:  0.13947302851035354
iteration : 10693
train acc:  0.8203125
train loss:  0.37255942821502686
train gradient:  0.19264259300244674
iteration : 10694
train acc:  0.859375
train loss:  0.3343220353126526
train gradient:  0.1816079535098034
iteration : 10695
train acc:  0.8046875
train loss:  0.3628038465976715
train gradient:  0.21678142011053322
iteration : 10696
train acc:  0.875
train loss:  0.28610333800315857
train gradient:  0.1662730398543536
iteration : 10697
train acc:  0.8515625
train loss:  0.31782734394073486
train gradient:  0.16865434410132937
iteration : 10698
train acc:  0.8984375
train loss:  0.2554142475128174
train gradient:  0.17012020975728018
iteration : 10699
train acc:  0.859375
train loss:  0.3263971209526062
train gradient:  0.20915174589852648
iteration : 10700
train acc:  0.828125
train loss:  0.388246089220047
train gradient:  0.2910761460644746
iteration : 10701
train acc:  0.9140625
train loss:  0.2637726068496704
train gradient:  0.15151220160606277
iteration : 10702
train acc:  0.890625
train loss:  0.34926286339759827
train gradient:  0.23511376701716558
iteration : 10703
train acc:  0.875
train loss:  0.3540656864643097
train gradient:  0.14549132979500812
iteration : 10704
train acc:  0.8515625
train loss:  0.3673626780509949
train gradient:  0.21484807359972918
iteration : 10705
train acc:  0.859375
train loss:  0.2950926423072815
train gradient:  0.16381584798757448
iteration : 10706
train acc:  0.9140625
train loss:  0.28284549713134766
train gradient:  0.13089301360492073
iteration : 10707
train acc:  0.8359375
train loss:  0.33828306198120117
train gradient:  0.19653962879589443
iteration : 10708
train acc:  0.796875
train loss:  0.3847193717956543
train gradient:  0.2184481904055181
iteration : 10709
train acc:  0.875
train loss:  0.2848457396030426
train gradient:  0.1456428117875141
iteration : 10710
train acc:  0.90625
train loss:  0.24434632062911987
train gradient:  0.08641430672583944
iteration : 10711
train acc:  0.8828125
train loss:  0.23791547119617462
train gradient:  0.11167728473770526
iteration : 10712
train acc:  0.8125
train loss:  0.38002538681030273
train gradient:  0.24210015028062348
iteration : 10713
train acc:  0.8671875
train loss:  0.33406195044517517
train gradient:  0.1590440570079616
iteration : 10714
train acc:  0.84375
train loss:  0.33956798911094666
train gradient:  0.19011708920316014
iteration : 10715
train acc:  0.7890625
train loss:  0.37949275970458984
train gradient:  0.21828515831905004
iteration : 10716
train acc:  0.859375
train loss:  0.2550930976867676
train gradient:  0.09565640477853157
iteration : 10717
train acc:  0.8515625
train loss:  0.3200148940086365
train gradient:  0.15735944929092516
iteration : 10718
train acc:  0.8984375
train loss:  0.28886449337005615
train gradient:  0.11778742875389421
iteration : 10719
train acc:  0.828125
train loss:  0.3984769880771637
train gradient:  0.23140706259942434
iteration : 10720
train acc:  0.8515625
train loss:  0.3586611747741699
train gradient:  0.1361083501108294
iteration : 10721
train acc:  0.8359375
train loss:  0.34015923738479614
train gradient:  0.15448573848681146
iteration : 10722
train acc:  0.828125
train loss:  0.3392874598503113
train gradient:  0.19602430471801166
iteration : 10723
train acc:  0.828125
train loss:  0.3554677367210388
train gradient:  0.16499692073923916
iteration : 10724
train acc:  0.8203125
train loss:  0.38955390453338623
train gradient:  0.20894533410662453
iteration : 10725
train acc:  0.8515625
train loss:  0.3729398250579834
train gradient:  0.28587862370421646
iteration : 10726
train acc:  0.828125
train loss:  0.3871919512748718
train gradient:  0.2004520198907695
iteration : 10727
train acc:  0.875
train loss:  0.32680386304855347
train gradient:  0.15255840973662968
iteration : 10728
train acc:  0.8984375
train loss:  0.30699092149734497
train gradient:  0.26944758211872266
iteration : 10729
train acc:  0.8203125
train loss:  0.4187285900115967
train gradient:  0.27091457213587744
iteration : 10730
train acc:  0.921875
train loss:  0.25446078181266785
train gradient:  0.11426501909989324
iteration : 10731
train acc:  0.84375
train loss:  0.3521069884300232
train gradient:  0.13982257392860425
iteration : 10732
train acc:  0.90625
train loss:  0.2614278793334961
train gradient:  0.10590350687339888
iteration : 10733
train acc:  0.890625
train loss:  0.3033776879310608
train gradient:  0.17126999486951955
iteration : 10734
train acc:  0.78125
train loss:  0.4520374834537506
train gradient:  0.2246241429535239
iteration : 10735
train acc:  0.8671875
train loss:  0.29607677459716797
train gradient:  0.1046068284818414
iteration : 10736
train acc:  0.890625
train loss:  0.25367504358291626
train gradient:  0.08661009744300213
iteration : 10737
train acc:  0.8515625
train loss:  0.36384186148643494
train gradient:  0.20469460841315146
iteration : 10738
train acc:  0.8515625
train loss:  0.3460220992565155
train gradient:  0.17542813212880312
iteration : 10739
train acc:  0.875
train loss:  0.2922903299331665
train gradient:  0.13903420990975132
iteration : 10740
train acc:  0.828125
train loss:  0.3465777039527893
train gradient:  0.15493253446145244
iteration : 10741
train acc:  0.859375
train loss:  0.301832914352417
train gradient:  0.16744225133356921
iteration : 10742
train acc:  0.875
train loss:  0.28126251697540283
train gradient:  0.16779138559467296
iteration : 10743
train acc:  0.8125
train loss:  0.4303998351097107
train gradient:  0.3095283326325768
iteration : 10744
train acc:  0.84375
train loss:  0.3466234803199768
train gradient:  0.18220762675901214
iteration : 10745
train acc:  0.8671875
train loss:  0.31548991799354553
train gradient:  0.12154549134995676
iteration : 10746
train acc:  0.8984375
train loss:  0.24860484898090363
train gradient:  0.10657910216800237
iteration : 10747
train acc:  0.875
train loss:  0.33297693729400635
train gradient:  0.13225196795044705
iteration : 10748
train acc:  0.8671875
train loss:  0.2802617847919464
train gradient:  0.13067289020948136
iteration : 10749
train acc:  0.8671875
train loss:  0.29900917410850525
train gradient:  0.11816827077962118
iteration : 10750
train acc:  0.796875
train loss:  0.4415488839149475
train gradient:  0.24899112688991443
iteration : 10751
train acc:  0.8515625
train loss:  0.3222118020057678
train gradient:  0.15636991662718536
iteration : 10752
train acc:  0.8828125
train loss:  0.3459463119506836
train gradient:  0.13129402211626243
iteration : 10753
train acc:  0.859375
train loss:  0.32371455430984497
train gradient:  0.1554606394796273
iteration : 10754
train acc:  0.8671875
train loss:  0.2879917621612549
train gradient:  0.127240057101615
iteration : 10755
train acc:  0.8828125
train loss:  0.27295491099357605
train gradient:  0.11209075438282617
iteration : 10756
train acc:  0.8515625
train loss:  0.3583506941795349
train gradient:  0.17221750090412252
iteration : 10757
train acc:  0.8828125
train loss:  0.2849014401435852
train gradient:  0.13131704720503068
iteration : 10758
train acc:  0.828125
train loss:  0.34272217750549316
train gradient:  0.1518931362989242
iteration : 10759
train acc:  0.8515625
train loss:  0.34616413712501526
train gradient:  0.1552209684622039
iteration : 10760
train acc:  0.8359375
train loss:  0.33162692189216614
train gradient:  0.16765762427844214
iteration : 10761
train acc:  0.8828125
train loss:  0.28354448080062866
train gradient:  0.1382635345064385
iteration : 10762
train acc:  0.8671875
train loss:  0.33383098244667053
train gradient:  0.1380066194704051
iteration : 10763
train acc:  0.796875
train loss:  0.3709790110588074
train gradient:  0.23436407166625506
iteration : 10764
train acc:  0.84375
train loss:  0.3032833933830261
train gradient:  0.14753667081880506
iteration : 10765
train acc:  0.8671875
train loss:  0.36095964908599854
train gradient:  0.19496625971901432
iteration : 10766
train acc:  0.8515625
train loss:  0.29802149534225464
train gradient:  0.12465792921046256
iteration : 10767
train acc:  0.90625
train loss:  0.2473798245191574
train gradient:  0.10244332395144551
iteration : 10768
train acc:  0.8671875
train loss:  0.32536569237709045
train gradient:  0.16230988468182256
iteration : 10769
train acc:  0.890625
train loss:  0.3004177212715149
train gradient:  0.12790427099376114
iteration : 10770
train acc:  0.8671875
train loss:  0.30788496136665344
train gradient:  0.14092789178305232
iteration : 10771
train acc:  0.8515625
train loss:  0.30337053537368774
train gradient:  0.15217135547361815
iteration : 10772
train acc:  0.859375
train loss:  0.2875853180885315
train gradient:  0.1292842892668986
iteration : 10773
train acc:  0.859375
train loss:  0.2755151689052582
train gradient:  0.15502378115916088
iteration : 10774
train acc:  0.84375
train loss:  0.31428202986717224
train gradient:  0.168017123906202
iteration : 10775
train acc:  0.8125
train loss:  0.39636537432670593
train gradient:  0.315405032891235
iteration : 10776
train acc:  0.859375
train loss:  0.32805952429771423
train gradient:  0.15246747617539547
iteration : 10777
train acc:  0.875
train loss:  0.31773728132247925
train gradient:  0.15033329758774677
iteration : 10778
train acc:  0.8671875
train loss:  0.27795854210853577
train gradient:  0.12713959186260448
iteration : 10779
train acc:  0.8359375
train loss:  0.31709185242652893
train gradient:  0.13970568877670297
iteration : 10780
train acc:  0.859375
train loss:  0.40320277214050293
train gradient:  0.19490127380199826
iteration : 10781
train acc:  0.8515625
train loss:  0.31695738434791565
train gradient:  0.13206338563509218
iteration : 10782
train acc:  0.875
train loss:  0.3221888244152069
train gradient:  0.16534106032554163
iteration : 10783
train acc:  0.84375
train loss:  0.3670687675476074
train gradient:  0.21190659420282953
iteration : 10784
train acc:  0.8515625
train loss:  0.30832648277282715
train gradient:  0.18135332839323892
iteration : 10785
train acc:  0.8984375
train loss:  0.24556288123130798
train gradient:  0.10452611755810134
iteration : 10786
train acc:  0.859375
train loss:  0.31694191694259644
train gradient:  0.11395550612539213
iteration : 10787
train acc:  0.8359375
train loss:  0.3782854676246643
train gradient:  0.23726346097327328
iteration : 10788
train acc:  0.859375
train loss:  0.31047046184539795
train gradient:  0.1625030406991506
iteration : 10789
train acc:  0.8515625
train loss:  0.3786143660545349
train gradient:  0.1694441159787044
iteration : 10790
train acc:  0.8125
train loss:  0.4084004759788513
train gradient:  0.20369998543794243
iteration : 10791
train acc:  0.8125
train loss:  0.3583982288837433
train gradient:  0.1654718596398202
iteration : 10792
train acc:  0.875
train loss:  0.29267850518226624
train gradient:  0.13264663388684245
iteration : 10793
train acc:  0.90625
train loss:  0.27700600028038025
train gradient:  0.11146595240735184
iteration : 10794
train acc:  0.8671875
train loss:  0.355098694562912
train gradient:  0.20917673137501772
iteration : 10795
train acc:  0.859375
train loss:  0.38404712080955505
train gradient:  0.22206330670071162
iteration : 10796
train acc:  0.8359375
train loss:  0.35983049869537354
train gradient:  0.20001289767317607
iteration : 10797
train acc:  0.796875
train loss:  0.41820383071899414
train gradient:  0.2307040766573671
iteration : 10798
train acc:  0.875
train loss:  0.32961219549179077
train gradient:  0.1216987127029851
iteration : 10799
train acc:  0.84375
train loss:  0.33739131689071655
train gradient:  0.1320913285893151
iteration : 10800
train acc:  0.8203125
train loss:  0.36955198645591736
train gradient:  0.21207821580129696
iteration : 10801
train acc:  0.796875
train loss:  0.3840932846069336
train gradient:  0.20084859756281292
iteration : 10802
train acc:  0.875
train loss:  0.2515515089035034
train gradient:  0.10282413987904476
iteration : 10803
train acc:  0.8671875
train loss:  0.34374135732650757
train gradient:  0.16360660591720722
iteration : 10804
train acc:  0.859375
train loss:  0.27651357650756836
train gradient:  0.11508278853991562
iteration : 10805
train acc:  0.859375
train loss:  0.3237078785896301
train gradient:  0.15656489621025427
iteration : 10806
train acc:  0.890625
train loss:  0.25582578778266907
train gradient:  0.17877964220887185
iteration : 10807
train acc:  0.8203125
train loss:  0.33192968368530273
train gradient:  0.2423242271696723
iteration : 10808
train acc:  0.796875
train loss:  0.44242435693740845
train gradient:  0.2301641141985269
iteration : 10809
train acc:  0.8203125
train loss:  0.3662567138671875
train gradient:  0.2396539029206966
iteration : 10810
train acc:  0.8515625
train loss:  0.3173007667064667
train gradient:  0.17047698551300033
iteration : 10811
train acc:  0.8984375
train loss:  0.26704537868499756
train gradient:  0.0925993116350147
iteration : 10812
train acc:  0.84375
train loss:  0.37850481271743774
train gradient:  0.16843536315011595
iteration : 10813
train acc:  0.78125
train loss:  0.4173690676689148
train gradient:  0.2547904723534977
iteration : 10814
train acc:  0.890625
train loss:  0.2991952896118164
train gradient:  0.12697304738624668
iteration : 10815
train acc:  0.8984375
train loss:  0.26700806617736816
train gradient:  0.10223099599773136
iteration : 10816
train acc:  0.8515625
train loss:  0.3208980858325958
train gradient:  0.137096371783049
iteration : 10817
train acc:  0.890625
train loss:  0.22640183568000793
train gradient:  0.09337503211302704
iteration : 10818
train acc:  0.7890625
train loss:  0.41388893127441406
train gradient:  0.2314921010697331
iteration : 10819
train acc:  0.875
train loss:  0.33689749240875244
train gradient:  0.19806038828863062
iteration : 10820
train acc:  0.7890625
train loss:  0.4677433967590332
train gradient:  0.3404934265070825
iteration : 10821
train acc:  0.8046875
train loss:  0.35582125186920166
train gradient:  0.1917515115737662
iteration : 10822
train acc:  0.859375
train loss:  0.2949087619781494
train gradient:  0.12637726018928633
iteration : 10823
train acc:  0.890625
train loss:  0.29796087741851807
train gradient:  0.1614152352499622
iteration : 10824
train acc:  0.8046875
train loss:  0.4293777048587799
train gradient:  0.28644436287372566
iteration : 10825
train acc:  0.8359375
train loss:  0.37909698486328125
train gradient:  0.18136006505995667
iteration : 10826
train acc:  0.8828125
train loss:  0.31908297538757324
train gradient:  0.12600649983399864
iteration : 10827
train acc:  0.875
train loss:  0.2778725326061249
train gradient:  0.11901438781215375
iteration : 10828
train acc:  0.875
train loss:  0.3056850731372833
train gradient:  0.18535693113770868
iteration : 10829
train acc:  0.8671875
train loss:  0.2911355793476105
train gradient:  0.16685047174763942
iteration : 10830
train acc:  0.8984375
train loss:  0.28643524646759033
train gradient:  0.09517969897350358
iteration : 10831
train acc:  0.796875
train loss:  0.36097633838653564
train gradient:  0.14325482802464068
iteration : 10832
train acc:  0.9140625
train loss:  0.26671266555786133
train gradient:  0.10823132973893071
iteration : 10833
train acc:  0.8984375
train loss:  0.2566213607788086
train gradient:  0.12725382345440245
iteration : 10834
train acc:  0.859375
train loss:  0.29904913902282715
train gradient:  0.09769458965220909
iteration : 10835
train acc:  0.8984375
train loss:  0.33602142333984375
train gradient:  0.09539182544539822
iteration : 10836
train acc:  0.8046875
train loss:  0.36675915122032166
train gradient:  0.22773407020445852
iteration : 10837
train acc:  0.90625
train loss:  0.25608110427856445
train gradient:  0.09383946188479689
iteration : 10838
train acc:  0.8515625
train loss:  0.36561039090156555
train gradient:  0.19802210204317422
iteration : 10839
train acc:  0.8515625
train loss:  0.3534553050994873
train gradient:  0.16564428249761276
iteration : 10840
train acc:  0.859375
train loss:  0.3669940233230591
train gradient:  0.14821301457955616
iteration : 10841
train acc:  0.8828125
train loss:  0.32028335332870483
train gradient:  0.17094256283695375
iteration : 10842
train acc:  0.828125
train loss:  0.3680070638656616
train gradient:  0.1812331151491059
iteration : 10843
train acc:  0.8359375
train loss:  0.38620567321777344
train gradient:  0.195065154117898
iteration : 10844
train acc:  0.8671875
train loss:  0.3503883183002472
train gradient:  0.13838694860832595
iteration : 10845
train acc:  0.8984375
train loss:  0.27828511595726013
train gradient:  0.12895875887970612
iteration : 10846
train acc:  0.84375
train loss:  0.34907159209251404
train gradient:  0.17702373045177883
iteration : 10847
train acc:  0.90625
train loss:  0.293185830116272
train gradient:  0.12545497733138214
iteration : 10848
train acc:  0.875
train loss:  0.33933138847351074
train gradient:  0.11412349668746252
iteration : 10849
train acc:  0.859375
train loss:  0.28204286098480225
train gradient:  0.1318793732775214
iteration : 10850
train acc:  0.8125
train loss:  0.37018638849258423
train gradient:  0.17247085457548766
iteration : 10851
train acc:  0.8125
train loss:  0.4060497283935547
train gradient:  0.23035573722405364
iteration : 10852
train acc:  0.8828125
train loss:  0.253462553024292
train gradient:  0.15965429932017514
iteration : 10853
train acc:  0.8125
train loss:  0.364479660987854
train gradient:  0.1554582143821318
iteration : 10854
train acc:  0.875
train loss:  0.2781212031841278
train gradient:  0.1086166250908708
iteration : 10855
train acc:  0.796875
train loss:  0.4245891571044922
train gradient:  0.22025471043875122
iteration : 10856
train acc:  0.7890625
train loss:  0.40545225143432617
train gradient:  0.23858372953304466
iteration : 10857
train acc:  0.859375
train loss:  0.29975277185440063
train gradient:  0.11680801444422599
iteration : 10858
train acc:  0.8671875
train loss:  0.3593442440032959
train gradient:  0.16091143890473258
iteration : 10859
train acc:  0.875
train loss:  0.2815616726875305
train gradient:  0.1123150053008761
iteration : 10860
train acc:  0.8671875
train loss:  0.2906886339187622
train gradient:  0.17269748533008436
iteration : 10861
train acc:  0.84375
train loss:  0.3685370683670044
train gradient:  0.14183153904783719
iteration : 10862
train acc:  0.859375
train loss:  0.34240272641181946
train gradient:  0.1369869874517357
iteration : 10863
train acc:  0.90625
train loss:  0.3173508048057556
train gradient:  0.1512008520268966
iteration : 10864
train acc:  0.859375
train loss:  0.34616708755493164
train gradient:  0.16118413640035015
iteration : 10865
train acc:  0.859375
train loss:  0.29257166385650635
train gradient:  0.08991380968043855
iteration : 10866
train acc:  0.859375
train loss:  0.2966148257255554
train gradient:  0.10457525004546092
iteration : 10867
train acc:  0.859375
train loss:  0.3754187226295471
train gradient:  0.18542445848271957
iteration : 10868
train acc:  0.84375
train loss:  0.37042027711868286
train gradient:  0.19801825785980648
iteration : 10869
train acc:  0.8671875
train loss:  0.4183415472507477
train gradient:  0.18602288019698088
iteration : 10870
train acc:  0.875
train loss:  0.33453160524368286
train gradient:  0.1667695373435325
iteration : 10871
train acc:  0.859375
train loss:  0.316639244556427
train gradient:  0.15304885842167784
iteration : 10872
train acc:  0.828125
train loss:  0.3656866252422333
train gradient:  0.26025469177581334
iteration : 10873
train acc:  0.8671875
train loss:  0.28307124972343445
train gradient:  0.1069357624518171
iteration : 10874
train acc:  0.890625
train loss:  0.2900589108467102
train gradient:  0.12672826594844444
iteration : 10875
train acc:  0.8046875
train loss:  0.37148648500442505
train gradient:  0.15821310519538673
iteration : 10876
train acc:  0.8984375
train loss:  0.2572087049484253
train gradient:  0.11884607717477247
iteration : 10877
train acc:  0.8359375
train loss:  0.32935214042663574
train gradient:  0.14988701985542163
iteration : 10878
train acc:  0.875
train loss:  0.3310531973838806
train gradient:  0.17648545720423586
iteration : 10879
train acc:  0.8828125
train loss:  0.2801287770271301
train gradient:  0.10586390766391951
iteration : 10880
train acc:  0.828125
train loss:  0.34791621565818787
train gradient:  0.1641108302970352
iteration : 10881
train acc:  0.890625
train loss:  0.32001692056655884
train gradient:  0.1697218420338979
iteration : 10882
train acc:  0.8359375
train loss:  0.3165980577468872
train gradient:  0.12848623378606105
iteration : 10883
train acc:  0.8828125
train loss:  0.2643170952796936
train gradient:  0.17620966222980905
iteration : 10884
train acc:  0.8671875
train loss:  0.3280431628227234
train gradient:  0.15518384512805827
iteration : 10885
train acc:  0.8359375
train loss:  0.32922473549842834
train gradient:  0.28505634332275154
iteration : 10886
train acc:  0.8359375
train loss:  0.41396379470825195
train gradient:  0.3030673621903212
iteration : 10887
train acc:  0.8828125
train loss:  0.2886022925376892
train gradient:  0.11371704010116009
iteration : 10888
train acc:  0.859375
train loss:  0.3857372999191284
train gradient:  0.22362969106442848
iteration : 10889
train acc:  0.8515625
train loss:  0.2869660556316376
train gradient:  0.15123860763787558
iteration : 10890
train acc:  0.9140625
train loss:  0.27625101804733276
train gradient:  0.13463542839826065
iteration : 10891
train acc:  0.8515625
train loss:  0.4429583251476288
train gradient:  0.25250806217347055
iteration : 10892
train acc:  0.796875
train loss:  0.43233343958854675
train gradient:  0.25268813254190403
iteration : 10893
train acc:  0.8984375
train loss:  0.251198947429657
train gradient:  0.13138171068885918
iteration : 10894
train acc:  0.890625
train loss:  0.30530455708503723
train gradient:  0.13114131611698815
iteration : 10895
train acc:  0.8828125
train loss:  0.28897204995155334
train gradient:  0.16182702328773402
iteration : 10896
train acc:  0.84375
train loss:  0.38065803050994873
train gradient:  0.17873485355384833
iteration : 10897
train acc:  0.8515625
train loss:  0.2916295528411865
train gradient:  0.12503202663229265
iteration : 10898
train acc:  0.859375
train loss:  0.30643796920776367
train gradient:  0.13833298255051854
iteration : 10899
train acc:  0.8125
train loss:  0.44399750232696533
train gradient:  0.2520738457860146
iteration : 10900
train acc:  0.8671875
train loss:  0.2822939157485962
train gradient:  0.15610171876477805
iteration : 10901
train acc:  0.859375
train loss:  0.31810107827186584
train gradient:  0.15632862163475741
iteration : 10902
train acc:  0.84375
train loss:  0.35822612047195435
train gradient:  0.2071633644335138
iteration : 10903
train acc:  0.828125
train loss:  0.3471825122833252
train gradient:  0.20608904187027677
iteration : 10904
train acc:  0.8828125
train loss:  0.2840072512626648
train gradient:  0.12866835809817845
iteration : 10905
train acc:  0.90625
train loss:  0.288716197013855
train gradient:  0.11595874919544834
iteration : 10906
train acc:  0.8828125
train loss:  0.3007526397705078
train gradient:  0.12824720081173097
iteration : 10907
train acc:  0.8828125
train loss:  0.29093700647354126
train gradient:  0.10719787362573503
iteration : 10908
train acc:  0.859375
train loss:  0.3693457841873169
train gradient:  0.17595483443757529
iteration : 10909
train acc:  0.8359375
train loss:  0.3012891411781311
train gradient:  0.11037012070084662
iteration : 10910
train acc:  0.828125
train loss:  0.3741408586502075
train gradient:  0.2535399640002135
iteration : 10911
train acc:  0.78125
train loss:  0.4377506673336029
train gradient:  0.2949907568534211
iteration : 10912
train acc:  0.828125
train loss:  0.3941301703453064
train gradient:  0.26349266170510427
iteration : 10913
train acc:  0.8203125
train loss:  0.35488274693489075
train gradient:  0.18689412605154082
iteration : 10914
train acc:  0.859375
train loss:  0.3316640853881836
train gradient:  0.19864214897266275
iteration : 10915
train acc:  0.8828125
train loss:  0.30411338806152344
train gradient:  0.1515692542366877
iteration : 10916
train acc:  0.828125
train loss:  0.3373035192489624
train gradient:  0.17490502440200834
iteration : 10917
train acc:  0.8515625
train loss:  0.4044342041015625
train gradient:  0.14068820775101776
iteration : 10918
train acc:  0.8359375
train loss:  0.40186625719070435
train gradient:  0.2100172114766195
iteration : 10919
train acc:  0.8671875
train loss:  0.3230978846549988
train gradient:  0.1560620133482799
iteration : 10920
train acc:  0.9140625
train loss:  0.2621931731700897
train gradient:  0.08846397843786662
iteration : 10921
train acc:  0.796875
train loss:  0.4135839641094208
train gradient:  0.21408272672313794
iteration : 10922
train acc:  0.890625
train loss:  0.3005320429801941
train gradient:  0.14831470658494914
iteration : 10923
train acc:  0.828125
train loss:  0.3677404224872589
train gradient:  0.17676886030339606
iteration : 10924
train acc:  0.8125
train loss:  0.40602946281433105
train gradient:  0.2869185464121405
iteration : 10925
train acc:  0.890625
train loss:  0.27431219816207886
train gradient:  0.14564976606231408
iteration : 10926
train acc:  0.8359375
train loss:  0.31101417541503906
train gradient:  0.12102056502628755
iteration : 10927
train acc:  0.8046875
train loss:  0.4187774062156677
train gradient:  0.2845291466645449
iteration : 10928
train acc:  0.8515625
train loss:  0.31970131397247314
train gradient:  0.1757683613642304
iteration : 10929
train acc:  0.84375
train loss:  0.33828461170196533
train gradient:  0.15942527868116346
iteration : 10930
train acc:  0.875
train loss:  0.3546060025691986
train gradient:  0.14331524507201154
iteration : 10931
train acc:  0.84375
train loss:  0.32298359274864197
train gradient:  0.1743909628828822
iteration : 10932
train acc:  0.828125
train loss:  0.34779131412506104
train gradient:  0.14094796571597465
iteration : 10933
train acc:  0.8671875
train loss:  0.27184611558914185
train gradient:  0.0700528424813465
iteration : 10934
train acc:  0.890625
train loss:  0.284315824508667
train gradient:  0.12778569214314384
iteration : 10935
train acc:  0.8671875
train loss:  0.3491741418838501
train gradient:  0.14201813888675155
iteration : 10936
train acc:  0.90625
train loss:  0.29325157403945923
train gradient:  0.11881640769443697
iteration : 10937
train acc:  0.8828125
train loss:  0.28287917375564575
train gradient:  0.09541098282708545
iteration : 10938
train acc:  0.84375
train loss:  0.3861955404281616
train gradient:  0.2094795238092041
iteration : 10939
train acc:  0.8359375
train loss:  0.32480761408805847
train gradient:  0.14069673714942554
iteration : 10940
train acc:  0.875
train loss:  0.2658395767211914
train gradient:  0.15228013431489065
iteration : 10941
train acc:  0.8828125
train loss:  0.2943743169307709
train gradient:  0.17390857074406196
iteration : 10942
train acc:  0.78125
train loss:  0.4392136335372925
train gradient:  0.2520918715033237
iteration : 10943
train acc:  0.8359375
train loss:  0.34051352739334106
train gradient:  0.19951581915886696
iteration : 10944
train acc:  0.875
train loss:  0.34340929985046387
train gradient:  0.17423307256322984
iteration : 10945
train acc:  0.875
train loss:  0.3279029130935669
train gradient:  0.13877649189913221
iteration : 10946
train acc:  0.921875
train loss:  0.2441335916519165
train gradient:  0.09370590685324068
iteration : 10947
train acc:  0.859375
train loss:  0.3092309534549713
train gradient:  0.15933610637663503
iteration : 10948
train acc:  0.8671875
train loss:  0.27281540632247925
train gradient:  0.12009148509264286
iteration : 10949
train acc:  0.8828125
train loss:  0.27810174226760864
train gradient:  0.0848311532569762
iteration : 10950
train acc:  0.8203125
train loss:  0.37739455699920654
train gradient:  0.17079571784556677
iteration : 10951
train acc:  0.8359375
train loss:  0.3599312901496887
train gradient:  0.1555690207176541
iteration : 10952
train acc:  0.875
train loss:  0.32642465829849243
train gradient:  0.15942010133183873
iteration : 10953
train acc:  0.8984375
train loss:  0.30453601479530334
train gradient:  0.14040703441361213
iteration : 10954
train acc:  0.8515625
train loss:  0.32255175709724426
train gradient:  0.12411904936997627
iteration : 10955
train acc:  0.875
train loss:  0.2849588394165039
train gradient:  0.11626408271789093
iteration : 10956
train acc:  0.8359375
train loss:  0.32832640409469604
train gradient:  0.15189281599673116
iteration : 10957
train acc:  0.890625
train loss:  0.2620837092399597
train gradient:  0.08449302017218661
iteration : 10958
train acc:  0.84375
train loss:  0.31296655535697937
train gradient:  0.1309834243392293
iteration : 10959
train acc:  0.84375
train loss:  0.3549337387084961
train gradient:  0.18063045400187686
iteration : 10960
train acc:  0.875
train loss:  0.3055253028869629
train gradient:  0.1277923179786173
iteration : 10961
train acc:  0.8359375
train loss:  0.4012860357761383
train gradient:  0.22710441652428015
iteration : 10962
train acc:  0.8203125
train loss:  0.41328346729278564
train gradient:  0.23871187387310527
iteration : 10963
train acc:  0.828125
train loss:  0.37516915798187256
train gradient:  0.16831982965252174
iteration : 10964
train acc:  0.8828125
train loss:  0.2757805585861206
train gradient:  0.12137375807947379
iteration : 10965
train acc:  0.8671875
train loss:  0.3082532286643982
train gradient:  0.14421919242350528
iteration : 10966
train acc:  0.859375
train loss:  0.31381574273109436
train gradient:  0.10138846792991912
iteration : 10967
train acc:  0.890625
train loss:  0.28195470571517944
train gradient:  0.15803322357183502
iteration : 10968
train acc:  0.84375
train loss:  0.4204932153224945
train gradient:  0.22767274107274968
iteration : 10969
train acc:  0.8203125
train loss:  0.40190738439559937
train gradient:  0.26722279073161237
iteration : 10970
train acc:  0.8671875
train loss:  0.3173465132713318
train gradient:  0.16428439782139864
iteration : 10971
train acc:  0.796875
train loss:  0.46116334199905396
train gradient:  0.3553915100367103
iteration : 10972
train acc:  0.8125
train loss:  0.38694360852241516
train gradient:  0.1924027314771828
iteration : 10973
train acc:  0.8828125
train loss:  0.2830914855003357
train gradient:  0.1010655915689171
iteration : 10974
train acc:  0.8203125
train loss:  0.39993155002593994
train gradient:  0.21428599013075683
iteration : 10975
train acc:  0.8984375
train loss:  0.257179319858551
train gradient:  0.11756851632346536
iteration : 10976
train acc:  0.859375
train loss:  0.33341875672340393
train gradient:  0.17959513547554204
iteration : 10977
train acc:  0.875
train loss:  0.2966729998588562
train gradient:  0.1260946633278764
iteration : 10978
train acc:  0.8671875
train loss:  0.2952696681022644
train gradient:  0.10522025247467924
iteration : 10979
train acc:  0.84375
train loss:  0.3165372312068939
train gradient:  0.17153034511397483
iteration : 10980
train acc:  0.8125
train loss:  0.41032326221466064
train gradient:  0.22329145737141531
iteration : 10981
train acc:  0.875
train loss:  0.3080381155014038
train gradient:  0.14810620804970898
iteration : 10982
train acc:  0.8984375
train loss:  0.2686844766139984
train gradient:  0.11789960497637123
iteration : 10983
train acc:  0.8515625
train loss:  0.3349042534828186
train gradient:  0.12674155223564226
iteration : 10984
train acc:  0.859375
train loss:  0.3104918599128723
train gradient:  0.16425564585373442
iteration : 10985
train acc:  0.8515625
train loss:  0.3040333390235901
train gradient:  0.13349015918747079
iteration : 10986
train acc:  0.84375
train loss:  0.37099823355674744
train gradient:  0.14729417948124127
iteration : 10987
train acc:  0.859375
train loss:  0.3154897093772888
train gradient:  0.16567926263834248
iteration : 10988
train acc:  0.8515625
train loss:  0.35155433416366577
train gradient:  0.12663739989642567
iteration : 10989
train acc:  0.90625
train loss:  0.316516637802124
train gradient:  0.1337247924530947
iteration : 10990
train acc:  0.8515625
train loss:  0.2723105549812317
train gradient:  0.1204669642609084
iteration : 10991
train acc:  0.890625
train loss:  0.2740646302700043
train gradient:  0.1056519708427027
iteration : 10992
train acc:  0.8125
train loss:  0.38518065214157104
train gradient:  0.2851243598530967
iteration : 10993
train acc:  0.875
train loss:  0.27872493863105774
train gradient:  0.09298664113860758
iteration : 10994
train acc:  0.859375
train loss:  0.3327227830886841
train gradient:  0.14688913828406794
iteration : 10995
train acc:  0.875
train loss:  0.3193967342376709
train gradient:  0.10553101884035251
iteration : 10996
train acc:  0.84375
train loss:  0.3486911952495575
train gradient:  0.17061481595381972
iteration : 10997
train acc:  0.84375
train loss:  0.37132471799850464
train gradient:  0.19071240846863158
iteration : 10998
train acc:  0.8828125
train loss:  0.2876237630844116
train gradient:  0.1313817822925345
iteration : 10999
train acc:  0.859375
train loss:  0.2980669438838959
train gradient:  0.12345357243419036
iteration : 11000
train acc:  0.8515625
train loss:  0.32512837648391724
train gradient:  0.1388739691202629
iteration : 11001
train acc:  0.8671875
train loss:  0.35306960344314575
train gradient:  0.14632760734676595
iteration : 11002
train acc:  0.875
train loss:  0.29051345586776733
train gradient:  0.1324847043064742
iteration : 11003
train acc:  0.828125
train loss:  0.3775117099285126
train gradient:  0.24131058936024485
iteration : 11004
train acc:  0.8203125
train loss:  0.34768927097320557
train gradient:  0.14109100579971137
iteration : 11005
train acc:  0.875
train loss:  0.2710023522377014
train gradient:  0.11909246080971697
iteration : 11006
train acc:  0.875
train loss:  0.3177903890609741
train gradient:  0.15409524760462312
iteration : 11007
train acc:  0.859375
train loss:  0.29445818066596985
train gradient:  0.13065046347593257
iteration : 11008
train acc:  0.8125
train loss:  0.41187584400177
train gradient:  0.29132810985674956
iteration : 11009
train acc:  0.8515625
train loss:  0.4008491039276123
train gradient:  0.19243194147143525
iteration : 11010
train acc:  0.8671875
train loss:  0.28130000829696655
train gradient:  0.1093975526919787
iteration : 11011
train acc:  0.8515625
train loss:  0.3995699882507324
train gradient:  0.28923705414937484
iteration : 11012
train acc:  0.890625
train loss:  0.2904217541217804
train gradient:  0.13485134211095834
iteration : 11013
train acc:  0.8359375
train loss:  0.41977548599243164
train gradient:  0.19537814191850367
iteration : 11014
train acc:  0.84375
train loss:  0.3097434639930725
train gradient:  0.1351729515456172
iteration : 11015
train acc:  0.8359375
train loss:  0.33212900161743164
train gradient:  0.17102444619000254
iteration : 11016
train acc:  0.8125
train loss:  0.36438533663749695
train gradient:  0.19038934501306637
iteration : 11017
train acc:  0.8046875
train loss:  0.3742653727531433
train gradient:  0.2018378989932988
iteration : 11018
train acc:  0.90625
train loss:  0.22121059894561768
train gradient:  0.08954067020700046
iteration : 11019
train acc:  0.890625
train loss:  0.3201751410961151
train gradient:  0.12506580280959667
iteration : 11020
train acc:  0.890625
train loss:  0.2819086015224457
train gradient:  0.1237014551731024
iteration : 11021
train acc:  0.8984375
train loss:  0.33392757177352905
train gradient:  0.16391864204485593
iteration : 11022
train acc:  0.875
train loss:  0.28049468994140625
train gradient:  0.10503472516951941
iteration : 11023
train acc:  0.875
train loss:  0.2684129476547241
train gradient:  0.12443373247913908
iteration : 11024
train acc:  0.875
train loss:  0.2865283489227295
train gradient:  0.14198507773392594
iteration : 11025
train acc:  0.8359375
train loss:  0.36519068479537964
train gradient:  0.18477314219641144
iteration : 11026
train acc:  0.859375
train loss:  0.3283729553222656
train gradient:  0.19436885957956868
iteration : 11027
train acc:  0.828125
train loss:  0.3355300724506378
train gradient:  0.16835011474354383
iteration : 11028
train acc:  0.921875
train loss:  0.25507017970085144
train gradient:  0.09171137693380224
iteration : 11029
train acc:  0.8515625
train loss:  0.35020437836647034
train gradient:  0.1733095109921679
iteration : 11030
train acc:  0.90625
train loss:  0.2846897542476654
train gradient:  0.1178490186968387
iteration : 11031
train acc:  0.890625
train loss:  0.24999991059303284
train gradient:  0.10223538423879232
iteration : 11032
train acc:  0.890625
train loss:  0.28441423177719116
train gradient:  0.12507073600023172
iteration : 11033
train acc:  0.875
train loss:  0.34728753566741943
train gradient:  0.16182610784953685
iteration : 11034
train acc:  0.90625
train loss:  0.2603881359100342
train gradient:  0.12457454400261722
iteration : 11035
train acc:  0.8203125
train loss:  0.38984909653663635
train gradient:  0.23145653681030612
iteration : 11036
train acc:  0.8203125
train loss:  0.38573944568634033
train gradient:  0.19535364432834879
iteration : 11037
train acc:  0.8515625
train loss:  0.3016813397407532
train gradient:  0.17158031123471237
iteration : 11038
train acc:  0.84375
train loss:  0.38146835565567017
train gradient:  0.16667100934367798
iteration : 11039
train acc:  0.84375
train loss:  0.31799525022506714
train gradient:  0.14570128003703958
iteration : 11040
train acc:  0.8203125
train loss:  0.3396558165550232
train gradient:  0.15054980138873064
iteration : 11041
train acc:  0.875
train loss:  0.3086977005004883
train gradient:  0.1257163833310624
iteration : 11042
train acc:  0.828125
train loss:  0.4080738425254822
train gradient:  0.21289286750511027
iteration : 11043
train acc:  0.90625
train loss:  0.26431000232696533
train gradient:  0.09768799450815215
iteration : 11044
train acc:  0.828125
train loss:  0.3480895161628723
train gradient:  0.1854781466953657
iteration : 11045
train acc:  0.8359375
train loss:  0.34777718782424927
train gradient:  0.18531800696506656
iteration : 11046
train acc:  0.859375
train loss:  0.29259246587753296
train gradient:  0.15840408351774732
iteration : 11047
train acc:  0.8515625
train loss:  0.334955096244812
train gradient:  0.16090855680129296
iteration : 11048
train acc:  0.875
train loss:  0.29000604152679443
train gradient:  0.13087834545582777
iteration : 11049
train acc:  0.890625
train loss:  0.30974310636520386
train gradient:  0.11011400881821518
iteration : 11050
train acc:  0.859375
train loss:  0.3156088888645172
train gradient:  0.15076795185188852
iteration : 11051
train acc:  0.8984375
train loss:  0.27373582124710083
train gradient:  0.1382735133094376
iteration : 11052
train acc:  0.8203125
train loss:  0.3664412498474121
train gradient:  0.22376104845079536
iteration : 11053
train acc:  0.8671875
train loss:  0.33600735664367676
train gradient:  0.13893005784855203
iteration : 11054
train acc:  0.8125
train loss:  0.44598329067230225
train gradient:  0.24966792570805937
iteration : 11055
train acc:  0.8828125
train loss:  0.2951658368110657
train gradient:  0.11104914158902729
iteration : 11056
train acc:  0.8984375
train loss:  0.29361337423324585
train gradient:  0.13014853490818845
iteration : 11057
train acc:  0.859375
train loss:  0.3252353072166443
train gradient:  0.15090222792523042
iteration : 11058
train acc:  0.8671875
train loss:  0.28490394353866577
train gradient:  0.13536144982651682
iteration : 11059
train acc:  0.84375
train loss:  0.3709055781364441
train gradient:  0.13064137681027516
iteration : 11060
train acc:  0.8515625
train loss:  0.317098468542099
train gradient:  0.1206463754846481
iteration : 11061
train acc:  0.8359375
train loss:  0.3707510828971863
train gradient:  0.1573571760721107
iteration : 11062
train acc:  0.8359375
train loss:  0.43248406052589417
train gradient:  0.19095673027277837
iteration : 11063
train acc:  0.890625
train loss:  0.24746868014335632
train gradient:  0.08635546248894495
iteration : 11064
train acc:  0.8359375
train loss:  0.33386170864105225
train gradient:  0.24631372084871156
iteration : 11065
train acc:  0.84375
train loss:  0.30428528785705566
train gradient:  0.09704159459817624
iteration : 11066
train acc:  0.8125
train loss:  0.36834150552749634
train gradient:  0.18656815223243822
iteration : 11067
train acc:  0.8984375
train loss:  0.2511550784111023
train gradient:  0.11349524700700254
iteration : 11068
train acc:  0.859375
train loss:  0.3679529130458832
train gradient:  0.1830119550394965
iteration : 11069
train acc:  0.8671875
train loss:  0.2908431887626648
train gradient:  0.13222824526994703
iteration : 11070
train acc:  0.828125
train loss:  0.32248198986053467
train gradient:  0.16564862995290797
iteration : 11071
train acc:  0.875
train loss:  0.30320972204208374
train gradient:  0.1868991742764723
iteration : 11072
train acc:  0.84375
train loss:  0.33587244153022766
train gradient:  0.17604039206924305
iteration : 11073
train acc:  0.8125
train loss:  0.3617848753929138
train gradient:  0.24461000807350788
iteration : 11074
train acc:  0.8125
train loss:  0.3357369899749756
train gradient:  0.1843224286830431
iteration : 11075
train acc:  0.84375
train loss:  0.34598061442375183
train gradient:  0.17869466459865574
iteration : 11076
train acc:  0.8203125
train loss:  0.3811149001121521
train gradient:  0.25282189939293387
iteration : 11077
train acc:  0.859375
train loss:  0.2848380208015442
train gradient:  0.10751251077976998
iteration : 11078
train acc:  0.828125
train loss:  0.33490854501724243
train gradient:  0.14147628524510725
iteration : 11079
train acc:  0.8984375
train loss:  0.30481550097465515
train gradient:  0.17942597635095986
iteration : 11080
train acc:  0.890625
train loss:  0.3085048198699951
train gradient:  0.1834841404621884
iteration : 11081
train acc:  0.90625
train loss:  0.23787911236286163
train gradient:  0.0885710541303187
iteration : 11082
train acc:  0.921875
train loss:  0.2542530596256256
train gradient:  0.09916041020590194
iteration : 11083
train acc:  0.8515625
train loss:  0.3618714511394501
train gradient:  0.16814031602622642
iteration : 11084
train acc:  0.84375
train loss:  0.33600902557373047
train gradient:  0.16550713425646643
iteration : 11085
train acc:  0.8671875
train loss:  0.28704577684402466
train gradient:  0.13515895014089446
iteration : 11086
train acc:  0.890625
train loss:  0.30233922600746155
train gradient:  0.12621513671690945
iteration : 11087
train acc:  0.8671875
train loss:  0.33091700077056885
train gradient:  0.14583298584372417
iteration : 11088
train acc:  0.8671875
train loss:  0.3639523386955261
train gradient:  0.18171610782663278
iteration : 11089
train acc:  0.84375
train loss:  0.3589942455291748
train gradient:  0.16812363929508467
iteration : 11090
train acc:  0.828125
train loss:  0.3374788761138916
train gradient:  0.16862541510465157
iteration : 11091
train acc:  0.828125
train loss:  0.3912196159362793
train gradient:  0.2566686659477403
iteration : 11092
train acc:  0.8828125
train loss:  0.3141166567802429
train gradient:  0.11059210493409757
iteration : 11093
train acc:  0.8515625
train loss:  0.28307437896728516
train gradient:  0.12686143703371183
iteration : 11094
train acc:  0.8203125
train loss:  0.32834750413894653
train gradient:  0.16081608908523304
iteration : 11095
train acc:  0.890625
train loss:  0.32019689679145813
train gradient:  0.15371470299114515
iteration : 11096
train acc:  0.875
train loss:  0.2630625367164612
train gradient:  0.1209908537048227
iteration : 11097
train acc:  0.875
train loss:  0.306820809841156
train gradient:  0.12912436172069705
iteration : 11098
train acc:  0.8671875
train loss:  0.29327529668807983
train gradient:  0.13854488579328594
iteration : 11099
train acc:  0.859375
train loss:  0.3160857558250427
train gradient:  0.15138770978158603
iteration : 11100
train acc:  0.8671875
train loss:  0.3114780783653259
train gradient:  0.15043693885556325
iteration : 11101
train acc:  0.859375
train loss:  0.3182465434074402
train gradient:  0.1710095774897586
iteration : 11102
train acc:  0.8515625
train loss:  0.36582112312316895
train gradient:  0.18647786348130518
iteration : 11103
train acc:  0.8125
train loss:  0.3676490783691406
train gradient:  0.16425937900607673
iteration : 11104
train acc:  0.8671875
train loss:  0.2661689817905426
train gradient:  0.09907891805112698
iteration : 11105
train acc:  0.84375
train loss:  0.3629578649997711
train gradient:  0.19893206001758973
iteration : 11106
train acc:  0.84375
train loss:  0.3284648656845093
train gradient:  0.18796731894893912
iteration : 11107
train acc:  0.890625
train loss:  0.2896735668182373
train gradient:  0.11747370654941504
iteration : 11108
train acc:  0.9140625
train loss:  0.26291579008102417
train gradient:  0.11497479305932717
iteration : 11109
train acc:  0.8359375
train loss:  0.3700597882270813
train gradient:  0.18243363896514875
iteration : 11110
train acc:  0.890625
train loss:  0.25864851474761963
train gradient:  0.14122375020361203
iteration : 11111
train acc:  0.8515625
train loss:  0.2975602149963379
train gradient:  0.10196828542165647
iteration : 11112
train acc:  0.84375
train loss:  0.2965394854545593
train gradient:  0.1867760290918169
iteration : 11113
train acc:  0.8671875
train loss:  0.2767106592655182
train gradient:  0.10543606812418357
iteration : 11114
train acc:  0.8984375
train loss:  0.2874920964241028
train gradient:  0.12376948466863251
iteration : 11115
train acc:  0.8671875
train loss:  0.2664451599121094
train gradient:  0.08616749500789357
iteration : 11116
train acc:  0.8984375
train loss:  0.3045279085636139
train gradient:  0.1207208894318226
iteration : 11117
train acc:  0.8671875
train loss:  0.32557016611099243
train gradient:  0.18291296877400412
iteration : 11118
train acc:  0.8203125
train loss:  0.35652458667755127
train gradient:  0.20114132779641156
iteration : 11119
train acc:  0.8203125
train loss:  0.3937973976135254
train gradient:  0.2637744054464306
iteration : 11120
train acc:  0.859375
train loss:  0.3103151321411133
train gradient:  0.15290215645634492
iteration : 11121
train acc:  0.828125
train loss:  0.3958984315395355
train gradient:  0.22312344776939982
iteration : 11122
train acc:  0.828125
train loss:  0.39495739340782166
train gradient:  0.18659690705163273
iteration : 11123
train acc:  0.8125
train loss:  0.41463518142700195
train gradient:  0.22681280219515304
iteration : 11124
train acc:  0.8671875
train loss:  0.2654440402984619
train gradient:  0.11361800639723647
iteration : 11125
train acc:  0.828125
train loss:  0.39517006278038025
train gradient:  0.19271701231007618
iteration : 11126
train acc:  0.8515625
train loss:  0.3660814166069031
train gradient:  0.27094014302597463
iteration : 11127
train acc:  0.765625
train loss:  0.45509636402130127
train gradient:  0.2423306394860836
iteration : 11128
train acc:  0.84375
train loss:  0.30702871084213257
train gradient:  0.12765674874209337
iteration : 11129
train acc:  0.8359375
train loss:  0.4244058132171631
train gradient:  0.2222810611594536
iteration : 11130
train acc:  0.8671875
train loss:  0.2974439263343811
train gradient:  0.10088393010416759
iteration : 11131
train acc:  0.8203125
train loss:  0.2971460819244385
train gradient:  0.17280659393567027
iteration : 11132
train acc:  0.8203125
train loss:  0.36416512727737427
train gradient:  0.1789651211502179
iteration : 11133
train acc:  0.859375
train loss:  0.33313822746276855
train gradient:  0.14745549423195736
iteration : 11134
train acc:  0.90625
train loss:  0.24948744475841522
train gradient:  0.17581792432810345
iteration : 11135
train acc:  0.8515625
train loss:  0.36916881799697876
train gradient:  0.18584025234403048
iteration : 11136
train acc:  0.875
train loss:  0.2968730926513672
train gradient:  0.11212120451194478
iteration : 11137
train acc:  0.875
train loss:  0.2825136184692383
train gradient:  0.1401491912815026
iteration : 11138
train acc:  0.8828125
train loss:  0.2933606505393982
train gradient:  0.09724483475500934
iteration : 11139
train acc:  0.875
train loss:  0.26932162046432495
train gradient:  0.1032882677631298
iteration : 11140
train acc:  0.875
train loss:  0.2927902936935425
train gradient:  0.10340368154824314
iteration : 11141
train acc:  0.859375
train loss:  0.28635671734809875
train gradient:  0.1434987421217874
iteration : 11142
train acc:  0.8515625
train loss:  0.36794304847717285
train gradient:  0.1698095630206139
iteration : 11143
train acc:  0.8984375
train loss:  0.2763746976852417
train gradient:  0.11069428933977597
iteration : 11144
train acc:  0.8359375
train loss:  0.3717651069164276
train gradient:  0.16109084833058485
iteration : 11145
train acc:  0.828125
train loss:  0.39036810398101807
train gradient:  0.26684242510890266
iteration : 11146
train acc:  0.828125
train loss:  0.34659910202026367
train gradient:  0.15833129667587026
iteration : 11147
train acc:  0.8671875
train loss:  0.3394084572792053
train gradient:  0.1319027993461772
iteration : 11148
train acc:  0.8359375
train loss:  0.3431446850299835
train gradient:  0.12589780485019297
iteration : 11149
train acc:  0.890625
train loss:  0.2686418890953064
train gradient:  0.12357716761820832
iteration : 11150
train acc:  0.8984375
train loss:  0.3101043403148651
train gradient:  0.11990355525094433
iteration : 11151
train acc:  0.828125
train loss:  0.4119485020637512
train gradient:  0.21027766601346126
iteration : 11152
train acc:  0.8359375
train loss:  0.39001697301864624
train gradient:  0.2154592644641255
iteration : 11153
train acc:  0.90625
train loss:  0.296500563621521
train gradient:  0.1782851664903471
iteration : 11154
train acc:  0.8515625
train loss:  0.3140886425971985
train gradient:  0.13613334176695172
iteration : 11155
train acc:  0.8671875
train loss:  0.28346729278564453
train gradient:  0.10397131452176667
iteration : 11156
train acc:  0.8984375
train loss:  0.2604754567146301
train gradient:  0.1309184313481467
iteration : 11157
train acc:  0.8828125
train loss:  0.30478590726852417
train gradient:  0.10969255078963541
iteration : 11158
train acc:  0.8515625
train loss:  0.3484145402908325
train gradient:  0.19071618597637574
iteration : 11159
train acc:  0.8125
train loss:  0.35349470376968384
train gradient:  0.16173102640463558
iteration : 11160
train acc:  0.875
train loss:  0.31433308124542236
train gradient:  0.1358215567747289
iteration : 11161
train acc:  0.859375
train loss:  0.28813403844833374
train gradient:  0.1161472850402188
iteration : 11162
train acc:  0.828125
train loss:  0.3992758095264435
train gradient:  0.1469951633243693
iteration : 11163
train acc:  0.8671875
train loss:  0.3106001317501068
train gradient:  0.16029626693869717
iteration : 11164
train acc:  0.796875
train loss:  0.4278198480606079
train gradient:  0.34013275698274015
iteration : 11165
train acc:  0.8515625
train loss:  0.337497353553772
train gradient:  0.15999605315400278
iteration : 11166
train acc:  0.890625
train loss:  0.28688931465148926
train gradient:  0.17565696107329343
iteration : 11167
train acc:  0.84375
train loss:  0.42438387870788574
train gradient:  0.19119241724277797
iteration : 11168
train acc:  0.859375
train loss:  0.29962751269340515
train gradient:  0.1780906535380467
iteration : 11169
train acc:  0.890625
train loss:  0.2574915885925293
train gradient:  0.10641928023219269
iteration : 11170
train acc:  0.84375
train loss:  0.3504437804222107
train gradient:  0.16273302847421228
iteration : 11171
train acc:  0.8125
train loss:  0.4141732156276703
train gradient:  0.23292384225987917
iteration : 11172
train acc:  0.859375
train loss:  0.332288920879364
train gradient:  0.13735499547760827
iteration : 11173
train acc:  0.8359375
train loss:  0.3844105899333954
train gradient:  0.19198943164269722
iteration : 11174
train acc:  0.84375
train loss:  0.4427739083766937
train gradient:  0.2313864169900832
iteration : 11175
train acc:  0.8828125
train loss:  0.2771473526954651
train gradient:  0.11826102725778853
iteration : 11176
train acc:  0.9375
train loss:  0.2092607617378235
train gradient:  0.09179058087399537
iteration : 11177
train acc:  0.859375
train loss:  0.3417893052101135
train gradient:  0.1619343241309716
iteration : 11178
train acc:  0.7890625
train loss:  0.44777095317840576
train gradient:  0.2416959620910129
iteration : 11179
train acc:  0.8984375
train loss:  0.28640106320381165
train gradient:  0.12306630672262506
iteration : 11180
train acc:  0.84375
train loss:  0.3324754238128662
train gradient:  0.1778461630936397
iteration : 11181
train acc:  0.859375
train loss:  0.3246219754219055
train gradient:  0.1453915167808857
iteration : 11182
train acc:  0.8671875
train loss:  0.3422100841999054
train gradient:  0.16312529790752783
iteration : 11183
train acc:  0.8828125
train loss:  0.29801318049430847
train gradient:  0.1247961497591067
iteration : 11184
train acc:  0.8984375
train loss:  0.24376779794692993
train gradient:  0.10036559738898557
iteration : 11185
train acc:  0.8125
train loss:  0.37928450107574463
train gradient:  0.16987885228479133
iteration : 11186
train acc:  0.859375
train loss:  0.30818596482276917
train gradient:  0.10849640377465936
iteration : 11187
train acc:  0.8203125
train loss:  0.3422727584838867
train gradient:  0.15745291910034404
iteration : 11188
train acc:  0.890625
train loss:  0.29174500703811646
train gradient:  0.1095160187345918
iteration : 11189
train acc:  0.8671875
train loss:  0.28127092123031616
train gradient:  0.16452874291333033
iteration : 11190
train acc:  0.875
train loss:  0.26553139090538025
train gradient:  0.09744243971731048
iteration : 11191
train acc:  0.859375
train loss:  0.2860116958618164
train gradient:  0.08918313614379872
iteration : 11192
train acc:  0.8828125
train loss:  0.29414093494415283
train gradient:  0.12360351955181927
iteration : 11193
train acc:  0.8671875
train loss:  0.28972741961479187
train gradient:  0.10842092866723829
iteration : 11194
train acc:  0.8671875
train loss:  0.37098872661590576
train gradient:  0.24317678376929278
iteration : 11195
train acc:  0.8203125
train loss:  0.345785528421402
train gradient:  0.176167995709803
iteration : 11196
train acc:  0.890625
train loss:  0.3138415217399597
train gradient:  0.1563852411569903
iteration : 11197
train acc:  0.875
train loss:  0.2996114492416382
train gradient:  0.14306598675889742
iteration : 11198
train acc:  0.8515625
train loss:  0.311176598072052
train gradient:  0.14794619658401043
iteration : 11199
train acc:  0.859375
train loss:  0.29337841272354126
train gradient:  0.13818981467099933
iteration : 11200
train acc:  0.8515625
train loss:  0.32499590516090393
train gradient:  0.14935285701039588
iteration : 11201
train acc:  0.875
train loss:  0.27714914083480835
train gradient:  0.1357645994967818
iteration : 11202
train acc:  0.875
train loss:  0.2939843535423279
train gradient:  0.12092453787856242
iteration : 11203
train acc:  0.90625
train loss:  0.24335253238677979
train gradient:  0.12129683420033645
iteration : 11204
train acc:  0.8203125
train loss:  0.37524354457855225
train gradient:  0.18226381550987703
iteration : 11205
train acc:  0.875
train loss:  0.2739700376987457
train gradient:  0.12457060221021404
iteration : 11206
train acc:  0.9140625
train loss:  0.299852579832077
train gradient:  0.15122764407368275
iteration : 11207
train acc:  0.921875
train loss:  0.21914951503276825
train gradient:  0.08505297812235436
iteration : 11208
train acc:  0.8828125
train loss:  0.2913433313369751
train gradient:  0.18305573217845905
iteration : 11209
train acc:  0.8203125
train loss:  0.41186079382896423
train gradient:  0.1889197311998935
iteration : 11210
train acc:  0.8203125
train loss:  0.40448319911956787
train gradient:  0.2519643484844482
iteration : 11211
train acc:  0.859375
train loss:  0.34808558225631714
train gradient:  0.19719175891315388
iteration : 11212
train acc:  0.859375
train loss:  0.35530370473861694
train gradient:  0.19263769068706052
iteration : 11213
train acc:  0.8671875
train loss:  0.3130047917366028
train gradient:  0.15900031552828978
iteration : 11214
train acc:  0.859375
train loss:  0.3504931330680847
train gradient:  0.17012675336936678
iteration : 11215
train acc:  0.84375
train loss:  0.3036155104637146
train gradient:  0.10527073284346969
iteration : 11216
train acc:  0.8125
train loss:  0.4331139326095581
train gradient:  0.23150515972396477
iteration : 11217
train acc:  0.8828125
train loss:  0.28559792041778564
train gradient:  0.1313826067366417
iteration : 11218
train acc:  0.84375
train loss:  0.45526036620140076
train gradient:  0.2528354525817436
iteration : 11219
train acc:  0.84375
train loss:  0.32950103282928467
train gradient:  0.1850293210955954
iteration : 11220
train acc:  0.875
train loss:  0.3697773814201355
train gradient:  0.2805168695439991
iteration : 11221
train acc:  0.84375
train loss:  0.35095643997192383
train gradient:  0.3591661697487365
iteration : 11222
train acc:  0.8515625
train loss:  0.2965533137321472
train gradient:  0.1116513999409128
iteration : 11223
train acc:  0.8359375
train loss:  0.39925938844680786
train gradient:  0.1702852005807086
iteration : 11224
train acc:  0.90625
train loss:  0.281356543302536
train gradient:  0.1370164311128907
iteration : 11225
train acc:  0.9140625
train loss:  0.2555948495864868
train gradient:  0.13701628615388012
iteration : 11226
train acc:  0.859375
train loss:  0.34121429920196533
train gradient:  0.13643903831421866
iteration : 11227
train acc:  0.8515625
train loss:  0.29297083616256714
train gradient:  0.1416951031460424
iteration : 11228
train acc:  0.859375
train loss:  0.39715760946273804
train gradient:  0.16709158369501403
iteration : 11229
train acc:  0.828125
train loss:  0.33400171995162964
train gradient:  0.18310853290959092
iteration : 11230
train acc:  0.8984375
train loss:  0.2317502796649933
train gradient:  0.09428519268304252
iteration : 11231
train acc:  0.8828125
train loss:  0.30517953634262085
train gradient:  0.1300347670247579
iteration : 11232
train acc:  0.890625
train loss:  0.27525627613067627
train gradient:  0.08966264353419552
iteration : 11233
train acc:  0.8515625
train loss:  0.37393802404403687
train gradient:  0.1954005830901653
iteration : 11234
train acc:  0.90625
train loss:  0.24365365505218506
train gradient:  0.10016704156337382
iteration : 11235
train acc:  0.8984375
train loss:  0.291975200176239
train gradient:  0.10712517545804366
iteration : 11236
train acc:  0.8828125
train loss:  0.273159384727478
train gradient:  0.17049699762245796
iteration : 11237
train acc:  0.859375
train loss:  0.3104984164237976
train gradient:  0.13576689381518334
iteration : 11238
train acc:  0.796875
train loss:  0.44717878103256226
train gradient:  0.2266595643693875
iteration : 11239
train acc:  0.8359375
train loss:  0.3299087882041931
train gradient:  0.1355997777350051
iteration : 11240
train acc:  0.8359375
train loss:  0.35930734872817993
train gradient:  0.18786210014069366
iteration : 11241
train acc:  0.921875
train loss:  0.26880326867103577
train gradient:  0.10440687088621048
iteration : 11242
train acc:  0.8984375
train loss:  0.30484986305236816
train gradient:  0.17004506881839548
iteration : 11243
train acc:  0.90625
train loss:  0.2903977930545807
train gradient:  0.13145721255163662
iteration : 11244
train acc:  0.890625
train loss:  0.30332550406455994
train gradient:  0.12382663377223664
iteration : 11245
train acc:  0.859375
train loss:  0.3238951563835144
train gradient:  0.09190994303886117
iteration : 11246
train acc:  0.8828125
train loss:  0.31950169801712036
train gradient:  0.131607565210472
iteration : 11247
train acc:  0.8515625
train loss:  0.3233327567577362
train gradient:  0.15341487088461858
iteration : 11248
train acc:  0.9140625
train loss:  0.2721782922744751
train gradient:  0.1515306729489427
iteration : 11249
train acc:  0.875
train loss:  0.23871716856956482
train gradient:  0.1045119107982784
iteration : 11250
train acc:  0.828125
train loss:  0.40640532970428467
train gradient:  0.1842250924255251
iteration : 11251
train acc:  0.8984375
train loss:  0.26348260045051575
train gradient:  0.11493281624828186
iteration : 11252
train acc:  0.8828125
train loss:  0.3172120153903961
train gradient:  0.21948399064605767
iteration : 11253
train acc:  0.875
train loss:  0.2905353903770447
train gradient:  0.13875390673632415
iteration : 11254
train acc:  0.875
train loss:  0.27001845836639404
train gradient:  0.10056676546020324
iteration : 11255
train acc:  0.8359375
train loss:  0.4352724254131317
train gradient:  0.25627667158267053
iteration : 11256
train acc:  0.875
train loss:  0.30788591504096985
train gradient:  0.11124928851119228
iteration : 11257
train acc:  0.890625
train loss:  0.2952790856361389
train gradient:  0.1219175125933374
iteration : 11258
train acc:  0.8515625
train loss:  0.3571191430091858
train gradient:  0.24904548730388792
iteration : 11259
train acc:  0.859375
train loss:  0.29020828008651733
train gradient:  0.11077330029449074
iteration : 11260
train acc:  0.8203125
train loss:  0.3555748462677002
train gradient:  0.15270991932615913
iteration : 11261
train acc:  0.8515625
train loss:  0.33494850993156433
train gradient:  0.17708550558396954
iteration : 11262
train acc:  0.859375
train loss:  0.3293127715587616
train gradient:  0.2111802374193557
iteration : 11263
train acc:  0.875
train loss:  0.31253474950790405
train gradient:  0.13108604746814234
iteration : 11264
train acc:  0.90625
train loss:  0.25735849142074585
train gradient:  0.08711748896297086
iteration : 11265
train acc:  0.84375
train loss:  0.3178032636642456
train gradient:  0.24002883450404264
iteration : 11266
train acc:  0.8671875
train loss:  0.3038533329963684
train gradient:  0.13365366381804095
iteration : 11267
train acc:  0.8671875
train loss:  0.3015768826007843
train gradient:  0.16978180199658427
iteration : 11268
train acc:  0.8046875
train loss:  0.3586495816707611
train gradient:  0.39502352227142873
iteration : 11269
train acc:  0.875
train loss:  0.2934022545814514
train gradient:  0.1468610182453855
iteration : 11270
train acc:  0.8359375
train loss:  0.37767210602760315
train gradient:  0.15058479415834702
iteration : 11271
train acc:  0.84375
train loss:  0.36965611577033997
train gradient:  0.182990462459032
iteration : 11272
train acc:  0.8671875
train loss:  0.32601064443588257
train gradient:  0.12617817698777745
iteration : 11273
train acc:  0.859375
train loss:  0.3839848041534424
train gradient:  0.19125217129540745
iteration : 11274
train acc:  0.8125
train loss:  0.3480122685432434
train gradient:  0.18904561197122507
iteration : 11275
train acc:  0.8828125
train loss:  0.29993772506713867
train gradient:  0.11775060564519674
iteration : 11276
train acc:  0.8203125
train loss:  0.35673055052757263
train gradient:  0.23975831248101906
iteration : 11277
train acc:  0.84375
train loss:  0.3170992136001587
train gradient:  0.1255962161616323
iteration : 11278
train acc:  0.875
train loss:  0.286423921585083
train gradient:  0.12469480914398123
iteration : 11279
train acc:  0.8828125
train loss:  0.3493971824645996
train gradient:  0.13477685225287073
iteration : 11280
train acc:  0.8515625
train loss:  0.33236658573150635
train gradient:  0.20915112538589387
iteration : 11281
train acc:  0.9140625
train loss:  0.25399816036224365
train gradient:  0.08912318846216405
iteration : 11282
train acc:  0.8984375
train loss:  0.27584415674209595
train gradient:  0.10478225410065846
iteration : 11283
train acc:  0.875
train loss:  0.3492385447025299
train gradient:  0.1565637252075489
iteration : 11284
train acc:  0.8671875
train loss:  0.286709189414978
train gradient:  0.14242939113978903
iteration : 11285
train acc:  0.8125
train loss:  0.40121990442276
train gradient:  0.19270948600415783
iteration : 11286
train acc:  0.84375
train loss:  0.32807493209838867
train gradient:  0.1346001550218699
iteration : 11287
train acc:  0.9140625
train loss:  0.2473709136247635
train gradient:  0.08499622789658617
iteration : 11288
train acc:  0.84375
train loss:  0.3371852934360504
train gradient:  0.16598867134809947
iteration : 11289
train acc:  0.828125
train loss:  0.42783474922180176
train gradient:  0.2547732127192434
iteration : 11290
train acc:  0.8203125
train loss:  0.38942211866378784
train gradient:  0.22288445860405137
iteration : 11291
train acc:  0.875
train loss:  0.30739670991897583
train gradient:  0.13493607333163765
iteration : 11292
train acc:  0.875
train loss:  0.30638575553894043
train gradient:  0.1425278074778196
iteration : 11293
train acc:  0.8984375
train loss:  0.261494517326355
train gradient:  0.13403103740996292
iteration : 11294
train acc:  0.8203125
train loss:  0.3629343509674072
train gradient:  0.15797710207190457
iteration : 11295
train acc:  0.8125
train loss:  0.35502031445503235
train gradient:  0.2953968498654538
iteration : 11296
train acc:  0.859375
train loss:  0.3533363342285156
train gradient:  0.14338766323404917
iteration : 11297
train acc:  0.90625
train loss:  0.2549418807029724
train gradient:  0.12265758567331742
iteration : 11298
train acc:  0.84375
train loss:  0.42334234714508057
train gradient:  0.27268901333122775
iteration : 11299
train acc:  0.8359375
train loss:  0.2971488833427429
train gradient:  0.12603167074235422
iteration : 11300
train acc:  0.8828125
train loss:  0.2677820324897766
train gradient:  0.14151438993256676
iteration : 11301
train acc:  0.90625
train loss:  0.3017747104167938
train gradient:  0.09598798342548356
iteration : 11302
train acc:  0.8515625
train loss:  0.2799381613731384
train gradient:  0.13907783856450573
iteration : 11303
train acc:  0.8515625
train loss:  0.2933800220489502
train gradient:  0.12238365086586384
iteration : 11304
train acc:  0.84375
train loss:  0.3463856875896454
train gradient:  0.17610521548696886
iteration : 11305
train acc:  0.8515625
train loss:  0.33612674474716187
train gradient:  0.17112252329188
iteration : 11306
train acc:  0.8203125
train loss:  0.3873125910758972
train gradient:  0.302143245366109
iteration : 11307
train acc:  0.8515625
train loss:  0.30377891659736633
train gradient:  0.13106570880882726
iteration : 11308
train acc:  0.8046875
train loss:  0.3726658821105957
train gradient:  0.20395553137981215
iteration : 11309
train acc:  0.8515625
train loss:  0.36912888288497925
train gradient:  0.19614006658697597
iteration : 11310
train acc:  0.859375
train loss:  0.36526599526405334
train gradient:  0.21796862119631785
iteration : 11311
train acc:  0.8671875
train loss:  0.2869357764720917
train gradient:  0.12286358586457745
iteration : 11312
train acc:  0.859375
train loss:  0.31882786750793457
train gradient:  0.11552213600719835
iteration : 11313
train acc:  0.8671875
train loss:  0.2906736731529236
train gradient:  0.10159944744769417
iteration : 11314
train acc:  0.8046875
train loss:  0.3665095567703247
train gradient:  0.1718935933943308
iteration : 11315
train acc:  0.8671875
train loss:  0.3351312279701233
train gradient:  0.17320521975147027
iteration : 11316
train acc:  0.8828125
train loss:  0.3109275996685028
train gradient:  0.16089489490316883
iteration : 11317
train acc:  0.8515625
train loss:  0.3684094250202179
train gradient:  0.17628393026758649
iteration : 11318
train acc:  0.8515625
train loss:  0.36561888456344604
train gradient:  0.25245481556300464
iteration : 11319
train acc:  0.875
train loss:  0.2708736062049866
train gradient:  0.10424409966106393
iteration : 11320
train acc:  0.859375
train loss:  0.35052645206451416
train gradient:  0.14443477149287381
iteration : 11321
train acc:  0.796875
train loss:  0.39057230949401855
train gradient:  0.21285205351324538
iteration : 11322
train acc:  0.90625
train loss:  0.2927507758140564
train gradient:  0.1362228929271647
iteration : 11323
train acc:  0.8203125
train loss:  0.3697766661643982
train gradient:  0.22635888906039614
iteration : 11324
train acc:  0.859375
train loss:  0.3082142472267151
train gradient:  0.11881344005931745
iteration : 11325
train acc:  0.84375
train loss:  0.31397518515586853
train gradient:  0.110871577459936
iteration : 11326
train acc:  0.8359375
train loss:  0.3857942223548889
train gradient:  0.22304334458478256
iteration : 11327
train acc:  0.859375
train loss:  0.36329543590545654
train gradient:  0.14554428695772895
iteration : 11328
train acc:  0.859375
train loss:  0.2857065200805664
train gradient:  0.11708671509964959
iteration : 11329
train acc:  0.84375
train loss:  0.31570175290107727
train gradient:  0.12895782813065
iteration : 11330
train acc:  0.8671875
train loss:  0.2493799328804016
train gradient:  0.07784273289973132
iteration : 11331
train acc:  0.859375
train loss:  0.3425911068916321
train gradient:  0.12447910729003653
iteration : 11332
train acc:  0.8125
train loss:  0.3835383653640747
train gradient:  0.19905406391379962
iteration : 11333
train acc:  0.8125
train loss:  0.31986352801322937
train gradient:  0.11305173078213669
iteration : 11334
train acc:  0.828125
train loss:  0.34721267223358154
train gradient:  0.17164007966376205
iteration : 11335
train acc:  0.8125
train loss:  0.38233157992362976
train gradient:  0.21227150203575068
iteration : 11336
train acc:  0.8359375
train loss:  0.38932621479034424
train gradient:  0.24225543566939572
iteration : 11337
train acc:  0.890625
train loss:  0.2757907509803772
train gradient:  0.20781468768505895
iteration : 11338
train acc:  0.8671875
train loss:  0.29741621017456055
train gradient:  0.23599299050725725
iteration : 11339
train acc:  0.859375
train loss:  0.3171088993549347
train gradient:  0.1878102444826749
iteration : 11340
train acc:  0.828125
train loss:  0.3922085165977478
train gradient:  0.21027606587340877
iteration : 11341
train acc:  0.90625
train loss:  0.24123840034008026
train gradient:  0.11407288525586609
iteration : 11342
train acc:  0.875
train loss:  0.2624044418334961
train gradient:  0.07004868929393511
iteration : 11343
train acc:  0.8671875
train loss:  0.31691136956214905
train gradient:  0.13015679420697282
iteration : 11344
train acc:  0.875
train loss:  0.35821211338043213
train gradient:  0.11959515245012665
iteration : 11345
train acc:  0.828125
train loss:  0.30535784363746643
train gradient:  0.12698681123038172
iteration : 11346
train acc:  0.8046875
train loss:  0.36609533429145813
train gradient:  0.22466695701833528
iteration : 11347
train acc:  0.859375
train loss:  0.34680500626564026
train gradient:  0.22687462630500704
iteration : 11348
train acc:  0.8359375
train loss:  0.3641778528690338
train gradient:  0.19527284965730501
iteration : 11349
train acc:  0.8359375
train loss:  0.3922441601753235
train gradient:  0.21842400296815742
iteration : 11350
train acc:  0.90625
train loss:  0.23971682786941528
train gradient:  0.0895481941390774
iteration : 11351
train acc:  0.859375
train loss:  0.3340575397014618
train gradient:  0.1750273545001794
iteration : 11352
train acc:  0.84375
train loss:  0.31862059235572815
train gradient:  0.13135236165298317
iteration : 11353
train acc:  0.8671875
train loss:  0.28658026456832886
train gradient:  0.12123518099045132
iteration : 11354
train acc:  0.8359375
train loss:  0.36796045303344727
train gradient:  0.29916856787650736
iteration : 11355
train acc:  0.859375
train loss:  0.37816035747528076
train gradient:  0.2271842894227974
iteration : 11356
train acc:  0.875
train loss:  0.3049553632736206
train gradient:  0.18892850413725418
iteration : 11357
train acc:  0.8515625
train loss:  0.36834681034088135
train gradient:  0.15417547564987988
iteration : 11358
train acc:  0.875
train loss:  0.2844511866569519
train gradient:  0.12331416476648903
iteration : 11359
train acc:  0.8359375
train loss:  0.37835726141929626
train gradient:  0.2578310679859166
iteration : 11360
train acc:  0.8671875
train loss:  0.32167676091194153
train gradient:  0.1682599761193941
iteration : 11361
train acc:  0.8125
train loss:  0.392442911863327
train gradient:  0.19293866334410945
iteration : 11362
train acc:  0.8515625
train loss:  0.3929833471775055
train gradient:  0.21102527940465166
iteration : 11363
train acc:  0.8515625
train loss:  0.3496383726596832
train gradient:  0.14204488113408348
iteration : 11364
train acc:  0.875
train loss:  0.2526804208755493
train gradient:  0.08495139347610754
iteration : 11365
train acc:  0.875
train loss:  0.2991411089897156
train gradient:  0.14981663830188915
iteration : 11366
train acc:  0.8984375
train loss:  0.31246620416641235
train gradient:  0.15928118369247402
iteration : 11367
train acc:  0.8828125
train loss:  0.29847830533981323
train gradient:  0.13619875426270361
iteration : 11368
train acc:  0.8359375
train loss:  0.3383723795413971
train gradient:  0.1486828723657095
iteration : 11369
train acc:  0.859375
train loss:  0.2850319743156433
train gradient:  0.12752677180464345
iteration : 11370
train acc:  0.8515625
train loss:  0.37838214635849
train gradient:  0.18682832900214452
iteration : 11371
train acc:  0.8125
train loss:  0.3707748055458069
train gradient:  0.1896135552132569
iteration : 11372
train acc:  0.75
train loss:  0.4490109086036682
train gradient:  0.2648546271223937
iteration : 11373
train acc:  0.828125
train loss:  0.40110692381858826
train gradient:  0.27032558545186386
iteration : 11374
train acc:  0.8828125
train loss:  0.28971725702285767
train gradient:  0.1157923729989251
iteration : 11375
train acc:  0.8359375
train loss:  0.36801329255104065
train gradient:  0.17640541080389202
iteration : 11376
train acc:  0.890625
train loss:  0.3210563063621521
train gradient:  0.16769124796822527
iteration : 11377
train acc:  0.8671875
train loss:  0.30011388659477234
train gradient:  0.11402060985697793
iteration : 11378
train acc:  0.8828125
train loss:  0.25121790170669556
train gradient:  0.08074460125021515
iteration : 11379
train acc:  0.828125
train loss:  0.3957086503505707
train gradient:  0.2019918103570949
iteration : 11380
train acc:  0.859375
train loss:  0.36741721630096436
train gradient:  0.1489298805519619
iteration : 11381
train acc:  0.84375
train loss:  0.3468530774116516
train gradient:  0.12324427849688516
iteration : 11382
train acc:  0.8359375
train loss:  0.3200189471244812
train gradient:  0.10200337503070181
iteration : 11383
train acc:  0.8671875
train loss:  0.2817479968070984
train gradient:  0.0743983378731618
iteration : 11384
train acc:  0.859375
train loss:  0.325562983751297
train gradient:  0.13121422385439022
iteration : 11385
train acc:  0.8515625
train loss:  0.40874728560447693
train gradient:  0.16796318760468845
iteration : 11386
train acc:  0.8359375
train loss:  0.35950058698654175
train gradient:  0.16432907490770288
iteration : 11387
train acc:  0.8515625
train loss:  0.31172293424606323
train gradient:  0.11069241076052583
iteration : 11388
train acc:  0.84375
train loss:  0.3113676607608795
train gradient:  0.10639568151492801
iteration : 11389
train acc:  0.828125
train loss:  0.3369964063167572
train gradient:  0.1491788158701664
iteration : 11390
train acc:  0.8203125
train loss:  0.34733688831329346
train gradient:  0.1570942714987796
iteration : 11391
train acc:  0.859375
train loss:  0.3384820520877838
train gradient:  0.1449642180702141
iteration : 11392
train acc:  0.890625
train loss:  0.24547696113586426
train gradient:  0.10124831284111041
iteration : 11393
train acc:  0.8359375
train loss:  0.37382254004478455
train gradient:  0.1891942280120687
iteration : 11394
train acc:  0.9140625
train loss:  0.26048922538757324
train gradient:  0.1192185242075209
iteration : 11395
train acc:  0.890625
train loss:  0.2564411163330078
train gradient:  0.08043287892395466
iteration : 11396
train acc:  0.8515625
train loss:  0.30496853590011597
train gradient:  0.11129956590937208
iteration : 11397
train acc:  0.890625
train loss:  0.2983347773551941
train gradient:  0.14771495562937548
iteration : 11398
train acc:  0.8515625
train loss:  0.3564688563346863
train gradient:  0.15662285075653248
iteration : 11399
train acc:  0.8359375
train loss:  0.33032721281051636
train gradient:  0.2304682931554835
iteration : 11400
train acc:  0.890625
train loss:  0.27390357851982117
train gradient:  0.09010866022320524
iteration : 11401
train acc:  0.8203125
train loss:  0.35385429859161377
train gradient:  0.16728645231116643
iteration : 11402
train acc:  0.890625
train loss:  0.25201699137687683
train gradient:  0.18094995838676525
iteration : 11403
train acc:  0.8828125
train loss:  0.3160974979400635
train gradient:  0.12192681148802535
iteration : 11404
train acc:  0.859375
train loss:  0.30240577459335327
train gradient:  0.14164191134665763
iteration : 11405
train acc:  0.8828125
train loss:  0.2591747045516968
train gradient:  0.11036020355469843
iteration : 11406
train acc:  0.8828125
train loss:  0.2856718897819519
train gradient:  0.13455789666623238
iteration : 11407
train acc:  0.875
train loss:  0.2597266435623169
train gradient:  0.13307560564998827
iteration : 11408
train acc:  0.84375
train loss:  0.31441932916641235
train gradient:  0.16261476291003563
iteration : 11409
train acc:  0.8671875
train loss:  0.35310888290405273
train gradient:  0.2089619668747612
iteration : 11410
train acc:  0.8671875
train loss:  0.2771053612232208
train gradient:  0.13083593773400412
iteration : 11411
train acc:  0.890625
train loss:  0.27463117241859436
train gradient:  0.11472515860585998
iteration : 11412
train acc:  0.8515625
train loss:  0.3207318186759949
train gradient:  0.13057916703673916
iteration : 11413
train acc:  0.875
train loss:  0.2705842852592468
train gradient:  0.1246488411848538
iteration : 11414
train acc:  0.8359375
train loss:  0.334271639585495
train gradient:  0.12453660917731338
iteration : 11415
train acc:  0.8046875
train loss:  0.3789795935153961
train gradient:  0.19721913147706505
iteration : 11416
train acc:  0.828125
train loss:  0.33020448684692383
train gradient:  0.22658767576464656
iteration : 11417
train acc:  0.859375
train loss:  0.41174834966659546
train gradient:  0.18013230015452034
iteration : 11418
train acc:  0.8671875
train loss:  0.3302946090698242
train gradient:  0.12324990101766044
iteration : 11419
train acc:  0.8828125
train loss:  0.32159337401390076
train gradient:  0.12546963594434113
iteration : 11420
train acc:  0.859375
train loss:  0.273327112197876
train gradient:  0.16156332553121205
iteration : 11421
train acc:  0.8671875
train loss:  0.321094274520874
train gradient:  0.15237733420164135
iteration : 11422
train acc:  0.84375
train loss:  0.3261658251285553
train gradient:  0.2726698662828091
iteration : 11423
train acc:  0.84375
train loss:  0.3352082371711731
train gradient:  0.15003503341743268
iteration : 11424
train acc:  0.84375
train loss:  0.36076563596725464
train gradient:  0.3361636079552407
iteration : 11425
train acc:  0.8828125
train loss:  0.24632304906845093
train gradient:  0.09413017356065967
iteration : 11426
train acc:  0.8828125
train loss:  0.30191749334335327
train gradient:  0.13798173423056542
iteration : 11427
train acc:  0.8671875
train loss:  0.32205748558044434
train gradient:  0.18011362518576743
iteration : 11428
train acc:  0.8515625
train loss:  0.3439929485321045
train gradient:  0.14938926985164988
iteration : 11429
train acc:  0.8984375
train loss:  0.3115957975387573
train gradient:  0.1634049948056056
iteration : 11430
train acc:  0.859375
train loss:  0.2746231257915497
train gradient:  0.09647949821747714
iteration : 11431
train acc:  0.8671875
train loss:  0.29657310247421265
train gradient:  0.11127005394386404
iteration : 11432
train acc:  0.828125
train loss:  0.36875349283218384
train gradient:  0.20585653254172975
iteration : 11433
train acc:  0.875
train loss:  0.2726117968559265
train gradient:  0.14622227642458197
iteration : 11434
train acc:  0.859375
train loss:  0.30993199348449707
train gradient:  0.11719350463839039
iteration : 11435
train acc:  0.8828125
train loss:  0.29954105615615845
train gradient:  0.18207435623207904
iteration : 11436
train acc:  0.8125
train loss:  0.4169125258922577
train gradient:  0.23512190647741021
iteration : 11437
train acc:  0.796875
train loss:  0.3694312572479248
train gradient:  0.19492203409486675
iteration : 11438
train acc:  0.8828125
train loss:  0.279738187789917
train gradient:  0.10186665591862071
iteration : 11439
train acc:  0.875
train loss:  0.35388991236686707
train gradient:  0.16414732686821734
iteration : 11440
train acc:  0.84375
train loss:  0.3146573305130005
train gradient:  0.1375594335580974
iteration : 11441
train acc:  0.8359375
train loss:  0.38328829407691956
train gradient:  0.20675109446632964
iteration : 11442
train acc:  0.8515625
train loss:  0.3232728838920593
train gradient:  0.1697271496293566
iteration : 11443
train acc:  0.90625
train loss:  0.24615749716758728
train gradient:  0.10917345225978849
iteration : 11444
train acc:  0.8984375
train loss:  0.21815568208694458
train gradient:  0.06451015952791206
iteration : 11445
train acc:  0.8125
train loss:  0.416218638420105
train gradient:  0.2168677734432117
iteration : 11446
train acc:  0.890625
train loss:  0.2647625207901001
train gradient:  0.1611213977420643
iteration : 11447
train acc:  0.875
train loss:  0.3051891326904297
train gradient:  0.16372177059056284
iteration : 11448
train acc:  0.8359375
train loss:  0.38078728318214417
train gradient:  0.297990699828464
iteration : 11449
train acc:  0.875
train loss:  0.29801493883132935
train gradient:  0.1031070781612678
iteration : 11450
train acc:  0.8203125
train loss:  0.3893998861312866
train gradient:  0.20042214036156125
iteration : 11451
train acc:  0.8671875
train loss:  0.2772716283798218
train gradient:  0.1112938992122789
iteration : 11452
train acc:  0.8828125
train loss:  0.2655895948410034
train gradient:  0.10469001469563773
iteration : 11453
train acc:  0.859375
train loss:  0.29920342564582825
train gradient:  0.17682314939437593
iteration : 11454
train acc:  0.875
train loss:  0.34486380219459534
train gradient:  0.17048268955888937
iteration : 11455
train acc:  0.8515625
train loss:  0.3265247941017151
train gradient:  0.1467429477929029
iteration : 11456
train acc:  0.8125
train loss:  0.3782210946083069
train gradient:  0.25571698946142396
iteration : 11457
train acc:  0.90625
train loss:  0.26347362995147705
train gradient:  0.12055556891481534
iteration : 11458
train acc:  0.8515625
train loss:  0.342332661151886
train gradient:  0.15569418530138854
iteration : 11459
train acc:  0.8984375
train loss:  0.3018823266029358
train gradient:  0.11584604290400698
iteration : 11460
train acc:  0.90625
train loss:  0.2480105459690094
train gradient:  0.1010513142312415
iteration : 11461
train acc:  0.84375
train loss:  0.35614335536956787
train gradient:  0.21303995753431176
iteration : 11462
train acc:  0.90625
train loss:  0.27208101749420166
train gradient:  0.1372575953072933
iteration : 11463
train acc:  0.890625
train loss:  0.2510823905467987
train gradient:  0.0796818139285696
iteration : 11464
train acc:  0.859375
train loss:  0.3099812865257263
train gradient:  0.15471144110135276
iteration : 11465
train acc:  0.828125
train loss:  0.3785584568977356
train gradient:  0.1547901887259091
iteration : 11466
train acc:  0.8203125
train loss:  0.3533296585083008
train gradient:  0.24417512780065734
iteration : 11467
train acc:  0.8203125
train loss:  0.3684401214122772
train gradient:  0.26053102971923375
iteration : 11468
train acc:  0.875
train loss:  0.2999257743358612
train gradient:  0.13349481291443482
iteration : 11469
train acc:  0.8828125
train loss:  0.28600507974624634
train gradient:  0.12380394539812523
iteration : 11470
train acc:  0.84375
train loss:  0.3825848400592804
train gradient:  0.20841513910948123
iteration : 11471
train acc:  0.8359375
train loss:  0.3967573046684265
train gradient:  0.2008273646916717
iteration : 11472
train acc:  0.90625
train loss:  0.3092228174209595
train gradient:  0.14059485955914788
iteration : 11473
train acc:  0.9140625
train loss:  0.2061809003353119
train gradient:  0.06314134287486745
iteration : 11474
train acc:  0.828125
train loss:  0.3790578246116638
train gradient:  0.23146977880815242
iteration : 11475
train acc:  0.8671875
train loss:  0.28049135208129883
train gradient:  0.12492976416938738
iteration : 11476
train acc:  0.8203125
train loss:  0.3643271028995514
train gradient:  0.19199079973620276
iteration : 11477
train acc:  0.8515625
train loss:  0.3403395414352417
train gradient:  0.16229043387688302
iteration : 11478
train acc:  0.8515625
train loss:  0.3845033347606659
train gradient:  0.18880147172339914
iteration : 11479
train acc:  0.875
train loss:  0.29508477449417114
train gradient:  0.11776319037336513
iteration : 11480
train acc:  0.8671875
train loss:  0.30758577585220337
train gradient:  0.10356794545673899
iteration : 11481
train acc:  0.8203125
train loss:  0.31914979219436646
train gradient:  0.13083137914351756
iteration : 11482
train acc:  0.859375
train loss:  0.3958306312561035
train gradient:  0.25213431328410624
iteration : 11483
train acc:  0.890625
train loss:  0.2705347239971161
train gradient:  0.12014506046749389
iteration : 11484
train acc:  0.828125
train loss:  0.3458893895149231
train gradient:  0.16506897887588812
iteration : 11485
train acc:  0.953125
train loss:  0.19819796085357666
train gradient:  0.07039295603623677
iteration : 11486
train acc:  0.875
train loss:  0.2822185158729553
train gradient:  0.18914562513293476
iteration : 11487
train acc:  0.921875
train loss:  0.2404126077890396
train gradient:  0.1288027184682894
iteration : 11488
train acc:  0.8671875
train loss:  0.3434574604034424
train gradient:  0.16244663313727653
iteration : 11489
train acc:  0.8515625
train loss:  0.2916988134384155
train gradient:  0.11753558603753686
iteration : 11490
train acc:  0.8359375
train loss:  0.3194093704223633
train gradient:  0.115514507983778
iteration : 11491
train acc:  0.8203125
train loss:  0.3558192849159241
train gradient:  0.1954306390291342
iteration : 11492
train acc:  0.8671875
train loss:  0.3008686900138855
train gradient:  0.1837880789713201
iteration : 11493
train acc:  0.8203125
train loss:  0.37198102474212646
train gradient:  0.1521187212098663
iteration : 11494
train acc:  0.875
train loss:  0.33700031042099
train gradient:  0.1476463705672733
iteration : 11495
train acc:  0.8515625
train loss:  0.3203900456428528
train gradient:  0.13781458835670815
iteration : 11496
train acc:  0.8828125
train loss:  0.2724483907222748
train gradient:  0.11126119211152032
iteration : 11497
train acc:  0.84375
train loss:  0.3006219267845154
train gradient:  0.15906677270911834
iteration : 11498
train acc:  0.9140625
train loss:  0.25273823738098145
train gradient:  0.10728319474442716
iteration : 11499
train acc:  0.84375
train loss:  0.319801926612854
train gradient:  0.18413214432900088
iteration : 11500
train acc:  0.796875
train loss:  0.4527026116847992
train gradient:  0.2886344023122016
iteration : 11501
train acc:  0.8359375
train loss:  0.38642793893814087
train gradient:  0.30686654876062525
iteration : 11502
train acc:  0.890625
train loss:  0.25000500679016113
train gradient:  0.09603050572969855
iteration : 11503
train acc:  0.875
train loss:  0.2612563669681549
train gradient:  0.10258371576814851
iteration : 11504
train acc:  0.8671875
train loss:  0.30510246753692627
train gradient:  0.11806295383637877
iteration : 11505
train acc:  0.9296875
train loss:  0.20659738779067993
train gradient:  0.06949253105456049
iteration : 11506
train acc:  0.890625
train loss:  0.3297120928764343
train gradient:  0.18093028044817494
iteration : 11507
train acc:  0.890625
train loss:  0.277511328458786
train gradient:  0.10141615927964111
iteration : 11508
train acc:  0.8984375
train loss:  0.28303471207618713
train gradient:  0.09774814529745776
iteration : 11509
train acc:  0.90625
train loss:  0.2609986662864685
train gradient:  0.13866655900723654
iteration : 11510
train acc:  0.890625
train loss:  0.3399713635444641
train gradient:  0.14224133601012623
iteration : 11511
train acc:  0.859375
train loss:  0.33083969354629517
train gradient:  0.3994914712943529
iteration : 11512
train acc:  0.8828125
train loss:  0.3038782477378845
train gradient:  0.1292655251390576
iteration : 11513
train acc:  0.8828125
train loss:  0.2834867835044861
train gradient:  0.11689859391140722
iteration : 11514
train acc:  0.8671875
train loss:  0.28598201274871826
train gradient:  0.17851310875876011
iteration : 11515
train acc:  0.796875
train loss:  0.4201846718788147
train gradient:  0.22650627373365242
iteration : 11516
train acc:  0.90625
train loss:  0.26687803864479065
train gradient:  0.11061059846914931
iteration : 11517
train acc:  0.8125
train loss:  0.4573427438735962
train gradient:  0.22434246808298902
iteration : 11518
train acc:  0.8515625
train loss:  0.3681527376174927
train gradient:  0.17779932034042406
iteration : 11519
train acc:  0.8515625
train loss:  0.35541296005249023
train gradient:  0.1893260800080968
iteration : 11520
train acc:  0.890625
train loss:  0.2699131369590759
train gradient:  0.08647187546421496
iteration : 11521
train acc:  0.859375
train loss:  0.2840800881385803
train gradient:  0.16190225847947193
iteration : 11522
train acc:  0.84375
train loss:  0.3651179075241089
train gradient:  0.2184708965203243
iteration : 11523
train acc:  0.8671875
train loss:  0.3049675226211548
train gradient:  0.1623555109988128
iteration : 11524
train acc:  0.8828125
train loss:  0.2303292155265808
train gradient:  0.10679026896885549
iteration : 11525
train acc:  0.890625
train loss:  0.27809572219848633
train gradient:  0.16448165018689864
iteration : 11526
train acc:  0.859375
train loss:  0.31506916880607605
train gradient:  0.19935032733950464
iteration : 11527
train acc:  0.90625
train loss:  0.29012903571128845
train gradient:  0.10612467317706331
iteration : 11528
train acc:  0.8671875
train loss:  0.31208717823028564
train gradient:  0.1374957160303621
iteration : 11529
train acc:  0.90625
train loss:  0.26348188519477844
train gradient:  0.10053260456983815
iteration : 11530
train acc:  0.828125
train loss:  0.3870866298675537
train gradient:  0.2405979703202073
iteration : 11531
train acc:  0.9375
train loss:  0.23006504774093628
train gradient:  0.08398347392945028
iteration : 11532
train acc:  0.8671875
train loss:  0.32294946908950806
train gradient:  0.1300409329639467
iteration : 11533
train acc:  0.8125
train loss:  0.3645562529563904
train gradient:  0.20694009387473286
iteration : 11534
train acc:  0.8828125
train loss:  0.31731441617012024
train gradient:  0.17141941217227205
iteration : 11535
train acc:  0.875
train loss:  0.37898027896881104
train gradient:  0.20676258118022028
iteration : 11536
train acc:  0.8828125
train loss:  0.2765294313430786
train gradient:  0.1303152356538707
iteration : 11537
train acc:  0.859375
train loss:  0.3514740765094757
train gradient:  0.15358629253509704
iteration : 11538
train acc:  0.8984375
train loss:  0.24479766190052032
train gradient:  0.1507425065288475
iteration : 11539
train acc:  0.8359375
train loss:  0.331094890832901
train gradient:  0.14546319370762156
iteration : 11540
train acc:  0.84375
train loss:  0.3426479697227478
train gradient:  0.17257988725748732
iteration : 11541
train acc:  0.8828125
train loss:  0.2861197292804718
train gradient:  0.13265334811217652
iteration : 11542
train acc:  0.890625
train loss:  0.30414652824401855
train gradient:  0.16340311575700148
iteration : 11543
train acc:  0.8359375
train loss:  0.37794581055641174
train gradient:  0.16938303545886998
iteration : 11544
train acc:  0.828125
train loss:  0.3460680842399597
train gradient:  0.17381366683224642
iteration : 11545
train acc:  0.859375
train loss:  0.30160731077194214
train gradient:  0.13721354305821853
iteration : 11546
train acc:  0.84375
train loss:  0.3308148682117462
train gradient:  0.1940950181212297
iteration : 11547
train acc:  0.859375
train loss:  0.3364412188529968
train gradient:  0.14085804646266692
iteration : 11548
train acc:  0.875
train loss:  0.2917642593383789
train gradient:  0.14085155807496405
iteration : 11549
train acc:  0.8359375
train loss:  0.35584789514541626
train gradient:  0.19187003965656918
iteration : 11550
train acc:  0.828125
train loss:  0.38669100403785706
train gradient:  0.164860561374864
iteration : 11551
train acc:  0.84375
train loss:  0.3441268503665924
train gradient:  0.22428820670432478
iteration : 11552
train acc:  0.8203125
train loss:  0.4278101921081543
train gradient:  0.2236622899258912
iteration : 11553
train acc:  0.7734375
train loss:  0.424998015165329
train gradient:  0.3293797170059478
iteration : 11554
train acc:  0.828125
train loss:  0.36017870903015137
train gradient:  0.20438748221584097
iteration : 11555
train acc:  0.90625
train loss:  0.29770296812057495
train gradient:  0.1444548822308011
iteration : 11556
train acc:  0.9140625
train loss:  0.26901012659072876
train gradient:  0.08277215464328161
iteration : 11557
train acc:  0.8671875
train loss:  0.2710492014884949
train gradient:  0.1472999893444658
iteration : 11558
train acc:  0.8203125
train loss:  0.37655067443847656
train gradient:  0.17118821244407612
iteration : 11559
train acc:  0.875
train loss:  0.3140614926815033
train gradient:  0.14839501662983437
iteration : 11560
train acc:  0.875
train loss:  0.32395869493484497
train gradient:  0.13037363330256124
iteration : 11561
train acc:  0.8359375
train loss:  0.4175819158554077
train gradient:  0.24406830282357872
iteration : 11562
train acc:  0.8984375
train loss:  0.2735746204853058
train gradient:  0.10488585538853226
iteration : 11563
train acc:  0.875
train loss:  0.29387789964675903
train gradient:  0.1087179751804815
iteration : 11564
train acc:  0.8828125
train loss:  0.34829196333885193
train gradient:  0.126847523247764
iteration : 11565
train acc:  0.84375
train loss:  0.3753301501274109
train gradient:  0.24562127037074394
iteration : 11566
train acc:  0.890625
train loss:  0.2719394564628601
train gradient:  0.12918836177601029
iteration : 11567
train acc:  0.8515625
train loss:  0.33833909034729004
train gradient:  0.1551018522833713
iteration : 11568
train acc:  0.84375
train loss:  0.38210558891296387
train gradient:  0.22192024299936658
iteration : 11569
train acc:  0.8671875
train loss:  0.28766706585884094
train gradient:  0.13069649937674793
iteration : 11570
train acc:  0.859375
train loss:  0.30806779861450195
train gradient:  0.25334195942982196
iteration : 11571
train acc:  0.796875
train loss:  0.42004090547561646
train gradient:  0.2919241865285385
iteration : 11572
train acc:  0.8046875
train loss:  0.3833962678909302
train gradient:  0.20657723768366426
iteration : 11573
train acc:  0.8515625
train loss:  0.3159492313861847
train gradient:  0.1375112280583245
iteration : 11574
train acc:  0.84375
train loss:  0.2996059060096741
train gradient:  0.11759177870962167
iteration : 11575
train acc:  0.8359375
train loss:  0.3424069881439209
train gradient:  0.1674849164963092
iteration : 11576
train acc:  0.859375
train loss:  0.3318611979484558
train gradient:  0.15892853783238453
iteration : 11577
train acc:  0.8671875
train loss:  0.3068917989730835
train gradient:  0.1886902921875443
iteration : 11578
train acc:  0.828125
train loss:  0.3437230587005615
train gradient:  0.18434309138416527
iteration : 11579
train acc:  0.8359375
train loss:  0.3546597957611084
train gradient:  0.14746102103534986
iteration : 11580
train acc:  0.84375
train loss:  0.35658562183380127
train gradient:  0.17792892345140454
iteration : 11581
train acc:  0.8046875
train loss:  0.36126509308815
train gradient:  0.1412973624036273
iteration : 11582
train acc:  0.875
train loss:  0.2662672996520996
train gradient:  0.11778461042632118
iteration : 11583
train acc:  0.8828125
train loss:  0.2528377175331116
train gradient:  0.12776812467133944
iteration : 11584
train acc:  0.8828125
train loss:  0.2632024586200714
train gradient:  0.1336230701473276
iteration : 11585
train acc:  0.84375
train loss:  0.343880295753479
train gradient:  0.17057227351851062
iteration : 11586
train acc:  0.796875
train loss:  0.43628808856010437
train gradient:  0.18178350622884845
iteration : 11587
train acc:  0.828125
train loss:  0.39540648460388184
train gradient:  0.19073301476829624
iteration : 11588
train acc:  0.84375
train loss:  0.3181689977645874
train gradient:  0.12683013091931028
iteration : 11589
train acc:  0.8984375
train loss:  0.29941892623901367
train gradient:  0.11153726913617586
iteration : 11590
train acc:  0.8671875
train loss:  0.346622109413147
train gradient:  0.1406376851057014
iteration : 11591
train acc:  0.8203125
train loss:  0.3471497893333435
train gradient:  0.1454914717993515
iteration : 11592
train acc:  0.796875
train loss:  0.45684614777565
train gradient:  0.21944728981602496
iteration : 11593
train acc:  0.8046875
train loss:  0.4396283030509949
train gradient:  0.2198067822043357
iteration : 11594
train acc:  0.890625
train loss:  0.26849365234375
train gradient:  0.09886855818056867
iteration : 11595
train acc:  0.859375
train loss:  0.35803258419036865
train gradient:  0.22253917367525067
iteration : 11596
train acc:  0.875
train loss:  0.3126237392425537
train gradient:  0.20072940303965772
iteration : 11597
train acc:  0.875
train loss:  0.2791396975517273
train gradient:  0.09030648772489192
iteration : 11598
train acc:  0.8359375
train loss:  0.3356570899486542
train gradient:  0.13219043707613884
iteration : 11599
train acc:  0.875
train loss:  0.313587486743927
train gradient:  0.15171008041942924
iteration : 11600
train acc:  0.8359375
train loss:  0.41547513008117676
train gradient:  0.19556200448676495
iteration : 11601
train acc:  0.828125
train loss:  0.36990395188331604
train gradient:  0.20296183628539685
iteration : 11602
train acc:  0.8671875
train loss:  0.34844937920570374
train gradient:  0.15789998387044957
iteration : 11603
train acc:  0.921875
train loss:  0.23266206681728363
train gradient:  0.12103188737281796
iteration : 11604
train acc:  0.890625
train loss:  0.28045663237571716
train gradient:  0.08511872317864007
iteration : 11605
train acc:  0.875
train loss:  0.3503122627735138
train gradient:  0.1876113541116765
iteration : 11606
train acc:  0.8828125
train loss:  0.3086928129196167
train gradient:  0.11386010292623142
iteration : 11607
train acc:  0.8203125
train loss:  0.3153802156448364
train gradient:  0.16762154090627585
iteration : 11608
train acc:  0.8515625
train loss:  0.32211852073669434
train gradient:  0.14677478600964625
iteration : 11609
train acc:  0.8671875
train loss:  0.3120068311691284
train gradient:  0.12097604884016654
iteration : 11610
train acc:  0.859375
train loss:  0.34101101756095886
train gradient:  0.1664031367795748
iteration : 11611
train acc:  0.8984375
train loss:  0.2402365803718567
train gradient:  0.08324259621857186
iteration : 11612
train acc:  0.90625
train loss:  0.2480703592300415
train gradient:  0.10755924731157296
iteration : 11613
train acc:  0.828125
train loss:  0.3477863669395447
train gradient:  0.16660157353762617
iteration : 11614
train acc:  0.8515625
train loss:  0.3195748031139374
train gradient:  0.13752297961527235
iteration : 11615
train acc:  0.8515625
train loss:  0.3409356474876404
train gradient:  0.16873378562767494
iteration : 11616
train acc:  0.8203125
train loss:  0.36318132281303406
train gradient:  0.13418780590380108
iteration : 11617
train acc:  0.8671875
train loss:  0.3150143325328827
train gradient:  0.12383051741175213
iteration : 11618
train acc:  0.8515625
train loss:  0.31272533535957336
train gradient:  0.14646354277274753
iteration : 11619
train acc:  0.8828125
train loss:  0.2663249969482422
train gradient:  0.12912324787797702
iteration : 11620
train acc:  0.8515625
train loss:  0.36452174186706543
train gradient:  0.20822054263789508
iteration : 11621
train acc:  0.8515625
train loss:  0.3367879390716553
train gradient:  0.12967439921720514
iteration : 11622
train acc:  0.796875
train loss:  0.463407963514328
train gradient:  0.2508645897720097
iteration : 11623
train acc:  0.890625
train loss:  0.34468191862106323
train gradient:  0.1348858388144173
iteration : 11624
train acc:  0.8984375
train loss:  0.3134140968322754
train gradient:  0.21563444372545976
iteration : 11625
train acc:  0.890625
train loss:  0.3163343667984009
train gradient:  0.17029344631396193
iteration : 11626
train acc:  0.859375
train loss:  0.3024023175239563
train gradient:  0.1308286999611483
iteration : 11627
train acc:  0.828125
train loss:  0.34579670429229736
train gradient:  0.14024994743740785
iteration : 11628
train acc:  0.90625
train loss:  0.24344989657402039
train gradient:  0.11754912742074028
iteration : 11629
train acc:  0.8359375
train loss:  0.3010362386703491
train gradient:  0.13611734256992813
iteration : 11630
train acc:  0.84375
train loss:  0.32776522636413574
train gradient:  0.21200693184973418
iteration : 11631
train acc:  0.8671875
train loss:  0.3989373743534088
train gradient:  0.1870294099069878
iteration : 11632
train acc:  0.890625
train loss:  0.2947666049003601
train gradient:  0.13353807261765463
iteration : 11633
train acc:  0.8671875
train loss:  0.324424147605896
train gradient:  0.09333257832729885
iteration : 11634
train acc:  0.890625
train loss:  0.3435017466545105
train gradient:  0.15476875620314814
iteration : 11635
train acc:  0.8984375
train loss:  0.2823319435119629
train gradient:  0.13801632877307432
iteration : 11636
train acc:  0.8125
train loss:  0.37793803215026855
train gradient:  0.18416306806351365
iteration : 11637
train acc:  0.8515625
train loss:  0.3478713631629944
train gradient:  0.14055199662803
iteration : 11638
train acc:  0.828125
train loss:  0.3470170497894287
train gradient:  0.3172511121511648
iteration : 11639
train acc:  0.8671875
train loss:  0.331964910030365
train gradient:  0.1275423164752328
iteration : 11640
train acc:  0.8828125
train loss:  0.24856741726398468
train gradient:  0.18301265654408766
iteration : 11641
train acc:  0.8828125
train loss:  0.3057834804058075
train gradient:  0.12712070031627168
iteration : 11642
train acc:  0.90625
train loss:  0.28984495997428894
train gradient:  0.10368549769579993
iteration : 11643
train acc:  0.8671875
train loss:  0.32550710439682007
train gradient:  0.1899884552881263
iteration : 11644
train acc:  0.8671875
train loss:  0.34800583124160767
train gradient:  0.14826534476557016
iteration : 11645
train acc:  0.84375
train loss:  0.4154481887817383
train gradient:  0.2722499287205251
iteration : 11646
train acc:  0.8515625
train loss:  0.31354978680610657
train gradient:  0.133459083514754
iteration : 11647
train acc:  0.8359375
train loss:  0.35399043560028076
train gradient:  0.2179710235651014
iteration : 11648
train acc:  0.8359375
train loss:  0.3693690001964569
train gradient:  0.22688081838947893
iteration : 11649
train acc:  0.8671875
train loss:  0.31132733821868896
train gradient:  0.1206808466493043
iteration : 11650
train acc:  0.875
train loss:  0.3238674998283386
train gradient:  0.1479102281514756
iteration : 11651
train acc:  0.875
train loss:  0.2954120635986328
train gradient:  0.11132427485054457
iteration : 11652
train acc:  0.90625
train loss:  0.2789131999015808
train gradient:  0.17267785899657642
iteration : 11653
train acc:  0.859375
train loss:  0.34450897574424744
train gradient:  0.13303297121255525
iteration : 11654
train acc:  0.7890625
train loss:  0.37644028663635254
train gradient:  0.21775997664944308
iteration : 11655
train acc:  0.8125
train loss:  0.42130154371261597
train gradient:  0.21690759078759375
iteration : 11656
train acc:  0.90625
train loss:  0.266130656003952
train gradient:  0.10611516014323161
iteration : 11657
train acc:  0.8828125
train loss:  0.2747766375541687
train gradient:  0.12706120859972397
iteration : 11658
train acc:  0.84375
train loss:  0.3503282368183136
train gradient:  0.147017772994692
iteration : 11659
train acc:  0.875
train loss:  0.3055976927280426
train gradient:  0.17894696313891906
iteration : 11660
train acc:  0.8359375
train loss:  0.40716874599456787
train gradient:  0.1857014983286993
iteration : 11661
train acc:  0.859375
train loss:  0.3389689028263092
train gradient:  0.1364025385015879
iteration : 11662
train acc:  0.8515625
train loss:  0.31403446197509766
train gradient:  0.18749892835284515
iteration : 11663
train acc:  0.875
train loss:  0.3245820999145508
train gradient:  0.17051304727862987
iteration : 11664
train acc:  0.859375
train loss:  0.33254316449165344
train gradient:  0.21688879259998967
iteration : 11665
train acc:  0.8515625
train loss:  0.3092863857746124
train gradient:  0.14752884538468042
iteration : 11666
train acc:  0.9140625
train loss:  0.25710344314575195
train gradient:  0.09057550352735629
iteration : 11667
train acc:  0.8359375
train loss:  0.41838663816452026
train gradient:  0.26941596171150367
iteration : 11668
train acc:  0.859375
train loss:  0.3469551205635071
train gradient:  0.16529980842907233
iteration : 11669
train acc:  0.8984375
train loss:  0.2694518566131592
train gradient:  0.09302801692194762
iteration : 11670
train acc:  0.8671875
train loss:  0.2822176218032837
train gradient:  0.1203803511745939
iteration : 11671
train acc:  0.8203125
train loss:  0.3750622272491455
train gradient:  0.16548724222365285
iteration : 11672
train acc:  0.890625
train loss:  0.27185213565826416
train gradient:  0.13474622121734722
iteration : 11673
train acc:  0.890625
train loss:  0.25320446491241455
train gradient:  0.08465822353032354
iteration : 11674
train acc:  0.859375
train loss:  0.3418273627758026
train gradient:  0.19473254790906258
iteration : 11675
train acc:  0.8828125
train loss:  0.29137128591537476
train gradient:  0.14621244414555035
iteration : 11676
train acc:  0.8515625
train loss:  0.3869502544403076
train gradient:  0.19898594649554568
iteration : 11677
train acc:  0.8984375
train loss:  0.2597567141056061
train gradient:  0.11235534057130003
iteration : 11678
train acc:  0.8359375
train loss:  0.3693290650844574
train gradient:  0.16506778413300519
iteration : 11679
train acc:  0.7890625
train loss:  0.4355781674385071
train gradient:  0.2580504684051838
iteration : 11680
train acc:  0.8203125
train loss:  0.35912656784057617
train gradient:  0.21690269454593236
iteration : 11681
train acc:  0.8671875
train loss:  0.28935930132865906
train gradient:  0.16210226518288362
iteration : 11682
train acc:  0.8828125
train loss:  0.3299602270126343
train gradient:  0.2347817947469248
iteration : 11683
train acc:  0.8359375
train loss:  0.30967971682548523
train gradient:  0.13634923420828576
iteration : 11684
train acc:  0.8359375
train loss:  0.3187302350997925
train gradient:  0.2060743655188212
iteration : 11685
train acc:  0.84375
train loss:  0.3254031836986542
train gradient:  0.12202445900780076
iteration : 11686
train acc:  0.875
train loss:  0.2847971022129059
train gradient:  0.12909476258025565
iteration : 11687
train acc:  0.8125
train loss:  0.3364158868789673
train gradient:  0.1638312423942793
iteration : 11688
train acc:  0.8828125
train loss:  0.23760177195072174
train gradient:  0.14388334703433392
iteration : 11689
train acc:  0.8671875
train loss:  0.3206266164779663
train gradient:  0.2163041784110317
iteration : 11690
train acc:  0.8671875
train loss:  0.2940060496330261
train gradient:  0.12766573930340638
iteration : 11691
train acc:  0.890625
train loss:  0.28770506381988525
train gradient:  0.0956037429690367
iteration : 11692
train acc:  0.8828125
train loss:  0.29559022188186646
train gradient:  0.17972321541707936
iteration : 11693
train acc:  0.90625
train loss:  0.2577238082885742
train gradient:  0.10889480292365095
iteration : 11694
train acc:  0.84375
train loss:  0.3236544728279114
train gradient:  0.13920147901711222
iteration : 11695
train acc:  0.8359375
train loss:  0.4161861836910248
train gradient:  0.38476085357833967
iteration : 11696
train acc:  0.8984375
train loss:  0.2764741778373718
train gradient:  0.10992636741996333
iteration : 11697
train acc:  0.875
train loss:  0.2941940724849701
train gradient:  0.12735460659850972
iteration : 11698
train acc:  0.875
train loss:  0.2867586612701416
train gradient:  0.11196789053482208
iteration : 11699
train acc:  0.875
train loss:  0.3300993740558624
train gradient:  0.1464863908279038
iteration : 11700
train acc:  0.8828125
train loss:  0.2850194573402405
train gradient:  0.12780028336549815
iteration : 11701
train acc:  0.8515625
train loss:  0.3177992105484009
train gradient:  0.2158721138317629
iteration : 11702
train acc:  0.84375
train loss:  0.3852732181549072
train gradient:  0.22625884482853864
iteration : 11703
train acc:  0.859375
train loss:  0.3100211024284363
train gradient:  0.1416815000441322
iteration : 11704
train acc:  0.8203125
train loss:  0.40333032608032227
train gradient:  0.3037148742671048
iteration : 11705
train acc:  0.8359375
train loss:  0.3489483594894409
train gradient:  0.20022816432814344
iteration : 11706
train acc:  0.875
train loss:  0.30023372173309326
train gradient:  0.13313288507699333
iteration : 11707
train acc:  0.8125
train loss:  0.36845067143440247
train gradient:  0.1670759618811093
iteration : 11708
train acc:  0.859375
train loss:  0.30869564414024353
train gradient:  0.1785203861417835
iteration : 11709
train acc:  0.8359375
train loss:  0.43226227164268494
train gradient:  0.2387813248340785
iteration : 11710
train acc:  0.890625
train loss:  0.26167523860931396
train gradient:  0.1439042815730234
iteration : 11711
train acc:  0.828125
train loss:  0.4157784581184387
train gradient:  0.22512457229480573
iteration : 11712
train acc:  0.8671875
train loss:  0.2890302538871765
train gradient:  0.11123494453450304
iteration : 11713
train acc:  0.84375
train loss:  0.3649941384792328
train gradient:  0.16547144708599915
iteration : 11714
train acc:  0.875
train loss:  0.31998828053474426
train gradient:  0.14086983547759058
iteration : 11715
train acc:  0.8515625
train loss:  0.37051793932914734
train gradient:  0.26424227483271645
iteration : 11716
train acc:  0.875
train loss:  0.31328701972961426
train gradient:  0.18511692935012636
iteration : 11717
train acc:  0.8671875
train loss:  0.3224688172340393
train gradient:  0.13407963953893434
iteration : 11718
train acc:  0.8125
train loss:  0.39754074811935425
train gradient:  0.28136118046227426
iteration : 11719
train acc:  0.859375
train loss:  0.3211115300655365
train gradient:  0.13146499503917713
iteration : 11720
train acc:  0.859375
train loss:  0.31312650442123413
train gradient:  0.16973218190910164
iteration : 11721
train acc:  0.8984375
train loss:  0.2982373833656311
train gradient:  0.2098733357814526
iteration : 11722
train acc:  0.8203125
train loss:  0.365440309047699
train gradient:  0.1601005343933184
iteration : 11723
train acc:  0.90625
train loss:  0.29468679428100586
train gradient:  0.11887854388643096
iteration : 11724
train acc:  0.828125
train loss:  0.3322879672050476
train gradient:  0.1799506258774674
iteration : 11725
train acc:  0.8671875
train loss:  0.3915254473686218
train gradient:  0.18200688770824502
iteration : 11726
train acc:  0.8828125
train loss:  0.24287444353103638
train gradient:  0.08489606564413162
iteration : 11727
train acc:  0.8125
train loss:  0.3844231367111206
train gradient:  0.17719356219243315
iteration : 11728
train acc:  0.8671875
train loss:  0.3325676918029785
train gradient:  0.15924495287596935
iteration : 11729
train acc:  0.8671875
train loss:  0.2699704170227051
train gradient:  0.10907643411095987
iteration : 11730
train acc:  0.8828125
train loss:  0.2939666211605072
train gradient:  0.1667974217947812
iteration : 11731
train acc:  0.875
train loss:  0.3546861410140991
train gradient:  0.15640187569732228
iteration : 11732
train acc:  0.8359375
train loss:  0.3517726957798004
train gradient:  0.14886546312885146
iteration : 11733
train acc:  0.8671875
train loss:  0.27079638838768005
train gradient:  0.1341722873624538
iteration : 11734
train acc:  0.84375
train loss:  0.3699767291545868
train gradient:  0.2191561529512599
iteration : 11735
train acc:  0.90625
train loss:  0.23370863497257233
train gradient:  0.10570487671491195
iteration : 11736
train acc:  0.828125
train loss:  0.3537907004356384
train gradient:  0.17164407816048313
iteration : 11737
train acc:  0.828125
train loss:  0.3413424789905548
train gradient:  0.17979844527318423
iteration : 11738
train acc:  0.8515625
train loss:  0.34313440322875977
train gradient:  0.16949926418905564
iteration : 11739
train acc:  0.859375
train loss:  0.2828354835510254
train gradient:  0.1360014297981858
iteration : 11740
train acc:  0.90625
train loss:  0.2818061113357544
train gradient:  0.07915931570286845
iteration : 11741
train acc:  0.875
train loss:  0.2585911750793457
train gradient:  0.07672341621790608
iteration : 11742
train acc:  0.8671875
train loss:  0.3085516095161438
train gradient:  0.11609939052281268
iteration : 11743
train acc:  0.859375
train loss:  0.3099784255027771
train gradient:  0.13526993156696787
iteration : 11744
train acc:  0.84375
train loss:  0.30384838581085205
train gradient:  0.1191262588996333
iteration : 11745
train acc:  0.8671875
train loss:  0.27911797165870667
train gradient:  0.10419381043420245
iteration : 11746
train acc:  0.8828125
train loss:  0.32733750343322754
train gradient:  0.13367626246046102
iteration : 11747
train acc:  0.84375
train loss:  0.35011908411979675
train gradient:  0.1850778259619594
iteration : 11748
train acc:  0.8515625
train loss:  0.2778819501399994
train gradient:  0.13279581128085388
iteration : 11749
train acc:  0.8984375
train loss:  0.2702813148498535
train gradient:  0.16660954818151855
iteration : 11750
train acc:  0.84375
train loss:  0.3402910828590393
train gradient:  0.12481586355761892
iteration : 11751
train acc:  0.828125
train loss:  0.28269749879837036
train gradient:  0.11736158654514668
iteration : 11752
train acc:  0.8828125
train loss:  0.33387380838394165
train gradient:  0.13288622941910105
iteration : 11753
train acc:  0.84375
train loss:  0.29885631799697876
train gradient:  0.1396465495101124
iteration : 11754
train acc:  0.8359375
train loss:  0.2903774678707123
train gradient:  0.11574149264743053
iteration : 11755
train acc:  0.875
train loss:  0.2830490469932556
train gradient:  0.13184051223104076
iteration : 11756
train acc:  0.859375
train loss:  0.3041371703147888
train gradient:  0.15655307762421034
iteration : 11757
train acc:  0.859375
train loss:  0.2912176847457886
train gradient:  0.14890707745074513
iteration : 11758
train acc:  0.8125
train loss:  0.44408899545669556
train gradient:  0.24050669785359174
iteration : 11759
train acc:  0.8359375
train loss:  0.3728945255279541
train gradient:  0.1455331416478113
iteration : 11760
train acc:  0.890625
train loss:  0.30123093724250793
train gradient:  0.13052287926365652
iteration : 11761
train acc:  0.90625
train loss:  0.2989467978477478
train gradient:  0.15190156112918385
iteration : 11762
train acc:  0.875
train loss:  0.28680914640426636
train gradient:  0.1709015598722904
iteration : 11763
train acc:  0.8515625
train loss:  0.3100023567676544
train gradient:  0.10755044731461678
iteration : 11764
train acc:  0.84375
train loss:  0.33940067887306213
train gradient:  0.19315117530589065
iteration : 11765
train acc:  0.8046875
train loss:  0.3362329602241516
train gradient:  0.17401123656950127
iteration : 11766
train acc:  0.875
train loss:  0.31081587076187134
train gradient:  0.1255873682643459
iteration : 11767
train acc:  0.8359375
train loss:  0.29968148469924927
train gradient:  0.11160430798054616
iteration : 11768
train acc:  0.8359375
train loss:  0.3446348309516907
train gradient:  0.1923659999927886
iteration : 11769
train acc:  0.8515625
train loss:  0.2898593246936798
train gradient:  0.1298516347523883
iteration : 11770
train acc:  0.8671875
train loss:  0.3290226459503174
train gradient:  0.22284215201602195
iteration : 11771
train acc:  0.890625
train loss:  0.2634199857711792
train gradient:  0.1182089414543373
iteration : 11772
train acc:  0.8671875
train loss:  0.3524867594242096
train gradient:  0.17888519557289867
iteration : 11773
train acc:  0.8359375
train loss:  0.34029462933540344
train gradient:  0.19973742690518448
iteration : 11774
train acc:  0.8515625
train loss:  0.30027157068252563
train gradient:  0.15325748107717707
iteration : 11775
train acc:  0.890625
train loss:  0.2787778079509735
train gradient:  0.11072676101807859
iteration : 11776
train acc:  0.828125
train loss:  0.3602234721183777
train gradient:  0.22528390795263992
iteration : 11777
train acc:  0.875
train loss:  0.2806655764579773
train gradient:  0.14728917088947338
iteration : 11778
train acc:  0.859375
train loss:  0.2786200940608978
train gradient:  0.13442415744031447
iteration : 11779
train acc:  0.8671875
train loss:  0.330026239156723
train gradient:  0.13987195886455756
iteration : 11780
train acc:  0.8203125
train loss:  0.4803760051727295
train gradient:  0.3384822058940643
iteration : 11781
train acc:  0.890625
train loss:  0.34271419048309326
train gradient:  0.11734871200886385
iteration : 11782
train acc:  0.84375
train loss:  0.3844911456108093
train gradient:  0.18756824369635314
iteration : 11783
train acc:  0.8671875
train loss:  0.2947552800178528
train gradient:  0.14675013801987832
iteration : 11784
train acc:  0.8359375
train loss:  0.3886759281158447
train gradient:  0.20034229102187945
iteration : 11785
train acc:  0.8828125
train loss:  0.28495004773139954
train gradient:  0.1230020032780752
iteration : 11786
train acc:  0.8359375
train loss:  0.3654654920101166
train gradient:  0.27554651343086395
iteration : 11787
train acc:  0.859375
train loss:  0.28793615102767944
train gradient:  0.11011355950754237
iteration : 11788
train acc:  0.875
train loss:  0.28914573788642883
train gradient:  0.19669670333085426
iteration : 11789
train acc:  0.8515625
train loss:  0.32029080390930176
train gradient:  0.1762249476822273
iteration : 11790
train acc:  0.890625
train loss:  0.30692917108535767
train gradient:  0.13027408070708657
iteration : 11791
train acc:  0.7890625
train loss:  0.45029133558273315
train gradient:  0.22788973845413515
iteration : 11792
train acc:  0.8671875
train loss:  0.3092089295387268
train gradient:  0.1749478053715436
iteration : 11793
train acc:  0.921875
train loss:  0.2649344205856323
train gradient:  0.11971164137172521
iteration : 11794
train acc:  0.8671875
train loss:  0.3330262005329132
train gradient:  0.13456938974504015
iteration : 11795
train acc:  0.890625
train loss:  0.28786277770996094
train gradient:  0.10367665688777682
iteration : 11796
train acc:  0.859375
train loss:  0.30518579483032227
train gradient:  0.1589876480378517
iteration : 11797
train acc:  0.8984375
train loss:  0.24770253896713257
train gradient:  0.08202543678676712
iteration : 11798
train acc:  0.875
train loss:  0.3340870141983032
train gradient:  0.13278507641396303
iteration : 11799
train acc:  0.859375
train loss:  0.29820936918258667
train gradient:  0.1320186692924546
iteration : 11800
train acc:  0.9140625
train loss:  0.2385815680027008
train gradient:  0.08447023488648588
iteration : 11801
train acc:  0.8828125
train loss:  0.2545483410358429
train gradient:  0.0834941005316133
iteration : 11802
train acc:  0.8984375
train loss:  0.2687632441520691
train gradient:  0.16176501934021692
iteration : 11803
train acc:  0.8984375
train loss:  0.25480759143829346
train gradient:  0.1319224013737465
iteration : 11804
train acc:  0.84375
train loss:  0.3026619255542755
train gradient:  0.13730700824225162
iteration : 11805
train acc:  0.828125
train loss:  0.3562127947807312
train gradient:  0.20282721613698995
iteration : 11806
train acc:  0.875
train loss:  0.34720510244369507
train gradient:  0.14749527261185463
iteration : 11807
train acc:  0.890625
train loss:  0.24241039156913757
train gradient:  0.09030947920010364
iteration : 11808
train acc:  0.8828125
train loss:  0.2655143737792969
train gradient:  0.13312835283230473
iteration : 11809
train acc:  0.8984375
train loss:  0.26686325669288635
train gradient:  0.11178595793858599
iteration : 11810
train acc:  0.890625
train loss:  0.35038575530052185
train gradient:  0.14767307272013858
iteration : 11811
train acc:  0.890625
train loss:  0.2677798271179199
train gradient:  0.1790003070321032
iteration : 11812
train acc:  0.859375
train loss:  0.32831162214279175
train gradient:  0.16158216406494166
iteration : 11813
train acc:  0.84375
train loss:  0.28330451250076294
train gradient:  0.19153235836473742
iteration : 11814
train acc:  0.90625
train loss:  0.32150644063949585
train gradient:  0.16851948823857144
iteration : 11815
train acc:  0.8671875
train loss:  0.29682138562202454
train gradient:  0.1740716346287781
iteration : 11816
train acc:  0.8828125
train loss:  0.2981414794921875
train gradient:  0.12858785224698255
iteration : 11817
train acc:  0.875
train loss:  0.3696836233139038
train gradient:  0.2362780388751813
iteration : 11818
train acc:  0.8671875
train loss:  0.3046327233314514
train gradient:  0.1893786121197077
iteration : 11819
train acc:  0.8203125
train loss:  0.4166397154331207
train gradient:  0.26164233107126356
iteration : 11820
train acc:  0.828125
train loss:  0.3579176366329193
train gradient:  0.17579673367018359
iteration : 11821
train acc:  0.8828125
train loss:  0.3042103052139282
train gradient:  0.14277204629574902
iteration : 11822
train acc:  0.90625
train loss:  0.35463765263557434
train gradient:  0.22260597783312341
iteration : 11823
train acc:  0.8828125
train loss:  0.29764342308044434
train gradient:  0.16074300712295736
iteration : 11824
train acc:  0.859375
train loss:  0.3334670662879944
train gradient:  0.13758147752690897
iteration : 11825
train acc:  0.8203125
train loss:  0.3959898352622986
train gradient:  0.2446293080741534
iteration : 11826
train acc:  0.8828125
train loss:  0.2860417664051056
train gradient:  0.21279381624718324
iteration : 11827
train acc:  0.828125
train loss:  0.3967552185058594
train gradient:  0.3145280431689947
iteration : 11828
train acc:  0.828125
train loss:  0.3346570134162903
train gradient:  0.18080003561962027
iteration : 11829
train acc:  0.8359375
train loss:  0.3465994894504547
train gradient:  0.1452551725263586
iteration : 11830
train acc:  0.9375
train loss:  0.1827872395515442
train gradient:  0.07208565720670757
iteration : 11831
train acc:  0.875
train loss:  0.2687826156616211
train gradient:  0.1965078711819053
iteration : 11832
train acc:  0.828125
train loss:  0.3656531572341919
train gradient:  0.1854609582666637
iteration : 11833
train acc:  0.8359375
train loss:  0.4018372595310211
train gradient:  0.2516013035872821
iteration : 11834
train acc:  0.8359375
train loss:  0.3515280485153198
train gradient:  0.17663969083978032
iteration : 11835
train acc:  0.875
train loss:  0.25225895643234253
train gradient:  0.0915623618506565
iteration : 11836
train acc:  0.8671875
train loss:  0.30329984426498413
train gradient:  0.15091499152668886
iteration : 11837
train acc:  0.875
train loss:  0.25219735503196716
train gradient:  0.12916909446709884
iteration : 11838
train acc:  0.8828125
train loss:  0.28602713346481323
train gradient:  0.11786693933084176
iteration : 11839
train acc:  0.90625
train loss:  0.26319557428359985
train gradient:  0.1141063609994559
iteration : 11840
train acc:  0.8828125
train loss:  0.22903038561344147
train gradient:  0.0954593565022691
iteration : 11841
train acc:  0.859375
train loss:  0.3129701614379883
train gradient:  0.16337289077052042
iteration : 11842
train acc:  0.828125
train loss:  0.3634178936481476
train gradient:  0.2161983083401703
iteration : 11843
train acc:  0.890625
train loss:  0.270602285861969
train gradient:  0.12982666036854523
iteration : 11844
train acc:  0.8984375
train loss:  0.28711503744125366
train gradient:  0.19200553710060053
iteration : 11845
train acc:  0.8359375
train loss:  0.30665892362594604
train gradient:  0.13137212778085985
iteration : 11846
train acc:  0.8984375
train loss:  0.25244104862213135
train gradient:  0.12157879463600958
iteration : 11847
train acc:  0.90625
train loss:  0.2704441249370575
train gradient:  0.11745318786545886
iteration : 11848
train acc:  0.84375
train loss:  0.3358731269836426
train gradient:  0.23014203053580623
iteration : 11849
train acc:  0.828125
train loss:  0.38251709938049316
train gradient:  0.1950197516414014
iteration : 11850
train acc:  0.84375
train loss:  0.35551899671554565
train gradient:  0.1851104944857882
iteration : 11851
train acc:  0.8984375
train loss:  0.26327353715896606
train gradient:  0.10128237970563356
iteration : 11852
train acc:  0.921875
train loss:  0.2609674334526062
train gradient:  0.0932345192044469
iteration : 11853
train acc:  0.90625
train loss:  0.23430591821670532
train gradient:  0.11664107211266714
iteration : 11854
train acc:  0.8359375
train loss:  0.36451900005340576
train gradient:  0.19636341404661384
iteration : 11855
train acc:  0.84375
train loss:  0.3890850245952606
train gradient:  0.22797530050933465
iteration : 11856
train acc:  0.90625
train loss:  0.27087876200675964
train gradient:  0.10732618692102605
iteration : 11857
train acc:  0.8515625
train loss:  0.40126413106918335
train gradient:  0.22133530536364404
iteration : 11858
train acc:  0.875
train loss:  0.3451150059700012
train gradient:  0.15876154508650092
iteration : 11859
train acc:  0.921875
train loss:  0.23976865410804749
train gradient:  0.12224768567250704
iteration : 11860
train acc:  0.84375
train loss:  0.3322962522506714
train gradient:  0.14723296608764366
iteration : 11861
train acc:  0.8984375
train loss:  0.26763662695884705
train gradient:  0.1204859826892883
iteration : 11862
train acc:  0.90625
train loss:  0.3134081959724426
train gradient:  0.2410962332164873
iteration : 11863
train acc:  0.859375
train loss:  0.30876901745796204
train gradient:  0.1217466905077698
iteration : 11864
train acc:  0.9140625
train loss:  0.28952866792678833
train gradient:  0.1684616232841621
iteration : 11865
train acc:  0.9140625
train loss:  0.2781074345111847
train gradient:  0.14739040010965415
iteration : 11866
train acc:  0.859375
train loss:  0.42110568284988403
train gradient:  0.21881145985944972
iteration : 11867
train acc:  0.9296875
train loss:  0.21188165247440338
train gradient:  0.0942952948428944
iteration : 11868
train acc:  0.875
train loss:  0.28991973400115967
train gradient:  0.10519576551998258
iteration : 11869
train acc:  0.828125
train loss:  0.36194607615470886
train gradient:  0.15075588641706983
iteration : 11870
train acc:  0.90625
train loss:  0.2637390196323395
train gradient:  0.08997386034617025
iteration : 11871
train acc:  0.890625
train loss:  0.3411129117012024
train gradient:  0.30587480448692145
iteration : 11872
train acc:  0.8046875
train loss:  0.4125268757343292
train gradient:  0.25191112081507205
iteration : 11873
train acc:  0.8359375
train loss:  0.3324565887451172
train gradient:  0.2066667118548758
iteration : 11874
train acc:  0.78125
train loss:  0.4477686882019043
train gradient:  0.27191000682564975
iteration : 11875
train acc:  0.84375
train loss:  0.3856363594532013
train gradient:  0.18807153742801902
iteration : 11876
train acc:  0.875
train loss:  0.3284028470516205
train gradient:  0.10436383029285494
iteration : 11877
train acc:  0.875
train loss:  0.29693061113357544
train gradient:  0.14996875363343837
iteration : 11878
train acc:  0.8359375
train loss:  0.31890159845352173
train gradient:  0.15218889139671937
iteration : 11879
train acc:  0.8828125
train loss:  0.3633474111557007
train gradient:  0.1711275463998479
iteration : 11880
train acc:  0.8515625
train loss:  0.3048066198825836
train gradient:  0.10018858803820245
iteration : 11881
train acc:  0.8828125
train loss:  0.3011403977870941
train gradient:  0.16984102411677438
iteration : 11882
train acc:  0.7734375
train loss:  0.46818089485168457
train gradient:  0.29072691909556475
iteration : 11883
train acc:  0.8125
train loss:  0.45641112327575684
train gradient:  0.2098808948504629
iteration : 11884
train acc:  0.859375
train loss:  0.3220357894897461
train gradient:  0.14123908708818284
iteration : 11885
train acc:  0.859375
train loss:  0.35387691855430603
train gradient:  0.15827765792764395
iteration : 11886
train acc:  0.9140625
train loss:  0.24685797095298767
train gradient:  0.13777514598177987
iteration : 11887
train acc:  0.9140625
train loss:  0.236875981092453
train gradient:  0.07989279486975842
iteration : 11888
train acc:  0.9140625
train loss:  0.29731497168540955
train gradient:  0.23470346971691586
iteration : 11889
train acc:  0.9140625
train loss:  0.28674107789993286
train gradient:  0.11443534705841728
iteration : 11890
train acc:  0.8125
train loss:  0.39215636253356934
train gradient:  0.24105223760065309
iteration : 11891
train acc:  0.84375
train loss:  0.36474886536598206
train gradient:  0.1826448204845919
iteration : 11892
train acc:  0.84375
train loss:  0.33144086599349976
train gradient:  0.1697935735623728
iteration : 11893
train acc:  0.8046875
train loss:  0.38273343443870544
train gradient:  0.23001559231631463
iteration : 11894
train acc:  0.875
train loss:  0.3098068833351135
train gradient:  0.1340720550619565
iteration : 11895
train acc:  0.859375
train loss:  0.310890793800354
train gradient:  0.12934818556035335
iteration : 11896
train acc:  0.890625
train loss:  0.30672168731689453
train gradient:  0.14851687899345095
iteration : 11897
train acc:  0.859375
train loss:  0.34114110469818115
train gradient:  0.16169427627047742
iteration : 11898
train acc:  0.8046875
train loss:  0.3370349407196045
train gradient:  0.16562427155862247
iteration : 11899
train acc:  0.84375
train loss:  0.3345721960067749
train gradient:  0.1326683820537006
iteration : 11900
train acc:  0.8984375
train loss:  0.2484215945005417
train gradient:  0.10181801636235488
iteration : 11901
train acc:  0.7578125
train loss:  0.4117525815963745
train gradient:  0.21350727062967448
iteration : 11902
train acc:  0.8359375
train loss:  0.39637917280197144
train gradient:  0.20762089890343055
iteration : 11903
train acc:  0.8125
train loss:  0.33989936113357544
train gradient:  0.19586114574168792
iteration : 11904
train acc:  0.8359375
train loss:  0.33640217781066895
train gradient:  0.11843688299591058
iteration : 11905
train acc:  0.875
train loss:  0.2931239604949951
train gradient:  0.10667565717464031
iteration : 11906
train acc:  0.8203125
train loss:  0.40786227583885193
train gradient:  0.21063684432062474
iteration : 11907
train acc:  0.8671875
train loss:  0.3357182741165161
train gradient:  0.22580957669198753
iteration : 11908
train acc:  0.8203125
train loss:  0.38646531105041504
train gradient:  0.2133911030902943
iteration : 11909
train acc:  0.9140625
train loss:  0.2227848768234253
train gradient:  0.08979312827765154
iteration : 11910
train acc:  0.921875
train loss:  0.2346607744693756
train gradient:  0.06896105275152331
iteration : 11911
train acc:  0.8203125
train loss:  0.37285691499710083
train gradient:  0.2584476134074044
iteration : 11912
train acc:  0.8828125
train loss:  0.2506716549396515
train gradient:  0.13535415311537474
iteration : 11913
train acc:  0.8828125
train loss:  0.2955783009529114
train gradient:  0.13577650496890625
iteration : 11914
train acc:  0.8203125
train loss:  0.3458458185195923
train gradient:  0.15264784678967008
iteration : 11915
train acc:  0.890625
train loss:  0.2976032793521881
train gradient:  0.11726019789278655
iteration : 11916
train acc:  0.828125
train loss:  0.41559433937072754
train gradient:  0.16269584336344145
iteration : 11917
train acc:  0.875
train loss:  0.3134711980819702
train gradient:  0.12587169299732148
iteration : 11918
train acc:  0.8515625
train loss:  0.3399650454521179
train gradient:  0.2486318785393077
iteration : 11919
train acc:  0.8359375
train loss:  0.3720582127571106
train gradient:  0.24143360800770275
iteration : 11920
train acc:  0.8671875
train loss:  0.24679584801197052
train gradient:  0.11155666045231652
iteration : 11921
train acc:  0.8984375
train loss:  0.2454792559146881
train gradient:  0.08749790756646275
iteration : 11922
train acc:  0.8359375
train loss:  0.40402114391326904
train gradient:  0.1814540026932057
iteration : 11923
train acc:  0.8515625
train loss:  0.35857170820236206
train gradient:  0.14988346721341977
iteration : 11924
train acc:  0.8984375
train loss:  0.2914481461048126
train gradient:  0.16486382285702295
iteration : 11925
train acc:  0.8984375
train loss:  0.26073235273361206
train gradient:  0.09429289459521469
iteration : 11926
train acc:  0.859375
train loss:  0.2623624801635742
train gradient:  0.07903463902753367
iteration : 11927
train acc:  0.9296875
train loss:  0.21735993027687073
train gradient:  0.08441238704051392
iteration : 11928
train acc:  0.8671875
train loss:  0.36365070939064026
train gradient:  0.19673757531560893
iteration : 11929
train acc:  0.8125
train loss:  0.38874155282974243
train gradient:  0.3231860002438858
iteration : 11930
train acc:  0.859375
train loss:  0.3682996928691864
train gradient:  0.18281729398814944
iteration : 11931
train acc:  0.8984375
train loss:  0.2854278087615967
train gradient:  0.10164709787284694
iteration : 11932
train acc:  0.8046875
train loss:  0.44400227069854736
train gradient:  0.3325648331844454
iteration : 11933
train acc:  0.8515625
train loss:  0.39117005467414856
train gradient:  0.21088410291240905
iteration : 11934
train acc:  0.84375
train loss:  0.3000599145889282
train gradient:  0.13889159732074027
iteration : 11935
train acc:  0.8125
train loss:  0.3402959108352661
train gradient:  0.1986781070250987
iteration : 11936
train acc:  0.875
train loss:  0.30893298983573914
train gradient:  0.09169464332000042
iteration : 11937
train acc:  0.8828125
train loss:  0.31045281887054443
train gradient:  0.11721691061001352
iteration : 11938
train acc:  0.859375
train loss:  0.31726449728012085
train gradient:  0.12466384287792483
iteration : 11939
train acc:  0.8671875
train loss:  0.29391998052597046
train gradient:  0.19373750739931966
iteration : 11940
train acc:  0.875
train loss:  0.24211503565311432
train gradient:  0.08803815950667682
iteration : 11941
train acc:  0.796875
train loss:  0.3925238847732544
train gradient:  0.19407598538088106
iteration : 11942
train acc:  0.8125
train loss:  0.36245182156562805
train gradient:  0.16910866682809106
iteration : 11943
train acc:  0.8203125
train loss:  0.3676835894584656
train gradient:  0.20860290290667804
iteration : 11944
train acc:  0.90625
train loss:  0.2266017496585846
train gradient:  0.07158140398918138
iteration : 11945
train acc:  0.8515625
train loss:  0.29502469301223755
train gradient:  0.11538331651955377
iteration : 11946
train acc:  0.9296875
train loss:  0.22985590994358063
train gradient:  0.08465876417384141
iteration : 11947
train acc:  0.8671875
train loss:  0.3621141314506531
train gradient:  0.19803095503737508
iteration : 11948
train acc:  0.84375
train loss:  0.3336934745311737
train gradient:  0.15837672753325255
iteration : 11949
train acc:  0.8359375
train loss:  0.31012070178985596
train gradient:  0.13208453262931544
iteration : 11950
train acc:  0.8828125
train loss:  0.29139143228530884
train gradient:  0.1272175020370434
iteration : 11951
train acc:  0.8046875
train loss:  0.40566161274909973
train gradient:  0.2650913998730238
iteration : 11952
train acc:  0.890625
train loss:  0.26968643069267273
train gradient:  0.08594777457755984
iteration : 11953
train acc:  0.8515625
train loss:  0.3112502098083496
train gradient:  0.1681733843269206
iteration : 11954
train acc:  0.828125
train loss:  0.36969852447509766
train gradient:  0.2022045536968321
iteration : 11955
train acc:  0.921875
train loss:  0.26229128241539
train gradient:  0.11970914013068773
iteration : 11956
train acc:  0.90625
train loss:  0.2218494415283203
train gradient:  0.07985027971292694
iteration : 11957
train acc:  0.875
train loss:  0.33565375208854675
train gradient:  0.17486854200212357
iteration : 11958
train acc:  0.8515625
train loss:  0.2690541446208954
train gradient:  0.11022736098826934
iteration : 11959
train acc:  0.828125
train loss:  0.38320547342300415
train gradient:  0.19012458473431176
iteration : 11960
train acc:  0.890625
train loss:  0.2888534665107727
train gradient:  0.12161684194924115
iteration : 11961
train acc:  0.875
train loss:  0.3744949996471405
train gradient:  0.18091671248667868
iteration : 11962
train acc:  0.8828125
train loss:  0.338119238615036
train gradient:  0.13254571372817708
iteration : 11963
train acc:  0.7890625
train loss:  0.38507938385009766
train gradient:  0.17050243924297265
iteration : 11964
train acc:  0.9375
train loss:  0.22074450552463531
train gradient:  0.09669407505546031
iteration : 11965
train acc:  0.875
train loss:  0.3205586075782776
train gradient:  0.24089230985320303
iteration : 11966
train acc:  0.828125
train loss:  0.3361935019493103
train gradient:  0.1962444665322805
iteration : 11967
train acc:  0.8515625
train loss:  0.34169143438339233
train gradient:  0.12846621146721054
iteration : 11968
train acc:  0.8828125
train loss:  0.3142707943916321
train gradient:  0.1249924453435822
iteration : 11969
train acc:  0.8828125
train loss:  0.29770874977111816
train gradient:  0.14287873516946745
iteration : 11970
train acc:  0.84375
train loss:  0.36086755990982056
train gradient:  0.13909391326953247
iteration : 11971
train acc:  0.8671875
train loss:  0.34194642305374146
train gradient:  0.16911647655125495
iteration : 11972
train acc:  0.84375
train loss:  0.38403648138046265
train gradient:  0.2493497764607847
iteration : 11973
train acc:  0.90625
train loss:  0.2577469050884247
train gradient:  0.1201958048580049
iteration : 11974
train acc:  0.859375
train loss:  0.2947997450828552
train gradient:  0.12426697311330409
iteration : 11975
train acc:  0.890625
train loss:  0.3104582726955414
train gradient:  0.1676517071088447
iteration : 11976
train acc:  0.8984375
train loss:  0.3267938792705536
train gradient:  0.11379132762103593
iteration : 11977
train acc:  0.859375
train loss:  0.31339675188064575
train gradient:  0.12309010741119662
iteration : 11978
train acc:  0.890625
train loss:  0.2975026071071625
train gradient:  0.12919562708610194
iteration : 11979
train acc:  0.8203125
train loss:  0.37192225456237793
train gradient:  0.1434232775857813
iteration : 11980
train acc:  0.84375
train loss:  0.3371187448501587
train gradient:  0.1672044643024983
iteration : 11981
train acc:  0.8828125
train loss:  0.2660530209541321
train gradient:  0.14065786617177112
iteration : 11982
train acc:  0.859375
train loss:  0.330066055059433
train gradient:  0.13557476933319348
iteration : 11983
train acc:  0.828125
train loss:  0.36683136224746704
train gradient:  0.12530257810182302
iteration : 11984
train acc:  0.8046875
train loss:  0.42038601636886597
train gradient:  0.22981448019077588
iteration : 11985
train acc:  0.8671875
train loss:  0.2919985055923462
train gradient:  0.13732337313920318
iteration : 11986
train acc:  0.859375
train loss:  0.34820035099983215
train gradient:  0.1707161076838234
iteration : 11987
train acc:  0.8359375
train loss:  0.35258787870407104
train gradient:  0.1582853629384336
iteration : 11988
train acc:  0.890625
train loss:  0.2702000141143799
train gradient:  0.08761822515238078
iteration : 11989
train acc:  0.84375
train loss:  0.3243507146835327
train gradient:  0.15299948402816815
iteration : 11990
train acc:  0.8359375
train loss:  0.3617512285709381
train gradient:  0.23246356225538753
iteration : 11991
train acc:  0.875
train loss:  0.2989433705806732
train gradient:  0.392099373011981
iteration : 11992
train acc:  0.8515625
train loss:  0.2825702428817749
train gradient:  0.12441940606610863
iteration : 11993
train acc:  0.8515625
train loss:  0.4035271108150482
train gradient:  0.20553356837362474
iteration : 11994
train acc:  0.90625
train loss:  0.31295469403266907
train gradient:  0.16113482298284834
iteration : 11995
train acc:  0.875
train loss:  0.2956538498401642
train gradient:  0.14538435871530436
iteration : 11996
train acc:  0.796875
train loss:  0.41884732246398926
train gradient:  0.20843579094922432
iteration : 11997
train acc:  0.828125
train loss:  0.3765140473842621
train gradient:  0.16454382060661926
iteration : 11998
train acc:  0.84375
train loss:  0.3372565805912018
train gradient:  0.14442456445830332
iteration : 11999
train acc:  0.8515625
train loss:  0.30714961886405945
train gradient:  0.11621454676637581
iteration : 12000
train acc:  0.8359375
train loss:  0.31233733892440796
train gradient:  0.15798990987601136
iteration : 12001
train acc:  0.8359375
train loss:  0.3407090902328491
train gradient:  0.15581717633941514
iteration : 12002
train acc:  0.828125
train loss:  0.3179992735385895
train gradient:  0.21245646206204277
iteration : 12003
train acc:  0.8359375
train loss:  0.41579270362854004
train gradient:  0.1899779370388731
iteration : 12004
train acc:  0.890625
train loss:  0.3133331537246704
train gradient:  0.11699136912393594
iteration : 12005
train acc:  0.875
train loss:  0.29795655608177185
train gradient:  0.18743353705841959
iteration : 12006
train acc:  0.84375
train loss:  0.34818780422210693
train gradient:  0.12602950280994668
iteration : 12007
train acc:  0.8671875
train loss:  0.322425901889801
train gradient:  0.18452143488637004
iteration : 12008
train acc:  0.890625
train loss:  0.33750689029693604
train gradient:  0.14622749128254997
iteration : 12009
train acc:  0.8046875
train loss:  0.41921377182006836
train gradient:  0.24432257406635982
iteration : 12010
train acc:  0.8515625
train loss:  0.35617220401763916
train gradient:  0.1705700563695398
iteration : 12011
train acc:  0.8359375
train loss:  0.35267508029937744
train gradient:  0.1595981438147148
iteration : 12012
train acc:  0.9140625
train loss:  0.2102312445640564
train gradient:  0.0755460197961656
iteration : 12013
train acc:  0.8203125
train loss:  0.3458796739578247
train gradient:  0.1581982733379898
iteration : 12014
train acc:  0.8515625
train loss:  0.3305600881576538
train gradient:  0.17459835514892402
iteration : 12015
train acc:  0.84375
train loss:  0.31621068716049194
train gradient:  0.10620911311266094
iteration : 12016
train acc:  0.890625
train loss:  0.23975059390068054
train gradient:  0.0841237281997345
iteration : 12017
train acc:  0.8203125
train loss:  0.4527691900730133
train gradient:  0.2774427081460814
iteration : 12018
train acc:  0.84375
train loss:  0.30564218759536743
train gradient:  0.11802765710110454
iteration : 12019
train acc:  0.8203125
train loss:  0.3731595277786255
train gradient:  0.18542255664046492
iteration : 12020
train acc:  0.84375
train loss:  0.39280155301094055
train gradient:  0.2489847562483572
iteration : 12021
train acc:  0.875
train loss:  0.27125632762908936
train gradient:  0.13753858904432692
iteration : 12022
train acc:  0.859375
train loss:  0.3362908363342285
train gradient:  0.10699476565776504
iteration : 12023
train acc:  0.90625
train loss:  0.24194395542144775
train gradient:  0.08067347013777794
iteration : 12024
train acc:  0.8359375
train loss:  0.386210173368454
train gradient:  0.1794753881122824
iteration : 12025
train acc:  0.859375
train loss:  0.3273999094963074
train gradient:  0.14052891331936523
iteration : 12026
train acc:  0.8828125
train loss:  0.296652615070343
train gradient:  0.10065127644867897
iteration : 12027
train acc:  0.875
train loss:  0.2890196740627289
train gradient:  0.10589293078681353
iteration : 12028
train acc:  0.8359375
train loss:  0.35402870178222656
train gradient:  0.13043398223636984
iteration : 12029
train acc:  0.8203125
train loss:  0.39373669028282166
train gradient:  0.18995433735978978
iteration : 12030
train acc:  0.84375
train loss:  0.3165120482444763
train gradient:  0.09794534804999455
iteration : 12031
train acc:  0.9296875
train loss:  0.23644742369651794
train gradient:  0.0871528186973643
iteration : 12032
train acc:  0.8515625
train loss:  0.29441869258880615
train gradient:  0.11623982025472507
iteration : 12033
train acc:  0.84375
train loss:  0.3541981875896454
train gradient:  0.18292387192707033
iteration : 12034
train acc:  0.859375
train loss:  0.34929102659225464
train gradient:  0.23456185402081298
iteration : 12035
train acc:  0.8359375
train loss:  0.3613763451576233
train gradient:  0.16942700786417025
iteration : 12036
train acc:  0.890625
train loss:  0.2768929600715637
train gradient:  0.09568427313541232
iteration : 12037
train acc:  0.9140625
train loss:  0.2307857722043991
train gradient:  0.08616559513306801
iteration : 12038
train acc:  0.890625
train loss:  0.26256072521209717
train gradient:  0.12133090896631171
iteration : 12039
train acc:  0.8671875
train loss:  0.31370824575424194
train gradient:  0.10672306395009418
iteration : 12040
train acc:  0.8671875
train loss:  0.3090684413909912
train gradient:  0.12475878025438285
iteration : 12041
train acc:  0.875
train loss:  0.29562681913375854
train gradient:  0.14100225147590315
iteration : 12042
train acc:  0.84375
train loss:  0.36766088008880615
train gradient:  0.12458517285396914
iteration : 12043
train acc:  0.8359375
train loss:  0.3783528208732605
train gradient:  0.20634881062489163
iteration : 12044
train acc:  0.8671875
train loss:  0.33270061016082764
train gradient:  0.12053473916128929
iteration : 12045
train acc:  0.8515625
train loss:  0.3382335305213928
train gradient:  0.13941284091236542
iteration : 12046
train acc:  0.8828125
train loss:  0.31512612104415894
train gradient:  0.11474120334885707
iteration : 12047
train acc:  0.8984375
train loss:  0.3054458498954773
train gradient:  0.09703430537312047
iteration : 12048
train acc:  0.8671875
train loss:  0.2543339431285858
train gradient:  0.11370505480760523
iteration : 12049
train acc:  0.875
train loss:  0.29634860157966614
train gradient:  0.11976790982189156
iteration : 12050
train acc:  0.8515625
train loss:  0.3023369610309601
train gradient:  0.16480933018127053
iteration : 12051
train acc:  0.8515625
train loss:  0.28559499979019165
train gradient:  0.11313283433919974
iteration : 12052
train acc:  0.859375
train loss:  0.3329904079437256
train gradient:  0.19629878408873663
iteration : 12053
train acc:  0.8359375
train loss:  0.3485938310623169
train gradient:  0.16326069218447473
iteration : 12054
train acc:  0.8671875
train loss:  0.36572033166885376
train gradient:  0.17264609971073147
iteration : 12055
train acc:  0.9296875
train loss:  0.2361481785774231
train gradient:  0.1671361193392379
iteration : 12056
train acc:  0.875
train loss:  0.26167502999305725
train gradient:  0.08364030303684337
iteration : 12057
train acc:  0.8984375
train loss:  0.23512297868728638
train gradient:  0.08862044351680415
iteration : 12058
train acc:  0.8515625
train loss:  0.339198499917984
train gradient:  0.14908676387205794
iteration : 12059
train acc:  0.8671875
train loss:  0.34153109788894653
train gradient:  0.13767100025980672
iteration : 12060
train acc:  0.84375
train loss:  0.33882975578308105
train gradient:  0.14446062476708
iteration : 12061
train acc:  0.8828125
train loss:  0.2847244143486023
train gradient:  0.10022344857478856
iteration : 12062
train acc:  0.859375
train loss:  0.3204028010368347
train gradient:  0.14429477907407884
iteration : 12063
train acc:  0.8515625
train loss:  0.3866375982761383
train gradient:  0.2016278848718669
iteration : 12064
train acc:  0.8828125
train loss:  0.2918757200241089
train gradient:  0.17772851234255121
iteration : 12065
train acc:  0.8671875
train loss:  0.34795188903808594
train gradient:  0.1511276943069873
iteration : 12066
train acc:  0.84375
train loss:  0.37507012486457825
train gradient:  0.27678214705709026
iteration : 12067
train acc:  0.875
train loss:  0.3400799036026001
train gradient:  0.13520516718393372
iteration : 12068
train acc:  0.9140625
train loss:  0.2329392433166504
train gradient:  0.08498098882506902
iteration : 12069
train acc:  0.875
train loss:  0.274935245513916
train gradient:  0.12402361453340223
iteration : 12070
train acc:  0.859375
train loss:  0.3002980351448059
train gradient:  0.10864512757565736
iteration : 12071
train acc:  0.8828125
train loss:  0.28725481033325195
train gradient:  0.16687967277718962
iteration : 12072
train acc:  0.8671875
train loss:  0.31532010436058044
train gradient:  0.1223962398850878
iteration : 12073
train acc:  0.796875
train loss:  0.41381916403770447
train gradient:  0.2521910268210956
iteration : 12074
train acc:  0.828125
train loss:  0.3920392394065857
train gradient:  0.21643934418332927
iteration : 12075
train acc:  0.84375
train loss:  0.3586328625679016
train gradient:  0.15175275583375
iteration : 12076
train acc:  0.8203125
train loss:  0.42370522022247314
train gradient:  0.44418639881357064
iteration : 12077
train acc:  0.875
train loss:  0.2953492999076843
train gradient:  0.11221233552986697
iteration : 12078
train acc:  0.8828125
train loss:  0.29886364936828613
train gradient:  0.13374525010717425
iteration : 12079
train acc:  0.9296875
train loss:  0.2193802148103714
train gradient:  0.09997743486228412
iteration : 12080
train acc:  0.859375
train loss:  0.2764899730682373
train gradient:  0.08450846116123345
iteration : 12081
train acc:  0.8359375
train loss:  0.375399649143219
train gradient:  0.1449905050716728
iteration : 12082
train acc:  0.8359375
train loss:  0.3271666169166565
train gradient:  0.1405622449758758
iteration : 12083
train acc:  0.8828125
train loss:  0.3260141611099243
train gradient:  0.19691869738804524
iteration : 12084
train acc:  0.921875
train loss:  0.2889810800552368
train gradient:  0.1094326337435429
iteration : 12085
train acc:  0.8671875
train loss:  0.3299306631088257
train gradient:  0.1601381734453146
iteration : 12086
train acc:  0.796875
train loss:  0.40021181106567383
train gradient:  0.2139584490812874
iteration : 12087
train acc:  0.890625
train loss:  0.2381550371646881
train gradient:  0.08441806426613967
iteration : 12088
train acc:  0.7890625
train loss:  0.379264771938324
train gradient:  0.17256025258366298
iteration : 12089
train acc:  0.875
train loss:  0.3038497567176819
train gradient:  0.12654840636485098
iteration : 12090
train acc:  0.8359375
train loss:  0.35518720746040344
train gradient:  0.15123553017639013
iteration : 12091
train acc:  0.84375
train loss:  0.3831636905670166
train gradient:  0.23217179132276153
iteration : 12092
train acc:  0.8515625
train loss:  0.3779086470603943
train gradient:  0.1974671602300766
iteration : 12093
train acc:  0.8359375
train loss:  0.29136568307876587
train gradient:  0.11212987572764256
iteration : 12094
train acc:  0.875
train loss:  0.2742612361907959
train gradient:  0.11902760906609527
iteration : 12095
train acc:  0.796875
train loss:  0.37484675645828247
train gradient:  0.17186687630624217
iteration : 12096
train acc:  0.8671875
train loss:  0.34176141023635864
train gradient:  0.15072297831917547
iteration : 12097
train acc:  0.8359375
train loss:  0.3154904246330261
train gradient:  0.14632304359496898
iteration : 12098
train acc:  0.84375
train loss:  0.30847910046577454
train gradient:  0.14906189911279763
iteration : 12099
train acc:  0.84375
train loss:  0.26717281341552734
train gradient:  0.1378424377723315
iteration : 12100
train acc:  0.8515625
train loss:  0.34675332903862
train gradient:  0.1583976913952949
iteration : 12101
train acc:  0.8828125
train loss:  0.3081344962120056
train gradient:  0.21207892102596088
iteration : 12102
train acc:  0.8125
train loss:  0.35875028371810913
train gradient:  0.22326881315628128
iteration : 12103
train acc:  0.84375
train loss:  0.365155965089798
train gradient:  0.5781582593896435
iteration : 12104
train acc:  0.8671875
train loss:  0.3072434663772583
train gradient:  0.18419233549576622
iteration : 12105
train acc:  0.9375
train loss:  0.20335686206817627
train gradient:  0.07529119273141148
iteration : 12106
train acc:  0.859375
train loss:  0.3592654764652252
train gradient:  0.13217814604217026
iteration : 12107
train acc:  0.90625
train loss:  0.2701408267021179
train gradient:  0.0971720957202443
iteration : 12108
train acc:  0.8515625
train loss:  0.36746788024902344
train gradient:  0.20504896860001154
iteration : 12109
train acc:  0.84375
train loss:  0.41097941994667053
train gradient:  0.17620031755162419
iteration : 12110
train acc:  0.8671875
train loss:  0.32253026962280273
train gradient:  0.12743083625368296
iteration : 12111
train acc:  0.8203125
train loss:  0.34201276302337646
train gradient:  0.19033941589884307
iteration : 12112
train acc:  0.84375
train loss:  0.3495640158653259
train gradient:  0.1404926846017511
iteration : 12113
train acc:  0.8125
train loss:  0.4586663246154785
train gradient:  0.32370191746012433
iteration : 12114
train acc:  0.8359375
train loss:  0.3551153242588043
train gradient:  0.1934170970081207
iteration : 12115
train acc:  0.90625
train loss:  0.299818754196167
train gradient:  0.11819308195383484
iteration : 12116
train acc:  0.875
train loss:  0.3447071313858032
train gradient:  0.17818210647001453
iteration : 12117
train acc:  0.8359375
train loss:  0.3904021978378296
train gradient:  0.2581790884963346
iteration : 12118
train acc:  0.7890625
train loss:  0.3504365086555481
train gradient:  0.20680608792187752
iteration : 12119
train acc:  0.8828125
train loss:  0.30672386288642883
train gradient:  0.10262573050249486
iteration : 12120
train acc:  0.84375
train loss:  0.3124997019767761
train gradient:  0.13514468599087154
iteration : 12121
train acc:  0.8359375
train loss:  0.3262113034725189
train gradient:  0.11903266576628425
iteration : 12122
train acc:  0.8359375
train loss:  0.3479573130607605
train gradient:  0.26623137608418485
iteration : 12123
train acc:  0.90625
train loss:  0.28282615542411804
train gradient:  0.16110803894427653
iteration : 12124
train acc:  0.875
train loss:  0.28109389543533325
train gradient:  0.10826094646867426
iteration : 12125
train acc:  0.875
train loss:  0.3165622353553772
train gradient:  0.11640420855920798
iteration : 12126
train acc:  0.796875
train loss:  0.43799519538879395
train gradient:  0.22544896320235114
iteration : 12127
train acc:  0.8125
train loss:  0.35310518741607666
train gradient:  0.11530796471350577
iteration : 12128
train acc:  0.90625
train loss:  0.22706682980060577
train gradient:  0.08287658054936689
iteration : 12129
train acc:  0.8046875
train loss:  0.38624387979507446
train gradient:  0.1756342587751326
iteration : 12130
train acc:  0.8984375
train loss:  0.21077848970890045
train gradient:  0.06886138169069618
iteration : 12131
train acc:  0.8359375
train loss:  0.3254620432853699
train gradient:  0.13210988855978284
iteration : 12132
train acc:  0.84375
train loss:  0.3239140212535858
train gradient:  0.12458551753175119
iteration : 12133
train acc:  0.8671875
train loss:  0.3596259355545044
train gradient:  0.2291832203310617
iteration : 12134
train acc:  0.84375
train loss:  0.3159175515174866
train gradient:  0.1498431995177681
iteration : 12135
train acc:  0.859375
train loss:  0.33192163705825806
train gradient:  0.1372973333668591
iteration : 12136
train acc:  0.8671875
train loss:  0.3119373917579651
train gradient:  0.11953469752804734
iteration : 12137
train acc:  0.8359375
train loss:  0.35485774278640747
train gradient:  0.2286468700125261
iteration : 12138
train acc:  0.8515625
train loss:  0.33290553092956543
train gradient:  0.1754820676075435
iteration : 12139
train acc:  0.8671875
train loss:  0.34480607509613037
train gradient:  0.11121954850317753
iteration : 12140
train acc:  0.828125
train loss:  0.3393194079399109
train gradient:  0.19223882640223897
iteration : 12141
train acc:  0.828125
train loss:  0.36066654324531555
train gradient:  0.18685330318465043
iteration : 12142
train acc:  0.8671875
train loss:  0.2756205201148987
train gradient:  0.09886111849092079
iteration : 12143
train acc:  0.859375
train loss:  0.3377351760864258
train gradient:  0.20953710206768467
iteration : 12144
train acc:  0.890625
train loss:  0.32818108797073364
train gradient:  0.191560889896506
iteration : 12145
train acc:  0.875
train loss:  0.2839815020561218
train gradient:  0.15299978557770683
iteration : 12146
train acc:  0.7890625
train loss:  0.37472280859947205
train gradient:  0.14240274674493697
iteration : 12147
train acc:  0.8984375
train loss:  0.3149355947971344
train gradient:  0.13464958564673923
iteration : 12148
train acc:  0.8984375
train loss:  0.23629270493984222
train gradient:  0.08536805912557205
iteration : 12149
train acc:  0.796875
train loss:  0.36491724848747253
train gradient:  0.17280532008314906
iteration : 12150
train acc:  0.8671875
train loss:  0.2993645668029785
train gradient:  0.13422783845357844
iteration : 12151
train acc:  0.859375
train loss:  0.31510284543037415
train gradient:  0.09707401854979184
iteration : 12152
train acc:  0.8828125
train loss:  0.3103037178516388
train gradient:  0.1090201807691963
iteration : 12153
train acc:  0.8828125
train loss:  0.3103364109992981
train gradient:  0.13107979886038995
iteration : 12154
train acc:  0.8125
train loss:  0.33054685592651367
train gradient:  0.1359656134314267
iteration : 12155
train acc:  0.8671875
train loss:  0.2857941687107086
train gradient:  0.12991074640306413
iteration : 12156
train acc:  0.828125
train loss:  0.3843323588371277
train gradient:  0.1733017973528627
iteration : 12157
train acc:  0.8515625
train loss:  0.36133161187171936
train gradient:  0.16413632560712663
iteration : 12158
train acc:  0.90625
train loss:  0.24345842003822327
train gradient:  0.07618442729745936
iteration : 12159
train acc:  0.8515625
train loss:  0.3292607069015503
train gradient:  0.15358470704166066
iteration : 12160
train acc:  0.9140625
train loss:  0.24668249487876892
train gradient:  0.08426996382743185
iteration : 12161
train acc:  0.8359375
train loss:  0.3415849208831787
train gradient:  0.15089243984871006
iteration : 12162
train acc:  0.84375
train loss:  0.3620750904083252
train gradient:  0.16256155629438895
iteration : 12163
train acc:  0.8984375
train loss:  0.2702566087245941
train gradient:  0.12950953245905636
iteration : 12164
train acc:  0.8359375
train loss:  0.30003654956817627
train gradient:  0.1545728192187708
iteration : 12165
train acc:  0.890625
train loss:  0.3111729323863983
train gradient:  0.21592973808959867
iteration : 12166
train acc:  0.890625
train loss:  0.26024967432022095
train gradient:  0.10958370126229565
iteration : 12167
train acc:  0.875
train loss:  0.3091001510620117
train gradient:  0.13999478077540858
iteration : 12168
train acc:  0.890625
train loss:  0.3095940947532654
train gradient:  0.13914856125052344
iteration : 12169
train acc:  0.78125
train loss:  0.4418514370918274
train gradient:  0.25050668496306117
iteration : 12170
train acc:  0.8671875
train loss:  0.33867859840393066
train gradient:  0.1251709470448355
iteration : 12171
train acc:  0.8515625
train loss:  0.34643006324768066
train gradient:  0.12578630187125092
iteration : 12172
train acc:  0.859375
train loss:  0.3195134699344635
train gradient:  0.13217201039644072
iteration : 12173
train acc:  0.859375
train loss:  0.3285403847694397
train gradient:  0.1446726751388122
iteration : 12174
train acc:  0.8984375
train loss:  0.24194161593914032
train gradient:  0.11977994030784306
iteration : 12175
train acc:  0.84375
train loss:  0.37722980976104736
train gradient:  0.21993739647064664
iteration : 12176
train acc:  0.765625
train loss:  0.4463376998901367
train gradient:  0.25160133163531107
iteration : 12177
train acc:  0.84375
train loss:  0.32904866337776184
train gradient:  0.13130785546977553
iteration : 12178
train acc:  0.8359375
train loss:  0.32912757992744446
train gradient:  0.1760393097948609
iteration : 12179
train acc:  0.8515625
train loss:  0.37133222818374634
train gradient:  0.1745260298268923
iteration : 12180
train acc:  0.859375
train loss:  0.3177778720855713
train gradient:  0.1703369434698358
iteration : 12181
train acc:  0.8984375
train loss:  0.30503764748573303
train gradient:  0.12473548799713899
iteration : 12182
train acc:  0.8671875
train loss:  0.3148650527000427
train gradient:  0.20509773206826726
iteration : 12183
train acc:  0.90625
train loss:  0.28030097484588623
train gradient:  0.11449284444045629
iteration : 12184
train acc:  0.890625
train loss:  0.2886531352996826
train gradient:  0.12346989757516248
iteration : 12185
train acc:  0.8359375
train loss:  0.36631232500076294
train gradient:  0.18160276068137776
iteration : 12186
train acc:  0.8671875
train loss:  0.34863826632499695
train gradient:  0.12343040160204002
iteration : 12187
train acc:  0.8515625
train loss:  0.30327099561691284
train gradient:  0.1335349399443685
iteration : 12188
train acc:  0.859375
train loss:  0.33057379722595215
train gradient:  0.16825420051861245
iteration : 12189
train acc:  0.8046875
train loss:  0.3714505732059479
train gradient:  0.18497405310438275
iteration : 12190
train acc:  0.8828125
train loss:  0.26874738931655884
train gradient:  0.09621315741571516
iteration : 12191
train acc:  0.90625
train loss:  0.27362656593322754
train gradient:  0.12669965704650785
iteration : 12192
train acc:  0.8359375
train loss:  0.36847639083862305
train gradient:  0.25948075307377194
iteration : 12193
train acc:  0.828125
train loss:  0.32790863513946533
train gradient:  0.11847336456404006
iteration : 12194
train acc:  0.875
train loss:  0.2976134419441223
train gradient:  0.1098688784320822
iteration : 12195
train acc:  0.8515625
train loss:  0.3102564215660095
train gradient:  0.15290772352749676
iteration : 12196
train acc:  0.8671875
train loss:  0.28013113141059875
train gradient:  0.10165832428234267
iteration : 12197
train acc:  0.890625
train loss:  0.300439715385437
train gradient:  0.12510824998590223
iteration : 12198
train acc:  0.8515625
train loss:  0.3223132789134979
train gradient:  0.1215207438351386
iteration : 12199
train acc:  0.8515625
train loss:  0.3121306896209717
train gradient:  0.16472879992005784
iteration : 12200
train acc:  0.8359375
train loss:  0.32735341787338257
train gradient:  0.11922018240863327
iteration : 12201
train acc:  0.8828125
train loss:  0.32351529598236084
train gradient:  0.16541277438921298
iteration : 12202
train acc:  0.859375
train loss:  0.36209553480148315
train gradient:  0.1957886685812411
iteration : 12203
train acc:  0.890625
train loss:  0.3081217110157013
train gradient:  0.10090734095478497
iteration : 12204
train acc:  0.8359375
train loss:  0.2994202971458435
train gradient:  0.14124781508767764
iteration : 12205
train acc:  0.890625
train loss:  0.3050187826156616
train gradient:  0.08661489157887574
iteration : 12206
train acc:  0.78125
train loss:  0.4664941430091858
train gradient:  0.2853235730372632
iteration : 12207
train acc:  0.875
train loss:  0.2793685495853424
train gradient:  0.10496706014858058
iteration : 12208
train acc:  0.9140625
train loss:  0.24826082587242126
train gradient:  0.08737703706935901
iteration : 12209
train acc:  0.8515625
train loss:  0.3586917519569397
train gradient:  0.21222996021916904
iteration : 12210
train acc:  0.8828125
train loss:  0.3108060359954834
train gradient:  0.13622555891573482
iteration : 12211
train acc:  0.8671875
train loss:  0.38270846009254456
train gradient:  0.17471566914955095
iteration : 12212
train acc:  0.84375
train loss:  0.311794638633728
train gradient:  0.12500879175264462
iteration : 12213
train acc:  0.8515625
train loss:  0.39726418256759644
train gradient:  0.19064933325777927
iteration : 12214
train acc:  0.8828125
train loss:  0.23626431822776794
train gradient:  0.11067450119407475
iteration : 12215
train acc:  0.8125
train loss:  0.4255264103412628
train gradient:  0.24957722165845253
iteration : 12216
train acc:  0.8671875
train loss:  0.27098023891448975
train gradient:  0.10251592441755629
iteration : 12217
train acc:  0.84375
train loss:  0.32992076873779297
train gradient:  0.21130117975898904
iteration : 12218
train acc:  0.9296875
train loss:  0.1928025484085083
train gradient:  0.07934672498699519
iteration : 12219
train acc:  0.8203125
train loss:  0.3719772696495056
train gradient:  0.163221392366334
iteration : 12220
train acc:  0.8359375
train loss:  0.4176381826400757
train gradient:  0.23897390483223696
iteration : 12221
train acc:  0.859375
train loss:  0.35607096552848816
train gradient:  0.18278172697940998
iteration : 12222
train acc:  0.859375
train loss:  0.33222711086273193
train gradient:  0.13736610464720828
iteration : 12223
train acc:  0.8046875
train loss:  0.3764628767967224
train gradient:  0.1948542802301384
iteration : 12224
train acc:  0.8359375
train loss:  0.34394246339797974
train gradient:  0.1363294637837542
iteration : 12225
train acc:  0.859375
train loss:  0.29505181312561035
train gradient:  0.09382905771529432
iteration : 12226
train acc:  0.8046875
train loss:  0.45529597997665405
train gradient:  0.20731771563580886
iteration : 12227
train acc:  0.8671875
train loss:  0.3457532227039337
train gradient:  0.10822735277676533
iteration : 12228
train acc:  0.8671875
train loss:  0.31232500076293945
train gradient:  0.07683340483582149
iteration : 12229
train acc:  0.8671875
train loss:  0.3093218505382538
train gradient:  0.11783054052976707
iteration : 12230
train acc:  0.8828125
train loss:  0.2769153118133545
train gradient:  0.12568380945738683
iteration : 12231
train acc:  0.921875
train loss:  0.21875585615634918
train gradient:  0.07669561974757982
iteration : 12232
train acc:  0.90625
train loss:  0.2589210867881775
train gradient:  0.1030258262522496
iteration : 12233
train acc:  0.8671875
train loss:  0.32114964723587036
train gradient:  0.13753228032016218
iteration : 12234
train acc:  0.890625
train loss:  0.26743215322494507
train gradient:  0.12185942744137448
iteration : 12235
train acc:  0.90625
train loss:  0.23998554050922394
train gradient:  0.1150563804129654
iteration : 12236
train acc:  0.8671875
train loss:  0.3381282091140747
train gradient:  0.1134536099818177
iteration : 12237
train acc:  0.8984375
train loss:  0.2706676721572876
train gradient:  0.13099180166876112
iteration : 12238
train acc:  0.875
train loss:  0.29140323400497437
train gradient:  0.11200065804514939
iteration : 12239
train acc:  0.8671875
train loss:  0.28962385654449463
train gradient:  0.12434185330310518
iteration : 12240
train acc:  0.8828125
train loss:  0.31858861446380615
train gradient:  0.20276804685817018
iteration : 12241
train acc:  0.9140625
train loss:  0.23404572904109955
train gradient:  0.1047760767989347
iteration : 12242
train acc:  0.8515625
train loss:  0.35141658782958984
train gradient:  0.16737231814431186
iteration : 12243
train acc:  0.9140625
train loss:  0.2555261254310608
train gradient:  0.10237459409855058
iteration : 12244
train acc:  0.84375
train loss:  0.3492825925350189
train gradient:  0.18331912998068686
iteration : 12245
train acc:  0.8203125
train loss:  0.38325539231300354
train gradient:  0.17853194960644606
iteration : 12246
train acc:  0.8671875
train loss:  0.29833394289016724
train gradient:  0.11121046964440955
iteration : 12247
train acc:  0.90625
train loss:  0.23227648437023163
train gradient:  0.12681136812198088
iteration : 12248
train acc:  0.875
train loss:  0.3359571099281311
train gradient:  0.1559056092010306
iteration : 12249
train acc:  0.8125
train loss:  0.3519081473350525
train gradient:  0.15959281353209784
iteration : 12250
train acc:  0.8203125
train loss:  0.35461780428886414
train gradient:  0.14180371280156284
iteration : 12251
train acc:  0.8359375
train loss:  0.4192698001861572
train gradient:  0.22150807951596627
iteration : 12252
train acc:  0.8671875
train loss:  0.3515571355819702
train gradient:  0.16407457536135206
iteration : 12253
train acc:  0.84375
train loss:  0.4017688035964966
train gradient:  0.18498027507284406
iteration : 12254
train acc:  0.921875
train loss:  0.233933225274086
train gradient:  0.0883866399887507
iteration : 12255
train acc:  0.8515625
train loss:  0.2788139879703522
train gradient:  0.10429417764288881
iteration : 12256
train acc:  0.9140625
train loss:  0.24603943526744843
train gradient:  0.11171570019728691
iteration : 12257
train acc:  0.84375
train loss:  0.2904272675514221
train gradient:  0.09721839822243616
iteration : 12258
train acc:  0.8203125
train loss:  0.3843058943748474
train gradient:  0.17077530189729723
iteration : 12259
train acc:  0.84375
train loss:  0.35895609855651855
train gradient:  0.15363437833823124
iteration : 12260
train acc:  0.890625
train loss:  0.2800139784812927
train gradient:  0.1942641947453187
iteration : 12261
train acc:  0.8671875
train loss:  0.3158312439918518
train gradient:  0.13218674244902062
iteration : 12262
train acc:  0.828125
train loss:  0.37726274132728577
train gradient:  0.1661542257305087
iteration : 12263
train acc:  0.8203125
train loss:  0.42163997888565063
train gradient:  0.24701493561712026
iteration : 12264
train acc:  0.8125
train loss:  0.39385998249053955
train gradient:  0.24961713112614414
iteration : 12265
train acc:  0.859375
train loss:  0.3561316132545471
train gradient:  0.16421099470497916
iteration : 12266
train acc:  0.84375
train loss:  0.4094942808151245
train gradient:  0.197657049632733
iteration : 12267
train acc:  0.875
train loss:  0.31631168723106384
train gradient:  0.1462084275211321
iteration : 12268
train acc:  0.828125
train loss:  0.3452833890914917
train gradient:  0.175242676464667
iteration : 12269
train acc:  0.828125
train loss:  0.33539849519729614
train gradient:  0.13117418326880587
iteration : 12270
train acc:  0.8359375
train loss:  0.38009825348854065
train gradient:  0.17617388225728248
iteration : 12271
train acc:  0.90625
train loss:  0.2752707302570343
train gradient:  0.11664441164490813
iteration : 12272
train acc:  0.8828125
train loss:  0.23790840804576874
train gradient:  0.13965643497084518
iteration : 12273
train acc:  0.8828125
train loss:  0.3104306161403656
train gradient:  0.12308157072252761
iteration : 12274
train acc:  0.8515625
train loss:  0.3120388984680176
train gradient:  0.10062211968085673
iteration : 12275
train acc:  0.84375
train loss:  0.4014701247215271
train gradient:  0.2448023633116731
iteration : 12276
train acc:  0.8359375
train loss:  0.3203478753566742
train gradient:  0.11845055848757162
iteration : 12277
train acc:  0.875
train loss:  0.29286351799964905
train gradient:  0.11588917941130406
iteration : 12278
train acc:  0.8125
train loss:  0.33819466829299927
train gradient:  0.16556913637972778
iteration : 12279
train acc:  0.8828125
train loss:  0.3004354238510132
train gradient:  0.12230612223841171
iteration : 12280
train acc:  0.859375
train loss:  0.35305047035217285
train gradient:  0.10388995418139141
iteration : 12281
train acc:  0.8828125
train loss:  0.37111473083496094
train gradient:  0.13990642310097373
iteration : 12282
train acc:  0.8203125
train loss:  0.3424282670021057
train gradient:  0.12931895806641533
iteration : 12283
train acc:  0.8359375
train loss:  0.34318238496780396
train gradient:  0.16763443901887337
iteration : 12284
train acc:  0.8359375
train loss:  0.36923936009407043
train gradient:  0.15883556388640513
iteration : 12285
train acc:  0.8515625
train loss:  0.3605109453201294
train gradient:  0.17471252673644938
iteration : 12286
train acc:  0.921875
train loss:  0.23895138502120972
train gradient:  0.07967129560058595
iteration : 12287
train acc:  0.8515625
train loss:  0.33927464485168457
train gradient:  0.12570295214690785
iteration : 12288
train acc:  0.875
train loss:  0.2508028745651245
train gradient:  0.10108279750596902
iteration : 12289
train acc:  0.890625
train loss:  0.3261148929595947
train gradient:  0.13546027729949017
iteration : 12290
train acc:  0.875
train loss:  0.3223416805267334
train gradient:  0.10332888448394538
iteration : 12291
train acc:  0.828125
train loss:  0.40238064527511597
train gradient:  0.1625780005316821
iteration : 12292
train acc:  0.8828125
train loss:  0.2581843137741089
train gradient:  0.13380444711905906
iteration : 12293
train acc:  0.8359375
train loss:  0.3862209916114807
train gradient:  0.17167239805723009
iteration : 12294
train acc:  0.875
train loss:  0.34669995307922363
train gradient:  0.18572312060924728
iteration : 12295
train acc:  0.796875
train loss:  0.3724318742752075
train gradient:  0.17142791794436713
iteration : 12296
train acc:  0.90625
train loss:  0.29928356409072876
train gradient:  0.11771931673350958
iteration : 12297
train acc:  0.8359375
train loss:  0.35457319021224976
train gradient:  0.1905085824356792
iteration : 12298
train acc:  0.8515625
train loss:  0.31095653772354126
train gradient:  0.1316813025216756
iteration : 12299
train acc:  0.9140625
train loss:  0.26751527190208435
train gradient:  0.13735263183090418
iteration : 12300
train acc:  0.8984375
train loss:  0.31277161836624146
train gradient:  0.12242940457869929
iteration : 12301
train acc:  0.890625
train loss:  0.3126974105834961
train gradient:  0.1529175691630525
iteration : 12302
train acc:  0.9140625
train loss:  0.2971031665802002
train gradient:  0.1330412887402331
iteration : 12303
train acc:  0.84375
train loss:  0.29574981331825256
train gradient:  0.1462387802469619
iteration : 12304
train acc:  0.8828125
train loss:  0.3042062520980835
train gradient:  0.153314822157099
iteration : 12305
train acc:  0.84375
train loss:  0.3521523177623749
train gradient:  0.1338235335307751
iteration : 12306
train acc:  0.8359375
train loss:  0.36253589391708374
train gradient:  0.17950591834730018
iteration : 12307
train acc:  0.859375
train loss:  0.34195104241371155
train gradient:  0.12248308520233132
iteration : 12308
train acc:  0.8671875
train loss:  0.3013573884963989
train gradient:  0.09004530358494184
iteration : 12309
train acc:  0.8671875
train loss:  0.30324018001556396
train gradient:  0.13631589992496046
iteration : 12310
train acc:  0.953125
train loss:  0.21084217727184296
train gradient:  0.06965289449507732
iteration : 12311
train acc:  0.859375
train loss:  0.3014845550060272
train gradient:  0.1461701185414872
iteration : 12312
train acc:  0.8515625
train loss:  0.29824578762054443
train gradient:  0.10421645711392125
iteration : 12313
train acc:  0.8828125
train loss:  0.28232672810554504
train gradient:  0.09705142904462544
iteration : 12314
train acc:  0.859375
train loss:  0.284523069858551
train gradient:  0.09674545792373365
iteration : 12315
train acc:  0.890625
train loss:  0.2682962417602539
train gradient:  0.097133397134822
iteration : 12316
train acc:  0.828125
train loss:  0.38467100262641907
train gradient:  0.19794429973822647
iteration : 12317
train acc:  0.796875
train loss:  0.47180768847465515
train gradient:  0.25098194597546825
iteration : 12318
train acc:  0.8828125
train loss:  0.2729807496070862
train gradient:  0.09737648987653094
iteration : 12319
train acc:  0.8984375
train loss:  0.2978706657886505
train gradient:  0.12790198258703245
iteration : 12320
train acc:  0.8515625
train loss:  0.29926609992980957
train gradient:  0.14392499165775835
iteration : 12321
train acc:  0.890625
train loss:  0.26224642992019653
train gradient:  0.09289474799806252
iteration : 12322
train acc:  0.8828125
train loss:  0.31656235456466675
train gradient:  0.11959529974500069
iteration : 12323
train acc:  0.8046875
train loss:  0.41106677055358887
train gradient:  0.21105297788655103
iteration : 12324
train acc:  0.890625
train loss:  0.2829045355319977
train gradient:  0.14789715011097576
iteration : 12325
train acc:  0.8359375
train loss:  0.4082987606525421
train gradient:  0.21256536456900496
iteration : 12326
train acc:  0.921875
train loss:  0.20716959238052368
train gradient:  0.06993805669156916
iteration : 12327
train acc:  0.8984375
train loss:  0.3203584551811218
train gradient:  0.14938545996626656
iteration : 12328
train acc:  0.8203125
train loss:  0.3387763202190399
train gradient:  0.13903340612243642
iteration : 12329
train acc:  0.890625
train loss:  0.3155037760734558
train gradient:  0.15319621418211088
iteration : 12330
train acc:  0.890625
train loss:  0.30012720823287964
train gradient:  0.10357597731782749
iteration : 12331
train acc:  0.828125
train loss:  0.3798380196094513
train gradient:  0.18208563863765398
iteration : 12332
train acc:  0.8515625
train loss:  0.310986191034317
train gradient:  0.12326776098236408
iteration : 12333
train acc:  0.8515625
train loss:  0.3634769916534424
train gradient:  0.16821434119518275
iteration : 12334
train acc:  0.8984375
train loss:  0.2238101363182068
train gradient:  0.10690561844929944
iteration : 12335
train acc:  0.828125
train loss:  0.36551016569137573
train gradient:  0.189643473304828
iteration : 12336
train acc:  0.8359375
train loss:  0.35146990418434143
train gradient:  0.15064131098536832
iteration : 12337
train acc:  0.8515625
train loss:  0.31333890557289124
train gradient:  0.14452171489237653
iteration : 12338
train acc:  0.8984375
train loss:  0.2795957326889038
train gradient:  0.10891975643793793
iteration : 12339
train acc:  0.8828125
train loss:  0.2861679196357727
train gradient:  0.11042563903819361
iteration : 12340
train acc:  0.890625
train loss:  0.27469825744628906
train gradient:  0.10805385800774549
iteration : 12341
train acc:  0.859375
train loss:  0.2896091639995575
train gradient:  0.12462532963176287
iteration : 12342
train acc:  0.8359375
train loss:  0.36039453744888306
train gradient:  0.2046970698207482
iteration : 12343
train acc:  0.9140625
train loss:  0.2507827579975128
train gradient:  0.0836886960358151
iteration : 12344
train acc:  0.90625
train loss:  0.26255273818969727
train gradient:  0.09720096454509715
iteration : 12345
train acc:  0.8515625
train loss:  0.312564492225647
train gradient:  0.14112195620862358
iteration : 12346
train acc:  0.8203125
train loss:  0.35565778613090515
train gradient:  0.14791397052166275
iteration : 12347
train acc:  0.8203125
train loss:  0.3364337682723999
train gradient:  0.144660247308615
iteration : 12348
train acc:  0.8671875
train loss:  0.3352448344230652
train gradient:  0.21560098678747902
iteration : 12349
train acc:  0.8359375
train loss:  0.29416996240615845
train gradient:  0.12641705392525365
iteration : 12350
train acc:  0.890625
train loss:  0.29186442494392395
train gradient:  0.14497233944953275
iteration : 12351
train acc:  0.8671875
train loss:  0.2977291941642761
train gradient:  0.10733075394050785
iteration : 12352
train acc:  0.8046875
train loss:  0.42664092779159546
train gradient:  0.3761843881629813
iteration : 12353
train acc:  0.8671875
train loss:  0.24098779261112213
train gradient:  0.0775739303351791
iteration : 12354
train acc:  0.8515625
train loss:  0.3763735890388489
train gradient:  0.1543608830809281
iteration : 12355
train acc:  0.8515625
train loss:  0.3624870181083679
train gradient:  0.15745495147079822
iteration : 12356
train acc:  0.84375
train loss:  0.3337678909301758
train gradient:  0.15212769841938478
iteration : 12357
train acc:  0.875
train loss:  0.34403979778289795
train gradient:  0.19216476808725785
iteration : 12358
train acc:  0.8203125
train loss:  0.39943671226501465
train gradient:  0.16537851567475353
iteration : 12359
train acc:  0.859375
train loss:  0.31467878818511963
train gradient:  0.1853928548458038
iteration : 12360
train acc:  0.859375
train loss:  0.3807319402694702
train gradient:  0.15546216867499923
iteration : 12361
train acc:  0.84375
train loss:  0.3126823604106903
train gradient:  0.12867401644407492
iteration : 12362
train acc:  0.8359375
train loss:  0.33599209785461426
train gradient:  0.14773600902086892
iteration : 12363
train acc:  0.859375
train loss:  0.2788432538509369
train gradient:  0.1097164700701539
iteration : 12364
train acc:  0.890625
train loss:  0.2929433584213257
train gradient:  0.17126238971693225
iteration : 12365
train acc:  0.828125
train loss:  0.4151751399040222
train gradient:  0.20691683741406972
iteration : 12366
train acc:  0.796875
train loss:  0.4323089122772217
train gradient:  0.2717823640463313
iteration : 12367
train acc:  0.8359375
train loss:  0.3754037916660309
train gradient:  0.17383915459551239
iteration : 12368
train acc:  0.8828125
train loss:  0.274769127368927
train gradient:  0.1132012439552461
iteration : 12369
train acc:  0.828125
train loss:  0.44551512598991394
train gradient:  0.268206050616888
iteration : 12370
train acc:  0.8359375
train loss:  0.31416797637939453
train gradient:  0.1325045229590489
iteration : 12371
train acc:  0.8203125
train loss:  0.35059142112731934
train gradient:  0.13115567134437017
iteration : 12372
train acc:  0.828125
train loss:  0.4198301434516907
train gradient:  0.2455058765843995
iteration : 12373
train acc:  0.8828125
train loss:  0.34174850583076477
train gradient:  0.18533758190218483
iteration : 12374
train acc:  0.8671875
train loss:  0.32107219099998474
train gradient:  0.09848660160658393
iteration : 12375
train acc:  0.796875
train loss:  0.4090901017189026
train gradient:  0.2043753194579664
iteration : 12376
train acc:  0.890625
train loss:  0.26450270414352417
train gradient:  0.08274786508399717
iteration : 12377
train acc:  0.828125
train loss:  0.36422649025917053
train gradient:  0.17587597889789083
iteration : 12378
train acc:  0.8984375
train loss:  0.26087474822998047
train gradient:  0.09333545682341153
iteration : 12379
train acc:  0.84375
train loss:  0.37930476665496826
train gradient:  0.12925678425587175
iteration : 12380
train acc:  0.859375
train loss:  0.2716202735900879
train gradient:  0.07705346703660933
iteration : 12381
train acc:  0.8984375
train loss:  0.2573847472667694
train gradient:  0.13636975967277293
iteration : 12382
train acc:  0.8359375
train loss:  0.3906307816505432
train gradient:  0.2324054130200237
iteration : 12383
train acc:  0.8671875
train loss:  0.305978387594223
train gradient:  0.11190509360460878
iteration : 12384
train acc:  0.8671875
train loss:  0.277732253074646
train gradient:  0.0853588106748772
iteration : 12385
train acc:  0.84375
train loss:  0.4200463891029358
train gradient:  0.21840154886314614
iteration : 12386
train acc:  0.8515625
train loss:  0.3150288462638855
train gradient:  0.13203528677960014
iteration : 12387
train acc:  0.859375
train loss:  0.3829745054244995
train gradient:  0.2215679480807103
iteration : 12388
train acc:  0.8203125
train loss:  0.39953330159187317
train gradient:  0.19937516840341485
iteration : 12389
train acc:  0.875
train loss:  0.3294978141784668
train gradient:  0.11710213009045681
iteration : 12390
train acc:  0.8984375
train loss:  0.2650659382343292
train gradient:  0.1156123837765513
iteration : 12391
train acc:  0.8203125
train loss:  0.35201331973075867
train gradient:  0.16768061584283267
iteration : 12392
train acc:  0.8359375
train loss:  0.3805692195892334
train gradient:  0.20180737917120978
iteration : 12393
train acc:  0.8671875
train loss:  0.31399205327033997
train gradient:  0.14753569055516097
iteration : 12394
train acc:  0.8828125
train loss:  0.3489674925804138
train gradient:  0.16946852482695907
iteration : 12395
train acc:  0.859375
train loss:  0.29529815912246704
train gradient:  0.12658547945619852
iteration : 12396
train acc:  0.8125
train loss:  0.3282346725463867
train gradient:  0.13911826445796727
iteration : 12397
train acc:  0.8359375
train loss:  0.32283827662467957
train gradient:  0.10306214768884164
iteration : 12398
train acc:  0.875
train loss:  0.3195285499095917
train gradient:  0.12341221242035018
iteration : 12399
train acc:  0.8984375
train loss:  0.2918151617050171
train gradient:  0.20096338303571679
iteration : 12400
train acc:  0.8984375
train loss:  0.2681867778301239
train gradient:  0.13084840780136842
iteration : 12401
train acc:  0.8984375
train loss:  0.310147762298584
train gradient:  0.11713985726020862
iteration : 12402
train acc:  0.8828125
train loss:  0.31444787979125977
train gradient:  0.1295514909760506
iteration : 12403
train acc:  0.8671875
train loss:  0.286354124546051
train gradient:  0.12483760164129182
iteration : 12404
train acc:  0.828125
train loss:  0.42931532859802246
train gradient:  0.19747710323080472
iteration : 12405
train acc:  0.875
train loss:  0.3461490273475647
train gradient:  0.14904533150334748
iteration : 12406
train acc:  0.90625
train loss:  0.2730986475944519
train gradient:  0.10382600763879991
iteration : 12407
train acc:  0.8828125
train loss:  0.2790728211402893
train gradient:  0.11934741869151327
iteration : 12408
train acc:  0.875
train loss:  0.25073015689849854
train gradient:  0.12159666617272234
iteration : 12409
train acc:  0.796875
train loss:  0.3905644416809082
train gradient:  0.1578091347845826
iteration : 12410
train acc:  0.8671875
train loss:  0.3007507622241974
train gradient:  0.15706553791402736
iteration : 12411
train acc:  0.921875
train loss:  0.23280751705169678
train gradient:  0.07434110421528428
iteration : 12412
train acc:  0.890625
train loss:  0.312015175819397
train gradient:  0.11071424464173703
iteration : 12413
train acc:  0.8125
train loss:  0.3962303400039673
train gradient:  0.20167747778174544
iteration : 12414
train acc:  0.9375
train loss:  0.24364101886749268
train gradient:  0.148947217342612
iteration : 12415
train acc:  0.828125
train loss:  0.2982769012451172
train gradient:  0.13861290954149133
iteration : 12416
train acc:  0.8515625
train loss:  0.3029479384422302
train gradient:  0.13501725290151084
iteration : 12417
train acc:  0.8671875
train loss:  0.30581721663475037
train gradient:  0.13444241666685636
iteration : 12418
train acc:  0.8828125
train loss:  0.36702635884284973
train gradient:  0.1643182196994455
iteration : 12419
train acc:  0.859375
train loss:  0.3645651936531067
train gradient:  0.14707810364801086
iteration : 12420
train acc:  0.8671875
train loss:  0.3005959689617157
train gradient:  0.13132957075703766
iteration : 12421
train acc:  0.8359375
train loss:  0.33733248710632324
train gradient:  0.13852664382106283
iteration : 12422
train acc:  0.8671875
train loss:  0.2581920623779297
train gradient:  0.0570979938424994
iteration : 12423
train acc:  0.8515625
train loss:  0.37400466203689575
train gradient:  0.16559421865439705
iteration : 12424
train acc:  0.90625
train loss:  0.26419922709465027
train gradient:  0.1319279457031628
iteration : 12425
train acc:  0.8984375
train loss:  0.26839175820350647
train gradient:  0.09464565529567684
iteration : 12426
train acc:  0.90625
train loss:  0.24602439999580383
train gradient:  0.11258834477859994
iteration : 12427
train acc:  0.8359375
train loss:  0.3239406645298004
train gradient:  0.11120198796102397
iteration : 12428
train acc:  0.90625
train loss:  0.24006758630275726
train gradient:  0.09150317462737442
iteration : 12429
train acc:  0.9140625
train loss:  0.2463323175907135
train gradient:  0.09908983791201428
iteration : 12430
train acc:  0.8515625
train loss:  0.3277547061443329
train gradient:  0.10839536549525139
iteration : 12431
train acc:  0.8984375
train loss:  0.27183303236961365
train gradient:  0.11050456105697523
iteration : 12432
train acc:  0.8828125
train loss:  0.28020423650741577
train gradient:  0.12155660281456407
iteration : 12433
train acc:  0.890625
train loss:  0.29469093680381775
train gradient:  0.14996170155346544
iteration : 12434
train acc:  0.875
train loss:  0.28564250469207764
train gradient:  0.10791604005583787
iteration : 12435
train acc:  0.8984375
train loss:  0.2716209888458252
train gradient:  0.08960056180594221
iteration : 12436
train acc:  0.859375
train loss:  0.29928088188171387
train gradient:  0.13432253498499092
iteration : 12437
train acc:  0.84375
train loss:  0.35102465748786926
train gradient:  0.12208889432074006
iteration : 12438
train acc:  0.8046875
train loss:  0.39963412284851074
train gradient:  0.1817940493217715
iteration : 12439
train acc:  0.828125
train loss:  0.3100448548793793
train gradient:  0.17587155302090499
iteration : 12440
train acc:  0.875
train loss:  0.32304704189300537
train gradient:  0.1240811378013242
iteration : 12441
train acc:  0.921875
train loss:  0.28383734822273254
train gradient:  0.09768066197042051
iteration : 12442
train acc:  0.84375
train loss:  0.3013361692428589
train gradient:  0.1387309692975942
iteration : 12443
train acc:  0.8828125
train loss:  0.2682511806488037
train gradient:  0.10680568385352104
iteration : 12444
train acc:  0.796875
train loss:  0.41502469778060913
train gradient:  0.22611571121892682
iteration : 12445
train acc:  0.890625
train loss:  0.25602152943611145
train gradient:  0.11252561233995649
iteration : 12446
train acc:  0.8671875
train loss:  0.30827611684799194
train gradient:  0.18743602710100626
iteration : 12447
train acc:  0.9296875
train loss:  0.2334776669740677
train gradient:  0.14723617873415487
iteration : 12448
train acc:  0.9140625
train loss:  0.22923544049263
train gradient:  0.1178763047711869
iteration : 12449
train acc:  0.828125
train loss:  0.37418681383132935
train gradient:  0.17783869291872706
iteration : 12450
train acc:  0.875
train loss:  0.2781721353530884
train gradient:  0.1534797541046496
iteration : 12451
train acc:  0.8515625
train loss:  0.293735146522522
train gradient:  0.15102231792764872
iteration : 12452
train acc:  0.8984375
train loss:  0.25128236413002014
train gradient:  0.10441780872991457
iteration : 12453
train acc:  0.7734375
train loss:  0.46079951524734497
train gradient:  0.2613621419045616
iteration : 12454
train acc:  0.875
train loss:  0.33617323637008667
train gradient:  0.14021459082957488
iteration : 12455
train acc:  0.828125
train loss:  0.3676144480705261
train gradient:  0.15183891174867398
iteration : 12456
train acc:  0.859375
train loss:  0.4114205837249756
train gradient:  0.21319926233558098
iteration : 12457
train acc:  0.8359375
train loss:  0.4134232997894287
train gradient:  0.23358158664185477
iteration : 12458
train acc:  0.8359375
train loss:  0.302114874124527
train gradient:  0.11948118463240147
iteration : 12459
train acc:  0.859375
train loss:  0.357921302318573
train gradient:  0.14073483840565532
iteration : 12460
train acc:  0.875
train loss:  0.3190021514892578
train gradient:  0.11773852030494057
iteration : 12461
train acc:  0.90625
train loss:  0.2706146240234375
train gradient:  0.18767965654493962
iteration : 12462
train acc:  0.9140625
train loss:  0.26080477237701416
train gradient:  0.10888401778527251
iteration : 12463
train acc:  0.8828125
train loss:  0.33496329188346863
train gradient:  0.14618308236664693
iteration : 12464
train acc:  0.8515625
train loss:  0.3192555010318756
train gradient:  0.13736609236169578
iteration : 12465
train acc:  0.875
train loss:  0.33859148621559143
train gradient:  0.15150343305334846
iteration : 12466
train acc:  0.84375
train loss:  0.4014219045639038
train gradient:  0.15962360410830426
iteration : 12467
train acc:  0.8828125
train loss:  0.3254442811012268
train gradient:  0.12165593981622715
iteration : 12468
train acc:  0.890625
train loss:  0.27791911363601685
train gradient:  0.10423820903956062
iteration : 12469
train acc:  0.8671875
train loss:  0.3422260880470276
train gradient:  0.1231835307483742
iteration : 12470
train acc:  0.8359375
train loss:  0.4045048952102661
train gradient:  0.17524992547816726
iteration : 12471
train acc:  0.8828125
train loss:  0.2970379590988159
train gradient:  0.13643467576009097
iteration : 12472
train acc:  0.8125
train loss:  0.3985708951950073
train gradient:  0.17146156519716538
iteration : 12473
train acc:  0.828125
train loss:  0.3256189525127411
train gradient:  0.1320232415849268
iteration : 12474
train acc:  0.8671875
train loss:  0.3300538659095764
train gradient:  0.1845521166932811
iteration : 12475
train acc:  0.8515625
train loss:  0.3167040944099426
train gradient:  0.11465270281192458
iteration : 12476
train acc:  0.8515625
train loss:  0.2965015769004822
train gradient:  0.13735399996306916
iteration : 12477
train acc:  0.890625
train loss:  0.2958446741104126
train gradient:  0.09591077519663149
iteration : 12478
train acc:  0.84375
train loss:  0.34298211336135864
train gradient:  0.14657757377218336
iteration : 12479
train acc:  0.890625
train loss:  0.2605409622192383
train gradient:  0.11697252948317292
iteration : 12480
train acc:  0.8984375
train loss:  0.24622739851474762
train gradient:  0.10843245769905863
iteration : 12481
train acc:  0.8828125
train loss:  0.272743821144104
train gradient:  0.08836631720113398
iteration : 12482
train acc:  0.859375
train loss:  0.3280799984931946
train gradient:  0.13075410605311572
iteration : 12483
train acc:  0.8359375
train loss:  0.31860285997390747
train gradient:  0.13646526928623265
iteration : 12484
train acc:  0.84375
train loss:  0.3807505965232849
train gradient:  0.16809249889198186
iteration : 12485
train acc:  0.8984375
train loss:  0.2774132490158081
train gradient:  0.10110295810973566
iteration : 12486
train acc:  0.8515625
train loss:  0.31811094284057617
train gradient:  0.1266619950256745
iteration : 12487
train acc:  0.859375
train loss:  0.3719449043273926
train gradient:  0.17636742895126695
iteration : 12488
train acc:  0.8671875
train loss:  0.29352304339408875
train gradient:  0.1729947929970119
iteration : 12489
train acc:  0.859375
train loss:  0.34306105971336365
train gradient:  0.14737205536814388
iteration : 12490
train acc:  0.8359375
train loss:  0.3933383822441101
train gradient:  0.19582178507679193
iteration : 12491
train acc:  0.875
train loss:  0.31285127997398376
train gradient:  0.12665097808421616
iteration : 12492
train acc:  0.8046875
train loss:  0.42258065938949585
train gradient:  0.3000208919085266
iteration : 12493
train acc:  0.8359375
train loss:  0.4440716505050659
train gradient:  0.2593323792525021
iteration : 12494
train acc:  0.8359375
train loss:  0.31068676710128784
train gradient:  0.14448583840229567
iteration : 12495
train acc:  0.8671875
train loss:  0.25624018907546997
train gradient:  0.09362277931615813
iteration : 12496
train acc:  0.8671875
train loss:  0.34672242403030396
train gradient:  0.11819877183145912
iteration : 12497
train acc:  0.828125
train loss:  0.3790554702281952
train gradient:  0.19010210513468362
iteration : 12498
train acc:  0.90625
train loss:  0.3238745927810669
train gradient:  0.126264261788874
iteration : 12499
train acc:  0.90625
train loss:  0.24860218167304993
train gradient:  0.11404140587540573
iteration : 12500
train acc:  0.8828125
train loss:  0.29930275678634644
train gradient:  0.1130433230161917
iteration : 12501
train acc:  0.8671875
train loss:  0.3399127721786499
train gradient:  0.18001448978392326
iteration : 12502
train acc:  0.921875
train loss:  0.29626893997192383
train gradient:  0.11923513929715927
iteration : 12503
train acc:  0.921875
train loss:  0.22945323586463928
train gradient:  0.10536332474002222
iteration : 12504
train acc:  0.875
train loss:  0.3222707211971283
train gradient:  0.1035192897563777
iteration : 12505
train acc:  0.890625
train loss:  0.368786484003067
train gradient:  0.19717112054806912
iteration : 12506
train acc:  0.8671875
train loss:  0.31556349992752075
train gradient:  0.1600319218245404
iteration : 12507
train acc:  0.8671875
train loss:  0.27977675199508667
train gradient:  0.12164234483322035
iteration : 12508
train acc:  0.8203125
train loss:  0.3804159164428711
train gradient:  0.20569238869910902
iteration : 12509
train acc:  0.859375
train loss:  0.28055304288864136
train gradient:  0.1446122030547475
iteration : 12510
train acc:  0.8828125
train loss:  0.25704076886177063
train gradient:  0.08106251056562551
iteration : 12511
train acc:  0.859375
train loss:  0.39301443099975586
train gradient:  0.17556607592457152
iteration : 12512
train acc:  0.8359375
train loss:  0.3229631781578064
train gradient:  0.11563897252426593
iteration : 12513
train acc:  0.8828125
train loss:  0.32026612758636475
train gradient:  0.13195349852205757
iteration : 12514
train acc:  0.8359375
train loss:  0.3515758812427521
train gradient:  0.15113178788693182
iteration : 12515
train acc:  0.8125
train loss:  0.42589741945266724
train gradient:  0.29084309212499504
iteration : 12516
train acc:  0.859375
train loss:  0.3173776865005493
train gradient:  0.15210889897061264
iteration : 12517
train acc:  0.859375
train loss:  0.3276675343513489
train gradient:  0.13782049602443516
iteration : 12518
train acc:  0.8828125
train loss:  0.3100745677947998
train gradient:  0.11130095965136705
iteration : 12519
train acc:  0.859375
train loss:  0.36279135942459106
train gradient:  0.18399933945362523
iteration : 12520
train acc:  0.828125
train loss:  0.34407663345336914
train gradient:  0.23017216245373193
iteration : 12521
train acc:  0.859375
train loss:  0.3084586262702942
train gradient:  0.12057253945307171
iteration : 12522
train acc:  0.90625
train loss:  0.33014222979545593
train gradient:  0.17828297459554837
iteration : 12523
train acc:  0.8515625
train loss:  0.3310755491256714
train gradient:  0.19365149822956018
iteration : 12524
train acc:  0.8203125
train loss:  0.38605496287345886
train gradient:  0.26176329488619626
iteration : 12525
train acc:  0.8359375
train loss:  0.40458840131759644
train gradient:  0.3175410329028677
iteration : 12526
train acc:  0.90625
train loss:  0.29102879762649536
train gradient:  0.10118775596893932
iteration : 12527
train acc:  0.8515625
train loss:  0.3980634808540344
train gradient:  0.1875040176000629
iteration : 12528
train acc:  0.921875
train loss:  0.2492293119430542
train gradient:  0.09557649294595391
iteration : 12529
train acc:  0.8671875
train loss:  0.35187631845474243
train gradient:  0.18757815471252914
iteration : 12530
train acc:  0.8359375
train loss:  0.35507315397262573
train gradient:  0.21376344380099652
iteration : 12531
train acc:  0.8984375
train loss:  0.25032520294189453
train gradient:  0.07406090563103278
iteration : 12532
train acc:  0.890625
train loss:  0.30043160915374756
train gradient:  0.14646086812801234
iteration : 12533
train acc:  0.8359375
train loss:  0.36874276399612427
train gradient:  0.17304839961421714
iteration : 12534
train acc:  0.875
train loss:  0.28552332520484924
train gradient:  0.08310008241745355
iteration : 12535
train acc:  0.90625
train loss:  0.24745839834213257
train gradient:  0.11107885939863421
iteration : 12536
train acc:  0.84375
train loss:  0.3605462312698364
train gradient:  0.14691965782274902
iteration : 12537
train acc:  0.8984375
train loss:  0.30772665143013
train gradient:  0.12001640217117221
iteration : 12538
train acc:  0.875
train loss:  0.28533709049224854
train gradient:  0.1320728435191405
iteration : 12539
train acc:  0.8125
train loss:  0.4415031671524048
train gradient:  0.339997740810001
iteration : 12540
train acc:  0.8984375
train loss:  0.26609060168266296
train gradient:  0.11441374122681518
iteration : 12541
train acc:  0.859375
train loss:  0.2789382040500641
train gradient:  0.11915230780166744
iteration : 12542
train acc:  0.8671875
train loss:  0.34829986095428467
train gradient:  0.1440059938850259
iteration : 12543
train acc:  0.8671875
train loss:  0.31868547201156616
train gradient:  0.14226750255675458
iteration : 12544
train acc:  0.8203125
train loss:  0.41332679986953735
train gradient:  0.27146756009310696
iteration : 12545
train acc:  0.7734375
train loss:  0.4702392816543579
train gradient:  0.3459303210776163
iteration : 12546
train acc:  0.8046875
train loss:  0.41425395011901855
train gradient:  0.19714420899445603
iteration : 12547
train acc:  0.828125
train loss:  0.38895174860954285
train gradient:  0.16907692792667836
iteration : 12548
train acc:  0.8828125
train loss:  0.22501899302005768
train gradient:  0.09916547674124357
iteration : 12549
train acc:  0.859375
train loss:  0.2892879247665405
train gradient:  0.1461465643769243
iteration : 12550
train acc:  0.8515625
train loss:  0.35221749544143677
train gradient:  0.15943889523127158
iteration : 12551
train acc:  0.859375
train loss:  0.3478572368621826
train gradient:  0.1786955707257894
iteration : 12552
train acc:  0.8671875
train loss:  0.29472672939300537
train gradient:  0.10385009337514722
iteration : 12553
train acc:  0.875
train loss:  0.2896358370780945
train gradient:  0.09904594957181234
iteration : 12554
train acc:  0.8984375
train loss:  0.31061428785324097
train gradient:  0.1963830152454853
iteration : 12555
train acc:  0.8984375
train loss:  0.257997065782547
train gradient:  0.09888180786610527
iteration : 12556
train acc:  0.859375
train loss:  0.33435702323913574
train gradient:  0.19191873588028657
iteration : 12557
train acc:  0.8515625
train loss:  0.3342961370944977
train gradient:  0.11822388030618937
iteration : 12558
train acc:  0.8515625
train loss:  0.2916477918624878
train gradient:  0.09415700355911456
iteration : 12559
train acc:  0.859375
train loss:  0.31019991636276245
train gradient:  0.12000614193359778
iteration : 12560
train acc:  0.859375
train loss:  0.3341759443283081
train gradient:  0.12583403299384754
iteration : 12561
train acc:  0.8359375
train loss:  0.36092984676361084
train gradient:  0.12612711139845428
iteration : 12562
train acc:  0.8359375
train loss:  0.3559802770614624
train gradient:  0.14493452065322743
iteration : 12563
train acc:  0.84375
train loss:  0.27982616424560547
train gradient:  0.11784180661502709
iteration : 12564
train acc:  0.828125
train loss:  0.3427835702896118
train gradient:  0.157638862434325
iteration : 12565
train acc:  0.859375
train loss:  0.2998529076576233
train gradient:  0.11358012516918342
iteration : 12566
train acc:  0.7890625
train loss:  0.3216971158981323
train gradient:  0.15333911258832766
iteration : 12567
train acc:  0.8671875
train loss:  0.3479583263397217
train gradient:  0.1370459996555129
iteration : 12568
train acc:  0.8984375
train loss:  0.2532268762588501
train gradient:  0.11260918947488198
iteration : 12569
train acc:  0.859375
train loss:  0.3581541180610657
train gradient:  0.16518083573236647
iteration : 12570
train acc:  0.875
train loss:  0.3660164773464203
train gradient:  0.14492873817284702
iteration : 12571
train acc:  0.8671875
train loss:  0.30912554264068604
train gradient:  0.10402919368491488
iteration : 12572
train acc:  0.9140625
train loss:  0.26839399337768555
train gradient:  0.11527027347660902
iteration : 12573
train acc:  0.859375
train loss:  0.3519746661186218
train gradient:  0.14695115632894964
iteration : 12574
train acc:  0.875
train loss:  0.3245159685611725
train gradient:  0.13991591581697224
iteration : 12575
train acc:  0.8359375
train loss:  0.34073859453201294
train gradient:  0.17057939559525176
iteration : 12576
train acc:  0.8359375
train loss:  0.38065671920776367
train gradient:  0.1531008592794177
iteration : 12577
train acc:  0.8984375
train loss:  0.29483145475387573
train gradient:  0.20825309154114963
iteration : 12578
train acc:  0.8828125
train loss:  0.3168885111808777
train gradient:  0.1358546278406336
iteration : 12579
train acc:  0.8359375
train loss:  0.3292805850505829
train gradient:  0.1441348942649553
iteration : 12580
train acc:  0.875
train loss:  0.31382107734680176
train gradient:  0.1688729778764026
iteration : 12581
train acc:  0.8828125
train loss:  0.3001303970813751
train gradient:  0.0753192298886539
iteration : 12582
train acc:  0.90625
train loss:  0.22450928390026093
train gradient:  0.09753248496393091
iteration : 12583
train acc:  0.8125
train loss:  0.4465794563293457
train gradient:  0.26688175240116147
iteration : 12584
train acc:  0.859375
train loss:  0.3198372721672058
train gradient:  0.15261901139233405
iteration : 12585
train acc:  0.8828125
train loss:  0.3237282633781433
train gradient:  0.1361988227031115
iteration : 12586
train acc:  0.875
train loss:  0.321134090423584
train gradient:  0.10694821228006755
iteration : 12587
train acc:  0.84375
train loss:  0.37038424611091614
train gradient:  0.1884423217500861
iteration : 12588
train acc:  0.8046875
train loss:  0.4064522087574005
train gradient:  0.1974795039854501
iteration : 12589
train acc:  0.8984375
train loss:  0.2635611295700073
train gradient:  0.0897127335603402
iteration : 12590
train acc:  0.8828125
train loss:  0.2883327007293701
train gradient:  0.09960765643418372
iteration : 12591
train acc:  0.90625
train loss:  0.24591004848480225
train gradient:  0.08128512105019031
iteration : 12592
train acc:  0.84375
train loss:  0.3791751265525818
train gradient:  0.17656367219034386
iteration : 12593
train acc:  0.8203125
train loss:  0.33237504959106445
train gradient:  0.16847229563328964
iteration : 12594
train acc:  0.90625
train loss:  0.2458287626504898
train gradient:  0.11212398652135347
iteration : 12595
train acc:  0.8828125
train loss:  0.24557432532310486
train gradient:  0.09577297429190398
iteration : 12596
train acc:  0.8984375
train loss:  0.27660393714904785
train gradient:  0.11008976580027591
iteration : 12597
train acc:  0.78125
train loss:  0.417178750038147
train gradient:  0.16985666503891012
iteration : 12598
train acc:  0.875
train loss:  0.3313353359699249
train gradient:  0.13947189646787472
iteration : 12599
train acc:  0.8359375
train loss:  0.3488391935825348
train gradient:  0.16153666073011747
iteration : 12600
train acc:  0.8359375
train loss:  0.38476598262786865
train gradient:  0.18107419507910466
iteration : 12601
train acc:  0.8984375
train loss:  0.2593822181224823
train gradient:  0.12088654592134099
iteration : 12602
train acc:  0.828125
train loss:  0.3522406816482544
train gradient:  0.11403307837025715
iteration : 12603
train acc:  0.9296875
train loss:  0.2705919146537781
train gradient:  0.08442503081177866
iteration : 12604
train acc:  0.890625
train loss:  0.28058111667633057
train gradient:  0.1507382656602461
iteration : 12605
train acc:  0.859375
train loss:  0.2990110516548157
train gradient:  0.12099666027378606
iteration : 12606
train acc:  0.890625
train loss:  0.2729385197162628
train gradient:  0.20710452416339942
iteration : 12607
train acc:  0.859375
train loss:  0.3325580954551697
train gradient:  0.1609120956329631
iteration : 12608
train acc:  0.8984375
train loss:  0.3584727346897125
train gradient:  0.19170941741031172
iteration : 12609
train acc:  0.8671875
train loss:  0.3242495059967041
train gradient:  0.15102925869770062
iteration : 12610
train acc:  0.8515625
train loss:  0.2861325740814209
train gradient:  0.13939496035774246
iteration : 12611
train acc:  0.890625
train loss:  0.3255993127822876
train gradient:  0.1140916685018322
iteration : 12612
train acc:  0.921875
train loss:  0.28301557898521423
train gradient:  0.08210254033750992
iteration : 12613
train acc:  0.8671875
train loss:  0.3266955614089966
train gradient:  0.12847478229751427
iteration : 12614
train acc:  0.796875
train loss:  0.47885581851005554
train gradient:  0.24626339760337723
iteration : 12615
train acc:  0.7734375
train loss:  0.3878006339073181
train gradient:  0.15807131302016014
iteration : 12616
train acc:  0.828125
train loss:  0.3699546456336975
train gradient:  0.22804147635029237
iteration : 12617
train acc:  0.9375
train loss:  0.20359297096729279
train gradient:  0.08008065401902104
iteration : 12618
train acc:  0.8046875
train loss:  0.47707125544548035
train gradient:  0.2725134480569417
iteration : 12619
train acc:  0.8515625
train loss:  0.30479896068573
train gradient:  0.13114421880793686
iteration : 12620
train acc:  0.8984375
train loss:  0.269736647605896
train gradient:  0.08158128488077637
iteration : 12621
train acc:  0.875
train loss:  0.30471402406692505
train gradient:  0.1347860009453709
iteration : 12622
train acc:  0.828125
train loss:  0.36939239501953125
train gradient:  0.15713282095520276
iteration : 12623
train acc:  0.8359375
train loss:  0.3261168897151947
train gradient:  0.11604767222585793
iteration : 12624
train acc:  0.859375
train loss:  0.3879973888397217
train gradient:  0.16142352837553214
iteration : 12625
train acc:  0.8359375
train loss:  0.3768848180770874
train gradient:  0.13642781770534013
iteration : 12626
train acc:  0.890625
train loss:  0.24941809475421906
train gradient:  0.13639314669701275
iteration : 12627
train acc:  0.875
train loss:  0.30896836519241333
train gradient:  0.13396956228850543
iteration : 12628
train acc:  0.8984375
train loss:  0.2882802188396454
train gradient:  0.20328253974518484
iteration : 12629
train acc:  0.84375
train loss:  0.33079391717910767
train gradient:  0.12307420671873912
iteration : 12630
train acc:  0.8515625
train loss:  0.36919891834259033
train gradient:  0.1618131347112894
iteration : 12631
train acc:  0.7734375
train loss:  0.4269820749759674
train gradient:  0.2500860923313336
iteration : 12632
train acc:  0.828125
train loss:  0.33637478947639465
train gradient:  0.12774204240034598
iteration : 12633
train acc:  0.875
train loss:  0.2921993136405945
train gradient:  0.11762676923138839
iteration : 12634
train acc:  0.859375
train loss:  0.30094167590141296
train gradient:  0.09736728623375221
iteration : 12635
train acc:  0.78125
train loss:  0.4077986478805542
train gradient:  0.16003705353292388
iteration : 12636
train acc:  0.875
train loss:  0.2944267988204956
train gradient:  0.10447111111965449
iteration : 12637
train acc:  0.8671875
train loss:  0.2826605439186096
train gradient:  0.10957298807535687
iteration : 12638
train acc:  0.8984375
train loss:  0.3088405728340149
train gradient:  0.15638917184114
iteration : 12639
train acc:  0.8828125
train loss:  0.32033661007881165
train gradient:  0.10700149318025953
iteration : 12640
train acc:  0.859375
train loss:  0.30366307497024536
train gradient:  0.10030794656720207
iteration : 12641
train acc:  0.8984375
train loss:  0.26677265763282776
train gradient:  0.09726517463341153
iteration : 12642
train acc:  0.859375
train loss:  0.309331476688385
train gradient:  0.09131978760817891
iteration : 12643
train acc:  0.890625
train loss:  0.26562315225601196
train gradient:  0.09392243526942437
iteration : 12644
train acc:  0.828125
train loss:  0.345614492893219
train gradient:  0.15538891550772121
iteration : 12645
train acc:  0.8984375
train loss:  0.25746485590934753
train gradient:  0.07015758865629394
iteration : 12646
train acc:  0.875
train loss:  0.30107051134109497
train gradient:  0.13590210719246215
iteration : 12647
train acc:  0.890625
train loss:  0.2579232156276703
train gradient:  0.11587811639813846
iteration : 12648
train acc:  0.8671875
train loss:  0.3536641597747803
train gradient:  0.14093107339139777
iteration : 12649
train acc:  0.921875
train loss:  0.26795732975006104
train gradient:  0.08965160792413071
iteration : 12650
train acc:  0.859375
train loss:  0.3661150634288788
train gradient:  0.20608498816694898
iteration : 12651
train acc:  0.890625
train loss:  0.2538989782333374
train gradient:  0.08668451267946091
iteration : 12652
train acc:  0.8515625
train loss:  0.27640125155448914
train gradient:  0.12090260376222968
iteration : 12653
train acc:  0.8671875
train loss:  0.3228558599948883
train gradient:  0.1434724915653096
iteration : 12654
train acc:  0.8125
train loss:  0.32155856490135193
train gradient:  0.12250601677676305
iteration : 12655
train acc:  0.8671875
train loss:  0.38207870721817017
train gradient:  0.17383769072932348
iteration : 12656
train acc:  0.8203125
train loss:  0.38571232557296753
train gradient:  0.16698326331484425
iteration : 12657
train acc:  0.8359375
train loss:  0.37023311853408813
train gradient:  0.16255387156779755
iteration : 12658
train acc:  0.7890625
train loss:  0.40376603603363037
train gradient:  0.33669634757400424
iteration : 12659
train acc:  0.78125
train loss:  0.42947447299957275
train gradient:  0.29447650682186594
iteration : 12660
train acc:  0.8359375
train loss:  0.3316612243652344
train gradient:  0.13125014206393418
iteration : 12661
train acc:  0.84375
train loss:  0.32832831144332886
train gradient:  0.14276636773280005
iteration : 12662
train acc:  0.9375
train loss:  0.21442577242851257
train gradient:  0.07008124827780852
iteration : 12663
train acc:  0.8515625
train loss:  0.3018372058868408
train gradient:  0.10992733033197288
iteration : 12664
train acc:  0.8125
train loss:  0.3577495813369751
train gradient:  0.14199349556507718
iteration : 12665
train acc:  0.859375
train loss:  0.33750420808792114
train gradient:  0.1434231756973616
iteration : 12666
train acc:  0.8359375
train loss:  0.3422262966632843
train gradient:  0.14380590173017382
iteration : 12667
train acc:  0.84375
train loss:  0.29514387249946594
train gradient:  0.13948446697721661
iteration : 12668
train acc:  0.8671875
train loss:  0.34888890385627747
train gradient:  0.15100021031006128
iteration : 12669
train acc:  0.8671875
train loss:  0.318951815366745
train gradient:  0.17756636243714397
iteration : 12670
train acc:  0.828125
train loss:  0.3459969162940979
train gradient:  0.15828015272380833
iteration : 12671
train acc:  0.8515625
train loss:  0.35474473237991333
train gradient:  0.15770339737880767
iteration : 12672
train acc:  0.8671875
train loss:  0.28744012117385864
train gradient:  0.12320339118713561
iteration : 12673
train acc:  0.8984375
train loss:  0.22938930988311768
train gradient:  0.05826183471150012
iteration : 12674
train acc:  0.859375
train loss:  0.33590278029441833
train gradient:  0.1454681131809662
iteration : 12675
train acc:  0.8515625
train loss:  0.3126027286052704
train gradient:  0.1112800465280012
iteration : 12676
train acc:  0.8671875
train loss:  0.3129917085170746
train gradient:  0.15128323443068503
iteration : 12677
train acc:  0.8515625
train loss:  0.3947445750236511
train gradient:  0.25994760260990435
iteration : 12678
train acc:  0.84375
train loss:  0.3680179715156555
train gradient:  0.1644608548597899
iteration : 12679
train acc:  0.8125
train loss:  0.3777661919593811
train gradient:  0.12774696639266017
iteration : 12680
train acc:  0.859375
train loss:  0.3398256301879883
train gradient:  0.15915416951378383
iteration : 12681
train acc:  0.8828125
train loss:  0.28197669982910156
train gradient:  0.14068539179359274
iteration : 12682
train acc:  0.8515625
train loss:  0.31290990114212036
train gradient:  0.17265688536382767
iteration : 12683
train acc:  0.84375
train loss:  0.3809642195701599
train gradient:  0.25820536199074984
iteration : 12684
train acc:  0.8984375
train loss:  0.2642994821071625
train gradient:  0.12545531295834023
iteration : 12685
train acc:  0.8515625
train loss:  0.3515760898590088
train gradient:  0.14253574149117482
iteration : 12686
train acc:  0.890625
train loss:  0.31119829416275024
train gradient:  0.12073336738701651
iteration : 12687
train acc:  0.859375
train loss:  0.3338943421840668
train gradient:  0.17115861990119452
iteration : 12688
train acc:  0.8671875
train loss:  0.3026064336299896
train gradient:  0.1091032768245108
iteration : 12689
train acc:  0.890625
train loss:  0.3220694363117218
train gradient:  0.15448254165042846
iteration : 12690
train acc:  0.8671875
train loss:  0.3171018362045288
train gradient:  0.10621002098492002
iteration : 12691
train acc:  0.875
train loss:  0.3334386348724365
train gradient:  0.10960207346389224
iteration : 12692
train acc:  0.8046875
train loss:  0.40204694867134094
train gradient:  0.20915552276592958
iteration : 12693
train acc:  0.8671875
train loss:  0.30141204595565796
train gradient:  0.14409828399929822
iteration : 12694
train acc:  0.890625
train loss:  0.2848179042339325
train gradient:  0.10658573981601181
iteration : 12695
train acc:  0.828125
train loss:  0.33150315284729004
train gradient:  0.1365717614712081
iteration : 12696
train acc:  0.875
train loss:  0.3054516911506653
train gradient:  0.12139274041259081
iteration : 12697
train acc:  0.890625
train loss:  0.2542697787284851
train gradient:  0.12655646935213677
iteration : 12698
train acc:  0.859375
train loss:  0.3100074529647827
train gradient:  0.19077083894671742
iteration : 12699
train acc:  0.84375
train loss:  0.3756260275840759
train gradient:  0.1850246285891644
iteration : 12700
train acc:  0.828125
train loss:  0.39083462953567505
train gradient:  0.28198893093973154
iteration : 12701
train acc:  0.875
train loss:  0.29999569058418274
train gradient:  0.14362906819864457
iteration : 12702
train acc:  0.875
train loss:  0.305139422416687
train gradient:  0.1134841152891702
iteration : 12703
train acc:  0.8828125
train loss:  0.3015129864215851
train gradient:  0.08003002390791684
iteration : 12704
train acc:  0.828125
train loss:  0.362778902053833
train gradient:  0.1828865699206716
iteration : 12705
train acc:  0.859375
train loss:  0.319419801235199
train gradient:  0.09240232008951749
iteration : 12706
train acc:  0.8984375
train loss:  0.2951834797859192
train gradient:  0.12856601521784028
iteration : 12707
train acc:  0.875
train loss:  0.2634392976760864
train gradient:  0.08287989522816654
iteration : 12708
train acc:  0.859375
train loss:  0.37514567375183105
train gradient:  0.1662155119974496
iteration : 12709
train acc:  0.875
train loss:  0.29697185754776
train gradient:  0.13005517710706538
iteration : 12710
train acc:  0.875
train loss:  0.31720903515815735
train gradient:  0.14627091263747447
iteration : 12711
train acc:  0.8515625
train loss:  0.35039663314819336
train gradient:  0.23925208593953712
iteration : 12712
train acc:  0.90625
train loss:  0.22005456686019897
train gradient:  0.07060515041037964
iteration : 12713
train acc:  0.8828125
train loss:  0.27658283710479736
train gradient:  0.11939458881391518
iteration : 12714
train acc:  0.859375
train loss:  0.3100993037223816
train gradient:  0.17436030939793062
iteration : 12715
train acc:  0.8515625
train loss:  0.31598734855651855
train gradient:  0.10407089883359269
iteration : 12716
train acc:  0.8515625
train loss:  0.3561266362667084
train gradient:  0.16754416557204882
iteration : 12717
train acc:  0.875
train loss:  0.2790590226650238
train gradient:  0.11319005086222539
iteration : 12718
train acc:  0.828125
train loss:  0.33414387702941895
train gradient:  0.15081826600433526
iteration : 12719
train acc:  0.8984375
train loss:  0.2461278885602951
train gradient:  0.08875944642753339
iteration : 12720
train acc:  0.8359375
train loss:  0.34862834215164185
train gradient:  0.15474551194324068
iteration : 12721
train acc:  0.8671875
train loss:  0.28216278553009033
train gradient:  0.12494431760342165
iteration : 12722
train acc:  0.859375
train loss:  0.3858821988105774
train gradient:  0.1606851566814673
iteration : 12723
train acc:  0.8046875
train loss:  0.39452722668647766
train gradient:  0.20627555123355773
iteration : 12724
train acc:  0.8359375
train loss:  0.35551249980926514
train gradient:  0.17926805310681712
iteration : 12725
train acc:  0.796875
train loss:  0.38450872898101807
train gradient:  0.28593464689090264
iteration : 12726
train acc:  0.8828125
train loss:  0.3232235610485077
train gradient:  0.21296462988870635
iteration : 12727
train acc:  0.90625
train loss:  0.24312278628349304
train gradient:  0.08856520818111983
iteration : 12728
train acc:  0.8125
train loss:  0.3633398413658142
train gradient:  0.18936039096907362
iteration : 12729
train acc:  0.875
train loss:  0.26848655939102173
train gradient:  0.11831105771514586
iteration : 12730
train acc:  0.921875
train loss:  0.21891158819198608
train gradient:  0.07610728763238733
iteration : 12731
train acc:  0.8359375
train loss:  0.3642313480377197
train gradient:  0.18890955176532062
iteration : 12732
train acc:  0.8828125
train loss:  0.2827233076095581
train gradient:  0.13069266828769607
iteration : 12733
train acc:  0.859375
train loss:  0.3981701731681824
train gradient:  0.16324941480017663
iteration : 12734
train acc:  0.8828125
train loss:  0.2906697392463684
train gradient:  0.08280999076711201
iteration : 12735
train acc:  0.8671875
train loss:  0.36719563603401184
train gradient:  0.203754627366943
iteration : 12736
train acc:  0.8671875
train loss:  0.29522523283958435
train gradient:  0.09259083353917043
iteration : 12737
train acc:  0.8984375
train loss:  0.31985044479370117
train gradient:  0.15032964975071333
iteration : 12738
train acc:  0.875
train loss:  0.2798781991004944
train gradient:  0.10719249898558834
iteration : 12739
train acc:  0.84375
train loss:  0.2878043055534363
train gradient:  0.10449455875762695
iteration : 12740
train acc:  0.90625
train loss:  0.235927551984787
train gradient:  0.1134626547470943
iteration : 12741
train acc:  0.8984375
train loss:  0.26816678047180176
train gradient:  0.10996909346678277
iteration : 12742
train acc:  0.8515625
train loss:  0.3094603419303894
train gradient:  0.1785540744828375
iteration : 12743
train acc:  0.875
train loss:  0.2763887941837311
train gradient:  0.13279864392779583
iteration : 12744
train acc:  0.859375
train loss:  0.3049287497997284
train gradient:  0.14356790255097157
iteration : 12745
train acc:  0.890625
train loss:  0.27649712562561035
train gradient:  0.10908120370593503
iteration : 12746
train acc:  0.828125
train loss:  0.41302263736724854
train gradient:  0.17762605286877103
iteration : 12747
train acc:  0.8828125
train loss:  0.24530284106731415
train gradient:  0.10185875935933124
iteration : 12748
train acc:  0.890625
train loss:  0.2503397762775421
train gradient:  0.1342924738365503
iteration : 12749
train acc:  0.84375
train loss:  0.2992458939552307
train gradient:  0.12641097765125536
iteration : 12750
train acc:  0.875
train loss:  0.3500176668167114
train gradient:  0.13581972954168178
iteration : 12751
train acc:  0.796875
train loss:  0.4155256152153015
train gradient:  0.19583495355563885
iteration : 12752
train acc:  0.828125
train loss:  0.33852875232696533
train gradient:  0.118011223647076
iteration : 12753
train acc:  0.8515625
train loss:  0.28562813997268677
train gradient:  0.15523120152231606
iteration : 12754
train acc:  0.8046875
train loss:  0.37271398305892944
train gradient:  0.20324048387398647
iteration : 12755
train acc:  0.84375
train loss:  0.35620665550231934
train gradient:  0.22746178865562228
iteration : 12756
train acc:  0.828125
train loss:  0.35489803552627563
train gradient:  0.12368178817221537
iteration : 12757
train acc:  0.859375
train loss:  0.30171313881874084
train gradient:  0.12398214592291845
iteration : 12758
train acc:  0.8125
train loss:  0.40678292512893677
train gradient:  0.19592724144099852
iteration : 12759
train acc:  0.8359375
train loss:  0.380319744348526
train gradient:  0.14514585606462682
iteration : 12760
train acc:  0.8828125
train loss:  0.2621999979019165
train gradient:  0.11598033282584488
iteration : 12761
train acc:  0.90625
train loss:  0.2826317846775055
train gradient:  0.07034251990136267
iteration : 12762
train acc:  0.890625
train loss:  0.27956897020339966
train gradient:  0.12794144988249379
iteration : 12763
train acc:  0.875
train loss:  0.29983043670654297
train gradient:  0.12845442226755718
iteration : 12764
train acc:  0.9140625
train loss:  0.20373713970184326
train gradient:  0.06529358708140988
iteration : 12765
train acc:  0.8828125
train loss:  0.2886907458305359
train gradient:  0.10305531095183079
iteration : 12766
train acc:  0.8515625
train loss:  0.350616991519928
train gradient:  0.13647926370523628
iteration : 12767
train acc:  0.859375
train loss:  0.3042340874671936
train gradient:  0.12540261298241512
iteration : 12768
train acc:  0.8828125
train loss:  0.27459415793418884
train gradient:  0.08657519880934257
iteration : 12769
train acc:  0.8984375
train loss:  0.25674915313720703
train gradient:  0.08749965665156603
iteration : 12770
train acc:  0.8515625
train loss:  0.3900129795074463
train gradient:  0.1652924111114648
iteration : 12771
train acc:  0.8828125
train loss:  0.24105669558048248
train gradient:  0.09451882409985989
iteration : 12772
train acc:  0.875
train loss:  0.36651313304901123
train gradient:  0.12483118924733952
iteration : 12773
train acc:  0.8203125
train loss:  0.4268725514411926
train gradient:  0.26066987688970494
iteration : 12774
train acc:  0.890625
train loss:  0.2777542471885681
train gradient:  0.08933634506056255
iteration : 12775
train acc:  0.8515625
train loss:  0.3268761932849884
train gradient:  0.11071653680531757
iteration : 12776
train acc:  0.8515625
train loss:  0.27996572852134705
train gradient:  0.10346447995870257
iteration : 12777
train acc:  0.8828125
train loss:  0.26764345169067383
train gradient:  0.06587465007410143
iteration : 12778
train acc:  0.8671875
train loss:  0.33527374267578125
train gradient:  0.19114914799348504
iteration : 12779
train acc:  0.8515625
train loss:  0.3555011451244354
train gradient:  0.13949044254293191
iteration : 12780
train acc:  0.859375
train loss:  0.3086267113685608
train gradient:  0.09625752617733667
iteration : 12781
train acc:  0.90625
train loss:  0.2791271209716797
train gradient:  0.1039866129911361
iteration : 12782
train acc:  0.8203125
train loss:  0.34218889474868774
train gradient:  0.15384057206146443
iteration : 12783
train acc:  0.8671875
train loss:  0.3103832006454468
train gradient:  0.11294718854919032
iteration : 12784
train acc:  0.828125
train loss:  0.3555418848991394
train gradient:  0.17388660946560291
iteration : 12785
train acc:  0.8359375
train loss:  0.38845735788345337
train gradient:  0.19599968174347754
iteration : 12786
train acc:  0.828125
train loss:  0.3226510286331177
train gradient:  0.1574512495712011
iteration : 12787
train acc:  0.875
train loss:  0.27412301301956177
train gradient:  0.09462701853724982
iteration : 12788
train acc:  0.84375
train loss:  0.30737632513046265
train gradient:  0.10683848169040043
iteration : 12789
train acc:  0.84375
train loss:  0.32093554735183716
train gradient:  0.11332238574826808
iteration : 12790
train acc:  0.90625
train loss:  0.2455715537071228
train gradient:  0.06908957652513721
iteration : 12791
train acc:  0.859375
train loss:  0.3390148878097534
train gradient:  0.12692534987940934
iteration : 12792
train acc:  0.890625
train loss:  0.2538329064846039
train gradient:  0.11480076947854102
iteration : 12793
train acc:  0.921875
train loss:  0.25623148679733276
train gradient:  0.08571457646897811
iteration : 12794
train acc:  0.8359375
train loss:  0.3587227165699005
train gradient:  0.09817591895217945
iteration : 12795
train acc:  0.875
train loss:  0.325996071100235
train gradient:  0.11281409155417438
iteration : 12796
train acc:  0.8125
train loss:  0.317260205745697
train gradient:  0.10592143813072347
iteration : 12797
train acc:  0.9140625
train loss:  0.24046969413757324
train gradient:  0.07050446869621414
iteration : 12798
train acc:  0.8671875
train loss:  0.25859078764915466
train gradient:  0.09325755602176736
iteration : 12799
train acc:  0.84375
train loss:  0.34545913338661194
train gradient:  0.1439620681735838
iteration : 12800
train acc:  0.8671875
train loss:  0.29260629415512085
train gradient:  0.11955891404700027
iteration : 12801
train acc:  0.828125
train loss:  0.3725692927837372
train gradient:  0.1496994750539455
iteration : 12802
train acc:  0.9375
train loss:  0.21522772312164307
train gradient:  0.07302154717744144
iteration : 12803
train acc:  0.875
train loss:  0.2499929666519165
train gradient:  0.08173744634217567
iteration : 12804
train acc:  0.84375
train loss:  0.331296443939209
train gradient:  0.22450742042949656
iteration : 12805
train acc:  0.8515625
train loss:  0.3749571442604065
train gradient:  0.15041238810848923
iteration : 12806
train acc:  0.8671875
train loss:  0.3467485308647156
train gradient:  0.12317598439945299
iteration : 12807
train acc:  0.8515625
train loss:  0.3551901876926422
train gradient:  0.16591167317383737
iteration : 12808
train acc:  0.875
train loss:  0.266899436712265
train gradient:  0.09526077069574122
iteration : 12809
train acc:  0.84375
train loss:  0.38457921147346497
train gradient:  0.1832063678671083
iteration : 12810
train acc:  0.875
train loss:  0.265499472618103
train gradient:  0.10061058915045877
iteration : 12811
train acc:  0.8828125
train loss:  0.3424364924430847
train gradient:  0.15482779816120612
iteration : 12812
train acc:  0.8046875
train loss:  0.4201129078865051
train gradient:  0.1583970289192862
iteration : 12813
train acc:  0.859375
train loss:  0.3227457106113434
train gradient:  0.14215417332544522
iteration : 12814
train acc:  0.84375
train loss:  0.32488417625427246
train gradient:  0.11867496673035606
iteration : 12815
train acc:  0.78125
train loss:  0.41107937693595886
train gradient:  0.2022862404884886
iteration : 12816
train acc:  0.875
train loss:  0.2513102889060974
train gradient:  0.0910967536696803
iteration : 12817
train acc:  0.8515625
train loss:  0.3202569782733917
train gradient:  0.16906715623377777
iteration : 12818
train acc:  0.8671875
train loss:  0.2600570321083069
train gradient:  0.19050127071404943
iteration : 12819
train acc:  0.8046875
train loss:  0.388283371925354
train gradient:  0.21308174714596126
iteration : 12820
train acc:  0.828125
train loss:  0.33108440041542053
train gradient:  0.1231508607086617
iteration : 12821
train acc:  0.90625
train loss:  0.2474168986082077
train gradient:  0.07307195950282686
iteration : 12822
train acc:  0.875
train loss:  0.3647010922431946
train gradient:  0.13288708814900607
iteration : 12823
train acc:  0.8984375
train loss:  0.33120307326316833
train gradient:  0.14392194429403934
iteration : 12824
train acc:  0.859375
train loss:  0.31342148780822754
train gradient:  0.15054235569383034
iteration : 12825
train acc:  0.890625
train loss:  0.26170992851257324
train gradient:  0.10592384604082741
iteration : 12826
train acc:  0.8515625
train loss:  0.36771804094314575
train gradient:  0.15040520354411605
iteration : 12827
train acc:  0.921875
train loss:  0.2194666713476181
train gradient:  0.10172433635335822
iteration : 12828
train acc:  0.828125
train loss:  0.4467877149581909
train gradient:  0.21407288582794998
iteration : 12829
train acc:  0.8515625
train loss:  0.3375222980976105
train gradient:  0.13488495746092638
iteration : 12830
train acc:  0.8671875
train loss:  0.28622376918792725
train gradient:  0.10467387262357201
iteration : 12831
train acc:  0.8515625
train loss:  0.36400967836380005
train gradient:  0.18013207824785843
iteration : 12832
train acc:  0.890625
train loss:  0.28210607171058655
train gradient:  0.09284025805719126
iteration : 12833
train acc:  0.90625
train loss:  0.27418017387390137
train gradient:  0.12830665806182706
iteration : 12834
train acc:  0.8515625
train loss:  0.31979724764823914
train gradient:  0.12014736131351794
iteration : 12835
train acc:  0.890625
train loss:  0.24107491970062256
train gradient:  0.0829672423235704
iteration : 12836
train acc:  0.8203125
train loss:  0.40343552827835083
train gradient:  0.15544453370703798
iteration : 12837
train acc:  0.8125
train loss:  0.36296993494033813
train gradient:  0.16143049706912427
iteration : 12838
train acc:  0.8828125
train loss:  0.2714431881904602
train gradient:  0.1033927148041335
iteration : 12839
train acc:  0.921875
train loss:  0.27752166986465454
train gradient:  0.07261466266831697
iteration : 12840
train acc:  0.875
train loss:  0.34801697731018066
train gradient:  0.13811515366800553
iteration : 12841
train acc:  0.8828125
train loss:  0.3126070201396942
train gradient:  0.13017935351758886
iteration : 12842
train acc:  0.8671875
train loss:  0.34911656379699707
train gradient:  0.14272518856227875
iteration : 12843
train acc:  0.8046875
train loss:  0.43655699491500854
train gradient:  0.24888917688819712
iteration : 12844
train acc:  0.78125
train loss:  0.42675936222076416
train gradient:  0.3086520677158014
iteration : 12845
train acc:  0.8671875
train loss:  0.3133518099784851
train gradient:  0.09974340750093304
iteration : 12846
train acc:  0.875
train loss:  0.2714228630065918
train gradient:  0.11703921522034536
iteration : 12847
train acc:  0.875
train loss:  0.29615485668182373
train gradient:  0.10848631103511736
iteration : 12848
train acc:  0.875
train loss:  0.24136868119239807
train gradient:  0.10577412663750647
iteration : 12849
train acc:  0.84375
train loss:  0.3333449363708496
train gradient:  0.13868317042986253
iteration : 12850
train acc:  0.828125
train loss:  0.40566426515579224
train gradient:  0.2292067381940373
iteration : 12851
train acc:  0.859375
train loss:  0.3277551829814911
train gradient:  0.176090010183115
iteration : 12852
train acc:  0.8203125
train loss:  0.386676549911499
train gradient:  0.21871786560864162
iteration : 12853
train acc:  0.7890625
train loss:  0.4316837191581726
train gradient:  0.28647423586285115
iteration : 12854
train acc:  0.875
train loss:  0.34806889295578003
train gradient:  0.14794786483090777
iteration : 12855
train acc:  0.8359375
train loss:  0.44481512904167175
train gradient:  0.23824770781626106
iteration : 12856
train acc:  0.8671875
train loss:  0.32780882716178894
train gradient:  0.1557570777536467
iteration : 12857
train acc:  0.890625
train loss:  0.3064815402030945
train gradient:  0.1351717723891257
iteration : 12858
train acc:  0.875
train loss:  0.2686399519443512
train gradient:  0.09886858709468156
iteration : 12859
train acc:  0.875
train loss:  0.29457342624664307
train gradient:  0.10624585578094044
iteration : 12860
train acc:  0.84375
train loss:  0.298168420791626
train gradient:  0.13053640113958448
iteration : 12861
train acc:  0.8203125
train loss:  0.3815167546272278
train gradient:  0.15480798052165895
iteration : 12862
train acc:  0.9296875
train loss:  0.2061220407485962
train gradient:  0.08285833215320884
iteration : 12863
train acc:  0.8828125
train loss:  0.28477394580841064
train gradient:  0.13793332259740265
iteration : 12864
train acc:  0.84375
train loss:  0.342326283454895
train gradient:  0.23910710129906565
iteration : 12865
train acc:  0.8984375
train loss:  0.26380857825279236
train gradient:  0.10145841685106982
iteration : 12866
train acc:  0.890625
train loss:  0.24596810340881348
train gradient:  0.09611535243922678
iteration : 12867
train acc:  0.859375
train loss:  0.3119902014732361
train gradient:  0.11132552112028461
iteration : 12868
train acc:  0.8203125
train loss:  0.39276817440986633
train gradient:  0.15306139375029146
iteration : 12869
train acc:  0.8359375
train loss:  0.35561156272888184
train gradient:  0.10786065375765624
iteration : 12870
train acc:  0.9140625
train loss:  0.2553708553314209
train gradient:  0.09966169103746007
iteration : 12871
train acc:  0.828125
train loss:  0.34748542308807373
train gradient:  0.11033578422709665
iteration : 12872
train acc:  0.859375
train loss:  0.32152259349823
train gradient:  0.14694284239476507
iteration : 12873
train acc:  0.84375
train loss:  0.40371644496917725
train gradient:  0.2300437130294638
iteration : 12874
train acc:  0.8828125
train loss:  0.332938551902771
train gradient:  0.14651207057722526
iteration : 12875
train acc:  0.8671875
train loss:  0.3225066065788269
train gradient:  0.17963768594718071
iteration : 12876
train acc:  0.8203125
train loss:  0.3644382655620575
train gradient:  0.17854112001074735
iteration : 12877
train acc:  0.8515625
train loss:  0.36674225330352783
train gradient:  0.1551324128978992
iteration : 12878
train acc:  0.859375
train loss:  0.32185038924217224
train gradient:  0.10641454992353157
iteration : 12879
train acc:  0.8203125
train loss:  0.3518425524234772
train gradient:  0.17295889516747548
iteration : 12880
train acc:  0.84375
train loss:  0.3469507098197937
train gradient:  0.1427757882259183
iteration : 12881
train acc:  0.8359375
train loss:  0.3379627764225006
train gradient:  0.11207245308758366
iteration : 12882
train acc:  0.859375
train loss:  0.35795751214027405
train gradient:  0.18047958809249137
iteration : 12883
train acc:  0.890625
train loss:  0.2746330797672272
train gradient:  0.12453138828673392
iteration : 12884
train acc:  0.8203125
train loss:  0.31934818625450134
train gradient:  0.14487357138716084
iteration : 12885
train acc:  0.890625
train loss:  0.2928237020969391
train gradient:  0.11985842395731328
iteration : 12886
train acc:  0.90625
train loss:  0.2352488487958908
train gradient:  0.08810537237831784
iteration : 12887
train acc:  0.859375
train loss:  0.25762248039245605
train gradient:  0.08670981037879257
iteration : 12888
train acc:  0.859375
train loss:  0.28312918543815613
train gradient:  0.10861290883338881
iteration : 12889
train acc:  0.859375
train loss:  0.3318312466144562
train gradient:  0.11906586521733914
iteration : 12890
train acc:  0.8359375
train loss:  0.37494727969169617
train gradient:  0.15465426400512755
iteration : 12891
train acc:  0.84375
train loss:  0.3107585608959198
train gradient:  0.2275263737394556
iteration : 12892
train acc:  0.8828125
train loss:  0.30174708366394043
train gradient:  0.12594294321164415
iteration : 12893
train acc:  0.8515625
train loss:  0.3171197175979614
train gradient:  0.1528734832763406
iteration : 12894
train acc:  0.8203125
train loss:  0.34605124592781067
train gradient:  0.151004485154596
iteration : 12895
train acc:  0.8046875
train loss:  0.3438640832901001
train gradient:  0.18285645372157003
iteration : 12896
train acc:  0.8984375
train loss:  0.2667820453643799
train gradient:  0.11796804943753206
iteration : 12897
train acc:  0.8359375
train loss:  0.36182090640068054
train gradient:  0.1696465218479884
iteration : 12898
train acc:  0.8515625
train loss:  0.32272857427597046
train gradient:  0.11562546995985441
iteration : 12899
train acc:  0.859375
train loss:  0.3238428235054016
train gradient:  0.1254591787597984
iteration : 12900
train acc:  0.8671875
train loss:  0.28512808680534363
train gradient:  0.0978742261990691
iteration : 12901
train acc:  0.890625
train loss:  0.2398776412010193
train gradient:  0.12457017064657622
iteration : 12902
train acc:  0.78125
train loss:  0.39969027042388916
train gradient:  0.2738757602615431
iteration : 12903
train acc:  0.9140625
train loss:  0.25301602482795715
train gradient:  0.10412789342945868
iteration : 12904
train acc:  0.8984375
train loss:  0.26189523935317993
train gradient:  0.07991289797698274
iteration : 12905
train acc:  0.8359375
train loss:  0.3396434783935547
train gradient:  0.15715809177887752
iteration : 12906
train acc:  0.8515625
train loss:  0.3358865976333618
train gradient:  0.15084598150878742
iteration : 12907
train acc:  0.828125
train loss:  0.3427359163761139
train gradient:  0.14360870935216846
iteration : 12908
train acc:  0.8515625
train loss:  0.33919644355773926
train gradient:  0.164792140377495
iteration : 12909
train acc:  0.8671875
train loss:  0.2928732633590698
train gradient:  0.10177295546712858
iteration : 12910
train acc:  0.859375
train loss:  0.30030733346939087
train gradient:  0.1340662031211629
iteration : 12911
train acc:  0.8671875
train loss:  0.26019203662872314
train gradient:  0.09428733753343813
iteration : 12912
train acc:  0.890625
train loss:  0.31966593861579895
train gradient:  0.1163000724780431
iteration : 12913
train acc:  0.859375
train loss:  0.33469104766845703
train gradient:  0.2284910900954794
iteration : 12914
train acc:  0.84375
train loss:  0.3953353464603424
train gradient:  0.1694133055101667
iteration : 12915
train acc:  0.859375
train loss:  0.39893656969070435
train gradient:  0.191194387399773
iteration : 12916
train acc:  0.921875
train loss:  0.2400820255279541
train gradient:  0.0637953279389253
iteration : 12917
train acc:  0.8984375
train loss:  0.3177298605442047
train gradient:  0.10169910178260913
iteration : 12918
train acc:  0.8828125
train loss:  0.2795029878616333
train gradient:  0.19021093940222328
iteration : 12919
train acc:  0.90625
train loss:  0.2551511526107788
train gradient:  0.11271894574403267
iteration : 12920
train acc:  0.8515625
train loss:  0.27528804540634155
train gradient:  0.15867949437960785
iteration : 12921
train acc:  0.8046875
train loss:  0.2934505045413971
train gradient:  0.11109272029661982
iteration : 12922
train acc:  0.8984375
train loss:  0.26759907603263855
train gradient:  0.07924904962226091
iteration : 12923
train acc:  0.9296875
train loss:  0.2506657838821411
train gradient:  0.10014415181402561
iteration : 12924
train acc:  0.8671875
train loss:  0.30282899737358093
train gradient:  0.13515504808632567
iteration : 12925
train acc:  0.84375
train loss:  0.36503180861473083
train gradient:  0.2211655345892811
iteration : 12926
train acc:  0.9140625
train loss:  0.26973193883895874
train gradient:  0.08958980299949484
iteration : 12927
train acc:  0.8515625
train loss:  0.30209600925445557
train gradient:  0.13598636590010804
iteration : 12928
train acc:  0.8984375
train loss:  0.2697521150112152
train gradient:  0.11518227897355442
iteration : 12929
train acc:  0.8828125
train loss:  0.2599610388278961
train gradient:  0.09422236849248787
iteration : 12930
train acc:  0.8125
train loss:  0.38399267196655273
train gradient:  0.15634581820043558
iteration : 12931
train acc:  0.890625
train loss:  0.2370658665895462
train gradient:  0.1563531665112646
iteration : 12932
train acc:  0.90625
train loss:  0.20005524158477783
train gradient:  0.10041737881413579
iteration : 12933
train acc:  0.8671875
train loss:  0.3201163411140442
train gradient:  0.14850676921675593
iteration : 12934
train acc:  0.8828125
train loss:  0.2956964075565338
train gradient:  0.1091862408133359
iteration : 12935
train acc:  0.890625
train loss:  0.31530827283859253
train gradient:  0.1357249916690399
iteration : 12936
train acc:  0.8671875
train loss:  0.31928950548171997
train gradient:  0.17292094694743354
iteration : 12937
train acc:  0.90625
train loss:  0.2227012664079666
train gradient:  0.07040894785197947
iteration : 12938
train acc:  0.84375
train loss:  0.33965861797332764
train gradient:  0.13186744797245997
iteration : 12939
train acc:  0.8046875
train loss:  0.42638522386550903
train gradient:  0.19964892094813497
iteration : 12940
train acc:  0.890625
train loss:  0.22854813933372498
train gradient:  0.1537764642467352
iteration : 12941
train acc:  0.9296875
train loss:  0.21801531314849854
train gradient:  0.07629186624063133
iteration : 12942
train acc:  0.890625
train loss:  0.26378393173217773
train gradient:  0.1001192527388834
iteration : 12943
train acc:  0.8671875
train loss:  0.3237948417663574
train gradient:  0.10454781677513195
iteration : 12944
train acc:  0.8515625
train loss:  0.34794527292251587
train gradient:  0.21667395633107048
iteration : 12945
train acc:  0.875
train loss:  0.280536949634552
train gradient:  0.15112635217024542
iteration : 12946
train acc:  0.875
train loss:  0.26704633235931396
train gradient:  0.19904269367560917
iteration : 12947
train acc:  0.8515625
train loss:  0.31514406204223633
train gradient:  0.1046154000085518
iteration : 12948
train acc:  0.8671875
train loss:  0.3920208215713501
train gradient:  0.1613319020583464
iteration : 12949
train acc:  0.8828125
train loss:  0.3027145266532898
train gradient:  0.1299999688102363
iteration : 12950
train acc:  0.8515625
train loss:  0.3429313004016876
train gradient:  0.23744364414929503
iteration : 12951
train acc:  0.859375
train loss:  0.345977246761322
train gradient:  0.16106230274424022
iteration : 12952
train acc:  0.8515625
train loss:  0.3668868839740753
train gradient:  0.19261510925135764
iteration : 12953
train acc:  0.875
train loss:  0.27078139781951904
train gradient:  0.10777823397306471
iteration : 12954
train acc:  0.890625
train loss:  0.28449326753616333
train gradient:  0.12125191823963385
iteration : 12955
train acc:  0.8828125
train loss:  0.30322888493537903
train gradient:  0.13145439079216736
iteration : 12956
train acc:  0.90625
train loss:  0.2717953622341156
train gradient:  0.13236592691113402
iteration : 12957
train acc:  0.7734375
train loss:  0.3549467921257019
train gradient:  0.13821652974761733
iteration : 12958
train acc:  0.8515625
train loss:  0.36378419399261475
train gradient:  0.1565602448872694
iteration : 12959
train acc:  0.90625
train loss:  0.29352134466171265
train gradient:  0.16876996257179105
iteration : 12960
train acc:  0.859375
train loss:  0.33363771438598633
train gradient:  0.18125940085573783
iteration : 12961
train acc:  0.8515625
train loss:  0.2777087688446045
train gradient:  0.13872860543192875
iteration : 12962
train acc:  0.8671875
train loss:  0.2935953736305237
train gradient:  0.11372377283053238
iteration : 12963
train acc:  0.84375
train loss:  0.3471662998199463
train gradient:  0.13167919416549945
iteration : 12964
train acc:  0.8828125
train loss:  0.3142108619213104
train gradient:  0.13478384067900337
iteration : 12965
train acc:  0.84375
train loss:  0.39005380868911743
train gradient:  0.25230886497030436
iteration : 12966
train acc:  0.8671875
train loss:  0.3887988328933716
train gradient:  0.1870192266101204
iteration : 12967
train acc:  0.875
train loss:  0.2885087728500366
train gradient:  0.13061776436540307
iteration : 12968
train acc:  0.875
train loss:  0.3443514108657837
train gradient:  0.21826650373905698
iteration : 12969
train acc:  0.8359375
train loss:  0.36761656403541565
train gradient:  0.11837269089423583
iteration : 12970
train acc:  0.8203125
train loss:  0.389090359210968
train gradient:  0.19418329814817684
iteration : 12971
train acc:  0.8515625
train loss:  0.32931387424468994
train gradient:  0.2244096386856304
iteration : 12972
train acc:  0.84375
train loss:  0.3607434630393982
train gradient:  0.15689572877240798
iteration : 12973
train acc:  0.890625
train loss:  0.32168638706207275
train gradient:  0.13012773692232346
iteration : 12974
train acc:  0.8671875
train loss:  0.3202761113643646
train gradient:  0.11968017614330995
iteration : 12975
train acc:  0.8828125
train loss:  0.3100517988204956
train gradient:  0.111143804296995
iteration : 12976
train acc:  0.8046875
train loss:  0.41480231285095215
train gradient:  0.22821884228037048
iteration : 12977
train acc:  0.828125
train loss:  0.39524856209754944
train gradient:  0.16558239687544113
iteration : 12978
train acc:  0.859375
train loss:  0.29946717619895935
train gradient:  0.12273715868986171
iteration : 12979
train acc:  0.8515625
train loss:  0.3719135522842407
train gradient:  0.1881806023495585
iteration : 12980
train acc:  0.8671875
train loss:  0.2893032133579254
train gradient:  0.1289159194418757
iteration : 12981
train acc:  0.890625
train loss:  0.26906073093414307
train gradient:  0.09819013345023637
iteration : 12982
train acc:  0.8671875
train loss:  0.3198409676551819
train gradient:  0.12437149538211813
iteration : 12983
train acc:  0.859375
train loss:  0.30915966629981995
train gradient:  0.11822055904313046
iteration : 12984
train acc:  0.8515625
train loss:  0.33397114276885986
train gradient:  0.14203010113418058
iteration : 12985
train acc:  0.828125
train loss:  0.34729665517807007
train gradient:  0.17751586558555815
iteration : 12986
train acc:  0.8828125
train loss:  0.3404107093811035
train gradient:  0.15025179581986312
iteration : 12987
train acc:  0.875
train loss:  0.2827283442020416
train gradient:  0.1217003816636934
iteration : 12988
train acc:  0.875
train loss:  0.29295146465301514
train gradient:  0.12014325674724508
iteration : 12989
train acc:  0.84375
train loss:  0.3272116184234619
train gradient:  0.15808899034524493
iteration : 12990
train acc:  0.90625
train loss:  0.27567237615585327
train gradient:  0.1062346477083616
iteration : 12991
train acc:  0.875
train loss:  0.30492228269577026
train gradient:  0.09089998846092548
iteration : 12992
train acc:  0.8515625
train loss:  0.2883596420288086
train gradient:  0.08160522087156755
iteration : 12993
train acc:  0.8359375
train loss:  0.36646568775177
train gradient:  0.17790349196546487
iteration : 12994
train acc:  0.9296875
train loss:  0.2562749981880188
train gradient:  0.0929476427371525
iteration : 12995
train acc:  0.8125
train loss:  0.43866482377052307
train gradient:  0.18582420037389674
iteration : 12996
train acc:  0.875
train loss:  0.33677512407302856
train gradient:  0.12217470133469766
iteration : 12997
train acc:  0.8828125
train loss:  0.3004070818424225
train gradient:  0.12678461195372295
iteration : 12998
train acc:  0.859375
train loss:  0.3080633282661438
train gradient:  0.13060392841356025
iteration : 12999
train acc:  0.9140625
train loss:  0.2750898003578186
train gradient:  0.08122601311008064
iteration : 13000
train acc:  0.9140625
train loss:  0.2680746614933014
train gradient:  0.07775202924603135
iteration : 13001
train acc:  0.875
train loss:  0.2712429463863373
train gradient:  0.08780693203533646
iteration : 13002
train acc:  0.796875
train loss:  0.4274590313434601
train gradient:  0.19245942898560686
iteration : 13003
train acc:  0.8671875
train loss:  0.304817259311676
train gradient:  0.11736645256291327
iteration : 13004
train acc:  0.8359375
train loss:  0.3188733160495758
train gradient:  0.12410558201090127
iteration : 13005
train acc:  0.84375
train loss:  0.30048173666000366
train gradient:  0.13339765959306732
iteration : 13006
train acc:  0.84375
train loss:  0.29256176948547363
train gradient:  0.0991572868477286
iteration : 13007
train acc:  0.8984375
train loss:  0.2779131531715393
train gradient:  0.08583533931936244
iteration : 13008
train acc:  0.890625
train loss:  0.27962160110473633
train gradient:  0.10481334821336631
iteration : 13009
train acc:  0.875
train loss:  0.30409932136535645
train gradient:  0.11701004661103075
iteration : 13010
train acc:  0.859375
train loss:  0.29885590076446533
train gradient:  0.12496074786777088
iteration : 13011
train acc:  0.8984375
train loss:  0.3185610771179199
train gradient:  0.12913614368802318
iteration : 13012
train acc:  0.8671875
train loss:  0.3458482623100281
train gradient:  0.13940496278161785
iteration : 13013
train acc:  0.8671875
train loss:  0.2973976731300354
train gradient:  0.13029396080274758
iteration : 13014
train acc:  0.8828125
train loss:  0.2802676260471344
train gradient:  0.09402728255270419
iteration : 13015
train acc:  0.8984375
train loss:  0.30407118797302246
train gradient:  0.14201595604066303
iteration : 13016
train acc:  0.8359375
train loss:  0.3673734664916992
train gradient:  0.17278205546189823
iteration : 13017
train acc:  0.828125
train loss:  0.33928853273391724
train gradient:  0.1276499179222624
iteration : 13018
train acc:  0.8671875
train loss:  0.30781084299087524
train gradient:  0.10200084793452595
iteration : 13019
train acc:  0.8359375
train loss:  0.3772779703140259
train gradient:  0.1973354045476271
iteration : 13020
train acc:  0.8671875
train loss:  0.32261061668395996
train gradient:  0.11496820528885222
iteration : 13021
train acc:  0.84375
train loss:  0.32543736696243286
train gradient:  0.1601360865871282
iteration : 13022
train acc:  0.90625
train loss:  0.2485182136297226
train gradient:  0.12736506577944884
iteration : 13023
train acc:  0.875
train loss:  0.2663114666938782
train gradient:  0.10029962989196202
iteration : 13024
train acc:  0.90625
train loss:  0.2688875198364258
train gradient:  0.08993598968193445
iteration : 13025
train acc:  0.84375
train loss:  0.3925265669822693
train gradient:  0.1770203208237755
iteration : 13026
train acc:  0.859375
train loss:  0.3325331211090088
train gradient:  0.14364188324122618
iteration : 13027
train acc:  0.8359375
train loss:  0.34426063299179077
train gradient:  0.1594206795246507
iteration : 13028
train acc:  0.8671875
train loss:  0.2654772102832794
train gradient:  0.10152950581163311
iteration : 13029
train acc:  0.84375
train loss:  0.302176833152771
train gradient:  0.10099295248135147
iteration : 13030
train acc:  0.8046875
train loss:  0.37866681814193726
train gradient:  0.14175562661618618
iteration : 13031
train acc:  0.8671875
train loss:  0.3023790717124939
train gradient:  0.13459120859539025
iteration : 13032
train acc:  0.8359375
train loss:  0.37241700291633606
train gradient:  0.19760765380933487
iteration : 13033
train acc:  0.8671875
train loss:  0.295895516872406
train gradient:  0.1131602486014932
iteration : 13034
train acc:  0.90625
train loss:  0.24241167306900024
train gradient:  0.09140339368883889
iteration : 13035
train acc:  0.8671875
train loss:  0.3302215337753296
train gradient:  0.13248632080898404
iteration : 13036
train acc:  0.8125
train loss:  0.34861046075820923
train gradient:  0.19170315626000234
iteration : 13037
train acc:  0.9140625
train loss:  0.23686838150024414
train gradient:  0.09670249016923353
iteration : 13038
train acc:  0.8203125
train loss:  0.30528050661087036
train gradient:  0.0939321576468461
iteration : 13039
train acc:  0.8828125
train loss:  0.27748608589172363
train gradient:  0.08638457723156732
iteration : 13040
train acc:  0.8359375
train loss:  0.3672105371952057
train gradient:  0.1476261896089729
iteration : 13041
train acc:  0.828125
train loss:  0.3736152648925781
train gradient:  0.1562178396141124
iteration : 13042
train acc:  0.828125
train loss:  0.31887495517730713
train gradient:  0.1253746917552349
iteration : 13043
train acc:  0.828125
train loss:  0.31651562452316284
train gradient:  0.11263072955359658
iteration : 13044
train acc:  0.953125
train loss:  0.22389110922813416
train gradient:  0.10253600019815533
iteration : 13045
train acc:  0.8828125
train loss:  0.3064793646335602
train gradient:  0.13264625084036513
iteration : 13046
train acc:  0.8984375
train loss:  0.2749963700771332
train gradient:  0.11084444824097704
iteration : 13047
train acc:  0.8828125
train loss:  0.3184698820114136
train gradient:  0.12774268328266652
iteration : 13048
train acc:  0.859375
train loss:  0.2917120158672333
train gradient:  0.10983945615107449
iteration : 13049
train acc:  0.8828125
train loss:  0.3425157964229584
train gradient:  0.1579877446643648
iteration : 13050
train acc:  0.921875
train loss:  0.22762127220630646
train gradient:  0.0967856442106072
iteration : 13051
train acc:  0.859375
train loss:  0.29494690895080566
train gradient:  0.10751322744041679
iteration : 13052
train acc:  0.9140625
train loss:  0.28090643882751465
train gradient:  0.10742681976207433
iteration : 13053
train acc:  0.8828125
train loss:  0.2547186017036438
train gradient:  0.1366329957164723
iteration : 13054
train acc:  0.921875
train loss:  0.23894426226615906
train gradient:  0.08993640208276568
iteration : 13055
train acc:  0.8515625
train loss:  0.2796640396118164
train gradient:  0.13219864936924144
iteration : 13056
train acc:  0.859375
train loss:  0.3601505160331726
train gradient:  0.151786666974765
iteration : 13057
train acc:  0.8515625
train loss:  0.3453117609024048
train gradient:  0.14697866219284358
iteration : 13058
train acc:  0.8203125
train loss:  0.3644266724586487
train gradient:  0.2103255865015742
iteration : 13059
train acc:  0.7890625
train loss:  0.4352119565010071
train gradient:  0.19662798922604574
iteration : 13060
train acc:  0.90625
train loss:  0.27328598499298096
train gradient:  0.18378123009081754
iteration : 13061
train acc:  0.859375
train loss:  0.3836619257926941
train gradient:  0.19163575630827637
iteration : 13062
train acc:  0.8515625
train loss:  0.3586244285106659
train gradient:  0.19006270800353922
iteration : 13063
train acc:  0.8671875
train loss:  0.3100099563598633
train gradient:  0.10966706281862652
iteration : 13064
train acc:  0.8515625
train loss:  0.31587931513786316
train gradient:  0.1617638895722753
iteration : 13065
train acc:  0.875
train loss:  0.27685976028442383
train gradient:  0.08322248660684477
iteration : 13066
train acc:  0.8828125
train loss:  0.2655406892299652
train gradient:  0.08306910711557573
iteration : 13067
train acc:  0.859375
train loss:  0.2664836049079895
train gradient:  0.09220318647697537
iteration : 13068
train acc:  0.828125
train loss:  0.31204983592033386
train gradient:  0.09929478385268525
iteration : 13069
train acc:  0.8046875
train loss:  0.3868311643600464
train gradient:  0.2372237082735149
iteration : 13070
train acc:  0.8828125
train loss:  0.27501678466796875
train gradient:  0.09057740491594886
iteration : 13071
train acc:  0.875
train loss:  0.33520400524139404
train gradient:  0.13049842371371517
iteration : 13072
train acc:  0.875
train loss:  0.3012334406375885
train gradient:  0.13607361095492237
iteration : 13073
train acc:  0.875
train loss:  0.2846337556838989
train gradient:  0.13187434609667276
iteration : 13074
train acc:  0.8671875
train loss:  0.28094083070755005
train gradient:  0.11247893979889322
iteration : 13075
train acc:  0.890625
train loss:  0.2765401303768158
train gradient:  0.09397575961420473
iteration : 13076
train acc:  0.8828125
train loss:  0.316555917263031
train gradient:  0.09986500494655277
iteration : 13077
train acc:  0.8671875
train loss:  0.3031487464904785
train gradient:  0.13872533978798163
iteration : 13078
train acc:  0.84375
train loss:  0.40084153413772583
train gradient:  0.19742327527639908
iteration : 13079
train acc:  0.890625
train loss:  0.2973235249519348
train gradient:  0.14819580548981884
iteration : 13080
train acc:  0.8515625
train loss:  0.348611980676651
train gradient:  0.15173097677612765
iteration : 13081
train acc:  0.90625
train loss:  0.2539080083370209
train gradient:  0.09002918533978051
iteration : 13082
train acc:  0.8203125
train loss:  0.4201568067073822
train gradient:  0.2892007046299744
iteration : 13083
train acc:  0.8828125
train loss:  0.30150341987609863
train gradient:  0.10557092076833267
iteration : 13084
train acc:  0.8515625
train loss:  0.2754550576210022
train gradient:  0.1030198108141961
iteration : 13085
train acc:  0.890625
train loss:  0.2958618998527527
train gradient:  0.11696439666997945
iteration : 13086
train acc:  0.875
train loss:  0.2989611029624939
train gradient:  0.10744223458066525
iteration : 13087
train acc:  0.7890625
train loss:  0.3703474998474121
train gradient:  0.1585352046002459
iteration : 13088
train acc:  0.8984375
train loss:  0.2784174382686615
train gradient:  0.13593507063957
iteration : 13089
train acc:  0.859375
train loss:  0.27434179186820984
train gradient:  0.10995041624016812
iteration : 13090
train acc:  0.90625
train loss:  0.2256760597229004
train gradient:  0.07719158522567496
iteration : 13091
train acc:  0.8203125
train loss:  0.36974823474884033
train gradient:  0.15293940417300142
iteration : 13092
train acc:  0.8515625
train loss:  0.37045544385910034
train gradient:  0.16436227301976128
iteration : 13093
train acc:  0.8828125
train loss:  0.29367655515670776
train gradient:  0.1121547697422106
iteration : 13094
train acc:  0.84375
train loss:  0.3423733115196228
train gradient:  0.2021529824901367
iteration : 13095
train acc:  0.8984375
train loss:  0.2847197651863098
train gradient:  0.1026974999625384
iteration : 13096
train acc:  0.8671875
train loss:  0.23683962225914001
train gradient:  0.08364733649480605
iteration : 13097
train acc:  0.8984375
train loss:  0.3130599558353424
train gradient:  0.1306301928119014
iteration : 13098
train acc:  0.8359375
train loss:  0.38704609870910645
train gradient:  0.21982619008576568
iteration : 13099
train acc:  0.8828125
train loss:  0.32054823637008667
train gradient:  0.3408680205118771
iteration : 13100
train acc:  0.84375
train loss:  0.3911585509777069
train gradient:  0.1915767905319547
iteration : 13101
train acc:  0.8984375
train loss:  0.26024818420410156
train gradient:  0.08524623578338449
iteration : 13102
train acc:  0.875
train loss:  0.2982074022293091
train gradient:  0.19189477272690003
iteration : 13103
train acc:  0.8984375
train loss:  0.23643027245998383
train gradient:  0.07894387357733573
iteration : 13104
train acc:  0.859375
train loss:  0.32284143567085266
train gradient:  0.1631524870153523
iteration : 13105
train acc:  0.875
train loss:  0.30602672696113586
train gradient:  0.11357641820888219
iteration : 13106
train acc:  0.875
train loss:  0.3346010446548462
train gradient:  0.13450208223431254
iteration : 13107
train acc:  0.90625
train loss:  0.26254767179489136
train gradient:  0.11715902226982923
iteration : 13108
train acc:  0.796875
train loss:  0.38470369577407837
train gradient:  0.18063900760714766
iteration : 13109
train acc:  0.8359375
train loss:  0.3614729344844818
train gradient:  0.1674897694126108
iteration : 13110
train acc:  0.8359375
train loss:  0.3986007869243622
train gradient:  0.23133177181862324
iteration : 13111
train acc:  0.8046875
train loss:  0.40447133779525757
train gradient:  0.24081707785064577
iteration : 13112
train acc:  0.8359375
train loss:  0.3661324083805084
train gradient:  0.1356793948952158
iteration : 13113
train acc:  0.9375
train loss:  0.19336150586605072
train gradient:  0.0801933440473611
iteration : 13114
train acc:  0.84375
train loss:  0.3544760048389435
train gradient:  0.15096905250235754
iteration : 13115
train acc:  0.890625
train loss:  0.27812278270721436
train gradient:  0.09915971752892394
iteration : 13116
train acc:  0.8515625
train loss:  0.35897719860076904
train gradient:  0.1633379525501485
iteration : 13117
train acc:  0.859375
train loss:  0.326254278421402
train gradient:  0.15574262386146406
iteration : 13118
train acc:  0.8515625
train loss:  0.3047548234462738
train gradient:  0.11148105644270703
iteration : 13119
train acc:  0.8828125
train loss:  0.3067132532596588
train gradient:  0.15662674460903303
iteration : 13120
train acc:  0.8984375
train loss:  0.28230470418930054
train gradient:  0.1280578801734153
iteration : 13121
train acc:  0.890625
train loss:  0.27125871181488037
train gradient:  0.130165556976589
iteration : 13122
train acc:  0.875
train loss:  0.30217427015304565
train gradient:  0.16240819473924897
iteration : 13123
train acc:  0.859375
train loss:  0.28238940238952637
train gradient:  0.08378828871943415
iteration : 13124
train acc:  0.8828125
train loss:  0.2808118164539337
train gradient:  0.09909580450068227
iteration : 13125
train acc:  0.828125
train loss:  0.3960339426994324
train gradient:  0.2653955039535164
iteration : 13126
train acc:  0.8984375
train loss:  0.28627848625183105
train gradient:  0.10468456570910432
iteration : 13127
train acc:  0.921875
train loss:  0.2816723585128784
train gradient:  0.12836596545498347
iteration : 13128
train acc:  0.8359375
train loss:  0.34669244289398193
train gradient:  0.15891306687346718
iteration : 13129
train acc:  0.8046875
train loss:  0.44645118713378906
train gradient:  0.24155024770178302
iteration : 13130
train acc:  0.859375
train loss:  0.31660816073417664
train gradient:  0.2071103170737485
iteration : 13131
train acc:  0.8984375
train loss:  0.204553484916687
train gradient:  0.06084287197311633
iteration : 13132
train acc:  0.8515625
train loss:  0.30934131145477295
train gradient:  0.12339770241076474
iteration : 13133
train acc:  0.828125
train loss:  0.33807480335235596
train gradient:  0.16087894050875926
iteration : 13134
train acc:  0.859375
train loss:  0.29901963472366333
train gradient:  0.13819406942386914
iteration : 13135
train acc:  0.8203125
train loss:  0.39548587799072266
train gradient:  0.16780162723171202
iteration : 13136
train acc:  0.8828125
train loss:  0.3072112202644348
train gradient:  0.14553914885258923
iteration : 13137
train acc:  0.8359375
train loss:  0.3305531442165375
train gradient:  0.18120783459128575
iteration : 13138
train acc:  0.8203125
train loss:  0.39154690504074097
train gradient:  0.16335034814307592
iteration : 13139
train acc:  0.8515625
train loss:  0.3377271294593811
train gradient:  0.13809696981309405
iteration : 13140
train acc:  0.8984375
train loss:  0.2915492653846741
train gradient:  0.0933876691978413
iteration : 13141
train acc:  0.875
train loss:  0.29618164896965027
train gradient:  0.16991992083581553
iteration : 13142
train acc:  0.8671875
train loss:  0.2793332040309906
train gradient:  0.10430214737046505
iteration : 13143
train acc:  0.8515625
train loss:  0.3217771053314209
train gradient:  0.14758584042984446
iteration : 13144
train acc:  0.8359375
train loss:  0.3344172239303589
train gradient:  0.1426833007853202
iteration : 13145
train acc:  0.8828125
train loss:  0.3154712915420532
train gradient:  0.14851875098258138
iteration : 13146
train acc:  0.8203125
train loss:  0.3405420482158661
train gradient:  0.13578925645526796
iteration : 13147
train acc:  0.8203125
train loss:  0.35709336400032043
train gradient:  0.2168629669025931
iteration : 13148
train acc:  0.8671875
train loss:  0.2732226252555847
train gradient:  0.08846029766947701
iteration : 13149
train acc:  0.890625
train loss:  0.2544199824333191
train gradient:  0.09986708651194671
iteration : 13150
train acc:  0.875
train loss:  0.3243112564086914
train gradient:  0.11492426260677814
iteration : 13151
train acc:  0.8828125
train loss:  0.29141226410865784
train gradient:  0.10793300337141477
iteration : 13152
train acc:  0.8359375
train loss:  0.29480671882629395
train gradient:  0.11405724333809979
iteration : 13153
train acc:  0.859375
train loss:  0.3877864480018616
train gradient:  0.16655371474080993
iteration : 13154
train acc:  0.8359375
train loss:  0.31602540612220764
train gradient:  0.16384727179643013
iteration : 13155
train acc:  0.84375
train loss:  0.360750675201416
train gradient:  0.17716019580729836
iteration : 13156
train acc:  0.890625
train loss:  0.38609635829925537
train gradient:  0.17151506572390027
iteration : 13157
train acc:  0.8359375
train loss:  0.29636675119400024
train gradient:  0.1234738217066866
iteration : 13158
train acc:  0.8203125
train loss:  0.34165236353874207
train gradient:  0.19519696449340762
iteration : 13159
train acc:  0.8515625
train loss:  0.3474263846874237
train gradient:  0.1932203077084259
iteration : 13160
train acc:  0.875
train loss:  0.27204981446266174
train gradient:  0.11308592792502364
iteration : 13161
train acc:  0.828125
train loss:  0.3524842858314514
train gradient:  0.20331506684753448
iteration : 13162
train acc:  0.8359375
train loss:  0.4259244501590729
train gradient:  0.18691289530880995
iteration : 13163
train acc:  0.8515625
train loss:  0.34519627690315247
train gradient:  0.19154379395304394
iteration : 13164
train acc:  0.84375
train loss:  0.3615492284297943
train gradient:  0.19011547978203747
iteration : 13165
train acc:  0.875
train loss:  0.31083548069000244
train gradient:  0.171548596825637
iteration : 13166
train acc:  0.8671875
train loss:  0.29819604754447937
train gradient:  0.09804178691268178
iteration : 13167
train acc:  0.8203125
train loss:  0.3787875771522522
train gradient:  0.1474094597636169
iteration : 13168
train acc:  0.859375
train loss:  0.2861018180847168
train gradient:  0.10626535929475284
iteration : 13169
train acc:  0.890625
train loss:  0.2577035427093506
train gradient:  0.11769348270371027
iteration : 13170
train acc:  0.875
train loss:  0.2908804416656494
train gradient:  0.10186664061399554
iteration : 13171
train acc:  0.890625
train loss:  0.3236376941204071
train gradient:  0.12662123038615652
iteration : 13172
train acc:  0.8828125
train loss:  0.2610684335231781
train gradient:  0.1050728972998963
iteration : 13173
train acc:  0.90625
train loss:  0.3236904740333557
train gradient:  0.14615562556404554
iteration : 13174
train acc:  0.875
train loss:  0.31032654643058777
train gradient:  0.17098487881044372
iteration : 13175
train acc:  0.859375
train loss:  0.28246527910232544
train gradient:  0.11829191077886589
iteration : 13176
train acc:  0.875
train loss:  0.2998325526714325
train gradient:  0.1586256240150098
iteration : 13177
train acc:  0.8125
train loss:  0.3655968904495239
train gradient:  0.18857998965938486
iteration : 13178
train acc:  0.8359375
train loss:  0.3304653763771057
train gradient:  0.1340082045148037
iteration : 13179
train acc:  0.875
train loss:  0.2922627925872803
train gradient:  0.14284186454713618
iteration : 13180
train acc:  0.8515625
train loss:  0.31683623790740967
train gradient:  0.10512135945854646
iteration : 13181
train acc:  0.90625
train loss:  0.2865843176841736
train gradient:  0.1526114599299322
iteration : 13182
train acc:  0.859375
train loss:  0.3347032070159912
train gradient:  0.18664432062379865
iteration : 13183
train acc:  0.875
train loss:  0.38680896162986755
train gradient:  0.14549327434562506
iteration : 13184
train acc:  0.875
train loss:  0.30818402767181396
train gradient:  0.14144254658654162
iteration : 13185
train acc:  0.8671875
train loss:  0.31927937269210815
train gradient:  0.19824318036643263
iteration : 13186
train acc:  0.859375
train loss:  0.3030409514904022
train gradient:  0.13270256606355646
iteration : 13187
train acc:  0.875
train loss:  0.2801123857498169
train gradient:  0.10639441599083607
iteration : 13188
train acc:  0.90625
train loss:  0.2652250826358795
train gradient:  0.10245091609390235
iteration : 13189
train acc:  0.8671875
train loss:  0.3094242215156555
train gradient:  0.1293617087778533
iteration : 13190
train acc:  0.890625
train loss:  0.2683747112751007
train gradient:  0.07753040106911559
iteration : 13191
train acc:  0.859375
train loss:  0.27141135931015015
train gradient:  0.13464778575414812
iteration : 13192
train acc:  0.9140625
train loss:  0.2866055965423584
train gradient:  0.32265222289978046
iteration : 13193
train acc:  0.890625
train loss:  0.291443407535553
train gradient:  0.14149971982945136
iteration : 13194
train acc:  0.84375
train loss:  0.3877445459365845
train gradient:  0.23578082806714018
iteration : 13195
train acc:  0.84375
train loss:  0.3738928735256195
train gradient:  0.31269670151945717
iteration : 13196
train acc:  0.8828125
train loss:  0.2875699996948242
train gradient:  0.09983545996840935
iteration : 13197
train acc:  0.8984375
train loss:  0.23376822471618652
train gradient:  0.10375292280556478
iteration : 13198
train acc:  0.8984375
train loss:  0.311455637216568
train gradient:  0.13025262115773129
iteration : 13199
train acc:  0.8984375
train loss:  0.3053106665611267
train gradient:  0.09841214537638181
iteration : 13200
train acc:  0.8359375
train loss:  0.37384623289108276
train gradient:  0.17294524509168535
iteration : 13201
train acc:  0.875
train loss:  0.26725757122039795
train gradient:  0.11693442476483466
iteration : 13202
train acc:  0.8984375
train loss:  0.2761304974555969
train gradient:  0.1190466696032151
iteration : 13203
train acc:  0.8125
train loss:  0.3606362044811249
train gradient:  0.1903095837335468
iteration : 13204
train acc:  0.8671875
train loss:  0.31477677822113037
train gradient:  0.13293967623093433
iteration : 13205
train acc:  0.890625
train loss:  0.2676699161529541
train gradient:  0.12420942691984097
iteration : 13206
train acc:  0.8828125
train loss:  0.29332584142684937
train gradient:  0.12759171915820822
iteration : 13207
train acc:  0.8828125
train loss:  0.29587048292160034
train gradient:  0.16024521098274522
iteration : 13208
train acc:  0.890625
train loss:  0.2692861258983612
train gradient:  0.10430694254468784
iteration : 13209
train acc:  0.875
train loss:  0.36012351512908936
train gradient:  0.13728213232718678
iteration : 13210
train acc:  0.8125
train loss:  0.35407719016075134
train gradient:  0.1994154344905637
iteration : 13211
train acc:  0.859375
train loss:  0.34483787417411804
train gradient:  0.1511892807524962
iteration : 13212
train acc:  0.921875
train loss:  0.2400204986333847
train gradient:  0.08810702236834721
iteration : 13213
train acc:  0.8515625
train loss:  0.2863157391548157
train gradient:  0.10848286867252227
iteration : 13214
train acc:  0.828125
train loss:  0.3910294473171234
train gradient:  0.2168722004001961
iteration : 13215
train acc:  0.8828125
train loss:  0.28647708892822266
train gradient:  0.14706974392424293
iteration : 13216
train acc:  0.8671875
train loss:  0.29518210887908936
train gradient:  0.14603859906660796
iteration : 13217
train acc:  0.859375
train loss:  0.29210662841796875
train gradient:  0.12549393605674314
iteration : 13218
train acc:  0.8359375
train loss:  0.34029459953308105
train gradient:  0.18285350444123577
iteration : 13219
train acc:  0.8671875
train loss:  0.3151465356349945
train gradient:  0.1312775781458123
iteration : 13220
train acc:  0.8984375
train loss:  0.2190435528755188
train gradient:  0.06470800858519586
iteration : 13221
train acc:  0.8671875
train loss:  0.30599695444107056
train gradient:  0.13388859165544098
iteration : 13222
train acc:  0.8828125
train loss:  0.2260587215423584
train gradient:  0.09258636056785348
iteration : 13223
train acc:  0.890625
train loss:  0.25658106803894043
train gradient:  0.10693123002636268
iteration : 13224
train acc:  0.875
train loss:  0.29681676626205444
train gradient:  0.1289597261347848
iteration : 13225
train acc:  0.875
train loss:  0.2672956883907318
train gradient:  0.13752481308307796
iteration : 13226
train acc:  0.828125
train loss:  0.34103912115097046
train gradient:  0.21159153952452323
iteration : 13227
train acc:  0.890625
train loss:  0.33194500207901
train gradient:  0.13570803605478038
iteration : 13228
train acc:  0.84375
train loss:  0.3684569001197815
train gradient:  0.21073938505455972
iteration : 13229
train acc:  0.8515625
train loss:  0.35881808400154114
train gradient:  0.14096381531618432
iteration : 13230
train acc:  0.859375
train loss:  0.2869664132595062
train gradient:  0.138199120601397
iteration : 13231
train acc:  0.8828125
train loss:  0.2303125411272049
train gradient:  0.0838605789466087
iteration : 13232
train acc:  0.859375
train loss:  0.2978915572166443
train gradient:  0.11788256903835494
iteration : 13233
train acc:  0.8046875
train loss:  0.40764862298965454
train gradient:  0.21993059063244658
iteration : 13234
train acc:  0.828125
train loss:  0.36375755071640015
train gradient:  0.17487470693240403
iteration : 13235
train acc:  0.84375
train loss:  0.3203597366809845
train gradient:  0.15888067782905213
iteration : 13236
train acc:  0.8359375
train loss:  0.428526371717453
train gradient:  0.18832248617496664
iteration : 13237
train acc:  0.828125
train loss:  0.35352009534835815
train gradient:  0.15357305250215786
iteration : 13238
train acc:  0.859375
train loss:  0.36707818508148193
train gradient:  0.2412645441061682
iteration : 13239
train acc:  0.921875
train loss:  0.21166737377643585
train gradient:  0.09370053895411451
iteration : 13240
train acc:  0.8828125
train loss:  0.26876121759414673
train gradient:  0.12395611156550988
iteration : 13241
train acc:  0.90625
train loss:  0.22349479794502258
train gradient:  0.12182785008425268
iteration : 13242
train acc:  0.90625
train loss:  0.21919040381908417
train gradient:  0.08508158540975426
iteration : 13243
train acc:  0.84375
train loss:  0.34025734663009644
train gradient:  0.17401823286244977
iteration : 13244
train acc:  0.8671875
train loss:  0.36130523681640625
train gradient:  0.18688040125398264
iteration : 13245
train acc:  0.90625
train loss:  0.26734811067581177
train gradient:  0.11146543828681035
iteration : 13246
train acc:  0.8046875
train loss:  0.3639364540576935
train gradient:  0.1662435986148328
iteration : 13247
train acc:  0.875
train loss:  0.2720296382904053
train gradient:  0.08951851793441315
iteration : 13248
train acc:  0.8359375
train loss:  0.3293651342391968
train gradient:  0.16741070016162707
iteration : 13249
train acc:  0.8203125
train loss:  0.4141171872615814
train gradient:  0.18716646941603474
iteration : 13250
train acc:  0.859375
train loss:  0.28201815485954285
train gradient:  0.13121789253602867
iteration : 13251
train acc:  0.8203125
train loss:  0.33786800503730774
train gradient:  0.1657708243722181
iteration : 13252
train acc:  0.8828125
train loss:  0.3385161757469177
train gradient:  0.18760425289057023
iteration : 13253
train acc:  0.890625
train loss:  0.27772605419158936
train gradient:  0.10706689034700571
iteration : 13254
train acc:  0.8515625
train loss:  0.30721455812454224
train gradient:  0.12508254812601632
iteration : 13255
train acc:  0.90625
train loss:  0.258989542722702
train gradient:  0.16495832212838032
iteration : 13256
train acc:  0.859375
train loss:  0.34168851375579834
train gradient:  0.16103515084186837
iteration : 13257
train acc:  0.828125
train loss:  0.33386239409446716
train gradient:  0.1249862112042196
iteration : 13258
train acc:  0.8515625
train loss:  0.2722615599632263
train gradient:  0.09086759023299228
iteration : 13259
train acc:  0.8828125
train loss:  0.2515566945075989
train gradient:  0.09080794590649306
iteration : 13260
train acc:  0.8125
train loss:  0.4009409248828888
train gradient:  0.2591713251366821
iteration : 13261
train acc:  0.875
train loss:  0.3051489293575287
train gradient:  0.10663950152951712
iteration : 13262
train acc:  0.84375
train loss:  0.342020720243454
train gradient:  0.17617008296385067
iteration : 13263
train acc:  0.8828125
train loss:  0.3276074528694153
train gradient:  0.1401125812044935
iteration : 13264
train acc:  0.8359375
train loss:  0.33056920766830444
train gradient:  0.1642777486697065
iteration : 13265
train acc:  0.859375
train loss:  0.3016898036003113
train gradient:  0.14795329769383087
iteration : 13266
train acc:  0.8828125
train loss:  0.2907153367996216
train gradient:  0.15591678539040485
iteration : 13267
train acc:  0.796875
train loss:  0.35818803310394287
train gradient:  0.1434399635154835
iteration : 13268
train acc:  0.8359375
train loss:  0.32647597789764404
train gradient:  0.10717589979931605
iteration : 13269
train acc:  0.8984375
train loss:  0.30161818861961365
train gradient:  0.10922283967108161
iteration : 13270
train acc:  0.8984375
train loss:  0.2785722613334656
train gradient:  0.11517436211510179
iteration : 13271
train acc:  0.9375
train loss:  0.20099370181560516
train gradient:  0.08059718819992719
iteration : 13272
train acc:  0.8359375
train loss:  0.3149062395095825
train gradient:  0.13958953019879325
iteration : 13273
train acc:  0.9140625
train loss:  0.21610252559185028
train gradient:  0.07992143753514501
iteration : 13274
train acc:  0.828125
train loss:  0.3272407352924347
train gradient:  0.1556870938898514
iteration : 13275
train acc:  0.890625
train loss:  0.2828204929828644
train gradient:  0.12316493082116098
iteration : 13276
train acc:  0.8671875
train loss:  0.2712717354297638
train gradient:  0.08475260242818657
iteration : 13277
train acc:  0.8984375
train loss:  0.2706095576286316
train gradient:  0.07935941160586835
iteration : 13278
train acc:  0.8671875
train loss:  0.277989000082016
train gradient:  0.16439932507276486
iteration : 13279
train acc:  0.8046875
train loss:  0.42767518758773804
train gradient:  0.23936435580132082
iteration : 13280
train acc:  0.84375
train loss:  0.3433089256286621
train gradient:  0.16229171190255132
iteration : 13281
train acc:  0.8828125
train loss:  0.2906947433948517
train gradient:  0.16028843976639046
iteration : 13282
train acc:  0.8203125
train loss:  0.35419580340385437
train gradient:  0.1598995500047497
iteration : 13283
train acc:  0.859375
train loss:  0.3209327459335327
train gradient:  0.1824985337947554
iteration : 13284
train acc:  0.8984375
train loss:  0.3155460059642792
train gradient:  0.12024854580881691
iteration : 13285
train acc:  0.8125
train loss:  0.3948264718055725
train gradient:  0.17825635066647086
iteration : 13286
train acc:  0.890625
train loss:  0.27993834018707275
train gradient:  0.13446365853114128
iteration : 13287
train acc:  0.9296875
train loss:  0.2725377082824707
train gradient:  0.10038453194537512
iteration : 13288
train acc:  0.890625
train loss:  0.26598721742630005
train gradient:  0.11380341449009704
iteration : 13289
train acc:  0.8359375
train loss:  0.4152997136116028
train gradient:  0.2222210697661401
iteration : 13290
train acc:  0.8203125
train loss:  0.3708024024963379
train gradient:  0.1712429674012531
iteration : 13291
train acc:  0.796875
train loss:  0.5057825446128845
train gradient:  0.29957576567266997
iteration : 13292
train acc:  0.8828125
train loss:  0.317082017660141
train gradient:  0.1460320023442627
iteration : 13293
train acc:  0.875
train loss:  0.25787779688835144
train gradient:  0.10007304476889858
iteration : 13294
train acc:  0.875
train loss:  0.30964410305023193
train gradient:  0.10750510041303435
iteration : 13295
train acc:  0.8828125
train loss:  0.27813032269477844
train gradient:  0.12818839116245567
iteration : 13296
train acc:  0.8671875
train loss:  0.322817862033844
train gradient:  0.16787052542380254
iteration : 13297
train acc:  0.8359375
train loss:  0.39098209142684937
train gradient:  0.16760122471225058
iteration : 13298
train acc:  0.8515625
train loss:  0.3275752663612366
train gradient:  0.1459373585755172
iteration : 13299
train acc:  0.84375
train loss:  0.35975828766822815
train gradient:  0.1628201675929883
iteration : 13300
train acc:  0.9140625
train loss:  0.28401482105255127
train gradient:  0.09575861109646502
iteration : 13301
train acc:  0.875
train loss:  0.3310805559158325
train gradient:  0.14941860373621055
iteration : 13302
train acc:  0.859375
train loss:  0.4091913104057312
train gradient:  0.15784301279193047
iteration : 13303
train acc:  0.90625
train loss:  0.2744751274585724
train gradient:  0.11372929103527243
iteration : 13304
train acc:  0.84375
train loss:  0.37780314683914185
train gradient:  0.18204748374149582
iteration : 13305
train acc:  0.90625
train loss:  0.3190992474555969
train gradient:  0.147345881594278
iteration : 13306
train acc:  0.9140625
train loss:  0.22348140180110931
train gradient:  0.12454047398931227
iteration : 13307
train acc:  0.8671875
train loss:  0.3824717402458191
train gradient:  0.19620311979404068
iteration : 13308
train acc:  0.8359375
train loss:  0.3970680832862854
train gradient:  0.16510285483800446
iteration : 13309
train acc:  0.875
train loss:  0.27636414766311646
train gradient:  0.11397302798851083
iteration : 13310
train acc:  0.84375
train loss:  0.32740098237991333
train gradient:  0.14418167200420892
iteration : 13311
train acc:  0.8515625
train loss:  0.3033919930458069
train gradient:  0.17678574713242673
iteration : 13312
train acc:  0.890625
train loss:  0.2620394229888916
train gradient:  0.08650104434677756
iteration : 13313
train acc:  0.8671875
train loss:  0.3637748956680298
train gradient:  0.15752607107264507
iteration : 13314
train acc:  0.8359375
train loss:  0.3220354914665222
train gradient:  0.1477050621585555
iteration : 13315
train acc:  0.921875
train loss:  0.2585298418998718
train gradient:  0.1049161848117398
iteration : 13316
train acc:  0.8984375
train loss:  0.25843900442123413
train gradient:  0.09845947781039964
iteration : 13317
train acc:  0.875
train loss:  0.35744810104370117
train gradient:  0.120238590956673
iteration : 13318
train acc:  0.8828125
train loss:  0.28466713428497314
train gradient:  0.09211696437062368
iteration : 13319
train acc:  0.875
train loss:  0.3279082775115967
train gradient:  0.1408207479300005
iteration : 13320
train acc:  0.90625
train loss:  0.30790555477142334
train gradient:  0.23337315665379132
iteration : 13321
train acc:  0.8515625
train loss:  0.3383483290672302
train gradient:  0.1931437654750507
iteration : 13322
train acc:  0.875
train loss:  0.2606796622276306
train gradient:  0.11374123271980519
iteration : 13323
train acc:  0.8359375
train loss:  0.3541520833969116
train gradient:  0.15830961527312865
iteration : 13324
train acc:  0.828125
train loss:  0.34327980875968933
train gradient:  0.20390499318717473
iteration : 13325
train acc:  0.8046875
train loss:  0.4239954948425293
train gradient:  0.20361153405118987
iteration : 13326
train acc:  0.8359375
train loss:  0.350771427154541
train gradient:  0.17466220519132167
iteration : 13327
train acc:  0.8046875
train loss:  0.40557119250297546
train gradient:  0.1588899661972184
iteration : 13328
train acc:  0.8515625
train loss:  0.3393746018409729
train gradient:  0.15143685418992384
iteration : 13329
train acc:  0.8515625
train loss:  0.34513506293296814
train gradient:  0.1460784868736238
iteration : 13330
train acc:  0.8359375
train loss:  0.37337568402290344
train gradient:  0.16741834081966045
iteration : 13331
train acc:  0.8359375
train loss:  0.3467296361923218
train gradient:  0.1726721152928119
iteration : 13332
train acc:  0.890625
train loss:  0.2918850779533386
train gradient:  0.11503828339698632
iteration : 13333
train acc:  0.8046875
train loss:  0.42485344409942627
train gradient:  0.1878033224036933
iteration : 13334
train acc:  0.8203125
train loss:  0.39911848306655884
train gradient:  0.19184550364375713
iteration : 13335
train acc:  0.8984375
train loss:  0.30237382650375366
train gradient:  0.13993999928211126
iteration : 13336
train acc:  0.8515625
train loss:  0.31366103887557983
train gradient:  0.13872394671032912
iteration : 13337
train acc:  0.890625
train loss:  0.2610722780227661
train gradient:  0.10445597244525077
iteration : 13338
train acc:  0.875
train loss:  0.30831706523895264
train gradient:  0.1597431066275568
iteration : 13339
train acc:  0.84375
train loss:  0.3626054525375366
train gradient:  0.12738766975286178
iteration : 13340
train acc:  0.84375
train loss:  0.3298574984073639
train gradient:  0.15011882079209055
iteration : 13341
train acc:  0.84375
train loss:  0.3451341986656189
train gradient:  0.1660839961088148
iteration : 13342
train acc:  0.8828125
train loss:  0.27900230884552
train gradient:  0.09548177713309074
iteration : 13343
train acc:  0.921875
train loss:  0.2772648334503174
train gradient:  0.11382069198629563
iteration : 13344
train acc:  0.84375
train loss:  0.339411199092865
train gradient:  0.15271312169830803
iteration : 13345
train acc:  0.9296875
train loss:  0.20735234022140503
train gradient:  0.08813631392856894
iteration : 13346
train acc:  0.921875
train loss:  0.2598732113838196
train gradient:  0.08503714511150533
iteration : 13347
train acc:  0.859375
train loss:  0.31717604398727417
train gradient:  0.11066115561249115
iteration : 13348
train acc:  0.84375
train loss:  0.2939678430557251
train gradient:  0.13895984536538752
iteration : 13349
train acc:  0.8671875
train loss:  0.32433903217315674
train gradient:  0.09620756081396689
iteration : 13350
train acc:  0.8359375
train loss:  0.30691319704055786
train gradient:  0.12457800459260304
iteration : 13351
train acc:  0.875
train loss:  0.3769601881504059
train gradient:  0.12253248568430947
iteration : 13352
train acc:  0.875
train loss:  0.3199712634086609
train gradient:  0.16488463566221573
iteration : 13353
train acc:  0.84375
train loss:  0.3199038803577423
train gradient:  0.1724719922638105
iteration : 13354
train acc:  0.875
train loss:  0.32558944821357727
train gradient:  0.11337966205153842
iteration : 13355
train acc:  0.859375
train loss:  0.3145691752433777
train gradient:  0.13628891808671953
iteration : 13356
train acc:  0.859375
train loss:  0.3280298411846161
train gradient:  0.13275300432516374
iteration : 13357
train acc:  0.8984375
train loss:  0.27903175354003906
train gradient:  0.1462052495080497
iteration : 13358
train acc:  0.859375
train loss:  0.3347938358783722
train gradient:  0.13200078674266102
iteration : 13359
train acc:  0.8671875
train loss:  0.4269770085811615
train gradient:  0.1984331375991961
iteration : 13360
train acc:  0.8515625
train loss:  0.2974957227706909
train gradient:  0.13156508964585106
iteration : 13361
train acc:  0.8984375
train loss:  0.2909981906414032
train gradient:  0.1270520492671283
iteration : 13362
train acc:  0.8515625
train loss:  0.3412737250328064
train gradient:  0.14951197986903053
iteration : 13363
train acc:  0.828125
train loss:  0.3470039367675781
train gradient:  0.13663235363228032
iteration : 13364
train acc:  0.8359375
train loss:  0.3117341995239258
train gradient:  0.12526041999465357
iteration : 13365
train acc:  0.890625
train loss:  0.2598493695259094
train gradient:  0.12407284800173453
iteration : 13366
train acc:  0.8515625
train loss:  0.39861494302749634
train gradient:  0.28990601292870044
iteration : 13367
train acc:  0.828125
train loss:  0.35894763469696045
train gradient:  0.14412477905437232
iteration : 13368
train acc:  0.859375
train loss:  0.35178565979003906
train gradient:  0.1599227521708605
iteration : 13369
train acc:  0.875
train loss:  0.3518291115760803
train gradient:  0.14894372174182757
iteration : 13370
train acc:  0.8203125
train loss:  0.3743249773979187
train gradient:  0.20886886969279267
iteration : 13371
train acc:  0.8671875
train loss:  0.2669002413749695
train gradient:  0.10736913877995707
iteration : 13372
train acc:  0.8359375
train loss:  0.3821786046028137
train gradient:  0.18492404490270872
iteration : 13373
train acc:  0.859375
train loss:  0.2983802258968353
train gradient:  0.07961297229824776
iteration : 13374
train acc:  0.8125
train loss:  0.3624686002731323
train gradient:  0.16727347625624678
iteration : 13375
train acc:  0.859375
train loss:  0.34570205211639404
train gradient:  0.16137494096856486
iteration : 13376
train acc:  0.8671875
train loss:  0.3045482635498047
train gradient:  0.12328392828747178
iteration : 13377
train acc:  0.890625
train loss:  0.2798374891281128
train gradient:  0.07997967354875338
iteration : 13378
train acc:  0.8515625
train loss:  0.30673474073410034
train gradient:  0.3110014329469456
iteration : 13379
train acc:  0.859375
train loss:  0.283427357673645
train gradient:  0.09352240702457872
iteration : 13380
train acc:  0.8828125
train loss:  0.31723982095718384
train gradient:  0.14042657473719056
iteration : 13381
train acc:  0.8515625
train loss:  0.2775474786758423
train gradient:  0.09727122661007737
iteration : 13382
train acc:  0.8515625
train loss:  0.30194705724716187
train gradient:  0.13384984292179364
iteration : 13383
train acc:  0.890625
train loss:  0.2691267430782318
train gradient:  0.13386409993193757
iteration : 13384
train acc:  0.8515625
train loss:  0.3176729083061218
train gradient:  0.12645331728611325
iteration : 13385
train acc:  0.875
train loss:  0.26227620244026184
train gradient:  0.08386165165783806
iteration : 13386
train acc:  0.84375
train loss:  0.34671297669410706
train gradient:  0.13761859320959438
iteration : 13387
train acc:  0.921875
train loss:  0.3061766028404236
train gradient:  0.1329980172530959
iteration : 13388
train acc:  0.8359375
train loss:  0.36936360597610474
train gradient:  0.23050379791620024
iteration : 13389
train acc:  0.8984375
train loss:  0.21323856711387634
train gradient:  0.08187772714264314
iteration : 13390
train acc:  0.8671875
train loss:  0.3609350621700287
train gradient:  0.20171309572663854
iteration : 13391
train acc:  0.8671875
train loss:  0.3709719777107239
train gradient:  0.17564224883543472
iteration : 13392
train acc:  0.859375
train loss:  0.3531297743320465
train gradient:  0.20005408627639848
iteration : 13393
train acc:  0.8671875
train loss:  0.36152178049087524
train gradient:  0.197315940907794
iteration : 13394
train acc:  0.875
train loss:  0.3119136095046997
train gradient:  0.1320869079067184
iteration : 13395
train acc:  0.8671875
train loss:  0.2512836456298828
train gradient:  0.07171264853604505
iteration : 13396
train acc:  0.8515625
train loss:  0.3301282525062561
train gradient:  0.13802457585849245
iteration : 13397
train acc:  0.875
train loss:  0.30221349000930786
train gradient:  0.0908685874764884
iteration : 13398
train acc:  0.8359375
train loss:  0.36307579278945923
train gradient:  0.15811435443575012
iteration : 13399
train acc:  0.8515625
train loss:  0.3510637879371643
train gradient:  0.17949226885591646
iteration : 13400
train acc:  0.90625
train loss:  0.27818986773490906
train gradient:  0.09672830171497908
iteration : 13401
train acc:  0.8671875
train loss:  0.3037932813167572
train gradient:  0.1324441242207855
iteration : 13402
train acc:  0.828125
train loss:  0.43494778871536255
train gradient:  0.2476712069838487
iteration : 13403
train acc:  0.828125
train loss:  0.38613617420196533
train gradient:  0.14331925830385608
iteration : 13404
train acc:  0.75
train loss:  0.4309478998184204
train gradient:  0.23309613201841095
iteration : 13405
train acc:  0.8671875
train loss:  0.2787102162837982
train gradient:  0.12931574981284388
iteration : 13406
train acc:  0.8515625
train loss:  0.291728138923645
train gradient:  0.12107297104331537
iteration : 13407
train acc:  0.828125
train loss:  0.45886415243148804
train gradient:  0.26644368293691223
iteration : 13408
train acc:  0.8515625
train loss:  0.2839011251926422
train gradient:  0.11381546550099714
iteration : 13409
train acc:  0.875
train loss:  0.2904896140098572
train gradient:  0.12711269920457385
iteration : 13410
train acc:  0.90625
train loss:  0.2860119640827179
train gradient:  0.09272866440723797
iteration : 13411
train acc:  0.8203125
train loss:  0.3245236277580261
train gradient:  0.1162877499640637
iteration : 13412
train acc:  0.8828125
train loss:  0.3486599624156952
train gradient:  0.19420056718162276
iteration : 13413
train acc:  0.875
train loss:  0.2785836458206177
train gradient:  0.10448990300138813
iteration : 13414
train acc:  0.875
train loss:  0.34225353598594666
train gradient:  0.13931814431555486
iteration : 13415
train acc:  0.875
train loss:  0.30235540866851807
train gradient:  0.16343866351516512
iteration : 13416
train acc:  0.8515625
train loss:  0.36725229024887085
train gradient:  0.2618096627696201
iteration : 13417
train acc:  0.828125
train loss:  0.3387613892555237
train gradient:  0.12498825042897398
iteration : 13418
train acc:  0.8359375
train loss:  0.308229923248291
train gradient:  0.16156107082684878
iteration : 13419
train acc:  0.8125
train loss:  0.4587233066558838
train gradient:  0.1944681173036415
iteration : 13420
train acc:  0.8828125
train loss:  0.2992744445800781
train gradient:  0.12537573836439977
iteration : 13421
train acc:  0.90625
train loss:  0.23437581956386566
train gradient:  0.10017753474419717
iteration : 13422
train acc:  0.828125
train loss:  0.3642405569553375
train gradient:  0.20804208657431128
iteration : 13423
train acc:  0.8984375
train loss:  0.25759246945381165
train gradient:  0.10538096065991177
iteration : 13424
train acc:  0.875
train loss:  0.35194286704063416
train gradient:  0.11117525527129478
iteration : 13425
train acc:  0.8125
train loss:  0.3911378085613251
train gradient:  0.18806320735241
iteration : 13426
train acc:  0.8515625
train loss:  0.3687940835952759
train gradient:  0.17142335155456395
iteration : 13427
train acc:  0.890625
train loss:  0.2993590235710144
train gradient:  0.13197391003304174
iteration : 13428
train acc:  0.8828125
train loss:  0.37036463618278503
train gradient:  0.20523648087902835
iteration : 13429
train acc:  0.859375
train loss:  0.3159542977809906
train gradient:  0.11860903857537702
iteration : 13430
train acc:  0.890625
train loss:  0.31070879101753235
train gradient:  0.09616881812221416
iteration : 13431
train acc:  0.796875
train loss:  0.38749605417251587
train gradient:  0.16563460548558329
iteration : 13432
train acc:  0.8984375
train loss:  0.33728063106536865
train gradient:  0.0962850734547167
iteration : 13433
train acc:  0.8671875
train loss:  0.28065502643585205
train gradient:  0.08550766449764689
iteration : 13434
train acc:  0.8359375
train loss:  0.3621763586997986
train gradient:  0.12560251287644225
iteration : 13435
train acc:  0.8515625
train loss:  0.30077874660491943
train gradient:  0.12001464929879714
iteration : 13436
train acc:  0.828125
train loss:  0.3073731064796448
train gradient:  0.10149597735025787
iteration : 13437
train acc:  0.8515625
train loss:  0.3171464800834656
train gradient:  0.1491143972656045
iteration : 13438
train acc:  0.8828125
train loss:  0.2844776511192322
train gradient:  0.07938949688323887
iteration : 13439
train acc:  0.875
train loss:  0.27299821376800537
train gradient:  0.10378433591285471
iteration : 13440
train acc:  0.890625
train loss:  0.2764905095100403
train gradient:  0.0992015608538746
iteration : 13441
train acc:  0.859375
train loss:  0.26459312438964844
train gradient:  0.0949485849275972
iteration : 13442
train acc:  0.8515625
train loss:  0.353693425655365
train gradient:  0.14618577029764426
iteration : 13443
train acc:  0.890625
train loss:  0.24507619440555573
train gradient:  0.11256201555379294
iteration : 13444
train acc:  0.828125
train loss:  0.348225474357605
train gradient:  0.14169706776466207
iteration : 13445
train acc:  0.84375
train loss:  0.31648528575897217
train gradient:  0.09620699944093428
iteration : 13446
train acc:  0.8828125
train loss:  0.2705402970314026
train gradient:  0.12662675888986066
iteration : 13447
train acc:  0.8671875
train loss:  0.30021655559539795
train gradient:  0.133461337585162
iteration : 13448
train acc:  0.8359375
train loss:  0.34409087896347046
train gradient:  0.10040946661317168
iteration : 13449
train acc:  0.859375
train loss:  0.2806522846221924
train gradient:  0.10113312531425384
iteration : 13450
train acc:  0.84375
train loss:  0.3083127737045288
train gradient:  0.13691528078592624
iteration : 13451
train acc:  0.859375
train loss:  0.3851674795150757
train gradient:  0.20046694567385687
iteration : 13452
train acc:  0.890625
train loss:  0.3352900445461273
train gradient:  0.20628897085945197
iteration : 13453
train acc:  0.8671875
train loss:  0.32080399990081787
train gradient:  0.11879122597258199
iteration : 13454
train acc:  0.859375
train loss:  0.35240790247917175
train gradient:  0.13460395285927634
iteration : 13455
train acc:  0.8515625
train loss:  0.24222911894321442
train gradient:  0.08712973821869448
iteration : 13456
train acc:  0.921875
train loss:  0.22898724675178528
train gradient:  0.09008397575069411
iteration : 13457
train acc:  0.8515625
train loss:  0.2992364168167114
train gradient:  0.10909770417358625
iteration : 13458
train acc:  0.8515625
train loss:  0.30084866285324097
train gradient:  0.1351170101517044
iteration : 13459
train acc:  0.8515625
train loss:  0.33849942684173584
train gradient:  0.14117774413751
iteration : 13460
train acc:  0.8125
train loss:  0.37844613194465637
train gradient:  0.1895825580272388
iteration : 13461
train acc:  0.890625
train loss:  0.28645452857017517
train gradient:  0.10552894076240817
iteration : 13462
train acc:  0.890625
train loss:  0.24572068452835083
train gradient:  0.059052461573366605
iteration : 13463
train acc:  0.8671875
train loss:  0.3097381591796875
train gradient:  0.10046293369706899
iteration : 13464
train acc:  0.8515625
train loss:  0.3016277253627777
train gradient:  0.11203516501316771
iteration : 13465
train acc:  0.859375
train loss:  0.31656667590141296
train gradient:  0.11025157925541595
iteration : 13466
train acc:  0.84375
train loss:  0.34648096561431885
train gradient:  0.15241439919461153
iteration : 13467
train acc:  0.90625
train loss:  0.2295992076396942
train gradient:  0.07375318413074261
iteration : 13468
train acc:  0.859375
train loss:  0.2895413339138031
train gradient:  0.09985662801645069
iteration : 13469
train acc:  0.84375
train loss:  0.3882230222225189
train gradient:  0.24008245070570666
iteration : 13470
train acc:  0.8515625
train loss:  0.3073083758354187
train gradient:  0.13660431586844604
iteration : 13471
train acc:  0.8203125
train loss:  0.3682253360748291
train gradient:  0.15134122983481502
iteration : 13472
train acc:  0.9375
train loss:  0.2482266128063202
train gradient:  0.09788894406489558
iteration : 13473
train acc:  0.9140625
train loss:  0.1957167685031891
train gradient:  0.0714303288382172
iteration : 13474
train acc:  0.8671875
train loss:  0.2934800982475281
train gradient:  0.11895820547999093
iteration : 13475
train acc:  0.875
train loss:  0.2709435224533081
train gradient:  0.1189982171069896
iteration : 13476
train acc:  0.8984375
train loss:  0.24225571751594543
train gradient:  0.08333509389723605
iteration : 13477
train acc:  0.8515625
train loss:  0.26324135065078735
train gradient:  0.087788416359782
iteration : 13478
train acc:  0.890625
train loss:  0.275256872177124
train gradient:  0.18110867523157634
iteration : 13479
train acc:  0.828125
train loss:  0.29999011754989624
train gradient:  0.11189318543309133
iteration : 13480
train acc:  0.8203125
train loss:  0.36983221769332886
train gradient:  0.21443001808540613
iteration : 13481
train acc:  0.8515625
train loss:  0.3949844241142273
train gradient:  0.21777691446955727
iteration : 13482
train acc:  0.84375
train loss:  0.3099263906478882
train gradient:  0.14271126435815745
iteration : 13483
train acc:  0.8203125
train loss:  0.3773778975009918
train gradient:  0.2561455275344122
iteration : 13484
train acc:  0.859375
train loss:  0.32090437412261963
train gradient:  0.14718212730283642
iteration : 13485
train acc:  0.8984375
train loss:  0.272060751914978
train gradient:  0.11287341416325662
iteration : 13486
train acc:  0.828125
train loss:  0.37713849544525146
train gradient:  0.18766132608845665
iteration : 13487
train acc:  0.828125
train loss:  0.34489917755126953
train gradient:  0.1425510170927854
iteration : 13488
train acc:  0.9140625
train loss:  0.24851037561893463
train gradient:  0.09778481113872804
iteration : 13489
train acc:  0.7890625
train loss:  0.4210016131401062
train gradient:  0.18716242239154007
iteration : 13490
train acc:  0.875
train loss:  0.2948433756828308
train gradient:  0.11905385357835489
iteration : 13491
train acc:  0.828125
train loss:  0.39019539952278137
train gradient:  0.21595667072731112
iteration : 13492
train acc:  0.90625
train loss:  0.27407997846603394
train gradient:  0.11624839043315283
iteration : 13493
train acc:  0.8515625
train loss:  0.30095016956329346
train gradient:  0.13885338827868787
iteration : 13494
train acc:  0.8125
train loss:  0.38338860869407654
train gradient:  0.19820301011731822
iteration : 13495
train acc:  0.890625
train loss:  0.29400938749313354
train gradient:  0.09269781951660791
iteration : 13496
train acc:  0.8671875
train loss:  0.35066694021224976
train gradient:  0.15633725941626642
iteration : 13497
train acc:  0.859375
train loss:  0.3575630187988281
train gradient:  0.16308480632300354
iteration : 13498
train acc:  0.8359375
train loss:  0.36569899320602417
train gradient:  0.16897134194756236
iteration : 13499
train acc:  0.8359375
train loss:  0.37373465299606323
train gradient:  0.2289667117023423
iteration : 13500
train acc:  0.84375
train loss:  0.2974459230899811
train gradient:  0.09290272692700019
iteration : 13501
train acc:  0.8125
train loss:  0.35012391209602356
train gradient:  0.13015131893240733
iteration : 13502
train acc:  0.8359375
train loss:  0.31368398666381836
train gradient:  0.16262340198230488
iteration : 13503
train acc:  0.7734375
train loss:  0.43508613109588623
train gradient:  0.25349112095003506
iteration : 13504
train acc:  0.8984375
train loss:  0.26557987928390503
train gradient:  0.12210574514644904
iteration : 13505
train acc:  0.8515625
train loss:  0.35214003920555115
train gradient:  0.12075328246822437
iteration : 13506
train acc:  0.9140625
train loss:  0.31933102011680603
train gradient:  0.10281868093924493
iteration : 13507
train acc:  0.8359375
train loss:  0.3861325979232788
train gradient:  0.20157765833753322
iteration : 13508
train acc:  0.8671875
train loss:  0.36462438106536865
train gradient:  0.14538920757958673
iteration : 13509
train acc:  0.8515625
train loss:  0.4304283857345581
train gradient:  0.1977938886844789
iteration : 13510
train acc:  0.8203125
train loss:  0.33396944403648376
train gradient:  0.25071767229505193
iteration : 13511
train acc:  0.8046875
train loss:  0.38780754804611206
train gradient:  0.160786407688117
iteration : 13512
train acc:  0.859375
train loss:  0.3083553910255432
train gradient:  0.120632952563359
iteration : 13513
train acc:  0.8828125
train loss:  0.2879815995693207
train gradient:  0.1492282471709262
iteration : 13514
train acc:  0.8125
train loss:  0.36618128418922424
train gradient:  0.13946779581027868
iteration : 13515
train acc:  0.90625
train loss:  0.2667972445487976
train gradient:  0.24164505412223075
iteration : 13516
train acc:  0.8671875
train loss:  0.3279929757118225
train gradient:  0.13273243595820697
iteration : 13517
train acc:  0.828125
train loss:  0.38274598121643066
train gradient:  0.15233240680733798
iteration : 13518
train acc:  0.8203125
train loss:  0.39807969331741333
train gradient:  0.22592302642495204
iteration : 13519
train acc:  0.8359375
train loss:  0.3176518976688385
train gradient:  0.11771680376390806
iteration : 13520
train acc:  0.8671875
train loss:  0.2911628484725952
train gradient:  0.10803954649385032
iteration : 13521
train acc:  0.90625
train loss:  0.23259037733078003
train gradient:  0.1327077476683281
iteration : 13522
train acc:  0.8828125
train loss:  0.32626524567604065
train gradient:  0.09921834975712274
iteration : 13523
train acc:  0.921875
train loss:  0.2739817798137665
train gradient:  0.09305499324295402
iteration : 13524
train acc:  0.8671875
train loss:  0.32500994205474854
train gradient:  0.17507221557129055
iteration : 13525
train acc:  0.890625
train loss:  0.262696236371994
train gradient:  0.1104887287377261
iteration : 13526
train acc:  0.8828125
train loss:  0.2770744562149048
train gradient:  0.10642633335755768
iteration : 13527
train acc:  0.8359375
train loss:  0.32940810918807983
train gradient:  0.14558997208078434
iteration : 13528
train acc:  0.890625
train loss:  0.2580500841140747
train gradient:  0.10427421559397597
iteration : 13529
train acc:  0.828125
train loss:  0.3885486423969269
train gradient:  0.3279948523692944
iteration : 13530
train acc:  0.875
train loss:  0.2617429494857788
train gradient:  0.08082939946777075
iteration : 13531
train acc:  0.875
train loss:  0.3278737962245941
train gradient:  0.11377660418254913
iteration : 13532
train acc:  0.84375
train loss:  0.34735310077667236
train gradient:  0.13430920097440535
iteration : 13533
train acc:  0.875
train loss:  0.26732444763183594
train gradient:  0.09830167056509255
iteration : 13534
train acc:  0.84375
train loss:  0.3609517812728882
train gradient:  0.1371540223101378
iteration : 13535
train acc:  0.8359375
train loss:  0.3619111478328705
train gradient:  0.11087585469614857
iteration : 13536
train acc:  0.8515625
train loss:  0.41271907091140747
train gradient:  0.2883316624799933
iteration : 13537
train acc:  0.8984375
train loss:  0.2646777033805847
train gradient:  0.14323638832764024
iteration : 13538
train acc:  0.8515625
train loss:  0.3073865473270416
train gradient:  0.11533942506376814
iteration : 13539
train acc:  0.84375
train loss:  0.3401230573654175
train gradient:  0.11147086847524673
iteration : 13540
train acc:  0.859375
train loss:  0.3046209514141083
train gradient:  0.12187649021789305
iteration : 13541
train acc:  0.8671875
train loss:  0.3176195025444031
train gradient:  0.16205187510979677
iteration : 13542
train acc:  0.8671875
train loss:  0.30425676703453064
train gradient:  0.0988817573723978
iteration : 13543
train acc:  0.859375
train loss:  0.32301801443099976
train gradient:  0.18403260485116002
iteration : 13544
train acc:  0.8515625
train loss:  0.34081482887268066
train gradient:  0.10532351851629225
iteration : 13545
train acc:  0.8359375
train loss:  0.38794204592704773
train gradient:  0.1962761787259536
iteration : 13546
train acc:  0.859375
train loss:  0.32976436614990234
train gradient:  0.13496822583569174
iteration : 13547
train acc:  0.875
train loss:  0.35590431094169617
train gradient:  0.15385781942746884
iteration : 13548
train acc:  0.828125
train loss:  0.3556973338127136
train gradient:  0.16162653125455725
iteration : 13549
train acc:  0.859375
train loss:  0.3246256709098816
train gradient:  0.12758871855480308
iteration : 13550
train acc:  0.84375
train loss:  0.27803972363471985
train gradient:  0.12699637553844476
iteration : 13551
train acc:  0.875
train loss:  0.31104356050491333
train gradient:  0.15906292169260822
iteration : 13552
train acc:  0.7890625
train loss:  0.40002578496932983
train gradient:  0.14598356648922467
iteration : 13553
train acc:  0.84375
train loss:  0.3393535315990448
train gradient:  0.19723472339396958
iteration : 13554
train acc:  0.90625
train loss:  0.23845213651657104
train gradient:  0.09896818013292948
iteration : 13555
train acc:  0.8125
train loss:  0.3433360457420349
train gradient:  0.190766079765317
iteration : 13556
train acc:  0.890625
train loss:  0.2714079022407532
train gradient:  0.08740197043313239
iteration : 13557
train acc:  0.8671875
train loss:  0.29487186670303345
train gradient:  0.10676412955476178
iteration : 13558
train acc:  0.8046875
train loss:  0.38725119829177856
train gradient:  0.14721741968357072
iteration : 13559
train acc:  0.8515625
train loss:  0.3335498571395874
train gradient:  0.12296735480815382
iteration : 13560
train acc:  0.859375
train loss:  0.300586998462677
train gradient:  0.14038581219469604
iteration : 13561
train acc:  0.875
train loss:  0.2982133626937866
train gradient:  0.17331065772677792
iteration : 13562
train acc:  0.84375
train loss:  0.3426191508769989
train gradient:  0.14083041125263523
iteration : 13563
train acc:  0.828125
train loss:  0.32888534665107727
train gradient:  0.1319442667366112
iteration : 13564
train acc:  0.8984375
train loss:  0.27781277894973755
train gradient:  0.09161416134854884
iteration : 13565
train acc:  0.828125
train loss:  0.35892388224601746
train gradient:  0.22485499096228873
iteration : 13566
train acc:  0.859375
train loss:  0.3867000639438629
train gradient:  0.14341002990124904
iteration : 13567
train acc:  0.8359375
train loss:  0.35289210081100464
train gradient:  0.1419871618921013
iteration : 13568
train acc:  0.875
train loss:  0.27058616280555725
train gradient:  0.09515444306606395
iteration : 13569
train acc:  0.8671875
train loss:  0.36914411187171936
train gradient:  0.11756240221613222
iteration : 13570
train acc:  0.8359375
train loss:  0.3534238338470459
train gradient:  0.15648642477812863
iteration : 13571
train acc:  0.8671875
train loss:  0.32756471633911133
train gradient:  0.10374345220980749
iteration : 13572
train acc:  0.859375
train loss:  0.32677900791168213
train gradient:  0.12058535742566144
iteration : 13573
train acc:  0.8359375
train loss:  0.3259055018424988
train gradient:  0.1390514125686127
iteration : 13574
train acc:  0.8671875
train loss:  0.3140917122364044
train gradient:  0.18413149719216154
iteration : 13575
train acc:  0.8125
train loss:  0.3636115789413452
train gradient:  0.1162535216663997
iteration : 13576
train acc:  0.8203125
train loss:  0.38693761825561523
train gradient:  0.15519371820187383
iteration : 13577
train acc:  0.859375
train loss:  0.31779831647872925
train gradient:  0.12092816984719076
iteration : 13578
train acc:  0.84375
train loss:  0.37447866797447205
train gradient:  0.1373406262273012
iteration : 13579
train acc:  0.8984375
train loss:  0.2586979866027832
train gradient:  0.10484526173888135
iteration : 13580
train acc:  0.8359375
train loss:  0.38541629910469055
train gradient:  0.20099920907459176
iteration : 13581
train acc:  0.875
train loss:  0.3374131917953491
train gradient:  0.11603270631480213
iteration : 13582
train acc:  0.8828125
train loss:  0.35428541898727417
train gradient:  0.19610641564084308
iteration : 13583
train acc:  0.8984375
train loss:  0.2763634920120239
train gradient:  0.12419338559223149
iteration : 13584
train acc:  0.84375
train loss:  0.3500595688819885
train gradient:  0.13459269098566687
iteration : 13585
train acc:  0.890625
train loss:  0.26855596899986267
train gradient:  0.07684544623760628
iteration : 13586
train acc:  0.890625
train loss:  0.27658113837242126
train gradient:  0.11708544372959671
iteration : 13587
train acc:  0.8203125
train loss:  0.37325364351272583
train gradient:  0.15724886427128792
iteration : 13588
train acc:  0.859375
train loss:  0.33123186230659485
train gradient:  0.15026258991407923
iteration : 13589
train acc:  0.890625
train loss:  0.3039000630378723
train gradient:  0.15348123920816528
iteration : 13590
train acc:  0.8515625
train loss:  0.33508652448654175
train gradient:  0.13562882480980695
iteration : 13591
train acc:  0.8125
train loss:  0.4115206301212311
train gradient:  0.19189916366073212
iteration : 13592
train acc:  0.8359375
train loss:  0.3716009855270386
train gradient:  0.1669101337263475
iteration : 13593
train acc:  0.875
train loss:  0.3060179650783539
train gradient:  0.12109729314278166
iteration : 13594
train acc:  0.90625
train loss:  0.26973581314086914
train gradient:  0.13284083903167795
iteration : 13595
train acc:  0.875
train loss:  0.27555379271507263
train gradient:  0.1324415099372067
iteration : 13596
train acc:  0.859375
train loss:  0.32850298285484314
train gradient:  0.11933254439497676
iteration : 13597
train acc:  0.8671875
train loss:  0.3175513446331024
train gradient:  0.08191034354901885
iteration : 13598
train acc:  0.890625
train loss:  0.27667364478111267
train gradient:  0.08742819518503622
iteration : 13599
train acc:  0.8515625
train loss:  0.36781802773475647
train gradient:  0.1464943819070677
iteration : 13600
train acc:  0.8203125
train loss:  0.40782982110977173
train gradient:  0.1866338174883253
iteration : 13601
train acc:  0.875
train loss:  0.30821460485458374
train gradient:  0.09107590157224094
iteration : 13602
train acc:  0.859375
train loss:  0.3595086336135864
train gradient:  0.13404669793787102
iteration : 13603
train acc:  0.8984375
train loss:  0.2677293121814728
train gradient:  0.10628986803378218
iteration : 13604
train acc:  0.875
train loss:  0.3223462700843811
train gradient:  0.14914318633680265
iteration : 13605
train acc:  0.8671875
train loss:  0.2791125774383545
train gradient:  0.08673118905687895
iteration : 13606
train acc:  0.8515625
train loss:  0.26347362995147705
train gradient:  0.09918508804943171
iteration : 13607
train acc:  0.84375
train loss:  0.34302350878715515
train gradient:  0.1255644130796948
iteration : 13608
train acc:  0.859375
train loss:  0.29439979791641235
train gradient:  0.11363533765344153
iteration : 13609
train acc:  0.890625
train loss:  0.2829728126525879
train gradient:  0.11305481981207965
iteration : 13610
train acc:  0.90625
train loss:  0.25456878542900085
train gradient:  0.0733833571291775
iteration : 13611
train acc:  0.859375
train loss:  0.294454962015152
train gradient:  0.11863589938894507
iteration : 13612
train acc:  0.8828125
train loss:  0.26424986124038696
train gradient:  0.08992131553743264
iteration : 13613
train acc:  0.8828125
train loss:  0.26262879371643066
train gradient:  0.11103939145467652
iteration : 13614
train acc:  0.7890625
train loss:  0.3971464931964874
train gradient:  0.17734452450536503
iteration : 13615
train acc:  0.8984375
train loss:  0.2535398304462433
train gradient:  0.11542357312144275
iteration : 13616
train acc:  0.8671875
train loss:  0.3241674304008484
train gradient:  0.1136317127028156
iteration : 13617
train acc:  0.9296875
train loss:  0.2413935512304306
train gradient:  0.08758348975866194
iteration : 13618
train acc:  0.890625
train loss:  0.29389166831970215
train gradient:  0.10760757959242918
iteration : 13619
train acc:  0.859375
train loss:  0.26554813981056213
train gradient:  0.10642242821989673
iteration : 13620
train acc:  0.828125
train loss:  0.3638576865196228
train gradient:  0.23611295113294228
iteration : 13621
train acc:  0.8359375
train loss:  0.326504111289978
train gradient:  0.14493212491438062
iteration : 13622
train acc:  0.859375
train loss:  0.2995642125606537
train gradient:  0.12568867035485992
iteration : 13623
train acc:  0.84375
train loss:  0.30273598432540894
train gradient:  0.15799684825958954
iteration : 13624
train acc:  0.84375
train loss:  0.31402334570884705
train gradient:  0.12468908265447327
iteration : 13625
train acc:  0.8671875
train loss:  0.30901795625686646
train gradient:  0.11495670502585671
iteration : 13626
train acc:  0.8984375
train loss:  0.24110263586044312
train gradient:  0.0973107989422026
iteration : 13627
train acc:  0.84375
train loss:  0.32157647609710693
train gradient:  0.12293173174016234
iteration : 13628
train acc:  0.8671875
train loss:  0.3066401481628418
train gradient:  0.1375160958603982
iteration : 13629
train acc:  0.890625
train loss:  0.32447585463523865
train gradient:  0.11710062150018546
iteration : 13630
train acc:  0.84375
train loss:  0.34713777899742126
train gradient:  0.1557685495397093
iteration : 13631
train acc:  0.8828125
train loss:  0.3043241500854492
train gradient:  0.1228552799459112
iteration : 13632
train acc:  0.90625
train loss:  0.2957489788532257
train gradient:  0.17523871437726626
iteration : 13633
train acc:  0.890625
train loss:  0.26438960433006287
train gradient:  0.1172564049688916
iteration : 13634
train acc:  0.890625
train loss:  0.29979366064071655
train gradient:  0.11947938458828136
iteration : 13635
train acc:  0.859375
train loss:  0.3288583755493164
train gradient:  0.13231024019894158
iteration : 13636
train acc:  0.90625
train loss:  0.2920786142349243
train gradient:  0.22230957114849328
iteration : 13637
train acc:  0.9453125
train loss:  0.21204543113708496
train gradient:  0.08421311349651565
iteration : 13638
train acc:  0.90625
train loss:  0.24324285984039307
train gradient:  0.10886888062797308
iteration : 13639
train acc:  0.875
train loss:  0.322210431098938
train gradient:  0.1479781882872691
iteration : 13640
train acc:  0.8515625
train loss:  0.36216843128204346
train gradient:  0.15592538441526826
iteration : 13641
train acc:  0.875
train loss:  0.2656494081020355
train gradient:  0.08534031242710947
iteration : 13642
train acc:  0.9140625
train loss:  0.2573704719543457
train gradient:  0.13889787388428265
iteration : 13643
train acc:  0.8515625
train loss:  0.3167368173599243
train gradient:  0.13236853759600858
iteration : 13644
train acc:  0.8515625
train loss:  0.3162272274494171
train gradient:  0.15457100657030629
iteration : 13645
train acc:  0.8671875
train loss:  0.3018306493759155
train gradient:  0.1969338421797553
iteration : 13646
train acc:  0.828125
train loss:  0.3359130620956421
train gradient:  0.1561256419207071
iteration : 13647
train acc:  0.859375
train loss:  0.40547728538513184
train gradient:  0.29383284671541454
iteration : 13648
train acc:  0.8046875
train loss:  0.41806861758232117
train gradient:  0.18339482633138443
iteration : 13649
train acc:  0.859375
train loss:  0.34823668003082275
train gradient:  0.13255581208002568
iteration : 13650
train acc:  0.9296875
train loss:  0.2205142080783844
train gradient:  0.08604750937166952
iteration : 13651
train acc:  0.875
train loss:  0.27887243032455444
train gradient:  0.11167356677114701
iteration : 13652
train acc:  0.78125
train loss:  0.40678510069847107
train gradient:  0.303055357146036
iteration : 13653
train acc:  0.8359375
train loss:  0.443300724029541
train gradient:  0.2607303944680216
iteration : 13654
train acc:  0.8515625
train loss:  0.2920684814453125
train gradient:  0.10416587223096797
iteration : 13655
train acc:  0.8828125
train loss:  0.33750852942466736
train gradient:  0.14033823438835225
iteration : 13656
train acc:  0.90625
train loss:  0.2546161413192749
train gradient:  0.13771190744678613
iteration : 13657
train acc:  0.8984375
train loss:  0.29135316610336304
train gradient:  0.10678962200559682
iteration : 13658
train acc:  0.875
train loss:  0.32620298862457275
train gradient:  0.18093616715258054
iteration : 13659
train acc:  0.828125
train loss:  0.2911764979362488
train gradient:  0.13064865992650837
iteration : 13660
train acc:  0.8125
train loss:  0.3956737518310547
train gradient:  0.2508397651039005
iteration : 13661
train acc:  0.8671875
train loss:  0.2755706310272217
train gradient:  0.16148235483217427
iteration : 13662
train acc:  0.8515625
train loss:  0.36258190870285034
train gradient:  0.1844102107439612
iteration : 13663
train acc:  0.875
train loss:  0.2993611693382263
train gradient:  0.12991748327662597
iteration : 13664
train acc:  0.8828125
train loss:  0.28465884923934937
train gradient:  0.12112142235898862
iteration : 13665
train acc:  0.890625
train loss:  0.3359430134296417
train gradient:  0.20043778744259563
iteration : 13666
train acc:  0.8125
train loss:  0.38559043407440186
train gradient:  0.15825745435835442
iteration : 13667
train acc:  0.8515625
train loss:  0.3418618440628052
train gradient:  0.1204631008936538
iteration : 13668
train acc:  0.875
train loss:  0.3001684546470642
train gradient:  0.14959032833828623
iteration : 13669
train acc:  0.828125
train loss:  0.3488287925720215
train gradient:  0.13709652048101023
iteration : 13670
train acc:  0.8828125
train loss:  0.2963142991065979
train gradient:  0.1368961645374614
iteration : 13671
train acc:  0.84375
train loss:  0.3667908310890198
train gradient:  0.23809310335291703
iteration : 13672
train acc:  0.8671875
train loss:  0.3061894178390503
train gradient:  0.1441069110032766
iteration : 13673
train acc:  0.8515625
train loss:  0.32881590723991394
train gradient:  0.12730535154452316
iteration : 13674
train acc:  0.890625
train loss:  0.3232072591781616
train gradient:  0.13513550504422756
iteration : 13675
train acc:  0.875
train loss:  0.38057631254196167
train gradient:  0.1931698201712474
iteration : 13676
train acc:  0.8203125
train loss:  0.46821486949920654
train gradient:  0.2830170342460692
iteration : 13677
train acc:  0.8828125
train loss:  0.3419339954853058
train gradient:  0.14411057893007212
iteration : 13678
train acc:  0.8984375
train loss:  0.2756887674331665
train gradient:  0.11134033532067023
iteration : 13679
train acc:  0.8828125
train loss:  0.24426209926605225
train gradient:  0.10287976973526154
iteration : 13680
train acc:  0.8359375
train loss:  0.3936939537525177
train gradient:  0.16656670348618638
iteration : 13681
train acc:  0.921875
train loss:  0.22438833117485046
train gradient:  0.08060718909468056
iteration : 13682
train acc:  0.921875
train loss:  0.23998865485191345
train gradient:  0.08526289258797609
iteration : 13683
train acc:  0.8359375
train loss:  0.30904924869537354
train gradient:  0.15888931093295963
iteration : 13684
train acc:  0.828125
train loss:  0.44824013113975525
train gradient:  0.2370871952503717
iteration : 13685
train acc:  0.8671875
train loss:  0.34037578105926514
train gradient:  0.12346682483186786
iteration : 13686
train acc:  0.8125
train loss:  0.40045857429504395
train gradient:  0.22899947727677716
iteration : 13687
train acc:  0.8515625
train loss:  0.3665573000907898
train gradient:  0.1465570386872717
iteration : 13688
train acc:  0.84375
train loss:  0.35103729367256165
train gradient:  0.15892561880723952
iteration : 13689
train acc:  0.875
train loss:  0.28001153469085693
train gradient:  0.15103066689091804
iteration : 13690
train acc:  0.8984375
train loss:  0.2321723997592926
train gradient:  0.10046436830617592
iteration : 13691
train acc:  0.859375
train loss:  0.3166525363922119
train gradient:  0.14394042438888732
iteration : 13692
train acc:  0.8671875
train loss:  0.3336701989173889
train gradient:  0.11399209117827572
iteration : 13693
train acc:  0.90625
train loss:  0.2688107490539551
train gradient:  0.14302842372188276
iteration : 13694
train acc:  0.890625
train loss:  0.2699163854122162
train gradient:  0.09658818875561616
iteration : 13695
train acc:  0.8671875
train loss:  0.2772005498409271
train gradient:  0.11219163568323644
iteration : 13696
train acc:  0.8671875
train loss:  0.28196126222610474
train gradient:  0.1311766413900055
iteration : 13697
train acc:  0.8984375
train loss:  0.2596728205680847
train gradient:  0.09790612847694935
iteration : 13698
train acc:  0.8125
train loss:  0.39942413568496704
train gradient:  0.24612865771584197
iteration : 13699
train acc:  0.8671875
train loss:  0.2792803645133972
train gradient:  0.0991189629826307
iteration : 13700
train acc:  0.8515625
train loss:  0.29422634840011597
train gradient:  0.11519293501658574
iteration : 13701
train acc:  0.8984375
train loss:  0.2347278892993927
train gradient:  0.08116733570397379
iteration : 13702
train acc:  0.859375
train loss:  0.3247155249118805
train gradient:  0.16214774285454703
iteration : 13703
train acc:  0.8515625
train loss:  0.298624724149704
train gradient:  0.13462834794861622
iteration : 13704
train acc:  0.84375
train loss:  0.38593974709510803
train gradient:  0.2411270340342928
iteration : 13705
train acc:  0.8984375
train loss:  0.3160797953605652
train gradient:  0.13462790050088966
iteration : 13706
train acc:  0.890625
train loss:  0.2672573924064636
train gradient:  0.1218015345815699
iteration : 13707
train acc:  0.84375
train loss:  0.30061113834381104
train gradient:  0.13777924341190953
iteration : 13708
train acc:  0.8671875
train loss:  0.27191105484962463
train gradient:  0.1358107926451016
iteration : 13709
train acc:  0.8515625
train loss:  0.3438376784324646
train gradient:  0.1591900405004615
iteration : 13710
train acc:  0.828125
train loss:  0.38669389486312866
train gradient:  0.184525617147332
iteration : 13711
train acc:  0.8359375
train loss:  0.43001043796539307
train gradient:  0.15482121626228662
iteration : 13712
train acc:  0.8359375
train loss:  0.34539344906806946
train gradient:  0.16312584145637773
iteration : 13713
train acc:  0.796875
train loss:  0.4108114242553711
train gradient:  0.2039879084630442
iteration : 13714
train acc:  0.8828125
train loss:  0.3006131649017334
train gradient:  0.13425108170582245
iteration : 13715
train acc:  0.8203125
train loss:  0.37213215231895447
train gradient:  0.25543028246276384
iteration : 13716
train acc:  0.859375
train loss:  0.3101402223110199
train gradient:  0.12218584496768842
iteration : 13717
train acc:  0.8515625
train loss:  0.31253379583358765
train gradient:  0.12405238682740846
iteration : 13718
train acc:  0.859375
train loss:  0.31590867042541504
train gradient:  0.1402350798482781
iteration : 13719
train acc:  0.875
train loss:  0.31032609939575195
train gradient:  0.16416296803793407
iteration : 13720
train acc:  0.8671875
train loss:  0.2947297692298889
train gradient:  0.0927232913707839
iteration : 13721
train acc:  0.8671875
train loss:  0.30230459570884705
train gradient:  0.1447219439130903
iteration : 13722
train acc:  0.8828125
train loss:  0.2836688160896301
train gradient:  0.1355657670749834
iteration : 13723
train acc:  0.8515625
train loss:  0.32169824838638306
train gradient:  0.16060913679489863
iteration : 13724
train acc:  0.9375
train loss:  0.22528624534606934
train gradient:  0.1878777267477517
iteration : 13725
train acc:  0.8125
train loss:  0.3577139973640442
train gradient:  0.1902806299307802
iteration : 13726
train acc:  0.8828125
train loss:  0.23847994208335876
train gradient:  0.0945958529633407
iteration : 13727
train acc:  0.875
train loss:  0.29101961851119995
train gradient:  0.11244542699253318
iteration : 13728
train acc:  0.828125
train loss:  0.34448543190956116
train gradient:  0.14431955405116834
iteration : 13729
train acc:  0.890625
train loss:  0.25932902097702026
train gradient:  0.14463117122952052
iteration : 13730
train acc:  0.828125
train loss:  0.347984254360199
train gradient:  0.14886736591343308
iteration : 13731
train acc:  0.890625
train loss:  0.27556151151657104
train gradient:  0.2147955146615399
iteration : 13732
train acc:  0.8984375
train loss:  0.25938260555267334
train gradient:  0.09295834481706766
iteration : 13733
train acc:  0.828125
train loss:  0.3490583300590515
train gradient:  0.12754336255572224
iteration : 13734
train acc:  0.8671875
train loss:  0.3497074842453003
train gradient:  0.1557906413825003
iteration : 13735
train acc:  0.8671875
train loss:  0.2685082256793976
train gradient:  0.16122882133804564
iteration : 13736
train acc:  0.90625
train loss:  0.32801079750061035
train gradient:  0.15877524674379379
iteration : 13737
train acc:  0.90625
train loss:  0.2571939527988434
train gradient:  0.0955729037694618
iteration : 13738
train acc:  0.8515625
train loss:  0.33527278900146484
train gradient:  0.12146879297035344
iteration : 13739
train acc:  0.796875
train loss:  0.42151665687561035
train gradient:  0.21171476491737545
iteration : 13740
train acc:  0.890625
train loss:  0.2677302360534668
train gradient:  0.09657302707932543
iteration : 13741
train acc:  0.8671875
train loss:  0.31524237990379333
train gradient:  0.33466849170004653
iteration : 13742
train acc:  0.8828125
train loss:  0.302353173494339
train gradient:  0.11383797870479773
iteration : 13743
train acc:  0.875
train loss:  0.32747459411621094
train gradient:  0.14946120838756677
iteration : 13744
train acc:  0.828125
train loss:  0.3118214011192322
train gradient:  0.12460018768686575
iteration : 13745
train acc:  0.828125
train loss:  0.37876906991004944
train gradient:  0.1728815631456589
iteration : 13746
train acc:  0.890625
train loss:  0.2589356005191803
train gradient:  0.10119758743109115
iteration : 13747
train acc:  0.828125
train loss:  0.3883384168148041
train gradient:  0.18924154675084223
iteration : 13748
train acc:  0.8515625
train loss:  0.3003676235675812
train gradient:  0.11145094222971588
iteration : 13749
train acc:  0.90625
train loss:  0.2899267077445984
train gradient:  0.1547033150736192
iteration : 13750
train acc:  0.875
train loss:  0.3309047818183899
train gradient:  0.12501509744445172
iteration : 13751
train acc:  0.8984375
train loss:  0.2553846836090088
train gradient:  0.1215136803452032
iteration : 13752
train acc:  0.8671875
train loss:  0.30260729789733887
train gradient:  0.10738094667638014
iteration : 13753
train acc:  0.875
train loss:  0.26998084783554077
train gradient:  0.12148534604708597
iteration : 13754
train acc:  0.8359375
train loss:  0.31109854578971863
train gradient:  0.09857225900170892
iteration : 13755
train acc:  0.8828125
train loss:  0.2586306631565094
train gradient:  0.09903734624763803
iteration : 13756
train acc:  0.8515625
train loss:  0.3396327793598175
train gradient:  0.17616072767740698
iteration : 13757
train acc:  0.828125
train loss:  0.3186641335487366
train gradient:  0.13004617785742179
iteration : 13758
train acc:  0.875
train loss:  0.3135761022567749
train gradient:  0.12880405534205705
iteration : 13759
train acc:  0.796875
train loss:  0.4125133752822876
train gradient:  0.22608926338016958
iteration : 13760
train acc:  0.859375
train loss:  0.3130553364753723
train gradient:  0.12564720545790997
iteration : 13761
train acc:  0.8515625
train loss:  0.3211115598678589
train gradient:  0.14704068481202823
iteration : 13762
train acc:  0.8671875
train loss:  0.3038366734981537
train gradient:  0.1367225212231939
iteration : 13763
train acc:  0.8828125
train loss:  0.29218876361846924
train gradient:  0.09218463861694372
iteration : 13764
train acc:  0.8515625
train loss:  0.3169347047805786
train gradient:  0.13014118157608265
iteration : 13765
train acc:  0.921875
train loss:  0.22595468163490295
train gradient:  0.08681228787763429
iteration : 13766
train acc:  0.859375
train loss:  0.3370967507362366
train gradient:  0.16488385081813434
iteration : 13767
train acc:  0.875
train loss:  0.27368369698524475
train gradient:  0.0970018788627347
iteration : 13768
train acc:  0.875
train loss:  0.24705302715301514
train gradient:  0.11907543178895263
iteration : 13769
train acc:  0.8671875
train loss:  0.2999686598777771
train gradient:  0.11608457304308036
iteration : 13770
train acc:  0.8515625
train loss:  0.3355448842048645
train gradient:  0.1252194483213812
iteration : 13771
train acc:  0.890625
train loss:  0.2722126245498657
train gradient:  0.08587066354473198
iteration : 13772
train acc:  0.9140625
train loss:  0.22532817721366882
train gradient:  0.0870417337632058
iteration : 13773
train acc:  0.8828125
train loss:  0.28084659576416016
train gradient:  0.1410720637412407
iteration : 13774
train acc:  0.8671875
train loss:  0.3391386866569519
train gradient:  0.15277727175562883
iteration : 13775
train acc:  0.8828125
train loss:  0.2693334221839905
train gradient:  0.09767462960984148
iteration : 13776
train acc:  0.8671875
train loss:  0.27231359481811523
train gradient:  0.10409938017022966
iteration : 13777
train acc:  0.796875
train loss:  0.4289906322956085
train gradient:  0.19113801088096483
iteration : 13778
train acc:  0.828125
train loss:  0.3439933657646179
train gradient:  0.13673770620496087
iteration : 13779
train acc:  0.875
train loss:  0.2831652760505676
train gradient:  0.15691403669318893
iteration : 13780
train acc:  0.828125
train loss:  0.3623631000518799
train gradient:  0.17045562997864988
iteration : 13781
train acc:  0.828125
train loss:  0.37777218222618103
train gradient:  0.17942372131912976
iteration : 13782
train acc:  0.90625
train loss:  0.27558720111846924
train gradient:  0.12394845925648595
iteration : 13783
train acc:  0.828125
train loss:  0.37702134251594543
train gradient:  0.12146597924218606
iteration : 13784
train acc:  0.859375
train loss:  0.2926851212978363
train gradient:  0.15255236320965207
iteration : 13785
train acc:  0.859375
train loss:  0.3081716001033783
train gradient:  0.11672296804405179
iteration : 13786
train acc:  0.8671875
train loss:  0.33131518959999084
train gradient:  0.23000665681233565
iteration : 13787
train acc:  0.9296875
train loss:  0.23821347951889038
train gradient:  0.07560503310945738
iteration : 13788
train acc:  0.859375
train loss:  0.3075537085533142
train gradient:  0.11845353863052335
iteration : 13789
train acc:  0.8125
train loss:  0.41343992948532104
train gradient:  0.2049614060441426
iteration : 13790
train acc:  0.90625
train loss:  0.29365211725234985
train gradient:  0.09931490539908273
iteration : 13791
train acc:  0.84375
train loss:  0.3007974326610565
train gradient:  0.11445057390325444
iteration : 13792
train acc:  0.8671875
train loss:  0.3379865288734436
train gradient:  0.19093561186586872
iteration : 13793
train acc:  0.875
train loss:  0.31797587871551514
train gradient:  0.1351697194697126
iteration : 13794
train acc:  0.8828125
train loss:  0.25463059544563293
train gradient:  0.08889921770169715
iteration : 13795
train acc:  0.8671875
train loss:  0.2866553068161011
train gradient:  0.11220239612489157
iteration : 13796
train acc:  0.8125
train loss:  0.334524005651474
train gradient:  0.15916383450101562
iteration : 13797
train acc:  0.8828125
train loss:  0.2902873456478119
train gradient:  0.10748048045602798
iteration : 13798
train acc:  0.828125
train loss:  0.37443846464157104
train gradient:  0.1565212345982112
iteration : 13799
train acc:  0.8828125
train loss:  0.2604895234107971
train gradient:  0.09796666934213029
iteration : 13800
train acc:  0.859375
train loss:  0.3408334255218506
train gradient:  0.21416471377246948
iteration : 13801
train acc:  0.859375
train loss:  0.3206251859664917
train gradient:  0.1117617170296933
iteration : 13802
train acc:  0.859375
train loss:  0.30639010667800903
train gradient:  0.1347502913786927
iteration : 13803
train acc:  0.8515625
train loss:  0.30701079964637756
train gradient:  0.1239843021715984
iteration : 13804
train acc:  0.8984375
train loss:  0.30093085765838623
train gradient:  0.10490660357690325
iteration : 13805
train acc:  0.8984375
train loss:  0.3006187677383423
train gradient:  0.1385022235724454
iteration : 13806
train acc:  0.8828125
train loss:  0.30804958939552307
train gradient:  0.11114929941483878
iteration : 13807
train acc:  0.8203125
train loss:  0.3875866234302521
train gradient:  0.15693843606730293
iteration : 13808
train acc:  0.9140625
train loss:  0.24710386991500854
train gradient:  0.08526956914737861
iteration : 13809
train acc:  0.8671875
train loss:  0.35043007135391235
train gradient:  0.13111643521331334
iteration : 13810
train acc:  0.8828125
train loss:  0.2506195902824402
train gradient:  0.10242903296079177
iteration : 13811
train acc:  0.921875
train loss:  0.1964482218027115
train gradient:  0.07284841950456272
iteration : 13812
train acc:  0.84375
train loss:  0.3048684000968933
train gradient:  0.14205701206801316
iteration : 13813
train acc:  0.84375
train loss:  0.3784961402416229
train gradient:  0.15153079208719467
iteration : 13814
train acc:  0.8671875
train loss:  0.2946128845214844
train gradient:  0.10126194078430299
iteration : 13815
train acc:  0.875
train loss:  0.29372355341911316
train gradient:  0.11026887265803012
iteration : 13816
train acc:  0.890625
train loss:  0.244649738073349
train gradient:  0.12858431551310312
iteration : 13817
train acc:  0.859375
train loss:  0.32720404863357544
train gradient:  0.15615239325363164
iteration : 13818
train acc:  0.7890625
train loss:  0.4188767075538635
train gradient:  0.21593540325635507
iteration : 13819
train acc:  0.8359375
train loss:  0.346316397190094
train gradient:  0.19030904374994895
iteration : 13820
train acc:  0.8203125
train loss:  0.4078884720802307
train gradient:  0.2176661285258892
iteration : 13821
train acc:  0.8671875
train loss:  0.2604216933250427
train gradient:  0.09057136365562805
iteration : 13822
train acc:  0.8671875
train loss:  0.2887365520000458
train gradient:  0.10284815019111292
iteration : 13823
train acc:  0.859375
train loss:  0.33241403102874756
train gradient:  0.17148476733908188
iteration : 13824
train acc:  0.8515625
train loss:  0.3058393895626068
train gradient:  0.1267234624270258
iteration : 13825
train acc:  0.90625
train loss:  0.258140504360199
train gradient:  0.10066377139834203
iteration : 13826
train acc:  0.8671875
train loss:  0.30450552701950073
train gradient:  0.1389392416540063
iteration : 13827
train acc:  0.828125
train loss:  0.38410335779190063
train gradient:  0.16410030407914322
iteration : 13828
train acc:  0.859375
train loss:  0.34000205993652344
train gradient:  0.16854396564832733
iteration : 13829
train acc:  0.8515625
train loss:  0.3276861906051636
train gradient:  0.1434597666541936
iteration : 13830
train acc:  0.8984375
train loss:  0.26669856905937195
train gradient:  0.07223755050934991
iteration : 13831
train acc:  0.890625
train loss:  0.26698464155197144
train gradient:  0.0838463354476934
iteration : 13832
train acc:  0.8671875
train loss:  0.29685333371162415
train gradient:  0.11327377890712133
iteration : 13833
train acc:  0.8359375
train loss:  0.3308679163455963
train gradient:  0.1592206804729523
iteration : 13834
train acc:  0.859375
train loss:  0.32678163051605225
train gradient:  0.1134451618533633
iteration : 13835
train acc:  0.859375
train loss:  0.31133782863616943
train gradient:  0.14127548163413123
iteration : 13836
train acc:  0.8828125
train loss:  0.33145445585250854
train gradient:  0.1271845589990328
iteration : 13837
train acc:  0.875
train loss:  0.28458425402641296
train gradient:  0.11739683670429596
iteration : 13838
train acc:  0.875
train loss:  0.31742969155311584
train gradient:  0.10867251224461541
iteration : 13839
train acc:  0.8359375
train loss:  0.4118197560310364
train gradient:  0.2624801195171842
iteration : 13840
train acc:  0.8671875
train loss:  0.3436986207962036
train gradient:  0.16064693664214924
iteration : 13841
train acc:  0.8515625
train loss:  0.3053678870201111
train gradient:  0.13070231550112377
iteration : 13842
train acc:  0.8046875
train loss:  0.393929123878479
train gradient:  0.17491026275390995
iteration : 13843
train acc:  0.7734375
train loss:  0.45169973373413086
train gradient:  0.2919496326995496
iteration : 13844
train acc:  0.8828125
train loss:  0.27590256929397583
train gradient:  0.09242637548601466
iteration : 13845
train acc:  0.8203125
train loss:  0.3572782576084137
train gradient:  0.18909181429073627
iteration : 13846
train acc:  0.859375
train loss:  0.2864528298377991
train gradient:  0.1246795707394366
iteration : 13847
train acc:  0.8359375
train loss:  0.3773563504219055
train gradient:  0.191318126727264
iteration : 13848
train acc:  0.8359375
train loss:  0.386832594871521
train gradient:  0.18042814576505914
iteration : 13849
train acc:  0.8203125
train loss:  0.42248818278312683
train gradient:  0.23993489845027965
iteration : 13850
train acc:  0.8515625
train loss:  0.34353727102279663
train gradient:  0.11609585843819306
iteration : 13851
train acc:  0.8203125
train loss:  0.3486705422401428
train gradient:  0.20534262857677188
iteration : 13852
train acc:  0.8515625
train loss:  0.3213809132575989
train gradient:  0.1449735681610015
iteration : 13853
train acc:  0.84375
train loss:  0.3339995741844177
train gradient:  0.32606321327078086
iteration : 13854
train acc:  0.84375
train loss:  0.33981257677078247
train gradient:  0.2050654036460538
iteration : 13855
train acc:  0.8984375
train loss:  0.24492299556732178
train gradient:  0.08797770901633248
iteration : 13856
train acc:  0.8828125
train loss:  0.2970552444458008
train gradient:  0.08225219713783714
iteration : 13857
train acc:  0.875
train loss:  0.32042396068573
train gradient:  0.09348733063607177
iteration : 13858
train acc:  0.8359375
train loss:  0.382464200258255
train gradient:  0.14404272439232785
iteration : 13859
train acc:  0.828125
train loss:  0.3776213228702545
train gradient:  0.2339994657446673
iteration : 13860
train acc:  0.8984375
train loss:  0.3299250602722168
train gradient:  0.10936247227440825
iteration : 13861
train acc:  0.8828125
train loss:  0.25639650225639343
train gradient:  0.10992878522294235
iteration : 13862
train acc:  0.8671875
train loss:  0.3485582172870636
train gradient:  0.17950160656374778
iteration : 13863
train acc:  0.8515625
train loss:  0.3233712911605835
train gradient:  0.11163569968530182
iteration : 13864
train acc:  0.890625
train loss:  0.26696085929870605
train gradient:  0.10536805922604178
iteration : 13865
train acc:  0.9296875
train loss:  0.19594554603099823
train gradient:  0.09564130220651533
iteration : 13866
train acc:  0.875
train loss:  0.28463664650917053
train gradient:  0.10702296140933332
iteration : 13867
train acc:  0.875
train loss:  0.3240671157836914
train gradient:  0.2235316649034877
iteration : 13868
train acc:  0.890625
train loss:  0.282779723405838
train gradient:  0.11315634521332439
iteration : 13869
train acc:  0.9140625
train loss:  0.2508840560913086
train gradient:  0.09091936178776042
iteration : 13870
train acc:  0.828125
train loss:  0.33553600311279297
train gradient:  0.13476705650964077
iteration : 13871
train acc:  0.8671875
train loss:  0.30742329359054565
train gradient:  0.09140871185385371
iteration : 13872
train acc:  0.8515625
train loss:  0.3835619390010834
train gradient:  0.19325181605178304
iteration : 13873
train acc:  0.875
train loss:  0.32183894515037537
train gradient:  0.33263093764752444
iteration : 13874
train acc:  0.875
train loss:  0.3678327202796936
train gradient:  0.17841066912791048
iteration : 13875
train acc:  0.859375
train loss:  0.2932294011116028
train gradient:  0.10650111298555912
iteration : 13876
train acc:  0.796875
train loss:  0.4042675495147705
train gradient:  0.2312268158425096
iteration : 13877
train acc:  0.8671875
train loss:  0.2510106563568115
train gradient:  0.10246569057456434
iteration : 13878
train acc:  0.8359375
train loss:  0.3684864044189453
train gradient:  0.188187106096236
iteration : 13879
train acc:  0.90625
train loss:  0.2573241889476776
train gradient:  0.09616517695843174
iteration : 13880
train acc:  0.8984375
train loss:  0.2645779848098755
train gradient:  0.11996771083294659
iteration : 13881
train acc:  0.8828125
train loss:  0.3741493225097656
train gradient:  0.15819538178678264
iteration : 13882
train acc:  0.84375
train loss:  0.3070690631866455
train gradient:  0.12147682464406409
iteration : 13883
train acc:  0.8203125
train loss:  0.376603901386261
train gradient:  0.17170661612796334
iteration : 13884
train acc:  0.859375
train loss:  0.35763686895370483
train gradient:  0.11779808550860468
iteration : 13885
train acc:  0.890625
train loss:  0.3219755291938782
train gradient:  0.1523368829160684
iteration : 13886
train acc:  0.8515625
train loss:  0.30710798501968384
train gradient:  0.1262109258014522
iteration : 13887
train acc:  0.84375
train loss:  0.36652815341949463
train gradient:  0.15490258300466103
iteration : 13888
train acc:  0.890625
train loss:  0.31060612201690674
train gradient:  0.13401121696524868
iteration : 13889
train acc:  0.828125
train loss:  0.34683239459991455
train gradient:  0.11174348160273623
iteration : 13890
train acc:  0.84375
train loss:  0.338142067193985
train gradient:  0.15907364478937402
iteration : 13891
train acc:  0.859375
train loss:  0.39010465145111084
train gradient:  0.16904790360932237
iteration : 13892
train acc:  0.859375
train loss:  0.32907170057296753
train gradient:  0.1366644967515506
iteration : 13893
train acc:  0.84375
train loss:  0.31084781885147095
train gradient:  0.09838319106794809
iteration : 13894
train acc:  0.78125
train loss:  0.46328026056289673
train gradient:  0.21718040229851307
iteration : 13895
train acc:  0.875
train loss:  0.33625471591949463
train gradient:  0.11644685946110622
iteration : 13896
train acc:  0.9140625
train loss:  0.2195289582014084
train gradient:  0.10282223838285083
iteration : 13897
train acc:  0.8671875
train loss:  0.31494009494781494
train gradient:  0.12633926612341942
iteration : 13898
train acc:  0.84375
train loss:  0.38094842433929443
train gradient:  0.13289791514497365
iteration : 13899
train acc:  0.84375
train loss:  0.35607194900512695
train gradient:  0.12950865119293287
iteration : 13900
train acc:  0.859375
train loss:  0.2709120512008667
train gradient:  0.0958393394466462
iteration : 13901
train acc:  0.859375
train loss:  0.38383224606513977
train gradient:  0.1605790633989902
iteration : 13902
train acc:  0.8671875
train loss:  0.27678436040878296
train gradient:  0.10644347515522852
iteration : 13903
train acc:  0.84375
train loss:  0.3455653786659241
train gradient:  0.14466948285275266
iteration : 13904
train acc:  0.8828125
train loss:  0.33100616931915283
train gradient:  0.12909354546206214
iteration : 13905
train acc:  0.9140625
train loss:  0.26971834897994995
train gradient:  0.09877377102498569
iteration : 13906
train acc:  0.859375
train loss:  0.2845127284526825
train gradient:  0.10541553267945473
iteration : 13907
train acc:  0.875
train loss:  0.3278532028198242
train gradient:  0.11918920323601892
iteration : 13908
train acc:  0.8828125
train loss:  0.31640952825546265
train gradient:  0.11158812196437894
iteration : 13909
train acc:  0.8359375
train loss:  0.3075440526008606
train gradient:  0.09730033197296444
iteration : 13910
train acc:  0.8828125
train loss:  0.2902698516845703
train gradient:  0.09785075843841459
iteration : 13911
train acc:  0.90625
train loss:  0.27172496914863586
train gradient:  0.1029472927965728
iteration : 13912
train acc:  0.8125
train loss:  0.3992239534854889
train gradient:  0.1743382593164882
iteration : 13913
train acc:  0.859375
train loss:  0.34816092252731323
train gradient:  0.14518623502809858
iteration : 13914
train acc:  0.875
train loss:  0.3032752573490143
train gradient:  0.14059788820459101
iteration : 13915
train acc:  0.8515625
train loss:  0.26478028297424316
train gradient:  0.12297917889651051
iteration : 13916
train acc:  0.8828125
train loss:  0.2970547378063202
train gradient:  0.10204998730020562
iteration : 13917
train acc:  0.8359375
train loss:  0.40421992540359497
train gradient:  0.21063923301082985
iteration : 13918
train acc:  0.8203125
train loss:  0.35687255859375
train gradient:  0.167927432053043
iteration : 13919
train acc:  0.890625
train loss:  0.30237990617752075
train gradient:  0.11774091732480876
iteration : 13920
train acc:  0.8515625
train loss:  0.32321858406066895
train gradient:  0.12391884079751614
iteration : 13921
train acc:  0.828125
train loss:  0.4199259579181671
train gradient:  0.24521335587532167
iteration : 13922
train acc:  0.8046875
train loss:  0.3732822835445404
train gradient:  0.13873409151918295
iteration : 13923
train acc:  0.8671875
train loss:  0.27138346433639526
train gradient:  0.08969097932404056
iteration : 13924
train acc:  0.8671875
train loss:  0.29095232486724854
train gradient:  0.0941667614395431
iteration : 13925
train acc:  0.828125
train loss:  0.39556628465652466
train gradient:  0.2005960578192698
iteration : 13926
train acc:  0.8203125
train loss:  0.37501388788223267
train gradient:  0.16546320669927134
iteration : 13927
train acc:  0.90625
train loss:  0.2794090211391449
train gradient:  0.12739220348965308
iteration : 13928
train acc:  0.8046875
train loss:  0.32968807220458984
train gradient:  0.11706640658635645
iteration : 13929
train acc:  0.84375
train loss:  0.3129458427429199
train gradient:  0.12919623554394238
iteration : 13930
train acc:  0.8828125
train loss:  0.2905220687389374
train gradient:  0.0948647434426068
iteration : 13931
train acc:  0.8671875
train loss:  0.26206615567207336
train gradient:  0.08598374626371391
iteration : 13932
train acc:  0.84375
train loss:  0.3992150127887726
train gradient:  0.20017882882239463
iteration : 13933
train acc:  0.828125
train loss:  0.35056355595588684
train gradient:  0.1610193656095612
iteration : 13934
train acc:  0.796875
train loss:  0.4205440878868103
train gradient:  0.19830083719479497
iteration : 13935
train acc:  0.84375
train loss:  0.35044288635253906
train gradient:  0.14158569968793883
iteration : 13936
train acc:  0.828125
train loss:  0.3762516975402832
train gradient:  0.16394227244990303
iteration : 13937
train acc:  0.9140625
train loss:  0.2284070998430252
train gradient:  0.09194238604987486
iteration : 13938
train acc:  0.8828125
train loss:  0.2858648896217346
train gradient:  0.11173962322092831
iteration : 13939
train acc:  0.8046875
train loss:  0.4343283772468567
train gradient:  0.2340783921869969
iteration : 13940
train acc:  0.8359375
train loss:  0.33327722549438477
train gradient:  0.1425991658243094
iteration : 13941
train acc:  0.8203125
train loss:  0.34369200468063354
train gradient:  0.21744239141200458
iteration : 13942
train acc:  0.796875
train loss:  0.37989187240600586
train gradient:  0.22573043153841033
iteration : 13943
train acc:  0.875
train loss:  0.34325888752937317
train gradient:  0.1870032527916842
iteration : 13944
train acc:  0.8046875
train loss:  0.36746031045913696
train gradient:  0.12800136251568212
iteration : 13945
train acc:  0.875
train loss:  0.3387775421142578
train gradient:  0.11598671278008975
iteration : 13946
train acc:  0.8125
train loss:  0.35402798652648926
train gradient:  0.1630392745779271
iteration : 13947
train acc:  0.8515625
train loss:  0.2993229627609253
train gradient:  0.14472292283185692
iteration : 13948
train acc:  0.8828125
train loss:  0.3394888639450073
train gradient:  0.1118796510813044
iteration : 13949
train acc:  0.859375
train loss:  0.296793133020401
train gradient:  0.10755528476875953
iteration : 13950
train acc:  0.8984375
train loss:  0.26006245613098145
train gradient:  0.09364757935125466
iteration : 13951
train acc:  0.875
train loss:  0.2946473956108093
train gradient:  0.11486602658515169
iteration : 13952
train acc:  0.8359375
train loss:  0.3224698603153229
train gradient:  0.14747466709316764
iteration : 13953
train acc:  0.8828125
train loss:  0.2894737124443054
train gradient:  0.10101753337480655
iteration : 13954
train acc:  0.859375
train loss:  0.295113742351532
train gradient:  0.12206135584678335
iteration : 13955
train acc:  0.890625
train loss:  0.31729504466056824
train gradient:  0.11007974999105757
iteration : 13956
train acc:  0.84375
train loss:  0.3702406883239746
train gradient:  0.1532435439660006
iteration : 13957
train acc:  0.8671875
train loss:  0.3468027114868164
train gradient:  0.31220110653120453
iteration : 13958
train acc:  0.8671875
train loss:  0.30569958686828613
train gradient:  0.11623473316125141
iteration : 13959
train acc:  0.859375
train loss:  0.2674373686313629
train gradient:  0.07860591476280984
iteration : 13960
train acc:  0.8828125
train loss:  0.2556227147579193
train gradient:  0.1059816868306601
iteration : 13961
train acc:  0.875
train loss:  0.3118510842323303
train gradient:  0.14563933729830125
iteration : 13962
train acc:  0.859375
train loss:  0.31345120072364807
train gradient:  0.12363032734272877
iteration : 13963
train acc:  0.859375
train loss:  0.2997778654098511
train gradient:  0.1880558941502129
iteration : 13964
train acc:  0.859375
train loss:  0.34443992376327515
train gradient:  0.14884827028952133
iteration : 13965
train acc:  0.84375
train loss:  0.35824087262153625
train gradient:  0.15195777087036774
iteration : 13966
train acc:  0.84375
train loss:  0.36314135789871216
train gradient:  0.17268415525936853
iteration : 13967
train acc:  0.828125
train loss:  0.39111804962158203
train gradient:  0.1963443638491344
iteration : 13968
train acc:  0.78125
train loss:  0.4301972985267639
train gradient:  0.22895867903709866
iteration : 13969
train acc:  0.90625
train loss:  0.20544373989105225
train gradient:  0.07645617197448706
iteration : 13970
train acc:  0.8515625
train loss:  0.3365081250667572
train gradient:  0.12911434507671685
iteration : 13971
train acc:  0.8359375
train loss:  0.34222546219825745
train gradient:  0.1600286292407279
iteration : 13972
train acc:  0.8671875
train loss:  0.2951909303665161
train gradient:  0.0989405457324517
iteration : 13973
train acc:  0.8828125
train loss:  0.3165581226348877
train gradient:  0.14966892675404628
iteration : 13974
train acc:  0.890625
train loss:  0.30134618282318115
train gradient:  0.114581840475458
iteration : 13975
train acc:  0.8515625
train loss:  0.34630483388900757
train gradient:  0.13005350487406184
iteration : 13976
train acc:  0.859375
train loss:  0.3607921898365021
train gradient:  0.13679867986453745
iteration : 13977
train acc:  0.828125
train loss:  0.38366344571113586
train gradient:  0.16842545808492781
iteration : 13978
train acc:  0.859375
train loss:  0.2638170123100281
train gradient:  0.06637482269105138
iteration : 13979
train acc:  0.90625
train loss:  0.27393975853919983
train gradient:  0.09651603190346994
iteration : 13980
train acc:  0.859375
train loss:  0.26330891251564026
train gradient:  0.11497804361736444
iteration : 13981
train acc:  0.90625
train loss:  0.2604336142539978
train gradient:  0.072452550798001
iteration : 13982
train acc:  0.9375
train loss:  0.2435932159423828
train gradient:  0.08732318016215306
iteration : 13983
train acc:  0.875
train loss:  0.2502758204936981
train gradient:  0.07269015514070125
iteration : 13984
train acc:  0.8515625
train loss:  0.3441031575202942
train gradient:  0.11908053960992022
iteration : 13985
train acc:  0.8984375
train loss:  0.2711624503135681
train gradient:  0.09657627862973453
iteration : 13986
train acc:  0.8359375
train loss:  0.36066433787345886
train gradient:  0.18313005910546443
iteration : 13987
train acc:  0.921875
train loss:  0.25597652792930603
train gradient:  0.12231312250214205
iteration : 13988
train acc:  0.859375
train loss:  0.32748332619667053
train gradient:  0.10413352603846769
iteration : 13989
train acc:  0.90625
train loss:  0.25695812702178955
train gradient:  0.09689243662519348
iteration : 13990
train acc:  0.84375
train loss:  0.3682745099067688
train gradient:  0.15846674724985765
iteration : 13991
train acc:  0.8515625
train loss:  0.3405534625053406
train gradient:  0.1618241131263889
iteration : 13992
train acc:  0.8359375
train loss:  0.3901427388191223
train gradient:  0.18101392730529647
iteration : 13993
train acc:  0.875
train loss:  0.3577345907688141
train gradient:  0.16638333866760657
iteration : 13994
train acc:  0.875
train loss:  0.30037304759025574
train gradient:  0.11994000011206099
iteration : 13995
train acc:  0.90625
train loss:  0.3040459454059601
train gradient:  0.12438758935403013
iteration : 13996
train acc:  0.84375
train loss:  0.3548740744590759
train gradient:  0.14800068580435355
iteration : 13997
train acc:  0.8671875
train loss:  0.33996084332466125
train gradient:  0.1277041923614085
iteration : 13998
train acc:  0.8515625
train loss:  0.3473101854324341
train gradient:  0.13780429888314893
iteration : 13999
train acc:  0.9375
train loss:  0.21074765920639038
train gradient:  0.09559213322231454
iteration : 14000
train acc:  0.8515625
train loss:  0.3017024099826813
train gradient:  0.17002812192559863
iteration : 14001
train acc:  0.8671875
train loss:  0.3343510031700134
train gradient:  0.11026856970949267
iteration : 14002
train acc:  0.84375
train loss:  0.30051741003990173
train gradient:  0.11899593390596672
iteration : 14003
train acc:  0.8515625
train loss:  0.3473852872848511
train gradient:  0.15974836925847574
iteration : 14004
train acc:  0.8828125
train loss:  0.27474528551101685
train gradient:  0.1254009901564141
iteration : 14005
train acc:  0.90625
train loss:  0.2428756207227707
train gradient:  0.11491177253576733
iteration : 14006
train acc:  0.890625
train loss:  0.3410561680793762
train gradient:  0.13259860575444682
iteration : 14007
train acc:  0.8671875
train loss:  0.2737884223461151
train gradient:  0.10313773601429137
iteration : 14008
train acc:  0.828125
train loss:  0.31777772307395935
train gradient:  0.17971088052716486
iteration : 14009
train acc:  0.875
train loss:  0.27723073959350586
train gradient:  0.09775927966426799
iteration : 14010
train acc:  0.796875
train loss:  0.40481042861938477
train gradient:  0.22526653396813856
iteration : 14011
train acc:  0.8984375
train loss:  0.24013254046440125
train gradient:  0.11048038119900618
iteration : 14012
train acc:  0.8671875
train loss:  0.3412778377532959
train gradient:  0.131425736134285
iteration : 14013
train acc:  0.8203125
train loss:  0.34614700078964233
train gradient:  0.1470259965984124
iteration : 14014
train acc:  0.859375
train loss:  0.3709498643875122
train gradient:  0.19881491375573884
iteration : 14015
train acc:  0.8828125
train loss:  0.2839386463165283
train gradient:  0.106528750029559
iteration : 14016
train acc:  0.8515625
train loss:  0.29168522357940674
train gradient:  0.09280466188434855
iteration : 14017
train acc:  0.859375
train loss:  0.3137098550796509
train gradient:  0.1456250807138743
iteration : 14018
train acc:  0.9140625
train loss:  0.23822498321533203
train gradient:  0.059960242116407214
iteration : 14019
train acc:  0.875
train loss:  0.3184741139411926
train gradient:  0.14991022650683383
iteration : 14020
train acc:  0.859375
train loss:  0.3191472291946411
train gradient:  0.13820605531671054
iteration : 14021
train acc:  0.9140625
train loss:  0.20374663174152374
train gradient:  0.06766524185355965
iteration : 14022
train acc:  0.8828125
train loss:  0.2961054742336273
train gradient:  0.12989193088486778
iteration : 14023
train acc:  0.9140625
train loss:  0.23648448288440704
train gradient:  0.0858401854671507
iteration : 14024
train acc:  0.8359375
train loss:  0.4180574417114258
train gradient:  0.2972674171472835
iteration : 14025
train acc:  0.9140625
train loss:  0.21350163221359253
train gradient:  0.08912770713593621
iteration : 14026
train acc:  0.8671875
train loss:  0.30941158533096313
train gradient:  0.1363268911744971
iteration : 14027
train acc:  0.890625
train loss:  0.29456138610839844
train gradient:  0.1403985050192246
iteration : 14028
train acc:  0.8671875
train loss:  0.2970477342605591
train gradient:  0.18731408029303032
iteration : 14029
train acc:  0.84375
train loss:  0.3404527008533478
train gradient:  0.2236520390398597
iteration : 14030
train acc:  0.8046875
train loss:  0.3914102613925934
train gradient:  0.21333895466369587
iteration : 14031
train acc:  0.8671875
train loss:  0.28696462512016296
train gradient:  0.10905198951946268
iteration : 14032
train acc:  0.8828125
train loss:  0.27497363090515137
train gradient:  0.170106200832416
iteration : 14033
train acc:  0.90625
train loss:  0.23246030509471893
train gradient:  0.09037063793414925
iteration : 14034
train acc:  0.796875
train loss:  0.39247724413871765
train gradient:  0.23643073115555344
iteration : 14035
train acc:  0.8515625
train loss:  0.30679813027381897
train gradient:  0.14935764885861194
iteration : 14036
train acc:  0.8984375
train loss:  0.29684752225875854
train gradient:  0.1131956606167134
iteration : 14037
train acc:  0.8671875
train loss:  0.2901250123977661
train gradient:  0.15438192936235381
iteration : 14038
train acc:  0.890625
train loss:  0.29436033964157104
train gradient:  0.1472287146297961
iteration : 14039
train acc:  0.875
train loss:  0.3351101279258728
train gradient:  0.13099951962188766
iteration : 14040
train acc:  0.8359375
train loss:  0.3312748074531555
train gradient:  0.19092035694057172
iteration : 14041
train acc:  0.890625
train loss:  0.28090840578079224
train gradient:  0.11917172358117643
iteration : 14042
train acc:  0.921875
train loss:  0.2533894181251526
train gradient:  0.08061285132510469
iteration : 14043
train acc:  0.796875
train loss:  0.5244567394256592
train gradient:  0.30438933243562283
iteration : 14044
train acc:  0.7890625
train loss:  0.43869689106941223
train gradient:  0.23468842673950982
iteration : 14045
train acc:  0.859375
train loss:  0.35363849997520447
train gradient:  0.18826849032878235
iteration : 14046
train acc:  0.859375
train loss:  0.2779054045677185
train gradient:  0.1132771969404308
iteration : 14047
train acc:  0.796875
train loss:  0.3780030310153961
train gradient:  0.17056614707788742
iteration : 14048
train acc:  0.890625
train loss:  0.3131273090839386
train gradient:  0.14597342236829608
iteration : 14049
train acc:  0.890625
train loss:  0.30560922622680664
train gradient:  0.13418818883272243
iteration : 14050
train acc:  0.8984375
train loss:  0.30131399631500244
train gradient:  0.1286989036872659
iteration : 14051
train acc:  0.859375
train loss:  0.2807776927947998
train gradient:  0.0947187506167805
iteration : 14052
train acc:  0.84375
train loss:  0.3204837441444397
train gradient:  0.11900981079320026
iteration : 14053
train acc:  0.8671875
train loss:  0.26033082604408264
train gradient:  0.09317542286075883
iteration : 14054
train acc:  0.8203125
train loss:  0.3654698133468628
train gradient:  0.127346390522885
iteration : 14055
train acc:  0.859375
train loss:  0.28777390718460083
train gradient:  0.12644210427105979
iteration : 14056
train acc:  0.859375
train loss:  0.3296982944011688
train gradient:  0.12488966503042667
iteration : 14057
train acc:  0.921875
train loss:  0.22963695228099823
train gradient:  0.10261930768687677
iteration : 14058
train acc:  0.828125
train loss:  0.3701016306877136
train gradient:  0.16231621064455487
iteration : 14059
train acc:  0.8984375
train loss:  0.28991585969924927
train gradient:  0.1797147496112369
iteration : 14060
train acc:  0.796875
train loss:  0.4986063241958618
train gradient:  0.31071395872985275
iteration : 14061
train acc:  0.8515625
train loss:  0.28923994302749634
train gradient:  0.10172773956868124
iteration : 14062
train acc:  0.828125
train loss:  0.35522764921188354
train gradient:  0.19768351845631899
iteration : 14063
train acc:  0.8984375
train loss:  0.2618764638900757
train gradient:  0.10581654940627457
iteration : 14064
train acc:  0.8515625
train loss:  0.3666187524795532
train gradient:  0.132872685594994
iteration : 14065
train acc:  0.890625
train loss:  0.3046109676361084
train gradient:  0.09795569967589345
iteration : 14066
train acc:  0.875
train loss:  0.3067857623100281
train gradient:  0.15040366756956222
iteration : 14067
train acc:  0.8671875
train loss:  0.2599773406982422
train gradient:  0.0951462577003356
iteration : 14068
train acc:  0.875
train loss:  0.27342337369918823
train gradient:  0.10979167780621102
iteration : 14069
train acc:  0.84375
train loss:  0.3523050844669342
train gradient:  0.11172314092104403
iteration : 14070
train acc:  0.84375
train loss:  0.3531273305416107
train gradient:  0.17459462181698027
iteration : 14071
train acc:  0.9140625
train loss:  0.29794079065322876
train gradient:  0.19386265951987622
iteration : 14072
train acc:  0.7890625
train loss:  0.41909438371658325
train gradient:  0.23573762899977813
iteration : 14073
train acc:  0.8828125
train loss:  0.26601576805114746
train gradient:  0.09110711493990084
iteration : 14074
train acc:  0.8828125
train loss:  0.28039824962615967
train gradient:  0.09360755744290751
iteration : 14075
train acc:  0.84375
train loss:  0.32314532995224
train gradient:  0.10410579396210336
iteration : 14076
train acc:  0.8359375
train loss:  0.32566824555397034
train gradient:  0.15075714092550213
iteration : 14077
train acc:  0.84375
train loss:  0.3099835216999054
train gradient:  0.11517969514148625
iteration : 14078
train acc:  0.8828125
train loss:  0.2974034547805786
train gradient:  0.11515196799583188
iteration : 14079
train acc:  0.90625
train loss:  0.2668336033821106
train gradient:  0.11383762268101658
iteration : 14080
train acc:  0.875
train loss:  0.32169127464294434
train gradient:  0.11781907344012997
iteration : 14081
train acc:  0.8203125
train loss:  0.38130587339401245
train gradient:  0.20888735197031189
iteration : 14082
train acc:  0.84375
train loss:  0.3351963758468628
train gradient:  0.17880650475235527
iteration : 14083
train acc:  0.8515625
train loss:  0.3252130448818207
train gradient:  0.1085584007370735
iteration : 14084
train acc:  0.859375
train loss:  0.3109819293022156
train gradient:  0.17384181297884493
iteration : 14085
train acc:  0.8125
train loss:  0.4032665491104126
train gradient:  0.1590442013345322
iteration : 14086
train acc:  0.8671875
train loss:  0.2991029918193817
train gradient:  0.12130391131118747
iteration : 14087
train acc:  0.8359375
train loss:  0.3382987976074219
train gradient:  0.25755880104360934
iteration : 14088
train acc:  0.8671875
train loss:  0.35662713646888733
train gradient:  0.19653189040337832
iteration : 14089
train acc:  0.8359375
train loss:  0.3667830526828766
train gradient:  0.18040920536672572
iteration : 14090
train acc:  0.8515625
train loss:  0.3159964084625244
train gradient:  0.10244002656154562
iteration : 14091
train acc:  0.890625
train loss:  0.3138170838356018
train gradient:  0.1477691995209883
iteration : 14092
train acc:  0.8359375
train loss:  0.34513425827026367
train gradient:  0.12412664236878519
iteration : 14093
train acc:  0.8828125
train loss:  0.3225811719894409
train gradient:  0.22444573329609985
iteration : 14094
train acc:  0.859375
train loss:  0.4021455943584442
train gradient:  0.21522215932927408
iteration : 14095
train acc:  0.828125
train loss:  0.387617826461792
train gradient:  0.20887034288990083
iteration : 14096
train acc:  0.8671875
train loss:  0.29576319456100464
train gradient:  0.14134889725900696
iteration : 14097
train acc:  0.890625
train loss:  0.272029310464859
train gradient:  0.10036711690118101
iteration : 14098
train acc:  0.921875
train loss:  0.22377218306064606
train gradient:  0.07629258453965498
iteration : 14099
train acc:  0.8203125
train loss:  0.3499818444252014
train gradient:  0.1529282113204455
iteration : 14100
train acc:  0.875
train loss:  0.3079424202442169
train gradient:  0.1267463437760532
iteration : 14101
train acc:  0.828125
train loss:  0.3458101153373718
train gradient:  0.19816904198053753
iteration : 14102
train acc:  0.859375
train loss:  0.31987571716308594
train gradient:  0.11361057552406362
iteration : 14103
train acc:  0.90625
train loss:  0.2652451992034912
train gradient:  0.10176720253025133
iteration : 14104
train acc:  0.875
train loss:  0.28266480565071106
train gradient:  0.14394778248479279
iteration : 14105
train acc:  0.8515625
train loss:  0.3851009011268616
train gradient:  0.19463059894042256
iteration : 14106
train acc:  0.859375
train loss:  0.3241296410560608
train gradient:  0.10938759198532831
iteration : 14107
train acc:  0.859375
train loss:  0.3422320485115051
train gradient:  0.16451675292302542
iteration : 14108
train acc:  0.875
train loss:  0.27131786942481995
train gradient:  0.10850823311107503
iteration : 14109
train acc:  0.859375
train loss:  0.30454695224761963
train gradient:  0.15589592463314372
iteration : 14110
train acc:  0.8203125
train loss:  0.35446009039878845
train gradient:  0.17141628073268114
iteration : 14111
train acc:  0.8515625
train loss:  0.3356848955154419
train gradient:  0.1306569971015363
iteration : 14112
train acc:  0.8203125
train loss:  0.39663267135620117
train gradient:  0.14423696070647518
iteration : 14113
train acc:  0.8203125
train loss:  0.38398438692092896
train gradient:  0.17249987181395174
iteration : 14114
train acc:  0.7890625
train loss:  0.37488338351249695
train gradient:  0.22008037458897078
iteration : 14115
train acc:  0.90625
train loss:  0.27327150106430054
train gradient:  0.10087910134150366
iteration : 14116
train acc:  0.8515625
train loss:  0.31662043929100037
train gradient:  0.17560538329562364
iteration : 14117
train acc:  0.8515625
train loss:  0.38580232858657837
train gradient:  0.17107991906210043
iteration : 14118
train acc:  0.859375
train loss:  0.29770857095718384
train gradient:  0.08963658093825028
iteration : 14119
train acc:  0.8671875
train loss:  0.2733396887779236
train gradient:  0.10144915864574064
iteration : 14120
train acc:  0.875
train loss:  0.326885849237442
train gradient:  0.13580067339446633
iteration : 14121
train acc:  0.8671875
train loss:  0.34294602274894714
train gradient:  0.16776582997613015
iteration : 14122
train acc:  0.859375
train loss:  0.31144440174102783
train gradient:  0.13884630736194614
iteration : 14123
train acc:  0.875
train loss:  0.28034985065460205
train gradient:  0.11443818273433853
iteration : 14124
train acc:  0.8828125
train loss:  0.3726238012313843
train gradient:  0.14063564102933648
iteration : 14125
train acc:  0.8125
train loss:  0.40683621168136597
train gradient:  0.21738733435577712
iteration : 14126
train acc:  0.8671875
train loss:  0.29246389865875244
train gradient:  0.11457410433711293
iteration : 14127
train acc:  0.8828125
train loss:  0.30390113592147827
train gradient:  0.17078051611084047
iteration : 14128
train acc:  0.8046875
train loss:  0.4091755151748657
train gradient:  0.16826560423512354
iteration : 14129
train acc:  0.84375
train loss:  0.39881736040115356
train gradient:  0.14256406383671466
iteration : 14130
train acc:  0.8828125
train loss:  0.2805149555206299
train gradient:  0.1123061051156788
iteration : 14131
train acc:  0.8359375
train loss:  0.3358873128890991
train gradient:  0.14710964654955982
iteration : 14132
train acc:  0.8359375
train loss:  0.33925312757492065
train gradient:  0.12340574277325343
iteration : 14133
train acc:  0.8828125
train loss:  0.2923744320869446
train gradient:  0.10784716563862341
iteration : 14134
train acc:  0.859375
train loss:  0.29569366574287415
train gradient:  0.11000765279296572
iteration : 14135
train acc:  0.828125
train loss:  0.37666282057762146
train gradient:  0.1597870428049429
iteration : 14136
train acc:  0.8515625
train loss:  0.32502710819244385
train gradient:  0.1519794620721668
iteration : 14137
train acc:  0.859375
train loss:  0.33558952808380127
train gradient:  0.14161298325362343
iteration : 14138
train acc:  0.8671875
train loss:  0.3165375888347626
train gradient:  0.1137984111587818
iteration : 14139
train acc:  0.8671875
train loss:  0.2708035707473755
train gradient:  0.11091563381692693
iteration : 14140
train acc:  0.890625
train loss:  0.31433793902397156
train gradient:  0.08526236032439365
iteration : 14141
train acc:  0.890625
train loss:  0.2663201093673706
train gradient:  0.07624655817199309
iteration : 14142
train acc:  0.84375
train loss:  0.33367249369621277
train gradient:  0.12963063098795696
iteration : 14143
train acc:  0.8984375
train loss:  0.24312704801559448
train gradient:  0.07972123279671073
iteration : 14144
train acc:  0.8203125
train loss:  0.36892327666282654
train gradient:  0.11357863647011836
iteration : 14145
train acc:  0.84375
train loss:  0.31360095739364624
train gradient:  0.14438823533888032
iteration : 14146
train acc:  0.9140625
train loss:  0.25265002250671387
train gradient:  0.10120884147570901
iteration : 14147
train acc:  0.90625
train loss:  0.30756819248199463
train gradient:  0.19254842206697503
iteration : 14148
train acc:  0.8515625
train loss:  0.32792091369628906
train gradient:  0.14801368832003653
iteration : 14149
train acc:  0.8515625
train loss:  0.28860828280448914
train gradient:  0.08000514086579986
iteration : 14150
train acc:  0.859375
train loss:  0.3232220411300659
train gradient:  0.1595995600732958
iteration : 14151
train acc:  0.8515625
train loss:  0.290240615606308
train gradient:  0.1867945694700427
iteration : 14152
train acc:  0.8828125
train loss:  0.3549056649208069
train gradient:  0.18649901287795592
iteration : 14153
train acc:  0.8203125
train loss:  0.3342055082321167
train gradient:  0.11222301482002908
iteration : 14154
train acc:  0.890625
train loss:  0.2965398132801056
train gradient:  0.10496301242853526
iteration : 14155
train acc:  0.8671875
train loss:  0.35394978523254395
train gradient:  0.14877348281291686
iteration : 14156
train acc:  0.875
train loss:  0.3043586015701294
train gradient:  0.13027972004581867
iteration : 14157
train acc:  0.890625
train loss:  0.31980034708976746
train gradient:  0.13857193035871576
iteration : 14158
train acc:  0.890625
train loss:  0.271202027797699
train gradient:  0.10193758710477177
iteration : 14159
train acc:  0.859375
train loss:  0.3556601405143738
train gradient:  0.1681487825960271
iteration : 14160
train acc:  0.890625
train loss:  0.29428738355636597
train gradient:  0.09506031758751217
iteration : 14161
train acc:  0.8671875
train loss:  0.2935180068016052
train gradient:  0.10807067095843104
iteration : 14162
train acc:  0.796875
train loss:  0.40499234199523926
train gradient:  0.21367189975909306
iteration : 14163
train acc:  0.8828125
train loss:  0.3086068630218506
train gradient:  0.09000075712643597
iteration : 14164
train acc:  0.8828125
train loss:  0.2514705955982208
train gradient:  0.09197782965152242
iteration : 14165
train acc:  0.90625
train loss:  0.26991981267929077
train gradient:  0.12734949810300322
iteration : 14166
train acc:  0.8515625
train loss:  0.31555989384651184
train gradient:  0.12031586842366004
iteration : 14167
train acc:  0.84375
train loss:  0.3215782642364502
train gradient:  0.12871446035403933
iteration : 14168
train acc:  0.90625
train loss:  0.25328969955444336
train gradient:  0.08426486044139057
iteration : 14169
train acc:  0.8203125
train loss:  0.37793928384780884
train gradient:  0.2507928850762851
iteration : 14170
train acc:  0.8515625
train loss:  0.3225247263908386
train gradient:  0.13692507170988116
iteration : 14171
train acc:  0.8671875
train loss:  0.29426640272140503
train gradient:  0.13233928759906877
iteration : 14172
train acc:  0.8984375
train loss:  0.2725060284137726
train gradient:  0.11918431954188122
iteration : 14173
train acc:  0.90625
train loss:  0.212505042552948
train gradient:  0.06006174024318336
iteration : 14174
train acc:  0.859375
train loss:  0.28880226612091064
train gradient:  0.09028633779097442
iteration : 14175
train acc:  0.8515625
train loss:  0.2971343994140625
train gradient:  0.10752094189769339
iteration : 14176
train acc:  0.8359375
train loss:  0.350659042596817
train gradient:  0.1606428247230601
iteration : 14177
train acc:  0.875
train loss:  0.27980220317840576
train gradient:  0.08812164251747069
iteration : 14178
train acc:  0.8671875
train loss:  0.28048813343048096
train gradient:  0.09752032529237845
iteration : 14179
train acc:  0.84375
train loss:  0.34912344813346863
train gradient:  0.2142038301804457
iteration : 14180
train acc:  0.8828125
train loss:  0.23604726791381836
train gradient:  0.09263077669999231
iteration : 14181
train acc:  0.8515625
train loss:  0.308692991733551
train gradient:  0.12028762117534406
iteration : 14182
train acc:  0.8515625
train loss:  0.31339240074157715
train gradient:  0.14707414979952224
iteration : 14183
train acc:  0.890625
train loss:  0.2678764760494232
train gradient:  0.15898433437925608
iteration : 14184
train acc:  0.859375
train loss:  0.4000459313392639
train gradient:  0.27119375787047845
iteration : 14185
train acc:  0.8828125
train loss:  0.2802950143814087
train gradient:  0.2008338469853842
iteration : 14186
train acc:  1.0
train loss:  0.1350453644990921
train gradient:  0.4293963580617369
val acc:  0.8704593854578643
val f1:  0.8713672889855598
val confusion matrix:  [[85140 13470]
 [12078 86532]]

----------------------------------------new_epoch--------------------------------------

epoch:  1
iteration : 0
train acc:  0.859375
train loss:  0.3125019073486328
train gradient:  0.16279570574665064
iteration : 1
train acc:  0.8671875
train loss:  0.29389482736587524
train gradient:  0.12377555097612304
iteration : 2
train acc:  0.875
train loss:  0.30648618936538696
train gradient:  0.1555850735852613
iteration : 3
train acc:  0.828125
train loss:  0.32659804821014404
train gradient:  0.15453864091834366
iteration : 4
train acc:  0.8671875
train loss:  0.2957303524017334
train gradient:  0.08990920159208807
iteration : 5
train acc:  0.90625
train loss:  0.22718024253845215
train gradient:  0.09739538412736169
iteration : 6
train acc:  0.875
train loss:  0.3091508746147156
train gradient:  0.13064725332966912
iteration : 7
train acc:  0.90625
train loss:  0.25707799196243286
train gradient:  0.11064146273708118
iteration : 8
train acc:  0.9296875
train loss:  0.23164036870002747
train gradient:  0.08715569040442613
iteration : 9
train acc:  0.9140625
train loss:  0.2710585594177246
train gradient:  0.07382504086442565
iteration : 10
train acc:  0.8828125
train loss:  0.3294435739517212
train gradient:  0.27748313594501517
iteration : 11
train acc:  0.8671875
train loss:  0.22939643263816833
train gradient:  0.10234756742733656
iteration : 12
train acc:  0.875
train loss:  0.2689184546470642
train gradient:  0.1737822330183172
iteration : 13
train acc:  0.890625
train loss:  0.28603190183639526
train gradient:  0.1182143455945967
iteration : 14
train acc:  0.859375
train loss:  0.32340604066848755
train gradient:  0.13663630134315702
iteration : 15
train acc:  0.8671875
train loss:  0.3775342106819153
train gradient:  0.26876648499001815
iteration : 16
train acc:  0.921875
train loss:  0.23896430432796478
train gradient:  0.07653909281036886
iteration : 17
train acc:  0.875
train loss:  0.34152454137802124
train gradient:  0.1576700859291016
iteration : 18
train acc:  0.8515625
train loss:  0.28828319907188416
train gradient:  0.15365878179084996
iteration : 19
train acc:  0.875
train loss:  0.3424709141254425
train gradient:  0.1922009899797775
iteration : 20
train acc:  0.8515625
train loss:  0.3237905204296112
train gradient:  0.167107183096355
iteration : 21
train acc:  0.859375
train loss:  0.3761520981788635
train gradient:  0.19468589223956306
iteration : 22
train acc:  0.84375
train loss:  0.32647964358329773
train gradient:  0.150492640654035
iteration : 23
train acc:  0.828125
train loss:  0.3636726140975952
train gradient:  0.16070189591364686
iteration : 24
train acc:  0.8125
train loss:  0.398123562335968
train gradient:  0.20555250997581392
iteration : 25
train acc:  0.859375
train loss:  0.30637505650520325
train gradient:  0.12908332474963674
iteration : 26
train acc:  0.8984375
train loss:  0.30372023582458496
train gradient:  0.13152093853079108
iteration : 27
train acc:  0.8515625
train loss:  0.2787853181362152
train gradient:  0.1626902232405686
iteration : 28
train acc:  0.84375
train loss:  0.3672426640987396
train gradient:  0.3711249663489816
iteration : 29
train acc:  0.875
train loss:  0.31154555082321167
train gradient:  0.14509588741689441
iteration : 30
train acc:  0.859375
train loss:  0.35022497177124023
train gradient:  0.1405880927235764
iteration : 31
train acc:  0.84375
train loss:  0.30899590253829956
train gradient:  0.10626413567795487
iteration : 32
train acc:  0.8359375
train loss:  0.34809380769729614
train gradient:  0.17459727129326352
iteration : 33
train acc:  0.8984375
train loss:  0.28414225578308105
train gradient:  0.09301987302194954
iteration : 34
train acc:  0.8515625
train loss:  0.31823092699050903
train gradient:  0.1348568033566513
iteration : 35
train acc:  0.8828125
train loss:  0.28748977184295654
train gradient:  0.09312755958492289
iteration : 36
train acc:  0.8828125
train loss:  0.28586775064468384
train gradient:  0.13656630863571395
iteration : 37
train acc:  0.9296875
train loss:  0.25634241104125977
train gradient:  0.09355319967252089
iteration : 38
train acc:  0.8671875
train loss:  0.2934151291847229
train gradient:  0.16601302030796544
iteration : 39
train acc:  0.859375
train loss:  0.3146618902683258
train gradient:  0.14623176170108004
iteration : 40
train acc:  0.859375
train loss:  0.2945554852485657
train gradient:  0.1421446694185126
iteration : 41
train acc:  0.8359375
train loss:  0.3571659326553345
train gradient:  0.179955117847737
iteration : 42
train acc:  0.828125
train loss:  0.34784936904907227
train gradient:  0.1589474728071149
iteration : 43
train acc:  0.875
train loss:  0.2814963757991791
train gradient:  0.14170266632434914
iteration : 44
train acc:  0.8515625
train loss:  0.3038923144340515
train gradient:  0.14034651362939715
iteration : 45
train acc:  0.859375
train loss:  0.3191145062446594
train gradient:  0.1510509967246675
iteration : 46
train acc:  0.8125
train loss:  0.4306107461452484
train gradient:  0.18302317365913506
iteration : 47
train acc:  0.875
train loss:  0.30978551506996155
train gradient:  0.199671436139084
iteration : 48
train acc:  0.84375
train loss:  0.27682822942733765
train gradient:  0.10186869998945086
iteration : 49
train acc:  0.8671875
train loss:  0.31740713119506836
train gradient:  0.13936076628837957
iteration : 50
train acc:  0.8203125
train loss:  0.3660944402217865
train gradient:  0.18782093472648742
iteration : 51
train acc:  0.8671875
train loss:  0.3166334629058838
train gradient:  0.17422489299288843
iteration : 52
train acc:  0.890625
train loss:  0.2635039687156677
train gradient:  0.08354892040605261
iteration : 53
train acc:  0.8984375
train loss:  0.2600412368774414
train gradient:  0.12548110080346492
iteration : 54
train acc:  0.8203125
train loss:  0.36559709906578064
train gradient:  0.18760439056979528
iteration : 55
train acc:  0.828125
train loss:  0.37222349643707275
train gradient:  0.21219480586839262
iteration : 56
train acc:  0.8984375
train loss:  0.32323157787323
train gradient:  0.1711849753693962
iteration : 57
train acc:  0.84375
train loss:  0.341964453458786
train gradient:  0.11424994333334694
iteration : 58
train acc:  0.859375
train loss:  0.30658090114593506
train gradient:  0.15987271836486072
iteration : 59
train acc:  0.890625
train loss:  0.2740565240383148
train gradient:  0.10521911653480781
iteration : 60
train acc:  0.8671875
train loss:  0.28834596276283264
train gradient:  0.12473592202371714
iteration : 61
train acc:  0.8671875
train loss:  0.2841140031814575
train gradient:  0.09589880767948508
iteration : 62
train acc:  0.828125
train loss:  0.43624746799468994
train gradient:  0.2748116214479592
iteration : 63
train acc:  0.890625
train loss:  0.2153734266757965
train gradient:  0.08257900422996413
iteration : 64
train acc:  0.8828125
train loss:  0.26957958936691284
train gradient:  0.10152089162080254
iteration : 65
train acc:  0.859375
train loss:  0.2836264967918396
train gradient:  0.13236441417722672
iteration : 66
train acc:  0.8671875
train loss:  0.3315141499042511
train gradient:  0.14411469844295247
iteration : 67
train acc:  0.8046875
train loss:  0.34770655632019043
train gradient:  0.1360238345158844
iteration : 68
train acc:  0.859375
train loss:  0.32139667868614197
train gradient:  0.16712570173583527
iteration : 69
train acc:  0.84375
train loss:  0.3331872224807739
train gradient:  0.14313587676210726
iteration : 70
train acc:  0.8828125
train loss:  0.3012727200984955
train gradient:  0.10558713961611914
iteration : 71
train acc:  0.8828125
train loss:  0.2713582217693329
train gradient:  0.1199365602341057
iteration : 72
train acc:  0.8828125
train loss:  0.2667457163333893
train gradient:  0.11835466346174377
iteration : 73
train acc:  0.890625
train loss:  0.29467859864234924
train gradient:  0.11724310354370546
iteration : 74
train acc:  0.90625
train loss:  0.24805894494056702
train gradient:  0.08526478598372503
iteration : 75
train acc:  0.8359375
train loss:  0.33907991647720337
train gradient:  0.14770995961814135
iteration : 76
train acc:  0.9140625
train loss:  0.23937809467315674
train gradient:  0.14662075690954113
iteration : 77
train acc:  0.8515625
train loss:  0.34725451469421387
train gradient:  0.16444602735418878
iteration : 78
train acc:  0.859375
train loss:  0.30368056893348694
train gradient:  0.11761634952804387
iteration : 79
train acc:  0.890625
train loss:  0.2572782039642334
train gradient:  0.0821708964033714
iteration : 80
train acc:  0.8203125
train loss:  0.3185276687145233
train gradient:  0.15916990571554174
iteration : 81
train acc:  0.890625
train loss:  0.2673267424106598
train gradient:  0.10595620144925057
iteration : 82
train acc:  0.890625
train loss:  0.27035877108573914
train gradient:  0.18554762716641227
iteration : 83
train acc:  0.859375
train loss:  0.27080070972442627
train gradient:  0.094832022794089
iteration : 84
train acc:  0.9296875
train loss:  0.23885893821716309
train gradient:  0.07865443754355647
iteration : 85
train acc:  0.8984375
train loss:  0.249211847782135
train gradient:  0.08219545725061239
iteration : 86
train acc:  0.875
train loss:  0.29383838176727295
train gradient:  0.11618012911718169
iteration : 87
train acc:  0.8984375
train loss:  0.24587705731391907
train gradient:  0.1250766591652987
iteration : 88
train acc:  0.84375
train loss:  0.3558498024940491
train gradient:  0.21005430538135786
iteration : 89
train acc:  0.859375
train loss:  0.31793150305747986
train gradient:  0.16351698361239897
iteration : 90
train acc:  0.8984375
train loss:  0.2805319130420685
train gradient:  0.095346110063506
iteration : 91
train acc:  0.796875
train loss:  0.3937039077281952
train gradient:  0.20158263032305188
iteration : 92
train acc:  0.859375
train loss:  0.2931371033191681
train gradient:  0.11734345726150089
iteration : 93
train acc:  0.9140625
train loss:  0.2489684820175171
train gradient:  0.15522696811662973
iteration : 94
train acc:  0.8671875
train loss:  0.29259419441223145
train gradient:  0.1304502946592093
iteration : 95
train acc:  0.8515625
train loss:  0.3071249723434448
train gradient:  0.12732065041876192
iteration : 96
train acc:  0.890625
train loss:  0.31203481554985046
train gradient:  0.12596013364886374
iteration : 97
train acc:  0.859375
train loss:  0.33091089129447937
train gradient:  0.18589161445600771
iteration : 98
train acc:  0.8671875
train loss:  0.3270884156227112
train gradient:  0.18590703519805074
iteration : 99
train acc:  0.84375
train loss:  0.32128584384918213
train gradient:  0.12799458919245582
iteration : 100
train acc:  0.875
train loss:  0.30300548672676086
train gradient:  0.1705862198215613
iteration : 101
train acc:  0.875
train loss:  0.29771050810813904
train gradient:  0.11474405034480387
iteration : 102
train acc:  0.8359375
train loss:  0.3087104856967926
train gradient:  0.17566702564998316
iteration : 103
train acc:  0.8671875
train loss:  0.2830538749694824
train gradient:  0.09116003042428358
iteration : 104
train acc:  0.8984375
train loss:  0.29305800795555115
train gradient:  0.14598737556657135
iteration : 105
train acc:  0.90625
train loss:  0.289553165435791
train gradient:  0.10093555664066678
iteration : 106
train acc:  0.84375
train loss:  0.340134859085083
train gradient:  0.15454855481840957
iteration : 107
train acc:  0.9375
train loss:  0.1990390121936798
train gradient:  0.056999700010906686
iteration : 108
train acc:  0.9140625
train loss:  0.22350192070007324
train gradient:  0.0904934863469404
iteration : 109
train acc:  0.890625
train loss:  0.2663097381591797
train gradient:  0.12712281994451455
iteration : 110
train acc:  0.8203125
train loss:  0.3821507692337036
train gradient:  0.2220446994511803
iteration : 111
train acc:  0.90625
train loss:  0.2082061469554901
train gradient:  0.0810646088543851
iteration : 112
train acc:  0.9375
train loss:  0.19093161821365356
train gradient:  0.0767487068726392
iteration : 113
train acc:  0.8515625
train loss:  0.2960985004901886
train gradient:  0.1168729695637147
iteration : 114
train acc:  0.8203125
train loss:  0.3969343900680542
train gradient:  0.1818828269054355
iteration : 115
train acc:  0.8671875
train loss:  0.33799174427986145
train gradient:  0.14283829360897227
iteration : 116
train acc:  0.8984375
train loss:  0.20839446783065796
train gradient:  0.06958443532248026
iteration : 117
train acc:  0.8125
train loss:  0.3531145751476288
train gradient:  0.2212589846841272
iteration : 118
train acc:  0.7890625
train loss:  0.41971713304519653
train gradient:  0.17941888673496903
iteration : 119
train acc:  0.9296875
train loss:  0.21079802513122559
train gradient:  0.08413125136868817
iteration : 120
train acc:  0.84375
train loss:  0.3418530821800232
train gradient:  0.14715525862215298
iteration : 121
train acc:  0.875
train loss:  0.28175342082977295
train gradient:  0.10115140266630077
iteration : 122
train acc:  0.8203125
train loss:  0.3840316832065582
train gradient:  0.20849787487450638
iteration : 123
train acc:  0.90625
train loss:  0.2677493989467621
train gradient:  0.09840106149358353
iteration : 124
train acc:  0.8671875
train loss:  0.373925119638443
train gradient:  0.18811785535042724
iteration : 125
train acc:  0.890625
train loss:  0.2726735472679138
train gradient:  0.0861215423373203
iteration : 126
train acc:  0.84375
train loss:  0.35476499795913696
train gradient:  0.14783957352343033
iteration : 127
train acc:  0.8671875
train loss:  0.22054536640644073
train gradient:  0.1139877320972774
iteration : 128
train acc:  0.8671875
train loss:  0.3492223918437958
train gradient:  0.17393802018590154
iteration : 129
train acc:  0.796875
train loss:  0.3750726878643036
train gradient:  0.2616693678639102
iteration : 130
train acc:  0.8671875
train loss:  0.3314436674118042
train gradient:  0.20498397313024522
iteration : 131
train acc:  0.8359375
train loss:  0.362307608127594
train gradient:  0.15699208470748083
iteration : 132
train acc:  0.9140625
train loss:  0.24627357721328735
train gradient:  0.07999813795736162
iteration : 133
train acc:  0.859375
train loss:  0.3510758578777313
train gradient:  0.14521260937840844
iteration : 134
train acc:  0.828125
train loss:  0.3289541006088257
train gradient:  0.1999332954947951
iteration : 135
train acc:  0.921875
train loss:  0.24733825027942657
train gradient:  0.09049409599057162
iteration : 136
train acc:  0.859375
train loss:  0.32960397005081177
train gradient:  0.2105687538320979
iteration : 137
train acc:  0.8828125
train loss:  0.31703710556030273
train gradient:  0.10255556809213497
iteration : 138
train acc:  0.828125
train loss:  0.4725094437599182
train gradient:  0.29033774928206957
iteration : 139
train acc:  0.828125
train loss:  0.3267812728881836
train gradient:  0.11747791260622092
iteration : 140
train acc:  0.90625
train loss:  0.3103356957435608
train gradient:  0.11566637992484602
iteration : 141
train acc:  0.7890625
train loss:  0.36516669392585754
train gradient:  0.15279995248764283
iteration : 142
train acc:  0.828125
train loss:  0.3121028542518616
train gradient:  0.1540090003032263
iteration : 143
train acc:  0.8515625
train loss:  0.3036268949508667
train gradient:  0.1142037793865855
iteration : 144
train acc:  0.9140625
train loss:  0.23933428525924683
train gradient:  0.09701664454722346
iteration : 145
train acc:  0.859375
train loss:  0.3541963994503021
train gradient:  0.1638090351050683
iteration : 146
train acc:  0.84375
train loss:  0.32548630237579346
train gradient:  0.1576846754392816
iteration : 147
train acc:  0.8515625
train loss:  0.2808675765991211
train gradient:  0.09942740184035936
iteration : 148
train acc:  0.8828125
train loss:  0.2794432044029236
train gradient:  0.13174726398003045
iteration : 149
train acc:  0.84375
train loss:  0.367330402135849
train gradient:  0.2617868503929521
iteration : 150
train acc:  0.875
train loss:  0.2884019613265991
train gradient:  0.15787217129552322
iteration : 151
train acc:  0.8359375
train loss:  0.3787573575973511
train gradient:  0.23088279872653836
iteration : 152
train acc:  0.8359375
train loss:  0.31199923157691956
train gradient:  0.12674323915184973
iteration : 153
train acc:  0.8359375
train loss:  0.3417327404022217
train gradient:  0.13710290647314965
iteration : 154
train acc:  0.84375
train loss:  0.3379293978214264
train gradient:  0.18278880620854154
iteration : 155
train acc:  0.8515625
train loss:  0.3167397677898407
train gradient:  0.14890925236168373
iteration : 156
train acc:  0.8515625
train loss:  0.3442281484603882
train gradient:  0.175113554695691
iteration : 157
train acc:  0.8828125
train loss:  0.2731434106826782
train gradient:  0.11979414078620491
iteration : 158
train acc:  0.8828125
train loss:  0.2884695529937744
train gradient:  0.11359062296346074
iteration : 159
train acc:  0.8515625
train loss:  0.2932460606098175
train gradient:  0.17725952818369997
iteration : 160
train acc:  0.859375
train loss:  0.3013341724872589
train gradient:  0.13347944579857177
iteration : 161
train acc:  0.8515625
train loss:  0.3869789242744446
train gradient:  0.2433879438356426
iteration : 162
train acc:  0.890625
train loss:  0.27990826964378357
train gradient:  0.2093384594704558
iteration : 163
train acc:  0.8359375
train loss:  0.2944068908691406
train gradient:  0.15513591191044374
iteration : 164
train acc:  0.8671875
train loss:  0.2965596318244934
train gradient:  0.15822711798048647
iteration : 165
train acc:  0.8515625
train loss:  0.3391323983669281
train gradient:  0.14132044599858298
iteration : 166
train acc:  0.890625
train loss:  0.2727890610694885
train gradient:  0.09724628564707946
iteration : 167
train acc:  0.8359375
train loss:  0.37256133556365967
train gradient:  0.18599231455816906
iteration : 168
train acc:  0.8828125
train loss:  0.27875393629074097
train gradient:  0.11708773435886838
iteration : 169
train acc:  0.8515625
train loss:  0.34988418221473694
train gradient:  0.304232673438703
iteration : 170
train acc:  0.859375
train loss:  0.35239553451538086
train gradient:  0.15308223374673352
iteration : 171
train acc:  0.8515625
train loss:  0.29441776871681213
train gradient:  0.11823605339283062
iteration : 172
train acc:  0.8984375
train loss:  0.27502521872520447
train gradient:  0.10469410606951247
iteration : 173
train acc:  0.875
train loss:  0.353586345911026
train gradient:  0.16803009745638992
iteration : 174
train acc:  0.7890625
train loss:  0.47064656019210815
train gradient:  0.1893142319081786
iteration : 175
train acc:  0.8359375
train loss:  0.3653622269630432
train gradient:  0.15417915473675786
iteration : 176
train acc:  0.8359375
train loss:  0.36602354049682617
train gradient:  0.17610497427605887
iteration : 177
train acc:  0.8671875
train loss:  0.2713267505168915
train gradient:  0.08828875074105996
iteration : 178
train acc:  0.828125
train loss:  0.4134441912174225
train gradient:  0.25536962068743585
iteration : 179
train acc:  0.90625
train loss:  0.2799854874610901
train gradient:  0.10429731180515005
iteration : 180
train acc:  0.8125
train loss:  0.43786340951919556
train gradient:  0.24222454496442183
iteration : 181
train acc:  0.8671875
train loss:  0.3212771713733673
train gradient:  0.12095034052854309
iteration : 182
train acc:  0.84375
train loss:  0.31895989179611206
train gradient:  0.12528523623816284
iteration : 183
train acc:  0.875
train loss:  0.3263762593269348
train gradient:  0.15998867845980905
iteration : 184
train acc:  0.859375
train loss:  0.3496060371398926
train gradient:  0.14956216896191069
iteration : 185
train acc:  0.859375
train loss:  0.3500169515609741
train gradient:  0.2266655455481871
iteration : 186
train acc:  0.859375
train loss:  0.2684679627418518
train gradient:  0.18710789077451823
iteration : 187
train acc:  0.921875
train loss:  0.23551392555236816
train gradient:  0.08899563969895358
iteration : 188
train acc:  0.859375
train loss:  0.28047317266464233
train gradient:  0.1142679517631927
iteration : 189
train acc:  0.84375
train loss:  0.3424597680568695
train gradient:  0.2472778041726148
iteration : 190
train acc:  0.8515625
train loss:  0.3997657597064972
train gradient:  0.2603059305161767
iteration : 191
train acc:  0.8671875
train loss:  0.2935308516025543
train gradient:  0.12213582359545189
iteration : 192
train acc:  0.828125
train loss:  0.33855944871902466
train gradient:  0.12275858689277946
iteration : 193
train acc:  0.828125
train loss:  0.39783236384391785
train gradient:  0.26128423253775357
iteration : 194
train acc:  0.8515625
train loss:  0.2961614429950714
train gradient:  0.1453913657349905
iteration : 195
train acc:  0.8828125
train loss:  0.25568854808807373
train gradient:  0.14332616799564257
iteration : 196
train acc:  0.890625
train loss:  0.3071260452270508
train gradient:  0.13448972386584784
iteration : 197
train acc:  0.8984375
train loss:  0.27404695749282837
train gradient:  0.10186946943215851
iteration : 198
train acc:  0.8359375
train loss:  0.3399994969367981
train gradient:  0.10939523388368343
iteration : 199
train acc:  0.84375
train loss:  0.34196776151657104
train gradient:  0.1518416186714202
iteration : 200
train acc:  0.8515625
train loss:  0.359592467546463
train gradient:  0.16213829544944192
iteration : 201
train acc:  0.859375
train loss:  0.3526274561882019
train gradient:  0.16269614513453795
iteration : 202
train acc:  0.84375
train loss:  0.3168719410896301
train gradient:  0.14052166874007138
iteration : 203
train acc:  0.8515625
train loss:  0.29179853200912476
train gradient:  0.11432157237758005
iteration : 204
train acc:  0.8671875
train loss:  0.3136920630931854
train gradient:  0.13451301798229498
iteration : 205
train acc:  0.859375
train loss:  0.31786075234413147
train gradient:  0.11343160526105835
iteration : 206
train acc:  0.8671875
train loss:  0.3146744966506958
train gradient:  0.10938340686958041
iteration : 207
train acc:  0.8828125
train loss:  0.22550731897354126
train gradient:  0.07616879397281569
iteration : 208
train acc:  0.8515625
train loss:  0.3277938961982727
train gradient:  0.10499375986927714
iteration : 209
train acc:  0.8984375
train loss:  0.2552271783351898
train gradient:  0.1434987497465285
iteration : 210
train acc:  0.84375
train loss:  0.38293832540512085
train gradient:  0.18363864942020303
iteration : 211
train acc:  0.890625
train loss:  0.3073979616165161
train gradient:  0.10003010880740365
iteration : 212
train acc:  0.9140625
train loss:  0.2670411467552185
train gradient:  0.08489581525919788
iteration : 213
train acc:  0.90625
train loss:  0.2508939802646637
train gradient:  0.09537421884216878
iteration : 214
train acc:  0.8359375
train loss:  0.28748318552970886
train gradient:  0.11189712175046879
iteration : 215
train acc:  0.8359375
train loss:  0.3284434676170349
train gradient:  0.20515918371170647
iteration : 216
train acc:  0.8671875
train loss:  0.3052372932434082
train gradient:  0.1029499674668369
iteration : 217
train acc:  0.8515625
train loss:  0.3232988715171814
train gradient:  0.16823128970404855
iteration : 218
train acc:  0.90625
train loss:  0.2703387141227722
train gradient:  0.13373113637591663
iteration : 219
train acc:  0.8515625
train loss:  0.3180696368217468
train gradient:  0.11082796471841488
iteration : 220
train acc:  0.875
train loss:  0.3254168927669525
train gradient:  0.12459134118406237
iteration : 221
train acc:  0.875
train loss:  0.275520384311676
train gradient:  0.11501676098427754
iteration : 222
train acc:  0.9140625
train loss:  0.2702957093715668
train gradient:  0.12341948098640992
iteration : 223
train acc:  0.8515625
train loss:  0.3205685019493103
train gradient:  0.19350938609056195
iteration : 224
train acc:  0.8359375
train loss:  0.3557744026184082
train gradient:  0.2482365074022309
iteration : 225
train acc:  0.859375
train loss:  0.26204490661621094
train gradient:  0.1196052516209571
iteration : 226
train acc:  0.8359375
train loss:  0.3819613754749298
train gradient:  0.15993977162205567
iteration : 227
train acc:  0.8671875
train loss:  0.37056589126586914
train gradient:  0.1668709143909087
iteration : 228
train acc:  0.8203125
train loss:  0.36394575238227844
train gradient:  0.17898107433664312
iteration : 229
train acc:  0.8828125
train loss:  0.31473273038864136
train gradient:  0.1324668127434761
iteration : 230
train acc:  0.8125
train loss:  0.3592033386230469
train gradient:  0.15681019242300062
iteration : 231
train acc:  0.828125
train loss:  0.3229466378688812
train gradient:  0.13876939859052204
iteration : 232
train acc:  0.78125
train loss:  0.4230457544326782
train gradient:  0.1787445139361762
iteration : 233
train acc:  0.8125
train loss:  0.36730465292930603
train gradient:  0.15434118008675032
iteration : 234
train acc:  0.875
train loss:  0.3264867067337036
train gradient:  0.1094854330897802
iteration : 235
train acc:  0.8671875
train loss:  0.3245452046394348
train gradient:  0.10729869873403447
iteration : 236
train acc:  0.84375
train loss:  0.3663047254085541
train gradient:  0.23773621861975552
iteration : 237
train acc:  0.8203125
train loss:  0.41544288396835327
train gradient:  0.2506412291009631
iteration : 238
train acc:  0.921875
train loss:  0.2612905502319336
train gradient:  0.10780915737820039
iteration : 239
train acc:  0.8203125
train loss:  0.3590617775917053
train gradient:  0.17829644972233122
iteration : 240
train acc:  0.8515625
train loss:  0.3411516547203064
train gradient:  0.16586181887654416
iteration : 241
train acc:  0.90625
train loss:  0.21085360646247864
train gradient:  0.07043383318795907
iteration : 242
train acc:  0.7890625
train loss:  0.41649264097213745
train gradient:  0.17581176798916268
iteration : 243
train acc:  0.875
train loss:  0.25677233934402466
train gradient:  0.09101659210440231
iteration : 244
train acc:  0.875
train loss:  0.2977430820465088
train gradient:  0.09318522967112589
iteration : 245
train acc:  0.78125
train loss:  0.4564937353134155
train gradient:  0.2098436464686394
iteration : 246
train acc:  0.8125
train loss:  0.375062495470047
train gradient:  0.19076908194857004
iteration : 247
train acc:  0.859375
train loss:  0.3648865222930908
train gradient:  0.14981088681522778
iteration : 248
train acc:  0.875
train loss:  0.2644074559211731
train gradient:  0.12314580297512845
iteration : 249
train acc:  0.890625
train loss:  0.3069934546947479
train gradient:  0.16023874564486798
iteration : 250
train acc:  0.84375
train loss:  0.38111644983291626
train gradient:  0.17932402329387637
iteration : 251
train acc:  0.875
train loss:  0.3497089147567749
train gradient:  0.17433675366096002
iteration : 252
train acc:  0.8515625
train loss:  0.36517614126205444
train gradient:  0.139906535032356
iteration : 253
train acc:  0.8515625
train loss:  0.3490030765533447
train gradient:  0.15895517694122438
iteration : 254
train acc:  0.859375
train loss:  0.30068469047546387
train gradient:  0.1455597957891291
iteration : 255
train acc:  0.875
train loss:  0.2991713881492615
train gradient:  0.1131006580172987
iteration : 256
train acc:  0.8515625
train loss:  0.31996971368789673
train gradient:  0.12621831710600923
iteration : 257
train acc:  0.90625
train loss:  0.3193024694919586
train gradient:  0.10224710442247342
iteration : 258
train acc:  0.890625
train loss:  0.26457762718200684
train gradient:  0.09033330861707581
iteration : 259
train acc:  0.875
train loss:  0.35782483220100403
train gradient:  0.18849259016069225
iteration : 260
train acc:  0.8046875
train loss:  0.4276277720928192
train gradient:  0.1781985752540337
iteration : 261
train acc:  0.8359375
train loss:  0.360044002532959
train gradient:  0.10805269993441796
iteration : 262
train acc:  0.9140625
train loss:  0.2534084618091583
train gradient:  0.11987500377962791
iteration : 263
train acc:  0.875
train loss:  0.24360571801662445
train gradient:  0.17911798669593496
iteration : 264
train acc:  0.921875
train loss:  0.2649989128112793
train gradient:  0.07839971023529804
iteration : 265
train acc:  0.875
train loss:  0.3218686282634735
train gradient:  0.14943666264448677
iteration : 266
train acc:  0.875
train loss:  0.30891886353492737
train gradient:  0.0929445449558241
iteration : 267
train acc:  0.921875
train loss:  0.22761917114257812
train gradient:  0.06844616735758997
iteration : 268
train acc:  0.8671875
train loss:  0.32624414563179016
train gradient:  0.12731152105677884
iteration : 269
train acc:  0.859375
train loss:  0.32694804668426514
train gradient:  0.12283627484885608
iteration : 270
train acc:  0.859375
train loss:  0.36264634132385254
train gradient:  0.12207788926514669
iteration : 271
train acc:  0.8125
train loss:  0.3577590584754944
train gradient:  0.17444479032015092
iteration : 272
train acc:  0.90625
train loss:  0.22130009531974792
train gradient:  0.08588095807363055
iteration : 273
train acc:  0.84375
train loss:  0.34435880184173584
train gradient:  0.1513528297771255
iteration : 274
train acc:  0.8671875
train loss:  0.3731650114059448
train gradient:  0.1958245451391435
iteration : 275
train acc:  0.875
train loss:  0.29126161336898804
train gradient:  0.1376312538621155
iteration : 276
train acc:  0.921875
train loss:  0.22100499272346497
train gradient:  0.09319157715633107
iteration : 277
train acc:  0.8515625
train loss:  0.35554614663124084
train gradient:  0.13351148162907295
iteration : 278
train acc:  0.8828125
train loss:  0.2679024338722229
train gradient:  0.0909473551731949
iteration : 279
train acc:  0.8984375
train loss:  0.2772408425807953
train gradient:  0.13075599599951807
iteration : 280
train acc:  0.8828125
train loss:  0.26437628269195557
train gradient:  0.07379631255572222
iteration : 281
train acc:  0.8515625
train loss:  0.35027721524238586
train gradient:  0.12691212343815164
iteration : 282
train acc:  0.8671875
train loss:  0.3023890554904938
train gradient:  0.12504898109311918
iteration : 283
train acc:  0.828125
train loss:  0.38134658336639404
train gradient:  0.270744976065011
iteration : 284
train acc:  0.8984375
train loss:  0.21949422359466553
train gradient:  0.0841623294428639
iteration : 285
train acc:  0.890625
train loss:  0.2800118327140808
train gradient:  0.15547170893708362
iteration : 286
train acc:  0.90625
train loss:  0.26558923721313477
train gradient:  0.09803986697567274
iteration : 287
train acc:  0.8671875
train loss:  0.308468759059906
train gradient:  0.1112286903791131
iteration : 288
train acc:  0.8828125
train loss:  0.26753830909729004
train gradient:  0.13437729444604457
iteration : 289
train acc:  0.859375
train loss:  0.30219724774360657
train gradient:  0.13751736724020547
iteration : 290
train acc:  0.8828125
train loss:  0.29240626096725464
train gradient:  0.10909766779696083
iteration : 291
train acc:  0.8984375
train loss:  0.26134753227233887
train gradient:  0.07902975216513396
iteration : 292
train acc:  0.8671875
train loss:  0.28262144327163696
train gradient:  0.1345549894317212
iteration : 293
train acc:  0.84375
train loss:  0.3493044972419739
train gradient:  0.11286394130185959
iteration : 294
train acc:  0.8515625
train loss:  0.3291686773300171
train gradient:  0.11342463699877653
iteration : 295
train acc:  0.8828125
train loss:  0.22585536539554596
train gradient:  0.06490767429556374
iteration : 296
train acc:  0.828125
train loss:  0.3713887929916382
train gradient:  0.17185981672528883
iteration : 297
train acc:  0.8515625
train loss:  0.2894488573074341
train gradient:  0.10195985057309186
iteration : 298
train acc:  0.8828125
train loss:  0.3098684251308441
train gradient:  0.166702021391665
iteration : 299
train acc:  0.859375
train loss:  0.3116598129272461
train gradient:  0.1488704572954524
iteration : 300
train acc:  0.8125
train loss:  0.36191028356552124
train gradient:  0.16817815589709134
iteration : 301
train acc:  0.8359375
train loss:  0.3843408226966858
train gradient:  0.15369118507037294
iteration : 302
train acc:  0.8203125
train loss:  0.36754322052001953
train gradient:  0.12920690212537073
iteration : 303
train acc:  0.875
train loss:  0.25946706533432007
train gradient:  0.07921106428179116
iteration : 304
train acc:  0.890625
train loss:  0.2530434727668762
train gradient:  0.08133625510883313
iteration : 305
train acc:  0.8359375
train loss:  0.3379044532775879
train gradient:  0.1478681886212299
iteration : 306
train acc:  0.8984375
train loss:  0.3118595480918884
train gradient:  0.1583690674723248
iteration : 307
train acc:  0.875
train loss:  0.33545100688934326
train gradient:  0.10610988765219409
iteration : 308
train acc:  0.8671875
train loss:  0.29745423793792725
train gradient:  0.12030101261551475
iteration : 309
train acc:  0.8359375
train loss:  0.42454424500465393
train gradient:  0.22905192176791223
iteration : 310
train acc:  0.875
train loss:  0.28195205330848694
train gradient:  0.10558383284423185
iteration : 311
train acc:  0.90625
train loss:  0.25191056728363037
train gradient:  0.12285608703950365
iteration : 312
train acc:  0.828125
train loss:  0.33628159761428833
train gradient:  0.1813924088343609
iteration : 313
train acc:  0.828125
train loss:  0.3598071336746216
train gradient:  0.21370862573259708
iteration : 314
train acc:  0.875
train loss:  0.26160645484924316
train gradient:  0.09962226376102175
iteration : 315
train acc:  0.859375
train loss:  0.33744511008262634
train gradient:  0.12908013049756667
iteration : 316
train acc:  0.84375
train loss:  0.35819298028945923
train gradient:  0.13516672048070472
iteration : 317
train acc:  0.84375
train loss:  0.4231550693511963
train gradient:  0.23348785333672337
iteration : 318
train acc:  0.8984375
train loss:  0.23631083965301514
train gradient:  0.08266950284232408
iteration : 319
train acc:  0.859375
train loss:  0.3162306547164917
train gradient:  0.11905957198125243
iteration : 320
train acc:  0.9375
train loss:  0.20254141092300415
train gradient:  0.08915451845878214
iteration : 321
train acc:  0.8828125
train loss:  0.2890872359275818
train gradient:  0.14091418983142948
iteration : 322
train acc:  0.8828125
train loss:  0.25031131505966187
train gradient:  0.09828170851515737
iteration : 323
train acc:  0.8984375
train loss:  0.2550716996192932
train gradient:  0.10860227122548863
iteration : 324
train acc:  0.8515625
train loss:  0.30865857005119324
train gradient:  0.14455688928523044
iteration : 325
train acc:  0.859375
train loss:  0.29900580644607544
train gradient:  0.13784586681750163
iteration : 326
train acc:  0.90625
train loss:  0.23209266364574432
train gradient:  0.08223686083892703
iteration : 327
train acc:  0.84375
train loss:  0.41097018122673035
train gradient:  0.2003417073338965
iteration : 328
train acc:  0.9140625
train loss:  0.21345937252044678
train gradient:  0.057178556974972705
iteration : 329
train acc:  0.90625
train loss:  0.3035731017589569
train gradient:  0.17912185130266017
iteration : 330
train acc:  0.8359375
train loss:  0.3723960816860199
train gradient:  0.17752855135785417
iteration : 331
train acc:  0.8671875
train loss:  0.2635959982872009
train gradient:  0.07461381085494063
iteration : 332
train acc:  0.8359375
train loss:  0.36198437213897705
train gradient:  0.20097906381329889
iteration : 333
train acc:  0.859375
train loss:  0.329067587852478
train gradient:  0.12313867479391842
iteration : 334
train acc:  0.8671875
train loss:  0.26942986249923706
train gradient:  0.21396277770795336
iteration : 335
train acc:  0.8828125
train loss:  0.30281078815460205
train gradient:  0.10204291052204317
iteration : 336
train acc:  0.875
train loss:  0.2889465093612671
train gradient:  0.0927169670512299
iteration : 337
train acc:  0.8046875
train loss:  0.37500134110450745
train gradient:  0.18338124160852787
iteration : 338
train acc:  0.8671875
train loss:  0.3201797306537628
train gradient:  0.1343564305066411
iteration : 339
train acc:  0.90625
train loss:  0.2711941599845886
train gradient:  0.12948029076728962
iteration : 340
train acc:  0.84375
train loss:  0.39819008111953735
train gradient:  0.22559899939475125
iteration : 341
train acc:  0.8984375
train loss:  0.2570672631263733
train gradient:  0.08218701935691042
iteration : 342
train acc:  0.9140625
train loss:  0.2475685179233551
train gradient:  0.1112138640563586
iteration : 343
train acc:  0.8203125
train loss:  0.32820698618888855
train gradient:  0.16724315052277375
iteration : 344
train acc:  0.84375
train loss:  0.31328126788139343
train gradient:  0.15621892411161992
iteration : 345
train acc:  0.8515625
train loss:  0.29619359970092773
train gradient:  0.13190998303801446
iteration : 346
train acc:  0.859375
train loss:  0.30957353115081787
train gradient:  0.11845099951120508
iteration : 347
train acc:  0.8671875
train loss:  0.3146260678768158
train gradient:  0.14973758716240126
iteration : 348
train acc:  0.8828125
train loss:  0.2595420479774475
train gradient:  0.11282515994771128
iteration : 349
train acc:  0.8671875
train loss:  0.2944955825805664
train gradient:  0.15176315537566387
iteration : 350
train acc:  0.875
train loss:  0.32399290800094604
train gradient:  0.12873112839079742
iteration : 351
train acc:  0.8984375
train loss:  0.274035781621933
train gradient:  0.10421751333193131
iteration : 352
train acc:  0.8984375
train loss:  0.22255739569664001
train gradient:  0.07344735408606602
iteration : 353
train acc:  0.8359375
train loss:  0.3512830436229706
train gradient:  0.1546245837594163
iteration : 354
train acc:  0.8671875
train loss:  0.2899097800254822
train gradient:  0.10851260430055631
iteration : 355
train acc:  0.8203125
train loss:  0.3609160780906677
train gradient:  0.1723770476124679
iteration : 356
train acc:  0.8046875
train loss:  0.3576313257217407
train gradient:  0.1596642581140693
iteration : 357
train acc:  0.8828125
train loss:  0.30040496587753296
train gradient:  0.12904100426743625
iteration : 358
train acc:  0.8125
train loss:  0.31715402007102966
train gradient:  0.11539935762415948
iteration : 359
train acc:  0.8828125
train loss:  0.27689898014068604
train gradient:  0.08335041994213256
iteration : 360
train acc:  0.859375
train loss:  0.3002069294452667
train gradient:  0.12466929170917294
iteration : 361
train acc:  0.8515625
train loss:  0.35896486043930054
train gradient:  0.17097904700688543
iteration : 362
train acc:  0.890625
train loss:  0.2692318558692932
train gradient:  0.1770103677219621
iteration : 363
train acc:  0.84375
train loss:  0.31246113777160645
train gradient:  0.13236543942085616
iteration : 364
train acc:  0.890625
train loss:  0.2630102336406708
train gradient:  0.09532098049690924
iteration : 365
train acc:  0.8515625
train loss:  0.3018847703933716
train gradient:  0.16293152538953076
iteration : 366
train acc:  0.875
train loss:  0.2670252323150635
train gradient:  0.1053179533966537
iteration : 367
train acc:  0.8828125
train loss:  0.2963956594467163
train gradient:  0.10601300086817339
iteration : 368
train acc:  0.890625
train loss:  0.25577211380004883
train gradient:  0.39350255045029914
iteration : 369
train acc:  0.90625
train loss:  0.23984214663505554
train gradient:  0.10332392677202532
iteration : 370
train acc:  0.84375
train loss:  0.34796056151390076
train gradient:  0.1274431250952585
iteration : 371
train acc:  0.875
train loss:  0.3321300745010376
train gradient:  0.11501986127670556
iteration : 372
train acc:  0.8828125
train loss:  0.33867037296295166
train gradient:  0.1251296042488006
iteration : 373
train acc:  0.84375
train loss:  0.34930384159088135
train gradient:  0.13767672915498838
iteration : 374
train acc:  0.875
train loss:  0.2853805422782898
train gradient:  0.13202120975318538
iteration : 375
train acc:  0.8828125
train loss:  0.28906434774398804
train gradient:  0.09445511668839092
iteration : 376
train acc:  0.8671875
train loss:  0.3293873369693756
train gradient:  0.16066224810134325
iteration : 377
train acc:  0.8515625
train loss:  0.2994934022426605
train gradient:  0.14172936656703333
iteration : 378
train acc:  0.90625
train loss:  0.29417675733566284
train gradient:  0.09849381920238562
iteration : 379
train acc:  0.8671875
train loss:  0.37566491961479187
train gradient:  0.27720090123124747
iteration : 380
train acc:  0.828125
train loss:  0.38527432084083557
train gradient:  0.2435975783453564
iteration : 381
train acc:  0.8984375
train loss:  0.2363353967666626
train gradient:  0.1393224816344611
iteration : 382
train acc:  0.8203125
train loss:  0.38609620928764343
train gradient:  0.23387229282688654
iteration : 383
train acc:  0.828125
train loss:  0.35033655166625977
train gradient:  0.13039506551337363
iteration : 384
train acc:  0.8515625
train loss:  0.2982293367385864
train gradient:  0.13929200096262195
iteration : 385
train acc:  0.90625
train loss:  0.23586972057819366
train gradient:  0.08662109928003585
iteration : 386
train acc:  0.8515625
train loss:  0.3345257639884949
train gradient:  0.1451775508422947
iteration : 387
train acc:  0.8984375
train loss:  0.24782593548297882
train gradient:  0.08223764671694038
iteration : 388
train acc:  0.84375
train loss:  0.3764442801475525
train gradient:  0.218350549963848
iteration : 389
train acc:  0.84375
train loss:  0.2930147051811218
train gradient:  0.09791623854407405
iteration : 390
train acc:  0.8203125
train loss:  0.3707970976829529
train gradient:  0.19777392981420105
iteration : 391
train acc:  0.8125
train loss:  0.4161621332168579
train gradient:  0.244246414801713
iteration : 392
train acc:  0.84375
train loss:  0.38808608055114746
train gradient:  0.26066220103998744
iteration : 393
train acc:  0.8828125
train loss:  0.28381699323654175
train gradient:  0.10015177702049792
iteration : 394
train acc:  0.890625
train loss:  0.2732730209827423
train gradient:  0.1091634133375572
iteration : 395
train acc:  0.859375
train loss:  0.3606380820274353
train gradient:  0.17830819841431234
iteration : 396
train acc:  0.9140625
train loss:  0.25812286138534546
train gradient:  0.07679590961052017
iteration : 397
train acc:  0.8671875
train loss:  0.27570831775665283
train gradient:  0.1412457955882383
iteration : 398
train acc:  0.8828125
train loss:  0.2868853211402893
train gradient:  0.10756478373577627
iteration : 399
train acc:  0.859375
train loss:  0.3618689179420471
train gradient:  0.31298766907228126
iteration : 400
train acc:  0.8359375
train loss:  0.3692375421524048
train gradient:  0.21149713347845317
iteration : 401
train acc:  0.875
train loss:  0.33940720558166504
train gradient:  0.1481066128996893
iteration : 402
train acc:  0.8359375
train loss:  0.36954185366630554
train gradient:  0.14760984725063747
iteration : 403
train acc:  0.875
train loss:  0.2963370382785797
train gradient:  0.17399113461482885
iteration : 404
train acc:  0.84375
train loss:  0.38099080324172974
train gradient:  0.21337119249095618
iteration : 405
train acc:  0.8515625
train loss:  0.3313101530075073
train gradient:  0.22026958741783242
iteration : 406
train acc:  0.8203125
train loss:  0.3333936333656311
train gradient:  0.15536909434088084
iteration : 407
train acc:  0.8984375
train loss:  0.24852558970451355
train gradient:  0.0880741681895492
iteration : 408
train acc:  0.796875
train loss:  0.4116172790527344
train gradient:  0.22085843001377403
iteration : 409
train acc:  0.8203125
train loss:  0.3356357216835022
train gradient:  0.16330554688489488
iteration : 410
train acc:  0.875
train loss:  0.3066665232181549
train gradient:  0.12222137770471075
iteration : 411
train acc:  0.84375
train loss:  0.3264539837837219
train gradient:  0.13132295579662057
iteration : 412
train acc:  0.84375
train loss:  0.3467022180557251
train gradient:  0.15095993675103142
iteration : 413
train acc:  0.84375
train loss:  0.3402206599712372
train gradient:  0.15135888754777144
iteration : 414
train acc:  0.84375
train loss:  0.353045791387558
train gradient:  0.14274591940073983
iteration : 415
train acc:  0.8828125
train loss:  0.2765481472015381
train gradient:  0.08908002554142189
iteration : 416
train acc:  0.8515625
train loss:  0.32213395833969116
train gradient:  0.12317753668126512
iteration : 417
train acc:  0.8515625
train loss:  0.3803446292877197
train gradient:  0.15548004985763283
iteration : 418
train acc:  0.8984375
train loss:  0.3181162476539612
train gradient:  0.15816479157478552
iteration : 419
train acc:  0.890625
train loss:  0.23176956176757812
train gradient:  0.1017511062169594
iteration : 420
train acc:  0.8984375
train loss:  0.24939504265785217
train gradient:  0.09539836776518139
iteration : 421
train acc:  0.875
train loss:  0.2745826244354248
train gradient:  0.08814515183630271
iteration : 422
train acc:  0.859375
train loss:  0.3996700048446655
train gradient:  0.18270668715278393
iteration : 423
train acc:  0.8359375
train loss:  0.3736628293991089
train gradient:  0.1604681736524581
iteration : 424
train acc:  0.859375
train loss:  0.29364213347435
train gradient:  0.18261762411576948
iteration : 425
train acc:  0.859375
train loss:  0.3128582537174225
train gradient:  0.1646788831221263
iteration : 426
train acc:  0.8359375
train loss:  0.3663071393966675
train gradient:  0.16154812643349653
iteration : 427
train acc:  0.8984375
train loss:  0.3067639470100403
train gradient:  0.09166872801208105
iteration : 428
train acc:  0.7890625
train loss:  0.3524976372718811
train gradient:  0.17107736144046254
iteration : 429
train acc:  0.859375
train loss:  0.34920012950897217
train gradient:  0.1376058866385677
iteration : 430
train acc:  0.8515625
train loss:  0.3231874406337738
train gradient:  0.12765442328900678
iteration : 431
train acc:  0.875
train loss:  0.3231695294380188
train gradient:  0.1949491741348091
iteration : 432
train acc:  0.890625
train loss:  0.28815263509750366
train gradient:  0.10987387073221566
iteration : 433
train acc:  0.859375
train loss:  0.32193994522094727
train gradient:  0.1402517930239528
iteration : 434
train acc:  0.8515625
train loss:  0.2744435667991638
train gradient:  0.06733534877164431
iteration : 435
train acc:  0.8359375
train loss:  0.37530747056007385
train gradient:  0.12143526943295625
iteration : 436
train acc:  0.8359375
train loss:  0.3167024254798889
train gradient:  0.12603438223743402
iteration : 437
train acc:  0.90625
train loss:  0.26192471385002136
train gradient:  0.1440703306701148
iteration : 438
train acc:  0.8671875
train loss:  0.27782535552978516
train gradient:  0.1139247632987504
iteration : 439
train acc:  0.8671875
train loss:  0.3950890600681305
train gradient:  0.23239838456759337
iteration : 440
train acc:  0.875
train loss:  0.30173856019973755
train gradient:  0.1434135133204535
iteration : 441
train acc:  0.8359375
train loss:  0.33943915367126465
train gradient:  0.18436168389533963
iteration : 442
train acc:  0.8828125
train loss:  0.3035666346549988
train gradient:  0.12022151371537092
iteration : 443
train acc:  0.8828125
train loss:  0.2831261456012726
train gradient:  0.1337299864393665
iteration : 444
train acc:  0.890625
train loss:  0.28618311882019043
train gradient:  0.1692254935836155
iteration : 445
train acc:  0.9140625
train loss:  0.24868327379226685
train gradient:  0.08489888132729083
iteration : 446
train acc:  0.890625
train loss:  0.3184526264667511
train gradient:  0.12714397563802896
iteration : 447
train acc:  0.8046875
train loss:  0.4501636028289795
train gradient:  0.20299262612937038
iteration : 448
train acc:  0.8203125
train loss:  0.4143086075782776
train gradient:  0.17187175456230316
iteration : 449
train acc:  0.8671875
train loss:  0.31713631749153137
train gradient:  0.11821958283771263
iteration : 450
train acc:  0.8828125
train loss:  0.3108586072921753
train gradient:  0.16436529023076235
iteration : 451
train acc:  0.8671875
train loss:  0.3305619955062866
train gradient:  0.14244435513284515
iteration : 452
train acc:  0.8984375
train loss:  0.3000795841217041
train gradient:  0.10684071086120342
iteration : 453
train acc:  0.8515625
train loss:  0.3080420196056366
train gradient:  0.11580128587012989
iteration : 454
train acc:  0.8515625
train loss:  0.33740025758743286
train gradient:  0.16486148030581013
iteration : 455
train acc:  0.84375
train loss:  0.3421614170074463
train gradient:  0.1284505162274735
iteration : 456
train acc:  0.8125
train loss:  0.392098605632782
train gradient:  0.1658607098534131
iteration : 457
train acc:  0.859375
train loss:  0.30234572291374207
train gradient:  0.08015070573774583
iteration : 458
train acc:  0.8828125
train loss:  0.2923344671726227
train gradient:  0.10345047164258724
iteration : 459
train acc:  0.90625
train loss:  0.2436015009880066
train gradient:  0.14071725695354065
iteration : 460
train acc:  0.859375
train loss:  0.2953859567642212
train gradient:  0.12636931163658732
iteration : 461
train acc:  0.8671875
train loss:  0.3106911778450012
train gradient:  0.1280098359682093
iteration : 462
train acc:  0.8359375
train loss:  0.33825045824050903
train gradient:  0.12699533002776314
iteration : 463
train acc:  0.859375
train loss:  0.2959398627281189
train gradient:  0.11207111022419979
iteration : 464
train acc:  0.8671875
train loss:  0.27768081426620483
train gradient:  0.09675176278097705
iteration : 465
train acc:  0.8671875
train loss:  0.32474830746650696
train gradient:  0.10788427830299313
iteration : 466
train acc:  0.828125
train loss:  0.3606782853603363
train gradient:  0.1310119511965533
iteration : 467
train acc:  0.8125
train loss:  0.36140966415405273
train gradient:  0.1460520422180197
iteration : 468
train acc:  0.921875
train loss:  0.26904499530792236
train gradient:  0.13640668854623592
iteration : 469
train acc:  0.7734375
train loss:  0.4255650043487549
train gradient:  0.1820687087488842
iteration : 470
train acc:  0.875
train loss:  0.28952765464782715
train gradient:  0.1277275810188988
iteration : 471
train acc:  0.859375
train loss:  0.3566487431526184
train gradient:  0.1546841531260923
iteration : 472
train acc:  0.8125
train loss:  0.39940333366394043
train gradient:  0.16597704807871633
iteration : 473
train acc:  0.859375
train loss:  0.364969402551651
train gradient:  0.13557594672582574
iteration : 474
train acc:  0.8359375
train loss:  0.3056848645210266
train gradient:  0.1525151845233867
iteration : 475
train acc:  0.875
train loss:  0.3219168186187744
train gradient:  0.23113989063197432
iteration : 476
train acc:  0.9140625
train loss:  0.22922970354557037
train gradient:  0.09927945656195823
iteration : 477
train acc:  0.8125
train loss:  0.3501976728439331
train gradient:  0.10848910860241465
iteration : 478
train acc:  0.875
train loss:  0.3092869520187378
train gradient:  0.1378678715067115
iteration : 479
train acc:  0.859375
train loss:  0.3075113594532013
train gradient:  0.11670362605573446
iteration : 480
train acc:  0.8828125
train loss:  0.25372859835624695
train gradient:  0.0914400613974983
iteration : 481
train acc:  0.875
train loss:  0.3057287335395813
train gradient:  0.13291544275121991
iteration : 482
train acc:  0.859375
train loss:  0.27265995740890503
train gradient:  0.18284826196215448
iteration : 483
train acc:  0.8984375
train loss:  0.23542360961437225
train gradient:  0.060731271506960754
iteration : 484
train acc:  0.8828125
train loss:  0.2657218277454376
train gradient:  0.07525443538551252
iteration : 485
train acc:  0.8515625
train loss:  0.3773565888404846
train gradient:  0.17854722795188488
iteration : 486
train acc:  0.8515625
train loss:  0.37280118465423584
train gradient:  0.11430723009908417
iteration : 487
train acc:  0.84375
train loss:  0.29918062686920166
train gradient:  0.11890842616694756
iteration : 488
train acc:  0.859375
train loss:  0.3213983178138733
train gradient:  0.1462895673898325
iteration : 489
train acc:  0.84375
train loss:  0.33711597323417664
train gradient:  0.17780819455147717
iteration : 490
train acc:  0.828125
train loss:  0.36727914214134216
train gradient:  0.13379167685806803
iteration : 491
train acc:  0.8671875
train loss:  0.3284660577774048
train gradient:  0.14168764056295186
iteration : 492
train acc:  0.859375
train loss:  0.2933666706085205
train gradient:  0.12196530676106923
iteration : 493
train acc:  0.90625
train loss:  0.2644704282283783
train gradient:  0.09297312774501951
iteration : 494
train acc:  0.8359375
train loss:  0.3496114909648895
train gradient:  0.13035059274820923
iteration : 495
train acc:  0.9140625
train loss:  0.2042033076286316
train gradient:  0.07295194148909409
iteration : 496
train acc:  0.8828125
train loss:  0.2722155749797821
train gradient:  0.10511717004068101
iteration : 497
train acc:  0.9140625
train loss:  0.27959322929382324
train gradient:  0.11659607856770103
iteration : 498
train acc:  0.8515625
train loss:  0.3111041784286499
train gradient:  0.10925279927696437
iteration : 499
train acc:  0.890625
train loss:  0.23935379087924957
train gradient:  0.09470776955155119
iteration : 500
train acc:  0.8515625
train loss:  0.3176707327365875
train gradient:  0.16136901951862004
iteration : 501
train acc:  0.8671875
train loss:  0.31586360931396484
train gradient:  0.12253833217508571
iteration : 502
train acc:  0.8671875
train loss:  0.34658122062683105
train gradient:  0.1269594777485724
iteration : 503
train acc:  0.875
train loss:  0.2791864275932312
train gradient:  0.14856462152544792
iteration : 504
train acc:  0.890625
train loss:  0.27123627066612244
train gradient:  0.09077623574522255
iteration : 505
train acc:  0.828125
train loss:  0.37951186299324036
train gradient:  0.14763333270595028
iteration : 506
train acc:  0.90625
train loss:  0.26361608505249023
train gradient:  0.08063536114479476
iteration : 507
train acc:  0.8515625
train loss:  0.3411223888397217
train gradient:  0.22106101138415302
iteration : 508
train acc:  0.859375
train loss:  0.28635385632514954
train gradient:  0.09303954115261508
iteration : 509
train acc:  0.859375
train loss:  0.3971114158630371
train gradient:  0.19145111363233758
iteration : 510
train acc:  0.90625
train loss:  0.23947158455848694
train gradient:  0.07335267783967006
iteration : 511
train acc:  0.875
train loss:  0.2839987277984619
train gradient:  0.09276254587015233
iteration : 512
train acc:  0.828125
train loss:  0.37623125314712524
train gradient:  0.12695850690783145
iteration : 513
train acc:  0.8828125
train loss:  0.29615554213523865
train gradient:  0.12032449215242101
iteration : 514
train acc:  0.84375
train loss:  0.30518966913223267
train gradient:  0.13910089211437132
iteration : 515
train acc:  0.859375
train loss:  0.29563939571380615
train gradient:  0.11928983194338198
iteration : 516
train acc:  0.90625
train loss:  0.26934194564819336
train gradient:  0.08295433380322433
iteration : 517
train acc:  0.7734375
train loss:  0.42152008414268494
train gradient:  0.2902724958981716
iteration : 518
train acc:  0.8359375
train loss:  0.33608829975128174
train gradient:  0.11711053003253584
iteration : 519
train acc:  0.875
train loss:  0.29971107840538025
train gradient:  0.13539498568178748
iteration : 520
train acc:  0.8828125
train loss:  0.3242080807685852
train gradient:  0.15043042635030973
iteration : 521
train acc:  0.8984375
train loss:  0.29227662086486816
train gradient:  0.08556383822894585
iteration : 522
train acc:  0.8984375
train loss:  0.25693845748901367
train gradient:  0.07768215461753189
iteration : 523
train acc:  0.8046875
train loss:  0.4114897847175598
train gradient:  0.22699116141930586
iteration : 524
train acc:  0.8515625
train loss:  0.3373493552207947
train gradient:  0.13881149610349774
iteration : 525
train acc:  0.8671875
train loss:  0.35787415504455566
train gradient:  0.2039456312152179
iteration : 526
train acc:  0.8671875
train loss:  0.3168885409832001
train gradient:  0.1645883018199252
iteration : 527
train acc:  0.859375
train loss:  0.310982882976532
train gradient:  0.09328504380244419
iteration : 528
train acc:  0.890625
train loss:  0.25464579463005066
train gradient:  0.10340277249579818
iteration : 529
train acc:  0.890625
train loss:  0.3711280822753906
train gradient:  0.1704772043482931
iteration : 530
train acc:  0.8359375
train loss:  0.3726669251918793
train gradient:  0.1700560722101086
iteration : 531
train acc:  0.890625
train loss:  0.23434074223041534
train gradient:  0.09963054431943642
iteration : 532
train acc:  0.875
train loss:  0.2998470664024353
train gradient:  0.09436258281134788
iteration : 533
train acc:  0.9453125
train loss:  0.189261794090271
train gradient:  0.0732698469075182
iteration : 534
train acc:  0.84375
train loss:  0.3269486427307129
train gradient:  0.1492243709232169
iteration : 535
train acc:  0.90625
train loss:  0.2551153302192688
train gradient:  0.15358728633765162
iteration : 536
train acc:  0.9140625
train loss:  0.27605295181274414
train gradient:  0.14849982964263775
iteration : 537
train acc:  0.8125
train loss:  0.330096960067749
train gradient:  0.16320309485379184
iteration : 538
train acc:  0.8203125
train loss:  0.3726581931114197
train gradient:  0.17375859959038703
iteration : 539
train acc:  0.859375
train loss:  0.3951700031757355
train gradient:  0.15659031536966728
iteration : 540
train acc:  0.8828125
train loss:  0.2710743546485901
train gradient:  0.08505046454135731
iteration : 541
train acc:  0.859375
train loss:  0.3395163118839264
train gradient:  0.11897962792353796
iteration : 542
train acc:  0.8203125
train loss:  0.33150357007980347
train gradient:  0.17020489484745577
iteration : 543
train acc:  0.859375
train loss:  0.324130117893219
train gradient:  0.10199444856967045
iteration : 544
train acc:  0.859375
train loss:  0.35324639081954956
train gradient:  0.1496606076386282
iteration : 545
train acc:  0.8828125
train loss:  0.3100554049015045
train gradient:  0.12737429446121556
iteration : 546
train acc:  0.8671875
train loss:  0.2966034412384033
train gradient:  0.1304815441270514
iteration : 547
train acc:  0.8984375
train loss:  0.30108875036239624
train gradient:  0.11359473392224557
iteration : 548
train acc:  0.8671875
train loss:  0.26394984126091003
train gradient:  0.14357565422551932
iteration : 549
train acc:  0.875
train loss:  0.3996186852455139
train gradient:  0.26070846609051934
iteration : 550
train acc:  0.875
train loss:  0.2968316972255707
train gradient:  0.13260244217125725
iteration : 551
train acc:  0.8828125
train loss:  0.2992706298828125
train gradient:  0.17594343749182684
iteration : 552
train acc:  0.8359375
train loss:  0.35243669152259827
train gradient:  0.1294392683792706
iteration : 553
train acc:  0.8359375
train loss:  0.3254912495613098
train gradient:  0.1428336373199082
iteration : 554
train acc:  0.8359375
train loss:  0.33981120586395264
train gradient:  0.2526191449099927
iteration : 555
train acc:  0.8828125
train loss:  0.3066653609275818
train gradient:  0.0975546628172526
iteration : 556
train acc:  0.8203125
train loss:  0.3626704812049866
train gradient:  0.1597255760820538
iteration : 557
train acc:  0.8671875
train loss:  0.23956327140331268
train gradient:  0.06556070694286092
iteration : 558
train acc:  0.8125
train loss:  0.34951484203338623
train gradient:  0.1253022318480947
iteration : 559
train acc:  0.921875
train loss:  0.2405054122209549
train gradient:  0.09862839607323115
iteration : 560
train acc:  0.8671875
train loss:  0.2628527283668518
train gradient:  0.06939026500711443
iteration : 561
train acc:  0.8203125
train loss:  0.32442086935043335
train gradient:  0.11552558461874672
iteration : 562
train acc:  0.9375
train loss:  0.2222708761692047
train gradient:  0.09466140772029936
iteration : 563
train acc:  0.84375
train loss:  0.33193305134773254
train gradient:  0.14269224752777565
iteration : 564
train acc:  0.8828125
train loss:  0.2392440140247345
train gradient:  0.10659815242976307
iteration : 565
train acc:  0.828125
train loss:  0.3724987506866455
train gradient:  0.1964322334301035
iteration : 566
train acc:  0.859375
train loss:  0.3571763038635254
train gradient:  0.1266199804449703
iteration : 567
train acc:  0.8515625
train loss:  0.33896559476852417
train gradient:  0.1861991772776363
iteration : 568
train acc:  0.8828125
train loss:  0.3280136287212372
train gradient:  0.15654367506294867
iteration : 569
train acc:  0.921875
train loss:  0.24416714906692505
train gradient:  0.09050820163445655
iteration : 570
train acc:  0.84375
train loss:  0.4075295329093933
train gradient:  0.23132090765060512
iteration : 571
train acc:  0.8046875
train loss:  0.3766345679759979
train gradient:  0.17878469646440964
iteration : 572
train acc:  0.890625
train loss:  0.3265680968761444
train gradient:  0.14285276488768317
iteration : 573
train acc:  0.84375
train loss:  0.37064164876937866
train gradient:  0.2643274622028073
iteration : 574
train acc:  0.90625
train loss:  0.286714643239975
train gradient:  0.1130620466642016
iteration : 575
train acc:  0.8203125
train loss:  0.37401121854782104
train gradient:  0.147780730670795
iteration : 576
train acc:  0.8203125
train loss:  0.42417487502098083
train gradient:  0.2640082843748435
iteration : 577
train acc:  0.8828125
train loss:  0.292234867811203
train gradient:  0.08596050376879717
iteration : 578
train acc:  0.890625
train loss:  0.29926103353500366
train gradient:  0.16117639993517552
iteration : 579
train acc:  0.8984375
train loss:  0.32334333658218384
train gradient:  0.1124185428874726
iteration : 580
train acc:  0.875
train loss:  0.2696232497692108
train gradient:  0.10792135365585556
iteration : 581
train acc:  0.8671875
train loss:  0.323748379945755
train gradient:  0.21648254057061594
iteration : 582
train acc:  0.859375
train loss:  0.32333022356033325
train gradient:  0.11986767994152103
iteration : 583
train acc:  0.8671875
train loss:  0.354594349861145
train gradient:  0.1509550512325098
iteration : 584
train acc:  0.859375
train loss:  0.27780255675315857
train gradient:  0.09121298935426853
iteration : 585
train acc:  0.90625
train loss:  0.23326393961906433
train gradient:  0.09814498580347225
iteration : 586
train acc:  0.8515625
train loss:  0.35006946325302124
train gradient:  0.1359925337842116
iteration : 587
train acc:  0.8515625
train loss:  0.3026033639907837
train gradient:  0.16143167030496647
iteration : 588
train acc:  0.8046875
train loss:  0.3757156729698181
train gradient:  0.19343517452202616
iteration : 589
train acc:  0.859375
train loss:  0.30517303943634033
train gradient:  0.12565068530765675
iteration : 590
train acc:  0.859375
train loss:  0.354681134223938
train gradient:  0.21716755237928428
iteration : 591
train acc:  0.875
train loss:  0.2906971573829651
train gradient:  0.10522140445739421
iteration : 592
train acc:  0.859375
train loss:  0.39429107308387756
train gradient:  0.14570430851072202
iteration : 593
train acc:  0.8359375
train loss:  0.33727195858955383
train gradient:  0.2120397514736755
iteration : 594
train acc:  0.8125
train loss:  0.40639710426330566
train gradient:  0.22878725029029262
iteration : 595
train acc:  0.9140625
train loss:  0.23952451348304749
train gradient:  0.08720227127137811
iteration : 596
train acc:  0.8828125
train loss:  0.2501067519187927
train gradient:  0.12108617925739536
iteration : 597
train acc:  0.8203125
train loss:  0.3365601599216461
train gradient:  0.11306153564720907
iteration : 598
train acc:  0.8671875
train loss:  0.2716061472892761
train gradient:  0.13685849933534133
iteration : 599
train acc:  0.90625
train loss:  0.3051758408546448
train gradient:  0.10766679092504086
iteration : 600
train acc:  0.890625
train loss:  0.2673642039299011
train gradient:  0.11405225159296815
iteration : 601
train acc:  0.8828125
train loss:  0.3092172145843506
train gradient:  0.17349740387495696
iteration : 602
train acc:  0.84375
train loss:  0.32590076327323914
train gradient:  0.14710571138371004
iteration : 603
train acc:  0.828125
train loss:  0.33410418033599854
train gradient:  0.14682613343402096
iteration : 604
train acc:  0.9140625
train loss:  0.2649065852165222
train gradient:  0.1720861206442333
iteration : 605
train acc:  0.8125
train loss:  0.335236132144928
train gradient:  0.13375713722010335
iteration : 606
train acc:  0.8046875
train loss:  0.41758817434310913
train gradient:  0.20451427487810317
iteration : 607
train acc:  0.8515625
train loss:  0.3319295644760132
train gradient:  0.21122709263255512
iteration : 608
train acc:  0.875
train loss:  0.32219672203063965
train gradient:  0.12868745076657867
iteration : 609
train acc:  0.8984375
train loss:  0.24484838545322418
train gradient:  0.12385954226528137
iteration : 610
train acc:  0.8515625
train loss:  0.3417865037918091
train gradient:  0.14891300967727938
iteration : 611
train acc:  0.875
train loss:  0.33943602442741394
train gradient:  0.16123430321528925
iteration : 612
train acc:  0.9140625
train loss:  0.2271154820919037
train gradient:  0.11078708249212353
iteration : 613
train acc:  0.8671875
train loss:  0.3196983337402344
train gradient:  0.14551846127737322
iteration : 614
train acc:  0.8203125
train loss:  0.3694494664669037
train gradient:  0.16564993351376212
iteration : 615
train acc:  0.8515625
train loss:  0.38029009103775024
train gradient:  0.1502876986744675
iteration : 616
train acc:  0.8359375
train loss:  0.3070693910121918
train gradient:  0.13652240784801178
iteration : 617
train acc:  0.8828125
train loss:  0.2651204764842987
train gradient:  0.10376246157746304
iteration : 618
train acc:  0.8828125
train loss:  0.2888629734516144
train gradient:  0.0937046191879419
iteration : 619
train acc:  0.828125
train loss:  0.37998276948928833
train gradient:  0.1410242851611027
iteration : 620
train acc:  0.890625
train loss:  0.2612350881099701
train gradient:  0.1354696063764826
iteration : 621
train acc:  0.8203125
train loss:  0.40961310267448425
train gradient:  0.22367262254036624
iteration : 622
train acc:  0.84375
train loss:  0.3300858438014984
train gradient:  0.1645237379695209
iteration : 623
train acc:  0.875
train loss:  0.2732987701892853
train gradient:  0.11973844679441892
iteration : 624
train acc:  0.9140625
train loss:  0.24227522313594818
train gradient:  0.07725116231023466
iteration : 625
train acc:  0.828125
train loss:  0.37449583411216736
train gradient:  0.13654141601883701
iteration : 626
train acc:  0.8671875
train loss:  0.318103551864624
train gradient:  0.10355806648399353
iteration : 627
train acc:  0.890625
train loss:  0.2501973509788513
train gradient:  0.07870389939007004
iteration : 628
train acc:  0.875
train loss:  0.33282607793807983
train gradient:  0.1565515051366404
iteration : 629
train acc:  0.8203125
train loss:  0.329351007938385
train gradient:  0.11714990704231436
iteration : 630
train acc:  0.90625
train loss:  0.2579458951950073
train gradient:  0.10433854402691477
iteration : 631
train acc:  0.8984375
train loss:  0.2522832751274109
train gradient:  0.08305335387796198
iteration : 632
train acc:  0.8046875
train loss:  0.39149728417396545
train gradient:  0.16196788976251497
iteration : 633
train acc:  0.9140625
train loss:  0.2416359782218933
train gradient:  0.07374955735539551
iteration : 634
train acc:  0.8359375
train loss:  0.3733019530773163
train gradient:  0.16655252688103478
iteration : 635
train acc:  0.9140625
train loss:  0.281180739402771
train gradient:  0.12483488383939975
iteration : 636
train acc:  0.8125
train loss:  0.396124929189682
train gradient:  0.18839132661780253
iteration : 637
train acc:  0.90625
train loss:  0.22163125872612
train gradient:  0.06331587823333713
iteration : 638
train acc:  0.8984375
train loss:  0.2838231325149536
train gradient:  0.11738754065834195
iteration : 639
train acc:  0.8828125
train loss:  0.2990754544734955
train gradient:  0.09516246150428359
iteration : 640
train acc:  0.875
train loss:  0.2957218289375305
train gradient:  0.13122410091720832
iteration : 641
train acc:  0.8984375
train loss:  0.2376386523246765
train gradient:  0.07630388448550283
iteration : 642
train acc:  0.875
train loss:  0.27029210329055786
train gradient:  0.08621637212998376
iteration : 643
train acc:  0.875
train loss:  0.32700788974761963
train gradient:  0.19827702748602444
iteration : 644
train acc:  0.8515625
train loss:  0.27833423018455505
train gradient:  0.15071250418363724
iteration : 645
train acc:  0.8359375
train loss:  0.3759320080280304
train gradient:  0.16431508734782696
iteration : 646
train acc:  0.90625
train loss:  0.22616758942604065
train gradient:  0.05918186150475867
iteration : 647
train acc:  0.875
train loss:  0.30807292461395264
train gradient:  0.08964679209265623
iteration : 648
train acc:  0.890625
train loss:  0.2869632840156555
train gradient:  0.1386598731438667
iteration : 649
train acc:  0.84375
train loss:  0.31733596324920654
train gradient:  0.23037395415927056
iteration : 650
train acc:  0.84375
train loss:  0.3244349956512451
train gradient:  0.16965605840964637
iteration : 651
train acc:  0.8984375
train loss:  0.2577621638774872
train gradient:  0.10596647672161272
iteration : 652
train acc:  0.8359375
train loss:  0.3325877785682678
train gradient:  0.1571535886897919
iteration : 653
train acc:  0.875
train loss:  0.3040371239185333
train gradient:  0.10744105976077288
iteration : 654
train acc:  0.8515625
train loss:  0.3137977123260498
train gradient:  0.16197886472031864
iteration : 655
train acc:  0.8046875
train loss:  0.41774511337280273
train gradient:  0.19638439930327647
iteration : 656
train acc:  0.890625
train loss:  0.25884637236595154
train gradient:  0.09952153831955264
iteration : 657
train acc:  0.8359375
train loss:  0.37259554862976074
train gradient:  0.17244386332399136
iteration : 658
train acc:  0.8359375
train loss:  0.3234248161315918
train gradient:  0.15103282539187032
iteration : 659
train acc:  0.90625
train loss:  0.2347021847963333
train gradient:  0.060026716448507114
iteration : 660
train acc:  0.8203125
train loss:  0.35544121265411377
train gradient:  0.13166916274541546
iteration : 661
train acc:  0.8046875
train loss:  0.4099271297454834
train gradient:  0.19211501765336206
iteration : 662
train acc:  0.890625
train loss:  0.2586628496646881
train gradient:  0.1464744906608847
iteration : 663
train acc:  0.828125
train loss:  0.3769073784351349
train gradient:  0.2648304171429643
iteration : 664
train acc:  0.90625
train loss:  0.3114485740661621
train gradient:  0.16892734952311356
iteration : 665
train acc:  0.84375
train loss:  0.33094534277915955
train gradient:  0.1133998814116795
iteration : 666
train acc:  0.828125
train loss:  0.43262338638305664
train gradient:  0.35995199128174654
iteration : 667
train acc:  0.9296875
train loss:  0.2681266963481903
train gradient:  0.12198350950847629
iteration : 668
train acc:  0.84375
train loss:  0.380096971988678
train gradient:  0.1840003631065669
iteration : 669
train acc:  0.828125
train loss:  0.30388182401657104
train gradient:  0.11143592574455172
iteration : 670
train acc:  0.890625
train loss:  0.25454181432724
train gradient:  0.06344437738272464
iteration : 671
train acc:  0.8671875
train loss:  0.2982378602027893
train gradient:  0.12616760860201084
iteration : 672
train acc:  0.859375
train loss:  0.30775338411331177
train gradient:  0.12742801329578937
iteration : 673
train acc:  0.8671875
train loss:  0.2609085440635681
train gradient:  0.08675283886312478
iteration : 674
train acc:  0.8515625
train loss:  0.2993696928024292
train gradient:  0.13817808497944847
iteration : 675
train acc:  0.8828125
train loss:  0.29104578495025635
train gradient:  0.16872634080277488
iteration : 676
train acc:  0.8671875
train loss:  0.2439090460538864
train gradient:  0.18277803504642057
iteration : 677
train acc:  0.8125
train loss:  0.3621695637702942
train gradient:  0.1707121120265045
iteration : 678
train acc:  0.8515625
train loss:  0.3486971855163574
train gradient:  0.1723557471960541
iteration : 679
train acc:  0.875
train loss:  0.27703794836997986
train gradient:  0.08587416415467355
iteration : 680
train acc:  0.8828125
train loss:  0.27878373861312866
train gradient:  0.08632041042667576
iteration : 681
train acc:  0.8359375
train loss:  0.42645177245140076
train gradient:  0.23890177543196572
iteration : 682
train acc:  0.84375
train loss:  0.3746699094772339
train gradient:  0.16921364990679202
iteration : 683
train acc:  0.8515625
train loss:  0.3459848463535309
train gradient:  0.1138120624331126
iteration : 684
train acc:  0.84375
train loss:  0.3011939525604248
train gradient:  0.13327860583863216
iteration : 685
train acc:  0.8671875
train loss:  0.31969642639160156
train gradient:  0.11659438012632818
iteration : 686
train acc:  0.84375
train loss:  0.32243987917900085
train gradient:  0.1493064088240436
iteration : 687
train acc:  0.8359375
train loss:  0.31909656524658203
train gradient:  0.13714335346382453
iteration : 688
train acc:  0.8125
train loss:  0.3520895540714264
train gradient:  0.16317755197466477
iteration : 689
train acc:  0.875
train loss:  0.2629506289958954
train gradient:  0.10127278872638887
iteration : 690
train acc:  0.84375
train loss:  0.3553480803966522
train gradient:  0.14117937996543906
iteration : 691
train acc:  0.8828125
train loss:  0.24882236123085022
train gradient:  0.09970443868219679
iteration : 692
train acc:  0.84375
train loss:  0.2863501310348511
train gradient:  0.08971367842516899
iteration : 693
train acc:  0.875
train loss:  0.29400181770324707
train gradient:  0.08265862903856377
iteration : 694
train acc:  0.8671875
train loss:  0.3258916139602661
train gradient:  0.10718824726110307
iteration : 695
train acc:  0.9140625
train loss:  0.2568173408508301
train gradient:  0.0637455260253479
iteration : 696
train acc:  0.8671875
train loss:  0.2835721969604492
train gradient:  0.11400235899604992
iteration : 697
train acc:  0.8515625
train loss:  0.293521523475647
train gradient:  0.11400983591372478
iteration : 698
train acc:  0.8515625
train loss:  0.2990540564060211
train gradient:  0.13201726881651699
iteration : 699
train acc:  0.8359375
train loss:  0.30244967341423035
train gradient:  0.1459183860349645
iteration : 700
train acc:  0.9140625
train loss:  0.25160473585128784
train gradient:  0.09069211856716212
iteration : 701
train acc:  0.9375
train loss:  0.21220895648002625
train gradient:  0.09529747883492885
iteration : 702
train acc:  0.875
train loss:  0.3076426684856415
train gradient:  0.20828514666630815
iteration : 703
train acc:  0.8359375
train loss:  0.3436574339866638
train gradient:  0.16810604748927466
iteration : 704
train acc:  0.859375
train loss:  0.30485591292381287
train gradient:  0.13446034185811045
iteration : 705
train acc:  0.859375
train loss:  0.29849737882614136
train gradient:  0.12978146362594423
iteration : 706
train acc:  0.859375
train loss:  0.30512142181396484
train gradient:  0.08446733011738163
iteration : 707
train acc:  0.84375
train loss:  0.32426244020462036
train gradient:  0.12725944687629467
iteration : 708
train acc:  0.8828125
train loss:  0.3365083932876587
train gradient:  0.13640919526968864
iteration : 709
train acc:  0.859375
train loss:  0.33047667145729065
train gradient:  0.1735309884873532
iteration : 710
train acc:  0.875
train loss:  0.3105775713920593
train gradient:  0.13368054440600996
iteration : 711
train acc:  0.859375
train loss:  0.28732752799987793
train gradient:  0.13208720637632287
iteration : 712
train acc:  0.875
train loss:  0.2960585355758667
train gradient:  0.1604861900629217
iteration : 713
train acc:  0.8515625
train loss:  0.3229949474334717
train gradient:  0.10684334422091607
iteration : 714
train acc:  0.890625
train loss:  0.31145039200782776
train gradient:  0.11743876318704588
iteration : 715
train acc:  0.875
train loss:  0.3159483075141907
train gradient:  0.09957627845676041
iteration : 716
train acc:  0.859375
train loss:  0.2989671528339386
train gradient:  0.11242899417067179
iteration : 717
train acc:  0.8671875
train loss:  0.400594025850296
train gradient:  0.1548206893316066
iteration : 718
train acc:  0.8671875
train loss:  0.4242217540740967
train gradient:  0.33208965486457964
iteration : 719
train acc:  0.859375
train loss:  0.30185288190841675
train gradient:  0.20061425201907898
iteration : 720
train acc:  0.8125
train loss:  0.34835803508758545
train gradient:  0.11528915108122405
iteration : 721
train acc:  0.828125
train loss:  0.3783975839614868
train gradient:  0.16605133189360743
iteration : 722
train acc:  0.890625
train loss:  0.261502742767334
train gradient:  0.07173093748781081
iteration : 723
train acc:  0.9140625
train loss:  0.24003352224826813
train gradient:  0.07013261952804582
iteration : 724
train acc:  0.859375
train loss:  0.31637322902679443
train gradient:  0.11418434028350928
iteration : 725
train acc:  0.8515625
train loss:  0.3373912274837494
train gradient:  0.23196333931627952
iteration : 726
train acc:  0.8671875
train loss:  0.32207366824150085
train gradient:  0.18611679031265538
iteration : 727
train acc:  0.84375
train loss:  0.3479976952075958
train gradient:  0.14562333259143234
iteration : 728
train acc:  0.8828125
train loss:  0.28373983502388
train gradient:  0.09902168550980821
iteration : 729
train acc:  0.84375
train loss:  0.3301965594291687
train gradient:  0.14333774428087778
iteration : 730
train acc:  0.875
train loss:  0.29671069979667664
train gradient:  0.11120022950289628
iteration : 731
train acc:  0.8515625
train loss:  0.3063129782676697
train gradient:  0.1497002172421831
iteration : 732
train acc:  0.796875
train loss:  0.4180211126804352
train gradient:  0.17040439559798679
iteration : 733
train acc:  0.8515625
train loss:  0.3853517472743988
train gradient:  0.1706418608298072
iteration : 734
train acc:  0.9296875
train loss:  0.224111869931221
train gradient:  0.0893362027331268
iteration : 735
train acc:  0.7890625
train loss:  0.3812412917613983
train gradient:  0.23334235996493932
iteration : 736
train acc:  0.8828125
train loss:  0.3103454113006592
train gradient:  0.10720281443860345
iteration : 737
train acc:  0.8828125
train loss:  0.27887386083602905
train gradient:  0.11474346963392909
iteration : 738
train acc:  0.890625
train loss:  0.22059103846549988
train gradient:  0.11388893368900714
iteration : 739
train acc:  0.8671875
train loss:  0.2842126786708832
train gradient:  0.10015864594110449
iteration : 740
train acc:  0.84375
train loss:  0.39706259965896606
train gradient:  0.2210341796698887
iteration : 741
train acc:  0.890625
train loss:  0.26631367206573486
train gradient:  0.12842800850915004
iteration : 742
train acc:  0.8671875
train loss:  0.31022775173187256
train gradient:  0.11042492112256289
iteration : 743
train acc:  0.8359375
train loss:  0.33271679282188416
train gradient:  0.11611095357907036
iteration : 744
train acc:  0.890625
train loss:  0.2545938491821289
train gradient:  0.12338048433153762
iteration : 745
train acc:  0.90625
train loss:  0.24992120265960693
train gradient:  0.11521205496598387
iteration : 746
train acc:  0.859375
train loss:  0.28381118178367615
train gradient:  0.1073252205453873
iteration : 747
train acc:  0.8125
train loss:  0.36708468198776245
train gradient:  0.14848404495036654
iteration : 748
train acc:  0.90625
train loss:  0.2832498848438263
train gradient:  0.20639518166696075
iteration : 749
train acc:  0.8125
train loss:  0.3625063896179199
train gradient:  0.16881923923549563
iteration : 750
train acc:  0.8515625
train loss:  0.26717624068260193
train gradient:  0.16068834071144014
iteration : 751
train acc:  0.8828125
train loss:  0.26774585247039795
train gradient:  0.12972495706154613
iteration : 752
train acc:  0.875
train loss:  0.357871413230896
train gradient:  0.13993019146504632
iteration : 753
train acc:  0.921875
train loss:  0.24534276127815247
train gradient:  0.07814234807825415
iteration : 754
train acc:  0.828125
train loss:  0.3355937898159027
train gradient:  0.11411037224033184
iteration : 755
train acc:  0.8671875
train loss:  0.3455177843570709
train gradient:  0.12330145663657999
iteration : 756
train acc:  0.875
train loss:  0.27146226167678833
train gradient:  0.13842943327434012
iteration : 757
train acc:  0.859375
train loss:  0.33244091272354126
train gradient:  0.20036834300293416
iteration : 758
train acc:  0.875
train loss:  0.2859387993812561
train gradient:  0.1410075727235632
iteration : 759
train acc:  0.859375
train loss:  0.38097018003463745
train gradient:  0.16760035664018164
iteration : 760
train acc:  0.9140625
train loss:  0.24083366990089417
train gradient:  0.08432201380392243
iteration : 761
train acc:  0.84375
train loss:  0.31146448850631714
train gradient:  0.1418419114739658
iteration : 762
train acc:  0.8515625
train loss:  0.33959338068962097
train gradient:  0.19094140815731425
iteration : 763
train acc:  0.859375
train loss:  0.3447427749633789
train gradient:  0.1607794452257691
iteration : 764
train acc:  0.828125
train loss:  0.40132755041122437
train gradient:  0.19110431015495638
iteration : 765
train acc:  0.921875
train loss:  0.24216404557228088
train gradient:  0.10578568149828307
iteration : 766
train acc:  0.84375
train loss:  0.32568058371543884
train gradient:  0.13056153854508096
iteration : 767
train acc:  0.9140625
train loss:  0.2841159999370575
train gradient:  0.1110416683867103
iteration : 768
train acc:  0.890625
train loss:  0.2653976380825043
train gradient:  0.10585800784669913
iteration : 769
train acc:  0.84375
train loss:  0.33528268337249756
train gradient:  0.18407115528758838
iteration : 770
train acc:  0.8671875
train loss:  0.32495957612991333
train gradient:  0.1198551571551146
iteration : 771
train acc:  0.8828125
train loss:  0.24057123064994812
train gradient:  0.0836684806459005
iteration : 772
train acc:  0.890625
train loss:  0.23024597764015198
train gradient:  0.11137245600713394
iteration : 773
train acc:  0.828125
train loss:  0.3896685242652893
train gradient:  0.2018242391840607
iteration : 774
train acc:  0.828125
train loss:  0.3360123634338379
train gradient:  0.1698808442040891
iteration : 775
train acc:  0.875
train loss:  0.26334649324417114
train gradient:  0.09980204104485166
iteration : 776
train acc:  0.828125
train loss:  0.38332363963127136
train gradient:  0.15116269727902173
iteration : 777
train acc:  0.9140625
train loss:  0.25142478942871094
train gradient:  0.10722393127689438
iteration : 778
train acc:  0.8984375
train loss:  0.31722134351730347
train gradient:  0.19729305873293249
iteration : 779
train acc:  0.8046875
train loss:  0.4066815972328186
train gradient:  0.18225644223555437
iteration : 780
train acc:  0.828125
train loss:  0.3877300024032593
train gradient:  0.21297533900974935
iteration : 781
train acc:  0.890625
train loss:  0.28566205501556396
train gradient:  0.1069462369744141
iteration : 782
train acc:  0.8359375
train loss:  0.34906649589538574
train gradient:  0.12393952387075952
iteration : 783
train acc:  0.8203125
train loss:  0.34251174330711365
train gradient:  0.14420146561461902
iteration : 784
train acc:  0.8515625
train loss:  0.30759912729263306
train gradient:  0.12862458608222027
iteration : 785
train acc:  0.875
train loss:  0.29244354367256165
train gradient:  0.1049203648314482
iteration : 786
train acc:  0.8828125
train loss:  0.26177263259887695
train gradient:  0.06666329137562854
iteration : 787
train acc:  0.8359375
train loss:  0.31318676471710205
train gradient:  0.09606930100685572
iteration : 788
train acc:  0.8359375
train loss:  0.37127116322517395
train gradient:  0.1457439070891959
iteration : 789
train acc:  0.7734375
train loss:  0.40145665407180786
train gradient:  0.165089608672938
iteration : 790
train acc:  0.859375
train loss:  0.28187018632888794
train gradient:  0.14426061910025412
iteration : 791
train acc:  0.8515625
train loss:  0.3188060522079468
train gradient:  0.14159591935755853
iteration : 792
train acc:  0.890625
train loss:  0.24994181096553802
train gradient:  0.08066725954488776
iteration : 793
train acc:  0.859375
train loss:  0.3761831521987915
train gradient:  0.1478248782176701
iteration : 794
train acc:  0.8671875
train loss:  0.27177879214286804
train gradient:  0.08561739432589019
iteration : 795
train acc:  0.90625
train loss:  0.2267467975616455
train gradient:  0.0696830995999007
iteration : 796
train acc:  0.8125
train loss:  0.44472557306289673
train gradient:  0.19259855493183425
iteration : 797
train acc:  0.8046875
train loss:  0.45942485332489014
train gradient:  0.25067534128425734
iteration : 798
train acc:  0.875
train loss:  0.32035768032073975
train gradient:  0.11370350071087536
iteration : 799
train acc:  0.875
train loss:  0.27727723121643066
train gradient:  0.09887380894257022
iteration : 800
train acc:  0.84375
train loss:  0.3079105019569397
train gradient:  0.09043708263091858
iteration : 801
train acc:  0.8671875
train loss:  0.3348424434661865
train gradient:  0.1397005096204667
iteration : 802
train acc:  0.8203125
train loss:  0.30564701557159424
train gradient:  0.15031852275693908
iteration : 803
train acc:  0.8671875
train loss:  0.32647526264190674
train gradient:  0.1051036890393232
iteration : 804
train acc:  0.8828125
train loss:  0.28888362646102905
train gradient:  0.1295892626219803
iteration : 805
train acc:  0.875
train loss:  0.2692827582359314
train gradient:  0.09725443268534714
iteration : 806
train acc:  0.8046875
train loss:  0.42350849509239197
train gradient:  0.19567188916349953
iteration : 807
train acc:  0.7734375
train loss:  0.40115076303482056
train gradient:  0.2775239272081368
iteration : 808
train acc:  0.8671875
train loss:  0.2827740013599396
train gradient:  0.08878575510255718
iteration : 809
train acc:  0.84375
train loss:  0.3479127287864685
train gradient:  0.1495666986364745
iteration : 810
train acc:  0.8828125
train loss:  0.2638614773750305
train gradient:  0.08869054410035358
iteration : 811
train acc:  0.890625
train loss:  0.3160800337791443
train gradient:  0.13714828461703693
iteration : 812
train acc:  0.875
train loss:  0.2724500596523285
train gradient:  0.10077329170521827
iteration : 813
train acc:  0.8828125
train loss:  0.27528250217437744
train gradient:  0.13210904722664407
iteration : 814
train acc:  0.859375
train loss:  0.34313100576400757
train gradient:  0.10905625774199146
iteration : 815
train acc:  0.8203125
train loss:  0.34317803382873535
train gradient:  0.12412038389505761
iteration : 816
train acc:  0.8046875
train loss:  0.3476162552833557
train gradient:  0.2658075533362186
iteration : 817
train acc:  0.8359375
train loss:  0.3531174659729004
train gradient:  0.18881334127306532
iteration : 818
train acc:  0.828125
train loss:  0.4032476544380188
train gradient:  0.19489666890449736
iteration : 819
train acc:  0.8671875
train loss:  0.3246975839138031
train gradient:  0.1637480694864697
iteration : 820
train acc:  0.8671875
train loss:  0.2753281593322754
train gradient:  0.09824010428586971
iteration : 821
train acc:  0.9140625
train loss:  0.2646562457084656
train gradient:  0.12111425292178223
iteration : 822
train acc:  0.8515625
train loss:  0.31383568048477173
train gradient:  0.10648410513095492
iteration : 823
train acc:  0.8515625
train loss:  0.3077608346939087
train gradient:  0.12488832825798221
iteration : 824
train acc:  0.8359375
train loss:  0.43398135900497437
train gradient:  0.23802742212155034
iteration : 825
train acc:  0.8828125
train loss:  0.2798611521720886
train gradient:  0.11096913562355583
iteration : 826
train acc:  0.9140625
train loss:  0.2323569655418396
train gradient:  0.2105915705492939
iteration : 827
train acc:  0.84375
train loss:  0.41075339913368225
train gradient:  0.33986037902288985
iteration : 828
train acc:  0.8515625
train loss:  0.3118579685688019
train gradient:  0.10361797312691653
iteration : 829
train acc:  0.875
train loss:  0.3105342984199524
train gradient:  0.1464244968824508
iteration : 830
train acc:  0.8359375
train loss:  0.33117830753326416
train gradient:  0.14166602607012355
iteration : 831
train acc:  0.875
train loss:  0.3291889429092407
train gradient:  0.1150266688021469
iteration : 832
train acc:  0.9140625
train loss:  0.2417844831943512
train gradient:  0.09124350937431973
iteration : 833
train acc:  0.8671875
train loss:  0.29382556676864624
train gradient:  0.1219591307333334
iteration : 834
train acc:  0.8828125
train loss:  0.29702192544937134
train gradient:  0.09978736985686214
iteration : 835
train acc:  0.84375
train loss:  0.3548850417137146
train gradient:  0.18648187333541186
iteration : 836
train acc:  0.8515625
train loss:  0.341775119304657
train gradient:  0.17850349158579948
iteration : 837
train acc:  0.90625
train loss:  0.26018399000167847
train gradient:  0.10487709722328213
iteration : 838
train acc:  0.78125
train loss:  0.43400272727012634
train gradient:  0.23912480626849592
iteration : 839
train acc:  0.84375
train loss:  0.3095715343952179
train gradient:  0.13844068862292191
iteration : 840
train acc:  0.8828125
train loss:  0.29085928201675415
train gradient:  0.09217467676713573
iteration : 841
train acc:  0.796875
train loss:  0.37559396028518677
train gradient:  0.2933687915848141
iteration : 842
train acc:  0.921875
train loss:  0.2309190034866333
train gradient:  0.07370101497323607
iteration : 843
train acc:  0.890625
train loss:  0.2590481638908386
train gradient:  0.0797657068770503
iteration : 844
train acc:  0.875
train loss:  0.27097922563552856
train gradient:  0.10643878570304893
iteration : 845
train acc:  0.84375
train loss:  0.33162960410118103
train gradient:  0.1988200290333486
iteration : 846
train acc:  0.90625
train loss:  0.24331755936145782
train gradient:  0.12285171202046184
iteration : 847
train acc:  0.8828125
train loss:  0.2697751224040985
train gradient:  0.1609697062997084
iteration : 848
train acc:  0.859375
train loss:  0.3609359860420227
train gradient:  0.17153872995359132
iteration : 849
train acc:  0.8671875
train loss:  0.32451438903808594
train gradient:  0.1313336875316921
iteration : 850
train acc:  0.8828125
train loss:  0.286815345287323
train gradient:  0.09300120134742154
iteration : 851
train acc:  0.8984375
train loss:  0.24848636984825134
train gradient:  0.06864128009091074
iteration : 852
train acc:  0.828125
train loss:  0.37767165899276733
train gradient:  0.1912741746815611
iteration : 853
train acc:  0.90625
train loss:  0.2425410896539688
train gradient:  0.09542954648913865
iteration : 854
train acc:  0.890625
train loss:  0.2686039209365845
train gradient:  0.10942963053027323
iteration : 855
train acc:  0.875
train loss:  0.3601367473602295
train gradient:  0.2000657871518214
iteration : 856
train acc:  0.8828125
train loss:  0.2498171180486679
train gradient:  0.09417921068734637
iteration : 857
train acc:  0.875
train loss:  0.3344799280166626
train gradient:  0.1614883513547521
iteration : 858
train acc:  0.859375
train loss:  0.3478044867515564
train gradient:  0.11280708599306236
iteration : 859
train acc:  0.8515625
train loss:  0.2868788242340088
train gradient:  0.10960090452499553
iteration : 860
train acc:  0.875
train loss:  0.26805421710014343
train gradient:  0.09970122107703032
iteration : 861
train acc:  0.890625
train loss:  0.23344002664089203
train gradient:  0.09540173243969331
iteration : 862
train acc:  0.8515625
train loss:  0.32676705718040466
train gradient:  0.1720934278723864
iteration : 863
train acc:  0.8828125
train loss:  0.26224973797798157
train gradient:  0.16986075254685357
iteration : 864
train acc:  0.8515625
train loss:  0.2549981474876404
train gradient:  0.08872056181970898
iteration : 865
train acc:  0.8046875
train loss:  0.31698405742645264
train gradient:  0.09868738849837227
iteration : 866
train acc:  0.8671875
train loss:  0.29903340339660645
train gradient:  0.2051565181112318
iteration : 867
train acc:  0.84375
train loss:  0.258680135011673
train gradient:  0.09572306266717676
iteration : 868
train acc:  0.8828125
train loss:  0.22939975559711456
train gradient:  0.11132990676630647
iteration : 869
train acc:  0.8359375
train loss:  0.3742983043193817
train gradient:  0.19012908166267303
iteration : 870
train acc:  0.859375
train loss:  0.3317680358886719
train gradient:  0.18717617990304566
iteration : 871
train acc:  0.8359375
train loss:  0.3163495361804962
train gradient:  0.1436646101698495
iteration : 872
train acc:  0.8515625
train loss:  0.40671592950820923
train gradient:  0.18593693973753814
iteration : 873
train acc:  0.828125
train loss:  0.34650322794914246
train gradient:  0.17341957765581995
iteration : 874
train acc:  0.8671875
train loss:  0.3738524913787842
train gradient:  0.19299023563461287
iteration : 875
train acc:  0.8203125
train loss:  0.329242467880249
train gradient:  0.1972605494158387
iteration : 876
train acc:  0.8046875
train loss:  0.38719817996025085
train gradient:  0.2094416074010142
iteration : 877
train acc:  0.8359375
train loss:  0.31461358070373535
train gradient:  0.15486225265557998
iteration : 878
train acc:  0.84375
train loss:  0.3210517168045044
train gradient:  0.13720735342269158
iteration : 879
train acc:  0.90625
train loss:  0.2367883026599884
train gradient:  0.12705943291823513
iteration : 880
train acc:  0.8125
train loss:  0.4163779616355896
train gradient:  0.26619244565070255
iteration : 881
train acc:  0.8828125
train loss:  0.306183397769928
train gradient:  0.17837306299901814
iteration : 882
train acc:  0.890625
train loss:  0.2822968363761902
train gradient:  0.11457278887016913
iteration : 883
train acc:  0.890625
train loss:  0.26263701915740967
train gradient:  0.13559453818993344
iteration : 884
train acc:  0.8515625
train loss:  0.33894872665405273
train gradient:  0.15730891480919929
iteration : 885
train acc:  0.8984375
train loss:  0.2564478814601898
train gradient:  0.10826447936546466
iteration : 886
train acc:  0.8671875
train loss:  0.2928275167942047
train gradient:  0.1190444241746703
iteration : 887
train acc:  0.90625
train loss:  0.26618900895118713
train gradient:  0.11678668990752591
iteration : 888
train acc:  0.84375
train loss:  0.34467241168022156
train gradient:  0.15726930493710028
iteration : 889
train acc:  0.8828125
train loss:  0.2611203193664551
train gradient:  0.13128542654608502
iteration : 890
train acc:  0.8515625
train loss:  0.3653212785720825
train gradient:  0.16966331391675837
iteration : 891
train acc:  0.859375
train loss:  0.3380346894264221
train gradient:  0.11831808364439657
iteration : 892
train acc:  0.8984375
train loss:  0.25341111421585083
train gradient:  0.11648902669230782
iteration : 893
train acc:  0.890625
train loss:  0.24198836088180542
train gradient:  0.08383924559844301
iteration : 894
train acc:  0.9140625
train loss:  0.2362566739320755
train gradient:  0.08376636200363093
iteration : 895
train acc:  0.890625
train loss:  0.2693527340888977
train gradient:  0.1422606702650337
iteration : 896
train acc:  0.9140625
train loss:  0.2477901577949524
train gradient:  0.07759721038572248
iteration : 897
train acc:  0.859375
train loss:  0.3046877384185791
train gradient:  0.130079614821986
iteration : 898
train acc:  0.859375
train loss:  0.3773611783981323
train gradient:  0.17835437470795978
iteration : 899
train acc:  0.8671875
train loss:  0.31316429376602173
train gradient:  0.1824207374912662
iteration : 900
train acc:  0.8359375
train loss:  0.34222716093063354
train gradient:  0.1819956395805994
iteration : 901
train acc:  0.84375
train loss:  0.36820924282073975
train gradient:  0.17263018409188224
iteration : 902
train acc:  0.9140625
train loss:  0.31553301215171814
train gradient:  0.14817667495586387
iteration : 903
train acc:  0.8515625
train loss:  0.3115570545196533
train gradient:  0.1400049039338691
iteration : 904
train acc:  0.8671875
train loss:  0.3127628564834595
train gradient:  0.1348573721432378
iteration : 905
train acc:  0.859375
train loss:  0.2736455202102661
train gradient:  0.15017581195190827
iteration : 906
train acc:  0.8515625
train loss:  0.392117440700531
train gradient:  0.17907292857751386
iteration : 907
train acc:  0.921875
train loss:  0.25057125091552734
train gradient:  0.09388382561501293
iteration : 908
train acc:  0.875
train loss:  0.3327987790107727
train gradient:  0.13592157233629107
iteration : 909
train acc:  0.8984375
train loss:  0.30172795057296753
train gradient:  0.127817588992424
iteration : 910
train acc:  0.8046875
train loss:  0.3547629117965698
train gradient:  0.1637899131846347
iteration : 911
train acc:  0.859375
train loss:  0.3065372407436371
train gradient:  0.1377499648573982
iteration : 912
train acc:  0.890625
train loss:  0.27227795124053955
train gradient:  0.1323909469013047
iteration : 913
train acc:  0.875
train loss:  0.4220018982887268
train gradient:  0.277551623908514
iteration : 914
train acc:  0.859375
train loss:  0.33530038595199585
train gradient:  0.14973228829804208
iteration : 915
train acc:  0.8984375
train loss:  0.2748059928417206
train gradient:  0.09728650685231781
iteration : 916
train acc:  0.8359375
train loss:  0.36364758014678955
train gradient:  0.1683285148632559
iteration : 917
train acc:  0.890625
train loss:  0.32785266637802124
train gradient:  0.11458853137201562
iteration : 918
train acc:  0.8359375
train loss:  0.33453670144081116
train gradient:  0.21086217153356696
iteration : 919
train acc:  0.9453125
train loss:  0.2391786128282547
train gradient:  0.09789404247800815
iteration : 920
train acc:  0.890625
train loss:  0.2821042835712433
train gradient:  0.14206335659508731
iteration : 921
train acc:  0.8515625
train loss:  0.29841387271881104
train gradient:  0.10450689390137134
iteration : 922
train acc:  0.859375
train loss:  0.34876391291618347
train gradient:  0.18836326723076008
iteration : 923
train acc:  0.8828125
train loss:  0.3015812933444977
train gradient:  0.16696738331430838
iteration : 924
train acc:  0.8671875
train loss:  0.2761775851249695
train gradient:  0.11468225871638806
iteration : 925
train acc:  0.859375
train loss:  0.26435378193855286
train gradient:  0.12805806996413527
iteration : 926
train acc:  0.9296875
train loss:  0.24194850027561188
train gradient:  0.09251831684698274
iteration : 927
train acc:  0.9140625
train loss:  0.251018762588501
train gradient:  0.12703367725634587
iteration : 928
train acc:  0.875
train loss:  0.33650022745132446
train gradient:  0.1724412450397419
iteration : 929
train acc:  0.8515625
train loss:  0.3139728009700775
train gradient:  0.12657060792162939
iteration : 930
train acc:  0.8671875
train loss:  0.3541572093963623
train gradient:  0.1831119913215245
iteration : 931
train acc:  0.890625
train loss:  0.2579161524772644
train gradient:  0.11597003903456125
iteration : 932
train acc:  0.859375
train loss:  0.356684148311615
train gradient:  0.1719241696354462
iteration : 933
train acc:  0.921875
train loss:  0.21043051779270172
train gradient:  0.1313967537348104
iteration : 934
train acc:  0.8359375
train loss:  0.3292156755924225
train gradient:  0.1377437940146148
iteration : 935
train acc:  0.84375
train loss:  0.3415733575820923
train gradient:  0.17404607809840603
iteration : 936
train acc:  0.8125
train loss:  0.32845038175582886
train gradient:  0.18015147817232288
iteration : 937
train acc:  0.8515625
train loss:  0.33918896317481995
train gradient:  0.1748042718759596
iteration : 938
train acc:  0.8671875
train loss:  0.31326210498809814
train gradient:  0.1052065422456008
iteration : 939
train acc:  0.7890625
train loss:  0.4242679476737976
train gradient:  0.2883904660796281
iteration : 940
train acc:  0.8984375
train loss:  0.2466086894273758
train gradient:  0.11921145380759109
iteration : 941
train acc:  0.8515625
train loss:  0.3885814845561981
train gradient:  0.17450891205838745
iteration : 942
train acc:  0.828125
train loss:  0.3373796343803406
train gradient:  0.1615550886574431
iteration : 943
train acc:  0.8515625
train loss:  0.3193339705467224
train gradient:  0.18132863359681664
iteration : 944
train acc:  0.8828125
train loss:  0.34038546681404114
train gradient:  0.1169958824839281
iteration : 945
train acc:  0.890625
train loss:  0.2426600605249405
train gradient:  0.10003690710072263
iteration : 946
train acc:  0.8359375
train loss:  0.30963242053985596
train gradient:  0.12057019475114193
iteration : 947
train acc:  0.8359375
train loss:  0.38476240634918213
train gradient:  0.20006590657557305
iteration : 948
train acc:  0.84375
train loss:  0.3145103454589844
train gradient:  0.1951301771115767
iteration : 949
train acc:  0.84375
train loss:  0.35010582208633423
train gradient:  0.14431007296094944
iteration : 950
train acc:  0.8203125
train loss:  0.31231141090393066
train gradient:  0.1513753076601259
iteration : 951
train acc:  0.84375
train loss:  0.30473944544792175
train gradient:  0.1428251664904081
iteration : 952
train acc:  0.8828125
train loss:  0.3310968279838562
train gradient:  0.1974124543006427
iteration : 953
train acc:  0.8671875
train loss:  0.30321362614631653
train gradient:  0.19396357179086912
iteration : 954
train acc:  0.8515625
train loss:  0.31016314029693604
train gradient:  0.15726942409495528
iteration : 955
train acc:  0.8359375
train loss:  0.30554714798927307
train gradient:  0.12421990909192648
iteration : 956
train acc:  0.859375
train loss:  0.30064260959625244
train gradient:  0.10383539887857136
iteration : 957
train acc:  0.8984375
train loss:  0.2927071154117584
train gradient:  0.127759442600116
iteration : 958
train acc:  0.9140625
train loss:  0.2330358624458313
train gradient:  0.10767107228464864
iteration : 959
train acc:  0.859375
train loss:  0.2942562699317932
train gradient:  0.1565281704221153
iteration : 960
train acc:  0.8515625
train loss:  0.3019556403160095
train gradient:  0.16618245346104993
iteration : 961
train acc:  0.828125
train loss:  0.359208881855011
train gradient:  0.19725796613457816
iteration : 962
train acc:  0.84375
train loss:  0.34219616651535034
train gradient:  0.1872761937165618
iteration : 963
train acc:  0.8671875
train loss:  0.32759031653404236
train gradient:  0.14471385489546207
iteration : 964
train acc:  0.828125
train loss:  0.34699416160583496
train gradient:  0.1614260114624834
iteration : 965
train acc:  0.890625
train loss:  0.2473982870578766
train gradient:  0.11109284602837563
iteration : 966
train acc:  0.8359375
train loss:  0.3351927399635315
train gradient:  0.20092704489071792
iteration : 967
train acc:  0.8828125
train loss:  0.3068450689315796
train gradient:  0.11762979139819217
iteration : 968
train acc:  0.828125
train loss:  0.29669076204299927
train gradient:  0.09089202913868616
iteration : 969
train acc:  0.875
train loss:  0.30501994490623474
train gradient:  0.11378376131693589
iteration : 970
train acc:  0.859375
train loss:  0.29054680466651917
train gradient:  0.11771317595542095
iteration : 971
train acc:  0.8125
train loss:  0.3775674104690552
train gradient:  0.21863850472538457
iteration : 972
train acc:  0.8984375
train loss:  0.2598731219768524
train gradient:  0.13314270443909332
iteration : 973
train acc:  0.8515625
train loss:  0.38590487837791443
train gradient:  0.20220728381887554
iteration : 974
train acc:  0.8984375
train loss:  0.3072584271430969
train gradient:  0.12949529730366538
iteration : 975
train acc:  0.8671875
train loss:  0.3484756350517273
train gradient:  0.16920854347876285
iteration : 976
train acc:  0.875
train loss:  0.27505552768707275
train gradient:  0.15024901764916015
iteration : 977
train acc:  0.8828125
train loss:  0.24133707582950592
train gradient:  0.08499646701531144
iteration : 978
train acc:  0.84375
train loss:  0.36743801832199097
train gradient:  0.15972796313021148
iteration : 979
train acc:  0.875
train loss:  0.27579593658447266
train gradient:  0.12290846520130551
iteration : 980
train acc:  0.84375
train loss:  0.36786025762557983
train gradient:  0.15628985672930434
iteration : 981
train acc:  0.8359375
train loss:  0.3108704388141632
train gradient:  0.20758345559064928
iteration : 982
train acc:  0.8671875
train loss:  0.3213733434677124
train gradient:  0.09632567271175746
iteration : 983
train acc:  0.890625
train loss:  0.2987968921661377
train gradient:  0.12568120998763888
iteration : 984
train acc:  0.8515625
train loss:  0.3009765148162842
train gradient:  0.12099631253267822
iteration : 985
train acc:  0.84375
train loss:  0.37234681844711304
train gradient:  0.14696699637190608
iteration : 986
train acc:  0.8984375
train loss:  0.2700100839138031
train gradient:  0.1010247091619751
iteration : 987
train acc:  0.859375
train loss:  0.31579622626304626
train gradient:  0.11126586349958568
iteration : 988
train acc:  0.828125
train loss:  0.35587599873542786
train gradient:  0.15748114359581117
iteration : 989
train acc:  0.8671875
train loss:  0.34754079580307007
train gradient:  0.14582872579276254
iteration : 990
train acc:  0.8671875
train loss:  0.35817664861679077
train gradient:  0.20287435119251884
iteration : 991
train acc:  0.8828125
train loss:  0.26648515462875366
train gradient:  0.12249767702830773
iteration : 992
train acc:  0.8828125
train loss:  0.2875111699104309
train gradient:  0.11340942561834812
iteration : 993
train acc:  0.8359375
train loss:  0.38387420773506165
train gradient:  0.2039704218249692
iteration : 994
train acc:  0.890625
train loss:  0.2991023063659668
train gradient:  0.08774530568514097
iteration : 995
train acc:  0.859375
train loss:  0.33182525634765625
train gradient:  0.20347007007859413
iteration : 996
train acc:  0.8359375
train loss:  0.36487582325935364
train gradient:  0.20926996507912923
iteration : 997
train acc:  0.8671875
train loss:  0.32627126574516296
train gradient:  0.10622866967439278
iteration : 998
train acc:  0.8203125
train loss:  0.37864577770233154
train gradient:  0.16123513452894017
iteration : 999
train acc:  0.8203125
train loss:  0.38730698823928833
train gradient:  0.186733736760048
iteration : 1000
train acc:  0.8515625
train loss:  0.33354097604751587
train gradient:  0.12495151165962473
iteration : 1001
train acc:  0.8515625
train loss:  0.3212745785713196
train gradient:  0.17181943620191803
iteration : 1002
train acc:  0.828125
train loss:  0.36774349212646484
train gradient:  0.2392638350133123
iteration : 1003
train acc:  0.8515625
train loss:  0.3196040689945221
train gradient:  0.1150928990747323
iteration : 1004
train acc:  0.8984375
train loss:  0.3365750014781952
train gradient:  0.13121144388394265
iteration : 1005
train acc:  0.875
train loss:  0.32029637694358826
train gradient:  0.19417456889283008
iteration : 1006
train acc:  0.8984375
train loss:  0.3140609860420227
train gradient:  0.1146257206024052
iteration : 1007
train acc:  0.890625
train loss:  0.34333139657974243
train gradient:  0.138809417485678
iteration : 1008
train acc:  0.875
train loss:  0.32817044854164124
train gradient:  0.12562547542849078
iteration : 1009
train acc:  0.890625
train loss:  0.26062309741973877
train gradient:  0.09143931523796428
iteration : 1010
train acc:  0.828125
train loss:  0.32190290093421936
train gradient:  0.09734321605368701
iteration : 1011
train acc:  0.796875
train loss:  0.43235623836517334
train gradient:  0.18660134231455797
iteration : 1012
train acc:  0.8671875
train loss:  0.3124900460243225
train gradient:  0.10999872723887065
iteration : 1013
train acc:  0.8359375
train loss:  0.31122809648513794
train gradient:  0.127258802662296
iteration : 1014
train acc:  0.90625
train loss:  0.27565717697143555
train gradient:  0.11758051206466036
iteration : 1015
train acc:  0.859375
train loss:  0.31475579738616943
train gradient:  0.23240749189441626
iteration : 1016
train acc:  0.8515625
train loss:  0.3869435787200928
train gradient:  0.18278160285809503
iteration : 1017
train acc:  0.8515625
train loss:  0.3659226894378662
train gradient:  0.17107805971940193
iteration : 1018
train acc:  0.84375
train loss:  0.3264590799808502
train gradient:  0.12993027515901506
iteration : 1019
train acc:  0.9296875
train loss:  0.2338201403617859
train gradient:  0.1002758414915451
iteration : 1020
train acc:  0.921875
train loss:  0.26453912258148193
train gradient:  0.0710125191779513
iteration : 1021
train acc:  0.84375
train loss:  0.39582422375679016
train gradient:  0.1306947877430269
iteration : 1022
train acc:  0.890625
train loss:  0.27949103713035583
train gradient:  0.08874049926300395
iteration : 1023
train acc:  0.90625
train loss:  0.252789169549942
train gradient:  0.08915574635722662
iteration : 1024
train acc:  0.859375
train loss:  0.3348829746246338
train gradient:  0.1428644415805681
iteration : 1025
train acc:  0.8359375
train loss:  0.3708951771259308
train gradient:  0.1174170890546283
iteration : 1026
train acc:  0.890625
train loss:  0.31613820791244507
train gradient:  0.10321254872966439
iteration : 1027
train acc:  0.8828125
train loss:  0.2893119156360626
train gradient:  0.10412588932194267
iteration : 1028
train acc:  0.875
train loss:  0.30357813835144043
train gradient:  0.1527660854798279
iteration : 1029
train acc:  0.828125
train loss:  0.3638952374458313
train gradient:  0.14064793136091594
iteration : 1030
train acc:  0.859375
train loss:  0.32969358563423157
train gradient:  0.12890613332572232
iteration : 1031
train acc:  0.8515625
train loss:  0.34343641996383667
train gradient:  0.14420955027454627
iteration : 1032
train acc:  0.7578125
train loss:  0.4141492247581482
train gradient:  0.18093144278142018
iteration : 1033
train acc:  0.8359375
train loss:  0.33672410249710083
train gradient:  0.11644037768479115
iteration : 1034
train acc:  0.875
train loss:  0.2741734981536865
train gradient:  0.06691306163643428
iteration : 1035
train acc:  0.8984375
train loss:  0.2525253891944885
train gradient:  0.08450989912541843
iteration : 1036
train acc:  0.8359375
train loss:  0.3656677007675171
train gradient:  0.12604942236852826
iteration : 1037
train acc:  0.8671875
train loss:  0.31910470128059387
train gradient:  0.10797267178802969
iteration : 1038
train acc:  0.8515625
train loss:  0.32031503319740295
train gradient:  0.1361923510115879
iteration : 1039
train acc:  0.8984375
train loss:  0.319084495306015
train gradient:  0.1403938533672911
iteration : 1040
train acc:  0.90625
train loss:  0.2205018401145935
train gradient:  0.07427054391162038
iteration : 1041
train acc:  0.8984375
train loss:  0.2599095404148102
train gradient:  0.06271605623231433
iteration : 1042
train acc:  0.8203125
train loss:  0.4016953110694885
train gradient:  0.16856149155818484
iteration : 1043
train acc:  0.875
train loss:  0.29488301277160645
train gradient:  0.09753319897379681
iteration : 1044
train acc:  0.875
train loss:  0.2846090793609619
train gradient:  0.10360925190058712
iteration : 1045
train acc:  0.828125
train loss:  0.41474393010139465
train gradient:  0.1972358039642266
iteration : 1046
train acc:  0.8671875
train loss:  0.3447350263595581
train gradient:  0.10041663676237735
iteration : 1047
train acc:  0.8671875
train loss:  0.31352055072784424
train gradient:  0.09513504327179947
iteration : 1048
train acc:  0.875
train loss:  0.2985799312591553
train gradient:  0.18029998337455483
iteration : 1049
train acc:  0.8359375
train loss:  0.36143958568573
train gradient:  0.23917119829558448
iteration : 1050
train acc:  0.9140625
train loss:  0.24166421592235565
train gradient:  0.06119964998208704
iteration : 1051
train acc:  0.890625
train loss:  0.27919626235961914
train gradient:  0.10486202014479552
iteration : 1052
train acc:  0.8125
train loss:  0.4152218699455261
train gradient:  0.19294028493092108
iteration : 1053
train acc:  0.953125
train loss:  0.21537259221076965
train gradient:  0.09369870767025275
iteration : 1054
train acc:  0.8671875
train loss:  0.30439895391464233
train gradient:  0.12626125923938708
iteration : 1055
train acc:  0.8671875
train loss:  0.304496705532074
train gradient:  0.15260285912593557
iteration : 1056
train acc:  0.8125
train loss:  0.36424851417541504
train gradient:  0.15845519854146928
iteration : 1057
train acc:  0.921875
train loss:  0.2194315642118454
train gradient:  0.04957346166948956
iteration : 1058
train acc:  0.90625
train loss:  0.2515188455581665
train gradient:  0.06596906229763067
iteration : 1059
train acc:  0.828125
train loss:  0.4559270143508911
train gradient:  0.4047098874078806
iteration : 1060
train acc:  0.8515625
train loss:  0.2945125997066498
train gradient:  0.12542232740310447
iteration : 1061
train acc:  0.859375
train loss:  0.34089043736457825
train gradient:  0.13527607914550666
iteration : 1062
train acc:  0.859375
train loss:  0.3762880563735962
train gradient:  0.15090550848303072
iteration : 1063
train acc:  0.8203125
train loss:  0.33863985538482666
train gradient:  0.10754230526551449
iteration : 1064
train acc:  0.875
train loss:  0.32908761501312256
train gradient:  0.12960317302171867
iteration : 1065
train acc:  0.890625
train loss:  0.30554884672164917
train gradient:  0.24648914665952762
iteration : 1066
train acc:  0.84375
train loss:  0.35871613025665283
train gradient:  0.20170824283056343
iteration : 1067
train acc:  0.8984375
train loss:  0.32770276069641113
train gradient:  0.0961123850373858
iteration : 1068
train acc:  0.890625
train loss:  0.31232091784477234
train gradient:  0.12943434963152697
iteration : 1069
train acc:  0.84375
train loss:  0.36711564660072327
train gradient:  0.15732837199824296
iteration : 1070
train acc:  0.875
train loss:  0.2540799677371979
train gradient:  0.09652720733415433
iteration : 1071
train acc:  0.8828125
train loss:  0.250817209482193
train gradient:  0.09930624921591827
iteration : 1072
train acc:  0.8671875
train loss:  0.2972604036331177
train gradient:  0.10410847895675654
iteration : 1073
train acc:  0.8359375
train loss:  0.2635205388069153
train gradient:  0.09629844267723583
iteration : 1074
train acc:  0.875
train loss:  0.24510011076927185
train gradient:  0.08271903037870963
iteration : 1075
train acc:  0.8671875
train loss:  0.35086697340011597
train gradient:  0.1719188058914489
iteration : 1076
train acc:  0.8984375
train loss:  0.2604005038738251
train gradient:  0.12593258453748343
iteration : 1077
train acc:  0.8671875
train loss:  0.3172871768474579
train gradient:  0.1053163734242906
iteration : 1078
train acc:  0.90625
train loss:  0.2515432834625244
train gradient:  0.08165097244089987
iteration : 1079
train acc:  0.84375
train loss:  0.340213805437088
train gradient:  0.10132703075835352
iteration : 1080
train acc:  0.8671875
train loss:  0.3659551739692688
train gradient:  0.0978797229556064
iteration : 1081
train acc:  0.8828125
train loss:  0.25253427028656006
train gradient:  0.08376082385316755
iteration : 1082
train acc:  0.8984375
train loss:  0.236490860581398
train gradient:  0.0996520629264804
iteration : 1083
train acc:  0.9140625
train loss:  0.28333479166030884
train gradient:  0.14373569259916968
iteration : 1084
train acc:  0.875
train loss:  0.3524630665779114
train gradient:  0.14377238745288992
iteration : 1085
train acc:  0.890625
train loss:  0.23882180452346802
train gradient:  0.06259528076275941
iteration : 1086
train acc:  0.859375
train loss:  0.28103533387184143
train gradient:  0.09131588954748361
iteration : 1087
train acc:  0.8515625
train loss:  0.3424205780029297
train gradient:  0.12015408938592075
iteration : 1088
train acc:  0.890625
train loss:  0.26437699794769287
train gradient:  0.08197577950253465
iteration : 1089
train acc:  0.8984375
train loss:  0.2786477208137512
train gradient:  0.13130794124379563
iteration : 1090
train acc:  0.90625
train loss:  0.23581065237522125
train gradient:  0.08460401651598967
iteration : 1091
train acc:  0.8203125
train loss:  0.3327610492706299
train gradient:  0.15949581136995192
iteration : 1092
train acc:  0.8515625
train loss:  0.28507691621780396
train gradient:  0.10531592478743565
iteration : 1093
train acc:  0.9140625
train loss:  0.20752757787704468
train gradient:  0.07328784333760356
iteration : 1094
train acc:  0.921875
train loss:  0.2332065999507904
train gradient:  0.10292633847822473
iteration : 1095
train acc:  0.8671875
train loss:  0.278639018535614
train gradient:  0.10436759383686252
iteration : 1096
train acc:  0.859375
train loss:  0.30257701873779297
train gradient:  0.11487579487991326
iteration : 1097
train acc:  0.8828125
train loss:  0.2822367548942566
train gradient:  0.10522971391434242
iteration : 1098
train acc:  0.8515625
train loss:  0.30606740713119507
train gradient:  0.1115339578602879
iteration : 1099
train acc:  0.8671875
train loss:  0.25622624158859253
train gradient:  0.0914590878118826
iteration : 1100
train acc:  0.8515625
train loss:  0.35119688510894775
train gradient:  0.14009464512549002
iteration : 1101
train acc:  0.8671875
train loss:  0.29876893758773804
train gradient:  0.11213388202381305
iteration : 1102
train acc:  0.921875
train loss:  0.227193683385849
train gradient:  0.10443361177066786
iteration : 1103
train acc:  0.9140625
train loss:  0.3043404221534729
train gradient:  0.13001330879032202
iteration : 1104
train acc:  0.8203125
train loss:  0.3971904516220093
train gradient:  0.3731303067198421
iteration : 1105
train acc:  0.8359375
train loss:  0.29434046149253845
train gradient:  0.10614262120988742
iteration : 1106
train acc:  0.90625
train loss:  0.24547936022281647
train gradient:  0.07744545798293193
iteration : 1107
train acc:  0.9140625
train loss:  0.24657398462295532
train gradient:  0.14695692877761163
iteration : 1108
train acc:  0.859375
train loss:  0.35124415159225464
train gradient:  0.14316571560016383
iteration : 1109
train acc:  0.8203125
train loss:  0.35780084133148193
train gradient:  0.17015502464292312
iteration : 1110
train acc:  0.828125
train loss:  0.3427168130874634
train gradient:  0.21609240447592676
iteration : 1111
train acc:  0.8359375
train loss:  0.3574448525905609
train gradient:  0.1331385153307439
iteration : 1112
train acc:  0.8671875
train loss:  0.28161895275115967
train gradient:  0.16127146765374975
iteration : 1113
train acc:  0.90625
train loss:  0.25462105870246887
train gradient:  0.10145364001047569
iteration : 1114
train acc:  0.875
train loss:  0.3542960286140442
train gradient:  0.12473943625831231
iteration : 1115
train acc:  0.8984375
train loss:  0.29510390758514404
train gradient:  0.16020251158942467
iteration : 1116
train acc:  0.84375
train loss:  0.3275831639766693
train gradient:  0.12612493563618926
iteration : 1117
train acc:  0.859375
train loss:  0.2831764817237854
train gradient:  0.11063546233503452
iteration : 1118
train acc:  0.8203125
train loss:  0.3936449885368347
train gradient:  0.22199188943695705
iteration : 1119
train acc:  0.875
train loss:  0.3426766097545624
train gradient:  0.1086671795624271
iteration : 1120
train acc:  0.828125
train loss:  0.36569374799728394
train gradient:  0.22156612023266292
iteration : 1121
train acc:  0.828125
train loss:  0.37612298130989075
train gradient:  0.2956856322428937
iteration : 1122
train acc:  0.84375
train loss:  0.32799994945526123
train gradient:  0.16650331605548707
iteration : 1123
train acc:  0.875
train loss:  0.3012685477733612
train gradient:  0.1561914381362975
iteration : 1124
train acc:  0.875
train loss:  0.3033919334411621
train gradient:  0.17344761901716402
iteration : 1125
train acc:  0.890625
train loss:  0.32021164894104004
train gradient:  0.14534062880176207
iteration : 1126
train acc:  0.859375
train loss:  0.3511536717414856
train gradient:  0.1392420451580058
iteration : 1127
train acc:  0.8671875
train loss:  0.3645448684692383
train gradient:  0.18748720327791263
iteration : 1128
train acc:  0.921875
train loss:  0.2158958613872528
train gradient:  0.05273172434136886
iteration : 1129
train acc:  0.890625
train loss:  0.2884056568145752
train gradient:  0.10082440504098436
iteration : 1130
train acc:  0.9375
train loss:  0.20768679678440094
train gradient:  0.08130110298475643
iteration : 1131
train acc:  0.875
train loss:  0.3214811682701111
train gradient:  0.16248393405596678
iteration : 1132
train acc:  0.8984375
train loss:  0.24241627752780914
train gradient:  0.11006447191853572
iteration : 1133
train acc:  0.8203125
train loss:  0.35858458280563354
train gradient:  0.1508178505172866
iteration : 1134
train acc:  0.84375
train loss:  0.3804788887500763
train gradient:  0.189972930706938
iteration : 1135
train acc:  0.8828125
train loss:  0.2693299651145935
train gradient:  0.07797911509217674
iteration : 1136
train acc:  0.8984375
train loss:  0.25640466809272766
train gradient:  0.09806367691626147
iteration : 1137
train acc:  0.8984375
train loss:  0.2506496012210846
train gradient:  0.11744996012577548
iteration : 1138
train acc:  0.84375
train loss:  0.37198394536972046
train gradient:  0.165120834268578
iteration : 1139
train acc:  0.875
train loss:  0.27181798219680786
train gradient:  0.13187293436181208
iteration : 1140
train acc:  0.875
train loss:  0.25790613889694214
train gradient:  0.09171408396077388
iteration : 1141
train acc:  0.890625
train loss:  0.2950976490974426
train gradient:  0.12861019063261314
iteration : 1142
train acc:  0.875
train loss:  0.29950597882270813
train gradient:  0.10082241583268735
iteration : 1143
train acc:  0.8828125
train loss:  0.34402763843536377
train gradient:  0.12674275244625233
iteration : 1144
train acc:  0.828125
train loss:  0.37640580534935
train gradient:  0.1535605065416698
iteration : 1145
train acc:  0.8515625
train loss:  0.3026091158390045
train gradient:  0.14003752241178918
iteration : 1146
train acc:  0.8828125
train loss:  0.24895495176315308
train gradient:  0.10894774841144463
iteration : 1147
train acc:  0.859375
train loss:  0.2541816532611847
train gradient:  0.14142978664274503
iteration : 1148
train acc:  0.8984375
train loss:  0.2556595206260681
train gradient:  0.09159443781759741
iteration : 1149
train acc:  0.8046875
train loss:  0.44905731081962585
train gradient:  0.2648499933302316
iteration : 1150
train acc:  0.859375
train loss:  0.36029693484306335
train gradient:  0.24867657596841808
iteration : 1151
train acc:  0.8515625
train loss:  0.3130292296409607
train gradient:  0.14754994437361213
iteration : 1152
train acc:  0.8828125
train loss:  0.28736627101898193
train gradient:  0.10153093150142346
iteration : 1153
train acc:  0.84375
train loss:  0.37772148847579956
train gradient:  0.14360222188187566
iteration : 1154
train acc:  0.859375
train loss:  0.30083417892456055
train gradient:  0.1429667928177868
iteration : 1155
train acc:  0.8671875
train loss:  0.3134519159793854
train gradient:  0.17398282551645805
iteration : 1156
train acc:  0.8203125
train loss:  0.414098858833313
train gradient:  0.1843548553456682
iteration : 1157
train acc:  0.9140625
train loss:  0.20762845873832703
train gradient:  0.08160179649615522
iteration : 1158
train acc:  0.859375
train loss:  0.2572496831417084
train gradient:  0.0866216790044153
iteration : 1159
train acc:  0.8828125
train loss:  0.2896745204925537
train gradient:  0.11309778696901822
iteration : 1160
train acc:  0.8515625
train loss:  0.30213648080825806
train gradient:  0.15365241444245775
iteration : 1161
train acc:  0.890625
train loss:  0.28179025650024414
train gradient:  0.10253014156908198
iteration : 1162
train acc:  0.875
train loss:  0.28531450033187866
train gradient:  0.16495860330165535
iteration : 1163
train acc:  0.8515625
train loss:  0.3369283080101013
train gradient:  0.12421642129359779
iteration : 1164
train acc:  0.875
train loss:  0.2715910077095032
train gradient:  0.14077053445612797
iteration : 1165
train acc:  0.90625
train loss:  0.24617016315460205
train gradient:  0.08938344721691785
iteration : 1166
train acc:  0.8828125
train loss:  0.27151164412498474
train gradient:  0.14140480686212786
iteration : 1167
train acc:  0.8828125
train loss:  0.2338678538799286
train gradient:  0.0892156447954529
iteration : 1168
train acc:  0.8515625
train loss:  0.29506972432136536
train gradient:  0.1636276927027992
iteration : 1169
train acc:  0.890625
train loss:  0.29830822348594666
train gradient:  0.11095913524629203
iteration : 1170
train acc:  0.828125
train loss:  0.3554401397705078
train gradient:  0.1770908473518764
iteration : 1171
train acc:  0.8671875
train loss:  0.34423378109931946
train gradient:  0.17789400860597987
iteration : 1172
train acc:  0.8515625
train loss:  0.3263159990310669
train gradient:  0.11658620879235759
iteration : 1173
train acc:  0.8984375
train loss:  0.22615395486354828
train gradient:  0.10616287155124879
iteration : 1174
train acc:  0.8984375
train loss:  0.3031684160232544
train gradient:  0.23761552692015336
iteration : 1175
train acc:  0.84375
train loss:  0.34538379311561584
train gradient:  0.15592388687346553
iteration : 1176
train acc:  0.8515625
train loss:  0.2808326184749603
train gradient:  0.11000305699558632
iteration : 1177
train acc:  0.8515625
train loss:  0.32536810636520386
train gradient:  0.1562636189687443
iteration : 1178
train acc:  0.8671875
train loss:  0.3134305477142334
train gradient:  0.13316386278741552
iteration : 1179
train acc:  0.84375
train loss:  0.3113451302051544
train gradient:  0.13392133620513652
iteration : 1180
train acc:  0.921875
train loss:  0.28582459688186646
train gradient:  0.12057002136214823
iteration : 1181
train acc:  0.875
train loss:  0.3218422532081604
train gradient:  0.14866087002192496
iteration : 1182
train acc:  0.875
train loss:  0.27630847692489624
train gradient:  0.17633999607283157
iteration : 1183
train acc:  0.859375
train loss:  0.3495151400566101
train gradient:  0.12788441525063252
iteration : 1184
train acc:  0.8828125
train loss:  0.3455967307090759
train gradient:  0.17222995061748975
iteration : 1185
train acc:  0.8671875
train loss:  0.2883044481277466
train gradient:  0.1171833280392203
iteration : 1186
train acc:  0.8515625
train loss:  0.34209758043289185
train gradient:  0.13509902117422395
iteration : 1187
train acc:  0.8671875
train loss:  0.2876116633415222
train gradient:  0.1545917493147665
iteration : 1188
train acc:  0.8828125
train loss:  0.25967514514923096
train gradient:  0.13292644709105292
iteration : 1189
train acc:  0.8984375
train loss:  0.2863045036792755
train gradient:  0.11727016829573779
iteration : 1190
train acc:  0.8515625
train loss:  0.3791837692260742
train gradient:  0.14288334055005017
iteration : 1191
train acc:  0.8203125
train loss:  0.3346157670021057
train gradient:  0.13140823417354563
iteration : 1192
train acc:  0.8515625
train loss:  0.33770060539245605
train gradient:  0.15072693772005352
iteration : 1193
train acc:  0.8984375
train loss:  0.2856130301952362
train gradient:  0.10815962051747283
iteration : 1194
train acc:  0.8984375
train loss:  0.2759571671485901
train gradient:  0.08797829345266098
iteration : 1195
train acc:  0.84375
train loss:  0.3765057921409607
train gradient:  0.25717136765354126
iteration : 1196
train acc:  0.8046875
train loss:  0.38017141819000244
train gradient:  0.1995215992479832
iteration : 1197
train acc:  0.8125
train loss:  0.38716623187065125
train gradient:  0.18008258245717773
iteration : 1198
train acc:  0.8984375
train loss:  0.3018334209918976
train gradient:  0.1643167929250727
iteration : 1199
train acc:  0.859375
train loss:  0.2888614535331726
train gradient:  0.1150746630799562
iteration : 1200
train acc:  0.84375
train loss:  0.3503277003765106
train gradient:  0.23242464927168502
iteration : 1201
train acc:  0.8125
train loss:  0.37526124715805054
train gradient:  0.19448805351240056
iteration : 1202
train acc:  0.921875
train loss:  0.24090918898582458
train gradient:  0.09548235378800628
iteration : 1203
train acc:  0.875
train loss:  0.3082718253135681
train gradient:  0.14098783442734178
iteration : 1204
train acc:  0.875
train loss:  0.29165101051330566
train gradient:  0.09295127723967418
iteration : 1205
train acc:  0.8671875
train loss:  0.3509553074836731
train gradient:  0.16420253027916554
iteration : 1206
train acc:  0.875
train loss:  0.2918274402618408
train gradient:  0.1737733327451414
iteration : 1207
train acc:  0.8828125
train loss:  0.3109835386276245
train gradient:  0.15962713077552165
iteration : 1208
train acc:  0.8671875
train loss:  0.3179078698158264
train gradient:  0.12760846833839765
iteration : 1209
train acc:  0.8203125
train loss:  0.3511628806591034
train gradient:  0.14226947771584456
iteration : 1210
train acc:  0.84375
train loss:  0.358725368976593
train gradient:  0.1564879034867186
iteration : 1211
train acc:  0.875
train loss:  0.2565491795539856
train gradient:  0.08803339276629535
iteration : 1212
train acc:  0.90625
train loss:  0.28076261281967163
train gradient:  0.1732448948099719
iteration : 1213
train acc:  0.8515625
train loss:  0.27466925978660583
train gradient:  0.13477281020383292
iteration : 1214
train acc:  0.8359375
train loss:  0.3580740690231323
train gradient:  0.1710207041334953
iteration : 1215
train acc:  0.84375
train loss:  0.2868291437625885
train gradient:  0.13804398492889847
iteration : 1216
train acc:  0.859375
train loss:  0.27490299940109253
train gradient:  0.11629049710187771
iteration : 1217
train acc:  0.796875
train loss:  0.37270694971084595
train gradient:  0.2029337237872689
iteration : 1218
train acc:  0.8359375
train loss:  0.34106358885765076
train gradient:  0.22778031921174902
iteration : 1219
train acc:  0.90625
train loss:  0.2358688861131668
train gradient:  0.08485351253270586
iteration : 1220
train acc:  0.8671875
train loss:  0.29105719923973083
train gradient:  0.12096298562301804
iteration : 1221
train acc:  0.84375
train loss:  0.38657766580581665
train gradient:  0.1990948430593407
iteration : 1222
train acc:  0.875
train loss:  0.3044669032096863
train gradient:  0.1114038528004982
iteration : 1223
train acc:  0.8203125
train loss:  0.395458847284317
train gradient:  0.1572405775362724
iteration : 1224
train acc:  0.859375
train loss:  0.353637158870697
train gradient:  0.17156894122544886
iteration : 1225
train acc:  0.90625
train loss:  0.26929110288619995
train gradient:  0.1469881560520283
iteration : 1226
train acc:  0.8671875
train loss:  0.30422845482826233
train gradient:  0.13458511904736464
iteration : 1227
train acc:  0.90625
train loss:  0.24935056269168854
train gradient:  0.1015977106966206
iteration : 1228
train acc:  0.890625
train loss:  0.25356918573379517
train gradient:  0.11994730626805454
iteration : 1229
train acc:  0.8125
train loss:  0.3243980407714844
train gradient:  0.1737477488705818
iteration : 1230
train acc:  0.8359375
train loss:  0.32472091913223267
train gradient:  0.12530520334323125
iteration : 1231
train acc:  0.84375
train loss:  0.32227784395217896
train gradient:  0.1321505351255925
iteration : 1232
train acc:  0.90625
train loss:  0.21777966618537903
train gradient:  0.11048257811583155
iteration : 1233
train acc:  0.84375
train loss:  0.3061143159866333
train gradient:  0.13068173839979588
iteration : 1234
train acc:  0.859375
train loss:  0.34611794352531433
train gradient:  0.14280917769206736
iteration : 1235
train acc:  0.8203125
train loss:  0.46669262647628784
train gradient:  0.26556687578410654
iteration : 1236
train acc:  0.859375
train loss:  0.3167162537574768
train gradient:  0.10018697264440175
iteration : 1237
train acc:  0.8671875
train loss:  0.28820711374282837
train gradient:  0.0898387532849171
iteration : 1238
train acc:  0.9140625
train loss:  0.28236982226371765
train gradient:  0.11430153099998015
iteration : 1239
train acc:  0.8828125
train loss:  0.27531105279922485
train gradient:  0.09822402038950703
iteration : 1240
train acc:  0.875
train loss:  0.33462563157081604
train gradient:  0.10500955046035423
iteration : 1241
train acc:  0.859375
train loss:  0.318418025970459
train gradient:  0.13795452507393857
iteration : 1242
train acc:  0.8671875
train loss:  0.2551625669002533
train gradient:  0.09182277045411893
iteration : 1243
train acc:  0.90625
train loss:  0.2682873010635376
train gradient:  0.11033171148543208
iteration : 1244
train acc:  0.84375
train loss:  0.3597240447998047
train gradient:  0.18953924257349106
iteration : 1245
train acc:  0.890625
train loss:  0.2708752751350403
train gradient:  0.15520943708310775
iteration : 1246
train acc:  0.8125
train loss:  0.43948137760162354
train gradient:  0.34671785128290405
iteration : 1247
train acc:  0.8671875
train loss:  0.2921624183654785
train gradient:  0.09973851742501992
iteration : 1248
train acc:  0.8515625
train loss:  0.33314287662506104
train gradient:  0.1353957524543276
iteration : 1249
train acc:  0.8359375
train loss:  0.41306227445602417
train gradient:  0.1851856244142206
iteration : 1250
train acc:  0.8515625
train loss:  0.2875816226005554
train gradient:  0.09132466176288263
iteration : 1251
train acc:  0.8359375
train loss:  0.33802151679992676
train gradient:  0.14443105445233123
iteration : 1252
train acc:  0.9140625
train loss:  0.2441614717245102
train gradient:  0.09899500965865657
iteration : 1253
train acc:  0.828125
train loss:  0.3622477650642395
train gradient:  0.15502260116664382
iteration : 1254
train acc:  0.859375
train loss:  0.315775990486145
train gradient:  0.12296250759860106
iteration : 1255
train acc:  0.84375
train loss:  0.33426550030708313
train gradient:  0.11218798305915492
iteration : 1256
train acc:  0.8515625
train loss:  0.35058829188346863
train gradient:  0.12309074758551015
iteration : 1257
train acc:  0.8515625
train loss:  0.3235221803188324
train gradient:  0.13734214580155593
iteration : 1258
train acc:  0.8359375
train loss:  0.37774115800857544
train gradient:  0.19433496952825668
iteration : 1259
train acc:  0.8515625
train loss:  0.2552100121974945
train gradient:  0.08792606745392012
iteration : 1260
train acc:  0.8515625
train loss:  0.2829774022102356
train gradient:  0.12247320906206595
iteration : 1261
train acc:  0.875
train loss:  0.3347122073173523
train gradient:  0.16492351570726277
iteration : 1262
train acc:  0.8515625
train loss:  0.34224003553390503
train gradient:  0.16358411822762114
iteration : 1263
train acc:  0.890625
train loss:  0.2893357276916504
train gradient:  0.10461215526559381
iteration : 1264
train acc:  0.875
train loss:  0.2585049867630005
train gradient:  0.09465898104806299
iteration : 1265
train acc:  0.875
train loss:  0.29047489166259766
train gradient:  0.118767987048493
iteration : 1266
train acc:  0.8671875
train loss:  0.2787797749042511
train gradient:  0.11289891220418595
iteration : 1267
train acc:  0.921875
train loss:  0.2687041759490967
train gradient:  0.10597925671409446
iteration : 1268
train acc:  0.8046875
train loss:  0.3932722806930542
train gradient:  0.1857460725174367
iteration : 1269
train acc:  0.875
train loss:  0.30749011039733887
train gradient:  0.1626212391964974
iteration : 1270
train acc:  0.890625
train loss:  0.2628389000892639
train gradient:  0.07755004181922621
iteration : 1271
train acc:  0.8515625
train loss:  0.28620678186416626
train gradient:  0.13477729541230926
iteration : 1272
train acc:  0.875
train loss:  0.27146071195602417
train gradient:  0.11868863093567869
iteration : 1273
train acc:  0.8515625
train loss:  0.3790731132030487
train gradient:  0.34393756377010654
iteration : 1274
train acc:  0.890625
train loss:  0.2422083616256714
train gradient:  0.08048568408630699
iteration : 1275
train acc:  0.890625
train loss:  0.2526307702064514
train gradient:  0.09157832673341912
iteration : 1276
train acc:  0.8828125
train loss:  0.2687906324863434
train gradient:  0.08839020958304485
iteration : 1277
train acc:  0.875
train loss:  0.3105766177177429
train gradient:  0.16510203077642924
iteration : 1278
train acc:  0.8984375
train loss:  0.30070585012435913
train gradient:  0.14200651036600037
iteration : 1279
train acc:  0.8515625
train loss:  0.30519577860832214
train gradient:  0.1268631825339635
iteration : 1280
train acc:  0.78125
train loss:  0.43807530403137207
train gradient:  0.16977426671198553
iteration : 1281
train acc:  0.8671875
train loss:  0.24596728384494781
train gradient:  0.10544536705819875
iteration : 1282
train acc:  0.8671875
train loss:  0.310005247592926
train gradient:  0.1517946327627096
iteration : 1283
train acc:  0.890625
train loss:  0.25475621223449707
train gradient:  0.11203570701062837
iteration : 1284
train acc:  0.8671875
train loss:  0.2794525623321533
train gradient:  0.10127838424307235
iteration : 1285
train acc:  0.8515625
train loss:  0.36695921421051025
train gradient:  0.18437314995456888
iteration : 1286
train acc:  0.8515625
train loss:  0.3138132691383362
train gradient:  0.12242824877944306
iteration : 1287
train acc:  0.875
train loss:  0.2713674306869507
train gradient:  0.11482759890559069
iteration : 1288
train acc:  0.859375
train loss:  0.38437676429748535
train gradient:  0.16727821047868652
iteration : 1289
train acc:  0.8515625
train loss:  0.3173321783542633
train gradient:  0.1607112710860495
iteration : 1290
train acc:  0.890625
train loss:  0.2731645405292511
train gradient:  0.11825523466312493
iteration : 1291
train acc:  0.921875
train loss:  0.21040388941764832
train gradient:  0.07787447241225792
iteration : 1292
train acc:  0.8203125
train loss:  0.3771974444389343
train gradient:  0.16536498592914745
iteration : 1293
train acc:  0.8203125
train loss:  0.3810320198535919
train gradient:  0.18564361884605352
iteration : 1294
train acc:  0.8671875
train loss:  0.31763771176338196
train gradient:  0.125722112680673
iteration : 1295
train acc:  0.8828125
train loss:  0.312400221824646
train gradient:  0.1341374699426462
iteration : 1296
train acc:  0.859375
train loss:  0.2390662431716919
train gradient:  0.10692976844288489
iteration : 1297
train acc:  0.875
train loss:  0.28789690136909485
train gradient:  0.17755084605765104
iteration : 1298
train acc:  0.90625
train loss:  0.28059911727905273
train gradient:  0.1508316141912575
iteration : 1299
train acc:  0.8828125
train loss:  0.2873481214046478
train gradient:  0.11616974478752636
iteration : 1300
train acc:  0.8671875
train loss:  0.2757863700389862
train gradient:  0.1052483271429013
iteration : 1301
train acc:  0.890625
train loss:  0.30200761556625366
train gradient:  0.15017897377354378
iteration : 1302
train acc:  0.8671875
train loss:  0.3328096866607666
train gradient:  0.15942959322812997
iteration : 1303
train acc:  0.8828125
train loss:  0.3037724494934082
train gradient:  0.16835031956999952
iteration : 1304
train acc:  0.8515625
train loss:  0.3535248637199402
train gradient:  0.16175416344024277
iteration : 1305
train acc:  0.828125
train loss:  0.4600796401500702
train gradient:  0.24708464200937452
iteration : 1306
train acc:  0.921875
train loss:  0.23943816125392914
train gradient:  0.11943404779262258
iteration : 1307
train acc:  0.8828125
train loss:  0.3417564928531647
train gradient:  0.20495098379337728
iteration : 1308
train acc:  0.859375
train loss:  0.338451623916626
train gradient:  0.17580084814301003
iteration : 1309
train acc:  0.84375
train loss:  0.3247280716896057
train gradient:  0.1310880015019338
iteration : 1310
train acc:  0.859375
train loss:  0.2717498540878296
train gradient:  0.10806913512467607
iteration : 1311
train acc:  0.8984375
train loss:  0.2824578285217285
train gradient:  0.1070744308084636
iteration : 1312
train acc:  0.8515625
train loss:  0.3192383050918579
train gradient:  0.10489844151065972
iteration : 1313
train acc:  0.875
train loss:  0.28532159328460693
train gradient:  0.17033336850097297
iteration : 1314
train acc:  0.875
train loss:  0.33892351388931274
train gradient:  0.22162504059419869
iteration : 1315
train acc:  0.796875
train loss:  0.40904372930526733
train gradient:  0.20635766697433167
iteration : 1316
train acc:  0.875
train loss:  0.2687399387359619
train gradient:  0.13231451890763946
iteration : 1317
train acc:  0.84375
train loss:  0.32978004217147827
train gradient:  0.13885811434702838
iteration : 1318
train acc:  0.8515625
train loss:  0.37435460090637207
train gradient:  0.14909354471011974
iteration : 1319
train acc:  0.890625
train loss:  0.2960447072982788
train gradient:  0.10023917627617691
iteration : 1320
train acc:  0.796875
train loss:  0.4591534733772278
train gradient:  0.23265920693130787
iteration : 1321
train acc:  0.875
train loss:  0.28094416856765747
train gradient:  0.11463357403433581
iteration : 1322
train acc:  0.8203125
train loss:  0.34540054202079773
train gradient:  0.14281837874719966
iteration : 1323
train acc:  0.84375
train loss:  0.3670361042022705
train gradient:  0.11420526834957764
iteration : 1324
train acc:  0.859375
train loss:  0.2833648920059204
train gradient:  0.09634680706158691
iteration : 1325
train acc:  0.921875
train loss:  0.22220571339130402
train gradient:  0.06922797315568285
iteration : 1326
train acc:  0.84375
train loss:  0.35621994733810425
train gradient:  0.13150296157658825
iteration : 1327
train acc:  0.8671875
train loss:  0.338312029838562
train gradient:  0.1292034483127189
iteration : 1328
train acc:  0.875
train loss:  0.26554957032203674
train gradient:  0.0906704226267344
iteration : 1329
train acc:  0.8359375
train loss:  0.3452972173690796
train gradient:  0.3657103145097639
iteration : 1330
train acc:  0.8671875
train loss:  0.2605445981025696
train gradient:  0.0930128409092372
iteration : 1331
train acc:  0.8671875
train loss:  0.33068758249282837
train gradient:  0.13699625213180638
iteration : 1332
train acc:  0.796875
train loss:  0.4592438042163849
train gradient:  0.26464534095855957
iteration : 1333
train acc:  0.9296875
train loss:  0.22455063462257385
train gradient:  0.05780665571127726
iteration : 1334
train acc:  0.875
train loss:  0.2997995615005493
train gradient:  0.09587271829693776
iteration : 1335
train acc:  0.84375
train loss:  0.3394353985786438
train gradient:  0.16229994474380532
iteration : 1336
train acc:  0.8203125
train loss:  0.3159559369087219
train gradient:  0.12239977579009238
iteration : 1337
train acc:  0.8203125
train loss:  0.3998591899871826
train gradient:  0.18151584420407763
iteration : 1338
train acc:  0.8828125
train loss:  0.2783365547657013
train gradient:  0.08100246440993276
iteration : 1339
train acc:  0.8515625
train loss:  0.32417160272598267
train gradient:  0.11758337704493135
iteration : 1340
train acc:  0.859375
train loss:  0.3137916326522827
train gradient:  0.1566338421331778
iteration : 1341
train acc:  0.8125
train loss:  0.38663914799690247
train gradient:  0.15516226388861104
iteration : 1342
train acc:  0.8828125
train loss:  0.28572458028793335
train gradient:  0.09790692826730335
iteration : 1343
train acc:  0.7890625
train loss:  0.4153760075569153
train gradient:  0.2188553871425809
iteration : 1344
train acc:  0.8984375
train loss:  0.32360970973968506
train gradient:  0.11710304787205683
iteration : 1345
train acc:  0.8359375
train loss:  0.34402528405189514
train gradient:  0.1562433872175396
iteration : 1346
train acc:  0.8671875
train loss:  0.3330380916595459
train gradient:  0.15941791044390693
iteration : 1347
train acc:  0.8828125
train loss:  0.2817305326461792
train gradient:  0.07309789832060257
iteration : 1348
train acc:  0.859375
train loss:  0.30611979961395264
train gradient:  0.12146338497021196
iteration : 1349
train acc:  0.859375
train loss:  0.3133997619152069
train gradient:  0.08718448933205603
iteration : 1350
train acc:  0.8125
train loss:  0.3338211178779602
train gradient:  0.12843311752125103
iteration : 1351
train acc:  0.890625
train loss:  0.23670458793640137
train gradient:  0.1802876217934714
iteration : 1352
train acc:  0.8515625
train loss:  0.32719719409942627
train gradient:  0.1304917261621706
iteration : 1353
train acc:  0.8671875
train loss:  0.27870064973831177
train gradient:  0.14499154900676497
iteration : 1354
train acc:  0.8671875
train loss:  0.3217107653617859
train gradient:  0.1359361414080031
iteration : 1355
train acc:  0.8359375
train loss:  0.32100555300712585
train gradient:  0.1326091810278899
iteration : 1356
train acc:  0.8828125
train loss:  0.25321927666664124
train gradient:  0.10104658280244022
iteration : 1357
train acc:  0.8671875
train loss:  0.27670273184776306
train gradient:  0.09217118502513502
iteration : 1358
train acc:  0.859375
train loss:  0.28444918990135193
train gradient:  0.10349180081146493
iteration : 1359
train acc:  0.828125
train loss:  0.4156728982925415
train gradient:  0.36623256282675115
iteration : 1360
train acc:  0.9296875
train loss:  0.20508968830108643
train gradient:  0.0679490619852208
iteration : 1361
train acc:  0.8125
train loss:  0.40735501050949097
train gradient:  0.20304148558241814
iteration : 1362
train acc:  0.8203125
train loss:  0.3929542899131775
train gradient:  0.1713476327041495
iteration : 1363
train acc:  0.828125
train loss:  0.36480480432510376
train gradient:  0.12197747272727916
iteration : 1364
train acc:  0.8828125
train loss:  0.2667846977710724
train gradient:  0.15326219772485378
iteration : 1365
train acc:  0.8671875
train loss:  0.3464590311050415
train gradient:  0.16935617525642283
iteration : 1366
train acc:  0.859375
train loss:  0.27834177017211914
train gradient:  0.08376140372779257
iteration : 1367
train acc:  0.8671875
train loss:  0.2964528799057007
train gradient:  0.13641778125027237
iteration : 1368
train acc:  0.8515625
train loss:  0.3030088543891907
train gradient:  0.12156707061163502
iteration : 1369
train acc:  0.875
train loss:  0.26333296298980713
train gradient:  0.10163683598150164
iteration : 1370
train acc:  0.84375
train loss:  0.32431674003601074
train gradient:  0.1360502039132629
iteration : 1371
train acc:  0.8515625
train loss:  0.311229944229126
train gradient:  0.13927821317595074
iteration : 1372
train acc:  0.890625
train loss:  0.2686607837677002
train gradient:  0.10351020165843554
iteration : 1373
train acc:  0.8125
train loss:  0.4037405252456665
train gradient:  0.29147806682178845
iteration : 1374
train acc:  0.8828125
train loss:  0.28897905349731445
train gradient:  0.09980363742576834
iteration : 1375
train acc:  0.8828125
train loss:  0.2686290442943573
train gradient:  0.1008064286093911
iteration : 1376
train acc:  0.8671875
train loss:  0.2930230498313904
train gradient:  0.09742411868402723
iteration : 1377
train acc:  0.8359375
train loss:  0.31952834129333496
train gradient:  0.14862345257162396
iteration : 1378
train acc:  0.796875
train loss:  0.4581671357154846
train gradient:  0.2594650589756522
iteration : 1379
train acc:  0.8671875
train loss:  0.31531214714050293
train gradient:  0.14337071685651703
iteration : 1380
train acc:  0.828125
train loss:  0.33602121472358704
train gradient:  0.15504882354641336
iteration : 1381
train acc:  0.8671875
train loss:  0.29129064083099365
train gradient:  0.12304136200641132
iteration : 1382
train acc:  0.90625
train loss:  0.23698794841766357
train gradient:  0.12302120213921391
iteration : 1383
train acc:  0.8515625
train loss:  0.288784384727478
train gradient:  0.1209898122468298
iteration : 1384
train acc:  0.8984375
train loss:  0.2766284942626953
train gradient:  0.08846687924891963
iteration : 1385
train acc:  0.84375
train loss:  0.3219636082649231
train gradient:  0.1404465302642334
iteration : 1386
train acc:  0.828125
train loss:  0.34263506531715393
train gradient:  0.18900438991944515
iteration : 1387
train acc:  0.875
train loss:  0.2981964349746704
train gradient:  0.13256801855832567
iteration : 1388
train acc:  0.9140625
train loss:  0.2396920919418335
train gradient:  0.1014921261129633
iteration : 1389
train acc:  0.8828125
train loss:  0.2901914119720459
train gradient:  0.14218246072391882
iteration : 1390
train acc:  0.90625
train loss:  0.24280834197998047
train gradient:  0.12967500233853674
iteration : 1391
train acc:  0.828125
train loss:  0.381446897983551
train gradient:  0.1748183688150362
iteration : 1392
train acc:  0.8984375
train loss:  0.23934367299079895
train gradient:  0.11469194004354236
iteration : 1393
train acc:  0.859375
train loss:  0.3301960229873657
train gradient:  0.15898098778139974
iteration : 1394
train acc:  0.8984375
train loss:  0.2800213396549225
train gradient:  0.11038362487689544
iteration : 1395
train acc:  0.9140625
train loss:  0.25561216473579407
train gradient:  0.10840022896150249
iteration : 1396
train acc:  0.875
train loss:  0.24513988196849823
train gradient:  0.06450757776674242
iteration : 1397
train acc:  0.859375
train loss:  0.31054940819740295
train gradient:  0.11111775758116621
iteration : 1398
train acc:  0.8359375
train loss:  0.3666451871395111
train gradient:  0.20537799355207964
iteration : 1399
train acc:  0.8828125
train loss:  0.372411847114563
train gradient:  0.16889282156783436
iteration : 1400
train acc:  0.859375
train loss:  0.3235504627227783
train gradient:  0.10748101686789858
iteration : 1401
train acc:  0.8515625
train loss:  0.3554469645023346
train gradient:  0.15049166716055395
iteration : 1402
train acc:  0.9375
train loss:  0.20946456491947174
train gradient:  0.07168936065566608
iteration : 1403
train acc:  0.8515625
train loss:  0.32500261068344116
train gradient:  0.10835323212433835
iteration : 1404
train acc:  0.8046875
train loss:  0.4508380889892578
train gradient:  0.24709168949255184
iteration : 1405
train acc:  0.828125
train loss:  0.3993741273880005
train gradient:  0.22022809811795613
iteration : 1406
train acc:  0.8515625
train loss:  0.34591057896614075
train gradient:  0.17481845210321068
iteration : 1407
train acc:  0.8828125
train loss:  0.2643497586250305
train gradient:  0.13611890697407034
iteration : 1408
train acc:  0.90625
train loss:  0.22697237133979797
train gradient:  0.08261082040307612
iteration : 1409
train acc:  0.890625
train loss:  0.2917369604110718
train gradient:  0.09176940676645727
iteration : 1410
train acc:  0.8671875
train loss:  0.32995283603668213
train gradient:  0.13926271033984816
iteration : 1411
train acc:  0.8984375
train loss:  0.23920783400535583
train gradient:  0.08477140782978751
iteration : 1412
train acc:  0.8671875
train loss:  0.3134915828704834
train gradient:  0.11835243229186533
iteration : 1413
train acc:  0.90625
train loss:  0.24970854818820953
train gradient:  0.1042880342521037
iteration : 1414
train acc:  0.859375
train loss:  0.2781250476837158
train gradient:  0.1178302659807979
iteration : 1415
train acc:  0.8984375
train loss:  0.24360360205173492
train gradient:  0.07567955658630698
iteration : 1416
train acc:  0.84375
train loss:  0.38450706005096436
train gradient:  0.1874842561890626
iteration : 1417
train acc:  0.8046875
train loss:  0.4497421085834503
train gradient:  0.2005764831363316
iteration : 1418
train acc:  0.890625
train loss:  0.3087772727012634
train gradient:  0.1368418830045504
iteration : 1419
train acc:  0.875
train loss:  0.3116896152496338
train gradient:  0.09778333946974649
iteration : 1420
train acc:  0.8671875
train loss:  0.2876717150211334
train gradient:  0.08515063181398691
iteration : 1421
train acc:  0.84375
train loss:  0.3493954539299011
train gradient:  0.11858781878415939
iteration : 1422
train acc:  0.90625
train loss:  0.3101433217525482
train gradient:  0.11403308173466181
iteration : 1423
train acc:  0.875
train loss:  0.28103700280189514
train gradient:  0.12734276408607656
iteration : 1424
train acc:  0.859375
train loss:  0.3273577094078064
train gradient:  0.16136494390650957
iteration : 1425
train acc:  0.875
train loss:  0.29526007175445557
train gradient:  0.11124209263084954
iteration : 1426
train acc:  0.890625
train loss:  0.2907441258430481
train gradient:  0.11946376571531642
iteration : 1427
train acc:  0.8984375
train loss:  0.29363134503364563
train gradient:  0.1289459114555564
iteration : 1428
train acc:  0.90625
train loss:  0.2980746328830719
train gradient:  0.14850098165036432
iteration : 1429
train acc:  0.859375
train loss:  0.4234640598297119
train gradient:  0.26127303240404515
iteration : 1430
train acc:  0.84375
train loss:  0.30891069769859314
train gradient:  0.14610923429680078
iteration : 1431
train acc:  0.8359375
train loss:  0.40882572531700134
train gradient:  0.31284937051511885
iteration : 1432
train acc:  0.859375
train loss:  0.31061381101608276
train gradient:  0.10650977313241221
iteration : 1433
train acc:  0.875
train loss:  0.25394126772880554
train gradient:  0.16649661243813357
iteration : 1434
train acc:  0.890625
train loss:  0.2898674011230469
train gradient:  0.08122634481296594
iteration : 1435
train acc:  0.875
train loss:  0.28765010833740234
train gradient:  0.1899959701158849
iteration : 1436
train acc:  0.8359375
train loss:  0.3601607084274292
train gradient:  0.189809169009053
iteration : 1437
train acc:  0.859375
train loss:  0.32549232244491577
train gradient:  0.16260661133622137
iteration : 1438
train acc:  0.8515625
train loss:  0.34121400117874146
train gradient:  0.1409058623557989
iteration : 1439
train acc:  0.90625
train loss:  0.22911246120929718
train gradient:  0.09286960516396621
iteration : 1440
train acc:  0.875
train loss:  0.3068538308143616
train gradient:  0.13172415345674104
iteration : 1441
train acc:  0.8046875
train loss:  0.3604215681552887
train gradient:  0.18057211659159084
iteration : 1442
train acc:  0.8671875
train loss:  0.3385753035545349
train gradient:  0.1492183773394432
iteration : 1443
train acc:  0.8984375
train loss:  0.26569703221321106
train gradient:  0.09619632995799832
iteration : 1444
train acc:  0.875
train loss:  0.3015033006668091
train gradient:  0.1283830534988123
iteration : 1445
train acc:  0.828125
train loss:  0.34018057584762573
train gradient:  0.25069540402132023
iteration : 1446
train acc:  0.828125
train loss:  0.3436492681503296
train gradient:  0.14977117486072558
iteration : 1447
train acc:  0.8828125
train loss:  0.29091334342956543
train gradient:  0.11895123031661907
iteration : 1448
train acc:  0.8828125
train loss:  0.3413034975528717
train gradient:  0.12610633417201908
iteration : 1449
train acc:  0.8671875
train loss:  0.327854722738266
train gradient:  0.13891031632560624
iteration : 1450
train acc:  0.8515625
train loss:  0.38291794061660767
train gradient:  0.160792840786385
iteration : 1451
train acc:  0.875
train loss:  0.30097806453704834
train gradient:  0.14862919903032967
iteration : 1452
train acc:  0.890625
train loss:  0.2598004639148712
train gradient:  0.08056687197539415
iteration : 1453
train acc:  0.890625
train loss:  0.3094484508037567
train gradient:  0.0926185588009109
iteration : 1454
train acc:  0.875
train loss:  0.33865392208099365
train gradient:  0.10855481629680763
iteration : 1455
train acc:  0.890625
train loss:  0.30453217029571533
train gradient:  0.11088312391417364
iteration : 1456
train acc:  0.8671875
train loss:  0.2794356644153595
train gradient:  0.09261801256369197
iteration : 1457
train acc:  0.8515625
train loss:  0.3238290846347809
train gradient:  0.14097532507954902
iteration : 1458
train acc:  0.90625
train loss:  0.25025802850723267
train gradient:  0.07000920910299027
iteration : 1459
train acc:  0.859375
train loss:  0.35814493894577026
train gradient:  0.16254406335623986
iteration : 1460
train acc:  0.84375
train loss:  0.35827553272247314
train gradient:  0.17340933205747325
iteration : 1461
train acc:  0.859375
train loss:  0.3008134961128235
train gradient:  0.1261464280732454
iteration : 1462
train acc:  0.8359375
train loss:  0.2968641221523285
train gradient:  0.1419122642507743
iteration : 1463
train acc:  0.796875
train loss:  0.42386290431022644
train gradient:  0.2502063348840439
iteration : 1464
train acc:  0.90625
train loss:  0.2639164328575134
train gradient:  0.11190513371434634
iteration : 1465
train acc:  0.8671875
train loss:  0.3246440887451172
train gradient:  0.10391276774396811
iteration : 1466
train acc:  0.875
train loss:  0.30253180861473083
train gradient:  0.14499744334051345
iteration : 1467
train acc:  0.9140625
train loss:  0.2800431251525879
train gradient:  0.11798610790582188
iteration : 1468
train acc:  0.8515625
train loss:  0.3404526114463806
train gradient:  0.12751873622377222
iteration : 1469
train acc:  0.8359375
train loss:  0.33957958221435547
train gradient:  0.181844907563981
iteration : 1470
train acc:  0.875
train loss:  0.2757779657840729
train gradient:  0.09865202839736398
iteration : 1471
train acc:  0.90625
train loss:  0.23678644001483917
train gradient:  0.14869312861141626
iteration : 1472
train acc:  0.8671875
train loss:  0.2898487448692322
train gradient:  0.11641238925412799
iteration : 1473
train acc:  0.84375
train loss:  0.38889631628990173
train gradient:  0.11702429646462799
iteration : 1474
train acc:  0.859375
train loss:  0.31349918246269226
train gradient:  0.17091761341735218
iteration : 1475
train acc:  0.8203125
train loss:  0.3625643849372864
train gradient:  0.1448469468701483
iteration : 1476
train acc:  0.796875
train loss:  0.3554302155971527
train gradient:  0.23264939456417386
iteration : 1477
train acc:  0.875
train loss:  0.2491963654756546
train gradient:  0.10409042984928168
iteration : 1478
train acc:  0.875
train loss:  0.32419925928115845
train gradient:  0.15737463465570795
iteration : 1479
train acc:  0.875
train loss:  0.24584487080574036
train gradient:  0.1039898221623013
iteration : 1480
train acc:  0.8984375
train loss:  0.2838534712791443
train gradient:  0.14444745372163528
iteration : 1481
train acc:  0.8984375
train loss:  0.270775705575943
train gradient:  0.10974795461862542
iteration : 1482
train acc:  0.8125
train loss:  0.43020009994506836
train gradient:  0.19025274362398978
iteration : 1483
train acc:  0.859375
train loss:  0.3256509602069855
train gradient:  0.12340038661573452
iteration : 1484
train acc:  0.890625
train loss:  0.25778117775917053
train gradient:  0.11864742060678678
iteration : 1485
train acc:  0.859375
train loss:  0.2763584554195404
train gradient:  0.13244680234160766
iteration : 1486
train acc:  0.84375
train loss:  0.338273286819458
train gradient:  0.2317849774080279
iteration : 1487
train acc:  0.9140625
train loss:  0.27473729848861694
train gradient:  0.08223500997679761
iteration : 1488
train acc:  0.8203125
train loss:  0.33882567286491394
train gradient:  0.14798339777781788
iteration : 1489
train acc:  0.8125
train loss:  0.3333054184913635
train gradient:  0.1592635630977286
iteration : 1490
train acc:  0.828125
train loss:  0.37988537549972534
train gradient:  0.14722998388775277
iteration : 1491
train acc:  0.859375
train loss:  0.3328685760498047
train gradient:  0.12454827070912404
iteration : 1492
train acc:  0.8515625
train loss:  0.30446261167526245
train gradient:  0.1008867117238971
iteration : 1493
train acc:  0.8828125
train loss:  0.31601735949516296
train gradient:  0.1338233477438701
iteration : 1494
train acc:  0.90625
train loss:  0.21304666996002197
train gradient:  0.07485387770683188
iteration : 1495
train acc:  0.859375
train loss:  0.2708723545074463
train gradient:  0.10795022452783629
iteration : 1496
train acc:  0.890625
train loss:  0.2823342978954315
train gradient:  0.12338903102265308
iteration : 1497
train acc:  0.859375
train loss:  0.33697277307510376
train gradient:  0.12796876221776946
iteration : 1498
train acc:  0.8359375
train loss:  0.3406262695789337
train gradient:  0.15328039318676445
iteration : 1499
train acc:  0.875
train loss:  0.2975975573062897
train gradient:  0.11528011777719784
iteration : 1500
train acc:  0.90625
train loss:  0.30350425839424133
train gradient:  0.1533007775729962
iteration : 1501
train acc:  0.84375
train loss:  0.3143234848976135
train gradient:  0.15859877712068463
iteration : 1502
train acc:  0.8828125
train loss:  0.25048011541366577
train gradient:  0.0823593663448224
iteration : 1503
train acc:  0.890625
train loss:  0.2748941481113434
train gradient:  0.16423294906655386
iteration : 1504
train acc:  0.890625
train loss:  0.33997631072998047
train gradient:  0.1689272679980025
iteration : 1505
train acc:  0.890625
train loss:  0.2725760340690613
train gradient:  0.09601670752897
iteration : 1506
train acc:  0.859375
train loss:  0.339496910572052
train gradient:  0.14637508750165068
iteration : 1507
train acc:  0.8359375
train loss:  0.3413110375404358
train gradient:  0.122554545028084
iteration : 1508
train acc:  0.8671875
train loss:  0.34512490034103394
train gradient:  0.18254415949108482
iteration : 1509
train acc:  0.8359375
train loss:  0.38090449571609497
train gradient:  0.19892545039975162
iteration : 1510
train acc:  0.8046875
train loss:  0.3843770921230316
train gradient:  0.210037168418937
iteration : 1511
train acc:  0.828125
train loss:  0.430179238319397
train gradient:  0.23476329473583465
iteration : 1512
train acc:  0.890625
train loss:  0.251589298248291
train gradient:  0.15114275845713998
iteration : 1513
train acc:  0.8359375
train loss:  0.3880833387374878
train gradient:  0.1493840375863726
iteration : 1514
train acc:  0.875
train loss:  0.3177000880241394
train gradient:  0.14382130163146264
iteration : 1515
train acc:  0.84375
train loss:  0.34011390805244446
train gradient:  0.10143373762815372
iteration : 1516
train acc:  0.875
train loss:  0.3082185983657837
train gradient:  0.10160558809912677
iteration : 1517
train acc:  0.8671875
train loss:  0.256388396024704
train gradient:  0.09457305227655227
iteration : 1518
train acc:  0.890625
train loss:  0.2527349889278412
train gradient:  0.10541133980021457
iteration : 1519
train acc:  0.8984375
train loss:  0.25004705786705017
train gradient:  0.07538000948269423
iteration : 1520
train acc:  0.8203125
train loss:  0.41038599610328674
train gradient:  0.22053572057001186
iteration : 1521
train acc:  0.8984375
train loss:  0.24324655532836914
train gradient:  0.09418646226737658
iteration : 1522
train acc:  0.859375
train loss:  0.29407989978790283
train gradient:  0.11718262661967684
iteration : 1523
train acc:  0.84375
train loss:  0.36307552456855774
train gradient:  0.11284454535172903
iteration : 1524
train acc:  0.859375
train loss:  0.2775267958641052
train gradient:  0.09158306478850968
iteration : 1525
train acc:  0.8515625
train loss:  0.3119530975818634
train gradient:  0.127662136214843
iteration : 1526
train acc:  0.8515625
train loss:  0.33042633533477783
train gradient:  0.11825175444161824
iteration : 1527
train acc:  0.859375
train loss:  0.29893729090690613
train gradient:  0.10962708166058029
iteration : 1528
train acc:  0.859375
train loss:  0.32336851954460144
train gradient:  0.1674799129057759
iteration : 1529
train acc:  0.8828125
train loss:  0.3158276081085205
train gradient:  0.10588553245050665
iteration : 1530
train acc:  0.84375
train loss:  0.33380991220474243
train gradient:  0.1411814831764021
iteration : 1531
train acc:  0.875
train loss:  0.25113487243652344
train gradient:  0.06889096585082033
iteration : 1532
train acc:  0.828125
train loss:  0.4143206477165222
train gradient:  0.15523376756589846
iteration : 1533
train acc:  0.8203125
train loss:  0.34239035844802856
train gradient:  0.12976313995052274
iteration : 1534
train acc:  0.8515625
train loss:  0.35214465856552124
train gradient:  0.11396252160295822
iteration : 1535
train acc:  0.8671875
train loss:  0.30426129698753357
train gradient:  0.1331372313033029
iteration : 1536
train acc:  0.8984375
train loss:  0.24485324323177338
train gradient:  0.09817145847698842
iteration : 1537
train acc:  0.859375
train loss:  0.34324169158935547
train gradient:  0.11284145351527164
iteration : 1538
train acc:  0.8828125
train loss:  0.30377036333084106
train gradient:  0.13411277742659053
iteration : 1539
train acc:  0.890625
train loss:  0.23763129115104675
train gradient:  0.09382659691412834
iteration : 1540
train acc:  0.921875
train loss:  0.19698801636695862
train gradient:  0.06914203313550765
iteration : 1541
train acc:  0.875
train loss:  0.31689125299453735
train gradient:  0.15489925924027656
iteration : 1542
train acc:  0.8203125
train loss:  0.4134927988052368
train gradient:  0.16845910577482365
iteration : 1543
train acc:  0.8984375
train loss:  0.2158368080854416
train gradient:  0.06280542397964857
iteration : 1544
train acc:  0.8828125
train loss:  0.3157106935977936
train gradient:  0.12356505206148526
iteration : 1545
train acc:  0.859375
train loss:  0.33799341320991516
train gradient:  0.14464883241420626
iteration : 1546
train acc:  0.875
train loss:  0.31500571966171265
train gradient:  0.09286758373696047
iteration : 1547
train acc:  0.9140625
train loss:  0.226777121424675
train gradient:  0.08688049701612445
iteration : 1548
train acc:  0.875
train loss:  0.28748124837875366
train gradient:  0.1023294683217662
iteration : 1549
train acc:  0.8828125
train loss:  0.24499736726284027
train gradient:  0.09925832096405911
iteration : 1550
train acc:  0.875
train loss:  0.2574923634529114
train gradient:  0.12369041232477547
iteration : 1551
train acc:  0.875
train loss:  0.30369123816490173
train gradient:  0.11131823173867401
iteration : 1552
train acc:  0.875
train loss:  0.2825784683227539
train gradient:  0.14765851608795094
iteration : 1553
train acc:  0.828125
train loss:  0.33405619859695435
train gradient:  0.1678588815174869
iteration : 1554
train acc:  0.84375
train loss:  0.30015990138053894
train gradient:  0.13406992691318048
iteration : 1555
train acc:  0.8203125
train loss:  0.3679735064506531
train gradient:  0.2343576410823099
iteration : 1556
train acc:  0.875
train loss:  0.2777925133705139
train gradient:  0.09458364337162757
iteration : 1557
train acc:  0.875
train loss:  0.26201021671295166
train gradient:  0.09980067293314542
iteration : 1558
train acc:  0.9140625
train loss:  0.2941133677959442
train gradient:  0.09199275851146725
iteration : 1559
train acc:  0.890625
train loss:  0.35229402780532837
train gradient:  0.17719297013245233
iteration : 1560
train acc:  0.859375
train loss:  0.29716357588768005
train gradient:  0.1222401588475998
iteration : 1561
train acc:  0.875
train loss:  0.3256245255470276
train gradient:  0.1895088895012678
iteration : 1562
train acc:  0.8828125
train loss:  0.3019065260887146
train gradient:  0.13867846435848513
iteration : 1563
train acc:  0.875
train loss:  0.28528913855552673
train gradient:  0.1318358896722343
iteration : 1564
train acc:  0.7890625
train loss:  0.3563331961631775
train gradient:  0.20148819710402727
iteration : 1565
train acc:  0.8671875
train loss:  0.32054197788238525
train gradient:  0.11785920362316606
iteration : 1566
train acc:  0.8828125
train loss:  0.3902573585510254
train gradient:  0.15888564721736037
iteration : 1567
train acc:  0.8515625
train loss:  0.3269566595554352
train gradient:  0.1669110347444096
iteration : 1568
train acc:  0.890625
train loss:  0.2907121181488037
train gradient:  0.11183715133117349
iteration : 1569
train acc:  0.8203125
train loss:  0.3720642924308777
train gradient:  0.16026347860353551
iteration : 1570
train acc:  0.84375
train loss:  0.3713873326778412
train gradient:  0.18162735960754578
iteration : 1571
train acc:  0.8828125
train loss:  0.27238571643829346
train gradient:  0.07329655320957029
iteration : 1572
train acc:  0.875
train loss:  0.3084500730037689
train gradient:  0.13470288234757813
iteration : 1573
train acc:  0.8828125
train loss:  0.31588178873062134
train gradient:  0.16213055943578647
iteration : 1574
train acc:  0.8671875
train loss:  0.3644873797893524
train gradient:  0.1393889767915511
iteration : 1575
train acc:  0.8984375
train loss:  0.2527807056903839
train gradient:  0.09222791794135775
iteration : 1576
train acc:  0.859375
train loss:  0.32091832160949707
train gradient:  0.12885417629521145
iteration : 1577
train acc:  0.890625
train loss:  0.3234001398086548
train gradient:  0.2044444381180301
iteration : 1578
train acc:  0.8671875
train loss:  0.3305255174636841
train gradient:  0.12008957117545511
iteration : 1579
train acc:  0.84375
train loss:  0.32476603984832764
train gradient:  0.1367577679831769
iteration : 1580
train acc:  0.84375
train loss:  0.38005584478378296
train gradient:  0.16892752158045
iteration : 1581
train acc:  0.8203125
train loss:  0.34137198328971863
train gradient:  0.14892573323809727
iteration : 1582
train acc:  0.859375
train loss:  0.33123210072517395
train gradient:  0.18235715632028052
iteration : 1583
train acc:  0.78125
train loss:  0.38392430543899536
train gradient:  0.21349391174541157
iteration : 1584
train acc:  0.8671875
train loss:  0.29494336247444153
train gradient:  0.13902415731896153
iteration : 1585
train acc:  0.7734375
train loss:  0.5211932063102722
train gradient:  0.37359374787923294
iteration : 1586
train acc:  0.8671875
train loss:  0.3564174175262451
train gradient:  0.16781850808200055
iteration : 1587
train acc:  0.8359375
train loss:  0.30280497670173645
train gradient:  0.1128583577824434
iteration : 1588
train acc:  0.8203125
train loss:  0.33774906396865845
train gradient:  0.11035214114496475
iteration : 1589
train acc:  0.8359375
train loss:  0.3570379614830017
train gradient:  0.12891586855312048
iteration : 1590
train acc:  0.9140625
train loss:  0.20859885215759277
train gradient:  0.07826017436026671
iteration : 1591
train acc:  0.8984375
train loss:  0.2914326786994934
train gradient:  0.078373419918183
iteration : 1592
train acc:  0.8359375
train loss:  0.3896387219429016
train gradient:  0.16403501644421897
iteration : 1593
train acc:  0.8359375
train loss:  0.3798544406890869
train gradient:  0.12121182022166216
iteration : 1594
train acc:  0.8984375
train loss:  0.27244025468826294
train gradient:  0.08027362116962591
iteration : 1595
train acc:  0.8203125
train loss:  0.3629496097564697
train gradient:  0.14063852745731614
iteration : 1596
train acc:  0.8671875
train loss:  0.3487333059310913
train gradient:  0.10415876830837158
iteration : 1597
train acc:  0.84375
train loss:  0.34608012437820435
train gradient:  0.16545043376031648
iteration : 1598
train acc:  0.8671875
train loss:  0.3276020884513855
train gradient:  0.1289129202192209
iteration : 1599
train acc:  0.8984375
train loss:  0.261677086353302
train gradient:  0.08732547932465194
iteration : 1600
train acc:  0.8828125
train loss:  0.35980647802352905
train gradient:  0.12533045306477902
iteration : 1601
train acc:  0.8359375
train loss:  0.33939749002456665
train gradient:  0.15202965807061292
iteration : 1602
train acc:  0.8203125
train loss:  0.33464157581329346
train gradient:  0.1271538417762632
iteration : 1603
train acc:  0.8359375
train loss:  0.3212493658065796
train gradient:  0.14155734863505232
iteration : 1604
train acc:  0.8984375
train loss:  0.2664076089859009
train gradient:  0.19924026028836433
iteration : 1605
train acc:  0.8671875
train loss:  0.29886019229888916
train gradient:  0.12023398520466315
iteration : 1606
train acc:  0.859375
train loss:  0.35258352756500244
train gradient:  0.14036793847363277
iteration : 1607
train acc:  0.875
train loss:  0.3460802435874939
train gradient:  0.16024662510572585
iteration : 1608
train acc:  0.8515625
train loss:  0.2985462546348572
train gradient:  0.10282414482205034
iteration : 1609
train acc:  0.859375
train loss:  0.2762129306793213
train gradient:  0.08534048089960951
iteration : 1610
train acc:  0.890625
train loss:  0.30003273487091064
train gradient:  0.09832123608233213
iteration : 1611
train acc:  0.8984375
train loss:  0.2513306140899658
train gradient:  0.07945135248045085
iteration : 1612
train acc:  0.8828125
train loss:  0.2698463797569275
train gradient:  0.11413113138686044
iteration : 1613
train acc:  0.859375
train loss:  0.2730310261249542
train gradient:  0.08418146214335895
iteration : 1614
train acc:  0.8515625
train loss:  0.3371313810348511
train gradient:  0.13496728316225956
iteration : 1615
train acc:  0.8671875
train loss:  0.2846655547618866
train gradient:  0.10739058528748542
iteration : 1616
train acc:  0.859375
train loss:  0.34474873542785645
train gradient:  0.1531701429918031
iteration : 1617
train acc:  0.84375
train loss:  0.34473204612731934
train gradient:  0.13405151998826653
iteration : 1618
train acc:  0.8828125
train loss:  0.3035128712654114
train gradient:  0.10518967278121352
iteration : 1619
train acc:  0.90625
train loss:  0.24367745220661163
train gradient:  0.05730994328180526
iteration : 1620
train acc:  0.84375
train loss:  0.38975414633750916
train gradient:  0.16581351468031608
iteration : 1621
train acc:  0.8828125
train loss:  0.28121185302734375
train gradient:  0.07814659497621522
iteration : 1622
train acc:  0.875
train loss:  0.3249957263469696
train gradient:  0.10829350097122917
iteration : 1623
train acc:  0.84375
train loss:  0.362675279378891
train gradient:  0.18640524244719348
iteration : 1624
train acc:  0.8671875
train loss:  0.3096744120121002
train gradient:  0.16877371274866854
iteration : 1625
train acc:  0.8984375
train loss:  0.2599110007286072
train gradient:  0.16949906360594058
iteration : 1626
train acc:  0.8671875
train loss:  0.30175071954727173
train gradient:  0.12575284762699176
iteration : 1627
train acc:  0.890625
train loss:  0.29931026697158813
train gradient:  0.13410097834776907
iteration : 1628
train acc:  0.8203125
train loss:  0.3043102025985718
train gradient:  0.11732040896005133
iteration : 1629
train acc:  0.859375
train loss:  0.3107859492301941
train gradient:  0.09203030147085085
iteration : 1630
train acc:  0.8828125
train loss:  0.3166862726211548
train gradient:  0.10120987014831807
iteration : 1631
train acc:  0.8671875
train loss:  0.3137252926826477
train gradient:  0.09212357313327695
iteration : 1632
train acc:  0.859375
train loss:  0.33764803409576416
train gradient:  0.156127781184682
iteration : 1633
train acc:  0.8515625
train loss:  0.3659413456916809
train gradient:  0.18367854613614887
iteration : 1634
train acc:  0.84375
train loss:  0.314328670501709
train gradient:  0.10969626435983919
iteration : 1635
train acc:  0.8046875
train loss:  0.37602734565734863
train gradient:  0.24506952519342345
iteration : 1636
train acc:  0.84375
train loss:  0.3414962887763977
train gradient:  0.14353711205610853
iteration : 1637
train acc:  0.84375
train loss:  0.3532894551753998
train gradient:  0.1805052916429173
iteration : 1638
train acc:  0.90625
train loss:  0.2552247643470764
train gradient:  0.07604145970383912
iteration : 1639
train acc:  0.875
train loss:  0.27533310651779175
train gradient:  0.08285408445812137
iteration : 1640
train acc:  0.84375
train loss:  0.33040475845336914
train gradient:  0.11976974544669496
iteration : 1641
train acc:  0.84375
train loss:  0.32797718048095703
train gradient:  0.13262554073497457
iteration : 1642
train acc:  0.8671875
train loss:  0.2992232143878937
train gradient:  0.12107271586658884
iteration : 1643
train acc:  0.890625
train loss:  0.2764454782009125
train gradient:  0.08846004561410047
iteration : 1644
train acc:  0.8125
train loss:  0.3536747395992279
train gradient:  0.12853464845651852
iteration : 1645
train acc:  0.8828125
train loss:  0.28910279273986816
train gradient:  0.10761522297303269
iteration : 1646
train acc:  0.921875
train loss:  0.2165958136320114
train gradient:  0.06763409905577929
iteration : 1647
train acc:  0.8515625
train loss:  0.30814915895462036
train gradient:  0.09677280424379626
iteration : 1648
train acc:  0.8828125
train loss:  0.28195035457611084
train gradient:  0.14208023145982862
iteration : 1649
train acc:  0.859375
train loss:  0.35037291049957275
train gradient:  0.18369414708461873
iteration : 1650
train acc:  0.8984375
train loss:  0.30338814854621887
train gradient:  0.1220761257738262
iteration : 1651
train acc:  0.8125
train loss:  0.3674665689468384
train gradient:  0.15384504297731277
iteration : 1652
train acc:  0.8671875
train loss:  0.32237571477890015
train gradient:  0.1826435508604876
iteration : 1653
train acc:  0.8828125
train loss:  0.29081177711486816
train gradient:  0.10044034656417358
iteration : 1654
train acc:  0.828125
train loss:  0.34107816219329834
train gradient:  0.13572542852934572
iteration : 1655
train acc:  0.8515625
train loss:  0.346498966217041
train gradient:  0.12582463754666637
iteration : 1656
train acc:  0.8671875
train loss:  0.3244282007217407
train gradient:  0.13355698930093957
iteration : 1657
train acc:  0.84375
train loss:  0.2814939618110657
train gradient:  0.0979154041127427
iteration : 1658
train acc:  0.828125
train loss:  0.3358080983161926
train gradient:  0.192785233179858
iteration : 1659
train acc:  0.890625
train loss:  0.2976301610469818
train gradient:  0.09552567915539886
iteration : 1660
train acc:  0.8828125
train loss:  0.22689498960971832
train gradient:  0.0827495454157441
iteration : 1661
train acc:  0.9140625
train loss:  0.3505004346370697
train gradient:  0.1588330878937968
iteration : 1662
train acc:  0.8828125
train loss:  0.2796855568885803
train gradient:  0.1298874384669878
iteration : 1663
train acc:  0.859375
train loss:  0.32136887311935425
train gradient:  0.12258624822260146
iteration : 1664
train acc:  0.875
train loss:  0.3033050000667572
train gradient:  0.11350776559335854
iteration : 1665
train acc:  0.84375
train loss:  0.33876872062683105
train gradient:  0.13375723987911536
iteration : 1666
train acc:  0.8984375
train loss:  0.29406964778900146
train gradient:  0.10115914952169668
iteration : 1667
train acc:  0.828125
train loss:  0.37987005710601807
train gradient:  0.20099804274615693
iteration : 1668
train acc:  0.890625
train loss:  0.30166852474212646
train gradient:  0.1560946266053968
iteration : 1669
train acc:  0.9140625
train loss:  0.2366456538438797
train gradient:  0.07785887222959999
iteration : 1670
train acc:  0.859375
train loss:  0.30297011137008667
train gradient:  0.15616386557810863
iteration : 1671
train acc:  0.890625
train loss:  0.30057239532470703
train gradient:  0.12166381988033902
iteration : 1672
train acc:  0.890625
train loss:  0.25903087854385376
train gradient:  0.10760093565928154
iteration : 1673
train acc:  0.84375
train loss:  0.3068423271179199
train gradient:  0.11329266701572258
iteration : 1674
train acc:  0.875
train loss:  0.2868925929069519
train gradient:  0.12063320887944681
iteration : 1675
train acc:  0.8984375
train loss:  0.2866208553314209
train gradient:  0.10956902072845646
iteration : 1676
train acc:  0.84375
train loss:  0.3906523883342743
train gradient:  0.2122420866857111
iteration : 1677
train acc:  0.8671875
train loss:  0.2586607038974762
train gradient:  0.09101485564379817
iteration : 1678
train acc:  0.8515625
train loss:  0.3334652781486511
train gradient:  0.13375073164072468
iteration : 1679
train acc:  0.8515625
train loss:  0.3206927180290222
train gradient:  0.1652948706808064
iteration : 1680
train acc:  0.890625
train loss:  0.2968018352985382
train gradient:  0.09895936195459543
iteration : 1681
train acc:  0.875
train loss:  0.3256504535675049
train gradient:  0.1546603046043781
iteration : 1682
train acc:  0.8671875
train loss:  0.28049537539482117
train gradient:  0.1199813931392361
iteration : 1683
train acc:  0.8359375
train loss:  0.31317853927612305
train gradient:  0.11419520657961009
iteration : 1684
train acc:  0.90625
train loss:  0.2376617193222046
train gradient:  0.07969454664422455
iteration : 1685
train acc:  0.8984375
train loss:  0.27421900629997253
train gradient:  0.11679489206401115
iteration : 1686
train acc:  0.875
train loss:  0.30685538053512573
train gradient:  0.1017580771942667
iteration : 1687
train acc:  0.8984375
train loss:  0.2548815608024597
train gradient:  0.08607136138707604
iteration : 1688
train acc:  0.890625
train loss:  0.25554853677749634
train gradient:  0.14917132102040404
iteration : 1689
train acc:  0.90625
train loss:  0.2541216015815735
train gradient:  0.0875625516731047
iteration : 1690
train acc:  0.8203125
train loss:  0.3350091576576233
train gradient:  0.1344896703374791
iteration : 1691
train acc:  0.828125
train loss:  0.3900298476219177
train gradient:  0.16113140299828924
iteration : 1692
train acc:  0.8515625
train loss:  0.31554752588272095
train gradient:  0.13711431759094633
iteration : 1693
train acc:  0.90625
train loss:  0.23257504403591156
train gradient:  0.08945931476378695
iteration : 1694
train acc:  0.8515625
train loss:  0.32479575276374817
train gradient:  0.11288665942828217
iteration : 1695
train acc:  0.8828125
train loss:  0.2849203646183014
train gradient:  0.10332262580424063
iteration : 1696
train acc:  0.828125
train loss:  0.3474125266075134
train gradient:  0.24398572949457809
iteration : 1697
train acc:  0.8671875
train loss:  0.2774468660354614
train gradient:  0.10463865689542588
iteration : 1698
train acc:  0.8671875
train loss:  0.3116041421890259
train gradient:  0.1266000117176191
iteration : 1699
train acc:  0.84375
train loss:  0.3258514702320099
train gradient:  0.12745460547042137
iteration : 1700
train acc:  0.921875
train loss:  0.24932511150836945
train gradient:  0.0834860786816174
iteration : 1701
train acc:  0.8671875
train loss:  0.3164527416229248
train gradient:  0.15174235424018961
iteration : 1702
train acc:  0.8984375
train loss:  0.26571130752563477
train gradient:  0.11707212434823243
iteration : 1703
train acc:  0.8515625
train loss:  0.34915339946746826
train gradient:  0.1605189954227128
iteration : 1704
train acc:  0.8515625
train loss:  0.42397546768188477
train gradient:  0.2532149138395098
iteration : 1705
train acc:  0.8515625
train loss:  0.27108320593833923
train gradient:  0.0967268936478233
iteration : 1706
train acc:  0.875
train loss:  0.2908197045326233
train gradient:  0.11597673244100278
iteration : 1707
train acc:  0.84375
train loss:  0.34281474351882935
train gradient:  0.14012431449353574
iteration : 1708
train acc:  0.8515625
train loss:  0.3701353371143341
train gradient:  0.22897214364041363
iteration : 1709
train acc:  0.8828125
train loss:  0.2318948656320572
train gradient:  0.08149056732632125
iteration : 1710
train acc:  0.859375
train loss:  0.2895541787147522
train gradient:  0.16192341336989818
