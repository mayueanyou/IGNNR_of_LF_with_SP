program start:
num_rounds= 3
node_emb_dim= 32

----------------------------------------new_epoch--------------------------------------

epoch:  0
iteration : 0
train acc:  0.453125
train loss:  0.7417659163475037
train gradient:  5.75920755105108
iteration : 1
train acc:  0.421875
train loss:  0.7423382997512817
train gradient:  3.2033480863339068
iteration : 2
train acc:  0.4609375
train loss:  0.7430390119552612
train gradient:  3.081766059856136
iteration : 3
train acc:  0.4375
train loss:  0.7170336246490479
train gradient:  1.7749189711885442
iteration : 4
train acc:  0.5859375
train loss:  0.6903506517410278
train gradient:  2.040084696796445
iteration : 5
train acc:  0.53125
train loss:  0.6607891917228699
train gradient:  0.4948572484691562
iteration : 6
train acc:  0.546875
train loss:  0.6916635036468506
train gradient:  1.1269216243321638
iteration : 7
train acc:  0.4921875
train loss:  0.7028419375419617
train gradient:  1.4182929676211187
iteration : 8
train acc:  0.6328125
train loss:  0.6597094535827637
train gradient:  1.2914825654973243
iteration : 9
train acc:  0.5546875
train loss:  0.6785540580749512
train gradient:  1.4172625990891379
iteration : 10
train acc:  0.609375
train loss:  0.6563369035720825
train gradient:  0.38703730806765657
iteration : 11
train acc:  0.609375
train loss:  0.6522424221038818
train gradient:  2.024235716211453
iteration : 12
train acc:  0.6015625
train loss:  0.6870844960212708
train gradient:  0.6460867799468386
iteration : 13
train acc:  0.5078125
train loss:  0.688723087310791
train gradient:  1.3961703772143408
iteration : 14
train acc:  0.578125
train loss:  0.6759294867515564
train gradient:  0.7966275256446533
iteration : 15
train acc:  0.6328125
train loss:  0.7080437541007996
train gradient:  1.2600408870111868
iteration : 16
train acc:  0.578125
train loss:  0.6931349635124207
train gradient:  3.4145526634959094
iteration : 17
train acc:  0.53125
train loss:  0.7273553013801575
train gradient:  1.98700580846288
iteration : 18
train acc:  0.65625
train loss:  0.6555620431900024
train gradient:  0.5839131705182201
iteration : 19
train acc:  0.625
train loss:  0.6420214176177979
train gradient:  0.561351480111395
iteration : 20
train acc:  0.6171875
train loss:  0.6711710691452026
train gradient:  0.4273281815343098
iteration : 21
train acc:  0.6328125
train loss:  0.6687222123146057
train gradient:  0.7979507418912701
iteration : 22
train acc:  0.59375
train loss:  0.6621695756912231
train gradient:  0.28574238128193624
iteration : 23
train acc:  0.640625
train loss:  0.6717942953109741
train gradient:  0.5457371364055547
iteration : 24
train acc:  0.6484375
train loss:  0.7124146819114685
train gradient:  1.4869877583985964
iteration : 25
train acc:  0.6015625
train loss:  0.7328407168388367
train gradient:  0.8301658809308963
iteration : 26
train acc:  0.609375
train loss:  0.6630874872207642
train gradient:  0.5344242038186008
iteration : 27
train acc:  0.578125
train loss:  0.669152021408081
train gradient:  0.38748162801448677
iteration : 28
train acc:  0.578125
train loss:  0.7172896862030029
train gradient:  0.8807367620505545
iteration : 29
train acc:  0.625
train loss:  0.679410457611084
train gradient:  0.8779911922961604
iteration : 30
train acc:  0.5234375
train loss:  0.6993837356567383
train gradient:  0.8367718441837018
iteration : 31
train acc:  0.5859375
train loss:  0.6826454997062683
train gradient:  0.5586106269099695
iteration : 32
train acc:  0.59375
train loss:  0.6542713046073914
train gradient:  0.48527858286132847
iteration : 33
train acc:  0.59375
train loss:  0.6577026844024658
train gradient:  0.3449390264576677
iteration : 34
train acc:  0.6484375
train loss:  0.6519081592559814
train gradient:  0.5934531643594201
iteration : 35
train acc:  0.578125
train loss:  0.6560260057449341
train gradient:  0.2724541820510207
iteration : 36
train acc:  0.640625
train loss:  0.6537693738937378
train gradient:  0.4120188933464632
iteration : 37
train acc:  0.6015625
train loss:  0.7090719938278198
train gradient:  2.0648772982936077
iteration : 38
train acc:  0.6171875
train loss:  0.6484220027923584
train gradient:  0.5037852982390558
iteration : 39
train acc:  0.625
train loss:  0.665823221206665
train gradient:  0.40609054764842084
iteration : 40
train acc:  0.515625
train loss:  0.7196791768074036
train gradient:  0.525883108500969
iteration : 41
train acc:  0.6875
train loss:  0.6450357437133789
train gradient:  0.41225589492506576
iteration : 42
train acc:  0.6171875
train loss:  0.6317362189292908
train gradient:  0.24760911958702142
iteration : 43
train acc:  0.640625
train loss:  0.6309153437614441
train gradient:  0.16784025486464316
iteration : 44
train acc:  0.578125
train loss:  0.6689963936805725
train gradient:  0.6250928046914691
iteration : 45
train acc:  0.59375
train loss:  0.652474045753479
train gradient:  0.3392667011384423
iteration : 46
train acc:  0.6171875
train loss:  0.6831480264663696
train gradient:  0.7867937131403638
iteration : 47
train acc:  0.609375
train loss:  0.7076915502548218
train gradient:  0.47568783522058333
iteration : 48
train acc:  0.5625
train loss:  0.6547173261642456
train gradient:  0.20264263839656163
iteration : 49
train acc:  0.5703125
train loss:  0.6601848602294922
train gradient:  0.17565484192734965
iteration : 50
train acc:  0.609375
train loss:  0.6759471893310547
train gradient:  0.8134762186048584
iteration : 51
train acc:  0.59375
train loss:  0.6531822681427002
train gradient:  0.37336089139570244
iteration : 52
train acc:  0.6484375
train loss:  0.6591209173202515
train gradient:  0.454183231459173
iteration : 53
train acc:  0.578125
train loss:  0.6628532409667969
train gradient:  0.5100376405365261
iteration : 54
train acc:  0.6328125
train loss:  0.6555153131484985
train gradient:  0.6926415902953768
iteration : 55
train acc:  0.5703125
train loss:  0.683488130569458
train gradient:  0.5266555203881143
iteration : 56
train acc:  0.65625
train loss:  0.6356400847434998
train gradient:  0.2247371315991637
iteration : 57
train acc:  0.546875
train loss:  0.6715262532234192
train gradient:  0.24435621624829987
iteration : 58
train acc:  0.5390625
train loss:  0.6906145215034485
train gradient:  0.32106474568242727
iteration : 59
train acc:  0.6328125
train loss:  0.6644951701164246
train gradient:  0.42998275774695305
iteration : 60
train acc:  0.6484375
train loss:  0.6679115295410156
train gradient:  0.5679585775977875
iteration : 61
train acc:  0.59375
train loss:  0.6463798880577087
train gradient:  0.15572509456365735
iteration : 62
train acc:  0.59375
train loss:  0.6738719940185547
train gradient:  0.30188070679904444
iteration : 63
train acc:  0.640625
train loss:  0.6348234415054321
train gradient:  0.32604476363626533
iteration : 64
train acc:  0.6796875
train loss:  0.6290540099143982
train gradient:  0.3512332518501485
iteration : 65
train acc:  0.6796875
train loss:  0.6226519346237183
train gradient:  0.18823422979345567
iteration : 66
train acc:  0.609375
train loss:  0.6502156257629395
train gradient:  0.3387652248879997
iteration : 67
train acc:  0.6875
train loss:  0.6439212560653687
train gradient:  0.29484089606611513
iteration : 68
train acc:  0.59375
train loss:  0.6616238951683044
train gradient:  0.31870091328870304
iteration : 69
train acc:  0.703125
train loss:  0.6191848516464233
train gradient:  0.7420842412003692
iteration : 70
train acc:  0.59375
train loss:  0.6688786745071411
train gradient:  0.514958411125576
iteration : 71
train acc:  0.609375
train loss:  0.6461329460144043
train gradient:  0.22482511195157828
iteration : 72
train acc:  0.5859375
train loss:  0.6720079779624939
train gradient:  0.19326713721377084
iteration : 73
train acc:  0.6796875
train loss:  0.6159688830375671
train gradient:  0.4180950086843874
iteration : 74
train acc:  0.5625
train loss:  0.7046640515327454
train gradient:  0.8589324436043271
iteration : 75
train acc:  0.6171875
train loss:  0.6719150543212891
train gradient:  0.40143379694328074
iteration : 76
train acc:  0.5546875
train loss:  0.6604650616645813
train gradient:  0.30051453448224985
iteration : 77
train acc:  0.625
train loss:  0.6399964094161987
train gradient:  0.214295157040097
iteration : 78
train acc:  0.671875
train loss:  0.6495197415351868
train gradient:  0.2047224039505403
iteration : 79
train acc:  0.640625
train loss:  0.6276921629905701
train gradient:  0.3651457203432768
iteration : 80
train acc:  0.5859375
train loss:  0.6711534261703491
train gradient:  0.4363480771060368
iteration : 81
train acc:  0.546875
train loss:  0.6702374815940857
train gradient:  0.6782623737440668
iteration : 82
train acc:  0.625
train loss:  0.6297807693481445
train gradient:  0.40327914299402773
iteration : 83
train acc:  0.6328125
train loss:  0.6371555924415588
train gradient:  0.1973723982904083
iteration : 84
train acc:  0.65625
train loss:  0.6093040108680725
train gradient:  0.45843834282354395
iteration : 85
train acc:  0.671875
train loss:  0.612796425819397
train gradient:  0.26209777348944713
iteration : 86
train acc:  0.6328125
train loss:  0.6192547082901001
train gradient:  0.3540698654658672
iteration : 87
train acc:  0.5703125
train loss:  0.6683477759361267
train gradient:  0.31098435475410185
iteration : 88
train acc:  0.5234375
train loss:  0.688040018081665
train gradient:  0.29091724419792997
iteration : 89
train acc:  0.5859375
train loss:  0.6735209226608276
train gradient:  0.6925655288903645
iteration : 90
train acc:  0.625
train loss:  0.6512284278869629
train gradient:  0.5362564113999576
iteration : 91
train acc:  0.640625
train loss:  0.623864471912384
train gradient:  0.6727612056522042
iteration : 92
train acc:  0.5703125
train loss:  0.6657612323760986
train gradient:  0.3648583301180682
iteration : 93
train acc:  0.6875
train loss:  0.6068844199180603
train gradient:  0.3008761403170405
iteration : 94
train acc:  0.5
train loss:  0.7239422798156738
train gradient:  0.7036993591593719
iteration : 95
train acc:  0.703125
train loss:  0.6156527996063232
train gradient:  0.44711691763737654
iteration : 96
train acc:  0.6953125
train loss:  0.6195169687271118
train gradient:  0.23924138404956014
iteration : 97
train acc:  0.609375
train loss:  0.667582631111145
train gradient:  0.6295940042998632
iteration : 98
train acc:  0.6328125
train loss:  0.6628503799438477
train gradient:  0.49184441964484965
iteration : 99
train acc:  0.6640625
train loss:  0.6119468212127686
train gradient:  0.31509842762617635
iteration : 100
train acc:  0.59375
train loss:  0.6650151610374451
train gradient:  0.6095029760049118
iteration : 101
train acc:  0.5625
train loss:  0.6977195739746094
train gradient:  0.4842061483781471
iteration : 102
train acc:  0.609375
train loss:  0.6857574582099915
train gradient:  0.6220335952800534
iteration : 103
train acc:  0.609375
train loss:  0.6516169309616089
train gradient:  0.8888537394837465
iteration : 104
train acc:  0.6171875
train loss:  0.6419537663459778
train gradient:  0.5683880435880662
iteration : 105
train acc:  0.609375
train loss:  0.6638962030410767
train gradient:  0.37861611861468064
iteration : 106
train acc:  0.703125
train loss:  0.6216123104095459
train gradient:  0.6877783020690621
iteration : 107
train acc:  0.6171875
train loss:  0.6273404359817505
train gradient:  0.2601246114095393
iteration : 108
train acc:  0.578125
train loss:  0.6863198280334473
train gradient:  0.5559206814095612
iteration : 109
train acc:  0.609375
train loss:  0.6477571725845337
train gradient:  0.38026722041630556
iteration : 110
train acc:  0.640625
train loss:  0.6278479695320129
train gradient:  0.3795162254015236
iteration : 111
train acc:  0.6796875
train loss:  0.6434086561203003
train gradient:  0.35014710531795934
iteration : 112
train acc:  0.6875
train loss:  0.6020299196243286
train gradient:  0.25533925315512607
iteration : 113
train acc:  0.640625
train loss:  0.6187176704406738
train gradient:  0.2616656004344567
iteration : 114
train acc:  0.6171875
train loss:  0.6744186878204346
train gradient:  0.9519579558324196
iteration : 115
train acc:  0.546875
train loss:  0.67964768409729
train gradient:  0.5165673192739164
iteration : 116
train acc:  0.671875
train loss:  0.6439918279647827
train gradient:  0.9735619346744835
iteration : 117
train acc:  0.640625
train loss:  0.6323512196540833
train gradient:  0.2526153776887355
iteration : 118
train acc:  0.65625
train loss:  0.6369109749794006
train gradient:  0.46068166291679025
iteration : 119
train acc:  0.6640625
train loss:  0.6161950826644897
train gradient:  0.27963961786149955
iteration : 120
train acc:  0.625
train loss:  0.6577932834625244
train gradient:  0.4227536830363024
iteration : 121
train acc:  0.546875
train loss:  0.7078714966773987
train gradient:  0.3646969043265817
iteration : 122
train acc:  0.625
train loss:  0.6352826356887817
train gradient:  0.35216493268120835
iteration : 123
train acc:  0.5234375
train loss:  0.7021087408065796
train gradient:  0.4285367195835746
iteration : 124
train acc:  0.59375
train loss:  0.6782330870628357
train gradient:  0.411197095422375
iteration : 125
train acc:  0.625
train loss:  0.6190433502197266
train gradient:  0.4116135253661211
iteration : 126
train acc:  0.6484375
train loss:  0.6428381204605103
train gradient:  0.41290665496707407
iteration : 127
train acc:  0.5703125
train loss:  0.6672033071517944
train gradient:  0.40236156723413646
iteration : 128
train acc:  0.640625
train loss:  0.6525105237960815
train gradient:  0.46773767732446786
iteration : 129
train acc:  0.625
train loss:  0.6346690654754639
train gradient:  0.523366743326006
iteration : 130
train acc:  0.65625
train loss:  0.6664825677871704
train gradient:  0.6312579685613988
iteration : 131
train acc:  0.6796875
train loss:  0.6255784034729004
train gradient:  0.32938706306313137
iteration : 132
train acc:  0.65625
train loss:  0.5987817049026489
train gradient:  0.2779789102182685
iteration : 133
train acc:  0.703125
train loss:  0.6138008832931519
train gradient:  0.356202869192543
iteration : 134
train acc:  0.6484375
train loss:  0.6400035619735718
train gradient:  0.4891056749689126
iteration : 135
train acc:  0.59375
train loss:  0.6508493423461914
train gradient:  0.2978925945423011
iteration : 136
train acc:  0.5390625
train loss:  0.6638368368148804
train gradient:  0.23655695797371773
iteration : 137
train acc:  0.65625
train loss:  0.601658821105957
train gradient:  0.33605523056708814
iteration : 138
train acc:  0.6484375
train loss:  0.6309770345687866
train gradient:  0.17908292914689392
iteration : 139
train acc:  0.4921875
train loss:  0.7069495916366577
train gradient:  0.6101571564195887
iteration : 140
train acc:  0.640625
train loss:  0.6329642534255981
train gradient:  0.38439072252653744
iteration : 141
train acc:  0.6875
train loss:  0.6086896061897278
train gradient:  0.3036888954888287
iteration : 142
train acc:  0.7265625
train loss:  0.5991960763931274
train gradient:  0.2679278116585766
iteration : 143
train acc:  0.6875
train loss:  0.5930907726287842
train gradient:  0.38545364868377024
iteration : 144
train acc:  0.5859375
train loss:  0.6710184216499329
train gradient:  0.26197227774302756
iteration : 145
train acc:  0.65625
train loss:  0.6081878542900085
train gradient:  0.2370956111130857
iteration : 146
train acc:  0.6796875
train loss:  0.6093112230300903
train gradient:  0.307123731609834
iteration : 147
train acc:  0.6171875
train loss:  0.6318486928939819
train gradient:  0.2261098903917057
iteration : 148
train acc:  0.703125
train loss:  0.6513835191726685
train gradient:  0.41051855749753735
iteration : 149
train acc:  0.703125
train loss:  0.5613971948623657
train gradient:  0.2625610018724263
iteration : 150
train acc:  0.6796875
train loss:  0.5825047492980957
train gradient:  0.2782139353479964
iteration : 151
train acc:  0.625
train loss:  0.6171504855155945
train gradient:  0.27566111483798483
iteration : 152
train acc:  0.703125
train loss:  0.5780885219573975
train gradient:  0.30517032098812147
iteration : 153
train acc:  0.6171875
train loss:  0.6141744256019592
train gradient:  0.3791390685430129
iteration : 154
train acc:  0.6953125
train loss:  0.5993279218673706
train gradient:  0.2880755940790807
iteration : 155
train acc:  0.71875
train loss:  0.5734592080116272
train gradient:  0.34167050003576216
iteration : 156
train acc:  0.6640625
train loss:  0.6219083070755005
train gradient:  0.41238900922616384
iteration : 157
train acc:  0.71875
train loss:  0.5744532346725464
train gradient:  0.25395851466211483
iteration : 158
train acc:  0.6640625
train loss:  0.5819274187088013
train gradient:  0.28526323584850294
iteration : 159
train acc:  0.671875
train loss:  0.659321129322052
train gradient:  0.9348165626583607
iteration : 160
train acc:  0.65625
train loss:  0.6314687728881836
train gradient:  0.3456260686369908
iteration : 161
train acc:  0.609375
train loss:  0.6532226800918579
train gradient:  0.7425998421143101
iteration : 162
train acc:  0.6796875
train loss:  0.5929683446884155
train gradient:  0.6483326129376795
iteration : 163
train acc:  0.625
train loss:  0.6560888290405273
train gradient:  0.43576198357664636
iteration : 164
train acc:  0.65625
train loss:  0.6092812418937683
train gradient:  0.32144827725323777
iteration : 165
train acc:  0.625
train loss:  0.6359162330627441
train gradient:  0.8525514344485846
iteration : 166
train acc:  0.7265625
train loss:  0.5348145961761475
train gradient:  0.24932481127449743
iteration : 167
train acc:  0.671875
train loss:  0.5761515498161316
train gradient:  0.29781112637157414
iteration : 168
train acc:  0.6328125
train loss:  0.6190296411514282
train gradient:  0.29075499718250053
iteration : 169
train acc:  0.6796875
train loss:  0.6165868043899536
train gradient:  0.3571260489467054
iteration : 170
train acc:  0.59375
train loss:  0.6680642366409302
train gradient:  0.5066240199602703
iteration : 171
train acc:  0.6953125
train loss:  0.5813882946968079
train gradient:  0.3715100204701844
iteration : 172
train acc:  0.703125
train loss:  0.5599315166473389
train gradient:  0.3348190620264323
iteration : 173
train acc:  0.546875
train loss:  0.700713574886322
train gradient:  0.3524316135174232
iteration : 174
train acc:  0.6640625
train loss:  0.6092917323112488
train gradient:  0.3865623116421525
iteration : 175
train acc:  0.71875
train loss:  0.5576168298721313
train gradient:  1.2386763887736296
iteration : 176
train acc:  0.7265625
train loss:  0.5508578419685364
train gradient:  0.30530566787072466
iteration : 177
train acc:  0.640625
train loss:  0.6309741735458374
train gradient:  0.31296515422068977
iteration : 178
train acc:  0.6796875
train loss:  0.649980366230011
train gradient:  0.8064085131471961
iteration : 179
train acc:  0.6796875
train loss:  0.5496978759765625
train gradient:  0.29949570415545934
iteration : 180
train acc:  0.6640625
train loss:  0.6346896886825562
train gradient:  0.5700274699834205
iteration : 181
train acc:  0.6484375
train loss:  0.6452508568763733
train gradient:  0.7039985494802178
iteration : 182
train acc:  0.734375
train loss:  0.5764780640602112
train gradient:  0.4460318041552537
iteration : 183
train acc:  0.65625
train loss:  0.6007750034332275
train gradient:  0.484964219343203
iteration : 184
train acc:  0.640625
train loss:  0.6511101722717285
train gradient:  0.2605123252421679
iteration : 185
train acc:  0.6953125
train loss:  0.5626639127731323
train gradient:  0.25248829173357457
iteration : 186
train acc:  0.6484375
train loss:  0.6928918957710266
train gradient:  0.8045329162232941
iteration : 187
train acc:  0.703125
train loss:  0.6197324991226196
train gradient:  0.40801864232582036
iteration : 188
train acc:  0.671875
train loss:  0.6038237810134888
train gradient:  0.827650868665058
iteration : 189
train acc:  0.6875
train loss:  0.591702401638031
train gradient:  0.6771876858287762
iteration : 190
train acc:  0.625
train loss:  0.6303740739822388
train gradient:  0.3996218568628966
iteration : 191
train acc:  0.65625
train loss:  0.6209178566932678
train gradient:  0.7266690042653952
iteration : 192
train acc:  0.640625
train loss:  0.6238313913345337
train gradient:  0.5106780058638344
iteration : 193
train acc:  0.5546875
train loss:  0.6748829483985901
train gradient:  0.674521303230783
iteration : 194
train acc:  0.546875
train loss:  0.6704031229019165
train gradient:  0.5515894142561633
iteration : 195
train acc:  0.59375
train loss:  0.6586042642593384
train gradient:  0.5735687800791465
iteration : 196
train acc:  0.6640625
train loss:  0.5740500688552856
train gradient:  0.26457194709281656
iteration : 197
train acc:  0.6328125
train loss:  0.6234019994735718
train gradient:  0.6154388353539326
iteration : 198
train acc:  0.5703125
train loss:  0.6557013392448425
train gradient:  0.6117383624662748
iteration : 199
train acc:  0.6328125
train loss:  0.6456150412559509
train gradient:  0.37627720662768416
iteration : 200
train acc:  0.640625
train loss:  0.6426689028739929
train gradient:  0.44850531048991943
iteration : 201
train acc:  0.65625
train loss:  0.604182779788971
train gradient:  0.39873923342197537
iteration : 202
train acc:  0.609375
train loss:  0.6614473462104797
train gradient:  0.6380849715729258
iteration : 203
train acc:  0.6171875
train loss:  0.6178421974182129
train gradient:  0.9763921419267128
iteration : 204
train acc:  0.5703125
train loss:  0.6771600246429443
train gradient:  0.3828111838373568
iteration : 205
train acc:  0.640625
train loss:  0.5968666076660156
train gradient:  0.3771433558510542
iteration : 206
train acc:  0.65625
train loss:  0.6059496402740479
train gradient:  0.2690580611816763
iteration : 207
train acc:  0.6171875
train loss:  0.6345393657684326
train gradient:  0.4774782515269912
iteration : 208
train acc:  0.640625
train loss:  0.6371526718139648
train gradient:  0.258336559237491
iteration : 209
train acc:  0.6953125
train loss:  0.5677806735038757
train gradient:  0.2920854741579441
iteration : 210
train acc:  0.671875
train loss:  0.6302560567855835
train gradient:  0.34618519041278045
iteration : 211
train acc:  0.6484375
train loss:  0.6205815672874451
train gradient:  0.4777512737647867
iteration : 212
train acc:  0.65625
train loss:  0.6246914863586426
train gradient:  0.4051016781309531
iteration : 213
train acc:  0.640625
train loss:  0.6068612933158875
train gradient:  0.3797176171071734
iteration : 214
train acc:  0.59375
train loss:  0.6509408950805664
train gradient:  0.31453545492079915
iteration : 215
train acc:  0.6796875
train loss:  0.586571216583252
train gradient:  0.2061168122041978
iteration : 216
train acc:  0.6328125
train loss:  0.6354637145996094
train gradient:  0.22560740849215272
iteration : 217
train acc:  0.59375
train loss:  0.6164271235466003
train gradient:  0.3577483164924911
iteration : 218
train acc:  0.71875
train loss:  0.556447446346283
train gradient:  0.2348355855005574
iteration : 219
train acc:  0.65625
train loss:  0.5917150378227234
train gradient:  0.49358323125953113
iteration : 220
train acc:  0.640625
train loss:  0.6589272022247314
train gradient:  0.33190694502933654
iteration : 221
train acc:  0.5859375
train loss:  0.6522700786590576
train gradient:  0.37239695744751244
iteration : 222
train acc:  0.6796875
train loss:  0.5651783347129822
train gradient:  0.20478307990838662
iteration : 223
train acc:  0.6328125
train loss:  0.640822172164917
train gradient:  0.2665826898794397
iteration : 224
train acc:  0.6171875
train loss:  0.6790199279785156
train gradient:  0.6134950368055145
iteration : 225
train acc:  0.6796875
train loss:  0.55659419298172
train gradient:  0.21994846778708588
iteration : 226
train acc:  0.6796875
train loss:  0.6330735087394714
train gradient:  0.41618137401395444
iteration : 227
train acc:  0.578125
train loss:  0.6426052451133728
train gradient:  0.3705467387192384
iteration : 228
train acc:  0.59375
train loss:  0.6451461315155029
train gradient:  0.6878338076785122
iteration : 229
train acc:  0.671875
train loss:  0.6058589220046997
train gradient:  0.5750840197327722
iteration : 230
train acc:  0.6171875
train loss:  0.6606082916259766
train gradient:  0.7147252793292558
iteration : 231
train acc:  0.6640625
train loss:  0.5904743671417236
train gradient:  0.266099704297317
iteration : 232
train acc:  0.6484375
train loss:  0.6199780702590942
train gradient:  0.28829162187570073
iteration : 233
train acc:  0.625
train loss:  0.6603090763092041
train gradient:  0.3049704140984603
iteration : 234
train acc:  0.6953125
train loss:  0.5747568011283875
train gradient:  0.2127042933319891
iteration : 235
train acc:  0.6640625
train loss:  0.6119008660316467
train gradient:  0.33464318615345534
iteration : 236
train acc:  0.7109375
train loss:  0.5735952258110046
train gradient:  0.22670713227139055
iteration : 237
train acc:  0.7265625
train loss:  0.5406668782234192
train gradient:  0.2786114460696805
iteration : 238
train acc:  0.640625
train loss:  0.6237202882766724
train gradient:  0.36000172397124225
iteration : 239
train acc:  0.640625
train loss:  0.6120702028274536
train gradient:  0.2949191559930288
iteration : 240
train acc:  0.640625
train loss:  0.6176783442497253
train gradient:  0.320448096440128
iteration : 241
train acc:  0.6015625
train loss:  0.6100969910621643
train gradient:  0.3334895169856525
iteration : 242
train acc:  0.7421875
train loss:  0.5304151773452759
train gradient:  0.2614703029316512
iteration : 243
train acc:  0.6640625
train loss:  0.5999060273170471
train gradient:  0.4101156561050327
iteration : 244
train acc:  0.6171875
train loss:  0.6254256963729858
train gradient:  0.2789713886037923
iteration : 245
train acc:  0.6484375
train loss:  0.6089596748352051
train gradient:  0.21722159305460045
iteration : 246
train acc:  0.65625
train loss:  0.6243571639060974
train gradient:  0.20500052837604243
iteration : 247
train acc:  0.7109375
train loss:  0.5887559056282043
train gradient:  0.3584854810819911
iteration : 248
train acc:  0.703125
train loss:  0.5949076414108276
train gradient:  0.28070796325790287
iteration : 249
train acc:  0.640625
train loss:  0.603321373462677
train gradient:  0.19023026303134943
iteration : 250
train acc:  0.6953125
train loss:  0.5679593682289124
train gradient:  0.2536031428747392
iteration : 251
train acc:  0.65625
train loss:  0.5933454036712646
train gradient:  0.21588410447017803
iteration : 252
train acc:  0.6484375
train loss:  0.5826171040534973
train gradient:  0.26968429031146623
iteration : 253
train acc:  0.765625
train loss:  0.5637160539627075
train gradient:  0.9879846354951956
iteration : 254
train acc:  0.625
train loss:  0.665207028388977
train gradient:  0.4314417228489008
iteration : 255
train acc:  0.7109375
train loss:  0.5681899785995483
train gradient:  0.2713236926727811
iteration : 256
train acc:  0.625
train loss:  0.5913659930229187
train gradient:  0.19578201706368412
iteration : 257
train acc:  0.6796875
train loss:  0.5929164886474609
train gradient:  0.3255108826333237
iteration : 258
train acc:  0.6875
train loss:  0.6362457275390625
train gradient:  0.44614434806143455
iteration : 259
train acc:  0.6171875
train loss:  0.6783812046051025
train gradient:  0.5716830657606196
iteration : 260
train acc:  0.6875
train loss:  0.5518651008605957
train gradient:  0.2512864461244861
iteration : 261
train acc:  0.625
train loss:  0.6590871810913086
train gradient:  0.5438806364244171
iteration : 262
train acc:  0.6875
train loss:  0.5717967748641968
train gradient:  0.276713703638431
iteration : 263
train acc:  0.671875
train loss:  0.5828378200531006
train gradient:  0.33377866042400445
iteration : 264
train acc:  0.609375
train loss:  0.635591447353363
train gradient:  0.31533295596417954
iteration : 265
train acc:  0.6796875
train loss:  0.5966591835021973
train gradient:  0.32393997084891074
iteration : 266
train acc:  0.6171875
train loss:  0.5921279191970825
train gradient:  0.2997876839652382
iteration : 267
train acc:  0.734375
train loss:  0.5656222701072693
train gradient:  0.25939570650292826
iteration : 268
train acc:  0.6953125
train loss:  0.5781158208847046
train gradient:  0.27269075562637957
iteration : 269
train acc:  0.6796875
train loss:  0.5854647159576416
train gradient:  0.49438675817429073
iteration : 270
train acc:  0.609375
train loss:  0.6781301498413086
train gradient:  1.2566268264049452
iteration : 271
train acc:  0.6484375
train loss:  0.6678093075752258
train gradient:  0.4737641344128751
iteration : 272
train acc:  0.625
train loss:  0.6577655076980591
train gradient:  0.6032303815034151
iteration : 273
train acc:  0.65625
train loss:  0.6424872279167175
train gradient:  0.43319528443849914
iteration : 274
train acc:  0.6015625
train loss:  0.6469777822494507
train gradient:  0.7191782112180973
iteration : 275
train acc:  0.6640625
train loss:  0.6097714900970459
train gradient:  0.537850360568894
iteration : 276
train acc:  0.6875
train loss:  0.550761342048645
train gradient:  0.43894481002746766
iteration : 277
train acc:  0.6640625
train loss:  0.592546820640564
train gradient:  0.2014976939428814
iteration : 278
train acc:  0.6875
train loss:  0.5423319935798645
train gradient:  0.42429370754524304
iteration : 279
train acc:  0.6015625
train loss:  0.6721591949462891
train gradient:  0.6471677071886129
iteration : 280
train acc:  0.6953125
train loss:  0.59096360206604
train gradient:  0.3917789684589445
iteration : 281
train acc:  0.7421875
train loss:  0.6156232953071594
train gradient:  0.34610600854775236
iteration : 282
train acc:  0.703125
train loss:  0.5702934265136719
train gradient:  0.31747418092445234
iteration : 283
train acc:  0.6328125
train loss:  0.6257314682006836
train gradient:  0.5753543499565442
iteration : 284
train acc:  0.6484375
train loss:  0.6197465658187866
train gradient:  0.2626727506041004
iteration : 285
train acc:  0.671875
train loss:  0.6106808185577393
train gradient:  0.9981386962162447
iteration : 286
train acc:  0.71875
train loss:  0.5584082007408142
train gradient:  0.357721377732915
iteration : 287
train acc:  0.6796875
train loss:  0.60850590467453
train gradient:  0.3857196553853738
iteration : 288
train acc:  0.6796875
train loss:  0.6187732219696045
train gradient:  0.2940216019320022
iteration : 289
train acc:  0.6640625
train loss:  0.6235542893409729
train gradient:  0.3940070098487766
iteration : 290
train acc:  0.65625
train loss:  0.6255916357040405
train gradient:  0.5075388581659104
iteration : 291
train acc:  0.6328125
train loss:  0.606200098991394
train gradient:  0.44310365521766865
iteration : 292
train acc:  0.609375
train loss:  0.624951958656311
train gradient:  0.37348763845942623
iteration : 293
train acc:  0.6796875
train loss:  0.601959228515625
train gradient:  0.4074576217720264
iteration : 294
train acc:  0.6015625
train loss:  0.6562480926513672
train gradient:  0.5316618212791948
iteration : 295
train acc:  0.6875
train loss:  0.5782119035720825
train gradient:  0.31687399163060653
iteration : 296
train acc:  0.625
train loss:  0.6153886914253235
train gradient:  0.29022496628144806
iteration : 297
train acc:  0.671875
train loss:  0.5842818021774292
train gradient:  0.27534025221734826
iteration : 298
train acc:  0.6796875
train loss:  0.6021585464477539
train gradient:  0.23424707687645152
iteration : 299
train acc:  0.7265625
train loss:  0.5557822585105896
train gradient:  0.31625508785251333
iteration : 300
train acc:  0.6640625
train loss:  0.6386945247650146
train gradient:  0.38101164214883604
iteration : 301
train acc:  0.6328125
train loss:  0.6359858512878418
train gradient:  0.42211948309999486
iteration : 302
train acc:  0.671875
train loss:  0.6104354858398438
train gradient:  0.32062530319389704
iteration : 303
train acc:  0.5703125
train loss:  0.6879534721374512
train gradient:  0.5696459065676613
iteration : 304
train acc:  0.6484375
train loss:  0.6248181462287903
train gradient:  0.22211740549534642
iteration : 305
train acc:  0.7578125
train loss:  0.5307676792144775
train gradient:  0.2339667710512402
iteration : 306
train acc:  0.71875
train loss:  0.5647817850112915
train gradient:  0.3582110015019029
iteration : 307
train acc:  0.6015625
train loss:  0.6213616132736206
train gradient:  0.28559816771100033
iteration : 308
train acc:  0.609375
train loss:  0.6388944983482361
train gradient:  0.3665862849291302
iteration : 309
train acc:  0.6796875
train loss:  0.5809975862503052
train gradient:  0.3181157002353383
iteration : 310
train acc:  0.671875
train loss:  0.6013398170471191
train gradient:  0.17713345001446595
iteration : 311
train acc:  0.6484375
train loss:  0.6121777296066284
train gradient:  0.26005652313002064
iteration : 312
train acc:  0.71875
train loss:  0.5614241361618042
train gradient:  0.23340955356058812
iteration : 313
train acc:  0.6796875
train loss:  0.580951452255249
train gradient:  0.21925903010725634
iteration : 314
train acc:  0.6640625
train loss:  0.606261134147644
train gradient:  0.26053856766389166
iteration : 315
train acc:  0.6484375
train loss:  0.6009964942932129
train gradient:  0.4625386568113721
iteration : 316
train acc:  0.6015625
train loss:  0.6313704252243042
train gradient:  0.25073131939981824
iteration : 317
train acc:  0.6328125
train loss:  0.6195545196533203
train gradient:  0.36216711300624044
iteration : 318
train acc:  0.6484375
train loss:  0.6179009079933167
train gradient:  0.2775174135345725
iteration : 319
train acc:  0.6953125
train loss:  0.5732400417327881
train gradient:  0.21215114335704455
iteration : 320
train acc:  0.6875
train loss:  0.5942705869674683
train gradient:  0.611936324007879
iteration : 321
train acc:  0.6640625
train loss:  0.59079909324646
train gradient:  0.3106399599355152
iteration : 322
train acc:  0.7109375
train loss:  0.5713115930557251
train gradient:  0.2632902993343636
iteration : 323
train acc:  0.6640625
train loss:  0.6191320419311523
train gradient:  0.4611812089841623
iteration : 324
train acc:  0.6875
train loss:  0.5913553833961487
train gradient:  0.40657769120075105
iteration : 325
train acc:  0.6953125
train loss:  0.6086007356643677
train gradient:  0.3952248419118955
iteration : 326
train acc:  0.75
train loss:  0.5559173822402954
train gradient:  0.29175547124505274
iteration : 327
train acc:  0.6953125
train loss:  0.6329401731491089
train gradient:  0.3205569413793906
iteration : 328
train acc:  0.6640625
train loss:  0.6145483255386353
train gradient:  0.4459563010302862
iteration : 329
train acc:  0.765625
train loss:  0.5625523328781128
train gradient:  0.7256310723748051
iteration : 330
train acc:  0.6484375
train loss:  0.6260262727737427
train gradient:  0.36043623970957506
iteration : 331
train acc:  0.640625
train loss:  0.6119475364685059
train gradient:  0.5974956516615308
iteration : 332
train acc:  0.6953125
train loss:  0.5796542167663574
train gradient:  0.39545257129561956
iteration : 333
train acc:  0.6640625
train loss:  0.5828343629837036
train gradient:  0.47171618748020266
iteration : 334
train acc:  0.7109375
train loss:  0.5574084520339966
train gradient:  0.3408863075081862
iteration : 335
train acc:  0.7109375
train loss:  0.5652156472206116
train gradient:  0.34934688796453905
iteration : 336
train acc:  0.640625
train loss:  0.6467788219451904
train gradient:  0.49397478643551285
iteration : 337
train acc:  0.6640625
train loss:  0.5946792364120483
train gradient:  0.42507673044623984
iteration : 338
train acc:  0.6796875
train loss:  0.5903434753417969
train gradient:  0.36954612499693396
iteration : 339
train acc:  0.765625
train loss:  0.5164549350738525
train gradient:  0.4155853466319992
iteration : 340
train acc:  0.703125
train loss:  0.5751264691352844
train gradient:  0.5737662577909313
iteration : 341
train acc:  0.6875
train loss:  0.5894800424575806
train gradient:  0.34645891462890693
iteration : 342
train acc:  0.6953125
train loss:  0.5999635457992554
train gradient:  0.34116215591210397
iteration : 343
train acc:  0.6640625
train loss:  0.6027365922927856
train gradient:  0.4148118460437923
iteration : 344
train acc:  0.625
train loss:  0.6148468852043152
train gradient:  0.3711487188536922
iteration : 345
train acc:  0.65625
train loss:  0.610388994216919
train gradient:  0.32555406110479373
iteration : 346
train acc:  0.640625
train loss:  0.6134436726570129
train gradient:  0.396010058325518
iteration : 347
train acc:  0.734375
train loss:  0.5696513652801514
train gradient:  0.2659154764437205
iteration : 348
train acc:  0.7265625
train loss:  0.5690423250198364
train gradient:  0.3890128515829635
iteration : 349
train acc:  0.734375
train loss:  0.5546377301216125
train gradient:  0.29210752018533415
iteration : 350
train acc:  0.6875
train loss:  0.587761402130127
train gradient:  0.4225306540184821
iteration : 351
train acc:  0.6796875
train loss:  0.5420961976051331
train gradient:  0.2655136450944325
iteration : 352
train acc:  0.703125
train loss:  0.6074822545051575
train gradient:  0.5264739304740331
iteration : 353
train acc:  0.640625
train loss:  0.5897148847579956
train gradient:  0.4216724341955991
iteration : 354
train acc:  0.671875
train loss:  0.5910748839378357
train gradient:  0.2810004040524856
iteration : 355
train acc:  0.7421875
train loss:  0.49901720881462097
train gradient:  0.3184460208100017
iteration : 356
train acc:  0.6640625
train loss:  0.5646836757659912
train gradient:  0.4163486448319548
iteration : 357
train acc:  0.671875
train loss:  0.6055757999420166
train gradient:  0.37233626836334516
iteration : 358
train acc:  0.609375
train loss:  0.6120067834854126
train gradient:  0.3734954957460124
iteration : 359
train acc:  0.71875
train loss:  0.6098774075508118
train gradient:  0.3881075599207713
iteration : 360
train acc:  0.7421875
train loss:  0.5417096614837646
train gradient:  0.41133301482810425
iteration : 361
train acc:  0.6875
train loss:  0.5967177748680115
train gradient:  0.4004759184365335
iteration : 362
train acc:  0.59375
train loss:  0.631859540939331
train gradient:  0.34258487297111406
iteration : 363
train acc:  0.71875
train loss:  0.5897235870361328
train gradient:  0.3085217480717065
iteration : 364
train acc:  0.7109375
train loss:  0.5561498403549194
train gradient:  0.2094738159157492
iteration : 365
train acc:  0.7265625
train loss:  0.5589861869812012
train gradient:  0.4650430931003966
iteration : 366
train acc:  0.71875
train loss:  0.5527578592300415
train gradient:  0.41240574073167424
iteration : 367
train acc:  0.703125
train loss:  0.5572217702865601
train gradient:  0.3075014630343185
iteration : 368
train acc:  0.7265625
train loss:  0.5237339735031128
train gradient:  0.34325876775392217
iteration : 369
train acc:  0.671875
train loss:  0.5902481079101562
train gradient:  0.3617281686399391
iteration : 370
train acc:  0.71875
train loss:  0.5763759613037109
train gradient:  0.3523669079396717
iteration : 371
train acc:  0.65625
train loss:  0.5658928155899048
train gradient:  0.4524510033901032
iteration : 372
train acc:  0.6640625
train loss:  0.6095993518829346
train gradient:  0.5951172986958687
iteration : 373
train acc:  0.6484375
train loss:  0.5994207859039307
train gradient:  0.35548276335013435
iteration : 374
train acc:  0.6640625
train loss:  0.5835022926330566
train gradient:  0.301923897838081
iteration : 375
train acc:  0.6640625
train loss:  0.6173555850982666
train gradient:  0.343037221179385
iteration : 376
train acc:  0.640625
train loss:  0.5918807983398438
train gradient:  0.40570453815546853
iteration : 377
train acc:  0.625
train loss:  0.6710456609725952
train gradient:  0.7697732596086644
iteration : 378
train acc:  0.6796875
train loss:  0.5970268249511719
train gradient:  0.3192326491223032
iteration : 379
train acc:  0.7265625
train loss:  0.541297197341919
train gradient:  0.4008234063796468
iteration : 380
train acc:  0.703125
train loss:  0.5848300457000732
train gradient:  0.29857721476056504
iteration : 381
train acc:  0.7109375
train loss:  0.541683554649353
train gradient:  0.32938829269217984
iteration : 382
train acc:  0.7109375
train loss:  0.5525290966033936
train gradient:  0.31032604408069414
iteration : 383
train acc:  0.6328125
train loss:  0.6859135031700134
train gradient:  0.8147448550874807
iteration : 384
train acc:  0.765625
train loss:  0.5609515309333801
train gradient:  0.29988351688575116
iteration : 385
train acc:  0.7421875
train loss:  0.5354136228561401
train gradient:  0.3830118792392183
iteration : 386
train acc:  0.7109375
train loss:  0.5042037963867188
train gradient:  0.35580203407741334
iteration : 387
train acc:  0.734375
train loss:  0.5377427935600281
train gradient:  0.42869114537225705
iteration : 388
train acc:  0.65625
train loss:  0.6393885612487793
train gradient:  0.6894649930429885
iteration : 389
train acc:  0.6953125
train loss:  0.6199794411659241
train gradient:  0.6262709310975907
iteration : 390
train acc:  0.6640625
train loss:  0.5923902988433838
train gradient:  0.6933418746712734
iteration : 391
train acc:  0.7109375
train loss:  0.5670301914215088
train gradient:  0.651614124231081
iteration : 392
train acc:  0.6953125
train loss:  0.6101781725883484
train gradient:  0.5163600055453591
iteration : 393
train acc:  0.671875
train loss:  0.6000664830207825
train gradient:  0.4655786934039377
iteration : 394
train acc:  0.6328125
train loss:  0.6329615712165833
train gradient:  0.4607234603745103
iteration : 395
train acc:  0.625
train loss:  0.6176045536994934
train gradient:  0.4289413224840316
iteration : 396
train acc:  0.7109375
train loss:  0.5634824633598328
train gradient:  0.3639869360455486
iteration : 397
train acc:  0.734375
train loss:  0.544528603553772
train gradient:  0.49242063686188886
iteration : 398
train acc:  0.7421875
train loss:  0.5066609978675842
train gradient:  0.32974233518774354
iteration : 399
train acc:  0.640625
train loss:  0.5830023884773254
train gradient:  0.34032775646869806
iteration : 400
train acc:  0.6328125
train loss:  0.602086067199707
train gradient:  0.47762168268673993
iteration : 401
train acc:  0.6953125
train loss:  0.5904878973960876
train gradient:  0.504668083297151
iteration : 402
train acc:  0.6640625
train loss:  0.5736086964607239
train gradient:  0.2619027784935007
iteration : 403
train acc:  0.625
train loss:  0.6218731999397278
train gradient:  0.551848284073797
iteration : 404
train acc:  0.71875
train loss:  0.5392798185348511
train gradient:  0.3286666785131573
iteration : 405
train acc:  0.609375
train loss:  0.606647789478302
train gradient:  0.4692104023662191
iteration : 406
train acc:  0.6328125
train loss:  0.618559718132019
train gradient:  0.553571420950035
iteration : 407
train acc:  0.65625
train loss:  0.6009777188301086
train gradient:  0.4381045616279081
iteration : 408
train acc:  0.640625
train loss:  0.5847678184509277
train gradient:  0.3155692008011064
iteration : 409
train acc:  0.71875
train loss:  0.5744941234588623
train gradient:  1.0986611898198784
iteration : 410
train acc:  0.734375
train loss:  0.5563076734542847
train gradient:  0.4353854719348471
iteration : 411
train acc:  0.65625
train loss:  0.5620640516281128
train gradient:  0.5544724912821053
iteration : 412
train acc:  0.703125
train loss:  0.5606877207756042
train gradient:  0.33513265092029876
iteration : 413
train acc:  0.65625
train loss:  0.5902414917945862
train gradient:  0.5270728309978046
iteration : 414
train acc:  0.671875
train loss:  0.6194406747817993
train gradient:  0.5753814418122508
iteration : 415
train acc:  0.6875
train loss:  0.5757994651794434
train gradient:  0.45010827088538974
iteration : 416
train acc:  0.71875
train loss:  0.5557649731636047
train gradient:  0.5702655881398542
iteration : 417
train acc:  0.8203125
train loss:  0.5163607001304626
train gradient:  0.381273543251284
iteration : 418
train acc:  0.6875
train loss:  0.5783358812332153
train gradient:  0.5843217988458882
iteration : 419
train acc:  0.765625
train loss:  0.5557410717010498
train gradient:  0.5872862537788998
iteration : 420
train acc:  0.734375
train loss:  0.5572304725646973
train gradient:  0.3556167852215907
iteration : 421
train acc:  0.75
train loss:  0.5353268384933472
train gradient:  0.44605784401644066
iteration : 422
train acc:  0.7421875
train loss:  0.5376574993133545
train gradient:  0.4233908524588711
iteration : 423
train acc:  0.7265625
train loss:  0.5747880935668945
train gradient:  0.5113000520538482
iteration : 424
train acc:  0.71875
train loss:  0.5324693918228149
train gradient:  0.5031529327818397
iteration : 425
train acc:  0.71875
train loss:  0.5485159158706665
train gradient:  0.5302260784530519
iteration : 426
train acc:  0.7421875
train loss:  0.5331459045410156
train gradient:  0.36104766854733633
iteration : 427
train acc:  0.640625
train loss:  0.5965486764907837
train gradient:  0.5278007378596943
iteration : 428
train acc:  0.7109375
train loss:  0.5319079160690308
train gradient:  0.3598328088636524
iteration : 429
train acc:  0.6875
train loss:  0.5486772060394287
train gradient:  0.4037137920822799
iteration : 430
train acc:  0.6640625
train loss:  0.6036620736122131
train gradient:  0.4207594628665762
iteration : 431
train acc:  0.7265625
train loss:  0.5712605714797974
train gradient:  0.33090282747005956
iteration : 432
train acc:  0.71875
train loss:  0.5308727025985718
train gradient:  0.5003327188447854
iteration : 433
train acc:  0.6875
train loss:  0.6070035696029663
train gradient:  0.42177756838033564
iteration : 434
train acc:  0.7109375
train loss:  0.5219658613204956
train gradient:  0.3815778145107254
iteration : 435
train acc:  0.7265625
train loss:  0.5421772599220276
train gradient:  0.4597458161526503
iteration : 436
train acc:  0.796875
train loss:  0.48960670828819275
train gradient:  0.3364784011001941
iteration : 437
train acc:  0.71875
train loss:  0.5175595879554749
train gradient:  0.41247349777985476
iteration : 438
train acc:  0.6640625
train loss:  0.6089274883270264
train gradient:  0.5310131548857728
iteration : 439
train acc:  0.6875
train loss:  0.548747181892395
train gradient:  0.35738741473075686
iteration : 440
train acc:  0.7421875
train loss:  0.5274877548217773
train gradient:  0.38208759732417097
iteration : 441
train acc:  0.7109375
train loss:  0.571648359298706
train gradient:  0.6823632808277414
iteration : 442
train acc:  0.7578125
train loss:  0.5175849199295044
train gradient:  0.4052450612744962
iteration : 443
train acc:  0.6875
train loss:  0.5688618421554565
train gradient:  0.5146750170921537
iteration : 444
train acc:  0.625
train loss:  0.656973123550415
train gradient:  0.7173183983613078
iteration : 445
train acc:  0.6953125
train loss:  0.5551728010177612
train gradient:  0.577891894689985
iteration : 446
train acc:  0.671875
train loss:  0.5991514325141907
train gradient:  0.4901787725692979
iteration : 447
train acc:  0.6640625
train loss:  0.5842219591140747
train gradient:  0.47065696623187164
iteration : 448
train acc:  0.671875
train loss:  0.5621407628059387
train gradient:  0.46849005730305476
iteration : 449
train acc:  0.6796875
train loss:  0.5743356943130493
train gradient:  0.42570159681427866
iteration : 450
train acc:  0.6953125
train loss:  0.5706473588943481
train gradient:  0.7275945636078073
iteration : 451
train acc:  0.6796875
train loss:  0.5780758261680603
train gradient:  0.3237555084515343
iteration : 452
train acc:  0.703125
train loss:  0.5521538257598877
train gradient:  0.5154567684002449
iteration : 453
train acc:  0.7265625
train loss:  0.5223686695098877
train gradient:  0.35888699448686084
iteration : 454
train acc:  0.6953125
train loss:  0.5871247053146362
train gradient:  0.5086211472037667
iteration : 455
train acc:  0.7578125
train loss:  0.5203475952148438
train gradient:  0.32971750396670174
iteration : 456
train acc:  0.7734375
train loss:  0.4760528802871704
train gradient:  0.3740938244492089
iteration : 457
train acc:  0.7421875
train loss:  0.5136287808418274
train gradient:  0.31842564287383296
iteration : 458
train acc:  0.6953125
train loss:  0.5804859399795532
train gradient:  0.3832598189732143
iteration : 459
train acc:  0.7578125
train loss:  0.5455002188682556
train gradient:  0.4653975400517587
iteration : 460
train acc:  0.7578125
train loss:  0.5237885117530823
train gradient:  0.4078656750196107
iteration : 461
train acc:  0.6875
train loss:  0.5810085535049438
train gradient:  0.7186678760039633
iteration : 462
train acc:  0.703125
train loss:  0.5400253534317017
train gradient:  0.3836947421169692
iteration : 463
train acc:  0.6796875
train loss:  0.5782045722007751
train gradient:  0.509320423216096
iteration : 464
train acc:  0.71875
train loss:  0.5035399198532104
train gradient:  0.31717564508669244
iteration : 465
train acc:  0.7421875
train loss:  0.5096582174301147
train gradient:  0.41353147380111194
iteration : 466
train acc:  0.71875
train loss:  0.5226962566375732
train gradient:  0.33366352081446105
iteration : 467
train acc:  0.7578125
train loss:  0.5357876420021057
train gradient:  0.4440168582286552
iteration : 468
train acc:  0.734375
train loss:  0.595116913318634
train gradient:  0.761998700779598
iteration : 469
train acc:  0.6796875
train loss:  0.6189436912536621
train gradient:  0.6136539079665393
iteration : 470
train acc:  0.7109375
train loss:  0.54746413230896
train gradient:  0.46557762698667793
iteration : 471
train acc:  0.671875
train loss:  0.5854771137237549
train gradient:  0.6583264974867278
iteration : 472
train acc:  0.7265625
train loss:  0.5192748308181763
train gradient:  0.4936639870273346
iteration : 473
train acc:  0.6875
train loss:  0.5507121086120605
train gradient:  0.518423291505858
iteration : 474
train acc:  0.765625
train loss:  0.4919723570346832
train gradient:  0.39545122352140205
iteration : 475
train acc:  0.71875
train loss:  0.5435763001441956
train gradient:  0.378220492724381
iteration : 476
train acc:  0.7265625
train loss:  0.5239884853363037
train gradient:  0.37845627481778366
iteration : 477
train acc:  0.75
train loss:  0.5261327028274536
train gradient:  0.36249393252063117
iteration : 478
train acc:  0.734375
train loss:  0.5557039976119995
train gradient:  0.4144749321465854
iteration : 479
train acc:  0.6875
train loss:  0.5534359812736511
train gradient:  0.5812710981062486
iteration : 480
train acc:  0.71875
train loss:  0.603649377822876
train gradient:  0.6922450947136635
iteration : 481
train acc:  0.7109375
train loss:  0.5621686577796936
train gradient:  0.534087887941485
iteration : 482
train acc:  0.7421875
train loss:  0.5040670037269592
train gradient:  0.32108327732640934
iteration : 483
train acc:  0.78125
train loss:  0.5320777893066406
train gradient:  0.5955668769831484
iteration : 484
train acc:  0.625
train loss:  0.6087954640388489
train gradient:  0.5282528619617421
iteration : 485
train acc:  0.7578125
train loss:  0.48117494583129883
train gradient:  0.3897671031802556
iteration : 486
train acc:  0.734375
train loss:  0.5052981972694397
train gradient:  0.47735829104064664
iteration : 487
train acc:  0.7578125
train loss:  0.5080437660217285
train gradient:  0.47502891995005325
iteration : 488
train acc:  0.71875
train loss:  0.5363504886627197
train gradient:  0.4099847313949382
iteration : 489
train acc:  0.765625
train loss:  0.5072826743125916
train gradient:  0.35659401224163617
iteration : 490
train acc:  0.8046875
train loss:  0.4769045412540436
train gradient:  0.4875108960946178
iteration : 491
train acc:  0.8046875
train loss:  0.45181652903556824
train gradient:  0.3923895452411764
iteration : 492
train acc:  0.625
train loss:  0.596326470375061
train gradient:  0.39883906613990533
iteration : 493
train acc:  0.6328125
train loss:  0.6376495957374573
train gradient:  0.6200673059001418
iteration : 494
train acc:  0.6875
train loss:  0.567737877368927
train gradient:  0.6358085969229023
iteration : 495
train acc:  0.7265625
train loss:  0.5430346131324768
train gradient:  0.6393496070413793
iteration : 496
train acc:  0.7109375
train loss:  0.5601316690444946
train gradient:  0.6533586337101702
iteration : 497
train acc:  0.7109375
train loss:  0.5607589483261108
train gradient:  0.7978257792058532
iteration : 498
train acc:  0.6953125
train loss:  0.557263970375061
train gradient:  0.4561716085604495
iteration : 499
train acc:  0.7265625
train loss:  0.5186645984649658
train gradient:  0.7244527718775479
iteration : 500
train acc:  0.796875
train loss:  0.4552706778049469
train gradient:  0.4816576675465813
iteration : 501
train acc:  0.5859375
train loss:  0.7068750858306885
train gradient:  1.0612805097345674
iteration : 502
train acc:  0.7578125
train loss:  0.5111950039863586
train gradient:  0.805927090102694
iteration : 503
train acc:  0.7265625
train loss:  0.5887736678123474
train gradient:  0.6440060014172277
iteration : 504
train acc:  0.671875
train loss:  0.5975875854492188
train gradient:  0.613375271002253
iteration : 505
train acc:  0.7421875
train loss:  0.5193707942962646
train gradient:  0.6535242653558089
iteration : 506
train acc:  0.78125
train loss:  0.47648295760154724
train gradient:  0.5618510562682375
iteration : 507
train acc:  0.75
train loss:  0.49681004881858826
train gradient:  0.4581275246186984
iteration : 508
train acc:  0.6953125
train loss:  0.6411632299423218
train gradient:  1.5240843737924024
iteration : 509
train acc:  0.7421875
train loss:  0.5415926575660706
train gradient:  0.3797193994876125
iteration : 510
train acc:  0.78125
train loss:  0.47875675559043884
train gradient:  0.3576777000122108
iteration : 511
train acc:  0.7421875
train loss:  0.5088790655136108
train gradient:  0.3382841150761628
iteration : 512
train acc:  0.7109375
train loss:  0.5608856678009033
train gradient:  0.5506316445734631
iteration : 513
train acc:  0.7265625
train loss:  0.5078991651535034
train gradient:  0.3893650097241725
iteration : 514
train acc:  0.6953125
train loss:  0.5502365827560425
train gradient:  0.3991472598649287
iteration : 515
train acc:  0.78125
train loss:  0.4881289005279541
train gradient:  0.4603889651271922
iteration : 516
train acc:  0.7421875
train loss:  0.48297175765037537
train gradient:  0.46665482926802887
iteration : 517
train acc:  0.671875
train loss:  0.5471558570861816
train gradient:  0.5108163927039253
iteration : 518
train acc:  0.71875
train loss:  0.5534184575080872
train gradient:  0.5810504636122157
iteration : 519
train acc:  0.7265625
train loss:  0.5279271602630615
train gradient:  0.3982888898971855
iteration : 520
train acc:  0.734375
train loss:  0.5349998474121094
train gradient:  0.6277967352397985
iteration : 521
train acc:  0.671875
train loss:  0.6269166469573975
train gradient:  0.7016063648218644
iteration : 522
train acc:  0.7421875
train loss:  0.5334165096282959
train gradient:  0.5937118511277435
iteration : 523
train acc:  0.7265625
train loss:  0.5461621880531311
train gradient:  0.4559790195241864
iteration : 524
train acc:  0.703125
train loss:  0.5394665002822876
train gradient:  0.5507429410328037
iteration : 525
train acc:  0.703125
train loss:  0.5775377154350281
train gradient:  0.9249293646226577
iteration : 526
train acc:  0.78125
train loss:  0.5230209827423096
train gradient:  0.4337753825675779
iteration : 527
train acc:  0.7265625
train loss:  0.5173388719558716
train gradient:  0.4197226660547834
iteration : 528
train acc:  0.703125
train loss:  0.5656763315200806
train gradient:  0.5552909295456158
iteration : 529
train acc:  0.7578125
train loss:  0.47831958532333374
train gradient:  0.4115436994942693
iteration : 530
train acc:  0.6875
train loss:  0.5437295436859131
train gradient:  0.5987425412007039
iteration : 531
train acc:  0.734375
train loss:  0.5121614933013916
train gradient:  0.39972589685741045
iteration : 532
train acc:  0.71875
train loss:  0.5541715621948242
train gradient:  0.5194739571245021
iteration : 533
train acc:  0.703125
train loss:  0.5569989085197449
train gradient:  0.5324468067532742
iteration : 534
train acc:  0.671875
train loss:  0.5538393259048462
train gradient:  0.4245474268851758
iteration : 535
train acc:  0.765625
train loss:  0.5357893705368042
train gradient:  0.3821482643499079
iteration : 536
train acc:  0.7734375
train loss:  0.4635055661201477
train gradient:  0.3878343935118621
iteration : 537
train acc:  0.7109375
train loss:  0.6103923320770264
train gradient:  0.6431199486062045
iteration : 538
train acc:  0.6796875
train loss:  0.5697987079620361
train gradient:  0.5959732249445772
iteration : 539
train acc:  0.765625
train loss:  0.5081378221511841
train gradient:  0.3688646278498211
iteration : 540
train acc:  0.765625
train loss:  0.5367537140846252
train gradient:  0.5068929211736721
iteration : 541
train acc:  0.78125
train loss:  0.5031708478927612
train gradient:  0.5618510744272154
iteration : 542
train acc:  0.7734375
train loss:  0.5020766258239746
train gradient:  0.42183517802866144
iteration : 543
train acc:  0.6796875
train loss:  0.6357367038726807
train gradient:  0.5991186006174116
iteration : 544
train acc:  0.7265625
train loss:  0.5508934259414673
train gradient:  0.47639883402166183
iteration : 545
train acc:  0.75
train loss:  0.5080955028533936
train gradient:  0.5650494666671326
iteration : 546
train acc:  0.765625
train loss:  0.5105768442153931
train gradient:  0.369187980064576
iteration : 547
train acc:  0.703125
train loss:  0.5298301577568054
train gradient:  0.405801522901481
iteration : 548
train acc:  0.6484375
train loss:  0.5892388820648193
train gradient:  0.576343855444832
iteration : 549
train acc:  0.71875
train loss:  0.528702437877655
train gradient:  0.45839656480907426
iteration : 550
train acc:  0.7578125
train loss:  0.5189672708511353
train gradient:  0.37499305828770485
iteration : 551
train acc:  0.6953125
train loss:  0.6169947385787964
train gradient:  0.6850899078319073
iteration : 552
train acc:  0.7578125
train loss:  0.5387570858001709
train gradient:  0.35806681526706907
iteration : 553
train acc:  0.71875
train loss:  0.5951066017150879
train gradient:  0.49352336967514354
iteration : 554
train acc:  0.65625
train loss:  0.5766168236732483
train gradient:  0.49302467323684673
iteration : 555
train acc:  0.71875
train loss:  0.5558870434761047
train gradient:  0.4266580599270695
iteration : 556
train acc:  0.7578125
train loss:  0.47237443923950195
train gradient:  0.5173606849575807
iteration : 557
train acc:  0.71875
train loss:  0.5767054557800293
train gradient:  0.7424926844735051
iteration : 558
train acc:  0.7890625
train loss:  0.48516225814819336
train gradient:  0.4949897995514938
iteration : 559
train acc:  0.71875
train loss:  0.5420103669166565
train gradient:  0.5150228510543385
iteration : 560
train acc:  0.75
train loss:  0.5355448722839355
train gradient:  0.6599834968007774
iteration : 561
train acc:  0.75
train loss:  0.5298208594322205
train gradient:  0.6194321540045307
iteration : 562
train acc:  0.7734375
train loss:  0.4992653727531433
train gradient:  0.5914051485411489
iteration : 563
train acc:  0.7265625
train loss:  0.5346906185150146
train gradient:  0.44221056534310116
iteration : 564
train acc:  0.6953125
train loss:  0.5561395883560181
train gradient:  0.49227757524726923
iteration : 565
train acc:  0.765625
train loss:  0.4815641939640045
train gradient:  0.35958784566753943
iteration : 566
train acc:  0.7421875
train loss:  0.5207491517066956
train gradient:  0.46653657549195193
iteration : 567
train acc:  0.71875
train loss:  0.5300567746162415
train gradient:  0.504097378996881
iteration : 568
train acc:  0.7109375
train loss:  0.520609974861145
train gradient:  0.45355265700004344
iteration : 569
train acc:  0.703125
train loss:  0.5393736362457275
train gradient:  0.4002357869801293
iteration : 570
train acc:  0.6875
train loss:  0.656165599822998
train gradient:  0.5435262918378343
iteration : 571
train acc:  0.6875
train loss:  0.5760833024978638
train gradient:  0.6607156949205788
iteration : 572
train acc:  0.7578125
train loss:  0.45575404167175293
train gradient:  0.37046897012844227
iteration : 573
train acc:  0.6640625
train loss:  0.6184096336364746
train gradient:  0.49999421389615734
iteration : 574
train acc:  0.7109375
train loss:  0.5239452123641968
train gradient:  0.6710570430116676
iteration : 575
train acc:  0.7421875
train loss:  0.49153468012809753
train gradient:  0.4340620234534699
iteration : 576
train acc:  0.75
train loss:  0.5058573484420776
train gradient:  0.38724646500687854
iteration : 577
train acc:  0.734375
train loss:  0.5420562028884888
train gradient:  0.4855501194475767
iteration : 578
train acc:  0.7578125
train loss:  0.4558931887149811
train gradient:  0.2690870039806803
iteration : 579
train acc:  0.7109375
train loss:  0.6243778467178345
train gradient:  0.49739588844556143
iteration : 580
train acc:  0.765625
train loss:  0.49559056758880615
train gradient:  0.38859096102441126
iteration : 581
train acc:  0.703125
train loss:  0.5659617185592651
train gradient:  0.4942629496573755
iteration : 582
train acc:  0.671875
train loss:  0.5600299835205078
train gradient:  0.5428752898400776
iteration : 583
train acc:  0.7265625
train loss:  0.4930753707885742
train gradient:  0.3943348578136914
iteration : 584
train acc:  0.7265625
train loss:  0.5095281004905701
train gradient:  0.45108285044217367
iteration : 585
train acc:  0.8125
train loss:  0.4740392863750458
train gradient:  0.4083330815952997
iteration : 586
train acc:  0.796875
train loss:  0.47529032826423645
train gradient:  0.3160412899983587
iteration : 587
train acc:  0.703125
train loss:  0.5425053834915161
train gradient:  0.48137099718594717
iteration : 588
train acc:  0.765625
train loss:  0.4944124221801758
train gradient:  0.3814171492793487
iteration : 589
train acc:  0.734375
train loss:  0.5368083715438843
train gradient:  0.4232697302657524
iteration : 590
train acc:  0.71875
train loss:  0.517065167427063
train gradient:  0.2956428586188662
iteration : 591
train acc:  0.703125
train loss:  0.5440119504928589
train gradient:  0.3698247642595797
iteration : 592
train acc:  0.7578125
train loss:  0.565418541431427
train gradient:  0.5138861308991786
iteration : 593
train acc:  0.7890625
train loss:  0.4828501343727112
train gradient:  0.3488716456025248
iteration : 594
train acc:  0.7265625
train loss:  0.5541554093360901
train gradient:  0.46696179691153833
iteration : 595
train acc:  0.7265625
train loss:  0.5356757640838623
train gradient:  0.4438468294540383
iteration : 596
train acc:  0.7421875
train loss:  0.5227748155593872
train gradient:  0.42839053511057357
iteration : 597
train acc:  0.71875
train loss:  0.5033746361732483
train gradient:  0.5447022378476845
iteration : 598
train acc:  0.71875
train loss:  0.5557712912559509
train gradient:  0.4662990280290105
iteration : 599
train acc:  0.703125
train loss:  0.506676197052002
train gradient:  0.4928112647140964
iteration : 600
train acc:  0.7265625
train loss:  0.5351114869117737
train gradient:  0.3418161277938053
iteration : 601
train acc:  0.71875
train loss:  0.5281155109405518
train gradient:  0.4653126615757174
iteration : 602
train acc:  0.703125
train loss:  0.540495753288269
train gradient:  0.43839187860494994
iteration : 603
train acc:  0.7734375
train loss:  0.4814150333404541
train gradient:  0.440371340692587
iteration : 604
train acc:  0.7734375
train loss:  0.49750006198883057
train gradient:  0.49925057878884854
iteration : 605
train acc:  0.703125
train loss:  0.5685200095176697
train gradient:  0.7764514638619217
iteration : 606
train acc:  0.7421875
train loss:  0.5492395758628845
train gradient:  0.8379607681562011
iteration : 607
train acc:  0.703125
train loss:  0.5699078440666199
train gradient:  0.5571280275144308
iteration : 608
train acc:  0.71875
train loss:  0.5433857440948486
train gradient:  0.6725141388382834
iteration : 609
train acc:  0.7890625
train loss:  0.49283087253570557
train gradient:  0.6183868876257835
iteration : 610
train acc:  0.71875
train loss:  0.5307103991508484
train gradient:  0.44515968939121947
iteration : 611
train acc:  0.7421875
train loss:  0.5480010509490967
train gradient:  0.5141403392929378
iteration : 612
train acc:  0.7109375
train loss:  0.5773247480392456
train gradient:  0.8187768174308058
iteration : 613
train acc:  0.7734375
train loss:  0.5148725509643555
train gradient:  0.5823216013505287
iteration : 614
train acc:  0.7890625
train loss:  0.4735787808895111
train gradient:  0.32773460685742845
iteration : 615
train acc:  0.6796875
train loss:  0.54736328125
train gradient:  0.49392205273940637
iteration : 616
train acc:  0.75
train loss:  0.5554320216178894
train gradient:  0.4614391824553706
iteration : 617
train acc:  0.765625
train loss:  0.49401557445526123
train gradient:  0.3802525802326474
iteration : 618
train acc:  0.71875
train loss:  0.5496768951416016
train gradient:  0.4325394394677879
iteration : 619
train acc:  0.7734375
train loss:  0.516268253326416
train gradient:  0.44184648900664364
iteration : 620
train acc:  0.7109375
train loss:  0.5815030336380005
train gradient:  0.68641766268224
iteration : 621
train acc:  0.703125
train loss:  0.5990384817123413
train gradient:  0.49651716996451384
iteration : 622
train acc:  0.7734375
train loss:  0.4723307192325592
train gradient:  0.47614405278772065
iteration : 623
train acc:  0.78125
train loss:  0.4896133840084076
train gradient:  0.3349073252338206
iteration : 624
train acc:  0.71875
train loss:  0.55159592628479
train gradient:  0.6183391544920098
iteration : 625
train acc:  0.7109375
train loss:  0.5535262823104858
train gradient:  0.5544506947821379
iteration : 626
train acc:  0.71875
train loss:  0.5301440954208374
train gradient:  0.6523793545659278
iteration : 627
train acc:  0.765625
train loss:  0.50115966796875
train gradient:  0.3616504791794991
iteration : 628
train acc:  0.8125
train loss:  0.4625662565231323
train gradient:  0.3451066689131503
iteration : 629
train acc:  0.7578125
train loss:  0.5080159902572632
train gradient:  0.500309149621458
iteration : 630
train acc:  0.796875
train loss:  0.45175278186798096
train gradient:  0.32911378353800247
iteration : 631
train acc:  0.7109375
train loss:  0.5781548023223877
train gradient:  0.4303543156400617
iteration : 632
train acc:  0.78125
train loss:  0.45348966121673584
train gradient:  0.4570569771999777
iteration : 633
train acc:  0.7734375
train loss:  0.47577816247940063
train gradient:  0.48539839506719057
iteration : 634
train acc:  0.796875
train loss:  0.4398949146270752
train gradient:  0.4045529192480999
iteration : 635
train acc:  0.671875
train loss:  0.5764538645744324
train gradient:  0.41995512492728987
iteration : 636
train acc:  0.7578125
train loss:  0.5280870795249939
train gradient:  0.4871636458372288
iteration : 637
train acc:  0.7578125
train loss:  0.5041648745536804
train gradient:  0.47361182066157004
iteration : 638
train acc:  0.7578125
train loss:  0.5284477472305298
train gradient:  0.4749081111697888
iteration : 639
train acc:  0.734375
train loss:  0.4826275110244751
train gradient:  0.43232362407303715
iteration : 640
train acc:  0.65625
train loss:  0.5698450803756714
train gradient:  0.5291499741238246
iteration : 641
train acc:  0.6796875
train loss:  0.6017723083496094
train gradient:  0.7660395439501198
iteration : 642
train acc:  0.7265625
train loss:  0.5558841228485107
train gradient:  0.5235144354089467
iteration : 643
train acc:  0.7578125
train loss:  0.5028138160705566
train gradient:  0.32251445327581524
iteration : 644
train acc:  0.703125
train loss:  0.504603922367096
train gradient:  0.49037700740371293
iteration : 645
train acc:  0.7734375
train loss:  0.4577838182449341
train gradient:  0.40556223431644145
iteration : 646
train acc:  0.71875
train loss:  0.5126646161079407
train gradient:  0.44774189648753004
iteration : 647
train acc:  0.734375
train loss:  0.5194153785705566
train gradient:  0.49955962647558727
iteration : 648
train acc:  0.8046875
train loss:  0.4477817416191101
train gradient:  0.40871006633250884
iteration : 649
train acc:  0.703125
train loss:  0.5241455435752869
train gradient:  0.5601384118912841
iteration : 650
train acc:  0.75
train loss:  0.49886268377304077
train gradient:  0.378957264134567
iteration : 651
train acc:  0.671875
train loss:  0.5944013595581055
train gradient:  0.47998227536061555
iteration : 652
train acc:  0.8359375
train loss:  0.41313982009887695
train gradient:  0.32009121867391915
iteration : 653
train acc:  0.7109375
train loss:  0.5312014818191528
train gradient:  0.3607759281240913
iteration : 654
train acc:  0.765625
train loss:  0.4859691858291626
train gradient:  0.3610289352383012
iteration : 655
train acc:  0.71875
train loss:  0.5529721975326538
train gradient:  0.5478459961779576
iteration : 656
train acc:  0.71875
train loss:  0.5139017105102539
train gradient:  0.37968561663725725
iteration : 657
train acc:  0.671875
train loss:  0.5954124331474304
train gradient:  0.5863666497059794
iteration : 658
train acc:  0.7265625
train loss:  0.5622848272323608
train gradient:  0.623253156184928
iteration : 659
train acc:  0.7109375
train loss:  0.5466612577438354
train gradient:  0.5318073921854112
iteration : 660
train acc:  0.71875
train loss:  0.5225930213928223
train gradient:  0.43957826040867337
iteration : 661
train acc:  0.71875
train loss:  0.5419844388961792
train gradient:  0.4751433344641413
iteration : 662
train acc:  0.734375
train loss:  0.5165360569953918
train gradient:  0.5895722935101235
iteration : 663
train acc:  0.796875
train loss:  0.45642441511154175
train gradient:  0.33823987933278526
iteration : 664
train acc:  0.7265625
train loss:  0.5352830290794373
train gradient:  0.518233449797155
iteration : 665
train acc:  0.7109375
train loss:  0.5259075164794922
train gradient:  0.6950957021669579
iteration : 666
train acc:  0.7578125
train loss:  0.4990514814853668
train gradient:  0.356904783201573
iteration : 667
train acc:  0.734375
train loss:  0.5652697086334229
train gradient:  0.6577826288459412
iteration : 668
train acc:  0.765625
train loss:  0.48932498693466187
train gradient:  0.38732044619859757
iteration : 669
train acc:  0.765625
train loss:  0.5180583000183105
train gradient:  0.512425502182908
iteration : 670
train acc:  0.7734375
train loss:  0.45774197578430176
train gradient:  0.3042653669218318
iteration : 671
train acc:  0.734375
train loss:  0.49407392740249634
train gradient:  0.33231510994253416
iteration : 672
train acc:  0.7109375
train loss:  0.5469869375228882
train gradient:  0.650914137403813
iteration : 673
train acc:  0.78125
train loss:  0.4930577874183655
train gradient:  0.3156414501440077
iteration : 674
train acc:  0.7734375
train loss:  0.5413386821746826
train gradient:  0.392086798046549
iteration : 675
train acc:  0.71875
train loss:  0.5943630933761597
train gradient:  0.4621266914622979
iteration : 676
train acc:  0.7265625
train loss:  0.5405224561691284
train gradient:  0.4348090488852971
iteration : 677
train acc:  0.7109375
train loss:  0.5887050628662109
train gradient:  0.8421467737103279
iteration : 678
train acc:  0.71875
train loss:  0.5174506902694702
train gradient:  0.47613416516592905
iteration : 679
train acc:  0.734375
train loss:  0.4919065535068512
train gradient:  0.3192969100735108
iteration : 680
train acc:  0.765625
train loss:  0.4761956036090851
train gradient:  0.3714275554209075
iteration : 681
train acc:  0.796875
train loss:  0.4598740339279175
train gradient:  0.3449976365810036
iteration : 682
train acc:  0.7578125
train loss:  0.46086329221725464
train gradient:  0.2800013457465943
iteration : 683
train acc:  0.75
train loss:  0.5337314009666443
train gradient:  0.5511349258568076
iteration : 684
train acc:  0.7421875
train loss:  0.5040309429168701
train gradient:  0.3401533954801004
iteration : 685
train acc:  0.7578125
train loss:  0.5007487535476685
train gradient:  0.3422940189882478
iteration : 686
train acc:  0.6328125
train loss:  0.6176813840866089
train gradient:  0.7311229770054345
iteration : 687
train acc:  0.8046875
train loss:  0.47516414523124695
train gradient:  0.3347483857323749
iteration : 688
train acc:  0.7421875
train loss:  0.4835537374019623
train gradient:  0.23748493368256743
iteration : 689
train acc:  0.828125
train loss:  0.4692595601081848
train gradient:  0.3127972605722477
iteration : 690
train acc:  0.78125
train loss:  0.4700070321559906
train gradient:  0.38420557837603625
iteration : 691
train acc:  0.734375
train loss:  0.5388879776000977
train gradient:  0.3717130733034849
iteration : 692
train acc:  0.734375
train loss:  0.4879353940486908
train gradient:  0.38305658077602145
iteration : 693
train acc:  0.765625
train loss:  0.5104790329933167
train gradient:  0.3835065024954213
iteration : 694
train acc:  0.6640625
train loss:  0.550105094909668
train gradient:  0.44520075194797215
iteration : 695
train acc:  0.7265625
train loss:  0.5063920021057129
train gradient:  0.3716687418787401
iteration : 696
train acc:  0.78125
train loss:  0.46923646330833435
train gradient:  0.3858378400888805
iteration : 697
train acc:  0.703125
train loss:  0.5415923595428467
train gradient:  0.3957330372296686
iteration : 698
train acc:  0.734375
train loss:  0.4808042645454407
train gradient:  0.2946374522704609
iteration : 699
train acc:  0.7578125
train loss:  0.4945635199546814
train gradient:  0.44841548835953093
iteration : 700
train acc:  0.7734375
train loss:  0.4966430068016052
train gradient:  0.4367827341784411
iteration : 701
train acc:  0.7578125
train loss:  0.5195568203926086
train gradient:  0.34696915635788045
iteration : 702
train acc:  0.75
train loss:  0.4837602376937866
train gradient:  0.4256716002730011
iteration : 703
train acc:  0.7734375
train loss:  0.5042585134506226
train gradient:  0.4031226328050067
iteration : 704
train acc:  0.71875
train loss:  0.5595458745956421
train gradient:  0.5950478627546392
iteration : 705
train acc:  0.7265625
train loss:  0.5312833786010742
train gradient:  0.5649454891962882
iteration : 706
train acc:  0.6640625
train loss:  0.6068945527076721
train gradient:  0.7560433163058566
iteration : 707
train acc:  0.703125
train loss:  0.5979334115982056
train gradient:  0.8080093945327775
iteration : 708
train acc:  0.7890625
train loss:  0.4423995614051819
train gradient:  0.442892041111583
iteration : 709
train acc:  0.75
train loss:  0.4699293076992035
train gradient:  0.25793305646498926
iteration : 710
train acc:  0.703125
train loss:  0.5240466594696045
train gradient:  0.4928009941675291
iteration : 711
train acc:  0.78125
train loss:  0.46751201152801514
train gradient:  0.4610076938935813
iteration : 712
train acc:  0.78125
train loss:  0.45722711086273193
train gradient:  0.2830772251495882
iteration : 713
train acc:  0.7578125
train loss:  0.5404935479164124
train gradient:  0.507044483497805
iteration : 714
train acc:  0.7890625
train loss:  0.4393391013145447
train gradient:  0.2831333615583948
iteration : 715
train acc:  0.75
train loss:  0.5397028923034668
train gradient:  0.3310375044642565
iteration : 716
train acc:  0.75
train loss:  0.5122092962265015
train gradient:  0.5467786868132952
iteration : 717
train acc:  0.734375
train loss:  0.5166875720024109
train gradient:  0.29436127019335445
iteration : 718
train acc:  0.7421875
train loss:  0.5103839039802551
train gradient:  0.3165978851009052
iteration : 719
train acc:  0.7421875
train loss:  0.5018350481987
train gradient:  0.2991634138067816
iteration : 720
train acc:  0.703125
train loss:  0.5412337779998779
train gradient:  0.4164197966262981
iteration : 721
train acc:  0.75
train loss:  0.4962345063686371
train gradient:  0.6513969133968132
iteration : 722
train acc:  0.7890625
train loss:  0.43125325441360474
train gradient:  0.3095492186000366
iteration : 723
train acc:  0.703125
train loss:  0.6054493188858032
train gradient:  0.4798000843718842
iteration : 724
train acc:  0.7421875
train loss:  0.49382972717285156
train gradient:  0.4617705262909525
iteration : 725
train acc:  0.71875
train loss:  0.5324046611785889
train gradient:  0.5100548661622589
iteration : 726
train acc:  0.7421875
train loss:  0.5220032334327698
train gradient:  0.5083335920932046
iteration : 727
train acc:  0.6953125
train loss:  0.5439557433128357
train gradient:  0.4678799075206648
iteration : 728
train acc:  0.765625
train loss:  0.5096924304962158
train gradient:  0.48171955615128054
iteration : 729
train acc:  0.703125
train loss:  0.5818649530410767
train gradient:  0.9743829808228874
iteration : 730
train acc:  0.78125
train loss:  0.5003407001495361
train gradient:  0.5874466937987637
iteration : 731
train acc:  0.703125
train loss:  0.5643788576126099
train gradient:  0.46861114260871256
iteration : 732
train acc:  0.78125
train loss:  0.4545287489891052
train gradient:  0.3741141314606283
iteration : 733
train acc:  0.6875
train loss:  0.5363014340400696
train gradient:  0.5767751382110966
iteration : 734
train acc:  0.7421875
train loss:  0.5171096324920654
train gradient:  0.7376905721764235
iteration : 735
train acc:  0.8046875
train loss:  0.4284346401691437
train gradient:  0.34296248116996947
iteration : 736
train acc:  0.7421875
train loss:  0.5352784395217896
train gradient:  0.6167257169658821
iteration : 737
train acc:  0.703125
train loss:  0.5473619699478149
train gradient:  0.3842715318778422
iteration : 738
train acc:  0.7578125
train loss:  0.5145996809005737
train gradient:  0.5412961211529075
iteration : 739
train acc:  0.7734375
train loss:  0.48395252227783203
train gradient:  0.25878300403727916
iteration : 740
train acc:  0.65625
train loss:  0.6020559668540955
train gradient:  0.5964867720928553
iteration : 741
train acc:  0.78125
train loss:  0.49806153774261475
train gradient:  0.47687483718874524
iteration : 742
train acc:  0.78125
train loss:  0.46919333934783936
train gradient:  0.2983072175476426
iteration : 743
train acc:  0.78125
train loss:  0.4862891733646393
train gradient:  0.3269440837809972
iteration : 744
train acc:  0.6796875
train loss:  0.5535921454429626
train gradient:  0.42559376846847036
iteration : 745
train acc:  0.78125
train loss:  0.4717024564743042
train gradient:  0.43083957399097655
iteration : 746
train acc:  0.7265625
train loss:  0.4861522316932678
train gradient:  0.5016346377494522
iteration : 747
train acc:  0.7265625
train loss:  0.48785901069641113
train gradient:  0.5634149349279007
iteration : 748
train acc:  0.765625
train loss:  0.47586584091186523
train gradient:  0.4174883967896295
iteration : 749
train acc:  0.75
train loss:  0.5033526420593262
train gradient:  0.45363350866144847
iteration : 750
train acc:  0.7421875
train loss:  0.5263572931289673
train gradient:  0.3865999199319918
iteration : 751
train acc:  0.7578125
train loss:  0.5110514163970947
train gradient:  0.5100400722289749
iteration : 752
train acc:  0.796875
train loss:  0.4699530303478241
train gradient:  0.3385188893031994
iteration : 753
train acc:  0.6875
train loss:  0.5588321685791016
train gradient:  0.6292959034680521
iteration : 754
train acc:  0.71875
train loss:  0.5507711172103882
train gradient:  0.5504426780599141
iteration : 755
train acc:  0.7578125
train loss:  0.49770885705947876
train gradient:  0.4331872016985068
iteration : 756
train acc:  0.8125
train loss:  0.4453686475753784
train gradient:  0.33925446319690666
iteration : 757
train acc:  0.8046875
train loss:  0.4740402102470398
train gradient:  0.4748442668049735
iteration : 758
train acc:  0.6953125
train loss:  0.6014046669006348
train gradient:  0.7402235619542368
iteration : 759
train acc:  0.7890625
train loss:  0.46269384026527405
train gradient:  0.32387319207349946
iteration : 760
train acc:  0.796875
train loss:  0.4412296414375305
train gradient:  0.41576015663761867
iteration : 761
train acc:  0.7890625
train loss:  0.48627567291259766
train gradient:  0.40968965424855475
iteration : 762
train acc:  0.7421875
train loss:  0.5626643896102905
train gradient:  0.4854451393028543
iteration : 763
train acc:  0.84375
train loss:  0.39952826499938965
train gradient:  0.4027864208998027
iteration : 764
train acc:  0.796875
train loss:  0.4515796899795532
train gradient:  0.36623773296216516
iteration : 765
train acc:  0.765625
train loss:  0.4739432632923126
train gradient:  0.6621825518858075
iteration : 766
train acc:  0.7734375
train loss:  0.48202040791511536
train gradient:  0.4963530100998757
iteration : 767
train acc:  0.7421875
train loss:  0.4901977777481079
train gradient:  0.4853376100844952
iteration : 768
train acc:  0.75
train loss:  0.457461953163147
train gradient:  0.3768686629434972
iteration : 769
train acc:  0.7109375
train loss:  0.5299239754676819
train gradient:  0.540792181083361
iteration : 770
train acc:  0.765625
train loss:  0.4133569002151489
train gradient:  0.2811611953366163
iteration : 771
train acc:  0.7578125
train loss:  0.4897744059562683
train gradient:  0.4536929914495908
iteration : 772
train acc:  0.6953125
train loss:  0.5464343428611755
train gradient:  0.4954845718789486
iteration : 773
train acc:  0.6796875
train loss:  0.6000571250915527
train gradient:  0.4265628048525521
iteration : 774
train acc:  0.71875
train loss:  0.5317035913467407
train gradient:  0.5752534554489742
iteration : 775
train acc:  0.765625
train loss:  0.5238963961601257
train gradient:  0.5161970939043514
iteration : 776
train acc:  0.7890625
train loss:  0.46973714232444763
train gradient:  0.5395992638476329
iteration : 777
train acc:  0.75
train loss:  0.5548953413963318
train gradient:  0.5402364839628537
iteration : 778
train acc:  0.75
train loss:  0.5248327255249023
train gradient:  0.48674256529683935
iteration : 779
train acc:  0.765625
train loss:  0.4835593104362488
train gradient:  0.5156648205195626
iteration : 780
train acc:  0.7578125
train loss:  0.4898386299610138
train gradient:  0.39080563312429456
iteration : 781
train acc:  0.7734375
train loss:  0.469078004360199
train gradient:  0.42065961268693663
iteration : 782
train acc:  0.78125
train loss:  0.47118762135505676
train gradient:  0.33789241453002794
iteration : 783
train acc:  0.7421875
train loss:  0.525221586227417
train gradient:  0.5041575648102333
iteration : 784
train acc:  0.8203125
train loss:  0.4704276919364929
train gradient:  0.40518972812375603
iteration : 785
train acc:  0.7734375
train loss:  0.45788437128067017
train gradient:  0.4940448306264423
iteration : 786
train acc:  0.7578125
train loss:  0.5483918190002441
train gradient:  0.6526780485853467
iteration : 787
train acc:  0.71875
train loss:  0.529782235622406
train gradient:  0.5275781484229607
iteration : 788
train acc:  0.8515625
train loss:  0.42684441804885864
train gradient:  0.30860088258936647
iteration : 789
train acc:  0.7265625
train loss:  0.5072323083877563
train gradient:  0.7959025948999987
iteration : 790
train acc:  0.8125
train loss:  0.4183136224746704
train gradient:  0.4306013054101737
iteration : 791
train acc:  0.734375
train loss:  0.5295256972312927
train gradient:  0.5022855616759524
iteration : 792
train acc:  0.7578125
train loss:  0.48828405141830444
train gradient:  0.4222699266554152
iteration : 793
train acc:  0.7734375
train loss:  0.49973076581954956
train gradient:  0.4281890330723316
iteration : 794
train acc:  0.7578125
train loss:  0.5005244612693787
train gradient:  0.43044534571248294
iteration : 795
train acc:  0.75
train loss:  0.5241143107414246
train gradient:  0.4508010688823141
iteration : 796
train acc:  0.75
train loss:  0.5121170282363892
train gradient:  0.46840767772613273
iteration : 797
train acc:  0.7734375
train loss:  0.47642505168914795
train gradient:  0.3912604303219799
iteration : 798
train acc:  0.765625
train loss:  0.5064620971679688
train gradient:  0.41388277440068977
iteration : 799
train acc:  0.75
train loss:  0.4967230260372162
train gradient:  0.4579400512659299
iteration : 800
train acc:  0.734375
train loss:  0.4945959448814392
train gradient:  0.4253690582443792
iteration : 801
train acc:  0.734375
train loss:  0.5165815353393555
train gradient:  0.4445113343427998
iteration : 802
train acc:  0.7265625
train loss:  0.5360131859779358
train gradient:  0.5224613961711693
iteration : 803
train acc:  0.78125
train loss:  0.46846911311149597
train gradient:  0.33420720224173206
iteration : 804
train acc:  0.6953125
train loss:  0.5724020004272461
train gradient:  0.4794145234975321
iteration : 805
train acc:  0.78125
train loss:  0.5003103613853455
train gradient:  0.354111619567157
iteration : 806
train acc:  0.671875
train loss:  0.5692181587219238
train gradient:  0.658721797684214
iteration : 807
train acc:  0.7265625
train loss:  0.5468717813491821
train gradient:  0.4610684518086399
iteration : 808
train acc:  0.7421875
train loss:  0.48906779289245605
train gradient:  0.4204662681389265
iteration : 809
train acc:  0.828125
train loss:  0.48003557324409485
train gradient:  0.37785197763964073
iteration : 810
train acc:  0.71875
train loss:  0.5223513245582581
train gradient:  0.5366343736212666
iteration : 811
train acc:  0.734375
train loss:  0.5231701731681824
train gradient:  0.6426679803053248
iteration : 812
train acc:  0.7734375
train loss:  0.5139825940132141
train gradient:  0.46999184281384354
iteration : 813
train acc:  0.734375
train loss:  0.49788516759872437
train gradient:  0.5460176376074767
iteration : 814
train acc:  0.7734375
train loss:  0.4815146028995514
train gradient:  0.3307670541778288
iteration : 815
train acc:  0.78125
train loss:  0.4983499050140381
train gradient:  0.5437326379490395
iteration : 816
train acc:  0.7265625
train loss:  0.5608983039855957
train gradient:  0.8564797483012546
iteration : 817
train acc:  0.703125
train loss:  0.5698735117912292
train gradient:  0.6929583176526228
iteration : 818
train acc:  0.765625
train loss:  0.504889190196991
train gradient:  0.577362832745121
iteration : 819
train acc:  0.8203125
train loss:  0.4179542064666748
train gradient:  0.32110670964398846
iteration : 820
train acc:  0.7890625
train loss:  0.5200432538986206
train gradient:  0.45470780947709527
iteration : 821
train acc:  0.7109375
train loss:  0.5412828922271729
train gradient:  0.4554680616051292
iteration : 822
train acc:  0.7421875
train loss:  0.6000057458877563
train gradient:  0.912294277169293
iteration : 823
train acc:  0.765625
train loss:  0.48124969005584717
train gradient:  0.35769781609579154
iteration : 824
train acc:  0.734375
train loss:  0.5043688416481018
train gradient:  0.442623310836399
iteration : 825
train acc:  0.7734375
train loss:  0.4424095153808594
train gradient:  0.27345308629878035
iteration : 826
train acc:  0.765625
train loss:  0.5410048961639404
train gradient:  0.4422561229598046
iteration : 827
train acc:  0.734375
train loss:  0.5128259658813477
train gradient:  0.6664823776412383
iteration : 828
train acc:  0.8046875
train loss:  0.4110221266746521
train gradient:  0.23942771499032237
iteration : 829
train acc:  0.7421875
train loss:  0.45600324869155884
train gradient:  0.3707751818355958
iteration : 830
train acc:  0.671875
train loss:  0.5783169269561768
train gradient:  0.5865460279592019
iteration : 831
train acc:  0.7265625
train loss:  0.517781138420105
train gradient:  0.3624906656065736
iteration : 832
train acc:  0.765625
train loss:  0.5037580132484436
train gradient:  0.4701600929681201
iteration : 833
train acc:  0.796875
train loss:  0.4252488613128662
train gradient:  0.49738258178713424
iteration : 834
train acc:  0.8046875
train loss:  0.4118731617927551
train gradient:  0.39892689788179264
iteration : 835
train acc:  0.828125
train loss:  0.471483051776886
train gradient:  0.361585056730891
iteration : 836
train acc:  0.7578125
train loss:  0.4765031337738037
train gradient:  0.3532719732311048
iteration : 837
train acc:  0.7734375
train loss:  0.49245911836624146
train gradient:  0.5511990603236239
iteration : 838
train acc:  0.796875
train loss:  0.47455453872680664
train gradient:  0.5521271061341796
iteration : 839
train acc:  0.8046875
train loss:  0.4890134930610657
train gradient:  0.7982265068266183
iteration : 840
train acc:  0.84375
train loss:  0.4283077120780945
train gradient:  0.3749966265559054
iteration : 841
train acc:  0.7421875
train loss:  0.4993571639060974
train gradient:  0.5181289753633345
iteration : 842
train acc:  0.7578125
train loss:  0.4933964014053345
train gradient:  0.5743797833273129
iteration : 843
train acc:  0.75
train loss:  0.5506227612495422
train gradient:  0.5862133759760058
iteration : 844
train acc:  0.7734375
train loss:  0.4605858027935028
train gradient:  0.4062314416778533
iteration : 845
train acc:  0.7421875
train loss:  0.46248793601989746
train gradient:  0.4835155206644321
iteration : 846
train acc:  0.765625
train loss:  0.5220281481742859
train gradient:  0.6884021126178193
iteration : 847
train acc:  0.8046875
train loss:  0.46165788173675537
train gradient:  0.5550115142536519
iteration : 848
train acc:  0.75
train loss:  0.48736611008644104
train gradient:  0.5984705283830203
iteration : 849
train acc:  0.71875
train loss:  0.563449501991272
train gradient:  0.5761796369769584
iteration : 850
train acc:  0.7421875
train loss:  0.5291661024093628
train gradient:  0.6011591381629756
iteration : 851
train acc:  0.7421875
train loss:  0.48980462551116943
train gradient:  0.47844484851775027
iteration : 852
train acc:  0.796875
train loss:  0.4529110789299011
train gradient:  0.4716066188786558
iteration : 853
train acc:  0.7109375
train loss:  0.5707896947860718
train gradient:  0.7931186823880564
iteration : 854
train acc:  0.796875
train loss:  0.4299823045730591
train gradient:  0.44195468536786364
iteration : 855
train acc:  0.7265625
train loss:  0.5506592392921448
train gradient:  0.6418374798148697
iteration : 856
train acc:  0.734375
train loss:  0.5403937101364136
train gradient:  0.5380373063337501
iteration : 857
train acc:  0.7421875
train loss:  0.49719107151031494
train gradient:  0.39987499025547013
iteration : 858
train acc:  0.8046875
train loss:  0.46105724573135376
train gradient:  0.45537878806049165
iteration : 859
train acc:  0.8046875
train loss:  0.4989188313484192
train gradient:  0.46201715754555583
iteration : 860
train acc:  0.796875
train loss:  0.4566386044025421
train gradient:  0.33958069789961287
iteration : 861
train acc:  0.7734375
train loss:  0.5116174221038818
train gradient:  0.613011733634834
iteration : 862
train acc:  0.7890625
train loss:  0.48034483194351196
train gradient:  0.44781361643749834
iteration : 863
train acc:  0.7578125
train loss:  0.487045556306839
train gradient:  0.4110230648999582
iteration : 864
train acc:  0.8125
train loss:  0.45146048069000244
train gradient:  0.30762170035388964
iteration : 865
train acc:  0.734375
train loss:  0.4550526440143585
train gradient:  0.3773188931721136
iteration : 866
train acc:  0.7578125
train loss:  0.47032418847084045
train gradient:  0.4461879261494223
iteration : 867
train acc:  0.7734375
train loss:  0.47971516847610474
train gradient:  0.4580767928040008
iteration : 868
train acc:  0.75
train loss:  0.48668569326400757
train gradient:  0.3946949819368888
iteration : 869
train acc:  0.8046875
train loss:  0.4269438683986664
train gradient:  0.334038272406092
iteration : 870
train acc:  0.671875
train loss:  0.5536729097366333
train gradient:  0.5104359335802566
iteration : 871
train acc:  0.7265625
train loss:  0.5494049787521362
train gradient:  0.5473669121411906
iteration : 872
train acc:  0.71875
train loss:  0.5012161731719971
train gradient:  0.5402114417676362
iteration : 873
train acc:  0.7734375
train loss:  0.4401000142097473
train gradient:  0.49096504003705566
iteration : 874
train acc:  0.765625
train loss:  0.4626069962978363
train gradient:  0.477853907554671
iteration : 875
train acc:  0.75
train loss:  0.48332396149635315
train gradient:  0.2738373401267986
iteration : 876
train acc:  0.7421875
train loss:  0.48961305618286133
train gradient:  0.5117598177204599
iteration : 877
train acc:  0.734375
train loss:  0.48099982738494873
train gradient:  0.5520164029659118
iteration : 878
train acc:  0.7890625
train loss:  0.4537450671195984
train gradient:  0.5274273689500404
iteration : 879
train acc:  0.734375
train loss:  0.46984225511550903
train gradient:  0.5366410839394928
iteration : 880
train acc:  0.75
train loss:  0.5062557458877563
train gradient:  0.41772011223775074
iteration : 881
train acc:  0.8359375
train loss:  0.431176096200943
train gradient:  0.4165603746836975
iteration : 882
train acc:  0.75
train loss:  0.45748230814933777
train gradient:  0.30867559045781207
iteration : 883
train acc:  0.8125
train loss:  0.4542415738105774
train gradient:  0.36364573010515855
iteration : 884
train acc:  0.78125
train loss:  0.5029491186141968
train gradient:  0.41332324282615535
iteration : 885
train acc:  0.8046875
train loss:  0.40177738666534424
train gradient:  0.2441004448247648
iteration : 886
train acc:  0.8046875
train loss:  0.45498502254486084
train gradient:  0.34148777970081695
iteration : 887
train acc:  0.7890625
train loss:  0.44769567251205444
train gradient:  0.31718816643732817
iteration : 888
train acc:  0.734375
train loss:  0.548123300075531
train gradient:  0.4916161956046359
iteration : 889
train acc:  0.7734375
train loss:  0.4669215679168701
train gradient:  0.4283398705246161
iteration : 890
train acc:  0.703125
train loss:  0.5483306050300598
train gradient:  0.473964221570935
iteration : 891
train acc:  0.8046875
train loss:  0.510345458984375
train gradient:  0.5254548198660884
iteration : 892
train acc:  0.7734375
train loss:  0.43346869945526123
train gradient:  0.40086365355522224
iteration : 893
train acc:  0.7578125
train loss:  0.47993332147598267
train gradient:  0.38557913434047014
iteration : 894
train acc:  0.7421875
train loss:  0.5431980490684509
train gradient:  0.6114860124580567
iteration : 895
train acc:  0.7734375
train loss:  0.44449949264526367
train gradient:  0.4048012282149402
iteration : 896
train acc:  0.7265625
train loss:  0.5136127471923828
train gradient:  0.428425664397198
iteration : 897
train acc:  0.75
train loss:  0.5034082531929016
train gradient:  0.5580147014893702
iteration : 898
train acc:  0.7890625
train loss:  0.4498455226421356
train gradient:  0.32907451342921035
iteration : 899
train acc:  0.8046875
train loss:  0.4710255265235901
train gradient:  0.3455933637915159
iteration : 900
train acc:  0.765625
train loss:  0.5294276475906372
train gradient:  0.5273600517750852
iteration : 901
train acc:  0.8359375
train loss:  0.40623873472213745
train gradient:  0.4221989765954307
iteration : 902
train acc:  0.828125
train loss:  0.38560983538627625
train gradient:  0.3419025202289433
iteration : 903
train acc:  0.7890625
train loss:  0.43525204062461853
train gradient:  0.48073836499884487
iteration : 904
train acc:  0.703125
train loss:  0.5465813875198364
train gradient:  0.7030422607744434
iteration : 905
train acc:  0.7578125
train loss:  0.48126593232154846
train gradient:  0.3300128092686103
iteration : 906
train acc:  0.765625
train loss:  0.4546002447605133
train gradient:  0.4837436421667977
iteration : 907
train acc:  0.765625
train loss:  0.5132750272750854
train gradient:  0.627141522981718
iteration : 908
train acc:  0.78125
train loss:  0.4522976577281952
train gradient:  0.44290108075861123
iteration : 909
train acc:  0.7734375
train loss:  0.4832940399646759
train gradient:  0.5950342690899469
iteration : 910
train acc:  0.75
train loss:  0.4987821578979492
train gradient:  0.6551596157847553
iteration : 911
train acc:  0.84375
train loss:  0.3935387134552002
train gradient:  0.27243491411053566
iteration : 912
train acc:  0.796875
train loss:  0.4441859722137451
train gradient:  0.3926216850331388
iteration : 913
train acc:  0.765625
train loss:  0.4641416072845459
train gradient:  0.5133284828871947
iteration : 914
train acc:  0.7734375
train loss:  0.45511651039123535
train gradient:  0.47893021108692446
iteration : 915
train acc:  0.7578125
train loss:  0.43706002831459045
train gradient:  0.39395551832796877
iteration : 916
train acc:  0.75
train loss:  0.49425259232521057
train gradient:  0.5628991740040618
iteration : 917
train acc:  0.75
train loss:  0.47359269857406616
train gradient:  0.5844508111367208
iteration : 918
train acc:  0.71875
train loss:  0.5462995767593384
train gradient:  0.6191516845471249
iteration : 919
train acc:  0.8203125
train loss:  0.4306667149066925
train gradient:  0.47125270348055853
iteration : 920
train acc:  0.7890625
train loss:  0.45732277631759644
train gradient:  0.6049362913836649
iteration : 921
train acc:  0.765625
train loss:  0.46103841066360474
train gradient:  0.48528011352398426
iteration : 922
train acc:  0.7890625
train loss:  0.4466986656188965
train gradient:  0.57108907345815
iteration : 923
train acc:  0.7734375
train loss:  0.4484618306159973
train gradient:  0.443488217428412
iteration : 924
train acc:  0.7421875
train loss:  0.5204363465309143
train gradient:  0.7237480792476433
iteration : 925
train acc:  0.7578125
train loss:  0.493587464094162
train gradient:  0.4685328576193564
iteration : 926
train acc:  0.8359375
train loss:  0.3753863573074341
train gradient:  0.4433231484641839
iteration : 927
train acc:  0.8203125
train loss:  0.38095545768737793
train gradient:  0.33423572553168807
iteration : 928
train acc:  0.828125
train loss:  0.4100385010242462
train gradient:  0.346758580359976
iteration : 929
train acc:  0.7578125
train loss:  0.48248612880706787
train gradient:  0.4647345434244653
iteration : 930
train acc:  0.8046875
train loss:  0.4619871973991394
train gradient:  0.5443759797070783
iteration : 931
train acc:  0.78125
train loss:  0.47997140884399414
train gradient:  0.3990402701590495
iteration : 932
train acc:  0.7734375
train loss:  0.5276018381118774
train gradient:  0.5783902925045213
iteration : 933
train acc:  0.734375
train loss:  0.5259678959846497
train gradient:  0.7383891248021908
iteration : 934
train acc:  0.7421875
train loss:  0.5389190912246704
train gradient:  0.6472755530705645
iteration : 935
train acc:  0.7890625
train loss:  0.44206786155700684
train gradient:  0.3899119950692018
iteration : 936
train acc:  0.78125
train loss:  0.45517247915267944
train gradient:  0.4138467706881178
iteration : 937
train acc:  0.7265625
train loss:  0.48944777250289917
train gradient:  0.5070318445075839
iteration : 938
train acc:  0.7578125
train loss:  0.4256213307380676
train gradient:  0.3533842256084565
iteration : 939
train acc:  0.796875
train loss:  0.4425193667411804
train gradient:  0.44753166456110044
iteration : 940
train acc:  0.734375
train loss:  0.5319957733154297
train gradient:  0.3995497724313431
iteration : 941
train acc:  0.734375
train loss:  0.48960256576538086
train gradient:  0.4823687344539701
iteration : 942
train acc:  0.765625
train loss:  0.4585569202899933
train gradient:  0.42050660233871523
iteration : 943
train acc:  0.8359375
train loss:  0.40972137451171875
train gradient:  0.3739925254039887
iteration : 944
train acc:  0.8125
train loss:  0.44338712096214294
train gradient:  0.304121397499654
iteration : 945
train acc:  0.7578125
train loss:  0.45207762718200684
train gradient:  0.3604097816176325
iteration : 946
train acc:  0.7890625
train loss:  0.4394456148147583
train gradient:  0.4866837255400594
iteration : 947
train acc:  0.6953125
train loss:  0.5476641058921814
train gradient:  0.6429586361249875
iteration : 948
train acc:  0.765625
train loss:  0.49691781401634216
train gradient:  0.44648818474999336
iteration : 949
train acc:  0.7578125
train loss:  0.5402595400810242
train gradient:  0.7407899027237606
iteration : 950
train acc:  0.734375
train loss:  0.5458393096923828
train gradient:  0.5118386483994016
iteration : 951
train acc:  0.7890625
train loss:  0.42519956827163696
train gradient:  0.42052837648959346
iteration : 952
train acc:  0.828125
train loss:  0.4151691496372223
train gradient:  0.3369674174999469
iteration : 953
train acc:  0.7578125
train loss:  0.509363055229187
train gradient:  0.5707390156380148
iteration : 954
train acc:  0.765625
train loss:  0.5083088874816895
train gradient:  0.7101233941508005
iteration : 955
train acc:  0.796875
train loss:  0.39654988050460815
train gradient:  0.36637854231328726
iteration : 956
train acc:  0.828125
train loss:  0.45891663432121277
train gradient:  0.46119077624212595
iteration : 957
train acc:  0.765625
train loss:  0.4593941569328308
train gradient:  0.48422670128300344
iteration : 958
train acc:  0.796875
train loss:  0.4161781072616577
train gradient:  0.38914908569102014
iteration : 959
train acc:  0.7578125
train loss:  0.4997526705265045
train gradient:  0.5883576692073564
iteration : 960
train acc:  0.765625
train loss:  0.47443512082099915
train gradient:  0.41540496828577544
iteration : 961
train acc:  0.7265625
train loss:  0.5437132716178894
train gradient:  0.6188563553525999
iteration : 962
train acc:  0.84375
train loss:  0.37499481439590454
train gradient:  0.2958926806136096
iteration : 963
train acc:  0.7578125
train loss:  0.46565887331962585
train gradient:  0.49598084611616344
iteration : 964
train acc:  0.8203125
train loss:  0.4133481979370117
train gradient:  0.5327100385202328
iteration : 965
train acc:  0.8359375
train loss:  0.41758543252944946
train gradient:  0.6085043146386631
iteration : 966
train acc:  0.828125
train loss:  0.4464651346206665
train gradient:  0.42805264643157304
iteration : 967
train acc:  0.734375
train loss:  0.47919410467147827
train gradient:  0.8377444403953416
iteration : 968
train acc:  0.7265625
train loss:  0.5496197938919067
train gradient:  0.6227645889098433
iteration : 969
train acc:  0.8203125
train loss:  0.43914860486984253
train gradient:  0.5302767025046263
iteration : 970
train acc:  0.7578125
train loss:  0.4795299768447876
train gradient:  0.5397078067505086
iteration : 971
train acc:  0.75
train loss:  0.5476956963539124
train gradient:  0.7636253209252618
iteration : 972
train acc:  0.796875
train loss:  0.45906805992126465
train gradient:  0.4728036416421045
iteration : 973
train acc:  0.703125
train loss:  0.4451058506965637
train gradient:  0.43191649305508323
iteration : 974
train acc:  0.7578125
train loss:  0.5150824785232544
train gradient:  0.8458902472297022
iteration : 975
train acc:  0.7734375
train loss:  0.4393700957298279
train gradient:  0.570252511486949
iteration : 976
train acc:  0.765625
train loss:  0.44257473945617676
train gradient:  0.3934215123853092
iteration : 977
train acc:  0.84375
train loss:  0.3930249810218811
train gradient:  0.47916063696394195
iteration : 978
train acc:  0.7578125
train loss:  0.48367735743522644
train gradient:  0.3916473097816146
iteration : 979
train acc:  0.765625
train loss:  0.47013580799102783
train gradient:  0.5730751624602861
iteration : 980
train acc:  0.7421875
train loss:  0.49357736110687256
train gradient:  0.4914428323618161
iteration : 981
train acc:  0.84375
train loss:  0.3770869970321655
train gradient:  0.4576208200286656
iteration : 982
train acc:  0.765625
train loss:  0.5355308651924133
train gradient:  0.4714433336752478
iteration : 983
train acc:  0.78125
train loss:  0.4308403730392456
train gradient:  0.41797269391087377
iteration : 984
train acc:  0.8125
train loss:  0.43744418025016785
train gradient:  0.38334556052541785
iteration : 985
train acc:  0.8125
train loss:  0.4143902063369751
train gradient:  0.4691127973080062
iteration : 986
train acc:  0.7578125
train loss:  0.5025986433029175
train gradient:  0.48925173082227696
iteration : 987
train acc:  0.734375
train loss:  0.45626169443130493
train gradient:  0.5681070925981171
iteration : 988
train acc:  0.703125
train loss:  0.5402346253395081
train gradient:  0.9001850347457325
iteration : 989
train acc:  0.828125
train loss:  0.44434601068496704
train gradient:  0.46313940539483656
iteration : 990
train acc:  0.734375
train loss:  0.46458274126052856
train gradient:  0.30259894081508165
iteration : 991
train acc:  0.71875
train loss:  0.5709584951400757
train gradient:  0.7522608468806906
iteration : 992
train acc:  0.8125
train loss:  0.40284302830696106
train gradient:  0.4463352212479823
iteration : 993
train acc:  0.796875
train loss:  0.3943403959274292
train gradient:  0.34545773149759895
iteration : 994
train acc:  0.796875
train loss:  0.43171820044517517
train gradient:  0.45578793296745557
iteration : 995
train acc:  0.78125
train loss:  0.49222105741500854
train gradient:  0.4211728572598488
iteration : 996
train acc:  0.7734375
train loss:  0.45358818769454956
train gradient:  0.4756535776455763
iteration : 997
train acc:  0.8203125
train loss:  0.39704379439353943
train gradient:  0.30255815957662235
iteration : 998
train acc:  0.8203125
train loss:  0.45361456274986267
train gradient:  0.5110752207530738
iteration : 999
train acc:  0.8203125
train loss:  0.4384716749191284
train gradient:  0.4994185455333507
iteration : 1000
train acc:  0.78125
train loss:  0.43844708800315857
train gradient:  0.5049925992520635
iteration : 1001
train acc:  0.765625
train loss:  0.43417245149612427
train gradient:  0.347375298998743
iteration : 1002
train acc:  0.828125
train loss:  0.4312683641910553
train gradient:  0.386277387747415
iteration : 1003
train acc:  0.8515625
train loss:  0.3603900671005249
train gradient:  0.3541062627906447
iteration : 1004
train acc:  0.6484375
train loss:  0.5892990231513977
train gradient:  0.799598619729812
iteration : 1005
train acc:  0.8203125
train loss:  0.45065703988075256
train gradient:  0.38315833889340684
iteration : 1006
train acc:  0.7578125
train loss:  0.4546639323234558
train gradient:  0.4450194206708351
iteration : 1007
train acc:  0.7890625
train loss:  0.41407498717308044
train gradient:  0.4628801083102914
iteration : 1008
train acc:  0.796875
train loss:  0.42446964979171753
train gradient:  0.4779450195282959
iteration : 1009
train acc:  0.8046875
train loss:  0.3789677917957306
train gradient:  0.3337799656401507
iteration : 1010
train acc:  0.8125
train loss:  0.4054391384124756
train gradient:  0.48162370930042864
iteration : 1011
train acc:  0.796875
train loss:  0.399469792842865
train gradient:  0.4656173304829849
iteration : 1012
train acc:  0.78125
train loss:  0.4790396988391876
train gradient:  0.5846391479503765
iteration : 1013
train acc:  0.71875
train loss:  0.4982624650001526
train gradient:  0.5337423521111813
iteration : 1014
train acc:  0.7734375
train loss:  0.5590950846672058
train gradient:  0.7873319887910533
iteration : 1015
train acc:  0.734375
train loss:  0.505111575126648
train gradient:  0.721841824467013
iteration : 1016
train acc:  0.7265625
train loss:  0.5061141848564148
train gradient:  0.6402310746467239
iteration : 1017
train acc:  0.8125
train loss:  0.4696355164051056
train gradient:  0.5014253929483408
iteration : 1018
train acc:  0.84375
train loss:  0.44205427169799805
train gradient:  0.5189542182492677
iteration : 1019
train acc:  0.734375
train loss:  0.48723167181015015
train gradient:  0.5691221469090557
iteration : 1020
train acc:  0.734375
train loss:  0.5185038447380066
train gradient:  0.5069145648062781
iteration : 1021
train acc:  0.7578125
train loss:  0.4777241051197052
train gradient:  0.5859948692929189
iteration : 1022
train acc:  0.7734375
train loss:  0.4906335473060608
train gradient:  0.7214119013560931
iteration : 1023
train acc:  0.7265625
train loss:  0.5213456749916077
train gradient:  0.5564753126546282
iteration : 1024
train acc:  0.796875
train loss:  0.4417848289012909
train gradient:  0.4682135052755609
iteration : 1025
train acc:  0.796875
train loss:  0.4024728238582611
train gradient:  0.3750504328029377
iteration : 1026
train acc:  0.75
train loss:  0.5075158476829529
train gradient:  0.49861330769555806
iteration : 1027
train acc:  0.8125
train loss:  0.4264988303184509
train gradient:  0.5043174357234856
iteration : 1028
train acc:  0.78125
train loss:  0.4699891209602356
train gradient:  0.5210957147041864
iteration : 1029
train acc:  0.8125
train loss:  0.42646875977516174
train gradient:  0.34102339109718693
iteration : 1030
train acc:  0.8046875
train loss:  0.4314086437225342
train gradient:  0.4671412625891791
iteration : 1031
train acc:  0.7578125
train loss:  0.4780852794647217
train gradient:  0.5024714233906322
iteration : 1032
train acc:  0.7109375
train loss:  0.5812703967094421
train gradient:  0.8336494297506268
iteration : 1033
train acc:  0.7734375
train loss:  0.46684759855270386
train gradient:  0.48403613651784816
iteration : 1034
train acc:  0.7734375
train loss:  0.5286365747451782
train gradient:  0.5499049629379078
iteration : 1035
train acc:  0.75
train loss:  0.541169285774231
train gradient:  0.7286338076157763
iteration : 1036
train acc:  0.7265625
train loss:  0.5077754259109497
train gradient:  0.5366878951319204
iteration : 1037
train acc:  0.8359375
train loss:  0.4216633439064026
train gradient:  0.3393420921053703
iteration : 1038
train acc:  0.7578125
train loss:  0.4598901867866516
train gradient:  0.5704790144536152
iteration : 1039
train acc:  0.8125
train loss:  0.40868282318115234
train gradient:  0.32420177802751304
iteration : 1040
train acc:  0.7578125
train loss:  0.4678788483142853
train gradient:  0.5571401937183534
iteration : 1041
train acc:  0.78125
train loss:  0.44456028938293457
train gradient:  0.4999600479492552
iteration : 1042
train acc:  0.7578125
train loss:  0.5145841836929321
train gradient:  0.45415377328616374
iteration : 1043
train acc:  0.828125
train loss:  0.44091853499412537
train gradient:  0.5189145757394779
iteration : 1044
train acc:  0.75
train loss:  0.4534851908683777
train gradient:  0.4793805673367217
iteration : 1045
train acc:  0.78125
train loss:  0.429078072309494
train gradient:  0.6950926726336965
iteration : 1046
train acc:  0.8359375
train loss:  0.40383362770080566
train gradient:  0.38096501819245543
iteration : 1047
train acc:  0.8046875
train loss:  0.4420057237148285
train gradient:  0.5261130745150276
iteration : 1048
train acc:  0.7578125
train loss:  0.4683936536312103
train gradient:  0.5868312228775209
iteration : 1049
train acc:  0.828125
train loss:  0.4097806215286255
train gradient:  0.4122321344811467
iteration : 1050
train acc:  0.8125
train loss:  0.39054813981056213
train gradient:  0.36826717523398234
iteration : 1051
train acc:  0.859375
train loss:  0.36847054958343506
train gradient:  0.38577375825011656
iteration : 1052
train acc:  0.75
train loss:  0.5037932395935059
train gradient:  0.5045522675282708
iteration : 1053
train acc:  0.7265625
train loss:  0.500504195690155
train gradient:  0.8482266072984549
iteration : 1054
train acc:  0.734375
train loss:  0.5368151068687439
train gradient:  0.7493015207189958
iteration : 1055
train acc:  0.7578125
train loss:  0.47750091552734375
train gradient:  0.48805524662046024
iteration : 1056
train acc:  0.7734375
train loss:  0.44711095094680786
train gradient:  0.6065140248168559
iteration : 1057
train acc:  0.7578125
train loss:  0.5640462636947632
train gradient:  0.8117644425949768
iteration : 1058
train acc:  0.703125
train loss:  0.5368452072143555
train gradient:  0.7187835547394847
iteration : 1059
train acc:  0.765625
train loss:  0.4213895797729492
train gradient:  0.39191538264419107
iteration : 1060
train acc:  0.75
train loss:  0.4590454697608948
train gradient:  0.616379507492117
iteration : 1061
train acc:  0.75
train loss:  0.48798519372940063
train gradient:  0.7617516404155744
iteration : 1062
train acc:  0.796875
train loss:  0.468061625957489
train gradient:  0.5403913301979076
iteration : 1063
train acc:  0.7734375
train loss:  0.46011579036712646
train gradient:  0.44597031420113825
iteration : 1064
train acc:  0.75
train loss:  0.48185086250305176
train gradient:  0.4326427273529466
iteration : 1065
train acc:  0.84375
train loss:  0.4236660599708557
train gradient:  0.4143952371959929
iteration : 1066
train acc:  0.7421875
train loss:  0.509418249130249
train gradient:  0.5648533891226413
iteration : 1067
train acc:  0.765625
train loss:  0.45574885606765747
train gradient:  0.5228084299897724
iteration : 1068
train acc:  0.7890625
train loss:  0.37915295362472534
train gradient:  0.5185544860533023
iteration : 1069
train acc:  0.7578125
train loss:  0.47721773386001587
train gradient:  0.4495994720600064
iteration : 1070
train acc:  0.7734375
train loss:  0.46511393785476685
train gradient:  0.3673462658864094
iteration : 1071
train acc:  0.7734375
train loss:  0.42238056659698486
train gradient:  0.5061122182090846
iteration : 1072
train acc:  0.75
train loss:  0.5514469742774963
train gradient:  0.8395431415263671
iteration : 1073
train acc:  0.8125
train loss:  0.4084467589855194
train gradient:  0.3179747633968002
iteration : 1074
train acc:  0.7734375
train loss:  0.41323190927505493
train gradient:  0.41623153435305255
iteration : 1075
train acc:  0.8046875
train loss:  0.4803101420402527
train gradient:  0.5144015015807758
iteration : 1076
train acc:  0.7890625
train loss:  0.49504369497299194
train gradient:  0.5409036010444642
iteration : 1077
train acc:  0.8125
train loss:  0.4365536570549011
train gradient:  0.5355071985199527
iteration : 1078
train acc:  0.78125
train loss:  0.4620981514453888
train gradient:  0.36948773723285316
iteration : 1079
train acc:  0.75
train loss:  0.479920357465744
train gradient:  0.6066214683656199
iteration : 1080
train acc:  0.8046875
train loss:  0.43614161014556885
train gradient:  0.46573308581552486
iteration : 1081
train acc:  0.7421875
train loss:  0.5384061336517334
train gradient:  0.5265653545727396
iteration : 1082
train acc:  0.78125
train loss:  0.4434317648410797
train gradient:  0.4309405227894601
iteration : 1083
train acc:  0.75
train loss:  0.5141237378120422
train gradient:  0.612488113324485
iteration : 1084
train acc:  0.7421875
train loss:  0.5037993788719177
train gradient:  0.5471235511647898
iteration : 1085
train acc:  0.78125
train loss:  0.45824843645095825
train gradient:  0.4929251415195318
iteration : 1086
train acc:  0.765625
train loss:  0.501070499420166
train gradient:  0.4048217380265716
iteration : 1087
train acc:  0.7578125
train loss:  0.4895789921283722
train gradient:  0.4413989706883949
iteration : 1088
train acc:  0.765625
train loss:  0.49268460273742676
train gradient:  0.4355608476417681
iteration : 1089
train acc:  0.7421875
train loss:  0.49282869696617126
train gradient:  0.4480501585347677
iteration : 1090
train acc:  0.75
train loss:  0.4430238604545593
train gradient:  0.46211549695260934
iteration : 1091
train acc:  0.796875
train loss:  0.4539496898651123
train gradient:  0.41663466885431966
iteration : 1092
train acc:  0.7578125
train loss:  0.5377696752548218
train gradient:  0.5383631160615454
iteration : 1093
train acc:  0.796875
train loss:  0.4397146701812744
train gradient:  0.3858115871680902
iteration : 1094
train acc:  0.734375
train loss:  0.5256764888763428
train gradient:  0.5626068519779301
iteration : 1095
train acc:  0.703125
train loss:  0.5460336208343506
train gradient:  0.5962334944652754
iteration : 1096
train acc:  0.828125
train loss:  0.3918006718158722
train gradient:  0.31908885758591443
iteration : 1097
train acc:  0.8046875
train loss:  0.4250708818435669
train gradient:  0.3079640859046322
iteration : 1098
train acc:  0.765625
train loss:  0.45697009563446045
train gradient:  0.4198780172208169
iteration : 1099
train acc:  0.765625
train loss:  0.4918693006038666
train gradient:  0.45333293046024115
iteration : 1100
train acc:  0.8515625
train loss:  0.37926530838012695
train gradient:  0.3991065816203607
iteration : 1101
train acc:  0.7734375
train loss:  0.43843668699264526
train gradient:  0.4362663723992947
iteration : 1102
train acc:  0.703125
train loss:  0.5401259064674377
train gradient:  0.6260031455976444
iteration : 1103
train acc:  0.75
train loss:  0.4899536967277527
train gradient:  0.5522845944020304
iteration : 1104
train acc:  0.765625
train loss:  0.4580807089805603
train gradient:  0.48316763503954757
iteration : 1105
train acc:  0.75
train loss:  0.4902230501174927
train gradient:  0.3537009048338955
iteration : 1106
train acc:  0.6796875
train loss:  0.5488088130950928
train gradient:  0.7297415906062164
iteration : 1107
train acc:  0.8515625
train loss:  0.38762348890304565
train gradient:  0.34422420602140036
iteration : 1108
train acc:  0.7734375
train loss:  0.44612783193588257
train gradient:  0.3679281829863178
iteration : 1109
train acc:  0.7421875
train loss:  0.5311601758003235
train gradient:  0.6664211996845569
iteration : 1110
train acc:  0.75
train loss:  0.4451225996017456
train gradient:  0.3444630118942817
iteration : 1111
train acc:  0.8125
train loss:  0.43914997577667236
train gradient:  0.45124093902487333
iteration : 1112
train acc:  0.7890625
train loss:  0.4352158010005951
train gradient:  0.4379337043492956
iteration : 1113
train acc:  0.7265625
train loss:  0.5498915910720825
train gradient:  0.7024463005566621
iteration : 1114
train acc:  0.7734375
train loss:  0.4800596833229065
train gradient:  0.4079210955605269
iteration : 1115
train acc:  0.7890625
train loss:  0.482501745223999
train gradient:  0.5819175217406742
iteration : 1116
train acc:  0.8046875
train loss:  0.4361318349838257
train gradient:  0.3915233161365649
iteration : 1117
train acc:  0.8515625
train loss:  0.37893253564834595
train gradient:  0.358766719193536
iteration : 1118
train acc:  0.75
train loss:  0.4923291504383087
train gradient:  0.5654480158538482
iteration : 1119
train acc:  0.8046875
train loss:  0.44565606117248535
train gradient:  0.4312530810813924
iteration : 1120
train acc:  0.765625
train loss:  0.451568067073822
train gradient:  0.5755851453289546
iteration : 1121
train acc:  0.796875
train loss:  0.432856947183609
train gradient:  0.39476977653640266
iteration : 1122
train acc:  0.859375
train loss:  0.37058356404304504
train gradient:  0.35863783200511923
iteration : 1123
train acc:  0.796875
train loss:  0.46431735157966614
train gradient:  0.4138733958105906
iteration : 1124
train acc:  0.796875
train loss:  0.4496668577194214
train gradient:  0.4047744060687828
iteration : 1125
train acc:  0.828125
train loss:  0.44601717591285706
train gradient:  0.49433624840553586
iteration : 1126
train acc:  0.6953125
train loss:  0.5186995267868042
train gradient:  0.42257558258303574
iteration : 1127
train acc:  0.7890625
train loss:  0.46681004762649536
train gradient:  0.5019382424753636
iteration : 1128
train acc:  0.796875
train loss:  0.46102282404899597
train gradient:  0.42248046115973903
iteration : 1129
train acc:  0.8515625
train loss:  0.39796388149261475
train gradient:  0.4058316253009826
iteration : 1130
train acc:  0.71875
train loss:  0.4805183410644531
train gradient:  0.5199450221456599
iteration : 1131
train acc:  0.75
train loss:  0.49025970697402954
train gradient:  0.49200359151101075
iteration : 1132
train acc:  0.7890625
train loss:  0.46073251962661743
train gradient:  0.6084743396172126
iteration : 1133
train acc:  0.7109375
train loss:  0.5390127897262573
train gradient:  0.733214014183057
iteration : 1134
train acc:  0.78125
train loss:  0.43857142329216003
train gradient:  0.4210846315940292
iteration : 1135
train acc:  0.765625
train loss:  0.5057723522186279
train gradient:  0.40817032564207545
iteration : 1136
train acc:  0.8359375
train loss:  0.4605230987071991
train gradient:  0.43830928143218567
iteration : 1137
train acc:  0.796875
train loss:  0.43877944350242615
train gradient:  0.525151474067727
iteration : 1138
train acc:  0.8125
train loss:  0.44224637746810913
train gradient:  0.5782172384997292
iteration : 1139
train acc:  0.78125
train loss:  0.4569416046142578
train gradient:  0.5610275930195291
iteration : 1140
train acc:  0.84375
train loss:  0.3975028097629547
train gradient:  0.4061771216596304
iteration : 1141
train acc:  0.78125
train loss:  0.45193031430244446
train gradient:  0.42756607601675023
iteration : 1142
train acc:  0.7734375
train loss:  0.4372875392436981
train gradient:  0.44749386888777565
iteration : 1143
train acc:  0.84375
train loss:  0.3847845792770386
train gradient:  0.3347590930209804
iteration : 1144
train acc:  0.8203125
train loss:  0.3965200185775757
train gradient:  0.3183615192479905
iteration : 1145
train acc:  0.8046875
train loss:  0.46522045135498047
train gradient:  0.5081179245253251
iteration : 1146
train acc:  0.8046875
train loss:  0.44656944274902344
train gradient:  0.4068428725268556
iteration : 1147
train acc:  0.78125
train loss:  0.44025880098342896
train gradient:  0.39976982286392
iteration : 1148
train acc:  0.796875
train loss:  0.4222249388694763
train gradient:  0.4445996909255814
iteration : 1149
train acc:  0.8046875
train loss:  0.43470117449760437
train gradient:  0.3210137233766694
iteration : 1150
train acc:  0.859375
train loss:  0.34148770570755005
train gradient:  0.28319949530886473
iteration : 1151
train acc:  0.8125
train loss:  0.42315924167633057
train gradient:  0.45845898525102985
iteration : 1152
train acc:  0.796875
train loss:  0.4187803566455841
train gradient:  0.6021764742971478
iteration : 1153
train acc:  0.7890625
train loss:  0.4009666442871094
train gradient:  0.4070901623486793
iteration : 1154
train acc:  0.859375
train loss:  0.4096979796886444
train gradient:  0.4363350628895875
iteration : 1155
train acc:  0.8203125
train loss:  0.45681431889533997
train gradient:  0.5702433509986246
iteration : 1156
train acc:  0.7734375
train loss:  0.46572762727737427
train gradient:  0.5314187897340766
iteration : 1157
train acc:  0.8125
train loss:  0.396401584148407
train gradient:  0.43297856964259107
iteration : 1158
train acc:  0.7578125
train loss:  0.4643118381500244
train gradient:  0.41263061881803853
iteration : 1159
train acc:  0.890625
train loss:  0.33559492230415344
train gradient:  0.2377151211699682
iteration : 1160
train acc:  0.796875
train loss:  0.4195736050605774
train gradient:  0.44334058992390857
iteration : 1161
train acc:  0.8203125
train loss:  0.44061046838760376
train gradient:  0.5142820243087576
iteration : 1162
train acc:  0.7890625
train loss:  0.48205631971359253
train gradient:  0.553393594177067
iteration : 1163
train acc:  0.828125
train loss:  0.37090784311294556
train gradient:  0.2771837752356749
iteration : 1164
train acc:  0.8046875
train loss:  0.42150646448135376
train gradient:  0.5919312985378372
iteration : 1165
train acc:  0.7890625
train loss:  0.4405195713043213
train gradient:  0.48767859061583557
iteration : 1166
train acc:  0.78125
train loss:  0.4555777907371521
train gradient:  0.528415331419058
iteration : 1167
train acc:  0.7890625
train loss:  0.4072719216346741
train gradient:  0.4814038193929941
iteration : 1168
train acc:  0.828125
train loss:  0.39023101329803467
train gradient:  0.3970418673860519
iteration : 1169
train acc:  0.7734375
train loss:  0.45051810145378113
train gradient:  0.662652706608266
iteration : 1170
train acc:  0.7265625
train loss:  0.528796374797821
train gradient:  0.8861657594885203
iteration : 1171
train acc:  0.75
train loss:  0.4626138508319855
train gradient:  0.5269139442781876
iteration : 1172
train acc:  0.78125
train loss:  0.39382994174957275
train gradient:  0.37317835939701205
iteration : 1173
train acc:  0.8125
train loss:  0.4072573781013489
train gradient:  0.5050561000928154
iteration : 1174
train acc:  0.7734375
train loss:  0.5442044734954834
train gradient:  0.7194251111922236
iteration : 1175
train acc:  0.828125
train loss:  0.42202961444854736
train gradient:  0.4460992269743067
iteration : 1176
train acc:  0.765625
train loss:  0.44888508319854736
train gradient:  0.4557618021057508
iteration : 1177
train acc:  0.828125
train loss:  0.3769569993019104
train gradient:  0.37805629165062804
iteration : 1178
train acc:  0.8125
train loss:  0.4036564528942108
train gradient:  0.4742089729115311
iteration : 1179
train acc:  0.859375
train loss:  0.3717409372329712
train gradient:  0.4244586903798007
iteration : 1180
train acc:  0.7890625
train loss:  0.4907959997653961
train gradient:  0.5736716034998595
iteration : 1181
train acc:  0.78125
train loss:  0.39285099506378174
train gradient:  0.5844535520349198
iteration : 1182
train acc:  0.7890625
train loss:  0.4457835555076599
train gradient:  0.5102948576918063
iteration : 1183
train acc:  0.75
train loss:  0.4928513765335083
train gradient:  0.5856683141924471
iteration : 1184
train acc:  0.8046875
train loss:  0.4278661906719208
train gradient:  0.4976589279759596
iteration : 1185
train acc:  0.7734375
train loss:  0.46431249380111694
train gradient:  0.5958495301335445
iteration : 1186
train acc:  0.8203125
train loss:  0.40627577900886536
train gradient:  0.44453193344833314
iteration : 1187
train acc:  0.7734375
train loss:  0.45388263463974
train gradient:  0.6317545208787552
iteration : 1188
train acc:  0.7109375
train loss:  0.49711644649505615
train gradient:  0.7462305858549012
iteration : 1189
train acc:  0.7734375
train loss:  0.42004090547561646
train gradient:  0.5002346471550039
iteration : 1190
train acc:  0.8046875
train loss:  0.44592440128326416
train gradient:  0.5602480354604304
iteration : 1191
train acc:  0.8359375
train loss:  0.4353312849998474
train gradient:  0.37614639996746935
iteration : 1192
train acc:  0.7890625
train loss:  0.4306896924972534
train gradient:  0.5771365666312285
iteration : 1193
train acc:  0.8125
train loss:  0.39927589893341064
train gradient:  0.43120583576613636
iteration : 1194
train acc:  0.78125
train loss:  0.5188461542129517
train gradient:  0.6768705872049186
iteration : 1195
train acc:  0.7734375
train loss:  0.4544028639793396
train gradient:  0.5825368860573876
iteration : 1196
train acc:  0.8203125
train loss:  0.4427366852760315
train gradient:  0.5210641441020425
iteration : 1197
train acc:  0.8125
train loss:  0.43269234895706177
train gradient:  0.46336676602533367
iteration : 1198
train acc:  0.828125
train loss:  0.4376133978366852
train gradient:  0.5887045114236058
iteration : 1199
train acc:  0.796875
train loss:  0.44423168897628784
train gradient:  0.5242139658833562
iteration : 1200
train acc:  0.78125
train loss:  0.4590187668800354
train gradient:  0.553095809629415
iteration : 1201
train acc:  0.7890625
train loss:  0.486538827419281
train gradient:  0.5633283100390349
iteration : 1202
train acc:  0.78125
train loss:  0.42816776037216187
train gradient:  0.5577620834696922
iteration : 1203
train acc:  0.796875
train loss:  0.4575214087963104
train gradient:  0.5787001291030078
iteration : 1204
train acc:  0.7734375
train loss:  0.4616078734397888
train gradient:  0.6644227797298754
iteration : 1205
train acc:  0.78125
train loss:  0.4518235921859741
train gradient:  0.9534030322649533
iteration : 1206
train acc:  0.7421875
train loss:  0.4698992073535919
train gradient:  0.4210952672814642
iteration : 1207
train acc:  0.8125
train loss:  0.4461366832256317
train gradient:  0.44856927274732566
iteration : 1208
train acc:  0.765625
train loss:  0.4851326644420624
train gradient:  0.5406319860952782
iteration : 1209
train acc:  0.8046875
train loss:  0.4487600326538086
train gradient:  0.5664208586773871
iteration : 1210
train acc:  0.75
train loss:  0.4787856340408325
train gradient:  0.6215912277872464
iteration : 1211
train acc:  0.7734375
train loss:  0.5174659490585327
train gradient:  0.6103262931097536
iteration : 1212
train acc:  0.796875
train loss:  0.43260037899017334
train gradient:  0.4603666307895578
iteration : 1213
train acc:  0.8125
train loss:  0.42925968766212463
train gradient:  0.48335548026880937
iteration : 1214
train acc:  0.75
train loss:  0.46561509370803833
train gradient:  0.5398184517516442
iteration : 1215
train acc:  0.8046875
train loss:  0.35890549421310425
train gradient:  0.5086582158582122
iteration : 1216
train acc:  0.84375
train loss:  0.3769289255142212
train gradient:  0.44955843119145267
iteration : 1217
train acc:  0.796875
train loss:  0.41502153873443604
train gradient:  0.44223845794627714
iteration : 1218
train acc:  0.7890625
train loss:  0.3979635238647461
train gradient:  0.5342544566211574
iteration : 1219
train acc:  0.7734375
train loss:  0.46818476915359497
train gradient:  0.5850714653158363
iteration : 1220
train acc:  0.7421875
train loss:  0.5632116794586182
train gradient:  0.8335256455211136
iteration : 1221
train acc:  0.78125
train loss:  0.4386560022830963
train gradient:  0.6354843882716597
iteration : 1222
train acc:  0.8359375
train loss:  0.39041054248809814
train gradient:  0.3892920923502528
iteration : 1223
train acc:  0.8203125
train loss:  0.41393357515335083
train gradient:  0.41628790267307925
iteration : 1224
train acc:  0.734375
train loss:  0.48109275102615356
train gradient:  0.5332558340030902
iteration : 1225
train acc:  0.7421875
train loss:  0.5104104280471802
train gradient:  0.5870499048446832
iteration : 1226
train acc:  0.75
train loss:  0.49496185779571533
train gradient:  0.6843862492547308
iteration : 1227
train acc:  0.8125
train loss:  0.42784738540649414
train gradient:  0.49910755838042037
iteration : 1228
train acc:  0.8203125
train loss:  0.4061766564846039
train gradient:  0.5991439328340415
iteration : 1229
train acc:  0.765625
train loss:  0.4899780750274658
train gradient:  0.5660696545382577
iteration : 1230
train acc:  0.6953125
train loss:  0.5258474946022034
train gradient:  0.6175487322028828
iteration : 1231
train acc:  0.828125
train loss:  0.3888598680496216
train gradient:  0.3912010601128144
iteration : 1232
train acc:  0.796875
train loss:  0.43664562702178955
train gradient:  0.3442927147671391
iteration : 1233
train acc:  0.7421875
train loss:  0.48544085025787354
train gradient:  0.42181081499294826
iteration : 1234
train acc:  0.765625
train loss:  0.4205063581466675
train gradient:  0.29833373948497127
iteration : 1235
train acc:  0.7421875
train loss:  0.4764366149902344
train gradient:  0.482587172665058
iteration : 1236
train acc:  0.8203125
train loss:  0.37293750047683716
train gradient:  0.35094832001439635
iteration : 1237
train acc:  0.765625
train loss:  0.45763468742370605
train gradient:  0.4841949883426964
iteration : 1238
train acc:  0.765625
train loss:  0.42874178290367126
train gradient:  0.37727246751443605
iteration : 1239
train acc:  0.7578125
train loss:  0.5163049697875977
train gradient:  0.7285519367005473
iteration : 1240
train acc:  0.7890625
train loss:  0.4812593162059784
train gradient:  0.6773755639964862
iteration : 1241
train acc:  0.8359375
train loss:  0.39059051871299744
train gradient:  0.48149180458403384
iteration : 1242
train acc:  0.796875
train loss:  0.4143156409263611
train gradient:  0.3725527564182206
iteration : 1243
train acc:  0.7734375
train loss:  0.5410616397857666
train gradient:  0.6574208741612542
iteration : 1244
train acc:  0.796875
train loss:  0.4602094292640686
train gradient:  0.43275645870906615
iteration : 1245
train acc:  0.828125
train loss:  0.37831079959869385
train gradient:  0.3379944149736109
iteration : 1246
train acc:  0.796875
train loss:  0.42749351263046265
train gradient:  0.40046622660926295
iteration : 1247
train acc:  0.7890625
train loss:  0.4165037274360657
train gradient:  0.44194107053544923
iteration : 1248
train acc:  0.8046875
train loss:  0.42067182064056396
train gradient:  0.3414418731245876
iteration : 1249
train acc:  0.734375
train loss:  0.46798813343048096
train gradient:  0.4371966246053473
iteration : 1250
train acc:  0.7734375
train loss:  0.501282811164856
train gradient:  0.6005058328468209
iteration : 1251
train acc:  0.8046875
train loss:  0.4073702096939087
train gradient:  0.3663390299289193
iteration : 1252
train acc:  0.8125
train loss:  0.43684375286102295
train gradient:  0.5512178733961585
iteration : 1253
train acc:  0.7890625
train loss:  0.4274636507034302
train gradient:  0.39056477322410355
iteration : 1254
train acc:  0.8125
train loss:  0.42499661445617676
train gradient:  0.46285722917907796
iteration : 1255
train acc:  0.8125
train loss:  0.4019530117511749
train gradient:  0.3975638358210773
iteration : 1256
train acc:  0.8359375
train loss:  0.38204139471054077
train gradient:  0.4052296147661934
iteration : 1257
train acc:  0.8046875
train loss:  0.40613678097724915
train gradient:  0.40619075245821695
iteration : 1258
train acc:  0.75
train loss:  0.4720744490623474
train gradient:  0.45046713277277883
iteration : 1259
train acc:  0.8046875
train loss:  0.40097934007644653
train gradient:  0.3601295305170412
iteration : 1260
train acc:  0.7734375
train loss:  0.44293707609176636
train gradient:  0.38655770131204403
iteration : 1261
train acc:  0.8046875
train loss:  0.42329177260398865
train gradient:  0.36667195043505174
iteration : 1262
train acc:  0.75
train loss:  0.5139732360839844
train gradient:  0.5602430648136829
iteration : 1263
train acc:  0.828125
train loss:  0.39690107107162476
train gradient:  0.34335671293136666
iteration : 1264
train acc:  0.78125
train loss:  0.406002402305603
train gradient:  0.3362319088211559
iteration : 1265
train acc:  0.765625
train loss:  0.5198315382003784
train gradient:  0.8297687994219198
iteration : 1266
train acc:  0.7578125
train loss:  0.5470715761184692
train gradient:  0.7302517686925218
iteration : 1267
train acc:  0.7890625
train loss:  0.44175073504447937
train gradient:  0.4668274165305223
iteration : 1268
train acc:  0.7578125
train loss:  0.5478658080101013
train gradient:  0.716423279761425
iteration : 1269
train acc:  0.78125
train loss:  0.4181181490421295
train gradient:  0.4018023661950302
iteration : 1270
train acc:  0.765625
train loss:  0.4898827075958252
train gradient:  0.5798890942793765
iteration : 1271
train acc:  0.84375
train loss:  0.3717840313911438
train gradient:  0.44979646971479187
iteration : 1272
train acc:  0.765625
train loss:  0.4799751043319702
train gradient:  0.6082356273135062
iteration : 1273
train acc:  0.828125
train loss:  0.41103243827819824
train gradient:  0.47349309522475036
iteration : 1274
train acc:  0.796875
train loss:  0.44014137983322144
train gradient:  0.3861746122905152
iteration : 1275
train acc:  0.765625
train loss:  0.4991984963417053
train gradient:  0.5322929523962949
iteration : 1276
train acc:  0.7421875
train loss:  0.4526897668838501
train gradient:  0.5688373171716066
iteration : 1277
train acc:  0.796875
train loss:  0.45529913902282715
train gradient:  0.4683939492061796
iteration : 1278
train acc:  0.7890625
train loss:  0.43568652868270874
train gradient:  0.48792155836935014
iteration : 1279
train acc:  0.828125
train loss:  0.3735210597515106
train gradient:  0.3040635501840863
iteration : 1280
train acc:  0.78125
train loss:  0.4199930429458618
train gradient:  0.4275110444157336
iteration : 1281
train acc:  0.8515625
train loss:  0.3842146396636963
train gradient:  0.549380388232074
iteration : 1282
train acc:  0.8359375
train loss:  0.36418333649635315
train gradient:  0.41483424642250105
iteration : 1283
train acc:  0.8359375
train loss:  0.37112608551979065
train gradient:  0.32853674153812756
iteration : 1284
train acc:  0.8671875
train loss:  0.34128087759017944
train gradient:  0.39741872439276765
iteration : 1285
train acc:  0.8203125
train loss:  0.46914923191070557
train gradient:  0.48529439831401355
iteration : 1286
train acc:  0.7421875
train loss:  0.532279372215271
train gradient:  0.6098239384787194
iteration : 1287
train acc:  0.796875
train loss:  0.43060219287872314
train gradient:  0.45733289993515336
iteration : 1288
train acc:  0.8046875
train loss:  0.41170406341552734
train gradient:  0.46592395860210384
iteration : 1289
train acc:  0.7109375
train loss:  0.5835768580436707
train gradient:  0.8494527249405419
iteration : 1290
train acc:  0.8046875
train loss:  0.43217095732688904
train gradient:  0.42135809421177833
iteration : 1291
train acc:  0.8515625
train loss:  0.35663914680480957
train gradient:  0.38837339005968846
iteration : 1292
train acc:  0.8203125
train loss:  0.4077547788619995
train gradient:  0.5238714601670318
iteration : 1293
train acc:  0.6953125
train loss:  0.5832126140594482
train gradient:  0.9305978521965431
iteration : 1294
train acc:  0.796875
train loss:  0.41075801849365234
train gradient:  0.319139235602381
iteration : 1295
train acc:  0.8125
train loss:  0.41302186250686646
train gradient:  0.4153443023696291
iteration : 1296
train acc:  0.7734375
train loss:  0.44255316257476807
train gradient:  0.42535951920651743
iteration : 1297
train acc:  0.78125
train loss:  0.4300885796546936
train gradient:  0.34650849718426313
iteration : 1298
train acc:  0.8359375
train loss:  0.4106065034866333
train gradient:  0.4521746435614617
iteration : 1299
train acc:  0.765625
train loss:  0.4747064709663391
train gradient:  0.43850916569242904
iteration : 1300
train acc:  0.8203125
train loss:  0.38002556562423706
train gradient:  0.3558119622538244
iteration : 1301
train acc:  0.78125
train loss:  0.4843440353870392
train gradient:  0.7773819162868433
iteration : 1302
train acc:  0.796875
train loss:  0.4675478935241699
train gradient:  0.6192584663644608
iteration : 1303
train acc:  0.8125
train loss:  0.4274771511554718
train gradient:  0.41535075292210394
iteration : 1304
train acc:  0.8203125
train loss:  0.4110105037689209
train gradient:  0.3504484860889274
iteration : 1305
train acc:  0.8046875
train loss:  0.42108315229415894
train gradient:  0.38372783214926093
iteration : 1306
train acc:  0.78125
train loss:  0.46328696608543396
train gradient:  0.5459059862962437
iteration : 1307
train acc:  0.75
train loss:  0.47394615411758423
train gradient:  0.6668460053807903
iteration : 1308
train acc:  0.8046875
train loss:  0.39466676115989685
train gradient:  0.4395947238232795
iteration : 1309
train acc:  0.78125
train loss:  0.5001546144485474
train gradient:  0.5880472746952528
iteration : 1310
train acc:  0.828125
train loss:  0.40298864245414734
train gradient:  0.4509240835921168
iteration : 1311
train acc:  0.75
train loss:  0.5128064751625061
train gradient:  0.44929078747184675
iteration : 1312
train acc:  0.828125
train loss:  0.37737011909484863
train gradient:  0.3339413461579159
iteration : 1313
train acc:  0.7734375
train loss:  0.40459632873535156
train gradient:  0.4047554240174299
iteration : 1314
train acc:  0.7265625
train loss:  0.5540325045585632
train gradient:  0.8826058276667725
iteration : 1315
train acc:  0.7578125
train loss:  0.5072809457778931
train gradient:  0.5122893989428443
iteration : 1316
train acc:  0.8046875
train loss:  0.44657444953918457
train gradient:  0.5483752057056993
iteration : 1317
train acc:  0.78125
train loss:  0.4273529052734375
train gradient:  0.475398115358252
iteration : 1318
train acc:  0.7890625
train loss:  0.44288143515586853
train gradient:  0.5129616608860857
iteration : 1319
train acc:  0.8125
train loss:  0.4070286154747009
train gradient:  0.4350921340354983
iteration : 1320
train acc:  0.859375
train loss:  0.3498811423778534
train gradient:  0.33516157469609076
iteration : 1321
train acc:  0.7890625
train loss:  0.49497532844543457
train gradient:  0.6290563398138309
iteration : 1322
train acc:  0.828125
train loss:  0.41530096530914307
train gradient:  0.3914928471427986
iteration : 1323
train acc:  0.84375
train loss:  0.38265442848205566
train gradient:  0.30209790792219426
iteration : 1324
train acc:  0.78125
train loss:  0.4668084979057312
train gradient:  0.5949806409439531
iteration : 1325
train acc:  0.7890625
train loss:  0.4328939914703369
train gradient:  0.37815928401447757
iteration : 1326
train acc:  0.75
train loss:  0.5045191049575806
train gradient:  0.7074718134952074
iteration : 1327
train acc:  0.7890625
train loss:  0.4506690502166748
train gradient:  0.5856694029674393
iteration : 1328
train acc:  0.8203125
train loss:  0.41721010208129883
train gradient:  0.41625766600958647
iteration : 1329
train acc:  0.7734375
train loss:  0.4765588045120239
train gradient:  0.46916220174087836
iteration : 1330
train acc:  0.8203125
train loss:  0.3963024318218231
train gradient:  0.37686559939112824
iteration : 1331
train acc:  0.84375
train loss:  0.3732443153858185
train gradient:  0.3349082952858032
iteration : 1332
train acc:  0.7734375
train loss:  0.4488895833492279
train gradient:  0.4875585886896214
iteration : 1333
train acc:  0.8203125
train loss:  0.4190552234649658
train gradient:  0.39349877733838323
iteration : 1334
train acc:  0.75
train loss:  0.43275949358940125
train gradient:  0.4242163133034731
iteration : 1335
train acc:  0.8125
train loss:  0.439388245344162
train gradient:  0.4314790559406199
iteration : 1336
train acc:  0.8125
train loss:  0.40412166714668274
train gradient:  0.351161004311881
iteration : 1337
train acc:  0.7421875
train loss:  0.4862399101257324
train gradient:  0.6048081490685355
iteration : 1338
train acc:  0.765625
train loss:  0.45618385076522827
train gradient:  0.41074248683537534
iteration : 1339
train acc:  0.765625
train loss:  0.4327995479106903
train gradient:  0.7092990941939749
iteration : 1340
train acc:  0.7890625
train loss:  0.4718141555786133
train gradient:  0.5650754109777858
iteration : 1341
train acc:  0.75
train loss:  0.4886597692966461
train gradient:  0.5520650858070766
iteration : 1342
train acc:  0.8046875
train loss:  0.44545698165893555
train gradient:  0.48927358684329625
iteration : 1343
train acc:  0.7734375
train loss:  0.4705125689506531
train gradient:  0.45751228152960194
iteration : 1344
train acc:  0.8125
train loss:  0.4206606149673462
train gradient:  0.3992099556875389
iteration : 1345
train acc:  0.765625
train loss:  0.4550395905971527
train gradient:  0.4581070430868244
iteration : 1346
train acc:  0.8359375
train loss:  0.4013119637966156
train gradient:  0.446581018114515
iteration : 1347
train acc:  0.8359375
train loss:  0.3973744213581085
train gradient:  0.4374075043401841
iteration : 1348
train acc:  0.828125
train loss:  0.37476062774658203
train gradient:  0.40943152457087856
iteration : 1349
train acc:  0.78125
train loss:  0.47119301557540894
train gradient:  0.5321962657036078
iteration : 1350
train acc:  0.7890625
train loss:  0.4064417779445648
train gradient:  0.4265835577917196
iteration : 1351
train acc:  0.8046875
train loss:  0.4412960410118103
train gradient:  0.44151284563640075
iteration : 1352
train acc:  0.7734375
train loss:  0.484741747379303
train gradient:  0.5588414951138576
iteration : 1353
train acc:  0.7265625
train loss:  0.5445365905761719
train gradient:  0.5242761458288143
iteration : 1354
train acc:  0.8359375
train loss:  0.3563334345817566
train gradient:  0.37269840775169866
iteration : 1355
train acc:  0.765625
train loss:  0.4348689317703247
train gradient:  0.5720274151911745
iteration : 1356
train acc:  0.8046875
train loss:  0.42650771141052246
train gradient:  0.46559244229015334
iteration : 1357
train acc:  0.8125
train loss:  0.3996981084346771
train gradient:  0.31958051534274573
iteration : 1358
train acc:  0.7890625
train loss:  0.45006251335144043
train gradient:  0.46728771758105087
iteration : 1359
train acc:  0.78125
train loss:  0.44967395067214966
train gradient:  0.424872628005729
iteration : 1360
train acc:  0.7421875
train loss:  0.4684315025806427
train gradient:  0.5394881648661126
iteration : 1361
train acc:  0.7890625
train loss:  0.40997374057769775
train gradient:  0.4872464073188589
iteration : 1362
train acc:  0.734375
train loss:  0.4621262550354004
train gradient:  0.5034200073261711
iteration : 1363
train acc:  0.7890625
train loss:  0.4112064838409424
train gradient:  0.424296360250349
iteration : 1364
train acc:  0.7734375
train loss:  0.4520654082298279
train gradient:  0.5878217411033936
iteration : 1365
train acc:  0.78125
train loss:  0.450512558221817
train gradient:  0.5621660575680536
iteration : 1366
train acc:  0.7578125
train loss:  0.45350176095962524
train gradient:  0.5316591269333646
iteration : 1367
train acc:  0.78125
train loss:  0.4614636301994324
train gradient:  0.542211750867235
iteration : 1368
train acc:  0.8359375
train loss:  0.41171184182167053
train gradient:  0.32096099877403605
iteration : 1369
train acc:  0.765625
train loss:  0.47938740253448486
train gradient:  0.5806071674199729
iteration : 1370
train acc:  0.8515625
train loss:  0.3688187599182129
train gradient:  0.33598239390757995
iteration : 1371
train acc:  0.828125
train loss:  0.40634363889694214
train gradient:  0.39788572852451204
iteration : 1372
train acc:  0.765625
train loss:  0.4948219656944275
train gradient:  0.5371058388776093
iteration : 1373
train acc:  0.7890625
train loss:  0.39176470041275024
train gradient:  0.5313427969250888
iteration : 1374
train acc:  0.8203125
train loss:  0.38760972023010254
train gradient:  0.40114393205653004
iteration : 1375
train acc:  0.7890625
train loss:  0.44157296419143677
train gradient:  0.4571577384068332
iteration : 1376
train acc:  0.8671875
train loss:  0.34741562604904175
train gradient:  0.28730990796370526
iteration : 1377
train acc:  0.7421875
train loss:  0.4997413158416748
train gradient:  0.563927469024361
iteration : 1378
train acc:  0.7734375
train loss:  0.4335199296474457
train gradient:  0.5265820874486444
iteration : 1379
train acc:  0.8203125
train loss:  0.4362194538116455
train gradient:  0.3640185087574481
iteration : 1380
train acc:  0.7578125
train loss:  0.5333874821662903
train gradient:  0.6339741245350241
iteration : 1381
train acc:  0.7421875
train loss:  0.5365439653396606
train gradient:  0.5389835375691803
iteration : 1382
train acc:  0.796875
train loss:  0.42331552505493164
train gradient:  0.3860632420268382
iteration : 1383
train acc:  0.84375
train loss:  0.38073405623435974
train gradient:  0.30466572300327727
iteration : 1384
train acc:  0.7109375
train loss:  0.4676458239555359
train gradient:  0.44286340675742597
iteration : 1385
train acc:  0.8203125
train loss:  0.4266541600227356
train gradient:  0.5352721982972861
iteration : 1386
train acc:  0.7734375
train loss:  0.4172208309173584
train gradient:  0.44078288484802985
iteration : 1387
train acc:  0.7421875
train loss:  0.5218353271484375
train gradient:  0.5547264505609933
iteration : 1388
train acc:  0.7734375
train loss:  0.494398832321167
train gradient:  0.5572535724877368
iteration : 1389
train acc:  0.765625
train loss:  0.5013821721076965
train gradient:  0.5707823175844573
iteration : 1390
train acc:  0.7109375
train loss:  0.47825297713279724
train gradient:  0.5321906498695956
iteration : 1391
train acc:  0.8359375
train loss:  0.3725646734237671
train gradient:  0.39413454438871776
iteration : 1392
train acc:  0.84375
train loss:  0.4149005711078644
train gradient:  0.41054662227822114
iteration : 1393
train acc:  0.8046875
train loss:  0.43953582644462585
train gradient:  0.4754648113332627
iteration : 1394
train acc:  0.8359375
train loss:  0.3845772445201874
train gradient:  0.3141011298441221
iteration : 1395
train acc:  0.7734375
train loss:  0.4499005675315857
train gradient:  0.5366157602761176
iteration : 1396
train acc:  0.7734375
train loss:  0.46319329738616943
train gradient:  0.47667456407833025
iteration : 1397
train acc:  0.78125
train loss:  0.5118857622146606
train gradient:  0.5871592778227752
iteration : 1398
train acc:  0.78125
train loss:  0.4840506315231323
train gradient:  0.6170600176733574
iteration : 1399
train acc:  0.828125
train loss:  0.3637286126613617
train gradient:  0.2608503276047546
iteration : 1400
train acc:  0.8046875
train loss:  0.42457708716392517
train gradient:  0.4059224717936067
iteration : 1401
train acc:  0.7734375
train loss:  0.5050874948501587
train gradient:  0.7196957764382089
iteration : 1402
train acc:  0.765625
train loss:  0.4679901897907257
train gradient:  0.5357488381880934
iteration : 1403
train acc:  0.796875
train loss:  0.4586159586906433
train gradient:  0.43907055537182177
iteration : 1404
train acc:  0.8125
train loss:  0.42944085597991943
train gradient:  0.4115159609157833
iteration : 1405
train acc:  0.75
train loss:  0.498119980096817
train gradient:  0.5390653992262899
iteration : 1406
train acc:  0.78125
train loss:  0.48093974590301514
train gradient:  0.5330945757338783
iteration : 1407
train acc:  0.796875
train loss:  0.4587518870830536
train gradient:  0.5341460275674011
iteration : 1408
train acc:  0.796875
train loss:  0.450735479593277
train gradient:  0.45799816351211237
iteration : 1409
train acc:  0.796875
train loss:  0.4251733124256134
train gradient:  0.4608165475574163
iteration : 1410
train acc:  0.8046875
train loss:  0.4362630844116211
train gradient:  0.4179169586628938
iteration : 1411
train acc:  0.8046875
train loss:  0.45205458998680115
train gradient:  0.42487029773001456
iteration : 1412
train acc:  0.7421875
train loss:  0.48109984397888184
train gradient:  0.4362308335233674
iteration : 1413
train acc:  0.796875
train loss:  0.4400789737701416
train gradient:  0.43743647212667347
iteration : 1414
train acc:  0.796875
train loss:  0.4081735610961914
train gradient:  0.48867508284468253
iteration : 1415
train acc:  0.796875
train loss:  0.4737761318683624
train gradient:  0.4770045029530255
iteration : 1416
train acc:  0.8203125
train loss:  0.41455405950546265
train gradient:  0.4284292214368502
iteration : 1417
train acc:  0.8046875
train loss:  0.40502166748046875
train gradient:  0.44917535366633254
iteration : 1418
train acc:  0.8359375
train loss:  0.4221469461917877
train gradient:  0.44209223194594727
iteration : 1419
train acc:  0.8203125
train loss:  0.4482387900352478
train gradient:  0.39185692716603404
iteration : 1420
train acc:  0.8203125
train loss:  0.42346036434173584
train gradient:  0.44051848007154315
iteration : 1421
train acc:  0.796875
train loss:  0.41233354806900024
train gradient:  0.43026111518349375
iteration : 1422
train acc:  0.796875
train loss:  0.47022733092308044
train gradient:  0.6993017636097305
iteration : 1423
train acc:  0.84375
train loss:  0.38774096965789795
train gradient:  0.29333296827277017
iteration : 1424
train acc:  0.7265625
train loss:  0.5416368246078491
train gradient:  0.5091073683449875
iteration : 1425
train acc:  0.78125
train loss:  0.47164827585220337
train gradient:  0.46929470236930515
iteration : 1426
train acc:  0.8125
train loss:  0.42518216371536255
train gradient:  0.3079828158949553
iteration : 1427
train acc:  0.796875
train loss:  0.41357994079589844
train gradient:  0.4506943790952346
iteration : 1428
train acc:  0.796875
train loss:  0.4490275979042053
train gradient:  0.627855514531924
iteration : 1429
train acc:  0.6640625
train loss:  0.5382077693939209
train gradient:  0.6985974397672909
iteration : 1430
train acc:  0.78125
train loss:  0.4401540756225586
train gradient:  0.32897524509380477
iteration : 1431
train acc:  0.84375
train loss:  0.3941093683242798
train gradient:  0.34975411409968743
iteration : 1432
train acc:  0.796875
train loss:  0.42387855052948
train gradient:  0.5715773522500214
iteration : 1433
train acc:  0.796875
train loss:  0.3902551531791687
train gradient:  0.39462795933206535
iteration : 1434
train acc:  0.796875
train loss:  0.41164425015449524
train gradient:  0.46970238426994654
iteration : 1435
train acc:  0.875
train loss:  0.34728720784187317
train gradient:  0.31867341487568246
iteration : 1436
train acc:  0.828125
train loss:  0.37045103311538696
train gradient:  0.36885182313494247
iteration : 1437
train acc:  0.875
train loss:  0.366137832403183
train gradient:  0.3192803640913603
iteration : 1438
train acc:  0.8046875
train loss:  0.40144777297973633
train gradient:  0.7777435074357268
iteration : 1439
train acc:  0.796875
train loss:  0.3977755308151245
train gradient:  0.3830494839147924
iteration : 1440
train acc:  0.7890625
train loss:  0.4230062961578369
train gradient:  0.3894769389496596
iteration : 1441
train acc:  0.7578125
train loss:  0.4699390232563019
train gradient:  0.5834269817573322
iteration : 1442
train acc:  0.8203125
train loss:  0.40887147188186646
train gradient:  0.47063785973938327
iteration : 1443
train acc:  0.8359375
train loss:  0.3595421612262726
train gradient:  0.25802688445157934
iteration : 1444
train acc:  0.8203125
train loss:  0.4239905774593353
train gradient:  0.5920422331150105
iteration : 1445
train acc:  0.8125
train loss:  0.4119442105293274
train gradient:  0.4123950392127837
iteration : 1446
train acc:  0.796875
train loss:  0.40605229139328003
train gradient:  0.4274992785553087
iteration : 1447
train acc:  0.7421875
train loss:  0.49677786231040955
train gradient:  0.5640522497627222
iteration : 1448
train acc:  0.8125
train loss:  0.44952887296676636
train gradient:  0.3561248529891973
iteration : 1449
train acc:  0.8046875
train loss:  0.4289332628250122
train gradient:  0.5287148162950739
iteration : 1450
train acc:  0.7265625
train loss:  0.5477154850959778
train gradient:  0.6057854012538433
iteration : 1451
train acc:  0.859375
train loss:  0.33550214767456055
train gradient:  0.33550126163761873
iteration : 1452
train acc:  0.8046875
train loss:  0.4144867956638336
train gradient:  0.4257701810110014
iteration : 1453
train acc:  0.828125
train loss:  0.3921867311000824
train gradient:  0.40044479879591327
iteration : 1454
train acc:  0.8828125
train loss:  0.3406843841075897
train gradient:  0.30182216304516024
iteration : 1455
train acc:  0.8203125
train loss:  0.4052128791809082
train gradient:  0.40588814351817953
iteration : 1456
train acc:  0.84375
train loss:  0.3727337121963501
train gradient:  0.3308851414362671
iteration : 1457
train acc:  0.796875
train loss:  0.45591533184051514
train gradient:  0.3191191706685671
iteration : 1458
train acc:  0.828125
train loss:  0.3847140073776245
train gradient:  0.40509417785104357
iteration : 1459
train acc:  0.8359375
train loss:  0.40884092450141907
train gradient:  0.5046755975507149
iteration : 1460
train acc:  0.859375
train loss:  0.32750558853149414
train gradient:  0.28015803850721843
iteration : 1461
train acc:  0.7890625
train loss:  0.4297593832015991
train gradient:  0.5678956751599215
iteration : 1462
train acc:  0.8125
train loss:  0.4644134044647217
train gradient:  0.5125639578559291
iteration : 1463
train acc:  0.828125
train loss:  0.4235067367553711
train gradient:  0.38273692373358587
iteration : 1464
train acc:  0.8828125
train loss:  0.33898136019706726
train gradient:  0.305980784440138
iteration : 1465
train acc:  0.7890625
train loss:  0.5168777704238892
train gradient:  0.6261889966930988
iteration : 1466
train acc:  0.7578125
train loss:  0.5610808730125427
train gradient:  0.8090822846959611
iteration : 1467
train acc:  0.796875
train loss:  0.3707233667373657
train gradient:  0.33986179463459126
iteration : 1468
train acc:  0.7890625
train loss:  0.41961920261383057
train gradient:  0.4138370036209631
iteration : 1469
train acc:  0.8515625
train loss:  0.38293761014938354
train gradient:  0.4322232700949885
iteration : 1470
train acc:  0.875
train loss:  0.3071082532405853
train gradient:  0.4612419500338556
iteration : 1471
train acc:  0.8671875
train loss:  0.32295453548431396
train gradient:  0.28081239567872146
iteration : 1472
train acc:  0.8359375
train loss:  0.3562280535697937
train gradient:  0.3921992462288069
iteration : 1473
train acc:  0.8125
train loss:  0.38405054807662964
train gradient:  0.4216592662630678
iteration : 1474
train acc:  0.8125
train loss:  0.37725692987442017
train gradient:  0.6868707012872963
iteration : 1475
train acc:  0.78125
train loss:  0.41759106516838074
train gradient:  0.5639128335770704
iteration : 1476
train acc:  0.8125
train loss:  0.4566947817802429
train gradient:  0.40394886924766993
iteration : 1477
train acc:  0.796875
train loss:  0.3822208046913147
train gradient:  0.40527477449799687
iteration : 1478
train acc:  0.8359375
train loss:  0.4250800609588623
train gradient:  0.3587339387091973
iteration : 1479
train acc:  0.8515625
train loss:  0.3496808409690857
train gradient:  0.5470138300592173
iteration : 1480
train acc:  0.7890625
train loss:  0.4673800766468048
train gradient:  0.5127781992662432
iteration : 1481
train acc:  0.796875
train loss:  0.44347500801086426
train gradient:  0.5003378796953522
iteration : 1482
train acc:  0.765625
train loss:  0.3885765075683594
train gradient:  0.5883154481653696
iteration : 1483
train acc:  0.78125
train loss:  0.4468459188938141
train gradient:  0.6108999369552176
iteration : 1484
train acc:  0.8203125
train loss:  0.36742791533470154
train gradient:  0.340916517782203
iteration : 1485
train acc:  0.796875
train loss:  0.3911275863647461
train gradient:  0.4387639322294263
iteration : 1486
train acc:  0.7578125
train loss:  0.453340083360672
train gradient:  0.507544212581847
iteration : 1487
train acc:  0.8515625
train loss:  0.3081549406051636
train gradient:  0.34829326422209006
iteration : 1488
train acc:  0.828125
train loss:  0.35764068365097046
train gradient:  0.5128679204451383
iteration : 1489
train acc:  0.8125
train loss:  0.4092057943344116
train gradient:  0.5580796378861543
iteration : 1490
train acc:  0.859375
train loss:  0.32441484928131104
train gradient:  0.28966443735649233
iteration : 1491
train acc:  0.8125
train loss:  0.41683173179626465
train gradient:  0.47179613900646633
iteration : 1492
train acc:  0.7734375
train loss:  0.4110994338989258
train gradient:  0.5125169409574267
iteration : 1493
train acc:  0.7734375
train loss:  0.5020421743392944
train gradient:  0.603626378090485
iteration : 1494
train acc:  0.859375
train loss:  0.3407154381275177
train gradient:  0.35069218526633356
iteration : 1495
train acc:  0.765625
train loss:  0.4443679451942444
train gradient:  0.5585965117150972
iteration : 1496
train acc:  0.8515625
train loss:  0.36501258611679077
train gradient:  0.4006430227603722
iteration : 1497
train acc:  0.78125
train loss:  0.4531377851963043
train gradient:  0.48162654729549476
iteration : 1498
train acc:  0.8125
train loss:  0.4562826156616211
train gradient:  0.4163522634766112
iteration : 1499
train acc:  0.8359375
train loss:  0.38500604033470154
train gradient:  0.5430540459397935
iteration : 1500
train acc:  0.8046875
train loss:  0.41930416226387024
train gradient:  0.4843356217537267
iteration : 1501
train acc:  0.8203125
train loss:  0.4141000509262085
train gradient:  0.3694027694653325
iteration : 1502
train acc:  0.7734375
train loss:  0.4463750123977661
train gradient:  0.5469921883246076
iteration : 1503
train acc:  0.7421875
train loss:  0.48986825346946716
train gradient:  0.49832075337982135
iteration : 1504
train acc:  0.7890625
train loss:  0.43185824155807495
train gradient:  0.546757384694392
iteration : 1505
train acc:  0.828125
train loss:  0.4055178761482239
train gradient:  0.3993762832664429
iteration : 1506
train acc:  0.84375
train loss:  0.3949414789676666
train gradient:  0.4602978283476485
iteration : 1507
train acc:  0.7734375
train loss:  0.46553942561149597
train gradient:  0.6041778257872688
iteration : 1508
train acc:  0.78125
train loss:  0.4823952913284302
train gradient:  0.45642302405903296
iteration : 1509
train acc:  0.796875
train loss:  0.4056660234928131
train gradient:  0.40140042355932526
iteration : 1510
train acc:  0.7265625
train loss:  0.5861871242523193
train gradient:  0.8424887801815732
iteration : 1511
train acc:  0.7421875
train loss:  0.4379350543022156
train gradient:  0.4323541194553959
iteration : 1512
train acc:  0.828125
train loss:  0.39113643765449524
train gradient:  0.34441806063675484
iteration : 1513
train acc:  0.828125
train loss:  0.41452354192733765
train gradient:  0.41500159720360497
iteration : 1514
train acc:  0.8046875
train loss:  0.4255741238594055
train gradient:  0.5033578095017495
iteration : 1515
train acc:  0.75
train loss:  0.4978625774383545
train gradient:  0.6650009801927175
iteration : 1516
train acc:  0.7109375
train loss:  0.5059490203857422
train gradient:  0.5685519611653516
iteration : 1517
train acc:  0.8359375
train loss:  0.40725424885749817
train gradient:  0.43549931682855286
iteration : 1518
train acc:  0.8515625
train loss:  0.3746912479400635
train gradient:  0.41338046642301246
iteration : 1519
train acc:  0.7421875
train loss:  0.49391740560531616
train gradient:  0.5612409894407727
iteration : 1520
train acc:  0.7421875
train loss:  0.5153957605361938
train gradient:  0.5704492068124409
iteration : 1521
train acc:  0.796875
train loss:  0.4823119640350342
train gradient:  0.7471491831352604
iteration : 1522
train acc:  0.8125
train loss:  0.45196783542633057
train gradient:  0.665262463397096
iteration : 1523
train acc:  0.78125
train loss:  0.42610082030296326
train gradient:  0.40447767371758314
iteration : 1524
train acc:  0.8046875
train loss:  0.44065341353416443
train gradient:  0.42401849429373833
iteration : 1525
train acc:  0.7421875
train loss:  0.4707143306732178
train gradient:  0.6426055318419441
iteration : 1526
train acc:  0.7421875
train loss:  0.46035683155059814
train gradient:  0.4915810981600943
iteration : 1527
train acc:  0.8125
train loss:  0.4240522086620331
train gradient:  0.3747600969495864
iteration : 1528
train acc:  0.8828125
train loss:  0.3352651000022888
train gradient:  0.3545691755062907
iteration : 1529
train acc:  0.84375
train loss:  0.39943772554397583
train gradient:  0.3800803660644115
iteration : 1530
train acc:  0.7578125
train loss:  0.44866836071014404
train gradient:  0.4960137151550875
iteration : 1531
train acc:  0.75
train loss:  0.5317654609680176
train gradient:  0.5132836954128753
iteration : 1532
train acc:  0.8046875
train loss:  0.46305301785469055
train gradient:  0.660641723806515
iteration : 1533
train acc:  0.8515625
train loss:  0.32331666350364685
train gradient:  0.27508373826322835
iteration : 1534
train acc:  0.828125
train loss:  0.37065672874450684
train gradient:  0.31981396382819555
iteration : 1535
train acc:  0.8046875
train loss:  0.3740231990814209
train gradient:  0.31363985173342185
iteration : 1536
train acc:  0.796875
train loss:  0.43656912446022034
train gradient:  0.38373503358454003
iteration : 1537
train acc:  0.8046875
train loss:  0.42393648624420166
train gradient:  0.5534794767697878
iteration : 1538
train acc:  0.8359375
train loss:  0.35508742928504944
train gradient:  0.3281879880125345
iteration : 1539
train acc:  0.828125
train loss:  0.4569523334503174
train gradient:  0.43765748712943964
iteration : 1540
train acc:  0.78125
train loss:  0.4330669641494751
train gradient:  0.6270529263151322
iteration : 1541
train acc:  0.7890625
train loss:  0.45065200328826904
train gradient:  0.4403287175911003
iteration : 1542
train acc:  0.78125
train loss:  0.4986850321292877
train gradient:  0.5191034974291739
iteration : 1543
train acc:  0.78125
train loss:  0.4679832458496094
train gradient:  0.4923702229534221
iteration : 1544
train acc:  0.8125
train loss:  0.4248991012573242
train gradient:  0.34971655586069195
iteration : 1545
train acc:  0.8125
train loss:  0.3912471532821655
train gradient:  0.36852490196961724
iteration : 1546
train acc:  0.8203125
train loss:  0.4419378638267517
train gradient:  0.3280477296669446
iteration : 1547
train acc:  0.8671875
train loss:  0.35890623927116394
train gradient:  0.29068757991862676
iteration : 1548
train acc:  0.7890625
train loss:  0.44713088870048523
train gradient:  0.4852650239679983
iteration : 1549
train acc:  0.8203125
train loss:  0.4129400849342346
train gradient:  0.4057611747878566
iteration : 1550
train acc:  0.8125
train loss:  0.4664376974105835
train gradient:  0.5003758827339683
iteration : 1551
train acc:  0.859375
train loss:  0.3349097669124603
train gradient:  0.3167116548630119
iteration : 1552
train acc:  0.8203125
train loss:  0.36929792165756226
train gradient:  0.3244529862485619
iteration : 1553
train acc:  0.8125
train loss:  0.39793506264686584
train gradient:  0.33966875685089515
iteration : 1554
train acc:  0.8359375
train loss:  0.3774791359901428
train gradient:  0.37796221214797704
iteration : 1555
train acc:  0.8125
train loss:  0.3967827558517456
train gradient:  0.3253621530241811
iteration : 1556
train acc:  0.828125
train loss:  0.38044917583465576
train gradient:  0.3378501331299022
iteration : 1557
train acc:  0.796875
train loss:  0.43238797783851624
train gradient:  0.5114115913543855
iteration : 1558
train acc:  0.8046875
train loss:  0.40449267625808716
train gradient:  0.44510791402973593
iteration : 1559
train acc:  0.796875
train loss:  0.4410114288330078
train gradient:  0.4313646628886962
iteration : 1560
train acc:  0.8203125
train loss:  0.4355061650276184
train gradient:  0.37963536598004055
iteration : 1561
train acc:  0.796875
train loss:  0.4654659032821655
train gradient:  0.4507817111168522
iteration : 1562
train acc:  0.8046875
train loss:  0.4766065180301666
train gradient:  0.5797795775941763
iteration : 1563
train acc:  0.7734375
train loss:  0.41395139694213867
train gradient:  0.42359081227066203
iteration : 1564
train acc:  0.8125
train loss:  0.4206799268722534
train gradient:  0.6335042485171118
iteration : 1565
train acc:  0.8046875
train loss:  0.36917996406555176
train gradient:  0.3006274171994219
iteration : 1566
train acc:  0.84375
train loss:  0.38842862844467163
train gradient:  0.38968109699657955
iteration : 1567
train acc:  0.8125
train loss:  0.3965038061141968
train gradient:  0.5340045578957013
iteration : 1568
train acc:  0.7890625
train loss:  0.49027198553085327
train gradient:  0.6402349264801233
iteration : 1569
train acc:  0.75
train loss:  0.4698345363140106
train gradient:  0.5659280430870812
iteration : 1570
train acc:  0.75
train loss:  0.46918609738349915
train gradient:  0.46913839229066295
iteration : 1571
train acc:  0.75
train loss:  0.4886282682418823
train gradient:  0.4480571592188205
iteration : 1572
train acc:  0.7890625
train loss:  0.4420696496963501
train gradient:  0.46321220806628616
iteration : 1573
train acc:  0.828125
train loss:  0.450435996055603
train gradient:  0.5362308139402238
iteration : 1574
train acc:  0.7890625
train loss:  0.456299364566803
train gradient:  0.44589793892914664
iteration : 1575
train acc:  0.796875
train loss:  0.4459570050239563
train gradient:  0.40385733232125903
iteration : 1576
train acc:  0.734375
train loss:  0.4882858693599701
train gradient:  0.5561782540335362
iteration : 1577
train acc:  0.8046875
train loss:  0.4350977838039398
train gradient:  0.46011354057376325
iteration : 1578
train acc:  0.8203125
train loss:  0.3823581039905548
train gradient:  0.428984848361061
iteration : 1579
train acc:  0.828125
train loss:  0.37747108936309814
train gradient:  0.3095207777934965
iteration : 1580
train acc:  0.8046875
train loss:  0.4250534176826477
train gradient:  0.34155026462628246
iteration : 1581
train acc:  0.7890625
train loss:  0.42183545231819153
train gradient:  0.434591497079241
iteration : 1582
train acc:  0.8046875
train loss:  0.45822158455848694
train gradient:  0.6747500203896314
iteration : 1583
train acc:  0.8515625
train loss:  0.4152027368545532
train gradient:  0.41910839510023046
iteration : 1584
train acc:  0.8125
train loss:  0.4620274007320404
train gradient:  0.4331814868270496
iteration : 1585
train acc:  0.765625
train loss:  0.4921302795410156
train gradient:  0.4529379002216079
iteration : 1586
train acc:  0.796875
train loss:  0.48056483268737793
train gradient:  0.5747631897407345
iteration : 1587
train acc:  0.8125
train loss:  0.41602325439453125
train gradient:  0.3037574385377064
iteration : 1588
train acc:  0.734375
train loss:  0.46062687039375305
train gradient:  0.545293532320569
iteration : 1589
train acc:  0.7734375
train loss:  0.4382130801677704
train gradient:  0.6936760841985324
iteration : 1590
train acc:  0.828125
train loss:  0.3874184489250183
train gradient:  0.3922593258321761
iteration : 1591
train acc:  0.796875
train loss:  0.4010109305381775
train gradient:  0.3915869527600977
iteration : 1592
train acc:  0.78125
train loss:  0.42427438497543335
train gradient:  0.48699407256156524
iteration : 1593
train acc:  0.8125
train loss:  0.4123799800872803
train gradient:  0.4430957514178351
iteration : 1594
train acc:  0.78125
train loss:  0.4549892246723175
train gradient:  0.4743486315676055
iteration : 1595
train acc:  0.7890625
train loss:  0.4407612681388855
train gradient:  0.39062194251544025
iteration : 1596
train acc:  0.859375
train loss:  0.32373109459877014
train gradient:  0.2862471577815431
iteration : 1597
train acc:  0.78125
train loss:  0.4823096692562103
train gradient:  0.5780172912254715
iteration : 1598
train acc:  0.8125
train loss:  0.4991830587387085
train gradient:  0.4788147629586451
iteration : 1599
train acc:  0.8125
train loss:  0.3948962092399597
train gradient:  0.406432637604896
iteration : 1600
train acc:  0.828125
train loss:  0.41502493619918823
train gradient:  0.3253247522335009
iteration : 1601
train acc:  0.8125
train loss:  0.39472877979278564
train gradient:  0.3228137526933467
iteration : 1602
train acc:  0.7421875
train loss:  0.5212039351463318
train gradient:  0.5561802794607134
iteration : 1603
train acc:  0.71875
train loss:  0.5859137177467346
train gradient:  0.6971631357455788
iteration : 1604
train acc:  0.8515625
train loss:  0.4099450707435608
train gradient:  0.464010852041752
iteration : 1605
train acc:  0.7734375
train loss:  0.4215804934501648
train gradient:  0.43968036064039134
iteration : 1606
train acc:  0.78125
train loss:  0.43072158098220825
train gradient:  0.4705426975865706
iteration : 1607
train acc:  0.8046875
train loss:  0.3894055187702179
train gradient:  0.38313656717398087
iteration : 1608
train acc:  0.7578125
train loss:  0.47258225083351135
train gradient:  0.44787255801625475
iteration : 1609
train acc:  0.7734375
train loss:  0.4450567662715912
train gradient:  0.4427158386485701
iteration : 1610
train acc:  0.8125
train loss:  0.4366927742958069
train gradient:  0.4758167469586011
iteration : 1611
train acc:  0.8515625
train loss:  0.3482820689678192
train gradient:  0.44972668091741175
iteration : 1612
train acc:  0.796875
train loss:  0.409241646528244
train gradient:  0.3747576019861554
iteration : 1613
train acc:  0.78125
train loss:  0.4210060238838196
train gradient:  0.3821119359135372
iteration : 1614
train acc:  0.796875
train loss:  0.48077669739723206
train gradient:  0.3776065348861852
iteration : 1615
train acc:  0.7734375
train loss:  0.484741747379303
train gradient:  0.39556056114864946
iteration : 1616
train acc:  0.7578125
train loss:  0.44309771060943604
train gradient:  0.515122067821124
iteration : 1617
train acc:  0.875
train loss:  0.33601492643356323
train gradient:  0.37689957903679877
iteration : 1618
train acc:  0.8046875
train loss:  0.3936261832714081
train gradient:  0.3579777337585047
iteration : 1619
train acc:  0.8125
train loss:  0.4282852113246918
train gradient:  0.44626164379642513
iteration : 1620
train acc:  0.765625
train loss:  0.4850100874900818
train gradient:  0.6673872024671255
iteration : 1621
train acc:  0.796875
train loss:  0.40567004680633545
train gradient:  0.3907677917935349
iteration : 1622
train acc:  0.84375
train loss:  0.4160027503967285
train gradient:  0.42924563204156024
iteration : 1623
train acc:  0.8046875
train loss:  0.46170416474342346
train gradient:  0.4933158515236295
iteration : 1624
train acc:  0.78125
train loss:  0.4758014678955078
train gradient:  0.6792527171237448
iteration : 1625
train acc:  0.8671875
train loss:  0.35193932056427
train gradient:  0.3524924888521212
iteration : 1626
train acc:  0.8046875
train loss:  0.3533996343612671
train gradient:  0.3369551500410201
iteration : 1627
train acc:  0.765625
train loss:  0.5238556861877441
train gradient:  0.6189856187310665
iteration : 1628
train acc:  0.75
train loss:  0.4951993227005005
train gradient:  0.46555395501739244
iteration : 1629
train acc:  0.78125
train loss:  0.5145191550254822
train gradient:  0.5232623991977792
iteration : 1630
train acc:  0.7265625
train loss:  0.5120893120765686
train gradient:  0.4393755591401103
iteration : 1631
train acc:  0.796875
train loss:  0.47140687704086304
train gradient:  0.5499712407940598
iteration : 1632
train acc:  0.7890625
train loss:  0.446280837059021
train gradient:  0.41513166710274185
iteration : 1633
train acc:  0.8046875
train loss:  0.40888911485671997
train gradient:  0.39261917055680245
iteration : 1634
train acc:  0.84375
train loss:  0.35400235652923584
train gradient:  0.23836811747379144
iteration : 1635
train acc:  0.8203125
train loss:  0.3943418562412262
train gradient:  0.4508995862623739
iteration : 1636
train acc:  0.8203125
train loss:  0.4177076518535614
train gradient:  0.4390476249841841
iteration : 1637
train acc:  0.78125
train loss:  0.4301573932170868
train gradient:  0.3271423072017822
iteration : 1638
train acc:  0.7578125
train loss:  0.4828108847141266
train gradient:  0.6299956114382879
iteration : 1639
train acc:  0.78125
train loss:  0.39680957794189453
train gradient:  0.4334994401669395
iteration : 1640
train acc:  0.875
train loss:  0.3301081657409668
train gradient:  0.31806769721317135
iteration : 1641
train acc:  0.78125
train loss:  0.4900294542312622
train gradient:  0.8675559621848407
iteration : 1642
train acc:  0.8203125
train loss:  0.3797755837440491
train gradient:  0.43205809225614283
iteration : 1643
train acc:  0.828125
train loss:  0.35269322991371155
train gradient:  0.3551857308255033
iteration : 1644
train acc:  0.84375
train loss:  0.36028385162353516
train gradient:  0.31716445057871046
iteration : 1645
train acc:  0.7578125
train loss:  0.519534170627594
train gradient:  0.5281015781662836
iteration : 1646
train acc:  0.8671875
train loss:  0.35745394229888916
train gradient:  0.42858380096538196
iteration : 1647
train acc:  0.7734375
train loss:  0.42264610528945923
train gradient:  0.3799018172775563
iteration : 1648
train acc:  0.8203125
train loss:  0.42437654733657837
train gradient:  0.5233109869323721
iteration : 1649
train acc:  0.828125
train loss:  0.4122508466243744
train gradient:  0.45430985116010203
iteration : 1650
train acc:  0.7890625
train loss:  0.46310949325561523
train gradient:  0.43059541556018455
iteration : 1651
train acc:  0.765625
train loss:  0.47890225052833557
train gradient:  0.5940769126153326
iteration : 1652
train acc:  0.7890625
train loss:  0.4326792359352112
train gradient:  0.41244976080094875
iteration : 1653
train acc:  0.765625
train loss:  0.5143074989318848
train gradient:  0.5301864156208039
iteration : 1654
train acc:  0.765625
train loss:  0.42742785811424255
train gradient:  0.38237889874877146
iteration : 1655
train acc:  0.796875
train loss:  0.49079570174217224
train gradient:  0.47669851923815665
iteration : 1656
train acc:  0.796875
train loss:  0.4380650520324707
train gradient:  0.3936498969947778
iteration : 1657
train acc:  0.8203125
train loss:  0.4485781192779541
train gradient:  0.5332107000791585
iteration : 1658
train acc:  0.78125
train loss:  0.44334205985069275
train gradient:  0.5073364259073214
iteration : 1659
train acc:  0.78125
train loss:  0.5060372352600098
train gradient:  0.6117653856170301
iteration : 1660
train acc:  0.84375
train loss:  0.3786953389644623
train gradient:  0.40859779517573935
iteration : 1661
train acc:  0.765625
train loss:  0.41355669498443604
train gradient:  0.38741601730061154
iteration : 1662
train acc:  0.7890625
train loss:  0.39338165521621704
train gradient:  0.31959356252184623
iteration : 1663
train acc:  0.8125
train loss:  0.41187357902526855
train gradient:  0.44774081192341425
iteration : 1664
train acc:  0.8046875
train loss:  0.38886696100234985
train gradient:  0.35753949001187035
iteration : 1665
train acc:  0.796875
train loss:  0.4276476800441742
train gradient:  0.3731810626323659
iteration : 1666
train acc:  0.796875
train loss:  0.44703444838523865
train gradient:  0.40806319558574533
iteration : 1667
train acc:  0.7578125
train loss:  0.5648490190505981
train gradient:  0.6461940777252843
iteration : 1668
train acc:  0.8046875
train loss:  0.4063049554824829
train gradient:  0.30769424027214776
iteration : 1669
train acc:  0.796875
train loss:  0.4058569371700287
train gradient:  0.3572505496707743
iteration : 1670
train acc:  0.765625
train loss:  0.5178029537200928
train gradient:  0.5575205613324306
iteration : 1671
train acc:  0.84375
train loss:  0.35454750061035156
train gradient:  0.7549254111745469
iteration : 1672
train acc:  0.796875
train loss:  0.4721260070800781
train gradient:  0.3976625297219868
iteration : 1673
train acc:  0.78125
train loss:  0.41073447465896606
train gradient:  0.3473220589717438
iteration : 1674
train acc:  0.78125
train loss:  0.5071201324462891
train gradient:  0.44912878593116223
iteration : 1675
train acc:  0.796875
train loss:  0.4334963858127594
train gradient:  0.3253230777611934
iteration : 1676
train acc:  0.828125
train loss:  0.44461584091186523
train gradient:  0.3468160983550948
iteration : 1677
train acc:  0.8203125
train loss:  0.4343477487564087
train gradient:  0.3974946605013532
iteration : 1678
train acc:  0.8359375
train loss:  0.3881700038909912
train gradient:  0.4124296796570106
iteration : 1679
train acc:  0.796875
train loss:  0.38326066732406616
train gradient:  0.3478677120397861
iteration : 1680
train acc:  0.78125
train loss:  0.45096948742866516
train gradient:  0.5480563148412699
iteration : 1681
train acc:  0.765625
train loss:  0.4444011449813843
train gradient:  0.371071545947887
iteration : 1682
train acc:  0.8125
train loss:  0.415214478969574
train gradient:  0.32497416485328645
iteration : 1683
train acc:  0.7578125
train loss:  0.48110583424568176
train gradient:  0.5010201238378073
iteration : 1684
train acc:  0.8359375
train loss:  0.37630516290664673
train gradient:  0.30787877019723625
iteration : 1685
train acc:  0.734375
train loss:  0.45764273405075073
train gradient:  0.45250522258274706
iteration : 1686
train acc:  0.8359375
train loss:  0.37072068452835083
train gradient:  0.33179927021867917
iteration : 1687
train acc:  0.84375
train loss:  0.3652900457382202
train gradient:  0.30782995305008415
iteration : 1688
train acc:  0.828125
train loss:  0.4443659484386444
train gradient:  0.7350351604222253
iteration : 1689
train acc:  0.84375
train loss:  0.36431580781936646
train gradient:  0.3278241544027134
iteration : 1690
train acc:  0.7421875
train loss:  0.45785003900527954
train gradient:  0.41979906318498783
iteration : 1691
train acc:  0.78125
train loss:  0.3932885229587555
train gradient:  0.3044709741317492
iteration : 1692
train acc:  0.84375
train loss:  0.358216255903244
train gradient:  0.3514832610419822
iteration : 1693
train acc:  0.8515625
train loss:  0.3455347716808319
train gradient:  0.3393775593276446
iteration : 1694
train acc:  0.78125
train loss:  0.41262686252593994
train gradient:  0.37180123320718317
iteration : 1695
train acc:  0.765625
train loss:  0.48344096541404724
train gradient:  0.5326778923162767
iteration : 1696
train acc:  0.796875
train loss:  0.46766120195388794
train gradient:  0.4182442570116102
iteration : 1697
train acc:  0.78125
train loss:  0.46730831265449524
train gradient:  0.6474608161824911
iteration : 1698
train acc:  0.828125
train loss:  0.4019944965839386
train gradient:  0.34693069402047244
iteration : 1699
train acc:  0.8359375
train loss:  0.4151708483695984
train gradient:  0.4156386248819293
iteration : 1700
train acc:  0.7578125
train loss:  0.4207058250904083
train gradient:  0.40390079266147827
iteration : 1701
train acc:  0.78125
train loss:  0.3992772698402405
train gradient:  0.43552432735538843
iteration : 1702
train acc:  0.8125
train loss:  0.4283956289291382
train gradient:  0.3693046197066542
iteration : 1703
train acc:  0.7890625
train loss:  0.4246201813220978
train gradient:  0.36179849570336375
iteration : 1704
train acc:  0.8203125
train loss:  0.3810392916202545
train gradient:  0.3888600836388904
iteration : 1705
train acc:  0.84375
train loss:  0.3502393662929535
train gradient:  0.30582742420854564
iteration : 1706
train acc:  0.828125
train loss:  0.3668171167373657
train gradient:  0.3016432685954969
iteration : 1707
train acc:  0.796875
train loss:  0.42273789644241333
train gradient:  0.45838774170448926
iteration : 1708
train acc:  0.875
train loss:  0.31046441197395325
train gradient:  0.3107750677537745
iteration : 1709
train acc:  0.8203125
train loss:  0.42517799139022827
train gradient:  0.4306203407317361
iteration : 1710
train acc:  0.84375
train loss:  0.387920618057251
train gradient:  0.5376984706099991
iteration : 1711
train acc:  0.796875
train loss:  0.4263942837715149
train gradient:  0.39150328022667563
iteration : 1712
train acc:  0.8125
train loss:  0.40057697892189026
train gradient:  0.3570537752888231
iteration : 1713
train acc:  0.796875
train loss:  0.45290783047676086
train gradient:  0.38854883546571106
iteration : 1714
train acc:  0.7421875
train loss:  0.5201767683029175
train gradient:  0.5170077988492989
iteration : 1715
train acc:  0.8515625
train loss:  0.4152522683143616
train gradient:  0.42424081221303195
iteration : 1716
train acc:  0.8203125
train loss:  0.4223793148994446
train gradient:  0.4580692832369045
iteration : 1717
train acc:  0.7890625
train loss:  0.4167935848236084
train gradient:  0.4693426407398978
iteration : 1718
train acc:  0.8125
train loss:  0.4114350378513336
train gradient:  0.40602753699275185
iteration : 1719
train acc:  0.828125
train loss:  0.37598010897636414
train gradient:  0.2838368412297334
iteration : 1720
train acc:  0.7890625
train loss:  0.41444242000579834
train gradient:  0.4652163205420977
iteration : 1721
train acc:  0.8359375
train loss:  0.4144183397293091
train gradient:  0.23758184232277688
iteration : 1722
train acc:  0.78125
train loss:  0.4183916449546814
train gradient:  0.39902195916704614
iteration : 1723
train acc:  0.78125
train loss:  0.46702444553375244
train gradient:  0.4542358709566644
iteration : 1724
train acc:  0.828125
train loss:  0.4551996886730194
train gradient:  0.45726760839115904
iteration : 1725
train acc:  0.796875
train loss:  0.3877399265766144
train gradient:  0.3872920817538142
iteration : 1726
train acc:  0.828125
train loss:  0.3902870714664459
train gradient:  0.3705762763544664
iteration : 1727
train acc:  0.8046875
train loss:  0.4827435612678528
train gradient:  0.6283692063305419
iteration : 1728
train acc:  0.8515625
train loss:  0.36119431257247925
train gradient:  0.2733314407711853
iteration : 1729
train acc:  0.8125
train loss:  0.3923950791358948
train gradient:  0.3497930682716542
iteration : 1730
train acc:  0.765625
train loss:  0.3933291733264923
train gradient:  0.3520599459571689
iteration : 1731
train acc:  0.8046875
train loss:  0.43240898847579956
train gradient:  0.4065886672146742
iteration : 1732
train acc:  0.8359375
train loss:  0.3703678250312805
train gradient:  0.31162771236099146
iteration : 1733
train acc:  0.8671875
train loss:  0.34283047914505005
train gradient:  0.22168030770604585
iteration : 1734
train acc:  0.796875
train loss:  0.3946365714073181
train gradient:  0.3068284895599336
iteration : 1735
train acc:  0.75
train loss:  0.4581276774406433
train gradient:  0.5998194762186999
iteration : 1736
train acc:  0.8125
train loss:  0.4619902968406677
train gradient:  0.429810776577451
iteration : 1737
train acc:  0.890625
train loss:  0.360531210899353
train gradient:  0.29503610814283
iteration : 1738
train acc:  0.8671875
train loss:  0.3851964473724365
train gradient:  0.3715895696867129
iteration : 1739
train acc:  0.78125
train loss:  0.45722633600234985
train gradient:  0.4699243719436942
iteration : 1740
train acc:  0.828125
train loss:  0.3873004913330078
train gradient:  0.36686220906630357
iteration : 1741
train acc:  0.765625
train loss:  0.4207186996936798
train gradient:  0.547685974441506
iteration : 1742
train acc:  0.8359375
train loss:  0.40625375509262085
train gradient:  0.6422951452950675
iteration : 1743
train acc:  0.78125
train loss:  0.41943901777267456
train gradient:  0.37778727706363435
iteration : 1744
train acc:  0.8203125
train loss:  0.38802531361579895
train gradient:  0.3325952953183291
iteration : 1745
train acc:  0.8125
train loss:  0.47266924381256104
train gradient:  0.5418350380004805
iteration : 1746
train acc:  0.8125
train loss:  0.4153091311454773
train gradient:  0.3688390636018291
iteration : 1747
train acc:  0.8671875
train loss:  0.37313684821128845
train gradient:  0.2988319612766261
iteration : 1748
train acc:  0.84375
train loss:  0.3665492534637451
train gradient:  0.3976789297505648
iteration : 1749
train acc:  0.8359375
train loss:  0.33461064100265503
train gradient:  0.41562011011942196
iteration : 1750
train acc:  0.890625
train loss:  0.33037036657333374
train gradient:  0.34939041943501564
iteration : 1751
train acc:  0.8046875
train loss:  0.3888775110244751
train gradient:  0.3269612510338066
iteration : 1752
train acc:  0.8125
train loss:  0.4723173975944519
train gradient:  0.44666654603255623
iteration : 1753
train acc:  0.828125
train loss:  0.3731374740600586
train gradient:  0.291844791071601
iteration : 1754
train acc:  0.8125
train loss:  0.37206345796585083
train gradient:  0.36131426063059346
iteration : 1755
train acc:  0.796875
train loss:  0.39801132678985596
train gradient:  0.5537907939388551
iteration : 1756
train acc:  0.765625
train loss:  0.4891464114189148
train gradient:  0.6970648161160787
iteration : 1757
train acc:  0.8515625
train loss:  0.3698730766773224
train gradient:  0.4078333134264203
iteration : 1758
train acc:  0.8046875
train loss:  0.4432423710823059
train gradient:  0.48400560607656334
iteration : 1759
train acc:  0.859375
train loss:  0.35422104597091675
train gradient:  0.305056366977507
iteration : 1760
train acc:  0.8046875
train loss:  0.4535166621208191
train gradient:  0.6069466154903113
iteration : 1761
train acc:  0.7890625
train loss:  0.3967238962650299
train gradient:  0.5757067272204517
iteration : 1762
train acc:  0.78125
train loss:  0.4324621558189392
train gradient:  0.4672968885686262
iteration : 1763
train acc:  0.796875
train loss:  0.41198819875717163
train gradient:  0.5728210358261838
iteration : 1764
train acc:  0.796875
train loss:  0.41970378160476685
train gradient:  0.48657547613124097
iteration : 1765
train acc:  0.8671875
train loss:  0.3333052098751068
train gradient:  0.2632021139222125
iteration : 1766
train acc:  0.8203125
train loss:  0.39050164818763733
train gradient:  0.42104561719245015
iteration : 1767
train acc:  0.796875
train loss:  0.4358336925506592
train gradient:  0.4022347965318394
iteration : 1768
train acc:  0.84375
train loss:  0.3900901675224304
train gradient:  0.5076193868680903
iteration : 1769
train acc:  0.796875
train loss:  0.4763486385345459
train gradient:  0.511290005795661
iteration : 1770
train acc:  0.828125
train loss:  0.3659262955188751
train gradient:  0.3783793745145754
iteration : 1771
train acc:  0.78125
train loss:  0.45862120389938354
train gradient:  0.6686979370215769
iteration : 1772
train acc:  0.828125
train loss:  0.3989570140838623
train gradient:  0.33028891619548045
iteration : 1773
train acc:  0.828125
train loss:  0.3669224977493286
train gradient:  0.3769874497382609
iteration : 1774
train acc:  0.859375
train loss:  0.3925971984863281
train gradient:  0.33035374561969544
iteration : 1775
train acc:  0.796875
train loss:  0.4869479238986969
train gradient:  0.4329411018765498
iteration : 1776
train acc:  0.8359375
train loss:  0.3752495050430298
train gradient:  0.3609776095958516
iteration : 1777
train acc:  0.796875
train loss:  0.39210760593414307
train gradient:  0.43656736549022535
iteration : 1778
train acc:  0.7890625
train loss:  0.39853379130363464
train gradient:  0.39203947804199285
iteration : 1779
train acc:  0.8203125
train loss:  0.3953619599342346
train gradient:  0.3702380312026233
iteration : 1780
train acc:  0.828125
train loss:  0.4752271771430969
train gradient:  0.7146440960800765
iteration : 1781
train acc:  0.78125
train loss:  0.4831177592277527
train gradient:  0.6938434723209888
iteration : 1782
train acc:  0.7734375
train loss:  0.4888504147529602
train gradient:  0.5954845206449052
iteration : 1783
train acc:  0.7734375
train loss:  0.44807755947113037
train gradient:  0.4583067925098375
iteration : 1784
train acc:  0.7578125
train loss:  0.5193498134613037
train gradient:  0.47559569882597463
iteration : 1785
train acc:  0.8671875
train loss:  0.34900975227355957
train gradient:  0.3258354294316028
iteration : 1786
train acc:  0.8203125
train loss:  0.3913295567035675
train gradient:  0.3522805507090991
iteration : 1787
train acc:  0.796875
train loss:  0.40570467710494995
train gradient:  0.3942679966271853
iteration : 1788
train acc:  0.7421875
train loss:  0.5401574373245239
train gradient:  0.8369440232296645
iteration : 1789
train acc:  0.734375
train loss:  0.5505734086036682
train gradient:  0.74793505895438
iteration : 1790
train acc:  0.7578125
train loss:  0.4557976722717285
train gradient:  0.5903198483774222
iteration : 1791
train acc:  0.8125
train loss:  0.4485856592655182
train gradient:  0.45557948472784754
iteration : 1792
train acc:  0.8125
train loss:  0.44486334919929504
train gradient:  0.4750746857872548
iteration : 1793
train acc:  0.8359375
train loss:  0.3998751640319824
train gradient:  0.4494363276421759
iteration : 1794
train acc:  0.765625
train loss:  0.48326951265335083
train gradient:  0.47691360462829707
iteration : 1795
train acc:  0.8203125
train loss:  0.3746795952320099
train gradient:  0.3536541973384157
iteration : 1796
train acc:  0.765625
train loss:  0.482852578163147
train gradient:  0.6294068104447681
iteration : 1797
train acc:  0.7734375
train loss:  0.4757159948348999
train gradient:  0.5383043686643735
iteration : 1798
train acc:  0.8359375
train loss:  0.37199875712394714
train gradient:  0.41130311942318004
iteration : 1799
train acc:  0.8515625
train loss:  0.34259462356567383
train gradient:  0.3230857115278786
iteration : 1800
train acc:  0.78125
train loss:  0.4333727955818176
train gradient:  0.4350919515828177
iteration : 1801
train acc:  0.84375
train loss:  0.34210139513015747
train gradient:  0.3741858262553295
iteration : 1802
train acc:  0.828125
train loss:  0.4193233549594879
train gradient:  0.41063882196201884
iteration : 1803
train acc:  0.8203125
train loss:  0.3776717185974121
train gradient:  0.3567413839572657
iteration : 1804
train acc:  0.78125
train loss:  0.44861334562301636
train gradient:  0.5087056029484418
iteration : 1805
train acc:  0.7734375
train loss:  0.4251510798931122
train gradient:  0.48684380528368904
iteration : 1806
train acc:  0.84375
train loss:  0.37421905994415283
train gradient:  0.3121262905837055
iteration : 1807
train acc:  0.796875
train loss:  0.39638853073120117
train gradient:  0.48474401644677895
iteration : 1808
train acc:  0.8125
train loss:  0.35788530111312866
train gradient:  0.3742295921677195
iteration : 1809
train acc:  0.7734375
train loss:  0.4203430712223053
train gradient:  0.3274777985698465
iteration : 1810
train acc:  0.8203125
train loss:  0.4095195531845093
train gradient:  0.46284157254465835
iteration : 1811
train acc:  0.75
train loss:  0.4807308614253998
train gradient:  0.43011340508770207
iteration : 1812
train acc:  0.859375
train loss:  0.346338152885437
train gradient:  0.3366553058649896
iteration : 1813
train acc:  0.8125
train loss:  0.4215206503868103
train gradient:  0.3661428057438478
iteration : 1814
train acc:  0.9140625
train loss:  0.30864983797073364
train gradient:  0.26805241062570045
iteration : 1815
train acc:  0.84375
train loss:  0.3996308445930481
train gradient:  0.37106507489351825
iteration : 1816
train acc:  0.875
train loss:  0.30226409435272217
train gradient:  0.2729171288233087
iteration : 1817
train acc:  0.859375
train loss:  0.34179240465164185
train gradient:  0.3443673040739083
iteration : 1818
train acc:  0.828125
train loss:  0.42064368724823
train gradient:  0.41006913317332216
iteration : 1819
train acc:  0.8125
train loss:  0.42305541038513184
train gradient:  0.332068498438764
iteration : 1820
train acc:  0.8203125
train loss:  0.3545647859573364
train gradient:  0.3555550914098531
iteration : 1821
train acc:  0.828125
train loss:  0.36118918657302856
train gradient:  0.37533099652237756
iteration : 1822
train acc:  0.875
train loss:  0.3490642309188843
train gradient:  0.2987799784821519
iteration : 1823
train acc:  0.7109375
train loss:  0.5354434251785278
train gradient:  0.7834990914126676
iteration : 1824
train acc:  0.828125
train loss:  0.3832899332046509
train gradient:  0.4360233695736331
iteration : 1825
train acc:  0.859375
train loss:  0.34537243843078613
train gradient:  0.46530469113567563
iteration : 1826
train acc:  0.8125
train loss:  0.3956274092197418
train gradient:  0.6865018131600572
iteration : 1827
train acc:  0.8671875
train loss:  0.3205738663673401
train gradient:  0.27896949413769434
iteration : 1828
train acc:  0.78125
train loss:  0.41886118054389954
train gradient:  0.4785583464095014
iteration : 1829
train acc:  0.796875
train loss:  0.3452731668949127
train gradient:  0.3785115139248891
iteration : 1830
train acc:  0.8359375
train loss:  0.39015036821365356
train gradient:  0.43315498270269065
iteration : 1831
train acc:  0.796875
train loss:  0.4288725256919861
train gradient:  0.46863485280276745
iteration : 1832
train acc:  0.8046875
train loss:  0.4101707339286804
train gradient:  0.36839890199642383
iteration : 1833
train acc:  0.7734375
train loss:  0.4990791082382202
train gradient:  0.5938918570431334
iteration : 1834
train acc:  0.828125
train loss:  0.3771953582763672
train gradient:  0.29236846684019957
iteration : 1835
train acc:  0.828125
train loss:  0.4110393524169922
train gradient:  0.45105693509184225
iteration : 1836
train acc:  0.8203125
train loss:  0.3882683217525482
train gradient:  0.6621367711753174
iteration : 1837
train acc:  0.8671875
train loss:  0.33002200722694397
train gradient:  0.3058223037207713
iteration : 1838
train acc:  0.8125
train loss:  0.40655776858329773
train gradient:  0.5224227885848515
iteration : 1839
train acc:  0.8359375
train loss:  0.4093483090400696
train gradient:  0.5102860768530828
iteration : 1840
train acc:  0.765625
train loss:  0.4172402024269104
train gradient:  0.8444465566911925
iteration : 1841
train acc:  0.8125
train loss:  0.3849679231643677
train gradient:  0.6072357851169162
iteration : 1842
train acc:  0.859375
train loss:  0.3002657890319824
train gradient:  0.28384348285521904
iteration : 1843
train acc:  0.796875
train loss:  0.41271287202835083
train gradient:  0.4382580071192248
iteration : 1844
train acc:  0.8046875
train loss:  0.43170052766799927
train gradient:  0.42635486992519334
iteration : 1845
train acc:  0.8515625
train loss:  0.35740354657173157
train gradient:  0.46282555792632774
iteration : 1846
train acc:  0.875
train loss:  0.32778850197792053
train gradient:  0.39400485938683444
iteration : 1847
train acc:  0.8046875
train loss:  0.40120571851730347
train gradient:  0.5934607478015258
iteration : 1848
train acc:  0.875
train loss:  0.34480470418930054
train gradient:  0.43447619122455733
iteration : 1849
train acc:  0.7890625
train loss:  0.4520919919013977
train gradient:  0.4547323843355802
iteration : 1850
train acc:  0.8046875
train loss:  0.40439122915267944
train gradient:  0.4491797122948362
iteration : 1851
train acc:  0.796875
train loss:  0.4048944115638733
train gradient:  0.5678534557540077
iteration : 1852
train acc:  0.75
train loss:  0.4775615930557251
train gradient:  0.6848371606996488
iteration : 1853
train acc:  0.8203125
train loss:  0.4560979902744293
train gradient:  0.6214015261574917
iteration : 1854
train acc:  0.8125
train loss:  0.4404701590538025
train gradient:  0.5111256723690819
iteration : 1855
train acc:  0.828125
train loss:  0.38718587160110474
train gradient:  0.5414937783596085
iteration : 1856
train acc:  0.8125
train loss:  0.39767348766326904
train gradient:  0.4191845106141911
iteration : 1857
train acc:  0.796875
train loss:  0.4435362219810486
train gradient:  0.42706542281025756
iteration : 1858
train acc:  0.828125
train loss:  0.46136045455932617
train gradient:  0.6043256117046585
iteration : 1859
train acc:  0.7890625
train loss:  0.3933449685573578
train gradient:  0.5288755536401257
iteration : 1860
train acc:  0.8515625
train loss:  0.331540584564209
train gradient:  0.347113558841808
iteration : 1861
train acc:  0.8203125
train loss:  0.391230970621109
train gradient:  0.43623634657815724
iteration : 1862
train acc:  0.8359375
train loss:  0.4212772846221924
train gradient:  0.5711752356539495
iteration : 1863
train acc:  0.828125
train loss:  0.3962613344192505
train gradient:  0.724827941693287
iteration : 1864
train acc:  0.859375
train loss:  0.32034850120544434
train gradient:  0.2435674199222311
iteration : 1865
train acc:  0.8203125
train loss:  0.459869384765625
train gradient:  0.5335244757891998
iteration : 1866
train acc:  0.7578125
train loss:  0.4735872149467468
train gradient:  0.5950498064787018
iteration : 1867
train acc:  0.8359375
train loss:  0.3976360261440277
train gradient:  0.5087361956292037
iteration : 1868
train acc:  0.7890625
train loss:  0.4183238446712494
train gradient:  0.35638759381049
iteration : 1869
train acc:  0.7578125
train loss:  0.41574013233184814
train gradient:  0.40919522306894046
iteration : 1870
train acc:  0.8125
train loss:  0.36797982454299927
train gradient:  0.40236324659445377
iteration : 1871
train acc:  0.8046875
train loss:  0.40751057863235474
train gradient:  0.5180921338168816
iteration : 1872
train acc:  0.828125
train loss:  0.42593616247177124
train gradient:  0.5652246766884594
iteration : 1873
train acc:  0.7890625
train loss:  0.41567277908325195
train gradient:  0.4899799599412548
iteration : 1874
train acc:  0.859375
train loss:  0.33026623725891113
train gradient:  0.37248247236093784
iteration : 1875
train acc:  0.84375
train loss:  0.36980313062667847
train gradient:  0.5264490557113439
iteration : 1876
train acc:  0.8046875
train loss:  0.39205822348594666
train gradient:  0.41602826049633945
iteration : 1877
train acc:  0.8203125
train loss:  0.4209468960762024
train gradient:  0.4241747018272076
iteration : 1878
train acc:  0.859375
train loss:  0.3314574360847473
train gradient:  0.45554875941946066
iteration : 1879
train acc:  0.8125
train loss:  0.4156990945339203
train gradient:  0.4612131761514477
iteration : 1880
train acc:  0.7734375
train loss:  0.47430503368377686
train gradient:  0.5835121642271317
iteration : 1881
train acc:  0.78125
train loss:  0.4687601923942566
train gradient:  0.5026005137862254
iteration : 1882
train acc:  0.7734375
train loss:  0.47145479917526245
train gradient:  0.4277586168326553
iteration : 1883
train acc:  0.78125
train loss:  0.41799813508987427
train gradient:  0.508316293545453
iteration : 1884
train acc:  0.78125
train loss:  0.4326969087123871
train gradient:  0.5679708131493618
iteration : 1885
train acc:  0.796875
train loss:  0.5103847980499268
train gradient:  0.7578686587699558
iteration : 1886
train acc:  0.78125
train loss:  0.427351713180542
train gradient:  0.44119185939562533
iteration : 1887
train acc:  0.828125
train loss:  0.3817668557167053
train gradient:  0.36367700027275746
iteration : 1888
train acc:  0.8046875
train loss:  0.3896598815917969
train gradient:  0.3670431402598046
iteration : 1889
train acc:  0.828125
train loss:  0.3781374394893646
train gradient:  0.2953473081225219
iteration : 1890
train acc:  0.8203125
train loss:  0.39448413252830505
train gradient:  0.35885929857922455
iteration : 1891
train acc:  0.8671875
train loss:  0.35545140504837036
train gradient:  0.2626553598212316
iteration : 1892
train acc:  0.8203125
train loss:  0.4233912229537964
train gradient:  0.4903614865114991
iteration : 1893
train acc:  0.78125
train loss:  0.41811510920524597
train gradient:  0.38530444858598323
iteration : 1894
train acc:  0.828125
train loss:  0.4076704680919647
train gradient:  0.561595567591171
iteration : 1895
train acc:  0.8203125
train loss:  0.37144702672958374
train gradient:  0.3943574221319319
iteration : 1896
train acc:  0.7890625
train loss:  0.450944721698761
train gradient:  0.4455981586710904
iteration : 1897
train acc:  0.796875
train loss:  0.41955506801605225
train gradient:  0.35880330714660497
iteration : 1898
train acc:  0.796875
train loss:  0.4736935496330261
train gradient:  0.45745199655966245
iteration : 1899
train acc:  0.7890625
train loss:  0.4098590612411499
train gradient:  0.5328002232902509
iteration : 1900
train acc:  0.8203125
train loss:  0.38323765993118286
train gradient:  0.336004187257826
iteration : 1901
train acc:  0.8359375
train loss:  0.36678704619407654
train gradient:  0.4921683874434968
iteration : 1902
train acc:  0.8046875
train loss:  0.3967192769050598
train gradient:  0.4875985012738823
iteration : 1903
train acc:  0.8515625
train loss:  0.3788606524467468
train gradient:  0.5044564406448087
iteration : 1904
train acc:  0.8203125
train loss:  0.3689020872116089
train gradient:  0.41628019670447036
iteration : 1905
train acc:  0.8203125
train loss:  0.3734465539455414
train gradient:  0.3446903043749647
iteration : 1906
train acc:  0.8359375
train loss:  0.38902875781059265
train gradient:  0.44223226310042113
iteration : 1907
train acc:  0.8203125
train loss:  0.41071900725364685
train gradient:  0.458852776940774
iteration : 1908
train acc:  0.8359375
train loss:  0.42912566661834717
train gradient:  0.5460178201788246
iteration : 1909
train acc:  0.8359375
train loss:  0.3815687894821167
train gradient:  0.6107075200305945
iteration : 1910
train acc:  0.8046875
train loss:  0.4095829725265503
train gradient:  0.431792752201206
iteration : 1911
train acc:  0.84375
train loss:  0.41190287470817566
train gradient:  0.4887186957573093
iteration : 1912
train acc:  0.8203125
train loss:  0.41279441118240356
train gradient:  0.3427349884358537
iteration : 1913
train acc:  0.8203125
train loss:  0.41105544567108154
train gradient:  0.3698303996178223
iteration : 1914
train acc:  0.796875
train loss:  0.41266921162605286
train gradient:  0.41848215785208354
iteration : 1915
train acc:  0.828125
train loss:  0.399629682302475
train gradient:  0.40667591402113396
iteration : 1916
train acc:  0.828125
train loss:  0.3556254506111145
train gradient:  0.4667690995602562
iteration : 1917
train acc:  0.8203125
train loss:  0.3923671245574951
train gradient:  0.29823710994442776
iteration : 1918
train acc:  0.796875
train loss:  0.4086531102657318
train gradient:  0.3464797462586057
iteration : 1919
train acc:  0.7890625
train loss:  0.4469171166419983
train gradient:  0.4194938460336782
iteration : 1920
train acc:  0.75
train loss:  0.48583507537841797
train gradient:  0.4711487385618633
iteration : 1921
train acc:  0.84375
train loss:  0.4159364402294159
train gradient:  0.4894328449901854
iteration : 1922
train acc:  0.84375
train loss:  0.37684309482574463
train gradient:  0.40634327220738115
iteration : 1923
train acc:  0.859375
train loss:  0.314584881067276
train gradient:  0.28073525029490387
iteration : 1924
train acc:  0.796875
train loss:  0.46820858120918274
train gradient:  0.40267895042214596
iteration : 1925
train acc:  0.7890625
train loss:  0.40369778871536255
train gradient:  0.32670794076329235
iteration : 1926
train acc:  0.7734375
train loss:  0.4873163104057312
train gradient:  0.6274767718934436
iteration : 1927
train acc:  0.8515625
train loss:  0.3769291043281555
train gradient:  0.3615677832471928
iteration : 1928
train acc:  0.78125
train loss:  0.42119383811950684
train gradient:  0.4302339722998344
iteration : 1929
train acc:  0.8828125
train loss:  0.34279876947402954
train gradient:  0.25692657599013025
iteration : 1930
train acc:  0.8828125
train loss:  0.3305833339691162
train gradient:  0.27053535732009276
iteration : 1931
train acc:  0.8359375
train loss:  0.3569989800453186
train gradient:  0.3468339554713001
iteration : 1932
train acc:  0.859375
train loss:  0.3501943349838257
train gradient:  0.35393201655877277
iteration : 1933
train acc:  0.859375
train loss:  0.3430323600769043
train gradient:  0.4381246780998398
iteration : 1934
train acc:  0.828125
train loss:  0.41760966181755066
train gradient:  0.4348563596400324
iteration : 1935
train acc:  0.828125
train loss:  0.39670631289482117
train gradient:  0.30625920393362555
iteration : 1936
train acc:  0.7578125
train loss:  0.4422277808189392
train gradient:  0.42919398986212587
iteration : 1937
train acc:  0.8125
train loss:  0.42832303047180176
train gradient:  0.4775693634719074
iteration : 1938
train acc:  0.78125
train loss:  0.4510936737060547
train gradient:  0.5793464314731038
iteration : 1939
train acc:  0.8046875
train loss:  0.44578874111175537
train gradient:  0.3771297346955147
iteration : 1940
train acc:  0.796875
train loss:  0.42975398898124695
train gradient:  0.5315746065685157
iteration : 1941
train acc:  0.8203125
train loss:  0.4205630123615265
train gradient:  0.454335283739314
iteration : 1942
train acc:  0.8046875
train loss:  0.38039064407348633
train gradient:  0.4366916995426995
iteration : 1943
train acc:  0.8203125
train loss:  0.3982693552970886
train gradient:  0.2854691006748069
iteration : 1944
train acc:  0.796875
train loss:  0.4157227873802185
train gradient:  0.29128651029805425
iteration : 1945
train acc:  0.828125
train loss:  0.3946264088153839
train gradient:  0.33153858600184793
iteration : 1946
train acc:  0.84375
train loss:  0.3516424298286438
train gradient:  0.25781685638345575
iteration : 1947
train acc:  0.8125
train loss:  0.42057928442955017
train gradient:  0.38094820710933786
iteration : 1948
train acc:  0.7734375
train loss:  0.4898393154144287
train gradient:  0.4878633240366149
iteration : 1949
train acc:  0.7578125
train loss:  0.5055898427963257
train gradient:  0.5061471343944713
iteration : 1950
train acc:  0.8203125
train loss:  0.38386833667755127
train gradient:  0.24431526059197428
iteration : 1951
train acc:  0.8125
train loss:  0.4256443977355957
train gradient:  0.36665508786593554
iteration : 1952
train acc:  0.8125
train loss:  0.38364842534065247
train gradient:  0.3215015589883529
iteration : 1953
train acc:  0.7890625
train loss:  0.417045533657074
train gradient:  0.46180546514416615
iteration : 1954
train acc:  0.78125
train loss:  0.4281001687049866
train gradient:  0.4922745509695164
iteration : 1955
train acc:  0.78125
train loss:  0.42016613483428955
train gradient:  0.45354896945472106
iteration : 1956
train acc:  0.8125
train loss:  0.37125611305236816
train gradient:  0.28901568854532306
iteration : 1957
train acc:  0.8203125
train loss:  0.41310176253318787
train gradient:  0.3513364453965332
iteration : 1958
train acc:  0.828125
train loss:  0.391625314950943
train gradient:  0.42106605164788297
iteration : 1959
train acc:  0.8203125
train loss:  0.41056761145591736
train gradient:  0.3418509929105067
iteration : 1960
train acc:  0.8515625
train loss:  0.35679614543914795
train gradient:  0.35048169600495827
iteration : 1961
train acc:  0.859375
train loss:  0.3499698042869568
train gradient:  0.33968206188467187
iteration : 1962
train acc:  0.7890625
train loss:  0.418610543012619
train gradient:  0.5451320418952812
iteration : 1963
train acc:  0.859375
train loss:  0.32846155762672424
train gradient:  0.25387219246234877
iteration : 1964
train acc:  0.7890625
train loss:  0.4274096190929413
train gradient:  0.4969136656850319
iteration : 1965
train acc:  0.828125
train loss:  0.40425339341163635
train gradient:  0.4123333184040289
iteration : 1966
train acc:  0.828125
train loss:  0.42773982882499695
train gradient:  0.3395832751290944
iteration : 1967
train acc:  0.796875
train loss:  0.38777315616607666
train gradient:  0.28419667300555645
iteration : 1968
train acc:  0.78125
train loss:  0.4894527196884155
train gradient:  0.5199109610449902
iteration : 1969
train acc:  0.7734375
train loss:  0.4950054883956909
train gradient:  0.628403594967922
iteration : 1970
train acc:  0.7421875
train loss:  0.5046904683113098
train gradient:  0.6231949683306243
iteration : 1971
train acc:  0.8359375
train loss:  0.41110938787460327
train gradient:  0.4726580357513917
iteration : 1972
train acc:  0.796875
train loss:  0.4131961464881897
train gradient:  0.5213200279013208
iteration : 1973
train acc:  0.84375
train loss:  0.40130043029785156
train gradient:  0.3624075523329021
iteration : 1974
train acc:  0.8046875
train loss:  0.3886720836162567
train gradient:  0.28415728024109765
iteration : 1975
train acc:  0.84375
train loss:  0.37882131338119507
train gradient:  0.30627834356271816
iteration : 1976
train acc:  0.8203125
train loss:  0.391396164894104
train gradient:  0.2986736423372609
iteration : 1977
train acc:  0.796875
train loss:  0.417357861995697
train gradient:  0.4271709903448809
iteration : 1978
train acc:  0.8203125
train loss:  0.3737177550792694
train gradient:  0.2671055176684619
iteration : 1979
train acc:  0.796875
train loss:  0.4628375768661499
train gradient:  0.36990415272732324
iteration : 1980
train acc:  0.796875
train loss:  0.4008617103099823
train gradient:  0.243960072149438
iteration : 1981
train acc:  0.8203125
train loss:  0.43323588371276855
train gradient:  0.35285962767940177
iteration : 1982
train acc:  0.8203125
train loss:  0.4335801899433136
train gradient:  0.2915769598398886
iteration : 1983
train acc:  0.890625
train loss:  0.33914750814437866
train gradient:  0.2551819649123983
iteration : 1984
train acc:  0.8359375
train loss:  0.3492250442504883
train gradient:  0.290019719556581
iteration : 1985
train acc:  0.828125
train loss:  0.46085214614868164
train gradient:  0.4545695191359249
iteration : 1986
train acc:  0.7734375
train loss:  0.4548805058002472
train gradient:  0.4015962205906931
iteration : 1987
train acc:  0.7890625
train loss:  0.5016307830810547
train gradient:  0.48511068473366525
iteration : 1988
train acc:  0.8515625
train loss:  0.3918147683143616
train gradient:  0.30576952921918976
iteration : 1989
train acc:  0.8203125
train loss:  0.4169267416000366
train gradient:  0.27789202385653416
iteration : 1990
train acc:  0.859375
train loss:  0.34686988592147827
train gradient:  0.2386203645380261
iteration : 1991
train acc:  0.8359375
train loss:  0.42729759216308594
train gradient:  0.3885185737455057
iteration : 1992
train acc:  0.7734375
train loss:  0.4677073359489441
train gradient:  0.6392738378616217
iteration : 1993
train acc:  0.796875
train loss:  0.4416118860244751
train gradient:  0.31457165376286206
iteration : 1994
train acc:  0.78125
train loss:  0.4030812978744507
train gradient:  0.4115013037889855
iteration : 1995
train acc:  0.84375
train loss:  0.3832462430000305
train gradient:  0.2987443946037673
iteration : 1996
train acc:  0.8359375
train loss:  0.3941757082939148
train gradient:  0.3370110789140428
iteration : 1997
train acc:  0.78125
train loss:  0.4426262378692627
train gradient:  0.46657259124499634
iteration : 1998
train acc:  0.828125
train loss:  0.43033474683761597
train gradient:  0.3129759027222616
iteration : 1999
train acc:  0.796875
train loss:  0.4019830822944641
train gradient:  0.46913693455783306
iteration : 2000
train acc:  0.828125
train loss:  0.4021275043487549
train gradient:  0.3539224330877732
iteration : 2001
train acc:  0.765625
train loss:  0.42518484592437744
train gradient:  0.4444866111029417
iteration : 2002
train acc:  0.859375
train loss:  0.3901864290237427
train gradient:  0.34893795895763957
iteration : 2003
train acc:  0.828125
train loss:  0.4197424650192261
train gradient:  0.3038058865857297
iteration : 2004
train acc:  0.8046875
train loss:  0.4331430196762085
train gradient:  0.34746886688298445
iteration : 2005
train acc:  0.828125
train loss:  0.3781639337539673
train gradient:  0.32947740889631016
iteration : 2006
train acc:  0.765625
train loss:  0.5171685218811035
train gradient:  0.477196533510897
iteration : 2007
train acc:  0.7734375
train loss:  0.4704285264015198
train gradient:  0.4095766377839497
iteration : 2008
train acc:  0.8359375
train loss:  0.3821712136268616
train gradient:  0.5216591095852277
iteration : 2009
train acc:  0.828125
train loss:  0.35940611362457275
train gradient:  0.36230548572805377
iteration : 2010
train acc:  0.8125
train loss:  0.37290245294570923
train gradient:  0.1947799211780752
iteration : 2011
train acc:  0.7890625
train loss:  0.39730849862098694
train gradient:  0.25057360287535013
iteration : 2012
train acc:  0.8203125
train loss:  0.36978644132614136
train gradient:  0.27113255301245737
iteration : 2013
train acc:  0.8125
train loss:  0.3978428244590759
train gradient:  0.3240706181287934
iteration : 2014
train acc:  0.8359375
train loss:  0.3832920789718628
train gradient:  0.3617837772944187
iteration : 2015
train acc:  0.796875
train loss:  0.3945365846157074
train gradient:  0.3826106261571599
iteration : 2016
train acc:  0.828125
train loss:  0.40323406457901
train gradient:  0.3823576228376408
iteration : 2017
train acc:  0.7734375
train loss:  0.45850855112075806
train gradient:  0.49205920412088333
iteration : 2018
train acc:  0.8125
train loss:  0.4218478798866272
train gradient:  0.30199577562446883
iteration : 2019
train acc:  0.8515625
train loss:  0.4126220941543579
train gradient:  0.33430107986074775
iteration : 2020
train acc:  0.8828125
train loss:  0.3327782154083252
train gradient:  0.28708494636028953
iteration : 2021
train acc:  0.84375
train loss:  0.35170483589172363
train gradient:  0.346446579500529
iteration : 2022
train acc:  0.7734375
train loss:  0.4471950829029083
train gradient:  0.4625263756436748
iteration : 2023
train acc:  0.8125
train loss:  0.44787508249282837
train gradient:  0.513052727882223
iteration : 2024
train acc:  0.84375
train loss:  0.3156718611717224
train gradient:  0.23486521992503123
iteration : 2025
train acc:  0.7890625
train loss:  0.46387359499931335
train gradient:  0.3762802746267871
iteration : 2026
train acc:  0.84375
train loss:  0.3228887915611267
train gradient:  0.2748880686681215
iteration : 2027
train acc:  0.8359375
train loss:  0.3635561764240265
train gradient:  0.405632026769934
iteration : 2028
train acc:  0.875
train loss:  0.31373506784439087
train gradient:  0.3008635626804111
iteration : 2029
train acc:  0.84375
train loss:  0.44083428382873535
train gradient:  0.374324453047991
iteration : 2030
train acc:  0.7734375
train loss:  0.5219755172729492
train gradient:  0.5576464031281035
iteration : 2031
train acc:  0.8203125
train loss:  0.3720710873603821
train gradient:  0.2686324207833689
iteration : 2032
train acc:  0.8515625
train loss:  0.3266385793685913
train gradient:  0.3396753063745164
iteration : 2033
train acc:  0.84375
train loss:  0.411895751953125
train gradient:  0.4015083413846779
iteration : 2034
train acc:  0.828125
train loss:  0.40397197008132935
train gradient:  0.346372582657479
iteration : 2035
train acc:  0.796875
train loss:  0.40157902240753174
train gradient:  0.4150976199572955
iteration : 2036
train acc:  0.859375
train loss:  0.3992292284965515
train gradient:  0.32527986234228384
iteration : 2037
train acc:  0.8125
train loss:  0.35118213295936584
train gradient:  0.2505274299807353
iteration : 2038
train acc:  0.8125
train loss:  0.3872450590133667
train gradient:  0.29938133727547434
iteration : 2039
train acc:  0.8046875
train loss:  0.4723585844039917
train gradient:  0.5759954938768108
iteration : 2040
train acc:  0.859375
train loss:  0.3440195918083191
train gradient:  0.24132477156264776
iteration : 2041
train acc:  0.78125
train loss:  0.42489132285118103
train gradient:  0.440860157895156
iteration : 2042
train acc:  0.8203125
train loss:  0.4712062180042267
train gradient:  0.4840856848198557
iteration : 2043
train acc:  0.8359375
train loss:  0.3647025525569916
train gradient:  0.33563879150437803
iteration : 2044
train acc:  0.7578125
train loss:  0.474549800157547
train gradient:  0.5061762170132117
iteration : 2045
train acc:  0.796875
train loss:  0.41339415311813354
train gradient:  0.6462345621706335
iteration : 2046
train acc:  0.859375
train loss:  0.3409264087677002
train gradient:  0.2871243931185825
iteration : 2047
train acc:  0.8203125
train loss:  0.42166653275489807
train gradient:  0.3957527948188153
iteration : 2048
train acc:  0.8125
train loss:  0.43069037795066833
train gradient:  0.367566279682031
iteration : 2049
train acc:  0.78125
train loss:  0.4957326352596283
train gradient:  0.39374987663744343
iteration : 2050
train acc:  0.8203125
train loss:  0.42867422103881836
train gradient:  0.5022503573123773
iteration : 2051
train acc:  0.796875
train loss:  0.3834992051124573
train gradient:  0.29316932019362435
iteration : 2052
train acc:  0.7734375
train loss:  0.506570041179657
train gradient:  0.5461538042075662
iteration : 2053
train acc:  0.796875
train loss:  0.44778531789779663
train gradient:  0.33953935695249027
iteration : 2054
train acc:  0.8671875
train loss:  0.3566095530986786
train gradient:  0.2876757010309456
iteration : 2055
train acc:  0.8203125
train loss:  0.42235150933265686
train gradient:  0.3648733105681683
iteration : 2056
train acc:  0.8515625
train loss:  0.39206966757774353
train gradient:  0.3096960217612482
iteration : 2057
train acc:  0.8046875
train loss:  0.3888678252696991
train gradient:  0.361838650442045
iteration : 2058
train acc:  0.859375
train loss:  0.34159985184669495
train gradient:  0.3863105494700116
iteration : 2059
train acc:  0.8515625
train loss:  0.38012027740478516
train gradient:  0.3626881707882416
iteration : 2060
train acc:  0.8515625
train loss:  0.34345388412475586
train gradient:  0.2892589031882233
iteration : 2061
train acc:  0.7421875
train loss:  0.5056002140045166
train gradient:  0.49224563891110446
iteration : 2062
train acc:  0.796875
train loss:  0.4415857493877411
train gradient:  0.6674092917680938
iteration : 2063
train acc:  0.75
train loss:  0.494939386844635
train gradient:  0.5295845286766097
iteration : 2064
train acc:  0.8515625
train loss:  0.34934625029563904
train gradient:  0.35285624146135175
iteration : 2065
train acc:  0.765625
train loss:  0.49645835161209106
train gradient:  0.7091148642437006
iteration : 2066
train acc:  0.796875
train loss:  0.5103246569633484
train gradient:  0.5306415452006833
iteration : 2067
train acc:  0.8515625
train loss:  0.37350162863731384
train gradient:  0.3100544462893954
iteration : 2068
train acc:  0.78125
train loss:  0.4289586544036865
train gradient:  0.3542725326553637
iteration : 2069
train acc:  0.796875
train loss:  0.4196717143058777
train gradient:  0.37879760509153015
iteration : 2070
train acc:  0.7734375
train loss:  0.43702030181884766
train gradient:  0.4645597278025725
iteration : 2071
train acc:  0.8046875
train loss:  0.4364057183265686
train gradient:  0.38482912762850646
iteration : 2072
train acc:  0.78125
train loss:  0.45432278513908386
train gradient:  0.47803971447892313
iteration : 2073
train acc:  0.8046875
train loss:  0.45435553789138794
train gradient:  0.41296760943256006
iteration : 2074
train acc:  0.8671875
train loss:  0.3510841131210327
train gradient:  0.4277658568027427
iteration : 2075
train acc:  0.8203125
train loss:  0.37995341420173645
train gradient:  0.3174527480425025
iteration : 2076
train acc:  0.7890625
train loss:  0.39975935220718384
train gradient:  0.2691837363001771
iteration : 2077
train acc:  0.78125
train loss:  0.4433194696903229
train gradient:  0.3821832452060823
iteration : 2078
train acc:  0.8515625
train loss:  0.3301331400871277
train gradient:  0.27100887194310336
iteration : 2079
train acc:  0.828125
train loss:  0.4158245623111725
train gradient:  0.32901314120652286
iteration : 2080
train acc:  0.8046875
train loss:  0.4076548218727112
train gradient:  0.5292793744802239
iteration : 2081
train acc:  0.78125
train loss:  0.42070960998535156
train gradient:  0.48208416666786913
iteration : 2082
train acc:  0.8359375
train loss:  0.32415223121643066
train gradient:  0.27746616627716053
iteration : 2083
train acc:  0.8125
train loss:  0.3997375965118408
train gradient:  0.3318959208069182
iteration : 2084
train acc:  0.8125
train loss:  0.3934100866317749
train gradient:  0.29679124778592536
iteration : 2085
train acc:  0.7734375
train loss:  0.4375256299972534
train gradient:  0.4297608082824396
iteration : 2086
train acc:  0.7890625
train loss:  0.46128129959106445
train gradient:  0.5126931716102442
iteration : 2087
train acc:  0.8359375
train loss:  0.37221434712409973
train gradient:  0.33135038859225413
iteration : 2088
train acc:  0.7890625
train loss:  0.47797274589538574
train gradient:  0.3674376144453407
iteration : 2089
train acc:  0.8359375
train loss:  0.4003625512123108
train gradient:  0.30329798071591574
iteration : 2090
train acc:  0.7734375
train loss:  0.3786677122116089
train gradient:  0.3127715726481419
iteration : 2091
train acc:  0.7734375
train loss:  0.43054914474487305
train gradient:  0.2827930839052074
iteration : 2092
train acc:  0.796875
train loss:  0.4592020511627197
train gradient:  0.41944292506036096
iteration : 2093
train acc:  0.8125
train loss:  0.3835567831993103
train gradient:  0.28647223052032805
iteration : 2094
train acc:  0.7578125
train loss:  0.5170471668243408
train gradient:  0.6454815321021377
iteration : 2095
train acc:  0.7890625
train loss:  0.4347374439239502
train gradient:  0.3786622169972736
iteration : 2096
train acc:  0.8671875
train loss:  0.31120020151138306
train gradient:  0.34632734690425127
iteration : 2097
train acc:  0.8515625
train loss:  0.35611534118652344
train gradient:  0.22213410612679582
iteration : 2098
train acc:  0.8515625
train loss:  0.3670629858970642
train gradient:  0.2823356839132227
iteration : 2099
train acc:  0.875
train loss:  0.32860901951789856
train gradient:  0.2966296014993194
iteration : 2100
train acc:  0.8125
train loss:  0.4324360489845276
train gradient:  0.4093900561020607
iteration : 2101
train acc:  0.8046875
train loss:  0.3915501832962036
train gradient:  0.49999024682610493
iteration : 2102
train acc:  0.8203125
train loss:  0.3549705743789673
train gradient:  0.33372751804618744
iteration : 2103
train acc:  0.7421875
train loss:  0.5061484575271606
train gradient:  0.6260695446688302
iteration : 2104
train acc:  0.8828125
train loss:  0.2881718873977661
train gradient:  0.20112421604172653
iteration : 2105
train acc:  0.8046875
train loss:  0.4502065181732178
train gradient:  0.4168056955864227
iteration : 2106
train acc:  0.875
train loss:  0.3277236521244049
train gradient:  0.2582186137441863
iteration : 2107
train acc:  0.84375
train loss:  0.3960466980934143
train gradient:  0.4460887375318044
iteration : 2108
train acc:  0.8046875
train loss:  0.38826438784599304
train gradient:  0.3435291650336352
iteration : 2109
train acc:  0.7421875
train loss:  0.46249958872795105
train gradient:  0.5142593595014988
iteration : 2110
train acc:  0.7890625
train loss:  0.39086246490478516
train gradient:  0.376362063317976
iteration : 2111
train acc:  0.8125
train loss:  0.4195866584777832
train gradient:  0.49944369495719665
iteration : 2112
train acc:  0.8828125
train loss:  0.3261122703552246
train gradient:  0.30432009880225874
iteration : 2113
train acc:  0.90625
train loss:  0.3100956082344055
train gradient:  0.31674783873870266
iteration : 2114
train acc:  0.8125
train loss:  0.37824273109436035
train gradient:  0.3433540043265487
iteration : 2115
train acc:  0.7578125
train loss:  0.44663506746292114
train gradient:  0.4753553317511525
iteration : 2116
train acc:  0.78125
train loss:  0.46609508991241455
train gradient:  0.5741360305843686
iteration : 2117
train acc:  0.875
train loss:  0.3509368896484375
train gradient:  0.2404523733517358
iteration : 2118
train acc:  0.765625
train loss:  0.3939116299152374
train gradient:  0.3138627949262318
iteration : 2119
train acc:  0.8359375
train loss:  0.3326508104801178
train gradient:  0.2757936355358147
iteration : 2120
train acc:  0.8671875
train loss:  0.3385774493217468
train gradient:  0.278010502912052
iteration : 2121
train acc:  0.8359375
train loss:  0.3651486039161682
train gradient:  0.36123819596987594
iteration : 2122
train acc:  0.8203125
train loss:  0.38030949234962463
train gradient:  0.3028772900034139
iteration : 2123
train acc:  0.8984375
train loss:  0.3030344843864441
train gradient:  0.28664890662170495
iteration : 2124
train acc:  0.828125
train loss:  0.37821871042251587
train gradient:  0.2712480272197788
iteration : 2125
train acc:  0.8828125
train loss:  0.27200445532798767
train gradient:  0.23426700975188297
iteration : 2126
train acc:  0.8359375
train loss:  0.3637249171733856
train gradient:  0.3618479353412242
iteration : 2127
train acc:  0.8046875
train loss:  0.4181530177593231
train gradient:  0.3723875527922331
iteration : 2128
train acc:  0.828125
train loss:  0.36916062235832214
train gradient:  0.34214578402268175
iteration : 2129
train acc:  0.8515625
train loss:  0.3611077666282654
train gradient:  0.3183898191685548
iteration : 2130
train acc:  0.796875
train loss:  0.43119731545448303
train gradient:  0.43077819685535074
iteration : 2131
train acc:  0.8671875
train loss:  0.3151136040687561
train gradient:  0.34702936812860374
iteration : 2132
train acc:  0.8671875
train loss:  0.35371819138526917
train gradient:  0.36299186200546535
iteration : 2133
train acc:  0.8515625
train loss:  0.2970055341720581
train gradient:  0.19848555903818751
iteration : 2134
train acc:  0.8046875
train loss:  0.4084797501564026
train gradient:  0.40057667868717284
iteration : 2135
train acc:  0.7578125
train loss:  0.4646988809108734
train gradient:  0.62234682595814
iteration : 2136
train acc:  0.78125
train loss:  0.48776185512542725
train gradient:  0.46617319349823994
iteration : 2137
train acc:  0.828125
train loss:  0.422376424074173
train gradient:  0.3030588513889958
iteration : 2138
train acc:  0.7890625
train loss:  0.46075600385665894
train gradient:  0.34414109364069095
iteration : 2139
train acc:  0.828125
train loss:  0.3862651288509369
train gradient:  0.2881021533781086
iteration : 2140
train acc:  0.8359375
train loss:  0.38971149921417236
train gradient:  0.3692114118189182
iteration : 2141
train acc:  0.796875
train loss:  0.4331895112991333
train gradient:  0.37962690242788566
iteration : 2142
train acc:  0.8203125
train loss:  0.3810211718082428
train gradient:  0.296015313940577
iteration : 2143
train acc:  0.90625
train loss:  0.3120819628238678
train gradient:  0.23672037539521346
iteration : 2144
train acc:  0.828125
train loss:  0.40389519929885864
train gradient:  0.42214175315075386
iteration : 2145
train acc:  0.8515625
train loss:  0.34712398052215576
train gradient:  0.3220366768510878
iteration : 2146
train acc:  0.75
train loss:  0.5174756050109863
train gradient:  0.6182304193606731
iteration : 2147
train acc:  0.828125
train loss:  0.4106828570365906
train gradient:  0.3645584249730274
iteration : 2148
train acc:  0.8359375
train loss:  0.3754348158836365
train gradient:  0.2909468013251691
iteration : 2149
train acc:  0.828125
train loss:  0.36099496483802795
train gradient:  0.3748671483300283
iteration : 2150
train acc:  0.78125
train loss:  0.45833510160446167
train gradient:  0.4750119624574764
iteration : 2151
train acc:  0.8359375
train loss:  0.39771994948387146
train gradient:  0.3414167968002951
iteration : 2152
train acc:  0.8125
train loss:  0.36771029233932495
train gradient:  0.3355107220142208
iteration : 2153
train acc:  0.8671875
train loss:  0.3408641815185547
train gradient:  0.48919289311952013
iteration : 2154
train acc:  0.8125
train loss:  0.395640105009079
train gradient:  0.4059398139824388
iteration : 2155
train acc:  0.84375
train loss:  0.3704037368297577
train gradient:  0.3388051685127783
iteration : 2156
train acc:  0.7890625
train loss:  0.42605364322662354
train gradient:  0.5491482830475554
iteration : 2157
train acc:  0.8203125
train loss:  0.3653492033481598
train gradient:  0.4741476919184688
iteration : 2158
train acc:  0.8203125
train loss:  0.4027506709098816
train gradient:  0.3328185044513338
iteration : 2159
train acc:  0.78125
train loss:  0.4869711399078369
train gradient:  0.6252962351095732
iteration : 2160
train acc:  0.8046875
train loss:  0.39361533522605896
train gradient:  0.43832766809421136
iteration : 2161
train acc:  0.796875
train loss:  0.3794515132904053
train gradient:  0.42332909141410596
iteration : 2162
train acc:  0.7578125
train loss:  0.474926233291626
train gradient:  0.47526314077488346
iteration : 2163
train acc:  0.859375
train loss:  0.32469117641448975
train gradient:  0.32086090901649256
iteration : 2164
train acc:  0.8046875
train loss:  0.40584906935691833
train gradient:  0.3547459153144113
iteration : 2165
train acc:  0.8359375
train loss:  0.3390014171600342
train gradient:  0.4162191678270929
iteration : 2166
train acc:  0.8203125
train loss:  0.3971560001373291
train gradient:  0.3650997179329744
iteration : 2167
train acc:  0.8046875
train loss:  0.39330172538757324
train gradient:  0.3169222071645653
iteration : 2168
train acc:  0.828125
train loss:  0.408719003200531
train gradient:  0.5048794412849583
iteration : 2169
train acc:  0.7890625
train loss:  0.47256752848625183
train gradient:  0.6533951260627038
iteration : 2170
train acc:  0.78125
train loss:  0.41698962450027466
train gradient:  0.573201973914073
iteration : 2171
train acc:  0.7890625
train loss:  0.3869495391845703
train gradient:  0.430827019183416
iteration : 2172
train acc:  0.890625
train loss:  0.3086238503456116
train gradient:  0.26685354427390306
iteration : 2173
train acc:  0.8125
train loss:  0.37873274087905884
train gradient:  0.38070519931983965
iteration : 2174
train acc:  0.7578125
train loss:  0.5164320468902588
train gradient:  0.6515871720846446
iteration : 2175
train acc:  0.78125
train loss:  0.44903233647346497
train gradient:  0.5099471212386135
iteration : 2176
train acc:  0.8125
train loss:  0.40120527148246765
train gradient:  0.45986684306945025
iteration : 2177
train acc:  0.7890625
train loss:  0.39469653367996216
train gradient:  0.4382500468875115
iteration : 2178
train acc:  0.8515625
train loss:  0.3803028166294098
train gradient:  0.35095661127153055
iteration : 2179
train acc:  0.8203125
train loss:  0.4259205460548401
train gradient:  0.46315644570330405
iteration : 2180
train acc:  0.8671875
train loss:  0.3440394401550293
train gradient:  0.3726273973054491
iteration : 2181
train acc:  0.84375
train loss:  0.4416084289550781
train gradient:  0.5384219955918935
iteration : 2182
train acc:  0.8359375
train loss:  0.4282434582710266
train gradient:  0.4931976355734116
iteration : 2183
train acc:  0.8046875
train loss:  0.43004417419433594
train gradient:  0.5027063121884372
iteration : 2184
train acc:  0.8125
train loss:  0.39962536096572876
train gradient:  0.47826482011247773
iteration : 2185
train acc:  0.90625
train loss:  0.28868889808654785
train gradient:  0.27798009735996976
iteration : 2186
train acc:  0.8046875
train loss:  0.4111533761024475
train gradient:  0.3795127069226283
iteration : 2187
train acc:  0.84375
train loss:  0.3589114248752594
train gradient:  0.3258233115103943
iteration : 2188
train acc:  0.84375
train loss:  0.37576165795326233
train gradient:  0.3652072624166607
iteration : 2189
train acc:  0.8359375
train loss:  0.39381569623947144
train gradient:  0.3919134174307774
iteration : 2190
train acc:  0.7890625
train loss:  0.39133793115615845
train gradient:  0.3580086222071502
iteration : 2191
train acc:  0.875
train loss:  0.28734463453292847
train gradient:  0.2459943313232621
iteration : 2192
train acc:  0.7734375
train loss:  0.3856627941131592
train gradient:  0.42487045438939564
iteration : 2193
train acc:  0.859375
train loss:  0.3589247167110443
train gradient:  0.3300430924529154
iteration : 2194
train acc:  0.8671875
train loss:  0.33805230259895325
train gradient:  0.36468713673136166
iteration : 2195
train acc:  0.828125
train loss:  0.4069055914878845
train gradient:  0.3911267139821577
iteration : 2196
train acc:  0.8515625
train loss:  0.3503406345844269
train gradient:  0.31046794536378647
iteration : 2197
train acc:  0.8359375
train loss:  0.3695884644985199
train gradient:  0.36601532767256717
iteration : 2198
train acc:  0.8125
train loss:  0.5399251580238342
train gradient:  0.5429149765065164
iteration : 2199
train acc:  0.84375
train loss:  0.4253312945365906
train gradient:  0.47389219532079646
iteration : 2200
train acc:  0.8125
train loss:  0.41221028566360474
train gradient:  0.5562563186452825
iteration : 2201
train acc:  0.7734375
train loss:  0.5216255187988281
train gradient:  0.6933789749157452
iteration : 2202
train acc:  0.8515625
train loss:  0.31889697909355164
train gradient:  0.36579228420942067
iteration : 2203
train acc:  0.828125
train loss:  0.37977635860443115
train gradient:  0.4357707162658227
iteration : 2204
train acc:  0.8671875
train loss:  0.3737606406211853
train gradient:  0.2921949459679574
iteration : 2205
train acc:  0.8359375
train loss:  0.4124501943588257
train gradient:  0.32383473153949494
iteration : 2206
train acc:  0.859375
train loss:  0.33670833706855774
train gradient:  0.359467545906779
iteration : 2207
train acc:  0.7890625
train loss:  0.4793277978897095
train gradient:  0.5522296470002299
iteration : 2208
train acc:  0.78125
train loss:  0.5092722177505493
train gradient:  0.4946531800657306
iteration : 2209
train acc:  0.828125
train loss:  0.3823314309120178
train gradient:  0.41783516728483994
iteration : 2210
train acc:  0.7890625
train loss:  0.43088722229003906
train gradient:  0.35851944050877366
iteration : 2211
train acc:  0.828125
train loss:  0.3706362247467041
train gradient:  0.26636916352513534
iteration : 2212
train acc:  0.8203125
train loss:  0.45133504271507263
train gradient:  0.5942872211974242
iteration : 2213
train acc:  0.828125
train loss:  0.37729737162590027
train gradient:  0.3611661888096048
iteration : 2214
train acc:  0.859375
train loss:  0.3410319685935974
train gradient:  0.3018700098287532
iteration : 2215
train acc:  0.84375
train loss:  0.3642965853214264
train gradient:  0.3667816999831475
iteration : 2216
train acc:  0.84375
train loss:  0.40073978900909424
train gradient:  0.4542453568420881
iteration : 2217
train acc:  0.8359375
train loss:  0.3681923449039459
train gradient:  0.28103822626488084
iteration : 2218
train acc:  0.8203125
train loss:  0.38396158814430237
train gradient:  0.32759167090577046
iteration : 2219
train acc:  0.796875
train loss:  0.38503187894821167
train gradient:  0.3195883913641254
iteration : 2220
train acc:  0.84375
train loss:  0.39726996421813965
train gradient:  0.4439418107042001
iteration : 2221
train acc:  0.8203125
train loss:  0.3371332883834839
train gradient:  0.35780011046939947
iteration : 2222
train acc:  0.8359375
train loss:  0.39325380325317383
train gradient:  0.374321726238478
iteration : 2223
train acc:  0.84375
train loss:  0.3623730540275574
train gradient:  0.26406814135323314
iteration : 2224
train acc:  0.84375
train loss:  0.40952038764953613
train gradient:  0.3764205613565964
iteration : 2225
train acc:  0.875
train loss:  0.3737453818321228
train gradient:  0.3203819544110979
iteration : 2226
train acc:  0.8359375
train loss:  0.3863338232040405
train gradient:  0.26401400850251316
iteration : 2227
train acc:  0.828125
train loss:  0.33900973200798035
train gradient:  0.2826048695925502
iteration : 2228
train acc:  0.8203125
train loss:  0.40554359555244446
train gradient:  0.4238057432081637
iteration : 2229
train acc:  0.796875
train loss:  0.4677358567714691
train gradient:  0.42353534855376296
iteration : 2230
train acc:  0.8359375
train loss:  0.42550331354141235
train gradient:  0.3128624155404457
iteration : 2231
train acc:  0.828125
train loss:  0.3891127109527588
train gradient:  0.4134444211836602
iteration : 2232
train acc:  0.8359375
train loss:  0.37422531843185425
train gradient:  0.31132675557839357
iteration : 2233
train acc:  0.7890625
train loss:  0.5456933379173279
train gradient:  0.5670016669364286
iteration : 2234
train acc:  0.78125
train loss:  0.41385316848754883
train gradient:  0.3520654161149688
iteration : 2235
train acc:  0.8203125
train loss:  0.4526892304420471
train gradient:  0.36285808358988786
iteration : 2236
train acc:  0.8046875
train loss:  0.40044617652893066
train gradient:  0.3595151499903174
iteration : 2237
train acc:  0.8046875
train loss:  0.4276975691318512
train gradient:  0.3783931806982297
iteration : 2238
train acc:  0.84375
train loss:  0.3791358470916748
train gradient:  0.289618047316822
iteration : 2239
train acc:  0.8671875
train loss:  0.36325404047966003
train gradient:  0.2711729180913814
iteration : 2240
train acc:  0.7890625
train loss:  0.39972060918807983
train gradient:  0.3682718741476168
iteration : 2241
train acc:  0.828125
train loss:  0.3972175717353821
train gradient:  0.2617023279969838
iteration : 2242
train acc:  0.8203125
train loss:  0.41614478826522827
train gradient:  0.3262476789835177
iteration : 2243
train acc:  0.8515625
train loss:  0.34683382511138916
train gradient:  0.2575649829430861
iteration : 2244
train acc:  0.8359375
train loss:  0.3893624246120453
train gradient:  0.3538731079788426
iteration : 2245
train acc:  0.8671875
train loss:  0.33601170778274536
train gradient:  0.362630360137659
iteration : 2246
train acc:  0.7734375
train loss:  0.424278199672699
train gradient:  0.39199347263184686
iteration : 2247
train acc:  0.8125
train loss:  0.43567201495170593
train gradient:  0.3535296149615787
iteration : 2248
train acc:  0.859375
train loss:  0.341994047164917
train gradient:  0.27686612170835145
iteration : 2249
train acc:  0.796875
train loss:  0.3761556148529053
train gradient:  0.30842579725092467
iteration : 2250
train acc:  0.78125
train loss:  0.4180329442024231
train gradient:  0.37829356193327784
iteration : 2251
train acc:  0.84375
train loss:  0.41098594665527344
train gradient:  0.41751882843388793
iteration : 2252
train acc:  0.8359375
train loss:  0.3576582670211792
train gradient:  0.3197257586935332
iteration : 2253
train acc:  0.7578125
train loss:  0.4434884190559387
train gradient:  0.46003641603853235
iteration : 2254
train acc:  0.84375
train loss:  0.3571767807006836
train gradient:  0.38673443356285364
iteration : 2255
train acc:  0.75
train loss:  0.4642600417137146
train gradient:  0.31860785536330927
iteration : 2256
train acc:  0.8046875
train loss:  0.3732367157936096
train gradient:  0.34201261004336936
iteration : 2257
train acc:  0.8359375
train loss:  0.3823886513710022
train gradient:  0.28101578685270434
iteration : 2258
train acc:  0.8515625
train loss:  0.3013911247253418
train gradient:  0.27825304716718113
iteration : 2259
train acc:  0.8515625
train loss:  0.35519644618034363
train gradient:  0.3734356560528874
iteration : 2260
train acc:  0.8359375
train loss:  0.39043503999710083
train gradient:  0.37789423660710064
iteration : 2261
train acc:  0.8046875
train loss:  0.430268794298172
train gradient:  0.43990786608449617
iteration : 2262
train acc:  0.7890625
train loss:  0.4301187992095947
train gradient:  0.4304646772351935
iteration : 2263
train acc:  0.8359375
train loss:  0.3562624454498291
train gradient:  0.28951891485465725
iteration : 2264
train acc:  0.796875
train loss:  0.4148186445236206
train gradient:  0.5525688387481472
iteration : 2265
train acc:  0.8125
train loss:  0.3777284026145935
train gradient:  0.41763738500412123
iteration : 2266
train acc:  0.8984375
train loss:  0.29497385025024414
train gradient:  0.2517460181785029
iteration : 2267
train acc:  0.7890625
train loss:  0.4397445619106293
train gradient:  0.4412327310253761
iteration : 2268
train acc:  0.8203125
train loss:  0.42779627442359924
train gradient:  0.34531949729266387
iteration : 2269
train acc:  0.8515625
train loss:  0.4086921215057373
train gradient:  0.363907281316331
iteration : 2270
train acc:  0.84375
train loss:  0.3465762138366699
train gradient:  0.3324792406759208
iteration : 2271
train acc:  0.796875
train loss:  0.5103528499603271
train gradient:  0.5543530439800528
iteration : 2272
train acc:  0.8828125
train loss:  0.3358492851257324
train gradient:  0.3077771551682596
iteration : 2273
train acc:  0.84375
train loss:  0.33403196930885315
train gradient:  0.46734412822616006
iteration : 2274
train acc:  0.8046875
train loss:  0.45441293716430664
train gradient:  0.6874407631299088
iteration : 2275
train acc:  0.8671875
train loss:  0.3732423782348633
train gradient:  0.35828396088342934
iteration : 2276
train acc:  0.8203125
train loss:  0.36051028966903687
train gradient:  0.4307288527152088
iteration : 2277
train acc:  0.828125
train loss:  0.3807469606399536
train gradient:  0.33471241753165376
iteration : 2278
train acc:  0.7421875
train loss:  0.5269055366516113
train gradient:  0.5692929922519773
iteration : 2279
train acc:  0.8046875
train loss:  0.3790373206138611
train gradient:  0.27034990408881143
iteration : 2280
train acc:  0.8515625
train loss:  0.3350868821144104
train gradient:  0.3196774862531104
iteration : 2281
train acc:  0.7734375
train loss:  0.47021666169166565
train gradient:  0.4771170741424949
iteration : 2282
train acc:  0.765625
train loss:  0.4586747884750366
train gradient:  0.586703921466151
iteration : 2283
train acc:  0.828125
train loss:  0.4151539206504822
train gradient:  0.31343594622602994
iteration : 2284
train acc:  0.8515625
train loss:  0.3380296230316162
train gradient:  0.3431810368907087
iteration : 2285
train acc:  0.84375
train loss:  0.3723621666431427
train gradient:  0.3552883425133715
iteration : 2286
train acc:  0.8203125
train loss:  0.3928155303001404
train gradient:  0.3019473326744781
iteration : 2287
train acc:  0.8359375
train loss:  0.3212657570838928
train gradient:  0.2747079141161094
iteration : 2288
train acc:  0.8203125
train loss:  0.43021970987319946
train gradient:  0.4411629624534757
iteration : 2289
train acc:  0.78125
train loss:  0.4548611044883728
train gradient:  0.4508506787713194
iteration : 2290
train acc:  0.828125
train loss:  0.3770064413547516
train gradient:  0.3016699617312854
iteration : 2291
train acc:  0.8203125
train loss:  0.3674660921096802
train gradient:  0.3016695750406358
iteration : 2292
train acc:  0.78125
train loss:  0.4061466157436371
train gradient:  0.31550172591772485
iteration : 2293
train acc:  0.8125
train loss:  0.4323338568210602
train gradient:  0.45125039749919826
iteration : 2294
train acc:  0.8828125
train loss:  0.3068300187587738
train gradient:  0.2454106270890054
iteration : 2295
train acc:  0.84375
train loss:  0.3453734517097473
train gradient:  0.33533202494994224
iteration : 2296
train acc:  0.8359375
train loss:  0.37053829431533813
train gradient:  0.31478954437466494
iteration : 2297
train acc:  0.9140625
train loss:  0.295472115278244
train gradient:  0.2232271827179156
iteration : 2298
train acc:  0.828125
train loss:  0.4093477725982666
train gradient:  0.5897236360215233
iteration : 2299
train acc:  0.8125
train loss:  0.41949525475502014
train gradient:  0.32099865733749217
iteration : 2300
train acc:  0.84375
train loss:  0.3321504592895508
train gradient:  0.32349985832070594
iteration : 2301
train acc:  0.84375
train loss:  0.35802093148231506
train gradient:  0.2848886311148121
iteration : 2302
train acc:  0.8828125
train loss:  0.3159707188606262
train gradient:  0.23262639559685536
iteration : 2303
train acc:  0.875
train loss:  0.31159067153930664
train gradient:  0.3099624137652176
iteration : 2304
train acc:  0.78125
train loss:  0.4452380836009979
train gradient:  0.44315743014532966
iteration : 2305
train acc:  0.8203125
train loss:  0.40151286125183105
train gradient:  0.600621442571142
iteration : 2306
train acc:  0.8671875
train loss:  0.3450934886932373
train gradient:  0.2958172451999481
iteration : 2307
train acc:  0.828125
train loss:  0.4055890738964081
train gradient:  0.3183920190513765
iteration : 2308
train acc:  0.8203125
train loss:  0.411555677652359
train gradient:  0.44218564815752925
iteration : 2309
train acc:  0.8515625
train loss:  0.3296352028846741
train gradient:  0.31184324311751105
iteration : 2310
train acc:  0.8203125
train loss:  0.4077027440071106
train gradient:  0.3977352120472816
iteration : 2311
train acc:  0.84375
train loss:  0.370483934879303
train gradient:  0.5197165580474248
iteration : 2312
train acc:  0.8125
train loss:  0.3749528229236603
train gradient:  0.46146307688595284
iteration : 2313
train acc:  0.8203125
train loss:  0.3630584478378296
train gradient:  0.3932499040524932
iteration : 2314
train acc:  0.8828125
train loss:  0.3162528872489929
train gradient:  0.2987004288227009
iteration : 2315
train acc:  0.8515625
train loss:  0.3465685248374939
train gradient:  0.2996283545450201
iteration : 2316
train acc:  0.8046875
train loss:  0.39940354228019714
train gradient:  0.32873401590367807
iteration : 2317
train acc:  0.875
train loss:  0.3457365334033966
train gradient:  0.3714846272415316
iteration : 2318
train acc:  0.859375
train loss:  0.3505129814147949
train gradient:  0.26538339747020845
iteration : 2319
train acc:  0.796875
train loss:  0.446114182472229
train gradient:  0.6159043128071031
iteration : 2320
train acc:  0.859375
train loss:  0.33624476194381714
train gradient:  0.31281115616871374
iteration : 2321
train acc:  0.8203125
train loss:  0.4034305214881897
train gradient:  0.6151580885473973
iteration : 2322
train acc:  0.8046875
train loss:  0.40761634707450867
train gradient:  0.4466207024827069
iteration : 2323
train acc:  0.8515625
train loss:  0.3320024311542511
train gradient:  0.44185950343397756
iteration : 2324
train acc:  0.828125
train loss:  0.3783983588218689
train gradient:  0.359189546589837
iteration : 2325
train acc:  0.8671875
train loss:  0.3452381193637848
train gradient:  0.26507811682578375
iteration : 2326
train acc:  0.8203125
train loss:  0.3924080729484558
train gradient:  0.3688372294510759
iteration : 2327
train acc:  0.859375
train loss:  0.33330681920051575
train gradient:  0.5428128696565871
iteration : 2328
train acc:  0.8046875
train loss:  0.42200767993927
train gradient:  0.37141194490291535
iteration : 2329
train acc:  0.875
train loss:  0.3087518811225891
train gradient:  0.2591383742435209
iteration : 2330
train acc:  0.78125
train loss:  0.44444990158081055
train gradient:  0.6761745275899507
iteration : 2331
train acc:  0.84375
train loss:  0.35931551456451416
train gradient:  0.29717045023945476
iteration : 2332
train acc:  0.8203125
train loss:  0.45733460783958435
train gradient:  0.547174165050035
iteration : 2333
train acc:  0.7890625
train loss:  0.46176648139953613
train gradient:  0.5239868665526857
iteration : 2334
train acc:  0.828125
train loss:  0.38454172015190125
train gradient:  0.47377250961349743
iteration : 2335
train acc:  0.84375
train loss:  0.39593255519866943
train gradient:  0.40037477521059095
iteration : 2336
train acc:  0.78125
train loss:  0.4585108757019043
train gradient:  0.56598815642978
iteration : 2337
train acc:  0.84375
train loss:  0.3839740455150604
train gradient:  0.3225961937244315
iteration : 2338
train acc:  0.8203125
train loss:  0.37705451250076294
train gradient:  0.40472996294860697
iteration : 2339
train acc:  0.875
train loss:  0.3384476602077484
train gradient:  0.385774682842254
iteration : 2340
train acc:  0.84375
train loss:  0.3653313219547272
train gradient:  0.3885778414894423
iteration : 2341
train acc:  0.8671875
train loss:  0.37582120299339294
train gradient:  0.3726038957413486
iteration : 2342
train acc:  0.84375
train loss:  0.4156862497329712
train gradient:  0.4149974075003179
iteration : 2343
train acc:  0.8359375
train loss:  0.3585309386253357
train gradient:  0.45250578289705357
iteration : 2344
train acc:  0.8515625
train loss:  0.3806484043598175
train gradient:  0.272745980066912
iteration : 2345
train acc:  0.8359375
train loss:  0.3542427718639374
train gradient:  0.25984903880361615
iteration : 2346
train acc:  0.8125
train loss:  0.4634377360343933
train gradient:  0.6018548701019723
iteration : 2347
train acc:  0.8125
train loss:  0.4558261036872864
train gradient:  0.5160478739635891
iteration : 2348
train acc:  0.84375
train loss:  0.37671133875846863
train gradient:  0.30169650875881265
iteration : 2349
train acc:  0.8515625
train loss:  0.3219479024410248
train gradient:  0.21565058573439438
iteration : 2350
train acc:  0.8515625
train loss:  0.3529701828956604
train gradient:  0.40137403763710117
iteration : 2351
train acc:  0.8828125
train loss:  0.3265474736690521
train gradient:  0.3217358301756333
iteration : 2352
train acc:  0.8359375
train loss:  0.35888129472732544
train gradient:  0.35594295728136016
iteration : 2353
train acc:  0.84375
train loss:  0.36241087317466736
train gradient:  0.2528630329107557
iteration : 2354
train acc:  0.7734375
train loss:  0.44767534732818604
train gradient:  0.4270714152522384
iteration : 2355
train acc:  0.84375
train loss:  0.346381276845932
train gradient:  0.20770375639391575
iteration : 2356
train acc:  0.8203125
train loss:  0.3807903528213501
train gradient:  0.5138099583941591
iteration : 2357
train acc:  0.8828125
train loss:  0.3033609390258789
train gradient:  0.2589923510869647
iteration : 2358
train acc:  0.8984375
train loss:  0.26715320348739624
train gradient:  0.3085467997735731
iteration : 2359
train acc:  0.7265625
train loss:  0.4894139766693115
train gradient:  0.5242193753242177
iteration : 2360
train acc:  0.8203125
train loss:  0.3438149392604828
train gradient:  0.3094457639862827
iteration : 2361
train acc:  0.7734375
train loss:  0.4911017417907715
train gradient:  0.41686677149975826
iteration : 2362
train acc:  0.8125
train loss:  0.4267996549606323
train gradient:  0.4542648791500278
iteration : 2363
train acc:  0.828125
train loss:  0.37865984439849854
train gradient:  0.37886566733260313
iteration : 2364
train acc:  0.734375
train loss:  0.5159350633621216
train gradient:  0.6085048168138587
iteration : 2365
train acc:  0.8671875
train loss:  0.32357388734817505
train gradient:  0.34085389843272146
iteration : 2366
train acc:  0.8046875
train loss:  0.39709505438804626
train gradient:  0.36059891663797367
iteration : 2367
train acc:  0.8515625
train loss:  0.3554202616214752
train gradient:  0.375455291239436
iteration : 2368
train acc:  0.828125
train loss:  0.4045476019382477
train gradient:  0.5155228037405114
iteration : 2369
train acc:  0.8125
train loss:  0.4498397707939148
train gradient:  0.3972648194631441
iteration : 2370
train acc:  0.828125
train loss:  0.4347378611564636
train gradient:  0.45938896666826073
iteration : 2371
train acc:  0.8671875
train loss:  0.33839553594589233
train gradient:  0.24489897574323988
iteration : 2372
train acc:  0.8515625
train loss:  0.3768509030342102
train gradient:  0.37265288735626284
iteration : 2373
train acc:  0.8671875
train loss:  0.3622836470603943
train gradient:  0.41682343849760956
iteration : 2374
train acc:  0.828125
train loss:  0.40854135155677795
train gradient:  0.5677112770779815
iteration : 2375
train acc:  0.8515625
train loss:  0.3347955048084259
train gradient:  0.3125958516988379
iteration : 2376
train acc:  0.8515625
train loss:  0.40731000900268555
train gradient:  0.4144119245611766
iteration : 2377
train acc:  0.8125
train loss:  0.37498122453689575
train gradient:  0.3794509676780868
iteration : 2378
train acc:  0.90625
train loss:  0.29992949962615967
train gradient:  0.2311599794562632
iteration : 2379
train acc:  0.8046875
train loss:  0.3860458731651306
train gradient:  0.31827751963022693
iteration : 2380
train acc:  0.875
train loss:  0.34513765573501587
train gradient:  0.3663106615438482
iteration : 2381
train acc:  0.8046875
train loss:  0.4526493549346924
train gradient:  0.5042010470159453
iteration : 2382
train acc:  0.7578125
train loss:  0.5064007043838501
train gradient:  0.5682330486644994
iteration : 2383
train acc:  0.8515625
train loss:  0.3727099299430847
train gradient:  0.38556794257225513
iteration : 2384
train acc:  0.78125
train loss:  0.45949307084083557
train gradient:  0.5874200189873299
iteration : 2385
train acc:  0.828125
train loss:  0.36100268363952637
train gradient:  0.3731073928296558
iteration : 2386
train acc:  0.84375
train loss:  0.3065449595451355
train gradient:  0.30463989779516804
iteration : 2387
train acc:  0.8671875
train loss:  0.3418157398700714
train gradient:  0.3065442348537207
iteration : 2388
train acc:  0.8515625
train loss:  0.34404096007347107
train gradient:  0.34460868227829816
iteration : 2389
train acc:  0.7890625
train loss:  0.4628230631351471
train gradient:  0.5857217337785041
iteration : 2390
train acc:  0.8984375
train loss:  0.29840797185897827
train gradient:  0.3291870098811737
iteration : 2391
train acc:  0.8125
train loss:  0.4063918888568878
train gradient:  0.3380324405043489
iteration : 2392
train acc:  0.8203125
train loss:  0.37438124418258667
train gradient:  0.32455349330442523
iteration : 2393
train acc:  0.7734375
train loss:  0.43913257122039795
train gradient:  0.4752193274997156
iteration : 2394
train acc:  0.8359375
train loss:  0.3753054738044739
train gradient:  0.36415551819263603
iteration : 2395
train acc:  0.7890625
train loss:  0.3802095651626587
train gradient:  0.39229280729573346
iteration : 2396
train acc:  0.828125
train loss:  0.3566854000091553
train gradient:  0.37934633682999286
iteration : 2397
train acc:  0.8125
train loss:  0.41469594836235046
train gradient:  0.5368872425062554
iteration : 2398
train acc:  0.8125
train loss:  0.372985303401947
train gradient:  0.45915127621888685
iteration : 2399
train acc:  0.7890625
train loss:  0.4131366014480591
train gradient:  0.39338871218731813
iteration : 2400
train acc:  0.859375
train loss:  0.3479810655117035
train gradient:  0.34528228153255297
iteration : 2401
train acc:  0.796875
train loss:  0.42414528131484985
train gradient:  0.5925846062576756
iteration : 2402
train acc:  0.796875
train loss:  0.41741204261779785
train gradient:  0.686000752422936
iteration : 2403
train acc:  0.84375
train loss:  0.3659364581108093
train gradient:  0.401681091640242
iteration : 2404
train acc:  0.8125
train loss:  0.385673463344574
train gradient:  0.3615855505349932
iteration : 2405
train acc:  0.8125
train loss:  0.40596434473991394
train gradient:  0.4894637168358071
iteration : 2406
train acc:  0.828125
train loss:  0.3571327328681946
train gradient:  0.4195277936411852
iteration : 2407
train acc:  0.84375
train loss:  0.4149671196937561
train gradient:  0.3599562514264362
iteration : 2408
train acc:  0.8125
train loss:  0.3662760853767395
train gradient:  0.29253466913122633
iteration : 2409
train acc:  0.8125
train loss:  0.46875515580177307
train gradient:  0.5149709064108199
iteration : 2410
train acc:  0.7890625
train loss:  0.4779980182647705
train gradient:  0.47664014287909867
iteration : 2411
train acc:  0.828125
train loss:  0.3860433101654053
train gradient:  0.3742840692978396
iteration : 2412
train acc:  0.8671875
train loss:  0.28920990228652954
train gradient:  0.2805695326794235
iteration : 2413
train acc:  0.7890625
train loss:  0.4345357418060303
train gradient:  0.49783806137797215
iteration : 2414
train acc:  0.7890625
train loss:  0.41061097383499146
train gradient:  0.48740876026781227
iteration : 2415
train acc:  0.8359375
train loss:  0.36905789375305176
train gradient:  0.31389792422627183
iteration : 2416
train acc:  0.8203125
train loss:  0.34965193271636963
train gradient:  0.31914647500705173
iteration : 2417
train acc:  0.8671875
train loss:  0.3400534391403198
train gradient:  0.3082840345209223
iteration : 2418
train acc:  0.8515625
train loss:  0.3860700726509094
train gradient:  0.4442117473653474
iteration : 2419
train acc:  0.78125
train loss:  0.40250468254089355
train gradient:  0.41615696465681046
iteration : 2420
train acc:  0.90625
train loss:  0.28430694341659546
train gradient:  0.20900989409341392
iteration : 2421
train acc:  0.8359375
train loss:  0.3648640513420105
train gradient:  0.3175132270754802
iteration : 2422
train acc:  0.859375
train loss:  0.29724496603012085
train gradient:  0.2367687411290807
iteration : 2423
train acc:  0.7890625
train loss:  0.43388497829437256
train gradient:  0.40011585190960974
iteration : 2424
train acc:  0.8203125
train loss:  0.40334296226501465
train gradient:  0.3695063429118095
iteration : 2425
train acc:  0.765625
train loss:  0.4934384226799011
train gradient:  0.6140004895383101
iteration : 2426
train acc:  0.8125
train loss:  0.48850199580192566
train gradient:  0.53608381643129
iteration : 2427
train acc:  0.859375
train loss:  0.3702961206436157
train gradient:  0.3468727040612847
iteration : 2428
train acc:  0.828125
train loss:  0.344085693359375
train gradient:  0.31375728843107226
iteration : 2429
train acc:  0.84375
train loss:  0.3607037663459778
train gradient:  0.3210765820265778
iteration : 2430
train acc:  0.890625
train loss:  0.28359681367874146
train gradient:  0.2113820270980285
iteration : 2431
train acc:  0.8515625
train loss:  0.34372368454933167
train gradient:  0.36081701691459805
iteration : 2432
train acc:  0.7578125
train loss:  0.47107458114624023
train gradient:  0.5687501359017935
iteration : 2433
train acc:  0.828125
train loss:  0.4535157382488251
train gradient:  0.5159015494655219
iteration : 2434
train acc:  0.890625
train loss:  0.29439470171928406
train gradient:  0.2246102875278023
iteration : 2435
train acc:  0.828125
train loss:  0.4447287917137146
train gradient:  0.42312528605399435
iteration : 2436
train acc:  0.75
train loss:  0.4719001054763794
train gradient:  0.6523662128191171
iteration : 2437
train acc:  0.7734375
train loss:  0.4061393737792969
train gradient:  0.3082601120846061
iteration : 2438
train acc:  0.84375
train loss:  0.40889114141464233
train gradient:  0.48733383207534786
iteration : 2439
train acc:  0.875
train loss:  0.36832195520401
train gradient:  0.4236826425056598
iteration : 2440
train acc:  0.8125
train loss:  0.3881221413612366
train gradient:  0.29926028079833167
iteration : 2441
train acc:  0.7421875
train loss:  0.496359646320343
train gradient:  0.5853135870659375
iteration : 2442
train acc:  0.8828125
train loss:  0.2667839527130127
train gradient:  0.25804382262850417
iteration : 2443
train acc:  0.765625
train loss:  0.458702027797699
train gradient:  0.5081847550827946
iteration : 2444
train acc:  0.8203125
train loss:  0.38656800985336304
train gradient:  0.34979625905393985
iteration : 2445
train acc:  0.84375
train loss:  0.4046168923377991
train gradient:  0.4096491848019719
iteration : 2446
train acc:  0.8359375
train loss:  0.3318253755569458
train gradient:  0.41452065504849417
iteration : 2447
train acc:  0.796875
train loss:  0.4858657121658325
train gradient:  0.49086200770845884
iteration : 2448
train acc:  0.8046875
train loss:  0.3922286331653595
train gradient:  0.480788637498345
iteration : 2449
train acc:  0.796875
train loss:  0.46101275086402893
train gradient:  0.32007509627159153
iteration : 2450
train acc:  0.8046875
train loss:  0.3927612900733948
train gradient:  0.4025346812872515
iteration : 2451
train acc:  0.8984375
train loss:  0.29690438508987427
train gradient:  0.2432350990993519
iteration : 2452
train acc:  0.8359375
train loss:  0.3686216473579407
train gradient:  0.36919540700439074
iteration : 2453
train acc:  0.8359375
train loss:  0.4047219157218933
train gradient:  0.5574781781377536
iteration : 2454
train acc:  0.828125
train loss:  0.4263988137245178
train gradient:  0.35336293606218316
iteration : 2455
train acc:  0.828125
train loss:  0.37640833854675293
train gradient:  0.3386986056577449
iteration : 2456
train acc:  0.796875
train loss:  0.48211079835891724
train gradient:  0.4943699376225386
iteration : 2457
train acc:  0.84375
train loss:  0.36358675360679626
train gradient:  0.3527522162902886
iteration : 2458
train acc:  0.828125
train loss:  0.36758795380592346
train gradient:  0.31375998674502775
iteration : 2459
train acc:  0.7578125
train loss:  0.5003644824028015
train gradient:  0.6171964722315869
iteration : 2460
train acc:  0.84375
train loss:  0.45109227299690247
train gradient:  0.44492834575060464
iteration : 2461
train acc:  0.875
train loss:  0.370033323764801
train gradient:  0.36826388210635197
iteration : 2462
train acc:  0.8203125
train loss:  0.4166299104690552
train gradient:  0.41974966574982714
iteration : 2463
train acc:  0.84375
train loss:  0.39113178849220276
train gradient:  0.37719785604959866
iteration : 2464
train acc:  0.8125
train loss:  0.3770708441734314
train gradient:  0.3463673555301686
iteration : 2465
train acc:  0.8125
train loss:  0.4387328624725342
train gradient:  0.4717896671769137
iteration : 2466
train acc:  0.8203125
train loss:  0.3445429801940918
train gradient:  0.392277403478243
iteration : 2467
train acc:  0.828125
train loss:  0.36355018615722656
train gradient:  0.3748294535588541
iteration : 2468
train acc:  0.84375
train loss:  0.3428250551223755
train gradient:  0.25297756511531505
iteration : 2469
train acc:  0.84375
train loss:  0.329124391078949
train gradient:  0.2764616154671863
iteration : 2470
train acc:  0.9140625
train loss:  0.2590813636779785
train gradient:  0.2219017019132437
iteration : 2471
train acc:  0.8125
train loss:  0.3745875656604767
train gradient:  0.378150909586053
iteration : 2472
train acc:  0.8515625
train loss:  0.3575587570667267
train gradient:  0.26426652277627133
iteration : 2473
train acc:  0.78125
train loss:  0.3979705572128296
train gradient:  0.4454057945683426
iteration : 2474
train acc:  0.8203125
train loss:  0.4051439166069031
train gradient:  0.38588926326753514
iteration : 2475
train acc:  0.796875
train loss:  0.4396399259567261
train gradient:  0.3590201627182323
iteration : 2476
train acc:  0.78125
train loss:  0.42049068212509155
train gradient:  0.45152570579881246
iteration : 2477
train acc:  0.8203125
train loss:  0.44078516960144043
train gradient:  0.5757486051865295
iteration : 2478
train acc:  0.8359375
train loss:  0.3668445944786072
train gradient:  0.3329946249594239
iteration : 2479
train acc:  0.75
train loss:  0.48752182722091675
train gradient:  0.5051443902846889
iteration : 2480
train acc:  0.8359375
train loss:  0.40376517176628113
train gradient:  0.358114361599575
iteration : 2481
train acc:  0.796875
train loss:  0.5198537707328796
train gradient:  0.608652315543327
iteration : 2482
train acc:  0.8515625
train loss:  0.34612327814102173
train gradient:  0.28554015991562326
iteration : 2483
train acc:  0.7734375
train loss:  0.47024691104888916
train gradient:  0.4909914345501715
iteration : 2484
train acc:  0.8515625
train loss:  0.3135582208633423
train gradient:  0.31441285214035836
iteration : 2485
train acc:  0.796875
train loss:  0.43917372822761536
train gradient:  0.4632795055126795
iteration : 2486
train acc:  0.8125
train loss:  0.43492650985717773
train gradient:  0.4810796799647569
iteration : 2487
train acc:  0.8359375
train loss:  0.36038222908973694
train gradient:  0.37781514810465966
iteration : 2488
train acc:  0.796875
train loss:  0.4099297523498535
train gradient:  0.3165077664373297
iteration : 2489
train acc:  0.828125
train loss:  0.41929489374160767
train gradient:  0.4436012670967118
iteration : 2490
train acc:  0.8046875
train loss:  0.45950087904930115
train gradient:  0.4043547856147797
iteration : 2491
train acc:  0.859375
train loss:  0.37014061212539673
train gradient:  0.40255717400614016
iteration : 2492
train acc:  0.765625
train loss:  0.4863508939743042
train gradient:  0.5131534889037981
iteration : 2493
train acc:  0.8046875
train loss:  0.42372196912765503
train gradient:  0.3794541238173884
iteration : 2494
train acc:  0.859375
train loss:  0.34696078300476074
train gradient:  0.3217235068621545
iteration : 2495
train acc:  0.8046875
train loss:  0.448164701461792
train gradient:  0.5141335742270807
iteration : 2496
train acc:  0.828125
train loss:  0.3781578242778778
train gradient:  0.36091893963864996
iteration : 2497
train acc:  0.7734375
train loss:  0.4259330630302429
train gradient:  0.3832645802550218
iteration : 2498
train acc:  0.8203125
train loss:  0.365802139043808
train gradient:  0.3646824316739408
iteration : 2499
train acc:  0.8203125
train loss:  0.4074594974517822
train gradient:  0.39628994980975457
iteration : 2500
train acc:  0.7890625
train loss:  0.4724797308444977
train gradient:  0.4645978877310075
iteration : 2501
train acc:  0.75
train loss:  0.4311056137084961
train gradient:  0.45781024006270205
iteration : 2502
train acc:  0.84375
train loss:  0.3662751317024231
train gradient:  0.3216233925526036
iteration : 2503
train acc:  0.84375
train loss:  0.42134198546409607
train gradient:  0.35892294582186873
iteration : 2504
train acc:  0.7578125
train loss:  0.5427096486091614
train gradient:  0.6118337113667893
iteration : 2505
train acc:  0.8359375
train loss:  0.364459753036499
train gradient:  0.3319169323385072
iteration : 2506
train acc:  0.8515625
train loss:  0.35569649934768677
train gradient:  0.3712942750296511
iteration : 2507
train acc:  0.8359375
train loss:  0.41171717643737793
train gradient:  0.3647481092173294
iteration : 2508
train acc:  0.8046875
train loss:  0.42121756076812744
train gradient:  0.39894191206989205
iteration : 2509
train acc:  0.78125
train loss:  0.48563653230667114
train gradient:  0.4305073267686337
iteration : 2510
train acc:  0.796875
train loss:  0.4173877239227295
train gradient:  0.3675411349427624
iteration : 2511
train acc:  0.84375
train loss:  0.3306170105934143
train gradient:  0.3095615855833351
iteration : 2512
train acc:  0.8125
train loss:  0.40089917182922363
train gradient:  0.26903836505736123
iteration : 2513
train acc:  0.8515625
train loss:  0.3698570132255554
train gradient:  0.3098142841236957
iteration : 2514
train acc:  0.78125
train loss:  0.4062846899032593
train gradient:  0.36073197935475304
iteration : 2515
train acc:  0.8515625
train loss:  0.37194639444351196
train gradient:  0.3419753574342715
iteration : 2516
train acc:  0.8359375
train loss:  0.33471110463142395
train gradient:  0.18930437913253784
iteration : 2517
train acc:  0.8203125
train loss:  0.34637224674224854
train gradient:  0.26674895830092943
iteration : 2518
train acc:  0.8046875
train loss:  0.38441574573516846
train gradient:  0.3167920531295898
iteration : 2519
train acc:  0.8828125
train loss:  0.3367135524749756
train gradient:  0.21450415062819278
iteration : 2520
train acc:  0.8203125
train loss:  0.37190303206443787
train gradient:  0.34443788648989593
iteration : 2521
train acc:  0.7734375
train loss:  0.47779881954193115
train gradient:  0.48763607960543986
iteration : 2522
train acc:  0.8203125
train loss:  0.3861103653907776
train gradient:  0.28232678448551274
iteration : 2523
train acc:  0.75
train loss:  0.44888269901275635
train gradient:  0.6066517213292475
iteration : 2524
train acc:  0.7421875
train loss:  0.5669835805892944
train gradient:  0.6922483368023922
iteration : 2525
train acc:  0.78125
train loss:  0.4646338224411011
train gradient:  0.39430500535611046
iteration : 2526
train acc:  0.875
train loss:  0.29018133878707886
train gradient:  0.22274087804257836
iteration : 2527
train acc:  0.7890625
train loss:  0.4098886251449585
train gradient:  0.3559667218603193
iteration : 2528
train acc:  0.8359375
train loss:  0.3769359588623047
train gradient:  0.3744577258955744
iteration : 2529
train acc:  0.859375
train loss:  0.3101672828197479
train gradient:  0.29639813834007567
iteration : 2530
train acc:  0.8359375
train loss:  0.37248021364212036
train gradient:  0.45414503320128136
iteration : 2531
train acc:  0.78125
train loss:  0.41560035943984985
train gradient:  0.4254479643123412
iteration : 2532
train acc:  0.765625
train loss:  0.4486168622970581
train gradient:  0.48731549099257476
iteration : 2533
train acc:  0.84375
train loss:  0.3804038465023041
train gradient:  0.3584158958197715
iteration : 2534
train acc:  0.8359375
train loss:  0.36785170435905457
train gradient:  0.242398806767322
iteration : 2535
train acc:  0.8828125
train loss:  0.2848956882953644
train gradient:  0.1868545735110524
iteration : 2536
train acc:  0.828125
train loss:  0.38852500915527344
train gradient:  0.32662104730150976
iteration : 2537
train acc:  0.8125
train loss:  0.3940412402153015
train gradient:  0.2913418497180356
iteration : 2538
train acc:  0.8515625
train loss:  0.42713773250579834
train gradient:  0.32433991649274596
iteration : 2539
train acc:  0.7734375
train loss:  0.4022548198699951
train gradient:  0.34942000122788697
iteration : 2540
train acc:  0.84375
train loss:  0.35589975118637085
train gradient:  0.36969182084336843
iteration : 2541
train acc:  0.8359375
train loss:  0.3137350082397461
train gradient:  0.23602833627218783
iteration : 2542
train acc:  0.8359375
train loss:  0.39706718921661377
train gradient:  0.6290378418286651
iteration : 2543
train acc:  0.875
train loss:  0.31411004066467285
train gradient:  0.26978717065997765
iteration : 2544
train acc:  0.8125
train loss:  0.38089776039123535
train gradient:  0.3766030911459183
iteration : 2545
train acc:  0.8515625
train loss:  0.3844023644924164
train gradient:  0.39128332472822297
iteration : 2546
train acc:  0.8125
train loss:  0.3692839741706848
train gradient:  0.32113314123471015
iteration : 2547
train acc:  0.84375
train loss:  0.41437026858329773
train gradient:  0.3854358244962775
iteration : 2548
train acc:  0.8046875
train loss:  0.4145451784133911
train gradient:  0.3470285702631313
iteration : 2549
train acc:  0.78125
train loss:  0.5032016038894653
train gradient:  0.6026612672252696
iteration : 2550
train acc:  0.8359375
train loss:  0.35487988591194153
train gradient:  0.32079024695356617
iteration : 2551
train acc:  0.8125
train loss:  0.4039561450481415
train gradient:  0.3866987949766967
iteration : 2552
train acc:  0.8359375
train loss:  0.3860689401626587
train gradient:  0.3287327751370594
iteration : 2553
train acc:  0.8515625
train loss:  0.3292405605316162
train gradient:  0.3601768265692707
iteration : 2554
train acc:  0.8671875
train loss:  0.32565373182296753
train gradient:  0.3023214640916225
iteration : 2555
train acc:  0.84375
train loss:  0.3509533405303955
train gradient:  0.27028626811928774
iteration : 2556
train acc:  0.8671875
train loss:  0.32398200035095215
train gradient:  0.3071662304349762
iteration : 2557
train acc:  0.8515625
train loss:  0.3333476483821869
train gradient:  0.3612576755199064
iteration : 2558
train acc:  0.78125
train loss:  0.40591105818748474
train gradient:  0.3341378216137045
iteration : 2559
train acc:  0.84375
train loss:  0.40423083305358887
train gradient:  0.355534585583956
iteration : 2560
train acc:  0.8125
train loss:  0.4212319850921631
train gradient:  0.45436067649848116
iteration : 2561
train acc:  0.8359375
train loss:  0.3161745071411133
train gradient:  0.34306760088544996
iteration : 2562
train acc:  0.8828125
train loss:  0.276203453540802
train gradient:  0.2998170154752066
iteration : 2563
train acc:  0.84375
train loss:  0.3655836582183838
train gradient:  0.37044020972510616
iteration : 2564
train acc:  0.859375
train loss:  0.34084099531173706
train gradient:  0.34368314967830926
iteration : 2565
train acc:  0.828125
train loss:  0.3861750364303589
train gradient:  0.3076066174725338
iteration : 2566
train acc:  0.8203125
train loss:  0.4471197724342346
train gradient:  0.4614844718261063
iteration : 2567
train acc:  0.8359375
train loss:  0.32367050647735596
train gradient:  0.22908340399639177
iteration : 2568
train acc:  0.8046875
train loss:  0.367895245552063
train gradient:  0.2963329369731463
iteration : 2569
train acc:  0.7890625
train loss:  0.40554821491241455
train gradient:  0.3212779745536109
iteration : 2570
train acc:  0.859375
train loss:  0.35692355036735535
train gradient:  0.33099557442607375
iteration : 2571
train acc:  0.8125
train loss:  0.4329613447189331
train gradient:  0.5936707354895974
iteration : 2572
train acc:  0.78125
train loss:  0.3813927173614502
train gradient:  0.4764658497860555
iteration : 2573
train acc:  0.8046875
train loss:  0.4114335775375366
train gradient:  0.37152783509739307
iteration : 2574
train acc:  0.8515625
train loss:  0.3459125757217407
train gradient:  0.2647031404080201
iteration : 2575
train acc:  0.8203125
train loss:  0.3908313512802124
train gradient:  0.4056492683858708
iteration : 2576
train acc:  0.8046875
train loss:  0.4019063413143158
train gradient:  0.4316685182779539
iteration : 2577
train acc:  0.8671875
train loss:  0.36003291606903076
train gradient:  0.32303093269339384
iteration : 2578
train acc:  0.8203125
train loss:  0.4045897424221039
train gradient:  0.42862116418954105
iteration : 2579
train acc:  0.84375
train loss:  0.33387014269828796
train gradient:  0.39762704795601805
iteration : 2580
train acc:  0.8359375
train loss:  0.3667006194591522
train gradient:  0.3468670476831976
iteration : 2581
train acc:  0.796875
train loss:  0.402641236782074
train gradient:  0.3706378554353247
iteration : 2582
train acc:  0.828125
train loss:  0.38864514231681824
train gradient:  0.4396495016978622
iteration : 2583
train acc:  0.8203125
train loss:  0.4334281086921692
train gradient:  0.506669418926508
iteration : 2584
train acc:  0.8046875
train loss:  0.39046335220336914
train gradient:  0.3983224948193129
iteration : 2585
train acc:  0.765625
train loss:  0.4640737771987915
train gradient:  0.4081218218439293
iteration : 2586
train acc:  0.8359375
train loss:  0.34320008754730225
train gradient:  0.3398305895089416
iteration : 2587
train acc:  0.8203125
train loss:  0.429328590631485
train gradient:  0.44264671843622583
iteration : 2588
train acc:  0.8125
train loss:  0.44135355949401855
train gradient:  0.49252603879926665
iteration : 2589
train acc:  0.8359375
train loss:  0.34815993905067444
train gradient:  0.3403181405760796
iteration : 2590
train acc:  0.8203125
train loss:  0.41763585805892944
train gradient:  0.3503959925554107
iteration : 2591
train acc:  0.859375
train loss:  0.36327439546585083
train gradient:  0.41551256542128734
iteration : 2592
train acc:  0.9140625
train loss:  0.304647296667099
train gradient:  0.2923306607162624
iteration : 2593
train acc:  0.828125
train loss:  0.38100379705429077
train gradient:  0.39016955769693545
iteration : 2594
train acc:  0.7578125
train loss:  0.4143981635570526
train gradient:  0.35939103635450476
iteration : 2595
train acc:  0.7890625
train loss:  0.4904555082321167
train gradient:  0.7292916124905813
iteration : 2596
train acc:  0.859375
train loss:  0.36402714252471924
train gradient:  0.2963299494437498
iteration : 2597
train acc:  0.8203125
train loss:  0.4149998426437378
train gradient:  0.43576592354876
iteration : 2598
train acc:  0.8515625
train loss:  0.3117748200893402
train gradient:  0.2926824049999552
iteration : 2599
train acc:  0.859375
train loss:  0.3264881372451782
train gradient:  0.33327730675975625
iteration : 2600
train acc:  0.7578125
train loss:  0.44796836376190186
train gradient:  0.5754397995390538
iteration : 2601
train acc:  0.8125
train loss:  0.4018755257129669
train gradient:  0.38125937663561965
iteration : 2602
train acc:  0.8125
train loss:  0.39194539189338684
train gradient:  0.3809984141020447
iteration : 2603
train acc:  0.8203125
train loss:  0.4348672926425934
train gradient:  0.4737704520589351
iteration : 2604
train acc:  0.828125
train loss:  0.4744502305984497
train gradient:  1.7619221821778748
iteration : 2605
train acc:  0.8359375
train loss:  0.3951253294944763
train gradient:  0.5734471966991477
iteration : 2606
train acc:  0.796875
train loss:  0.4394727349281311
train gradient:  0.41457227488327963
iteration : 2607
train acc:  0.828125
train loss:  0.40504372119903564
train gradient:  0.37745949122686917
iteration : 2608
train acc:  0.8359375
train loss:  0.40747153759002686
train gradient:  0.3702223394711738
iteration : 2609
train acc:  0.8671875
train loss:  0.3235040009021759
train gradient:  0.24072123391394645
iteration : 2610
train acc:  0.8125
train loss:  0.35519078373908997
train gradient:  0.3535485230573846
iteration : 2611
train acc:  0.765625
train loss:  0.5286521911621094
train gradient:  0.7199189160382184
iteration : 2612
train acc:  0.7890625
train loss:  0.4653918147087097
train gradient:  0.3521120940077354
iteration : 2613
train acc:  0.8203125
train loss:  0.4038797914981842
train gradient:  0.38416314465654666
iteration : 2614
train acc:  0.875
train loss:  0.3502475619316101
train gradient:  0.28702318262913473
iteration : 2615
train acc:  0.78125
train loss:  0.428226113319397
train gradient:  0.463254445124141
iteration : 2616
train acc:  0.8125
train loss:  0.3409166932106018
train gradient:  0.3076918701731767
iteration : 2617
train acc:  0.84375
train loss:  0.3856678605079651
train gradient:  0.5579172349075043
iteration : 2618
train acc:  0.8203125
train loss:  0.38339763879776
train gradient:  0.2827974631452934
iteration : 2619
train acc:  0.84375
train loss:  0.3742135763168335
train gradient:  0.279665890121983
iteration : 2620
train acc:  0.8515625
train loss:  0.32317787408828735
train gradient:  0.2524932263467874
iteration : 2621
train acc:  0.8359375
train loss:  0.3652164340019226
train gradient:  0.30588514920457205
iteration : 2622
train acc:  0.765625
train loss:  0.45327919721603394
train gradient:  0.3849164818524618
iteration : 2623
train acc:  0.875
train loss:  0.3225773870944977
train gradient:  0.2753416229405861
iteration : 2624
train acc:  0.8671875
train loss:  0.316469669342041
train gradient:  0.15704645419751329
iteration : 2625
train acc:  0.8203125
train loss:  0.42713427543640137
train gradient:  0.504240488016671
iteration : 2626
train acc:  0.875
train loss:  0.3049331605434418
train gradient:  0.3381097891118855
iteration : 2627
train acc:  0.796875
train loss:  0.4118894338607788
train gradient:  0.4676950696308136
iteration : 2628
train acc:  0.78125
train loss:  0.4477483034133911
train gradient:  0.5772412621094366
iteration : 2629
train acc:  0.8203125
train loss:  0.3891315162181854
train gradient:  0.35199703432549556
iteration : 2630
train acc:  0.8125
train loss:  0.37422674894332886
train gradient:  0.24803396815067397
iteration : 2631
train acc:  0.765625
train loss:  0.44515490531921387
train gradient:  0.3777947873533132
iteration : 2632
train acc:  0.8359375
train loss:  0.39033207297325134
train gradient:  0.4285057071064307
iteration : 2633
train acc:  0.796875
train loss:  0.450697660446167
train gradient:  0.5663843324453671
iteration : 2634
train acc:  0.8828125
train loss:  0.30308225750923157
train gradient:  0.2317072165421467
iteration : 2635
train acc:  0.78125
train loss:  0.4639648199081421
train gradient:  0.40264639237694433
iteration : 2636
train acc:  0.8203125
train loss:  0.3854461908340454
train gradient:  0.3691262398582749
iteration : 2637
train acc:  0.8046875
train loss:  0.4079791307449341
train gradient:  0.3212748823239373
iteration : 2638
train acc:  0.90625
train loss:  0.28192445635795593
train gradient:  0.20481334840886856
iteration : 2639
train acc:  0.828125
train loss:  0.3402023911476135
train gradient:  0.3405730630337599
iteration : 2640
train acc:  0.84375
train loss:  0.3364218473434448
train gradient:  0.26136079899036013
iteration : 2641
train acc:  0.78125
train loss:  0.4004196524620056
train gradient:  0.3599038300181046
iteration : 2642
train acc:  0.78125
train loss:  0.3985428810119629
train gradient:  0.3913933370910983
iteration : 2643
train acc:  0.859375
train loss:  0.3356785178184509
train gradient:  0.3115164812998517
iteration : 2644
train acc:  0.9140625
train loss:  0.25811219215393066
train gradient:  0.22142628886455534
iteration : 2645
train acc:  0.8671875
train loss:  0.2891855239868164
train gradient:  0.23636687869978795
iteration : 2646
train acc:  0.8671875
train loss:  0.350330650806427
train gradient:  0.2494930239793664
iteration : 2647
train acc:  0.8046875
train loss:  0.3845776319503784
train gradient:  0.46564935950345854
iteration : 2648
train acc:  0.796875
train loss:  0.4610247015953064
train gradient:  0.5647587771637681
iteration : 2649
train acc:  0.8359375
train loss:  0.3281710743904114
train gradient:  0.37354057716778183
iteration : 2650
train acc:  0.8515625
train loss:  0.3801853358745575
train gradient:  0.3673216648975124
iteration : 2651
train acc:  0.859375
train loss:  0.3494773209095001
train gradient:  0.3780382043981132
iteration : 2652
train acc:  0.8125
train loss:  0.40171122550964355
train gradient:  0.49305312141572366
iteration : 2653
train acc:  0.8046875
train loss:  0.433431476354599
train gradient:  0.37411771417576306
iteration : 2654
train acc:  0.875
train loss:  0.34595659375190735
train gradient:  0.37659699268129776
iteration : 2655
train acc:  0.828125
train loss:  0.3676866292953491
train gradient:  0.2664644371122197
iteration : 2656
train acc:  0.8671875
train loss:  0.37861037254333496
train gradient:  0.4246562022172635
iteration : 2657
train acc:  0.8671875
train loss:  0.3557665944099426
train gradient:  0.29755231902424273
iteration : 2658
train acc:  0.8515625
train loss:  0.3759903013706207
train gradient:  0.3247363217981782
iteration : 2659
train acc:  0.8671875
train loss:  0.3121632933616638
train gradient:  0.3159031501362273
iteration : 2660
train acc:  0.875
train loss:  0.36789971590042114
train gradient:  0.4208035983655538
iteration : 2661
train acc:  0.859375
train loss:  0.33775585889816284
train gradient:  0.2858562874003769
iteration : 2662
train acc:  0.8359375
train loss:  0.33969664573669434
train gradient:  0.2690122552258761
iteration : 2663
train acc:  0.8515625
train loss:  0.37909775972366333
train gradient:  0.3740035364860202
iteration : 2664
train acc:  0.8203125
train loss:  0.4108505845069885
train gradient:  0.43339276584136555
iteration : 2665
train acc:  0.8359375
train loss:  0.34496578574180603
train gradient:  0.35498491716889485
iteration : 2666
train acc:  0.7890625
train loss:  0.45210450887680054
train gradient:  0.42223244996881604
iteration : 2667
train acc:  0.8671875
train loss:  0.3500840663909912
train gradient:  0.2086869596867378
iteration : 2668
train acc:  0.8203125
train loss:  0.4297349750995636
train gradient:  0.3936902877950852
iteration : 2669
train acc:  0.75
train loss:  0.5043287873268127
train gradient:  0.5448269401799144
iteration : 2670
train acc:  0.8828125
train loss:  0.3250639736652374
train gradient:  0.23323145630841566
iteration : 2671
train acc:  0.8125
train loss:  0.3936217427253723
train gradient:  0.27470242069696876
iteration : 2672
train acc:  0.8203125
train loss:  0.4234985411167145
train gradient:  0.4676983181395548
iteration : 2673
train acc:  0.796875
train loss:  0.4929490089416504
train gradient:  0.49420083916476254
iteration : 2674
train acc:  0.8046875
train loss:  0.4115074872970581
train gradient:  0.41345268513796746
iteration : 2675
train acc:  0.7890625
train loss:  0.4471336305141449
train gradient:  0.4681277559870165
iteration : 2676
train acc:  0.8203125
train loss:  0.37477707862854004
train gradient:  0.24980161054968358
iteration : 2677
train acc:  0.921875
train loss:  0.2555219531059265
train gradient:  0.204882636485118
iteration : 2678
train acc:  0.84375
train loss:  0.37039270997047424
train gradient:  0.30016074028170425
iteration : 2679
train acc:  0.8359375
train loss:  0.3854597210884094
train gradient:  0.37619721760318275
iteration : 2680
train acc:  0.7890625
train loss:  0.3759797215461731
train gradient:  0.37251140050200504
iteration : 2681
train acc:  0.859375
train loss:  0.31382375955581665
train gradient:  0.3306384393144649
iteration : 2682
train acc:  0.78125
train loss:  0.44184285402297974
train gradient:  0.48994204643986844
iteration : 2683
train acc:  0.8203125
train loss:  0.3754850924015045
train gradient:  0.43039449674557906
iteration : 2684
train acc:  0.8828125
train loss:  0.35058480501174927
train gradient:  0.27691143090075776
iteration : 2685
train acc:  0.78125
train loss:  0.3913693428039551
train gradient:  0.4366670820348325
iteration : 2686
train acc:  0.765625
train loss:  0.4388122260570526
train gradient:  0.5603137357428277
iteration : 2687
train acc:  0.8125
train loss:  0.36886686086654663
train gradient:  0.31726967382663673
iteration : 2688
train acc:  0.828125
train loss:  0.4348002076148987
train gradient:  0.4966089861488531
iteration : 2689
train acc:  0.859375
train loss:  0.35646161437034607
train gradient:  0.36947735144159566
iteration : 2690
train acc:  0.8671875
train loss:  0.3185732364654541
train gradient:  0.29072828310110016
iteration : 2691
train acc:  0.8671875
train loss:  0.2971508502960205
train gradient:  0.23228224681119414
iteration : 2692
train acc:  0.8046875
train loss:  0.4601963758468628
train gradient:  0.42621872896912333
iteration : 2693
train acc:  0.78125
train loss:  0.45144346356391907
train gradient:  0.5549398531440684
iteration : 2694
train acc:  0.8125
train loss:  0.38689500093460083
train gradient:  0.2932473167819015
iteration : 2695
train acc:  0.8203125
train loss:  0.36919349431991577
train gradient:  0.41174941053152814
iteration : 2696
train acc:  0.7890625
train loss:  0.46643170714378357
train gradient:  0.7080206347242722
iteration : 2697
train acc:  0.84375
train loss:  0.3441920578479767
train gradient:  0.25987742329098384
iteration : 2698
train acc:  0.828125
train loss:  0.3479534387588501
train gradient:  0.29947429264796743
iteration : 2699
train acc:  0.8359375
train loss:  0.370083212852478
train gradient:  0.39636866996497183
iteration : 2700
train acc:  0.8359375
train loss:  0.3451772928237915
train gradient:  0.42837538218222926
iteration : 2701
train acc:  0.78125
train loss:  0.45176783204078674
train gradient:  0.46671133669621234
iteration : 2702
train acc:  0.859375
train loss:  0.3348870575428009
train gradient:  0.23958859567620933
iteration : 2703
train acc:  0.765625
train loss:  0.4320257306098938
train gradient:  0.46759465108728554
iteration : 2704
train acc:  0.8203125
train loss:  0.41580861806869507
train gradient:  0.31527781104230496
iteration : 2705
train acc:  0.875
train loss:  0.3257255554199219
train gradient:  0.2544493508680443
iteration : 2706
train acc:  0.84375
train loss:  0.32498759031295776
train gradient:  0.24727351978986684
iteration : 2707
train acc:  0.8671875
train loss:  0.3805306851863861
train gradient:  0.2590164568818151
iteration : 2708
train acc:  0.765625
train loss:  0.45450639724731445
train gradient:  0.332565011534016
iteration : 2709
train acc:  0.7421875
train loss:  0.5545357465744019
train gradient:  0.9507921362657898
iteration : 2710
train acc:  0.8125
train loss:  0.4013870656490326
train gradient:  0.31092401927962904
iteration : 2711
train acc:  0.8828125
train loss:  0.3294358253479004
train gradient:  0.20825049391985428
iteration : 2712
train acc:  0.8046875
train loss:  0.4748474061489105
train gradient:  0.41822283316057823
iteration : 2713
train acc:  0.84375
train loss:  0.3493528366088867
train gradient:  0.23238410260395242
iteration : 2714
train acc:  0.84375
train loss:  0.3736734986305237
train gradient:  0.29622149972152784
iteration : 2715
train acc:  0.796875
train loss:  0.4084477424621582
train gradient:  0.4529733355196644
iteration : 2716
train acc:  0.859375
train loss:  0.30041617155075073
train gradient:  0.2919761594603876
iteration : 2717
train acc:  0.8828125
train loss:  0.30568814277648926
train gradient:  0.24157064756844865
iteration : 2718
train acc:  0.8046875
train loss:  0.4104830026626587
train gradient:  0.5072266259355739
iteration : 2719
train acc:  0.8515625
train loss:  0.37061023712158203
train gradient:  0.2901558827902711
iteration : 2720
train acc:  0.84375
train loss:  0.37725770473480225
train gradient:  0.3440844259121609
iteration : 2721
train acc:  0.75
train loss:  0.4892338216304779
train gradient:  0.545225371265896
iteration : 2722
train acc:  0.890625
train loss:  0.3381742238998413
train gradient:  0.2875440789902661
iteration : 2723
train acc:  0.8125
train loss:  0.40706223249435425
train gradient:  0.28956438834966625
iteration : 2724
train acc:  0.8359375
train loss:  0.3022569417953491
train gradient:  0.2256850444662386
iteration : 2725
train acc:  0.8359375
train loss:  0.3602423667907715
train gradient:  0.3010728560861438
iteration : 2726
train acc:  0.8125
train loss:  0.40519219636917114
train gradient:  0.2917282700778611
iteration : 2727
train acc:  0.8515625
train loss:  0.34147927165031433
train gradient:  0.29019468917506863
iteration : 2728
train acc:  0.796875
train loss:  0.4311385154724121
train gradient:  0.4287355881449282
iteration : 2729
train acc:  0.8203125
train loss:  0.3633687496185303
train gradient:  0.3029295696704282
iteration : 2730
train acc:  0.8203125
train loss:  0.394770085811615
train gradient:  0.3541534843217595
iteration : 2731
train acc:  0.8359375
train loss:  0.38491207361221313
train gradient:  0.3635946181996954
iteration : 2732
train acc:  0.796875
train loss:  0.4261709749698639
train gradient:  0.3737969376840243
iteration : 2733
train acc:  0.8671875
train loss:  0.31722530722618103
train gradient:  0.2863502599609908
iteration : 2734
train acc:  0.84375
train loss:  0.35135066509246826
train gradient:  0.39994476183934413
iteration : 2735
train acc:  0.8515625
train loss:  0.3165670931339264
train gradient:  0.23767230932407338
iteration : 2736
train acc:  0.8203125
train loss:  0.38968658447265625
train gradient:  0.3099916518240179
iteration : 2737
train acc:  0.8125
train loss:  0.4227261543273926
train gradient:  0.3568904569213781
iteration : 2738
train acc:  0.8828125
train loss:  0.3043661415576935
train gradient:  0.25804652903865105
iteration : 2739
train acc:  0.828125
train loss:  0.36740434169769287
train gradient:  0.3525396077805892
iteration : 2740
train acc:  0.7734375
train loss:  0.39439284801483154
train gradient:  0.33518626594825457
iteration : 2741
train acc:  0.8125
train loss:  0.4044049382209778
train gradient:  0.38432860552376413
iteration : 2742
train acc:  0.8046875
train loss:  0.4247841238975525
train gradient:  0.46297135311093673
iteration : 2743
train acc:  0.7734375
train loss:  0.4094010293483734
train gradient:  0.42725846884098373
iteration : 2744
train acc:  0.8203125
train loss:  0.39918649196624756
train gradient:  0.34887412117934097
iteration : 2745
train acc:  0.734375
train loss:  0.5105606913566589
train gradient:  0.5711448523489423
iteration : 2746
train acc:  0.859375
train loss:  0.3387984037399292
train gradient:  0.35905558646193514
iteration : 2747
train acc:  0.828125
train loss:  0.3963889181613922
train gradient:  0.341302068092851
iteration : 2748
train acc:  0.859375
train loss:  0.3806915283203125
train gradient:  0.30080909645507653
iteration : 2749
train acc:  0.765625
train loss:  0.4449167847633362
train gradient:  0.4532828402701854
iteration : 2750
train acc:  0.78125
train loss:  0.4444246292114258
train gradient:  0.43510208020918295
iteration : 2751
train acc:  0.796875
train loss:  0.40558889508247375
train gradient:  0.3933629500728772
iteration : 2752
train acc:  0.7578125
train loss:  0.5058104395866394
train gradient:  0.4658361052993881
iteration : 2753
train acc:  0.8359375
train loss:  0.4077712893486023
train gradient:  0.3828634926455189
iteration : 2754
train acc:  0.8359375
train loss:  0.3667747378349304
train gradient:  0.332958446141834
iteration : 2755
train acc:  0.796875
train loss:  0.44398653507232666
train gradient:  0.4930496780341536
iteration : 2756
train acc:  0.890625
train loss:  0.32222193479537964
train gradient:  0.24895008779400968
iteration : 2757
train acc:  0.8359375
train loss:  0.41448545455932617
train gradient:  0.5361814432988542
iteration : 2758
train acc:  0.8125
train loss:  0.37537258863449097
train gradient:  0.3739699295929017
iteration : 2759
train acc:  0.828125
train loss:  0.3768959939479828
train gradient:  0.3216469409307314
iteration : 2760
train acc:  0.828125
train loss:  0.3220996856689453
train gradient:  0.29556678912172973
iteration : 2761
train acc:  0.8359375
train loss:  0.3422302007675171
train gradient:  0.24690463516827166
iteration : 2762
train acc:  0.8671875
train loss:  0.31844043731689453
train gradient:  0.2819548906657728
iteration : 2763
train acc:  0.7890625
train loss:  0.48172837495803833
train gradient:  0.40369531954933296
iteration : 2764
train acc:  0.90625
train loss:  0.3022933602333069
train gradient:  0.2814844424705518
iteration : 2765
train acc:  0.765625
train loss:  0.49408629536628723
train gradient:  0.6208162944021083
iteration : 2766
train acc:  0.859375
train loss:  0.340618371963501
train gradient:  0.2183463195110059
iteration : 2767
train acc:  0.875
train loss:  0.3115484118461609
train gradient:  0.22775709670902344
iteration : 2768
train acc:  0.84375
train loss:  0.3795226514339447
train gradient:  0.351010628569865
iteration : 2769
train acc:  0.8203125
train loss:  0.4121423363685608
train gradient:  0.3624047642251653
iteration : 2770
train acc:  0.8828125
train loss:  0.30126380920410156
train gradient:  0.23807788032878727
iteration : 2771
train acc:  0.890625
train loss:  0.2824971675872803
train gradient:  0.28041680586384726
iteration : 2772
train acc:  0.8203125
train loss:  0.42960047721862793
train gradient:  0.3939035621890654
iteration : 2773
train acc:  0.84375
train loss:  0.316344290971756
train gradient:  0.2672914862573608
iteration : 2774
train acc:  0.8125
train loss:  0.4458373785018921
train gradient:  0.5065961064048388
iteration : 2775
train acc:  0.78125
train loss:  0.4297396242618561
train gradient:  0.3511608926413587
iteration : 2776
train acc:  0.8203125
train loss:  0.35542216897010803
train gradient:  0.3740649052629897
iteration : 2777
train acc:  0.828125
train loss:  0.3545735478401184
train gradient:  0.41230922102844
iteration : 2778
train acc:  0.7578125
train loss:  0.5525606870651245
train gradient:  0.639556651925941
iteration : 2779
train acc:  0.8515625
train loss:  0.35738492012023926
train gradient:  0.36878019422572916
iteration : 2780
train acc:  0.8125
train loss:  0.41722893714904785
train gradient:  0.4506848141631197
iteration : 2781
train acc:  0.75
train loss:  0.44946861267089844
train gradient:  0.5127937793329428
iteration : 2782
train acc:  0.8828125
train loss:  0.3309173285961151
train gradient:  0.23958101865720904
iteration : 2783
train acc:  0.8515625
train loss:  0.3487245738506317
train gradient:  0.24401984098578922
iteration : 2784
train acc:  0.875
train loss:  0.3532261252403259
train gradient:  0.2082075586086114
iteration : 2785
train acc:  0.765625
train loss:  0.4294722080230713
train gradient:  0.39927775178929453
iteration : 2786
train acc:  0.8671875
train loss:  0.3488081693649292
train gradient:  0.3393097093704085
iteration : 2787
train acc:  0.828125
train loss:  0.3657720983028412
train gradient:  0.3602200820704226
iteration : 2788
train acc:  0.921875
train loss:  0.2701076865196228
train gradient:  0.19942864882188632
iteration : 2789
train acc:  0.9140625
train loss:  0.2713133692741394
train gradient:  0.2036135372071353
iteration : 2790
train acc:  0.828125
train loss:  0.4244488477706909
train gradient:  0.3508645714683543
iteration : 2791
train acc:  0.84375
train loss:  0.39295250177383423
train gradient:  0.4308402463808834
iteration : 2792
train acc:  0.8125
train loss:  0.42893752455711365
train gradient:  0.4014667401656507
iteration : 2793
train acc:  0.8046875
train loss:  0.4075930118560791
train gradient:  0.3826632700026935
iteration : 2794
train acc:  0.7890625
train loss:  0.42478638887405396
train gradient:  0.46588152844322106
iteration : 2795
train acc:  0.8203125
train loss:  0.36785876750946045
train gradient:  0.33308507268501764
iteration : 2796
train acc:  0.8515625
train loss:  0.40736138820648193
train gradient:  0.5049726298639591
iteration : 2797
train acc:  0.8515625
train loss:  0.31329938769340515
train gradient:  0.26669091500673103
iteration : 2798
train acc:  0.875
train loss:  0.2719510793685913
train gradient:  0.2953464176940653
iteration : 2799
train acc:  0.828125
train loss:  0.3679485321044922
train gradient:  0.47254712246970243
iteration : 2800
train acc:  0.796875
train loss:  0.3758423328399658
train gradient:  0.2904157562808784
iteration : 2801
train acc:  0.8125
train loss:  0.4039434790611267
train gradient:  0.3859039840106593
iteration : 2802
train acc:  0.8046875
train loss:  0.4226863980293274
train gradient:  0.45631327489654905
iteration : 2803
train acc:  0.7890625
train loss:  0.43545612692832947
train gradient:  0.31650042620349994
iteration : 2804
train acc:  0.8828125
train loss:  0.2913191318511963
train gradient:  0.27344278044245385
iteration : 2805
train acc:  0.8515625
train loss:  0.3716127276420593
train gradient:  0.474802475121074
iteration : 2806
train acc:  0.8046875
train loss:  0.4546678960323334
train gradient:  0.4855594487589233
iteration : 2807
train acc:  0.875
train loss:  0.343345582485199
train gradient:  0.2481776876339466
iteration : 2808
train acc:  0.8359375
train loss:  0.3783908188343048
train gradient:  0.30148333636680197
iteration : 2809
train acc:  0.7890625
train loss:  0.4401695132255554
train gradient:  0.45743931498735213
iteration : 2810
train acc:  0.796875
train loss:  0.3953467011451721
train gradient:  0.4956096998171728
iteration : 2811
train acc:  0.8203125
train loss:  0.47979551553726196
train gradient:  0.5332626025331951
iteration : 2812
train acc:  0.8359375
train loss:  0.4278581738471985
train gradient:  0.3912366566879919
iteration : 2813
train acc:  0.8125
train loss:  0.40748798847198486
train gradient:  0.32168408361838063
iteration : 2814
train acc:  0.8359375
train loss:  0.3320651054382324
train gradient:  0.21785977820413865
iteration : 2815
train acc:  0.78125
train loss:  0.46497395634651184
train gradient:  0.356364951839616
iteration : 2816
train acc:  0.75
train loss:  0.4712601900100708
train gradient:  0.43059885871316117
iteration : 2817
train acc:  0.8046875
train loss:  0.44398579001426697
train gradient:  0.4865232963670819
iteration : 2818
train acc:  0.7890625
train loss:  0.4248986840248108
train gradient:  0.44774493636814394
iteration : 2819
train acc:  0.8984375
train loss:  0.33221375942230225
train gradient:  0.1909235071659673
iteration : 2820
train acc:  0.8515625
train loss:  0.4294300079345703
train gradient:  0.37944855686067347
iteration : 2821
train acc:  0.8046875
train loss:  0.4295496642589569
train gradient:  0.5393897110428435
iteration : 2822
train acc:  0.8828125
train loss:  0.29729732871055603
train gradient:  0.2229876151808125
iteration : 2823
train acc:  0.796875
train loss:  0.38186997175216675
train gradient:  0.3118437451348245
iteration : 2824
train acc:  0.8203125
train loss:  0.37820079922676086
train gradient:  0.3988972447342922
iteration : 2825
train acc:  0.8125
train loss:  0.43758755922317505
train gradient:  0.36901757830741627
iteration : 2826
train acc:  0.8125
train loss:  0.41395220160484314
train gradient:  0.5456704375578647
iteration : 2827
train acc:  0.8203125
train loss:  0.3875502943992615
train gradient:  0.43712766470882425
iteration : 2828
train acc:  0.8125
train loss:  0.40911054611206055
train gradient:  0.45828171922352606
iteration : 2829
train acc:  0.8671875
train loss:  0.4128492474555969
train gradient:  0.28034275602681
iteration : 2830
train acc:  0.828125
train loss:  0.47081977128982544
train gradient:  0.47981296537866036
iteration : 2831
train acc:  0.859375
train loss:  0.31548386812210083
train gradient:  0.2706264003066911
iteration : 2832
train acc:  0.796875
train loss:  0.4568910002708435
train gradient:  0.47157325972918634
iteration : 2833
train acc:  0.890625
train loss:  0.28628993034362793
train gradient:  0.21309673344963442
iteration : 2834
train acc:  0.8125
train loss:  0.37242257595062256
train gradient:  0.32666305965062464
iteration : 2835
train acc:  0.796875
train loss:  0.41312092542648315
train gradient:  0.32022923499243106
iteration : 2836
train acc:  0.859375
train loss:  0.3754613399505615
train gradient:  0.3405211315796193
iteration : 2837
train acc:  0.8203125
train loss:  0.3609407842159271
train gradient:  0.3845265670337936
iteration : 2838
train acc:  0.8203125
train loss:  0.3700941503047943
train gradient:  0.3470901409232848
iteration : 2839
train acc:  0.8125
train loss:  0.36871081590652466
train gradient:  0.3040025941730648
iteration : 2840
train acc:  0.8515625
train loss:  0.35391417145729065
train gradient:  0.26931202921049446
iteration : 2841
train acc:  0.828125
train loss:  0.3988040089607239
train gradient:  0.30371392345142045
iteration : 2842
train acc:  0.8359375
train loss:  0.376521497964859
train gradient:  0.36994647634495537
iteration : 2843
train acc:  0.78125
train loss:  0.42802438139915466
train gradient:  0.3240897924266395
iteration : 2844
train acc:  0.84375
train loss:  0.3366318941116333
train gradient:  0.3199080526216071
iteration : 2845
train acc:  0.796875
train loss:  0.37878650426864624
train gradient:  0.33561990910917394
iteration : 2846
train acc:  0.7890625
train loss:  0.4534350633621216
train gradient:  0.36542735374441704
iteration : 2847
train acc:  0.8359375
train loss:  0.3767673671245575
train gradient:  0.2794775842092811
iteration : 2848
train acc:  0.765625
train loss:  0.44744592905044556
train gradient:  0.34696657029836586
iteration : 2849
train acc:  0.796875
train loss:  0.38719642162323
train gradient:  0.38922030068652075
iteration : 2850
train acc:  0.8359375
train loss:  0.3255021870136261
train gradient:  0.24600981961202173
iteration : 2851
train acc:  0.875
train loss:  0.2868152856826782
train gradient:  0.19176847872264596
iteration : 2852
train acc:  0.84375
train loss:  0.36527085304260254
train gradient:  0.2938031802594421
iteration : 2853
train acc:  0.8046875
train loss:  0.3846557140350342
train gradient:  0.34236211968344377
iteration : 2854
train acc:  0.796875
train loss:  0.3988322913646698
train gradient:  0.31613764534208993
iteration : 2855
train acc:  0.796875
train loss:  0.39097315073013306
train gradient:  0.3164117857716904
iteration : 2856
train acc:  0.8359375
train loss:  0.36000579595565796
train gradient:  0.3503897841916935
iteration : 2857
train acc:  0.828125
train loss:  0.33502432703971863
train gradient:  0.2634648808428218
iteration : 2858
train acc:  0.7578125
train loss:  0.48600831627845764
train gradient:  0.43749657576255496
iteration : 2859
train acc:  0.8203125
train loss:  0.38044804334640503
train gradient:  0.2549976045153429
iteration : 2860
train acc:  0.8046875
train loss:  0.4169650077819824
train gradient:  0.4390137884109964
iteration : 2861
train acc:  0.859375
train loss:  0.35231274366378784
train gradient:  0.26602143861348926
iteration : 2862
train acc:  0.796875
train loss:  0.40849006175994873
train gradient:  0.3019607270267479
iteration : 2863
train acc:  0.7734375
train loss:  0.4170195460319519
train gradient:  0.3754326659572185
iteration : 2864
train acc:  0.78125
train loss:  0.41879457235336304
train gradient:  0.46963420026976077
iteration : 2865
train acc:  0.8125
train loss:  0.4198257327079773
train gradient:  0.4340240718197581
iteration : 2866
train acc:  0.859375
train loss:  0.3497992157936096
train gradient:  0.21745873042174435
iteration : 2867
train acc:  0.8125
train loss:  0.3863380551338196
train gradient:  0.3331571291105946
iteration : 2868
train acc:  0.875
train loss:  0.38570117950439453
train gradient:  0.2560547450619167
iteration : 2869
train acc:  0.796875
train loss:  0.41742807626724243
train gradient:  0.32059694206536105
iteration : 2870
train acc:  0.7734375
train loss:  0.4379706382751465
train gradient:  0.4090270108900265
iteration : 2871
train acc:  0.828125
train loss:  0.387988805770874
train gradient:  0.38759945941455215
iteration : 2872
train acc:  0.7890625
train loss:  0.4586161971092224
train gradient:  0.5430910682943615
iteration : 2873
train acc:  0.8671875
train loss:  0.31351304054260254
train gradient:  0.3150420907057923
iteration : 2874
train acc:  0.765625
train loss:  0.4827832579612732
train gradient:  0.46491589652465715
iteration : 2875
train acc:  0.8203125
train loss:  0.34559938311576843
train gradient:  0.3231524107525261
iteration : 2876
train acc:  0.8671875
train loss:  0.375837504863739
train gradient:  0.3263382654684591
iteration : 2877
train acc:  0.796875
train loss:  0.42440807819366455
train gradient:  0.45938067602247923
iteration : 2878
train acc:  0.8359375
train loss:  0.3692563772201538
train gradient:  0.2988994700174033
iteration : 2879
train acc:  0.7890625
train loss:  0.4124346673488617
train gradient:  0.3631268106523537
iteration : 2880
train acc:  0.8984375
train loss:  0.2820461392402649
train gradient:  0.23603529429775638
iteration : 2881
train acc:  0.8359375
train loss:  0.4032436013221741
train gradient:  0.29245513141812773
iteration : 2882
train acc:  0.84375
train loss:  0.40072542428970337
train gradient:  0.2792645946298816
iteration : 2883
train acc:  0.859375
train loss:  0.33377307653427124
train gradient:  0.22873558996888566
iteration : 2884
train acc:  0.7734375
train loss:  0.4188615083694458
train gradient:  0.30180587819804894
iteration : 2885
train acc:  0.8203125
train loss:  0.37050193548202515
train gradient:  0.20804753765972542
iteration : 2886
train acc:  0.8203125
train loss:  0.36316925287246704
train gradient:  0.25098205220883874
iteration : 2887
train acc:  0.859375
train loss:  0.32072293758392334
train gradient:  0.19223414849449308
iteration : 2888
train acc:  0.8203125
train loss:  0.36505717039108276
train gradient:  0.3880317065463961
iteration : 2889
train acc:  0.8203125
train loss:  0.43558770418167114
train gradient:  0.4118231655140011
iteration : 2890
train acc:  0.8125
train loss:  0.4030020833015442
train gradient:  0.3314734594567061
iteration : 2891
train acc:  0.828125
train loss:  0.3736315965652466
train gradient:  0.25218948538270847
iteration : 2892
train acc:  0.890625
train loss:  0.31925320625305176
train gradient:  0.3071994265097012
iteration : 2893
train acc:  0.8203125
train loss:  0.4045608639717102
train gradient:  0.41988624397128266
iteration : 2894
train acc:  0.84375
train loss:  0.36405766010284424
train gradient:  0.2843878812163715
iteration : 2895
train acc:  0.859375
train loss:  0.34467142820358276
train gradient:  0.32979945161112206
iteration : 2896
train acc:  0.8203125
train loss:  0.37855446338653564
train gradient:  0.2957197366764423
iteration : 2897
train acc:  0.8671875
train loss:  0.3101575970649719
train gradient:  0.24972046524072491
iteration : 2898
train acc:  0.859375
train loss:  0.33917272090911865
train gradient:  0.3240870409757224
iteration : 2899
train acc:  0.8125
train loss:  0.40005847811698914
train gradient:  0.31699849592081897
iteration : 2900
train acc:  0.7890625
train loss:  0.4317479133605957
train gradient:  0.3137735156637746
iteration : 2901
train acc:  0.8125
train loss:  0.390026718378067
train gradient:  0.31529821951479253
iteration : 2902
train acc:  0.8515625
train loss:  0.31741905212402344
train gradient:  0.23207293814185262
iteration : 2903
train acc:  0.796875
train loss:  0.3996630609035492
train gradient:  0.39365866263630056
iteration : 2904
train acc:  0.84375
train loss:  0.35525596141815186
train gradient:  0.2885227936398116
iteration : 2905
train acc:  0.828125
train loss:  0.3456540107727051
train gradient:  0.261871294097062
iteration : 2906
train acc:  0.828125
train loss:  0.3649466633796692
train gradient:  0.27078773769726455
iteration : 2907
train acc:  0.84375
train loss:  0.31278929114341736
train gradient:  0.24733992619338085
iteration : 2908
train acc:  0.8671875
train loss:  0.3680592179298401
train gradient:  0.3051142860012489
iteration : 2909
train acc:  0.890625
train loss:  0.29352059960365295
train gradient:  0.2662759820350593
iteration : 2910
train acc:  0.8984375
train loss:  0.31194841861724854
train gradient:  0.2952710023541679
iteration : 2911
train acc:  0.8203125
train loss:  0.37805402278900146
train gradient:  0.3307561800191279
iteration : 2912
train acc:  0.828125
train loss:  0.36910200119018555
train gradient:  0.35262100102911553
iteration : 2913
train acc:  0.8671875
train loss:  0.3895203471183777
train gradient:  0.5063762766757438
iteration : 2914
train acc:  0.828125
train loss:  0.36319828033447266
train gradient:  0.25322823691058605
iteration : 2915
train acc:  0.890625
train loss:  0.3492165803909302
train gradient:  0.24411246885954346
iteration : 2916
train acc:  0.796875
train loss:  0.4174189567565918
train gradient:  0.4240172483149192
iteration : 2917
train acc:  0.859375
train loss:  0.34978121519088745
train gradient:  0.33543149741688183
iteration : 2918
train acc:  0.84375
train loss:  0.3814861476421356
train gradient:  0.2246044202522154
iteration : 2919
train acc:  0.796875
train loss:  0.48121142387390137
train gradient:  0.5256412608804273
iteration : 2920
train acc:  0.8125
train loss:  0.4166682958602905
train gradient:  0.4335504899415984
iteration : 2921
train acc:  0.8125
train loss:  0.42554861307144165
train gradient:  0.35186894978176336
iteration : 2922
train acc:  0.84375
train loss:  0.34352362155914307
train gradient:  0.32376077289291366
iteration : 2923
train acc:  0.8046875
train loss:  0.41209855675697327
train gradient:  0.29745683908557313
iteration : 2924
train acc:  0.8359375
train loss:  0.3664103150367737
train gradient:  0.3100752760361251
iteration : 2925
train acc:  0.8125
train loss:  0.36794233322143555
train gradient:  0.2825566903309307
iteration : 2926
train acc:  0.8671875
train loss:  0.3607042133808136
train gradient:  0.37853044318986406
iteration : 2927
train acc:  0.8203125
train loss:  0.34178948402404785
train gradient:  0.402299208207128
iteration : 2928
train acc:  0.7890625
train loss:  0.42605167627334595
train gradient:  0.41167075938654635
iteration : 2929
train acc:  0.8671875
train loss:  0.32226672768592834
train gradient:  0.313196112926647
iteration : 2930
train acc:  0.8359375
train loss:  0.37666577100753784
train gradient:  0.3144161092368449
iteration : 2931
train acc:  0.8046875
train loss:  0.40016037225723267
train gradient:  0.37912407531355463
iteration : 2932
train acc:  0.7734375
train loss:  0.44659391045570374
train gradient:  0.49594215695192917
iteration : 2933
train acc:  0.8046875
train loss:  0.4023463726043701
train gradient:  0.35124311526402363
iteration : 2934
train acc:  0.8046875
train loss:  0.3929954171180725
train gradient:  0.3176180972537841
iteration : 2935
train acc:  0.8671875
train loss:  0.35126522183418274
train gradient:  0.3937405767477131
iteration : 2936
train acc:  0.7734375
train loss:  0.44840800762176514
train gradient:  0.5276225875705736
iteration : 2937
train acc:  0.8515625
train loss:  0.4013117551803589
train gradient:  0.28746459560102977
iteration : 2938
train acc:  0.8203125
train loss:  0.3971980810165405
train gradient:  0.38496416943740414
iteration : 2939
train acc:  0.8671875
train loss:  0.33304938673973083
train gradient:  0.27721687792349226
iteration : 2940
train acc:  0.8046875
train loss:  0.40625885128974915
train gradient:  0.32042042149606953
iteration : 2941
train acc:  0.75
train loss:  0.48630815744400024
train gradient:  0.4642169967591679
iteration : 2942
train acc:  0.78125
train loss:  0.4404400587081909
train gradient:  0.4607903865198053
iteration : 2943
train acc:  0.8359375
train loss:  0.41452640295028687
train gradient:  0.40643743758638134
iteration : 2944
train acc:  0.828125
train loss:  0.39134958386421204
train gradient:  0.36775952486264346
iteration : 2945
train acc:  0.8046875
train loss:  0.41791778802871704
train gradient:  0.42656055503584583
iteration : 2946
train acc:  0.8828125
train loss:  0.32425349950790405
train gradient:  0.23692935955766603
iteration : 2947
train acc:  0.8984375
train loss:  0.29623642563819885
train gradient:  0.2992290689723829
iteration : 2948
train acc:  0.875
train loss:  0.361691951751709
train gradient:  0.21015717386668126
iteration : 2949
train acc:  0.8671875
train loss:  0.3346536159515381
train gradient:  0.3856123138381272
iteration : 2950
train acc:  0.8515625
train loss:  0.3912394344806671
train gradient:  0.24411449661620677
iteration : 2951
train acc:  0.8203125
train loss:  0.413058876991272
train gradient:  0.4039029976616008
iteration : 2952
train acc:  0.890625
train loss:  0.2982425093650818
train gradient:  0.21056003390250455
iteration : 2953
train acc:  0.84375
train loss:  0.38079017400741577
train gradient:  0.2825997624888786
iteration : 2954
train acc:  0.8515625
train loss:  0.3476700782775879
train gradient:  0.31297210341605647
iteration : 2955
train acc:  0.859375
train loss:  0.32065334916114807
train gradient:  0.22236941333976828
iteration : 2956
train acc:  0.84375
train loss:  0.39159834384918213
train gradient:  0.41734825688283655
iteration : 2957
train acc:  0.8515625
train loss:  0.3362075984477997
train gradient:  0.2618465474854164
iteration : 2958
train acc:  0.8046875
train loss:  0.47299614548683167
train gradient:  0.4931525773744918
iteration : 2959
train acc:  0.84375
train loss:  0.3583908677101135
train gradient:  0.45834926628948164
iteration : 2960
train acc:  0.8046875
train loss:  0.41197797656059265
train gradient:  0.3575344636192977
iteration : 2961
train acc:  0.828125
train loss:  0.38182514905929565
train gradient:  0.3286031750466233
iteration : 2962
train acc:  0.8359375
train loss:  0.37077587842941284
train gradient:  0.26189700977956465
iteration : 2963
train acc:  0.84375
train loss:  0.353488028049469
train gradient:  0.2695987741250517
iteration : 2964
train acc:  0.859375
train loss:  0.3277864456176758
train gradient:  0.2391255991549142
iteration : 2965
train acc:  0.8359375
train loss:  0.36242997646331787
train gradient:  0.31058804758665604
iteration : 2966
train acc:  0.828125
train loss:  0.3392162322998047
train gradient:  0.3455056102533656
iteration : 2967
train acc:  0.8203125
train loss:  0.40907493233680725
train gradient:  0.5784848206524775
iteration : 2968
train acc:  0.84375
train loss:  0.4159770607948303
train gradient:  0.39052486518772606
iteration : 2969
train acc:  0.90625
train loss:  0.35846537351608276
train gradient:  0.28371473398979374
iteration : 2970
train acc:  0.828125
train loss:  0.3408668041229248
train gradient:  0.3002623359043111
iteration : 2971
train acc:  0.84375
train loss:  0.3187099099159241
train gradient:  0.25865568351165963
iteration : 2972
train acc:  0.796875
train loss:  0.42960184812545776
train gradient:  0.42487167713846513
iteration : 2973
train acc:  0.8984375
train loss:  0.31496715545654297
train gradient:  0.19092554650235777
iteration : 2974
train acc:  0.78125
train loss:  0.4241190552711487
train gradient:  0.39018019472996546
iteration : 2975
train acc:  0.875
train loss:  0.30420008301734924
train gradient:  0.3327490398386799
iteration : 2976
train acc:  0.8671875
train loss:  0.35206902027130127
train gradient:  0.24967222565385203
iteration : 2977
train acc:  0.828125
train loss:  0.3564707040786743
train gradient:  0.47358493119999784
iteration : 2978
train acc:  0.8125
train loss:  0.373883992433548
train gradient:  0.24291533622074718
iteration : 2979
train acc:  0.8671875
train loss:  0.3536478877067566
train gradient:  0.24394361634735887
iteration : 2980
train acc:  0.828125
train loss:  0.41114962100982666
train gradient:  0.40275454320170373
iteration : 2981
train acc:  0.84375
train loss:  0.3584979176521301
train gradient:  0.41557807388583584
iteration : 2982
train acc:  0.8359375
train loss:  0.3859504461288452
train gradient:  0.3222989201322773
iteration : 2983
train acc:  0.84375
train loss:  0.3795987367630005
train gradient:  0.3044286533426619
iteration : 2984
train acc:  0.7890625
train loss:  0.3779006004333496
train gradient:  0.37323099473482424
iteration : 2985
train acc:  0.8828125
train loss:  0.313353031873703
train gradient:  0.2783542835107354
iteration : 2986
train acc:  0.8828125
train loss:  0.3666771650314331
train gradient:  0.2783387479639517
iteration : 2987
train acc:  0.859375
train loss:  0.3459186255931854
train gradient:  0.3718676711340873
iteration : 2988
train acc:  0.828125
train loss:  0.3887677788734436
train gradient:  0.2732105122481432
iteration : 2989
train acc:  0.765625
train loss:  0.4944313168525696
train gradient:  0.5774197474773192
iteration : 2990
train acc:  0.8203125
train loss:  0.4357151389122009
train gradient:  0.661147335638106
iteration : 2991
train acc:  0.84375
train loss:  0.37603503465652466
train gradient:  0.3312713735972069
iteration : 2992
train acc:  0.890625
train loss:  0.35418039560317993
train gradient:  0.3278178280932718
iteration : 2993
train acc:  0.84375
train loss:  0.33337894082069397
train gradient:  0.23484862975922205
iteration : 2994
train acc:  0.859375
train loss:  0.3102092146873474
train gradient:  0.3715483031605822
iteration : 2995
train acc:  0.8359375
train loss:  0.384628027677536
train gradient:  0.524201784161813
iteration : 2996
train acc:  0.84375
train loss:  0.4002363681793213
train gradient:  0.45083331199581406
iteration : 2997
train acc:  0.8203125
train loss:  0.42618852853775024
train gradient:  0.3721288307021684
iteration : 2998
train acc:  0.8359375
train loss:  0.45501410961151123
train gradient:  0.6585894656833883
iteration : 2999
train acc:  0.828125
train loss:  0.37584272027015686
train gradient:  0.25841390466558783
iteration : 3000
train acc:  0.84375
train loss:  0.3274022042751312
train gradient:  0.30854123999329464
iteration : 3001
train acc:  0.8671875
train loss:  0.335856556892395
train gradient:  0.3461002505576667
iteration : 3002
train acc:  0.7734375
train loss:  0.4443472623825073
train gradient:  0.483982145837505
iteration : 3003
train acc:  0.8046875
train loss:  0.3782368004322052
train gradient:  0.46826825432252933
iteration : 3004
train acc:  0.8359375
train loss:  0.35715216398239136
train gradient:  0.294139312514695
iteration : 3005
train acc:  0.7890625
train loss:  0.5427274107933044
train gradient:  0.5922172014610776
iteration : 3006
train acc:  0.890625
train loss:  0.31662213802337646
train gradient:  0.27947198805849877
iteration : 3007
train acc:  0.8515625
train loss:  0.34430837631225586
train gradient:  0.2224008392640775
iteration : 3008
train acc:  0.796875
train loss:  0.45972388982772827
train gradient:  0.4639999711321124
iteration : 3009
train acc:  0.84375
train loss:  0.44879448413848877
train gradient:  0.38124227488489876
iteration : 3010
train acc:  0.8046875
train loss:  0.45208099484443665
train gradient:  0.42229205334844677
iteration : 3011
train acc:  0.8046875
train loss:  0.3850562572479248
train gradient:  0.3618031393853725
iteration : 3012
train acc:  0.8828125
train loss:  0.3103351294994354
train gradient:  0.25410919632534146
iteration : 3013
train acc:  0.890625
train loss:  0.30914902687072754
train gradient:  0.2541823808388754
iteration : 3014
train acc:  0.796875
train loss:  0.41760051250457764
train gradient:  0.4707958340646188
iteration : 3015
train acc:  0.8203125
train loss:  0.43794918060302734
train gradient:  0.4396585505833462
iteration : 3016
train acc:  0.8203125
train loss:  0.37843024730682373
train gradient:  0.39635946827964574
iteration : 3017
train acc:  0.8203125
train loss:  0.41570889949798584
train gradient:  0.3727030997999863
iteration : 3018
train acc:  0.8984375
train loss:  0.28455668687820435
train gradient:  0.24679748932862344
iteration : 3019
train acc:  0.8828125
train loss:  0.31696611642837524
train gradient:  0.16552112537584016
iteration : 3020
train acc:  0.765625
train loss:  0.41055092215538025
train gradient:  0.32050631319251355
iteration : 3021
train acc:  0.796875
train loss:  0.41268080472946167
train gradient:  0.4228696587169231
iteration : 3022
train acc:  0.8359375
train loss:  0.34990933537483215
train gradient:  0.2874214033867335
iteration : 3023
train acc:  0.890625
train loss:  0.3799837529659271
train gradient:  0.3268541759998696
iteration : 3024
train acc:  0.8203125
train loss:  0.3803139626979828
train gradient:  0.33925153402436564
iteration : 3025
train acc:  0.8046875
train loss:  0.40794122219085693
train gradient:  0.36921800260655213
iteration : 3026
train acc:  0.859375
train loss:  0.3761281669139862
train gradient:  0.3318375858921952
iteration : 3027
train acc:  0.828125
train loss:  0.40207362174987793
train gradient:  0.21878996627648004
iteration : 3028
train acc:  0.796875
train loss:  0.4214293360710144
train gradient:  0.33507076006261494
iteration : 3029
train acc:  0.8671875
train loss:  0.29441359639167786
train gradient:  0.19137955132161819
iteration : 3030
train acc:  0.8203125
train loss:  0.4017057418823242
train gradient:  0.3077082724475414
iteration : 3031
train acc:  0.8203125
train loss:  0.3916475772857666
train gradient:  0.388364716795645
iteration : 3032
train acc:  0.8203125
train loss:  0.4393949806690216
train gradient:  0.5473353736471217
iteration : 3033
train acc:  0.84375
train loss:  0.36525195837020874
train gradient:  0.34913509923229075
iteration : 3034
train acc:  0.828125
train loss:  0.36759525537490845
train gradient:  0.4010206642322841
iteration : 3035
train acc:  0.796875
train loss:  0.4545890688896179
train gradient:  0.33705435957860075
iteration : 3036
train acc:  0.859375
train loss:  0.37580406665802
train gradient:  0.30047833053097583
iteration : 3037
train acc:  0.84375
train loss:  0.33926865458488464
train gradient:  0.19813284054806704
iteration : 3038
train acc:  0.8671875
train loss:  0.3152189254760742
train gradient:  0.2054248587999723
iteration : 3039
train acc:  0.90625
train loss:  0.2592484951019287
train gradient:  0.1830881784955296
iteration : 3040
train acc:  0.890625
train loss:  0.28525441884994507
train gradient:  0.24418208071912284
iteration : 3041
train acc:  0.8125
train loss:  0.45703426003456116
train gradient:  0.4877043099928793
iteration : 3042
train acc:  0.8046875
train loss:  0.4523881673812866
train gradient:  0.4359667211672566
iteration : 3043
train acc:  0.8125
train loss:  0.4465722441673279
train gradient:  0.42096868781882896
iteration : 3044
train acc:  0.796875
train loss:  0.4026528000831604
train gradient:  0.4120351004979888
iteration : 3045
train acc:  0.875
train loss:  0.3678622245788574
train gradient:  0.3704282940704919
iteration : 3046
train acc:  0.84375
train loss:  0.37305766344070435
train gradient:  0.27189105519815926
iteration : 3047
train acc:  0.8203125
train loss:  0.34364044666290283
train gradient:  0.3109507412354713
iteration : 3048
train acc:  0.8671875
train loss:  0.34141066670417786
train gradient:  0.3057231242802295
iteration : 3049
train acc:  0.875
train loss:  0.3138445019721985
train gradient:  0.2407286625270279
iteration : 3050
train acc:  0.875
train loss:  0.3180447518825531
train gradient:  0.21003063163024327
iteration : 3051
train acc:  0.84375
train loss:  0.3517439067363739
train gradient:  0.23527430823361872
iteration : 3052
train acc:  0.78125
train loss:  0.4996170103549957
train gradient:  0.5704052336989481
iteration : 3053
train acc:  0.8046875
train loss:  0.4239088296890259
train gradient:  0.4076681289882215
iteration : 3054
train acc:  0.828125
train loss:  0.3634147047996521
train gradient:  0.2872611138768324
iteration : 3055
train acc:  0.8984375
train loss:  0.2806650996208191
train gradient:  0.28670245273450523
iteration : 3056
train acc:  0.8203125
train loss:  0.37653815746307373
train gradient:  0.4710527481164727
iteration : 3057
train acc:  0.890625
train loss:  0.32656538486480713
train gradient:  0.33218519505440874
iteration : 3058
train acc:  0.8125
train loss:  0.35706669092178345
train gradient:  0.4113953480777354
iteration : 3059
train acc:  0.8671875
train loss:  0.34451037645339966
train gradient:  0.3620473009537418
iteration : 3060
train acc:  0.8359375
train loss:  0.39970141649246216
train gradient:  0.3801152130040431
iteration : 3061
train acc:  0.84375
train loss:  0.3998965919017792
train gradient:  0.3434673062999002
iteration : 3062
train acc:  0.8125
train loss:  0.44229015707969666
train gradient:  0.3839304507112723
iteration : 3063
train acc:  0.8984375
train loss:  0.2972196936607361
train gradient:  0.2451876127676169
iteration : 3064
train acc:  0.859375
train loss:  0.3347564935684204
train gradient:  0.32952176301250297
iteration : 3065
train acc:  0.8359375
train loss:  0.3366498351097107
train gradient:  0.2944523884169768
iteration : 3066
train acc:  0.828125
train loss:  0.3785459101200104
train gradient:  0.29475836255601096
iteration : 3067
train acc:  0.84375
train loss:  0.3389841616153717
train gradient:  0.27507540131301167
iteration : 3068
train acc:  0.8125
train loss:  0.3635425269603729
train gradient:  0.34793132705882085
iteration : 3069
train acc:  0.859375
train loss:  0.33127695322036743
train gradient:  0.31976342250086
iteration : 3070
train acc:  0.8359375
train loss:  0.3762073814868927
train gradient:  0.3769608511940409
iteration : 3071
train acc:  0.828125
train loss:  0.34940105676651
train gradient:  0.27474258350231684
iteration : 3072
train acc:  0.8671875
train loss:  0.3346747159957886
train gradient:  0.25051014044498965
iteration : 3073
train acc:  0.8359375
train loss:  0.33386337757110596
train gradient:  0.30739281168423127
iteration : 3074
train acc:  0.78125
train loss:  0.43681418895721436
train gradient:  0.5033870146150916
iteration : 3075
train acc:  0.8203125
train loss:  0.4036743640899658
train gradient:  0.352381528940669
iteration : 3076
train acc:  0.859375
train loss:  0.35540223121643066
train gradient:  0.39457710531566953
iteration : 3077
train acc:  0.828125
train loss:  0.4040001928806305
train gradient:  0.3971892660673023
iteration : 3078
train acc:  0.828125
train loss:  0.3853015899658203
train gradient:  0.30318230689148395
iteration : 3079
train acc:  0.8671875
train loss:  0.3417559862136841
train gradient:  0.2074743584155328
iteration : 3080
train acc:  0.8671875
train loss:  0.3784085810184479
train gradient:  0.38346684169312173
iteration : 3081
train acc:  0.859375
train loss:  0.31737738847732544
train gradient:  0.15500130289232875
iteration : 3082
train acc:  0.796875
train loss:  0.39271461963653564
train gradient:  0.43441028091645545
iteration : 3083
train acc:  0.828125
train loss:  0.3860677480697632
train gradient:  0.3257213745195042
iteration : 3084
train acc:  0.8359375
train loss:  0.3038029074668884
train gradient:  0.3052456421863371
iteration : 3085
train acc:  0.8359375
train loss:  0.36405909061431885
train gradient:  0.3873158846103546
iteration : 3086
train acc:  0.859375
train loss:  0.3685259222984314
train gradient:  0.3261483107839935
iteration : 3087
train acc:  0.828125
train loss:  0.37241485714912415
train gradient:  0.3920887928652438
iteration : 3088
train acc:  0.8828125
train loss:  0.2887205481529236
train gradient:  0.2110957959704157
iteration : 3089
train acc:  0.8828125
train loss:  0.3257659375667572
train gradient:  0.3358990680641605
iteration : 3090
train acc:  0.828125
train loss:  0.36270004510879517
train gradient:  0.324182291744806
iteration : 3091
train acc:  0.828125
train loss:  0.397625207901001
train gradient:  0.35954555138284716
iteration : 3092
train acc:  0.8671875
train loss:  0.35851889848709106
train gradient:  0.31966260529330975
iteration : 3093
train acc:  0.8046875
train loss:  0.4560811221599579
train gradient:  0.4214336005959435
iteration : 3094
train acc:  0.8515625
train loss:  0.3434140682220459
train gradient:  0.32717474697466503
iteration : 3095
train acc:  0.8984375
train loss:  0.29209238290786743
train gradient:  0.22974509733036025
iteration : 3096
train acc:  0.8203125
train loss:  0.3719838261604309
train gradient:  0.41382917096058036
iteration : 3097
train acc:  0.78125
train loss:  0.43658971786499023
train gradient:  0.4030956673426911
iteration : 3098
train acc:  0.8203125
train loss:  0.36372557282447815
train gradient:  0.2806093948055199
iteration : 3099
train acc:  0.8359375
train loss:  0.417548269033432
train gradient:  0.4578583822479388
iteration : 3100
train acc:  0.796875
train loss:  0.4734552502632141
train gradient:  0.7899518745754612
iteration : 3101
train acc:  0.78125
train loss:  0.4023665189743042
train gradient:  0.35944100847172583
iteration : 3102
train acc:  0.8359375
train loss:  0.332263320684433
train gradient:  0.27541025735806846
iteration : 3103
train acc:  0.796875
train loss:  0.4573853611946106
train gradient:  0.3986222611373809
iteration : 3104
train acc:  0.8046875
train loss:  0.4046337604522705
train gradient:  0.3401108063863517
iteration : 3105
train acc:  0.8515625
train loss:  0.3733859956264496
train gradient:  0.3653620209746232
iteration : 3106
train acc:  0.8515625
train loss:  0.3371424078941345
train gradient:  0.3290864155875148
iteration : 3107
train acc:  0.796875
train loss:  0.3723680377006531
train gradient:  0.3387661136598366
iteration : 3108
train acc:  0.828125
train loss:  0.3251696825027466
train gradient:  0.29593278729876454
iteration : 3109
train acc:  0.8515625
train loss:  0.35201776027679443
train gradient:  0.30280099278738026
iteration : 3110
train acc:  0.7890625
train loss:  0.43471238017082214
train gradient:  0.5776272280028683
iteration : 3111
train acc:  0.8671875
train loss:  0.33924251794815063
train gradient:  0.36208968379239126
iteration : 3112
train acc:  0.8359375
train loss:  0.3513801693916321
train gradient:  0.2814356938231777
iteration : 3113
train acc:  0.828125
train loss:  0.3918180763721466
train gradient:  0.5069313896063384
iteration : 3114
train acc:  0.8671875
train loss:  0.35018306970596313
train gradient:  0.3404487365442464
iteration : 3115
train acc:  0.875
train loss:  0.30884450674057007
train gradient:  0.34091013976659446
iteration : 3116
train acc:  0.8203125
train loss:  0.4308459758758545
train gradient:  0.3608876626075518
iteration : 3117
train acc:  0.8046875
train loss:  0.4072435200214386
train gradient:  0.29654298168480425
iteration : 3118
train acc:  0.796875
train loss:  0.41049760580062866
train gradient:  0.4089952229931198
iteration : 3119
train acc:  0.8359375
train loss:  0.3569505214691162
train gradient:  0.33519703248728905
iteration : 3120
train acc:  0.8203125
train loss:  0.417630672454834
train gradient:  0.4488379585372543
iteration : 3121
train acc:  0.7890625
train loss:  0.41099464893341064
train gradient:  0.4062792551056218
iteration : 3122
train acc:  0.7890625
train loss:  0.44942420721054077
train gradient:  0.5329348746427863
iteration : 3123
train acc:  0.84375
train loss:  0.38207095861434937
train gradient:  0.3346643596326687
iteration : 3124
train acc:  0.875
train loss:  0.3099532127380371
train gradient:  0.21758203863936165
iteration : 3125
train acc:  0.8125
train loss:  0.38658246397972107
train gradient:  0.34508443836461955
iteration : 3126
train acc:  0.796875
train loss:  0.4278411269187927
train gradient:  0.5071011559797327
iteration : 3127
train acc:  0.8125
train loss:  0.4132600426673889
train gradient:  0.3333062864474036
iteration : 3128
train acc:  0.859375
train loss:  0.3239172697067261
train gradient:  0.31220680441605086
iteration : 3129
train acc:  0.90625
train loss:  0.2926703095436096
train gradient:  0.24512359673296247
iteration : 3130
train acc:  0.890625
train loss:  0.2855552136898041
train gradient:  0.22591167012774221
iteration : 3131
train acc:  0.8515625
train loss:  0.3698619604110718
train gradient:  0.28688750514039923
iteration : 3132
train acc:  0.7734375
train loss:  0.4511057138442993
train gradient:  0.3431968031449189
iteration : 3133
train acc:  0.8125
train loss:  0.3784535229206085
train gradient:  0.3272257156780759
iteration : 3134
train acc:  0.828125
train loss:  0.40822553634643555
train gradient:  0.3311844008989057
iteration : 3135
train acc:  0.828125
train loss:  0.34767550230026245
train gradient:  0.269025671996772
iteration : 3136
train acc:  0.90625
train loss:  0.2903767228126526
train gradient:  0.18365265989695317
iteration : 3137
train acc:  0.828125
train loss:  0.3763931691646576
train gradient:  0.35449811220353455
iteration : 3138
train acc:  0.859375
train loss:  0.3023623526096344
train gradient:  0.1990831675815769
iteration : 3139
train acc:  0.8515625
train loss:  0.3484642505645752
train gradient:  0.22947024943483882
iteration : 3140
train acc:  0.7890625
train loss:  0.4214720129966736
train gradient:  0.3837964438812301
iteration : 3141
train acc:  0.8046875
train loss:  0.3896014094352722
train gradient:  0.2817563109978189
iteration : 3142
train acc:  0.90625
train loss:  0.2778548300266266
train gradient:  0.2628252716332237
iteration : 3143
train acc:  0.828125
train loss:  0.42190268635749817
train gradient:  0.3843550578098303
iteration : 3144
train acc:  0.828125
train loss:  0.3533305823802948
train gradient:  0.27242491489798437
iteration : 3145
train acc:  0.8828125
train loss:  0.3653080463409424
train gradient:  0.2954439558720179
iteration : 3146
train acc:  0.859375
train loss:  0.36596494913101196
train gradient:  0.27891079105482786
iteration : 3147
train acc:  0.8671875
train loss:  0.3828422427177429
train gradient:  0.46468419151928964
iteration : 3148
train acc:  0.859375
train loss:  0.37694430351257324
train gradient:  0.3135347276873324
iteration : 3149
train acc:  0.859375
train loss:  0.30887913703918457
train gradient:  0.22801436245320864
iteration : 3150
train acc:  0.71875
train loss:  0.6697742342948914
train gradient:  1.017913772512915
iteration : 3151
train acc:  0.828125
train loss:  0.36716020107269287
train gradient:  0.268887674675373
iteration : 3152
train acc:  0.78125
train loss:  0.4158656895160675
train gradient:  0.5792544015495137
iteration : 3153
train acc:  0.859375
train loss:  0.3683772385120392
train gradient:  0.3234733263530515
iteration : 3154
train acc:  0.78125
train loss:  0.4936405420303345
train gradient:  0.44982477134013465
iteration : 3155
train acc:  0.859375
train loss:  0.33826664090156555
train gradient:  0.26353760249314884
iteration : 3156
train acc:  0.7890625
train loss:  0.46038445830345154
train gradient:  1.1722693977605312
iteration : 3157
train acc:  0.8828125
train loss:  0.3420974612236023
train gradient:  0.23122105841923382
iteration : 3158
train acc:  0.8671875
train loss:  0.3361586928367615
train gradient:  0.34660619173628254
iteration : 3159
train acc:  0.8671875
train loss:  0.3920823931694031
train gradient:  0.41516877528681073
iteration : 3160
train acc:  0.8125
train loss:  0.38178664445877075
train gradient:  0.2921291562483703
iteration : 3161
train acc:  0.8203125
train loss:  0.36044472455978394
train gradient:  0.2769582469567594
iteration : 3162
train acc:  0.8515625
train loss:  0.3740852475166321
train gradient:  0.29843674728485664
iteration : 3163
train acc:  0.828125
train loss:  0.3189517855644226
train gradient:  0.3214690967259962
iteration : 3164
train acc:  0.7890625
train loss:  0.392549604177475
train gradient:  0.2643752136927706
iteration : 3165
train acc:  0.8203125
train loss:  0.39539283514022827
train gradient:  0.4515696177303984
iteration : 3166
train acc:  0.875
train loss:  0.3125946521759033
train gradient:  0.2536124967583009
iteration : 3167
train acc:  0.7734375
train loss:  0.4426456093788147
train gradient:  0.33669092954467944
iteration : 3168
train acc:  0.8125
train loss:  0.4059297442436218
train gradient:  0.2717558638402621
iteration : 3169
train acc:  0.8515625
train loss:  0.3121596872806549
train gradient:  0.23056197158936131
iteration : 3170
train acc:  0.8125
train loss:  0.40871378779411316
train gradient:  0.2901490740065161
iteration : 3171
train acc:  0.859375
train loss:  0.3843199610710144
train gradient:  0.36719275091475256
iteration : 3172
train acc:  0.8359375
train loss:  0.33906567096710205
train gradient:  0.34963463285075047
iteration : 3173
train acc:  0.8515625
train loss:  0.4159574508666992
train gradient:  0.4089565024700212
iteration : 3174
train acc:  0.828125
train loss:  0.396295964717865
train gradient:  0.30553964539083667
iteration : 3175
train acc:  0.8046875
train loss:  0.4602004289627075
train gradient:  0.3800881977265114
iteration : 3176
train acc:  0.8671875
train loss:  0.33340737223625183
train gradient:  0.25430615874993157
iteration : 3177
train acc:  0.7890625
train loss:  0.4265459179878235
train gradient:  0.5562732225008417
iteration : 3178
train acc:  0.859375
train loss:  0.3145759105682373
train gradient:  0.21857436039496425
iteration : 3179
train acc:  0.8203125
train loss:  0.40398287773132324
train gradient:  0.38550039466119157
iteration : 3180
train acc:  0.7890625
train loss:  0.4351242482662201
train gradient:  0.570842668244938
iteration : 3181
train acc:  0.8359375
train loss:  0.35151323676109314
train gradient:  0.4021085276287091
iteration : 3182
train acc:  0.8828125
train loss:  0.35005730390548706
train gradient:  0.26814277768953204
iteration : 3183
train acc:  0.8125
train loss:  0.4176320433616638
train gradient:  0.41623660351746855
iteration : 3184
train acc:  0.8515625
train loss:  0.34541523456573486
train gradient:  0.2216463720181295
iteration : 3185
train acc:  0.84375
train loss:  0.37058568000793457
train gradient:  0.4091086107691441
iteration : 3186
train acc:  0.8828125
train loss:  0.3169638514518738
train gradient:  0.2909341005293573
iteration : 3187
train acc:  0.8359375
train loss:  0.3374204635620117
train gradient:  0.3501210022261304
iteration : 3188
train acc:  0.8515625
train loss:  0.37019604444503784
train gradient:  0.33102717681113886
iteration : 3189
train acc:  0.78125
train loss:  0.4563846290111542
train gradient:  0.46748211067368706
iteration : 3190
train acc:  0.7734375
train loss:  0.4221804141998291
train gradient:  0.37767192863288634
iteration : 3191
train acc:  0.8671875
train loss:  0.30988919734954834
train gradient:  0.2864244454594623
iteration : 3192
train acc:  0.8671875
train loss:  0.31702521443367004
train gradient:  0.2328224735025699
iteration : 3193
train acc:  0.8203125
train loss:  0.36584264039993286
train gradient:  0.2747791124323331
iteration : 3194
train acc:  0.8125
train loss:  0.41842782497406006
train gradient:  0.3518711381312531
iteration : 3195
train acc:  0.8359375
train loss:  0.33717024326324463
train gradient:  0.34158197519008027
iteration : 3196
train acc:  0.8671875
train loss:  0.3603054881095886
train gradient:  0.30271724951833257
iteration : 3197
train acc:  0.8515625
train loss:  0.3723057806491852
train gradient:  0.21239432960077753
iteration : 3198
train acc:  0.734375
train loss:  0.5152307152748108
train gradient:  0.45058530284324394
iteration : 3199
train acc:  0.828125
train loss:  0.39940446615219116
train gradient:  0.49822262484522034
iteration : 3200
train acc:  0.8046875
train loss:  0.38841307163238525
train gradient:  0.2859533547107059
iteration : 3201
train acc:  0.8203125
train loss:  0.46084892749786377
train gradient:  0.3726268374883198
iteration : 3202
train acc:  0.8203125
train loss:  0.39285850524902344
train gradient:  0.3243095249681347
iteration : 3203
train acc:  0.8125
train loss:  0.3835197389125824
train gradient:  0.31955959391488076
iteration : 3204
train acc:  0.84375
train loss:  0.33218252658843994
train gradient:  0.28759297032145403
iteration : 3205
train acc:  0.8203125
train loss:  0.36185407638549805
train gradient:  0.2658726974925079
iteration : 3206
train acc:  0.8125
train loss:  0.388024240732193
train gradient:  0.26926063596484284
iteration : 3207
train acc:  0.828125
train loss:  0.40385764837265015
train gradient:  0.3690003715126639
iteration : 3208
train acc:  0.8046875
train loss:  0.4160130023956299
train gradient:  0.37694726166702647
iteration : 3209
train acc:  0.8515625
train loss:  0.35327646136283875
train gradient:  0.2742250744537634
iteration : 3210
train acc:  0.828125
train loss:  0.34099674224853516
train gradient:  0.24411780476371714
iteration : 3211
train acc:  0.8125
train loss:  0.42619481682777405
train gradient:  0.43421101272268187
iteration : 3212
train acc:  0.796875
train loss:  0.41537463665008545
train gradient:  0.3307950319633209
iteration : 3213
train acc:  0.84375
train loss:  0.3848618268966675
train gradient:  0.26710305523549377
iteration : 3214
train acc:  0.8125
train loss:  0.4084399938583374
train gradient:  0.3477601628174056
iteration : 3215
train acc:  0.875
train loss:  0.3285468816757202
train gradient:  0.3136140913713844
iteration : 3216
train acc:  0.84375
train loss:  0.35384684801101685
train gradient:  0.24634038064723096
iteration : 3217
train acc:  0.8515625
train loss:  0.38469886779785156
train gradient:  0.2287569172140686
iteration : 3218
train acc:  0.875
train loss:  0.3501277267932892
train gradient:  0.330571112802296
iteration : 3219
train acc:  0.8046875
train loss:  0.39069586992263794
train gradient:  0.46415736331988106
iteration : 3220
train acc:  0.859375
train loss:  0.3379253149032593
train gradient:  0.2172343408760477
iteration : 3221
train acc:  0.84375
train loss:  0.3744713068008423
train gradient:  0.2593033367314129
iteration : 3222
train acc:  0.84375
train loss:  0.3747604489326477
train gradient:  0.17291120963347653
iteration : 3223
train acc:  0.8359375
train loss:  0.4033958315849304
train gradient:  0.23954603708965524
iteration : 3224
train acc:  0.8125
train loss:  0.37918975949287415
train gradient:  0.25316604815506394
iteration : 3225
train acc:  0.890625
train loss:  0.25816136598587036
train gradient:  0.11033741029374906
iteration : 3226
train acc:  0.84375
train loss:  0.38258975744247437
train gradient:  0.30406711572504286
iteration : 3227
train acc:  0.8125
train loss:  0.3651229441165924
train gradient:  0.22307095865100118
iteration : 3228
train acc:  0.890625
train loss:  0.3087614178657532
train gradient:  0.2625206219144978
iteration : 3229
train acc:  0.8984375
train loss:  0.2811736762523651
train gradient:  0.24691645188025682
iteration : 3230
train acc:  0.8203125
train loss:  0.38026052713394165
train gradient:  0.3943706131252754
iteration : 3231
train acc:  0.78125
train loss:  0.4491797387599945
train gradient:  0.39899197195974667
iteration : 3232
train acc:  0.828125
train loss:  0.4001825451850891
train gradient:  0.4430033736707049
iteration : 3233
train acc:  0.8203125
train loss:  0.46359747648239136
train gradient:  0.41213292728820305
iteration : 3234
train acc:  0.8515625
train loss:  0.3494669795036316
train gradient:  0.264812117687464
iteration : 3235
train acc:  0.8515625
train loss:  0.38018614053726196
train gradient:  0.4254177657049864
iteration : 3236
train acc:  0.796875
train loss:  0.41389745473861694
train gradient:  0.2968586454730593
iteration : 3237
train acc:  0.7578125
train loss:  0.421891450881958
train gradient:  0.3841069915910691
iteration : 3238
train acc:  0.7578125
train loss:  0.4193844795227051
train gradient:  0.3311864554792441
iteration : 3239
train acc:  0.7890625
train loss:  0.41354453563690186
train gradient:  0.3348931251514117
iteration : 3240
train acc:  0.78125
train loss:  0.44833412766456604
train gradient:  0.33669096708924295
iteration : 3241
train acc:  0.8125
train loss:  0.3788319230079651
train gradient:  0.3369320826088539
iteration : 3242
train acc:  0.8203125
train loss:  0.3566555380821228
train gradient:  0.3075536469725148
iteration : 3243
train acc:  0.859375
train loss:  0.2994317412376404
train gradient:  0.2632095425723909
iteration : 3244
train acc:  0.8203125
train loss:  0.3843975365161896
train gradient:  0.3652609144273306
iteration : 3245
train acc:  0.8046875
train loss:  0.39547014236450195
train gradient:  0.33144164814222643
iteration : 3246
train acc:  0.8828125
train loss:  0.3386349081993103
train gradient:  0.31281261022838003
iteration : 3247
train acc:  0.828125
train loss:  0.40861964225769043
train gradient:  0.4142370492330599
iteration : 3248
train acc:  0.859375
train loss:  0.34693193435668945
train gradient:  0.4078042625937217
iteration : 3249
train acc:  0.8125
train loss:  0.3700575828552246
train gradient:  0.28660348426557497
iteration : 3250
train acc:  0.859375
train loss:  0.3553680181503296
train gradient:  0.30539918644360664
iteration : 3251
train acc:  0.8203125
train loss:  0.3923065662384033
train gradient:  0.32326236563530586
iteration : 3252
train acc:  0.796875
train loss:  0.37426018714904785
train gradient:  0.2620892622934171
iteration : 3253
train acc:  0.8125
train loss:  0.38489797711372375
train gradient:  0.2982232094594176
iteration : 3254
train acc:  0.8046875
train loss:  0.46934086084365845
train gradient:  0.4690068154051379
iteration : 3255
train acc:  0.9296875
train loss:  0.25601479411125183
train gradient:  0.2161892655405639
iteration : 3256
train acc:  0.84375
train loss:  0.3650710880756378
train gradient:  0.269682199289986
iteration : 3257
train acc:  0.84375
train loss:  0.3172743618488312
train gradient:  0.25030878477248636
iteration : 3258
train acc:  0.8671875
train loss:  0.31389573216438293
train gradient:  0.24938110285931148
iteration : 3259
train acc:  0.890625
train loss:  0.3447813391685486
train gradient:  0.27380275507208873
iteration : 3260
train acc:  0.8671875
train loss:  0.30360648036003113
train gradient:  0.29110958222287936
iteration : 3261
train acc:  0.7734375
train loss:  0.45874565839767456
train gradient:  0.37589351198717985
iteration : 3262
train acc:  0.8203125
train loss:  0.4265650510787964
train gradient:  0.36739080578000205
iteration : 3263
train acc:  0.8203125
train loss:  0.39520031213760376
train gradient:  0.3750945793439177
iteration : 3264
train acc:  0.859375
train loss:  0.3648870289325714
train gradient:  0.30340792618986323
iteration : 3265
train acc:  0.84375
train loss:  0.4210243821144104
train gradient:  0.34832163955540435
iteration : 3266
train acc:  0.7890625
train loss:  0.3891731798648834
train gradient:  0.3538726674360172
iteration : 3267
train acc:  0.7734375
train loss:  0.43012648820877075
train gradient:  0.36805821840450303
iteration : 3268
train acc:  0.796875
train loss:  0.40877366065979004
train gradient:  0.4156033833314392
iteration : 3269
train acc:  0.8515625
train loss:  0.3901405930519104
train gradient:  0.3681776766343873
iteration : 3270
train acc:  0.8671875
train loss:  0.3193792998790741
train gradient:  0.24016628568441714
iteration : 3271
train acc:  0.875
train loss:  0.2893664538860321
train gradient:  0.19783159419025712
iteration : 3272
train acc:  0.8671875
train loss:  0.3486836850643158
train gradient:  0.21774139600622522
iteration : 3273
train acc:  0.8125
train loss:  0.4020158350467682
train gradient:  0.3822320894257175
iteration : 3274
train acc:  0.859375
train loss:  0.3074573278427124
train gradient:  0.24164167649243623
iteration : 3275
train acc:  0.8046875
train loss:  0.4298219680786133
train gradient:  0.4212049811535895
iteration : 3276
train acc:  0.921875
train loss:  0.273054838180542
train gradient:  0.2282431121652003
iteration : 3277
train acc:  0.8359375
train loss:  0.3504998981952667
train gradient:  0.2769721139196089
iteration : 3278
train acc:  0.8359375
train loss:  0.3146328926086426
train gradient:  0.3028863561170368
iteration : 3279
train acc:  0.875
train loss:  0.3285679221153259
train gradient:  0.29845933126035107
iteration : 3280
train acc:  0.7734375
train loss:  0.4147726893424988
train gradient:  0.41887774546679774
iteration : 3281
train acc:  0.796875
train loss:  0.4186903238296509
train gradient:  0.40130401995048837
iteration : 3282
train acc:  0.8203125
train loss:  0.36395028233528137
train gradient:  0.26207017926016785
iteration : 3283
train acc:  0.859375
train loss:  0.33730125427246094
train gradient:  0.2758067152227979
iteration : 3284
train acc:  0.8046875
train loss:  0.42356622219085693
train gradient:  0.4091245703155822
iteration : 3285
train acc:  0.8515625
train loss:  0.37295567989349365
train gradient:  0.4503128821573084
iteration : 3286
train acc:  0.7578125
train loss:  0.5148000121116638
train gradient:  0.517583004924161
iteration : 3287
train acc:  0.8359375
train loss:  0.40521058440208435
train gradient:  0.4834058064448846
iteration : 3288
train acc:  0.828125
train loss:  0.36709368228912354
train gradient:  0.4251960910683628
iteration : 3289
train acc:  0.828125
train loss:  0.38906052708625793
train gradient:  0.48713869652669384
iteration : 3290
train acc:  0.875
train loss:  0.3165194094181061
train gradient:  0.28593156877196
iteration : 3291
train acc:  0.8515625
train loss:  0.31070172786712646
train gradient:  0.3734236691949697
iteration : 3292
train acc:  0.8046875
train loss:  0.4307836890220642
train gradient:  0.48270193184223126
iteration : 3293
train acc:  0.7890625
train loss:  0.40067869424819946
train gradient:  0.5343939497889287
iteration : 3294
train acc:  0.828125
train loss:  0.36110907793045044
train gradient:  0.46119573190059654
iteration : 3295
train acc:  0.7734375
train loss:  0.4627339839935303
train gradient:  0.4991357100547543
iteration : 3296
train acc:  0.8203125
train loss:  0.46384331583976746
train gradient:  0.5173779976983544
iteration : 3297
train acc:  0.8359375
train loss:  0.34419649839401245
train gradient:  0.23033724226644325
iteration : 3298
train acc:  0.828125
train loss:  0.36795756220817566
train gradient:  0.2867587789317952
iteration : 3299
train acc:  0.828125
train loss:  0.31966298818588257
train gradient:  0.30970566497477986
iteration : 3300
train acc:  0.8203125
train loss:  0.4291442036628723
train gradient:  0.48825402867615847
iteration : 3301
train acc:  0.828125
train loss:  0.399334192276001
train gradient:  0.47010585736297616
iteration : 3302
train acc:  0.8125
train loss:  0.4341476261615753
train gradient:  0.3668120351494807
iteration : 3303
train acc:  0.8046875
train loss:  0.3628404140472412
train gradient:  0.2503296705018587
iteration : 3304
train acc:  0.828125
train loss:  0.36225706338882446
train gradient:  0.317384389728756
iteration : 3305
train acc:  0.8515625
train loss:  0.3559046983718872
train gradient:  0.2242780868597268
iteration : 3306
train acc:  0.8359375
train loss:  0.35040873289108276
train gradient:  0.32710642560870756
iteration : 3307
train acc:  0.84375
train loss:  0.3248966932296753
train gradient:  0.2568862280442096
iteration : 3308
train acc:  0.828125
train loss:  0.3798332214355469
train gradient:  0.35646582997112564
iteration : 3309
train acc:  0.875
train loss:  0.3194708228111267
train gradient:  0.21999884809709114
iteration : 3310
train acc:  0.84375
train loss:  0.3688447177410126
train gradient:  0.2125727349623203
iteration : 3311
train acc:  0.7890625
train loss:  0.47987979650497437
train gradient:  0.45449605956747885
iteration : 3312
train acc:  0.8828125
train loss:  0.32288193702697754
train gradient:  0.28395296376517826
iteration : 3313
train acc:  0.8203125
train loss:  0.34593719244003296
train gradient:  0.3982582537169448
iteration : 3314
train acc:  0.7578125
train loss:  0.5039035081863403
train gradient:  0.42636924248044983
iteration : 3315
train acc:  0.8828125
train loss:  0.2907201051712036
train gradient:  0.24688506011079628
iteration : 3316
train acc:  0.859375
train loss:  0.31771978735923767
train gradient:  0.27614518857633596
iteration : 3317
train acc:  0.8359375
train loss:  0.3704398274421692
train gradient:  0.2725878198466459
iteration : 3318
train acc:  0.828125
train loss:  0.43378961086273193
train gradient:  0.34545237228180237
iteration : 3319
train acc:  0.8046875
train loss:  0.4108852744102478
train gradient:  0.36929325758292364
iteration : 3320
train acc:  0.859375
train loss:  0.3832792639732361
train gradient:  0.4209993272065642
iteration : 3321
train acc:  0.8125
train loss:  0.402082622051239
train gradient:  0.29949843091273387
iteration : 3322
train acc:  0.8046875
train loss:  0.4230530560016632
train gradient:  0.40527314481129284
iteration : 3323
train acc:  0.84375
train loss:  0.3429921269416809
train gradient:  0.2909425100634642
iteration : 3324
train acc:  0.8125
train loss:  0.37813234329223633
train gradient:  0.24681121495585728
iteration : 3325
train acc:  0.796875
train loss:  0.4180595874786377
train gradient:  0.45616330639300684
iteration : 3326
train acc:  0.8671875
train loss:  0.32621222734451294
train gradient:  0.23156727082023326
iteration : 3327
train acc:  0.8125
train loss:  0.3893236815929413
train gradient:  0.3896348513509664
iteration : 3328
train acc:  0.8359375
train loss:  0.3689989447593689
train gradient:  0.2568965815688974
iteration : 3329
train acc:  0.8203125
train loss:  0.3674744963645935
train gradient:  0.28956546991614307
iteration : 3330
train acc:  0.828125
train loss:  0.32269299030303955
train gradient:  0.337008234966938
iteration : 3331
train acc:  0.8515625
train loss:  0.3504180312156677
train gradient:  0.5120201120062808
iteration : 3332
train acc:  0.8203125
train loss:  0.3633708953857422
train gradient:  0.45835207899588176
iteration : 3333
train acc:  0.8359375
train loss:  0.3841848075389862
train gradient:  0.39906138326750845
iteration : 3334
train acc:  0.859375
train loss:  0.3393242061138153
train gradient:  0.24237180235462208
iteration : 3335
train acc:  0.8359375
train loss:  0.3777339458465576
train gradient:  0.33459995976541757
iteration : 3336
train acc:  0.875
train loss:  0.33070623874664307
train gradient:  0.25439790394143985
iteration : 3337
train acc:  0.8359375
train loss:  0.421183317899704
train gradient:  0.3578223666681894
iteration : 3338
train acc:  0.875
train loss:  0.30977267026901245
train gradient:  0.23663843412720303
iteration : 3339
train acc:  0.8515625
train loss:  0.39048701524734497
train gradient:  0.4767713890306091
iteration : 3340
train acc:  0.8828125
train loss:  0.32523366808891296
train gradient:  0.2748699539205949
iteration : 3341
train acc:  0.78125
train loss:  0.3862886428833008
train gradient:  0.329526550614642
iteration : 3342
train acc:  0.8359375
train loss:  0.405905157327652
train gradient:  0.25905021805320205
iteration : 3343
train acc:  0.8203125
train loss:  0.3669683635234833
train gradient:  0.35126543984065567
iteration : 3344
train acc:  0.8515625
train loss:  0.4108477830886841
train gradient:  0.3554613713676828
iteration : 3345
train acc:  0.8359375
train loss:  0.3424885869026184
train gradient:  0.25554227693558396
iteration : 3346
train acc:  0.8203125
train loss:  0.32138681411743164
train gradient:  0.23050274326407583
iteration : 3347
train acc:  0.8515625
train loss:  0.3411823511123657
train gradient:  0.26915630228939647
iteration : 3348
train acc:  0.828125
train loss:  0.32867124676704407
train gradient:  0.4118189737478287
iteration : 3349
train acc:  0.7734375
train loss:  0.45432114601135254
train gradient:  0.412012214894229
iteration : 3350
train acc:  0.8359375
train loss:  0.4341684877872467
train gradient:  0.5501318442759737
iteration : 3351
train acc:  0.8671875
train loss:  0.3442651033401489
train gradient:  0.2560274203949948
iteration : 3352
train acc:  0.828125
train loss:  0.3627415895462036
train gradient:  0.3074740528798283
iteration : 3353
train acc:  0.8203125
train loss:  0.3770776391029358
train gradient:  0.30122062045114073
iteration : 3354
train acc:  0.7734375
train loss:  0.4370872378349304
train gradient:  0.43258301428333257
iteration : 3355
train acc:  0.8359375
train loss:  0.3401239216327667
train gradient:  0.3433474481317996
iteration : 3356
train acc:  0.8046875
train loss:  0.3667154908180237
train gradient:  0.42233641562482105
iteration : 3357
train acc:  0.8203125
train loss:  0.3890199661254883
train gradient:  0.3178004756832627
iteration : 3358
train acc:  0.859375
train loss:  0.3240591883659363
train gradient:  0.27689430968736173
iteration : 3359
train acc:  0.765625
train loss:  0.40609726309776306
train gradient:  0.34733045941772944
iteration : 3360
train acc:  0.8671875
train loss:  0.34117451310157776
train gradient:  0.4714415146158585
iteration : 3361
train acc:  0.8359375
train loss:  0.36573290824890137
train gradient:  0.40321697690588804
iteration : 3362
train acc:  0.8359375
train loss:  0.41946256160736084
train gradient:  0.39089451800558994
iteration : 3363
train acc:  0.859375
train loss:  0.3929736018180847
train gradient:  0.2888373588260493
iteration : 3364
train acc:  0.84375
train loss:  0.3897739052772522
train gradient:  0.32354051109603343
iteration : 3365
train acc:  0.78125
train loss:  0.4666864275932312
train gradient:  0.46368958948828803
iteration : 3366
train acc:  0.8046875
train loss:  0.3784409463405609
train gradient:  0.3084668139763767
iteration : 3367
train acc:  0.78125
train loss:  0.45601963996887207
train gradient:  0.5095492920979
iteration : 3368
train acc:  0.796875
train loss:  0.45605766773223877
train gradient:  0.42042683065403186
iteration : 3369
train acc:  0.8828125
train loss:  0.29897060990333557
train gradient:  0.23371034869437862
iteration : 3370
train acc:  0.8203125
train loss:  0.3669959306716919
train gradient:  0.27371753206057936
iteration : 3371
train acc:  0.8125
train loss:  0.47125327587127686
train gradient:  0.36882751838723793
iteration : 3372
train acc:  0.828125
train loss:  0.38222986459732056
train gradient:  0.33105932269452326
iteration : 3373
train acc:  0.84375
train loss:  0.36331331729888916
train gradient:  0.2403559261922268
iteration : 3374
train acc:  0.84375
train loss:  0.342170774936676
train gradient:  0.21911485564924904
iteration : 3375
train acc:  0.8359375
train loss:  0.39570873975753784
train gradient:  0.3462205099470082
iteration : 3376
train acc:  0.8515625
train loss:  0.35641640424728394
train gradient:  0.2702186378708214
iteration : 3377
train acc:  0.765625
train loss:  0.48298296332359314
train gradient:  0.5632816566288158
iteration : 3378
train acc:  0.8359375
train loss:  0.4045218527317047
train gradient:  0.2789429204549062
iteration : 3379
train acc:  0.8828125
train loss:  0.2964843809604645
train gradient:  0.28069816918662005
iteration : 3380
train acc:  0.8671875
train loss:  0.3142451345920563
train gradient:  0.2569195760180546
iteration : 3381
train acc:  0.8984375
train loss:  0.3025045394897461
train gradient:  0.33610588269922104
iteration : 3382
train acc:  0.890625
train loss:  0.31128302216529846
train gradient:  0.2512187413417498
iteration : 3383
train acc:  0.8125
train loss:  0.41376596689224243
train gradient:  0.3403385656139534
iteration : 3384
train acc:  0.8671875
train loss:  0.3405386507511139
train gradient:  0.30133158988405284
iteration : 3385
train acc:  0.8203125
train loss:  0.3432089686393738
train gradient:  0.3044166593820425
iteration : 3386
train acc:  0.796875
train loss:  0.4173339605331421
train gradient:  0.26947254708930063
iteration : 3387
train acc:  0.875
train loss:  0.3258737325668335
train gradient:  0.3267423053513982
iteration : 3388
train acc:  0.859375
train loss:  0.41598832607269287
train gradient:  0.47004681976969714
iteration : 3389
train acc:  0.7734375
train loss:  0.46268194913864136
train gradient:  0.5018104073208204
iteration : 3390
train acc:  0.8125
train loss:  0.4228745996952057
train gradient:  0.28676741979984355
iteration : 3391
train acc:  0.859375
train loss:  0.3337923586368561
train gradient:  0.3422461258232246
iteration : 3392
train acc:  0.8125
train loss:  0.4020976424217224
train gradient:  0.397133946822261
iteration : 3393
train acc:  0.8046875
train loss:  0.4159229099750519
train gradient:  0.41419887542985984
iteration : 3394
train acc:  0.859375
train loss:  0.30863434076309204
train gradient:  0.2735812071255533
iteration : 3395
train acc:  0.796875
train loss:  0.431327223777771
train gradient:  0.39612683683983135
iteration : 3396
train acc:  0.828125
train loss:  0.384794145822525
train gradient:  0.48936186534999115
iteration : 3397
train acc:  0.8203125
train loss:  0.35084736347198486
train gradient:  0.32445727850293593
iteration : 3398
train acc:  0.828125
train loss:  0.3365991711616516
train gradient:  0.18300258568807698
iteration : 3399
train acc:  0.8828125
train loss:  0.3425900936126709
train gradient:  0.30247689071438205
iteration : 3400
train acc:  0.8359375
train loss:  0.33821335434913635
train gradient:  0.2453647364710815
iteration : 3401
train acc:  0.8203125
train loss:  0.37247008085250854
train gradient:  0.39477029308568473
iteration : 3402
train acc:  0.828125
train loss:  0.3994360864162445
train gradient:  0.36394357634049124
iteration : 3403
train acc:  0.796875
train loss:  0.3854662775993347
train gradient:  0.3375989785450297
iteration : 3404
train acc:  0.8984375
train loss:  0.2607851028442383
train gradient:  0.1667085237297585
iteration : 3405
train acc:  0.8203125
train loss:  0.42268985509872437
train gradient:  0.3490835193658225
iteration : 3406
train acc:  0.8515625
train loss:  0.36870279908180237
train gradient:  0.38209237110826605
iteration : 3407
train acc:  0.8359375
train loss:  0.39024120569229126
train gradient:  0.3164341992540444
iteration : 3408
train acc:  0.84375
train loss:  0.346548855304718
train gradient:  0.28952092330885987
iteration : 3409
train acc:  0.8515625
train loss:  0.34439271688461304
train gradient:  0.36043890897437886
iteration : 3410
train acc:  0.8359375
train loss:  0.3775256872177124
train gradient:  0.4082015010748067
iteration : 3411
train acc:  0.78125
train loss:  0.4865063428878784
train gradient:  0.45191285015459404
iteration : 3412
train acc:  0.8203125
train loss:  0.3391592800617218
train gradient:  0.37404398797964866
iteration : 3413
train acc:  0.84375
train loss:  0.44576582312583923
train gradient:  0.4358246982076204
iteration : 3414
train acc:  0.8515625
train loss:  0.3331787586212158
train gradient:  0.21998894174714767
iteration : 3415
train acc:  0.7890625
train loss:  0.4250484108924866
train gradient:  0.32763661641230024
iteration : 3416
train acc:  0.8515625
train loss:  0.3858661651611328
train gradient:  0.27913479946028913
iteration : 3417
train acc:  0.8125
train loss:  0.39676791429519653
train gradient:  0.33670921547707794
iteration : 3418
train acc:  0.8515625
train loss:  0.38723742961883545
train gradient:  0.3940280646149107
iteration : 3419
train acc:  0.828125
train loss:  0.41664397716522217
train gradient:  0.2851796107642408
iteration : 3420
train acc:  0.8125
train loss:  0.458124041557312
train gradient:  0.6995239665085198
iteration : 3421
train acc:  0.7890625
train loss:  0.4101889431476593
train gradient:  0.33716569593458967
iteration : 3422
train acc:  0.8203125
train loss:  0.38600242137908936
train gradient:  0.2982690500232824
iteration : 3423
train acc:  0.7890625
train loss:  0.5133585929870605
train gradient:  0.36700044201083376
iteration : 3424
train acc:  0.8203125
train loss:  0.45015355944633484
train gradient:  0.4903080644837034
iteration : 3425
train acc:  0.8046875
train loss:  0.4766150712966919
train gradient:  0.34672556851491143
iteration : 3426
train acc:  0.828125
train loss:  0.3729403614997864
train gradient:  0.2856346288778668
iteration : 3427
train acc:  0.7734375
train loss:  0.44306546449661255
train gradient:  0.37125828282169676
iteration : 3428
train acc:  0.7890625
train loss:  0.4765164256095886
train gradient:  0.42848562838173965
iteration : 3429
train acc:  0.828125
train loss:  0.3745744228363037
train gradient:  0.286766338598216
iteration : 3430
train acc:  0.828125
train loss:  0.33589503169059753
train gradient:  0.26159038149345626
iteration : 3431
train acc:  0.859375
train loss:  0.3069673180580139
train gradient:  0.18847170897101245
iteration : 3432
train acc:  0.828125
train loss:  0.367036908864975
train gradient:  0.21442953616659893
iteration : 3433
train acc:  0.84375
train loss:  0.35830825567245483
train gradient:  0.32705682360233795
iteration : 3434
train acc:  0.796875
train loss:  0.5270001888275146
train gradient:  0.4843793844555322
iteration : 3435
train acc:  0.8515625
train loss:  0.36218947172164917
train gradient:  0.2491856546187974
iteration : 3436
train acc:  0.8359375
train loss:  0.333105206489563
train gradient:  0.2063197666224421
iteration : 3437
train acc:  0.84375
train loss:  0.38114574551582336
train gradient:  0.4318457505164846
iteration : 3438
train acc:  0.90625
train loss:  0.28830423951148987
train gradient:  0.17773989725761696
iteration : 3439
train acc:  0.84375
train loss:  0.37075403332710266
train gradient:  0.2947676744203719
iteration : 3440
train acc:  0.8203125
train loss:  0.36701661348342896
train gradient:  0.2585529929812686
iteration : 3441
train acc:  0.8203125
train loss:  0.37061795592308044
train gradient:  0.20654466004299574
iteration : 3442
train acc:  0.8046875
train loss:  0.3849353790283203
train gradient:  0.2937027135551836
iteration : 3443
train acc:  0.8671875
train loss:  0.34203898906707764
train gradient:  0.22327267564544792
iteration : 3444
train acc:  0.8359375
train loss:  0.3691176176071167
train gradient:  0.2232461522662117
iteration : 3445
train acc:  0.859375
train loss:  0.33800333738327026
train gradient:  0.3652129700790993
iteration : 3446
train acc:  0.84375
train loss:  0.340421199798584
train gradient:  0.3644459000845194
iteration : 3447
train acc:  0.828125
train loss:  0.36106523871421814
train gradient:  0.28222029938050897
iteration : 3448
train acc:  0.8046875
train loss:  0.3922975957393646
train gradient:  0.294069356822778
iteration : 3449
train acc:  0.8203125
train loss:  0.41176891326904297
train gradient:  0.3315820330391457
iteration : 3450
train acc:  0.8671875
train loss:  0.3179854154586792
train gradient:  0.16950626880922476
iteration : 3451
train acc:  0.828125
train loss:  0.36514225602149963
train gradient:  0.18474938516145306
iteration : 3452
train acc:  0.8125
train loss:  0.37494659423828125
train gradient:  0.22791704851019098
iteration : 3453
train acc:  0.8515625
train loss:  0.32283449172973633
train gradient:  0.2340989149284639
iteration : 3454
train acc:  0.7890625
train loss:  0.4022389054298401
train gradient:  0.23592805949875556
iteration : 3455
train acc:  0.875
train loss:  0.34734421968460083
train gradient:  0.32353041009118527
iteration : 3456
train acc:  0.8359375
train loss:  0.35407042503356934
train gradient:  0.2724894116117302
iteration : 3457
train acc:  0.765625
train loss:  0.49993303418159485
train gradient:  0.6739536936495357
iteration : 3458
train acc:  0.8125
train loss:  0.38445574045181274
train gradient:  0.25221437094554194
iteration : 3459
train acc:  0.8046875
train loss:  0.41481325030326843
train gradient:  0.3679847053537727
iteration : 3460
train acc:  0.84375
train loss:  0.3747773766517639
train gradient:  0.2090498764056325
iteration : 3461
train acc:  0.8125
train loss:  0.47818723320961
train gradient:  0.46677283120586993
iteration : 3462
train acc:  0.875
train loss:  0.32346031069755554
train gradient:  0.26096269661488514
iteration : 3463
train acc:  0.8125
train loss:  0.3815121650695801
train gradient:  0.2989454485636264
iteration : 3464
train acc:  0.8203125
train loss:  0.38468170166015625
train gradient:  0.3272170610500134
iteration : 3465
train acc:  0.828125
train loss:  0.4253659248352051
train gradient:  0.3712652736530653
iteration : 3466
train acc:  0.8671875
train loss:  0.3264845907688141
train gradient:  0.27466976943341054
iteration : 3467
train acc:  0.9140625
train loss:  0.2699982523918152
train gradient:  0.18382398551357298
iteration : 3468
train acc:  0.828125
train loss:  0.3266955316066742
train gradient:  0.2762253142670696
iteration : 3469
train acc:  0.8515625
train loss:  0.3178393244743347
train gradient:  0.24916466478116783
iteration : 3470
train acc:  0.84375
train loss:  0.3688153624534607
train gradient:  0.3026684725802382
iteration : 3471
train acc:  0.8671875
train loss:  0.31937041878700256
train gradient:  0.25366043768514246
iteration : 3472
train acc:  0.90625
train loss:  0.2714475989341736
train gradient:  0.22113773300794504
iteration : 3473
train acc:  0.90625
train loss:  0.2816791534423828
train gradient:  0.20811830638635254
iteration : 3474
train acc:  0.828125
train loss:  0.34771397709846497
train gradient:  0.20784790199652506
iteration : 3475
train acc:  0.8359375
train loss:  0.36833101511001587
train gradient:  0.21750858803642137
iteration : 3476
train acc:  0.8046875
train loss:  0.4373738169670105
train gradient:  0.3193937961289074
iteration : 3477
train acc:  0.8515625
train loss:  0.34705787897109985
train gradient:  0.24863176360068617
iteration : 3478
train acc:  0.875
train loss:  0.3037404417991638
train gradient:  0.24703930106387142
iteration : 3479
train acc:  0.859375
train loss:  0.3016388416290283
train gradient:  0.19941134732720636
iteration : 3480
train acc:  0.84375
train loss:  0.3366723954677582
train gradient:  0.23172239075926482
iteration : 3481
train acc:  0.8515625
train loss:  0.3944060206413269
train gradient:  0.38083368671459944
iteration : 3482
train acc:  0.8359375
train loss:  0.3507322072982788
train gradient:  0.4201340164461358
iteration : 3483
train acc:  0.8125
train loss:  0.4020121693611145
train gradient:  0.27876398321050283
iteration : 3484
train acc:  0.8515625
train loss:  0.34812018275260925
train gradient:  0.3355679861823686
iteration : 3485
train acc:  0.8203125
train loss:  0.35230064392089844
train gradient:  0.2615910638164819
iteration : 3486
train acc:  0.8046875
train loss:  0.4410642087459564
train gradient:  0.4374447348158258
iteration : 3487
train acc:  0.84375
train loss:  0.39097559452056885
train gradient:  0.3010604484566338
iteration : 3488
train acc:  0.765625
train loss:  0.4724988341331482
train gradient:  0.4740453388474965
iteration : 3489
train acc:  0.8359375
train loss:  0.348992258310318
train gradient:  0.24851914392446411
iteration : 3490
train acc:  0.78125
train loss:  0.4141695499420166
train gradient:  0.7790116474921424
iteration : 3491
train acc:  0.84375
train loss:  0.3866886496543884
train gradient:  0.3921410300045983
iteration : 3492
train acc:  0.84375
train loss:  0.33900678157806396
train gradient:  0.22767060696555888
iteration : 3493
train acc:  0.8515625
train loss:  0.33990317583084106
train gradient:  0.24331455596180823
iteration : 3494
train acc:  0.8515625
train loss:  0.36477896571159363
train gradient:  0.3094944040172795
iteration : 3495
train acc:  0.828125
train loss:  0.3563067615032196
train gradient:  0.2783402038970391
iteration : 3496
train acc:  0.875
train loss:  0.33566272258758545
train gradient:  0.2781003959029938
iteration : 3497
train acc:  0.828125
train loss:  0.3306025266647339
train gradient:  0.2245046307833803
iteration : 3498
train acc:  0.8046875
train loss:  0.42706558108329773
train gradient:  0.5015946000835059
iteration : 3499
train acc:  0.8046875
train loss:  0.3708764910697937
train gradient:  0.40778627092768
iteration : 3500
train acc:  0.796875
train loss:  0.4862835705280304
train gradient:  0.34879007998806827
iteration : 3501
train acc:  0.8359375
train loss:  0.35943034291267395
train gradient:  0.3618222096054275
iteration : 3502
train acc:  0.8359375
train loss:  0.3592369556427002
train gradient:  0.2766965005247237
iteration : 3503
train acc:  0.84375
train loss:  0.38154321908950806
train gradient:  0.25446296553278597
iteration : 3504
train acc:  0.890625
train loss:  0.29046669602394104
train gradient:  0.2677513305448363
iteration : 3505
train acc:  0.8359375
train loss:  0.3811652660369873
train gradient:  0.29980062139535624
iteration : 3506
train acc:  0.859375
train loss:  0.3255288004875183
train gradient:  0.3001421554156343
iteration : 3507
train acc:  0.84375
train loss:  0.36597496271133423
train gradient:  0.4193063423446204
iteration : 3508
train acc:  0.828125
train loss:  0.33345597982406616
train gradient:  0.3402554102155732
iteration : 3509
train acc:  0.890625
train loss:  0.32510003447532654
train gradient:  0.3531264741910571
iteration : 3510
train acc:  0.8515625
train loss:  0.33821022510528564
train gradient:  0.25562661457869174
iteration : 3511
train acc:  0.8359375
train loss:  0.3478986322879791
train gradient:  0.30121962971909216
iteration : 3512
train acc:  0.8515625
train loss:  0.3671208322048187
train gradient:  0.27664573333269477
iteration : 3513
train acc:  0.8125
train loss:  0.3815256357192993
train gradient:  0.4060465838784221
iteration : 3514
train acc:  0.828125
train loss:  0.37694257497787476
train gradient:  0.28719350388608245
iteration : 3515
train acc:  0.7890625
train loss:  0.40519165992736816
train gradient:  0.40305031095210403
iteration : 3516
train acc:  0.8515625
train loss:  0.37354668974876404
train gradient:  0.3182778479395328
iteration : 3517
train acc:  0.84375
train loss:  0.33780401945114136
train gradient:  0.25247716190703445
iteration : 3518
train acc:  0.8203125
train loss:  0.3881017565727234
train gradient:  0.5206729324746658
iteration : 3519
train acc:  0.84375
train loss:  0.3725157380104065
train gradient:  0.33362138913115386
iteration : 3520
train acc:  0.8359375
train loss:  0.3748001456260681
train gradient:  0.37744514922379063
iteration : 3521
train acc:  0.84375
train loss:  0.3908522427082062
train gradient:  0.36142507305138344
iteration : 3522
train acc:  0.8515625
train loss:  0.33796870708465576
train gradient:  0.24339813045169045
iteration : 3523
train acc:  0.859375
train loss:  0.3736035227775574
train gradient:  0.3517038871764793
iteration : 3524
train acc:  0.8359375
train loss:  0.3034123480319977
train gradient:  0.1802832348505499
iteration : 3525
train acc:  0.8125
train loss:  0.38869351148605347
train gradient:  0.38762313946960325
iteration : 3526
train acc:  0.7734375
train loss:  0.4728960394859314
train gradient:  0.6090093855203464
iteration : 3527
train acc:  0.796875
train loss:  0.4008433520793915
train gradient:  0.3643759614622554
iteration : 3528
train acc:  0.8125
train loss:  0.36890241503715515
train gradient:  0.2916646577377704
iteration : 3529
train acc:  0.8046875
train loss:  0.43866676092147827
train gradient:  0.289386846833637
iteration : 3530
train acc:  0.828125
train loss:  0.38263747096061707
train gradient:  0.3146693374096819
iteration : 3531
train acc:  0.8515625
train loss:  0.35853976011276245
train gradient:  0.22032999482952004
iteration : 3532
train acc:  0.8671875
train loss:  0.3204212188720703
train gradient:  0.20694122829252723
iteration : 3533
train acc:  0.875
train loss:  0.2682381272315979
train gradient:  0.19511959558087483
iteration : 3534
train acc:  0.84375
train loss:  0.3353674113750458
train gradient:  0.2522681496923461
iteration : 3535
train acc:  0.859375
train loss:  0.28843194246292114
train gradient:  0.25830368103498713
iteration : 3536
train acc:  0.8671875
train loss:  0.3720729351043701
train gradient:  0.3153325029766174
iteration : 3537
train acc:  0.828125
train loss:  0.35360023379325867
train gradient:  0.3623031894509113
iteration : 3538
train acc:  0.8515625
train loss:  0.2748115360736847
train gradient:  0.1866007009625576
iteration : 3539
train acc:  0.84375
train loss:  0.3894517719745636
train gradient:  0.2455282618468256
iteration : 3540
train acc:  0.8671875
train loss:  0.33018338680267334
train gradient:  0.23559542023929642
iteration : 3541
train acc:  0.7890625
train loss:  0.46339428424835205
train gradient:  0.36603255734735146
iteration : 3542
train acc:  0.8515625
train loss:  0.38002413511276245
train gradient:  0.23898651847947192
iteration : 3543
train acc:  0.8515625
train loss:  0.33331844210624695
train gradient:  0.44464181302614025
iteration : 3544
train acc:  0.8203125
train loss:  0.3549257814884186
train gradient:  0.25890243920128647
iteration : 3545
train acc:  0.8515625
train loss:  0.3639260530471802
train gradient:  0.2566425231975667
iteration : 3546
train acc:  0.859375
train loss:  0.33296263217926025
train gradient:  0.21581308058932697
iteration : 3547
train acc:  0.84375
train loss:  0.3327232301235199
train gradient:  0.29251111474487734
iteration : 3548
train acc:  0.890625
train loss:  0.28183692693710327
train gradient:  0.19380603719544723
iteration : 3549
train acc:  0.859375
train loss:  0.3314003646373749
train gradient:  0.2837524334561103
iteration : 3550
train acc:  0.84375
train loss:  0.3533729314804077
train gradient:  0.3763836603532614
iteration : 3551
train acc:  0.8046875
train loss:  0.4092315435409546
train gradient:  0.30741696838811167
iteration : 3552
train acc:  0.796875
train loss:  0.3921021521091461
train gradient:  0.38280907843101314
iteration : 3553
train acc:  0.8125
train loss:  0.3859380781650543
train gradient:  0.3201776891792482
iteration : 3554
train acc:  0.8828125
train loss:  0.3583906888961792
train gradient:  0.24705537947649195
iteration : 3555
train acc:  0.7890625
train loss:  0.41048794984817505
train gradient:  0.33677032635713094
iteration : 3556
train acc:  0.796875
train loss:  0.4065351188182831
train gradient:  0.43157519092157287
iteration : 3557
train acc:  0.8203125
train loss:  0.38451147079467773
train gradient:  0.36755433573848206
iteration : 3558
train acc:  0.7734375
train loss:  0.4801863729953766
train gradient:  0.37444492126030554
iteration : 3559
train acc:  0.859375
train loss:  0.3946998119354248
train gradient:  0.27720196765026167
iteration : 3560
train acc:  0.78125
train loss:  0.41475823521614075
train gradient:  0.325593129876245
iteration : 3561
train acc:  0.8125
train loss:  0.43405336141586304
train gradient:  0.44580293189546555
iteration : 3562
train acc:  0.828125
train loss:  0.41179871559143066
train gradient:  0.34863596660796065
iteration : 3563
train acc:  0.859375
train loss:  0.35835397243499756
train gradient:  0.3149150856103355
iteration : 3564
train acc:  0.890625
train loss:  0.29906877875328064
train gradient:  0.17542442463256736
iteration : 3565
train acc:  0.84375
train loss:  0.358248770236969
train gradient:  0.5078203609408063
iteration : 3566
train acc:  0.796875
train loss:  0.3853924870491028
train gradient:  0.40037143228930466
iteration : 3567
train acc:  0.859375
train loss:  0.33890849351882935
train gradient:  0.2439199006389801
iteration : 3568
train acc:  0.8125
train loss:  0.42844611406326294
train gradient:  0.41070706511355776
iteration : 3569
train acc:  0.7734375
train loss:  0.47788792848587036
train gradient:  0.4616142302398642
iteration : 3570
train acc:  0.8515625
train loss:  0.3289170563220978
train gradient:  0.24950568356048985
iteration : 3571
train acc:  0.8203125
train loss:  0.4712473750114441
train gradient:  0.4118035617769883
iteration : 3572
train acc:  0.828125
train loss:  0.37902918457984924
train gradient:  0.2375256426459431
iteration : 3573
train acc:  0.84375
train loss:  0.3622373342514038
train gradient:  0.29133699816136144
iteration : 3574
train acc:  0.8359375
train loss:  0.39466914534568787
train gradient:  0.3643089747139895
iteration : 3575
train acc:  0.84375
train loss:  0.38712581992149353
train gradient:  0.3655618140998954
iteration : 3576
train acc:  0.8828125
train loss:  0.3422176241874695
train gradient:  0.34389479923538324
iteration : 3577
train acc:  0.8671875
train loss:  0.38007545471191406
train gradient:  0.29234670220429926
iteration : 3578
train acc:  0.84375
train loss:  0.33088722825050354
train gradient:  0.24491729838304852
iteration : 3579
train acc:  0.8984375
train loss:  0.32950568199157715
train gradient:  0.30058970914022254
iteration : 3580
train acc:  0.84375
train loss:  0.3712083101272583
train gradient:  0.2571410680689761
iteration : 3581
train acc:  0.8984375
train loss:  0.2690994143486023
train gradient:  0.1482719557188002
iteration : 3582
train acc:  0.8125
train loss:  0.3629935681819916
train gradient:  0.5370744803150354
iteration : 3583
train acc:  0.8203125
train loss:  0.3828381597995758
train gradient:  0.29851882566935833
iteration : 3584
train acc:  0.8125
train loss:  0.44681745767593384
train gradient:  0.38411352890015726
iteration : 3585
train acc:  0.7890625
train loss:  0.4384492039680481
train gradient:  0.4261900907781797
iteration : 3586
train acc:  0.828125
train loss:  0.35805535316467285
train gradient:  0.2557967544298265
iteration : 3587
train acc:  0.7890625
train loss:  0.421570748090744
train gradient:  0.36813255258222455
iteration : 3588
train acc:  0.8359375
train loss:  0.3573330044746399
train gradient:  0.28669247198275677
iteration : 3589
train acc:  0.8046875
train loss:  0.3939438760280609
train gradient:  0.2748398862579365
iteration : 3590
train acc:  0.8203125
train loss:  0.343580961227417
train gradient:  0.26173608741764864
iteration : 3591
train acc:  0.8203125
train loss:  0.3647165298461914
train gradient:  0.2814439744356481
iteration : 3592
train acc:  0.828125
train loss:  0.38106635212898254
train gradient:  0.5445512793174383
iteration : 3593
train acc:  0.8515625
train loss:  0.38112592697143555
train gradient:  0.4387157321768393
iteration : 3594
train acc:  0.859375
train loss:  0.3532443940639496
train gradient:  0.2899214406525885
iteration : 3595
train acc:  0.8828125
train loss:  0.2993455231189728
train gradient:  0.27493309626528
iteration : 3596
train acc:  0.890625
train loss:  0.33193641901016235
train gradient:  0.19299621029117758
iteration : 3597
train acc:  0.859375
train loss:  0.3463398218154907
train gradient:  0.24639926455149824
iteration : 3598
train acc:  0.7890625
train loss:  0.42897292971611023
train gradient:  0.3021932671617974
iteration : 3599
train acc:  0.796875
train loss:  0.39474257826805115
train gradient:  0.517596670560015
iteration : 3600
train acc:  0.8515625
train loss:  0.34679102897644043
train gradient:  0.2621864960534612
iteration : 3601
train acc:  0.7890625
train loss:  0.392547607421875
train gradient:  0.2901707015778064
iteration : 3602
train acc:  0.8203125
train loss:  0.3855542540550232
train gradient:  0.2645695264676954
iteration : 3603
train acc:  0.8671875
train loss:  0.31231361627578735
train gradient:  0.23533620673575883
iteration : 3604
train acc:  0.828125
train loss:  0.3901289105415344
train gradient:  0.2682619266167559
iteration : 3605
train acc:  0.828125
train loss:  0.3886616826057434
train gradient:  0.36659823406848707
iteration : 3606
train acc:  0.859375
train loss:  0.3236783444881439
train gradient:  0.27757175505995624
iteration : 3607
train acc:  0.8359375
train loss:  0.3638763427734375
train gradient:  0.3310455101107034
iteration : 3608
train acc:  0.828125
train loss:  0.35856643319129944
train gradient:  0.2435747297910156
iteration : 3609
train acc:  0.796875
train loss:  0.35547688603401184
train gradient:  0.3115915786603568
iteration : 3610
train acc:  0.8671875
train loss:  0.39030715823173523
train gradient:  0.2617977162373539
iteration : 3611
train acc:  0.8125
train loss:  0.39103829860687256
train gradient:  0.368890748847496
iteration : 3612
train acc:  0.78125
train loss:  0.4166933000087738
train gradient:  0.36096559961395314
iteration : 3613
train acc:  0.8046875
train loss:  0.37403029203414917
train gradient:  0.3459110209964891
iteration : 3614
train acc:  0.8671875
train loss:  0.3562198877334595
train gradient:  0.2659414233320334
iteration : 3615
train acc:  0.8359375
train loss:  0.39489829540252686
train gradient:  0.227584402124418
iteration : 3616
train acc:  0.875
train loss:  0.37605467438697815
train gradient:  0.4073205570101109
iteration : 3617
train acc:  0.8515625
train loss:  0.3221172094345093
train gradient:  0.2332798309787031
iteration : 3618
train acc:  0.8203125
train loss:  0.3919371962547302
train gradient:  0.29023093461387
iteration : 3619
train acc:  0.7734375
train loss:  0.39408475160598755
train gradient:  0.3115922909384466
iteration : 3620
train acc:  0.828125
train loss:  0.34488382935523987
train gradient:  0.3738974840608735
iteration : 3621
train acc:  0.828125
train loss:  0.3562602996826172
train gradient:  0.24594961694381406
iteration : 3622
train acc:  0.8359375
train loss:  0.29717135429382324
train gradient:  0.17292571683794572
iteration : 3623
train acc:  0.8515625
train loss:  0.34940603375434875
train gradient:  0.2269351032420434
iteration : 3624
train acc:  0.8828125
train loss:  0.310383677482605
train gradient:  0.23358887033474024
iteration : 3625
train acc:  0.890625
train loss:  0.24613119661808014
train gradient:  0.15253843168953085
iteration : 3626
train acc:  0.8671875
train loss:  0.3001261353492737
train gradient:  0.22967088571919475
iteration : 3627
train acc:  0.7890625
train loss:  0.40035372972488403
train gradient:  0.4278035414470551
iteration : 3628
train acc:  0.8515625
train loss:  0.4150540828704834
train gradient:  0.37894247262073605
iteration : 3629
train acc:  0.828125
train loss:  0.3667084872722626
train gradient:  0.2530099287701682
iteration : 3630
train acc:  0.84375
train loss:  0.3366522192955017
train gradient:  0.24200015643714373
iteration : 3631
train acc:  0.8515625
train loss:  0.3538873791694641
train gradient:  0.31910621281238016
iteration : 3632
train acc:  0.8203125
train loss:  0.3621697425842285
train gradient:  0.33490526621950784
iteration : 3633
train acc:  0.8359375
train loss:  0.3522719144821167
train gradient:  0.25264998770468106
iteration : 3634
train acc:  0.7421875
train loss:  0.5209057331085205
train gradient:  0.5376564558220247
iteration : 3635
train acc:  0.8671875
train loss:  0.31176334619522095
train gradient:  0.25423630753846016
iteration : 3636
train acc:  0.828125
train loss:  0.4024263918399811
train gradient:  0.44474961925239426
iteration : 3637
train acc:  0.828125
train loss:  0.330984890460968
train gradient:  0.30528075231903573
iteration : 3638
train acc:  0.8828125
train loss:  0.3020946681499481
train gradient:  0.21178457590972216
iteration : 3639
train acc:  0.84375
train loss:  0.3261633813381195
train gradient:  0.3274648731318237
iteration : 3640
train acc:  0.859375
train loss:  0.2749825119972229
train gradient:  0.1996681108179251
iteration : 3641
train acc:  0.890625
train loss:  0.27701467275619507
train gradient:  0.2080376453861693
iteration : 3642
train acc:  0.796875
train loss:  0.42201828956604004
train gradient:  0.4837142766794237
iteration : 3643
train acc:  0.8515625
train loss:  0.32852625846862793
train gradient:  0.2695408543905563
iteration : 3644
train acc:  0.875
train loss:  0.31110501289367676
train gradient:  0.23147649842217763
iteration : 3645
train acc:  0.8828125
train loss:  0.38327765464782715
train gradient:  0.3050171067484337
iteration : 3646
train acc:  0.8671875
train loss:  0.3247189223766327
train gradient:  0.25230980362633365
iteration : 3647
train acc:  0.8125
train loss:  0.41635599732398987
train gradient:  0.3875038334129727
iteration : 3648
train acc:  0.8359375
train loss:  0.36280807852745056
train gradient:  0.28145666127393315
iteration : 3649
train acc:  0.8828125
train loss:  0.3272378146648407
train gradient:  0.23104240026568115
iteration : 3650
train acc:  0.796875
train loss:  0.3920987844467163
train gradient:  0.4496877164748851
iteration : 3651
train acc:  0.8125
train loss:  0.4483547806739807
train gradient:  0.2994011161796201
iteration : 3652
train acc:  0.8125
train loss:  0.3886842727661133
train gradient:  0.48373318987488556
iteration : 3653
train acc:  0.8125
train loss:  0.41868913173675537
train gradient:  0.3246876365290575
iteration : 3654
train acc:  0.7734375
train loss:  0.417891263961792
train gradient:  0.40353687825000106
iteration : 3655
train acc:  0.8046875
train loss:  0.3705716133117676
train gradient:  0.32188605615764426
iteration : 3656
train acc:  0.78125
train loss:  0.45510441064834595
train gradient:  0.412532245508207
iteration : 3657
train acc:  0.8203125
train loss:  0.41327691078186035
train gradient:  0.32176553417118264
iteration : 3658
train acc:  0.890625
train loss:  0.31037062406539917
train gradient:  0.3466655101620901
iteration : 3659
train acc:  0.8203125
train loss:  0.3589235544204712
train gradient:  0.24650316085746266
iteration : 3660
train acc:  0.84375
train loss:  0.38953137397766113
train gradient:  0.3633338364174575
iteration : 3661
train acc:  0.8359375
train loss:  0.35537683963775635
train gradient:  0.3124086653046872
iteration : 3662
train acc:  0.8046875
train loss:  0.3446062505245209
train gradient:  0.2881399403887224
iteration : 3663
train acc:  0.84375
train loss:  0.36071279644966125
train gradient:  0.23593228388282966
iteration : 3664
train acc:  0.8515625
train loss:  0.347140371799469
train gradient:  0.20302121402162962
iteration : 3665
train acc:  0.8203125
train loss:  0.36660560965538025
train gradient:  0.44376809704931897
iteration : 3666
train acc:  0.8125
train loss:  0.4248989522457123
train gradient:  0.30962265889227397
iteration : 3667
train acc:  0.7890625
train loss:  0.4568901062011719
train gradient:  0.3982989300301989
iteration : 3668
train acc:  0.8125
train loss:  0.4203101396560669
train gradient:  0.39411775649751546
iteration : 3669
train acc:  0.8515625
train loss:  0.3543075919151306
train gradient:  0.3895433990704818
iteration : 3670
train acc:  0.8046875
train loss:  0.41588205099105835
train gradient:  0.26537466101039336
iteration : 3671
train acc:  0.8359375
train loss:  0.3266058564186096
train gradient:  0.26025808084799196
iteration : 3672
train acc:  0.8828125
train loss:  0.2712203860282898
train gradient:  0.2086868668328129
iteration : 3673
train acc:  0.78125
train loss:  0.44730693101882935
train gradient:  0.4099857141159883
iteration : 3674
train acc:  0.8515625
train loss:  0.3549298048019409
train gradient:  0.3559202932328063
iteration : 3675
train acc:  0.796875
train loss:  0.4234694540500641
train gradient:  0.3581128624430903
iteration : 3676
train acc:  0.859375
train loss:  0.38202232122421265
train gradient:  0.3461366759325916
iteration : 3677
train acc:  0.8046875
train loss:  0.49329784512519836
train gradient:  0.608782409723842
iteration : 3678
train acc:  0.84375
train loss:  0.35251736640930176
train gradient:  0.2769658563964875
iteration : 3679
train acc:  0.8125
train loss:  0.3638033866882324
train gradient:  0.31570933539472823
iteration : 3680
train acc:  0.796875
train loss:  0.41175931692123413
train gradient:  0.27683724292437956
iteration : 3681
train acc:  0.8359375
train loss:  0.3907290995121002
train gradient:  0.30897579532513564
iteration : 3682
train acc:  0.8125
train loss:  0.4137594699859619
train gradient:  0.31062622253986716
iteration : 3683
train acc:  0.84375
train loss:  0.3126332461833954
train gradient:  0.19790759566816118
iteration : 3684
train acc:  0.828125
train loss:  0.33542659878730774
train gradient:  0.2829921687180399
iteration : 3685
train acc:  0.859375
train loss:  0.3191874027252197
train gradient:  0.23591443054355782
iteration : 3686
train acc:  0.8984375
train loss:  0.29355287551879883
train gradient:  0.1827799921546694
iteration : 3687
train acc:  0.8125
train loss:  0.4221178889274597
train gradient:  0.42187379904661343
iteration : 3688
train acc:  0.8671875
train loss:  0.328066349029541
train gradient:  0.18286932215405646
iteration : 3689
train acc:  0.8203125
train loss:  0.35121968388557434
train gradient:  0.2098890579271328
iteration : 3690
train acc:  0.8671875
train loss:  0.34227216243743896
train gradient:  0.2394709001085601
iteration : 3691
train acc:  0.828125
train loss:  0.37238219380378723
train gradient:  0.2922554821207918
iteration : 3692
train acc:  0.890625
train loss:  0.30651652812957764
train gradient:  0.30256940776329067
iteration : 3693
train acc:  0.84375
train loss:  0.333640456199646
train gradient:  0.23578961097929105
iteration : 3694
train acc:  0.828125
train loss:  0.36412596702575684
train gradient:  0.329288222559203
iteration : 3695
train acc:  0.8046875
train loss:  0.36963456869125366
train gradient:  0.28894990986055785
iteration : 3696
train acc:  0.875
train loss:  0.30261749029159546
train gradient:  0.22824469707149336
iteration : 3697
train acc:  0.9140625
train loss:  0.25072795152664185
train gradient:  0.1522167786264193
iteration : 3698
train acc:  0.84375
train loss:  0.35846227407455444
train gradient:  0.29808632916588657
iteration : 3699
train acc:  0.796875
train loss:  0.4124976694583893
train gradient:  0.3266596877099227
iteration : 3700
train acc:  0.8046875
train loss:  0.37918514013290405
train gradient:  0.41220862706406486
iteration : 3701
train acc:  0.8984375
train loss:  0.28670692443847656
train gradient:  0.1506941907943981
iteration : 3702
train acc:  0.875
train loss:  0.32046669721603394
train gradient:  0.21620451256247605
iteration : 3703
train acc:  0.859375
train loss:  0.28060397505760193
train gradient:  0.1487759446263452
iteration : 3704
train acc:  0.8203125
train loss:  0.3721770644187927
train gradient:  0.2743937534607831
iteration : 3705
train acc:  0.84375
train loss:  0.353484570980072
train gradient:  0.2704718511651378
iteration : 3706
train acc:  0.8984375
train loss:  0.31960076093673706
train gradient:  0.31952997646934783
iteration : 3707
train acc:  0.796875
train loss:  0.39396360516548157
train gradient:  0.29613532083495553
iteration : 3708
train acc:  0.8671875
train loss:  0.35658013820648193
train gradient:  0.21210543168500667
iteration : 3709
train acc:  0.8046875
train loss:  0.36579805612564087
train gradient:  0.35398726686411547
iteration : 3710
train acc:  0.828125
train loss:  0.3852093815803528
train gradient:  0.34003321304710066
iteration : 3711
train acc:  0.8828125
train loss:  0.2858280539512634
train gradient:  0.20056364725703882
iteration : 3712
train acc:  0.796875
train loss:  0.43269407749176025
train gradient:  0.4375797602604236
iteration : 3713
train acc:  0.8671875
train loss:  0.34217020869255066
train gradient:  0.38655506398019623
iteration : 3714
train acc:  0.8203125
train loss:  0.4015013575553894
train gradient:  0.4294460849368479
iteration : 3715
train acc:  0.90625
train loss:  0.2896222770214081
train gradient:  0.18883559963649055
iteration : 3716
train acc:  0.7734375
train loss:  0.43696069717407227
train gradient:  0.47138453609196956
iteration : 3717
train acc:  0.859375
train loss:  0.3726308345794678
train gradient:  0.303616779636946
iteration : 3718
train acc:  0.859375
train loss:  0.3026241064071655
train gradient:  0.25681871068746004
iteration : 3719
train acc:  0.828125
train loss:  0.3666650056838989
train gradient:  0.38579143115070813
iteration : 3720
train acc:  0.875
train loss:  0.3254541754722595
train gradient:  0.23275520049330983
iteration : 3721
train acc:  0.875
train loss:  0.3284890055656433
train gradient:  0.2845817565840818
iteration : 3722
train acc:  0.8359375
train loss:  0.40823981165885925
train gradient:  0.29789275511319774
iteration : 3723
train acc:  0.828125
train loss:  0.3776389956474304
train gradient:  0.3482584207415836
iteration : 3724
train acc:  0.828125
train loss:  0.39069634675979614
train gradient:  0.334421860632952
iteration : 3725
train acc:  0.875
train loss:  0.2930199205875397
train gradient:  0.2288211283842136
iteration : 3726
train acc:  0.8984375
train loss:  0.2576392889022827
train gradient:  0.21995417475940954
iteration : 3727
train acc:  0.859375
train loss:  0.34205299615859985
train gradient:  0.2738232441421115
iteration : 3728
train acc:  0.859375
train loss:  0.3276481330394745
train gradient:  0.2819360249964294
iteration : 3729
train acc:  0.8203125
train loss:  0.35271120071411133
train gradient:  0.31832052736688365
iteration : 3730
train acc:  0.8828125
train loss:  0.29702359437942505
train gradient:  0.22578045434117355
iteration : 3731
train acc:  0.8671875
train loss:  0.3263608515262604
train gradient:  0.2801212732978669
iteration : 3732
train acc:  0.8984375
train loss:  0.2956463694572449
train gradient:  0.20777740872501885
iteration : 3733
train acc:  0.8515625
train loss:  0.29973703622817993
train gradient:  0.30516822039815017
iteration : 3734
train acc:  0.859375
train loss:  0.3280908465385437
train gradient:  0.25252494962580063
iteration : 3735
train acc:  0.796875
train loss:  0.3607395887374878
train gradient:  0.3104465743284739
iteration : 3736
train acc:  0.8125
train loss:  0.4433888792991638
train gradient:  0.39018686827436827
iteration : 3737
train acc:  0.8359375
train loss:  0.3708479404449463
train gradient:  0.28078186112086273
iteration : 3738
train acc:  0.8359375
train loss:  0.3510291278362274
train gradient:  0.265412012598838
iteration : 3739
train acc:  0.8203125
train loss:  0.369294136762619
train gradient:  0.23284569254351847
iteration : 3740
train acc:  0.8203125
train loss:  0.4361618161201477
train gradient:  0.48876504638160323
iteration : 3741
train acc:  0.796875
train loss:  0.3826652765274048
train gradient:  0.29324132584990487
iteration : 3742
train acc:  0.8359375
train loss:  0.3464554250240326
train gradient:  0.32536651523256643
iteration : 3743
train acc:  0.8203125
train loss:  0.38517051935195923
train gradient:  0.4155087508349782
iteration : 3744
train acc:  0.8671875
train loss:  0.2888646125793457
train gradient:  0.1978658120766975
iteration : 3745
train acc:  0.828125
train loss:  0.4357870817184448
train gradient:  0.36558021509746336
iteration : 3746
train acc:  0.8046875
train loss:  0.4474829435348511
train gradient:  0.4732738158396178
iteration : 3747
train acc:  0.8203125
train loss:  0.3641816973686218
train gradient:  0.2912214578385873
iteration : 3748
train acc:  0.75
train loss:  0.45301419496536255
train gradient:  0.3626461899965919
iteration : 3749
train acc:  0.859375
train loss:  0.32686781883239746
train gradient:  0.3120398832600018
iteration : 3750
train acc:  0.8203125
train loss:  0.37439876794815063
train gradient:  0.3447093834850145
iteration : 3751
train acc:  0.8203125
train loss:  0.39752206206321716
train gradient:  0.364421088970234
iteration : 3752
train acc:  0.8125
train loss:  0.3578278720378876
train gradient:  0.4036391843277045
iteration : 3753
train acc:  0.859375
train loss:  0.3099721074104309
train gradient:  0.17956443791781776
iteration : 3754
train acc:  0.84375
train loss:  0.3668861389160156
train gradient:  0.3360420542142784
iteration : 3755
train acc:  0.84375
train loss:  0.33945268392562866
train gradient:  0.2419885284627944
iteration : 3756
train acc:  0.859375
train loss:  0.31739941239356995
train gradient:  0.29773880030176725
iteration : 3757
train acc:  0.84375
train loss:  0.3601286709308624
train gradient:  0.3724036203482251
iteration : 3758
train acc:  0.8515625
train loss:  0.3723592460155487
train gradient:  0.3334459367463225
iteration : 3759
train acc:  0.8125
train loss:  0.4414025545120239
train gradient:  0.3772792704818416
iteration : 3760
train acc:  0.90625
train loss:  0.2790931761264801
train gradient:  0.22791044328628482
iteration : 3761
train acc:  0.8515625
train loss:  0.40296387672424316
train gradient:  0.4450014869808486
iteration : 3762
train acc:  0.8671875
train loss:  0.37311577796936035
train gradient:  0.38934717221714965
iteration : 3763
train acc:  0.875
train loss:  0.3195836842060089
train gradient:  0.3241972856810772
iteration : 3764
train acc:  0.875
train loss:  0.3522724509239197
train gradient:  0.21367796226070893
iteration : 3765
train acc:  0.84375
train loss:  0.4281500577926636
train gradient:  0.4428959591600672
iteration : 3766
train acc:  0.8203125
train loss:  0.3951806128025055
train gradient:  0.27840472342369754
iteration : 3767
train acc:  0.78125
train loss:  0.3874051868915558
train gradient:  0.2703416553902671
iteration : 3768
train acc:  0.8046875
train loss:  0.45425310730934143
train gradient:  0.3685901064253874
iteration : 3769
train acc:  0.828125
train loss:  0.3760567903518677
train gradient:  0.34311915821398953
iteration : 3770
train acc:  0.8359375
train loss:  0.3748278021812439
train gradient:  0.28874316810135225
iteration : 3771
train acc:  0.875
train loss:  0.30608585476875305
train gradient:  0.19361212072807651
iteration : 3772
train acc:  0.8046875
train loss:  0.3644310235977173
train gradient:  0.2698478912563637
iteration : 3773
train acc:  0.90625
train loss:  0.28822147846221924
train gradient:  0.23001161474673915
iteration : 3774
train acc:  0.9140625
train loss:  0.2653542757034302
train gradient:  0.18818470234803444
iteration : 3775
train acc:  0.8359375
train loss:  0.363925963640213
train gradient:  0.26095403576233694
iteration : 3776
train acc:  0.796875
train loss:  0.4577459692955017
train gradient:  0.3699880947854885
iteration : 3777
train acc:  0.8125
train loss:  0.3883838951587677
train gradient:  0.2548207350763465
iteration : 3778
train acc:  0.859375
train loss:  0.29539692401885986
train gradient:  0.25362781826930353
iteration : 3779
train acc:  0.8203125
train loss:  0.40368974208831787
train gradient:  0.3274405230338031
iteration : 3780
train acc:  0.859375
train loss:  0.33450961112976074
train gradient:  0.38042310844695215
iteration : 3781
train acc:  0.84375
train loss:  0.37134236097335815
train gradient:  0.24409410473171486
iteration : 3782
train acc:  0.84375
train loss:  0.2942240834236145
train gradient:  0.25388236476296305
iteration : 3783
train acc:  0.828125
train loss:  0.33846020698547363
train gradient:  0.31392083911578406
iteration : 3784
train acc:  0.8203125
train loss:  0.3940756320953369
train gradient:  0.24138519531603458
iteration : 3785
train acc:  0.8515625
train loss:  0.3525562286376953
train gradient:  0.27081484988345705
iteration : 3786
train acc:  0.8515625
train loss:  0.35346388816833496
train gradient:  0.29813103814104436
iteration : 3787
train acc:  0.90625
train loss:  0.28015202283859253
train gradient:  0.22996018572136379
iteration : 3788
train acc:  0.859375
train loss:  0.29327255487442017
train gradient:  0.27343977072156667
iteration : 3789
train acc:  0.796875
train loss:  0.4086074233055115
train gradient:  0.3655235306779206
iteration : 3790
train acc:  0.859375
train loss:  0.31029844284057617
train gradient:  0.21814987299069066
iteration : 3791
train acc:  0.8359375
train loss:  0.3916128873825073
train gradient:  0.26286261449171405
iteration : 3792
train acc:  0.8359375
train loss:  0.3745785355567932
train gradient:  0.2962225314259406
iteration : 3793
train acc:  0.9140625
train loss:  0.33250075578689575
train gradient:  0.2282215247662932
iteration : 3794
train acc:  0.875
train loss:  0.32172584533691406
train gradient:  0.1879305793933959
iteration : 3795
train acc:  0.84375
train loss:  0.390688419342041
train gradient:  0.2762390321548174
iteration : 3796
train acc:  0.828125
train loss:  0.39252933859825134
train gradient:  0.23136012668780923
iteration : 3797
train acc:  0.8359375
train loss:  0.3810030221939087
train gradient:  0.269311255446731
iteration : 3798
train acc:  0.859375
train loss:  0.31816112995147705
train gradient:  0.2603431100050967
iteration : 3799
train acc:  0.84375
train loss:  0.37760627269744873
train gradient:  0.2857520731862348
iteration : 3800
train acc:  0.8125
train loss:  0.37743139266967773
train gradient:  0.30292968603183396
iteration : 3801
train acc:  0.828125
train loss:  0.3941200375556946
train gradient:  0.3156417185911571
iteration : 3802
train acc:  0.7890625
train loss:  0.4185449182987213
train gradient:  0.31876994413747656
iteration : 3803
train acc:  0.84375
train loss:  0.32108074426651
train gradient:  0.23980445005863638
iteration : 3804
train acc:  0.8671875
train loss:  0.3306888937950134
train gradient:  0.18842794388037742
iteration : 3805
train acc:  0.8203125
train loss:  0.3867139518260956
train gradient:  0.22538797918118098
iteration : 3806
train acc:  0.8671875
train loss:  0.3038991093635559
train gradient:  0.14419729120835909
iteration : 3807
train acc:  0.796875
train loss:  0.39697885513305664
train gradient:  0.311603685742074
iteration : 3808
train acc:  0.8984375
train loss:  0.2867351770401001
train gradient:  0.1953063891715418
iteration : 3809
train acc:  0.796875
train loss:  0.46481865644454956
train gradient:  0.332826227656953
iteration : 3810
train acc:  0.8515625
train loss:  0.31542766094207764
train gradient:  0.19512938103327418
iteration : 3811
train acc:  0.8359375
train loss:  0.3272467851638794
train gradient:  0.188695945813554
iteration : 3812
train acc:  0.875
train loss:  0.36357343196868896
train gradient:  0.23182149539126984
iteration : 3813
train acc:  0.8515625
train loss:  0.3399372398853302
train gradient:  0.245879421684276
iteration : 3814
train acc:  0.8671875
train loss:  0.3604608178138733
train gradient:  0.26155724569696137
iteration : 3815
train acc:  0.875
train loss:  0.3109420835971832
train gradient:  0.2526250183145958
iteration : 3816
train acc:  0.8203125
train loss:  0.40464043617248535
train gradient:  0.39719982056757663
iteration : 3817
train acc:  0.828125
train loss:  0.40485814213752747
train gradient:  0.3046617338475179
iteration : 3818
train acc:  0.8046875
train loss:  0.42608150839805603
train gradient:  0.32440098967979475
iteration : 3819
train acc:  0.8203125
train loss:  0.33589524030685425
train gradient:  0.32669049725046356
iteration : 3820
train acc:  0.828125
train loss:  0.3954591155052185
train gradient:  0.2902425753091084
iteration : 3821
train acc:  0.859375
train loss:  0.35562068223953247
train gradient:  0.23200667217917154
iteration : 3822
train acc:  0.8515625
train loss:  0.37968355417251587
train gradient:  0.2908046404285801
iteration : 3823
train acc:  0.84375
train loss:  0.345287024974823
train gradient:  0.288806112486214
iteration : 3824
train acc:  0.859375
train loss:  0.3453361988067627
train gradient:  0.2559401060953694
iteration : 3825
train acc:  0.84375
train loss:  0.35615190863609314
train gradient:  0.2871456273096551
iteration : 3826
train acc:  0.8203125
train loss:  0.44577282667160034
train gradient:  0.27654491204829457
iteration : 3827
train acc:  0.8671875
train loss:  0.3398844599723816
train gradient:  0.26616058478861937
iteration : 3828
train acc:  0.8828125
train loss:  0.28979477286338806
train gradient:  0.23105440345624245
iteration : 3829
train acc:  0.7890625
train loss:  0.3537314534187317
train gradient:  0.2884240151276124
iteration : 3830
train acc:  0.8203125
train loss:  0.3407396674156189
train gradient:  0.3088495392384634
iteration : 3831
train acc:  0.9140625
train loss:  0.26593559980392456
train gradient:  0.1569893003877821
iteration : 3832
train acc:  0.796875
train loss:  0.394395649433136
train gradient:  0.3443873781780692
iteration : 3833
train acc:  0.8203125
train loss:  0.41419827938079834
train gradient:  0.260090115263519
iteration : 3834
train acc:  0.8671875
train loss:  0.35527026653289795
train gradient:  0.23685552938376916
iteration : 3835
train acc:  0.8671875
train loss:  0.340636283159256
train gradient:  0.3021581959340923
iteration : 3836
train acc:  0.859375
train loss:  0.4090254306793213
train gradient:  0.292259960354835
iteration : 3837
train acc:  0.8359375
train loss:  0.3792543113231659
train gradient:  0.3435019418463572
iteration : 3838
train acc:  0.8125
train loss:  0.42185425758361816
train gradient:  0.29137281664665565
iteration : 3839
train acc:  0.8671875
train loss:  0.3305777311325073
train gradient:  0.2513622675077594
iteration : 3840
train acc:  0.7734375
train loss:  0.46721136569976807
train gradient:  0.5188032661425913
iteration : 3841
train acc:  0.84375
train loss:  0.34321141242980957
train gradient:  0.20387703560690962
iteration : 3842
train acc:  0.8828125
train loss:  0.26712384819984436
train gradient:  0.25338157094390673
iteration : 3843
train acc:  0.8359375
train loss:  0.3828125298023224
train gradient:  0.29127204839336845
iteration : 3844
train acc:  0.859375
train loss:  0.33681315183639526
train gradient:  0.19933617285148114
iteration : 3845
train acc:  0.78125
train loss:  0.45669829845428467
train gradient:  0.4055409155930484
iteration : 3846
train acc:  0.890625
train loss:  0.29407286643981934
train gradient:  0.19530085776683115
iteration : 3847
train acc:  0.8203125
train loss:  0.39426884055137634
train gradient:  0.33697659508252176
iteration : 3848
train acc:  0.8359375
train loss:  0.3766501247882843
train gradient:  0.19672321063437995
iteration : 3849
train acc:  0.84375
train loss:  0.333599328994751
train gradient:  0.27826479669924553
iteration : 3850
train acc:  0.9140625
train loss:  0.2721341550350189
train gradient:  0.19567409677004421
iteration : 3851
train acc:  0.8671875
train loss:  0.3072049617767334
train gradient:  0.19218660876846166
iteration : 3852
train acc:  0.8203125
train loss:  0.38702720403671265
train gradient:  0.2727519874434369
iteration : 3853
train acc:  0.8203125
train loss:  0.39585673809051514
train gradient:  0.31861347760530145
iteration : 3854
train acc:  0.859375
train loss:  0.34453240036964417
train gradient:  0.1967182296077584
iteration : 3855
train acc:  0.859375
train loss:  0.364500492811203
train gradient:  0.356381098590428
iteration : 3856
train acc:  0.84375
train loss:  0.4248500168323517
train gradient:  0.3492531951129103
iteration : 3857
train acc:  0.7890625
train loss:  0.42856907844543457
train gradient:  0.3219620564982438
iteration : 3858
train acc:  0.8515625
train loss:  0.3247740864753723
train gradient:  0.2128657000382072
iteration : 3859
train acc:  0.828125
train loss:  0.364617258310318
train gradient:  0.2312853476714275
iteration : 3860
train acc:  0.8828125
train loss:  0.3377910852432251
train gradient:  0.2661798416300305
iteration : 3861
train acc:  0.8203125
train loss:  0.36012840270996094
train gradient:  0.3140465107083678
iteration : 3862
train acc:  0.8203125
train loss:  0.3936488628387451
train gradient:  0.2629911279455929
iteration : 3863
train acc:  0.8359375
train loss:  0.3400890827178955
train gradient:  0.21764533199534875
iteration : 3864
train acc:  0.8359375
train loss:  0.40099793672561646
train gradient:  0.2647685986488879
iteration : 3865
train acc:  0.828125
train loss:  0.33850473165512085
train gradient:  0.2725513321855904
iteration : 3866
train acc:  0.8125
train loss:  0.38133400678634644
train gradient:  0.3187289148357426
iteration : 3867
train acc:  0.8046875
train loss:  0.4502611756324768
train gradient:  0.40515117371055787
iteration : 3868
train acc:  0.8125
train loss:  0.4859852194786072
train gradient:  0.4590223564265162
iteration : 3869
train acc:  0.765625
train loss:  0.40251195430755615
train gradient:  0.29593705176834123
iteration : 3870
train acc:  0.8671875
train loss:  0.3774076998233795
train gradient:  0.30978702370235833
iteration : 3871
train acc:  0.8203125
train loss:  0.4569038450717926
train gradient:  0.42573018940276425
iteration : 3872
train acc:  0.8046875
train loss:  0.4720805883407593
train gradient:  0.38257311127716165
iteration : 3873
train acc:  0.859375
train loss:  0.2985343933105469
train gradient:  0.18470108368699945
iteration : 3874
train acc:  0.7734375
train loss:  0.39604470133781433
train gradient:  0.25677198837840537
iteration : 3875
train acc:  0.84375
train loss:  0.35005471110343933
train gradient:  0.3462899113606785
iteration : 3876
train acc:  0.7890625
train loss:  0.4500707983970642
train gradient:  0.5628342377307942
iteration : 3877
train acc:  0.7734375
train loss:  0.40601837635040283
train gradient:  0.34673245160124583
iteration : 3878
train acc:  0.859375
train loss:  0.32149916887283325
train gradient:  0.208717815529025
iteration : 3879
train acc:  0.8671875
train loss:  0.336098313331604
train gradient:  0.20109278986255716
iteration : 3880
train acc:  0.84375
train loss:  0.3966611623764038
train gradient:  0.3360334278836243
iteration : 3881
train acc:  0.8046875
train loss:  0.44460827112197876
train gradient:  0.34351049867404704
iteration : 3882
train acc:  0.78125
train loss:  0.423613965511322
train gradient:  0.2992453695089093
iteration : 3883
train acc:  0.8359375
train loss:  0.35211849212646484
train gradient:  0.24416537666360535
iteration : 3884
train acc:  0.8671875
train loss:  0.3941023349761963
train gradient:  0.26650603052501215
iteration : 3885
train acc:  0.8515625
train loss:  0.4311154782772064
train gradient:  0.22686115284481662
iteration : 3886
train acc:  0.75
train loss:  0.4678758382797241
train gradient:  0.3734926676366327
iteration : 3887
train acc:  0.8671875
train loss:  0.35235804319381714
train gradient:  0.22159963407486555
iteration : 3888
train acc:  0.8203125
train loss:  0.3533781170845032
train gradient:  0.3022353321252185
iteration : 3889
train acc:  0.859375
train loss:  0.333834171295166
train gradient:  0.20230209482789904
iteration : 3890
train acc:  0.84375
train loss:  0.37564247846603394
train gradient:  0.3116996470858275
iteration : 3891
train acc:  0.8828125
train loss:  0.32301580905914307
train gradient:  0.18664587703940924
iteration : 3892
train acc:  0.8359375
train loss:  0.3703640103340149
train gradient:  0.25436944491036406
iteration : 3893
train acc:  0.8828125
train loss:  0.328668475151062
train gradient:  0.2770719001540478
iteration : 3894
train acc:  0.84375
train loss:  0.30908912420272827
train gradient:  0.2634243749014404
iteration : 3895
train acc:  0.84375
train loss:  0.40125781297683716
train gradient:  0.22199547056054508
iteration : 3896
train acc:  0.8359375
train loss:  0.3652482032775879
train gradient:  0.2191008185306136
iteration : 3897
train acc:  0.8671875
train loss:  0.37259089946746826
train gradient:  0.21829177934365473
iteration : 3898
train acc:  0.828125
train loss:  0.3352118730545044
train gradient:  0.2229862262963333
iteration : 3899
train acc:  0.84375
train loss:  0.36570680141448975
train gradient:  0.2394729220950148
iteration : 3900
train acc:  0.8515625
train loss:  0.37437623739242554
train gradient:  0.3125592756250464
iteration : 3901
train acc:  0.8671875
train loss:  0.32977449893951416
train gradient:  0.21045799534057164
iteration : 3902
train acc:  0.8203125
train loss:  0.36505043506622314
train gradient:  0.35589747482043044
iteration : 3903
train acc:  0.8828125
train loss:  0.3151141405105591
train gradient:  0.18642263626372846
iteration : 3904
train acc:  0.8359375
train loss:  0.40096282958984375
train gradient:  0.3501465962034597
iteration : 3905
train acc:  0.890625
train loss:  0.2868066430091858
train gradient:  0.14938866284059876
iteration : 3906
train acc:  0.8515625
train loss:  0.33281442523002625
train gradient:  0.21811981881781461
iteration : 3907
train acc:  0.8125
train loss:  0.3550240397453308
train gradient:  0.23080953872680735
iteration : 3908
train acc:  0.8671875
train loss:  0.3642365634441376
train gradient:  0.21778433060420482
iteration : 3909
train acc:  0.84375
train loss:  0.32472503185272217
train gradient:  0.20718605468742374
iteration : 3910
train acc:  0.859375
train loss:  0.3958778381347656
train gradient:  0.26708088070303176
iteration : 3911
train acc:  0.8984375
train loss:  0.26279160380363464
train gradient:  0.16241104283621366
iteration : 3912
train acc:  0.8203125
train loss:  0.41396671533584595
train gradient:  0.27221264039947357
iteration : 3913
train acc:  0.8984375
train loss:  0.3719085454940796
train gradient:  0.39613097528308
iteration : 3914
train acc:  0.859375
train loss:  0.3238910734653473
train gradient:  0.24243365072711384
iteration : 3915
train acc:  0.84375
train loss:  0.3711291551589966
train gradient:  0.28868487014238714
iteration : 3916
train acc:  0.875
train loss:  0.3212563991546631
train gradient:  0.2336912510569102
iteration : 3917
train acc:  0.875
train loss:  0.28106480836868286
train gradient:  0.3017668013767539
iteration : 3918
train acc:  0.78125
train loss:  0.40007632970809937
train gradient:  0.2732706358703083
iteration : 3919
train acc:  0.796875
train loss:  0.4059085547924042
train gradient:  0.33724856270675163
iteration : 3920
train acc:  0.859375
train loss:  0.3055493235588074
train gradient:  0.3120214761689443
iteration : 3921
train acc:  0.8359375
train loss:  0.3948155641555786
train gradient:  0.24794665485657594
iteration : 3922
train acc:  0.8125
train loss:  0.3548702597618103
train gradient:  0.3029788111180833
iteration : 3923
train acc:  0.8125
train loss:  0.3516959547996521
train gradient:  0.4401122198814826
iteration : 3924
train acc:  0.8828125
train loss:  0.2890145778656006
train gradient:  0.2175485827666874
iteration : 3925
train acc:  0.828125
train loss:  0.3798508644104004
train gradient:  0.31366363751723214
iteration : 3926
train acc:  0.859375
train loss:  0.3234071135520935
train gradient:  0.2417561821959971
iteration : 3927
train acc:  0.8359375
train loss:  0.3891645669937134
train gradient:  0.37398529449291384
iteration : 3928
train acc:  0.875
train loss:  0.2974827289581299
train gradient:  0.17989777222906056
iteration : 3929
train acc:  0.828125
train loss:  0.45762020349502563
train gradient:  0.2939718358453018
iteration : 3930
train acc:  0.8828125
train loss:  0.2949281930923462
train gradient:  0.20319656472655187
iteration : 3931
train acc:  0.90625
train loss:  0.27522289752960205
train gradient:  0.1632676382265139
iteration : 3932
train acc:  0.8359375
train loss:  0.39641693234443665
train gradient:  0.41996231097823666
iteration : 3933
train acc:  0.8203125
train loss:  0.4074972867965698
train gradient:  0.3804961276425733
iteration : 3934
train acc:  0.859375
train loss:  0.3120715618133545
train gradient:  0.2535607756546438
iteration : 3935
train acc:  0.8359375
train loss:  0.35605889558792114
train gradient:  0.23276983970108725
iteration : 3936
train acc:  0.8515625
train loss:  0.35080093145370483
train gradient:  0.3160649202142164
iteration : 3937
train acc:  0.8515625
train loss:  0.3452041447162628
train gradient:  0.22805158505039494
iteration : 3938
train acc:  0.828125
train loss:  0.34741586446762085
train gradient:  0.3225526082096773
iteration : 3939
train acc:  0.8671875
train loss:  0.3265722393989563
train gradient:  0.2090536196100724
iteration : 3940
train acc:  0.859375
train loss:  0.344938188791275
train gradient:  0.21457607456085515
iteration : 3941
train acc:  0.8828125
train loss:  0.32721659541130066
train gradient:  0.17502286310893878
iteration : 3942
train acc:  0.84375
train loss:  0.37128934264183044
train gradient:  0.2642451816087815
iteration : 3943
train acc:  0.8359375
train loss:  0.37481868267059326
train gradient:  0.27085317329764863
iteration : 3944
train acc:  0.828125
train loss:  0.36641889810562134
train gradient:  0.2355886423651029
iteration : 3945
train acc:  0.796875
train loss:  0.4952515959739685
train gradient:  0.5902615704576399
iteration : 3946
train acc:  0.8515625
train loss:  0.35846763849258423
train gradient:  0.2829458163852121
iteration : 3947
train acc:  0.8984375
train loss:  0.33411628007888794
train gradient:  0.21727240880866217
iteration : 3948
train acc:  0.8359375
train loss:  0.38342535495758057
train gradient:  0.27121432715259364
iteration : 3949
train acc:  0.8515625
train loss:  0.38673675060272217
train gradient:  0.27410309038429737
iteration : 3950
train acc:  0.875
train loss:  0.2708759903907776
train gradient:  0.25120915960620693
iteration : 3951
train acc:  0.84375
train loss:  0.3769095838069916
train gradient:  0.25868195573491715
iteration : 3952
train acc:  0.8515625
train loss:  0.3571828603744507
train gradient:  0.2779504040171778
iteration : 3953
train acc:  0.875
train loss:  0.30828118324279785
train gradient:  0.2287064767954008
iteration : 3954
train acc:  0.8671875
train loss:  0.3400688171386719
train gradient:  0.22661299433249849
iteration : 3955
train acc:  0.78125
train loss:  0.46650710701942444
train gradient:  0.38926069208795727
iteration : 3956
train acc:  0.78125
train loss:  0.4815308451652527
train gradient:  0.773366316306614
iteration : 3957
train acc:  0.8359375
train loss:  0.38030552864074707
train gradient:  0.2915636502168832
iteration : 3958
train acc:  0.8125
train loss:  0.34564387798309326
train gradient:  0.22697097351114853
iteration : 3959
train acc:  0.84375
train loss:  0.3616333603858948
train gradient:  0.22859565630775275
iteration : 3960
train acc:  0.8359375
train loss:  0.34531208872795105
train gradient:  0.21597180975619246
iteration : 3961
train acc:  0.84375
train loss:  0.3414749205112457
train gradient:  0.2772096014848642
iteration : 3962
train acc:  0.828125
train loss:  0.3915613293647766
train gradient:  0.2556660005942262
iteration : 3963
train acc:  0.8125
train loss:  0.3642059862613678
train gradient:  0.31949071934557316
iteration : 3964
train acc:  0.7734375
train loss:  0.483281672000885
train gradient:  0.4865955373925444
iteration : 3965
train acc:  0.78125
train loss:  0.43484413623809814
train gradient:  0.34420235518099435
iteration : 3966
train acc:  0.8125
train loss:  0.3779833912849426
train gradient:  0.3837296385518414
iteration : 3967
train acc:  0.859375
train loss:  0.3469347655773163
train gradient:  0.22910382426907577
iteration : 3968
train acc:  0.921875
train loss:  0.27419576048851013
train gradient:  0.21252571864646203
iteration : 3969
train acc:  0.859375
train loss:  0.3280448913574219
train gradient:  0.20908793758573765
iteration : 3970
train acc:  0.859375
train loss:  0.3167869746685028
train gradient:  0.21506568751965655
iteration : 3971
train acc:  0.875
train loss:  0.27653902769088745
train gradient:  0.24495372857097986
iteration : 3972
train acc:  0.890625
train loss:  0.29294103384017944
train gradient:  0.19557220246683812
iteration : 3973
train acc:  0.8671875
train loss:  0.33943837881088257
train gradient:  0.2825322706456388
iteration : 3974
train acc:  0.8203125
train loss:  0.5298792123794556
train gradient:  0.4541220381505215
iteration : 3975
train acc:  0.890625
train loss:  0.2671499252319336
train gradient:  0.16887514589514893
iteration : 3976
train acc:  0.8359375
train loss:  0.39361560344696045
train gradient:  0.27365955537307457
iteration : 3977
train acc:  0.8125
train loss:  0.3814377784729004
train gradient:  0.2756093762324499
iteration : 3978
train acc:  0.8515625
train loss:  0.3250640034675598
train gradient:  0.21569090975601296
iteration : 3979
train acc:  0.828125
train loss:  0.3459867537021637
train gradient:  0.26010164342344777
iteration : 3980
train acc:  0.8203125
train loss:  0.3339548707008362
train gradient:  0.2789960184535302
iteration : 3981
train acc:  0.8203125
train loss:  0.41335225105285645
train gradient:  0.40064917685339974
iteration : 3982
train acc:  0.8828125
train loss:  0.3050013482570648
train gradient:  0.24124436887362954
iteration : 3983
train acc:  0.8515625
train loss:  0.3456639051437378
train gradient:  0.22008474662958827
iteration : 3984
train acc:  0.859375
train loss:  0.301404744386673
train gradient:  0.18993768112061638
iteration : 3985
train acc:  0.8046875
train loss:  0.36679959297180176
train gradient:  0.3073389835498623
iteration : 3986
train acc:  0.828125
train loss:  0.3482702970504761
train gradient:  0.22783346382267491
iteration : 3987
train acc:  0.8125
train loss:  0.3953458368778229
train gradient:  0.40800868985740185
iteration : 3988
train acc:  0.9140625
train loss:  0.3001258075237274
train gradient:  0.2647405735538947
iteration : 3989
train acc:  0.796875
train loss:  0.3961384892463684
train gradient:  0.34155408887636246
iteration : 3990
train acc:  0.828125
train loss:  0.38240090012550354
train gradient:  0.24791543959736226
iteration : 3991
train acc:  0.8359375
train loss:  0.36405402421951294
train gradient:  0.22752897455013002
iteration : 3992
train acc:  0.859375
train loss:  0.35000497102737427
train gradient:  0.23549978921274076
iteration : 3993
train acc:  0.8203125
train loss:  0.3858683407306671
train gradient:  0.28565094882421527
iteration : 3994
train acc:  0.8671875
train loss:  0.30490338802337646
train gradient:  0.257276055781712
iteration : 3995
train acc:  0.890625
train loss:  0.276908278465271
train gradient:  0.18406391069024192
iteration : 3996
train acc:  0.8515625
train loss:  0.3161390423774719
train gradient:  0.19356912393221928
iteration : 3997
train acc:  0.828125
train loss:  0.38556069135665894
train gradient:  0.38542871193084416
iteration : 3998
train acc:  0.8984375
train loss:  0.32142627239227295
train gradient:  0.30266025132983904
iteration : 3999
train acc:  0.828125
train loss:  0.3820355534553528
train gradient:  0.3045882509074363
iteration : 4000
train acc:  0.84375
train loss:  0.34441548585891724
train gradient:  0.3156805452579941
iteration : 4001
train acc:  0.875
train loss:  0.29425716400146484
train gradient:  0.2427071697280424
iteration : 4002
train acc:  0.875
train loss:  0.3259280323982239
train gradient:  0.24811643766113534
iteration : 4003
train acc:  0.796875
train loss:  0.40574151277542114
train gradient:  0.28085039389598215
iteration : 4004
train acc:  0.8046875
train loss:  0.4070911407470703
train gradient:  0.25212512634060336
iteration : 4005
train acc:  0.875
train loss:  0.30435711145401
train gradient:  0.28278200733361225
iteration : 4006
train acc:  0.8046875
train loss:  0.4186643362045288
train gradient:  0.37297257388868377
iteration : 4007
train acc:  0.8359375
train loss:  0.3922823667526245
train gradient:  0.2845279720588968
iteration : 4008
train acc:  0.8671875
train loss:  0.3642982244491577
train gradient:  0.2776751321869717
iteration : 4009
train acc:  0.8515625
train loss:  0.3471250534057617
train gradient:  0.3755763003409035
iteration : 4010
train acc:  0.828125
train loss:  0.3662126958370209
train gradient:  0.3624070990081539
iteration : 4011
train acc:  0.8671875
train loss:  0.33830389380455017
train gradient:  0.24497400740357061
iteration : 4012
train acc:  0.8515625
train loss:  0.285291850566864
train gradient:  0.1977823334991368
iteration : 4013
train acc:  0.8515625
train loss:  0.3760688006877899
train gradient:  0.24561002711682126
iteration : 4014
train acc:  0.8828125
train loss:  0.28563013672828674
train gradient:  0.18434428477596626
iteration : 4015
train acc:  0.8671875
train loss:  0.3373876214027405
train gradient:  0.25302104176953316
iteration : 4016
train acc:  0.859375
train loss:  0.3270758092403412
train gradient:  0.1797677788388847
iteration : 4017
train acc:  0.7890625
train loss:  0.423091322183609
train gradient:  0.41160468755525176
iteration : 4018
train acc:  0.84375
train loss:  0.39143258333206177
train gradient:  0.23766359329654885
iteration : 4019
train acc:  0.84375
train loss:  0.31582075357437134
train gradient:  0.2331248096374237
iteration : 4020
train acc:  0.8359375
train loss:  0.37368959188461304
train gradient:  0.25026102346407864
iteration : 4021
train acc:  0.78125
train loss:  0.415404349565506
train gradient:  0.36736516397443814
iteration : 4022
train acc:  0.8671875
train loss:  0.3087741732597351
train gradient:  0.17578315190986238
iteration : 4023
train acc:  0.8515625
train loss:  0.3148862421512604
train gradient:  0.28030738652614845
iteration : 4024
train acc:  0.8125
train loss:  0.4050380289554596
train gradient:  0.3203691745697063
iteration : 4025
train acc:  0.90625
train loss:  0.31274402141571045
train gradient:  0.5326079605256735
iteration : 4026
train acc:  0.7890625
train loss:  0.4245244562625885
train gradient:  0.48081837577735015
iteration : 4027
train acc:  0.8515625
train loss:  0.36568379402160645
train gradient:  0.2900182355155099
iteration : 4028
train acc:  0.828125
train loss:  0.49388259649276733
train gradient:  0.3894137062960412
iteration : 4029
train acc:  0.875
train loss:  0.3828851580619812
train gradient:  0.32655009123998424
iteration : 4030
train acc:  0.8515625
train loss:  0.3209414482116699
train gradient:  0.2721968366004118
iteration : 4031
train acc:  0.84375
train loss:  0.33938509225845337
train gradient:  0.30401325755206793
iteration : 4032
train acc:  0.84375
train loss:  0.36565572023391724
train gradient:  0.25536200971652606
iteration : 4033
train acc:  0.8046875
train loss:  0.4289230406284332
train gradient:  0.31483540737224225
iteration : 4034
train acc:  0.8203125
train loss:  0.4029942750930786
train gradient:  0.3509489520889265
iteration : 4035
train acc:  0.84375
train loss:  0.3252016305923462
train gradient:  0.17893673939079596
iteration : 4036
train acc:  0.84375
train loss:  0.3838382065296173
train gradient:  0.312883933853778
iteration : 4037
train acc:  0.875
train loss:  0.3012029826641083
train gradient:  0.21969431450994628
iteration : 4038
train acc:  0.8828125
train loss:  0.3178297281265259
train gradient:  0.19173705111630499
iteration : 4039
train acc:  0.8359375
train loss:  0.385860800743103
train gradient:  0.28746256967302003
iteration : 4040
train acc:  0.78125
train loss:  0.47616046667099
train gradient:  0.4153368306485627
iteration : 4041
train acc:  0.8359375
train loss:  0.4061221480369568
train gradient:  0.2939499003059933
iteration : 4042
train acc:  0.7734375
train loss:  0.5264056921005249
train gradient:  0.6650473073218499
iteration : 4043
train acc:  0.84375
train loss:  0.3872132897377014
train gradient:  0.3506432078790059
iteration : 4044
train acc:  0.8125
train loss:  0.42566561698913574
train gradient:  0.26111451168924477
iteration : 4045
train acc:  0.84375
train loss:  0.3410412073135376
train gradient:  0.24989634740001837
iteration : 4046
train acc:  0.8203125
train loss:  0.37084606289863586
train gradient:  0.23246782949080347
iteration : 4047
train acc:  0.8359375
train loss:  0.3870551288127899
train gradient:  0.29802985547067845
iteration : 4048
train acc:  0.8515625
train loss:  0.37507739663124084
train gradient:  0.3152228770187292
iteration : 4049
train acc:  0.8203125
train loss:  0.3466166853904724
train gradient:  0.19574419315413955
iteration : 4050
train acc:  0.7578125
train loss:  0.4784691631793976
train gradient:  0.4027603264663872
iteration : 4051
train acc:  0.8671875
train loss:  0.34068578481674194
train gradient:  0.19346757023638217
iteration : 4052
train acc:  0.9140625
train loss:  0.27272409200668335
train gradient:  0.14028778896834201
iteration : 4053
train acc:  0.890625
train loss:  0.27591267228126526
train gradient:  0.20539734890626965
iteration : 4054
train acc:  0.828125
train loss:  0.3695142865180969
train gradient:  0.2565418582512611
iteration : 4055
train acc:  0.7734375
train loss:  0.429223895072937
train gradient:  0.39766877193067157
iteration : 4056
train acc:  0.859375
train loss:  0.2890358865261078
train gradient:  0.2061535545978908
iteration : 4057
train acc:  0.7578125
train loss:  0.4943072497844696
train gradient:  0.4341211234905174
iteration : 4058
train acc:  0.8515625
train loss:  0.3415689468383789
train gradient:  0.28331507121205246
iteration : 4059
train acc:  0.8515625
train loss:  0.32309097051620483
train gradient:  0.2526983344527842
iteration : 4060
train acc:  0.796875
train loss:  0.3952042758464813
train gradient:  0.22699484991119678
iteration : 4061
train acc:  0.7890625
train loss:  0.47804224491119385
train gradient:  0.35751366181192695
iteration : 4062
train acc:  0.828125
train loss:  0.37291669845581055
train gradient:  0.279841737938326
iteration : 4063
train acc:  0.859375
train loss:  0.34526538848876953
train gradient:  0.25019501733548694
iteration : 4064
train acc:  0.84375
train loss:  0.3754608631134033
train gradient:  0.22644198338857824
iteration : 4065
train acc:  0.7734375
train loss:  0.4386306703090668
train gradient:  0.3973567495392271
iteration : 4066
train acc:  0.8046875
train loss:  0.3664044737815857
train gradient:  0.23513279142337895
iteration : 4067
train acc:  0.8515625
train loss:  0.34996941685676575
train gradient:  0.21819293170539938
iteration : 4068
train acc:  0.8359375
train loss:  0.37858110666275024
train gradient:  0.24407369182001293
iteration : 4069
train acc:  0.859375
train loss:  0.2954612374305725
train gradient:  0.18131149513872885
iteration : 4070
train acc:  0.859375
train loss:  0.31889456510543823
train gradient:  0.18422376693353826
iteration : 4071
train acc:  0.78125
train loss:  0.4404500722885132
train gradient:  0.33892765070422665
iteration : 4072
train acc:  0.9296875
train loss:  0.2582496404647827
train gradient:  0.14615131799818074
iteration : 4073
train acc:  0.859375
train loss:  0.3858802318572998
train gradient:  0.27656457886883923
iteration : 4074
train acc:  0.8046875
train loss:  0.4451080560684204
train gradient:  0.37631011486119614
iteration : 4075
train acc:  0.8359375
train loss:  0.3686624765396118
train gradient:  0.3351366413054578
iteration : 4076
train acc:  0.90625
train loss:  0.2943831980228424
train gradient:  0.15514871853313286
iteration : 4077
train acc:  0.8203125
train loss:  0.40644681453704834
train gradient:  0.35082706527672913
iteration : 4078
train acc:  0.8046875
train loss:  0.3935093283653259
train gradient:  0.31775762089954485
iteration : 4079
train acc:  0.84375
train loss:  0.42718854546546936
train gradient:  0.36681442030016415
iteration : 4080
train acc:  0.8515625
train loss:  0.3421308696269989
train gradient:  0.242590213318273
iteration : 4081
train acc:  0.8359375
train loss:  0.36092251539230347
train gradient:  0.3166407431786269
iteration : 4082
train acc:  0.859375
train loss:  0.33505547046661377
train gradient:  0.25738660046419193
iteration : 4083
train acc:  0.8515625
train loss:  0.34351736307144165
train gradient:  0.18196186236618034
iteration : 4084
train acc:  0.890625
train loss:  0.2887864112854004
train gradient:  0.170162570384121
iteration : 4085
train acc:  0.8046875
train loss:  0.40594249963760376
train gradient:  0.3164062791609314
iteration : 4086
train acc:  0.875
train loss:  0.3127766251564026
train gradient:  0.25720984605383174
iteration : 4087
train acc:  0.8203125
train loss:  0.4023496210575104
train gradient:  0.2411732560366875
iteration : 4088
train acc:  0.84375
train loss:  0.34098535776138306
train gradient:  0.4211755526671895
iteration : 4089
train acc:  0.8515625
train loss:  0.3532086908817291
train gradient:  0.251902271363323
iteration : 4090
train acc:  0.8515625
train loss:  0.3436073064804077
train gradient:  0.17566360050814206
iteration : 4091
train acc:  0.8203125
train loss:  0.38483577966690063
train gradient:  0.400477843051593
iteration : 4092
train acc:  0.859375
train loss:  0.29088717699050903
train gradient:  0.18178480970400618
iteration : 4093
train acc:  0.828125
train loss:  0.3639678359031677
train gradient:  0.25807329108290294
iteration : 4094
train acc:  0.859375
train loss:  0.33717772364616394
train gradient:  0.3343151097436713
iteration : 4095
train acc:  0.828125
train loss:  0.38076603412628174
train gradient:  0.2805182546028771
iteration : 4096
train acc:  0.7265625
train loss:  0.5035011768341064
train gradient:  0.3834076100372682
iteration : 4097
train acc:  0.84375
train loss:  0.3663119971752167
train gradient:  0.24005046248075165
iteration : 4098
train acc:  0.796875
train loss:  0.4134696125984192
train gradient:  0.2813683553116848
iteration : 4099
train acc:  0.8828125
train loss:  0.2739298939704895
train gradient:  0.2194213819401977
iteration : 4100
train acc:  0.8828125
train loss:  0.31436654925346375
train gradient:  0.14378283104537523
iteration : 4101
train acc:  0.8203125
train loss:  0.36266249418258667
train gradient:  0.20041532538392445
iteration : 4102
train acc:  0.875
train loss:  0.3315832018852234
train gradient:  0.2784694637074532
iteration : 4103
train acc:  0.828125
train loss:  0.400359570980072
train gradient:  0.26622837540115746
iteration : 4104
train acc:  0.8828125
train loss:  0.3736717104911804
train gradient:  0.26902893851924015
iteration : 4105
train acc:  0.8203125
train loss:  0.35782554745674133
train gradient:  0.17132357176644608
iteration : 4106
train acc:  0.8046875
train loss:  0.45752230286598206
train gradient:  0.3930533088676907
iteration : 4107
train acc:  0.8046875
train loss:  0.42271244525909424
train gradient:  0.30429235006284194
iteration : 4108
train acc:  0.8359375
train loss:  0.3272877335548401
train gradient:  0.2121152222353293
iteration : 4109
train acc:  0.8515625
train loss:  0.32971787452697754
train gradient:  0.23593008918270678
iteration : 4110
train acc:  0.7890625
train loss:  0.4251890182495117
train gradient:  0.28622509220276193
iteration : 4111
train acc:  0.8125
train loss:  0.3906569480895996
train gradient:  0.29973030318637034
iteration : 4112
train acc:  0.8515625
train loss:  0.37209799885749817
train gradient:  0.31659940734842756
iteration : 4113
train acc:  0.7734375
train loss:  0.40853995084762573
train gradient:  0.33990806474230756
iteration : 4114
train acc:  0.8671875
train loss:  0.3099827468395233
train gradient:  0.19250836774811453
iteration : 4115
train acc:  0.84375
train loss:  0.3369517922401428
train gradient:  0.23478159741591087
iteration : 4116
train acc:  0.8828125
train loss:  0.30566340684890747
train gradient:  0.24629732582109706
iteration : 4117
train acc:  0.8203125
train loss:  0.4300891160964966
train gradient:  0.29058902519508806
iteration : 4118
train acc:  0.8515625
train loss:  0.357166051864624
train gradient:  0.3158872336187009
iteration : 4119
train acc:  0.859375
train loss:  0.35226649045944214
train gradient:  0.2930158142222163
iteration : 4120
train acc:  0.859375
train loss:  0.31450003385543823
train gradient:  0.23235848796274844
iteration : 4121
train acc:  0.859375
train loss:  0.302529513835907
train gradient:  0.26268810063075815
iteration : 4122
train acc:  0.828125
train loss:  0.3529670238494873
train gradient:  0.18432233724269176
iteration : 4123
train acc:  0.875
train loss:  0.3070213496685028
train gradient:  0.12160302432705943
iteration : 4124
train acc:  0.8203125
train loss:  0.4070640504360199
train gradient:  0.3465180648008143
iteration : 4125
train acc:  0.8515625
train loss:  0.3466643691062927
train gradient:  0.20033987649679158
iteration : 4126
train acc:  0.8046875
train loss:  0.38095593452453613
train gradient:  0.29124033093937374
iteration : 4127
train acc:  0.8046875
train loss:  0.4969099462032318
train gradient:  0.519857009674524
iteration : 4128
train acc:  0.8125
train loss:  0.3603692054748535
train gradient:  0.29886858351302553
iteration : 4129
train acc:  0.859375
train loss:  0.38172972202301025
train gradient:  0.23530418398816585
iteration : 4130
train acc:  0.84375
train loss:  0.3448996841907501
train gradient:  0.24959178125818532
iteration : 4131
train acc:  0.859375
train loss:  0.42983490228652954
train gradient:  0.4239227968735487
iteration : 4132
train acc:  0.765625
train loss:  0.44592946767807007
train gradient:  0.40750731178402333
iteration : 4133
train acc:  0.8046875
train loss:  0.44105422496795654
train gradient:  0.43623224270870414
iteration : 4134
train acc:  0.8359375
train loss:  0.3793502748012543
train gradient:  0.22533399557155015
iteration : 4135
train acc:  0.8359375
train loss:  0.4112989902496338
train gradient:  0.3405071319384084
iteration : 4136
train acc:  0.9140625
train loss:  0.28892749547958374
train gradient:  0.1798082348130432
iteration : 4137
train acc:  0.765625
train loss:  0.4631653130054474
train gradient:  0.45730511902846016
iteration : 4138
train acc:  0.8359375
train loss:  0.41179633140563965
train gradient:  0.3028044352166007
iteration : 4139
train acc:  0.828125
train loss:  0.36429256200790405
train gradient:  0.24129150138396646
iteration : 4140
train acc:  0.828125
train loss:  0.3468198776245117
train gradient:  0.24391607645648175
iteration : 4141
train acc:  0.8359375
train loss:  0.40954768657684326
train gradient:  0.3301814039049798
iteration : 4142
train acc:  0.859375
train loss:  0.33101582527160645
train gradient:  0.33730456273697795
iteration : 4143
train acc:  0.8671875
train loss:  0.3492963910102844
train gradient:  0.29792602155937153
iteration : 4144
train acc:  0.8515625
train loss:  0.3476066589355469
train gradient:  0.295377397093412
iteration : 4145
train acc:  0.8359375
train loss:  0.33021923899650574
train gradient:  0.1757755648337484
iteration : 4146
train acc:  0.8203125
train loss:  0.3956618905067444
train gradient:  0.3931377300382541
iteration : 4147
train acc:  0.828125
train loss:  0.3476933240890503
train gradient:  0.2824009674690855
iteration : 4148
train acc:  0.8515625
train loss:  0.32422298192977905
train gradient:  0.16549717744004472
iteration : 4149
train acc:  0.828125
train loss:  0.36823570728302
train gradient:  0.3881852997612632
iteration : 4150
train acc:  0.8671875
train loss:  0.41097721457481384
train gradient:  0.32097273755294214
iteration : 4151
train acc:  0.859375
train loss:  0.3210958242416382
train gradient:  0.16719093637967952
iteration : 4152
train acc:  0.8125
train loss:  0.39716222882270813
train gradient:  0.2852884712881197
iteration : 4153
train acc:  0.890625
train loss:  0.26258140802383423
train gradient:  0.1543260449704925
iteration : 4154
train acc:  0.828125
train loss:  0.32973283529281616
train gradient:  0.21843335619703055
iteration : 4155
train acc:  0.875
train loss:  0.3304504156112671
train gradient:  0.25450714509155387
iteration : 4156
train acc:  0.8359375
train loss:  0.3451632261276245
train gradient:  0.16627194568806944
iteration : 4157
train acc:  0.890625
train loss:  0.3239991366863251
train gradient:  0.24120966663706223
iteration : 4158
train acc:  0.8984375
train loss:  0.2953600287437439
train gradient:  0.33648490620779115
iteration : 4159
train acc:  0.8828125
train loss:  0.28251612186431885
train gradient:  0.22806611016336692
iteration : 4160
train acc:  0.78125
train loss:  0.39644601941108704
train gradient:  0.26127618645380535
iteration : 4161
train acc:  0.8359375
train loss:  0.3681003153324127
train gradient:  0.20623813100945
iteration : 4162
train acc:  0.8828125
train loss:  0.28392893075942993
train gradient:  0.23814054729748285
iteration : 4163
train acc:  0.7890625
train loss:  0.4598793387413025
train gradient:  0.3383719372036918
iteration : 4164
train acc:  0.8125
train loss:  0.3564797639846802
train gradient:  0.272006114997995
iteration : 4165
train acc:  0.84375
train loss:  0.3350781798362732
train gradient:  0.2284922944376291
iteration : 4166
train acc:  0.859375
train loss:  0.33371102809906006
train gradient:  0.4037010326653494
iteration : 4167
train acc:  0.84375
train loss:  0.339915931224823
train gradient:  0.28455530952504593
iteration : 4168
train acc:  0.8515625
train loss:  0.353578120470047
train gradient:  0.2675125374685209
iteration : 4169
train acc:  0.828125
train loss:  0.37849920988082886
train gradient:  0.2793684148580599
iteration : 4170
train acc:  0.8515625
train loss:  0.417286217212677
train gradient:  0.2676967977998942
iteration : 4171
train acc:  0.890625
train loss:  0.34119635820388794
train gradient:  0.1743415659743611
iteration : 4172
train acc:  0.8359375
train loss:  0.4284195005893707
train gradient:  0.3437549826333174
iteration : 4173
train acc:  0.796875
train loss:  0.38898688554763794
train gradient:  0.27021272415167635
iteration : 4174
train acc:  0.859375
train loss:  0.32811829447746277
train gradient:  0.3341582495946856
iteration : 4175
train acc:  0.8984375
train loss:  0.30090785026550293
train gradient:  0.2096565947272775
iteration : 4176
train acc:  0.8359375
train loss:  0.35318464040756226
train gradient:  0.3055557907735526
iteration : 4177
train acc:  0.90625
train loss:  0.294988751411438
train gradient:  0.20177862349839767
iteration : 4178
train acc:  0.8203125
train loss:  0.3776231110095978
train gradient:  0.3356263039468815
iteration : 4179
train acc:  0.8671875
train loss:  0.35769200325012207
train gradient:  0.24345716904107298
iteration : 4180
train acc:  0.84375
train loss:  0.35411572456359863
train gradient:  0.2695805400685312
iteration : 4181
train acc:  0.7890625
train loss:  0.41026100516319275
train gradient:  0.47135311916360845
iteration : 4182
train acc:  0.7890625
train loss:  0.48592692613601685
train gradient:  0.389893494200078
iteration : 4183
train acc:  0.875
train loss:  0.32049912214279175
train gradient:  0.20746754778974058
iteration : 4184
train acc:  0.828125
train loss:  0.4106527864933014
train gradient:  0.4019573198340267
iteration : 4185
train acc:  0.875
train loss:  0.28224843740463257
train gradient:  0.17166952559020604
iteration : 4186
train acc:  0.84375
train loss:  0.3148844242095947
train gradient:  0.16687748853935702
iteration : 4187
train acc:  0.890625
train loss:  0.2596839368343353
train gradient:  0.20953399754456736
iteration : 4188
train acc:  0.8359375
train loss:  0.3459121584892273
train gradient:  0.2297355314264366
iteration : 4189
train acc:  0.8359375
train loss:  0.39534780383110046
train gradient:  0.24784772158790466
iteration : 4190
train acc:  0.796875
train loss:  0.39137154817581177
train gradient:  0.3233048281989617
iteration : 4191
train acc:  0.859375
train loss:  0.35038942098617554
train gradient:  0.19923646591005503
iteration : 4192
train acc:  0.8125
train loss:  0.451130747795105
train gradient:  0.4783565467390262
iteration : 4193
train acc:  0.8046875
train loss:  0.3807426393032074
train gradient:  0.3203352026199339
iteration : 4194
train acc:  0.8515625
train loss:  0.3574753999710083
train gradient:  0.2540036702562444
iteration : 4195
train acc:  0.859375
train loss:  0.35505878925323486
train gradient:  0.3298023198413394
iteration : 4196
train acc:  0.8125
train loss:  0.3604711890220642
train gradient:  0.22968607601784863
iteration : 4197
train acc:  0.8828125
train loss:  0.2821212708950043
train gradient:  0.1995964743013627
iteration : 4198
train acc:  0.828125
train loss:  0.3854817748069763
train gradient:  0.3305590575620401
iteration : 4199
train acc:  0.8359375
train loss:  0.3495836853981018
train gradient:  0.27464413008075295
iteration : 4200
train acc:  0.828125
train loss:  0.3834236264228821
train gradient:  0.39684884657086983
iteration : 4201
train acc:  0.8828125
train loss:  0.3317860960960388
train gradient:  0.25444173226881434
iteration : 4202
train acc:  0.8359375
train loss:  0.3798156976699829
train gradient:  0.22853371808918088
iteration : 4203
train acc:  0.8828125
train loss:  0.2737899720668793
train gradient:  0.26100613048592225
iteration : 4204
train acc:  0.84375
train loss:  0.340007483959198
train gradient:  0.29322615836121735
iteration : 4205
train acc:  0.7578125
train loss:  0.4736509621143341
train gradient:  0.4663544971957421
iteration : 4206
train acc:  0.8984375
train loss:  0.36648523807525635
train gradient:  0.2445418104506364
iteration : 4207
train acc:  0.8515625
train loss:  0.41463541984558105
train gradient:  0.29916100210203334
iteration : 4208
train acc:  0.84375
train loss:  0.3554648458957672
train gradient:  0.25152945827552425
iteration : 4209
train acc:  0.7890625
train loss:  0.40630465745925903
train gradient:  0.3025648896687525
iteration : 4210
train acc:  0.8125
train loss:  0.46301600337028503
train gradient:  0.3391169848903401
iteration : 4211
train acc:  0.8359375
train loss:  0.39818161725997925
train gradient:  0.3273356941062317
iteration : 4212
train acc:  0.8359375
train loss:  0.41534683108329773
train gradient:  0.32516198270707775
iteration : 4213
train acc:  0.828125
train loss:  0.4137917160987854
train gradient:  0.40336645387518577
iteration : 4214
train acc:  0.8671875
train loss:  0.3340581953525543
train gradient:  0.2803736534622768
iteration : 4215
train acc:  0.84375
train loss:  0.32415127754211426
train gradient:  0.2558514742112624
iteration : 4216
train acc:  0.859375
train loss:  0.3625803589820862
train gradient:  0.32281324281857493
iteration : 4217
train acc:  0.8828125
train loss:  0.27852964401245117
train gradient:  0.18256243873465444
iteration : 4218
train acc:  0.875
train loss:  0.26986175775527954
train gradient:  0.14576194287435323
iteration : 4219
train acc:  0.8515625
train loss:  0.38947781920433044
train gradient:  0.31099065097103046
iteration : 4220
train acc:  0.859375
train loss:  0.3474658131599426
train gradient:  0.22277217364962223
iteration : 4221
train acc:  0.8828125
train loss:  0.2732604742050171
train gradient:  0.19658905307472127
iteration : 4222
train acc:  0.8671875
train loss:  0.2999277710914612
train gradient:  0.18343640671577843
iteration : 4223
train acc:  0.84375
train loss:  0.37714308500289917
train gradient:  0.26906397232989415
iteration : 4224
train acc:  0.8125
train loss:  0.3664732873439789
train gradient:  0.1926197178509025
iteration : 4225
train acc:  0.859375
train loss:  0.3223475217819214
train gradient:  0.2112207021364887
iteration : 4226
train acc:  0.859375
train loss:  0.37085896730422974
train gradient:  0.23562084194063715
iteration : 4227
train acc:  0.8046875
train loss:  0.44285398721694946
train gradient:  0.30916167138947304
iteration : 4228
train acc:  0.8359375
train loss:  0.38647401332855225
train gradient:  0.2514446005894252
iteration : 4229
train acc:  0.8671875
train loss:  0.34077325463294983
train gradient:  0.35740686573734415
iteration : 4230
train acc:  0.8125
train loss:  0.4258209466934204
train gradient:  0.39294926417731724
iteration : 4231
train acc:  0.8046875
train loss:  0.3770805895328522
train gradient:  0.23816503056828078
iteration : 4232
train acc:  0.859375
train loss:  0.382611483335495
train gradient:  0.3711542669100987
iteration : 4233
train acc:  0.796875
train loss:  0.4439277648925781
train gradient:  0.2917441881467714
iteration : 4234
train acc:  0.8203125
train loss:  0.4449411928653717
train gradient:  0.4784137347423098
iteration : 4235
train acc:  0.8515625
train loss:  0.4010617136955261
train gradient:  0.3114008343177769
iteration : 4236
train acc:  0.8828125
train loss:  0.28216370940208435
train gradient:  0.1244502711469482
iteration : 4237
train acc:  0.8671875
train loss:  0.3360556960105896
train gradient:  0.15806210691174705
iteration : 4238
train acc:  0.84375
train loss:  0.3644384741783142
train gradient:  0.21762228140522125
iteration : 4239
train acc:  0.859375
train loss:  0.3635830283164978
train gradient:  0.2480989421647293
iteration : 4240
train acc:  0.859375
train loss:  0.2831640839576721
train gradient:  0.24701428565342293
iteration : 4241
train acc:  0.8046875
train loss:  0.42011934518814087
train gradient:  0.2968587230778691
iteration : 4242
train acc:  0.7890625
train loss:  0.45300066471099854
train gradient:  0.3567160576494009
iteration : 4243
train acc:  0.859375
train loss:  0.3424072563648224
train gradient:  0.25718431901098665
iteration : 4244
train acc:  0.8671875
train loss:  0.2825580835342407
train gradient:  0.12223523548332392
iteration : 4245
train acc:  0.8671875
train loss:  0.3658355474472046
train gradient:  0.2251103796814916
iteration : 4246
train acc:  0.8828125
train loss:  0.31668657064437866
train gradient:  0.17507729450368417
iteration : 4247
train acc:  0.84375
train loss:  0.3407779932022095
train gradient:  0.2928288022914489
iteration : 4248
train acc:  0.890625
train loss:  0.30958977341651917
train gradient:  0.25335691243318115
iteration : 4249
train acc:  0.859375
train loss:  0.36365723609924316
train gradient:  0.2230275632742797
iteration : 4250
train acc:  0.875
train loss:  0.2936658263206482
train gradient:  0.18245440356633158
iteration : 4251
train acc:  0.8203125
train loss:  0.348821222782135
train gradient:  0.22671716940824782
iteration : 4252
train acc:  0.8125
train loss:  0.3888828754425049
train gradient:  0.35151383470931596
iteration : 4253
train acc:  0.8359375
train loss:  0.36090606451034546
train gradient:  0.32232905814290563
iteration : 4254
train acc:  0.828125
train loss:  0.37566879391670227
train gradient:  0.29355705871871945
iteration : 4255
train acc:  0.8359375
train loss:  0.3677195906639099
train gradient:  0.2581351018594064
iteration : 4256
train acc:  0.890625
train loss:  0.3039999008178711
train gradient:  0.1910765269911184
iteration : 4257
train acc:  0.890625
train loss:  0.30760523676872253
train gradient:  0.13176258033617277
iteration : 4258
train acc:  0.84375
train loss:  0.33182093501091003
train gradient:  0.2519872708911856
iteration : 4259
train acc:  0.828125
train loss:  0.3541487455368042
train gradient:  0.2727169440881796
iteration : 4260
train acc:  0.8125
train loss:  0.37633025646209717
train gradient:  0.2723126154006874
iteration : 4261
train acc:  0.8125
train loss:  0.41202160716056824
train gradient:  0.344581325505012
iteration : 4262
train acc:  0.84375
train loss:  0.4205371141433716
train gradient:  0.3472993635711949
iteration : 4263
train acc:  0.796875
train loss:  0.3546096086502075
train gradient:  0.2188559293317087
iteration : 4264
train acc:  0.84375
train loss:  0.3990600109100342
train gradient:  0.21270329742994368
iteration : 4265
train acc:  0.8671875
train loss:  0.34501999616622925
train gradient:  0.23250856073318973
iteration : 4266
train acc:  0.875
train loss:  0.322151780128479
train gradient:  0.2572053223717485
iteration : 4267
train acc:  0.90625
train loss:  0.3034604787826538
train gradient:  0.19857108461166334
iteration : 4268
train acc:  0.828125
train loss:  0.3701024353504181
train gradient:  0.41529512797155566
iteration : 4269
train acc:  0.8203125
train loss:  0.47977468371391296
train gradient:  0.3650158657805087
iteration : 4270
train acc:  0.8125
train loss:  0.45252707600593567
train gradient:  0.42673132023628374
iteration : 4271
train acc:  0.84375
train loss:  0.3893865942955017
train gradient:  0.2559631359722615
iteration : 4272
train acc:  0.7734375
train loss:  0.4011588394641876
train gradient:  0.3747227895847224
iteration : 4273
train acc:  0.8203125
train loss:  0.3734017014503479
train gradient:  0.37485114666704344
iteration : 4274
train acc:  0.8203125
train loss:  0.41844189167022705
train gradient:  0.5756870107501348
iteration : 4275
train acc:  0.890625
train loss:  0.27342116832733154
train gradient:  0.21414809654323036
iteration : 4276
train acc:  0.8203125
train loss:  0.36606043577194214
train gradient:  0.2925291983497567
iteration : 4277
train acc:  0.8828125
train loss:  0.2884279489517212
train gradient:  0.18338494372042072
iteration : 4278
train acc:  0.8203125
train loss:  0.3980639576911926
train gradient:  0.28658968849668526
iteration : 4279
train acc:  0.8515625
train loss:  0.3070117235183716
train gradient:  0.23266796884459212
iteration : 4280
train acc:  0.859375
train loss:  0.3523399531841278
train gradient:  0.236883889849812
iteration : 4281
train acc:  0.9140625
train loss:  0.25324133038520813
train gradient:  0.16705857418008777
iteration : 4282
train acc:  0.8515625
train loss:  0.35756444931030273
train gradient:  0.28792706903578064
iteration : 4283
train acc:  0.859375
train loss:  0.3534517288208008
train gradient:  0.26014665022912625
iteration : 4284
train acc:  0.796875
train loss:  0.4123457372188568
train gradient:  0.4137775090473712
iteration : 4285
train acc:  0.8828125
train loss:  0.2744980454444885
train gradient:  0.21987950652942434
iteration : 4286
train acc:  0.8125
train loss:  0.346487432718277
train gradient:  0.23664106529216827
iteration : 4287
train acc:  0.8046875
train loss:  0.3348996043205261
train gradient:  0.2941455482450419
iteration : 4288
train acc:  0.8671875
train loss:  0.32369157671928406
train gradient:  0.2148398653084461
iteration : 4289
train acc:  0.8828125
train loss:  0.2972579002380371
train gradient:  0.24054425537991403
iteration : 4290
train acc:  0.8203125
train loss:  0.38565731048583984
train gradient:  0.2758390851919934
iteration : 4291
train acc:  0.8515625
train loss:  0.30683809518814087
train gradient:  0.18433025167486072
iteration : 4292
train acc:  0.8203125
train loss:  0.45131629705429077
train gradient:  0.37301023482848744
iteration : 4293
train acc:  0.8125
train loss:  0.3454602360725403
train gradient:  0.3824981377678387
iteration : 4294
train acc:  0.859375
train loss:  0.33796995878219604
train gradient:  0.23686941612924062
iteration : 4295
train acc:  0.890625
train loss:  0.30979204177856445
train gradient:  0.1513748414417854
iteration : 4296
train acc:  0.8203125
train loss:  0.3697364926338196
train gradient:  0.24762969002092475
iteration : 4297
train acc:  0.875
train loss:  0.3006609082221985
train gradient:  0.15201072302226895
iteration : 4298
train acc:  0.8046875
train loss:  0.43527281284332275
train gradient:  0.3667076187800796
iteration : 4299
train acc:  0.7890625
train loss:  0.4235708713531494
train gradient:  0.4006765851054646
iteration : 4300
train acc:  0.8203125
train loss:  0.36868658661842346
train gradient:  0.2700720539397457
iteration : 4301
train acc:  0.859375
train loss:  0.3243389427661896
train gradient:  0.2939838591266453
iteration : 4302
train acc:  0.90625
train loss:  0.2698478698730469
train gradient:  0.17827036096384297
iteration : 4303
train acc:  0.890625
train loss:  0.30050426721572876
train gradient:  0.2838855768689641
iteration : 4304
train acc:  0.921875
train loss:  0.23995918035507202
train gradient:  0.1507336288063475
iteration : 4305
train acc:  0.7734375
train loss:  0.4613024592399597
train gradient:  0.40716171361007863
iteration : 4306
train acc:  0.8671875
train loss:  0.28911861777305603
train gradient:  0.21572222530001633
iteration : 4307
train acc:  0.8359375
train loss:  0.3362070620059967
train gradient:  0.2851925691543638
iteration : 4308
train acc:  0.8203125
train loss:  0.3618590831756592
train gradient:  0.30073708485406275
iteration : 4309
train acc:  0.78125
train loss:  0.42584291100502014
train gradient:  0.4162832666950016
iteration : 4310
train acc:  0.8359375
train loss:  0.3524823784828186
train gradient:  0.3234247728033754
iteration : 4311
train acc:  0.8046875
train loss:  0.37106671929359436
train gradient:  0.23676919542612873
iteration : 4312
train acc:  0.8671875
train loss:  0.3610435724258423
train gradient:  0.34420966048178947
iteration : 4313
train acc:  0.8515625
train loss:  0.34413784742355347
train gradient:  0.3034420783059175
iteration : 4314
train acc:  0.859375
train loss:  0.36589664220809937
train gradient:  0.4347847150666029
iteration : 4315
train acc:  0.84375
train loss:  0.3800804018974304
train gradient:  0.36022257631931265
iteration : 4316
train acc:  0.875
train loss:  0.32045847177505493
train gradient:  0.23544795917010125
iteration : 4317
train acc:  0.828125
train loss:  0.3826484978199005
train gradient:  0.2896887347972727
iteration : 4318
train acc:  0.796875
train loss:  0.42254823446273804
train gradient:  0.2743147751091193
iteration : 4319
train acc:  0.8359375
train loss:  0.3233078122138977
train gradient:  0.25656608500938316
iteration : 4320
train acc:  0.84375
train loss:  0.3764774203300476
train gradient:  0.39621903997515473
iteration : 4321
train acc:  0.90625
train loss:  0.26529043912887573
train gradient:  0.2091646256510596
iteration : 4322
train acc:  0.859375
train loss:  0.3480290472507477
train gradient:  0.22354454892116135
iteration : 4323
train acc:  0.875
train loss:  0.36223104596138
train gradient:  0.2778620259234977
iteration : 4324
train acc:  0.8359375
train loss:  0.3380751609802246
train gradient:  0.20488623567601594
iteration : 4325
train acc:  0.765625
train loss:  0.41355907917022705
train gradient:  0.26869928719469177
iteration : 4326
train acc:  0.84375
train loss:  0.3336266577243805
train gradient:  0.3718678258388356
iteration : 4327
train acc:  0.828125
train loss:  0.4139033257961273
train gradient:  0.2932258103752925
iteration : 4328
train acc:  0.90625
train loss:  0.259613573551178
train gradient:  0.2284855669062129
iteration : 4329
train acc:  0.8671875
train loss:  0.3212725818157196
train gradient:  0.3042195583108071
iteration : 4330
train acc:  0.90625
train loss:  0.2992810606956482
train gradient:  0.22646856014876543
iteration : 4331
train acc:  0.875
train loss:  0.3229018747806549
train gradient:  0.2286871319017027
iteration : 4332
train acc:  0.828125
train loss:  0.3740122318267822
train gradient:  0.25642881150233304
iteration : 4333
train acc:  0.8359375
train loss:  0.39880022406578064
train gradient:  0.36467755733188156
iteration : 4334
train acc:  0.8515625
train loss:  0.383261114358902
train gradient:  0.31483053002384215
iteration : 4335
train acc:  0.8515625
train loss:  0.33928877115249634
train gradient:  0.2035399997010684
iteration : 4336
train acc:  0.8515625
train loss:  0.37716007232666016
train gradient:  0.273918264101441
iteration : 4337
train acc:  0.8046875
train loss:  0.4127108156681061
train gradient:  0.30968444391023503
iteration : 4338
train acc:  0.8828125
train loss:  0.31988775730133057
train gradient:  0.20813667582578535
iteration : 4339
train acc:  0.875
train loss:  0.32731303572654724
train gradient:  0.25663293276010196
iteration : 4340
train acc:  0.8671875
train loss:  0.3142906725406647
train gradient:  0.20116378550376646
iteration : 4341
train acc:  0.8671875
train loss:  0.32770591974258423
train gradient:  0.19510404905625428
iteration : 4342
train acc:  0.8671875
train loss:  0.3358878493309021
train gradient:  0.24097648194868412
iteration : 4343
train acc:  0.890625
train loss:  0.31124913692474365
train gradient:  0.2280126576166659
iteration : 4344
train acc:  0.8515625
train loss:  0.3145424425601959
train gradient:  0.16583401595294736
iteration : 4345
train acc:  0.8828125
train loss:  0.2933090329170227
train gradient:  0.24814227279039372
iteration : 4346
train acc:  0.828125
train loss:  0.3964444398880005
train gradient:  0.25012463410217506
iteration : 4347
train acc:  0.8203125
train loss:  0.37469595670700073
train gradient:  0.24366627491459358
iteration : 4348
train acc:  0.8671875
train loss:  0.2981032133102417
train gradient:  0.23756229332606507
iteration : 4349
train acc:  0.796875
train loss:  0.3661158084869385
train gradient:  0.28614626696864837
iteration : 4350
train acc:  0.8671875
train loss:  0.33525338768959045
train gradient:  0.17304663177189192
iteration : 4351
train acc:  0.8359375
train loss:  0.35327041149139404
train gradient:  0.26166054588968973
iteration : 4352
train acc:  0.8125
train loss:  0.3859573006629944
train gradient:  0.27978439103422587
iteration : 4353
train acc:  0.8046875
train loss:  0.4261423945426941
train gradient:  0.3737747431603074
iteration : 4354
train acc:  0.859375
train loss:  0.33431267738342285
train gradient:  0.27929925501860614
iteration : 4355
train acc:  0.8515625
train loss:  0.3343959450721741
train gradient:  0.32753474649735326
iteration : 4356
train acc:  0.890625
train loss:  0.28441882133483887
train gradient:  0.20692797299933502
iteration : 4357
train acc:  0.8671875
train loss:  0.3209874629974365
train gradient:  0.23958308675766365
iteration : 4358
train acc:  0.890625
train loss:  0.2554873526096344
train gradient:  0.13421220984328128
iteration : 4359
train acc:  0.8671875
train loss:  0.33128535747528076
train gradient:  0.18146283046804976
iteration : 4360
train acc:  0.890625
train loss:  0.28517404198646545
train gradient:  0.27712516801898085
iteration : 4361
train acc:  0.859375
train loss:  0.29904770851135254
train gradient:  0.21761732243714152
iteration : 4362
train acc:  0.8984375
train loss:  0.37584102153778076
train gradient:  0.2166912804114531
iteration : 4363
train acc:  0.78125
train loss:  0.4375018775463104
train gradient:  0.41028006192414
iteration : 4364
train acc:  0.84375
train loss:  0.34181880950927734
train gradient:  0.19826094521180487
iteration : 4365
train acc:  0.875
train loss:  0.2928347587585449
train gradient:  0.14557345949345435
iteration : 4366
train acc:  0.8203125
train loss:  0.36122849583625793
train gradient:  0.3795453260615208
iteration : 4367
train acc:  0.8515625
train loss:  0.35073837637901306
train gradient:  0.23763016049000668
iteration : 4368
train acc:  0.9296875
train loss:  0.24224112927913666
train gradient:  0.15862447616484515
iteration : 4369
train acc:  0.84375
train loss:  0.33317074179649353
train gradient:  0.1851666883817282
iteration : 4370
train acc:  0.8515625
train loss:  0.28186434507369995
train gradient:  0.23191424004985012
iteration : 4371
train acc:  0.84375
train loss:  0.4145994484424591
train gradient:  0.34443521800341836
iteration : 4372
train acc:  0.8828125
train loss:  0.3105494976043701
train gradient:  0.25006610613249325
iteration : 4373
train acc:  0.8359375
train loss:  0.4333726167678833
train gradient:  0.3745431907231564
iteration : 4374
train acc:  0.8359375
train loss:  0.2951023578643799
train gradient:  0.20298521515872583
iteration : 4375
train acc:  0.875
train loss:  0.2627065181732178
train gradient:  0.2610368798971406
iteration : 4376
train acc:  0.796875
train loss:  0.4354763329029083
train gradient:  0.4516845149815631
iteration : 4377
train acc:  0.84375
train loss:  0.3947204351425171
train gradient:  0.2837773353536477
iteration : 4378
train acc:  0.890625
train loss:  0.27374136447906494
train gradient:  0.19602360748136874
iteration : 4379
train acc:  0.8515625
train loss:  0.37216484546661377
train gradient:  0.38090580090609044
iteration : 4380
train acc:  0.84375
train loss:  0.34411779046058655
train gradient:  0.3229449504065358
iteration : 4381
train acc:  0.828125
train loss:  0.42695847153663635
train gradient:  0.2933869069068502
iteration : 4382
train acc:  0.875
train loss:  0.3297792077064514
train gradient:  0.26815932663875974
iteration : 4383
train acc:  0.875
train loss:  0.2790517508983612
train gradient:  0.17886358880318778
iteration : 4384
train acc:  0.875
train loss:  0.3146214485168457
train gradient:  0.23658330011319006
iteration : 4385
train acc:  0.8125
train loss:  0.3651449680328369
train gradient:  0.2206485082986554
iteration : 4386
train acc:  0.8515625
train loss:  0.3395424485206604
train gradient:  0.2541226944717541
iteration : 4387
train acc:  0.84375
train loss:  0.3446022868156433
train gradient:  0.45423624622676884
iteration : 4388
train acc:  0.8828125
train loss:  0.3293243944644928
train gradient:  0.2743073455284795
iteration : 4389
train acc:  0.8984375
train loss:  0.271312952041626
train gradient:  0.17111785031557503
iteration : 4390
train acc:  0.796875
train loss:  0.4274563789367676
train gradient:  0.5101157516864778
iteration : 4391
train acc:  0.828125
train loss:  0.4215013384819031
train gradient:  0.3595973667701372
iteration : 4392
train acc:  0.8359375
train loss:  0.4122230112552643
train gradient:  0.3463825529408923
iteration : 4393
train acc:  0.828125
train loss:  0.4030853509902954
train gradient:  0.3087937902536853
iteration : 4394
train acc:  0.84375
train loss:  0.40618640184402466
train gradient:  0.446226070120049
iteration : 4395
train acc:  0.8359375
train loss:  0.37612074613571167
train gradient:  0.25146938502372057
iteration : 4396
train acc:  0.9140625
train loss:  0.2884334921836853
train gradient:  0.2109641650988523
iteration : 4397
train acc:  0.8203125
train loss:  0.37866079807281494
train gradient:  0.3939121390800729
iteration : 4398
train acc:  0.859375
train loss:  0.31596052646636963
train gradient:  0.16422058142873577
iteration : 4399
train acc:  0.875
train loss:  0.3035024106502533
train gradient:  0.24824555777246624
iteration : 4400
train acc:  0.8515625
train loss:  0.3808384835720062
train gradient:  0.3736181129111738
iteration : 4401
train acc:  0.8671875
train loss:  0.36264556646347046
train gradient:  0.4416223867169026
iteration : 4402
train acc:  0.859375
train loss:  0.32559651136398315
train gradient:  0.21149801119337325
iteration : 4403
train acc:  0.8046875
train loss:  0.4319968521595001
train gradient:  0.382454064207115
iteration : 4404
train acc:  0.875
train loss:  0.3471304774284363
train gradient:  0.23389669279357042
iteration : 4405
train acc:  0.8359375
train loss:  0.3735722303390503
train gradient:  0.24758974731121003
iteration : 4406
train acc:  0.8515625
train loss:  0.3420308828353882
train gradient:  0.22520191662766537
iteration : 4407
train acc:  0.8359375
train loss:  0.4017138183116913
train gradient:  0.3568721287747633
iteration : 4408
train acc:  0.828125
train loss:  0.45091208815574646
train gradient:  0.39251375426679136
iteration : 4409
train acc:  0.8125
train loss:  0.37349653244018555
train gradient:  0.37573107792209604
iteration : 4410
train acc:  0.84375
train loss:  0.33680427074432373
train gradient:  0.25365132877806174
iteration : 4411
train acc:  0.8671875
train loss:  0.3489975929260254
train gradient:  0.3116254828848968
iteration : 4412
train acc:  0.8671875
train loss:  0.3181137442588806
train gradient:  0.23020073594641552
iteration : 4413
train acc:  0.8671875
train loss:  0.33596616983413696
train gradient:  0.21974231768302072
iteration : 4414
train acc:  0.859375
train loss:  0.3467632830142975
train gradient:  0.1783225381660146
iteration : 4415
train acc:  0.8359375
train loss:  0.3741813898086548
train gradient:  0.28228809559995816
iteration : 4416
train acc:  0.8125
train loss:  0.4628555476665497
train gradient:  0.4340822635520562
iteration : 4417
train acc:  0.8046875
train loss:  0.4851006865501404
train gradient:  0.43796991158904736
iteration : 4418
train acc:  0.84375
train loss:  0.42169487476348877
train gradient:  0.35559427054302595
iteration : 4419
train acc:  0.8359375
train loss:  0.3536910116672516
train gradient:  0.2660970771431966
iteration : 4420
train acc:  0.8515625
train loss:  0.38816529512405396
train gradient:  0.23845135662853123
iteration : 4421
train acc:  0.84375
train loss:  0.41370174288749695
train gradient:  0.3125096226843865
iteration : 4422
train acc:  0.7890625
train loss:  0.428139328956604
train gradient:  0.4551426034926721
iteration : 4423
train acc:  0.8125
train loss:  0.38207927346229553
train gradient:  0.20543873101670054
iteration : 4424
train acc:  0.8125
train loss:  0.48594558238983154
train gradient:  0.49407760672885426
iteration : 4425
train acc:  0.875
train loss:  0.2873852849006653
train gradient:  0.17654337354387611
iteration : 4426
train acc:  0.8359375
train loss:  0.3726409673690796
train gradient:  0.1505108566265717
iteration : 4427
train acc:  0.828125
train loss:  0.42046689987182617
train gradient:  0.35343356176189955
iteration : 4428
train acc:  0.8671875
train loss:  0.32788223028182983
train gradient:  0.2902941663211496
iteration : 4429
train acc:  0.84375
train loss:  0.3447315990924835
train gradient:  0.21878144670213354
iteration : 4430
train acc:  0.8828125
train loss:  0.3252425193786621
train gradient:  0.17238737720156522
iteration : 4431
train acc:  0.8203125
train loss:  0.38085970282554626
train gradient:  0.2432500229552579
iteration : 4432
train acc:  0.828125
train loss:  0.43193748593330383
train gradient:  0.2083680827358339
iteration : 4433
train acc:  0.8671875
train loss:  0.31378161907196045
train gradient:  0.18625186786319387
iteration : 4434
train acc:  0.8359375
train loss:  0.38810643553733826
train gradient:  0.20946089349354946
iteration : 4435
train acc:  0.8359375
train loss:  0.361830472946167
train gradient:  0.1682596696768932
iteration : 4436
train acc:  0.8515625
train loss:  0.2670027017593384
train gradient:  0.14023360849758165
iteration : 4437
train acc:  0.875
train loss:  0.308721125125885
train gradient:  0.16097887031102606
iteration : 4438
train acc:  0.78125
train loss:  0.38207128643989563
train gradient:  0.3939884826618386
iteration : 4439
train acc:  0.859375
train loss:  0.35326093435287476
train gradient:  0.23236116138572976
iteration : 4440
train acc:  0.921875
train loss:  0.28930094838142395
train gradient:  0.1394196340787865
iteration : 4441
train acc:  0.8046875
train loss:  0.3913246989250183
train gradient:  0.217848819133851
iteration : 4442
train acc:  0.8515625
train loss:  0.3663329482078552
train gradient:  0.23562999718017782
iteration : 4443
train acc:  0.8671875
train loss:  0.3106883466243744
train gradient:  0.16255921449188493
iteration : 4444
train acc:  0.8515625
train loss:  0.31408989429473877
train gradient:  0.16479389728120653
iteration : 4445
train acc:  0.8125
train loss:  0.3683975040912628
train gradient:  0.26717643254396883
iteration : 4446
train acc:  0.84375
train loss:  0.3697315454483032
train gradient:  0.19613227835136351
iteration : 4447
train acc:  0.90625
train loss:  0.2705070376396179
train gradient:  0.16341446852163014
iteration : 4448
train acc:  0.8671875
train loss:  0.3615548610687256
train gradient:  0.24925930574001065
iteration : 4449
train acc:  0.8203125
train loss:  0.39431649446487427
train gradient:  0.2903540153647366
iteration : 4450
train acc:  0.8125
train loss:  0.4407534599304199
train gradient:  0.40326567813736053
iteration : 4451
train acc:  0.8359375
train loss:  0.37147215008735657
train gradient:  0.2179018939292043
iteration : 4452
train acc:  0.859375
train loss:  0.3021446466445923
train gradient:  0.2099374057801868
iteration : 4453
train acc:  0.8671875
train loss:  0.33354949951171875
train gradient:  0.2481188938711944
iteration : 4454
train acc:  0.796875
train loss:  0.4471064805984497
train gradient:  0.44510026052159946
iteration : 4455
train acc:  0.8046875
train loss:  0.36412572860717773
train gradient:  0.307266415249321
iteration : 4456
train acc:  0.796875
train loss:  0.34396111965179443
train gradient:  0.19822201240443574
iteration : 4457
train acc:  0.8515625
train loss:  0.36034172773361206
train gradient:  0.2303639959460333
iteration : 4458
train acc:  0.84375
train loss:  0.40452808141708374
train gradient:  0.3553574044196156
iteration : 4459
train acc:  0.875
train loss:  0.3379460275173187
train gradient:  0.20872045783503224
iteration : 4460
train acc:  0.8515625
train loss:  0.34922128915786743
train gradient:  0.22734438699708687
iteration : 4461
train acc:  0.875
train loss:  0.34254419803619385
train gradient:  0.24114254730784035
iteration : 4462
train acc:  0.8515625
train loss:  0.34252500534057617
train gradient:  0.1976039528824584
iteration : 4463
train acc:  0.8359375
train loss:  0.3689183294773102
train gradient:  0.25353840602871447
iteration : 4464
train acc:  0.8671875
train loss:  0.35822397470474243
train gradient:  0.26467223237516313
iteration : 4465
train acc:  0.8828125
train loss:  0.3589645028114319
train gradient:  0.1679180056276249
iteration : 4466
train acc:  0.8359375
train loss:  0.36892178654670715
train gradient:  0.31403195899090813
iteration : 4467
train acc:  0.78125
train loss:  0.42787790298461914
train gradient:  0.3420199086476463
iteration : 4468
train acc:  0.8515625
train loss:  0.3622298836708069
train gradient:  0.17868514715665645
iteration : 4469
train acc:  0.8359375
train loss:  0.3659819960594177
train gradient:  0.2089128921018989
iteration : 4470
train acc:  0.859375
train loss:  0.33044350147247314
train gradient:  0.2002505306711048
iteration : 4471
train acc:  0.8828125
train loss:  0.3325790762901306
train gradient:  0.21101808140238115
iteration : 4472
train acc:  0.8125
train loss:  0.41513651609420776
train gradient:  0.35933021864963877
iteration : 4473
train acc:  0.8671875
train loss:  0.32837915420532227
train gradient:  0.29052491460393987
iteration : 4474
train acc:  0.8203125
train loss:  0.3371777832508087
train gradient:  0.2307735508474435
iteration : 4475
train acc:  0.8671875
train loss:  0.3271191716194153
train gradient:  0.28729264469872945
iteration : 4476
train acc:  0.828125
train loss:  0.3829677700996399
train gradient:  0.198470552528914
iteration : 4477
train acc:  0.8515625
train loss:  0.3638854920864105
train gradient:  0.2723468978027091
iteration : 4478
train acc:  0.796875
train loss:  0.42637598514556885
train gradient:  0.3545238577987516
iteration : 4479
train acc:  0.8359375
train loss:  0.31756943464279175
train gradient:  0.20091076614669023
iteration : 4480
train acc:  0.8671875
train loss:  0.3295642137527466
train gradient:  0.213225364651905
iteration : 4481
train acc:  0.90625
train loss:  0.23971697688102722
train gradient:  0.15929330734662045
iteration : 4482
train acc:  0.828125
train loss:  0.350528359413147
train gradient:  0.28743061912169965
iteration : 4483
train acc:  0.84375
train loss:  0.34410560131073
train gradient:  0.3387188044263004
iteration : 4484
train acc:  0.8671875
train loss:  0.3252441883087158
train gradient:  0.21668223866583122
iteration : 4485
train acc:  0.84375
train loss:  0.3647943139076233
train gradient:  0.2792071437689232
iteration : 4486
train acc:  0.84375
train loss:  0.42867976427078247
train gradient:  0.2839481964171348
iteration : 4487
train acc:  0.8359375
train loss:  0.3114999532699585
train gradient:  0.1958211892373441
iteration : 4488
train acc:  0.8359375
train loss:  0.36718249320983887
train gradient:  0.23783174221239733
iteration : 4489
train acc:  0.78125
train loss:  0.42879655957221985
train gradient:  0.4165426872458206
iteration : 4490
train acc:  0.890625
train loss:  0.29945528507232666
train gradient:  0.23509831639510367
iteration : 4491
train acc:  0.7890625
train loss:  0.3656367063522339
train gradient:  0.2749003136925579
iteration : 4492
train acc:  0.859375
train loss:  0.301464319229126
train gradient:  0.19988980001525947
iteration : 4493
train acc:  0.796875
train loss:  0.45017579197883606
train gradient:  0.4149053266724543
iteration : 4494
train acc:  0.8125
train loss:  0.358830988407135
train gradient:  0.22549981993415363
iteration : 4495
train acc:  0.828125
train loss:  0.38364291191101074
train gradient:  0.24285888211168055
iteration : 4496
train acc:  0.7734375
train loss:  0.45702576637268066
train gradient:  0.6822070610036526
iteration : 4497
train acc:  0.828125
train loss:  0.4263628423213959
train gradient:  0.4115016575076169
iteration : 4498
train acc:  0.8828125
train loss:  0.31199267506599426
train gradient:  0.2630207939363285
iteration : 4499
train acc:  0.7734375
train loss:  0.48697346448898315
train gradient:  0.5685391915695562
iteration : 4500
train acc:  0.859375
train loss:  0.33099472522735596
train gradient:  0.24801436077474942
iteration : 4501
train acc:  0.8828125
train loss:  0.2994082570075989
train gradient:  0.15950962667532817
iteration : 4502
train acc:  0.8203125
train loss:  0.38019052147865295
train gradient:  0.2435639410071282
iteration : 4503
train acc:  0.7734375
train loss:  0.4934723973274231
train gradient:  0.41747155732490493
iteration : 4504
train acc:  0.828125
train loss:  0.37557077407836914
train gradient:  0.28635722368139976
iteration : 4505
train acc:  0.8515625
train loss:  0.3750944435596466
train gradient:  0.24053535981473284
iteration : 4506
train acc:  0.8125
train loss:  0.4268187880516052
train gradient:  0.3101905935066972
iteration : 4507
train acc:  0.84375
train loss:  0.3389844000339508
train gradient:  0.28838841636266355
iteration : 4508
train acc:  0.890625
train loss:  0.25180965662002563
train gradient:  0.167086877039385
iteration : 4509
train acc:  0.8359375
train loss:  0.3768383860588074
train gradient:  0.26096401718533785
iteration : 4510
train acc:  0.8515625
train loss:  0.3599820137023926
train gradient:  0.2822659513663622
iteration : 4511
train acc:  0.828125
train loss:  0.38799434900283813
train gradient:  0.24732748948071837
iteration : 4512
train acc:  0.8828125
train loss:  0.34863555431365967
train gradient:  0.23586026984335173
iteration : 4513
train acc:  0.8125
train loss:  0.44337350130081177
train gradient:  0.2846570017129183
iteration : 4514
train acc:  0.8515625
train loss:  0.36293697357177734
train gradient:  0.24545289240762896
iteration : 4515
train acc:  0.859375
train loss:  0.33093905448913574
train gradient:  0.16199349515797862
iteration : 4516
train acc:  0.828125
train loss:  0.3463934659957886
train gradient:  0.21243688227312563
iteration : 4517
train acc:  0.8984375
train loss:  0.26069027185440063
train gradient:  0.2539953084531282
iteration : 4518
train acc:  0.875
train loss:  0.2955475151538849
train gradient:  0.17331511137369565
iteration : 4519
train acc:  0.796875
train loss:  0.37113064527511597
train gradient:  0.25731108439524053
iteration : 4520
train acc:  0.859375
train loss:  0.30350518226623535
train gradient:  0.17277378879353555
iteration : 4521
train acc:  0.8515625
train loss:  0.3196268677711487
train gradient:  0.2770725792384036
iteration : 4522
train acc:  0.796875
train loss:  0.4477892518043518
train gradient:  0.3626672946680678
iteration : 4523
train acc:  0.8203125
train loss:  0.41722989082336426
train gradient:  0.2672985202268125
iteration : 4524
train acc:  0.8515625
train loss:  0.34261637926101685
train gradient:  0.1855392271839016
iteration : 4525
train acc:  0.8671875
train loss:  0.3299214839935303
train gradient:  0.23540327854049092
iteration : 4526
train acc:  0.890625
train loss:  0.31698882579803467
train gradient:  0.2085691370409025
iteration : 4527
train acc:  0.8671875
train loss:  0.3634035587310791
train gradient:  0.2760472855939825
iteration : 4528
train acc:  0.8828125
train loss:  0.29832977056503296
train gradient:  0.18528921181091149
iteration : 4529
train acc:  0.859375
train loss:  0.32907402515411377
train gradient:  0.19572891625618133
iteration : 4530
train acc:  0.8828125
train loss:  0.2550082802772522
train gradient:  0.266651283964665
iteration : 4531
train acc:  0.8046875
train loss:  0.4047403633594513
train gradient:  0.3734036843661688
iteration : 4532
train acc:  0.8828125
train loss:  0.33505678176879883
train gradient:  0.19632032456651857
iteration : 4533
train acc:  0.78125
train loss:  0.47697627544403076
train gradient:  0.5759046238507336
iteration : 4534
train acc:  0.8203125
train loss:  0.3425370752811432
train gradient:  0.2072328516091913
iteration : 4535
train acc:  0.796875
train loss:  0.35631799697875977
train gradient:  0.24326416073578033
iteration : 4536
train acc:  0.859375
train loss:  0.3064681887626648
train gradient:  0.23635567817647973
iteration : 4537
train acc:  0.8359375
train loss:  0.3574281334877014
train gradient:  0.28837678609829115
iteration : 4538
train acc:  0.84375
train loss:  0.33134013414382935
train gradient:  0.19606322467913329
iteration : 4539
train acc:  0.859375
train loss:  0.33127498626708984
train gradient:  0.19523828090125542
iteration : 4540
train acc:  0.859375
train loss:  0.30487319827079773
train gradient:  0.19135474760736226
iteration : 4541
train acc:  0.8125
train loss:  0.4670780599117279
train gradient:  0.5072962108343211
iteration : 4542
train acc:  0.8125
train loss:  0.42327800393104553
train gradient:  0.3193897253289316
iteration : 4543
train acc:  0.84375
train loss:  0.3869372308254242
train gradient:  0.21582268185481301
iteration : 4544
train acc:  0.8046875
train loss:  0.33375781774520874
train gradient:  0.22548300140623945
iteration : 4545
train acc:  0.828125
train loss:  0.3219043016433716
train gradient:  0.16708106732719252
iteration : 4546
train acc:  0.828125
train loss:  0.35835906863212585
train gradient:  0.22897831147530254
iteration : 4547
train acc:  0.8515625
train loss:  0.35639116168022156
train gradient:  0.23192345716758342
iteration : 4548
train acc:  0.84375
train loss:  0.36844873428344727
train gradient:  0.2167992987187039
iteration : 4549
train acc:  0.8125
train loss:  0.41716665029525757
train gradient:  0.5027365400777017
iteration : 4550
train acc:  0.796875
train loss:  0.4562019407749176
train gradient:  0.36130028274159515
iteration : 4551
train acc:  0.8515625
train loss:  0.3151116371154785
train gradient:  0.20663876179491458
iteration : 4552
train acc:  0.828125
train loss:  0.42358115315437317
train gradient:  0.2703993773983291
iteration : 4553
train acc:  0.8359375
train loss:  0.3700833320617676
train gradient:  0.28316838720525705
iteration : 4554
train acc:  0.8359375
train loss:  0.3787822723388672
train gradient:  0.20721096658406385
iteration : 4555
train acc:  0.859375
train loss:  0.339530348777771
train gradient:  0.2128279385007211
iteration : 4556
train acc:  0.859375
train loss:  0.40011489391326904
train gradient:  0.3028742607456927
iteration : 4557
train acc:  0.828125
train loss:  0.4066281318664551
train gradient:  0.2697046749756365
iteration : 4558
train acc:  0.890625
train loss:  0.3372197151184082
train gradient:  0.23793370539448827
iteration : 4559
train acc:  0.8515625
train loss:  0.3526223301887512
train gradient:  0.31929736639126605
iteration : 4560
train acc:  0.859375
train loss:  0.3679300546646118
train gradient:  0.20624074881897483
iteration : 4561
train acc:  0.8359375
train loss:  0.36444371938705444
train gradient:  0.1959349014650164
iteration : 4562
train acc:  0.828125
train loss:  0.3491173982620239
train gradient:  0.21617017256656904
iteration : 4563
train acc:  0.859375
train loss:  0.3183375597000122
train gradient:  0.280876309415044
iteration : 4564
train acc:  0.828125
train loss:  0.35739678144454956
train gradient:  0.28037375802557635
iteration : 4565
train acc:  0.8671875
train loss:  0.3507365584373474
train gradient:  0.22198103343497874
iteration : 4566
train acc:  0.859375
train loss:  0.32636788487434387
train gradient:  0.18043993792144425
iteration : 4567
train acc:  0.8828125
train loss:  0.28831976652145386
train gradient:  0.2178345101065886
iteration : 4568
train acc:  0.7734375
train loss:  0.4088015854358673
train gradient:  0.33761705463844155
iteration : 4569
train acc:  0.8359375
train loss:  0.34300777316093445
train gradient:  0.2838572979316449
iteration : 4570
train acc:  0.8125
train loss:  0.40262049436569214
train gradient:  0.28142337647770355
iteration : 4571
train acc:  0.859375
train loss:  0.3766824007034302
train gradient:  0.31656940491311913
iteration : 4572
train acc:  0.890625
train loss:  0.2752780318260193
train gradient:  0.15701674629725787
iteration : 4573
train acc:  0.890625
train loss:  0.29051336646080017
train gradient:  0.18110450941730133
iteration : 4574
train acc:  0.8203125
train loss:  0.3734665513038635
train gradient:  0.24927731912303996
iteration : 4575
train acc:  0.828125
train loss:  0.34977424144744873
train gradient:  0.33706252617128424
iteration : 4576
train acc:  0.7734375
train loss:  0.43292784690856934
train gradient:  0.2697521988085014
iteration : 4577
train acc:  0.8125
train loss:  0.3924269676208496
train gradient:  0.2295870987542433
iteration : 4578
train acc:  0.8828125
train loss:  0.32169294357299805
train gradient:  0.25395731897407364
iteration : 4579
train acc:  0.84375
train loss:  0.35077881813049316
train gradient:  0.25914518490911387
iteration : 4580
train acc:  0.796875
train loss:  0.3739202320575714
train gradient:  0.28213043642166236
iteration : 4581
train acc:  0.8515625
train loss:  0.38552188873291016
train gradient:  0.21823391504310924
iteration : 4582
train acc:  0.78125
train loss:  0.47551023960113525
train gradient:  0.5017707429842714
iteration : 4583
train acc:  0.8125
train loss:  0.38995906710624695
train gradient:  0.25066796149100784
iteration : 4584
train acc:  0.734375
train loss:  0.47329217195510864
train gradient:  0.3692363147379158
iteration : 4585
train acc:  0.875
train loss:  0.31528347730636597
train gradient:  0.1807518463878714
iteration : 4586
train acc:  0.8515625
train loss:  0.4888622462749481
train gradient:  0.5407342016510637
iteration : 4587
train acc:  0.828125
train loss:  0.4100867509841919
train gradient:  0.3090344684109497
iteration : 4588
train acc:  0.8515625
train loss:  0.38308361172676086
train gradient:  0.2935776728925282
iteration : 4589
train acc:  0.8515625
train loss:  0.39732128381729126
train gradient:  0.3640648904054724
iteration : 4590
train acc:  0.828125
train loss:  0.3498430848121643
train gradient:  0.28243896703463567
iteration : 4591
train acc:  0.7890625
train loss:  0.3949308693408966
train gradient:  0.39713760427129086
iteration : 4592
train acc:  0.7890625
train loss:  0.38391029834747314
train gradient:  0.26323407731610127
iteration : 4593
train acc:  0.859375
train loss:  0.357858270406723
train gradient:  0.20204425996147662
iteration : 4594
train acc:  0.859375
train loss:  0.323497474193573
train gradient:  0.2382059105472703
iteration : 4595
train acc:  0.875
train loss:  0.2979942560195923
train gradient:  0.14109462194882957
iteration : 4596
train acc:  0.828125
train loss:  0.34937992691993713
train gradient:  0.17941491671211796
iteration : 4597
train acc:  0.7734375
train loss:  0.4704105854034424
train gradient:  0.31494496185140486
iteration : 4598
train acc:  0.859375
train loss:  0.3371985852718353
train gradient:  0.16152476013405545
iteration : 4599
train acc:  0.8515625
train loss:  0.3812546133995056
train gradient:  0.18499781242944302
iteration : 4600
train acc:  0.84375
train loss:  0.3682618737220764
train gradient:  0.22125319261284881
iteration : 4601
train acc:  0.859375
train loss:  0.3532707691192627
train gradient:  0.31979735352597205
iteration : 4602
train acc:  0.859375
train loss:  0.32501542568206787
train gradient:  0.23645059370672988
iteration : 4603
train acc:  0.8671875
train loss:  0.3920575976371765
train gradient:  0.2651183941632094
iteration : 4604
train acc:  0.828125
train loss:  0.35629940032958984
train gradient:  0.2711568050819575
iteration : 4605
train acc:  0.8515625
train loss:  0.3781088888645172
train gradient:  0.25552598454082803
iteration : 4606
train acc:  0.8671875
train loss:  0.3474341928958893
train gradient:  0.1788244363417531
iteration : 4607
train acc:  0.8359375
train loss:  0.3271867632865906
train gradient:  0.2269841144112487
iteration : 4608
train acc:  0.859375
train loss:  0.32798445224761963
train gradient:  0.1965892962722595
iteration : 4609
train acc:  0.875
train loss:  0.2824447453022003
train gradient:  0.16056403797518423
iteration : 4610
train acc:  0.84375
train loss:  0.3396543264389038
train gradient:  0.21576050571259142
iteration : 4611
train acc:  0.8828125
train loss:  0.33610793948173523
train gradient:  0.27733874710077877
iteration : 4612
train acc:  0.7734375
train loss:  0.4729330539703369
train gradient:  0.35727744754405705
iteration : 4613
train acc:  0.8203125
train loss:  0.37942618131637573
train gradient:  0.2594900027226967
iteration : 4614
train acc:  0.84375
train loss:  0.40636661648750305
train gradient:  0.3411635305323906
iteration : 4615
train acc:  0.8203125
train loss:  0.3428425192832947
train gradient:  0.21781013369670174
iteration : 4616
train acc:  0.84375
train loss:  0.39372217655181885
train gradient:  0.29291801796525
iteration : 4617
train acc:  0.8671875
train loss:  0.3142220377922058
train gradient:  0.17849012565925043
iteration : 4618
train acc:  0.8828125
train loss:  0.34933918714523315
train gradient:  0.3015192309010028
iteration : 4619
train acc:  0.90625
train loss:  0.2889230251312256
train gradient:  0.1667284822049871
iteration : 4620
train acc:  0.828125
train loss:  0.31510740518569946
train gradient:  0.19378315728531004
iteration : 4621
train acc:  0.8359375
train loss:  0.32049107551574707
train gradient:  0.1919151549148153
iteration : 4622
train acc:  0.875
train loss:  0.31648361682891846
train gradient:  0.18612291007233736
iteration : 4623
train acc:  0.8125
train loss:  0.37230628728866577
train gradient:  0.34009144821976034
iteration : 4624
train acc:  0.828125
train loss:  0.337812602519989
train gradient:  0.3802943266467681
iteration : 4625
train acc:  0.8359375
train loss:  0.3168242573738098
train gradient:  0.21218485569213688
iteration : 4626
train acc:  0.859375
train loss:  0.3147462010383606
train gradient:  0.2543006144893841
iteration : 4627
train acc:  0.8515625
train loss:  0.3598436117172241
train gradient:  0.2664461483123066
iteration : 4628
train acc:  0.8828125
train loss:  0.28683483600616455
train gradient:  0.2181253309925547
iteration : 4629
train acc:  0.859375
train loss:  0.3148599863052368
train gradient:  0.22305400407046516
iteration : 4630
train acc:  0.8515625
train loss:  0.315023273229599
train gradient:  0.29749519227679627
iteration : 4631
train acc:  0.828125
train loss:  0.3448495864868164
train gradient:  0.22320439738023223
iteration : 4632
train acc:  0.8671875
train loss:  0.3002675175666809
train gradient:  0.1640058354947856
iteration : 4633
train acc:  0.8515625
train loss:  0.31500133872032166
train gradient:  0.18395755087149238
iteration : 4634
train acc:  0.8359375
train loss:  0.34362244606018066
train gradient:  0.20891189632524323
iteration : 4635
train acc:  0.8671875
train loss:  0.30477669835090637
train gradient:  0.1594314677111127
iteration : 4636
train acc:  0.859375
train loss:  0.3050076365470886
train gradient:  0.21632779270502764
iteration : 4637
train acc:  0.859375
train loss:  0.36202430725097656
train gradient:  0.31589905521520595
iteration : 4638
train acc:  0.765625
train loss:  0.42151784896850586
train gradient:  0.3736164419857089
iteration : 4639
train acc:  0.84375
train loss:  0.3751067817211151
train gradient:  0.2825659408569622
iteration : 4640
train acc:  0.8203125
train loss:  0.3690246641635895
train gradient:  0.2121745512808778
iteration : 4641
train acc:  0.859375
train loss:  0.29867494106292725
train gradient:  0.15618508748412685
iteration : 4642
train acc:  0.8203125
train loss:  0.3477891683578491
train gradient:  0.3244650203216734
iteration : 4643
train acc:  0.828125
train loss:  0.3855705261230469
train gradient:  0.2548198934179605
iteration : 4644
train acc:  0.8515625
train loss:  0.36674392223358154
train gradient:  0.24823341496450566
iteration : 4645
train acc:  0.90625
train loss:  0.27040374279022217
train gradient:  0.16532883701706946
iteration : 4646
train acc:  0.859375
train loss:  0.4062644839286804
train gradient:  0.3389895435489512
iteration : 4647
train acc:  0.828125
train loss:  0.3621588349342346
train gradient:  0.25959759984673314
iteration : 4648
train acc:  0.8828125
train loss:  0.3012990355491638
train gradient:  0.2092782115780365
iteration : 4649
train acc:  0.828125
train loss:  0.3738548755645752
train gradient:  0.3333226271895815
iteration : 4650
train acc:  0.828125
train loss:  0.3645479083061218
train gradient:  0.27753759085268476
iteration : 4651
train acc:  0.796875
train loss:  0.4572196900844574
train gradient:  0.39455884141378744
iteration : 4652
train acc:  0.8828125
train loss:  0.3094663619995117
train gradient:  0.44597379531034564
iteration : 4653
train acc:  0.8671875
train loss:  0.3310086727142334
train gradient:  0.3756759907551576
iteration : 4654
train acc:  0.8671875
train loss:  0.3366648852825165
train gradient:  0.2056475636616436
iteration : 4655
train acc:  0.8359375
train loss:  0.3632338047027588
train gradient:  0.2787638575009235
iteration : 4656
train acc:  0.8671875
train loss:  0.32582607865333557
train gradient:  0.1792848563423988
iteration : 4657
train acc:  0.8671875
train loss:  0.3435158133506775
train gradient:  0.22662248667180485
iteration : 4658
train acc:  0.8203125
train loss:  0.39509129524230957
train gradient:  0.2539659560685207
iteration : 4659
train acc:  0.8515625
train loss:  0.33864477276802063
train gradient:  0.2152020207679881
iteration : 4660
train acc:  0.8515625
train loss:  0.3726038634777069
train gradient:  0.25921297557972023
iteration : 4661
train acc:  0.84375
train loss:  0.3709243834018707
train gradient:  0.44059056454416046
iteration : 4662
train acc:  0.828125
train loss:  0.40154099464416504
train gradient:  0.5102319924767774
iteration : 4663
train acc:  0.828125
train loss:  0.3329223692417145
train gradient:  0.2625506679768859
iteration : 4664
train acc:  0.84375
train loss:  0.33387643098831177
train gradient:  0.20523591120454054
iteration : 4665
train acc:  0.84375
train loss:  0.38249319791793823
train gradient:  0.44783015980387536
iteration : 4666
train acc:  0.8046875
train loss:  0.37380120158195496
train gradient:  0.2681585889582611
iteration : 4667
train acc:  0.7421875
train loss:  0.5438587665557861
train gradient:  0.40290778972899993
iteration : 4668
train acc:  0.8671875
train loss:  0.36450791358947754
train gradient:  0.262810925458519
iteration : 4669
train acc:  0.8046875
train loss:  0.3837078809738159
train gradient:  0.2877113629484455
iteration : 4670
train acc:  0.828125
train loss:  0.3987146019935608
train gradient:  0.3020299595346292
iteration : 4671
train acc:  0.859375
train loss:  0.322349488735199
train gradient:  0.24175446481739465
iteration : 4672
train acc:  0.8046875
train loss:  0.4039570391178131
train gradient:  0.3962302699176081
iteration : 4673
train acc:  0.84375
train loss:  0.32338687777519226
train gradient:  0.19356678635760405
iteration : 4674
train acc:  0.8359375
train loss:  0.32786378264427185
train gradient:  0.2413375138232957
iteration : 4675
train acc:  0.8359375
train loss:  0.4154662489891052
train gradient:  0.24454477338313613
iteration : 4676
train acc:  0.8046875
train loss:  0.37703293561935425
train gradient:  0.2940044362685649
iteration : 4677
train acc:  0.875
train loss:  0.3402562141418457
train gradient:  0.219515714255766
iteration : 4678
train acc:  0.8125
train loss:  0.3605537414550781
train gradient:  0.25111269228517746
iteration : 4679
train acc:  0.875
train loss:  0.258789598941803
train gradient:  0.12236633526144407
iteration : 4680
train acc:  0.828125
train loss:  0.45714619755744934
train gradient:  0.4111699701624592
iteration : 4681
train acc:  0.8828125
train loss:  0.2834121584892273
train gradient:  0.20560327332718437
iteration : 4682
train acc:  0.8515625
train loss:  0.3544437289237976
train gradient:  0.20813283612166736
iteration : 4683
train acc:  0.8828125
train loss:  0.31620651483535767
train gradient:  0.2221456128717426
iteration : 4684
train acc:  0.90625
train loss:  0.28066378831863403
train gradient:  0.20013619962933255
iteration : 4685
train acc:  0.7734375
train loss:  0.439291775226593
train gradient:  0.3347952256564864
iteration : 4686
train acc:  0.8359375
train loss:  0.35773682594299316
train gradient:  0.2746675147522738
iteration : 4687
train acc:  0.84375
train loss:  0.36372047662734985
train gradient:  0.21882857745807316
iteration : 4688
train acc:  0.84375
train loss:  0.3888701796531677
train gradient:  0.30280250309614704
iteration : 4689
train acc:  0.796875
train loss:  0.3714045286178589
train gradient:  0.27322773399652395
iteration : 4690
train acc:  0.875
train loss:  0.3300017714500427
train gradient:  0.17868096511096598
iteration : 4691
train acc:  0.875
train loss:  0.2898608148097992
train gradient:  0.13787710503001424
iteration : 4692
train acc:  0.8046875
train loss:  0.4210100471973419
train gradient:  0.406300826487135
iteration : 4693
train acc:  0.7578125
train loss:  0.43680572509765625
train gradient:  0.3355027060313095
iteration : 4694
train acc:  0.84375
train loss:  0.3804757595062256
train gradient:  0.3727339737449607
iteration : 4695
train acc:  0.8125
train loss:  0.40877196192741394
train gradient:  0.3821359301440626
iteration : 4696
train acc:  0.796875
train loss:  0.36211615800857544
train gradient:  0.22885081828047976
iteration : 4697
train acc:  0.8671875
train loss:  0.38180404901504517
train gradient:  0.264811891277413
iteration : 4698
train acc:  0.8515625
train loss:  0.32466620206832886
train gradient:  0.22117737185586572
iteration : 4699
train acc:  0.8125
train loss:  0.4190818667411804
train gradient:  0.31133055831882445
iteration : 4700
train acc:  0.859375
train loss:  0.328957200050354
train gradient:  0.28056227297925507
iteration : 4701
train acc:  0.7734375
train loss:  0.4694189131259918
train gradient:  0.464960813625162
iteration : 4702
train acc:  0.84375
train loss:  0.37032651901245117
train gradient:  0.23728948116360968
iteration : 4703
train acc:  0.7890625
train loss:  0.4634557068347931
train gradient:  0.3508475990789406
iteration : 4704
train acc:  0.8203125
train loss:  0.3773806095123291
train gradient:  0.22164416141691176
iteration : 4705
train acc:  0.859375
train loss:  0.3077683746814728
train gradient:  0.19787448394956647
iteration : 4706
train acc:  0.828125
train loss:  0.3438527584075928
train gradient:  0.19698687505443446
iteration : 4707
train acc:  0.8671875
train loss:  0.3157169222831726
train gradient:  0.16628158532301743
iteration : 4708
train acc:  0.8125
train loss:  0.3893502950668335
train gradient:  0.40614949425308794
iteration : 4709
train acc:  0.84375
train loss:  0.32733845710754395
train gradient:  0.26027902265710584
iteration : 4710
train acc:  0.84375
train loss:  0.38312625885009766
train gradient:  0.25155583084021654
iteration : 4711
train acc:  0.90625
train loss:  0.2574543356895447
train gradient:  0.18288481074376245
iteration : 4712
train acc:  0.8125
train loss:  0.4081895053386688
train gradient:  0.2974591571171239
iteration : 4713
train acc:  0.859375
train loss:  0.3330402970314026
train gradient:  0.2335939302616188
iteration : 4714
train acc:  0.859375
train loss:  0.35744643211364746
train gradient:  0.2845449666574678
iteration : 4715
train acc:  0.828125
train loss:  0.4055830240249634
train gradient:  0.34349059340632143
iteration : 4716
train acc:  0.84375
train loss:  0.36072349548339844
train gradient:  0.22695152521715567
iteration : 4717
train acc:  0.828125
train loss:  0.39857354760169983
train gradient:  0.2880461613629265
iteration : 4718
train acc:  0.8359375
train loss:  0.3847106099128723
train gradient:  0.3153150824283847
iteration : 4719
train acc:  0.8515625
train loss:  0.3209706246852875
train gradient:  0.1998253284206118
iteration : 4720
train acc:  0.8359375
train loss:  0.35389041900634766
train gradient:  0.19452240280531063
iteration : 4721
train acc:  0.84375
train loss:  0.32478469610214233
train gradient:  0.22101066887317258
iteration : 4722
train acc:  0.859375
train loss:  0.36340969800949097
train gradient:  0.2280756123063511
iteration : 4723
train acc:  0.875
train loss:  0.3036421239376068
train gradient:  0.19458629629653323
iteration : 4724
train acc:  0.859375
train loss:  0.29093992710113525
train gradient:  0.19729704577529197
iteration : 4725
train acc:  0.890625
train loss:  0.3347008526325226
train gradient:  0.2720789428304246
iteration : 4726
train acc:  0.859375
train loss:  0.2969655990600586
train gradient:  0.15914710044723943
iteration : 4727
train acc:  0.8046875
train loss:  0.37301602959632874
train gradient:  0.20600671687306704
iteration : 4728
train acc:  0.8359375
train loss:  0.33572638034820557
train gradient:  0.3171102311761209
iteration : 4729
train acc:  0.828125
train loss:  0.3858881890773773
train gradient:  0.2387701332017384
iteration : 4730
train acc:  0.859375
train loss:  0.3282698392868042
train gradient:  0.16963433029570793
iteration : 4731
train acc:  0.8671875
train loss:  0.3284461200237274
train gradient:  0.2991235107655343
iteration : 4732
train acc:  0.84375
train loss:  0.3820912539958954
train gradient:  0.26111838732141424
iteration : 4733
train acc:  0.859375
train loss:  0.3994519114494324
train gradient:  0.3205226765883644
iteration : 4734
train acc:  0.8984375
train loss:  0.2942192256450653
train gradient:  0.16006885153707207
iteration : 4735
train acc:  0.78125
train loss:  0.4482378661632538
train gradient:  0.38369586975609554
iteration : 4736
train acc:  0.875
train loss:  0.3021882176399231
train gradient:  0.2197293257971591
iteration : 4737
train acc:  0.8671875
train loss:  0.3223314881324768
train gradient:  0.21978883540719052
iteration : 4738
train acc:  0.84375
train loss:  0.3498386740684509
train gradient:  0.21187479061919315
iteration : 4739
train acc:  0.828125
train loss:  0.35892802476882935
train gradient:  0.29490672335004026
iteration : 4740
train acc:  0.8203125
train loss:  0.3786441683769226
train gradient:  0.2162542337547063
iteration : 4741
train acc:  0.8203125
train loss:  0.37800484895706177
train gradient:  0.2729522039286773
iteration : 4742
train acc:  0.8203125
train loss:  0.32688120007514954
train gradient:  0.21524132922375228
iteration : 4743
train acc:  0.8671875
train loss:  0.29965120553970337
train gradient:  0.11347187170881172
iteration : 4744
train acc:  0.90625
train loss:  0.2664716839790344
train gradient:  0.1559049673134137
iteration : 4745
train acc:  0.859375
train loss:  0.3193089962005615
train gradient:  0.1874754868217677
iteration : 4746
train acc:  0.796875
train loss:  0.4874267876148224
train gradient:  0.372683946609618
iteration : 4747
train acc:  0.8828125
train loss:  0.36176347732543945
train gradient:  0.2661678982810056
iteration : 4748
train acc:  0.8671875
train loss:  0.3279370665550232
train gradient:  0.20998048157499608
iteration : 4749
train acc:  0.7890625
train loss:  0.43621399998664856
train gradient:  0.4246839770798447
iteration : 4750
train acc:  0.8828125
train loss:  0.28845706582069397
train gradient:  0.2483727261418036
iteration : 4751
train acc:  0.8203125
train loss:  0.3722272515296936
train gradient:  0.2325174828772697
iteration : 4752
train acc:  0.796875
train loss:  0.4214652478694916
train gradient:  0.3433402541526614
iteration : 4753
train acc:  0.8671875
train loss:  0.3075239062309265
train gradient:  0.2632386701595019
iteration : 4754
train acc:  0.796875
train loss:  0.440351277589798
train gradient:  0.3885534879508872
iteration : 4755
train acc:  0.8359375
train loss:  0.33809125423431396
train gradient:  0.26544244797380423
iteration : 4756
train acc:  0.875
train loss:  0.32901880145072937
train gradient:  0.2039963746321957
iteration : 4757
train acc:  0.8203125
train loss:  0.413951575756073
train gradient:  0.30836549986837986
iteration : 4758
train acc:  0.8203125
train loss:  0.40294110774993896
train gradient:  0.38883130865595467
iteration : 4759
train acc:  0.84375
train loss:  0.4004201889038086
train gradient:  0.27313665877630683
iteration : 4760
train acc:  0.8671875
train loss:  0.3605077862739563
train gradient:  0.24038555485663154
iteration : 4761
train acc:  0.8671875
train loss:  0.381083607673645
train gradient:  0.4367159170162859
iteration : 4762
train acc:  0.8359375
train loss:  0.352206289768219
train gradient:  0.2280983502252367
iteration : 4763
train acc:  0.859375
train loss:  0.3155829906463623
train gradient:  0.5412955400209893
iteration : 4764
train acc:  0.875
train loss:  0.299949586391449
train gradient:  0.19452906719363616
iteration : 4765
train acc:  0.8515625
train loss:  0.3508596122264862
train gradient:  0.2677253863569611
iteration : 4766
train acc:  0.8984375
train loss:  0.2924339771270752
train gradient:  0.2181253868041533
iteration : 4767
train acc:  0.8359375
train loss:  0.40375107526779175
train gradient:  0.24831169155856123
iteration : 4768
train acc:  0.890625
train loss:  0.272319495677948
train gradient:  0.13738748984377663
iteration : 4769
train acc:  0.8515625
train loss:  0.3424578905105591
train gradient:  0.23479130180933544
iteration : 4770
train acc:  0.8515625
train loss:  0.30965495109558105
train gradient:  0.159731834924362
iteration : 4771
train acc:  0.875
train loss:  0.32002484798431396
train gradient:  0.2155603544719428
iteration : 4772
train acc:  0.8515625
train loss:  0.3595857322216034
train gradient:  0.22899258250042048
iteration : 4773
train acc:  0.8828125
train loss:  0.3121650516986847
train gradient:  0.19201290147730868
iteration : 4774
train acc:  0.859375
train loss:  0.32297325134277344
train gradient:  0.17890678781358801
iteration : 4775
train acc:  0.796875
train loss:  0.46972590684890747
train gradient:  0.3720816636202892
iteration : 4776
train acc:  0.8828125
train loss:  0.31945687532424927
train gradient:  0.22695951205157078
iteration : 4777
train acc:  0.875
train loss:  0.34990251064300537
train gradient:  0.21399771296941322
iteration : 4778
train acc:  0.84375
train loss:  0.3880378007888794
train gradient:  0.2510504959599659
iteration : 4779
train acc:  0.828125
train loss:  0.40556201338768005
train gradient:  0.30758028930618186
iteration : 4780
train acc:  0.8125
train loss:  0.40135306119918823
train gradient:  0.25680623312465356
iteration : 4781
train acc:  0.8671875
train loss:  0.340117871761322
train gradient:  0.20765423313752787
iteration : 4782
train acc:  0.8515625
train loss:  0.3376070559024811
train gradient:  0.24792327822448967
iteration : 4783
train acc:  0.8984375
train loss:  0.2876884341239929
train gradient:  0.1438119055762368
iteration : 4784
train acc:  0.84375
train loss:  0.3728245496749878
train gradient:  0.2539457096626679
iteration : 4785
train acc:  0.8125
train loss:  0.4119417071342468
train gradient:  0.3887295506310958
iteration : 4786
train acc:  0.859375
train loss:  0.33123236894607544
train gradient:  0.17459759567710997
iteration : 4787
train acc:  0.890625
train loss:  0.32921677827835083
train gradient:  0.2058379084262532
iteration : 4788
train acc:  0.796875
train loss:  0.397208034992218
train gradient:  0.24859099818003721
iteration : 4789
train acc:  0.8359375
train loss:  0.32558825612068176
train gradient:  0.25665976704840715
iteration : 4790
train acc:  0.8125
train loss:  0.4834006130695343
train gradient:  0.46890495768368035
iteration : 4791
train acc:  0.765625
train loss:  0.4201328158378601
train gradient:  0.35919893191959756
iteration : 4792
train acc:  0.8359375
train loss:  0.3429609537124634
train gradient:  0.17145807290361118
iteration : 4793
train acc:  0.8515625
train loss:  0.3369542360305786
train gradient:  0.506738570407832
iteration : 4794
train acc:  0.859375
train loss:  0.3350464999675751
train gradient:  0.2030213039090023
iteration : 4795
train acc:  0.828125
train loss:  0.34255194664001465
train gradient:  0.22548424784728915
iteration : 4796
train acc:  0.7734375
train loss:  0.41195279359817505
train gradient:  0.28601360689555205
iteration : 4797
train acc:  0.859375
train loss:  0.3344738185405731
train gradient:  0.25922974330818893
iteration : 4798
train acc:  0.84375
train loss:  0.42211925983428955
train gradient:  0.44257309987446886
iteration : 4799
train acc:  0.875
train loss:  0.3198106586933136
train gradient:  0.15142832540158976
iteration : 4800
train acc:  0.84375
train loss:  0.365286648273468
train gradient:  0.3588661382129465
iteration : 4801
train acc:  0.8046875
train loss:  0.3988003730773926
train gradient:  0.4159482002279424
iteration : 4802
train acc:  0.8515625
train loss:  0.332011878490448
train gradient:  0.22547394491650863
iteration : 4803
train acc:  0.8359375
train loss:  0.3730611801147461
train gradient:  0.2552918475272465
iteration : 4804
train acc:  0.8515625
train loss:  0.3439244329929352
train gradient:  0.25814578557155504
iteration : 4805
train acc:  0.90625
train loss:  0.2732372283935547
train gradient:  0.15181299782153118
iteration : 4806
train acc:  0.8359375
train loss:  0.3670102655887604
train gradient:  0.27934127854191715
iteration : 4807
train acc:  0.8359375
train loss:  0.40314972400665283
train gradient:  0.49262496229939406
iteration : 4808
train acc:  0.90625
train loss:  0.2607074975967407
train gradient:  0.15607923868648088
iteration : 4809
train acc:  0.8359375
train loss:  0.44992536306381226
train gradient:  0.43107602059325933
iteration : 4810
train acc:  0.8203125
train loss:  0.3839493989944458
train gradient:  0.2788650439004192
iteration : 4811
train acc:  0.8671875
train loss:  0.29232269525527954
train gradient:  0.2370849121312893
iteration : 4812
train acc:  0.8515625
train loss:  0.33351653814315796
train gradient:  0.24090542993822855
iteration : 4813
train acc:  0.8046875
train loss:  0.36121273040771484
train gradient:  0.3002420120018026
iteration : 4814
train acc:  0.8359375
train loss:  0.30474627017974854
train gradient:  0.23682737649953114
iteration : 4815
train acc:  0.8125
train loss:  0.41615593433380127
train gradient:  0.33414793516740643
iteration : 4816
train acc:  0.859375
train loss:  0.32051515579223633
train gradient:  0.2983204607887291
iteration : 4817
train acc:  0.828125
train loss:  0.38871461153030396
train gradient:  0.2683866910481675
iteration : 4818
train acc:  0.7890625
train loss:  0.47791796922683716
train gradient:  0.3277074752691092
iteration : 4819
train acc:  0.8515625
train loss:  0.35382455587387085
train gradient:  0.31746320980052156
iteration : 4820
train acc:  0.8515625
train loss:  0.3193280100822449
train gradient:  0.27744533967056073
iteration : 4821
train acc:  0.8828125
train loss:  0.32204383611679077
train gradient:  0.20248739583406325
iteration : 4822
train acc:  0.828125
train loss:  0.39263659715652466
train gradient:  0.2787758135008877
iteration : 4823
train acc:  0.8671875
train loss:  0.31856340169906616
train gradient:  0.1850015006679609
iteration : 4824
train acc:  0.8828125
train loss:  0.30619141459465027
train gradient:  0.25252298801679224
iteration : 4825
train acc:  0.859375
train loss:  0.30512726306915283
train gradient:  0.19300697982548876
iteration : 4826
train acc:  0.7890625
train loss:  0.4342688322067261
train gradient:  0.3017736402338874
iteration : 4827
train acc:  0.8359375
train loss:  0.3499472737312317
train gradient:  0.23032325148505117
iteration : 4828
train acc:  0.890625
train loss:  0.2804450988769531
train gradient:  0.15993930575663445
iteration : 4829
train acc:  0.8359375
train loss:  0.36795222759246826
train gradient:  0.3357777887588755
iteration : 4830
train acc:  0.859375
train loss:  0.34866970777511597
train gradient:  0.20408251596530547
iteration : 4831
train acc:  0.8515625
train loss:  0.370589017868042
train gradient:  0.24190615607192884
iteration : 4832
train acc:  0.7734375
train loss:  0.38793593645095825
train gradient:  0.2462884336772707
iteration : 4833
train acc:  0.875
train loss:  0.3585209846496582
train gradient:  0.25485488362271613
iteration : 4834
train acc:  0.859375
train loss:  0.2953631579875946
train gradient:  0.253243094684961
iteration : 4835
train acc:  0.890625
train loss:  0.3271941840648651
train gradient:  0.3134327987211111
iteration : 4836
train acc:  0.84375
train loss:  0.31224682927131653
train gradient:  0.23045144095494474
iteration : 4837
train acc:  0.84375
train loss:  0.3339551091194153
train gradient:  0.19880739392298952
iteration : 4838
train acc:  0.859375
train loss:  0.3811893165111542
train gradient:  0.22843851024573397
iteration : 4839
train acc:  0.859375
train loss:  0.38349080085754395
train gradient:  0.2864647293898367
iteration : 4840
train acc:  0.875
train loss:  0.3259696066379547
train gradient:  0.20222978197003805
iteration : 4841
train acc:  0.890625
train loss:  0.3160322904586792
train gradient:  0.18156176542575053
iteration : 4842
train acc:  0.8203125
train loss:  0.3719249963760376
train gradient:  0.30466529706082296
iteration : 4843
train acc:  0.8046875
train loss:  0.42206016182899475
train gradient:  0.365120525657705
iteration : 4844
train acc:  0.84375
train loss:  0.4301750659942627
train gradient:  0.3280472348474221
iteration : 4845
train acc:  0.7578125
train loss:  0.43871158361434937
train gradient:  0.2636373662883389
iteration : 4846
train acc:  0.7578125
train loss:  0.5241826176643372
train gradient:  0.4630183697253913
iteration : 4847
train acc:  0.8515625
train loss:  0.35635727643966675
train gradient:  0.22795353484488604
iteration : 4848
train acc:  0.8203125
train loss:  0.38364970684051514
train gradient:  0.2489883650887768
iteration : 4849
train acc:  0.796875
train loss:  0.3412766456604004
train gradient:  0.33818823234962175
iteration : 4850
train acc:  0.875
train loss:  0.28558024764060974
train gradient:  0.1944443153496898
iteration : 4851
train acc:  0.8359375
train loss:  0.368221253156662
train gradient:  0.18723080168805745
iteration : 4852
train acc:  0.8046875
train loss:  0.3978354334831238
train gradient:  0.2521829756494603
iteration : 4853
train acc:  0.8203125
train loss:  0.42564114928245544
train gradient:  0.3056067522325432
iteration : 4854
train acc:  0.8671875
train loss:  0.356180340051651
train gradient:  0.19115849712935945
iteration : 4855
train acc:  0.875
train loss:  0.3790731430053711
train gradient:  0.24049628312537707
iteration : 4856
train acc:  0.8359375
train loss:  0.3647534251213074
train gradient:  0.19917153976655796
iteration : 4857
train acc:  0.890625
train loss:  0.3427935242652893
train gradient:  0.23941721122008255
iteration : 4858
train acc:  0.859375
train loss:  0.36501210927963257
train gradient:  0.41318931338905523
iteration : 4859
train acc:  0.828125
train loss:  0.37978804111480713
train gradient:  0.2377421176620813
iteration : 4860
train acc:  0.828125
train loss:  0.3484264314174652
train gradient:  0.24362756137339042
iteration : 4861
train acc:  0.890625
train loss:  0.26117923855781555
train gradient:  0.17241358831214154
iteration : 4862
train acc:  0.859375
train loss:  0.28952157497406006
train gradient:  0.17543570348125892
iteration : 4863
train acc:  0.7734375
train loss:  0.4130656123161316
train gradient:  0.29808402791184857
iteration : 4864
train acc:  0.875
train loss:  0.2627115249633789
train gradient:  0.19960545385789225
iteration : 4865
train acc:  0.8671875
train loss:  0.3365289568901062
train gradient:  0.23308394338739039
iteration : 4866
train acc:  0.8203125
train loss:  0.4242081344127655
train gradient:  0.2502344601376031
iteration : 4867
train acc:  0.828125
train loss:  0.344009667634964
train gradient:  0.313547996037296
iteration : 4868
train acc:  0.8359375
train loss:  0.3599700629711151
train gradient:  0.26224762219523834
iteration : 4869
train acc:  0.8515625
train loss:  0.3496330976486206
train gradient:  0.2528925482185219
iteration : 4870
train acc:  0.8515625
train loss:  0.33284759521484375
train gradient:  0.2377761117422809
iteration : 4871
train acc:  0.828125
train loss:  0.41852179169654846
train gradient:  0.2862814648772675
iteration : 4872
train acc:  0.875
train loss:  0.33622825145721436
train gradient:  0.21426431851016203
iteration : 4873
train acc:  0.859375
train loss:  0.37085437774658203
train gradient:  0.2464824488764943
iteration : 4874
train acc:  0.875
train loss:  0.2848438322544098
train gradient:  0.21478314080334057
iteration : 4875
train acc:  0.7734375
train loss:  0.436878502368927
train gradient:  0.3371973362970014
iteration : 4876
train acc:  0.8203125
train loss:  0.4280945062637329
train gradient:  0.34805840466969756
iteration : 4877
train acc:  0.84375
train loss:  0.32932302355766296
train gradient:  0.2473109538330981
iteration : 4878
train acc:  0.828125
train loss:  0.42219245433807373
train gradient:  0.3707134108803058
iteration : 4879
train acc:  0.8046875
train loss:  0.41975995898246765
train gradient:  0.23379086063915114
iteration : 4880
train acc:  0.859375
train loss:  0.3252248167991638
train gradient:  0.25666559106140474
iteration : 4881
train acc:  0.8359375
train loss:  0.3536358177661896
train gradient:  0.2491666246439302
iteration : 4882
train acc:  0.765625
train loss:  0.4546700119972229
train gradient:  0.37873713072836
iteration : 4883
train acc:  0.8828125
train loss:  0.3098015785217285
train gradient:  0.18625664277753926
iteration : 4884
train acc:  0.8046875
train loss:  0.3710883557796478
train gradient:  0.2593673274193606
iteration : 4885
train acc:  0.78125
train loss:  0.42028021812438965
train gradient:  0.2741086547256642
iteration : 4886
train acc:  0.84375
train loss:  0.33761274814605713
train gradient:  0.22710559068489883
iteration : 4887
train acc:  0.8984375
train loss:  0.29214832186698914
train gradient:  0.15616514432279777
iteration : 4888
train acc:  0.8203125
train loss:  0.39177337288856506
train gradient:  0.44745773672460143
iteration : 4889
train acc:  0.859375
train loss:  0.2899651825428009
train gradient:  0.19788408130099916
iteration : 4890
train acc:  0.7890625
train loss:  0.38389503955841064
train gradient:  0.3287189954133205
iteration : 4891
train acc:  0.828125
train loss:  0.44027382135391235
train gradient:  0.31047721386042243
iteration : 4892
train acc:  0.7890625
train loss:  0.45558053255081177
train gradient:  0.3489031961377537
iteration : 4893
train acc:  0.8359375
train loss:  0.3788352608680725
train gradient:  0.2195818737184856
iteration : 4894
train acc:  0.890625
train loss:  0.29126912355422974
train gradient:  0.2780372124874523
iteration : 4895
train acc:  0.8515625
train loss:  0.34899723529815674
train gradient:  0.2778743381359432
iteration : 4896
train acc:  0.8515625
train loss:  0.32320037484169006
train gradient:  0.19277872368333038
iteration : 4897
train acc:  0.859375
train loss:  0.30934643745422363
train gradient:  0.1523424973408031
iteration : 4898
train acc:  0.8125
train loss:  0.4072612524032593
train gradient:  0.2988774051203708
iteration : 4899
train acc:  0.84375
train loss:  0.34020495414733887
train gradient:  0.179413710259547
iteration : 4900
train acc:  0.8359375
train loss:  0.3339455723762512
train gradient:  0.19987755185843747
iteration : 4901
train acc:  0.8203125
train loss:  0.3807550072669983
train gradient:  0.24752933219380033
iteration : 4902
train acc:  0.796875
train loss:  0.3865759074687958
train gradient:  0.24938495857614443
iteration : 4903
train acc:  0.890625
train loss:  0.28500625491142273
train gradient:  0.2097730355998204
iteration : 4904
train acc:  0.875
train loss:  0.3011464476585388
train gradient:  0.1659337623362491
iteration : 4905
train acc:  0.8671875
train loss:  0.2984669506549835
train gradient:  0.20154166353959757
iteration : 4906
train acc:  0.84375
train loss:  0.3654371201992035
train gradient:  0.21394296015217196
iteration : 4907
train acc:  0.84375
train loss:  0.33078697323799133
train gradient:  0.21400580399075791
iteration : 4908
train acc:  0.84375
train loss:  0.3623843193054199
train gradient:  0.2679542807477539
iteration : 4909
train acc:  0.8125
train loss:  0.44755983352661133
train gradient:  0.3244995713981458
iteration : 4910
train acc:  0.8359375
train loss:  0.32361602783203125
train gradient:  0.21192634401834537
iteration : 4911
train acc:  0.8359375
train loss:  0.3355778455734253
train gradient:  0.24590365394703567
iteration : 4912
train acc:  0.90625
train loss:  0.28537338972091675
train gradient:  0.14853859977733527
iteration : 4913
train acc:  0.859375
train loss:  0.29252827167510986
train gradient:  0.18680042464365723
iteration : 4914
train acc:  0.84375
train loss:  0.30807510018348694
train gradient:  0.18066728990569156
iteration : 4915
train acc:  0.734375
train loss:  0.5551933646202087
train gradient:  0.5272901163699184
iteration : 4916
train acc:  0.8828125
train loss:  0.2859877943992615
train gradient:  0.15044678614175705
iteration : 4917
train acc:  0.84375
train loss:  0.3677107095718384
train gradient:  0.3189466532740667
iteration : 4918
train acc:  0.8046875
train loss:  0.42347538471221924
train gradient:  0.3410018478980146
iteration : 4919
train acc:  0.8671875
train loss:  0.3032793402671814
train gradient:  0.15514878791972658
iteration : 4920
train acc:  0.7734375
train loss:  0.45461305975914
train gradient:  0.3324657479431789
iteration : 4921
train acc:  0.796875
train loss:  0.3772612512111664
train gradient:  0.2200075015490706
iteration : 4922
train acc:  0.828125
train loss:  0.3582723140716553
train gradient:  0.23407790083148317
iteration : 4923
train acc:  0.8671875
train loss:  0.3490070700645447
train gradient:  0.21760015929015475
iteration : 4924
train acc:  0.84375
train loss:  0.3817201554775238
train gradient:  0.18794085316813644
iteration : 4925
train acc:  0.8515625
train loss:  0.30968692898750305
train gradient:  0.23002689896017686
iteration : 4926
train acc:  0.8203125
train loss:  0.3497094511985779
train gradient:  0.3704795037081313
iteration : 4927
train acc:  0.84375
train loss:  0.3990861773490906
train gradient:  0.24653602799311217
iteration : 4928
train acc:  0.84375
train loss:  0.33884432911872864
train gradient:  0.1855990539787209
iteration : 4929
train acc:  0.8203125
train loss:  0.38670551776885986
train gradient:  0.25166196111222855
iteration : 4930
train acc:  0.8203125
train loss:  0.39642924070358276
train gradient:  0.3015999850742872
iteration : 4931
train acc:  0.8359375
train loss:  0.3373931646347046
train gradient:  0.26535613629008886
iteration : 4932
train acc:  0.8671875
train loss:  0.26286935806274414
train gradient:  0.17228949514028788
iteration : 4933
train acc:  0.8671875
train loss:  0.3624108135700226
train gradient:  0.19008567955654632
iteration : 4934
train acc:  0.8203125
train loss:  0.4214913547039032
train gradient:  0.3060823567077313
iteration : 4935
train acc:  0.7734375
train loss:  0.4516252875328064
train gradient:  0.2657619011106974
iteration : 4936
train acc:  0.8984375
train loss:  0.2776562571525574
train gradient:  0.15640627137415952
iteration : 4937
train acc:  0.8046875
train loss:  0.4026782214641571
train gradient:  0.3289201686855077
iteration : 4938
train acc:  0.84375
train loss:  0.32403886318206787
train gradient:  0.16237436128824384
iteration : 4939
train acc:  0.8828125
train loss:  0.3026261329650879
train gradient:  0.19121390938011767
iteration : 4940
train acc:  0.828125
train loss:  0.32792869210243225
train gradient:  0.16771059833083954
iteration : 4941
train acc:  0.7890625
train loss:  0.4535754919052124
train gradient:  0.27014414465424097
iteration : 4942
train acc:  0.875
train loss:  0.3746478855609894
train gradient:  0.1791406556522494
iteration : 4943
train acc:  0.84375
train loss:  0.3722023367881775
train gradient:  0.2121543077795788
iteration : 4944
train acc:  0.8359375
train loss:  0.4295703172683716
train gradient:  0.26321923928126595
iteration : 4945
train acc:  0.84375
train loss:  0.38878557085990906
train gradient:  0.2573681409033568
iteration : 4946
train acc:  0.8828125
train loss:  0.29911747574806213
train gradient:  0.1335057192998392
iteration : 4947
train acc:  0.8828125
train loss:  0.3629506230354309
train gradient:  0.21391566137474835
iteration : 4948
train acc:  0.8828125
train loss:  0.28943800926208496
train gradient:  0.1828500365253438
iteration : 4949
train acc:  0.890625
train loss:  0.28490743041038513
train gradient:  0.13813017849108616
iteration : 4950
train acc:  0.8984375
train loss:  0.3445696234703064
train gradient:  0.27722606492282176
iteration : 4951
train acc:  0.765625
train loss:  0.4887011647224426
train gradient:  0.3227007843784124
iteration : 4952
train acc:  0.8203125
train loss:  0.3586999773979187
train gradient:  0.24812378383183553
iteration : 4953
train acc:  0.8359375
train loss:  0.38536667823791504
train gradient:  0.23843989370472501
iteration : 4954
train acc:  0.859375
train loss:  0.3531818389892578
train gradient:  0.25993278035956546
iteration : 4955
train acc:  0.8515625
train loss:  0.3740869164466858
train gradient:  0.2709034809376227
iteration : 4956
train acc:  0.8359375
train loss:  0.3676590025424957
train gradient:  0.241410715548351
iteration : 4957
train acc:  0.8203125
train loss:  0.3934107720851898
train gradient:  0.2686437630445674
iteration : 4958
train acc:  0.8125
train loss:  0.39791327714920044
train gradient:  0.1989952579141731
iteration : 4959
train acc:  0.8203125
train loss:  0.33988261222839355
train gradient:  0.2929379918585498
iteration : 4960
train acc:  0.78125
train loss:  0.4321664869785309
train gradient:  0.3985509947229821
iteration : 4961
train acc:  0.84375
train loss:  0.37370914220809937
train gradient:  0.28663883349061353
iteration : 4962
train acc:  0.875
train loss:  0.31523245573043823
train gradient:  0.20106506638539962
iteration : 4963
train acc:  0.8359375
train loss:  0.3943212926387787
train gradient:  0.4531889549904007
iteration : 4964
train acc:  0.890625
train loss:  0.26503515243530273
train gradient:  0.15876850218932176
iteration : 4965
train acc:  0.828125
train loss:  0.3504759669303894
train gradient:  0.22146856980652097
iteration : 4966
train acc:  0.8046875
train loss:  0.3665803372859955
train gradient:  0.34342056240131535
iteration : 4967
train acc:  0.7890625
train loss:  0.43879586458206177
train gradient:  0.32428664217596165
iteration : 4968
train acc:  0.859375
train loss:  0.3351990282535553
train gradient:  0.1931500164714957
iteration : 4969
train acc:  0.8046875
train loss:  0.3543989658355713
train gradient:  0.3199026319177404
iteration : 4970
train acc:  0.84375
train loss:  0.35053080320358276
train gradient:  0.2596027151632883
iteration : 4971
train acc:  0.859375
train loss:  0.3399202525615692
train gradient:  0.16774087393782378
iteration : 4972
train acc:  0.84375
train loss:  0.3511661887168884
train gradient:  0.1942560928131385
iteration : 4973
train acc:  0.875
train loss:  0.2945302724838257
train gradient:  0.21094293401907477
iteration : 4974
train acc:  0.8125
train loss:  0.3729671239852905
train gradient:  0.27020253070839884
iteration : 4975
train acc:  0.890625
train loss:  0.27131927013397217
train gradient:  0.14521285815000423
iteration : 4976
train acc:  0.8203125
train loss:  0.3806406259536743
train gradient:  0.19535046837073608
iteration : 4977
train acc:  0.8984375
train loss:  0.24467624723911285
train gradient:  0.1622378684658552
iteration : 4978
train acc:  0.859375
train loss:  0.3453001379966736
train gradient:  0.1556493307345439
iteration : 4979
train acc:  0.8359375
train loss:  0.36823880672454834
train gradient:  0.27271489074508204
iteration : 4980
train acc:  0.8828125
train loss:  0.2867460548877716
train gradient:  0.11609735877322622
iteration : 4981
train acc:  0.8515625
train loss:  0.3526307940483093
train gradient:  0.2242652311535398
iteration : 4982
train acc:  0.875
train loss:  0.29571253061294556
train gradient:  0.195258543410631
iteration : 4983
train acc:  0.8671875
train loss:  0.34700828790664673
train gradient:  0.2483149381543869
iteration : 4984
train acc:  0.8359375
train loss:  0.32505548000335693
train gradient:  0.23827608074315904
iteration : 4985
train acc:  0.8125
train loss:  0.34949690103530884
train gradient:  0.22540050899129774
iteration : 4986
train acc:  0.859375
train loss:  0.3178988993167877
train gradient:  0.2022010264043334
iteration : 4987
train acc:  0.8359375
train loss:  0.3434720039367676
train gradient:  0.2426144584081179
iteration : 4988
train acc:  0.8046875
train loss:  0.3728179931640625
train gradient:  0.2528867026894365
iteration : 4989
train acc:  0.8046875
train loss:  0.4251900315284729
train gradient:  0.4187398096983151
iteration : 4990
train acc:  0.828125
train loss:  0.3860158324241638
train gradient:  0.2449591854672573
iteration : 4991
train acc:  0.875
train loss:  0.28910839557647705
train gradient:  0.1799775780253614
iteration : 4992
train acc:  0.84375
train loss:  0.35180968046188354
train gradient:  0.35022318624885584
iteration : 4993
train acc:  0.7890625
train loss:  0.41091275215148926
train gradient:  0.3246532296143036
iteration : 4994
train acc:  0.8515625
train loss:  0.2896294593811035
train gradient:  0.16447942413054042
iteration : 4995
train acc:  0.8828125
train loss:  0.25610899925231934
train gradient:  0.17409557425627098
iteration : 4996
train acc:  0.8515625
train loss:  0.34277138113975525
train gradient:  0.23656662269186085
iteration : 4997
train acc:  0.828125
train loss:  0.37314367294311523
train gradient:  0.2450071248732547
iteration : 4998
train acc:  0.875
train loss:  0.3099977970123291
train gradient:  0.191356723659167
iteration : 4999
train acc:  0.828125
train loss:  0.34998226165771484
train gradient:  0.20882682562686128
iteration : 5000
train acc:  0.8515625
train loss:  0.2881501615047455
train gradient:  0.1637605857677079
iteration : 5001
train acc:  0.8359375
train loss:  0.33433711528778076
train gradient:  0.29562381793464093
iteration : 5002
train acc:  0.890625
train loss:  0.29751941561698914
train gradient:  0.22845985807133007
iteration : 5003
train acc:  0.828125
train loss:  0.3838418126106262
train gradient:  0.2924914159672769
iteration : 5004
train acc:  0.875
train loss:  0.3365374803543091
train gradient:  0.20502051044455788
iteration : 5005
train acc:  0.890625
train loss:  0.30879777669906616
train gradient:  0.2222185384415599
iteration : 5006
train acc:  0.8828125
train loss:  0.3111603260040283
train gradient:  0.22503455579776632
iteration : 5007
train acc:  0.796875
train loss:  0.40786781907081604
train gradient:  0.28058560305473595
iteration : 5008
train acc:  0.8203125
train loss:  0.40223950147628784
train gradient:  0.3715819617112882
iteration : 5009
train acc:  0.8359375
train loss:  0.362186998128891
train gradient:  0.3299571244941874
iteration : 5010
train acc:  0.7734375
train loss:  0.4365624785423279
train gradient:  0.3429731199810767
iteration : 5011
train acc:  0.8359375
train loss:  0.3564813733100891
train gradient:  0.22278843918850771
iteration : 5012
train acc:  0.828125
train loss:  0.3408730626106262
train gradient:  0.22840258759866716
iteration : 5013
train acc:  0.78125
train loss:  0.4319513738155365
train gradient:  0.35035523416972403
iteration : 5014
train acc:  0.828125
train loss:  0.39179879426956177
train gradient:  0.35370419486908017
iteration : 5015
train acc:  0.8203125
train loss:  0.36562493443489075
train gradient:  0.28175282834178456
iteration : 5016
train acc:  0.7890625
train loss:  0.41272056102752686
train gradient:  0.2580324019692103
iteration : 5017
train acc:  0.8359375
train loss:  0.3224000334739685
train gradient:  0.23662470487604964
iteration : 5018
train acc:  0.8515625
train loss:  0.3345123529434204
train gradient:  0.21779368997726917
iteration : 5019
train acc:  0.8359375
train loss:  0.34853237867355347
train gradient:  0.3013621757660499
iteration : 5020
train acc:  0.859375
train loss:  0.35273608565330505
train gradient:  0.23380159782447743
iteration : 5021
train acc:  0.8828125
train loss:  0.29710209369659424
train gradient:  0.1970474761534293
iteration : 5022
train acc:  0.8359375
train loss:  0.40171945095062256
train gradient:  0.28657552861356417
iteration : 5023
train acc:  0.8046875
train loss:  0.38670188188552856
train gradient:  0.3604033346855701
iteration : 5024
train acc:  0.859375
train loss:  0.29813262820243835
train gradient:  0.16982870508361003
iteration : 5025
train acc:  0.8515625
train loss:  0.3242088556289673
train gradient:  0.22655083958815012
iteration : 5026
train acc:  0.78125
train loss:  0.44565731287002563
train gradient:  0.39016781911202136
iteration : 5027
train acc:  0.8203125
train loss:  0.41317254304885864
train gradient:  0.2819602632465046
iteration : 5028
train acc:  0.90625
train loss:  0.2699841260910034
train gradient:  0.1470165463498217
iteration : 5029
train acc:  0.890625
train loss:  0.2902703881263733
train gradient:  0.12844284804812178
iteration : 5030
train acc:  0.78125
train loss:  0.4601178765296936
train gradient:  0.4060923030591007
iteration : 5031
train acc:  0.9375
train loss:  0.2774365246295929
train gradient:  0.1421372763955252
iteration : 5032
train acc:  0.8046875
train loss:  0.46880578994750977
train gradient:  0.33782567218137316
iteration : 5033
train acc:  0.8125
train loss:  0.4097553491592407
train gradient:  0.4947021668336266
iteration : 5034
train acc:  0.84375
train loss:  0.350429892539978
train gradient:  0.2209969494592676
iteration : 5035
train acc:  0.8125
train loss:  0.389972060918808
train gradient:  0.2615779787672491
iteration : 5036
train acc:  0.875
train loss:  0.3108232021331787
train gradient:  0.2194031156741585
iteration : 5037
train acc:  0.8203125
train loss:  0.4342731833457947
train gradient:  0.2735469423903105
iteration : 5038
train acc:  0.8515625
train loss:  0.30457890033721924
train gradient:  0.25066879106792195
iteration : 5039
train acc:  0.8515625
train loss:  0.3386761546134949
train gradient:  0.23301895059861888
iteration : 5040
train acc:  0.8828125
train loss:  0.30260491371154785
train gradient:  0.16176276416664248
iteration : 5041
train acc:  0.9375
train loss:  0.21533292531967163
train gradient:  0.1272871788841125
iteration : 5042
train acc:  0.859375
train loss:  0.30749183893203735
train gradient:  0.2545717623460505
iteration : 5043
train acc:  0.8359375
train loss:  0.3196425139904022
train gradient:  0.23459524714246668
iteration : 5044
train acc:  0.8359375
train loss:  0.3887786269187927
train gradient:  0.2356298375585346
iteration : 5045
train acc:  0.859375
train loss:  0.33476999402046204
train gradient:  0.21403078182242755
iteration : 5046
train acc:  0.8828125
train loss:  0.27996158599853516
train gradient:  0.18762505877608032
iteration : 5047
train acc:  0.890625
train loss:  0.2643386125564575
train gradient:  0.11384167758377127
iteration : 5048
train acc:  0.8671875
train loss:  0.36071622371673584
train gradient:  0.3146011905967443
iteration : 5049
train acc:  0.84375
train loss:  0.4597189426422119
train gradient:  0.33543672453612805
iteration : 5050
train acc:  0.8359375
train loss:  0.3440023958683014
train gradient:  0.24241013601759429
iteration : 5051
train acc:  0.859375
train loss:  0.3755580186843872
train gradient:  0.3496595325658314
iteration : 5052
train acc:  0.8046875
train loss:  0.41997581720352173
train gradient:  0.25909660993976774
iteration : 5053
train acc:  0.8359375
train loss:  0.34415870904922485
train gradient:  0.251799239190284
iteration : 5054
train acc:  0.875
train loss:  0.28252771496772766
train gradient:  0.20114837941948288
iteration : 5055
train acc:  0.828125
train loss:  0.34004634618759155
train gradient:  0.2973858826863959
iteration : 5056
train acc:  0.84375
train loss:  0.35767120122909546
train gradient:  0.3280031869417844
iteration : 5057
train acc:  0.859375
train loss:  0.29923897981643677
train gradient:  0.22432025360821728
iteration : 5058
train acc:  0.84375
train loss:  0.3791217803955078
train gradient:  0.23996947526048007
iteration : 5059
train acc:  0.8671875
train loss:  0.3114561140537262
train gradient:  0.220976068512345
iteration : 5060
train acc:  0.8359375
train loss:  0.3453023433685303
train gradient:  0.26128532612660554
iteration : 5061
train acc:  0.8359375
train loss:  0.316139817237854
train gradient:  0.17511067560986016
iteration : 5062
train acc:  0.8125
train loss:  0.4117591083049774
train gradient:  0.43942550581624523
iteration : 5063
train acc:  0.8515625
train loss:  0.3393074572086334
train gradient:  0.24019453908228466
iteration : 5064
train acc:  0.8359375
train loss:  0.3503376245498657
train gradient:  0.2140356017844212
iteration : 5065
train acc:  0.8515625
train loss:  0.3460489511489868
train gradient:  0.2192256357494724
iteration : 5066
train acc:  0.8671875
train loss:  0.2921961545944214
train gradient:  0.22665787441244858
iteration : 5067
train acc:  0.7890625
train loss:  0.3830443024635315
train gradient:  0.31010623574096197
iteration : 5068
train acc:  0.875
train loss:  0.2975232005119324
train gradient:  0.18157097058842808
iteration : 5069
train acc:  0.875
train loss:  0.3162713646888733
train gradient:  0.27210396219909383
iteration : 5070
train acc:  0.859375
train loss:  0.3234308362007141
train gradient:  0.235824802264613
iteration : 5071
train acc:  0.8046875
train loss:  0.39005476236343384
train gradient:  0.39734862668607457
iteration : 5072
train acc:  0.84375
train loss:  0.3046177327632904
train gradient:  0.17958703654182478
iteration : 5073
train acc:  0.8359375
train loss:  0.40331724286079407
train gradient:  0.2967394308292824
iteration : 5074
train acc:  0.7890625
train loss:  0.47154346108436584
train gradient:  0.39433213906019476
iteration : 5075
train acc:  0.8359375
train loss:  0.34185540676116943
train gradient:  0.2736188655319049
iteration : 5076
train acc:  0.8515625
train loss:  0.3305289149284363
train gradient:  0.25805640533807994
iteration : 5077
train acc:  0.84375
train loss:  0.3458966016769409
train gradient:  0.2787559996126351
iteration : 5078
train acc:  0.7734375
train loss:  0.4862503409385681
train gradient:  0.45061193294086743
iteration : 5079
train acc:  0.84375
train loss:  0.3993721008300781
train gradient:  0.3069325771261022
iteration : 5080
train acc:  0.8046875
train loss:  0.3465951383113861
train gradient:  0.30373893177738664
iteration : 5081
train acc:  0.875
train loss:  0.2901739478111267
train gradient:  0.2896574705390372
iteration : 5082
train acc:  0.8828125
train loss:  0.3252926170825958
train gradient:  0.24894638748447634
iteration : 5083
train acc:  0.84375
train loss:  0.34571215510368347
train gradient:  0.2330906025707852
iteration : 5084
train acc:  0.859375
train loss:  0.32016074657440186
train gradient:  0.17761078841932268
iteration : 5085
train acc:  0.875
train loss:  0.3267306089401245
train gradient:  0.2787822504695468
iteration : 5086
train acc:  0.921875
train loss:  0.2500879168510437
train gradient:  0.15211358183966506
iteration : 5087
train acc:  0.875
train loss:  0.3479766249656677
train gradient:  0.2195921459292684
iteration : 5088
train acc:  0.7890625
train loss:  0.4809030592441559
train gradient:  0.3452054957490043
iteration : 5089
train acc:  0.8828125
train loss:  0.32640331983566284
train gradient:  0.23814222092442555
iteration : 5090
train acc:  0.875
train loss:  0.2982490062713623
train gradient:  0.23572576728986064
iteration : 5091
train acc:  0.75
train loss:  0.4118877053260803
train gradient:  0.29965091221393725
iteration : 5092
train acc:  0.8515625
train loss:  0.3551565706729889
train gradient:  0.26402452090830003
iteration : 5093
train acc:  0.8515625
train loss:  0.4071890711784363
train gradient:  0.3693442698798253
iteration : 5094
train acc:  0.78125
train loss:  0.4242779016494751
train gradient:  0.3963790763725416
iteration : 5095
train acc:  0.859375
train loss:  0.3141830861568451
train gradient:  0.19319977437827823
iteration : 5096
train acc:  0.8359375
train loss:  0.3782663345336914
train gradient:  0.34874012339565563
iteration : 5097
train acc:  0.8515625
train loss:  0.3308843970298767
train gradient:  0.1756970386064925
iteration : 5098
train acc:  0.7734375
train loss:  0.4528815746307373
train gradient:  0.3089523469498322
iteration : 5099
train acc:  0.8671875
train loss:  0.3318921625614166
train gradient:  0.22875095319487765
iteration : 5100
train acc:  0.8515625
train loss:  0.3501860499382019
train gradient:  0.2620970866755461
iteration : 5101
train acc:  0.84375
train loss:  0.38519760966300964
train gradient:  0.25310164079754643
iteration : 5102
train acc:  0.890625
train loss:  0.2833970785140991
train gradient:  0.15861749292259597
iteration : 5103
train acc:  0.7734375
train loss:  0.4891764521598816
train gradient:  0.3704472788886614
iteration : 5104
train acc:  0.8203125
train loss:  0.4070015549659729
train gradient:  0.2621408055767875
iteration : 5105
train acc:  0.8515625
train loss:  0.38635748624801636
train gradient:  0.22277589981385398
iteration : 5106
train acc:  0.8203125
train loss:  0.4514392614364624
train gradient:  0.3563461885731741
iteration : 5107
train acc:  0.859375
train loss:  0.39110323786735535
train gradient:  0.2312989415025163
iteration : 5108
train acc:  0.8359375
train loss:  0.3107721209526062
train gradient:  0.19232545629026063
iteration : 5109
train acc:  0.859375
train loss:  0.32895177602767944
train gradient:  0.22835349059544474
iteration : 5110
train acc:  0.859375
train loss:  0.31392472982406616
train gradient:  0.1569133888673356
iteration : 5111
train acc:  0.8671875
train loss:  0.322319358587265
train gradient:  0.18113317314865135
iteration : 5112
train acc:  0.8828125
train loss:  0.3082605004310608
train gradient:  0.17941625784396242
iteration : 5113
train acc:  0.875
train loss:  0.3282756805419922
train gradient:  0.13004192301211515
iteration : 5114
train acc:  0.890625
train loss:  0.2881927192211151
train gradient:  0.19482808961315778
iteration : 5115
train acc:  0.8515625
train loss:  0.3121193051338196
train gradient:  0.12460377582532876
iteration : 5116
train acc:  0.8125
train loss:  0.4438524544239044
train gradient:  0.4604004982712266
iteration : 5117
train acc:  0.8359375
train loss:  0.4194955825805664
train gradient:  0.27824808854243627
iteration : 5118
train acc:  0.828125
train loss:  0.37109023332595825
train gradient:  0.30554912481949326
iteration : 5119
train acc:  0.8125
train loss:  0.3757973909378052
train gradient:  0.2698830670953534
iteration : 5120
train acc:  0.8515625
train loss:  0.3301495313644409
train gradient:  0.2668209494322427
iteration : 5121
train acc:  0.8984375
train loss:  0.28647685050964355
train gradient:  0.2522411907221021
iteration : 5122
train acc:  0.8515625
train loss:  0.3217170238494873
train gradient:  0.2782645447771445
iteration : 5123
train acc:  0.84375
train loss:  0.3542381525039673
train gradient:  0.2509376692413764
iteration : 5124
train acc:  0.8828125
train loss:  0.3674696683883667
train gradient:  0.40513609324419425
iteration : 5125
train acc:  0.8671875
train loss:  0.31283628940582275
train gradient:  0.23106122625557107
iteration : 5126
train acc:  0.8046875
train loss:  0.4488179087638855
train gradient:  0.28666594475124424
iteration : 5127
train acc:  0.8359375
train loss:  0.32601580023765564
train gradient:  0.20500080782612495
iteration : 5128
train acc:  0.8515625
train loss:  0.4026950001716614
train gradient:  0.22221381434794124
iteration : 5129
train acc:  0.8515625
train loss:  0.3641600012779236
train gradient:  0.2183028176072762
iteration : 5130
train acc:  0.7890625
train loss:  0.4596068263053894
train gradient:  0.2834831794965712
iteration : 5131
train acc:  0.765625
train loss:  0.40237024426460266
train gradient:  0.3184197473899723
iteration : 5132
train acc:  0.8125
train loss:  0.43919166922569275
train gradient:  0.31095239323458146
iteration : 5133
train acc:  0.84375
train loss:  0.39065462350845337
train gradient:  0.2351794888407307
iteration : 5134
train acc:  0.8515625
train loss:  0.41046178340911865
train gradient:  0.3489955787278783
iteration : 5135
train acc:  0.90625
train loss:  0.26962876319885254
train gradient:  0.2025260454498103
iteration : 5136
train acc:  0.8828125
train loss:  0.35123583674430847
train gradient:  0.2255995566185856
iteration : 5137
train acc:  0.8671875
train loss:  0.29002508521080017
train gradient:  0.15511975703562822
iteration : 5138
train acc:  0.8515625
train loss:  0.39443325996398926
train gradient:  0.3075664526693692
iteration : 5139
train acc:  0.8828125
train loss:  0.26668450236320496
train gradient:  0.13473030180699497
iteration : 5140
train acc:  0.8203125
train loss:  0.36803242564201355
train gradient:  0.2975306984381486
iteration : 5141
train acc:  0.859375
train loss:  0.33848699927330017
train gradient:  0.18408663832460365
iteration : 5142
train acc:  0.828125
train loss:  0.36603426933288574
train gradient:  0.22068895151556933
iteration : 5143
train acc:  0.859375
train loss:  0.3343275785446167
train gradient:  0.2357561875381249
iteration : 5144
train acc:  0.8203125
train loss:  0.35275739431381226
train gradient:  0.2328328561911334
iteration : 5145
train acc:  0.875
train loss:  0.3321288228034973
train gradient:  0.18961262593537528
iteration : 5146
train acc:  0.78125
train loss:  0.5140784978866577
train gradient:  0.3503514282645644
iteration : 5147
train acc:  0.859375
train loss:  0.3339964747428894
train gradient:  0.17729597605445868
iteration : 5148
train acc:  0.796875
train loss:  0.38839513063430786
train gradient:  0.24274090288601674
iteration : 5149
train acc:  0.8671875
train loss:  0.330383837223053
train gradient:  0.2313569163681891
iteration : 5150
train acc:  0.8046875
train loss:  0.4008159339427948
train gradient:  0.2481461820545633
iteration : 5151
train acc:  0.84375
train loss:  0.33583056926727295
train gradient:  0.18931103813639513
iteration : 5152
train acc:  0.890625
train loss:  0.2689778208732605
train gradient:  0.1683677430604386
iteration : 5153
train acc:  0.84375
train loss:  0.3534504771232605
train gradient:  0.2667270534647645
iteration : 5154
train acc:  0.8359375
train loss:  0.3529064655303955
train gradient:  0.2076308371831365
iteration : 5155
train acc:  0.8984375
train loss:  0.3122526705265045
train gradient:  0.1746887374593466
iteration : 5156
train acc:  0.9140625
train loss:  0.2574811577796936
train gradient:  0.15352723433642937
iteration : 5157
train acc:  0.8046875
train loss:  0.4182828664779663
train gradient:  0.4087297058685128
iteration : 5158
train acc:  0.8359375
train loss:  0.39502787590026855
train gradient:  0.3136922050304743
iteration : 5159
train acc:  0.8359375
train loss:  0.3655548393726349
train gradient:  0.33998362557868067
iteration : 5160
train acc:  0.859375
train loss:  0.3365127444267273
train gradient:  0.1971805224323947
iteration : 5161
train acc:  0.828125
train loss:  0.3592514395713806
train gradient:  0.22166273286375
iteration : 5162
train acc:  0.8203125
train loss:  0.38339850306510925
train gradient:  0.22245065177213785
iteration : 5163
train acc:  0.8671875
train loss:  0.3242650628089905
train gradient:  0.21945179156299616
iteration : 5164
train acc:  0.8828125
train loss:  0.31036078929901123
train gradient:  0.15048411492356778
iteration : 5165
train acc:  0.875
train loss:  0.29200059175491333
train gradient:  0.2921657761237638
iteration : 5166
train acc:  0.875
train loss:  0.28450489044189453
train gradient:  0.2772691398102999
iteration : 5167
train acc:  0.8203125
train loss:  0.33556872606277466
train gradient:  0.31908854086121274
iteration : 5168
train acc:  0.84375
train loss:  0.3801810145378113
train gradient:  0.4020931677579648
iteration : 5169
train acc:  0.8359375
train loss:  0.34160131216049194
train gradient:  0.21057574321905373
iteration : 5170
train acc:  0.8359375
train loss:  0.4002760052680969
train gradient:  0.39140180459145024
iteration : 5171
train acc:  0.8046875
train loss:  0.39615559577941895
train gradient:  0.33690361094521404
iteration : 5172
train acc:  0.8046875
train loss:  0.3748576045036316
train gradient:  0.3749671454895387
iteration : 5173
train acc:  0.8828125
train loss:  0.29417985677719116
train gradient:  0.22109342761587947
iteration : 5174
train acc:  0.796875
train loss:  0.35977664589881897
train gradient:  0.43363586100788565
iteration : 5175
train acc:  0.8828125
train loss:  0.28306692838668823
train gradient:  0.17984272404310833
iteration : 5176
train acc:  0.8359375
train loss:  0.3681318759918213
train gradient:  0.2402025184221002
iteration : 5177
train acc:  0.90625
train loss:  0.26452499628067017
train gradient:  0.25799549554136825
iteration : 5178
train acc:  0.8671875
train loss:  0.2974408268928528
train gradient:  0.22009992269771783
iteration : 5179
train acc:  0.8671875
train loss:  0.3169621229171753
train gradient:  0.22155953530172484
iteration : 5180
train acc:  0.859375
train loss:  0.3168298006057739
train gradient:  0.19164651816333517
iteration : 5181
train acc:  0.8046875
train loss:  0.46575528383255005
train gradient:  0.2935820644679113
iteration : 5182
train acc:  0.828125
train loss:  0.3435133695602417
train gradient:  0.26901815516642674
iteration : 5183
train acc:  0.796875
train loss:  0.4602828025817871
train gradient:  0.4368825870738839
iteration : 5184
train acc:  0.8828125
train loss:  0.28990525007247925
train gradient:  0.19301060675652704
iteration : 5185
train acc:  0.8515625
train loss:  0.31740453839302063
train gradient:  0.20466422722372596
iteration : 5186
train acc:  0.7890625
train loss:  0.4019383192062378
train gradient:  0.2253192039314105
iteration : 5187
train acc:  0.828125
train loss:  0.35274529457092285
train gradient:  0.21490076130810037
iteration : 5188
train acc:  0.859375
train loss:  0.3864811062812805
train gradient:  0.3331394673023699
iteration : 5189
train acc:  0.859375
train loss:  0.31453222036361694
train gradient:  0.25037764871023593
iteration : 5190
train acc:  0.84375
train loss:  0.3938610255718231
train gradient:  0.25597508444606604
iteration : 5191
train acc:  0.7890625
train loss:  0.47171932458877563
train gradient:  0.3909256397800779
iteration : 5192
train acc:  0.875
train loss:  0.33608999848365784
train gradient:  0.3364248775667522
iteration : 5193
train acc:  0.8984375
train loss:  0.303028404712677
train gradient:  0.19344689475974752
iteration : 5194
train acc:  0.859375
train loss:  0.36157548427581787
train gradient:  0.27146963019438913
iteration : 5195
train acc:  0.8046875
train loss:  0.36176007986068726
train gradient:  0.2429711684427383
iteration : 5196
train acc:  0.875
train loss:  0.3049282133579254
train gradient:  0.18733823328712898
iteration : 5197
train acc:  0.828125
train loss:  0.36229658126831055
train gradient:  0.25282189534461735
iteration : 5198
train acc:  0.8671875
train loss:  0.3548066318035126
train gradient:  0.21955399385882934
iteration : 5199
train acc:  0.8203125
train loss:  0.37482577562332153
train gradient:  0.15687297950512924
iteration : 5200
train acc:  0.8359375
train loss:  0.3262794613838196
train gradient:  0.1785716856208524
iteration : 5201
train acc:  0.828125
train loss:  0.4135836362838745
train gradient:  0.319507124576663
iteration : 5202
train acc:  0.8359375
train loss:  0.3613438308238983
train gradient:  0.22422395256229216
iteration : 5203
train acc:  0.859375
train loss:  0.3221958577632904
train gradient:  0.2018192726715589
iteration : 5204
train acc:  0.796875
train loss:  0.4316185712814331
train gradient:  0.45803923682946834
iteration : 5205
train acc:  0.9140625
train loss:  0.3269972801208496
train gradient:  0.21191618842637217
iteration : 5206
train acc:  0.8359375
train loss:  0.3475878834724426
train gradient:  0.19288367028907047
iteration : 5207
train acc:  0.78125
train loss:  0.45204272866249084
train gradient:  0.33097743290983284
iteration : 5208
train acc:  0.8046875
train loss:  0.43887895345687866
train gradient:  0.2931133689558674
iteration : 5209
train acc:  0.8359375
train loss:  0.372954398393631
train gradient:  0.2528028902593176
iteration : 5210
train acc:  0.84375
train loss:  0.37690889835357666
train gradient:  0.34229009397085514
iteration : 5211
train acc:  0.8515625
train loss:  0.31496462225914
train gradient:  0.18093341270544194
iteration : 5212
train acc:  0.890625
train loss:  0.2892378866672516
train gradient:  0.13818695664593356
iteration : 5213
train acc:  0.8515625
train loss:  0.311638742685318
train gradient:  0.22641865081128676
iteration : 5214
train acc:  0.8203125
train loss:  0.3965371251106262
train gradient:  0.33589994561692055
iteration : 5215
train acc:  0.8828125
train loss:  0.2741392254829407
train gradient:  0.24891983216930252
iteration : 5216
train acc:  0.8671875
train loss:  0.38205859065055847
train gradient:  0.2556172100444644
iteration : 5217
train acc:  0.84375
train loss:  0.3034909665584564
train gradient:  0.16789119249082937
iteration : 5218
train acc:  0.8515625
train loss:  0.37811774015426636
train gradient:  0.38725086315163326
iteration : 5219
train acc:  0.8203125
train loss:  0.33819568157196045
train gradient:  0.2286907022856052
iteration : 5220
train acc:  0.796875
train loss:  0.4273712635040283
train gradient:  0.4308449737100721
iteration : 5221
train acc:  0.8203125
train loss:  0.3899482190608978
train gradient:  0.2607326615282287
iteration : 5222
train acc:  0.8515625
train loss:  0.365988552570343
train gradient:  0.25758638809508183
iteration : 5223
train acc:  0.875
train loss:  0.31296324729919434
train gradient:  0.2140368556812215
iteration : 5224
train acc:  0.84375
train loss:  0.33368781208992004
train gradient:  0.27245072850214397
iteration : 5225
train acc:  0.8671875
train loss:  0.2811225652694702
train gradient:  0.14850923549456121
iteration : 5226
train acc:  0.859375
train loss:  0.3232596516609192
train gradient:  0.2291910069589989
iteration : 5227
train acc:  0.8046875
train loss:  0.4483819007873535
train gradient:  0.43536174272901385
iteration : 5228
train acc:  0.8671875
train loss:  0.29058587551116943
train gradient:  0.21884862700687904
iteration : 5229
train acc:  0.8515625
train loss:  0.3664141297340393
train gradient:  0.23376699177199442
iteration : 5230
train acc:  0.8515625
train loss:  0.381958544254303
train gradient:  0.2594645156973594
iteration : 5231
train acc:  0.8671875
train loss:  0.32510632276535034
train gradient:  0.1741812745404579
iteration : 5232
train acc:  0.859375
train loss:  0.3267024755477905
train gradient:  0.14611871027727238
iteration : 5233
train acc:  0.828125
train loss:  0.36228978633880615
train gradient:  0.28959624389022187
iteration : 5234
train acc:  0.859375
train loss:  0.3551711142063141
train gradient:  0.24668818214109023
iteration : 5235
train acc:  0.8203125
train loss:  0.3641030192375183
train gradient:  0.3011601833842749
iteration : 5236
train acc:  0.875
train loss:  0.3551546633243561
train gradient:  0.2426175665603862
iteration : 5237
train acc:  0.8828125
train loss:  0.30100759863853455
train gradient:  0.17111720791187365
iteration : 5238
train acc:  0.8203125
train loss:  0.3784533441066742
train gradient:  0.3368011434486292
iteration : 5239
train acc:  0.859375
train loss:  0.282325804233551
train gradient:  0.1456352067511352
iteration : 5240
train acc:  0.8359375
train loss:  0.36631569266319275
train gradient:  0.30465062997587233
iteration : 5241
train acc:  0.8515625
train loss:  0.32801738381385803
train gradient:  0.2165852735928845
iteration : 5242
train acc:  0.8515625
train loss:  0.3290092945098877
train gradient:  0.1975919526244494
iteration : 5243
train acc:  0.84375
train loss:  0.33824223279953003
train gradient:  0.23750730798055825
iteration : 5244
train acc:  0.8125
train loss:  0.4266793429851532
train gradient:  0.3104351099906721
iteration : 5245
train acc:  0.828125
train loss:  0.3583773672580719
train gradient:  0.27237520868793486
iteration : 5246
train acc:  0.8671875
train loss:  0.2753767967224121
train gradient:  0.16505518517871787
iteration : 5247
train acc:  0.8984375
train loss:  0.25194382667541504
train gradient:  0.13295351566528268
iteration : 5248
train acc:  0.828125
train loss:  0.3605149984359741
train gradient:  0.3301903341664165
iteration : 5249
train acc:  0.8203125
train loss:  0.37863725423812866
train gradient:  0.2523718300566185
iteration : 5250
train acc:  0.7890625
train loss:  0.3980242908000946
train gradient:  0.4956332170273116
iteration : 5251
train acc:  0.859375
train loss:  0.32490456104278564
train gradient:  0.19461614296488577
iteration : 5252
train acc:  0.859375
train loss:  0.3286818861961365
train gradient:  0.18624822271537425
iteration : 5253
train acc:  0.828125
train loss:  0.3570423722267151
train gradient:  0.23523292031859827
iteration : 5254
train acc:  0.9296875
train loss:  0.24841061234474182
train gradient:  0.1135907602900738
iteration : 5255
train acc:  0.8046875
train loss:  0.4428691267967224
train gradient:  0.49698164608179
iteration : 5256
train acc:  0.8828125
train loss:  0.27241161465644836
train gradient:  0.12788488153189242
iteration : 5257
train acc:  0.828125
train loss:  0.3471246361732483
train gradient:  0.2035824311213228
iteration : 5258
train acc:  0.8671875
train loss:  0.2831951677799225
train gradient:  0.20249302846795855
iteration : 5259
train acc:  0.859375
train loss:  0.325790137052536
train gradient:  0.23586792473203944
iteration : 5260
train acc:  0.890625
train loss:  0.30585986375808716
train gradient:  0.2216809048869448
iteration : 5261
train acc:  0.8828125
train loss:  0.3132762908935547
train gradient:  0.3253335765154651
iteration : 5262
train acc:  0.8359375
train loss:  0.36224228143692017
train gradient:  0.26336248442274524
iteration : 5263
train acc:  0.8203125
train loss:  0.34891340136528015
train gradient:  0.21874793635274137
iteration : 5264
train acc:  0.859375
train loss:  0.3594171106815338
train gradient:  0.29227198987237185
iteration : 5265
train acc:  0.8515625
train loss:  0.36327463388442993
train gradient:  0.3408932048473904
iteration : 5266
train acc:  0.8125
train loss:  0.42829471826553345
train gradient:  0.4073646104926264
iteration : 5267
train acc:  0.8359375
train loss:  0.32678326964378357
train gradient:  0.22426314771276312
iteration : 5268
train acc:  0.875
train loss:  0.31614232063293457
train gradient:  0.16823331030860117
iteration : 5269
train acc:  0.8828125
train loss:  0.3129825294017792
train gradient:  0.19217507690598823
iteration : 5270
train acc:  0.84375
train loss:  0.3212859630584717
train gradient:  0.3817270368361465
iteration : 5271
train acc:  0.828125
train loss:  0.4131465256214142
train gradient:  0.4042689299667224
iteration : 5272
train acc:  0.8359375
train loss:  0.40713241696357727
train gradient:  0.2106274450888816
iteration : 5273
train acc:  0.90625
train loss:  0.2724475860595703
train gradient:  0.22052120909002032
iteration : 5274
train acc:  0.859375
train loss:  0.35628366470336914
train gradient:  0.3531930463836782
iteration : 5275
train acc:  0.8828125
train loss:  0.3373497724533081
train gradient:  0.2167900270250642
iteration : 5276
train acc:  0.875
train loss:  0.31275609135627747
train gradient:  0.2307143910742192
iteration : 5277
train acc:  0.890625
train loss:  0.32280680537223816
train gradient:  0.18853126844501272
iteration : 5278
train acc:  0.9375
train loss:  0.2373964786529541
train gradient:  0.10883170046208075
iteration : 5279
train acc:  0.828125
train loss:  0.44215595722198486
train gradient:  0.4594847973918931
iteration : 5280
train acc:  0.8359375
train loss:  0.35931968688964844
train gradient:  0.263873288415108
iteration : 5281
train acc:  0.7734375
train loss:  0.4703231453895569
train gradient:  0.49914785050798116
iteration : 5282
train acc:  0.859375
train loss:  0.3420183062553406
train gradient:  0.21603188816232466
iteration : 5283
train acc:  0.8125
train loss:  0.38188186287879944
train gradient:  0.4102439131669867
iteration : 5284
train acc:  0.8671875
train loss:  0.27991822361946106
train gradient:  0.2021366427025269
iteration : 5285
train acc:  0.84375
train loss:  0.335646390914917
train gradient:  0.2199797518894502
iteration : 5286
train acc:  0.8828125
train loss:  0.2928726375102997
train gradient:  0.21836310188288854
iteration : 5287
train acc:  0.8515625
train loss:  0.3192726969718933
train gradient:  0.2939399809102218
iteration : 5288
train acc:  0.9140625
train loss:  0.2641168236732483
train gradient:  0.11307243380369625
iteration : 5289
train acc:  0.875
train loss:  0.3521440327167511
train gradient:  0.2144330150668441
iteration : 5290
train acc:  0.8359375
train loss:  0.36687102913856506
train gradient:  0.27569972068118637
iteration : 5291
train acc:  0.8828125
train loss:  0.31383103132247925
train gradient:  0.19624331780629772
iteration : 5292
train acc:  0.859375
train loss:  0.3225843012332916
train gradient:  0.18340779830598786
iteration : 5293
train acc:  0.8203125
train loss:  0.3938009738922119
train gradient:  0.33667264704305566
iteration : 5294
train acc:  0.859375
train loss:  0.299511194229126
train gradient:  0.15871508941123325
iteration : 5295
train acc:  0.8359375
train loss:  0.3168889880180359
train gradient:  0.25371619810798074
iteration : 5296
train acc:  0.7890625
train loss:  0.441718727350235
train gradient:  0.3243343030315128
iteration : 5297
train acc:  0.7578125
train loss:  0.43740564584732056
train gradient:  0.3451343767145686
iteration : 5298
train acc:  0.8203125
train loss:  0.44690942764282227
train gradient:  0.2739259864936529
iteration : 5299
train acc:  0.8125
train loss:  0.3990163803100586
train gradient:  0.28733607114075194
iteration : 5300
train acc:  0.84375
train loss:  0.34889453649520874
train gradient:  0.30881144335043686
iteration : 5301
train acc:  0.8671875
train loss:  0.32526695728302
train gradient:  0.2422105952247761
iteration : 5302
train acc:  0.859375
train loss:  0.3490908741950989
train gradient:  0.2320215678343701
iteration : 5303
train acc:  0.8359375
train loss:  0.40947848558425903
train gradient:  0.27300368544910025
iteration : 5304
train acc:  0.8515625
train loss:  0.35425305366516113
train gradient:  0.2855580554611127
iteration : 5305
train acc:  0.8125
train loss:  0.36668193340301514
train gradient:  0.21979003394923352
iteration : 5306
train acc:  0.796875
train loss:  0.41379469633102417
train gradient:  0.3082735724394539
iteration : 5307
train acc:  0.875
train loss:  0.3121592104434967
train gradient:  0.30062552619475497
iteration : 5308
train acc:  0.8203125
train loss:  0.37425166368484497
train gradient:  0.2656195159538323
iteration : 5309
train acc:  0.8046875
train loss:  0.42332062125205994
train gradient:  0.26506245382845073
iteration : 5310
train acc:  0.796875
train loss:  0.425150603055954
train gradient:  0.2998210505415469
iteration : 5311
train acc:  0.796875
train loss:  0.4213019013404846
train gradient:  0.31474200062540264
iteration : 5312
train acc:  0.8671875
train loss:  0.32428333163261414
train gradient:  0.20105743471314086
iteration : 5313
train acc:  0.828125
train loss:  0.43041011691093445
train gradient:  0.3126753646755584
iteration : 5314
train acc:  0.8671875
train loss:  0.33321815729141235
train gradient:  0.1904833146499693
iteration : 5315
train acc:  0.8671875
train loss:  0.30254149436950684
train gradient:  0.2233050449829399
iteration : 5316
train acc:  0.8203125
train loss:  0.43033188581466675
train gradient:  0.31694973908695295
iteration : 5317
train acc:  0.84375
train loss:  0.3486623167991638
train gradient:  0.2731270522783298
iteration : 5318
train acc:  0.8828125
train loss:  0.28812798857688904
train gradient:  0.1557687855160316
iteration : 5319
train acc:  0.8203125
train loss:  0.4173581898212433
train gradient:  0.30654030973680796
iteration : 5320
train acc:  0.8671875
train loss:  0.34918540716171265
train gradient:  0.18774295800891655
iteration : 5321
train acc:  0.8515625
train loss:  0.3478783071041107
train gradient:  0.29528005775568594
iteration : 5322
train acc:  0.875
train loss:  0.2974293828010559
train gradient:  0.143031055132637
iteration : 5323
train acc:  0.84375
train loss:  0.3451582193374634
train gradient:  0.2550405968260011
iteration : 5324
train acc:  0.8515625
train loss:  0.324670672416687
train gradient:  0.20265943181227863
iteration : 5325
train acc:  0.8515625
train loss:  0.33723461627960205
train gradient:  0.16317567930463378
iteration : 5326
train acc:  0.859375
train loss:  0.30929338932037354
train gradient:  0.19558163583163865
iteration : 5327
train acc:  0.8125
train loss:  0.35817569494247437
train gradient:  0.24465318096657718
iteration : 5328
train acc:  0.8359375
train loss:  0.35479891300201416
train gradient:  0.2386901390241817
iteration : 5329
train acc:  0.890625
train loss:  0.30916136503219604
train gradient:  0.22657749149067302
iteration : 5330
train acc:  0.8671875
train loss:  0.31546321511268616
train gradient:  0.20185594484421898
iteration : 5331
train acc:  0.7890625
train loss:  0.4964682459831238
train gradient:  0.52029801671374
iteration : 5332
train acc:  0.890625
train loss:  0.29809659719467163
train gradient:  0.35218160863831016
iteration : 5333
train acc:  0.8515625
train loss:  0.35695767402648926
train gradient:  0.26064275258882685
iteration : 5334
train acc:  0.90625
train loss:  0.2576141655445099
train gradient:  0.14723063298534134
iteration : 5335
train acc:  0.8359375
train loss:  0.4073339104652405
train gradient:  0.2957938207611801
iteration : 5336
train acc:  0.8203125
train loss:  0.36412709951400757
train gradient:  0.25526477221043803
iteration : 5337
train acc:  0.84375
train loss:  0.33946412801742554
train gradient:  0.1932149526558603
iteration : 5338
train acc:  0.8046875
train loss:  0.4196244776248932
train gradient:  0.3562405551012276
iteration : 5339
train acc:  0.84375
train loss:  0.35062769055366516
train gradient:  0.26142286359502764
iteration : 5340
train acc:  0.8515625
train loss:  0.3700607120990753
train gradient:  0.3191827870817383
iteration : 5341
train acc:  0.8671875
train loss:  0.26542654633522034
train gradient:  0.16373877653248353
iteration : 5342
train acc:  0.8828125
train loss:  0.33004030585289
train gradient:  0.23890649933653874
iteration : 5343
train acc:  0.8671875
train loss:  0.3296666443347931
train gradient:  0.25313716822839977
iteration : 5344
train acc:  0.8359375
train loss:  0.3137226998806
train gradient:  0.18546763697425378
iteration : 5345
train acc:  0.828125
train loss:  0.35455313324928284
train gradient:  0.24872628056031232
iteration : 5346
train acc:  0.8828125
train loss:  0.3298411965370178
train gradient:  0.19768137320132093
iteration : 5347
train acc:  0.8515625
train loss:  0.32977163791656494
train gradient:  0.1762189638858618
iteration : 5348
train acc:  0.890625
train loss:  0.27425065636634827
train gradient:  0.2249379982536695
iteration : 5349
train acc:  0.796875
train loss:  0.40702342987060547
train gradient:  0.28085074289326745
iteration : 5350
train acc:  0.8515625
train loss:  0.35050493478775024
train gradient:  0.24776118040542944
iteration : 5351
train acc:  0.890625
train loss:  0.3314972519874573
train gradient:  0.1834022752226354
iteration : 5352
train acc:  0.84375
train loss:  0.34298568964004517
train gradient:  0.2600296088803222
iteration : 5353
train acc:  0.875
train loss:  0.3306279182434082
train gradient:  0.18302616455298998
iteration : 5354
train acc:  0.8671875
train loss:  0.2923869788646698
train gradient:  0.2504560507564116
iteration : 5355
train acc:  0.859375
train loss:  0.35736048221588135
train gradient:  0.25195385005375054
iteration : 5356
train acc:  0.8515625
train loss:  0.34331250190734863
train gradient:  0.2659178445490189
iteration : 5357
train acc:  0.8203125
train loss:  0.38982629776000977
train gradient:  0.30215444811439335
iteration : 5358
train acc:  0.90625
train loss:  0.24814173579216003
train gradient:  0.1526726948434403
iteration : 5359
train acc:  0.8046875
train loss:  0.430047869682312
train gradient:  0.4747458137585591
iteration : 5360
train acc:  0.828125
train loss:  0.3496890664100647
train gradient:  0.22889028038899714
iteration : 5361
train acc:  0.828125
train loss:  0.3266281485557556
train gradient:  0.2773456560161837
iteration : 5362
train acc:  0.9296875
train loss:  0.21593715250492096
train gradient:  0.144093232687464
iteration : 5363
train acc:  0.8515625
train loss:  0.2767179310321808
train gradient:  0.19319145772974256
iteration : 5364
train acc:  0.84375
train loss:  0.37729623913764954
train gradient:  0.30095839029031407
iteration : 5365
train acc:  0.8203125
train loss:  0.3819611072540283
train gradient:  0.37256376552139586
iteration : 5366
train acc:  0.8359375
train loss:  0.36948591470718384
train gradient:  0.3385727986536859
iteration : 5367
train acc:  0.8515625
train loss:  0.36512959003448486
train gradient:  0.23678327222557047
iteration : 5368
train acc:  0.8203125
train loss:  0.3410522937774658
train gradient:  0.26255532523037206
iteration : 5369
train acc:  0.84375
train loss:  0.3524646759033203
train gradient:  0.21832076771764158
iteration : 5370
train acc:  0.8828125
train loss:  0.26077795028686523
train gradient:  0.18051109145633137
iteration : 5371
train acc:  0.859375
train loss:  0.3916003406047821
train gradient:  0.27253208220137964
iteration : 5372
train acc:  0.8515625
train loss:  0.3281598687171936
train gradient:  0.2276469138077142
iteration : 5373
train acc:  0.796875
train loss:  0.37556955218315125
train gradient:  0.30284870100106165
iteration : 5374
train acc:  0.8671875
train loss:  0.3211464285850525
train gradient:  0.3028632209301849
iteration : 5375
train acc:  0.859375
train loss:  0.3562867045402527
train gradient:  0.3310599844372009
iteration : 5376
train acc:  0.8984375
train loss:  0.3156508803367615
train gradient:  0.2901883357007595
iteration : 5377
train acc:  0.8203125
train loss:  0.3741956949234009
train gradient:  0.2702934609549097
iteration : 5378
train acc:  0.8359375
train loss:  0.41910699009895325
train gradient:  0.3162861809930733
iteration : 5379
train acc:  0.8671875
train loss:  0.3272044062614441
train gradient:  0.31641672334821425
iteration : 5380
train acc:  0.8359375
train loss:  0.3862878680229187
train gradient:  0.4598553805700858
iteration : 5381
train acc:  0.8046875
train loss:  0.3895459771156311
train gradient:  0.27366283734807745
iteration : 5382
train acc:  0.8359375
train loss:  0.3836468458175659
train gradient:  0.3202487474887151
iteration : 5383
train acc:  0.8828125
train loss:  0.2855989933013916
train gradient:  0.1843325850630969
iteration : 5384
train acc:  0.8828125
train loss:  0.27968427538871765
train gradient:  0.1935980312920383
iteration : 5385
train acc:  0.8984375
train loss:  0.2966533899307251
train gradient:  0.19455005662106262
iteration : 5386
train acc:  0.875
train loss:  0.3339460492134094
train gradient:  0.2202679115468469
iteration : 5387
train acc:  0.7578125
train loss:  0.5342843532562256
train gradient:  0.4928272073549317
iteration : 5388
train acc:  0.8984375
train loss:  0.28319209814071655
train gradient:  0.202329861767367
iteration : 5389
train acc:  0.7890625
train loss:  0.47621211409568787
train gradient:  0.41398581754203956
iteration : 5390
train acc:  0.8125
train loss:  0.37569865584373474
train gradient:  0.2603699339863663
iteration : 5391
train acc:  0.875
train loss:  0.31672534346580505
train gradient:  0.17342278903282857
iteration : 5392
train acc:  0.8671875
train loss:  0.317173570394516
train gradient:  0.18059704721076306
iteration : 5393
train acc:  0.8125
train loss:  0.33389759063720703
train gradient:  0.1909447061803339
iteration : 5394
train acc:  0.8828125
train loss:  0.2810826897621155
train gradient:  0.2463333522338053
iteration : 5395
train acc:  0.84375
train loss:  0.3877108693122864
train gradient:  0.27461559951679426
iteration : 5396
train acc:  0.8046875
train loss:  0.3640311062335968
train gradient:  0.27717922304410453
iteration : 5397
train acc:  0.8046875
train loss:  0.4076712727546692
train gradient:  0.27588836532912986
iteration : 5398
train acc:  0.859375
train loss:  0.3234865963459015
train gradient:  0.21742803039442699
iteration : 5399
train acc:  0.78125
train loss:  0.5215237736701965
train gradient:  0.5967459742927523
iteration : 5400
train acc:  0.8046875
train loss:  0.3836967349052429
train gradient:  0.28039366417237316
iteration : 5401
train acc:  0.8828125
train loss:  0.3053207993507385
train gradient:  0.20293188608052407
iteration : 5402
train acc:  0.84375
train loss:  0.3377113342285156
train gradient:  0.18906976651864205
iteration : 5403
train acc:  0.8046875
train loss:  0.40026384592056274
train gradient:  0.2749098677427761
iteration : 5404
train acc:  0.8125
train loss:  0.41088610887527466
train gradient:  0.27917541854062505
iteration : 5405
train acc:  0.8125
train loss:  0.3567938506603241
train gradient:  0.24604782229633307
iteration : 5406
train acc:  0.7734375
train loss:  0.46857064962387085
train gradient:  0.2815349330908543
iteration : 5407
train acc:  0.765625
train loss:  0.4301031827926636
train gradient:  0.27793162363613433
iteration : 5408
train acc:  0.890625
train loss:  0.30943089723587036
train gradient:  0.19170791818705699
iteration : 5409
train acc:  0.8515625
train loss:  0.3520808219909668
train gradient:  0.2360339978170586
iteration : 5410
train acc:  0.828125
train loss:  0.34467053413391113
train gradient:  0.2002793058640558
iteration : 5411
train acc:  0.8984375
train loss:  0.2641623318195343
train gradient:  0.1522477824348335
iteration : 5412
train acc:  0.84375
train loss:  0.339091420173645
train gradient:  0.2418839826436563
iteration : 5413
train acc:  0.7734375
train loss:  0.4622051417827606
train gradient:  0.37078337765935687
iteration : 5414
train acc:  0.828125
train loss:  0.37083864212036133
train gradient:  0.21763383480442644
iteration : 5415
train acc:  0.84375
train loss:  0.32223212718963623
train gradient:  0.24590073119437247
iteration : 5416
train acc:  0.84375
train loss:  0.35546445846557617
train gradient:  0.5153006181644625
iteration : 5417
train acc:  0.78125
train loss:  0.4112592935562134
train gradient:  0.23965214082758868
iteration : 5418
train acc:  0.8828125
train loss:  0.31442990899086
train gradient:  0.208418773621939
iteration : 5419
train acc:  0.859375
train loss:  0.3239908814430237
train gradient:  0.1771452197882229
iteration : 5420
train acc:  0.8515625
train loss:  0.3379475474357605
train gradient:  0.2665653888019215
iteration : 5421
train acc:  0.84375
train loss:  0.3285149931907654
train gradient:  0.24990645327650574
iteration : 5422
train acc:  0.8828125
train loss:  0.29575806856155396
train gradient:  0.17389211328495915
iteration : 5423
train acc:  0.8828125
train loss:  0.2464403510093689
train gradient:  0.1739662091453057
iteration : 5424
train acc:  0.8515625
train loss:  0.34036701917648315
train gradient:  0.19799534972810187
iteration : 5425
train acc:  0.890625
train loss:  0.3138580322265625
train gradient:  0.16646392560769196
iteration : 5426
train acc:  0.8203125
train loss:  0.38152801990509033
train gradient:  0.26403755130876705
iteration : 5427
train acc:  0.8515625
train loss:  0.3512314558029175
train gradient:  0.26419994726762486
iteration : 5428
train acc:  0.8359375
train loss:  0.366547167301178
train gradient:  0.25659916940055816
iteration : 5429
train acc:  0.828125
train loss:  0.42367249727249146
train gradient:  0.4009443101361406
iteration : 5430
train acc:  0.8515625
train loss:  0.35718026757240295
train gradient:  0.2848641156134513
iteration : 5431
train acc:  0.8125
train loss:  0.3883778750896454
train gradient:  0.2352149476929797
iteration : 5432
train acc:  0.8984375
train loss:  0.26608309149742126
train gradient:  0.15806858105806085
iteration : 5433
train acc:  0.828125
train loss:  0.3841690421104431
train gradient:  0.3057399743225696
iteration : 5434
train acc:  0.796875
train loss:  0.36963167786598206
train gradient:  0.24493102333843453
iteration : 5435
train acc:  0.8359375
train loss:  0.41675010323524475
train gradient:  0.495581365008783
iteration : 5436
train acc:  0.7890625
train loss:  0.44453269243240356
train gradient:  0.45896593210682024
iteration : 5437
train acc:  0.8203125
train loss:  0.38444358110427856
train gradient:  0.2684471409070417
iteration : 5438
train acc:  0.84375
train loss:  0.3388751149177551
train gradient:  0.14370232439323383
iteration : 5439
train acc:  0.84375
train loss:  0.31927216053009033
train gradient:  0.1716898214161252
iteration : 5440
train acc:  0.859375
train loss:  0.3574697971343994
train gradient:  0.27471425014150075
iteration : 5441
train acc:  0.8125
train loss:  0.39968806505203247
train gradient:  0.29568470262759233
iteration : 5442
train acc:  0.84375
train loss:  0.3259434401988983
train gradient:  0.3122687973021253
iteration : 5443
train acc:  0.828125
train loss:  0.3303646743297577
train gradient:  0.20422086947816126
iteration : 5444
train acc:  0.859375
train loss:  0.33842670917510986
train gradient:  0.20370268218699084
iteration : 5445
train acc:  0.8125
train loss:  0.46552303433418274
train gradient:  0.3376696437619841
iteration : 5446
train acc:  0.890625
train loss:  0.3012460470199585
train gradient:  0.2786627543472766
iteration : 5447
train acc:  0.859375
train loss:  0.372539758682251
train gradient:  0.259487361129769
iteration : 5448
train acc:  0.84375
train loss:  0.35728248953819275
train gradient:  0.22881916759020038
iteration : 5449
train acc:  0.8359375
train loss:  0.3316332697868347
train gradient:  0.21730515175423226
iteration : 5450
train acc:  0.7890625
train loss:  0.3882759213447571
train gradient:  0.3432890785617534
iteration : 5451
train acc:  0.8125
train loss:  0.3704966604709625
train gradient:  0.2586386393913269
iteration : 5452
train acc:  0.859375
train loss:  0.3135618269443512
train gradient:  0.16605623398196498
iteration : 5453
train acc:  0.921875
train loss:  0.25922128558158875
train gradient:  0.14329251831434092
iteration : 5454
train acc:  0.796875
train loss:  0.5071093440055847
train gradient:  0.41581517810910046
iteration : 5455
train acc:  0.859375
train loss:  0.2844497561454773
train gradient:  0.20495861934869708
iteration : 5456
train acc:  0.78125
train loss:  0.4074968993663788
train gradient:  0.28937017419050304
iteration : 5457
train acc:  0.828125
train loss:  0.38503187894821167
train gradient:  0.31522921900086004
iteration : 5458
train acc:  0.8671875
train loss:  0.3201334476470947
train gradient:  0.16300766548973222
iteration : 5459
train acc:  0.8359375
train loss:  0.33590996265411377
train gradient:  0.3316766708313041
iteration : 5460
train acc:  0.8828125
train loss:  0.31937316060066223
train gradient:  0.23781351120286864
iteration : 5461
train acc:  0.84375
train loss:  0.3686498701572418
train gradient:  0.2464076472771918
iteration : 5462
train acc:  0.8828125
train loss:  0.2681851387023926
train gradient:  0.210519124790293
iteration : 5463
train acc:  0.859375
train loss:  0.3257998526096344
train gradient:  0.26495250111651497
iteration : 5464
train acc:  0.828125
train loss:  0.32763391733169556
train gradient:  0.2217591631679473
iteration : 5465
train acc:  0.796875
train loss:  0.4635663628578186
train gradient:  0.24840927774226845
iteration : 5466
train acc:  0.8125
train loss:  0.33405500650405884
train gradient:  0.19082799975091916
iteration : 5467
train acc:  0.84375
train loss:  0.31836774945259094
train gradient:  0.25205572314330826
iteration : 5468
train acc:  0.828125
train loss:  0.40021902322769165
train gradient:  0.2091987310025344
iteration : 5469
train acc:  0.796875
train loss:  0.3353389501571655
train gradient:  0.29486956284209204
iteration : 5470
train acc:  0.875
train loss:  0.3524590730667114
train gradient:  0.25520284148827094
iteration : 5471
train acc:  0.8515625
train loss:  0.3187849223613739
train gradient:  0.15984603106453565
iteration : 5472
train acc:  0.8203125
train loss:  0.3342284560203552
train gradient:  0.2332972304964336
iteration : 5473
train acc:  0.8828125
train loss:  0.3423667848110199
train gradient:  0.18233868332445624
iteration : 5474
train acc:  0.8359375
train loss:  0.4167124629020691
train gradient:  0.2661639143221613
iteration : 5475
train acc:  0.8828125
train loss:  0.3191317915916443
train gradient:  0.3773597744495384
iteration : 5476
train acc:  0.8203125
train loss:  0.3503588140010834
train gradient:  0.1861466471498172
iteration : 5477
train acc:  0.8359375
train loss:  0.3210338354110718
train gradient:  0.15232858699419868
iteration : 5478
train acc:  0.9140625
train loss:  0.2751375734806061
train gradient:  0.14622625108369025
iteration : 5479
train acc:  0.8203125
train loss:  0.3661351203918457
train gradient:  0.33494636483162415
iteration : 5480
train acc:  0.875
train loss:  0.32344937324523926
train gradient:  0.1814127917542241
iteration : 5481
train acc:  0.84375
train loss:  0.3168869912624359
train gradient:  0.12564204052012
iteration : 5482
train acc:  0.8125
train loss:  0.3708650469779968
train gradient:  0.21146431027877505
iteration : 5483
train acc:  0.859375
train loss:  0.3486132025718689
train gradient:  0.18896519942331746
iteration : 5484
train acc:  0.8828125
train loss:  0.30525344610214233
train gradient:  0.15395060433114163
iteration : 5485
train acc:  0.8125
train loss:  0.42373326420783997
train gradient:  0.3388631185685104
iteration : 5486
train acc:  0.8671875
train loss:  0.299091100692749
train gradient:  0.17470906304509914
iteration : 5487
train acc:  0.859375
train loss:  0.38887888193130493
train gradient:  0.3686404520069585
iteration : 5488
train acc:  0.8359375
train loss:  0.3621976375579834
train gradient:  0.2619742993331529
iteration : 5489
train acc:  0.828125
train loss:  0.3928108215332031
train gradient:  0.27190238622443064
iteration : 5490
train acc:  0.8046875
train loss:  0.3909611105918884
train gradient:  0.22717631493160728
iteration : 5491
train acc:  0.8515625
train loss:  0.3598092794418335
train gradient:  0.2856340411065044
iteration : 5492
train acc:  0.8671875
train loss:  0.30435115098953247
train gradient:  0.18844851268042762
iteration : 5493
train acc:  0.8125
train loss:  0.4305993914604187
train gradient:  0.29332751808742497
iteration : 5494
train acc:  0.8515625
train loss:  0.31143128871917725
train gradient:  0.14600533567503365
iteration : 5495
train acc:  0.8515625
train loss:  0.3164902925491333
train gradient:  0.15275388815857033
iteration : 5496
train acc:  0.8828125
train loss:  0.3075675964355469
train gradient:  0.20918492441054037
iteration : 5497
train acc:  0.890625
train loss:  0.2534829378128052
train gradient:  0.19124924725053377
iteration : 5498
train acc:  0.84375
train loss:  0.3720443844795227
train gradient:  0.2807899538853555
iteration : 5499
train acc:  0.84375
train loss:  0.38546109199523926
train gradient:  0.21047698982231855
iteration : 5500
train acc:  0.8515625
train loss:  0.343586802482605
train gradient:  0.17687348853302692
iteration : 5501
train acc:  0.859375
train loss:  0.34836333990097046
train gradient:  0.22928440209961254
iteration : 5502
train acc:  0.828125
train loss:  0.38317590951919556
train gradient:  0.2435812312267014
iteration : 5503
train acc:  0.890625
train loss:  0.3041900098323822
train gradient:  0.21463416573847396
iteration : 5504
train acc:  0.8203125
train loss:  0.36051660776138306
train gradient:  0.23501796129251853
iteration : 5505
train acc:  0.8984375
train loss:  0.3339615762233734
train gradient:  0.18457730691349272
iteration : 5506
train acc:  0.8515625
train loss:  0.3066365718841553
train gradient:  0.15210711877907118
iteration : 5507
train acc:  0.8828125
train loss:  0.2720099091529846
train gradient:  0.12005025126031411
iteration : 5508
train acc:  0.8671875
train loss:  0.3568161725997925
train gradient:  0.2109610092949481
iteration : 5509
train acc:  0.8203125
train loss:  0.39197927713394165
train gradient:  0.2798875531736618
iteration : 5510
train acc:  0.875
train loss:  0.29267600178718567
train gradient:  0.20060223548408326
iteration : 5511
train acc:  0.859375
train loss:  0.35653072595596313
train gradient:  0.25256523069341913
iteration : 5512
train acc:  0.8828125
train loss:  0.3423466086387634
train gradient:  0.21281350717631742
iteration : 5513
train acc:  0.84375
train loss:  0.3109095096588135
train gradient:  0.15924522787414994
iteration : 5514
train acc:  0.7890625
train loss:  0.3921396732330322
train gradient:  0.2836742526945178
iteration : 5515
train acc:  0.8359375
train loss:  0.34740209579467773
train gradient:  0.24460752992834645
iteration : 5516
train acc:  0.8671875
train loss:  0.3063793182373047
train gradient:  0.1921514345798081
iteration : 5517
train acc:  0.8671875
train loss:  0.30639731884002686
train gradient:  0.16232196631352408
iteration : 5518
train acc:  0.8359375
train loss:  0.34640681743621826
train gradient:  0.17391915910243602
iteration : 5519
train acc:  0.8203125
train loss:  0.41463175415992737
train gradient:  0.2466710299132222
iteration : 5520
train acc:  0.8359375
train loss:  0.3644081950187683
train gradient:  0.24481641895583606
iteration : 5521
train acc:  0.8046875
train loss:  0.3603099584579468
train gradient:  0.24947073782150903
iteration : 5522
train acc:  0.8984375
train loss:  0.3286033272743225
train gradient:  0.21055828727834774
iteration : 5523
train acc:  0.828125
train loss:  0.3606724441051483
train gradient:  0.2652910418513757
iteration : 5524
train acc:  0.875
train loss:  0.28598567843437195
train gradient:  0.1689073072310193
iteration : 5525
train acc:  0.8984375
train loss:  0.29925137758255005
train gradient:  0.19013358615494888
iteration : 5526
train acc:  0.921875
train loss:  0.3055627942085266
train gradient:  0.24280738487810544
iteration : 5527
train acc:  0.8515625
train loss:  0.33657100796699524
train gradient:  0.20097876026496697
iteration : 5528
train acc:  0.859375
train loss:  0.38538533449172974
train gradient:  0.2719822284410243
iteration : 5529
train acc:  0.828125
train loss:  0.345583975315094
train gradient:  0.21924731564712707
iteration : 5530
train acc:  0.859375
train loss:  0.3549984097480774
train gradient:  0.212041601079643
iteration : 5531
train acc:  0.8671875
train loss:  0.3710578382015228
train gradient:  0.23112979229515862
iteration : 5532
train acc:  0.84375
train loss:  0.3807966709136963
train gradient:  0.23520335054972785
iteration : 5533
train acc:  0.828125
train loss:  0.32895269989967346
train gradient:  0.2112099107277446
iteration : 5534
train acc:  0.890625
train loss:  0.31458017230033875
train gradient:  0.20855882240696177
iteration : 5535
train acc:  0.828125
train loss:  0.3518001437187195
train gradient:  0.21668845155838692
iteration : 5536
train acc:  0.8671875
train loss:  0.35466286540031433
train gradient:  0.23842458021402374
iteration : 5537
train acc:  0.890625
train loss:  0.2732716202735901
train gradient:  0.18824716415733178
iteration : 5538
train acc:  0.84375
train loss:  0.34372478723526
train gradient:  0.17109545774531587
iteration : 5539
train acc:  0.7890625
train loss:  0.4725591540336609
train gradient:  0.3532155208900935
iteration : 5540
train acc:  0.859375
train loss:  0.3191903233528137
train gradient:  0.26934814112806893
iteration : 5541
train acc:  0.8828125
train loss:  0.3089030385017395
train gradient:  0.2105266173114358
iteration : 5542
train acc:  0.8125
train loss:  0.38271018862724304
train gradient:  0.2444835525804124
iteration : 5543
train acc:  0.8359375
train loss:  0.38464418053627014
train gradient:  0.21262892613414233
iteration : 5544
train acc:  0.8359375
train loss:  0.3596850037574768
train gradient:  0.2704049957968874
iteration : 5545
train acc:  0.9375
train loss:  0.21903561055660248
train gradient:  0.15296833042356536
iteration : 5546
train acc:  0.84375
train loss:  0.343339204788208
train gradient:  0.216156779523044
iteration : 5547
train acc:  0.890625
train loss:  0.25157231092453003
train gradient:  0.1886847436339928
iteration : 5548
train acc:  0.8359375
train loss:  0.35737743973731995
train gradient:  0.255177393919912
iteration : 5549
train acc:  0.8125
train loss:  0.4104176461696625
train gradient:  0.2545445904783331
iteration : 5550
train acc:  0.9140625
train loss:  0.24514815211296082
train gradient:  0.1080246178125444
iteration : 5551
train acc:  0.828125
train loss:  0.36386436223983765
train gradient:  0.18255284877890032
iteration : 5552
train acc:  0.859375
train loss:  0.34466809034347534
train gradient:  0.19656679493364812
iteration : 5553
train acc:  0.8515625
train loss:  0.33092445135116577
train gradient:  0.19971742348067578
iteration : 5554
train acc:  0.859375
train loss:  0.30447033047676086
train gradient:  0.2584823898402001
iteration : 5555
train acc:  0.875
train loss:  0.2544734477996826
train gradient:  0.10735363858630745
iteration : 5556
train acc:  0.8359375
train loss:  0.34158855676651
train gradient:  0.2546452595981502
iteration : 5557
train acc:  0.8984375
train loss:  0.2636030912399292
train gradient:  0.2856925702752824
iteration : 5558
train acc:  0.78125
train loss:  0.3828258514404297
train gradient:  0.3716367630985337
iteration : 5559
train acc:  0.8828125
train loss:  0.2613874673843384
train gradient:  0.24322962943025112
iteration : 5560
train acc:  0.8125
train loss:  0.4260922074317932
train gradient:  0.36136914002690845
iteration : 5561
train acc:  0.8671875
train loss:  0.32628458738327026
train gradient:  0.20300490521499304
iteration : 5562
train acc:  0.8671875
train loss:  0.3282514214515686
train gradient:  0.18700973612603594
iteration : 5563
train acc:  0.8046875
train loss:  0.39708563685417175
train gradient:  0.3185512349128245
iteration : 5564
train acc:  0.828125
train loss:  0.3527260422706604
train gradient:  0.26489005370511914
iteration : 5565
train acc:  0.8203125
train loss:  0.36632004380226135
train gradient:  0.2049049174853625
iteration : 5566
train acc:  0.8125
train loss:  0.3824006915092468
train gradient:  0.2806893526965693
iteration : 5567
train acc:  0.8359375
train loss:  0.3957711160182953
train gradient:  0.4049835070945327
iteration : 5568
train acc:  0.8359375
train loss:  0.36515527963638306
train gradient:  0.19455495021708497
iteration : 5569
train acc:  0.796875
train loss:  0.3834279775619507
train gradient:  0.30279692132870495
iteration : 5570
train acc:  0.84375
train loss:  0.3714044690132141
train gradient:  0.19069399428941394
iteration : 5571
train acc:  0.875
train loss:  0.28718966245651245
train gradient:  0.19027058159430932
iteration : 5572
train acc:  0.8515625
train loss:  0.422258198261261
train gradient:  0.3724396278552501
iteration : 5573
train acc:  0.875
train loss:  0.2623752951622009
train gradient:  0.17463677009881842
iteration : 5574
train acc:  0.8671875
train loss:  0.31481608748435974
train gradient:  0.1645606559247176
iteration : 5575
train acc:  0.84375
train loss:  0.3419725298881531
train gradient:  0.21376181498547508
iteration : 5576
train acc:  0.8046875
train loss:  0.40288153290748596
train gradient:  0.24364710027190123
iteration : 5577
train acc:  0.828125
train loss:  0.381255567073822
train gradient:  0.24248797518403647
iteration : 5578
train acc:  0.828125
train loss:  0.3732367157936096
train gradient:  0.24288262935525473
iteration : 5579
train acc:  0.8828125
train loss:  0.2905164957046509
train gradient:  0.24656475859956545
iteration : 5580
train acc:  0.796875
train loss:  0.37915557622909546
train gradient:  0.2899222493675667
iteration : 5581
train acc:  0.8515625
train loss:  0.2878516912460327
train gradient:  0.18520791533747719
iteration : 5582
train acc:  0.828125
train loss:  0.320748507976532
train gradient:  0.18548151715022843
iteration : 5583
train acc:  0.8359375
train loss:  0.3858461380004883
train gradient:  0.2385822696152632
iteration : 5584
train acc:  0.875
train loss:  0.27083098888397217
train gradient:  0.14033966755237282
iteration : 5585
train acc:  0.8125
train loss:  0.3650404214859009
train gradient:  0.24443246148289538
iteration : 5586
train acc:  0.875
train loss:  0.3403794765472412
train gradient:  0.2824736668212653
iteration : 5587
train acc:  0.8125
train loss:  0.441744327545166
train gradient:  0.40598253178912985
iteration : 5588
train acc:  0.8203125
train loss:  0.4448479413986206
train gradient:  0.308770772835156
iteration : 5589
train acc:  0.78125
train loss:  0.46446216106414795
train gradient:  0.32553055849337326
iteration : 5590
train acc:  0.8203125
train loss:  0.4057973325252533
train gradient:  0.3209026019640086
iteration : 5591
train acc:  0.890625
train loss:  0.2882726788520813
train gradient:  0.20905664042392785
iteration : 5592
train acc:  0.796875
train loss:  0.3920666575431824
train gradient:  0.26743514288914405
iteration : 5593
train acc:  0.8046875
train loss:  0.41522520780563354
train gradient:  0.27631658114189056
iteration : 5594
train acc:  0.8203125
train loss:  0.38043975830078125
train gradient:  0.23208519814660375
iteration : 5595
train acc:  0.828125
train loss:  0.39397817850112915
train gradient:  0.2616044012855144
iteration : 5596
train acc:  0.8671875
train loss:  0.3212278187274933
train gradient:  0.15160522575880403
iteration : 5597
train acc:  0.8359375
train loss:  0.3899969458580017
train gradient:  0.19238129531937076
iteration : 5598
train acc:  0.859375
train loss:  0.2973411977291107
train gradient:  0.22926321778778286
iteration : 5599
train acc:  0.875
train loss:  0.345190167427063
train gradient:  0.20068435029240966
iteration : 5600
train acc:  0.8359375
train loss:  0.3517841100692749
train gradient:  0.18069213431731526
iteration : 5601
train acc:  0.8203125
train loss:  0.3250502943992615
train gradient:  0.17042031564294366
iteration : 5602
train acc:  0.859375
train loss:  0.2955361306667328
train gradient:  0.16094345332048074
iteration : 5603
train acc:  0.875
train loss:  0.3803378939628601
train gradient:  0.2596416514519348
iteration : 5604
train acc:  0.8046875
train loss:  0.4161369204521179
train gradient:  0.28416402366354593
iteration : 5605
train acc:  0.90625
train loss:  0.28057608008384705
train gradient:  0.10224078039851388
iteration : 5606
train acc:  0.8671875
train loss:  0.30241990089416504
train gradient:  0.15775031234710507
iteration : 5607
train acc:  0.8671875
train loss:  0.29625165462493896
train gradient:  0.12603866793994936
iteration : 5608
train acc:  0.875
train loss:  0.3154832124710083
train gradient:  0.16102134050438693
iteration : 5609
train acc:  0.84375
train loss:  0.30406951904296875
train gradient:  0.13357881903002966
iteration : 5610
train acc:  0.8125
train loss:  0.43983396887779236
train gradient:  0.3136457288935594
iteration : 5611
train acc:  0.8671875
train loss:  0.29863226413726807
train gradient:  0.19168551023307917
iteration : 5612
train acc:  0.8515625
train loss:  0.3165433704853058
train gradient:  0.20712387904957333
iteration : 5613
train acc:  0.8203125
train loss:  0.35596901178359985
train gradient:  0.23448656680777213
iteration : 5614
train acc:  0.8203125
train loss:  0.3672453761100769
train gradient:  0.2172648070409613
iteration : 5615
train acc:  0.90625
train loss:  0.2690075933933258
train gradient:  0.20044688464356722
iteration : 5616
train acc:  0.84375
train loss:  0.3832704424858093
train gradient:  0.16466951336686386
iteration : 5617
train acc:  0.859375
train loss:  0.29827001690864563
train gradient:  0.13427384668354364
iteration : 5618
train acc:  0.8515625
train loss:  0.35414615273475647
train gradient:  0.45348977792823697
iteration : 5619
train acc:  0.8125
train loss:  0.42679646611213684
train gradient:  0.30756443196530225
iteration : 5620
train acc:  0.875
train loss:  0.35942238569259644
train gradient:  0.18658207747027228
iteration : 5621
train acc:  0.8359375
train loss:  0.319768488407135
train gradient:  0.18473009205812152
iteration : 5622
train acc:  0.859375
train loss:  0.28723564743995667
train gradient:  0.15329375214866597
iteration : 5623
train acc:  0.859375
train loss:  0.30283281207084656
train gradient:  0.21887003821154907
iteration : 5624
train acc:  0.8125
train loss:  0.41479820013046265
train gradient:  0.23636963526860685
iteration : 5625
train acc:  0.875
train loss:  0.290591835975647
train gradient:  0.13411060725776114
iteration : 5626
train acc:  0.859375
train loss:  0.3007190525531769
train gradient:  0.21772362705600865
iteration : 5627
train acc:  0.859375
train loss:  0.36758166551589966
train gradient:  0.28864973071667777
iteration : 5628
train acc:  0.8359375
train loss:  0.41052472591400146
train gradient:  0.3091887872227693
iteration : 5629
train acc:  0.84375
train loss:  0.3168719410896301
train gradient:  0.18648132660624003
iteration : 5630
train acc:  0.859375
train loss:  0.35239890217781067
train gradient:  0.3939442532296276
iteration : 5631
train acc:  0.7890625
train loss:  0.3614223897457123
train gradient:  0.2590764145707146
iteration : 5632
train acc:  0.8125
train loss:  0.3634607791900635
train gradient:  0.2575375982549708
iteration : 5633
train acc:  0.84375
train loss:  0.3324930667877197
train gradient:  0.3024113660656048
iteration : 5634
train acc:  0.796875
train loss:  0.4357765316963196
train gradient:  0.2959413231767497
iteration : 5635
train acc:  0.8671875
train loss:  0.29397884011268616
train gradient:  0.15490752105955533
iteration : 5636
train acc:  0.859375
train loss:  0.3222116529941559
train gradient:  0.259147663026711
iteration : 5637
train acc:  0.84375
train loss:  0.3337956666946411
train gradient:  0.17287650672507218
iteration : 5638
train acc:  0.859375
train loss:  0.322441041469574
train gradient:  0.1493309334461396
iteration : 5639
train acc:  0.890625
train loss:  0.25343164801597595
train gradient:  0.18929483067127226
iteration : 5640
train acc:  0.8828125
train loss:  0.3034738302230835
train gradient:  0.145888417874338
iteration : 5641
train acc:  0.8515625
train loss:  0.3448027968406677
train gradient:  0.21196862346517348
iteration : 5642
train acc:  0.90625
train loss:  0.22742681205272675
train gradient:  0.1487669134898246
iteration : 5643
train acc:  0.84375
train loss:  0.34195852279663086
train gradient:  0.22577347403225806
iteration : 5644
train acc:  0.859375
train loss:  0.30728986859321594
train gradient:  0.1428423966764629
iteration : 5645
train acc:  0.859375
train loss:  0.26582401990890503
train gradient:  0.16731354038924728
iteration : 5646
train acc:  0.8203125
train loss:  0.3561890125274658
train gradient:  0.24615307005567771
iteration : 5647
train acc:  0.9453125
train loss:  0.22363780438899994
train gradient:  0.12553402175290418
iteration : 5648
train acc:  0.8203125
train loss:  0.34492725133895874
train gradient:  0.21718843276435656
iteration : 5649
train acc:  0.84375
train loss:  0.33894720673561096
train gradient:  0.21251590692188033
iteration : 5650
train acc:  0.8359375
train loss:  0.3553713262081146
train gradient:  0.18920538987842342
iteration : 5651
train acc:  0.859375
train loss:  0.32328587770462036
train gradient:  0.4270203842527019
iteration : 5652
train acc:  0.8671875
train loss:  0.3202050030231476
train gradient:  0.2370072584276792
iteration : 5653
train acc:  0.8515625
train loss:  0.2891457676887512
train gradient:  0.19554558489748214
iteration : 5654
train acc:  0.8828125
train loss:  0.3323873281478882
train gradient:  0.25753409560419116
iteration : 5655
train acc:  0.84375
train loss:  0.33806735277175903
train gradient:  0.2913965112721709
iteration : 5656
train acc:  0.78125
train loss:  0.4306408762931824
train gradient:  0.28433513854205805
iteration : 5657
train acc:  0.8515625
train loss:  0.32601869106292725
train gradient:  0.1610392562812092
iteration : 5658
train acc:  0.8359375
train loss:  0.34156978130340576
train gradient:  0.24012205390671587
iteration : 5659
train acc:  0.890625
train loss:  0.29006636142730713
train gradient:  0.18878980835329112
iteration : 5660
train acc:  0.875
train loss:  0.3281497359275818
train gradient:  0.22436486792079224
iteration : 5661
train acc:  0.71875
train loss:  0.4830421805381775
train gradient:  0.3358113749833872
iteration : 5662
train acc:  0.8359375
train loss:  0.3031010627746582
train gradient:  0.21397995257949917
iteration : 5663
train acc:  0.875
train loss:  0.2695430815219879
train gradient:  0.11123298953239517
iteration : 5664
train acc:  0.8046875
train loss:  0.3913169801235199
train gradient:  0.2886911074464619
iteration : 5665
train acc:  0.84375
train loss:  0.3878132700920105
train gradient:  0.3469814589146178
iteration : 5666
train acc:  0.8515625
train loss:  0.34732815623283386
train gradient:  0.22687796872267643
iteration : 5667
train acc:  0.8671875
train loss:  0.3031347990036011
train gradient:  0.17161542755418743
iteration : 5668
train acc:  0.84375
train loss:  0.35970669984817505
train gradient:  0.36723291548941894
iteration : 5669
train acc:  0.890625
train loss:  0.27678221464157104
train gradient:  0.16866623216943677
iteration : 5670
train acc:  0.84375
train loss:  0.36704015731811523
train gradient:  0.24144914993065744
iteration : 5671
train acc:  0.8671875
train loss:  0.29506316781044006
train gradient:  0.1741704947226712
iteration : 5672
train acc:  0.8203125
train loss:  0.3373986780643463
train gradient:  0.3058532924009743
iteration : 5673
train acc:  0.796875
train loss:  0.4438796043395996
train gradient:  0.4323900231974144
iteration : 5674
train acc:  0.8984375
train loss:  0.2610865831375122
train gradient:  0.13625846946838835
iteration : 5675
train acc:  0.890625
train loss:  0.29561883211135864
train gradient:  0.2573469990226584
iteration : 5676
train acc:  0.8671875
train loss:  0.34033143520355225
train gradient:  0.3155579192837404
iteration : 5677
train acc:  0.8046875
train loss:  0.431626558303833
train gradient:  0.390787671416666
iteration : 5678
train acc:  0.921875
train loss:  0.21926890313625336
train gradient:  0.09261106047748335
iteration : 5679
train acc:  0.8671875
train loss:  0.3394009470939636
train gradient:  0.22467345254948345
iteration : 5680
train acc:  0.84375
train loss:  0.29952478408813477
train gradient:  0.20501570939545707
iteration : 5681
train acc:  0.859375
train loss:  0.30495285987854004
train gradient:  0.19956191662665843
iteration : 5682
train acc:  0.8046875
train loss:  0.403508722782135
train gradient:  0.37832245664972747
iteration : 5683
train acc:  0.8828125
train loss:  0.2944253981113434
train gradient:  0.19919537341953183
iteration : 5684
train acc:  0.8515625
train loss:  0.29678720235824585
train gradient:  0.29419566740614933
iteration : 5685
train acc:  0.78125
train loss:  0.4313037693500519
train gradient:  0.37314349922225454
iteration : 5686
train acc:  0.8984375
train loss:  0.3017916679382324
train gradient:  0.15492619994325196
iteration : 5687
train acc:  0.8515625
train loss:  0.39079225063323975
train gradient:  0.25079980620355524
iteration : 5688
train acc:  0.8828125
train loss:  0.3141573667526245
train gradient:  0.16842375714185212
iteration : 5689
train acc:  0.875
train loss:  0.2925030589103699
train gradient:  0.19449354873338012
iteration : 5690
train acc:  0.8515625
train loss:  0.3731168508529663
train gradient:  0.300584768406047
iteration : 5691
train acc:  0.8828125
train loss:  0.2963290810585022
train gradient:  0.1928525022673977
iteration : 5692
train acc:  0.84375
train loss:  0.4378368556499481
train gradient:  0.4408593016089927
iteration : 5693
train acc:  0.8125
train loss:  0.4058164954185486
train gradient:  0.28328032787097635
iteration : 5694
train acc:  0.828125
train loss:  0.3515840768814087
train gradient:  0.2132744494136436
iteration : 5695
train acc:  0.8046875
train loss:  0.41658830642700195
train gradient:  0.26939082118812463
iteration : 5696
train acc:  0.875
train loss:  0.3169313073158264
train gradient:  0.2557602664347378
iteration : 5697
train acc:  0.8515625
train loss:  0.29847437143325806
train gradient:  0.21018427899971673
iteration : 5698
train acc:  0.84375
train loss:  0.3340868353843689
train gradient:  0.2281495704470218
iteration : 5699
train acc:  0.8515625
train loss:  0.35921710729599
train gradient:  0.2433144090251302
iteration : 5700
train acc:  0.8828125
train loss:  0.42141759395599365
train gradient:  0.355947664926485
iteration : 5701
train acc:  0.8515625
train loss:  0.3285617530345917
train gradient:  0.20081416437587057
iteration : 5702
train acc:  0.8515625
train loss:  0.36356109380722046
train gradient:  0.1825237616861461
iteration : 5703
train acc:  0.8203125
train loss:  0.3790747821331024
train gradient:  0.2868004095733483
iteration : 5704
train acc:  0.8984375
train loss:  0.27704083919525146
train gradient:  0.1982228035255912
iteration : 5705
train acc:  0.859375
train loss:  0.38948655128479004
train gradient:  0.42323108997104236
iteration : 5706
train acc:  0.8359375
train loss:  0.3339521884918213
train gradient:  0.21429105149208993
iteration : 5707
train acc:  0.8359375
train loss:  0.3942980468273163
train gradient:  0.24083194254550228
iteration : 5708
train acc:  0.8203125
train loss:  0.39267516136169434
train gradient:  0.2617977541712395
iteration : 5709
train acc:  0.828125
train loss:  0.3357264995574951
train gradient:  0.17324977527447996
iteration : 5710
train acc:  0.8515625
train loss:  0.33290717005729675
train gradient:  0.21193701073915377
iteration : 5711
train acc:  0.875
train loss:  0.2851916551589966
train gradient:  0.24034259900546256
iteration : 5712
train acc:  0.8828125
train loss:  0.25594985485076904
train gradient:  0.14791015223319898
iteration : 5713
train acc:  0.859375
train loss:  0.29436805844306946
train gradient:  0.1982751195301301
iteration : 5714
train acc:  0.8984375
train loss:  0.30506637692451477
train gradient:  0.20589520993496577
iteration : 5715
train acc:  0.8203125
train loss:  0.4308241903781891
train gradient:  0.350210929746095
iteration : 5716
train acc:  0.859375
train loss:  0.30225512385368347
train gradient:  0.1497647135536062
iteration : 5717
train acc:  0.859375
train loss:  0.3550705909729004
train gradient:  0.19482560580512975
iteration : 5718
train acc:  0.8828125
train loss:  0.2819599509239197
train gradient:  0.2757228370705861
iteration : 5719
train acc:  0.84375
train loss:  0.35724326968193054
train gradient:  0.2588118354918118
iteration : 5720
train acc:  0.8984375
train loss:  0.297326922416687
train gradient:  0.16320590529532203
iteration : 5721
train acc:  0.84375
train loss:  0.37337493896484375
train gradient:  0.2354540380481824
iteration : 5722
train acc:  0.8125
train loss:  0.36540353298187256
train gradient:  0.31559560264806097
iteration : 5723
train acc:  0.84375
train loss:  0.29532182216644287
train gradient:  0.19853265094459147
iteration : 5724
train acc:  0.78125
train loss:  0.46619534492492676
train gradient:  0.3651322712711719
iteration : 5725
train acc:  0.796875
train loss:  0.3520132899284363
train gradient:  0.3403246984265693
iteration : 5726
train acc:  0.8359375
train loss:  0.34974807500839233
train gradient:  0.25126926572388714
iteration : 5727
train acc:  0.9140625
train loss:  0.24000267684459686
train gradient:  0.19260622676144995
iteration : 5728
train acc:  0.8359375
train loss:  0.38118213415145874
train gradient:  0.1978976840982999
iteration : 5729
train acc:  0.84375
train loss:  0.32784587144851685
train gradient:  0.19379608983604807
iteration : 5730
train acc:  0.875
train loss:  0.3169132471084595
train gradient:  0.18749041566248792
iteration : 5731
train acc:  0.8984375
train loss:  0.2679259181022644
train gradient:  0.13330915513244088
iteration : 5732
train acc:  0.875
train loss:  0.3085494935512543
train gradient:  0.1629325780710853
iteration : 5733
train acc:  0.890625
train loss:  0.2990253269672394
train gradient:  0.1931314405982556
iteration : 5734
train acc:  0.9453125
train loss:  0.2025887668132782
train gradient:  0.11296154500097759
iteration : 5735
train acc:  0.8671875
train loss:  0.3565406799316406
train gradient:  0.221216455991314
iteration : 5736
train acc:  0.84375
train loss:  0.34231847524642944
train gradient:  0.19910531599186593
iteration : 5737
train acc:  0.7890625
train loss:  0.3711313009262085
train gradient:  0.3084818257237985
iteration : 5738
train acc:  0.8671875
train loss:  0.3469410538673401
train gradient:  0.29368177612038027
iteration : 5739
train acc:  0.8515625
train loss:  0.35932040214538574
train gradient:  0.32719775233838827
iteration : 5740
train acc:  0.8515625
train loss:  0.38733339309692383
train gradient:  0.25027409090392067
iteration : 5741
train acc:  0.890625
train loss:  0.3123982846736908
train gradient:  0.306000047267663
iteration : 5742
train acc:  0.84375
train loss:  0.4000350832939148
train gradient:  0.44815115154371915
iteration : 5743
train acc:  0.8203125
train loss:  0.4415202736854553
train gradient:  0.49116796769542065
iteration : 5744
train acc:  0.890625
train loss:  0.28154072165489197
train gradient:  0.17306335243213672
iteration : 5745
train acc:  0.875
train loss:  0.32000017166137695
train gradient:  0.2808030063979136
iteration : 5746
train acc:  0.7734375
train loss:  0.5072630643844604
train gradient:  0.48537790852994706
iteration : 5747
train acc:  0.8359375
train loss:  0.36831337213516235
train gradient:  0.20255256610210393
iteration : 5748
train acc:  0.890625
train loss:  0.3594093322753906
train gradient:  0.2633973895775429
iteration : 5749
train acc:  0.8671875
train loss:  0.3040545582771301
train gradient:  0.2429690320727526
iteration : 5750
train acc:  0.8828125
train loss:  0.3213917315006256
train gradient:  0.16560810497582865
iteration : 5751
train acc:  0.78125
train loss:  0.42065417766571045
train gradient:  0.2810022678233734
iteration : 5752
train acc:  0.890625
train loss:  0.3088906407356262
train gradient:  0.14642722474970504
iteration : 5753
train acc:  0.8359375
train loss:  0.32507261633872986
train gradient:  0.20691159329035513
iteration : 5754
train acc:  0.859375
train loss:  0.3309965133666992
train gradient:  0.18505248472830996
iteration : 5755
train acc:  0.9296875
train loss:  0.23321861028671265
train gradient:  0.13710975171439233
iteration : 5756
train acc:  0.8828125
train loss:  0.28900566697120667
train gradient:  0.27685677806133185
iteration : 5757
train acc:  0.7734375
train loss:  0.5078108310699463
train gradient:  1.0241917407407626
iteration : 5758
train acc:  0.859375
train loss:  0.33270692825317383
train gradient:  0.21827866754271097
iteration : 5759
train acc:  0.84375
train loss:  0.31411996483802795
train gradient:  0.18444543980070768
iteration : 5760
train acc:  0.828125
train loss:  0.35728719830513
train gradient:  0.2562075744922075
iteration : 5761
train acc:  0.828125
train loss:  0.3369079828262329
train gradient:  0.3214643976513063
iteration : 5762
train acc:  0.8359375
train loss:  0.39027678966522217
train gradient:  0.207176060370645
iteration : 5763
train acc:  0.8671875
train loss:  0.282040536403656
train gradient:  0.13771477934560472
iteration : 5764
train acc:  0.828125
train loss:  0.345786988735199
train gradient:  0.255037365161387
iteration : 5765
train acc:  0.796875
train loss:  0.4534602463245392
train gradient:  0.4193058289644511
iteration : 5766
train acc:  0.8671875
train loss:  0.3155604302883148
train gradient:  0.16763910797555992
iteration : 5767
train acc:  0.8359375
train loss:  0.3315049707889557
train gradient:  0.22038367952096496
iteration : 5768
train acc:  0.765625
train loss:  0.3820095658302307
train gradient:  0.21130319977967343
iteration : 5769
train acc:  0.890625
train loss:  0.28972598910331726
train gradient:  0.21915535688288285
iteration : 5770
train acc:  0.8984375
train loss:  0.32996654510498047
train gradient:  0.2598480854913565
iteration : 5771
train acc:  0.8359375
train loss:  0.3327203691005707
train gradient:  0.23514725827006233
iteration : 5772
train acc:  0.859375
train loss:  0.3647494316101074
train gradient:  0.25564930478117187
iteration : 5773
train acc:  0.8671875
train loss:  0.3106885552406311
train gradient:  0.24040258399853462
iteration : 5774
train acc:  0.921875
train loss:  0.2135557383298874
train gradient:  0.13509522684127628
iteration : 5775
train acc:  0.921875
train loss:  0.23226962983608246
train gradient:  0.14347804645721174
iteration : 5776
train acc:  0.875
train loss:  0.34149402379989624
train gradient:  0.3265808294643653
iteration : 5777
train acc:  0.8515625
train loss:  0.3384183645248413
train gradient:  0.24058649250439504
iteration : 5778
train acc:  0.8125
train loss:  0.37752413749694824
train gradient:  0.22802443588502674
iteration : 5779
train acc:  0.8359375
train loss:  0.3921404778957367
train gradient:  0.3376207792762547
iteration : 5780
train acc:  0.859375
train loss:  0.32099199295043945
train gradient:  0.2572232306180736
iteration : 5781
train acc:  0.8671875
train loss:  0.34005099534988403
train gradient:  0.2347439065097133
iteration : 5782
train acc:  0.8046875
train loss:  0.39963799715042114
train gradient:  0.27234338908852984
iteration : 5783
train acc:  0.8125
train loss:  0.3891752362251282
train gradient:  0.3119432786505934
iteration : 5784
train acc:  0.8984375
train loss:  0.30267417430877686
train gradient:  0.1759158988720456
iteration : 5785
train acc:  0.84375
train loss:  0.3718577027320862
train gradient:  0.28424765509819044
iteration : 5786
train acc:  0.8671875
train loss:  0.2923497259616852
train gradient:  0.317522291497252
iteration : 5787
train acc:  0.859375
train loss:  0.34698185324668884
train gradient:  0.2764300834557434
iteration : 5788
train acc:  0.8828125
train loss:  0.29935407638549805
train gradient:  0.19577756785770775
iteration : 5789
train acc:  0.8671875
train loss:  0.32479000091552734
train gradient:  0.34463601488723655
iteration : 5790
train acc:  0.8203125
train loss:  0.4933265447616577
train gradient:  0.45922002949429075
iteration : 5791
train acc:  0.828125
train loss:  0.3919711709022522
train gradient:  0.25318330544143636
iteration : 5792
train acc:  0.796875
train loss:  0.4296097755432129
train gradient:  0.25596378141757564
iteration : 5793
train acc:  0.8203125
train loss:  0.45450878143310547
train gradient:  0.4492977984128856
iteration : 5794
train acc:  0.828125
train loss:  0.3522436022758484
train gradient:  0.3276504017110354
iteration : 5795
train acc:  0.828125
train loss:  0.4018332362174988
train gradient:  0.47615734084133704
iteration : 5796
train acc:  0.7890625
train loss:  0.3883523941040039
train gradient:  0.2912607317839736
iteration : 5797
train acc:  0.8671875
train loss:  0.33340921998023987
train gradient:  0.15571710759017804
iteration : 5798
train acc:  0.890625
train loss:  0.2974180281162262
train gradient:  0.12885963690614194
iteration : 5799
train acc:  0.8515625
train loss:  0.38756853342056274
train gradient:  0.29727806209720964
iteration : 5800
train acc:  0.859375
train loss:  0.3456217348575592
train gradient:  0.2937984445880345
iteration : 5801
train acc:  0.8359375
train loss:  0.40425264835357666
train gradient:  0.2325287871103163
iteration : 5802
train acc:  0.8515625
train loss:  0.3161756992340088
train gradient:  0.22396116868433857
iteration : 5803
train acc:  0.890625
train loss:  0.2727649211883545
train gradient:  0.18662802039556337
iteration : 5804
train acc:  0.828125
train loss:  0.38743066787719727
train gradient:  0.3886069610381631
iteration : 5805
train acc:  0.84375
train loss:  0.3556044399738312
train gradient:  0.1934920301763291
iteration : 5806
train acc:  0.8125
train loss:  0.38897258043289185
train gradient:  0.3197020923033093
iteration : 5807
train acc:  0.90625
train loss:  0.32148081064224243
train gradient:  0.1998800467231675
iteration : 5808
train acc:  0.8515625
train loss:  0.40801042318344116
train gradient:  0.24431659174024195
iteration : 5809
train acc:  0.84375
train loss:  0.3834743797779083
train gradient:  0.2970819405650685
iteration : 5810
train acc:  0.8359375
train loss:  0.39654165506362915
train gradient:  0.20766098044830664
iteration : 5811
train acc:  0.828125
train loss:  0.4331902861595154
train gradient:  0.3021979417921075
iteration : 5812
train acc:  0.8203125
train loss:  0.3510053753852844
train gradient:  0.2795498980251511
iteration : 5813
train acc:  0.84375
train loss:  0.34656861424446106
train gradient:  0.20363023724989693
iteration : 5814
train acc:  0.8671875
train loss:  0.34646475315093994
train gradient:  0.2700284515452427
iteration : 5815
train acc:  0.8125
train loss:  0.3728444576263428
train gradient:  0.2502068571131219
iteration : 5816
train acc:  0.84375
train loss:  0.3568980097770691
train gradient:  0.2711989477440255
iteration : 5817
train acc:  0.8671875
train loss:  0.44247323274612427
train gradient:  0.2286225285318604
iteration : 5818
train acc:  0.8359375
train loss:  0.3659904897212982
train gradient:  0.21580897924957465
iteration : 5819
train acc:  0.890625
train loss:  0.3100358247756958
train gradient:  0.1954588397744767
iteration : 5820
train acc:  0.7734375
train loss:  0.49985915422439575
train gradient:  0.2864638149437919
iteration : 5821
train acc:  0.84375
train loss:  0.3440279960632324
train gradient:  0.18508324682870636
iteration : 5822
train acc:  0.7421875
train loss:  0.5069568157196045
train gradient:  0.3571686367610981
iteration : 5823
train acc:  0.84375
train loss:  0.31738924980163574
train gradient:  0.20784477527982573
iteration : 5824
train acc:  0.796875
train loss:  0.4304298162460327
train gradient:  0.3305320124393439
iteration : 5825
train acc:  0.8828125
train loss:  0.3124818205833435
train gradient:  0.23548838722021662
iteration : 5826
train acc:  0.8125
train loss:  0.4005191922187805
train gradient:  0.39223472344675403
iteration : 5827
train acc:  0.8671875
train loss:  0.3019300103187561
train gradient:  0.17174692839375297
iteration : 5828
train acc:  0.859375
train loss:  0.3355609178543091
train gradient:  0.1640676170208601
iteration : 5829
train acc:  0.84375
train loss:  0.31346672773361206
train gradient:  0.2367744191843414
iteration : 5830
train acc:  0.875
train loss:  0.33420082926750183
train gradient:  0.1879528565101623
iteration : 5831
train acc:  0.828125
train loss:  0.3557601273059845
train gradient:  0.20688101857033908
iteration : 5832
train acc:  0.8359375
train loss:  0.35443949699401855
train gradient:  0.24454165541470274
iteration : 5833
train acc:  0.8125
train loss:  0.38574403524398804
train gradient:  0.19502270446134276
iteration : 5834
train acc:  0.875
train loss:  0.31059572100639343
train gradient:  0.20072255068571415
iteration : 5835
train acc:  0.890625
train loss:  0.3007774353027344
train gradient:  0.15093687967334385
iteration : 5836
train acc:  0.8671875
train loss:  0.3241080641746521
train gradient:  0.2048655923577951
iteration : 5837
train acc:  0.8984375
train loss:  0.23894090950489044
train gradient:  0.12402590130064807
iteration : 5838
train acc:  0.8515625
train loss:  0.32694923877716064
train gradient:  0.19032472972299347
iteration : 5839
train acc:  0.7734375
train loss:  0.44295963644981384
train gradient:  0.2825803673504564
iteration : 5840
train acc:  0.828125
train loss:  0.38543471693992615
train gradient:  0.251185903666645
iteration : 5841
train acc:  0.8671875
train loss:  0.32321029901504517
train gradient:  0.24951038232984318
iteration : 5842
train acc:  0.765625
train loss:  0.42248350381851196
train gradient:  0.35515971997881607
iteration : 5843
train acc:  0.8828125
train loss:  0.3367568254470825
train gradient:  0.35618673164756687
iteration : 5844
train acc:  0.828125
train loss:  0.34150630235671997
train gradient:  0.22153601038330395
iteration : 5845
train acc:  0.84375
train loss:  0.30869680643081665
train gradient:  0.1908541108534215
iteration : 5846
train acc:  0.828125
train loss:  0.3764380216598511
train gradient:  0.28883251688243095
iteration : 5847
train acc:  0.9140625
train loss:  0.26970338821411133
train gradient:  0.1837563473885181
iteration : 5848
train acc:  0.828125
train loss:  0.3477104902267456
train gradient:  0.2628757751007198
iteration : 5849
train acc:  0.859375
train loss:  0.33099833130836487
train gradient:  0.23520739186152928
iteration : 5850
train acc:  0.8125
train loss:  0.41565901041030884
train gradient:  0.27755688945153234
iteration : 5851
train acc:  0.8515625
train loss:  0.3824564218521118
train gradient:  0.27038434575318887
iteration : 5852
train acc:  0.8359375
train loss:  0.39549171924591064
train gradient:  0.2147040386763119
iteration : 5853
train acc:  0.828125
train loss:  0.3477775454521179
train gradient:  0.18615735988072374
iteration : 5854
train acc:  0.828125
train loss:  0.3454356789588928
train gradient:  0.2758571608478482
iteration : 5855
train acc:  0.859375
train loss:  0.35683944821357727
train gradient:  0.17382510679674815
iteration : 5856
train acc:  0.828125
train loss:  0.375870943069458
train gradient:  0.21573620567884716
iteration : 5857
train acc:  0.828125
train loss:  0.32749831676483154
train gradient:  0.2650110106911548
iteration : 5858
train acc:  0.84375
train loss:  0.38498738408088684
train gradient:  0.24806207122226906
iteration : 5859
train acc:  0.8359375
train loss:  0.325004905462265
train gradient:  0.24055427060614737
iteration : 5860
train acc:  0.765625
train loss:  0.4448888301849365
train gradient:  0.3288669087453996
iteration : 5861
train acc:  0.859375
train loss:  0.36439448595046997
train gradient:  0.20788357455329318
iteration : 5862
train acc:  0.9296875
train loss:  0.26368701457977295
train gradient:  0.16714792671571724
iteration : 5863
train acc:  0.8515625
train loss:  0.37100130319595337
train gradient:  0.2660680131211507
iteration : 5864
train acc:  0.9140625
train loss:  0.24153324961662292
train gradient:  0.11012991923964514
iteration : 5865
train acc:  0.890625
train loss:  0.2750706076622009
train gradient:  0.15858433072407047
iteration : 5866
train acc:  0.796875
train loss:  0.42564502358436584
train gradient:  0.3248944048482863
iteration : 5867
train acc:  0.8125
train loss:  0.42170846462249756
train gradient:  0.2423856005580134
iteration : 5868
train acc:  0.8359375
train loss:  0.3333657383918762
train gradient:  0.192454735405107
iteration : 5869
train acc:  0.8515625
train loss:  0.4024360775947571
train gradient:  0.20153258471466612
iteration : 5870
train acc:  0.828125
train loss:  0.3769721984863281
train gradient:  0.2237740485246631
iteration : 5871
train acc:  0.8359375
train loss:  0.4253482222557068
train gradient:  0.36713047191862846
iteration : 5872
train acc:  0.84375
train loss:  0.29449281096458435
train gradient:  0.15997529085935214
iteration : 5873
train acc:  0.8203125
train loss:  0.3693299889564514
train gradient:  0.2470783764344702
iteration : 5874
train acc:  0.78125
train loss:  0.4359022378921509
train gradient:  0.3728336177617876
iteration : 5875
train acc:  0.8359375
train loss:  0.4034823179244995
train gradient:  0.2437300381189505
iteration : 5876
train acc:  0.84375
train loss:  0.38308942317962646
train gradient:  0.2893745020841575
iteration : 5877
train acc:  0.8671875
train loss:  0.3450004458427429
train gradient:  0.26725477726087066
iteration : 5878
train acc:  0.84375
train loss:  0.33263713121414185
train gradient:  0.24251410199941695
iteration : 5879
train acc:  0.859375
train loss:  0.3266529440879822
train gradient:  0.14774641053680482
iteration : 5880
train acc:  0.8515625
train loss:  0.3428433835506439
train gradient:  0.19607924745141364
iteration : 5881
train acc:  0.84375
train loss:  0.33890068531036377
train gradient:  0.17555790581102004
iteration : 5882
train acc:  0.875
train loss:  0.3673558235168457
train gradient:  0.21904673536169614
iteration : 5883
train acc:  0.8828125
train loss:  0.3346869945526123
train gradient:  0.14465477152552353
iteration : 5884
train acc:  0.84375
train loss:  0.3541587293148041
train gradient:  0.16788751032546523
iteration : 5885
train acc:  0.8828125
train loss:  0.3239312767982483
train gradient:  0.1949550004732682
iteration : 5886
train acc:  0.8359375
train loss:  0.3935324549674988
train gradient:  0.25891473534843346
iteration : 5887
train acc:  0.8515625
train loss:  0.3160623610019684
train gradient:  0.16544902501368575
iteration : 5888
train acc:  0.8671875
train loss:  0.280098557472229
train gradient:  0.19685180412615455
iteration : 5889
train acc:  0.8984375
train loss:  0.2613000273704529
train gradient:  0.17162835278362693
iteration : 5890
train acc:  0.875
train loss:  0.3786447048187256
train gradient:  0.34750435372537447
iteration : 5891
train acc:  0.859375
train loss:  0.3543930947780609
train gradient:  0.21172662727100347
iteration : 5892
train acc:  0.859375
train loss:  0.3067960739135742
train gradient:  0.15744028719728181
iteration : 5893
train acc:  0.8203125
train loss:  0.4302058815956116
train gradient:  0.3073288746189838
iteration : 5894
train acc:  0.8046875
train loss:  0.4227626323699951
train gradient:  0.3921654361894431
iteration : 5895
train acc:  0.828125
train loss:  0.3624669015407562
train gradient:  0.271616333574184
iteration : 5896
train acc:  0.890625
train loss:  0.30158716440200806
train gradient:  0.1616188001773695
iteration : 5897
train acc:  0.859375
train loss:  0.3065420389175415
train gradient:  0.21022538034282812
iteration : 5898
train acc:  0.8046875
train loss:  0.4227314293384552
train gradient:  0.2127558992515579
iteration : 5899
train acc:  0.8046875
train loss:  0.3737856149673462
train gradient:  0.2450466910272795
iteration : 5900
train acc:  0.8359375
train loss:  0.4475403428077698
train gradient:  0.43345207470539876
iteration : 5901
train acc:  0.828125
train loss:  0.3515406548976898
train gradient:  0.1777943266326501
iteration : 5902
train acc:  0.8515625
train loss:  0.3167342245578766
train gradient:  0.14518060988802445
iteration : 5903
train acc:  0.90625
train loss:  0.27894139289855957
train gradient:  0.16169705913441418
iteration : 5904
train acc:  0.8125
train loss:  0.3459438681602478
train gradient:  0.1848753664009071
iteration : 5905
train acc:  0.8515625
train loss:  0.3305400013923645
train gradient:  0.19355201715466022
iteration : 5906
train acc:  0.8203125
train loss:  0.3574501574039459
train gradient:  0.2887595687554092
iteration : 5907
train acc:  0.8515625
train loss:  0.36207789182662964
train gradient:  0.2446801707864436
iteration : 5908
train acc:  0.8828125
train loss:  0.33681991696357727
train gradient:  0.1756960744965791
iteration : 5909
train acc:  0.84375
train loss:  0.38037723302841187
train gradient:  0.18154052015319125
iteration : 5910
train acc:  0.9140625
train loss:  0.23446616530418396
train gradient:  0.12301569572431434
iteration : 5911
train acc:  0.8515625
train loss:  0.30221259593963623
train gradient:  0.2062666063940337
iteration : 5912
train acc:  0.8203125
train loss:  0.3345779776573181
train gradient:  0.227744177736654
iteration : 5913
train acc:  0.8828125
train loss:  0.28943103551864624
train gradient:  0.183440346381327
iteration : 5914
train acc:  0.7734375
train loss:  0.4848012924194336
train gradient:  0.3313881516795412
iteration : 5915
train acc:  0.828125
train loss:  0.3510895371437073
train gradient:  0.21447875745747852
iteration : 5916
train acc:  0.8359375
train loss:  0.38382500410079956
train gradient:  0.2907758785439853
iteration : 5917
train acc:  0.828125
train loss:  0.36644473671913147
train gradient:  0.19898113767204834
iteration : 5918
train acc:  0.8515625
train loss:  0.36700284481048584
train gradient:  0.23745629036296445
iteration : 5919
train acc:  0.859375
train loss:  0.3766273558139801
train gradient:  0.2416359590304158
iteration : 5920
train acc:  0.84375
train loss:  0.35639822483062744
train gradient:  0.28660179875409064
iteration : 5921
train acc:  0.8203125
train loss:  0.3666439950466156
train gradient:  0.2683404562360674
iteration : 5922
train acc:  0.8359375
train loss:  0.42956483364105225
train gradient:  0.3026553363638567
iteration : 5923
train acc:  0.859375
train loss:  0.3772343397140503
train gradient:  0.1829862520792854
iteration : 5924
train acc:  0.8125
train loss:  0.4035753607749939
train gradient:  0.2625225508989327
iteration : 5925
train acc:  0.8515625
train loss:  0.3532205820083618
train gradient:  0.1844054757558815
iteration : 5926
train acc:  0.8515625
train loss:  0.3421591520309448
train gradient:  0.23699067056930173
iteration : 5927
train acc:  0.7734375
train loss:  0.44837015867233276
train gradient:  0.2760630480697705
iteration : 5928
train acc:  0.84375
train loss:  0.37757331132888794
train gradient:  0.20594254118591154
iteration : 5929
train acc:  0.875
train loss:  0.2885485887527466
train gradient:  0.2149139353072581
iteration : 5930
train acc:  0.875
train loss:  0.3287414610385895
train gradient:  0.2730338745792353
iteration : 5931
train acc:  0.8203125
train loss:  0.37413880228996277
train gradient:  0.26079496177111056
iteration : 5932
train acc:  0.875
train loss:  0.32536110281944275
train gradient:  0.18405407447953614
iteration : 5933
train acc:  0.859375
train loss:  0.30069413781166077
train gradient:  0.2010007633270356
iteration : 5934
train acc:  0.8515625
train loss:  0.3953361511230469
train gradient:  0.19386211882578858
iteration : 5935
train acc:  0.890625
train loss:  0.3057973384857178
train gradient:  0.19256415325499665
iteration : 5936
train acc:  0.8359375
train loss:  0.33271533250808716
train gradient:  0.1624103749280496
iteration : 5937
train acc:  0.8828125
train loss:  0.32058417797088623
train gradient:  0.165499764893201
iteration : 5938
train acc:  0.8515625
train loss:  0.3034583032131195
train gradient:  0.17130596549133137
iteration : 5939
train acc:  0.796875
train loss:  0.4218631386756897
train gradient:  0.3273004814615643
iteration : 5940
train acc:  0.859375
train loss:  0.2805717885494232
train gradient:  0.1950975602308413
iteration : 5941
train acc:  0.8359375
train loss:  0.35511109232902527
train gradient:  0.1550800815591556
iteration : 5942
train acc:  0.8828125
train loss:  0.2987635135650635
train gradient:  0.14603121222391666
iteration : 5943
train acc:  0.796875
train loss:  0.4082026183605194
train gradient:  0.2676076413871108
iteration : 5944
train acc:  0.8515625
train loss:  0.3452131450176239
train gradient:  0.17979728900922878
iteration : 5945
train acc:  0.84375
train loss:  0.38934069871902466
train gradient:  0.29105166325715276
iteration : 5946
train acc:  0.84375
train loss:  0.3595864176750183
train gradient:  0.15886713259549962
iteration : 5947
train acc:  0.84375
train loss:  0.3794447183609009
train gradient:  0.16608898394059177
iteration : 5948
train acc:  0.8671875
train loss:  0.31421566009521484
train gradient:  0.18828585458756605
iteration : 5949
train acc:  0.796875
train loss:  0.43025919795036316
train gradient:  0.2845501488327948
iteration : 5950
train acc:  0.8671875
train loss:  0.30843713879585266
train gradient:  0.1242570936917278
iteration : 5951
train acc:  0.8515625
train loss:  0.3184906542301178
train gradient:  0.19046468253329304
iteration : 5952
train acc:  0.7734375
train loss:  0.4584868550300598
train gradient:  0.3410387041415196
iteration : 5953
train acc:  0.859375
train loss:  0.3480491638183594
train gradient:  0.1536537939983757
iteration : 5954
train acc:  0.7890625
train loss:  0.4692991375923157
train gradient:  0.3572795647449083
iteration : 5955
train acc:  0.875
train loss:  0.37843871116638184
train gradient:  0.18441017828231454
iteration : 5956
train acc:  0.8671875
train loss:  0.3140760660171509
train gradient:  0.17968743724021258
iteration : 5957
train acc:  0.8515625
train loss:  0.34408268332481384
train gradient:  0.21742086342116865
iteration : 5958
train acc:  0.875
train loss:  0.27769923210144043
train gradient:  0.2764414819294746
iteration : 5959
train acc:  0.8671875
train loss:  0.32152390480041504
train gradient:  0.16814893564670974
iteration : 5960
train acc:  0.828125
train loss:  0.370572566986084
train gradient:  0.2529413200628113
iteration : 5961
train acc:  0.859375
train loss:  0.32833734154701233
train gradient:  0.17557796605505072
iteration : 5962
train acc:  0.84375
train loss:  0.3218652009963989
train gradient:  0.2552055007634211
iteration : 5963
train acc:  0.8359375
train loss:  0.3977194130420685
train gradient:  0.19263002567824677
iteration : 5964
train acc:  0.859375
train loss:  0.34453916549682617
train gradient:  0.170213777947947
iteration : 5965
train acc:  0.8125
train loss:  0.37900957465171814
train gradient:  0.24890266421324522
iteration : 5966
train acc:  0.8671875
train loss:  0.3200616240501404
train gradient:  0.13921152064393827
iteration : 5967
train acc:  0.875
train loss:  0.27111202478408813
train gradient:  0.24518933016211492
iteration : 5968
train acc:  0.8203125
train loss:  0.3958345651626587
train gradient:  0.20239356506753836
iteration : 5969
train acc:  0.875
train loss:  0.3427475690841675
train gradient:  0.18485070303627277
iteration : 5970
train acc:  0.8125
train loss:  0.33836063742637634
train gradient:  0.2220041427979474
iteration : 5971
train acc:  0.90625
train loss:  0.29747438430786133
train gradient:  0.145965733418579
iteration : 5972
train acc:  0.8671875
train loss:  0.319102942943573
train gradient:  0.1860708099890528
iteration : 5973
train acc:  0.875
train loss:  0.3232066035270691
train gradient:  0.18984212755700594
iteration : 5974
train acc:  0.8203125
train loss:  0.35381025075912476
train gradient:  0.2798135417223196
iteration : 5975
train acc:  0.8125
train loss:  0.4437111020088196
train gradient:  0.4280987769936995
iteration : 5976
train acc:  0.84375
train loss:  0.34007546305656433
train gradient:  0.1355098560768505
iteration : 5977
train acc:  0.8984375
train loss:  0.2660777270793915
train gradient:  0.10932943662592325
iteration : 5978
train acc:  0.8515625
train loss:  0.3097226619720459
train gradient:  0.13414431867624751
iteration : 5979
train acc:  0.9140625
train loss:  0.2927781641483307
train gradient:  0.2030527565380436
iteration : 5980
train acc:  0.84375
train loss:  0.3956379294395447
train gradient:  0.23017733693609127
iteration : 5981
train acc:  0.8984375
train loss:  0.2858644723892212
train gradient:  0.1478177034891727
iteration : 5982
train acc:  0.859375
train loss:  0.3118353486061096
train gradient:  0.20341885395492892
iteration : 5983
train acc:  0.8515625
train loss:  0.3261111080646515
train gradient:  0.21495504312581326
iteration : 5984
train acc:  0.8359375
train loss:  0.35733628273010254
train gradient:  0.21546374065052928
iteration : 5985
train acc:  0.8984375
train loss:  0.27237647771835327
train gradient:  0.15678619946136757
iteration : 5986
train acc:  0.828125
train loss:  0.34502655267715454
train gradient:  0.22998903108263563
iteration : 5987
train acc:  0.8515625
train loss:  0.3732210695743561
train gradient:  0.16337000065112894
iteration : 5988
train acc:  0.875
train loss:  0.31212854385375977
train gradient:  0.19100581809534597
iteration : 5989
train acc:  0.8828125
train loss:  0.2881123423576355
train gradient:  0.11865628940208754
iteration : 5990
train acc:  0.8359375
train loss:  0.4548199772834778
train gradient:  0.25194627825471333
iteration : 5991
train acc:  0.828125
train loss:  0.3890647292137146
train gradient:  0.35741877760250357
iteration : 5992
train acc:  0.859375
train loss:  0.36295121908187866
train gradient:  0.21218079230512532
iteration : 5993
train acc:  0.8984375
train loss:  0.32818228006362915
train gradient:  0.16455338159831606
iteration : 5994
train acc:  0.8984375
train loss:  0.32644855976104736
train gradient:  0.12761276096970336
iteration : 5995
train acc:  0.8046875
train loss:  0.42688751220703125
train gradient:  0.3656591282038894
iteration : 5996
train acc:  0.8828125
train loss:  0.35646432638168335
train gradient:  0.16571314984045035
iteration : 5997
train acc:  0.84375
train loss:  0.3742486536502838
train gradient:  0.32000150026466756
iteration : 5998
train acc:  0.7890625
train loss:  0.4694075286388397
train gradient:  0.27112497127520957
iteration : 5999
train acc:  0.890625
train loss:  0.32676512002944946
train gradient:  0.18530574046326515
iteration : 6000
train acc:  0.828125
train loss:  0.3899451494216919
train gradient:  0.2415514652943202
iteration : 6001
train acc:  0.8828125
train loss:  0.28159818053245544
train gradient:  0.17649248042111343
iteration : 6002
train acc:  0.859375
train loss:  0.3347787857055664
train gradient:  0.18250772653996003
iteration : 6003
train acc:  0.8828125
train loss:  0.33932870626449585
train gradient:  0.16641137344375945
iteration : 6004
train acc:  0.828125
train loss:  0.36674681305885315
train gradient:  0.15304968145447578
iteration : 6005
train acc:  0.84375
train loss:  0.3073360323905945
train gradient:  0.13505054103969238
iteration : 6006
train acc:  0.859375
train loss:  0.3022708296775818
train gradient:  0.28288593871657525
iteration : 6007
train acc:  0.8359375
train loss:  0.3650814890861511
train gradient:  0.18515299003921093
iteration : 6008
train acc:  0.828125
train loss:  0.35273510217666626
train gradient:  0.1758620236065821
iteration : 6009
train acc:  0.7890625
train loss:  0.3994753062725067
train gradient:  0.3056390920304637
iteration : 6010
train acc:  0.8984375
train loss:  0.23978443443775177
train gradient:  0.16112052177839004
iteration : 6011
train acc:  0.796875
train loss:  0.3953350782394409
train gradient:  0.2555132753785651
iteration : 6012
train acc:  0.859375
train loss:  0.3314448595046997
train gradient:  0.23463428520028307
iteration : 6013
train acc:  0.8203125
train loss:  0.3901844024658203
train gradient:  0.21947486756156626
iteration : 6014
train acc:  0.8671875
train loss:  0.3375453054904938
train gradient:  0.39050888462435457
iteration : 6015
train acc:  0.8125
train loss:  0.39051175117492676
train gradient:  0.3009611514502206
iteration : 6016
train acc:  0.8046875
train loss:  0.4475310146808624
train gradient:  0.36071533505492165
iteration : 6017
train acc:  0.828125
train loss:  0.37885919213294983
train gradient:  0.2235090099895071
iteration : 6018
train acc:  0.84375
train loss:  0.3402380645275116
train gradient:  0.1963244044511386
iteration : 6019
train acc:  0.8203125
train loss:  0.3486076593399048
train gradient:  0.3044921846370736
iteration : 6020
train acc:  0.84375
train loss:  0.3635125756263733
train gradient:  0.1925599621991936
iteration : 6021
train acc:  0.7578125
train loss:  0.46549907326698303
train gradient:  0.37084688139771294
iteration : 6022
train acc:  0.84375
train loss:  0.36419934034347534
train gradient:  0.3077870331080856
iteration : 6023
train acc:  0.84375
train loss:  0.31797683238983154
train gradient:  0.16663655615753906
iteration : 6024
train acc:  0.8359375
train loss:  0.39954131841659546
train gradient:  0.2370270020587466
iteration : 6025
train acc:  0.875
train loss:  0.3063134551048279
train gradient:  0.1434789298503793
iteration : 6026
train acc:  0.84375
train loss:  0.3166966438293457
train gradient:  0.16229535502500936
iteration : 6027
train acc:  0.828125
train loss:  0.3555694818496704
train gradient:  0.23706115436169273
iteration : 6028
train acc:  0.84375
train loss:  0.35120296478271484
train gradient:  0.19260176264454099
iteration : 6029
train acc:  0.8359375
train loss:  0.37829431891441345
train gradient:  0.33995301971172354
iteration : 6030
train acc:  0.859375
train loss:  0.33508652448654175
train gradient:  0.16002146355176686
iteration : 6031
train acc:  0.84375
train loss:  0.3386797308921814
train gradient:  0.17306712227269072
iteration : 6032
train acc:  0.859375
train loss:  0.3504752516746521
train gradient:  0.3674932424925058
iteration : 6033
train acc:  0.8515625
train loss:  0.29960593581199646
train gradient:  0.19296984460943895
iteration : 6034
train acc:  0.8671875
train loss:  0.3146332800388336
train gradient:  0.13603962395185198
iteration : 6035
train acc:  0.9140625
train loss:  0.26241248846054077
train gradient:  0.13313270861997584
iteration : 6036
train acc:  0.875
train loss:  0.36564239859580994
train gradient:  0.381226452692999
iteration : 6037
train acc:  0.8515625
train loss:  0.355219304561615
train gradient:  0.1344200758102544
iteration : 6038
train acc:  0.828125
train loss:  0.3691611588001251
train gradient:  0.22512490788358416
iteration : 6039
train acc:  0.84375
train loss:  0.3882852792739868
train gradient:  0.3874000312980396
iteration : 6040
train acc:  0.8125
train loss:  0.38800039887428284
train gradient:  0.23993038228604655
iteration : 6041
train acc:  0.890625
train loss:  0.3143584132194519
train gradient:  0.21596976690399897
iteration : 6042
train acc:  0.875
train loss:  0.29633980989456177
train gradient:  0.19927331205901289
iteration : 6043
train acc:  0.84375
train loss:  0.3178803026676178
train gradient:  0.20255117373414266
iteration : 6044
train acc:  0.796875
train loss:  0.3660295605659485
train gradient:  0.27889988126921955
iteration : 6045
train acc:  0.84375
train loss:  0.31295931339263916
train gradient:  0.19035540118640698
iteration : 6046
train acc:  0.890625
train loss:  0.2785438895225525
train gradient:  0.13717613439432286
iteration : 6047
train acc:  0.8515625
train loss:  0.304713636636734
train gradient:  0.28260149078149427
iteration : 6048
train acc:  0.84375
train loss:  0.3297708034515381
train gradient:  0.21763028014402053
iteration : 6049
train acc:  0.8046875
train loss:  0.435668408870697
train gradient:  0.27621639662452724
iteration : 6050
train acc:  0.796875
train loss:  0.3503257930278778
train gradient:  0.17769967231248623
iteration : 6051
train acc:  0.859375
train loss:  0.3504660427570343
train gradient:  0.19117835837207703
iteration : 6052
train acc:  0.8515625
train loss:  0.3135190010070801
train gradient:  0.29239276129113223
iteration : 6053
train acc:  0.8203125
train loss:  0.4080369770526886
train gradient:  0.2577657161425201
iteration : 6054
train acc:  0.828125
train loss:  0.3652644157409668
train gradient:  0.24625899206231489
iteration : 6055
train acc:  0.8515625
train loss:  0.30550509691238403
train gradient:  0.21765928189545625
iteration : 6056
train acc:  0.8515625
train loss:  0.35745882987976074
train gradient:  0.2155381842269515
iteration : 6057
train acc:  0.875
train loss:  0.30769479274749756
train gradient:  0.21183664018012877
iteration : 6058
train acc:  0.8671875
train loss:  0.2896846830844879
train gradient:  0.15781481956036164
iteration : 6059
train acc:  0.9375
train loss:  0.20474304258823395
train gradient:  0.09727905024939457
iteration : 6060
train acc:  0.8203125
train loss:  0.39295417070388794
train gradient:  0.23891218056420763
iteration : 6061
train acc:  0.8828125
train loss:  0.299935519695282
train gradient:  0.147039853144056
iteration : 6062
train acc:  0.84375
train loss:  0.3993745446205139
train gradient:  0.3267582471151486
iteration : 6063
train acc:  0.8515625
train loss:  0.37903138995170593
train gradient:  0.3451856033606877
iteration : 6064
train acc:  0.828125
train loss:  0.4141080379486084
train gradient:  0.25460286830409146
iteration : 6065
train acc:  0.8359375
train loss:  0.36240071058273315
train gradient:  0.20947771819999217
iteration : 6066
train acc:  0.78125
train loss:  0.4904022514820099
train gradient:  0.5250514611400399
iteration : 6067
train acc:  0.8203125
train loss:  0.4647590219974518
train gradient:  0.36837721886456387
iteration : 6068
train acc:  0.875
train loss:  0.32334399223327637
train gradient:  0.21772028600861132
iteration : 6069
train acc:  0.8671875
train loss:  0.355093389749527
train gradient:  0.26500876299582443
iteration : 6070
train acc:  0.8125
train loss:  0.3485313653945923
train gradient:  0.23628594341729797
iteration : 6071
train acc:  0.890625
train loss:  0.24862238764762878
train gradient:  0.10910550940529248
iteration : 6072
train acc:  0.875
train loss:  0.32221126556396484
train gradient:  0.22284765967030137
iteration : 6073
train acc:  0.8515625
train loss:  0.29559317231178284
train gradient:  0.15535599788176682
iteration : 6074
train acc:  0.875
train loss:  0.32765042781829834
train gradient:  0.3212820741024289
iteration : 6075
train acc:  0.875
train loss:  0.35349154472351074
train gradient:  0.17992578125238434
iteration : 6076
train acc:  0.8671875
train loss:  0.3458242416381836
train gradient:  0.1810210563166776
iteration : 6077
train acc:  0.8828125
train loss:  0.28944453597068787
train gradient:  0.241604695591914
iteration : 6078
train acc:  0.84375
train loss:  0.3624778091907501
train gradient:  0.26363697921629137
iteration : 6079
train acc:  0.8359375
train loss:  0.32369357347488403
train gradient:  0.20939443147480313
iteration : 6080
train acc:  0.796875
train loss:  0.44143199920654297
train gradient:  0.28830824680242817
iteration : 6081
train acc:  0.890625
train loss:  0.2543623745441437
train gradient:  0.14602487137459164
iteration : 6082
train acc:  0.8203125
train loss:  0.3962225317955017
train gradient:  0.21239036984666465
iteration : 6083
train acc:  0.875
train loss:  0.3380886912345886
train gradient:  0.18898031392062478
iteration : 6084
train acc:  0.796875
train loss:  0.4739389419555664
train gradient:  0.29970018466272963
iteration : 6085
train acc:  0.84375
train loss:  0.3411654829978943
train gradient:  0.18736617522172117
iteration : 6086
train acc:  0.8828125
train loss:  0.24122829735279083
train gradient:  0.13997376570488151
iteration : 6087
train acc:  0.8828125
train loss:  0.2798498272895813
train gradient:  0.13850470005051213
iteration : 6088
train acc:  0.8515625
train loss:  0.32399630546569824
train gradient:  0.22571052810075903
iteration : 6089
train acc:  0.859375
train loss:  0.32943931221961975
train gradient:  0.2092637639151162
iteration : 6090
train acc:  0.828125
train loss:  0.3899945616722107
train gradient:  0.20932577064613067
iteration : 6091
train acc:  0.8984375
train loss:  0.26806628704071045
train gradient:  0.12556766782734974
iteration : 6092
train acc:  0.9140625
train loss:  0.25041744112968445
train gradient:  0.11225803645089708
iteration : 6093
train acc:  0.90625
train loss:  0.324360728263855
train gradient:  0.14003415970919034
iteration : 6094
train acc:  0.8671875
train loss:  0.3045884966850281
train gradient:  0.17969953681959397
iteration : 6095
train acc:  0.8671875
train loss:  0.301069974899292
train gradient:  0.14819703314674906
iteration : 6096
train acc:  0.859375
train loss:  0.3293963372707367
train gradient:  0.2159871539438064
iteration : 6097
train acc:  0.828125
train loss:  0.35674700140953064
train gradient:  0.42422222626891354
iteration : 6098
train acc:  0.84375
train loss:  0.36538976430892944
train gradient:  0.21360629362438283
iteration : 6099
train acc:  0.828125
train loss:  0.32932382822036743
train gradient:  0.22850259261162975
iteration : 6100
train acc:  0.8359375
train loss:  0.3063938021659851
train gradient:  0.17316866292423905
iteration : 6101
train acc:  0.765625
train loss:  0.5089401006698608
train gradient:  0.39615737626184006
iteration : 6102
train acc:  0.828125
train loss:  0.3702600598335266
train gradient:  0.23168531524933383
iteration : 6103
train acc:  0.7890625
train loss:  0.47337156534194946
train gradient:  0.4164108258346844
iteration : 6104
train acc:  0.90625
train loss:  0.2618533968925476
train gradient:  0.14760592623786195
iteration : 6105
train acc:  0.8125
train loss:  0.4152758717536926
train gradient:  0.3462984342492384
iteration : 6106
train acc:  0.8515625
train loss:  0.319078266620636
train gradient:  0.1530126832675994
iteration : 6107
train acc:  0.7890625
train loss:  0.36591702699661255
train gradient:  0.21713140834933775
iteration : 6108
train acc:  0.78125
train loss:  0.44478318095207214
train gradient:  0.33135070997414273
iteration : 6109
train acc:  0.8515625
train loss:  0.32960087060928345
train gradient:  0.17976142868831063
iteration : 6110
train acc:  0.84375
train loss:  0.367908775806427
train gradient:  0.18288091643637908
iteration : 6111
train acc:  0.8359375
train loss:  0.3653939962387085
train gradient:  0.2502747106021419
iteration : 6112
train acc:  0.8046875
train loss:  0.35419541597366333
train gradient:  0.19698680880982075
iteration : 6113
train acc:  0.828125
train loss:  0.3736172020435333
train gradient:  0.16847627553311276
iteration : 6114
train acc:  0.8359375
train loss:  0.3823837637901306
train gradient:  0.23955898406267706
iteration : 6115
train acc:  0.828125
train loss:  0.3744381070137024
train gradient:  0.1846774077029239
iteration : 6116
train acc:  0.8515625
train loss:  0.3308223485946655
train gradient:  0.15684513742873757
iteration : 6117
train acc:  0.8671875
train loss:  0.30978822708129883
train gradient:  0.12474596204646338
iteration : 6118
train acc:  0.8125
train loss:  0.41725239157676697
train gradient:  0.31118026909974994
iteration : 6119
train acc:  0.8046875
train loss:  0.35734793543815613
train gradient:  0.2002424331791018
iteration : 6120
train acc:  0.84375
train loss:  0.3919987082481384
train gradient:  0.20512222984384276
iteration : 6121
train acc:  0.7890625
train loss:  0.3968786597251892
train gradient:  0.24483190780400743
iteration : 6122
train acc:  0.796875
train loss:  0.40999582409858704
train gradient:  0.27187965607451486
iteration : 6123
train acc:  0.8828125
train loss:  0.3278089761734009
train gradient:  0.21642739591653554
iteration : 6124
train acc:  0.8359375
train loss:  0.3271384537220001
train gradient:  0.13205546651216965
iteration : 6125
train acc:  0.84375
train loss:  0.3871089816093445
train gradient:  0.22489922014076408
iteration : 6126
train acc:  0.8515625
train loss:  0.31985360383987427
train gradient:  0.24405412322425357
iteration : 6127
train acc:  0.84375
train loss:  0.3537623882293701
train gradient:  0.25686824631141225
iteration : 6128
train acc:  0.828125
train loss:  0.36092743277549744
train gradient:  0.2679395118228774
iteration : 6129
train acc:  0.875
train loss:  0.32123905420303345
train gradient:  0.2526299484011853
iteration : 6130
train acc:  0.8359375
train loss:  0.3320341110229492
train gradient:  0.39048834671182053
iteration : 6131
train acc:  0.8203125
train loss:  0.358428955078125
train gradient:  0.23578739648583974
iteration : 6132
train acc:  0.8671875
train loss:  0.3312056064605713
train gradient:  0.1721165988787548
iteration : 6133
train acc:  0.84375
train loss:  0.3504703640937805
train gradient:  0.275072938274162
iteration : 6134
train acc:  0.9140625
train loss:  0.2988026738166809
train gradient:  0.16176863450470488
iteration : 6135
train acc:  0.890625
train loss:  0.3014064431190491
train gradient:  0.16893672123765374
iteration : 6136
train acc:  0.875
train loss:  0.29429394006729126
train gradient:  0.15496038456978695
iteration : 6137
train acc:  0.859375
train loss:  0.2973790764808655
train gradient:  0.11554459614668695
iteration : 6138
train acc:  0.8359375
train loss:  0.41810521483421326
train gradient:  0.2682711709105815
iteration : 6139
train acc:  0.859375
train loss:  0.3552132844924927
train gradient:  0.17534980172721037
iteration : 6140
train acc:  0.8828125
train loss:  0.28400588035583496
train gradient:  0.1403450842225546
iteration : 6141
train acc:  0.84375
train loss:  0.3676718771457672
train gradient:  0.28580694669338
iteration : 6142
train acc:  0.828125
train loss:  0.37564748525619507
train gradient:  0.21159072828158998
iteration : 6143
train acc:  0.84375
train loss:  0.3389926254749298
train gradient:  0.21764600154259037
iteration : 6144
train acc:  0.84375
train loss:  0.3974694609642029
train gradient:  0.2746261566843352
iteration : 6145
train acc:  0.8671875
train loss:  0.3132069706916809
train gradient:  0.17701487635316351
iteration : 6146
train acc:  0.875
train loss:  0.3260219693183899
train gradient:  0.21003552573098994
iteration : 6147
train acc:  0.8515625
train loss:  0.33801060914993286
train gradient:  0.25186873838451757
iteration : 6148
train acc:  0.8359375
train loss:  0.38248154520988464
train gradient:  0.2703223436850657
iteration : 6149
train acc:  0.796875
train loss:  0.3684394955635071
train gradient:  0.22118639271666463
iteration : 6150
train acc:  0.859375
train loss:  0.32556816935539246
train gradient:  0.23250540518071816
iteration : 6151
train acc:  0.7890625
train loss:  0.3400722146034241
train gradient:  0.22946496262661478
iteration : 6152
train acc:  0.8515625
train loss:  0.35428133606910706
train gradient:  0.24750218405755453
iteration : 6153
train acc:  0.875
train loss:  0.2741498649120331
train gradient:  0.1541871804063353
iteration : 6154
train acc:  0.8046875
train loss:  0.4290271997451782
train gradient:  0.35417156423328355
iteration : 6155
train acc:  0.9375
train loss:  0.24995997548103333
train gradient:  0.11596120249217826
iteration : 6156
train acc:  0.78125
train loss:  0.5106198191642761
train gradient:  0.4075238420733303
iteration : 6157
train acc:  0.765625
train loss:  0.4268012046813965
train gradient:  0.3285582367919216
iteration : 6158
train acc:  0.859375
train loss:  0.31605902314186096
train gradient:  0.1745985860893446
iteration : 6159
train acc:  0.8671875
train loss:  0.2551839351654053
train gradient:  0.13599669138432624
iteration : 6160
train acc:  0.8671875
train loss:  0.30560415983200073
train gradient:  0.24058615719197254
iteration : 6161
train acc:  0.8515625
train loss:  0.3429853022098541
train gradient:  0.2032900112923868
iteration : 6162
train acc:  0.8515625
train loss:  0.3221726715564728
train gradient:  0.1945453381875655
iteration : 6163
train acc:  0.8203125
train loss:  0.35068219900131226
train gradient:  0.2896801326353598
iteration : 6164
train acc:  0.8359375
train loss:  0.3263206481933594
train gradient:  0.2192592520731526
iteration : 6165
train acc:  0.84375
train loss:  0.35602104663848877
train gradient:  0.3111430944425926
iteration : 6166
train acc:  0.796875
train loss:  0.3602806627750397
train gradient:  0.29219576948912634
iteration : 6167
train acc:  0.8125
train loss:  0.3349936306476593
train gradient:  0.22331524662058969
iteration : 6168
train acc:  0.8125
train loss:  0.3745273947715759
train gradient:  0.29862054546271943
iteration : 6169
train acc:  0.8125
train loss:  0.3589644432067871
train gradient:  0.2620524157650951
iteration : 6170
train acc:  0.859375
train loss:  0.29184961318969727
train gradient:  0.14400904632061334
iteration : 6171
train acc:  0.859375
train loss:  0.2874223589897156
train gradient:  0.15807078830359395
iteration : 6172
train acc:  0.78125
train loss:  0.4424697160720825
train gradient:  0.3746258666699949
iteration : 6173
train acc:  0.84375
train loss:  0.31960976123809814
train gradient:  0.3590628268893813
iteration : 6174
train acc:  0.8515625
train loss:  0.3573462963104248
train gradient:  0.25965965361253923
iteration : 6175
train acc:  0.8671875
train loss:  0.28663116693496704
train gradient:  0.16495024576984196
iteration : 6176
train acc:  0.875
train loss:  0.32575082778930664
train gradient:  0.2504505994280613
iteration : 6177
train acc:  0.8984375
train loss:  0.24182182550430298
train gradient:  0.1815162922828601
iteration : 6178
train acc:  0.8203125
train loss:  0.40419331192970276
train gradient:  0.21068496821039517
iteration : 6179
train acc:  0.8125
train loss:  0.43574196100234985
train gradient:  0.35257544939095004
iteration : 6180
train acc:  0.8203125
train loss:  0.41938817501068115
train gradient:  0.2922807832577202
iteration : 6181
train acc:  0.859375
train loss:  0.3084765672683716
train gradient:  0.23586005259510084
iteration : 6182
train acc:  0.875
train loss:  0.34742307662963867
train gradient:  0.24060498969121502
iteration : 6183
train acc:  0.8359375
train loss:  0.35818326473236084
train gradient:  0.16765353274094796
iteration : 6184
train acc:  0.8515625
train loss:  0.31274116039276123
train gradient:  0.1802689841720213
iteration : 6185
train acc:  0.828125
train loss:  0.3777867257595062
train gradient:  0.281795575781139
iteration : 6186
train acc:  0.859375
train loss:  0.3787352442741394
train gradient:  0.18814755620853324
iteration : 6187
train acc:  0.8828125
train loss:  0.32816821336746216
train gradient:  0.24188451154703935
iteration : 6188
train acc:  0.8984375
train loss:  0.2582557499408722
train gradient:  0.15751175113860755
iteration : 6189
train acc:  0.8203125
train loss:  0.3874964714050293
train gradient:  0.2604215573346971
iteration : 6190
train acc:  0.875
train loss:  0.29403263330459595
train gradient:  0.1631885052327887
iteration : 6191
train acc:  0.8125
train loss:  0.3755006194114685
train gradient:  0.2807869972096808
iteration : 6192
train acc:  0.8671875
train loss:  0.2930775582790375
train gradient:  0.13819116313352833
iteration : 6193
train acc:  0.8671875
train loss:  0.36757901310920715
train gradient:  0.190904333433809
iteration : 6194
train acc:  0.8359375
train loss:  0.33699458837509155
train gradient:  0.24213252185111633
iteration : 6195
train acc:  0.8984375
train loss:  0.2715884745121002
train gradient:  0.13342152940013877
iteration : 6196
train acc:  0.859375
train loss:  0.3257242441177368
train gradient:  0.20781406766714414
iteration : 6197
train acc:  0.7890625
train loss:  0.45342734456062317
train gradient:  0.2965434776939489
iteration : 6198
train acc:  0.8359375
train loss:  0.32805776596069336
train gradient:  0.13703465035193485
iteration : 6199
train acc:  0.8515625
train loss:  0.3100922703742981
train gradient:  0.15955372037167764
iteration : 6200
train acc:  0.8359375
train loss:  0.3858593702316284
train gradient:  0.235664914362954
iteration : 6201
train acc:  0.7578125
train loss:  0.4050672948360443
train gradient:  0.23478862828013558
iteration : 6202
train acc:  0.84375
train loss:  0.336428701877594
train gradient:  0.20147303901150881
iteration : 6203
train acc:  0.828125
train loss:  0.39204174280166626
train gradient:  0.27952330597136515
iteration : 6204
train acc:  0.859375
train loss:  0.32052433490753174
train gradient:  0.20586607722809425
iteration : 6205
train acc:  0.796875
train loss:  0.4071010947227478
train gradient:  0.2527390251009556
iteration : 6206
train acc:  0.8984375
train loss:  0.2789437174797058
train gradient:  0.14911666739249776
iteration : 6207
train acc:  0.875
train loss:  0.31214767694473267
train gradient:  0.1527434081919178
iteration : 6208
train acc:  0.84375
train loss:  0.36235183477401733
train gradient:  0.34966429048789033
iteration : 6209
train acc:  0.8671875
train loss:  0.35612112283706665
train gradient:  0.16341254463886637
iteration : 6210
train acc:  0.84375
train loss:  0.3656221032142639
train gradient:  0.19689643827929706
iteration : 6211
train acc:  0.8515625
train loss:  0.3345312178134918
train gradient:  0.21780278816014942
iteration : 6212
train acc:  0.796875
train loss:  0.35446274280548096
train gradient:  0.22894520015977418
iteration : 6213
train acc:  0.78125
train loss:  0.4405258893966675
train gradient:  0.3502110317680406
iteration : 6214
train acc:  0.859375
train loss:  0.33994752168655396
train gradient:  0.21241206496807052
iteration : 6215
train acc:  0.796875
train loss:  0.46054187417030334
train gradient:  0.24269373953323742
iteration : 6216
train acc:  0.859375
train loss:  0.3708510100841522
train gradient:  0.2686453593033785
iteration : 6217
train acc:  0.8203125
train loss:  0.37918832898139954
train gradient:  0.2408014745647816
iteration : 6218
train acc:  0.875
train loss:  0.2750137448310852
train gradient:  0.1335633163401357
iteration : 6219
train acc:  0.8046875
train loss:  0.38054800033569336
train gradient:  0.2177102887606392
iteration : 6220
train acc:  0.8984375
train loss:  0.28410398960113525
train gradient:  0.14244858301588456
iteration : 6221
train acc:  0.7734375
train loss:  0.4363571107387543
train gradient:  0.3236285760058926
iteration : 6222
train acc:  0.8359375
train loss:  0.32233500480651855
train gradient:  0.15157617747066998
iteration : 6223
train acc:  0.8984375
train loss:  0.2541164457798004
train gradient:  0.14205599742932445
iteration : 6224
train acc:  0.734375
train loss:  0.5553781390190125
train gradient:  0.412871947450292
iteration : 6225
train acc:  0.8515625
train loss:  0.3074830174446106
train gradient:  0.17189394433315533
iteration : 6226
train acc:  0.8828125
train loss:  0.30822888016700745
train gradient:  0.18799556073507506
iteration : 6227
train acc:  0.8359375
train loss:  0.33953291177749634
train gradient:  0.18093463798165157
iteration : 6228
train acc:  0.8515625
train loss:  0.34570544958114624
train gradient:  0.2393855795966997
iteration : 6229
train acc:  0.859375
train loss:  0.32358160614967346
train gradient:  0.14778971638497584
iteration : 6230
train acc:  0.8515625
train loss:  0.31451141834259033
train gradient:  0.17462734117697065
iteration : 6231
train acc:  0.875
train loss:  0.31886374950408936
train gradient:  0.13544880445698815
iteration : 6232
train acc:  0.84375
train loss:  0.29342007637023926
train gradient:  0.16351851196068892
iteration : 6233
train acc:  0.875
train loss:  0.3544168472290039
train gradient:  0.25828592225821195
iteration : 6234
train acc:  0.84375
train loss:  0.3771930932998657
train gradient:  0.16425967439707048
iteration : 6235
train acc:  0.84375
train loss:  0.35243356227874756
train gradient:  0.24364338445859715
iteration : 6236
train acc:  0.8046875
train loss:  0.31913623213768005
train gradient:  0.13522380537293577
iteration : 6237
train acc:  0.859375
train loss:  0.3583023250102997
train gradient:  0.19563242988161064
iteration : 6238
train acc:  0.796875
train loss:  0.32690608501434326
train gradient:  0.21777249334050358
iteration : 6239
train acc:  0.8828125
train loss:  0.2949962913990021
train gradient:  0.11464700709208804
iteration : 6240
train acc:  0.890625
train loss:  0.3185389041900635
train gradient:  0.19150402783322643
iteration : 6241
train acc:  0.8046875
train loss:  0.5104399919509888
train gradient:  0.37503637651790717
iteration : 6242
train acc:  0.828125
train loss:  0.3349303901195526
train gradient:  0.1975579687406582
iteration : 6243
train acc:  0.859375
train loss:  0.3340527415275574
train gradient:  0.24417118287284226
iteration : 6244
train acc:  0.875
train loss:  0.31616902351379395
train gradient:  0.19127499405317902
iteration : 6245
train acc:  0.8671875
train loss:  0.32176730036735535
train gradient:  0.18669207601028442
iteration : 6246
train acc:  0.84375
train loss:  0.290600061416626
train gradient:  0.1211658589494058
iteration : 6247
train acc:  0.8515625
train loss:  0.30420684814453125
train gradient:  0.15559685693080053
iteration : 6248
train acc:  0.84375
train loss:  0.3789212703704834
train gradient:  0.24944000045077575
iteration : 6249
train acc:  0.796875
train loss:  0.4365162253379822
train gradient:  0.23351306750727405
iteration : 6250
train acc:  0.875
train loss:  0.30476486682891846
train gradient:  0.22267821249967537
iteration : 6251
train acc:  0.8515625
train loss:  0.3648689091205597
train gradient:  0.21374265828883082
iteration : 6252
train acc:  0.75
train loss:  0.5398067235946655
train gradient:  0.669758351906667
iteration : 6253
train acc:  0.875
train loss:  0.2995480000972748
train gradient:  0.205592143085711
iteration : 6254
train acc:  0.84375
train loss:  0.34799081087112427
train gradient:  0.26363691113324295
iteration : 6255
train acc:  0.84375
train loss:  0.31505632400512695
train gradient:  0.15364497584021722
iteration : 6256
train acc:  0.828125
train loss:  0.37448880076408386
train gradient:  0.1749523438710916
iteration : 6257
train acc:  0.8671875
train loss:  0.32222622632980347
train gradient:  0.17579215503119167
iteration : 6258
train acc:  0.828125
train loss:  0.3519212603569031
train gradient:  0.20333315217940906
iteration : 6259
train acc:  0.8515625
train loss:  0.3208984136581421
train gradient:  0.17762193745335575
iteration : 6260
train acc:  0.859375
train loss:  0.3096677362918854
train gradient:  0.11531002426333244
iteration : 6261
train acc:  0.859375
train loss:  0.28379547595977783
train gradient:  0.15847430772689394
iteration : 6262
train acc:  0.8359375
train loss:  0.39299890398979187
train gradient:  0.22139836932922946
iteration : 6263
train acc:  0.8671875
train loss:  0.3302549421787262
train gradient:  0.21938123103443646
iteration : 6264
train acc:  0.84375
train loss:  0.3583834767341614
train gradient:  0.292048037312025
iteration : 6265
train acc:  0.8671875
train loss:  0.31984829902648926
train gradient:  0.2075608186278805
iteration : 6266
train acc:  0.890625
train loss:  0.2836873531341553
train gradient:  0.14683562013685683
iteration : 6267
train acc:  0.84375
train loss:  0.417856365442276
train gradient:  0.3075781878169046
iteration : 6268
train acc:  0.8671875
train loss:  0.34681108593940735
train gradient:  0.1992274179418635
iteration : 6269
train acc:  0.859375
train loss:  0.3392457067966461
train gradient:  0.17629130741753385
iteration : 6270
train acc:  0.8046875
train loss:  0.3982555866241455
train gradient:  0.2675183219899696
iteration : 6271
train acc:  0.828125
train loss:  0.38578158617019653
train gradient:  0.16435607287780474
iteration : 6272
train acc:  0.828125
train loss:  0.37874794006347656
train gradient:  0.2264941395864029
iteration : 6273
train acc:  0.859375
train loss:  0.37702250480651855
train gradient:  0.21266802771103738
iteration : 6274
train acc:  0.828125
train loss:  0.34840646386146545
train gradient:  0.19923901260427185
iteration : 6275
train acc:  0.890625
train loss:  0.3001335859298706
train gradient:  0.17423217793112517
iteration : 6276
train acc:  0.890625
train loss:  0.31322941184043884
train gradient:  0.20453134737983825
iteration : 6277
train acc:  0.78125
train loss:  0.42983749508857727
train gradient:  0.3103642430007153
iteration : 6278
train acc:  0.78125
train loss:  0.4824492931365967
train gradient:  0.35241558109946425
iteration : 6279
train acc:  0.8359375
train loss:  0.3415534496307373
train gradient:  0.18047208523385447
iteration : 6280
train acc:  0.859375
train loss:  0.3037158250808716
train gradient:  0.1560987633926716
iteration : 6281
train acc:  0.875
train loss:  0.2866581678390503
train gradient:  0.21309972868806576
iteration : 6282
train acc:  0.8359375
train loss:  0.37943974137306213
train gradient:  0.20765806353444904
iteration : 6283
train acc:  0.8046875
train loss:  0.3556476831436157
train gradient:  0.2279886596739803
iteration : 6284
train acc:  0.8515625
train loss:  0.34810158610343933
train gradient:  0.35449534698781304
iteration : 6285
train acc:  0.8203125
train loss:  0.4011930525302887
train gradient:  0.44421129480922467
iteration : 6286
train acc:  0.8203125
train loss:  0.335580438375473
train gradient:  0.2195694402070185
iteration : 6287
train acc:  0.875
train loss:  0.298595130443573
train gradient:  0.16354430862001246
iteration : 6288
train acc:  0.84375
train loss:  0.32011616230010986
train gradient:  0.1729471947352631
iteration : 6289
train acc:  0.8828125
train loss:  0.2750830054283142
train gradient:  0.1281587293033144
iteration : 6290
train acc:  0.8515625
train loss:  0.33777424693107605
train gradient:  0.27762393379952927
iteration : 6291
train acc:  0.8515625
train loss:  0.3191981315612793
train gradient:  0.21823013239558858
iteration : 6292
train acc:  0.875
train loss:  0.3326396644115448
train gradient:  0.17800402930269094
iteration : 6293
train acc:  0.8203125
train loss:  0.4156572222709656
train gradient:  0.26506532897891333
iteration : 6294
train acc:  0.8671875
train loss:  0.3111766278743744
train gradient:  0.1990908085503793
iteration : 6295
train acc:  0.9296875
train loss:  0.2332219034433365
train gradient:  0.13831425627465965
iteration : 6296
train acc:  0.859375
train loss:  0.29792487621307373
train gradient:  0.21415074500472112
iteration : 6297
train acc:  0.8828125
train loss:  0.33959800004959106
train gradient:  0.1853845646003572
iteration : 6298
train acc:  0.859375
train loss:  0.3427863121032715
train gradient:  0.2593105608657278
iteration : 6299
train acc:  0.8671875
train loss:  0.28682366013526917
train gradient:  0.14043358293439473
iteration : 6300
train acc:  0.8671875
train loss:  0.2595791220664978
train gradient:  0.11884782454269506
iteration : 6301
train acc:  0.78125
train loss:  0.445667564868927
train gradient:  0.33698162780417273
iteration : 6302
train acc:  0.890625
train loss:  0.2773532271385193
train gradient:  0.14466248404756923
iteration : 6303
train acc:  0.8359375
train loss:  0.3429450988769531
train gradient:  0.14087448735739097
iteration : 6304
train acc:  0.8203125
train loss:  0.3685145974159241
train gradient:  0.2669794109587301
iteration : 6305
train acc:  0.859375
train loss:  0.324849396944046
train gradient:  0.2867739636154531
iteration : 6306
train acc:  0.8671875
train loss:  0.28406885266304016
train gradient:  0.19343392180757024
iteration : 6307
train acc:  0.8125
train loss:  0.41646039485931396
train gradient:  0.3294309088595536
iteration : 6308
train acc:  0.8125
train loss:  0.3939678370952606
train gradient:  0.35817197194170075
iteration : 6309
train acc:  0.7578125
train loss:  0.5248379111289978
train gradient:  0.4607794486467767
iteration : 6310
train acc:  0.8671875
train loss:  0.36082935333251953
train gradient:  0.20989796147533263
iteration : 6311
train acc:  0.8671875
train loss:  0.3512156009674072
train gradient:  0.23789219583919757
iteration : 6312
train acc:  0.859375
train loss:  0.3880561590194702
train gradient:  0.2514353838473875
iteration : 6313
train acc:  0.859375
train loss:  0.31124719977378845
train gradient:  0.15819980671511988
iteration : 6314
train acc:  0.8984375
train loss:  0.27005088329315186
train gradient:  0.14263881929451622
iteration : 6315
train acc:  0.8359375
train loss:  0.3876854181289673
train gradient:  0.305907451854125
iteration : 6316
train acc:  0.8515625
train loss:  0.33665961027145386
train gradient:  0.24472734378706962
iteration : 6317
train acc:  0.8125
train loss:  0.43997347354888916
train gradient:  0.35427153597886857
iteration : 6318
train acc:  0.84375
train loss:  0.3740639090538025
train gradient:  0.22899224283208253
iteration : 6319
train acc:  0.8984375
train loss:  0.3151986300945282
train gradient:  0.2833712154794043
iteration : 6320
train acc:  0.8671875
train loss:  0.2916172444820404
train gradient:  0.17162653781703752
iteration : 6321
train acc:  0.8125
train loss:  0.33842214941978455
train gradient:  0.20761081895289565
iteration : 6322
train acc:  0.8671875
train loss:  0.3508942723274231
train gradient:  0.2382233869186204
iteration : 6323
train acc:  0.8828125
train loss:  0.28034937381744385
train gradient:  0.1338444998589442
iteration : 6324
train acc:  0.8359375
train loss:  0.3367270529270172
train gradient:  0.23838420528826879
iteration : 6325
train acc:  0.84375
train loss:  0.36805459856987
train gradient:  0.23437661115748884
iteration : 6326
train acc:  0.8046875
train loss:  0.39733463525772095
train gradient:  0.30057434681370443
iteration : 6327
train acc:  0.8515625
train loss:  0.3391709625720978
train gradient:  0.16867742163818467
iteration : 6328
train acc:  0.828125
train loss:  0.33393239974975586
train gradient:  0.19924522036855646
iteration : 6329
train acc:  0.8984375
train loss:  0.2718718647956848
train gradient:  0.16905912565100994
iteration : 6330
train acc:  0.8359375
train loss:  0.3673619031906128
train gradient:  0.21700352130036465
iteration : 6331
train acc:  0.859375
train loss:  0.3481179177761078
train gradient:  0.440803150401943
iteration : 6332
train acc:  0.84375
train loss:  0.3878989815711975
train gradient:  0.24211773479331508
iteration : 6333
train acc:  0.8828125
train loss:  0.2719001770019531
train gradient:  0.19316015003306097
iteration : 6334
train acc:  0.8359375
train loss:  0.4395105242729187
train gradient:  0.2769651317304052
iteration : 6335
train acc:  0.8203125
train loss:  0.3579416871070862
train gradient:  0.21673318607175088
iteration : 6336
train acc:  0.859375
train loss:  0.30086904764175415
train gradient:  0.23478050638581222
iteration : 6337
train acc:  0.8671875
train loss:  0.3582035303115845
train gradient:  0.24875522952490597
iteration : 6338
train acc:  0.8671875
train loss:  0.37237074971199036
train gradient:  0.2572922081825798
iteration : 6339
train acc:  0.8671875
train loss:  0.3119748830795288
train gradient:  0.15603256412881122
iteration : 6340
train acc:  0.8515625
train loss:  0.3132967948913574
train gradient:  0.2181895111793551
iteration : 6341
train acc:  0.7890625
train loss:  0.4668787717819214
train gradient:  0.34384601344071203
iteration : 6342
train acc:  0.875
train loss:  0.31669142842292786
train gradient:  0.2024139779087586
iteration : 6343
train acc:  0.8203125
train loss:  0.37892767786979675
train gradient:  0.2898367287135445
iteration : 6344
train acc:  0.8671875
train loss:  0.2821316123008728
train gradient:  0.11330903783094941
iteration : 6345
train acc:  0.84375
train loss:  0.32161468267440796
train gradient:  0.20940626681355007
iteration : 6346
train acc:  0.859375
train loss:  0.3678794503211975
train gradient:  0.21606938003137835
iteration : 6347
train acc:  0.84375
train loss:  0.3791653513908386
train gradient:  0.31407647753448875
iteration : 6348
train acc:  0.8203125
train loss:  0.363716721534729
train gradient:  0.2091122125213469
iteration : 6349
train acc:  0.78125
train loss:  0.45534518361091614
train gradient:  0.3843915442009184
iteration : 6350
train acc:  0.84375
train loss:  0.31506800651550293
train gradient:  0.16474158593345184
iteration : 6351
train acc:  0.890625
train loss:  0.3155512809753418
train gradient:  0.11740880817329974
iteration : 6352
train acc:  0.84375
train loss:  0.3108435869216919
train gradient:  0.15674284283315573
iteration : 6353
train acc:  0.8515625
train loss:  0.2952825129032135
train gradient:  0.18795613637571312
iteration : 6354
train acc:  0.8828125
train loss:  0.2783966660499573
train gradient:  0.14909038901630134
iteration : 6355
train acc:  0.8515625
train loss:  0.32666435837745667
train gradient:  0.22252851766117382
iteration : 6356
train acc:  0.796875
train loss:  0.4116126298904419
train gradient:  0.2594925930631829
iteration : 6357
train acc:  0.875
train loss:  0.3584423065185547
train gradient:  0.2517831955297614
iteration : 6358
train acc:  0.828125
train loss:  0.34746989607810974
train gradient:  0.25673529929348504
iteration : 6359
train acc:  0.8359375
train loss:  0.37901365756988525
train gradient:  0.24486156188630998
iteration : 6360
train acc:  0.8984375
train loss:  0.261948823928833
train gradient:  0.15535547321681964
iteration : 6361
train acc:  0.859375
train loss:  0.36757469177246094
train gradient:  0.23234483672797993
iteration : 6362
train acc:  0.8515625
train loss:  0.32110536098480225
train gradient:  0.15822773227562875
iteration : 6363
train acc:  0.828125
train loss:  0.34032613039016724
train gradient:  0.24380978551046645
iteration : 6364
train acc:  0.8671875
train loss:  0.3344876766204834
train gradient:  0.18691836851718668
iteration : 6365
train acc:  0.8515625
train loss:  0.3881639540195465
train gradient:  0.19264329809208144
iteration : 6366
train acc:  0.875
train loss:  0.2911669611930847
train gradient:  0.19820383820242127
iteration : 6367
train acc:  0.875
train loss:  0.2942505478858948
train gradient:  0.16918212351013087
iteration : 6368
train acc:  0.90625
train loss:  0.2565463185310364
train gradient:  0.16664384633236254
iteration : 6369
train acc:  0.8046875
train loss:  0.42649611830711365
train gradient:  0.2598254413032948
iteration : 6370
train acc:  0.8515625
train loss:  0.32505324482917786
train gradient:  0.13291107788121212
iteration : 6371
train acc:  0.921875
train loss:  0.26854291558265686
train gradient:  0.1259137957557094
iteration : 6372
train acc:  0.84375
train loss:  0.4099768400192261
train gradient:  0.3205188642796129
iteration : 6373
train acc:  0.7109375
train loss:  0.4609854221343994
train gradient:  0.3955699172328861
iteration : 6374
train acc:  0.859375
train loss:  0.32936006784439087
train gradient:  0.19320217897900488
iteration : 6375
train acc:  0.859375
train loss:  0.3331814408302307
train gradient:  0.19149551011070476
iteration : 6376
train acc:  0.859375
train loss:  0.3411214351654053
train gradient:  0.3142023366004237
iteration : 6377
train acc:  0.8359375
train loss:  0.34723979234695435
train gradient:  0.26135423290175863
iteration : 6378
train acc:  0.78125
train loss:  0.43957966566085815
train gradient:  0.3880546902645958
iteration : 6379
train acc:  0.8984375
train loss:  0.2574947774410248
train gradient:  0.1471307127154603
iteration : 6380
train acc:  0.8046875
train loss:  0.4242520034313202
train gradient:  0.21833607027803398
iteration : 6381
train acc:  0.8515625
train loss:  0.3677547574043274
train gradient:  0.2584674750752404
iteration : 6382
train acc:  0.8515625
train loss:  0.4210694134235382
train gradient:  0.2510447807979602
iteration : 6383
train acc:  0.84375
train loss:  0.33681297302246094
train gradient:  0.27001180141408027
iteration : 6384
train acc:  0.859375
train loss:  0.3345732092857361
train gradient:  0.2307110989713308
iteration : 6385
train acc:  0.828125
train loss:  0.3681310713291168
train gradient:  0.4020616029831377
iteration : 6386
train acc:  0.8671875
train loss:  0.31891894340515137
train gradient:  0.24196470005995963
iteration : 6387
train acc:  0.8203125
train loss:  0.3502422571182251
train gradient:  0.21706694916092173
iteration : 6388
train acc:  0.84375
train loss:  0.428845077753067
train gradient:  0.3091260706054698
iteration : 6389
train acc:  0.84375
train loss:  0.2864654064178467
train gradient:  0.12245430125974466
iteration : 6390
train acc:  0.8125
train loss:  0.42048415541648865
train gradient:  0.24870839739432066
iteration : 6391
train acc:  0.8515625
train loss:  0.38900429010391235
train gradient:  0.3397873663338123
iteration : 6392
train acc:  0.8359375
train loss:  0.41116851568222046
train gradient:  0.26219209313852104
iteration : 6393
train acc:  0.8359375
train loss:  0.3862632215023041
train gradient:  0.27958561976837365
iteration : 6394
train acc:  0.859375
train loss:  0.3971429467201233
train gradient:  0.31851451767938105
iteration : 6395
train acc:  0.875
train loss:  0.3281884789466858
train gradient:  0.19561364015899801
iteration : 6396
train acc:  0.8359375
train loss:  0.3593200445175171
train gradient:  0.20145791422987674
iteration : 6397
train acc:  0.8046875
train loss:  0.37752652168273926
train gradient:  0.21671410890186443
iteration : 6398
train acc:  0.84375
train loss:  0.32154417037963867
train gradient:  0.22543035865432767
iteration : 6399
train acc:  0.8125
train loss:  0.3972632884979248
train gradient:  0.29792573935341843
iteration : 6400
train acc:  0.84375
train loss:  0.327170193195343
train gradient:  0.2117199332656772
iteration : 6401
train acc:  0.8359375
train loss:  0.39655590057373047
train gradient:  0.178822725698227
iteration : 6402
train acc:  0.8203125
train loss:  0.3439668118953705
train gradient:  0.24114134365061235
iteration : 6403
train acc:  0.859375
train loss:  0.3528119921684265
train gradient:  0.20692116164853303
iteration : 6404
train acc:  0.8671875
train loss:  0.3233622610569
train gradient:  0.23565463402744197
iteration : 6405
train acc:  0.859375
train loss:  0.33749282360076904
train gradient:  0.2206950276817266
iteration : 6406
train acc:  0.8671875
train loss:  0.32276809215545654
train gradient:  0.19017851890580284
iteration : 6407
train acc:  0.84375
train loss:  0.34673717617988586
train gradient:  0.17685518585152554
iteration : 6408
train acc:  0.875
train loss:  0.3113681972026825
train gradient:  0.13186209598945825
iteration : 6409
train acc:  0.828125
train loss:  0.4437219798564911
train gradient:  0.26346644180321266
iteration : 6410
train acc:  0.796875
train loss:  0.44686636328697205
train gradient:  0.29258057911325214
iteration : 6411
train acc:  0.8515625
train loss:  0.34080320596694946
train gradient:  0.15655887630431747
iteration : 6412
train acc:  0.8359375
train loss:  0.36731499433517456
train gradient:  0.2180103518622505
iteration : 6413
train acc:  0.8671875
train loss:  0.28708380460739136
train gradient:  0.16582129010130925
iteration : 6414
train acc:  0.8671875
train loss:  0.3573946952819824
train gradient:  0.15056753120732053
iteration : 6415
train acc:  0.8828125
train loss:  0.31321603059768677
train gradient:  0.17420819832607037
iteration : 6416
train acc:  0.8203125
train loss:  0.3640071749687195
train gradient:  0.23199095031834055
iteration : 6417
train acc:  0.875
train loss:  0.32427918910980225
train gradient:  0.1624600225231636
iteration : 6418
train acc:  0.8828125
train loss:  0.3050072491168976
train gradient:  0.1833600641985575
iteration : 6419
train acc:  0.828125
train loss:  0.38656970858573914
train gradient:  0.20416496878966586
iteration : 6420
train acc:  0.8046875
train loss:  0.3545413613319397
train gradient:  0.18299357331382332
iteration : 6421
train acc:  0.859375
train loss:  0.3132662773132324
train gradient:  0.13310206506892233
iteration : 6422
train acc:  0.8359375
train loss:  0.3166937232017517
train gradient:  0.1310274901678285
iteration : 6423
train acc:  0.890625
train loss:  0.2965487539768219
train gradient:  0.15722015732005462
iteration : 6424
train acc:  0.8828125
train loss:  0.32617658376693726
train gradient:  0.15010624668657585
iteration : 6425
train acc:  0.828125
train loss:  0.35416656732559204
train gradient:  0.21483825414131996
iteration : 6426
train acc:  0.859375
train loss:  0.3259148597717285
train gradient:  0.13825089763739384
iteration : 6427
train acc:  0.84375
train loss:  0.3612877428531647
train gradient:  0.18786166226996626
iteration : 6428
train acc:  0.8125
train loss:  0.3979976773262024
train gradient:  0.23030841945363129
iteration : 6429
train acc:  0.7890625
train loss:  0.3848455548286438
train gradient:  0.30170830381038105
iteration : 6430
train acc:  0.875
train loss:  0.3187703490257263
train gradient:  0.1746752697673234
iteration : 6431
train acc:  0.84375
train loss:  0.31800374388694763
train gradient:  0.1309679516361703
iteration : 6432
train acc:  0.8359375
train loss:  0.41302379965782166
train gradient:  0.3066064973878614
iteration : 6433
train acc:  0.796875
train loss:  0.37130191922187805
train gradient:  0.2700266667616306
iteration : 6434
train acc:  0.8671875
train loss:  0.341305136680603
train gradient:  0.20182392264161103
iteration : 6435
train acc:  0.8515625
train loss:  0.36978679895401
train gradient:  0.20476544471113445
iteration : 6436
train acc:  0.8203125
train loss:  0.3622393012046814
train gradient:  0.17873442695085573
iteration : 6437
train acc:  0.890625
train loss:  0.26754897832870483
train gradient:  0.10340804144369517
iteration : 6438
train acc:  0.8828125
train loss:  0.37264811992645264
train gradient:  0.23838226918295755
iteration : 6439
train acc:  0.8828125
train loss:  0.29015833139419556
train gradient:  0.11810708554283406
iteration : 6440
train acc:  0.859375
train loss:  0.3868648409843445
train gradient:  0.1698309410453781
iteration : 6441
train acc:  0.875
train loss:  0.28429290652275085
train gradient:  0.18361364916752826
iteration : 6442
train acc:  0.8828125
train loss:  0.29910197854042053
train gradient:  0.20221648421397354
iteration : 6443
train acc:  0.8203125
train loss:  0.401729941368103
train gradient:  0.1935357708790096
iteration : 6444
train acc:  0.84375
train loss:  0.32146555185317993
train gradient:  0.1998586450906542
iteration : 6445
train acc:  0.8828125
train loss:  0.32401689887046814
train gradient:  0.20080641614947092
iteration : 6446
train acc:  0.828125
train loss:  0.43458566069602966
train gradient:  0.2643502952028978
iteration : 6447
train acc:  0.8828125
train loss:  0.39128589630126953
train gradient:  0.24503445678827873
iteration : 6448
train acc:  0.890625
train loss:  0.29879966378211975
train gradient:  0.1434140143379719
iteration : 6449
train acc:  0.8359375
train loss:  0.3572353720664978
train gradient:  0.17379609591546477
iteration : 6450
train acc:  0.8203125
train loss:  0.3327396512031555
train gradient:  0.23089774775309962
iteration : 6451
train acc:  0.84375
train loss:  0.36414146423339844
train gradient:  0.20218243168128375
iteration : 6452
train acc:  0.8984375
train loss:  0.30747777223587036
train gradient:  0.224709597715924
iteration : 6453
train acc:  0.8828125
train loss:  0.317870557308197
train gradient:  0.14271171331774846
iteration : 6454
train acc:  0.875
train loss:  0.341402530670166
train gradient:  0.20010023496591567
iteration : 6455
train acc:  0.875
train loss:  0.292050302028656
train gradient:  0.14669293466228414
iteration : 6456
train acc:  0.90625
train loss:  0.2767343521118164
train gradient:  0.13024159591868417
iteration : 6457
train acc:  0.90625
train loss:  0.25856950879096985
train gradient:  0.137872229157739
iteration : 6458
train acc:  0.8984375
train loss:  0.28525596857070923
train gradient:  0.1587092786011532
iteration : 6459
train acc:  0.875
train loss:  0.24268922209739685
train gradient:  0.18472663035174486
iteration : 6460
train acc:  0.859375
train loss:  0.3443487584590912
train gradient:  0.33826385771725753
iteration : 6461
train acc:  0.8125
train loss:  0.37418919801712036
train gradient:  0.2608960290897904
iteration : 6462
train acc:  0.828125
train loss:  0.3175995349884033
train gradient:  0.2579059262657119
iteration : 6463
train acc:  0.8515625
train loss:  0.32027631998062134
train gradient:  0.23753339479648578
iteration : 6464
train acc:  0.8671875
train loss:  0.29917818307876587
train gradient:  0.20731571125033416
iteration : 6465
train acc:  0.8359375
train loss:  0.3757830262184143
train gradient:  0.3139842960432742
iteration : 6466
train acc:  0.84375
train loss:  0.4056062698364258
train gradient:  0.2262366060387594
iteration : 6467
train acc:  0.8203125
train loss:  0.36381542682647705
train gradient:  0.2160035089255213
iteration : 6468
train acc:  0.84375
train loss:  0.33322951197624207
train gradient:  0.3473199616169358
iteration : 6469
train acc:  0.859375
train loss:  0.3060738444328308
train gradient:  0.1506520082360473
iteration : 6470
train acc:  0.8515625
train loss:  0.30680620670318604
train gradient:  0.23485864889284985
iteration : 6471
train acc:  0.8671875
train loss:  0.3196572959423065
train gradient:  0.18435403770137232
iteration : 6472
train acc:  0.8828125
train loss:  0.2591816782951355
train gradient:  0.1434335289766887
iteration : 6473
train acc:  0.7734375
train loss:  0.4669562578201294
train gradient:  0.2998908327580554
iteration : 6474
train acc:  0.8671875
train loss:  0.2895641028881073
train gradient:  0.11484612084416661
iteration : 6475
train acc:  0.8359375
train loss:  0.37810391187667847
train gradient:  0.3443457860480884
iteration : 6476
train acc:  0.8671875
train loss:  0.2912377715110779
train gradient:  0.1536027516297143
iteration : 6477
train acc:  0.84375
train loss:  0.28851938247680664
train gradient:  0.18620266111895012
iteration : 6478
train acc:  0.8359375
train loss:  0.34259191155433655
train gradient:  0.16581485971945423
iteration : 6479
train acc:  0.828125
train loss:  0.4026668071746826
train gradient:  0.24161339299488327
iteration : 6480
train acc:  0.8671875
train loss:  0.3602583408355713
train gradient:  0.25261140739456556
iteration : 6481
train acc:  0.828125
train loss:  0.35820436477661133
train gradient:  0.18171620063966226
iteration : 6482
train acc:  0.8515625
train loss:  0.3138583302497864
train gradient:  0.21061774412268003
iteration : 6483
train acc:  0.8125
train loss:  0.435076504945755
train gradient:  0.2686106037883921
iteration : 6484
train acc:  0.8359375
train loss:  0.31998100876808167
train gradient:  0.1716511034980046
iteration : 6485
train acc:  0.890625
train loss:  0.27222198247909546
train gradient:  0.14469973893702687
iteration : 6486
train acc:  0.9140625
train loss:  0.2549787759780884
train gradient:  0.13341388073278232
iteration : 6487
train acc:  0.8359375
train loss:  0.36585819721221924
train gradient:  0.24193725165664604
iteration : 6488
train acc:  0.8359375
train loss:  0.4187202453613281
train gradient:  0.30683015691706905
iteration : 6489
train acc:  0.84375
train loss:  0.31489241123199463
train gradient:  0.15882593988286653
iteration : 6490
train acc:  0.8984375
train loss:  0.3404880166053772
train gradient:  0.20842252019158097
iteration : 6491
train acc:  0.7890625
train loss:  0.42204636335372925
train gradient:  0.32365603663918546
iteration : 6492
train acc:  0.875
train loss:  0.28951436281204224
train gradient:  0.11216139899704924
iteration : 6493
train acc:  0.84375
train loss:  0.35522162914276123
train gradient:  0.23576258843334633
iteration : 6494
train acc:  0.828125
train loss:  0.38154077529907227
train gradient:  0.25070585095557646
iteration : 6495
train acc:  0.8515625
train loss:  0.3211752474308014
train gradient:  0.16932310864387312
iteration : 6496
train acc:  0.859375
train loss:  0.3309706449508667
train gradient:  0.4144327611408161
iteration : 6497
train acc:  0.828125
train loss:  0.38341814279556274
train gradient:  0.27799385398431614
iteration : 6498
train acc:  0.8359375
train loss:  0.3831358253955841
train gradient:  0.3199356568946611
iteration : 6499
train acc:  0.8203125
train loss:  0.39415034651756287
train gradient:  0.1788342997379683
iteration : 6500
train acc:  0.859375
train loss:  0.32311922311782837
train gradient:  0.14536657116760884
iteration : 6501
train acc:  0.875
train loss:  0.2885250449180603
train gradient:  0.11651050665689655
iteration : 6502
train acc:  0.8125
train loss:  0.37508875131607056
train gradient:  0.20390474294058467
iteration : 6503
train acc:  0.8125
train loss:  0.37576282024383545
train gradient:  0.2971726240202958
iteration : 6504
train acc:  0.8046875
train loss:  0.4050863981246948
train gradient:  0.3257799634462964
iteration : 6505
train acc:  0.8203125
train loss:  0.34656786918640137
train gradient:  0.2942623794367244
iteration : 6506
train acc:  0.828125
train loss:  0.3650662899017334
train gradient:  0.22646736338125356
iteration : 6507
train acc:  0.828125
train loss:  0.3198812007904053
train gradient:  0.2241797681829668
iteration : 6508
train acc:  0.8828125
train loss:  0.33121371269226074
train gradient:  0.1688968846160263
iteration : 6509
train acc:  0.828125
train loss:  0.3444100320339203
train gradient:  0.2924469637451163
iteration : 6510
train acc:  0.8359375
train loss:  0.4277670681476593
train gradient:  0.3645406405768115
iteration : 6511
train acc:  0.8046875
train loss:  0.36541908979415894
train gradient:  0.19441289199584955
iteration : 6512
train acc:  0.84375
train loss:  0.34472227096557617
train gradient:  0.24419630446173207
iteration : 6513
train acc:  0.875
train loss:  0.27567413449287415
train gradient:  0.1214577009882266
iteration : 6514
train acc:  0.8046875
train loss:  0.4043310284614563
train gradient:  0.31952898754593223
iteration : 6515
train acc:  0.8671875
train loss:  0.3363090455532074
train gradient:  0.17705618009370644
iteration : 6516
train acc:  0.875
train loss:  0.3053705394268036
train gradient:  0.1447452898766628
iteration : 6517
train acc:  0.890625
train loss:  0.2835336923599243
train gradient:  0.12215854868172764
iteration : 6518
train acc:  0.8203125
train loss:  0.34241998195648193
train gradient:  0.18313633760696263
iteration : 6519
train acc:  0.8828125
train loss:  0.25363802909851074
train gradient:  0.14794176051355315
iteration : 6520
train acc:  0.8046875
train loss:  0.38043713569641113
train gradient:  0.23147802978887505
iteration : 6521
train acc:  0.8203125
train loss:  0.3923051059246063
train gradient:  0.2547468735592866
iteration : 6522
train acc:  0.84375
train loss:  0.3495737314224243
train gradient:  0.2214417856381088
iteration : 6523
train acc:  0.890625
train loss:  0.3363127112388611
train gradient:  0.13841292266663463
iteration : 6524
train acc:  0.890625
train loss:  0.3033575117588043
train gradient:  0.23907954431560163
iteration : 6525
train acc:  0.8671875
train loss:  0.3154039978981018
train gradient:  0.17681105244527887
iteration : 6526
train acc:  0.875
train loss:  0.28553763031959534
train gradient:  0.16653509484940987
iteration : 6527
train acc:  0.875
train loss:  0.2825591564178467
train gradient:  0.16392027289785552
iteration : 6528
train acc:  0.8203125
train loss:  0.36118847131729126
train gradient:  0.24782738684113598
iteration : 6529
train acc:  0.875
train loss:  0.32911795377731323
train gradient:  0.16392213657089383
iteration : 6530
train acc:  0.8046875
train loss:  0.375818133354187
train gradient:  0.2644350663017459
iteration : 6531
train acc:  0.8828125
train loss:  0.29200848937034607
train gradient:  0.2371296336014594
iteration : 6532
train acc:  0.8203125
train loss:  0.37890106439590454
train gradient:  0.22751133756201725
iteration : 6533
train acc:  0.828125
train loss:  0.38114631175994873
train gradient:  0.20281509717936863
iteration : 6534
train acc:  0.8125
train loss:  0.4609498381614685
train gradient:  0.35686219059145
iteration : 6535
train acc:  0.859375
train loss:  0.32844001054763794
train gradient:  0.16384729199300252
iteration : 6536
train acc:  0.9140625
train loss:  0.31475162506103516
train gradient:  0.17480926665275665
iteration : 6537
train acc:  0.8203125
train loss:  0.4410512447357178
train gradient:  0.3602099853175361
iteration : 6538
train acc:  0.90625
train loss:  0.32619717717170715
train gradient:  0.15644872193013673
iteration : 6539
train acc:  0.8125
train loss:  0.3341066837310791
train gradient:  0.13011977237322414
iteration : 6540
train acc:  0.8359375
train loss:  0.31059277057647705
train gradient:  0.1911383294903465
iteration : 6541
train acc:  0.8671875
train loss:  0.3073127865791321
train gradient:  0.13764001462443393
iteration : 6542
train acc:  0.8515625
train loss:  0.3721300959587097
train gradient:  0.17891284626867401
iteration : 6543
train acc:  0.875
train loss:  0.38280314207077026
train gradient:  0.21938369063544128
iteration : 6544
train acc:  0.90625
train loss:  0.32240352034568787
train gradient:  0.1484225740129729
iteration : 6545
train acc:  0.8359375
train loss:  0.43041011691093445
train gradient:  0.2919000760648817
iteration : 6546
train acc:  0.8359375
train loss:  0.34578967094421387
train gradient:  0.17474345723868945
iteration : 6547
train acc:  0.8359375
train loss:  0.34916794300079346
train gradient:  0.230299796550765
iteration : 6548
train acc:  0.8359375
train loss:  0.287223219871521
train gradient:  0.16684067716575812
iteration : 6549
train acc:  0.8203125
train loss:  0.3966520428657532
train gradient:  0.5553225750501081
iteration : 6550
train acc:  0.8359375
train loss:  0.3481174111366272
train gradient:  0.23643952549194497
iteration : 6551
train acc:  0.84375
train loss:  0.3132511079311371
train gradient:  0.19737876215740072
iteration : 6552
train acc:  0.8828125
train loss:  0.2950829267501831
train gradient:  0.1464325809386134
iteration : 6553
train acc:  0.859375
train loss:  0.3552343249320984
train gradient:  0.21813828710097127
iteration : 6554
train acc:  0.8359375
train loss:  0.3716168701648712
train gradient:  0.20415248800603142
iteration : 6555
train acc:  0.859375
train loss:  0.31037062406539917
train gradient:  0.1587619605603841
iteration : 6556
train acc:  0.828125
train loss:  0.2999475598335266
train gradient:  0.14027217883099788
iteration : 6557
train acc:  0.8515625
train loss:  0.324870765209198
train gradient:  0.14076177575724286
iteration : 6558
train acc:  0.7890625
train loss:  0.40472671389579773
train gradient:  0.2910386516505522
iteration : 6559
train acc:  0.84375
train loss:  0.3382231593132019
train gradient:  0.22852896948124823
iteration : 6560
train acc:  0.859375
train loss:  0.3199915587902069
train gradient:  0.1594855930477899
iteration : 6561
train acc:  0.84375
train loss:  0.34702354669570923
train gradient:  0.20712183711646837
iteration : 6562
train acc:  0.8671875
train loss:  0.33056676387786865
train gradient:  0.17320972113431746
iteration : 6563
train acc:  0.890625
train loss:  0.3318314552307129
train gradient:  0.33183705246316786
iteration : 6564
train acc:  0.8984375
train loss:  0.32816779613494873
train gradient:  0.18669122656417592
iteration : 6565
train acc:  0.828125
train loss:  0.35247647762298584
train gradient:  0.1846632535276454
iteration : 6566
train acc:  0.8671875
train loss:  0.3495124578475952
train gradient:  0.3872266600390327
iteration : 6567
train acc:  0.8828125
train loss:  0.3266167938709259
train gradient:  0.16361551703774155
iteration : 6568
train acc:  0.8515625
train loss:  0.37748849391937256
train gradient:  0.20759979175735332
iteration : 6569
train acc:  0.875
train loss:  0.2986658811569214
train gradient:  0.13268945463448095
iteration : 6570
train acc:  0.90625
train loss:  0.2645115852355957
train gradient:  0.1296010859930943
iteration : 6571
train acc:  0.8671875
train loss:  0.3032410740852356
train gradient:  0.18859885013809463
iteration : 6572
train acc:  0.875
train loss:  0.29286515712738037
train gradient:  0.13961254313310342
iteration : 6573
train acc:  0.8203125
train loss:  0.3821720778942108
train gradient:  0.21633476109766958
iteration : 6574
train acc:  0.828125
train loss:  0.4147723317146301
train gradient:  0.2668429074070809
iteration : 6575
train acc:  0.828125
train loss:  0.3642897307872772
train gradient:  0.3272469979173914
iteration : 6576
train acc:  0.84375
train loss:  0.34343504905700684
train gradient:  0.21978351222244033
iteration : 6577
train acc:  0.9140625
train loss:  0.2619604170322418
train gradient:  0.13553713788825222
iteration : 6578
train acc:  0.859375
train loss:  0.341028094291687
train gradient:  0.22998787526664882
iteration : 6579
train acc:  0.8671875
train loss:  0.348671019077301
train gradient:  0.17140330982038168
iteration : 6580
train acc:  0.890625
train loss:  0.2824399173259735
train gradient:  0.2524845194095164
iteration : 6581
train acc:  0.8828125
train loss:  0.34364980459213257
train gradient:  0.21066483774006184
iteration : 6582
train acc:  0.8125
train loss:  0.3445616364479065
train gradient:  0.20313658138332327
iteration : 6583
train acc:  0.8671875
train loss:  0.282390832901001
train gradient:  0.18723654820909574
iteration : 6584
train acc:  0.8515625
train loss:  0.29661092162132263
train gradient:  0.19632631459415922
iteration : 6585
train acc:  0.8828125
train loss:  0.312622606754303
train gradient:  0.1306009632374593
iteration : 6586
train acc:  0.8046875
train loss:  0.4385967254638672
train gradient:  0.29890480024038263
iteration : 6587
train acc:  0.8671875
train loss:  0.3281945586204529
train gradient:  0.16369491557917318
iteration : 6588
train acc:  0.8515625
train loss:  0.31144195795059204
train gradient:  0.18833504472505816
iteration : 6589
train acc:  0.8359375
train loss:  0.3480815887451172
train gradient:  0.23849221916961633
iteration : 6590
train acc:  0.890625
train loss:  0.25731217861175537
train gradient:  0.12352670539986939
iteration : 6591
train acc:  0.8828125
train loss:  0.30531829595565796
train gradient:  0.22979309889267271
iteration : 6592
train acc:  0.7734375
train loss:  0.41613513231277466
train gradient:  0.3494314842736438
iteration : 6593
train acc:  0.859375
train loss:  0.315731942653656
train gradient:  0.22054077040187905
iteration : 6594
train acc:  0.90625
train loss:  0.2455907166004181
train gradient:  0.12062582354480979
iteration : 6595
train acc:  0.859375
train loss:  0.288509339094162
train gradient:  0.12826707527273987
iteration : 6596
train acc:  0.8359375
train loss:  0.387645423412323
train gradient:  0.2994778000335936
iteration : 6597
train acc:  0.875
train loss:  0.3454875946044922
train gradient:  0.17796354194126546
iteration : 6598
train acc:  0.84375
train loss:  0.310996949672699
train gradient:  0.20592883855979693
iteration : 6599
train acc:  0.859375
train loss:  0.33192604780197144
train gradient:  0.3401886100363912
iteration : 6600
train acc:  0.8828125
train loss:  0.3268752992153168
train gradient:  0.1875725275680703
iteration : 6601
train acc:  0.828125
train loss:  0.330647349357605
train gradient:  0.23797135273217418
iteration : 6602
train acc:  0.8671875
train loss:  0.34610188007354736
train gradient:  0.20762514371556887
iteration : 6603
train acc:  0.8203125
train loss:  0.36447447538375854
train gradient:  0.2200039964920773
iteration : 6604
train acc:  0.8671875
train loss:  0.3112032115459442
train gradient:  0.23428166402707695
iteration : 6605
train acc:  0.8046875
train loss:  0.4458424150943756
train gradient:  0.3110610156475892
iteration : 6606
train acc:  0.90625
train loss:  0.2658884525299072
train gradient:  0.15419473462889577
iteration : 6607
train acc:  0.8203125
train loss:  0.3999525010585785
train gradient:  0.3650431129113597
iteration : 6608
train acc:  0.859375
train loss:  0.3518258333206177
train gradient:  0.4227093930946905
iteration : 6609
train acc:  0.8046875
train loss:  0.39099767804145813
train gradient:  0.2168528451690966
iteration : 6610
train acc:  0.8671875
train loss:  0.30994951725006104
train gradient:  0.1851968908866206
iteration : 6611
train acc:  0.8984375
train loss:  0.2771056592464447
train gradient:  0.14029880002502032
iteration : 6612
train acc:  0.875
train loss:  0.2991623282432556
train gradient:  0.15937396354395006
iteration : 6613
train acc:  0.84375
train loss:  0.34176912903785706
train gradient:  0.24432323191146388
iteration : 6614
train acc:  0.9140625
train loss:  0.3407021760940552
train gradient:  0.25121179580732117
iteration : 6615
train acc:  0.8671875
train loss:  0.35595816373825073
train gradient:  0.2869257384446862
iteration : 6616
train acc:  0.8359375
train loss:  0.4335562586784363
train gradient:  0.3225706888392737
iteration : 6617
train acc:  0.8984375
train loss:  0.30200523138046265
train gradient:  0.217829377836462
iteration : 6618
train acc:  0.890625
train loss:  0.249141126871109
train gradient:  0.1812453107722558
iteration : 6619
train acc:  0.84375
train loss:  0.3709603548049927
train gradient:  0.23597819275141552
iteration : 6620
train acc:  0.859375
train loss:  0.2925127446651459
train gradient:  0.16697359485920013
iteration : 6621
train acc:  0.828125
train loss:  0.42658787965774536
train gradient:  0.29866184204321655
iteration : 6622
train acc:  0.8671875
train loss:  0.33811038732528687
train gradient:  0.18262970738707313
iteration : 6623
train acc:  0.7890625
train loss:  0.3764660954475403
train gradient:  0.25712033387721717
iteration : 6624
train acc:  0.8671875
train loss:  0.28869447112083435
train gradient:  0.20323747834941824
iteration : 6625
train acc:  0.875
train loss:  0.3116902709007263
train gradient:  0.13119171671557317
iteration : 6626
train acc:  0.90625
train loss:  0.275404691696167
train gradient:  0.10000151260067329
iteration : 6627
train acc:  0.9140625
train loss:  0.24590952694416046
train gradient:  0.1643682893813097
iteration : 6628
train acc:  0.875
train loss:  0.30232304334640503
train gradient:  0.15235372789108348
iteration : 6629
train acc:  0.8515625
train loss:  0.32015007734298706
train gradient:  0.23520849733058619
iteration : 6630
train acc:  0.8359375
train loss:  0.40206193923950195
train gradient:  0.3139278554621574
iteration : 6631
train acc:  0.8515625
train loss:  0.3213441073894501
train gradient:  0.19115889673185596
iteration : 6632
train acc:  0.8671875
train loss:  0.3446468710899353
train gradient:  0.25979844800619806
iteration : 6633
train acc:  0.796875
train loss:  0.391005277633667
train gradient:  0.24619472251231267
iteration : 6634
train acc:  0.859375
train loss:  0.303908109664917
train gradient:  0.1279173296396085
iteration : 6635
train acc:  0.8671875
train loss:  0.3140438199043274
train gradient:  0.2257116797456804
iteration : 6636
train acc:  0.8671875
train loss:  0.3726869821548462
train gradient:  0.2202764462992176
iteration : 6637
train acc:  0.8828125
train loss:  0.3312288522720337
train gradient:  0.26211961020981334
iteration : 6638
train acc:  0.7890625
train loss:  0.4551162123680115
train gradient:  0.3861232112257127
iteration : 6639
train acc:  0.859375
train loss:  0.3250289559364319
train gradient:  0.18684579399219975
iteration : 6640
train acc:  0.8203125
train loss:  0.36171025037765503
train gradient:  0.17240575823979165
iteration : 6641
train acc:  0.8671875
train loss:  0.31772714853286743
train gradient:  0.22675163216220745
iteration : 6642
train acc:  0.8046875
train loss:  0.335918128490448
train gradient:  0.27839297687494674
iteration : 6643
train acc:  0.8359375
train loss:  0.3858382999897003
train gradient:  0.27605805947660805
iteration : 6644
train acc:  0.7734375
train loss:  0.43082767724990845
train gradient:  0.3104971979533397
iteration : 6645
train acc:  0.8515625
train loss:  0.3041328489780426
train gradient:  0.22882913105606073
iteration : 6646
train acc:  0.8359375
train loss:  0.3280567228794098
train gradient:  0.2653704723589125
iteration : 6647
train acc:  0.875
train loss:  0.3069952130317688
train gradient:  0.1868654857203379
iteration : 6648
train acc:  0.8125
train loss:  0.378121942281723
train gradient:  0.21703268625357586
iteration : 6649
train acc:  0.8046875
train loss:  0.3144645094871521
train gradient:  0.18270923124778485
iteration : 6650
train acc:  0.875
train loss:  0.3667741119861603
train gradient:  0.30025810397030556
iteration : 6651
train acc:  0.8515625
train loss:  0.35664796829223633
train gradient:  0.1941294373626352
iteration : 6652
train acc:  0.8671875
train loss:  0.33381789922714233
train gradient:  0.17981106551809023
iteration : 6653
train acc:  0.875
train loss:  0.29436105489730835
train gradient:  0.1842077687555974
iteration : 6654
train acc:  0.7890625
train loss:  0.4474107027053833
train gradient:  0.29612198428753284
iteration : 6655
train acc:  0.890625
train loss:  0.26100462675094604
train gradient:  0.13154385261126955
iteration : 6656
train acc:  0.859375
train loss:  0.3083629906177521
train gradient:  0.2246558932303311
iteration : 6657
train acc:  0.8125
train loss:  0.3946290612220764
train gradient:  0.20909125563942724
iteration : 6658
train acc:  0.8203125
train loss:  0.3334065079689026
train gradient:  0.15603172074651106
iteration : 6659
train acc:  0.8203125
train loss:  0.3537325859069824
train gradient:  0.3250879109618189
iteration : 6660
train acc:  0.8984375
train loss:  0.23378941416740417
train gradient:  0.12915866481245458
iteration : 6661
train acc:  0.8671875
train loss:  0.2968597412109375
train gradient:  0.14330739786149807
iteration : 6662
train acc:  0.8984375
train loss:  0.2809433937072754
train gradient:  0.20932888020945853
iteration : 6663
train acc:  0.8359375
train loss:  0.3552318215370178
train gradient:  0.195923133160812
iteration : 6664
train acc:  0.8671875
train loss:  0.38338136672973633
train gradient:  0.20329102522407677
iteration : 6665
train acc:  0.828125
train loss:  0.4055810570716858
train gradient:  0.27409360211505074
iteration : 6666
train acc:  0.875
train loss:  0.2901116609573364
train gradient:  0.38814283214989825
iteration : 6667
train acc:  0.84375
train loss:  0.36865347623825073
train gradient:  0.21603225422556277
iteration : 6668
train acc:  0.8046875
train loss:  0.3764547109603882
train gradient:  0.28893306072677866
iteration : 6669
train acc:  0.8671875
train loss:  0.33270707726478577
train gradient:  0.1760809239428266
iteration : 6670
train acc:  0.8515625
train loss:  0.40818506479263306
train gradient:  0.3448169240532604
iteration : 6671
train acc:  0.8515625
train loss:  0.30551016330718994
train gradient:  0.17711274814688965
iteration : 6672
train acc:  0.875
train loss:  0.29613780975341797
train gradient:  0.13525655595612154
iteration : 6673
train acc:  0.90625
train loss:  0.2875003516674042
train gradient:  0.1409605271349654
iteration : 6674
train acc:  0.7890625
train loss:  0.42213353514671326
train gradient:  0.2678876119945591
iteration : 6675
train acc:  0.7890625
train loss:  0.4027624726295471
train gradient:  0.3260988497195894
iteration : 6676
train acc:  0.8046875
train loss:  0.4179128408432007
train gradient:  0.251743828897986
iteration : 6677
train acc:  0.890625
train loss:  0.32285481691360474
train gradient:  0.1983263022509277
iteration : 6678
train acc:  0.84375
train loss:  0.34251856803894043
train gradient:  0.22589503795923963
iteration : 6679
train acc:  0.84375
train loss:  0.3756961226463318
train gradient:  0.2103949851689108
iteration : 6680
train acc:  0.859375
train loss:  0.3272905647754669
train gradient:  0.1454748084863639
iteration : 6681
train acc:  0.765625
train loss:  0.44909924268722534
train gradient:  0.4026320825519972
iteration : 6682
train acc:  0.8359375
train loss:  0.30715349316596985
train gradient:  0.15481313898061408
iteration : 6683
train acc:  0.8359375
train loss:  0.3897531032562256
train gradient:  0.16943940474845562
iteration : 6684
train acc:  0.84375
train loss:  0.32302606105804443
train gradient:  0.14633346896100008
iteration : 6685
train acc:  0.875
train loss:  0.35774144530296326
train gradient:  0.1947161720561118
iteration : 6686
train acc:  0.8125
train loss:  0.40325450897216797
train gradient:  0.2467789605257726
iteration : 6687
train acc:  0.8515625
train loss:  0.3243463337421417
train gradient:  0.18335717175215907
iteration : 6688
train acc:  0.8359375
train loss:  0.38274478912353516
train gradient:  0.3328810881856064
iteration : 6689
train acc:  0.8203125
train loss:  0.37184664607048035
train gradient:  0.2693589526546732
iteration : 6690
train acc:  0.8515625
train loss:  0.31061893701553345
train gradient:  0.19871373198697165
iteration : 6691
train acc:  0.84375
train loss:  0.3654753565788269
train gradient:  0.34855490109866294
iteration : 6692
train acc:  0.8203125
train loss:  0.38069796562194824
train gradient:  0.26072668124781323
iteration : 6693
train acc:  0.828125
train loss:  0.4066508412361145
train gradient:  0.278434425420203
iteration : 6694
train acc:  0.8828125
train loss:  0.31374287605285645
train gradient:  0.19676891621287632
iteration : 6695
train acc:  0.8046875
train loss:  0.4357348382472992
train gradient:  0.3025993382926095
iteration : 6696
train acc:  0.8515625
train loss:  0.3138916492462158
train gradient:  0.2453025339528152
iteration : 6697
train acc:  0.9140625
train loss:  0.26056185364723206
train gradient:  0.14136016180288719
iteration : 6698
train acc:  0.8515625
train loss:  0.3899270296096802
train gradient:  0.1813375567934174
iteration : 6699
train acc:  0.8515625
train loss:  0.3246707618236542
train gradient:  0.17365698083955797
iteration : 6700
train acc:  0.859375
train loss:  0.37174269556999207
train gradient:  0.2154422845483116
iteration : 6701
train acc:  0.8828125
train loss:  0.3642584979534149
train gradient:  0.32876137950407364
iteration : 6702
train acc:  0.8515625
train loss:  0.36769306659698486
train gradient:  0.25406381562071206
iteration : 6703
train acc:  0.8671875
train loss:  0.368208646774292
train gradient:  0.20301939008327835
iteration : 6704
train acc:  0.8828125
train loss:  0.2997451424598694
train gradient:  0.1430653124815082
iteration : 6705
train acc:  0.8359375
train loss:  0.3253534436225891
train gradient:  0.1600034697292827
iteration : 6706
train acc:  0.875
train loss:  0.34204816818237305
train gradient:  0.15838416487416715
iteration : 6707
train acc:  0.828125
train loss:  0.34930741786956787
train gradient:  0.22477350592232093
iteration : 6708
train acc:  0.875
train loss:  0.32388943433761597
train gradient:  0.14137388363972253
iteration : 6709
train acc:  0.8984375
train loss:  0.30561089515686035
train gradient:  0.1219684333122932
iteration : 6710
train acc:  0.8359375
train loss:  0.3419061601161957
train gradient:  0.1837854462189452
iteration : 6711
train acc:  0.859375
train loss:  0.3047521710395813
train gradient:  0.17325576785676167
iteration : 6712
train acc:  0.859375
train loss:  0.3612537980079651
train gradient:  0.27636197240378746
iteration : 6713
train acc:  0.8671875
train loss:  0.31145399808883667
train gradient:  0.133620766775622
iteration : 6714
train acc:  0.8828125
train loss:  0.3202948570251465
train gradient:  0.2126041322185183
iteration : 6715
train acc:  0.796875
train loss:  0.4098542332649231
train gradient:  0.2938343676641286
iteration : 6716
train acc:  0.8359375
train loss:  0.3439793586730957
train gradient:  0.1615291949456889
iteration : 6717
train acc:  0.875
train loss:  0.2821943759918213
train gradient:  0.14672074632219517
iteration : 6718
train acc:  0.90625
train loss:  0.229763463139534
train gradient:  0.11233116443359886
iteration : 6719
train acc:  0.8671875
train loss:  0.29613548517227173
train gradient:  0.14142665421630002
iteration : 6720
train acc:  0.7890625
train loss:  0.44594842195510864
train gradient:  0.23724492780299725
iteration : 6721
train acc:  0.8359375
train loss:  0.3489823341369629
train gradient:  0.2119054827696732
iteration : 6722
train acc:  0.84375
train loss:  0.3512713313102722
train gradient:  0.1637507950605388
iteration : 6723
train acc:  0.875
train loss:  0.3183305561542511
train gradient:  0.18977938462593574
iteration : 6724
train acc:  0.890625
train loss:  0.24791106581687927
train gradient:  0.10447441121904166
iteration : 6725
train acc:  0.859375
train loss:  0.3191675543785095
train gradient:  0.13444330396113147
iteration : 6726
train acc:  0.8125
train loss:  0.3416915237903595
train gradient:  0.20987388566388832
iteration : 6727
train acc:  0.828125
train loss:  0.396917462348938
train gradient:  0.2777674750764644
iteration : 6728
train acc:  0.8359375
train loss:  0.3418734669685364
train gradient:  0.18988872433381437
iteration : 6729
train acc:  0.8046875
train loss:  0.38557228446006775
train gradient:  0.22081690911335577
iteration : 6730
train acc:  0.8359375
train loss:  0.3549143671989441
train gradient:  0.23371211578921125
iteration : 6731
train acc:  0.8671875
train loss:  0.31789907813072205
train gradient:  0.17507189476382976
iteration : 6732
train acc:  0.8046875
train loss:  0.46169912815093994
train gradient:  0.3132873204647844
iteration : 6733
train acc:  0.875
train loss:  0.2869582772254944
train gradient:  0.19233383996686376
iteration : 6734
train acc:  0.8671875
train loss:  0.29391157627105713
train gradient:  0.1728085217982303
iteration : 6735
train acc:  0.8359375
train loss:  0.34923243522644043
train gradient:  0.2635187713139099
iteration : 6736
train acc:  0.84375
train loss:  0.34142357110977173
train gradient:  0.20260563703480278
iteration : 6737
train acc:  0.8984375
train loss:  0.28195664286613464
train gradient:  0.13663593095841747
iteration : 6738
train acc:  0.8359375
train loss:  0.3545687794685364
train gradient:  0.15169499238694553
iteration : 6739
train acc:  0.859375
train loss:  0.3016427159309387
train gradient:  0.14963641224533555
iteration : 6740
train acc:  0.890625
train loss:  0.29527077078819275
train gradient:  0.20052093719229686
iteration : 6741
train acc:  0.8515625
train loss:  0.35471782088279724
train gradient:  0.24861105738383946
iteration : 6742
train acc:  0.84375
train loss:  0.3227088749408722
train gradient:  0.16618449231707527
iteration : 6743
train acc:  0.8046875
train loss:  0.3723946213722229
train gradient:  0.3180385499141462
iteration : 6744
train acc:  0.8359375
train loss:  0.3164316415786743
train gradient:  0.22403502056758642
iteration : 6745
train acc:  0.828125
train loss:  0.35958132147789
train gradient:  0.24397346057616595
iteration : 6746
train acc:  0.875
train loss:  0.31615766882896423
train gradient:  0.12069845774489127
iteration : 6747
train acc:  0.84375
train loss:  0.39645370841026306
train gradient:  0.2212672986448509
iteration : 6748
train acc:  0.859375
train loss:  0.3863665461540222
train gradient:  0.24589881217475498
iteration : 6749
train acc:  0.8671875
train loss:  0.28622716665267944
train gradient:  0.13909716749202478
iteration : 6750
train acc:  0.8359375
train loss:  0.36783459782600403
train gradient:  0.2598521976091492
iteration : 6751
train acc:  0.8125
train loss:  0.3993620276451111
train gradient:  0.3794572170837524
iteration : 6752
train acc:  0.84375
train loss:  0.33593928813934326
train gradient:  0.19478733419441893
iteration : 6753
train acc:  0.8203125
train loss:  0.4039507508277893
train gradient:  0.2257774544763363
iteration : 6754
train acc:  0.7890625
train loss:  0.42361539602279663
train gradient:  0.2358030349947715
iteration : 6755
train acc:  0.8515625
train loss:  0.3084036707878113
train gradient:  0.17289799945936551
iteration : 6756
train acc:  0.8671875
train loss:  0.268259733915329
train gradient:  0.21004853218152944
iteration : 6757
train acc:  0.8984375
train loss:  0.29998981952667236
train gradient:  0.14024895407969723
iteration : 6758
train acc:  0.90625
train loss:  0.2622045576572418
train gradient:  0.09505613889583635
iteration : 6759
train acc:  0.8515625
train loss:  0.325728178024292
train gradient:  0.2832825933894235
iteration : 6760
train acc:  0.8125
train loss:  0.4233839511871338
train gradient:  0.2583506934959299
iteration : 6761
train acc:  0.8125
train loss:  0.3648430109024048
train gradient:  0.1797210200709336
iteration : 6762
train acc:  0.8671875
train loss:  0.3134363293647766
train gradient:  0.18610100827277767
iteration : 6763
train acc:  0.859375
train loss:  0.2813560962677002
train gradient:  0.17899317383527724
iteration : 6764
train acc:  0.859375
train loss:  0.3518543839454651
train gradient:  0.25241642293914707
iteration : 6765
train acc:  0.8515625
train loss:  0.36350587010383606
train gradient:  0.18696576519093555
iteration : 6766
train acc:  0.828125
train loss:  0.325080931186676
train gradient:  0.1491085231392847
iteration : 6767
train acc:  0.8515625
train loss:  0.3448805510997772
train gradient:  0.26088308489363987
iteration : 6768
train acc:  0.875
train loss:  0.31476300954818726
train gradient:  0.19602508119317258
iteration : 6769
train acc:  0.875
train loss:  0.3094348609447479
train gradient:  0.17120969371142084
iteration : 6770
train acc:  0.84375
train loss:  0.37665224075317383
train gradient:  0.22448439461680392
iteration : 6771
train acc:  0.8046875
train loss:  0.42637473344802856
train gradient:  0.2588045565737901
iteration : 6772
train acc:  0.8984375
train loss:  0.25981831550598145
train gradient:  0.16956926036017267
iteration : 6773
train acc:  0.8046875
train loss:  0.3659321069717407
train gradient:  0.1967677790309955
iteration : 6774
train acc:  0.8359375
train loss:  0.3183899521827698
train gradient:  0.13537262489814877
iteration : 6775
train acc:  0.8359375
train loss:  0.38270139694213867
train gradient:  0.2915287264177633
iteration : 6776
train acc:  0.8125
train loss:  0.38878893852233887
train gradient:  0.21712146468696214
iteration : 6777
train acc:  0.796875
train loss:  0.3977825939655304
train gradient:  0.2866446905981033
iteration : 6778
train acc:  0.875
train loss:  0.3147962689399719
train gradient:  0.1852694522577112
iteration : 6779
train acc:  0.859375
train loss:  0.3523675799369812
train gradient:  0.18577686319588574
iteration : 6780
train acc:  0.8515625
train loss:  0.34787121415138245
train gradient:  0.21529707804958306
iteration : 6781
train acc:  0.875
train loss:  0.30501043796539307
train gradient:  0.26812842979886475
iteration : 6782
train acc:  0.890625
train loss:  0.2917400896549225
train gradient:  0.13943785184666058
iteration : 6783
train acc:  0.890625
train loss:  0.27952051162719727
train gradient:  0.11907156217240923
iteration : 6784
train acc:  0.90625
train loss:  0.25946682691574097
train gradient:  0.14732674448837102
iteration : 6785
train acc:  0.828125
train loss:  0.34953930974006653
train gradient:  0.3092931771739866
iteration : 6786
train acc:  0.75
train loss:  0.442477285861969
train gradient:  0.38600093938557867
iteration : 6787
train acc:  0.8359375
train loss:  0.3708982765674591
train gradient:  0.312065716704298
iteration : 6788
train acc:  0.8984375
train loss:  0.29111772775650024
train gradient:  0.282671459269589
iteration : 6789
train acc:  0.859375
train loss:  0.30550286173820496
train gradient:  0.15073509493343884
iteration : 6790
train acc:  0.875
train loss:  0.31742584705352783
train gradient:  0.13910386380396209
iteration : 6791
train acc:  0.8671875
train loss:  0.2840733826160431
train gradient:  0.14909241644089036
iteration : 6792
train acc:  0.796875
train loss:  0.40720134973526
train gradient:  0.30459238484265866
iteration : 6793
train acc:  0.8671875
train loss:  0.3190987706184387
train gradient:  0.1628484087955399
iteration : 6794
train acc:  0.8125
train loss:  0.3621985614299774
train gradient:  0.2130676818822423
iteration : 6795
train acc:  0.890625
train loss:  0.2990884482860565
train gradient:  0.2277746593243476
iteration : 6796
train acc:  0.8125
train loss:  0.33024394512176514
train gradient:  0.20522420555525966
iteration : 6797
train acc:  0.875
train loss:  0.2775265872478485
train gradient:  0.14060752130216686
iteration : 6798
train acc:  0.890625
train loss:  0.29134973883628845
train gradient:  0.23635146500728788
iteration : 6799
train acc:  0.8671875
train loss:  0.3133731484413147
train gradient:  0.19956853384674939
iteration : 6800
train acc:  0.8515625
train loss:  0.29183048009872437
train gradient:  0.14244179216859668
iteration : 6801
train acc:  0.875
train loss:  0.26950740814208984
train gradient:  0.1801284398683137
iteration : 6802
train acc:  0.8984375
train loss:  0.2710038423538208
train gradient:  0.14030296316360813
iteration : 6803
train acc:  0.84375
train loss:  0.37309324741363525
train gradient:  0.2011260401579659
iteration : 6804
train acc:  0.8671875
train loss:  0.2733641564846039
train gradient:  0.17368538174975995
iteration : 6805
train acc:  0.8984375
train loss:  0.2762419283390045
train gradient:  0.15116478093016528
iteration : 6806
train acc:  0.8515625
train loss:  0.39828288555145264
train gradient:  0.3068597930438499
iteration : 6807
train acc:  0.828125
train loss:  0.41951391100883484
train gradient:  0.3155268751570761
iteration : 6808
train acc:  0.890625
train loss:  0.3103020191192627
train gradient:  0.2148297260780931
iteration : 6809
train acc:  0.8828125
train loss:  0.3016629219055176
train gradient:  0.1657273362495437
iteration : 6810
train acc:  0.7890625
train loss:  0.4576601982116699
train gradient:  0.3453849773646937
iteration : 6811
train acc:  0.828125
train loss:  0.39003562927246094
train gradient:  0.2603661577685509
iteration : 6812
train acc:  0.8671875
train loss:  0.30144602060317993
train gradient:  0.18029091292437605
iteration : 6813
train acc:  0.921875
train loss:  0.23796239495277405
train gradient:  0.11103736414096567
iteration : 6814
train acc:  0.828125
train loss:  0.33976614475250244
train gradient:  0.23684558681887363
iteration : 6815
train acc:  0.8515625
train loss:  0.34356236457824707
train gradient:  0.17002908545531076
iteration : 6816
train acc:  0.8125
train loss:  0.2987951636314392
train gradient:  0.17494970366894055
iteration : 6817
train acc:  0.8203125
train loss:  0.4197387099266052
train gradient:  0.2923683490487398
iteration : 6818
train acc:  0.859375
train loss:  0.35939228534698486
train gradient:  0.2619479942393223
iteration : 6819
train acc:  0.84375
train loss:  0.3878622055053711
train gradient:  0.2489405917472482
iteration : 6820
train acc:  0.84375
train loss:  0.30562350153923035
train gradient:  0.17791945576675905
iteration : 6821
train acc:  0.8515625
train loss:  0.33858728408813477
train gradient:  0.19238739238103802
iteration : 6822
train acc:  0.8828125
train loss:  0.3147185444831848
train gradient:  0.15928342414427432
iteration : 6823
train acc:  0.84375
train loss:  0.3177955746650696
train gradient:  0.19343216904509758
iteration : 6824
train acc:  0.8203125
train loss:  0.41775673627853394
train gradient:  0.3283404751740344
iteration : 6825
train acc:  0.8828125
train loss:  0.28032606840133667
train gradient:  0.12981344861996758
iteration : 6826
train acc:  0.921875
train loss:  0.19174182415008545
train gradient:  0.11122397641026628
iteration : 6827
train acc:  0.875
train loss:  0.2760790288448334
train gradient:  0.16103874432902696
iteration : 6828
train acc:  0.84375
train loss:  0.35592347383499146
train gradient:  0.2730868190798469
iteration : 6829
train acc:  0.796875
train loss:  0.40455949306488037
train gradient:  0.2552343958537565
iteration : 6830
train acc:  0.859375
train loss:  0.33619046211242676
train gradient:  0.2143600121274551
iteration : 6831
train acc:  0.828125
train loss:  0.32725441455841064
train gradient:  0.23671271539832583
iteration : 6832
train acc:  0.8359375
train loss:  0.3467344641685486
train gradient:  0.2912134089994126
iteration : 6833
train acc:  0.84375
train loss:  0.314270555973053
train gradient:  0.20640860427552543
iteration : 6834
train acc:  0.84375
train loss:  0.35516852140426636
train gradient:  0.17540793969650462
iteration : 6835
train acc:  0.8828125
train loss:  0.35424095392227173
train gradient:  0.18781033222847357
iteration : 6836
train acc:  0.8828125
train loss:  0.2800665497779846
train gradient:  0.09809501520728771
iteration : 6837
train acc:  0.828125
train loss:  0.32964810729026794
train gradient:  0.2000012724947397
iteration : 6838
train acc:  0.796875
train loss:  0.3495842516422272
train gradient:  0.2640372183760706
iteration : 6839
train acc:  0.84375
train loss:  0.3148282766342163
train gradient:  0.31562744229212786
iteration : 6840
train acc:  0.8828125
train loss:  0.28428584337234497
train gradient:  0.11888508619786363
iteration : 6841
train acc:  0.84375
train loss:  0.31850665807724
train gradient:  0.22238509439758866
iteration : 6842
train acc:  0.8828125
train loss:  0.2960485517978668
train gradient:  0.15346160184755842
iteration : 6843
train acc:  0.8828125
train loss:  0.29850757122039795
train gradient:  0.17202760495926347
iteration : 6844
train acc:  0.8359375
train loss:  0.3402981758117676
train gradient:  0.21733248885351286
iteration : 6845
train acc:  0.8125
train loss:  0.36482760310173035
train gradient:  0.2025244064487717
iteration : 6846
train acc:  0.8671875
train loss:  0.29892605543136597
train gradient:  0.2632002566048691
iteration : 6847
train acc:  0.8828125
train loss:  0.2802833914756775
train gradient:  0.201461450654173
iteration : 6848
train acc:  0.8671875
train loss:  0.3379438817501068
train gradient:  0.2761634909523623
iteration : 6849
train acc:  0.859375
train loss:  0.2936822772026062
train gradient:  0.18213591361020828
iteration : 6850
train acc:  0.8046875
train loss:  0.39907991886138916
train gradient:  0.2365222581065006
iteration : 6851
train acc:  0.8359375
train loss:  0.4226268529891968
train gradient:  0.30553371018094755
iteration : 6852
train acc:  0.8359375
train loss:  0.463386595249176
train gradient:  0.36246151670352456
iteration : 6853
train acc:  0.7890625
train loss:  0.4752293527126312
train gradient:  0.2902874531243499
iteration : 6854
train acc:  0.84375
train loss:  0.38553938269615173
train gradient:  0.2864962533936626
iteration : 6855
train acc:  0.8359375
train loss:  0.2991163730621338
train gradient:  0.130325648638563
iteration : 6856
train acc:  0.84375
train loss:  0.32225853204727173
train gradient:  0.15164803357164386
iteration : 6857
train acc:  0.84375
train loss:  0.32180482149124146
train gradient:  0.16214517358705757
iteration : 6858
train acc:  0.8984375
train loss:  0.28641772270202637
train gradient:  0.10499108275011715
iteration : 6859
train acc:  0.8359375
train loss:  0.36835598945617676
train gradient:  0.3107240497978245
iteration : 6860
train acc:  0.84375
train loss:  0.35058867931365967
train gradient:  0.5957908972413466
iteration : 6861
train acc:  0.8203125
train loss:  0.3695211410522461
train gradient:  0.2146412293350019
iteration : 6862
train acc:  0.859375
train loss:  0.32889324426651
train gradient:  0.25584586489195354
iteration : 6863
train acc:  0.8359375
train loss:  0.33947092294692993
train gradient:  0.19359506404236765
iteration : 6864
train acc:  0.8515625
train loss:  0.3505699038505554
train gradient:  0.17167070794361322
iteration : 6865
train acc:  0.875
train loss:  0.340610146522522
train gradient:  0.20074015094011266
iteration : 6866
train acc:  0.875
train loss:  0.31249701976776123
train gradient:  0.18558316994069823
iteration : 6867
train acc:  0.859375
train loss:  0.37275052070617676
train gradient:  0.2642388455496934
iteration : 6868
train acc:  0.890625
train loss:  0.2730731964111328
train gradient:  0.11946194805810763
iteration : 6869
train acc:  0.8515625
train loss:  0.3013436198234558
train gradient:  0.19491498190527354
iteration : 6870
train acc:  0.8125
train loss:  0.4125208854675293
train gradient:  0.3278192187350596
iteration : 6871
train acc:  0.8125
train loss:  0.41993317008018494
train gradient:  0.42584968501041554
iteration : 6872
train acc:  0.8125
train loss:  0.37779921293258667
train gradient:  0.2930349447587797
iteration : 6873
train acc:  0.8515625
train loss:  0.3333721160888672
train gradient:  0.2956738961724106
iteration : 6874
train acc:  0.7890625
train loss:  0.4197741746902466
train gradient:  0.2863444207319297
iteration : 6875
train acc:  0.890625
train loss:  0.2784814238548279
train gradient:  0.2272683017819334
iteration : 6876
train acc:  0.8828125
train loss:  0.29722359776496887
train gradient:  0.18149767771511954
iteration : 6877
train acc:  0.859375
train loss:  0.4369983673095703
train gradient:  0.23992231207337456
iteration : 6878
train acc:  0.8203125
train loss:  0.3738434314727783
train gradient:  0.24731574941920226
iteration : 6879
train acc:  0.8359375
train loss:  0.3641226887702942
train gradient:  0.221809439940885
iteration : 6880
train acc:  0.78125
train loss:  0.41756588220596313
train gradient:  0.2747240690387865
iteration : 6881
train acc:  0.84375
train loss:  0.33232852816581726
train gradient:  0.1992502325149777
iteration : 6882
train acc:  0.90625
train loss:  0.24281178414821625
train gradient:  0.12699119285265037
iteration : 6883
train acc:  0.8671875
train loss:  0.35580140352249146
train gradient:  0.3245310850863768
iteration : 6884
train acc:  0.875
train loss:  0.3291890025138855
train gradient:  0.2048947839853931
iteration : 6885
train acc:  0.828125
train loss:  0.3984992802143097
train gradient:  0.2511072999723261
iteration : 6886
train acc:  0.8203125
train loss:  0.3470447063446045
train gradient:  0.2413493535699374
iteration : 6887
train acc:  0.859375
train loss:  0.35373640060424805
train gradient:  0.18769977174705482
iteration : 6888
train acc:  0.859375
train loss:  0.31136438250541687
train gradient:  0.2007740892159756
iteration : 6889
train acc:  0.8515625
train loss:  0.32697200775146484
train gradient:  0.15991108178963478
iteration : 6890
train acc:  0.8359375
train loss:  0.38792312145233154
train gradient:  0.2245359754136146
iteration : 6891
train acc:  0.859375
train loss:  0.34891432523727417
train gradient:  0.26569789177367925
iteration : 6892
train acc:  0.875
train loss:  0.3324665427207947
train gradient:  0.1720748119848386
iteration : 6893
train acc:  0.875
train loss:  0.339779794216156
train gradient:  0.2087978367312091
iteration : 6894
train acc:  0.828125
train loss:  0.3800302743911743
train gradient:  0.20925494300472927
iteration : 6895
train acc:  0.84375
train loss:  0.3662887215614319
train gradient:  0.2517436861897972
iteration : 6896
train acc:  0.90625
train loss:  0.27606362104415894
train gradient:  0.10643737794538118
iteration : 6897
train acc:  0.875
train loss:  0.3950842022895813
train gradient:  0.33572920932339784
iteration : 6898
train acc:  0.875
train loss:  0.3633542060852051
train gradient:  0.49854367852667203
iteration : 6899
train acc:  0.8515625
train loss:  0.33305805921554565
train gradient:  0.2795033923248622
iteration : 6900
train acc:  0.875
train loss:  0.2951611280441284
train gradient:  0.1950929056666914
iteration : 6901
train acc:  0.84375
train loss:  0.3299689292907715
train gradient:  0.25780074296559385
iteration : 6902
train acc:  0.8203125
train loss:  0.4419351816177368
train gradient:  0.3018424279405481
iteration : 6903
train acc:  0.8515625
train loss:  0.394681841135025
train gradient:  0.24801829421576949
iteration : 6904
train acc:  0.8515625
train loss:  0.32062357664108276
train gradient:  0.2776764294646572
iteration : 6905
train acc:  0.828125
train loss:  0.3597540855407715
train gradient:  0.2332719567637913
iteration : 6906
train acc:  0.8125
train loss:  0.4163937568664551
train gradient:  0.3136739544300683
iteration : 6907
train acc:  0.859375
train loss:  0.3311222493648529
train gradient:  0.19323309188397303
iteration : 6908
train acc:  0.8515625
train loss:  0.33120492100715637
train gradient:  0.14097726765526475
iteration : 6909
train acc:  0.8359375
train loss:  0.3199581503868103
train gradient:  0.21609238373466158
iteration : 6910
train acc:  0.796875
train loss:  0.43934980034828186
train gradient:  0.3862508999229938
iteration : 6911
train acc:  0.859375
train loss:  0.3216133117675781
train gradient:  0.18167810750193092
iteration : 6912
train acc:  0.859375
train loss:  0.32805919647216797
train gradient:  0.25175147461603975
iteration : 6913
train acc:  0.84375
train loss:  0.300319105386734
train gradient:  0.1651972577083721
iteration : 6914
train acc:  0.90625
train loss:  0.2648223638534546
train gradient:  0.10189670769050527
iteration : 6915
train acc:  0.8515625
train loss:  0.30519023537635803
train gradient:  0.1663347773167342
iteration : 6916
train acc:  0.84375
train loss:  0.3817360997200012
train gradient:  0.28214170457733223
iteration : 6917
train acc:  0.8984375
train loss:  0.30219608545303345
train gradient:  0.1723217641895102
iteration : 6918
train acc:  0.8984375
train loss:  0.2740684449672699
train gradient:  0.12642692783964404
iteration : 6919
train acc:  0.8671875
train loss:  0.34203964471817017
train gradient:  0.24201123393589982
iteration : 6920
train acc:  0.8828125
train loss:  0.3173007071018219
train gradient:  0.22112904004873096
iteration : 6921
train acc:  0.828125
train loss:  0.3536072373390198
train gradient:  0.22286760696825
iteration : 6922
train acc:  0.8203125
train loss:  0.43988922238349915
train gradient:  0.26366341749278027
iteration : 6923
train acc:  0.828125
train loss:  0.37473684549331665
train gradient:  0.2413445332114354
iteration : 6924
train acc:  0.8046875
train loss:  0.4587763547897339
train gradient:  0.27209617068297576
iteration : 6925
train acc:  0.828125
train loss:  0.4182892441749573
train gradient:  0.26701159015336545
iteration : 6926
train acc:  0.875
train loss:  0.3171326816082001
train gradient:  0.2469664771046387
iteration : 6927
train acc:  0.8828125
train loss:  0.28748494386672974
train gradient:  0.14659339285769507
iteration : 6928
train acc:  0.8984375
train loss:  0.29718518257141113
train gradient:  0.1302606760344016
iteration : 6929
train acc:  0.8203125
train loss:  0.40433523058891296
train gradient:  0.29838939128281444
iteration : 6930
train acc:  0.8671875
train loss:  0.39073503017425537
train gradient:  0.3041064185934253
iteration : 6931
train acc:  0.84375
train loss:  0.32322466373443604
train gradient:  0.1822238227486646
iteration : 6932
train acc:  0.8515625
train loss:  0.3287414312362671
train gradient:  0.1751672687751201
iteration : 6933
train acc:  0.75
train loss:  0.49668997526168823
train gradient:  0.38556131266054844
iteration : 6934
train acc:  0.8671875
train loss:  0.28385576605796814
train gradient:  0.16616351438802435
iteration : 6935
train acc:  0.8515625
train loss:  0.2918093204498291
train gradient:  0.16983249432067787
iteration : 6936
train acc:  0.8671875
train loss:  0.32914823293685913
train gradient:  0.20525260886958635
iteration : 6937
train acc:  0.8828125
train loss:  0.3677111268043518
train gradient:  0.21439110544898377
iteration : 6938
train acc:  0.875
train loss:  0.321902334690094
train gradient:  0.16467785296138324
iteration : 6939
train acc:  0.828125
train loss:  0.4142336845397949
train gradient:  0.26841829428451086
iteration : 6940
train acc:  0.8203125
train loss:  0.4039047658443451
train gradient:  0.2714212632388161
iteration : 6941
train acc:  0.828125
train loss:  0.4000750482082367
train gradient:  0.2791114574579033
iteration : 6942
train acc:  0.7890625
train loss:  0.4428402781486511
train gradient:  0.3939206837197824
iteration : 6943
train acc:  0.890625
train loss:  0.2898094654083252
train gradient:  0.16782912766991998
iteration : 6944
train acc:  0.859375
train loss:  0.3123921751976013
train gradient:  0.19644329281899015
iteration : 6945
train acc:  0.84375
train loss:  0.33612126111984253
train gradient:  0.14903063306203793
iteration : 6946
train acc:  0.90625
train loss:  0.24550384283065796
train gradient:  0.13538706666960626
iteration : 6947
train acc:  0.8359375
train loss:  0.39718741178512573
train gradient:  0.42850269203128366
iteration : 6948
train acc:  0.8203125
train loss:  0.3542420566082001
train gradient:  0.20928771177185518
iteration : 6949
train acc:  0.8515625
train loss:  0.33676987886428833
train gradient:  0.1774382663127907
iteration : 6950
train acc:  0.859375
train loss:  0.31142711639404297
train gradient:  0.1370488541257161
iteration : 6951
train acc:  0.875
train loss:  0.289787232875824
train gradient:  0.1874779601394733
iteration : 6952
train acc:  0.8828125
train loss:  0.34825703501701355
train gradient:  0.22525241312220523
iteration : 6953
train acc:  0.8984375
train loss:  0.26774919033050537
train gradient:  0.14796022978953183
iteration : 6954
train acc:  0.8984375
train loss:  0.2799869477748871
train gradient:  0.12833500618958568
iteration : 6955
train acc:  0.8984375
train loss:  0.3171048164367676
train gradient:  0.21804571411588836
iteration : 6956
train acc:  0.828125
train loss:  0.4237426817417145
train gradient:  0.3599579069532641
iteration : 6957
train acc:  0.8671875
train loss:  0.31333693861961365
train gradient:  0.14477725007126835
iteration : 6958
train acc:  0.9140625
train loss:  0.22829288244247437
train gradient:  0.16371924644323285
iteration : 6959
train acc:  0.84375
train loss:  0.42040640115737915
train gradient:  0.22192108916486375
iteration : 6960
train acc:  0.8515625
train loss:  0.30097290873527527
train gradient:  0.15572266311003657
iteration : 6961
train acc:  0.875
train loss:  0.3103034496307373
train gradient:  0.1628939304666426
iteration : 6962
train acc:  0.875
train loss:  0.257289856672287
train gradient:  0.13127419722595024
iteration : 6963
train acc:  0.8671875
train loss:  0.28366702795028687
train gradient:  0.17110314840350693
iteration : 6964
train acc:  0.859375
train loss:  0.2955735921859741
train gradient:  0.19303784674352997
iteration : 6965
train acc:  0.8671875
train loss:  0.28977933526039124
train gradient:  0.2253505655706981
iteration : 6966
train acc:  0.8828125
train loss:  0.2839008569717407
train gradient:  0.18112770822856158
iteration : 6967
train acc:  0.8671875
train loss:  0.2887113094329834
train gradient:  0.16634494020332735
iteration : 6968
train acc:  0.8359375
train loss:  0.3641475439071655
train gradient:  0.22329473155490523
iteration : 6969
train acc:  0.8203125
train loss:  0.3993913531303406
train gradient:  0.22634372448874623
iteration : 6970
train acc:  0.84375
train loss:  0.32387208938598633
train gradient:  0.14868306726582403
iteration : 6971
train acc:  0.90625
train loss:  0.27155590057373047
train gradient:  0.12043176736895979
iteration : 6972
train acc:  0.875
train loss:  0.3274674415588379
train gradient:  0.2102928964107288
iteration : 6973
train acc:  0.8359375
train loss:  0.3392053544521332
train gradient:  0.1912667020584661
iteration : 6974
train acc:  0.890625
train loss:  0.2611725330352783
train gradient:  0.1579831258683425
iteration : 6975
train acc:  0.84375
train loss:  0.3370518088340759
train gradient:  0.20545440890241537
iteration : 6976
train acc:  0.890625
train loss:  0.2789466083049774
train gradient:  0.14878636929680616
iteration : 6977
train acc:  0.890625
train loss:  0.27900418639183044
train gradient:  0.14827848623940762
iteration : 6978
train acc:  0.828125
train loss:  0.4110356867313385
train gradient:  0.31192534878835476
iteration : 6979
train acc:  0.859375
train loss:  0.2961891293525696
train gradient:  0.15710168147667986
iteration : 6980
train acc:  0.8515625
train loss:  0.3548080325126648
train gradient:  0.17082559193162195
iteration : 6981
train acc:  0.84375
train loss:  0.3669544756412506
train gradient:  0.15469042515699194
iteration : 6982
train acc:  0.8671875
train loss:  0.34270477294921875
train gradient:  0.23187702813209976
iteration : 6983
train acc:  0.8515625
train loss:  0.36563006043434143
train gradient:  0.25989898424664976
iteration : 6984
train acc:  0.84375
train loss:  0.308976411819458
train gradient:  0.15397547769085734
iteration : 6985
train acc:  0.875
train loss:  0.3051566481590271
train gradient:  0.14069185955948393
iteration : 6986
train acc:  0.8828125
train loss:  0.3071076571941376
train gradient:  0.21082308274450975
iteration : 6987
train acc:  0.9375
train loss:  0.20210814476013184
train gradient:  0.13980386859343752
iteration : 6988
train acc:  0.8515625
train loss:  0.35340625047683716
train gradient:  0.18536206502583055
iteration : 6989
train acc:  0.8671875
train loss:  0.29059934616088867
train gradient:  0.15348719848799508
iteration : 6990
train acc:  0.8515625
train loss:  0.3206328749656677
train gradient:  0.1457491525027424
iteration : 6991
train acc:  0.796875
train loss:  0.38416147232055664
train gradient:  0.20527374449550506
iteration : 6992
train acc:  0.8515625
train loss:  0.3303496837615967
train gradient:  0.1558213876036066
iteration : 6993
train acc:  0.8671875
train loss:  0.3062030076980591
train gradient:  0.1686056176983471
iteration : 6994
train acc:  0.8515625
train loss:  0.2953186631202698
train gradient:  0.18869567714077673
iteration : 6995
train acc:  0.828125
train loss:  0.4121851325035095
train gradient:  0.2463736724152944
iteration : 6996
train acc:  0.84375
train loss:  0.3429299592971802
train gradient:  0.20281624187643044
iteration : 6997
train acc:  0.8359375
train loss:  0.3853781223297119
train gradient:  0.19595423064963793
iteration : 6998
train acc:  0.8828125
train loss:  0.23342721164226532
train gradient:  0.10386055674897833
iteration : 6999
train acc:  0.9140625
train loss:  0.22396431863307953
train gradient:  0.12269399987989583
iteration : 7000
train acc:  0.8671875
train loss:  0.2961161732673645
train gradient:  0.132627982503294
iteration : 7001
train acc:  0.84375
train loss:  0.3547028601169586
train gradient:  0.22325822954229363
iteration : 7002
train acc:  0.8828125
train loss:  0.29982447624206543
train gradient:  0.16836416012049255
iteration : 7003
train acc:  0.84375
train loss:  0.3520430326461792
train gradient:  0.19887386570006074
iteration : 7004
train acc:  0.828125
train loss:  0.3518994152545929
train gradient:  0.25314169607199877
iteration : 7005
train acc:  0.8203125
train loss:  0.395236611366272
train gradient:  0.28675741462034543
iteration : 7006
train acc:  0.828125
train loss:  0.383036345243454
train gradient:  0.2914615014930244
iteration : 7007
train acc:  0.8984375
train loss:  0.30125662684440613
train gradient:  0.18929686323928727
iteration : 7008
train acc:  0.8125
train loss:  0.38007956743240356
train gradient:  0.27407371499258293
iteration : 7009
train acc:  0.828125
train loss:  0.3848956227302551
train gradient:  0.2778035312987108
iteration : 7010
train acc:  0.765625
train loss:  0.6200050711631775
train gradient:  0.6067973870200289
iteration : 7011
train acc:  0.828125
train loss:  0.3523682653903961
train gradient:  0.21204781118452634
iteration : 7012
train acc:  0.8671875
train loss:  0.29101550579071045
train gradient:  0.11016104090518064
iteration : 7013
train acc:  0.890625
train loss:  0.2995031177997589
train gradient:  0.17451809397336948
iteration : 7014
train acc:  0.8203125
train loss:  0.41930484771728516
train gradient:  0.23414317532138623
iteration : 7015
train acc:  0.875
train loss:  0.27403658628463745
train gradient:  0.14632798934399693
iteration : 7016
train acc:  0.8359375
train loss:  0.29537904262542725
train gradient:  0.14398403411847197
iteration : 7017
train acc:  0.8984375
train loss:  0.29511553049087524
train gradient:  0.19162253177893235
iteration : 7018
train acc:  0.859375
train loss:  0.31791067123413086
train gradient:  0.226301401963288
iteration : 7019
train acc:  0.8046875
train loss:  0.3853444755077362
train gradient:  0.1988030992924565
iteration : 7020
train acc:  0.8671875
train loss:  0.3202756345272064
train gradient:  0.18283595959179935
iteration : 7021
train acc:  0.8203125
train loss:  0.3645686209201813
train gradient:  0.24321947312984907
iteration : 7022
train acc:  0.859375
train loss:  0.3133023679256439
train gradient:  0.19897621571962615
iteration : 7023
train acc:  0.84375
train loss:  0.3947400450706482
train gradient:  0.2597739187721659
iteration : 7024
train acc:  0.8125
train loss:  0.4188882112503052
train gradient:  0.29593023018543757
iteration : 7025
train acc:  0.84375
train loss:  0.35380852222442627
train gradient:  0.2363887465573692
iteration : 7026
train acc:  0.8671875
train loss:  0.2888774573802948
train gradient:  0.17217222055133863
iteration : 7027
train acc:  0.8515625
train loss:  0.3324095606803894
train gradient:  0.22781616507934832
iteration : 7028
train acc:  0.90625
train loss:  0.26343828439712524
train gradient:  0.12602048055734727
iteration : 7029
train acc:  0.8828125
train loss:  0.3008556365966797
train gradient:  0.21479586221610644
iteration : 7030
train acc:  0.859375
train loss:  0.2916485667228699
train gradient:  0.20027520754927908
iteration : 7031
train acc:  0.8203125
train loss:  0.34091901779174805
train gradient:  0.284031394291484
iteration : 7032
train acc:  0.8515625
train loss:  0.36277657747268677
train gradient:  0.274629138837035
iteration : 7033
train acc:  0.8203125
train loss:  0.3551085591316223
train gradient:  0.3093819308637847
iteration : 7034
train acc:  0.8046875
train loss:  0.3935711979866028
train gradient:  0.5331306765126389
iteration : 7035
train acc:  0.828125
train loss:  0.3702409863471985
train gradient:  0.22089890178704344
iteration : 7036
train acc:  0.8359375
train loss:  0.3663068413734436
train gradient:  0.20381812368374924
iteration : 7037
train acc:  0.796875
train loss:  0.37457382678985596
train gradient:  0.25567015230780277
iteration : 7038
train acc:  0.8828125
train loss:  0.2810623347759247
train gradient:  0.19670623205742155
iteration : 7039
train acc:  0.8515625
train loss:  0.3756515085697174
train gradient:  0.2890372957615694
iteration : 7040
train acc:  0.8828125
train loss:  0.28996706008911133
train gradient:  0.1379728035208344
iteration : 7041
train acc:  0.8984375
train loss:  0.28470540046691895
train gradient:  0.18601065038603326
iteration : 7042
train acc:  0.890625
train loss:  0.2838380038738251
train gradient:  0.16409529735001488
iteration : 7043
train acc:  0.875
train loss:  0.3396990895271301
train gradient:  0.24357576201716163
iteration : 7044
train acc:  0.8828125
train loss:  0.2684767246246338
train gradient:  0.13590359981199684
iteration : 7045
train acc:  0.8359375
train loss:  0.2943383455276489
train gradient:  0.17230072867702045
iteration : 7046
train acc:  0.890625
train loss:  0.2739434540271759
train gradient:  0.21835936093903002
iteration : 7047
train acc:  0.796875
train loss:  0.4514807164669037
train gradient:  0.30171884131340126
iteration : 7048
train acc:  0.84375
train loss:  0.31678587198257446
train gradient:  0.1802117918788072
iteration : 7049
train acc:  0.78125
train loss:  0.3655346632003784
train gradient:  0.2048796106990629
iteration : 7050
train acc:  0.90625
train loss:  0.26479002833366394
train gradient:  0.1283730634032007
iteration : 7051
train acc:  0.859375
train loss:  0.4234016537666321
train gradient:  0.39133057800034887
iteration : 7052
train acc:  0.828125
train loss:  0.3713931441307068
train gradient:  0.23398930265922707
iteration : 7053
train acc:  0.8203125
train loss:  0.38279080390930176
train gradient:  0.21945078337354998
iteration : 7054
train acc:  0.890625
train loss:  0.24868062138557434
train gradient:  0.11862570794245697
iteration : 7055
train acc:  0.796875
train loss:  0.37766745686531067
train gradient:  0.2364338621194478
iteration : 7056
train acc:  0.8515625
train loss:  0.33187979459762573
train gradient:  0.20308283232565222
iteration : 7057
train acc:  0.8125
train loss:  0.388193815946579
train gradient:  0.2514990817102918
iteration : 7058
train acc:  0.8671875
train loss:  0.32471561431884766
train gradient:  0.17835174829051367
iteration : 7059
train acc:  0.8828125
train loss:  0.32427430152893066
train gradient:  0.1965478059944383
iteration : 7060
train acc:  0.8359375
train loss:  0.35818809270858765
train gradient:  0.20395075333865942
iteration : 7061
train acc:  0.8359375
train loss:  0.362545907497406
train gradient:  0.17845786108244044
iteration : 7062
train acc:  0.9140625
train loss:  0.2718456983566284
train gradient:  0.12473133657859044
iteration : 7063
train acc:  0.8125
train loss:  0.3828083872795105
train gradient:  0.22425871398740427
iteration : 7064
train acc:  0.890625
train loss:  0.2846166789531708
train gradient:  0.18757638883914524
iteration : 7065
train acc:  0.875
train loss:  0.33406853675842285
train gradient:  0.20485022466601327
iteration : 7066
train acc:  0.8359375
train loss:  0.35569828748703003
train gradient:  0.20056566014625504
iteration : 7067
train acc:  0.84375
train loss:  0.3743641972541809
train gradient:  0.1915147578230382
iteration : 7068
train acc:  0.8203125
train loss:  0.4058046042919159
train gradient:  0.2776281180411013
iteration : 7069
train acc:  0.8203125
train loss:  0.3653737008571625
train gradient:  0.16930141974206672
iteration : 7070
train acc:  0.8515625
train loss:  0.32945579290390015
train gradient:  0.2518520580029134
iteration : 7071
train acc:  0.890625
train loss:  0.3365565836429596
train gradient:  0.22378708048197457
iteration : 7072
train acc:  0.90625
train loss:  0.26602423191070557
train gradient:  0.10087639567634626
iteration : 7073
train acc:  0.84375
train loss:  0.3464903235435486
train gradient:  0.29699174617488994
iteration : 7074
train acc:  0.8828125
train loss:  0.30317914485931396
train gradient:  0.14551589714759838
iteration : 7075
train acc:  0.8671875
train loss:  0.3472488522529602
train gradient:  0.17912169660022773
iteration : 7076
train acc:  0.7890625
train loss:  0.42389461398124695
train gradient:  0.32044321364506156
iteration : 7077
train acc:  0.8203125
train loss:  0.4506836533546448
train gradient:  0.40481661210323183
iteration : 7078
train acc:  0.890625
train loss:  0.2563524842262268
train gradient:  0.14486933766517324
iteration : 7079
train acc:  0.890625
train loss:  0.32907789945602417
train gradient:  0.24017173644474052
iteration : 7080
train acc:  0.8671875
train loss:  0.335540771484375
train gradient:  0.2465472378038646
iteration : 7081
train acc:  0.828125
train loss:  0.3894687294960022
train gradient:  0.20459033889510486
iteration : 7082
train acc:  0.8515625
train loss:  0.33985063433647156
train gradient:  0.24663774887512555
iteration : 7083
train acc:  0.828125
train loss:  0.31731563806533813
train gradient:  0.19556318640154238
iteration : 7084
train acc:  0.8515625
train loss:  0.3358628451824188
train gradient:  0.2941394462604116
iteration : 7085
train acc:  0.8125
train loss:  0.35895031690597534
train gradient:  0.15982845936766607
iteration : 7086
train acc:  0.828125
train loss:  0.37963879108428955
train gradient:  0.3430877788498391
iteration : 7087
train acc:  0.828125
train loss:  0.37788987159729004
train gradient:  0.22843278113041696
iteration : 7088
train acc:  0.859375
train loss:  0.303732693195343
train gradient:  0.1743255344286839
iteration : 7089
train acc:  0.875
train loss:  0.25946348905563354
train gradient:  0.10611165005673784
iteration : 7090
train acc:  0.921875
train loss:  0.2440013289451599
train gradient:  0.14387410066583667
iteration : 7091
train acc:  0.8828125
train loss:  0.3005737066268921
train gradient:  0.1367888393084733
iteration : 7092
train acc:  0.8046875
train loss:  0.44559717178344727
train gradient:  0.3198034628309125
iteration : 7093
train acc:  0.8203125
train loss:  0.4206188917160034
train gradient:  0.4144905523753991
iteration : 7094
train acc:  0.8671875
train loss:  0.29551953077316284
train gradient:  0.17700074658947448
iteration : 7095
train acc:  0.8515625
train loss:  0.4145652949810028
train gradient:  0.27104761630591045
iteration : 7096
train acc:  0.84375
train loss:  0.36986052989959717
train gradient:  0.22748576723568556
iteration : 7097
train acc:  0.8359375
train loss:  0.35844072699546814
train gradient:  0.29332890090786795
iteration : 7098
train acc:  0.859375
train loss:  0.30679598450660706
train gradient:  0.1936263435212615
iteration : 7099
train acc:  0.84375
train loss:  0.3516473174095154
train gradient:  0.20950223644023147
iteration : 7100
train acc:  0.8203125
train loss:  0.35957667231559753
train gradient:  0.2227082484870019
iteration : 7101
train acc:  0.875
train loss:  0.25715816020965576
train gradient:  0.15411921067823506
iteration : 7102
train acc:  0.859375
train loss:  0.345201313495636
train gradient:  0.3840911304177152
iteration : 7103
train acc:  0.8046875
train loss:  0.39878231287002563
train gradient:  0.2942109028341501
iteration : 7104
train acc:  0.8359375
train loss:  0.41160523891448975
train gradient:  0.2306112265949023
iteration : 7105
train acc:  0.859375
train loss:  0.37609827518463135
train gradient:  0.224849297026054
iteration : 7106
train acc:  0.8515625
train loss:  0.30466538667678833
train gradient:  0.16278096789531063
iteration : 7107
train acc:  0.84375
train loss:  0.3785138428211212
train gradient:  0.2161597910326918
iteration : 7108
train acc:  0.9140625
train loss:  0.26200389862060547
train gradient:  0.15697397155582288
iteration : 7109
train acc:  0.8046875
train loss:  0.3686012029647827
train gradient:  0.19148736439912342
iteration : 7110
train acc:  0.8359375
train loss:  0.337557315826416
train gradient:  0.15567263451870605
iteration : 7111
train acc:  0.8125
train loss:  0.3674546778202057
train gradient:  0.24669568893735971
iteration : 7112
train acc:  0.828125
train loss:  0.3970237374305725
train gradient:  0.21080977086871996
iteration : 7113
train acc:  0.859375
train loss:  0.3019019663333893
train gradient:  0.13185724534319515
iteration : 7114
train acc:  0.859375
train loss:  0.3242461085319519
train gradient:  0.16806143661716438
iteration : 7115
train acc:  0.875
train loss:  0.31256887316703796
train gradient:  0.16598439557699668
iteration : 7116
train acc:  0.84375
train loss:  0.34148210287094116
train gradient:  0.18793349069271273
iteration : 7117
train acc:  0.9140625
train loss:  0.28466200828552246
train gradient:  0.19016384936196665
iteration : 7118
train acc:  0.84375
train loss:  0.32506030797958374
train gradient:  0.20701772309383898
iteration : 7119
train acc:  0.8515625
train loss:  0.3401382863521576
train gradient:  0.20084355231405598
iteration : 7120
train acc:  0.828125
train loss:  0.31707265973091125
train gradient:  0.16268519908898757
iteration : 7121
train acc:  0.8671875
train loss:  0.29453569650650024
train gradient:  0.13850406594998527
iteration : 7122
train acc:  0.875
train loss:  0.3093300461769104
train gradient:  0.13789681093584652
iteration : 7123
train acc:  0.84375
train loss:  0.34317541122436523
train gradient:  0.23293211671140618
iteration : 7124
train acc:  0.859375
train loss:  0.32051214575767517
train gradient:  0.21209725913524935
iteration : 7125
train acc:  0.8515625
train loss:  0.2987918257713318
train gradient:  0.19916291495428412
iteration : 7126
train acc:  0.8828125
train loss:  0.28054890036582947
train gradient:  0.13444702187532453
iteration : 7127
train acc:  0.8984375
train loss:  0.2808942198753357
train gradient:  0.19910024877759525
iteration : 7128
train acc:  0.8125
train loss:  0.3960374593734741
train gradient:  0.2356260083383652
iteration : 7129
train acc:  0.8359375
train loss:  0.3979731798171997
train gradient:  0.1968783921360974
iteration : 7130
train acc:  0.84375
train loss:  0.33573269844055176
train gradient:  0.20648196102593658
iteration : 7131
train acc:  0.8359375
train loss:  0.35725143551826477
train gradient:  0.28805369097040834
iteration : 7132
train acc:  0.8359375
train loss:  0.385417640209198
train gradient:  0.2100622998319413
iteration : 7133
train acc:  0.875
train loss:  0.34273314476013184
train gradient:  0.15525214852826574
iteration : 7134
train acc:  0.828125
train loss:  0.35194212198257446
train gradient:  0.22671255982955574
iteration : 7135
train acc:  0.8203125
train loss:  0.33527815341949463
train gradient:  0.1958586489932861
iteration : 7136
train acc:  0.8203125
train loss:  0.3785521984100342
train gradient:  0.2177555537890921
iteration : 7137
train acc:  0.8203125
train loss:  0.38405901193618774
train gradient:  0.25454965556949904
iteration : 7138
train acc:  0.890625
train loss:  0.2922154664993286
train gradient:  0.14470230316295127
iteration : 7139
train acc:  0.828125
train loss:  0.36185628175735474
train gradient:  0.21511974306115242
iteration : 7140
train acc:  0.8359375
train loss:  0.31566646695137024
train gradient:  0.16244730195395618
iteration : 7141
train acc:  0.8984375
train loss:  0.25936391949653625
train gradient:  0.14705238872781734
iteration : 7142
train acc:  0.890625
train loss:  0.33608680963516235
train gradient:  0.1523397343208293
iteration : 7143
train acc:  0.8125
train loss:  0.38343310356140137
train gradient:  0.24720111629451996
iteration : 7144
train acc:  0.8515625
train loss:  0.31890222430229187
train gradient:  0.17916969448084744
iteration : 7145
train acc:  0.8671875
train loss:  0.3146805763244629
train gradient:  0.20167566343292676
iteration : 7146
train acc:  0.875
train loss:  0.29816508293151855
train gradient:  0.2090181236848172
iteration : 7147
train acc:  0.8203125
train loss:  0.3821963667869568
train gradient:  0.18913919085542286
iteration : 7148
train acc:  0.8671875
train loss:  0.3075554668903351
train gradient:  0.16640142990655954
iteration : 7149
train acc:  0.828125
train loss:  0.3770310580730438
train gradient:  0.2231567196759447
iteration : 7150
train acc:  0.8515625
train loss:  0.3468978703022003
train gradient:  0.19688971894470952
iteration : 7151
train acc:  0.8828125
train loss:  0.2990047037601471
train gradient:  0.18301400160768153
iteration : 7152
train acc:  0.8203125
train loss:  0.42711371183395386
train gradient:  0.2368963964549275
iteration : 7153
train acc:  0.8359375
train loss:  0.326032817363739
train gradient:  0.14335000910097287
iteration : 7154
train acc:  0.859375
train loss:  0.36621928215026855
train gradient:  0.1774289811578755
iteration : 7155
train acc:  0.796875
train loss:  0.3765113353729248
train gradient:  0.2229166808265109
iteration : 7156
train acc:  0.8671875
train loss:  0.2917449176311493
train gradient:  0.1949229053311365
iteration : 7157
train acc:  0.78125
train loss:  0.4065985083580017
train gradient:  0.2738484863320902
iteration : 7158
train acc:  0.84375
train loss:  0.30199122428894043
train gradient:  0.16987053716026626
iteration : 7159
train acc:  0.890625
train loss:  0.23242659866809845
train gradient:  0.120637179652765
iteration : 7160
train acc:  0.8359375
train loss:  0.3104124069213867
train gradient:  0.13344595984005025
iteration : 7161
train acc:  0.8203125
train loss:  0.35303640365600586
train gradient:  0.17363016980635018
iteration : 7162
train acc:  0.8671875
train loss:  0.338221937417984
train gradient:  0.19792694684802511
iteration : 7163
train acc:  0.890625
train loss:  0.29289698600769043
train gradient:  0.12432244139335592
iteration : 7164
train acc:  0.9140625
train loss:  0.20078670978546143
train gradient:  0.09808812064638056
iteration : 7165
train acc:  0.8515625
train loss:  0.3377370536327362
train gradient:  0.1916868353088844
iteration : 7166
train acc:  0.8515625
train loss:  0.32700151205062866
train gradient:  0.29138456240191835
iteration : 7167
train acc:  0.8828125
train loss:  0.2721487879753113
train gradient:  0.14993809888584417
iteration : 7168
train acc:  0.7890625
train loss:  0.3932586908340454
train gradient:  0.2801092611299778
iteration : 7169
train acc:  0.8515625
train loss:  0.37250110507011414
train gradient:  0.2588307914459798
iteration : 7170
train acc:  0.875
train loss:  0.32233214378356934
train gradient:  0.13695397164712403
iteration : 7171
train acc:  0.84375
train loss:  0.38193410634994507
train gradient:  0.36095980899937086
iteration : 7172
train acc:  0.8828125
train loss:  0.3373905420303345
train gradient:  0.17656335212732832
iteration : 7173
train acc:  0.859375
train loss:  0.3276306986808777
train gradient:  0.19518306935083046
iteration : 7174
train acc:  0.8515625
train loss:  0.36545196175575256
train gradient:  0.3477990986495171
iteration : 7175
train acc:  0.890625
train loss:  0.24404066801071167
train gradient:  0.12159126903419255
iteration : 7176
train acc:  0.84375
train loss:  0.34093761444091797
train gradient:  0.18901451931102706
iteration : 7177
train acc:  0.8203125
train loss:  0.39065489172935486
train gradient:  0.271183384421347
iteration : 7178
train acc:  0.8359375
train loss:  0.35781949758529663
train gradient:  0.2645095725324455
iteration : 7179
train acc:  0.859375
train loss:  0.3111456036567688
train gradient:  0.20689320408555703
iteration : 7180
train acc:  0.8203125
train loss:  0.36978909373283386
train gradient:  0.2788737465189004
iteration : 7181
train acc:  0.8359375
train loss:  0.3743528127670288
train gradient:  0.29038618340505257
iteration : 7182
train acc:  0.8125
train loss:  0.36922144889831543
train gradient:  0.1989139532547565
iteration : 7183
train acc:  0.8125
train loss:  0.404008150100708
train gradient:  0.2901698230783142
iteration : 7184
train acc:  0.7890625
train loss:  0.45208579301834106
train gradient:  0.4616744411338584
iteration : 7185
train acc:  0.84375
train loss:  0.3866064250469208
train gradient:  0.16859284474817923
iteration : 7186
train acc:  0.8828125
train loss:  0.2547394931316376
train gradient:  0.17827835847568418
iteration : 7187
train acc:  0.890625
train loss:  0.34390443563461304
train gradient:  0.24692340376385136
iteration : 7188
train acc:  0.90625
train loss:  0.2983601689338684
train gradient:  0.2126254969078768
iteration : 7189
train acc:  0.828125
train loss:  0.3883822560310364
train gradient:  0.2639651319692865
iteration : 7190
train acc:  0.8359375
train loss:  0.3634037375450134
train gradient:  0.24098290736909045
iteration : 7191
train acc:  0.875
train loss:  0.33402806520462036
train gradient:  0.1747520992431461
iteration : 7192
train acc:  0.859375
train loss:  0.323503315448761
train gradient:  0.1469403546581269
iteration : 7193
train acc:  0.875
train loss:  0.2939934730529785
train gradient:  0.18705834515623915
iteration : 7194
train acc:  0.875
train loss:  0.3074946403503418
train gradient:  0.1821573998525084
iteration : 7195
train acc:  0.8203125
train loss:  0.34939295053482056
train gradient:  0.21013792720652513
iteration : 7196
train acc:  0.84375
train loss:  0.3482728600502014
train gradient:  0.19806863413515846
iteration : 7197
train acc:  0.828125
train loss:  0.3489099144935608
train gradient:  0.1968512620224951
iteration : 7198
train acc:  0.890625
train loss:  0.33299148082733154
train gradient:  0.20730723250209623
iteration : 7199
train acc:  0.765625
train loss:  0.5171903371810913
train gradient:  0.4236911689030101
iteration : 7200
train acc:  0.8046875
train loss:  0.3802449107170105
train gradient:  0.26380049924197674
iteration : 7201
train acc:  0.8125
train loss:  0.42374154925346375
train gradient:  0.32950319664011757
iteration : 7202
train acc:  0.8359375
train loss:  0.287675142288208
train gradient:  0.1374858306599484
iteration : 7203
train acc:  0.8203125
train loss:  0.4432521164417267
train gradient:  0.32514410672942373
iteration : 7204
train acc:  0.8515625
train loss:  0.37463611364364624
train gradient:  0.1653239778276207
iteration : 7205
train acc:  0.8203125
train loss:  0.41672641038894653
train gradient:  0.23341363779535307
iteration : 7206
train acc:  0.84375
train loss:  0.3754955232143402
train gradient:  0.2920564503519221
iteration : 7207
train acc:  0.8671875
train loss:  0.34633129835128784
train gradient:  0.14537139395703985
iteration : 7208
train acc:  0.84375
train loss:  0.3007632791996002
train gradient:  0.1758053163734122
iteration : 7209
train acc:  0.84375
train loss:  0.3851126730442047
train gradient:  0.2095139701703624
iteration : 7210
train acc:  0.84375
train loss:  0.3620706796646118
train gradient:  0.3034214937106078
iteration : 7211
train acc:  0.8046875
train loss:  0.41419535875320435
train gradient:  0.2157613286561627
iteration : 7212
train acc:  0.828125
train loss:  0.40428921580314636
train gradient:  0.3008252175143665
iteration : 7213
train acc:  0.8359375
train loss:  0.4186105728149414
train gradient:  0.28722018473348415
iteration : 7214
train acc:  0.875
train loss:  0.2855703830718994
train gradient:  0.17253808737565424
iteration : 7215
train acc:  0.8359375
train loss:  0.35608142614364624
train gradient:  0.19240255106030568
iteration : 7216
train acc:  0.8515625
train loss:  0.37454479932785034
train gradient:  0.3006743487151981
iteration : 7217
train acc:  0.8828125
train loss:  0.3195355534553528
train gradient:  0.14318462643944818
iteration : 7218
train acc:  0.90625
train loss:  0.2725266218185425
train gradient:  0.09011207705576685
iteration : 7219
train acc:  0.828125
train loss:  0.3580249547958374
train gradient:  0.21560309707197198
iteration : 7220
train acc:  0.8359375
train loss:  0.35785651206970215
train gradient:  0.25259940029008865
iteration : 7221
train acc:  0.875
train loss:  0.3409067988395691
train gradient:  0.14734289873520368
iteration : 7222
train acc:  0.8828125
train loss:  0.26122841238975525
train gradient:  0.11594800271825566
iteration : 7223
train acc:  0.828125
train loss:  0.36211153864860535
train gradient:  0.1678634810689318
iteration : 7224
train acc:  0.828125
train loss:  0.38572144508361816
train gradient:  0.17436210578650507
iteration : 7225
train acc:  0.8984375
train loss:  0.2897779941558838
train gradient:  0.09500579590374944
iteration : 7226
train acc:  0.890625
train loss:  0.2794685959815979
train gradient:  0.15857734016151553
iteration : 7227
train acc:  0.7890625
train loss:  0.49115997552871704
train gradient:  0.3673916866411624
iteration : 7228
train acc:  0.8203125
train loss:  0.3670487403869629
train gradient:  0.1815161530675169
iteration : 7229
train acc:  0.8828125
train loss:  0.3399641513824463
train gradient:  0.1769076810711261
iteration : 7230
train acc:  0.796875
train loss:  0.3952828645706177
train gradient:  0.20053746044028298
iteration : 7231
train acc:  0.8359375
train loss:  0.3823302984237671
train gradient:  0.2087161895513458
iteration : 7232
train acc:  0.84375
train loss:  0.40967291593551636
train gradient:  0.22369216305294903
iteration : 7233
train acc:  0.8671875
train loss:  0.3438177704811096
train gradient:  0.18775169041168693
iteration : 7234
train acc:  0.828125
train loss:  0.38797527551651
train gradient:  0.2275124747681175
iteration : 7235
train acc:  0.8203125
train loss:  0.3925962448120117
train gradient:  0.22035837548142792
iteration : 7236
train acc:  0.8046875
train loss:  0.37870824337005615
train gradient:  0.31345598381702955
iteration : 7237
train acc:  0.8515625
train loss:  0.3592146039009094
train gradient:  0.15439964114133226
iteration : 7238
train acc:  0.765625
train loss:  0.45702600479125977
train gradient:  0.315293202321014
iteration : 7239
train acc:  0.859375
train loss:  0.34731000661849976
train gradient:  0.1819740513345347
iteration : 7240
train acc:  0.8515625
train loss:  0.39158743619918823
train gradient:  0.18800785012660443
iteration : 7241
train acc:  0.8984375
train loss:  0.3256957232952118
train gradient:  0.160861975277529
iteration : 7242
train acc:  0.828125
train loss:  0.39359092712402344
train gradient:  0.23201165160680032
iteration : 7243
train acc:  0.828125
train loss:  0.3792341947555542
train gradient:  0.20075611255838724
iteration : 7244
train acc:  0.8359375
train loss:  0.37622207403182983
train gradient:  0.24090257077205368
iteration : 7245
train acc:  0.8203125
train loss:  0.3626226484775543
train gradient:  0.18860802663537515
iteration : 7246
train acc:  0.78125
train loss:  0.41742390394210815
train gradient:  0.244976525583502
iteration : 7247
train acc:  0.828125
train loss:  0.3894156217575073
train gradient:  0.22304385425096573
iteration : 7248
train acc:  0.921875
train loss:  0.23259887099266052
train gradient:  0.09076044292362566
iteration : 7249
train acc:  0.9140625
train loss:  0.23837348818778992
train gradient:  0.1172681021391326
iteration : 7250
train acc:  0.859375
train loss:  0.32377171516418457
train gradient:  0.1818742503380009
iteration : 7251
train acc:  0.8359375
train loss:  0.3847998380661011
train gradient:  0.23482760658426516
iteration : 7252
train acc:  0.8203125
train loss:  0.3535425662994385
train gradient:  0.16420006446911398
iteration : 7253
train acc:  0.8125
train loss:  0.36807793378829956
train gradient:  0.19697564243057852
iteration : 7254
train acc:  0.8125
train loss:  0.3787476420402527
train gradient:  0.21064958216051682
iteration : 7255
train acc:  0.828125
train loss:  0.34901750087738037
train gradient:  0.2574165073649053
iteration : 7256
train acc:  0.84375
train loss:  0.3503192663192749
train gradient:  0.19325080656041765
iteration : 7257
train acc:  0.84375
train loss:  0.3304043412208557
train gradient:  0.18575923069772854
iteration : 7258
train acc:  0.8515625
train loss:  0.37240520119667053
train gradient:  0.19766856103414776
iteration : 7259
train acc:  0.78125
train loss:  0.4283314347267151
train gradient:  0.25652707198429714
iteration : 7260
train acc:  0.828125
train loss:  0.3769505023956299
train gradient:  0.19389199414547342
iteration : 7261
train acc:  0.84375
train loss:  0.3969884216785431
train gradient:  0.21117701493805208
iteration : 7262
train acc:  0.8515625
train loss:  0.3303464651107788
train gradient:  0.1752372848275966
iteration : 7263
train acc:  0.859375
train loss:  0.3357708752155304
train gradient:  0.12696962136718085
iteration : 7264
train acc:  0.890625
train loss:  0.2727457880973816
train gradient:  0.14790635651750336
iteration : 7265
train acc:  0.890625
train loss:  0.3020077347755432
train gradient:  0.14589045603438672
iteration : 7266
train acc:  0.875
train loss:  0.30092477798461914
train gradient:  0.11887370994207595
iteration : 7267
train acc:  0.8125
train loss:  0.38338667154312134
train gradient:  0.25067170251794385
iteration : 7268
train acc:  0.828125
train loss:  0.36522653698921204
train gradient:  0.22972757146567704
iteration : 7269
train acc:  0.8515625
train loss:  0.32770299911499023
train gradient:  0.1955271654564769
iteration : 7270
train acc:  0.84375
train loss:  0.33367919921875
train gradient:  0.19374863658707392
iteration : 7271
train acc:  0.8984375
train loss:  0.24729301035404205
train gradient:  0.14583925306816065
iteration : 7272
train acc:  0.8515625
train loss:  0.35260719060897827
train gradient:  0.26073373849855325
iteration : 7273
train acc:  0.890625
train loss:  0.31902194023132324
train gradient:  0.2628165197725934
iteration : 7274
train acc:  0.890625
train loss:  0.299067884683609
train gradient:  0.14754662934662674
iteration : 7275
train acc:  0.8984375
train loss:  0.317213237285614
train gradient:  0.19769313868976596
iteration : 7276
train acc:  0.8671875
train loss:  0.3302341103553772
train gradient:  0.14782090872604325
iteration : 7277
train acc:  0.8125
train loss:  0.34792226552963257
train gradient:  0.18972391490396923
iteration : 7278
train acc:  0.8203125
train loss:  0.37100833654403687
train gradient:  0.20049094056706337
iteration : 7279
train acc:  0.8984375
train loss:  0.289669394493103
train gradient:  0.1946437310096248
iteration : 7280
train acc:  0.8828125
train loss:  0.3155805170536041
train gradient:  0.2766642830854968
iteration : 7281
train acc:  0.8359375
train loss:  0.3174366056919098
train gradient:  0.25306232678603263
iteration : 7282
train acc:  0.8515625
train loss:  0.36551761627197266
train gradient:  0.18552983535885526
iteration : 7283
train acc:  0.859375
train loss:  0.3159780502319336
train gradient:  0.20024284761656985
iteration : 7284
train acc:  0.8671875
train loss:  0.35717374086380005
train gradient:  0.21437950271541556
iteration : 7285
train acc:  0.8203125
train loss:  0.34778380393981934
train gradient:  0.2333758521822758
iteration : 7286
train acc:  0.828125
train loss:  0.3831501603126526
train gradient:  0.27920654653387794
iteration : 7287
train acc:  0.8671875
train loss:  0.34848839044570923
train gradient:  0.20269896333368945
iteration : 7288
train acc:  0.8984375
train loss:  0.2974579930305481
train gradient:  0.18025231634929278
iteration : 7289
train acc:  0.8984375
train loss:  0.30522477626800537
train gradient:  0.10790512793187924
iteration : 7290
train acc:  0.84375
train loss:  0.3440919816493988
train gradient:  0.24779500878449112
iteration : 7291
train acc:  0.8671875
train loss:  0.37102726101875305
train gradient:  0.22811084492748854
iteration : 7292
train acc:  0.8359375
train loss:  0.38669630885124207
train gradient:  0.2465935796450629
iteration : 7293
train acc:  0.8125
train loss:  0.3350374102592468
train gradient:  0.20014247346006953
iteration : 7294
train acc:  0.8984375
train loss:  0.28587621450424194
train gradient:  0.12781038645251186
iteration : 7295
train acc:  0.84375
train loss:  0.3822323679924011
train gradient:  0.26288772819450396
iteration : 7296
train acc:  0.90625
train loss:  0.3056723475456238
train gradient:  0.19969088213616035
iteration : 7297
train acc:  0.8203125
train loss:  0.3906368017196655
train gradient:  0.2212899879422761
iteration : 7298
train acc:  0.8671875
train loss:  0.30341005325317383
train gradient:  0.18309641916562353
iteration : 7299
train acc:  0.875
train loss:  0.3123587965965271
train gradient:  0.13046222673116664
iteration : 7300
train acc:  0.828125
train loss:  0.3941138982772827
train gradient:  0.2527846982360499
iteration : 7301
train acc:  0.859375
train loss:  0.312904953956604
train gradient:  0.23814718922023986
iteration : 7302
train acc:  0.890625
train loss:  0.3185199499130249
train gradient:  0.17639833925911608
iteration : 7303
train acc:  0.8359375
train loss:  0.3466772735118866
train gradient:  0.2032982890307145
iteration : 7304
train acc:  0.8203125
train loss:  0.3775146007537842
train gradient:  0.1591826915745897
iteration : 7305
train acc:  0.84375
train loss:  0.3638770580291748
train gradient:  0.19737813617102914
iteration : 7306
train acc:  0.8671875
train loss:  0.2965205907821655
train gradient:  0.15038874107166944
iteration : 7307
train acc:  0.8203125
train loss:  0.41762465238571167
train gradient:  0.2585588412196387
iteration : 7308
train acc:  0.8828125
train loss:  0.30508074164390564
train gradient:  0.16039947284651174
iteration : 7309
train acc:  0.875
train loss:  0.3564216196537018
train gradient:  0.17686920533921263
iteration : 7310
train acc:  0.8515625
train loss:  0.29830509424209595
train gradient:  0.17516348115353403
iteration : 7311
train acc:  0.875
train loss:  0.28825512528419495
train gradient:  0.15594188468749431
iteration : 7312
train acc:  0.8515625
train loss:  0.33812522888183594
train gradient:  0.1737506597371336
iteration : 7313
train acc:  0.8515625
train loss:  0.3276042342185974
train gradient:  0.3003195141582439
iteration : 7314
train acc:  0.8671875
train loss:  0.2996266484260559
train gradient:  0.1797656391405799
iteration : 7315
train acc:  0.828125
train loss:  0.36380860209465027
train gradient:  0.20665508204694627
iteration : 7316
train acc:  0.8671875
train loss:  0.2974027693271637
train gradient:  0.2036835195581966
iteration : 7317
train acc:  0.875
train loss:  0.3418172001838684
train gradient:  0.20856877109785169
iteration : 7318
train acc:  0.8359375
train loss:  0.37803077697753906
train gradient:  0.19344125102241583
iteration : 7319
train acc:  0.8515625
train loss:  0.3439326286315918
train gradient:  0.2291405116402418
iteration : 7320
train acc:  0.875
train loss:  0.31663745641708374
train gradient:  0.1735722041789806
iteration : 7321
train acc:  0.9296875
train loss:  0.2140713334083557
train gradient:  0.1827498003808582
iteration : 7322
train acc:  0.828125
train loss:  0.2986830770969391
train gradient:  0.15273066250271414
iteration : 7323
train acc:  0.8828125
train loss:  0.30759209394454956
train gradient:  0.2535395191288944
iteration : 7324
train acc:  0.90625
train loss:  0.2727104425430298
train gradient:  0.15935817308073968
iteration : 7325
train acc:  0.8203125
train loss:  0.3388140797615051
train gradient:  0.17029868784024582
iteration : 7326
train acc:  0.8828125
train loss:  0.293533593416214
train gradient:  0.14131851947312019
iteration : 7327
train acc:  0.859375
train loss:  0.3883904218673706
train gradient:  0.3280060975887679
iteration : 7328
train acc:  0.890625
train loss:  0.28079402446746826
train gradient:  0.1399750408205881
iteration : 7329
train acc:  0.84375
train loss:  0.33922529220581055
train gradient:  0.1887372235910909
iteration : 7330
train acc:  0.84375
train loss:  0.32670360803604126
train gradient:  0.23150102481488558
iteration : 7331
train acc:  0.84375
train loss:  0.33385005593299866
train gradient:  0.22923416869830537
iteration : 7332
train acc:  0.8515625
train loss:  0.3637787103652954
train gradient:  0.24487992352601762
iteration : 7333
train acc:  0.8359375
train loss:  0.32663992047309875
train gradient:  0.19093472864348981
iteration : 7334
train acc:  0.84375
train loss:  0.3902461528778076
train gradient:  0.29423509278008425
iteration : 7335
train acc:  0.84375
train loss:  0.3202134370803833
train gradient:  0.22966974837262438
iteration : 7336
train acc:  0.796875
train loss:  0.4389118254184723
train gradient:  0.38951678104053955
iteration : 7337
train acc:  0.8671875
train loss:  0.29035884141921997
train gradient:  0.182307828851933
iteration : 7338
train acc:  0.8359375
train loss:  0.3453214168548584
train gradient:  0.16382487351663827
iteration : 7339
train acc:  0.8515625
train loss:  0.302596777677536
train gradient:  0.15800322091874175
iteration : 7340
train acc:  0.8828125
train loss:  0.290962278842926
train gradient:  0.15200860677633868
iteration : 7341
train acc:  0.8984375
train loss:  0.2996637225151062
train gradient:  0.20203519355714572
iteration : 7342
train acc:  0.8515625
train loss:  0.35090500116348267
train gradient:  0.3156503261094276
iteration : 7343
train acc:  0.84375
train loss:  0.33321940898895264
train gradient:  0.21258765411029162
iteration : 7344
train acc:  0.8046875
train loss:  0.41160109639167786
train gradient:  0.2631840450074367
iteration : 7345
train acc:  0.8515625
train loss:  0.4143037796020508
train gradient:  0.26287903555630204
iteration : 7346
train acc:  0.84375
train loss:  0.3307777941226959
train gradient:  0.14594025456155824
iteration : 7347
train acc:  0.8515625
train loss:  0.3816787898540497
train gradient:  0.17781008385112138
iteration : 7348
train acc:  0.8046875
train loss:  0.42341071367263794
train gradient:  0.4777713224327475
iteration : 7349
train acc:  0.8671875
train loss:  0.2816235423088074
train gradient:  0.13920709770066184
iteration : 7350
train acc:  0.828125
train loss:  0.3731454610824585
train gradient:  0.22999715676932186
iteration : 7351
train acc:  0.8359375
train loss:  0.35679715871810913
train gradient:  0.21829740527904723
iteration : 7352
train acc:  0.7890625
train loss:  0.4173763394355774
train gradient:  0.30616201797224857
iteration : 7353
train acc:  0.8125
train loss:  0.4360862076282501
train gradient:  0.2663162856195021
iteration : 7354
train acc:  0.875
train loss:  0.33679044246673584
train gradient:  0.22789836877216813
iteration : 7355
train acc:  0.84375
train loss:  0.3179805278778076
train gradient:  0.2779541056186524
iteration : 7356
train acc:  0.828125
train loss:  0.33545637130737305
train gradient:  0.17451906953528928
iteration : 7357
train acc:  0.8671875
train loss:  0.3214312195777893
train gradient:  0.20130003699351695
iteration : 7358
train acc:  0.875
train loss:  0.2948111295700073
train gradient:  0.19798467141993242
iteration : 7359
train acc:  0.84375
train loss:  0.3685367703437805
train gradient:  0.26892988482297403
iteration : 7360
train acc:  0.796875
train loss:  0.44206467270851135
train gradient:  0.48787989229912365
iteration : 7361
train acc:  0.84375
train loss:  0.44166240096092224
train gradient:  0.3357708135101887
iteration : 7362
train acc:  0.84375
train loss:  0.3355792760848999
train gradient:  0.19524255879433094
iteration : 7363
train acc:  0.8515625
train loss:  0.3450503349304199
train gradient:  0.15590553563354184
iteration : 7364
train acc:  0.84375
train loss:  0.36586326360702515
train gradient:  0.14855424295085418
iteration : 7365
train acc:  0.828125
train loss:  0.30878978967666626
train gradient:  0.1299010353930436
iteration : 7366
train acc:  0.8125
train loss:  0.4055134057998657
train gradient:  0.616047669426985
iteration : 7367
train acc:  0.8203125
train loss:  0.46165719628334045
train gradient:  0.31738087638728374
iteration : 7368
train acc:  0.890625
train loss:  0.29045403003692627
train gradient:  0.12903499365100535
iteration : 7369
train acc:  0.828125
train loss:  0.335604727268219
train gradient:  0.14412039017638711
iteration : 7370
train acc:  0.828125
train loss:  0.3770332932472229
train gradient:  0.18643946310911835
iteration : 7371
train acc:  0.84375
train loss:  0.3771132826805115
train gradient:  0.1482104938687498
iteration : 7372
train acc:  0.859375
train loss:  0.29422903060913086
train gradient:  0.17370212941919122
iteration : 7373
train acc:  0.84375
train loss:  0.3958202004432678
train gradient:  0.3175977287670133
iteration : 7374
train acc:  0.8515625
train loss:  0.31502386927604675
train gradient:  0.28604633500025917
iteration : 7375
train acc:  0.8515625
train loss:  0.3282000422477722
train gradient:  0.12592867693522064
iteration : 7376
train acc:  0.8046875
train loss:  0.31962114572525024
train gradient:  0.15856501079342827
iteration : 7377
train acc:  0.8671875
train loss:  0.3564543128013611
train gradient:  0.19057162686973622
iteration : 7378
train acc:  0.859375
train loss:  0.34495195746421814
train gradient:  0.20442449327286072
iteration : 7379
train acc:  0.859375
train loss:  0.34234729409217834
train gradient:  0.20112561179657257
iteration : 7380
train acc:  0.90625
train loss:  0.26065173745155334
train gradient:  0.13600690659671097
iteration : 7381
train acc:  0.8671875
train loss:  0.31779664754867554
train gradient:  0.13027670892217239
iteration : 7382
train acc:  0.8203125
train loss:  0.3560705780982971
train gradient:  0.16099519953451835
iteration : 7383
train acc:  0.8203125
train loss:  0.38959309458732605
train gradient:  0.2308146187774458
iteration : 7384
train acc:  0.8515625
train loss:  0.4031013250350952
train gradient:  0.2283180903394771
iteration : 7385
train acc:  0.78125
train loss:  0.46550673246383667
train gradient:  0.38010165173919364
iteration : 7386
train acc:  0.890625
train loss:  0.2895018458366394
train gradient:  0.14155661652471374
iteration : 7387
train acc:  0.9140625
train loss:  0.23634067177772522
train gradient:  0.11342112207049547
iteration : 7388
train acc:  0.875
train loss:  0.3394799828529358
train gradient:  0.21545862329670537
iteration : 7389
train acc:  0.8125
train loss:  0.4109792113304138
train gradient:  0.24697223433211274
iteration : 7390
train acc:  0.8203125
train loss:  0.346787691116333
train gradient:  0.18657206684256217
iteration : 7391
train acc:  0.828125
train loss:  0.39217376708984375
train gradient:  0.2245652696872335
iteration : 7392
train acc:  0.828125
train loss:  0.33763042092323303
train gradient:  0.20235642160563694
iteration : 7393
train acc:  0.9140625
train loss:  0.2788715362548828
train gradient:  0.11181758726509014
iteration : 7394
train acc:  0.875
train loss:  0.30893784761428833
train gradient:  0.13631584996836593
iteration : 7395
train acc:  0.875
train loss:  0.3064173758029938
train gradient:  0.15913938568090974
iteration : 7396
train acc:  0.8671875
train loss:  0.3143223822116852
train gradient:  0.185455241247423
iteration : 7397
train acc:  0.8046875
train loss:  0.3708104193210602
train gradient:  0.26388520927003145
iteration : 7398
train acc:  0.8671875
train loss:  0.3121020793914795
train gradient:  0.18796236408475173
iteration : 7399
train acc:  0.921875
train loss:  0.27727818489074707
train gradient:  0.13812531664401065
iteration : 7400
train acc:  0.8671875
train loss:  0.3081161677837372
train gradient:  0.16470830108120071
iteration : 7401
train acc:  0.828125
train loss:  0.3210211992263794
train gradient:  0.14103490375386324
iteration : 7402
train acc:  0.828125
train loss:  0.34732550382614136
train gradient:  0.2820074305400268
iteration : 7403
train acc:  0.875
train loss:  0.2594704031944275
train gradient:  0.10538679477501947
iteration : 7404
train acc:  0.8125
train loss:  0.3598136305809021
train gradient:  0.23647926672971056
iteration : 7405
train acc:  0.8828125
train loss:  0.30873435735702515
train gradient:  0.20156769212533993
iteration : 7406
train acc:  0.8359375
train loss:  0.35298413038253784
train gradient:  0.18740972463571645
iteration : 7407
train acc:  0.8515625
train loss:  0.3281002342700958
train gradient:  0.19987636891899077
iteration : 7408
train acc:  0.828125
train loss:  0.37625953555107117
train gradient:  0.17418538295671865
iteration : 7409
train acc:  0.8671875
train loss:  0.31299611926078796
train gradient:  0.16099023609491736
iteration : 7410
train acc:  0.8828125
train loss:  0.2760939300060272
train gradient:  0.1183560236154411
iteration : 7411
train acc:  0.8125
train loss:  0.37436020374298096
train gradient:  0.1743184968412565
iteration : 7412
train acc:  0.8515625
train loss:  0.2920699119567871
train gradient:  0.16809884032424027
iteration : 7413
train acc:  0.8671875
train loss:  0.285943865776062
train gradient:  0.11911473656299053
iteration : 7414
train acc:  0.890625
train loss:  0.29793810844421387
train gradient:  0.15178592794184897
iteration : 7415
train acc:  0.890625
train loss:  0.3260294795036316
train gradient:  0.12486365459187618
iteration : 7416
train acc:  0.8125
train loss:  0.3741827607154846
train gradient:  0.23301308343224747
iteration : 7417
train acc:  0.8671875
train loss:  0.34887826442718506
train gradient:  0.18364881518006182
iteration : 7418
train acc:  0.8671875
train loss:  0.2992323040962219
train gradient:  0.20711031639795688
iteration : 7419
train acc:  0.8828125
train loss:  0.2435973733663559
train gradient:  0.11218840111469362
iteration : 7420
train acc:  0.859375
train loss:  0.3544643521308899
train gradient:  0.23644395202491358
iteration : 7421
train acc:  0.8125
train loss:  0.3896223306655884
train gradient:  0.21391833059219478
iteration : 7422
train acc:  0.8671875
train loss:  0.33841538429260254
train gradient:  0.1840311836285653
iteration : 7423
train acc:  0.84375
train loss:  0.3584257960319519
train gradient:  0.16307748212595946
iteration : 7424
train acc:  0.8671875
train loss:  0.3087354302406311
train gradient:  0.14597471834019882
iteration : 7425
train acc:  0.8984375
train loss:  0.2579430937767029
train gradient:  0.10685340869709047
iteration : 7426
train acc:  0.8125
train loss:  0.38393354415893555
train gradient:  0.3842706366827143
iteration : 7427
train acc:  0.8203125
train loss:  0.4125426113605499
train gradient:  0.4311651960170412
iteration : 7428
train acc:  0.8671875
train loss:  0.3154730200767517
train gradient:  0.15488561390042294
iteration : 7429
train acc:  0.8984375
train loss:  0.2550637125968933
train gradient:  0.1522048485273798
iteration : 7430
train acc:  0.96875
train loss:  0.20009848475456238
train gradient:  0.07734734105624395
iteration : 7431
train acc:  0.875
train loss:  0.4133071303367615
train gradient:  0.3280245182071039
iteration : 7432
train acc:  0.8515625
train loss:  0.3644280433654785
train gradient:  0.18035114653073891
iteration : 7433
train acc:  0.875
train loss:  0.2639521062374115
train gradient:  0.14450675875797234
iteration : 7434
train acc:  0.921875
train loss:  0.24161116778850555
train gradient:  0.14605478725550686
iteration : 7435
train acc:  0.8359375
train loss:  0.3628690838813782
train gradient:  0.19202728556050813
iteration : 7436
train acc:  0.890625
train loss:  0.30506235361099243
train gradient:  0.23346577340588
iteration : 7437
train acc:  0.8671875
train loss:  0.28306472301483154
train gradient:  0.15075194318935523
iteration : 7438
train acc:  0.8203125
train loss:  0.34294068813323975
train gradient:  0.19785521481555
iteration : 7439
train acc:  0.8671875
train loss:  0.31853583455085754
train gradient:  0.2886282267422046
iteration : 7440
train acc:  0.8203125
train loss:  0.35387060046195984
train gradient:  0.27850807170119923
iteration : 7441
train acc:  0.8984375
train loss:  0.27681297063827515
train gradient:  0.13711947763476318
iteration : 7442
train acc:  0.8359375
train loss:  0.3662320375442505
train gradient:  0.16853915588136356
iteration : 7443
train acc:  0.8203125
train loss:  0.3747018575668335
train gradient:  0.24857062855947637
iteration : 7444
train acc:  0.875
train loss:  0.2573060393333435
train gradient:  0.12229829122312613
iteration : 7445
train acc:  0.875
train loss:  0.3004983961582184
train gradient:  0.165226930636329
iteration : 7446
train acc:  0.8984375
train loss:  0.2532859742641449
train gradient:  0.124513474472809
iteration : 7447
train acc:  0.90625
train loss:  0.23919379711151123
train gradient:  0.12880861065668486
iteration : 7448
train acc:  0.8515625
train loss:  0.34621283411979675
train gradient:  0.28465224862397553
iteration : 7449
train acc:  0.859375
train loss:  0.2645430862903595
train gradient:  0.11774208418928409
iteration : 7450
train acc:  0.8671875
train loss:  0.3090348243713379
train gradient:  0.14077257069851545
iteration : 7451
train acc:  0.8671875
train loss:  0.33070051670074463
train gradient:  0.2029238036868014
iteration : 7452
train acc:  0.7890625
train loss:  0.40861159563064575
train gradient:  0.2721131569053036
iteration : 7453
train acc:  0.875
train loss:  0.36348772048950195
train gradient:  0.2557130834753628
iteration : 7454
train acc:  0.84375
train loss:  0.3566012382507324
train gradient:  0.28614163596347586
iteration : 7455
train acc:  0.8828125
train loss:  0.3231706917285919
train gradient:  0.17328319774199427
iteration : 7456
train acc:  0.8359375
train loss:  0.310863196849823
train gradient:  0.1564176949461566
iteration : 7457
train acc:  0.859375
train loss:  0.3798472583293915
train gradient:  0.33206058345474293
iteration : 7458
train acc:  0.8515625
train loss:  0.35430747270584106
train gradient:  0.35149193974297516
iteration : 7459
train acc:  0.828125
train loss:  0.3804512917995453
train gradient:  0.23508896547825622
iteration : 7460
train acc:  0.8359375
train loss:  0.3048439919948578
train gradient:  0.14212207254818296
iteration : 7461
train acc:  0.78125
train loss:  0.40587273240089417
train gradient:  0.23899357435666213
iteration : 7462
train acc:  0.84375
train loss:  0.3514200448989868
train gradient:  0.15754579335336671
iteration : 7463
train acc:  0.84375
train loss:  0.33332639932632446
train gradient:  0.23152859494705338
iteration : 7464
train acc:  0.8671875
train loss:  0.3678969144821167
train gradient:  0.31895926091418647
iteration : 7465
train acc:  0.9140625
train loss:  0.2880421280860901
train gradient:  0.16870482061176903
iteration : 7466
train acc:  0.8671875
train loss:  0.34615522623062134
train gradient:  0.18726673763490262
iteration : 7467
train acc:  0.875
train loss:  0.33884531259536743
train gradient:  0.15394879876066137
iteration : 7468
train acc:  0.8203125
train loss:  0.37586745619773865
train gradient:  0.26727177034151345
iteration : 7469
train acc:  0.890625
train loss:  0.25756895542144775
train gradient:  0.13846265384408535
iteration : 7470
train acc:  0.8828125
train loss:  0.2900685667991638
train gradient:  0.1443917877324205
iteration : 7471
train acc:  0.84375
train loss:  0.3012712001800537
train gradient:  0.14725019953343194
iteration : 7472
train acc:  0.8515625
train loss:  0.3414948582649231
train gradient:  0.15978800921302716
iteration : 7473
train acc:  0.828125
train loss:  0.3995218276977539
train gradient:  0.2683948105442475
iteration : 7474
train acc:  0.8359375
train loss:  0.32733702659606934
train gradient:  0.18035137365796955
iteration : 7475
train acc:  0.796875
train loss:  0.3864436745643616
train gradient:  0.2566753197404814
iteration : 7476
train acc:  0.890625
train loss:  0.28781449794769287
train gradient:  0.16356944471385887
iteration : 7477
train acc:  0.875
train loss:  0.2911704182624817
train gradient:  0.13652386209328227
iteration : 7478
train acc:  0.8203125
train loss:  0.37828120589256287
train gradient:  0.275355559745783
iteration : 7479
train acc:  0.8828125
train loss:  0.2813185751438141
train gradient:  0.14812464070699494
iteration : 7480
train acc:  0.8359375
train loss:  0.38621723651885986
train gradient:  0.20727625429792862
iteration : 7481
train acc:  0.796875
train loss:  0.4106862545013428
train gradient:  0.28789413203269637
iteration : 7482
train acc:  0.8515625
train loss:  0.38951557874679565
train gradient:  0.25097483377077384
iteration : 7483
train acc:  0.8515625
train loss:  0.3438757359981537
train gradient:  0.17078430965803343
iteration : 7484
train acc:  0.84375
train loss:  0.38333266973495483
train gradient:  0.2285465570838638
iteration : 7485
train acc:  0.8125
train loss:  0.36728978157043457
train gradient:  0.17148176664949455
iteration : 7486
train acc:  0.875
train loss:  0.29964926838874817
train gradient:  0.18829694073360514
iteration : 7487
train acc:  0.8515625
train loss:  0.28444987535476685
train gradient:  0.17472924982768356
iteration : 7488
train acc:  0.90625
train loss:  0.25590744614601135
train gradient:  0.10563891154016226
iteration : 7489
train acc:  0.8046875
train loss:  0.3870770037174225
train gradient:  0.18482799760625493
iteration : 7490
train acc:  0.8671875
train loss:  0.34558218717575073
train gradient:  0.2072202081856474
iteration : 7491
train acc:  0.8515625
train loss:  0.3638847768306732
train gradient:  0.2203884408465339
iteration : 7492
train acc:  0.8515625
train loss:  0.30092930793762207
train gradient:  0.1969764492941767
iteration : 7493
train acc:  0.84375
train loss:  0.35923266410827637
train gradient:  0.22123630893154958
iteration : 7494
train acc:  0.875
train loss:  0.3229997754096985
train gradient:  0.14241936269177663
iteration : 7495
train acc:  0.796875
train loss:  0.37392860651016235
train gradient:  0.20357106988428125
iteration : 7496
train acc:  0.9140625
train loss:  0.2882649898529053
train gradient:  0.1753407943675409
iteration : 7497
train acc:  0.84375
train loss:  0.35932400822639465
train gradient:  0.19938657061779502
iteration : 7498
train acc:  0.875
train loss:  0.31642428040504456
train gradient:  0.1775485543190773
iteration : 7499
train acc:  0.8515625
train loss:  0.33674150705337524
train gradient:  0.21739874076625135
iteration : 7500
train acc:  0.8515625
train loss:  0.31461048126220703
train gradient:  0.15692998014306614
iteration : 7501
train acc:  0.8671875
train loss:  0.3136228322982788
train gradient:  0.18608405444294665
iteration : 7502
train acc:  0.90625
train loss:  0.24646098911762238
train gradient:  0.12793324262762726
iteration : 7503
train acc:  0.8515625
train loss:  0.3053646683692932
train gradient:  0.18497383634264458
iteration : 7504
train acc:  0.8984375
train loss:  0.2994752824306488
train gradient:  0.14140825871117285
iteration : 7505
train acc:  0.8984375
train loss:  0.3003554940223694
train gradient:  0.24896967004892523
iteration : 7506
train acc:  0.8359375
train loss:  0.3645929992198944
train gradient:  0.16081096039867696
iteration : 7507
train acc:  0.859375
train loss:  0.3239578604698181
train gradient:  0.1805766695050285
iteration : 7508
train acc:  0.8359375
train loss:  0.3811666965484619
train gradient:  0.366067080034416
iteration : 7509
train acc:  0.8828125
train loss:  0.2626173496246338
train gradient:  0.23103127289130554
iteration : 7510
train acc:  0.8359375
train loss:  0.36072325706481934
train gradient:  0.25775135508813685
iteration : 7511
train acc:  0.875
train loss:  0.30287623405456543
train gradient:  0.16997465689132715
iteration : 7512
train acc:  0.875
train loss:  0.33376264572143555
train gradient:  0.19083619002822252
iteration : 7513
train acc:  0.8203125
train loss:  0.3512030243873596
train gradient:  0.23547365826498506
iteration : 7514
train acc:  0.875
train loss:  0.2986719012260437
train gradient:  0.1819218127826426
iteration : 7515
train acc:  0.8515625
train loss:  0.3498878479003906
train gradient:  0.1879717381367619
iteration : 7516
train acc:  0.8359375
train loss:  0.36083436012268066
train gradient:  0.20693588588451167
iteration : 7517
train acc:  0.8828125
train loss:  0.3017999529838562
train gradient:  0.10888454124154484
iteration : 7518
train acc:  0.890625
train loss:  0.31508171558380127
train gradient:  0.16510780849058956
iteration : 7519
train acc:  0.8515625
train loss:  0.3745030164718628
train gradient:  0.15202581903768114
iteration : 7520
train acc:  0.8515625
train loss:  0.31786853075027466
train gradient:  0.16016862042932678
iteration : 7521
train acc:  0.8984375
train loss:  0.2748514413833618
train gradient:  0.14060739203533545
iteration : 7522
train acc:  0.8359375
train loss:  0.35255303978919983
train gradient:  0.20208375823791766
iteration : 7523
train acc:  0.828125
train loss:  0.3270939588546753
train gradient:  0.23834063367290328
iteration : 7524
train acc:  0.84375
train loss:  0.335246205329895
train gradient:  0.19352933695496127
iteration : 7525
train acc:  0.828125
train loss:  0.30492761731147766
train gradient:  0.20464017008737595
iteration : 7526
train acc:  0.875
train loss:  0.3165774643421173
train gradient:  0.1570015022620757
iteration : 7527
train acc:  0.8359375
train loss:  0.4535198509693146
train gradient:  0.31592540328588065
iteration : 7528
train acc:  0.859375
train loss:  0.34209567308425903
train gradient:  0.19943365892154102
iteration : 7529
train acc:  0.8125
train loss:  0.3515404462814331
train gradient:  0.24516970100830768
iteration : 7530
train acc:  0.8515625
train loss:  0.2797047197818756
train gradient:  0.10536787350080327
iteration : 7531
train acc:  0.8359375
train loss:  0.35406726598739624
train gradient:  0.1851100484560791
iteration : 7532
train acc:  0.84375
train loss:  0.34229540824890137
train gradient:  0.1491078486714698
iteration : 7533
train acc:  0.8828125
train loss:  0.3073924779891968
train gradient:  0.17472948070163027
iteration : 7534
train acc:  0.8671875
train loss:  0.3409128785133362
train gradient:  0.17487453236239142
iteration : 7535
train acc:  0.796875
train loss:  0.42222124338150024
train gradient:  0.2233817082343351
iteration : 7536
train acc:  0.8984375
train loss:  0.2934191823005676
train gradient:  0.13901520087221103
iteration : 7537
train acc:  0.8359375
train loss:  0.3705044686794281
train gradient:  0.2406175537192755
iteration : 7538
train acc:  0.8515625
train loss:  0.30069077014923096
train gradient:  0.13847753789777914
iteration : 7539
train acc:  0.8046875
train loss:  0.36520880460739136
train gradient:  0.21714036965912337
iteration : 7540
train acc:  0.796875
train loss:  0.3936467468738556
train gradient:  0.24061852756338953
iteration : 7541
train acc:  0.8046875
train loss:  0.40885207056999207
train gradient:  0.34713879554290983
iteration : 7542
train acc:  0.875
train loss:  0.35791829228401184
train gradient:  0.21963152016762227
iteration : 7543
train acc:  0.8671875
train loss:  0.2819652855396271
train gradient:  0.118097738662684
iteration : 7544
train acc:  0.859375
train loss:  0.3180423974990845
train gradient:  0.16505189132066575
iteration : 7545
train acc:  0.875
train loss:  0.28225159645080566
train gradient:  0.1773913325554489
iteration : 7546
train acc:  0.8671875
train loss:  0.27961426973342896
train gradient:  0.12619808266642513
iteration : 7547
train acc:  0.8359375
train loss:  0.3696041703224182
train gradient:  0.22775454005218848
iteration : 7548
train acc:  0.8046875
train loss:  0.4141274094581604
train gradient:  0.20824523309273552
iteration : 7549
train acc:  0.84375
train loss:  0.3521616458892822
train gradient:  0.3080128443596638
iteration : 7550
train acc:  0.859375
train loss:  0.4167201817035675
train gradient:  0.23923917632915856
iteration : 7551
train acc:  0.828125
train loss:  0.346917062997818
train gradient:  0.2076906756288655
iteration : 7552
train acc:  0.8515625
train loss:  0.39810022711753845
train gradient:  0.21670818680487514
iteration : 7553
train acc:  0.765625
train loss:  0.4326670467853546
train gradient:  0.34005626129573724
iteration : 7554
train acc:  0.8515625
train loss:  0.309885710477829
train gradient:  0.18112779166810955
iteration : 7555
train acc:  0.890625
train loss:  0.3045915961265564
train gradient:  0.19694512067839237
iteration : 7556
train acc:  0.8203125
train loss:  0.438129186630249
train gradient:  0.27669253221684875
iteration : 7557
train acc:  0.8828125
train loss:  0.2919067144393921
train gradient:  0.12886613648826528
iteration : 7558
train acc:  0.84375
train loss:  0.33193641901016235
train gradient:  0.1547351655564772
iteration : 7559
train acc:  0.8515625
train loss:  0.32181084156036377
train gradient:  0.14704787456703608
iteration : 7560
train acc:  0.875
train loss:  0.3336329460144043
train gradient:  0.17152479430678708
iteration : 7561
train acc:  0.8671875
train loss:  0.35862064361572266
train gradient:  0.17267489186516352
iteration : 7562
train acc:  0.8515625
train loss:  0.36666250228881836
train gradient:  0.4003807504524499
iteration : 7563
train acc:  0.8671875
train loss:  0.2831285297870636
train gradient:  0.17446894362720056
iteration : 7564
train acc:  0.84375
train loss:  0.35067376494407654
train gradient:  0.1647177225177316
iteration : 7565
train acc:  0.8515625
train loss:  0.3153231143951416
train gradient:  0.22637297479646237
iteration : 7566
train acc:  0.828125
train loss:  0.44253700971603394
train gradient:  0.3414420973172728
iteration : 7567
train acc:  0.8671875
train loss:  0.3313133716583252
train gradient:  0.2203771996883238
iteration : 7568
train acc:  0.8046875
train loss:  0.4174429774284363
train gradient:  0.26137678647209794
iteration : 7569
train acc:  0.875
train loss:  0.31588754057884216
train gradient:  0.17930293245362736
iteration : 7570
train acc:  0.7890625
train loss:  0.4166077673435211
train gradient:  0.2763324956431624
iteration : 7571
train acc:  0.8125
train loss:  0.40456610918045044
train gradient:  0.21107866224488367
iteration : 7572
train acc:  0.8515625
train loss:  0.3166588246822357
train gradient:  0.259601314158268
iteration : 7573
train acc:  0.8515625
train loss:  0.2937161922454834
train gradient:  0.13534815607286613
iteration : 7574
train acc:  0.8203125
train loss:  0.3683474361896515
train gradient:  0.2331182018287336
iteration : 7575
train acc:  0.8359375
train loss:  0.3006516098976135
train gradient:  0.14948526200995244
iteration : 7576
train acc:  0.8046875
train loss:  0.3861185908317566
train gradient:  0.29413322105428336
iteration : 7577
train acc:  0.8671875
train loss:  0.30576983094215393
train gradient:  0.15767108086755327
iteration : 7578
train acc:  0.8828125
train loss:  0.2775857448577881
train gradient:  0.22741389726692532
iteration : 7579
train acc:  0.8828125
train loss:  0.2880803942680359
train gradient:  0.11899979893047093
iteration : 7580
train acc:  0.875
train loss:  0.382375568151474
train gradient:  0.23638815024991539
iteration : 7581
train acc:  0.84375
train loss:  0.38054782152175903
train gradient:  0.18919545132383542
iteration : 7582
train acc:  0.875
train loss:  0.32071709632873535
train gradient:  0.14802242485781597
iteration : 7583
train acc:  0.8515625
train loss:  0.3022347092628479
train gradient:  0.13676660320418715
iteration : 7584
train acc:  0.8984375
train loss:  0.3020859360694885
train gradient:  0.16976031401027036
iteration : 7585
train acc:  0.8046875
train loss:  0.40133947134017944
train gradient:  0.20217719804575157
iteration : 7586
train acc:  0.8359375
train loss:  0.42103615403175354
train gradient:  0.28318847041889156
iteration : 7587
train acc:  0.8359375
train loss:  0.3232806921005249
train gradient:  0.2650089856963056
iteration : 7588
train acc:  0.890625
train loss:  0.2730545401573181
train gradient:  0.18429517492848585
iteration : 7589
train acc:  0.8515625
train loss:  0.3657749891281128
train gradient:  0.23095564762323423
iteration : 7590
train acc:  0.8203125
train loss:  0.37244123220443726
train gradient:  0.30924020850928347
iteration : 7591
train acc:  0.8359375
train loss:  0.3752500116825104
train gradient:  0.25346452502209627
iteration : 7592
train acc:  0.890625
train loss:  0.295427531003952
train gradient:  0.1432352766888127
iteration : 7593
train acc:  0.875
train loss:  0.3294299840927124
train gradient:  0.2258843079554739
iteration : 7594
train acc:  0.8515625
train loss:  0.3183850646018982
train gradient:  0.17274745547244968
iteration : 7595
train acc:  0.84375
train loss:  0.3487347662448883
train gradient:  0.19299769346345472
iteration : 7596
train acc:  0.8203125
train loss:  0.36528611183166504
train gradient:  0.23054687165284313
iteration : 7597
train acc:  0.875
train loss:  0.26910102367401123
train gradient:  0.15275262400900558
iteration : 7598
train acc:  0.9140625
train loss:  0.2798386812210083
train gradient:  0.15982594260793737
iteration : 7599
train acc:  0.90625
train loss:  0.21683946251869202
train gradient:  0.13029810795989044
iteration : 7600
train acc:  0.8984375
train loss:  0.3066922724246979
train gradient:  0.16710495327150532
iteration : 7601
train acc:  0.796875
train loss:  0.40127038955688477
train gradient:  0.27465114951192415
iteration : 7602
train acc:  0.859375
train loss:  0.33971232175827026
train gradient:  0.15125384860140495
iteration : 7603
train acc:  0.8671875
train loss:  0.29388338327407837
train gradient:  0.10808134158115398
iteration : 7604
train acc:  0.875
train loss:  0.3389070928096771
train gradient:  0.2108725304069057
iteration : 7605
train acc:  0.8203125
train loss:  0.4110868573188782
train gradient:  0.24124002286702487
iteration : 7606
train acc:  0.8671875
train loss:  0.32485583424568176
train gradient:  0.25237805398774316
iteration : 7607
train acc:  0.890625
train loss:  0.3322374224662781
train gradient:  0.1462650314742141
iteration : 7608
train acc:  0.84375
train loss:  0.361888587474823
train gradient:  0.19645870536319557
iteration : 7609
train acc:  0.8046875
train loss:  0.38421928882598877
train gradient:  0.20786422795560866
iteration : 7610
train acc:  0.875
train loss:  0.2830619215965271
train gradient:  0.0869244393443831
iteration : 7611
train acc:  0.8828125
train loss:  0.3245258927345276
train gradient:  0.16471783208008445
iteration : 7612
train acc:  0.8671875
train loss:  0.3473885953426361
train gradient:  0.19482797962057555
iteration : 7613
train acc:  0.859375
train loss:  0.3444368839263916
train gradient:  0.1649198700084743
iteration : 7614
train acc:  0.8515625
train loss:  0.2898711562156677
train gradient:  0.11807585652803665
iteration : 7615
train acc:  0.875
train loss:  0.3688305616378784
train gradient:  0.2275573020754254
iteration : 7616
train acc:  0.8046875
train loss:  0.4394201636314392
train gradient:  0.36837274053824404
iteration : 7617
train acc:  0.8203125
train loss:  0.3530817925930023
train gradient:  0.22835201153400436
iteration : 7618
train acc:  0.890625
train loss:  0.27448374032974243
train gradient:  0.1109793168842351
iteration : 7619
train acc:  0.796875
train loss:  0.42881298065185547
train gradient:  0.24093247829283146
iteration : 7620
train acc:  0.8515625
train loss:  0.3539636731147766
train gradient:  0.18003687594938378
iteration : 7621
train acc:  0.828125
train loss:  0.3178589940071106
train gradient:  0.24906537862806963
iteration : 7622
train acc:  0.8515625
train loss:  0.3535202741622925
train gradient:  0.1910607719767994
iteration : 7623
train acc:  0.8671875
train loss:  0.31816768646240234
train gradient:  0.1932913270170104
iteration : 7624
train acc:  0.859375
train loss:  0.38866573572158813
train gradient:  0.19500287094932842
iteration : 7625
train acc:  0.8828125
train loss:  0.3126438856124878
train gradient:  0.2001782889801964
iteration : 7626
train acc:  0.8203125
train loss:  0.3434434235095978
train gradient:  0.19283441111587102
iteration : 7627
train acc:  0.8359375
train loss:  0.3321458697319031
train gradient:  0.18472875284406426
iteration : 7628
train acc:  0.9375
train loss:  0.23327511548995972
train gradient:  0.10593162707943811
iteration : 7629
train acc:  0.84375
train loss:  0.38384199142456055
train gradient:  0.22192469392616965
iteration : 7630
train acc:  0.8671875
train loss:  0.2691878080368042
train gradient:  0.14701840477997563
iteration : 7631
train acc:  0.8828125
train loss:  0.306527316570282
train gradient:  0.25647164108112747
iteration : 7632
train acc:  0.8125
train loss:  0.4320184290409088
train gradient:  0.20825507450784925
iteration : 7633
train acc:  0.7890625
train loss:  0.4160844683647156
train gradient:  0.2465422925268373
iteration : 7634
train acc:  0.875
train loss:  0.2973704934120178
train gradient:  0.14264397052330202
iteration : 7635
train acc:  0.8515625
train loss:  0.404194712638855
train gradient:  0.3134772644789164
iteration : 7636
train acc:  0.859375
train loss:  0.3365616798400879
train gradient:  0.1426112516242149
iteration : 7637
train acc:  0.8671875
train loss:  0.34447139501571655
train gradient:  0.19134894570717867
iteration : 7638
train acc:  0.8515625
train loss:  0.3340148329734802
train gradient:  0.13570925031632047
iteration : 7639
train acc:  0.796875
train loss:  0.49564698338508606
train gradient:  0.3519729888916589
iteration : 7640
train acc:  0.8359375
train loss:  0.35438650846481323
train gradient:  0.2050599048682275
iteration : 7641
train acc:  0.8671875
train loss:  0.3169209361076355
train gradient:  0.19129264099984195
iteration : 7642
train acc:  0.8828125
train loss:  0.30985093116760254
train gradient:  0.20284844404341068
iteration : 7643
train acc:  0.8984375
train loss:  0.35148099064826965
train gradient:  0.2695341763158594
iteration : 7644
train acc:  0.859375
train loss:  0.2943057417869568
train gradient:  0.22982075061154938
iteration : 7645
train acc:  0.828125
train loss:  0.4007454216480255
train gradient:  0.2825428911039061
iteration : 7646
train acc:  0.84375
train loss:  0.4136520028114319
train gradient:  0.21505404863017677
iteration : 7647
train acc:  0.8359375
train loss:  0.3357318639755249
train gradient:  0.18150190897182794
iteration : 7648
train acc:  0.8203125
train loss:  0.3456663489341736
train gradient:  0.1547993352871102
iteration : 7649
train acc:  0.8671875
train loss:  0.3129591941833496
train gradient:  0.1838937565361381
iteration : 7650
train acc:  0.859375
train loss:  0.29931172728538513
train gradient:  0.1313357218214653
iteration : 7651
train acc:  0.875
train loss:  0.3388897180557251
train gradient:  0.19161432086666202
iteration : 7652
train acc:  0.8671875
train loss:  0.301880419254303
train gradient:  0.17211344525583067
iteration : 7653
train acc:  0.8359375
train loss:  0.379677951335907
train gradient:  0.1977427435457857
iteration : 7654
train acc:  0.8515625
train loss:  0.3761463463306427
train gradient:  0.17461709135835968
iteration : 7655
train acc:  0.859375
train loss:  0.3303681015968323
train gradient:  0.12388926927105358
iteration : 7656
train acc:  0.8515625
train loss:  0.3522997200489044
train gradient:  0.28361180345784015
iteration : 7657
train acc:  0.859375
train loss:  0.33608222007751465
train gradient:  0.18298014891090875
iteration : 7658
train acc:  0.8828125
train loss:  0.356874942779541
train gradient:  0.15496582315172705
iteration : 7659
train acc:  0.859375
train loss:  0.3153682351112366
train gradient:  0.17670235421786312
iteration : 7660
train acc:  0.890625
train loss:  0.35643643140792847
train gradient:  0.22334872766654507
iteration : 7661
train acc:  0.9140625
train loss:  0.24722619354724884
train gradient:  0.08558594002379843
iteration : 7662
train acc:  0.828125
train loss:  0.3674631118774414
train gradient:  0.2131633438828952
iteration : 7663
train acc:  0.8515625
train loss:  0.3302035331726074
train gradient:  0.16405471071116806
iteration : 7664
train acc:  0.8125
train loss:  0.4253528118133545
train gradient:  0.20599564230991008
iteration : 7665
train acc:  0.90625
train loss:  0.2936628460884094
train gradient:  0.1682921835394845
iteration : 7666
train acc:  0.8984375
train loss:  0.2748063802719116
train gradient:  0.13453303852730825
iteration : 7667
train acc:  0.765625
train loss:  0.442804753780365
train gradient:  0.26333451485877074
iteration : 7668
train acc:  0.84375
train loss:  0.34359505772590637
train gradient:  0.17239331968593524
iteration : 7669
train acc:  0.8828125
train loss:  0.27839726209640503
train gradient:  0.1404504760811291
iteration : 7670
train acc:  0.8984375
train loss:  0.2973594069480896
train gradient:  0.19801075285275543
iteration : 7671
train acc:  0.796875
train loss:  0.4365413188934326
train gradient:  0.30314814178184113
iteration : 7672
train acc:  0.84375
train loss:  0.3269440531730652
train gradient:  0.12847369602098732
iteration : 7673
train acc:  0.921875
train loss:  0.2517976760864258
train gradient:  0.09730454740559209
iteration : 7674
train acc:  0.859375
train loss:  0.31236952543258667
train gradient:  0.17481652901663633
iteration : 7675
train acc:  0.8359375
train loss:  0.3495175838470459
train gradient:  0.13636525165476082
iteration : 7676
train acc:  0.796875
train loss:  0.3978990316390991
train gradient:  0.24359137583916735
iteration : 7677
train acc:  0.828125
train loss:  0.3987422287464142
train gradient:  0.2679366870264071
iteration : 7678
train acc:  0.8984375
train loss:  0.2839500308036804
train gradient:  0.12390299003790872
iteration : 7679
train acc:  0.859375
train loss:  0.27223914861679077
train gradient:  0.11203166806937549
iteration : 7680
train acc:  0.84375
train loss:  0.3588866591453552
train gradient:  0.2646702165652259
iteration : 7681
train acc:  0.828125
train loss:  0.33626896142959595
train gradient:  0.16564710887470635
iteration : 7682
train acc:  0.8359375
train loss:  0.3220202624797821
train gradient:  0.11429820471411412
iteration : 7683
train acc:  0.875
train loss:  0.34800341725349426
train gradient:  0.2245336290888882
iteration : 7684
train acc:  0.859375
train loss:  0.3043127655982971
train gradient:  0.2029729351147923
iteration : 7685
train acc:  0.8515625
train loss:  0.3857656717300415
train gradient:  0.26539773467770283
iteration : 7686
train acc:  0.859375
train loss:  0.31460487842559814
train gradient:  0.15368796231500487
iteration : 7687
train acc:  0.8671875
train loss:  0.3536011874675751
train gradient:  0.17495535868572992
iteration : 7688
train acc:  0.859375
train loss:  0.3062271177768707
train gradient:  0.17416416683789304
iteration : 7689
train acc:  0.8828125
train loss:  0.31398555636405945
train gradient:  0.11599398838854286
iteration : 7690
train acc:  0.8125
train loss:  0.40255171060562134
train gradient:  0.22846578961567462
iteration : 7691
train acc:  0.8125
train loss:  0.42683154344558716
train gradient:  0.25072486368414443
iteration : 7692
train acc:  0.8515625
train loss:  0.3632858991622925
train gradient:  0.17347800146821035
iteration : 7693
train acc:  0.8515625
train loss:  0.35146933794021606
train gradient:  0.22745001660011022
iteration : 7694
train acc:  0.84375
train loss:  0.2961574196815491
train gradient:  0.157164390557941
iteration : 7695
train acc:  0.8203125
train loss:  0.35897260904312134
train gradient:  0.1694573669054301
iteration : 7696
train acc:  0.8046875
train loss:  0.4154176712036133
train gradient:  0.27385045855092516
iteration : 7697
train acc:  0.8203125
train loss:  0.40200209617614746
train gradient:  0.2576614388207312
iteration : 7698
train acc:  0.875
train loss:  0.2889326214790344
train gradient:  0.18321840749494872
iteration : 7699
train acc:  0.859375
train loss:  0.3388248682022095
train gradient:  0.15070907083234214
iteration : 7700
train acc:  0.8125
train loss:  0.43821096420288086
train gradient:  0.32783446170698805
iteration : 7701
train acc:  0.859375
train loss:  0.2897287607192993
train gradient:  0.1617583856726487
iteration : 7702
train acc:  0.8515625
train loss:  0.3311495780944824
train gradient:  0.16155518717698694
iteration : 7703
train acc:  0.8515625
train loss:  0.3119151294231415
train gradient:  0.18115632336527016
iteration : 7704
train acc:  0.859375
train loss:  0.36393386125564575
train gradient:  0.24960050727400923
iteration : 7705
train acc:  0.8515625
train loss:  0.35194361209869385
train gradient:  0.1428715402968931
iteration : 7706
train acc:  0.859375
train loss:  0.30707284808158875
train gradient:  0.11901653123517714
iteration : 7707
train acc:  0.875
train loss:  0.2973923087120056
train gradient:  0.17943791212973495
iteration : 7708
train acc:  0.8515625
train loss:  0.3602251708507538
train gradient:  0.2093907208306976
iteration : 7709
train acc:  0.8828125
train loss:  0.32551077008247375
train gradient:  0.1631462374767565
iteration : 7710
train acc:  0.8515625
train loss:  0.300471693277359
train gradient:  0.1418609020131863
iteration : 7711
train acc:  0.8671875
train loss:  0.34262511134147644
train gradient:  0.14317799884682314
iteration : 7712
train acc:  0.875
train loss:  0.289763867855072
train gradient:  0.1772695492318896
iteration : 7713
train acc:  0.84375
train loss:  0.40252766013145447
train gradient:  0.1937170647525511
iteration : 7714
train acc:  0.875
train loss:  0.30737388134002686
train gradient:  0.10763500121540846
iteration : 7715
train acc:  0.859375
train loss:  0.2953551411628723
train gradient:  0.11317946174973317
iteration : 7716
train acc:  0.8828125
train loss:  0.2857350707054138
train gradient:  0.13336708281146184
iteration : 7717
train acc:  0.765625
train loss:  0.46502476930618286
train gradient:  0.5448283680432888
iteration : 7718
train acc:  0.875
train loss:  0.3388688862323761
train gradient:  0.1449724863695481
iteration : 7719
train acc:  0.84375
train loss:  0.3310181498527527
train gradient:  0.17208511017401548
iteration : 7720
train acc:  0.8828125
train loss:  0.3050853908061981
train gradient:  0.14526118356378181
iteration : 7721
train acc:  0.8671875
train loss:  0.27027565240859985
train gradient:  0.10922472450934921
iteration : 7722
train acc:  0.828125
train loss:  0.42141395807266235
train gradient:  0.26608228600305034
iteration : 7723
train acc:  0.859375
train loss:  0.3606559634208679
train gradient:  0.17334440710726148
iteration : 7724
train acc:  0.890625
train loss:  0.269103467464447
train gradient:  0.1289165821227554
iteration : 7725
train acc:  0.8828125
train loss:  0.2747802734375
train gradient:  0.151964990465599
iteration : 7726
train acc:  0.84375
train loss:  0.35061585903167725
train gradient:  0.1774121597580129
iteration : 7727
train acc:  0.8828125
train loss:  0.3586600422859192
train gradient:  0.2338442174516493
iteration : 7728
train acc:  0.8515625
train loss:  0.3247112035751343
train gradient:  0.17956008111168098
iteration : 7729
train acc:  0.8359375
train loss:  0.37587982416152954
train gradient:  0.1679574522180164
iteration : 7730
train acc:  0.890625
train loss:  0.2859041392803192
train gradient:  0.20150756249600255
iteration : 7731
train acc:  0.8671875
train loss:  0.35204994678497314
train gradient:  0.19236246599185677
iteration : 7732
train acc:  0.90625
train loss:  0.24837003648281097
train gradient:  0.10355630202428766
iteration : 7733
train acc:  0.8203125
train loss:  0.35302332043647766
train gradient:  0.15764322087510232
iteration : 7734
train acc:  0.8203125
train loss:  0.397429883480072
train gradient:  0.3355110255424939
iteration : 7735
train acc:  0.890625
train loss:  0.3173847198486328
train gradient:  0.16761165620725668
iteration : 7736
train acc:  0.8515625
train loss:  0.3935440182685852
train gradient:  0.23673820538184742
iteration : 7737
train acc:  0.890625
train loss:  0.28805607557296753
train gradient:  0.13210015193673338
iteration : 7738
train acc:  0.8515625
train loss:  0.30815044045448303
train gradient:  0.16642887928313305
iteration : 7739
train acc:  0.8359375
train loss:  0.38610291481018066
train gradient:  0.1871159049074415
iteration : 7740
train acc:  0.8671875
train loss:  0.2814689874649048
train gradient:  0.12250667421967
iteration : 7741
train acc:  0.828125
train loss:  0.3670080900192261
train gradient:  0.21804166995999527
iteration : 7742
train acc:  0.859375
train loss:  0.3186284303665161
train gradient:  0.14250761736248627
iteration : 7743
train acc:  0.828125
train loss:  0.40976107120513916
train gradient:  0.30159035580314686
iteration : 7744
train acc:  0.8828125
train loss:  0.27227121591567993
train gradient:  0.17455627481872688
iteration : 7745
train acc:  0.8671875
train loss:  0.32657942175865173
train gradient:  0.10910733721280413
iteration : 7746
train acc:  0.8359375
train loss:  0.30641889572143555
train gradient:  0.17309252629350375
iteration : 7747
train acc:  0.8125
train loss:  0.30800753831863403
train gradient:  0.1968939546415206
iteration : 7748
train acc:  0.859375
train loss:  0.36609891057014465
train gradient:  0.2448537419537012
iteration : 7749
train acc:  0.8359375
train loss:  0.3585924506187439
train gradient:  0.2943058720709977
iteration : 7750
train acc:  0.8828125
train loss:  0.31464025378227234
train gradient:  0.15892855043251072
iteration : 7751
train acc:  0.859375
train loss:  0.29364845156669617
train gradient:  0.15763491591736017
iteration : 7752
train acc:  0.8359375
train loss:  0.36346179246902466
train gradient:  0.21851584867437385
iteration : 7753
train acc:  0.8671875
train loss:  0.3041568994522095
train gradient:  0.21712424094345922
iteration : 7754
train acc:  0.875
train loss:  0.2916777729988098
train gradient:  0.11692318461410342
iteration : 7755
train acc:  0.875
train loss:  0.2854243218898773
train gradient:  0.10532990248162138
iteration : 7756
train acc:  0.8671875
train loss:  0.3166182339191437
train gradient:  0.19449259586138595
iteration : 7757
train acc:  0.875
train loss:  0.27397990226745605
train gradient:  0.16785622109867115
iteration : 7758
train acc:  0.8203125
train loss:  0.37955552339553833
train gradient:  0.2425619561876216
iteration : 7759
train acc:  0.84375
train loss:  0.3515259921550751
train gradient:  0.193548486713608
iteration : 7760
train acc:  0.890625
train loss:  0.27580076456069946
train gradient:  0.1470837082241181
iteration : 7761
train acc:  0.8515625
train loss:  0.303188681602478
train gradient:  0.2313317687863267
iteration : 7762
train acc:  0.84375
train loss:  0.2955992817878723
train gradient:  0.1513291349895392
iteration : 7763
train acc:  0.828125
train loss:  0.35505956411361694
train gradient:  0.18282472667994465
iteration : 7764
train acc:  0.8359375
train loss:  0.32253894209861755
train gradient:  0.1685357157436519
iteration : 7765
train acc:  0.828125
train loss:  0.3794063925743103
train gradient:  0.18785675868625618
iteration : 7766
train acc:  0.8828125
train loss:  0.22800858318805695
train gradient:  0.10721455354857659
iteration : 7767
train acc:  0.859375
train loss:  0.40907591581344604
train gradient:  0.19329580327331986
iteration : 7768
train acc:  0.875
train loss:  0.3479219377040863
train gradient:  0.258031139834877
iteration : 7769
train acc:  0.8515625
train loss:  0.3320807218551636
train gradient:  0.20885120671014729
iteration : 7770
train acc:  0.8046875
train loss:  0.3797515034675598
train gradient:  0.2087374075690162
iteration : 7771
train acc:  0.8828125
train loss:  0.2524948716163635
train gradient:  0.09946939668717364
iteration : 7772
train acc:  0.8515625
train loss:  0.3061167597770691
train gradient:  0.21192151024917838
iteration : 7773
train acc:  0.8671875
train loss:  0.3379545509815216
train gradient:  0.1911349978434443
iteration : 7774
train acc:  0.84375
train loss:  0.3546890318393707
train gradient:  0.4234935195842825
iteration : 7775
train acc:  0.8828125
train loss:  0.27182894945144653
train gradient:  0.1272164301945689
iteration : 7776
train acc:  0.875
train loss:  0.3303755223751068
train gradient:  0.1872268511851776
iteration : 7777
train acc:  0.828125
train loss:  0.35195326805114746
train gradient:  0.23494555910493037
iteration : 7778
train acc:  0.8359375
train loss:  0.31244778633117676
train gradient:  0.14856969408723805
iteration : 7779
train acc:  0.84375
train loss:  0.33067959547042847
train gradient:  0.19806951598835748
iteration : 7780
train acc:  0.7890625
train loss:  0.4418749213218689
train gradient:  0.2678159859935265
iteration : 7781
train acc:  0.8828125
train loss:  0.2725455164909363
train gradient:  0.14059462402567746
iteration : 7782
train acc:  0.8359375
train loss:  0.34899401664733887
train gradient:  0.19746420058747977
iteration : 7783
train acc:  0.84375
train loss:  0.3105866312980652
train gradient:  0.12739289402821008
iteration : 7784
train acc:  0.78125
train loss:  0.4490816593170166
train gradient:  0.37075085316944295
iteration : 7785
train acc:  0.828125
train loss:  0.3666892349720001
train gradient:  0.23556773342516218
iteration : 7786
train acc:  0.890625
train loss:  0.34248220920562744
train gradient:  0.16900731639481462
iteration : 7787
train acc:  0.828125
train loss:  0.33785155415534973
train gradient:  0.14800251266483094
iteration : 7788
train acc:  0.8359375
train loss:  0.3561890721321106
train gradient:  0.27786734923995804
iteration : 7789
train acc:  0.8984375
train loss:  0.2449859082698822
train gradient:  0.09691795173036807
iteration : 7790
train acc:  0.8125
train loss:  0.399003803730011
train gradient:  0.24607721009159755
iteration : 7791
train acc:  0.8828125
train loss:  0.27221035957336426
train gradient:  0.19466588725104045
iteration : 7792
train acc:  0.875
train loss:  0.3108806014060974
train gradient:  0.16469792830492225
iteration : 7793
train acc:  0.8125
train loss:  0.4104771018028259
train gradient:  0.29454920229681864
iteration : 7794
train acc:  0.8984375
train loss:  0.304050087928772
train gradient:  0.13342588370475184
iteration : 7795
train acc:  0.8828125
train loss:  0.3391203284263611
train gradient:  0.24355295989370118
iteration : 7796
train acc:  0.890625
train loss:  0.25807613134384155
train gradient:  0.09359173063739178
iteration : 7797
train acc:  0.8671875
train loss:  0.2950105667114258
train gradient:  0.1561996915461625
iteration : 7798
train acc:  0.859375
train loss:  0.32678475975990295
train gradient:  0.22095390610815577
iteration : 7799
train acc:  0.78125
train loss:  0.45291227102279663
train gradient:  0.3364581634787275
iteration : 7800
train acc:  0.8359375
train loss:  0.33379068970680237
train gradient:  0.2525491132320601
iteration : 7801
train acc:  0.8359375
train loss:  0.2939881980419159
train gradient:  0.17282255750995532
iteration : 7802
train acc:  0.8984375
train loss:  0.2442101240158081
train gradient:  0.1108533714183093
iteration : 7803
train acc:  0.875
train loss:  0.31177642941474915
train gradient:  0.1352371113676643
iteration : 7804
train acc:  0.8671875
train loss:  0.3335249423980713
train gradient:  0.13614272244511855
iteration : 7805
train acc:  0.8984375
train loss:  0.29490938782691956
train gradient:  0.18983762204871626
iteration : 7806
train acc:  0.875
train loss:  0.2890746295452118
train gradient:  0.1649744601552674
iteration : 7807
train acc:  0.8203125
train loss:  0.366494357585907
train gradient:  0.2417484978637079
iteration : 7808
train acc:  0.90625
train loss:  0.27265092730522156
train gradient:  0.18821039300525705
iteration : 7809
train acc:  0.875
train loss:  0.30492451786994934
train gradient:  0.22051390607889793
iteration : 7810
train acc:  0.828125
train loss:  0.33806371688842773
train gradient:  0.141841533557856
iteration : 7811
train acc:  0.8359375
train loss:  0.35427969694137573
train gradient:  0.2674133249754959
iteration : 7812
train acc:  0.90625
train loss:  0.2848811745643616
train gradient:  0.14791186009781726
iteration : 7813
train acc:  0.8671875
train loss:  0.3122915029525757
train gradient:  0.1770398886351507
iteration : 7814
train acc:  0.8828125
train loss:  0.33624452352523804
train gradient:  0.21711047572035203
iteration : 7815
train acc:  0.8828125
train loss:  0.26740604639053345
train gradient:  0.12063856441016028
iteration : 7816
train acc:  0.875
train loss:  0.34506991505622864
train gradient:  0.16136655379321208
iteration : 7817
train acc:  0.90625
train loss:  0.27086299657821655
train gradient:  0.15186545239636692
iteration : 7818
train acc:  0.8671875
train loss:  0.3253214955329895
train gradient:  0.21500700598097666
iteration : 7819
train acc:  0.828125
train loss:  0.3743508756160736
train gradient:  0.2231648810346589
iteration : 7820
train acc:  0.84375
train loss:  0.35672613978385925
train gradient:  0.23942871377325353
iteration : 7821
train acc:  0.8671875
train loss:  0.37256935238838196
train gradient:  0.20684717627553545
iteration : 7822
train acc:  0.828125
train loss:  0.3517111539840698
train gradient:  0.2597923711704638
iteration : 7823
train acc:  0.921875
train loss:  0.26888954639434814
train gradient:  0.2952160511129498
iteration : 7824
train acc:  0.890625
train loss:  0.2707884907722473
train gradient:  0.12534739741282078
iteration : 7825
train acc:  0.8515625
train loss:  0.37233293056488037
train gradient:  0.23922162260946622
iteration : 7826
train acc:  0.859375
train loss:  0.3194008469581604
train gradient:  0.17768350468623773
iteration : 7827
train acc:  0.859375
train loss:  0.34269750118255615
train gradient:  0.21043609746917027
iteration : 7828
train acc:  0.7578125
train loss:  0.4480368494987488
train gradient:  0.45270880806216024
iteration : 7829
train acc:  0.8671875
train loss:  0.300420880317688
train gradient:  0.11676176186775462
iteration : 7830
train acc:  0.828125
train loss:  0.32024067640304565
train gradient:  0.24976045752527123
iteration : 7831
train acc:  0.828125
train loss:  0.36855781078338623
train gradient:  0.1770667992263218
iteration : 7832
train acc:  0.875
train loss:  0.2753410339355469
train gradient:  0.1013806262730415
iteration : 7833
train acc:  0.8671875
train loss:  0.35534822940826416
train gradient:  0.19898560472249205
iteration : 7834
train acc:  0.8671875
train loss:  0.2822990119457245
train gradient:  0.09267001396916512
iteration : 7835
train acc:  0.875
train loss:  0.240210622549057
train gradient:  0.11601806763350879
iteration : 7836
train acc:  0.8203125
train loss:  0.3765929341316223
train gradient:  0.17890404190871062
iteration : 7837
train acc:  0.8515625
train loss:  0.3169949948787689
train gradient:  0.2450789962020726
iteration : 7838
train acc:  0.859375
train loss:  0.3065371513366699
train gradient:  0.20265027880442316
iteration : 7839
train acc:  0.875
train loss:  0.2859484553337097
train gradient:  0.18829493380604406
iteration : 7840
train acc:  0.8359375
train loss:  0.34879785776138306
train gradient:  0.21049379458996495
iteration : 7841
train acc:  0.8515625
train loss:  0.3135078549385071
train gradient:  0.15773917695711887
iteration : 7842
train acc:  0.8671875
train loss:  0.28894147276878357
train gradient:  0.1773522306425364
iteration : 7843
train acc:  0.8515625
train loss:  0.2921341061592102
train gradient:  0.11826565171619005
iteration : 7844
train acc:  0.859375
train loss:  0.36122947931289673
train gradient:  0.20110732146640548
iteration : 7845
train acc:  0.8671875
train loss:  0.30382513999938965
train gradient:  0.14620455180259456
iteration : 7846
train acc:  0.8515625
train loss:  0.29509520530700684
train gradient:  0.23401737322074012
iteration : 7847
train acc:  0.859375
train loss:  0.30009666085243225
train gradient:  0.1923935346503612
iteration : 7848
train acc:  0.8203125
train loss:  0.4006292223930359
train gradient:  0.2941404614085669
iteration : 7849
train acc:  0.8828125
train loss:  0.2840886116027832
train gradient:  0.2743945116470046
iteration : 7850
train acc:  0.859375
train loss:  0.3005349040031433
train gradient:  0.17648855945775327
iteration : 7851
train acc:  0.828125
train loss:  0.3406866788864136
train gradient:  0.19262101063107015
iteration : 7852
train acc:  0.875
train loss:  0.3818894028663635
train gradient:  0.2524127010300739
iteration : 7853
train acc:  0.7890625
train loss:  0.4539744555950165
train gradient:  0.25869063775769163
iteration : 7854
train acc:  0.8828125
train loss:  0.3112291395664215
train gradient:  0.2062692594892344
iteration : 7855
train acc:  0.8671875
train loss:  0.31174561381340027
train gradient:  0.1648934654253516
iteration : 7856
train acc:  0.890625
train loss:  0.2712201476097107
train gradient:  0.12493682838418645
iteration : 7857
train acc:  0.859375
train loss:  0.32472044229507446
train gradient:  0.1913984851088127
iteration : 7858
train acc:  0.859375
train loss:  0.2733866572380066
train gradient:  0.1562911546075919
iteration : 7859
train acc:  0.8984375
train loss:  0.27258315682411194
train gradient:  0.12400842623251684
iteration : 7860
train acc:  0.8125
train loss:  0.3166448771953583
train gradient:  0.23063367721250028
iteration : 7861
train acc:  0.859375
train loss:  0.32215237617492676
train gradient:  0.13863357653199018
iteration : 7862
train acc:  0.8515625
train loss:  0.34004658460617065
train gradient:  0.1727926433665561
iteration : 7863
train acc:  0.8828125
train loss:  0.31338608264923096
train gradient:  0.1968720921339459
iteration : 7864
train acc:  0.8125
train loss:  0.4317965507507324
train gradient:  0.3088176198964348
iteration : 7865
train acc:  0.890625
train loss:  0.28956931829452515
train gradient:  0.16926478390427163
iteration : 7866
train acc:  0.8515625
train loss:  0.36381208896636963
train gradient:  0.23687134140857166
iteration : 7867
train acc:  0.875
train loss:  0.27774858474731445
train gradient:  0.16428767368286334
iteration : 7868
train acc:  0.8515625
train loss:  0.3228698670864105
train gradient:  0.17007628463699942
iteration : 7869
train acc:  0.9140625
train loss:  0.2999098300933838
train gradient:  0.17279787770179628
iteration : 7870
train acc:  0.8046875
train loss:  0.4225783944129944
train gradient:  0.31835029729193987
iteration : 7871
train acc:  0.875
train loss:  0.2680877447128296
train gradient:  0.13764794234350256
iteration : 7872
train acc:  0.8828125
train loss:  0.2975512146949768
train gradient:  0.16774702960903967
iteration : 7873
train acc:  0.8671875
train loss:  0.3546045422554016
train gradient:  0.25814685202832677
iteration : 7874
train acc:  0.84375
train loss:  0.3470839262008667
train gradient:  0.16699900050780425
iteration : 7875
train acc:  0.84375
train loss:  0.38374751806259155
train gradient:  0.25146332609327793
iteration : 7876
train acc:  0.8359375
train loss:  0.33888569474220276
train gradient:  0.16078109046815547
iteration : 7877
train acc:  0.859375
train loss:  0.31535279750823975
train gradient:  0.17240334082965178
iteration : 7878
train acc:  0.8671875
train loss:  0.28836849331855774
train gradient:  0.15747096622497858
iteration : 7879
train acc:  0.8828125
train loss:  0.340927392244339
train gradient:  0.19948310056898297
iteration : 7880
train acc:  0.8125
train loss:  0.4003450572490692
train gradient:  0.30804185287070884
iteration : 7881
train acc:  0.90625
train loss:  0.2868989109992981
train gradient:  0.18571241700200558
iteration : 7882
train acc:  0.8515625
train loss:  0.30173543095588684
train gradient:  0.13264230767804921
iteration : 7883
train acc:  0.8046875
train loss:  0.3523417115211487
train gradient:  0.22774489325590158
iteration : 7884
train acc:  0.796875
train loss:  0.3676125109195709
train gradient:  0.21163535996538402
iteration : 7885
train acc:  0.8125
train loss:  0.35000109672546387
train gradient:  0.17341143316637278
iteration : 7886
train acc:  0.9140625
train loss:  0.30077028274536133
train gradient:  0.2257547756313244
iteration : 7887
train acc:  0.8359375
train loss:  0.3844851553440094
train gradient:  0.3191755425239006
iteration : 7888
train acc:  0.8203125
train loss:  0.3420414924621582
train gradient:  0.23362116480250006
iteration : 7889
train acc:  0.8984375
train loss:  0.2785831391811371
train gradient:  0.18208101673731258
iteration : 7890
train acc:  0.8359375
train loss:  0.3611930012702942
train gradient:  0.18092064667952718
iteration : 7891
train acc:  0.8203125
train loss:  0.3266414999961853
train gradient:  0.17767504733270856
iteration : 7892
train acc:  0.84375
train loss:  0.362333208322525
train gradient:  0.18768075189368427
iteration : 7893
train acc:  0.90625
train loss:  0.26931139826774597
train gradient:  0.1508419435272145
iteration : 7894
train acc:  0.8671875
train loss:  0.32470178604125977
train gradient:  0.14222899359481422
iteration : 7895
train acc:  0.8203125
train loss:  0.31831979751586914
train gradient:  0.22640760218385753
iteration : 7896
train acc:  0.828125
train loss:  0.36213403940200806
train gradient:  0.18765022173161391
iteration : 7897
train acc:  0.90625
train loss:  0.3028067946434021
train gradient:  0.1635724451555609
iteration : 7898
train acc:  0.8984375
train loss:  0.23711058497428894
train gradient:  0.1548306492167319
iteration : 7899
train acc:  0.8359375
train loss:  0.34396642446517944
train gradient:  0.15493547844713254
iteration : 7900
train acc:  0.8828125
train loss:  0.3233339488506317
train gradient:  0.15444954784471174
iteration : 7901
train acc:  0.8515625
train loss:  0.32684555649757385
train gradient:  0.18432572893313523
iteration : 7902
train acc:  0.8828125
train loss:  0.31419146060943604
train gradient:  0.12233309441301694
iteration : 7903
train acc:  0.859375
train loss:  0.35982632637023926
train gradient:  0.3603434820703047
iteration : 7904
train acc:  0.828125
train loss:  0.33644771575927734
train gradient:  0.14810449154279115
iteration : 7905
train acc:  0.84375
train loss:  0.3317190408706665
train gradient:  0.17691365581385327
iteration : 7906
train acc:  0.84375
train loss:  0.35030311346054077
train gradient:  0.22562903202843285
iteration : 7907
train acc:  0.8984375
train loss:  0.2666567862033844
train gradient:  0.14208400233539042
iteration : 7908
train acc:  0.8125
train loss:  0.3962947130203247
train gradient:  0.19212510697864987
iteration : 7909
train acc:  0.8359375
train loss:  0.3325352966785431
train gradient:  0.17040033063562518
iteration : 7910
train acc:  0.8515625
train loss:  0.35680288076400757
train gradient:  0.18225870988199203
iteration : 7911
train acc:  0.8515625
train loss:  0.3493742346763611
train gradient:  0.2118156445220566
iteration : 7912
train acc:  0.8359375
train loss:  0.3727618455886841
train gradient:  0.21806043902472094
iteration : 7913
train acc:  0.859375
train loss:  0.3127080798149109
train gradient:  0.1478695033514061
iteration : 7914
train acc:  0.8203125
train loss:  0.3375489115715027
train gradient:  0.2162498177913623
iteration : 7915
train acc:  0.9140625
train loss:  0.32995322346687317
train gradient:  0.20053271382238547
iteration : 7916
train acc:  0.8671875
train loss:  0.32600530982017517
train gradient:  0.24026691515642457
iteration : 7917
train acc:  0.890625
train loss:  0.29468488693237305
train gradient:  0.13614016473898966
iteration : 7918
train acc:  0.859375
train loss:  0.27864259481430054
train gradient:  0.1461777214672882
iteration : 7919
train acc:  0.7578125
train loss:  0.40633395314216614
train gradient:  0.25920170841401996
iteration : 7920
train acc:  0.84375
train loss:  0.35058194398880005
train gradient:  0.17885426554564995
iteration : 7921
train acc:  0.8671875
train loss:  0.3085325360298157
train gradient:  0.16881048485802155
iteration : 7922
train acc:  0.9140625
train loss:  0.2609168291091919
train gradient:  0.13205840732410484
iteration : 7923
train acc:  0.828125
train loss:  0.3975185453891754
train gradient:  0.23083328600096234
iteration : 7924
train acc:  0.890625
train loss:  0.2821756601333618
train gradient:  0.1116663322983458
iteration : 7925
train acc:  0.8203125
train loss:  0.3778229355812073
train gradient:  0.2173356511588369
iteration : 7926
train acc:  0.8203125
train loss:  0.3794625997543335
train gradient:  0.1624095196759666
iteration : 7927
train acc:  0.84375
train loss:  0.31248247623443604
train gradient:  0.16002297311692754
iteration : 7928
train acc:  0.890625
train loss:  0.3000149726867676
train gradient:  0.2613798207757295
iteration : 7929
train acc:  0.859375
train loss:  0.3049704432487488
train gradient:  0.1467329313881929
iteration : 7930
train acc:  0.8984375
train loss:  0.25049757957458496
train gradient:  0.1651499844493796
iteration : 7931
train acc:  0.8515625
train loss:  0.3504561483860016
train gradient:  0.20453516646233272
iteration : 7932
train acc:  0.890625
train loss:  0.2702939808368683
train gradient:  0.1322951350109029
iteration : 7933
train acc:  0.875
train loss:  0.30321788787841797
train gradient:  0.2335245235563747
iteration : 7934
train acc:  0.8203125
train loss:  0.37622421979904175
train gradient:  0.20643305460571273
iteration : 7935
train acc:  0.8125
train loss:  0.38397902250289917
train gradient:  0.2528914711076622
iteration : 7936
train acc:  0.859375
train loss:  0.3391190469264984
train gradient:  0.18893631040392547
iteration : 7937
train acc:  0.890625
train loss:  0.27232712507247925
train gradient:  0.10522682265107614
iteration : 7938
train acc:  0.859375
train loss:  0.3346831798553467
train gradient:  0.20255120081808356
iteration : 7939
train acc:  0.8671875
train loss:  0.304276704788208
train gradient:  0.13855847501112328
iteration : 7940
train acc:  0.8515625
train loss:  0.3029940724372864
train gradient:  0.17582281035403907
iteration : 7941
train acc:  0.84375
train loss:  0.3025696873664856
train gradient:  0.13131742578761463
iteration : 7942
train acc:  0.859375
train loss:  0.2956356406211853
train gradient:  0.15547706559953828
iteration : 7943
train acc:  0.8984375
train loss:  0.2795977294445038
train gradient:  0.1300449872146705
iteration : 7944
train acc:  0.8515625
train loss:  0.2890436053276062
train gradient:  0.21983191814639255
iteration : 7945
train acc:  0.8359375
train loss:  0.3465541899204254
train gradient:  0.20153465848184623
iteration : 7946
train acc:  0.859375
train loss:  0.281602680683136
train gradient:  0.19913023836272592
iteration : 7947
train acc:  0.859375
train loss:  0.3224644958972931
train gradient:  0.1389899185119867
iteration : 7948
train acc:  0.8203125
train loss:  0.4369485676288605
train gradient:  0.31700653602281526
iteration : 7949
train acc:  0.8046875
train loss:  0.4377991557121277
train gradient:  0.35079061990806
iteration : 7950
train acc:  0.796875
train loss:  0.47309792041778564
train gradient:  0.3967407003349533
iteration : 7951
train acc:  0.875
train loss:  0.30664217472076416
train gradient:  0.20015385592322965
iteration : 7952
train acc:  0.8203125
train loss:  0.40760621428489685
train gradient:  0.26223983727803407
iteration : 7953
train acc:  0.875
train loss:  0.2556338906288147
train gradient:  0.1301250892349215
iteration : 7954
train acc:  0.8515625
train loss:  0.399048388004303
train gradient:  0.25847596785734656
iteration : 7955
train acc:  0.8515625
train loss:  0.31787312030792236
train gradient:  0.23899680970444784
iteration : 7956
train acc:  0.8671875
train loss:  0.3402109742164612
train gradient:  0.13360051951512575
iteration : 7957
train acc:  0.875
train loss:  0.29071152210235596
train gradient:  0.1874704589489513
iteration : 7958
train acc:  0.8671875
train loss:  0.34239763021469116
train gradient:  0.22467236133523713
iteration : 7959
train acc:  0.875
train loss:  0.27960920333862305
train gradient:  0.11935344386021247
iteration : 7960
train acc:  0.8671875
train loss:  0.2951119542121887
train gradient:  0.15765244799934075
iteration : 7961
train acc:  0.859375
train loss:  0.3462894558906555
train gradient:  0.15946528999739293
iteration : 7962
train acc:  0.8515625
train loss:  0.35254645347595215
train gradient:  0.18669076055625938
iteration : 7963
train acc:  0.8359375
train loss:  0.36236798763275146
train gradient:  0.23599881080152235
iteration : 7964
train acc:  0.890625
train loss:  0.2993031144142151
train gradient:  0.18039117637873903
iteration : 7965
train acc:  0.859375
train loss:  0.3405952751636505
train gradient:  0.1882012655637287
iteration : 7966
train acc:  0.8671875
train loss:  0.2828793525695801
train gradient:  0.12884226587636455
iteration : 7967
train acc:  0.859375
train loss:  0.27636459469795227
train gradient:  0.09939579514708899
iteration : 7968
train acc:  0.8359375
train loss:  0.393463671207428
train gradient:  0.3892112460536028
iteration : 7969
train acc:  0.8046875
train loss:  0.4120558798313141
train gradient:  0.26429311592982807
iteration : 7970
train acc:  0.8671875
train loss:  0.3323579728603363
train gradient:  0.1581549105922292
iteration : 7971
train acc:  0.8828125
train loss:  0.31555819511413574
train gradient:  0.1575381910150317
iteration : 7972
train acc:  0.828125
train loss:  0.3802107572555542
train gradient:  0.2782401435733459
iteration : 7973
train acc:  0.8515625
train loss:  0.36325469613075256
train gradient:  0.1772204347628518
iteration : 7974
train acc:  0.8828125
train loss:  0.3202010989189148
train gradient:  0.14186575667963627
iteration : 7975
train acc:  0.8125
train loss:  0.4619009792804718
train gradient:  0.27630578191071203
iteration : 7976
train acc:  0.8203125
train loss:  0.40047433972358704
train gradient:  0.32058491283927826
iteration : 7977
train acc:  0.78125
train loss:  0.42502081394195557
train gradient:  0.26032375002277935
iteration : 7978
train acc:  0.828125
train loss:  0.4033844470977783
train gradient:  0.18037269474595857
iteration : 7979
train acc:  0.8203125
train loss:  0.3776795268058777
train gradient:  0.2424880255362681
iteration : 7980
train acc:  0.875
train loss:  0.2577003836631775
train gradient:  0.13034631548745373
iteration : 7981
train acc:  0.8359375
train loss:  0.33975037932395935
train gradient:  0.1840376785967907
iteration : 7982
train acc:  0.8671875
train loss:  0.31653323769569397
train gradient:  0.1987877530565705
iteration : 7983
train acc:  0.796875
train loss:  0.34551262855529785
train gradient:  0.2945375629363717
iteration : 7984
train acc:  0.796875
train loss:  0.39505845308303833
train gradient:  0.19003164509084047
iteration : 7985
train acc:  0.875
train loss:  0.33375656604766846
train gradient:  0.18462488457383072
iteration : 7986
train acc:  0.9140625
train loss:  0.2754513621330261
train gradient:  0.13279966997418616
iteration : 7987
train acc:  0.828125
train loss:  0.34234359860420227
train gradient:  0.13727584655441902
iteration : 7988
train acc:  0.890625
train loss:  0.29257315397262573
train gradient:  0.13695921519087342
iteration : 7989
train acc:  0.8515625
train loss:  0.33215034008026123
train gradient:  0.19823018821146965
iteration : 7990
train acc:  0.90625
train loss:  0.2670285701751709
train gradient:  0.15850041919252353
iteration : 7991
train acc:  0.828125
train loss:  0.41965365409851074
train gradient:  0.2323962126555538
iteration : 7992
train acc:  0.828125
train loss:  0.3683779537677765
train gradient:  0.1896283571625408
iteration : 7993
train acc:  0.8671875
train loss:  0.3628285825252533
train gradient:  0.20348876405358476
iteration : 7994
train acc:  0.921875
train loss:  0.2464352548122406
train gradient:  0.12999807811948388
iteration : 7995
train acc:  0.859375
train loss:  0.31864094734191895
train gradient:  0.16090574976582026
iteration : 7996
train acc:  0.8671875
train loss:  0.2884202301502228
train gradient:  0.13817629160428707
iteration : 7997
train acc:  0.8671875
train loss:  0.2928682565689087
train gradient:  0.26563130473023655
iteration : 7998
train acc:  0.8203125
train loss:  0.3870455026626587
train gradient:  0.21139807204790892
iteration : 7999
train acc:  0.7734375
train loss:  0.500555694103241
train gradient:  0.3774013013322548
iteration : 8000
train acc:  0.8046875
train loss:  0.379634290933609
train gradient:  0.18942613666374974
iteration : 8001
train acc:  0.84375
train loss:  0.38648754358291626
train gradient:  0.19182715293006852
iteration : 8002
train acc:  0.9296875
train loss:  0.2656153738498688
train gradient:  0.17314934226900255
iteration : 8003
train acc:  0.8125
train loss:  0.3295930027961731
train gradient:  0.1249312638173053
iteration : 8004
train acc:  0.8125
train loss:  0.39863550662994385
train gradient:  0.20711630376505705
iteration : 8005
train acc:  0.84375
train loss:  0.32541191577911377
train gradient:  0.14291541630840318
iteration : 8006
train acc:  0.8203125
train loss:  0.3822582960128784
train gradient:  0.2691478689623451
iteration : 8007
train acc:  0.8515625
train loss:  0.3540060520172119
train gradient:  0.1563926619932589
iteration : 8008
train acc:  0.875
train loss:  0.3372126817703247
train gradient:  0.15660311381488967
iteration : 8009
train acc:  0.875
train loss:  0.30266574025154114
train gradient:  0.13514982115550256
iteration : 8010
train acc:  0.8359375
train loss:  0.36599016189575195
train gradient:  0.20801192919325584
iteration : 8011
train acc:  0.8828125
train loss:  0.2863251566886902
train gradient:  0.17396582717124068
iteration : 8012
train acc:  0.796875
train loss:  0.408265620470047
train gradient:  0.2058280393290301
iteration : 8013
train acc:  0.828125
train loss:  0.3843432068824768
train gradient:  0.22259719077328385
iteration : 8014
train acc:  0.8125
train loss:  0.42429256439208984
train gradient:  0.2595840766131873
iteration : 8015
train acc:  0.875
train loss:  0.29126301407814026
train gradient:  0.13713351443659716
iteration : 8016
train acc:  0.890625
train loss:  0.27554380893707275
train gradient:  0.15508335239811752
iteration : 8017
train acc:  0.8203125
train loss:  0.3500574827194214
train gradient:  0.24084217145545694
iteration : 8018
train acc:  0.828125
train loss:  0.418046772480011
train gradient:  0.22652410143658064
iteration : 8019
train acc:  0.890625
train loss:  0.28722190856933594
train gradient:  0.17356232218679213
iteration : 8020
train acc:  0.8359375
train loss:  0.40666645765304565
train gradient:  0.24994477313756255
iteration : 8021
train acc:  0.859375
train loss:  0.32378947734832764
train gradient:  0.15407580195024734
iteration : 8022
train acc:  0.84375
train loss:  0.31048330664634705
train gradient:  0.12458672875977309
iteration : 8023
train acc:  0.796875
train loss:  0.4479754567146301
train gradient:  0.2651155943127078
iteration : 8024
train acc:  0.875
train loss:  0.2639677822589874
train gradient:  0.14619382925490904
iteration : 8025
train acc:  0.890625
train loss:  0.25739115476608276
train gradient:  0.12610700212640968
iteration : 8026
train acc:  0.8515625
train loss:  0.33880484104156494
train gradient:  0.16547531000353227
iteration : 8027
train acc:  0.8671875
train loss:  0.35934388637542725
train gradient:  0.16564887355675226
iteration : 8028
train acc:  0.875
train loss:  0.3154647946357727
train gradient:  0.15670723505053663
iteration : 8029
train acc:  0.90625
train loss:  0.2946687936782837
train gradient:  0.16657868196205083
iteration : 8030
train acc:  0.8515625
train loss:  0.32734137773513794
train gradient:  0.2147866070169761
iteration : 8031
train acc:  0.8359375
train loss:  0.36973971128463745
train gradient:  0.28710042344020315
iteration : 8032
train acc:  0.8359375
train loss:  0.4045136570930481
train gradient:  0.24599679838088034
iteration : 8033
train acc:  0.8828125
train loss:  0.3662055730819702
train gradient:  0.23717582622863084
iteration : 8034
train acc:  0.8046875
train loss:  0.3452916741371155
train gradient:  0.2018406731133536
iteration : 8035
train acc:  0.8125
train loss:  0.4584015905857086
train gradient:  0.2837659474870369
iteration : 8036
train acc:  0.8671875
train loss:  0.35443469882011414
train gradient:  0.16405397751673456
iteration : 8037
train acc:  0.828125
train loss:  0.3909805417060852
train gradient:  0.20958885671342595
iteration : 8038
train acc:  0.90625
train loss:  0.2453741431236267
train gradient:  0.11146831685030718
iteration : 8039
train acc:  0.8515625
train loss:  0.3288143277168274
train gradient:  0.15723972493997626
iteration : 8040
train acc:  0.9296875
train loss:  0.28917717933654785
train gradient:  0.1783757174667114
iteration : 8041
train acc:  0.8359375
train loss:  0.31119316816329956
train gradient:  0.12136010878094171
iteration : 8042
train acc:  0.875
train loss:  0.37700891494750977
train gradient:  0.318953154622666
iteration : 8043
train acc:  0.8203125
train loss:  0.376848965883255
train gradient:  0.3704201255476869
iteration : 8044
train acc:  0.84375
train loss:  0.3195648491382599
train gradient:  0.14658712225392267
iteration : 8045
train acc:  0.8828125
train loss:  0.3297455906867981
train gradient:  0.138231537330342
iteration : 8046
train acc:  0.890625
train loss:  0.30317384004592896
train gradient:  0.19250863383158906
iteration : 8047
train acc:  0.8125
train loss:  0.3578857481479645
train gradient:  0.16405471656502418
iteration : 8048
train acc:  0.8515625
train loss:  0.3309860825538635
train gradient:  0.19912784070594394
iteration : 8049
train acc:  0.859375
train loss:  0.2911158800125122
train gradient:  0.09731895255392653
iteration : 8050
train acc:  0.8359375
train loss:  0.34680283069610596
train gradient:  0.22065499380176107
iteration : 8051
train acc:  0.8828125
train loss:  0.2976665198802948
train gradient:  0.13312786555377037
iteration : 8052
train acc:  0.8203125
train loss:  0.32524463534355164
train gradient:  0.169211571297624
iteration : 8053
train acc:  0.859375
train loss:  0.3484857976436615
train gradient:  0.2044048123075896
iteration : 8054
train acc:  0.84375
train loss:  0.32450050115585327
train gradient:  0.2023021824237204
iteration : 8055
train acc:  0.8828125
train loss:  0.23634476959705353
train gradient:  0.11207927107590966
iteration : 8056
train acc:  0.7890625
train loss:  0.3978874385356903
train gradient:  0.20800479651147233
iteration : 8057
train acc:  0.859375
train loss:  0.3742173910140991
train gradient:  0.2534137236505531
iteration : 8058
train acc:  0.8125
train loss:  0.3803845942020416
train gradient:  0.16615634077592828
iteration : 8059
train acc:  0.890625
train loss:  0.394268274307251
train gradient:  0.20907010378022106
iteration : 8060
train acc:  0.828125
train loss:  0.3610988259315491
train gradient:  0.18228484550685117
iteration : 8061
train acc:  0.84375
train loss:  0.3397985100746155
train gradient:  0.23473181683757197
iteration : 8062
train acc:  0.8671875
train loss:  0.3183172345161438
train gradient:  0.165084537524521
iteration : 8063
train acc:  0.828125
train loss:  0.3287678360939026
train gradient:  0.19015313894125174
iteration : 8064
train acc:  0.8515625
train loss:  0.3031449019908905
train gradient:  0.1521599513522967
iteration : 8065
train acc:  0.8828125
train loss:  0.25392934679985046
train gradient:  0.11324578691154635
iteration : 8066
train acc:  0.84375
train loss:  0.33628615736961365
train gradient:  0.21262484451689184
iteration : 8067
train acc:  0.8125
train loss:  0.33601823449134827
train gradient:  0.2015419661358884
iteration : 8068
train acc:  0.8515625
train loss:  0.305050790309906
train gradient:  0.14067689122483867
iteration : 8069
train acc:  0.7890625
train loss:  0.519450843334198
train gradient:  0.28186826011459154
iteration : 8070
train acc:  0.8671875
train loss:  0.29726937413215637
train gradient:  0.12313932548689455
iteration : 8071
train acc:  0.8359375
train loss:  0.3376735746860504
train gradient:  0.24239762135907109
iteration : 8072
train acc:  0.859375
train loss:  0.2944483458995819
train gradient:  0.16615246987715326
iteration : 8073
train acc:  0.8515625
train loss:  0.38061100244522095
train gradient:  0.15197057929422822
iteration : 8074
train acc:  0.8203125
train loss:  0.34973716735839844
train gradient:  0.17022115652155873
iteration : 8075
train acc:  0.8515625
train loss:  0.3562321960926056
train gradient:  0.17089097377680545
iteration : 8076
train acc:  0.875
train loss:  0.3095821738243103
train gradient:  0.12587543819721508
iteration : 8077
train acc:  0.8359375
train loss:  0.318112313747406
train gradient:  0.20090550615350383
iteration : 8078
train acc:  0.84375
train loss:  0.35244420170783997
train gradient:  0.15648278536913246
iteration : 8079
train acc:  0.8828125
train loss:  0.30313026905059814
train gradient:  0.1299255303634934
iteration : 8080
train acc:  0.8046875
train loss:  0.3790372312068939
train gradient:  0.24714200223106114
iteration : 8081
train acc:  0.8984375
train loss:  0.23219984769821167
train gradient:  0.09983590424927516
iteration : 8082
train acc:  0.8515625
train loss:  0.36527055501937866
train gradient:  0.227892799428891
iteration : 8083
train acc:  0.8828125
train loss:  0.29409587383270264
train gradient:  0.16897818915587468
iteration : 8084
train acc:  0.84375
train loss:  0.35472142696380615
train gradient:  0.18018711657511133
iteration : 8085
train acc:  0.8828125
train loss:  0.264046311378479
train gradient:  0.18208675250277254
iteration : 8086
train acc:  0.828125
train loss:  0.38981035351753235
train gradient:  0.2119135554503801
iteration : 8087
train acc:  0.8828125
train loss:  0.27134034037590027
train gradient:  0.11318314494657802
iteration : 8088
train acc:  0.84375
train loss:  0.35827919840812683
train gradient:  0.17940340607750588
iteration : 8089
train acc:  0.828125
train loss:  0.3593626618385315
train gradient:  0.16316887015150702
iteration : 8090
train acc:  0.8515625
train loss:  0.36044472455978394
train gradient:  0.21956835737545327
iteration : 8091
train acc:  0.8671875
train loss:  0.3064841628074646
train gradient:  0.23646415918956287
iteration : 8092
train acc:  0.84375
train loss:  0.376382052898407
train gradient:  0.21976192323388366
iteration : 8093
train acc:  0.84375
train loss:  0.3301728367805481
train gradient:  0.1703855098310461
iteration : 8094
train acc:  0.8984375
train loss:  0.31313979625701904
train gradient:  0.12493311352204715
iteration : 8095
train acc:  0.84375
train loss:  0.3833661377429962
train gradient:  0.16859836506700654
iteration : 8096
train acc:  0.78125
train loss:  0.41423746943473816
train gradient:  0.23440234471043042
iteration : 8097
train acc:  0.90625
train loss:  0.2649720311164856
train gradient:  0.11528718940012872
iteration : 8098
train acc:  0.8125
train loss:  0.35465800762176514
train gradient:  0.16074627315962126
iteration : 8099
train acc:  0.859375
train loss:  0.29029497504234314
train gradient:  0.1696693151679235
iteration : 8100
train acc:  0.796875
train loss:  0.33543214201927185
train gradient:  0.16844619665387242
iteration : 8101
train acc:  0.875
train loss:  0.27988365292549133
train gradient:  0.14360781574019804
iteration : 8102
train acc:  0.9296875
train loss:  0.2190076857805252
train gradient:  0.08542492885385689
iteration : 8103
train acc:  0.859375
train loss:  0.2901325225830078
train gradient:  0.13559217347974387
iteration : 8104
train acc:  0.859375
train loss:  0.3610779047012329
train gradient:  0.17926457613465208
iteration : 8105
train acc:  0.890625
train loss:  0.23829711973667145
train gradient:  0.12411369953670942
iteration : 8106
train acc:  0.8203125
train loss:  0.4123713970184326
train gradient:  0.24223817818286594
iteration : 8107
train acc:  0.875
train loss:  0.28931689262390137
train gradient:  0.202612870516498
iteration : 8108
train acc:  0.875
train loss:  0.261322021484375
train gradient:  0.18815034256036167
iteration : 8109
train acc:  0.859375
train loss:  0.3239133059978485
train gradient:  0.19507440364815742
iteration : 8110
train acc:  0.8359375
train loss:  0.3713145852088928
train gradient:  0.296157448220435
iteration : 8111
train acc:  0.8359375
train loss:  0.2851872742176056
train gradient:  0.1475522129609785
iteration : 8112
train acc:  0.84375
train loss:  0.3908655345439911
train gradient:  0.17303744653096587
iteration : 8113
train acc:  0.8828125
train loss:  0.26124274730682373
train gradient:  0.16224157102411643
iteration : 8114
train acc:  0.890625
train loss:  0.26595115661621094
train gradient:  0.10588196728419869
iteration : 8115
train acc:  0.875
train loss:  0.2798830568790436
train gradient:  0.18687537983754865
iteration : 8116
train acc:  0.875
train loss:  0.34127581119537354
train gradient:  0.17675124817452503
iteration : 8117
train acc:  0.8671875
train loss:  0.33078789710998535
train gradient:  0.22995084493846643
iteration : 8118
train acc:  0.8671875
train loss:  0.3269820213317871
train gradient:  0.1787427725817951
iteration : 8119
train acc:  0.8359375
train loss:  0.3493537902832031
train gradient:  0.26654493153983205
iteration : 8120
train acc:  0.8515625
train loss:  0.3181016445159912
train gradient:  0.1679850085716893
iteration : 8121
train acc:  0.859375
train loss:  0.2921287417411804
train gradient:  0.22791818039786982
iteration : 8122
train acc:  0.84375
train loss:  0.3462637960910797
train gradient:  0.2099682962445216
iteration : 8123
train acc:  0.875
train loss:  0.3380051851272583
train gradient:  0.15555685213234197
iteration : 8124
train acc:  0.8828125
train loss:  0.29529252648353577
train gradient:  0.16621554260818336
iteration : 8125
train acc:  0.8671875
train loss:  0.3511524796485901
train gradient:  0.2563038004967412
iteration : 8126
train acc:  0.8359375
train loss:  0.3305138349533081
train gradient:  0.2756086878060349
iteration : 8127
train acc:  0.8203125
train loss:  0.36279159784317017
train gradient:  0.21806363580566088
iteration : 8128
train acc:  0.8203125
train loss:  0.3276013433933258
train gradient:  0.18489397676730163
iteration : 8129
train acc:  0.8125
train loss:  0.43900829553604126
train gradient:  0.34554537481466446
iteration : 8130
train acc:  0.859375
train loss:  0.3774893283843994
train gradient:  0.20468707339309528
iteration : 8131
train acc:  0.84375
train loss:  0.34649205207824707
train gradient:  0.24535405724155507
iteration : 8132
train acc:  0.875
train loss:  0.298799067735672
train gradient:  0.14926884470782778
iteration : 8133
train acc:  0.8828125
train loss:  0.3050324618816376
train gradient:  0.140050955106796
iteration : 8134
train acc:  0.8828125
train loss:  0.3019750118255615
train gradient:  0.17323540372406476
iteration : 8135
train acc:  0.8046875
train loss:  0.40597277879714966
train gradient:  0.29405622970562945
iteration : 8136
train acc:  0.8046875
train loss:  0.4206031560897827
train gradient:  0.2769623894381792
iteration : 8137
train acc:  0.8671875
train loss:  0.3051784336566925
train gradient:  0.15430414297123912
iteration : 8138
train acc:  0.9140625
train loss:  0.3067317306995392
train gradient:  0.14452355136230763
iteration : 8139
train acc:  0.796875
train loss:  0.3845158815383911
train gradient:  0.21866544471776686
iteration : 8140
train acc:  0.859375
train loss:  0.31767064332962036
train gradient:  0.15550764268247455
iteration : 8141
train acc:  0.828125
train loss:  0.3574364185333252
train gradient:  0.20205873191315926
iteration : 8142
train acc:  0.8515625
train loss:  0.3145095705986023
train gradient:  0.16616485690989446
iteration : 8143
train acc:  0.8671875
train loss:  0.3037237823009491
train gradient:  0.16583434038562683
iteration : 8144
train acc:  0.8671875
train loss:  0.2965219020843506
train gradient:  0.1219026229677941
iteration : 8145
train acc:  0.8671875
train loss:  0.3499282896518707
train gradient:  0.19044458862224084
iteration : 8146
train acc:  0.8359375
train loss:  0.47072577476501465
train gradient:  0.3245722290752968
iteration : 8147
train acc:  0.8515625
train loss:  0.3255198299884796
train gradient:  0.23701444326865104
iteration : 8148
train acc:  0.90625
train loss:  0.24227634072303772
train gradient:  0.09732718573099347
iteration : 8149
train acc:  0.8203125
train loss:  0.35518938302993774
train gradient:  0.15867448897263176
iteration : 8150
train acc:  0.8671875
train loss:  0.2700819671154022
train gradient:  0.11158875941602256
iteration : 8151
train acc:  0.828125
train loss:  0.3959513008594513
train gradient:  0.22811437782373356
iteration : 8152
train acc:  0.8125
train loss:  0.34081241488456726
train gradient:  0.16700778304252215
iteration : 8153
train acc:  0.8515625
train loss:  0.3071376383304596
train gradient:  0.14849389088956955
iteration : 8154
train acc:  0.828125
train loss:  0.4252190887928009
train gradient:  0.32902349866361846
iteration : 8155
train acc:  0.8515625
train loss:  0.3211532235145569
train gradient:  0.20981169368412225
iteration : 8156
train acc:  0.8515625
train loss:  0.37729188799858093
train gradient:  0.21065333678904596
iteration : 8157
train acc:  0.875
train loss:  0.33875101804733276
train gradient:  0.18046599087581885
iteration : 8158
train acc:  0.8984375
train loss:  0.254310667514801
train gradient:  0.12897772891240003
iteration : 8159
train acc:  0.8515625
train loss:  0.3214764893054962
train gradient:  0.18805550913866131
iteration : 8160
train acc:  0.8125
train loss:  0.3342849612236023
train gradient:  0.17715668198631387
iteration : 8161
train acc:  0.8359375
train loss:  0.41017115116119385
train gradient:  0.42011574463472073
iteration : 8162
train acc:  0.875
train loss:  0.3091820180416107
train gradient:  0.20763676332672332
iteration : 8163
train acc:  0.8828125
train loss:  0.31697794795036316
train gradient:  0.19074882556201592
iteration : 8164
train acc:  0.859375
train loss:  0.30871766805648804
train gradient:  0.14919995449440815
iteration : 8165
train acc:  0.8203125
train loss:  0.42826029658317566
train gradient:  0.2965680286999431
iteration : 8166
train acc:  0.8203125
train loss:  0.33951056003570557
train gradient:  0.16734785351748954
iteration : 8167
train acc:  0.828125
train loss:  0.35628971457481384
train gradient:  0.18340529974461986
iteration : 8168
train acc:  0.890625
train loss:  0.28799986839294434
train gradient:  0.1536192337931056
iteration : 8169
train acc:  0.8359375
train loss:  0.4301764965057373
train gradient:  0.3248273573796978
iteration : 8170
train acc:  0.8125
train loss:  0.4068686366081238
train gradient:  0.2526070386340281
iteration : 8171
train acc:  0.875
train loss:  0.2715175449848175
train gradient:  0.12382917621637621
iteration : 8172
train acc:  0.890625
train loss:  0.26601606607437134
train gradient:  0.11346397314141395
iteration : 8173
train acc:  0.828125
train loss:  0.4577747881412506
train gradient:  0.2660151346815097
iteration : 8174
train acc:  0.9375
train loss:  0.18947291374206543
train gradient:  0.09084194619978124
iteration : 8175
train acc:  0.8515625
train loss:  0.43032941222190857
train gradient:  0.3302585695339461
iteration : 8176
train acc:  0.8515625
train loss:  0.3181774318218231
train gradient:  0.18104944736976897
iteration : 8177
train acc:  0.875
train loss:  0.33744093775749207
train gradient:  0.242636330805727
iteration : 8178
train acc:  0.859375
train loss:  0.2972400486469269
train gradient:  0.13718922791903823
iteration : 8179
train acc:  0.84375
train loss:  0.3230333924293518
train gradient:  0.17132719060451904
iteration : 8180
train acc:  0.890625
train loss:  0.3035823702812195
train gradient:  0.13321969791113772
iteration : 8181
train acc:  0.8046875
train loss:  0.38065940141677856
train gradient:  0.19580073955403232
iteration : 8182
train acc:  0.8515625
train loss:  0.35185694694519043
train gradient:  0.1992829820951129
iteration : 8183
train acc:  0.859375
train loss:  0.29938703775405884
train gradient:  0.15756691855577662
iteration : 8184
train acc:  0.828125
train loss:  0.3762356638908386
train gradient:  0.2509822725996738
iteration : 8185
train acc:  0.8203125
train loss:  0.3774219751358032
train gradient:  0.26162472818701815
iteration : 8186
train acc:  0.859375
train loss:  0.3280128240585327
train gradient:  0.19482058869896204
iteration : 8187
train acc:  0.890625
train loss:  0.3055713474750519
train gradient:  0.18799710064965355
iteration : 8188
train acc:  0.8828125
train loss:  0.29885077476501465
train gradient:  0.20575831536897293
iteration : 8189
train acc:  0.84375
train loss:  0.3459247946739197
train gradient:  0.20677859901134044
iteration : 8190
train acc:  0.796875
train loss:  0.4691769480705261
train gradient:  0.27602740197244235
iteration : 8191
train acc:  0.84375
train loss:  0.3583289384841919
train gradient:  0.19297282708353708
iteration : 8192
train acc:  0.8984375
train loss:  0.2756139636039734
train gradient:  0.11558069878233951
iteration : 8193
train acc:  0.8671875
train loss:  0.2960262894630432
train gradient:  0.1376705056729172
iteration : 8194
train acc:  0.8125
train loss:  0.36708804965019226
train gradient:  0.2169120670712846
iteration : 8195
train acc:  0.8359375
train loss:  0.3387838900089264
train gradient:  0.21967890896206319
iteration : 8196
train acc:  0.8515625
train loss:  0.3743238151073456
train gradient:  0.23276829910611044
iteration : 8197
train acc:  0.890625
train loss:  0.2516433596611023
train gradient:  0.11633244633146103
iteration : 8198
train acc:  0.8828125
train loss:  0.3194957375526428
train gradient:  0.13756043859472367
iteration : 8199
train acc:  0.84375
train loss:  0.2633324861526489
train gradient:  0.12712779905129393
iteration : 8200
train acc:  0.8125
train loss:  0.3797679543495178
train gradient:  0.21304265685219229
iteration : 8201
train acc:  0.890625
train loss:  0.2703421711921692
train gradient:  0.18310359778188207
iteration : 8202
train acc:  0.8671875
train loss:  0.3274472951889038
train gradient:  0.18675782808537644
iteration : 8203
train acc:  0.8046875
train loss:  0.4327649474143982
train gradient:  0.28727698872954877
iteration : 8204
train acc:  0.8515625
train loss:  0.29068195819854736
train gradient:  0.1379018904085606
iteration : 8205
train acc:  0.8359375
train loss:  0.35101228952407837
train gradient:  0.18169206615806519
iteration : 8206
train acc:  0.765625
train loss:  0.4011668264865875
train gradient:  0.19910110299829262
iteration : 8207
train acc:  0.8359375
train loss:  0.37217187881469727
train gradient:  0.2613166287124019
iteration : 8208
train acc:  0.84375
train loss:  0.3550102114677429
train gradient:  0.21372783573271842
iteration : 8209
train acc:  0.8828125
train loss:  0.30962198972702026
train gradient:  0.17477401593064112
iteration : 8210
train acc:  0.859375
train loss:  0.2983279228210449
train gradient:  0.17019210605651822
iteration : 8211
train acc:  0.8984375
train loss:  0.31798654794692993
train gradient:  0.1523549304750399
iteration : 8212
train acc:  0.828125
train loss:  0.31816259026527405
train gradient:  0.2205476970098707
iteration : 8213
train acc:  0.8515625
train loss:  0.3430825173854828
train gradient:  0.1697287692313013
iteration : 8214
train acc:  0.78125
train loss:  0.3957066833972931
train gradient:  0.24270670347266698
iteration : 8215
train acc:  0.84375
train loss:  0.31684690713882446
train gradient:  0.17324054425074067
iteration : 8216
train acc:  0.890625
train loss:  0.2740602493286133
train gradient:  0.10045644948397314
iteration : 8217
train acc:  0.859375
train loss:  0.3467121124267578
train gradient:  0.2116176845412995
iteration : 8218
train acc:  0.84375
train loss:  0.39007967710494995
train gradient:  0.23428999784285287
iteration : 8219
train acc:  0.8671875
train loss:  0.30285218358039856
train gradient:  0.13801162133051942
iteration : 8220
train acc:  0.84375
train loss:  0.3889220952987671
train gradient:  0.17638401988129343
iteration : 8221
train acc:  0.8515625
train loss:  0.33484432101249695
train gradient:  0.27914025936259174
iteration : 8222
train acc:  0.7890625
train loss:  0.4042811989784241
train gradient:  0.23539803425806322
iteration : 8223
train acc:  0.828125
train loss:  0.32510438561439514
train gradient:  0.15261275055554022
iteration : 8224
train acc:  0.859375
train loss:  0.29448726773262024
train gradient:  0.17520384035931055
iteration : 8225
train acc:  0.8203125
train loss:  0.3678855001926422
train gradient:  0.24314863153026883
iteration : 8226
train acc:  0.8359375
train loss:  0.37539637088775635
train gradient:  0.35899351024197557
iteration : 8227
train acc:  0.8671875
train loss:  0.3525974154472351
train gradient:  0.2310334163052501
iteration : 8228
train acc:  0.8359375
train loss:  0.33454906940460205
train gradient:  0.21967102592289905
iteration : 8229
train acc:  0.890625
train loss:  0.3050440847873688
train gradient:  0.14359971359846285
iteration : 8230
train acc:  0.8828125
train loss:  0.28164413571357727
train gradient:  0.12331558638448739
iteration : 8231
train acc:  0.875
train loss:  0.2744732201099396
train gradient:  0.15244682981906377
iteration : 8232
train acc:  0.8671875
train loss:  0.3128812313079834
train gradient:  0.21569057100395234
iteration : 8233
train acc:  0.8828125
train loss:  0.3025595545768738
train gradient:  0.12410013013004019
iteration : 8234
train acc:  0.9140625
train loss:  0.23089303076267242
train gradient:  0.13368087117763214
iteration : 8235
train acc:  0.8828125
train loss:  0.2334582358598709
train gradient:  0.1097861018206746
iteration : 8236
train acc:  0.8828125
train loss:  0.2883613705635071
train gradient:  0.1293879302932485
iteration : 8237
train acc:  0.8359375
train loss:  0.3847169876098633
train gradient:  0.24080658957449375
iteration : 8238
train acc:  0.8671875
train loss:  0.31863367557525635
train gradient:  0.15609340573616337
iteration : 8239
train acc:  0.859375
train loss:  0.3224923610687256
train gradient:  0.14341862168122077
iteration : 8240
train acc:  0.875
train loss:  0.2821120321750641
train gradient:  0.14226576625961063
iteration : 8241
train acc:  0.765625
train loss:  0.3723412752151489
train gradient:  0.28543737252385165
iteration : 8242
train acc:  0.84375
train loss:  0.3488518297672272
train gradient:  0.22003297287366785
iteration : 8243
train acc:  0.828125
train loss:  0.36831510066986084
train gradient:  0.22709388127757463
iteration : 8244
train acc:  0.859375
train loss:  0.2899808883666992
train gradient:  0.1285761605989485
iteration : 8245
train acc:  0.875
train loss:  0.33927565813064575
train gradient:  0.25047282123937753
iteration : 8246
train acc:  0.875
train loss:  0.30942302942276
train gradient:  0.16197413747070172
iteration : 8247
train acc:  0.828125
train loss:  0.3851015567779541
train gradient:  0.18912757069387837
iteration : 8248
train acc:  0.84375
train loss:  0.33895188570022583
train gradient:  0.17715884244407187
iteration : 8249
train acc:  0.8125
train loss:  0.36629658937454224
train gradient:  0.3115609803209374
iteration : 8250
train acc:  0.8671875
train loss:  0.29025140404701233
train gradient:  0.20058210883283908
iteration : 8251
train acc:  0.9296875
train loss:  0.21067775785923004
train gradient:  0.09771000646902683
iteration : 8252
train acc:  0.875
train loss:  0.31344062089920044
train gradient:  0.22706145789314064
iteration : 8253
train acc:  0.8984375
train loss:  0.2536010146141052
train gradient:  0.10493843170449237
iteration : 8254
train acc:  0.8828125
train loss:  0.2805304229259491
train gradient:  0.1819004889240773
iteration : 8255
train acc:  0.765625
train loss:  0.46069854497909546
train gradient:  0.29085315276131757
iteration : 8256
train acc:  0.7890625
train loss:  0.34525060653686523
train gradient:  0.3584631030175482
iteration : 8257
train acc:  0.8671875
train loss:  0.2754533290863037
train gradient:  0.11750471379793453
iteration : 8258
train acc:  0.859375
train loss:  0.2973130941390991
train gradient:  0.1984913826013649
iteration : 8259
train acc:  0.859375
train loss:  0.35601893067359924
train gradient:  0.2056116211707092
iteration : 8260
train acc:  0.8984375
train loss:  0.2746802270412445
train gradient:  0.11223871772101693
iteration : 8261
train acc:  0.8828125
train loss:  0.27623486518859863
train gradient:  0.1375248388467602
iteration : 8262
train acc:  0.84375
train loss:  0.39215338230133057
train gradient:  0.25477405924119223
iteration : 8263
train acc:  0.84375
train loss:  0.3492106795310974
train gradient:  0.1679031041020821
iteration : 8264
train acc:  0.796875
train loss:  0.47855639457702637
train gradient:  0.3761432952911807
iteration : 8265
train acc:  0.8515625
train loss:  0.3402593731880188
train gradient:  0.21050511974067215
iteration : 8266
train acc:  0.828125
train loss:  0.325827956199646
train gradient:  0.1390479092913567
iteration : 8267
train acc:  0.8515625
train loss:  0.33521994948387146
train gradient:  0.17796526182766947
iteration : 8268
train acc:  0.8125
train loss:  0.4549816846847534
train gradient:  0.4133479039376412
iteration : 8269
train acc:  0.90625
train loss:  0.26140785217285156
train gradient:  0.1259846295685192
iteration : 8270
train acc:  0.859375
train loss:  0.3041563630104065
train gradient:  0.15735479304621094
iteration : 8271
train acc:  0.84375
train loss:  0.3409191966056824
train gradient:  0.2309126549035051
iteration : 8272
train acc:  0.84375
train loss:  0.3399093747138977
train gradient:  0.22594407382171253
iteration : 8273
train acc:  0.8203125
train loss:  0.36284688115119934
train gradient:  0.15995328519602758
iteration : 8274
train acc:  0.8515625
train loss:  0.3206138610839844
train gradient:  0.16931225550118345
iteration : 8275
train acc:  0.8671875
train loss:  0.3070538341999054
train gradient:  0.1952067969970693
iteration : 8276
train acc:  0.8671875
train loss:  0.31597888469696045
train gradient:  0.13635226037553816
iteration : 8277
train acc:  0.8046875
train loss:  0.4121190309524536
train gradient:  0.20868422769573824
iteration : 8278
train acc:  0.84375
train loss:  0.34685230255126953
train gradient:  0.16649337080024607
iteration : 8279
train acc:  0.875
train loss:  0.3114550709724426
train gradient:  0.13594205854737462
iteration : 8280
train acc:  0.8203125
train loss:  0.3656740188598633
train gradient:  0.23382705405962628
iteration : 8281
train acc:  0.8828125
train loss:  0.2653956115245819
train gradient:  0.11697654528037223
iteration : 8282
train acc:  0.8828125
train loss:  0.35317736864089966
train gradient:  0.2261357132226243
iteration : 8283
train acc:  0.8203125
train loss:  0.3321678042411804
train gradient:  0.15119299789252086
iteration : 8284
train acc:  0.796875
train loss:  0.4713975787162781
train gradient:  0.4345837021246634
iteration : 8285
train acc:  0.7890625
train loss:  0.3891720771789551
train gradient:  0.2997283209508168
iteration : 8286
train acc:  0.8984375
train loss:  0.3020191192626953
train gradient:  0.1652228519499929
iteration : 8287
train acc:  0.828125
train loss:  0.3354532718658447
train gradient:  0.5202643470040887
iteration : 8288
train acc:  0.84375
train loss:  0.3211931586265564
train gradient:  0.18938663564956051
iteration : 8289
train acc:  0.8828125
train loss:  0.34937936067581177
train gradient:  0.17396331824761424
iteration : 8290
train acc:  0.8515625
train loss:  0.3534790575504303
train gradient:  0.1482396510774851
iteration : 8291
train acc:  0.875
train loss:  0.31542521715164185
train gradient:  0.18261166770263815
iteration : 8292
train acc:  0.8125
train loss:  0.42475882172584534
train gradient:  0.2555611567641304
iteration : 8293
train acc:  0.8671875
train loss:  0.3822610676288605
train gradient:  0.23719569525906628
iteration : 8294
train acc:  0.8671875
train loss:  0.29448020458221436
train gradient:  0.3002913517596128
iteration : 8295
train acc:  0.859375
train loss:  0.32062530517578125
train gradient:  0.12712210015695996
iteration : 8296
train acc:  0.8515625
train loss:  0.28766167163848877
train gradient:  0.17548675369029199
iteration : 8297
train acc:  0.8671875
train loss:  0.32822105288505554
train gradient:  0.1426215530586455
iteration : 8298
train acc:  0.8671875
train loss:  0.338783860206604
train gradient:  0.21971792173081706
iteration : 8299
train acc:  0.890625
train loss:  0.2641218304634094
train gradient:  0.19610250891348346
iteration : 8300
train acc:  0.8671875
train loss:  0.3308931887149811
train gradient:  0.1598647806772047
iteration : 8301
train acc:  0.8125
train loss:  0.4026424288749695
train gradient:  0.2581621014301511
iteration : 8302
train acc:  0.8671875
train loss:  0.2853422164916992
train gradient:  0.14629108335625995
iteration : 8303
train acc:  0.8125
train loss:  0.3848583698272705
train gradient:  0.2900299452652673
iteration : 8304
train acc:  0.875
train loss:  0.2897071838378906
train gradient:  0.1149210213254438
iteration : 8305
train acc:  0.84375
train loss:  0.33417844772338867
train gradient:  0.18215817451020888
iteration : 8306
train acc:  0.828125
train loss:  0.3807447552680969
train gradient:  0.16122354684961906
iteration : 8307
train acc:  0.875
train loss:  0.2850459814071655
train gradient:  0.13798083100954597
iteration : 8308
train acc:  0.859375
train loss:  0.31851887702941895
train gradient:  0.22836812189967126
iteration : 8309
train acc:  0.7890625
train loss:  0.49315738677978516
train gradient:  0.35612515276542145
iteration : 8310
train acc:  0.859375
train loss:  0.2777405381202698
train gradient:  0.15014423137934496
iteration : 8311
train acc:  0.84375
train loss:  0.38794365525245667
train gradient:  0.1947667215181802
iteration : 8312
train acc:  0.828125
train loss:  0.3524136543273926
train gradient:  0.20418388653084352
iteration : 8313
train acc:  0.8515625
train loss:  0.311112642288208
train gradient:  0.16858156728098608
iteration : 8314
train acc:  0.8125
train loss:  0.36619848012924194
train gradient:  0.26648628768944116
iteration : 8315
train acc:  0.8671875
train loss:  0.32876312732696533
train gradient:  0.13948887180223404
iteration : 8316
train acc:  0.8359375
train loss:  0.3886041045188904
train gradient:  0.20824139292051166
iteration : 8317
train acc:  0.8125
train loss:  0.37912100553512573
train gradient:  0.2123886526460232
iteration : 8318
train acc:  0.8359375
train loss:  0.4243159294128418
train gradient:  0.3306936342508387
iteration : 8319
train acc:  0.8515625
train loss:  0.34893763065338135
train gradient:  0.16630252525386124
iteration : 8320
train acc:  0.859375
train loss:  0.3359597623348236
train gradient:  0.14490353636382897
iteration : 8321
train acc:  0.8359375
train loss:  0.3629516363143921
train gradient:  0.23348463298661068
iteration : 8322
train acc:  0.8515625
train loss:  0.3380509614944458
train gradient:  0.14123875688196535
iteration : 8323
train acc:  0.84375
train loss:  0.3332419991493225
train gradient:  0.1986390542357917
iteration : 8324
train acc:  0.875
train loss:  0.26578688621520996
train gradient:  0.08902398806222452
iteration : 8325
train acc:  0.84375
train loss:  0.34404873847961426
train gradient:  0.15597326622124674
iteration : 8326
train acc:  0.8671875
train loss:  0.340142160654068
train gradient:  0.19035042621779208
iteration : 8327
train acc:  0.890625
train loss:  0.282484233379364
train gradient:  0.11070221960799774
iteration : 8328
train acc:  0.890625
train loss:  0.3111010789871216
train gradient:  0.18255582212268234
iteration : 8329
train acc:  0.8515625
train loss:  0.3442564606666565
train gradient:  0.2066999128301708
iteration : 8330
train acc:  0.875
train loss:  0.32368212938308716
train gradient:  0.19550453216323854
iteration : 8331
train acc:  0.8046875
train loss:  0.40186187624931335
train gradient:  0.29741252479822416
iteration : 8332
train acc:  0.859375
train loss:  0.33956974744796753
train gradient:  0.16330369286922086
iteration : 8333
train acc:  0.84375
train loss:  0.35622066259384155
train gradient:  0.18301011045159926
iteration : 8334
train acc:  0.8671875
train loss:  0.3305351436138153
train gradient:  0.14233708776095766
iteration : 8335
train acc:  0.875
train loss:  0.3028571307659149
train gradient:  0.1470389691419481
iteration : 8336
train acc:  0.828125
train loss:  0.3536403179168701
train gradient:  0.18744205025078342
iteration : 8337
train acc:  0.8515625
train loss:  0.28928959369659424
train gradient:  0.18011780323217613
iteration : 8338
train acc:  0.8671875
train loss:  0.2900705933570862
train gradient:  0.11592769870970286
iteration : 8339
train acc:  0.859375
train loss:  0.35883939266204834
train gradient:  0.2716118448006666
iteration : 8340
train acc:  0.828125
train loss:  0.3915563225746155
train gradient:  0.2087949505156483
iteration : 8341
train acc:  0.828125
train loss:  0.4134884774684906
train gradient:  0.285655243613475
iteration : 8342
train acc:  0.875
train loss:  0.2836255729198456
train gradient:  0.21241579194148213
iteration : 8343
train acc:  0.8203125
train loss:  0.3613157570362091
train gradient:  0.2555246835330448
iteration : 8344
train acc:  0.8984375
train loss:  0.27562081813812256
train gradient:  0.15087437597157327
iteration : 8345
train acc:  0.8671875
train loss:  0.32065659761428833
train gradient:  0.16685594304752943
iteration : 8346
train acc:  0.9296875
train loss:  0.23597452044487
train gradient:  0.09154693779563153
iteration : 8347
train acc:  0.828125
train loss:  0.4190884232521057
train gradient:  0.21838321854572174
iteration : 8348
train acc:  0.84375
train loss:  0.34005433320999146
train gradient:  0.16747672552745696
iteration : 8349
train acc:  0.875
train loss:  0.308000385761261
train gradient:  0.1494580495404117
iteration : 8350
train acc:  0.8828125
train loss:  0.32590436935424805
train gradient:  0.16204184817403552
iteration : 8351
train acc:  0.8359375
train loss:  0.40199315547943115
train gradient:  0.21406818488616727
iteration : 8352
train acc:  0.8046875
train loss:  0.40047168731689453
train gradient:  0.2089531490309155
iteration : 8353
train acc:  0.8984375
train loss:  0.2579675018787384
train gradient:  0.145497373683045
iteration : 8354
train acc:  0.8671875
train loss:  0.2974494695663452
train gradient:  0.15315749609525003
iteration : 8355
train acc:  0.8125
train loss:  0.37123990058898926
train gradient:  0.24892267815852787
iteration : 8356
train acc:  0.8515625
train loss:  0.3347557783126831
train gradient:  0.18864830363095952
iteration : 8357
train acc:  0.8046875
train loss:  0.3828064799308777
train gradient:  0.17851526666848763
iteration : 8358
train acc:  0.890625
train loss:  0.3176153302192688
train gradient:  0.15883881558375357
iteration : 8359
train acc:  0.859375
train loss:  0.30520129203796387
train gradient:  0.1711211282849527
iteration : 8360
train acc:  0.8671875
train loss:  0.3056224584579468
train gradient:  0.12753246917667443
iteration : 8361
train acc:  0.8828125
train loss:  0.2804923355579376
train gradient:  0.10672379582452492
iteration : 8362
train acc:  0.8828125
train loss:  0.3067209720611572
train gradient:  0.1714874258666721
iteration : 8363
train acc:  0.859375
train loss:  0.3454814851284027
train gradient:  0.15885847080892718
iteration : 8364
train acc:  0.8671875
train loss:  0.3395586609840393
train gradient:  0.24766756891164426
iteration : 8365
train acc:  0.8203125
train loss:  0.3498281240463257
train gradient:  0.22731378823875548
iteration : 8366
train acc:  0.8203125
train loss:  0.3683459162712097
train gradient:  0.17467555270073643
iteration : 8367
train acc:  0.8359375
train loss:  0.3725971579551697
train gradient:  0.17228591481308098
iteration : 8368
train acc:  0.8125
train loss:  0.3500822186470032
train gradient:  0.21200069819112893
iteration : 8369
train acc:  0.8515625
train loss:  0.3724936246871948
train gradient:  0.26158128519768276
iteration : 8370
train acc:  0.84375
train loss:  0.32916009426116943
train gradient:  0.19387271493148814
iteration : 8371
train acc:  0.8671875
train loss:  0.2998109757900238
train gradient:  0.12827192092809436
iteration : 8372
train acc:  0.8359375
train loss:  0.38620489835739136
train gradient:  0.22041289568812644
iteration : 8373
train acc:  0.84375
train loss:  0.37783825397491455
train gradient:  0.2775597290937201
iteration : 8374
train acc:  0.890625
train loss:  0.2887072265148163
train gradient:  0.15084195206335962
iteration : 8375
train acc:  0.8828125
train loss:  0.28714340925216675
train gradient:  0.14556683961343042
iteration : 8376
train acc:  0.84375
train loss:  0.31970012187957764
train gradient:  0.17155311772064274
iteration : 8377
train acc:  0.8984375
train loss:  0.2441135048866272
train gradient:  0.10354261038641312
iteration : 8378
train acc:  0.84375
train loss:  0.3888675570487976
train gradient:  0.2245355939186759
iteration : 8379
train acc:  0.8671875
train loss:  0.3623287081718445
train gradient:  0.16420988226605115
iteration : 8380
train acc:  0.90625
train loss:  0.26625901460647583
train gradient:  0.1317902429400632
iteration : 8381
train acc:  0.90625
train loss:  0.2614452838897705
train gradient:  0.14553495332528177
iteration : 8382
train acc:  0.8359375
train loss:  0.36041945219039917
train gradient:  0.21197384595675242
iteration : 8383
train acc:  0.8984375
train loss:  0.28649765253067017
train gradient:  0.12323437018812529
iteration : 8384
train acc:  0.8671875
train loss:  0.3063190281391144
train gradient:  0.13978622267651475
iteration : 8385
train acc:  0.8359375
train loss:  0.36797034740448
train gradient:  0.20100995426102802
iteration : 8386
train acc:  0.84375
train loss:  0.37798959016799927
train gradient:  0.19214307436298658
iteration : 8387
train acc:  0.84375
train loss:  0.3241279423236847
train gradient:  0.1844842935858093
iteration : 8388
train acc:  0.875
train loss:  0.30015212297439575
train gradient:  0.136768985493613
iteration : 8389
train acc:  0.8828125
train loss:  0.3192421793937683
train gradient:  0.14433189890471487
iteration : 8390
train acc:  0.8203125
train loss:  0.42410850524902344
train gradient:  0.44438130919715224
iteration : 8391
train acc:  0.8515625
train loss:  0.3048246204853058
train gradient:  0.14417565571940927
iteration : 8392
train acc:  0.8671875
train loss:  0.2962520718574524
train gradient:  0.17590570121896726
iteration : 8393
train acc:  0.8359375
train loss:  0.34919166564941406
train gradient:  0.19083992429931446
iteration : 8394
train acc:  0.84375
train loss:  0.3855980336666107
train gradient:  0.2342688718652316
iteration : 8395
train acc:  0.84375
train loss:  0.40903446078300476
train gradient:  0.17526684330751363
iteration : 8396
train acc:  0.828125
train loss:  0.37717434763908386
train gradient:  0.16367786561556674
iteration : 8397
train acc:  0.875
train loss:  0.30846983194351196
train gradient:  0.19716020102982731
iteration : 8398
train acc:  0.8671875
train loss:  0.27679911255836487
train gradient:  0.15267173451094862
iteration : 8399
train acc:  0.84375
train loss:  0.3314612805843353
train gradient:  0.21056432475196413
iteration : 8400
train acc:  0.8515625
train loss:  0.31153392791748047
train gradient:  0.17803528462789003
iteration : 8401
train acc:  0.90625
train loss:  0.2614225745201111
train gradient:  0.13708961283875387
iteration : 8402
train acc:  0.8515625
train loss:  0.34495800733566284
train gradient:  0.2133887392431488
iteration : 8403
train acc:  0.84375
train loss:  0.35116344690322876
train gradient:  0.23034783375980877
iteration : 8404
train acc:  0.8671875
train loss:  0.2874622642993927
train gradient:  0.19633018812521758
iteration : 8405
train acc:  0.8359375
train loss:  0.4302392899990082
train gradient:  0.29324139190682175
iteration : 8406
train acc:  0.765625
train loss:  0.4605630040168762
train gradient:  0.3022876599224823
iteration : 8407
train acc:  0.875
train loss:  0.3011159300804138
train gradient:  0.18520779832798573
iteration : 8408
train acc:  0.8359375
train loss:  0.33717554807662964
train gradient:  0.24863598297322564
iteration : 8409
train acc:  0.890625
train loss:  0.32456764578819275
train gradient:  0.16833013037222389
iteration : 8410
train acc:  0.890625
train loss:  0.2874806821346283
train gradient:  0.12729788400597689
iteration : 8411
train acc:  0.875
train loss:  0.3243626356124878
train gradient:  0.12129801386685941
iteration : 8412
train acc:  0.8828125
train loss:  0.3385283052921295
train gradient:  0.16016656679040664
iteration : 8413
train acc:  0.875
train loss:  0.28719061613082886
train gradient:  0.12199157286821724
iteration : 8414
train acc:  0.828125
train loss:  0.3693039119243622
train gradient:  0.1507256989799196
iteration : 8415
train acc:  0.890625
train loss:  0.2655642628669739
train gradient:  0.14063255480651327
iteration : 8416
train acc:  0.8671875
train loss:  0.3598501682281494
train gradient:  0.17870113326551726
iteration : 8417
train acc:  0.84375
train loss:  0.33031487464904785
train gradient:  0.18206402685872464
iteration : 8418
train acc:  0.84375
train loss:  0.3238510489463806
train gradient:  0.14010482419458403
iteration : 8419
train acc:  0.84375
train loss:  0.34213829040527344
train gradient:  0.14366767919247844
iteration : 8420
train acc:  0.7890625
train loss:  0.364470899105072
train gradient:  0.24603086174827882
iteration : 8421
train acc:  0.859375
train loss:  0.29472780227661133
train gradient:  0.13151258190328227
iteration : 8422
train acc:  0.875
train loss:  0.31403928995132446
train gradient:  0.12414068905421519
iteration : 8423
train acc:  0.890625
train loss:  0.26043370366096497
train gradient:  0.1250021011735555
iteration : 8424
train acc:  0.8515625
train loss:  0.32030412554740906
train gradient:  0.17918000170690362
iteration : 8425
train acc:  0.9140625
train loss:  0.30466172099113464
train gradient:  0.2155112652496199
iteration : 8426
train acc:  0.84375
train loss:  0.28767985105514526
train gradient:  0.12927611333358743
iteration : 8427
train acc:  0.8359375
train loss:  0.35666030645370483
train gradient:  0.1840116285029763
iteration : 8428
train acc:  0.8515625
train loss:  0.3052434027194977
train gradient:  0.1529141442017402
iteration : 8429
train acc:  0.78125
train loss:  0.4012574553489685
train gradient:  0.2247276414945511
iteration : 8430
train acc:  0.859375
train loss:  0.29151707887649536
train gradient:  0.2034428469109853
iteration : 8431
train acc:  0.9140625
train loss:  0.2615734934806824
train gradient:  0.1143576004016465
iteration : 8432
train acc:  0.8515625
train loss:  0.32370725274086
train gradient:  0.11272921737049749
iteration : 8433
train acc:  0.9140625
train loss:  0.2643967866897583
train gradient:  0.17704646577902228
iteration : 8434
train acc:  0.8515625
train loss:  0.33401787281036377
train gradient:  0.17131540307449789
iteration : 8435
train acc:  0.90625
train loss:  0.28078794479370117
train gradient:  0.1615042158639252
iteration : 8436
train acc:  0.8671875
train loss:  0.3398358225822449
train gradient:  0.18873328604063402
iteration : 8437
train acc:  0.84375
train loss:  0.3030936121940613
train gradient:  0.18148185075215456
iteration : 8438
train acc:  0.8203125
train loss:  0.4445967674255371
train gradient:  0.26613834838125655
iteration : 8439
train acc:  0.8828125
train loss:  0.2824642062187195
train gradient:  0.1358675799459707
iteration : 8440
train acc:  0.8671875
train loss:  0.2922002375125885
train gradient:  0.19705015370730772
iteration : 8441
train acc:  0.828125
train loss:  0.40106895565986633
train gradient:  0.21484562679879646
iteration : 8442
train acc:  0.84375
train loss:  0.31839776039123535
train gradient:  0.13273751364944156
iteration : 8443
train acc:  0.84375
train loss:  0.34900957345962524
train gradient:  0.21988113235649584
iteration : 8444
train acc:  0.8359375
train loss:  0.3337244689464569
train gradient:  0.19310010547476608
iteration : 8445
train acc:  0.859375
train loss:  0.295798122882843
train gradient:  0.10850324311496112
iteration : 8446
train acc:  0.875
train loss:  0.2673180103302002
train gradient:  0.13164032594169278
iteration : 8447
train acc:  0.8125
train loss:  0.39711490273475647
train gradient:  0.24046837246590558
iteration : 8448
train acc:  0.8828125
train loss:  0.29626762866973877
train gradient:  0.15572642714234916
iteration : 8449
train acc:  0.8203125
train loss:  0.37601375579833984
train gradient:  0.3946476921921517
iteration : 8450
train acc:  0.8984375
train loss:  0.26913371682167053
train gradient:  0.12168637921556771
iteration : 8451
train acc:  0.8515625
train loss:  0.3280501365661621
train gradient:  0.18466976160180032
iteration : 8452
train acc:  0.8671875
train loss:  0.32655251026153564
train gradient:  0.2789302567855905
iteration : 8453
train acc:  0.8203125
train loss:  0.39482754468917847
train gradient:  0.32579475401109326
iteration : 8454
train acc:  0.8671875
train loss:  0.30196818709373474
train gradient:  0.25360413077180466
iteration : 8455
train acc:  0.859375
train loss:  0.29948872327804565
train gradient:  0.24371386256399277
iteration : 8456
train acc:  0.8046875
train loss:  0.37801676988601685
train gradient:  0.22973959700928795
iteration : 8457
train acc:  0.8671875
train loss:  0.3040567636489868
train gradient:  0.1514034728256909
iteration : 8458
train acc:  0.8515625
train loss:  0.2981714606285095
train gradient:  0.13884474865875948
iteration : 8459
train acc:  0.7890625
train loss:  0.6036592721939087
train gradient:  0.44174838876348943
iteration : 8460
train acc:  0.8828125
train loss:  0.24530567228794098
train gradient:  0.11608296539447496
iteration : 8461
train acc:  0.8359375
train loss:  0.33333662152290344
train gradient:  0.18604809777663375
iteration : 8462
train acc:  0.890625
train loss:  0.31595221161842346
train gradient:  0.15164540920213734
iteration : 8463
train acc:  0.8828125
train loss:  0.24081969261169434
train gradient:  0.14614618145265795
iteration : 8464
train acc:  0.828125
train loss:  0.3837907910346985
train gradient:  0.20438959825227104
iteration : 8465
train acc:  0.8203125
train loss:  0.3565724492073059
train gradient:  0.1937328719823451
iteration : 8466
train acc:  0.8515625
train loss:  0.3626915216445923
train gradient:  0.24701748016871156
iteration : 8467
train acc:  0.859375
train loss:  0.32918310165405273
train gradient:  0.14089743686655506
iteration : 8468
train acc:  0.875
train loss:  0.33735519647598267
train gradient:  0.13946332109803608
iteration : 8469
train acc:  0.8671875
train loss:  0.3273013234138489
train gradient:  0.14393551735182814
iteration : 8470
train acc:  0.875
train loss:  0.3425251543521881
train gradient:  0.14561324377817494
iteration : 8471
train acc:  0.84375
train loss:  0.3470962345600128
train gradient:  0.1777953039472981
iteration : 8472
train acc:  0.78125
train loss:  0.45178088545799255
train gradient:  0.40271986950939603
iteration : 8473
train acc:  0.8828125
train loss:  0.33779487013816833
train gradient:  0.17094110290178763
iteration : 8474
train acc:  0.8203125
train loss:  0.3551398515701294
train gradient:  0.16592503540934456
iteration : 8475
train acc:  0.8515625
train loss:  0.37594783306121826
train gradient:  0.1792661093711222
iteration : 8476
train acc:  0.8828125
train loss:  0.31860557198524475
train gradient:  0.18978589089079426
iteration : 8477
train acc:  0.84375
train loss:  0.3525119423866272
train gradient:  0.22236279651184926
iteration : 8478
train acc:  0.828125
train loss:  0.36640605330467224
train gradient:  0.1943396925759142
iteration : 8479
train acc:  0.8359375
train loss:  0.3148719370365143
train gradient:  0.13077509827213343
iteration : 8480
train acc:  0.8515625
train loss:  0.3353902995586395
train gradient:  0.26938410925292683
iteration : 8481
train acc:  0.90625
train loss:  0.28828173875808716
train gradient:  0.12397480265855623
iteration : 8482
train acc:  0.859375
train loss:  0.354301393032074
train gradient:  0.17151165786329017
iteration : 8483
train acc:  0.84375
train loss:  0.39164406061172485
train gradient:  0.19779585975153816
iteration : 8484
train acc:  0.8125
train loss:  0.38970211148262024
train gradient:  0.17905467119673155
iteration : 8485
train acc:  0.8984375
train loss:  0.26048046350479126
train gradient:  0.12375721601465295
iteration : 8486
train acc:  0.828125
train loss:  0.358216792345047
train gradient:  0.19675274808898674
iteration : 8487
train acc:  0.8125
train loss:  0.33092114329338074
train gradient:  0.19740371186985112
iteration : 8488
train acc:  0.8515625
train loss:  0.3438551425933838
train gradient:  0.20328368979166928
iteration : 8489
train acc:  0.8359375
train loss:  0.3541405200958252
train gradient:  0.27817487784041034
iteration : 8490
train acc:  0.859375
train loss:  0.3571507930755615
train gradient:  0.21293302483528492
iteration : 8491
train acc:  0.921875
train loss:  0.26451271772384644
train gradient:  0.10468126435918829
iteration : 8492
train acc:  0.84375
train loss:  0.29480427503585815
train gradient:  0.18604681473395013
iteration : 8493
train acc:  0.8359375
train loss:  0.3626267910003662
train gradient:  0.16172680224064206
iteration : 8494
train acc:  0.8515625
train loss:  0.34456098079681396
train gradient:  0.18470416353233365
iteration : 8495
train acc:  0.859375
train loss:  0.2967607378959656
train gradient:  0.13365755451163192
iteration : 8496
train acc:  0.828125
train loss:  0.3383769392967224
train gradient:  0.23889363185761614
iteration : 8497
train acc:  0.8046875
train loss:  0.3228653371334076
train gradient:  0.17855631999726318
iteration : 8498
train acc:  0.8203125
train loss:  0.35880738496780396
train gradient:  0.17562332084574211
iteration : 8499
train acc:  0.8671875
train loss:  0.3133339285850525
train gradient:  0.11672838770213595
iteration : 8500
train acc:  0.8828125
train loss:  0.268648624420166
train gradient:  0.16186607283966836
iteration : 8501
train acc:  0.8671875
train loss:  0.3618800640106201
train gradient:  0.12517289687836894
iteration : 8502
train acc:  0.859375
train loss:  0.34045806527137756
train gradient:  0.18228534949401917
iteration : 8503
train acc:  0.8671875
train loss:  0.30878928303718567
train gradient:  0.2474564294849249
iteration : 8504
train acc:  0.8671875
train loss:  0.3641347587108612
train gradient:  0.22069789962041653
iteration : 8505
train acc:  0.8671875
train loss:  0.33020591735839844
train gradient:  0.21476496167229298
iteration : 8506
train acc:  0.8671875
train loss:  0.31701362133026123
train gradient:  0.1513109829179533
iteration : 8507
train acc:  0.7890625
train loss:  0.41647928953170776
train gradient:  0.27323358867567415
iteration : 8508
train acc:  0.8359375
train loss:  0.37129315733909607
train gradient:  0.1875265178046702
iteration : 8509
train acc:  0.875
train loss:  0.28238919377326965
train gradient:  0.1092876194347285
iteration : 8510
train acc:  0.8203125
train loss:  0.38950270414352417
train gradient:  0.3120889629347238
iteration : 8511
train acc:  0.828125
train loss:  0.3517506718635559
train gradient:  0.18095471375037137
iteration : 8512
train acc:  0.8515625
train loss:  0.3452678322792053
train gradient:  0.1569331045069099
iteration : 8513
train acc:  0.84375
train loss:  0.35406091809272766
train gradient:  0.1959737780453334
iteration : 8514
train acc:  0.8515625
train loss:  0.3563747704029083
train gradient:  0.17623092693606282
iteration : 8515
train acc:  0.84375
train loss:  0.3013153076171875
train gradient:  0.1255942195697552
iteration : 8516
train acc:  0.8671875
train loss:  0.2927163243293762
train gradient:  0.20502139682182252
iteration : 8517
train acc:  0.8359375
train loss:  0.41745465993881226
train gradient:  0.21787566349994153
iteration : 8518
train acc:  0.8203125
train loss:  0.41580039262771606
train gradient:  0.23930337754223258
iteration : 8519
train acc:  0.9140625
train loss:  0.2543209195137024
train gradient:  0.14087867840162308
iteration : 8520
train acc:  0.8671875
train loss:  0.2717558741569519
train gradient:  0.17392124353492072
iteration : 8521
train acc:  0.8125
train loss:  0.38783761858940125
train gradient:  0.2391288410529001
iteration : 8522
train acc:  0.8671875
train loss:  0.2976452708244324
train gradient:  0.15547891694887261
iteration : 8523
train acc:  0.8203125
train loss:  0.3743962347507477
train gradient:  0.2845595255422285
iteration : 8524
train acc:  0.875
train loss:  0.3263648748397827
train gradient:  0.15813422624301937
iteration : 8525
train acc:  0.859375
train loss:  0.34432506561279297
train gradient:  0.21889095273445197
iteration : 8526
train acc:  0.859375
train loss:  0.3327670097351074
train gradient:  0.24743638020858516
iteration : 8527
train acc:  0.8671875
train loss:  0.2667025923728943
train gradient:  0.1796549063659209
iteration : 8528
train acc:  0.875
train loss:  0.2777516841888428
train gradient:  0.18287867956754167
iteration : 8529
train acc:  0.890625
train loss:  0.27039864659309387
train gradient:  0.11000270976020565
iteration : 8530
train acc:  0.8046875
train loss:  0.42028820514678955
train gradient:  0.26120539665409603
iteration : 8531
train acc:  0.875
train loss:  0.28300797939300537
train gradient:  0.1797180065636053
iteration : 8532
train acc:  0.828125
train loss:  0.348027765750885
train gradient:  0.16764679543480462
iteration : 8533
train acc:  0.875
train loss:  0.3012741208076477
train gradient:  0.13250354115056745
iteration : 8534
train acc:  0.8046875
train loss:  0.3940422236919403
train gradient:  0.20941872093794123
iteration : 8535
train acc:  0.8515625
train loss:  0.33930909633636475
train gradient:  0.1903551488860803
iteration : 8536
train acc:  0.859375
train loss:  0.357915997505188
train gradient:  0.1888886421153753
iteration : 8537
train acc:  0.8828125
train loss:  0.3120397627353668
train gradient:  0.35047829910802497
iteration : 8538
train acc:  0.8515625
train loss:  0.3438870906829834
train gradient:  0.23744980225109064
iteration : 8539
train acc:  0.828125
train loss:  0.40756967663764954
train gradient:  0.23145565824673037
iteration : 8540
train acc:  0.859375
train loss:  0.34405583143234253
train gradient:  0.17340741374873317
iteration : 8541
train acc:  0.8203125
train loss:  0.3197452425956726
train gradient:  0.14478798500473472
iteration : 8542
train acc:  0.84375
train loss:  0.35948503017425537
train gradient:  0.2890289444123227
iteration : 8543
train acc:  0.859375
train loss:  0.3580660820007324
train gradient:  0.23541330864219084
iteration : 8544
train acc:  0.8515625
train loss:  0.35843563079833984
train gradient:  0.22896379524239655
iteration : 8545
train acc:  0.875
train loss:  0.27271318435668945
train gradient:  0.12546528017721809
iteration : 8546
train acc:  0.828125
train loss:  0.3858530521392822
train gradient:  0.28942198953447124
iteration : 8547
train acc:  0.875
train loss:  0.2854228913784027
train gradient:  0.17045119052990715
iteration : 8548
train acc:  0.890625
train loss:  0.2717035114765167
train gradient:  0.157442201314425
iteration : 8549
train acc:  0.8515625
train loss:  0.3711870014667511
train gradient:  0.15351364455297362
iteration : 8550
train acc:  0.8359375
train loss:  0.3370143473148346
train gradient:  0.14116875511579677
iteration : 8551
train acc:  0.8125
train loss:  0.42306846380233765
train gradient:  0.4328672079105254
iteration : 8552
train acc:  0.8515625
train loss:  0.32885774970054626
train gradient:  0.13638431535446546
iteration : 8553
train acc:  0.8828125
train loss:  0.3074696660041809
train gradient:  0.19679367478338403
iteration : 8554
train acc:  0.8125
train loss:  0.4140516519546509
train gradient:  0.22354723730391685
iteration : 8555
train acc:  0.8359375
train loss:  0.3571639060974121
train gradient:  0.19728556350151066
iteration : 8556
train acc:  0.8671875
train loss:  0.3050227761268616
train gradient:  0.16540061725868055
iteration : 8557
train acc:  0.8828125
train loss:  0.29823750257492065
train gradient:  0.13959959942965208
iteration : 8558
train acc:  0.8828125
train loss:  0.29661768674850464
train gradient:  0.11774046867543411
iteration : 8559
train acc:  0.828125
train loss:  0.3417956233024597
train gradient:  0.1687603974603334
iteration : 8560
train acc:  0.859375
train loss:  0.3088875412940979
train gradient:  0.11303461959245069
iteration : 8561
train acc:  0.8203125
train loss:  0.39439815282821655
train gradient:  0.251397082585694
iteration : 8562
train acc:  0.84375
train loss:  0.3470841944217682
train gradient:  0.13922806088947887
iteration : 8563
train acc:  0.8515625
train loss:  0.3187301456928253
train gradient:  0.1492214671589613
iteration : 8564
train acc:  0.8203125
train loss:  0.39096513390541077
train gradient:  0.22128786284875346
iteration : 8565
train acc:  0.8984375
train loss:  0.2610164284706116
train gradient:  0.10847084329989859
iteration : 8566
train acc:  0.8359375
train loss:  0.3795413374900818
train gradient:  0.23086373458108722
iteration : 8567
train acc:  0.8828125
train loss:  0.2971925735473633
train gradient:  0.12728839629641597
iteration : 8568
train acc:  0.8359375
train loss:  0.31422173976898193
train gradient:  0.1608822835155825
iteration : 8569
train acc:  0.8359375
train loss:  0.28914159536361694
train gradient:  0.1687845893075557
iteration : 8570
train acc:  0.9140625
train loss:  0.3163490891456604
train gradient:  0.18405147465854918
iteration : 8571
train acc:  0.8203125
train loss:  0.38712549209594727
train gradient:  0.16295211217461503
iteration : 8572
train acc:  0.875
train loss:  0.33877474069595337
train gradient:  0.40336841937943224
iteration : 8573
train acc:  0.828125
train loss:  0.4003335237503052
train gradient:  0.17925889087426955
iteration : 8574
train acc:  0.8359375
train loss:  0.3473621606826782
train gradient:  0.130346710344157
iteration : 8575
train acc:  0.8359375
train loss:  0.342864066362381
train gradient:  0.14658794482704046
iteration : 8576
train acc:  0.84375
train loss:  0.31133878231048584
train gradient:  0.1459563737793475
iteration : 8577
train acc:  0.8828125
train loss:  0.32331037521362305
train gradient:  0.18235904732750308
iteration : 8578
train acc:  0.875
train loss:  0.2950993776321411
train gradient:  0.1267713248198598
iteration : 8579
train acc:  0.8671875
train loss:  0.29769694805145264
train gradient:  0.15160276083105584
iteration : 8580
train acc:  0.84375
train loss:  0.30788785219192505
train gradient:  0.1556339962798009
iteration : 8581
train acc:  0.8203125
train loss:  0.38491520285606384
train gradient:  0.1984193272108078
iteration : 8582
train acc:  0.8515625
train loss:  0.32315027713775635
train gradient:  0.15235104908117206
iteration : 8583
train acc:  0.8828125
train loss:  0.31803464889526367
train gradient:  0.14356982652537473
iteration : 8584
train acc:  0.8359375
train loss:  0.3274728059768677
train gradient:  0.16690292655507943
iteration : 8585
train acc:  0.859375
train loss:  0.3443194627761841
train gradient:  0.18076905577560531
iteration : 8586
train acc:  0.8828125
train loss:  0.33403289318084717
train gradient:  0.25312771782565735
iteration : 8587
train acc:  0.8359375
train loss:  0.3092713952064514
train gradient:  0.11852019535350243
iteration : 8588
train acc:  0.875
train loss:  0.3061524033546448
train gradient:  0.13965631978674853
iteration : 8589
train acc:  0.8828125
train loss:  0.366812527179718
train gradient:  0.1892279231718663
iteration : 8590
train acc:  0.875
train loss:  0.2975938618183136
train gradient:  0.20343544788234932
iteration : 8591
train acc:  0.828125
train loss:  0.3350551128387451
train gradient:  0.1500398761860901
iteration : 8592
train acc:  0.8046875
train loss:  0.4100530743598938
train gradient:  0.3091221409639597
iteration : 8593
train acc:  0.8203125
train loss:  0.3287653923034668
train gradient:  0.22425470292195135
iteration : 8594
train acc:  0.8984375
train loss:  0.2750305235385895
train gradient:  0.1193538906438029
iteration : 8595
train acc:  0.8515625
train loss:  0.3098064959049225
train gradient:  0.13979535239328342
iteration : 8596
train acc:  0.8359375
train loss:  0.3479388952255249
train gradient:  0.23657369999992497
iteration : 8597
train acc:  0.9296875
train loss:  0.2630152404308319
train gradient:  0.18003729729342993
iteration : 8598
train acc:  0.8515625
train loss:  0.3714202046394348
train gradient:  0.20712610606788967
iteration : 8599
train acc:  0.8515625
train loss:  0.33462926745414734
train gradient:  0.17295819871736517
iteration : 8600
train acc:  0.859375
train loss:  0.36020511388778687
train gradient:  0.1940938448721654
iteration : 8601
train acc:  0.8671875
train loss:  0.3203847408294678
train gradient:  0.13938052968892017
iteration : 8602
train acc:  0.8125
train loss:  0.3801138401031494
train gradient:  0.19294321461948732
iteration : 8603
train acc:  0.84375
train loss:  0.3337027430534363
train gradient:  0.14034396399598498
iteration : 8604
train acc:  0.84375
train loss:  0.3042742908000946
train gradient:  0.12191385439038033
iteration : 8605
train acc:  0.8671875
train loss:  0.32324105501174927
train gradient:  0.15627324224457795
iteration : 8606
train acc:  0.84375
train loss:  0.2953473925590515
train gradient:  0.14933105528021218
iteration : 8607
train acc:  0.828125
train loss:  0.3624194860458374
train gradient:  0.303344693335334
iteration : 8608
train acc:  0.8515625
train loss:  0.3521822988986969
train gradient:  0.17187874987417323
iteration : 8609
train acc:  0.8515625
train loss:  0.3212425112724304
train gradient:  0.17903055029229797
iteration : 8610
train acc:  0.828125
train loss:  0.38410085439682007
train gradient:  0.15254353089206635
iteration : 8611
train acc:  0.828125
train loss:  0.29624491930007935
train gradient:  0.12265944578165906
iteration : 8612
train acc:  0.8828125
train loss:  0.3446030914783478
train gradient:  0.2176601233831183
iteration : 8613
train acc:  0.8359375
train loss:  0.4071352481842041
train gradient:  0.2928834422889855
iteration : 8614
train acc:  0.8671875
train loss:  0.30381515622138977
train gradient:  0.16365052747696968
iteration : 8615
train acc:  0.921875
train loss:  0.2388439178466797
train gradient:  0.1301390332563114
iteration : 8616
train acc:  0.90625
train loss:  0.24429571628570557
train gradient:  0.12542923300852155
iteration : 8617
train acc:  0.859375
train loss:  0.3564191162586212
train gradient:  0.3137011511657215
iteration : 8618
train acc:  0.875
train loss:  0.30281734466552734
train gradient:  0.13567573173427883
iteration : 8619
train acc:  0.84375
train loss:  0.31266504526138306
train gradient:  0.13859800558588264
iteration : 8620
train acc:  0.875
train loss:  0.2900225520133972
train gradient:  0.1354009626397628
iteration : 8621
train acc:  0.8046875
train loss:  0.418856143951416
train gradient:  0.28896379406674577
iteration : 8622
train acc:  0.7890625
train loss:  0.46475088596343994
train gradient:  0.34602887740726995
iteration : 8623
train acc:  0.8828125
train loss:  0.29511091113090515
train gradient:  0.14737598478612263
iteration : 8624
train acc:  0.8515625
train loss:  0.3718215823173523
train gradient:  0.18350404001071363
iteration : 8625
train acc:  0.84375
train loss:  0.3816653788089752
train gradient:  0.2587802125425941
iteration : 8626
train acc:  0.859375
train loss:  0.341536283493042
train gradient:  0.1689126648729427
iteration : 8627
train acc:  0.9140625
train loss:  0.244867205619812
train gradient:  0.10253444514479644
iteration : 8628
train acc:  0.8828125
train loss:  0.2851657569408417
train gradient:  0.1030897353838514
iteration : 8629
train acc:  0.890625
train loss:  0.307727187871933
train gradient:  0.15571111455020592
iteration : 8630
train acc:  0.875
train loss:  0.3127160668373108
train gradient:  0.11904580230946878
iteration : 8631
train acc:  0.8359375
train loss:  0.35568416118621826
train gradient:  0.23767189888100304
iteration : 8632
train acc:  0.8515625
train loss:  0.3674430847167969
train gradient:  0.2069714911378937
iteration : 8633
train acc:  0.859375
train loss:  0.2878255844116211
train gradient:  0.16024403548896382
iteration : 8634
train acc:  0.8828125
train loss:  0.2493896335363388
train gradient:  0.16724133149142967
iteration : 8635
train acc:  0.859375
train loss:  0.3445841670036316
train gradient:  0.16396062854425286
iteration : 8636
train acc:  0.890625
train loss:  0.3158115744590759
train gradient:  0.2346459616338351
iteration : 8637
train acc:  0.7890625
train loss:  0.4495251178741455
train gradient:  0.32819632481303457
iteration : 8638
train acc:  0.8671875
train loss:  0.33214008808135986
train gradient:  0.1640868498787445
iteration : 8639
train acc:  0.8984375
train loss:  0.26886358857154846
train gradient:  0.1266941777359968
iteration : 8640
train acc:  0.890625
train loss:  0.3014408051967621
train gradient:  0.1450544406297514
iteration : 8641
train acc:  0.8671875
train loss:  0.2829536199569702
train gradient:  0.12301641669383304
iteration : 8642
train acc:  0.8828125
train loss:  0.34310996532440186
train gradient:  0.15025515525199828
iteration : 8643
train acc:  0.84375
train loss:  0.3647409677505493
train gradient:  0.2976627298579694
iteration : 8644
train acc:  0.796875
train loss:  0.520790159702301
train gradient:  0.3625018857041641
iteration : 8645
train acc:  0.875
train loss:  0.29495400190353394
train gradient:  0.14124550004370112
iteration : 8646
train acc:  0.859375
train loss:  0.32164865732192993
train gradient:  0.23083147711838675
iteration : 8647
train acc:  0.8671875
train loss:  0.2725278437137604
train gradient:  0.15321515717357462
iteration : 8648
train acc:  0.859375
train loss:  0.3010399341583252
train gradient:  0.15225358479821777
iteration : 8649
train acc:  0.8984375
train loss:  0.30351483821868896
train gradient:  0.21272677367457005
iteration : 8650
train acc:  0.8828125
train loss:  0.3023049831390381
train gradient:  0.17786029715262608
iteration : 8651
train acc:  0.796875
train loss:  0.42096495628356934
train gradient:  0.35821625965290127
iteration : 8652
train acc:  0.8203125
train loss:  0.42621129751205444
train gradient:  0.31789206941693954
iteration : 8653
train acc:  0.90625
train loss:  0.24942532181739807
train gradient:  0.09094954680575298
iteration : 8654
train acc:  0.8515625
train loss:  0.33960723876953125
train gradient:  0.1786220610572103
iteration : 8655
train acc:  0.890625
train loss:  0.3175528943538666
train gradient:  0.22468152471129046
iteration : 8656
train acc:  0.875
train loss:  0.29997771978378296
train gradient:  0.1677783332005297
iteration : 8657
train acc:  0.8671875
train loss:  0.2748607397079468
train gradient:  0.17809088983364013
iteration : 8658
train acc:  0.8984375
train loss:  0.31322264671325684
train gradient:  0.2135995831853692
iteration : 8659
train acc:  0.84375
train loss:  0.30803462862968445
train gradient:  0.19122578192972778
iteration : 8660
train acc:  0.84375
train loss:  0.34456130862236023
train gradient:  0.1652629344965016
iteration : 8661
train acc:  0.8046875
train loss:  0.370932936668396
train gradient:  0.25067657845628694
iteration : 8662
train acc:  0.828125
train loss:  0.35135847330093384
train gradient:  0.17216365868672348
iteration : 8663
train acc:  0.8984375
train loss:  0.2973597049713135
train gradient:  0.1607556989526684
iteration : 8664
train acc:  0.859375
train loss:  0.33988434076309204
train gradient:  0.23675637642670977
iteration : 8665
train acc:  0.8671875
train loss:  0.2529602646827698
train gradient:  0.0939474946410339
iteration : 8666
train acc:  0.875
train loss:  0.27639877796173096
train gradient:  0.08285397017657248
iteration : 8667
train acc:  0.8203125
train loss:  0.43289315700531006
train gradient:  0.4217068859755522
iteration : 8668
train acc:  0.84375
train loss:  0.3471040725708008
train gradient:  0.2203199819003483
iteration : 8669
train acc:  0.875
train loss:  0.29081571102142334
train gradient:  0.17205449424237912
iteration : 8670
train acc:  0.890625
train loss:  0.29126569628715515
train gradient:  0.1326119595340403
iteration : 8671
train acc:  0.8203125
train loss:  0.36948123574256897
train gradient:  0.25268518224316766
iteration : 8672
train acc:  0.859375
train loss:  0.2856982350349426
train gradient:  0.15467281625974477
iteration : 8673
train acc:  0.859375
train loss:  0.3341595530509949
train gradient:  0.12807786029541568
iteration : 8674
train acc:  0.8359375
train loss:  0.3309265971183777
train gradient:  0.32924811753950695
iteration : 8675
train acc:  0.84375
train loss:  0.33971577882766724
train gradient:  0.19037394576791286
iteration : 8676
train acc:  0.859375
train loss:  0.3716067373752594
train gradient:  0.22582013896484543
iteration : 8677
train acc:  0.8203125
train loss:  0.4337797164916992
train gradient:  0.28008376382135736
iteration : 8678
train acc:  0.8671875
train loss:  0.32911425828933716
train gradient:  0.14142734756736322
iteration : 8679
train acc:  0.8515625
train loss:  0.40420109033584595
train gradient:  0.21205113927255528
iteration : 8680
train acc:  0.84375
train loss:  0.3316804766654968
train gradient:  0.16210103235561446
iteration : 8681
train acc:  0.8984375
train loss:  0.3179263770580292
train gradient:  0.18736308194618656
iteration : 8682
train acc:  0.859375
train loss:  0.29598933458328247
train gradient:  0.12927044990347736
iteration : 8683
train acc:  0.8515625
train loss:  0.35729503631591797
train gradient:  0.26774299018502196
iteration : 8684
train acc:  0.859375
train loss:  0.3158310055732727
train gradient:  0.2183784954791091
iteration : 8685
train acc:  0.875
train loss:  0.3498073220252991
train gradient:  0.237201528416598
iteration : 8686
train acc:  0.8671875
train loss:  0.31588461995124817
train gradient:  0.17330086750320256
iteration : 8687
train acc:  0.8984375
train loss:  0.2917190194129944
train gradient:  0.1522759489355177
iteration : 8688
train acc:  0.8125
train loss:  0.4835888743400574
train gradient:  0.2959992294962201
iteration : 8689
train acc:  0.84375
train loss:  0.3417283892631531
train gradient:  0.18113503263025196
iteration : 8690
train acc:  0.8671875
train loss:  0.3115653395652771
train gradient:  0.23716349058183256
iteration : 8691
train acc:  0.859375
train loss:  0.3594173491001129
train gradient:  0.18946180550256975
iteration : 8692
train acc:  0.8359375
train loss:  0.3654762804508209
train gradient:  0.23436088918060616
iteration : 8693
train acc:  0.8515625
train loss:  0.34632134437561035
train gradient:  0.19422062750883567
iteration : 8694
train acc:  0.8984375
train loss:  0.2937971353530884
train gradient:  0.13567790144552005
iteration : 8695
train acc:  0.8359375
train loss:  0.35250604152679443
train gradient:  0.16710469842683656
iteration : 8696
train acc:  0.828125
train loss:  0.3684558570384979
train gradient:  0.19251254092044606
iteration : 8697
train acc:  0.8671875
train loss:  0.3372430205345154
train gradient:  0.13871420962342956
iteration : 8698
train acc:  0.8359375
train loss:  0.33185404539108276
train gradient:  0.1839310238956523
iteration : 8699
train acc:  0.8046875
train loss:  0.3891262710094452
train gradient:  0.19767700827268347
iteration : 8700
train acc:  0.875
train loss:  0.25790056586265564
train gradient:  0.1026302324449823
iteration : 8701
train acc:  0.8359375
train loss:  0.37967854738235474
train gradient:  0.20256954909154476
iteration : 8702
train acc:  0.8671875
train loss:  0.3399313688278198
train gradient:  0.18754232450883876
iteration : 8703
train acc:  0.875
train loss:  0.2986767888069153
train gradient:  0.1652634872522209
iteration : 8704
train acc:  0.8359375
train loss:  0.35515010356903076
train gradient:  0.25222721245296936
iteration : 8705
train acc:  0.875
train loss:  0.2959046959877014
train gradient:  0.14643923439652773
iteration : 8706
train acc:  0.9140625
train loss:  0.2996872067451477
train gradient:  0.141935163098944
iteration : 8707
train acc:  0.8359375
train loss:  0.332797110080719
train gradient:  0.16834056023806432
iteration : 8708
train acc:  0.9296875
train loss:  0.21309638023376465
train gradient:  0.17709513930413612
iteration : 8709
train acc:  0.8203125
train loss:  0.34862858057022095
train gradient:  0.18034884948825236
iteration : 8710
train acc:  0.84375
train loss:  0.32889485359191895
train gradient:  0.19600612691423097
iteration : 8711
train acc:  0.8515625
train loss:  0.30880314111709595
train gradient:  0.13998083729457766
iteration : 8712
train acc:  0.796875
train loss:  0.3609820604324341
train gradient:  0.19257674133765154
iteration : 8713
train acc:  0.8203125
train loss:  0.4512999653816223
train gradient:  0.2573942334789969
iteration : 8714
train acc:  0.9453125
train loss:  0.2661454677581787
train gradient:  0.105042485512538
iteration : 8715
train acc:  0.875
train loss:  0.3012494444847107
train gradient:  0.2143709741309101
iteration : 8716
train acc:  0.859375
train loss:  0.25108712911605835
train gradient:  0.09390816378543235
iteration : 8717
train acc:  0.84375
train loss:  0.323491632938385
train gradient:  0.12767380892814462
iteration : 8718
train acc:  0.8203125
train loss:  0.3633825182914734
train gradient:  0.22616630430795165
iteration : 8719
train acc:  0.8671875
train loss:  0.38529080152511597
train gradient:  0.2812884789095768
iteration : 8720
train acc:  0.90625
train loss:  0.2638075351715088
train gradient:  0.1522919565558878
iteration : 8721
train acc:  0.875
train loss:  0.31459155678749084
train gradient:  0.12967629195461583
iteration : 8722
train acc:  0.84375
train loss:  0.4140622913837433
train gradient:  0.24726707720332916
iteration : 8723
train acc:  0.9140625
train loss:  0.2525835633277893
train gradient:  0.11234750610200017
iteration : 8724
train acc:  0.8359375
train loss:  0.33008745312690735
train gradient:  0.13220641730404847
iteration : 8725
train acc:  0.84375
train loss:  0.36443692445755005
train gradient:  0.1564246879773008
iteration : 8726
train acc:  0.890625
train loss:  0.26045697927474976
train gradient:  0.15822366390406511
iteration : 8727
train acc:  0.890625
train loss:  0.2834279537200928
train gradient:  0.1408215092900925
iteration : 8728
train acc:  0.8671875
train loss:  0.3182275593280792
train gradient:  0.23858821786438048
iteration : 8729
train acc:  0.828125
train loss:  0.3363678455352783
train gradient:  0.21506166547997246
iteration : 8730
train acc:  0.859375
train loss:  0.28484857082366943
train gradient:  0.18960947249278656
iteration : 8731
train acc:  0.859375
train loss:  0.3147684335708618
train gradient:  0.2286272636419272
iteration : 8732
train acc:  0.8046875
train loss:  0.4104345738887787
train gradient:  0.22908980422120195
iteration : 8733
train acc:  0.8671875
train loss:  0.3019986152648926
train gradient:  0.1640273420312458
iteration : 8734
train acc:  0.84375
train loss:  0.35167211294174194
train gradient:  0.25793055602510906
iteration : 8735
train acc:  0.8515625
train loss:  0.30116137862205505
train gradient:  0.2621092842715206
iteration : 8736
train acc:  0.828125
train loss:  0.3696613907814026
train gradient:  0.1850617911349689
iteration : 8737
train acc:  0.8671875
train loss:  0.3216409683227539
train gradient:  0.19270205851515487
iteration : 8738
train acc:  0.78125
train loss:  0.45720475912094116
train gradient:  0.2293261603253782
iteration : 8739
train acc:  0.8671875
train loss:  0.30777478218078613
train gradient:  0.1340917957979396
iteration : 8740
train acc:  0.875
train loss:  0.3274117708206177
train gradient:  0.19227007563140797
iteration : 8741
train acc:  0.859375
train loss:  0.3737873435020447
train gradient:  0.22300422399381176
iteration : 8742
train acc:  0.8046875
train loss:  0.3809862732887268
train gradient:  0.24573445087397383
iteration : 8743
train acc:  0.890625
train loss:  0.27110162377357483
train gradient:  0.13116216511509682
iteration : 8744
train acc:  0.828125
train loss:  0.3140479624271393
train gradient:  0.12827879994707883
iteration : 8745
train acc:  0.859375
train loss:  0.34672778844833374
train gradient:  0.17787444588656792
iteration : 8746
train acc:  0.90625
train loss:  0.2217683047056198
train gradient:  0.09712715382846976
iteration : 8747
train acc:  0.84375
train loss:  0.33881402015686035
train gradient:  0.2160558845529617
iteration : 8748
train acc:  0.828125
train loss:  0.45840126276016235
train gradient:  0.2596414971774561
iteration : 8749
train acc:  0.7890625
train loss:  0.4155994951725006
train gradient:  0.22480907321449897
iteration : 8750
train acc:  0.8671875
train loss:  0.336650550365448
train gradient:  0.15385556020364644
iteration : 8751
train acc:  0.875
train loss:  0.2829945683479309
train gradient:  0.14434643714828174
iteration : 8752
train acc:  0.90625
train loss:  0.2607225179672241
train gradient:  0.18836326556571803
iteration : 8753
train acc:  0.8671875
train loss:  0.29132431745529175
train gradient:  0.15538214891220292
iteration : 8754
train acc:  0.8359375
train loss:  0.31751930713653564
train gradient:  0.2111878812008772
iteration : 8755
train acc:  0.8828125
train loss:  0.2581019103527069
train gradient:  0.14842677960293232
iteration : 8756
train acc:  0.8671875
train loss:  0.29983440041542053
train gradient:  0.1288815032271
iteration : 8757
train acc:  0.8515625
train loss:  0.3138141632080078
train gradient:  0.1579673388947332
iteration : 8758
train acc:  0.890625
train loss:  0.25317519903182983
train gradient:  0.1794889592459352
iteration : 8759
train acc:  0.890625
train loss:  0.25357306003570557
train gradient:  0.12450354028227971
iteration : 8760
train acc:  0.8515625
train loss:  0.3656580448150635
train gradient:  0.23477267446382005
iteration : 8761
train acc:  0.828125
train loss:  0.3888683617115021
train gradient:  0.2017705219944517
iteration : 8762
train acc:  0.8046875
train loss:  0.3910181522369385
train gradient:  0.215307295990926
iteration : 8763
train acc:  0.890625
train loss:  0.318571001291275
train gradient:  0.11755167602536175
iteration : 8764
train acc:  0.8515625
train loss:  0.32887399196624756
train gradient:  0.19119015658431893
iteration : 8765
train acc:  0.828125
train loss:  0.3801213800907135
train gradient:  0.161404992121929
iteration : 8766
train acc:  0.8828125
train loss:  0.2929263710975647
train gradient:  0.1527438274560894
iteration : 8767
train acc:  0.859375
train loss:  0.29584598541259766
train gradient:  0.16127409062573594
iteration : 8768
train acc:  0.8828125
train loss:  0.2751948833465576
train gradient:  0.13126409183790266
iteration : 8769
train acc:  0.875
train loss:  0.286330908536911
train gradient:  0.1405533473287397
iteration : 8770
train acc:  0.8671875
train loss:  0.28409498929977417
train gradient:  0.14501818499658953
iteration : 8771
train acc:  0.875
train loss:  0.29330533742904663
train gradient:  0.12493801509383878
iteration : 8772
train acc:  0.890625
train loss:  0.2539862394332886
train gradient:  0.09328859525554029
iteration : 8773
train acc:  0.8515625
train loss:  0.36640554666519165
train gradient:  0.2061969536096406
iteration : 8774
train acc:  0.8203125
train loss:  0.35044682025909424
train gradient:  0.19803322730371287
iteration : 8775
train acc:  0.8671875
train loss:  0.2864898145198822
train gradient:  0.16110497931371184
iteration : 8776
train acc:  0.90625
train loss:  0.2476710081100464
train gradient:  0.1022666117323292
iteration : 8777
train acc:  0.8671875
train loss:  0.30271703004837036
train gradient:  0.13163984750711272
iteration : 8778
train acc:  0.8515625
train loss:  0.3534284830093384
train gradient:  0.1653084991392077
iteration : 8779
train acc:  0.8515625
train loss:  0.3604157567024231
train gradient:  0.1564368129779946
iteration : 8780
train acc:  0.8515625
train loss:  0.3712542653083801
train gradient:  0.2560147690375565
iteration : 8781
train acc:  0.8515625
train loss:  0.32256460189819336
train gradient:  0.14983935572379775
iteration : 8782
train acc:  0.8359375
train loss:  0.3728756308555603
train gradient:  0.2818856495220478
iteration : 8783
train acc:  0.8828125
train loss:  0.27164554595947266
train gradient:  0.11586166223832595
iteration : 8784
train acc:  0.78125
train loss:  0.40915700793266296
train gradient:  0.2491270586684886
iteration : 8785
train acc:  0.890625
train loss:  0.25641512870788574
train gradient:  0.15427859524981793
iteration : 8786
train acc:  0.8828125
train loss:  0.290603905916214
train gradient:  0.18340821641891053
iteration : 8787
train acc:  0.8515625
train loss:  0.35553157329559326
train gradient:  0.2093216202469143
iteration : 8788
train acc:  0.8515625
train loss:  0.39226704835891724
train gradient:  0.20324596990330557
iteration : 8789
train acc:  0.84375
train loss:  0.3309587240219116
train gradient:  0.18216683108845622
iteration : 8790
train acc:  0.8515625
train loss:  0.3539980351924896
train gradient:  0.21827885266038893
iteration : 8791
train acc:  0.8203125
train loss:  0.402583509683609
train gradient:  0.28175730473994887
iteration : 8792
train acc:  0.8828125
train loss:  0.26769036054611206
train gradient:  0.19740785839663855
iteration : 8793
train acc:  0.875
train loss:  0.2757647633552551
train gradient:  0.1377446652106458
iteration : 8794
train acc:  0.875
train loss:  0.2996254563331604
train gradient:  0.14079545865739784
iteration : 8795
train acc:  0.84375
train loss:  0.4309132695198059
train gradient:  0.21892908747824225
iteration : 8796
train acc:  0.8203125
train loss:  0.38858699798583984
train gradient:  0.19462177262097247
iteration : 8797
train acc:  0.8828125
train loss:  0.31041473150253296
train gradient:  0.15484335023192847
iteration : 8798
train acc:  0.9296875
train loss:  0.23872853815555573
train gradient:  0.13668756857412365
iteration : 8799
train acc:  0.8125
train loss:  0.3725508153438568
train gradient:  0.26896923286335783
iteration : 8800
train acc:  0.8515625
train loss:  0.33024004101753235
train gradient:  0.2104156616079385
iteration : 8801
train acc:  0.8359375
train loss:  0.35637688636779785
train gradient:  0.23527631274025695
iteration : 8802
train acc:  0.8125
train loss:  0.34232085943222046
train gradient:  0.23658753008160693
iteration : 8803
train acc:  0.859375
train loss:  0.3152812719345093
train gradient:  0.1687847977658098
iteration : 8804
train acc:  0.8359375
train loss:  0.34033703804016113
train gradient:  0.14225920082947602
iteration : 8805
train acc:  0.859375
train loss:  0.35015228390693665
train gradient:  0.2196666750033529
iteration : 8806
train acc:  0.8984375
train loss:  0.2510298192501068
train gradient:  0.1250122869645339
iteration : 8807
train acc:  0.875
train loss:  0.31821510195732117
train gradient:  0.15362164875140497
iteration : 8808
train acc:  0.8984375
train loss:  0.3499183654785156
train gradient:  0.17528889265678518
iteration : 8809
train acc:  0.78125
train loss:  0.48343023657798767
train gradient:  0.30128182349976423
iteration : 8810
train acc:  0.8515625
train loss:  0.3630330264568329
train gradient:  0.29652959623527475
iteration : 8811
train acc:  0.890625
train loss:  0.35112178325653076
train gradient:  0.1803514101753341
iteration : 8812
train acc:  0.8515625
train loss:  0.3300948143005371
train gradient:  0.18575704900753048
iteration : 8813
train acc:  0.8828125
train loss:  0.3335217833518982
train gradient:  0.20145472994441635
iteration : 8814
train acc:  0.8984375
train loss:  0.31540054082870483
train gradient:  0.1384065320697958
iteration : 8815
train acc:  0.8359375
train loss:  0.3385689854621887
train gradient:  0.31399392675570903
iteration : 8816
train acc:  0.828125
train loss:  0.3097016215324402
train gradient:  0.23094161670000796
iteration : 8817
train acc:  0.8671875
train loss:  0.3342353105545044
train gradient:  0.19076376895910202
iteration : 8818
train acc:  0.8671875
train loss:  0.2749546468257904
train gradient:  0.1664824858602914
iteration : 8819
train acc:  0.8828125
train loss:  0.2577313780784607
train gradient:  0.12890047128261742
iteration : 8820
train acc:  0.84375
train loss:  0.3335866928100586
train gradient:  0.20004613271823113
iteration : 8821
train acc:  0.8984375
train loss:  0.25896406173706055
train gradient:  0.1638305714152909
iteration : 8822
train acc:  0.8359375
train loss:  0.39435526728630066
train gradient:  0.28206202823357907
iteration : 8823
train acc:  0.828125
train loss:  0.3016253411769867
train gradient:  0.14106699797805727
iteration : 8824
train acc:  0.8671875
train loss:  0.3038897216320038
train gradient:  0.1358131248321357
iteration : 8825
train acc:  0.875
train loss:  0.345129132270813
train gradient:  0.18035799990754076
iteration : 8826
train acc:  0.8671875
train loss:  0.29935210943222046
train gradient:  0.12365125046475664
iteration : 8827
train acc:  0.859375
train loss:  0.3551021218299866
train gradient:  0.18139479873945322
iteration : 8828
train acc:  0.84375
train loss:  0.3562241792678833
train gradient:  0.21396679353174136
iteration : 8829
train acc:  0.875
train loss:  0.3070002794265747
train gradient:  0.13512010131591395
iteration : 8830
train acc:  0.890625
train loss:  0.2571735978126526
train gradient:  0.15219225789745333
iteration : 8831
train acc:  0.90625
train loss:  0.2527965009212494
train gradient:  0.13134562274290396
iteration : 8832
train acc:  0.875
train loss:  0.2893238961696625
train gradient:  0.14627555957200747
iteration : 8833
train acc:  0.890625
train loss:  0.24949514865875244
train gradient:  0.17285706934034745
iteration : 8834
train acc:  0.8515625
train loss:  0.3307116627693176
train gradient:  0.22272080443382938
iteration : 8835
train acc:  0.8359375
train loss:  0.38585689663887024
train gradient:  0.24022894730922273
iteration : 8836
train acc:  0.875
train loss:  0.32260650396347046
train gradient:  0.24743047034535345
iteration : 8837
train acc:  0.8671875
train loss:  0.2931760549545288
train gradient:  0.16938088591002495
iteration : 8838
train acc:  0.796875
train loss:  0.3927183449268341
train gradient:  0.23658799477779985
iteration : 8839
train acc:  0.8671875
train loss:  0.2993953227996826
train gradient:  0.20073730353927288
iteration : 8840
train acc:  0.9140625
train loss:  0.22203750908374786
train gradient:  0.13532039306080562
iteration : 8841
train acc:  0.859375
train loss:  0.3644408583641052
train gradient:  0.15754715262434071
iteration : 8842
train acc:  0.7734375
train loss:  0.40614843368530273
train gradient:  0.2403437113967091
iteration : 8843
train acc:  0.859375
train loss:  0.2937621474266052
train gradient:  0.142366024718122
iteration : 8844
train acc:  0.859375
train loss:  0.3362426161766052
train gradient:  0.19220881212139207
iteration : 8845
train acc:  0.859375
train loss:  0.33197471499443054
train gradient:  0.198674110236072
iteration : 8846
train acc:  0.8359375
train loss:  0.33904919028282166
train gradient:  0.19075378334152232
iteration : 8847
train acc:  0.859375
train loss:  0.2983356714248657
train gradient:  0.15587034538298367
iteration : 8848
train acc:  0.828125
train loss:  0.37668702006340027
train gradient:  0.2435179092232327
iteration : 8849
train acc:  0.84375
train loss:  0.3435995578765869
train gradient:  0.20338307810423728
iteration : 8850
train acc:  0.859375
train loss:  0.36488282680511475
train gradient:  0.1797174304037244
iteration : 8851
train acc:  0.8046875
train loss:  0.4061567783355713
train gradient:  0.24651470524669267
iteration : 8852
train acc:  0.859375
train loss:  0.34290921688079834
train gradient:  0.22234649816372926
iteration : 8853
train acc:  0.921875
train loss:  0.2556777000427246
train gradient:  0.07425342654539385
iteration : 8854
train acc:  0.84375
train loss:  0.3642067313194275
train gradient:  0.15898074841634638
iteration : 8855
train acc:  0.875
train loss:  0.28675010800361633
train gradient:  0.12842894821929202
iteration : 8856
train acc:  0.8359375
train loss:  0.32639479637145996
train gradient:  0.14365431190147163
iteration : 8857
train acc:  0.84375
train loss:  0.32096636295318604
train gradient:  0.13674555946811634
iteration : 8858
train acc:  0.84375
train loss:  0.3138101100921631
train gradient:  0.16885278214857186
iteration : 8859
train acc:  0.890625
train loss:  0.28493040800094604
train gradient:  0.20522434976600945
iteration : 8860
train acc:  0.828125
train loss:  0.3603290915489197
train gradient:  0.14526740715696693
iteration : 8861
train acc:  0.8125
train loss:  0.4099843502044678
train gradient:  0.19068952363982236
iteration : 8862
train acc:  0.8828125
train loss:  0.28934168815612793
train gradient:  0.18124918086403025
iteration : 8863
train acc:  0.8671875
train loss:  0.3545551896095276
train gradient:  0.16398535134677375
iteration : 8864
train acc:  0.8671875
train loss:  0.304179847240448
train gradient:  0.1826483510416015
iteration : 8865
train acc:  0.8359375
train loss:  0.38756951689720154
train gradient:  0.21577027812381488
iteration : 8866
train acc:  0.875
train loss:  0.3235839605331421
train gradient:  0.15714648310706242
iteration : 8867
train acc:  0.90625
train loss:  0.31820517778396606
train gradient:  0.14616977270476372
iteration : 8868
train acc:  0.875
train loss:  0.31772059202194214
train gradient:  0.35999208515260195
iteration : 8869
train acc:  0.859375
train loss:  0.308210551738739
train gradient:  0.15643092711372958
iteration : 8870
train acc:  0.8125
train loss:  0.3584252595901489
train gradient:  0.19293912738703783
iteration : 8871
train acc:  0.828125
train loss:  0.3135460615158081
train gradient:  0.16366744290296004
iteration : 8872
train acc:  0.8359375
train loss:  0.32385310530662537
train gradient:  0.19333925521472756
iteration : 8873
train acc:  0.875
train loss:  0.34808266162872314
train gradient:  0.14984805449807465
iteration : 8874
train acc:  0.8203125
train loss:  0.37404417991638184
train gradient:  0.19799865587950471
iteration : 8875
train acc:  0.875
train loss:  0.35794466733932495
train gradient:  0.2543957452093032
iteration : 8876
train acc:  0.8828125
train loss:  0.31709396839141846
train gradient:  0.14749710456038692
iteration : 8877
train acc:  0.84375
train loss:  0.3232482075691223
train gradient:  0.14729006032438535
iteration : 8878
train acc:  0.8359375
train loss:  0.3577585220336914
train gradient:  0.2393453040106129
iteration : 8879
train acc:  0.8203125
train loss:  0.4272155165672302
train gradient:  0.2217754942372183
iteration : 8880
train acc:  0.875
train loss:  0.3594139516353607
train gradient:  0.19722928382043634
iteration : 8881
train acc:  0.8671875
train loss:  0.31935179233551025
train gradient:  0.27338040389011997
iteration : 8882
train acc:  0.8359375
train loss:  0.3483031988143921
train gradient:  0.19765977028952775
iteration : 8883
train acc:  0.8203125
train loss:  0.34304559230804443
train gradient:  0.1778496883407817
iteration : 8884
train acc:  0.875
train loss:  0.34112635254859924
train gradient:  0.21373957099406543
iteration : 8885
train acc:  0.84375
train loss:  0.342070996761322
train gradient:  0.2381297672908379
iteration : 8886
train acc:  0.8515625
train loss:  0.3609579801559448
train gradient:  0.23752954529807832
iteration : 8887
train acc:  0.84375
train loss:  0.38022857904434204
train gradient:  0.22942633460718337
iteration : 8888
train acc:  0.8671875
train loss:  0.32407039403915405
train gradient:  0.1510751827478672
iteration : 8889
train acc:  0.8515625
train loss:  0.26510486006736755
train gradient:  0.12009251616282489
iteration : 8890
train acc:  0.859375
train loss:  0.2870849072933197
train gradient:  0.16126706923053516
iteration : 8891
train acc:  0.8203125
train loss:  0.34968721866607666
train gradient:  0.18764318163295574
iteration : 8892
train acc:  0.8359375
train loss:  0.34078073501586914
train gradient:  0.19776254613380992
iteration : 8893
train acc:  0.8515625
train loss:  0.3239295482635498
train gradient:  0.1832261640484872
iteration : 8894
train acc:  0.875
train loss:  0.30010467767715454
train gradient:  0.1282269762073064
iteration : 8895
train acc:  0.8671875
train loss:  0.29176634550094604
train gradient:  0.12034697874087809
iteration : 8896
train acc:  0.8515625
train loss:  0.2813148498535156
train gradient:  0.14995481324109072
iteration : 8897
train acc:  0.8515625
train loss:  0.28595471382141113
train gradient:  0.1392150000335976
iteration : 8898
train acc:  0.7734375
train loss:  0.5088776350021362
train gradient:  0.3399924370224274
iteration : 8899
train acc:  0.890625
train loss:  0.2914445102214813
train gradient:  0.1396840886539335
iteration : 8900
train acc:  0.84375
train loss:  0.36836594343185425
train gradient:  0.22053435847119707
iteration : 8901
train acc:  0.8671875
train loss:  0.3103824257850647
train gradient:  0.13784423003352636
iteration : 8902
train acc:  0.8984375
train loss:  0.31824421882629395
train gradient:  0.16580484828916855
iteration : 8903
train acc:  0.859375
train loss:  0.3384551703929901
train gradient:  0.3126997983106852
iteration : 8904
train acc:  0.84375
train loss:  0.3138780891895294
train gradient:  0.1520601423676078
iteration : 8905
train acc:  0.9140625
train loss:  0.27878955006599426
train gradient:  0.13959829595659767
iteration : 8906
train acc:  0.8828125
train loss:  0.3015454411506653
train gradient:  0.13406338826126457
iteration : 8907
train acc:  0.8671875
train loss:  0.28895747661590576
train gradient:  0.17488206400059503
iteration : 8908
train acc:  0.8828125
train loss:  0.281472384929657
train gradient:  0.11543533940929386
iteration : 8909
train acc:  0.859375
train loss:  0.32279592752456665
train gradient:  0.17607666005116113
iteration : 8910
train acc:  0.84375
train loss:  0.36282527446746826
train gradient:  0.21454372462211085
iteration : 8911
train acc:  0.8828125
train loss:  0.26602715253829956
train gradient:  0.11884650937015043
iteration : 8912
train acc:  0.8515625
train loss:  0.3222476541996002
train gradient:  0.1336840987877413
iteration : 8913
train acc:  0.796875
train loss:  0.41213569045066833
train gradient:  0.3068853143102289
iteration : 8914
train acc:  0.8359375
train loss:  0.34494954347610474
train gradient:  0.2149036724145793
iteration : 8915
train acc:  0.875
train loss:  0.296373188495636
train gradient:  0.1800865777180751
iteration : 8916
train acc:  0.8515625
train loss:  0.31985726952552795
train gradient:  0.15189419055113096
iteration : 8917
train acc:  0.796875
train loss:  0.40422672033309937
train gradient:  0.28029881508344145
iteration : 8918
train acc:  0.8671875
train loss:  0.33333277702331543
train gradient:  0.1368897608303279
iteration : 8919
train acc:  0.8515625
train loss:  0.28135740756988525
train gradient:  0.2067020825990193
iteration : 8920
train acc:  0.8515625
train loss:  0.35645997524261475
train gradient:  0.1563510783264151
iteration : 8921
train acc:  0.890625
train loss:  0.2708279490470886
train gradient:  0.1093756655335391
iteration : 8922
train acc:  0.8828125
train loss:  0.2851927876472473
train gradient:  0.14722502941099208
iteration : 8923
train acc:  0.8125
train loss:  0.3498268723487854
train gradient:  0.14800032829121346
iteration : 8924
train acc:  0.828125
train loss:  0.380757212638855
train gradient:  0.24477676473573232
iteration : 8925
train acc:  0.8359375
train loss:  0.35499146580696106
train gradient:  0.1896042263257054
iteration : 8926
train acc:  0.8515625
train loss:  0.3106430172920227
train gradient:  0.09578163440439498
iteration : 8927
train acc:  0.8671875
train loss:  0.3553696870803833
train gradient:  0.1627246685101888
iteration : 8928
train acc:  0.8828125
train loss:  0.3023449778556824
train gradient:  0.13458901423917283
iteration : 8929
train acc:  0.859375
train loss:  0.37106120586395264
train gradient:  0.22596784068256737
iteration : 8930
train acc:  0.875
train loss:  0.35744377970695496
train gradient:  0.17520129234133136
iteration : 8931
train acc:  0.8515625
train loss:  0.32335615158081055
train gradient:  0.17565432346710813
iteration : 8932
train acc:  0.84375
train loss:  0.30086344480514526
train gradient:  0.19397439707031672
iteration : 8933
train acc:  0.9375
train loss:  0.27195945382118225
train gradient:  0.12660700206697573
iteration : 8934
train acc:  0.84375
train loss:  0.3585720360279083
train gradient:  0.2697893668123647
iteration : 8935
train acc:  0.8359375
train loss:  0.4106953740119934
train gradient:  0.2832673863333571
iteration : 8936
train acc:  0.8671875
train loss:  0.2851319909095764
train gradient:  0.22592707219114264
iteration : 8937
train acc:  0.875
train loss:  0.25349172949790955
train gradient:  0.1487860484582419
iteration : 8938
train acc:  0.875
train loss:  0.30031490325927734
train gradient:  0.3215310310788122
iteration : 8939
train acc:  0.8671875
train loss:  0.3374919891357422
train gradient:  0.1363498751901247
iteration : 8940
train acc:  0.8515625
train loss:  0.37314683198928833
train gradient:  0.268407830589123
iteration : 8941
train acc:  0.84375
train loss:  0.3565254211425781
train gradient:  0.19152316187055432
iteration : 8942
train acc:  0.890625
train loss:  0.33579301834106445
train gradient:  0.16248103676357376
iteration : 8943
train acc:  0.875
train loss:  0.3350500166416168
train gradient:  0.16294267013371977
iteration : 8944
train acc:  0.8671875
train loss:  0.3398038148880005
train gradient:  0.21852069591149897
iteration : 8945
train acc:  0.8359375
train loss:  0.3100332021713257
train gradient:  0.15612467406525235
iteration : 8946
train acc:  0.84375
train loss:  0.33053529262542725
train gradient:  0.18578442606056694
iteration : 8947
train acc:  0.9140625
train loss:  0.23452068865299225
train gradient:  0.1219493598940235
iteration : 8948
train acc:  0.875
train loss:  0.31839072704315186
train gradient:  1.0947186382938359
iteration : 8949
train acc:  0.859375
train loss:  0.29195860028266907
train gradient:  0.17604421662425845
iteration : 8950
train acc:  0.8125
train loss:  0.33847880363464355
train gradient:  0.13694023855514162
iteration : 8951
train acc:  0.8203125
train loss:  0.4192204773426056
train gradient:  0.23151101314201034
iteration : 8952
train acc:  0.84375
train loss:  0.3457597494125366
train gradient:  0.1390728523431092
iteration : 8953
train acc:  0.8671875
train loss:  0.29793667793273926
train gradient:  0.17479737219760824
iteration : 8954
train acc:  0.8828125
train loss:  0.30778998136520386
train gradient:  0.16810103913399868
iteration : 8955
train acc:  0.8984375
train loss:  0.23857301473617554
train gradient:  0.1497325388655583
iteration : 8956
train acc:  0.859375
train loss:  0.309203177690506
train gradient:  0.13718059929707765
iteration : 8957
train acc:  0.890625
train loss:  0.24383525550365448
train gradient:  0.09277698092052986
iteration : 8958
train acc:  0.859375
train loss:  0.29651209712028503
train gradient:  0.17252861642459655
iteration : 8959
train acc:  0.8828125
train loss:  0.33872532844543457
train gradient:  0.1566193737803356
iteration : 8960
train acc:  0.7421875
train loss:  0.465869665145874
train gradient:  0.31076362394597207
iteration : 8961
train acc:  0.828125
train loss:  0.41297516226768494
train gradient:  0.26084782873526213
iteration : 8962
train acc:  0.8046875
train loss:  0.501250147819519
train gradient:  0.2993508343851772
iteration : 8963
train acc:  0.8984375
train loss:  0.2748590111732483
train gradient:  0.19231339509405576
iteration : 8964
train acc:  0.7734375
train loss:  0.44909411668777466
train gradient:  0.2555599326993319
iteration : 8965
train acc:  0.8125
train loss:  0.31172889471054077
train gradient:  0.19432401417236506
iteration : 8966
train acc:  0.859375
train loss:  0.3060027062892914
train gradient:  0.15647921351683136
iteration : 8967
train acc:  0.84375
train loss:  0.31215131282806396
train gradient:  0.14353576931551368
iteration : 8968
train acc:  0.8203125
train loss:  0.40383145213127136
train gradient:  0.2711121003919998
iteration : 8969
train acc:  0.859375
train loss:  0.37213748693466187
train gradient:  0.20055716864221756
iteration : 8970
train acc:  0.8984375
train loss:  0.2834257185459137
train gradient:  0.2039751087057254
iteration : 8971
train acc:  0.8203125
train loss:  0.41642510890960693
train gradient:  0.2524447372741188
iteration : 8972
train acc:  0.8671875
train loss:  0.3404066264629364
train gradient:  0.3426002578591691
iteration : 8973
train acc:  0.859375
train loss:  0.32326310873031616
train gradient:  0.18811932252075042
iteration : 8974
train acc:  0.8515625
train loss:  0.3279898166656494
train gradient:  0.17651180108182912
iteration : 8975
train acc:  0.84375
train loss:  0.31869232654571533
train gradient:  0.19829852566015382
iteration : 8976
train acc:  0.84375
train loss:  0.3249773383140564
train gradient:  0.17002201640009884
iteration : 8977
train acc:  0.78125
train loss:  0.39322566986083984
train gradient:  0.17124209908038046
iteration : 8978
train acc:  0.8828125
train loss:  0.30909430980682373
train gradient:  0.12707520913690779
iteration : 8979
train acc:  0.84375
train loss:  0.4162001609802246
train gradient:  0.5115905658667411
iteration : 8980
train acc:  0.8515625
train loss:  0.3448558449745178
train gradient:  0.17251880959357346
iteration : 8981
train acc:  0.828125
train loss:  0.3185940980911255
train gradient:  0.19801574432167487
iteration : 8982
train acc:  0.890625
train loss:  0.3109966218471527
train gradient:  0.310003385855964
iteration : 8983
train acc:  0.875
train loss:  0.31924790143966675
train gradient:  0.2143813791660702
iteration : 8984
train acc:  0.8984375
train loss:  0.2540228068828583
train gradient:  0.15098167057053527
iteration : 8985
train acc:  0.828125
train loss:  0.4015854597091675
train gradient:  0.25379997892923634
iteration : 8986
train acc:  0.8515625
train loss:  0.3283836543560028
train gradient:  0.1734187047636381
iteration : 8987
train acc:  0.859375
train loss:  0.3662092089653015
train gradient:  0.1784680163529558
iteration : 8988
train acc:  0.8828125
train loss:  0.26532164216041565
train gradient:  0.11214111948930382
iteration : 8989
train acc:  0.8359375
train loss:  0.37545913457870483
train gradient:  0.18317759693657923
iteration : 8990
train acc:  0.8828125
train loss:  0.29633697867393494
train gradient:  0.14395887533488705
iteration : 8991
train acc:  0.859375
train loss:  0.3457295298576355
train gradient:  0.19814127294626682
iteration : 8992
train acc:  0.828125
train loss:  0.4004085659980774
train gradient:  0.23633080088454647
iteration : 8993
train acc:  0.875
train loss:  0.3046823740005493
train gradient:  0.23883088984652717
iteration : 8994
train acc:  0.828125
train loss:  0.37416133284568787
train gradient:  0.1917321235918371
iteration : 8995
train acc:  0.828125
train loss:  0.42149198055267334
train gradient:  0.2291758390828195
iteration : 8996
train acc:  0.84375
train loss:  0.2835606336593628
train gradient:  0.13448211902877455
iteration : 8997
train acc:  0.828125
train loss:  0.3973258137702942
train gradient:  0.24902703010916494
iteration : 8998
train acc:  0.8203125
train loss:  0.3587648570537567
train gradient:  0.26738946999031515
iteration : 8999
train acc:  0.875
train loss:  0.25302547216415405
train gradient:  0.36416751395206454
iteration : 9000
train acc:  0.859375
train loss:  0.30801212787628174
train gradient:  0.18723589223944626
iteration : 9001
train acc:  0.8515625
train loss:  0.3413016200065613
train gradient:  0.4258108776838208
iteration : 9002
train acc:  0.8359375
train loss:  0.37162312865257263
train gradient:  0.28582596754716283
iteration : 9003
train acc:  0.8515625
train loss:  0.34083378314971924
train gradient:  0.13329388451095506
iteration : 9004
train acc:  0.8359375
train loss:  0.3088887333869934
train gradient:  0.12529961484295615
iteration : 9005
train acc:  0.84375
train loss:  0.3120647668838501
train gradient:  0.2373492926687733
iteration : 9006
train acc:  0.8359375
train loss:  0.3728277087211609
train gradient:  0.28007609192501814
iteration : 9007
train acc:  0.8203125
train loss:  0.41969388723373413
train gradient:  0.2532742066227801
iteration : 9008
train acc:  0.8515625
train loss:  0.3497268855571747
train gradient:  0.22432652967851868
iteration : 9009
train acc:  0.84375
train loss:  0.3191732168197632
train gradient:  0.2095830201640794
iteration : 9010
train acc:  0.890625
train loss:  0.28546690940856934
train gradient:  0.13975253253866343
iteration : 9011
train acc:  0.859375
train loss:  0.3135499358177185
train gradient:  0.15772527120547128
iteration : 9012
train acc:  0.859375
train loss:  0.34680813550949097
train gradient:  0.16441524168014485
iteration : 9013
train acc:  0.875
train loss:  0.29611995816230774
train gradient:  0.18041491352351147
iteration : 9014
train acc:  0.84375
train loss:  0.35365384817123413
train gradient:  0.2603696512440292
iteration : 9015
train acc:  0.8984375
train loss:  0.24069981276988983
train gradient:  0.18761607151006215
iteration : 9016
train acc:  0.828125
train loss:  0.34842559695243835
train gradient:  0.13575907205707038
iteration : 9017
train acc:  0.859375
train loss:  0.3459515869617462
train gradient:  0.14293608019166695
iteration : 9018
train acc:  0.8671875
train loss:  0.3278611898422241
train gradient:  0.21600927468884268
iteration : 9019
train acc:  0.8359375
train loss:  0.36140570044517517
train gradient:  0.1836225652041395
iteration : 9020
train acc:  0.84375
train loss:  0.3230431079864502
train gradient:  0.21177751661270222
iteration : 9021
train acc:  0.90625
train loss:  0.29193323850631714
train gradient:  0.1632977788272504
iteration : 9022
train acc:  0.8671875
train loss:  0.2963135540485382
train gradient:  0.11408647029125141
iteration : 9023
train acc:  0.90625
train loss:  0.26866990327835083
train gradient:  0.11923981524098821
iteration : 9024
train acc:  0.859375
train loss:  0.3379085063934326
train gradient:  0.18170243191060667
iteration : 9025
train acc:  0.8359375
train loss:  0.40016603469848633
train gradient:  0.23570554998235518
iteration : 9026
train acc:  0.875
train loss:  0.3362239599227905
train gradient:  0.20723174943113387
iteration : 9027
train acc:  0.8515625
train loss:  0.2781725525856018
train gradient:  0.12121355457777581
iteration : 9028
train acc:  0.8515625
train loss:  0.31799107789993286
train gradient:  0.15069922577111855
iteration : 9029
train acc:  0.8359375
train loss:  0.35402631759643555
train gradient:  0.25776354822721104
iteration : 9030
train acc:  0.8671875
train loss:  0.36585676670074463
train gradient:  0.16722415115446077
iteration : 9031
train acc:  0.8828125
train loss:  0.3184506893157959
train gradient:  0.15268657102230793
iteration : 9032
train acc:  0.828125
train loss:  0.4359917938709259
train gradient:  0.2829488642646835
iteration : 9033
train acc:  0.890625
train loss:  0.27060720324516296
train gradient:  0.1464712516976609
iteration : 9034
train acc:  0.8125
train loss:  0.4319617748260498
train gradient:  0.25876919331835935
iteration : 9035
train acc:  0.8359375
train loss:  0.3630977272987366
train gradient:  0.2088515361224552
iteration : 9036
train acc:  0.84375
train loss:  0.3657531440258026
train gradient:  0.21789417947578044
iteration : 9037
train acc:  0.8984375
train loss:  0.2865162491798401
train gradient:  0.14125536755059767
iteration : 9038
train acc:  0.859375
train loss:  0.36993831396102905
train gradient:  0.22945198454914092
iteration : 9039
train acc:  0.859375
train loss:  0.3016808032989502
train gradient:  0.1219031931765644
iteration : 9040
train acc:  0.8203125
train loss:  0.41640639305114746
train gradient:  0.27237773671437243
iteration : 9041
train acc:  0.8046875
train loss:  0.46351781487464905
train gradient:  0.20408047156465886
iteration : 9042
train acc:  0.875
train loss:  0.33288830518722534
train gradient:  0.23254389148145263
iteration : 9043
train acc:  0.8671875
train loss:  0.3057747483253479
train gradient:  0.18501991953614644
iteration : 9044
train acc:  0.875
train loss:  0.3250097632408142
train gradient:  0.14335326943048807
iteration : 9045
train acc:  0.84375
train loss:  0.297524631023407
train gradient:  0.16126731577509784
iteration : 9046
train acc:  0.8046875
train loss:  0.3435612916946411
train gradient:  0.18543001552797025
iteration : 9047
train acc:  0.8671875
train loss:  0.35188499093055725
train gradient:  0.160535928174524
iteration : 9048
train acc:  0.8359375
train loss:  0.331525981426239
train gradient:  0.24695800198217438
iteration : 9049
train acc:  0.8828125
train loss:  0.3009873032569885
train gradient:  0.1300002860258339
iteration : 9050
train acc:  0.8828125
train loss:  0.27576345205307007
train gradient:  0.11256137096489732
iteration : 9051
train acc:  0.8359375
train loss:  0.3199855089187622
train gradient:  0.13844374661881775
iteration : 9052
train acc:  0.875
train loss:  0.3193187713623047
train gradient:  0.13779011441860475
iteration : 9053
train acc:  0.890625
train loss:  0.2875344753265381
train gradient:  0.11821549181012085
iteration : 9054
train acc:  0.8828125
train loss:  0.26304084062576294
train gradient:  0.08924139190555405
iteration : 9055
train acc:  0.8671875
train loss:  0.33357155323028564
train gradient:  0.1793312385276461
iteration : 9056
train acc:  0.8671875
train loss:  0.27299678325653076
train gradient:  0.12861844070168718
iteration : 9057
train acc:  0.875
train loss:  0.29653242230415344
train gradient:  0.15195517412756424
iteration : 9058
train acc:  0.8046875
train loss:  0.40043848752975464
train gradient:  0.17424177536658234
iteration : 9059
train acc:  0.8515625
train loss:  0.37036558985710144
train gradient:  0.1397514875016533
iteration : 9060
train acc:  0.84375
train loss:  0.36060672998428345
train gradient:  0.1916212004600819
iteration : 9061
train acc:  0.875
train loss:  0.24773429334163666
train gradient:  0.11761050611872134
iteration : 9062
train acc:  0.890625
train loss:  0.2809171676635742
train gradient:  0.1327537325682939
iteration : 9063
train acc:  0.8515625
train loss:  0.348937064409256
train gradient:  0.14919866302683074
iteration : 9064
train acc:  0.90625
train loss:  0.2975730895996094
train gradient:  0.15533377122363248
iteration : 9065
train acc:  0.7734375
train loss:  0.4162362217903137
train gradient:  0.21410423469726017
iteration : 9066
train acc:  0.828125
train loss:  0.3151179254055023
train gradient:  0.11372729272616858
iteration : 9067
train acc:  0.875
train loss:  0.25463372468948364
train gradient:  0.11848140979488175
iteration : 9068
train acc:  0.8828125
train loss:  0.29158225655555725
train gradient:  0.13035864964439445
iteration : 9069
train acc:  0.890625
train loss:  0.23044705390930176
train gradient:  0.14572437083382894
iteration : 9070
train acc:  0.8359375
train loss:  0.3511926829814911
train gradient:  0.1669022656467251
iteration : 9071
train acc:  0.9140625
train loss:  0.25422435998916626
train gradient:  0.11317157267667172
iteration : 9072
train acc:  0.8515625
train loss:  0.4054844379425049
train gradient:  0.33319171952935633
iteration : 9073
train acc:  0.875
train loss:  0.29221460223197937
train gradient:  0.11220311306677011
iteration : 9074
train acc:  0.7890625
train loss:  0.4249354600906372
train gradient:  0.22208955031969135
iteration : 9075
train acc:  0.8828125
train loss:  0.30020958185195923
train gradient:  0.13064225421555242
iteration : 9076
train acc:  0.875
train loss:  0.27166277170181274
train gradient:  0.21587225956405534
iteration : 9077
train acc:  0.828125
train loss:  0.3834645748138428
train gradient:  0.21838496330475957
iteration : 9078
train acc:  0.8828125
train loss:  0.2522238790988922
train gradient:  0.12830224771490956
iteration : 9079
train acc:  0.875
train loss:  0.2743784189224243
train gradient:  0.15037698500135063
iteration : 9080
train acc:  0.8515625
train loss:  0.34879690408706665
train gradient:  0.14702355328487957
iteration : 9081
train acc:  0.8046875
train loss:  0.41281944513320923
train gradient:  0.28075998685906595
iteration : 9082
train acc:  0.8828125
train loss:  0.2905080318450928
train gradient:  0.1391201917803962
iteration : 9083
train acc:  0.8671875
train loss:  0.3356075882911682
train gradient:  0.149392219039378
iteration : 9084
train acc:  0.875
train loss:  0.3286914825439453
train gradient:  0.1156432119983417
iteration : 9085
train acc:  0.8203125
train loss:  0.40315741300582886
train gradient:  0.28983712948806845
iteration : 9086
train acc:  0.859375
train loss:  0.32104939222335815
train gradient:  0.17585815454020598
iteration : 9087
train acc:  0.875
train loss:  0.3276931643486023
train gradient:  0.17257517081775964
iteration : 9088
train acc:  0.8828125
train loss:  0.24918711185455322
train gradient:  0.10963609801723392
iteration : 9089
train acc:  0.8984375
train loss:  0.26171639561653137
train gradient:  0.11937614625951262
iteration : 9090
train acc:  0.8125
train loss:  0.40868741273880005
train gradient:  0.24567423791463236
iteration : 9091
train acc:  0.890625
train loss:  0.2878146767616272
train gradient:  0.14566614784238632
iteration : 9092
train acc:  0.8359375
train loss:  0.31101852655410767
train gradient:  0.1882957438288178
iteration : 9093
train acc:  0.859375
train loss:  0.3155403137207031
train gradient:  0.20843525573325847
iteration : 9094
train acc:  0.78125
train loss:  0.4638347625732422
train gradient:  0.39923761637558813
iteration : 9095
train acc:  0.8671875
train loss:  0.3624347150325775
train gradient:  0.16843699180110636
iteration : 9096
train acc:  0.875
train loss:  0.25827401876449585
train gradient:  0.11162576873018687
iteration : 9097
train acc:  0.890625
train loss:  0.24050003290176392
train gradient:  0.11796136038384131
iteration : 9098
train acc:  0.9140625
train loss:  0.2218792736530304
train gradient:  0.11557493030649937
iteration : 9099
train acc:  0.8671875
train loss:  0.3001541793346405
train gradient:  0.19886115639525478
iteration : 9100
train acc:  0.8046875
train loss:  0.39950042963027954
train gradient:  0.2496668851699788
iteration : 9101
train acc:  0.7734375
train loss:  0.48997247219085693
train gradient:  0.34996022488279505
iteration : 9102
train acc:  0.8203125
train loss:  0.3339543044567108
train gradient:  0.16853887937432677
iteration : 9103
train acc:  0.8359375
train loss:  0.38692256808280945
train gradient:  0.2381121136117238
iteration : 9104
train acc:  0.875
train loss:  0.2804667353630066
train gradient:  0.09737367314434264
iteration : 9105
train acc:  0.859375
train loss:  0.3752068281173706
train gradient:  0.1899281575407919
iteration : 9106
train acc:  0.828125
train loss:  0.3848683536052704
train gradient:  0.20946669666831086
iteration : 9107
train acc:  0.859375
train loss:  0.34667903184890747
train gradient:  0.21261219541320833
iteration : 9108
train acc:  0.9140625
train loss:  0.2630632519721985
train gradient:  0.1730500272998683
iteration : 9109
train acc:  0.84375
train loss:  0.3688028156757355
train gradient:  0.1797345447853228
iteration : 9110
train acc:  0.875
train loss:  0.2759855091571808
train gradient:  0.16979537447296222
iteration : 9111
train acc:  0.8515625
train loss:  0.2975686490535736
train gradient:  0.13386092239544992
iteration : 9112
train acc:  0.8125
train loss:  0.4528971016407013
train gradient:  0.25171118337042664
iteration : 9113
train acc:  0.890625
train loss:  0.2852960526943207
train gradient:  0.13189359520886976
iteration : 9114
train acc:  0.7890625
train loss:  0.4150699973106384
train gradient:  0.28100370198062435
iteration : 9115
train acc:  0.84375
train loss:  0.30833542346954346
train gradient:  0.18520262202163984
iteration : 9116
train acc:  0.8125
train loss:  0.403654545545578
train gradient:  0.3280958871202244
iteration : 9117
train acc:  0.890625
train loss:  0.2737008333206177
train gradient:  0.17349719693444443
iteration : 9118
train acc:  0.8203125
train loss:  0.4008414149284363
train gradient:  0.24224822680934954
iteration : 9119
train acc:  0.9140625
train loss:  0.2412497103214264
train gradient:  0.1120787958230857
iteration : 9120
train acc:  0.875
train loss:  0.31262779235839844
train gradient:  0.13886470558705485
iteration : 9121
train acc:  0.8828125
train loss:  0.3323800563812256
train gradient:  0.25348493359255
iteration : 9122
train acc:  0.8203125
train loss:  0.3388305604457855
train gradient:  0.14022162616390643
iteration : 9123
train acc:  0.9140625
train loss:  0.26440727710723877
train gradient:  0.12123384376086425
iteration : 9124
train acc:  0.8515625
train loss:  0.3032715618610382
train gradient:  0.17201565154573056
iteration : 9125
train acc:  0.8515625
train loss:  0.3106726408004761
train gradient:  0.18903067004773788
iteration : 9126
train acc:  0.8046875
train loss:  0.44607704877853394
train gradient:  0.23597276868580697
iteration : 9127
train acc:  0.8359375
train loss:  0.36493709683418274
train gradient:  0.204190763307145
iteration : 9128
train acc:  0.8359375
train loss:  0.3714010417461395
train gradient:  0.1646811429217585
iteration : 9129
train acc:  0.8828125
train loss:  0.29229500889778137
train gradient:  0.16160723504044444
iteration : 9130
train acc:  0.8671875
train loss:  0.34563183784484863
train gradient:  0.14588184768714105
iteration : 9131
train acc:  0.859375
train loss:  0.34547099471092224
train gradient:  0.13315777754343977
iteration : 9132
train acc:  0.8359375
train loss:  0.35119473934173584
train gradient:  0.16204218581401614
iteration : 9133
train acc:  0.8046875
train loss:  0.40671998262405396
train gradient:  0.30610075281154503
iteration : 9134
train acc:  0.8671875
train loss:  0.2953451871871948
train gradient:  0.18011314247350962
iteration : 9135
train acc:  0.8359375
train loss:  0.37844306230545044
train gradient:  0.21173991516325158
iteration : 9136
train acc:  0.8671875
train loss:  0.3150392174720764
train gradient:  0.17711531643117318
iteration : 9137
train acc:  0.828125
train loss:  0.3830243945121765
train gradient:  0.1728284089696543
iteration : 9138
train acc:  0.90625
train loss:  0.25801658630371094
train gradient:  0.11409500579551059
iteration : 9139
train acc:  0.8671875
train loss:  0.3029077649116516
train gradient:  0.14655654823717437
iteration : 9140
train acc:  0.8984375
train loss:  0.2806045114994049
train gradient:  0.11127484785392845
iteration : 9141
train acc:  0.921875
train loss:  0.28661274909973145
train gradient:  0.1291867928849617
iteration : 9142
train acc:  0.84375
train loss:  0.3952457308769226
train gradient:  0.2074669846838053
iteration : 9143
train acc:  0.8671875
train loss:  0.3222966194152832
train gradient:  0.13965747491624608
iteration : 9144
train acc:  0.8515625
train loss:  0.33499956130981445
train gradient:  0.12689439589618245
iteration : 9145
train acc:  0.875
train loss:  0.28322774171829224
train gradient:  0.1735245534139876
iteration : 9146
train acc:  0.84375
train loss:  0.3962635099887848
train gradient:  0.21481666416297684
iteration : 9147
train acc:  0.8359375
train loss:  0.3864699602127075
train gradient:  0.19894871117762203
iteration : 9148
train acc:  0.84375
train loss:  0.32093024253845215
train gradient:  0.16067104429589016
iteration : 9149
train acc:  0.796875
train loss:  0.3650808036327362
train gradient:  0.20312642514101062
iteration : 9150
train acc:  0.90625
train loss:  0.25507569313049316
train gradient:  0.12099310491694568
iteration : 9151
train acc:  0.8515625
train loss:  0.36154723167419434
train gradient:  0.24458363072025996
iteration : 9152
train acc:  0.8828125
train loss:  0.28687894344329834
train gradient:  0.1276139836189452
iteration : 9153
train acc:  0.8984375
train loss:  0.27598273754119873
train gradient:  0.13444732702264978
iteration : 9154
train acc:  0.828125
train loss:  0.329612672328949
train gradient:  0.1657585079011325
iteration : 9155
train acc:  0.8359375
train loss:  0.3635733425617218
train gradient:  0.17379181144735467
iteration : 9156
train acc:  0.78125
train loss:  0.41478070616722107
train gradient:  0.235018254725759
iteration : 9157
train acc:  0.8359375
train loss:  0.40245959162712097
train gradient:  0.24366908961216122
iteration : 9158
train acc:  0.859375
train loss:  0.288847416639328
train gradient:  0.13682093289206043
iteration : 9159
train acc:  0.8515625
train loss:  0.34152424335479736
train gradient:  0.15014904966490586
iteration : 9160
train acc:  0.8671875
train loss:  0.3257874548435211
train gradient:  0.1608729031569851
iteration : 9161
train acc:  0.8203125
train loss:  0.35641366243362427
train gradient:  0.24224363478343694
iteration : 9162
train acc:  0.8515625
train loss:  0.3214801549911499
train gradient:  0.12338344693249258
iteration : 9163
train acc:  0.859375
train loss:  0.32122665643692017
train gradient:  0.20458169825132735
iteration : 9164
train acc:  0.875
train loss:  0.2851831316947937
train gradient:  0.14282107339238193
iteration : 9165
train acc:  0.875
train loss:  0.3227153420448303
train gradient:  0.13613260068653943
iteration : 9166
train acc:  0.8671875
train loss:  0.3583340048789978
train gradient:  0.18016379123911128
iteration : 9167
train acc:  0.8125
train loss:  0.3816533088684082
train gradient:  0.1896816014248363
iteration : 9168
train acc:  0.828125
train loss:  0.3501715660095215
train gradient:  0.3006043574634063
iteration : 9169
train acc:  0.8828125
train loss:  0.30672213435173035
train gradient:  0.16992532223055593
iteration : 9170
train acc:  0.8515625
train loss:  0.390781968832016
train gradient:  0.2779099866113147
iteration : 9171
train acc:  0.765625
train loss:  0.4229586720466614
train gradient:  0.2247003963644193
iteration : 9172
train acc:  0.84375
train loss:  0.36230358481407166
train gradient:  0.184556393720332
iteration : 9173
train acc:  0.84375
train loss:  0.4056944251060486
train gradient:  0.2470495715811823
iteration : 9174
train acc:  0.8515625
train loss:  0.33045628666877747
train gradient:  0.1854457962631018
iteration : 9175
train acc:  0.8359375
train loss:  0.413274347782135
train gradient:  0.2003465925741053
iteration : 9176
train acc:  0.828125
train loss:  0.35795336961746216
train gradient:  0.2239892924669679
iteration : 9177
train acc:  0.8984375
train loss:  0.29293107986450195
train gradient:  0.15204557849657968
iteration : 9178
train acc:  0.8828125
train loss:  0.3080627918243408
train gradient:  0.14286439188249572
iteration : 9179
train acc:  0.8515625
train loss:  0.31212306022644043
train gradient:  0.14547987099756027
iteration : 9180
train acc:  0.8515625
train loss:  0.32695117592811584
train gradient:  0.14553205988533974
iteration : 9181
train acc:  0.859375
train loss:  0.33170658349990845
train gradient:  0.17664514237936235
iteration : 9182
train acc:  0.84375
train loss:  0.3510003685951233
train gradient:  0.2282357451625467
iteration : 9183
train acc:  0.875
train loss:  0.2980595529079437
train gradient:  0.14749820737984232
iteration : 9184
train acc:  0.8046875
train loss:  0.4104488790035248
train gradient:  0.24801922422226208
iteration : 9185
train acc:  0.890625
train loss:  0.3153006434440613
train gradient:  0.1656977729873406
iteration : 9186
train acc:  0.859375
train loss:  0.27365925908088684
train gradient:  0.1696206345735302
iteration : 9187
train acc:  0.8828125
train loss:  0.3032377064228058
train gradient:  0.14252786470157328
iteration : 9188
train acc:  0.8671875
train loss:  0.3106091618537903
train gradient:  0.12165286721654034
iteration : 9189
train acc:  0.828125
train loss:  0.3474878966808319
train gradient:  0.16851830339994267
iteration : 9190
train acc:  0.875
train loss:  0.3485044538974762
train gradient:  0.15303912930318117
iteration : 9191
train acc:  0.84375
train loss:  0.33791500329971313
train gradient:  0.1570287222845991
iteration : 9192
train acc:  0.8671875
train loss:  0.26279252767562866
train gradient:  0.13345450840994005
iteration : 9193
train acc:  0.8359375
train loss:  0.35184958577156067
train gradient:  0.16667751812084064
iteration : 9194
train acc:  0.8984375
train loss:  0.2674303650856018
train gradient:  0.13419024861116552
iteration : 9195
train acc:  0.875
train loss:  0.3293223977088928
train gradient:  0.16135515242578935
iteration : 9196
train acc:  0.8671875
train loss:  0.2665287256240845
train gradient:  0.12558600269552267
iteration : 9197
train acc:  0.8671875
train loss:  0.2978594899177551
train gradient:  0.19744369725807218
iteration : 9198
train acc:  0.8203125
train loss:  0.4134003520011902
train gradient:  0.20546656671565375
iteration : 9199
train acc:  0.890625
train loss:  0.2705525755882263
train gradient:  0.10620301261896471
iteration : 9200
train acc:  0.890625
train loss:  0.2604585289955139
train gradient:  0.11979884798784553
iteration : 9201
train acc:  0.828125
train loss:  0.38363975286483765
train gradient:  0.19111276555010037
iteration : 9202
train acc:  0.8828125
train loss:  0.3034796416759491
train gradient:  0.18621764813559644
iteration : 9203
train acc:  0.8984375
train loss:  0.231705904006958
train gradient:  0.10733552661981906
iteration : 9204
train acc:  0.8359375
train loss:  0.3457792401313782
train gradient:  0.2152645937232917
iteration : 9205
train acc:  0.8984375
train loss:  0.27647891640663147
train gradient:  0.14998300408780377
iteration : 9206
train acc:  0.8828125
train loss:  0.30992239713668823
train gradient:  0.1482671841808766
iteration : 9207
train acc:  0.84375
train loss:  0.3034486770629883
train gradient:  0.14403033421259281
iteration : 9208
train acc:  0.828125
train loss:  0.3903740644454956
train gradient:  0.28375911746416327
iteration : 9209
train acc:  0.8828125
train loss:  0.37391334772109985
train gradient:  0.18289943561643754
iteration : 9210
train acc:  0.8515625
train loss:  0.32684653997421265
train gradient:  0.1954253589065395
iteration : 9211
train acc:  0.90625
train loss:  0.27163705229759216
train gradient:  0.14958661490018887
iteration : 9212
train acc:  0.859375
train loss:  0.3762779235839844
train gradient:  0.18073321497599315
iteration : 9213
train acc:  0.859375
train loss:  0.3768841028213501
train gradient:  0.23381080084647277
iteration : 9214
train acc:  0.890625
train loss:  0.28419211506843567
train gradient:  0.1462383842342872
iteration : 9215
train acc:  0.875
train loss:  0.2782825827598572
train gradient:  0.16144177259605857
iteration : 9216
train acc:  0.84375
train loss:  0.31645023822784424
train gradient:  0.20762074200091518
iteration : 9217
train acc:  0.8515625
train loss:  0.347624808549881
train gradient:  0.19977471140382447
iteration : 9218
train acc:  0.8203125
train loss:  0.36157435178756714
train gradient:  0.18121998208092888
iteration : 9219
train acc:  0.828125
train loss:  0.3595241904258728
train gradient:  0.20996880303224846
iteration : 9220
train acc:  0.890625
train loss:  0.37473180890083313
train gradient:  0.1875490909356684
iteration : 9221
train acc:  0.859375
train loss:  0.3403434753417969
train gradient:  0.176601538587707
iteration : 9222
train acc:  0.8515625
train loss:  0.30766820907592773
train gradient:  0.1320915366974416
iteration : 9223
train acc:  0.890625
train loss:  0.2935057282447815
train gradient:  0.214413312150302
iteration : 9224
train acc:  0.859375
train loss:  0.3724013566970825
train gradient:  0.22232489315447473
iteration : 9225
train acc:  0.8046875
train loss:  0.4220632314682007
train gradient:  0.2628416224511038
iteration : 9226
train acc:  0.8671875
train loss:  0.29497435688972473
train gradient:  0.15104881652313407
iteration : 9227
train acc:  0.8984375
train loss:  0.23551689088344574
train gradient:  0.1401715132485063
iteration : 9228
train acc:  0.859375
train loss:  0.3434547185897827
train gradient:  0.20821825148058404
iteration : 9229
train acc:  0.828125
train loss:  0.35289809107780457
train gradient:  0.1949555608819241
iteration : 9230
train acc:  0.84375
train loss:  0.34057319164276123
train gradient:  0.18607166679316756
iteration : 9231
train acc:  0.84375
train loss:  0.4266756772994995
train gradient:  0.28589326584144154
iteration : 9232
train acc:  0.8515625
train loss:  0.37079447507858276
train gradient:  0.25983017909690753
iteration : 9233
train acc:  0.8671875
train loss:  0.34416598081588745
train gradient:  0.1900029568577485
iteration : 9234
train acc:  0.8203125
train loss:  0.3770945966243744
train gradient:  0.18146141588221065
iteration : 9235
train acc:  0.84375
train loss:  0.319669634103775
train gradient:  0.18922226401526737
iteration : 9236
train acc:  0.84375
train loss:  0.3287471532821655
train gradient:  0.24251202072812456
iteration : 9237
train acc:  0.859375
train loss:  0.3200240135192871
train gradient:  0.12138191128332348
iteration : 9238
train acc:  0.8203125
train loss:  0.3830369710922241
train gradient:  0.2349083977113308
iteration : 9239
train acc:  0.8515625
train loss:  0.3153953552246094
train gradient:  0.16306563863203782
iteration : 9240
train acc:  0.859375
train loss:  0.2654802203178406
train gradient:  0.24018038530675753
iteration : 9241
train acc:  0.875
train loss:  0.3203721046447754
train gradient:  0.15630087263598527
iteration : 9242
train acc:  0.8671875
train loss:  0.3422276973724365
train gradient:  0.1619603266028039
iteration : 9243
train acc:  0.8359375
train loss:  0.3812273144721985
train gradient:  0.19340406792730053
iteration : 9244
train acc:  0.859375
train loss:  0.32436972856521606
train gradient:  0.23629143886469128
iteration : 9245
train acc:  0.828125
train loss:  0.30311185121536255
train gradient:  0.18709553231101106
iteration : 9246
train acc:  0.796875
train loss:  0.3512992858886719
train gradient:  0.23049215701352493
iteration : 9247
train acc:  0.859375
train loss:  0.3633323907852173
train gradient:  0.21530357939653588
iteration : 9248
train acc:  0.8046875
train loss:  0.38262900710105896
train gradient:  0.21854339417661295
iteration : 9249
train acc:  0.8984375
train loss:  0.233674094080925
train gradient:  0.11054090256291504
iteration : 9250
train acc:  0.890625
train loss:  0.25197455286979675
train gradient:  0.13648741682308768
iteration : 9251
train acc:  0.828125
train loss:  0.4089409112930298
train gradient:  0.26795080378251096
iteration : 9252
train acc:  0.8828125
train loss:  0.2654699683189392
train gradient:  0.14211380531804113
iteration : 9253
train acc:  0.890625
train loss:  0.30493199825286865
train gradient:  0.15111011710124045
iteration : 9254
train acc:  0.8515625
train loss:  0.35089346766471863
train gradient:  0.153170910255799
iteration : 9255
train acc:  0.859375
train loss:  0.3228100538253784
train gradient:  0.14481347454630275
iteration : 9256
train acc:  0.8359375
train loss:  0.30224186182022095
train gradient:  0.13019816386133692
iteration : 9257
train acc:  0.796875
train loss:  0.3677060008049011
train gradient:  0.19644440753806847
iteration : 9258
train acc:  0.875
train loss:  0.31916284561157227
train gradient:  0.13013528455130502
iteration : 9259
train acc:  0.8203125
train loss:  0.36450111865997314
train gradient:  0.18010018296424937
iteration : 9260
train acc:  0.828125
train loss:  0.34073859453201294
train gradient:  0.12869147027172131
iteration : 9261
train acc:  0.8203125
train loss:  0.40913885831832886
train gradient:  0.32489419057270286
iteration : 9262
train acc:  0.8359375
train loss:  0.3855237364768982
train gradient:  0.23998613210930775
iteration : 9263
train acc:  0.8671875
train loss:  0.350253701210022
train gradient:  0.15865711006015526
iteration : 9264
train acc:  0.8203125
train loss:  0.40537071228027344
train gradient:  0.4687062523628238
iteration : 9265
train acc:  0.8203125
train loss:  0.4340845048427582
train gradient:  0.22201355169836096
iteration : 9266
train acc:  0.8046875
train loss:  0.38297849893569946
train gradient:  0.17769095022637107
iteration : 9267
train acc:  0.84375
train loss:  0.3324688673019409
train gradient:  0.24596393036093583
iteration : 9268
train acc:  0.8125
train loss:  0.3935099244117737
train gradient:  0.2464492105475277
iteration : 9269
train acc:  0.84375
train loss:  0.3124162554740906
train gradient:  0.13912341711346832
iteration : 9270
train acc:  0.859375
train loss:  0.3241073191165924
train gradient:  0.17809193810002905
iteration : 9271
train acc:  0.828125
train loss:  0.3424645662307739
train gradient:  0.2312735898006299
iteration : 9272
train acc:  0.84375
train loss:  0.3272891640663147
train gradient:  0.15005683110556972
iteration : 9273
train acc:  0.7890625
train loss:  0.3681575655937195
train gradient:  0.21698967591121238
iteration : 9274
train acc:  0.8984375
train loss:  0.2779081463813782
train gradient:  0.1742393333898885
iteration : 9275
train acc:  0.8203125
train loss:  0.3618553876876831
train gradient:  0.19912101102364124
iteration : 9276
train acc:  0.8515625
train loss:  0.33983004093170166
train gradient:  0.19120213476087655
iteration : 9277
train acc:  0.8984375
train loss:  0.27682816982269287
train gradient:  0.17248934033766927
iteration : 9278
train acc:  0.8515625
train loss:  0.34276750683784485
train gradient:  0.15547346667751136
iteration : 9279
train acc:  0.90625
train loss:  0.2641918659210205
train gradient:  0.10805888176643616
iteration : 9280
train acc:  0.890625
train loss:  0.27417004108428955
train gradient:  0.11414235347794839
iteration : 9281
train acc:  0.859375
train loss:  0.38527315855026245
train gradient:  0.40490510638754706
iteration : 9282
train acc:  0.8515625
train loss:  0.36156195402145386
train gradient:  0.13972230830212065
iteration : 9283
train acc:  0.890625
train loss:  0.28576719760894775
train gradient:  0.178483472336187
iteration : 9284
train acc:  0.8203125
train loss:  0.35921674966812134
train gradient:  0.23705398334237798
iteration : 9285
train acc:  0.8828125
train loss:  0.329684853553772
train gradient:  0.1785892600918588
iteration : 9286
train acc:  0.7578125
train loss:  0.5111981630325317
train gradient:  0.47159921960134854
iteration : 9287
train acc:  0.8671875
train loss:  0.33089983463287354
train gradient:  0.1501754193512309
iteration : 9288
train acc:  0.8671875
train loss:  0.2903386950492859
train gradient:  0.1229919532098453
iteration : 9289
train acc:  0.921875
train loss:  0.2422570437192917
train gradient:  0.10823078958662917
iteration : 9290
train acc:  0.8203125
train loss:  0.37011563777923584
train gradient:  0.20681943026398944
iteration : 9291
train acc:  0.859375
train loss:  0.2893517315387726
train gradient:  0.13386054865634123
iteration : 9292
train acc:  0.828125
train loss:  0.36418771743774414
train gradient:  0.15796102781286897
iteration : 9293
train acc:  0.8203125
train loss:  0.34581446647644043
train gradient:  0.17606243925324025
iteration : 9294
train acc:  0.9140625
train loss:  0.2298833429813385
train gradient:  0.10913247869566622
iteration : 9295
train acc:  0.859375
train loss:  0.31334856152534485
train gradient:  0.13280895296007905
iteration : 9296
train acc:  0.8671875
train loss:  0.295015811920166
train gradient:  0.15957036737204464
iteration : 9297
train acc:  0.890625
train loss:  0.2722972631454468
train gradient:  0.11505861364914716
iteration : 9298
train acc:  0.859375
train loss:  0.29346102476119995
train gradient:  0.2425184710147714
iteration : 9299
train acc:  0.796875
train loss:  0.38883328437805176
train gradient:  0.20357878654379472
iteration : 9300
train acc:  0.875
train loss:  0.31798404455184937
train gradient:  0.16345642471684238
iteration : 9301
train acc:  0.8203125
train loss:  0.3473747968673706
train gradient:  0.17015195273892975
iteration : 9302
train acc:  0.8984375
train loss:  0.2945714592933655
train gradient:  0.22295267311547018
iteration : 9303
train acc:  0.8515625
train loss:  0.3550054430961609
train gradient:  0.15505987892755851
iteration : 9304
train acc:  0.8203125
train loss:  0.402008056640625
train gradient:  0.2490223210426701
iteration : 9305
train acc:  0.78125
train loss:  0.3750118017196655
train gradient:  0.26998153179482537
iteration : 9306
train acc:  0.859375
train loss:  0.25605303049087524
train gradient:  0.12230253281818358
iteration : 9307
train acc:  0.859375
train loss:  0.3865719735622406
train gradient:  0.26554036935102643
iteration : 9308
train acc:  0.875
train loss:  0.2965015769004822
train gradient:  0.15206828883959783
iteration : 9309
train acc:  0.8203125
train loss:  0.34815332293510437
train gradient:  0.22387110786914272
iteration : 9310
train acc:  0.8828125
train loss:  0.2853317856788635
train gradient:  0.16814588710886913
iteration : 9311
train acc:  0.8984375
train loss:  0.26000380516052246
train gradient:  0.10667825297961507
iteration : 9312
train acc:  0.84375
train loss:  0.3676651120185852
train gradient:  0.14816089088771445
iteration : 9313
train acc:  0.890625
train loss:  0.24821126461029053
train gradient:  0.09243852730259426
iteration : 9314
train acc:  0.859375
train loss:  0.27582278847694397
train gradient:  0.15113791924870637
iteration : 9315
train acc:  0.8671875
train loss:  0.3364725708961487
train gradient:  0.2043690412473198
iteration : 9316
train acc:  0.859375
train loss:  0.3425523042678833
train gradient:  0.18288152894425103
iteration : 9317
train acc:  0.875
train loss:  0.3429667353630066
train gradient:  0.21720576121555368
iteration : 9318
train acc:  0.8203125
train loss:  0.39731696248054504
train gradient:  0.26870260407086793
iteration : 9319
train acc:  0.875
train loss:  0.30560100078582764
train gradient:  0.1375147438092275
iteration : 9320
train acc:  0.828125
train loss:  0.39893999695777893
train gradient:  0.22714238561417782
iteration : 9321
train acc:  0.921875
train loss:  0.2487715780735016
train gradient:  0.1307861564435347
iteration : 9322
train acc:  0.90625
train loss:  0.2472093254327774
train gradient:  0.09873162119936732
iteration : 9323
train acc:  0.84375
train loss:  0.3633412718772888
train gradient:  0.20437937512419752
iteration : 9324
train acc:  0.8515625
train loss:  0.30963408946990967
train gradient:  0.18475385038298062
iteration : 9325
train acc:  0.8984375
train loss:  0.2813500761985779
train gradient:  0.14028292287929456
iteration : 9326
train acc:  0.90625
train loss:  0.24027498066425323
train gradient:  0.11529740126358362
iteration : 9327
train acc:  0.78125
train loss:  0.453993022441864
train gradient:  0.2202451828560756
iteration : 9328
train acc:  0.84375
train loss:  0.3653329014778137
train gradient:  0.2212814191486812
iteration : 9329
train acc:  0.84375
train loss:  0.3846975564956665
train gradient:  0.2447073784307711
iteration : 9330
train acc:  0.953125
train loss:  0.23264440894126892
train gradient:  0.11238901527273011
iteration : 9331
train acc:  0.890625
train loss:  0.263461172580719
train gradient:  0.1259870749862944
iteration : 9332
train acc:  0.890625
train loss:  0.3038642406463623
train gradient:  0.150798089871842
iteration : 9333
train acc:  0.859375
train loss:  0.252204954624176
train gradient:  0.14018963565910697
iteration : 9334
train acc:  0.8046875
train loss:  0.42375466227531433
train gradient:  0.28577831307330703
iteration : 9335
train acc:  0.8125
train loss:  0.47365766763687134
train gradient:  0.3214350676306499
iteration : 9336
train acc:  0.9140625
train loss:  0.2551392912864685
train gradient:  0.11242895556610297
iteration : 9337
train acc:  0.8984375
train loss:  0.2772534489631653
train gradient:  0.2300353182096112
iteration : 9338
train acc:  0.875
train loss:  0.3240342140197754
train gradient:  0.1959219238822706
iteration : 9339
train acc:  0.8203125
train loss:  0.414075642824173
train gradient:  0.14648242256723346
iteration : 9340
train acc:  0.84375
train loss:  0.3294180929660797
train gradient:  0.1807282222446272
iteration : 9341
train acc:  0.8046875
train loss:  0.3197351396083832
train gradient:  0.16910823424727375
iteration : 9342
train acc:  0.8203125
train loss:  0.34650343656539917
train gradient:  0.22315642945144476
iteration : 9343
train acc:  0.9140625
train loss:  0.2570244073867798
train gradient:  0.10090949013609976
iteration : 9344
train acc:  0.859375
train loss:  0.27626222372055054
train gradient:  0.13038208776509208
iteration : 9345
train acc:  0.8828125
train loss:  0.3058285117149353
train gradient:  0.15239502469242808
iteration : 9346
train acc:  0.8828125
train loss:  0.33318233489990234
train gradient:  0.21331117017300305
iteration : 9347
train acc:  0.8359375
train loss:  0.29459473490715027
train gradient:  0.14234186722790185
iteration : 9348
train acc:  0.8125
train loss:  0.45171791315078735
train gradient:  0.24603031367523567
iteration : 9349
train acc:  0.8203125
train loss:  0.38997477293014526
train gradient:  0.2958605877422968
iteration : 9350
train acc:  0.8828125
train loss:  0.30277249217033386
train gradient:  0.10081836921388691
iteration : 9351
train acc:  0.875
train loss:  0.27226436138153076
train gradient:  0.12176674283323197
iteration : 9352
train acc:  0.890625
train loss:  0.30713072419166565
train gradient:  0.17772549428938672
iteration : 9353
train acc:  0.8515625
train loss:  0.3358335494995117
train gradient:  0.15629811893252774
iteration : 9354
train acc:  0.84375
train loss:  0.381181001663208
train gradient:  0.21637281865768027
iteration : 9355
train acc:  0.8125
train loss:  0.35205668210983276
train gradient:  0.20921179435313206
iteration : 9356
train acc:  0.8125
train loss:  0.38141971826553345
train gradient:  0.2788847245639297
iteration : 9357
train acc:  0.8671875
train loss:  0.27725672721862793
train gradient:  0.12388046226134224
iteration : 9358
train acc:  0.8828125
train loss:  0.26977652311325073
train gradient:  0.14869634812347743
iteration : 9359
train acc:  0.8515625
train loss:  0.29451873898506165
train gradient:  0.1022631773966486
iteration : 9360
train acc:  0.8828125
train loss:  0.29432207345962524
train gradient:  0.2544359454002017
iteration : 9361
train acc:  0.8828125
train loss:  0.2982880175113678
train gradient:  0.12180076522265637
iteration : 9362
train acc:  0.84375
train loss:  0.2872863709926605
train gradient:  0.13111680626504219
iteration : 9363
train acc:  0.890625
train loss:  0.2646090090274811
train gradient:  0.14008825037552344
iteration : 9364
train acc:  0.890625
train loss:  0.25911223888397217
train gradient:  0.09629897943293962
iteration : 9365
train acc:  0.84375
train loss:  0.3524266183376312
train gradient:  0.18650113699089974
iteration : 9366
train acc:  0.8359375
train loss:  0.3073872923851013
train gradient:  0.11162845876087947
iteration : 9367
train acc:  0.8359375
train loss:  0.348008394241333
train gradient:  0.15350511515490117
iteration : 9368
train acc:  0.828125
train loss:  0.3663868308067322
train gradient:  0.16687993462609185
iteration : 9369
train acc:  0.8828125
train loss:  0.287556916475296
train gradient:  0.15103598336346313
iteration : 9370
train acc:  0.8125
train loss:  0.40263357758522034
train gradient:  0.164361561441854
iteration : 9371
train acc:  0.8671875
train loss:  0.3109281659126282
train gradient:  0.15999489695330155
iteration : 9372
train acc:  0.875
train loss:  0.2893662750720978
train gradient:  0.13940645192098622
iteration : 9373
train acc:  0.890625
train loss:  0.2636858820915222
train gradient:  0.14107804659753514
iteration : 9374
train acc:  0.8515625
train loss:  0.2979896068572998
train gradient:  0.2177217819618682
iteration : 9375
train acc:  0.8359375
train loss:  0.3585018515586853
train gradient:  0.21952142733221452
iteration : 9376
train acc:  0.8515625
train loss:  0.3886089324951172
train gradient:  0.17635604260583337
iteration : 9377
train acc:  0.8125
train loss:  0.40754765272140503
train gradient:  0.2869399234588963
iteration : 9378
train acc:  0.8203125
train loss:  0.3830164968967438
train gradient:  0.20116011629384362
iteration : 9379
train acc:  0.8359375
train loss:  0.3224135637283325
train gradient:  0.14271938814565327
iteration : 9380
train acc:  0.828125
train loss:  0.3126753568649292
train gradient:  0.21027471254827249
iteration : 9381
train acc:  0.8828125
train loss:  0.3011023998260498
train gradient:  0.1493251244433465
iteration : 9382
train acc:  0.890625
train loss:  0.305806428194046
train gradient:  0.1061022693063519
iteration : 9383
train acc:  0.875
train loss:  0.30223697423934937
train gradient:  0.10658478947106104
iteration : 9384
train acc:  0.8203125
train loss:  0.4099818170070648
train gradient:  0.2705726665479035
iteration : 9385
train acc:  0.8359375
train loss:  0.36546391248703003
train gradient:  0.15958817904182596
iteration : 9386
train acc:  0.796875
train loss:  0.45721614360809326
train gradient:  0.24474060974887885
iteration : 9387
train acc:  0.875
train loss:  0.2947981357574463
train gradient:  0.1380812783925759
iteration : 9388
train acc:  0.8515625
train loss:  0.4209662079811096
train gradient:  0.21896051911856984
iteration : 9389
train acc:  0.8203125
train loss:  0.4280822277069092
train gradient:  0.2630820653955366
iteration : 9390
train acc:  0.890625
train loss:  0.2890324592590332
train gradient:  0.11123477755529809
iteration : 9391
train acc:  0.8359375
train loss:  0.3717532753944397
train gradient:  0.21760219460606478
iteration : 9392
train acc:  0.859375
train loss:  0.2646164894104004
train gradient:  0.11664720064512735
iteration : 9393
train acc:  0.8125
train loss:  0.4314793348312378
train gradient:  0.3575861602187699
iteration : 9394
train acc:  0.84375
train loss:  0.3457457721233368
train gradient:  0.2448672039313896
iteration : 9395
train acc:  0.8671875
train loss:  0.3556347191333771
train gradient:  0.1791208663068527
iteration : 9396
train acc:  0.84375
train loss:  0.3096122741699219
train gradient:  0.12324202165792049
iteration : 9397
train acc:  0.796875
train loss:  0.4401571750640869
train gradient:  0.19460277471470122
iteration : 9398
train acc:  0.8515625
train loss:  0.3177802562713623
train gradient:  0.2070728366250736
iteration : 9399
train acc:  0.8125
train loss:  0.38738778233528137
train gradient:  0.20220167330048722
iteration : 9400
train acc:  0.890625
train loss:  0.2729256749153137
train gradient:  0.11406510290682972
iteration : 9401
train acc:  0.8828125
train loss:  0.24574753642082214
train gradient:  0.14464078737491382
iteration : 9402
train acc:  0.859375
train loss:  0.2986946702003479
train gradient:  0.18173225109516028
iteration : 9403
train acc:  0.8828125
train loss:  0.3310060501098633
train gradient:  0.1583110061154797
iteration : 9404
train acc:  0.84375
train loss:  0.3426140546798706
train gradient:  0.16765987044575828
iteration : 9405
train acc:  0.8359375
train loss:  0.3273632526397705
train gradient:  0.20033092372563865
iteration : 9406
train acc:  0.8515625
train loss:  0.28692567348480225
train gradient:  0.16768481042530586
iteration : 9407
train acc:  0.8671875
train loss:  0.2988608479499817
train gradient:  0.11454900870354374
iteration : 9408
train acc:  0.8046875
train loss:  0.41045865416526794
train gradient:  0.341105481460443
iteration : 9409
train acc:  0.875
train loss:  0.332552433013916
train gradient:  0.14361789008205084
iteration : 9410
train acc:  0.8671875
train loss:  0.39221757650375366
train gradient:  0.1778380361727718
iteration : 9411
train acc:  0.875
train loss:  0.31876760721206665
train gradient:  0.1809533003376264
iteration : 9412
train acc:  0.828125
train loss:  0.3288094103336334
train gradient:  0.14060599721250244
iteration : 9413
train acc:  0.875
train loss:  0.3470223546028137
train gradient:  0.16720579180876338
iteration : 9414
train acc:  0.90625
train loss:  0.2515947222709656
train gradient:  0.09434046368002308
iteration : 9415
train acc:  0.8671875
train loss:  0.34643369913101196
train gradient:  0.1414832362572542
iteration : 9416
train acc:  0.84375
train loss:  0.30033594369888306
train gradient:  0.12593858941079844
iteration : 9417
train acc:  0.84375
train loss:  0.4016391634941101
train gradient:  0.20232936447620006
iteration : 9418
train acc:  0.890625
train loss:  0.3188053071498871
train gradient:  0.1593686783572077
iteration : 9419
train acc:  0.828125
train loss:  0.34859615564346313
train gradient:  0.1434022464493454
iteration : 9420
train acc:  0.921875
train loss:  0.2619585692882538
train gradient:  0.10142907129818658
iteration : 9421
train acc:  0.875
train loss:  0.2986256182193756
train gradient:  0.18079059557268562
iteration : 9422
train acc:  0.8984375
train loss:  0.25106874108314514
train gradient:  0.08778993431699351
iteration : 9423
train acc:  0.890625
train loss:  0.2841029763221741
train gradient:  0.18345067308887958
iteration : 9424
train acc:  0.9140625
train loss:  0.27716881036758423
train gradient:  0.09230715196213407
iteration : 9425
train acc:  0.8984375
train loss:  0.3087090849876404
train gradient:  0.13135515582417
iteration : 9426
train acc:  0.84375
train loss:  0.2986619770526886
train gradient:  0.2007254583271648
iteration : 9427
train acc:  0.796875
train loss:  0.3804579973220825
train gradient:  0.16443205371746056
iteration : 9428
train acc:  0.8828125
train loss:  0.2985374927520752
train gradient:  0.12240563433795781
iteration : 9429
train acc:  0.8515625
train loss:  0.3064408600330353
train gradient:  0.14433464493468234
iteration : 9430
train acc:  0.875
train loss:  0.3666725158691406
train gradient:  0.21637343319294217
iteration : 9431
train acc:  0.828125
train loss:  0.44181621074676514
train gradient:  0.31395852447714245
iteration : 9432
train acc:  0.828125
train loss:  0.388197124004364
train gradient:  0.2457618784678554
iteration : 9433
train acc:  0.84375
train loss:  0.34833699464797974
train gradient:  0.24131970272550013
iteration : 9434
train acc:  0.8828125
train loss:  0.3081233501434326
train gradient:  0.141237830243871
iteration : 9435
train acc:  0.84375
train loss:  0.3850635588169098
train gradient:  0.17780292866418856
iteration : 9436
train acc:  0.8046875
train loss:  0.33297502994537354
train gradient:  0.15495377276048922
iteration : 9437
train acc:  0.875
train loss:  0.33344268798828125
train gradient:  0.15562046029193935
iteration : 9438
train acc:  0.8671875
train loss:  0.30539268255233765
train gradient:  0.1540803829329359
iteration : 9439
train acc:  0.875
train loss:  0.2997487187385559
train gradient:  0.1774583687120785
iteration : 9440
train acc:  0.875
train loss:  0.33181384205818176
train gradient:  0.15200916037328704
iteration : 9441
train acc:  0.8203125
train loss:  0.33918869495391846
train gradient:  0.18592384237142687
iteration : 9442
train acc:  0.8515625
train loss:  0.3099987506866455
train gradient:  0.13905974765219506
iteration : 9443
train acc:  0.796875
train loss:  0.3751658797264099
train gradient:  0.1854407768577694
iteration : 9444
train acc:  0.8359375
train loss:  0.3370073437690735
train gradient:  0.15977711746821466
iteration : 9445
train acc:  0.8828125
train loss:  0.344302237033844
train gradient:  0.18928190321059407
iteration : 9446
train acc:  0.8515625
train loss:  0.3161173164844513
train gradient:  0.13376151018842758
iteration : 9447
train acc:  0.8671875
train loss:  0.28166308999061584
train gradient:  0.14287503896139425
iteration : 9448
train acc:  0.8203125
train loss:  0.40666908025741577
train gradient:  0.2567993420278269
iteration : 9449
train acc:  0.890625
train loss:  0.31899720430374146
train gradient:  0.17277397899674235
iteration : 9450
train acc:  0.8828125
train loss:  0.268261194229126
train gradient:  0.13072505423658587
iteration : 9451
train acc:  0.828125
train loss:  0.3127044439315796
train gradient:  0.13546124415664107
iteration : 9452
train acc:  0.859375
train loss:  0.36953914165496826
train gradient:  0.14930652338615624
iteration : 9453
train acc:  0.828125
train loss:  0.341940313577652
train gradient:  0.23719473588729542
iteration : 9454
train acc:  0.8359375
train loss:  0.3409309387207031
train gradient:  0.18376100527118627
iteration : 9455
train acc:  0.8359375
train loss:  0.3169253170490265
train gradient:  0.17401712490266336
iteration : 9456
train acc:  0.859375
train loss:  0.3811059594154358
train gradient:  0.1930025911226757
iteration : 9457
train acc:  0.8984375
train loss:  0.26566702127456665
train gradient:  0.09171677222079185
iteration : 9458
train acc:  0.859375
train loss:  0.38940954208374023
train gradient:  0.2366241042348453
iteration : 9459
train acc:  0.8203125
train loss:  0.41605016589164734
train gradient:  0.20797418217271985
iteration : 9460
train acc:  0.84375
train loss:  0.4109385013580322
train gradient:  0.22685241852442983
iteration : 9461
train acc:  0.875
train loss:  0.30513226985931396
train gradient:  0.13244749588287943
iteration : 9462
train acc:  0.796875
train loss:  0.37813758850097656
train gradient:  0.21435070949974577
iteration : 9463
train acc:  0.8046875
train loss:  0.3671761155128479
train gradient:  0.16886302475096396
iteration : 9464
train acc:  0.8203125
train loss:  0.3350379467010498
train gradient:  0.1479667527223385
iteration : 9465
train acc:  0.796875
train loss:  0.41665422916412354
train gradient:  0.2330157190670399
iteration : 9466
train acc:  0.8671875
train loss:  0.27519655227661133
train gradient:  0.12491559682007448
iteration : 9467
train acc:  0.796875
train loss:  0.37023064494132996
train gradient:  0.15963234927441178
iteration : 9468
train acc:  0.828125
train loss:  0.3718884289264679
train gradient:  0.2691680275532072
iteration : 9469
train acc:  0.8359375
train loss:  0.3160727322101593
train gradient:  0.15754945136632825
iteration : 9470
train acc:  0.8671875
train loss:  0.272367924451828
train gradient:  0.13106455089828734
iteration : 9471
train acc:  0.8203125
train loss:  0.3849541246891022
train gradient:  0.17639527414755
iteration : 9472
train acc:  0.875
train loss:  0.3141497075557709
train gradient:  0.24161095800467958
iteration : 9473
train acc:  0.8515625
train loss:  0.30721959471702576
train gradient:  0.11792997348577702
iteration : 9474
train acc:  0.859375
train loss:  0.3444809317588806
train gradient:  0.1423793401531463
iteration : 9475
train acc:  0.8203125
train loss:  0.3223305940628052
train gradient:  0.11636423533349574
iteration : 9476
train acc:  0.859375
train loss:  0.37089741230010986
train gradient:  0.15264078723287994
iteration : 9477
train acc:  0.84375
train loss:  0.3088516592979431
train gradient:  0.1856742557485891
iteration : 9478
train acc:  0.8515625
train loss:  0.29341256618499756
train gradient:  0.11579733019913539
iteration : 9479
train acc:  0.890625
train loss:  0.2665092349052429
train gradient:  0.10833073402651369
iteration : 9480
train acc:  0.8359375
train loss:  0.3165091276168823
train gradient:  0.1050034696047934
iteration : 9481
train acc:  0.84375
train loss:  0.3536939322948456
train gradient:  0.12447793981908838
iteration : 9482
train acc:  0.859375
train loss:  0.35232287645339966
train gradient:  0.1315911477974144
iteration : 9483
train acc:  0.875
train loss:  0.3001893162727356
train gradient:  0.18300267737136156
iteration : 9484
train acc:  0.8671875
train loss:  0.2905600666999817
train gradient:  0.14533567920940163
iteration : 9485
train acc:  0.8984375
train loss:  0.28347474336624146
train gradient:  0.1393931776170123
iteration : 9486
train acc:  0.875
train loss:  0.3945203423500061
train gradient:  0.2004575035152951
iteration : 9487
train acc:  0.859375
train loss:  0.28648167848587036
train gradient:  0.11640150507368861
iteration : 9488
train acc:  0.8359375
train loss:  0.33425065875053406
train gradient:  0.19623555953444083
iteration : 9489
train acc:  0.90625
train loss:  0.2485847920179367
train gradient:  0.12934380493069692
iteration : 9490
train acc:  0.8359375
train loss:  0.3778115510940552
train gradient:  0.16778767205200315
iteration : 9491
train acc:  0.890625
train loss:  0.27559894323349
train gradient:  0.10447369408466224
iteration : 9492
train acc:  0.84375
train loss:  0.3822975158691406
train gradient:  0.2230175766514858
iteration : 9493
train acc:  0.90625
train loss:  0.24559460580348969
train gradient:  0.11536414666267968
iteration : 9494
train acc:  0.8125
train loss:  0.36515623331069946
train gradient:  0.14828901873327516
iteration : 9495
train acc:  0.828125
train loss:  0.38854509592056274
train gradient:  0.27168804679522796
iteration : 9496
train acc:  0.8515625
train loss:  0.29742497205734253
train gradient:  0.1087951567561759
iteration : 9497
train acc:  0.8671875
train loss:  0.30812913179397583
train gradient:  0.12644791260389812
iteration : 9498
train acc:  0.875
train loss:  0.2636570334434509
train gradient:  0.11470510526764194
iteration : 9499
train acc:  0.859375
train loss:  0.32159513235092163
train gradient:  0.156394977615977
iteration : 9500
train acc:  0.875
train loss:  0.3387840986251831
train gradient:  0.16421792234741314
iteration : 9501
train acc:  0.8671875
train loss:  0.29182660579681396
train gradient:  0.15706222601171177
iteration : 9502
train acc:  0.859375
train loss:  0.31797873973846436
train gradient:  0.16521946335658122
iteration : 9503
train acc:  0.8671875
train loss:  0.3229488134384155
train gradient:  0.18197424083225733
iteration : 9504
train acc:  0.8359375
train loss:  0.32232558727264404
train gradient:  0.14411762674900774
iteration : 9505
train acc:  0.890625
train loss:  0.22662869095802307
train gradient:  0.1123955303080766
iteration : 9506
train acc:  0.8671875
train loss:  0.3282161355018616
train gradient:  0.16512877575387302
iteration : 9507
train acc:  0.8515625
train loss:  0.3191375732421875
train gradient:  0.15794510382719326
iteration : 9508
train acc:  0.875
train loss:  0.37348663806915283
train gradient:  0.19101013200697992
iteration : 9509
train acc:  0.8359375
train loss:  0.40001243352890015
train gradient:  0.2510010278964459
iteration : 9510
train acc:  0.8671875
train loss:  0.32306361198425293
train gradient:  0.1876524172230921
iteration : 9511
train acc:  0.875
train loss:  0.2731476128101349
train gradient:  0.11729091545573525
iteration : 9512
train acc:  0.9296875
train loss:  0.24074871838092804
train gradient:  0.1572236636944499
iteration : 9513
train acc:  0.875
train loss:  0.259658545255661
train gradient:  0.14577808943629278
iteration : 9514
train acc:  0.859375
train loss:  0.32828235626220703
train gradient:  0.149761885363715
iteration : 9515
train acc:  0.8984375
train loss:  0.20666547119617462
train gradient:  0.10005211578482728
iteration : 9516
train acc:  0.84375
train loss:  0.31572964787483215
train gradient:  0.12135190277014478
iteration : 9517
train acc:  0.8515625
train loss:  0.301841676235199
train gradient:  0.15273869305193638
iteration : 9518
train acc:  0.8671875
train loss:  0.3107198178768158
train gradient:  0.12671665901886212
iteration : 9519
train acc:  0.84375
train loss:  0.3499385714530945
train gradient:  0.34759928126887213
iteration : 9520
train acc:  0.875
train loss:  0.2561289370059967
train gradient:  0.11474169473196795
iteration : 9521
train acc:  0.875
train loss:  0.33743783831596375
train gradient:  0.16276991878289643
iteration : 9522
train acc:  0.7734375
train loss:  0.4778274595737457
train gradient:  0.30012525521284117
iteration : 9523
train acc:  0.84375
train loss:  0.3467085063457489
train gradient:  0.1829131684726172
iteration : 9524
train acc:  0.875
train loss:  0.33016103506088257
train gradient:  0.11532156043689788
iteration : 9525
train acc:  0.8515625
train loss:  0.3638295531272888
train gradient:  0.15982055055023112
iteration : 9526
train acc:  0.875
train loss:  0.3140871524810791
train gradient:  0.1363987992520088
iteration : 9527
train acc:  0.8203125
train loss:  0.40340089797973633
train gradient:  0.2047433725456715
iteration : 9528
train acc:  0.828125
train loss:  0.39475977420806885
train gradient:  0.23666601584797609
iteration : 9529
train acc:  0.84375
train loss:  0.31820979714393616
train gradient:  0.1520571987830311
iteration : 9530
train acc:  0.8515625
train loss:  0.28930923342704773
train gradient:  0.1355486961190911
iteration : 9531
train acc:  0.8671875
train loss:  0.296225905418396
train gradient:  0.14481737767513336
iteration : 9532
train acc:  0.875
train loss:  0.31151020526885986
train gradient:  0.10129083287710872
iteration : 9533
train acc:  0.8671875
train loss:  0.32653260231018066
train gradient:  0.1469260133037824
iteration : 9534
train acc:  0.8515625
train loss:  0.38076186180114746
train gradient:  0.2182909767112185
iteration : 9535
train acc:  0.8828125
train loss:  0.2927855849266052
train gradient:  0.1561472445249092
iteration : 9536
train acc:  0.8125
train loss:  0.31734368205070496
train gradient:  0.16815835512909177
iteration : 9537
train acc:  0.9375
train loss:  0.23767989873886108
train gradient:  0.0972368164996278
iteration : 9538
train acc:  0.890625
train loss:  0.2966471016407013
train gradient:  0.1389637764689664
iteration : 9539
train acc:  0.890625
train loss:  0.2792983055114746
train gradient:  0.14889147701109515
iteration : 9540
train acc:  0.8984375
train loss:  0.27811604738235474
train gradient:  0.1448229319871497
iteration : 9541
train acc:  0.8671875
train loss:  0.3180578351020813
train gradient:  0.15526293191159843
iteration : 9542
train acc:  0.8984375
train loss:  0.25088658928871155
train gradient:  0.14863675909163995
iteration : 9543
train acc:  0.859375
train loss:  0.307073175907135
train gradient:  0.1398218629748596
iteration : 9544
train acc:  0.8359375
train loss:  0.3199228346347809
train gradient:  0.15125663845027587
iteration : 9545
train acc:  0.8984375
train loss:  0.264294296503067
train gradient:  0.09980964073759764
iteration : 9546
train acc:  0.8203125
train loss:  0.3662831783294678
train gradient:  0.14610955639480055
iteration : 9547
train acc:  0.8046875
train loss:  0.4793550372123718
train gradient:  0.30015412401536473
iteration : 9548
train acc:  0.8046875
train loss:  0.3781193792819977
train gradient:  0.2654434182839069
iteration : 9549
train acc:  0.8828125
train loss:  0.2762453556060791
train gradient:  0.13027224676549365
iteration : 9550
train acc:  0.875
train loss:  0.2780267000198364
train gradient:  0.13605480771702655
iteration : 9551
train acc:  0.796875
train loss:  0.39401373267173767
train gradient:  0.24563583114130555
iteration : 9552
train acc:  0.859375
train loss:  0.3107849955558777
train gradient:  0.17682982486378496
iteration : 9553
train acc:  0.8671875
train loss:  0.3478453457355499
train gradient:  0.18279449912466156
iteration : 9554
train acc:  0.8046875
train loss:  0.39355260133743286
train gradient:  0.2671053376991783
iteration : 9555
train acc:  0.859375
train loss:  0.3552297353744507
train gradient:  0.17988815087890558
iteration : 9556
train acc:  0.859375
train loss:  0.3773045837879181
train gradient:  0.22969088543801114
iteration : 9557
train acc:  0.921875
train loss:  0.2303096204996109
train gradient:  0.11964002995333624
iteration : 9558
train acc:  0.8515625
train loss:  0.3004705309867859
train gradient:  0.15014331050842902
iteration : 9559
train acc:  0.8671875
train loss:  0.3185844421386719
train gradient:  0.14100184986285128
iteration : 9560
train acc:  0.84375
train loss:  0.3573753833770752
train gradient:  0.16504980501415206
iteration : 9561
train acc:  0.828125
train loss:  0.33094263076782227
train gradient:  0.28720569264333395
iteration : 9562
train acc:  0.8828125
train loss:  0.28379374742507935
train gradient:  0.1381948106895703
iteration : 9563
train acc:  0.828125
train loss:  0.3926292061805725
train gradient:  0.18837397010090606
iteration : 9564
train acc:  0.828125
train loss:  0.3576725125312805
train gradient:  0.16418550698134582
iteration : 9565
train acc:  0.90625
train loss:  0.2568552792072296
train gradient:  0.12285925212969334
iteration : 9566
train acc:  0.8515625
train loss:  0.33502858877182007
train gradient:  0.14596022728245006
iteration : 9567
train acc:  0.8671875
train loss:  0.3202894926071167
train gradient:  0.2066047658301179
iteration : 9568
train acc:  0.890625
train loss:  0.28778529167175293
train gradient:  0.10777265990682344
iteration : 9569
train acc:  0.828125
train loss:  0.3543355464935303
train gradient:  0.18433938374016828
iteration : 9570
train acc:  0.9296875
train loss:  0.2065114825963974
train gradient:  0.07224824308105872
iteration : 9571
train acc:  0.8359375
train loss:  0.3464832901954651
train gradient:  0.14887795393624503
iteration : 9572
train acc:  0.859375
train loss:  0.32206177711486816
train gradient:  0.17392350070389526
iteration : 9573
train acc:  0.8671875
train loss:  0.3169533908367157
train gradient:  0.20369303796202065
iteration : 9574
train acc:  0.890625
train loss:  0.2700249254703522
train gradient:  0.13591261110470815
iteration : 9575
train acc:  0.8359375
train loss:  0.31180310249328613
train gradient:  0.14470427559472593
iteration : 9576
train acc:  0.875
train loss:  0.31842130422592163
train gradient:  0.1418306910071503
iteration : 9577
train acc:  0.84375
train loss:  0.3434201776981354
train gradient:  0.14314761199481968
iteration : 9578
train acc:  0.8359375
train loss:  0.3934926986694336
train gradient:  0.2000424511495768
iteration : 9579
train acc:  0.734375
train loss:  0.5500608086585999
train gradient:  0.3407189438061892
iteration : 9580
train acc:  0.8125
train loss:  0.37123623490333557
train gradient:  0.19382293890684743
iteration : 9581
train acc:  0.8984375
train loss:  0.27020859718322754
train gradient:  0.13842824078492838
iteration : 9582
train acc:  0.9140625
train loss:  0.22618646919727325
train gradient:  0.10205491203411682
iteration : 9583
train acc:  0.828125
train loss:  0.3346560597419739
train gradient:  0.1364404828508864
iteration : 9584
train acc:  0.84375
train loss:  0.3684132397174835
train gradient:  0.276778619313455
iteration : 9585
train acc:  0.828125
train loss:  0.40254104137420654
train gradient:  0.1837609231333205
iteration : 9586
train acc:  0.875
train loss:  0.2839463949203491
train gradient:  0.11289833507104342
iteration : 9587
train acc:  0.890625
train loss:  0.3547338843345642
train gradient:  0.29567625328555364
iteration : 9588
train acc:  0.8359375
train loss:  0.424025297164917
train gradient:  0.2855946718310587
iteration : 9589
train acc:  0.9453125
train loss:  0.22313252091407776
train gradient:  0.13471605799755407
iteration : 9590
train acc:  0.859375
train loss:  0.3109731674194336
train gradient:  0.21103487926554915
iteration : 9591
train acc:  0.8203125
train loss:  0.32382822036743164
train gradient:  0.23660601801060627
iteration : 9592
train acc:  0.7890625
train loss:  0.4910445213317871
train gradient:  0.3584452550088111
iteration : 9593
train acc:  0.84375
train loss:  0.3783279359340668
train gradient:  0.27211960457210793
iteration : 9594
train acc:  0.796875
train loss:  0.4022045433521271
train gradient:  0.3849502984116084
iteration : 9595
train acc:  0.828125
train loss:  0.3869137763977051
train gradient:  0.19581317184483554
iteration : 9596
train acc:  0.90625
train loss:  0.2234525978565216
train gradient:  0.11340338270295648
iteration : 9597
train acc:  0.8671875
train loss:  0.31424346566200256
train gradient:  0.1525103927701468
iteration : 9598
train acc:  0.890625
train loss:  0.25202056765556335
train gradient:  0.1301914567439607
iteration : 9599
train acc:  0.84375
train loss:  0.3627985119819641
train gradient:  0.1463983482209793
iteration : 9600
train acc:  0.8046875
train loss:  0.3429822623729706
train gradient:  0.141999103175054
iteration : 9601
train acc:  0.9140625
train loss:  0.276142418384552
train gradient:  0.15814426846465893
iteration : 9602
train acc:  0.8828125
train loss:  0.3125859797000885
train gradient:  0.11087717073396257
iteration : 9603
train acc:  0.8203125
train loss:  0.3761202096939087
train gradient:  0.18289331827934782
iteration : 9604
train acc:  0.8671875
train loss:  0.29657602310180664
train gradient:  0.1420564198290527
iteration : 9605
train acc:  0.8984375
train loss:  0.26679563522338867
train gradient:  0.17030973655080173
iteration : 9606
train acc:  0.84375
train loss:  0.29102540016174316
train gradient:  0.12104540855276322
iteration : 9607
train acc:  0.859375
train loss:  0.3173523545265198
train gradient:  0.12855854017927132
iteration : 9608
train acc:  0.8359375
train loss:  0.3235762119293213
train gradient:  0.12232815981687666
iteration : 9609
train acc:  0.8515625
train loss:  0.35812410712242126
train gradient:  0.16478369506030868
iteration : 9610
train acc:  0.875
train loss:  0.27602630853652954
train gradient:  0.1753934709826017
iteration : 9611
train acc:  0.8125
train loss:  0.37233078479766846
train gradient:  0.27093825872634036
iteration : 9612
train acc:  0.8046875
train loss:  0.3729288578033447
train gradient:  0.2797697207224562
iteration : 9613
train acc:  0.84375
train loss:  0.33593660593032837
train gradient:  0.1654506474208896
iteration : 9614
train acc:  0.859375
train loss:  0.31133702397346497
train gradient:  0.1242470481066156
iteration : 9615
train acc:  0.90625
train loss:  0.3011692762374878
train gradient:  0.1816103796114305
iteration : 9616
train acc:  0.8828125
train loss:  0.2740227282047272
train gradient:  0.12697527486422197
iteration : 9617
train acc:  0.828125
train loss:  0.36451756954193115
train gradient:  0.23150863478657424
iteration : 9618
train acc:  0.84375
train loss:  0.34545648097991943
train gradient:  0.2252345606857008
iteration : 9619
train acc:  0.8359375
train loss:  0.3589497208595276
train gradient:  0.1219947101894419
iteration : 9620
train acc:  0.8828125
train loss:  0.28129979968070984
train gradient:  0.11569413282758068
iteration : 9621
train acc:  0.8515625
train loss:  0.3635634779930115
train gradient:  0.19592698792814844
iteration : 9622
train acc:  0.8203125
train loss:  0.348968505859375
train gradient:  0.23791624898509137
iteration : 9623
train acc:  0.8515625
train loss:  0.391848623752594
train gradient:  0.31272840852094896
iteration : 9624
train acc:  0.84375
train loss:  0.37783288955688477
train gradient:  0.1566606298342965
iteration : 9625
train acc:  0.859375
train loss:  0.3261755108833313
train gradient:  0.2123207950839595
iteration : 9626
train acc:  0.859375
train loss:  0.395028293132782
train gradient:  0.23119160937173588
iteration : 9627
train acc:  0.859375
train loss:  0.2701840400695801
train gradient:  0.180273114796017
iteration : 9628
train acc:  0.84375
train loss:  0.3208395838737488
train gradient:  0.10973143418169672
iteration : 9629
train acc:  0.875
train loss:  0.2944309711456299
train gradient:  0.09534736612869839
iteration : 9630
train acc:  0.8671875
train loss:  0.3053056001663208
train gradient:  0.1279079044345564
iteration : 9631
train acc:  0.8515625
train loss:  0.38595789670944214
train gradient:  0.197313918022487
iteration : 9632
train acc:  0.875
train loss:  0.26889699697494507
train gradient:  0.1262425439899122
iteration : 9633
train acc:  0.84375
train loss:  0.31752029061317444
train gradient:  0.18979821383366602
iteration : 9634
train acc:  0.890625
train loss:  0.26375752687454224
train gradient:  0.12117074634689338
iteration : 9635
train acc:  0.859375
train loss:  0.2866462767124176
train gradient:  0.1536595582739298
iteration : 9636
train acc:  0.8125
train loss:  0.40627825260162354
train gradient:  0.23927290642287546
iteration : 9637
train acc:  0.8828125
train loss:  0.27001869678497314
train gradient:  0.17008976557475525
iteration : 9638
train acc:  0.796875
train loss:  0.37519627809524536
train gradient:  0.20025303086422933
iteration : 9639
train acc:  0.890625
train loss:  0.24715211987495422
train gradient:  0.17087036907256803
iteration : 9640
train acc:  0.8828125
train loss:  0.2806732952594757
train gradient:  0.15150514814720362
iteration : 9641
train acc:  0.8359375
train loss:  0.3422737121582031
train gradient:  0.17108183097422172
iteration : 9642
train acc:  0.8828125
train loss:  0.31054866313934326
train gradient:  0.15803134826226306
iteration : 9643
train acc:  0.859375
train loss:  0.3041069209575653
train gradient:  0.19000827527692968
iteration : 9644
train acc:  0.84375
train loss:  0.39282914996147156
train gradient:  0.18930088833875003
iteration : 9645
train acc:  0.8515625
train loss:  0.36466842889785767
train gradient:  0.22972554461048805
iteration : 9646
train acc:  0.8671875
train loss:  0.29541242122650146
train gradient:  0.223134457335146
iteration : 9647
train acc:  0.8828125
train loss:  0.29963061213493347
train gradient:  0.13441347449203087
iteration : 9648
train acc:  0.8203125
train loss:  0.40152332186698914
train gradient:  0.22236707699431635
iteration : 9649
train acc:  0.875
train loss:  0.2758791148662567
train gradient:  0.15107476924877955
iteration : 9650
train acc:  0.8359375
train loss:  0.307733416557312
train gradient:  0.1430079548405816
iteration : 9651
train acc:  0.8828125
train loss:  0.27048060297966003
train gradient:  0.10179189119169056
iteration : 9652
train acc:  0.796875
train loss:  0.3807331919670105
train gradient:  0.3203611235731247
iteration : 9653
train acc:  0.859375
train loss:  0.2916436791419983
train gradient:  0.17285646426849458
iteration : 9654
train acc:  0.8828125
train loss:  0.30931729078292847
train gradient:  0.16468406181454343
iteration : 9655
train acc:  0.875
train loss:  0.32244783639907837
train gradient:  0.19871181874652327
iteration : 9656
train acc:  0.796875
train loss:  0.40200668573379517
train gradient:  0.28922115704460083
iteration : 9657
train acc:  0.78125
train loss:  0.4758593440055847
train gradient:  0.2908494745484797
iteration : 9658
train acc:  0.7890625
train loss:  0.4050667881965637
train gradient:  0.2636876052185376
iteration : 9659
train acc:  0.875
train loss:  0.37395650148391724
train gradient:  0.20718523006241296
iteration : 9660
train acc:  0.8125
train loss:  0.40829867124557495
train gradient:  0.20751157313922075
iteration : 9661
train acc:  0.8515625
train loss:  0.25806018710136414
train gradient:  0.10494365518880662
iteration : 9662
train acc:  0.875
train loss:  0.3129226863384247
train gradient:  0.2049594694476289
iteration : 9663
train acc:  0.8125
train loss:  0.3413398861885071
train gradient:  0.23519142942564888
iteration : 9664
train acc:  0.8125
train loss:  0.38345658779144287
train gradient:  0.25633569125397637
iteration : 9665
train acc:  0.8671875
train loss:  0.3083975911140442
train gradient:  0.10904871602470503
iteration : 9666
train acc:  0.8671875
train loss:  0.33611494302749634
train gradient:  0.24230313831318506
iteration : 9667
train acc:  0.8828125
train loss:  0.30628103017807007
train gradient:  0.148770352245074
iteration : 9668
train acc:  0.8671875
train loss:  0.302430659532547
train gradient:  0.18251063724345595
iteration : 9669
train acc:  0.8828125
train loss:  0.2964645028114319
train gradient:  0.14428653552039977
iteration : 9670
train acc:  0.8671875
train loss:  0.3976093530654907
train gradient:  0.18495703559278298
iteration : 9671
train acc:  0.8671875
train loss:  0.28656792640686035
train gradient:  0.18165787604257727
iteration : 9672
train acc:  0.8515625
train loss:  0.29196697473526
train gradient:  0.26606141439511777
iteration : 9673
train acc:  0.8203125
train loss:  0.3587804436683655
train gradient:  0.2388135294041656
iteration : 9674
train acc:  0.8671875
train loss:  0.3396438956260681
train gradient:  0.1733608164567023
iteration : 9675
train acc:  0.8671875
train loss:  0.27971774339675903
train gradient:  0.1181636842192631
iteration : 9676
train acc:  0.859375
train loss:  0.3597351014614105
train gradient:  0.15827937136566048
iteration : 9677
train acc:  0.859375
train loss:  0.3094428777694702
train gradient:  0.2159850456363635
iteration : 9678
train acc:  0.875
train loss:  0.2964860498905182
train gradient:  0.15548516837184553
iteration : 9679
train acc:  0.859375
train loss:  0.2993871569633484
train gradient:  0.11584904572689697
iteration : 9680
train acc:  0.8671875
train loss:  0.2958744764328003
train gradient:  0.12133650882445074
iteration : 9681
train acc:  0.828125
train loss:  0.34109771251678467
train gradient:  0.16975064025773967
iteration : 9682
train acc:  0.8203125
train loss:  0.3231144845485687
train gradient:  0.14571849064535497
iteration : 9683
train acc:  0.90625
train loss:  0.2631942331790924
train gradient:  0.10921223532550053
iteration : 9684
train acc:  0.875
train loss:  0.3278348445892334
train gradient:  0.15411242077199613
iteration : 9685
train acc:  0.828125
train loss:  0.3235275149345398
train gradient:  0.14962829169540381
iteration : 9686
train acc:  0.8828125
train loss:  0.2609322965145111
train gradient:  0.14287159194202695
iteration : 9687
train acc:  0.859375
train loss:  0.325531005859375
train gradient:  0.21085201063762465
iteration : 9688
train acc:  0.84375
train loss:  0.34606993198394775
train gradient:  0.16510128094491425
iteration : 9689
train acc:  0.8203125
train loss:  0.3713778257369995
train gradient:  0.1881811963247454
iteration : 9690
train acc:  0.875
train loss:  0.3046143651008606
train gradient:  0.16268897854072129
iteration : 9691
train acc:  0.8046875
train loss:  0.36791887879371643
train gradient:  0.2097472035856658
iteration : 9692
train acc:  0.9140625
train loss:  0.23644062876701355
train gradient:  0.1714956425475189
iteration : 9693
train acc:  0.8984375
train loss:  0.3005766272544861
train gradient:  0.12083777745899851
iteration : 9694
train acc:  0.921875
train loss:  0.2601427137851715
train gradient:  0.09516853840719257
iteration : 9695
train acc:  0.828125
train loss:  0.4027002453804016
train gradient:  0.2504880749999882
iteration : 9696
train acc:  0.8359375
train loss:  0.4191194772720337
train gradient:  0.23045847399681196
iteration : 9697
train acc:  0.8984375
train loss:  0.28317439556121826
train gradient:  0.11650786515493942
iteration : 9698
train acc:  0.875
train loss:  0.315700888633728
train gradient:  0.16607906347028942
iteration : 9699
train acc:  0.859375
train loss:  0.33212578296661377
train gradient:  0.15157846331509472
iteration : 9700
train acc:  0.890625
train loss:  0.2882222533226013
train gradient:  0.13402987105539313
iteration : 9701
train acc:  0.875
train loss:  0.3152538239955902
train gradient:  0.1338117250195951
iteration : 9702
train acc:  0.859375
train loss:  0.373285174369812
train gradient:  0.14282473595406725
iteration : 9703
train acc:  0.84375
train loss:  0.32806527614593506
train gradient:  0.1104931825860788
iteration : 9704
train acc:  0.8203125
train loss:  0.3467080593109131
train gradient:  0.16235149717820324
iteration : 9705
train acc:  0.875
train loss:  0.2622244656085968
train gradient:  0.10160293428036068
iteration : 9706
train acc:  0.875
train loss:  0.32479918003082275
train gradient:  0.14819204627582264
iteration : 9707
train acc:  0.8515625
train loss:  0.3302043676376343
train gradient:  0.1533415362784492
iteration : 9708
train acc:  0.8828125
train loss:  0.2526111900806427
train gradient:  0.18088510439588712
iteration : 9709
train acc:  0.875
train loss:  0.2799641788005829
train gradient:  0.1302832527655108
iteration : 9710
train acc:  0.84375
train loss:  0.30494531989097595
train gradient:  0.10260816494629726
iteration : 9711
train acc:  0.8515625
train loss:  0.33800098299980164
train gradient:  0.1759983454571874
iteration : 9712
train acc:  0.8515625
train loss:  0.3880099654197693
train gradient:  0.2033257129984986
iteration : 9713
train acc:  0.8671875
train loss:  0.2961537539958954
train gradient:  0.15752464825269744
iteration : 9714
train acc:  0.8515625
train loss:  0.33714497089385986
train gradient:  0.14691858984337874
iteration : 9715
train acc:  0.8359375
train loss:  0.4091089367866516
train gradient:  0.20005958981406569
iteration : 9716
train acc:  0.8359375
train loss:  0.3821335434913635
train gradient:  0.23904488727694095
iteration : 9717
train acc:  0.7890625
train loss:  0.44566309452056885
train gradient:  0.26508800824800804
iteration : 9718
train acc:  0.84375
train loss:  0.38997143507003784
train gradient:  0.4166741936002102
iteration : 9719
train acc:  0.875
train loss:  0.2874911427497864
train gradient:  0.1250350249140869
iteration : 9720
train acc:  0.859375
train loss:  0.35715678334236145
train gradient:  0.15917865157040179
iteration : 9721
train acc:  0.84375
train loss:  0.3150613307952881
train gradient:  0.13306815378390013
iteration : 9722
train acc:  0.8671875
train loss:  0.30339881777763367
train gradient:  0.14443179622123486
iteration : 9723
train acc:  0.875
train loss:  0.28490591049194336
train gradient:  0.11571397265855153
iteration : 9724
train acc:  0.890625
train loss:  0.2585103511810303
train gradient:  0.10888563538517493
iteration : 9725
train acc:  0.90625
train loss:  0.3054960370063782
train gradient:  0.17133426218308023
iteration : 9726
train acc:  0.84375
train loss:  0.37247198820114136
train gradient:  0.17654517685186327
iteration : 9727
train acc:  0.8828125
train loss:  0.28491297364234924
train gradient:  0.18913359668290614
iteration : 9728
train acc:  0.8046875
train loss:  0.41958820819854736
train gradient:  0.17010738489827815
iteration : 9729
train acc:  0.8984375
train loss:  0.29449307918548584
train gradient:  0.08001506169284375
iteration : 9730
train acc:  0.890625
train loss:  0.2452126443386078
train gradient:  0.09403679620944388
iteration : 9731
train acc:  0.859375
train loss:  0.3193783760070801
train gradient:  0.13147399899396572
iteration : 9732
train acc:  0.8515625
train loss:  0.3141583204269409
train gradient:  0.19215402433224213
iteration : 9733
train acc:  0.859375
train loss:  0.34982162714004517
train gradient:  0.20901673464343662
iteration : 9734
train acc:  0.890625
train loss:  0.30258315801620483
train gradient:  0.1220085611174085
iteration : 9735
train acc:  0.8671875
train loss:  0.4065207540988922
train gradient:  0.22317355976362302
iteration : 9736
train acc:  0.890625
train loss:  0.310435026884079
train gradient:  0.13945347158479715
iteration : 9737
train acc:  0.8203125
train loss:  0.4145340919494629
train gradient:  0.20242777080013896
iteration : 9738
train acc:  0.8515625
train loss:  0.3630622327327728
train gradient:  0.1452429907393862
iteration : 9739
train acc:  0.9140625
train loss:  0.2429090142250061
train gradient:  0.09839826222819896
iteration : 9740
train acc:  0.890625
train loss:  0.2869299650192261
train gradient:  0.13382512499916352
iteration : 9741
train acc:  0.828125
train loss:  0.3533538579940796
train gradient:  0.2210155266610298
iteration : 9742
train acc:  0.953125
train loss:  0.21498998999595642
train gradient:  0.10553733887395643
iteration : 9743
train acc:  0.828125
train loss:  0.311937153339386
train gradient:  0.11726018625888303
iteration : 9744
train acc:  0.859375
train loss:  0.303439736366272
train gradient:  0.0917087013100827
iteration : 9745
train acc:  0.84375
train loss:  0.36863261461257935
train gradient:  0.18814960324391938
iteration : 9746
train acc:  0.828125
train loss:  0.3523540496826172
train gradient:  0.2106712389840919
iteration : 9747
train acc:  0.8828125
train loss:  0.34475260972976685
train gradient:  0.16933453868744805
iteration : 9748
train acc:  0.8125
train loss:  0.4200880527496338
train gradient:  0.18460993802709874
iteration : 9749
train acc:  0.8515625
train loss:  0.3768690228462219
train gradient:  0.19532868875563908
iteration : 9750
train acc:  0.8125
train loss:  0.33567512035369873
train gradient:  0.15738724729924278
iteration : 9751
train acc:  0.84375
train loss:  0.3228847086429596
train gradient:  0.16566074485312637
iteration : 9752
train acc:  0.8359375
train loss:  0.3739738166332245
train gradient:  0.2318660007003493
iteration : 9753
train acc:  0.8046875
train loss:  0.3712894022464752
train gradient:  0.1866337560516224
iteration : 9754
train acc:  0.8203125
train loss:  0.40601426362991333
train gradient:  0.2531869759049958
iteration : 9755
train acc:  0.859375
train loss:  0.37472283840179443
train gradient:  0.240063407624167
iteration : 9756
train acc:  0.8828125
train loss:  0.26091834902763367
train gradient:  0.11795380610297865
iteration : 9757
train acc:  0.875
train loss:  0.26224493980407715
train gradient:  0.1150999959123944
iteration : 9758
train acc:  0.8515625
train loss:  0.3371785283088684
train gradient:  0.2068529030111822
iteration : 9759
train acc:  0.859375
train loss:  0.313924640417099
train gradient:  0.13305260137267955
iteration : 9760
train acc:  0.796875
train loss:  0.4298524856567383
train gradient:  0.2540743905629284
iteration : 9761
train acc:  0.8515625
train loss:  0.3078992962837219
train gradient:  0.12430173793003307
iteration : 9762
train acc:  0.8359375
train loss:  0.35094553232192993
train gradient:  0.21667820591588852
iteration : 9763
train acc:  0.875
train loss:  0.3657132089138031
train gradient:  0.1461230435871246
iteration : 9764
train acc:  0.828125
train loss:  0.31417542695999146
train gradient:  0.16433889987244482
iteration : 9765
train acc:  0.890625
train loss:  0.2707555294036865
train gradient:  0.18030405128428958
iteration : 9766
train acc:  0.9140625
train loss:  0.24205926060676575
train gradient:  0.1924980427418222
iteration : 9767
train acc:  0.828125
train loss:  0.3988562822341919
train gradient:  0.2577640719567163
iteration : 9768
train acc:  0.8203125
train loss:  0.4143739938735962
train gradient:  0.1967057470146153
iteration : 9769
train acc:  0.859375
train loss:  0.3321254849433899
train gradient:  0.17610176472714345
iteration : 9770
train acc:  0.859375
train loss:  0.30232277512550354
train gradient:  0.1480116358506119
iteration : 9771
train acc:  0.859375
train loss:  0.3505019545555115
train gradient:  0.15139038479103228
iteration : 9772
train acc:  0.828125
train loss:  0.3009813129901886
train gradient:  0.2717160901443235
iteration : 9773
train acc:  0.875
train loss:  0.2851719856262207
train gradient:  0.11035826346293795
iteration : 9774
train acc:  0.8984375
train loss:  0.293423593044281
train gradient:  0.12312525592126661
iteration : 9775
train acc:  0.8046875
train loss:  0.4290398359298706
train gradient:  0.2177474276930579
iteration : 9776
train acc:  0.859375
train loss:  0.3572087287902832
train gradient:  0.17498380377877826
iteration : 9777
train acc:  0.90625
train loss:  0.2804409861564636
train gradient:  0.1490376668446815
iteration : 9778
train acc:  0.8046875
train loss:  0.3703726530075073
train gradient:  0.15698916325117895
iteration : 9779
train acc:  0.8046875
train loss:  0.37674540281295776
train gradient:  0.24519658405052397
iteration : 9780
train acc:  0.859375
train loss:  0.28595221042633057
train gradient:  0.13005875478992174
iteration : 9781
train acc:  0.8359375
train loss:  0.312414288520813
train gradient:  0.12217301135210329
iteration : 9782
train acc:  0.796875
train loss:  0.5209808349609375
train gradient:  0.3540669526100929
iteration : 9783
train acc:  0.890625
train loss:  0.32569408416748047
train gradient:  0.18159567057859366
iteration : 9784
train acc:  0.875
train loss:  0.30020904541015625
train gradient:  0.10736853253253412
iteration : 9785
train acc:  0.875
train loss:  0.3057108521461487
train gradient:  0.11317213486941938
iteration : 9786
train acc:  0.90625
train loss:  0.29335224628448486
train gradient:  0.10077084574899992
iteration : 9787
train acc:  0.8671875
train loss:  0.3828173875808716
train gradient:  0.17951894792360046
iteration : 9788
train acc:  0.8984375
train loss:  0.25721806287765503
train gradient:  0.11379982427559372
iteration : 9789
train acc:  0.890625
train loss:  0.28507542610168457
train gradient:  0.12502655649187028
iteration : 9790
train acc:  0.828125
train loss:  0.3364596962928772
train gradient:  0.1967742231557981
iteration : 9791
train acc:  0.8359375
train loss:  0.3138571083545685
train gradient:  0.2846648918160392
iteration : 9792
train acc:  0.921875
train loss:  0.24104736745357513
train gradient:  0.1143841423579528
iteration : 9793
train acc:  0.84375
train loss:  0.3593609929084778
train gradient:  0.24470450238522398
iteration : 9794
train acc:  0.8828125
train loss:  0.258592426776886
train gradient:  0.13199843456264437
iteration : 9795
train acc:  0.8828125
train loss:  0.26915764808654785
train gradient:  0.10362014640089963
iteration : 9796
train acc:  0.859375
train loss:  0.32186761498451233
train gradient:  0.18975935051104023
iteration : 9797
train acc:  0.84375
train loss:  0.29628318548202515
train gradient:  0.10225318172809927
iteration : 9798
train acc:  0.859375
train loss:  0.2833267152309418
train gradient:  0.1349809235200164
iteration : 9799
train acc:  0.8125
train loss:  0.36729860305786133
train gradient:  0.1951415272557322
iteration : 9800
train acc:  0.890625
train loss:  0.3373560309410095
train gradient:  0.12260689677131743
iteration : 9801
train acc:  0.8359375
train loss:  0.38292309641838074
train gradient:  0.19015497050095415
iteration : 9802
train acc:  0.875
train loss:  0.31370943784713745
train gradient:  0.23818748639538362
iteration : 9803
train acc:  0.90625
train loss:  0.2548610270023346
train gradient:  0.11916425603442553
iteration : 9804
train acc:  0.890625
train loss:  0.2811034321784973
train gradient:  0.1090567004750923
iteration : 9805
train acc:  0.875
train loss:  0.29628631472587585
train gradient:  0.10047667925494516
iteration : 9806
train acc:  0.859375
train loss:  0.36715322732925415
train gradient:  0.14484937585499222
iteration : 9807
train acc:  0.859375
train loss:  0.31309422850608826
train gradient:  0.17436880822930895
iteration : 9808
train acc:  0.875
train loss:  0.2542795240879059
train gradient:  0.10058083745303932
iteration : 9809
train acc:  0.7890625
train loss:  0.40721219778060913
train gradient:  0.2792647660445233
iteration : 9810
train acc:  0.828125
train loss:  0.36996981501579285
train gradient:  0.16711380788662317
iteration : 9811
train acc:  0.8828125
train loss:  0.30963578820228577
train gradient:  0.16059096096781694
iteration : 9812
train acc:  0.875
train loss:  0.3995765745639801
train gradient:  0.21506131665438316
iteration : 9813
train acc:  0.8671875
train loss:  0.3055894374847412
train gradient:  0.12629696333554985
iteration : 9814
train acc:  0.8671875
train loss:  0.3199586272239685
train gradient:  0.11700268796085266
iteration : 9815
train acc:  0.8125
train loss:  0.37461328506469727
train gradient:  0.16446942598328493
iteration : 9816
train acc:  0.859375
train loss:  0.33244478702545166
train gradient:  0.12810433489301576
iteration : 9817
train acc:  0.828125
train loss:  0.3683156371116638
train gradient:  0.29066092062343046
iteration : 9818
train acc:  0.8359375
train loss:  0.3192700743675232
train gradient:  0.1693644120228159
iteration : 9819
train acc:  0.859375
train loss:  0.3091132938861847
train gradient:  0.15054624141169598
iteration : 9820
train acc:  0.875
train loss:  0.3214040994644165
train gradient:  0.13134140248873627
iteration : 9821
train acc:  0.8671875
train loss:  0.3531651496887207
train gradient:  0.15960839406347568
iteration : 9822
train acc:  0.921875
train loss:  0.22302700579166412
train gradient:  0.11724613514919048
iteration : 9823
train acc:  0.8671875
train loss:  0.2859123945236206
train gradient:  0.15846689157925276
iteration : 9824
train acc:  0.875
train loss:  0.3202466368675232
train gradient:  0.11738603750276143
iteration : 9825
train acc:  0.8125
train loss:  0.40799111127853394
train gradient:  0.24719198924263353
iteration : 9826
train acc:  0.8203125
train loss:  0.3495218753814697
train gradient:  0.171329508251869
iteration : 9827
train acc:  0.828125
train loss:  0.3537876009941101
train gradient:  0.18664176529726062
iteration : 9828
train acc:  0.875
train loss:  0.29182642698287964
train gradient:  0.20521871619374615
iteration : 9829
train acc:  0.859375
train loss:  0.3113882541656494
train gradient:  0.1483103944521778
iteration : 9830
train acc:  0.859375
train loss:  0.26652273535728455
train gradient:  0.12468749272180468
iteration : 9831
train acc:  0.8515625
train loss:  0.29488998651504517
train gradient:  0.1569940505567743
iteration : 9832
train acc:  0.8671875
train loss:  0.30220282077789307
train gradient:  0.11464003748279207
iteration : 9833
train acc:  0.875
train loss:  0.28822746872901917
train gradient:  0.18895910551392484
iteration : 9834
train acc:  0.84375
train loss:  0.33258092403411865
train gradient:  0.13437592088782385
iteration : 9835
train acc:  0.921875
train loss:  0.2911151945590973
train gradient:  0.1411214604136793
iteration : 9836
train acc:  0.84375
train loss:  0.35298001766204834
train gradient:  0.17100047348739464
iteration : 9837
train acc:  0.875
train loss:  0.31241458654403687
train gradient:  0.20157429814808825
iteration : 9838
train acc:  0.859375
train loss:  0.33278918266296387
train gradient:  0.1688328917233047
iteration : 9839
train acc:  0.875
train loss:  0.3320891261100769
train gradient:  0.12058483449089978
iteration : 9840
train acc:  0.859375
train loss:  0.32819461822509766
train gradient:  0.15491760229753732
iteration : 9841
train acc:  0.8984375
train loss:  0.3015099763870239
train gradient:  0.10780023507911629
iteration : 9842
train acc:  0.8828125
train loss:  0.2900092601776123
train gradient:  0.10936797608646273
iteration : 9843
train acc:  0.875
train loss:  0.3137153685092926
train gradient:  0.11501939800060682
iteration : 9844
train acc:  0.8671875
train loss:  0.31465858221054077
train gradient:  0.15105817322525084
iteration : 9845
train acc:  0.84375
train loss:  0.3649359941482544
train gradient:  0.1748976359599318
iteration : 9846
train acc:  0.875
train loss:  0.31370168924331665
train gradient:  0.12923090215019595
iteration : 9847
train acc:  0.8515625
train loss:  0.3627066910266876
train gradient:  0.279994717949586
iteration : 9848
train acc:  0.84375
train loss:  0.41879215836524963
train gradient:  0.38541855973656297
iteration : 9849
train acc:  0.890625
train loss:  0.33488497138023376
train gradient:  0.19497350435918354
iteration : 9850
train acc:  0.828125
train loss:  0.44955873489379883
train gradient:  0.25992906307713864
iteration : 9851
train acc:  0.8671875
train loss:  0.3455268442630768
train gradient:  0.14550080353947722
iteration : 9852
train acc:  0.8828125
train loss:  0.30842140316963196
train gradient:  0.13266186864479151
iteration : 9853
train acc:  0.890625
train loss:  0.344619482755661
train gradient:  0.26280957394136584
iteration : 9854
train acc:  0.8515625
train loss:  0.35160723328590393
train gradient:  0.18529453262251255
iteration : 9855
train acc:  0.90625
train loss:  0.274543821811676
train gradient:  0.10626725692706723
iteration : 9856
train acc:  0.890625
train loss:  0.32729119062423706
train gradient:  0.19688438806231934
iteration : 9857
train acc:  0.8203125
train loss:  0.3965950906276703
train gradient:  0.2035476356098091
iteration : 9858
train acc:  0.78125
train loss:  0.46331125497817993
train gradient:  0.4248304868711368
iteration : 9859
train acc:  0.8515625
train loss:  0.3553611636161804
train gradient:  0.26636673382032205
iteration : 9860
train acc:  0.890625
train loss:  0.22745656967163086
train gradient:  0.09220353258160864
iteration : 9861
train acc:  0.8203125
train loss:  0.40549448132514954
train gradient:  0.3119027623996023
iteration : 9862
train acc:  0.84375
train loss:  0.3450549840927124
train gradient:  0.1123420897790378
iteration : 9863
train acc:  0.921875
train loss:  0.2473062127828598
train gradient:  0.09564182314307405
iteration : 9864
train acc:  0.875
train loss:  0.32969558238983154
train gradient:  0.17166215084073388
iteration : 9865
train acc:  0.84375
train loss:  0.33891361951828003
train gradient:  0.1071847456810757
iteration : 9866
train acc:  0.84375
train loss:  0.39443373680114746
train gradient:  0.22143403924930655
iteration : 9867
train acc:  0.828125
train loss:  0.35659924149513245
train gradient:  0.22615218708924623
iteration : 9868
train acc:  0.84375
train loss:  0.3411521017551422
train gradient:  0.1508399689069199
iteration : 9869
train acc:  0.828125
train loss:  0.3397276997566223
train gradient:  0.24463933329179194
iteration : 9870
train acc:  0.828125
train loss:  0.3871860206127167
train gradient:  0.17432687359039217
iteration : 9871
train acc:  0.9140625
train loss:  0.2695249021053314
train gradient:  0.11548094535349111
iteration : 9872
train acc:  0.9296875
train loss:  0.2364598959684372
train gradient:  0.1264716157294377
iteration : 9873
train acc:  0.8671875
train loss:  0.31512463092803955
train gradient:  0.1774587476619257
iteration : 9874
train acc:  0.859375
train loss:  0.301413893699646
train gradient:  0.1196427099647089
iteration : 9875
train acc:  0.875
train loss:  0.31824371218681335
train gradient:  0.13092794849352388
iteration : 9876
train acc:  0.796875
train loss:  0.3679327666759491
train gradient:  0.21178905843195417
iteration : 9877
train acc:  0.796875
train loss:  0.4149421155452728
train gradient:  0.3106719040440838
iteration : 9878
train acc:  0.8359375
train loss:  0.30432894825935364
train gradient:  0.15555633015640186
iteration : 9879
train acc:  0.8984375
train loss:  0.2636992037296295
train gradient:  0.0991596224486213
iteration : 9880
train acc:  0.828125
train loss:  0.4366911053657532
train gradient:  0.2389302975220193
iteration : 9881
train acc:  0.8359375
train loss:  0.37609195709228516
train gradient:  0.20982896081416066
iteration : 9882
train acc:  0.8515625
train loss:  0.392502099275589
train gradient:  0.23846217367113937
iteration : 9883
train acc:  0.8046875
train loss:  0.4143843352794647
train gradient:  0.21169519999055914
iteration : 9884
train acc:  0.8046875
train loss:  0.34905385971069336
train gradient:  0.1421331983624495
iteration : 9885
train acc:  0.8046875
train loss:  0.41153275966644287
train gradient:  0.2957058102658542
iteration : 9886
train acc:  0.828125
train loss:  0.3673763573169708
train gradient:  0.15994641562446493
iteration : 9887
train acc:  0.8828125
train loss:  0.3454715609550476
train gradient:  0.14202944049826358
iteration : 9888
train acc:  0.875
train loss:  0.25558608770370483
train gradient:  0.10711158134245104
iteration : 9889
train acc:  0.828125
train loss:  0.35867220163345337
train gradient:  0.1482164197362817
iteration : 9890
train acc:  0.8359375
train loss:  0.37798336148262024
train gradient:  0.23024419932273196
iteration : 9891
train acc:  0.8359375
train loss:  0.3387703001499176
train gradient:  0.12230983957961783
iteration : 9892
train acc:  0.8125
train loss:  0.428862601518631
train gradient:  0.2533341386068209
iteration : 9893
train acc:  0.8671875
train loss:  0.28766369819641113
train gradient:  0.1646099400991975
iteration : 9894
train acc:  0.890625
train loss:  0.29655009508132935
train gradient:  0.15023596264984698
iteration : 9895
train acc:  0.8359375
train loss:  0.38476282358169556
train gradient:  0.2573783758718659
iteration : 9896
train acc:  0.8671875
train loss:  0.3776835799217224
train gradient:  0.2260252410529599
iteration : 9897
train acc:  0.796875
train loss:  0.4348820745944977
train gradient:  0.33797928982180664
iteration : 9898
train acc:  0.8515625
train loss:  0.30890369415283203
train gradient:  0.1510163341118208
iteration : 9899
train acc:  0.859375
train loss:  0.33066070079803467
train gradient:  0.17444206690879488
iteration : 9900
train acc:  0.8515625
train loss:  0.32412129640579224
train gradient:  0.1287810820068087
iteration : 9901
train acc:  0.8515625
train loss:  0.30810797214508057
train gradient:  0.1549181714507271
iteration : 9902
train acc:  0.8671875
train loss:  0.3015780448913574
train gradient:  0.17046106522042087
iteration : 9903
train acc:  0.8046875
train loss:  0.35819995403289795
train gradient:  0.14332126294363265
iteration : 9904
train acc:  0.90625
train loss:  0.278446763753891
train gradient:  0.11947786997083264
iteration : 9905
train acc:  0.8203125
train loss:  0.3259207606315613
train gradient:  0.2372706915095053
iteration : 9906
train acc:  0.859375
train loss:  0.32369616627693176
train gradient:  0.12999751022729028
iteration : 9907
train acc:  0.8203125
train loss:  0.388091504573822
train gradient:  0.22840926080307794
iteration : 9908
train acc:  0.8515625
train loss:  0.3374379873275757
train gradient:  0.12838756913868243
iteration : 9909
train acc:  0.8203125
train loss:  0.3744308352470398
train gradient:  0.15745411509020377
iteration : 9910
train acc:  0.875
train loss:  0.2725776433944702
train gradient:  0.10559210720740618
iteration : 9911
train acc:  0.84375
train loss:  0.3649510443210602
train gradient:  0.13712451189282576
iteration : 9912
train acc:  0.9140625
train loss:  0.3040452003479004
train gradient:  0.12379999168144608
iteration : 9913
train acc:  0.875
train loss:  0.2931734621524811
train gradient:  0.17199600053048786
iteration : 9914
train acc:  0.875
train loss:  0.32157909870147705
train gradient:  0.21030115591216658
iteration : 9915
train acc:  0.8046875
train loss:  0.39096200466156006
train gradient:  0.18039553029502425
iteration : 9916
train acc:  0.875
train loss:  0.2735751271247864
train gradient:  0.11634456148323272
iteration : 9917
train acc:  0.8203125
train loss:  0.3858122229576111
train gradient:  0.3112231658355514
iteration : 9918
train acc:  0.7890625
train loss:  0.43330004811286926
train gradient:  0.26281215542967645
iteration : 9919
train acc:  0.8984375
train loss:  0.27261197566986084
train gradient:  0.18769520159913716
iteration : 9920
train acc:  0.8671875
train loss:  0.28124547004699707
train gradient:  0.13049738680158213
iteration : 9921
train acc:  0.921875
train loss:  0.25178098678588867
train gradient:  0.10233307679275346
iteration : 9922
train acc:  0.859375
train loss:  0.28634271025657654
train gradient:  0.10264693832128031
iteration : 9923
train acc:  0.875
train loss:  0.28731316328048706
train gradient:  0.13486766105621956
iteration : 9924
train acc:  0.8515625
train loss:  0.356780469417572
train gradient:  0.24931394416254282
iteration : 9925
train acc:  0.875
train loss:  0.29853713512420654
train gradient:  0.1231789804070051
iteration : 9926
train acc:  0.90625
train loss:  0.27398744225502014
train gradient:  0.14399631392525777
iteration : 9927
train acc:  0.8515625
train loss:  0.2848441004753113
train gradient:  0.10108160263662798
iteration : 9928
train acc:  0.828125
train loss:  0.37067484855651855
train gradient:  0.18655999770790438
iteration : 9929
train acc:  0.8984375
train loss:  0.3179493546485901
train gradient:  0.16062709235216072
iteration : 9930
train acc:  0.8671875
train loss:  0.3444875180721283
train gradient:  0.12993245571625153
iteration : 9931
train acc:  0.859375
train loss:  0.3373047709465027
train gradient:  0.11698351806208385
iteration : 9932
train acc:  0.8671875
train loss:  0.29228895902633667
train gradient:  0.10004392554412007
iteration : 9933
train acc:  0.890625
train loss:  0.2811865210533142
train gradient:  0.1211430908419988
iteration : 9934
train acc:  0.875
train loss:  0.34852302074432373
train gradient:  0.2888121853335831
iteration : 9935
train acc:  0.796875
train loss:  0.359394371509552
train gradient:  0.21623659344819207
iteration : 9936
train acc:  0.8359375
train loss:  0.4206271767616272
train gradient:  0.23944471328948402
iteration : 9937
train acc:  0.921875
train loss:  0.24051332473754883
train gradient:  0.15048939061072397
iteration : 9938
train acc:  0.828125
train loss:  0.34508782625198364
train gradient:  0.12814059945673578
iteration : 9939
train acc:  0.890625
train loss:  0.2791687250137329
train gradient:  0.15326015280262667
iteration : 9940
train acc:  0.796875
train loss:  0.4411182105541229
train gradient:  0.24609904844923086
iteration : 9941
train acc:  0.9140625
train loss:  0.28548702597618103
train gradient:  0.1252810391275645
iteration : 9942
train acc:  0.8515625
train loss:  0.3199235498905182
train gradient:  0.1108962628736107
iteration : 9943
train acc:  0.8203125
train loss:  0.36352187395095825
train gradient:  0.18495422500246314
iteration : 9944
train acc:  0.8984375
train loss:  0.31960880756378174
train gradient:  0.12678776430615885
iteration : 9945
train acc:  0.8671875
train loss:  0.32104796171188354
train gradient:  0.22369042806342043
iteration : 9946
train acc:  0.8359375
train loss:  0.4130744934082031
train gradient:  0.20664710556471722
iteration : 9947
train acc:  0.8671875
train loss:  0.3054868280887604
train gradient:  0.14196927148325444
iteration : 9948
train acc:  0.90625
train loss:  0.20827791094779968
train gradient:  0.10639916033763472
iteration : 9949
train acc:  0.8359375
train loss:  0.3216935098171234
train gradient:  0.132318482430945
iteration : 9950
train acc:  0.8515625
train loss:  0.3424603343009949
train gradient:  0.15418599571826708
iteration : 9951
train acc:  0.8671875
train loss:  0.31298744678497314
train gradient:  0.17180202581221488
iteration : 9952
train acc:  0.921875
train loss:  0.21242009103298187
train gradient:  0.10810369296955094
iteration : 9953
train acc:  0.8671875
train loss:  0.3301354646682739
train gradient:  0.16697953633768395
iteration : 9954
train acc:  0.8046875
train loss:  0.4171558618545532
train gradient:  0.22392160433128944
iteration : 9955
train acc:  0.859375
train loss:  0.2921147644519806
train gradient:  0.15712308045229784
iteration : 9956
train acc:  0.84375
train loss:  0.40388673543930054
train gradient:  0.16079720943757395
iteration : 9957
train acc:  0.8359375
train loss:  0.40783435106277466
train gradient:  0.3515261833340596
iteration : 9958
train acc:  0.875
train loss:  0.31703275442123413
train gradient:  0.14508869581956163
iteration : 9959
train acc:  0.828125
train loss:  0.37931710481643677
train gradient:  0.17786542413139045
iteration : 9960
train acc:  0.859375
train loss:  0.33342403173446655
train gradient:  0.18905621015260204
iteration : 9961
train acc:  0.8828125
train loss:  0.3072933256626129
train gradient:  0.1744333237103433
iteration : 9962
train acc:  0.890625
train loss:  0.2515159249305725
train gradient:  0.10882163946696981
iteration : 9963
train acc:  0.859375
train loss:  0.31202465295791626
train gradient:  0.18601609923604423
iteration : 9964
train acc:  0.796875
train loss:  0.42224276065826416
train gradient:  0.2950481360960499
iteration : 9965
train acc:  0.8828125
train loss:  0.28383100032806396
train gradient:  0.10153324916530988
iteration : 9966
train acc:  0.90625
train loss:  0.26558202505111694
train gradient:  0.12521367201129133
iteration : 9967
train acc:  0.8515625
train loss:  0.3649260997772217
train gradient:  0.21704664347452662
iteration : 9968
train acc:  0.7890625
train loss:  0.37315428256988525
train gradient:  0.16068299553968263
iteration : 9969
train acc:  0.78125
train loss:  0.42232322692871094
train gradient:  0.2595742791753345
iteration : 9970
train acc:  0.875
train loss:  0.29529696702957153
train gradient:  0.10687762787614329
iteration : 9971
train acc:  0.890625
train loss:  0.23827213048934937
train gradient:  0.14461676847247473
iteration : 9972
train acc:  0.828125
train loss:  0.3766322433948517
train gradient:  0.2822104564056688
iteration : 9973
train acc:  0.8671875
train loss:  0.3211761713027954
train gradient:  0.17738184655617664
iteration : 9974
train acc:  0.84375
train loss:  0.34150975942611694
train gradient:  0.15991082255760375
iteration : 9975
train acc:  0.8359375
train loss:  0.36650601029396057
train gradient:  0.2330080332033161
iteration : 9976
train acc:  0.875
train loss:  0.3221516013145447
train gradient:  0.13186750996490273
iteration : 9977
train acc:  0.859375
train loss:  0.37049391865730286
train gradient:  0.15535383223721175
iteration : 9978
train acc:  0.859375
train loss:  0.32641878724098206
train gradient:  0.15410354790992364
iteration : 9979
train acc:  0.859375
train loss:  0.3229147493839264
train gradient:  0.1604785542571559
iteration : 9980
train acc:  0.859375
train loss:  0.3835172951221466
train gradient:  0.17539498977710236
iteration : 9981
train acc:  0.8984375
train loss:  0.2944813370704651
train gradient:  0.11155782215099543
iteration : 9982
train acc:  0.875
train loss:  0.3228321671485901
train gradient:  0.12224171459960571
iteration : 9983
train acc:  0.8828125
train loss:  0.23181381821632385
train gradient:  0.08045001065575826
iteration : 9984
train acc:  0.84375
train loss:  0.36024975776672363
train gradient:  0.1566140978070836
iteration : 9985
train acc:  0.875
train loss:  0.33518120646476746
train gradient:  0.11374483524656354
iteration : 9986
train acc:  0.890625
train loss:  0.29264315962791443
train gradient:  0.20500035753498896
iteration : 9987
train acc:  0.8359375
train loss:  0.2843010425567627
train gradient:  0.14270887483562894
iteration : 9988
train acc:  0.875
train loss:  0.27670973539352417
train gradient:  0.1543710217335717
iteration : 9989
train acc:  0.7734375
train loss:  0.4599916934967041
train gradient:  0.21745943769793194
iteration : 9990
train acc:  0.84375
train loss:  0.3352172076702118
train gradient:  0.18066237022185283
iteration : 9991
train acc:  0.8671875
train loss:  0.32023194432258606
train gradient:  0.16439422358359618
iteration : 9992
train acc:  0.84375
train loss:  0.3797379732131958
train gradient:  0.2382664370068434
iteration : 9993
train acc:  0.8359375
train loss:  0.3267677128314972
train gradient:  0.18171520593914403
iteration : 9994
train acc:  0.859375
train loss:  0.3370649814605713
train gradient:  0.13086902967910774
iteration : 9995
train acc:  0.90625
train loss:  0.3046913146972656
train gradient:  0.1371589341357357
iteration : 9996
train acc:  0.84375
train loss:  0.28056925535202026
train gradient:  0.13712733404367955
iteration : 9997
train acc:  0.84375
train loss:  0.2980009615421295
train gradient:  0.16030978996863093
iteration : 9998
train acc:  0.84375
train loss:  0.35183584690093994
train gradient:  0.1898081966039018
iteration : 9999
train acc:  0.78125
train loss:  0.4333111643791199
train gradient:  0.27335270979352344
iteration : 10000
train acc:  0.8515625
train loss:  0.29256725311279297
train gradient:  0.1179381770337464
iteration : 10001
train acc:  0.8671875
train loss:  0.29692816734313965
train gradient:  0.10549726117489817
iteration : 10002
train acc:  0.8515625
train loss:  0.34150782227516174
train gradient:  0.17098160907015364
iteration : 10003
train acc:  0.8203125
train loss:  0.33980944752693176
train gradient:  0.19424735136678323
iteration : 10004
train acc:  0.84375
train loss:  0.35724449157714844
train gradient:  0.16333534110886566
iteration : 10005
train acc:  0.828125
train loss:  0.33866748213768005
train gradient:  0.12347539572738978
iteration : 10006
train acc:  0.7890625
train loss:  0.36501356959342957
train gradient:  0.17830643832930557
iteration : 10007
train acc:  0.84375
train loss:  0.32507872581481934
train gradient:  0.13879222299418986
iteration : 10008
train acc:  0.828125
train loss:  0.3529985547065735
train gradient:  0.1538696224983848
iteration : 10009
train acc:  0.9296875
train loss:  0.2801251709461212
train gradient:  0.16332378080567794
iteration : 10010
train acc:  0.8828125
train loss:  0.27006471157073975
train gradient:  0.09997876961845663
iteration : 10011
train acc:  0.8359375
train loss:  0.3566548824310303
train gradient:  0.18285228935523942
iteration : 10012
train acc:  0.875
train loss:  0.3031945824623108
train gradient:  0.12418797231749432
iteration : 10013
train acc:  0.859375
train loss:  0.3253183662891388
train gradient:  0.1231344048203583
iteration : 10014
train acc:  0.8203125
train loss:  0.3812626302242279
train gradient:  0.19958282365920083
iteration : 10015
train acc:  0.8359375
train loss:  0.35288286209106445
train gradient:  0.14877117984704158
iteration : 10016
train acc:  0.8984375
train loss:  0.3099314868450165
train gradient:  0.12969610854637786
iteration : 10017
train acc:  0.8828125
train loss:  0.30962640047073364
train gradient:  0.1400376708221345
iteration : 10018
train acc:  0.8515625
train loss:  0.34384921193122864
train gradient:  0.13887640268523138
iteration : 10019
train acc:  0.8828125
train loss:  0.32395654916763306
train gradient:  0.14385751413264636
iteration : 10020
train acc:  0.875
train loss:  0.3238728642463684
train gradient:  0.153814681350813
iteration : 10021
train acc:  0.9140625
train loss:  0.25085604190826416
train gradient:  0.08920693077747298
iteration : 10022
train acc:  0.859375
train loss:  0.32177090644836426
train gradient:  0.17299890821314515
iteration : 10023
train acc:  0.8671875
train loss:  0.30254825949668884
train gradient:  0.1471712661814601
iteration : 10024
train acc:  0.8984375
train loss:  0.31908565759658813
train gradient:  0.11509096192075773
iteration : 10025
train acc:  0.8359375
train loss:  0.28734588623046875
train gradient:  0.09750107556002165
iteration : 10026
train acc:  0.8515625
train loss:  0.32265570759773254
train gradient:  0.16748531253211468
iteration : 10027
train acc:  0.8671875
train loss:  0.3041146695613861
train gradient:  0.18333605190469135
iteration : 10028
train acc:  0.8828125
train loss:  0.31285107135772705
train gradient:  0.1288835867101501
iteration : 10029
train acc:  0.859375
train loss:  0.304922878742218
train gradient:  0.15421954474942134
iteration : 10030
train acc:  0.921875
train loss:  0.23920197784900665
train gradient:  0.10762611133064155
iteration : 10031
train acc:  0.859375
train loss:  0.2606687843799591
train gradient:  0.11131384250365464
iteration : 10032
train acc:  0.859375
train loss:  0.28352004289627075
train gradient:  0.1906562669953617
iteration : 10033
train acc:  0.8828125
train loss:  0.28940990567207336
train gradient:  0.12709911937218904
iteration : 10034
train acc:  0.8359375
train loss:  0.3114946782588959
train gradient:  0.11477626490045531
iteration : 10035
train acc:  0.8671875
train loss:  0.2933703660964966
train gradient:  0.11613992764473964
iteration : 10036
train acc:  0.890625
train loss:  0.2798853814601898
train gradient:  0.15174850262775133
iteration : 10037
train acc:  0.8828125
train loss:  0.28594931960105896
train gradient:  0.16834182675139908
iteration : 10038
train acc:  0.84375
train loss:  0.4115939438343048
train gradient:  0.22917725463665303
iteration : 10039
train acc:  0.8671875
train loss:  0.29238760471343994
train gradient:  0.14771478627476445
iteration : 10040
train acc:  0.859375
train loss:  0.3319082260131836
train gradient:  0.1426268106280868
iteration : 10041
train acc:  0.8828125
train loss:  0.27007654309272766
train gradient:  0.11834745453305268
iteration : 10042
train acc:  0.84375
train loss:  0.34381240606307983
train gradient:  0.34866883377336305
iteration : 10043
train acc:  0.875
train loss:  0.3042357563972473
train gradient:  0.16480144431139193
iteration : 10044
train acc:  0.7734375
train loss:  0.4683459401130676
train gradient:  0.38336375216989
iteration : 10045
train acc:  0.828125
train loss:  0.3216574490070343
train gradient:  0.1567304821622481
iteration : 10046
train acc:  0.8671875
train loss:  0.3213263750076294
train gradient:  0.11315081805207061
iteration : 10047
train acc:  0.8515625
train loss:  0.31681519746780396
train gradient:  0.16512709499280986
iteration : 10048
train acc:  0.859375
train loss:  0.3006504774093628
train gradient:  0.14913238558931297
iteration : 10049
train acc:  0.84375
train loss:  0.3536912798881531
train gradient:  0.22164101338262215
iteration : 10050
train acc:  0.8671875
train loss:  0.34210172295570374
train gradient:  0.15785548070129074
iteration : 10051
train acc:  0.828125
train loss:  0.4220476448535919
train gradient:  0.20863498593349222
iteration : 10052
train acc:  0.828125
train loss:  0.3776357173919678
train gradient:  0.17481670391789958
iteration : 10053
train acc:  0.8828125
train loss:  0.30675050616264343
train gradient:  0.17906473354404845
iteration : 10054
train acc:  0.8203125
train loss:  0.38948845863342285
train gradient:  0.20855589697152993
iteration : 10055
train acc:  0.8671875
train loss:  0.28661948442459106
train gradient:  0.1443483801077247
iteration : 10056
train acc:  0.9296875
train loss:  0.23081032931804657
train gradient:  0.09338796964416268
iteration : 10057
train acc:  0.8515625
train loss:  0.3288930654525757
train gradient:  0.1527668642056556
iteration : 10058
train acc:  0.890625
train loss:  0.3002444803714752
train gradient:  0.14098367231344233
iteration : 10059
train acc:  0.8125
train loss:  0.3625114858150482
train gradient:  0.1609144305733486
iteration : 10060
train acc:  0.828125
train loss:  0.34046077728271484
train gradient:  0.1561725252587774
iteration : 10061
train acc:  0.8359375
train loss:  0.3060123324394226
train gradient:  0.19976062703612582
iteration : 10062
train acc:  0.84375
train loss:  0.37045177817344666
train gradient:  0.20456856362058312
iteration : 10063
train acc:  0.859375
train loss:  0.3354385495185852
train gradient:  0.15119661313025762
iteration : 10064
train acc:  0.8671875
train loss:  0.2557557225227356
train gradient:  0.12007841558228387
iteration : 10065
train acc:  0.90625
train loss:  0.26988857984542847
train gradient:  0.13365535821580804
iteration : 10066
train acc:  0.7734375
train loss:  0.4188263714313507
train gradient:  0.27347375220653153
iteration : 10067
train acc:  0.828125
train loss:  0.3440645933151245
train gradient:  0.20572633083507205
iteration : 10068
train acc:  0.8671875
train loss:  0.3508150577545166
train gradient:  0.1544195313310918
iteration : 10069
train acc:  0.765625
train loss:  0.5082188844680786
train gradient:  0.3868647013070847
iteration : 10070
train acc:  0.921875
train loss:  0.21005898714065552
train gradient:  0.07790419209966025
iteration : 10071
train acc:  0.8671875
train loss:  0.29188424348831177
train gradient:  0.15370803888853113
iteration : 10072
train acc:  0.8203125
train loss:  0.343159019947052
train gradient:  0.16405843382571866
iteration : 10073
train acc:  0.8671875
train loss:  0.3116491436958313
train gradient:  0.1421579498018592
iteration : 10074
train acc:  0.890625
train loss:  0.31061652302742004
train gradient:  0.16124833117892012
iteration : 10075
train acc:  0.828125
train loss:  0.38805651664733887
train gradient:  0.23473046081798532
iteration : 10076
train acc:  0.8671875
train loss:  0.31119048595428467
train gradient:  0.15783381139020491
iteration : 10077
train acc:  0.859375
train loss:  0.33632034063339233
train gradient:  0.17370415895537766
iteration : 10078
train acc:  0.8515625
train loss:  0.29971879720687866
train gradient:  0.1781152701060139
iteration : 10079
train acc:  0.84375
train loss:  0.35473501682281494
train gradient:  0.1492667117739364
iteration : 10080
train acc:  0.8515625
train loss:  0.3170677125453949
train gradient:  0.13070683094012955
iteration : 10081
train acc:  0.8125
train loss:  0.42686784267425537
train gradient:  0.2546204005401747
iteration : 10082
train acc:  0.8828125
train loss:  0.3404430150985718
train gradient:  0.20360438268734574
iteration : 10083
train acc:  0.875
train loss:  0.28170865774154663
train gradient:  0.12502759444297334
iteration : 10084
train acc:  0.8671875
train loss:  0.32687148451805115
train gradient:  0.1410441523265237
iteration : 10085
train acc:  0.859375
train loss:  0.29003554582595825
train gradient:  0.13437236029281538
iteration : 10086
train acc:  0.875
train loss:  0.26184752583503723
train gradient:  0.1290526056863819
iteration : 10087
train acc:  0.859375
train loss:  0.304718554019928
train gradient:  0.11815724974173689
iteration : 10088
train acc:  0.8828125
train loss:  0.30529680848121643
train gradient:  0.11531766063055922
iteration : 10089
train acc:  0.84375
train loss:  0.3832382559776306
train gradient:  0.28763669785688223
iteration : 10090
train acc:  0.8359375
train loss:  0.32722219824790955
train gradient:  0.1714026033517129
iteration : 10091
train acc:  0.8671875
train loss:  0.2917325496673584
train gradient:  0.15069241257003102
iteration : 10092
train acc:  0.90625
train loss:  0.27639082074165344
train gradient:  0.15924890811343329
iteration : 10093
train acc:  0.9296875
train loss:  0.26299476623535156
train gradient:  0.1015314122112023
iteration : 10094
train acc:  0.8984375
train loss:  0.3168467581272125
train gradient:  0.14506798852291575
iteration : 10095
train acc:  0.8515625
train loss:  0.3321215510368347
train gradient:  0.17883422103148772
iteration : 10096
train acc:  0.84375
train loss:  0.2987169027328491
train gradient:  0.16971374227992608
iteration : 10097
train acc:  0.859375
train loss:  0.34512171149253845
train gradient:  0.15383990740825848
iteration : 10098
train acc:  0.8671875
train loss:  0.3153103291988373
train gradient:  0.24144474345307693
iteration : 10099
train acc:  0.8359375
train loss:  0.32968246936798096
train gradient:  0.30984224641272046
iteration : 10100
train acc:  0.84375
train loss:  0.327736496925354
train gradient:  0.20077232860258576
iteration : 10101
train acc:  0.8125
train loss:  0.47581928968429565
train gradient:  0.2250209566315558
iteration : 10102
train acc:  0.828125
train loss:  0.3774220049381256
train gradient:  0.22066173916757786
iteration : 10103
train acc:  0.859375
train loss:  0.3538742661476135
train gradient:  0.20739894072325077
iteration : 10104
train acc:  0.828125
train loss:  0.340053915977478
train gradient:  0.16105803110566586
iteration : 10105
train acc:  0.828125
train loss:  0.411824107170105
train gradient:  0.24544731181829166
iteration : 10106
train acc:  0.8046875
train loss:  0.3702109456062317
train gradient:  0.2205335051833723
iteration : 10107
train acc:  0.8046875
train loss:  0.3744613826274872
train gradient:  0.23617869352980217
iteration : 10108
train acc:  0.921875
train loss:  0.24478274583816528
train gradient:  0.12970661648254772
iteration : 10109
train acc:  0.796875
train loss:  0.45008471608161926
train gradient:  0.28846520221399213
iteration : 10110
train acc:  0.859375
train loss:  0.27958500385284424
train gradient:  0.09872470794390262
iteration : 10111
train acc:  0.859375
train loss:  0.33478593826293945
train gradient:  0.1692052110172044
iteration : 10112
train acc:  0.8515625
train loss:  0.30917152762413025
train gradient:  0.1691471969448009
iteration : 10113
train acc:  0.8359375
train loss:  0.35452398657798767
train gradient:  0.19087675292718453
iteration : 10114
train acc:  0.8359375
train loss:  0.3191130757331848
train gradient:  0.1652864677442016
iteration : 10115
train acc:  0.8125
train loss:  0.41294410824775696
train gradient:  0.21190451495475826
iteration : 10116
train acc:  0.859375
train loss:  0.30800822377204895
train gradient:  0.16866181098175137
iteration : 10117
train acc:  0.890625
train loss:  0.265728235244751
train gradient:  0.12645768178848893
iteration : 10118
train acc:  0.859375
train loss:  0.3431859016418457
train gradient:  0.25134037538017273
iteration : 10119
train acc:  0.8046875
train loss:  0.4346799850463867
train gradient:  0.2818217829504662
iteration : 10120
train acc:  0.8359375
train loss:  0.3707157075405121
train gradient:  0.18837646923286128
iteration : 10121
train acc:  0.859375
train loss:  0.32708173990249634
train gradient:  0.13405993890735554
iteration : 10122
train acc:  0.8515625
train loss:  0.4316972494125366
train gradient:  0.2842913950476188
iteration : 10123
train acc:  0.8515625
train loss:  0.3001912832260132
train gradient:  0.14573526243354534
iteration : 10124
train acc:  0.90625
train loss:  0.22621086239814758
train gradient:  0.10981079396205584
iteration : 10125
train acc:  0.8359375
train loss:  0.379684180021286
train gradient:  0.1683681142322528
iteration : 10126
train acc:  0.8984375
train loss:  0.2451583743095398
train gradient:  0.09962991994595397
iteration : 10127
train acc:  0.859375
train loss:  0.32639652490615845
train gradient:  0.141766549095715
iteration : 10128
train acc:  0.84375
train loss:  0.37297362089157104
train gradient:  0.1553470490963985
iteration : 10129
train acc:  0.890625
train loss:  0.3164128065109253
train gradient:  0.17079722701957006
iteration : 10130
train acc:  0.8828125
train loss:  0.30237555503845215
train gradient:  0.16926496965407617
iteration : 10131
train acc:  0.8203125
train loss:  0.3252127766609192
train gradient:  0.1731285000148507
iteration : 10132
train acc:  0.828125
train loss:  0.3781875669956207
train gradient:  0.16585229367141785
iteration : 10133
train acc:  0.8828125
train loss:  0.3132796883583069
train gradient:  0.12086747117730032
iteration : 10134
train acc:  0.8828125
train loss:  0.29275208711624146
train gradient:  0.17648571642669048
iteration : 10135
train acc:  0.890625
train loss:  0.27336156368255615
train gradient:  0.1553088852171682
iteration : 10136
train acc:  0.90625
train loss:  0.24796229600906372
train gradient:  0.11744124400804686
iteration : 10137
train acc:  0.8515625
train loss:  0.3258861303329468
train gradient:  0.11368275641522456
iteration : 10138
train acc:  0.859375
train loss:  0.33747345209121704
train gradient:  0.14952179444560776
iteration : 10139
train acc:  0.90625
train loss:  0.28841522336006165
train gradient:  0.16146338149983916
iteration : 10140
train acc:  0.8359375
train loss:  0.35845625400543213
train gradient:  0.13853465118904226
iteration : 10141
train acc:  0.8828125
train loss:  0.2830324172973633
train gradient:  0.1166495329111327
iteration : 10142
train acc:  0.9140625
train loss:  0.2723173499107361
train gradient:  0.1614895778619069
iteration : 10143
train acc:  0.890625
train loss:  0.3027798533439636
train gradient:  0.13425097346748202
iteration : 10144
train acc:  0.8671875
train loss:  0.3598792850971222
train gradient:  0.1600195060242492
iteration : 10145
train acc:  0.8984375
train loss:  0.24795162677764893
train gradient:  0.10161208162269321
iteration : 10146
train acc:  0.8515625
train loss:  0.2992416024208069
train gradient:  0.10499542402717875
iteration : 10147
train acc:  0.90625
train loss:  0.25970399379730225
train gradient:  0.12106203369900184
iteration : 10148
train acc:  0.828125
train loss:  0.3878162205219269
train gradient:  0.24459572168559074
iteration : 10149
train acc:  0.8515625
train loss:  0.3856547474861145
train gradient:  0.17138138949432133
iteration : 10150
train acc:  0.90625
train loss:  0.24515528976917267
train gradient:  0.12453705574745538
iteration : 10151
train acc:  0.84375
train loss:  0.30809736251831055
train gradient:  0.13805465290383317
iteration : 10152
train acc:  0.8515625
train loss:  0.3003298342227936
train gradient:  0.19228480481421933
iteration : 10153
train acc:  0.9140625
train loss:  0.2714579105377197
train gradient:  0.14241770837465373
iteration : 10154
train acc:  0.921875
train loss:  0.2506932020187378
train gradient:  0.10553119233285772
iteration : 10155
train acc:  0.859375
train loss:  0.344629168510437
train gradient:  0.18422190421233514
iteration : 10156
train acc:  0.875
train loss:  0.30597859621047974
train gradient:  0.20679148173378356
iteration : 10157
train acc:  0.8515625
train loss:  0.3302040100097656
train gradient:  0.12671112850130287
iteration : 10158
train acc:  0.8359375
train loss:  0.35175588726997375
train gradient:  0.23132319628997067
iteration : 10159
train acc:  0.8828125
train loss:  0.331756055355072
train gradient:  0.10087442795456147
iteration : 10160
train acc:  0.8046875
train loss:  0.41099101305007935
train gradient:  0.34617654085162775
iteration : 10161
train acc:  0.8671875
train loss:  0.3079717755317688
train gradient:  0.14397175608075077
iteration : 10162
train acc:  0.875
train loss:  0.27641576528549194
train gradient:  0.12454392399968557
iteration : 10163
train acc:  0.8828125
train loss:  0.29215019941329956
train gradient:  0.16850356681329667
iteration : 10164
train acc:  0.890625
train loss:  0.2810819745063782
train gradient:  0.14922123105440052
iteration : 10165
train acc:  0.90625
train loss:  0.24882671236991882
train gradient:  0.12283078275281453
iteration : 10166
train acc:  0.8515625
train loss:  0.3197590410709381
train gradient:  0.21164373702701278
iteration : 10167
train acc:  0.859375
train loss:  0.30785563588142395
train gradient:  0.13251429176152868
iteration : 10168
train acc:  0.90625
train loss:  0.28364866971969604
train gradient:  0.11955158754651436
iteration : 10169
train acc:  0.8671875
train loss:  0.32012221217155457
train gradient:  0.16140266270399664
iteration : 10170
train acc:  0.859375
train loss:  0.36162862181663513
train gradient:  0.14594245504822095
iteration : 10171
train acc:  0.8515625
train loss:  0.2758125066757202
train gradient:  0.17597020924797935
iteration : 10172
train acc:  0.84375
train loss:  0.345566064119339
train gradient:  0.2136713347464524
iteration : 10173
train acc:  0.8828125
train loss:  0.31997352838516235
train gradient:  0.1613185252845975
iteration : 10174
train acc:  0.828125
train loss:  0.3850429356098175
train gradient:  0.2227514563783205
iteration : 10175
train acc:  0.8671875
train loss:  0.31190258264541626
train gradient:  0.1606688419640778
iteration : 10176
train acc:  0.84375
train loss:  0.31466859579086304
train gradient:  0.17311096332086884
iteration : 10177
train acc:  0.859375
train loss:  0.3540608286857605
train gradient:  0.19813833450077015
iteration : 10178
train acc:  0.8828125
train loss:  0.3133387267589569
train gradient:  0.15628474145783425
iteration : 10179
train acc:  0.8046875
train loss:  0.39832600951194763
train gradient:  0.2615402494715318
iteration : 10180
train acc:  0.875
train loss:  0.278854101896286
train gradient:  0.14361693215713542
iteration : 10181
train acc:  0.859375
train loss:  0.32678645849227905
train gradient:  0.1803799760195328
iteration : 10182
train acc:  0.84375
train loss:  0.33247649669647217
train gradient:  0.15420177621043496
iteration : 10183
train acc:  0.8671875
train loss:  0.2951369881629944
train gradient:  0.16529869220080518
iteration : 10184
train acc:  0.8671875
train loss:  0.3168158233165741
train gradient:  0.17948998953728906
iteration : 10185
train acc:  0.8203125
train loss:  0.3301962614059448
train gradient:  0.17006824808998897
iteration : 10186
train acc:  0.859375
train loss:  0.26279711723327637
train gradient:  0.12251130995476867
iteration : 10187
train acc:  0.84375
train loss:  0.2976509928703308
train gradient:  0.14220512167505478
iteration : 10188
train acc:  0.921875
train loss:  0.20674549043178558
train gradient:  0.07944555339199116
iteration : 10189
train acc:  0.8359375
train loss:  0.3049312233924866
train gradient:  0.1385231291254998
iteration : 10190
train acc:  0.8125
train loss:  0.3631507158279419
train gradient:  0.1607086434836755
iteration : 10191
train acc:  0.8671875
train loss:  0.33128464221954346
train gradient:  0.1562698949097865
iteration : 10192
train acc:  0.9140625
train loss:  0.23787924647331238
train gradient:  0.09976974369730138
iteration : 10193
train acc:  0.875
train loss:  0.2907789945602417
train gradient:  0.19706019485347548
iteration : 10194
train acc:  0.859375
train loss:  0.3008316457271576
train gradient:  0.11974852542995663
iteration : 10195
train acc:  0.859375
train loss:  0.31635814905166626
train gradient:  0.1616770023598264
iteration : 10196
train acc:  0.8671875
train loss:  0.30474576354026794
train gradient:  0.32363918822299864
iteration : 10197
train acc:  0.9140625
train loss:  0.24000097811222076
train gradient:  0.1569739163092737
iteration : 10198
train acc:  0.8984375
train loss:  0.3128722310066223
train gradient:  0.14874903753658225
iteration : 10199
train acc:  0.9375
train loss:  0.20940792560577393
train gradient:  0.0876967239013946
iteration : 10200
train acc:  0.90625
train loss:  0.26108667254447937
train gradient:  0.10029815034476847
iteration : 10201
train acc:  0.8515625
train loss:  0.362143874168396
train gradient:  0.17277561834763666
iteration : 10202
train acc:  0.84375
train loss:  0.36846956610679626
train gradient:  0.23175769733786378
iteration : 10203
train acc:  0.875
train loss:  0.36520546674728394
train gradient:  0.15036777516869634
iteration : 10204
train acc:  0.890625
train loss:  0.2529612183570862
train gradient:  0.12806843634892845
iteration : 10205
train acc:  0.8671875
train loss:  0.29617828130722046
train gradient:  0.13767230914298062
iteration : 10206
train acc:  0.8203125
train loss:  0.3926721513271332
train gradient:  0.18520202265501554
iteration : 10207
train acc:  0.84375
train loss:  0.31463196873664856
train gradient:  0.2249038911369457
iteration : 10208
train acc:  0.890625
train loss:  0.30309855937957764
train gradient:  0.1984991300902097
iteration : 10209
train acc:  0.84375
train loss:  0.36336207389831543
train gradient:  0.15908895980852922
iteration : 10210
train acc:  0.8828125
train loss:  0.26695120334625244
train gradient:  0.11519080414055036
iteration : 10211
train acc:  0.890625
train loss:  0.2633154094219208
train gradient:  0.1587146951556706
iteration : 10212
train acc:  0.8515625
train loss:  0.32064902782440186
train gradient:  0.13123888798226857
iteration : 10213
train acc:  0.84375
train loss:  0.33453473448753357
train gradient:  0.20478918416532407
iteration : 10214
train acc:  0.7734375
train loss:  0.4276542067527771
train gradient:  0.30970270875828243
iteration : 10215
train acc:  0.8671875
train loss:  0.3049854636192322
train gradient:  0.14807092208164738
iteration : 10216
train acc:  0.859375
train loss:  0.37034928798675537
train gradient:  0.18380569561271595
iteration : 10217
train acc:  0.84375
train loss:  0.4383048713207245
train gradient:  0.21209290186822638
iteration : 10218
train acc:  0.875
train loss:  0.27090275287628174
train gradient:  0.13133858409894814
iteration : 10219
train acc:  0.8359375
train loss:  0.3266657590866089
train gradient:  0.16231214374546749
iteration : 10220
train acc:  0.8203125
train loss:  0.41047948598861694
train gradient:  0.2910673457904525
iteration : 10221
train acc:  0.8671875
train loss:  0.28503841161727905
train gradient:  0.15310000690477024
iteration : 10222
train acc:  0.8984375
train loss:  0.2308516949415207
train gradient:  0.0784561505799633
iteration : 10223
train acc:  0.8359375
train loss:  0.338111013174057
train gradient:  0.20364270009686883
iteration : 10224
train acc:  0.84375
train loss:  0.40354400873184204
train gradient:  0.2669929432059449
iteration : 10225
train acc:  0.890625
train loss:  0.2628920376300812
train gradient:  0.13245678840872163
iteration : 10226
train acc:  0.8671875
train loss:  0.37276554107666016
train gradient:  0.23030929247606968
iteration : 10227
train acc:  0.8828125
train loss:  0.3319920301437378
train gradient:  0.1684093005615438
iteration : 10228
train acc:  0.7421875
train loss:  0.4511006474494934
train gradient:  0.2736844006131441
iteration : 10229
train acc:  0.796875
train loss:  0.3841693103313446
train gradient:  0.20405741411626127
iteration : 10230
train acc:  0.8515625
train loss:  0.32372498512268066
train gradient:  0.18266912425185583
iteration : 10231
train acc:  0.8671875
train loss:  0.2947574555873871
train gradient:  0.18806431636953624
iteration : 10232
train acc:  0.875
train loss:  0.26217103004455566
train gradient:  0.12315299587859725
iteration : 10233
train acc:  0.875
train loss:  0.34265658259391785
train gradient:  0.22073773697390298
iteration : 10234
train acc:  0.8515625
train loss:  0.28448063135147095
train gradient:  0.14848859361299657
iteration : 10235
train acc:  0.8359375
train loss:  0.45394667983055115
train gradient:  0.2204221472630237
iteration : 10236
train acc:  0.8671875
train loss:  0.28656667470932007
train gradient:  0.13644977422070353
iteration : 10237
train acc:  0.8984375
train loss:  0.24114449322223663
train gradient:  0.11076906985250537
iteration : 10238
train acc:  0.859375
train loss:  0.342060923576355
train gradient:  0.19402695840502338
iteration : 10239
train acc:  0.859375
train loss:  0.3345399498939514
train gradient:  0.17140696862185323
iteration : 10240
train acc:  0.859375
train loss:  0.30835217237472534
train gradient:  0.17853193578699125
iteration : 10241
train acc:  0.8046875
train loss:  0.37167394161224365
train gradient:  0.2374659129033937
iteration : 10242
train acc:  0.8359375
train loss:  0.35687845945358276
train gradient:  0.16204546103286882
iteration : 10243
train acc:  0.8125
train loss:  0.348305881023407
train gradient:  0.21436320884570845
iteration : 10244
train acc:  0.859375
train loss:  0.3300873637199402
train gradient:  0.19645993979981477
iteration : 10245
train acc:  0.8125
train loss:  0.3729921579360962
train gradient:  0.31729481045698577
iteration : 10246
train acc:  0.8671875
train loss:  0.37102729082107544
train gradient:  0.16034498688219379
iteration : 10247
train acc:  0.875
train loss:  0.3534994125366211
train gradient:  0.16564610034363034
iteration : 10248
train acc:  0.828125
train loss:  0.3605184853076935
train gradient:  0.17454960445413953
iteration : 10249
train acc:  0.890625
train loss:  0.24453797936439514
train gradient:  0.09624367568584871
iteration : 10250
train acc:  0.8515625
train loss:  0.307542085647583
train gradient:  0.17039809760957308
iteration : 10251
train acc:  0.8515625
train loss:  0.3308466672897339
train gradient:  0.17508591735499926
iteration : 10252
train acc:  0.828125
train loss:  0.3531135320663452
train gradient:  0.2647732840848615
iteration : 10253
train acc:  0.7890625
train loss:  0.3979175090789795
train gradient:  0.27668512253905586
iteration : 10254
train acc:  0.859375
train loss:  0.31729090213775635
train gradient:  0.1635331332143608
iteration : 10255
train acc:  0.8671875
train loss:  0.3157854676246643
train gradient:  0.12887372340617517
iteration : 10256
train acc:  0.859375
train loss:  0.31351059675216675
train gradient:  0.1285748578299985
iteration : 10257
train acc:  0.828125
train loss:  0.343522846698761
train gradient:  0.20007665459591556
iteration : 10258
train acc:  0.8359375
train loss:  0.3323022425174713
train gradient:  0.1504817682745105
iteration : 10259
train acc:  0.8359375
train loss:  0.4044486880302429
train gradient:  0.23091637517537905
iteration : 10260
train acc:  0.84375
train loss:  0.38086384534835815
train gradient:  0.22146234993334646
iteration : 10261
train acc:  0.8984375
train loss:  0.259357750415802
train gradient:  0.0921195468719332
iteration : 10262
train acc:  0.8515625
train loss:  0.2988716959953308
train gradient:  0.14223996015530088
iteration : 10263
train acc:  0.8671875
train loss:  0.2832249104976654
train gradient:  0.12699150705332943
iteration : 10264
train acc:  0.8984375
train loss:  0.24156415462493896
train gradient:  0.1040961093215408
iteration : 10265
train acc:  0.84375
train loss:  0.35655033588409424
train gradient:  0.17904946529791335
iteration : 10266
train acc:  0.9453125
train loss:  0.2139928936958313
train gradient:  0.08294092680836028
iteration : 10267
train acc:  0.84375
train loss:  0.30677276849746704
train gradient:  0.45333140854444687
iteration : 10268
train acc:  0.8515625
train loss:  0.3567057251930237
train gradient:  0.2328532976288034
iteration : 10269
train acc:  0.8359375
train loss:  0.3171182870864868
train gradient:  0.1573068471714499
iteration : 10270
train acc:  0.8359375
train loss:  0.2880480885505676
train gradient:  0.15757063474394079
iteration : 10271
train acc:  0.8671875
train loss:  0.32769376039505005
train gradient:  0.11400790985885804
iteration : 10272
train acc:  0.8828125
train loss:  0.27103447914123535
train gradient:  0.15707393447972806
iteration : 10273
train acc:  0.90625
train loss:  0.2275892049074173
train gradient:  0.08497744389390838
iteration : 10274
train acc:  0.8515625
train loss:  0.3243294060230255
train gradient:  0.1355795938440321
iteration : 10275
train acc:  0.875
train loss:  0.30430567264556885
train gradient:  0.16943249774909128
iteration : 10276
train acc:  0.8828125
train loss:  0.3346962630748749
train gradient:  0.1622420249479636
iteration : 10277
train acc:  0.8671875
train loss:  0.3473336398601532
train gradient:  0.12622916160125663
iteration : 10278
train acc:  0.8359375
train loss:  0.3559284508228302
train gradient:  0.16993637103370726
iteration : 10279
train acc:  0.8515625
train loss:  0.40321797132492065
train gradient:  0.2244656520435136
iteration : 10280
train acc:  0.875
train loss:  0.2681353688240051
train gradient:  0.09339993041680682
iteration : 10281
train acc:  0.84375
train loss:  0.3362877666950226
train gradient:  0.2401922204966841
iteration : 10282
train acc:  0.8359375
train loss:  0.33129313588142395
train gradient:  0.1080660542215098
iteration : 10283
train acc:  0.8203125
train loss:  0.3668361306190491
train gradient:  0.1641807331901564
iteration : 10284
train acc:  0.8125
train loss:  0.38444504141807556
train gradient:  0.24433483367129505
iteration : 10285
train acc:  0.90625
train loss:  0.2780039310455322
train gradient:  0.0854716738681084
iteration : 10286
train acc:  0.8671875
train loss:  0.35937803983688354
train gradient:  0.13533724573863168
iteration : 10287
train acc:  0.8671875
train loss:  0.32337436079978943
train gradient:  0.4266282996272572
iteration : 10288
train acc:  0.84375
train loss:  0.34578943252563477
train gradient:  0.13577130823141875
iteration : 10289
train acc:  0.8828125
train loss:  0.3222658634185791
train gradient:  0.15693688324936583
iteration : 10290
train acc:  0.890625
train loss:  0.2744186818599701
train gradient:  0.13636982214682608
iteration : 10291
train acc:  0.84375
train loss:  0.3842477798461914
train gradient:  0.14781075968120727
iteration : 10292
train acc:  0.8828125
train loss:  0.3463575541973114
train gradient:  0.18126084391423058
iteration : 10293
train acc:  0.8203125
train loss:  0.3511641025543213
train gradient:  0.2900442026603359
iteration : 10294
train acc:  0.8671875
train loss:  0.27068087458610535
train gradient:  0.1855738508440224
iteration : 10295
train acc:  0.90625
train loss:  0.33233198523521423
train gradient:  0.16321804994169603
iteration : 10296
train acc:  0.8515625
train loss:  0.3104490637779236
train gradient:  0.17227573807463475
iteration : 10297
train acc:  0.8671875
train loss:  0.3097658157348633
train gradient:  0.13575456483691922
iteration : 10298
train acc:  0.921875
train loss:  0.2467709332704544
train gradient:  0.10372801006128606
iteration : 10299
train acc:  0.875
train loss:  0.3276582956314087
train gradient:  0.15040135426636148
iteration : 10300
train acc:  0.8359375
train loss:  0.3139168918132782
train gradient:  0.13294996527477282
iteration : 10301
train acc:  0.828125
train loss:  0.280350923538208
train gradient:  0.11495006676529736
iteration : 10302
train acc:  0.8359375
train loss:  0.4037136137485504
train gradient:  0.23489726632593716
iteration : 10303
train acc:  0.8984375
train loss:  0.27892470359802246
train gradient:  0.11668496034653518
iteration : 10304
train acc:  0.828125
train loss:  0.3768467307090759
train gradient:  0.22941236695730474
iteration : 10305
train acc:  0.8984375
train loss:  0.2684500813484192
train gradient:  0.160617812856514
iteration : 10306
train acc:  0.90625
train loss:  0.2616206407546997
train gradient:  0.14750859730528376
iteration : 10307
train acc:  0.84375
train loss:  0.34295278787612915
train gradient:  0.19209169380860044
iteration : 10308
train acc:  0.875
train loss:  0.27676722407341003
train gradient:  0.16003191792218907
iteration : 10309
train acc:  0.890625
train loss:  0.33736056089401245
train gradient:  0.1348473134661895
iteration : 10310
train acc:  0.890625
train loss:  0.21942639350891113
train gradient:  0.11744173470851738
iteration : 10311
train acc:  0.8515625
train loss:  0.3510048985481262
train gradient:  0.2149304643513672
iteration : 10312
train acc:  0.828125
train loss:  0.3354640007019043
train gradient:  0.17099857973622776
iteration : 10313
train acc:  0.8203125
train loss:  0.3503981828689575
train gradient:  0.20089951921108767
iteration : 10314
train acc:  0.890625
train loss:  0.27188146114349365
train gradient:  0.19689469687092387
iteration : 10315
train acc:  0.9296875
train loss:  0.23782463371753693
train gradient:  0.12149180357554089
iteration : 10316
train acc:  0.875
train loss:  0.3140139579772949
train gradient:  0.18965128757932817
iteration : 10317
train acc:  0.8359375
train loss:  0.2975199520587921
train gradient:  0.13197682469331562
iteration : 10318
train acc:  0.859375
train loss:  0.3274915814399719
train gradient:  0.15259977013624287
iteration : 10319
train acc:  0.84375
train loss:  0.3138088881969452
train gradient:  0.19365971775685095
iteration : 10320
train acc:  0.8984375
train loss:  0.28136730194091797
train gradient:  0.10903217048153666
iteration : 10321
train acc:  0.859375
train loss:  0.3490251898765564
train gradient:  0.15518466610480006
iteration : 10322
train acc:  0.875
train loss:  0.26719874143600464
train gradient:  0.11722618774444658
iteration : 10323
train acc:  0.8359375
train loss:  0.3461349308490753
train gradient:  0.1662633331692165
iteration : 10324
train acc:  0.828125
train loss:  0.49205854535102844
train gradient:  0.3259411186225324
iteration : 10325
train acc:  0.875
train loss:  0.3193961977958679
train gradient:  0.26052009090741274
iteration : 10326
train acc:  0.8359375
train loss:  0.3515535295009613
train gradient:  0.21925018243862887
iteration : 10327
train acc:  0.8046875
train loss:  0.3672810196876526
train gradient:  0.19834514695856886
iteration : 10328
train acc:  0.8359375
train loss:  0.33883994817733765
train gradient:  0.17343015922079377
iteration : 10329
train acc:  0.90625
train loss:  0.2595973014831543
train gradient:  0.10757363840183266
iteration : 10330
train acc:  0.828125
train loss:  0.3223431706428528
train gradient:  0.1277242456799767
iteration : 10331
train acc:  0.84375
train loss:  0.3386857807636261
train gradient:  0.1858930609345055
iteration : 10332
train acc:  0.8671875
train loss:  0.40966957807540894
train gradient:  0.21685561183645385
iteration : 10333
train acc:  0.84375
train loss:  0.32762226462364197
train gradient:  0.11091670661092926
iteration : 10334
train acc:  0.859375
train loss:  0.3204624652862549
train gradient:  0.16515700318444126
iteration : 10335
train acc:  0.9296875
train loss:  0.2545475959777832
train gradient:  0.15855318292228376
iteration : 10336
train acc:  0.859375
train loss:  0.3085769712924957
train gradient:  0.1660371403558552
iteration : 10337
train acc:  0.84375
train loss:  0.31326979398727417
train gradient:  0.12052892521921905
iteration : 10338
train acc:  0.8359375
train loss:  0.40499985218048096
train gradient:  0.23115052007376244
iteration : 10339
train acc:  0.84375
train loss:  0.32100698351860046
train gradient:  0.1070926998207396
iteration : 10340
train acc:  0.8828125
train loss:  0.3315204679965973
train gradient:  0.17229603535212393
iteration : 10341
train acc:  0.90625
train loss:  0.22496730089187622
train gradient:  0.1256589892876418
iteration : 10342
train acc:  0.828125
train loss:  0.39665114879608154
train gradient:  0.19175054964946048
iteration : 10343
train acc:  0.8671875
train loss:  0.3266593813896179
train gradient:  0.17210634444023704
iteration : 10344
train acc:  0.8125
train loss:  0.36424124240875244
train gradient:  0.17504244288058265
iteration : 10345
train acc:  0.890625
train loss:  0.28957399725914
train gradient:  0.19995041253597862
iteration : 10346
train acc:  0.875
train loss:  0.2672039866447449
train gradient:  0.09374652260550699
iteration : 10347
train acc:  0.859375
train loss:  0.31962108612060547
train gradient:  0.13332895455229557
iteration : 10348
train acc:  0.8828125
train loss:  0.333612859249115
train gradient:  0.12969335281105465
iteration : 10349
train acc:  0.859375
train loss:  0.33717232942581177
train gradient:  0.16561939159283073
iteration : 10350
train acc:  0.8515625
train loss:  0.33982646465301514
train gradient:  0.15319704752412544
iteration : 10351
train acc:  0.8828125
train loss:  0.334529310464859
train gradient:  0.15138323146217003
iteration : 10352
train acc:  0.875
train loss:  0.3175388276576996
train gradient:  0.1087415244110554
iteration : 10353
train acc:  0.8671875
train loss:  0.2939485013484955
train gradient:  0.1431311955349377
iteration : 10354
train acc:  0.9140625
train loss:  0.24301566183567047
train gradient:  0.13084005429107284
iteration : 10355
train acc:  0.8046875
train loss:  0.3842458724975586
train gradient:  0.15772869621136254
iteration : 10356
train acc:  0.8984375
train loss:  0.24405145645141602
train gradient:  0.09165433094114565
iteration : 10357
train acc:  0.796875
train loss:  0.4109064042568207
train gradient:  0.1983713905321754
iteration : 10358
train acc:  0.8203125
train loss:  0.46387380361557007
train gradient:  0.31100763553009414
iteration : 10359
train acc:  0.859375
train loss:  0.3174680173397064
train gradient:  0.11990620768750589
iteration : 10360
train acc:  0.8828125
train loss:  0.35312575101852417
train gradient:  0.19231923639301418
iteration : 10361
train acc:  0.84375
train loss:  0.3273458480834961
train gradient:  0.16492425392619586
iteration : 10362
train acc:  0.859375
train loss:  0.3347077965736389
train gradient:  0.15582904825802055
iteration : 10363
train acc:  0.890625
train loss:  0.2640855610370636
train gradient:  0.08739460241774763
iteration : 10364
train acc:  0.8671875
train loss:  0.31390947103500366
train gradient:  0.1231198355681153
iteration : 10365
train acc:  0.8984375
train loss:  0.25910091400146484
train gradient:  0.10667683157258556
iteration : 10366
train acc:  0.84375
train loss:  0.33219119906425476
train gradient:  0.19338483900038828
iteration : 10367
train acc:  0.84375
train loss:  0.3487562835216522
train gradient:  0.21332664156707934
iteration : 10368
train acc:  0.859375
train loss:  0.3098764419555664
train gradient:  0.14811445795514028
iteration : 10369
train acc:  0.828125
train loss:  0.3915272653102875
train gradient:  0.2214056720409624
iteration : 10370
train acc:  0.9453125
train loss:  0.22423091530799866
train gradient:  0.08010801587439749
iteration : 10371
train acc:  0.828125
train loss:  0.3032774329185486
train gradient:  0.14616586372184093
iteration : 10372
train acc:  0.78125
train loss:  0.38677531480789185
train gradient:  0.17622577420883734
iteration : 10373
train acc:  0.7734375
train loss:  0.4376165568828583
train gradient:  0.3104670317938833
iteration : 10374
train acc:  0.90625
train loss:  0.27853521704673767
train gradient:  0.11548859116142586
iteration : 10375
train acc:  0.90625
train loss:  0.24039654433727264
train gradient:  0.10229267527463831
iteration : 10376
train acc:  0.875
train loss:  0.3057866096496582
train gradient:  0.20888794784239104
iteration : 10377
train acc:  0.859375
train loss:  0.30934256315231323
train gradient:  0.1459872091129269
iteration : 10378
train acc:  0.8828125
train loss:  0.2797725796699524
train gradient:  0.10994588334722985
iteration : 10379
train acc:  0.84375
train loss:  0.35483431816101074
train gradient:  0.2941037790769611
iteration : 10380
train acc:  0.90625
train loss:  0.27199840545654297
train gradient:  0.11548706553462999
iteration : 10381
train acc:  0.8046875
train loss:  0.32755085825920105
train gradient:  0.1540987989454657
iteration : 10382
train acc:  0.8828125
train loss:  0.2597197890281677
train gradient:  0.09012732144463509
iteration : 10383
train acc:  0.8671875
train loss:  0.27420562505722046
train gradient:  0.107671401713224
iteration : 10384
train acc:  0.8046875
train loss:  0.37145423889160156
train gradient:  0.18234067019383016
iteration : 10385
train acc:  0.859375
train loss:  0.29586607217788696
train gradient:  0.1595171992403834
iteration : 10386
train acc:  0.921875
train loss:  0.2613915503025055
train gradient:  0.08120335943457883
iteration : 10387
train acc:  0.859375
train loss:  0.3037697970867157
train gradient:  0.13293110395280838
iteration : 10388
train acc:  0.890625
train loss:  0.270708292722702
train gradient:  0.14080697435039166
iteration : 10389
train acc:  0.8515625
train loss:  0.317318856716156
train gradient:  0.14957453208814614
iteration : 10390
train acc:  0.8828125
train loss:  0.32303187251091003
train gradient:  0.14424732112043714
iteration : 10391
train acc:  0.875
train loss:  0.2720804810523987
train gradient:  0.15860152232477814
iteration : 10392
train acc:  0.8671875
train loss:  0.3016837239265442
train gradient:  0.12291371775617173
iteration : 10393
train acc:  0.8515625
train loss:  0.397757351398468
train gradient:  0.24018335000940144
iteration : 10394
train acc:  0.84375
train loss:  0.33329880237579346
train gradient:  0.15499807350376293
iteration : 10395
train acc:  0.890625
train loss:  0.20947860181331635
train gradient:  0.09565282924834045
iteration : 10396
train acc:  0.8125
train loss:  0.3339529037475586
train gradient:  0.1942803708302063
iteration : 10397
train acc:  0.84375
train loss:  0.2946752905845642
train gradient:  0.1332734370387732
iteration : 10398
train acc:  0.84375
train loss:  0.3023729920387268
train gradient:  0.15096921091156168
iteration : 10399
train acc:  0.84375
train loss:  0.35200831294059753
train gradient:  0.17941729134022558
iteration : 10400
train acc:  0.8984375
train loss:  0.30348703265190125
train gradient:  0.1048016189915435
iteration : 10401
train acc:  0.8671875
train loss:  0.3956124186515808
train gradient:  0.1901429398767085
iteration : 10402
train acc:  0.84375
train loss:  0.3341559171676636
train gradient:  0.21041669165168816
iteration : 10403
train acc:  0.8359375
train loss:  0.36962831020355225
train gradient:  0.222679618589041
iteration : 10404
train acc:  0.8125
train loss:  0.3832703232765198
train gradient:  0.18036043477147226
iteration : 10405
train acc:  0.875
train loss:  0.3033829629421234
train gradient:  0.13083100304723624
iteration : 10406
train acc:  0.84375
train loss:  0.317787766456604
train gradient:  0.20748926433523812
iteration : 10407
train acc:  0.8515625
train loss:  0.3338194489479065
train gradient:  0.17056959484958933
iteration : 10408
train acc:  0.8828125
train loss:  0.3015018701553345
train gradient:  0.12438810804921478
iteration : 10409
train acc:  0.8125
train loss:  0.42480194568634033
train gradient:  0.3364597303075343
iteration : 10410
train acc:  0.828125
train loss:  0.3273366093635559
train gradient:  0.13237447320277734
iteration : 10411
train acc:  0.859375
train loss:  0.3491145074367523
train gradient:  0.1669333696071306
iteration : 10412
train acc:  0.8671875
train loss:  0.3397707939147949
train gradient:  0.15439342471169143
iteration : 10413
train acc:  0.8203125
train loss:  0.34754523634910583
train gradient:  0.17285901433131381
iteration : 10414
train acc:  0.84375
train loss:  0.34989434480667114
train gradient:  0.13246550734383075
iteration : 10415
train acc:  0.9140625
train loss:  0.2367614060640335
train gradient:  0.08713302387868087
iteration : 10416
train acc:  0.84375
train loss:  0.35457801818847656
train gradient:  0.21373140808406385
iteration : 10417
train acc:  0.8203125
train loss:  0.363006591796875
train gradient:  0.1608285297401243
iteration : 10418
train acc:  0.859375
train loss:  0.36723700165748596
train gradient:  0.19731483137265826
iteration : 10419
train acc:  0.8203125
train loss:  0.3651161789894104
train gradient:  0.21693159818133467
iteration : 10420
train acc:  0.8515625
train loss:  0.312552809715271
train gradient:  0.14637729260163673
iteration : 10421
train acc:  0.8671875
train loss:  0.3240511417388916
train gradient:  0.1635672217926519
iteration : 10422
train acc:  0.8828125
train loss:  0.2832755446434021
train gradient:  0.13404070033163792
iteration : 10423
train acc:  0.859375
train loss:  0.31040364503860474
train gradient:  0.15247981474667352
iteration : 10424
train acc:  0.8359375
train loss:  0.30070120096206665
train gradient:  0.14793981434185197
iteration : 10425
train acc:  0.859375
train loss:  0.3397658169269562
train gradient:  0.19099438296931703
iteration : 10426
train acc:  0.8828125
train loss:  0.2820456624031067
train gradient:  0.1257282378834667
iteration : 10427
train acc:  0.890625
train loss:  0.29482024908065796
train gradient:  0.12134984874997731
iteration : 10428
train acc:  0.828125
train loss:  0.30593904852867126
train gradient:  0.117373323825926
iteration : 10429
train acc:  0.8984375
train loss:  0.3090718984603882
train gradient:  0.21519141078422577
iteration : 10430
train acc:  0.8359375
train loss:  0.43495598435401917
train gradient:  0.2067186071506573
iteration : 10431
train acc:  0.8671875
train loss:  0.2902088165283203
train gradient:  0.10388048141140546
iteration : 10432
train acc:  0.8515625
train loss:  0.3474273085594177
train gradient:  0.291728248298879
iteration : 10433
train acc:  0.84375
train loss:  0.3203411102294922
train gradient:  0.15020086345140898
iteration : 10434
train acc:  0.84375
train loss:  0.3544144630432129
train gradient:  0.15965205763271878
iteration : 10435
train acc:  0.84375
train loss:  0.3434782028198242
train gradient:  0.16554410570397027
iteration : 10436
train acc:  0.8671875
train loss:  0.2669709324836731
train gradient:  0.10237516689852233
iteration : 10437
train acc:  0.8359375
train loss:  0.3778327703475952
train gradient:  0.2593486136398019
iteration : 10438
train acc:  0.8671875
train loss:  0.281546950340271
train gradient:  0.1229091714857665
iteration : 10439
train acc:  0.875
train loss:  0.2878001630306244
train gradient:  0.0869905585778303
iteration : 10440
train acc:  0.828125
train loss:  0.295016884803772
train gradient:  0.11193996860592526
iteration : 10441
train acc:  0.8203125
train loss:  0.34398043155670166
train gradient:  0.1860243059838974
iteration : 10442
train acc:  0.90625
train loss:  0.2776603102684021
train gradient:  0.15000380617054876
iteration : 10443
train acc:  0.8984375
train loss:  0.24588288366794586
train gradient:  0.10038245608584818
iteration : 10444
train acc:  0.8125
train loss:  0.3523871600627899
train gradient:  0.14239603945842358
iteration : 10445
train acc:  0.8203125
train loss:  0.3279128074645996
train gradient:  0.2748719511429249
iteration : 10446
train acc:  0.9140625
train loss:  0.27762874960899353
train gradient:  0.11961672946951832
iteration : 10447
train acc:  0.7734375
train loss:  0.5332396030426025
train gradient:  0.36917033163592117
iteration : 10448
train acc:  0.84375
train loss:  0.31360629200935364
train gradient:  0.14035806243516727
iteration : 10449
train acc:  0.875
train loss:  0.32611382007598877
train gradient:  0.11386416694805211
iteration : 10450
train acc:  0.8125
train loss:  0.405649870634079
train gradient:  0.19157432433956817
iteration : 10451
train acc:  0.828125
train loss:  0.3395003080368042
train gradient:  0.15796946595311623
iteration : 10452
train acc:  0.859375
train loss:  0.28726017475128174
train gradient:  0.16712437065887759
iteration : 10453
train acc:  0.9140625
train loss:  0.22416290640830994
train gradient:  0.08612461891127332
iteration : 10454
train acc:  0.8671875
train loss:  0.33996671438217163
train gradient:  0.19064495193135986
iteration : 10455
train acc:  0.84375
train loss:  0.31837642192840576
train gradient:  0.17069385834766165
iteration : 10456
train acc:  0.7890625
train loss:  0.4359349310398102
train gradient:  0.2636160025831253
iteration : 10457
train acc:  0.84375
train loss:  0.3541838526725769
train gradient:  0.1485467300716788
iteration : 10458
train acc:  0.8125
train loss:  0.37203192710876465
train gradient:  0.21075551884035298
iteration : 10459
train acc:  0.859375
train loss:  0.3074355721473694
train gradient:  0.11902709654932832
iteration : 10460
train acc:  0.859375
train loss:  0.33808398246765137
train gradient:  0.14820311382811105
iteration : 10461
train acc:  0.8671875
train loss:  0.2864053249359131
train gradient:  0.09241696682350936
iteration : 10462
train acc:  0.859375
train loss:  0.30558478832244873
train gradient:  0.12789969755385222
iteration : 10463
train acc:  0.8828125
train loss:  0.26676344871520996
train gradient:  0.11125926085477902
iteration : 10464
train acc:  0.859375
train loss:  0.29277944564819336
train gradient:  0.15641802949049288
iteration : 10465
train acc:  0.890625
train loss:  0.2686576843261719
train gradient:  0.17443840809003858
iteration : 10466
train acc:  0.875
train loss:  0.30980825424194336
train gradient:  0.147332598088625
iteration : 10467
train acc:  0.875
train loss:  0.25735414028167725
train gradient:  0.09520615681122428
iteration : 10468
train acc:  0.828125
train loss:  0.4133325219154358
train gradient:  0.18757840781375082
iteration : 10469
train acc:  0.9375
train loss:  0.20497553050518036
train gradient:  0.06631837176932887
iteration : 10470
train acc:  0.8515625
train loss:  0.3559073805809021
train gradient:  0.21522136807707543
iteration : 10471
train acc:  0.8671875
train loss:  0.30277836322784424
train gradient:  0.17820484016729216
iteration : 10472
train acc:  0.84375
train loss:  0.3675521910190582
train gradient:  0.14132868537139393
iteration : 10473
train acc:  0.859375
train loss:  0.2993033528327942
train gradient:  0.12338491602642297
iteration : 10474
train acc:  0.8203125
train loss:  0.33553096652030945
train gradient:  0.20318227372669923
iteration : 10475
train acc:  0.84375
train loss:  0.2992967367172241
train gradient:  0.12622337353591465
iteration : 10476
train acc:  0.828125
train loss:  0.38117527961730957
train gradient:  0.17242152687236695
iteration : 10477
train acc:  0.875
train loss:  0.2865881323814392
train gradient:  0.11067813519935454
iteration : 10478
train acc:  0.8515625
train loss:  0.3687519133090973
train gradient:  0.13618208795824327
iteration : 10479
train acc:  0.890625
train loss:  0.29964780807495117
train gradient:  0.14802394463483726
iteration : 10480
train acc:  0.8515625
train loss:  0.3507063388824463
train gradient:  0.1251177761841784
iteration : 10481
train acc:  0.84375
train loss:  0.3066275715827942
train gradient:  0.13435307438551325
iteration : 10482
train acc:  0.859375
train loss:  0.300680935382843
train gradient:  0.13699087703566104
iteration : 10483
train acc:  0.84375
train loss:  0.31105586886405945
train gradient:  0.12130880403665958
iteration : 10484
train acc:  0.8515625
train loss:  0.28622204065322876
train gradient:  0.09050195578843952
iteration : 10485
train acc:  0.8203125
train loss:  0.356061726808548
train gradient:  0.18675155031771784
iteration : 10486
train acc:  0.8515625
train loss:  0.3438783288002014
train gradient:  0.13862290084961065
iteration : 10487
train acc:  0.859375
train loss:  0.30013757944107056
train gradient:  0.11431491371058256
iteration : 10488
train acc:  0.890625
train loss:  0.30513471364974976
train gradient:  0.16907690624176802
iteration : 10489
train acc:  0.8671875
train loss:  0.31746429204940796
train gradient:  0.13983196973590312
iteration : 10490
train acc:  0.859375
train loss:  0.30571603775024414
train gradient:  0.10819467320989792
iteration : 10491
train acc:  0.8671875
train loss:  0.35313326120376587
train gradient:  0.14215433364969965
iteration : 10492
train acc:  0.84375
train loss:  0.36117038130760193
train gradient:  0.16458329144464567
iteration : 10493
train acc:  0.7890625
train loss:  0.41962122917175293
train gradient:  0.2253817728043827
iteration : 10494
train acc:  0.8671875
train loss:  0.31261497735977173
train gradient:  0.18515447707996313
iteration : 10495
train acc:  0.8515625
train loss:  0.29436489939689636
train gradient:  0.18754647979437328
iteration : 10496
train acc:  0.890625
train loss:  0.25284743309020996
train gradient:  0.11777516328237514
iteration : 10497
train acc:  0.8828125
train loss:  0.2800045609474182
train gradient:  0.11677676169703166
iteration : 10498
train acc:  0.8828125
train loss:  0.29790645837783813
train gradient:  1.575724068322864
iteration : 10499
train acc:  0.8828125
train loss:  0.28392958641052246
train gradient:  0.12126672640165613
iteration : 10500
train acc:  0.90625
train loss:  0.2417837679386139
train gradient:  0.0973672294105158
iteration : 10501
train acc:  0.828125
train loss:  0.3443012237548828
train gradient:  0.1417846728907543
iteration : 10502
train acc:  0.84375
train loss:  0.3360941708087921
train gradient:  0.27384236343464285
iteration : 10503
train acc:  0.859375
train loss:  0.3048412799835205
train gradient:  0.11554396131867654
iteration : 10504
train acc:  0.921875
train loss:  0.27717164158821106
train gradient:  0.09519126240504663
iteration : 10505
train acc:  0.8359375
train loss:  0.31844156980514526
train gradient:  0.12479693638232645
iteration : 10506
train acc:  0.8828125
train loss:  0.3041137456893921
train gradient:  0.3115461288334851
iteration : 10507
train acc:  0.8125
train loss:  0.3183225989341736
train gradient:  0.22773649214628017
iteration : 10508
train acc:  0.890625
train loss:  0.2618965804576874
train gradient:  0.11037769288281898
iteration : 10509
train acc:  0.890625
train loss:  0.28258800506591797
train gradient:  0.09821002106449866
iteration : 10510
train acc:  0.875
train loss:  0.3307245969772339
train gradient:  0.1613142960124757
iteration : 10511
train acc:  0.8125
train loss:  0.39258259534835815
train gradient:  0.1815167999358333
iteration : 10512
train acc:  0.875
train loss:  0.2929529547691345
train gradient:  0.1649667134874211
iteration : 10513
train acc:  0.84375
train loss:  0.31266146898269653
train gradient:  0.14418129700704324
iteration : 10514
train acc:  0.890625
train loss:  0.28341129422187805
train gradient:  0.14285117252585833
iteration : 10515
train acc:  0.890625
train loss:  0.33022257685661316
train gradient:  0.18220416810430173
iteration : 10516
train acc:  0.8359375
train loss:  0.34004586935043335
train gradient:  0.15885717668112648
iteration : 10517
train acc:  0.859375
train loss:  0.2742258906364441
train gradient:  0.10809078622966502
iteration : 10518
train acc:  0.9140625
train loss:  0.26238787174224854
train gradient:  0.1930270264897596
iteration : 10519
train acc:  0.8203125
train loss:  0.42546287178993225
train gradient:  0.2365454888422039
iteration : 10520
train acc:  0.859375
train loss:  0.26704734563827515
train gradient:  0.14819344961761033
iteration : 10521
train acc:  0.875
train loss:  0.30810683965682983
train gradient:  0.1666838885272126
iteration : 10522
train acc:  0.8515625
train loss:  0.306320458650589
train gradient:  0.15447283106186638
iteration : 10523
train acc:  0.8984375
train loss:  0.23077145218849182
train gradient:  0.1494622029182104
iteration : 10524
train acc:  0.875
train loss:  0.33826541900634766
train gradient:  0.18661809440276655
iteration : 10525
train acc:  0.84375
train loss:  0.33349609375
train gradient:  0.18363180485164782
iteration : 10526
train acc:  0.828125
train loss:  0.36345207691192627
train gradient:  0.15245861759948134
iteration : 10527
train acc:  0.8359375
train loss:  0.39728212356567383
train gradient:  0.16329664816374995
iteration : 10528
train acc:  0.8984375
train loss:  0.2888863980770111
train gradient:  0.11443468621003351
iteration : 10529
train acc:  0.9140625
train loss:  0.34329697489738464
train gradient:  0.1505945467060255
iteration : 10530
train acc:  0.8515625
train loss:  0.38320428133010864
train gradient:  0.18802494089379024
iteration : 10531
train acc:  0.84375
train loss:  0.28761208057403564
train gradient:  0.14861591192763937
iteration : 10532
train acc:  0.8828125
train loss:  0.3439064025878906
train gradient:  0.13413274875278658
iteration : 10533
train acc:  0.875
train loss:  0.3287830054759979
train gradient:  0.20316479811763727
iteration : 10534
train acc:  0.78125
train loss:  0.40221884846687317
train gradient:  0.23236054531900416
iteration : 10535
train acc:  0.8515625
train loss:  0.3010762333869934
train gradient:  0.14597054375907517
iteration : 10536
train acc:  0.8203125
train loss:  0.3709234893321991
train gradient:  0.20891538872718396
iteration : 10537
train acc:  0.890625
train loss:  0.3028663396835327
train gradient:  0.12797570029057487
iteration : 10538
train acc:  0.859375
train loss:  0.3183577060699463
train gradient:  0.17965874499834072
iteration : 10539
train acc:  0.8203125
train loss:  0.3633256256580353
train gradient:  0.21321462924624718
iteration : 10540
train acc:  0.8515625
train loss:  0.31220367550849915
train gradient:  0.2783628771044001
iteration : 10541
train acc:  0.8515625
train loss:  0.26861515641212463
train gradient:  0.1359459105253918
iteration : 10542
train acc:  0.875
train loss:  0.3051818013191223
train gradient:  0.18362642009699415
iteration : 10543
train acc:  0.8359375
train loss:  0.3299976587295532
train gradient:  0.20807979491467682
iteration : 10544
train acc:  0.8125
train loss:  0.4317750930786133
train gradient:  0.2636099820485001
iteration : 10545
train acc:  0.875
train loss:  0.32943978905677795
train gradient:  0.17931336511060614
iteration : 10546
train acc:  0.8984375
train loss:  0.2380015105009079
train gradient:  0.12897794566152349
iteration : 10547
train acc:  0.8125
train loss:  0.3703136742115021
train gradient:  0.18455833202549238
iteration : 10548
train acc:  0.8828125
train loss:  0.28086057305336
train gradient:  0.1372092071417355
iteration : 10549
train acc:  0.8125
train loss:  0.40930816531181335
train gradient:  0.24587338538590053
iteration : 10550
train acc:  0.875
train loss:  0.35945555567741394
train gradient:  0.18386254628875295
iteration : 10551
train acc:  0.859375
train loss:  0.27817901968955994
train gradient:  0.11109263529096759
iteration : 10552
train acc:  0.8515625
train loss:  0.3608408570289612
train gradient:  0.2158234828316828
iteration : 10553
train acc:  0.84375
train loss:  0.33996742963790894
train gradient:  0.1919897883444045
iteration : 10554
train acc:  0.875
train loss:  0.3213635981082916
train gradient:  0.18172039077268642
iteration : 10555
train acc:  0.8828125
train loss:  0.24978387355804443
train gradient:  0.1197963071719288
iteration : 10556
train acc:  0.7890625
train loss:  0.41147381067276
train gradient:  0.20214101568971626
iteration : 10557
train acc:  0.9375
train loss:  0.20434266328811646
train gradient:  0.08932950877475453
iteration : 10558
train acc:  0.8359375
train loss:  0.4043920636177063
train gradient:  0.24356202046471692
iteration : 10559
train acc:  0.8671875
train loss:  0.2679881453514099
train gradient:  0.16145809371988193
iteration : 10560
train acc:  0.859375
train loss:  0.2764475345611572
train gradient:  0.10765938037024611
iteration : 10561
train acc:  0.828125
train loss:  0.4853077530860901
train gradient:  0.2940404420758759
iteration : 10562
train acc:  0.8515625
train loss:  0.3235572576522827
train gradient:  0.2510276016676838
iteration : 10563
train acc:  0.8671875
train loss:  0.30261775851249695
train gradient:  0.13546473456305527
iteration : 10564
train acc:  0.859375
train loss:  0.3389953672885895
train gradient:  0.13697481604112355
iteration : 10565
train acc:  0.8984375
train loss:  0.24557550251483917
train gradient:  0.1017866639786641
iteration : 10566
train acc:  0.90625
train loss:  0.26457759737968445
train gradient:  0.13729941553911781
iteration : 10567
train acc:  0.8359375
train loss:  0.362770676612854
train gradient:  0.23253627845639657
iteration : 10568
train acc:  0.8671875
train loss:  0.2821665406227112
train gradient:  0.14869179266692112
iteration : 10569
train acc:  0.8671875
train loss:  0.3472638428211212
train gradient:  0.26865224173871166
iteration : 10570
train acc:  0.8203125
train loss:  0.3768742084503174
train gradient:  0.2453115845068758
iteration : 10571
train acc:  0.828125
train loss:  0.33077365159988403
train gradient:  0.15617959176148527
iteration : 10572
train acc:  0.8671875
train loss:  0.33058273792266846
train gradient:  0.26413725134034377
iteration : 10573
train acc:  0.765625
train loss:  0.4186954200267792
train gradient:  0.32060106014721
iteration : 10574
train acc:  0.8828125
train loss:  0.250306636095047
train gradient:  0.10612901574838869
iteration : 10575
train acc:  0.8359375
train loss:  0.40213537216186523
train gradient:  0.1486312646292971
iteration : 10576
train acc:  0.890625
train loss:  0.3158740997314453
train gradient:  0.23623234728755316
iteration : 10577
train acc:  0.8984375
train loss:  0.3131263852119446
train gradient:  0.13393418446072053
iteration : 10578
train acc:  0.859375
train loss:  0.30726832151412964
train gradient:  0.16237289516551073
iteration : 10579
train acc:  0.8671875
train loss:  0.29641640186309814
train gradient:  0.1343128877157742
iteration : 10580
train acc:  0.890625
train loss:  0.2898237109184265
train gradient:  0.1322910855532209
iteration : 10581
train acc:  0.859375
train loss:  0.32506006956100464
train gradient:  0.13093299887601048
iteration : 10582
train acc:  0.84375
train loss:  0.2895895540714264
train gradient:  0.0967341980435833
iteration : 10583
train acc:  0.8515625
train loss:  0.3092579245567322
train gradient:  0.12631157759558764
iteration : 10584
train acc:  0.8515625
train loss:  0.32746055722236633
train gradient:  0.15516116339570613
iteration : 10585
train acc:  0.859375
train loss:  0.3138282895088196
train gradient:  0.1963956228889005
iteration : 10586
train acc:  0.8046875
train loss:  0.3962917923927307
train gradient:  0.19762065780585295
iteration : 10587
train acc:  0.890625
train loss:  0.2953791618347168
train gradient:  0.13784481898391304
iteration : 10588
train acc:  0.8671875
train loss:  0.29704952239990234
train gradient:  0.15692543276473275
iteration : 10589
train acc:  0.8671875
train loss:  0.25626859068870544
train gradient:  0.13255576337652564
iteration : 10590
train acc:  0.859375
train loss:  0.3643648624420166
train gradient:  0.19763315645380636
iteration : 10591
train acc:  0.90625
train loss:  0.2271333932876587
train gradient:  0.11556275686770626
iteration : 10592
train acc:  0.9296875
train loss:  0.2628173232078552
train gradient:  0.1355447674215175
iteration : 10593
train acc:  0.8828125
train loss:  0.28849828243255615
train gradient:  0.1716377019755882
iteration : 10594
train acc:  0.828125
train loss:  0.376226007938385
train gradient:  0.2423665472829939
iteration : 10595
train acc:  0.859375
train loss:  0.36099886894226074
train gradient:  0.1528174463011498
iteration : 10596
train acc:  0.890625
train loss:  0.2525649964809418
train gradient:  0.1066505646186179
iteration : 10597
train acc:  0.84375
train loss:  0.3343890309333801
train gradient:  0.17273037068356767
iteration : 10598
train acc:  0.828125
train loss:  0.3663268983364105
train gradient:  0.17181491884268735
iteration : 10599
train acc:  0.828125
train loss:  0.37257128953933716
train gradient:  0.2739264334015383
iteration : 10600
train acc:  0.7734375
train loss:  0.4752282500267029
train gradient:  0.31617460068335945
iteration : 10601
train acc:  0.8671875
train loss:  0.3625084161758423
train gradient:  0.23168096795261717
iteration : 10602
train acc:  0.8203125
train loss:  0.3765009045600891
train gradient:  0.21741657916984636
iteration : 10603
train acc:  0.796875
train loss:  0.4358328878879547
train gradient:  0.2516963601598957
iteration : 10604
train acc:  0.859375
train loss:  0.3658287823200226
train gradient:  0.20736252063292904
iteration : 10605
train acc:  0.8359375
train loss:  0.36253294348716736
train gradient:  0.17906500310379075
iteration : 10606
train acc:  0.8671875
train loss:  0.32742759585380554
train gradient:  0.1930636141024465
iteration : 10607
train acc:  0.90625
train loss:  0.26358678936958313
train gradient:  0.1001527087027155
iteration : 10608
train acc:  0.90625
train loss:  0.24478061497211456
train gradient:  0.08198654700263878
iteration : 10609
train acc:  0.8359375
train loss:  0.3078383207321167
train gradient:  0.11896760961384563
iteration : 10610
train acc:  0.859375
train loss:  0.32117655873298645
train gradient:  0.15091661152012442
iteration : 10611
train acc:  0.8515625
train loss:  0.3498552441596985
train gradient:  0.14359006885314904
iteration : 10612
train acc:  0.8828125
train loss:  0.3100242018699646
train gradient:  0.18515366944797246
iteration : 10613
train acc:  0.859375
train loss:  0.3841497600078583
train gradient:  0.19053669995589093
iteration : 10614
train acc:  0.8515625
train loss:  0.2701302170753479
train gradient:  0.10987335649857964
iteration : 10615
train acc:  0.828125
train loss:  0.35751771926879883
train gradient:  0.18035973897196006
iteration : 10616
train acc:  0.9140625
train loss:  0.2740422487258911
train gradient:  0.11569892788660002
iteration : 10617
train acc:  0.8828125
train loss:  0.2969675362110138
train gradient:  0.16488349422044488
iteration : 10618
train acc:  0.8515625
train loss:  0.32680758833885193
train gradient:  0.1461316686153924
iteration : 10619
train acc:  0.8671875
train loss:  0.3114306926727295
train gradient:  0.12759135719109377
iteration : 10620
train acc:  0.875
train loss:  0.3310525417327881
train gradient:  0.1493998299396955
iteration : 10621
train acc:  0.84375
train loss:  0.34558194875717163
train gradient:  0.1332409364117268
iteration : 10622
train acc:  0.8515625
train loss:  0.35913681983947754
train gradient:  0.15634466292784258
iteration : 10623
train acc:  0.8515625
train loss:  0.29771679639816284
train gradient:  0.12121285265389539
iteration : 10624
train acc:  0.859375
train loss:  0.30305126309394836
train gradient:  0.16468171356128722
iteration : 10625
train acc:  0.8359375
train loss:  0.29499945044517517
train gradient:  0.11309838270005705
iteration : 10626
train acc:  0.9375
train loss:  0.23409391939640045
train gradient:  0.11380309532013297
iteration : 10627
train acc:  0.8359375
train loss:  0.351718008518219
train gradient:  0.16965589292769703
iteration : 10628
train acc:  0.9453125
train loss:  0.20281192660331726
train gradient:  0.07135248803722746
iteration : 10629
train acc:  0.890625
train loss:  0.3419225513935089
train gradient:  0.14596440588128917
iteration : 10630
train acc:  0.8828125
train loss:  0.3050718307495117
train gradient:  0.11678400576873674
iteration : 10631
train acc:  0.890625
train loss:  0.28892213106155396
train gradient:  0.16913465872446498
iteration : 10632
train acc:  0.8125
train loss:  0.3664342164993286
train gradient:  0.2178050199886236
iteration : 10633
train acc:  0.9140625
train loss:  0.23324359953403473
train gradient:  0.1346235178305813
iteration : 10634
train acc:  0.9296875
train loss:  0.24792951345443726
train gradient:  0.10891413161558888
iteration : 10635
train acc:  0.84375
train loss:  0.3472330570220947
train gradient:  0.12274909577610571
iteration : 10636
train acc:  0.828125
train loss:  0.4091188907623291
train gradient:  0.2003852451710766
iteration : 10637
train acc:  0.8359375
train loss:  0.3761405944824219
train gradient:  0.18515248697844489
iteration : 10638
train acc:  0.8984375
train loss:  0.2838711738586426
train gradient:  0.09301859968716299
iteration : 10639
train acc:  0.8046875
train loss:  0.44451624155044556
train gradient:  0.2656567418908865
iteration : 10640
train acc:  0.8984375
train loss:  0.30067020654678345
train gradient:  0.1628809132495485
iteration : 10641
train acc:  0.875
train loss:  0.32257795333862305
train gradient:  0.15454906654798523
iteration : 10642
train acc:  0.8828125
train loss:  0.2635759711265564
train gradient:  0.16145803861646946
iteration : 10643
train acc:  0.9296875
train loss:  0.2134607583284378
train gradient:  0.11314905204242906
iteration : 10644
train acc:  0.875
train loss:  0.3386287987232208
train gradient:  0.20200868411907386
iteration : 10645
train acc:  0.8671875
train loss:  0.3572409749031067
train gradient:  0.1532420795827697
iteration : 10646
train acc:  0.84375
train loss:  0.33193355798721313
train gradient:  0.14158902030164194
iteration : 10647
train acc:  0.8515625
train loss:  0.2807086408138275
train gradient:  0.11897042565486056
iteration : 10648
train acc:  0.828125
train loss:  0.37015968561172485
train gradient:  0.19132190036445715
iteration : 10649
train acc:  0.90625
train loss:  0.25912541151046753
train gradient:  0.0863895630074773
iteration : 10650
train acc:  0.875
train loss:  0.30460724234580994
train gradient:  0.2406631744217999
iteration : 10651
train acc:  0.8125
train loss:  0.3452545404434204
train gradient:  0.1686823070049655
iteration : 10652
train acc:  0.8515625
train loss:  0.35279223322868347
train gradient:  0.13914962377836965
iteration : 10653
train acc:  0.8984375
train loss:  0.2864680588245392
train gradient:  0.14630222389352493
iteration : 10654
train acc:  0.828125
train loss:  0.34300583600997925
train gradient:  0.19013924960496104
iteration : 10655
train acc:  0.8046875
train loss:  0.39606010913848877
train gradient:  0.19315585052783124
iteration : 10656
train acc:  0.796875
train loss:  0.3971266746520996
train gradient:  0.18996111283091177
iteration : 10657
train acc:  0.90625
train loss:  0.27448904514312744
train gradient:  0.10564905792517419
iteration : 10658
train acc:  0.890625
train loss:  0.2712675929069519
train gradient:  0.1185996281728636
iteration : 10659
train acc:  0.8671875
train loss:  0.3638378977775574
train gradient:  0.17661584171014483
iteration : 10660
train acc:  0.859375
train loss:  0.2842414081096649
train gradient:  0.1265047365429818
iteration : 10661
train acc:  0.8828125
train loss:  0.3469722867012024
train gradient:  0.15951003427720947
iteration : 10662
train acc:  0.8984375
train loss:  0.24376560747623444
train gradient:  0.1197639612973526
iteration : 10663
train acc:  0.8671875
train loss:  0.36198198795318604
train gradient:  0.21697204353651794
iteration : 10664
train acc:  0.875
train loss:  0.27815866470336914
train gradient:  0.12499324272655314
iteration : 10665
train acc:  0.8359375
train loss:  0.350166380405426
train gradient:  0.17299450136854913
iteration : 10666
train acc:  0.8046875
train loss:  0.3968859612941742
train gradient:  0.19632764098192693
iteration : 10667
train acc:  0.90625
train loss:  0.27352309226989746
train gradient:  0.17486302144285754
iteration : 10668
train acc:  0.828125
train loss:  0.3921334147453308
train gradient:  0.20015315681783621
iteration : 10669
train acc:  0.890625
train loss:  0.26059502363204956
train gradient:  0.1554187284054333
iteration : 10670
train acc:  0.84375
train loss:  0.38707828521728516
train gradient:  0.1868815008209982
iteration : 10671
train acc:  0.890625
train loss:  0.3530365824699402
train gradient:  0.13633306232963263
iteration : 10672
train acc:  0.875
train loss:  0.34351420402526855
train gradient:  0.23916385738944346
iteration : 10673
train acc:  0.8984375
train loss:  0.2800685465335846
train gradient:  0.11491551768761901
iteration : 10674
train acc:  0.875
train loss:  0.27555546164512634
train gradient:  0.08742579058986033
iteration : 10675
train acc:  0.8515625
train loss:  0.3274635076522827
train gradient:  0.1291381980802453
iteration : 10676
train acc:  0.921875
train loss:  0.22468885779380798
train gradient:  0.11865227061611974
iteration : 10677
train acc:  0.8828125
train loss:  0.2722835838794708
train gradient:  0.122809398604136
iteration : 10678
train acc:  0.875
train loss:  0.303324431180954
train gradient:  0.15536164333915672
iteration : 10679
train acc:  0.8671875
train loss:  0.32342690229415894
train gradient:  0.19142735055217902
iteration : 10680
train acc:  0.875
train loss:  0.27541229128837585
train gradient:  0.09796993659125167
iteration : 10681
train acc:  0.828125
train loss:  0.39481621980667114
train gradient:  0.21101768677041233
iteration : 10682
train acc:  0.8515625
train loss:  0.2973862290382385
train gradient:  0.11505250418823634
iteration : 10683
train acc:  0.8046875
train loss:  0.3767436146736145
train gradient:  0.19405535376519265
iteration : 10684
train acc:  0.859375
train loss:  0.3312022089958191
train gradient:  0.15307816002738656
iteration : 10685
train acc:  0.875
train loss:  0.32252296805381775
train gradient:  0.14037476198099777
iteration : 10686
train acc:  0.90625
train loss:  0.29245156049728394
train gradient:  0.1344007619570427
iteration : 10687
train acc:  0.8984375
train loss:  0.2893935739994049
train gradient:  0.12090052419948087
iteration : 10688
train acc:  0.875
train loss:  0.2808980345726013
train gradient:  0.11930733097367809
iteration : 10689
train acc:  0.8984375
train loss:  0.28155744075775146
train gradient:  0.13481340608828463
iteration : 10690
train acc:  0.8203125
train loss:  0.41259804368019104
train gradient:  0.21968968372931108
iteration : 10691
train acc:  0.875
train loss:  0.27629169821739197
train gradient:  0.106177565703067
iteration : 10692
train acc:  0.890625
train loss:  0.26567113399505615
train gradient:  0.15273227067571254
iteration : 10693
train acc:  0.8515625
train loss:  0.3059437572956085
train gradient:  0.14362049647263894
iteration : 10694
train acc:  0.890625
train loss:  0.28008323907852173
train gradient:  0.11636526772210888
iteration : 10695
train acc:  0.859375
train loss:  0.348359614610672
train gradient:  0.15525931370009588
iteration : 10696
train acc:  0.859375
train loss:  0.31274864077568054
train gradient:  0.13405789213590794
iteration : 10697
train acc:  0.8828125
train loss:  0.28361594676971436
train gradient:  0.1254557942998081
iteration : 10698
train acc:  0.859375
train loss:  0.3690580129623413
train gradient:  0.19914895694681373
iteration : 10699
train acc:  0.921875
train loss:  0.30193307995796204
train gradient:  0.13538188393942419
iteration : 10700
train acc:  0.84375
train loss:  0.3562408685684204
train gradient:  0.1906212190319808
iteration : 10701
train acc:  0.875
train loss:  0.3363160192966461
train gradient:  0.15417157663181627
iteration : 10702
train acc:  0.84375
train loss:  0.3133230209350586
train gradient:  0.20161377269181735
iteration : 10703
train acc:  0.859375
train loss:  0.3378681242465973
train gradient:  0.10825613159507243
iteration : 10704
train acc:  0.8984375
train loss:  0.2651478052139282
train gradient:  0.09611792431834967
iteration : 10705
train acc:  0.8515625
train loss:  0.308398962020874
train gradient:  0.18097739340903285
iteration : 10706
train acc:  0.8359375
train loss:  0.37249380350112915
train gradient:  0.15681053533368142
iteration : 10707
train acc:  0.9453125
train loss:  0.2068672776222229
train gradient:  0.07423920289265146
iteration : 10708
train acc:  0.84375
train loss:  0.33498743176460266
train gradient:  0.12078104420879802
iteration : 10709
train acc:  0.859375
train loss:  0.34422725439071655
train gradient:  0.16462098421831176
iteration : 10710
train acc:  0.84375
train loss:  0.2987420856952667
train gradient:  0.13120292721684162
iteration : 10711
train acc:  0.8515625
train loss:  0.2805449962615967
train gradient:  0.09615295758186816
iteration : 10712
train acc:  0.9375
train loss:  0.22160454094409943
train gradient:  0.07237759710625774
iteration : 10713
train acc:  0.8203125
train loss:  0.5048395991325378
train gradient:  0.25112723087435673
iteration : 10714
train acc:  0.90625
train loss:  0.256234347820282
train gradient:  0.0876945712627931
iteration : 10715
train acc:  0.9453125
train loss:  0.21021920442581177
train gradient:  0.06855592897355896
iteration : 10716
train acc:  0.828125
train loss:  0.3141811192035675
train gradient:  0.14424314769533714
iteration : 10717
train acc:  0.8828125
train loss:  0.3080504834651947
train gradient:  0.11671615977233922
iteration : 10718
train acc:  0.8203125
train loss:  0.3589145541191101
train gradient:  0.18474871568728482
iteration : 10719
train acc:  0.8359375
train loss:  0.363934725522995
train gradient:  0.13436134905836966
iteration : 10720
train acc:  0.8515625
train loss:  0.3182435631752014
train gradient:  0.19553365532357758
iteration : 10721
train acc:  0.8671875
train loss:  0.31664353609085083
train gradient:  0.12869624858728437
iteration : 10722
train acc:  0.890625
train loss:  0.24040617048740387
train gradient:  0.1225392913194014
iteration : 10723
train acc:  0.859375
train loss:  0.29473280906677246
train gradient:  0.10549694881988372
iteration : 10724
train acc:  0.8203125
train loss:  0.3405734896659851
train gradient:  0.14310258028043213
iteration : 10725
train acc:  0.90625
train loss:  0.25735583901405334
train gradient:  0.12969548754430735
iteration : 10726
train acc:  0.890625
train loss:  0.2629563808441162
train gradient:  0.0709249628244315
iteration : 10727
train acc:  0.875
train loss:  0.2768841087818146
train gradient:  0.119491834307385
iteration : 10728
train acc:  0.8671875
train loss:  0.291055828332901
train gradient:  0.11043757303103353
iteration : 10729
train acc:  0.9296875
train loss:  0.18846651911735535
train gradient:  0.07734036649171919
iteration : 10730
train acc:  0.8359375
train loss:  0.3263149857521057
train gradient:  0.10953897707801794
iteration : 10731
train acc:  0.8671875
train loss:  0.3850351572036743
train gradient:  0.2294196193703113
iteration : 10732
train acc:  0.8203125
train loss:  0.42651253938674927
train gradient:  0.31756156755983683
iteration : 10733
train acc:  0.890625
train loss:  0.2552502453327179
train gradient:  0.08245235805330722
iteration : 10734
train acc:  0.828125
train loss:  0.3278830349445343
train gradient:  0.1983588427828086
iteration : 10735
train acc:  0.859375
train loss:  0.2987918257713318
train gradient:  0.15946895587774904
iteration : 10736
train acc:  0.859375
train loss:  0.28596073389053345
train gradient:  0.1787576946511581
iteration : 10737
train acc:  0.8359375
train loss:  0.34474998712539673
train gradient:  0.18600601841210346
iteration : 10738
train acc:  0.875
train loss:  0.33325833082199097
train gradient:  0.16835107471602667
iteration : 10739
train acc:  0.8671875
train loss:  0.3138795793056488
train gradient:  0.1753424060642464
iteration : 10740
train acc:  0.8515625
train loss:  0.3654370903968811
train gradient:  0.2016524324209559
iteration : 10741
train acc:  0.90625
train loss:  0.2839491069316864
train gradient:  0.09861165083664523
iteration : 10742
train acc:  0.84375
train loss:  0.3186645209789276
train gradient:  0.14112676418856443
iteration : 10743
train acc:  0.859375
train loss:  0.29838547110557556
train gradient:  0.11936313427334443
iteration : 10744
train acc:  0.8828125
train loss:  0.30289098620414734
train gradient:  0.16524713621631837
iteration : 10745
train acc:  0.8828125
train loss:  0.3091559112071991
train gradient:  0.1715056084327881
iteration : 10746
train acc:  0.84375
train loss:  0.39405128359794617
train gradient:  0.24603097210557032
iteration : 10747
train acc:  0.8671875
train loss:  0.3223261833190918
train gradient:  0.1579473564602959
iteration : 10748
train acc:  0.8828125
train loss:  0.2792817950248718
train gradient:  0.13029499178504045
iteration : 10749
train acc:  0.875
train loss:  0.3032684326171875
train gradient:  0.15365512643412316
iteration : 10750
train acc:  0.8828125
train loss:  0.32809120416641235
train gradient:  0.14721607642639167
iteration : 10751
train acc:  0.8203125
train loss:  0.3780394196510315
train gradient:  0.19703334817955642
iteration : 10752
train acc:  0.8671875
train loss:  0.3862687647342682
train gradient:  0.21839302661621673
iteration : 10753
train acc:  0.828125
train loss:  0.35317203402519226
train gradient:  0.22614635250960252
iteration : 10754
train acc:  0.859375
train loss:  0.3649888038635254
train gradient:  0.16301903128631942
iteration : 10755
train acc:  0.859375
train loss:  0.3499993085861206
train gradient:  0.19252615033980025
iteration : 10756
train acc:  0.859375
train loss:  0.3223671317100525
train gradient:  0.11354120116445915
iteration : 10757
train acc:  0.84375
train loss:  0.33901485800743103
train gradient:  0.17819702044212005
iteration : 10758
train acc:  0.8515625
train loss:  0.3070932626724243
train gradient:  0.11068816171400778
iteration : 10759
train acc:  0.875
train loss:  0.30168724060058594
train gradient:  0.16577045681097624
iteration : 10760
train acc:  0.875
train loss:  0.310112863779068
train gradient:  0.11045698951485909
iteration : 10761
train acc:  0.8671875
train loss:  0.3099532127380371
train gradient:  0.13988811044184846
iteration : 10762
train acc:  0.8359375
train loss:  0.3935088515281677
train gradient:  0.20890211149898813
iteration : 10763
train acc:  0.8984375
train loss:  0.3192906379699707
train gradient:  0.14892092819463304
iteration : 10764
train acc:  0.8984375
train loss:  0.27528515458106995
train gradient:  0.10445903496545028
iteration : 10765
train acc:  0.8828125
train loss:  0.29673683643341064
train gradient:  0.1381584547555325
iteration : 10766
train acc:  0.796875
train loss:  0.38298141956329346
train gradient:  0.22110644455158954
iteration : 10767
train acc:  0.828125
train loss:  0.4083871841430664
train gradient:  0.36583423543796145
iteration : 10768
train acc:  0.8828125
train loss:  0.26051902770996094
train gradient:  0.12316420919357862
iteration : 10769
train acc:  0.78125
train loss:  0.39725571870803833
train gradient:  0.17885717572218396
iteration : 10770
train acc:  0.875
train loss:  0.3061103820800781
train gradient:  0.1701129579207285
iteration : 10771
train acc:  0.8984375
train loss:  0.2510816752910614
train gradient:  0.12054288608134872
iteration : 10772
train acc:  0.828125
train loss:  0.38220828771591187
train gradient:  0.20022080418183721
iteration : 10773
train acc:  0.875
train loss:  0.27411940693855286
train gradient:  0.10142575903134878
iteration : 10774
train acc:  0.8515625
train loss:  0.3298208713531494
train gradient:  0.16572962699767793
iteration : 10775
train acc:  0.875
train loss:  0.27410733699798584
train gradient:  0.08808308045830188
iteration : 10776
train acc:  0.859375
train loss:  0.3353070616722107
train gradient:  0.17227952106828698
iteration : 10777
train acc:  0.875
train loss:  0.3164856433868408
train gradient:  0.13544545997718227
iteration : 10778
train acc:  0.8828125
train loss:  0.3238688111305237
train gradient:  0.17506417757897225
iteration : 10779
train acc:  0.8359375
train loss:  0.31374508142471313
train gradient:  0.13010663852007753
iteration : 10780
train acc:  0.875
train loss:  0.2922336757183075
train gradient:  0.15394984583600285
iteration : 10781
train acc:  0.8828125
train loss:  0.30732062458992004
train gradient:  0.12065663797653792
iteration : 10782
train acc:  0.8515625
train loss:  0.36982211470603943
train gradient:  0.13308505351039035
iteration : 10783
train acc:  0.859375
train loss:  0.3052739202976227
train gradient:  0.1966291604920713
iteration : 10784
train acc:  0.875
train loss:  0.27280402183532715
train gradient:  0.1084855488067646
iteration : 10785
train acc:  0.8515625
train loss:  0.3445301949977875
train gradient:  0.13194385551203588
iteration : 10786
train acc:  0.8359375
train loss:  0.35101428627967834
train gradient:  0.1375171652345593
iteration : 10787
train acc:  0.8828125
train loss:  0.29969581961631775
train gradient:  0.1665315765642687
iteration : 10788
train acc:  0.859375
train loss:  0.30606457591056824
train gradient:  0.14748144788869255
iteration : 10789
train acc:  0.8125
train loss:  0.33740803599357605
train gradient:  0.13760187599074647
iteration : 10790
train acc:  0.859375
train loss:  0.38041821122169495
train gradient:  0.3169784458740321
iteration : 10791
train acc:  0.84375
train loss:  0.4153009057044983
train gradient:  0.19526799223658464
iteration : 10792
train acc:  0.8515625
train loss:  0.3009932339191437
train gradient:  0.10447544031072528
iteration : 10793
train acc:  0.8671875
train loss:  0.3204190731048584
train gradient:  0.2184308724535466
iteration : 10794
train acc:  0.84375
train loss:  0.3571479022502899
train gradient:  0.14440224634986196
iteration : 10795
train acc:  0.8515625
train loss:  0.33069366216659546
train gradient:  0.16353827162191023
iteration : 10796
train acc:  0.8515625
train loss:  0.31732702255249023
train gradient:  0.1266888881709592
iteration : 10797
train acc:  0.7890625
train loss:  0.4285229444503784
train gradient:  0.2285623314455699
iteration : 10798
train acc:  0.8203125
train loss:  0.3979683816432953
train gradient:  0.2907813769423619
iteration : 10799
train acc:  0.890625
train loss:  0.2626999020576477
train gradient:  0.11144112590288732
iteration : 10800
train acc:  0.90625
train loss:  0.2110297679901123
train gradient:  0.15691275786573589
iteration : 10801
train acc:  0.828125
train loss:  0.3764963448047638
train gradient:  0.1884804317007639
iteration : 10802
train acc:  0.84375
train loss:  0.35936784744262695
train gradient:  0.20086068073141944
iteration : 10803
train acc:  0.890625
train loss:  0.26457345485687256
train gradient:  0.08170390670761535
iteration : 10804
train acc:  0.8671875
train loss:  0.29685378074645996
train gradient:  0.13997169686976685
iteration : 10805
train acc:  0.8515625
train loss:  0.33493441343307495
train gradient:  0.11401113077455663
iteration : 10806
train acc:  0.8359375
train loss:  0.42732810974121094
train gradient:  0.2935880515678728
iteration : 10807
train acc:  0.78125
train loss:  0.45900076627731323
train gradient:  0.22980056082175626
iteration : 10808
train acc:  0.828125
train loss:  0.3995940089225769
train gradient:  0.20731998140398394
iteration : 10809
train acc:  0.8671875
train loss:  0.3158951997756958
train gradient:  0.14291569239597923
iteration : 10810
train acc:  0.859375
train loss:  0.31628167629241943
train gradient:  0.14878622535821162
iteration : 10811
train acc:  0.8671875
train loss:  0.37742680311203003
train gradient:  0.14441300601266832
iteration : 10812
train acc:  0.8984375
train loss:  0.2997339963912964
train gradient:  0.11214512117900909
iteration : 10813
train acc:  0.90625
train loss:  0.3073890805244446
train gradient:  0.13374837582196622
iteration : 10814
train acc:  0.8203125
train loss:  0.40381911396980286
train gradient:  0.1895120785745626
iteration : 10815
train acc:  0.7890625
train loss:  0.433246374130249
train gradient:  0.2710878886485381
iteration : 10816
train acc:  0.8671875
train loss:  0.3001725673675537
train gradient:  0.17987988399858013
iteration : 10817
train acc:  0.828125
train loss:  0.33476361632347107
train gradient:  0.17319058143055563
iteration : 10818
train acc:  0.8515625
train loss:  0.3457779288291931
train gradient:  0.17261711586062028
iteration : 10819
train acc:  0.8828125
train loss:  0.28684568405151367
train gradient:  0.11828763580384247
iteration : 10820
train acc:  0.875
train loss:  0.27496957778930664
train gradient:  0.11566794404010758
iteration : 10821
train acc:  0.84375
train loss:  0.33924171328544617
train gradient:  0.16082691201979502
iteration : 10822
train acc:  0.84375
train loss:  0.37694692611694336
train gradient:  0.15971623138711705
iteration : 10823
train acc:  0.859375
train loss:  0.2996949553489685
train gradient:  0.13903376666746287
iteration : 10824
train acc:  0.875
train loss:  0.3104900121688843
train gradient:  0.17284558284398205
iteration : 10825
train acc:  0.9140625
train loss:  0.24516841769218445
train gradient:  0.07956552863300993
iteration : 10826
train acc:  0.8828125
train loss:  0.26965755224227905
train gradient:  0.12031081341880796
iteration : 10827
train acc:  0.859375
train loss:  0.31153348088264465
train gradient:  0.14607147242885393
iteration : 10828
train acc:  0.890625
train loss:  0.23430636525154114
train gradient:  0.10243115594294308
iteration : 10829
train acc:  0.84375
train loss:  0.2893168330192566
train gradient:  0.09716732067881405
iteration : 10830
train acc:  0.828125
train loss:  0.4013260304927826
train gradient:  0.19507471425525058
iteration : 10831
train acc:  0.765625
train loss:  0.49289366602897644
train gradient:  0.33435129979108497
iteration : 10832
train acc:  0.8515625
train loss:  0.3135516345500946
train gradient:  0.13501468539821973
iteration : 10833
train acc:  0.859375
train loss:  0.36425045132637024
train gradient:  0.1651578131649878
iteration : 10834
train acc:  0.859375
train loss:  0.2711488604545593
train gradient:  0.1324813265231426
iteration : 10835
train acc:  0.875
train loss:  0.2661505341529846
train gradient:  0.14493066823027642
iteration : 10836
train acc:  0.8515625
train loss:  0.3355114459991455
train gradient:  0.14883810792914184
iteration : 10837
train acc:  0.8515625
train loss:  0.3700464963912964
train gradient:  0.154856074616953
iteration : 10838
train acc:  0.859375
train loss:  0.347420334815979
train gradient:  0.282921506804383
iteration : 10839
train acc:  0.8984375
train loss:  0.2816714644432068
train gradient:  0.08542600079506556
iteration : 10840
train acc:  0.828125
train loss:  0.38339900970458984
train gradient:  0.19901677003928925
iteration : 10841
train acc:  0.8828125
train loss:  0.3188841640949249
train gradient:  0.12526296375209628
iteration : 10842
train acc:  0.8359375
train loss:  0.33305662870407104
train gradient:  0.12967812289358688
iteration : 10843
train acc:  0.8671875
train loss:  0.30554184317588806
train gradient:  0.1655055980391824
iteration : 10844
train acc:  0.8125
train loss:  0.37398064136505127
train gradient:  0.16584309454111296
iteration : 10845
train acc:  0.8828125
train loss:  0.2882094979286194
train gradient:  0.12518150413868998
iteration : 10846
train acc:  0.890625
train loss:  0.28832656145095825
train gradient:  0.10353877408849027
iteration : 10847
train acc:  0.8359375
train loss:  0.3582969009876251
train gradient:  0.13831196693567072
iteration : 10848
train acc:  0.8125
train loss:  0.40748530626296997
train gradient:  0.19107958253683044
iteration : 10849
train acc:  0.8671875
train loss:  0.30735963582992554
train gradient:  0.14298853190199529
iteration : 10850
train acc:  0.84375
train loss:  0.2892640233039856
train gradient:  0.10332536270747446
iteration : 10851
train acc:  0.8359375
train loss:  0.4208034873008728
train gradient:  0.2355021451777769
iteration : 10852
train acc:  0.875
train loss:  0.313269704580307
train gradient:  0.16776973661346717
iteration : 10853
train acc:  0.8671875
train loss:  0.3184528052806854
train gradient:  0.10032610761635935
iteration : 10854
train acc:  0.8359375
train loss:  0.34371811151504517
train gradient:  0.14921138086174995
iteration : 10855
train acc:  0.8984375
train loss:  0.23736897110939026
train gradient:  0.09513859284420496
iteration : 10856
train acc:  0.90625
train loss:  0.2577311396598816
train gradient:  0.11088587913501394
iteration : 10857
train acc:  0.8671875
train loss:  0.28192567825317383
train gradient:  0.08460779259255066
iteration : 10858
train acc:  0.8203125
train loss:  0.47354546189308167
train gradient:  0.2681256668348026
iteration : 10859
train acc:  0.84375
train loss:  0.3198821544647217
train gradient:  0.19856505270197689
iteration : 10860
train acc:  0.859375
train loss:  0.3474269509315491
train gradient:  0.2064771601266381
iteration : 10861
train acc:  0.84375
train loss:  0.36565738916397095
train gradient:  0.3193959062671565
iteration : 10862
train acc:  0.8046875
train loss:  0.3990352749824524
train gradient:  0.21318303781567902
iteration : 10863
train acc:  0.8828125
train loss:  0.3188477158546448
train gradient:  0.21282435591391197
iteration : 10864
train acc:  0.921875
train loss:  0.28455236554145813
train gradient:  0.10320361227455832
iteration : 10865
train acc:  0.8828125
train loss:  0.2562555968761444
train gradient:  0.09707654510607129
iteration : 10866
train acc:  0.8515625
train loss:  0.30139651894569397
train gradient:  0.1782955083509688
iteration : 10867
train acc:  0.90625
train loss:  0.2895745038986206
train gradient:  0.2109627664025011
iteration : 10868
train acc:  0.78125
train loss:  0.44512882828712463
train gradient:  0.25458178350180527
iteration : 10869
train acc:  0.9140625
train loss:  0.24740709364414215
train gradient:  0.09992990518787762
iteration : 10870
train acc:  0.8515625
train loss:  0.3621028661727905
train gradient:  0.15074177501441804
iteration : 10871
train acc:  0.8828125
train loss:  0.2956615686416626
train gradient:  0.09090560687030055
iteration : 10872
train acc:  0.8671875
train loss:  0.3357953429222107
train gradient:  0.195003379911521
iteration : 10873
train acc:  0.84375
train loss:  0.4086357355117798
train gradient:  0.2270075422319494
iteration : 10874
train acc:  0.8125
train loss:  0.3875201940536499
train gradient:  0.18703874663126835
iteration : 10875
train acc:  0.8515625
train loss:  0.3252864181995392
train gradient:  0.1711113328757083
iteration : 10876
train acc:  0.8671875
train loss:  0.30854475498199463
train gradient:  0.14232254101430464
iteration : 10877
train acc:  0.859375
train loss:  0.3634081482887268
train gradient:  0.25953079578294663
iteration : 10878
train acc:  0.859375
train loss:  0.31252148747444153
train gradient:  0.7196927840793064
iteration : 10879
train acc:  0.8359375
train loss:  0.36549052596092224
train gradient:  0.20847115602119506
iteration : 10880
train acc:  0.796875
train loss:  0.3477548062801361
train gradient:  0.15210531569306276
iteration : 10881
train acc:  0.875
train loss:  0.332548588514328
train gradient:  0.1691845416719111
iteration : 10882
train acc:  0.8515625
train loss:  0.34945279359817505
train gradient:  0.2383460750648041
iteration : 10883
train acc:  0.828125
train loss:  0.39190396666526794
train gradient:  0.1624036925746747
iteration : 10884
train acc:  0.859375
train loss:  0.3658169209957123
train gradient:  0.20646328214556064
iteration : 10885
train acc:  0.9140625
train loss:  0.26865166425704956
train gradient:  0.08765780147886057
iteration : 10886
train acc:  0.8359375
train loss:  0.3898204565048218
train gradient:  0.18034381635861108
iteration : 10887
train acc:  0.875
train loss:  0.28971564769744873
train gradient:  0.1364777655816627
iteration : 10888
train acc:  0.8515625
train loss:  0.31968721747398376
train gradient:  0.15206150557382886
iteration : 10889
train acc:  0.8671875
train loss:  0.40781649947166443
train gradient:  0.1950805857889073
iteration : 10890
train acc:  0.8203125
train loss:  0.39886024594306946
train gradient:  0.20749348437423218
iteration : 10891
train acc:  0.875
train loss:  0.32898345589637756
train gradient:  0.11714478877674699
iteration : 10892
train acc:  0.890625
train loss:  0.3137247562408447
train gradient:  0.12453603294859834
iteration : 10893
train acc:  0.875
train loss:  0.29971057176589966
train gradient:  0.11943135797112925
iteration : 10894
train acc:  0.84375
train loss:  0.33396270871162415
train gradient:  0.21212083553915562
iteration : 10895
train acc:  0.84375
train loss:  0.32324498891830444
train gradient:  0.14954547517183692
iteration : 10896
train acc:  0.890625
train loss:  0.26666921377182007
train gradient:  0.09853158505651287
iteration : 10897
train acc:  0.8515625
train loss:  0.32016265392303467
train gradient:  0.17452975881482075
iteration : 10898
train acc:  0.8671875
train loss:  0.30135154724121094
train gradient:  0.17601363636429004
iteration : 10899
train acc:  0.859375
train loss:  0.28867268562316895
train gradient:  0.11025486183306418
iteration : 10900
train acc:  0.9296875
train loss:  0.24616211652755737
train gradient:  0.09623324757021468
iteration : 10901
train acc:  0.875
train loss:  0.3028436005115509
train gradient:  0.11931515358903755
iteration : 10902
train acc:  0.8125
train loss:  0.38025015592575073
train gradient:  0.25054801159081036
iteration : 10903
train acc:  0.8984375
train loss:  0.2663472890853882
train gradient:  0.12753959689469746
iteration : 10904
train acc:  0.828125
train loss:  0.3204839527606964
train gradient:  0.15083478316152182
iteration : 10905
train acc:  0.84375
train loss:  0.3195514678955078
train gradient:  0.07821805204963578
iteration : 10906
train acc:  0.7734375
train loss:  0.4604435861110687
train gradient:  0.2497974774487456
iteration : 10907
train acc:  0.859375
train loss:  0.34528958797454834
train gradient:  0.1942232213755875
iteration : 10908
train acc:  0.859375
train loss:  0.31557130813598633
train gradient:  0.18800172183914385
iteration : 10909
train acc:  0.859375
train loss:  0.29067257046699524
train gradient:  0.12079440016151399
iteration : 10910
train acc:  0.8515625
train loss:  0.3679734170436859
train gradient:  0.16187834593384254
iteration : 10911
train acc:  0.84375
train loss:  0.3010856509208679
train gradient:  0.1920304154230827
iteration : 10912
train acc:  0.90625
train loss:  0.2732830345630646
train gradient:  0.13269621501768777
iteration : 10913
train acc:  0.84375
train loss:  0.36956584453582764
train gradient:  0.1957972910450867
iteration : 10914
train acc:  0.8515625
train loss:  0.3430970311164856
train gradient:  0.1573416066924428
iteration : 10915
train acc:  0.8046875
train loss:  0.36880964040756226
train gradient:  0.1455030045023317
iteration : 10916
train acc:  0.828125
train loss:  0.34340453147888184
train gradient:  0.15059689524712927
iteration : 10917
train acc:  0.90625
train loss:  0.26381462812423706
train gradient:  0.09931454359863993
iteration : 10918
train acc:  0.8203125
train loss:  0.4108571410179138
train gradient:  0.16085251895016756
iteration : 10919
train acc:  0.859375
train loss:  0.35750600695610046
train gradient:  0.21604171081008622
iteration : 10920
train acc:  0.8671875
train loss:  0.330902099609375
train gradient:  0.18437484113325492
iteration : 10921
train acc:  0.8515625
train loss:  0.3327796161174774
train gradient:  0.13149464077540998
iteration : 10922
train acc:  0.8203125
train loss:  0.4229712188243866
train gradient:  0.18413516715519915
iteration : 10923
train acc:  0.921875
train loss:  0.23889309167861938
train gradient:  0.10287009396426078
iteration : 10924
train acc:  0.859375
train loss:  0.30663001537323
train gradient:  0.12125287580696742
iteration : 10925
train acc:  0.8515625
train loss:  0.3166714310646057
train gradient:  0.12769307792139717
iteration : 10926
train acc:  0.859375
train loss:  0.30799752473831177
train gradient:  0.16365749743850655
iteration : 10927
train acc:  0.859375
train loss:  0.3507521152496338
train gradient:  0.16835704453475886
iteration : 10928
train acc:  0.890625
train loss:  0.3094952702522278
train gradient:  0.12969321785317622
iteration : 10929
train acc:  0.8359375
train loss:  0.35137298703193665
train gradient:  0.13459496787713998
iteration : 10930
train acc:  0.8671875
train loss:  0.3618096113204956
train gradient:  0.16955413014941384
iteration : 10931
train acc:  0.8671875
train loss:  0.3515844941139221
train gradient:  0.3021849149443655
iteration : 10932
train acc:  0.8515625
train loss:  0.3268764913082123
train gradient:  0.13267564002314922
iteration : 10933
train acc:  0.828125
train loss:  0.40522727370262146
train gradient:  0.17841588435628958
iteration : 10934
train acc:  0.84375
train loss:  0.3106797933578491
train gradient:  0.15828731710816965
iteration : 10935
train acc:  0.8515625
train loss:  0.31263649463653564
train gradient:  0.1066904881037019
iteration : 10936
train acc:  0.8203125
train loss:  0.36573755741119385
train gradient:  0.11810501735032905
iteration : 10937
train acc:  0.8515625
train loss:  0.31758591532707214
train gradient:  0.15482079956067812
iteration : 10938
train acc:  0.8515625
train loss:  0.35830509662628174
train gradient:  0.14381611739971117
iteration : 10939
train acc:  0.8203125
train loss:  0.3758386969566345
train gradient:  0.1878179594383006
iteration : 10940
train acc:  0.8671875
train loss:  0.3255773186683655
train gradient:  0.19347984535725543
iteration : 10941
train acc:  0.8828125
train loss:  0.2595052719116211
train gradient:  0.09687327293577205
iteration : 10942
train acc:  0.890625
train loss:  0.25049281120300293
train gradient:  0.10502192916563598
iteration : 10943
train acc:  0.875
train loss:  0.3264656066894531
train gradient:  0.19728829796601777
iteration : 10944
train acc:  0.765625
train loss:  0.42079734802246094
train gradient:  0.2247965431388757
iteration : 10945
train acc:  0.8828125
train loss:  0.31357207894325256
train gradient:  0.1923888284205544
iteration : 10946
train acc:  0.8359375
train loss:  0.3697963058948517
train gradient:  0.2231568605933051
iteration : 10947
train acc:  0.875
train loss:  0.33969926834106445
train gradient:  0.12757746114340962
iteration : 10948
train acc:  0.8359375
train loss:  0.34656810760498047
train gradient:  0.13614668128543422
iteration : 10949
train acc:  0.8203125
train loss:  0.3314504325389862
train gradient:  0.11389570714345328
iteration : 10950
train acc:  0.828125
train loss:  0.34812650084495544
train gradient:  0.15406087008338804
iteration : 10951
train acc:  0.8203125
train loss:  0.39431115984916687
train gradient:  0.2875666972726322
iteration : 10952
train acc:  0.8984375
train loss:  0.29069650173187256
train gradient:  0.0988112199451024
iteration : 10953
train acc:  0.8515625
train loss:  0.31750747561454773
train gradient:  0.16987794339226592
iteration : 10954
train acc:  0.8671875
train loss:  0.3379983901977539
train gradient:  0.12829345950773055
iteration : 10955
train acc:  0.8203125
train loss:  0.43575793504714966
train gradient:  0.25320431174884495
iteration : 10956
train acc:  0.921875
train loss:  0.2768074870109558
train gradient:  0.10047646260179018
iteration : 10957
train acc:  0.8359375
train loss:  0.3954315483570099
train gradient:  0.1908073314686476
iteration : 10958
train acc:  0.8671875
train loss:  0.3068522810935974
train gradient:  0.14464895837815656
iteration : 10959
train acc:  0.84375
train loss:  0.33153036236763
train gradient:  0.12448299930076649
iteration : 10960
train acc:  0.8671875
train loss:  0.3424531817436218
train gradient:  0.1077687182231377
iteration : 10961
train acc:  0.859375
train loss:  0.32056453824043274
train gradient:  0.14252279592629133
iteration : 10962
train acc:  0.8515625
train loss:  0.3638150691986084
train gradient:  0.2066268284955992
iteration : 10963
train acc:  0.8984375
train loss:  0.2767120599746704
train gradient:  0.11931743255323583
iteration : 10964
train acc:  0.890625
train loss:  0.3855626881122589
train gradient:  0.21302795169874764
iteration : 10965
train acc:  0.8515625
train loss:  0.312518835067749
train gradient:  0.11392343370924235
iteration : 10966
train acc:  0.8203125
train loss:  0.3214685320854187
train gradient:  0.13782195720359372
iteration : 10967
train acc:  0.84375
train loss:  0.4040040373802185
train gradient:  0.1548916406401878
iteration : 10968
train acc:  0.828125
train loss:  0.37448936700820923
train gradient:  0.1745710270560093
iteration : 10969
train acc:  0.8515625
train loss:  0.3592723608016968
train gradient:  0.1763442528844859
iteration : 10970
train acc:  0.875
train loss:  0.3557458519935608
train gradient:  0.2562803208674799
iteration : 10971
train acc:  0.859375
train loss:  0.3421243727207184
train gradient:  0.14218851294586793
iteration : 10972
train acc:  0.8984375
train loss:  0.29196298122406006
train gradient:  0.1506465844997435
iteration : 10973
train acc:  0.7890625
train loss:  0.39192521572113037
train gradient:  0.17688346269607091
iteration : 10974
train acc:  0.875
train loss:  0.2819302976131439
train gradient:  0.11678394236029223
iteration : 10975
train acc:  0.859375
train loss:  0.3208499848842621
train gradient:  0.11276412540361112
iteration : 10976
train acc:  0.8828125
train loss:  0.2895083427429199
train gradient:  0.16084579639405153
iteration : 10977
train acc:  0.8515625
train loss:  0.35499322414398193
train gradient:  0.20282582741095664
iteration : 10978
train acc:  0.8828125
train loss:  0.27799341082572937
train gradient:  0.10755533169844665
iteration : 10979
train acc:  0.859375
train loss:  0.31279170513153076
train gradient:  0.12095031139235747
iteration : 10980
train acc:  0.8515625
train loss:  0.3211808204650879
train gradient:  0.2751464341448953
iteration : 10981
train acc:  0.828125
train loss:  0.3584953546524048
train gradient:  0.17874247411333827
iteration : 10982
train acc:  0.8125
train loss:  0.3518263101577759
train gradient:  0.15292113375673483
iteration : 10983
train acc:  0.828125
train loss:  0.40006178617477417
train gradient:  0.22364053960934643
iteration : 10984
train acc:  0.8828125
train loss:  0.27307015657424927
train gradient:  0.10712343775529967
iteration : 10985
train acc:  0.84375
train loss:  0.3544155955314636
train gradient:  0.16171989429130051
iteration : 10986
train acc:  0.875
train loss:  0.2957364320755005
train gradient:  0.12888695492459834
iteration : 10987
train acc:  0.8046875
train loss:  0.4144739508628845
train gradient:  0.4292165688097683
iteration : 10988
train acc:  0.90625
train loss:  0.25623273849487305
train gradient:  0.1009541512182382
iteration : 10989
train acc:  0.8359375
train loss:  0.2795719504356384
train gradient:  0.09705622502023283
iteration : 10990
train acc:  0.84375
train loss:  0.3600728511810303
train gradient:  0.21051968141221306
iteration : 10991
train acc:  0.859375
train loss:  0.2946282923221588
train gradient:  0.0998234703080199
iteration : 10992
train acc:  0.8828125
train loss:  0.2719242572784424
train gradient:  0.1563319782871443
iteration : 10993
train acc:  0.84375
train loss:  0.344579815864563
train gradient:  0.177352944147799
iteration : 10994
train acc:  0.9140625
train loss:  0.23592813313007355
train gradient:  0.08813528790270557
iteration : 10995
train acc:  0.796875
train loss:  0.3669757843017578
train gradient:  0.1477921354404
iteration : 10996
train acc:  0.890625
train loss:  0.24929288029670715
train gradient:  0.12345262168421184
iteration : 10997
train acc:  0.8203125
train loss:  0.3352789580821991
train gradient:  0.17253965222061818
iteration : 10998
train acc:  0.8515625
train loss:  0.41486072540283203
train gradient:  0.23627022056775646
iteration : 10999
train acc:  0.8359375
train loss:  0.4612562656402588
train gradient:  0.3099337856465644
iteration : 11000
train acc:  0.8515625
train loss:  0.3442859649658203
train gradient:  0.15427149202325563
iteration : 11001
train acc:  0.875
train loss:  0.3041209280490875
train gradient:  0.14415601966054778
iteration : 11002
train acc:  0.8828125
train loss:  0.2983231246471405
train gradient:  0.14092800412428205
iteration : 11003
train acc:  0.8515625
train loss:  0.3032248914241791
train gradient:  0.1405777537382929
iteration : 11004
train acc:  0.875
train loss:  0.3457285761833191
train gradient:  0.19183694800560447
iteration : 11005
train acc:  0.84375
train loss:  0.291124165058136
train gradient:  0.12526260709154063
iteration : 11006
train acc:  0.90625
train loss:  0.28466159105300903
train gradient:  0.11834905824718904
iteration : 11007
train acc:  0.8046875
train loss:  0.3541901111602783
train gradient:  0.16414956037244932
iteration : 11008
train acc:  0.8359375
train loss:  0.37238311767578125
train gradient:  0.16384586393280537
iteration : 11009
train acc:  0.8359375
train loss:  0.33318862318992615
train gradient:  0.15290657195784127
iteration : 11010
train acc:  0.84375
train loss:  0.32658839225769043
train gradient:  0.12689751235734986
iteration : 11011
train acc:  0.84375
train loss:  0.3905419409275055
train gradient:  0.23557922102218454
iteration : 11012
train acc:  0.828125
train loss:  0.3338664472103119
train gradient:  0.16316774327608177
iteration : 11013
train acc:  0.8125
train loss:  0.4189801812171936
train gradient:  0.21375244060183143
iteration : 11014
train acc:  0.8203125
train loss:  0.4229001998901367
train gradient:  0.22023537263737913
iteration : 11015
train acc:  0.8828125
train loss:  0.3360815644264221
train gradient:  0.15077005769863044
iteration : 11016
train acc:  0.8515625
train loss:  0.40902286767959595
train gradient:  0.1368686636340689
iteration : 11017
train acc:  0.828125
train loss:  0.36483678221702576
train gradient:  0.15012850977722642
iteration : 11018
train acc:  0.828125
train loss:  0.4162040650844574
train gradient:  0.18762335975846578
iteration : 11019
train acc:  0.8359375
train loss:  0.3679176867008209
train gradient:  0.22337891928270592
iteration : 11020
train acc:  0.8984375
train loss:  0.3128226399421692
train gradient:  0.12157835801058822
iteration : 11021
train acc:  0.875
train loss:  0.30227670073509216
train gradient:  0.11379111138973785
iteration : 11022
train acc:  0.8671875
train loss:  0.329233855009079
train gradient:  0.1565589108980554
iteration : 11023
train acc:  0.859375
train loss:  0.3454127311706543
train gradient:  0.1275951889156869
iteration : 11024
train acc:  0.8046875
train loss:  0.38885563611984253
train gradient:  0.1659338402555191
iteration : 11025
train acc:  0.8125
train loss:  0.3959196209907532
train gradient:  0.26239530061274563
iteration : 11026
train acc:  0.8515625
train loss:  0.3931213617324829
train gradient:  0.1766680549488483
iteration : 11027
train acc:  0.8828125
train loss:  0.2686823010444641
train gradient:  0.12299298979826069
iteration : 11028
train acc:  0.8515625
train loss:  0.3084878921508789
train gradient:  0.12410951467059862
iteration : 11029
train acc:  0.8203125
train loss:  0.4278247356414795
train gradient:  0.20568949543748316
iteration : 11030
train acc:  0.828125
train loss:  0.42085975408554077
train gradient:  0.20412162541137152
iteration : 11031
train acc:  0.8671875
train loss:  0.2872026860713959
train gradient:  0.1610119489175314
iteration : 11032
train acc:  0.8125
train loss:  0.3910738229751587
train gradient:  0.1569674736690446
iteration : 11033
train acc:  0.875
train loss:  0.34539520740509033
train gradient:  0.17550037552786346
iteration : 11034
train acc:  0.84375
train loss:  0.32978010177612305
train gradient:  0.12947493746760405
iteration : 11035
train acc:  0.8671875
train loss:  0.24471424520015717
train gradient:  0.07900175829319558
iteration : 11036
train acc:  0.9140625
train loss:  0.21736076474189758
train gradient:  0.0843901619102017
iteration : 11037
train acc:  0.859375
train loss:  0.3132915794849396
train gradient:  0.13124860239327168
iteration : 11038
train acc:  0.890625
train loss:  0.2909001111984253
train gradient:  0.11187914532889091
iteration : 11039
train acc:  0.8203125
train loss:  0.37958431243896484
train gradient:  0.14957763935813
iteration : 11040
train acc:  0.84375
train loss:  0.32163530588150024
train gradient:  0.0985018481632666
iteration : 11041
train acc:  0.828125
train loss:  0.35318616032600403
train gradient:  0.16758724681411843
iteration : 11042
train acc:  0.8828125
train loss:  0.2865244746208191
train gradient:  0.1546876390138622
iteration : 11043
train acc:  0.8515625
train loss:  0.29181671142578125
train gradient:  0.10244424495153628
iteration : 11044
train acc:  0.8984375
train loss:  0.2524450421333313
train gradient:  0.09395446880262986
iteration : 11045
train acc:  0.84375
train loss:  0.3281942307949066
train gradient:  0.10639805387962264
iteration : 11046
train acc:  0.84375
train loss:  0.32297492027282715
train gradient:  0.11918960322260379
iteration : 11047
train acc:  0.8984375
train loss:  0.2562183439731598
train gradient:  0.0956213579274743
iteration : 11048
train acc:  0.8828125
train loss:  0.28998732566833496
train gradient:  0.12406785036953029
iteration : 11049
train acc:  0.859375
train loss:  0.29773974418640137
train gradient:  0.10498926619530913
iteration : 11050
train acc:  0.8828125
train loss:  0.3581424951553345
train gradient:  0.20313168271641896
iteration : 11051
train acc:  0.875
train loss:  0.3169250190258026
train gradient:  0.1413093795983545
iteration : 11052
train acc:  0.8828125
train loss:  0.32501503825187683
train gradient:  0.1168719525048749
iteration : 11053
train acc:  0.9140625
train loss:  0.2449253350496292
train gradient:  0.12201563855368168
iteration : 11054
train acc:  0.875
train loss:  0.30102917551994324
train gradient:  0.1291163182061974
iteration : 11055
train acc:  0.890625
train loss:  0.3062984347343445
train gradient:  0.1032867962641929
iteration : 11056
train acc:  0.8515625
train loss:  0.3371936082839966
train gradient:  0.15745786921200627
iteration : 11057
train acc:  0.8984375
train loss:  0.2601885497570038
train gradient:  0.11467764574889179
iteration : 11058
train acc:  0.859375
train loss:  0.3020106554031372
train gradient:  0.15103813924254672
iteration : 11059
train acc:  0.859375
train loss:  0.31550395488739014
train gradient:  0.15496029510426157
iteration : 11060
train acc:  0.90625
train loss:  0.2237699031829834
train gradient:  0.09467979832628981
iteration : 11061
train acc:  0.859375
train loss:  0.3783171474933624
train gradient:  0.1921979154958805
iteration : 11062
train acc:  0.90625
train loss:  0.230120450258255
train gradient:  0.09990485487473112
iteration : 11063
train acc:  0.875
train loss:  0.3067907989025116
train gradient:  0.19667339799284983
iteration : 11064
train acc:  0.8515625
train loss:  0.28302955627441406
train gradient:  0.12570866367324635
iteration : 11065
train acc:  0.8359375
train loss:  0.36813852190971375
train gradient:  0.15201146927127307
iteration : 11066
train acc:  0.8515625
train loss:  0.317097544670105
train gradient:  0.15443559011724017
iteration : 11067
train acc:  0.8828125
train loss:  0.2601233720779419
train gradient:  0.11665217911978458
iteration : 11068
train acc:  0.859375
train loss:  0.3417591452598572
train gradient:  0.1615859969289693
iteration : 11069
train acc:  0.8046875
train loss:  0.3565850257873535
train gradient:  0.16650745183464516
iteration : 11070
train acc:  0.8828125
train loss:  0.29774945974349976
train gradient:  0.1195793168056718
iteration : 11071
train acc:  0.9140625
train loss:  0.21831543743610382
train gradient:  0.07778784254636333
iteration : 11072
train acc:  0.8671875
train loss:  0.3003609776496887
train gradient:  0.17280949260918554
iteration : 11073
train acc:  0.7734375
train loss:  0.4085231423377991
train gradient:  0.24635253026200227
iteration : 11074
train acc:  0.8203125
train loss:  0.3599267303943634
train gradient:  0.2849099319304831
iteration : 11075
train acc:  0.828125
train loss:  0.39964812994003296
train gradient:  0.22444485328995145
iteration : 11076
train acc:  0.8671875
train loss:  0.340327650308609
train gradient:  0.1431525755002711
iteration : 11077
train acc:  0.8984375
train loss:  0.2904251217842102
train gradient:  0.10831469596924825
iteration : 11078
train acc:  0.875
train loss:  0.2827025055885315
train gradient:  0.09042181688946153
iteration : 11079
train acc:  0.828125
train loss:  0.46944481134414673
train gradient:  0.41572073216500427
iteration : 11080
train acc:  0.8828125
train loss:  0.2963593602180481
train gradient:  0.08039459239158789
iteration : 11081
train acc:  0.890625
train loss:  0.2375178039073944
train gradient:  0.08140943546318564
iteration : 11082
train acc:  0.8671875
train loss:  0.37094855308532715
train gradient:  0.18425055072551
iteration : 11083
train acc:  0.875
train loss:  0.29302647709846497
train gradient:  0.15656115806297655
iteration : 11084
train acc:  0.8359375
train loss:  0.412992388010025
train gradient:  0.2681175800127009
iteration : 11085
train acc:  0.9140625
train loss:  0.23874320089817047
train gradient:  0.1296024393177775
iteration : 11086
train acc:  0.8671875
train loss:  0.28484708070755005
train gradient:  0.16229481614764457
iteration : 11087
train acc:  0.859375
train loss:  0.3061077892780304
train gradient:  0.1510717452180766
iteration : 11088
train acc:  0.859375
train loss:  0.2590561509132385
train gradient:  0.1554092249254985
iteration : 11089
train acc:  0.8203125
train loss:  0.35286885499954224
train gradient:  0.23996820102439692
iteration : 11090
train acc:  0.8984375
train loss:  0.3000880181789398
train gradient:  0.2119407256413669
iteration : 11091
train acc:  0.8359375
train loss:  0.3155577778816223
train gradient:  0.16545974766505214
iteration : 11092
train acc:  0.8359375
train loss:  0.3552522659301758
train gradient:  0.2104160104493451
iteration : 11093
train acc:  0.890625
train loss:  0.2949979603290558
train gradient:  0.1435273557117552
iteration : 11094
train acc:  0.8671875
train loss:  0.32522475719451904
train gradient:  0.11845131735513946
iteration : 11095
train acc:  0.875
train loss:  0.294002503156662
train gradient:  0.1041209771640887
iteration : 11096
train acc:  0.8671875
train loss:  0.33803489804267883
train gradient:  0.1979115004909786
iteration : 11097
train acc:  0.8359375
train loss:  0.4006064534187317
train gradient:  0.21417921989684752
iteration : 11098
train acc:  0.8515625
train loss:  0.3359946012496948
train gradient:  0.14980447072734154
iteration : 11099
train acc:  0.8515625
train loss:  0.3417159616947174
train gradient:  0.2178228890113127
iteration : 11100
train acc:  0.8984375
train loss:  0.24652986228466034
train gradient:  0.10938173583773637
iteration : 11101
train acc:  0.8359375
train loss:  0.3638504147529602
train gradient:  0.1360656588437175
iteration : 11102
train acc:  0.78125
train loss:  0.4997234642505646
train gradient:  0.30941280543432825
iteration : 11103
train acc:  0.8828125
train loss:  0.343065083026886
train gradient:  0.145420318321551
iteration : 11104
train acc:  0.8515625
train loss:  0.3569627106189728
train gradient:  0.16413762565679055
iteration : 11105
train acc:  0.84375
train loss:  0.33130133152008057
train gradient:  0.1502699414726323
iteration : 11106
train acc:  0.8125
train loss:  0.3722514808177948
train gradient:  0.23152840044172163
iteration : 11107
train acc:  0.84375
train loss:  0.37218159437179565
train gradient:  0.19976015608604075
iteration : 11108
train acc:  0.8359375
train loss:  0.3205566704273224
train gradient:  0.16005500478722076
iteration : 11109
train acc:  0.8515625
train loss:  0.35704028606414795
train gradient:  0.23449566542156294
iteration : 11110
train acc:  0.8359375
train loss:  0.3148813247680664
train gradient:  0.1449450789345481
iteration : 11111
train acc:  0.8671875
train loss:  0.3246150314807892
train gradient:  0.12165993530385782
iteration : 11112
train acc:  0.921875
train loss:  0.2123871147632599
train gradient:  0.10351021292008372
iteration : 11113
train acc:  0.84375
train loss:  0.3397314250469208
train gradient:  0.18430927338406278
iteration : 11114
train acc:  0.8046875
train loss:  0.3860756456851959
train gradient:  0.19934280282951458
iteration : 11115
train acc:  0.8984375
train loss:  0.24827995896339417
train gradient:  0.11882951331054556
iteration : 11116
train acc:  0.875
train loss:  0.29101431369781494
train gradient:  0.1729654360974412
iteration : 11117
train acc:  0.8515625
train loss:  0.3879741430282593
train gradient:  0.2171857539969242
iteration : 11118
train acc:  0.8984375
train loss:  0.29152101278305054
train gradient:  0.10342969214485064
iteration : 11119
train acc:  0.8671875
train loss:  0.33460503816604614
train gradient:  0.14395143985569617
iteration : 11120
train acc:  0.8359375
train loss:  0.3777011036872864
train gradient:  0.1754465153368068
iteration : 11121
train acc:  0.875
train loss:  0.2775755524635315
train gradient:  0.1518945333650707
iteration : 11122
train acc:  0.8828125
train loss:  0.32010596990585327
train gradient:  0.209268001694037
iteration : 11123
train acc:  0.8828125
train loss:  0.3006313443183899
train gradient:  0.12130429059687242
iteration : 11124
train acc:  0.875
train loss:  0.3428908586502075
train gradient:  0.19670552809556646
iteration : 11125
train acc:  0.859375
train loss:  0.292836993932724
train gradient:  0.12986448893522956
iteration : 11126
train acc:  0.859375
train loss:  0.33162763714790344
train gradient:  0.1694314452257522
iteration : 11127
train acc:  0.84375
train loss:  0.3237459063529968
train gradient:  0.151160751023819
iteration : 11128
train acc:  0.828125
train loss:  0.3342350721359253
train gradient:  0.17648051518094626
iteration : 11129
train acc:  0.875
train loss:  0.3040028214454651
train gradient:  0.12823909725578309
iteration : 11130
train acc:  0.828125
train loss:  0.3434118330478668
train gradient:  0.15046310526355425
iteration : 11131
train acc:  0.859375
train loss:  0.33262336254119873
train gradient:  0.17017164497135104
iteration : 11132
train acc:  0.890625
train loss:  0.31870073080062866
train gradient:  0.13904760888770396
iteration : 11133
train acc:  0.890625
train loss:  0.255353182554245
train gradient:  0.09989912585487914
iteration : 11134
train acc:  0.8828125
train loss:  0.2997659742832184
train gradient:  0.11799063662838026
iteration : 11135
train acc:  0.7890625
train loss:  0.36882251501083374
train gradient:  0.22797077105644228
iteration : 11136
train acc:  0.9140625
train loss:  0.2446765899658203
train gradient:  0.12189995243954929
iteration : 11137
train acc:  0.8515625
train loss:  0.35703742504119873
train gradient:  0.1943250189776584
iteration : 11138
train acc:  0.8515625
train loss:  0.34790265560150146
train gradient:  0.186225837278014
iteration : 11139
train acc:  0.859375
train loss:  0.28048497438430786
train gradient:  0.09914130708762167
iteration : 11140
train acc:  0.8828125
train loss:  0.3055115044116974
train gradient:  0.1358506316774638
iteration : 11141
train acc:  0.8515625
train loss:  0.30842870473861694
train gradient:  0.23522432118599984
iteration : 11142
train acc:  0.8984375
train loss:  0.29517602920532227
train gradient:  0.1258492513137494
iteration : 11143
train acc:  0.8125
train loss:  0.34560203552246094
train gradient:  0.1385989718178111
iteration : 11144
train acc:  0.890625
train loss:  0.26507827639579773
train gradient:  0.13161548427699515
iteration : 11145
train acc:  0.8359375
train loss:  0.33188748359680176
train gradient:  0.15955662506555485
iteration : 11146
train acc:  0.859375
train loss:  0.33801499009132385
train gradient:  0.15106904916379216
iteration : 11147
train acc:  0.8828125
train loss:  0.25495684146881104
train gradient:  0.12259135193604191
iteration : 11148
train acc:  0.859375
train loss:  0.3115992248058319
train gradient:  0.12000139217283522
iteration : 11149
train acc:  0.875
train loss:  0.300801157951355
train gradient:  0.1677256185989118
iteration : 11150
train acc:  0.9140625
train loss:  0.2630767822265625
train gradient:  0.10795790526714835
iteration : 11151
train acc:  0.8203125
train loss:  0.3775653541088104
train gradient:  0.16901452253081228
iteration : 11152
train acc:  0.9609375
train loss:  0.1806301772594452
train gradient:  0.07591877635947349
iteration : 11153
train acc:  0.890625
train loss:  0.2659197449684143
train gradient:  0.13977213393730087
iteration : 11154
train acc:  0.84375
train loss:  0.30616864562034607
train gradient:  0.10620943849468673
iteration : 11155
train acc:  0.9140625
train loss:  0.3118213415145874
train gradient:  0.157850479079397
iteration : 11156
train acc:  0.90625
train loss:  0.23414205014705658
train gradient:  0.1317663829902981
iteration : 11157
train acc:  0.875
train loss:  0.2992089092731476
train gradient:  0.15344146583290585
iteration : 11158
train acc:  0.828125
train loss:  0.37639081478118896
train gradient:  0.17541845659325317
iteration : 11159
train acc:  0.796875
train loss:  0.3647167682647705
train gradient:  0.2083741000628967
iteration : 11160
train acc:  0.84375
train loss:  0.3196612596511841
train gradient:  0.149365413128132
iteration : 11161
train acc:  0.859375
train loss:  0.28044724464416504
train gradient:  0.21440226440063648
iteration : 11162
train acc:  0.8515625
train loss:  0.29937106370925903
train gradient:  0.17967063496034041
iteration : 11163
train acc:  0.8984375
train loss:  0.25251707434654236
train gradient:  0.09831410823698186
iteration : 11164
train acc:  0.765625
train loss:  0.44265130162239075
train gradient:  0.2590991189169464
iteration : 11165
train acc:  0.90625
train loss:  0.288030743598938
train gradient:  0.18655211033565444
iteration : 11166
train acc:  0.875
train loss:  0.3076024651527405
train gradient:  0.17671419271506755
iteration : 11167
train acc:  0.875
train loss:  0.2882893681526184
train gradient:  0.0993897517069861
iteration : 11168
train acc:  0.9140625
train loss:  0.2558511793613434
train gradient:  0.08057832381301032
iteration : 11169
train acc:  0.8671875
train loss:  0.26749753952026367
train gradient:  0.09854700697935537
iteration : 11170
train acc:  0.8828125
train loss:  0.3129795789718628
train gradient:  0.12217620080440361
iteration : 11171
train acc:  0.8984375
train loss:  0.24982082843780518
train gradient:  0.10020505872953521
iteration : 11172
train acc:  0.875
train loss:  0.308856338262558
train gradient:  0.14627121323676873
iteration : 11173
train acc:  0.9140625
train loss:  0.25981515645980835
train gradient:  0.0937661921641215
iteration : 11174
train acc:  0.9140625
train loss:  0.20294968783855438
train gradient:  0.06683231741219355
iteration : 11175
train acc:  0.8359375
train loss:  0.36126041412353516
train gradient:  0.20595571167082516
iteration : 11176
train acc:  0.8359375
train loss:  0.3391845226287842
train gradient:  0.1798095021800495
iteration : 11177
train acc:  0.828125
train loss:  0.4124745726585388
train gradient:  0.35552817181413715
iteration : 11178
train acc:  0.90625
train loss:  0.24723492562770844
train gradient:  0.12037671703436618
iteration : 11179
train acc:  0.8359375
train loss:  0.3113488256931305
train gradient:  0.1185297072489204
iteration : 11180
train acc:  0.90625
train loss:  0.2703160047531128
train gradient:  0.11533770290300041
iteration : 11181
train acc:  0.8671875
train loss:  0.3240870237350464
train gradient:  0.18691209235105083
iteration : 11182
train acc:  0.8359375
train loss:  0.45707374811172485
train gradient:  0.31591406424540297
iteration : 11183
train acc:  0.859375
train loss:  0.3224792182445526
train gradient:  0.18468871400444556
iteration : 11184
train acc:  0.8671875
train loss:  0.2988220453262329
train gradient:  0.1609014883800297
iteration : 11185
train acc:  0.8515625
train loss:  0.3415300250053406
train gradient:  0.21983187547846295
iteration : 11186
train acc:  0.8984375
train loss:  0.29280781745910645
train gradient:  0.1331537084404339
iteration : 11187
train acc:  0.828125
train loss:  0.3355780839920044
train gradient:  0.1862848429378035
iteration : 11188
train acc:  0.8359375
train loss:  0.3881610929965973
train gradient:  0.2417339925506365
iteration : 11189
train acc:  0.84375
train loss:  0.3462291955947876
train gradient:  0.18214876338782754
iteration : 11190
train acc:  0.8203125
train loss:  0.3545728921890259
train gradient:  0.16977946339989092
iteration : 11191
train acc:  0.8828125
train loss:  0.2884499430656433
train gradient:  0.11835334401447942
iteration : 11192
train acc:  0.8203125
train loss:  0.3753385841846466
train gradient:  0.22337189579931452
iteration : 11193
train acc:  0.8125
train loss:  0.3875565528869629
train gradient:  0.13430278541198562
iteration : 11194
train acc:  0.859375
train loss:  0.3144979476928711
train gradient:  0.17470991854905726
iteration : 11195
train acc:  0.9140625
train loss:  0.2380654662847519
train gradient:  0.0957310336100479
iteration : 11196
train acc:  0.84375
train loss:  0.3991706073284149
train gradient:  0.183958796077143
iteration : 11197
train acc:  0.8125
train loss:  0.41630756855010986
train gradient:  0.29968470525353175
iteration : 11198
train acc:  0.8515625
train loss:  0.31096094846725464
train gradient:  0.12280931940286796
iteration : 11199
train acc:  0.859375
train loss:  0.36841630935668945
train gradient:  0.16047340134800375
iteration : 11200
train acc:  0.859375
train loss:  0.2776123881340027
train gradient:  0.11933877672858871
iteration : 11201
train acc:  0.8671875
train loss:  0.26424503326416016
train gradient:  0.11366442701241664
iteration : 11202
train acc:  0.8046875
train loss:  0.42639410495758057
train gradient:  0.41116649291949475
iteration : 11203
train acc:  0.859375
train loss:  0.331278920173645
train gradient:  0.2060089328922894
iteration : 11204
train acc:  0.875
train loss:  0.32270753383636475
train gradient:  0.13136495283918131
iteration : 11205
train acc:  0.8359375
train loss:  0.36255204677581787
train gradient:  0.15186409258155786
iteration : 11206
train acc:  0.8828125
train loss:  0.3048560619354248
train gradient:  0.1380053225563812
iteration : 11207
train acc:  0.890625
train loss:  0.2938912510871887
train gradient:  0.1509550467613931
iteration : 11208
train acc:  0.8984375
train loss:  0.26908695697784424
train gradient:  0.09940077491655055
iteration : 11209
train acc:  0.8671875
train loss:  0.31878483295440674
train gradient:  0.16886733531747788
iteration : 11210
train acc:  0.8671875
train loss:  0.3216622471809387
train gradient:  0.11210675233895588
iteration : 11211
train acc:  0.84375
train loss:  0.36758047342300415
train gradient:  0.17320293758276578
iteration : 11212
train acc:  0.84375
train loss:  0.3817829489707947
train gradient:  0.21662741682405134
iteration : 11213
train acc:  0.859375
train loss:  0.36177462339401245
train gradient:  0.22067319013094383
iteration : 11214
train acc:  0.8984375
train loss:  0.23965293169021606
train gradient:  0.0684161579920727
iteration : 11215
train acc:  0.890625
train loss:  0.2912362813949585
train gradient:  0.1324463783021072
iteration : 11216
train acc:  0.890625
train loss:  0.2899317145347595
train gradient:  0.14706118565996545
iteration : 11217
train acc:  0.84375
train loss:  0.35036540031433105
train gradient:  0.14263196169164305
iteration : 11218
train acc:  0.8515625
train loss:  0.28193020820617676
train gradient:  0.11644239483297829
iteration : 11219
train acc:  0.7890625
train loss:  0.40437549352645874
train gradient:  0.20289884055389976
iteration : 11220
train acc:  0.8828125
train loss:  0.2668328881263733
train gradient:  0.09413143463565213
iteration : 11221
train acc:  0.8828125
train loss:  0.26963815093040466
train gradient:  0.1176822278100017
iteration : 11222
train acc:  0.8984375
train loss:  0.26950594782829285
train gradient:  0.10255723693677654
iteration : 11223
train acc:  0.828125
train loss:  0.37255895137786865
train gradient:  0.1352960743544117
iteration : 11224
train acc:  0.8671875
train loss:  0.26814645528793335
train gradient:  0.14328002710329457
iteration : 11225
train acc:  0.90625
train loss:  0.2103305459022522
train gradient:  0.0710776631191113
iteration : 11226
train acc:  0.78125
train loss:  0.49964192509651184
train gradient:  0.28365133131408166
iteration : 11227
train acc:  0.890625
train loss:  0.24129073321819305
train gradient:  0.10241371690031617
iteration : 11228
train acc:  0.875
train loss:  0.30433914065361023
train gradient:  0.11206734295450907
iteration : 11229
train acc:  0.8671875
train loss:  0.3331827223300934
train gradient:  0.1601092240802192
iteration : 11230
train acc:  0.796875
train loss:  0.42766013741493225
train gradient:  0.2912403099752372
iteration : 11231
train acc:  0.8828125
train loss:  0.2971563935279846
train gradient:  0.13409381612979554
iteration : 11232
train acc:  0.8671875
train loss:  0.30627596378326416
train gradient:  0.15307129550622234
iteration : 11233
train acc:  0.8828125
train loss:  0.24364887177944183
train gradient:  0.11233093249607601
iteration : 11234
train acc:  0.859375
train loss:  0.29881712794303894
train gradient:  0.12455872567610696
iteration : 11235
train acc:  0.890625
train loss:  0.3471529185771942
train gradient:  0.10169189651954996
iteration : 11236
train acc:  0.8984375
train loss:  0.2791115939617157
train gradient:  0.16986899792531368
iteration : 11237
train acc:  0.84375
train loss:  0.34462401270866394
train gradient:  0.14180852492076734
iteration : 11238
train acc:  0.8515625
train loss:  0.3546028733253479
train gradient:  0.17839824301924714
iteration : 11239
train acc:  0.859375
train loss:  0.3072165250778198
train gradient:  0.16680855061485206
iteration : 11240
train acc:  0.84375
train loss:  0.340298593044281
train gradient:  0.18735697070004398
iteration : 11241
train acc:  0.859375
train loss:  0.29184848070144653
train gradient:  0.1296513393002225
iteration : 11242
train acc:  0.8828125
train loss:  0.28607863187789917
train gradient:  0.1232519393173872
iteration : 11243
train acc:  0.8984375
train loss:  0.2742559015750885
train gradient:  0.12058782086308031
iteration : 11244
train acc:  0.8828125
train loss:  0.280794620513916
train gradient:  0.12385680736176219
iteration : 11245
train acc:  0.84375
train loss:  0.37597551941871643
train gradient:  0.1916439801239308
iteration : 11246
train acc:  0.8515625
train loss:  0.3338996171951294
train gradient:  0.1818181204117839
iteration : 11247
train acc:  0.875
train loss:  0.26253852248191833
train gradient:  0.08610238448004942
iteration : 11248
train acc:  0.828125
train loss:  0.41358256340026855
train gradient:  0.2785389736959102
iteration : 11249
train acc:  0.84375
train loss:  0.32092031836509705
train gradient:  0.20352850649253051
iteration : 11250
train acc:  0.84375
train loss:  0.34999117255210876
train gradient:  0.18969343899970376
iteration : 11251
train acc:  0.8203125
train loss:  0.3764286935329437
train gradient:  0.20888622384964314
iteration : 11252
train acc:  0.8671875
train loss:  0.32054898142814636
train gradient:  0.1347665101731512
iteration : 11253
train acc:  0.8984375
train loss:  0.2928973436355591
train gradient:  0.12847733501781286
iteration : 11254
train acc:  0.8515625
train loss:  0.3063420355319977
train gradient:  0.1346449728745226
iteration : 11255
train acc:  0.828125
train loss:  0.3661297559738159
train gradient:  0.2121109104755509
iteration : 11256
train acc:  0.8984375
train loss:  0.26310113072395325
train gradient:  0.13726975988506504
iteration : 11257
train acc:  0.921875
train loss:  0.21109125018119812
train gradient:  0.10973841797014817
iteration : 11258
train acc:  0.8359375
train loss:  0.33275946974754333
train gradient:  0.1557983443379634
iteration : 11259
train acc:  0.8203125
train loss:  0.3676743507385254
train gradient:  0.17182036594779979
iteration : 11260
train acc:  0.859375
train loss:  0.305337518453598
train gradient:  0.13251842235577516
iteration : 11261
train acc:  0.84375
train loss:  0.38569581508636475
train gradient:  0.18909483675680086
iteration : 11262
train acc:  0.8515625
train loss:  0.2586451768875122
train gradient:  0.15093143399265815
iteration : 11263
train acc:  0.8671875
train loss:  0.3060225248336792
train gradient:  0.13497744036865145
iteration : 11264
train acc:  0.859375
train loss:  0.29208701848983765
train gradient:  0.12496246614539064
iteration : 11265
train acc:  0.8984375
train loss:  0.3026413917541504
train gradient:  0.1023555572641927
iteration : 11266
train acc:  0.8671875
train loss:  0.3836987018585205
train gradient:  0.19338459751942205
iteration : 11267
train acc:  0.875
train loss:  0.31459665298461914
train gradient:  0.1397811355059432
iteration : 11268
train acc:  0.828125
train loss:  0.32815033197402954
train gradient:  0.1359686534692589
iteration : 11269
train acc:  0.8046875
train loss:  0.4282848834991455
train gradient:  0.2683290729590499
iteration : 11270
train acc:  0.8828125
train loss:  0.3374270796775818
train gradient:  0.19654604283016502
iteration : 11271
train acc:  0.890625
train loss:  0.2505018711090088
train gradient:  0.11854666033609558
iteration : 11272
train acc:  0.9140625
train loss:  0.2003578245639801
train gradient:  0.06464829234049961
iteration : 11273
train acc:  0.859375
train loss:  0.3440468907356262
train gradient:  0.12841087164064569
iteration : 11274
train acc:  0.875
train loss:  0.24810202419757843
train gradient:  0.07459260890419397
iteration : 11275
train acc:  0.8671875
train loss:  0.3428274095058441
train gradient:  0.1883672765496805
iteration : 11276
train acc:  0.8515625
train loss:  0.37222108244895935
train gradient:  0.22185471423734926
iteration : 11277
train acc:  0.8828125
train loss:  0.30901864171028137
train gradient:  0.12675669792171357
iteration : 11278
train acc:  0.875
train loss:  0.3237088918685913
train gradient:  0.12769609375882782
iteration : 11279
train acc:  0.8828125
train loss:  0.27430886030197144
train gradient:  0.07964770703734382
iteration : 11280
train acc:  0.8515625
train loss:  0.3458706736564636
train gradient:  0.18016784768735156
iteration : 11281
train acc:  0.859375
train loss:  0.3352828621864319
train gradient:  0.1930227448903559
iteration : 11282
train acc:  0.828125
train loss:  0.38084906339645386
train gradient:  0.21466767906496642
iteration : 11283
train acc:  0.828125
train loss:  0.33777865767478943
train gradient:  0.13672234257937144
iteration : 11284
train acc:  0.78125
train loss:  0.4513646364212036
train gradient:  0.26183902821714544
iteration : 11285
train acc:  0.8515625
train loss:  0.28926050662994385
train gradient:  0.10218340764944725
iteration : 11286
train acc:  0.859375
train loss:  0.3300057649612427
train gradient:  0.1531987065902348
iteration : 11287
train acc:  0.8046875
train loss:  0.42182543873786926
train gradient:  0.2080132167207599
iteration : 11288
train acc:  0.8203125
train loss:  0.3256571292877197
train gradient:  0.14054722401157393
iteration : 11289
train acc:  0.84375
train loss:  0.3663012683391571
train gradient:  0.18435938045179465
iteration : 11290
train acc:  0.890625
train loss:  0.30041027069091797
train gradient:  0.13636159980681256
iteration : 11291
train acc:  0.8515625
train loss:  0.28395333886146545
train gradient:  0.0849237088314532
iteration : 11292
train acc:  0.859375
train loss:  0.2969527840614319
train gradient:  0.1310281243812311
iteration : 11293
train acc:  0.8046875
train loss:  0.4027821123600006
train gradient:  0.18803123581409342
iteration : 11294
train acc:  0.8203125
train loss:  0.4284992814064026
train gradient:  0.16202829676745123
iteration : 11295
train acc:  0.8984375
train loss:  0.27015990018844604
train gradient:  0.09259359569825673
iteration : 11296
train acc:  0.9296875
train loss:  0.24968817830085754
train gradient:  0.12540779470380134
iteration : 11297
train acc:  0.84375
train loss:  0.31764858961105347
train gradient:  0.15130268880417455
iteration : 11298
train acc:  0.8828125
train loss:  0.25787514448165894
train gradient:  0.0966845590604915
iteration : 11299
train acc:  0.8515625
train loss:  0.3060397505760193
train gradient:  0.11629619899250922
iteration : 11300
train acc:  0.875
train loss:  0.3225533366203308
train gradient:  0.17590945533997956
iteration : 11301
train acc:  0.84375
train loss:  0.328143835067749
train gradient:  0.1961690673152034
iteration : 11302
train acc:  0.8828125
train loss:  0.35922712087631226
train gradient:  0.1675040373623166
iteration : 11303
train acc:  0.8828125
train loss:  0.2974923253059387
train gradient:  0.14716782755250013
iteration : 11304
train acc:  0.8984375
train loss:  0.2668374180793762
train gradient:  0.1410063197479433
iteration : 11305
train acc:  0.8515625
train loss:  0.33224940299987793
train gradient:  0.1398223242191006
iteration : 11306
train acc:  0.8671875
train loss:  0.28926289081573486
train gradient:  0.1274421330601739
iteration : 11307
train acc:  0.8515625
train loss:  0.3358112871646881
train gradient:  0.11346591912943141
iteration : 11308
train acc:  0.890625
train loss:  0.28011348843574524
train gradient:  0.11549669397174005
iteration : 11309
train acc:  0.8671875
train loss:  0.3471805155277252
train gradient:  0.25480408955305267
iteration : 11310
train acc:  0.8671875
train loss:  0.33508801460266113
train gradient:  0.1847570507468687
iteration : 11311
train acc:  0.859375
train loss:  0.29566699266433716
train gradient:  0.12511337071816292
iteration : 11312
train acc:  0.828125
train loss:  0.35566872358322144
train gradient:  0.1583956072715827
iteration : 11313
train acc:  0.8671875
train loss:  0.2922351360321045
train gradient:  0.1014049332786039
iteration : 11314
train acc:  0.828125
train loss:  0.34028521180152893
train gradient:  0.1245553409532413
iteration : 11315
train acc:  0.8359375
train loss:  0.2990811765193939
train gradient:  0.12319506840383156
iteration : 11316
train acc:  0.8828125
train loss:  0.3201809525489807
train gradient:  0.13875380516026894
iteration : 11317
train acc:  0.859375
train loss:  0.2991167902946472
train gradient:  0.11791917562096524
iteration : 11318
train acc:  0.84375
train loss:  0.41298210620880127
train gradient:  0.17750751994019742
iteration : 11319
train acc:  0.78125
train loss:  0.4229310154914856
train gradient:  0.22540705389477214
iteration : 11320
train acc:  0.90625
train loss:  0.2534913420677185
train gradient:  0.13352135400811538
iteration : 11321
train acc:  0.875
train loss:  0.3105320632457733
train gradient:  0.12271815084332527
iteration : 11322
train acc:  0.8671875
train loss:  0.2685970664024353
train gradient:  0.13549870560457805
iteration : 11323
train acc:  0.875
train loss:  0.27908575534820557
train gradient:  0.10577752310248179
iteration : 11324
train acc:  0.875
train loss:  0.325543075799942
train gradient:  0.14132253996450667
iteration : 11325
train acc:  0.828125
train loss:  0.4677773118019104
train gradient:  0.30001181609071925
iteration : 11326
train acc:  0.890625
train loss:  0.2979264557361603
train gradient:  0.08816786063804832
iteration : 11327
train acc:  0.8515625
train loss:  0.31311413645744324
train gradient:  0.115360847670282
iteration : 11328
train acc:  0.90625
train loss:  0.24278490245342255
train gradient:  0.1029300815836288
iteration : 11329
train acc:  0.828125
train loss:  0.37942934036254883
train gradient:  0.14964889944882903
iteration : 11330
train acc:  0.875
train loss:  0.26615110039711
train gradient:  0.11083317458893815
iteration : 11331
train acc:  0.859375
train loss:  0.3287651836872101
train gradient:  0.10436000377019444
iteration : 11332
train acc:  0.8984375
train loss:  0.3057131767272949
train gradient:  0.12375472537180311
iteration : 11333
train acc:  0.8828125
train loss:  0.24077610671520233
train gradient:  0.07143742357273533
iteration : 11334
train acc:  0.8515625
train loss:  0.3667491674423218
train gradient:  0.3437227678142353
iteration : 11335
train acc:  0.8984375
train loss:  0.2769968509674072
train gradient:  0.14784143964596586
iteration : 11336
train acc:  0.8359375
train loss:  0.28408896923065186
train gradient:  0.11664172823339879
iteration : 11337
train acc:  0.8671875
train loss:  0.30188220739364624
train gradient:  0.18968201422635173
iteration : 11338
train acc:  0.84375
train loss:  0.35719892382621765
train gradient:  0.17217829195660428
iteration : 11339
train acc:  0.8984375
train loss:  0.33525288105010986
train gradient:  0.17894780258937704
iteration : 11340
train acc:  0.859375
train loss:  0.3023276925086975
train gradient:  0.12169953400080773
iteration : 11341
train acc:  0.859375
train loss:  0.35337793827056885
train gradient:  0.13982392080229888
iteration : 11342
train acc:  0.8515625
train loss:  0.3371788561344147
train gradient:  0.13555152677625198
iteration : 11343
train acc:  0.8125
train loss:  0.34688323736190796
train gradient:  0.15969906533267628
iteration : 11344
train acc:  0.84375
train loss:  0.2876397669315338
train gradient:  0.1421553092703602
iteration : 11345
train acc:  0.875
train loss:  0.24642398953437805
train gradient:  0.10977484068368282
iteration : 11346
train acc:  0.8984375
train loss:  0.32148265838623047
train gradient:  0.12018606813889463
iteration : 11347
train acc:  0.921875
train loss:  0.226476788520813
train gradient:  0.06735511197709042
iteration : 11348
train acc:  0.8515625
train loss:  0.32992756366729736
train gradient:  0.1418799915336295
iteration : 11349
train acc:  0.859375
train loss:  0.32598111033439636
train gradient:  0.13978219272121994
iteration : 11350
train acc:  0.8203125
train loss:  0.35927391052246094
train gradient:  0.21969835077226757
iteration : 11351
train acc:  0.8125
train loss:  0.3760952353477478
train gradient:  0.18985915051355548
iteration : 11352
train acc:  0.8828125
train loss:  0.24506402015686035
train gradient:  0.21044840610806087
iteration : 11353
train acc:  0.8515625
train loss:  0.3490804433822632
train gradient:  0.13913853596802983
iteration : 11354
train acc:  0.859375
train loss:  0.27921050786972046
train gradient:  0.09955339893308408
iteration : 11355
train acc:  0.859375
train loss:  0.33986762166023254
train gradient:  0.18457984664939533
iteration : 11356
train acc:  0.8515625
train loss:  0.322964072227478
train gradient:  0.11972533380011582
iteration : 11357
train acc:  0.890625
train loss:  0.3022911548614502
train gradient:  0.1298261233606751
iteration : 11358
train acc:  0.859375
train loss:  0.3019537925720215
train gradient:  0.11704840644705751
iteration : 11359
train acc:  0.84375
train loss:  0.3337163031101227
train gradient:  0.1814598282553666
iteration : 11360
train acc:  0.8671875
train loss:  0.3389400243759155
train gradient:  0.19561183691432263
iteration : 11361
train acc:  0.8359375
train loss:  0.34664085507392883
train gradient:  0.14495565883197262
iteration : 11362
train acc:  0.84375
train loss:  0.295294851064682
train gradient:  0.14758522963742193
iteration : 11363
train acc:  0.859375
train loss:  0.35265499353408813
train gradient:  0.16407614031679368
iteration : 11364
train acc:  0.84375
train loss:  0.31397610902786255
train gradient:  0.18996951123775285
iteration : 11365
train acc:  0.84375
train loss:  0.36349430680274963
train gradient:  0.1730820523621455
iteration : 11366
train acc:  0.8515625
train loss:  0.3501827120780945
train gradient:  0.17903231572493933
iteration : 11367
train acc:  0.9296875
train loss:  0.2421623319387436
train gradient:  0.08782166391702974
iteration : 11368
train acc:  0.8359375
train loss:  0.40597212314605713
train gradient:  0.24109947092747014
iteration : 11369
train acc:  0.8359375
train loss:  0.3965533375740051
train gradient:  0.22496809734334167
iteration : 11370
train acc:  0.859375
train loss:  0.3642340898513794
train gradient:  0.15195760922615784
iteration : 11371
train acc:  0.90625
train loss:  0.297563374042511
train gradient:  0.13763386527578053
iteration : 11372
train acc:  0.859375
train loss:  0.3460838794708252
train gradient:  0.17751114918634217
iteration : 11373
train acc:  0.796875
train loss:  0.4000048041343689
train gradient:  0.18224259716020608
iteration : 11374
train acc:  0.890625
train loss:  0.31919461488723755
train gradient:  0.12142007050402419
iteration : 11375
train acc:  0.8203125
train loss:  0.4140147268772125
train gradient:  0.27749129827954394
iteration : 11376
train acc:  0.8671875
train loss:  0.29331064224243164
train gradient:  0.1540145602192191
iteration : 11377
train acc:  0.8828125
train loss:  0.27883222699165344
train gradient:  0.08969450395611679
iteration : 11378
train acc:  0.828125
train loss:  0.33413803577423096
train gradient:  0.15072743148251666
iteration : 11379
train acc:  0.84375
train loss:  0.2895248532295227
train gradient:  0.13710384221872257
iteration : 11380
train acc:  0.8671875
train loss:  0.31914544105529785
train gradient:  0.14804587866242008
iteration : 11381
train acc:  0.921875
train loss:  0.2458399534225464
train gradient:  0.09920265551247855
iteration : 11382
train acc:  0.8125
train loss:  0.38018471002578735
train gradient:  0.2523937120737234
iteration : 11383
train acc:  0.8515625
train loss:  0.348818302154541
train gradient:  0.2067059377682604
iteration : 11384
train acc:  0.9140625
train loss:  0.30692121386528015
train gradient:  0.09917929746559292
iteration : 11385
train acc:  0.8359375
train loss:  0.29738402366638184
train gradient:  0.1215043660408556
iteration : 11386
train acc:  0.8984375
train loss:  0.2628667950630188
train gradient:  0.14305837123574408
iteration : 11387
train acc:  0.875
train loss:  0.24719102680683136
train gradient:  0.1343103708748904
iteration : 11388
train acc:  0.8359375
train loss:  0.30178409814834595
train gradient:  0.12974836587881328
iteration : 11389
train acc:  0.90625
train loss:  0.2648788094520569
train gradient:  0.08886704155823631
iteration : 11390
train acc:  0.8984375
train loss:  0.24892501533031464
train gradient:  0.14091834766128114
iteration : 11391
train acc:  0.8671875
train loss:  0.3327059745788574
train gradient:  0.15101930611472186
iteration : 11392
train acc:  0.796875
train loss:  0.40809381008148193
train gradient:  0.2189118625540934
iteration : 11393
train acc:  0.90625
train loss:  0.258482962846756
train gradient:  0.09017937274658186
iteration : 11394
train acc:  0.921875
train loss:  0.28510618209838867
train gradient:  0.12691642896408922
iteration : 11395
train acc:  0.890625
train loss:  0.3030547797679901
train gradient:  0.13366003864827475
iteration : 11396
train acc:  0.890625
train loss:  0.24095122516155243
train gradient:  0.09210786033828275
iteration : 11397
train acc:  0.8671875
train loss:  0.3289933204650879
train gradient:  0.13270171476506873
iteration : 11398
train acc:  0.8828125
train loss:  0.2637936770915985
train gradient:  0.08798117989582971
iteration : 11399
train acc:  0.8515625
train loss:  0.30932819843292236
train gradient:  0.13000954809450083
iteration : 11400
train acc:  0.8828125
train loss:  0.29189518094062805
train gradient:  0.1461112589753471
iteration : 11401
train acc:  0.8984375
train loss:  0.3000102639198303
train gradient:  0.14531047379626746
iteration : 11402
train acc:  0.859375
train loss:  0.3475635349750519
train gradient:  0.2205026773589041
iteration : 11403
train acc:  0.921875
train loss:  0.2401459515094757
train gradient:  0.11411069844836043
iteration : 11404
train acc:  0.828125
train loss:  0.4266382157802582
train gradient:  0.2651814116424397
iteration : 11405
train acc:  0.890625
train loss:  0.2731344997882843
train gradient:  0.11199431100296148
iteration : 11406
train acc:  0.84375
train loss:  0.3412686586380005
train gradient:  0.24457947865762286
iteration : 11407
train acc:  0.8515625
train loss:  0.35957229137420654
train gradient:  0.18657126857559075
iteration : 11408
train acc:  0.9140625
train loss:  0.2774936258792877
train gradient:  0.1305503983720719
iteration : 11409
train acc:  0.8671875
train loss:  0.27516859769821167
train gradient:  0.10175679384624266
iteration : 11410
train acc:  0.890625
train loss:  0.258933961391449
train gradient:  0.13085195938939903
iteration : 11411
train acc:  0.8828125
train loss:  0.25653690099716187
train gradient:  0.26813698585230467
iteration : 11412
train acc:  0.8515625
train loss:  0.28298014402389526
train gradient:  0.17661119865194702
iteration : 11413
train acc:  0.8828125
train loss:  0.28054332733154297
train gradient:  0.13943152509743872
iteration : 11414
train acc:  0.8125
train loss:  0.36302441358566284
train gradient:  0.2034417529161212
iteration : 11415
train acc:  0.890625
train loss:  0.3285384178161621
train gradient:  0.13850374389309475
iteration : 11416
train acc:  0.8515625
train loss:  0.3446584939956665
train gradient:  0.17272170595487815
iteration : 11417
train acc:  0.8984375
train loss:  0.2862505316734314
train gradient:  0.13734854965688578
iteration : 11418
train acc:  0.8515625
train loss:  0.3287537097930908
train gradient:  0.16196273112256637
iteration : 11419
train acc:  0.84375
train loss:  0.36550992727279663
train gradient:  0.31177498142772136
iteration : 11420
train acc:  0.8515625
train loss:  0.34534692764282227
train gradient:  0.14638494469721405
iteration : 11421
train acc:  0.8671875
train loss:  0.29778844118118286
train gradient:  0.14459457165097095
iteration : 11422
train acc:  0.875
train loss:  0.3019009530544281
train gradient:  0.12974935208522492
iteration : 11423
train acc:  0.9140625
train loss:  0.2549530863761902
train gradient:  0.09616729198626965
iteration : 11424
train acc:  0.9140625
train loss:  0.23090112209320068
train gradient:  0.10499503022479961
iteration : 11425
train acc:  0.875
train loss:  0.348844975233078
train gradient:  0.1760867277783008
iteration : 11426
train acc:  0.9140625
train loss:  0.2356921285390854
train gradient:  0.09946343295661228
iteration : 11427
train acc:  0.796875
train loss:  0.4907573163509369
train gradient:  0.2908910515855752
iteration : 11428
train acc:  0.84375
train loss:  0.3884778618812561
train gradient:  0.24311020067885086
iteration : 11429
train acc:  0.84375
train loss:  0.32129958271980286
train gradient:  0.22805180445066842
iteration : 11430
train acc:  0.828125
train loss:  0.34229037165641785
train gradient:  0.18556661828721413
iteration : 11431
train acc:  0.8984375
train loss:  0.23489367961883545
train gradient:  0.11591379031624022
iteration : 11432
train acc:  0.828125
train loss:  0.3260799050331116
train gradient:  0.15874965789166287
iteration : 11433
train acc:  0.8828125
train loss:  0.3051392436027527
train gradient:  0.1206700012682841
iteration : 11434
train acc:  0.859375
train loss:  0.3057261109352112
train gradient:  0.17479629523349496
iteration : 11435
train acc:  0.875
train loss:  0.3426457345485687
train gradient:  0.23426968309211954
iteration : 11436
train acc:  0.875
train loss:  0.29332780838012695
train gradient:  0.17733405282141418
iteration : 11437
train acc:  0.890625
train loss:  0.32452839612960815
train gradient:  0.1989526512833815
iteration : 11438
train acc:  0.875
train loss:  0.3231388032436371
train gradient:  0.16577669222124575
iteration : 11439
train acc:  0.875
train loss:  0.3429611921310425
train gradient:  0.18331329021715195
iteration : 11440
train acc:  0.8828125
train loss:  0.32736849784851074
train gradient:  0.12649196014258365
iteration : 11441
train acc:  0.8828125
train loss:  0.2882586121559143
train gradient:  0.20421134681181788
iteration : 11442
train acc:  0.8671875
train loss:  0.2739177942276001
train gradient:  0.16358346382280067
iteration : 11443
train acc:  0.84375
train loss:  0.32836854457855225
train gradient:  0.14503343727931617
iteration : 11444
train acc:  0.9140625
train loss:  0.28451237082481384
train gradient:  0.12057697490660178
iteration : 11445
train acc:  0.84375
train loss:  0.37215131521224976
train gradient:  0.18306854234972725
iteration : 11446
train acc:  0.828125
train loss:  0.33416134119033813
train gradient:  0.1586878098633327
iteration : 11447
train acc:  0.8671875
train loss:  0.3216656446456909
train gradient:  0.14296563321906638
iteration : 11448
train acc:  0.8671875
train loss:  0.3441775143146515
train gradient:  0.1271474651161661
iteration : 11449
train acc:  0.84375
train loss:  0.36634743213653564
train gradient:  0.16667792852349508
iteration : 11450
train acc:  0.8828125
train loss:  0.28910818696022034
train gradient:  0.11081629448469214
iteration : 11451
train acc:  0.875
train loss:  0.29141685366630554
train gradient:  0.14215129048792058
iteration : 11452
train acc:  0.8203125
train loss:  0.33192646503448486
train gradient:  0.22007452089097354
iteration : 11453
train acc:  0.890625
train loss:  0.26114851236343384
train gradient:  0.10211333466671929
iteration : 11454
train acc:  0.8359375
train loss:  0.34832635521888733
train gradient:  0.1426788394919351
iteration : 11455
train acc:  0.84375
train loss:  0.368632435798645
train gradient:  0.21659941600651864
iteration : 11456
train acc:  0.84375
train loss:  0.2996467053890228
train gradient:  0.19790156406636258
iteration : 11457
train acc:  0.8515625
train loss:  0.3597124218940735
train gradient:  0.2501392754391247
iteration : 11458
train acc:  0.8203125
train loss:  0.3717944324016571
train gradient:  0.20655195717022562
iteration : 11459
train acc:  0.84375
train loss:  0.3597707450389862
train gradient:  0.20195799533744
iteration : 11460
train acc:  0.921875
train loss:  0.2219865620136261
train gradient:  0.10843202679116926
iteration : 11461
train acc:  0.8984375
train loss:  0.24480009078979492
train gradient:  0.09125070280095945
iteration : 11462
train acc:  0.8515625
train loss:  0.3187945783138275
train gradient:  0.17635431223456297
iteration : 11463
train acc:  0.921875
train loss:  0.22380459308624268
train gradient:  0.08252091385974136
iteration : 11464
train acc:  0.8671875
train loss:  0.2734595537185669
train gradient:  0.1110215360484205
iteration : 11465
train acc:  0.8828125
train loss:  0.3136383891105652
train gradient:  0.17224728859494357
iteration : 11466
train acc:  0.859375
train loss:  0.3359462022781372
train gradient:  0.17793525293677942
iteration : 11467
train acc:  0.8515625
train loss:  0.3705418109893799
train gradient:  0.22987276354283043
iteration : 11468
train acc:  0.8359375
train loss:  0.37004852294921875
train gradient:  0.17135639510647485
iteration : 11469
train acc:  0.875
train loss:  0.26080548763275146
train gradient:  0.1270345586790882
iteration : 11470
train acc:  0.828125
train loss:  0.3262287378311157
train gradient:  0.20785407872415634
iteration : 11471
train acc:  0.8515625
train loss:  0.32735151052474976
train gradient:  0.17263839408015452
iteration : 11472
train acc:  0.8984375
train loss:  0.28258946537971497
train gradient:  0.1825592622744583
iteration : 11473
train acc:  0.8984375
train loss:  0.3038851022720337
train gradient:  0.13568930041908434
iteration : 11474
train acc:  0.859375
train loss:  0.3549272418022156
train gradient:  0.1552024694066181
iteration : 11475
train acc:  0.8359375
train loss:  0.3270255923271179
train gradient:  0.11576375306293028
iteration : 11476
train acc:  0.8359375
train loss:  0.36658167839050293
train gradient:  0.19514305493128611
iteration : 11477
train acc:  0.8828125
train loss:  0.31629782915115356
train gradient:  0.135897885485942
iteration : 11478
train acc:  0.8828125
train loss:  0.2788465619087219
train gradient:  0.12258584773249499
iteration : 11479
train acc:  0.875
train loss:  0.25840210914611816
train gradient:  0.10327216496110474
iteration : 11480
train acc:  0.890625
train loss:  0.2551337480545044
train gradient:  0.10021342204307097
iteration : 11481
train acc:  0.875
train loss:  0.2376420944929123
train gradient:  0.13801233034213928
iteration : 11482
train acc:  0.890625
train loss:  0.2914382219314575
train gradient:  0.1357862430665695
iteration : 11483
train acc:  0.9140625
train loss:  0.286354124546051
train gradient:  0.10928826143418217
iteration : 11484
train acc:  0.9140625
train loss:  0.2512539029121399
train gradient:  0.10499802253665774
iteration : 11485
train acc:  0.84375
train loss:  0.3243356943130493
train gradient:  0.20108907589592412
iteration : 11486
train acc:  0.765625
train loss:  0.43329644203186035
train gradient:  0.23050672386646648
iteration : 11487
train acc:  0.859375
train loss:  0.38419410586357117
train gradient:  0.229420748120916
iteration : 11488
train acc:  0.8359375
train loss:  0.36459246277809143
train gradient:  0.21806014651212213
iteration : 11489
train acc:  0.8359375
train loss:  0.330968976020813
train gradient:  0.154947127441561
iteration : 11490
train acc:  0.8203125
train loss:  0.43548280000686646
train gradient:  0.18189950576527558
iteration : 11491
train acc:  0.8984375
train loss:  0.2451128363609314
train gradient:  0.14900249803584123
iteration : 11492
train acc:  0.875
train loss:  0.27097365260124207
train gradient:  0.22893170910858612
iteration : 11493
train acc:  0.8203125
train loss:  0.4259919226169586
train gradient:  0.2633876613800917
iteration : 11494
train acc:  0.859375
train loss:  0.27859950065612793
train gradient:  0.15009864479559637
iteration : 11495
train acc:  0.8515625
train loss:  0.3082450032234192
train gradient:  0.13175544717866916
iteration : 11496
train acc:  0.8671875
train loss:  0.30631768703460693
train gradient:  0.12156151488726369
iteration : 11497
train acc:  0.8671875
train loss:  0.3469748795032501
train gradient:  0.18480542313143486
iteration : 11498
train acc:  0.8125
train loss:  0.3569831848144531
train gradient:  0.18731253727612907
iteration : 11499
train acc:  0.8671875
train loss:  0.31436628103256226
train gradient:  0.12650059441025652
iteration : 11500
train acc:  0.8984375
train loss:  0.2776504158973694
train gradient:  0.11140553870664542
iteration : 11501
train acc:  0.8203125
train loss:  0.3178076148033142
train gradient:  0.13003811437491575
iteration : 11502
train acc:  0.8984375
train loss:  0.24571534991264343
train gradient:  0.09885569659388313
iteration : 11503
train acc:  0.84375
train loss:  0.35486137866973877
train gradient:  0.16754328576920963
iteration : 11504
train acc:  0.9375
train loss:  0.20646020770072937
train gradient:  0.08556263288849507
iteration : 11505
train acc:  0.8984375
train loss:  0.22179189324378967
train gradient:  0.08169260632513493
iteration : 11506
train acc:  0.890625
train loss:  0.26537251472473145
train gradient:  0.08037545366600772
iteration : 11507
train acc:  0.8125
train loss:  0.4325661063194275
train gradient:  0.25843229455932293
iteration : 11508
train acc:  0.8671875
train loss:  0.3568710684776306
train gradient:  0.1662847328200326
iteration : 11509
train acc:  0.859375
train loss:  0.35604503750801086
train gradient:  0.2757800246975813
iteration : 11510
train acc:  0.8203125
train loss:  0.40876901149749756
train gradient:  0.2172068368523426
iteration : 11511
train acc:  0.828125
train loss:  0.35850203037261963
train gradient:  0.17911031902677005
iteration : 11512
train acc:  0.8671875
train loss:  0.33316922187805176
train gradient:  0.15714075186601803
iteration : 11513
train acc:  0.875
train loss:  0.2994416356086731
train gradient:  0.11088173675055153
iteration : 11514
train acc:  0.8515625
train loss:  0.3449587821960449
train gradient:  0.1526564948476274
iteration : 11515
train acc:  0.8515625
train loss:  0.27315157651901245
train gradient:  0.14371790740814422
iteration : 11516
train acc:  0.859375
train loss:  0.3647690415382385
train gradient:  0.24398267009458688
iteration : 11517
train acc:  0.9140625
train loss:  0.3264346122741699
train gradient:  0.19898601096818658
iteration : 11518
train acc:  0.78125
train loss:  0.4359612762928009
train gradient:  0.17834484636001374
iteration : 11519
train acc:  0.875
train loss:  0.330797016620636
train gradient:  0.12600051950342375
iteration : 11520
train acc:  0.84375
train loss:  0.3342946171760559
train gradient:  0.18649532296032217
iteration : 11521
train acc:  0.8359375
train loss:  0.3648903965950012
train gradient:  0.14261229475118478
iteration : 11522
train acc:  0.8515625
train loss:  0.3835137188434601
train gradient:  0.1412210213656766
iteration : 11523
train acc:  0.890625
train loss:  0.28934574127197266
train gradient:  0.12414950560645201
iteration : 11524
train acc:  0.9140625
train loss:  0.2271144986152649
train gradient:  0.10452225249881966
iteration : 11525
train acc:  0.8984375
train loss:  0.26071199774742126
train gradient:  0.08613715429932128
iteration : 11526
train acc:  0.828125
train loss:  0.3573608696460724
train gradient:  0.17889713557188475
iteration : 11527
train acc:  0.8515625
train loss:  0.35128939151763916
train gradient:  0.15634212409250772
iteration : 11528
train acc:  0.875
train loss:  0.30864018201828003
train gradient:  0.11115380252073334
iteration : 11529
train acc:  0.859375
train loss:  0.3009188771247864
train gradient:  0.14045128998109774
iteration : 11530
train acc:  0.890625
train loss:  0.24933624267578125
train gradient:  0.09787472403296993
iteration : 11531
train acc:  0.8125
train loss:  0.357667475938797
train gradient:  0.2172549751833379
iteration : 11532
train acc:  0.875
train loss:  0.36608773469924927
train gradient:  0.1569532320511443
iteration : 11533
train acc:  0.875
train loss:  0.32120269536972046
train gradient:  0.14230609823968976
iteration : 11534
train acc:  0.8515625
train loss:  0.3096925914287567
train gradient:  0.14644788600821285
iteration : 11535
train acc:  0.8515625
train loss:  0.31537994742393494
train gradient:  0.09971832458712956
iteration : 11536
train acc:  0.8359375
train loss:  0.3198237419128418
train gradient:  0.19165578105271136
iteration : 11537
train acc:  0.84375
train loss:  0.29609447717666626
train gradient:  0.15911116647742993
iteration : 11538
train acc:  0.8828125
train loss:  0.2944018840789795
train gradient:  0.12025961254004316
iteration : 11539
train acc:  0.890625
train loss:  0.24396160244941711
train gradient:  0.09324338793862857
iteration : 11540
train acc:  0.8671875
train loss:  0.2986048460006714
train gradient:  0.1296325950411834
iteration : 11541
train acc:  0.890625
train loss:  0.26433536410331726
train gradient:  0.1328005705092057
iteration : 11542
train acc:  0.890625
train loss:  0.2922515571117401
train gradient:  0.11832917468465244
iteration : 11543
train acc:  0.84375
train loss:  0.36669397354125977
train gradient:  0.18049280327610068
iteration : 11544
train acc:  0.8671875
train loss:  0.27705246210098267
train gradient:  0.11448382197543655
iteration : 11545
train acc:  0.8359375
train loss:  0.33001405000686646
train gradient:  0.1330809916060337
iteration : 11546
train acc:  0.8984375
train loss:  0.23202936351299286
train gradient:  0.10322001811887574
iteration : 11547
train acc:  0.8671875
train loss:  0.33077472448349
train gradient:  0.13467517044266686
iteration : 11548
train acc:  0.828125
train loss:  0.32349634170532227
train gradient:  0.13581640879401452
iteration : 11549
train acc:  0.875
train loss:  0.2544022798538208
train gradient:  0.11582689127935453
iteration : 11550
train acc:  0.875
train loss:  0.3072168231010437
train gradient:  0.1893055368616512
iteration : 11551
train acc:  0.8125
train loss:  0.3701530694961548
train gradient:  0.21920858032991525
iteration : 11552
train acc:  0.8046875
train loss:  0.3812406063079834
train gradient:  0.18733138324636092
iteration : 11553
train acc:  0.890625
train loss:  0.2662293314933777
train gradient:  0.07761142762226594
iteration : 11554
train acc:  0.890625
train loss:  0.2659558057785034
train gradient:  0.11216739014678752
iteration : 11555
train acc:  0.875
train loss:  0.2970140874385834
train gradient:  0.15833665065517694
iteration : 11556
train acc:  0.8671875
train loss:  0.3152617812156677
train gradient:  0.17206851291643488
iteration : 11557
train acc:  0.875
train loss:  0.3540807366371155
train gradient:  0.1587157722143454
iteration : 11558
train acc:  0.859375
train loss:  0.3437270522117615
train gradient:  0.16349319300572224
iteration : 11559
train acc:  0.8203125
train loss:  0.46321597695350647
train gradient:  0.3110083513749878
iteration : 11560
train acc:  0.8671875
train loss:  0.3714965283870697
train gradient:  0.18692420833109635
iteration : 11561
train acc:  0.859375
train loss:  0.3231240212917328
train gradient:  0.12003572925275889
iteration : 11562
train acc:  0.8203125
train loss:  0.36045169830322266
train gradient:  0.2450734858876118
iteration : 11563
train acc:  0.890625
train loss:  0.3053753972053528
train gradient:  0.1292480621193128
iteration : 11564
train acc:  0.8671875
train loss:  0.3295556902885437
train gradient:  0.13984677390265396
iteration : 11565
train acc:  0.890625
train loss:  0.2528932988643646
train gradient:  0.11112332343663552
iteration : 11566
train acc:  0.8671875
train loss:  0.3299140930175781
train gradient:  0.13934688643223572
iteration : 11567
train acc:  0.90625
train loss:  0.22645610570907593
train gradient:  0.11169197859692731
iteration : 11568
train acc:  0.8125
train loss:  0.35945039987564087
train gradient:  0.145159692907317
iteration : 11569
train acc:  0.84375
train loss:  0.407545804977417
train gradient:  0.18547267704356205
iteration : 11570
train acc:  0.8515625
train loss:  0.3652326464653015
train gradient:  0.19781497920984248
iteration : 11571
train acc:  0.8359375
train loss:  0.3417162299156189
train gradient:  0.17576492140371247
iteration : 11572
train acc:  0.8515625
train loss:  0.317173033952713
train gradient:  0.13250363155812597
iteration : 11573
train acc:  0.8828125
train loss:  0.29287251830101013
train gradient:  0.09854635904697706
iteration : 11574
train acc:  0.875
train loss:  0.2723751664161682
train gradient:  0.0972830817512058
iteration : 11575
train acc:  0.828125
train loss:  0.37008509039878845
train gradient:  0.21940385169101764
iteration : 11576
train acc:  0.828125
train loss:  0.34002622961997986
train gradient:  0.14983234650566563
iteration : 11577
train acc:  0.8671875
train loss:  0.30786144733428955
train gradient:  0.13884061853417737
iteration : 11578
train acc:  0.8671875
train loss:  0.2817174792289734
train gradient:  0.10197821616256253
iteration : 11579
train acc:  0.9140625
train loss:  0.32876262068748474
train gradient:  0.17970562142912255
iteration : 11580
train acc:  0.84375
train loss:  0.368373841047287
train gradient:  0.179157024356585
iteration : 11581
train acc:  0.8515625
train loss:  0.32527610659599304
train gradient:  0.13587576488253064
iteration : 11582
train acc:  0.875
train loss:  0.3289842903614044
train gradient:  0.10336265928748802
iteration : 11583
train acc:  0.8203125
train loss:  0.426626592874527
train gradient:  0.21706947131892348
iteration : 11584
train acc:  0.8671875
train loss:  0.30701708793640137
train gradient:  0.13146363672649922
iteration : 11585
train acc:  0.890625
train loss:  0.2972790598869324
train gradient:  0.10889695699963323
iteration : 11586
train acc:  0.8671875
train loss:  0.291334331035614
train gradient:  0.1453376543140102
iteration : 11587
train acc:  0.8515625
train loss:  0.33056098222732544
train gradient:  0.14343565954793908
iteration : 11588
train acc:  0.859375
train loss:  0.3063695430755615
train gradient:  0.19433555689008494
iteration : 11589
train acc:  0.8515625
train loss:  0.3332635164260864
train gradient:  0.1397584070460854
iteration : 11590
train acc:  0.9140625
train loss:  0.2518662214279175
train gradient:  0.09105543115642546
iteration : 11591
train acc:  0.828125
train loss:  0.39875322580337524
train gradient:  0.22158237439035444
iteration : 11592
train acc:  0.875
train loss:  0.2980159521102905
train gradient:  0.13535834174491262
iteration : 11593
train acc:  0.8671875
train loss:  0.35170406103134155
train gradient:  0.16574840569417135
iteration : 11594
train acc:  0.8359375
train loss:  0.39480090141296387
train gradient:  0.19680526288069713
iteration : 11595
train acc:  0.828125
train loss:  0.3781738877296448
train gradient:  0.2188393908658965
iteration : 11596
train acc:  0.8515625
train loss:  0.3613167405128479
train gradient:  0.20883430698335448
iteration : 11597
train acc:  0.8515625
train loss:  0.39382410049438477
train gradient:  0.19362853906543387
iteration : 11598
train acc:  0.875
train loss:  0.3038782775402069
train gradient:  0.09445135317492995
iteration : 11599
train acc:  0.90625
train loss:  0.23185822367668152
train gradient:  0.14979857780711955
iteration : 11600
train acc:  0.875
train loss:  0.28536003828048706
train gradient:  0.11695693437364112
iteration : 11601
train acc:  0.875
train loss:  0.3084784746170044
train gradient:  0.13078342858904413
iteration : 11602
train acc:  0.8515625
train loss:  0.37127500772476196
train gradient:  0.1462131717802278
iteration : 11603
train acc:  0.859375
train loss:  0.2992607057094574
train gradient:  0.1072831251769715
iteration : 11604
train acc:  0.875
train loss:  0.32805758714675903
train gradient:  0.13595652679350126
iteration : 11605
train acc:  0.890625
train loss:  0.2749819755554199
train gradient:  0.10281711699336592
iteration : 11606
train acc:  0.828125
train loss:  0.36885449290275574
train gradient:  0.17440595635962852
iteration : 11607
train acc:  0.8828125
train loss:  0.3010448217391968
train gradient:  0.14763279361303927
iteration : 11608
train acc:  0.8359375
train loss:  0.3387072682380676
train gradient:  0.13603892544566631
iteration : 11609
train acc:  0.8828125
train loss:  0.2860640287399292
train gradient:  0.08125698930377895
iteration : 11610
train acc:  0.8828125
train loss:  0.2964940071105957
train gradient:  0.13523299140712963
iteration : 11611
train acc:  0.9140625
train loss:  0.2323007434606552
train gradient:  0.09767318355607207
iteration : 11612
train acc:  0.8828125
train loss:  0.304293155670166
train gradient:  0.10075415568501796
iteration : 11613
train acc:  0.890625
train loss:  0.3395419418811798
train gradient:  0.17218440244628183
iteration : 11614
train acc:  0.8515625
train loss:  0.3258388042449951
train gradient:  0.19164276793072113
iteration : 11615
train acc:  0.8203125
train loss:  0.3772268295288086
train gradient:  0.22173108190220703
iteration : 11616
train acc:  0.875
train loss:  0.28607481718063354
train gradient:  0.13239334935043717
iteration : 11617
train acc:  0.859375
train loss:  0.3619500696659088
train gradient:  0.1843126235601206
iteration : 11618
train acc:  0.8125
train loss:  0.3956587314605713
train gradient:  0.21924056025297023
iteration : 11619
train acc:  0.8984375
train loss:  0.2562212347984314
train gradient:  0.09664300669918148
iteration : 11620
train acc:  0.875
train loss:  0.3641352653503418
train gradient:  0.14920865315408469
iteration : 11621
train acc:  0.8359375
train loss:  0.3561434745788574
train gradient:  0.14833777673352033
iteration : 11622
train acc:  0.828125
train loss:  0.3690585196018219
train gradient:  0.17205661441237513
iteration : 11623
train acc:  0.8125
train loss:  0.33310115337371826
train gradient:  0.1233482382844243
iteration : 11624
train acc:  0.8828125
train loss:  0.2982785701751709
train gradient:  0.0973137907480618
iteration : 11625
train acc:  0.8203125
train loss:  0.34202420711517334
train gradient:  0.11754832863322352
iteration : 11626
train acc:  0.9140625
train loss:  0.2573468089103699
train gradient:  0.09269172422784976
iteration : 11627
train acc:  0.8671875
train loss:  0.3191039562225342
train gradient:  0.16697625966179858
iteration : 11628
train acc:  0.875
train loss:  0.2897303104400635
train gradient:  0.10907629971171529
iteration : 11629
train acc:  0.8984375
train loss:  0.22727885842323303
train gradient:  0.07615641868220428
iteration : 11630
train acc:  0.8671875
train loss:  0.3112075924873352
train gradient:  0.10664018070401941
iteration : 11631
train acc:  0.8515625
train loss:  0.40190571546554565
train gradient:  0.20386588984457693
iteration : 11632
train acc:  0.828125
train loss:  0.3383573889732361
train gradient:  0.13450577925434967
iteration : 11633
train acc:  0.890625
train loss:  0.26027482748031616
train gradient:  0.07542321790724624
iteration : 11634
train acc:  0.90625
train loss:  0.28253960609436035
train gradient:  0.12122082373656895
iteration : 11635
train acc:  0.859375
train loss:  0.3306008577346802
train gradient:  0.14770574788214152
iteration : 11636
train acc:  0.8203125
train loss:  0.4094476103782654
train gradient:  0.1772228794649591
iteration : 11637
train acc:  0.8359375
train loss:  0.4402976632118225
train gradient:  0.21929747728274082
iteration : 11638
train acc:  0.84375
train loss:  0.3393538296222687
train gradient:  0.1897449473625245
iteration : 11639
train acc:  0.8671875
train loss:  0.36672449111938477
train gradient:  0.18347419391861056
iteration : 11640
train acc:  0.859375
train loss:  0.27656251192092896
train gradient:  0.08965597498430328
iteration : 11641
train acc:  0.890625
train loss:  0.2903401553630829
train gradient:  0.11240634151650666
iteration : 11642
train acc:  0.859375
train loss:  0.3105485737323761
train gradient:  0.14664225777757378
iteration : 11643
train acc:  0.828125
train loss:  0.365200936794281
train gradient:  0.17052308592040832
iteration : 11644
train acc:  0.890625
train loss:  0.31545963883399963
train gradient:  0.19055156610128188
iteration : 11645
train acc:  0.90625
train loss:  0.2623555660247803
train gradient:  0.11460695454372945
iteration : 11646
train acc:  0.8046875
train loss:  0.42403823137283325
train gradient:  0.23612412553128875
iteration : 11647
train acc:  0.8125
train loss:  0.36409592628479004
train gradient:  0.1470561534750015
iteration : 11648
train acc:  0.890625
train loss:  0.3005485534667969
train gradient:  0.16396014798895553
iteration : 11649
train acc:  0.84375
train loss:  0.3023808002471924
train gradient:  0.15542617713146928
iteration : 11650
train acc:  0.9140625
train loss:  0.24624906480312347
train gradient:  0.11998039153867854
iteration : 11651
train acc:  0.8828125
train loss:  0.28461527824401855
train gradient:  0.10827912975536773
iteration : 11652
train acc:  0.8984375
train loss:  0.26436182856559753
train gradient:  0.09865411210389052
iteration : 11653
train acc:  0.859375
train loss:  0.35540568828582764
train gradient:  0.22595300102851457
iteration : 11654
train acc:  0.8359375
train loss:  0.334399938583374
train gradient:  0.13169522356403907
iteration : 11655
train acc:  0.8828125
train loss:  0.2742609977722168
train gradient:  0.10594604907424406
iteration : 11656
train acc:  0.828125
train loss:  0.3218599855899811
train gradient:  0.16361436784440228
iteration : 11657
train acc:  0.828125
train loss:  0.42127084732055664
train gradient:  0.18325256884298036
iteration : 11658
train acc:  0.8984375
train loss:  0.2477205991744995
train gradient:  0.1035761255465237
iteration : 11659
train acc:  0.84375
train loss:  0.3275478482246399
train gradient:  0.175787486790933
iteration : 11660
train acc:  0.8984375
train loss:  0.2937401533126831
train gradient:  0.18439495211904794
iteration : 11661
train acc:  0.8828125
train loss:  0.30154967308044434
train gradient:  0.1697161145077885
iteration : 11662
train acc:  0.921875
train loss:  0.26447242498397827
train gradient:  0.14278467057928662
iteration : 11663
train acc:  0.8125
train loss:  0.4058283567428589
train gradient:  0.19186118695415286
iteration : 11664
train acc:  0.8203125
train loss:  0.3796656131744385
train gradient:  0.19311090520886098
iteration : 11665
train acc:  0.90625
train loss:  0.24855929613113403
train gradient:  0.13741152381057312
iteration : 11666
train acc:  0.875
train loss:  0.41064581274986267
train gradient:  0.18568377921394935
iteration : 11667
train acc:  0.8359375
train loss:  0.33558034896850586
train gradient:  0.156658764018664
iteration : 11668
train acc:  0.8671875
train loss:  0.3063451945781708
train gradient:  0.13924075585500387
iteration : 11669
train acc:  0.8203125
train loss:  0.43431368470191956
train gradient:  0.24014482055103678
iteration : 11670
train acc:  0.8046875
train loss:  0.3403928279876709
train gradient:  0.17898593288409714
iteration : 11671
train acc:  0.8671875
train loss:  0.2732585072517395
train gradient:  0.13975722274199864
iteration : 11672
train acc:  0.8984375
train loss:  0.30336782336235046
train gradient:  0.12059101026542289
iteration : 11673
train acc:  0.84375
train loss:  0.3213236331939697
train gradient:  0.1799870842503712
iteration : 11674
train acc:  0.8515625
train loss:  0.3396148979663849
train gradient:  0.2209348105762901
iteration : 11675
train acc:  0.8359375
train loss:  0.3381282389163971
train gradient:  0.1311192572615506
iteration : 11676
train acc:  0.8359375
train loss:  0.37559419870376587
train gradient:  0.16387453294943155
iteration : 11677
train acc:  0.890625
train loss:  0.2770969271659851
train gradient:  0.10042575990793871
iteration : 11678
train acc:  0.859375
train loss:  0.30710309743881226
train gradient:  0.1641544721144772
iteration : 11679
train acc:  0.8671875
train loss:  0.348971426486969
train gradient:  0.19659358166595298
iteration : 11680
train acc:  0.8125
train loss:  0.37452441453933716
train gradient:  0.20267618089508876
iteration : 11681
train acc:  0.84375
train loss:  0.4059712588787079
train gradient:  0.23466944642770365
iteration : 11682
train acc:  0.890625
train loss:  0.28040528297424316
train gradient:  0.08804347250185073
iteration : 11683
train acc:  0.8203125
train loss:  0.436229407787323
train gradient:  0.3475144584642997
iteration : 11684
train acc:  0.8671875
train loss:  0.25915729999542236
train gradient:  0.12117020096154704
iteration : 11685
train acc:  0.84375
train loss:  0.364099383354187
train gradient:  0.16697861511072398
iteration : 11686
train acc:  0.875
train loss:  0.28038185834884644
train gradient:  0.08184107611211737
iteration : 11687
train acc:  0.8671875
train loss:  0.3476470112800598
train gradient:  0.14167819159811454
iteration : 11688
train acc:  0.859375
train loss:  0.31235402822494507
train gradient:  0.14784964977390935
iteration : 11689
train acc:  0.875
train loss:  0.2729939818382263
train gradient:  0.09972945977544433
iteration : 11690
train acc:  0.8828125
train loss:  0.29669588804244995
train gradient:  0.1418458586996087
iteration : 11691
train acc:  0.8984375
train loss:  0.2990073561668396
train gradient:  0.14029423454867063
iteration : 11692
train acc:  0.8515625
train loss:  0.2686428427696228
train gradient:  0.10273449282208129
iteration : 11693
train acc:  0.8359375
train loss:  0.3783629238605499
train gradient:  0.26662597427076995
iteration : 11694
train acc:  0.859375
train loss:  0.27059462666511536
train gradient:  0.1226983432432032
iteration : 11695
train acc:  0.8671875
train loss:  0.31196218729019165
train gradient:  0.12222655764217254
iteration : 11696
train acc:  0.7890625
train loss:  0.41688597202301025
train gradient:  0.2819942392022706
iteration : 11697
train acc:  0.859375
train loss:  0.2659141719341278
train gradient:  0.12128487231040389
iteration : 11698
train acc:  0.8515625
train loss:  0.29751646518707275
train gradient:  0.16632577911526755
iteration : 11699
train acc:  0.90625
train loss:  0.2547655701637268
train gradient:  0.16476007759305655
iteration : 11700
train acc:  0.8125
train loss:  0.3908103108406067
train gradient:  0.1775229931553912
iteration : 11701
train acc:  0.8515625
train loss:  0.35458651185035706
train gradient:  0.15979008793337937
iteration : 11702
train acc:  0.859375
train loss:  0.3194379210472107
train gradient:  0.14616999380935886
iteration : 11703
train acc:  0.890625
train loss:  0.24451662600040436
train gradient:  0.0912956900272655
iteration : 11704
train acc:  0.7890625
train loss:  0.4430360198020935
train gradient:  0.18047098272474074
iteration : 11705
train acc:  0.9140625
train loss:  0.35149911046028137
train gradient:  0.16939498271108572
iteration : 11706
train acc:  0.8203125
train loss:  0.33118748664855957
train gradient:  0.17359410996708313
iteration : 11707
train acc:  0.8984375
train loss:  0.25505301356315613
train gradient:  0.10892169410611863
iteration : 11708
train acc:  0.9453125
train loss:  0.22648034989833832
train gradient:  0.10372137670667102
iteration : 11709
train acc:  0.8515625
train loss:  0.3237566649913788
train gradient:  0.131352395932015
iteration : 11710
train acc:  0.8125
train loss:  0.3813460171222687
train gradient:  0.20119124767379826
iteration : 11711
train acc:  0.890625
train loss:  0.36674463748931885
train gradient:  0.23143470526702023
iteration : 11712
train acc:  0.8359375
train loss:  0.394050657749176
train gradient:  0.18380307949647692
iteration : 11713
train acc:  0.8828125
train loss:  0.3117592930793762
train gradient:  0.164856389034298
iteration : 11714
train acc:  0.890625
train loss:  0.3085760474205017
train gradient:  0.13582825978740182
iteration : 11715
train acc:  0.859375
train loss:  0.31035611033439636
train gradient:  0.09137757388125782
iteration : 11716
train acc:  0.859375
train loss:  0.30435675382614136
train gradient:  0.10812368026794461
iteration : 11717
train acc:  0.859375
train loss:  0.29425862431526184
train gradient:  0.11789667654443058
iteration : 11718
train acc:  0.8515625
train loss:  0.33042436838150024
train gradient:  0.10820371884114637
iteration : 11719
train acc:  0.84375
train loss:  0.36208099126815796
train gradient:  0.22504734888183137
iteration : 11720
train acc:  0.828125
train loss:  0.356773316860199
train gradient:  0.20630127838807427
iteration : 11721
train acc:  0.8359375
train loss:  0.3438752293586731
train gradient:  0.1009327523202265
iteration : 11722
train acc:  0.890625
train loss:  0.30408769845962524
train gradient:  0.1305568440480208
iteration : 11723
train acc:  0.8515625
train loss:  0.3016795814037323
train gradient:  0.11770547051711823
iteration : 11724
train acc:  0.828125
train loss:  0.38301384449005127
train gradient:  0.18732762373729847
iteration : 11725
train acc:  0.8203125
train loss:  0.333312451839447
train gradient:  0.14181894315069488
iteration : 11726
train acc:  0.8359375
train loss:  0.3310260474681854
train gradient:  0.158163107301404
iteration : 11727
train acc:  0.8984375
train loss:  0.25016793608665466
train gradient:  0.14909949664224192
iteration : 11728
train acc:  0.8125
train loss:  0.32102030515670776
train gradient:  0.1701319548514924
iteration : 11729
train acc:  0.875
train loss:  0.26291561126708984
train gradient:  0.11033087851136417
iteration : 11730
train acc:  0.8359375
train loss:  0.3038966655731201
train gradient:  0.11581441556342299
iteration : 11731
train acc:  0.8515625
train loss:  0.36950647830963135
train gradient:  0.2423192643453674
iteration : 11732
train acc:  0.8203125
train loss:  0.4385339319705963
train gradient:  0.2135958897772388
iteration : 11733
train acc:  0.84375
train loss:  0.35361289978027344
train gradient:  0.15041284260481277
iteration : 11734
train acc:  0.8515625
train loss:  0.3066748380661011
train gradient:  0.14608972893726563
iteration : 11735
train acc:  0.8515625
train loss:  0.3583128750324249
train gradient:  0.23106251733947225
iteration : 11736
train acc:  0.890625
train loss:  0.31552064418792725
train gradient:  0.15445886988349866
iteration : 11737
train acc:  0.84375
train loss:  0.3890383243560791
train gradient:  0.15489912786490767
iteration : 11738
train acc:  0.90625
train loss:  0.22673922777175903
train gradient:  0.10128125548460926
iteration : 11739
train acc:  0.84375
train loss:  0.3766629695892334
train gradient:  0.2305159071323386
iteration : 11740
train acc:  0.84375
train loss:  0.316405713558197
train gradient:  0.17440307281342815
iteration : 11741
train acc:  0.875
train loss:  0.32729142904281616
train gradient:  0.07979199644883019
iteration : 11742
train acc:  0.875
train loss:  0.30396896600723267
train gradient:  0.1480583312107646
iteration : 11743
train acc:  0.8671875
train loss:  0.36223316192626953
train gradient:  0.27369675458753895
iteration : 11744
train acc:  0.8828125
train loss:  0.2752758860588074
train gradient:  0.1403259113323031
iteration : 11745
train acc:  0.8828125
train loss:  0.28795239329338074
train gradient:  0.117401502064929
iteration : 11746
train acc:  0.8515625
train loss:  0.3872014284133911
train gradient:  0.24260656345166126
iteration : 11747
train acc:  0.875
train loss:  0.3236781656742096
train gradient:  0.11325834196188483
iteration : 11748
train acc:  0.8671875
train loss:  0.28666478395462036
train gradient:  0.10974129873730676
iteration : 11749
train acc:  0.8515625
train loss:  0.29648250341415405
train gradient:  0.10247244469835701
iteration : 11750
train acc:  0.8828125
train loss:  0.26671528816223145
train gradient:  0.1047711400578303
iteration : 11751
train acc:  0.828125
train loss:  0.3665890693664551
train gradient:  0.133764143897666
iteration : 11752
train acc:  0.84375
train loss:  0.30541062355041504
train gradient:  0.19474440027669904
iteration : 11753
train acc:  0.828125
train loss:  0.40321671962738037
train gradient:  0.23294949347979113
iteration : 11754
train acc:  0.8515625
train loss:  0.30323082208633423
train gradient:  0.13207444692810588
iteration : 11755
train acc:  0.8671875
train loss:  0.3207560181617737
train gradient:  0.14048261445006105
iteration : 11756
train acc:  0.8984375
train loss:  0.2816981077194214
train gradient:  0.10573719331896707
iteration : 11757
train acc:  0.875
train loss:  0.3254466652870178
train gradient:  0.1449621261862752
iteration : 11758
train acc:  0.8984375
train loss:  0.30300721526145935
train gradient:  0.12075689792943892
iteration : 11759
train acc:  0.8984375
train loss:  0.2894842028617859
train gradient:  0.10298645696982252
iteration : 11760
train acc:  0.8515625
train loss:  0.33819520473480225
train gradient:  0.1336406192015936
iteration : 11761
train acc:  0.8359375
train loss:  0.34704065322875977
train gradient:  0.17959925144353261
iteration : 11762
train acc:  0.8671875
train loss:  0.27402985095977783
train gradient:  0.37295041050686034
iteration : 11763
train acc:  0.8515625
train loss:  0.3126794695854187
train gradient:  0.12037805640129134
iteration : 11764
train acc:  0.828125
train loss:  0.3628288805484772
train gradient:  0.17974624411790488
iteration : 11765
train acc:  0.8515625
train loss:  0.31316182017326355
train gradient:  0.1692820646219874
iteration : 11766
train acc:  0.859375
train loss:  0.35841402411460876
train gradient:  0.15417695819749982
iteration : 11767
train acc:  0.84375
train loss:  0.4049187898635864
train gradient:  0.20383334397657338
iteration : 11768
train acc:  0.8125
train loss:  0.4024878740310669
train gradient:  0.18338816359705623
iteration : 11769
train acc:  0.8515625
train loss:  0.38931113481521606
train gradient:  0.1762272884356326
iteration : 11770
train acc:  0.84375
train loss:  0.29386284947395325
train gradient:  0.15400860542715228
iteration : 11771
train acc:  0.859375
train loss:  0.31494849920272827
train gradient:  0.1539117703540839
iteration : 11772
train acc:  0.8828125
train loss:  0.2635664939880371
train gradient:  0.09522720099400205
iteration : 11773
train acc:  0.828125
train loss:  0.37440401315689087
train gradient:  0.15936535530619841
iteration : 11774
train acc:  0.8046875
train loss:  0.46433037519454956
train gradient:  0.3564417957007991
iteration : 11775
train acc:  0.8828125
train loss:  0.27378812432289124
train gradient:  0.11820052749863798
iteration : 11776
train acc:  0.84375
train loss:  0.3316121995449066
train gradient:  0.143254282104274
iteration : 11777
train acc:  0.8359375
train loss:  0.3269916772842407
train gradient:  0.14636097568659404
iteration : 11778
train acc:  0.875
train loss:  0.3851226270198822
train gradient:  0.1601318704714777
iteration : 11779
train acc:  0.84375
train loss:  0.38066402077674866
train gradient:  0.1844647099300789
iteration : 11780
train acc:  0.8359375
train loss:  0.304640531539917
train gradient:  0.138643578525781
iteration : 11781
train acc:  0.859375
train loss:  0.34805190563201904
train gradient:  0.1440753744244037
iteration : 11782
train acc:  0.8203125
train loss:  0.3412272036075592
train gradient:  0.20814592359795808
iteration : 11783
train acc:  0.90625
train loss:  0.2514111399650574
train gradient:  0.1520500429827356
iteration : 11784
train acc:  0.8515625
train loss:  0.3344966769218445
train gradient:  0.14145557891441718
iteration : 11785
train acc:  0.875
train loss:  0.3492925763130188
train gradient:  0.15161559291614463
iteration : 11786
train acc:  0.8359375
train loss:  0.37216416001319885
train gradient:  0.18539733652843232
iteration : 11787
train acc:  0.828125
train loss:  0.37635037302970886
train gradient:  0.16364656873093392
iteration : 11788
train acc:  0.8671875
train loss:  0.33630090951919556
train gradient:  0.1633647137292351
iteration : 11789
train acc:  0.8515625
train loss:  0.35330334305763245
train gradient:  0.20619750484975202
iteration : 11790
train acc:  0.875
train loss:  0.2850135266780853
train gradient:  0.11021080539153125
iteration : 11791
train acc:  0.8828125
train loss:  0.3006347417831421
train gradient:  0.1194269292293702
iteration : 11792
train acc:  0.859375
train loss:  0.317716121673584
train gradient:  0.21897702232547378
iteration : 11793
train acc:  0.8125
train loss:  0.3900492489337921
train gradient:  0.17859324677914595
iteration : 11794
train acc:  0.8828125
train loss:  0.29609429836273193
train gradient:  0.1492404853543006
iteration : 11795
train acc:  0.875
train loss:  0.31124448776245117
train gradient:  0.1326951347912224
iteration : 11796
train acc:  0.8984375
train loss:  0.3214395046234131
train gradient:  0.15418531505061567
iteration : 11797
train acc:  0.9140625
train loss:  0.23544521629810333
train gradient:  0.08738157830398573
iteration : 11798
train acc:  0.8203125
train loss:  0.3735331892967224
train gradient:  0.17135664556727376
iteration : 11799
train acc:  0.8125
train loss:  0.36060142517089844
train gradient:  0.14985342670487262
iteration : 11800
train acc:  0.8671875
train loss:  0.3511314392089844
train gradient:  0.12742887941088957
iteration : 11801
train acc:  0.875
train loss:  0.26725515723228455
train gradient:  0.10373837605666161
iteration : 11802
train acc:  0.859375
train loss:  0.3404708802700043
train gradient:  0.11162885034683252
iteration : 11803
train acc:  0.8515625
train loss:  0.31395721435546875
train gradient:  0.1480847800719296
iteration : 11804
train acc:  0.8203125
train loss:  0.32868731021881104
train gradient:  0.12569994017659858
iteration : 11805
train acc:  0.890625
train loss:  0.2925659418106079
train gradient:  0.13619286418578735
iteration : 11806
train acc:  0.8046875
train loss:  0.36609145998954773
train gradient:  0.15237583372806354
iteration : 11807
train acc:  0.8671875
train loss:  0.320928156375885
train gradient:  0.10857542560889305
iteration : 11808
train acc:  0.8671875
train loss:  0.297902375459671
train gradient:  0.1669552372124197
iteration : 11809
train acc:  0.859375
train loss:  0.3238058388233185
train gradient:  0.13944190559334085
iteration : 11810
train acc:  0.8671875
train loss:  0.3823990225791931
train gradient:  0.12114032104796629
iteration : 11811
train acc:  0.8828125
train loss:  0.3398304581642151
train gradient:  0.14093376915859107
iteration : 11812
train acc:  0.8359375
train loss:  0.3519936203956604
train gradient:  0.21661367869216167
iteration : 11813
train acc:  0.875
train loss:  0.26396095752716064
train gradient:  0.1078188896426515
iteration : 11814
train acc:  0.8828125
train loss:  0.3435259163379669
train gradient:  0.3466178301465328
iteration : 11815
train acc:  0.8125
train loss:  0.34804627299308777
train gradient:  0.21585273781490805
iteration : 11816
train acc:  0.859375
train loss:  0.34588152170181274
train gradient:  0.11536784912713975
iteration : 11817
train acc:  0.859375
train loss:  0.29417645931243896
train gradient:  0.08220511206043608
iteration : 11818
train acc:  0.8828125
train loss:  0.2779346704483032
train gradient:  0.09626307378423618
iteration : 11819
train acc:  0.8359375
train loss:  0.37836599349975586
train gradient:  0.1576670929841784
iteration : 11820
train acc:  0.8125
train loss:  0.3825887441635132
train gradient:  0.15231668900266956
iteration : 11821
train acc:  0.8203125
train loss:  0.3459458351135254
train gradient:  0.1289601882429307
iteration : 11822
train acc:  0.7890625
train loss:  0.4731018841266632
train gradient:  0.38715967239753113
iteration : 11823
train acc:  0.8046875
train loss:  0.3817234933376312
train gradient:  0.18062168996578176
iteration : 11824
train acc:  0.8515625
train loss:  0.40337061882019043
train gradient:  0.23935973433245292
iteration : 11825
train acc:  0.8671875
train loss:  0.283550888299942
train gradient:  0.08905520641773032
iteration : 11826
train acc:  0.8203125
train loss:  0.3416348695755005
train gradient:  0.17061231638975893
iteration : 11827
train acc:  0.828125
train loss:  0.352080762386322
train gradient:  0.1407419889862024
iteration : 11828
train acc:  0.890625
train loss:  0.27922242879867554
train gradient:  0.13068679332244235
iteration : 11829
train acc:  0.8515625
train loss:  0.29327648878097534
train gradient:  0.0995910370124509
iteration : 11830
train acc:  0.890625
train loss:  0.2780136466026306
train gradient:  0.12742634825518168
iteration : 11831
train acc:  0.8359375
train loss:  0.3183742165565491
train gradient:  0.12191167839743078
iteration : 11832
train acc:  0.890625
train loss:  0.29618555307388306
train gradient:  0.1487480587880173
iteration : 11833
train acc:  0.8359375
train loss:  0.3370405435562134
train gradient:  0.14046164532424216
iteration : 11834
train acc:  0.84375
train loss:  0.375364750623703
train gradient:  0.14856583788764255
iteration : 11835
train acc:  0.8515625
train loss:  0.34061866998672485
train gradient:  0.14610386690519195
iteration : 11836
train acc:  0.8671875
train loss:  0.3226608335971832
train gradient:  0.09881444250283633
iteration : 11837
train acc:  0.8984375
train loss:  0.25514155626296997
train gradient:  0.07751253759298593
iteration : 11838
train acc:  0.859375
train loss:  0.34635522961616516
train gradient:  0.14825514544743842
iteration : 11839
train acc:  0.828125
train loss:  0.35831961035728455
train gradient:  0.213020905849258
iteration : 11840
train acc:  0.84375
train loss:  0.3229016065597534
train gradient:  0.11213108935911083
iteration : 11841
train acc:  0.8359375
train loss:  0.3256164789199829
train gradient:  0.15127075772168047
iteration : 11842
train acc:  0.8125
train loss:  0.3666641116142273
train gradient:  0.1695636434555201
iteration : 11843
train acc:  0.875
train loss:  0.28583601117134094
train gradient:  0.1003445224951451
iteration : 11844
train acc:  0.875
train loss:  0.2928586006164551
train gradient:  0.11328765385730093
iteration : 11845
train acc:  0.8515625
train loss:  0.3134770393371582
train gradient:  0.09939823348826683
iteration : 11846
train acc:  0.8671875
train loss:  0.2947908639907837
train gradient:  0.14110917788822874
iteration : 11847
train acc:  0.8359375
train loss:  0.29262182116508484
train gradient:  0.11314716313842556
iteration : 11848
train acc:  0.8515625
train loss:  0.35122066736221313
train gradient:  0.1730596273423396
iteration : 11849
train acc:  0.875
train loss:  0.3008560836315155
train gradient:  0.11841949515250523
iteration : 11850
train acc:  0.875
train loss:  0.3067828416824341
train gradient:  0.1161757085054662
iteration : 11851
train acc:  0.8671875
train loss:  0.2803589105606079
train gradient:  0.10043010664055396
iteration : 11852
train acc:  0.9140625
train loss:  0.26381754875183105
train gradient:  0.09220090595904173
iteration : 11853
train acc:  0.8828125
train loss:  0.3495265245437622
train gradient:  0.17800536185996776
iteration : 11854
train acc:  0.8359375
train loss:  0.4398038387298584
train gradient:  0.26533943558131745
iteration : 11855
train acc:  0.859375
train loss:  0.3528618812561035
train gradient:  0.19615075043915015
iteration : 11856
train acc:  0.8359375
train loss:  0.3455849885940552
train gradient:  0.16689761472024595
iteration : 11857
train acc:  0.8671875
train loss:  0.2744828462600708
train gradient:  0.10945835584020511
iteration : 11858
train acc:  0.84375
train loss:  0.3239888548851013
train gradient:  0.1512430978398163
iteration : 11859
train acc:  0.8515625
train loss:  0.3344619870185852
train gradient:  0.15140080676907774
iteration : 11860
train acc:  0.875
train loss:  0.2554512321949005
train gradient:  0.07561633209329607
iteration : 11861
train acc:  0.828125
train loss:  0.37766844034194946
train gradient:  0.2221061286978973
iteration : 11862
train acc:  0.859375
train loss:  0.35701602697372437
train gradient:  0.14639662415945556
iteration : 11863
train acc:  0.8125
train loss:  0.37110623717308044
train gradient:  0.1818644478418177
iteration : 11864
train acc:  0.8125
train loss:  0.36767521500587463
train gradient:  0.15922390181232984
iteration : 11865
train acc:  0.875
train loss:  0.30747562646865845
train gradient:  0.15325111820638823
iteration : 11866
train acc:  0.8046875
train loss:  0.4455466568470001
train gradient:  0.17467615248688248
iteration : 11867
train acc:  0.921875
train loss:  0.24315640330314636
train gradient:  0.1052941528921669
iteration : 11868
train acc:  0.84375
train loss:  0.3018743395805359
train gradient:  0.13156493645334377
iteration : 11869
train acc:  0.859375
train loss:  0.32281309366226196
train gradient:  0.1846667548314897
iteration : 11870
train acc:  0.8671875
train loss:  0.2972208261489868
train gradient:  0.10480449939687861
iteration : 11871
train acc:  0.875
train loss:  0.2928784191608429
train gradient:  0.09177078777661302
iteration : 11872
train acc:  0.8671875
train loss:  0.30975019931793213
train gradient:  0.1410215094321906
iteration : 11873
train acc:  0.8828125
train loss:  0.27193373441696167
train gradient:  0.08928337851302456
iteration : 11874
train acc:  0.8984375
train loss:  0.2518003582954407
train gradient:  0.111665405012963
iteration : 11875
train acc:  0.8125
train loss:  0.3815794885158539
train gradient:  0.1748473309811108
iteration : 11876
train acc:  0.90625
train loss:  0.22945280373096466
train gradient:  0.08096992401596051
iteration : 11877
train acc:  0.8359375
train loss:  0.3757317066192627
train gradient:  0.2114150151903838
iteration : 11878
train acc:  0.84375
train loss:  0.3513515591621399
train gradient:  0.1793407930076079
iteration : 11879
train acc:  0.875
train loss:  0.32567471265792847
train gradient:  0.13386156259689735
iteration : 11880
train acc:  0.875
train loss:  0.3105415105819702
train gradient:  0.15508443751644962
iteration : 11881
train acc:  0.8671875
train loss:  0.3323706388473511
train gradient:  0.16177538048621073
iteration : 11882
train acc:  0.9140625
train loss:  0.258504718542099
train gradient:  0.15063797691537173
iteration : 11883
train acc:  0.8671875
train loss:  0.3215070962905884
train gradient:  0.13104112229244397
iteration : 11884
train acc:  0.859375
train loss:  0.3348163962364197
train gradient:  0.23902494979355643
iteration : 11885
train acc:  0.8515625
train loss:  0.31002968549728394
train gradient:  0.14607890728436673
iteration : 11886
train acc:  0.8984375
train loss:  0.3164326548576355
train gradient:  0.1207798254713928
iteration : 11887
train acc:  0.8125
train loss:  0.34675711393356323
train gradient:  0.1804289918211207
iteration : 11888
train acc:  0.828125
train loss:  0.36964306235313416
train gradient:  0.13730620227790885
iteration : 11889
train acc:  0.84375
train loss:  0.33939796686172485
train gradient:  0.16267222079187857
iteration : 11890
train acc:  0.859375
train loss:  0.2835127115249634
train gradient:  0.12982760004344765
iteration : 11891
train acc:  0.8359375
train loss:  0.3249797224998474
train gradient:  0.15158794614296123
iteration : 11892
train acc:  0.859375
train loss:  0.3621707856655121
train gradient:  0.2245085340786261
iteration : 11893
train acc:  0.890625
train loss:  0.3002697229385376
train gradient:  0.1468647466585176
iteration : 11894
train acc:  0.859375
train loss:  0.2904413640499115
train gradient:  0.16245866104471018
iteration : 11895
train acc:  0.859375
train loss:  0.36801913380622864
train gradient:  0.17260960665596206
iteration : 11896
train acc:  0.8125
train loss:  0.4887791574001312
train gradient:  0.36724829021857397
iteration : 11897
train acc:  0.859375
train loss:  0.3387792110443115
train gradient:  0.17430469433751494
iteration : 11898
train acc:  0.859375
train loss:  0.2835998237133026
train gradient:  0.13112504469181532
iteration : 11899
train acc:  0.7421875
train loss:  0.43182995915412903
train gradient:  0.21856826324971929
iteration : 11900
train acc:  0.859375
train loss:  0.3381006121635437
train gradient:  0.25281420055394416
iteration : 11901
train acc:  0.8203125
train loss:  0.38349097967147827
train gradient:  0.25530992822969606
iteration : 11902
train acc:  0.859375
train loss:  0.30496788024902344
train gradient:  0.2033654691836101
iteration : 11903
train acc:  0.8984375
train loss:  0.27151191234588623
train gradient:  0.10884541021025244
iteration : 11904
train acc:  0.8515625
train loss:  0.32663679122924805
train gradient:  0.22253797263903108
iteration : 11905
train acc:  0.8359375
train loss:  0.3587439954280853
train gradient:  0.21302932869012012
iteration : 11906
train acc:  0.8515625
train loss:  0.3156491219997406
train gradient:  0.12450982577735677
iteration : 11907
train acc:  0.8515625
train loss:  0.3249422013759613
train gradient:  0.13519531410903562
iteration : 11908
train acc:  0.90625
train loss:  0.23221465945243835
train gradient:  0.10826572553829403
iteration : 11909
train acc:  0.859375
train loss:  0.35777249932289124
train gradient:  0.20374252433410023
iteration : 11910
train acc:  0.8671875
train loss:  0.2983752191066742
train gradient:  0.1297015105361486
iteration : 11911
train acc:  0.84375
train loss:  0.3938782811164856
train gradient:  0.18341337332555593
iteration : 11912
train acc:  0.8515625
train loss:  0.291469931602478
train gradient:  0.11520388830057576
iteration : 11913
train acc:  0.8203125
train loss:  0.3072740435600281
train gradient:  0.12465367814079709
iteration : 11914
train acc:  0.859375
train loss:  0.32944172620773315
train gradient:  0.1640824563029833
iteration : 11915
train acc:  0.8984375
train loss:  0.2554309368133545
train gradient:  0.12654462831047736
iteration : 11916
train acc:  0.859375
train loss:  0.2783438563346863
train gradient:  0.12178482351620662
iteration : 11917
train acc:  0.8046875
train loss:  0.4016214907169342
train gradient:  0.22176485894504422
iteration : 11918
train acc:  0.859375
train loss:  0.3794962167739868
train gradient:  0.1821322290085638
iteration : 11919
train acc:  0.8359375
train loss:  0.3166128993034363
train gradient:  0.1397496532807967
iteration : 11920
train acc:  0.890625
train loss:  0.290079265832901
train gradient:  0.0944245078099333
iteration : 11921
train acc:  0.859375
train loss:  0.3164542317390442
train gradient:  0.12063992154585369
iteration : 11922
train acc:  0.84375
train loss:  0.32278525829315186
train gradient:  0.16346956066420748
iteration : 11923
train acc:  0.8671875
train loss:  0.34064429998397827
train gradient:  0.1658265042860549
iteration : 11924
train acc:  0.8828125
train loss:  0.30547094345092773
train gradient:  0.16340026822347842
iteration : 11925
train acc:  0.828125
train loss:  0.36631473898887634
train gradient:  0.15605758894028426
iteration : 11926
train acc:  0.875
train loss:  0.30763986706733704
train gradient:  0.10826174928268903
iteration : 11927
train acc:  0.875
train loss:  0.269763320684433
train gradient:  0.11270509529345984
iteration : 11928
train acc:  0.84375
train loss:  0.3024255633354187
train gradient:  0.15957098191498906
iteration : 11929
train acc:  0.890625
train loss:  0.2865055501461029
train gradient:  0.1502800388836829
iteration : 11930
train acc:  0.8203125
train loss:  0.35244613885879517
train gradient:  0.1797929035698465
iteration : 11931
train acc:  0.8984375
train loss:  0.35498443245887756
train gradient:  0.12614184901986353
iteration : 11932
train acc:  0.875
train loss:  0.3329954445362091
train gradient:  0.14107417809723727
iteration : 11933
train acc:  0.8984375
train loss:  0.2643042206764221
train gradient:  0.1303155335106731
iteration : 11934
train acc:  0.90625
train loss:  0.2070530354976654
train gradient:  0.0693178095171204
iteration : 11935
train acc:  0.890625
train loss:  0.2663426399230957
train gradient:  0.12270535230781868
iteration : 11936
train acc:  0.859375
train loss:  0.34366533160209656
train gradient:  0.12155402842131245
iteration : 11937
train acc:  0.859375
train loss:  0.3151918053627014
train gradient:  0.1042688265120884
iteration : 11938
train acc:  0.7578125
train loss:  0.4170507788658142
train gradient:  0.17208989573424927
iteration : 11939
train acc:  0.8671875
train loss:  0.2879652976989746
train gradient:  0.13241970010263843
iteration : 11940
train acc:  0.84375
train loss:  0.3469187021255493
train gradient:  0.19458050854607503
iteration : 11941
train acc:  0.859375
train loss:  0.3055263161659241
train gradient:  0.078132291233041
iteration : 11942
train acc:  0.875
train loss:  0.3086714744567871
train gradient:  0.10797265786592039
iteration : 11943
train acc:  0.8515625
train loss:  0.30282527208328247
train gradient:  0.12847333202186642
iteration : 11944
train acc:  0.8828125
train loss:  0.3301762640476227
train gradient:  0.16455756221567683
iteration : 11945
train acc:  0.859375
train loss:  0.3167306184768677
train gradient:  0.16942264239284027
iteration : 11946
train acc:  0.828125
train loss:  0.384547621011734
train gradient:  0.13579239718879915
iteration : 11947
train acc:  0.8671875
train loss:  0.33388805389404297
train gradient:  0.1411441366607804
iteration : 11948
train acc:  0.9140625
train loss:  0.3063766658306122
train gradient:  0.14819085670506638
iteration : 11949
train acc:  0.9140625
train loss:  0.2221396416425705
train gradient:  0.07456823558108124
iteration : 11950
train acc:  0.8515625
train loss:  0.2837235927581787
train gradient:  0.14380433398666265
iteration : 11951
train acc:  0.8125
train loss:  0.3582919239997864
train gradient:  0.18430971176507535
iteration : 11952
train acc:  0.8984375
train loss:  0.24895019829273224
train gradient:  0.10471313598247896
iteration : 11953
train acc:  0.8671875
train loss:  0.2894532084465027
train gradient:  0.11168785358001702
iteration : 11954
train acc:  0.84375
train loss:  0.3131459653377533
train gradient:  0.1701403041672518
iteration : 11955
train acc:  0.8671875
train loss:  0.355265349149704
train gradient:  0.15650260592466056
iteration : 11956
train acc:  0.828125
train loss:  0.400138795375824
train gradient:  0.2122375227274576
iteration : 11957
train acc:  0.8359375
train loss:  0.3496297299861908
train gradient:  0.13799189729909514
iteration : 11958
train acc:  0.8984375
train loss:  0.27489930391311646
train gradient:  0.107832526294909
iteration : 11959
train acc:  0.90625
train loss:  0.26711463928222656
train gradient:  0.113753915578363
iteration : 11960
train acc:  0.890625
train loss:  0.31326746940612793
train gradient:  0.14971372432769225
iteration : 11961
train acc:  0.8359375
train loss:  0.3221747577190399
train gradient:  0.11702943571125259
iteration : 11962
train acc:  0.90625
train loss:  0.22569505870342255
train gradient:  0.08465549965128621
iteration : 11963
train acc:  0.828125
train loss:  0.3455810546875
train gradient:  0.1996407775364522
iteration : 11964
train acc:  0.78125
train loss:  0.376048743724823
train gradient:  0.1959484295409583
iteration : 11965
train acc:  0.8984375
train loss:  0.30673855543136597
train gradient:  0.13963982904767147
iteration : 11966
train acc:  0.8359375
train loss:  0.4027709364891052
train gradient:  0.26021965118317003
iteration : 11967
train acc:  0.84375
train loss:  0.30257448554039
train gradient:  0.11609208768799543
iteration : 11968
train acc:  0.8828125
train loss:  0.2853342294692993
train gradient:  0.0890450383010125
iteration : 11969
train acc:  0.8671875
train loss:  0.32391855120658875
train gradient:  0.12224041227467254
iteration : 11970
train acc:  0.890625
train loss:  0.2790299654006958
train gradient:  0.1473401022257873
iteration : 11971
train acc:  0.8515625
train loss:  0.3428434729576111
train gradient:  0.133974693475869
iteration : 11972
train acc:  0.8671875
train loss:  0.27502310276031494
train gradient:  0.1166624432121091
iteration : 11973
train acc:  0.8515625
train loss:  0.3528916835784912
train gradient:  0.18103116559957855
iteration : 11974
train acc:  0.8359375
train loss:  0.34147611260414124
train gradient:  0.14071510337758203
iteration : 11975
train acc:  0.8046875
train loss:  0.4234880208969116
train gradient:  0.18047407877116423
iteration : 11976
train acc:  0.890625
train loss:  0.29754915833473206
train gradient:  0.13109909090096933
iteration : 11977
train acc:  0.875
train loss:  0.28271982073783875
train gradient:  0.12309849277341879
iteration : 11978
train acc:  0.859375
train loss:  0.36139804124832153
train gradient:  0.12240967989842474
iteration : 11979
train acc:  0.8671875
train loss:  0.28472626209259033
train gradient:  0.1021893188695707
iteration : 11980
train acc:  0.8828125
train loss:  0.23354431986808777
train gradient:  0.09350429836861225
iteration : 11981
train acc:  0.8828125
train loss:  0.2676069736480713
train gradient:  0.08133915883416164
iteration : 11982
train acc:  0.8671875
train loss:  0.31388187408447266
train gradient:  0.14870249489379342
iteration : 11983
train acc:  0.8515625
train loss:  0.3728500008583069
train gradient:  0.13760922906506018
iteration : 11984
train acc:  0.8671875
train loss:  0.33107802271842957
train gradient:  0.1748001730214349
iteration : 11985
train acc:  0.8203125
train loss:  0.35611897706985474
train gradient:  0.21105232920813977
iteration : 11986
train acc:  0.8671875
train loss:  0.34071969985961914
train gradient:  0.08872577648500612
iteration : 11987
train acc:  0.8515625
train loss:  0.29673731327056885
train gradient:  0.16067916171206342
iteration : 11988
train acc:  0.875
train loss:  0.31294116377830505
train gradient:  0.14159197003451074
iteration : 11989
train acc:  0.890625
train loss:  0.23011770844459534
train gradient:  0.1154664136957373
iteration : 11990
train acc:  0.8359375
train loss:  0.35313624143600464
train gradient:  0.13929275770918806
iteration : 11991
train acc:  0.8671875
train loss:  0.31702539324760437
train gradient:  0.19696719443575322
iteration : 11992
train acc:  0.8671875
train loss:  0.30413252115249634
train gradient:  0.15324313970232156
iteration : 11993
train acc:  0.890625
train loss:  0.2727736830711365
train gradient:  0.07588806673888908
iteration : 11994
train acc:  0.859375
train loss:  0.32574981451034546
train gradient:  0.14831846617741395
iteration : 11995
train acc:  0.828125
train loss:  0.40044209361076355
train gradient:  0.18204968389212964
iteration : 11996
train acc:  0.8828125
train loss:  0.32072198390960693
train gradient:  0.1518960594795771
iteration : 11997
train acc:  0.84375
train loss:  0.3206956386566162
train gradient:  0.18041627641866648
iteration : 11998
train acc:  0.828125
train loss:  0.39186379313468933
train gradient:  0.20023538467083696
iteration : 11999
train acc:  0.859375
train loss:  0.2961272895336151
train gradient:  0.09584642525887256
iteration : 12000
train acc:  0.890625
train loss:  0.23521457612514496
train gradient:  0.07523078599599144
iteration : 12001
train acc:  0.8515625
train loss:  0.2926415801048279
train gradient:  0.12652323827241033
iteration : 12002
train acc:  0.859375
train loss:  0.322510302066803
train gradient:  0.18764240328726323
iteration : 12003
train acc:  0.875
train loss:  0.2982202172279358
train gradient:  0.11343440180918249
iteration : 12004
train acc:  0.9140625
train loss:  0.22199216485023499
train gradient:  0.12052410207886399
iteration : 12005
train acc:  0.8515625
train loss:  0.26950085163116455
train gradient:  0.12967795014227446
iteration : 12006
train acc:  0.8828125
train loss:  0.33303242921829224
train gradient:  0.1405417298710031
iteration : 12007
train acc:  0.8359375
train loss:  0.3762720227241516
train gradient:  0.19515288023532013
iteration : 12008
train acc:  0.84375
train loss:  0.3405158519744873
train gradient:  0.1373949205468471
iteration : 12009
train acc:  0.9296875
train loss:  0.2566622793674469
train gradient:  0.12822417629034238
iteration : 12010
train acc:  0.875
train loss:  0.34983450174331665
train gradient:  0.13909116375605068
iteration : 12011
train acc:  0.890625
train loss:  0.2386009842157364
train gradient:  0.11273754058236082
iteration : 12012
train acc:  0.8515625
train loss:  0.32403793931007385
train gradient:  0.22992514962276733
iteration : 12013
train acc:  0.8203125
train loss:  0.3677489161491394
train gradient:  0.19000100070592893
iteration : 12014
train acc:  0.890625
train loss:  0.24873413145542145
train gradient:  0.11131899174751829
iteration : 12015
train acc:  0.921875
train loss:  0.22760383784770966
train gradient:  0.07336131274709827
iteration : 12016
train acc:  0.8515625
train loss:  0.2905485928058624
train gradient:  0.15633113304921242
iteration : 12017
train acc:  0.8515625
train loss:  0.3884695768356323
train gradient:  0.22848695202038144
iteration : 12018
train acc:  0.8359375
train loss:  0.3685058355331421
train gradient:  0.1971376669523992
iteration : 12019
train acc:  0.8125
train loss:  0.42870864272117615
train gradient:  0.3048864104323216
iteration : 12020
train acc:  0.875
train loss:  0.2681213915348053
train gradient:  0.10388958210817101
iteration : 12021
train acc:  0.8203125
train loss:  0.3519364297389984
train gradient:  0.20769610288705898
iteration : 12022
train acc:  0.8984375
train loss:  0.2710227370262146
train gradient:  0.09501081452739314
iteration : 12023
train acc:  0.828125
train loss:  0.42602184414863586
train gradient:  0.25811192709706093
iteration : 12024
train acc:  0.875
train loss:  0.31255823373794556
train gradient:  0.12616293920848767
iteration : 12025
train acc:  0.90625
train loss:  0.3222240209579468
train gradient:  0.11078231610333394
iteration : 12026
train acc:  0.875
train loss:  0.3067225515842438
train gradient:  0.1703904742754277
iteration : 12027
train acc:  0.828125
train loss:  0.356873095035553
train gradient:  0.14078780121545223
iteration : 12028
train acc:  0.8671875
train loss:  0.30100569128990173
train gradient:  0.11978095261388624
iteration : 12029
train acc:  0.90625
train loss:  0.31056660413742065
train gradient:  0.1781866401537185
iteration : 12030
train acc:  0.8515625
train loss:  0.33340078592300415
train gradient:  0.15973404805784774
iteration : 12031
train acc:  0.828125
train loss:  0.3023620843887329
train gradient:  0.1030961166247108
iteration : 12032
train acc:  0.7890625
train loss:  0.3876478672027588
train gradient:  0.23130121830742323
iteration : 12033
train acc:  0.859375
train loss:  0.3418520390987396
train gradient:  0.14070814680464916
iteration : 12034
train acc:  0.8984375
train loss:  0.26906585693359375
train gradient:  0.13176839411421046
iteration : 12035
train acc:  0.8828125
train loss:  0.3179928660392761
train gradient:  0.11316524966676328
iteration : 12036
train acc:  0.859375
train loss:  0.34439730644226074
train gradient:  0.18278064175979214
iteration : 12037
train acc:  0.890625
train loss:  0.2413608431816101
train gradient:  0.17072563559602522
iteration : 12038
train acc:  0.859375
train loss:  0.3557584881782532
train gradient:  0.16071716635903743
iteration : 12039
train acc:  0.84375
train loss:  0.31390368938446045
train gradient:  0.13441420980520288
iteration : 12040
train acc:  0.859375
train loss:  0.3240017890930176
train gradient:  0.14818241357455736
iteration : 12041
train acc:  0.9140625
train loss:  0.27289026975631714
train gradient:  0.0731157866161665
iteration : 12042
train acc:  0.8671875
train loss:  0.2937138080596924
train gradient:  0.08793379689931015
iteration : 12043
train acc:  0.859375
train loss:  0.3027153015136719
train gradient:  0.1302613759030233
iteration : 12044
train acc:  0.8203125
train loss:  0.3550608456134796
train gradient:  0.14803639908268085
iteration : 12045
train acc:  0.84375
train loss:  0.3607342839241028
train gradient:  0.18126474877440624
iteration : 12046
train acc:  0.84375
train loss:  0.37991905212402344
train gradient:  0.19109147702086232
iteration : 12047
train acc:  0.8359375
train loss:  0.3109407126903534
train gradient:  0.11771465323868761
iteration : 12048
train acc:  0.8984375
train loss:  0.2541222870349884
train gradient:  0.09208271756347429
iteration : 12049
train acc:  0.8984375
train loss:  0.2830497622489929
train gradient:  0.1478532687313932
iteration : 12050
train acc:  0.828125
train loss:  0.31161049008369446
train gradient:  0.1650589880542234
iteration : 12051
train acc:  0.859375
train loss:  0.3111416697502136
train gradient:  0.1496101253315852
iteration : 12052
train acc:  0.8515625
train loss:  0.3344417214393616
train gradient:  0.13454622279898984
iteration : 12053
train acc:  0.84375
train loss:  0.3378463387489319
train gradient:  0.16510949831364607
iteration : 12054
train acc:  0.8671875
train loss:  0.3244914412498474
train gradient:  0.20263958870768956
iteration : 12055
train acc:  0.8515625
train loss:  0.31281954050064087
train gradient:  0.13113052084023902
iteration : 12056
train acc:  0.8828125
train loss:  0.3285253047943115
train gradient:  0.1790724265510295
iteration : 12057
train acc:  0.875
train loss:  0.2851670980453491
train gradient:  0.12343348657457598
iteration : 12058
train acc:  0.8671875
train loss:  0.3000718653202057
train gradient:  0.1661838993960838
iteration : 12059
train acc:  0.859375
train loss:  0.2915140986442566
train gradient:  0.10617888791005374
iteration : 12060
train acc:  0.921875
train loss:  0.21089419722557068
train gradient:  0.06648833621411686
iteration : 12061
train acc:  0.84375
train loss:  0.3516429662704468
train gradient:  0.18962130451129877
iteration : 12062
train acc:  0.875
train loss:  0.3868248760700226
train gradient:  0.18705249481131694
iteration : 12063
train acc:  0.8515625
train loss:  0.30517008900642395
train gradient:  0.10557909995337089
iteration : 12064
train acc:  0.84375
train loss:  0.3664798140525818
train gradient:  0.18187997691995372
iteration : 12065
train acc:  0.8984375
train loss:  0.3126089572906494
train gradient:  0.11224073080698922
iteration : 12066
train acc:  0.8671875
train loss:  0.2840338945388794
train gradient:  0.12391540322545247
iteration : 12067
train acc:  0.8984375
train loss:  0.22963997721672058
train gradient:  0.051402129888253045
iteration : 12068
train acc:  0.8125
train loss:  0.4092687964439392
train gradient:  0.23305459678638202
iteration : 12069
train acc:  0.8515625
train loss:  0.3007405996322632
train gradient:  0.10668533621447764
iteration : 12070
train acc:  0.859375
train loss:  0.3152051866054535
train gradient:  0.12456007260623536
iteration : 12071
train acc:  0.828125
train loss:  0.35748517513275146
train gradient:  0.181386045302734
iteration : 12072
train acc:  0.8203125
train loss:  0.33815574645996094
train gradient:  0.21142940766517826
iteration : 12073
train acc:  0.796875
train loss:  0.3935742974281311
train gradient:  0.18675782854617295
iteration : 12074
train acc:  0.890625
train loss:  0.2714855968952179
train gradient:  0.12003591152427921
iteration : 12075
train acc:  0.875
train loss:  0.2722039818763733
train gradient:  0.10764813846074693
iteration : 12076
train acc:  0.8984375
train loss:  0.2588929831981659
train gradient:  0.08815367959759263
iteration : 12077
train acc:  0.828125
train loss:  0.369595468044281
train gradient:  0.1460329434372808
iteration : 12078
train acc:  0.8828125
train loss:  0.3390181362628937
train gradient:  0.145616771752029
iteration : 12079
train acc:  0.8671875
train loss:  0.26931601762771606
train gradient:  0.18833230032317627
iteration : 12080
train acc:  0.859375
train loss:  0.2953464984893799
train gradient:  0.13918864378471518
iteration : 12081
train acc:  0.8515625
train loss:  0.3232661187648773
train gradient:  0.11821859521977801
iteration : 12082
train acc:  0.8203125
train loss:  0.37058067321777344
train gradient:  0.24129311096755207
iteration : 12083
train acc:  0.8203125
train loss:  0.3866782784461975
train gradient:  0.21424735724283728
iteration : 12084
train acc:  0.8203125
train loss:  0.33632761240005493
train gradient:  0.14448715855645178
iteration : 12085
train acc:  0.8671875
train loss:  0.29353025555610657
train gradient:  0.11154585237694203
iteration : 12086
train acc:  0.890625
train loss:  0.3520229458808899
train gradient:  0.15368786819799563
iteration : 12087
train acc:  0.9296875
train loss:  0.25211143493652344
train gradient:  0.09636238380888702
iteration : 12088
train acc:  0.765625
train loss:  0.4765276312828064
train gradient:  0.23796246348796224
iteration : 12089
train acc:  0.8828125
train loss:  0.2840237021446228
train gradient:  0.10558515066924967
iteration : 12090
train acc:  0.84375
train loss:  0.3629980981349945
train gradient:  0.1639815522476681
iteration : 12091
train acc:  0.859375
train loss:  0.29592156410217285
train gradient:  0.15711418742924838
iteration : 12092
train acc:  0.8125
train loss:  0.38504189252853394
train gradient:  0.16229079623413417
iteration : 12093
train acc:  0.8984375
train loss:  0.27551931142807007
train gradient:  0.14189074326404302
iteration : 12094
train acc:  0.875
train loss:  0.333660751581192
train gradient:  0.16402394402119305
iteration : 12095
train acc:  0.796875
train loss:  0.3977954685688019
train gradient:  0.1894997410707526
iteration : 12096
train acc:  0.921875
train loss:  0.2452521175146103
train gradient:  0.13932600468510603
iteration : 12097
train acc:  0.8984375
train loss:  0.24537082016468048
train gradient:  0.12048546711598904
iteration : 12098
train acc:  0.8359375
train loss:  0.40242937207221985
train gradient:  0.14202371073479936
iteration : 12099
train acc:  0.8515625
train loss:  0.3690304756164551
train gradient:  0.21809154617183957
iteration : 12100
train acc:  0.859375
train loss:  0.28017210960388184
train gradient:  0.15351380829065941
iteration : 12101
train acc:  0.8828125
train loss:  0.30145183205604553
train gradient:  0.10941575090283578
iteration : 12102
train acc:  0.8359375
train loss:  0.34910666942596436
train gradient:  0.19462389752904413
iteration : 12103
train acc:  0.8671875
train loss:  0.30488356947898865
train gradient:  0.1345744144878283
iteration : 12104
train acc:  0.8671875
train loss:  0.2771347165107727
train gradient:  0.11101841212369801
iteration : 12105
train acc:  0.875
train loss:  0.2948874235153198
train gradient:  0.13366583214924893
iteration : 12106
train acc:  0.921875
train loss:  0.21670037508010864
train gradient:  0.0796969863443536
iteration : 12107
train acc:  0.8828125
train loss:  0.3393557667732239
train gradient:  0.12612809136974448
iteration : 12108
train acc:  0.90625
train loss:  0.3134748637676239
train gradient:  0.13102629135998578
iteration : 12109
train acc:  0.875
train loss:  0.3258960247039795
train gradient:  0.16469742872034648
iteration : 12110
train acc:  0.8515625
train loss:  0.34460681676864624
train gradient:  0.12114207954586463
iteration : 12111
train acc:  0.84375
train loss:  0.37758851051330566
train gradient:  4.197048412505213
iteration : 12112
train acc:  0.8203125
train loss:  0.32821840047836304
train gradient:  0.13993732813066934
iteration : 12113
train acc:  0.8515625
train loss:  0.3330276608467102
train gradient:  0.13207273064356695
iteration : 12114
train acc:  0.84375
train loss:  0.41541773080825806
train gradient:  0.20545865744394634
iteration : 12115
train acc:  0.90625
train loss:  0.2974560558795929
train gradient:  0.08939792829578139
iteration : 12116
train acc:  0.8515625
train loss:  0.3510465919971466
train gradient:  0.1729817541273772
iteration : 12117
train acc:  0.921875
train loss:  0.29018521308898926
train gradient:  0.15014244159256201
iteration : 12118
train acc:  0.859375
train loss:  0.4018074870109558
train gradient:  0.16463742345910365
iteration : 12119
train acc:  0.890625
train loss:  0.27082163095474243
train gradient:  0.11028066040193521
iteration : 12120
train acc:  0.890625
train loss:  0.2736661434173584
train gradient:  0.19166461341484897
iteration : 12121
train acc:  0.859375
train loss:  0.3919992446899414
train gradient:  0.14641281244553805
iteration : 12122
train acc:  0.890625
train loss:  0.26584577560424805
train gradient:  0.09315960605002674
iteration : 12123
train acc:  0.78125
train loss:  0.42685145139694214
train gradient:  0.18296906400202786
iteration : 12124
train acc:  0.8671875
train loss:  0.32395511865615845
train gradient:  0.13497880418672553
iteration : 12125
train acc:  0.8515625
train loss:  0.3045061528682709
train gradient:  0.1484610258276916
iteration : 12126
train acc:  0.859375
train loss:  0.37005814909935
train gradient:  0.15129254268794895
iteration : 12127
train acc:  0.890625
train loss:  0.2724584937095642
train gradient:  0.09262983972542915
iteration : 12128
train acc:  0.8984375
train loss:  0.23020504415035248
train gradient:  0.09993154269371164
iteration : 12129
train acc:  0.84375
train loss:  0.3926679790019989
train gradient:  0.14448295551792784
iteration : 12130
train acc:  0.84375
train loss:  0.36088770627975464
train gradient:  0.17634885657285002
iteration : 12131
train acc:  0.84375
train loss:  0.28451991081237793
train gradient:  0.09863794212691407
iteration : 12132
train acc:  0.84375
train loss:  0.36652761697769165
train gradient:  0.1817184337984244
iteration : 12133
train acc:  0.875
train loss:  0.34915751218795776
train gradient:  0.13209374721798794
iteration : 12134
train acc:  0.859375
train loss:  0.2607367932796478
train gradient:  0.08103824981866085
iteration : 12135
train acc:  0.875
train loss:  0.28378981351852417
train gradient:  0.1383133690815555
iteration : 12136
train acc:  0.8828125
train loss:  0.2465270459651947
train gradient:  0.10175359976317068
iteration : 12137
train acc:  0.859375
train loss:  0.3514465391635895
train gradient:  0.13357289558326618
iteration : 12138
train acc:  0.8828125
train loss:  0.2446480244398117
train gradient:  0.08965478595508279
iteration : 12139
train acc:  0.890625
train loss:  0.26178818941116333
train gradient:  0.12667589521959016
iteration : 12140
train acc:  0.890625
train loss:  0.2819627523422241
train gradient:  0.08857700186187983
iteration : 12141
train acc:  0.921875
train loss:  0.2294018566608429
train gradient:  0.0956539921240869
iteration : 12142
train acc:  0.8359375
train loss:  0.34374141693115234
train gradient:  0.14644873939198932
iteration : 12143
train acc:  0.828125
train loss:  0.34201762080192566
train gradient:  0.10434055206973507
iteration : 12144
train acc:  0.859375
train loss:  0.29730063676834106
train gradient:  0.10717573260453614
iteration : 12145
train acc:  0.8671875
train loss:  0.3198753595352173
train gradient:  0.1295860854981976
iteration : 12146
train acc:  0.84375
train loss:  0.33827516436576843
train gradient:  0.2708495521951827
iteration : 12147
train acc:  0.8671875
train loss:  0.33989807963371277
train gradient:  0.15988625231839282
iteration : 12148
train acc:  0.859375
train loss:  0.3127848207950592
train gradient:  0.1333619852123234
iteration : 12149
train acc:  0.8828125
train loss:  0.2683042585849762
train gradient:  0.0875367613466691
iteration : 12150
train acc:  0.8515625
train loss:  0.34172940254211426
train gradient:  0.1860349043522035
iteration : 12151
train acc:  0.890625
train loss:  0.3049607574939728
train gradient:  0.13603105121724898
iteration : 12152
train acc:  0.828125
train loss:  0.34785696864128113
train gradient:  0.14501621032929735
iteration : 12153
train acc:  0.8359375
train loss:  0.3551551103591919
train gradient:  0.16161306409672932
iteration : 12154
train acc:  0.890625
train loss:  0.2957048714160919
train gradient:  0.10973424480936739
iteration : 12155
train acc:  0.859375
train loss:  0.2769644260406494
train gradient:  0.13112788196062208
iteration : 12156
train acc:  0.9140625
train loss:  0.28137269616127014
train gradient:  0.0983291389065921
iteration : 12157
train acc:  0.8671875
train loss:  0.28045421838760376
train gradient:  0.14571536864095655
iteration : 12158
train acc:  0.875
train loss:  0.3135375380516052
train gradient:  0.12229097791525241
iteration : 12159
train acc:  0.8125
train loss:  0.4050242304801941
train gradient:  0.18682777732771816
iteration : 12160
train acc:  0.8046875
train loss:  0.36741936206817627
train gradient:  0.1532364539714676
iteration : 12161
train acc:  0.8828125
train loss:  0.30298784375190735
train gradient:  0.17927703780328913
iteration : 12162
train acc:  0.8828125
train loss:  0.27266451716423035
train gradient:  0.14379184886491989
iteration : 12163
train acc:  0.8984375
train loss:  0.2796643376350403
train gradient:  0.1290362971319985
iteration : 12164
train acc:  0.8671875
train loss:  0.3038654327392578
train gradient:  0.2223902048127124
iteration : 12165
train acc:  0.9140625
train loss:  0.22561737895011902
train gradient:  0.0828532538051112
iteration : 12166
train acc:  0.8828125
train loss:  0.32710593938827515
train gradient:  0.1332556350967301
iteration : 12167
train acc:  0.8125
train loss:  0.3588370978832245
train gradient:  0.2303954361551649
iteration : 12168
train acc:  0.859375
train loss:  0.3496622145175934
train gradient:  0.13884655388302553
iteration : 12169
train acc:  0.8828125
train loss:  0.31903737783432007
train gradient:  0.142738467425838
iteration : 12170
train acc:  0.890625
train loss:  0.2694830298423767
train gradient:  0.09336386572024678
iteration : 12171
train acc:  0.8515625
train loss:  0.35317885875701904
train gradient:  0.1513487152848927
iteration : 12172
train acc:  0.8671875
train loss:  0.3353150486946106
train gradient:  0.14337270713169858
iteration : 12173
train acc:  0.8828125
train loss:  0.2862793803215027
train gradient:  0.11091093521715274
iteration : 12174
train acc:  0.875
train loss:  0.3602537214756012
train gradient:  0.1618730175011373
iteration : 12175
train acc:  0.8828125
train loss:  0.2845920920372009
train gradient:  0.1620422514837984
iteration : 12176
train acc:  0.890625
train loss:  0.26836833357810974
train gradient:  0.10775420228270252
iteration : 12177
train acc:  0.8671875
train loss:  0.3355933725833893
train gradient:  0.11504843518238837
iteration : 12178
train acc:  0.859375
train loss:  0.32682591676712036
train gradient:  0.11618903780161254
iteration : 12179
train acc:  0.8984375
train loss:  0.2810558080673218
train gradient:  0.11853249387495018
iteration : 12180
train acc:  0.8828125
train loss:  0.2806921601295471
train gradient:  0.12645430301824356
iteration : 12181
train acc:  0.8671875
train loss:  0.29698795080184937
train gradient:  0.13291579235389503
iteration : 12182
train acc:  0.8671875
train loss:  0.2971702814102173
train gradient:  0.1581320679822054
iteration : 12183
train acc:  0.859375
train loss:  0.32383278012275696
train gradient:  0.158502963703838
iteration : 12184
train acc:  0.84375
train loss:  0.38750600814819336
train gradient:  0.18194883907852308
iteration : 12185
train acc:  0.890625
train loss:  0.30433928966522217
train gradient:  0.1597683942239141
iteration : 12186
train acc:  0.8515625
train loss:  0.36646267771720886
train gradient:  0.2023010915060121
iteration : 12187
train acc:  0.859375
train loss:  0.28315261006355286
train gradient:  0.11961494678470254
iteration : 12188
train acc:  0.84375
train loss:  0.2995142340660095
train gradient:  0.1418946531823262
iteration : 12189
train acc:  0.859375
train loss:  0.3141556978225708
train gradient:  0.2096329846937687
iteration : 12190
train acc:  0.8515625
train loss:  0.37656790018081665
train gradient:  0.21413960769796886
iteration : 12191
train acc:  0.921875
train loss:  0.2993992567062378
train gradient:  0.13471295770029454
iteration : 12192
train acc:  0.8671875
train loss:  0.2944672703742981
train gradient:  0.14466123234720177
iteration : 12193
train acc:  0.84375
train loss:  0.30509424209594727
train gradient:  0.15086142316088616
iteration : 12194
train acc:  0.84375
train loss:  0.35756200551986694
train gradient:  0.1591528528182664
iteration : 12195
train acc:  0.8671875
train loss:  0.3322100043296814
train gradient:  0.15333871763501533
iteration : 12196
train acc:  0.890625
train loss:  0.26526355743408203
train gradient:  0.1506786929961526
iteration : 12197
train acc:  0.8515625
train loss:  0.28852003812789917
train gradient:  0.09641234161050095
iteration : 12198
train acc:  0.890625
train loss:  0.28405284881591797
train gradient:  0.1101905286691463
iteration : 12199
train acc:  0.8984375
train loss:  0.24764664471149445
train gradient:  0.1076451643948193
iteration : 12200
train acc:  0.875
train loss:  0.3204975128173828
train gradient:  0.1377212759428966
iteration : 12201
train acc:  0.875
train loss:  0.29572948813438416
train gradient:  0.12265064290977908
iteration : 12202
train acc:  0.9140625
train loss:  0.21170014142990112
train gradient:  0.08239895456255021
iteration : 12203
train acc:  0.859375
train loss:  0.27979058027267456
train gradient:  0.10048437073947782
iteration : 12204
train acc:  0.8359375
train loss:  0.3469078540802002
train gradient:  0.16664025350093936
iteration : 12205
train acc:  0.9140625
train loss:  0.3106762766838074
train gradient:  0.24418962338027494
iteration : 12206
train acc:  0.859375
train loss:  0.3440452218055725
train gradient:  0.12836313713819575
iteration : 12207
train acc:  0.8515625
train loss:  0.36610764265060425
train gradient:  0.1833350684682335
iteration : 12208
train acc:  0.8359375
train loss:  0.34652799367904663
train gradient:  0.18238723144756405
iteration : 12209
train acc:  0.875
train loss:  0.3129037022590637
train gradient:  0.12843577359637826
iteration : 12210
train acc:  0.8515625
train loss:  0.3331012725830078
train gradient:  0.18765605307802996
iteration : 12211
train acc:  0.9140625
train loss:  0.2506134510040283
train gradient:  0.10466515130918008
iteration : 12212
train acc:  0.875
train loss:  0.29490724205970764
train gradient:  0.15398539277693857
iteration : 12213
train acc:  0.8984375
train loss:  0.25857287645339966
train gradient:  0.10285191445527267
iteration : 12214
train acc:  0.8203125
train loss:  0.3780589699745178
train gradient:  0.15413112454176298
iteration : 12215
train acc:  0.8515625
train loss:  0.3099740147590637
train gradient:  0.10826731159216677
iteration : 12216
train acc:  0.8984375
train loss:  0.2361578345298767
train gradient:  0.08816660714721326
iteration : 12217
train acc:  0.8125
train loss:  0.36827194690704346
train gradient:  0.2680205230410322
iteration : 12218
train acc:  0.796875
train loss:  0.353082537651062
train gradient:  0.18660453350662903
iteration : 12219
train acc:  0.8828125
train loss:  0.2722982168197632
train gradient:  0.11312317449462785
iteration : 12220
train acc:  0.859375
train loss:  0.3458103537559509
train gradient:  0.14902138071397608
iteration : 12221
train acc:  0.875
train loss:  0.343180775642395
train gradient:  0.10745249293134204
iteration : 12222
train acc:  0.875
train loss:  0.3027873635292053
train gradient:  0.11507150097811947
iteration : 12223
train acc:  0.90625
train loss:  0.27626681327819824
train gradient:  0.1498739978809388
iteration : 12224
train acc:  0.8671875
train loss:  0.3278253674507141
train gradient:  0.22149309925342628
iteration : 12225
train acc:  0.828125
train loss:  0.35389626026153564
train gradient:  0.1389688176930699
iteration : 12226
train acc:  0.828125
train loss:  0.33836835622787476
train gradient:  0.18387744367804498
iteration : 12227
train acc:  0.859375
train loss:  0.3269837498664856
train gradient:  0.19724613649421863
iteration : 12228
train acc:  0.9140625
train loss:  0.28489649295806885
train gradient:  0.09966980036389769
iteration : 12229
train acc:  0.8671875
train loss:  0.31255900859832764
train gradient:  0.13982045597605303
iteration : 12230
train acc:  0.8984375
train loss:  0.2554370164871216
train gradient:  0.11115570901969204
iteration : 12231
train acc:  0.84375
train loss:  0.34006863832473755
train gradient:  0.15466822030074653
iteration : 12232
train acc:  0.890625
train loss:  0.26930952072143555
train gradient:  0.10688562661551874
iteration : 12233
train acc:  0.84375
train loss:  0.3393963575363159
train gradient:  0.1560417175066215
iteration : 12234
train acc:  0.84375
train loss:  0.35052362084388733
train gradient:  0.15425169959350654
iteration : 12235
train acc:  0.8828125
train loss:  0.2969830632209778
train gradient:  0.09575683531451061
iteration : 12236
train acc:  0.859375
train loss:  0.33227309584617615
train gradient:  0.1557896409878467
iteration : 12237
train acc:  0.90625
train loss:  0.2584117650985718
train gradient:  0.08863339521663878
iteration : 12238
train acc:  0.8984375
train loss:  0.2930511236190796
train gradient:  0.10694892661062763
iteration : 12239
train acc:  0.828125
train loss:  0.29901769757270813
train gradient:  0.11356977092880322
iteration : 12240
train acc:  0.8828125
train loss:  0.29540595412254333
train gradient:  0.15780024386400848
iteration : 12241
train acc:  0.8515625
train loss:  0.3870319724082947
train gradient:  0.24065291202139832
iteration : 12242
train acc:  0.859375
train loss:  0.3166373074054718
train gradient:  0.1635846901149797
iteration : 12243
train acc:  0.8359375
train loss:  0.34076768159866333
train gradient:  0.12331327409557852
iteration : 12244
train acc:  0.828125
train loss:  0.33069515228271484
train gradient:  0.2154371627491623
iteration : 12245
train acc:  0.859375
train loss:  0.2937825918197632
train gradient:  0.11367611227455676
iteration : 12246
train acc:  0.8515625
train loss:  0.32724517583847046
train gradient:  0.14844641750774032
iteration : 12247
train acc:  0.8359375
train loss:  0.317685067653656
train gradient:  0.12802121536401087
iteration : 12248
train acc:  0.8828125
train loss:  0.27508869767189026
train gradient:  0.16456265712099993
iteration : 12249
train acc:  0.796875
train loss:  0.44145524501800537
train gradient:  0.23319056035740385
iteration : 12250
train acc:  0.84375
train loss:  0.3352314233779907
train gradient:  0.1259304435768325
iteration : 12251
train acc:  0.859375
train loss:  0.3233976364135742
train gradient:  0.13077703646698496
iteration : 12252
train acc:  0.875
train loss:  0.31628698110580444
train gradient:  0.1135659708286833
iteration : 12253
train acc:  0.8359375
train loss:  0.370266854763031
train gradient:  0.1909781950358306
iteration : 12254
train acc:  0.859375
train loss:  0.3192058503627777
train gradient:  0.11427386397549327
iteration : 12255
train acc:  0.890625
train loss:  0.24401424825191498
train gradient:  0.07735175730370147
iteration : 12256
train acc:  0.875
train loss:  0.3388136625289917
train gradient:  0.12394140224751517
iteration : 12257
train acc:  0.828125
train loss:  0.35911399126052856
train gradient:  0.15519779903556172
iteration : 12258
train acc:  0.84375
train loss:  0.38635820150375366
train gradient:  0.14559546742892046
iteration : 12259
train acc:  0.890625
train loss:  0.2566725015640259
train gradient:  0.11328023200649968
iteration : 12260
train acc:  0.8515625
train loss:  0.3103756308555603
train gradient:  0.14995522032232433
iteration : 12261
train acc:  0.9140625
train loss:  0.31935805082321167
train gradient:  0.18862382110973747
iteration : 12262
train acc:  0.8359375
train loss:  0.36421942710876465
train gradient:  0.1852814829748908
iteration : 12263
train acc:  0.875
train loss:  0.2800201177597046
train gradient:  0.09254819235056533
iteration : 12264
train acc:  0.875
train loss:  0.2968828082084656
train gradient:  0.11747009915173681
iteration : 12265
train acc:  0.859375
train loss:  0.2787477374076843
train gradient:  0.11481789121858822
iteration : 12266
train acc:  0.8828125
train loss:  0.3101672828197479
train gradient:  0.12148191389735606
iteration : 12267
train acc:  0.890625
train loss:  0.2981075048446655
train gradient:  0.1361971866288751
iteration : 12268
train acc:  0.8359375
train loss:  0.4147259294986725
train gradient:  0.21370806945092738
iteration : 12269
train acc:  0.875
train loss:  0.2968493402004242
train gradient:  0.1437177797098566
iteration : 12270
train acc:  0.9375
train loss:  0.2281831055879593
train gradient:  0.1443931776949901
iteration : 12271
train acc:  0.8203125
train loss:  0.4178808927536011
train gradient:  0.2663600682419148
iteration : 12272
train acc:  0.8828125
train loss:  0.27088800072669983
train gradient:  0.13588902979801692
iteration : 12273
train acc:  0.8671875
train loss:  0.34248974919319153
train gradient:  0.2009443323561228
iteration : 12274
train acc:  0.8828125
train loss:  0.26228857040405273
train gradient:  0.11319651592926404
iteration : 12275
train acc:  0.859375
train loss:  0.3191124200820923
train gradient:  0.15414201701336058
iteration : 12276
train acc:  0.8828125
train loss:  0.2790418863296509
train gradient:  0.11236831353133789
iteration : 12277
train acc:  0.84375
train loss:  0.34521156549453735
train gradient:  0.21249248810993576
iteration : 12278
train acc:  0.8359375
train loss:  0.3568926453590393
train gradient:  0.1863847571191739
iteration : 12279
train acc:  0.890625
train loss:  0.27253663539886475
train gradient:  0.09167170458563664
iteration : 12280
train acc:  0.8671875
train loss:  0.3116987347602844
train gradient:  0.13685500353556068
iteration : 12281
train acc:  0.796875
train loss:  0.4118505120277405
train gradient:  0.2437224844590845
iteration : 12282
train acc:  0.8203125
train loss:  0.33253708481788635
train gradient:  0.1757600184331607
iteration : 12283
train acc:  0.765625
train loss:  0.40954509377479553
train gradient:  0.20346863959805433
iteration : 12284
train acc:  0.859375
train loss:  0.2682187557220459
train gradient:  0.12918121436275204
iteration : 12285
train acc:  0.8203125
train loss:  0.36474478244781494
train gradient:  0.24658368050047022
iteration : 12286
train acc:  0.84375
train loss:  0.31771498918533325
train gradient:  0.1559192125232537
iteration : 12287
train acc:  0.8671875
train loss:  0.3403517007827759
train gradient:  0.1396777769008133
iteration : 12288
train acc:  0.8515625
train loss:  0.303375244140625
train gradient:  0.12255602182976395
iteration : 12289
train acc:  0.890625
train loss:  0.27537864446640015
train gradient:  0.13197890946856533
iteration : 12290
train acc:  0.875
train loss:  0.26430362462997437
train gradient:  0.10873394240374268
iteration : 12291
train acc:  0.9453125
train loss:  0.23722147941589355
train gradient:  0.06642299043973128
iteration : 12292
train acc:  0.890625
train loss:  0.27698689699172974
train gradient:  0.13729457630562064
iteration : 12293
train acc:  0.8828125
train loss:  0.2877982556819916
train gradient:  0.13339238298106115
iteration : 12294
train acc:  0.8828125
train loss:  0.3307361602783203
train gradient:  0.18072092222532915
iteration : 12295
train acc:  0.8359375
train loss:  0.32314956188201904
train gradient:  0.13506432984147632
iteration : 12296
train acc:  0.8515625
train loss:  0.3340045213699341
train gradient:  0.12107501668393482
iteration : 12297
train acc:  0.859375
train loss:  0.37377017736434937
train gradient:  0.17681158555421325
iteration : 12298
train acc:  0.90625
train loss:  0.24212519824504852
train gradient:  0.09468398258445304
iteration : 12299
train acc:  0.859375
train loss:  0.32422327995300293
train gradient:  0.16320803558211122
iteration : 12300
train acc:  0.8203125
train loss:  0.3699615001678467
train gradient:  0.17736636236769204
iteration : 12301
train acc:  0.828125
train loss:  0.3868369162082672
train gradient:  0.17767100695578897
iteration : 12302
train acc:  0.8046875
train loss:  0.39704519510269165
train gradient:  0.19847883905443475
iteration : 12303
train acc:  0.921875
train loss:  0.2219286412000656
train gradient:  0.0740334998793258
iteration : 12304
train acc:  0.875
train loss:  0.28012746572494507
train gradient:  0.13517544129595654
iteration : 12305
train acc:  0.828125
train loss:  0.30643904209136963
train gradient:  0.16534595581615494
iteration : 12306
train acc:  0.859375
train loss:  0.3018612861633301
train gradient:  0.163316480068894
iteration : 12307
train acc:  0.8671875
train loss:  0.3331800699234009
train gradient:  0.09726198707644987
iteration : 12308
train acc:  0.921875
train loss:  0.2431403249502182
train gradient:  0.07571885600565498
iteration : 12309
train acc:  0.7734375
train loss:  0.47062358260154724
train gradient:  0.2350151862176133
iteration : 12310
train acc:  0.890625
train loss:  0.30395668745040894
train gradient:  0.16043588316168866
iteration : 12311
train acc:  0.8125
train loss:  0.33900171518325806
train gradient:  0.15626124472268033
iteration : 12312
train acc:  0.8203125
train loss:  0.34848374128341675
train gradient:  0.15615803319220484
iteration : 12313
train acc:  0.8671875
train loss:  0.27911943197250366
train gradient:  0.11724763718842458
iteration : 12314
train acc:  0.9140625
train loss:  0.22355927526950836
train gradient:  0.07032691836224328
iteration : 12315
train acc:  0.859375
train loss:  0.32526150345802307
train gradient:  0.18792217861951765
iteration : 12316
train acc:  0.890625
train loss:  0.2873075008392334
train gradient:  0.11526212307263067
iteration : 12317
train acc:  0.8671875
train loss:  0.34801506996154785
train gradient:  0.14746092229211344
iteration : 12318
train acc:  0.8984375
train loss:  0.2766135334968567
train gradient:  0.1306695689831129
iteration : 12319
train acc:  0.875
train loss:  0.21790874004364014
train gradient:  0.07916143517126706
iteration : 12320
train acc:  0.8828125
train loss:  0.29945385456085205
train gradient:  0.13529273204474873
iteration : 12321
train acc:  0.8046875
train loss:  0.3339399993419647
train gradient:  0.18615613528674724
iteration : 12322
train acc:  0.8359375
train loss:  0.33389806747436523
train gradient:  0.14566822150105863
iteration : 12323
train acc:  0.84375
train loss:  0.3024889826774597
train gradient:  0.10971053189810194
iteration : 12324
train acc:  0.8046875
train loss:  0.35977989435195923
train gradient:  0.15872044214344078
iteration : 12325
train acc:  0.8359375
train loss:  0.3207123279571533
train gradient:  0.17578507270411248
iteration : 12326
train acc:  0.8984375
train loss:  0.2953265905380249
train gradient:  0.1521345726506594
iteration : 12327
train acc:  0.8359375
train loss:  0.34010279178619385
train gradient:  0.21670335041420324
iteration : 12328
train acc:  0.828125
train loss:  0.35198062658309937
train gradient:  0.15253374792076047
iteration : 12329
train acc:  0.828125
train loss:  0.39759957790374756
train gradient:  0.1706660535211454
iteration : 12330
train acc:  0.8828125
train loss:  0.2901085913181305
train gradient:  0.12851556226970118
iteration : 12331
train acc:  0.890625
train loss:  0.3198084831237793
train gradient:  0.17559591616750514
iteration : 12332
train acc:  0.8125
train loss:  0.3175649046897888
train gradient:  0.2780619858324542
iteration : 12333
train acc:  0.84375
train loss:  0.3260166347026825
train gradient:  0.10934449882911938
iteration : 12334
train acc:  0.9296875
train loss:  0.23732958734035492
train gradient:  0.10788686515129084
iteration : 12335
train acc:  0.875
train loss:  0.28866147994995117
train gradient:  0.10370130861149539
iteration : 12336
train acc:  0.8671875
train loss:  0.28142088651657104
train gradient:  0.10278977969786002
iteration : 12337
train acc:  0.8359375
train loss:  0.3016294836997986
train gradient:  0.0997147967741671
iteration : 12338
train acc:  0.84375
train loss:  0.3817616105079651
train gradient:  0.20888546443776018
iteration : 12339
train acc:  0.84375
train loss:  0.3913780450820923
train gradient:  0.15586664210824672
iteration : 12340
train acc:  0.8359375
train loss:  0.39994221925735474
train gradient:  0.2038876935781519
iteration : 12341
train acc:  0.8359375
train loss:  0.3317111134529114
train gradient:  0.12140882757205793
iteration : 12342
train acc:  0.8828125
train loss:  0.30116692185401917
train gradient:  0.1255487389088774
iteration : 12343
train acc:  0.8828125
train loss:  0.2784880995750427
train gradient:  0.08068287340081277
iteration : 12344
train acc:  0.796875
train loss:  0.39653825759887695
train gradient:  0.2519859760897258
iteration : 12345
train acc:  0.921875
train loss:  0.2590996026992798
train gradient:  0.1393253912433119
iteration : 12346
train acc:  0.8359375
train loss:  0.37168043851852417
train gradient:  0.14012355125326245
iteration : 12347
train acc:  0.8203125
train loss:  0.3965004086494446
train gradient:  0.17338589319338632
iteration : 12348
train acc:  0.8671875
train loss:  0.3138396739959717
train gradient:  0.1381978474362292
iteration : 12349
train acc:  0.8515625
train loss:  0.3641016185283661
train gradient:  0.1596917442815618
iteration : 12350
train acc:  0.8828125
train loss:  0.3360271751880646
train gradient:  0.16977816962134157
iteration : 12351
train acc:  0.859375
train loss:  0.27110400795936584
train gradient:  0.10074698637526039
iteration : 12352
train acc:  0.8515625
train loss:  0.3605872392654419
train gradient:  0.1235179559329188
iteration : 12353
train acc:  0.859375
train loss:  0.36207932233810425
train gradient:  0.15982402682020058
iteration : 12354
train acc:  0.84375
train loss:  0.3387640416622162
train gradient:  0.23764014476736073
iteration : 12355
train acc:  0.8359375
train loss:  0.31499195098876953
train gradient:  0.11301391441601226
iteration : 12356
train acc:  0.8515625
train loss:  0.3721354603767395
train gradient:  0.31464951681784703
iteration : 12357
train acc:  0.8125
train loss:  0.4214099049568176
train gradient:  0.17142478929379518
iteration : 12358
train acc:  0.8828125
train loss:  0.32727622985839844
train gradient:  0.11302255186913679
iteration : 12359
train acc:  0.90625
train loss:  0.27767860889434814
train gradient:  0.0863496358793513
iteration : 12360
train acc:  0.8203125
train loss:  0.40631425380706787
train gradient:  0.2423112090958982
iteration : 12361
train acc:  0.84375
train loss:  0.3166086673736572
train gradient:  0.14647077053741017
iteration : 12362
train acc:  0.875
train loss:  0.3077675402164459
train gradient:  0.12843268991998313
iteration : 12363
train acc:  0.796875
train loss:  0.40733832120895386
train gradient:  0.23468767014736414
iteration : 12364
train acc:  0.8515625
train loss:  0.411476731300354
train gradient:  0.171661322771344
iteration : 12365
train acc:  0.90625
train loss:  0.24243655800819397
train gradient:  0.06922778688275422
iteration : 12366
train acc:  0.8828125
train loss:  0.36989927291870117
train gradient:  0.16513094415715718
iteration : 12367
train acc:  0.90625
train loss:  0.27881914377212524
train gradient:  0.11032628633584741
iteration : 12368
train acc:  0.8125
train loss:  0.41943952441215515
train gradient:  0.214170445967045
iteration : 12369
train acc:  0.8671875
train loss:  0.3001684844493866
train gradient:  0.14451399503374188
iteration : 12370
train acc:  0.859375
train loss:  0.3185436427593231
train gradient:  0.1618337026739896
iteration : 12371
train acc:  0.8203125
train loss:  0.3597625494003296
train gradient:  0.21598728464897152
iteration : 12372
train acc:  0.8671875
train loss:  0.3079703450202942
train gradient:  0.14417715818206372
iteration : 12373
train acc:  0.890625
train loss:  0.2896746098995209
train gradient:  0.08672010056473942
iteration : 12374
train acc:  0.8671875
train loss:  0.2701182961463928
train gradient:  0.11059702605848042
iteration : 12375
train acc:  0.921875
train loss:  0.32213059067726135
train gradient:  0.11276461219075244
iteration : 12376
train acc:  0.8671875
train loss:  0.32194167375564575
train gradient:  0.17543119056650194
iteration : 12377
train acc:  0.8046875
train loss:  0.42807528376579285
train gradient:  0.19982366642399707
iteration : 12378
train acc:  0.859375
train loss:  0.3017105460166931
train gradient:  0.1645006509013459
iteration : 12379
train acc:  0.8984375
train loss:  0.26135629415512085
train gradient:  0.09495829309758065
iteration : 12380
train acc:  0.828125
train loss:  0.3397471308708191
train gradient:  0.1917992824644974
iteration : 12381
train acc:  0.8515625
train loss:  0.3404248058795929
train gradient:  0.11985568970417894
iteration : 12382
train acc:  0.859375
train loss:  0.3134271502494812
train gradient:  0.1496979536374795
iteration : 12383
train acc:  0.8359375
train loss:  0.4099910855293274
train gradient:  0.17320067061228145
iteration : 12384
train acc:  0.8359375
train loss:  0.37334731221199036
train gradient:  0.15755882713392072
iteration : 12385
train acc:  0.875
train loss:  0.26380038261413574
train gradient:  0.1061438505957409
iteration : 12386
train acc:  0.890625
train loss:  0.33007511496543884
train gradient:  0.08629514576608349
iteration : 12387
train acc:  0.8671875
train loss:  0.3891857862472534
train gradient:  0.1942304496947161
iteration : 12388
train acc:  0.8203125
train loss:  0.3591306209564209
train gradient:  0.21521675643545415
iteration : 12389
train acc:  0.8515625
train loss:  0.3400918245315552
train gradient:  0.1438371561553915
iteration : 12390
train acc:  0.859375
train loss:  0.3252931237220764
train gradient:  0.24073564288697683
iteration : 12391
train acc:  0.8515625
train loss:  0.32468611001968384
train gradient:  0.14196365013900303
iteration : 12392
train acc:  0.890625
train loss:  0.29209595918655396
train gradient:  0.19066977086835996
iteration : 12393
train acc:  0.8671875
train loss:  0.30783236026763916
train gradient:  0.13443802113455183
iteration : 12394
train acc:  0.7890625
train loss:  0.4469236135482788
train gradient:  0.2527463717657768
iteration : 12395
train acc:  0.875
train loss:  0.24737274646759033
train gradient:  0.10074356006793563
iteration : 12396
train acc:  0.8046875
train loss:  0.3986568748950958
train gradient:  0.18178870100837347
iteration : 12397
train acc:  0.875
train loss:  0.33359307050704956
train gradient:  0.12463450783414227
iteration : 12398
train acc:  0.8203125
train loss:  0.44271230697631836
train gradient:  0.3077087253673128
iteration : 12399
train acc:  0.8515625
train loss:  0.35077592730522156
train gradient:  0.18264976966241384
iteration : 12400
train acc:  0.8828125
train loss:  0.28330305218696594
train gradient:  0.1533214060658456
iteration : 12401
train acc:  0.875
train loss:  0.32314059138298035
train gradient:  0.12245485733937947
iteration : 12402
train acc:  0.84375
train loss:  0.32621923089027405
train gradient:  0.1299865280142882
iteration : 12403
train acc:  0.90625
train loss:  0.2556101083755493
train gradient:  0.09225937220080724
iteration : 12404
train acc:  0.8359375
train loss:  0.3972071409225464
train gradient:  0.2254577149965139
iteration : 12405
train acc:  0.90625
train loss:  0.2675004303455353
train gradient:  0.10219344537653605
iteration : 12406
train acc:  0.8515625
train loss:  0.31270700693130493
train gradient:  0.15429090994406341
iteration : 12407
train acc:  0.8984375
train loss:  0.27921658754348755
train gradient:  0.09550417635855156
iteration : 12408
train acc:  0.890625
train loss:  0.2871412932872772
train gradient:  0.10201536373289767
iteration : 12409
train acc:  0.875
train loss:  0.2794656753540039
train gradient:  0.10568518275826837
iteration : 12410
train acc:  0.8125
train loss:  0.30969804525375366
train gradient:  0.13831261093804292
iteration : 12411
train acc:  0.8515625
train loss:  0.35765475034713745
train gradient:  0.12883159952063233
iteration : 12412
train acc:  0.859375
train loss:  0.3762555718421936
train gradient:  0.17063918105512738
iteration : 12413
train acc:  0.8515625
train loss:  0.37575608491897583
train gradient:  0.13350533925476027
iteration : 12414
train acc:  0.875
train loss:  0.31302371621131897
train gradient:  0.12113577244296506
iteration : 12415
train acc:  0.8828125
train loss:  0.31143617630004883
train gradient:  0.10111265857457101
iteration : 12416
train acc:  0.8984375
train loss:  0.23151473701000214
train gradient:  0.06332706265175368
iteration : 12417
train acc:  0.875
train loss:  0.3058704733848572
train gradient:  0.15247192889356437
iteration : 12418
train acc:  0.8828125
train loss:  0.2666212320327759
train gradient:  0.10833841255718855
iteration : 12419
train acc:  0.828125
train loss:  0.308175265789032
train gradient:  0.13584976505923282
iteration : 12420
train acc:  0.90625
train loss:  0.27248966693878174
train gradient:  0.10404672937390227
iteration : 12421
train acc:  0.8671875
train loss:  0.3160250782966614
train gradient:  0.17386666366270193
iteration : 12422
train acc:  0.875
train loss:  0.2800830006599426
train gradient:  0.09201297166302823
iteration : 12423
train acc:  0.8359375
train loss:  0.37739044427871704
train gradient:  0.14710713317036422
iteration : 12424
train acc:  0.828125
train loss:  0.38019421696662903
train gradient:  0.19175579429338774
iteration : 12425
train acc:  0.8046875
train loss:  0.3152129352092743
train gradient:  0.09617283844305423
iteration : 12426
train acc:  0.8515625
train loss:  0.2869512438774109
train gradient:  0.1051102012584437
iteration : 12427
train acc:  0.8828125
train loss:  0.2891201376914978
train gradient:  0.12451718991249404
iteration : 12428
train acc:  0.8515625
train loss:  0.3435553312301636
train gradient:  0.14999926496134977
iteration : 12429
train acc:  0.8828125
train loss:  0.3068373501300812
train gradient:  0.13206280504578627
iteration : 12430
train acc:  0.8984375
train loss:  0.3617756962776184
train gradient:  0.14459021532989574
iteration : 12431
train acc:  0.84375
train loss:  0.3647937774658203
train gradient:  0.28022700523613026
iteration : 12432
train acc:  0.8203125
train loss:  0.37526750564575195
train gradient:  0.141699538494233
iteration : 12433
train acc:  0.859375
train loss:  0.33109986782073975
train gradient:  0.16409021530494589
iteration : 12434
train acc:  0.875
train loss:  0.2983953058719635
train gradient:  0.11224351419462729
iteration : 12435
train acc:  0.8359375
train loss:  0.3416679799556732
train gradient:  0.19411631893767634
iteration : 12436
train acc:  0.890625
train loss:  0.3464805781841278
train gradient:  0.15085247671148305
iteration : 12437
train acc:  0.921875
train loss:  0.24800455570220947
train gradient:  0.1144078025806667
iteration : 12438
train acc:  0.8359375
train loss:  0.3751586675643921
train gradient:  0.17293910259163336
iteration : 12439
train acc:  0.8515625
train loss:  0.3819432854652405
train gradient:  0.17432932074305213
iteration : 12440
train acc:  0.84375
train loss:  0.29021942615509033
train gradient:  0.09917217519845908
iteration : 12441
train acc:  0.8671875
train loss:  0.40296244621276855
train gradient:  0.38365279504661015
iteration : 12442
train acc:  0.8203125
train loss:  0.322736918926239
train gradient:  0.11209997924075525
iteration : 12443
train acc:  0.828125
train loss:  0.27307939529418945
train gradient:  0.11927581529950873
iteration : 12444
train acc:  0.890625
train loss:  0.3317144513130188
train gradient:  0.10918384041654512
iteration : 12445
train acc:  0.875
train loss:  0.38040226697921753
train gradient:  0.17715722639481726
iteration : 12446
train acc:  0.8203125
train loss:  0.32323816418647766
train gradient:  0.1239494770674721
iteration : 12447
train acc:  0.890625
train loss:  0.3027753233909607
train gradient:  0.07295580995361413
iteration : 12448
train acc:  0.84375
train loss:  0.37088266015052795
train gradient:  0.1591587318303853
iteration : 12449
train acc:  0.8984375
train loss:  0.3176170587539673
train gradient:  0.1713869877990142
iteration : 12450
train acc:  0.8515625
train loss:  0.34648337960243225
train gradient:  0.1611637691057362
iteration : 12451
train acc:  0.84375
train loss:  0.31440263986587524
train gradient:  0.17939511195031163
iteration : 12452
train acc:  0.8203125
train loss:  0.4139557480812073
train gradient:  0.15238163000568744
iteration : 12453
train acc:  0.8671875
train loss:  0.3457021713256836
train gradient:  0.11613481667342483
iteration : 12454
train acc:  0.890625
train loss:  0.29594263434410095
train gradient:  0.1188775350584201
iteration : 12455
train acc:  0.8828125
train loss:  0.280459463596344
train gradient:  0.08949452785343587
iteration : 12456
train acc:  0.828125
train loss:  0.4070654511451721
train gradient:  0.1917817293260387
iteration : 12457
train acc:  0.8203125
train loss:  0.3854595422744751
train gradient:  0.14892733783590806
iteration : 12458
train acc:  0.890625
train loss:  0.3227398991584778
train gradient:  0.5900624430051473
iteration : 12459
train acc:  0.828125
train loss:  0.37883251905441284
train gradient:  0.14042797203930119
iteration : 12460
train acc:  0.8125
train loss:  0.3568202555179596
train gradient:  0.13761420876591474
iteration : 12461
train acc:  0.84375
train loss:  0.35007673501968384
train gradient:  0.15759567753740322
iteration : 12462
train acc:  0.859375
train loss:  0.3035030961036682
train gradient:  0.114805606295994
iteration : 12463
train acc:  0.8515625
train loss:  0.339156836271286
train gradient:  0.1323812043975678
iteration : 12464
train acc:  0.921875
train loss:  0.26311564445495605
train gradient:  0.10624813534580838
iteration : 12465
train acc:  0.8828125
train loss:  0.2790895402431488
train gradient:  0.11929911578077292
iteration : 12466
train acc:  0.8671875
train loss:  0.2888558506965637
train gradient:  0.11835248095000912
iteration : 12467
train acc:  0.9140625
train loss:  0.24412143230438232
train gradient:  0.09687029463204637
iteration : 12468
train acc:  0.8203125
train loss:  0.37612634897232056
train gradient:  0.16634484233072605
iteration : 12469
train acc:  0.8671875
train loss:  0.3898507356643677
train gradient:  0.2196831904448358
iteration : 12470
train acc:  0.875
train loss:  0.3008086085319519
train gradient:  0.10321128174640136
iteration : 12471
train acc:  0.875
train loss:  0.27926012873649597
train gradient:  0.10597641434568561
iteration : 12472
train acc:  0.8828125
train loss:  0.32761961221694946
train gradient:  0.1690596462497529
iteration : 12473
train acc:  0.84375
train loss:  0.37839996814727783
train gradient:  0.15714527579760834
iteration : 12474
train acc:  0.90625
train loss:  0.25921326875686646
train gradient:  0.0857131008833724
iteration : 12475
train acc:  0.8828125
train loss:  0.22393976151943207
train gradient:  0.08969702323731217
iteration : 12476
train acc:  0.828125
train loss:  0.34144967794418335
train gradient:  0.14854147778336876
iteration : 12477
train acc:  0.859375
train loss:  0.25381362438201904
train gradient:  0.07854992581183307
iteration : 12478
train acc:  0.8125
train loss:  0.3728412389755249
train gradient:  0.179022027572379
iteration : 12479
train acc:  0.875
train loss:  0.2868804335594177
train gradient:  0.11217018458721088
iteration : 12480
train acc:  0.8125
train loss:  0.35536402463912964
train gradient:  0.18375351846349364
iteration : 12481
train acc:  0.8828125
train loss:  0.2695139944553375
train gradient:  0.12365976859991984
iteration : 12482
train acc:  0.8671875
train loss:  0.3511902093887329
train gradient:  0.15502679827965413
iteration : 12483
train acc:  0.8828125
train loss:  0.31293362379074097
train gradient:  0.11526389188171882
iteration : 12484
train acc:  0.890625
train loss:  0.3047606647014618
train gradient:  0.17903987384118963
iteration : 12485
train acc:  0.84375
train loss:  0.3194340765476227
train gradient:  0.14945819023881768
iteration : 12486
train acc:  0.8515625
train loss:  0.3572620749473572
train gradient:  0.16073290364570728
iteration : 12487
train acc:  0.9140625
train loss:  0.26278936862945557
train gradient:  0.09093684770118342
iteration : 12488
train acc:  0.8828125
train loss:  0.2556923031806946
train gradient:  0.1307387265674435
iteration : 12489
train acc:  0.90625
train loss:  0.2977430820465088
train gradient:  0.10574741880173187
iteration : 12490
train acc:  0.875
train loss:  0.32481399178504944
train gradient:  0.12733846434441032
iteration : 12491
train acc:  0.8984375
train loss:  0.26509732007980347
train gradient:  0.20765107415980036
iteration : 12492
train acc:  0.859375
train loss:  0.36534547805786133
train gradient:  0.16250970589818917
iteration : 12493
train acc:  0.8125
train loss:  0.4281906485557556
train gradient:  0.29772196639656745
iteration : 12494
train acc:  0.84375
train loss:  0.393975168466568
train gradient:  0.18076357006001348
iteration : 12495
train acc:  0.8671875
train loss:  0.32024893164634705
train gradient:  0.10205722971558026
iteration : 12496
train acc:  0.859375
train loss:  0.39366093277931213
train gradient:  0.1706548934957467
iteration : 12497
train acc:  0.8671875
train loss:  0.3341933488845825
train gradient:  0.15075231166729922
iteration : 12498
train acc:  0.8515625
train loss:  0.30285269021987915
train gradient:  0.12894656260272747
iteration : 12499
train acc:  0.90625
train loss:  0.25184279680252075
train gradient:  0.10044901094236845
iteration : 12500
train acc:  0.828125
train loss:  0.38333576917648315
train gradient:  0.2228855706174271
iteration : 12501
train acc:  0.8671875
train loss:  0.3359067440032959
train gradient:  0.11961436372905519
iteration : 12502
train acc:  0.8984375
train loss:  0.2671886384487152
train gradient:  0.11192331194547675
iteration : 12503
train acc:  0.8828125
train loss:  0.30228060483932495
train gradient:  0.1444507449090527
iteration : 12504
train acc:  0.859375
train loss:  0.28921541571617126
train gradient:  0.1171569022434563
iteration : 12505
train acc:  0.8828125
train loss:  0.2804141342639923
train gradient:  0.14424250480211454
iteration : 12506
train acc:  0.8984375
train loss:  0.2532842755317688
train gradient:  0.23171728769998468
iteration : 12507
train acc:  0.8984375
train loss:  0.2878226935863495
train gradient:  0.1009565587074743
iteration : 12508
train acc:  0.8984375
train loss:  0.22980205714702606
train gradient:  0.07549882466116596
iteration : 12509
train acc:  0.859375
train loss:  0.3291984796524048
train gradient:  0.14897481869673343
iteration : 12510
train acc:  0.84375
train loss:  0.3477948307991028
train gradient:  0.14123483650343194
iteration : 12511
train acc:  0.890625
train loss:  0.31860899925231934
train gradient:  0.1898134185166519
iteration : 12512
train acc:  0.8828125
train loss:  0.23543187975883484
train gradient:  0.1202215632384677
iteration : 12513
train acc:  0.828125
train loss:  0.38076791167259216
train gradient:  0.1466433396812357
iteration : 12514
train acc:  0.8671875
train loss:  0.349564790725708
train gradient:  0.15355242771155414
iteration : 12515
train acc:  0.875
train loss:  0.32075124979019165
train gradient:  0.1307530044610995
iteration : 12516
train acc:  0.875
train loss:  0.3792842626571655
train gradient:  0.16385931710401774
iteration : 12517
train acc:  0.9140625
train loss:  0.24724584817886353
train gradient:  0.12353590948170688
iteration : 12518
train acc:  0.8671875
train loss:  0.32779955863952637
train gradient:  0.14689212926766115
iteration : 12519
train acc:  0.859375
train loss:  0.3355591297149658
train gradient:  0.1286254075647069
iteration : 12520
train acc:  0.875
train loss:  0.28881341218948364
train gradient:  0.08156269398506669
iteration : 12521
train acc:  0.8515625
train loss:  0.2858068346977234
train gradient:  0.09464875776608699
iteration : 12522
train acc:  0.875
train loss:  0.29837870597839355
train gradient:  0.0923012939017318
iteration : 12523
train acc:  0.875
train loss:  0.26059508323669434
train gradient:  0.1698107439499552
iteration : 12524
train acc:  0.828125
train loss:  0.37789374589920044
train gradient:  0.16702736387385958
iteration : 12525
train acc:  0.890625
train loss:  0.30097734928131104
train gradient:  0.09955709300086796
iteration : 12526
train acc:  0.875
train loss:  0.31450745463371277
train gradient:  0.13442649722290076
iteration : 12527
train acc:  0.84375
train loss:  0.3266022801399231
train gradient:  0.1806749839606736
iteration : 12528
train acc:  0.84375
train loss:  0.3078131079673767
train gradient:  0.13463844272277836
iteration : 12529
train acc:  0.875
train loss:  0.30339568853378296
train gradient:  0.16047376551982118
iteration : 12530
train acc:  0.8828125
train loss:  0.3288404941558838
train gradient:  0.18150287198302287
iteration : 12531
train acc:  0.875
train loss:  0.2817192077636719
train gradient:  0.1335788928170154
iteration : 12532
train acc:  0.8359375
train loss:  0.3837530016899109
train gradient:  0.1821922855536341
iteration : 12533
train acc:  0.8984375
train loss:  0.27437785267829895
train gradient:  0.11095025698916271
iteration : 12534
train acc:  0.8671875
train loss:  0.2710440158843994
train gradient:  0.09936698290191585
iteration : 12535
train acc:  0.875
train loss:  0.33829301595687866
train gradient:  0.1955678018892415
iteration : 12536
train acc:  0.8671875
train loss:  0.3823739290237427
train gradient:  0.2492263010871369
iteration : 12537
train acc:  0.890625
train loss:  0.2831266522407532
train gradient:  0.12072201854092358
iteration : 12538
train acc:  0.8671875
train loss:  0.34745875000953674
train gradient:  0.18152334542348825
iteration : 12539
train acc:  0.8671875
train loss:  0.2935997545719147
train gradient:  0.10654062552306345
iteration : 12540
train acc:  0.8359375
train loss:  0.449346125125885
train gradient:  0.20510064499799413
iteration : 12541
train acc:  0.8515625
train loss:  0.33387038111686707
train gradient:  0.1280729881334003
iteration : 12542
train acc:  0.875
train loss:  0.2561621367931366
train gradient:  0.07167074418927656
iteration : 12543
train acc:  0.8125
train loss:  0.3713104724884033
train gradient:  0.18013412092138903
iteration : 12544
train acc:  0.8515625
train loss:  0.35619938373565674
train gradient:  0.17915660491580965
iteration : 12545
train acc:  0.8828125
train loss:  0.34915536642074585
train gradient:  0.15920976822932859
iteration : 12546
train acc:  0.8984375
train loss:  0.26059579849243164
train gradient:  0.1120834665241906
iteration : 12547
train acc:  0.8359375
train loss:  0.3801301121711731
train gradient:  0.2030324732192323
iteration : 12548
train acc:  0.875
train loss:  0.2731306552886963
train gradient:  0.16061676183436802
iteration : 12549
train acc:  0.890625
train loss:  0.225660502910614
train gradient:  0.07206254089600962
iteration : 12550
train acc:  0.890625
train loss:  0.25391197204589844
train gradient:  0.08970035161578538
iteration : 12551
train acc:  0.828125
train loss:  0.3637238144874573
train gradient:  0.13125722289405645
iteration : 12552
train acc:  0.859375
train loss:  0.2850799560546875
train gradient:  0.16493484415499796
iteration : 12553
train acc:  0.90625
train loss:  0.29831406474113464
train gradient:  0.14488156078470518
iteration : 12554
train acc:  0.8671875
train loss:  0.2959405481815338
train gradient:  0.140788527239813
iteration : 12555
train acc:  0.8515625
train loss:  0.3151950240135193
train gradient:  0.14350542461696528
iteration : 12556
train acc:  0.8515625
train loss:  0.314746230840683
train gradient:  0.11201361180324626
iteration : 12557
train acc:  0.8359375
train loss:  0.3448755741119385
train gradient:  0.11616879756633179
iteration : 12558
train acc:  0.875
train loss:  0.2871129810810089
train gradient:  0.10822580459974897
iteration : 12559
train acc:  0.9140625
train loss:  0.2588807940483093
train gradient:  0.11862092904724082
iteration : 12560
train acc:  0.921875
train loss:  0.23453953862190247
train gradient:  0.0875780043000742
iteration : 12561
train acc:  0.875
train loss:  0.3354870676994324
train gradient:  0.1900495809102053
iteration : 12562
train acc:  0.8515625
train loss:  0.3269599676132202
train gradient:  0.19662591311120972
iteration : 12563
train acc:  0.8515625
train loss:  0.31240010261535645
train gradient:  0.147571188344933
iteration : 12564
train acc:  0.8671875
train loss:  0.331478476524353
train gradient:  0.13764844028566603
iteration : 12565
train acc:  0.78125
train loss:  0.4982655942440033
train gradient:  0.23072414589431872
iteration : 12566
train acc:  0.921875
train loss:  0.25263872742652893
train gradient:  0.1394019142471764
iteration : 12567
train acc:  0.8515625
train loss:  0.35839739441871643
train gradient:  0.1630363142850877
iteration : 12568
train acc:  0.8125
train loss:  0.40070125460624695
train gradient:  0.19636394829819814
iteration : 12569
train acc:  0.875
train loss:  0.29531726241111755
train gradient:  0.1493914685554811
iteration : 12570
train acc:  0.890625
train loss:  0.29329630732536316
train gradient:  0.14109540253753522
iteration : 12571
train acc:  0.859375
train loss:  0.3257875144481659
train gradient:  0.19827651656430534
iteration : 12572
train acc:  0.8828125
train loss:  0.2522052228450775
train gradient:  0.11078346833430633
iteration : 12573
train acc:  0.8515625
train loss:  0.3102191686630249
train gradient:  0.18317186133562086
iteration : 12574
train acc:  0.8359375
train loss:  0.33791089057922363
train gradient:  0.17116228906348313
iteration : 12575
train acc:  0.859375
train loss:  0.3113648295402527
train gradient:  0.17970885288501637
iteration : 12576
train acc:  0.828125
train loss:  0.30181390047073364
train gradient:  0.10184280584854011
iteration : 12577
train acc:  0.84375
train loss:  0.29860642552375793
train gradient:  0.13871244951090467
iteration : 12578
train acc:  0.875
train loss:  0.30414777994155884
train gradient:  0.13152094191192032
iteration : 12579
train acc:  0.828125
train loss:  0.4117254614830017
train gradient:  0.1918014731853
iteration : 12580
train acc:  0.8515625
train loss:  0.33988407254219055
train gradient:  0.1514295035620797
iteration : 12581
train acc:  0.875
train loss:  0.29472649097442627
train gradient:  0.13298974631245045
iteration : 12582
train acc:  0.875
train loss:  0.28064805269241333
train gradient:  0.08902178236692627
iteration : 12583
train acc:  0.8828125
train loss:  0.3246784508228302
train gradient:  0.14914694861882952
iteration : 12584
train acc:  0.8671875
train loss:  0.303396999835968
train gradient:  0.16697477512447204
iteration : 12585
train acc:  0.8671875
train loss:  0.25533145666122437
train gradient:  0.13817274378878916
iteration : 12586
train acc:  0.890625
train loss:  0.3048096299171448
train gradient:  0.1525503805955447
iteration : 12587
train acc:  0.8984375
train loss:  0.27016180753707886
train gradient:  0.1068673251084527
iteration : 12588
train acc:  0.7890625
train loss:  0.39700618386268616
train gradient:  0.19216552068688164
iteration : 12589
train acc:  0.8359375
train loss:  0.34862756729125977
train gradient:  0.15446289967857327
iteration : 12590
train acc:  0.8984375
train loss:  0.26533234119415283
train gradient:  0.14339464164703444
iteration : 12591
train acc:  0.890625
train loss:  0.29674994945526123
train gradient:  0.10441304906040283
iteration : 12592
train acc:  0.8515625
train loss:  0.3309677243232727
train gradient:  0.1779934140458417
iteration : 12593
train acc:  0.875
train loss:  0.2696898877620697
train gradient:  0.12247756634297349
iteration : 12594
train acc:  0.8359375
train loss:  0.30475080013275146
train gradient:  0.15468493851534615
iteration : 12595
train acc:  0.8359375
train loss:  0.32740873098373413
train gradient:  0.11176696779463677
iteration : 12596
train acc:  0.828125
train loss:  0.3793397843837738
train gradient:  0.17186962788332638
iteration : 12597
train acc:  0.890625
train loss:  0.34103119373321533
train gradient:  0.14608580218526745
iteration : 12598
train acc:  0.8203125
train loss:  0.4044593572616577
train gradient:  0.2169528315508501
iteration : 12599
train acc:  0.8515625
train loss:  0.2957254648208618
train gradient:  0.16431164918504787
iteration : 12600
train acc:  0.8203125
train loss:  0.336114764213562
train gradient:  0.21009074764612878
iteration : 12601
train acc:  0.875
train loss:  0.3027087450027466
train gradient:  0.17346947475719562
iteration : 12602
train acc:  0.8515625
train loss:  0.3080860674381256
train gradient:  0.14108841576332123
iteration : 12603
train acc:  0.875
train loss:  0.3612726628780365
train gradient:  0.17405655303565098
iteration : 12604
train acc:  0.859375
train loss:  0.2967979311943054
train gradient:  0.10191090000050655
iteration : 12605
train acc:  0.8984375
train loss:  0.27400821447372437
train gradient:  0.13964124087243462
iteration : 12606
train acc:  0.875
train loss:  0.22269472479820251
train gradient:  0.08926068953427264
iteration : 12607
train acc:  0.8359375
train loss:  0.3239286243915558
train gradient:  0.12378587453227369
iteration : 12608
train acc:  0.8828125
train loss:  0.31936904788017273
train gradient:  0.148561987269683
iteration : 12609
train acc:  0.828125
train loss:  0.3531317114830017
train gradient:  0.14086547496912377
iteration : 12610
train acc:  0.8359375
train loss:  0.35671466588974
train gradient:  0.18729596772019363
iteration : 12611
train acc:  0.84375
train loss:  0.35064005851745605
train gradient:  0.18188468734139418
iteration : 12612
train acc:  0.8515625
train loss:  0.3030255138874054
train gradient:  0.11975991549682852
iteration : 12613
train acc:  0.8515625
train loss:  0.3079570531845093
train gradient:  0.13079276621307323
iteration : 12614
train acc:  0.828125
train loss:  0.36319833993911743
train gradient:  0.16269853499165393
iteration : 12615
train acc:  0.8984375
train loss:  0.2365661859512329
train gradient:  0.09833818193056436
iteration : 12616
train acc:  0.859375
train loss:  0.3371083438396454
train gradient:  0.12861840342651257
iteration : 12617
train acc:  0.84375
train loss:  0.3580310642719269
train gradient:  0.14177283646895245
iteration : 12618
train acc:  0.8515625
train loss:  0.4189804792404175
train gradient:  0.22809490836800028
iteration : 12619
train acc:  0.8515625
train loss:  0.31313177943229675
train gradient:  0.16611608473284498
iteration : 12620
train acc:  0.8203125
train loss:  0.41272473335266113
train gradient:  0.25989759856297356
iteration : 12621
train acc:  0.8984375
train loss:  0.24867767095565796
train gradient:  0.10854295110319716
iteration : 12622
train acc:  0.921875
train loss:  0.20824065804481506
train gradient:  0.12029417805880233
iteration : 12623
train acc:  0.859375
train loss:  0.31728625297546387
train gradient:  0.14856497643853867
iteration : 12624
train acc:  0.8828125
train loss:  0.30669093132019043
train gradient:  0.1592870763841423
iteration : 12625
train acc:  0.8828125
train loss:  0.28102460503578186
train gradient:  0.10335956839898623
iteration : 12626
train acc:  0.8828125
train loss:  0.2543027102947235
train gradient:  0.11932730084953072
iteration : 12627
train acc:  0.8984375
train loss:  0.27373701333999634
train gradient:  0.15653755624748256
iteration : 12628
train acc:  0.9296875
train loss:  0.23941653966903687
train gradient:  0.09581804159388455
iteration : 12629
train acc:  0.8828125
train loss:  0.2611992359161377
train gradient:  0.08585621374844589
iteration : 12630
train acc:  0.8359375
train loss:  0.36083927750587463
train gradient:  0.18369106528729284
iteration : 12631
train acc:  0.8359375
train loss:  0.3284754753112793
train gradient:  0.14930135785816356
iteration : 12632
train acc:  0.875
train loss:  0.2660277485847473
train gradient:  0.11597798402851618
iteration : 12633
train acc:  0.8671875
train loss:  0.3600969910621643
train gradient:  0.13303630719095044
iteration : 12634
train acc:  0.859375
train loss:  0.3258311152458191
train gradient:  0.13887189167241887
iteration : 12635
train acc:  0.8046875
train loss:  0.40425121784210205
train gradient:  0.2565861925023029
iteration : 12636
train acc:  0.84375
train loss:  0.3741544783115387
train gradient:  0.22175016532113107
iteration : 12637
train acc:  0.8828125
train loss:  0.2963101863861084
train gradient:  0.13150064403346215
iteration : 12638
train acc:  0.8828125
train loss:  0.3520814776420593
train gradient:  0.1578745089700372
iteration : 12639
train acc:  0.8828125
train loss:  0.27349430322647095
train gradient:  0.1403151504879407
iteration : 12640
train acc:  0.8671875
train loss:  0.2982337176799774
train gradient:  0.10728425418510251
iteration : 12641
train acc:  0.8984375
train loss:  0.2280164659023285
train gradient:  0.06937406132635156
iteration : 12642
train acc:  0.875
train loss:  0.29314130544662476
train gradient:  0.15439869185331584
iteration : 12643
train acc:  0.84375
train loss:  0.36620640754699707
train gradient:  0.15390422199485426
iteration : 12644
train acc:  0.875
train loss:  0.3644285202026367
train gradient:  0.23426457161533198
iteration : 12645
train acc:  0.8828125
train loss:  0.32634037733078003
train gradient:  0.13296940671053245
iteration : 12646
train acc:  0.8359375
train loss:  0.36119091510772705
train gradient:  0.17567235550232366
iteration : 12647
train acc:  0.875
train loss:  0.24592575430870056
train gradient:  0.07041964301995261
iteration : 12648
train acc:  0.8515625
train loss:  0.3035867214202881
train gradient:  0.1033109399601863
iteration : 12649
train acc:  0.890625
train loss:  0.2470453381538391
train gradient:  0.11267546330564232
iteration : 12650
train acc:  0.8671875
train loss:  0.33667951822280884
train gradient:  0.24163157538165603
iteration : 12651
train acc:  0.796875
train loss:  0.4517403841018677
train gradient:  0.22344550097115112
iteration : 12652
train acc:  0.921875
train loss:  0.2759285867214203
train gradient:  0.14126822745878928
iteration : 12653
train acc:  0.8828125
train loss:  0.2351032942533493
train gradient:  0.13415638132252206
iteration : 12654
train acc:  0.890625
train loss:  0.26383721828460693
train gradient:  0.11706289759706687
iteration : 12655
train acc:  0.8828125
train loss:  0.3194016218185425
train gradient:  0.24679010824791375
iteration : 12656
train acc:  0.8203125
train loss:  0.4112708568572998
train gradient:  0.2003112214274425
iteration : 12657
train acc:  0.84375
train loss:  0.36730077862739563
train gradient:  0.13326141949138393
iteration : 12658
train acc:  0.890625
train loss:  0.2563363313674927
train gradient:  0.11357213157373718
iteration : 12659
train acc:  0.84375
train loss:  0.3038586378097534
train gradient:  0.11644820937791026
iteration : 12660
train acc:  0.859375
train loss:  0.22863635420799255
train gradient:  0.10486805969427747
iteration : 12661
train acc:  0.8515625
train loss:  0.31171655654907227
train gradient:  0.21858166418136699
iteration : 12662
train acc:  0.8671875
train loss:  0.3070385456085205
train gradient:  0.14620787149952452
iteration : 12663
train acc:  0.8671875
train loss:  0.3170658349990845
train gradient:  0.13661788491562613
iteration : 12664
train acc:  0.890625
train loss:  0.3704673647880554
train gradient:  0.22171812130417246
iteration : 12665
train acc:  0.9375
train loss:  0.2444324493408203
train gradient:  0.09488442595421823
iteration : 12666
train acc:  0.8984375
train loss:  0.2582392692565918
train gradient:  0.10838377139203227
iteration : 12667
train acc:  0.8828125
train loss:  0.31467175483703613
train gradient:  0.10502551514336576
iteration : 12668
train acc:  0.8984375
train loss:  0.23135285079479218
train gradient:  0.10467878636946944
iteration : 12669
train acc:  0.890625
train loss:  0.25747808814048767
train gradient:  0.09254243278483934
iteration : 12670
train acc:  0.875
train loss:  0.3429170846939087
train gradient:  0.314080783426059
iteration : 12671
train acc:  0.9296875
train loss:  0.21648237109184265
train gradient:  0.12294002575626357
iteration : 12672
train acc:  0.9140625
train loss:  0.21882182359695435
train gradient:  0.08170817555986089
iteration : 12673
train acc:  0.8828125
train loss:  0.32223886251449585
train gradient:  0.13040843762366566
iteration : 12674
train acc:  0.828125
train loss:  0.4427148103713989
train gradient:  0.25403415127418966
iteration : 12675
train acc:  0.8828125
train loss:  0.30056697130203247
train gradient:  0.07689668291101945
iteration : 12676
train acc:  0.859375
train loss:  0.3169596791267395
train gradient:  0.18780646538880197
iteration : 12677
train acc:  0.828125
train loss:  0.3285343050956726
train gradient:  0.1544844207278371
iteration : 12678
train acc:  0.875
train loss:  0.2567492723464966
train gradient:  0.135489561895091
iteration : 12679
train acc:  0.859375
train loss:  0.28325703740119934
train gradient:  0.164593210983415
iteration : 12680
train acc:  0.8671875
train loss:  0.2716760039329529
train gradient:  0.10866096578898465
iteration : 12681
train acc:  0.8984375
train loss:  0.22963400185108185
train gradient:  0.08528211553246427
iteration : 12682
train acc:  0.8515625
train loss:  0.3111630380153656
train gradient:  0.14464272594964878
iteration : 12683
train acc:  0.796875
train loss:  0.37930023670196533
train gradient:  0.1948778778180662
iteration : 12684
train acc:  0.8515625
train loss:  0.3385395109653473
train gradient:  0.14549794300123148
iteration : 12685
train acc:  0.8671875
train loss:  0.32825136184692383
train gradient:  0.20701501615605833
iteration : 12686
train acc:  0.8671875
train loss:  0.2472815215587616
train gradient:  0.10588157587744529
iteration : 12687
train acc:  0.859375
train loss:  0.2898518443107605
train gradient:  0.11220143914095575
iteration : 12688
train acc:  0.890625
train loss:  0.268568754196167
train gradient:  0.097447693146072
iteration : 12689
train acc:  0.84375
train loss:  0.33337974548339844
train gradient:  0.16702021181454008
iteration : 12690
train acc:  0.8125
train loss:  0.4300084412097931
train gradient:  0.28618570552319655
iteration : 12691
train acc:  0.8359375
train loss:  0.34910961985588074
train gradient:  0.14733764973642288
iteration : 12692
train acc:  0.875
train loss:  0.2751650810241699
train gradient:  0.11971334500205658
iteration : 12693
train acc:  0.875
train loss:  0.2625773549079895
train gradient:  0.11090880791034914
iteration : 12694
train acc:  0.828125
train loss:  0.4087662398815155
train gradient:  0.2112737732002432
iteration : 12695
train acc:  0.9375
train loss:  0.22455279529094696
train gradient:  0.08879186638696575
iteration : 12696
train acc:  0.8515625
train loss:  0.3391781449317932
train gradient:  0.4217702528391017
iteration : 12697
train acc:  0.890625
train loss:  0.313742995262146
train gradient:  0.14343239259831353
iteration : 12698
train acc:  0.828125
train loss:  0.40587708353996277
train gradient:  0.25784517844685795
iteration : 12699
train acc:  0.8125
train loss:  0.4519292712211609
train gradient:  0.4013855418441761
iteration : 12700
train acc:  0.8984375
train loss:  0.24617686867713928
train gradient:  0.11864408000019293
iteration : 12701
train acc:  0.9375
train loss:  0.24321617186069489
train gradient:  0.11371452741038893
iteration : 12702
train acc:  0.8671875
train loss:  0.3428335189819336
train gradient:  0.156704070258718
iteration : 12703
train acc:  0.875
train loss:  0.2557159960269928
train gradient:  0.10524968870784214
iteration : 12704
train acc:  0.8828125
train loss:  0.26068204641342163
train gradient:  0.12060156694778841
iteration : 12705
train acc:  0.8671875
train loss:  0.288880318403244
train gradient:  0.10962084118349232
iteration : 12706
train acc:  0.875
train loss:  0.2789781093597412
train gradient:  0.1169583603109207
iteration : 12707
train acc:  0.796875
train loss:  0.4368780255317688
train gradient:  0.22955323524171964
iteration : 12708
train acc:  0.8359375
train loss:  0.3184783160686493
train gradient:  0.13053277811679298
iteration : 12709
train acc:  0.828125
train loss:  0.3549531400203705
train gradient:  0.1486396953486115
iteration : 12710
train acc:  0.828125
train loss:  0.40487322211265564
train gradient:  0.21004866530217498
iteration : 12711
train acc:  0.8203125
train loss:  0.40832293033599854
train gradient:  0.21641357336114253
iteration : 12712
train acc:  0.890625
train loss:  0.26029229164123535
train gradient:  0.10368753099686419
iteration : 12713
train acc:  0.828125
train loss:  0.30808329582214355
train gradient:  0.11931224716494772
iteration : 12714
train acc:  0.9140625
train loss:  0.23929738998413086
train gradient:  0.09690793205929216
iteration : 12715
train acc:  0.9140625
train loss:  0.27240145206451416
train gradient:  0.1252685085497236
iteration : 12716
train acc:  0.859375
train loss:  0.3039690852165222
train gradient:  0.15828213709001077
iteration : 12717
train acc:  0.8671875
train loss:  0.3315776586532593
train gradient:  0.13615306382679138
iteration : 12718
train acc:  0.9140625
train loss:  0.23093795776367188
train gradient:  0.08792838093074179
iteration : 12719
train acc:  0.796875
train loss:  0.4021429121494293
train gradient:  0.21433256596627567
iteration : 12720
train acc:  0.921875
train loss:  0.23576778173446655
train gradient:  0.09323757559287545
iteration : 12721
train acc:  0.828125
train loss:  0.34848856925964355
train gradient:  0.15477052413362047
iteration : 12722
train acc:  0.8828125
train loss:  0.317440003156662
train gradient:  0.17225372358250568
iteration : 12723
train acc:  0.8359375
train loss:  0.34507739543914795
train gradient:  0.15308353958252185
iteration : 12724
train acc:  0.9140625
train loss:  0.2577418088912964
train gradient:  0.12150515055821376
iteration : 12725
train acc:  0.8828125
train loss:  0.3259624242782593
train gradient:  0.338884468443053
iteration : 12726
train acc:  0.9140625
train loss:  0.2587404251098633
train gradient:  0.11883623584788387
iteration : 12727
train acc:  0.8828125
train loss:  0.28865736722946167
train gradient:  0.15457810215996326
iteration : 12728
train acc:  0.8359375
train loss:  0.3504406213760376
train gradient:  0.1597192162224788
iteration : 12729
train acc:  0.8515625
train loss:  0.3770599961280823
train gradient:  0.20166226673355314
iteration : 12730
train acc:  0.8515625
train loss:  0.2921803593635559
train gradient:  0.13702267185312567
iteration : 12731
train acc:  0.84375
train loss:  0.30808305740356445
train gradient:  0.164919836532267
iteration : 12732
train acc:  0.8671875
train loss:  0.2728275954723358
train gradient:  0.2276289102813902
iteration : 12733
train acc:  0.890625
train loss:  0.2989983558654785
train gradient:  0.10372025547060763
iteration : 12734
train acc:  0.8515625
train loss:  0.33121925592422485
train gradient:  0.11663251188081274
iteration : 12735
train acc:  0.84375
train loss:  0.337013840675354
train gradient:  0.21294337424759402
iteration : 12736
train acc:  0.90625
train loss:  0.2855164408683777
train gradient:  0.11915971408935984
iteration : 12737
train acc:  0.828125
train loss:  0.42058026790618896
train gradient:  0.23535783566961416
iteration : 12738
train acc:  0.8515625
train loss:  0.3178882598876953
train gradient:  0.15691361230953182
iteration : 12739
train acc:  0.8203125
train loss:  0.4030735194683075
train gradient:  0.1984675893961491
iteration : 12740
train acc:  0.875
train loss:  0.3096107244491577
train gradient:  0.16770504647903609
iteration : 12741
train acc:  0.875
train loss:  0.3138706684112549
train gradient:  0.1813161848569013
iteration : 12742
train acc:  0.8671875
train loss:  0.31522276997566223
train gradient:  0.15292660932750063
iteration : 12743
train acc:  0.859375
train loss:  0.29281556606292725
train gradient:  0.13351906856126225
iteration : 12744
train acc:  0.890625
train loss:  0.2982831597328186
train gradient:  0.13335563030725608
iteration : 12745
train acc:  0.890625
train loss:  0.3154534697532654
train gradient:  0.12371610847144067
iteration : 12746
train acc:  0.8828125
train loss:  0.2542804181575775
train gradient:  0.11997716843839495
iteration : 12747
train acc:  0.875
train loss:  0.3155040144920349
train gradient:  0.16262641894209245
iteration : 12748
train acc:  0.90625
train loss:  0.2122841775417328
train gradient:  0.09387997266038237
iteration : 12749
train acc:  0.8671875
train loss:  0.28795576095581055
train gradient:  0.16072574943004908
iteration : 12750
train acc:  0.8515625
train loss:  0.361232727766037
train gradient:  0.16987234456811506
iteration : 12751
train acc:  0.8671875
train loss:  0.3211722671985626
train gradient:  0.16028072449258618
iteration : 12752
train acc:  0.8203125
train loss:  0.3533569574356079
train gradient:  0.14413845704557698
iteration : 12753
train acc:  0.84375
train loss:  0.3787437677383423
train gradient:  0.1559968416067895
iteration : 12754
train acc:  0.8828125
train loss:  0.27032235264778137
train gradient:  0.10263108590346003
iteration : 12755
train acc:  0.875
train loss:  0.33271121978759766
train gradient:  0.11753117183604533
iteration : 12756
train acc:  0.828125
train loss:  0.3526473641395569
train gradient:  0.17021248582570186
iteration : 12757
train acc:  0.8828125
train loss:  0.23473772406578064
train gradient:  0.09683549511883754
iteration : 12758
train acc:  0.8515625
train loss:  0.3281557559967041
train gradient:  0.16342400536886645
iteration : 12759
train acc:  0.8828125
train loss:  0.29588615894317627
train gradient:  0.17407657101391333
iteration : 12760
train acc:  0.875
train loss:  0.32561713457107544
train gradient:  0.17560540245134787
iteration : 12761
train acc:  0.875
train loss:  0.2924538254737854
train gradient:  0.11535650088982698
iteration : 12762
train acc:  0.8984375
train loss:  0.27841344475746155
train gradient:  0.11029754360573858
iteration : 12763
train acc:  0.890625
train loss:  0.29167860746383667
train gradient:  0.1597002219573209
iteration : 12764
train acc:  0.8984375
train loss:  0.2803356349468231
train gradient:  0.14837770315772414
iteration : 12765
train acc:  0.8984375
train loss:  0.291836142539978
train gradient:  0.0804246115792299
iteration : 12766
train acc:  0.8671875
train loss:  0.3227986693382263
train gradient:  0.1378679890548955
iteration : 12767
train acc:  0.8515625
train loss:  0.29587846994400024
train gradient:  0.17965296304422557
iteration : 12768
train acc:  0.90625
train loss:  0.2924339473247528
train gradient:  0.11281279685272938
iteration : 12769
train acc:  0.796875
train loss:  0.380747526884079
train gradient:  0.17805450214061647
iteration : 12770
train acc:  0.8203125
train loss:  0.3807119131088257
train gradient:  0.17521882743862988
iteration : 12771
train acc:  0.8125
train loss:  0.4022103548049927
train gradient:  0.3542361138225021
iteration : 12772
train acc:  0.8515625
train loss:  0.3097376525402069
train gradient:  0.11737736425723799
iteration : 12773
train acc:  0.875
train loss:  0.31391042470932007
train gradient:  0.16610636844654705
iteration : 12774
train acc:  0.875
train loss:  0.3116397261619568
train gradient:  0.2472745575756809
iteration : 12775
train acc:  0.84375
train loss:  0.3674396574497223
train gradient:  0.16476351770999395
iteration : 12776
train acc:  0.875
train loss:  0.289737731218338
train gradient:  0.13247152479214674
iteration : 12777
train acc:  0.8828125
train loss:  0.32153671979904175
train gradient:  0.1545637946077929
iteration : 12778
train acc:  0.90625
train loss:  0.24831797182559967
train gradient:  0.08534790951193132
iteration : 12779
train acc:  0.875
train loss:  0.32648974657058716
train gradient:  0.17312562326124414
iteration : 12780
train acc:  0.890625
train loss:  0.29101961851119995
train gradient:  0.11121155648288725
iteration : 12781
train acc:  0.8671875
train loss:  0.285275936126709
train gradient:  0.0913717868388627
iteration : 12782
train acc:  0.859375
train loss:  0.275313138961792
train gradient:  0.10859862655953754
iteration : 12783
train acc:  0.8828125
train loss:  0.2951628565788269
train gradient:  0.14192165794738973
iteration : 12784
train acc:  0.8828125
train loss:  0.26878345012664795
train gradient:  0.09475934115594235
iteration : 12785
train acc:  0.8828125
train loss:  0.25686877965927124
train gradient:  0.08071768994349426
iteration : 12786
train acc:  0.8828125
train loss:  0.28079891204833984
train gradient:  0.11200423989607168
iteration : 12787
train acc:  0.8515625
train loss:  0.3535781502723694
train gradient:  0.17209848812615425
iteration : 12788
train acc:  0.890625
train loss:  0.2525215744972229
train gradient:  0.1060654707235734
iteration : 12789
train acc:  0.859375
train loss:  0.25874555110931396
train gradient:  0.10669020967592546
iteration : 12790
train acc:  0.8984375
train loss:  0.23465710878372192
train gradient:  0.08634486959128272
iteration : 12791
train acc:  0.8046875
train loss:  0.3757404685020447
train gradient:  0.2480971961980711
iteration : 12792
train acc:  0.828125
train loss:  0.3471180498600006
train gradient:  0.19722612682244484
iteration : 12793
train acc:  0.890625
train loss:  0.26240772008895874
train gradient:  0.12736152899056585
iteration : 12794
train acc:  0.875
train loss:  0.30492955446243286
train gradient:  0.13846776015025042
iteration : 12795
train acc:  0.8671875
train loss:  0.32708650827407837
train gradient:  0.12802953507530818
iteration : 12796
train acc:  0.8828125
train loss:  0.3047042489051819
train gradient:  0.1339992340801104
iteration : 12797
train acc:  0.8984375
train loss:  0.281025767326355
train gradient:  0.12819099896988345
iteration : 12798
train acc:  0.875
train loss:  0.35104280710220337
train gradient:  0.18313173008420033
iteration : 12799
train acc:  0.890625
train loss:  0.2653762400150299
train gradient:  0.09119195251614373
iteration : 12800
train acc:  0.8828125
train loss:  0.3046790361404419
train gradient:  0.1704930782202229
iteration : 12801
train acc:  0.8203125
train loss:  0.3985382914543152
train gradient:  0.19745788698135391
iteration : 12802
train acc:  0.890625
train loss:  0.2741440534591675
train gradient:  0.09515433210939063
iteration : 12803
train acc:  0.84375
train loss:  0.35447001457214355
train gradient:  0.16975336294840684
iteration : 12804
train acc:  0.8671875
train loss:  0.3462751507759094
train gradient:  0.1546319331670666
iteration : 12805
train acc:  0.875
train loss:  0.2625601887702942
train gradient:  0.11125402216199788
iteration : 12806
train acc:  0.890625
train loss:  0.26322662830352783
train gradient:  0.10064624454054655
iteration : 12807
train acc:  0.84375
train loss:  0.32104796171188354
train gradient:  0.1045137025159553
iteration : 12808
train acc:  0.8828125
train loss:  0.2536928653717041
train gradient:  0.11110338500842362
iteration : 12809
train acc:  0.8828125
train loss:  0.2609366178512573
train gradient:  0.21199402504101977
iteration : 12810
train acc:  0.84375
train loss:  0.3215712308883667
train gradient:  0.14830356587170976
iteration : 12811
train acc:  0.828125
train loss:  0.37675657868385315
train gradient:  0.15046284718088654
iteration : 12812
train acc:  0.8515625
train loss:  0.339868426322937
train gradient:  0.17054341843682613
iteration : 12813
train acc:  0.875
train loss:  0.2912222445011139
train gradient:  0.17462926943217416
iteration : 12814
train acc:  0.8671875
train loss:  0.37039297819137573
train gradient:  0.2906731999685413
iteration : 12815
train acc:  0.8515625
train loss:  0.3415197730064392
train gradient:  0.1719034219429747
iteration : 12816
train acc:  0.8359375
train loss:  0.38958439230918884
train gradient:  0.17796933392383785
iteration : 12817
train acc:  0.8984375
train loss:  0.2798290252685547
train gradient:  0.14694399056429497
iteration : 12818
train acc:  0.8984375
train loss:  0.2732667624950409
train gradient:  0.18057693935447927
iteration : 12819
train acc:  0.8671875
train loss:  0.3232367932796478
train gradient:  0.13016714719352968
iteration : 12820
train acc:  0.8671875
train loss:  0.3165780305862427
train gradient:  0.1049989211008653
iteration : 12821
train acc:  0.890625
train loss:  0.25633734464645386
train gradient:  0.11521537235054645
iteration : 12822
train acc:  0.875
train loss:  0.29201316833496094
train gradient:  0.12137538461622989
iteration : 12823
train acc:  0.8203125
train loss:  0.37380343675613403
train gradient:  0.18313245777195156
iteration : 12824
train acc:  0.828125
train loss:  0.3650447130203247
train gradient:  0.2213911654737275
iteration : 12825
train acc:  0.875
train loss:  0.266075074672699
train gradient:  0.10333172871530039
iteration : 12826
train acc:  0.8984375
train loss:  0.27828744053840637
train gradient:  0.1287895551674803
iteration : 12827
train acc:  0.90625
train loss:  0.24642352759838104
train gradient:  0.08603838093453459
iteration : 12828
train acc:  0.8359375
train loss:  0.38345372676849365
train gradient:  0.1945609125592875
iteration : 12829
train acc:  0.90625
train loss:  0.25355690717697144
train gradient:  0.13041868429976916
iteration : 12830
train acc:  0.859375
train loss:  0.36534684896469116
train gradient:  0.19081553448531888
iteration : 12831
train acc:  0.8984375
train loss:  0.23694267868995667
train gradient:  0.14252442335730636
iteration : 12832
train acc:  0.8359375
train loss:  0.3524550199508667
train gradient:  0.128674785634521
iteration : 12833
train acc:  0.875
train loss:  0.2759742736816406
train gradient:  0.1313775988796726
iteration : 12834
train acc:  0.921875
train loss:  0.23455429077148438
train gradient:  0.09042261037768833
iteration : 12835
train acc:  0.8515625
train loss:  0.3910500109195709
train gradient:  0.1685954544622152
iteration : 12836
train acc:  0.8515625
train loss:  0.29018110036849976
train gradient:  0.1815882101791684
iteration : 12837
train acc:  0.875
train loss:  0.353326678276062
train gradient:  0.2304064598232334
iteration : 12838
train acc:  0.8828125
train loss:  0.29399603605270386
train gradient:  0.11154854392709329
iteration : 12839
train acc:  0.90625
train loss:  0.30332523584365845
train gradient:  0.12765101167731668
iteration : 12840
train acc:  0.9140625
train loss:  0.2262314409017563
train gradient:  0.07748818965004813
iteration : 12841
train acc:  0.84375
train loss:  0.36185187101364136
train gradient:  0.1685051727207728
iteration : 12842
train acc:  0.859375
train loss:  0.34665119647979736
train gradient:  0.14255542653732398
iteration : 12843
train acc:  0.8203125
train loss:  0.38234943151474
train gradient:  0.1685024691031608
iteration : 12844
train acc:  0.84375
train loss:  0.4122064709663391
train gradient:  0.2433782607122634
iteration : 12845
train acc:  0.8125
train loss:  0.3867707848548889
train gradient:  0.1930605448305803
iteration : 12846
train acc:  0.875
train loss:  0.319846510887146
train gradient:  0.14281177804241524
iteration : 12847
train acc:  0.8671875
train loss:  0.35058557987213135
train gradient:  0.17881131643177522
iteration : 12848
train acc:  0.84375
train loss:  0.31669503450393677
train gradient:  0.1771830849615006
iteration : 12849
train acc:  0.8984375
train loss:  0.27400919795036316
train gradient:  0.13650263535122903
iteration : 12850
train acc:  0.8203125
train loss:  0.37804707884788513
train gradient:  0.1994301516983277
iteration : 12851
train acc:  0.84375
train loss:  0.33127129077911377
train gradient:  0.14689029466636108
iteration : 12852
train acc:  0.890625
train loss:  0.2661800682544708
train gradient:  0.11105978387038919
iteration : 12853
train acc:  0.8828125
train loss:  0.32434892654418945
train gradient:  0.21033018056210456
iteration : 12854
train acc:  0.859375
train loss:  0.32754287123680115
train gradient:  0.1113501318640385
iteration : 12855
train acc:  0.890625
train loss:  0.32215726375579834
train gradient:  0.14437926750746766
iteration : 12856
train acc:  0.8046875
train loss:  0.36046019196510315
train gradient:  0.2232331763042436
iteration : 12857
train acc:  0.875
train loss:  0.3824142515659332
train gradient:  0.2198642901186832
iteration : 12858
train acc:  0.84375
train loss:  0.324327677488327
train gradient:  0.18552753624673912
iteration : 12859
train acc:  0.84375
train loss:  0.32515454292297363
train gradient:  0.10590524436962157
iteration : 12860
train acc:  0.890625
train loss:  0.2626173198223114
train gradient:  0.14021515159759068
iteration : 12861
train acc:  0.8828125
train loss:  0.27463266253471375
train gradient:  0.12517691601277392
iteration : 12862
train acc:  0.8515625
train loss:  0.34762275218963623
train gradient:  0.12297087422838443
iteration : 12863
train acc:  0.8984375
train loss:  0.22761841118335724
train gradient:  0.09875058945278598
iteration : 12864
train acc:  0.859375
train loss:  0.29587098956108093
train gradient:  0.1476811591556835
iteration : 12865
train acc:  0.8515625
train loss:  0.34957727789878845
train gradient:  0.11297236060801182
iteration : 12866
train acc:  0.8046875
train loss:  0.3738812208175659
train gradient:  0.22335147603648808
iteration : 12867
train acc:  0.828125
train loss:  0.3980947732925415
train gradient:  0.2610349816455244
iteration : 12868
train acc:  0.890625
train loss:  0.2658240795135498
train gradient:  0.09566515639845376
iteration : 12869
train acc:  0.875
train loss:  0.29845553636550903
train gradient:  0.13565912103234753
iteration : 12870
train acc:  0.828125
train loss:  0.375729501247406
train gradient:  0.16548045162279287
iteration : 12871
train acc:  0.9140625
train loss:  0.28468120098114014
train gradient:  0.1359568932107314
iteration : 12872
train acc:  0.84375
train loss:  0.33551618456840515
train gradient:  0.21990571747934112
iteration : 12873
train acc:  0.8984375
train loss:  0.2670614421367645
train gradient:  0.09744862929971611
iteration : 12874
train acc:  0.8515625
train loss:  0.3112473785877228
train gradient:  0.18045988462469795
iteration : 12875
train acc:  0.8515625
train loss:  0.4067652225494385
train gradient:  0.27181042792698673
iteration : 12876
train acc:  0.8359375
train loss:  0.3711971640586853
train gradient:  0.17415467659579403
iteration : 12877
train acc:  0.8671875
train loss:  0.2854096591472626
train gradient:  0.1250412676552121
iteration : 12878
train acc:  0.8984375
train loss:  0.2580154240131378
train gradient:  0.10523809985689725
iteration : 12879
train acc:  0.8125
train loss:  0.37649571895599365
train gradient:  0.18219167349376475
iteration : 12880
train acc:  0.8359375
train loss:  0.386115700006485
train gradient:  0.19795619721667554
iteration : 12881
train acc:  0.8984375
train loss:  0.2413432002067566
train gradient:  0.0904631326500685
iteration : 12882
train acc:  0.8515625
train loss:  0.3237380087375641
train gradient:  0.1331313575005331
iteration : 12883
train acc:  0.8671875
train loss:  0.3269995450973511
train gradient:  0.12726717212237498
iteration : 12884
train acc:  0.828125
train loss:  0.30865874886512756
train gradient:  0.10536033285525642
iteration : 12885
train acc:  0.859375
train loss:  0.29635000228881836
train gradient:  0.14624765978654689
iteration : 12886
train acc:  0.84375
train loss:  0.3417304754257202
train gradient:  0.214721756206604
iteration : 12887
train acc:  0.796875
train loss:  0.404121458530426
train gradient:  0.19883539260699498
iteration : 12888
train acc:  0.828125
train loss:  0.3480014503002167
train gradient:  0.14501593002449584
iteration : 12889
train acc:  0.921875
train loss:  0.2441144585609436
train gradient:  0.12176291994856579
iteration : 12890
train acc:  0.8515625
train loss:  0.34666764736175537
train gradient:  0.142336437796234
iteration : 12891
train acc:  0.890625
train loss:  0.26660647988319397
train gradient:  0.10185879897950323
iteration : 12892
train acc:  0.921875
train loss:  0.2889474630355835
train gradient:  0.10986537041158305
iteration : 12893
train acc:  0.8828125
train loss:  0.30395159125328064
train gradient:  0.11760542222673785
iteration : 12894
train acc:  0.875
train loss:  0.2887713313102722
train gradient:  0.07502738649161673
iteration : 12895
train acc:  0.8359375
train loss:  0.37604883313179016
train gradient:  0.16100501718751692
iteration : 12896
train acc:  0.8671875
train loss:  0.30321943759918213
train gradient:  0.19195177269992286
iteration : 12897
train acc:  0.8984375
train loss:  0.28646230697631836
train gradient:  0.11834641249047632
iteration : 12898
train acc:  0.828125
train loss:  0.3542574644088745
train gradient:  0.22426435273081663
iteration : 12899
train acc:  0.8828125
train loss:  0.29846158623695374
train gradient:  0.10563986006731838
iteration : 12900
train acc:  0.84375
train loss:  0.33298540115356445
train gradient:  0.12536749268028577
iteration : 12901
train acc:  0.9140625
train loss:  0.21818283200263977
train gradient:  0.09740341513689857
iteration : 12902
train acc:  0.9609375
train loss:  0.24594691395759583
train gradient:  0.08294746400190296
iteration : 12903
train acc:  0.9140625
train loss:  0.31025072932243347
train gradient:  0.16445022391335287
iteration : 12904
train acc:  0.828125
train loss:  0.4010220766067505
train gradient:  0.17471748846513652
iteration : 12905
train acc:  0.859375
train loss:  0.26966020464897156
train gradient:  0.10349543407847443
iteration : 12906
train acc:  0.828125
train loss:  0.38774728775024414
train gradient:  0.20856171017922026
iteration : 12907
train acc:  0.8515625
train loss:  0.2992175817489624
train gradient:  0.1125439283477631
iteration : 12908
train acc:  0.7734375
train loss:  0.4166046380996704
train gradient:  0.21519535249619595
iteration : 12909
train acc:  0.875
train loss:  0.34301215410232544
train gradient:  0.11641539491723528
iteration : 12910
train acc:  0.859375
train loss:  0.31574946641921997
train gradient:  0.13838280508344092
iteration : 12911
train acc:  0.90625
train loss:  0.24086737632751465
train gradient:  0.10245048090814318
iteration : 12912
train acc:  0.859375
train loss:  0.3412053883075714
train gradient:  0.15019443179626824
iteration : 12913
train acc:  0.84375
train loss:  0.32159584760665894
train gradient:  0.17795189046540882
iteration : 12914
train acc:  0.828125
train loss:  0.3662447929382324
train gradient:  0.19993185085539678
iteration : 12915
train acc:  0.8828125
train loss:  0.2905075252056122
train gradient:  0.09631956666290943
iteration : 12916
train acc:  0.8515625
train loss:  0.33763617277145386
train gradient:  0.1450716356545942
iteration : 12917
train acc:  0.828125
train loss:  0.36263492703437805
train gradient:  0.13409749645536345
iteration : 12918
train acc:  0.859375
train loss:  0.3411276340484619
train gradient:  0.15071039678528347
iteration : 12919
train acc:  0.8671875
train loss:  0.3366779088973999
train gradient:  0.14473294463651865
iteration : 12920
train acc:  0.8515625
train loss:  0.2899404764175415
train gradient:  0.1299049777236141
iteration : 12921
train acc:  0.84375
train loss:  0.43339812755584717
train gradient:  0.326668367805652
iteration : 12922
train acc:  0.796875
train loss:  0.4068394601345062
train gradient:  0.20426401932854782
iteration : 12923
train acc:  0.890625
train loss:  0.28549033403396606
train gradient:  0.09164526462931703
iteration : 12924
train acc:  0.8046875
train loss:  0.47443926334381104
train gradient:  0.2819914402572484
iteration : 12925
train acc:  0.8984375
train loss:  0.3038022816181183
train gradient:  0.13381512660665096
iteration : 12926
train acc:  0.8671875
train loss:  0.3099978566169739
train gradient:  0.1051968656520403
iteration : 12927
train acc:  0.8671875
train loss:  0.2758387625217438
train gradient:  0.10104161841284517
iteration : 12928
train acc:  0.8671875
train loss:  0.29951244592666626
train gradient:  0.14948205094695977
iteration : 12929
train acc:  0.828125
train loss:  0.323813259601593
train gradient:  0.11638911633534112
iteration : 12930
train acc:  0.890625
train loss:  0.28273019194602966
train gradient:  0.1390252651993898
iteration : 12931
train acc:  0.84375
train loss:  0.31620556116104126
train gradient:  0.12932047966028606
iteration : 12932
train acc:  0.8828125
train loss:  0.27914732694625854
train gradient:  0.21141129858354127
iteration : 12933
train acc:  0.9140625
train loss:  0.2786230444908142
train gradient:  0.09569647929302944
iteration : 12934
train acc:  0.875
train loss:  0.3038070797920227
train gradient:  0.10551586919017418
iteration : 12935
train acc:  0.84375
train loss:  0.3328530192375183
train gradient:  0.1156588750571854
iteration : 12936
train acc:  0.8515625
train loss:  0.30978524684906006
train gradient:  0.11378657075245675
iteration : 12937
train acc:  0.890625
train loss:  0.2511160373687744
train gradient:  0.09883009940849323
iteration : 12938
train acc:  0.8984375
train loss:  0.22726796567440033
train gradient:  0.08657839429057153
iteration : 12939
train acc:  0.8671875
train loss:  0.28763341903686523
train gradient:  0.10898912052346343
iteration : 12940
train acc:  0.84375
train loss:  0.36342522501945496
train gradient:  0.16597323274935688
iteration : 12941
train acc:  0.8828125
train loss:  0.29082417488098145
train gradient:  0.09797763246672223
iteration : 12942
train acc:  0.8984375
train loss:  0.2908889651298523
train gradient:  0.11824164563459381
iteration : 12943
train acc:  0.828125
train loss:  0.36834481358528137
train gradient:  0.23520134762472344
iteration : 12944
train acc:  0.828125
train loss:  0.3771253824234009
train gradient:  0.14408945063774475
iteration : 12945
train acc:  0.8125
train loss:  0.3789394497871399
train gradient:  0.2204687473367261
iteration : 12946
train acc:  0.8828125
train loss:  0.313347727060318
train gradient:  0.1525007904373779
iteration : 12947
train acc:  0.8984375
train loss:  0.2697920799255371
train gradient:  0.10981900875793027
iteration : 12948
train acc:  0.8515625
train loss:  0.3938155770301819
train gradient:  0.16316091993273601
iteration : 12949
train acc:  0.8671875
train loss:  0.2977082133293152
train gradient:  0.12611240631277987
iteration : 12950
train acc:  0.8359375
train loss:  0.33328068256378174
train gradient:  0.12085270873127461
iteration : 12951
train acc:  0.8515625
train loss:  0.3021531105041504
train gradient:  0.12584464295833633
iteration : 12952
train acc:  0.875
train loss:  0.30229681730270386
train gradient:  0.2621206343918345
iteration : 12953
train acc:  0.859375
train loss:  0.3187454044818878
train gradient:  0.16812947931802807
iteration : 12954
train acc:  0.84375
train loss:  0.34405988454818726
train gradient:  0.14014916634656657
iteration : 12955
train acc:  0.8984375
train loss:  0.287384569644928
train gradient:  0.08346830659028116
iteration : 12956
train acc:  0.875
train loss:  0.2998781204223633
train gradient:  0.10630634599529695
iteration : 12957
train acc:  0.890625
train loss:  0.24701040983200073
train gradient:  0.0792192593441566
iteration : 12958
train acc:  0.8828125
train loss:  0.2959776520729065
train gradient:  0.12677801884934023
iteration : 12959
train acc:  0.890625
train loss:  0.30270540714263916
train gradient:  0.11640748218870135
iteration : 12960
train acc:  0.828125
train loss:  0.3412548899650574
train gradient:  0.13421853813590903
iteration : 12961
train acc:  0.859375
train loss:  0.34896087646484375
train gradient:  0.11676793021859416
iteration : 12962
train acc:  0.8671875
train loss:  0.300619900226593
train gradient:  0.12527500186435042
iteration : 12963
train acc:  0.8828125
train loss:  0.23312920331954956
train gradient:  0.10510496729722171
iteration : 12964
train acc:  0.890625
train loss:  0.3105650544166565
train gradient:  0.16160937301164607
iteration : 12965
train acc:  0.8515625
train loss:  0.3463798761367798
train gradient:  0.17103095679491037
iteration : 12966
train acc:  0.9140625
train loss:  0.22119855880737305
train gradient:  0.0866578933732207
iteration : 12967
train acc:  0.875
train loss:  0.25751420855522156
train gradient:  0.10448991142749564
iteration : 12968
train acc:  0.8515625
train loss:  0.3397037982940674
train gradient:  0.12278864635127373
iteration : 12969
train acc:  0.890625
train loss:  0.29179561138153076
train gradient:  0.09692382181290143
iteration : 12970
train acc:  0.8984375
train loss:  0.2537454664707184
train gradient:  0.11843550304079613
iteration : 12971
train acc:  0.9453125
train loss:  0.20323330163955688
train gradient:  0.06624416778621
iteration : 12972
train acc:  0.890625
train loss:  0.2815678119659424
train gradient:  0.1150403977261072
iteration : 12973
train acc:  0.859375
train loss:  0.31597986817359924
train gradient:  0.17396534818031978
iteration : 12974
train acc:  0.84375
train loss:  0.38096630573272705
train gradient:  0.18800326488375926
iteration : 12975
train acc:  0.8671875
train loss:  0.2897569239139557
train gradient:  0.12098324755625356
iteration : 12976
train acc:  0.8125
train loss:  0.41909825801849365
train gradient:  0.2641081937985182
iteration : 12977
train acc:  0.9140625
train loss:  0.2631111145019531
train gradient:  0.1306601782645242
iteration : 12978
train acc:  0.859375
train loss:  0.3201250433921814
train gradient:  0.16559013712513004
iteration : 12979
train acc:  0.8515625
train loss:  0.3384389579296112
train gradient:  0.1619567753904681
iteration : 12980
train acc:  0.875
train loss:  0.3101288378238678
train gradient:  0.16103189721298072
iteration : 12981
train acc:  0.7890625
train loss:  0.3910793662071228
train gradient:  0.208316777241912
iteration : 12982
train acc:  0.84375
train loss:  0.33231601119041443
train gradient:  0.13522057326357562
iteration : 12983
train acc:  0.875
train loss:  0.2951442003250122
train gradient:  0.10300932247328472
iteration : 12984
train acc:  0.9140625
train loss:  0.2541426122188568
train gradient:  0.08522154671972537
iteration : 12985
train acc:  0.8125
train loss:  0.370302677154541
train gradient:  0.2008622877127623
iteration : 12986
train acc:  0.890625
train loss:  0.2723334729671478
train gradient:  0.09295419468455324
iteration : 12987
train acc:  0.8671875
train loss:  0.2874053716659546
train gradient:  0.13239489300271443
iteration : 12988
train acc:  0.8515625
train loss:  0.33715516328811646
train gradient:  0.30643643798770703
iteration : 12989
train acc:  0.8515625
train loss:  0.31067171692848206
train gradient:  0.0931457131636259
iteration : 12990
train acc:  0.8515625
train loss:  0.2822760343551636
train gradient:  0.10063988183588563
iteration : 12991
train acc:  0.8671875
train loss:  0.31267470121383667
train gradient:  0.1286703034527518
iteration : 12992
train acc:  0.84375
train loss:  0.3597923815250397
train gradient:  0.1300245370959588
iteration : 12993
train acc:  0.875
train loss:  0.33313706517219543
train gradient:  0.10265888035999392
iteration : 12994
train acc:  0.84375
train loss:  0.3494798541069031
train gradient:  0.12477359888138903
iteration : 12995
train acc:  0.859375
train loss:  0.3138008117675781
train gradient:  0.15391393116345886
iteration : 12996
train acc:  0.859375
train loss:  0.3583487868309021
train gradient:  0.132444045835362
iteration : 12997
train acc:  0.8515625
train loss:  0.3171621859073639
train gradient:  0.10710710125670103
iteration : 12998
train acc:  0.90625
train loss:  0.3361090421676636
train gradient:  0.16528527668512255
iteration : 12999
train acc:  0.859375
train loss:  0.3251827359199524
train gradient:  0.1409615758523009
iteration : 13000
train acc:  0.859375
train loss:  0.2693682909011841
train gradient:  0.10135072360422177
iteration : 13001
train acc:  0.7890625
train loss:  0.4144212007522583
train gradient:  0.20568149675274477
iteration : 13002
train acc:  0.890625
train loss:  0.25006550550460815
train gradient:  0.09456384569630381
iteration : 13003
train acc:  0.8125
train loss:  0.3701670467853546
train gradient:  0.16096312328850337
iteration : 13004
train acc:  0.875
train loss:  0.3241584897041321
train gradient:  0.1298255817792558
iteration : 13005
train acc:  0.8828125
train loss:  0.2351721227169037
train gradient:  0.08062390922597834
iteration : 13006
train acc:  0.8671875
train loss:  0.2715424597263336
train gradient:  0.1652724687987982
iteration : 13007
train acc:  0.890625
train loss:  0.28088030219078064
train gradient:  0.15192995872362658
iteration : 13008
train acc:  0.859375
train loss:  0.32678478956222534
train gradient:  0.10813033371097408
iteration : 13009
train acc:  0.9140625
train loss:  0.23848998546600342
train gradient:  0.07706017958487704
iteration : 13010
train acc:  0.8828125
train loss:  0.2814348638057709
train gradient:  0.08333996347947709
iteration : 13011
train acc:  0.8671875
train loss:  0.314657986164093
train gradient:  0.11842409309119985
iteration : 13012
train acc:  0.9296875
train loss:  0.27014437317848206
train gradient:  0.1442413408356005
iteration : 13013
train acc:  0.8515625
train loss:  0.3419804275035858
train gradient:  0.1583636136069032
iteration : 13014
train acc:  0.859375
train loss:  0.32174888253211975
train gradient:  0.11612563432601403
iteration : 13015
train acc:  0.8203125
train loss:  0.38433322310447693
train gradient:  0.1481366132193347
iteration : 13016
train acc:  0.828125
train loss:  0.34000837802886963
train gradient:  0.20182383175853122
iteration : 13017
train acc:  0.828125
train loss:  0.36762455105781555
train gradient:  0.12030895470368688
iteration : 13018
train acc:  0.890625
train loss:  0.24169908463954926
train gradient:  0.08480508915402968
iteration : 13019
train acc:  0.8671875
train loss:  0.2785762548446655
train gradient:  0.3563363095174909
iteration : 13020
train acc:  0.859375
train loss:  0.37289151549339294
train gradient:  0.2821731641142679
iteration : 13021
train acc:  0.875
train loss:  0.3045859932899475
train gradient:  0.16536998421796945
iteration : 13022
train acc:  0.8671875
train loss:  0.29738742113113403
train gradient:  0.14018294508751028
iteration : 13023
train acc:  0.8671875
train loss:  0.31620103120803833
train gradient:  0.12989444178403872
iteration : 13024
train acc:  0.875
train loss:  0.3270488381385803
train gradient:  0.12827637778297285
iteration : 13025
train acc:  0.8984375
train loss:  0.29638099670410156
train gradient:  0.09959528168913172
iteration : 13026
train acc:  0.890625
train loss:  0.2633627653121948
train gradient:  0.14628392893921377
iteration : 13027
train acc:  0.921875
train loss:  0.27881866693496704
train gradient:  0.1098484514434483
iteration : 13028
train acc:  0.859375
train loss:  0.28833162784576416
train gradient:  0.14658994868230762
iteration : 13029
train acc:  0.828125
train loss:  0.38255375623703003
train gradient:  0.2045737921642939
iteration : 13030
train acc:  0.8203125
train loss:  0.37523239850997925
train gradient:  0.23577406801874806
iteration : 13031
train acc:  0.875
train loss:  0.25922778248786926
train gradient:  0.09209333132549806
iteration : 13032
train acc:  0.875
train loss:  0.27963417768478394
train gradient:  0.14491549660988903
iteration : 13033
train acc:  0.8828125
train loss:  0.2788620591163635
train gradient:  0.1295832650829508
iteration : 13034
train acc:  0.875
train loss:  0.3522404432296753
train gradient:  0.14800774635776154
iteration : 13035
train acc:  0.890625
train loss:  0.25177645683288574
train gradient:  0.11030321818775382
iteration : 13036
train acc:  0.8359375
train loss:  0.3494439125061035
train gradient:  0.14454912026224265
iteration : 13037
train acc:  0.921875
train loss:  0.26941072940826416
train gradient:  0.11414469529616883
iteration : 13038
train acc:  0.8125
train loss:  0.34474390745162964
train gradient:  0.13777601366117537
iteration : 13039
train acc:  0.890625
train loss:  0.2675653100013733
train gradient:  0.12196097019751838
iteration : 13040
train acc:  0.875
train loss:  0.33609187602996826
train gradient:  0.1991868845484464
iteration : 13041
train acc:  0.8671875
train loss:  0.3276549279689789
train gradient:  0.12211239868302527
iteration : 13042
train acc:  0.8828125
train loss:  0.3222314715385437
train gradient:  0.16820481644151505
iteration : 13043
train acc:  0.84375
train loss:  0.3176516592502594
train gradient:  0.12091423594493525
iteration : 13044
train acc:  0.8359375
train loss:  0.33827412128448486
train gradient:  0.13622568226828496
iteration : 13045
train acc:  0.8828125
train loss:  0.27278560400009155
train gradient:  0.09306912808290511
iteration : 13046
train acc:  0.859375
train loss:  0.353837251663208
train gradient:  0.16789848149199954
iteration : 13047
train acc:  0.8671875
train loss:  0.29353100061416626
train gradient:  0.10553516976992594
iteration : 13048
train acc:  0.8359375
train loss:  0.3642846941947937
train gradient:  0.18478398997436388
iteration : 13049
train acc:  0.8671875
train loss:  0.2926301658153534
train gradient:  0.11502511429559718
iteration : 13050
train acc:  0.8359375
train loss:  0.3195376992225647
train gradient:  0.14254184572465517
iteration : 13051
train acc:  0.796875
train loss:  0.42613571882247925
train gradient:  0.250676625648214
iteration : 13052
train acc:  0.859375
train loss:  0.33868855237960815
train gradient:  0.12641959433868555
iteration : 13053
train acc:  0.8984375
train loss:  0.23080974817276
train gradient:  0.08777207884885052
iteration : 13054
train acc:  0.890625
train loss:  0.3023343086242676
train gradient:  0.07579945489749147
iteration : 13055
train acc:  0.875
train loss:  0.2844173014163971
train gradient:  0.10761411809560703
iteration : 13056
train acc:  0.9140625
train loss:  0.26025915145874023
train gradient:  0.108155667203721
iteration : 13057
train acc:  0.8515625
train loss:  0.30186015367507935
train gradient:  0.13469950896076072
iteration : 13058
train acc:  0.8671875
train loss:  0.2651558518409729
train gradient:  0.13146460703667945
iteration : 13059
train acc:  0.84375
train loss:  0.382293164730072
train gradient:  0.18959501266914586
iteration : 13060
train acc:  0.8828125
train loss:  0.24067296087741852
train gradient:  0.06311558616250297
iteration : 13061
train acc:  0.84375
train loss:  0.3882789611816406
train gradient:  0.1660259114012349
iteration : 13062
train acc:  0.875
train loss:  0.27386897802352905
train gradient:  0.10440447901864823
iteration : 13063
train acc:  0.9296875
train loss:  0.2793307900428772
train gradient:  0.08305675869036983
iteration : 13064
train acc:  0.859375
train loss:  0.28644299507141113
train gradient:  0.18836384764252762
iteration : 13065
train acc:  0.859375
train loss:  0.38466548919677734
train gradient:  0.23052786742408166
iteration : 13066
train acc:  0.8359375
train loss:  0.3304854929447174
train gradient:  0.153458294955753
iteration : 13067
train acc:  0.9140625
train loss:  0.2605602443218231
train gradient:  0.09320733709646224
iteration : 13068
train acc:  0.8125
train loss:  0.3709852993488312
train gradient:  0.3346021125259326
iteration : 13069
train acc:  0.8359375
train loss:  0.26782816648483276
train gradient:  0.08863641838071296
iteration : 13070
train acc:  0.828125
train loss:  0.33458423614501953
train gradient:  0.13070578615563855
iteration : 13071
train acc:  0.84375
train loss:  0.3307432532310486
train gradient:  0.18273744925771412
iteration : 13072
train acc:  0.8671875
train loss:  0.31238818168640137
train gradient:  0.12112319885000031
iteration : 13073
train acc:  0.84375
train loss:  0.3281695246696472
train gradient:  0.18305818649750913
iteration : 13074
train acc:  0.8828125
train loss:  0.2920023202896118
train gradient:  0.1574955446415277
iteration : 13075
train acc:  0.875
train loss:  0.2981223464012146
train gradient:  0.10168814058296714
iteration : 13076
train acc:  0.8828125
train loss:  0.28932368755340576
train gradient:  0.15193114664496793
iteration : 13077
train acc:  0.8203125
train loss:  0.3258473575115204
train gradient:  0.12205683547602196
iteration : 13078
train acc:  0.8671875
train loss:  0.31073153018951416
train gradient:  0.10500429417216949
iteration : 13079
train acc:  0.875
train loss:  0.32752102613449097
train gradient:  0.16126537098589153
iteration : 13080
train acc:  0.890625
train loss:  0.25908392667770386
train gradient:  0.11667593699304049
iteration : 13081
train acc:  0.875
train loss:  0.30849388241767883
train gradient:  0.12819651266761584
iteration : 13082
train acc:  0.8203125
train loss:  0.3657945692539215
train gradient:  0.12781111162531877
iteration : 13083
train acc:  0.875
train loss:  0.2535054683685303
train gradient:  0.08463222365690415
iteration : 13084
train acc:  0.859375
train loss:  0.3513307273387909
train gradient:  0.12575099797637954
iteration : 13085
train acc:  0.890625
train loss:  0.28816938400268555
train gradient:  0.338338898846854
iteration : 13086
train acc:  0.8515625
train loss:  0.30234527587890625
train gradient:  0.2177340560452975
iteration : 13087
train acc:  0.8125
train loss:  0.4260658025741577
train gradient:  0.19139178943965396
iteration : 13088
train acc:  0.78125
train loss:  0.4573463797569275
train gradient:  0.2541349412004836
iteration : 13089
train acc:  0.90625
train loss:  0.2511986792087555
train gradient:  0.06613384154995529
iteration : 13090
train acc:  0.859375
train loss:  0.3110653758049011
train gradient:  0.1261679179235214
iteration : 13091
train acc:  0.8359375
train loss:  0.36205077171325684
train gradient:  0.16948954117119325
iteration : 13092
train acc:  0.875
train loss:  0.31784704327583313
train gradient:  0.15552031407386083
iteration : 13093
train acc:  0.8203125
train loss:  0.406509667634964
train gradient:  0.19828435293466679
iteration : 13094
train acc:  0.875
train loss:  0.3617440462112427
train gradient:  0.16540795419715962
iteration : 13095
train acc:  0.8671875
train loss:  0.3371003568172455
train gradient:  0.1570387678281897
iteration : 13096
train acc:  0.890625
train loss:  0.2142259031534195
train gradient:  0.05997409902886986
iteration : 13097
train acc:  0.875
train loss:  0.2791905999183655
train gradient:  0.08488169519151671
iteration : 13098
train acc:  0.859375
train loss:  0.3579064607620239
train gradient:  0.15878944139037526
iteration : 13099
train acc:  0.796875
train loss:  0.38550877571105957
train gradient:  0.24711883970291437
iteration : 13100
train acc:  0.875
train loss:  0.26630091667175293
train gradient:  0.11332963927423756
iteration : 13101
train acc:  0.8671875
train loss:  0.29040980339050293
train gradient:  0.10720562748995466
iteration : 13102
train acc:  0.875
train loss:  0.32172706723213196
train gradient:  0.13904544911283379
iteration : 13103
train acc:  0.84375
train loss:  0.30683207511901855
train gradient:  0.1383039209066606
iteration : 13104
train acc:  0.84375
train loss:  0.4296749234199524
train gradient:  0.19213950321147785
iteration : 13105
train acc:  0.8671875
train loss:  0.3750085234642029
train gradient:  0.18626915427932692
iteration : 13106
train acc:  0.859375
train loss:  0.34766265749931335
train gradient:  0.1404320932335266
iteration : 13107
train acc:  0.8828125
train loss:  0.31290748715400696
train gradient:  0.12477009691681867
iteration : 13108
train acc:  0.90625
train loss:  0.25651609897613525
train gradient:  0.10395305543540495
iteration : 13109
train acc:  0.8515625
train loss:  0.35491883754730225
train gradient:  0.15480598086616082
iteration : 13110
train acc:  0.859375
train loss:  0.3034169673919678
train gradient:  0.14692152103678724
iteration : 13111
train acc:  0.84375
train loss:  0.36031877994537354
train gradient:  0.18721204046783396
iteration : 13112
train acc:  0.8203125
train loss:  0.3286946415901184
train gradient:  0.13309477901223773
iteration : 13113
train acc:  0.8671875
train loss:  0.2954792380332947
train gradient:  0.15979350022172606
iteration : 13114
train acc:  0.8203125
train loss:  0.34977036714553833
train gradient:  0.16764425044146325
iteration : 13115
train acc:  0.8046875
train loss:  0.356121689081192
train gradient:  0.15968483469135786
iteration : 13116
train acc:  0.875
train loss:  0.257587194442749
train gradient:  0.11206949958787961
iteration : 13117
train acc:  0.9375
train loss:  0.19824865460395813
train gradient:  0.06361874936596029
iteration : 13118
train acc:  0.8671875
train loss:  0.2628515958786011
train gradient:  0.09434689913944289
iteration : 13119
train acc:  0.828125
train loss:  0.3819952607154846
train gradient:  0.2418894710753267
iteration : 13120
train acc:  0.8671875
train loss:  0.3615177273750305
train gradient:  0.18824204752024318
iteration : 13121
train acc:  0.859375
train loss:  0.3386179804801941
train gradient:  0.2114416970724437
iteration : 13122
train acc:  0.8359375
train loss:  0.39608195424079895
train gradient:  0.16635067077144675
iteration : 13123
train acc:  0.8125
train loss:  0.3909742832183838
train gradient:  0.13946323252259435
iteration : 13124
train acc:  0.875
train loss:  0.32299381494522095
train gradient:  0.11081917088621429
iteration : 13125
train acc:  0.8359375
train loss:  0.4038633704185486
train gradient:  0.23975402990632433
iteration : 13126
train acc:  0.8359375
train loss:  0.397644579410553
train gradient:  0.27557693331817446
iteration : 13127
train acc:  0.8828125
train loss:  0.2560104727745056
train gradient:  0.07515285155015759
iteration : 13128
train acc:  0.8203125
train loss:  0.3256717622280121
train gradient:  0.12498102101543672
iteration : 13129
train acc:  0.7890625
train loss:  0.39826488494873047
train gradient:  0.22742588882381143
iteration : 13130
train acc:  0.8828125
train loss:  0.29642894864082336
train gradient:  0.11951194205350514
iteration : 13131
train acc:  0.9140625
train loss:  0.23701608180999756
train gradient:  0.12391323012139571
iteration : 13132
train acc:  0.8671875
train loss:  0.32744064927101135
train gradient:  0.13530640363492802
iteration : 13133
train acc:  0.8671875
train loss:  0.32192617654800415
train gradient:  0.13500377753246984
iteration : 13134
train acc:  0.8828125
train loss:  0.2745576500892639
train gradient:  0.1250180165523507
iteration : 13135
train acc:  0.9140625
train loss:  0.23752623796463013
train gradient:  0.07504122614728885
iteration : 13136
train acc:  0.8359375
train loss:  0.32996058464050293
train gradient:  0.1437237391867338
iteration : 13137
train acc:  0.8203125
train loss:  0.34711095690727234
train gradient:  0.1296743028527055
iteration : 13138
train acc:  0.9140625
train loss:  0.23888854682445526
train gradient:  0.10132297790269212
iteration : 13139
train acc:  0.8671875
train loss:  0.3425338566303253
train gradient:  0.16367446359325116
iteration : 13140
train acc:  0.8671875
train loss:  0.35532718896865845
train gradient:  0.11307861588522658
iteration : 13141
train acc:  0.8203125
train loss:  0.4516846537590027
train gradient:  0.1691799768178222
iteration : 13142
train acc:  0.890625
train loss:  0.2803478538990021
train gradient:  0.09075561498101735
iteration : 13143
train acc:  0.8046875
train loss:  0.4226064682006836
train gradient:  0.2520332591565454
iteration : 13144
train acc:  0.8125
train loss:  0.42731061577796936
train gradient:  0.2590327291105408
iteration : 13145
train acc:  0.8671875
train loss:  0.2929673492908478
train gradient:  0.11358652705020102
iteration : 13146
train acc:  0.859375
train loss:  0.2954005300998688
train gradient:  0.10864858394136902
iteration : 13147
train acc:  0.8671875
train loss:  0.3049326241016388
train gradient:  0.18867391433525244
iteration : 13148
train acc:  0.8515625
train loss:  0.30507898330688477
train gradient:  0.15718999710569448
iteration : 13149
train acc:  0.8671875
train loss:  0.2529846429824829
train gradient:  0.08533783377519914
iteration : 13150
train acc:  0.859375
train loss:  0.35734713077545166
train gradient:  0.13687801264564742
iteration : 13151
train acc:  0.9140625
train loss:  0.2688602805137634
train gradient:  0.12061906443846299
iteration : 13152
train acc:  0.859375
train loss:  0.2865023612976074
train gradient:  0.14165391327015414
iteration : 13153
train acc:  0.890625
train loss:  0.2519541084766388
train gradient:  0.12170687053396127
iteration : 13154
train acc:  0.875
train loss:  0.33320116996765137
train gradient:  0.23917587631831416
iteration : 13155
train acc:  0.890625
train loss:  0.3133416473865509
train gradient:  0.11534115812078176
iteration : 13156
train acc:  0.84375
train loss:  0.33743423223495483
train gradient:  0.16160522132231842
iteration : 13157
train acc:  0.84375
train loss:  0.3108987808227539
train gradient:  0.11789552674479749
iteration : 13158
train acc:  0.84375
train loss:  0.32042327523231506
train gradient:  0.11803885847133208
iteration : 13159
train acc:  0.890625
train loss:  0.2851105034351349
train gradient:  0.1089063622751883
iteration : 13160
train acc:  0.859375
train loss:  0.30059361457824707
train gradient:  0.11224762505217417
iteration : 13161
train acc:  0.890625
train loss:  0.26882779598236084
train gradient:  0.10353984871748731
iteration : 13162
train acc:  0.8671875
train loss:  0.35857635736465454
train gradient:  0.14019746482387524
iteration : 13163
train acc:  0.828125
train loss:  0.34253939986228943
train gradient:  0.15810004088446095
iteration : 13164
train acc:  0.90625
train loss:  0.2661702632904053
train gradient:  0.10191968549213191
iteration : 13165
train acc:  0.8515625
train loss:  0.3116154074668884
train gradient:  0.12416296047040513
iteration : 13166
train acc:  0.859375
train loss:  0.3221396207809448
train gradient:  0.1628446679892192
iteration : 13167
train acc:  0.8828125
train loss:  0.271779328584671
train gradient:  0.1279950997527467
iteration : 13168
train acc:  0.8046875
train loss:  0.3696625232696533
train gradient:  0.24076089013329766
iteration : 13169
train acc:  0.8359375
train loss:  0.35342204570770264
train gradient:  0.12996248702484242
iteration : 13170
train acc:  0.8984375
train loss:  0.2692278027534485
train gradient:  0.09429194602283311
iteration : 13171
train acc:  0.8828125
train loss:  0.310421884059906
train gradient:  0.1415176540208981
iteration : 13172
train acc:  0.828125
train loss:  0.3933636546134949
train gradient:  0.16100014716426125
iteration : 13173
train acc:  0.890625
train loss:  0.27652400732040405
train gradient:  0.11733146244623176
iteration : 13174
train acc:  0.921875
train loss:  0.26511678099632263
train gradient:  0.09478931483005688
iteration : 13175
train acc:  0.8515625
train loss:  0.3295740485191345
train gradient:  0.1399628801224974
iteration : 13176
train acc:  0.875
train loss:  0.315224289894104
train gradient:  0.11341281679841964
iteration : 13177
train acc:  0.8671875
train loss:  0.3280075192451477
train gradient:  0.12229996278575672
iteration : 13178
train acc:  0.8984375
train loss:  0.30798453092575073
train gradient:  0.13427521226149364
iteration : 13179
train acc:  0.8671875
train loss:  0.32367223501205444
train gradient:  0.18081260528927623
iteration : 13180
train acc:  0.8828125
train loss:  0.2925916314125061
train gradient:  0.12998388922516896
iteration : 13181
train acc:  0.8984375
train loss:  0.3024187982082367
train gradient:  0.1107134731659858
iteration : 13182
train acc:  0.890625
train loss:  0.29771238565444946
train gradient:  0.12902850098421953
iteration : 13183
train acc:  0.828125
train loss:  0.3662914037704468
train gradient:  0.13071471391813314
iteration : 13184
train acc:  0.8125
train loss:  0.39984357357025146
train gradient:  0.1414637078312022
iteration : 13185
train acc:  0.859375
train loss:  0.2991936504840851
train gradient:  0.12624216471782976
iteration : 13186
train acc:  0.9296875
train loss:  0.21121177077293396
train gradient:  0.06314349027161498
iteration : 13187
train acc:  0.8125
train loss:  0.3924882411956787
train gradient:  0.16190864774430308
iteration : 13188
train acc:  0.8671875
train loss:  0.3069400191307068
train gradient:  0.12064630031636318
iteration : 13189
train acc:  0.9296875
train loss:  0.23669104278087616
train gradient:  0.08907627796014707
iteration : 13190
train acc:  0.8671875
train loss:  0.36123085021972656
train gradient:  0.15388831431146013
iteration : 13191
train acc:  0.8359375
train loss:  0.3679082989692688
train gradient:  0.191500043355939
iteration : 13192
train acc:  0.8828125
train loss:  0.2727070450782776
train gradient:  0.12579581430084358
iteration : 13193
train acc:  0.875
train loss:  0.2933213412761688
train gradient:  0.09523960607412416
iteration : 13194
train acc:  0.8984375
train loss:  0.2839745879173279
train gradient:  0.1583066154909773
iteration : 13195
train acc:  0.8984375
train loss:  0.2611711025238037
train gradient:  0.10131557242021623
iteration : 13196
train acc:  0.8828125
train loss:  0.2658010721206665
train gradient:  0.0921719714184843
iteration : 13197
train acc:  0.84375
train loss:  0.3407149910926819
train gradient:  0.13445322381526842
iteration : 13198
train acc:  0.875
train loss:  0.24828821420669556
train gradient:  0.12250952983710005
iteration : 13199
train acc:  0.90625
train loss:  0.2889743447303772
train gradient:  0.08791832153547392
iteration : 13200
train acc:  0.84375
train loss:  0.37313249707221985
train gradient:  0.14362504742738783
iteration : 13201
train acc:  0.8984375
train loss:  0.29289865493774414
train gradient:  0.13371757720122307
iteration : 13202
train acc:  0.875
train loss:  0.3741426169872284
train gradient:  0.1686059831139032
iteration : 13203
train acc:  0.90625
train loss:  0.2501834034919739
train gradient:  0.1380430194312196
iteration : 13204
train acc:  0.859375
train loss:  0.28570058941841125
train gradient:  0.09406492733500058
iteration : 13205
train acc:  0.875
train loss:  0.28806084394454956
train gradient:  0.1578934293142566
iteration : 13206
train acc:  0.875
train loss:  0.2919084429740906
train gradient:  0.10980506340561857
iteration : 13207
train acc:  0.8828125
train loss:  0.2499198168516159
train gradient:  0.10607732230600646
iteration : 13208
train acc:  0.890625
train loss:  0.2599180340766907
train gradient:  0.09698821294465981
iteration : 13209
train acc:  0.8671875
train loss:  0.30909472703933716
train gradient:  0.18654263582346614
iteration : 13210
train acc:  0.890625
train loss:  0.2950436770915985
train gradient:  0.125274602288467
iteration : 13211
train acc:  0.8515625
train loss:  0.3027540147304535
train gradient:  0.1159393023550935
iteration : 13212
train acc:  0.8203125
train loss:  0.32740598917007446
train gradient:  0.12699296274252744
iteration : 13213
train acc:  0.890625
train loss:  0.28508448600769043
train gradient:  0.06948466911482046
iteration : 13214
train acc:  0.859375
train loss:  0.2907051146030426
train gradient:  0.19013980228332622
iteration : 13215
train acc:  0.8671875
train loss:  0.33123400807380676
train gradient:  0.1125211821722299
iteration : 13216
train acc:  0.8125
train loss:  0.3403806686401367
train gradient:  0.13498339870357035
iteration : 13217
train acc:  0.828125
train loss:  0.3699921667575836
train gradient:  0.20310663684288036
iteration : 13218
train acc:  0.890625
train loss:  0.27437978982925415
train gradient:  0.16043669841144254
iteration : 13219
train acc:  0.8828125
train loss:  0.28427329659461975
train gradient:  0.09642579978396547
iteration : 13220
train acc:  0.828125
train loss:  0.40033361315727234
train gradient:  0.17471930626074583
iteration : 13221
train acc:  0.8671875
train loss:  0.2918919324874878
train gradient:  0.09585226163292067
iteration : 13222
train acc:  0.875
train loss:  0.3472660183906555
train gradient:  0.15408865803538394
iteration : 13223
train acc:  0.921875
train loss:  0.20561106503009796
train gradient:  0.07330251932900989
iteration : 13224
train acc:  0.9296875
train loss:  0.19599975645542145
train gradient:  0.09249005247411134
iteration : 13225
train acc:  0.8359375
train loss:  0.33392852544784546
train gradient:  0.27288332805515486
iteration : 13226
train acc:  0.875
train loss:  0.4013296067714691
train gradient:  0.13531383079063508
iteration : 13227
train acc:  0.8984375
train loss:  0.2773311734199524
train gradient:  0.07793245244208706
iteration : 13228
train acc:  0.8984375
train loss:  0.27988767623901367
train gradient:  0.0992982918825348
iteration : 13229
train acc:  0.8359375
train loss:  0.3682120442390442
train gradient:  0.14053212110969374
iteration : 13230
train acc:  0.8515625
train loss:  0.32666733860969543
train gradient:  0.1462236439890053
iteration : 13231
train acc:  0.8671875
train loss:  0.3080326318740845
train gradient:  0.09628871817925728
iteration : 13232
train acc:  0.8671875
train loss:  0.31755712628364563
train gradient:  0.14311993617311403
iteration : 13233
train acc:  0.890625
train loss:  0.27557769417762756
train gradient:  0.08166380649331216
iteration : 13234
train acc:  0.8515625
train loss:  0.3324176073074341
train gradient:  0.1413763386127744
iteration : 13235
train acc:  0.875
train loss:  0.32002848386764526
train gradient:  0.15982886587784126
iteration : 13236
train acc:  0.8515625
train loss:  0.28234967589378357
train gradient:  0.12230654295753701
iteration : 13237
train acc:  0.8203125
train loss:  0.3878397047519684
train gradient:  0.21601250731924276
iteration : 13238
train acc:  0.8828125
train loss:  0.3311217427253723
train gradient:  0.12627508308150234
iteration : 13239
train acc:  0.84375
train loss:  0.3122999668121338
train gradient:  0.2027696724418377
iteration : 13240
train acc:  0.90625
train loss:  0.2490864396095276
train gradient:  0.1107872292689288
iteration : 13241
train acc:  0.84375
train loss:  0.3012341260910034
train gradient:  0.1525627574566868
iteration : 13242
train acc:  0.875
train loss:  0.28196126222610474
train gradient:  0.09236073652948754
iteration : 13243
train acc:  0.875
train loss:  0.32717522978782654
train gradient:  0.14406195476255446
iteration : 13244
train acc:  0.8671875
train loss:  0.3057039976119995
train gradient:  0.16339354111411203
iteration : 13245
train acc:  0.8671875
train loss:  0.2895607352256775
train gradient:  0.07559973962850575
iteration : 13246
train acc:  0.9140625
train loss:  0.29804301261901855
train gradient:  0.15226076300276836
iteration : 13247
train acc:  0.8671875
train loss:  0.3040580153465271
train gradient:  0.10316950736908245
iteration : 13248
train acc:  0.8984375
train loss:  0.27173078060150146
train gradient:  0.0756937722425776
iteration : 13249
train acc:  0.8203125
train loss:  0.3535687327384949
train gradient:  0.1891304366420483
iteration : 13250
train acc:  0.875
train loss:  0.2695049047470093
train gradient:  0.10142492590670579
iteration : 13251
train acc:  0.84375
train loss:  0.3449004888534546
train gradient:  0.15546853940040262
iteration : 13252
train acc:  0.8203125
train loss:  0.34234219789505005
train gradient:  0.13657409913529733
iteration : 13253
train acc:  0.859375
train loss:  0.3367312550544739
train gradient:  0.18152328415614224
iteration : 13254
train acc:  0.796875
train loss:  0.46820127964019775
train gradient:  0.2705172754414093
iteration : 13255
train acc:  0.8359375
train loss:  0.3386284410953522
train gradient:  0.17883655621758987
iteration : 13256
train acc:  0.8125
train loss:  0.39194709062576294
train gradient:  0.18655844150123294
iteration : 13257
train acc:  0.90625
train loss:  0.28630393743515015
train gradient:  0.10337998592141596
iteration : 13258
train acc:  0.921875
train loss:  0.30832985043525696
train gradient:  0.1417843811413736
iteration : 13259
train acc:  0.890625
train loss:  0.24356073141098022
train gradient:  0.08266761344975464
iteration : 13260
train acc:  0.84375
train loss:  0.2829493284225464
train gradient:  0.08564495315495706
iteration : 13261
train acc:  0.8671875
train loss:  0.3252941071987152
train gradient:  0.11546198010510267
iteration : 13262
train acc:  0.828125
train loss:  0.3242325186729431
train gradient:  0.1430943380903499
iteration : 13263
train acc:  0.875
train loss:  0.2715771794319153
train gradient:  0.14239433240179
iteration : 13264
train acc:  0.90625
train loss:  0.2465323805809021
train gradient:  0.06630224316826783
iteration : 13265
train acc:  0.875
train loss:  0.3037526607513428
train gradient:  0.16291651532403328
iteration : 13266
train acc:  0.8828125
train loss:  0.2714053988456726
train gradient:  0.13004825605000195
iteration : 13267
train acc:  0.859375
train loss:  0.2898152768611908
train gradient:  0.09038023330606479
iteration : 13268
train acc:  0.828125
train loss:  0.3035910427570343
train gradient:  0.12000610896580115
iteration : 13269
train acc:  0.8125
train loss:  0.40255099534988403
train gradient:  0.1692543991560811
iteration : 13270
train acc:  0.7890625
train loss:  0.404449999332428
train gradient:  0.17148761523469858
iteration : 13271
train acc:  0.8671875
train loss:  0.31844234466552734
train gradient:  0.15877099799637762
iteration : 13272
train acc:  0.8984375
train loss:  0.2634314298629761
train gradient:  0.10523534404520284
iteration : 13273
train acc:  0.859375
train loss:  0.28706181049346924
train gradient:  0.11679108309423514
iteration : 13274
train acc:  0.9296875
train loss:  0.2086976170539856
train gradient:  0.0668870096143386
iteration : 13275
train acc:  0.859375
train loss:  0.28424692153930664
train gradient:  0.14903563315782364
iteration : 13276
train acc:  0.875
train loss:  0.2874767780303955
train gradient:  0.10665102509771793
iteration : 13277
train acc:  0.8515625
train loss:  0.34185096621513367
train gradient:  0.12632711429014598
iteration : 13278
train acc:  0.8359375
train loss:  0.3159872889518738
train gradient:  0.13577316823333682
iteration : 13279
train acc:  0.8515625
train loss:  0.35857143998146057
train gradient:  0.14143124516243183
iteration : 13280
train acc:  0.890625
train loss:  0.30393606424331665
train gradient:  0.12093911674431891
iteration : 13281
train acc:  0.8046875
train loss:  0.43372759222984314
train gradient:  0.1960393254802624
iteration : 13282
train acc:  0.875
train loss:  0.30209124088287354
train gradient:  0.1007240897062647
iteration : 13283
train acc:  0.84375
train loss:  0.2956622540950775
train gradient:  0.1337148143748783
iteration : 13284
train acc:  0.921875
train loss:  0.2158520221710205
train gradient:  0.0810458169863669
iteration : 13285
train acc:  0.8203125
train loss:  0.40111908316612244
train gradient:  0.20683851815851034
iteration : 13286
train acc:  0.8984375
train loss:  0.21670936048030853
train gradient:  0.0804244706263438
iteration : 13287
train acc:  0.8515625
train loss:  0.40531736612319946
train gradient:  0.24104844580434287
iteration : 13288
train acc:  0.828125
train loss:  0.3497781455516815
train gradient:  0.15097429834130263
iteration : 13289
train acc:  0.8515625
train loss:  0.2919517755508423
train gradient:  0.09491951570761041
iteration : 13290
train acc:  0.8359375
train loss:  0.3636716604232788
train gradient:  0.1761503890462536
iteration : 13291
train acc:  0.8984375
train loss:  0.2932782471179962
train gradient:  0.160680862920011
iteration : 13292
train acc:  0.8359375
train loss:  0.3407321870326996
train gradient:  0.1361426587702721
iteration : 13293
train acc:  0.859375
train loss:  0.2962096631526947
train gradient:  0.1311704665368306
iteration : 13294
train acc:  0.8828125
train loss:  0.317322313785553
train gradient:  0.09334210337803349
iteration : 13295
train acc:  0.890625
train loss:  0.2880725562572479
train gradient:  0.1425595354745136
iteration : 13296
train acc:  0.859375
train loss:  0.36574816703796387
train gradient:  0.11864476031315414
iteration : 13297
train acc:  0.828125
train loss:  0.42572906613349915
train gradient:  0.178643551848213
iteration : 13298
train acc:  0.8203125
train loss:  0.3572618365287781
train gradient:  0.14449740086000817
iteration : 13299
train acc:  0.84375
train loss:  0.326404869556427
train gradient:  0.16502945103453295
iteration : 13300
train acc:  0.9140625
train loss:  0.22439411282539368
train gradient:  0.06901575135461067
iteration : 13301
train acc:  0.8359375
train loss:  0.3445843458175659
train gradient:  0.184412019951214
iteration : 13302
train acc:  0.8515625
train loss:  0.3277830481529236
train gradient:  0.09929922128769128
iteration : 13303
train acc:  0.84375
train loss:  0.38711777329444885
train gradient:  0.16777396013720086
iteration : 13304
train acc:  0.8515625
train loss:  0.29619723558425903
train gradient:  0.1260251676288195
iteration : 13305
train acc:  0.8828125
train loss:  0.2682635486125946
train gradient:  0.1048012062802669
iteration : 13306
train acc:  0.8828125
train loss:  0.29678091406822205
train gradient:  0.11469830158454762
iteration : 13307
train acc:  0.8515625
train loss:  0.3828544616699219
train gradient:  0.1494368139235167
iteration : 13308
train acc:  0.875
train loss:  0.2670741081237793
train gradient:  0.10169461195643453
iteration : 13309
train acc:  0.8359375
train loss:  0.4038968086242676
train gradient:  0.24937543229814485
iteration : 13310
train acc:  0.84375
train loss:  0.3662812113761902
train gradient:  0.14795328393175106
iteration : 13311
train acc:  0.8515625
train loss:  0.3208719491958618
train gradient:  0.10308799962624657
iteration : 13312
train acc:  0.8359375
train loss:  0.3235611915588379
train gradient:  0.21018161334962643
iteration : 13313
train acc:  0.8515625
train loss:  0.3086704611778259
train gradient:  0.13757792413812542
iteration : 13314
train acc:  0.859375
train loss:  0.34022995829582214
train gradient:  0.15377636720795132
iteration : 13315
train acc:  0.8515625
train loss:  0.32349705696105957
train gradient:  0.11085028526235331
iteration : 13316
train acc:  0.8125
train loss:  0.4596610367298126
train gradient:  0.29814158647481614
iteration : 13317
train acc:  0.890625
train loss:  0.27053317427635193
train gradient:  0.096927548075593
iteration : 13318
train acc:  0.921875
train loss:  0.249925896525383
train gradient:  0.09136700399748156
iteration : 13319
train acc:  0.859375
train loss:  0.34043705463409424
train gradient:  0.13998244551111544
iteration : 13320
train acc:  0.8671875
train loss:  0.3144284188747406
train gradient:  0.11221179880064396
iteration : 13321
train acc:  0.8515625
train loss:  0.3350791335105896
train gradient:  0.10138905938152505
iteration : 13322
train acc:  0.8359375
train loss:  0.32895100116729736
train gradient:  0.1401269387300637
iteration : 13323
train acc:  0.8359375
train loss:  0.3747727870941162
train gradient:  0.1571766070373436
iteration : 13324
train acc:  0.8828125
train loss:  0.2634296715259552
train gradient:  0.1204702873872589
iteration : 13325
train acc:  0.890625
train loss:  0.3097730576992035
train gradient:  0.2087553911298139
iteration : 13326
train acc:  0.890625
train loss:  0.29874664545059204
train gradient:  0.12146352386673658
iteration : 13327
train acc:  0.890625
train loss:  0.268418550491333
train gradient:  0.10185589329713325
iteration : 13328
train acc:  0.890625
train loss:  0.2824466824531555
train gradient:  0.10531592717488479
iteration : 13329
train acc:  0.890625
train loss:  0.2906065881252289
train gradient:  0.10301606681873335
iteration : 13330
train acc:  0.8828125
train loss:  0.360994815826416
train gradient:  0.19592244818337223
iteration : 13331
train acc:  0.859375
train loss:  0.32333827018737793
train gradient:  0.12267751577320102
iteration : 13332
train acc:  0.84375
train loss:  0.34416624903678894
train gradient:  0.11482034034400988
iteration : 13333
train acc:  0.8359375
train loss:  0.33228206634521484
train gradient:  0.13821155234905266
iteration : 13334
train acc:  0.875
train loss:  0.28315770626068115
train gradient:  0.1198597786166293
iteration : 13335
train acc:  0.8671875
train loss:  0.3208604156970978
train gradient:  0.17332442245524582
iteration : 13336
train acc:  0.859375
train loss:  0.32260188460350037
train gradient:  0.10225887651077506
iteration : 13337
train acc:  0.8828125
train loss:  0.29830294847488403
train gradient:  0.15070766085547627
iteration : 13338
train acc:  0.84375
train loss:  0.30832114815711975
train gradient:  0.1138466266175886
iteration : 13339
train acc:  0.8203125
train loss:  0.3727944791316986
train gradient:  0.1608643604877778
iteration : 13340
train acc:  0.8359375
train loss:  0.4396810531616211
train gradient:  0.28533437241918164
iteration : 13341
train acc:  0.859375
train loss:  0.30946654081344604
train gradient:  0.15625459634621203
iteration : 13342
train acc:  0.8828125
train loss:  0.31970787048339844
train gradient:  0.14683787050463729
iteration : 13343
train acc:  0.8125
train loss:  0.3925546407699585
train gradient:  0.1746473027292454
iteration : 13344
train acc:  0.8671875
train loss:  0.2743011713027954
train gradient:  0.11124473726733951
iteration : 13345
train acc:  0.875
train loss:  0.2763611972332001
train gradient:  0.10389479665820066
iteration : 13346
train acc:  0.859375
train loss:  0.33096760511398315
train gradient:  0.12274815615231763
iteration : 13347
train acc:  0.9296875
train loss:  0.26584428548812866
train gradient:  0.09881247862665672
iteration : 13348
train acc:  0.84375
train loss:  0.31762075424194336
train gradient:  0.11667993488597286
iteration : 13349
train acc:  0.859375
train loss:  0.30854523181915283
train gradient:  0.1578129418231535
iteration : 13350
train acc:  0.8671875
train loss:  0.32144850492477417
train gradient:  0.11715537350776745
iteration : 13351
train acc:  0.8984375
train loss:  0.27046898007392883
train gradient:  0.08117556736797624
iteration : 13352
train acc:  0.8828125
train loss:  0.29259902238845825
train gradient:  0.1114130482832975
iteration : 13353
train acc:  0.859375
train loss:  0.29845455288887024
train gradient:  0.1553185385889202
iteration : 13354
train acc:  0.84375
train loss:  0.34361618757247925
train gradient:  0.14721906276011854
iteration : 13355
train acc:  0.8125
train loss:  0.3630014657974243
train gradient:  0.10319845788704482
iteration : 13356
train acc:  0.8359375
train loss:  0.32858267426490784
train gradient:  0.23745577028963694
iteration : 13357
train acc:  0.8828125
train loss:  0.3144119083881378
train gradient:  0.11270013705022275
iteration : 13358
train acc:  0.890625
train loss:  0.2901511490345001
train gradient:  0.125262557839121
iteration : 13359
train acc:  0.859375
train loss:  0.3745220899581909
train gradient:  0.18732557810849082
iteration : 13360
train acc:  0.8671875
train loss:  0.3666779398918152
train gradient:  0.16105039193233534
iteration : 13361
train acc:  0.8359375
train loss:  0.30432140827178955
train gradient:  0.11309608730595545
iteration : 13362
train acc:  0.8203125
train loss:  0.30654269456863403
train gradient:  0.14013431344434313
iteration : 13363
train acc:  0.8359375
train loss:  0.3539712131023407
train gradient:  0.1367536077402691
iteration : 13364
train acc:  0.90625
train loss:  0.22731702029705048
train gradient:  0.06290784776683202
iteration : 13365
train acc:  0.8828125
train loss:  0.23712754249572754
train gradient:  0.0904326598902086
iteration : 13366
train acc:  0.859375
train loss:  0.3273553252220154
train gradient:  0.1211477080976049
iteration : 13367
train acc:  0.828125
train loss:  0.353373646736145
train gradient:  0.17951072538954038
iteration : 13368
train acc:  0.828125
train loss:  0.381903737783432
train gradient:  0.23499897895597227
iteration : 13369
train acc:  0.8828125
train loss:  0.31465578079223633
train gradient:  0.13859801099735128
iteration : 13370
train acc:  0.875
train loss:  0.3887820243835449
train gradient:  0.1784270622601051
iteration : 13371
train acc:  0.8359375
train loss:  0.303006649017334
train gradient:  0.10357636811999987
iteration : 13372
train acc:  0.84375
train loss:  0.3580830693244934
train gradient:  0.1554538694251411
iteration : 13373
train acc:  0.890625
train loss:  0.28506794571876526
train gradient:  0.09865200745339733
iteration : 13374
train acc:  0.828125
train loss:  0.34654369950294495
train gradient:  0.176688004945972
iteration : 13375
train acc:  0.8671875
train loss:  0.3253119885921478
train gradient:  0.11609758020622188
iteration : 13376
train acc:  0.84375
train loss:  0.33385783433914185
train gradient:  0.1517486745551614
iteration : 13377
train acc:  0.875
train loss:  0.30574101209640503
train gradient:  0.09770275391800488
iteration : 13378
train acc:  0.8671875
train loss:  0.27800077199935913
train gradient:  0.1496493501973714
iteration : 13379
train acc:  0.890625
train loss:  0.28043535351753235
train gradient:  0.08581860434498867
iteration : 13380
train acc:  0.890625
train loss:  0.3180178999900818
train gradient:  0.12329287361373396
iteration : 13381
train acc:  0.8515625
train loss:  0.3046083450317383
train gradient:  0.12789881348525944
iteration : 13382
train acc:  0.859375
train loss:  0.32761532068252563
train gradient:  0.19997671721066235
iteration : 13383
train acc:  0.8828125
train loss:  0.3191583752632141
train gradient:  0.1263748461062897
iteration : 13384
train acc:  0.8125
train loss:  0.3793095052242279
train gradient:  0.18912679687894668
iteration : 13385
train acc:  0.875
train loss:  0.3579646944999695
train gradient:  0.1533529196299398
iteration : 13386
train acc:  0.8203125
train loss:  0.4004824161529541
train gradient:  0.17103761840786874
iteration : 13387
train acc:  0.875
train loss:  0.3207751214504242
train gradient:  0.08670715357502912
iteration : 13388
train acc:  0.8203125
train loss:  0.39448004961013794
train gradient:  0.2116206029021951
iteration : 13389
train acc:  0.8984375
train loss:  0.23198077082633972
train gradient:  0.0953524030653016
iteration : 13390
train acc:  0.859375
train loss:  0.2978361248970032
train gradient:  0.09408176599693481
iteration : 13391
train acc:  0.84375
train loss:  0.2949921786785126
train gradient:  0.11294868989581591
iteration : 13392
train acc:  0.828125
train loss:  0.32669633626937866
train gradient:  0.14703372358512395
iteration : 13393
train acc:  0.828125
train loss:  0.35432562232017517
train gradient:  0.18694658821685256
iteration : 13394
train acc:  0.90625
train loss:  0.28371959924697876
train gradient:  0.09858557304721294
iteration : 13395
train acc:  0.8671875
train loss:  0.2973143458366394
train gradient:  0.1324233719874217
iteration : 13396
train acc:  0.890625
train loss:  0.26435911655426025
train gradient:  0.10967292406295721
iteration : 13397
train acc:  0.84375
train loss:  0.3190596103668213
train gradient:  0.13240205559879747
iteration : 13398
train acc:  0.9140625
train loss:  0.228338360786438
train gradient:  0.08941855319217712
iteration : 13399
train acc:  0.890625
train loss:  0.2774510979652405
train gradient:  0.09256630221393952
iteration : 13400
train acc:  0.8359375
train loss:  0.3882405459880829
train gradient:  0.17692983228676018
iteration : 13401
train acc:  0.8515625
train loss:  0.31557509303092957
train gradient:  0.10988432173894815
iteration : 13402
train acc:  0.8984375
train loss:  0.266324907541275
train gradient:  0.06934519275005002
iteration : 13403
train acc:  0.8828125
train loss:  0.31223711371421814
train gradient:  0.10661541147091638
iteration : 13404
train acc:  0.8515625
train loss:  0.39768877625465393
train gradient:  0.1310166986432897
iteration : 13405
train acc:  0.8515625
train loss:  0.3989492654800415
train gradient:  0.20590343926335647
iteration : 13406
train acc:  0.8828125
train loss:  0.23978373408317566
train gradient:  0.10913650334485879
iteration : 13407
train acc:  0.8671875
train loss:  0.3302222490310669
train gradient:  0.1558931902288625
iteration : 13408
train acc:  0.90625
train loss:  0.2685551047325134
train gradient:  0.0751278142619021
iteration : 13409
train acc:  0.859375
train loss:  0.31994569301605225
train gradient:  0.11962613424081338
iteration : 13410
train acc:  0.859375
train loss:  0.34647417068481445
train gradient:  0.16716324410142794
iteration : 13411
train acc:  0.8125
train loss:  0.3946356177330017
train gradient:  0.1999457356587354
iteration : 13412
train acc:  0.8359375
train loss:  0.3215733468532562
train gradient:  0.11341692362324152
iteration : 13413
train acc:  0.890625
train loss:  0.2854970097541809
train gradient:  0.15313733253477388
iteration : 13414
train acc:  0.921875
train loss:  0.24373725056648254
train gradient:  0.13932494956867458
iteration : 13415
train acc:  0.875
train loss:  0.3284143805503845
train gradient:  0.12929207963975922
iteration : 13416
train acc:  0.8671875
train loss:  0.3244836926460266
train gradient:  0.13214756927146526
iteration : 13417
train acc:  0.90625
train loss:  0.2804025709629059
train gradient:  0.10886731486843938
iteration : 13418
train acc:  0.875
train loss:  0.2897343635559082
train gradient:  0.14101296824083806
iteration : 13419
train acc:  0.8515625
train loss:  0.32567864656448364
train gradient:  0.14691018536791461
iteration : 13420
train acc:  0.890625
train loss:  0.23833756148815155
train gradient:  0.10395368378087605
iteration : 13421
train acc:  0.8125
train loss:  0.35788822174072266
train gradient:  0.18859637141626978
iteration : 13422
train acc:  0.8671875
train loss:  0.32315486669540405
train gradient:  0.13042167840921431
iteration : 13423
train acc:  0.84375
train loss:  0.3130326271057129
train gradient:  0.10168130568998453
iteration : 13424
train acc:  0.9140625
train loss:  0.21786126494407654
train gradient:  0.07711168082865742
iteration : 13425
train acc:  0.8984375
train loss:  0.24456951022148132
train gradient:  0.14120609670139653
iteration : 13426
train acc:  0.8203125
train loss:  0.3659038543701172
train gradient:  0.13946432908879322
iteration : 13427
train acc:  0.8125
train loss:  0.3848443925380707
train gradient:  0.21093641988039566
iteration : 13428
train acc:  0.8203125
train loss:  0.2901269495487213
train gradient:  0.2274169672523218
iteration : 13429
train acc:  0.9140625
train loss:  0.2764323353767395
train gradient:  0.1262332519795853
iteration : 13430
train acc:  0.8671875
train loss:  0.2893878221511841
train gradient:  0.13597166472803335
iteration : 13431
train acc:  0.8515625
train loss:  0.34582647681236267
train gradient:  0.17234148272070662
iteration : 13432
train acc:  0.7734375
train loss:  0.4855181574821472
train gradient:  0.3428621702135852
iteration : 13433
train acc:  0.8671875
train loss:  0.3622238039970398
train gradient:  0.13319508222989102
iteration : 13434
train acc:  0.84375
train loss:  0.3046332001686096
train gradient:  0.13708232171561163
iteration : 13435
train acc:  0.8828125
train loss:  0.2843201458454132
train gradient:  0.09758403410215832
iteration : 13436
train acc:  0.8359375
train loss:  0.4178673326969147
train gradient:  0.22897950796088157
iteration : 13437
train acc:  0.90625
train loss:  0.25451743602752686
train gradient:  0.08032596143018914
iteration : 13438
train acc:  0.8828125
train loss:  0.26895833015441895
train gradient:  0.11051818892605415
iteration : 13439
train acc:  0.8515625
train loss:  0.38117387890815735
train gradient:  0.18371788114458487
iteration : 13440
train acc:  0.8515625
train loss:  0.32510119676589966
train gradient:  0.1443502944869388
iteration : 13441
train acc:  0.8515625
train loss:  0.3411371111869812
train gradient:  0.16947553284582223
iteration : 13442
train acc:  0.8203125
train loss:  0.3439869284629822
train gradient:  0.17363108625711704
iteration : 13443
train acc:  0.875
train loss:  0.31647032499313354
train gradient:  0.17880839684226663
iteration : 13444
train acc:  0.8359375
train loss:  0.3501051664352417
train gradient:  0.19140001800030387
iteration : 13445
train acc:  0.8671875
train loss:  0.30180615186691284
train gradient:  0.12221018955624693
iteration : 13446
train acc:  0.8671875
train loss:  0.32311350107192993
train gradient:  0.13220333962201972
iteration : 13447
train acc:  0.875
train loss:  0.2901179790496826
train gradient:  0.11652159857026066
iteration : 13448
train acc:  0.859375
train loss:  0.3028721809387207
train gradient:  0.14831627307910866
iteration : 13449
train acc:  0.8984375
train loss:  0.22845673561096191
train gradient:  0.10034929783655193
iteration : 13450
train acc:  0.859375
train loss:  0.3255723714828491
train gradient:  0.13902087212732805
iteration : 13451
train acc:  0.859375
train loss:  0.3547411561012268
train gradient:  0.1404267985270234
iteration : 13452
train acc:  0.9140625
train loss:  0.22464826703071594
train gradient:  0.0870923587153515
iteration : 13453
train acc:  0.859375
train loss:  0.36506205797195435
train gradient:  0.25788103809707846
iteration : 13454
train acc:  0.8515625
train loss:  0.29037654399871826
train gradient:  0.08949643015151591
iteration : 13455
train acc:  0.875
train loss:  0.2559261918067932
train gradient:  0.06438289855378955
iteration : 13456
train acc:  0.8828125
train loss:  0.2538883090019226
train gradient:  0.10013123095850368
iteration : 13457
train acc:  0.8828125
train loss:  0.29725125432014465
train gradient:  0.0873598651355324
iteration : 13458
train acc:  0.8515625
train loss:  0.3161960244178772
train gradient:  0.1584875289667563
iteration : 13459
train acc:  0.875
train loss:  0.30338984727859497
train gradient:  0.16771623551621767
iteration : 13460
train acc:  0.859375
train loss:  0.33841899037361145
train gradient:  0.198549508358067
iteration : 13461
train acc:  0.8046875
train loss:  0.3877687454223633
train gradient:  0.1920725891089849
iteration : 13462
train acc:  0.9140625
train loss:  0.28505709767341614
train gradient:  0.12371915032089471
iteration : 13463
train acc:  0.84375
train loss:  0.3270830512046814
train gradient:  0.12773982999687633
iteration : 13464
train acc:  0.921875
train loss:  0.2570831775665283
train gradient:  0.06318731354891569
iteration : 13465
train acc:  0.8828125
train loss:  0.27904003858566284
train gradient:  0.1309109057247792
iteration : 13466
train acc:  0.875
train loss:  0.26794296503067017
train gradient:  0.09107657410074349
iteration : 13467
train acc:  0.953125
train loss:  0.18300631642341614
train gradient:  0.06600849440748627
iteration : 13468
train acc:  0.8671875
train loss:  0.3381030559539795
train gradient:  0.15212787710830536
iteration : 13469
train acc:  0.84375
train loss:  0.3499595820903778
train gradient:  0.15078067039220766
iteration : 13470
train acc:  0.90625
train loss:  0.260206401348114
train gradient:  0.09869033034039426
iteration : 13471
train acc:  0.8515625
train loss:  0.3592742681503296
train gradient:  0.18736098801268924
iteration : 13472
train acc:  0.8671875
train loss:  0.3150525987148285
train gradient:  0.13349685741924994
iteration : 13473
train acc:  0.875
train loss:  0.2720000743865967
train gradient:  0.14398924349585548
iteration : 13474
train acc:  0.875
train loss:  0.289337694644928
train gradient:  0.17062631408771653
iteration : 13475
train acc:  0.8828125
train loss:  0.35931119322776794
train gradient:  0.1398838879583271
iteration : 13476
train acc:  0.890625
train loss:  0.2790064513683319
train gradient:  0.13289485595001577
iteration : 13477
train acc:  0.8671875
train loss:  0.277133584022522
train gradient:  0.0984106743177025
iteration : 13478
train acc:  0.84375
train loss:  0.35089239478111267
train gradient:  0.18771209147215323
iteration : 13479
train acc:  0.8203125
train loss:  0.36543869972229004
train gradient:  0.19192846162453783
iteration : 13480
train acc:  0.8828125
train loss:  0.2528752088546753
train gradient:  0.0844357890185201
iteration : 13481
train acc:  0.8828125
train loss:  0.3144848942756653
train gradient:  0.1494439418998377
iteration : 13482
train acc:  0.8359375
train loss:  0.3063599467277527
train gradient:  0.12982318824559902
iteration : 13483
train acc:  0.8515625
train loss:  0.30519139766693115
train gradient:  0.11610318875713245
iteration : 13484
train acc:  0.859375
train loss:  0.35731860995292664
train gradient:  0.13706844388619702
iteration : 13485
train acc:  0.859375
train loss:  0.3644678294658661
train gradient:  0.16767243252286843
iteration : 13486
train acc:  0.8515625
train loss:  0.34421324729919434
train gradient:  0.14103238371578358
iteration : 13487
train acc:  0.859375
train loss:  0.3045729398727417
train gradient:  0.11987700495618434
iteration : 13488
train acc:  0.8828125
train loss:  0.3435254693031311
train gradient:  0.15367710077134378
iteration : 13489
train acc:  0.8828125
train loss:  0.2798190116882324
train gradient:  0.09268230879071696
iteration : 13490
train acc:  0.875
train loss:  0.27434876561164856
train gradient:  0.08499366418583408
iteration : 13491
train acc:  0.84375
train loss:  0.33150187134742737
train gradient:  0.17941266495331487
iteration : 13492
train acc:  0.8828125
train loss:  0.23382535576820374
train gradient:  0.08975428926661039
iteration : 13493
train acc:  0.90625
train loss:  0.24849382042884827
train gradient:  0.0786585262984172
iteration : 13494
train acc:  0.8984375
train loss:  0.2363136112689972
train gradient:  0.09562735304814522
iteration : 13495
train acc:  0.8671875
train loss:  0.3311043381690979
train gradient:  0.1424104167003473
iteration : 13496
train acc:  0.8671875
train loss:  0.2855706512928009
train gradient:  0.11067480979289306
iteration : 13497
train acc:  0.8671875
train loss:  0.29614561796188354
train gradient:  0.1234279306532311
iteration : 13498
train acc:  0.8515625
train loss:  0.29026317596435547
train gradient:  0.18634771776124903
iteration : 13499
train acc:  0.859375
train loss:  0.27281200885772705
train gradient:  0.13661412379341836
iteration : 13500
train acc:  0.8984375
train loss:  0.24824266135692596
train gradient:  0.08607133386091005
iteration : 13501
train acc:  0.8515625
train loss:  0.33425313234329224
train gradient:  0.11804818243021989
iteration : 13502
train acc:  0.90625
train loss:  0.24143576622009277
train gradient:  0.12572529593317694
iteration : 13503
train acc:  0.8828125
train loss:  0.3084957003593445
train gradient:  0.16136972820399348
iteration : 13504
train acc:  0.84375
train loss:  0.2863706946372986
train gradient:  0.12432193827109461
iteration : 13505
train acc:  0.90625
train loss:  0.28425005078315735
train gradient:  0.15542754919818313
iteration : 13506
train acc:  0.859375
train loss:  0.3588455319404602
train gradient:  0.14808802890736245
iteration : 13507
train acc:  0.84375
train loss:  0.3604186177253723
train gradient:  0.2403257494772622
iteration : 13508
train acc:  0.8125
train loss:  0.3655470013618469
train gradient:  0.14784040511081925
iteration : 13509
train acc:  0.875
train loss:  0.28624147176742554
train gradient:  0.0962949438737183
iteration : 13510
train acc:  0.84375
train loss:  0.3413150906562805
train gradient:  0.13623212559319448
iteration : 13511
train acc:  0.84375
train loss:  0.39069974422454834
train gradient:  0.17323324464081996
iteration : 13512
train acc:  0.890625
train loss:  0.28363969922065735
train gradient:  0.15972865765076866
iteration : 13513
train acc:  0.875
train loss:  0.26817071437835693
train gradient:  0.10132412744092718
iteration : 13514
train acc:  0.8984375
train loss:  0.3362392485141754
train gradient:  0.1304452410327399
iteration : 13515
train acc:  0.8671875
train loss:  0.2746780514717102
train gradient:  0.14484824749838054
iteration : 13516
train acc:  0.8984375
train loss:  0.29251334071159363
train gradient:  0.11843120636490354
iteration : 13517
train acc:  0.828125
train loss:  0.3393559455871582
train gradient:  0.150694549096506
iteration : 13518
train acc:  0.8984375
train loss:  0.2810373902320862
train gradient:  0.13855602705092868
iteration : 13519
train acc:  0.859375
train loss:  0.30390459299087524
train gradient:  0.12467591412704365
iteration : 13520
train acc:  0.8203125
train loss:  0.3842586576938629
train gradient:  0.20805207098903958
iteration : 13521
train acc:  0.859375
train loss:  0.29317721724510193
train gradient:  0.12473660682647311
iteration : 13522
train acc:  0.84375
train loss:  0.36795902252197266
train gradient:  0.2116111847318463
iteration : 13523
train acc:  0.84375
train loss:  0.3770812749862671
train gradient:  0.24029206276321746
iteration : 13524
train acc:  0.828125
train loss:  0.3500608801841736
train gradient:  0.13960000864469502
iteration : 13525
train acc:  0.890625
train loss:  0.33839115500450134
train gradient:  0.18827182893607666
iteration : 13526
train acc:  0.8828125
train loss:  0.3200114369392395
train gradient:  0.16011421009549298
iteration : 13527
train acc:  0.7890625
train loss:  0.38390281796455383
train gradient:  0.21257786297144038
iteration : 13528
train acc:  0.890625
train loss:  0.26020997762680054
train gradient:  0.12277125841517368
iteration : 13529
train acc:  0.84375
train loss:  0.3878587484359741
train gradient:  0.22268557271686357
iteration : 13530
train acc:  0.90625
train loss:  0.2731732726097107
train gradient:  0.09674668303366331
iteration : 13531
train acc:  0.8515625
train loss:  0.30540257692337036
train gradient:  0.13116117611981498
iteration : 13532
train acc:  0.875
train loss:  0.29999202489852905
train gradient:  0.20632278065378934
iteration : 13533
train acc:  0.8671875
train loss:  0.304545134305954
train gradient:  0.13458854676929546
iteration : 13534
train acc:  0.859375
train loss:  0.2796564996242523
train gradient:  0.08694135551677028
iteration : 13535
train acc:  0.8984375
train loss:  0.2634317874908447
train gradient:  0.10674479305409046
iteration : 13536
train acc:  0.9140625
train loss:  0.24489232897758484
train gradient:  0.10019767250219047
iteration : 13537
train acc:  0.8359375
train loss:  0.38988643884658813
train gradient:  0.21269473150036092
iteration : 13538
train acc:  0.84375
train loss:  0.3268665075302124
train gradient:  0.1171434619176349
iteration : 13539
train acc:  0.859375
train loss:  0.3607374429702759
train gradient:  0.12732343493413192
iteration : 13540
train acc:  0.8828125
train loss:  0.2666131556034088
train gradient:  0.10579283122559402
iteration : 13541
train acc:  0.84375
train loss:  0.3381481170654297
train gradient:  0.1533690355430008
iteration : 13542
train acc:  0.859375
train loss:  0.2882026135921478
train gradient:  0.13588127161017466
iteration : 13543
train acc:  0.8828125
train loss:  0.2931770086288452
train gradient:  0.12932043163733775
iteration : 13544
train acc:  0.8828125
train loss:  0.2952437996864319
train gradient:  0.18905422854231185
iteration : 13545
train acc:  0.875
train loss:  0.27786511182785034
train gradient:  0.08612175249705227
iteration : 13546
train acc:  0.875
train loss:  0.29288339614868164
train gradient:  0.1502588170746093
iteration : 13547
train acc:  0.9140625
train loss:  0.2534444332122803
train gradient:  0.08759855468721524
iteration : 13548
train acc:  0.8828125
train loss:  0.2712852358818054
train gradient:  0.1473018385017703
iteration : 13549
train acc:  0.9140625
train loss:  0.22167561948299408
train gradient:  0.10057656569088587
iteration : 13550
train acc:  0.875
train loss:  0.25212374329566956
train gradient:  0.09454927116884357
iteration : 13551
train acc:  0.8515625
train loss:  0.3397296667098999
train gradient:  0.15828247906973625
iteration : 13552
train acc:  0.8828125
train loss:  0.28580909967422485
train gradient:  0.13390748005748976
iteration : 13553
train acc:  0.78125
train loss:  0.4319700598716736
train gradient:  0.23058608339671027
iteration : 13554
train acc:  0.875
train loss:  0.33114877343177795
train gradient:  0.14025262352569431
iteration : 13555
train acc:  0.8671875
train loss:  0.3381763696670532
train gradient:  0.15038524078920928
iteration : 13556
train acc:  0.9140625
train loss:  0.20285630226135254
train gradient:  0.07862204535863168
iteration : 13557
train acc:  0.90625
train loss:  0.27846550941467285
train gradient:  0.10574862315720637
iteration : 13558
train acc:  0.8828125
train loss:  0.29404884576797485
train gradient:  0.09551478197344004
iteration : 13559
train acc:  0.8515625
train loss:  0.34635698795318604
train gradient:  0.20514137367996793
iteration : 13560
train acc:  0.921875
train loss:  0.26221537590026855
train gradient:  0.08528137463246234
iteration : 13561
train acc:  0.875
train loss:  0.30265113711357117
train gradient:  0.21746963461883534
iteration : 13562
train acc:  0.859375
train loss:  0.29748106002807617
train gradient:  0.15952237357105806
iteration : 13563
train acc:  0.8359375
train loss:  0.34164804220199585
train gradient:  0.1581317505895964
iteration : 13564
train acc:  0.8515625
train loss:  0.2810544967651367
train gradient:  0.24598078073029125
iteration : 13565
train acc:  0.90625
train loss:  0.25723201036453247
train gradient:  0.12086787359730927
iteration : 13566
train acc:  0.8828125
train loss:  0.2979308068752289
train gradient:  0.10887632709158471
iteration : 13567
train acc:  0.8515625
train loss:  0.38909223675727844
train gradient:  0.16899765062303895
iteration : 13568
train acc:  0.8828125
train loss:  0.28356385231018066
train gradient:  0.1422926966040424
iteration : 13569
train acc:  0.8828125
train loss:  0.3050146698951721
train gradient:  0.11492803601467291
iteration : 13570
train acc:  0.859375
train loss:  0.3294597268104553
train gradient:  0.15254153856966773
iteration : 13571
train acc:  0.8359375
train loss:  0.3366817831993103
train gradient:  0.18176035458750944
iteration : 13572
train acc:  0.890625
train loss:  0.2865474224090576
train gradient:  0.14477022119597205
iteration : 13573
train acc:  0.8828125
train loss:  0.305855393409729
train gradient:  0.21162373328725437
iteration : 13574
train acc:  0.890625
train loss:  0.2687492370605469
train gradient:  0.1852230950503821
iteration : 13575
train acc:  0.875
train loss:  0.27870625257492065
train gradient:  0.0890670617446436
iteration : 13576
train acc:  0.8515625
train loss:  0.30243396759033203
train gradient:  0.1345377804039642
iteration : 13577
train acc:  0.875
train loss:  0.3390575349330902
train gradient:  0.17721620867477267
iteration : 13578
train acc:  0.8359375
train loss:  0.35077863931655884
train gradient:  0.14724934992853567
iteration : 13579
train acc:  0.828125
train loss:  0.3557097911834717
train gradient:  0.20369291990095809
iteration : 13580
train acc:  0.8359375
train loss:  0.3291698396205902
train gradient:  0.13818546426593473
iteration : 13581
train acc:  0.859375
train loss:  0.32809752225875854
train gradient:  0.19455547712265397
iteration : 13582
train acc:  0.7578125
train loss:  0.5051194429397583
train gradient:  0.2933171388430956
iteration : 13583
train acc:  0.8828125
train loss:  0.2890704572200775
train gradient:  0.12366558786597392
iteration : 13584
train acc:  0.8046875
train loss:  0.40245017409324646
train gradient:  0.1956560873242716
iteration : 13585
train acc:  0.8671875
train loss:  0.2955550253391266
train gradient:  0.16788850351439594
iteration : 13586
train acc:  0.8671875
train loss:  0.26422441005706787
train gradient:  0.11179554914620689
iteration : 13587
train acc:  0.8828125
train loss:  0.24566532671451569
train gradient:  0.12758702187418963
iteration : 13588
train acc:  0.9140625
train loss:  0.2352910041809082
train gradient:  0.07641065340019737
iteration : 13589
train acc:  0.8984375
train loss:  0.2583780288696289
train gradient:  0.09862423813717733
iteration : 13590
train acc:  0.8984375
train loss:  0.26689308881759644
train gradient:  0.12787820243127263
iteration : 13591
train acc:  0.859375
train loss:  0.2885078191757202
train gradient:  0.11748698910266629
iteration : 13592
train acc:  0.890625
train loss:  0.26169919967651367
train gradient:  0.13384591474200075
iteration : 13593
train acc:  0.8671875
train loss:  0.3068350553512573
train gradient:  0.11199368496192845
iteration : 13594
train acc:  0.859375
train loss:  0.31180042028427124
train gradient:  0.14927592053090288
iteration : 13595
train acc:  0.875
train loss:  0.2912197411060333
train gradient:  0.14757887130229863
iteration : 13596
train acc:  0.875
train loss:  0.34022390842437744
train gradient:  0.20030723591075794
iteration : 13597
train acc:  0.828125
train loss:  0.3282212018966675
train gradient:  0.151226306862623
iteration : 13598
train acc:  0.8125
train loss:  0.3434801697731018
train gradient:  0.1282274158212564
iteration : 13599
train acc:  0.875
train loss:  0.26842883229255676
train gradient:  0.09758588240051137
iteration : 13600
train acc:  0.8515625
train loss:  0.3632868528366089
train gradient:  0.14946904909666275
iteration : 13601
train acc:  0.8515625
train loss:  0.3834277391433716
train gradient:  0.17989051540113987
iteration : 13602
train acc:  0.8359375
train loss:  0.3758862614631653
train gradient:  0.19810384266305825
iteration : 13603
train acc:  0.796875
train loss:  0.43368202447891235
train gradient:  0.1899914693644142
iteration : 13604
train acc:  0.859375
train loss:  0.32260680198669434
train gradient:  0.15145227034141728
iteration : 13605
train acc:  0.890625
train loss:  0.2954775393009186
train gradient:  0.129554444355249
iteration : 13606
train acc:  0.90625
train loss:  0.2729870080947876
train gradient:  0.10587150348726306
iteration : 13607
train acc:  0.9140625
train loss:  0.23189178109169006
train gradient:  0.09405839715916708
iteration : 13608
train acc:  0.8828125
train loss:  0.3286958336830139
train gradient:  0.13276646369361184
iteration : 13609
train acc:  0.859375
train loss:  0.34778302907943726
train gradient:  0.12656320217976166
iteration : 13610
train acc:  0.90625
train loss:  0.215640589594841
train gradient:  0.09949814874686946
iteration : 13611
train acc:  0.8515625
train loss:  0.3275071382522583
train gradient:  0.1286600529489166
iteration : 13612
train acc:  0.8671875
train loss:  0.29543572664260864
train gradient:  0.4053360792095687
iteration : 13613
train acc:  0.8359375
train loss:  0.3160291910171509
train gradient:  0.11252626536925137
iteration : 13614
train acc:  0.8828125
train loss:  0.2888542413711548
train gradient:  0.1021873244616478
iteration : 13615
train acc:  0.8125
train loss:  0.36679476499557495
train gradient:  0.18894075640862146
iteration : 13616
train acc:  0.8828125
train loss:  0.2924656569957733
train gradient:  0.1361069069453129
iteration : 13617
train acc:  0.890625
train loss:  0.2903713583946228
train gradient:  0.11089670905751578
iteration : 13618
train acc:  0.875
train loss:  0.31368380784988403
train gradient:  0.13703505950000233
iteration : 13619
train acc:  0.8984375
train loss:  0.277828574180603
train gradient:  0.12003177501280353
iteration : 13620
train acc:  0.84375
train loss:  0.34485962986946106
train gradient:  0.12466203631627325
iteration : 13621
train acc:  0.90625
train loss:  0.2730982005596161
train gradient:  0.12621691576686056
iteration : 13622
train acc:  0.828125
train loss:  0.36594483256340027
train gradient:  0.19904565076223524
iteration : 13623
train acc:  0.8984375
train loss:  0.2573694884777069
train gradient:  0.11596563964658449
iteration : 13624
train acc:  0.875
train loss:  0.321039617061615
train gradient:  0.1545653675885892
iteration : 13625
train acc:  0.875
train loss:  0.2803683280944824
train gradient:  0.08542069119431896
iteration : 13626
train acc:  0.875
train loss:  0.3013518452644348
train gradient:  0.12009632162339093
iteration : 13627
train acc:  0.8125
train loss:  0.4351799190044403
train gradient:  0.1938961752869088
iteration : 13628
train acc:  0.875
train loss:  0.29032403230667114
train gradient:  0.1104644104367724
iteration : 13629
train acc:  0.859375
train loss:  0.3474193215370178
train gradient:  0.2558448736614938
iteration : 13630
train acc:  0.8984375
train loss:  0.25799208879470825
train gradient:  0.1318163879455984
iteration : 13631
train acc:  0.828125
train loss:  0.3321133553981781
train gradient:  0.1604904102215094
iteration : 13632
train acc:  0.90625
train loss:  0.2567959129810333
train gradient:  0.09143477052182178
iteration : 13633
train acc:  0.8203125
train loss:  0.37899404764175415
train gradient:  0.18221190365517903
iteration : 13634
train acc:  0.8515625
train loss:  0.33693191409111023
train gradient:  0.1718147966993091
iteration : 13635
train acc:  0.84375
train loss:  0.31741875410079956
train gradient:  0.18066010349870396
iteration : 13636
train acc:  0.8671875
train loss:  0.33239084482192993
train gradient:  0.1344539697100786
iteration : 13637
train acc:  0.8359375
train loss:  0.30447086691856384
train gradient:  0.16019990731985
iteration : 13638
train acc:  0.875
train loss:  0.312273770570755
train gradient:  0.1597754511397963
iteration : 13639
train acc:  0.796875
train loss:  0.34596124291419983
train gradient:  0.1576108901659447
iteration : 13640
train acc:  0.7578125
train loss:  0.5151364207267761
train gradient:  0.34929322528150786
iteration : 13641
train acc:  0.859375
train loss:  0.2846207022666931
train gradient:  0.09852601665791996
iteration : 13642
train acc:  0.7890625
train loss:  0.40920937061309814
train gradient:  0.18887367655290715
iteration : 13643
train acc:  0.8671875
train loss:  0.30256789922714233
train gradient:  0.11766217967636348
iteration : 13644
train acc:  0.84375
train loss:  0.3609316349029541
train gradient:  0.15763813036787463
iteration : 13645
train acc:  0.8359375
train loss:  0.38296371698379517
train gradient:  0.2214358165164384
iteration : 13646
train acc:  0.921875
train loss:  0.21318379044532776
train gradient:  0.10587899102115914
iteration : 13647
train acc:  0.8671875
train loss:  0.2798588275909424
train gradient:  0.1024415201435009
iteration : 13648
train acc:  0.90625
train loss:  0.22194547951221466
train gradient:  0.07606108973002569
iteration : 13649
train acc:  0.8671875
train loss:  0.30841898918151855
train gradient:  0.18551609746861802
iteration : 13650
train acc:  0.8515625
train loss:  0.3480633497238159
train gradient:  0.17124152540195545
iteration : 13651
train acc:  0.828125
train loss:  0.3569697141647339
train gradient:  0.12393643077096363
iteration : 13652
train acc:  0.9140625
train loss:  0.2090199738740921
train gradient:  0.07417664138442145
iteration : 13653
train acc:  0.859375
train loss:  0.3146910071372986
train gradient:  0.12132147203013216
iteration : 13654
train acc:  0.8984375
train loss:  0.27047502994537354
train gradient:  0.08031211813294639
iteration : 13655
train acc:  0.875
train loss:  0.3193659484386444
train gradient:  0.1412438324013378
iteration : 13656
train acc:  0.90625
train loss:  0.23629388213157654
train gradient:  0.10996051621629042
iteration : 13657
train acc:  0.859375
train loss:  0.34485727548599243
train gradient:  0.1930820536287571
iteration : 13658
train acc:  0.859375
train loss:  0.3539450764656067
train gradient:  0.1411358253086187
iteration : 13659
train acc:  0.8671875
train loss:  0.33455756306648254
train gradient:  0.12170224880105554
iteration : 13660
train acc:  0.8671875
train loss:  0.32589322328567505
train gradient:  0.10318022646005819
iteration : 13661
train acc:  0.8359375
train loss:  0.42267435789108276
train gradient:  0.17817648151519022
iteration : 13662
train acc:  0.8203125
train loss:  0.3606894016265869
train gradient:  0.18147774793946453
iteration : 13663
train acc:  0.8359375
train loss:  0.33734697103500366
train gradient:  0.22951632088474577
iteration : 13664
train acc:  0.8359375
train loss:  0.34821635484695435
train gradient:  0.14552730121709934
iteration : 13665
train acc:  0.8671875
train loss:  0.36910533905029297
train gradient:  0.13409391945768798
iteration : 13666
train acc:  0.90625
train loss:  0.3375255763530731
train gradient:  0.17944741483798976
iteration : 13667
train acc:  0.8671875
train loss:  0.279812753200531
train gradient:  0.0919784688039973
iteration : 13668
train acc:  0.8125
train loss:  0.3946961760520935
train gradient:  0.19306564799348788
iteration : 13669
train acc:  0.8125
train loss:  0.39589565992355347
train gradient:  0.25389104087473024
iteration : 13670
train acc:  0.8828125
train loss:  0.26589235663414
train gradient:  0.11577330148037442
iteration : 13671
train acc:  0.9140625
train loss:  0.2331206351518631
train gradient:  0.07961057846528345
iteration : 13672
train acc:  0.84375
train loss:  0.43016332387924194
train gradient:  0.2138969463553589
iteration : 13673
train acc:  0.84375
train loss:  0.43697232007980347
train gradient:  0.2319678990464727
iteration : 13674
train acc:  0.8828125
train loss:  0.28459715843200684
train gradient:  0.10807554601533025
iteration : 13675
train acc:  0.8359375
train loss:  0.3364940881729126
train gradient:  0.15619796580635162
iteration : 13676
train acc:  0.828125
train loss:  0.3462125062942505
train gradient:  0.16307441602753198
iteration : 13677
train acc:  0.8203125
train loss:  0.3150975704193115
train gradient:  0.1370854686832031
iteration : 13678
train acc:  0.8828125
train loss:  0.2617614269256592
train gradient:  0.11613815311255672
iteration : 13679
train acc:  0.875
train loss:  0.2854280471801758
train gradient:  0.11274230753103837
iteration : 13680
train acc:  0.84375
train loss:  0.3651301860809326
train gradient:  0.22607790960412322
iteration : 13681
train acc:  0.8671875
train loss:  0.3040478229522705
train gradient:  0.1690208618231216
iteration : 13682
train acc:  0.875
train loss:  0.25791847705841064
train gradient:  0.09587041406469457
iteration : 13683
train acc:  0.84375
train loss:  0.38482221961021423
train gradient:  0.16897988999071087
iteration : 13684
train acc:  0.8515625
train loss:  0.3132263422012329
train gradient:  0.1313242887068459
iteration : 13685
train acc:  0.8125
train loss:  0.38871967792510986
train gradient:  0.19286949606563242
iteration : 13686
train acc:  0.859375
train loss:  0.3055209815502167
train gradient:  0.14517292597066406
iteration : 13687
train acc:  0.875
train loss:  0.26391008496284485
train gradient:  0.10983290517122647
iteration : 13688
train acc:  0.9140625
train loss:  0.2410505712032318
train gradient:  0.10620417612319442
iteration : 13689
train acc:  0.8828125
train loss:  0.2945686876773834
train gradient:  0.1372578739329628
iteration : 13690
train acc:  0.921875
train loss:  0.2416420876979828
train gradient:  0.11368590222110395
iteration : 13691
train acc:  0.8671875
train loss:  0.3468402028083801
train gradient:  0.1335760687325347
iteration : 13692
train acc:  0.8671875
train loss:  0.2978343665599823
train gradient:  0.12937832688163692
iteration : 13693
train acc:  0.828125
train loss:  0.3226829171180725
train gradient:  0.169644238008532
iteration : 13694
train acc:  0.8671875
train loss:  0.3020373582839966
train gradient:  0.17494755694110775
iteration : 13695
train acc:  0.8671875
train loss:  0.2667753994464874
train gradient:  0.09818785153028368
iteration : 13696
train acc:  0.859375
train loss:  0.3621963858604431
train gradient:  0.18675927366544998
iteration : 13697
train acc:  0.84375
train loss:  0.3263525366783142
train gradient:  0.12810730290463102
iteration : 13698
train acc:  0.875
train loss:  0.3056647777557373
train gradient:  0.10498954457728754
iteration : 13699
train acc:  0.875
train loss:  0.35191118717193604
train gradient:  0.11906754323505069
iteration : 13700
train acc:  0.8671875
train loss:  0.3874646723270416
train gradient:  0.1660564596243069
iteration : 13701
train acc:  0.8984375
train loss:  0.29884833097457886
train gradient:  0.13465190512005884
iteration : 13702
train acc:  0.8359375
train loss:  0.3140101134777069
train gradient:  0.14334692032575203
iteration : 13703
train acc:  0.8828125
train loss:  0.23724821209907532
train gradient:  0.0814354743987671
iteration : 13704
train acc:  0.828125
train loss:  0.368632048368454
train gradient:  0.18050606359978744
iteration : 13705
train acc:  0.828125
train loss:  0.3119358420372009
train gradient:  0.14396552233657023
iteration : 13706
train acc:  0.9140625
train loss:  0.23389990627765656
train gradient:  0.08631472029529087
iteration : 13707
train acc:  0.859375
train loss:  0.3282696008682251
train gradient:  0.1617373827986472
iteration : 13708
train acc:  0.9140625
train loss:  0.24307255446910858
train gradient:  0.08551208577046739
iteration : 13709
train acc:  0.875
train loss:  0.32475554943084717
train gradient:  0.11565948778163962
iteration : 13710
train acc:  0.9296875
train loss:  0.2562861442565918
train gradient:  0.0812292762096675
iteration : 13711
train acc:  0.8359375
train loss:  0.3782620131969452
train gradient:  0.15155717143079045
iteration : 13712
train acc:  0.890625
train loss:  0.2456480860710144
train gradient:  0.13216272889049607
iteration : 13713
train acc:  0.8515625
train loss:  0.34102076292037964
train gradient:  0.11351032582504325
iteration : 13714
train acc:  0.8671875
train loss:  0.2749357223510742
train gradient:  0.12600698388560516
iteration : 13715
train acc:  0.8359375
train loss:  0.2871619462966919
train gradient:  0.14602261468116115
iteration : 13716
train acc:  0.8125
train loss:  0.395294189453125
train gradient:  0.2142127124668449
iteration : 13717
train acc:  0.8359375
train loss:  0.3265284597873688
train gradient:  0.095872867772051
iteration : 13718
train acc:  0.890625
train loss:  0.31170374155044556
train gradient:  0.12838674201701955
iteration : 13719
train acc:  0.84375
train loss:  0.29983973503112793
train gradient:  0.15643697400148082
iteration : 13720
train acc:  0.890625
train loss:  0.32389265298843384
train gradient:  0.12090158734333434
iteration : 13721
train acc:  0.9140625
train loss:  0.2542153596878052
train gradient:  0.07784382242914809
iteration : 13722
train acc:  0.828125
train loss:  0.33812329173088074
train gradient:  0.12004857410184597
iteration : 13723
train acc:  0.8515625
train loss:  0.32603132724761963
train gradient:  0.08643123635190446
iteration : 13724
train acc:  0.9296875
train loss:  0.25076761841773987
train gradient:  0.08225357115952715
iteration : 13725
train acc:  0.8125
train loss:  0.32633617520332336
train gradient:  0.15963572286139505
iteration : 13726
train acc:  0.828125
train loss:  0.3876631557941437
train gradient:  0.14282584272992568
iteration : 13727
train acc:  0.8671875
train loss:  0.29395371675491333
train gradient:  0.12380698078577249
iteration : 13728
train acc:  0.8515625
train loss:  0.3062012791633606
train gradient:  0.1501107794506343
iteration : 13729
train acc:  0.859375
train loss:  0.3639985918998718
train gradient:  0.16892582979740564
iteration : 13730
train acc:  0.9609375
train loss:  0.19237062335014343
train gradient:  0.07841721544127378
iteration : 13731
train acc:  0.890625
train loss:  0.2855263948440552
train gradient:  0.11396941662695013
iteration : 13732
train acc:  0.875
train loss:  0.2700113356113434
train gradient:  0.08034234169214464
iteration : 13733
train acc:  0.8828125
train loss:  0.312931090593338
train gradient:  0.16723981002916882
iteration : 13734
train acc:  0.84375
train loss:  0.3084012269973755
train gradient:  0.12990740839804543
iteration : 13735
train acc:  0.7890625
train loss:  0.4074207544326782
train gradient:  0.1893771054495751
iteration : 13736
train acc:  0.859375
train loss:  0.30931252241134644
train gradient:  0.12369264545813785
iteration : 13737
train acc:  0.890625
train loss:  0.2385311722755432
train gradient:  0.10103471308032869
iteration : 13738
train acc:  0.8203125
train loss:  0.4045531153678894
train gradient:  0.2885123979346472
iteration : 13739
train acc:  0.8359375
train loss:  0.3300940990447998
train gradient:  0.11732827645284374
iteration : 13740
train acc:  0.9296875
train loss:  0.22352376580238342
train gradient:  0.0887291820607148
iteration : 13741
train acc:  0.859375
train loss:  0.37596988677978516
train gradient:  0.26694505664033746
iteration : 13742
train acc:  0.875
train loss:  0.2864993214607239
train gradient:  0.08239101721852783
iteration : 13743
train acc:  0.8671875
train loss:  0.26563167572021484
train gradient:  0.1241757351544039
iteration : 13744
train acc:  0.875
train loss:  0.28892987966537476
train gradient:  0.10655547608391779
iteration : 13745
train acc:  0.8671875
train loss:  0.30748945474624634
train gradient:  0.1552030257251671
iteration : 13746
train acc:  0.8671875
train loss:  0.3002296984195709
train gradient:  0.16855614591507084
iteration : 13747
train acc:  0.90625
train loss:  0.24325093626976013
train gradient:  0.08026804716606518
iteration : 13748
train acc:  0.84375
train loss:  0.4365791082382202
train gradient:  0.5037684263620211
iteration : 13749
train acc:  0.890625
train loss:  0.271657794713974
train gradient:  0.08241130475161391
iteration : 13750
train acc:  0.828125
train loss:  0.35687926411628723
train gradient:  0.19659585250003309
iteration : 13751
train acc:  0.8828125
train loss:  0.25009673833847046
train gradient:  0.08549895902619577
iteration : 13752
train acc:  0.8671875
train loss:  0.31292545795440674
train gradient:  0.1199462343828793
iteration : 13753
train acc:  0.8125
train loss:  0.3494069576263428
train gradient:  0.1848324042125729
iteration : 13754
train acc:  0.875
train loss:  0.3344866633415222
train gradient:  0.11362831631116921
iteration : 13755
train acc:  0.828125
train loss:  0.4111075699329376
train gradient:  0.2791196617846243
iteration : 13756
train acc:  0.8359375
train loss:  0.36312055587768555
train gradient:  0.23387438370202746
iteration : 13757
train acc:  0.921875
train loss:  0.2649000287055969
train gradient:  0.07789644725493468
iteration : 13758
train acc:  0.875
train loss:  0.3336018919944763
train gradient:  0.13035197411493457
iteration : 13759
train acc:  0.84375
train loss:  0.37156927585601807
train gradient:  0.13470097132439657
iteration : 13760
train acc:  0.828125
train loss:  0.38113683462142944
train gradient:  0.16962262949111065
iteration : 13761
train acc:  0.90625
train loss:  0.25060299038887024
train gradient:  0.10904329127657625
iteration : 13762
train acc:  0.90625
train loss:  0.24167785048484802
train gradient:  0.0795860563980143
iteration : 13763
train acc:  0.875
train loss:  0.3050000071525574
train gradient:  0.1256890449007387
iteration : 13764
train acc:  0.8515625
train loss:  0.34042656421661377
train gradient:  0.116444099098805
iteration : 13765
train acc:  0.84375
train loss:  0.4165513515472412
train gradient:  0.2668213401100859
iteration : 13766
train acc:  0.859375
train loss:  0.29790234565734863
train gradient:  0.12224870223566783
iteration : 13767
train acc:  0.8515625
train loss:  0.31910616159439087
train gradient:  0.212945796968484
iteration : 13768
train acc:  0.875
train loss:  0.2902682423591614
train gradient:  0.2384230778062898
iteration : 13769
train acc:  0.84375
train loss:  0.3455137610435486
train gradient:  0.15786207500335672
iteration : 13770
train acc:  0.90625
train loss:  0.24746303260326385
train gradient:  0.10030492899170484
iteration : 13771
train acc:  0.875
train loss:  0.3110179305076599
train gradient:  0.08672271155394819
iteration : 13772
train acc:  0.8828125
train loss:  0.29389137029647827
train gradient:  0.09986002639472272
iteration : 13773
train acc:  0.859375
train loss:  0.3688667416572571
train gradient:  0.16165845962958447
iteration : 13774
train acc:  0.8125
train loss:  0.37872326374053955
train gradient:  0.12589673033494755
iteration : 13775
train acc:  0.8203125
train loss:  0.34939366579055786
train gradient:  0.1559460090925226
iteration : 13776
train acc:  0.8671875
train loss:  0.36097076535224915
train gradient:  0.140475195046447
iteration : 13777
train acc:  0.859375
train loss:  0.3326105773448944
train gradient:  0.11460121732792346
iteration : 13778
train acc:  0.8515625
train loss:  0.32376545667648315
train gradient:  0.1909690011755113
iteration : 13779
train acc:  0.8671875
train loss:  0.3312038779258728
train gradient:  0.1460711677025128
iteration : 13780
train acc:  0.8828125
train loss:  0.3069199323654175
train gradient:  0.1326974651156862
iteration : 13781
train acc:  0.890625
train loss:  0.26976245641708374
train gradient:  0.07593076054036497
iteration : 13782
train acc:  0.8828125
train loss:  0.3289435803890228
train gradient:  0.10563794987933395
iteration : 13783
train acc:  0.8671875
train loss:  0.26374951004981995
train gradient:  0.10102502940607103
iteration : 13784
train acc:  0.8671875
train loss:  0.28458207845687866
train gradient:  0.09676966746875107
iteration : 13785
train acc:  0.8359375
train loss:  0.3685277998447418
train gradient:  0.18873756209806566
iteration : 13786
train acc:  0.8515625
train loss:  0.3002859055995941
train gradient:  0.12857446077050216
iteration : 13787
train acc:  0.9140625
train loss:  0.2625059485435486
train gradient:  0.0639192083858531
iteration : 13788
train acc:  0.8984375
train loss:  0.30547896027565
train gradient:  0.11104435239499696
iteration : 13789
train acc:  0.8515625
train loss:  0.3382273018360138
train gradient:  0.17337819236517948
iteration : 13790
train acc:  0.875
train loss:  0.2757663130760193
train gradient:  0.12460800990878078
iteration : 13791
train acc:  0.859375
train loss:  0.35594213008880615
train gradient:  0.16687765671126908
iteration : 13792
train acc:  0.8984375
train loss:  0.2777300477027893
train gradient:  0.09721364191868624
iteration : 13793
train acc:  0.8125
train loss:  0.403646320104599
train gradient:  0.20838489285914358
iteration : 13794
train acc:  0.890625
train loss:  0.30828434228897095
train gradient:  0.14381661672387747
iteration : 13795
train acc:  0.890625
train loss:  0.24869677424430847
train gradient:  0.08064609773235083
iteration : 13796
train acc:  0.8125
train loss:  0.36388587951660156
train gradient:  0.1849772185441044
iteration : 13797
train acc:  0.875
train loss:  0.3239026665687561
train gradient:  0.0934783668745771
iteration : 13798
train acc:  0.8828125
train loss:  0.33216753602027893
train gradient:  0.1302602582331565
iteration : 13799
train acc:  0.8984375
train loss:  0.22705978155136108
train gradient:  0.09689867488712427
iteration : 13800
train acc:  0.859375
train loss:  0.2652992010116577
train gradient:  0.0982591772197403
iteration : 13801
train acc:  0.875
train loss:  0.26843446493148804
train gradient:  0.11976406337152301
iteration : 13802
train acc:  0.796875
train loss:  0.3904475271701813
train gradient:  0.18794573074920531
iteration : 13803
train acc:  0.9140625
train loss:  0.22544583678245544
train gradient:  0.07856918117291052
iteration : 13804
train acc:  0.8984375
train loss:  0.25414982438087463
train gradient:  0.09183755764071938
iteration : 13805
train acc:  0.921875
train loss:  0.2536787688732147
train gradient:  0.07786687828792728
iteration : 13806
train acc:  0.875
train loss:  0.273493230342865
train gradient:  0.10239518278663647
iteration : 13807
train acc:  0.8671875
train loss:  0.32448720932006836
train gradient:  0.10717455536656312
iteration : 13808
train acc:  0.859375
train loss:  0.30123886466026306
train gradient:  0.16118138418362965
iteration : 13809
train acc:  0.8515625
train loss:  0.3621198832988739
train gradient:  0.16891144226361976
iteration : 13810
train acc:  0.875
train loss:  0.28276997804641724
train gradient:  0.09172197557230953
iteration : 13811
train acc:  0.8671875
train loss:  0.31552010774612427
train gradient:  0.10157684203110674
iteration : 13812
train acc:  0.875
train loss:  0.3064594268798828
train gradient:  0.08728236362073473
iteration : 13813
train acc:  0.9140625
train loss:  0.23205460608005524
train gradient:  0.10246491598677311
iteration : 13814
train acc:  0.9140625
train loss:  0.2325161248445511
train gradient:  0.10003884444837777
iteration : 13815
train acc:  0.859375
train loss:  0.35304978489875793
train gradient:  0.15054758082188047
iteration : 13816
train acc:  0.828125
train loss:  0.3365938663482666
train gradient:  0.172863457712846
iteration : 13817
train acc:  0.875
train loss:  0.2815379202365875
train gradient:  0.11800342161034559
iteration : 13818
train acc:  0.90625
train loss:  0.27912408113479614
train gradient:  0.12965767240207018
iteration : 13819
train acc:  0.8828125
train loss:  0.33930009603500366
train gradient:  0.13042501726057937
iteration : 13820
train acc:  0.859375
train loss:  0.30306726694107056
train gradient:  0.09805212658966815
iteration : 13821
train acc:  0.84375
train loss:  0.3456689119338989
train gradient:  0.14432212432815894
iteration : 13822
train acc:  0.8828125
train loss:  0.32488077878952026
train gradient:  0.09401126132621158
iteration : 13823
train acc:  0.8515625
train loss:  0.35194826126098633
train gradient:  0.15971718520862654
iteration : 13824
train acc:  0.875
train loss:  0.30808669328689575
train gradient:  0.18092829906551489
iteration : 13825
train acc:  0.859375
train loss:  0.284582257270813
train gradient:  0.13522448962268402
iteration : 13826
train acc:  0.875
train loss:  0.31233835220336914
train gradient:  0.15450057044494117
iteration : 13827
train acc:  0.8046875
train loss:  0.4321867823600769
train gradient:  0.26291990017077893
iteration : 13828
train acc:  0.828125
train loss:  0.34354519844055176
train gradient:  0.17334690714034737
iteration : 13829
train acc:  0.84375
train loss:  0.33653730154037476
train gradient:  0.17123254942321892
iteration : 13830
train acc:  0.859375
train loss:  0.2942620515823364
train gradient:  0.12220425337711308
iteration : 13831
train acc:  0.8046875
train loss:  0.3720608651638031
train gradient:  0.17170566470228354
iteration : 13832
train acc:  0.8984375
train loss:  0.2769494652748108
train gradient:  0.10519100240473343
iteration : 13833
train acc:  0.859375
train loss:  0.3241908848285675
train gradient:  0.11626341501187513
iteration : 13834
train acc:  0.8671875
train loss:  0.3351200819015503
train gradient:  0.12407187934685615
iteration : 13835
train acc:  0.8828125
train loss:  0.30207759141921997
train gradient:  0.11305602049498695
iteration : 13836
train acc:  0.8515625
train loss:  0.29055824875831604
train gradient:  0.1591545217503923
iteration : 13837
train acc:  0.8828125
train loss:  0.21945658326148987
train gradient:  0.06985041318830862
iteration : 13838
train acc:  0.8984375
train loss:  0.22218617796897888
train gradient:  0.06938121103014655
iteration : 13839
train acc:  0.8515625
train loss:  0.28521448373794556
train gradient:  0.07671582409443264
iteration : 13840
train acc:  0.875
train loss:  0.27801764011383057
train gradient:  0.11514405734296612
iteration : 13841
train acc:  0.8671875
train loss:  0.3341503143310547
train gradient:  0.1479173883653544
iteration : 13842
train acc:  0.765625
train loss:  0.48079007863998413
train gradient:  0.2801435093651131
iteration : 13843
train acc:  0.8671875
train loss:  0.31070151925086975
train gradient:  0.12557550935338216
iteration : 13844
train acc:  0.84375
train loss:  0.3344271183013916
train gradient:  0.12845624814136386
iteration : 13845
train acc:  0.8125
train loss:  0.4016116261482239
train gradient:  0.16422331399435797
iteration : 13846
train acc:  0.859375
train loss:  0.30533015727996826
train gradient:  0.10956840738057716
iteration : 13847
train acc:  0.8046875
train loss:  0.4078030288219452
train gradient:  0.21974547361747243
iteration : 13848
train acc:  0.84375
train loss:  0.2699277698993683
train gradient:  0.09827200065094199
iteration : 13849
train acc:  0.8515625
train loss:  0.3835371136665344
train gradient:  0.18044715810801676
iteration : 13850
train acc:  0.8671875
train loss:  0.32990172505378723
train gradient:  0.10771231730330896
iteration : 13851
train acc:  0.8984375
train loss:  0.27718326449394226
train gradient:  0.1016452570940938
iteration : 13852
train acc:  0.890625
train loss:  0.25759071111679077
train gradient:  0.10670843606791836
iteration : 13853
train acc:  0.8359375
train loss:  0.3786923289299011
train gradient:  0.1887398545205095
iteration : 13854
train acc:  0.8984375
train loss:  0.29125696420669556
train gradient:  0.1100414133917761
iteration : 13855
train acc:  0.828125
train loss:  0.3729693591594696
train gradient:  0.20521358282193747
iteration : 13856
train acc:  0.875
train loss:  0.3082166314125061
train gradient:  0.09816240424161003
iteration : 13857
train acc:  0.875
train loss:  0.2731674313545227
train gradient:  0.12101000449923555
iteration : 13858
train acc:  0.8984375
train loss:  0.2881948947906494
train gradient:  0.11196597973313557
iteration : 13859
train acc:  0.828125
train loss:  0.35330671072006226
train gradient:  0.11419826553845655
iteration : 13860
train acc:  0.8828125
train loss:  0.31352078914642334
train gradient:  0.11441733828737638
iteration : 13861
train acc:  0.8984375
train loss:  0.2392473816871643
train gradient:  0.08777572440513065
iteration : 13862
train acc:  0.8359375
train loss:  0.35975944995880127
train gradient:  0.15581992976136705
iteration : 13863
train acc:  0.859375
train loss:  0.3408169150352478
train gradient:  0.13956568631393412
iteration : 13864
train acc:  0.8984375
train loss:  0.24748504161834717
train gradient:  0.08039728966165008
iteration : 13865
train acc:  0.8203125
train loss:  0.3202419877052307
train gradient:  0.10937906892918518
iteration : 13866
train acc:  0.859375
train loss:  0.34906262159347534
train gradient:  0.14151814683659408
iteration : 13867
train acc:  0.890625
train loss:  0.29054582118988037
train gradient:  0.12383465436690215
iteration : 13868
train acc:  0.8984375
train loss:  0.25638338923454285
train gradient:  0.10185374993614323
iteration : 13869
train acc:  0.875
train loss:  0.35634276270866394
train gradient:  0.1702259136005399
iteration : 13870
train acc:  0.859375
train loss:  0.37390244007110596
train gradient:  0.15623854815428903
iteration : 13871
train acc:  0.8203125
train loss:  0.322270005941391
train gradient:  0.12932363154136453
iteration : 13872
train acc:  0.8984375
train loss:  0.28505975008010864
train gradient:  0.1012638920593622
iteration : 13873
train acc:  0.84375
train loss:  0.30872464179992676
train gradient:  0.11522584116416772
iteration : 13874
train acc:  0.8984375
train loss:  0.2593202292919159
train gradient:  0.07913732612014343
iteration : 13875
train acc:  0.890625
train loss:  0.2987522482872009
train gradient:  0.13900771111685548
iteration : 13876
train acc:  0.859375
train loss:  0.33041244745254517
train gradient:  0.12880282211905794
iteration : 13877
train acc:  0.8671875
train loss:  0.3035879135131836
train gradient:  0.12972447625387293
iteration : 13878
train acc:  0.84375
train loss:  0.38637953996658325
train gradient:  0.19447874719740676
iteration : 13879
train acc:  0.828125
train loss:  0.36821550130844116
train gradient:  0.17042670477589217
iteration : 13880
train acc:  0.8671875
train loss:  0.3294074833393097
train gradient:  0.11692296797674807
iteration : 13881
train acc:  0.8359375
train loss:  0.3649136424064636
train gradient:  0.19893569591994947
iteration : 13882
train acc:  0.8984375
train loss:  0.26758265495300293
train gradient:  0.0942078057541715
iteration : 13883
train acc:  0.875
train loss:  0.24796068668365479
train gradient:  0.09301125419227609
iteration : 13884
train acc:  0.859375
train loss:  0.37855204939842224
train gradient:  0.16718913952936973
iteration : 13885
train acc:  0.8828125
train loss:  0.23599492013454437
train gradient:  0.0772177628890719
iteration : 13886
train acc:  0.8203125
train loss:  0.4493109881877899
train gradient:  0.2751336314642183
iteration : 13887
train acc:  0.859375
train loss:  0.29347991943359375
train gradient:  0.09257986872061823
iteration : 13888
train acc:  0.8828125
train loss:  0.24537751078605652
train gradient:  0.11303690664913144
iteration : 13889
train acc:  0.921875
train loss:  0.21644431352615356
train gradient:  0.05779939402562474
iteration : 13890
train acc:  0.8515625
train loss:  0.34629011154174805
train gradient:  0.15846545192676365
iteration : 13891
train acc:  0.8359375
train loss:  0.3392174541950226
train gradient:  0.1441894892430619
iteration : 13892
train acc:  0.9140625
train loss:  0.2518467307090759
train gradient:  0.08376600400142092
iteration : 13893
train acc:  0.84375
train loss:  0.3182814121246338
train gradient:  0.1332222053661456
iteration : 13894
train acc:  0.84375
train loss:  0.32567721605300903
train gradient:  0.10602951800267585
iteration : 13895
train acc:  0.9140625
train loss:  0.25928330421447754
train gradient:  0.06926305284117457
iteration : 13896
train acc:  0.8359375
train loss:  0.3743923306465149
train gradient:  0.13171871357645792
iteration : 13897
train acc:  0.84375
train loss:  0.3223533034324646
train gradient:  0.2040809052637853
iteration : 13898
train acc:  0.859375
train loss:  0.2908479571342468
train gradient:  0.14225575168857935
iteration : 13899
train acc:  0.8671875
train loss:  0.2483021765947342
train gradient:  0.07915795557844281
iteration : 13900
train acc:  0.8359375
train loss:  0.3988885283470154
train gradient:  0.15245351224222578
iteration : 13901
train acc:  0.84375
train loss:  0.3113711476325989
train gradient:  0.16875287795390514
iteration : 13902
train acc:  0.8984375
train loss:  0.216229647397995
train gradient:  0.09311711372484088
iteration : 13903
train acc:  0.859375
train loss:  0.3197578191757202
train gradient:  0.09723165612960422
iteration : 13904
train acc:  0.84375
train loss:  0.36444714665412903
train gradient:  0.14079360428207682
iteration : 13905
train acc:  0.875
train loss:  0.2909772992134094
train gradient:  0.08770053158961874
iteration : 13906
train acc:  0.859375
train loss:  0.3092891275882721
train gradient:  0.10323112894063283
iteration : 13907
train acc:  0.8671875
train loss:  0.3169538974761963
train gradient:  0.17232258852647167
iteration : 13908
train acc:  0.859375
train loss:  0.3743330240249634
train gradient:  0.17353124640511686
iteration : 13909
train acc:  0.859375
train loss:  0.3057815432548523
train gradient:  0.14338253715878121
iteration : 13910
train acc:  0.8359375
train loss:  0.3428155779838562
train gradient:  0.14816515630563093
iteration : 13911
train acc:  0.828125
train loss:  0.36356979608535767
train gradient:  0.17454656201043092
iteration : 13912
train acc:  0.84375
train loss:  0.3680734932422638
train gradient:  0.1757471584823072
iteration : 13913
train acc:  0.890625
train loss:  0.25733035802841187
train gradient:  0.07853688038789891
iteration : 13914
train acc:  0.8984375
train loss:  0.2975286543369293
train gradient:  0.10083541613765162
iteration : 13915
train acc:  0.890625
train loss:  0.319710910320282
train gradient:  0.13201832811420144
iteration : 13916
train acc:  0.875
train loss:  0.35488200187683105
train gradient:  0.1584140916437313
iteration : 13917
train acc:  0.8046875
train loss:  0.40246763825416565
train gradient:  0.1645298273882747
iteration : 13918
train acc:  0.875
train loss:  0.2686392664909363
train gradient:  0.09337138620695919
iteration : 13919
train acc:  0.875
train loss:  0.2878169119358063
train gradient:  0.0856106966745382
iteration : 13920
train acc:  0.859375
train loss:  0.31026583909988403
train gradient:  0.12043096877929024
iteration : 13921
train acc:  0.8984375
train loss:  0.2313835471868515
train gradient:  0.06205427079199361
iteration : 13922
train acc:  0.84375
train loss:  0.3825973570346832
train gradient:  0.15533700045957133
iteration : 13923
train acc:  0.828125
train loss:  0.3752135634422302
train gradient:  0.13092773187672457
iteration : 13924
train acc:  0.7890625
train loss:  0.45001494884490967
train gradient:  0.17403063148468967
iteration : 13925
train acc:  0.875
train loss:  0.30086618661880493
train gradient:  0.11825314104061721
iteration : 13926
train acc:  0.8359375
train loss:  0.3965928554534912
train gradient:  0.1498783310099306
iteration : 13927
train acc:  0.8515625
train loss:  0.36537909507751465
train gradient:  0.14901190717504442
iteration : 13928
train acc:  0.8515625
train loss:  0.39600545167922974
train gradient:  0.17971780509985003
iteration : 13929
train acc:  0.8046875
train loss:  0.39458176493644714
train gradient:  0.22291522338745792
iteration : 13930
train acc:  0.8828125
train loss:  0.2795211672782898
train gradient:  0.19537425148981238
iteration : 13931
train acc:  0.828125
train loss:  0.3126989006996155
train gradient:  0.10949028072858691
iteration : 13932
train acc:  0.8203125
train loss:  0.35544705390930176
train gradient:  0.143796117600775
iteration : 13933
train acc:  0.90625
train loss:  0.3177574872970581
train gradient:  0.09689038268014943
iteration : 13934
train acc:  0.90625
train loss:  0.25772738456726074
train gradient:  0.07276530353493944
iteration : 13935
train acc:  0.8671875
train loss:  0.3032335638999939
train gradient:  0.10478788899810275
iteration : 13936
train acc:  0.890625
train loss:  0.25409799814224243
train gradient:  0.06929583082729213
iteration : 13937
train acc:  0.8203125
train loss:  0.39055028557777405
train gradient:  0.15414514807717455
iteration : 13938
train acc:  0.8828125
train loss:  0.2549256682395935
train gradient:  0.09205790543063234
iteration : 13939
train acc:  0.8203125
train loss:  0.39576414227485657
train gradient:  0.14900494983097254
iteration : 13940
train acc:  0.890625
train loss:  0.2971128821372986
train gradient:  0.15633282191640166
iteration : 13941
train acc:  0.859375
train loss:  0.36395883560180664
train gradient:  0.21015827353528477
iteration : 13942
train acc:  0.90625
train loss:  0.2710447609424591
train gradient:  0.11234120001249671
iteration : 13943
train acc:  0.828125
train loss:  0.40390124917030334
train gradient:  0.21535095704168733
iteration : 13944
train acc:  0.90625
train loss:  0.2541572153568268
train gradient:  0.2229935452875106
iteration : 13945
train acc:  0.875
train loss:  0.2851993143558502
train gradient:  0.10240389253039955
iteration : 13946
train acc:  0.90625
train loss:  0.25598394870758057
train gradient:  0.09566936722070549
iteration : 13947
train acc:  0.8203125
train loss:  0.3546897768974304
train gradient:  0.14803587541745178
iteration : 13948
train acc:  0.8203125
train loss:  0.399757444858551
train gradient:  0.17709985906391856
iteration : 13949
train acc:  0.875
train loss:  0.29750242829322815
train gradient:  0.1800484172902611
iteration : 13950
train acc:  0.796875
train loss:  0.4295135736465454
train gradient:  0.24710908592907294
iteration : 13951
train acc:  0.84375
train loss:  0.32387787103652954
train gradient:  0.1517243959160359
iteration : 13952
train acc:  0.8359375
train loss:  0.3652459979057312
train gradient:  0.1532545235989884
iteration : 13953
train acc:  0.8671875
train loss:  0.28987154364585876
train gradient:  0.08689443532086363
iteration : 13954
train acc:  0.859375
train loss:  0.36023539304733276
train gradient:  0.19008394770922377
iteration : 13955
train acc:  0.8984375
train loss:  0.2529996931552887
train gradient:  0.0921725011907253
iteration : 13956
train acc:  0.8671875
train loss:  0.27302390336990356
train gradient:  0.09134569986894658
iteration : 13957
train acc:  0.8671875
train loss:  0.31121137738227844
train gradient:  0.13681320130632404
iteration : 13958
train acc:  0.890625
train loss:  0.3351762294769287
train gradient:  0.14534658240473652
iteration : 13959
train acc:  0.890625
train loss:  0.30134347081184387
train gradient:  0.09527721827958974
iteration : 13960
train acc:  0.8828125
train loss:  0.2762157917022705
train gradient:  0.07802213152808442
iteration : 13961
train acc:  0.8828125
train loss:  0.2685530185699463
train gradient:  0.07887470519912029
iteration : 13962
train acc:  0.84375
train loss:  0.3225313425064087
train gradient:  0.14299271888908135
iteration : 13963
train acc:  0.8359375
train loss:  0.3549277186393738
train gradient:  0.12629487207711682
iteration : 13964
train acc:  0.8515625
train loss:  0.36040568351745605
train gradient:  0.1641967550909626
iteration : 13965
train acc:  0.8203125
train loss:  0.3751285672187805
train gradient:  0.13501445433617196
iteration : 13966
train acc:  0.8515625
train loss:  0.3298632502555847
train gradient:  0.11953517201083545
iteration : 13967
train acc:  0.859375
train loss:  0.3167235255241394
train gradient:  0.14659727442738776
iteration : 13968
train acc:  0.8125
train loss:  0.34226468205451965
train gradient:  0.12846541322706492
iteration : 13969
train acc:  0.859375
train loss:  0.33304715156555176
train gradient:  0.13141125531963516
iteration : 13970
train acc:  0.8828125
train loss:  0.3262028396129608
train gradient:  0.139704208683253
iteration : 13971
train acc:  0.890625
train loss:  0.26163250207901
train gradient:  0.2439874769878438
iteration : 13972
train acc:  0.8671875
train loss:  0.3077445328235626
train gradient:  0.10749429255802587
iteration : 13973
train acc:  0.9140625
train loss:  0.2491019070148468
train gradient:  0.0827851475090588
iteration : 13974
train acc:  0.859375
train loss:  0.2904106378555298
train gradient:  0.1616988549652551
iteration : 13975
train acc:  0.84375
train loss:  0.35214608907699585
train gradient:  0.14114478742292794
iteration : 13976
train acc:  0.859375
train loss:  0.31815117597579956
train gradient:  0.1256995869654492
iteration : 13977
train acc:  0.859375
train loss:  0.3498915731906891
train gradient:  0.15571946098525985
iteration : 13978
train acc:  0.859375
train loss:  0.34721118211746216
train gradient:  0.15972773070896856
iteration : 13979
train acc:  0.859375
train loss:  0.37101948261260986
train gradient:  0.16390471531900982
iteration : 13980
train acc:  0.859375
train loss:  0.3302972912788391
train gradient:  0.1164582555362972
iteration : 13981
train acc:  0.890625
train loss:  0.27807313203811646
train gradient:  0.18176339217214432
iteration : 13982
train acc:  0.8359375
train loss:  0.3134046494960785
train gradient:  0.13551181325723655
iteration : 13983
train acc:  0.8359375
train loss:  0.3418964743614197
train gradient:  0.17323468587746685
iteration : 13984
train acc:  0.8828125
train loss:  0.263288676738739
train gradient:  0.09229576620882471
iteration : 13985
train acc:  0.859375
train loss:  0.3343707323074341
train gradient:  0.17224590248373428
iteration : 13986
train acc:  0.8515625
train loss:  0.3128400444984436
train gradient:  0.17087404957456065
iteration : 13987
train acc:  0.8671875
train loss:  0.27158066630363464
train gradient:  0.09979333034046806
iteration : 13988
train acc:  0.859375
train loss:  0.32089781761169434
train gradient:  0.12172004193636639
iteration : 13989
train acc:  0.859375
train loss:  0.3285865783691406
train gradient:  0.1097726895808985
iteration : 13990
train acc:  0.8671875
train loss:  0.33084243535995483
train gradient:  0.17056921692683635
iteration : 13991
train acc:  0.8359375
train loss:  0.36525070667266846
train gradient:  0.1415048535142156
iteration : 13992
train acc:  0.90625
train loss:  0.28471988439559937
train gradient:  0.11173850412424768
iteration : 13993
train acc:  0.875
train loss:  0.27259308099746704
train gradient:  0.09255991551297289
iteration : 13994
train acc:  0.8984375
train loss:  0.29939520359039307
train gradient:  0.11126688821175562
iteration : 13995
train acc:  0.890625
train loss:  0.2938244938850403
train gradient:  0.08094141838208822
iteration : 13996
train acc:  0.8671875
train loss:  0.3153666853904724
train gradient:  0.14025906953962541
iteration : 13997
train acc:  0.8125
train loss:  0.41485071182250977
train gradient:  0.1731198190537206
iteration : 13998
train acc:  0.8203125
train loss:  0.3748908042907715
train gradient:  0.19796985121283076
iteration : 13999
train acc:  0.8828125
train loss:  0.2827901542186737
train gradient:  0.13774149805842606
iteration : 14000
train acc:  0.8515625
train loss:  0.3160492479801178
train gradient:  0.18324524180778143
iteration : 14001
train acc:  0.875
train loss:  0.26017868518829346
train gradient:  0.08027535833376667
iteration : 14002
train acc:  0.8828125
train loss:  0.28243082761764526
train gradient:  0.15170272066833795
iteration : 14003
train acc:  0.8671875
train loss:  0.31503552198410034
train gradient:  0.09637306848017103
iteration : 14004
train acc:  0.7734375
train loss:  0.40121498703956604
train gradient:  0.1621942544145579
iteration : 14005
train acc:  0.8984375
train loss:  0.2636426091194153
train gradient:  0.11314509358382789
iteration : 14006
train acc:  0.8984375
train loss:  0.29551050066947937
train gradient:  0.09830198523232357
iteration : 14007
train acc:  0.8125
train loss:  0.3783748745918274
train gradient:  0.16500604410812547
iteration : 14008
train acc:  0.8984375
train loss:  0.27521997690200806
train gradient:  0.10783464862091502
iteration : 14009
train acc:  0.828125
train loss:  0.4347109794616699
train gradient:  0.3499057194717476
iteration : 14010
train acc:  0.8671875
train loss:  0.3295949101448059
train gradient:  0.1215431086525469
iteration : 14011
train acc:  0.9140625
train loss:  0.24279071390628815
train gradient:  0.06998859168372026
iteration : 14012
train acc:  0.9296875
train loss:  0.21932828426361084
train gradient:  0.0800227999337087
iteration : 14013
train acc:  0.8125
train loss:  0.4211530089378357
train gradient:  0.21526684909864097
iteration : 14014
train acc:  0.859375
train loss:  0.35183781385421753
train gradient:  0.12345829673074632
iteration : 14015
train acc:  0.8046875
train loss:  0.37246403098106384
train gradient:  0.12617338155232763
iteration : 14016
train acc:  0.875
train loss:  0.30006295442581177
train gradient:  0.1918452978472282
iteration : 14017
train acc:  0.8203125
train loss:  0.35470083355903625
train gradient:  0.11874162658305897
iteration : 14018
train acc:  0.890625
train loss:  0.27294647693634033
train gradient:  0.13121741355287214
iteration : 14019
train acc:  0.8671875
train loss:  0.28264960646629333
train gradient:  0.0841626298041071
iteration : 14020
train acc:  0.84375
train loss:  0.34365221858024597
train gradient:  0.13951533576158817
iteration : 14021
train acc:  0.8125
train loss:  0.40423494577407837
train gradient:  0.1719933366166489
iteration : 14022
train acc:  0.8984375
train loss:  0.24867233633995056
train gradient:  0.1014315334000149
iteration : 14023
train acc:  0.8515625
train loss:  0.31929945945739746
train gradient:  0.11837064815902988
iteration : 14024
train acc:  0.8203125
train loss:  0.3584486246109009
train gradient:  0.17666390174097618
iteration : 14025
train acc:  0.859375
train loss:  0.3118332326412201
train gradient:  0.14152657420049666
iteration : 14026
train acc:  0.8671875
train loss:  0.3265087604522705
train gradient:  0.16842536397565355
iteration : 14027
train acc:  0.8203125
train loss:  0.39505407214164734
train gradient:  0.3087349337495235
iteration : 14028
train acc:  0.921875
train loss:  0.23630481958389282
train gradient:  0.10346120344369249
iteration : 14029
train acc:  0.8671875
train loss:  0.2627403736114502
train gradient:  0.09795486941249108
iteration : 14030
train acc:  0.8203125
train loss:  0.35446619987487793
train gradient:  0.15651647555545634
iteration : 14031
train acc:  0.890625
train loss:  0.3297866880893707
train gradient:  0.1477125743880986
iteration : 14032
train acc:  0.8515625
train loss:  0.31094712018966675
train gradient:  0.15501716001247712
iteration : 14033
train acc:  0.875
train loss:  0.28323811292648315
train gradient:  0.09645463006269349
iteration : 14034
train acc:  0.8671875
train loss:  0.3489210605621338
train gradient:  0.14439726662774421
iteration : 14035
train acc:  0.875
train loss:  0.32324787974357605
train gradient:  0.12577648567939925
iteration : 14036
train acc:  0.8828125
train loss:  0.2808353900909424
train gradient:  0.07368312365750158
iteration : 14037
train acc:  0.8359375
train loss:  0.36338603496551514
train gradient:  0.145282588389567
iteration : 14038
train acc:  0.859375
train loss:  0.3600129187107086
train gradient:  0.17408053690025294
iteration : 14039
train acc:  0.875
train loss:  0.34069254994392395
train gradient:  0.14321677784111797
iteration : 14040
train acc:  0.8671875
train loss:  0.3227853775024414
train gradient:  0.10463277039426816
iteration : 14041
train acc:  0.84375
train loss:  0.3526611328125
train gradient:  0.19057493274995357
iteration : 14042
train acc:  0.859375
train loss:  0.30809351801872253
train gradient:  0.11522544649462783
iteration : 14043
train acc:  0.875
train loss:  0.30263552069664
train gradient:  0.09807621534928385
iteration : 14044
train acc:  0.828125
train loss:  0.3806244730949402
train gradient:  0.20303762338811115
iteration : 14045
train acc:  0.859375
train loss:  0.3678912818431854
train gradient:  0.13258064236277456
iteration : 14046
train acc:  0.8515625
train loss:  0.3269374966621399
train gradient:  0.13634739310546934
iteration : 14047
train acc:  0.9140625
train loss:  0.24368730187416077
train gradient:  0.09269136976912223
iteration : 14048
train acc:  0.8828125
train loss:  0.26615771651268005
train gradient:  0.06327528595506299
iteration : 14049
train acc:  0.859375
train loss:  0.269491970539093
train gradient:  0.1710493391457507
iteration : 14050
train acc:  0.8984375
train loss:  0.2375747710466385
train gradient:  0.13121898785855257
iteration : 14051
train acc:  0.8359375
train loss:  0.3182017207145691
train gradient:  0.09351789992159762
iteration : 14052
train acc:  0.8828125
train loss:  0.2924635708332062
train gradient:  0.09989886972654259
iteration : 14053
train acc:  0.84375
train loss:  0.35830289125442505
train gradient:  0.16787741495047662
iteration : 14054
train acc:  0.828125
train loss:  0.27519991993904114
train gradient:  0.09416902313017472
iteration : 14055
train acc:  0.8828125
train loss:  0.295250803232193
train gradient:  0.09648404362058997
iteration : 14056
train acc:  0.8828125
train loss:  0.3115949034690857
train gradient:  0.1463035504291394
iteration : 14057
train acc:  0.8671875
train loss:  0.28609761595726013
train gradient:  0.1257757557153909
iteration : 14058
train acc:  0.8671875
train loss:  0.26327231526374817
train gradient:  0.11865974026398897
iteration : 14059
train acc:  0.8671875
train loss:  0.2906242609024048
train gradient:  0.09290928208911615
iteration : 14060
train acc:  0.84375
train loss:  0.3293360471725464
train gradient:  0.16793263829579075
iteration : 14061
train acc:  0.8828125
train loss:  0.265447735786438
train gradient:  0.07451460294045355
iteration : 14062
train acc:  0.859375
train loss:  0.3171504735946655
train gradient:  0.1516081288630088
iteration : 14063
train acc:  0.84375
train loss:  0.330659955739975
train gradient:  0.13647658144512362
iteration : 14064
train acc:  0.90625
train loss:  0.2385682612657547
train gradient:  0.06603289481987669
iteration : 14065
train acc:  0.8828125
train loss:  0.3215476870536804
train gradient:  0.1684700993363208
iteration : 14066
train acc:  0.8515625
train loss:  0.39125126600265503
train gradient:  0.19413451799174586
iteration : 14067
train acc:  0.875
train loss:  0.2805396020412445
train gradient:  0.1653392658668193
iteration : 14068
train acc:  0.8828125
train loss:  0.29025381803512573
train gradient:  0.1497929383404022
iteration : 14069
train acc:  0.8984375
train loss:  0.2921033501625061
train gradient:  0.09296913001239217
iteration : 14070
train acc:  0.875
train loss:  0.2711145281791687
train gradient:  0.10383347398247551
iteration : 14071
train acc:  0.84375
train loss:  0.37059929966926575
train gradient:  0.14915422869371484
iteration : 14072
train acc:  0.8828125
train loss:  0.2766203284263611
train gradient:  0.10005122435605517
iteration : 14073
train acc:  0.875
train loss:  0.30846545100212097
train gradient:  0.10124757262167021
iteration : 14074
train acc:  0.828125
train loss:  0.344760924577713
train gradient:  0.2142242020564391
iteration : 14075
train acc:  0.7890625
train loss:  0.4775666296482086
train gradient:  0.2796117348940627
iteration : 14076
train acc:  0.859375
train loss:  0.3330739140510559
train gradient:  0.16978502517755045
iteration : 14077
train acc:  0.8359375
train loss:  0.30454105138778687
train gradient:  0.16994966458094046
iteration : 14078
train acc:  0.8828125
train loss:  0.2885194420814514
train gradient:  0.0736286157388572
iteration : 14079
train acc:  0.8984375
train loss:  0.3181397318840027
train gradient:  0.10840517854957832
iteration : 14080
train acc:  0.890625
train loss:  0.3130134344100952
train gradient:  0.14252693916567943
iteration : 14081
train acc:  0.828125
train loss:  0.3725048303604126
train gradient:  0.18844465454578604
iteration : 14082
train acc:  0.828125
train loss:  0.41010525822639465
train gradient:  0.17723838717194632
iteration : 14083
train acc:  0.8828125
train loss:  0.32447683811187744
train gradient:  0.13390753761748583
iteration : 14084
train acc:  0.84375
train loss:  0.3448517918586731
train gradient:  0.09263468018914664
iteration : 14085
train acc:  0.890625
train loss:  0.31378889083862305
train gradient:  0.12687253089671877
iteration : 14086
train acc:  0.90625
train loss:  0.24397343397140503
train gradient:  0.0965949827318353
iteration : 14087
train acc:  0.8671875
train loss:  0.30326855182647705
train gradient:  0.1369756964635768
iteration : 14088
train acc:  0.859375
train loss:  0.29414308071136475
train gradient:  0.12048946657935476
iteration : 14089
train acc:  0.875
train loss:  0.29483485221862793
train gradient:  0.13699156154962822
iteration : 14090
train acc:  0.84375
train loss:  0.4002101421356201
train gradient:  0.23511843908750363
iteration : 14091
train acc:  0.890625
train loss:  0.23769530653953552
train gradient:  0.06018435880356446
iteration : 14092
train acc:  0.84375
train loss:  0.29084575176239014
train gradient:  0.10883527824904461
iteration : 14093
train acc:  0.8828125
train loss:  0.31996092200279236
train gradient:  0.11582190070530413
iteration : 14094
train acc:  0.875
train loss:  0.2805371880531311
train gradient:  0.08291122696103155
iteration : 14095
train acc:  0.890625
train loss:  0.26911479234695435
train gradient:  0.10249552583547586
iteration : 14096
train acc:  0.8515625
train loss:  0.36876818537712097
train gradient:  0.1869733858727431
iteration : 14097
train acc:  0.84375
train loss:  0.32759612798690796
train gradient:  0.11868942629094796
iteration : 14098
train acc:  0.859375
train loss:  0.3464891016483307
train gradient:  0.10877951368744064
iteration : 14099
train acc:  0.8828125
train loss:  0.34265103936195374
train gradient:  0.18518238082677435
iteration : 14100
train acc:  0.9296875
train loss:  0.2535320222377777
train gradient:  0.09968455042442163
iteration : 14101
train acc:  0.8671875
train loss:  0.32823795080184937
train gradient:  0.1903715993328038
iteration : 14102
train acc:  0.875
train loss:  0.2752646505832672
train gradient:  0.0854518674450726
iteration : 14103
train acc:  0.8671875
train loss:  0.26635345816612244
train gradient:  0.08848775444309008
iteration : 14104
train acc:  0.828125
train loss:  0.3423640727996826
train gradient:  0.14428393821470803
iteration : 14105
train acc:  0.8359375
train loss:  0.38415974378585815
train gradient:  0.343709486302477
iteration : 14106
train acc:  0.859375
train loss:  0.32106709480285645
train gradient:  0.11061125981916935
iteration : 14107
train acc:  0.8359375
train loss:  0.3902757167816162
train gradient:  0.1887981272496227
iteration : 14108
train acc:  0.8984375
train loss:  0.2787032127380371
train gradient:  0.16878786809393387
iteration : 14109
train acc:  0.8828125
train loss:  0.26623275876045227
train gradient:  0.0875695352471016
iteration : 14110
train acc:  0.8359375
train loss:  0.38371390104293823
train gradient:  0.23202128152608892
iteration : 14111
train acc:  0.828125
train loss:  0.3376206159591675
train gradient:  0.12091462885499706
iteration : 14112
train acc:  0.8984375
train loss:  0.2631961405277252
train gradient:  0.1231861874228739
iteration : 14113
train acc:  0.859375
train loss:  0.33635932207107544
train gradient:  0.1298041975511593
iteration : 14114
train acc:  0.84375
train loss:  0.36959394812583923
train gradient:  0.21154404675186428
iteration : 14115
train acc:  0.921875
train loss:  0.2368030697107315
train gradient:  0.09878626786676237
iteration : 14116
train acc:  0.8828125
train loss:  0.31165486574172974
train gradient:  0.1332141670772138
iteration : 14117
train acc:  0.9375
train loss:  0.2177400290966034
train gradient:  0.09683738023657298
iteration : 14118
train acc:  0.8671875
train loss:  0.2875968813896179
train gradient:  0.09214043502394362
iteration : 14119
train acc:  0.90625
train loss:  0.232807457447052
train gradient:  0.07641067063424886
iteration : 14120
train acc:  0.8203125
train loss:  0.36056461930274963
train gradient:  0.14697977866317233
iteration : 14121
train acc:  0.890625
train loss:  0.32469719648361206
train gradient:  0.143579730778985
iteration : 14122
train acc:  0.828125
train loss:  0.34992116689682007
train gradient:  0.1442179374141978
iteration : 14123
train acc:  0.859375
train loss:  0.3413492441177368
train gradient:  0.11528151923782982
iteration : 14124
train acc:  0.8203125
train loss:  0.40680035948753357
train gradient:  0.30651656283195877
iteration : 14125
train acc:  0.8671875
train loss:  0.2447299063205719
train gradient:  0.11845895600402602
iteration : 14126
train acc:  0.84375
train loss:  0.3241031765937805
train gradient:  0.11169898989837074
iteration : 14127
train acc:  0.9140625
train loss:  0.23800936341285706
train gradient:  0.08130151042843049
iteration : 14128
train acc:  0.875
train loss:  0.3208343982696533
train gradient:  0.1470307031855107
iteration : 14129
train acc:  0.8203125
train loss:  0.4368566870689392
train gradient:  0.20601502221097961
iteration : 14130
train acc:  0.828125
train loss:  0.38321882486343384
train gradient:  0.17031904583169494
iteration : 14131
train acc:  0.921875
train loss:  0.31866466999053955
train gradient:  0.1475690732530733
iteration : 14132
train acc:  0.8046875
train loss:  0.42342233657836914
train gradient:  0.2356823838738944
iteration : 14133
train acc:  0.890625
train loss:  0.2761313021183014
train gradient:  0.1064060319967514
iteration : 14134
train acc:  0.890625
train loss:  0.2775515913963318
train gradient:  0.08801011859575843
iteration : 14135
train acc:  0.890625
train loss:  0.3199588358402252
train gradient:  0.16401591427544426
iteration : 14136
train acc:  0.84375
train loss:  0.33630791306495667
train gradient:  0.11288711889993688
iteration : 14137
train acc:  0.8984375
train loss:  0.24181631207466125
train gradient:  0.07921310398861872
iteration : 14138
train acc:  0.8515625
train loss:  0.3396705985069275
train gradient:  0.23234081976652238
iteration : 14139
train acc:  0.90625
train loss:  0.21652936935424805
train gradient:  0.08214631408569249
iteration : 14140
train acc:  0.90625
train loss:  0.2715684771537781
train gradient:  0.09370958673837378
iteration : 14141
train acc:  0.875
train loss:  0.2740362584590912
train gradient:  0.08896009299369496
iteration : 14142
train acc:  0.8828125
train loss:  0.29509037733078003
train gradient:  0.14291777746182036
iteration : 14143
train acc:  0.8359375
train loss:  0.326015442609787
train gradient:  0.14397119769755512
iteration : 14144
train acc:  0.90625
train loss:  0.24377752840518951
train gradient:  0.07455531043347174
iteration : 14145
train acc:  0.90625
train loss:  0.2749331593513489
train gradient:  0.1000342938794499
iteration : 14146
train acc:  0.8359375
train loss:  0.3344954252243042
train gradient:  0.1274992695536069
iteration : 14147
train acc:  0.8203125
train loss:  0.3677288293838501
train gradient:  0.15040911511638
iteration : 14148
train acc:  0.8671875
train loss:  0.2695549726486206
train gradient:  0.09714353048363869
iteration : 14149
train acc:  0.8515625
train loss:  0.35698452591896057
train gradient:  0.14841692100301568
iteration : 14150
train acc:  0.8515625
train loss:  0.3355580270290375
train gradient:  0.16441212622072932
iteration : 14151
train acc:  0.8671875
train loss:  0.3438175320625305
train gradient:  0.11159858304206878
iteration : 14152
train acc:  0.875
train loss:  0.3312023878097534
train gradient:  0.15141820230222047
iteration : 14153
train acc:  0.8203125
train loss:  0.39530786871910095
train gradient:  0.21566416556074813
iteration : 14154
train acc:  0.8203125
train loss:  0.3644174635410309
train gradient:  0.21407183307280842
iteration : 14155
train acc:  0.8359375
train loss:  0.3302389979362488
train gradient:  0.16985183507104612
iteration : 14156
train acc:  0.84375
train loss:  0.33156031370162964
train gradient:  0.11371041286627771
iteration : 14157
train acc:  0.859375
train loss:  0.34268391132354736
train gradient:  0.12626184970537352
iteration : 14158
train acc:  0.8515625
train loss:  0.327762246131897
train gradient:  0.1029509604132025
iteration : 14159
train acc:  0.8984375
train loss:  0.336045503616333
train gradient:  0.1600564310283831
iteration : 14160
train acc:  0.8359375
train loss:  0.30826157331466675
train gradient:  0.12681236680720492
iteration : 14161
train acc:  0.8203125
train loss:  0.340517520904541
train gradient:  0.21866318970640525
iteration : 14162
train acc:  0.890625
train loss:  0.2992677688598633
train gradient:  0.1089743027399209
iteration : 14163
train acc:  0.8515625
train loss:  0.289728045463562
train gradient:  0.07468161476535828
iteration : 14164
train acc:  0.8828125
train loss:  0.29108279943466187
train gradient:  0.10265407747696693
iteration : 14165
train acc:  0.828125
train loss:  0.332644522190094
train gradient:  0.14269622202844592
iteration : 14166
train acc:  0.8984375
train loss:  0.2667370140552521
train gradient:  0.08035012597911356
iteration : 14167
train acc:  0.90625
train loss:  0.26265352964401245
train gradient:  0.07423490957822192
iteration : 14168
train acc:  0.875
train loss:  0.3298855423927307
train gradient:  0.12325516334347178
iteration : 14169
train acc:  0.8828125
train loss:  0.32829341292381287
train gradient:  0.11065006434123957
iteration : 14170
train acc:  0.8046875
train loss:  0.44402775168418884
train gradient:  0.1994827878577119
iteration : 14171
train acc:  0.8515625
train loss:  0.35266780853271484
train gradient:  0.13279307668388907
iteration : 14172
train acc:  0.8671875
train loss:  0.34790265560150146
train gradient:  0.18601747488886317
iteration : 14173
train acc:  0.8671875
train loss:  0.2864285111427307
train gradient:  0.08974619300566003
iteration : 14174
train acc:  0.875
train loss:  0.2835244834423065
train gradient:  0.0739622392384705
iteration : 14175
train acc:  0.8984375
train loss:  0.24592742323875427
train gradient:  0.07951839899870583
iteration : 14176
train acc:  0.8828125
train loss:  0.2868330478668213
train gradient:  0.10098070036539082
iteration : 14177
train acc:  0.859375
train loss:  0.31266647577285767
train gradient:  0.11632380146286446
iteration : 14178
train acc:  0.8671875
train loss:  0.2857303023338318
train gradient:  0.14418016378308357
iteration : 14179
train acc:  0.9296875
train loss:  0.24130253493785858
train gradient:  0.08102763584201392
iteration : 14180
train acc:  0.890625
train loss:  0.26674121618270874
train gradient:  0.06110633370795615
iteration : 14181
train acc:  0.8125
train loss:  0.36666205525398254
train gradient:  0.11805283039200663
iteration : 14182
train acc:  0.890625
train loss:  0.2761472165584564
train gradient:  0.08871570493783876
iteration : 14183
train acc:  0.890625
train loss:  0.30013537406921387
train gradient:  0.11624087967884647
iteration : 14184
train acc:  0.8359375
train loss:  0.32449984550476074
train gradient:  0.18551725389815613
iteration : 14185
train acc:  0.875
train loss:  0.33945897221565247
train gradient:  0.11707068589829912
iteration : 14186
train acc:  0.7777777777777778
train loss:  0.5156700611114502
train gradient:  3.2053296335590455
val acc:  0.8685630260622655
val f1:  0.8693157756760572
val confusion matrix:  [[85081 13529]
 [12393 86217]]

----------------------------------------new_epoch--------------------------------------

epoch:  1
iteration : 0
train acc:  0.8828125
train loss:  0.29973074793815613
train gradient:  0.10583943940271581
iteration : 1
train acc:  0.859375
train loss:  0.31614917516708374
train gradient:  0.10905986022628465
iteration : 2
train acc:  0.8671875
train loss:  0.3205232620239258
train gradient:  0.10336258586344031
iteration : 3
train acc:  0.828125
train loss:  0.3359619975090027
train gradient:  0.1264423279662286
iteration : 4
train acc:  0.8671875
train loss:  0.32903486490249634
train gradient:  0.16719015033795173
iteration : 5
train acc:  0.8984375
train loss:  0.25685226917266846
train gradient:  0.0918847860644456
iteration : 6
train acc:  0.9140625
train loss:  0.24963048100471497
train gradient:  0.12411540485567549
iteration : 7
train acc:  0.8046875
train loss:  0.47309666872024536
train gradient:  0.28091963436614414
iteration : 8
train acc:  0.8046875
train loss:  0.3682076930999756
train gradient:  0.13567899269965197
iteration : 9
train acc:  0.875
train loss:  0.28294384479522705
train gradient:  0.10972628567779712
iteration : 10
train acc:  0.8671875
train loss:  0.3129194378852844
train gradient:  0.11241865582254845
iteration : 11
train acc:  0.8984375
train loss:  0.2855115830898285
train gradient:  0.11239660077853071
iteration : 12
train acc:  0.8671875
train loss:  0.30115675926208496
train gradient:  0.14594660820042915
iteration : 13
train acc:  0.9296875
train loss:  0.24934741854667664
train gradient:  0.10076375266803572
iteration : 14
train acc:  0.875
train loss:  0.2952132225036621
train gradient:  0.09667526870983087
iteration : 15
train acc:  0.875
train loss:  0.34001612663269043
train gradient:  0.10006243437244171
iteration : 16
train acc:  0.859375
train loss:  0.36647695302963257
train gradient:  0.16140149374137458
iteration : 17
train acc:  0.8359375
train loss:  0.3863966464996338
train gradient:  0.15065976788489888
iteration : 18
train acc:  0.921875
train loss:  0.22784514725208282
train gradient:  0.07495567557595953
iteration : 19
train acc:  0.8515625
train loss:  0.2975085377693176
train gradient:  0.08972461647139432
iteration : 20
train acc:  0.8828125
train loss:  0.29033100605010986
train gradient:  0.08784109581238732
iteration : 21
train acc:  0.8515625
train loss:  0.3281310796737671
train gradient:  0.09935797659436048
iteration : 22
train acc:  0.8515625
train loss:  0.3064180612564087
train gradient:  0.1076130006108525
iteration : 23
train acc:  0.8984375
train loss:  0.2375076860189438
train gradient:  0.09520485945054771
iteration : 24
train acc:  0.8984375
train loss:  0.2849607467651367
train gradient:  0.17674383140957894
iteration : 25
train acc:  0.859375
train loss:  0.3393198549747467
train gradient:  0.1014615072846815
iteration : 26
train acc:  0.890625
train loss:  0.25310376286506653
train gradient:  0.08555006525089462
iteration : 27
train acc:  0.8984375
train loss:  0.22447152435779572
train gradient:  0.07024329122277963
iteration : 28
train acc:  0.8125
train loss:  0.3624601662158966
train gradient:  0.13322569284683813
iteration : 29
train acc:  0.8671875
train loss:  0.3147892355918884
train gradient:  0.08808984441253497
iteration : 30
train acc:  0.859375
train loss:  0.43038421869277954
train gradient:  0.2646616417784533
iteration : 31
train acc:  0.84375
train loss:  0.3601445257663727
train gradient:  0.20139467082747892
iteration : 32
train acc:  0.859375
train loss:  0.36424461007118225
train gradient:  0.15692302928471114
iteration : 33
train acc:  0.8125
train loss:  0.36706429719924927
train gradient:  0.12447195506827095
iteration : 34
train acc:  0.8671875
train loss:  0.30401259660720825
train gradient:  0.12548594542640007
iteration : 35
train acc:  0.8515625
train loss:  0.35752663016319275
train gradient:  0.16248231866865173
iteration : 36
train acc:  0.890625
train loss:  0.27054691314697266
train gradient:  0.08221088133399074
iteration : 37
train acc:  0.90625
train loss:  0.2532777190208435
train gradient:  0.1680816417529552
iteration : 38
train acc:  0.8671875
train loss:  0.3145545721054077
train gradient:  0.14639486805759266
iteration : 39
train acc:  0.859375
train loss:  0.32677027583122253
train gradient:  0.20220638202923594
iteration : 40
train acc:  0.859375
train loss:  0.3286755084991455
train gradient:  0.1403623391925298
iteration : 41
train acc:  0.828125
train loss:  0.35427236557006836
train gradient:  0.12028063858013358
iteration : 42
train acc:  0.8828125
train loss:  0.2833898067474365
train gradient:  0.11232137240205088
iteration : 43
train acc:  0.8671875
train loss:  0.36435359716415405
train gradient:  0.13936302623299615
iteration : 44
train acc:  0.8515625
train loss:  0.33424389362335205
train gradient:  0.11669274106665019
iteration : 45
train acc:  0.8203125
train loss:  0.4264160990715027
train gradient:  0.19938745225765772
iteration : 46
train acc:  0.9296875
train loss:  0.23946553468704224
train gradient:  0.06691254406856523
iteration : 47
train acc:  0.875
train loss:  0.28185564279556274
train gradient:  0.11045986428386013
iteration : 48
train acc:  0.8515625
train loss:  0.33892822265625
train gradient:  0.10296234998716533
iteration : 49
train acc:  0.875
train loss:  0.2853500247001648
train gradient:  0.12394991358333644
iteration : 50
train acc:  0.8671875
train loss:  0.3038616180419922
train gradient:  0.11831935899153304
iteration : 51
train acc:  0.8671875
train loss:  0.32109618186950684
train gradient:  0.1471385189837218
iteration : 52
train acc:  0.890625
train loss:  0.20458857715129852
train gradient:  0.06369303296199924
iteration : 53
train acc:  0.8515625
train loss:  0.2810653746128082
train gradient:  0.12813919589130401
iteration : 54
train acc:  0.90625
train loss:  0.2675471305847168
train gradient:  0.11088443700092313
iteration : 55
train acc:  0.890625
train loss:  0.27397942543029785
train gradient:  0.0807939395228791
iteration : 56
train acc:  0.890625
train loss:  0.29970380663871765
train gradient:  0.09232722804879832
iteration : 57
train acc:  0.84375
train loss:  0.33648306131362915
train gradient:  0.09286800409295953
iteration : 58
train acc:  0.8828125
train loss:  0.3301057517528534
train gradient:  0.155130043374199
iteration : 59
train acc:  0.8671875
train loss:  0.2910010814666748
train gradient:  0.12172443356327486
iteration : 60
train acc:  0.8671875
train loss:  0.26791292428970337
train gradient:  0.10632408503284889
iteration : 61
train acc:  0.8359375
train loss:  0.3807748854160309
train gradient:  0.15692042075823853
iteration : 62
train acc:  0.90625
train loss:  0.2546273171901703
train gradient:  0.12982750266286086
iteration : 63
train acc:  0.875
train loss:  0.37456515431404114
train gradient:  0.13775215551318948
iteration : 64
train acc:  0.75
train loss:  0.49036887288093567
train gradient:  0.23814754591721105
iteration : 65
train acc:  0.7890625
train loss:  0.3797299563884735
train gradient:  0.15397432166067726
iteration : 66
train acc:  0.8515625
train loss:  0.3764655590057373
train gradient:  0.15311244486256195
iteration : 67
train acc:  0.8203125
train loss:  0.4049602746963501
train gradient:  0.22832978072006654
iteration : 68
train acc:  0.828125
train loss:  0.34557390213012695
train gradient:  0.16759633296290843
iteration : 69
train acc:  0.8046875
train loss:  0.325161337852478
train gradient:  0.1275444596161011
iteration : 70
train acc:  0.8671875
train loss:  0.30368247628211975
train gradient:  0.09411697965486872
iteration : 71
train acc:  0.8359375
train loss:  0.39174413681030273
train gradient:  0.13820840489727365
iteration : 72
train acc:  0.875
train loss:  0.34838539361953735
train gradient:  0.16120538194607298
iteration : 73
train acc:  0.828125
train loss:  0.32266178727149963
train gradient:  0.12852313745247285
iteration : 74
train acc:  0.8828125
train loss:  0.2703545093536377
train gradient:  0.10459248199857304
iteration : 75
train acc:  0.8515625
train loss:  0.34070640802383423
train gradient:  0.16452189693137625
iteration : 76
train acc:  0.875
train loss:  0.30331066250801086
train gradient:  0.1167986182578579
iteration : 77
train acc:  0.859375
train loss:  0.3298032283782959
train gradient:  0.15620749770347073
iteration : 78
train acc:  0.9296875
train loss:  0.23974451422691345
train gradient:  0.11484763236047812
iteration : 79
train acc:  0.8671875
train loss:  0.3249935209751129
train gradient:  0.13619696595874375
iteration : 80
train acc:  0.8671875
train loss:  0.2651715874671936
train gradient:  0.09686751826553762
iteration : 81
train acc:  0.8671875
train loss:  0.2865992486476898
train gradient:  0.16012742272520447
iteration : 82
train acc:  0.9453125
train loss:  0.2228652834892273
train gradient:  0.05659396043990218
iteration : 83
train acc:  0.859375
train loss:  0.32806098461151123
train gradient:  0.11379898064845857
iteration : 84
train acc:  0.828125
train loss:  0.3601871728897095
train gradient:  0.14597709320303665
iteration : 85
train acc:  0.8671875
train loss:  0.31297069787979126
train gradient:  0.11267761435432744
iteration : 86
train acc:  0.859375
train loss:  0.329689085483551
train gradient:  0.15732521299990782
iteration : 87
train acc:  0.8828125
train loss:  0.3273497223854065
train gradient:  0.11533022644317706
iteration : 88
train acc:  0.9140625
train loss:  0.2540554404258728
train gradient:  0.101756271897995
iteration : 89
train acc:  0.8671875
train loss:  0.30292046070098877
train gradient:  0.09915537275302157
iteration : 90
train acc:  0.8828125
train loss:  0.29349344968795776
train gradient:  0.12320455398136371
iteration : 91
train acc:  0.8515625
train loss:  0.29102814197540283
train gradient:  0.11654386064775248
iteration : 92
train acc:  0.84375
train loss:  0.30919045209884644
train gradient:  0.10307999603328075
iteration : 93
train acc:  0.90625
train loss:  0.23319081962108612
train gradient:  0.09628015733658836
iteration : 94
train acc:  0.8203125
train loss:  0.35137319564819336
train gradient:  0.14772598418806193
iteration : 95
train acc:  0.9140625
train loss:  0.24164025485515594
train gradient:  0.06664556115546436
iteration : 96
train acc:  0.84375
train loss:  0.31664150953292847
train gradient:  0.15930367749794686
iteration : 97
train acc:  0.84375
train loss:  0.3081411123275757
train gradient:  0.12657426092874652
iteration : 98
train acc:  0.8671875
train loss:  0.316000759601593
train gradient:  0.12229497674852251
iteration : 99
train acc:  0.859375
train loss:  0.2670292854309082
train gradient:  0.14257338188794366
iteration : 100
train acc:  0.875
train loss:  0.2965658903121948
train gradient:  0.09924436881656121
iteration : 101
train acc:  0.8359375
train loss:  0.32378530502319336
train gradient:  0.1132827285521636
iteration : 102
train acc:  0.8671875
train loss:  0.40380722284317017
train gradient:  0.23659412149937592
iteration : 103
train acc:  0.90625
train loss:  0.2478245198726654
train gradient:  0.09287134595770508
iteration : 104
train acc:  0.84375
train loss:  0.3447462320327759
train gradient:  0.1494641112243626
iteration : 105
train acc:  0.84375
train loss:  0.3603617250919342
train gradient:  0.16413147159607672
iteration : 106
train acc:  0.8203125
train loss:  0.310229629278183
train gradient:  0.17119005486433808
iteration : 107
train acc:  0.875
train loss:  0.28359827399253845
train gradient:  0.12594689390781244
iteration : 108
train acc:  0.890625
train loss:  0.28629636764526367
train gradient:  0.10550608318797661
iteration : 109
train acc:  0.8984375
train loss:  0.28001976013183594
train gradient:  0.12382410164990071
iteration : 110
train acc:  0.875
train loss:  0.2898586392402649
train gradient:  0.14376630683862704
iteration : 111
train acc:  0.890625
train loss:  0.26345857977867126
train gradient:  0.1312749839005017
iteration : 112
train acc:  0.921875
train loss:  0.265667200088501
train gradient:  0.08174307777802414
iteration : 113
train acc:  0.8359375
train loss:  0.3447856605052948
train gradient:  0.1768122254127778
iteration : 114
train acc:  0.8828125
train loss:  0.24145422875881195
train gradient:  0.14259185424584975
iteration : 115
train acc:  0.890625
train loss:  0.328398197889328
train gradient:  0.25976274532386645
iteration : 116
train acc:  0.8984375
train loss:  0.1977032572031021
train gradient:  0.10268690753134459
iteration : 117
train acc:  0.8984375
train loss:  0.24327047169208527
train gradient:  0.09977106228038998
iteration : 118
train acc:  0.828125
train loss:  0.31856831908226013
train gradient:  0.13838842645048688
iteration : 119
train acc:  0.84375
train loss:  0.30669668316841125
train gradient:  0.15191409021637245
iteration : 120
train acc:  0.84375
train loss:  0.37973248958587646
train gradient:  0.19334624708930262
iteration : 121
train acc:  0.8671875
train loss:  0.2913367748260498
train gradient:  0.07747509112453133
iteration : 122
train acc:  0.8984375
train loss:  0.30998262763023376
train gradient:  0.1040614880035077
iteration : 123
train acc:  0.8828125
train loss:  0.28204357624053955
train gradient:  0.10825672714617778
iteration : 124
train acc:  0.8671875
train loss:  0.30099743604660034
train gradient:  0.10685123573884484
iteration : 125
train acc:  0.890625
train loss:  0.2837139368057251
train gradient:  0.1314549861914575
iteration : 126
train acc:  0.875
train loss:  0.3125436305999756
train gradient:  0.1196423905560729
iteration : 127
train acc:  0.8671875
train loss:  0.2904607951641083
train gradient:  0.18337073627457373
iteration : 128
train acc:  0.875
train loss:  0.27876055240631104
train gradient:  0.14432706862802586
iteration : 129
train acc:  0.8515625
train loss:  0.24801862239837646
train gradient:  0.11171149157972633
iteration : 130
train acc:  0.8359375
train loss:  0.4109986424446106
train gradient:  0.18307622643410093
iteration : 131
train acc:  0.8203125
train loss:  0.4114852547645569
train gradient:  0.2382195631605506
iteration : 132
train acc:  0.9453125
train loss:  0.21183539927005768
train gradient:  0.0686751808233504
iteration : 133
train acc:  0.890625
train loss:  0.2432345449924469
train gradient:  0.08354449950359642
iteration : 134
train acc:  0.828125
train loss:  0.3877376914024353
train gradient:  0.20132292840293065
iteration : 135
train acc:  0.875
train loss:  0.2945605218410492
train gradient:  0.09149743247530119
iteration : 136
train acc:  0.8671875
train loss:  0.25787222385406494
train gradient:  0.10184310519437464
iteration : 137
train acc:  0.8515625
train loss:  0.34516507387161255
train gradient:  0.21233168516235063
iteration : 138
train acc:  0.8671875
train loss:  0.31831473112106323
train gradient:  0.11118010564838712
iteration : 139
train acc:  0.8828125
train loss:  0.32271960377693176
train gradient:  0.27090156630789797
iteration : 140
train acc:  0.890625
train loss:  0.29182666540145874
train gradient:  0.08415639294431017
iteration : 141
train acc:  0.9140625
train loss:  0.2507587969303131
train gradient:  0.12422195531206107
iteration : 142
train acc:  0.8828125
train loss:  0.29679784178733826
train gradient:  0.12618494272395733
iteration : 143
train acc:  0.875
train loss:  0.30067741870880127
train gradient:  0.13930056814184266
iteration : 144
train acc:  0.8515625
train loss:  0.35305237770080566
train gradient:  0.14527095240399301
iteration : 145
train acc:  0.8984375
train loss:  0.238226518034935
train gradient:  0.08618146965914886
iteration : 146
train acc:  0.859375
train loss:  0.32372528314590454
train gradient:  0.15549102103710138
iteration : 147
train acc:  0.8515625
train loss:  0.3300725221633911
train gradient:  0.13005933135746084
iteration : 148
train acc:  0.8671875
train loss:  0.29730984568595886
train gradient:  0.118704784438287
iteration : 149
train acc:  0.859375
train loss:  0.3065877854824066
train gradient:  0.13132402115923805
iteration : 150
train acc:  0.8359375
train loss:  0.40069496631622314
train gradient:  0.2237894916826968
iteration : 151
train acc:  0.8203125
train loss:  0.40172600746154785
train gradient:  0.2047923512215814
iteration : 152
train acc:  0.859375
train loss:  0.28825393319129944
train gradient:  0.10950090521383866
iteration : 153
train acc:  0.8359375
train loss:  0.3961656987667084
train gradient:  0.18103245066999993
iteration : 154
train acc:  0.8671875
train loss:  0.37351101636886597
train gradient:  0.1547380750655405
iteration : 155
train acc:  0.90625
train loss:  0.23706097900867462
train gradient:  0.1031886319258246
iteration : 156
train acc:  0.875
train loss:  0.2988069951534271
train gradient:  0.12968117630653941
iteration : 157
train acc:  0.8984375
train loss:  0.2796170711517334
train gradient:  0.1013901783248781
iteration : 158
train acc:  0.875
train loss:  0.27384644746780396
train gradient:  0.10671938211273314
iteration : 159
train acc:  0.8671875
train loss:  0.3054787516593933
train gradient:  0.11674116634983808
iteration : 160
train acc:  0.84375
train loss:  0.4120643436908722
train gradient:  0.18307612369940704
iteration : 161
train acc:  0.859375
train loss:  0.30872470140457153
train gradient:  0.09227846358842202
iteration : 162
train acc:  0.875
train loss:  0.30430862307548523
train gradient:  0.12014166254236446
iteration : 163
train acc:  0.859375
train loss:  0.3203718364238739
train gradient:  0.1628332478779593
iteration : 164
train acc:  0.8671875
train loss:  0.2951899766921997
train gradient:  0.11374711190976099
iteration : 165
train acc:  0.8359375
train loss:  0.3525342047214508
train gradient:  0.1586020047040882
iteration : 166
train acc:  0.90625
train loss:  0.252746045589447
train gradient:  0.10155944455958968
iteration : 167
train acc:  0.890625
train loss:  0.2814911901950836
train gradient:  0.11653399099453247
iteration : 168
train acc:  0.859375
train loss:  0.37528762221336365
train gradient:  0.12923518948955104
iteration : 169
train acc:  0.8515625
train loss:  0.323581337928772
train gradient:  0.12463360371874428
iteration : 170
train acc:  0.859375
train loss:  0.367434561252594
train gradient:  0.16533738658600466
iteration : 171
train acc:  0.8515625
train loss:  0.2779492735862732
train gradient:  0.11802684030100612
iteration : 172
train acc:  0.890625
train loss:  0.2729790508747101
train gradient:  0.1374768079388324
iteration : 173
train acc:  0.828125
train loss:  0.3503867983818054
train gradient:  0.15612519162925004
iteration : 174
train acc:  0.8359375
train loss:  0.34675514698028564
train gradient:  0.1599554273420314
iteration : 175
train acc:  0.828125
train loss:  0.26119565963745117
train gradient:  0.07581113626097367
iteration : 176
train acc:  0.8828125
train loss:  0.2670183777809143
train gradient:  0.12859850169437145
iteration : 177
train acc:  0.875
train loss:  0.34799259901046753
train gradient:  0.11409781713402463
iteration : 178
train acc:  0.8359375
train loss:  0.3424888849258423
train gradient:  0.13914110323673812
iteration : 179
train acc:  0.890625
train loss:  0.2402881681919098
train gradient:  0.08745990802751034
iteration : 180
train acc:  0.8359375
train loss:  0.37139376997947693
train gradient:  0.24318776492966096
iteration : 181
train acc:  0.875
train loss:  0.32457539439201355
train gradient:  0.1191770125780981
iteration : 182
train acc:  0.8515625
train loss:  0.3481358289718628
train gradient:  0.18030592405025905
iteration : 183
train acc:  0.875
train loss:  0.340617835521698
train gradient:  0.15381894102160792
iteration : 184
train acc:  0.8984375
train loss:  0.24268996715545654
train gradient:  0.11978501861055064
iteration : 185
train acc:  0.84375
train loss:  0.30399155616760254
train gradient:  0.120265440944839
iteration : 186
train acc:  0.8515625
train loss:  0.3630414605140686
train gradient:  0.11847491492630241
iteration : 187
train acc:  0.84375
train loss:  0.30738550424575806
train gradient:  0.12647915831587622
iteration : 188
train acc:  0.859375
train loss:  0.34506741166114807
train gradient:  0.11358902555557196
iteration : 189
train acc:  0.890625
train loss:  0.27821794152259827
train gradient:  0.11441760436622414
iteration : 190
train acc:  0.859375
train loss:  0.33993086218833923
train gradient:  0.17631297733947887
iteration : 191
train acc:  0.84375
train loss:  0.39207226037979126
train gradient:  0.2118550378466913
iteration : 192
train acc:  0.8984375
train loss:  0.27069559693336487
train gradient:  0.11533917784041656
iteration : 193
train acc:  0.890625
train loss:  0.24465203285217285
train gradient:  0.08493174223329787
iteration : 194
train acc:  0.8203125
train loss:  0.4328235983848572
train gradient:  0.15710937921927956
iteration : 195
train acc:  0.8671875
train loss:  0.328712522983551
train gradient:  0.11140575459808898
iteration : 196
train acc:  0.8671875
train loss:  0.3304198384284973
train gradient:  0.12960629601513782
iteration : 197
train acc:  0.8828125
train loss:  0.2908046245574951
train gradient:  0.15338961771056125
iteration : 198
train acc:  0.859375
train loss:  0.30136173963546753
train gradient:  0.10987795692859102
iteration : 199
train acc:  0.828125
train loss:  0.3396005630493164
train gradient:  0.12100125516858255
iteration : 200
train acc:  0.875
train loss:  0.2563073933124542
train gradient:  0.11789825196671498
iteration : 201
train acc:  0.859375
train loss:  0.2992026209831238
train gradient:  0.10570951466593421
iteration : 202
train acc:  0.8515625
train loss:  0.3667911887168884
train gradient:  0.12345261651481276
iteration : 203
train acc:  0.8828125
train loss:  0.37404417991638184
train gradient:  0.23109233942784135
iteration : 204
train acc:  0.859375
train loss:  0.33951765298843384
train gradient:  0.14338814080360826
iteration : 205
train acc:  0.84375
train loss:  0.3352814018726349
train gradient:  0.11567939932050038
iteration : 206
train acc:  0.875
train loss:  0.3460811376571655
train gradient:  0.12222042674526125
iteration : 207
train acc:  0.8671875
train loss:  0.3165435791015625
train gradient:  0.11207572086371544
iteration : 208
train acc:  0.8828125
train loss:  0.33055579662323
train gradient:  0.13384551330836955
iteration : 209
train acc:  0.9140625
train loss:  0.25714391469955444
train gradient:  0.09768978957472653
iteration : 210
train acc:  0.796875
train loss:  0.39005520939826965
train gradient:  0.14821524643216255
iteration : 211
train acc:  0.890625
train loss:  0.27563440799713135
train gradient:  0.1518000125517486
iteration : 212
train acc:  0.84375
train loss:  0.3237643837928772
train gradient:  0.09075481651955089
iteration : 213
train acc:  0.859375
train loss:  0.3018516004085541
train gradient:  0.11971020153297521
iteration : 214
train acc:  0.8828125
train loss:  0.2766861617565155
train gradient:  0.13174409854467167
iteration : 215
train acc:  0.8828125
train loss:  0.317473441362381
train gradient:  0.10668866722774376
iteration : 216
train acc:  0.84375
train loss:  0.2755623459815979
train gradient:  0.08923751865253556
iteration : 217
train acc:  0.8359375
train loss:  0.3644399046897888
train gradient:  0.14700060024705125
iteration : 218
train acc:  0.8828125
train loss:  0.2935529351234436
train gradient:  0.1426584237204728
iteration : 219
train acc:  0.890625
train loss:  0.298633873462677
train gradient:  0.12453467596779287
iteration : 220
train acc:  0.875
train loss:  0.29208511114120483
train gradient:  0.10698511426095318
iteration : 221
train acc:  0.90625
train loss:  0.23623281717300415
train gradient:  0.08701146445474632
iteration : 222
train acc:  0.8515625
train loss:  0.3340933918952942
train gradient:  0.10647952710180222
iteration : 223
train acc:  0.8671875
train loss:  0.3187170624732971
train gradient:  0.10999498920396827
iteration : 224
train acc:  0.8984375
train loss:  0.26818907260894775
train gradient:  0.08162491367924167
iteration : 225
train acc:  0.875
train loss:  0.32555198669433594
train gradient:  0.11766006297448024
iteration : 226
train acc:  0.8828125
train loss:  0.28482115268707275
train gradient:  0.08262643002473022
iteration : 227
train acc:  0.828125
train loss:  0.43195948004722595
train gradient:  0.28423723372716536
iteration : 228
train acc:  0.828125
train loss:  0.38377875089645386
train gradient:  0.19826564171305708
iteration : 229
train acc:  0.921875
train loss:  0.24357841908931732
train gradient:  0.09937363468114516
iteration : 230
train acc:  0.875
train loss:  0.28215014934539795
train gradient:  0.08461351116727943
iteration : 231
train acc:  0.859375
train loss:  0.2760363221168518
train gradient:  0.11913972035123821
iteration : 232
train acc:  0.859375
train loss:  0.3079923391342163
train gradient:  0.1512128055508018
iteration : 233
train acc:  0.9296875
train loss:  0.25666114687919617
train gradient:  0.0794491974933505
iteration : 234
train acc:  0.859375
train loss:  0.3057856559753418
train gradient:  0.09934394435034807
iteration : 235
train acc:  0.8828125
train loss:  0.2657722532749176
train gradient:  0.07816172334207587
iteration : 236
train acc:  0.859375
train loss:  0.37954360246658325
train gradient:  0.18387241730128168
iteration : 237
train acc:  0.8203125
train loss:  0.3696592450141907
train gradient:  0.18724109459622745
iteration : 238
train acc:  0.8515625
train loss:  0.2528933882713318
train gradient:  0.10382234707558484
iteration : 239
train acc:  0.8671875
train loss:  0.2947271466255188
train gradient:  0.13272738675308157
iteration : 240
train acc:  0.8984375
train loss:  0.26306045055389404
train gradient:  0.07966068057382944
iteration : 241
train acc:  0.8203125
train loss:  0.4055563807487488
train gradient:  0.24315358855426616
iteration : 242
train acc:  0.9140625
train loss:  0.2185029238462448
train gradient:  0.06097126279003043
iteration : 243
train acc:  0.8671875
train loss:  0.34385377168655396
train gradient:  0.15595854096194472
iteration : 244
train acc:  0.953125
train loss:  0.18542319536209106
train gradient:  0.08404195818994165
iteration : 245
train acc:  0.84375
train loss:  0.2651084065437317
train gradient:  0.08799454411419097
iteration : 246
train acc:  0.859375
train loss:  0.30205783247947693
train gradient:  0.22259436468172578
iteration : 247
train acc:  0.9296875
train loss:  0.23029714822769165
train gradient:  0.10328656461979621
iteration : 248
train acc:  0.8984375
train loss:  0.2889241874217987
train gradient:  0.08556613023790285
iteration : 249
train acc:  0.859375
train loss:  0.25303035974502563
train gradient:  0.13992903632522807
iteration : 250
train acc:  0.875
train loss:  0.3511515259742737
train gradient:  0.15971615053853044
iteration : 251
train acc:  0.8671875
train loss:  0.3330152630805969
train gradient:  0.1700397287375794
iteration : 252
train acc:  0.84375
train loss:  0.35640662908554077
train gradient:  0.24194948184708737
iteration : 253
train acc:  0.859375
train loss:  0.34379342198371887
train gradient:  0.13935856763420007
iteration : 254
train acc:  0.8515625
train loss:  0.36868512630462646
train gradient:  0.22239582159017396
iteration : 255
train acc:  0.8671875
train loss:  0.2968006432056427
train gradient:  0.14909796322611296
iteration : 256
train acc:  0.8515625
train loss:  0.36662372946739197
train gradient:  0.16138618970394714
iteration : 257
train acc:  0.8671875
train loss:  0.32322579622268677
train gradient:  0.1249807249131946
iteration : 258
train acc:  0.9375
train loss:  0.22171394526958466
train gradient:  0.08523821862323934
iteration : 259
train acc:  0.859375
train loss:  0.3253931999206543
train gradient:  0.10161597350882404
iteration : 260
train acc:  0.8359375
train loss:  0.35514938831329346
train gradient:  0.1107535939727989
iteration : 261
train acc:  0.921875
train loss:  0.23605114221572876
train gradient:  0.09102359841760972
iteration : 262
train acc:  0.8828125
train loss:  0.27733927965164185
train gradient:  0.10584317651762556
iteration : 263
train acc:  0.9140625
train loss:  0.24503690004348755
train gradient:  0.12917449898622985
iteration : 264
train acc:  0.84375
train loss:  0.30072322487831116
train gradient:  0.16831686026692866
iteration : 265
train acc:  0.875
train loss:  0.2695840299129486
train gradient:  0.08523032774051395
iteration : 266
train acc:  0.84375
train loss:  0.3344656229019165
train gradient:  0.13418173250793564
iteration : 267
train acc:  0.84375
train loss:  0.3504367470741272
train gradient:  0.15887897798377035
iteration : 268
train acc:  0.8828125
train loss:  0.31135159730911255
train gradient:  0.12389944416252498
iteration : 269
train acc:  0.8671875
train loss:  0.33355867862701416
train gradient:  0.1320915182452757
iteration : 270
train acc:  0.8359375
train loss:  0.33551108837127686
train gradient:  0.2102054824545262
iteration : 271
train acc:  0.8828125
train loss:  0.23282533884048462
train gradient:  0.08570052222361049
iteration : 272
train acc:  0.9140625
train loss:  0.24259217083454132
train gradient:  0.1046136004320167
iteration : 273
train acc:  0.8671875
train loss:  0.25419923663139343
train gradient:  0.1820683554845523
iteration : 274
train acc:  0.8984375
train loss:  0.33242031931877136
train gradient:  0.11351776139034327
iteration : 275
train acc:  0.875
train loss:  0.31856584548950195
train gradient:  0.19631282794480337
iteration : 276
train acc:  0.8671875
train loss:  0.29590103030204773
train gradient:  0.1501548012531319
iteration : 277
train acc:  0.8671875
train loss:  0.28924405574798584
train gradient:  0.12070479868284006
iteration : 278
train acc:  0.84375
train loss:  0.33001449704170227
train gradient:  0.14436729453258282
iteration : 279
train acc:  0.9296875
train loss:  0.22902825474739075
train gradient:  0.084172008261852
iteration : 280
train acc:  0.78125
train loss:  0.5198243856430054
train gradient:  0.31243735263921496
iteration : 281
train acc:  0.8125
train loss:  0.33207911252975464
train gradient:  0.15363537480209172
iteration : 282
train acc:  0.8125
train loss:  0.3649803698062897
train gradient:  0.16821928010679327
iteration : 283
train acc:  0.890625
train loss:  0.25340837240219116
train gradient:  0.2741714188790302
iteration : 284
train acc:  0.8984375
train loss:  0.2892425060272217
train gradient:  0.10047131451193506
iteration : 285
train acc:  0.8828125
train loss:  0.27611637115478516
train gradient:  0.14564086612713842
iteration : 286
train acc:  0.8984375
train loss:  0.2662312984466553
train gradient:  0.11399845511885999
iteration : 287
train acc:  0.8203125
train loss:  0.34566259384155273
train gradient:  0.1957686878460453
iteration : 288
train acc:  0.8125
train loss:  0.4418948292732239
train gradient:  0.23565098658525196
iteration : 289
train acc:  0.8984375
train loss:  0.2745171785354614
train gradient:  0.1428816260293018
iteration : 290
train acc:  0.84375
train loss:  0.37796735763549805
train gradient:  0.21780680731433655
iteration : 291
train acc:  0.8984375
train loss:  0.31355294585227966
train gradient:  0.13078340296985558
iteration : 292
train acc:  0.875
train loss:  0.2881156802177429
train gradient:  0.11090423478549474
iteration : 293
train acc:  0.8671875
train loss:  0.24971003830432892
train gradient:  0.10104644084728469
iteration : 294
train acc:  0.875
train loss:  0.2996269464492798
train gradient:  0.11224085905171623
iteration : 295
train acc:  0.8671875
train loss:  0.2881152033805847
train gradient:  0.08233227382236352
iteration : 296
train acc:  0.875
train loss:  0.2698526382446289
train gradient:  0.1127792384572417
iteration : 297
train acc:  0.8515625
train loss:  0.30977708101272583
train gradient:  0.09835187674437454
iteration : 298
train acc:  0.84375
train loss:  0.30159199237823486
train gradient:  0.1108670062007644
iteration : 299
train acc:  0.828125
train loss:  0.3745354413986206
train gradient:  0.1606721293125645
iteration : 300
train acc:  0.875
train loss:  0.31662747263908386
train gradient:  0.12046434286126259
iteration : 301
train acc:  0.90625
train loss:  0.2841808795928955
train gradient:  0.09171809902012382
iteration : 302
train acc:  0.8359375
train loss:  0.30859318375587463
train gradient:  0.33855565875016314
iteration : 303
train acc:  0.8671875
train loss:  0.33492088317871094
train gradient:  0.13137844476170762
iteration : 304
train acc:  0.890625
train loss:  0.3119358420372009
train gradient:  0.15308795503613837
iteration : 305
train acc:  0.8125
train loss:  0.34390151500701904
train gradient:  0.13756924706780865
iteration : 306
train acc:  0.859375
train loss:  0.33098405599594116
train gradient:  0.14292499120337077
iteration : 307
train acc:  0.890625
train loss:  0.3503071069717407
train gradient:  0.16868852595021355
iteration : 308
train acc:  0.828125
train loss:  0.3247068226337433
train gradient:  0.13087643257332227
iteration : 309
train acc:  0.8984375
train loss:  0.2911869287490845
train gradient:  0.09690475990385024
iteration : 310
train acc:  0.8359375
train loss:  0.3960164785385132
train gradient:  0.21980729217066564
iteration : 311
train acc:  0.8671875
train loss:  0.3051362633705139
train gradient:  0.13144387394664955
iteration : 312
train acc:  0.875
train loss:  0.3038329780101776
train gradient:  0.1021014559344428
iteration : 313
train acc:  0.8671875
train loss:  0.3270043134689331
train gradient:  0.1380735263411883
iteration : 314
train acc:  0.859375
train loss:  0.30557525157928467
train gradient:  0.12098603053532798
iteration : 315
train acc:  0.890625
train loss:  0.28716257214546204
train gradient:  0.09302954120184119
iteration : 316
train acc:  0.8515625
train loss:  0.29593175649642944
train gradient:  0.10815679671174416
iteration : 317
train acc:  0.859375
train loss:  0.31182700395584106
train gradient:  0.14283617734501144
iteration : 318
train acc:  0.84375
train loss:  0.3593149781227112
train gradient:  0.13810497276742345
iteration : 319
train acc:  0.8203125
train loss:  0.37386026978492737
train gradient:  0.1796377181018145
iteration : 320
train acc:  0.90625
train loss:  0.268277645111084
train gradient:  0.12753308423261434
iteration : 321
train acc:  0.796875
train loss:  0.42623278498649597
train gradient:  0.28745767172892417
iteration : 322
train acc:  0.8515625
train loss:  0.3552815914154053
train gradient:  0.13858221200163365
iteration : 323
train acc:  0.890625
train loss:  0.3251793384552002
train gradient:  0.14414364033876215
iteration : 324
train acc:  0.84375
train loss:  0.318681001663208
train gradient:  0.12750971106423362
iteration : 325
train acc:  0.84375
train loss:  0.35809066891670227
train gradient:  0.22026456916860485
iteration : 326
train acc:  0.890625
train loss:  0.2710151672363281
train gradient:  0.11002861439364603
iteration : 327
train acc:  0.8671875
train loss:  0.29354310035705566
train gradient:  0.11477982044541708
iteration : 328
train acc:  0.921875
train loss:  0.21981994807720184
train gradient:  0.0723355967927616
iteration : 329
train acc:  0.9140625
train loss:  0.29109030961990356
train gradient:  0.11834677878257711
iteration : 330
train acc:  0.8671875
train loss:  0.295025110244751
train gradient:  0.13729793720610556
iteration : 331
train acc:  0.8515625
train loss:  0.3095453381538391
train gradient:  0.21741676609040195
iteration : 332
train acc:  0.8515625
train loss:  0.34110546112060547
train gradient:  0.15051585568190406
iteration : 333
train acc:  0.8125
train loss:  0.33814752101898193
train gradient:  0.1479561296562976
iteration : 334
train acc:  0.828125
train loss:  0.3874354362487793
train gradient:  0.1794959743633563
iteration : 335
train acc:  0.890625
train loss:  0.3081144690513611
train gradient:  0.1365138922760079
iteration : 336
train acc:  0.859375
train loss:  0.33362245559692383
train gradient:  0.14286892882604862
iteration : 337
train acc:  0.8125
train loss:  0.39665114879608154
train gradient:  0.1752957070164617
iteration : 338
train acc:  0.859375
train loss:  0.38611900806427
train gradient:  0.20505016402917708
iteration : 339
train acc:  0.8203125
train loss:  0.38760697841644287
train gradient:  0.17221351458341688
iteration : 340
train acc:  0.9140625
train loss:  0.247639998793602
train gradient:  0.10364383771790123
iteration : 341
train acc:  0.9140625
train loss:  0.2835901081562042
train gradient:  0.10945172856049289
iteration : 342
train acc:  0.8984375
train loss:  0.237348735332489
train gradient:  0.08245239706369051
iteration : 343
train acc:  0.9140625
train loss:  0.23786839842796326
train gradient:  0.0693017810490097
iteration : 344
train acc:  0.8203125
train loss:  0.3696688711643219
train gradient:  0.13362473344462952
iteration : 345
train acc:  0.84375
train loss:  0.37003713846206665
train gradient:  0.12792192452873036
iteration : 346
train acc:  0.875
train loss:  0.3362128734588623
train gradient:  0.16630679065467618
iteration : 347
train acc:  0.859375
train loss:  0.3136562407016754
train gradient:  0.23131004463118568
iteration : 348
train acc:  0.8046875
train loss:  0.400563508272171
train gradient:  0.20671462456546702
iteration : 349
train acc:  0.8125
train loss:  0.3902286887168884
train gradient:  0.149757469843748
iteration : 350
train acc:  0.875
train loss:  0.2996068596839905
train gradient:  0.1250007931882244
iteration : 351
train acc:  0.8359375
train loss:  0.3234117925167084
train gradient:  0.13935125306035503
iteration : 352
train acc:  0.8984375
train loss:  0.25932836532592773
train gradient:  0.09513574552638196
iteration : 353
train acc:  0.8359375
train loss:  0.34195059537887573
train gradient:  0.1434486228646052
iteration : 354
train acc:  0.7890625
train loss:  0.4594849944114685
train gradient:  0.2591112037110669
iteration : 355
train acc:  0.8515625
train loss:  0.3228723406791687
train gradient:  0.1405288908752455
iteration : 356
train acc:  0.8828125
train loss:  0.30144914984703064
train gradient:  0.11659760975664143
iteration : 357
train acc:  0.8203125
train loss:  0.34922879934310913
train gradient:  0.12524154888026368
iteration : 358
train acc:  0.8515625
train loss:  0.3107014298439026
train gradient:  0.1984456241781481
iteration : 359
train acc:  0.84375
train loss:  0.3330923318862915
train gradient:  0.1619125971391474
iteration : 360
train acc:  0.8125
train loss:  0.3741154372692108
train gradient:  0.11034072286137113
iteration : 361
train acc:  0.84375
train loss:  0.369059681892395
train gradient:  0.15736573759098976
iteration : 362
train acc:  0.8671875
train loss:  0.30268043279647827
train gradient:  0.10281728564805204
iteration : 363
train acc:  0.828125
train loss:  0.37061554193496704
train gradient:  0.22172120801733286
iteration : 364
train acc:  0.8671875
train loss:  0.3057900369167328
train gradient:  0.10593159763531483
iteration : 365
train acc:  0.828125
train loss:  0.3187216520309448
train gradient:  0.13272749887568047
iteration : 366
train acc:  0.8515625
train loss:  0.31340208649635315
train gradient:  0.13408170320842594
iteration : 367
train acc:  0.8984375
train loss:  0.2401924431324005
train gradient:  0.07616261316218428
iteration : 368
train acc:  0.828125
train loss:  0.34030881524086
train gradient:  0.15830515338652051
iteration : 369
train acc:  0.859375
train loss:  0.28838375210762024
train gradient:  0.08966154329786258
iteration : 370
train acc:  0.828125
train loss:  0.3487626314163208
train gradient:  0.15350503225395254
iteration : 371
train acc:  0.890625
train loss:  0.2790471315383911
train gradient:  0.11315730281237815
iteration : 372
train acc:  0.8984375
train loss:  0.2485269457101822
train gradient:  0.10720698301693989
iteration : 373
train acc:  0.9140625
train loss:  0.25875285267829895
train gradient:  0.11650871532046811
iteration : 374
train acc:  0.8359375
train loss:  0.3101162314414978
train gradient:  0.13076389871914135
iteration : 375
train acc:  0.9140625
train loss:  0.2992515563964844
train gradient:  0.12840936513265752
iteration : 376
train acc:  0.8515625
train loss:  0.3414192795753479
train gradient:  0.1441988347444325
iteration : 377
train acc:  0.8515625
train loss:  0.3256048560142517
train gradient:  0.11565016032799597
iteration : 378
train acc:  0.84375
train loss:  0.3288794159889221
train gradient:  0.11731070257460892
iteration : 379
train acc:  0.8203125
train loss:  0.4356212317943573
train gradient:  0.23840679563339473
iteration : 380
train acc:  0.8671875
train loss:  0.29701581597328186
train gradient:  0.10031727535359551
iteration : 381
train acc:  0.9140625
train loss:  0.26091793179512024
train gradient:  0.10540559013014315
iteration : 382
train acc:  0.8203125
train loss:  0.35571277141571045
train gradient:  0.13148645331620135
iteration : 383
train acc:  0.8828125
train loss:  0.31319114565849304
train gradient:  0.0894063901535322
iteration : 384
train acc:  0.84375
train loss:  0.3103656470775604
train gradient:  0.1379950039379162
iteration : 385
train acc:  0.828125
train loss:  0.3780476748943329
train gradient:  0.16734660595261674
iteration : 386
train acc:  0.8359375
train loss:  0.3175438940525055
train gradient:  0.10059294706045313
iteration : 387
train acc:  0.859375
train loss:  0.26819926500320435
train gradient:  0.10133795987530206
iteration : 388
train acc:  0.8671875
train loss:  0.3465772271156311
train gradient:  0.12790543105327454
iteration : 389
train acc:  0.8046875
train loss:  0.45563623309135437
train gradient:  0.192553147486762
iteration : 390
train acc:  0.921875
train loss:  0.193380668759346
train gradient:  0.07127300776469254
iteration : 391
train acc:  0.8125
train loss:  0.3922566771507263
train gradient:  0.13010045814809168
iteration : 392
train acc:  0.875
train loss:  0.3252559304237366
train gradient:  0.11371430051452806
iteration : 393
train acc:  0.8828125
train loss:  0.30385515093803406
train gradient:  0.19455247132910322
iteration : 394
train acc:  0.8828125
train loss:  0.2743072509765625
train gradient:  0.14682252893346695
iteration : 395
train acc:  0.84375
train loss:  0.3628418445587158
train gradient:  0.1016213470515409
iteration : 396
train acc:  0.8671875
train loss:  0.3352118730545044
train gradient:  0.11939990471233737
iteration : 397
train acc:  0.9375
train loss:  0.18654438853263855
train gradient:  0.06348448768473354
iteration : 398
train acc:  0.8984375
train loss:  0.2760120630264282
train gradient:  0.10739810685506179
iteration : 399
train acc:  0.8828125
train loss:  0.3074491024017334
train gradient:  0.12804289161921115
iteration : 400
train acc:  0.890625
train loss:  0.27184468507766724
train gradient:  0.08049980291869963
iteration : 401
train acc:  0.8515625
train loss:  0.33020612597465515
train gradient:  0.20007170669908098
iteration : 402
train acc:  0.890625
train loss:  0.2910180985927582
train gradient:  0.11733039677166274
iteration : 403
train acc:  0.8671875
train loss:  0.31141266226768494
train gradient:  0.10942883361443537
iteration : 404
train acc:  0.8828125
train loss:  0.26821333169937134
train gradient:  0.08505796578596925
iteration : 405
train acc:  0.875
train loss:  0.24882099032402039
train gradient:  0.09127539758726136
iteration : 406
train acc:  0.8828125
train loss:  0.32920947670936584
train gradient:  0.14849228083471228
iteration : 407
train acc:  0.828125
train loss:  0.36644887924194336
train gradient:  0.16828257526132828
iteration : 408
train acc:  0.90625
train loss:  0.28362974524497986
train gradient:  0.09633238523361347
iteration : 409
train acc:  0.890625
train loss:  0.2264411300420761
train gradient:  0.06115280169275279
iteration : 410
train acc:  0.8515625
train loss:  0.3199988007545471
train gradient:  0.13571833519349846
iteration : 411
train acc:  0.8203125
train loss:  0.41262584924697876
train gradient:  0.1982292399687854
iteration : 412
train acc:  0.8046875
train loss:  0.4579872786998749
train gradient:  0.21289803804946245
iteration : 413
train acc:  0.8984375
train loss:  0.3121781349182129
train gradient:  0.12608182848150604
iteration : 414
train acc:  0.8515625
train loss:  0.3043554127216339
train gradient:  0.13322368872571755
iteration : 415
train acc:  0.828125
train loss:  0.3312212824821472
train gradient:  0.15634083403913068
iteration : 416
train acc:  0.84375
train loss:  0.2768544554710388
train gradient:  0.1015187421960003
iteration : 417
train acc:  0.8046875
train loss:  0.39921292662620544
train gradient:  0.18106274597737682
iteration : 418
train acc:  0.8984375
train loss:  0.2510134279727936
train gradient:  0.11668286280010219
iteration : 419
train acc:  0.875
train loss:  0.3145183324813843
train gradient:  0.14974867498399735
iteration : 420
train acc:  0.8984375
train loss:  0.2600722908973694
train gradient:  0.09733290761299954
iteration : 421
train acc:  0.828125
train loss:  0.39896175265312195
train gradient:  0.22253263832234138
iteration : 422
train acc:  0.8828125
train loss:  0.3142648935317993
train gradient:  0.15085419672255887
iteration : 423
train acc:  0.8046875
train loss:  0.3598060607910156
train gradient:  0.18676409222681872
iteration : 424
train acc:  0.9296875
train loss:  0.22712835669517517
train gradient:  0.09374668118722172
iteration : 425
train acc:  0.90625
train loss:  0.2192080318927765
train gradient:  0.0706109743830863
iteration : 426
train acc:  0.8828125
train loss:  0.26028120517730713
train gradient:  0.10330775793847999
iteration : 427
train acc:  0.8984375
train loss:  0.2503066062927246
train gradient:  0.1213941730494715
iteration : 428
train acc:  0.859375
train loss:  0.2935294210910797
train gradient:  0.12348089332339213
iteration : 429
train acc:  0.859375
train loss:  0.32867753505706787
train gradient:  0.12256951184877024
iteration : 430
train acc:  0.828125
train loss:  0.38483479619026184
train gradient:  0.19422081374824285
iteration : 431
train acc:  0.8671875
train loss:  0.27554094791412354
train gradient:  0.10176054335443731
iteration : 432
train acc:  0.8515625
train loss:  0.33561769127845764
train gradient:  0.14118375997225444
iteration : 433
train acc:  0.8671875
train loss:  0.29556363821029663
train gradient:  0.135718196029749
iteration : 434
train acc:  0.828125
train loss:  0.38193443417549133
train gradient:  0.1681298024720845
iteration : 435
train acc:  0.8828125
train loss:  0.319988489151001
train gradient:  0.12364680310886782
iteration : 436
train acc:  0.8671875
train loss:  0.37232786417007446
train gradient:  0.12093457720647113
iteration : 437
train acc:  0.90625
train loss:  0.2630438804626465
train gradient:  0.09786135579876629
iteration : 438
train acc:  0.828125
train loss:  0.350139319896698
train gradient:  0.10898840329447541
iteration : 439
train acc:  0.8828125
train loss:  0.276233971118927
train gradient:  0.0924467356561261
iteration : 440
train acc:  0.8828125
train loss:  0.2394210696220398
train gradient:  0.09786312981283789
iteration : 441
train acc:  0.859375
train loss:  0.3387737572193146
train gradient:  0.16143606917397607
iteration : 442
train acc:  0.8125
train loss:  0.35920965671539307
train gradient:  0.13941407449434462
iteration : 443
train acc:  0.8359375
train loss:  0.32193225622177124
train gradient:  0.12594894890218247
iteration : 444
train acc:  0.8828125
train loss:  0.2559242248535156
train gradient:  0.09102415187013738
iteration : 445
train acc:  0.890625
train loss:  0.2469082474708557
train gradient:  0.0927406967658034
iteration : 446
train acc:  0.8671875
train loss:  0.30772754549980164
train gradient:  0.12536370426310614
iteration : 447
train acc:  0.890625
train loss:  0.269581139087677
train gradient:  0.13491154657007962
iteration : 448
train acc:  0.8515625
train loss:  0.3199438452720642
train gradient:  0.12032116797982309
iteration : 449
train acc:  0.90625
train loss:  0.2553950548171997
train gradient:  0.10046447506359374
iteration : 450
train acc:  0.875
train loss:  0.3121868669986725
train gradient:  0.10261139458208711
iteration : 451
train acc:  0.8125
train loss:  0.3712655007839203
train gradient:  0.13656296656058314
iteration : 452
train acc:  0.828125
train loss:  0.30801957845687866
train gradient:  0.11828488967363518
iteration : 453
train acc:  0.828125
train loss:  0.37454354763031006
train gradient:  0.13914726007975148
iteration : 454
train acc:  0.8359375
train loss:  0.33825212717056274
train gradient:  0.14787799652287031
iteration : 455
train acc:  0.84375
train loss:  0.2769767642021179
train gradient:  0.21183072416387327
iteration : 456
train acc:  0.8828125
train loss:  0.25420236587524414
train gradient:  0.10102177439624568
iteration : 457
train acc:  0.8515625
train loss:  0.28684893250465393
train gradient:  0.10170805150551965
iteration : 458
train acc:  0.8671875
train loss:  0.2545730769634247
train gradient:  0.07399280336170808
iteration : 459
train acc:  0.8359375
train loss:  0.3137722611427307
train gradient:  0.14528331432075317
iteration : 460
train acc:  0.859375
train loss:  0.285860151052475
train gradient:  0.12344950326581436
iteration : 461
train acc:  0.8984375
train loss:  0.2987077832221985
train gradient:  0.12014721535654459
iteration : 462
train acc:  0.8515625
train loss:  0.37225258350372314
train gradient:  0.180802161950591
iteration : 463
train acc:  0.890625
train loss:  0.30790162086486816
train gradient:  0.13494793993138277
iteration : 464
train acc:  0.828125
train loss:  0.3596588671207428
train gradient:  0.12829165629063727
iteration : 465
train acc:  0.875
train loss:  0.29655203223228455
train gradient:  0.10889811912337201
iteration : 466
train acc:  0.890625
train loss:  0.3215954601764679
train gradient:  0.09926228546746026
iteration : 467
train acc:  0.859375
train loss:  0.3446817696094513
train gradient:  0.17259837387964025
iteration : 468
train acc:  0.875
train loss:  0.258217990398407
train gradient:  0.09696701012414206
iteration : 469
train acc:  0.78125
train loss:  0.38469478487968445
train gradient:  0.1823007350385039
iteration : 470
train acc:  0.9296875
train loss:  0.277201771736145
train gradient:  0.08376419171933261
iteration : 471
train acc:  0.8828125
train loss:  0.30640363693237305
train gradient:  0.15851932046500372
iteration : 472
train acc:  0.890625
train loss:  0.29457464814186096
train gradient:  0.1115280684899972
iteration : 473
train acc:  0.890625
train loss:  0.2811338007450104
train gradient:  0.15886237621090002
iteration : 474
train acc:  0.90625
train loss:  0.28137874603271484
train gradient:  0.13058555717314954
iteration : 475
train acc:  0.8515625
train loss:  0.38026148080825806
train gradient:  0.1478639104449028
iteration : 476
train acc:  0.8515625
train loss:  0.31066104769706726
train gradient:  0.11541623705034809
iteration : 477
train acc:  0.8671875
train loss:  0.30723387002944946
train gradient:  0.08391095288820273
iteration : 478
train acc:  0.8828125
train loss:  0.3232373595237732
train gradient:  0.09749535406508524
iteration : 479
train acc:  0.8671875
train loss:  0.32023513317108154
train gradient:  0.09473723713665058
iteration : 480
train acc:  0.8515625
train loss:  0.3177379369735718
train gradient:  0.10510621170927821
iteration : 481
train acc:  0.859375
train loss:  0.3438417315483093
train gradient:  0.15694871997880622
iteration : 482
train acc:  0.8671875
train loss:  0.32049277424812317
train gradient:  0.10335343928577319
iteration : 483
train acc:  0.8359375
train loss:  0.3063361346721649
train gradient:  0.1551886676588001
iteration : 484
train acc:  0.8671875
train loss:  0.3612176179885864
train gradient:  0.16810641155427952
iteration : 485
train acc:  0.8359375
train loss:  0.3259629011154175
train gradient:  0.10672706039522514
iteration : 486
train acc:  0.8359375
train loss:  0.3300272226333618
train gradient:  0.11825638238801764
iteration : 487
train acc:  0.8828125
train loss:  0.3050239086151123
train gradient:  0.14353578086715346
iteration : 488
train acc:  0.90625
train loss:  0.2527899742126465
train gradient:  0.10718457916736081
iteration : 489
train acc:  0.828125
train loss:  0.4171644449234009
train gradient:  0.2089182953610298
iteration : 490
train acc:  0.84375
train loss:  0.33404314517974854
train gradient:  0.11780709713358452
iteration : 491
train acc:  0.8671875
train loss:  0.29864269495010376
train gradient:  0.14862794165612814
iteration : 492
train acc:  0.890625
train loss:  0.24317365884780884
train gradient:  0.08213823107223094
iteration : 493
train acc:  0.9140625
train loss:  0.2806394696235657
train gradient:  0.08747689230462334
iteration : 494
train acc:  0.8671875
train loss:  0.26910343766212463
train gradient:  0.13969551890769966
iteration : 495
train acc:  0.890625
train loss:  0.24604272842407227
train gradient:  0.08334406513232814
iteration : 496
train acc:  0.84375
train loss:  0.3267991840839386
train gradient:  0.11150418783351651
iteration : 497
train acc:  0.8671875
train loss:  0.325496107339859
train gradient:  0.12162997287861321
iteration : 498
train acc:  0.7890625
train loss:  0.3459332287311554
train gradient:  0.1257171745730897
iteration : 499
train acc:  0.8671875
train loss:  0.2998291850090027
train gradient:  0.0909309479687546
iteration : 500
train acc:  0.8125
train loss:  0.4355127215385437
train gradient:  0.1962942639690743
iteration : 501
train acc:  0.90625
train loss:  0.23208820819854736
train gradient:  0.09489551159713257
iteration : 502
train acc:  0.875
train loss:  0.3166421949863434
train gradient:  0.16805174562072328
iteration : 503
train acc:  0.859375
train loss:  0.2904844880104065
train gradient:  0.09978825010815112
iteration : 504
train acc:  0.8515625
train loss:  0.31872886419296265
train gradient:  0.1888243845476532
iteration : 505
train acc:  0.890625
train loss:  0.33553919196128845
train gradient:  0.1463203574478348
iteration : 506
train acc:  0.8359375
train loss:  0.34426528215408325
train gradient:  0.12550262162493384
iteration : 507
train acc:  0.8046875
train loss:  0.3970624804496765
train gradient:  0.21559207999849575
iteration : 508
train acc:  0.890625
train loss:  0.2933388948440552
train gradient:  0.09127250954882646
iteration : 509
train acc:  0.8515625
train loss:  0.3586813807487488
train gradient:  0.20420991172479508
iteration : 510
train acc:  0.9140625
train loss:  0.27897071838378906
train gradient:  0.10685424115746961
iteration : 511
train acc:  0.84375
train loss:  0.3313424587249756
train gradient:  0.10454044753502668
iteration : 512
train acc:  0.890625
train loss:  0.33135461807250977
train gradient:  0.15684009831769585
iteration : 513
train acc:  0.9140625
train loss:  0.2172430455684662
train gradient:  0.08657116868086857
iteration : 514
train acc:  0.8046875
train loss:  0.40398675203323364
train gradient:  0.18867429061023547
iteration : 515
train acc:  0.9140625
train loss:  0.24457599222660065
train gradient:  0.07902892079872188
iteration : 516
train acc:  0.921875
train loss:  0.23644790053367615
train gradient:  0.09171205094992589
iteration : 517
train acc:  0.8046875
train loss:  0.32519960403442383
train gradient:  0.0999135817077079
iteration : 518
train acc:  0.828125
train loss:  0.3614884316921234
train gradient:  0.14584697362607207
iteration : 519
train acc:  0.921875
train loss:  0.2368113100528717
train gradient:  0.07325182679602289
iteration : 520
train acc:  0.859375
train loss:  0.38127684593200684
train gradient:  0.18069328793582276
iteration : 521
train acc:  0.875
train loss:  0.2893311381340027
train gradient:  0.08682476644440211
iteration : 522
train acc:  0.7890625
train loss:  0.48556283116340637
train gradient:  0.25720347911035524
iteration : 523
train acc:  0.8359375
train loss:  0.42926615476608276
train gradient:  0.17259333657085452
iteration : 524
train acc:  0.84375
train loss:  0.3371421694755554
train gradient:  0.11554484372728589
iteration : 525
train acc:  0.8515625
train loss:  0.35467779636383057
train gradient:  0.1368510621941136
iteration : 526
train acc:  0.9140625
train loss:  0.2555372714996338
train gradient:  0.11831563541964499
iteration : 527
train acc:  0.9296875
train loss:  0.19666951894760132
train gradient:  0.08664026744447818
iteration : 528
train acc:  0.859375
train loss:  0.28677576780319214
train gradient:  0.11812302465887158
iteration : 529
train acc:  0.8515625
train loss:  0.3696308434009552
train gradient:  0.19211854546120788
iteration : 530
train acc:  0.796875
train loss:  0.41710329055786133
train gradient:  0.18866893824159797
iteration : 531
train acc:  0.875
train loss:  0.3210432529449463
train gradient:  0.4246421919947252
iteration : 532
train acc:  0.8828125
train loss:  0.2965068817138672
train gradient:  0.10803658537330052
iteration : 533
train acc:  0.890625
train loss:  0.28167182207107544
train gradient:  0.10268799614972911
iteration : 534
train acc:  0.8515625
train loss:  0.32928213477134705
train gradient:  0.11027466276105474
iteration : 535
train acc:  0.8515625
train loss:  0.3647006154060364
train gradient:  0.1339026356954432
iteration : 536
train acc:  0.84375
train loss:  0.341880202293396
train gradient:  0.13864135829117785
iteration : 537
train acc:  0.84375
train loss:  0.35166335105895996
train gradient:  0.14376774318855723
iteration : 538
train acc:  0.875
train loss:  0.2705995738506317
train gradient:  0.0782757848185905
iteration : 539
train acc:  0.875
train loss:  0.3628554344177246
train gradient:  0.16013086759699766
iteration : 540
train acc:  0.859375
train loss:  0.32007282972335815
train gradient:  0.11976190239126436
iteration : 541
train acc:  0.8671875
train loss:  0.30274343490600586
train gradient:  0.11085333279973013
iteration : 542
train acc:  0.9140625
train loss:  0.30213627219200134
train gradient:  0.12928555795855767
iteration : 543
train acc:  0.8671875
train loss:  0.3519723415374756
train gradient:  0.19050704675970226
iteration : 544
train acc:  0.8828125
train loss:  0.33574002981185913
train gradient:  0.1252640262215198
iteration : 545
train acc:  0.8359375
train loss:  0.3507874011993408
train gradient:  0.11647513917536431
iteration : 546
train acc:  0.9375
train loss:  0.21900880336761475
train gradient:  0.0693724003160671
iteration : 547
train acc:  0.8671875
train loss:  0.2974809408187866
train gradient:  0.09565368915284377
iteration : 548
train acc:  0.875
train loss:  0.29304808378219604
train gradient:  0.10384309689296799
iteration : 549
train acc:  0.8125
train loss:  0.3498295843601227
train gradient:  0.1530230411240219
iteration : 550
train acc:  0.84375
train loss:  0.3230148255825043
train gradient:  0.141043071628242
iteration : 551
train acc:  0.8984375
train loss:  0.30403849482536316
train gradient:  0.10290337250319519
iteration : 552
train acc:  0.828125
train loss:  0.40529707074165344
train gradient:  0.18628799029293958
iteration : 553
train acc:  0.875
train loss:  0.28575778007507324
train gradient:  0.08127511734478127
iteration : 554
train acc:  0.875
train loss:  0.2852576673030853
train gradient:  0.10889345608992548
iteration : 555
train acc:  0.8828125
train loss:  0.2319124937057495
train gradient:  0.08125068878474466
iteration : 556
train acc:  0.875
train loss:  0.25993508100509644
train gradient:  0.10240271719572475
iteration : 557
train acc:  0.859375
train loss:  0.32229337096214294
train gradient:  0.13775048597352274
iteration : 558
train acc:  0.859375
train loss:  0.33443763852119446
train gradient:  0.11874478756149151
iteration : 559
train acc:  0.9375
train loss:  0.2731677293777466
train gradient:  0.07091232224322849
iteration : 560
train acc:  0.84375
train loss:  0.34254947304725647
train gradient:  0.1474421769142096
iteration : 561
train acc:  0.84375
train loss:  0.3682241439819336
train gradient:  0.13640339069855717
iteration : 562
train acc:  0.90625
train loss:  0.24128685891628265
train gradient:  0.10164413551323889
iteration : 563
train acc:  0.8984375
train loss:  0.32173657417297363
train gradient:  0.12033297271473728
iteration : 564
train acc:  0.8515625
train loss:  0.368907630443573
train gradient:  0.16870843404060343
iteration : 565
train acc:  0.890625
train loss:  0.29475653171539307
train gradient:  0.09878078591524171
iteration : 566
train acc:  0.8359375
train loss:  0.3722243309020996
train gradient:  0.16880652058789014
iteration : 567
train acc:  0.875
train loss:  0.3734200596809387
train gradient:  0.24424950253176864
iteration : 568
train acc:  0.8828125
train loss:  0.27797195315361023
train gradient:  0.10016837101989912
iteration : 569
train acc:  0.84375
train loss:  0.32289355993270874
train gradient:  0.11455884921593483
iteration : 570
train acc:  0.859375
train loss:  0.3571946918964386
train gradient:  0.12808449494513027
iteration : 571
train acc:  0.828125
train loss:  0.42063626646995544
train gradient:  0.19104209855489115
iteration : 572
train acc:  0.8203125
train loss:  0.424679696559906
train gradient:  0.21014291147616548
iteration : 573
train acc:  0.890625
train loss:  0.32145756483078003
train gradient:  0.12160638788584265
iteration : 574
train acc:  0.84375
train loss:  0.37160149216651917
train gradient:  0.12544877368427645
iteration : 575
train acc:  0.875
train loss:  0.2556959390640259
train gradient:  0.113274855845146
iteration : 576
train acc:  0.859375
train loss:  0.3598369061946869
train gradient:  0.1429238181795971
iteration : 577
train acc:  0.8203125
train loss:  0.4065123200416565
train gradient:  0.21559733515286705
iteration : 578
train acc:  0.8359375
train loss:  0.40163135528564453
train gradient:  0.1345591446977536
iteration : 579
train acc:  0.875
train loss:  0.2723732590675354
train gradient:  0.1349183199738915
iteration : 580
train acc:  0.859375
train loss:  0.31826817989349365
train gradient:  0.10232894564570655
iteration : 581
train acc:  0.875
train loss:  0.27732956409454346
train gradient:  0.07475340288782137
iteration : 582
train acc:  0.8515625
train loss:  0.30119091272354126
train gradient:  0.09376595447976051
iteration : 583
train acc:  0.7890625
train loss:  0.39200466871261597
train gradient:  0.18954342039636274
iteration : 584
train acc:  0.890625
train loss:  0.268375039100647
train gradient:  0.11108742090526132
iteration : 585
train acc:  0.84375
train loss:  0.3048318922519684
train gradient:  0.13159678782874334
iteration : 586
train acc:  0.8828125
train loss:  0.27247297763824463
train gradient:  0.11934579764743726
iteration : 587
train acc:  0.84375
train loss:  0.3393990993499756
train gradient:  0.13732641027223225
iteration : 588
train acc:  0.8671875
train loss:  0.3073037266731262
train gradient:  0.1803941494185095
iteration : 589
train acc:  0.8828125
train loss:  0.256445974111557
train gradient:  0.09136343909113315
iteration : 590
train acc:  0.890625
train loss:  0.3097856938838959
train gradient:  0.1125584215619548
iteration : 591
train acc:  0.859375
train loss:  0.3021823763847351
train gradient:  0.13722719412940643
iteration : 592
train acc:  0.9296875
train loss:  0.2386004775762558
train gradient:  0.08427181144041247
iteration : 593
train acc:  0.921875
train loss:  0.20231468975543976
train gradient:  0.0776588716294706
iteration : 594
train acc:  0.8515625
train loss:  0.3275567889213562
train gradient:  0.1304176242412392
iteration : 595
train acc:  0.90625
train loss:  0.2368248999118805
train gradient:  0.09168753343024261
iteration : 596
train acc:  0.859375
train loss:  0.2712119519710541
train gradient:  0.09105675667081849
iteration : 597
train acc:  0.8828125
train loss:  0.25736480951309204
train gradient:  0.08528519354111888
iteration : 598
train acc:  0.859375
train loss:  0.3001096248626709
train gradient:  0.09501829115702054
iteration : 599
train acc:  0.9140625
train loss:  0.2426087111234665
train gradient:  0.08032022774580087
iteration : 600
train acc:  0.8984375
train loss:  0.2258181869983673
train gradient:  0.09074390211743176
iteration : 601
train acc:  0.84375
train loss:  0.3848711848258972
train gradient:  0.15195485538893366
iteration : 602
train acc:  0.828125
train loss:  0.36689239740371704
train gradient:  0.13198283863350518
iteration : 603
train acc:  0.875
train loss:  0.31163525581359863
train gradient:  0.1120883224716584
iteration : 604
train acc:  0.8671875
train loss:  0.26815497875213623
train gradient:  0.08199614446542641
iteration : 605
train acc:  0.8984375
train loss:  0.29613810777664185
train gradient:  0.11574334892073786
iteration : 606
train acc:  0.890625
train loss:  0.26644688844680786
train gradient:  0.08687001293388898
iteration : 607
train acc:  0.890625
train loss:  0.22934484481811523
train gradient:  0.08173457571400927
iteration : 608
train acc:  0.859375
train loss:  0.3323741555213928
train gradient:  0.20916591873379906
iteration : 609
train acc:  0.875
train loss:  0.28913724422454834
train gradient:  0.1294329274495758
iteration : 610
train acc:  0.8046875
train loss:  0.3212414085865021
train gradient:  0.11941728857798647
iteration : 611
train acc:  0.859375
train loss:  0.36193519830703735
train gradient:  0.15874343072430397
iteration : 612
train acc:  0.8671875
train loss:  0.2507040798664093
train gradient:  0.10982368597764286
iteration : 613
train acc:  0.8671875
train loss:  0.32848677039146423
train gradient:  0.17487895730242292
iteration : 614
train acc:  0.8515625
train loss:  0.3203970789909363
train gradient:  0.12999437371870542
iteration : 615
train acc:  0.8828125
train loss:  0.24616739153862
train gradient:  0.09130132280976508
iteration : 616
train acc:  0.875
train loss:  0.31138449907302856
train gradient:  0.11356075071033091
iteration : 617
train acc:  0.875
train loss:  0.3381590247154236
train gradient:  0.1443317973929607
iteration : 618
train acc:  0.875
train loss:  0.2573586106300354
train gradient:  0.10310164710641885
iteration : 619
train acc:  0.8359375
train loss:  0.33829712867736816
train gradient:  0.12497064524687283
iteration : 620
train acc:  0.859375
train loss:  0.27617818117141724
train gradient:  0.09974078481954861
iteration : 621
train acc:  0.90625
train loss:  0.2539038360118866
train gradient:  0.08669896831718935
iteration : 622
train acc:  0.859375
train loss:  0.32852858304977417
train gradient:  0.13555378460732911
iteration : 623
train acc:  0.8671875
train loss:  0.2509031295776367
train gradient:  0.07244201859628754
iteration : 624
train acc:  0.8359375
train loss:  0.38175731897354126
train gradient:  0.15666409256086464
iteration : 625
train acc:  0.9140625
train loss:  0.2540757954120636
train gradient:  0.10046354323605987
iteration : 626
train acc:  0.828125
train loss:  0.33626171946525574
train gradient:  0.1856875958552422
iteration : 627
train acc:  0.828125
train loss:  0.36139389872550964
train gradient:  0.16986821499457377
iteration : 628
train acc:  0.875
train loss:  0.3486335873603821
train gradient:  0.6447702108882245
iteration : 629
train acc:  0.8515625
train loss:  0.3111893832683563
train gradient:  0.155237560406443
iteration : 630
train acc:  0.8359375
train loss:  0.38153815269470215
train gradient:  0.20242707998986692
iteration : 631
train acc:  0.875
train loss:  0.29208678007125854
train gradient:  0.11749419749264461
iteration : 632
train acc:  0.90625
train loss:  0.2600637674331665
train gradient:  0.0967667310840663
iteration : 633
train acc:  0.828125
train loss:  0.3525046706199646
train gradient:  0.2425197520994587
iteration : 634
train acc:  0.8671875
train loss:  0.313693106174469
train gradient:  0.14337177803066808
iteration : 635
train acc:  0.875
train loss:  0.2966969311237335
train gradient:  0.12639060594017096
iteration : 636
train acc:  0.890625
train loss:  0.2532142996788025
train gradient:  0.1071959746578567
iteration : 637
train acc:  0.8359375
train loss:  0.35556477308273315
train gradient:  0.1398581643791994
iteration : 638
train acc:  0.875
train loss:  0.29247671365737915
train gradient:  0.1058879872148598
iteration : 639
train acc:  0.921875
train loss:  0.2737455666065216
train gradient:  0.1253731317487074
iteration : 640
train acc:  0.890625
train loss:  0.34333229064941406
train gradient:  0.12945609270252634
iteration : 641
train acc:  0.84375
train loss:  0.3805188536643982
train gradient:  0.17879190688282232
iteration : 642
train acc:  0.8359375
train loss:  0.3767445683479309
train gradient:  0.13234443430812695
iteration : 643
train acc:  0.9140625
train loss:  0.2599087357521057
train gradient:  0.11566422326166591
iteration : 644
train acc:  0.8984375
train loss:  0.24160799384117126
train gradient:  0.07890749313755013
iteration : 645
train acc:  0.875
train loss:  0.27517056465148926
train gradient:  0.13856481083262334
iteration : 646
train acc:  0.8515625
train loss:  0.35222306847572327
train gradient:  0.17999724487550953
iteration : 647
train acc:  0.8828125
train loss:  0.31390833854675293
train gradient:  0.15938127144353056
iteration : 648
train acc:  0.90625
train loss:  0.25346890091896057
train gradient:  0.10246172392906051
iteration : 649
train acc:  0.859375
train loss:  0.30615079402923584
train gradient:  0.11813346676647812
iteration : 650
train acc:  0.859375
train loss:  0.3507351875305176
train gradient:  0.15912908516457944
iteration : 651
train acc:  0.859375
train loss:  0.37780022621154785
train gradient:  0.24592352212142476
iteration : 652
train acc:  0.859375
train loss:  0.42702218890190125
train gradient:  0.25162165476244014
iteration : 653
train acc:  0.8359375
train loss:  0.375765323638916
train gradient:  0.1655312634063486
iteration : 654
train acc:  0.8671875
train loss:  0.3320290446281433
train gradient:  0.16459543108591526
iteration : 655
train acc:  0.875
train loss:  0.3435080051422119
train gradient:  0.20147857219970933
iteration : 656
train acc:  0.921875
train loss:  0.23308107256889343
train gradient:  0.06980922664022343
iteration : 657
train acc:  0.8046875
train loss:  0.3877207338809967
train gradient:  0.1472210197051236
iteration : 658
train acc:  0.859375
train loss:  0.31802845001220703
train gradient:  0.1360229696462411
iteration : 659
train acc:  0.890625
train loss:  0.3072073459625244
train gradient:  0.12343217262058145
iteration : 660
train acc:  0.875
train loss:  0.355232298374176
train gradient:  0.18857720454198248
iteration : 661
train acc:  0.8828125
train loss:  0.32619619369506836
train gradient:  0.12847882458979692
iteration : 662
train acc:  0.8828125
train loss:  0.31202369928359985
train gradient:  0.07100180162186039
iteration : 663
train acc:  0.8515625
train loss:  0.3435627818107605
train gradient:  0.13082517824812173
iteration : 664
train acc:  0.859375
train loss:  0.29534196853637695
train gradient:  0.0951748171239441
iteration : 665
train acc:  0.8515625
train loss:  0.3195589780807495
train gradient:  0.1076158631859419
iteration : 666
train acc:  0.8671875
train loss:  0.2889719009399414
train gradient:  0.08983026366150348
iteration : 667
train acc:  0.875
train loss:  0.2686781883239746
train gradient:  0.09619617414202009
iteration : 668
train acc:  0.8203125
train loss:  0.34889477491378784
train gradient:  0.12219076220241054
iteration : 669
train acc:  0.8828125
train loss:  0.2654818892478943
train gradient:  0.07260233809260887
iteration : 670
train acc:  0.8984375
train loss:  0.2606361508369446
train gradient:  0.08687379534631161
iteration : 671
train acc:  0.8828125
train loss:  0.41882622241973877
train gradient:  0.1573514500760742
iteration : 672
train acc:  0.875
train loss:  0.30745238065719604
train gradient:  0.12164031812167594
iteration : 673
train acc:  0.875
train loss:  0.3398052155971527
train gradient:  0.1576686273309287
iteration : 674
train acc:  0.9140625
train loss:  0.26830965280532837
train gradient:  0.06702527480059885
iteration : 675
train acc:  0.890625
train loss:  0.30596470832824707
train gradient:  0.12582929558175693
iteration : 676
train acc:  0.8828125
train loss:  0.31224149465560913
train gradient:  0.11013140448534715
iteration : 677
train acc:  0.8046875
train loss:  0.4137630760669708
train gradient:  0.19171525862745908
iteration : 678
train acc:  0.84375
train loss:  0.3226880729198456
train gradient:  0.1541062693578868
iteration : 679
train acc:  0.8828125
train loss:  0.25058773159980774
train gradient:  0.09372232956232833
iteration : 680
train acc:  0.828125
train loss:  0.3402988910675049
train gradient:  0.15057645754244114
iteration : 681
train acc:  0.890625
train loss:  0.23810796439647675
train gradient:  0.07421056593447917
iteration : 682
train acc:  0.828125
train loss:  0.34873244166374207
train gradient:  0.15991829820753461
iteration : 683
train acc:  0.875
train loss:  0.2893311381340027
train gradient:  0.08714752600561465
iteration : 684
train acc:  0.890625
train loss:  0.2963414490222931
train gradient:  0.12368125221544905
iteration : 685
train acc:  0.890625
train loss:  0.2993302643299103
train gradient:  0.1200762666973076
iteration : 686
train acc:  0.8671875
train loss:  0.316336989402771
train gradient:  0.15358205982006606
iteration : 687
train acc:  0.8828125
train loss:  0.2905215322971344
train gradient:  0.14024397406800393
iteration : 688
train acc:  0.8671875
train loss:  0.3815186619758606
train gradient:  0.18348397826285784
iteration : 689
train acc:  0.8671875
train loss:  0.2991311848163605
train gradient:  0.12135923435658284
iteration : 690
train acc:  0.8046875
train loss:  0.40239810943603516
train gradient:  0.2031441812284488
iteration : 691
train acc:  0.84375
train loss:  0.30738964676856995
train gradient:  0.19838787193774976
iteration : 692
train acc:  0.875
train loss:  0.28157782554626465
train gradient:  0.12404224730698095
iteration : 693
train acc:  0.8359375
train loss:  0.3532240390777588
train gradient:  0.1678460630494497
iteration : 694
train acc:  0.8359375
train loss:  0.36039048433303833
train gradient:  0.2131304797144768
iteration : 695
train acc:  0.875
train loss:  0.2960251569747925
train gradient:  0.12783515769575435
iteration : 696
train acc:  0.8359375
train loss:  0.34185606241226196
train gradient:  0.14921896594186806
iteration : 697
train acc:  0.890625
train loss:  0.27612024545669556
train gradient:  0.12632411174872638
iteration : 698
train acc:  0.828125
train loss:  0.3285517394542694
train gradient:  0.13926550528170678
iteration : 699
train acc:  0.8828125
train loss:  0.2819974720478058
train gradient:  0.09341667809035456
iteration : 700
train acc:  0.859375
train loss:  0.3331438899040222
train gradient:  0.1842266802370151
iteration : 701
train acc:  0.84375
train loss:  0.360274076461792
train gradient:  0.14559597251141707
iteration : 702
train acc:  0.890625
train loss:  0.28813785314559937
train gradient:  0.14803560289612594
iteration : 703
train acc:  0.875
train loss:  0.3322981297969818
train gradient:  0.11868460173873062
iteration : 704
train acc:  0.90625
train loss:  0.2132563292980194
train gradient:  0.08176415630930108
iteration : 705
train acc:  0.859375
train loss:  0.29628583788871765
train gradient:  0.12756839880645177
iteration : 706
train acc:  0.8671875
train loss:  0.2885247766971588
train gradient:  0.11805663717900879
iteration : 707
train acc:  0.859375
train loss:  0.269824743270874
train gradient:  0.10607569512467419
iteration : 708
train acc:  0.8515625
train loss:  0.36153531074523926
train gradient:  0.13725970821465971
iteration : 709
train acc:  0.8828125
train loss:  0.309814453125
train gradient:  0.12083265898260839
iteration : 710
train acc:  0.8671875
train loss:  0.3380660116672516
train gradient:  0.16024808513209998
iteration : 711
train acc:  0.84375
train loss:  0.38133126497268677
train gradient:  0.199645043263378
iteration : 712
train acc:  0.828125
train loss:  0.34918463230133057
train gradient:  0.18932938450877007
iteration : 713
train acc:  0.921875
train loss:  0.2719714641571045
train gradient:  0.1458660601171872
iteration : 714
train acc:  0.90625
train loss:  0.258284330368042
train gradient:  0.09158577132896979
iteration : 715
train acc:  0.8359375
train loss:  0.40632206201553345
train gradient:  0.1479263687625762
iteration : 716
train acc:  0.921875
train loss:  0.22475460171699524
train gradient:  0.11074556682527717
iteration : 717
train acc:  0.90625
train loss:  0.282719224691391
train gradient:  0.11274183991335293
iteration : 718
train acc:  0.890625
train loss:  0.2631721496582031
train gradient:  0.1057502014955034
iteration : 719
train acc:  0.890625
train loss:  0.2860036790370941
train gradient:  0.10613029452336772
iteration : 720
train acc:  0.8671875
train loss:  0.34579598903656006
train gradient:  0.10966676772835786
iteration : 721
train acc:  0.875
train loss:  0.301726371049881
train gradient:  0.09638569979115257
iteration : 722
train acc:  0.8671875
train loss:  0.27554211020469666
train gradient:  0.11357936537837454
iteration : 723
train acc:  0.84375
train loss:  0.38244014978408813
train gradient:  0.20847144724436617
iteration : 724
train acc:  0.890625
train loss:  0.277099072933197
train gradient:  0.09794520335161687
iteration : 725
train acc:  0.84375
train loss:  0.3646327257156372
train gradient:  0.140978141837297
iteration : 726
train acc:  0.8515625
train loss:  0.302792489528656
train gradient:  0.12288425127451347
iteration : 727
train acc:  0.8046875
train loss:  0.3357992172241211
train gradient:  0.1191870337971709
iteration : 728
train acc:  0.84375
train loss:  0.3004031777381897
train gradient:  0.08490671821993218
iteration : 729
train acc:  0.828125
train loss:  0.39127445220947266
train gradient:  0.1603499146565963
iteration : 730
train acc:  0.8828125
train loss:  0.2664695382118225
train gradient:  0.10608882031190454
iteration : 731
train acc:  0.8984375
train loss:  0.24412357807159424
train gradient:  0.09572690270869912
iteration : 732
train acc:  0.875
train loss:  0.3083330988883972
train gradient:  0.08864671153330604
iteration : 733
train acc:  0.859375
train loss:  0.3438745141029358
train gradient:  0.12241965607442953
iteration : 734
train acc:  0.8515625
train loss:  0.3401170074939728
train gradient:  0.13306620109610545
iteration : 735
train acc:  0.828125
train loss:  0.3415621817111969
train gradient:  0.2229938787590014
iteration : 736
train acc:  0.875
train loss:  0.31103670597076416
train gradient:  0.09405015661965749
iteration : 737
train acc:  0.859375
train loss:  0.2763407826423645
train gradient:  0.14184629547303257
iteration : 738
train acc:  0.8125
train loss:  0.43580394983291626
train gradient:  0.23122973329769353
iteration : 739
train acc:  0.84375
train loss:  0.3129695653915405
train gradient:  0.12982926162340908
iteration : 740
train acc:  0.8984375
train loss:  0.2813498079776764
train gradient:  0.11635220793517273
iteration : 741
train acc:  0.8828125
train loss:  0.29201003909111023
train gradient:  0.11020143965579586
iteration : 742
train acc:  0.8671875
train loss:  0.26530295610427856
train gradient:  0.10747270851819389
iteration : 743
train acc:  0.8203125
train loss:  0.3201916813850403
train gradient:  0.10345194163918635
iteration : 744
train acc:  0.890625
train loss:  0.2968212068080902
train gradient:  0.1341237351317422
iteration : 745
train acc:  0.84375
train loss:  0.32062768936157227
train gradient:  0.10646161553140729
iteration : 746
train acc:  0.84375
train loss:  0.34059903025627136
train gradient:  0.10220403157928418
iteration : 747
train acc:  0.8515625
train loss:  0.3004413843154907
train gradient:  0.10055209356458822
iteration : 748
train acc:  0.8515625
train loss:  0.30004289746284485
train gradient:  0.09854052706972045
iteration : 749
train acc:  0.8515625
train loss:  0.27337193489074707
train gradient:  0.13240177877708748
iteration : 750
train acc:  0.859375
train loss:  0.2862381339073181
train gradient:  0.13347186121829815
iteration : 751
train acc:  0.8828125
train loss:  0.30375146865844727
train gradient:  0.11898330275943332
iteration : 752
train acc:  0.875
train loss:  0.299696683883667
train gradient:  0.0866025703802292
iteration : 753
train acc:  0.8046875
train loss:  0.45827165246009827
train gradient:  0.2297147174303455
iteration : 754
train acc:  0.84375
train loss:  0.29682838916778564
train gradient:  0.13440009180387952
iteration : 755
train acc:  0.8359375
train loss:  0.3225758373737335
train gradient:  0.21176723194244543
iteration : 756
train acc:  0.875
train loss:  0.3019319772720337
train gradient:  0.11143067631327268
iteration : 757
train acc:  0.890625
train loss:  0.256002813577652
train gradient:  0.08793336842325007
iteration : 758
train acc:  0.859375
train loss:  0.3371950387954712
train gradient:  0.14943188307600852
iteration : 759
train acc:  0.8359375
train loss:  0.32576191425323486
train gradient:  0.13909762743344206
iteration : 760
train acc:  0.890625
train loss:  0.29810404777526855
train gradient:  0.21548907312480353
iteration : 761
train acc:  0.8828125
train loss:  0.2768399715423584
train gradient:  0.12688822759457552
iteration : 762
train acc:  0.890625
train loss:  0.2668881416320801
train gradient:  0.06253842518435962
iteration : 763
train acc:  0.8359375
train loss:  0.3739146590232849
train gradient:  0.1668722183775786
iteration : 764
train acc:  0.828125
train loss:  0.38364845514297485
train gradient:  0.1807609349256875
iteration : 765
train acc:  0.859375
train loss:  0.29805678129196167
train gradient:  0.08281997913410867
iteration : 766
train acc:  0.90625
train loss:  0.24749349057674408
train gradient:  0.06832924021116392
iteration : 767
train acc:  0.8828125
train loss:  0.2881529629230499
train gradient:  0.1169299580923446
iteration : 768
train acc:  0.8671875
train loss:  0.3101959824562073
train gradient:  0.15526541295402196
iteration : 769
train acc:  0.890625
train loss:  0.2700566053390503
train gradient:  0.11891865886819025
iteration : 770
train acc:  0.8203125
train loss:  0.3650408387184143
train gradient:  0.16359299978170472
iteration : 771
train acc:  0.859375
train loss:  0.3168138265609741
train gradient:  0.11774822361801113
iteration : 772
train acc:  0.8828125
train loss:  0.28050220012664795
train gradient:  0.0942065827001164
iteration : 773
train acc:  0.9140625
train loss:  0.2151317596435547
train gradient:  0.0633118105146986
iteration : 774
train acc:  0.9140625
train loss:  0.29358842968940735
train gradient:  0.0983433153685475
iteration : 775
train acc:  0.8671875
train loss:  0.3095705211162567
train gradient:  0.15152224250105345
iteration : 776
train acc:  0.8203125
train loss:  0.373722642660141
train gradient:  0.15175115082728302
iteration : 777
train acc:  0.9296875
train loss:  0.20946228504180908
train gradient:  0.0646758452913943
iteration : 778
train acc:  0.859375
train loss:  0.3040117025375366
train gradient:  0.10849007456103435
iteration : 779
train acc:  0.8359375
train loss:  0.328176349401474
train gradient:  0.1349383804917481
iteration : 780
train acc:  0.875
train loss:  0.2916681170463562
train gradient:  0.15622088791153615
iteration : 781
train acc:  0.890625
train loss:  0.24538052082061768
train gradient:  0.13661492464136685
iteration : 782
train acc:  0.84375
train loss:  0.333357572555542
train gradient:  0.16306740001690045
iteration : 783
train acc:  0.8984375
train loss:  0.2761479318141937
train gradient:  0.10901906093379105
iteration : 784
train acc:  0.8515625
train loss:  0.31536006927490234
train gradient:  0.12147015551459767
iteration : 785
train acc:  0.8671875
train loss:  0.29707857966423035
train gradient:  0.12159695333082522
iteration : 786
train acc:  0.859375
train loss:  0.31174468994140625
train gradient:  0.24248382841830815
iteration : 787
train acc:  0.8515625
train loss:  0.37300539016723633
train gradient:  0.18719166053168407
iteration : 788
train acc:  0.8515625
train loss:  0.3166158199310303
train gradient:  0.16112996645856864
iteration : 789
train acc:  0.8359375
train loss:  0.33705198764801025
train gradient:  0.11136125488986044
iteration : 790
train acc:  0.859375
train loss:  0.29419758915901184
train gradient:  0.10719683091336331
iteration : 791
train acc:  0.8984375
train loss:  0.24970188736915588
train gradient:  0.09541920545722259
iteration : 792
train acc:  0.8671875
train loss:  0.2894226908683777
train gradient:  0.12526286569853326
iteration : 793
train acc:  0.921875
train loss:  0.2439376562833786
train gradient:  0.11106739112411818
iteration : 794
train acc:  0.90625
train loss:  0.21284638345241547
train gradient:  0.0750492185530377
iteration : 795
train acc:  0.875
train loss:  0.3113875985145569
train gradient:  0.10655152938050802
iteration : 796
train acc:  0.84375
train loss:  0.3631453216075897
train gradient:  0.23524901252371547
iteration : 797
train acc:  0.8984375
train loss:  0.3082798421382904
train gradient:  0.16569004075316776
iteration : 798
train acc:  0.8828125
train loss:  0.28228259086608887
train gradient:  0.11405320829051199
iteration : 799
train acc:  0.9140625
train loss:  0.3076564073562622
train gradient:  0.10195716600505494
iteration : 800
train acc:  0.8515625
train loss:  0.33041757345199585
train gradient:  0.1516816443724658
iteration : 801
train acc:  0.84375
train loss:  0.33294999599456787
train gradient:  0.1512869968076075
iteration : 802
train acc:  0.8671875
train loss:  0.36421293020248413
train gradient:  0.1564338130189617
iteration : 803
train acc:  0.8359375
train loss:  0.325866162776947
train gradient:  0.17145774332993163
iteration : 804
train acc:  0.8671875
train loss:  0.3076583743095398
train gradient:  0.16607129167432386
iteration : 805
train acc:  0.875
train loss:  0.32170581817626953
train gradient:  0.12254855794943087
iteration : 806
train acc:  0.9296875
train loss:  0.2198115736246109
train gradient:  0.0841375464930039
iteration : 807
train acc:  0.84375
train loss:  0.32347697019577026
train gradient:  0.12563094767496108
iteration : 808
train acc:  0.84375
train loss:  0.3378385007381439
train gradient:  0.10466631660499039
iteration : 809
train acc:  0.8203125
train loss:  0.399030864238739
train gradient:  0.22251542906815258
iteration : 810
train acc:  0.8671875
train loss:  0.31596213579177856
train gradient:  0.11844609914755466
iteration : 811
train acc:  0.859375
train loss:  0.3529185652732849
train gradient:  0.20049162687654526
iteration : 812
train acc:  0.84375
train loss:  0.37417545914649963
train gradient:  0.16056749550842736
iteration : 813
train acc:  0.859375
train loss:  0.36682382225990295
train gradient:  0.14885363649375377
iteration : 814
train acc:  0.8984375
train loss:  0.23767709732055664
train gradient:  0.07914171272119765
iteration : 815
train acc:  0.9375
train loss:  0.2070121169090271
train gradient:  0.0662870055327746
iteration : 816
train acc:  0.84375
train loss:  0.3483961820602417
train gradient:  0.12363798370924495
iteration : 817
train acc:  0.8671875
train loss:  0.3324349522590637
train gradient:  0.10938699753307149
iteration : 818
train acc:  0.828125
train loss:  0.33644288778305054
train gradient:  0.141723144673157
iteration : 819
train acc:  0.828125
train loss:  0.34282830357551575
train gradient:  0.17022701352828778
iteration : 820
train acc:  0.875
train loss:  0.2961970865726471
train gradient:  0.11131563995723992
iteration : 821
train acc:  0.875
train loss:  0.28423595428466797
train gradient:  0.07958626537038237
iteration : 822
train acc:  0.8359375
train loss:  0.36914384365081787
train gradient:  0.15791550081829592
iteration : 823
train acc:  0.8203125
train loss:  0.40357068181037903
train gradient:  0.17280561974775233
iteration : 824
train acc:  0.828125
train loss:  0.3557494282722473
train gradient:  0.24688835767090106
iteration : 825
train acc:  0.875
train loss:  0.3266282379627228
train gradient:  0.1282039036640773
iteration : 826
train acc:  0.8203125
train loss:  0.3919188976287842
train gradient:  0.13955466749515932
iteration : 827
train acc:  0.8671875
train loss:  0.3321267068386078
train gradient:  0.1305102456037104
iteration : 828
train acc:  0.84375
train loss:  0.33923065662384033
train gradient:  0.16947961246430177
iteration : 829
train acc:  0.84375
train loss:  0.2802480161190033
train gradient:  0.08614920888255395
iteration : 830
train acc:  0.8046875
train loss:  0.35142022371292114
train gradient:  0.13284480823468703
iteration : 831
train acc:  0.9140625
train loss:  0.25031402707099915
train gradient:  0.07380077684285674
iteration : 832
train acc:  0.8984375
train loss:  0.28153538703918457
train gradient:  0.07743927599040913
iteration : 833
train acc:  0.8515625
train loss:  0.3073595464229584
train gradient:  0.09355181940780825
iteration : 834
train acc:  0.875
train loss:  0.2929467260837555
train gradient:  0.10191572156300772
iteration : 835
train acc:  0.90625
train loss:  0.2765459716320038
train gradient:  0.1201568861078111
iteration : 836
train acc:  0.8359375
train loss:  0.36430084705352783
train gradient:  0.12269363435225017
iteration : 837
train acc:  0.8984375
train loss:  0.2918001413345337
train gradient:  0.13812830235589704
iteration : 838
train acc:  0.8828125
train loss:  0.2824249863624573
train gradient:  0.09007220112509497
iteration : 839
train acc:  0.875
train loss:  0.3287495970726013
train gradient:  0.18841708466147444
iteration : 840
train acc:  0.890625
train loss:  0.29177790880203247
train gradient:  0.09769857298280737
iteration : 841
train acc:  0.84375
train loss:  0.3346196413040161
train gradient:  0.13887495211649514
iteration : 842
train acc:  0.875
train loss:  0.2902982831001282
train gradient:  0.1437224863120204
iteration : 843
train acc:  0.875
train loss:  0.30132415890693665
train gradient:  0.09103452389084558
iteration : 844
train acc:  0.8671875
train loss:  0.3208029866218567
train gradient:  0.10969468296071253
iteration : 845
train acc:  0.84375
train loss:  0.44331514835357666
train gradient:  0.19118541633165417
iteration : 846
train acc:  0.8671875
train loss:  0.33103883266448975
train gradient:  0.09524677546954902
iteration : 847
train acc:  0.875
train loss:  0.29144638776779175
train gradient:  0.09643850158128103
iteration : 848
train acc:  0.796875
train loss:  0.4860185384750366
train gradient:  0.21965635380747156
iteration : 849
train acc:  0.859375
train loss:  0.2899908125400543
train gradient:  0.09041193912822848
iteration : 850
train acc:  0.859375
train loss:  0.2928517758846283
train gradient:  0.12800221560029124
iteration : 851
train acc:  0.875
train loss:  0.31284546852111816
train gradient:  0.1304615836543617
iteration : 852
train acc:  0.8828125
train loss:  0.30830708146095276
train gradient:  0.0940063353725862
iteration : 853
train acc:  0.84375
train loss:  0.3336295187473297
train gradient:  0.21028979779184476
iteration : 854
train acc:  0.7890625
train loss:  0.42038553953170776
train gradient:  0.228222831009393
iteration : 855
train acc:  0.84375
train loss:  0.3180769383907318
train gradient:  0.09397621746310676
iteration : 856
train acc:  0.859375
train loss:  0.3633612096309662
train gradient:  0.13091803649370248
iteration : 857
train acc:  0.828125
train loss:  0.37795716524124146
train gradient:  0.18801316221423658
iteration : 858
train acc:  0.8671875
train loss:  0.32823047041893005
train gradient:  0.11045382597310383
iteration : 859
train acc:  0.8671875
train loss:  0.3028353452682495
train gradient:  0.11041043182672045
iteration : 860
train acc:  0.84375
train loss:  0.34089869260787964
train gradient:  0.1257873915064049
iteration : 861
train acc:  0.875
train loss:  0.3041192293167114
train gradient:  0.07886116675091075
iteration : 862
train acc:  0.859375
train loss:  0.33822453022003174
train gradient:  0.15723075714339266
iteration : 863
train acc:  0.84375
train loss:  0.3195570707321167
train gradient:  0.10781437759469767
iteration : 864
train acc:  0.859375
train loss:  0.3030798137187958
train gradient:  0.11449734616571465
iteration : 865
train acc:  0.890625
train loss:  0.2760947644710541
train gradient:  0.12765672238527578
iteration : 866
train acc:  0.8359375
train loss:  0.3192114233970642
train gradient:  0.15231835323652146
iteration : 867
train acc:  0.84375
train loss:  0.3374641239643097
train gradient:  0.1193854450271624
iteration : 868
train acc:  0.8125
train loss:  0.3641616702079773
train gradient:  0.13063206353802712
iteration : 869
train acc:  0.8515625
train loss:  0.3679806590080261
train gradient:  0.17270375032600901
iteration : 870
train acc:  0.84375
train loss:  0.3310863971710205
train gradient:  0.167491959994461
iteration : 871
train acc:  0.8828125
train loss:  0.26889967918395996
train gradient:  0.14606057312661724
iteration : 872
train acc:  0.8984375
train loss:  0.276789128780365
train gradient:  0.06407162950772144
iteration : 873
train acc:  0.921875
train loss:  0.21887047588825226
train gradient:  0.06931837498296362
iteration : 874
train acc:  0.90625
train loss:  0.23031437397003174
train gradient:  0.06643120306078282
iteration : 875
train acc:  0.890625
train loss:  0.26170164346694946
train gradient:  0.12674258246930611
iteration : 876
train acc:  0.84375
train loss:  0.34997326135635376
train gradient:  0.17623166207664162
iteration : 877
train acc:  0.8515625
train loss:  0.3257829248905182
train gradient:  0.11026469064732292
iteration : 878
train acc:  0.8515625
train loss:  0.3230288028717041
train gradient:  0.12792406158710024
iteration : 879
train acc:  0.8359375
train loss:  0.32194989919662476
train gradient:  0.11611218477122486
iteration : 880
train acc:  0.828125
train loss:  0.4664885401725769
train gradient:  0.18287242982736604
iteration : 881
train acc:  0.859375
train loss:  0.34454774856567383
train gradient:  0.15038457656295418
iteration : 882
train acc:  0.8828125
train loss:  0.2618746757507324
train gradient:  0.09352588974299997
iteration : 883
train acc:  0.8515625
train loss:  0.32467228174209595
train gradient:  0.2244232381600491
iteration : 884
train acc:  0.8515625
train loss:  0.338943749666214
train gradient:  0.18837140347778375
iteration : 885
train acc:  0.9453125
train loss:  0.21124669909477234
train gradient:  0.051147893581847625
iteration : 886
train acc:  0.9296875
train loss:  0.2769756615161896
train gradient:  0.10647371532053694
iteration : 887
train acc:  0.828125
train loss:  0.3462999165058136
train gradient:  0.14030304894088463
iteration : 888
train acc:  0.8828125
train loss:  0.3296152353286743
train gradient:  0.1285888860718043
iteration : 889
train acc:  0.8671875
train loss:  0.3019029498100281
train gradient:  0.1765798288000217
iteration : 890
train acc:  0.8359375
train loss:  0.3603423237800598
train gradient:  0.16240918827816675
iteration : 891
train acc:  0.8984375
train loss:  0.2617918848991394
train gradient:  0.12485123042491092
iteration : 892
train acc:  0.875
train loss:  0.30595633387565613
train gradient:  0.13827290660888344
iteration : 893
train acc:  0.84375
train loss:  0.34713369607925415
train gradient:  0.12705044425626777
iteration : 894
train acc:  0.875
train loss:  0.28456243872642517
train gradient:  0.07181504832617991
iteration : 895
train acc:  0.8515625
train loss:  0.309614360332489
train gradient:  0.1693443862527258
iteration : 896
train acc:  0.8828125
train loss:  0.28257545828819275
train gradient:  0.09936508917077229
iteration : 897
train acc:  0.84375
train loss:  0.3894053101539612
train gradient:  0.13407737127593405
iteration : 898
train acc:  0.8984375
train loss:  0.3183298110961914
train gradient:  0.16629210547805856
iteration : 899
train acc:  0.8828125
train loss:  0.35419511795043945
train gradient:  0.12419023501262785
iteration : 900
train acc:  0.8984375
train loss:  0.3428259491920471
train gradient:  0.1488913142684058
iteration : 901
train acc:  0.9140625
train loss:  0.24971552193164825
train gradient:  0.09934790374771228
iteration : 902
train acc:  0.8515625
train loss:  0.34345775842666626
train gradient:  0.0986606763609877
iteration : 903
train acc:  0.875
train loss:  0.3054691553115845
train gradient:  0.21339897803705316
iteration : 904
train acc:  0.8828125
train loss:  0.2897135019302368
train gradient:  0.08953746018907811
iteration : 905
train acc:  0.90625
train loss:  0.2654695212841034
train gradient:  0.15768215875208122
iteration : 906
train acc:  0.859375
train loss:  0.3389527499675751
train gradient:  0.10082185857554903
iteration : 907
train acc:  0.921875
train loss:  0.23092137277126312
train gradient:  0.0846306532694798
iteration : 908
train acc:  0.8515625
train loss:  0.32289478182792664
train gradient:  0.1259882748213474
iteration : 909
train acc:  0.8828125
train loss:  0.374610960483551
train gradient:  0.11728773403073081
iteration : 910
train acc:  0.875
train loss:  0.29508763551712036
train gradient:  0.10212994644950378
iteration : 911
train acc:  0.8515625
train loss:  0.309084415435791
train gradient:  0.10705507864492482
iteration : 912
train acc:  0.8671875
train loss:  0.2785673141479492
train gradient:  0.10216345300961077
iteration : 913
train acc:  0.8359375
train loss:  0.3752897083759308
train gradient:  0.4831792602644212
iteration : 914
train acc:  0.84375
train loss:  0.34157994389533997
train gradient:  0.12211673340709718
iteration : 915
train acc:  0.8671875
train loss:  0.2736102342605591
train gradient:  0.1015149044028452
iteration : 916
train acc:  0.859375
train loss:  0.32127657532691956
train gradient:  0.11936587355450025
iteration : 917
train acc:  0.796875
train loss:  0.4497080147266388
train gradient:  0.19824172662480777
iteration : 918
train acc:  0.8671875
train loss:  0.29041773080825806
train gradient:  0.10671676222472924
iteration : 919
train acc:  0.859375
train loss:  0.35477757453918457
train gradient:  0.13715043169721397
iteration : 920
train acc:  0.8828125
train loss:  0.29113835096359253
train gradient:  0.10368875423365691
iteration : 921
train acc:  0.8828125
train loss:  0.3014935255050659
train gradient:  0.11130976179732457
iteration : 922
train acc:  0.8671875
train loss:  0.3101758360862732
train gradient:  0.11666635893490258
iteration : 923
train acc:  0.8984375
train loss:  0.27074503898620605
train gradient:  0.08750499645384673
iteration : 924
train acc:  0.9140625
train loss:  0.22036229074001312
train gradient:  0.05802186941204106
iteration : 925
train acc:  0.828125
train loss:  0.30923137068748474
train gradient:  0.10279059457381637
iteration : 926
train acc:  0.828125
train loss:  0.35105907917022705
train gradient:  0.11839983143419157
iteration : 927
train acc:  0.84375
train loss:  0.32258298993110657
train gradient:  0.10505933639541908
iteration : 928
train acc:  0.9375
train loss:  0.1992153525352478
train gradient:  0.0713882087257134
iteration : 929
train acc:  0.8515625
train loss:  0.3557261824607849
train gradient:  0.12960496087755635
iteration : 930
train acc:  0.890625
train loss:  0.271189421415329
train gradient:  0.11538844522167055
iteration : 931
train acc:  0.8828125
train loss:  0.2764284312725067
train gradient:  0.11609039162070153
iteration : 932
train acc:  0.921875
train loss:  0.24270983040332794
train gradient:  0.08975447793771135
iteration : 933
train acc:  0.8671875
train loss:  0.28883254528045654
train gradient:  0.09234846388788275
iteration : 934
train acc:  0.8828125
train loss:  0.28036075830459595
train gradient:  0.07640095956349499
iteration : 935
train acc:  0.8671875
train loss:  0.2650296986103058
train gradient:  0.08888592897595872
iteration : 936
train acc:  0.9296875
train loss:  0.18878328800201416
train gradient:  0.060466618072758285
iteration : 937
train acc:  0.8984375
train loss:  0.2514674663543701
train gradient:  0.09634047393256104
iteration : 938
train acc:  0.8359375
train loss:  0.31107020378112793
train gradient:  0.13713660390773608
iteration : 939
train acc:  0.8671875
train loss:  0.39596807956695557
train gradient:  0.15726122378663593
iteration : 940
train acc:  0.8359375
train loss:  0.32228419184684753
train gradient:  0.15306578830723036
iteration : 941
train acc:  0.8515625
train loss:  0.29559487104415894
train gradient:  0.12114655588973725
iteration : 942
train acc:  0.890625
train loss:  0.2906910181045532
train gradient:  0.0697013302449963
iteration : 943
train acc:  0.8984375
train loss:  0.27181583642959595
train gradient:  0.16525192712224007
iteration : 944
train acc:  0.875
train loss:  0.30249929428100586
train gradient:  0.1184608081159764
iteration : 945
train acc:  0.890625
train loss:  0.3116379380226135
train gradient:  0.11499905022616681
iteration : 946
train acc:  0.890625
train loss:  0.28077036142349243
train gradient:  0.13196599230309974
iteration : 947
train acc:  0.84375
train loss:  0.292774498462677
train gradient:  0.1273912809577905
iteration : 948
train acc:  0.875
train loss:  0.2521919012069702
train gradient:  0.0860227801739804
iteration : 949
train acc:  0.90625
train loss:  0.3088465929031372
train gradient:  0.09622018209429892
iteration : 950
train acc:  0.8671875
train loss:  0.3097096383571625
train gradient:  0.15199121874764493
iteration : 951
train acc:  0.8203125
train loss:  0.4120051860809326
train gradient:  0.19844155370780406
iteration : 952
train acc:  0.8671875
train loss:  0.3260580897331238
train gradient:  0.14558498397947695
iteration : 953
train acc:  0.859375
train loss:  0.3020870089530945
train gradient:  0.10611365318166922
iteration : 954
train acc:  0.8359375
train loss:  0.24352923035621643
train gradient:  0.08901130728019255
iteration : 955
train acc:  0.8359375
train loss:  0.37867245078086853
train gradient:  0.18417501787700483
iteration : 956
train acc:  0.8671875
train loss:  0.3431183099746704
train gradient:  0.12603144055781265
iteration : 957
train acc:  0.8984375
train loss:  0.2746320962905884
train gradient:  0.14602746063328553
iteration : 958
train acc:  0.84375
train loss:  0.3025549054145813
train gradient:  0.13551745479817726
iteration : 959
train acc:  0.8828125
train loss:  0.3093853294849396
train gradient:  0.0967786451923517
iteration : 960
train acc:  0.90625
train loss:  0.22225919365882874
train gradient:  0.100329230229888
iteration : 961
train acc:  0.890625
train loss:  0.31623589992523193
train gradient:  0.0820420449857284
iteration : 962
train acc:  0.8359375
train loss:  0.37814074754714966
train gradient:  0.1693159461878851
iteration : 963
train acc:  0.8125
train loss:  0.33637863397598267
train gradient:  0.16252064085461118
iteration : 964
train acc:  0.8515625
train loss:  0.3531448543071747
train gradient:  0.309359834706725
iteration : 965
train acc:  0.921875
train loss:  0.23249566555023193
train gradient:  0.08658087852968262
iteration : 966
train acc:  0.84375
train loss:  0.3302232027053833
train gradient:  0.10244642527552267
iteration : 967
train acc:  0.796875
train loss:  0.3630732595920563
train gradient:  0.1596982875121815
iteration : 968
train acc:  0.8515625
train loss:  0.31059759855270386
train gradient:  0.11483775275687144
iteration : 969
train acc:  0.875
train loss:  0.26491087675094604
train gradient:  0.07473069199103659
iteration : 970
train acc:  0.859375
train loss:  0.30092981457710266
train gradient:  0.11490909356734919
iteration : 971
train acc:  0.859375
train loss:  0.28988781571388245
train gradient:  0.125823577439875
iteration : 972
train acc:  0.8515625
train loss:  0.29995763301849365
train gradient:  0.13290571015789904
iteration : 973
train acc:  0.8203125
train loss:  0.35630497336387634
train gradient:  0.24008457053199184
iteration : 974
train acc:  0.859375
train loss:  0.32482674717903137
train gradient:  0.23184392627763284
iteration : 975
train acc:  0.8203125
train loss:  0.33375197649002075
train gradient:  0.12241168097314672
iteration : 976
train acc:  0.890625
train loss:  0.28161564469337463
train gradient:  0.0920541731002029
iteration : 977
train acc:  0.84375
train loss:  0.3549557328224182
train gradient:  0.13364777804219719
iteration : 978
train acc:  0.859375
train loss:  0.3124217391014099
train gradient:  0.11403289289112453
iteration : 979
train acc:  0.8515625
train loss:  0.3089180290699005
train gradient:  0.15969027194932467
iteration : 980
train acc:  0.875
train loss:  0.30983278155326843
train gradient:  0.13920843544172512
iteration : 981
train acc:  0.8515625
train loss:  0.33645910024642944
train gradient:  0.14678673734639366
iteration : 982
train acc:  0.859375
train loss:  0.32339316606521606
train gradient:  0.11652249099427932
iteration : 983
train acc:  0.8984375
train loss:  0.3058243989944458
train gradient:  0.09770719106943673
iteration : 984
train acc:  0.859375
train loss:  0.2998630404472351
train gradient:  0.09881715881777446
iteration : 985
train acc:  0.859375
train loss:  0.29695749282836914
train gradient:  0.12579809072909354
iteration : 986
train acc:  0.8984375
train loss:  0.2601795494556427
train gradient:  0.08737015514572802
iteration : 987
train acc:  0.8984375
train loss:  0.2863980233669281
train gradient:  0.1261207159498272
iteration : 988
train acc:  0.875
train loss:  0.29381293058395386
train gradient:  0.11263208282403726
iteration : 989
train acc:  0.8984375
train loss:  0.28374844789505005
train gradient:  0.1146911434859847
iteration : 990
train acc:  0.8125
train loss:  0.39977240562438965
train gradient:  0.16758636989230513
iteration : 991
train acc:  0.859375
train loss:  0.40275537967681885
train gradient:  0.18265987233703584
iteration : 992
train acc:  0.828125
train loss:  0.3791385591030121
train gradient:  0.14540178820433453
iteration : 993
train acc:  0.8203125
train loss:  0.31896767020225525
train gradient:  0.18355587270614815
iteration : 994
train acc:  0.8515625
train loss:  0.3306583762168884
train gradient:  0.17459625325037864
iteration : 995
train acc:  0.796875
train loss:  0.34258192777633667
train gradient:  0.1756064937487294
iteration : 996
train acc:  0.90625
train loss:  0.23971588909626007
train gradient:  0.06101268075036384
iteration : 997
train acc:  0.8984375
train loss:  0.2527392506599426
train gradient:  0.08071853958728009
iteration : 998
train acc:  0.8203125
train loss:  0.3430333733558655
train gradient:  0.15649803875570906
iteration : 999
train acc:  0.828125
train loss:  0.3849005699157715
train gradient:  0.19906852200513486
iteration : 1000
train acc:  0.875
train loss:  0.27945756912231445
train gradient:  0.12164544013143382
iteration : 1001
train acc:  0.8984375
train loss:  0.26436087489128113
train gradient:  0.08816205749165304
iteration : 1002
train acc:  0.8671875
train loss:  0.2785317301750183
train gradient:  0.14484012023426157
iteration : 1003
train acc:  0.890625
train loss:  0.3297421932220459
train gradient:  0.14824547976607627
iteration : 1004
train acc:  0.8671875
train loss:  0.26140958070755005
train gradient:  0.09613688081606135
iteration : 1005
train acc:  0.890625
train loss:  0.3159908056259155
train gradient:  0.24555156638837905
iteration : 1006
train acc:  0.8515625
train loss:  0.28630509972572327
train gradient:  0.2575026503308658
iteration : 1007
train acc:  0.8671875
train loss:  0.30383169651031494
train gradient:  0.0958613249831074
iteration : 1008
train acc:  0.8671875
train loss:  0.2880288362503052
train gradient:  0.08599001929172016
iteration : 1009
train acc:  0.8359375
train loss:  0.3230670988559723
train gradient:  0.10718484099309614
iteration : 1010
train acc:  0.890625
train loss:  0.27069514989852905
train gradient:  0.087191981426209
iteration : 1011
train acc:  0.890625
train loss:  0.2944158613681793
train gradient:  0.11803899029976786
iteration : 1012
train acc:  0.84375
train loss:  0.3364643454551697
train gradient:  0.12081665365581747
iteration : 1013
train acc:  0.9140625
train loss:  0.24236175417900085
train gradient:  0.08288717583615558
iteration : 1014
train acc:  0.875
train loss:  0.2686815559864044
train gradient:  0.08866477243198405
iteration : 1015
train acc:  0.8828125
train loss:  0.306111216545105
train gradient:  0.16502563335536025
iteration : 1016
train acc:  0.90625
train loss:  0.24720248579978943
train gradient:  0.10966481626929024
iteration : 1017
train acc:  0.875
train loss:  0.3180106282234192
train gradient:  0.10932620226904081
iteration : 1018
train acc:  0.8671875
train loss:  0.29523491859436035
train gradient:  0.1062139158591773
iteration : 1019
train acc:  0.8671875
train loss:  0.28247594833374023
train gradient:  0.14216941573332265
iteration : 1020
train acc:  0.8515625
train loss:  0.35447582602500916
train gradient:  0.17581286916971345
iteration : 1021
train acc:  0.875
train loss:  0.2963176965713501
train gradient:  0.18776262309743671
iteration : 1022
train acc:  0.8828125
train loss:  0.2562553882598877
train gradient:  0.12438285883037943
iteration : 1023
train acc:  0.8984375
train loss:  0.2668594717979431
train gradient:  0.11013493586123699
iteration : 1024
train acc:  0.84375
train loss:  0.3470909297466278
train gradient:  0.13067788067991992
iteration : 1025
train acc:  0.8203125
train loss:  0.38378190994262695
train gradient:  0.18181391456988427
iteration : 1026
train acc:  0.8828125
train loss:  0.2716381549835205
train gradient:  0.12591191332443402
iteration : 1027
train acc:  0.921875
train loss:  0.22061094641685486
train gradient:  0.08382325817368863
iteration : 1028
train acc:  0.859375
train loss:  0.3771069049835205
train gradient:  0.14817656039543792
iteration : 1029
train acc:  0.921875
train loss:  0.2316068559885025
train gradient:  0.08764171771507066
iteration : 1030
train acc:  0.8515625
train loss:  0.34146130084991455
train gradient:  0.21798763703106389
iteration : 1031
train acc:  0.8515625
train loss:  0.3487786054611206
train gradient:  0.1429495713790099
iteration : 1032
train acc:  0.8203125
train loss:  0.38522419333457947
train gradient:  0.19128248099619888
iteration : 1033
train acc:  0.875
train loss:  0.35513925552368164
train gradient:  0.1330266232788796
iteration : 1034
train acc:  0.875
train loss:  0.34682393074035645
train gradient:  0.1279200458772486
iteration : 1035
train acc:  0.8671875
train loss:  0.3352866470813751
train gradient:  0.15603359233246838
iteration : 1036
train acc:  0.8828125
train loss:  0.2871275544166565
train gradient:  0.12195588361358758
iteration : 1037
train acc:  0.9140625
train loss:  0.26716533303260803
train gradient:  0.10365079604396739
iteration : 1038
train acc:  0.8671875
train loss:  0.36539432406425476
train gradient:  0.1355213078563615
iteration : 1039
train acc:  0.859375
train loss:  0.34912967681884766
train gradient:  0.19117005180187574
iteration : 1040
train acc:  0.8984375
train loss:  0.24115760624408722
train gradient:  0.08846295153025889
iteration : 1041
train acc:  0.8359375
train loss:  0.31303611397743225
train gradient:  0.11449200287049226
iteration : 1042
train acc:  0.875
train loss:  0.28160560131073
train gradient:  0.12159382154863102
iteration : 1043
train acc:  0.8984375
train loss:  0.2934691607952118
train gradient:  0.12410728041578171
iteration : 1044
train acc:  0.875
train loss:  0.26543647050857544
train gradient:  0.11583512962719927
iteration : 1045
train acc:  0.8359375
train loss:  0.4141613245010376
train gradient:  0.20115945080807862
iteration : 1046
train acc:  0.84375
train loss:  0.3501180410385132
train gradient:  0.17052861105966743
iteration : 1047
train acc:  0.8671875
train loss:  0.34334272146224976
train gradient:  0.1452129889529769
iteration : 1048
train acc:  0.875
train loss:  0.29289305210113525
train gradient:  0.0692829556681813
iteration : 1049
train acc:  0.90625
train loss:  0.2617981433868408
train gradient:  0.10004076134793417
iteration : 1050
train acc:  0.8671875
train loss:  0.2862597405910492
train gradient:  0.14831796707526979
iteration : 1051
train acc:  0.9375
train loss:  0.21785569190979004
train gradient:  0.058277735531369376
iteration : 1052
train acc:  0.8671875
train loss:  0.2638266086578369
train gradient:  0.11360142851451373
iteration : 1053
train acc:  0.859375
train loss:  0.34197479486465454
train gradient:  0.1147523877122326
iteration : 1054
train acc:  0.875
train loss:  0.2942402958869934
train gradient:  0.09478690487719546
iteration : 1055
train acc:  0.890625
train loss:  0.24792659282684326
train gradient:  0.0792954447717708
iteration : 1056
train acc:  0.7734375
train loss:  0.39256566762924194
train gradient:  0.19137360297558886
iteration : 1057
train acc:  0.828125
train loss:  0.3458000123500824
train gradient:  0.18472642399816863
iteration : 1058
train acc:  0.8671875
train loss:  0.2753784656524658
train gradient:  0.09917117379395245
iteration : 1059
train acc:  0.8515625
train loss:  0.309263676404953
train gradient:  0.18322430708892382
iteration : 1060
train acc:  0.890625
train loss:  0.26600006222724915
train gradient:  0.08097814775354045
iteration : 1061
train acc:  0.8359375
train loss:  0.33398663997650146
train gradient:  0.1895436891534089
iteration : 1062
train acc:  0.90625
train loss:  0.23308008909225464
train gradient:  0.10426012123983741
iteration : 1063
train acc:  0.90625
train loss:  0.25001060962677
train gradient:  0.07247232599001596
iteration : 1064
train acc:  0.8515625
train loss:  0.32216304540634155
train gradient:  0.3172190528901213
iteration : 1065
train acc:  0.8515625
train loss:  0.3339923024177551
train gradient:  0.1568950346826859
iteration : 1066
train acc:  0.9296875
train loss:  0.226543590426445
train gradient:  0.0775568565683649
iteration : 1067
train acc:  0.8671875
train loss:  0.29625874757766724
train gradient:  0.14557075359367141
iteration : 1068
train acc:  0.8203125
train loss:  0.33279693126678467
train gradient:  0.12873080484663657
iteration : 1069
train acc:  0.8515625
train loss:  0.35152682662010193
train gradient:  0.1697755483487654
iteration : 1070
train acc:  0.8515625
train loss:  0.3128645420074463
train gradient:  0.12524861034529078
iteration : 1071
train acc:  0.8828125
train loss:  0.3209238350391388
train gradient:  0.12598003975806799
iteration : 1072
train acc:  0.796875
train loss:  0.4805437922477722
train gradient:  0.2707904850447135
iteration : 1073
train acc:  0.859375
train loss:  0.3575970530509949
train gradient:  0.18858210857368296
iteration : 1074
train acc:  0.84375
train loss:  0.3367648124694824
train gradient:  0.19662990562536237
iteration : 1075
train acc:  0.8359375
train loss:  0.4183180332183838
train gradient:  0.2065450638522976
iteration : 1076
train acc:  0.8515625
train loss:  0.294843852519989
train gradient:  0.10867865170526973
iteration : 1077
train acc:  0.875
train loss:  0.30905529856681824
train gradient:  0.11504214003594845
iteration : 1078
train acc:  0.8515625
train loss:  0.30055829882621765
train gradient:  0.1152010049743568
iteration : 1079
train acc:  0.8515625
train loss:  0.3070131540298462
train gradient:  0.13698728698193535
iteration : 1080
train acc:  0.890625
train loss:  0.2653566300868988
train gradient:  0.11523431464844174
iteration : 1081
train acc:  0.890625
train loss:  0.27464357018470764
train gradient:  0.16466396416381307
iteration : 1082
train acc:  0.8828125
train loss:  0.29329031705856323
train gradient:  0.08200647721688967
iteration : 1083
train acc:  0.84375
train loss:  0.3224928379058838
train gradient:  0.14515275765583593
iteration : 1084
train acc:  0.8671875
train loss:  0.2700001001358032
train gradient:  0.07908787463923499
iteration : 1085
train acc:  0.890625
train loss:  0.3172335624694824
train gradient:  0.12294389775108089
iteration : 1086
train acc:  0.859375
train loss:  0.38485926389694214
train gradient:  0.16375018442474226
iteration : 1087
train acc:  0.8671875
train loss:  0.26020699739456177
train gradient:  0.06853910933812508
iteration : 1088
train acc:  0.8671875
train loss:  0.37037909030914307
train gradient:  0.12204669103190836
iteration : 1089
train acc:  0.84375
train loss:  0.38856178522109985
train gradient:  0.16449068786320142
iteration : 1090
train acc:  0.9140625
train loss:  0.22483006119728088
train gradient:  0.06584759606344952
iteration : 1091
train acc:  0.859375
train loss:  0.26795506477355957
train gradient:  0.11242063998201261
iteration : 1092
train acc:  0.8515625
train loss:  0.2662433981895447
train gradient:  0.12067333213190641
iteration : 1093
train acc:  0.84375
train loss:  0.3172042965888977
train gradient:  0.12704939690277045
iteration : 1094
train acc:  0.828125
train loss:  0.40553098917007446
train gradient:  0.2083582874251103
iteration : 1095
train acc:  0.8671875
train loss:  0.27109527587890625
train gradient:  0.11649394221503956
iteration : 1096
train acc:  0.8828125
train loss:  0.31049251556396484
train gradient:  0.14309075158674325
iteration : 1097
train acc:  0.8671875
train loss:  0.29546797275543213
train gradient:  0.09596594576758326
iteration : 1098
train acc:  0.8515625
train loss:  0.344488263130188
train gradient:  0.13724483636309667
iteration : 1099
train acc:  0.8515625
train loss:  0.28577619791030884
train gradient:  0.12337804924117833
iteration : 1100
train acc:  0.8671875
train loss:  0.36272159218788147
train gradient:  0.15494348433106894
iteration : 1101
train acc:  0.859375
train loss:  0.28563135862350464
train gradient:  0.09266377055989701
iteration : 1102
train acc:  0.8828125
train loss:  0.2978663742542267
train gradient:  0.1275354215875143
iteration : 1103
train acc:  0.8984375
train loss:  0.28411245346069336
train gradient:  0.07926599865065523
iteration : 1104
train acc:  0.890625
train loss:  0.2277027815580368
train gradient:  0.07933308828905031
iteration : 1105
train acc:  0.8828125
train loss:  0.2913782596588135
train gradient:  0.12988751757992248
iteration : 1106
train acc:  0.875
train loss:  0.24940752983093262
train gradient:  0.09480959514717431
iteration : 1107
train acc:  0.9140625
train loss:  0.24150684475898743
train gradient:  0.06678897668064755
iteration : 1108
train acc:  0.8828125
train loss:  0.34571027755737305
train gradient:  0.13499387151972558
iteration : 1109
train acc:  0.875
train loss:  0.3159925937652588
train gradient:  0.1361697171015271
iteration : 1110
train acc:  0.8828125
train loss:  0.27334171533584595
train gradient:  0.08562902319174595
iteration : 1111
train acc:  0.921875
train loss:  0.29521289467811584
train gradient:  0.13779923802965796
iteration : 1112
train acc:  0.8671875
train loss:  0.2758844792842865
train gradient:  0.08797096515875785
iteration : 1113
train acc:  0.8828125
train loss:  0.3048803508281708
train gradient:  0.08854077026290529
iteration : 1114
train acc:  0.859375
train loss:  0.3238679766654968
train gradient:  0.1329603362955677
iteration : 1115
train acc:  0.875
train loss:  0.28951725363731384
train gradient:  0.0887876769644103
iteration : 1116
train acc:  0.7578125
train loss:  0.432407021522522
train gradient:  0.3315056232137517
iteration : 1117
train acc:  0.875
train loss:  0.2800115644931793
train gradient:  0.1203352765196008
iteration : 1118
train acc:  0.890625
train loss:  0.3197201192378998
train gradient:  0.1346109245859745
iteration : 1119
train acc:  0.8671875
train loss:  0.29808539152145386
train gradient:  0.11770303061420385
iteration : 1120
train acc:  0.90625
train loss:  0.29225823283195496
train gradient:  0.07405941252080739
iteration : 1121
train acc:  0.84375
train loss:  0.33367639780044556
train gradient:  0.16750352869909374
iteration : 1122
train acc:  0.8046875
train loss:  0.416448712348938
train gradient:  0.245246851406335
iteration : 1123
train acc:  0.84375
train loss:  0.31864994764328003
train gradient:  0.12155946761879005
iteration : 1124
train acc:  0.859375
train loss:  0.28168147802352905
train gradient:  0.08289349392560068
iteration : 1125
train acc:  0.875
train loss:  0.2891407310962677
train gradient:  0.13018257758528912
iteration : 1126
train acc:  0.8671875
train loss:  0.3055206835269928
train gradient:  0.12633714765161133
iteration : 1127
train acc:  0.875
train loss:  0.2958124577999115
train gradient:  0.07303996731062493
iteration : 1128
train acc:  0.84375
train loss:  0.33675259351730347
train gradient:  0.119126382081184
iteration : 1129
train acc:  0.8515625
train loss:  0.3397688865661621
train gradient:  0.16799114639676627
iteration : 1130
train acc:  0.859375
train loss:  0.3492962121963501
train gradient:  0.13846084986059612
iteration : 1131
train acc:  0.8671875
train loss:  0.2886388301849365
train gradient:  0.17881344212445424
iteration : 1132
train acc:  0.921875
train loss:  0.2573147416114807
train gradient:  0.15236425933833841
iteration : 1133
train acc:  0.8828125
train loss:  0.32032644748687744
train gradient:  0.10620567215314392
iteration : 1134
train acc:  0.9140625
train loss:  0.24141739308834076
train gradient:  0.09321850314938046
iteration : 1135
train acc:  0.8671875
train loss:  0.3934395909309387
train gradient:  0.19256895023664533
iteration : 1136
train acc:  0.9140625
train loss:  0.22900241613388062
train gradient:  0.10867499669602426
iteration : 1137
train acc:  0.828125
train loss:  0.3808119297027588
train gradient:  0.14668918246119997
iteration : 1138
train acc:  0.8203125
train loss:  0.3774462342262268
train gradient:  0.13873738646540607
iteration : 1139
train acc:  0.90625
train loss:  0.23091714084148407
train gradient:  0.07533517362933057
iteration : 1140
train acc:  0.8671875
train loss:  0.26469236612319946
train gradient:  0.08099591368785634
iteration : 1141
train acc:  0.8359375
train loss:  0.45951783657073975
train gradient:  0.21177773758954704
iteration : 1142
train acc:  0.84375
train loss:  0.3077809810638428
train gradient:  0.11399664879358988
iteration : 1143
train acc:  0.8984375
train loss:  0.2617124021053314
train gradient:  0.09600122639867625
iteration : 1144
train acc:  0.8984375
train loss:  0.2809429168701172
train gradient:  0.06478094981956466
iteration : 1145
train acc:  0.859375
train loss:  0.28153908252716064
train gradient:  0.1283441096928793
iteration : 1146
train acc:  0.921875
train loss:  0.25845253467559814
train gradient:  0.10677768085853835
iteration : 1147
train acc:  0.875
train loss:  0.3034886121749878
train gradient:  0.11222320125739432
iteration : 1148
train acc:  0.875
train loss:  0.35054510831832886
train gradient:  0.1319402383134332
iteration : 1149
train acc:  0.8671875
train loss:  0.28561365604400635
train gradient:  0.10442402233774443
iteration : 1150
train acc:  0.828125
train loss:  0.386652410030365
train gradient:  0.18514636786062832
iteration : 1151
train acc:  0.875
train loss:  0.3209642469882965
train gradient:  0.10831246284164227
iteration : 1152
train acc:  0.9453125
train loss:  0.23543347418308258
train gradient:  0.08999605579055397
iteration : 1153
train acc:  0.8671875
train loss:  0.30254340171813965
train gradient:  0.11689255974895972
iteration : 1154
train acc:  0.8828125
train loss:  0.32126444578170776
train gradient:  0.11638077066703675
iteration : 1155
train acc:  0.875
train loss:  0.2701188623905182
train gradient:  0.1503576212529511
iteration : 1156
train acc:  0.890625
train loss:  0.3066156804561615
train gradient:  0.1558257385403773
iteration : 1157
train acc:  0.8984375
train loss:  0.24232669174671173
train gradient:  0.08563756559613965
iteration : 1158
train acc:  0.8359375
train loss:  0.34294503927230835
train gradient:  0.12283371983398707
iteration : 1159
train acc:  0.8671875
train loss:  0.2833416163921356
train gradient:  0.07934682322899102
iteration : 1160
train acc:  0.84375
train loss:  0.40334147214889526
train gradient:  0.18251222614944543
iteration : 1161
train acc:  0.7890625
train loss:  0.4459739327430725
train gradient:  0.1729696718459803
iteration : 1162
train acc:  0.828125
train loss:  0.3405355215072632
train gradient:  0.12627858672782563
iteration : 1163
train acc:  0.890625
train loss:  0.256206750869751
train gradient:  0.09407611873877271
iteration : 1164
train acc:  0.8671875
train loss:  0.3317978084087372
train gradient:  0.1684762042071955
iteration : 1165
train acc:  0.859375
train loss:  0.31787294149398804
train gradient:  0.18764012528965446
iteration : 1166
train acc:  0.8203125
train loss:  0.40783941745758057
train gradient:  0.17700578968023187
iteration : 1167
train acc:  0.8828125
train loss:  0.2662566900253296
train gradient:  0.10799547375211638
iteration : 1168
train acc:  0.8984375
train loss:  0.24425214529037476
train gradient:  0.07762801909689676
iteration : 1169
train acc:  0.890625
train loss:  0.2297816276550293
train gradient:  0.06503716613155355
iteration : 1170
train acc:  0.921875
train loss:  0.21269044280052185
train gradient:  0.05413981177091591
iteration : 1171
train acc:  0.859375
train loss:  0.35886746644973755
train gradient:  0.14685427376905485
iteration : 1172
train acc:  0.8828125
train loss:  0.26845479011535645
train gradient:  0.07047292361283053
iteration : 1173
train acc:  0.828125
train loss:  0.3768370747566223
train gradient:  0.15497768935099396
iteration : 1174
train acc:  0.875
train loss:  0.2829160988330841
train gradient:  0.11265214387850088
iteration : 1175
train acc:  0.8515625
train loss:  0.36323919892311096
train gradient:  0.17972089733700286
iteration : 1176
train acc:  0.8828125
train loss:  0.3469598591327667
train gradient:  0.13931631212898515
iteration : 1177
train acc:  0.8828125
train loss:  0.25877976417541504
train gradient:  0.08963726251161229
iteration : 1178
train acc:  0.90625
train loss:  0.2576182782649994
train gradient:  0.10030541997812341
iteration : 1179
train acc:  0.84375
train loss:  0.31234365701675415
train gradient:  0.10980469706463392
iteration : 1180
train acc:  0.890625
train loss:  0.24466626346111298
train gradient:  0.06362143019002348
iteration : 1181
train acc:  0.890625
train loss:  0.2869611382484436
train gradient:  0.08038109779424513
iteration : 1182
train acc:  0.9140625
train loss:  0.25638628005981445
train gradient:  0.09081303711537705
iteration : 1183
train acc:  0.90625
train loss:  0.25157204270362854
train gradient:  0.08752830991745411
iteration : 1184
train acc:  0.8359375
train loss:  0.30990684032440186
train gradient:  0.1386406307737022
iteration : 1185
train acc:  0.9140625
train loss:  0.23098643124103546
train gradient:  0.09358738388022667
iteration : 1186
train acc:  0.8828125
train loss:  0.2433043271303177
train gradient:  0.1597448651529239
iteration : 1187
train acc:  0.8984375
train loss:  0.24071601033210754
train gradient:  0.07473831448968986
iteration : 1188
train acc:  0.8515625
train loss:  0.37646666169166565
train gradient:  0.23867963888274324
iteration : 1189
train acc:  0.8203125
train loss:  0.31795790791511536
train gradient:  0.1394165281960967
iteration : 1190
train acc:  0.890625
train loss:  0.2845245599746704
train gradient:  0.0967206327635465
iteration : 1191
train acc:  0.8828125
train loss:  0.30586883425712585
train gradient:  0.11429995888648452
iteration : 1192
train acc:  0.8671875
train loss:  0.29857441782951355
train gradient:  0.12167718500353246
iteration : 1193
train acc:  0.875
train loss:  0.3944038152694702
train gradient:  0.1932163249849277
iteration : 1194
train acc:  0.890625
train loss:  0.3154478371143341
train gradient:  0.1142270574094933
iteration : 1195
train acc:  0.8359375
train loss:  0.32914090156555176
train gradient:  0.09474399449073023
iteration : 1196
train acc:  0.875
train loss:  0.25611811876296997
train gradient:  0.0992467225136206
iteration : 1197
train acc:  0.8515625
train loss:  0.33567702770233154
train gradient:  0.12415425791522935
iteration : 1198
train acc:  0.8828125
train loss:  0.30229687690734863
train gradient:  0.11694463371115611
iteration : 1199
train acc:  0.859375
train loss:  0.36846840381622314
train gradient:  0.22961477203582198
iteration : 1200
train acc:  0.859375
train loss:  0.2346225380897522
train gradient:  0.06351465348053784
iteration : 1201
train acc:  0.8359375
train loss:  0.3624629080295563
train gradient:  0.17664827519371276
iteration : 1202
train acc:  0.8671875
train loss:  0.3038104772567749
train gradient:  0.1310104739441711
iteration : 1203
train acc:  0.828125
train loss:  0.3897656798362732
train gradient:  0.156498431597193
iteration : 1204
train acc:  0.828125
train loss:  0.3877277970314026
train gradient:  0.1837642426318012
iteration : 1205
train acc:  0.859375
train loss:  0.3396591544151306
train gradient:  0.1269544348485854
iteration : 1206
train acc:  0.8828125
train loss:  0.2525591552257538
train gradient:  0.12733565811322184
iteration : 1207
train acc:  0.9140625
train loss:  0.222171813249588
train gradient:  0.07888706325122002
iteration : 1208
train acc:  0.8828125
train loss:  0.33384495973587036
train gradient:  0.16725400461425882
iteration : 1209
train acc:  0.890625
train loss:  0.2426362931728363
train gradient:  0.058372924212464046
iteration : 1210
train acc:  0.90625
train loss:  0.2712782919406891
train gradient:  0.06919070669028637
iteration : 1211
train acc:  0.8203125
train loss:  0.3547651767730713
train gradient:  0.17673025298683426
iteration : 1212
train acc:  0.8828125
train loss:  0.3505534529685974
train gradient:  0.16224522455677692
iteration : 1213
train acc:  0.84375
train loss:  0.3437957763671875
train gradient:  0.1262847267379433
iteration : 1214
train acc:  0.859375
train loss:  0.32394734025001526
train gradient:  0.13720091082334784
iteration : 1215
train acc:  0.8671875
train loss:  0.29014310240745544
train gradient:  0.12068579819296926
iteration : 1216
train acc:  0.8671875
train loss:  0.2872838079929352
train gradient:  0.13782070220248888
iteration : 1217
train acc:  0.84375
train loss:  0.33333826065063477
train gradient:  0.12818920921856577
iteration : 1218
train acc:  0.8671875
train loss:  0.3076944947242737
train gradient:  0.09832614385348715
iteration : 1219
train acc:  0.828125
train loss:  0.3521365225315094
train gradient:  0.11959228579867641
iteration : 1220
train acc:  0.828125
train loss:  0.37925946712493896
train gradient:  0.16176046262078325
iteration : 1221
train acc:  0.8515625
train loss:  0.2804349958896637
train gradient:  0.07726475511835974
iteration : 1222
train acc:  0.90625
train loss:  0.2908380925655365
train gradient:  0.08145958173035164
iteration : 1223
train acc:  0.8671875
train loss:  0.31700336933135986
train gradient:  0.1146444662377248
iteration : 1224
train acc:  0.859375
train loss:  0.2806328535079956
train gradient:  0.09509515212093403
iteration : 1225
train acc:  0.875
train loss:  0.34213802218437195
train gradient:  0.13281476019580962
iteration : 1226
train acc:  0.84375
train loss:  0.43276309967041016
train gradient:  0.2619682177902242
iteration : 1227
train acc:  0.8515625
train loss:  0.30936017632484436
train gradient:  0.11022963547746137
iteration : 1228
train acc:  0.796875
train loss:  0.3935949206352234
train gradient:  0.12951037627101206
iteration : 1229
train acc:  0.890625
train loss:  0.22747883200645447
train gradient:  0.08172396714471929
iteration : 1230
train acc:  0.8203125
train loss:  0.40300339460372925
train gradient:  0.19358210323400724
iteration : 1231
train acc:  0.90625
train loss:  0.26561933755874634
train gradient:  0.09078906696922717
iteration : 1232
train acc:  0.875
train loss:  0.35213568806648254
train gradient:  0.14241256629387303
iteration : 1233
train acc:  0.90625
train loss:  0.23174110054969788
train gradient:  0.08803011315754068
iteration : 1234
train acc:  0.828125
train loss:  0.30918723344802856
train gradient:  0.11431218518337909
iteration : 1235
train acc:  0.84375
train loss:  0.3619406223297119
train gradient:  0.1308082462503794
iteration : 1236
train acc:  0.875
train loss:  0.2723848521709442
train gradient:  0.09449181649329215
iteration : 1237
train acc:  0.8046875
train loss:  0.3406863212585449
train gradient:  0.20405010764586523
iteration : 1238
train acc:  0.8359375
train loss:  0.32472479343414307
train gradient:  0.17709430462888792
iteration : 1239
train acc:  0.921875
train loss:  0.23779873549938202
train gradient:  0.12726979617001044
iteration : 1240
train acc:  0.890625
train loss:  0.35000407695770264
train gradient:  0.15235229746416099
iteration : 1241
train acc:  0.8984375
train loss:  0.313843309879303
train gradient:  0.15236600931740268
iteration : 1242
train acc:  0.8828125
train loss:  0.317975789308548
train gradient:  0.10501789308724577
iteration : 1243
train acc:  0.875
train loss:  0.24929095804691315
train gradient:  0.07815506128955417
iteration : 1244
train acc:  0.8515625
train loss:  0.3904944658279419
train gradient:  0.24372169651154896
iteration : 1245
train acc:  0.875
train loss:  0.31139177083969116
train gradient:  0.09396117843599369
iteration : 1246
train acc:  0.875
train loss:  0.23740020394325256
train gradient:  0.08741485381674996
iteration : 1247
train acc:  0.8359375
train loss:  0.39383912086486816
train gradient:  0.17726190309427733
iteration : 1248
train acc:  0.8359375
train loss:  0.3937869668006897
train gradient:  0.20111229161383798
iteration : 1249
train acc:  0.8671875
train loss:  0.26841476559638977
train gradient:  0.10794916031877741
iteration : 1250
train acc:  0.859375
train loss:  0.3462432026863098
train gradient:  0.13111486115210952
iteration : 1251
train acc:  0.859375
train loss:  0.34618431329727173
train gradient:  0.11191025888948722
iteration : 1252
train acc:  0.90625
train loss:  0.2730182409286499
train gradient:  0.11818518366989558
iteration : 1253
train acc:  0.8671875
train loss:  0.3605923652648926
train gradient:  0.12405939838777544
iteration : 1254
train acc:  0.859375
train loss:  0.32049423456192017
train gradient:  0.11687356238061475
iteration : 1255
train acc:  0.8515625
train loss:  0.3353036642074585
train gradient:  0.1613518701619831
iteration : 1256
train acc:  0.8671875
train loss:  0.250796914100647
train gradient:  0.09210101805561316
iteration : 1257
train acc:  0.84375
train loss:  0.3935992121696472
train gradient:  0.15754745905265027
iteration : 1258
train acc:  0.8359375
train loss:  0.3286896347999573
train gradient:  0.11277251268311989
iteration : 1259
train acc:  0.84375
train loss:  0.38690245151519775
train gradient:  0.13731763353431195
iteration : 1260
train acc:  0.8828125
train loss:  0.25421106815338135
train gradient:  0.09451512448095464
iteration : 1261
train acc:  0.890625
train loss:  0.2476029098033905
train gradient:  0.07785484145486014
iteration : 1262
train acc:  0.84375
train loss:  0.3129267394542694
train gradient:  0.16661017904212322
iteration : 1263
train acc:  0.8984375
train loss:  0.22953523695468903
train gradient:  0.06465986438791593
iteration : 1264
train acc:  0.8828125
train loss:  0.31624525785446167
train gradient:  0.08802291094369884
iteration : 1265
train acc:  0.8046875
train loss:  0.44110578298568726
train gradient:  0.315598044101347
iteration : 1266
train acc:  0.84375
train loss:  0.39469650387763977
train gradient:  0.1319966001680903
iteration : 1267
train acc:  0.8984375
train loss:  0.23916783928871155
train gradient:  0.10752614507520328
iteration : 1268
train acc:  0.890625
train loss:  0.2597682476043701
train gradient:  0.08148783157155622
iteration : 1269
train acc:  0.8359375
train loss:  0.38604283332824707
train gradient:  0.20658217022337375
iteration : 1270
train acc:  0.8984375
train loss:  0.2979239523410797
train gradient:  0.09230421052568873
iteration : 1271
train acc:  0.875
train loss:  0.29927492141723633
train gradient:  0.11704494650845786
iteration : 1272
train acc:  0.859375
train loss:  0.289853036403656
train gradient:  0.1938049335311432
iteration : 1273
train acc:  0.8671875
train loss:  0.31280088424682617
train gradient:  0.08110755330552175
iteration : 1274
train acc:  0.8671875
train loss:  0.3473416566848755
train gradient:  0.14330419429776126
iteration : 1275
train acc:  0.8828125
train loss:  0.23749464750289917
train gradient:  0.08866918942116951
iteration : 1276
train acc:  0.9296875
train loss:  0.2236761450767517
train gradient:  0.1012650112826823
iteration : 1277
train acc:  0.859375
train loss:  0.2900368273258209
train gradient:  0.07849086667195748
iteration : 1278
train acc:  0.8828125
train loss:  0.25400346517562866
train gradient:  0.08166229488827381
iteration : 1279
train acc:  0.890625
train loss:  0.2520276606082916
train gradient:  0.09725276254534153
iteration : 1280
train acc:  0.8515625
train loss:  0.32368993759155273
train gradient:  0.1751089799408882
iteration : 1281
train acc:  0.8359375
train loss:  0.40776997804641724
train gradient:  0.16930888259793764
iteration : 1282
train acc:  0.8515625
train loss:  0.40070652961730957
train gradient:  0.1654616811655209
iteration : 1283
train acc:  0.828125
train loss:  0.31612667441368103
train gradient:  0.11259187307016322
iteration : 1284
train acc:  0.8515625
train loss:  0.31377679109573364
train gradient:  0.11082727411598796
iteration : 1285
train acc:  0.8828125
train loss:  0.2853328585624695
train gradient:  0.08720336152246606
iteration : 1286
train acc:  0.9140625
train loss:  0.2558748722076416
train gradient:  0.07373334182405644
iteration : 1287
train acc:  0.859375
train loss:  0.287978857755661
train gradient:  0.07654767533123663
iteration : 1288
train acc:  0.8203125
train loss:  0.3285820186138153
train gradient:  0.12942338768060951
iteration : 1289
train acc:  0.859375
train loss:  0.34023362398147583
train gradient:  0.13347844269303505
iteration : 1290
train acc:  0.8515625
train loss:  0.3293956220149994
train gradient:  0.12833505798388684
iteration : 1291
train acc:  0.8671875
train loss:  0.30463936924934387
train gradient:  0.08910724170978791
iteration : 1292
train acc:  0.859375
train loss:  0.2821929156780243
train gradient:  0.08422813363207751
iteration : 1293
train acc:  0.8203125
train loss:  0.3505151569843292
train gradient:  0.10959807625543037
iteration : 1294
train acc:  0.8515625
train loss:  0.3092194199562073
train gradient:  0.12149179751488372
iteration : 1295
train acc:  0.8203125
train loss:  0.36903125047683716
train gradient:  0.17759557292120698
iteration : 1296
train acc:  0.84375
train loss:  0.3440512716770172
train gradient:  0.11583214459536859
iteration : 1297
train acc:  0.8203125
train loss:  0.4031996428966522
train gradient:  0.21529778004171862
iteration : 1298
train acc:  0.8359375
train loss:  0.3714450001716614
train gradient:  0.16297016446975132
iteration : 1299
train acc:  0.8671875
train loss:  0.31432241201400757
train gradient:  0.10578999571436529
iteration : 1300
train acc:  0.90625
train loss:  0.2424350082874298
train gradient:  0.08410444717416143
iteration : 1301
train acc:  0.9375
train loss:  0.24206788837909698
train gradient:  0.08925684560879815
iteration : 1302
train acc:  0.9140625
train loss:  0.2692827582359314
train gradient:  0.0765701613725435
iteration : 1303
train acc:  0.875
train loss:  0.3059861958026886
train gradient:  0.0814673531720195
iteration : 1304
train acc:  0.828125
train loss:  0.3815563917160034
train gradient:  0.20750367530155012
iteration : 1305
train acc:  0.9296875
train loss:  0.2144695520401001
train gradient:  0.06636695794634097
iteration : 1306
train acc:  0.8828125
train loss:  0.30277150869369507
train gradient:  0.10896005433474668
iteration : 1307
train acc:  0.84375
train loss:  0.3109779357910156
train gradient:  0.09623633335990046
iteration : 1308
train acc:  0.828125
train loss:  0.33782821893692017
train gradient:  0.10051903390994629
iteration : 1309
train acc:  0.859375
train loss:  0.31927400827407837
train gradient:  0.13709781743119304
iteration : 1310
train acc:  0.8671875
train loss:  0.3331955671310425
train gradient:  0.09793695594449467
iteration : 1311
train acc:  0.8359375
train loss:  0.39080387353897095
train gradient:  0.20101327352752485
iteration : 1312
train acc:  0.8515625
train loss:  0.28790727257728577
train gradient:  0.11993178912122106
iteration : 1313
train acc:  0.859375
train loss:  0.3746711015701294
train gradient:  0.17373644566037996
iteration : 1314
train acc:  0.8359375
train loss:  0.351928174495697
train gradient:  0.13141597808193414
iteration : 1315
train acc:  0.90625
train loss:  0.28434884548187256
train gradient:  0.14269760207373064
iteration : 1316
train acc:  0.9296875
train loss:  0.278742253780365
train gradient:  0.0759763385406482
iteration : 1317
train acc:  0.875
train loss:  0.26378965377807617
train gradient:  0.08717726917001328
iteration : 1318
train acc:  0.8515625
train loss:  0.3827939033508301
train gradient:  0.1383752335719745
iteration : 1319
train acc:  0.859375
train loss:  0.33566126227378845
train gradient:  0.092747587894703
iteration : 1320
train acc:  0.8203125
train loss:  0.40733757615089417
train gradient:  0.18305221100187086
iteration : 1321
train acc:  0.9140625
train loss:  0.3309023380279541
train gradient:  0.10182037934689059
iteration : 1322
train acc:  0.8984375
train loss:  0.26711657643318176
train gradient:  0.07629603247857572
iteration : 1323
train acc:  0.8515625
train loss:  0.39888179302215576
train gradient:  0.1717330651517867
iteration : 1324
train acc:  0.8984375
train loss:  0.2741216719150543
train gradient:  0.14196208620672962
iteration : 1325
train acc:  0.8359375
train loss:  0.34539595246315
train gradient:  0.12947268384652785
iteration : 1326
train acc:  0.8828125
train loss:  0.33549338579177856
train gradient:  0.16520920995522131
iteration : 1327
train acc:  0.921875
train loss:  0.2555767297744751
train gradient:  0.09501828648094757
iteration : 1328
train acc:  0.8828125
train loss:  0.34073135256767273
train gradient:  0.14981562555738723
iteration : 1329
train acc:  0.8828125
train loss:  0.3068311810493469
train gradient:  0.12576316845007526
iteration : 1330
train acc:  0.90625
train loss:  0.2404094934463501
train gradient:  0.07715866380592445
iteration : 1331
train acc:  0.8671875
train loss:  0.27411332726478577
train gradient:  0.10465679396133563
iteration : 1332
train acc:  0.8359375
train loss:  0.34241750836372375
train gradient:  0.14597565896832917
iteration : 1333
train acc:  0.8671875
train loss:  0.27337220311164856
train gradient:  0.09580476410545247
iteration : 1334
train acc:  0.921875
train loss:  0.2351665198802948
train gradient:  0.11809328809288927
iteration : 1335
train acc:  0.875
train loss:  0.3624812066555023
train gradient:  0.12960800403695638
iteration : 1336
train acc:  0.8984375
train loss:  0.2369222790002823
train gradient:  0.06059251867150252
iteration : 1337
train acc:  0.890625
train loss:  0.28869929909706116
train gradient:  0.1144707892879641
iteration : 1338
train acc:  0.8515625
train loss:  0.3293322026729584
train gradient:  0.13424213978142313
iteration : 1339
train acc:  0.8984375
train loss:  0.22250524163246155
train gradient:  0.08182915245403884
iteration : 1340
train acc:  0.8671875
train loss:  0.3297095000743866
train gradient:  0.09212348058784638
iteration : 1341
train acc:  0.875
train loss:  0.2924545109272003
train gradient:  0.13155086995784987
iteration : 1342
train acc:  0.859375
train loss:  0.3326433300971985
train gradient:  0.10557299748970125
iteration : 1343
train acc:  0.8828125
train loss:  0.33454790711402893
train gradient:  0.12327349356896254
iteration : 1344
train acc:  0.8984375
train loss:  0.24711845815181732
train gradient:  0.1101251642116957
iteration : 1345
train acc:  0.84375
train loss:  0.36363452672958374
train gradient:  0.17594826900160615
iteration : 1346
train acc:  0.859375
train loss:  0.28270307183265686
train gradient:  0.14561692690303674
iteration : 1347
train acc:  0.8828125
train loss:  0.23533904552459717
train gradient:  0.0738776703966164
iteration : 1348
train acc:  0.8828125
train loss:  0.3249368667602539
train gradient:  0.14484300422546803
iteration : 1349
train acc:  0.8984375
train loss:  0.35570216178894043
train gradient:  0.2836748050786722
iteration : 1350
train acc:  0.8359375
train loss:  0.3711050748825073
train gradient:  0.14956144084833922
iteration : 1351
train acc:  0.9140625
train loss:  0.2688896358013153
train gradient:  0.11012537228414915
iteration : 1352
train acc:  0.828125
train loss:  0.32525935769081116
train gradient:  0.15857210195351246
iteration : 1353
train acc:  0.78125
train loss:  0.38666173815727234
train gradient:  0.15147805012576987
iteration : 1354
train acc:  0.9140625
train loss:  0.24695990979671478
train gradient:  0.15638777092801628
iteration : 1355
train acc:  0.8515625
train loss:  0.27717113494873047
train gradient:  0.1553088429954029
iteration : 1356
train acc:  0.84375
train loss:  0.2566835880279541
train gradient:  0.10774069929241434
iteration : 1357
train acc:  0.8671875
train loss:  0.31475627422332764
train gradient:  0.08872467073028065
iteration : 1358
train acc:  0.8359375
train loss:  0.3382687568664551
train gradient:  0.14222303982541198
iteration : 1359
train acc:  0.828125
train loss:  0.3704356253147125
train gradient:  0.14846523546069912
iteration : 1360
train acc:  0.8359375
train loss:  0.3173155188560486
train gradient:  0.10498130919238552
iteration : 1361
train acc:  0.8671875
train loss:  0.2978118658065796
train gradient:  0.09996067570519565
iteration : 1362
train acc:  0.8984375
train loss:  0.27902650833129883
train gradient:  0.07912825506445104
iteration : 1363
train acc:  0.828125
train loss:  0.3327183127403259
train gradient:  0.1728239986571876
iteration : 1364
train acc:  0.875
train loss:  0.3116775155067444
train gradient:  0.14739103597539938
iteration : 1365
train acc:  0.859375
train loss:  0.31215012073516846
train gradient:  0.12143412478010619
iteration : 1366
train acc:  0.8515625
train loss:  0.3600993752479553
train gradient:  0.18872181148623768
iteration : 1367
train acc:  0.84375
train loss:  0.31163913011550903
train gradient:  0.10855598882574945
iteration : 1368
train acc:  0.859375
train loss:  0.3483167290687561
train gradient:  0.21231707740322053
iteration : 1369
train acc:  0.828125
train loss:  0.34478330612182617
train gradient:  0.12201257539656053
iteration : 1370
train acc:  0.8359375
train loss:  0.35769280791282654
train gradient:  0.1543470642182159
iteration : 1371
train acc:  0.8828125
train loss:  0.2592719793319702
train gradient:  0.12775063962898203
iteration : 1372
train acc:  0.859375
train loss:  0.3381809592247009
train gradient:  0.14901758054367364
iteration : 1373
train acc:  0.8125
train loss:  0.41956281661987305
train gradient:  0.21538019461019858
iteration : 1374
train acc:  0.8984375
train loss:  0.2796303629875183
train gradient:  0.12310123883393141
iteration : 1375
train acc:  0.8203125
train loss:  0.3481690287590027
train gradient:  0.15661087819759745
iteration : 1376
train acc:  0.8984375
train loss:  0.2932184040546417
train gradient:  0.1258181015232998
iteration : 1377
train acc:  0.859375
train loss:  0.34055474400520325
train gradient:  0.15189511639626072
iteration : 1378
train acc:  0.8828125
train loss:  0.27026617527008057
train gradient:  0.11471145516327626
iteration : 1379
train acc:  0.8671875
train loss:  0.33928045630455017
train gradient:  0.10754913689822992
iteration : 1380
train acc:  0.8671875
train loss:  0.2577185332775116
train gradient:  0.06395331818943945
iteration : 1381
train acc:  0.9296875
train loss:  0.23348788917064667
train gradient:  0.06877800417554698
iteration : 1382
train acc:  0.875
train loss:  0.309552401304245
train gradient:  0.11386822629721742
iteration : 1383
train acc:  0.7890625
train loss:  0.42069822549819946
train gradient:  0.17533936845030057
iteration : 1384
train acc:  0.859375
train loss:  0.31849196553230286
train gradient:  0.16688397673491462
iteration : 1385
train acc:  0.859375
train loss:  0.31027454137802124
train gradient:  0.11288253101007857
iteration : 1386
train acc:  0.8828125
train loss:  0.29372236132621765
train gradient:  0.08674863226760945
iteration : 1387
train acc:  0.8359375
train loss:  0.3099110722541809
train gradient:  0.12789295009658963
iteration : 1388
train acc:  0.8984375
train loss:  0.26669296622276306
train gradient:  0.11263451114837547
iteration : 1389
train acc:  0.8671875
train loss:  0.25986385345458984
train gradient:  0.09595618129408261
iteration : 1390
train acc:  0.8671875
train loss:  0.2568408250808716
train gradient:  0.09391334157824839
iteration : 1391
train acc:  0.90625
train loss:  0.2473260760307312
train gradient:  0.09737440616136869
iteration : 1392
train acc:  0.8125
train loss:  0.37736302614212036
train gradient:  0.1594899707392547
iteration : 1393
train acc:  0.8828125
train loss:  0.33744537830352783
train gradient:  0.1177023811635275
iteration : 1394
train acc:  0.875
train loss:  0.29818591475486755
train gradient:  0.1008489954318036
iteration : 1395
train acc:  0.859375
train loss:  0.29820162057876587
train gradient:  0.17198082310499668
iteration : 1396
train acc:  0.8125
train loss:  0.3493672013282776
train gradient:  0.17284250412195787
iteration : 1397
train acc:  0.875
train loss:  0.2558593153953552
train gradient:  0.09050966718138022
iteration : 1398
train acc:  0.8046875
train loss:  0.4490097165107727
train gradient:  0.21583144728407166
iteration : 1399
train acc:  0.8515625
train loss:  0.3191687762737274
train gradient:  0.14491826440338607
iteration : 1400
train acc:  0.875
train loss:  0.24274082481861115
train gradient:  0.06580914625734792
iteration : 1401
train acc:  0.890625
train loss:  0.22694599628448486
train gradient:  0.0853714104231019
iteration : 1402
train acc:  0.8828125
train loss:  0.29387372732162476
train gradient:  0.1367595188910407
iteration : 1403
train acc:  0.8828125
train loss:  0.30951812863349915
train gradient:  0.1148823549907312
iteration : 1404
train acc:  0.859375
train loss:  0.3197617828845978
train gradient:  0.14651605502189902
iteration : 1405
train acc:  0.8671875
train loss:  0.3064269423484802
train gradient:  0.1391707909700189
iteration : 1406
train acc:  0.875
train loss:  0.3065466582775116
train gradient:  0.15257646485708118
iteration : 1407
train acc:  0.8125
train loss:  0.3950611352920532
train gradient:  0.13080277866060586
iteration : 1408
train acc:  0.8984375
train loss:  0.25427669286727905
train gradient:  0.08821032005485589
iteration : 1409
train acc:  0.875
train loss:  0.3058610260486603
train gradient:  0.08888648198369908
iteration : 1410
train acc:  0.828125
train loss:  0.3556798994541168
train gradient:  0.1697334340269509
iteration : 1411
train acc:  0.921875
train loss:  0.240960031747818
train gradient:  0.08776763104919513
iteration : 1412
train acc:  0.828125
train loss:  0.37247923016548157
train gradient:  0.1437644117817406
iteration : 1413
train acc:  0.875
train loss:  0.2833411693572998
train gradient:  0.1293733181047458
iteration : 1414
train acc:  0.859375
train loss:  0.3344149589538574
train gradient:  0.2320220123158524
iteration : 1415
train acc:  0.8515625
train loss:  0.33061397075653076
train gradient:  0.1558802276789244
iteration : 1416
train acc:  0.875
train loss:  0.2753087878227234
train gradient:  0.1112090576633308
iteration : 1417
train acc:  0.859375
train loss:  0.3697879910469055
train gradient:  0.12958945772829095
iteration : 1418
train acc:  0.90625
train loss:  0.24624110758304596
train gradient:  0.08551633658149722
iteration : 1419
train acc:  0.859375
train loss:  0.27705687284469604
train gradient:  0.08688140720834207
iteration : 1420
train acc:  0.9140625
train loss:  0.25596293807029724
train gradient:  0.08327964161455505
iteration : 1421
train acc:  0.8515625
train loss:  0.31405672430992126
train gradient:  0.20747097688267763
iteration : 1422
train acc:  0.90625
train loss:  0.30926579236984253
train gradient:  0.13688256088916745
iteration : 1423
train acc:  0.828125
train loss:  0.3587009310722351
train gradient:  0.12994708410860517
iteration : 1424
train acc:  0.875
train loss:  0.3225027322769165
train gradient:  0.1102583464260867
iteration : 1425
train acc:  0.859375
train loss:  0.2767145037651062
train gradient:  0.08816474667928641
iteration : 1426
train acc:  0.8203125
train loss:  0.3397595286369324
train gradient:  0.14243312843115474
iteration : 1427
train acc:  0.875
train loss:  0.2978854775428772
train gradient:  0.12950391100264685
iteration : 1428
train acc:  0.8203125
train loss:  0.34979239106178284
train gradient:  0.1301720937347296
iteration : 1429
train acc:  0.859375
train loss:  0.2915889024734497
train gradient:  0.12460316467548144
iteration : 1430
train acc:  0.828125
train loss:  0.34875571727752686
train gradient:  0.21561490357660348
iteration : 1431
train acc:  0.875
train loss:  0.2624395787715912
train gradient:  0.09948225626401395
iteration : 1432
train acc:  0.8671875
train loss:  0.31277287006378174
train gradient:  0.12742208283613748
iteration : 1433
train acc:  0.8515625
train loss:  0.3280588388442993
train gradient:  0.13514443228637463
iteration : 1434
train acc:  0.84375
train loss:  0.3898165822029114
train gradient:  0.1943775400377814
iteration : 1435
train acc:  0.8671875
train loss:  0.3382371962070465
train gradient:  0.14371050108547323
iteration : 1436
train acc:  0.8125
train loss:  0.4033498466014862
train gradient:  0.1648840276022821
iteration : 1437
train acc:  0.8984375
train loss:  0.27467477321624756
train gradient:  0.13369404481710578
iteration : 1438
train acc:  0.8046875
train loss:  0.3383454382419586
train gradient:  0.12919655791662998
iteration : 1439
train acc:  0.875
train loss:  0.2773245573043823
train gradient:  0.16542207113096438
iteration : 1440
train acc:  0.875
train loss:  0.3179515302181244
train gradient:  0.1336079449236636
iteration : 1441
train acc:  0.84375
train loss:  0.29555439949035645
train gradient:  0.10396329665121717
iteration : 1442
train acc:  0.890625
train loss:  0.2644103765487671
train gradient:  0.11711280337761291
iteration : 1443
train acc:  0.8671875
train loss:  0.27872517704963684
train gradient:  0.10355547440739642
iteration : 1444
train acc:  0.8984375
train loss:  0.2759324014186859
train gradient:  0.0904036543397206
iteration : 1445
train acc:  0.8828125
train loss:  0.2600005865097046
train gradient:  0.08825053466576904
iteration : 1446
train acc:  0.8828125
train loss:  0.26992475986480713
train gradient:  0.09937055430796427
iteration : 1447
train acc:  0.890625
train loss:  0.2779761552810669
train gradient:  0.13688810109313693
iteration : 1448
train acc:  0.8671875
train loss:  0.285022497177124
train gradient:  0.09594482630561682
iteration : 1449
train acc:  0.8671875
train loss:  0.2967008054256439
train gradient:  0.11970054642692363
iteration : 1450
train acc:  0.890625
train loss:  0.2952320873737335
train gradient:  0.1305080809460397
iteration : 1451
train acc:  0.859375
train loss:  0.2859399914741516
train gradient:  0.1174483192773284
iteration : 1452
train acc:  0.8828125
train loss:  0.2782352864742279
train gradient:  0.11208874003226242
iteration : 1453
train acc:  0.8984375
train loss:  0.2899206876754761
train gradient:  0.11329719452862519
iteration : 1454
train acc:  0.8828125
train loss:  0.3084360957145691
train gradient:  0.15013456936172273
iteration : 1455
train acc:  0.8515625
train loss:  0.30586254596710205
train gradient:  0.13331319699649946
iteration : 1456
train acc:  0.890625
train loss:  0.2778920829296112
train gradient:  0.11947796358669703
iteration : 1457
train acc:  0.8671875
train loss:  0.29539424180984497
train gradient:  0.09174971791563682
iteration : 1458
train acc:  0.8671875
train loss:  0.32960471510887146
train gradient:  0.12832928429008003
iteration : 1459
train acc:  0.859375
train loss:  0.32136422395706177
train gradient:  0.11531350557798326
iteration : 1460
train acc:  0.828125
train loss:  0.3830028772354126
train gradient:  0.1636096278843029
iteration : 1461
train acc:  0.890625
train loss:  0.27471160888671875
train gradient:  0.09815231000981602
iteration : 1462
train acc:  0.8984375
train loss:  0.2654149532318115
train gradient:  0.08788577772041609
iteration : 1463
train acc:  0.8515625
train loss:  0.26206570863723755
train gradient:  0.11588082723460605
iteration : 1464
train acc:  0.890625
train loss:  0.2362532764673233
train gradient:  0.08808500704300239
iteration : 1465
train acc:  0.8359375
train loss:  0.3586497902870178
train gradient:  0.12663168415674636
iteration : 1466
train acc:  0.859375
train loss:  0.350841224193573
train gradient:  0.12198231380742258
iteration : 1467
train acc:  0.8515625
train loss:  0.3900061249732971
train gradient:  0.15737836244891382
iteration : 1468
train acc:  0.890625
train loss:  0.2556968629360199
train gradient:  0.1781825871946915
iteration : 1469
train acc:  0.90625
train loss:  0.2491908073425293
train gradient:  0.07214473258532299
iteration : 1470
train acc:  0.875
train loss:  0.2818019986152649
train gradient:  0.11848151827504974
iteration : 1471
train acc:  0.8828125
train loss:  0.26982754468917847
train gradient:  0.09953381095783398
iteration : 1472
train acc:  0.859375
train loss:  0.37858474254608154
train gradient:  0.12450204791822296
iteration : 1473
train acc:  0.9140625
train loss:  0.2306225299835205
train gradient:  0.07711277890643352
iteration : 1474
train acc:  0.90625
train loss:  0.26166704297065735
train gradient:  0.09503589104064361
iteration : 1475
train acc:  0.859375
train loss:  0.3402123749256134
train gradient:  0.14885886816177848
iteration : 1476
train acc:  0.8828125
train loss:  0.26044192910194397
train gradient:  0.11764731104884181
iteration : 1477
train acc:  0.859375
train loss:  0.30606651306152344
train gradient:  0.10002529100922763
iteration : 1478
train acc:  0.8515625
train loss:  0.37822195887565613
train gradient:  0.21307493721798537
iteration : 1479
train acc:  0.8046875
train loss:  0.4104453921318054
train gradient:  0.15434955525020383
iteration : 1480
train acc:  0.828125
train loss:  0.3793739080429077
train gradient:  0.16413622697292127
iteration : 1481
train acc:  0.875
train loss:  0.329936683177948
train gradient:  0.16559472551253118
iteration : 1482
train acc:  0.8671875
train loss:  0.3025009036064148
train gradient:  0.13987101215177666
iteration : 1483
train acc:  0.890625
train loss:  0.2368442565202713
train gradient:  0.08142935206492531
iteration : 1484
train acc:  0.859375
train loss:  0.37015676498413086
train gradient:  0.1643079084506049
iteration : 1485
train acc:  0.8515625
train loss:  0.3907580077648163
train gradient:  0.21699228198241527
iteration : 1486
train acc:  0.9375
train loss:  0.1720477193593979
train gradient:  0.0590236361355733
iteration : 1487
train acc:  0.8984375
train loss:  0.2262948900461197
train gradient:  0.11472266603390555
iteration : 1488
train acc:  0.8828125
train loss:  0.27834177017211914
train gradient:  0.13160623990364195
iteration : 1489
train acc:  0.90625
train loss:  0.2413559854030609
train gradient:  0.10089081946851326
iteration : 1490
train acc:  0.875
train loss:  0.29687052965164185
train gradient:  0.10175898132460154
iteration : 1491
train acc:  0.9140625
train loss:  0.26522737741470337
train gradient:  0.1285031220376866
iteration : 1492
train acc:  0.8515625
train loss:  0.3113739788532257
train gradient:  0.11469107910117537
iteration : 1493
train acc:  0.8671875
train loss:  0.3042878806591034
train gradient:  0.10736958294602088
iteration : 1494
train acc:  0.875
train loss:  0.2706506848335266
train gradient:  0.12782115654412696
iteration : 1495
train acc:  0.8828125
train loss:  0.3177821636199951
train gradient:  0.1390386767257826
iteration : 1496
train acc:  0.9140625
train loss:  0.25902828574180603
train gradient:  0.11613028727198338
iteration : 1497
train acc:  0.9140625
train loss:  0.22967106103897095
train gradient:  0.09971505931356235
iteration : 1498
train acc:  0.828125
train loss:  0.4159678816795349
train gradient:  0.2173485432880799
iteration : 1499
train acc:  0.875
train loss:  0.32370811700820923
train gradient:  0.1523740471915186
iteration : 1500
train acc:  0.9296875
train loss:  0.2452007383108139
train gradient:  0.07536197687522296
iteration : 1501
train acc:  0.8359375
train loss:  0.33652088046073914
train gradient:  0.19194093459101863
iteration : 1502
train acc:  0.8203125
train loss:  0.3401373624801636
train gradient:  0.14265579592347336
iteration : 1503
train acc:  0.9140625
train loss:  0.24691987037658691
train gradient:  0.10396534282281922
iteration : 1504
train acc:  0.859375
train loss:  0.2877163290977478
train gradient:  0.4344461860272463
iteration : 1505
train acc:  0.8359375
train loss:  0.36457252502441406
train gradient:  0.21236471894760395
iteration : 1506
train acc:  0.8515625
train loss:  0.3362129032611847
train gradient:  0.1447812114303717
iteration : 1507
train acc:  0.9140625
train loss:  0.2270946353673935
train gradient:  0.12705643575699155
iteration : 1508
train acc:  0.8671875
train loss:  0.3398668169975281
train gradient:  0.14868243397785924
iteration : 1509
train acc:  0.90625
train loss:  0.2766587436199188
train gradient:  0.11602861964374854
iteration : 1510
train acc:  0.890625
train loss:  0.24766811728477478
train gradient:  0.09945866431017228
iteration : 1511
train acc:  0.828125
train loss:  0.38503631949424744
train gradient:  0.1910245121626487
iteration : 1512
train acc:  0.8828125
train loss:  0.23259799182415009
train gradient:  0.0653348886466034
iteration : 1513
train acc:  0.8984375
train loss:  0.22436445951461792
train gradient:  0.08735609225301418
iteration : 1514
train acc:  0.828125
train loss:  0.38387924432754517
train gradient:  0.14850390896754057
iteration : 1515
train acc:  0.8828125
train loss:  0.2745640277862549
train gradient:  0.09270448284308197
iteration : 1516
train acc:  0.8203125
train loss:  0.43195000290870667
train gradient:  0.18735689078968823
iteration : 1517
train acc:  0.875
train loss:  0.30102723836898804
train gradient:  0.12081294884163458
iteration : 1518
train acc:  0.8203125
train loss:  0.3392649292945862
train gradient:  0.1448255869425839
iteration : 1519
train acc:  0.8671875
train loss:  0.317129909992218
train gradient:  0.13646802219931473
iteration : 1520
train acc:  0.8984375
train loss:  0.3073124885559082
train gradient:  0.147494003082907
iteration : 1521
train acc:  0.875
train loss:  0.30159467458724976
train gradient:  0.139550536013166
iteration : 1522
train acc:  0.8359375
train loss:  0.350251704454422
train gradient:  0.15920913520791946
iteration : 1523
train acc:  0.8515625
train loss:  0.268254816532135
train gradient:  0.10878967257647278
iteration : 1524
train acc:  0.90625
train loss:  0.27486640214920044
train gradient:  0.12454421388981145
iteration : 1525
train acc:  0.8828125
train loss:  0.3116981089115143
train gradient:  0.13317333617395605
iteration : 1526
train acc:  0.8671875
train loss:  0.30838122963905334
train gradient:  0.12007848076809839
iteration : 1527
train acc:  0.875
train loss:  0.30241817235946655
train gradient:  0.1504125787562121
iteration : 1528
train acc:  0.8984375
train loss:  0.24692393839359283
train gradient:  0.07790737771707727
iteration : 1529
train acc:  0.84375
train loss:  0.4023968279361725
train gradient:  0.2372348957862324
iteration : 1530
train acc:  0.890625
train loss:  0.2634437084197998
train gradient:  0.153325318423334
iteration : 1531
train acc:  0.8359375
train loss:  0.36490750312805176
train gradient:  0.1476640016055992
iteration : 1532
train acc:  0.859375
train loss:  0.32046929001808167
train gradient:  0.13727021472401627
iteration : 1533
train acc:  0.8203125
train loss:  0.3144569396972656
train gradient:  0.1361582153869863
iteration : 1534
train acc:  0.890625
train loss:  0.23971876502037048
train gradient:  0.09437010804371833
iteration : 1535
train acc:  0.8359375
train loss:  0.31153547763824463
train gradient:  0.13440566084925917
iteration : 1536
train acc:  0.8203125
train loss:  0.3376321792602539
train gradient:  0.1247558187353946
iteration : 1537
train acc:  0.8671875
train loss:  0.29478132724761963
train gradient:  0.1123723077265718
iteration : 1538
train acc:  0.8671875
train loss:  0.37187355756759644
train gradient:  0.1459089340512574
iteration : 1539
train acc:  0.8671875
train loss:  0.3452693819999695
train gradient:  0.15295372979874136
iteration : 1540
train acc:  0.8984375
train loss:  0.27479684352874756
train gradient:  0.09087342927140046
iteration : 1541
train acc:  0.828125
train loss:  0.3335563540458679
train gradient:  0.170356561914434
iteration : 1542
train acc:  0.8671875
train loss:  0.33544862270355225
train gradient:  0.13272741020366047
iteration : 1543
train acc:  0.8671875
train loss:  0.3502841889858246
train gradient:  0.1479999928424445
iteration : 1544
train acc:  0.90625
train loss:  0.26618990302085876
train gradient:  0.09863811003094294
iteration : 1545
train acc:  0.828125
train loss:  0.30496200919151306
train gradient:  0.22070167671939922
iteration : 1546
train acc:  0.890625
train loss:  0.27575206756591797
train gradient:  0.10820118961782191
iteration : 1547
train acc:  0.8984375
train loss:  0.2827451527118683
train gradient:  0.11931320515108988
iteration : 1548
train acc:  0.8515625
train loss:  0.32141897082328796
train gradient:  0.12882619245300037
iteration : 1549
train acc:  0.90625
train loss:  0.23217518627643585
train gradient:  0.11671357300265985
iteration : 1550
train acc:  0.875
train loss:  0.361732542514801
train gradient:  0.15394631777704065
iteration : 1551
train acc:  0.8359375
train loss:  0.3415593206882477
train gradient:  0.15139645797858337
iteration : 1552
train acc:  0.875
train loss:  0.25945883989334106
train gradient:  0.10183209549270518
iteration : 1553
train acc:  0.8671875
train loss:  0.28148353099823
train gradient:  0.0901066662552965
iteration : 1554
train acc:  0.9140625
train loss:  0.2363801896572113
train gradient:  0.08394160664497395
iteration : 1555
train acc:  0.8671875
train loss:  0.31256103515625
train gradient:  0.11297380174277759
iteration : 1556
train acc:  0.875
train loss:  0.2768484950065613
train gradient:  0.12084712840556633
iteration : 1557
train acc:  0.828125
train loss:  0.3615648150444031
train gradient:  0.2100809635416046
iteration : 1558
train acc:  0.8515625
train loss:  0.2995292842388153
train gradient:  0.14760483485553924
iteration : 1559
train acc:  0.8984375
train loss:  0.2600420117378235
train gradient:  0.09062499631712306
iteration : 1560
train acc:  0.8671875
train loss:  0.31654027104377747
train gradient:  0.1326459743231455
iteration : 1561
train acc:  0.8984375
train loss:  0.2891804575920105
train gradient:  0.10384724402047343
iteration : 1562
train acc:  0.859375
train loss:  0.27050328254699707
train gradient:  0.13582062655537347
iteration : 1563
train acc:  0.9375
train loss:  0.21983900666236877
train gradient:  0.07077343659990055
iteration : 1564
train acc:  0.890625
train loss:  0.31932157278060913
train gradient:  0.1271273479350286
iteration : 1565
train acc:  0.8671875
train loss:  0.35631710290908813
train gradient:  0.2057056084841413
iteration : 1566
train acc:  0.875
train loss:  0.2988290786743164
train gradient:  0.13865363542338682
iteration : 1567
train acc:  0.84375
train loss:  0.3490332365036011
train gradient:  0.15011140028380932
iteration : 1568
train acc:  0.875
train loss:  0.26469886302948
train gradient:  0.09831729844032441
iteration : 1569
train acc:  0.859375
train loss:  0.27492856979370117
train gradient:  0.10671897787292103
iteration : 1570
train acc:  0.8828125
train loss:  0.3053865134716034
train gradient:  0.12610983322211083
iteration : 1571
train acc:  0.8671875
train loss:  0.2864531874656677
train gradient:  0.09842979621847374
iteration : 1572
train acc:  0.890625
train loss:  0.24178455770015717
train gradient:  0.08880628141504984
iteration : 1573
train acc:  0.9453125
train loss:  0.22294588387012482
train gradient:  0.1019472423564661
iteration : 1574
train acc:  0.84375
train loss:  0.41107040643692017
train gradient:  0.18379564545020177
iteration : 1575
train acc:  0.84375
train loss:  0.39596641063690186
train gradient:  0.17418700308726703
iteration : 1576
train acc:  0.921875
train loss:  0.2149450182914734
train gradient:  0.07913164652049615
iteration : 1577
train acc:  0.8671875
train loss:  0.38642632961273193
train gradient:  0.14591572753534904
iteration : 1578
train acc:  0.8984375
train loss:  0.31203535199165344
train gradient:  0.14670153795532873
iteration : 1579
train acc:  0.875
train loss:  0.32253989577293396
train gradient:  0.15046282031758262
iteration : 1580
train acc:  0.8515625
train loss:  0.381184458732605
train gradient:  0.14080933761195497
iteration : 1581
train acc:  0.890625
train loss:  0.25747227668762207
train gradient:  0.08919902817171858
iteration : 1582
train acc:  0.875
train loss:  0.24852433800697327
train gradient:  0.10815084252657664
iteration : 1583
train acc:  0.890625
train loss:  0.266319215297699
train gradient:  0.0930099077613268
iteration : 1584
train acc:  0.8671875
train loss:  0.36621564626693726
train gradient:  0.2048747145031417
iteration : 1585
train acc:  0.8828125
train loss:  0.2890126705169678
train gradient:  0.07883513665000384
iteration : 1586
train acc:  0.859375
train loss:  0.3151894211769104
train gradient:  0.13512968529324343
iteration : 1587
train acc:  0.90625
train loss:  0.278299480676651
train gradient:  0.1734012034695521
iteration : 1588
train acc:  0.859375
train loss:  0.35008031129837036
train gradient:  0.1942429360499252
iteration : 1589
train acc:  0.8046875
train loss:  0.4389464855194092
train gradient:  0.15751110808407698
iteration : 1590
train acc:  0.875
train loss:  0.35521867871284485
train gradient:  0.16125018478221187
iteration : 1591
train acc:  0.84375
train loss:  0.33114680647850037
train gradient:  0.13077233696040175
iteration : 1592
train acc:  0.8671875
train loss:  0.31885531544685364
train gradient:  0.1129067962336781
iteration : 1593
train acc:  0.90625
train loss:  0.22058910131454468
train gradient:  0.07364335616914837
iteration : 1594
train acc:  0.890625
train loss:  0.33904147148132324
train gradient:  0.12304028793693858
iteration : 1595
train acc:  0.875
train loss:  0.26580196619033813
train gradient:  0.10892220253803857
iteration : 1596
train acc:  0.890625
train loss:  0.32965683937072754
train gradient:  0.10097308662399938
iteration : 1597
train acc:  0.9140625
train loss:  0.2635994851589203
train gradient:  0.1037301629275748
iteration : 1598
train acc:  0.8671875
train loss:  0.30892640352249146
train gradient:  0.11141657937931583
iteration : 1599
train acc:  0.8828125
train loss:  0.2671430706977844
train gradient:  0.11593810459937143
iteration : 1600
train acc:  0.8515625
train loss:  0.3073069453239441
train gradient:  0.1268699480139798
iteration : 1601
train acc:  0.84375
train loss:  0.372234582901001
train gradient:  0.18822992199770588
iteration : 1602
train acc:  0.859375
train loss:  0.27230000495910645
train gradient:  0.11238599193440724
iteration : 1603
train acc:  0.8671875
train loss:  0.26695817708969116
train gradient:  0.1284316396742491
iteration : 1604
train acc:  0.8671875
train loss:  0.2626551687717438
train gradient:  0.1138105271635086
iteration : 1605
train acc:  0.8671875
train loss:  0.31748831272125244
train gradient:  0.10278360377870983
iteration : 1606
train acc:  0.828125
train loss:  0.379932165145874
train gradient:  0.14768440974616043
iteration : 1607
train acc:  0.890625
train loss:  0.2738005220890045
train gradient:  0.1271804583645738
iteration : 1608
train acc:  0.8671875
train loss:  0.30901166796684265
train gradient:  0.11214076018105265
iteration : 1609
train acc:  0.890625
train loss:  0.2705911695957184
train gradient:  0.10925418862904278
iteration : 1610
train acc:  0.8359375
train loss:  0.39178889989852905
train gradient:  0.19294062158153802
iteration : 1611
train acc:  0.9296875
train loss:  0.2620890140533447
train gradient:  0.09595931833645048
iteration : 1612
train acc:  0.8359375
train loss:  0.35104483366012573
train gradient:  0.1737876612578308
iteration : 1613
train acc:  0.8515625
train loss:  0.33383360505104065
train gradient:  0.1998191358295005
iteration : 1614
train acc:  0.8515625
train loss:  0.3928672671318054
train gradient:  0.17364092971801953
iteration : 1615
train acc:  0.8203125
train loss:  0.4066547155380249
train gradient:  0.20049301911136333
iteration : 1616
train acc:  0.90625
train loss:  0.2421073317527771
train gradient:  0.09901562739733216
iteration : 1617
train acc:  0.8515625
train loss:  0.2751139998435974
train gradient:  0.104884024444779
iteration : 1618
train acc:  0.84375
train loss:  0.34954148530960083
train gradient:  0.11104575753801138
iteration : 1619
train acc:  0.8515625
train loss:  0.3579680919647217
train gradient:  0.14654335478226235
iteration : 1620
train acc:  0.8828125
train loss:  0.3070431649684906
train gradient:  0.1267227027049319
iteration : 1621
train acc:  0.8828125
train loss:  0.26948726177215576
train gradient:  0.08949734618654058
iteration : 1622
train acc:  0.8828125
train loss:  0.2674756944179535
train gradient:  0.1279029911503358
iteration : 1623
train acc:  0.828125
train loss:  0.33230501413345337
train gradient:  0.15622801506837236
iteration : 1624
train acc:  0.8515625
train loss:  0.3553087115287781
train gradient:  0.21305903193429485
iteration : 1625
train acc:  0.9140625
train loss:  0.24065233767032623
train gradient:  0.06766036095699864
iteration : 1626
train acc:  0.8671875
train loss:  0.27345454692840576
train gradient:  0.09040317597524365
iteration : 1627
train acc:  0.859375
train loss:  0.35691842436790466
train gradient:  0.17934666904006305
iteration : 1628
train acc:  0.84375
train loss:  0.30214959383010864
train gradient:  0.126303912306632
iteration : 1629
train acc:  0.90625
train loss:  0.2776445746421814
train gradient:  0.1006638995405368
iteration : 1630
train acc:  0.859375
train loss:  0.27734434604644775
train gradient:  0.1159183547241045
iteration : 1631
train acc:  0.859375
train loss:  0.3899327218532562
train gradient:  0.1397162060409778
iteration : 1632
train acc:  0.8671875
train loss:  0.3014456629753113
train gradient:  0.12461836606184273
iteration : 1633
train acc:  0.8359375
train loss:  0.3519231081008911
train gradient:  0.11891678255617773
iteration : 1634
train acc:  0.84375
train loss:  0.3933113217353821
train gradient:  0.1475657908661238
iteration : 1635
train acc:  0.859375
train loss:  0.32253551483154297
train gradient:  0.10973420168200944
iteration : 1636
train acc:  0.84375
train loss:  0.3541944622993469
train gradient:  0.13798866590566733
iteration : 1637
train acc:  0.8203125
train loss:  0.3527277112007141
train gradient:  0.1942863637469827
iteration : 1638
train acc:  0.84375
train loss:  0.36530840396881104
train gradient:  0.13626780758454557
iteration : 1639
train acc:  0.875
train loss:  0.3760682940483093
train gradient:  0.25408475089833693
iteration : 1640
train acc:  0.859375
train loss:  0.3146550953388214
train gradient:  0.11505674346972787
iteration : 1641
train acc:  0.8515625
train loss:  0.2951053977012634
train gradient:  0.14880147896147075
iteration : 1642
train acc:  0.890625
train loss:  0.23948350548744202
train gradient:  0.06049260537167651
iteration : 1643
train acc:  0.84375
train loss:  0.31379956007003784
train gradient:  0.13038917128471367
iteration : 1644
train acc:  0.859375
train loss:  0.29073861241340637
train gradient:  0.14760963171279934
iteration : 1645
train acc:  0.8515625
train loss:  0.3556727170944214
train gradient:  0.14650556488666128
iteration : 1646
train acc:  0.90625
train loss:  0.25937265157699585
train gradient:  0.12146274835001325
iteration : 1647
train acc:  0.8515625
train loss:  0.318664014339447
train gradient:  0.08118263585693074
iteration : 1648
train acc:  0.828125
train loss:  0.342584490776062
train gradient:  0.18083413717146857
iteration : 1649
train acc:  0.890625
train loss:  0.28771623969078064
train gradient:  0.08029247139464746
iteration : 1650
train acc:  0.8671875
train loss:  0.28702202439308167
train gradient:  0.0845316710388756
iteration : 1651
train acc:  0.9296875
train loss:  0.22412869334220886
train gradient:  0.09068846795075483
iteration : 1652
train acc:  0.8828125
train loss:  0.27226120233535767
train gradient:  0.10496385495992513
iteration : 1653
train acc:  0.8359375
train loss:  0.3660278022289276
train gradient:  0.12980374585133142
iteration : 1654
train acc:  0.8515625
train loss:  0.2846057116985321
train gradient:  0.08324516726197245
iteration : 1655
train acc:  0.859375
train loss:  0.36491721868515015
train gradient:  0.11856444918684822
iteration : 1656
train acc:  0.8984375
train loss:  0.27288150787353516
train gradient:  0.10593333677369392
iteration : 1657
train acc:  0.875
train loss:  0.3115226924419403
train gradient:  0.15194164619392314
iteration : 1658
train acc:  0.828125
train loss:  0.3309364914894104
train gradient:  0.16099750870178223
iteration : 1659
train acc:  0.8671875
train loss:  0.28214365243911743
train gradient:  0.09503420832011451
iteration : 1660
train acc:  0.890625
train loss:  0.3031289875507355
train gradient:  0.10414472313038428
iteration : 1661
train acc:  0.828125
train loss:  0.41149500012397766
train gradient:  0.17507602509811054
iteration : 1662
train acc:  0.8828125
train loss:  0.28572893142700195
train gradient:  0.10422152522577527
iteration : 1663
train acc:  0.84375
train loss:  0.32908111810684204
train gradient:  0.14411202341750437
iteration : 1664
train acc:  0.859375
train loss:  0.29306769371032715
train gradient:  0.10453765507721864
iteration : 1665
train acc:  0.8828125
train loss:  0.28293073177337646
train gradient:  0.09516628357703845
iteration : 1666
train acc:  0.859375
train loss:  0.2655516266822815
train gradient:  0.09303267465995146
iteration : 1667
train acc:  0.8671875
train loss:  0.28826630115509033
train gradient:  0.14375789692601593
iteration : 1668
train acc:  0.8515625
train loss:  0.2988162040710449
train gradient:  0.11589636501984664
iteration : 1669
train acc:  0.890625
train loss:  0.3103525638580322
train gradient:  0.12357277291303244
iteration : 1670
train acc:  0.8515625
train loss:  0.3397790491580963
train gradient:  0.12166644012474107
iteration : 1671
train acc:  0.84375
train loss:  0.30428582429885864
train gradient:  0.09994256271413514
iteration : 1672
train acc:  0.8515625
train loss:  0.3041459321975708
train gradient:  0.09572219431555284
iteration : 1673
train acc:  0.8671875
train loss:  0.2899559438228607
train gradient:  0.12936249620341062
iteration : 1674
train acc:  0.8671875
train loss:  0.33277761936187744
train gradient:  0.12455398613625604
iteration : 1675
train acc:  0.90625
train loss:  0.2453312873840332
train gradient:  0.0887315782371443
iteration : 1676
train acc:  0.8671875
train loss:  0.397897332906723
train gradient:  0.1443998918032994
iteration : 1677
train acc:  0.8671875
train loss:  0.3578815758228302
train gradient:  0.18594410135413109
iteration : 1678
train acc:  0.9140625
train loss:  0.2157232165336609
train gradient:  0.0550071037736785
iteration : 1679
train acc:  0.8515625
train loss:  0.31532615423202515
train gradient:  0.1379999086131261
iteration : 1680
train acc:  0.8125
train loss:  0.35024312138557434
train gradient:  0.14232302279517817
iteration : 1681
train acc:  0.90625
train loss:  0.2581300139427185
train gradient:  0.11528987464939282
iteration : 1682
train acc:  0.875
train loss:  0.30761757493019104
train gradient:  0.10172046999040368
iteration : 1683
train acc:  0.9453125
train loss:  0.20209425687789917
train gradient:  0.07335323639700293
iteration : 1684
train acc:  0.828125
train loss:  0.41848236322402954
train gradient:  0.1751284277168122
iteration : 1685
train acc:  0.8359375
train loss:  0.2995019555091858
train gradient:  0.12105376053979192
iteration : 1686
train acc:  0.859375
train loss:  0.3254186511039734
train gradient:  0.19062619815916593
iteration : 1687
train acc:  0.8515625
train loss:  0.3664998412132263
train gradient:  0.15293705693352397
iteration : 1688
train acc:  0.828125
train loss:  0.3439566493034363
train gradient:  0.193496380655537
iteration : 1689
train acc:  0.90625
train loss:  0.26994383335113525
train gradient:  0.1286443262962859
iteration : 1690
train acc:  0.890625
train loss:  0.28199702501296997
train gradient:  0.08982681569434023
iteration : 1691
train acc:  0.8984375
train loss:  0.2447061836719513
train gradient:  0.11578332695036204
iteration : 1692
train acc:  0.859375
train loss:  0.3597479462623596
train gradient:  0.17326323604109578
iteration : 1693
train acc:  0.859375
train loss:  0.28043484687805176
train gradient:  0.09427124483042676
iteration : 1694
train acc:  0.796875
train loss:  0.4419780969619751
train gradient:  0.21795740227563307
iteration : 1695
train acc:  0.796875
train loss:  0.46945199370384216
train gradient:  0.3052954828789183
iteration : 1696
train acc:  0.8828125
train loss:  0.25380444526672363
train gradient:  0.07492600845828506
iteration : 1697
train acc:  0.8359375
train loss:  0.3528731167316437
train gradient:  0.15112929298667577
iteration : 1698
train acc:  0.90625
train loss:  0.22006940841674805
train gradient:  0.06947065564766755
iteration : 1699
train acc:  0.859375
train loss:  0.35021358728408813
train gradient:  0.12608343059509752
iteration : 1700
train acc:  0.859375
train loss:  0.31094199419021606
train gradient:  0.17384806974000577
iteration : 1701
train acc:  0.859375
train loss:  0.3180880844593048
train gradient:  0.09221567934716991
iteration : 1702
train acc:  0.8984375
train loss:  0.23738692700862885
train gradient:  0.0924737543124091
iteration : 1703
train acc:  0.859375
train loss:  0.3820042014122009
train gradient:  0.18133359054994314
iteration : 1704
train acc:  0.8125
train loss:  0.35521382093429565
train gradient:  0.16182247011330791
iteration : 1705
train acc:  0.8828125
train loss:  0.2802427411079407
train gradient:  0.12842283502292512
iteration : 1706
train acc:  0.875
train loss:  0.2643100321292877
train gradient:  0.08274941609868262
iteration : 1707
train acc:  0.8671875
train loss:  0.3741286098957062
train gradient:  0.2585266314023149
iteration : 1708
train acc:  0.84375
train loss:  0.2860789895057678
train gradient:  0.10219365629984346
iteration : 1709
train acc:  0.875
train loss:  0.26040005683898926
train gradient:  0.08498177305198923
iteration : 1710
train acc:  0.8359375
train loss:  0.375426322221756
train gradient:  0.14717996148323978
iteration : 1711
train acc:  0.890625
train loss:  0.2932147979736328
train gradient:  0.10719397680622922
iteration : 1712
train acc:  0.875
train loss:  0.2941076159477234
train gradient:  0.11632782633201545
iteration : 1713
train acc:  0.8671875
train loss:  0.31074029207229614
train gradient:  0.09822400958057159
iteration : 1714
train acc:  0.8828125
train loss:  0.27192965149879456
train gradient:  0.09561039780751948
iteration : 1715
train acc:  0.8515625
train loss:  0.3342762887477875
train gradient:  0.13821370113058845
iteration : 1716
train acc:  0.8515625
train loss:  0.30724334716796875
train gradient:  0.1168518991329975
iteration : 1717
train acc:  0.8671875
train loss:  0.3209182918071747
train gradient:  0.10833453968225099
iteration : 1718
train acc:  0.84375
train loss:  0.3749062418937683
train gradient:  0.1490676790351782
iteration : 1719
train acc:  0.8671875
train loss:  0.3702942728996277
train gradient:  0.16062011769821655
iteration : 1720
train acc:  0.8203125
train loss:  0.33614635467529297
train gradient:  0.1612436129580343
iteration : 1721
train acc:  0.8515625
train loss:  0.36577022075653076
train gradient:  0.17342151674668782
iteration : 1722
train acc:  0.7734375
train loss:  0.4100925922393799
train gradient:  0.2139092135416909
iteration : 1723
train acc:  0.875
train loss:  0.31997188925743103
train gradient:  0.11993056037342606
iteration : 1724
train acc:  0.875
train loss:  0.27148497104644775
train gradient:  0.09709324055614882
iteration : 1725
train acc:  0.8671875
train loss:  0.3170338273048401
train gradient:  0.14795724240064706
iteration : 1726
train acc:  0.8671875
train loss:  0.2872743606567383
train gradient:  0.07713885239582362
iteration : 1727
train acc:  0.8125
train loss:  0.3922537565231323
train gradient:  0.35870682717019203
iteration : 1728
train acc:  0.859375
train loss:  0.3296229839324951
train gradient:  0.2048354174128007
iteration : 1729
train acc:  0.8203125
train loss:  0.3555889129638672
train gradient:  0.14710863766697121
iteration : 1730
train acc:  0.8828125
train loss:  0.2647615671157837
train gradient:  0.09821866932149442
iteration : 1731
train acc:  0.90625
train loss:  0.25494444370269775
train gradient:  0.08386865165408372
iteration : 1732
train acc:  0.8671875
train loss:  0.34092164039611816
train gradient:  0.13506708528328562
iteration : 1733
train acc:  0.8515625
train loss:  0.2769666910171509
train gradient:  0.1008811449926492
iteration : 1734
train acc:  0.8984375
train loss:  0.2352534532546997
train gradient:  0.10562712323557466
iteration : 1735
train acc:  0.8515625
train loss:  0.3362553119659424
train gradient:  0.12341107246682378
iteration : 1736
train acc:  0.890625
train loss:  0.30807673931121826
train gradient:  0.10634195122004668
iteration : 1737
train acc:  0.84375
train loss:  0.32624107599258423
train gradient:  0.13302839316589515
iteration : 1738
train acc:  0.90625
train loss:  0.24438568949699402
train gradient:  0.1268145700509461
iteration : 1739
train acc:  0.8203125
train loss:  0.31704509258270264
train gradient:  0.15232949815596442
iteration : 1740
train acc:  0.875
train loss:  0.271686315536499
train gradient:  0.13617621048291778
iteration : 1741
train acc:  0.8203125
train loss:  0.36299821734428406
train gradient:  0.13335454901444566
iteration : 1742
train acc:  0.8046875
train loss:  0.3957018256187439
train gradient:  0.28483584526212213
iteration : 1743
train acc:  0.8671875
train loss:  0.3656999468803406
train gradient:  0.19549145209236435
iteration : 1744
train acc:  0.8359375
train loss:  0.3353169560432434
train gradient:  0.12820958973813373
iteration : 1745
train acc:  0.8515625
train loss:  0.3559064269065857
train gradient:  0.1750258489981775
iteration : 1746
train acc:  0.8359375
train loss:  0.3512563407421112
train gradient:  0.20588956120935853
iteration : 1747
train acc:  0.8828125
train loss:  0.31289151310920715
train gradient:  0.17725205555592852
iteration : 1748
train acc:  0.9296875
train loss:  0.2277723252773285
train gradient:  0.08516269107137968
iteration : 1749
train acc:  0.8125
train loss:  0.36933577060699463
train gradient:  0.18476659644568666
iteration : 1750
train acc:  0.8828125
train loss:  0.3120551109313965
train gradient:  0.25360908323336523
iteration : 1751
train acc:  0.859375
train loss:  0.28159821033477783
train gradient:  0.10899162259431573
iteration : 1752
train acc:  0.9296875
train loss:  0.23248818516731262
train gradient:  0.09196445738632902
iteration : 1753
train acc:  0.90625
train loss:  0.2476651966571808
train gradient:  0.09100077635378155
iteration : 1754
train acc:  0.8984375
train loss:  0.25555533170700073
train gradient:  0.1191976890291844
iteration : 1755
train acc:  0.9375
train loss:  0.20842382311820984
train gradient:  0.07825801651526906
iteration : 1756
train acc:  0.8359375
train loss:  0.29514965415000916
train gradient:  0.11505360288458341
iteration : 1757
train acc:  0.9140625
train loss:  0.2554273009300232
train gradient:  0.08135794949328565
iteration : 1758
train acc:  0.8671875
train loss:  0.2968199551105499
train gradient:  0.14086384375409444
iteration : 1759
train acc:  0.8671875
train loss:  0.2831081748008728
train gradient:  0.15983503573881064
iteration : 1760
train acc:  0.84375
train loss:  0.40297597646713257
train gradient:  0.16383116586030255
iteration : 1761
train acc:  0.8359375
train loss:  0.40324288606643677
train gradient:  0.185815640608488
iteration : 1762
train acc:  0.859375
train loss:  0.28270238637924194
train gradient:  0.1945312076096445
iteration : 1763
train acc:  0.8671875
train loss:  0.29183506965637207
train gradient:  0.10205120914950512
iteration : 1764
train acc:  0.8515625
train loss:  0.3694465458393097
train gradient:  0.13175657383225026
iteration : 1765
train acc:  0.8671875
train loss:  0.32823532819747925
train gradient:  0.1114139268306757
iteration : 1766
train acc:  0.8984375
train loss:  0.24865645170211792
train gradient:  0.06974022821409297
iteration : 1767
train acc:  0.8828125
train loss:  0.2902328073978424
train gradient:  0.10997446339555295
iteration : 1768
train acc:  0.90625
train loss:  0.19599705934524536
train gradient:  0.07328289894508663
iteration : 1769
train acc:  0.8515625
train loss:  0.34307771921157837
train gradient:  0.12196374661513931
iteration : 1770
train acc:  0.8359375
train loss:  0.36056962609291077
train gradient:  0.15882581939157953
iteration : 1771
train acc:  0.8203125
train loss:  0.48830750584602356
train gradient:  0.25106309349860223
iteration : 1772
train acc:  0.8671875
train loss:  0.30449461936950684
train gradient:  0.12720180010126347
iteration : 1773
train acc:  0.8828125
train loss:  0.2591274380683899
train gradient:  0.12654674972238966
iteration : 1774
train acc:  0.859375
train loss:  0.3197830319404602
train gradient:  0.13960315270474863
iteration : 1775
train acc:  0.8515625
train loss:  0.29724380373954773
train gradient:  0.12061716854683945
iteration : 1776
train acc:  0.8828125
train loss:  0.30285143852233887
train gradient:  0.09385688721127473
iteration : 1777
train acc:  0.890625
train loss:  0.2384353131055832
train gradient:  0.08183239523418386
iteration : 1778
train acc:  0.828125
train loss:  0.38043278455734253
train gradient:  0.20004010426042862
iteration : 1779
train acc:  0.8125
train loss:  0.41153013706207275
train gradient:  0.19057895488507223
iteration : 1780
train acc:  0.8984375
train loss:  0.2632785439491272
train gradient:  0.09124558261188238
iteration : 1781
train acc:  0.84375
train loss:  0.356045663356781
train gradient:  0.12644677084935274
iteration : 1782
train acc:  0.9140625
train loss:  0.2842864990234375
train gradient:  0.1011507789155142
iteration : 1783
train acc:  0.84375
train loss:  0.446767121553421
train gradient:  0.16592970901825188
iteration : 1784
train acc:  0.9296875
train loss:  0.21993842720985413
train gradient:  0.08521385264395621
iteration : 1785
train acc:  0.8671875
train loss:  0.35035067796707153
train gradient:  0.12762220831611298
iteration : 1786
train acc:  0.90625
train loss:  0.2528815269470215
train gradient:  0.0848944702749167
iteration : 1787
train acc:  0.890625
train loss:  0.2437870353460312
train gradient:  0.10379712147000436
iteration : 1788
train acc:  0.875
train loss:  0.2749994993209839
train gradient:  0.117394070474865
iteration : 1789
train acc:  0.8828125
train loss:  0.22218963503837585
train gradient:  0.06627174265239753
iteration : 1790
train acc:  0.90625
train loss:  0.289513498544693
train gradient:  0.10177982559223839
iteration : 1791
train acc:  0.8828125
train loss:  0.28429439663887024
train gradient:  0.0990380115399513
iteration : 1792
train acc:  0.8125
train loss:  0.3519555330276489
train gradient:  0.13090644386084058
iteration : 1793
train acc:  0.859375
train loss:  0.28325700759887695
train gradient:  0.09980173322269371
iteration : 1794
train acc:  0.8671875
train loss:  0.2825837731361389
train gradient:  0.09935971129993786
iteration : 1795
train acc:  0.8671875
train loss:  0.30823200941085815
train gradient:  0.13922534557996358
iteration : 1796
train acc:  0.8671875
train loss:  0.2894902229309082
train gradient:  0.1190299279644494
iteration : 1797
train acc:  0.8984375
train loss:  0.3003268539905548
train gradient:  0.10258113649650775
iteration : 1798
train acc:  0.84375
train loss:  0.3872274160385132
train gradient:  0.1768779800729176
iteration : 1799
train acc:  0.8671875
train loss:  0.3169865906238556
train gradient:  0.10259746660343669
iteration : 1800
train acc:  0.890625
train loss:  0.263599157333374
train gradient:  0.13943963274828763
iteration : 1801
train acc:  0.875
train loss:  0.3293464779853821
train gradient:  0.15113600733103236
iteration : 1802
train acc:  0.859375
train loss:  0.3546293377876282
train gradient:  0.15981656352229515
iteration : 1803
train acc:  0.875
train loss:  0.30111122131347656
train gradient:  0.09727624199296699
iteration : 1804
train acc:  0.8671875
train loss:  0.26984602212905884
train gradient:  0.16022827473319756
iteration : 1805
train acc:  0.8828125
train loss:  0.32784226536750793
train gradient:  0.14715099919532193
iteration : 1806
train acc:  0.84375
train loss:  0.35304152965545654
train gradient:  0.15384039368704228
iteration : 1807
train acc:  0.8359375
train loss:  0.28977271914482117
train gradient:  0.13364357863329673
iteration : 1808
train acc:  0.8515625
train loss:  0.3108516335487366
train gradient:  0.11968285285453785
iteration : 1809
train acc:  0.8359375
train loss:  0.33939069509506226
train gradient:  0.1196763249820834
iteration : 1810
train acc:  0.8359375
train loss:  0.4070214033126831
train gradient:  0.17342906661784718
iteration : 1811
train acc:  0.875
train loss:  0.27864035964012146
train gradient:  0.14003792110804603
iteration : 1812
train acc:  0.8984375
train loss:  0.2716558575630188
train gradient:  0.09073527360492113
iteration : 1813
train acc:  0.8359375
train loss:  0.31847602128982544
train gradient:  0.18106093544028423
iteration : 1814
train acc:  0.8671875
train loss:  0.31527218222618103
train gradient:  0.11903854826888217
iteration : 1815
train acc:  0.78125
train loss:  0.4189176857471466
train gradient:  0.1461328589031159
iteration : 1816
train acc:  0.8359375
train loss:  0.40451115369796753
train gradient:  0.2151666446857658
iteration : 1817
train acc:  0.84375
train loss:  0.36091622710227966
train gradient:  0.14259822786565884
iteration : 1818
train acc:  0.84375
train loss:  0.38311588764190674
train gradient:  0.22924638466937544
iteration : 1819
train acc:  0.8203125
train loss:  0.3732987642288208
train gradient:  0.145316705607095
iteration : 1820
train acc:  0.8125
train loss:  0.45189905166625977
train gradient:  0.21923570767820807
iteration : 1821
train acc:  0.875
train loss:  0.3329034149646759
train gradient:  0.1721524732366812
iteration : 1822
train acc:  0.890625
train loss:  0.2689466178417206
train gradient:  0.11751481223747316
iteration : 1823
train acc:  0.8671875
train loss:  0.31965842843055725
train gradient:  0.1302479880375259
iteration : 1824
train acc:  0.8515625
train loss:  0.32662659883499146
train gradient:  0.12840693385928337
iteration : 1825
train acc:  0.875
train loss:  0.25773727893829346
train gradient:  0.0742896009184611
iteration : 1826
train acc:  0.8828125
train loss:  0.2655133605003357
train gradient:  0.08183674720121949
iteration : 1827
train acc:  0.8984375
train loss:  0.2933223843574524
train gradient:  0.10748973523233461
iteration : 1828
train acc:  0.8515625
train loss:  0.3280094265937805
train gradient:  0.10800642823901971
iteration : 1829
train acc:  0.8828125
train loss:  0.2733064889907837
train gradient:  0.07851316129135083
iteration : 1830
train acc:  0.921875
train loss:  0.27318888902664185
train gradient:  0.1058698962158087
iteration : 1831
train acc:  0.875
train loss:  0.28014159202575684
train gradient:  0.10668870304264935
iteration : 1832
train acc:  0.8828125
train loss:  0.30811113119125366
train gradient:  0.14502771896473998
iteration : 1833
train acc:  0.828125
train loss:  0.4064384698867798
train gradient:  0.23834952348962196
iteration : 1834
train acc:  0.859375
train loss:  0.29505419731140137
train gradient:  0.13375791603687248
iteration : 1835
train acc:  0.8125
train loss:  0.3649652302265167
train gradient:  0.4161094329799324
iteration : 1836
train acc:  0.9140625
train loss:  0.3039005398750305
train gradient:  0.09402846396390512
iteration : 1837
train acc:  0.8828125
train loss:  0.31696534156799316
train gradient:  0.10786489462856086
iteration : 1838
train acc:  0.8828125
train loss:  0.2598884701728821
train gradient:  0.07484410867605068
iteration : 1839
train acc:  0.8828125
train loss:  0.27476781606674194
train gradient:  0.07953526249193292
iteration : 1840
train acc:  0.9296875
train loss:  0.24141907691955566
train gradient:  0.08297404911223459
iteration : 1841
train acc:  0.8671875
train loss:  0.3805310130119324
train gradient:  0.1390852607922084
iteration : 1842
train acc:  0.828125
train loss:  0.3779023289680481
train gradient:  0.21450418658535067
iteration : 1843
train acc:  0.859375
train loss:  0.2956511378288269
train gradient:  0.10392924967017306
iteration : 1844
train acc:  0.8515625
train loss:  0.2738928198814392
train gradient:  0.08057024555710393
iteration : 1845
train acc:  0.8125
train loss:  0.3964892029762268
train gradient:  0.2141842288233871
iteration : 1846
train acc:  0.8203125
train loss:  0.46038785576820374
train gradient:  0.21144131007780637
iteration : 1847
train acc:  0.8515625
train loss:  0.33811044692993164
train gradient:  0.1078685855495885
iteration : 1848
train acc:  0.875
train loss:  0.28235989809036255
train gradient:  0.1139435956091619
iteration : 1849
train acc:  0.8359375
train loss:  0.37051981687545776
train gradient:  0.12128004296100271
iteration : 1850
train acc:  0.9140625
train loss:  0.2917834520339966
train gradient:  0.10848802580973722
iteration : 1851
train acc:  0.921875
train loss:  0.2531755566596985
train gradient:  0.11318226178758585
iteration : 1852
train acc:  0.8359375
train loss:  0.3554318845272064
train gradient:  0.14300364017839023
iteration : 1853
train acc:  0.84375
train loss:  0.42362821102142334
train gradient:  0.19186715335578614
iteration : 1854
train acc:  0.890625
train loss:  0.3005118668079376
train gradient:  0.28540250530783995
iteration : 1855
train acc:  0.9140625
train loss:  0.252127081155777
train gradient:  0.1277398566689481
iteration : 1856
train acc:  0.90625
train loss:  0.23837438225746155
train gradient:  0.13330758611504073
iteration : 1857
train acc:  0.8828125
train loss:  0.25422918796539307
train gradient:  0.11446158342103291
iteration : 1858
train acc:  0.8515625
train loss:  0.3490505516529083
train gradient:  0.1411834899596768
iteration : 1859
train acc:  0.9296875
train loss:  0.25061628222465515
train gradient:  0.06592506683498532
iteration : 1860
train acc:  0.8671875
train loss:  0.2772413492202759
train gradient:  0.1443954924688448
iteration : 1861
train acc:  0.8515625
train loss:  0.29315614700317383
train gradient:  0.1003465833197433
iteration : 1862
train acc:  0.859375
train loss:  0.34644177556037903
train gradient:  0.1398841502856017
iteration : 1863
train acc:  0.90625
train loss:  0.2578926086425781
train gradient:  0.09757993496487366
iteration : 1864
train acc:  0.875
train loss:  0.2928367555141449
train gradient:  0.10024867975101402
iteration : 1865
train acc:  0.8828125
train loss:  0.29993587732315063
train gradient:  0.12711616929329936
iteration : 1866
train acc:  0.8359375
train loss:  0.3288121819496155
train gradient:  0.1557988761927916
iteration : 1867
train acc:  0.921875
train loss:  0.27926814556121826
train gradient:  0.09282424148812318
iteration : 1868
train acc:  0.875
train loss:  0.2549648880958557
train gradient:  0.08501781297929512
iteration : 1869
train acc:  0.8671875
train loss:  0.29134467244148254
train gradient:  0.09678627825684746
iteration : 1870
train acc:  0.828125
train loss:  0.34628674387931824
train gradient:  0.15375831323372413
iteration : 1871
train acc:  0.8671875
train loss:  0.35726144909858704
train gradient:  0.13972168876338165
iteration : 1872
train acc:  0.8828125
train loss:  0.2984735369682312
train gradient:  0.09735077979068547
iteration : 1873
train acc:  0.859375
train loss:  0.3877149224281311
train gradient:  0.2094975016754309
iteration : 1874
train acc:  0.8203125
train loss:  0.39797693490982056
train gradient:  0.17898057090182196
iteration : 1875
train acc:  0.84375
train loss:  0.36124229431152344
train gradient:  0.14669491528181394
iteration : 1876
train acc:  0.8828125
train loss:  0.3124580979347229
train gradient:  0.12377859652711647
iteration : 1877
train acc:  0.84375
train loss:  0.28280404210090637
train gradient:  0.10166618848373213
iteration : 1878
train acc:  0.8984375
train loss:  0.23933546245098114
train gradient:  0.09448557888742228
iteration : 1879
train acc:  0.875
train loss:  0.30839473009109497
train gradient:  0.08681434724898762
iteration : 1880
train acc:  0.890625
train loss:  0.23761318624019623
train gradient:  0.07576043341664261
iteration : 1881
train acc:  0.8828125
train loss:  0.3011907935142517
train gradient:  0.12196248166603935
iteration : 1882
train acc:  0.8125
train loss:  0.41874927282333374
train gradient:  0.16754278962585584
iteration : 1883
train acc:  0.8515625
train loss:  0.3150123357772827
train gradient:  0.11043915771636587
iteration : 1884
train acc:  0.859375
train loss:  0.2884325385093689
train gradient:  0.11803153195488512
iteration : 1885
train acc:  0.90625
train loss:  0.259749174118042
train gradient:  0.09679908177998968
iteration : 1886
train acc:  0.8671875
train loss:  0.33940979838371277
train gradient:  0.15046201124266048
iteration : 1887
train acc:  0.890625
train loss:  0.27211183309555054
train gradient:  0.10018552743198685
iteration : 1888
train acc:  0.8359375
train loss:  0.3374536633491516
train gradient:  0.1725124938695088
iteration : 1889
train acc:  0.8515625
train loss:  0.42952513694763184
train gradient:  0.21956278056892892
iteration : 1890
train acc:  0.8203125
train loss:  0.4464177191257477
train gradient:  0.23237895047429277
iteration : 1891
train acc:  0.90625
train loss:  0.22672176361083984
train gradient:  0.07943505282452552
iteration : 1892
train acc:  0.828125
train loss:  0.3497758209705353
train gradient:  0.12273229685181597
iteration : 1893
train acc:  0.8359375
train loss:  0.3137074410915375
train gradient:  0.14813796484441077
iteration : 1894
train acc:  0.8515625
train loss:  0.3580257296562195
train gradient:  0.16858762321933085
iteration : 1895
train acc:  0.859375
train loss:  0.27576008439064026
train gradient:  0.11928543238854887
iteration : 1896
train acc:  0.875
train loss:  0.3244268000125885
train gradient:  0.10800372212834487
iteration : 1897
train acc:  0.8984375
train loss:  0.32085752487182617
train gradient:  0.13094912936980813
iteration : 1898
train acc:  0.8828125
train loss:  0.245475172996521
train gradient:  0.07742877851563054
iteration : 1899
train acc:  0.8515625
train loss:  0.39013028144836426
train gradient:  0.13475376641520614
iteration : 1900
train acc:  0.890625
train loss:  0.2523827850818634
train gradient:  0.09128746847907876
iteration : 1901
train acc:  0.8515625
train loss:  0.3425300717353821
train gradient:  0.1568646085814314
iteration : 1902
train acc:  0.8203125
train loss:  0.3651300370693207
train gradient:  0.15383287285927263
iteration : 1903
train acc:  0.875
train loss:  0.3511638939380646
train gradient:  0.17623638843770023
iteration : 1904
train acc:  0.8671875
train loss:  0.2609507143497467
train gradient:  0.10011062516272608
iteration : 1905
train acc:  0.8828125
train loss:  0.3219500482082367
train gradient:  0.13230509016041847
iteration : 1906
train acc:  0.90625
train loss:  0.2267221212387085
train gradient:  0.09318843040629511
iteration : 1907
train acc:  0.8515625
train loss:  0.33425581455230713
train gradient:  0.1369713932362263
iteration : 1908
train acc:  0.8828125
train loss:  0.2985774874687195
train gradient:  0.11380348865776076
iteration : 1909
train acc:  0.90625
train loss:  0.2937683165073395
train gradient:  0.11780423313097373
iteration : 1910
train acc:  0.8203125
train loss:  0.368648499250412
train gradient:  0.15433921960807892
iteration : 1911
train acc:  0.84375
train loss:  0.3126876950263977
train gradient:  0.09971373039799925
iteration : 1912
train acc:  0.859375
train loss:  0.2494809329509735
train gradient:  0.07329087265910068
iteration : 1913
train acc:  0.8828125
train loss:  0.2968848645687103
train gradient:  0.11796137583211125
iteration : 1914
train acc:  0.890625
train loss:  0.2769498825073242
train gradient:  0.1063355307171712
iteration : 1915
train acc:  0.8359375
train loss:  0.3474653363227844
train gradient:  0.1619231248658001
iteration : 1916
train acc:  0.859375
train loss:  0.3689708113670349
train gradient:  0.12466674488231316
iteration : 1917
train acc:  0.8515625
train loss:  0.3345631957054138
train gradient:  0.15953886489607932
iteration : 1918
train acc:  0.8671875
train loss:  0.3139149844646454
train gradient:  0.12095060220400097
iteration : 1919
train acc:  0.828125
train loss:  0.36009085178375244
train gradient:  0.1666848476239091
iteration : 1920
train acc:  0.875
train loss:  0.2923675775527954
train gradient:  0.10853112008504287
iteration : 1921
train acc:  0.8984375
train loss:  0.27156639099121094
train gradient:  0.13685476839357352
iteration : 1922
train acc:  0.8125
train loss:  0.38490867614746094
train gradient:  0.1321125049312313
iteration : 1923
train acc:  0.84375
train loss:  0.3679131269454956
train gradient:  0.13829360872175483
iteration : 1924
train acc:  0.890625
train loss:  0.3159332275390625
train gradient:  0.2561300132139842
iteration : 1925
train acc:  0.859375
train loss:  0.3331616222858429
train gradient:  0.11589599417602792
iteration : 1926
train acc:  0.875
train loss:  0.31001126766204834
train gradient:  0.09182250879054044
iteration : 1927
train acc:  0.875
train loss:  0.34123486280441284
train gradient:  0.12977189164823755
iteration : 1928
train acc:  0.8828125
train loss:  0.27121853828430176
train gradient:  0.08292705466612649
iteration : 1929
train acc:  0.796875
train loss:  0.3798843026161194
train gradient:  0.15145783092965104
iteration : 1930
train acc:  0.90625
train loss:  0.255010187625885
train gradient:  0.08003733343042857
iteration : 1931
train acc:  0.875
train loss:  0.3274845778942108
train gradient:  0.1407154122275564
iteration : 1932
train acc:  0.921875
train loss:  0.24769094586372375
train gradient:  0.07247200629742084
iteration : 1933
train acc:  0.890625
train loss:  0.24992816150188446
train gradient:  0.1035560606090156
iteration : 1934
train acc:  0.921875
train loss:  0.23414762318134308
train gradient:  0.11189975223487673
iteration : 1935
train acc:  0.8984375
train loss:  0.25794386863708496
train gradient:  0.08177226210498661
iteration : 1936
train acc:  0.8828125
train loss:  0.3360758125782013
train gradient:  0.14200725564419509
iteration : 1937
train acc:  0.890625
train loss:  0.2617323696613312
train gradient:  0.08247056821749801
iteration : 1938
train acc:  0.90625
train loss:  0.31165194511413574
train gradient:  0.1293891374906909
iteration : 1939
train acc:  0.859375
train loss:  0.292832612991333
train gradient:  0.11607966589598004
iteration : 1940
train acc:  0.8671875
train loss:  0.30705612897872925
train gradient:  0.09740215711471982
iteration : 1941
train acc:  0.8671875
train loss:  0.2951858639717102
train gradient:  0.10041321554603228
iteration : 1942
train acc:  0.8515625
train loss:  0.4117663502693176
train gradient:  0.13274477974316468
iteration : 1943
train acc:  0.8359375
train loss:  0.34194499254226685
train gradient:  0.21597243499375662
iteration : 1944
train acc:  0.890625
train loss:  0.28292232751846313
train gradient:  0.10222767492391184
iteration : 1945
train acc:  0.8046875
train loss:  0.38757532835006714
train gradient:  0.18152619493903654
iteration : 1946
train acc:  0.859375
train loss:  0.31874752044677734
train gradient:  0.10456513449035656
iteration : 1947
train acc:  0.859375
train loss:  0.33866268396377563
train gradient:  0.11514659339674957
iteration : 1948
train acc:  0.859375
train loss:  0.3470019996166229
train gradient:  0.1274909685472685
iteration : 1949
train acc:  0.859375
train loss:  0.3335522413253784
train gradient:  0.11652197097846198
iteration : 1950
train acc:  0.8359375
train loss:  0.3785191774368286
train gradient:  0.13570190399434273
iteration : 1951
train acc:  0.875
train loss:  0.2883310914039612
train gradient:  0.15565877333638545
iteration : 1952
train acc:  0.859375
train loss:  0.3388511538505554
train gradient:  0.20353012440172358
iteration : 1953
train acc:  0.875
train loss:  0.32891136407852173
train gradient:  0.14264363547676429
iteration : 1954
train acc:  0.90625
train loss:  0.24083641171455383
train gradient:  0.06710027130124123
iteration : 1955
train acc:  0.875
train loss:  0.3406626284122467
train gradient:  0.1368551464573846
iteration : 1956
train acc:  0.828125
train loss:  0.3453546166419983
train gradient:  0.11075785340686829
iteration : 1957
train acc:  0.796875
train loss:  0.3996294140815735
train gradient:  0.1704215807523382
iteration : 1958
train acc:  0.859375
train loss:  0.3144621253013611
train gradient:  0.10732112605361656
iteration : 1959
train acc:  0.8671875
train loss:  0.2969176471233368
train gradient:  0.09714339576724106
iteration : 1960
train acc:  0.875
train loss:  0.3095407485961914
train gradient:  0.13274068484638157
iteration : 1961
train acc:  0.8046875
train loss:  0.46181201934814453
train gradient:  0.25341962053560846
iteration : 1962
train acc:  0.8828125
train loss:  0.2791142463684082
train gradient:  0.14294083603294477
iteration : 1963
train acc:  0.9375
train loss:  0.22385437786579132
train gradient:  0.10292129163272505
iteration : 1964
train acc:  0.875
train loss:  0.3121609389781952
train gradient:  0.08934019896534506
